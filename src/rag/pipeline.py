# rag/pipeline.py
from __future__ import annotations
from typing import Any, Dict, List, Optional

import time
import jieba

from llm.base import build_llm
from llm.schemas import ChatMessage, GenerateConfig, LLMResponse

from .retrieval import DualIndexHybridRetriever, is_likely_english, sigmoid

_JIEBA_INITED = False


def translate_query(query: str, llm, target_lang: str) -> str:
    """åŒè·¯æ£€ç´¢çš„ç¿»è¯‘å™¨ï¼šåŒä¸€ä¸ªå¯æ’æ‹” LLM åç«¯å³å¯ï¼ˆAPI / æœ¬åœ°éƒ½è¡Œï¼‰"""
    if target_lang == "en":
        instr = "è¯·æŠŠä¸‹é¢é—®é¢˜ç¿»è¯‘æˆè‹±æ–‡ï¼Œåªè¾“å‡ºè‹±æ–‡ï¼Œä¸è¦è§£é‡Šï¼š"
    else:
        instr = "è¯·æŠŠä¸‹é¢é—®é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¾“å‡ºä¸­æ–‡ï¼Œä¸è¦è§£é‡Šï¼š"

    messages = [
        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¿»è¯‘å™¨ã€‚"),
        ChatMessage(role="user", content=f"{instr}\n{query}"),
    ]
    resp: LLMResponse = llm.generate(messages, GenerateConfig(max_tokens=128, temperature=0.0))
    return resp.text.strip()


def build_prompt(query: str, chunks: List[dict], max_chars: int = 4500) -> str:
    """å¼ºåˆ¶ citations + å…è®¸æ‹’ç­”ï¼ˆä¸¥æ ¼ JSON åè®®ï¼‰"""
    used = 0
    ctx_parts = []
    for i, ck in enumerate(chunks, 1):
        piece = f"[{i}] {ck.get('chunk_id','')} ({ck.get('page','')})\n{ck.get('content','')}\n"
        if used + len(piece) > max_chars:
            break
        ctx_parts.append(piece)
        used += len(piece)

    ctx = "\n".join(ctx_parts).strip()
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯æ®é—®ç­”åŠ©æ‰‹ã€‚ä½ åªèƒ½ä½¿ç”¨ã€èµ„æ–™ã€‘ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œç¦æ­¢ä½¿ç”¨å¸¸è¯†æˆ–çŒœæµ‹ã€‚

ã€è¾“å‡ºç¡¬æ€§è§„åˆ™ã€‘
- ä½ å¿…é¡»åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡æœ¬ã€ä¸è¦ markdownã€ä¸è¦ä»£ç å—ã€ä¸è¦ <think>ã€‚
- citations åªèƒ½ä»ã€èµ„æ–™ã€‘é‡Œå‡ºç°çš„ chunk_id ä¸­é€‰æ‹©ï¼›ä¸å¾—ç¼–é€ æˆ–å¼•å…¥æœªå‡ºç°çš„ chunk_idã€‚
- è‹¥èµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼šrefusal=trueï¼Œanswer=""ï¼Œcitations=[]ï¼ˆä¸è¦è§£é‡ŠåŸå› ï¼‰ã€‚
- è‹¥èµ„æ–™è¶³ä»¥å›ç­”ï¼šrefusal=falseï¼Œanswer ç”¨ä¸­æ–‡ç®€æ´å›ç­”ï¼ˆ1-3å¥ï¼‰ï¼Œcitations åˆ—å‡ºæ”¯æŒè¯¥ç»“è®ºçš„ chunk_idï¼ˆ1-3ä¸ªå³å¯ï¼‰ã€‚

ã€èµ„æ–™ã€‘
{ctx}

ã€é—®é¢˜ã€‘
{query}

ç°åœ¨åªè¾“å‡º JSONï¼š
{{"refusal": true/false, "answer": "...", "citations": ["chunk_id", "..."]}}
""".strip()


def run_rag_once(
    query: str,
    llm_cfg: Dict[str, Any],
    index_dir: str,
    refusal_threshold: float = 0.5,
    dual_lang: bool = True,
    dense_topk: int = 30,
    bm25_topk: int = 30,
    merge_topk: int = 60,
    fusion_topk: int = 120,
    rerank_topk: int = 8,
):
    """
    âœ… å®Œæ•´ RAGï¼šåŒè·¯æ£€ç´¢ +ï¼ˆé…é¢åˆ¶ï¼‰å€™é€‰èåˆ +ï¼ˆåŒè¯­ï¼‰rerank + ç”Ÿæˆ
    - å¼ºåˆ¶ï¼šæ£€ç´¢é˜¶æ®µä¿è¯ en/zh éƒ½è¿›å€™é€‰æ± ï¼Œé¿å… arxiv è¢« pdf æŒ¤æ‰
    - å¼ºåˆ¶ï¼šforced_refusal ç”¨ sigmoid æ¦‚ç‡é˜ˆå€¼åˆ¤æ–­ï¼ˆé˜ˆå€¼ 0.5 æ‰æœ‰æ„ä¹‰ï¼‰
    """
    global _JIEBA_INITED
    if not _JIEBA_INITED:
        jieba.initialize()
        _JIEBA_INITED = True

    t0 = time.time()

    # 1) æ„å»ºå¯æ’æ‹” LLMï¼ˆAPI / Local / LoRAï¼‰
    t1 = time.time()
    llm = build_llm(llm_cfg)
    print(f"[B] build_llm: {time.time()-t1:.3f}s")

    # 2) åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆFastAPI é‡Œå»ºè®®å¯åŠ¨æ—¶ new ä¸€æ¬¡ï¼Œè¿™é‡Œ demo å°±å…ˆè¿™æ ·ï¼‰
    t2 = time.time()
    retriever = DualIndexHybridRetriever(index_root=index_dir)
    print(f"[C] init retriever: {time.time()-t2:.3f}s")

    # 3) åŒè·¯ï¼šç¿»è¯‘ queryï¼ˆè®©ä¸­æ–‡é—®é¢˜ä¹Ÿèƒ½æœè‹±æ–‡ arxivï¼‰
    t3 = time.time()
    translated = None
    if dual_lang:
        if is_likely_english(query):
            translated = translate_query(query, llm, target_lang="zh")
        else:
            translated = translate_query(query, llm, target_lang="en")
    print(f"[D] translate_query: {time.time()-t3:.3f}s")

    # 4) retrieveï¼ˆdense+bm25 èåˆ + å€™é€‰é…é¢ + åŒè¯­ rerankï¼‰
    t4 = time.time()
    top_chunks, rmeta = retriever.retrieve(
        query=query,
        translated_query=translated,
        dense_topk=dense_topk,
        bm25_topk=bm25_topk,
        merge_topk=merge_topk,
        fusion_topk=fusion_topk,
        rerank_topk=rerank_topk,
        enable_rerank=True,
        return_meta=True,

        # âœ… ä½ å¯ä»¥æŒ‰éœ€æ±‚å¾®è°ƒï¼š
        # å€™é€‰æ±  en/zh å„ä¸€åŠï¼Œæœ€ç»ˆ top8 é‡Œä¹Ÿå°½é‡ä¿è¯ä¸€åŠè‹±æ–‡ï¼ˆé˜²æ­¢å…¨æ˜¯ pdfï¼‰
        fusion_en_ratio=0.5,
        rerank_en_ratio=0.5,
        min_en_candidates=10,
        min_zh_candidates=10,
    )
    print(f"[E] retrieve total: {time.time()-t4:.3f}s")

    # å– top1 åˆ†æ•°ï¼ˆlogitï¼‰ä¸æ¦‚ç‡ï¼ˆ0~1ï¼‰
    top1_logit = float(rmeta.get("top1_score") or -1e9)
    top1_prob = float(rmeta.get("top1_prob") or sigmoid(top1_logit))

    # âœ… é˜ˆå€¼åˆ¤æ–­åº”è¯¥åœ¨ 0~1 æ¦‚ç‡ç©ºé—´åš
    forced_refusal = top1_prob < float(refusal_threshold)

    citations = [c.get("chunk_id", "") for c in top_chunks if c.get("chunk_id")]

    # 5) prompt + generate
    t5 = time.time()
    prompt = build_prompt(query, top_chunks)
    messages = [
        ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é—®ç­”åŠ©æ‰‹ï¼Œåªæ ¹æ®è¯æ®å›ç­”ã€‚"),
        ChatMessage(role="user", content=prompt),
    ]
    resp = llm.generate(
        messages,
        GenerateConfig(max_tokens=512, temperature=0.2, return_meta=True),
    )
    print(f"[F] llm.generate: {time.time()-t5:.3f}s")

    print(f"[Z] total: {time.time()-t0:.3f}s")

    return {
        "query": query,
        "translated_query": translated,

        # æ£€ç´¢/æ‹’ç­”è¯Šæ–­ä¿¡æ¯ï¼ˆå»ºè®®å†™å…¥ runs ä¾¿äºåˆ†æï¼‰
        "top1_score": top1_logit,          # logit
        "top1_prob": top1_prob,            # 0~1
        "refusal_threshold": float(refusal_threshold),
        "forced_refusal": bool(forced_refusal),
        "retrieve_meta": rmeta,

        # è¯æ®ä¸è¾“å‡º
        "citations": citations,
        "top_chunks": [
            {"chunk_id": c.get("chunk_id"), "page": c.get("page"), "rerank_score": c.get("rerank_score")}
            for c in top_chunks
        ],
        "llm_raw": resp.text,
        "llm_meta": resp.meta,
    }
# æµå¼è¾“å‡º
def run_rag_stream(
    query: str,
    llm,
    retriever,
    dual_lang: bool = True,
    dense_topk: int = 30,
    bm25_topk: int = 30,
    merge_topk: int = 60,
    rerank_topk: int = 8,
):
    """
    âœ… çœŸÂ·æµå¼ RAGï¼ˆgeneratorï¼‰
    - å¯¹å¤–ï¼šyield æ–‡æœ¬ token
    - å†…éƒ¨ï¼šåŒæ—¶æ”¶é›† answer / citations / top_chunks
    """

    # ---------- 1) åŒè¯­ query ----------
    translated = None
    if dual_lang:
        if is_likely_english(query):
            translated = translate_query(query, llm, target_lang="zh")
        else:
            translated = translate_query(query, llm, target_lang="en")

    # ---------- 2) retrieve ----------
    top_chunks = retriever.retrieve(
        query=query,
        translated_query=translated,
        dense_topk=dense_topk,
        bm25_topk=bm25_topk,
        merge_topk=merge_topk,
        rerank_topk=rerank_topk,
    )

    # ---------- 3) prompt ----------
    # prompt = build_prompt(query, top_chunks)

    # messages = [
    #     ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŒ»å­¦é—®ç­”åŠ©æ‰‹ï¼Œåªè¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"),
    #     ChatMessage(role="user", content=prompt),
    # ]
    messages = build_rag_messages_strict(query, top_chunks)

    # ---------- 4) çœŸæ­£çš„æµå¼è¾“å‡º ----------
    answer_chunks = []
    # yield "[DEBUG] start streaming\n"
    for delta in llm.stream_generate(
        messages,
        GenerateConfig(max_tokens=512, temperature=0.2),
    ):
        answer_chunks.append(delta)
        yield delta   # ğŸ”¥ å…³é”®ï¼šå¯¹å¤– yield

    # ---------- 5) ç»“æŸæ ‡è®° ----------
    yield "\n"  # ç»™ CLI ä¸€ä¸ªè‡ªç„¶çš„ç»“æŸæ¢è¡Œ

    # ---------- 6) æŠŠç»“æ„åŒ–ç»“æœâ€œå¡â€åœ¨ generator çš„å±æ€§é‡Œ ----------
    final_answer = "".join(answer_chunks).strip()

    run_rag_stream.last_result = {
        "answer": final_answer,
        "citations": [c["chunk_id"] for c in top_chunks if c.get("chunk_id")],
        "top_chunks": [
            {
                "chunk_id": c.get("chunk_id"),
                "page": c.get("page"),
                "rerank_score": float(c.get("rerank_score", 0.0)),
            }
            for c in top_chunks
        ],
    }
