{"doc_id": "arxiv:2601.06193", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06193.pdf", "meta": {"doc_id": "arxiv:2601.06193", "source": "arxiv", "arxiv_id": "2601.06193", "title": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications", "authors": ["Qing He", "Dongsheng Bi", "Jianrong Lu", "Minghui Yang", "Zixiao Chen", "Jiacheng Lu", "Jing Chen", "Nannan Du", "Xiao Cu", "Sijing Wu", "Peng Xiang", "Yinyin Hu", "Yi Guo", "Chunpu Li", "Shaoyang Li", "Zhuo Dong", "Ming Jiang", "Shuai Guo", "Liyun Feng", "Jin Peng", "Jian Wang", "Jinjie Gu", "Junwei Liu"], "published": "2026-01-08T02:41:42Z", "updated": "2026-01-08T02:41:42Z", "summary": "The proliferation of Large Language Models (LLMs) presents transformative potential for healthcare, yet practical deployment is hindered by the absence of frameworks that assess real-world clinical utility. Existing benchmarks test static knowledge, failing to capture the dynamic, application-oriented capabilities required in clinical practice. To bridge this gap, we introduce a Medical LLM Benchmark MLB, a comprehensive benchmark evaluating LLMs on both foundational knowledge and scenario-based reasoning. MLB is structured around five core dimensions: Medical Knowledge (MedKQA), Safety and Ethics (MedSE), Medical Record Understanding (MedRU), Smart Services (SmartServ), and Smart Healthcare (SmartCare). The benchmark integrates 22 datasets (17 newly curated) from diverse Chinese clinical sources, covering 64 clinical specialties. Its design features a rigorous curation pipeline involving 300 licensed physicians. Besides, we provide a scalable evaluation methodology, centered on a specialized judge model trained via Supervised Fine-Tuning (SFT) on expert annotations. Our comprehensive evaluation of 10 leading models reveals a critical translational gap: while the top-ranked model, Kimi-K2-Instruct (77.3% accuracy overall), excels in structured tasks like information extraction (87.8% accuracy in MedRU), performance plummets in patient-facing scenarios (61.3% in SmartServ). Moreover, the exceptional safety score (90.6% in MedSE) of the much smaller Baichuan-M2-32B highlights that targeted training is equally critical. Our specialized judge model, trained via SFT on a 19k expert-annotated medical dataset, achieves 92.1% accuracy, an F1-score of 94.37%, and a Cohen's Kappa of 81.3% for human-AI consistency, validating a reproducible and expert-aligned evaluation protocol. MLB thus provides a rigorous framework to guide the development of clinically viable LLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06193v1", "url_pdf": "https://arxiv.org/pdf/2601.06193.pdf", "meta_path": "data/raw/arxiv/meta/2601.06193.json", "sha256": "1debc8e3632e987d5519ed9f37d70c5354d9790c4e97c48357d488f49fb9a347", "status": "ok", "fetched_at": "2026-02-18T02:22:25.708472+00:00"}, "pages": [{"page": 1, "text": "MLB: A Scenario-Driven Benchmark for Evaluating Large\nLanguage Models in Clinical Applications\nQing He1,* Dongsheng Bi1,*, Jianrong Lu2,1,*, Minghui Yang1, Zixiao Chen1, Jiacheng Lu1, Jing Chen1,\nNannan Du1, Xiao Cu1, Sijing Wu3, Peng Xiang4, Yinyin Hu3, Yi Guo3, Chunpu Li3, Shaoyang Li1,\nZhuo Dong1, Ming Jiang1, Shuai Guo1, Liyun Feng1, Jin Peng1, Jian Wang1, Jinjie Gu1, Junwei\nLiu1,5, †\n1 Ant Group, Hangzhou, China\n2 Zhejiang University, Hangzhou, China\n3 Health Information Center of Zhejiang Province, Hangzhou, China\n4 Department of AI and IT, The Second Affiliated Hospital, School of Medicine, Zhejiang University, Hangzhou, China\n5 School of Software and Microelectronics, Peking University, Beijing, China\n* The first three authors contributed equally to this work; †Corresponding Author.\n{bidongsheng.bds,minghui.ymh,bobblair.wj}@antgroup.com,jrong.alvin@gmail.com,liujunwei@stu.pku.edu.cn\nAbstract\nThe proliferation of Large Language Models (LLMs) presents trans-\nformative potential for healthcare, yet practical deployment is hin-\ndered by the absence of frameworks that assess real-world clini-\ncal utility. Existing benchmarks test static knowledge, failing to\ncapture the dynamic, application-oriented capabilities required in\nclinical practice. To bridge this gap, we introduce a Medical LLM\nBenchmark MLB, a comprehensive benchmark evaluating LLMs on\nboth foundational knowledge and scenario-based reasoning. MLB is\nstructured around five core dimensions: Medical Knowledge (Med-\nKQA), Safety and Ethics (MedSE), Medical Record Understanding\n(MedRU), Smart Services (SmartServ), and Smart Healthcare (Smart-\nCare). The benchmark integrates 22 datasets (17 newly curated)\nfrom diverse Chinese clinical sources, covering 64 clinical special-\nties. Its design features a rigorous curation pipeline involving 300\nlicensed physicians. Besides, we provide a scalable evaluation\nmethodology, centered on a specialized judge model trained via\nSupervised Fine-Tuning (SFT) on expert annotations. Our compre-\nhensive evaluation of 10 leading models reveals a critical transla-\ntional gap: while the top-ranked model, Kimi-K2-Instruct (77.3%\naccuracy overall), excels in structured tasks like information ex-\ntraction (87.8% accuracy in MedRU), performance plummets in\npatient-facing scenarios (61.3% in SmartServ). Moreover, the excep-\ntional safety score (90.6% in MedSE) of the much smaller Baichuan-\nM2-32B highlights that targeted training is equally critical. Our\nspecialized judge model, trained via SFT on a 19k expert-annotated\nmedical dataset, achieves 92.1% accuracy, an F1-score of 94.37%,\nand a Cohen’s Kappa of 81.3% for human–AI consistency, validat-\ning a reproducible and expert-aligned evaluation protocol. MLB\nthus provides a rigorous framework to guide the development of\nclinically viable LLMs.\nCCS Concepts\n• Computing methodologies →Natural language generation;\n• Applied computing →Health care information systems.\nKeywords\nLarge Language Models, Benchmark, Medical AI, Healthcare, Eval-\nuation, Supervised Fine-Tuning\n1\nIntroduction\nThe rapid advancement of LLMs holds immense promise for revolu-\ntionizing healthcare, with potential applications ranging from clini-\ncal decision support [30, 34] to specialized tasks like mental health\nanalysis [21, 22, 38] and patient message triage [31]. However, trans-\nlating this potential into safe and effective clinical practice requires\novercoming significant challenges related to professional accuracy,\nadaptability to complex scenarios, and adherence to stringent safety\nand ethical standards [3, 18, 28]. For example, current benchmarks\nfor medical LLMs, such as those based on standardized licensing\nexams [16, 25], primarily assess foundational medical knowledge\nthrough multiple-choice questions. While valuable, these evalu-\nations fail to measure the models’ practical utility in authentic\nclinical contexts [40] and, critically, lack a scalable and reliable\nmethod for assessing the open-ended, nuanced reasoning required\nin clinical interactions. This highlights an urgent need for a compre-\nhensive, application-oriented benchmark to guide the development\nand deployment of LLMs in healthcare.\nTo address this critical gap, we developed the MLB, a benchmark\ndesigned to systematically evaluate the end-to-end capabilities of\nLLMs in healthcare within the Chinese clinical healthcare context.\nThe MLB moves beyond static knowledge assessment to prioritize\nperformance in diverse, realistic clinical scenarios, thereby assess-\ning the breadth, depth, and practical relevance of current models.\nOur objective is to provide a scientific and objective framework\nfor quantifying the reliability, applicability, and safety of LLMs,\nfostering the development of high-quality, trustworthy medical AI,\naligning with the broader goal of leveraging web and AI technolo-\ngies for social good.\nThe main contributions of this work are as follows:\n(1) We introduce MLB a novel, scenario-driven benchmark\nconstructed from challenging, real-world data sources, in-\ncluding multi-turn physician-patient dialogues from web-\nbased telehealth platforms and authentic clinical records.\narXiv:2601.06193v1  [cs.LG]  8 Jan 2026\n"}, {"page": 2, "text": "Qing He et al.\nIts expert-driven (300 physicians) curation and difficulty\ngrading provide a robust assessment of practical clinical\nutility.\n(2) We propose and validate a scalable evaluation methodology\nfor complex, open-ended clinical tasks. By applying SFT to\na large corpus of expert-annotated judgments, we devel-\noped a specialized “judge\" model that achieves high-fidelity,\nreproducible, and cost-effective evaluation at scale.\n(3) We conducted a comprehensive evaluation of 10 leading\nLLMs. This analysis reveals a critical translational gap: top\nmodels like Claude-4-Sonnet achieve high proficiency in\nstructured tasks (88.8% in MedRU), yet the best-performing\nmodel, Kimi-K2-Instruct, only scores 61.3% in patient-facing\nscenarios (SmartServ). Our SFT-judge model proves essen-\ntial for this analysis, achieving 92.1% accuracy and 81.3%\nCohen’s Kappa, demonstrating near-human agreement.\n2\nRelated Work\nThe evaluation of LLMs [21, 41] in the medical domain has evolved\nfrom foundational knowledge tests to more comprehensive assess-\nments. Early and prominent benchmarks were largely adapted from\nexisting medical examinations. For instance, MedExQA [17] utilized\nquestions from the United States Medical Licensing Examination\n(USMLE), establishing a standard for assessing clinical knowledge.\nSimilarly, PubMedQA [16] focused on biomedical research ques-\ntions requiring yes/no/maybe answers, while MedMCQA [25] used\nmultiple-choice questions from Indian medical entrance exams.\nWhile these benchmarks are crucial for gauging a model’s knowl-\nedge base, their format does not capture the complexities of clinical\ninteraction and reasoning.\nRecognizing these limitations, subsequent efforts aimed for broader\nevaluations. The MultiMedQA benchmark [30] integrated several\nexisting datasets and introduced HealthSearchQA, a dataset of com-\nmonly searched consumer health questions, to assess the model’s\nability to provide layperson-friendly information. Another notable\neffort, MedBench [20], expanded the scope by including tasks re-\nlated to clinical notes, diagnostics, and patient conversations. These\nbenchmarks represent a significant step towards more holistic eval-\nuation.\nA recent, concurrent effort is OpenAI’s HealthBench [2], which\nalso emphasizes real-world scenarios through 5,000 simulated di-\nalogues. HealthBench employs a distinct methodology, utilizing\na generalist model (GPT-4.1) to score responses against unique,\nphysician-authored rubrics for each dialogue. While MLB shares\nthe goal of scenario-based evaluation, it provides a more diver-\nsified assessment framework, integrating 22 datasets across five\ndistinct dimensions (MedKQA, MedSE, MedRU, SmartServ, Smart-\nCare), rather than focusing solely on dialogue quality. Method-\nologically, our SFT-trained judge model is developed as a scalable\nproxy for human adjudication on disputed, open-ended tasks, a\ndifferent approach from HealthBench’s rubric-based model eval-\nuation. Furthermore, MLB’s scenarios are derived from authentic\nclinical records and web-based telehealth data, complementing\nHealthBench’s synthetically generated interactions.\nHowever, a critical gap remains in the systematic evaluation of\nLLMs in applied, scenario-based contexts that simulate real-world\nclinical workflows. While MedBench [20] incorporates clinical dia-\nlogue tasks, its scenarios often lack the complexity of real-world\ndata streams. MLB’s SceneCap dimension provides a more sub-\nstantial differentiation by simulating the entire clinical workflow.\nOur tasks are not isolated conversations but are constructed from\nauthentic, multi-turn patient-provider interactions sourced from\nweb-based telehealth platforms and anonymized electronic health\nrecords (EHRs).\nThis contrasts with benchmarks like MultiMedQA [30], which\nmerely aggregated existing datasets, or earlier web-tools focused\non information retrieval over clinical reasoning [29]. MLB’s innova-\ntion lies in its expert-driven, synthesis-oriented data curation. Our\nclinical experts (including web-health practitioners) fused disparate,\ncross-source medical records (e.g., lab reports, notes) to create novel,\ncomplex test cases. This methodology explicitly assesses an LLM’s\nability to handle the fragmented, conflicting data endemic to clinical\npractice—a dimension previous benchmarks failed to measure. Our\nuse of web-telehealth data aligns with recent work scaling medical\nAI via web technologies [9, 13, 14]. While parallel efforts address\nother modalities [8], robust conversational AI evaluation remains a\nkey challenge.\nCritically, while these benchmarks introduce more complex tasks,\nthey often rely on simplified metrics (e.g., string matching) or costly,\nnon-scalable human evaluation for open-ended questions. A signif-\nicant methodological gap remains in the scalable and reproducible\nscoring of nuanced clinical reasoning, a gap we address with our\nSFT-based evaluation protocol.\n3\nThe MLB Benchmark\nThe MLB is designed as a multi-layered evaluation system that\nassesses LLMs from foundational knowledge to practical applica-\ntion. A key challenge in this design is the reliable assessment of\nthe “SceneCap\" domain, which relies on open-ended clinical sce-\nnarios. We address this through a novel evaluation methodology\n(detailed in Section 3.6) centered on a specialized SFT-trained judge\nmodel. The benchmark’s hierarchical structure, detailed dataset\ncomposition, rigorous curation process, and sophisticated evalua-\ntion methodology collectively ensure a comprehensive and reliable\nassessment. Figure 2 shows the curation and evaluation pipeline.\nFigure 1: Overview of the MLB Benchmark. This figure illus-\ntrates the benchmark’s hierarchical structure, divided into\nFundamental Capabilities (FundCap) and Scenario-based Ca-\npabilities (SceneCap), which are further broken down into\nfive core dimensions (MedKQA, MedSE, MedRU, SmartServ,\nSmartCare).\n"}, {"page": 3, "text": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications\nFigure 2: Pipeline of Dataset Curation and Evaluation. The diagram outlines the end-to-end process for creating and evaluating\nthe MLB. It begins with data acquisition from diverse sources (e.g., EHRs, web-based dialogues), proceeds through a multi-\nparadigm generation pipeline (P1-P4), and undergoes rigorous expert review by 300 licensed physicians. The evaluation phase\nuses a hybrid scoring protocol in response judgement, culminating in our SFT-based judge model for resolving ambiguities.\n3.1\nEvaluation Dimensions\nThe benchmark framework is organized into a two-level hierar-\nchy. At the highest level, we distinguish between Fundamen-\ntal Capabilities (FundCap) and Scenario-based Capabilities\n(SceneCap). These are further broken down into five core dimen-\nsions, as illustrated in Figure 1.\n• Fundamental Capabilities (FundCap): This domain as-\nsesses the core knowledge and processing abilities of an\nLLM in the medical field. It is divided into three dimensions:\n(1) Medical Knowledge Question Answering (Med-\nKQA): Evaluates the accuracy and breadth of an LLM’s\nmedical knowledge and its ability to mitigate halluci-\nnations.\n(2) Medical Safety & Ethics (MedSE): Measures the\nmodel’s adherence to safety protocols, regulatory com-\npliance, and ethical principles in a medical context.\n(3) Medical Record Understanding (MedRU): Assesses\nthe model’s capacity to process and extract structured\ninformation from unstructured clinical text, such as\nelectronic health records (EHRs).\n• Scenario-based Capabilities (SceneCap): This domain\nevaluates the model’s performance in simulated real-world\nclinical applications. It is divided into two dimensions:\n(1) Smart Services (SmartServ): Focuses on patient-facing\ntasks, such as providing Health Education, Depart-\nment Guidance, report interpretation, and medica-\ntion instructions.\n(2) Smart Healthcare (SmartCare): Focuses on physician-\nsupport tasks, such as Medical Inspection Recom-\nmendation, assisting with Diagnostic Assistance,\nand Medical Record Writing Assistance.\n3.2\nDataset Composition\nTo operationalize this framework, the MLB comprises 22 distinct\ndatasets, including 17 newly constructed (proprietary) datasets and\n5 established public datasets. These datasets are derived from a\ndiverse range of sources, including clinical guidelines, medical text-\nbooks, authentic EHRs, and physician-patient dialogues, ensuring\nrobust and realistic evaluation scenarios. Figure 3 shows that the\noverall dataset contains over 350,000 questions, covering 64 clinical\nsub-specialties. Table 1 details the construction of the dataset The\nspecific specialty distribution is shown in Table 2.\n3.3\nDataset Curation Process\nThe integrity and quality of the benchmark are underpinned by a\nrigorous, human-in-the-loop curation process. This process com-\nbines the scalability of automated generation with the precision of\nexpert verification.\n3.3.1\nDataset Preparation. Our data curation strategy combined\nthe use of established public benchmarks with the development of\nnovel, purpose-built datasets to ensure both broad coverage and\ntargeted assessment of advanced capabilities. This dual approach\nyielded a comprehensive collection of 22 datasets, comprising 5\npublic and 17 proprietary datasets.\n"}, {"page": 4, "text": "Qing He et al.\nFigure 3: Overview of the dataset composition, including total sample size, distribution across dimensions, average question\nlength, and difficulty level. The chart displays the distribution of MLB’s 22 datasets across the five core dimensions. It highlights\nthe total sample size (4250 samples), the source of data (17 proprietary, 5 public), and the average token length of questions,\nindicating the complexity of the tasks.\nTable 1: Overview of Datasets, Evaluation Methods and Sampling Strategy in the MLB. This comprehensive table details\nall 22 datasets within MLB, organized by their primary, secondary, and tertiary dimensions. It specifies the data source\n(Public/Proprietary), question type, scoring metric, original and final sampled item counts (N), and the key attributes (e.g.,\ndifficulty level, department) used for our stratified and proportional sampling strategy.\nPrimary Dimension\nSecondary Dimension\nTertiary Dimension Intro.\nDataset\nSource\nType\nScoring Method\nOriginal N\nSampled N\nSampling Key(s)\nFundCap\nMedKQA\nMed. Knowledge\nCMExam\nPublic\nMultiple-Choice\nAccuracy\n6811\n100\ndifficulty level\nMedKQA\nMed. Knowledge\nMAExam\nProprietary\nMultiple-Choice\nAccuracy\n1000\n200\ndifficulty level\nMedKQA\nMed. Hallucination\nMAHalt\nProprietary\nMultiple-Choice\nAccuracy\n5000\n200\ntype, difficulty level\nMedSE\nGeneral Safety\nGeneralSafety\nProprietary\nOpen-Ended\nAI-Assisted w/ Expert Adj.\n3600\n200\ndifficulty level\nMedSE\nMedical Safety\nMASafety\nProprietary\nMultiple-Choice\nAccuracy\n5000\n200\nsource, difficulty level\nMedSE\nMed. Compliance\nMACompliance\nProprietary\nOpen-Ended\nAI-Assisted w/ Expert Adj.\n3000\n200\nrisk type, difficulty level\nMedSE\nMedical Ethics\nMAEthic\nProprietary\nOpen-Ended\nAI-Assisted w/ Expert Adj.\n500\n200\nrisk type, difficulty level\nMedRU\nTerm Normalization\nCHIP-CDN\nPublic\nOpen-Ended\nMicro-F1\n2000\n100\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nCMeEE\nPublic\nOpen-Ended\nMicro-F1\n5000\n100\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nCHIP-CDEE\nPublic\nOpen-Ended\nMicro-F1\n384\n100\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nCMeIE\nPublic\nOpen-Ended\nMicro-F1\n3585\n100\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nMAEESymp\nProprietary\nOpen-Ended\nMicro-F1\n1040\n200\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nMAEEDisease\nProprietary\nOpen-Ended\nMicro-F1\n716\n200\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nMAEEMedic\nProprietary\nOpen-Ended\nMicro-F1\n393\n150\ndepartment, difficulty level\nMedRU\nInfo. Extraction\nMAEEOper\nProprietary\nOpen-Ended\nMicro-F1\n580\n200\ndepartment, difficulty level\nSceneCap\nSmartServ\nHealth Education\nMAKnowledge\nProprietary\nMultiple-Choice\nAccuracy\n6983\n300\ndepartment, difficulty level\nSmartServ\nDepartment Guidance\nMAGuidance\nProprietary\nMultiple-Choice\nAccuracy\n4837\n300\ndepartment, difficulty level\nSmartServ\nReport Interp.\nMAReport\nProprietary\nMultiple-Choice\nAccuracy\n3000\n300\ndepartment, difficulty level\nSmartServ\nMedication Instruct.\nMAMedication\nProprietary\nMultiple-Choice\nAccuracy\n5932\n300\ndepartment, difficulty level\nSmartCare\nMedical Inspection Recommendation\nMAInspection\nProprietary\nOpen-Ended\nAI-Assisted w/ Expert Adj.\n3000\n200\ndepartment, difficulty level\nSmartCare\nDiagnostic Assistance\nMADiagnosis\nProprietary\nOpen-Ended\nAI-Assisted w/ Expert Adj.\n3000\n200\ndepartment, difficulty level\nSmartCare\nMed. Record Writing Assistance\nMARecord\nProprietary\nOpen-Ended\nAI-Assisted w/ Expert Adj.\n3000\n200\ndepartment, difficulty level\nPublic Datasets. To ground our benchmark in established tasks,\nwe incorporated five widely recognized Chinese medical language\nprocessing datasets. These included: CMExam [19], a dataset de-\nrived from the Chinese National Medical Licensing Examination;\nCHIP-CDN [4], a clinical diagnosis normalization task; and a suite\nof information extraction datasets including CMeEE [7] for named\nentity recognition, CHIP-CDEE [39] for structured clinical event\nextraction, and CMeIE [11] for identifying semantic relationships.\n"}, {"page": 5, "text": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications\nTable 2: Distribution of data samples across 9 major clinical\ndepartments and 64 sub-specialties. This table provides a\ndetailed breakdown of the samples annotated by clinical\nspecialty. Internal Medicine (722 samples) is the most highly\nrepresented category.\nMajor Department (Samples)\nSub-specialties\nInternal Medicine (722)\nRespiratory Medicine\nGastroenterology\nNeurology\nCardiology\nHematology\nNephrology\nEndocrinology\nRheumatology\nAllergy and Immunology\nGeriatrics Dept.\nSurgery (248)\nGeneral Surgery\nNeurosurgery\nOrthopedic Surgery\nUrology Surgery\nThoracic Surgery\nCardiovascular Surgery\nBurn Surgery\nPlastic Surgery\nPediatrics (43)\nNeonatology\nPediatric Cardiology\nPediatric Hematology\nPediatric Infectious Diseases Dept.\nPediatric Nephrology\nPediatric Genetics\nPediatric Gastroenterology\nPediatric Neurology\nPediatric Immunology\nPediatric Pulmonology\nPediatric Endocrinology\nObstetrics and Gynecology (119)\nGynecology\nWomen’s Health\nObstetrics\nReproductive Endocrinology and Infertility\nPediatric Surgery (24)\nPediatric General Surgery\nPediatric Cardiothoracic Surgery\nPediatric Orthopedics\nPediatric Neurosurgery\nPediatric Urology\nOther Departments (366)\nOphthalmology\nInfectious Diseases Dept.\nOccupational Medicine\nOtolaryngology\nTuberculosis (TB) Dept.\nAnesthesiology\nStomatology\nEndemic Diseases\nPain Management\nDermatology\nOncology\nCritical Care Medicine\nSexually Transmitted Diseases Dept.\nEmergency Medicine\nPreventive Medicine\nMedical Aesthetics Dept.\nRehabilitation Medicine\nNursing Dept.\nPsychiatry Dept.\nSports Medicine\nGeneral Health\nMedical Technology Dept. (11)\nPathology\nClinical Laboratory\nMedical Photography\nGeneral Practice Dept. (86)\nGeneral Practice Dept.\nChild Healthcare Dept. (8)\nChild Healthcare Dept.\nThese datasets provide a robust baseline for assessing foundational\nnatural language understanding capabilities in the medical domain.\nProprietary Dataset Construction. The 17 proprietary datasets\nwere meticulously constructed to address gaps in existing bench-\nmarks, particularly concerning clinical safety, ethics, and complex,\nscenario-based reasoning. Our construction methodology was cen-\ntered on a sophisticated, prompt-based data generation and anno-\ntation pipeline, leveraging advanced LLMs as scalable generators,\nfollowed by rigorous verification by clinical experts. This process\ncan be categorized into four primary paradigms.\nParadigm 1: Structured Knowledge Probing. This paradigm\nwas employed to create datasets for assessing core medical knowl-\nedge (MAExam, MAHalt), safety awareness (MASafety), and patient-\nfacing service capabilities (MAKnowledge, MAGuidance, MARe-\nport, MAMedication). The core methodology employed a suite of\nLLMs—namely Gemini-2.5-pro, Qwen3-235B-A22B, and DeepSeekR1-\n0528-671B—for the automated generation of medical examination\nitems. For instance, to generate items for MAReport, the LLM was\nprovided with an authentic, anonymized clinical report and a struc-\ntured prompt instructing it to formulate a clinically relevant multiple-\nchoice question, generate one unambiguously correct answer, and\ncreate four plausible yet incorrect distractors that target common\nmisconceptions.\nParadigm 2: Scenario-Based Assessment of Safety and Ethics.\nTo evaluate model alignment with legal and ethical standards, we de-\nveloped datasets comprising complex, open-ended scenarios (Gen-\neralSafety, MACompliance, MAEthic). The process began with the\ncuration of challenging scenarios by medical and legal experts.\nThese scenarios were framed as prompts to which a model gener-\nated a response. Subsequently, a separate set of structured, multi-\ndimensional evaluation prompts was used to score the generated\nresponse, assessing criteria such as compliance and risk awareness.\nParadigm 3: High-Fidelity Information Extraction from\nClinical Narratives. To assess nuanced information extraction,\nwe constructed four specialized datasets: MAEESymp (symptoms),\nMAEEDisease (diseases), MAEEMedic (medications), and MAEEOper\n(operations). The process started with a large corpus of anonymized\nChinese electronic health records. We employed a semi-automated\nannotation pipeline where an LLM performed an initial pass of en-\ntity labeling, followed by a two-stage manual review and correction\nprocess by our physician team to ensure gold-standard accuracy.\nParadigm 4: Simulation of Clinical Reasoning Tasks. This\ncategory of datasets (MAInspection, MADiagnosis, MARecord) was\ndesigned to simulate complex clinical reasoning, derived from au-\nthentic physician-patient dialogues and inpatient records sourced\nfrom web-based healthcare interactions. For instance, the construc-\ntion of the MADiagnosis dataset required clinical experts to sift\nthrough multiple, often conflicting, patient records to identify the\nmost salient information for formulating a diagnosis, thereby simu-\nlating the critical real-world skill of evidence synthesis from com-\nplex and noisy data sources. This methodology ensures that the\ntasks and their corresponding ground truths are firmly rooted in\nactual clinical practice.\n3.3.2\nClinical Department Annotation. To enable specialty-specific\nanalysis, we implemented a systematic department annotation pro-\ntocol for seven datasets within the SmartServ and SmartCare do-\nmains. We utilized a large language model to map each data in-\nstance to a standardized, two-tiered system of 9 primary and 64\nsecondary clinical departments. The annotation process adhered to\nstrict rules: a maximum of three departments per instance, restric-\ntion to the predefined department list, and a “No Department\" label\nfor non-clinical queries. This standardized annotation enhances\nthe benchmark’s utility for evaluating applications like automated\npatient department guidance.\n3.3.3\nExpert Review and Refinement. To ensure clinical fidelity,\nwe instituted a multi-stage quality assurance protocol led by a\npanel of 300 licensed physicians from Grade-A tertiary hospitals\nin China, including experts with experience in web-based medical\nservices. Our quality assurance involved a dual-annotator and cross-\nverification workflow. Two physicians independently reviewed each\nsample against five core criteria: natural and plausible, clinical\nvalidity, answer correctness, distractor plausibility (for multiple-\nchoice questions), and rubric matches question. Items that failed this\nrigorous review were either discarded or revised and re-validated,\nensuring every item in the MLB is accurate, relevant, and of high\nquality.\n3.4\nDifficulty Classification\nTo enable a fine-grained analysis of model capabilities, each ques-\ntion in the benchmark was assigned a difficulty level (Easy, Medium,\nHard). This classification was determined empirically based on the\nperformance of a panel of five diverse calibration LLMs (Mistral\nLarge 2 [15], Qwen2.5-72B [26], MedGemma-27B [27], Baichuan-\nM1-14B [35], Citrus1.0-Qwen-72B [36]). We employed three distinct\nmethodologies tailored to different question types.\n(1) Objective Multiple-Choice Questions: Difficulty was\nbased on model consensus. A question was classified as\nEasy if all 5 models answered correctly, Medium if 4 models\nanswered correctly, and Hard if 3 or fewer models answered\ncorrectly.\n(2) Open-Ended Questions with Standard Answers: For\ntasks like Named Entity Recognition (NER), we used the\n"}, {"page": 6, "text": "Qing He et al.\nmicro-F1 score. A response was considered correct if its\nmicro-F1 score exceeded a threshold of 0.8. Difficulty was\nthen assigned using the same consensus logic as for multiple-\nchoice questions.\n(3) Open-Ended Questions without Standard Answers:\nFor tasks in MedSE and SmartCare, we used a dual-model\nscoring system where two instances of our scoring model\n(GPT-4o-1120) independently scored each response on a\nscale of 0 to 1. If the scores diverged significantly (by more\nthan 0.25), the response was flagged for final judgement\nby a human medical expert. The final average score across\nthe five calibration models determined the difficulty: Easy\n(>0.7), Medium (0.6-0.7), and Hard (<0.6).\nThis data-driven approach to difficulty scoring allows for a nuanced\ndiagnosis of model weaknesses.\n3.5\nSample Selection\nTo ensure MLB is comprehensive and robust, we designed a system-\natic sampling strategy guided by two principles: comprehensive\ncoverage of clinical specialties/attributes and a controlled difficulty\ndistribution. Table 1 details the sampling attributes and data counts.\nFor datasets with explicit specialty labels (e.g., MADiagnosis,\nMARecord), we used stratified sampling. We partitioned data into 64\nclinical specialties and sampled proportionally from each stratum,\nensuring broad coverage. For datasets lacking specialty labels but\nhaving other key attributes (e.g., MASafety by risk type, MAHalt\nby hallucination type), we applied proportional sampling based on\nthe original distribution of attributes to ensure diverse assessment\ndimensions.\nTo ensure discriminatory power, we implemented a multi-step\niterative process to achieve a target difficulty distribution of 3:5:2\n(Hard:Medium:Easy), chosen to effectively differentiate model ca-\npabilities. The process was as follows:\n(1) Initial Sampling and Profiling: We sampled a prelim-\ninary set based on coverage principles and analyzed its\ndifficulty profile.\n(2) Gap Analysis and Supplementation: We calculated the\nshortfall against our 3:5:2 target and performed targeted\nsampling from the remaining data pool to fill these gaps,\nprioritizing under-represented categories.\n(3) Balancing and Verification: We made fine-tuning adjust-\nments (e.g., swapping items of the same specialty and diffi-\nculty) to precisely match the target ratio. The final set was\nverified to adhere to both coverage and difficulty principles.\n3.6\nEvaluation Methodology\nOur evaluation protocol was designed to be rigorous, fair, and effi-\ncient, incorporating a systematic sampling strategy, a mechanism\nto mitigate bias, and a hybrid scoring framework centered on our\nnovel SFT-based judge model.\n3.6.1\nDynamic Evaluation for Bias Mitigation. To mitigate posi-\ntional bias in multiple-choice questions, we implemented a dynamic\nshuffling strategy. Each question was presented to the model three\ntimes, with the order of options randomized in each trial. A question\nwas marked as correct only if the model provided the semantically\ncorrect answer across all three permutations. This method ensures\nthat models reason about the content of the options rather than\nrelying on positional heuristics.\n3.6.2\nHybrid Scoring Protocol. We developed a hybrid scoring pro-\ntocol that combines automated metrics for structured tasks with\nan AI-assisted, expert-adjudicated framework for complex, open-\nended questions.\nTable 3: List of LLMs Evaluated in This Study. The table\nlists the 10 diverse Large Language Models evaluated, in-\ncluding their originating organization, access type (Open-\nsource/Closed-source), and reported parameter size (where\navailable).\nModel\nOrganization\nType\nParameters\nKimi-K2-Instruct [32]\nMoonshot AI\nClosed\n∼1000B\nDeepSeekR1_0528 [5]\nDeepSeek\nOpen\n671B\nClaude-4-Sonnet [1]\nAnthropic\nClosed\nN/A\nGPT-5 [23]\nOpenAI\nClosed\nN/A\nBaichuan-M2 [33]\nBaichuan\nOpen\n32B\nDeepSeek-V3 [6]\nDeepSeek\nOpen\n671B\nQwen3-235B [37]\nAlibaba Cloud\nOpen\n235B\nGPT-4o [24]\nOpenAI\nClosed\nN/A\nCitrus1.0 [36]\nJD.com\nOpen\n72B\nLlama 3.3 [10]\nMeta AI\nOpen\n70B\nFully Automated Evaluation. For tasks with objective, verifiable\nanswers, scoring was fully automated.\n• Accuracy for Multiple-Choice Questions: This metric\nwas used for all multiple-choice datasets. A response was\nscored as correct only if it matched the ground truth across\nall three shuffled trials.\n• Micro-F1 Score for Structured Extraction: This metric\nwas applied to tasks like Named Entity Recognition. The\nMicro-F1 score was calculated by aggregating true positives,\nfalse negatives, and false positives across all entity classes.\nAI-Assisted Evaluation with Expert judgement. For open-ended\nquestions lacking a single ground truth, we employed a dual-assessor\nAI system with a human-in-the-loop workflow.\n• Initial AI Assessment: Each response was independently\nevaluated by two separate instances of our scoring model\n(GPT-4o-1120) against a predefined, multi-dimensional scor-\ning rubric.\n• Human Judgement Trigger: A manual review by a hu-\nman medical expert was automatically triggered if the scores\nfrom the two AI assessors exhibited a discrepancy greater\nthan 0.25 on a normalized 0-1 scale.\n• Expert Review Workflow: When triggered, such cases\nwere initially escalated to a physician from our expert panel\nfor definitive assessment.\nWhile this human-in-the-loop framework ensures gold-standard\naccuracy, it introduces a significant evaluation bottleneck. To ad-\ndress this challenge of scalability and cost, we developed our auto-\nmated judge model as a third-stage assessor.\n"}, {"page": 7, "text": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications\nAutomated Judge Model. A fundamental barrier to evaluating\ncomplex, open-ended scenarios at scale is the reliance on costly and\nsubjective human expert scoring. To overcome this, we introduce a\nspecialized “judge\" model as a core component of our evaluation\nmethodology. This model is engineered to serve as a consistent,\nscalable, and expert-aligned adjudicator for disputed or ambiguous\nresponses. We trained this model using Supervised Fine-Tuning\n(SFT) on a substantial corpus of 19,000 expert-annotated scoring\npoints, derived from complex cases where high-capability general-\nist models disagreed. This SFT process imparts the model with the\nnuanced decision-making capability of clinical experts, enabling a\nreproducible and cost-effective evaluation pipeline. The training\ncorpus was constructed by aggregating the expert-annotated scor-\ning points from the disputed questions generated during the evalu-\nation of nine distinct LLMs. This dataset captures a wide array of\ncomplex and ambiguous cases. The evaluation set was derived from\nthe complete set of 596 disputed questions generated by multiple\nmodels that was held out from the training data, ensuring a rigorous\ntest of generalization. This specialized SFT model is already in pre-\nliminary online deployment, providing decision-support assistance\nto clinicians and validating its real-world utility.\n4\nExperimental Results\n4.1\nEvaluated Models\nWe evaluated a diverse set of prominent LLMs on the MLB to as-\nsess the current state-of-the-art. The selection included both open-\nsource and closed-source models of varying sizes, representing a\ncross-section of the field’s leading contenders. The models evalu-\nated are listed in Table 3. All models were evaluated using their\nofficial API endpoints. To ensure the integrity of the evaluation\nand prevent any potential data contamination, the models were\nnot provided with any information about the questions, tasks, or\nanswer formats beforehand, and all interactions were conducted\nvia their respective APIs.\n4.2\nOverall Performance Analysis\nAs shown in Table 4, a clear performance hierarchy emerged. Kimi-\nK2-Instruct achieved the highest overall score of 77.29, demonstrat-\ning robust and well-rounded capabilities. It was closely followed\nby DeepSeekR1-0528-671B (75.43) and Claude-4-Sonnet (74.53). A\ncritical observation is the universal performance drop in the Smart-\nServ (Smart Services) dimension. Although Kimi-K2-Instruct leads\nthis category, its score of 61.25 is significantly lower than scores in\nother dimensions, and most models score below 55. This indicates a\nsubstantial gap between foundational knowledge and the ability to\napply it in interactive, patient-facing scenarios. In contrast, perfor-\nmance on MedRU (Medical Record Understanding) was consistently\nhigh across top models, suggesting that fundamental NLP tasks\nlike information extraction are relatively mature. Performance on\nMedSE (Medical Safety & Ethics) was also strong, with Baichuan-\nM2-32B achieving an exceptional 90.64, indicating highly effective\nalignment training.\n4.3\nPerformance on Sub-dimension SmartServ\nTo provide a more granular view, Figure 4 visualizes the perfor-\nmance of top models on SmartServ. This reveals specific strengths\nand weaknesses masked by aggregated scores. The results highlight\nthat even top-performing models have uneven capability profiles.\nFor example, Kimi-K2-Instruct showed superior performance, rank-\ning first in the MAKnowledge dimension. Notably, Baichuan-M2,\ndespite its smaller size, ranked fourth in the MAKnowledge.\nFigure 4: Bar chart comparing the performance of selected\nmodels on the SmartServ sub-dimension of the MLB.\n4.4\nJudge Model Performance\nValidating our SFT-based judge model is a prerequisite for our main\nfindings (Section 5.1). We confirmed it can reliably resolve scor-\ning discrepancies, surpassing general-purpose LLMs. The results\n(Table 5) demonstrate three key findings.\nFirst, our SFT-trained Qwen3-14B (trained on 19k data) achieves\nstate-of-the-art 92.13% accuracy and 94.37% F1-Score, significantly\noutperforming all general-purpose baselines, including much larger\nmodels like DeepSeek-V3 and GPT-5. Notably, the model main-\ntains consistent performance despite uneven input lengths (ranging\nfrom 764 to 1728 tokens). We utilized Qwen3-14B as the backbone\nbecause its compact parameter size, combined with robust per-\nformance, makes it a highly viable solution for practical clinical\ndeployment.\nSecond, this performance is a direct result of specialized SFT. An\nablation study confirms this: the base Qwen3-14B accuracy (71.02%)\nimproved to 83.25% (with 2k SFT samples) and finally to 92.13%\n(with 19k samples), validating that the model learns specialized\ncapabilities from the expert-annotated data.\nThird, the model achieved a Cohen’s Kappa of 81.32%, signifying\nhigh agreement with human experts. This confirms the model’s\njudgments are trustworthy and aligned with human reasoning,\nunlike the general-purpose models. Our SFT-based methodology\nthus provides a robust and trustworthy proxy for human-level\nevaluation, enabling the scalable analysis in this paper.\n5\nDiscussion\nThe results from our evaluation on the MLB provide several critical\ninsights into the landscape of medical LLMs.\nModel Scale and Specialization Drive Performance: The\ntop-ranking performance of Kimi-K2-Instruct, a model with a very\nlarge parameter count, suggests scale remains a significant factor in\nachieving high overall competence. Concurrently, the exceptional\nsafety score of the much smaller Baichuan-M2-32B highlights that\ntargeted training and fine-tuning are equally critical. The relatively\nmodest scores of powerful generalist models like GPT-4o and Llama\n3.3 further underscore that general-purpose excellence does not\nguarantee proficiency in the high-stakes medical domain.\nClear Evidence of Progress Within Model Families: Our\nresults demonstrate significant iterative improvement within model\nseries. For example, GPT-5 (73.87) substantially outperforms GPT-4o\n"}, {"page": 8, "text": "Qing He et al.\nTable 4: Overall Performance of Evaluated LLMs Across the Five Core Dimensions of the MLB. Scores represent the mean\nperformance. The best-performing model in each column is in bold, and the second-best is underlined.\nModel\nAverage\nMedKQA\nMedSE\nMedRU\nSmartServ\nSmartCare\nKimi-K2-Instruct [32]\n77.288\n73.17\n85.50\n87.75\n61.25\n78.77\nDeepSeekR1-0528-671B [12]\n75.434\n78.58\n85.09\n88.27\n53.33\n71.90\nClaude-4-Sonnet [1]\n74.534\n76.42\n73.67\n88.78\n58.42\n75.38\nGPT-5 [23]\n73.87\n76.00\n77.80\n88.02\n55.42\n72.11\nBaichuan-M2-32B [33]\n72.012\n65.25\n90.64\n83.60\n49.67\n70.90\nDeepSeek-V3 [6]\n70.498\n62.08\n83.92\n87.04\n49.75\n69.70\nQwen3-235B-A22B [37]\n65.886\n52.00\n80.61\n85.25\n32.58\n78.99\nGPT-4o [24]\n65.45\n49.25\n73.57\n85.86\n44.83\n73.74\nCitrus1.0-Qwen-72B [36]\n63.946\n67.50\n70.95\n78.14\n33.67\n69.47\nLlama 3.3-70B [10]\n54.022\n42.75\n58.70\n79.74\n32.67\n56.25\nTable 5: Performance of the Automated Judge Model on Dis-\nputed Cases, validating the SFT approach.\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\nCohen’s Kappa\nQwen3-235B-A22B, w/o SFT\n0.7772\n0.9416\n0.7160\n0.8135\n0.5493\nDeepSeek-V3, w/o SFT\n0.8677\n0.8559\n0.9679\n0.9085\n0.6730\nKimi-K2-Instruct-0905, w/o SFT\n0.8007\n0.8095\n0.9235\n0.8627\n0.5045\nGemini-2.5-pro, w/o SFT\n0.7588\n0.8872\n0.7383\n0.8059\n0.4943\nGLM-4.6, w/o SFT\n0.7513\n0.9103\n0.7030\n0.7933\n0.4937\nGPT-5-2025-08-07, w/o SFT\n0.6633\n0.9435\n0.5358\n0.6835\n0.3776\nQwen3-14B, w/o SFT\n0.7102\n0.8946\n0.6494\n0.7525\n0.4235\nQwen3-14B, with SFT on 2k data\n0.8325\n0.8458\n0.9210\n0.8818\n0.5962\nQwen3-14B, with SFT 19k data\n0.9213\n0.9163\n0.9728\n0.9437\n0.8132\n(65.45) across nearly all dimensions, indicating effective advance-\nments in architecture and training. A similar leap is observed from\nDeepSeek-V3 (70.50) to DeepSeekR1-0528-671B (75.43), showcasing\nthe rapid pace of progress in the field.\nScenario Application Bottleneck: The most striking finding\nis the universal difficulty models face in the SmartServ dimension.\nThis category requires more than simple knowledge retrieval; it\ndemands contextual understanding and the ability to interpret user\nintent from non-technical language. Tasks like report interpretation\nand Department Guidance are complex reasoning processes that\ncurrent LLMs struggle to execute reliably. This translational gap\nis the most significant challenge for the practical deployment of\nmedical LLMs in patient-facing roles.\nUnpacking Diverse Architectural Strengths: The MLB allows\nfor a nuanced analysis of each model’s unique profile.\n• Kimi-K2-Instruct is the best all-rounder, with a leading\noverall score for the challenging patient-facing (SmartServ)\ntasks.\n• DeepSeekR1-0528-671B excels in foundational knowl-\nedge, achieving the top score in MedKQA, making it a strong\ncandidate for knowledge-intensive applications.\n• Claude-4-Sonnet shows exceptional strength in structured\ndata processing, leading the MedRU dimension, which is\ncritical for tasks involving electronic health records.\n• Baichuan-M2-32B, despite its smaller size, presents a su-\nperior performance in safety alignment with its category-\ndefining score of 90.64 in MedSE.\n• Qwen3-235B-A22B demonstrates a surprising specializa-\ntion, securing the top position in SmartCare, suggesting it\nis highly optimized for physician-support tasks.\nA Methodological Contribution via SFT: Beyond the model\nperformance insights, our work presents a significant methodologi-\ncal contribution: the validation of an SFT-trained judge model for\nscalable, high-fidelity benchmark evaluation. The failure of even\npowerful generalist models (like DeepSeek-V3 and GPT-5) to match\nour specialized judge’s accuracy (92.13%) on disputed cases (Ta-\nble 5) proves that “judging\" nuanced medical responses is a distinct,\ncomplex skill. Our SFT approach provides a blueprint for future\nbenchmarks to move beyond costly human-in-the-loop evaluation,\nenabling the field to reliably assess the scenario-based capabilities.\nImplications and Future Directions: Our findings suggest that\nthe future of medical LLM development must prioritize scenario-\nbased training. This involves moving beyond static Q&A datasets to\nincorporate more interactive, dialogue-based tasks that mimic real\nclinical workflows. Furthermore, the development of specialized\nevaluation metrics for patient-facing communication is crucial. The\nMLB will continue to evolve, incorporating more complex clinical\nscenarios to push the frontier of medical AI evaluation.\n6\nConclusion\nIn this work, we introduced MLB, a comprehensive benchmark de-\nsigned to rigorously evaluate LLMs in clinical contexts. Through our\nmulti-dimensional assessment of leading models, we revealed criti-\ncal gaps in safety and reliability that traditional metrics overlook.\nBy leveraging our granular, scenario-driven evaluation benchmark\nto expose LLMs’ issues in the perspective of safety, ethics, and\ncompliance, this work bridges the trust gap between technical capa-\nbility and clinical reality, ensuring that future AI agents evolve from\nexperimental tools into trustworthy partners that safely empower\nphysicians and democratize high-quality patient care, adhering to\nthe vision of AI for social good.\n7\nEthical Considerations\nThe research protocol was approved by the Institutional Review\nBoard (IRB) under the protocol number 2025-074. This study was\nconducted in strict adherence to the ACM Publications Policy on\nResearch Involving Human Participants and Subjects. To ensure\n"}, {"page": 9, "text": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications\nprivacy and confidentiality, all data were fully de-identified prior\nto their use in this research.\nReferences\n[1] Anthropic. 2025. System Card: Claude Opus 4 and Claude Sonnet 4. Technical\nReport. Anthropic.\n[2] Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin\nQuiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, Johannes Heidecke, and Karan Singhal. 2025. Health-\nBench: Evaluating Large Language Models Towards Improved Human Health.\narXiv:2505.08775 [cs.CL] https://arxiv.org/abs/2505.08775\n[3] Lorenzo Cima, Alessio Miaschi, Amaury Trujillo, Marco Avvenuti, Felice\nDell’Orletta, and Stefano Cresci. 2025. Contextualized counterspeech: Strategies\nfor adaptation, personalization, and evaluation. In Proceedings of the ACM on\nWeb Conference 2025. 5022–5033.\n[4] Wenqian Cui, Xiangling Fu, Shaohui Liu, Xien Liu, and Ji Wu. 2023. Exploring\nsemantic information in disease: Simple Data Augmentation Techniques for\nChinese Disease Normalization. (2023).\n[5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu\nZhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang\nZhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li,\nZiyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda\nLu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai,\nDeli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo\nHao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\nWang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo,\nJiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li,\nJ. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan,\nKexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang,\nLiyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui\nTang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng\nZhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang,\nRuizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan\nZhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng\nZhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao\nYun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An,\nXiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin\nLiu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin,\nX. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang\nWang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q.\nWang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun,\nYaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi\nPiao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan\nOu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang\nLuo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping\nHuang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,\nYuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda\nXie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu,\nZihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan,\nZhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-\nR1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.\narXiv:2501.12948 [cs.CL] https://arxiv.org/abs/2501.12948\n[6] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun\nLin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang,\nHan Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian\nXin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi\nNi, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie\nQiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin\nHuang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong\nWang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua\nZhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen,\nR. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu\nZhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang\nZhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei,\nTianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wen-\nfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin,\nXianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang\nChen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang\nWang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song,\nXinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin,\nY. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong\nXu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang,\nYi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang,\nYishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,\nYu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He,\nYukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You,\nYuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu,\nZhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen\nHao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu\nWu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei\nXie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2025. DeepSeek-V3 Technical\nReport. arXiv:2412.19437 [cs.CL] https://arxiv.org/abs/2412.19437\n[7] Xiaojing Du, Hanjie Zhao, Danyan Xing, Yuxiang Jia, and Hongying Zan. 2024.\nMRC-based Nested Medical NER with Co-prediction and Adaptive Pre-training.\narXiv preprint arXiv:2403.15800 (2024).\n[8] Wei Fan, Jingru Fei, Dingyu Guo, Kun Yi, Xiaozhuang Song, Haolong Xiang,\nHangting Ye, and Min Li. 2025. Towards multi-resolution spatiotemporal graph\nlearning for medical time series classification. In Proceedings of the ACM on Web\nConference 2025. 5054–5064.\n[9] Michail N Giannakos. 2010. The evaluation of an e-learning web-based platform.\nIn International Conference on Computer Supported Education, Vol. 2. SCITEPRESS,\n433–438.\n[10] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo\nYang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun\nRao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste\nRoziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya\nNayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe\nTouret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,\nDamien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego\nPerino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank\nZhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That-\ntai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen,\nHannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra,\nIsabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon\nLee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer\nvan der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo\nPark, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu-\nden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth\nHeafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu,\nKunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten,\nLawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo\nMalo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh\nPasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli,\nMathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis,\nMin Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay\nBashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne,\nOnur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter\nWeng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,\nPuxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon\nCalderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Mah-\neswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan\nSumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana\nChennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov,\nShaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,\nShruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whit-\nman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky,\nTamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom,\nTobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,\nVibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie\nDo, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong,\nWenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang,\nXiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-\nschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue\nLi, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe\nPapakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam\nShajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon,\nAjay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet,\nAmit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew\nCaples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram-\nchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu\nChowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan,\nBeau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De\nPaola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram\nWasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl\n"}, {"page": 10, "text": "Qing He et al.\nParker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim,\nChao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph\nFeichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel\nLi, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana\nLiskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl-\ning, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood,\nEric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei\nSun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag-\ngioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz,\nGada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi,\nZhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han-\nnah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry\nAspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog,\nIgor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James\nGeboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus,\nJeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica\nZhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan\nMcPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U,\nKaran Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal\nChawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee\nBell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt,\nMadian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has-\nson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya\nLathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle\nRestrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike\nMacey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Raste-\ngari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White,\nNavyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko-\nlay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart,\nOmkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan\nBalaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag-\nina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel\nRodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran-\ngaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky\nWang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu,\nSamyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru\nPan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy,\nShaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha,\nShishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,\nSneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen,\nSteve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer\nDeng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney\nGoldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin-\nson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked,\nVarun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish\nKumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will\nConstable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao,\nYaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying\nZhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian,\nYunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen,\nZhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models.\narXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783\n[11] Tongfeng Guan, Hongying Zan, Xiabing Zhou, Hongfei Xu, and Kunli Zhang.\n2020. CMeIE: Construction and Evaluation of Chinese Medical Information\nExtraction Dataset. In Natural Language Processing and Chinese Computing,\nXiaodan Zhu, Min Zhang, Yu Hong, and Ruifang He (Eds.). Springer International\nPublishing, Cham, 270–282.\n[12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin\nXu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1:\nIncentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948 (2025).\n[13] Yu Guo, Zhengyi Ma, Jiaxin Mao, Hongjin Qian, Xinyu Zhang, Hao Jiang, Zhao\nCao, and Zhicheng Dou. 2022. Webformer: Pre-training with web pages for infor-\nmation retrieval. In Proceedings of the 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval. 1502–1512.\n[14] Cheng-Mei Hsu, Yu-Chu Yeh, and Jen Yen. 2009. Development of design criteria\nand evaluation scale for web-based learning platforms. International Journal of\nIndustrial Ergonomics 39, 1 (2009), 90–95.\n[15] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\nvendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023.\nMistral 7B.\narXiv:2310.06825 [cs.CL]\nhttps:\n//arxiv.org/abs/2310.06825\n[16] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu.\n2019. Pubmedqa: A dataset for biomedical research question answering. arXiv\npreprint arXiv:1909.06146 (2019).\n[17] Yunsoo Kim, Jinge Wu, Yusuf Abdulle, and Honghan Wu. 2024. MedExQA:\nMedical question answering benchmark with multiple explanations. arXiv\npreprint arXiv:2406.06331 (2024).\n[18] Peter Lee, Sebastien Bubeck, and Joseph Petro. 2023. Benefits, limits, and risks\nof GPT-4 as an AI chatbot for medicine. New England Journal of Medicine 388,\n13 (2023), 1233–1239.\n[19] Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu,\nHelin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, and Michael Lingzhi Li. 2023.\nBenchmarking Large Language Models on CMExam – A Comprehensive Chinese\nMedical Exam Dataset. arXiv:2306.03030 [cs.CL] https://arxiv.org/abs/2306.03030\n[20] Mianxin Liu, Jinru Ding, Jie Xu, Weiguo Hu, Xiaoyang Li, Lifeng Zhu, Zhian\nBai, Xiaoming Shi, Benyou Wang, Haitao Song, Pengfei Liu, Xiaofan Zhang,\nShanshan Wang, Kang Li, Haofen Wang, Tong Ruan, Xuanjing Huang, Xin Sun,\nand Shaoting Zhang. 2024. MedBench: A Comprehensive, Standardized, and\nReliable Benchmarking System for Evaluating Chinese Medical Large Language\nModels. arXiv:2407.10990 [cs.CL] https://arxiv.org/abs/2407.10990\n[21] Jianrong Lu, Shengshan Hu, Wei Wan, Minghui Li, Leo Yu Zhang, Lulu Xue,\nand Hai Jin. 2024. Depriving the survival space of adversaries against poisoned\ngradients in federated learning. IEEE Transactions on Information Forensics and\nSecurity 19 (2024), 5405–5418.\n[22] Jianrong Lu, Zhiyu Zhu, and Junhui Hou. 2025. ParaSolver: A Hierarchical Paral-\nlel Integral Solver for Diffusion Models. In The Thirteenth International Conference\non Learning Representations. https://openreview.net/forum?id=2JihLwirxO\n[23] OpenAI. [n. d.]. GPT-5 System Card. .\n[24] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam\nAltman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie\nBalcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan\nBello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bog-\ndonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, An-\ndrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan,\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen,\nMark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave\nCummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry,\nNoah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila\nDunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fe-\ndus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao,\nElie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha\nGontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene,\nJoshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade\nHickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu,\nJoost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer\nKaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim,\nJan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kon-\ndraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger,\nVishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel\nLevy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,\nTheresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam\nManning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMil-\nlan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey\nMishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing,\nTong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano,\nRajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,\nCullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,\nGiambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail\nPavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael\nPetrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,\nVitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl,\nRaul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis\nReal, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario\nSaltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica\nShieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Na-\ntalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang,\nNikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Eliza-\nbeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe,\nAndrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay\nWang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila\nWelihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\nClemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin\nWu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan,\n"}, {"page": 11, "text": "MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications\nWojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao,\nTianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4\nTechnical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774\n[25] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.\nMedmcqa: A large-scale multi-subject multi-choice dataset for medical domain\nquestion answering. In Conference on health, inference, and learning. PMLR,\n248–260.\n[26] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen\nYu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang\nLin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue,\nPei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia,\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical\nReport. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115\n[27] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine\nTraverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nJustin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Ste-\nfanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang,\nKejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Mercy\nAsiedu, Ines Mezerreg, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen,\nPreeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova,\nAlexandre Ramé, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geof-\nfrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Cas-\nbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry Lepikhin, Vlad Fein-\nberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi,\nLéonard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou,\nAvinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin,\nJonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins,\nPhoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak,\nRory Pilgrim, Daniel Golden, and Lin Yang. 2025. MedGemma Technical Report.\narXiv:2507.05201 [cs.AI] https://arxiv.org/abs/2507.05201\n[28] Lanyu Shang, Yang Zhang, Bozhang Chen, Ruohan Zong, Zhenrui Yue, Huimin\nZeng, Na Wei, and Dong Wang. 2024. MMAdapt: A knowledge-guided multi-\nsource multi-class domain adaptive framework for early health misinformation\ndetection. In Proceedings of the ACM Web Conference 2024. 4653–4663.\n[29] Barbara Simmons. 2010. Clinical reasoning: concept analysis. Journal of advanced\nnursing 66, 5 (2010), 1151–1158.\n[30] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won\nChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.\n2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023),\n172–180.\n[31] Prasanna Lakkur Subramanyam, Mohit Iyyer, and Brian N Levine. 2024. Triage\nof Messages and Conversations in a Large-Scale Child Victimization Corpus. In\nProceedings of the ACM Web Conference 2024. 4544–4554.\n[32] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen,\nRuijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui,\nHao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du,\nYu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong\nGao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao,\nTianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu,\nWeixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin,\nYongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao\nLi, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin,\nZongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu,\nLiang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo\nLiu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu\nMa, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo\nPeng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan\nSong, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen\nTao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang,\nJianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang,\nYao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang,\nZhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe\nWu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu,\nJinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu,\nZiyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang,\nZonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin,\nLonghui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao\nZhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang,\nYongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian\nZhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou,\nZaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. 2025. Kimi K2: Open\nAgentic Intelligence. arXiv:2507.20534 [cs.LG] https://arxiv.org/abs/2507.20534\n[33] M2 Team, Chengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang\nChen, Qiang Ju, Shuai Wang, Shunya Dang, Tianpeng Li, Xiangrong Zeng, Yijie\nZhou, Chenzheng Zhu, Da Pan, Fei Deng, Guangwei Ai, Guosheng Dong, Hongda\nZhang, Jinyang Tai, Jixiang Hong, Kai Lu, Linzhuang Sun, Peidong Guo, Qian\nMa, Rihui Xin, Shihui Yang, Shusen Zhang, Yichuan Mo, Zheng Liang, Zhishou\nZhang, Hengfu Cui, Zuyi Zhu, and Xiaochuan Wang. 2025. Baichuan-M2: Scaling\nMedical Capability with Large Verifier System. arXiv:2509.02208 [cs.LG] https:\n//arxiv.org/abs/2509.02208\n[34] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura\nGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models\nin medicine. Nature medicine 29, 8 (2023), 1930–1940.\n[35] Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei\nCheng, Xiangrong Zeng, Yupeng Zhang, Yuqi Huo, Zecheng Wang, Zhengyun\nZhao, Da Pan, Fei Kou, Fei Li, Fuzhong Chen, Guosheng Dong, Han Liu,\nHongda Zhang, Jin He, Jinjie Yang, Kangxi Wu, Kegeng Wu, Lei Su, Linlin Niu,\nLinzhuang Sun, Mang Wang, Pengcheng Fan, Qianli Shen, Rihui Xin, Shunya\nDang, Songchi Zhou, Weipeng Chen, Wenjing Luo, Xin Chen, Xin Men, Xionghai\nLin, Xuezhen Dong, Yan Zhang, Yifei Duan, Yuyan Zhou, Zhi Ma, and Zhiying\nWu. 2025. Baichuan-M1: Pushing the Medical Capability of Large Language\nModels. arXiv:2502.12671 [cs.CL] https://arxiv.org/abs/2502.12671\n[36] Guoxin Wang, Minyu Gao, Shuai Yang, Ya Zhang, Lizhi He, Liang Huang,\nHanlin Xiao, Yexuan Zhang, Wanyue Li, Lu Chen, Jintao Fei, and Xin Li.\n2025. Citrus: Leveraging Expert Cognitive Pathways in a Medical Language\nModel for Advanced Medical Decision Support.\narXiv:2502.18274 [cs.AI]\nhttps://arxiv.org/abs/2502.18274\n[37] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng\nLiu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong\nTang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou,\nJingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao\nDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,\nXingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su,\nYichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui,\nZhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Technical Report.\narXiv:2505.09388 [cs.CL] https://arxiv.org/abs/2505.09388\n[38] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia\nAnaniadou. 2024. MentaLLaMA: interpretable mental health analysis on social\nmedia with large language models. In Proceedings of the ACM Web Conference\n2024. 4489–4500.\n[39] Ningyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang, Lei Li, Xin Shang,\nKangping Yin, Chuanqi Tan, Jian Xu, Fei Huang, et al. 2021. Cblue: A chi-\nnese biomedical language understanding evaluation benchmark. arXiv preprint\narXiv:2106.08087 (2021).\n[40] Ziheng Zhang, Zhenxi Lin, Yefeng Zheng, and Xian Wu. 2025. How much Medical\nKnowledge do LLMs have? An Evaluation of Medical Knowledge Coverage for\nLLMs. In Proceedings of the ACM on Web Conference 2025. 5330–5341.\n[41] Ruiyang Zhou, Lu Chen, and Kai Yu. 2024. Is LLM a reliable reviewer? a compre-\nhensive evaluation of LLM on automatic paper reviewing tasks. In Proceedings\nof the 2024 joint international conference on computational linguistics, language\nresources and evaluation (LREC-COLING 2024). 9340–9351.\nReceived 23 November 2025\n"}]}