{"doc_id": "arxiv:2511.05933", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.05933.pdf", "meta": {"doc_id": "arxiv:2511.05933", "source": "arxiv", "arxiv_id": "2511.05933", "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs", "authors": ["Renfei Zhang", "Manasa Kaniselvan", "Niloofar Mireshghallah"], "published": "2025-11-08T08:56:29Z", "updated": "2025-11-08T08:56:29Z", "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.05933v1", "url_pdf": "https://arxiv.org/pdf/2511.05933.pdf", "meta_path": "data/raw/arxiv/meta/2511.05933.json", "sha256": "c7362868dd50f1e3a44679553c12267b2f8ccdb64820ee196e27e555a6354d07", "status": "ok", "fetched_at": "2026-02-18T02:28:13.829451+00:00"}, "pages": [{"page": 1, "text": "Reinforcement Learning Improves Traversal of\nHierarchical Knowledge in LLMs\nRenfei Zhang1, Manasa Kaniselvan2,3, Niloofar Mireshghallah2\n1Simon Fraser University, 2FAIR at Meta, 3ETH Zurich\nReinforcement learning (RL) is often credited with improving language model reasoning and generaliza-\ntion at the expense of degrading memorized knowledge. We challenge this narrative by observing that\nRL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on\npure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge\n(e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from\nimproved procedural skills in navigating and searching existing knowledge hierarchies within the model\nparameters. To support this hypothesis, we show that structured prompting—which explicitly guides\nSFTed models through hierarchical traversal—recovers most of the performance gap (reducing 24pp\nto 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves\nfinal-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths\non deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual\nrepresentations (e.g., activations for the statement “code 57.95 refers to urinary infection”) maintain\nhigh cosine similarity between SFT and RL models, query representations (e.g., “what is code 57.95”)\ndiverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than\nthe knowledge representation itself.\nMed\nQ: What is medical code 57.95 in ICD-9-CM?\nNon-reasoning model (V3): \nDirect Information Recall\n “ICD-9-CM code 57.95 refers to \nOther procedures for relief of \nelevated intraocular pressure, hence \nA is the answer”.\nA) Other procedures for relief of elevated intraocular pressure  B) Replacement of indwelling \nurinary catheter   C) Arthrodesis of other joint    D) Biopsy of heart\nAnswer: A\nReasoning model (R1): Hierarchical Navigation\n1. Define Problem Category: “57.95 is ICD-9-CM Volume 3, \nChapter 11, codes 57.0-57.99.” \n2. Identifies Main Procedure: “This is a subset of 57.0 … \nprocedures relating to clearance of bladder …”\n3. Eliminate other options : “Other procedures' in option \nA is not applicable ….” \n… So 57.95 must be Replacement of unary catheter …\nAnswer: B\nAlthough RLed models show superior accuracy, \nstructured prompting nearly closes this gap \nwithout training hinting that the knowledge \nexists in SFT models but needs navigation.\nAccuracy gap b/w  RL & SFT\nMedConceptsQA\nFigure 1\n(Left) Overview of our main observation: When querying structured medical codes, non-reasoning models\n(DeepSeek-V3) rely on direct memorization attempts, often selecting incorrect answers (here choosing A). In contrast,\nreasoning-enhanced RL models (DeepSeek-R1) employ systematic hierarchical navigation—first categorizing the\nproblem domain, then identifying relevant procedures, and finally interpreting ambiguous terminology—to successfully\nretrieve the correct answer (B). (Right) Reasoning models consistently outperform their instruction-tuned counterparts\nwhen prompted with conventional QA templates. This gap decreases when we optimize the prompt and is minimized\nwith our hand-crafted structured prompt, hinting that the necessary knowledge exists in the instruct models.\n1\nIntroduction\nLarge Language Models (LLMs) acquire vast parametric knowledge during pretraining, encoding facts,\nconcepts, and their relationships across billions of parameters. Post-training techniques—including supervised\n1\narXiv:2511.05933v1  [cs.CL]  8 Nov 2025\n"}, {"page": 2, "text": "fine-tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and specialized reasoning-focused\nRL—are then applied to transform these base models into instruction-following agents capable of complex\nreasoning (Yu et al., 2025; Wang et al., 2025c; Bai et al., 2022). While these methods improve performance\non reasoning benchmarks and user preference metrics, a growing body of evidence reveals a concerning\ntrade-off known as the “alignment tax” (Lin et al., 2024; Askell et al., 2021; Sorensen et al., 2025): models\nsacrifice factual memorization capabilities to optimize for other objectives, leading to reduced performance\non knowledge-intensive benchmarks (Yuan et al., 2024; Gekhman et al., 2024). However, existing work has\nprimarily focused on direct factual recall tasks over unstructured knowledge, leaving a critical gap: do these\ndegradation patterns hold for all forms of parametric knowledge retrieval tasks?\nTo address this question, we investigate tasks where retrieval demands navigating hierarchical structures\nencoded within the model’s parameters. Consider medical code lookup (Figure 1): to identify that ICD-9-CM\ncode 57.95 refers to “Replacement of indwelling urinary catheter,” a model can attempt direct recall—often\nfailing due to the vast code space—or systematically traverse the taxonomy (Chapter 11 →codes 57.0-57.99\n→specific procedure).\nSurprisingly, reasoning-enhanced models outperform their base counterparts by\n24 percentage points on MedConceptsQA, directly challenging the conventional wisdom that RL sacrifices\nmemorization for reasoning (Ghosh et al., 2024; Chu et al., 2025). We hypothesize these models succeed\nthrough systematic hierarchical navigation rather than direct recall, proposing that reinforcement learning\nenhances navigation of existing parametric knowledge rather than adding new factual content.\nTo disentangle knowledge acquisition from navigation, we design three complementary experiments. First,\ninspired by work showing prompt optimization can match RL gains (Agrawal et al., 2025; Khattab et al., 2023;\nZiems et al., 2025), we develop structured prompting that explicitly guides base models through hierarchical\ntraversal. If knowledge exists in base models, prompting should surface it. Structured prompting reduces\nthe 24pp gap between DeepSeek-V3 and DeepSeek-R1 to 7pp, suggesting information is present but inaccessible\nwithout proper navigation (Figure 1, right-hand side). Second, to validate that improved traversal drives these\ngains, we introduce a complexity-stratified patent classification dataset and Path Matching Score metric\nmeasuring traversal accuracy. We show that as recall depth increases (from fewer than 3 hops to more than\n5), reasoning models demonstrate superior path recall accuracy, with the performance gap widening from 5pp to\n9pp, demonstrating that reasoning models excel at complex hierarchical navigation (Table 5).\nThird, to provide internal validation, we conduct layer-wise representational analysis inspired by work\nexamining how post-training modifies internal model structure (Mukherjee et al., 2025; Skean et al., 2025;\nHuan et al., 2025; Agarwal et al., 2024). We extract layer-wise representations for matched query-answer\npairs, comparing interrogative queries (e.g., “What is the medical code 57.95?”) versus declarative statements\n(e.g., “Code 57.95 refers to urinary catheter replacement”). We find a striking pattern (Figure 3): declarative\nstatements maintain high cosine similarity (0.85-0.92) between base and RL models throughout most layers,\nwhile interrogative queries diverge substantially (similarity dropping to 0.65-0.73 in middle layers). This\nasymmetry reveals that RL and instruction tuning primarily transforms how models process questions while\nleaving factual knowledge representations intact, consistent with our hypothesis that RL enhances navigation\nmechanisms rather than knowledge content.\nWe further conduct ablation studies comparing distilled R1 models to R1 and base models (Kim et al., 2025;\nChen et al., 2025a), finding that distilled models capture only surface-level improvements without acquiring\nrobust navigation capabilities—achieving intermediate performance on complex retrieval tasks. Structured\nprompting provides minimal gains for distilled models, and layer-wise analysis reveals greater representational\nchanges than instruction-tuned variants, yet without improved deep-retrieval navigation.\nOur findings carry important implications: RL-enhanced models succeed not through expanded knowledge\nbut through improved cognitive scaffolding—the ability to systematically traverse structures already encoded\nduring pretraining, which is inline with recent work showing that RL surfaces intelligence (Huan et al., 2025;\nWu et al., 2025). While our experiments focus on two datasets (MedConceptsQA and IPC) and specific model\nfamilies (Qwen2.5, DeepSeek, Mistral), the patterns suggest more efficient training paradigms separating\nknowledge acquisition (pretraining) from organization (post-training). We encourage future work to investigate\nthese phenomena across broader domains and develop RL mechanisms that explicitly optimize for hierarchical\nnavigation.\n2\n"}, {"page": 3, "text": "2\nExperimental Methodology\nOur investigation into how reinforcement learning enhances hierarchical knowledge traversal is guided by\nthree research questions:\nResearch Questions\n1. RQ1: Does explicit prompting close the performance gap? If instruction-tuned models contain the\nrequired knowledge, can structured prompts that explicitly instruct hierarchical traversal match\nthe performance of RL-enhanced models?\n2. RQ2: Do reasoning models navigate deeper hierarchies better? On tasks requiring multi-step\nhierarchical traversal, do reasoning models demonstrate superior path accuracy beyond what\nprompting achieves?\n3. RQ3: How do internal representations differ? Do reasoning models transform how they encode\nqueries, factual knowledge, or both?\nWe address these questions through three complementary experiments.\nSection 2.1 demonstrates that\nstructured prompting can induce hierarchical reasoning in instruction-tuned models, reducing the performance\ngap by up to 68%. Section 2.2 introduces retrieval tasks of varying complexity with a path matching metric,\nrevealing that reasoning models excel particularly on deep-retrieval tasks requiring extensive hierarchical\nnavigation. Section 2.3 presents layer-wise activation analysis showing that while factual representations\nremain largely unchanged, query processing diverges substantially between SFT and RL models, supporting\nour hypothesis that RL primarily enhances navigation mechanisms rather than knowledge content.\n2.1\nHierarchical Navigation Through Structured Prompting\nWe investigate tasks requiring pure information recall without multi-step computation or logical deduction,\nto determine whether the performance gap between base and reasoning models can be mitigated through\nprompting strategies alone. Remarkably, structured prompting reduces the performance gap for 671B base\nmodels such as DeepSeek-V3 from 23.7 pp to 7.5 pp (a 68% gap reduction), demonstrating the effectiveness of\nour method.\nDatasets\n• MedConceptsQA: A multiple-choice question answering dataset focused on biomedical and clinical\nconcepts. The questions are designed to test factual recall of medical terminology, concept definitions,\nand their relationships, without reasoning over patient cases or performing calculations.\n• International Patent Classification (IPC): A dataset consists of queries mapped to patent classification\ncodes. The task requires identifying the correct category for a given technical description, relying on\nrecalling standardized knowledge of patent domains rather than multi-step reasoning.\nPrompting\n• Direct Question-Answering (QA) Prompting: This baseline requires the model to provide only a single-letter\nanswer to each multiple-choice question without any explanation.\n• Standard Chain-of-Thought (CoT) Prompting: This template requests both a final answer and a support-\ning explanation, aiming to capture the model’s intrinsic reasoning without imposing any procedural\nconstraints.\n• Structured Prompting: We introduce hierarchical instructions that enforce systematic reasoning. This\nstrategy involves a two-stage process: (1) recall the hierarchical structural breakdown of the relevant\nmedical code or concept, and (2) systematically evaluate each option with justification before elimination.\nThis approach tests our hypothesis that enforcing structured knowledge recall and stepwise elimination\ncan reduce performance gaps (see Appendix B.1 for complete prompt templates).\nModels We evaluate a diverse set of large language models, focusing on comparisons between base, instruction-\ntuned, reasoning, and distilled models. The first group includes instruction-tuned models, such as the\n3\n"}, {"page": 4, "text": "Table 1 Stratification of the “Nearest Common Ancestor” task by retrieval complexity, defined by the number of unique\nancestor nodes (traversals) recalled to find the common node.\nMemory-Light\n< 3\nQuery Nodes\nIntermediate Nodes\nCommon Ancestor\nQuestion :  Nearest common ancestor of H04B\n1/7075 and H04B 1/7083 is: A) H04B 1/707\nB) H04B 1/7073 C) H04B 1/69 D) H04B\nHierarchical Paths :\n H04B 1/7075 \n H04B 1/7073\n H04B 1/7083 \n H04B 1/7073\nAnswer :  B\nMemory-Heavy\n5+\nQuestion :  Nearest common ancestor of A01B\n3/421 and A01B 15/06 is: A) A01B 3/00 B)\nA01B 15/00 C) A01B D) A01\nHierarchical Paths :\n A01B 3/421 \n A01B 3/42 \n A01B 3/40 \n  A01B 3/36 \n A01B 3/00 \n A01B\n A01B 15/06 \n A01B 15/04 \n A01B 15/02\n  \n A01B 15/00 \n A01B\nAnswer :  C\nTask Complexity\nTraversals\nFigure Example\nExample\nQwen2.5 family (7B, 14B, 32B, and 72B parameters) (Team et al., 2024) and Mistral-Small-3.1-24B-Instruct\n(Karamcheti et al., 2021), each paired with their respective base models. The second group consists of reasoning\nmodels, including QwQ-32B (reasoning-enhanced Qwen2.5-32B), DeepSeek-R1 (from DeepSeek-V3), Magistral\n(from Mistral-Small-3.1-24B), and the reasoning model of Qwen3-235B-A22B (Liu et al., 2024; Guo et al.,\n2025a; Yang et al., 2025a). The third group includes models distilled from DeepSeek-R1: Qwen2.5-Math-7B,\nQwen2.5-32B, and Llama3.3-70B, each compared against their pre-distillation ones (Grattafiori et al., 2024).\nWe sample from all models using a temperature of 0.8 and top-p of 0.7 across three independent runs.\nPerformance is reported as both mean accuracy (± standard deviation) and majority-voted accuracy, where\nmajority voting selects the most frequent answer among the three runs for each question.\n2.2\nHierarchical Navigation Across Retrieval Complexity\nWhile we previously conclude that reasoning models use hierarchical navigation that can be externalized\nthrough structured prompting, a fundamental question remains: do reasoning models merely execute these\nstrategies more consistently, or are there tasks that they execute fundamentally better? To address this, we\nneed to analyze not just whether models retrieve correct answers but how they traverse knowledge hierarchies\nto reach those answers. Therefore, we extend the original IPC dataset to stratify it by retrieval complexity\nand introduce a new metric to measure path traversal quality. Subsequent results reveal that reasoning models\nshow superior hierarchical traversal-an ability that emerges on complex tasks requiring deeper knowledge\nnavigation.\nIPC Multi-Level Retrieval Dataset As shown in Table 1, this expanded dataset tests basic structural knowledge,\nincluding identifying common ancestors of a given pair of nodes. The questions are categorized by retrieval\ncomplexity, defined as the total number of ancestor nodes that must be recalled along both hierarchical paths\n(excluding the initial query nodes) to reach the nearest common ancestor. This stratification allows us to\nisolate the effect of retrieval depth on model performance.\n• Memory-Light (ML) tasks require recalling < 3 ancestor nodes total across both paths to reach the\ncommon ancestor.\n• Memory-Heavy (MH) tasks demand recalling ≥5 ancestor nodes across both paths.\nPath Matching Score To evaluate the quality of predicted hierarchical paths for IPC codes, we propose the\npath matching score, which combines two metrics:\n4\n"}, {"page": 5, "text": "Table 2 Performance comparison of Instruct vs. Reasoning models on MedConceptsQA and IPC datasets. The first\ncolumn indicates the dataset. Models are evaluated across three prompt templates (QA, CoT, Structured). Metrics\nshown are majority voting accuracy (Maj. Vote Acc.) and mean accuracy (Mean Acc.). Mean accuracy is reported\nas Mean Acc. (Std.), with the standard deviation in subscripted parentheses. For each model pair, a ∆row shows\nthe gap from the reasoning model for both Maj. Acc. (red) and Mean Acc. (green). Bold values indicate the best\nperformance within each model pair. ∆values are highlighted, with darker shades indicating larger gaps.\nDataset\nModel\nModel Type\nMaj. Vote Acc.\nMean Acc.(Std.)\nQA\nCoT\nStructured\nQA\nCoT\nStructured\nMedConceptsQA\nQwen2.5-32B\nInstruct\n0.379\n0.475\n0.469\n0.371(.012)\n0.449(.010)\n0.454(.007)\nReasoning\n0.482\n0.513\n0.505\n0.470(.012)\n0.487(.009)\n0.481(.005)\n∆\n+0.103\n+0.038\n+0.036\n+0.099\n+0.038\n+0.027\nQwen3-235B-A22B\nInstruct\n0.542\n0.548\n0.631\n0.503(.004)\n0.528(.005)\n0.589(.007)\nReasoning\n0.641\n0.656\n0.580\n0.599(.003)\n0.617(.003)\n0.554(.008)\n∆\n+0.099\n+0.108\n-0.051\n+0.096\n+0.089\n-0.035\nDeepSeek-V3\nInstruct\n0.541\n0.632\n0.717\n0.551(.014)\n0.636(.049)\n0.701(.026)\nReasoning\n0.778\n0.790\n0.792\n0.830(.006)\n0.774(.013)\n0.775(.026)\n∆\n+0.237\n+0.158\n+0.075\n+0.279\n+0.138\n+0.074\nIPC Codes\nQwen2.5-32B\nInstruct\n0.759\n0.754\n0.774\n0.759(.007)\n0.754(.000)\n0.774(.007)\nReasoning\n0.777\n0.875\n0.790\n0.713(.015)\n0.754(.070)\n0.769(.033)\n∆\n+0.018\n+0.121\n+0.016\n-0.046\n+0.000\n-0.005\nQwen3-235B-A22B\nInstruct\n0.800\n0.846\n0.846\n0.800(.013)\n0.846(.013)\n0.846(.013)\nReasoning\n0.908\n0.877\n0.893\n0.846(.013)\n0.836(.026)\n0.851(.015)\n∆\n+0.108\n+0.031\n+0.047\n+0.046\n-0.010\n+0.005\nDeepSeek-V3\nInstruct\n0.831\n0.923\n0.877\n0.846(.000)\n0.882(.007)\n0.872(.007)\nReasoning\n0.923\n0.892\n0.923\n0.913(.019)\n0.867(.026)\n0.903(.007)\n∆\n+0.092\n-0.031\n+0.046\n+0.067\n-0.015\n+0.031\n• F1-Score: Measures precision and recall of hierarchical ancestor identification, defined as F1 = 2×P ×R\nP +R ,\nwhere P and R denote precision and recall over the set of hierarchical ancestors (Buckland and Gey,\n1994).\n• Common Subsequence Score (CSS): Evaluates structural integrity of sequential paths via the ratio of\nthe Longest Common Subsequence (LCS) (Paterson and Dančík, 1994) between the predicted and true\npaths to the length of the true path: CSS = |LCS(predicted,ground truth)|\n|ground truth ancestors|\n.\nThe path matching score combines both components via harmonic mean: Path Matching = 2×F1×CSS\nF1+CSS . This\nmetric captures both structural accuracy and hierarchical coherence in patent classification navigation.\nModels To analyze the impact of retrieval complexity, we conduct a case study using the DeepSeek-V3 and R1\npair on our expanded IPC dataset. While a broader evaluation would be ideal, we select the DeepSeek pair\ndue to their instruction-following capabilities suitable for a reliable analysis.\n2.3\nHierarchical Navigation in Internal Representations\nTo investigate whether base and specialized models1 possess equivalent knowledge for hierarchical reasoning,\nwe analyze their internal activations on MedConceptsQA using contrastive question-answer pairs. We conduct\ntwo complementary analyses: an inter-model comparison to show how enhancement modifies representations\nrelative to the base model, and an intra-model comparison to trace how individual models transform questions\ninto answers across layers. Our findings show that enhancement refines query processing while preserving\nfactual knowledge.\nProbe Construction. We construct probes from the MedConceptsQA dataset, which spans five medical\nvocabularies: ATC, ICD9CM, ICD10CM, ICD9PROC, and ICD10PROC. To ensure balanced representation,\nwe randomly sample 100 question-answer pairs from each vocabulary. Each probe consists of a factual question\n1Here “base models” refer to the foundation model from which “specialized models” (instruction-tuned/reasoning/distilled)\nvariants are derived. We adopt this terminology throughout the section to clearly distinguish the two categories.\n5\n"}, {"page": 6, "text": "Table 3 Performance comparison of Base vs. Instruct models on MedConceptsQA and IPC datasets. The first column\nindicates the dataset. Models are evaluated across three prompt templates (QA, CoT, Structured). Metrics shown are\nmajority voting accuracy (Maj. Vote Acc.) and mean accuracy (Mean Acc.). Mean accuracy is reported as Mean\nAcc.(Std.), with the standard deviation in subscripted parentheses. For each model pair, an ∆row shows the gap from\nthe instruct model for both Maj. Acc. (red) and Mean Acc. (green). Bold values indicate the best performance within\neach model pair. ∆values are highlighted, with darker shades indicating larger gaps. This gap shrinks as we optimize the\nprompt, showing that the knowledge exists in the instruct model, it just needs to surface.\nDataset\nModel\nModel Type\nMaj. Vote Acc.\nMean Acc.(Std.)\nQA\nCoT\nStructured\nQA\nCoT\nStructured\nMedConceptsQA\nQwen2.5-7B\nBase\n0.148\n0.277\n0.286\n0.159(.007)\n0.239(.036)\n0.270(.012)\nInstruct\n0.295\n0.329\n0.313\n0.289(.006)\n0.316(.008)\n0.307(.015)\n∆\n+0.147\n+0.052\n+0.027\n+0.130\n+0.077\n+0.037\nQwen2.5-14B\nBase\n0.335\n0.332\n0.386\n0.316(.015)\n0.293(.025)\n0.372(.007)\nInstruct\n0.395\n0.420\n0.420\n0.385(.006)\n0.415(.007)\n0.409(.012)\n∆\n+0.060\n+0.088\n+0.034\n+0.069\n+0.122\n+0.037\nQwen2.5-32B\nBase\n0.221\n0.332\n0.404\n0.219(.012)\n0.260(.071)\n0.372(.007)\nInstruct\n0.379\n0.475\n0.469\n0.371(.012)\n0.449(.010)\n0.454(.007)\n∆\n+0.158\n+0.143\n+0.065\n+0.152\n+0.189\n+0.082\nQwen2.5-72B\nBase\n0.443\n0.351\n0.468\n0.389(.005)\n0.305(.028)\n0.418(.008)\nInstruct\n0.546\n0.520\n0.546\n0.519(.007)\n0.512(.005)\n0.537(.008)\n∆\n+0.103\n+0.169\n+0.078\n+0.130\n+0.207\n+0.119\nIPC Codes\nQwen2.5-7B\nBase\n0.463\n0.436\n0.588\n0.349(.040)\n0.364(.038)\n0.585(.038)\nInstruct\n0.615\n0.554\n0.574\n0.615(.025)\n0.554(.013)\n0.574(.015)\n∆\n+0.152\n+0.118\n-0.014\n+0.266\n+0.190\n-0.011\nQwen2.5-14B\nBase\n0.526\n0.608\n0.609\n0.421(.038)\n0.492(.033)\n0.600(.013)\nInstruct\n0.708\n0.691\n0.718\n0.708(.025)\n0.687(.029)\n0.718(.007)\n∆\n+0.182\n+0.083\n+0.109\n+0.287\n+0.195\n+0.118\nQwen2.5-32B\nBase\n0.644\n0.641\n0.777\n0.482(.059)\n0.503(.038)\n0.769(.013)\nInstruct\n0.759\n0.754\n0.774\n0.759(.007)\n0.754(.000)\n0.774(.007)\n∆\n+0.115\n+0.113\n-0.003\n+0.277\n+0.251\n+0.005\nand its corresponding ground-truth answer, formatted as declarative statements. For example, a probe for\nmedical code 0QD20Z from ICD10PROC takes the following form:\nQuestion: What is the description of the medical code 0QD20Z in ICD10PROC?\nAnswer: The description of the medical code 0QD20Z in ICD10PROC is extraction of right pelvic\nbone, open approach.\nWe process questions and answers independently through each model to extract their respective layer-wise\nrepresentations, enabling both inter-model and intra-model comparative analyses.\nRepresentation Extraction. For a model with L layers and hidden dimension d, we extract the hidden state\nat the final token position for each layer ℓ∈{1, . . . , L} as the layer’s representation vector hℓ∈Rd. This\nrepresentation attends to all preceding tokens, thereby capturing the full input context at that layer.\nRepresentation Analysis. We quantify representational differences across and within models using inter-model\nand intra-model analyses:\n• Inter-Model (Q-Q / A-A) Analysis. By comparing the question–question (Q–Q) and answer–answer (A–A)\nrepresentations between the base and specialized models, we assess how they differ at understanding\nquery and retrieving factual knowledge.\n• Intra-Model (Q-A) Comparison. This analysis investigates the internal transformation of information\nwithin a single model. By comparing a model’s question and answer representations layer by layer, we\ntrace how internal activations evolve from encoding a problem to producing a solution.\nComparison Metric For each layer ℓ∈{1, . . . , L}, we use cosine similarity, a measure of directional alignment,\nto define representation similarity:\n6\n"}, {"page": 7, "text": "Table 4 Performance of distilled models compared to the DeepSeek-R1 (reasoning model). Each cell for a distilled model\nshows its absolute score, followed in parentheses by the ∆gap (reasoning - distilled). ∆values for Maj. Vote Acc. are\nshaded red, and ∆values for Mean Acc. are shaded green. Darker shades indicate a larger performance gap. All ∆\nvalues are positive, showing the gap to the stronger R1 model.\nDataset\nModel\nMaj. Vote Acc. (∆vs. R1)\nMean Acc.(Std.) (∆vs. R1)\nQA\nCoT\nStructured\nQA\nCoT\nStructured\nMedConceptsQA\nDeepSeek-R1 (Reasoning)\n0.778\n0.790\n0.792\n0.830(.006)\n0.774(.013)\n0.775(.026)\nQwen2.5-Math-7B (Dist.)\n0.296 (+0.482)\n0.256 (+0.534)\n0.282 (+0.510)\n0.292(.010) (+0.538)\n0.250(.017) (+0.524)\n0.289(.017) (+0.486)\nQwen2.5-32B (Dist.)\n0.375 (+0.403)\n0.380 (+0.410)\n0.447 (+0.345)\n0.351(.009) (+0.479)\n0.369(.005) (+0.405)\n0.420(.002) (+0.355)\nLlama3.3-70B (Dist.)\n0.537 (+0.241)\n0.633 (+0.157)\n0.610 (+0.182)\n0.495(.002) (+0.335)\n0.609(.011) (+0.165)\n0.596(.012) (+0.179)\nIPC Codes\nDeepSeek-R1 (Reasoning)\n0.923\n0.892\n0.923\n0.913(.019)\n0.867(.026)\n0.903(.007)\nQwen2.5-32B (Dist.)\n0.778 (+0.145)\n0.730 (+0.162)\n0.788 (+0.135)\n0.754(.038) (+0.159)\n0.667(.019) (+0.200)\n0.780(.019) (+0.123)\nLlama3.3-70B (Dist.)\n0.785 (+0.138)\n0.831 (+0.061)\n0.815 (+0.108)\n0.785(.015) (+0.128)\n0.785(.041) (+0.082)\n0.790(.018) (+0.113)\nFigure 2 Comparative performance analysis of DeepSeek-V3 and DeepSeek-R1 across prompt strategies: direct question-\nanswering (Template 1), chain-of-thought (Template 2), and structured prompting (Template 3) on MedConceptsQA\ndataset. Four categories are defined based on the number of correct votes across three independent runs: “All Incorrect”\n(0/3 correct), “Majority Incorrect” (1/3 correct), “Majority Correct” (2/3 correct), and “All Correct” (3/3 correct).\nd(a,b)\ncos (ℓ) = 1 −1\nN\nN\nX\ni=1\nh(a)\nℓ(i)⊤h(b)\nℓ(i)\n∥h(a)\nℓ(i)∥2 ∥h(b)\nℓ(i)∥2\n,\n(1)\nHere, h(s)\nℓ(i) denotes the layer-ℓhidden representation for probe i from a source s. The set of sources\nS = {Qbase, Abase, Qspecialized, Aspecialized} includes representations for both the question (Q) and answer\n(A) components from the base and specialized models.\nPair (a, b) represents either inter-model (e.g.,\nQbase vs Qspecialized, Abase vs Aspecialized) or intra-model comparisons (e.g., Qbase vs Abase, Qspecialized vs\nAspecialized). Results are reported per vocabulary using N = 100 probes.\nModels We compare Qwen2.5-32B (base) against three specialized variants: Qwen2.5-32B-Instruct (instruction-\ntuned), DeepSeek-R1-Distill-Qwen-32B (distilled), and QwQ-32B (reasoning). We select this 32B parameter\nfamily because it spans multiple enhancement methods while remaining computationally tractable for single-\nGPU inference. A supplementary analysis comparing variants of the Mistral-Small-24B family (base, instruct,\nand reasoning) is included in Appendix C.\n7\n"}, {"page": 8, "text": "Table 5 Comparison of structured prompting performance by task complexity for DeepSeek-R1 and DeepSeek-V3\nmodels. Memory-Light tasks (1-2 hierarchical recalls); Memory-Heavy tasks (5+ hierarchical recalls). Bold values\nindicate the best performance for each metric within each complexity category. As we move to retrieval heavier tasks with\nstructure, the gap between path matching score of R1 and V3 increases.\nTask Complexity\nModel\nAccuracy (%)\nPath Matching Score\nMemory-Light\nDeepSeek-R1\n44.8\n0.681\nDeepSeek-V3\n37.9\n0.627\nMemory-Heavy\nDeepSeek-R1\n67.7\n0.597\nDeepSeek-V3\n67.7\n0.503\n3\nExperimental Results\n3.1\nHierarchical Navigation Through Structured Prompting\nHierarchical navigation and stepwise elimination strategies systematically narrow the accuracy gap between\nbase models and their reasoning-enhanced, or instruction-tuned versions across both MedConceptsQA and\nIPC code datasets. For example, on MedConceptsQA, structured prompting allows the Qwen3-235B Instruct\nmodel (Table 2) to outperform its reasoning counterpart, reversing a +0.108 majority vote accuracy gap (CoT)\nto a -0.051 advantage. Similarly, on the IPC dataset (Table 3), this prompting reduces the gap between the\nQwen2.5-32B base and instruct models from +0.115 (QA) to -0.003. However, this effect is less pronounced\nfor distilled models (Table 4), where the performance gap relative to the reasoning model remains substantial,\neven with structured prompts (e.g., Llama3.3-70B on MedConceptsQA, +0.182 gap).\nTo understand the mechanisms underlying structured prompting’s effectiveness, we examine response consis-\ntency patterns. Figure 2 presents results for DeepSeek-V3 and R1 across three independent runs under majority\nvoting on MedConceptsQA. When transitioning from the baseline to the structured prompt, DeepSeek-V3\nshows significant sample migration: questions initially categorized as “All Incorrect” and “Majority Incorrect”\nshift toward “Majority Correct” and “All Correct”. In contrast, R1 exhibits static distribution across these\ncategories, suggesting it already operates near its ceiling. This redistribution in V3 indicates that explicit\nstructural guidance improves the consistency of the model’s internal reasoning and that its underlying knowl-\nedge is sufficient. Therefore, the primary role of specialized post-training is not to introduce entirely novel\nknowledge, but rather to enhance the procedural consistency and strategic reasoning of existing knowledge\nstructures.\n3.2\nHierarchical Navigation Across Retrieval Complexity\nStratifying performance by retrieval complexity highlights a distinction between the base and reasoning\nmodels. Despite similar overall accuracy, R1 consistently achieves a higher path matching score, particularly\non complex tasks such as common ancestor identification, suggesting it can correctly navigate the hierarchy\nstep-by-step (Table 5). This is a deeper form of understanding that goes beyond simple memorization.\nUltimately, R1 understands the process of navigating a knowledge hierarchy better than the base model (V3),\neven when their final-answer accuracy is similar.\n3.3\nHierarchical Navigation in Internal Representations\nIntra-Model Representational Similarity. Within each model, representations for questions and answers are\ninitially highly similar, but this similarity decreases in later layers, suggesting that the representations\naccumulate increasingly distinct features.\nInter-Model Representational Similarity. Instruction-tuned and reasoning models show strong directional\nalignment with the base model for both question and answer representations, whereas the distilled model\nshows much greater divergence (Figure 3(d-f)). Notably, question representations diverge more than answer\nrepresentations across all specialized models, suggesting that performance gains arise primarily from refining\nquestion understanding rather than reorganizing factual knowledge.\n8\n"}, {"page": 9, "text": "Figure 3 Layerwise Representation Similarity for ICD9PROC Vocabulary from MedConceptsQA. Plots compare\nlast-token hidden state representations across layers (x-axis) using cosine similarity. Top Row (Intra-Model): Question\nvs. Answer representation similarity within QwQ-32B, Qwen2.5-32B-Instruct, and DeepSeek-R1-Distill-Qwen-32B.\nBottom Row (Inter-Model): Similarity between the base model (Qwen2.5-32B) and each respective advanced model,\ncomparing Question representations (QReason vs. QBase) and Answer representations (AReason vs. ABase) separately.\nThe representations of questions diverge more, specially in the last layer, compared to the answers. This hints at the knowledge\nbeing encoded similarly in base and reasoning models, but navigated differently.\n4\nRelated Work\nThis work is motivated by several research directions. Here, we focus on three areas and refer readers to\nAppendix A for an extended discussion of related work.\n4.1\nThe Alignment Tax and Factual Degradation\nThe trade-off between alignment and factual accuracy has been extensively explored. Lin et al. (2024)\nintroduced the concept of “alignment tax”, showing systematic degradation on factual benchmarks as RLHF\nreward strength increases. Achiam et al. (2023) similarly reported that RLHF “does not improve exam\nperformance (without active effort, it actually degrades it)” and can reduce calibration. Mechanistic analyses\nin Ghosh et al. (2024) reveal that instruction tuning primarily adjusts style rather than new knowledge, with\nresponses generated from pre-trained knowledge consistently outperforming those from models learning new\nknowledge through instruction tuning. Both Li et al. (2025) and Kirk et al. (2023) show that base models’\nparametric knowledge originates from pre-training while aligned models learn how to express it—training\ndirectly from base models mitigates knowledge forgetting and alignment tax incurred by SFT-based distillation.\nRecent work by Phan et al. (2025) reveals that optimizing for narrow verifiable rewards in reasoning-focused\nRL leads to regression in general capabilities, with models exhibiting increased hallucinations despite improved\nreasoning.\nWhile these studies document factual degradation from alignment, our work reveals a contrasting phenomenon:\nRL-enhanced models outperform their base counterparts on structured knowledge recall tasks. This apparent\ncontradiction suggests that alignment tax may not uniformly affect all forms of parametric knowledge\nretrieval—particularly when retrieval demands systematic navigation through hierarchical structures rather\nthan direct factual recall.\n9\n"}, {"page": 10, "text": "4.2\nReasoning Enhancement Through RL\nRL is commonly viewed as a means of amplifying reasoning ability. Process supervision and reward-driven\nmethods (Lightman et al., 2023; Ye et al., 2025) demonstrate clear improvements on reasoning tasks, with\nprocess-supervised models solving substantially more problems than outcome-supervised variants. However,\nrecent work hints at a more nuanced picture. Zelikman et al. (2024) introduce Quiet-STaR, showing that\ntraining models to generate internal rationales improves downstream reasoning by teaching systematic\nexploration of solution spaces—essentially navigation skills that achieve zero-shot improvements from 5.9% to\n10.9% on GSM8K. Shinn et al. (2023) demonstrate that reinforcement learning primarily helps models learn\nfrom feedback to refine their search through problem spaces, rather than acquiring new problem-solving rules.\nMost strikingly, Guo et al. (2025b) show that DeepSeek-R1 develops self-reflection, verification, and dynamic\nstrategy adaptation through RL alone, without human-labeled reasoning trajectories, increasing pass@1 scores\non AIME 2024 from 15.6% to 71.0%. Recent work on search-augmented reasoning (Jin et al., 2025; Chen\net al., 2025b) demonstrates models learning to autonomously generate search queries and self-correct, with\nbehaviors like pausing when detecting knowledge gaps emerging naturally.\nThese findings align with our hypothesis that RL enhances navigation of existing knowledge structures.\nHowever, while prior work focuses on mathematical and algorithmic reasoning, we examine whether these\nnavigation improvements extend to retrieval from structured factual hierarchies, providing complementary\nevidence that RL’s benefits stem from improved access patterns rather than new knowledge acquisition.\n4.3\nHierarchical Reasoning and Structured Navigation\nHierarchical reasoning frameworks further support our knowledge navigation hypothesis. Wang et al. (2025a)\npresent the Hierarchical Reasoning Model (HRM), a brain-inspired recurrent architecture that achieves\nnear-perfect performance on complex tasks with only 27 million parameters trained on 1000 samples, without\npre-training or chain-of-thought data. HRM’s architecture features interdependent modules for high-level\nabstract planning and low-level detailed computation, achieving 40.3% on ARC-AGI—precisely the type of\nstructured traversal we hypothesize enables medical code lookup. Yang et al. (2025b) show that hierarchical\nreinforcement learning on template sequences rather than long chain-of-thought data achieves 91.2% on\nMATH, outperforming models trained on detailed reasoning traces. Wang et al. (2025b) reveal RL training\ninduces emergent separation between high-level strategic planning and low-level procedural execution, with\ntwo-phase learning of procedural consolidation followed by strategic exploration.\nIn the medical domain, structured approaches demonstrate substantial gains. Liao et al. (2025) report that\nEHR-R1 achieves over 30 percentage points improvement on MIMIC-Bench (F1 of 0.6744 vs 0.3155 for\nGPT-4o) through graph-driven structured medical reasoning that converts raw EHR records into thinking\ngraphs encoding temporal relations and causal hypotheses. Work on ICD code classification (Liu et al., 2022;\nSen et al., 2021) shows that leveraging hierarchical structure through label-wise attention and multi-class\nreformulation improves classification, particularly at higher hierarchy levels.\nWhile these works demonstrate that hierarchical architectures and structured representations improve rea-\nsoning, they typically attribute gains to enhanced reasoning capabilities. Our work provides an alternative\ninterpretation: these improvements may stem from better navigation of knowledge hierarchies already encoded\nduring pretraining, rather than acquiring new reasoning abilities. We test this by showing that structured\nprompting—which explicitly guides traversal without modifying model parameters—recovers most performance\ngaps between base and RL models.\n5\nConclusion\nThis work challenges the view that reinforcement learning enhances reasoning at the expense of memorization.\nWe demonstrate that RL-enhanced models outperform base counterparts by 24pp on hierarchical knowledge\ntasks, not through acquiring new knowledge, but by improving navigation of existing structures. Structured\nprompting reduces this gap to 7pp on simple tasks, yet reasoning models maintain superior path traversal on\ncomplex deep-retrieval tasks (5pp to 9pp gap widening). Layer-wise analysis reveals that RL transforms query\n10\n"}, {"page": 11, "text": "processing (similarity drops to 0.65-0.73) while preserving factual representations (0.85-0.92), confirming that\nimprovements stem from enhanced navigation mechanisms rather than knowledge content changes.\nSeveral open questions warrant investigation. First, do similar navigation mechanisms underlie RL improve-\nments on other structured reasoning tasks such as mathematical proof generation, code debugging, or multi-hop\nquestion answering? Second, can we develop RL objectives that explicitly optimize for hierarchical navigation\nrather than relying on implicit emergence? Third, how do these findings extend to knowledge domains with\ndifferent structural properties—flat versus deeply nested hierarchies, dense versus sparse connectivity? Finally,\ncan we design hybrid approaches that combine the efficiency of structured prompting with the robustness of\nRL-trained navigation for practical deployment? Addressing these questions will deepen our understanding\nof how language models organize and access parametric knowledge, ultimately enabling more capable and\nefficient reasoning systems.\n11\n"}, {"page": 12, "text": "References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul\nBaltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim\nBrooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson,\nRory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen,\nMark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing\nDai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,\nSheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni,\nGabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShawn Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse,\nAlan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joanne Jang,\nAngela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz\nKaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\nKim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz\nKondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin,\nMateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor\nMarkov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,\nChristine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey\nMishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg\nMurk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo\nNoh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista\nParascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe\nde Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,\nVitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae,\nAditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song,\nNatalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B.\nThompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan\nFelipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin\nWang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian\nWeng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin\nWu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan\nZellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret\nZoph. Gpt-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774.\nRishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and\nOlivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The twelfth\ninternational conference on learning representations, 2024.\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi,\nHerumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G Dimakis, Ion Stoica,\nDan Klein, Matei Zaharia, and Omar Khattab. GEPA: Reflective prompt evolution can outperform reinforcement\nlearning, 2024. URL https://arxiv.org/abs/2507.19457. Note: Citation key was xu2024gepa but first author is\nAgrawal.\nLakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb\nShandilya, Michael J Ryan, Meng Jiang, et al. Gepa: Reflective prompt evolution can outperform reinforcement\nlearning. arXiv preprint arXiv:2507.19457, 2025.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph,\nBen Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal\nNdousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan.\nA general language assistant as a laboratory for alignment, 2021. URL https://arxiv.org/abs/2112.00861.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\n12\n"}, {"page": 13, "text": "Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez,\nDawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish,\nJoshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,\nNoemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,\nSheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan\nHume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom\nBrown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback, 2022. URL https://arxiv.org/abs/\n2212.08073.\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans.\nThe reversal curse: LLMs trained on \"A is B\" fail to learn \"B is A\". arXiv preprint arXiv:2309.12288, 2024.\nMichael Buckland and Fredric Gey. The relationship between recall and precision. Journal of the American society for\ninformation science, 45(1):12–19, 1994.\nHardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an\nearly investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468,\n2025a.\nMingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan,\nWen Zhang, Huajun Chen, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint\narXiv:2503.19470, 2025b.\nTianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine,\nand Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training. arXiv preprint\narXiv:2501.17161, 2025.\nMatteo Gabburo, Nicolaas Paul Jedema, Siddhant Garg, Leonardo FR Ribeiro, and Alessandro Moschitti. Measuring\nretrieval complexity in question answering systems. arXiv preprint arXiv:2406.03592, 2024.\nZorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does\nfine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904, 2024.\nSreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh\nManocha, et al. A closer look at the limitations of instruction tuning. arXiv preprint arXiv:2402.05119, 2024.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al.\nThe llama 3 herd of models.\narXiv preprint\narXiv:2407.21783, 2024.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\nXiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025a.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\nXiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025b.\nMaggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig,\nand Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning.\narXiv preprint arXiv:2507.00432, 2025.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-\nr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516,\n2025.\nSiddharth Karamcheti, Laurel Orr, Jason Bolton, Tianyi Zhang, Karan Goel, Avanika Narayan, Rishi Bommasani,\nDeepak Narayanan, Tatsunori Hashimoto, Dan Jurafsky, et al. Mistral–a journey towards reproducible language\nmodel training, 2021.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful\nHaq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls\ninto self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.\nMinwu Kim, Anubhav Shrestha, Safal Shrestha, Aadim Nepal, and Keith Ross. Reinforcement learning vs. distillation:\nUnderstanding accuracy and capability in llm reasoning. arXiv preprint arXiv:2505.14216, 2025.\n13\n"}, {"page": 14, "text": "Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette,\nand Roberta Raileanu.\nUnderstanding the effects of rlhf on llm generalisation and diversity.\narXiv preprint\narXiv:2310.06452, 2023.\nJunliang Li, Yucheng Wang, Yan Chen, Yu Ran, Ruiqing Zhang, Jing Liu, Hua Wu, and Haifeng Wang. Knowledge-level\nconsistency reinforcement learning: Dual-fact alignment for long-form factuality. arXiv preprint arXiv:2509.23765,\n2025.\nYusheng Liao, Chaoyi Wu, Junwei Liu, Shuyang Jiang, Pengcheng Qiu, Haowen Wang, Yun Yue, Shuai Zhen, Jian\nWang, Qianrui Fan, et al. Ehr-r1: A reasoning-enhanced foundational language model for electronic health record\nanalysis. arXiv preprint arXiv:2510.25628, 2025.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023. URL https://arxiv.org/abs/2305.20050.\nYong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu,\nHanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. Mitigating\nthe alignment tax of RLHF. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 2024.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.\nLeibo Liu, Oscar Perez-Concha, Anthony Nguyen, Vicki Bennett, and Louisa Jorm. Hierarchical label-wise attention\ntransformer model for explainable icd coding. Journal of biomedical informatics, 133:104161, 2022.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer,\nand Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.\narXiv preprint arXiv:2305.14251, 2023.\nSagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tür, and Hao Peng. Reinforcement learning finetunes small subnetworks\nin large language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025.\nURL https://openreview.net/forum?id=0NdS4xCngO.\nOded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval? comparing knowledge\ninjection in LLMs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\npages 237–250, 2024.\nMike Paterson and Vlado Dančík. Longest common subsequences. In International symposium on mathematical\nfoundations of computer science, pages 127–142. Springer, 1994.\nHoang Phan, Xianjun Yang, Kevin Yao, Jingyu Zhang, Shengjie Bi, Xiaocheng Tang, Madian Khabsa, Lijuan Liu,\nand Deren Lei. Beyond reasoning gains: Mitigating general capabilities forgetting in large reasoning models. arXiv\npreprint arXiv:2510.21978, 2025.\nLaura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert\nKirk, Tim Rocktäschel, Edward Grefenstette, and Max Bartolo. Procedural knowledge in pretraining drives reasoning\nin large language models. arXiv preprint arXiv:2411.12580, 2024.\nCansu Sen, Bingyang Ye, Javed Aslam, and Amir Tahmasebi. From extreme multi-label to multi-class: A hierarchical\napproach for automated icd-10 coding using phrase-level attention. arXiv preprint arXiv:2102.09136, 2021.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\nReflexion:\nLan-\nguage agents with verbal reinforcement learning.\nIn Advances in Neural Information Processing Systems 36\n(NeurIPS 2023). Curran Associates Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/\n1b44b878bb782e6954cd888628510e90-Abstract-Conference.html. arXiv:2303.11366.\nOscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by\nlayer: Uncovering hidden representations in language models. arXiv preprint arXiv:2502.02013, 2025.\nTaylor Sorensen, Benjamin Newman, Jared Moore, Chan Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang, and\nYejin Choi. Spectrum tuning: Post-training for distributional coverage and in-context steerability. arXiv preprint\narXiv:2510.06084, 2025.\nQwen Team et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2:3, 2024.\nGuan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori.\nHierarchical reasoning model. arXiv preprint arXiv:2506.21734, 2025a.\n14\n"}, {"page": 15, "text": "Haozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, and Wenhu Chen. Emergent hierarchical reasoning in\nllms through reinforcement learning. arXiv preprint arXiv:2509.03646, 2025b.\nZengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning\nscaling. arXiv preprint arXiv:2506.20512, 2025c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\nChain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages\n24824–24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.\nFang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui, and Yejin Choi. The invisible leash: Why\nrlvr may or may not escape its origin. arXiv preprint arXiv:2507.14843, 2025.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\nChenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a.\nLing Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux: Hierarchical llm reasoning via scaling thought\ntemplates. ArXiv, abs/2502.06772, 2025b. URL https://api.semanticscholar.org/CorpusID:276250066.\nYufan Ye, Ting Zhang, Wenbin Jiang, and Hua Huang. Process-supervised reinforcement learning for code generation.\narXiv preprint arXiv:2502.01715, 2025.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu,\nLingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476,\n2025.\nJiaqing Yuan, Lin Pan, Chung-Wei Hang, Jiang Guo, Jiarong Jiang, Bonan Min, Patrick Ng, and Zhiguo Wang.\nTowards a holistic evaluation of llms on factual knowledge recall. arXiv preprint arXiv:2404.16164, 2024.\nEric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. Quiet-STaR: Language\nmodels can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/2403.09629.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language\nmodels. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=WZH7099tgfM.\nNoah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan\nKlein, Matei Zaharia, et al. Multi-module grpo: Composing policy gradients and prompt optimization for language\nmodel programs. arXiv preprint arXiv:2508.04660, 2025.\n15\n"}, {"page": 16, "text": "A\nExtended Related Work\nA.1\nPrompting as an Alternative to RL\nThe possibility of achieving RL-like benefits through prompting has gained increasing attention. Agrawal\net al. (2024) demonstrate that Genetic-Evolution Prompt Alignment (GEPA) can outperform Group Relative\nPolicy Optimization by up to 20% while using 35× fewer computational resources. They argue that “the\ninterpretable nature of language provides a richer learning medium than sparse scalar rewards.” Wei et al.\n(2022) show that chain-of-thought prompting can match fine-tuned performance on reasoning tasks, while\nZhou et al. (2023) demonstrate that optimized prompts can exceed supervised fine-tuning. The “Invisible\nLeash” phenomenon (Wu et al., 2025) reveals that much of RLHF’s apparent benefit comes from teaching\nmodels to follow implicit formatting patterns—effects reproducible through prompting.\nA.2\nKnowledge Storage versus Knowledge Access\nThe distinction between knowledge acquisition and knowledge retrieval is crucial to our thesis. Ovadia et al.\n(2024) show that models fine-tuned on new knowledge often “hallucinate” by incorrectly combining existing\nknowledge rather than storing new information. Ruis et al. (2024) provide key insights with their finding\nthat models rely on procedural knowledge extracted from documents involving similar reasoning processes\nrather than memorizing new facts. This aligns with our hypothesis that RL enhances navigation strategies\nrather than expanding knowledge. Berglund et al. (2024) further support this through their “Reversal Curse”\nfindings—models trained on “A is B” cannot infer “B is A,” suggesting that training affects access patterns\nrather than creating bidirectional knowledge representations.\nA.3\nRetrieval Complexity in Knowledge-Intensive Tasks\nRecent work has begun to to examine the relationship between retrieval complexity and model performance in\nknowledge-intensive tasks. Gabburo et al. (2024) show that retrieval complexity extend beyond simple multi-\nhop reasoning—including temporal (15%), comparative (10%), and aggregate (16%) questions—suggesting that\ndifferent types of knowledge organization require distinct retrieval strategies. Min et al. (2023) demonstrate\nthat in long-form generation, factual accuracy in biographies drops as entity rarity increases, suggesting that\nretrieval difficulty directly impacts knowledge accessibility.\nB\nTechnical Appendices and Supplementary Material\nB.1\nZero-Shot Prompt Templates\nWe present three prompt templates used in MedConceptsQA and IPC, which are designed to elicit specific\nresponses from language models. These templates request:\n• Direct answers, both with and without explanations.\n• Structural recall of codes and a stepwise elimination of incorrect options.\nPrompt Template 1: MCQ with Final Answer Only\nAnswer only A,B,C,D according to the answer to this multiple choice question.\n[... Insert Question Text Here ...]\nAnswer (only the letter of your choice (A, B, C, or D)):\n16\n"}, {"page": 17, "text": "Prompt Template 2: MCQ with Explanation\nYou are a medical research assistant. Read the following multiple-choice question carefully. Your task\nis to:\n1. Answer each question with one of A/B/C/D, which corresponds to the four options.\n2. For my convenience, please give me a list of ANSWERs for the given instances in the format\n’Answer: ...’, with additional explanation for each answer in the format ’Explanation: ...’.\nRespond in the following format:\nAnswer: <A/B/C/D>\nExplanation: <your explanation here>\n[... Insert Question Text Here ...]\nAnswer:\nExplanation:\nPrompt Template 3: MCQ with Stepwise Reasoning\nYou are a medical classification expert. For each option, first recall the general category and structure\nbreakdown of the medical code, then explain why it might be wrong. Finally pick the correct one.\n[... Insert Question Text Here ...]\nSteps to follow:\n1. Recall the general category and structural break down of the code.\n2. Evaluate each option (A–D) briefly.\n3. Choose the best option and justify.\nAnswer format:\nStep 1: . . .\nStep 2A: . . .\nStep 2B: . . .\nStep 2C: . . .\nStep 2D: . . .\nFinal Answer: [A/B/C/D] because . . .\nC\nLayer-wise Representation Analysis\nC.1\nQuestion-Answer Pairwise Probing\nThis section provides supplementary results for the layer-wise representation divergence analysis presented in\nFigure 3, extending the comparison across additional MedConceptsQA vocabularies for two model families.\nC.1.1\nQwen2.5 Series\nFigure 8 presents the analysis for the Qwen2.5-32B base model compared against its instruction-tuned\n(-Instruct), distilled (DeepSeek-R1-Distill-), and reasoning-enhanced (QwQ-32B) variants across the ATC,\nICD10PROC, ICD9CM, and ICD10CM vocabularies.\nC.1.2\nMistral-Small-24B Series\nFigure 11 shows the corresponding analysis for the Mistral-Small model family, comparing the base (-Base-\n2503), instruction-tuned (-Instruct-2503), and reasoning-enhanced (Magistral-Small-2507) variants across all\nfive MedConceptsQA vocabularies (ATC, ICD9PROC, ICD9CM, ICD10CM, ICD10PROC).\n17\n"}, {"page": 18, "text": "C.2\nCoT Prompt Stepwise Probing\nTo analyze model representations under chain-of-thought (CoT) prompting, we construct a series of hierarchical\nprompts. For example, for the question “What is the description of the medical code 743.63 in ICD9CM?”,\nthe CoT series builds incrementally:\n• “hmm let me think. 001-999.99 refers to diseases and injuries”\n• “hmm let me think. 001-999.99 refers to diseases and injuries, and 740-759.99 refers to congenital\nanomalies”\n• . . .\n• “hmm let me think. . . . and 743.63 refers to other specified congenital anomalies of eyelid”\nFor each prompt in this series, we extract the activations from each layer of the model and group them by\ntheir corresponding vocabularies.\nAdditionally, we use L2 distance captures both directional and magnitude differences:\nd(a,b)\nL2\n(ℓ) = 1\nN\nN\nX\ni=1\n\r\rh(a)\nℓ(i) −h(b)\nℓ(i)\n\r\r\n2.\n(2)\nThe number of CoT steps varies across vocabularies. To standardize this, we predefine all CoT sequences to\nbe 5 steps long, with the exception of ICD10PROC, which uses 6 steps due to its more deeply embedded\ncode structure (e.g., 0Q894Z). After grouping the activations by vocabulary for each layer, we compute the\nlayerwise cosine similarity and L2 norm between the base and specialized models, following the methodology\nin Section 2.3.\n18\n"}, {"page": 19, "text": "19\n"}, {"page": 20, "text": "20\n"}, {"page": 21, "text": "Figure 6 Layer-wise Representation Divergence Across CoT Steps for All MedConceptsQA Vocabularies. This figure shows\nthe divergence analysis results for the ATC, ICD9PROC, ICD10PROC, ICD9CM, and ICD10CM vocabularies. The\ntop and bottom rows correspond to mean cosine similarity and L2 distance, respectively. Each column represents a\ndistinct step in the Chain-of-Thought (CoT) process, from Step 0 (the original question) to the final step (the original\nquestion plus the complete hierarchical traversal to the correct answer).\n21\n"}, {"page": 22, "text": "ATC\nICD10PROC\n22\n"}, {"page": 23, "text": "ICD9CM\nICD10CM\nFigure 8 Layer-wise Representation Divergence Across Remaining MedConceptsQA Vocabularies. Same visualization format\nas Figure 3, showing results for ATC, ICD10PROC, ICD9CM, and ICD10CM vocabularies. Top and bottom rows\ncorrespond to intra- and inter-model divergence, respectively.\n23\n"}, {"page": 24, "text": "ATC\nICD9PROC\n24\n"}, {"page": 25, "text": "ICD10PROC\nICD9CM\n25\n"}, {"page": 26, "text": "ICD10CM\nFigure 11 Layer-wise Representation Divergence Across Remaining MedConceptsQA Vocabularies. Same visualization format\nas Figure 3, showing results for ATC, ICD10PROC, ICD9CM, and ICD10CM vocabularies. Top and bottom rows\ncorrespond to intra- and inter-model divergence, respectively.\n26\n"}]}