{"doc_id": "arxiv:2512.17028", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.17028.pdf", "meta": {"doc_id": "arxiv:2512.17028", "source": "arxiv", "arxiv_id": "2512.17028", "title": "A Women's Health Benchmark for Large Language Models", "authors": ["Victoria-Elisabeth Gruber", "Razvan Marinescu", "Diego Fajardo", "Amin H. Nassar", "Christopher Arkfeld", "Alexandria Ludlow", "Shama Patel", "Mehrnoosh Samaei", "Valerie Klug", "Anna Huber", "Marcel Gühner", "Albert Botta i Orfila", "Irene Lagoja", "Kimya Tarr", "Haleigh Larson", "Mary Beth Howard"], "published": "2025-12-18T19:44:28Z", "updated": "2025-12-18T19:44:28Z", "summary": "As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with \"missed urgency\" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.17028v1", "url_pdf": "https://arxiv.org/pdf/2512.17028.pdf", "meta_path": "data/raw/arxiv/meta/2512.17028.json", "sha256": "b1fa7ea40d684b35ae8898faf56741eda2551dcde87a32c1b8bae4939e8b507c", "status": "ok", "fetched_at": "2026-02-18T02:24:06.206969+00:00"}, "pages": [{"page": 1, "text": "A Women’s Health Benchmark for Large Language\nModels\nVictoria-Elisabeth Gruber1\nRazvan Marinescu1\nDiego Fajardo1\nAmin H. Nassar2\nChristopher Arkfeld3\nAlexandria Ludlow4\nShama Patel5\nMehrnoosh Samaei6\nValerie Klug7\nAnna Huber7\nMarcel Gühner7\nAlbert Botta i Orfila7\nIrene Lagoja7\nKimya Tarr8\nHaleigh Larson9\nMary Beth Howard10\n1Lumos AI\n2Medical Oncology, Yale Cancer Center\n3Obstetrics and Gynecology, MGH, Harvard Medical School\n4Obstetrics, Gynecology & Reproductive Sciences, UCSF\n5Brown Division of Global Emergency Medicine\n6Department of Emergency Medicine, Emory University\n7Pharmacy Department, Clinic Ottakring\n8Windrush Surgery, Buckinghamshire, Oxfordshire and\nBerkshire West Integrated Care Board, NHS\n9Women’s Health Research, Yale School of Medicine\n10Johns Hopkins University School of Medicine\n{victoria,razvan,diego}@thelumos.ai\nAbstract\nAs large language models (LLMs) become primary sources of health information\nfor millions, their accuracy in women’s health remains critically unexamined. We\nintroduce the Women’s Health Benchmark (WHB), the first benchmark evaluating\nLLM performance specifically in women’s health. Our benchmark comprises 96\nrigorously validated model stumps covering five medical specialties (obstetrics\nand gynecology, emergency medicine, primary care, oncology, and neurology),\nthree query types (patient query, clinician query, and evidence/policy query), and\neight error types (dosage/medication errors, missing critical information, outdated\nguidelines/treatment recommendations, incorrect treatment advice, incorrect fac-\ntual information, missing/incorrect differential diagnosis, missed urgency, and\ninappropriate recommendations). We evaluated 13 state-of-the-art LLMs and re-\nvealed alarming gaps: current models show approximately 60% failure rates on\nthe women’s health benchmark, with performance varying dramatically across\nspecialties and error types. Notably, models universally struggle with \"missed\nurgency\" indicators, while newer models like GPT-5 show significant improve-\nments in avoiding inappropriate recommendations. Our findings underscore that\nAI chatbots are not yet fully able of providing reliable advice in women’s health.\n1\nIntroduction\nArtificial intelligence (AI) has made significant progress in various domains, including healthcare,\nwith the potential to significantly improve patient care and quality of life [1] [2]. Medical diagnosis is\none application area where AI has already shown promising results, as algorithms are able to identify\npatterns in medical images and text that are not readily visible to the human eye [3]. Another field\nwhere AI is making significant progress is personalized medicine, where algorithms are identifying\npatterns in vast volumes of patient data and providing personalized treatment plans taking into account\nthe patient’s unique genetics, lifestyle and medical history [4]. AI is also being utilized to increase the\neffectiveness of clinical operations, through integration of chatbots and virtual assistants, to help with\nPreprint.\narXiv:2512.17028v1  [cs.CL]  18 Dec 2025\n"}, {"page": 2, "text": "patient intake and triage [5]. Hence, it is not surprising that more and more patients and physicians\nare turning towards AI chatbots for health related questions and get help in clinical decision making\neveryday. Roughly one in six adults has used an AI chatbot for health related questions in the last\nyear [6]. This gives rise to the question: How accurate are AI models when it comes to women’s\nhealth-related questions?\nHistorically, women have been underrepresented in research and clinical trials, meaning that a majority\nof available data is biased toward male populations due to a myriad of reasons [7] [8]. Some of the\nmajor reasons include biological sex differences (physiological differences driven by chromosomal,\nhormonal, and anatomical factors), gender effects (differences arising from socially constructed roles,\nbehaviors, and identities) and inadequate research in women, such as excluding pregnant women and\nwomen of child-bearing age from participating in clinical studies for decades[9] [10] [11]. Bias in\nthe training data of large language models arises not only from the historical underrepresentation of\nwomen in research, but also from shortcut learning and spurious correlations present in imbalanced\nor noisy medical training datasets. As a result, models may rely on simplified patterns or stereotypes\ninstead of true medical reasoning, leading to confident but incorrect responses that can worsen sex-\nand gender-related gaps in healthcare.[12] [13] [14] A recent study examining online reproductive\nhealth misinformation across multiple social media platforms and websites found that 23% of the\ncontent included medical recommendations that do not align with professional guidelines [15].\nThey further found that potentially misleading claims and narratives about reproductive topics like\ncontraception, abortion, fertility, chronic disease, breast cancer, maternal health, and vaccines are\nabundant across several social media platforms and websites [15]. While some benchmarks exist\nfor general healthcare, to our knowledge, no specific benchmark currently exists that evaluates the\ncredibility and safety of AI models in the domain of women’s health.\nIn this paper, we investigate the accuracy of LLMs for women’s health-related questions. Our\nproposed benchmark consists of 5 specialties: obstetrics and gynecology, emergency medicine,\nprimary care, oncology, and neurology. We run three types of queries: patient queries, clinician\nqueries, and evidence/policy queries, as well as eight error types: (1) dosage/medication errors,\n(2) missing critical information, (3) outdated guidelines/treatment recommendations, (4) incorrect\ntreatment advice, (5) incorrect factual information, (6) missing/incorrect differential diagnosis, (7)\nmissed urgency, and (8) inappropriate recommendations. This is to ensure that the benchmark is\ncomprehensive and covers a wide range of women’s health topics. We benchmarked the following\nmodels: Claude 4.0 Opus, Claude 4.0 Sonnet, Gemini 2.5 Flash, Gemini 2.5 Pro, Gemini 3 Pro,\nGPT-4o Mini, GPT-5, GPT-5.1, o3, o3 Mini, Grok 4, Ministral-8B, and Mistral Large. We found\nthat the overall best performing model was GPT-5, followed by Gemini 3 Pro with best performance\nin obstetrics and gynecology. Importantly, no model demonstrated consistently high performance\nthroughout all specialties, error types and query types.\nAn overview of our contributions is as follows:\n• The Women’s Health Benchmark (WHB) which includes 96 realistic open-ended prompts\nacross five medical specialties (obstetrics and gynecology, emergency medicine, primary\ncare, oncology, and neurology).\n• It was produced with the help of 17 women’s health experts including clinicians, pharmacists,\nand researchers across the United States of America and Europe.\n• We measure the performance of the WHB across 13 LLMs.\n• Women’s health experts were asked to prompt from either a patient or doctor perspective or\nask evidence-based questions.\n• We release the WHB data and code via The Lumos AI Labs Hugging Face repository.\n2\nRelated Work\nA prime example of a field in which sex and gender differences have been extensively studied in\nrecent years is cardiovascular disease. Cardiovascular disease has been perceived as a ’man’s disease’,\nand this misconception has led to under-diagnosis and treatment for women worldwide [16]. In a\nrecent study Burgess et al. evaluated the representation of women in cardiovascular randomized\ncontrolled trials and reported that women are significantly under-represented in clinical trials both\nas participants and researchers [17]. Another study observed that women are less likely than men\n2\n"}, {"page": 3, "text": "to receive diagnostic tests, interventions, or appropriate preventive treatments, pointing to bias in\ndiagnostic and care protocols [18].\nMedical large language model benchmarks encompass a broad spectrum of tasks, including clinical\nknowledge representation, differential diagnosis generation, complex medical text summarization, and\nthe emulation of clinical reasoning and empathy in patient interactions [19] [20] [21] [22]. Another\nrecognized benchmark in the field, HealthBench, advances the field by assessing clinician–patient\ninteractions through question-specific evaluation rubrics [23]. One of the key challenges in AI in\nhealthcare is the need to ensure that the AI systems are able to learn from data in a way that is not\nbiased and that is able to generalize to new patients and new data.\nFigure 1: Overview of WHB methodology. Top: Dataset collection workflow showing how the expert\ncohort generated women’s health-related clinical prompts, which were randomly assigned to one of\nthe 13 LLMs to identify incorrect responses that became model stumps. Bottom: Model performance\nevaluation workflow showing how the 96 model stumps were used to benchmark all 13 LLMs.\n3\n"}, {"page": 4, "text": "3\nMethods\nIn Figure 1 we show an overview of our women’s health benchmark study. A group of experts\nprompted women’s health related questions which were randomly assigned to one of the 13 LLMs\nincluded in this study. The experts evaluated the model response and either accepted or rejected it.\nIf rejected, they needed to provide a justification plus reference to the source or truth. The rejected\nmodel prompt was then, after going through an approval process, added to the collection of model\nstumps. In total, 96 model stumps were generated and collectively referred to as the WHB dataset.\nThe WHB was then used to benchmark 13 LLMs.\n3.1\nData collection\nWomen’s health experts.\nThe model stumps (prompts) in WHB were created by a group of 17\nexperts over a period of six weeks. The group of experts included physicians (6), pharmacists (5),\nmedical researchers (5) and one nurse practitioner with practice and research experience across the\nUnited States of America, the United Kingdom, and the European Union. In the group, 67% were\nattending physicians or independent practitioners, and 33% were fellows. We vetted physicians for\ntheir participation in this benchmark dataset creation through a multi-step process. We screened\npublications on Pubmed and performed a thorough web-search to find women’s health experts.\nPhysicians had to have a minimum of two peer-reviewed publications in a women’s health related\nfield and/or practice experience of a minimum of 3 years with focus women’s health. Pharmacists had\nto have a minimum of 1 year of practical clinical experience after finishing their studies. Researchers\nwere enrolled in either a PhD program or postdoctoral fellowship in a women’s health related program.\nThe nurse practitioner had an experience of 9 years of working at the Department of Obstetrics,\nGynecology & Reproductive Sciences. The selected experts had to go through an educational webinar\nand the quality of their submissions was routinely reviewed by qualified personnel at Lumos AI.\nTable 1: Overview of evaluated lan-\nguage models.\nModel\nClaude 4.0 Opus (2025-05-14)\nClaude 4.0 Sonnet (2025-05-14)\nGemini 2.5 Flash (Preview 05-20)\nGemini 2.5 Pro (Preview 05-06)\nGemini 3 Pro\nGPT-4o Mini (2024-07-18)\nGPT-5 (2025-08-07)\nGPT-5.1\nGrok 4 (0709)\nMinistral-8B (Latest)\nMistral Large (Latest)\nOpenAI o3 (2025-04-16)\nOpenAI o3 Mini (2025-01-31)\nExpert instructions.\nAll participating experts received a\nstandardized instruction document outlining the process for\ndeveloping and evaluating benchmark questions. Experts\nwere instructed to generate realistic, women-specific clin-\nical prompts reflecting real-world contexts in which care,\npresentation, or treatment may differ for women. They\nwere encouraged to frame their questions from diverse\nperspectives, such as a patient seeking advice, a clinician\nconsidering management options, or an evidence-based\nreviewer referencing current literature. Experts did not\nknow which model would be used to answer their prompts.\nFollowing model response generation, experts were asked\nto assess each output for factual accuracy, safety, and com-\npleteness, explicitly identifying errors, outdated or unsafe\nrecommendations, and any critical omissions that could\npotentially lead to patient harm. For each identified issue,\nexperts were required to provide a concise written justifi-\ncation (1–3 sentences) describing the nature of the error\nand, whenever possible, include a supporting citation or\nlink to the authoritative source (e.g., peer-reviewed article,\nclinical guideline, or regulatory document) confirming the correct information.\n3.2\nModels\nTable 1 provides a comprehensive overview of the baseline models used in this paper. To generate\nour raw dataset of 96 model stumps, models were applied in a rotating fashion. In the next step, for\nbenchmarking, each model was evaluated on the 96 WHB prompts.\n3.3\nDataset\nDataset preparation.\nIn total, experts prompted 345 questions, a randomly selected model gener-\nated an answer. Experts then evaluated if the model provided a correct answer or not. 249 (72.2%)\n4\n"}, {"page": 5, "text": "questions lead to an answer classified as correct by the experts, and 96 (27.8%) were classified as\nincorrect, making up the raw dataset. Questions were manually generated and submitted by our expert\ngroup trained according Lumos AI’s standardized instructions. In total, experts contributed 143\nmodel stumps accompanied by written justifications and verified sources of truth, of which 96 were\napproved following internal quality review. To ensure data integrity and conceptual alignment with\nthe benchmark’s objectives, rigorous preprocessing was applied. This process included the removal\nof duplicate entries, prompts that did not meet the women-specific criteria, and instances where the\nexpert justification did not reflect a factually incorrect model response or where the identified error\nlacked sufficient clinical relevance or potential for harm.\nDataset organization.\nThe dataset is organized into five categories (obstetrics and gynecology,\nemergency medicine, primary care, oncology, and neurology), which reflect areas of real-world health\ninteractions each corresponding to a medical specialty. Each category contains a list of single-turn\nmodel stumps, each with a corresponding justification and reference.\nTypes of model stumps.\nIn the dataset, the model stumps are further organized into three query\ntypes: patient query, clinician query, and evidence/policy query. Patient queries are designed to\nbe from a patient’s perspective, asking questions about their health and treatment options. Doctor\nqueries are intended to be from a doctor’s perspective, such as prompting a realistic patient vignette.\nEvidence/policy queries are formulated to ask questions about the latest research and best practices\nin the field. The dataset contains 51 patient queries (53.1%), 33 clinician queries (34.4%), and 12\nevidence/policy queries (12.5%) (see Figure 2a).\nModel stumps by medical specialties.\nThe dataset consists of 41 obstetrics and gynecology\nquestions (42.7%), 24 emergency medicine questions (25%), 17 primary care questions (17.7%), 11\noncology questions (11.5%), and three neurology questions (3.1%) (see Figure 2b).\nModel stumps by error type.\nThe model stumps in our dataset can be categorized into dis-\ntinct error types based on the nature of observed model stumps. The most common category is\n“dosage/medication errors” (17 model stumps, 17.7%), defined as instances where the model provided\nan incorrect drug dosage, frequency, duration, or selected an inappropriate medication. This is\nfollowed by \"missing critical information\" (16 model stumps, 16.7%), which included responses\nthat omitted essential clinical details required to answer the question safely or accurately. Next is\n“outdated guidelines/treatment recommendations” (13 model stumps, 13.5%), consisting of answers\nthat reflected superseded clinical practices or failed to incorporate current standard-of-care guidance.\n“Incorrect treatment advice” (13 model stumps, 13.5%) was defined as recommendations that were\ninconsistent with evidence-based management despite not necessarily being outdated. Further, 12\nmodel stumps are categorized as “incorrect factual information” (12.5%), which included responses\nthat contained false or misleading information. The remaining categories are “missing/incorrect\ndifferential diagnosis” (12 model stumps, 12.5%), defined as either failing to list key differentials\nor proposing diagnostically implausible ones, “missed urgency” (9 model stumps, 9.4%), which\nincluded failures to recognize the need for urgent escalation, and “inappropriate recommendations”\n(4 model stumps, 4.7%), which included recommendations that were not appropriate for the patient’s\nsituation (see Figure 2c).\n5\n"}, {"page": 6, "text": "(a) Query types\n42.7%\n25.0%\n17.7%\n11.5%\n3.1%\nObstetrics\n& Gynecology\n(n=41)\nEmergency Medicine\n(n=24)\nPrimary Care\n(n=17)\nOncology\n(n=11)\nNeurology\n(n=3)\n(b) Medical specialties\n0\n5\n10 15 20\nNumber of Cases\nDosage/Medication Errors\nMissing Critical Information\nOutdated Guidelines/Treatment Recommendations\nIncorrect Treatment Advice\nIncorrect Factual Information\nMissing/Incorrect Differential Diagnosis\nMissed Urgency\nInappropriate Recommendations\n17 (17.7%)\n16 (16.7%)\n13 (13.5%)\n13 (13.5%)\n12 (12.5%)\n12 (12.5%)\n9 (9.4%)\n4 (4.2%)\n(c) Error types\nFigure 2: Distribution of model stumps in WHB. (A) Distribution by query type. (B) Distribution by\nmedical specialty. (C) Distribution by error type.\n3.4\nHuman evaluation\nTo evaluate the performance of the models on the WHB dataset, a human evaluator with a PhD in\nclinical sciences was used to assess the model answers to each stump. The evaluator screened the\nanswers for the respective error that was identified by the expert group and either approved or rejected\nthe answer if it included the identified error. The evaluator was only allowed to reject the answer if it\nincluded the exact same error as the one identified by the expert group, this does not mean that the\napproved answers are free of errors, but that the errors are not the same as the ones identified by the\nexpert cohort.\n3.5\nDefinitions\n• Model stump: A model stump is a prompt that leads to an incorrect model answer.\n• Case: A case is a model stump plus a model answer (see figure 6 in appendix).\n• Incorrect case: An incorrect case is a case that was rejected by the human evaluator on\ngrounds of the identified error by the expert group.\n• Correct case: A correct case is a case that was approved by the human evaluator on grounds\nof not identifying the exact same error as the one identified by the expert cohort.\n6\n"}, {"page": 7, "text": "4\nResults\n4.1\nModel performance\nFigure 3: Model performance on WHB. The approval rate was calculated as the percentage of correct\ncases (correct / total number of cases), shown in green columns. The failure rate was calculated as\nthe percentage of incorrect cases (incorrect/total number of cases), shown in red columns.\nFigure 3 shows for all analyzed LLMs the total number of correct and incorrect cases. As expected,\nlarger models generally perform better than the smaller models. The mean approval rate for large\nLLMs (Claude Opus 4, Claude Sonnet 4, Gemini 2.5 Pro, Gemini 3 Pro, GPT-5, GPT-5.1, o3, Grok\n4, Mistral Large) is 44% and for small models (Gemini 2.5 Flash, GPT-4o Mini, Ministral 8B and\no3 Mini) 34%. The best performing model is GPT-5 with an approval rate of 53.1%, followed by\nGemini 3 Pro with an approval rate of 47.9%, and o3 with an approval rate of 46.9%. The worst\nperforming model is Mistral 8B with an approval rate of 27.1%.\nModel performance by query type.\nWe analyzed the performance of the models on the three\ndifferent query types: patient query, clinician query, and evidence/policy query. Table 2 shows\nthe failure rates by model and query type. The failure rates were calculated as the percentage of\nincorrect cases (incorrect/total). Overall, GPT-5 shows the best performance on all query types, with a\nfailure rate of 47.1%, 51.5%, and 33.3% for patient query, clinician query, and evidence/policy query\nrespectively. Ministral-8B shows the worst performance on all query types, with a failure rate of\n78.4%, 57.6%, and 91.7% for patient query, clinician query, and evidence/policy query respectively.\nOverall, the models perform nearly equally on patient and clinician queries. However, models like\nGrok 4, Ministral 8B, and Gemini 3 Pro show dramatically better performance on clinician queries\ncompared to patient queries.\n7\n"}, {"page": 8, "text": "Table 2: Failure rates by model and query type. Values show percentage of incorrect responses\n(incorrect/total) with 95% confidence intervals. Bold shows the best values obtained in each metric.\nModel\nPatient Query\nClinician Query\nEvidence/Policy Query\nGPT-5\n47.1% (24/51) [34.1%, 60.5%]\n51.5% (17/33) [35.2%, 67.5%]\n33.3% (4/12) [13.8%, 60.9%]\nGemini 3 Pro Preview\n60.8% (31/51) [47.1%, 73.0%]\n42.4% (14/33) [27.2%, 59.2%]\n41.7% (5/12) [19.3%, 68.0%]\nGemini 2.5 Pro\n54.9% (28/51) [41.4%, 67.7%]\n45.5% (15/33) [29.8%, 62.0%]\n66.7% (8/12) [39.1%, 86.2%]\nOpenAI o3\n49.0% (25/51) [35.9%, 62.3%]\n60.6% (20/33) [43.7%, 75.3%]\n50.0% (6/12) [25.4%, 74.6%]\nGPT-5.1\n54.9% (28/51) [41.4%, 67.7%]\n51.5% (17/33) [35.2%, 67.5%]\n58.3% (7/12) [32.0%, 80.7%]\nGrok 4\n68.6% (35/51) [55.0%, 79.7%]\n42.4% (14/33) [27.2%, 59.2%]\n58.3% (7/12) [32.0%, 80.7%]\nGemini 2.5 Flash\n56.9% (29/51) [43.3%, 69.5%]\n63.6% (21/33) [46.6%, 77.8%]\n75.0% (9/12) [46.8%, 91.1%]\nOpenAI o3 Mini\n62.7% (32/51) [49.0%, 74.7%]\n66.7% (22/33) [49.6%, 80.2%]\n41.7% (5/12) [19.3%, 68.0%]\nClaude 4.0 Sonnet\n66.7% (34/51) [53.0%, 78.0%]\n60.6% (20/33) [43.7%, 75.3%]\n50.0% (6/12) [25.4%, 74.6%]\nMistral Large\n62.7% (32/51) [49.0%, 74.7%]\n57.6% (19/33) [40.8%, 72.8%]\n75.0% (9/12) [46.8%, 91.1%]\nClaude 4.0 Opus\n64.7% (33/51) [51.0%, 76.4%]\n69.7% (23/33) [52.7%, 82.6%]\n58.3% (7/12) [32.0%, 80.7%]\nGPT-4o Mini\n66.7% (34/51) [53.0%, 78.0%]\n72.7% (24/33) [55.8%, 84.9%]\n75.0% (9/12) [46.8%, 91.1%]\nMinistral-8B\n78.4% (40/51) [65.4%, 87.5%]\n57.6% (19/33) [40.8%, 72.8%]\n91.7% (11/12) [64.6%, 98.5%]\nNote: Confidence intervals calculated using Wilson score method (95% CI).\nModel performance by medical specialty.\nFigure 4a shows how well LLMs perform across\ndifferent medical specialties in women’s health, with stacked bars showing correct (green) vs incorrect\n(red) cases across all 13 models. The overall failure rates by specialty are shown in the bar chart\nwith 95% confidence intervals. Failure rates are calculated as the percentage of incorrect cases to\ntotal cases. With 76.9%, neurology shows the highest failure rate across all models. However, this\nis based on the smallest sample size (39 cases total across all models). Oncology is the second\nmost challenging with 67.8% failure rate. Emergency medicine shows the third highest failure rate\nwith 59.9% failure rate. Primary care and obstetrics and gynecology show the lowest failure rates\nwith 57.5% and 56.7% failure rate respectively. The heatmap in Figure 4b provides a more detailed\nview of model performance by specialty. Each cell in the heatmap represents the failure rate for a\nspecific model and specialty combination. The heatmap reveals several interesting patterns. GPT-5\nconsistently performs better than other models across all specialties, with the lowest failure rates in\nmost cases. This is followed by Gemini 3 Pro which also demonstrates relatively consistent lower\nfailure rates across most specialties. Neurology seems to be the most challenging specialty for all\nmodels, with the highest failure rates in most cases. However, this is based on the smallest sample\nsize (39 cases total across all models).\n8\n"}, {"page": 9, "text": "(a) Overall failure rates by medical specialty\n(b) Failure rates by model and medical specialty\nFigure 4: Model performance across medical specialties. (A) Overall failure rates (in percentage) by\nmedical specialty across all models with 95% confidence intervals. (B) Heatmap showing failure\nrates by model and medical specialty. Lower values (green) indicate better performance, while higher\nvalues (red) indicate worse performance.\nModel performance by error type.\nFigure 5a shows the overall failure rates by error type across\nall models with 95% confidence intervals, with stacked bars showing correct (green) vs incorrect\n(red) cases across all 13 models. Results revealed that the most common error type is \"incorrect\ntreatment advice\" with a failure rate of 76.3%. This is followed by \"outdated guidelines/treatment\nrecommendations\" with a failure rate of 69.2%. The least common error type is \"missing critical\ninformation\" with a failure rate of 49.5%. The heatmap in Figure 5b reveals that \"inappropriate\nrecommendations\" is the most variable error type, with GPT-5, GPT-5.1, o3 and Gemini 3 Pro\nshowing the lowest failure rates for this error type and GPT-4o-mini and Mistral-large-latest almost\n9\n"}, {"page": 10, "text": "always fail on this error type. GPT-5 shows consistently better performance across all error types,\nnotably low failure rate on both \"inappropriate recommendations\" and \"incorrect factual information\".\n(a) Overall failure rates by error type\n(b) Failure rates by model and error type\nFigure 5: Model performance across error types. (A) Overall failure rates (in percentage) by error\ntype across all models with 95% confidence intervals. (B) Heatmap showing failure rates by model\nand error type.\n5\nDiscussion\nOur WHB benchmark proposes an effective tool for evaluating LLMs in the field of women’s health.\nThe benchmark is composed of 96 model stumps, including justifications and references to the source\nof truth, covering five medical specialties, three query types, and eight error types. We revealed that\ncurrent LLMs are not yet able to reliably answer the questions in the women’s health benchmark,\nwith a failure rate of about 60% across all models. We also found that the performance of the models\nvaries by query type, medical specialty, and error type. Further, none of the tested models were\nuniversally reliable across all categories.\n10\n"}, {"page": 11, "text": "Women experience unique and complex medical needs related to pregnancy, postpartum recovery,\nmenstrual health, fertility, and menopause. At the same time, it is important to recognize that many\nmedical conditions affect both men and women differently, reflecting important sex- and gender-based\ndifferences beyond female-specific health issues. Together, these factors make women especially\nlikely to turn to large language models for quick, accessible health information [24, 25]. This reliance\nmakes it critical to understand how well LLMs perform on women’s health topics, motivating our\neffort to systematically evaluate their responses in this domain.\nWe observed that all models have similar performance across different query types (clinician, patient,\nand evidence/policy). However, we found that models like Grok 4, Mistral 8B, and Gemini 2.5 and\n3 Pro show better performance on clinician-queries compared to other models, thereby decreasing\nthe overall failure rate (see table 2). Further, we found that the performance of the models varies by\nmedical specialty. The models’ performance is generally better in obstetrics and gynecology and\nprimary care compared to oncology and emergency medicine. Given the small sample size of the\nneurology and oncology model stumps, we should be cautious with the conclusions drawn from\nthe performance of the models on these specialties (see Figure 4). We additionally found that the\nperformance of the models is highly variable across the error types. Interestingly, we observed that\n\"inappropriate recommendations\" is the most variable error type with GPT-5, Gemini-3-pro and o3\nshowing the lowest failure rates for this error type, while GPT-4o-mini and Mistral-large-latest almost\nalways failing on this error type, thereby suggesting that newer and larger models have significantly\nimproved in this dimension. However, we have to take into account that the sample size for this error\ntype is relatively small (52 cases), so this is not a definitive conclusion. Additionally, it seems that\n\"missed urgency\" is a universal weakness in all models, indicating the importance of human oversight\nfor time-sensitive cases (see Figure 5). Overall, we found that model size and novelty play a role in\nthe performance on WHB. Larger and newer models have a tendency to perform better than smaller\nmodels and older models.\nHowever, we have to take into account that our experts prompted a total of 345 questions to only one\nof the 13 LLMs in a randomized manner to identify model flaws in women’s health. While about\n28% were classified as model stumps and added to the WHB, it is important to recognize that a\ncorrect answer by the randomly selected LLM might have exposed weaknesses in other LLMs if\nqueried. Subsequently, our model-stump collection represents only the subset of cases in which a\ngiven model’s answer was rejected, roughly one third of all responses.\nSince women’s health is still underrepresented in scientific research, it is likely excluded or limited\nin training data for health AI models as well. This might lead to algorithms that potentially miss\nsex-specific symptoms, perform poorly for conditions that uniquely or disproportionately affect\nwomen. This highlights the urgent need for diverse, sex-aware datasets and evaluations. The WHB\nbenchmark is a step towards addressing this issue by providing a first evaluation framework for LLMs\nin the field of women’s health.\nLimitations.\nOur study has several limitations. First, the dataset included only 96 model stumps,\nresulting in a relatively small sample size overall and limiting statistical power, particularly within\nindividual medical specialties. This constraint was most pronounced in neurology, where very few\nexamples were available. Second, we evaluated only five medical specialties, three of which had fewer\nthan 20 cases each, restricting the breadth and representativeness of specialty-specific conclusions.\nAnother limitation is that confirmation of expert-identified model errors was conducted by a single\nhuman evaluator. Finally, the distribution of query types was unbalanced, with evidence- and policy-\nrelated questions notably under-represented. Together, these factors limit the generalizability of our\nfindings and highlight the need for larger, more diverse benchmarks in future work.\nOutlook.\nFuture work will extend our benchmark to include more medical specialties such as\nsurgery, cardiology, and dermatology and a higher number of model stumps per specialty. Further,\nwe will include more query types such as diagnostic reasoning, treatment planning, and patient\neducation. To minimize subjective interpretation of model outputs against predefined error categories\nand expert-provided justifications, we will use AI judges instead of a single evaluator. Another\nemphasis will be the development of a multi-turn benchmark to evaluate the performance of the\nmodels on longer conversations.\n11\n"}, {"page": 12, "text": "Conclusion.\nWith this benchmark, we provide the first evaluation framework for LLMs to assess\ntheir performance on women’s health and make a step towards addressing the under-representation\nof women’s health in scientific research and health AI models. We highlight that specialty-specific\nvalidation is essential. Current LLMs still show a relatively high failure rate on our benchmark of\nwomen’s health related questions, underscoring the need for further research and development in this\narea.\n6\nAcknowledgements\nWe would like to thank the IgG4-TREAT consortium led by Dr. Inga Koneczny and the participating\nexpert group of this doctoral program for their contributions to the dataset. Special thanks to:\nSofia-Natsouko Gkotzamani, Joan Faus Camarena, Inès Mountadir and Francisca Faber.\n7\nDataset availability\nThe dataset is available on Hugging Face at TheLumos/WHB_subset (https://huggingface.co/\ndatasets/TheLumos/WHB_subset).\nReferences\n[1] N. Patil, S. Patel, and S.D. Lawand. Research paper on artificial intelligence and it’s applications.\nJournal of Advanced Zoology, 44, 12 2023.\n[2] David B. Olawade, Aanuoluwapo C. David-Olawade, Ojima Z. Wada, Akinsola J. Asaolu,\nTemitope Adereni, and Jonathan Ling. Artificial intelligence in healthcare delivery: Prospects\nand pitfalls. Journal of Medicine, Surgery, and Public Health, 3:100108, 2024.\n[3] Luís Pinto-Coelho. How artificial intelligence is shaping medical imaging technology: a survey\nof innovations and applications. Bioengineering, 10(12):1435, 2023.\n[4] Kevin B Johnson, Wei-Qi Wei, Dilhan Weeraratne, Mark E Frisse, Karl Misulis, Kyu Rhee,\nJuan Zhao, and Jane L Snowdon. Precision medicine, ai, and the future of personalized health\ncare. Clinical and translational science, 14(1):86–93, 2021.\n[5] Shuroug A Alowais, Sahar S Alghamdi, Nada Alsuhebany, Tariq Alqahtani, Abdulrahman I\nAlshaya, Sumaya N Almohareb, Atheer Aldairem, Mohammed Alrashed, Khalid Bin Saleh,\nHisham A Badreldin, et al. Revolutionizing healthcare: the role of artificial intelligence in\nclinical practice. BMC medical education, 23(1):689, 2023.\n[6] Marley Presiado, Alex Montero, Lunna Lopes, and Liz Hamel. Kff health misinformation track-\ning poll: Artificial intelligence and health information, August 2024. Accessed on 11/11/2025.\n[7] Hannah C Karpel, Linda M Zambrano Guevara, BJ Rimel, Kari E Hacker, Victoria Bae-Jump,\nTara Castellano, John Curtin, and Bhavana Pothuri. The missing data: A review of gender and\nsex disparities in research. Cancer, 131(6):e35769, 2025.\n[8] E Rittenberg, CP Gross, M Wong, and SK Inouye. Women’s health and artificial intelligence.\nJAMA Intern Med, 185(10):1301–1302, 2025.\n[9] Deborah Bartz, Tanuja Chitnis, Ursula B Kaiser, Janet W Rich-Edwards, Kathryn M Rexrode,\nPage B Pennell, Jill M Goldstein, Mary Angela O’Neal, Meryl LeBoff, Maya Behn, et al.\nClinical advances in sex-and gender-informed medicine to improve the health of all: a review.\nJAMA internal medicine, 180(4):574–583, 2020.\n[10] National Academies of Sciences, Engineering, and Medicine. Advancing Clinical Research\nwith Pregnant and Lactating Populations: Overcoming Real and Perceived Liability Risks. The\nNational Academies Press, Washington, DC, 2024.\n[11] Theresa M. Wizemann and Mary-Lou Pardue, editors. Exploring the Biological Contributions\nto Human Health: Does Sex Matter? The National Academies Press, Washington, DC, 2001.\n12\n"}, {"page": 13, "text": "[12] Varun Dogra, Sahil Verma, Marcin Wo´zniak, Jana Shafi, Muhammad Fazal Ijaz, et al. Shortcut\nlearning explanations for deep natural language processing: A survey on dataset biases. IEEE\nAccess, 12:26183–26195, 2024.\n[13] Wenqian Ye, Luyang Jiang, Eric Xie, Guangtao Zheng, Yunsheng Ma, Xu Cao, Dongliang Guo,\nDaiqing Qi, Zeyu He, Yijun Tian, Megan Coffee, Zhe Zeng, Sheng Li, Ting-hao (Kenneth)\nHuang, Ziran Wang, James M. Rehg, Henry Kautz, and Aidong Zhang. The clever hans\nmirage: A comprehensive survey on spurious correlations in machine learning. arXiv preprint\narXiv:2402.12715, 2024.\n[14] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665–673, 2020.\n[15] Jennifer N John, Sara Gorman, David Scales, and Jack Gorman. Online misleading information\nabout women’s reproductive health: a narrative review. Journal of General Internal Medicine,\n40(5):1123–1131, 2025.\n[16] Karin Schenck-Gustafsson. Risk factors for cardiovascular disease in women. Maturitas,\n63(3):186–190, 2009.\n[17] Sonya Burgess, Sarah Zaman, Cindy Towns, Megan Coylewright, and F Aaysha Cader. The\nunder-representation of women in cardiovascular clinical trials: State-of-the-art review and\nethical considerations. American Heart Journal, 282:81–92, 2025.\n[18] Abdullah Al Hamid, Rachel Beckett, Megan Wilson, Zahra Jalal, Ejaz Cheema, Dhiya Al-\nJumeily Obe, Thomas Coombs, Komang Ralebitso-Senior, Sulaf Assi, and Sulaf Assi Sr. Gender\nbias in diagnosis, prevention, and treatment of cardiovascular diseases: a systematic review.\nCureus, 16(2), 2024.\n[19] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark,\nStephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed\nAmin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska,\nBlaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara\nMahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan\nKarthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with\nlarge language models, 2023.\n[20] Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan\nSinghal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu,\nS Sara Mahdavi, Sushant Prakash, Anupam Pathak, Christopher Semturs, Shwetak Patel, Dale R\nWebster, Ewa Dominowska, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado,\nYossi Matias, Jake Sunshine, Alan Karthikesalingam, and Vivek Natarajan. Towards accurate\ndifferential diagnosis with large language models, 2023.\n[21] Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Chris-\ntian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerová,\nNidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason\nHom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can\noutperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134–1142,\nFebruary 2024.\n[22] Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, and Jonathan H Chen. Di-\nagnostic reasoning prompts reveal the potential for large language model interpretability in\nmedicine, 2023.\n[23] Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-\nCandela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel,\nJohannes Heidecke, and Karan Singhal. Healthbench: Evaluating large language models towards\nimproved human health, 2025.\n[24] Hyun-Kyoung Kim. The effects of artificial intelligence chatbots on women’s health: A\nsystematic review and meta-analysis. Healthcare, 12(5):534, 2024.\n13\n"}, {"page": 14, "text": "[25] Padaphet Sayakhot and Mary Carolan-Olah. Internet use by pregnant women seeking pregnancy-\nrelated information: a systematic review. BMC pregnancy and childbirth, 16(1):65, 2016.\n14\n"}, {"page": 15, "text": "A\nAPPENDIX\nFigure 6: Example case. Expert prompts a question from clinician perspective and reviews the\nmodel’s answer. The expert rejected this answer and provided a justification of the observed error.\nThis model stump with justification was added to the women’s health benchmark.\n15\n"}]}