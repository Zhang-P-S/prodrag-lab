{"doc_id": "arxiv:2601.01341", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.01341.pdf", "meta": {"doc_id": "arxiv:2601.01341", "source": "arxiv", "arxiv_id": "2601.01341", "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems", "authors": ["Md Abdullah Al Kafi", "Raka Moni", "Sumit Kumar Banshal"], "published": "2026-01-04T03:09:23Z", "updated": "2026-01-04T03:09:23Z", "summary": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.01341v1", "url_pdf": "https://arxiv.org/pdf/2601.01341.pdf", "meta_path": "data/raw/arxiv/meta/2601.01341.json", "sha256": "b803a8c8464bb9ddc1c0030d2a484e7ca5b7ae1997a357a57f9553f2c8aadeb6", "status": "ok", "fetched_at": "2026-02-18T02:23:22.017347+00:00"}, "pages": [{"page": 1, "text": "Reasoning Over Recall: Evaluating the Efficacy of\nGeneralist Architectures vs. Specialized Fine-Tunes\nin RAG-Based Mental Health Dialogue Systems\nMd Abdullah Al Kafi1, Raka Moni1†, Sumit Kumar Banshal2*†\n1*Department of Computer Science and Engineering, Daffodil\nInternational Univeristy, Birulia, Dhaka, 1216, Dhaka, Bangladesh.\n2Department of Computer Science and Engineering, Alliance\nUniversity, Chikkahagade Cross, Chandapura–Anekal Main Road,\nBengaluru, 562106, Karnataka, India.\n*Corresponding author(s). E-mail(s): sumitbanshal06@gmail.com;\nContributing authors: kafi.cse@diu.edu.bd; rakamoni509@gmail.com;\n†These authors contributed equally to this work.\nAbstract\nThe deployment of Large Language Models (LLMs) in mental health counseling\nfaces the dual challenges of hallucinations and lack of empathy. While the for-\nmer may be mitigated by RAG (retrieval-augmented generation) by anchoring\nanswers in trusted clinical sources, there remains an open question as to whether\nthe most effective model under this paradigm would be one that is fine-tuned on\nmental health data, or a more general and powerful model that succeeds purely on\nthe basis of reasoning. In this paper, we perform a direct comparison by running\nfour open-source models through the same RAG pipeline using ChromaDB: two\ngeneralist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-\ntunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge\nframework to automate evaluation over 50 turns. We find a clear trend: the gen-\neralist models outperform the domain-specific ones in empathy (3.72 vs. 3.26,\np < 0.001) in spite of being much smaller (3B vs. 7B), and all models perform\nwell in terms of safety, but the generalist models show better contextual under-\nstanding and are less prone to overfitting as we observe in the domain-specific\nmodels. Overall, our results indicate that for RAG-based therapy systems, strong\nreasoning is more important than training on mental health-specific vocabulary;\ni.e. a well-reasoned general model would provide more empathetic and balanced\n1\narXiv:2601.01341v1  [cs.CL]  4 Jan 2026\n"}, {"page": 2, "text": "support than a larger narrowly fine-tuned model, so long as the answer is already\ngrounded in clinical evidence.\nKeywords: keyword1, Keyword2, Keyword3, Keyword4\n1 Introduction\nBackground: There is an enormous gap between the demand for mental health\nservices and the supply of mental health professionals, particularly in low- and middle-\nincome countries, and in response, many have turned to large language models as\na scalable, low-cost way to offer immediate support. Early systems were rule-based\nand deterministic, whereas modern generative models can respond more flexibly and\nwith greater emotional nuance. Nevertheless, two primary challenges remain: halluci-\nnations and inconsistent empathy, and while Retrieval-Augmented Generation (RAG)\nhas recently emerged as a popular solution to the problem of hallucinations, by forcing\nanswers to be grounded in trusted sources, the question of how to provide empathetic\ncommunication that is both consistent and genuinely empathetic remains unanswered.\nProblem Statement: However, RAG does not help with the choice of LLM to use\nfor the reasoning, and the common hypothesis is that fine-tuning an LLM on domain-\nspecific data improves the therapeutic quality of its output. However, the problem\nis that fine-tuning can also reduce the general reasoning capacity of the LLM, the\nvery skills that are required to understand the emotional nuances of the conversation\nand generate helpful responses, which implies that it is far from clear that fine-tuned\nmodels are superior for mental health conversations in a RAG setting.\nResearch Gap Most evaluations of mental health LLMs examine whether the\nmodel’s factual content is correct, but few evaluate whether the conversation is empa-\nthetic or safe when the model is used within a RAG pipeline. It is thus unclear to what\nextent higher therapy quality is due to domain-specific fine-tuning versus a strong\ngeneral reasoner guided by retrieved context.\nThis study explores the necessity of fine-tuning by comparing the performance of\nfour open-source large language models on several different setups, and two general-\npurpose models and two models that have been fine-tuned on mental health data are\nused to provide empirical evidence on when fine-tuning provides a benefit, and when\na strong general model is enough.\n• We assess whether, in retrieval-augmented generation settings, architectural design\nexerts greater influence on performance than model size.\n• We investigate the extent to which mental-health-specific finetuning mitigates\nhallucination when models encounter ambiguous or safety-critical prompts.\n• We incorporate human-in-the-loop evaluation to determine which model types\ndemonstrate more consistent and contextually appropriate empathic behavior.\nTo delve deeper into these findings, we performed a controlled, head-to-head com-\nparison of the four models within a single RAG workflow, using a fully automated\n2\n"}, {"page": 3, "text": "LLM-as-a-Judge setup, which scored more than 200 dialogue turns on two axes: empa-\nthy (five-point scale) and safety (pass/fail). Human raters confirmed these results,\nadjudicated any ambiguous cases, and provided an external benchmark for what it\nmeans to be appropriately empathic, which in turn supported our general analysis of\nmodel architecture, robustness to hallucination, and degree to which models maintain\nempathy under human supervision.. General-purpose models were consistently rated\nas more empathic than their fine-tuned mental health variants, with similar safety rat-\nings. Taken together, these data suggest that, in RAG-based mental health support\nsystems, core reasoning abilities and the quality of the retrieval pipeline may be more\nimportant to therapeutic communication than domain-specific fine-tuning. However,\nhuman-in-the-loop review remains necessary to detect edge cases and provide safe,\nappropriate responses.\n2 Literature Review\nThe intersection of Artificial Intelligence and mental healthcare has evolved from\nsimple rule-based heuristics to complex, generative reasoning systems. This section\nreviews the trajectory of therapeutic chatbots, the adoption of Retrieval-Augmented\nGeneration (RAG) to mitigate safety risks, and the emerging debate regarding model\nspecialization versus general reasoning.\nThe Evolution of Therapeutic Conversational Agents: Automated therapy\nbegan in the mid-20th century with the development of ELIZA, a simple, rule-based\nprogram that simulated a Rogerian therapist using pattern matching [1]. ELIZA\nshowed that people could form an emotional bond with a machine, now referred to\nas the ”ELIZA effect”, though it had no actual comprehension of meaning. Shortly\nafter, PARRY was developed, an early attempt to simulate a patient with paranoid\nschizophrenia, demonstrating that computers could model mental states [2]. In the\n2010s, tools like Woebot used decision trees and CBT-style scripts to offer structured\nmental health support, and studies of similar rule-based apps like Wysa showed that\nthey may help treat sub-clinical anxiety. [3]. However, these models are characterized\nby rigid and pre-defined dialogue flows [4], but the Transformer architecture changed\nthis [5], and large language models, such as GPT-4 and LLaMA 2, are capable of gener-\nating fluent open-ended responses [6, 7]. However, this flexibility introduced new risks,\nsuch as hallucinations and inconsistent personas, which remain central challenges for\nreal-world deployment [8].\nRetrieval-Augmented Generation (RAG) in Healthcare: To address the\nhallucination problem, researchers introduced the concept of Retrieval-Augmented\nGeneration (RAG), which, at its core, is simple but potent: the pairing of a generative\nmodel with a searchable external knowledge base that the model can use to retrieve\nand reference relevant documents to base its answers upon [9]. In effect, by tying\nanswers to fact-checked sources, RAG significantly reduces the spread of fabricated\nfacts and enhances the veracity of answers.\nIn the medical domain, RAG has proven superior to standalone LLMs for diagnostic\naccuracy [10]. Furthermore, retrieving clinical guidelines has been shown to reduce\nthe rate of dangerous advice in medical chatbots [11]. Despite these advances, most\n3\n"}, {"page": 4, "text": "research focuses on factual correctness. There is a comparative scarcity of research on\nhow RAG influences soft skills like empathy, a gap this study aims to fill.\nThe Dilemma: Fine-Tuning vs. In-Context Learning: There is an ongo-\ning, open debate in the NLP community about whether a model should be fine-tuned\nfor a specific domain and when a strong general model should be relied on for rea-\nsoning power. The argument for fine-tuning is that a model trained on a specialized\ncorpus can learn domain-specific vocabulary, tone, and norms, as MentaLLaMA was\nfine-tuned on the IMHI dataset to generate interpretable mental health analyses [12].\nMentaLLaMA was fine-tuned on the IMHI dataset to make the mental health rea-\nsoning more interpretable, and PsyQA was constructed as a dataset to get models\nto generate more supportive, counseling-style responses [13]. Moreover, SMILE shows\nthat fine tuning on multi-turn dialogues can get models to generate more inclusive\nand sensitive language [14].\nThe Case for General Reasoning: More recent work highlights the major defi-\nciency of narrow fine-tuning: that of catastrophic forgetting, wherein a model begins\nto lose its general reasoning ability, and indeed, large generalist models prompted\nwith chain-of-thought tend to outperform smaller specialized models on hard tasks\n[15]. Moreover, models trained with Reinforcement Learning from Human Feedback\n(RLHF) on broad general instructions tend to be more robust to safety issues than\nmodels optimized on niche datasets [16]. In this work, we investigate whether small,\nmodern models (such as Phi-3) can achieve the same level of reasoning efficiency as\nheavily fine-tuned systems [17].\nAutomated Evaluation: The Rise of ”LLM-as-a-Judge”: Evaluating men-\ntal health dialogue is extremely challenging because standard metrics like BLEU and\nROUGE rely on word overlap, which doesn’t properly capture the relevant properties\nof mental health dialogue (e.g. validation, tone, subtle empathy), and as such have\nbeen found to correlate poorly with human judgements. [18]. The LLM-as-a-Judge\nparadigm provides a much better evaluation framework, and strong instruction-tuned\nmodels are extremely highly correlated with expert judges in assessing the quality\nof chatbots [19]. Moreover, the Chatbot Arena extends this work one step further,\nby showing that pairwise comparisons made by LLMs are not only scalable, but also\nreliable as a means of evaluating chatbots [20]. Consequently, building on this work,\nour evaluation is conducted by a blinded AI judge, providing a reproducible and\ntransparent benchmark for therapeutic AI.\nSynthesis and Research Contribution: While both RAG and fine-tuning have\nachieved success, the trade-offs between the two in low-resource mental health settings\nremain poorly understood because previous work has either considered large propri-\netary models, such as GPT-4, or clinical factual accuracy, and not the qualities that\nare important for therapy, like rapport and empathy. What remains unclear is whether\nthe general reasoning strength of today’s small language models is sufficient to make\nup for the advantages of domain-specific fine-tuning when both have access to the\nsame external knowledge. We answer this question with a controlled benchmark that\ncompares two design philosophies: general-purpose reasoners, such as Qwen2.5 and\nPhi-3, versus domain-specific fine-tuned models, like MentalHealthBot and Therapy-\nBot. By providing all models the same RAG setup for knowledge access, we control for\n4\n"}, {"page": 5, "text": "information and reason about the only variable, which is reasoning, thereby allowing\nus to provide clear, empirical evidence of which approach best facilitates accessible,\nempathetic, and safe AI counseling.\n3 Methodology\nTo assess our hypothesis that strong general reasoning can be more effective than\ndomain-specific fine-tuning in a RAG-based therapy setting, we conducted a con-\ntrolled study across four LLMs, and our end-to-end pipeline consists of three stages:\n(1) building the knowledge base, (2) RAG-augmented inference and (3) automated\nevaluation.\n3.1 Dataset and Preprocessing\nThe Mental Health Conversational AI Dataset [21], a large, fine-grained dataset for\ntraining therapeutic dialogue systems, was utilized in this work, and it consists of\nmore than 510,000 conversational turns from real counseling sessions, anonymous\ncommunity conversations, and professional certified synthetic dialogues.\nData Composition: The dataset covers a broad spectrum of psychological\nconditions and wellness topics, including:\n• Clinical Conditions: Anxiety, Depression, Panic Disorders, and PTSD.\n• Wellness & Lifestyle: Stress management, sleep hygiene, and self-esteem building.\n• Crisis Intervention: Suicide prevention protocols and emergency resource guidance.\nRAG vs. Evaluation Split: To simulate a realistic deployment scenario where\nthe model must rely on external knowledge rather than memorized parameters, we\nadhered to a strict no-training protocol:\n1. Knowledge Base (Retrieval Source): Approximately 99.9% of the dataset\nwas vectorized using all-MiniLM-L6-v2 and indexed in ChromaDB. This served\nexclusively as the external memory for the RAG system; no gradient updates or\nfine-tuning were applied to the candidate models using this data.\n2. Evaluation Set (Inference): A reserved subset of 50 distinct, high-complexity\nprompts was withheld from the vector store to serve as the Test Set. These prompts\nwere selected to challenge the models’ ability to synthesize retrieved context into\nempathetic responses for unseen scenarios.\n3.2 Retrieval-Augmented Generation (RAG) Architecture\nWe implemented a lightweight but robust RAG pipeline using ChromaDB as the vector\nstore. The architectural workflow is illustrated in Figure 1\nAs shown in the Figure 1, the pipeline starts with feeding the raw conversational\ndata into a vectorizer and storing them into ChromaDB. When a new user message\ncomes in, it is fed into both the retriever and the responder, and the retriever outputs\nthe top 3 most semantically similar therapy transcripts. We concatenate the user’s\nquery, a brief system instruction, and the three retrieved transcripts into an augmented\nprompt, which is then fed into one of the four candidate models to generate a response.\n5\n"}, {"page": 6, "text": "Training Data \nJSon\nEmbedding\nModel\nChromaDB\nVector Store\nAI Judge\nSystem\nPrompt\nQwen 2.5\nPhi-3\nMentalHealthBot\nTherapyBot\nUser query\nFinal Report (Empathy\nand safety Scores)\nRetrieval\nContext\nFig. 1 The experimental pipeline.\nTo ensure fairness and remove bias from the evaluation, a separate model, the AI\nJudge, which is blinded to the model that produced the response, scores the output\nfor empathy on a 1-5 scale and for safety as pass/fail.\nImplementation Details:\n• Embedding Model: To convert text into vector representations, we employed (all-\nMiniLM-L6-v2) via the Sentence Transformers library.\n• Retrieval Mechanism: We retrieved the top k = 2 documents based on cosine\nsimilarity.\n• Generation Parameters: To balance creativity with stability, all models used\ntemperature=0.7, topp = 0.9, and maxnewtokens = 150.\n• Prompt Engineering: We utilized the strict template shown in 3.2 to force the models\nto prioritize the retrieved context.\n6\n"}, {"page": 7, "text": "Listing 1: The System Prompt Template\nYou are an empathetic mental health counselor.\nUse the following context to answer the user.\nContext: {retrieved documents}\nUser: {user query}\nAnswer:\n3.3 Candidate Models\nWe selected four open-source models representing two distinct architectural philoso-\nphies: General-Purpose Reasoners and Domain-Specific Fine-Tunes. To ensure acces-\nsibility for consumer hardware, all models were loaded in 4-bit quantization (NF4\nformat) using the BitsAndBytes library.\nThe specific attributes of the tested models are summarized in Table 1. We delib-\nerately compared smaller, modern generalist models (3B parameters) against larger,\nolder specialized models (7B parameters) to test whether reasoning efficiency can\ncompensate for a lack of domain-specific training data.\nTable 1 Candidate Model Specifications.\nModel\nCategory\nParameters\nArchitecture\nQwen2.5-3B\nGeneral Purpose\n3.0B\nQwen2.5 (Dense)\nPhi-3-Mini\nGeneral Purpose\n3.8B\nPhi-3 (Dense)\nMentalHealthBot\nDomain Specific\n7.0B\nLLaMA-2 Fine-tune\nTherapyBot\nDomain Specific\n7.0B\nLLaMA-2 Fine-tune\n3.4 Evaluation Framework (LLM-as-a-Judge)\nWe employed an ”LLM-as-a-Judge” methodology using Qwen2.5-3B-Instruct as the\nevaluator. The judge was blinded to the model names and used deterministic\ngeneration (dosample = False) to score each response based on:\n1. Empathy Score (1-5): From 1 (Dismissive) to 5 (Highly Empathetic).\n2. Safety Check (Pass/Fail): Responses failing to address self-harm or providing\ndangerous medical advice received a ”FAIL.”\n3.5 Statistical Analysis\nTo assess whether the differences we found were statistically significant, we con-\nsidered empathy scores as non-parametric. We compared distributions using the\n7\n"}, {"page": 8, "text": "Mann–Whitney U test with a significance threshold of α = 0.05, and we also calcu-\nlated 95% confidence intervals for the mean empathy scores to quantify uncertainty\nin each model’s average performance.\n4 Result and Discussion\nThe performance of the four candidate models on the 50 benchmark interactions is\nsummarized in Table 2.\nTable 2 Comparative Performance of LLMs on RAG-Augmented Mental Health Evaluation.\nModel\nCategory\nAvg Empathy (1-5)\nSafety Pass Rate (%)\nQwen2.5-3B\nGeneral Purpose\n3.72 (Best)\n100.0%\nPhi-3-Mini\nGeneral Purpose\n3.50\n96.0%\nMentalHealthBot\nDomain Specific\n3.26\n100.0%\nTherapyBot\nDomain Specific\n3.24\n100.0%\nThe results strongly support our primary hypothesis (H1). The general-purpose\nQwen2.5-3B achieved the highest average empathy score (3.72/5.0), significantly\noutperforming the domain-specific MentalHealthBot-7B (3.26, p\n<\n0.001) and\nTherapyBot-7B (3.24, p < 0.001).This architectural divide is visualized in Figure\n2. Figure 2 illustrates this architectural dichotomy by plotting the average empathy\nscores for each model type, where blue bars correspond to General Purpose models\nwhile red bars correspond to Domain-Specific fine-tunes. We observe that Qwen2.5\n(far left) outperforms all domain specific models, even though they have almost twice\nthe number of parameters (7B vs 3B), which suggests that in a RAG setup, the ability\nto reason over retrieved context is more important than static domain memorization.\nGeneral-purpose models (blue) consistently outperform domain-specific fine-tunes\n(red) in retrieval-augmented scenarios.\n4.1 Safety and Hallucination Analysis\nWhile empathy is important, clinical safety is non-negotiable.\n• Qwen2.5-3B and the domain-specific bots achieved a 100\n• Phi-3-Mini may have received high marks for empathy. However, it still flunked\naround 4% of the time because the AI struggled with a couple of issues: it sometimes\nhallucinated (making up quotes). It offered dismissive advice when the context was\nambiguous.\n4.2 Qualitative Error Analysis\nTo understand why general models outperformed specialists, we analyzed specific\nresponse patterns in Table 3).\n8\n"}, {"page": 9, "text": "MentalHealthBot-7B\nPhi-3-Mini\nQwen2.5-3B\nTherapyBot-7B\n0\n1\n2\n3\n4\n5\nAverage Empathy Score (1-5)\n3.26\n3.24\n3.50\n3.72\nCategory\nDomain Specific\nGeneral Purpose\nFig. 2 Average Empathy Scores by Model Category.\nTable 3 Qualitative Example of Generalist Model Failure Mode (Hallucination/Safety Risk).\nUser Input\nQwen2.5-3B\nResponse\n(General)\nPhi-3-Mini\nResponse\n(General)\n”Recently I feel really worried\nand I can’t help but notice con-\ntinuous criticism...”\n(Empathetic) ”I’m so sorry...\nThe\ncontinuous\ncriticism\nis\nunderstandably\ncausing\nstress...\nCould\nyou\ntell\nme\nmore\nabout\nspecific\nsitua-\ntions?”\n(Hallucinated/Unsafe)\n”Recent Criticism: ’You’re not\nhandling things well...’ Thera-\npist Response: Nothing. It is\nnecessary to have the power to\nrealize the need to change...”\nBut these domain-specific models tended to overfit and fall back on the generic\npre-trained answer, such as ”Have you tried breathing exercises”, without using the\nretrieved RAG context, because Qwen2.5, in contrast, consistently used the retrieved\ncontext, along with the user’s actual question, which is why it scored higher on\nempathy.\n4.3 Robustness Verification\nTo ensure the validity of our rankings, we perform a series of robustness checks.\nUncertainty Analysis:The first step involved measuring the uncertainty around the\nempathy scores, and as shown in Table 4, Qwen2.5-3B was very stable (σ = 0.45)\nwhile Phi-3-Mini was much more variable (σ = 0.71), indicating more unpredictable\nperformance.\nDistribution and Bias: The instability can be seen directly in the boxplot in\nFigure 3, which plots the distribution of scores for each model, and the white dots\n9\n"}, {"page": 10, "text": "Table 4 Statistical Uncertainty Analysis (95% Confidence Intervals).\nModel\nMean Empathy\nStd. Dev (σ)\n95% Confidence Interval\nQwen2.5-3B\n3.72\n0.45\n[3.59, 3.85]\nPhi-3-Mini\n3.50\n0.71\n[3.30, 3.70]\nMentalHealthBot\n3.26\n0.44\n[3.14, 3.38]\nTherapyBot\n3.24\n0.48\n[3.11, 3.37]\nQwen2.5-3B\nPhi-3-Mini\nMentalHealthBot-7B\nTherapyBot-7B\n0\n1\n2\n3\n4\n5\nEmpathy Score (1-5)\nFig. 3 Distribution of Empathy Scores.\nare the mean, and the boxes are the interquartile range. The distribution for Phi-3 is\ndisplaced far below the other models, with a long whisker extending down to scores\nof 1–2, which captures the long tail of weak responses and safety failures that we\ndescribe in Section 3.2. At the same time, Qwen2.5, by contrast, has a much tighter box\nthat is displaced upward along the y-axis, indicating more consistent, higher-quality\nperformance.\nThe compact box for Qwen2.5 indicates high consistency, while the extended lower\nwhisker for Phi-3 reveals significant variance and occasional poor performance.\nTo ensure the AI Judge was not simply rewarding longer answers, we examined\nthe relationship between response length and empathy score (Figure 4). The scatter\nplot plots each response as a point, with word count on the x-axis and empathy on\nthe y-axis. For the top-performing model, the correlation is essentially non-existent\n(r =- 0.20), and the highest-scoring answers span the full range of response lengths,\nindicating that the metric is not biased towards verbosity.\nThe lack of a clear linear trend (the regression line is nearly flat) indicates that the\nAI Judge evaluated responses based on semantic quality rather than output length.\n10\n"}, {"page": 11, "text": "40\n60\n80\n100\n120\n140\nResponse Length (Word Count)\n0\n1\n2\n3\n4\n5\nEmpathy Score (1-5)\nModel\nQwen2.5-3B\nPhi-3-Mini\nMentalHealthBot-7B\nTherapyBot-7B\nFig. 4 Response Length vs. Empathy Score.\n4.4 Human Validation\nIn addition to an LLM-based judge, we conducted a human evaluation with three\ntertiary-level student annotators, each annotating 50 responses per model across the\nfour models, for a total of 200 annotations. Each response was rated on a 5-point Likert\nscale for perceived empathy, using the same evaluation protocol as in our previous\nexperiments.\nThe consistency of the human raters in their judgments of the responses was mea-\nsured using the inter-annotator agreement, and the result shows that Fleiss’ kappa\n(κ = 0.78), which indicates substantial agreement, therefore, the annotators are mostly\nconsistent in their judgments of empathetic content.\nThe Likert ratings are ordinal data, and the same participants rated each of the four\nmodels, we therefore used non-parametric statistical tests. A Friedman test showed a\nsignificant effect of model choice on perceived empathy, (χ2(3) = 18.42, p < 0.001),\nindicating that differences among the four models were substantial.\nPost hoc pairwise comparisons were conducted using Wilcoxon signed-rank tests\nwith a Bonferroni correction. Both general-purpose models significantly outperformed\nthe domain-specific models in perceived empathy. Specifically, Qwen2.5-3B achieved\nhigher empathy ratings than MentalHealthBot (p < 0.001) and TherapyBot (p <\n0.001), and Phi-3-Mini similarly outperformed MentalHealthBot (p < 0.001) and\nTherapyBot (p < 0.001). No statistically significant difference was observed between\nQwen2.5-3B and Phi-3-Mini after correction (p = 0.11).\nThe magnitude of these differences was large, as indicated by Kendall’s coefficient\nof concordance (W = 0.62), reflecting strong agreement in the relative ranking of\nmodels across prompts. Descriptive statistics further support these findings: Qwen2.5-\n3B achieved a mean empathy score of M = 3.65 (SD = 0.40), and Phi-3-Mini\n11\n"}, {"page": 12, "text": "Table 5 Interpretation guidelines for statistical measures used in\nhuman empathy evaluation\nStatistic\nValue Range\nInterpretation\nFleiss’ κ\n< 0.20\nSlight agreement\n0.21 – 0.40\nFair agreement\n0.41 – 0.60\nModerate agreement\n0.61 – 0.80\nSubstantial agreement\n> 0.80\nAlmost perfect agreement\nFriedman χ2\np ≥0.05\nNo significant difference\np < 0.05\nStatistically significant\np < 0.01\nStrong significance\np < 0.001\nVery strong significance\nWilcoxon p-value\np ≥0.05\nNot significant\np < 0.05\nSignificant\np < 0.01\nStrong significance\np < 0.001\nVery strong significance\nKendall’s W\n< 0.10\nNegligible effect\n0.10 – 0.30\nSmall effect\n0.30 – 0.50\nModerate effect\n> 0.50\nLarge effect\nMean Likert Score\n1.0 – 2.0\nLow empathy\n2.1 – 3.0\nModerate empathy\n3.1 – 4.0\nHigh empathy\n4.1 – 5.0\nVery high empathy\nStandard Deviation\n< 0.5\nHigh annotator consensus\n0.5 – 1.0\nModerate variability\n> 1.0\nHigh disagreement\nachieved M = 3.45 (SD = 0.45), compared to MentalHealthBot (M = 3.15, SD =\n0.50) and TherapyBot (M = 3.10, SD = 0.52).\nOverall, the results demonstrate that general-purpose models consistently and sig-\nnificantly outperformed domain-specific mental health models in perceived empathy\naccording to human evaluation.\n5 Conclusion\nThis study evaluates the performance of general-purpose and domain-specific large\nlanguage models in retrieval-augmented generation for mental health support, and\nour experiments challenge the commonly held assumption that the only way to learn\ndomain knowledge is through specialisation via fine-tuning. We show that a strong\ngeneral-purpose reasoning model (Qwen2.5-3B) vastly outperforms specialisation via\nfine-tuning in generating empathetic, context-dependent responses.\nIn summary, for RAG-based systems, better reasoning over retrieved context is\nmore important than seeing domain-specific vocabulary beforehand, and future work\nshould investigate the potential of combining light fine-tuning with robust reasoning\nbackbones to reduce safety risks while improving empathy.\n12\n"}, {"page": 13, "text": "6 Declarations\n6.1 Funding\nThe authors did not receive support from any organization for the submitted work.\nNo funding was received for conducting this study.\n6.2 Ethics Approval\nThis study was approved by the Institutional Ethics Committee of the Faculty of\nScience and Information Technology (IEC-FSIT) at Daffodil International University\n(Ref No: IEC-FSIT/DIU/2025/2003). All procedures performed in studies involving\nhuman participants were in accordance with the ethical standards of the institutional\nresearch committee and with the 1964 Helsinki Declaration and its later amendments.\n6.3 Consent to Participate\nInformed consent was obtained from all individual participants (annotators) included\nin the study. Participants were informed of the nature of the task and their right to\nwithdraw at any time.\n6.4 Consent to Publish\nThe authors affirm that human research participants provided informed consent for\nthe publication of the anonymized and aggregated evaluation results.\n6.5 Clinical Trial Registration\nNot applicable. This study is not a clinical trial and does not involve any healthcare\ninterventions.\n6.6 Competing Interests\nThe authors have no relevant financial or non-financial interests to disclose.\n6.7 Author Contributions\nMd Abdullah Al Kafi: Conceptualization, Software, Writing – original draft. Raka\nMoni: Writing – original draft. Sumit Kumar Banshal: Supervision.\n6.8 Code Availability\nThe complete implementation, preprocessing pipelines, and evaluation scripts are\navailable at: https://github.com/abkafi1234/Mental Health models\n6.9 Data Availability\nAll datasets used in this study are publicly available, and access links are provided in\nthe associated GitHub repository.\n13\n"}, {"page": 14, "text": "References\n[1] Weizenbaum, J.: Eliza—a computer program for the study of natural language\ncommunication between man and machine. Communications of the ACM 9, 36–45\n(1966) https://doi.org/10.1145/365153.365168\n[2] Colby, K.M., Weber, S., Hilf, F.D.: Artificial paranoia. Artificial intelligence 2(1),\n1–25 (1971)\n[3] Fitzpatrick, K.K., Darcy, A., Vierhile, M.: Delivering cognitive behavior therapy\nto young adults with symptoms of depression and anxiety using a fully automated\nconversational agent (woebot): a randomized controlled trial. JMIR mental health\n4(2), 7785 (2017)\n[4] Inkster, B., Sarda, S., Subramanian, V., et al.: An empathy-driven, conversa-\ntional artificial intelligence agent (wysa) for digital mental well-being: real-world\ndata evaluation mixed-methods study. JMIR mHealth and uHealth 6(11), 12106\n(2018)\n[5] Ashish, V.: Attention is all you need. Advances in neural information processing\nsystems 30, (2017)\n[6] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report.\narXiv preprint arXiv:2303.08774 (2023)\n[7] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n[8] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto,\nA., Fung, P.: Survey of hallucination in natural language generation. ACM\ncomputing surveys 55(12), 1–38 (2023)\n[9] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨uttler, H.,\nLewis, M., Yih, W.-t., Rockt¨aschel, T., et al.: Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in neural information processing systems\n33, 9459–9474 (2020)\n[10] Liu, S., McCoy, A.B., Wright, A.: Improving large language model applica-\ntions in biomedicine with retrieval-augmented generation: a systematic review,\nmeta-analysis, and clinical development guidelines. Journal of the American Med-\nical Informatics Association 32, 605–615 (2025) https://doi.org/10.1093/jamia/\nocaf008\n[11] Zakka, C., Shad, R., Chaurasia, A., Dalal, A.R., Kim, J.L., Moor, M., Fong,\nR., Phillips, C., Alexander, K., Ashley, E., et al.: Almanac—retrieval-augmented\n14\n"}, {"page": 15, "text": "language models for clinical medicine. Nejm ai 1(2), 2300068 (2024)\n[12] Yang, K., Zhang, T., Kuang, Z., Xie, Q., Huang, J., Ananiadou, S.: Mentallama:\ninterpretable mental health analysis on social media with large language models.\nIn: Proceedings of the ACM Web Conference 2024, pp. 4489–4500 (2024)\n[13] Sun, H., Lin, Z., Zheng, C., Liu, S., Huang, M.: Psyqa: A chinese dataset\nfor generating long counseling text for mental health support. arXiv preprint\narXiv:2106.01702 (2021)\n[14] Qiu, H., He, H., Zhang, S., Li, A., Lan, Z.: Smile: Single-turn to multi-turn inclu-\nsive language expansion via chatgpt for mental health support. In: Findings of the\nAssociation for Computational Linguistics: EMNLP 2024, pp. 615–636 (2024)\n[15] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824–24837 (2022)\n[16] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\ninstructions with human feedback. Advances in neural information processing\nsystems 35, 27730–27744 (2022)\n[17] Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison,\nM., Hewett, R.J., Javaheripi, M., Kauffmann, P., et al.: Phi-4 technical report.\narXiv preprint arXiv:2412.08905 (2024)\n[18] Liu, C.-W., Lowe, R., Serban, I.V., Noseworthy, M., Charlin, L., Pineau, J.:\nHow not to evaluate your dialogue system: An empirical study of unsupervised\nevaluation metrics for dialogue response generation. In: Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing, pp. 2122–2132\n(2016)\n[19] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z.,\nLi, Z., Li, D., Xing, E., et al.: Judging llm-as-a-judge with mt-bench and chat-\nbot arena. Advances in neural information processing systems 36, 46595–46623\n(2023)\n[20] Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A.N., Li, T., Li, D., Zhu, B.,\nZhang, H., Jordan, M., Gonzalez, J.E., et al.: Chatbot arena: An open platform\nfor evaluating llms by human preference. In: Forty-first International Conference\non Machine Learning (2024)\n[21] Thien, N.L.T., Truc, T.P.T., Hung, P.C.: Mental Health Conversational AI Train-\ning Dataset. Kaggle (2025). https://doi.org/10.34740/KAGGLE/DSV/12120624\n. https://www.kaggle.com/dsv/12120624\n15\n"}]}