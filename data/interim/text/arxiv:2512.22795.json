{"doc_id": "arxiv:2512.22795", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.22795.pdf", "meta": {"doc_id": "arxiv:2512.22795", "source": "arxiv", "arxiv_id": "2512.22795", "title": "CNSight: Evaluation of Clinical Note Segmentation Tools", "authors": ["Risha Surana", "Adrian Law", "Sunwoo Kim", "Rishab Sridhar", "Angxiao Han", "Peiyu Hong"], "published": "2025-12-28T05:40:15Z", "updated": "2025-12-28T05:40:15Z", "summary": "Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.22795v1", "url_pdf": "https://arxiv.org/pdf/2512.22795.pdf", "meta_path": "data/raw/arxiv/meta/2512.22795.json", "sha256": "ca4b3314395b98eb27597157b8216f583b60e4ab9e1f3e6edc2f19e40e0406c9", "status": "ok", "fetched_at": "2026-02-18T02:23:44.596749+00:00"}, "pages": [{"page": 1, "text": "CNSight: Evaluation of Clinical Note Segmentation Tools\nRisha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong\nUniversity of Southern California\nAbstract\nClinical notes are often stored in unstructured\nor semi-structured formats after extraction\nfrom electronic medical record (EMR) systems,\nwhich complicates their use for secondary anal-\nysis and downstream clinical applications. Re-\nliable identification of section boundaries is\na key step toward structuring these notes, as\nsections such as history of present illness, med-\nications, and discharge instructions each pro-\nvide distinct clinical contexts. In this work, we\nevaluate rule-based baselines, domain-specific\ntransformer models, and large language models\nfor clinical note segmentation using a curated\ndataset of 1,000 notes from MIMIC-IV. Our\nexperiments show that large API-based mod-\nels achieve the best overall performance, with\nGPT-5-mini reaching a best average F1 of 72.4\nacross sentence-level and freetext segmenta-\ntion. Lightweight baselines remain competitive\non structured sentence-level tasks but falter on\nunstructured freetext. Our results provide guid-\nance for method selection and lay the ground-\nwork for downstream tasks such as information\nextraction, cohort identification, and automated\nsummarization.\n1\nIntroduction\nEHR data is often processed and presented in plain\ntext format for secondary use in modeling and data\nretrieval tasks. Clinical notes contain a wide range\nof information such as chief complaints, physician\nobservations, and past medical history that can sup-\nplement structured data like lab values or medica-\ntions. However the note text itself is often difficult\nto process with traditional or out-of-the-box models\nbecause of domain specific issues including med-\nical abbreviations, semi-structured text, unrelated\nor redundant documentation, and information that\nvaries depending on the patient or encounter.\nThe first step in analyzing clinical notes is to\nidentify sections from the full text and segment\nnotes into distinct categories. Our goal in this work\nis to evaluate the note segmentation effectiveness\nof different models from open source libraries and\nprior research using a curated dataset.\n2\nRelated Works\nSeveral prior studies have examined the problem of\nclinical note segmentation. Davis et al. (2025) in-\ntroduced MedSlice, a pipeline that uses fine-tuned\nlarge language models from open source libraries\nto securely segment clinical notes (Davis et al.,\n2025). Their work highlights the effectiveness of\nLLMs in capturing section boundaries in a domain\nwhere notes often vary in length and structure.\nEarlier work by Ganesan and Subotin (2014)\npresented a supervised approach to clinical text\nsegmentation. They evaluated the Essie 4 NLM li-\nbrary for its ability to identify and retrieve relevant\nsections from documents, demonstrating the util-\nity of domain-specific tools for segmentation tasks\n(Ganesan and Subotin, 2014). Edinger et al. (2018)\nevaluated multiple modeling approaches includ-\ning regularized logistic regression, support vector\nmachines, Naive Bayes, and conditional random\nfields for clinical text segmentation. Their study\nfocused on improving cohort retrieval through ac-\ncurate identification of note sections, emphasizing\nthe importance of segmentation as a foundation\nfor downstream clinical informatics applications\n(Edinger et al., 2018).\n3\nMethods\nWe evaluate whether different modeling ap-\nproaches can improve clinical note segmentation\ncompared to naive baselines.\n3.1\nDatasets\nWe use four main datasets in this project, de-\nrived from the MIMIC-IV corpus (Johnson et al.,\n2023) and supplemented with additional clinical-\ntext resources.\nMIMIC-IV is a publicly avail-\nable, de-identified dataset containing a wide range\nof clinical notes, including discharge summaries,\nphysician notes, and semi-structured chart data.\nFor evaluation across all datasets, we apply an\n80/10/10 train/validation/test split using k-fold\ncross-validation. A distribution of the most fre-\nquent tags in these datasets are displayed in Ap-\npendix B.\n1\narXiv:2512.22795v1  [cs.CL]  28 Dec 2025\n"}, {"page": 2, "text": "CNSight: Evaluation of Clinical Note Segmentation Tools\nPATIENT HISTORY\nCHIEF COMPLAINT\nMEDICATIONS\nDISCHARGE INSTRUCTIONS\nFree-text Clinical \nNotes\nSentence-split Clinical \nNotes\nLLMs\nBaseline Methods\nFigure 1: LLMs enable automated segmentation of clinical notes. Overview of the CNSight pipeline, where\nfree-text or sentence-split clinical notes are processed by large language models and baseline methods to extract\nstructured clinical sections such as patient history, chief complaint, medications, and discharge instructions.\nMIMIC Hospital.\nFrom MIMIC-IV, we con-\nstruct two custom datasets organized by note type\nusing the labeled “Hospital Course” subset (Aali\net al., 2025):\n• MIMIC Hospital Sentences: 1,000 unla-\nbeled free-text clinical notes, split into 17,487\nlabeled clinical note sections segmented at the\nsentence level.\n• MIMIC Hospital Freetext: 1,000 unlabeled\nfree-text clinical notes, generated from Mimic\nHospital Sentences, which preserve the origi-\nnal narrative structure of the documentation.\nMIMIC Note Freetext.\nThis dataset consists of\nunlabeled free-text clinical notes drawn directly\nfrom MIMIC-IV (Johnson et al., 2022). It provides\na large, unstructured resource of clinical language\nfor representation learning.\nAugmented Clinical Notes.\nThis dataset consists\nof additional unlabeled free-text clinical notes cu-\nrated for this project (Bonnet and Boulenger, 2024).\nIt is used to increase the diversity and size of the\ntraining corpus for representation learning.\n3.2\nModels\nAs baselines, we include a multinomial logistic re-\ngression classifier, a regex-based header matcher,\nand MedSpaCy (Eyre et al., 2021), a clinical NLP\ntoolkit for section detection and rule-based infor-\nmation extraction. These methods provide inter-\npretable and lightweight points of comparison.\nFor domain-specific transformer models, we\nevaluate LLaMA-2-7B (Touvron et al., 2023),\nMedAlpaca-7B (Han et al., 2023), and Meditron-\n7B (Chen et al., 2023). These open-source mod-\nels vary in their degree of biomedical adaptation\nand represent mid-scale transformer approaches\ntailored to clinical language tasks.\nFinally, we benchmark three API-based large\nlanguage models: GPT-5-mini (OpenAI, 2025),\nGemini 2.5 Flash (DeepMind, 2025), and Claude\n4.5 Haiku (Anthropic, 2025). These state-of-the-\nart systems offer broad-domain generalization and\nstrong zero- and few-shot performance.\n4\nEvaluation\n4.1\nMetrics\nModel performance is assessed using token-level\nPrecision, Recall, and F1. For clinical note segmen-\ntation, we treat each predicted section boundary\ntoken as a classification decision. In this setup, a\nTrue Positive (TP) is a predicted boundary token\nthat exactly matches a gold-standard boundary to-\nken, a False Positive (FP) is a predicted boundary\ntoken that does not correspond to any gold-standard\nboundary (i.e., a spurious split), and a False Neg-\native (FN) is a gold-standard boundary token that\n2\n"}, {"page": 3, "text": "Table 1: MIMIC Hospital results. Models are evaluated on two datasets: MIMIC Hospital Sentences (labeled\nclinical note sections) and MIMIC Hospital Freetext (unlabeled free-text clinical notes). We report Precision,\nRecall, and F1-score for each dataset. Baseline 1 and Baseline 2 are included for comparison. We bold the highest\nperforming model, and underline the second highest performing model per dataset. We additionally report the\naverage F1 across tasks.\nModel\nMIMIC Hospital Sentences\nMIMIC Hospital Freetext\nAvg. F1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nGPT-5-mini\n85.4\n81.6\n80.8\n87.8\n50.2\n63.9\n72.4\nGemini 2.5 Flash\n81.3\n79.4\n78.5\n87.3\n46.6\n60.8\n69.7\nClaude 4.5 Haiku\n83.3\n72.9\n76.5\n82.9\n33.6\n47.8\n62.2\nLLaMA-2-7B\n17.9\n12.4\n8.9\n87.3\n37.5\n52.4\n30.7\nMedAlpaca-7B\n5.5\n3.1\n0.6\n83.0\n25.2\n38.6\n19.6\nMeditron-7B\n0.4\n0.5\n0.3\n0.0\n0.0\n0.0\n0.2\nMultinomial Logistic Regression\n79.1\n77.7\n74.3\n-\n-\n-\n74.3\nMedSpaCy\n79.2\n78.1\n78.0\n94.4\n83.0\n88.3\n83.2\nthe model fails to predict (i.e., a missed split).\nFor the sentence classification task, we report\nweighted F1, which accounts for class imbalance\nby averaging per-class performance proportional to\nclass frequency. This choice reflects the fact that\nsome section types occur more frequently than oth-\ners in the labeled dataset. For the freetext segmen-\ntation task, we instead report micro-averaged F1,\nwhich aggregates decisions across all boundaries.\nThis metric provides a clearer measure of overall\nsegmentation quality when sections are highly vari-\nable in length and frequency.\n5\nResults\nIn Table 1, we analyze three API-based LLM mod-\nels, three HuggingFace locally hosted models, and\ntwo baselines for comparison. A summary of the\nmodel performance is observed in Figure 2.\n5.1\nBaseline Analysis\nTraditional baselines achieve solid but limited\nperformance on sentence-based analysis.\nThe\nMultinomial Logistic Regression classifier (Base-\nline 1) achieves strong performance on the sen-\ntence classification task, with a 74.3 F1. While\nthis lags behind API-based LLMs by 4–6 points,\nit is still competitive with weaker commercial sys-\ntems. MedSpaCy (Baseline 2) goes even further,\nreaching a 78.0 F1 on sentences—just 2.8 points\nbelow Gemini 2.5 Flash (78.5) and 2.8 points be-\nlow Claude 4.5 Haiku (76.5). This demonstrates\nthat rule-based, domain-specific NLP tools remain\nFigure 2: Summary of metrics across models. Com-\nparison of precision, recall, and F1 scores across med-\nical and general-purpose language models. General-\npurpose models outperform smaller domain-specific\nmodels.\nsurprisingly strong in structured settings.\nTraditional\nbaselines\nsurpass\nLLMs\nwith\nstrong performance on freetext-based analysis.\nMedSpaCy (Baseline 2) achieves the highest scores\non freetext analysis, achieving a high score of 94.4\nPrecision, which is over 6 points higher than GPT-\n5-mini.\n5.2\nAPI-Based LLMs\nAcross the board, GPT-5-mini performed the\nbest.\nOn both the Sentence Classification and\nFreetext tasks, GPT-5-mini achieved the highest\nscores across all metrics. Its F1 of 80.8 on sen-\n3\n"}, {"page": 4, "text": "tences and 63.9 on freetext demonstrates not only\nhigh precision but also consistently strong recall,\noutperforming other commercial LLMs by several\npoints. This indicates that GPT-5-mini is more\nrobust at identifying relevant spans without over-\npredicting, balancing sensitivity with specificity in\nthe clinical setting.\nGemini 2.5 Flash and Claude 4.5 Haiku trail\nclosely but unevenly.\nGemini performs competi-\ntively on both tasks, ranking second on freetext\nclassification (60.8 F1) and maintaining strong\nperformance on sentences (78.5 F1). Claude 4.5\nHaiku exhibits high precision (83.3) but suffers\nfrom lower recall (72.9) on sentence classification,\nindicating a conservative prediction style. On free-\ntext, its performance drops significantly (47.8 F1),\nunderscoring difficulty in handling unstructured,\nnoisy inputs.\n5.3\nSmall and Domain-Specific LLMs\nDomain-tuned small LLMs underperform sub-\nstantially.\nBoth LLaMA-2-7B and MedAlpaca-\n7B struggle, with sentence-level F1-scores of 8.9\nand 0.6 respectively. This reflects their inability to\ncapture the clinical context with limited scale and\ntraining. Although LLaMA-2-7B reaches moderate\nprecision on freetext (87.3), its recall (37.5) lags,\nleading to mediocre overall performance (52.4 F1).\nAbbreviations and domain-specific shorthand often\ncaused these models to miss boundaries, particu-\nlarly in medication and lab result sections where\nformatting was dense. Multi-line headers and ir-\nregular spacing also confused both rule-based base-\nlines and smaller LLMs, leading to false positives\nwhere extraneous breaks were inserted.\nMeditron-7B underperforms expectations.\nDe-\nspite being designed as a biomedical-focused LLM,\nMeditron-7B fails to produce competitive results,\nwith F1-scores of 0.3 on sentences and 0.0 on free-\ntext. This suggests either significant mismatches\nbetween training and evaluation domains or lim-\nitations in model scale and optimization. Unlike\nMedAlpaca, which shows partial utility in freetext,\nMeditron’s outputs approach non-functional levels\nof performance.\nANOVA Testing.\nWe conducted a one-way\nANOVA on model F1 scores across the two tasks.\nThe test yielded F = 2.45 with p = 0.18, indi-\ncating that observed variations among models are\nnot statistically significant at the 0.05 level. This\nTable 2: Annotator Agreement: Humans vs. LLMs and\nBaselines on Clinical Section Tagging\nModel\nKappa Agreement\nN\nBraintrust LLM Models\nClaude\n46.5\n53.4%\n786\nGemini\n46.4\n52.6% 1369\nGPT-5\n43.2\n49.5% 1578\nBaseline Models\nLog Regression\n18.8\n24.2% 3574\nEmbedding Spacy\n15.4\n20.6% 3574\nBlank Spacy\n13.6\n18.2% 3574\nsuggests that while performance differences across\ntasks are visible, they should be interpreted with\ncaution.\n6\nHuman Evaluations\nTo better understand how humans compare to auto-\nmated systems on difficult sentence classification\ntasks, we conducted a small human evaluation us-\ning an unlabeled subset of clinical notes. Since it\nis challenging to obtain large expert-annotated cor-\npora, our goal was not to create a new gold-standard\ndataset but to examine how general human anno-\ntators behave when presented with ambiguous or\nweakly structured inputs. This allows us to com-\npare annotator behavior to the behavior of both\nbaseline models and large language models and to\nevaluate where automated methods diverge from\nhuman judgment.\nWe sampled sentences from an unlabeled\nMIMIC-derived corpus by splitting raw notes into\nindividual units. These sentences were then pro-\nvided to nonexpert annotators who selected a sec-\ntion label from the same set of categories used in\nour supervised experiments. Because many ex-\ntracted sentences contain partial headers, demo-\ngraphic fragments, or minimal context, this setup\nreflects a more challenging classification scenario\nthan the segmented hospital course dataset used for\nmodel evaluation.\nWe report Cohen’s Kappa and Percent Agree-\nment between each system and the majority human\nlabel (Table 2). Agreement between human anno-\ntators and API-based LLMs is moderate, with\nClaude and Gemini reaching Kappa values of\n0.46 and agreements above 52 percent. GPT\n5 is slightly lower with 49.5 percent agreement.\nBaseline models show substantially weaker align-\nment with humans. Logistic regression reaches a\nKappa of 0.19, while embedding-based and blank\nMedSpaCy variants range from 0.14 to 0.13.\nTo further understand disagreement patterns, we\n4\n"}, {"page": 5, "text": "Table 3: Label Distribution Across Models for Ambiguous Header/Demographic Sections (First Row). Humans\nlabel 66% of header rows as “OTHER”, reflecting recognition of inherent ambiguity and multiple valid classification\noptions. In contrast, baseline and LLM models exhibit strong, often inaccurate biases: Blank Spacy defaults to\nALLERGIES (78/129), Embedding Spacy to FAMILY HISTORY (120/129), and Log Regression to ALLERGIES\n(108/129). This demonstrates that humans employ greater flexibility and judgment when faced with ambiguous\nsections, while automated systems tend toward fixed predictions.\nLabel\nHuman\nBlank Spacy\nEmbedding Spacy\nLog Reg\nClaude\nGemini\nGPT-5\nALLERGIES\n45\n78\n0\n108\n12\n15\n45\nSEX\n108\n0\n0\n0\n9\n27\n3\nOTHER\n189\n0\n0\n0\n0\n0\n0\nSERVICE\n27\n0\n0\n0\n0\n0\n0\nHISTORY OF PRESENT ILLNESS\n9\n3\n0\n6\n0\n6\n9\nMAJOR SURGICAL OR INV. PROC.\n9\n0\n0\n0\n0\n0\n0\nFAMILY HISTORY\n0\n0\n120\n0\n0\n0\n0\nDISCHARGE DIAGNOSIS\n0\n39\n0\n0\n0\n0\n0\nPHYSICAL EXAM\n0\n0\n9\n0\n0\n0\n0\nPERTINENT RESULTS\n0\n9\n0\n9\n0\n0\n0\nATTENDING\n0\n0\n0\n0\n3\n0\n0\nCHIEF COMPLAINT\n0\n0\n0\n3\n3\n0\n0\nPAST MEDICAL HISTORY\n0\n0\n0\n3\n0\n0\n0\nTotal\n387\n129\n129\n129\n27\n48\n57\nanalyzed the labels assigned to the most ambiguous\nsubset of sentences, which primarily consisted of\nheader-like fragments or demographic rows. As\nshown in Table 3, humans label 66 percent of\nthese sentences as OTHER, recognizing that many\nof these items have no single correct section la-\nbel. Automated systems behave very differently.\nThe logistic regression model strongly defaults to\nALLERGIES, embedding-based MedSpaCy pre-\ndicts FAMILY HISTORY for nearly all items,\nand blank MedSpaCy heavily favors ALLERGIES.\nAPI-based LLMs also exhibit biases but with less\nextreme concentration in a single class.\nThese results indicate that human annotators\ndemonstrate flexibility when dealing with ambigu-\nous clinical text and do not force a strict interpre-\ntation of section categories. Automated models, in\ncontrast, tend to collapse uncertainty into a small\nset of high-probability labels. This evaluation was\ntherefore used not as ground truth for training or\nbenchmarking but as a way to measure differences\nin behavior between humans and automated sys-\ntems when faced with difficult and underspecified\nsentences.\n7\nConclusion\nIn this study, we evaluated a wide range of\nclinical note segmentation approaches, including\nrule-based systems, traditional classifiers, domain-\nspecific transformer models, and state-of-the-art\nlarge language models. Our results show that gen-\neral lightweight baselines like MedSpacy remain\nstrong on unstructured freetext tasks, but their per-\nformance drops when applied to structured sen-\ntences. API-based large language models achieve\nthe highest overall performance on Sentence-level\nevaluations, with GPT 5 providing the most consis-\ntent gains across both sentence classification and\nfreetext segmentation.\nThe human evaluation highlights an additional\ndimension of this problem. When presented with\ndifficult or weakly contextualized clinical sen-\ntences, human annotators show flexible judgment\nand frequently select ambiguous categories rather\nthan forcing a narrow interpretation. In contrast,\nautomated systems tend to collapse uncertainty into\na small set of high-probability labels. This pattern\nis especially visible in header and demographic\nfragments, where baselines and LLMs often apply\nstrong but inaccurate defaults. These findings indi-\ncate that segmentation models are sensitive not only\nto domain structure but also to the way ambiguous\ninput is represented and labeled.\nOverall, our study demonstrates that high qual-\nity segmentation of clinical notes depends on both\nmodel capability and the nature of the input text.\nLarge models offer strong generalization to struc-\ntured documentation, while custom rule-based and\nclassical methods remain valuable for un-structured\nformats. The human evaluation further underscores\nthe importance of understanding annotation behav-\nior when designing or interpreting segmentation\nsystems. Future work should explore methods that\nbetter capture uncertainty in ambiguous sections\nand that leverage human-like flexibility in classifi-\ncation decisions.\n5\n"}, {"page": 6, "text": "References\nAsad Aali, Dave Van Veen, Yamin Arefeen, Jason\nHom, Christian Bluethgen, Eduardo Pontes Reis, Ser-\ngios Gatidis, Namuun Clifford, Joseph Daws, Arash\nTehrani, Jangwon Kim, and Akshay Chaudhari. 2025.\nMimic-iv-ext-bhc: Labeled clinical notes dataset\nfor hospital course summarization (version 1.2.0).\nPhysioNet.\nhttps://physionet.org/content/\nlabelled-notes-hospital-course/1.2.0/.\nAnthropic. 2025. Claude 4.5 model card. https://\nwww.anthropic.com.\nAntoine Bonnet and Paul Boulenger. 2024.\nAug-\nmented clinical notes.\nHugging Face Datasets.\nhttps://huggingface.co/datasets/AGBonnet/\naugmented-clinical-notes.\nQiao Chen and 1 others. 2023. Meditron: A specialized\nlarge language model for medicine. arXiv preprint\narXiv:2311.16079.\nJoshua Davis, Thomas Sounack, Kate Sciacca, Jessie\nBrain, Brigitte Durieux, Nicole Agaronnik, and Char-\nlotta Lindvall. 2025. Medslice: Fine-tuned large\nlanguage models for secure clinical note sectioning.\narXiv preprint arXiv:2501.14105.\nGoogle DeepMind. 2025.\nGemini 2.5 model card.\nhttps://deepmind.google.\nThomas Edinger, Dina Demner-Fushman, Aaron M Co-\nhen, Steven Bedrick, and William Hersh. 2018. Eval-\nuation of clinical text segmentation to facilitate co-\nhort retrieval. AMIA Annual Symposium Proceedings,\n2017:660–669.\nHarris Eyre and 1 others. 2021. medspacy: A library\nfor clinical natural language processing with spacy.\nIn AMIA Annual Symposium Proceedings, pages 438–\n447.\nKavita Ganesan and Michael Subotin. 2014. A general\nsupervised approach to segmentation of clinical texts.\nIn 2014 IEEE International Conference on Big Data\n(Big Data), pages 33–40. IEEE.\nShih-Cheng Han and 1 others. 2023.\nMedal-\npaca: An open-source biomedical instruction-tuned\nllama model.\nhttps://github.com/kbressem/\nmedAlpaca.\nAlistair E. W. Johnson, Lucas Bulgarelli, Tom J. Pollard,\nSteven Horng, Leo Anthony Celi, and Roger G. Mark.\n2023. Mimic-iv, a freely accessible electronic health\nrecord dataset. Scientific Data, 10(1):1–7.\nAlistair E. W. Johnson and 1 others. 2022.\nMimic-\niv-note: Deidentified free-text clinical notes (ver-\nsion 2.2). PhysioNet. https://physionet.org/\ncontent/mimic-iv-note/2.2/.\nOpenAI. 2025.\nGpt-5 technical report.\nhttps://\nopenai.com/research/gpt-5.\nHugo Touvron and 1 others. 2023. Llama: Open and\nefficient foundation language models. arXiv preprint\narXiv:2302.13971.\nA\nAcknowledgments\nThe authors gratefully acknowledge Braintrust for\nproviding access to API-based models and for their\nassistance with system setup and integration, which\nfacilitated the execution of these experiments.\nB\nData Distribution\n6\n"}, {"page": 7, "text": "Figure 3: A small set of clinical tags dominates the dataset. Bar chart showing the most frequent clinical tags\n(occurring at least 50 times), with counts on the y-axis and tag categories on the x-axis.\nFigure 4: Most notes contained approximately 18 tags. Bar chart showing the distribution of the number of tags\nacross all notes, shows a relatively consistent number across the dataset.\n7\n"}, {"page": 8, "text": "Figure 5: Sentence length variation. Bar chart showing the sentence length distributions across tags, shows a high\nnumber of short \"sentences\", referring to phrases and small extracted header sections for increased precision.\n8\n"}]}