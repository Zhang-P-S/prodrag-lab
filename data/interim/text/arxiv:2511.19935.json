{"doc_id": "arxiv:2511.19935", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.19935.pdf", "meta": {"doc_id": "arxiv:2511.19935", "source": "arxiv", "arxiv_id": "2511.19935", "title": "EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning", "authors": ["Songlin Zhao", "Michael Pitts", "Zhuwei Qin"], "published": "2025-11-25T05:20:17Z", "updated": "2026-01-21T23:43:42Z", "summary": "Large language models (LLMs) are increasingly adapted into domain-specific variants for applications in law, healthcare, and finance. Their scale, however, limits deployment in resource-constrained settings, and existing compression approaches often either degrade after domain adaptation or require substantial additional computation. We introduce EfficientXpert, a lightweight framework for domain pruning that integrates ForeSight Mask, a propagation-aware criterion for selecting weights to prune without backpropagation, and Partial Brain Surgeon, an efficient closed-form update for low-rank adapters under a fixed sparsity pattern. With fine-tuning cost comparable to standard LoRA, EfficientXpert converts a general pretrained model into a sparse, domain-adapted expert in a single pruning step. Across health and legal benchmarks, EfficientXpert reaches up to 98 percent of dense performance at 40 percent sparsity, improving over prior pruning baselines while matching LoRA training time and staying within 1 percent of LoRA peak GPU memory in our experiments.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.19935v2", "url_pdf": "https://arxiv.org/pdf/2511.19935.pdf", "meta_path": "data/raw/arxiv/meta/2511.19935.json", "sha256": "91950f5deac43f12abb86c18369fe2c625a40f0d4656288f9245c629a8c4e596", "status": "ok", "fetched_at": "2026-02-18T02:26:23.536032+00:00"}, "pages": [{"page": 1, "text": "EfficientXpert: Efficient Domain Adaptation for Large Language Models via\nPropagation-Aware Pruning\nSonglin Zhao1 , Michael Pitts2 , Zhuwei Qin2\n1Univeristy of California, Berkeley\n2San Francisco State University\ntagorezhao@berkeley.edu, {mpitts1, zwqin}@sfsu.edu,\nAbstract\nThe rapid advancement of large language mod-\nels (LLMs) has created an increasing demand for\ndomain-specialized variants in areas such as law,\nhealthcare, and finance.\nHowever, their large\nsize remains a barrier to deployment in resource-\nconstrained environments, and existing compres-\nsion methods either fail to generalize across do-\nmains or introduce high overhead. In this work,\nwe propose EfficientXpert, a lightweight domain\npruning framework that combines a propagation-\naware pruning criterion (ForeSight Mask) with an\nefficient adapter update algorithm (Partial Brain\nSurgeon).\nWith fine-tuning cost comparable to\nplain LoRA, EfficientXpert enables a one-step\ntransformation of general pretrained models into\nsparse, domain-adapted experts.\nIt achieves up\nto 98% of the dense model’s performance at 40%\nsparsity on health and legal tasks, outperforming\nstate of the art baselines while matching LoRA’s\ntraining time and operating within 1% of LoRA’s\npeak GPU memory footprint.\n1\nIntroduction\nDomain-specific LLMs have advanced rapidly in recent\nyears, driven by the growing demand in specialized domains\nsuch as law, healthcare, and finance [Song et al., 2025; Zheng\net al., 2021; Jin et al., 2019]. However, real-world deploy-\nments in these domains are often resource-constrained, as pri-\nvacy and compliance requirements frequently necessitate on-\npremises or private-infrastructure inference. Therefore, prac-\ntitioners commonly combine parameter-efficient fine-tuning\n(PEFT) to reduce adaptation cost with model compression\ntechniques to reduce inference memory and computation.\nPEFT enables efficient domain adaptation by updating only\na small subset of parameters, often achieving performance\ncomparable to full fine-tuning. Among PEFT approaches,\nLow-Rank Adaptation (LoRA) is particularly popular be-\ncause it injects domain-specific behavior via low-rank up-\ndates while keeping most model weights frozen [Hu et al.,\n2022]. However, it primarily reduces training cost: the result-\ning domain-adapted model largely retains the base model’s\nparameter count and therefore its inference-time memory\nand latency overhead, which complicates deployment in\nresource-constrained settings.\nThis motivates the need to\ncompress domain-specialized LLMs in a way that preserves\ndomain performance while reducing inference-time com-\npute and memory requirements [Frantar and Alistarh, 2023;\nHan et al., 2015].\nOne promising approach to reducing the size of LLMs is\nmodel pruning, which eliminates redundant weights to lower\nmemory and computation costs [Han et al., 2015]. Recent\nmethods such as SparseGPT [Frantar and Alistarh, 2023] and\nWanda [Sun et al., 2023] demonstrate how model can retain\ngeneral abilities with post-training pruning at 50% sparsity.\nHowever, applying these general-purpose pruning strategies\nto domain-adapted models is nontrivial. Two natural base-\nlines are (i) pruning a domain-adapted model post hoc and\n(ii) pruning first and then performing PEFT; both can sub-\nstantially degrade domain performance. First, as we show in\nSec. 3, directly applying post training pruning to a domain\nadapted model can cause substantially larger performance\ndegradation on domain tasks than on general benchmarks,\nsuggesting that domain adaptation shifts weight saliency in\nways not captured by general pruning criteria. Second, PEFT\nupdates (e.g., LoRA) introduce dense parameter changes and\ndo not preserve sparsity; consequently, re-enforcing the pre-\nPEFT pruning mask after fine-tuning is often misaligned\nwith the adapted model and leads to significant domain-\nperformance drops [Hu et al., 2022].\nTo address these challenges, recent methods such as D-\nPruner [Zhang et al., 2024] and ATP [Lu et al., 2024] in-\ncorporate domain-aware pruning and fine-tuning strategies to\nbridge this gap. D-Pruner uses a dual-pruning strategy that\njointly considers both general and domain-specific weight\nimportance using gradient-based approximations and regu-\nlarization techniques [Zhang et al., 2024].\nATP, in con-\ntrast, employs a trained pruning-decision generator, which\nadaptively refines model structure throughout the LoRA fine-\ntuning process [Lu et al., 2024]. While effective, these ap-\nproaches rely on gradient-based optimization and auxiliary\nnetworks, which substantially increase adaptation time and\nadd nontrivial memory overhead, compounding the cost of\nan already expensive fine-tuning pipeline [Lu et al., 2024;\nZhang et al., 2024]. Moreover, these prior methods evaluate\nonly a limited set of tasks, whereas our broader evaluation\ndemonstrates that pruning sensitivity depends more critically\narXiv:2511.19935v2  [cs.LG]  21 Jan 2026\n"}, {"page": 2, "text": "on domain than task, highlighting a key oversight in earlier\napproaches.\nIn this work, we introduce EfficientXpert, a lightweight\nframework that integrates pruning with LoRA fine-tuning to\nproduce sparse, domain-adapted LLMs. It introduces two key\ninnovations:\n• ForeSight Mask: A domain-aware, dynamic, gradient-\nfree pruning method that incorporates forward error\npropagation into its scoring mechanism by estimating\nthe impact of weights on downstream representations.\n• Partial Brain Surgeon: An efficient adapter realign-\nment step that solves a ridge regression to suppress\npruned coordinates in constant time complexity, ensur-\ning that low-rank updates stay aligned with the evolving\nsparse structure.\nAcross a comprehensive suite of health and legal tasks, Ef-\nficientXpert retains up to 99.8% and 98.43% of dense model\nperformance at 40% sparsity on LLaMA 7B, while keeping\nend to end wall clock runtime within 18% of plain LoRA un-\nder the same training configuration and maintaining a com-\nparable peak GPU memory footprint.\n2\nRelated Work\n2.1\nParameter-Efficient Fine-Tuning\nAs LLMs grow in size and capability, Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a vital strategy for adapt-\ning pretrained models to domains and downstream tasks [Li\nand Liang, 2021].\nAmong these, Low-Rank Adaptation\n(LoRA) has emerged as one of the most widely adopted ap-\nproaches for adapting LLMs to new domains and tasks.\nLoRA injects trainable low-rank matrices into the atten-\ntion and feedforward layers of transformer models, enabling\nefficient adaptation while leaving the original weights un-\nchanged [Hu et al., 2022].\nLoRA’s simplicity and strong\nempirical performance have driven its rapid adoption in do-\nmains such as health, law, and finance, where recent mod-\nels like FinGPT [Yang et al., 2023], Med-PaLM [Anil et al.,\n2023], and LawGPT [Zhou et al., 2024] successfully adapt\ngeneral-purpose LLMs to specialized tasks with limited su-\npervision and compute. Despite its success, LoRA does not\nreduce the size or latency of the final model, since the frozen\nbackbone remains fully intact. Recent works have begun to\nexplore combinations of LoRA with pruning on general tasks,\nseeking to preserve LoRA’s training efficiency while improv-\ning deployment efficiency [Zhang et al., 2023]. However, the\nmechanisms by which low-rank updates capture domain- and\ntask-specific information remain largely unexplored.\n2.2\nNeural Network Pruning\nPruning is a standard compression technique that removes\nweights while aiming to preserve task performance [Qin\net al., 2018; Park et al., 2020].\nRecent methods such as\nSparseGPT [Frantar and Alistarh, 2023] and Wanda [Sun et\nal., 2023] prune pretrained LLMs using importance signals\ncomputed from general calibration data, and can retain gen-\neral benchmark performance even at high sparsity.\nCaseHold\nPubMedQA\nGeneral\nDomain\n0\n10\n20\n30\n40\n50\n60\n70\nAccuracy Degradation (%)\n61.6%\n8.0%\n37.4%\n4.5%\n10.1%\n4.6%\n5.7%\nOne-shot Pruning Accuracy Degradation\nWanda+LoRA\nLoRA+Wanda\nOurs\nGeneral Wanda\nFigure 1: Accuracy degradation after single pass pruning on do-\nmain adapted LLaMA2 7B under four pipelines: Wanda then LoRA,\nLoRA then Wanda, our method, and Wanda with general calibration.\nDegradation is measured as the accuracy drop relative to the corre-\nsponding unpruned model. Results are averaged over 10 runs on two\ndomain QA benchmarks, CaseHold and PubMedQA. The General\nbar is the mean accuracy degradation reported in the Wanda paper.\nDomain adaptation complicates this picture.\nDomain\ndata can deviate substantially from the pretraining distribu-\ntion [Zhang et al., 2024], and fine tuning reshapes weight and\nactivation statistics. As a result, pruning decisions derived be-\nfore fine tuning often transfer poorly to domain tasks, while\npruning after domain fine tuning can also cause large domain\nperformance drops and may require additional fine tuning to\nrecover [Lu et al., 2024; Wu et al., 2025b; Wu et al., 2025a;\nGuo et al., 2025].\nTo address these issues, domain adaptive pruning methods\nsuch as D-Pruner and ATP update pruning decisions using do-\nmain signals during adaptation. D-Pruner combines general\nand domain specific importance scores via gradient based ap-\nproximations and regularization to preserve domain critical\nweights [Zhang et al., 2024]. ATP learns a pruning deci-\nsion network to update masks during LoRA fine tuning and\nregularizes adapters to align with the evolving sparse struc-\nture [Lu et al., 2024]. Although effective, both approaches\nadd substantial overhead: gradient based scoring or auxil-\niary networks increase training time and introduce nontrivial\nGPU memory cost on top of an already expensive fine tuning\npipeline.\n3\nMotivation\nThis section provides motivating evidence that general pur-\npose post training pruning does not reliably transfer to do-\nmain adapted LLMs. We start from LLaMA2 7B hf models\nfine tuned with LoRA on PubMedQA (health QA) and Case-\nHold (legal QA), then apply Wanda at 50% sparsity using a\nsingle pass pruning step. We consider two natural pipelines:\npruning before LoRA and keeping the mask fixed through-\nout fine tuning (Wanda+LoRA), and pruning after LoRA\n(LoRA+Wanda). For reference, we also include the mean ac-\ncuracy degradation reported for general domain pruning in\nthe original Wanda study.\nFigure 1 reports accuracy degradation, measured as the ac-\ncuracy drop relative to the corresponding unpruned model.\nWanda yields substantially larger degradation on the legal do-\nmain than on the health domain, indicating that pruning be-\n"}, {"page": 3, "text": "Figure 2: (a) Comparison of EfficientXpert with existing domain pruning methods. (b) Overview of EfficientXpert framework, including\nForeSight Mask and Partial Brain Surgeon. EfficientXpert iteratively updates adapters, smooths importance scores, applies corrections to\nsurviving weights, and merges the mask into dense weights to create a sparse, domain-specialized expert.\nhavior can vary sharply across domains after adaptation. In\ncontrast, our method maintains low degradation across do-\nmains. These results motivate domain aware pruning strate-\ngies that account for domain induced shifts during fine tuning.\n4\nMethodology\nTo address the failure modes identified in Sec. 3, we pro-\npose EfficientXpert, which consists of two tightly coupled\ncomponents. The ForeSight Mask dynamically determines\ndomain-aware pruning by combining frozen general weights\nwith evolving domain-specific adapters, selecting weights\nbased on their impact on downstream errors without gradi-\nents.\nComplementing this, Partial Brain Surgeon (PBS)\nperforms a post-pruning adapter recovery step: given the\nfixed mask, it updates the low-rank factors (e.g., updating B\nwith A fixed) by solving a closed-form weighted ridge prob-\nlem derived from a diagonal activation-norm approximation.\nThis recovery reallocates the limited rank budget to compen-\nsate for pruning-induced loss while explicitly controlling drift\non retained weights, thereby preserving domain-specific be-\nhavior that iterative gradient updates may fail to restore under\ntight sparsity constraints.(see Figure 2).\n4.1\nNotations\nFor clarity, we present the method on two consecutive LoRA-\naugmented linear layers in an MLP. The same procedure ap-\nplies to all prunable weights except the final layer in each\ntransformer block; implementation variants are provided in\nthe Appendix B. Let W1 ∈Rm×n and W2 ∈Rn×p be frozen\nbase weights with LoRA adapters (B1, A1) and (B2, A2),\nwhere B1 ∈Rm×r, A1 ∈Rr×n, B2 ∈Rn×r, A2 ∈Rr×p,\nand r ≪min(m, n, p). Given input activations X ∈Rl×m\nwith l = batch size × sequence length. Here, we focus on\nlearning a binary mask M for W1 + B1A1. Nonlinearities\ncan be ignored without loss of generality, as justified in Ap-\npendix C.\n4.2\nForeSight Mask: Propagation-Aware Pruning\nPruning upstream weights induces representation errors that\npropagate through subsequent layers, amplifying their down-\nstream impact, an effect that is particularly harmful in\ndomain-specific settings.\nTo explicitly account for this propagation, we score a prun-\ning mask M for the first layer by the induced change in the\ntwo-layer output:\nLFS(M) =\n\r\rX\n\u0000(M ⊙U1) −U1\n\u0001\nU2\n\r\r2\nF ,\n(1)\nwhere U1 := W1 + B1A1, U2 := W2 + B2A2.\nFrom ForeSight loss to a pruning criterion.\nRather than\noptimizing (1) directly, we derive a per-weight importance\nscore by approximating the loss increase incurred by zero-\ning a single entry θij := U1[i, j] while holding X and U2\nfixed. Using a second-order Taylor expansion around a (lo-\ncal) stationary point of ∥XU1U2∥2\nF (proof in Appendix C),\nthe predicted loss increase is\n∆Lij ≈\n1\n2 θ2\nij\n\u0000X⊤X\n\u0001\nii\n\u0000U2U ⊤\n2\n\u0001\njj.\n(2)\nWe use (2) as the ForeSight importance score: it couples\n(i) the magnitude of the pruned weight, (ii) the input-energy\nalong the affected row via (X⊤X)ii, and (iii) the downstream\namplification along the affected column via (U2U ⊤\n2 )jj.\nThe evolving adapters B1A1 and B2A2 encode early,\ndomain-specific updates learned during finetuning. ForeSight\nMask leverages these adapted effective weights through U1\nand U2 in (1)–(2), thereby injecting domain information into\npruning decisions without requiring gradient computation.\nWe compute this score for all weights in W1, and obtain the\nbinary mask M by removing the p-th percentile of weights\nrow-wise. For details about how group query attention mech-\nanisms and MLP layers are handled, please refer to the Ap-\npendix.\nConnection to Optimal Brain Damage.\nWanda estimates\nweight importance from local layer statistics (e.g., weight\nmagnitude and activation-based signals). In contrast, Fore-\nSight derives an importance score from a second-order ap-\nproximation of the downstream loss incurred by pruning an\nupstream weight, explicitly weighting each entry by input en-\nergy and downstream amplification. As a result, ForeSight\ncan be viewed as an Optimal Brain Damage–style (diago-\nnal) second-order criterion that is propagation-aware, which\nis particularly relevant for domain adaptation where preserv-\ning downstream fidelity is critical.\n"}, {"page": 4, "text": "4.3\nPartial Brain Surgeon\nAfter applying the ForeSight mask M, the effective weight\nbecomes sparse, while the LoRA adapter remains parameter-\nized in the dense space. With fixed rank r, gradient-based\ntraining may be unable to efficiently reallocate the limited\nadapter capacity to the surviving subnetwork, particularly at\nhigh sparsity.\nTo mitigate this mismatch, we introduce a post-hoc adapter\nrealignment step that updates the LoRA factor B1 while hold-\ning A1 fixed. Let U1 = W1 + B1A1. For each output row i,\nwe compute a rank-constrained update ∆bi ∈R1×r by solv-\ning a column-weighted ridge problem under the diagonal ac-\ntivation approximation g ≈diag(X⊤X), with D = diag(g):\nmin\n∆bi∈R1×r\n\r\r\r\n\u0000U1,i,Si + ∆biA1,:,Si\n\u0001\nD1/2\nSi\n\r\r\r\n2\n2\n+\n\r\r\r\n\u0000∆biA1,:,Ki\n\u0001\nD1/2\nKi\n\r\r\r\n2\n2 + λ∥∆bi∥2\n2,\n(3)\nwhere Si = {j : Mij = 0} and Ki = {j : Mij = 1} denote\nthe pruned and retained column sets in row i, and DSi and\nDKi are the corresponding principal submatrices of D. The\nparameter λ > 0 provides Tikhonov regularization.\nProblem (3) admits a closed-form solution. Let AS :=\nA1,:,Si, DS := DSi, AK := A1,:,Ki, and DK = DKi. Using\nthe decomposition A1DA⊤\n1 = ASDSA⊤\nS + AKDKA⊤\nK, we\nobtain\n∆bi = −U1,i,Si DS A⊤\nS\n\u0010\nA1DA⊤\n1 + λIr\n\u0011−1\n(4)\nwhich can be computed efficiently by precomputing A1DA⊤\n1\nonce per layer and solving an r × r linear system per row.\nStacking the rows yields ∆B = [∆b⊤\n1 · · ·∆b⊤\nm]⊤, and we re-\nplace B ←B + ∆B.\nThe Partial Brain Surgeon step suppresses pruned weights\nwith minimal adapter perturbation, ensuring its updates re-\nmain aligned with the sparse pattern and enabling more effi-\ncient learning within the ForeSight Mask’s support.\nCrucially, this correction also serves as a better initializa-\ntion point for subsequent gradient updates. Under sparsity\nconstraints, learning dynamics are highly sensitive to initial-\nization. By steering the adapter toward domain-relevant, un-\npruned pathways from the outset, Partial Brain Surgeon en-\nables more efficient adaptation and learning on the parts of\nthe model that matter most for the target domain.\n4.4\nEfficientXpert Algorithm\nAlgorithm 1 summarises EfficientXpert. Starting from a pre-\ntrained backbone, we finetune LoRA adapters on the domain\ncorpus D while simultaneously pruning with a calibration\nset Dcal. At each epoch we (i) update the low-rank adapters,\n(ii) compute layer-wise ForeSight importance scores, (iii)\nsmooth these scores with an Exponential Moving Average\n(EMA) so the mask can evolve during training, and (iv) apply\na Partial Brain Surgeon (PBS) correction that retargets the\nadapters to the surviving weights. After the final epoch the\nlearned mask M is merged into the dense weights, yielding a\nsparse, domain-specialised expert.\nAlgorithm 1 EfficientXpert\n1: Input: f(X | M, W, B, A), D, Dcal, s, r, T\n2: Output: f ⋆\n3: for t = 1 to T do\n4:\nupdate B, A on D\n5:\nfor layer l do\n6:\nSl\nt ←ForeSight(l, t)\n7:\nSl ←ηSl\nt + (1−η)Sl\nt−1\n8:\nremove last p fraction of entries in each row of Sl\n9:\n∆Bl ←PBS(Sl)\n10:\nBl ←Bl + ∆Bl\n11:\nend for\n12: end for\n13: merge BA into W, apply M\n14: return f ⋆(X | W ⊙M)\nTime Complexity\nForeSight costs O(mnr),\nwhereas\nWanda costs O(mn); because r ≪min(m, n) (typically\nr ∈{4, . . . , 64}), the additional overhead is only a small con-\nstant factor. Partial Brain Surgeon (PBS) can be executed in\nparallel across rows, achieving an effective constant time run-\ntime in practice under sufficient compute resources. Alterna-\ntively, it can be run sequentially with O(m) memory, yield-\ning a time complexity of O(mr3); since r is typically small,\nthis memory-efficient approach is practical for deployment\nin constrained environments. In contrast, ATP and D-Pruner\ninvoke transformer-scale decision networks or full-gradient\nback-propagation, incurring O(Lmn) time or worse, where\nL is the number of transformer layers in the backbone model.\n5\nExperiment\nDatasets\nExpanding on previous works, we evaluate Effi-\ncientXpert in two domains: Health and Legal. Each domain\nincludes tasks such as language modeling, question answer-\ning (QA), natural language inference (NLI), and summariza-\ntion. For the Health domain, we construct a training set from\nMedNLI, PubMedQA, and HQS at a ratio of 7:7:1, totaling\n15,000 instances with a sequence length of 2048 tokens. For\nthe Legal domain, we use CaseHold, ContractNLI, and Bill-\nSum at a 7:6:2 ratio, also totaling 15,000 instances with the\nsame sequence length. Ratios are based on task complex-\nity, data availability, and preliminary validation performance.\nDataset details are provided in Appendix D. As shown in Sec-\ntion 5.4, domain calibration does not improve pruning per-\nformance on domain tasks; therefore, for a fair comparison,\nwe use 128 C4 instances of length 2048 for both Wanda and\nForeSight.\nImplementation Details\nTo assess the effectiveness and\ngenerality of our methods, We carry out a series of exper-\niments on the LLaMA families [Touvron et al., 2023] and\nQwen families [Bai et al., 2023], finetuning them using Low-\nRank Adaptation (LoRA) with a rank of r = 8.\nLoRA\nadapters are attached to every weight in the models except\nthe last linear layer. Finetuning hyperparameters are consis-\ntently set as follows: global batch size of 16, training for\n3 epochs, learning rate of 1 × 10−4, weight decay of 0.01,\nand the torch Adam optimizer.\nTokenization remains un-\nchanged from the original Huggingface implementation. All\nfinetuning and pruning were conducted on 4 NVIDIA A100\n"}, {"page": 5, "text": "Table 1: Evaluation in the Health domain. Best scores are bold, performance surpassing dense baselines are starred*.\nMethods\nHarrison\nQA\nNLI\nHealthQuestion Summarization\nRel.%↑\nPerplexity\nMedNLI Acc.\nMedNLI F1\nPubMedQA Acc.\nPubMedQA F1\nR1\nR2\nRL\nLLaMA2-7B (sparsity 0.5)\nDense + LoRA\n5.53\n68.05\n60.22\n73.16\n43.98\n29.52\n10.63\n26.77\n-\nD-Pruner\n6.74\n61.88\n–\n–\n32.58\n36.49\n13.71\n31.85\n94.57\nATP\n9.53\n70.51\n–\n–\n42.06\n29.66\n10.36\n27.38\n81.38\nLoRA + Wanda\n6.22\n64.25\n41.46\n64.60\n47.31*\n25.86\n15.69\n23.19\n93.29\nForeSight\n6.21\n72.86*\n54.48\n69.65\n49.30*\n27.81\n9.27\n25.52\n99.41\nForeSight+PBS\n6.21\n59.70\n58.21\n66.40\n47.22*\n27.92\n9.38\n24.65\n94.83\nLLaMA2-7B (sparsity 0.4)\nATP\n8.47\n71.52\n–\n–\n44.60\n31.33\n11.15\n28.21\n85.73\nLoRA + Wanda\n5.82\n66.09\n62.18\n61.70\n44.02*\n26.94\n9.26\n24.16\n94.87\nForeSight\n5.82\n65.21\n57.66\n71.95\n47.84*\n29.10\n10.04\n26.11\n99.10\nForeSight+PBS\n5.74\n74.51*\n74.83*\n71.95\n51.08*\n30.93*\n11.30*\n27.74*\n106.6\nLLaMA3.1-8B (sparsity 0.5)\nDense + LoRA\n6.49\n68.28\n68.17\n73.60\n52.12\n27.98\n10.51\n25.04\n-\nATP\n13.94\n68.57\n–\n–\n36.72\n27.91\n9.70\n24.71\n82.44\nLoRA + Wanda\n7.93\n74.77*\n56.36\n71.7\n50.94\n26.4\n9.61\n23.34\n96.07\nForeSight\n7.92\n76.09*\n76.46*\n71.95\n51.05\n27.84\n10.13\n23.99\n103.31\nForeSight+PBS\n8.16\n57.42\n56.34\n70.55\n49.53\n25.80\n8.85\n22.95\n89.40\nLLaMA3.1-8B (sparsity 0.4)\nATP\n12.48\n75.32\n–\n–\n43.09\n29.86\n11.16\n27.01\n88.62\nLoRA + Wanda\n7.08\n72.66*\n54.97\n73.85\n52.12\n26.52\n9.95\n23.22\n96.29\nForeSight\n7.08\n69.03*\n63.63\n75.00*\n52.90*\n26.32\n9.42\n23.45\n98.06\nForeSight+PBS\n7.14\n67.88\n68.21*\n73.92*\n52.24*\n28.20*\n9.90\n25.45*\n99.80\nGPUs with 80GB memory, using the Huggingface peft li-\nbrary for LoRA training. Models were loaded and trained in\nfloat16 precision, with gradient checkpointing enabled to re-\nduce memory consumption. Both dense and Wanda baseline\nare finetuned using this setup.\nEvaluation\nMetrics\nWe\nmeasure\nlanguage\nmodeling\nperformance using perplexity computed on 300 paragraphs\nfrom InternalMed Harrison for the Health domain\nand 300 paragraphs from en legislation from Mul-\ntiLegalPile for the Legal domain.\nFor the QA and NLI\ntasks, we report accuracy and F1 scores specifically on the\nPubMedQA, CaseHold, MedNLI, and ContractNLI datasets.\nFor summarization tasks, the evaluation involves ROUGE-1,\nROUGE-2, and ROUGE-L scores on the HQS and Bill-\nsum datasets.\nModel inference settings remain consistent\nacross evaluations, with top k=50, top p=0.9, and\ntemperature=0.9. The final reported metrics are aver-\naged over four separate runs to ensure result robustness. We\nuse the Huggingface datasets and evaluate libraries to\ncompute ROUGE scores for summarization tasks. A detailed\ndescription of the evaluation datasets is provided in the\nAppendix. To enable a fair comparison with domain pruning\nmethods trained on the same datasets but under different\nfinetuning settings, we include a metric of relative perfor-\nmance, following the protocol proposed by ATP. This metric\nquantifies the performance of a pruned finetuned model rel-\native to its dense counterpart under an equivalent finetuning\nbudget. The relative performance is computed as: Rel. % =\n\u0012\n1\nn\nPn\ni=1\nAccpruned\ni\nAccdense\ni\n+ 1\nn\nPn\ni=1\nF1pruned\ni\nF1dense\ni\n+ 1\n3\nP3\nj=1\nROUGEpruned\nj\nROUGEdense\nj\n\u0013\nn is the number of evaluation tasks, and Accpruned\ni\n, Accdense\ni\n,\nF1pruned\ni\n, and F1dense\ni\ndenote the accuracy and F1 score of the\npruned and dense models on task i. The terms ROUGEpruned\nj\nand ROUGEdense\nj\ncorrespond to the ROUGE-1, ROUGE-2,\nand ROUGE. All evaluations are conducted on one NVIDIA\nA100 GPU with 80GB of memory with model loaded at float\n16 precision.\n5.1\nBaselines\nDense + LoRA\nWe finetune the LLaMA and Qwen model\nfamilies using the LoRA configuration described above; re-\nsults for the Qwen models are provided in the Appendix.\nLoRA + Wanda\nBased on the findings in Section 3, we ap-\nply Wanda [Sun et al., 2023] after LoRA finetuning, which\nyields the strongest performance among the considered base-\nlines. All domain-specific training datasets, LoRA finetuning\nsettings, and evaluation metrics follow Section 5.\nD-Pruner and ATP.\nWe also compare EfficientXpert\nwith two state-of-the-art domain pruning methods:\nD-\nPruner [Zhang et al., 2024] and ATP [Lu et al., 2024]. Since\nour evaluation spans a broader range of tasks and datasets\nthan prior work, we report previously published results as-is\nand compare only relative performance scores, which provide\na fair basis for cross-setting comparisons.\n5.2\nMain Results\nHealth Domain\nIn the health domain, EfficientXpert consistently outperforms\nprior pruning baselines, including Wanda, ATP, and D-Pruner.\nAcross model sizes and sparsity levels, EfficientXpert con-\nsistently outperforms existing methods in preserving harri-\nson perplexity and HQS summarization performance. At a\nsparsity level of 0.4, EfficientXpert augmented with the Fore-\nSight mask and Partial Brain Surgeon (PBS) attains rela-\ntive performance of 106.6% on LLaMA2-7B and 99.80% on\n"}, {"page": 6, "text": "Table 2: Evaluation in the Legal domain. Best pruned scores are bold, performance surpassing dense baselines are starred*.\nMethods\nMultiLegalPile\nQA\nNLI\nBillsum Summarization\nRel.%↑\nPerplexity\nContractNLI Acc.\nContractNLI F1\nCaseHold Acc.\nCaseHold F1\nR1\nR2\nRL\nLLaMA2-7B (sparsity 0.5)\nDense + LoRA\n3.84\n75.84\n72.50\n70.62\n68.19\n32.71\n13.93\n18.30\n-\nD-Pruner\n2.73\n–\n–\n–\n27.58\n31.00\n19.03\n25.96\n93.53\nATP\n3.67\n–\n–\n–\n–\n43.18\n23.12\n30.06\n81.99\nLoRA + Wanda\n4.35\n53.60\n46.73\n41.33\n41.15\n33.31*\n11.50\n17.79\n69.57\nForeSight\n4.10\n71.37\n68.38\n63.5\n63.46\n34.11*\n11.94\n18.43*\n93.65\nForeSight+PBS\n4.02\n69.61\n67.12\n63.5\n63.49\n33.94*\n12.04\n18.43*\n92.86\nLLaMA2-7B (sparsity 0.4)\nATP\n2.82\n–\n–\n–\n–\n43.8\n23.12\n30.06\n91.23\nLoRA + Wanda\n4.09\n57.62\n49.90\n46.12\n46.35\n35.33*\n11.83\n18.67*\n75.28\nForeSight\n3.96\n77.4*\n74.99*\n68.5\n68.50\n37.65*\n13.81\n19.94*\n100.52\nForeSight+PBS\n3.88\n74.13\n70.35\n70.5\n70.36*\n37.47*\n13.61\n19.91*\n100.96\nLLaMA3.1-8B (sparsity 0.5)\nDense + LoRA\n4.44\n76.14\n74.97\n82.5\n68.65\n33.64\n15.58\n19.66\n–\nATP\n4.28\n–\n–\n–\n–\n43.1\n22.6\n28.65\n82.44\nLoRA + Wanda\n5.35\n50.18\n39.60\n41.5\n41.58\n35.05*\n12.83\n19.11\n64.83\nForeSight\n5.32\n72.37\n71.27\n67.5\n67.06\n37.81*\n14.62\n20.56*\n94.73\nForeSight+PBS\n5.38\n74.38\n71.94\n68\n67.71*\n35.03*\n13.24\n19.52\n94.61\nLLaMA3.1-8B (sparsity 0.4)\nATP\n4.13\n–\n–\n–\n–\n45.28\n25.45\n30.15\n88.62\nLoRA + Wanda\n4.83\n59.46\n48.30\n62.5\n62.28\n35.54*\n14.05\n20.16*\n81.69\nForeSight\n4.85\n75.60\n74.29\n73.15\n72.94*\n35.38*\n14.08\n19.86*\n98.43\nForeSight+PBS\n4.82\n77.05*\n74.97*\n64\n63.48\n35.77*\n14.18\n20.23*\n94.26\nTable 3: Qwen 8B Health EfficientXpert Runtime and Memory.\nPrune%\nTotal Runtime\nPeak Memory\nHealth ppl\n–\n74 mins\n67949.30 MiB\n8.4086\n40%\n94 mins\n71239.47 MiB\n8.0113\n50%\n94 mins\n67949.30 MiB\n8.7747\nLLaMA3.1-8B, exceeding the dense baseline. At a higher\nsparsity level of 0.5, EfficientXpert with ForeSight alone re-\nmains robust, achieving 99.41% (LLaMA2-7B) and 103.31%\n(LLaMA3.1-8B) relative performance. Moreover, Efficien-\ntXpert maintains a larger margin over Wanda as sparsity in-\ncreases, as shown in Figure 3(a).\nLegal Domain\nNotably, EfficientXpert (with or without PBS) consistently\noutperforms domain pruning baselines such as ATP and D-\nPruner in relative performance across all evaluated sparsity\nlevels and model scales. Moreover, EfficientXpert exceeds\nthe general pruning method Wanda by a substantial margin\non every task at every sparsity level (Figure 3(b)), demon-\nstrating stronger adaptation to domain-specific objectives. At\np = 0.4 on LLaMA2-7B, EfficientXpert even surpasses the\ndense baseline: ForeSight achieves 100.54% relative perfor-\nmance, while ForeSight+PBS reaches 100.96%. Finally, we\nobserve that EfficientXpert is particularly effective at preserv-\ning summarization quality and low perplexity.\n5.3\nEfficiency Analysis\nTables 4 and 3 report end-to-end wall-clock runtime, average\npeak GPU memory, and in-domain perplexity for Qwen 8B\nadapted through EfficientXpert pipeline (ForeSight Mask +\nTable 4: Qwen 8B Legal EfficientXpert Runtime and Memory.\nPrune%\nTotal Runtime\nPeak Memory\nLegal ppl\n–\n106 mins\n45728.68 MiB\n5.1969\n40%\n125 mins\n46277.05 MiB\n5.4110\n50%\n125 mins\n45990.94 MiB\n5.8528\nPBS). Across both the legal and health domains, Efficien-\ntXpert incurs only modest additional finetuning time rel-\native to dense LoRA (e.g., 125 vs. 106 minutes on Le-\ngal, ∼18% overhead) while maintaining essentially the same\npeak memory footprint (within ∼1% when averaged over\nfour GPUs).\nThese observations align with the complex-\nity analysis in Sec. 4.4 and suggest that EfficientXpert de-\nlivers inference-efficient sparse domain models without the\nsubstantial training-time or memory costs typical of prior\ndomain-pruning approaches such as ATP and D-Pruner.\n5.4\nAblation\nHigher LoRA rank improves Partial Brain Surgeon\nWhen the recovery rank (r) is small relative to the number of\npruned entries |Si|, the reconstruction has limited degrees of\nfreedom and may not fully compensate for pruning-induced\nerrors. Increasing (r) expands the expressive capacity of PBS,\nenabling more accurate recovery, especially at higher sparsity\nlevels, where the number of pruned entries grows. We evalu-\nate this effect by varying (r) at 50% sparsity on Llama-3.2-1B.\nFigure 3 shows a clear and consistent trend: larger ranks yield\nhigher relative performance.\n"}, {"page": 7, "text": "0.4\n0.5\n0.6\n(a) Sparsity\n50\n60\n70\n80\n90\n100\n110\nRel. Performance (%)\nHealth Performance vs. Sparsity\nWanda\nForeSight Mask\nForeSight + PBS\n0.4\n0.5\n0.6\n(b) Sparsity\n50\n60\n70\n80\n90\n100\n110 Legal Performance vs. Sparsity\nWanda\nForeSight Mask\nForeSight + PBS\nr = 8\nr = 16\nr = 32\nr = 64\n(c) LoRA Rank\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\nRelative performance (%)\n77.69\n76.25\n74.25\n79.06\n74.22\n78.47\n76.18\n79.22\nRelative performance by rank (r)\nForeSight\nForeSight + PBS\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n(d) Grassmann Distance\n0\n5\n10\n15\n20\n25\n30\n35\n40\nFrequency\nHistogram of Grassmann Distances\nHealth vs. PubMedQA\nCasehold vs. PubMedQA\nLegal vs. Casehold\nLegal vs. Health\nFigure 3: (a–b) LLaMA2-7B relative performance vs. sparsity on Health and Legal for Wanda, FORESIGHT, and FORESIGHT+PBS. (c)\nPost-pruning Rel. Performance as a function of LoRA rank r. (d) Grassmann-distance histograms contrasting task shifts with domain shifts.\n0\n500\n1000\n1500\n2000\n2500\n(a) Global Step\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nEntropy\nTrain Entropy\ndense\nWanda\nForeSight\nForeSight_PBS\n0\n500\n1000\n1500\n2000\n2500\n(b) Global Step\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nLoss\nTrain Loss\ndense\nWanda\nForeSight\nForeSight_PBS\n0.96\n0.98\n1.00\n1.02\n(c) Rel. Perf\n0\n5\n10\n15\n20\n25\nDensity\nForeSight: Harrison vs C4\nHarrison\nC4\n0.96\n0.98\n1.00\n1.02\n1.04\n(d) Rel. Perf\n0\n5\n10\n15\n20\n25\n30\n35\nDensity\nWanda: Harrison vs C4\nHarrison\nC4\nFigure 4: (a) Training entropy on Qwen3-8B (Health): dense LoRA (and LoRA+Wanda) exhibits a late-stage entropy collapse, while FORE-\nSIGHT and FORESIGHT+PBS maintain higher entropy throughout finetuning. (b) Training loss on Qwen3-8B (Health). (c–d) Domain-\ncalibration ablation on Qwen3-0.6B at 30% sparsity: relative-performance distributions over 30 seeds on health tasks largely overlap when\ncalibrating on Harrison (health) versus C4 (general) for (c) FORESIGHT and (d) Wanda.\nDomain Calibration Does Not Improve Pruning\nWhile\ncalibration data can materially affect pruning outcomes in\ngeneral-purpose settings [Williams and Aletras, 2023; Ban-\ndari et al., 2024; Ji et al., 2024; Mitra et al., 2024], Fig-\nures 4(c) and (d) show that, at 30% sparsity, using a health\ndomain calibration set (Harrison) versus a general-domain\nset (C4) produces nearly identical distributions of relative\nperformance on our health domain evaluation tasks for both\nForeSight and Wanda. To control for confounders, we apply\neach pruning method to the same fully finetuned Qwen3-0.6B\nmodel and use matched calibration budgets across datasets\n(same number of sequences and context length). Across 30\nrandom seeds, the histograms largely overlap with no consis-\ntent shift favoring Harrison, suggesting that domain-specific\ncalibration provides no clear benefit in this setting.\nLoss–Entropy Balance\nTo explain the gains in health sum-\nmarization and perplexity, we analyze Qwen3-8B training dy-\nnamics. In Fig. 4(b), FORESIGHT+PBS closely follows the\ndense-LoRA loss curve, indicating that pruning and recov-\nery do not disrupt optimization. In Fig. 4(a), dense LoRA\n(and LoRA+Wanda) shows a late-stage entropy collapse,\nwhereas FORESIGHT, especially FORESIGHT+PBS, main-\ntains higher entropy throughout finetuning.\nThis indicates\na less over-confident output distribution, consistent with im-\nproved generation quality while preserving strong in-domain\nperplexity.\nDomains Matter More Than Tasks.\nGiven the perfor-\nmance gap we observe when transferring general purpose\npruning across domains, we ask whether pruning strategies\nshould be organized by task type or by domain. We probe\nthis question by comparing the geometry of LoRA adapters.\nFor each layer, we compute Grassmann distances between the\n8-dimensional leading eigenspaces (i.e., the learned low-rank\nadapter subspaces) for both cross-domain and within-domain\nmodel pairs, evaluated at the corresponding weight matrix.\nAs shown in Fig. 3(d), task similar but cross domain pairs,\nLegal versus Health and CaseHold versus PubMedQA, con-\ncentrate at noticeably larger distances, while task diverse but\nintra domain pairs, Legal versus CaseHold and Health ver-\nsus PubMedQA, cluster at smaller distances. This separation\nindicates that domain identity, rather than task type, more\nstrongly determines the subspace directions emphasized by\nfinetuning, reinforcing the need for domain aware pruning\nstrategies.\n6\nConclusion\nWe introduce EfficientXpert,\na domain-aware pruning\nframework that transforms general-purpose LLMs into\nsparse, domain-specialized experts with minimal overhead.\nBy integrating the propagation-aware ForeSight Mask and the\nlightweight Partial Brain Surgeon update into LoRA finetun-\ning, EfficientXpert enables a unified, end-to-end pruning and\nadaptation process. Our experiments in the health and legal\ndomains show that EfficientXpert consistently outperforms\nexisting pruning baselines across all sparsity levels, achiev-\ning performance comparable to the dense baseline at 40%\nand 50% sparsity. Moreover, EfficientXpert incurs only mod-\nest additional finetuning time relative to dense LoRA while\nmaintaining essentially the same peak memory footprint.\n"}, {"page": 8, "text": "References\n[Anil et al., 2023] Rohan Anil, Andrew M Dai, Orhan Fi-\nrat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al.\nPalm 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\n[Bai et al., 2023] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu\nCui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, et al. Qwen technical report. arXiv\npreprint arXiv:2309.16609, 2023.\n[Bandari et al., 2024] Abhinav Bandari, Lu Yin, Cheng-Yu\nHsieh, Ajay Kumar Jaiswal, Tianlong Chen, Li Shen, Ran-\njay Krishna, and Shiwei Liu.\nIs c4 dataset optimal for\npruning? an investigation of calibration data for llm prun-\ning. arXiv preprint arXiv:2410.07461, 2024.\n[Dubois et al., 2023] Yann Dubois, Chen Xuechen Li, Ro-\nhan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Car-\nlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto.\nAlpacafarm: A simulation framework for methods that\nlearn from human feedback. Advances in Neural Infor-\nmation Processing Systems, 36:30039–30069, 2023.\n[Frantar and Alistarh, 2023] Elias Frantar and Dan Alistarh.\nSparsegpt: Massive language models can be accurately\npruned in one-shot. In International conference on ma-\nchine learning, pages 10323–10337. PMLR, 2023.\n[Guo et al., 2025] Haojie Guo, Junyu Gao, and Yuan Yuan.\nEnhancing low-rank adaptation with recoverability-based\nreinforcement pruning for object counting. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 39, pages 3238–3246, 2025.\n[Han et al., 2015] Song Han, Huizi Mao, and William J\nDally. Deep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huffman cod-\ning. arXiv preprint arXiv:1510.00149, 2015.\n[Hu et al., 2022] Edward J Hu, Yelong Shen, Phillip Wallis,\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large\nlanguage models. ICLR, 1(2):3, 2022.\n[Ji et al., 2024] Yixin Ji, Yang Xiang, Juntao Li, Qingrong\nXia, Ping Li, Xinyu Duan, Zhefeng Wang, and Min Zhang.\nBeware of calibration data for pruning large language\nmodels. arXiv preprint arXiv:2410.17711, 2024.\n[Jin et al., 2019] Qiao Jin, Bhuwan Dhingra, Zhengping Liu,\nWilliam W Cohen, and Xinghua Lu.\nPubmedqa:\nA\ndataset for biomedical research question answering. arXiv\npreprint arXiv:1909.06146, 2019.\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Prefix-\ntuning: Optimizing continuous prompts for generation.\narXiv preprint arXiv:2101.00190, 2021.\n[Lu et al., 2024] Lei Lu, Zhepeng Wang, Runxue Bao,\nMengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang,\nJie Xu, Yanzhi Wang, and Shangqian Gao. All-in-one tun-\ning and structural pruning for domain-specific llms. arXiv\npreprint arXiv:2412.14426, 2024.\n[Mitra et al., 2024] Pallavi Mitra, Gesina Schwalbe, and\nNadja Klein. Investigating calibration and corruption ro-\nbustness of post-hoc pruned perception cnns: An image\nclassification benchmark study.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3542–3552, 2024.\n[Park et al., 2020] Sejun Park, Jaeho Lee, Sangwoo Mo,\nand Jinwoo Shin.\nLookahead:\nA far-sighted alter-\nnative of magnitude-based pruning.\narXiv preprint\narXiv:2002.04809, 2020.\n[Qin et al., 2018] Zhuwei Qin, Fuxun Yu, Chenchen Liu,\nand Xiang Chen. How convolutional neural network see\nthe world-a survey of convolutional neural network visual-\nization methods. arXiv preprint arXiv:1804.11191, 2018.\n[Song et al., 2025] Zirui Song, Bin Yan, Yuhan Liu, Miao\nFang, Mingzhe Li, Rui Yan, and Xiuying Chen. Injecting\ndomain-specific knowledge into large language models: a\ncomprehensive survey. arXiv preprint arXiv:2502.10708,\n2025.\n[Sun et al., 2023] Mingjie Sun, Zhuang Liu, Anna Bair, and\nJ Zico Kolter. A simple and effective pruning approach for\nlarge language models. arXiv preprint arXiv:2306.11695,\n2023.\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin\nStone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, et al.\nLlama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n[Williams and Aletras, 2023] Miles Williams and Nikolaos\nAletras.\nOn the impact of calibration data in post-\ntraining quantization and pruning.\narXiv preprint\narXiv:2311.09755, 2023.\n[Wu et al., 2025a] Dongyue Wu, Zilin Guo, Li Yu, Nong\nSang, and Changxin Gao. Structural pruning via spatial-\naware information redundancy for semantic segmentation.\nIn Proceedings of the AAAI Conference on Artificial Intel-\nligence, volume 39, pages 8368–8376, 2025.\n[Wu et al., 2025b] Zimeng Wu, Jiaxin Chen, and Yunhong\nWang. Unified knowledge maintenance pruning and pro-\ngressive recovery with weight recalling for large vision-\nlanguage models. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 39, pages 8550–8558,\n2025.\n[Yang et al., 2023] Hongyang Yang, Xiao-Yang Liu, and\nChristina Dan Wang. Fingpt: Open-source financial large\nlanguage models. arxiv. arXiv preprint arXiv:2306.06031,\n2023.\n[Zhang et al., 2023] Mingyang Zhang, Hao Chen, Chun-\nhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan\nZhuang. Loraprune: Pruning meets low-rank parameter-\nefficient fine-tuning. ICLR, 2023.\n[Zhang et al., 2024] Nan Zhang, Yanchi Liu, Xujiang Zhao,\nWei Cheng, Runxue Bao, Rui Zhang, Prasenjit Mitra, and\nHaifeng Chen. Pruning as a domain-specific llm extractor.\narXiv preprint arXiv:2405.06275, 2024.\n"}, {"page": 9, "text": "[Zheng et al., 2021] Lucia Zheng, Neel Guha, Brandon R\nAnderson, Peter Henderson, and Daniel E Ho. When does\npretraining help? assessing self-supervised learning for\nlaw and the casehold dataset of 53,000+ legal holdings.\npages 159–168, 2021.\n[Zhou et al., 2024] Zhi Zhou, Jiang-Xin Shi, Peng-Xiao\nSong, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe Guo, and\nYu-Feng Li.\nLawgpt:\nA chinese legal knowledge-\nenhanced\nlarge\nlanguage\nmodel.\narXiv\npreprint\narXiv:2406.04614, 2024.\nA\nA. An illustration of error propagation\nIllustrative Example. Consider the following matrix multi-\nplication sequence involving two consecutive linear layers:\n[3\n6]\n| {z }\nX\n·\n\u0014\n2\n2\n4\n1\n\u0015\n| {z }\nW1\n·\n\u0014\n4\n4\n8\n1\n\u0015\n| {z }\nW2\n= [192\n115] .\nThe local loss is defined as ∥X · (M ⊙W1 −W1)∥2\nF , and\nthe downstream loss as ∥X · (M ⊙W1 −W1) · W2∥2\nF , where\nM is the binary mask. Now consider pruning two different\nweights in W1.\nPruning entry W1,1 = 2 (Mask 1) results in a local loss of\n36 and a downstream loss of 1152. In contrast, pruning entry\nW2,2 = 1 (Mask 2) yields the same local loss of 36, but a\nsignificantly larger downstream loss of 2340. This toy exam-\nple illustrates that pruning decisions which are indistinguish-\nable under local error metrics may lead to dramatically dif-\nferent downstream errors due to error propagation. As layer\ndepth increases, this effect compounds, motivating the need\nfor pruning strategies that can foresee such interactions.\nB\nB. ForeSight for Attention and MLP\nForeSight extends to self-attention by pruning the query (Q)\nand key (K) projections jointly. Throughout, let X ∈Rl×d\ndenote the flattened input activations of a calibration batch\n(l = b×s, where b is the batch size and s the per-sample\nsequence length).\nWeights and adapters.\n• Base projections: W Q, W K ∈Rd×dk and W V ∈Rd×dv.\n• LoRA adapters: BQ, BK ∈Rd×r, AQ, AK ∈Rr×dk and\nBV , AV for the value path.\nThe effective projections are f\nW Q = W Q + BQAQ and\nf\nW K = W K + BKAK.\nTwo-layer interaction.\nThe attention “kernel” observed by\nthe softmax is\nX f\nW Q f\nW K⊤X⊤.\nMasking the Q projection.\nWith a binary mask M Q ∈\n{0, 1}d×dk and sparsity constraint ∥M Q∥0 ≤k, ForeSight\nselects the k coordinates whose removal minimizes the for-\nward error\nmin\nM Q\n\r\rX [f\nW Q −M Q⊙f\nW Q] f\nW K⊤\r\r2\nF .\nA second-order approximation yields the importance score\n∆Lij ∝\n\f\ff\nW Q\nij\n\f\f \r\rf\nW K\nj,:\n\r\r\n2\n\r\rX:,i\n\r\r\n2.\nMasking the K projection.\nAn analogous problem is\nsolved for M K ∈{0, 1}d×dk:\nmin\nM K\n\r\rf\nW Q [f\nW K −M K ⊙f\nW K]⊤X⊤\r\r2\nF ,\n∥M K∥0 ≤k,\nwith score\n∆Lij ∝\n\f\ff\nW K\nij\n\f\f \r\rf\nW Q\n:,j\n\r\r\n2\n\r\rX:,i\n\r\r\n2.\nThese propagation-aware scores prune Q and K in one\npass, ensuring that early-layer sparsity respects downstream\nattention dynamics.\n"}, {"page": 10, "text": "Masking the output projection O.\nThe attention output\nis projected back to the residual stream by f\nW O = W O +\nBOAO ∈Rd×d. Its output is consumed by the MLP input of\nboth the gate and up projections. We therefore weight each\noutput coordinate by the average downstream sensitivity\nνO\nu\n:=\n1\n2\n\u0010\r\rf\nW gate\n:,u\n\r\r\n2 +\n\r\rf\nW up\n:,u\n\r\r\n2\n\u0011\n,\nu ∈[d],\nand score each entry of f\nW O by\n∆LO\nuv ∝\n\f\ff\nW O\nuv\n\f\f \r\rX:,v\n\r\r\n2 νO\nu .\nThis matches the implementation |f\nW O| ⊙Xhat ⊙νO (with\nbroadcasting).\nMasking MLP gate/up projections.\nFor the gated MLP,\nlet f\nW gate, f\nW up ∈Rdff ×d and f\nW down ∈Rd×dff denote the\neffective (projection + LoRA) weights. Both gate and up out-\nputs are mapped back by f\nW down, so we use the per-hidden-\nunit propagation norm\nνdown\nh\n:=\n\r\rf\nW down\n:,h\n\r\r\n2,\nh ∈[dff].\nIgnoring elementwise nonlinearities (Appendix C), ForeSight\nassigns the importance scores\n∆Lgate\nhi\n∝\n\f\ff\nW gate\nhi\n\f\f \r\rX:,i\n\r\r\n2 νdown\nh\n,\n∆Lup\nhi ∝\n\f\ff\nW up\nhi\n\f\f \r\rX:,i\n\r\r\n2 νdown\nh\n.\nC\nC. Derivation of ForeSight importance\nScore\nLet X ∈Rm×d be a fixed data matrix and let U1 = W1 +\nB1A1 ∈Rd×h, U2 = W2 + B2A2 ∈Rh×p be the (affine,\nbias-free) effective weight matrices of two consecutive linear\nlayers. Define the loss\nL(U1) =\n\r\rXU1U2\n\r\r2\nF = Tr\n\u0000U⊤\n2 U⊤\n1 X⊤X U1U2\n\u0001\n.\nFix an entry θ = U1[i, j]. We study the loss change ∆Lij\nobtained by setting this entry to zero while keeping all other\nparameters constant. Throughout we assume (i) X and U2\nare constant during the perturbation, (ii) U1 corresponds to a\nfirst-order stationary point of L (i.e. ∇U1L = 0), and (iii)\nhigher–than–second-order terms in a Taylor expansion are\nnegligible for the magnitude of the perturbation considered.\nWrite Eij = eie⊤\nj , so that θEij extracts the affected entry.\nA standard matrix-calculus identity gives\n∂L\n∂U1\n= 2 X⊤X U1\n\u0000U2U⊤\n2\n\u0001\n,\n(5)\n=⇒\n∂L\n∂θ = 2 e⊤\ni X⊤X U1\n\u0000U2U⊤\n2\n\u0001\nej .\n(6)\nBecause U1 depends linearly on θ only through the i–th\nrow, differentiation once more yields the diagonal Hessian\nentry\n∂2L\n∂θ2 = 2 e⊤\ni X⊤X EijU2U⊤\n2 ej = 2 (X⊤X)ii (U2U⊤\n2 )jj.\nLetting δθ = −θ denote the pruning perturbation, a second-\norder Taylor expansion gives\n∆Lij = L\n\u0000U1 + δθ Eij\n\u0001\n−L(U1)\n= ∂L\n∂θ\n|{z}\n=0\nδθ + 1\n2\n∂2L\n∂θ2 (δθ)2 + O\n\u0000∥δθ∥3\u0001\n≈1\n2 θ2 (X⊤X)ii (U2 U⊤\n2 )jj .\n(7)\nHence, under the stated assumptions, the exact second-order\napproximation to the loss increase is non-negative and is\ngiven by\n∆Lij ≈\n1\n2 θ2\nij (X⊤X)ii (U2U⊤\n2 )jj\n(i∈[d], j ∈[h]).\nThis value serves as a principled importance score for weight\npruning: the larger the product of the data–covariance en-\ntry (X⊤X)ii, the downstream energy (U2U⊤\n2 )jj, and the\nsquared weight magnitude θ2\nij, the greater the predicted loss\nincurred by zeroing θij.\nNonlinearities in ForeSight can be safely ignored.\nIn a\npreLayerNorm model, calibration pre-activations a is approx-\nimately standard normal, and the exact GELU has a slope in\n[-0.129, 1.129]. Thus, GELU only reweights token contri-\nbutions by bounded factors, so our group selection by mag-\nnitudes is stable except for near ties. Empirically, the top k\noverlap between pre-activation and post-activation scoring is\nat least 90%.\nD\nD. Training Dataset Construction\nThe domain training corpora are listed in Tables 5 and 6. For\neach domain, we stratify the original training split by label,\nshuffle with a fixed seed (1234), and subsample to the desired\nsize. In PUBMEDQA, we retain only the yes and no classes\nbecause the released training set contains no maybe examples.\nFor all other datasets, labels with too few instances are up-\nsampled by sampling with replacement until the target count\nis reached. All training data and evaluation scripts follow the\nAlpaca template [Dubois et al., 2023], with the full prompt\nformats listed in Appendix ??.\nTable 5: Health domain training datasets with label distributions,\nshuffle settings, random seed, and sequence length.\nDataset\nLabel\nInstances Shuffle Seed Seq. Len.\nMedNLI\nentailment\n2333\nTRUE\n1234\n2048\ncontradiction\n2333\n1234\n2048\nneutral\n2334\n1234\n2048\nPubMedQA\nyes\n3500\nTRUE\n1234\n2048\nno\n3500\n1234\n2048\nmaybe\n0\n1234\n2048\nHQS\n–\n1000\nTRUE\n1234\n2048\nTwo domain-specific datasets are used in our experiments:\nPubMedQA and CaseHold. For PubMedQA, we sample\n7,000 yes and 7,000 no examples; the maybe label is excluded\ndue to lack of availability in the training split. For CaseHold,\nwe sample 3,000 instances from each class label {0, 1, 2, 3,\n4}. All dataset splits are shuffled once using a fixed seed\n"}, {"page": 11, "text": "Table 6: Legal domain training datasets with label distributions,\nshuffle settings, random seed, and sequence length.\nDataset\nLabel\nInstances Shuffle Seed Seq. Len.\nContractNLI\nentailment\n2100\nTRUE\n1234\n2048\ncontradiction\n1200\n1234\n2048\nneutral\n2700\n1234\n2048\nCaseHold\n0\n1400\nTRUE\n1234\n2048\n1\n1400\n1234\n2048\n2\n1400\n1234\n2048\n3\n1400\n1234\n2048\n4\n1400\n1234\n2048\nBillSum\n–\n1000\nTRUE\n1234\n2048\nE\nE. Hyperparameters Settings\nWe provide all the hyperparameter used for training in Ta-\nble 1, Table 2, and the two task specific domain model used in\nSection ??. For all dense Health domain models, we use train-\ning dataset described in Table 5 and the evaluation datasets\ndescribed in Table 11. For all dense Legal domain models,\nwe use training dataset described in Table 6 and the evaluation\ndatasets described in Table 12. The training hyperparameter\nare set the be the same for PubMedQA model and CaseHold\nmodel with training datasets described in previous section. To\nnote that all training are conducted on 4 NVIDIA A100 GPUs\nwith 80GB memory. We adjust the per-device batch size and\ngradient accumulation steps to maintain a global batch size of\n32.\nTable 7: Dense Health Domain Training Hyperparameters\nHyperparameter\nValue\nmodel name\nmeta-llama/Llama-2-7b-hf\nlearning rate\n1 × 10−4\nper device batch size\n2\ngradient accumulation steps\n4\nnum train epochs\n3\nweight decay\n0.01\nfp16\nTrue\nsave steps\n1000\nmax seq length\n2048\nlora r\n8\nlora alpha\n16\nlora dropout\n0\nlora bias\nnone\nlora target modules\nq proj, o proj,\nv proj,\nk proj, gate proj,\nup proj, down proj\noptimizer\ntorch Adam\nThe LoRA training hyperparameter and training dataset\nused for EfficientXpert are identical to those used for the\ndense model to ensure a fair comparison.\nFor ForeSight\nMask, the mask learning rate is set to 0.5. For Partial Brain\nSurgeon, the regularization coefficient is set to 1 × 10−8.\nF\nF. Evaluation Details\nThe name of the evaluation and number of instances in the\ntest set are listed in Table 11 and Table 12. For all the evalu-\nTable 8: Dense Health Domain Training Hyperparameters\nHyperparameter\nValue\nmodel name\nmeta-llama/Llama-2-7b-hf\nlearning rate\n1 × 10−4\nper device batch size\n2\ngradient accumulation steps\n4\nnum train epochs\n3\nweight decay\n0.01\nfp16\nTrue\nsave steps\n1000\nmax seq length\n2048\nlora r\n8\nlora alpha\n16\nlora dropout\n0\nlora bias\nnone\nlora target modules\nq proj, o proj,\nv proj,\nk proj, gate proj,\nup proj, down proj\noptimizer\ntorch Adam\nTable 9: Dense Legal Domain Training Hyperparameters\nHyperparameter\nValue\nmodel name\nmeta-llama/Llama-2-7b-hf\nlearning rate\n1 × 10−4\nper device batch size\n2\ngradient accumulation steps\n4\nnum train epochs\n3\nweight decay\n0.01\nfp16\nTrue\nsave steps\n1000\nmax seq length\n2048\nlora r\n8\nlora alpha\n16\nlora dropout\n0\nlora bias\nnone\nlora target modules\nq proj, o proj,\nv proj,\nk proj, gate proj,\nup proj, down proj\noptimizer\ntorch Adam\nations, we adopt the Alpaca template [Dubois et al., 2023] to\nevaluate the performance of the models. All Alpaca templates\nare listed in Appendix ??.\nG\nG. Further Analysis\nThe Grassmann distance between two LoRA adapters of the\nsame weight, parameterized by (B1, A1) and (B2, A2) re-\nspectively, is given by\nd(U1, U2) =\n\u0010 r\nX\ni=1\ncos−1(σi)2\u0011 1\n2 ,\n(8)\nwhere\nB1A1 = U1 Σ1 V ⊤\n1 ,\nB2A2 = U2 Σ2 V ⊤\n2 .\n(9)\nand σi are the singular values of U ⊤\n1 U2, r = 8 for our\nexperiment.\nTo understand how is task information encoded in the\nLoRA adapter, we further analyze the projection energy of\neach domain-specific or task-diverse model introduced in\nSection ??, as shown in Figure 5. For each model, we com-\npute the projection energy of the LoRA adapters and average\n"}, {"page": 12, "text": "Table 10: Dense Legal Domain Training Hyperparameters\nHyperparameter\nValue\nmodel name\nmeta-llama/Llama-3-8b\nlearning rate\n1 × 10−4\nper device batch size\n1\ngradient accumulation steps\n8\nnum train epochs\n3\nweight decay\n0.01\nfp16\nTrue\nsave steps\n1000\nmax seq length\n2048\nlora r\n8\nlora alpha\n16\nlora dropout\n0\nlora bias\nnone\nlora target modules\nq proj, o proj,\nv proj,\nk proj, gate proj,\nup proj, down proj\noptimizer\ntorch Adam\nTable 11: Overview of health datasets used for evaluation.\nInternalMed Harrison\nMedNLI\nPubMedQA\nHQS\nDomain\nHealth\nHealth\nHealth\nHealth\nTask / Type\nGeneration\nNLI\nQA\nSummarization\n# Instances in Test\n300\n1422\n500\n100\nMetrics\nPerplexity\nAcc & Macro-F1 Acc & Macro-F1\nROUGE\nit by weight type. The plotted results reveal that the distri-\nbution of average projection energy exhibits more consistent\nlocal patterns within the same task (e.g., CaseHold vs. Le-\ngal or PubMedQA vs. Health), even when the domains differ.\nFor instance, PubMedQA and Health, although from dif-\nferent domains, both exhibit strong activation in gate proj\nand up proj. In contrast, CaseHold and Legal emphasize\nk proj and up proj. This pattern suggests that task sim-\nilarity plays a dominant role in shaping local projection\nenergy patterns, whereas domain-level information may\nguide the broader global distribution. These findings imply\nthat LoRA adapters capture a global structure governed by\ndomain, while their local energy distribution reflects task-\nspecific characteristics, especially across components like\nattention, gate, up projection in MLP.\nq_proj\nk_proj\nv_proj\no_proj\nup_proj\ndown_proj gate_proj\nFull Name\n0\n1\n2\n3\n4\n5\n6\nAverage Grassmann Distance\nAverage Grassmann Distance by Weight Type\nDomain\nCaseHold\nHealth\nLegal\nPubMedQA\nFigure 8: Grassmann distance between LoRA adapters and weight\nmatrices by weight type.\nTable 12: Overview of legal datasets used for evaluation.\nMultiLegalPile\nContractNLI\nCaseHOLD\nBillSum\nDomain\nLegal\nLegal\nLegal\nLegal\nTask / Type\nGeneration\nNLI\nQA\nSummarization\n# Instances in Test\n300\n1991\n200\n200\nMetrics\nPerplexity\nAcc & Macro-F1 Acc & Macro-F1\nROUGE\nThese findings imply that while LoRA adapters may en-\ncode a global structure shaped by domain identity, task-\nspecific characteristics drive significant variation in projec-\ntion energy across weight types—particularly within atten-\ntion and MLP components.\nTo further explore this contrast, we visualize the Grass-\nmann distance between LoRA adapters and the corresponding\nbase weight matrices for each layer. As shown in Figure 6,\nthe Grassmann distances remain consistently high (above 4)\nacross all layers and domains, indicating that LoRA adapters\nsystematically amplify directions that diverge from the pre-\ntrained model’s dominant subspaces. This pattern is mirrored\nin Figure 8, where all weight types exhibit similarly large dis-\ntances.\nThese results suggest a fundamental duality: while the\nglobal subspace shift induced by LoRA is gov-\nerned by domain-level signals— producing consistently large\nGrassmann distances—the local alignment revealed\nby projection energy varies according to task-specific usage\npatterns.\nH\nAlpaca Templates\nPubMedQA\nBelow is an instruction that\ndescribes a task related to\nhealthcare, paired with detailed\ninput from a scientific article.\nInstruction:\nYou are an expert\nclinician.\nDetermine whether the\nfollowing statement is correct.\nAnswer yes, no, or maybe.\nInput:\nContexts:\n{contexts}\nSection Labels:\n{labels}\nMeSH Terms:\n{meshes}\nQuestion:\n{question}\nResponse:\nThe answer is\nMedNLI\nBelow is an instruction for analyzing\nclinical information.\nInstruction:\nYou are a licensed\nphysician.\nDetermine the logical\nrelationship between two clinical\nstatements based only on clinical\nknowledge and the given premise.\nDo\nnot assume unstated facts.\nAnswer with exactly one label:\n- entailment:\nthe hypothesis\nmust be true given the premise -\ncontradiction:\nthe hypothesis must\nbe false given the premise - neutral:\n"}, {"page": 13, "text": "q_proj\nk_proj\nv_proj\no_proj\nup_proj down_proj gate_proj\nFull Name\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nAverage Projection Energy\nAverage Projection Energy by Weight Type\nCaseHold\nq_proj\nk_proj\nv_proj\no_proj\nup_proj down_proj gate_proj\nFull Name\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nAverage Projection Energy\nAverage Projection Energy by Weight Type\nPubMedQA\nq_proj\nk_proj\nv_proj\no_proj\nup_proj down_proj gate_proj\nFull Name\n0.00\n0.02\n0.04\n0.06\n0.08\nAverage Projection Energy\nAverage Projection Energy by Weight Type\nLegal\nq_proj\nk_proj\nv_proj\no_proj\nup_proj down_proj gate_proj\nFull Name\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nAverage Projection Energy\nAverage Projection Energy by By Weight Type\nHealth\nFigure 5: Projection energy by full name across domains.\nL0\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9 L10 L11 L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22 L23 L24 L25 L26 L27 L28 L29 L30 L31\nLayer\n0\n1\n2\n3\n4\n5\nAverage Grassmann Distance\nLayerwise Average Grassmann Distance Across Domains\nDomain\nCaseHold\nHealth\nLegal\nPubMedQA\nFigure 6: Grassmann distance between LoRA adapters and weight matrices across layers.\nthe hypothesis may be true or false\ngiven the premise\nInput:\nPremise:\n{sentence1}\nHypothesis:\n{sentence2}\nResponse:\nThe answer is\nHQS\nBelow is an instruction that\ndescribes a task related to\nhealthcare, paired with further\ncontext.\nInstruction:\nSummarize the following\nconsumer health question into a\nshorter version that preserves all\ncritical information needed to answer\nit correctly.\nKeep key entities\n(e.g., conditions, treatments,\ntests) and the main intent.\nRemove\nperipheral details.\nWrite a fluent\nsentence.\nInput:\n{input question}\nResponse:\nThe shortened question is\nCaseHold\nBelow is an instruction that\ndescribes a task related to legal\nreasoning.\nInstruction:\nYou are given a citing\npassage from a legal decision and\nfive candidate holding statements.\nSelect the holding statement that\nbest matches the citing passage.\nFocus on the specific legal rule,\nfactual framing, and implication.\nAvoid choices that are merely\ntopically similar.\nAnswer with a single integer in\n{0,1,2,3,4}.\nInput:\nCiting passage:\n{citing prompt}\n0:\n{holding 0} 1:\n{holding 1}\n2:\n{holding 2} 3:\n{holding 3} 4:\n{holding 4}\nResponse:\nThe answer is\nContractNLI\nBelow is an instruction that\ndescribes a task related to contract\nanalysis.\nInstruction:\nDetermine the\nrelationship between the contract\npremise and the hypothesis.\nAnswer\nwith exactly one label:\nentailment,\ncontradiction, or neutral.\nConsider\nall three options and base your\ndecision only on the provided text.\nInput:\nPremise:\n{sentence1}\nHypothesis:\n{sentence2}\nResponse:\nThe answer is\n"}, {"page": 14, "text": "L0\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nL13\nL14\nL15\nL16\nL17\nL18\nL19\nL20\nL21\nL22\nL23\nL24\nL25\nL26\nL27\nL28\nL29\nL30\nL31\nLayer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nAverage Projection Energy\nLayerwise Average Projection Energy\nCaseHold\nLegal\nPubMedQA\nHealth\nFigure 7: Projection energy across layers.\nBillSum\nBelow is an instruction for\nsummarizing a legislative bill.\nInstruction:\nSummarize the following\nbill by explaining its major actions,\npurposes, and effects.\nFocus on\nwhat the bill aims to do and what\nchanges it makes in practice.\nParaphrase clearly for a general\npolicy audience.\nAvoid quoting\nlong spans or referencing subsection\nnumbers unless needed.\nInput:\nBill Title:\n{title}\nBill Text:\n{input text}\nResponse:\nH.1\nEffect of Rank r on Partial Brain Surgeon\nWhen the chosen rank r is substantially smaller than the num-\nber of pruned entries |Si|, the resulting system becomes over-\nconstrained, which can degrade downstream performance. To\nmitigate this, higher sparsity levels generally require a corre-\nspondingly larger rank r.\nWe evaluated the impact of varying r at 50% sparsity on\nLlama-3.2-1B. As shown in Figure 9, increasing the rank con-\nsistently improves relative performance. Each individual met-\nric is recorded in table 13\n8\n16\n32\n64\nRank r (categorical)\n74\n75\n76\n77\n78\n79\nRelative Performance (%)\nRelative Performance vs. Rank\nForeSight\nForeSight+PBS\nFigure 9: Relative performance of FourSight and FourSight + PBS\nat 50% sparsity on Llama-3.2-1B.\nH.2\nDiscuss on the Artifacts\nThe license for the models and datasets used in this paper is\nas follows:\n• Llama-2-7b-hf:\nThe model is licensed under the\nLLaMA 2 community license.\n• Llama-3-8b:\nThe model is licensed under META\nLLaMA 3 community license.\n• PubMedQA: The dataset is licensed under MIT license.\n• MedNLI: The dataset is licensed under the Physionet\nCredentialed Health Data License Version 1.5.0.\n• HQS: The dataset is licensed under Apache License 2.0.\n• CaseHold: The dataset is licensed under Apache Li-\ncense 2.0.\n• ContractNLI: The dataset is licensed under the CC BY\n4.0 license.\n"}, {"page": 15, "text": "Table 13: Performance metrics for different methods at various ranks r at 50% sparsity on Llama-3.2-1B.\nMethod\nRank Med PPL MedNLI Acc MedNLI F1 PubMedQA Acc PubMedQA F1 HQS R1 HQS R2 HQS RL Rel%\nDense\n8\n10.288\n0.5605\n0.5464\n0.6800\n0.4714\n0.2476\n0.0896\n0.2204\n–\n16\n10.145\n0.5956\n0.5759\n0.6600\n0.4674\n0.2609\n0.0852\n0.2343\n–\n32\n10.029\n0.6069\n0.5986\n0.6660\n0.4656\n0.2507\n0.0809\n0.2284\n–\n64\n9.902\n0.5872\n0.5733\n0.6500\n0.4575\n0.2439\n0.0755\n0.2122\n–\nours\n8\n14.716\n0.4286\n0.3961\n0.5200\n0.3710\n0.2171\n0.0688\n0.1948\n77.69\n16\n14.706\n0.4286\n0.3961\n0.5200\n0.3710\n0.2171\n0.0688\n0.1948\n76.26\n32\n14.685\n0.4197\n0.3620\n0.5205\n0.3716\n0.2114\n0.0672\n0.1914\n74.26\n64\n14.834\n0.3926\n0.3921\n0.5300\n0.3972\n0.2201\n0.0693\n0.1974\n79.06\noursPBS\n8\n15.854\n0.3643\n0.3412\n0.5720\n0.4027\n0.1960\n0.0568\n0.1757\n74.22\n16\n15.020\n0.4365\n0.4252\n0.5395\n0.3845\n0.2206\n0.0637\n0.1980\n78.47\n32\n14.908\n0.3968\n0.3979\n0.5505\n0.3917\n0.2117\n0.0641\n0.1894\n76.18\n64\n15.106\n0.3954\n0.3824\n0.5490\n0.3910\n0.2227\n0.0688\n0.1998\n79.22\n"}]}