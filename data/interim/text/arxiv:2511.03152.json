{"doc_id": "arxiv:2511.03152", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.03152.pdf", "meta": {"doc_id": "arxiv:2511.03152", "source": "arxiv", "arxiv_id": "2511.03152", "title": "Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment", "authors": ["Srishti Yadav", "Jasmina Gajcin", "Erik Miehling", "Elizabeth Daly"], "published": "2025-11-05T03:19:21Z", "updated": "2025-11-05T03:19:21Z", "summary": "Understanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.03152v1", "url_pdf": "https://arxiv.org/pdf/2511.03152.pdf", "meta_path": "data/raw/arxiv/meta/2511.03152.json", "sha256": "f903e41f498f6ad2ff60369212539a974891d4d1bbd53319928c3c3388347b75", "status": "ok", "fetched_at": "2026-02-18T02:28:25.497560+00:00"}, "pages": [{"page": 1, "text": "Who Sees the Risk?\nStakeholder Conflicts and Explanatory Policies in\nLLM-based Risk Assessment\nSrishti Yadav 1*, Jasmina Gajcin 2, Erik Miehling 2, Elizabeth Daly 2\n1University of Copenhagen, Denmark\n2IBM Research, Ireland\nsrya@di.ku.dk, jasmina.gajcin2@ibm.com, erik.miehling@ibm.com, elizabeth.daly@ie.ibm.com\nAbstract\nUnderstanding how different stakeholders perceive risks in\nAI systems is essential for their responsible deployment. This\npaper presents a framework for stakeholder-grounded risk as-\nsessment by using LLMs, acting as judges to predict and ex-\nplain risks. Using the Risk Atlas Nexus and GloVE explana-\ntion method, our framework generates stakeholder-specific,\ninterpretable policies that shows how different stakehold-\ners agree or disagree about the same risks. We demonstrate\nour method using three real-world AI use cases of medi-\ncal AI, autonomous vehicles, and fraud detection domain.\nWe further propose an interactive visualization that reveals\nhow and why conflicts emerge across stakeholder perspec-\ntives, enhancing transparency in conflict reasoning. Our re-\nsults show that stakeholder perspectives significantly influ-\nence risk perception and conflict patterns. Our work empha-\nsizes the importance of these stakeholder-aware explanations\nneeded to make LLM-based evaluations more transparent, in-\nterpretable, and aligned with human-centered AI governance\ngoals.\nIntroduction\nIn recent years, unprecedented deployment of large language\nmodels (LLMs) has raised concerns about reliability, ex-\nplainability and safety of these models in real world use-\ncases. The need for responsible use of these models has led\nto increased interest in governance of AI models which en-\ncompasses standardized principles and evaluations to ensure\nthat the AI systems behave reliably, robustly and reflect soci-\netal values. In order to address safety concerns, the research\ncommunity has developed specialized safety benchmarks\nlike SafetyBench (Zhang et al. 2023), HarmBench (Mazeika\net al. 2024), SG-Bench (Mou, Zhang, and Ye 2024) to eval-\nuate models for harmful behaviors such as toxicity, halluci-\nnation, or misuse.\nIn order to bring structure and guidance when consider-\ning risks, several taxonomies and frameworks have emerged\nsuch as the Top 10 for LLMs and Generative AI Apps\n(OWASP 2024), the NIST AI Risk Management Framework\n(NIST 2023), the MIT AI Risk Repository (MIT 2024; Slat-\ntery et al. 2024), and IBM Risk Atlas Nexus (Bagehorn et al.\n2025). These risk frameworks can help link risks to inform\n*Work done as part of internship at IBM Research, Ireland\ngovernance mechanisms. Recent works have explored the\nuse of LLMs take into account the use case context in order\nto prioritise which risks are most related to the AI system\n(Eiras et al. 2025; Mylius et al. 2025; Daly et al. 2025). How-\never, most existing approaches remain stakeholder-agnostic,\noverlooking the nuanced ways in which different stakehold-\ners may perceive and prioritize risks which is an essential\nconcern for responsible governance. This gap limits the ef-\nfectiveness of governance tools in multi-stakeholder envi-\nronments.\nWe propose leveraging stakeholder specific personas as\npart of the risk prioritisation process. By considering our\nframework on stakeholder perspectives, our approach re-\nveals points of alignment and disagreement on risk assess-\nment across stakeholders for a given AI use case. This en-\nables more context-sensitive governance decisions and sup-\nports inclusive risk mitigation planning. We generate expla-\nnations for stakeholder conflicts using the GloVE pipeline\n(Gajcin et al. 2025) that allows for a more transparent and\ncontext-sensitive interpretation of LLM-as-a-Judge behav-\niors helping bridge the gap between model understanding\nand human-centered risk understanding. In our paper, we\nmake the following contributions:\n1. We propose a stakeholder driven policy explanation\npipeline to observe the policy conflicts that emerge with\ndifferent stakeholders using rule-based explanations.\n2. We identify and explain how stakeholder conflicts\nemerge on three real-world AI usecases to demonstrate\nthe practicality of the framework.\n3. We propose a tool for visualizing and interpreting these\nemerged conflicts.\nLiterature Review\nPersona in LLMs\nAssigning distinct personas to LLMs has emerged as a\nmethod to elicit different behaviours across tasks, showing\nthat outputs vary under different assumed identities. For ex-\nample, PersonaLLM (Jiang et al. 2024) showed that GPT-3.5\nand GPT-4, when assigned Big Five personality (De Raad\n2000), produce writings that aligned with those personas.\nSimilarly Hu and Collier (2024)’s work on the effect of\narXiv:2511.03152v1  [cs.CL]  5 Nov 2025\n"}, {"page": 2, "text": "Stakeholders\nAI Use Case\nDoctor\nPatient\nStakeholder Risk Profiles\nHarmful Output\nData Bias\nDoctor\nPatient\nAI medical \ndiagnosis assistant \nthat determines if a \npatient requires a \nsurgery\nConflict Explanation \n(Using GloVE)\nData Bias\nUnexplainable Output\nUnexplainable Output\nHarmful Output\nIF reviewed by a medical professional \nDESPITE contains unreliable output\nIF involves poorly thought out choices \nDESPITE involves medical review process\nnot-a-risk\nrisk\nRisk Assessment\nGenerate \nStakeholders\n+ Others\nFigure 1: Overview of our stakeholder-centered AI risk assessment pipeline. The use case (“AI medical diagnosis assistant that\ndetermines if someone needs surgery”) generates relevant stakeholders such as doctors, patients etc.. Each stakeholder under-\ngoes a risk assessment that produces individual risk profiles (e.g., harmful output, data bias, unexplainable output). The GloVE\ncomponent then generates conflict explanations, showing the conflicts that emerge between stakeholders risk perspectives\npersona in LLM simulations show that incorporating per-\nsona (e.g. demographic) variables via prompting in LLMs\nprovides modest but statistically significant improvements.\nMore recently, Dash et al. (2025) assigned 8 personas across\n4 political and socio-demographic attributes to show that\npersonas induce motivated reasoning in LLMs. These works\nindicate that inducing personas in LLMs alters LLM be-\nhaviour and can be a useful method to study outputs where\ngrounding tasks on different personas can give us useful in-\nformation about how personas are affected in those tasks.\nPrompt Robustness\nRelying on a single prompt is brittle (Li, Papay, and Klinger\n2025) but aggregating results from meaning preserving para-\nphrases can lead to better claims. Mizrahi et al. (2024) show\nthat averaging over an instruction paraphrase set (rather than\nchoosing one prompt) yields more stable scores. Wahle et al.\n(2024) showed that different paraphrase types (lexical, syn-\ntactic, morphological) elicit divergent behaviors and Ceron\net al. (2024) argued that robustness to paraphrasing should\nbe a standard reliability check alongside other perturbations.\nMeier et al. (2025) showed that distinct paraphrase types\nelicit measurably different behaviors even when semantics\nis held constant. This is particularly important when work-\ning with real world usecases. Imagine a scenario where an\nLLM is tasked to help diagnose a patient. Even if the LLMs\ngives results with highest predictions, if its results vary on\ndifferent runs, a doctor can not rely on the evaluations of\nthe LLM. Hence, prompt robustness is an important step in\nLLM evaluation to ensure the robustness of the output pre-\ndiction.\nAI Risk Taxonomies\nRisk assessment is an integral step of AI system deploy-\nment. While several risk taxonomies have been proposed\nto map out the landscape of existing risks, the research on\nrisk assessment is still quite fragmented. Weidinger et al.\n(2022) initially developed a comprehensive taxonomy of\nLLM risks, categorizing 21 distinct risk types ranging from\ndiscrimination and misinformation to malicious use and en-\nvironmental harms into 6 broad categories. Subsequently,\nSlattery et al. (2024) proposed the AI Risk Repository,\nwhich looked at 777 risk statements from 43 prior frame-\nworks and categorized them into a hierarchical structure\ninto causal and domain-specific risks. Complimentary to this\nwork, there has been an emergence in governance-oriented\ntaxonomies to operationalize AI risk management like NIST\n(NIST 2023), OWASP (OWASP 2024) and IBM AI Risk\nAtlas (Bagehorn et al. 2025). These frameworks have at-\ntempted to structure how AI risks are identified, categorized,\nand mitigated across organizational and model-governance\ndimensions to identify, measure, and mitigate technical and\nsystemic risks.\nMethodology\nOur goal is to take risk prediction and stakeholder-grounded\nexplanation of these risks (what we call policies) to ana-\nlyze how risks and their interpretations vary across different\nstakeholders in different AI use cases. We start with syn-\nthetic dataset of three real world use cases and their asso-\nciated stakeholders to ground risk predictions in the stake-\nholders. Next, we use the IBM AI Risk Atlas Nexus for\nidentifying potential risks within each stakeholder’s con-\ntext. Finally, we use the GloVE explanation framework to\nget stakeholder-specific policies that capture how different\nstakeholders reason about the same risks. The following sec-\ntions describe each of these components in detail.\nDataset Construction\nWe introduce stakeholders as personas - which are intended\nto represent real world actors that are part of the system.\nHaving these stakeholders serves two purposes: a) contextu-\nalize our pipeline such that predicted risk explanations are\ngrounded in the role of the stakeholder in the usecase and\n"}, {"page": 3, "text": "b) for us to compare and analyse the differences in explana-\ntions of the diverse stakeholders for same usecase. In this pa-\nper, we look at 3 unique base usecases synthetically gener-\nated for different domains 1) AI medical diagnosis assistant\nthat determines if someone needs surgery 2) Autonomous\nvehicle system that determines if passengers reach destina-\ntion safely, and 3) AI fraud detection that determines if cus-\ntomer transactions get blocked. For each usecase, we then\ncreate stakeholder grounded usecases. Take for example a\nusecase “AI medical diagnosis assistant that determines if\nsomeone needs surgery” with following stakeholders: Sur-\ngeons, Primary Care Physicians, Radiologists, Patients re-\nquiring surgery, Patients with chronic conditions, Patients\nwith acute injuries, Family members, Nurses and Healthcare\nadministrators. An example stakeholder grounded usecase\ncan be “surgeons using ai medical diagnosis assistant that\ndetermines if someone needs surgery”. We used a structured\napproach to generate this synthetic dataset as described be-\nlow:\n1. We start with a list of base usecases for which we want\nto analyse risks.\n2. We then use gpt-4o to generate list of stakeholders (users\nand subjects) by asking top 3 high stake users, AI im-\npacted subjects and secondary impacted subjects each.\nFor consistency, we provide the definition of stakeholder\nfrom CSIRO responsible AI (RAI) catalogue1 in our\nprompt.\n3. For each stakeholder for each usecase, we generate a\nstakeholder specific usecase by substituting these stake-\nholders into base usecase. If the stakeholder is a user,\nwe use the format: “[STAKEHOLDER] using [base use-\ncase]” and if the stakeholder is a subject, we use the for-\nmat “[base usecase] that impacts [STAKEHOLDER]”.\n4. Finally to ensure the robustness in our experiments, in-\nspired by Meier et al. (2025), we generate paraphrases for\nthese stakeholder specific usecase without changing their\nmeaning. We use 6 different linguistic transformations\na) addition/deletion b) semantic change c) same polar-\nity substitution d) punctuation change e) change of order\nf) spelling change. Some examples of these paraphrases\ncan be found in Table 1.\nDetailed prompt for generating this stakeholder specific\nusecases is provided in the Appendix in Listing 1 and\nTable 3. The goal is to take usecases grounded in these\nstakeholders and then use policy explanation pipeline on\nthem to see how explanations differ and conflicts emerge.\nRisk Assessment\nWe use IBM AI Risk Atlas (Bagehorn et al. 2025), a com-\nprehensive taxonomy of governance-related risks, as a tax-\nonomy of risks for the model to predict from, along with\nRisk Atlas Nexus (RAN) (Bagehorn et al. 2025), a risk as-\nsessment tool that uses Large Language Models (LLMs) to\ninfer risks based on any given taxonomy, as shown in our\n1https://research.csiro.au/ss/science/projects/responsible-ai-\npattern-catalogue/\nTable 1: Examples of different linguistic transformations of\na base stakeholder usecase surgeons using ai medical diag-\nnosis assistant that determines if someone needs surgery.\nTransformation\nType\nExample\nAddition / Deletion\nsurgeons\nare\nusing\nan\nai\nmedical\ndiagnosis assistant which determines\nwhether a person requires surgery\nSemantic Change\nsurgeons are utilizing an ai medical\ndiagnosis tool which assesses whether\nsurgery is necessary.\nSame Polarity Sub-\nstitution\nsurgeons utilizing ai healthcare diag-\nnostic tool that determines if someone\nneeds surgery\nPunctuation\nChange\nsurgeons are using an ai medical diag-\nnosis assistant that determines if some-\none needs surgery.\nChange of Order\nusing an ai medical diagnosis assistant,\nsurgeons determine if someone needs\nsurgery\nSpelling Change\nsurgeons using ai medical diagnosis\nassistant that determines if someone\nneeds surgery.\npipleine in Figure 1. It is worth noting that Risk Atlas Nexus\nbuilds on the IBM AI Risk Atlas but our method is agnos-\ntic to the risk assessment tool and taxonomy used and hence\ncan be used with any taxonomy and risk prediction frame-\nwork suited for the task.\nIn our setup, as a first step, we use RAN to get risk pre-\ndictions. However since our dataset comprises of paraphrase\nfor each stakeholder per usecase, we start with taking the\nunion of all risks inferred (from RAN) across all stake-\nholders as the complete set of possible risks for that use\ncase. Next, for each stakeholder, we look at the paraphrased\nprompts that predicted at least one risk prediction. Finally,\nthe risk is retained for a stakeholder if it is predicted con-\nsistently across all such paraphrases of that stakeholder. We\nclassify all these risk-type as “risk” and all the risks types\nthat are not retained are classified as “not-a-risk”. This ap-\nproach ensures that the final risk set for each stakeholder\nreflects only stable, paraphrase-invariant predictions. For-\nmally, let Su denote the set of stakeholders for a use case\nu, and let Pu,s = {pu,s,1, pu,s,2, . . . , pu,s,ns} be the set\nof paraphrased prompts for stakeholder s ∈Su. For each\nparaphrase pu,s,i, the model outputs a set of predicted risks\nRu,s,i ⊆Ratlas, where Ratlas is all the risks predicted for\nusecase based on the taxonomy of IBM AI Risk Atlas.\nWe first define the full set of risks identified for the use\ncase as:\nRu =\n[\ns∈Su\nns\n[\ni=1\nRu,s,i.\n(1)\nWe then retain only paraphrases that yield at least one pre-\ndicted risk:\nP′\nu,s = {pu,s,i ∈Pu,s | |Ru,s,i| > 0}.\n(2)\nThe final, consistent risk set for stakeholder s is obtained\n"}, {"page": 4, "text": "Figure 2: Risk assessment label distribution for all three use-\ncase\nby intersecting predictions across its valid paraphrases:\nR∗\nu,s =\n\\\npu,s,i∈P′u,s\nRu,s,i.\n(3)\nThus, a risk r ∈Rusecase is associated with stakeholder s\nif and only if it appears in all paraphrases of s that produced\nany prediction 2.\nExplaining Stakeholder Conflicts\nAfter we compile a risk profile for each of the stakeholders,\nwe can identify risks on which they agree and disagree in a\ngiven use case. To explain the differences in risk assessments\nacross stakeholders, we utilize GloVE explanation pipeline.\nGloVE is a global explanation pipeline that extracts rule-\nbased explanations using LLM-as-a-Judge. In this work, we\nuse GloVE to explain risk assessment decisions by Risk At-\nlas Nexus from the perspective of individual stakeholders.\nFor a given use case, for each stakeholder and each risk, we\nuse GloVE to generate a set of rules using IF and DESPITE\ncluases that explain why the risk assessment might be rele-\nvant for that stakeholder. Formally,\nri ⊢s IF stakeholder-specific supporting concepts apply,\nDESPITE other contrasting factors.\n(4)\nwhere ri is the risk identified within use case u , s is the\nstakeholder s ∈Su\nExperiments and Results\nStakeholder Risk Perceptions\nWe first look at how stakeholders perceive risk within each\nuse case. Figure 2 shows the distribution of number of pre-\ndicted risks across stakeholders for all three usecases. Each\nstacked bar represents the share of risks labeled as risk (la-\nbel = 1) and not-a-risk (label = 0). It is to note that we\ninitially had 9 LLM generated stakeholder ”AI Fraud detec-\ntion“ usecase, however, one of the stakeholder produced 0\n2We adopt a strict consensus (100% intersection) rather than\nmajority vote to avoid brittle, prompt-specific risks. However, this\nis a design choice and users can make it more flexible depending\non their requirements.\nTable 2: Conflict statistics across the 3 chosen AI use cases\nAI Use Case\nStakeholders\nRisks\nConflicts\nAverage Conflict Rate\nAI Fraud Detection\n8\n30\n20\n66 %\nAI Medical Diagnosis\n9\n47\n10\n21.27 %\nAutonomous Vehicle\n9\n25\n14\n56 %\nrisks across all its paraphrases and was discarded from the\nresults. Hence, we report results for the 8 remaining stake-\nholders for this usecase. The results show that risk percep-\ntions vary by stakeholder role and context. It is worth noting\nthat design choice of choosing risk after stakeholder para-\nphrasing was very rigid as discussed before hence number\nof “risk” classifications are low as compared to “non-risks”\nthat can be made more flexible to allow as per usecase. Over-\nall, the results indicate substantial variation in how risks are\nperceived across stakeholders and use cases. For most stake-\nholders, the majority of predictions were classified as not-\na-risk, reflecting the conservative nature of our strict con-\nsensus rule for retaining risks across paraphrases. However,\nsome stakeholders e.g. fraud analysts in the fraud detection\ndomain, family members & patients in the medical diagno-\nsis use case, and transportation regulators in the autonomous\nvehicle context show higher proportions of risk-labeled pre-\ndictions. These patterns suggest that certain stakeholders are\ndirectly exposed to or affected by the AI systems decisions.\nThese variation support our core hypothesis that stakeholder\nroles and contexts significantly influence risk perception,\nunderscoring the importance of stakeholder-grounded ap-\nproaches for capturing the diversity of concerns in AI gov-\nernance.\nMeasuring Risk Conflicts\nOur experiments focus on using LLM-as-a-Judge to explain\npolicies grounded in stakeholder-specific reasoning across\ndiverse use cases. However, different stakeholders within the\nsame usecase may often disagree on whether a given situa-\ntion constitutes a risk or not. When this happens, we call this\na conflict. Formally, if Su is a set of stakeholders and Ru is\nthe risks for usecase u such that risk ri ∈Ru, yu,s,i ∈{0, 1}\ndenotes the LLM-as-a-Judge label (1 = risk, 0 = not-a-risk)\nfor stakeholder s ∈Su then conflict measure κu(ri) can be\ndefined as:\nκu(ri) =\n(\n1,\nif ∃s1, s2 : yu,s1,i ̸= yu,s2,i,\n0,\notherwise.\n(5)\nwhere κu(ri) is an indicator that equals 1 (hence conflict)\nif any two stakeholders in use case u disagree on whether\nrisk ri constitutes a risk, and 0 otherwise. Table 2 shows\nthe conflict rate statistics for all three usecases. We observe\nthat stakeholders in usecase AI Fraud Detection have most\nconflicts, followed by usecase Autonomous Vehicle and AI\nMedical Diagnosis.\nPolicy Conflicts and Proposed Visualization\nAn interesting subset of analyzing these conflicts is to see if\nthe stakeholders are opposing due to similar perspectives for\n"}, {"page": 5, "text": "Clickable stakeholders\nDropdown to select\nRisk Type\nDropdown to select\nUse Case\nFigure 3: Stakeholder conflict visualizations for the AI Medical Diagnosis Assistant use case. Each node represents a stake-\nholder, and edges indicate relationships based on overlapping or conflicting risk perceptions.\nthe same risk but from opposing directions. For example,\none stakeholder may state, “We do not view this as a risk\nif the decision is always reviewed by a human,” whereas\nanother stakeholder within same usecase may claim,“We\nstill consider this a risk despite human oversight.”. Hence,\nwhile their assessments conflict, their reasoning reflects sim-\nilar considerations approached from opposing perspectives.\nTo systematically understand such explanations in disagree-\nments, we look at the explanations and analyze the IF and\nDESPITE clauses in the stakeholder-specific explanations.\nWhen a conflict arises between two stakeholder, then intu-\nitively, we can look IF justification and DESPITE justifica-\ntion for the same risk of different stakeholder and see if they\nare using similar justifications but in opposite directions.\nIn such cases, we define a conflict score between two\nstakeholders s1, s2 ∈Su for a given risk ri as:\nCu(s1, s2, ri) = max\n\u0010\nsim(Iu,s1,i, Du,s2,i),\nsim(Iu,s2,i, Du,s1,i)\n\u0011\n,\n(6)\nwhere sim(·, ·) measures the semantic similarity between\ntwo textual clauses (e.g., cosine similarity of sentence em-\nbeddings). A high Cu(s1, s2, ri) indicates that two stake-\nholders are reasoning about the same underlying concept\nbut from opposing directions: one as a reason to support\nthe view (IF), the other framing it as an opposing reason\n(DESPITE). In our experiments, we use all-MiniLM-L6-v2\nmodel to compute semantic similarity between stakeholders\nIF and DESPITE statements and identify conceptual con-\nflicts.\nTo visualize these conflicts we propose an interactive vi-\nsualization as seen in Figure 3. Stakeholders are color-coded\nbubbles, with their size reflecting the number of conflicts\nthey’re involved in. Conflicts are represented by connecting\nlines between stakeholder pairs. Users can filter by use case\nand specific risk types (e.g. harmful output), then click on\nany stakeholder to view detailed conflict information includ-\ning full rule explanations with highlighted matching clauses\nthat cause the disagreement. In Figure 3, we show our pro-\nposed method to visualize the stakeholder conflicts identi-\nfied for the usecase AI Medical Diagnosis Assistant with re-\nspect to risk type “harmful output”. We believe such a tool\nwould be valuable to visualize conflicts and understand what\nled to those conflicts.\nDiscussion\nIn this paper, we introduced a stakeholder-grounded frame-\nwork for AI risk assessment that uses LLM-as-a-Judge to\npredict and explain risks through stakeholder-specific use-\ncases. Our findings highlight that stakeholder perspectives\nplay a central role in shaping how risks are perceived and\nexplained within AI systems. By grounding risk predic-\ntions and explanations in stakeholder contexts, our frame-\nwork shows variations in risk assessments that traditional,\nstakeholder-agnostic evaluations overlook. We also pro-\npose an interactive visualization to enhance transparency of\nthe conflict reasoning that emerge. Beyond interpretability,\nour stakeholder-aware explanations paves a way for more\ntransparent and auditable LLM-based evaluations.Together,\nthese contributions point toward a future where stakeholder-\naware, explainable evaluations can form the backbone of\ntrustworthy AI governance.\nLimitations\nThe current approach relies on synthetic, LLM-generated\nstakeholders, which although provides for scalability, may\nnot fully capture the complexity and unpredictability of\nreal-world scenarios. Our framework also focuses on a sin-\ngle risk taxonomy and a risk assessment tool whose per-\nformance is dependent on the robustness of their underly-\ning components. Finally, our framework gives binary out-\ncomes as either “risk” or “not-a-risk” which can be im-\nproved to cover more range of risk categories e.g., low,\nmedium, high, critical (e.g. as proposed in the EU AI Act).\nFuture work could integrate real stakeholder feedback, mul-\n"}, {"page": 6, "text": "tiple taxonomies, and graded risk levels to improve the gran-\nularity of stakeholder-aware evaluations.\nAcknowledgments\nThis work was funded by the EU Horizon project ELIAS\n(No. 101120237). Views and opinions expressed are those\nof the author(s) only and do not necessarily reflect those of\nthe European Union or The European Research Executive\nAgency.\nReferences\nBagehorn, F.; Brimijoin, K.; Daly, E. M.; He, J.; Hind, M.;\nGarces-Erice, L.; Giblin, C.; Giurgiu, I.; Martino, J.; Nair,\nR.; et al. 2025.\nAI Risk Atlas: Taxonomy and Tooling\nfor Navigating AI Risks and Resources.\narXiv preprint\narXiv:2503.05780.\nCeron, T.; Falk, N.; Bari´c, A.; Nikolaev, D.; and Pad´o, S.\n2024. Beyond prompt brittleness: Evaluating the reliability\nand consistency of political worldviews in llms. Transac-\ntions of the Association for Computational Linguistics, 12:\n1378–1400.\nDaly, E. M.; Tirupathi, S.; Rooney, S.; Vejsbjerg, I.; Salwala,\nD.; Giblin, C.; Bagehorn, F.; Garces-Erice, L.; Urbanetz, P.;\nand Wolf-Bauwens, M. L. 2025. Usage governance advisor:\nFrom intent to AI governance. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 39, 29628–\n29630.\nDash, S.; Reymond, A.; Spiro, E. S.; and Caliskan, A.\n2025.\nPersona-Assigned Large Language Models Ex-\nhibit Human-Like Motivated Reasoning.\narXiv preprint\narXiv:2506.20020.\nDe Raad, B. 2000. The big five personality factors: the psy-\ncholexical approach to personality. Hogrefe & Huber Pub-\nlishers.\nEiras, F.; Zemour, E.; Lin, E.; and Mugunthan, V. 2025.\nKnow Thy Judge: On the Robustness Meta-Evaluation of\nLLM Safety Judges. arXiv preprint arXiv:2503.04474.\nGajcin, J.; Miehling, E.; Nair, R.; Daly, E.; Marinescu, R.;\nand Tirupathi, S. 2025. Interpreting LLM-as-a-Judge Poli-\ncies via Verifiable Global Explanations.\narXiv preprint\narXiv:2510.08120.\nHu, T.; and Collier, N. 2024. Quantifying the Persona Effect\nin LLM Simulations. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 10289–10307.\nJiang, H.; Zhang, X.; Cao, X.; Breazeal, C.; Roy, D.; and\nKabbara, J. 2024. PersonaLLM: Investigating the Ability of\nLarge Language Models to Express Personality Traits. In\nFindings of the Association for Computational Linguistics:\nNAACL 2024, 3605–3627.\nLi, J.; Papay, S.; and Klinger, R. 2025.\nAre Humans\nas Brittle as Large Language Models?\narXiv preprint\narXiv:2509.07869.\nMazeika, M.; Phan, L.; Yin, X.; Zou, A.; Wang, Z.; Mu, N.;\nSakhaee, E.; Li, N.; Basart, S.; Li, B.; et al. 2024. Harm-\nBench: A Standardized Evaluation Framework for Auto-\nmated Red Teaming and Robust Refusal. In International\nConference on Machine Learning, 35181–35224. PMLR.\nMeier, D.; Wahle, J. P.; Ruas, T.; and Gipp, B. 2025. To-\nwards Human Understanding of Paraphrase Types in Large\nLanguage Models. In Rambow, O.; Wanner, L.; Apidianaki,\nM.; Al-Khalifa, H.; Eugenio, B. D.; and Schockaert, S., eds.,\nProceedings of the 31st International Conference on Com-\nputational Linguistics, 6298–6316. Abu Dhabi, UAE: Asso-\nciation for Computational Linguistics.\nMIT. 2024. MIT AI Risk Repository. https://airisk.mit.edu/.\nMizrahi, M.; Kaplan, G.; Malkin, D.; Dror, R.; Shahaf, D.;\nand Stanovsky, G. 2024. State of what art? a call for multi-\nprompt llm evaluation. Transactions of the Association for\nComputational Linguistics, 12: 933–949.\nMou, Y.; Zhang, S.; and Ye, W. 2024. Sg-bench: Evaluat-\ning llm safety generalization across diverse tasks and prompt\ntypes. Advances in Neural Information Processing Systems,\n37: 123032–123054.\nMylius, S.; Slattery, P.; Zhu, Y.; Narayanan, M.; Thinnyun,\nA.; Saeri, A. K.; Graham, J.; Noetel, M.; and Thompson, N.\n2025. Mapping the AI Governance Landscape: Pilot Test\nand Update. Technical report, The Ethics Centre. Report\npublished by The Ethics Centre.\nNIST. 2023. AI Risk Management Framework. https://www.\nnist.gov/itl/ai-risk-management-framework.\nOWASP. 2024. OWASP Top 10 for LLMs and Generative\nAI Apps. https://genai.owasp.org/llm-top-10/.\nSlattery, P.; Saeri, A. K.; Grundy, E. A.; Graham, J.; Noetel,\nM.; Uuk, R.; Dao, J.; Pour, S.; Casper, S.; and Thompson,\nN. 2024. The AI Risk Repository: A Comprehensive Meta-\nReview, Database, and Taxonomy of Risks From Artificial\nIntelligence. AGI-Artificial General Intelligence-Robotics-\nSafety & Alignment, 1(1).\nWahle, J. P.; Ruas, T.; Xu, Y.; and Gipp, B. 2024.\nPara-\nphrase Types Elicit Prompt Engineering Capabilities. In Al-\nOnaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings\nof the 2024 Conference on Empirical Methods in Natural\nLanguage Processing, 11004–11033. Miami, Florida, USA:\nAssociation for Computational Linguistics.\nWeidinger, L.; Uesato, J.; Rauh, M.; Griffin, C.; Huang, P.-\nS.; Mellor, J.; Glaese, A.; Cheng, M.; Balle, B.; Kasirzadeh,\nA.; et al. 2022. Taxonomy of risks posed by language mod-\nels. In Proceedings of the 2022 ACM conference on fairness,\naccountability, and transparency, 214–229.\nZhang, Z.; Lei, L.; Wu, L.; Sun, R.; Huang, Y.; Long, C.;\nLiu, X.; Lei, X.; Tang, J.; and Huang, M. 2023. Safetybench:\nEvaluating the safety of large language models with multiple\nchoice questions. CoRR.\nAppendix\n"}, {"page": 7, "text": "Table 3: Structured overview of all paraphrase types used in the stakeholder-specific dataset generation, showing their defini-\ntions, example transformations, and corresponding prompt templates. The base prompt that uses these details can be seen in\nListing 1.\nParaphrase Type\nDefinition\nExample (Input Output)\nCoT Reasoning Example\nAddition / Deletion\nAddition/Deletion consists of\nall additions/deletions of lexical\nand functional units.\nInput: Revenue in the first quarter of the year\ndropped by 15 percent from the same period a year\nearlier.\nOutput: Revenue in the first quarter of the year only\ndropped 15 percent from the same period a year\nearlier.\nThe task is about paraphrasing using adding/deletion such that meaning of the input\nsentence is preserved. So, I can add the word ”only” before ”dropped” to slightly\nemphasize the extent of decline without altering the meaning. Then I can also remove\nthe word ”by” after ”dropped,” since it is optional for sentence and does not affect\nthe meaning.\nSemantic-based Change\nSemantics-based changes in-\nvolve a different lexicalization\nof the same content units, often\naffecting multiple words.\nInput: WalMart said it would verify the employ-\nment status of all its million-plus domestic workers\nto ensure they were legally employed.\nOutput: WalMart announced that it would verify\nthe legal employment status of all its million-plus\ndomestic workers.\nThe task is about paraphrasing using semantics-based changes which can involve re-\nexpressing the same content units using different lexicalizations that often affect mul-\ntiple words together. In this case, I can change the reporting phrase ”said it would”\ninto ”announced that it would,” which is not a single-word substitution but a different\nway of expressing the act of communication. I can also transform the purpose clause\n”to ensure they were legally employed” into the adjectival phrase ”legal employ-\nment status.” This change spans multiple lexical units and shows how the meaning is\npreserved but expressed differently. The part ”all its million-plus domestic workers”\nwas kept intact to preserve the scope of the content. Together, these edits align with\nsemantics-based changes because they alter how the meaning is lexicalized rather\nthan simply adding or deleting words.\nSame Polarity Substitu-\ntion\nChanging one lexical unit for\nanother with approximately the\nsame meaning, such as syn-\nonymy or general/specific alter-\nnation.\nInput: They had published an advertisement on the\nInternet on June 10.\nOutput: They had posted an advertisement on the\nInternet on June 10.\nThe task is to paraphrase using same-polarity substitution, which means swapping a\nlexical unit with another that carries approximately the same meaningtypically via\n(for e.g.) synonymy without altering the proposition’s content or sentiment. Here,\nI can replace ”published” with ”posted” because, in an online context, both verbs\ndenote making material publicly available, preserving the event type and polarity. I\nwill keep all the words unchanged to maintain participants, setting, and timeline. This\nis a like-for-like predicate substitution aligning with the definition.\nPunctuation Change\nAny change in punctuation or\nsentence formatting without al-\ntering lexical units.\nInput: Trading in Loral was halted yesterday. The\nshares closed on Monday at $3.01.\nOutput: Trading in Loral was halted yesterday; the\nshares closed on Monday at $3.01.\nThe task is to paraphrase using punctuation and format changes, which means mod-\nifying how the sentence is punctuated or structured without altering the lexical units\nthemselves. In this case, I can merge the two related sentences into one by replacing\nthe period after ”yesterday” with a semicolon. This adjustment will signal a close\nconnection between the two clauses while keeping all the words unchanged. The\nmeaning and content will also remain identical. This aligns with the definition since\nthe change is purely in punctuation, not in vocabulary.”\nChange of Order\nReordering words, phrases, or\nclauses while maintaining the\nsame meaning.\nInput: The processors were announced in San Jose\nat the Intel Developer Forum.\nOutput: The processors were announced at the In-\ntel Developer Forum in San Jose.\nThe task is to paraphrase using change of order, which means re-arranging the po-\nsition of words, phrases, or clauses while keeping the meaning intact. In this case,\nI can shift the location of the phrase ”in San Jose” from before ”at the Intel Devel-\noper Forum” to after it. This reordering does not alter the entities involved, the event\n(the processors being announced), or the location information, but it changes how the\nsentence is structured. Since only the order of the phrases is modified and no lexical\nitems are added or removed, this matches the definition of change of order.”\nSpelling Change\nAltering the spelling or written\nformat\n(e.g.,\ncase\nchanges,\nabbreviations,\nor\ndigit/letter\nalternations) while preserving\nmeaning.\nInput: It said the damage to the wing provided a\npathway for hot gasses to penetrate the ship’s ther-\nmal armor during Columbia’s ill-fated reentry.\nOutput: It stated that the damage to the wing pro-\nvided a pathway for hot gases to penetrate the\nship’s thermal armour during Columbia’s ill-fated\nre-entry.\nThe task is to paraphrase using spelling and format changes, which involve altering\nthe orthography or written form of lexical units without changing their meaning.\nIn this case, I can replace the American English spelling ”gasses” with the more\nstandard form ”gases,” and ”armor” with the British English variant ”armour.” I can\nalso reformat ”reentry” as ”re-entry” by adding a hyphen. These changes do not affect\nthe semantics of the sentence but strictly concern spelling and format conventions,\nwhich aligns directly with the definition.”\n\"paraphrase_1\": {\n\"type\": <paraphrase type>,\n\"definition\": <definition of the paraphrase type>,\n\"example\": [\n\"Input: <input usecase>,\nCoT Reasoning: <example reasoning>,\nOutput: <example output>\"\n],\n\"prompt\": [\n\"In this task you will be given a definition of an alteration and an input sentence...\",\n\"Output the altered sentence at the end with ’Output:’ in front.\",\n\"\",\n\"Alteration: {definition}\",\n\"Example: {example}\",\n\"Input: {usecase}\"\n]\n}\nListing 1: Base format of the paraphrase prompt.\n"}]}