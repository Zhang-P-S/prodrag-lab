{"doc_id": "arxiv:2601.20221", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20221.pdf", "meta": {"doc_id": "arxiv:2601.20221", "source": "arxiv", "arxiv_id": "2601.20221", "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning", "authors": ["Hang Zhang", "Ruheng Wang", "Yuelyu Ji", "Mingu Kwak", "Xizhi Wu", "Chenyu Li", "Li Zhang", "Wenqi Shi", "Yifan Peng", "Yanshan Wang"], "published": "2026-01-28T03:44:20Z", "updated": "2026-01-28T03:44:20Z", "summary": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20221v1", "url_pdf": "https://arxiv.org/pdf/2601.20221.pdf", "meta_path": "data/raw/arxiv/meta/2601.20221.json", "sha256": "873c3c4ba91b5cc5bb3d1437f87050331cbc851e0399f5ee07e9c6a023d6756b", "status": "ok", "fetched_at": "2026-02-18T02:20:18.028614+00:00"}, "pages": [{"page": 1, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement\nLearning\nHang Zhang 1 Ruheng Wang 2 Yuelyu Ji 1 Mingu Kwak 1 Xizhi Wu 1 Chenyu Li 1 Li Zhang 1 Wenqi Shi 2\nYifan Peng 3 Yanshan Wang 1\nAbstract\nLarge language models have achieved strong per-\nformance on medical reasoning benchmarks, yet\ntheir deployment in clinical settings demands\nrigorous verification to ensure factual accuracy.\nWhile reward models offer a scalable approach for\nreasoning trace verification, existing methods face\ntwo limitations: they produce only scalar reward\nvalues without explicit justification, and they rely\non single-pass retrieval that precludes adaptive\nknowledge access as verification unfolds. We in-\ntroduce Med-TIV, an agentic framework that ad-\ndresses these limitations by training medical rea-\nsoning verifiers to iteratively query external medi-\ncal corpora during evaluation. Our approach com-\nbines tool-augmented verification with an itera-\ntive reinforcement learning paradigm that requires\nonly trace-level supervision, alongside an adap-\ntive curriculum mechanism that dynamically ad-\njusts training data distribution. Across four med-\nical reasoning benchmarks, Med-TIV achieves\nsubstantial gains over existing methods, improv-\ning MedQA accuracy by 23.5% and MedXpertQA\nby 32.0% relative to the base generator in partic-\nular. Crucially, Med-TIV demonstrates an 8√ó\nreduction in sampling budget requirement com-\npared to prior reward model baselines. These\nfindings establish that grounding verification in\ndynamically retrieved evidence offers a princi-\npled path toward more reliable medical reasoning\nsystems.\n1. Introduction\nLarge Language Models (LLMs) have demonstrated remark-\nable capabilities in medical reasoning, achieving competi-\ntive performance on clinical question answering, diagnostic\n1University of Pittsburgh 2UT Southwestern Medical Cen-\nter\n3Cornell University.\nCorrespondence to:\nHang Zhang\n<haz269@pitt.edu>, Yanshan Wang <yaw45@pitt.edu>.\nPreprint. January 29, 2026.\n67-year-old man with \nbladder cancer presents \nwith tinnitus after \nchemotherapy. Which \nmechanism caused this?\nOtotoxicity is common with \ntaxanes which \nhyperstabilize \nmicrotubules... \nAnswer: Hyperstabilization \nof microtubules \n<search> \ncisplatin \nbladder cancer \nototoxicity \n</search>\nThe trace incorrectly \nattributes ototoxicity to \ntaxanes. Cisplatin is \nstandard for bladder \ncancer. \nJudgment: Incorrect \nThe reasoning correctly identifies taxanes as ototoxic. \nJudgment: Correct \nCisplatin \ncauses \nototoxicity via \nDNA cross-\nlinking...\nText-based Verification\nTool-integrated Verification\nFigure 1. Comparison of medical reasoning verification paradigms.\nText-based judges rely on parametric knowledge and may validate\nerroneous reasoning, while tool-integrated judges dynamically\nretrieve evidence to ground their judgments.\ninference, and medical knowledge benchmarks (Ji et al.,\n2025a; Xiao et al., 2026). While these advances hold signif-\nicant promise for augmenting clinical decision making and\ndemocratizing access to medical expertise, the deployment\nof LLMs in high-stakes clinical settings demands rigorous\nverification mechanisms to ensure that generated reasoning\nis both factually accurate and logically sound (Zhang et al.,\n2025; Wang et al., 2025).\nReward-based judges have therefore emerged as a scalable\nsolution for evaluating model outputs, supporting both post-\ntraining refinement via reinforcement learning from human\nfeedback (RLHF) and inference-time scaling through tree\nsearch (Snell et al., 2024). These judges can be broadly cat-\negorized by the granularity of their supervision. Outcome\nReward Models (ORMs) provide sparse trace-level super-\nvision that quantifies the quality of the entire output, while\nProcess Reward Models (PRMs) offer dense step-level feed-\nback that scores each intermediate reasoning step, enabling\nfine-grained credit assignment and precise error localiza-\ntion within multi-step reasoning. Recent work has adapted\nboth paradigms to the medical domain to assess complex\nclinical reasoning traces. In parallel, advances in genera-\n1\narXiv:2601.20221v1  [cs.AI]  28 Jan 2026\n"}, {"page": 2, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\ntive reward modeling have extended judge models beyond\nscalar scoring, enabling them to produce natural-language\ncritiques that explicitly justify their decisions (Liu et al.,\n2025c; Xiong et al., 2025).\nDespite their effectiveness, reward-based judges exhibit\nfundamental limitations when applied to clinical reasoning\ntasks (Yun et al., 2025). A primary concern is the prevalence\nof hallucinations in critique traces, where judge models gen-\nerate plausible yet factually incorrect assessments (Figure 1).\nThis issue is particularly noticeable in the medical domain,\nwhere reliable verification demands grounding in authorita-\ntive clinical evidence and established medical knowledge.\nUnverified judgments could lead to the propagation of in-\ncorrect diagnostic or treatment recommendations. Existing\nmedical reasoning verifiers typically provide only scalar\nreward signals, offering little or no justification for their\njudgments and thus limiting interpretability (Jiang et al.,\n2025b). Furthermore, these methods often rely on a static\nRetrieval-Augmented Generation (RAG) pipeline, in which\na fixed set of retrieved documents is prefixed to the context\nand remains unchanged throughout evaluation (Yun et al.,\n2025). Such static design precludes adaptive, multi-turn\nevidence gathering and forces the verifier to a fixed retrieval\nbudget, thus limiting scalability.\nTo address these issues, we propose Med-TIV (Medical\nTool-Integrated reasoning Verifier), an agentic reinforce-\nment learning (RL) framework that trains LLMs to leverage\nexternal knowledge bases for judging medical reasoning\ntraces1. Med-TIV features three key design principles:\n(1) a tool-augmented verification paradigm that enables dy-\nnamic, iterative knowledge retrieval during the evaluation\nprocess; (2) an iterative RL approach that progressively im-\nproves verification capabilities without requiring step-level\nexpert annotations; and (3) an adaptive curriculum formula-\ntion strategy that adjusts the data distribution in response to\nthe evolving capability of the model. By equipping judge\nmodels with tool-use capabilities, Med-TIV grounds eval-\nuation decisions in external evidence rather than relying\nsolely on parametric knowledge, thereby mitigating hallu-\ncination, improving interpretability, and overcoming the\nlimitations of static RAG (Ji et al., 2025b; Xia et al., 2025).\nTo verify the effectiveness of Med-TIV, we conduct ex-\ntensive experiments on common medical reasoning bench-\nmarks. Our results demonstrate that Med-TIV trains strong\nmedical verifiers: when guiding inference-time search for\na 7B generator model, our trained verifier achieves relative\nimprovements of 23.5% on MedQA and 32.0% on MedX-\npertQA compared to the generator model alone. More-\nover, Med-TIV consistently outperforms existing medical\nreward model baselines and surpasses the performance of\n1Code\nis\navailable\nat\nhttps://github.com/\nPittNAIL/med-tiv\nmodels that are up to 4√ó larger in scale. Notably, Med-TIV\nalso demonstrates an 8√ó gain in sampling efficiency com-\npared to prior reward-based approaches, achieving equiva-\nlent accuracy with substantially fewer sampled reasoning\ntraces during test-time search.\nOur main contributions are summarized as follows:\n‚Ä¢ We propose Med-TIV, a novel tool-integrated verifica-\ntion framework that enables dynamic, iterative knowledge\nretrieval during medical reasoning evaluation, providing\nboth interpretable, fine-grained justification and improved\nfactual grounding.\n‚Ä¢ We introduce an iterative RL paradigm with curriculum-\nbased difficulty adaptation that progressively improves\nverification capabilities through self-bootstrapping, requir-\ning only trace-level supervision rather than dense step-\nlevel expert annotations.\n‚Ä¢ Med-TIV achieves state-of-the-art performance on four\nmedical reasoning benchmarks, with comprehensive abla-\ntion studies that validate each component‚Äôs contribution.\n2. Preliminaries\n2.1. Problem Setup\nWe define medical reasoning verification as the task of as-\nsessing the correctness of a multi-step reasoning trace gen-\nerated in response to a medical question. Formally, given\na medical question q ‚ààQ and a multi-step reasoning trace\nœÑ = (s1, s2, . . . , sm) from a generator model, a verifier\nmodel determines whether œÑ contains any errors. We formu-\nlate this problem as binary classification, where the verifier\nVŒ∏(q, œÑ) produces a judgment ‚Ñì‚àà{0, 1}, where ‚Ñì= 1 indi-\ncates a error-free reasoning trace, and ‚Ñì= 0 indicates the\npresence of one or more errors. Unlike scalar reward models\nthat output continuous scores, we adopt a generative judge\nparadigm in which the verifier produces a discrete judgment\naccompanied by a detailed critique trace that provides a\nstructured justification for the decision.\n2.2. Tool-Augmented Reasoning Verifier\nFollowing prior works (Jin et al., 2025), we extend the veri-\nfier with access to an external search engine E that retrieves\ntop-k documents from a curated medical corpus (See Ap-\npendix B.2 for details). Retrieved documents are appended\nverbatim to the verifier context. Given a verification in-\nstance (q, œÑ), the verifier constructs an iterative verfication\ntrajectory. At step k, the trajectory is represented as:\ntk = {r1, a1, o1, . . . , rk, ak, ok},\nwhere ri denotes a natural language reasoning step analyz-\ning the medical content, ai is a search query formulated to\nretrieve relevant medical knowledge, and oi = E(ai) rep-\nresents the retrieved documents. The iterative verification\n2\n"}, {"page": 3, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nTool-Integrated Reasoning Verification\nTraining Strategy\nInference-Time Guided Search\nINPUT\nVERIFICATION\nOUTPUT\nMedical\nQuestion\nMulti-step\nReasoning\nReasoning \nStep\nRetrieved\nDocuments\nMedical \nCorpus\nSearch\nQuery \nCorrect\nIncorrect\nCurriculum Formulation\nTrivial\nBoundary\nImpossible\nFiltered Out\nFiltered Out\nRetained\nLabeled Pool Sampled Batch Filtered Batch\nRL Training\nImproved \nAccuracy\n8√óSampling \nEfficiency Gain\nMedical\nQuestion\nFrozen \nGenerator\nCandidate \nTraces\nEnvironment\nReward\nRL Training Step\nùúãùúÉùë°‚ÜíùúãùúÉùë°+1\nùúè1\nùúè2\nùúè3\nùúè1 :\n(Confidence: 0.92)\nùúè2 :\n(Confidence: 0.37)\nùúè3 :\n(Confidence: 0.75)\nWeighted \nSelf-consistency\nFigure 2. Overview of Med-TIV. Left: Tool-integrated verification iteratively analyzes reasoning traces, formulates search queries,\nand retrieves medical evidence before producing correctness judgments. Middle: Curriculum formulation filters trivial and impossible\ninstances, retaining boundary cases for RL training. Right: At inference time, the verifier evaluates candidate medical reasoning traces\ngenerated by a frozen model and final answers are selected via weighted self-consistency.\nprocess is defined as:\n(rk, ak) ‚àºVŒ∏(q, œÑ, tk‚àí1),\nok = E(ak),\ntk = tk‚àí1 ‚äïrk ‚äïak ‚äïok,\nwhere ‚äïdenotes sequence concatenation. This process\ncontinues until the verifier produces a final judgment ‚Ñì‚àº\nVŒ∏(q, œÑ, tT ) at the terminal step T. By allowing multiple\ntool executions, the verifier dynamically retrievs medical\nknowledge as need to verify specific claims in the reasoning\ntrace. Table 5 in the Appendix shows the explicit instruction\nused in our experiments.\n2.3. Test-Time Search\nTest-time search strategies improve reasoning performance\nby leveraging reward models to evaluate and select among\nmultiple candidate solutions (Shi et al., 2024). Given a\nfrozen generator model œÄgen and a question q, we first sam-\nple N independent reasoning traces:\n{œÑ (j)}N\nj=1 ‚àºœÄgen(¬∑ | q).\nA trained verifier VŒ∏ then scores each candidate trace, and\nthe final output is selected based on these scores. Com-\nmon selection strategies include Best-of-N sampling, which\nselects the trace with the highest score:\nÀÜœÑ = arg max\nœÑ (j) VŒ∏(q, œÑ (j)),\nand verification-based majority voting, where candidate\ntraces are first filtered by the verifier and the final answer is\ndetermined by consensus among verified traces. Med-TIV\ntrains such a plug-in verifier that provides tool-grounded\nassessments that can be used to augment decision-making\nfor any frozen generator model at inference time.\n3. Tool-Integrated Medical Reasoning Verifier\nMed-TIV is an agentic verification framework that trains\nmodels to leverage external knowledge bases for verifying\nwhether a given medical reasoning trace contains errors. We\nadopt an iterative training approach based on dynamic cur-\nriculum learning, which requires no fine-grained step-level\nexpert supervision and trains solely through multiple rounds\nof reinforcement learning (Figure 2). We next describe the\ntraining procedure of Med-TIV in details.\n3.1. Tool-Integrated RL with Verifiable Rewards\nData Construction.\nAll training data across iterations is\nderived from the open-source Med-PRM dataset. Each orig-\ninal instance consists of a tuple (q, œÑ, ‚Ñìstep, ‚Ñìtrace), where q is\na medical question, œÑ is a multi-step reasoning trace, ‚Ñìstep de-\n3\n"}, {"page": 4, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nnotes step-level labels, and ‚Ñìtrace is a trace-level correctness\nlabel2.\nAt each training iteration, we only utilize the triplet\n(q, œÑ, ‚Ñìtrace) with human-annotated trace-level labels. Step-\nlevel labels ‚Ñìstep is intentionally excluded, as Med-TIV is\ndesigned to improve verification performance without reply-\ning on fine-grained supervision. For each training iteration,\nwe fix the training data budget to 20K instances and enforce\na balanced label distribution between correct (‚Ñìtrace = 1)\nand incorrect (‚Ñìtrace = 0) reasoning traces.\nAlgorithm.\nWe employ Dr. GRPO (Liu et al., 2025b) as\nthe RL algorithm for training the verifier. Given a verifica-\ntion instance (qi, œÑi), we sample a group of G verification\ntrajectories {oi}G\ni=1 from the current policy œÄŒ∏. Each tra-\njectory oi = (o1\ni , . . . , o|oi|\ni\n) consists of reasoning tokens,\nsearch queries, retrieved documents, and a final judgment.\nThe objective is:\n1\nG\nG\nX\ni=1\n|oi|\nX\nt=1\nn\nmin\nh\nrt\ni ÀÜAt\ni, clip\n\u0000rt\ni, 1 ‚àíœµl, 1 + œµh\n\u0001 ÀÜAt\ni\nio\n,\n(1)\nwhere rt\ni =\nœÄŒ∏(ot\ni|q,o<t\ni\n)\nœÄŒ∏old(ot\ni|q,o<t\ni\n), q = (q, œÑ) denotes the input\nprompt containing the question and reasoning trace, o<t\ni\nrepresents previously generated tokens, and œµl and œµh are\nthe clipping parameters. The advantage term ÀÜAt\ni is defined\nas:\nÀÜAt\ni = R(q, oi) ‚àímean ({R(q, o1), . . . , R(q, oG)}) .\nReward Designs.\nTo facilitate multi-turn RL with tool\nexecution, we design a structured reward covering two com-\nplementary objectives, following prior practices (Jin et al.,\n2025):\n(i) Correctness Reward (Rc): This component measures\nwhether the verifier‚Äôs judgment aligns with the ground-truth\nlabel. Let q = (q, œÑ) denote the verification prompt and\n‚Ñì‚àà{0, 1} the ground-truth label. We define:\nRc = 1\n\u0000extract(o) = ‚Ñì\n\u0001\n,\nwhere 1(¬∑) is the indicator function and extract(o)\nparses the final judgment from the <answer> tags in the\ngenerated trajectory o. Intuitively, Rc = 1 if the verifier‚Äôs\ndecision is correct, and Rc = 0 otherwise.\n(ii) Format Reward (Rf): To ensure reliable tool use and\nstructured outputs, the verifier is required to adhere to\na predefined format. Specifically, reasoning steps must\nbe enclosed within <think> tags, search queries within\n<search> tags, and the final judgment within <answer>\n2Dataset\nis\navailable\nat\nhttps://\nhuggingface.co/datasets/dmis-lab/llama-3.\n1-medprm-reward-training-set\ntags. To discourage degenerate outputs, we further penalize\nexcessive tag usage. Specifically, Rf = 1 if the output sat-\nisfies all formatting constraints and contains no more than\n10 <answer> tag pairs; Rf = 0.25 if the output is correct\nbut exhibits tag overflow; and Rf = 0 otherwise.\nThe final reward R is defined as the product of the two\ncomponents:\nR = Rc √ó Rf.\n3.2. Training Strategies\nAdaptive Curriculum Formulation.\nA central challenge\nin RL for verification is ensuring that training data remains\nappropriately calibrated to the evolving capability of the\nmodel. Instances that are either trivially easy or impossibly\ndifficult yields minimal learning signal, as the resulting\npolicy gradients approach zero. To address this issue, we\nadopt a model-aware curriculum formulation mechanism\nthat dynamically adapts the task distribution at each training\niteration.\nConcretely, before each iteration t, we perform online fil-\ntering on the sampled batch Bt to construct an effective\ntraining set Dt:\nDt = {(q, œÑ, ‚Ñì) ‚ààBt : ‚àÉg, g‚Ä≤ ‚àà{1, . . . , G} s.t. r(g) Ã∏= r(g‚Ä≤)}.\nHere, for each candidate instance (q, œÑ, ‚Ñì) ‚ààBt, we sample\nG verification trajectories {o(g)}G\ng=1 from the current policy\nœÄŒ∏t. We then compute the corresponding rewards {r(g)}G\ng=1.\nFinally, we retain only instances if any two rewards are\ndifferent, i.e., reward variance is non-zero (Khatri et al.,\n2025).\nThis criterion eliminates prompts where the model either\nconsistently succeeds or consistently fails across all sam-\npled trajectories. By filtering these zero-gradient instances,\noptimization is focused on decision-boundary cases where\nthe verifier exhibits uncertainty.\nTo maintain a fixed training budget per iteration, we itera-\ntively resample additional instances from the labeled pool\nB and apply the same filtering criterion until |Dt| = 20K.\nThis dynamic curriculum evolves naturally across iterations\nas the verifier improves, eliminating the need for manually\ndesigned difficulty schedules.\nIterative Training via Self-Bootstrapping.\nWe adopt an\niterative training approach that progressively improves veri-\nfication capabilities through multiple rounds of RL. Unlike\nprior work that alternates between rejection sampling, su-\npervised fine-tuning (SFT), and RL (Xu et al., 2025), our\napproach operates entirely through iterative RL, following\nthe RL-Zero paradigm where the model reinforces its verifi-\ncation capabilities without requiring dense turn-level expert\ndemonstrations for cold start.\n4\n"}, {"page": 5, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nAlgorithm 1 Iterative Training of Tool-Integrated Medical\nReasoning Verifier\nRequire: Base verifier œÄŒ∏0, labeled dataset pool D =\n{(qi, œÑi, ‚Ñìi)}N\ni=1, maximum iterations Tmax, batch size\nB, group size G, search engine E\nEnsure: Trained verifier œÄŒ∏‚àó\n1: for t = 1 to Tmax do\n2:\n‚ñ∑Sample labeled batch\n3:\nBt ‚ÜêSAMPLEBATCH(D, B)\n4:\nDt ‚Üê‚àÖ\n5:\n‚ñ∑Curriculum formulation\n6:\nfor each (q, œÑ, ‚Ñì) ‚ààBt do\n7:\nSample verification trajectories:\n{ÀÜ‚Ñì(g)}G\ng=1 ‚àºœÄŒ∏t(¬∑ | q, œÑ, E)\n8:\nCompute rewards within group:\nr(g) ‚Üê1[ÀÜ‚Ñì(g) = ‚Ñì], for g ‚àà1, . . . , G\n9:\nif ‚àÉg Ã∏= g‚Ä≤ such that r(g) Ã∏= r(g‚Ä≤) then\n10:\nAdd (q, œÑ, ‚Ñì) to curriculum set Dt\n11:\nend if\n12:\nend for\n13:\n‚ñ∑RL optimization on curriculum data\n14:\nœÄŒ∏t+1 ‚ÜêDR.GRPO(œÄŒ∏t, Dt, E)\n15: end for\n16: Return œÄŒ∏Tmax\nStarting from the base model œÄŒ∏0, we perform Tmax itera-\ntions. Each iteration consists of three stages:\nBt ‚ÜêSAMPLEBATCH(D, B),\nDt ‚ÜêFILTER(Bt, œÄŒ∏t),\nœÄŒ∏t+1 ‚ÜêRL(œÄŒ∏t, Dt).\nEach iteration draws a fresh batch Bt from the annotated\npool D with trace-level labels, ensuring a balanced distribu-\ntion of correct and incorrect reasoning traces. The curricu-\nlum filtering then constructs the training set Dt as described\nabove, and RL optimization updates the policy based on the\nstructured reward.\nThe key insight underlying this iterative approach is the co-\nevolution of model capability and training distribution. As\nthe verifier improves, the filtering mechanism automatically\nremoves instances that have become too easy, while the fresh\nsampling introduces new challenging cases. This creates a\nself-bootstrapping cycle: stronger models encounter harder\nverification tasks, which in turn drive further improvements.\nSince the trace-level correctness reward is deterministic\nand unambiguous, this self-bootstrapping process converges\nreliably without the instabilities that can arise from noisy\nsynthetic step-level labels. We summarize the overall train-\ning procedure in Algorithm 1.\n4. Experiments\n4.1. Experimental Setup\nEvaluation benchmarks.\nWe evaluated Med-TIV on\nfour open-source medical question-answering benchmarks:\nMedQA (Jin et al., 2020), MedMCQA (Pal et al.,\n2022), MMLU-Med (Hendrycks et al., 2021), and MedX-\npertQA (Zuo et al., 2025), using accuracy as the evalua-\ntion metric. These benchmarks collectively assess the veri-\nfier‚Äôs ability to distinguish correct from erroneous reasoning\nacross varying difficulty levels and medical subdomains.\nDetailed descriptions of benchmarks are in Appendix C.1.\nImplementation details.\nWe trained verifiers using two\nlight-weight backbone models: Llama3.1-8B and Qwen2.5-\n7B, with Llama3.1-8B as the default for results reporting.\nAll training was conducted using the VeRL-Tool frame-\nwork (Jiang et al., 2025a). Detailed hyperparameters are\nshown in Appendix B.1. All experiments were conducted\non 4 NVIDIA H100 GPUs with 80GB of memory. Due to\ncomputational constraints, we limit the maximum number\nof RL iterations to Tmax = 2 and we set the group size for\ncurriculum formulation (Section 3.2) to G = 8.\nFor inference, we used the default sampling hyperparame-\nters for all models. In reward-guided search experiments, un-\nless otherwise specified, we used Qwen2.5-7B as the frozen\ngenerator and sampled up to 32 candidate reasoning traces\nper question. We applied Hard-Weighted Self-Consistency\nas the default test-time search strategy.\nBaselines.\nWe compared Med-TIV against two groups\nof baselines. 1): Off-the-shelf LLMs: GPT-4o-mini (Ope-\nnAI et al., 2024), Gemini-2.0-Flash, DeepSeek-R1 se-\nries (Guo et al., 2025), Qwen2.5 series (Yang et al., 2025),\nLlama3.1 (Grattafiori et al., 2024), AlphaMed (Liu et al.,\n2025a), UltraMedical (Zhang et al., 2024), and HuatuoGPT-\no1 (Chen et al., 2024). 2): Medical domain-specialized\nReward Models: MedS3 (Jiang et al., 2025b) and Med-\nPRM (Yun et al., 2025). Detailed descriptions of each re-\nward model baseline are shown in Appendix B.3.\nTest-Time Search Strategies.\nWe evaluated three test-\ntime search strategies that leverage Med-TIV to improve\nthe reasoning performance of frozen generators. Given a\nreasoning trace œÑ = (s1, s2, . . . , sK) with K steps, our\nverifier assigns a confidence score rœÑ ‚àà[0, 1] for the entire\ntrace, defined as the softmax probability of the 1 token over\nthe logits of both 1 and 0 tokens.\n‚Ä¢ Best-of-N. Given a question q, we sampled N candidate\ntraces {œÑ (j)}N\nj=1 from the generator and selected the trace\nwith the highest verifier confidence score:\nÀÜœÑ = arg max\nœÑ (j) rœÑ (j).\n5\n"}, {"page": 6, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nTable 1. Main evaluation results on medical reasoning benchmarks. We report accuracy (%) on MedQA, MedMCQA, MMLU-Med, and\nMedXpertQA. Bold numbers indicate the best results among the reward model group. ‚úì: Verifier supports external tools for judging; ‚úó:\nVerifier does not support external tools for judging.\nBaselines\n√•\n|Train|\nSize\nMedQA\nMedMCQA\nMMLU-Med\nMedXpertQA\nAvg.\nProprietary Models\nGPT-4o-mini\n-\n-\n-\n79.03\n68.20\n87.79\n17.84\n63.22\nGemini-2.0-Flash\n-\n-\n-\n87.51\n72.60\n92.01\n20.57\n68.17\nGeneral Reasoning Models\nDeepSeek-R1\n-\n-\n671B\n90.34\n78.80\n94.40\n37.76\n75.33\nR1-Distill-Qwen\n-\n-\n7B\n24.82\n36.40\n47.47\n7.43\n29.03\nR1-Distill-Llama\n-\n-\n8B\n34.96\n43.60\n64.19\n5.35\n37.03\nGeneral Non-reasoning Models\nQwen2.5\n-\n-\n32B\n73.21\n64.83\n84.94\n13.87\n59.21\nQwen2.5\n-\n-\n7B\n60.96\n56.56\n76.96\n12.15\n51.66\nLlama3.1\n-\n-\n8B\n70.93\n61.60\n78.97\n13.02\n56.13\nMedical Reasoning Models\nAlphaMed\n-\n-\n7B\n71.01\n61.46\n81.16\n19.16\n58.20\nUltraMedical\n-\n-\n8B\n72.66\n62.60\n79.61\n15.25\n57.53\nHuatuoGPT-o1\n-\n-\n8B\n72.19\n63.60\n75.30\n16.84\n56.98\nMedical Reward Models\nMedS3\n‚úó\n225k\n7B\n64.89\n58.91\n80.53\n12.90\n54.31\nMed-PRM\n‚úì\n111k\n7B\n69.99\n62.36\n80.99\n13.51\n56.71\nMed-TIV (Ours)\n‚úì\n20k\n7B\n75.26\n64.70\n85.58\n16.04\n60.40\n‚Ä¢ Hard-Weighted Self-Consistency. We first filtered traces\nby the verifier‚Äôs binary judgment, keeping only those la-\nbeled correct (VŒ∏(q, œÑ) = 1). Among the filtered traces,\nwe applied majority voting to determine the final answer:\nÀÜa = arg max\na\nN\nX\nj=1\n1\n\u0002\nVŒ∏(q, œÑ (j)) = 1\n\u0003\n¬∑ 1\n\u0002\nans(œÑ (j)) = a\n\u0003\n.\n‚Ä¢ Soft-Weighted Self-Consistency. Instead of binary filtering,\nwe weighted each trace‚Äôs vote by the verifier‚Äôs confidence\nscore:\nÀÜa = arg max\na\nN\nX\nj=1\nrœÑ (j) ¬∑ 1\n\u0002\nans(œÑ (j)) = a\n\u0003\n.\n4.2. Main Results\nTable 1 presents the main results on four medical rea-\nsoning benchmarks. Models trained with Med-TIV con-\nsistently outperform existing baselines across all bench-\nmarks. Specifically, under guided-search using a Med-TIV-\ntrained verifier, Qwen2.5-7B attains accuracies of 75.26%\non MedQA, 64.70% on MedMCQA, 85.58% on MMLU-\nMed, and 16.04% on MedXpertQA, yielding an average\naccuracy of 60.40%. Notably, Med-TIV enables this 7B\ngenerator to rival substantially larger models, even surpass-\ning the base performance of Qwen2.5-32B despite using\na generator that is approximately 4√ó smaller. Compared\nto domain-specialized medical reasoning models of simi-\nlar scale, Med-TIV outperforms HuatuoGPT-o1-8B and\nUltraMedical-8B by 3.07% and 2.60% on MedQA, respec-\ntively, demonstrating the effectiveness of our tool-integrated\nverification. Case analysis in Appendix D further illustrates\nhow Med-TIV identifies subtle reasoning errors.\n4.3. Analysis\nWe conducted a series of ablation analyses to investigate six\nkey research questions regarding our proposed framework.\nQ1: Does Med-TIV generalize across different gen-\nerator models?\nTo evaluate the generalizability of the\ntrained verifier, we applied Med-TIV to guide test-time\nsearch across generator models of varying sizes and capa-\n6\n"}, {"page": 7, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\n1\n2\n4\n8\n16\n32\nSampling Budget (N)\n62\n64\n66\n68\n70\n72\n74\nAccuracy (%)\n8 x\nMedQA\nMed-TIV\nMed-PRM\nMedS3\nSelf-consistency\n1\n2\n4\n8\n16\n32\nSampling Budget (N)\n56\n58\n60\n62\n64\n8 x\nMedMCQA\n1\n2\n4\n8\n16\n32\nSampling Budget (N)\n74\n76\n78\n80\n82\n84\n86\n8 x\nMMLU-Med\nFigure 3. Test-time scaling analysis across three medical reasoning benchmarks. Each plot shows accuracy versus sampling budget\nN ‚àà{1, 2, 4, 8, 16, 32} for four baselines. Med-TIV consistently outperforms baselines across all sampling budgets and benchmarks.\nTable 2. Performance improvements from using Med-TIV as\na verifier on MedQA. For each generator model, the first row\nindicates the accuracy over single sampled trace per question.\nModels\nMedQA\nQwen2.5-7B\n60.96\n+ Self-Consistency\n66.38 (+5.42)\n+ Best-of-N (Med-TIV)\n72.35 (+11.39)\n+ Soft Weighted SC (Med-TIV)\n75.02 (+14.06)\n+ Hard Weighted SC (Med-TIV)\n75.26 (+14.30)\nAlphaMed-7B\n71.01\n+ Self-Consistency\n74.23 (+3.22)\n+ Best-of-N (Med-TIV)\n75.02 (+4.01)\n+ Soft Weighted SC (Med-TIV)\n75.33 (+4.32)\n+ Hard Weighted SC (Med-TIV)\n75.65 (+4.64)\nQwen2.5-32B\n73.21\n+ Self-Consistency\n75.26 (+2.05)\n+ Best-of-N (Med-TIV)\n75.57 (+2.36)\n+ Soft Weighted SC (Med-TIV)\n75.96 (+2.75)\n+ Hard Weighted SC (Med-TIV)\n75.96 (+2.75)\nbilities. As shown in Table 2, when using Qwen2.5-7B\nas the generator, Hard-Weighted Self-Consistency yields a\nrelative improvement of 23.5% over the base model‚Äôs single-\nsample accuracy, substantially outperforming the 12.2%\ngain achieved by standard Self-Consistency. Notably, the\ndomain-specialized AlphaMed-7B model also benefits from\nverifier guidance with a 6.5% relative improvement, indi-\ncating that our verifier provides complementary verifica-\ntion capabilities beyond domain-specific fine-tuning. The\nimprovements extend to larger models as well: Qwen2.5-\n32B achieves a 3.8% relative gain during test-time search,\ndemonstrating that a light-weight 8B verifier can effectively\nguide models that are significantly larger than itself. This\ncross-scale generalization suggests that Med-TIV learns\ntransferable verification patterns rather than overfitting to\nspecific generator characteristics.\nQ2: How do different test-time search strategies com-\npare under Med-TIV?\nWe then systematically compare\ndifferent test-time search strategies under verifier guidance\nto identify the most effective approach for leveraging ver-\nification signals. As shown in Table 2, Hard-Weighted\nSelf-Consistency consistently achieves the highest accu-\nracy across all generators, followed by Soft-Weighted Self-\nConsistency and Best-of-N selection. On Qwen2.5-7B,\nHard-Weighted Self-Consistency outperforms Best-of-N\nby 3% absolute accuracy, suggesting that majority voting\namong verified traces provides more robust answer selec-\ntion than simply choosing the highest-confidence individual\ntrace.\nQ3: Can Med-TIV reduce the sampling budget required\nto achieve state-of-the-art performance compared to ex-\nisting baselines?\nNext, we investigated how verification\nperformance scales with sampling budget, a critical consid-\neration for deployment under varying computational con-\nstraints. As shown in Figure 3, Med-TIV achieves substan-\ntial efficiency advantage over existing medical reward mod-\nels across all three benchmarks. In particular, Med-TIV\nmatches the performance of baselines using only 4 samples,\nwhereas the baselines require 32 samples, representing an\n8√ó reduction in sampling budget. On MedQA, Med-TIV\nachieves 72.1% accuracy at N = 4, while Med-PRM re-\nquires the full N = 32 budget to reach 70.0% accuracy.\nSince inference cost scales approximately linearly with the\nnumber of sampled traces, this translates to equivalent per-\nformance at one-eighth the generator inference cost in prac-\ntical deployment settings.\nQ4: Does Med-TIV generalize across different base\nmodels?\nTo assess the generality of our proposed frame-\nwork, we compared verification performance using two\n7\n"}, {"page": 8, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nBase\nIter-1\nIter-2\n55\n60\n65\n70\n75\n80\nAccuracy (%)\nMedQA\nLlama-3.1-8B\nQwen-2.5-7B\nFigure 4. Ablation on base model selection and training iterations.\ndistinct verifier backbones: Llama3.1-8B and Qwen2.5-\n7B. As shown in Figure 4, both backbones achieve strong\nperformance after two training iterations. Llama3.1-8B\nconsistently outperforms Qwen2.5-7B by approximately\n3.5% absolute accuracy on MedQA, achieving 75.86% ver-\nsus 72.35% after 2 iterations of training. The parallel per-\nformance gains observed across both models indicate that\nMed-TIV is agnostic to backbone architectures.\nQ5: What is the impact of iterative training?\nFigure 4\npresents ablation results examining the impact of iterative\ntraining with adaptive curriculum formulation. Llama3.1-\n8B improves from 60.96% to 75.26% after iteration 1, with\nmarginal gains to 75.86% at iteration 2. Qwen2.5-7B fol-\nlows a similar pattern, reaching 72.35% after two iterations.\nThe rapid convergence suggest that the majority of verifica-\ntion capability is acquired in the first round, with subsequent\niterations refining boundary cases.\nTable 3. Ablation on RL and tool integration.\nModels\nMedQA\nQwen2.5-7B\n60.96\n+ Med-TIV (RL)\n69.60\n+ Med-TIV (RL + Tool)\n70.54\nAlphaMed-7B\n71.01\n+ Med-TIV (RL)\n76.12\n+ Med-TIV (RL + Tool)\n77.14\nQ6:\nHow\ndoes RL and\ntool\nintegra-\ntion\nimpact\nverification\nperformance?\nTable 3 high-\nlights the dual\nbenefits of our\nframework\nacross\ntwo\ngenerators. RL\ntraining drives the primary gain, boosting MedQA accuracy\nof Qwen2.5-7B by 8.64%, confirming that the verifier\neffectively internalizes reasoning patterns. Tool integration\nprovides a critical secondary boost, further elevating\naccuracy to 70.54%. A similar cumulative trend is observed\nwith AlphaMed-7B. This demonstrates that while RL\nanchors logical verification, dynamic retrieval is essential\nfor resolving knowledge-intensive boundary cases beyond\nthe model‚Äôs parametric memory.\n5. Related Work\nMedical Reasoning Models.\nThe application of large lan-\nguage models to medical reasoning has attracted consider-\nable attention. Early efforts focused on domain-adaptive\npretraining and instruction tuning on medical corpora (Wu\net al., 2023; Singhal et al., 2025; Chen et al., 2023). More\nrecent work has explored reasoning-enhanced medical mod-\nels. HuatuoGPT-o1 (Chen et al., 2024) incorporates chain-\nof-thought reasoning with verification mechanisms, and\nUltraMedical (Zhang et al., 2024) combines high-quality in-\nstruction data with preference optimization. AlphaMed (Liu\net al., 2025a) employs RL to improve medical reasoning ca-\npabilities. Despite these advances, most existing approaches\nfocus on improving the generator model itself, whereas our\nwork addresses the complementary problem of training a\nplug-and-play verifier that can improve any frozen generator\nthrough test-time search.\nTool-Assisted Reward and Judge Models.\nStandard\nLLM-based judges typically function as passive scorers lim-\nited by parametric knowledge. Recent work addresses this\nthrough agentic reward modeling, equipping verifiers with\nexecutable tools. Themis (Li et al., 2024) established the\nfoundational framework by enabling access to calculators,\nsearch engines, and knowledge bases through structured\ntool-calling traces. TIR-Judge (Xu et al., 2025) advanced\nthis paradigm in the general domain by integrating code ex-\necution to judge paired responses. TIM-PRM (Kuang et al.,\n2025) introduced independent tool queries for multi-modal\nverification to eliminate confirmation bias. The concept\nhas further expanded to the Agent-as-a-Judge paradigm\n(You et al., 2026), which employs dynamic planning, tool\naugmentation and multi-agent coordination to decompose\ncomplex evaluation tasks. Our work instantiates this agentic\nparadigm within the medical domain, moving beyond static\nretrieval to iterative, evidence-grounded clinical verification.\n6. Conclusion\nWe presented Med-TIV, an agentic RL framework for med-\nical reasoning verification. Our approach addresses key\nlimitations of existing medical reward models by offering\nexplicit critique traces and enabling dynamic knowledge re-\ntrieval during verification. Empirical evaluations across four\nmedical reasoning benchmarks demonstrate that Med-TIV\nsubstantially outperforms prior approaches. More broadly,\nMed-TIV introduces a general paradigm for training tool-\naugmented verifiers that can be extended to other high-\nstakes domains requiring evidence-grounded evaluation.\n8\n"}, {"page": 9, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nImpact Statement\nThis paper introduces research aimed at improving the re-\nliability of large language models for medical reasoning\ntasks. We believe our work contributes positively to the de-\nvelopment of trustworthy medical AI systems by providing\nmechanisms to verify reasoning correctness before clini-\ncal deployment. Med-TIV holds potential to enhance the\nsafety of LLM-assisted clinical decision support by reducing\nerroneous reasoning outputs through systematic verification.\nBy grounding judgments in retrieved medical evidence, our\napproach offers improved transparency compared to opaque\nscalar reward models, enabling practitioners to better un-\nderstand and audit verification decisions. The efficiency\ngains demonstrated by Med-TIV could democratize access\nto reliable medical reasoning verification, making robust\nverification feasible even in resource-constrained settings.\nAcknowledgement\nThis\nstudy\nwas\nsupported\nby\nthe\nNational\nInsti-\ntutes of Health awards UL1TR001857, U24TR004111,\nR01LM014588, and R01LM014306. The sponsors had\nno role in study design, data collection, analysis, interpre-\ntation, report writing, or decision to submit the paper for\npublication.\nReferences\nChen, J., Cai, Z., Ji, K., Wang, X., Liu, W., Wang, R.,\nHou, J., and Wang, B. Huatuogpt-o1, towards medical\ncomplex reasoning with llms, 2024. URL https://\narxiv.org/abs/2412.18925.\nChen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba,\nK., Salvi, F., Pagliardini, M., Fan, S., K¬®opf, A., Mo-\nhtashami, A., Sallinen, A., Sakhaeirad, A., Swamy, V.,\nKrawczuk, I., Bayazit, D., Marmet, A., Montariol, S.,\nHartley, M.-A., Jaggi, M., and Bosselut, A. Meditron-\n70b: Scaling medical pretraining for large language mod-\nels, 2023. URL https://arxiv.org/abs/2311.\n16079.\nGrattafiori, A. et al. The llama 3 herd of models, 2024. URL\nhttps://arxiv.org/abs/2407.21783.\nGuo, D. et al. Deepseek-r1 incentivizes reasoning in llms\nthrough reinforcement learning. Nature, 645(8081), 2025.\ndoi: 10.1038/s41586-025-09422-z.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J.\nMeasuring massive\nmultitask language understanding, 2021. URL https:\n//arxiv.org/abs/2009.03300.\nJi, Y., Ma, W., Sivarajkumar, S., et al.\nMitigating the\nrisk of health inequity exacerbated by large language\nmodels.\nnpj Digital Medicine, 8:246, 2025a.\ndoi:\n10.1038/s41746-025-01576-4.\nJi, Y., Zhang, H., and Wang, Y. Bias evaluation and mitiga-\ntion in retrieval-augmented medical question-answering\nsystems, 2025b. URL https://arxiv.org/abs/\n2503.15454.\nJiang, D., Lu, Y., Li, Z., Lyu, Z., Nie, P., Wang, H., Su,\nA., Chen, H., Zou, K., Du, C., Pang, T., and Chen, W.\nVerltool: Towards holistic agentic reinforcement learning\nwith tool use, 2025a. URL https://arxiv.org/\nabs/2509.01055.\nJiang, S., Liao, Y., Chen, Z., Zhang, Y., Wang, Y., and\nWang, Y. Meds3: Towards medical slow thinking with\nself-evolved soft dual-sided process supervision, 2025b.\nURL https://arxiv.org/abs/2501.12051.\nJin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D.,\nZamani, H., and Han, J. Search-r1: Training llms to\nreason and leverage search engines with reinforcement\nlearning, 2025. URL https://arxiv.org/abs/\n2503.09516.\nJin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and\nSzolovits, P. What disease does this patient have? a\nlarge-scale open domain question answering dataset from\nmedical exams, 2020. URL https://arxiv.org/\nabs/2009.13081.\nJin, Q., Kim, W., Chen, Q., Comeau, D. C., Yeganova,\nL., Wilbur, W. J., and Lu, Z.\nMedcpt:\nCon-\ntrastive pre-trained transformers with large-scale pubmed\nsearch logs for zero-shot biomedical information re-\ntrieval.\nBioinformatics, 39(11), November 2023.\nISSN 1367-4811.\ndoi:\n10.1093/bioinformatics/\nbtad651.\nURL http://dx.doi.org/10.1093/\nbioinformatics/btad651.\nKhatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri,\nS. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., and\nAgarwal, R. The art of scaling reinforcement learning\ncompute for llms, 2025. URL https://arxiv.org/\nabs/2510.13786.\nKuang, P., Wang, X., Liu, W., Dong, J., and Xu, K. Tim-\nprm: Verifying multimodal reasoning with tool-integrated\nprm, 2025. URL https://arxiv.org/abs/2511.\n22998.\nLangley, P.\nCrafting papers on machine learning.\nIn\nLangley, P. (ed.), Proceedings of the 17th International\nConference on Machine Learning (ICML 2000), pp.\n1207‚Äì1216, Stanford, CA, 2000. Morgan Kaufmann.\nLi, L., Chai, Y., Wang, S., Sun, Y., Tian, H., Zhang, N., and\nWu, H. Tool-augmented reward modeling, 2024. URL\nhttps://arxiv.org/abs/2310.01045.\n9\n"}, {"page": 10, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nLiu, C., Wang, H., Pan, J., Wan, Z., Dai, Y., Lin, F.,\nBai, W., Rueckert, D., and Arcucci, R.\nBeyond dis-\ntillation: Pushing the limits of medical llm reasoning\nwith minimalist rule-based rl, 2025a.\nURL https:\n//arxiv.org/abs/2505.17952.\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee,\nW. S., and Lin, M. Understanding r1-zero-like training:\nA critical perspective, 2025b. URL https://arxiv.\norg/abs/2503.20783.\nLiu, Z., Wang, P., Xu, R., Ma, S., Ruan, C., Li, P., Liu, Y.,\nand Wu, Y. Inference-time scaling for generalist reward\nmodeling, 2025c. URL https://arxiv.org/abs/\n2504.02495.\nOpenAI et al. Gpt-4o system card, 2024. URL https:\n//arxiv.org/abs/2410.21276.\nPal, A., Umapathi, L. K., and Sankarasubbu, M. Medmcqa\n: A large-scale multi-subject multi-choice dataset for\nmedical domain question answering, 2022. URL https:\n//arxiv.org/abs/2203.14371.\nShi, W., Xu, R., Zhuang, Y., Yu, Y., Sun, H., Wu, H., Yang,\nC., and Wang, M. D. Medadapter: Efficient test-time\nadaptation of large language models towards medical\nreasoning, 2024. URL https://arxiv.org/abs/\n2405.03000.\nSinghal, K., Tu, T., Gottweis, J., et al. Toward expert-\nlevel medical question answering with large language\nmodels. Nature Medicine, 31:943‚Äì950, 2025. doi: 10.\n1038/s41591-024-03423-7.\nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-\ntime compute optimally can be more effective than scal-\ning model parameters, 2024. URL https://arxiv.\norg/abs/2408.03314.\nWang, K., Fu, Z., Xin, W., Zhou, L., and Chandrappa, S. K.\nDigital voices of survival: From social media disclo-\nsures to support provisions for domestic violence victims.\narXiv preprint arXiv:2509.12288, 2025.\nWu, C., Lin, W., Zhang, X., Zhang, Y., Wang, Y., and Xie,\nW. Pmc-llama: Towards building open-source language\nmodels for medicine, 2023. URL https://arxiv.\norg/abs/2304.14454.\nXia, C., Wu, Q., Tian, S., and Hao, Y.\nParallelism\nmeets adaptiveness: Scalable documents understand-\ning in multi-agent llm systems, 2025.\nURL https:\n//arxiv.org/abs/2507.17061.\nXiao, W., Lian, J. J., Ouyang, K., Gu, S., Ke, Z.,\nWei, D., Sha, X., Wang, J., Fu, S., Qiu, M., and\nXu, C.\nNewton downhill optimizer with applica-\ntion to engineering optimization and breast cancer\nfeature selection.\nBiomedical Signal Processing\nand Control, 117:109184, 2026.\nISSN 1746-8094.\ndoi:\nhttps://doi.org/10.1016/j.bspc.2025.109184.\nURL\nhttps://www.sciencedirect.com/\nscience/article/pii/S1746809425016957.\nXiong, W., Zhao, W., Yuan, W., Golovneva, O., Zhang,\nT., Weston, J., and Sukhbaatar, S.\nStepwiser: Step-\nwise generative judges for wiser reasoning, 2025. URL\nhttps://arxiv.org/abs/2508.19229.\nXu, R., Chen, J., Ye, J., Wu, Y., Yan, J., Yang, C., and\nYu, H. Incentivizing agentic reasoning in llm judges\nvia tool-integrated reinforcement learning, 2025. URL\nhttps://arxiv.org/abs/2510.23038.\nYang, A. et al. Qwen2.5 technical report, 2025.\nYou, R., Cai, H., Zhang, C., Xu, Q., Liu, M., Yu, T., Li,\nY., and Li, W. Agent-as-a-judge, 2026. URL https:\n//arxiv.org/abs/2601.05111.\nYun, J., Sohn, J., Park, J., Kim, H., Tang, X., Shao, Y., Koo,\nY., Ko, M., Chen, Q., Gerstein, M., Moor, M., and Kang,\nJ. Med-prm: Medical reasoning models with stepwise,\nguideline-verified process rewards, 2025. URL https:\n//arxiv.org/abs/2506.11474.\nZhang, H., Lou, Q., and Wang, Y.\nTowards safe ai\nclinicians: A comprehensive study on large language\nmodel jailbreaking in healthcare, 2025. URL https:\n//arxiv.org/abs/2501.18632.\nZhang, K., Zeng, S., Hua, E., Ding, N., Chen, Z.-R.,\nMa, Z., Li, H., Cui, G., Qi, B., Zhu, X., Lv, X., Jin-\nfang, H., Liu, Z., and Zhou, B. Ultramedical: Build-\ning specialized generalists in biomedicine, 2024. URL\nhttps://arxiv.org/abs/2406.03949.\nZhao, X., Liu, S., Yang, S.-Y., and Miao, C.\nMedrag:\nEnhancing retrieval-augmented generation with knowl-\nedge graph-elicited reasoning for healthcare copilot, 2025.\nURL https://arxiv.org/abs/2502.04413.\nZuo, Y., Qu, S., Li, Y., Chen, Z., Zhu, X., Hua, E., Zhang,\nK., Ding, N., and Zhou, B. Medxpertqa: Benchmarking\nexpert-level medical reasoning and understanding, 2025.\nURL https://arxiv.org/abs/2501.18362.\n10\n"}, {"page": 11, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nA. Limitation\nWhile Med-TIV demonstrates substantial improvements over existing medical reasoning verification approaches, several\nlimitations warrant discussion and suggest directions for future research.\nProcess Supervision.\nOur current training paradigm relies solely on trace-level outcome rewards, providing no supervision\non intermediate verification behaviors such as when to search, what queries to formulate, or how to integrate retrieved\nevidence. While this design eliminates the need for costly step-level annotations, it may lead to suboptimal search patterns or\nredundant retrieval operations. Future work could explore supervision for the verification task itself, or leverage techniques\nsuch as search behavior cloning from stronger models to provide denser optimization signals.\nRetrieval Corpus Coverage.\nMed-TIV‚Äôs verification accuracy is inherently bounded by the coverage and quality of\nthe underlying medical corpus. Our retrieval system indexes documents from PubMed abstracts and medical textbooks,\nwhich provides broad coverage of established medical knowledge but may lack recent findings, rare disease information, or\nregion-specific clinical guidelines. Verification of reasoning traces involving cutting-edge treatments or highly specialized\nsubspecialties may be limited by corpus gaps.\nLanguage and Domain Scope.\nAll training and evaluation are conducted on English-language medical reasoning\nbenchmarks. The generalization of Med-TIV to multilingual medical content or non-Western medical traditions remains\nunexplored. Additionally, while our benchmarks span multiple medical subdomains, certain specialized areas such as\ngenomics, radiology interpretation, and surgical planning may require domain-adapted retrieval corpora for optimal\nverification performance.\nB. Additional Implementation Details\nB.1. Hyperparameter Settings\nTable 4 provides comprehensive hyperparameter configurations for Med-TIV training across both iterations. We maintain\nmostly consistent settings between iterations to isolate the effect of iterative training from hyperparameter tuning.\nTable 4. Hyperparameter configurations for Med-TIV training across iterations.\nHyperparameters\nIteration 1\nIteration 2\nRL Algorithm\nDr.GRPO\nDr.GRPO\nClip ratio (low / high)\n0.2 / 0.3\n0.2 / 0.3\nLearning rate\n1e-6\n1e-6\nWarmup steps\n10\n10\nTraining epochs\n5\n5\nGlobal batch size\n256\n256\nMini-batch size\n256\n256\nGroup size (G)\n5\n8\nRollout sampling temperature\n1.0\n1.0\nRollout top-p\n0.95\n0.95\nCurriculum filtering\nEnabled\nEnabled\nB.2. Retrieval Setup\nWe construct our retrieval infrastructure using a dense retrieval architecture optimized for medical domain queries. The\ncorpus is derived from the MedRAG (Zhao et al., 2025) collection, specifically combining the PubMed and Textbooks\nsubcorpora into a unified index. The PubMed subset contains approximately 23.9 million biomedical abstracts covering\nresearch publications, while the Textbooks subset includes content from standard medical textbooks spanning clinical\nmedicine, pharmacology, pathology, and related disciplines. After deduplication and quality filtering, the combined corpus\ncontains approximately 24 million snippets.\nWe employ MedCPT (Jin et al., 2023) as our dense retrieval encoder, specifically the query encoder variant for encoding\n11\n"}, {"page": 12, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nTable 5. Prompt template.\nUser Prompt:\nYou are a reasoning validator for medical problems. Your task is to think step by step and evaluate whether the given\nreasoning trace of a medical problem contains errors.\nFirst, you must always perform a step-by-step analysis to examine the entire reasoning process. Then, based on your\nanalysis, you will make a definitive judgment.\n- Use 1 if the reasoning trace is free of errors.\n- Use 0 if the reasoning trace contains one or more errors.\nOutput Instruction:\nYou must conduct your step-by-step analysis inside <think> and </think> first every time you get new information.\nAfter reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search>\nand it will return the top searched results between <information> and </information>. You can search as\nmany times as you want. If you find no further external knowledge needed, you can directly provide the answer inside\n<answer> and </answer>, without detailed illustrations.\nMedical Problem:\n{The full Medical Problem on one or more lines.}\nReasoning Trace:\n{The full Reasoning Trace on one or more lines.}\nsearch queries and article encoder for encoding corpus snippets. Document embeddings are pre-computed and stored in a\nFAISS index using the Flat configuration for maximum retrieval accuracy, distributed across multiple GPUs using FAISS‚Äôs\nGPU sharding capability to enable parallel similarity search. For each search query, we retrieve the top-3 most relevant\ndocuments for both training and inference.\nB.3. Baseline Setup\nWe describe the configuration of reward model baselines used in our experiments. For Med-PRM, which employs static\nretrieval-augmented generation, we equip it with the same retrieval corpus, encoder, and top-k setting as our framework to\nensure a controlled comparison. MedS3 does not support external tool invocation and is therefore evaluated without retrieval\naugmentation. For confidence score extraction and inference hyperparameter settings, we follow the configurations specified\nin each baseline‚Äôs original publication.\nB.4. Prompt Template\nWe design a structured prompt template that guides the verifier through systematic reasoning with explicit tool invocation\nsyntax. The complete prompt is shown in Table 5.\nC. Benchmarks and Baselines\nC.1. Benchmarks\nWe evaluate Med-TIV on four established medical reasoning benchmarks that collectively assess verification capability\nacross varying difficulty levels and medical subdomains.\n‚Ä¢ MedQA (Jin et al., 2020): A dataset of multiple-choice questions derived from the United States Medical Licensing\nExamination (USMLE), designed to evaluate clinical reasoning and medical knowledge integration across diverse\nspecialties.\n‚Ä¢ MedMCQA (Pal et al., 2022): A large-scale multi-subject benchmark sourced from Indian medical entrance examinations\n(AIIMS and NEET-PG), covering 21 medical subjects with emphasis on factual knowledge and clinical application.\n‚Ä¢ MMLU-Med (Hendrycks et al., 2021): An aggregation of medical-related subsets from the Massive Multitask Language\nUnderstanding benchmark, encompassing anatomy, clinical knowledge, college biology, college medicine, medical\n12\n"}, {"page": 13, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\ngenetics, and professional medicine.\n‚Ä¢ MedXpertQA (Zuo et al., 2025): An expert-level benchmark featuring challenging questions that require multi-step\nclinical reasoning, differential diagnosis, and treatment planning at the level expected of practicing physicians.\nC.2. Baselines\nWe compare Med-TIV against comprehensive baselines spanning proprietary systems, general-purpose models, and domain-\nspecialized approaches.\nProprietary Models.\n‚Ä¢ GPT-4o-mini (OpenAI et al., 2024): A compact variant of OpenAI‚Äôs GPT-4o optimized for efficiency while maintaining\nstrong reasoning capabilities across diverse tasks.\n‚Ä¢ Gemini-2.0-Flash: Google‚Äôs efficient multimodal model designed for fast inference with competitive performance on\nknowledge-intensive benchmarks.\nGeneral Reasoning Models.\n‚Ä¢ DeepSeek-R1 (Guo et al., 2025): A 671B parameter reasoning model trained with RL, representing the current frontier of\nopen-weight reasoning capabilities.\n‚Ä¢ R1-Distill-Qwen / R1-Distill-Llama: Distilled variants of DeepSeek-R1 at 7B and 8B scales respectively, designed to\ntransfer reasoning capabilities to smaller architectures.\nGeneral Foundation Models.\n‚Ä¢ Qwen2.5 (Yang et al., 2025): A family of open-weight language models with strong multilingual and reasoning capabilities,\nevaluated at 7B and 32B parameter scales.\n‚Ä¢ Llama3.1 (Grattafiori et al., 2024): Meta‚Äôs open-source foundation model demonstrating competitive performance across\ndiverse benchmarks, evaluated at the 8B scale.\nMedical Domain Models.\n‚Ä¢ AlphaMed (Liu et al., 2025a): A medical reasoning model that employs RL with rule-based rewards to enhance clinical\nreasoning without reliance on distillation from larger models.\n‚Ä¢ UltraMedical (Zhang et al., 2024): A specialized medical model combining high-quality instruction tuning on curated\nbiomedical corpora with preference optimization for improved clinical accuracy.\n‚Ä¢ HuatuoGPT-o1 (Chen et al., 2024): A medical reasoning model incorporating chain-of-thought reasoning with internal\nverification mechanisms to improve diagnostic accuracy.\nMedical Reward Models.\n‚Ä¢ MedS3 (Jiang et al., 2025b): A self-evolved soft dual-sided process supervision framework for medical reasoning that\ngenerates training signals through iterative self-improvement without external annotations.\n‚Ä¢ Med-PRM (Yun et al., 2025): A process reward model for medical reasoning verification that provides step-level\nsupervision using static retrieval-augmented generation with guideline-based verification.\nD. Case Analysis\nTable 6 presents a complete verification example illustrating how a Med-TIV trained verifier identifies reasoning errors\nthrough dynamic evidence retrieval. The case involves a patient with bladder cancer who develops ototoxicity following\nchemotherapy. The generator‚Äôs reasoning trace incorrectly attributes the symptoms to taxanes based on their known associa-\ntion with ototoxicity, concluding with answer (B). However, the model retrieves evidence establishing that cisplatin‚Äîthe\nstandard neoadjuvant therapy for transitional cell carcinoma‚Äîis the causative agent, and its mechanism involves DNA\ncross-linking rather than microtubule hyperstabilization. Through iterative search and reasoning, verifier correctly identifies\nthe error, demonstrating the value of tool augmentation for catching subtle medical reasoning mistakes.\n13\n"}, {"page": 14, "text": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning\nTable 6. Complete verification demonstration. Given a medical problem and a reasoning trace, the verifier retrieves relevant evidence and\ncorrectly identifies the reasoning error within the trace.\nMedical Problem:\nQuestion: A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day\nhistory of ringing sensation in his ear. He received this first course of neoadjuvant chemotherapy 1 week ago. Pure\ntone audiometry shows a sensorineural hearing loss of 45 dB. The expected beneficial effect of the drug that caused\nthis patient‚Äôs symptoms is most likely due to which of the following actions?\nOptions: (A) Inhibition of proteasome (B) Hyperstabilization of microtubules (C) Generation of free radicals (D)\nCross-linking of DNA\nGenerator Reasoning Trace:\nThe patient‚Äôs presentation of ringing in the ear, known as tinnitus, and a sensorineural hearing loss of 45 dB, suggest\nototoxicity. Ototoxicity is a well-known side effect of certain chemotherapy drugs. Neoadjuvant chemotherapy refers\nto the use of chemotherapy before other treatments such as surgery. The patient has received a course of neoadjuvant\nchemotherapy 1 week ago, which aligns with the recent onset of ototoxic symptoms. To determine the cause of\nototoxicity from the provided options, we need to identify which of the options is likely to cause sensorineural\nhearing loss. Option (A), Inhibition of proteasome, is associated with drugs like bortezomib (Velcade), which are\nless commonly associated with ototoxicity. Option (B), Hyperstabilization of microtubules, is a known mechanism\nof taxanes (e.g., paclitaxel, docetaxel). Taxanes are known to cause ototoxicity, making this a potential candidate.\nOption (C), Generation of free radicals, is a general mechanism that can cause cellular damage but is not specifically\nknown to be associated with ototoxicity. Option (D), Cross-linking of DNA, is a known mechanism of alkylating\nagents like cyclophosphamide. While alkylating agents can cause ototoxicity, this mechanism is not as commonly\nlinked to ototoxicity as other options. Considering the options, the one most commonly associated with ototoxicity is\nhyperstabilization of microtubules. Therefore, the drug that most likely caused this patient‚Äôs symptoms is one that\nhyperstabilizes microtubules. The answer is (B).\nVerifier Judgment:\n<think> Let‚Äôs check the common chemotherapeutic agents used for bladder cancer and their known side effects.\nTransitional cell carcinoma of the bladder is often treated with cisplatin-based chemotherapy as neoadjuvant therapy.\nWe need to determine if any of the options match the known side effects of these drugs. Let‚Äôs search to confirm the\nside effects of cisplatin and other common bladder cancer chemotherapy drugs. </think>\n<search> common side effects of cisplatin chemotherapy </search>\n<information> [Search Results regarding Cisplatin toxicity and mechanisms...] </information>\n<think> From the information, cisplatin is known to cause ototoxicity, which aligns with the patient‚Äôs symptoms\nof sensorineural hearing loss. Now let‚Äôs check the mechanisms of action for the options: Option (A): Inhibition\nof proteasome - This is associated with drugs like bortezomib and is not used for bladder cancer. Option (B):\nHyperstabilization of microtubules - This is the mechanism of taxanes (e.g., paclitaxel, docetaxel). Option (C):\nGeneration of free radicals - This is not specific to a single mechanism and is not commonly associated with ototoxicity.\nOption (D): Cross-linking of DNA - This is the mechanism of alkylating agents (e.g., cyclophosphamide). Cisplatin is\nan alkylating agent, and its mechanism involves cross-linking of DNA. Therefore, the correct answer should be (D).\n</think>\n<answer>0</answer>\n14\n"}]}