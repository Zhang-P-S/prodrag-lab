{"doc_id": "arxiv:2601.03471", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03471.pdf", "meta": {"doc_id": "arxiv:2601.03471", "source": "arxiv", "arxiv_id": "2601.03471", "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning", "authors": ["Mingyang Wei", "Dehai Min", "Zewen Liu", "Yuzhang Xie", "Guanchen Wu", "Carl Yang", "Max S. Y. Lau", "Qi He", "Lu Cheng", "Wei Jin"], "published": "2026-01-06T23:49:10Z", "updated": "2026-01-06T23:49:10Z", "summary": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03471v1", "url_pdf": "https://arxiv.org/pdf/2601.03471.pdf", "meta_path": "data/raw/arxiv/meta/2601.03471.json", "sha256": "3af9483f0ab66b4c0e084e5e2970c044068ec7a8b283a209531439355a004c06", "status": "ok", "fetched_at": "2026-02-18T02:22:49.602996+00:00"}, "pages": [{"page": 1, "text": "EPIQAL: Benchmarking Large Language Models in Epidemiological\nQuestion Answering for Enhanced Alignment and Reasoning\nMingyang Wei1, Dehai Min2, Zewen Liu1, Yuzhang Xie1, Guanchen Wu1,\nCarl Yang1, Max S.Y. Lau1, Qi He3, Lu Cheng2, Wei Jin1*\n1Emory University,\n2University of Illinois Chicago,\n3Microsoft\n{mingyang.wei, zewen.liu, yuzhang.xie, guanchen.wu}@emory.edu\n{j.carlyang, msy.lau, wei.jin}@emory.edu\n{dmin10, lucheng}@uic.edu,\nqhe@microsoft.com\nAbstract\nReliable epidemiological reasoning requires\nsynthesizing study evidence to infer disease\nburden, transmission dynamics, and interven-\ntion effects at the population level. Existing\nmedical question answering benchmarks pri-\nmarily emphasize clinical knowledge or patient-\nlevel reasoning, yet few systematically evalu-\nate evidence-grounded epidemiological infer-\nence. We present EpiQAL, the first diagnos-\ntic benchmark for epidemiological question\nanswering across diverse diseases, compris-\ning three subsets built from open-access liter-\nature. The subsets respectively evaluate text-\ngrounded factual recall, multi-step inference\nlinking document evidence with epidemiolog-\nical principles, and conclusion reconstruction\nwith the Discussion section withheld. Construc-\ntion combines expert-designed taxonomy guid-\nance, multi-model verification, and retrieval-\nbased difficulty control. Experiments on ten\nopen models reveal that current LLMs show\nlimited performance on epidemiological rea-\nsoning, with multi-step inference posing the\ngreatest challenge. Model rankings shift across\nsubsets, and scale alone does not predict suc-\ncess.\nChain-of-Thought prompting benefits\nmulti-step inference but yields mixed results\nelsewhere. EpiQAL provides fine-grained diag-\nnostic signals for evidence grounding, inferen-\ntial reasoning, and conclusion reconstruction.1\n1\nIntroduction\nThe COVID-19 pandemic underscored the chal-\nlenge of extracting reliable insights from a\nrapidly\nexpanding\nepidemiological\nliterature\n(Wang and Tian, 2021; Diéguez-Campa et al.,\n2020). Evidence-informed public health practice\nrequires decisions grounded in the best available\nscientific evidence, yet such decisions target com-\nmunities or populations rather than individual pa-\n*Correspondence: wei.jin@emory.edu\n1Benchmark and code are available at https://github.\ncom/myweiii/EpiQAL.\ntients and often demand synthesizing heteroge-\nneous, context-dependent study findings (Brown-\nson et al., 2009; Orton et al., 2011). Biomedical\nquestion answering (QA) systems have been de-\nveloped to help users retrieve and summarize ev-\nidence from large article collections (Bauer and\nBerleant, 2012; Tsatsaronis et al., 2015; Wallace,\n2019), but these systems primarily support clini-\ncal knowledge retrieval and patient-level decision\nmaking. Epidemiological reasoning, by contrast,\nrequires population-level statistical and causal in-\nference about disease burden, transmission dynam-\nics, and intervention effects (Glass et al., 2013).\nThis gap motivates QA benchmarks tailored to epi-\ndemiological inference.\nA suitable benchmark must satisfy two proper-\nties. First, it should be controlled, limiting shortcut\ncues that allow models to exploit superficial pat-\nterns such as lexical overlap between questions\nand contexts (Shinoda et al., 2021). Second, it\nshould be trustworthy, anchoring answers to veri-\nfiable study evidence rather than relying solely on\nannotator judgment. Current QA resources only\npartially meet these requirements. Exam-style clin-\nical benchmarks such as MedQA and MedMCQA\n(Jin et al., 2021; Pal et al., 2022) primarily test\nmedical knowledge, offering limited coverage of\nstudy-level inference over population distributions.\nLiterature-grounded datasets like PubMedQA (Jin\net al., 2019) link questions to research text but\nrely on abstracts and constrained label spaces,\nwhereas epidemiological questions may admit mul-\ntiple valid conclusions and require richer method-\nological context. Epidemic-focused datasets such\nas COVID-QA, CoQUAD, and EPIC-QA (Möller\net al., 2020; Raza et al., 2022a; Goodwin et al.,\n2022) provide valuable resources, yet they are fre-\nquently disease-specific, adopt extractive formats\nvulnerable to surface matching, and lack system-\natic verification that inferences reflect authentic\nepidemiological reasoning. Moreover, expert anno-\narXiv:2601.03471v1  [cs.CL]  6 Jan 2026\n"}, {"page": 2, "text": "tation remains costly, limiting both scale and topic\ncoverage.\nWe present EpiQAL, Epidemiological QA over\nthe Literature, the first benchmark that systemati-\ncally evaluates epidemiological QA by combining\nbroad topic coverage, multi-answer evaluation, and\ndocument-grounded answer derivation. Building\nEpiQAL requires addressing four challenges.\n(1) Scope. Epidemiological research spans diverse\nphenomena from outbreak detection to vaccine\neffectiveness evaluation. A benchmark limited\nto a single disease cannot assess generalization\nacross the field.\n(2) Grounding.\nEpidemiological conclusions\nmust be traceable to study evidence. With-\nout such grounding, it is difficult to distinguish\ngenuine inference from hallucination.\n(3) Verification. Epidemiological questions often\nadmit multiple valid answers. Validating multi-\nanswer correctness at scale without exhaustive\nexpert annotation requires automated quality\ncontrol.\n(4) Difficulty. Models can exploit superficial cues\nsuch as lexical overlap between question stems\nand correct options, succeeding without gen-\nuine comprehension.\nOur framework addresses each challenge. For\nscope, we develop a taxonomy of six categories and\ntwenty-five topics with epidemiology experts, cov-\nering phenomena from surveillance and outbreak\ninvestigation to transmission modeling and fore-\ncasting. For grounding, we adopt subset-specific\nstrategies that require correct options to be sup-\nported by explicit document evidence, including\na masked-input setting that withholds the Discus-\nsion section at test time. For verification, we de-\nsign a checking model group where multiple LLMs\nindependently verify factual consistency, routing\nuncertain cases to human review. For difficulty,\nwe employ difficulty screening and stem refine-\nment that replaces salient entities with descriptive\nphrases (Bai et al., 2024; Wu et al., 2025).\nEpiQAL comprises three subsets probing dif-\nferent capabilities.\nEpiQAL-A measures text-\ngrounded factual recall where correct answers are\nexplicitly stated in the document. EpiQAL-B tar-\ngets multi-step inference linking document evi-\ndence with epidemiological principles. EpiQAL-C\nevaluates conclusion reconstruction under masked\ninputs where the Discussion section is withheld\nat test time. Together, these subsets enable fine-\ngrained diagnosis of model behavior across evi-\ndence retrieval, inferential reasoning, and synthesis.\nOur contributions are as follows.\n• We formalize epidemiological QA as a distinct\nproblem requiring population-level reasoning\nover study evidence.\n• We develop an expert-curated taxonomy ensur-\ning broad coverage across epidemiological sub-\ndomains.\n• We propose an automated construction frame-\nwork integrating multi-LLM verification, diffi-\nculty control, and targeted human review.\n• We release EpiQAL with three subsets and bench-\nmark ten open LLMs under a multi-answer eval-\nuation protocol.\n2\nRelated Work\nBiomedical QA benchmarks. Existing biomed-\nical QA benchmarks vary in format, evidence\nsource, and domain scope.\nExam-style bench-\nmarks such as MedQA and MedMCQA use single-\nanswer multiple-choice questions to test broad med-\nical knowledge (Jin et al., 2021; Pal et al., 2022).\nBioASQ provides expert-curated questions with\nsummaries and exact answers grounded in biomed-\nical literature (Krithara et al., 2023), while Pub-\nMedQA links questions to abstracts but adopts a\nconstrained yes/no/maybe label space that limits\nexpressiveness (Jin et al., 2019). Epidemic-focused\nbenchmarks such as COVID-QA, CoQUAD, and\nEPIC-QA ground questions in pandemic-related\nevidence but are typically disease-specific and use\nextractive formats (Möller et al., 2020; Raza et al.,\n2022b; Goodwin et al., 2022). In contrast, EpiQAL\ncovers diverse epidemiological topics, supports\nmulti-answer evaluation, and includes a masked-\ninput setting for conclusion reconstruction.\nAutomatic QA construction and quality con-\ntrol. Automatic QA construction has evolved from\ntemplate-based generation to neural pipelines con-\nditioned on passages (Du et al., 2017), with recent\nwork improving distractor plausibility for multiple-\nchoice formats (Lee et al., 2025). To reduce annota-\ntion artifacts and shortcut cues, model-in-the-loop\ncollection and adversarial filtering select harder or\nless biased instances (Bartolo et al., 2020; Kiela\net al., 2021; Bras et al., 2020), while multi-judge\nLLM verification helps mitigate single-model bi-\nases in quality control (Liu et al., 2023; Ma et al.,\n2025). For settings admitting multiple valid an-\nswers, benchmarks such as HotpotQA adopt set-\nbased F1 and Exact Match metrics (Yang et al.,\n"}, {"page": 3, "text": "QA Generation & Verification\nQuestion\nAnswer\nDistractors\nDoes the option make sense?\nReject\nMulti-Model \nVerification \nHuman\nReview\nEpiQAL-A&B&C\nStem Refinement\nQuestion\nCore Entity\nSearch\nSnippets\nDescription\nReplace\nNew\nQuestion\nDifficulty Judging\nRefinement\nEpiQAL-B&C\nSummarize\nDomain Knowledge Augmentation\nKG\nInput Doc\nDisease Name\nNER\nSummarize\nExternal\nInformation\nTaxonomy Constraint\nCategorize\nClass\nDomain \nTopic\nDefines \nprevalence, \nincidence...\nEpiQAL-B\nEpiQAL-A&B\nPaper Structure Constraint\nDiscussion\nRest Doc\nExtract\nConclusions\nKey Insight\nTest Input\nEpiQAL-C\nInput Doc\nInput Doc\nRetrieve\nCategorize\nAnswer\nVote Ratio v \nAccept\nMultiple LLMs solve the question \nDiffScore < \nEpiQAL-B&C\nLLM\nInput Constraints\nDifficulty Control\nFigure 1: Overall framework for EpiQAL construction. The pipeline begins with subset-specific input processing\n(upper left), followed by QA generation and multi-model verification that routes uncertain cases to human review\n(upper right). For EpiQAL-B&C, difficulty judging screens overly easy instances and triggers stem refinement when\nneeded (lower). EpiQAL-A outputs directly after verification.\n2018), and LIQUID demonstrates automatic multi-\nanswer evaluation at scale (Lee et al., 2023). Long-\nBench v2 further incorporates difficulty screen-\ning into benchmark construction (Bai et al., 2024).\nEpiQAL builds on these advances by combining\ntaxonomy-guided generation with multi-LLM veri-\nfication and difficulty control.\n3\nMethod\n3.1\nTask Formulation\nWe now define two tasks: dataset generation and\nbenchmarking.\nDataset generation. Given a source document\nD, the goal is to produce a question Q, a set of\ncorrect options Oc, and a set of distractors Od. We\nformulate this as constrained generation where a\nmodel Mg operates under a constraint schema G\nthat specifies topic scope, reasoning requirements,\nand option construction rules:\n(Q, Oc, Od) = Mg(D, E; G)\n(1)\nHere E denotes optional external knowledge. For\nEpiQAL-B, E consists of epidemiological relations\nfrom knowledge graphs used only during construc-\ntion; for EpiQAL-A and EpiQAL-C, E is empty.\nSection 3.4 details the constraint schema G and its\nsubset-specific instantiations.\nBenchmarking. The evaluation task is multiple\nchoice QA where multiple options may be correct.\nLet ˜D denote the test-time input. For EpiQAL-\nA and EpiQAL-B, ˜D = D. For EpiQAL-C, the\nDiscussion section Dd ⊂D is masked so that ˜D =\nD \\ Dd. Given ˜D, question Q, and candidates O =\nOc ∪Od, a tested model Mt predicts an answer\nset A: A = Mt( ˜D, Q, O). We allow A = ∅to\nrepresent abstention, and include instances where\nOc = ∅so that no option is correct. This design\npenalizes indiscriminate guessing. Evaluation uses\nset-based Exact Match: EM = 1[A = Oc].\n3.2\nFramework Overview\nEpidemiological reasoning spans a spectrum from\nretrieving stated facts to synthesizing conclusions\nfrom partial observations. To diagnose where mod-\nels succeed or fail along this spectrum, we design\nthree subsets that isolate distinct capabilities: text-\ngrounded recall in EpiQAL-A, multi-step inference\nin EpiQAL-B, and conclusion reconstruction under\nmasked inputs in EpiQAL-C.\nFigure 1 illustrates the construction pipeline. All\nthree subsets share a core structure of input pro-\ncessing, QA generation, and multi-model verifi-\n"}, {"page": 4, "text": "Table 1: Comparison of the three subsets in EpiQAL.\nEpiQAL-A\nEpiQAL-B\nEpiQAL-C\nCore Capability\nFact recall\nMulti-step inference\nConclusion reconstruction\nKnowledge Source\nDocument\nDocument + KG\nPaper structure\nTaxonomy Guided\nYes\nYes\nNo\nExternal Knowledge\nNo\nGeneration only\nNo\nTest Input\nFull document\nFull document\nDocument w/o Discussion\nDifficulty Control\nNo\nYes\nYes\ncation, while EpiQAL-B and EpiQAL-C undergo\nadditional difficulty control. The pipeline proceeds\nas follows: (1) subset-specific input processing\nderives supervision from taxonomy guidance or pa-\nper structure; (2) a generation model Mt produces\nQ, Oc, Od under explicit constraints G that enforce\nevidence grounding; (3) a multi-LLM checking\ngroup verifies factual consistency and option valid-\nity, routing uncertain cases to human review; (4)\ndifficulty control screens overly easy instances and\nrefines question stems when needed. Section 3.3\ndetails how each subset instantiates this pipeline.\nThese components address the construction chal-\nlenges identified in Section 1. The expert taxonomy\nensures broad topic coverage, addressing scope.\nSubset-specific constraints and evidence require-\nments yield traceable answers, addressing ground-\ning. Multi-model verification with human review\nenables scalable quality control, addressing veri-\nfication. Difficulty control reduces surface-level\nshortcuts, addressing difficulty. The following sub-\nsections detail each component.\n3.3\nSubset Design\nWe instantiate the framework into three subsets\nthat share a unified multiple-choice formulation\nbut differ in supervision source and test-time input\n˜D. Table 1 summarizes the key differences.\nEpiQAL-A: Text-grounded recall. EpiQAL-A\ncontains retrieval-based questions whose correct\noptions Oc are explicitly stated in the source doc-\nument D. Each correct option must be directly\nsupported by verbatim spans. Distractors Od are\ndocument-grounded confounders that match sur-\nface form but differ in role, population, or context.\nEpiQAL-B: Multi-step inference. EpiQAL-B tar-\ngets inference that links multiple cues in D with\nepidemiological knowledge. During construction,\nexternal knowledge E from knowledge graphs elic-\nits inference-oriented questions, but evaluation pro-\nvides only ˜D = D. Correct options Oc express de-\nrived implications rather than passage restatements.\nDistractors Od contain reasoning-level flaws such\nas causal reversal or entity misattribution.\nEpiQAL-C: Masked-input reasoning. EpiQAL-\nC evaluates reconstruction of author-stated conclu-\nsions when the Discussion section Dd is masked, so\n˜D = D\\Dd. Correct options Oc are salient conclu-\nsions extracted from Dd, but must be supportable\nby evidence in ˜D. Distractors Od are plausible\nunder the paper narrative but unsupported, contra-\ndictory, or logically inverted.\nAppendix A.4 provides detailed distractor design\nprinciples for each subset.\n3.4\nInput Constraints\nEpidemiology Taxonomy. To ensure broad cov-\nerage across epidemiological subdomains, we de-\nvelop a taxonomy with domain experts that de-\nfines question scope and guides generation for\nEpiQAL-A and EpiQAL-B. The taxonomy reflects\nthe workflow of epidemiological inquiry, empha-\nsizing population-level evidence synthesis rather\nthan individual-level clinical reasoning.\nThe taxonomy is organized into six high-level\nclasses covering complementary stages of epidemi-\nological investigation. Surveillance and Descrip-\ntive Epidemiology characterizes disease occurrence\nthrough rates, temporal trends, and demographic\npatterns.\nOutbreak Investigation and Field Re-\nsponse addresses case confirmation, attack rates,\nsource attribution, and immediate control measures.\nDeterminants and Exposures examines how expo-\nsure arises across behavioral, environmental, and\nsocial contexts. Susceptibility and Immunity de-\nscribes who is susceptible, correlates of protection,\nand vaccine effectiveness. Modeling, Methods, and\nEvaluation covers transmission modeling, study de-\nsign, bias handling, and program evaluation. Pro-\njections and Forecasts produces forward-looking\npredictions and supports decision making.\nEach class contains multiple topics that provide\nfiner-grained control over question intent.\nFor\nEpiQAL-A and EpiQAL-B, we sample a topic and\n"}, {"page": 5, "text": "use its description to steer evidence selection, ques-\ntion phrasing, and option design. EpiQAL-C de-\nrives supervision from paper structure rather than\ntaxonomy guidance, as its goal is to reconstruct\nauthor-stated conclusions regardless of topic. The\ncomplete taxonomy with all 25 topics and their\ndescriptions is provided in Appendix A.2.\nDomain Knowledge Augmentation. EpiQAL-\nB incorporates external knowledge E during con-\nstruction to encourage multi-evidence inference-\noriented questions and harder distractors. We ex-\ntract disease entities from the source document D,\nlink them to biomedical knowledge graphs, and\nsummarize related triples into natural language sig-\nnals. These signals help elicit questions whose\nsolution requires bridging document evidence with\nepidemiological principles. At evaluation time, E\nis withheld, so that success requires models to use\nparametric knowledge rather than relying on pro-\nvided signals. Appendix A.3 details the construc-\ntion procedure.\n3.5\nConstrained QA Generation\nWe define a constraint schema G to control ques-\ntion and option construction. The schema consists\nof three components: a topic constraint, a logic\nconstraint, and option constraints. External knowl-\nedge E is provided separately for EpiQAL-B (Sec-\ntion 3.4). The schema structure is shared across\nsubsets, while subset-specific instantiations differ-\nentiate text-grounded recall, multi-step inference,\nand masked conclusion reconstruction.\nTopic constraint. Topic constraint includes a Tax-\nonomy Constraint and a Paper Structure Constraint.\nFor EpiQAL-A and EpiQAL-B, the selected tax-\nonomy topic restricts generation to the intended\nepidemiological phenomenon. EpiQAL-C derives\nsupervision from paper structure and does not use\ntopic guidance.\nLogic constraint. The logic constraint specifies\nwhat constitutes a valid reasoning demand in the\nquestion stem Q and is the main mechanism for dif-\nferentiating the three subsets. In EpiQAL-A, stems\nare restricted to retrieval-style questions whose an-\nswers are explicitly stated in D. In EpiQAL-B,\nstems require synthesis-style questions that com-\nbine multiple pieces of document evidence with\nepidemiological principles. In EpiQAL-C, stems\nrequire reconstruction of an author-stated conclu-\nsion by reasoning over observations when Dd is\nmasked.\nOption constraint.\nThe constraint on correct\noptions Oc enforces evidence consistency, with\nsubset-specific rules. For EpiQAL-A and EpiQAL-\nB, Oc must be supported by document evidence.\nEpiQAL-B further requires that Oc express derived\nimplications rather than restatements of passage\nfacts. For EpiQAL-C, Oc are salient conclusions\nextracted from Dd. The constraint on distractors\nOd requires semantic and stylistic similarity to Oc\nwhile introducing controlled errors. EpiQAL-A\nuses document-grounded confounders that match\nsurface form but differ in role or context. EpiQAL-\nB uses reasoning-level adversarial errors such as\nentity misattribution or causal reversal. EpiQAL-\nC uses plausible traps that are unsupported by ˜D,\ncontradictory, or logically inverted.\nAppendix F provides the generation prompts for\neach subset.\n3.5.1\nMulti-model Verification\nAutomatically generated QA instances may contain\nfactual errors, label inconsistencies, or reasoning\nflaws. We address this through multi-model verifi-\ncation combined with targeted human review.\nChecking model group. A group of LLMs inde-\npendently verifies each generated option in Oc∪Od.\nCheckers assess two properties: whether the option\nis consistent with its assigned label given the cited\nevidence, and whether the implied reasoning is co-\nherent. Checkers operate at the option level rather\nthan re-solving the full question, which allows effi-\ncient verification at scale.\nTo ensure that correctness does not depend on\nconstruction-only information, we require that ac-\ncepted options be evidence-consistent with the test-\ntime input ˜D.\nFor EpiQAL-A and EpiQAL-B,\n˜D = D. For EpiQAL-C, ˜D = D \\ Dd. Although\nEpiQAL-C correct options are extracted from Dd,\ncheckers require that they be supported by spans in\n˜D.\nWe run each checker multiple times with stochas-\ntic decoding and aggregate decisions into a vote\nratio v ∈[0, 1] representing the fraction of keep\nvotes. Two thresholds govern the decision process:\noptions below the lower threshold are rejected au-\ntomatically, options above the upper threshold are\naccepted, and options in between are flagged for\nhuman review. This tiered approach balances au-\ntomation with quality control.\nHuman Review. Full manual auditing is infeasi-\nble at scale, so we reserve expert effort for uncer-\ntain cases. For flagged options, human reviewers\n"}, {"page": 6, "text": "inspect the evidence attribution and option label,\nthen either approve or discard the instance. This\npolicy concentrates expert attention on high-risk\ncases while keeping overall annotation cost modest.\n3.6\nDifficulty Control\nFor EpiQAL-B and EpiQAL-C, quality also de-\npends on whether items demand nontrivial reason-\ning. We apply difficulty control only to these two\nsubsets because EpiQAL-A targets text-grounded\nrecall rather than reasoning depth. Difficulty con-\ntrol consists of two steps: difficulty judging to\nidentify overly easy items, and stem refinement\nto reduce shortcut cues.\nDifficulty judging. We estimate instance difficulty\nusing a pool of models ranging from small to large.\nFor each model, we compare the predicted answer\nset A with the reference set Oc using set-based F1\nand Exact Match (Appendix A.1), then combine\nthem into a difficulty score:\nDiffScore = 1 −(α · F1 + (1 −α) · EM)\nwhere α ∈[0, 1] controls the trade-off between\npartial overlap and exact set recovery. We average\nDiffScore across the model pool. Items below a\nthreshold are treated as easy and passed to stem\nrefinement.\nStem refinement. Stem refinement is a rewriting\nstep that replaces salient entities in the question\nstem Q with descriptive phrases. This reduces sur-\nface matching between Q and Oc, requiring models\nto reason about the described concept rather than\npattern match on entity names. For example, a ques-\ntion mentioning cutaneous leishmaniasis might be\nrewritten to describe it as a vector-borne skin dis-\norder caused by Leishmania parasites transmitted\nvia sandfly bites. The rewritten stem preserves\nanswerability while increasing discriminative dif-\nficulty. No retrieved text is provided to models at\nevaluation time.\nThe refinement procedure iteratively extracts a\ncore entity from Q, retrieves its definition from web\nsources, and replaces the entity with a summarized\ndescription. This process repeats until DiffScore\nexceeds the threshold or a maximum number of\niterations is reached. Appendix B.1 provides the\ndetailed procedure, and Appendix B.2 analyzes\nthe effect of refinement iterations on model perfor-\nmance.\n4\nExperiment\nWe evaluate EpiQAL from three perspectives. First,\nwe report dataset statistics and construction effi-\nciency. Second, we benchmark a diverse set of\nopen-source models on the resulting subsets. Third,\nwe analyze the results and discuss implications for\nepidemiological QA evaluation.\n4.1\nGeneration Settings\nGeneration and verification. We use Qwen3-\n30B-A3B-Instruct-2507 as the generation model.\nFor EpiQAL-B, we extract disease entities using\nGLiNER and link them to knowledge graphs via\nSapBERT, with Llama-3.3-70B-Instruct summariz-\ning retrieved triples. Generated options are verified\nby a checking group of four models from different\nfamilies (GLM-4.5-Air, Mistral-Large, Llama-3.3-\n70B, Qwen3-30B), with decisions aggregated by\nvote ratio. Difficulty control uses a pool of nine\nmodels ranging from 3B to 110B parameters. Im-\nplementation details are provided in Appendix C.\nCorpus.\nWe build a corpus from the Jour-\nnal Archive of PLOS Neglected Tropical Dis-\neases (PLO, 2007–), collecting approximately\n10,600 research articles containing abstracts, main\ntext, author summaries, and acknowledgements.\nFor the main experiments, we use a randomly sam-\npled subset of 500 articles. All content is used\nunder the original open license.\nTable 2 summarizes dataset statistics. Each sub-\nset contains 500 instances with varying numbers of\noptions and correct answers. We allow instances\nwith an empty correct answer set, which penalizes\nguessing by requiring explicit abstention. Across\nall subsets, fewer than 4% of options require hu-\nman review, demonstrating the efficiency of multi-\nmodel verification. Additional analyses of class\nand topic coverage are provided in Appendix D.\n4.2\nEvaluation Protocol\nWe evaluate all models in a closed-book setting,\nproviding only the subset-specific input document\n˜D, the question Q, and the candidate options O.\nModels are instructed to select all correct options\nin a fixed output format. Although EpiQAL-C in-\nstances have on average one correct option, we do\nnot reveal this to models, preventing them from\nexploiting the single-answer structure as a shortcut.\nWe score only the final answer line and allow an\nempty set to represent abstention when no option\nis correct. We report set-based Exact Match (Ap-\n"}, {"page": 7, "text": "Table 2: Dataset statistics for each subset.\nSubset\nSamples\nAvg. #Options\nAvg. #Correct Options\n% Human Review\nEpiQAL-A\n500\n3.508\n1.432\n3.2%\nEpiQAL-B\n500\n2.898\n1.064\n3.4%\nEpiQAL-C\n500\n3.020\n0.998\n1.8%\npendix A.1), which equals 1 if the predicted set\nexactly matches the reference set and 0 otherwise.\nIn EpiQAL-C, the Discussion section is removed\nbefore evaluation. We use temperature 0.3 and\nreport results with and without Chain-of-Thought\nprompting. Chain-of-Thought adds a reasoning\ninstruction while preserving the same final answer\nformat.\nWe evaluate ten open models from five fam-\nilies:\nPhi-4-mini-instruct from Microsoft (Mi-\ncrosoft et al., 2025); Llama-3.2-3B-Instruct, Llama-\n3.1-8B-Instruct, and Llama-3.3-70B-Instruct from\nMeta (Grattafiori et al., 2024); Mistral-7B-Instruct-\nv0.3 and Mistral-Large-Instruct-2411 from Mistral\nAI (Jiang et al., 2023); Qwen3-8B, Qwen3-30B-\nA3B-Instruct-2507, and Qwen3-32B (Yang et al.,\n2025); and GLM-4.5-Air from Zhipu AI (Team\net al., 2025). Table 3 reports F1 Score and Exact\nMatch on all three subsets.\n4.3\nDiscussion\nTable 3 reports F1 and Exact Match across all three\nsubsets.\nCurrent LLMs show limited capabilities on epi-\ndemiological reasoning.\nThe best-performing\nmodels achieve Exact Match scores of 0.812 on\ntext-grounded recall, 0.760 on multi-step inference,\nand 0.800 on conclusion reconstruction. These\nnumbers fall well below the near-ceiling perfor-\nmance that state-of-the-art LLMs achieve on many\ngeneral NLP benchmarks. Most models score be-\nlow 0.70 on EpiQAL-B and EpiQAL-C, and the\nsmallest model Llama-3.2-3B scores below 0.15 on\nboth subsets. Epidemiological reasoning, which re-\nquires integrating scattered evidence with domain\nprinciples, remains unsolved by current LLMs.\nMulti-step inference is the key bottleneck.\nAmong the three reasoning types, multi-step in-\nference proves most difficult. EpiQAL-B scores\nrange from 0.094 to 0.760, and most models cluster\nbelow 0.70. Text-grounded recall and conclusion\nreconstruction yield higher scores, suggesting that\nmodels can retrieve explicit facts and generate plau-\nsible conclusions but struggle to integrate multiple\npieces of evidence into coherent inferences. This\nbottleneck likely reflects a fundamental limitation\nin how current architectures combine information\nacross long contexts with background knowledge.\nModel rankings shift across subsets. No single\nmodel dominates all three subsets. Mistral-Large\nleads on EpiQAL-A at 0.812 but drops to 0.574\non EpiQAL-B without CoT. Mistral-7B ranks be-\nlow average on EpiQAL-A at 0.632 but achieves\nthe best scores on both EpiQAL-B and EpiQAL-C.\nQwen3-30B-A3B shows the largest CoT gains on\nEpiQAL-B, improving from 0.568 to 0.720. These\nshifts suggest that text retrieval, evidence integra-\ntion, and conclusion reconstruction engage differ-\nent model capabilities. A single aggregate score\nwould obscure these distinctions.\nScale alone does not guarantee success. Mistral-\n7B outperforms Mistral-Large on both EpiQAL-B\nand EpiQAL-C by substantial margins. Llama-\n3.1-8B approaches Llama-3.3-70B on multi-step\ninference despite having fewer than one-eighth the\nparameters. At the same time, Llama-3.2-3B col-\nlapses on reasoning-intensive subsets while larger\nLlama models perform reasonably. These patterns\nsuggest a capability threshold below which mod-\nels cannot perform epidemiological reasoning, but\nabove which further scaling yields diminishing re-\nturns. Instruction tuning quality and architectural\nchoices appear to matter more than raw parameter\ncount.\nAnswer precision explains Mistral-7B’s success.\nMistral-7B achieves only moderate F1 scores but\nleads on Exact Match for EpiQAL-B and EpiQAL-\nC. The explanation lies in its F1-EM gap. Mistral-\n7B shows gaps of just 0.019 on EpiQAL-B and\n0.034 on EpiQAL-C, meaning it selects correct op-\ntions without over-selecting plausible distractors.\nLlama-3.1-8B achieves comparable F1 but shows\ngaps exceeding 0.35, losing substantially on Exact\nMatch because it hedges by selecting additional\noptions. For tasks where false positives carry sig-\nnificant costs, a model that abstains when uncertain\nmay outperform one that maximizes coverage.\nChain-of-Thought helps inference but not re-\n"}, {"page": 8, "text": "Table 3: F1 Score|Exact Match accuracy for each model across subsets, with and without Chain-of-Thought\nprompting.\nEpiQAL-A\nEpiQAL-B\nEpiQAL-C\nModel\nw/o CoT\nCoT\nw/o CoT\nCoT\nw/o CoT\nCoT\nMicrosoft\nPhi-4-mini-instruct\n0.772|0.494\n0.779|0.546\n0.654|0.240\n0.714|0.384\n0.726|0.410\n0.716|0.402\nMeta-Llama\nLlama-3.2-3B-Instruct\n0.473|0.308\n0.387|0.274\n0.402|0.120\n0.201|0.094\n0.270|0.124\n0.286|0.096\nLlama-3.1-8B-Instruct\n0.849|0.668\n0.856|0.698\n0.623|0.262\n0.751|0.584\n0.587|0.204\n0.694|0.382\nLlama-3.3-70B-Instruct\n0.826|0.676\n0.822|0.696\n0.778|0.588\n0.806|0.656\n0.779|0.552\n0.820|0.640\nMistral AI\nMistral-7B-Instruct-v0.3\n0.736|0.632\n0.742|0.632\n0.779|0.760\n0.742|0.732\n0.814|0.780\n0.811|0.800\nMistral-Large-Instruct-2411\n0.910|0.812\n0.911|0.810\n0.806|0.574\n0.828|0.650\n0.794|0.574\n0.801|0.588\nQwen\nQwen3-8B\n0.843|0.712\n0.865|0.764\n0.681|0.442\n0.747|0.562\n0.663|0.478\n0.708|0.500\nQwen3-30B-A3B-Instruct-2507\n0.893|0.784\n0.892|0.796\n0.771|0.568\n0.846|0.720\n0.720|0.526\n0.769|0.586\nQwen3-32B\n0.888|0.780\n0.886|0.768\n0.821|0.676\n0.814|0.672\n0.747|0.506\n0.750|0.524\nZhipu AI\nGLM-4.5-Air\n0.863|0.766\n0.849|0.754\n0.728|0.586\n0.754|0.612\n0.705|0.558\n0.635|0.526\ntrieval. CoT prompting substantially improves per-\nformance on EpiQAL-B for most models. Llama-\n3.1-8B improves from 0.262 to 0.584, and Qwen3-\n30B-A3B improves from 0.568 to 0.720.\nOn\nEpiQAL-A, CoT produces no consistent benefit.\nOn EpiQAL-C, results are mixed. Explicit rea-\nsoning steps appear to help when models must\nintegrate multiple evidence pieces but add little\nvalue for direct retrieval. Two exceptions stand\nout. First, CoT harms Llama-3.2-3B across all sub-\nsets, suggesting that small models lack the capacity\nto benefit from explicit reasoning. Second, CoT\nslightly degrades Mistral-7B on EpiQAL-B from\n0.760 to 0.732, possibly because explicit reason-\ning interferes with its already-calibrated implicit\ninference.\nGenerator bias does not dominate results.\nEpiQAL-B is constructed using a Qwen model as\nthe generator, raising the possibility of generator-\nfavoring artifacts. However, Mistral-7B from a dif-\nferent model family achieves the highest score on\nthis subset. Qwen models perform competitively\nbut do not lead. This cross-family result suggests\nthat the benchmark measures genuine reasoning\ncapabilities rather than superficial alignment with\nthe generator’s style.\nPractical implications.\nFor fact extraction,\nMistral-Large and Qwen3-32B perform best with-\nout needing CoT. For multi-step inference, Mistral-\n7B outperforms larger models and does not require\nCoT. For conclusion reconstruction with incom-\nplete evidence, Mistral-7B again leads. Deploy-\nments with strict precision requirements should\nprefer models with small F1-EM gaps. Systems\nwith limited compute should avoid models below\n7B parameters for reasoning tasks. These findings\nhighlight the value of task-specific evaluation over\nreliance on general benchmarks or scale assump-\ntions.\n5\nConclusion\nWe introduced EpiQAL, a benchmark for evidence-\ngrounded epidemiological question answering over\nresearch articles.\nOur construction framework\ncombines an expert-curated taxonomy, subset-\nspecific constraints for evidence grounding, multi-\nmodel verification, and difficulty screening. This\nyields three complementary subsets that isolate text-\ngrounded recall, multi-step inference, and conclu-\nsion reconstruction.\nExperiments across ten open models reveal that\ncurrent LLMs show limited capabilities on epidemi-\nological reasoning, with multi-step inference pos-\ning the greatest challenge. Model rankings shift\nacross subsets, and scale alone does not predict suc-\ncess. Chain-of-Thought prompting benefits multi-\nstep inference but yields mixed results elsewhere.\nThese findings support using EpiQAL as a diagnos-\ntic suite for epidemiological QA capabilities.\nWe release EpiQAL along with construction\ncode and baseline evaluations to facilitate future\nwork on evidence-grounded reasoning for public\nhealth.\n"}, {"page": 9, "text": "Limitations\nThis work has several limitations. First, our source\ncorpus is drawn solely from PLOS Neglected Trop-\nical Diseases, which may underrepresent domains\nsuch as respiratory surveillance, chronic disease\nepidemiology, and health policy. Second, we gener-\nate 500 instances per subset due to computational\nconstraints. Scaling up may surface new failure\nmodes on long-tail topics with sparse evidence.\nThird, EpiQAL-B is constructed using a single gen-\neration model from the Qwen family. Although the\ntop performer on this subset is Mistral-7B from a\ndifferent family, future work could explore cross-\nfamily or mixture-based generation to further re-\nduce potential generator-related artifacts. Fourth,\ndespite multi-model verification and targeted hu-\nman review, the benchmark may contain residual\nerrors from LLM-based generation. Fifth, we eval-\nuate open models up to approximately 110B param-\neters. Results may not transfer to larger proprietary\nsystems. Finally, EpiQAL remains a proxy for real-\nworld public health analysis, which often requires\nintegrating multiple documents and incorporating\ntemporal and geographic context beyond single-\narticle reasoning.\nReferences\n2007–. Plos neglected tropical diseases. Open Access\nJournal Archive.\nYushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xi-\naozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei\nHou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024.\nLongbench v2: Towards deeper understanding and\nreasoning on realistic long-context multitasks. arXiv\npreprint arXiv:2412.15204.\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebas-\ntian Riedel, and Pontus Stenetorp. 2020. Beat the ai:\nInvestigating adversarial human annotation for read-\ning comprehension. Transactions of the Association\nfor Computational Linguistics, 8:662–678.\nMichael Bauer and Daniel Berleant. 2012. Usability\nsurvey of biomedical question answering systems.\nHuman genomics, 6:17.\nAsma Ben Abacha and Dina Demner-Fushman. 2019. A\nquestion-entailment approach to question answering.\nBMC Bioinform., 20(1):511:1–511:23.\nBalu Bhasuran, Qiao Jin, Yuzhang Xie, Carl Yang,\nKarim Hanna, Jennifer Costa, Cindy Shavor, Wen-\nshan Han, Zhiyong Lu, and Zhe He. 2025. Prelim-\ninary analysis of the impact of lab results on large\nlanguage model generated differential diagnoses. npj\nDigital Medicine, 8(1):166.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bha-\ngavatula, Rowan Zellers, Matthew E. Peters, Ashish\nSabharwal, and Yejin Choi. 2020. Adversarial filters\nof dataset biases. Preprint, arXiv:2002.04108.\nRoss Brownson, Jonathan Fielding, and Christopher\nMaylahn. 2009. Evidence-based public health: A\nfundamental concept for public health practice. An-\nnual review of public health, 30:175–201.\nCarlos Diéguez-Campa, Iván Pérez-Neri, Gustavo\nReyes-Terán, Iliana Flores-Apodaca, Jorge Castillo\nLedon Pretelini, Omar Mercado-Bautista, Ricardo\nAlvarez Santana, Marco Zenteno, Brigham Bowles,\nand Angel Lee. 2020. The 2020 research pandemic:\nA bibliometric analysis of publications on covid-19\nand their scientific impact during the first months\nla pandemia de investigación del 2020: Un análisis\nbibliométrico de las publicaciones sobre covid-19\ny su impacto científico durante los primeros meses.\nArchivos de cardiología de México, 1.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1342–1352,\nVancouver, Canada. Association for Computational\nLinguistics.\nThomas Glass, Steven Goodman, Miguel Hernán, and\nJonathan Samet. 2013. Causal inference in public\nhealth. Annual review of public health, 34.\nTravis Goodwin, Dina Demner-Fushman, Kyle Lo, Lucy\nWang, Hoa Dang, and Ian Soboroff. 2022. Automatic\nquestion answering for multiple stakeholders, the\nepidemic question answering dataset. Scientific Data,\n9.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, and 542 others. 2024. The llama 3 herd of\nmodels. Preprint, arXiv:2407.21783.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. Preprint, arXiv:2009.03300.\nDaniel Scott Himmelstein, Antoine Lizee, Christine\nHessler, Leo Brueggeman, Sabrina L Chen, Dexter\nHadley, Ari Green, Pouya Khankhanian, and Ser-\ngio E Baranzini. 2017. Systematic integration of\nbiomedical knowledge prioritizes drugs for repurpos-\ning. eLife, 6:e26726.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\n"}, {"page": 10, "text": "pages 874–880, Online. Association for Computa-\ntional Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7b. Preprint,\narXiv:2310.06825.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019.\nPubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,\nZeerak Waseem, Pontus Stenetorp, Robin Jia, Mo-\nhit Bansal, Christopher Potts, and Adina Williams.\n2021. Dynabench: Rethinking benchmarking in nlp.\nPreprint, arXiv:2104.14337.\nAnastasia Krithara, Anastasios Nentidis, Konstantinos\nBougiatiotis, and Georgios Paliouras. 2023. Bioasq-\nqa: A manually curated corpus for biomedical ques-\ntion answering. Scientific Data, 10.\nSeongyun Lee, Hyunjae Kim, and Jaewoo Kang. 2023.\nLiquid: a framework for list question answering\ndataset generation.\nIn Proceedings of the Thirty-\nSeventh AAAI Conference on Artificial Intelligence\nand Thirty-Fifth Conference on Innovative Applica-\ntions of Artificial Intelligence and Thirteenth Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, AAAI’23/IAAI’23/EAAI’23. AAAI Press.\nYooseop Lee, Suin Kim, and Yohan Jo. 2025. Gen-\nerating plausible distractors for multiple-choice\nquestions via student choice prediction. Preprint,\narXiv:2501.13125.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Proceedings of the 34th Inter-\nnational Conference on Neural Information Process-\ning Systems, NIPS ’20, Red Hook, NY, USA. Curran\nAssociates Inc.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNLG evaluation using gpt-4 with better human align-\nment. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2511–2522, Singapore. Association for Com-\nputational Linguistics.\nChiyu Ma, Enpei Zhang, Yilun Zhao, Wenjun Liu,\nYaning Jia, Peijun Qing, Lin Shi, Arman Cohan,\nYujun Yan, and Soroush Vosoughi. 2025.\nJudg-\ning with many minds: Do more perspectives mean\nless prejudice?\non bias amplifications and resis-\ntance in multi-agent based llm-as-judge. Preprint,\narXiv:2505.19477.\nMicrosoft, :, Abdelrahman Abouelenin, Atabak Ash-\nfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach,\nJianmin Bao, Alon Benhaim, Martin Cai, Vishrav\nChaudhary, Congcong Chen, Dong Chen, Dong-\ndong Chen, Junkun Chen, Weizhu Chen, Yen-Chun\nChen, Yi ling Chen, Qi Dai, and 57 others. 2025.\nPhi-4-mini technical report: Compact yet powerful\nmultimodal language models via mixture-of-loras.\nPreprint, arXiv:2503.01743.\nDehai Min, Zhiyang Xu, Guilin Qi, Lifu Huang, and\nChenyu You. 2025. UniHGKR: Unified instruction-\naware heterogeneous knowledge retrievers. In Pro-\nceedings of the 2025 Conference of the Nations of\nthe Americas Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (Volume 1: Long Papers), pages 4577–4594,\nAlbuquerque, New Mexico. Association for Compu-\ntational Linguistics.\nTimo Möller, Anthony Reina, Raghavan Jayakumar,\nand Malte Pietsch. 2020. COVID-QA: A question\nanswering dataset for COVID-19. In Proceedings of\nthe 1st Workshop on NLP for COVID-19 at ACL 2020,\nOnline. Association for Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\n"}, {"page": 11, "text": "pages 4885–4901, Online. Association for Computa-\ntional Linguistics.\nLois Orton, Ffion Lloyd-Williams, David Taylor-\nRobinson, Martin O’Flaherty, and Simon Capewell.\n2011.\nThe use of research evidence in public\nhealth decision making processes: Systematic review.\nPLOS ONE, 6(7):e21704.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan\nSankarasubbu. 2022. Medmcqa: A large-scale multi-\nsubject multi-choice dataset for medical domain ques-\ntion answering. In Proceedings of the Conference\non Health, Inference, and Learning, volume 174 of\nProceedings of Machine Learning Research, pages\n248–260. PMLR.\nAnusri Pampari, Preethi Raghavan, Jennifer Liang, and\nJian Peng. 2018. emrqa: A large corpus for question\nanswering on electronic medical records. Preprint,\narXiv:1809.00732.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nShaina Raza, Brian Schwartz, and Laura Rosella. 2022a.\nCoquad: a covid-19 question answering dataset sys-\ntem, facilitating research, benchmarking, and prac-\ntice. BMC Bioinformatics, 23.\nShaina Raza, Brian Schwartz, and Laura Rosella. 2022b.\nCoquad: a covid-19 question answering dataset sys-\ntem, facilitating research, benchmarking, and prac-\ntice. BMC Bioinformatics, 23.\nConsoli S, Coletti P, Markov P, Orfei L, Biazzo I,\nSchuh L, Stefanovitch N, Bertolini L, Ceresa M,\nand Stilianakis N. 2025. An epidemiological knowl-\nedge graph extracted from the world health organi-\nzation’s disease outbreak news. SCIENTIFIC DATA,\n12(1):970.\nKazutoshi Shinoda, Saku Sugawara, and Akiko Aizawa.\n2021. Can question generation debias question an-\nswering models? a case study on question–context\nlexical overlap. In Proceedings of the 3rd Workshop\non Machine Reading for Question Answering, pages\n63–72, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nChang Su, Yu Hou, Suraj Rajendran, Jacqueline R. M. A.\nMaasch, Zehra Abedi, Haotan Zhang, Zilong Bai, An-\nthony Cuturrufo, Winston Guo, Fayzan F. Chaudhry,\nGregory Ghahramani, Jian Tang, Feixiong Cheng,\nYue Li, Rui Zhang, Jiang Bian, and Fei Wang. 2022.\nBiomedical discovery through the integrative biomed-\nical knowledge hub (ibkh). medRxiv.\n5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu\nHou, Bin Chen, Chengxing Xie, Cunxiang Wang,\nDa Yin, Hao Zeng, Jiajie Zhang, Kedong Wang,\nLucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao,\nXiaohan Zhang, Xuancheng Huang, Yao Wei, and\n152 others. 2025.\nGlm-4.5: Agentic, reasoning,\nand coding (arc) foundation models.\nPreprint,\narXiv:2508.06471.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael Alvers,\nDirk Weißenborn,\nAnastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, Yannis Almirantis, John Pavlopoulos, Nico-\nlas Baskiotis, Patrick Gallinari, Thierry Artieres,\nAxel-Cyrille Ngonga Ngomo, Norman Heino, Eric\nGaussier,\nLiliana Barrio-Alvers,\nand Georgios\nPaliouras. 2015. An overview of the bioasq large-\nscale biomedical semantic indexing and question an-\nswering competition. BMC Bioinformatics, 16:138.\nByron C. Wallace. 2019. What does the evidence say?\nmodels to help make sense of the biomedical liter-\nature. In Proceedings of the Twenty-Eighth Inter-\nnational Joint Conference on Artificial Intelligence,\nIJCAI-19, pages 6416–6420. International Joint Con-\nferences on Artificial Intelligence Organization.\nPanpan Wang and Deqiao Tian. 2021. Bibliometric\nanalysis of global scientific research on covid-19.\nJournal of Biosafety and Biosecurity, 3(1):4–9.\nJialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin,\nLiwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun\nXi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren\nZhou. 2025. Webdancer: Towards autonomous infor-\nmation seeking agency. Preprint, arXiv:2505.22648.\nYuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai\nShu, Fadi Nahab, Xiao Hu, and Carl Yang. 2025.\nKerap: A knowledge-enhanced reasoning approach\nfor accurate zero-shot diagnosis prediction using\nmulti-agent llms. Preprint, arXiv:2507.02773.\nYuzhang Xie, Jiaying Lu, Joyce Ho, Fadi Nahab, Xiao\nHu, and Carl Yang. 2024. Promptlink: leveraging\nlarge language models for cross-source biomedical\nconcept linking. In Proceedings of the 47th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 2589–\n2593.\nRan Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen\nXie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C.\nHo, Carl Yang, and Qi He. 2025. SimRAG: Self-\nimproving retrieval-augmented generation for adapt-\ning large language models to specialized domains.\nIn Proceedings of the 2025 Conference of the Na-\ntions of the Americas Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pages 11534–\n11550, Albuquerque, New Mexico. Association for\nComputational Linguistics.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 41\n"}, {"page": 12, "text": "others. 2025.\nQwen3 technical report.\nPreprint,\narXiv:2505.09388.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nUrchade Zaratiana, Nadi Tomeh, Pierre Holat, and\nThierry Charnois. 2024. GLiNER: Generalist model\nfor named entity recognition using bidirectional trans-\nformer. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 5364–5376,\nMexico City, Mexico. Association for Computational\nLinguistics.\nA\nAdditional Method Details\nA.1\nEvaluation Metrics\nLet setmodel denote the set of options predicted by\na model and setref denote the reference option set.\nWe compute\nF1 = 2 · |setreference ∩setmodel|\n|setreference| + |setmodel|\n(2)\nExactMatch =\n(\n1,\nif setmodel = setreference\n0,\notherwise\n(3)\nA.2\nEpidemiology Taxonomy\nThis appendix provides the complete taxonomy\nintroduced in Section 3.4. Each of the six classes\ncontains multiple topics, and each topic includes an\nexpert-curated description specifying its semantic\nscope. These descriptions serve as explicit con-\nstraints during question generation for EpiQAL-\nA and EpiQAL-B, steering the generation model\ntoward the intended epidemiological competency.\nThe taxonomy also supports topic-level analysis of\nmodel performance.\nTable 4 lists the six classes with their descrip-\ntions. Tables 5 through 7 provide all 25 topics\norganized by class.\nA.3\nExternal Knowledge Construction\nThis appendix describes how external knowledge E\nis constructed for EpiQAL-B. The procedure con-\nsists of four steps: entity extraction, entity linking,\ntriple retrieval, and summarization.\nWe first extract disease entities from the source\ndocument using GLiNER (Zaratiana et al., 2024).\nExtracted mentions are then normalized via en-\ntity linking using SapBERT (Liu et al., 2021),\nwhich is a SOTA biomedical entity linking method\n(Xie et al., 2024), to encode mentions and re-\ntrieve candidate entities. We retrieve related triples\nfrom two knowledge graphs: eKG-DONs (S et al.,\n2025), which compiles outbreak reports from offi-\ncial sources, and iBKH (Himmelstein et al., 2017;\nSu et al., 2022), which encodes broader biomedical\nrelations. Finally, a language model summarizes\nthe retrieved triples into compact natural language\nstatements used as generation signals (Xie et al.,\n2025).\nThese signals are used only during dataset con-\nstruction to steer the generation model toward\ninference-oriented questions. They are not pro-\nvided to models at evaluation time.\nA.4\nDistractor Design\nWe design distractors to be plausible under the pro-\nvided study context while remaining incorrect for\nthe specific question intent. Across all subsets, we\nenforce semantic type matching with correct op-\ntions, stylistic consistency, and diversity so that\ndifferent distractors reflect different confusable al-\nternatives rather than near duplicates. We attach\nevidence spans and brief rationales during construc-\ntion to support verification and error analysis.\nEpiQAL-A. Distractors in EpiQAL-A are passage-\ngrounded confounders. They are valid entities or\nfacts stated in the same document, matching the se-\nmantic category and tone of correct options. They\nare incorrect because they refer to a different role,\npopulation, setting, time window, or study context\nthan what the question requires. This design dis-\ncourages guessing by surface cues while preserving\na retrieval-based task in which all options are lo-\ncally supported by explicit spans.\nEpiQAL-B.\nDistractors\nin\nEpiQAL-B\nare\nreasoning-level adversaries.\nThey share the\ngrammatical structure and semantic category of\ncorrect options but express misleading implications\nthat require a reasoning process. We introduce\nsubtle flaws using three main categories:\n• Entity or attribution shift: a conclusion that\nholds for another entity in the passage is incor-\nrectly applied to the target entity.\n• Causal direction reversal: the direction of an\nimplied effect is flipped while keeping entities\n"}, {"page": 13, "text": "Cls\nClass\nDescription\n1\nSurveillance and Descriptive\nEpidemiology\nDescribes population occurrence from routine data, including rates, time place\nperson patterns, aberration signals, and basic system performance, without causal\nanalysis or forecasting.\n2\nOutbreak Investigation and Field\nResponse\nHandles outbreak specific confirmation, field case definitions, line lists, attack\nrates and curves, chain and source hypotheses, and immediate control with\nsituation reports.\n3\nDeterminants and Exposures\nExplains how exposure arises across settings, covering behavioral, environmen-\ntal, occupational, and social determinants, delineates canonical transmission\nroutes and contact structures, interprets exposure response with attention to\nmeasurement methods, units, detection limits, and thresholds, and situates risks\nwithin One Health interfaces involving reservoirs and vectors.\n4\nSusceptibility and Immunity\nDescribes who is susceptible and why, links serologic measures to correlates\nof protection, evaluates effectiveness after vaccination or prior infection and\nits waning with reinfection, hybrid immunity, and variant escape, including the\neffects of vaccine dose number and intervals, and assesses severity risk using\nclinical and contextual prognostic factors.\n5\nModeling, Methods, and\nEvaluation\nProvides analytical methods for transmission modeling and inference, real time\ndebiasing of surveillance data, study design and causal effects, measurement and\nbias handling, and program performance and burden evaluation.\n6\nProjections and Forecasts\nProduces forward looking forecasts and scenarios, evaluates and combines\nmodels, and supports decision making, it does not reconstruct recent under\nreported data.\nTable 4: Epidemiology taxonomy classes\nand study context fixed.\n• Principle mismatch: a correct passage fact is\ncombined with an incorrect epidemiological prin-\nciple to yield a plausible but wrong implication.\nConstruction-time external signals may validate the\nflawed reasoning chain but are not embedded as\nexplicit hints in the distractor text.\nEpiQAL-C. Distractors in EpiQAL-C are masked-\ninput traps tailored to the Discussion masking setup.\nWe draw candidates from either the non-Discussion\nsections or the Discussion, then refine them into\nself-contained sentences that are plausible but in-\ncorrect when only the non-Discussion sections are\navailable. We use five primary trap categories:\n• Limitations or future work: unproven hypothe-\nses that are not established as conclusions.\n• External literature dependence: claims sup-\nported only by cited outside work in the Discus-\nsion.\n• Background restatement: common knowledge\nrather than study-specific findings.\n• Incorrect conclusion: same entity but wrong\nconclusion under the question.\n• Causal reversal: reversed causal direction under\nthe study context.\nFor each distractor, we attach evidence revealing\nwhy it is not a valid answer under the masked-input\nsetting.\nB\nStem Refinement\nB.1\nProcedure\nStem refinement is a retrieval-based rewriting step\napplied during dataset construction. We adapt the\nrecursive retrieval approach from Wu et al. (2025)\nby iteratively replacing entities with their descrip-\ntions.\nThe procedure works as follows.\nFirst, we\nprompt a model to extract a core entity from the\nquestion stem as a replacement candidate. Second,\nwe construct a synthetic query to search for the\nentity’s definition and characteristics, retrieving the\ntop Kr relevant snippets from the web. Third, a\nmodel summarizes these snippets into a concise\ndescription that replaces the original entity in the\nstem. This process repeats until the DiffScore ex-\nceeds threshold θd or reaches the maximum number\nof iterations Tr. No retrieved text is provided to\nmodels at evaluation time; only the rewritten stem\nis used.\nB.2\nEffect on Model Performance\nTo isolate the effect of refinement, we construct\ncontrolled variants of EpiQAL-C by applying 0 to\nTr refinement iterations to the same base instances,\nregardless of whether they would be refined in the\nfinal pipeline. We evaluate each model with Chain-\nof-Thought prompting at temperature 0. Results\nare shown in Table 8.\n"}, {"page": 14, "text": "Cls\nClass\nTop\nTopic\nDescription\n1\nSurveillance and\nDescriptive Epidemiology\n1\nFrequency measures and\nstandardization\nDefines prevalence, incidence, person time, and\napplies standardization to make rates compara-\nble.\n2\nTime Place Person patterns,\nseasonality and clustering\nDescribes temporal trends, spatial distribution,\nand demographic profiles using routine popula-\ntion surveillance.\n3\nAberration and outbreak\ndetection\nBuilds statistical baselines and thresholds to flag\nunusual increases in counts, rates, or positivity,\nfocuses on signal detection rather than source\nattribution.\n4\nSystem performance,\ndeduplication and record\nlinkage\nAssesses sensitivity, timeliness, and complete-\nness, manages deduplication and linkage across\nmultiple data sources.\n2\nOutbreak Investigation and\nField Response\n1\nDiagnostic verification,\nfield case definitions and\nline lists\nConfirms the pathogen, applies field case defini-\ntions, and builds and cleans line lists.\n2\nEvent specific attack rates\nand epidemic curves\nQuantifies spread in defined groups and inter-\nprets epidemic curves for the event.\n3\nOutbreak hypothesis\nmapping and source\nattribution\nLinks cases by time, place, and shared expo-\nsures to identify likely sources and transmission\nchains, integrating line lists, environmental sam-\npling, traceback, and genomic evidence.\n4\nImmediate control and\nsituation reporting\nImplements urgent measures and documents\ncurrent status with concise situation reports.\nTable 5: Epidemiology taxonomy topics, Classes 1 and 2\nAs shown in Table 8, model performance de-\ncreases after refinement and generally continues\nto decline with additional iterations, though the\ndecrease becomes smaller over time. This pattern\nsuggests that iterative entity replacement increases\nreasoning difficulty by expanding the information\nmodels must integrate. Considering the trade-off\nbetween generation efficiency and difficulty gain,\nwe set Tr = 3.\nB.3\nExample\nTable 9 shows a representative instance before and\nafter refinement. Refinement replaces salient enti-\nties with descriptive phrases that preserve answer-\nability but remove direct lexical anchors. This\nrequires models to map descriptions back to the\ncorrect concepts and integrate evidence from the\npassage.\nIn Table 9, underlined text marks the entity\nselected for replacement at each iteration, and\nbold text indicates the retrieved description that\nreplaces the original surface form.\nIn Itera-\ntion 1, cutaneous leishmaniasis is replaced with\na descriptive paraphrase.\nIteration 2 expands\nLeishmania parasites into a higher-level descrip-\ntion while preserving question intent. In Iteration 3,\nneglected tropical diseases is replaced, further re-\nducing lexical overlap between the stem and source\nevidence. To answer correctly, models must iden-\ntify which epidemiological entity the description\nrefers to and use passage evidence to select the cor-\nrect options, rather than relying on surface-form\nmatching.\nC\nExperimental Details\nC.1\nCompute and Inference Settings\nExperiments run on NVIDIA H100 and H200\nGPUs. Llama-3.3-70B-Instruct, and GLM-4.5-Air\nuse four-bit inference, and all other models use\ndefault precision settings.\nC.2\nGeneration efficiency.\nAll experiments run on two NVIDIA H100 GPUs.\nGenerating 500 samples requires 43.78 hours for\nEpiQAL-A, 78.83 hours for EpiQAL-B, and 114.61\nhours for EpiQAL-C, corresponding to approxi-\nmately 5.3, 9.5, and 13.8 minutes per sample re-\nspectively. EpiQAL-B and EpiQAL-C take longer\nthan EpiQAL-A due to additional verification steps\nand difficulty control.\nCompared with expert-\nauthored annotation, the pipeline substantially re-\nduces human cost by routing only a small fraction\nof options to review.\n"}, {"page": 15, "text": "Cls\nClass\nTop\nTopic\nDescription\n3\nDeterminants and\nExposures\n1\nContextual determinants of\nexposure\nIntegrates individual behaviors with environ-\nmental, occupational, and social and structural\nconditions that shape exposure probability and\ninequities.\n2\nTransmission modes and\ncontact patterns\nDescribes general routes of spread and popula-\ntion contact structures across settings.\n3\nExposure response\ninterpretation\nSpecifies the exposure metric, determines\nwhether values are above or below assay limits\nand thresholds, and interprets exposure to in-\nfection, severity, or transmissibility patterns as\nreported in the passage.\n4\nZoonotic and One Health\ninterfaces, reservoirs and\nvectors\nIdentifies animal reservoirs, vectors, and human\nanimal environment interfaces where spillover\ncan occur.\n4\nSusceptibility and\nImmunity\n1\nSusceptibility stratification\nand special populations\nIdentifies groups more susceptible to infection\nbased on demographic and clinical traits and\nsetting specific contexts.\n2\nSerology and correlates of\nprotection\nEstimates seroprevalence and relates immune\nmarkers to protection thresholds and population\nlevel immunity.\n3\nProtection effectiveness,\nwaning, reinfection and\nimmune escape\nDescribes protection after vaccination or prior\ninfection, its change over time, risks of rein-\nfection, hybrid immunity, and variant related\nescape, considers how vaccine dose number and\ndose intervals influence vaccine effectiveness\nand its waning over time.\n4\nSeverity risk and prognostic\nfactors\nAssesses risk of severe outcomes conditional on\ninfection and stratifies prognosis by host factors.\nTable 6: Epidemiology taxonomy topics, Classes 3 and 4\nC.3\nPreprocessing.\nWe extract structured sections when available and\nnormalize raw text by removing reference lists and\nnon-content artifacts. Documents are assembled in\na fixed section order to reduce variance across in-\nstances. We drop papers with missing main text or\nabnormal formatting that prevents reliable section\nparsing.\nC.4\nModel Configuration.\nGeneration model. We use Qwen3-30B-A3B-\nInstruct-2507 as the generation model. For disease\nentity extraction, we use GLiNER (Zaratiana et al.,\n2024). For entity linking in EpiQAL-B construc-\ntion, we use SapBERT (Liu et al., 2021) to encode\nmentions and retrieve candidate disease entities\nfrom knowledge graphs. To summarize knowledge\ngraph triples into natural language signals, we use\nLlama-3.3-70B-Instruct. Generation temperature\nis set to 0 for reproducibility.\nChecking model group. We verify generated op-\ntions using instruction-tuned models from differ-\nent families: GLM-4.5-Air, Mistral-Large-Instruct-\n2411, Llama-3.3-70B-Instruct, and Qwen3-30B-\nA3B-Instruct-2507. Each checker runs 3 times\nwith temperature 1.0, and decisions are aggregated\ninto the vote ratio v defined in Section 3.5.1. We\nset the rejection threshold θc = 0.7 and acceptance\nthreshold θh = 0.8.\nDifficulty judging pool. To estimate difficulty as\ndescribed in Section 3.6, we evaluate a pool of mod-\nels ranging from small to large: Phi-4-mini-instruct,\nLlama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3,\nQwen3-8B, Llama-3.1-8B-Instruct, Qwen3-30B-\nA3B-Instruct-2507, Qwen3-32B, Llama-3.3-70B-\nInstruct, and GLM-4.5-Air. We compute DiffScore\nwith α = 0.7 and average across models. The diffi-\nculty threshold is θd = 0.9, maximum refinement\niterations Tr = 3, and retrieval budget Kr = 6\nsnippets.\nD\nDataset Analysis\nThis appendix provides additional analysis of\ndataset composition for EpiQAL-A and EpiQAL-B,\nwhich use taxonomy-guided generation. EpiQAL-\nC derives supervision from paper structure rather\nthan taxonomy and is not included in this analysis.\n"}, {"page": 16, "text": "Cls\nClass\nTop\nTopic\nDescription\n5\nModeling, Methods, and\nEvaluation\n1\nTransmission modeling and\ninference\nUses mechanistic or statistical models to esti-\nmate transmission parameters and infer trans-\nmission patterns.\n2\nReal time debiasing and\ndelay adjustment\nReconstructs recent incidence by adjusting for\nreporting delays, right truncation, and under\nascertainment.\n3\nStudy design and causal\neffects\nSelects designs and identification strategies and\ndefines effect measures for causal estimation.\n4\nMeasurement and bias\nhandling\nAddresses measurement validity, misclassifica-\ntion and measurement error, confounding and\nselection, generalizability, survey weighting,\nand sample size.\n5\nProgram performance and\nimpact evaluation\nAssesses coverage and implementation fidelity,\naudits routine data quality, evaluates real world\neffectiveness, and estimates disease burden.\n6\nProjections and Forecasts\n1\nNear term forecasting\nProduces short horizon probabilistic forecasts\nfor upcoming values and quantifies forecast un-\ncertainty.\n2\nScenario projections\nProjects future trajectories under stated assump-\ntions about policy, behavior, or immunity.\n3\nForecast evaluation and\nmodel combination\nAssesses forecast quality using proper scoring\nrules, calibration, and sharpness diagnostics,\nand develops or applies methods to combine\nmultiple forecasting models to improve predic-\ntive accuracy, stability, and robustness across\ncontexts.\n4\nDecision oriented\nforecasting and risk\ncommunication\nMaps forecast probabilities to operational\nthresholds or cost loss trade offs and communi-\ncates uncertainty for decision making.\nTable 7: Epidemiology taxonomy topics, Classes 5 and 6\nD.1\nClass Distribution\nFigure 2 shows the distribution of instances across\nthe six taxonomy classes. Both subsets achieve\nbroad coverage, with Surveillance and Descriptive\nEpidemiology and Determinants and Exposures\nbeing the most frequent classes. This distribution\nreflects the prevalence of these topics in the source\ncorpus of neglected tropical disease research.\nD.2\nTopic Distribution\nFigure 3 shows the distribution across all 25 top-\nics. Coverage is generally balanced, though some\nvariation exists due to the natural distribution of\ntopics in the source articles. Topics related to trans-\nmission modes, susceptibility, and disease burden\nappear most frequently.\nE\nAdditional Related Work\nMachine reading comprehension. Early work\non machine reading comprehension cast question\nanswering as span selection within controlled con-\ntexts, enabling precise evaluation of extractive mod-\nels (Rajpurkar et al., 2016; Joshi et al., 2017). With\nthe rise of instruction-tuned large language models,\ngeneration-based QA has become competitive, yet\nmultiple choice formats remain attractive because\nthey encourage targeted reasoning while preserv-\ning objective scoring (Nie et al., 2020; Hendrycks\net al., 2021). Scientific articles often restate conclu-\nsions with considerable lexical overlap, meaning\nthat purely extractive setups can overestimate gen-\nuine inference. This observation motivates evalua-\ntion formats that probe reasoning beyond surface\nmatching.\nAdditional biomedical QA resources. Beyond\nthe benchmarks discussed in the main text, sev-\neral resources address specific clinical needs. em-\nrQA constructs QA pairs from electronic medi-\ncal records using expert templates (Pampari et al.,\n2018). MedQuAD compiles question-answer pairs\nfrom trusted medical websites organized by topic\n(Ben Abacha and Demner-Fushman, 2019). These\ndatasets primarily target patient-level clinical rea-\nsoning rather than population-level epidemiologi-\ncal inference.\nRetrieval augmentation and knowledge re-\nsources. Retrieval-augmented generation grounds\nmodel outputs in retrieved passages and is often\n"}, {"page": 17, "text": "Table 8: Exact Match accuracy on EpiQAL-C across stem refinement iterations, w/o CoT.\nModel\nOriginal\nIter 1\nIter 2\nIter 3\nMicrosoft\nPhi-4-mini-instruct\n0.452\n0.436\n0.426\n0.410\nMeta-Llama\nLlama-3.2-3B-Instruct\n0.130\n0.096\n0.094\n0.124\nLlama-3.1-8B-Instruct\n0.274\n0.252\n0.238\n0.204\nMistral AI\nMistral-7B-Instruct-v0.3\n0.830\n0.806\n0.780\n0.780\nQwen\nQwen3-8B\n0.542\n0.502\n0.470\n0.478\nQwen3-30B-A3B-Instruct\n0.544\n0.518\n0.522\n0.526\nZhipu AI\nGLM-4.5-Air\n0.578\n0.558\n0.554\n0.558\nTable 9: An example of stem refinement. The options are unchanged, and only the question stem is rewritten.\nVersion\nQuestion stem\nOriginal\nWhich of the following best captures the primary implication of integrating patient-reported experiences\nand preferences into the early-stage development of medicinal products for neglected tropical diseases,\nbased on the qualitative findings from a multi-country study on cutaneous leishmaniasis?\nIteration 1\nWhich of the following best captures the primary implication of integrating patient-reported experiences\nand preferences into the early-stage development of medicinal products for neglected tropical diseases,\nbased on the qualitative findings from a multi-country study on a vector-borne skin disorder caused\nby Leishmania parasites, characterized by painless, chronic ulcers or nodules on exposed body parts,\nprimarily resulting from sandfly bites and affecting millions globally?\nIteration 2\nWhich of the following best captures the primary implication of integrating patient-reported experiences\nand preferences into the early-stage development of medicinal products for neglected tropical diseases,\nbased on the qualitative findings from a multi-country study on a vector-borne skin disorder caused by\nprotozoan parasites from over 20 species transmitted to humans via bites of infected phlebotomine\nsandflies, primarily causing chronic skin lesions through vector-borne transmission, affecting millions\nglobally?\nIteration 3\nWhich of the following best captures the primary implication of integrating patient-reported experiences\nand preferences into the early-stage development of medicinal products for a diverse group of communi-\ncable diseases caused by parasitic, bacterial, fungal, viral, and protozoan pathogens, predominantly\naffecting impoverished populations in tropical and subtropical regions and perpetuating cycles of\npoor health, social marginalization, and economic hardship, based on the qualitative findings from a\nmulti-country study on a vector-borne skin disorder caused by protozoan parasites from over 20 species\ntransmitted to humans via bites of infected phlebotomine sandflies, primarily causing chronic skin lesions\nthrough vector-borne transmission, affecting millions globally?\n10\n0\n10\n1\n10\n2\nNumber (log scale)\nDeterminants & Exposures\nModeling, Methods & Evaluation\nSusceptibility & Immunity\nSurveillance & Descriptive Epidemiology\nOutbreak Investigation & Field Response\nClass Name\n148\n239\n33\n71\n8\n218\n99\n158\n22\n3\nEpiQAL-A\nEpiQAL-B\nFigure 2: Class distribution for EpiQAL-A and EpiQAL-B.\nused to mitigate hallucination (Lewis et al., 2020;\nIzacard and Grave, 2021; Bhasuran et al., 2025).\nStructured resources such as Hetionet and iBKH\nencode biomedical entities and relations that can\n"}, {"page": 18, "text": "18.2%\n17.0%\n13.4%\n11.4%\n10.0%\n8.4%\n6.0%\n4.4%\n3.6%\n3.2%\nTopic Distribution for EpiQAL-A\n21.2%\n16.2%\n11.2%\n8.2%\n5.8%\n5.6%\n5.6%\n5.4%\n4.8%\n4.6%\n4.4%\n3.6%3.4%\nTopic Distribution for EpiQAL-B\nZoonotic/One Health interfaces, reservoirs & vectors\nTransmission modeling & inference\nProtection effectiveness, waning, reinfection & immune escape\nTime-Place-Person patterns, seasonality & clustering\nMeasurement & bias handling\nProgram performance & impact evaluation\nStudy design & causal effects\nSeverity risk & prognostic factors\nExposure-response interpretation\nContextual determinants of exposure\nSerology & correlates of protection\nTransmission modes & contact patterns\nOther\nFigure 3: Topic distribution for EpiQAL-A and EpiQAL-B.\nsupport downstream reasoning (Himmelstein et al.,\n2017; Su et al., 2022). For epidemiology-oriented\nknowledge, eKG-DONs compiles outbreak reports\nfrom official sources (S et al., 2025). Recent work\nstudies instruction-aware retrieval across heteroge-\nneous sources (Min et al., 2025) and integration\nof knowledge graphs with multi-agent reasoning\n(Xie et al., 2025; Xu et al., 2025). In EpiQAL-B\nconstruction, we operationalize structured relations\nby summarizing knowledge graph triples into natu-\nral language signals used only during generation;\nthese signals are withheld at evaluation time.\nF\nPrompt\nTables 10, 11, and 12 show the emphasized gen-\neration prompts for EpiQAL-A, EpiQAL-B, and\nEpiQAL-C, respectively.\n"}, {"page": 19, "text": "Table 10: Prompts used for EpiQAL-A Generation.\nQuestion Generation:\nYour task is to generate a retrieval-based question using the provided passage.\nThe question should be an-\nswerable by directly locating information in the passage, without requiring inference or external knowledge.\n[... ...]\nStep 3: Write one question that requires readers to locate and retrieve specific information from the passage. The\nquestion should have a clear, unambiguous answer that appears explicitly in the passage.\nStep 4: Apply quality requirements. A good retrieval question should target specific factual content rather than vague or\ngeneral information, have an answer that is explicitly stated in the passage in a locatable form, and not be answerable by\ngeneral knowledge alone without reading the passage.\nStep 5: Apply question stem constraints. The question stem should not copy phrases directly from the passage that\nwould make the answer obvious, should not be so broad that multiple unrelated answers could apply, and should be\ngrammatically complete and clear.\n[... ...]\nCorrect Option Generation:\nYour task is to generate correct options for a retrieval-based question. The correct options should be answers\nthat can be directly found in the passage. You will be given the passage, the question, and the evidence from question\ngeneration.\n[... ...]\nStep 3: Generate one or more correct options. Each option must be directly supported by explicit text in the passage. Do\nnot infer or add information not present in the passage.\nStep 4: Apply option constraints. Each option should use concise wording that captures the answer without copying the\nentire evidence sentence. Each option should be semantically complete, though it does not need to be a full sentence.\nEach option must not contradict any information in the passage.\nStep 5: If generating multiple options, ensure each represents a distinct correct answer from different parts of the\npassage. Options should not overlap or be redundant.\n[... ...]\nDitractor Generation:\nYour task is to generate distractors for a retrieval-based question.\nDistractors should be plausible-sounding\nanswers that appear in the passage but do not correctly answer the specific question asked. They test whether readers\ncan precisely locate the correct information rather than guessing based on keyword matching. You will be given the\npassage, the question, and the correct options.\n[... ...]\nStep 2: Identify content in the passage that could be confused with the correct answer. Good distractors share these\ncharacteristics:\n- They belong to the same semantic category as the correct option such as both being locations, numbers, time periods,\nor names\n- They appear in the passage and are factually accurate within the passage context\n- They relate to a different entity, time, place, or context than what the question specifically asks about\nStep 3: Generate distractors using only information from the passage. Each distractor must be a valid fact stated in the\npassage but incorrect as an answer to this specific question.\n[... ...]\n"}, {"page": 20, "text": "Table 11: Prompts used for EpiQAL-B Generation.\nQuestion Generation:\nYour task is to generate a multiple-choice style question that requires multi-step reasoning.\nThe question\nshould be grounded in the passage, guided by the topic, and optionally informed by external domain knowledge.\n[... ...]\nStep 2. Identify a passage-anchored detail that the question must rely on. This should be a specific fact, number,\nobservation, or finding that appears in the passage. The question must be impossible to answer without this anchored\ndetail.\nStep 3. Select at least two pieces of evidence from the passage that must be combined to answer the question. These\npieces of evidence should come from different sentences or different parts of the text.\nStep 4. Evaluate whether the external domain knowledge is relevant. If any meaningful connection exists, you must\nincorporate relevant information from the external domain knowledge as part of your evidence.\nStep 5. Establish the reasoning chain among your selected evidence. Before writing the question, plan how the evidence\npieces connect logically.\nStep 6. Before finalizing your question, verify that it truly requires multi-step reasoning.\nStep 7. Verify that the question asks about something the passage does not directly answer.\nStep 8. Write one question stem that requires the reasoning chain you planned.\nStep 9. Ensure the question leaves room for multiple plausible answer directions.\n[... ...]\nCorrect Option Generation:\nYour task is to generate correct options for a multiple-choice question that requires multi-step reasoning.\nThe options should be derived conclusions that emerge from integrating the provided evidence, not facts that can be\ndirectly retrieved from the passage.\n[... ...]\nStep 3. Draft one or more correct options. Each option must satisfy these requirements: - It must be a conclusion that\nrequires integrating at least two pieces of the provided evidence\n- It must not be a direct paraphrase of any single sentence in the passage\n- It must not be verifiable by reading only one evidence piece\n- It must require applying an epidemiological principle or methodological concept to interpret the evidence\n- It must use different vocabulary from the passage where possible while preserving accuracy\n[... ...]\nDitractor Generation:\nYour task is to generate distractors for a multiple-choice question that requires multi-step reasoning.\nDistrac-\ntors must look structurally identical to the correct options but contain a subtle logical flaw that can only be detected\nthrough careful reasoning.\n[... ...]\nStep 3. Identify multiple vulnerable points in the reasoning chain where a reader might go wrong. Consider these\ncategories of errors:\n- Confusing related but distinct concepts\n- Applying a valid method to an incompatible study design\n- Mixing up the target variable with a superficially similar variable\n- Using correct terminology but violating underlying assumptions\n- Drawing conclusions that would require different data than what is available\n[... ...]\n"}, {"page": 21, "text": "Table 12: Prompts used for EpiQAL-C Generation.\nCorrect Option Extraction:\nYour task is to extract one conclusion from the provided Discussion section that will serve as the Correct Op-\ntion for a reasoning test. Readers will see only the Passage Body and must identify which conclusion can be derived\nfrom it.\n[... ...]\nStep 3. Apply the novelty requirement. The conclusion must not be explicitly stated anywhere in the Passage Body.\nReject candidates where the same statement appears in the Results or other sections.\nStep 4. Apply the derivability requirement. The conclusion must be logically derivable from evidence in the Passage\nBody by applying general epidemiological principles.\nReject conclusions that require:\n- Results from other studies cited in the Discussion but not described in the Passage Body\n- Specific facts about diseases, treatments, or populations not mentioned in the Passage Body\n- Comparisons to external benchmarks or statistics not provided in the Passage Body\nStep 5. Apply the complexity requirement. Prefer conclusions that: - require integrating multiple pieces of evidence\nfrom the Passage Body\n- require applying epidemiological principles to interpret the data\n- represent a key finding rather than a minor observation\nStep 6. Apply exclusion criteria. Reject conclusions that: - are direct numerical summaries already stated in the Results\n- describe study limitations or future research directions\n- are speculative statements without clear evidential basis in the Passage Body\n- are generic statements applicable to any similar study\n[... ...]\nQuestion Generation:\nYour task is to generate a question stem for a single-choice reasoning test.\nThe question must be answerable\nonly by the provided Correct Option, which is a conclusion derived from the Passage Body through epidemiological\nreasoning.\n[... ...]\nStep 3. Design a question that requires readers to integrate the evidence pieces and apply the same epidemiological\nreasoning to arrive at the Correct Option. The question should set up a reasoning task without revealing the answer\ndirection.\nStep 4. Apply difficulty requirements. A good question should:\n- require integrating multiple pieces of evidence rather than relying on a single fact\n- require applying epidemiological principles to interpret the data\n- not be answerable by simply locating a sentence in the Passage Body\nStep 5. Apply concealment requirements. The question stem:\n- must not use any words or phrases that appear in the Option field\n- must not use synonyms or paraphrases that directly hint at the conclusion\n- must not indicate the type of answer expected such as prognosis, risk, or recommendation\n- must not reveal which evidence pieces are relevant\n[... ...]\nDitractor Generation:\nYour task is to generate distractors for a reasoning test.\nDistractors should be plausible-sounding conclusions\nthat cannot actually be derived from the Passage Body alone.\n[... ...]\nStep 2. Identify candidate distractor statements from the Discussion section. Good distractors fall into one of these\ncategories:\n- External dependency: Conclusions that require information from other studies cited in the Discussion but not described\nin the Passage Body\n- Speculation: Statements about future research directions, untested hypotheses, or possibilities using hedging language\nsuch as may, might, or could\n- Limitations: Statements about study limitations or methodological caveats\n- Background only: Statements that merely restate general background knowledge\n- Causal reversal: A statement created by reversing or misinterpreting the cause-effect relationship implied in the correct\noption\nStep 3. Verify each candidate meets two requirements:\n- It cannot answer the question. If a candidate could be derived from the Passage Body through valid reasoning, discard\nit.\n- It should be relevant to what the question asks. Prefer distractors that address similar aspects as the question and the\ncorrect option.\n[... ...]\n"}]}