{"doc_id": "arxiv:2601.15129", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.15129.pdf", "meta": {"doc_id": "arxiv:2601.15129", "source": "arxiv", "arxiv_id": "2601.15129", "title": "RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)", "authors": ["Yishu Wei", "Adam E. Flanders", "Errol Colak", "John Mongan", "Luciano M Prevedello", "Po-Hao Chen", "Henrique Min Ho Lee", "Gilberto Szarf", "Hamilton Shoji", "Jason Sho", "Katherine Andriole", "Tessa Cook", "Lisa C. Adams", "Linda C. Chu", "Maggie Chung", "Geraldine Brusca-Augello", "Djeven P. Deva", "Navneet Singh", "Felipe Sanchez Tijmes", "Jeffrey B. Alpert", "Elsie T. Nguyen", "Drew A. Torigian", "Kate Hanneman", "Lauren K Groner", "Alexander Phan", "Ali Islam", "Matias F. Callejas", "Gustavo Borges da Silva Teles", "Faisal Jamal", "Maryam Vazirabad", "Ali Tejani", "Hari Trivedi", "Paulo Kuriki", "Rajesh Bhayana", "Elana T. Benishay", "Yi Lin", "Yifan Peng", "George Shih"], "published": "2026-01-21T16:04:01Z", "updated": "2026-01-21T16:04:01Z", "summary": "Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked \"Agree all\", \"Agree mostly\" or \"Disagree\" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected \"Agree All\" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.15129v1", "url_pdf": "https://arxiv.org/pdf/2601.15129.pdf", "meta_path": "data/raw/arxiv/meta/2601.15129.json", "sha256": "98b764271cc7502ba5cd846582a01ba5fae08e3d6733aca6f59f1368a9cc3eea", "status": "ok", "fetched_at": "2026-02-18T02:20:53.266056+00:00"}, "pages": [{"page": 1, "text": "RSNA Large Language Model Benchmark Dataset for Chest Radio-\ngraphs of Cardiothoracic Disease: Radiologist Evaluation and Vali-\ndation Enhanced by AI Labels (REVEAL-CXR)\nYishu Wei1,2, Adam E. Flanders3, Errol Colak4, John Mongan5, Luciano M Prevedello6, Po-Hao Chen7,\nHenrique Min Ho Lee8, Gilberto Szarf8, Hamilton Shoji8, Jason Sho9, Katherine Andriole10, Tessa Cook11,\nLisa C. Adams12, Linda C. Chu13, Maggie Chung5, Geraldine Brusca-Augello1, Djeven P. Deva4, Navneet\nSingh14,15, Felipe Sanchez Tijmes16, Jeffrey B. Alpert1, Elsie T. Nguyen16, Drew A. Torigian11, Kate\nHanneman16, Lauren K Groner1, Alexander Phan1, Ali Islam17, Matias F.Callejas4, Gustavo Borges da Silva\nTeles8, Faisal Jamal12, Maryam Vazirabad9, Ali Tejani15, Hari Trivedi18, Paulo Kuriki19, Rajesh Bhayana19,\nElana T. Benishay1, Yi Lin2, Yifan Peng1,2, George Shih1,*\n1Department of Radiology, Weill Cornell Medicine, New York, NY, USA\n2Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA\n3Department of Radiology, Thomas Jefferson University, Philadelphia, PA, USA\n4Department of Medical Imaging, St. Michael’s Hospital/Unity Health Toronto, University of Toronto,\nToronto, ON, Canada\n5Department of Radiology and Biomedical Imaging; Division of Clinical Informatics and Digital Transfor-\nmation, Department of Medicine, University of California, San Francisco, CA, USA\n6Department of Radiology, Ohio State University Wexner Medical Center, OH, USA\n7Diagnostics Institute, Cleveland Clinic Foundation, Cleveland, OH, USA\n8Hospital Israelita Albert Einstein, Av. Albert Einstein, 627, S˜ao Paulo 05652, Brazil\n9Radiological Society of North America, Oak Brook, IL, USA\n10Department of Radiology, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA\n11Department of Radiology, Perelman School of Medicine at the University of Pennsylvania, Philadelphia,\nPA, USA\n12Department of Diagnostic and Interventional Radiology, Klinikum rechts der Isar, Technical University\nMunich, Munich, Germany\n13Department of Radiology, Johns Hopkins University School of Medicine, Baltimore, MD, USA\n14Trillium Health Partners, Department of Medical Imaging, Faculty of Medicine, University of Toronto\n15Department of Materials Science and Engineering, Faculty of Engineering, University of Toronto\n16Joint Department of Medical Imaging, Toronto General Hospital, University of Toronto, Toronto, ON,\nCanada\n17St. Joseph’s Health Care London, Western University, London, ON\n18Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA,\nUSA\n19Department of Radiology, UT Southwestern Medical Center, Dallas, TX, USA\n*Corresponding author(s). Email(s): george@cornellradiology.org\n1\narXiv:2601.15129v1  [cs.CL]  21 Jan 2026\n"}, {"page": 2, "text": "Abstract\nBackground: Multimodal large language models have demonstrated comparable performance to\nthat of radiology trainees on multiple-choice board-style exams. However, to develop clinically\nuseful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential.\nPurpose: To curate released and holdout datasets of 100 chest radiographic studies each and\npropose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to\nlabel studies more efficiently.\nMaterials and Methods: A total of 13,735 deidentified chest radiographs and their correspond-\ning reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports,\nwhich were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning).\nFrom these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for\nexpert review; the sampling algorithm ensured that the selected studies were clinically relevant\nand captured a range of difficulty levels. Seventeen chest radiologists participated, and they\nmarked “Agree all”, “Agree mostly” or “Disagree” to indicate their assessment of the correctness\nof the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at\nleast two radiologists selected “Agree All” for 381 radiographs. From this set, 200 were selected,\nprioritizing those with less common or multiple finding labels, and divided into 100 released ra-\ndiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by\nRSNA to independently evaluate different models.\nResults: A benchmark of 200 studies (100 released and 100 reserved as holdouts) was developed,\neach containing one to four benchmark labels. Cohen’s κ for the agreement categories among\nexperts was 0.622 (95% CI 0.590, 0.651). At the individual condition (radiographic abnormality)\nlevel, other than airspace opacity (κ 0.484 95% CI [0.440, 0.524]), most conditions had a Cohen’s\nκ above 0.75 (range, 0.744–0.809) among experts.\nConclusion\nA benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made\npublicly available (https://imaging.rsna.org), with each chest radiograph verified by three\nradiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists\nlabel at scale, minimize unnecessary omissions, and support a semicollaborative environment.\n1. Introduction\nMultimodal large language models have achieved performance comparable to that of radiology trainees\non board-style exams [1]. However, it is still unclear how effective these models are in clinical settings,\nparticularly in regard to abnormality detection on medical images. To properly evaluate these models, there\nis a critical need for high-quality benchmarks, curated by experts and grounded in complex, real-world\nscenarios. Such benchmarks can help provide an accurate and meaningful assessment of their performance,\nwhich may differ depending on the clinical presentation and disease. This benchmark was developed by\nthe RSNA AI Committee Large Language Model (LLM) Workgroup. Chest radiographs were selected for\nthe initial multimodal LLM benchmark because they are among the most widely performed imaging studies\nworldwide.\nWhile several datasets and benchmarks have been published in the field of radiology, several limitations\nremain. First, most datasets (e.g., NIH-CXR [2], MIMIC-CXR-JPG [3], RadGraph [4], and CheXpert [5])\nhave applied natural language processing (NLP) to extract labels from radiology reports alone and lack\ninformation derived from direct image interpretation. Second, past annotation efforts typically involved\n2\n"}, {"page": 3, "text": "only a small number of radiologists rather than a broader pool of experts, reducing representativeness and\nincreasing the potential for individual biases in the labeling process [6]. Additionally, existing datasets\navailable for use consist of downsampled images in JPEG or PNG rather than DICOM format, resulting in\ninformation loss.\nTo address these limitations, the RSNA AI Committee Large Language Model (LLM) Workgroup presents\nthe RSNA LLM Benchmark Dataset: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-\nCXR), which offers several major contributions. A curated dataset annotated by 17 board-certified cardio-\nthoracic radiologists from 10 institutions across 4 countries is presented, encouraging collaboration and\nreducing individual biases. An LLM-assisted workflow was implemented to support radiologists in review-\ning images more efficiently by extracting labels from radiology reports, thereby minimizing missed diag-\nnoses and facilitating collaborative discussion. The released dataset focuses on cardiothoracic abnormalities\nidentified on chest radiographs, with diagnostic labels determined through radiologist interpretations of the\nimages. Majority voting was used to achieve a consensus among the radiologists. Users can use the bench-\nmark to evaluate their models. Although the sample size is not large, it focuses on rare conditions and\ncomplex cases involving multiple diseases, which are the most informative data points for model evaluation.\n2. Materials and Methods\n2.1. Data collection and initial label generation\nREVEAL-CXR uses deidentified chest radiographs and associated reports from the Medical Imaging and\nData Resource Center (MIDRC), a registry of deidentified examinations and corresponding reports collected\nunder IRB oversight beginning in 2020. As such, this study was exempt from the need for institutional\nreview board approval. The chest radiographs and reports used in the development of the benchmark have\nnot been previously released to the public (https://www.midrc.org/). For conciseness, we will use\n“study” as a shorthand for “chest radiograph study” throughout this work and use the terms interchangeably.\nTo ensure that this benchmark reflects clinically relevant and widely recognized findings on chest radio-\ngraphy, a multidisciplinary and multinational committee of radiologists and data scientists was convened.\nUsing a formal consensus process including literature reviews, iterative discussions, and voting, a set of 12\nbenchmark labels was identified that represent commonly encountered findings on chest radiographs. This\ncollaborative and cross-institutional approach ensured that the selected labels were both clinically mean-\ningful and broadly applicable across diverse healthcare settings. The final set of labels included airspace\nopacity, aneurysm, cardiomegaly, chronic obstructive pulmonary disease (COPD), hiatal hernia, interstitial\nopacity, lung mass, lung nodule, lymphadenopathy, pleural effusion, pleural thickening, and pneumothorax.\nThe mapping of these labels to RadLex and SNOMED is provided in Extended Data Table S1. Each study\nis allowed to have multiple benchmark labels.\nThe MIDRC provided access to an unreleased set of 13,735 deidentified chest radiograph studies paired with\ntheir original corresponding reports. LLMs were used to estimate the distribution of potential abnormalities\n(Figure 1). This process involved two steps. First, a prompt was provided to a frontier LLM (GPT-4o\n2024-08-01-preview [7]) (Extended Data Box 1) to extract abnormal findings from the radiology reports;\nthe instructions in the prompt are detailed in the Supplementary Materials. Second, a locally hosted large-\nlanguage model (Phi-4-Reasoning [8]) was used to map the extracted findings to the predefined set of 12\nbenchmark labels. This mapping process uses the guided decoding function of the virtual LLM [9] frame-\nwork, which limits the output of the model to a structured format (Extended Data Box 2 and Extended\nData Box 3).\nAfter generating the initial label predictions, a stratified sampling algorithm was applied to select studies\nwith the benchmark labels identified by the Phi-4 model. The sample sizes for each stratum, that is, the\n3\n"}, {"page": 4, "text": "13,735 de-identified CXR images \nand radiology reports\nGPT-4o to extract abnormal findings\nLocally hosted model to match abnormal\nfindings to benchmark labels\nSample 1,000 studies to make sure each of\nthe 12 labels are represented\nThree radiologists evaluate each study to\nassess the correctness of LLM suggested\nlabels. 381 studies have at least two\nradiologists select \"Agree all\" \n200 studies are selected prioritizing rare and\nmultiple conditions. Randomly split into\nholdout (100) and released data (100)\nFigure 1: Process for generating candidate findings from the original radiology reports, assisted by LLMs.\nnumber of labels per study, are as follows: 300 studies with one label, 300 with two labels, 319 with three\nlabels, 76 with four labels, 4 with five labels, and 1 with six labels. This algorithm ensured that each study\nselected contained at least one of the 12 benchmark labels as specified by the LLM-generated annotations.\nUltimately, a candidate dataset of 1,000 chest radiographs from different patients was created, selected for\nits clinical relevance and representing a range of difficulty levels.\n2.2. Data annotation\nA multinational team of 17 fellowship-trained cardiothoracic radiologists was recruited from 10 institutions\nacross 4 countries to adjudicate the labels extracted by the LLM for each imaging study. The radiologists’\nexperience ranged as follows: G.B (21 years), D.D (15 years), N.S (6 years), F.S (8 years), J.B.A (15 years),\nE.N (20 years), G.S (26 years), L.C.A (9 years) D.A.T (25 years), K.H (11 years), L.G (6 years), H.S\n(15 years), A.P (2 years), A.I (21 years), M.C (7 years), G.T (17 years), and F.J (5 years). A web-based\nannotation platform, which shares features of a picture archiving and communication system (PACS), was\nused for this task.\nThe radiologists were presented with the chest radiographs and a summary of the LLM-extracted labels\n(Figure 2) but not the original radiology reports. For each study, the radiologists were instructed to review\nthe images and assess the suggested labels by selecting one of three options: “Agree All,” “Agree Mostly,” or\n“Disagree.” If the choice was not “Agree All,” an optional comment field allowed them to provide feedback\non their choice. The radiologists were instructed to select “Agree Mostly” when the suggested labels were\nplausible, even in cases where a finding was subtle, equivocal, or potentially subject to interpretation (e.g.,\na faint pulmonary nodule). Each study was independently reviewed by three radiologists. The annotation\nplatform was operated in “crowd sourcing” mode, allowing annotators to select the number of studies they\nwished to review. The studies were presented to the radiologists on the platform in random order. To promote\ndiversity in the reviews and avoid overrepresentation from a small subset of reviewers, the radiologists were\nlimited to reviewing no more than 300 studies. The radiologists were blinded to the assessments of their\n4\n"}, {"page": 5, "text": "Figure 2: The web-based platform where radiologists indicated Agree, Agree Mostly, or Disagree with the\nlabel set extracted by an LLM from the original radiology report.\npeers.\n2.3. Dataset curation\nTo ensure that the final dataset was of sufficient quality, only studies with at least two “Agree All” ratings out\nof the three radiologists’ ratings were considered, resulting in a total of 381 studies. Next, to maximize the\nutility of the dataset, all studies containing less common findings (COPD, pleural thickening, lung nodule,\nlung mass, aortic aneurysm, pneumothorax, hiatal hernia, lymphadenopathy, and interstitial opacity) were\nfirst retained, yielding a dataset of 123 studies. These studies were then randomly split into a released dataset\nand a holdout dataset. The remaining studies were then ranked by the number of findings; an additional 77\nstudies with the highest label counts were selected and randomly split. In the end, both the released and\nholdout datasets contained 100 studies each. The level of agreement among experts (whether all selected\n“agree all” or only two did) is also included in the dataset.\n3. Statistical methods\nTo assess whether the released and holdout subsets of the REVEAL-CXR dataset are comparable in imaging\nacquisition properties, we evaluated key DICOM characteristics – such as manufacturer, model name, detec-\ntor type, view position, KVP, exposure, and pixel spacing – and compared their distributions using Pearson’s\nChi-square test. The χ2 test is selected because the variables are categorical or discretized, and the goal is\nto determine whether the two subsets differ in their distributions. All characteristics showed p-values above\n0.05 (e.g., Manufacturer p=0.057, Detector Type p=0.726), indicating no significant differences. Overall,\nthe acquisition-related distributions are well balanced between released and holdout data.\n5\n"}, {"page": 6, "text": "Table 1: Demographic and patient characteristics of the benchmark dataset.\nReleased\nHoldout\nTotal\n100\n100\nAge\n80+\n23\n23\n70-79\n26\n29\n60-69\n22\n20\n50-59\n15\n16\n40-49\n13\n5\n0-39\n1\n4\nUnknown\n0\n3\nSex\nFemale\n50\n49\nMale\n50\n48\nUnknown\n0\n3\nRace\nAsian\n6\n5\nAfrican American\n14\n20\nWhite\n46\n34\nOther\n3\n2\nUnknown\n31\n39\nPatient class\nInpatient\n89\n80\nOutpatient\n11\n17\nUnknown\n0\n3\nCohen’s κ was used to measure the agreement level between two categorical ratings. κ compares the ob-\nserved agreement to that expected from the raters’ marginal label frequencies, with values closer to 1 indi-\ncating stronger concordance. For each κ estimate, 1,000 bootstrap resamples were used to compute 95%\nconfidence intervals. Majority vote served as the reference standard in two settings: (1) comparing each\nradiologist’s binary “Agree” versus “Disagree” rating to the group consensus, and (2) constructing a study-\nlevel findings list when radiologists adjusted the LLM-suggested labels. κ was then calculated between each\nradiologist and this majority-voted reference to summarize interrater reliability at both the study and finding\nlevels.\n4. Results\n4.1. Descriptive statistics\nThe acquisition characteristics of both the holdout and released datasets, including manufacturer, model,\ndetector type, view position, etc, are summarized in Extended Data Table S2. The corresponding p-values\nfrom Chi-square tests assessing balance between the two datasets are also reported. All p-values exceed\n0.05, indicating no significant differences. The demographic characteristics of the final released and holdout\ndatasets are detailed in Table 1.\nFigure 3 presents the frequency of each final annotated finding. Airspace opacity was the most prevalent\n6\n"}, {"page": 7, "text": "0\n20\n40\n60\n80\nPneumothorax\nPleural Thickening\nPleural Effusion\nLymphadenopathy\nLung Nodule\nLung Mass\nInterstitial Opacity\nHiatal Hernia\nCOPD\nCardiomegaly\nAneurysm\nAirspace Opacity\nHoldout\nReleased\nFigure 3: Prevalence of radiologic findings across the released and holdout benchmark datasets. Distribution\nof the findings in the released and holdout datasets.\nfinding in the dataset, followed by pleural effusion, cardiomegaly, and interstitial opacity. The overall dis-\ntribution of the findings exhibited a strong, long-tailed pattern, with less common findings, such as hiatal\nhernia and lymphadenopathy, appearing in fewer than ten cases.\nFigure 4 shows the distribution of final finding counts per study. Because studies with more findings were\noversampled when selecting the final datasets, the label distribution shows higher finding counts than does\nthe sampling distribution described in the Methods section.\n4.2. Interannotator agreement\nWe first assessed the agreement between radiologist annotators regarding the labels ’agree all,' ’agree\nmostly,' and ’disagree. Because the radiologists did not consistently distinguish between “Disagree” and\n“Agree Mostly,” these responses were combined into a single “Disagree” category. On this binary scale,\nCohen’s κ between individual radiologists and the majority vote (i.e., at least 2 raters selecting the same\ncategory) was 0.622 (1,000 bootstrapped resamples, 95% CI: 0.590, 0.651), reflecting substantial agreement\naccording to the Landis & Koch scale. The lower end of the confidence interval, 0.590, falls in the ’moderate\nrange', which is a more conservative estimation. The relatively low agreement level is primarily attributable\nto the specific condition of airspace opacity, which is discussed later. Furthermore, this disagreement re-\nflects the inherent complexity of real-world radiographs. These difficult cases are valuable for evaluating\nLLMs, as hard examples reveal model limitations more effectively. Regarding the extent to which radiol-\nogists agree with the labels suggested by LLMs, in 619/1,000 cases (61.9%), the majority of the votes fell\ninto the “Disagree” category, indicating that the radiologists’ labels often diverged from those suggested by\nthe LLM.\nGiven the frequent divergence from the LLM labels, next, the interrater agreement among radiologists was\nexamined for individual findings (Extended Data Table S3). When a radiologist chose “Agree All”, the\noriginal LLM-suggested list was retained; otherwise, the candidate findings were adjusted on the basis\nof the annotator’s notes (Extended Data Box 4). Because not all radiologists provided notes when they\n7\n"}, {"page": 8, "text": "1.0\n2.0\n3.0\n4.0\nLabel count\n0\n10\n20\n30\n40\n50\nNumber of chest radiographs\nHoldout\nReleased\nFigure 4: Distribution of the number of findings per chest radiograph in the released and holdout datasets.\ndisagreed, we constructed a majority-voted findings list for 681 studies. Cohen’s κ was then calculated\nbetween each radiologist’s annotation and this majority list. For most findings, the agreement was relatively\nhigh: nine findings had a κ value greater than 0.7, with the highest calculated for hiatal hernia (κ = 0.809,\n95% CI [0.688, 0.902]), indicating substantial consistency across the radiologists. The notable exception\nwas airspace opacity (κ = 0.484, 95% CI [0.440, 0.524]), which is consistent with previous reports of low\ninterrater agreement for this finding [10–13].\n5. Discussion\nIn this work, we developed a benchmark specifically focused on cardiothoracic findings on chest radio-\ngraphs. We also presented a process that uses an LLM to assist radiologists with efficient labeling and to\nstreamline data curation for the creation of future benchmarks. The RSNA AI Committee has already cre-\nated several labeled datasets for ML challenges (https://imaging.rsna.org), which were annotated de\nnovo (no suggested labels). We believe that with the help of LLMs to parse these reports to provide initial\nlabels, radiologists can validate these annotations more efficiently than by creating annotations without this\nstep. Additionally, leveraging LLMs in this process should not compromise accuracy, as each study is still\nverified by three radiologists. This framework is particularly valuable for creating larger-scale benchmarks\nin the future. Furthermore, the benchmark dataset was curated in collaboration with a large group of sub-\nspecialized radiologists from across the world, which is unusual for most existing benchmarks. The original\nimages, provided in DICOM format, were used to maintain the highest quality and are available in the final\nbenchmark dataset.\nThe benchmark produced with this method has several limitations. First, the process for mapping the find-\nings to the twelve benchmark labels relies only on the findings extracted by the first LLM rather than the\nentire report. Second, we used a relatively small model (Phi-4-Reasoning) for the mapping step; while this\napproach is faster, its performance may not match that of larger models such as GPT-4o. Therefore, the map-\nping performance may be suboptimal. Third, we lacked access to comprehensive patient clinical conditions\nand demographic characteristics, which are often crucial for achieving a more accurate diagnosis. Finally,\nmore granular criteria for annotation could be implemented. Upon reviewing the annotations, we found that\n8\n"}, {"page": 9, "text": "the radiologists did not systematically distinguish between “Disagree” and “Agree Mostly”, which required\nus to treat these responses as equivalent during postprocessing.\nAs large multimodal LLMs become more prevalent and increasingly accepted by patients and healthcare\nproviders, even prior to formal clinical validation and potentially outside of controlled clinical environments,\nrobust benchmarking will become essential. These benchmarks may offer valuable insights into the expected\nperformance of emerging multimodal LLMs in real-world scenarios. In future studies, we plan to expand our\nefforts by developing additional multimodal benchmarks encompassing other medical imaging modalities,\nincluding computed tomography (CT), magnetic resonance imaging (MRI), ultrasonography, and others.\nFunding Sources\nThis work was supported by the National Institutes of Health (NIH) under grant numbers R01CA289249 and\n75N92020D00021, U.S. National Science Foundation (NSF) under grant numbers NSF CAREER 2145640.\nReferences\n[1] Rajesh Bhayana, Satheesh Krishna, and Robert R Bleakney. Performance of ChatGPT on a radiology\nboard-style examination: Insights into current strengths and limitations. Radiology, 307(5):e230582,\nJune 2023. ISSN 0033-8419,1527-1315. doi: 10.1148/radiol.230582.\n[2] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.\nChestX-Ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classifica-\ntion and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 3462–3471, July 2017. doi: 10.1109/CVPR.2017.369.\n[3] Alistair E W Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-Ying Deng,\nYifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. MIMIC-CXR-JPG, a\nlarge publicly available database of labeled chest radiographs. arXiv [cs.CV], 21 January 2019.\n[4] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven Truong, Du Nguyen Duong, Tan Bui, Pierre\nChambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, Curtis Langlotz, and Pranav Rajpurkar.\nRadGraph: Extracting clinical entities and relations from radiology reports. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 8 June 2021.\n[5] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik\nMarklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A Mong, Safwan S\nHalabi, Jesse K Sandberg, Ricky Jones, David B Larson, Curtis P Langlotz, Bhavik N Patel, Matthew P\nLungren, and Andrew Y Ng. CheXpert: A large chest radiograph dataset with uncertainty labels\nand expert comparison. Proc. Conf. AAAI Artif. Intell., 33(01):590–597, 17 July 2019. ISSN 2159-\n5399,2374-3468. doi: 10.1609/aaai.v33i01.3301590.\n[6] Anna Majkowska, Sid Mittal, David F Steiner, Joshua J Reicher, Scott Mayer McKinney, Gavin E\nDuggan, Krish Eswaran, Po-Hsuan Cameron Chen, Yun Liu, Sreenivasa Raju Kalidindi, Alexander\nDing, Greg S Corrado, Daniel Tse, and Shravya Shetty. Chest radiograph interpretation with deep\nlearning models: Assessment with radiologist-adjudicated reference standards and population-adjusted\nevaluation. Radiology, 294(2):421–431, February 2020. ISSN 0033-8419,1527-1315. doi: 10.1148/\nradiol.2019191293.\n[7] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor\nBabuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff\nBelgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,\nOleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,\n9\n"}, {"page": 10, "text": "Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory\nCarmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja-\nson Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka\nDhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David\nFarhi, Liam Fedus, Niko Felix, Sim´on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie\nGeorges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan\nGordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo,\nChris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan\nHickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny\nJin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kan-\nitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,\nYongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk,\nAndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Mal-\nfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,\nBob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Med-\nina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco,\nEvan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M´ely, Ashvin Nair, Rei-\nichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,\nCullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Paras-\ncandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman,\nFilipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny,\nMichelle Pokrass, Vitchyr H Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul\nPuri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar,\nGirish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,\nJordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Fe-\nlipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cer´on Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay\nWang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C J Weinmann, Akila Welihinda, Peter\nWelinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,\nKevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia\nZhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical report. arXiv\n[cs.CL], page 08774, 15 March 2023.\n[8] Marah Abdin, Jyoti Aneja, Harkirat Behl, S´ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, James R Lee, Yin Tat Lee, Yuanzhi\nLi, Weishung Liu, Caio C T Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil\nSalim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4\ntechnical report. arXiv [cs.CL], 11 December 2024.\n[9] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving\n10\n"}, {"page": 11, "text": "with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages\n611–626, New York, NY, USA, 23 October 2023. ACM. doi: 10.1145/3600006.3613165.\n[10] Mark B Loeb, Soo B Chan Carusone, Tom J Marrie, Kevin Brazil, Paul Krueger, Lynne Lohfeld,\nAndrew E Simor, and Stephen D Walter. Interobserver reliability of radiologists’ interpretations of\nmobile chest radiographs for nursing home-acquired pneumonia. J. Am. Med. Dir. Assoc., 7(7):416–\n419, September 2006. ISSN 1525-8610,1538-9375. doi: 10.1016/j.jamda.2006.02.004.\n[11] Gesche M Voigt, Dominik Thiele, Martin Wetzke, J¨urgen Weidemann, Patricia-Maria Parpatt, Tobias\nWelte, J¨urgen Seidenberg, Christian Vogelberg, Holger Koster, Gernot G U Rohde, Christoph H¨artel,\nGesine Hansen, and Matthias V Kopp. Interobserver agreement in interpretation of chest radiographs\nfor pediatric community acquired pneumonia: Findings of the pedCAPNETZ-cohort. Pediatr. Pul-\nmonol., 56(8):2676–2685, August 2021. ISSN 8755-6863,1099-0496. doi: 10.1002/ppul.25528.\n[12] Michael N Albaum, Lisa C Hill, Miles Murphy, Yi-Hwei Li, Carl R Fuhrman, Cynthia A Britton,\nWishwa N Kapoor, and Michael J Fine. Interobserver reliability of the chest radiograph in community-\nacquired pneumonia. Chest, 110(2):343–350, August 1996. ISSN 0012-3692,1931-3543. doi: 10.\n1378/chest.110.2.343.\n[13] Alexander Makhnevich, Liron Sinvani, Stuart L Cohen, Kenneth H Feldhamer, Meng Zhang, Martin L\nLesser, and Thomas G McGinn. The clinical utility of chest radiography for identifying pneumonia:\nAccounting for diagnostic uncertainty in radiology reports. AJR Am. J. Roentgenol., 213(6):1207–\n1212, December 2019. ISSN 0361-803X,1546-3141. doi: 10.2214/AJR.19.21521.\n11\n"}, {"page": 12, "text": "Supplementary data\nTable S1: Mapping of disease labels to RadLex and SNOMED terms\nAneurysm\nFocal dilation of a\nvessel (artery or\naorta) > 50% of its\nnormal diameter.\nRID28677 –\nAneurysm\n(morphologic\nabnormality)\n233985008 –\nAneurysm (disorder)\nLP38233-3 –\nAneurysm [Imaging\nobservation]\nAirspace opacity\nIncreased\nparenchymal\nattenuation\nobscuring vascular\nmarkings due to\nalveolar filling\n(fluid, pus, blood,\ncells).\nRID28663 –\nAirspace opacity\n(finding)\n301857004 – Air\nspace opacity\n(finding)\nLP38249-9 –\nAirspace opacity\n[Imaging\nobservation]\nCardiomegaly\nCardiac silhouette\nenlargement\n(cardiothoracic ratio\n> 0.5 on PA film).\nRID35639 –\nCardiomegaly\n(finding)\n8186001 –\nCardiomegaly\n(disorder)\nLP38234-1 –\nCardiomegaly\n[Imaging\nobservation]\nCOPD\nChronic airflow\nlimitation with\nradiographic signs\nsuch as\nhyperinflated lungs,\nflattened diaphragm,\nand decreased\nvascular markings.\nRID43230 –\nChronic obstructive\npulmonary disease\n13645005 – Chronic\nobstructive lung\ndisease (disorder)\nLP38235-8 –\nChronic obstructive\npulmonary disease\n[Imaging\nobservation]\nHiatal hernia\nProtrusion of\nstomach through the\nesophageal hiatus\ninto the thorax.\nRID5671 – Hiatal\nhernia\n39839004 – Hiatus\nhernia (disorder)\nLP38237-4 – Hiatal\nhernia [Imaging\nobservation]\nInterstitial opacity\nFine or coarse linear,\nreticular, or\nreticulonodular\nopacities due to\ninterstitial\nthickening or\nfibrosis.\nRID28664 –\nInterstitial opacity\n(finding)\n301858009 –\nInterstitial opacity\n(finding)\nLP38250-7 –\nInterstitial opacity\n[Imaging\nobservation]\nLymphadenopathy\nEnlargement of\nlymph nodes,\nusually > 1 cm\nshort-axis, may be\nhilar or mediastinal.\nRID5694 –\nLymphadenopathy\n30746006 –\nLymphadenopathy\n(disorder)\nLP38238-2 –\nLymphadenopathy\n[Imaging\nobservation]\nFinding / Label\nOperational\nDefinition\n(Radiologic)\nRadLex Term (RID)\nSNOMED CT\nConcept ID\nLOINC Code\n(Name)\nContinued on next page\n12\n"}, {"page": 13, "text": "Table S1: Mapping of disease labels to RadLex and SNOMED terms (Continued)\nLung mass\nFocal pulmonary\nopacity > 3 cm in\ndiameter.\nRID43235 – Lung\nmass\n126713003 – Mass\nof lung (finding)\nLP38248-1 – Mass\n[Imaging\nobservation]\nLung nodule\nRounded or irregular\npulmonary opacity\n≤3 cm in diameter.\nRID12780 –\nPulmonary nodule\n39607008 – Lung\nnodule (finding)\nLP38246-5 –\nNodule [Imaging\nobservation]\nFinding / Label\nOperational\nDefinition\n(Radiologic)\nRadLex Term (RID)\nSNOMED CT\nConcept ID\nLOINC Code\n(Name)\n13\n"}, {"page": 14, "text": "Table S2: Key acquisition characteristics of holdout and released datasets\nManufacturer\n0.057\nCARESTREAM\n1.0\n0.0\nCARESTREAM HEALTH\n49.0\n37.0\nCarestream\n5.0\n10.0\nCarestream Health\n35.0\n39.0\nGE Healthcare\n8.0\n5.0\nGE MEDICAL SYSTEMS\n2.0\n0.0\nImaging Dynamics Company Ltd.\n0.0\n4.0\nKODAK\n4.0\n1.0\nKONICA MINOLTA\n1.0\n1.0\nPhilips Medical Systems\n7.0\n2.0\nManufacturer Model Name\n0.066\nCS-7\n1.0\n1.0\nDefinium 5000\n5.0\n11.0\nDigitalDiagnost\n7.0\n2.0\nDiscovery XR656\n8.0\n5.0\nDR 7500\n2.0\n0.0\nDRX-1\n1.0\n2.0\nDRX-EVOLUTION\n2.0\n1.0\nDRX-REVOLUTION\n50.0\n36.0\nDRX-Revolution\n39.0\n48.0\nRevolution XRd ADS 28.4\n2.0\n0.0\nX3\n0.0\n4.0\nDetector Type\n0.726\nDIRECT\n78\n77\nSCINTILLATOR\n22\n18\nView Position\n0.115\nAP\n96.0\n88\nLATERAL\n0.0\n2\nLL\n4.0\n8\nPA\n16.0\n9\nPOSTERO ANTERIOR\n0.0\n2\nDistance Source To Detector\n0.426\np-value\nholdout\nreleased\nContinued on next page\n14\n"}, {"page": 15, "text": "Table S2: Key acquisition characteristics of holdout and released datasets (Continued)\n0.0\n8.0\n10.0\n12.4\n1.0\n0.0\n18.4\n1.0\n0.0\n1000.0\n9.0\n13.0\n1800.0\n2.0\n0.0\n1810.0\n1.0\n0.0\n1811.0\n0.0\n1.0\n1817.0\n2.0\n0.0\n1828.0\n4.0\n3.0\n1829.0\n2.0\n1.0\n1830.0\n1.0\n0.0\n1885.0\n1.0\n0.0\nDistance Source To Patient\n0.152\n946.0\n5.0\n11.0\n994.0\n4.0\n2.0\n1751.0\n2.0\n0.0\n1756.0\n1.0\n0.0\n1757.0\n0.0\n1.0\n1763.0\n2.0\n0.0\n1775.0\n2.0\n0.0\n1776.0\n5.0\n3.0\n1831.0\n1.0\n0.0\nKVP\n0.341\n83.0\n0.0\n1.0\n85.0\n36.0\n33.0\n87.0\n1.0\n0.0\n88.0\n1.0\n0.0\n89.0\n0.0\n2.0\n90.0\n2.0\n2.0\n91.0\n0.0\n1.0\n95.0\n5.0\n2.0\n100.0\n1.0\n3.0\n104.0\n1.0\n0.0\np-value\nholdout\nreleased\nContinued on next page\n15\n"}, {"page": 16, "text": "Table S2: Key acquisition characteristics of holdout and released datasets (Continued)\n105.0\n1.0\n0.0\n106.0\n1.0\n0.0\n107.0\n0.0\n1.0\n110.0\n46.0\n33.0\n112.0\n0.0\n1.0\n115.0\n6.0\n5.0\n116.0\n0.0\n1.0\n120.0\n10.0\n20.0\n125.0\n3.0\n2.0\n140.0\n1.0\n0.0\nExposure Inm As\n0.447\n1.1\n0.0\n1.0\n1.6\n14.0\n15.0\n1.7\n1.0\n1.0\n1.8\n1.0\n0.0\n2.0\n12.0\n9.0\n2.2\n0.0\n2.0\n2.3\n1.0\n0.0\n2.5\n0.0\n5.0\n2.8\n1.0\n5.0\n3.1\n3.0\n4.0\n3.2\n11.0\n11.0\n3.6\n1.0\n0.0\n3.9\n1.0\n0.0\n4.0\n3.0\n5.0\n4.5\n2.0\n1.0\n5.0\n0.0\n2.0\n5.6\n1.0\n1.0\n6.3\n2.0\n1.0\n7.1\n1.0\n2.0\n8.0\n1.0\n0.0\n10.1\n0.0\n1.0\n12.5\n1.0\n0.0\np-value\nholdout\nreleased\nContinued on next page\n16\n"}, {"page": 17, "text": "Table S2: Key acquisition characteristics of holdout and released datasets (Continued)\n19.8\n0.0\n1.0\n25.0\n1.0\n0.0\nPixel Spacing\n0.220\n0.139\\0.139\n92.0\n87.0\n0.143\\0.143\n2.0\n0.0\n0.144\\0.144\n0.0\n4.0\n0.1488636604775\\0.1488636604775\n1.0\n0.0\n0.1488896174863\\0.1488896174863\n1.0\n0.0\n0.1488901038819\\0.1488901038819\n2.0\n0.0\n0.1488959823886\\0.1488959823886\n2.0\n0.0\n0.1488989508559\\0.1488989508559\n0.0\n1.0\n0.1488994475138\\0.1488994475138\n1.0\n0.0\n0.14948\\0.14948\n0.0\n1.0\n0.175\\0.175\n1.0\n1.0\n0.194311\\0.194311\n4.0\n3.0\n0.1988\\0.1988\n4.0\n2.0\np-value\nholdout\nreleased\n17\n"}, {"page": 18, "text": "Table S3: Cohen’s Kappa at disease level and prevalence in the 1000 sampled studies\nDisease\nCohen’s Kappa\nGwet’s AC1\nPositive ratio for\nfinal annotation\nairspace opacity\n0.484 [0.440, 0.524]\n0.501 [0.445, 0.556]\n0.846\npleural effusion\n0.756 [0.727, 0.784]\n0.667 [0.617, 0.716]\n0.455\ncardiomegaly\n0.759 [0.729, 0.789]\n0.633 [0.582, 0.685]\n0.460\ninterstitial opacity\n0.746 [0.710, 0.780]\n0.759 [0.716, 0.801]\n0.230\ncopd\n0.759 [0.689, 0.821]\n0.955 [0.939, 0.970]\n0.055\npleural thickening\n0.744 [0.669, 0.811]\n0.948 [0.932, 0.965]\n0.048\nlung nodule\n0.709 [0.626, 0.781]\n0.959 [0.945, 0.974]\n0.049\nlung mass\n0.801 [0.714, 0.875]\n0.979 [0.969, 0.989]\n0.033\naneurysm\n0.651 [0.536, 0.758]\n0.965 [0.952, 0.978]\n0.025\npneumothorax\n0.753 [0.617, 0.855]\n0.982 [0.972, 0.991]\n0.018\nhiatal hernia\n0.809 [0.688, 0.902]\n0.988 [0.980, 0.995]\n0.017\nlymphadenopathy\n0.634 [0.454, 0.775]\n0.984 [0.976, 0.993]\n0.013\n18\n"}, {"page": 19, "text": "Table S4: Studies finished per radiologist\nRadiologist\nStudies finished\nN.S\n300\nG.B\n300\nD.D\n300\nF.S\n273\nJ.A\n229\nE.N\n222\nG.S\n213\nD.T\n203\nK.H\n177\nL.G\n151\nH.S\n140\nA.P\n133\nA.I\n121\nL.A\n94\nM.C\n57\nG.T\n53\nF.J\n34\n19\n"}, {"page": 20, "text": "Box 1: Prompt for GPT4-o to extract clinical findings from radiology reports\nFind all diseases in the report that can have an ICD-10 code, and provide a summary in a\ntable format where the positive clinical conditions are 1 and the negative clinical\nconditions are 0. Designate each condition as left side, right side, midline, or\nbilateral. Provide an ICD-10 code and ICD-10 Description in separate columns for\npositive findings only or N/A if not applicable.\nTable columns include: [Exam No., Finding No., Clinical Finding, Left Side, Right Side,\nMidline, Bilateral, Midline, ICD-10 Code, ICD-10 Description]\nAdditional instructions:\n1. Normal findings should be excluded from each table\n2. Group similar findings together where possible for each table\n3. Create a table and also a code block highlighted CSV (without quotes)\nHere is the full report:\n{note}\n20\n"}, {"page": 21, "text": "Box 2: Schema for transfer table to benchmark labels\nbenchmark_labels_schema = {\n\"$schema\": \"https://json -schema.org/draft/2020-12/schema\",\n\"$id\": \"https:// example.com/keyfindings.schema.json\",\n\"title\": \" KeyFindingsToLabels \",\n\"description\": \"Schema for\nmapping\nkey\nfindings to benchmark\nlabels.\",\n\"type\": \"object\",\n\"properties\": {\n\"key_findings\": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\"\n},\n\"description\": \"List of key\npositive (abnormal) findings. If no\nabnormalities, the array\nshould\ncontain ’Normal exam ’.\"\n},\n\" benchmark_labels \": {\n\"type\": \"array\",\n\"items\": {\n\"type\": \"string\",\n\"enum\": [\n\"Aneurysm\",\n\"Airspace\nopacity\",\n\"Cardiomegaly\",\n\"COPD\",\n\"Hiatal\nhernia\",\n\"Interstitial\nopacity\",\n\" Lymphadenopathy \",\n\"Lung Mass\",\n\"Lung\nNodule\",\n\"Pleural\neffusion\",\n\"Pleural\nthickening\",\n\"Pneumothorax\",\n\"None\"\n]\n},\n\"minItems\": 1,\n\"description\": \"Zero or more\nlabels\nfrom\nkey_findings; otherwise\nreturn\nNone.\"\n}\n},\n\"required\": [\"key_findings\", \" benchmark_labels \"],\n\" additionalProperties \": False\n}\n21\n"}, {"page": 22, "text": "Box 3: Prompt to transform table to benchmark labels\nFor each finding in the <Findings> table:\n- Match finding to ONLY ONE corresponding label from <Labels> if possible\n- Otherwise return ’None’ for that finding\n- Each finding in <Findings> table should return exactly ONE <Label>\n- The number of Findings should EQUAL the number of corresponding Labels in the final\noutput. It also equals the number of findings in the input table.\n- The output is structured as JSON, with two keys ’key_findings’ extracted from table\nand ’benchmark_labels’ being your matched label\n<Labels>\nAneurysm\nAirspace opacity\nCardiomegaly\nCOPD\nHiatal hernia\nInterstitial opacity\nLymphadenopathy\nLung Mass\nLung Nodule\nPleural effusion\nPleural thickening\nPneumothorax\n</Labels>\n<Examples For Labels>\nAneurysm = Aortic Aneurysm\nAirspace opacity = atelectasis, pneumonia, hemorrhage\nCOPD = emphysema, hyperinflated lungs\nLymphadenopathy = usually in hilar region\nGranuloma is a benign finding\n</Examples For Labels>\n######## Here is the clinical findings input ########\n‘‘‘\n{table}\n######## Your output ########\n22\n"}, {"page": 23, "text": "Box 4: Prompt for GPT4-o to suggest revised label based on radiologist comment\nAnother LLM is trying to predict disease from a radiology report. The diseases are from\nthis list:\n[’Aneurysm’, ’Airspace opacity’, ’Cardiomegaly’, ’COPD’, ’Hiatal hernia’, ’Interstitial\nopacity’, ’Lymphadenopathy’, ’Lung Mass’, ’Lung Nodule’, ’Pleural effusion’, ’Pleural\nthickening’,’Pneumothorax’]\nThen we have a radiologist to evaluate its diagnosis. The radiologist will give a\ngeneral comment (agree mostly, disagree) and note. I want you to give the final list\nbased on the radiologist’s comment.\n#### Instructions:\n1. Radiologist comment will only be ’disagree’ or ’agree mostly’\n2. The radiologist note is most informative. If the radiologist didn’t give a meaningful\nnote or there is no note, which can be very common due to system error. Return \"NA\"\n3. Only return diseases that are certain. If the radiologist is not certain, do not\ninclude it as well.\n4. Give your output in the first line, followed by your reasoning starting second line\n#### Example 1:\n* LLM Diagnosis: [’Airspace Opacity’, ’Interstitial Opacity’]\n* Radiologist comment: Disagree.\n* Radiologist note: No pneumonia\n* Output\nNA\nReason: The radiologist comment is not making sense since the LLM didn’t mention\npneumonia\n#### Example 2:\n* LLM Diagnosis: [’Airspace Opacity’]\n* Radiologist comment: Disagree\n* Radiologist note: also cardiomegaly and pleural effusions\n* Output:\n[Airspace Opacity, Cardiomegaly, Pleural effusion]\nReason: Radiologist add those diseases to the list\n#### Here is your input:\n* LLM Diagnosis: {llm_labels}\n* Radiologist comment: {annotation}\n* Radiologist note: {note}\n* Output:\n23\n"}]}