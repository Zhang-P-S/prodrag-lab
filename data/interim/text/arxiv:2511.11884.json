{"doc_id": "arxiv:2511.11884", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.11884.pdf", "meta": {"doc_id": "arxiv:2511.11884", "source": "arxiv", "arxiv_id": "2511.11884", "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support", "authors": ["Eric Hua Qing Zhang", "Julia Ive"], "published": "2025-11-14T21:32:10Z", "updated": "2026-02-16T09:54:31Z", "summary": "Mental health disorders impose a substantial global socioeconomic burden. While large language models (LLMs) offer 24/7, non-judgmental interactions to address this gap, pretrained models lack contextual coherence and emotional alignment for appropriate therapeutic dialogue. Existing methods suffer from three critical methodological gaps: 1) Supervised Fine-Tuning (SFT) produces repetitive, context-insensitive outputs that fail to balance clinical accuracy with genuine empathy; 2) Reinforcement Learning (RL)-based therapeutic systems rely on generic reward functions (e.g., BLEU, ROUGE) that prioritise lexical similarity over clinical-specific emotional appropriateness and contextual relevance; 3) LLMs are resource-intensive and pose data privacy risks, making local deployment in clinical settings infeasible. To address these gaps, this study investigates the application of SFT and RL techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a novel multi-component reward function that explicitly aligns model outputs with professional therapeutic logic (not just lexical overlap) and annotated emotions. Results demonstrated substantial improvements through RLs over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while RL achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate RL's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists, while maintaining essential human clinical oversight.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.11884v2", "url_pdf": "https://arxiv.org/pdf/2511.11884.pdf", "meta_path": "data/raw/arxiv/meta/2511.11884.json", "sha256": "9472a30e992e96abebe884d5766b56baff419d9b1743bbd6d00f6306173eac69", "status": "ok", "fetched_at": "2026-02-18T02:27:04.877040+00:00"}, "pages": [{"page": 1, "text": "Context-Emotion Aware Therapeutic Dialogue \nGeneration: A Multi-component Reinforcement Learning \nApproach to Large Language Models for Mental Health \nSupport \nEric H.Q. Zhang and Julia Ive \n University College London, London, United Kingdom \neric.zhang.24@ucl.ac.uk, j.ive@ucl.ac.uk \nAbstract. Mental health disorders impose a substantial global socioeconomic \nburden, with COVID-19 exacerbating mental health service accessibility chal-\nlenges and boosting demand for telehealth support. While large language mod-\nels (LLMs) offer 24/7, non-judgmental interactions to address this gap, pre-\ntrained models lack contextual coherence and emotional alignment for appro-\npriate therapeutic dialogue. Existing methods suffer from three critical method-\nological gaps: 1) Supervised Fine-Tuning (SFT) produces repetitive, context-\ninsensitive outputs that fail to balance clinical accuracy with genuine empathy; \n2) Reinforcement Learning (RL)-based therapeutic systems rely on generic re-\nward functions (e.g., BLEU, ROUGE) that prioritise lexical similarity over clin-\nical-specific emotional appropriateness and contextual relevance; 3) LLMs are \nresource-intensive and pose data privacy risks, making local deployment in \nclinical settings infeasible. To address these gaps, this study investigates the \napplication of SFT and RL techniques to enhance GPT-2's capacity for thera-\npeutic dialogue generation. The methodology restructured input formats to ena-\nble simultaneous processing of contextual information and emotional states \nalongside user input, employing a novel multi-component reward function that \nexplicitly aligns model outputs with professional therapeutic logic (not just lex-\nical overlap) and annotated emotions. Results demonstrated substantial im-\nprovements through RLs over baseline GPT-2 across multiple evaluation met-\nrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L \n(0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual \nrelevance and professionalism, while RL achieved 99.34% emotion accuracy \ncompared to 66.96% for baseline GPT-2. These findings demonstrate RL's ef-\nfectiveness in developing therapeutic dialogue systems that can serve as valua-\nble assistive tools for therapists, while maintaining essential human clinical \noversight. \nCode \n& \nAppendix: \nhttps://github.com/ez-anthro-tech-\ndesign/RLforGPT2MentalHealth  \nKeywords: Reinforcement learning, Supervised fine-tuning, Large language \nmodels, Therapeutic dialogue, Empathy alignment \n"}, {"page": 2, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n2 \n1 \nIntroduction \nMental illness imposes a significant socioeconomic burden, with one in five adults in \nthe United States experiencing a mental health condition each year [1]. Patients are \nfurther deterred from seeking care due to societal stigma [2], attitudinal barriers [3], \nand high costs and lack of knowledge about accessing these services [4]. \nDirect patient-facing AI systems without clinical supervision face significant ob-\nstacles, as patients demonstrate a lack of trust and lower adherence towards autono-\nmous non-human consultations [5]. Concerns exist regarding patient safety and accu-\nracy when AI-assisted dialogue tools operate without clinical oversight [6], while \nlimited laws governing ethical use, data privacy, and transparent communication in \nautonomous AI systems compound these issues [7]. Additionally, LLMs exhibit hal-\nlucination and may be pretrained on datasets of unknown validation status, creating \nliability uncertainties on AI-generated clinical guidance as standard-of-care evidence \ndespite the absence of expert review protocols [8]. Given these constraints, LLMs \nmay be most effectively deployed as supervised dialogue generation tools that support \nhealthcare professionals across various therapeutic contexts â€“ including telehealth and \nin-person sessions â€“ by providing real-time conversational suggestions rather than \nreplacing clinical judgment.  \nBeyond these deployment challenges, technical limitations in existing fine-tuning \nmethods create critical methodological gaps for therapeutic dialogue generation: \n1. SFT operates on a supervised learning paradigm, where pre-trained models learn \nfrom input-output pairs via cross-entropy loss minimisation, updating parameters \nto reduce prediction errors on labelled datasets [9]. However, Stiennon et al. [10] \nshowed that SFT is limited in penalising equally critical errors and minor lexical \nvariations, risking reinforcing biases in low-quality training data, and generating \nrepetitive outputs. SFT struggles to align with human therapeutic preferences [11] \n[12] â€“ patients prioritise genuine empathy over technically correct but emotionally \nshallow responses [13], yet SFT models lack mechanisms to integrate affective \nempathy with clinical logic.  \n2. RL offers a robust approach to align model outputs with human preferences \nthrough human preference collection, reward model learning, and policy optimisa-\ntion [14]. RL learns via environmental interactions, using reward signals to refine \npolicies through techniques like proximal policy optimisation (PPO) [18]. Existing \nRL-based therapeutic systems (e.g., [15] [16]) use reward functions measured by \ngeneric NLP metrics (BLEU, ROUGE-L) or surface-level sentiment scores. These \nmetrics prioritise lexical overlap with reference texts over clinical relevance, re-\nsulting in responses that are grammatically consistent but fail to address patient-\nspecific contexts or align with therapeutic emotional boundaries.  \nIn this study, we present three key contributions that directly address the limitations \nof existing methods:  \n1. A clinical-specific multi-component reward function: Unlike generic reward \nfunctions (e.g., [16]) that prioritise lexical similarity, our reward function integrates \n"}, {"page": 3, "text": "3   \nE. H.Q. Zhang and J. Ive \nfive interdependent clinical dimensions-text quality (professionalism), emotional \nalignment (therapeutic boundary-aware), contextual relevance (patient-specific), \nempathetic content (genuine understanding), and sentiment appropriateness (sup-\nportive tone). This explicitly solves the problem of â€œreward hackingâ€ (e.g., generic \nempathy/ repetitive phrases) and ensures responses meet clinical standards, not just \nNLP metrics. \n2. An RL framework for affective empathy enhancement: Our framework builds \non SFT to model context and emotion as a unified input (via structured tokens), ra-\nther than separate features. This addresses the context-emotion modeling deficit of \nprior work, enabling the model to generate emotionally appropriate responses that \nare tied to patient context. Restructuring GPT-2â€™s structure allows for explicit gen-\neration of emotion tokens alongside text, providing therapists with transparent ex-\namples of ideal therapeutic communication (context + emotion) for supervised use \nand clear demonstrations of ideal therapeutic communication for use across super-\nvised clinical settings.  \n3. A lightweight, locally deployable implementation: By leveraging GPT-2 (124M \nparameters), we avoid the resource and privacy barriers of large LLMs. GPT-2â€™s \nopen-source nature and low computational requirements enable easy deployment, \nsolving the accessibility gap for clinics with limited resources. \nOur results demonstrated substantial improvements through reinforcement learning \nover baseline GPT-2 across multiple evaluation metrics: BLEU improved by 428% \n(0.0021 to 0.0111), ROUGE-1 by 192% (0.0478 to 0.1397), ROUGE-2 by 233% \n(0.0064 to 0.0213), ROUGE-L by 223% (0.0408 to 0.1317). LLM-as-a-judge con-\nfirmed high contextual relevance and professionalism, whilst reinforcement learning \nachieved 99.34% emotion accuracy, representing a 48% improvement over baseline \nGPT-2's 66.96%. \nThe remainder of this study is structured as follows: Section 2 presents the related \nwork; Section 3 introduces the dataset and research questions; Section 4 details the \nmethodology, including the multi-component reward function and RL framework; \nSection 5 presents the experimental results; Section 6 discusses the findings and con-\ntextualises them with related work; and Section 7 concludes with key contributions \nand future directions. Finally, Section 8 details the ethics. \n2 \nRelated Work \n2.1 \nAI-Assisted Therapeutic Dialogue Tools in Clinical Practice \nThe integration of AI-assisted dialogue tools in mental health services has emerged as \na promising approach to enhance therapeutic communication while maintaining nec-\nessary clinical supervision. Telehealth services have demonstrated effectiveness as \nscalable and cost-effective alternatives to traditional face-to-face mental health ser-\nvices, helping to overcome barriers such as social stigma, limited accessibility, and \nhigh costs [17] [18]. Similarly, text-based therapeutic interventions have also shown \npromise, with email-based exercises incorporating positive psychology interventions \n"}, {"page": 4, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n4 \ndemonstrating improvements in stress levels and psychological well-being [19], indi-\ncating that text-based digital therapeutic communication can effectively support psy-\nchological health. \nAI-assisted dialogue generation tools, when integrated into supervised therapeutic \nsettings, offer potential solutions to enhance communication quality while addressing \nthese limitations. These tools can support therapists by providing contextually appro-\npriate dialogue suggestions in real-time, whether during telehealth sessions or in-\nperson consultations. \nStudies have demonstrated the effectiveness of LLM-backed therapeutic support \ntools, which are valued for their non-judgmental nature and ability to provide struc-\ntured conversational frameworks that enhance patient comfort [20]. When used under \nclinical supervision, these tools can offer consistent dialogue suggestions and emo-\ntional support frameworks during periods when therapists need alternative phrasing \noptions or guidance on addressing sensitive topics [20]. The key distinction lies in \npositioning these tools as augmentative rather than replacement technologies, ensur-\ning that clinical judgment and human empathy remain central to therapeutic practice \nwhile leveraging AI capabilities to enhance communication effectiveness. \n2.2 \nFine-Tuning LLMs for Mental Health Support \nHowever, existing tools fail to address core clinical needs. Much of the literature ex-\nploring the use of pre-trained LLMs for mental health support (MHS) relies heavily \non prompt engineering approaches. For example, Gabriel et al. [21] found that GPT-4 \ncan generate responses with nearly 45% greater overall empathy than human peer-to-\npeer responses. This contrasts with human therapists, who balance empathy with neu-\ntrality to avoid overwhelming patients [22] \nBeyond prompt engineering, supervised fine-tuning approaches have shown signif-\nicant promise [23]. Jin et al. [24] demonstrated that fine-tuned models outperform \npre-trained models on emotional support tasks, yet argued that fine-tuned models \nstruggle with human-like emotional support and risk producing empty, repetitive out-\nputs. These findings highlight the need for more sophisticated training approaches \nthat go beyond traditional supervised learning. \nRL has emerged as a promising complementary approach to address these limita-\ntions. Sharma et al. [15] applied RL to DialoGPT, a large dialogue generation model \nbased on GPT-2, for empathic rewriting, aiming to transform low-empathy conversa-\ntional posts to higher empathy responses. While Sharma et al.'s [15] work demon-\nstrated the feasibility of RL in improving response generation, achieving a BLEU \nscore of 0.1391, they focused on â€œempathic rewritingâ€ (improving existing low-\nempathy posts) rather than generative dialogue. Their reward function prioritises par-\naphrasing quality, not clinical appropriateness â€“ making it irrelevant for real-time \ntherapeutic interactions. Similarly, Saha et al. [16] employed RL to fine-tune GPT-2 \n[25] for empathetic text generation using a combined reward function including \nBLEU, ROUGE-L, and sentiment scores. However, their focus on lexical and senti-\nment metrics means their model cannot distinguish between contextually relevant \nempathy. For example, a patient with PTSD experiencing flashbacks requires neutral, \ngrounding responses [22], but existing RL-fine-tuned small LM models (e.g., [16]) \nwould reward overly empathetic responses (e.g., â€œThat must be terrifying â€“ youâ€™re so \n"}, {"page": 5, "text": "5   \nE. H.Q. Zhang and J. Ive \nbraveâ€) that may exacerbate hyperarousal. These studies primarily focused on general \nempathy metrics rather than clinically specific emotional appropriateness and contex-\ntual relevance, indicating a gap that our work aims to address.  \n3 \nDataset \n3.1 \nTraining Dataset \nTable 1. Dialogue characteristics of the MESC dataset \n \nTotal Records \nUnique \nProblem \nTypes \nMin. Dia-\nlogue Turns \nMax. Dia-\nlogue Turns \nAvg. Dia-\nlogue Turns \nTrain \n815 \n15 \n8 \n99 \n28.4 \nVal \n102 \n15 \n11 \n47 \n26.6 \nTest \n102 \n15 \n9 \n57 \n28.6 \nTable 1 describes the structure of the MESC dataset [26], which was selected to \naddress the context-emotion modeling deficit of prior work. Notably, this dataset \ncaptures multimodal cues â€“ including spoken dialogue, facial expressions, and body \nlanguage â€“ alongside text, thereby addressing a critical limitation of earlier text-only \nemotional support datasets. The dataset is adapted from the TV series In Treatment, \nwhich features professional actors portraying therapists and patients in realistic thera-\npy sessions. The performances were validated by mental health experts at the Shang-\nhai Mental Health Centre to ensure alignment with genuine therapeutic interactions \n[26]. \nThe dataset includes seven emotion categories (anger, sadness, depression, disgust, \nfear, joy, and neutral) with labels calibrated by GPT-3.5 and manually validated by \nemotional support researchers. Fig. 1 shows the emotion distribution, in which neutral \nemotions are dominant for therapists â€“ a reflection of clinical best practices [22] that \nguided our reward functionâ€™s emphasis on emotion alignment. \n \nFig. 1. Distribution of emotion categories by speaker (MESC dataset). \n"}, {"page": 6, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n6 \n3.2 \nEvaluation Dataset \nThe ESConv dataset is a high-quality collection of emotional support dialogues be-\ntween â€œhelp-seekersâ€ (simulating patients) and â€œsupportersâ€ (simulating therapists), \nwith rich metadata, including emotion types, problem categories, and support strate-\ngies [27]. It was chosen to test generalizability beyond MESCâ€™s therapy-specific sce-\nnarios. As shown in Table 2, it contains 1300 annotated dialogues with problem cate-\ngories and support strategies, providing a complementary context of â€œemotional sup-\nportâ€. To align with our context-emotion framework, we used the roberta-base-\ngo_emotions model to classify ESConvâ€™s utterances into MESCâ€™s 7 emotion catego-\nries. ESConvâ€™s dialogues focus on â€œemotional supportâ€ (rather than MESCâ€™s â€œthera-\npeutic sessionsâ€) - a related but distinct context. This ensures the modelâ€™s improve-\nments are not limited to the training dataâ€™s specific scenario but apply to broader men-\ntal health support contexts. \nTable 2. Dialogue characteristics of the ESConv dataset \nMetric \nValue \nTotal conversations \n1300 \nMinimum dialogue turns \n16 \nMaximum dialogue turns \n120 \nMean dialogue turns \n29.51 \nMedian dialogue turns \n27.0 \nMean words per turn \n16.4 \n3.3 \nData Preprocessing \nDuring preprocessing, text extraction isolated authentic speech from metadata de-\nscriptions using regular expressions to remove patterns beginning with â€œThe speakerâ€ \nor â€œThe emotion stateâ€. Utterances were then concatenated before changing speakers. \nSequence Filtering. A 128-token threshold (see Fig. 2) was selected to preserve \ncomputational efficiency while retaining ~95% of data. This ensures that the model \nprocesses complete contextual-emotional units without excessive padding. \nTokenizer Setup and Vocabulary Extension. This study extended the original GPT-\n2 tokenizer (~50,257 tokens) with custom special tokens (<problem>, <user>, <us-\ner_emotion>, <therapist>, <therapist_emotion>) and seven emotion tokens ([â€œangerâ€, \nâ€œsadnessâ€, â€œdepressionâ€, â€œdisgustâ€, â€œfearâ€, â€œjoyâ€, â€œneutralâ€]). This structure (Table 3) \nenables delineation of dialogue components, allowing the model to distinguish be-\ntween patient context, utterances, and emotional states. Emotion tokens are treated as \natomic units to preserve semantic integrity â€“ otherwise, subword tokenisation might \nfragment â€œsadnessâ€ into [â€œsadâ€, â€œnessâ€]. Crucially, it also allows explicit generation \nof emotion tokens. \n"}, {"page": 7, "text": "7   \nE. H.Q. Zhang and J. Ive \n \nFig. 2. Sequence length distribution and coverage analysis for 128-token threshold. \n \nTable 3. Sequence construction \n<bos><problem>problem_text<user>user_text<user_emotion>emotion \n<therapist>therapist_text<therapist_emotion>emotion<eos> \nLabel Creation. To train the model for response prediction, a selective labelling \nstrategy masked context tokens (<problem>, <user>, <user_emotion>) and their con-\ntent with a mask of -100. This methodology employed HuggingFace's Transformers \nlibrary, where mask token -100 creates an â€œignore indexâ€ without contributing to \ngradient loss. The response section employed differentiated labelling: whilst <thera-\npist> markers were masked (-100), therapist text and <therapist_emotion> tokens \nreceived standard labels for prediction. This enabled response generation, including \nemotional state prediction, allowing the generation of content and emotional classifi-\ncation during inference. \n4 \nMethodology \n4.1 \nExperiment Setup \nAll experiments were conducted on Google Colab using an NVIDIA A100 GPU \n(40GB RAM). The baseline GPT-2 model (124M parameters) was imported via Hug-\ngingFace Transformers. RL used TRL 0.11.3 with PPO. Training was monitored us-\ning TensorBoard for SFT and Weights & Biases for RL experiments. \n4.2 \nBaseline GPT-2 \nA retokenizer adapted GPT-2 to the structured input format (Table 3) for fair compar-\nison. \n"}, {"page": 8, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n8 \n4.3 \nSupervised Fine-tuning (With Therapist Emotions) \nSFT used batches of 32 sequences (128 tokens) with AdamW optimizer (lr=2e-5), and \ncross-entropy loss. The model processed context-emotion unified inputs to establish a \nbaseline for clinical dialogue generation. \n4.4 \nSupervised Fine-tuning (No Therapist Emotions) \nAn ablation study excluded <therapist_emotion> tokens to quantify the value of joint \ntext-emotion generation, producing only therapist text without emotional classifica-\ntion. This model helped quantify the joint text-emotion generation task's contribution \nto dialogue quality. The same hyperparameters were used as SFT (with emotions). \n4.5 \nSupervised Fine-tuning + Reinforcement Learning \nThis study adapted the standard RL framework consisting of contexts C, response \ngeneration actions G, a policy Ï€, and multi-dimensional rewards R. In this framework, \ngiven a context c âˆˆ C, the model generated a response g âˆˆ G according to the policy \nÏ€: C â†’ P(G), where P(G) represented the probability distribution over possible re-\nsponses. RL was implemented on top of SFT according to Stiennon et al. 's [10] clas-\nsical RL approach that utilised SFT as a stable starting policy and established basic \ntask actions. \n \nFig. 3. The overall architectural representation of the reinforcement learning framework. \nBy employing a KL divergence penalty in RL, this approach prevented the RL pol-\nicy from drifting too far from the SFT policy. Additionally, pre-trained token distribu-\ntions may skew policy updates towards frequent words regardless of rewards. To \nmitigate this, this study integrated reward scaling to a range of -10 to 10 into the re-\nward function [28]. A learning rate of 1 Ã— 10âˆ’6, training epochs of 8, batch size of \n16, and data shuffling disabled were employed. Fig. 3 presents the architectural repre-\nsentation of the reinforcement learning framework. \nProximal Policy Optimization (PPO). PPO is a policy gradient method addressing \ntraining instability in reinforcement learning by constraining policy updates through a \nclipped objective function. The algorithm enables multiple epochs of minibatch up-\n"}, {"page": 9, "text": "9   \nE. H.Q. Zhang and J. Ive \ndates whilst preventing excessive parameter changes that could destabilise training \n[29]. This stability was crucial for this study, where catastrophic forgetting posed \nrisks to response patterns acquired during SFT. \nThe PPO objective optimised the clipped surrogate loss [see Eq. (1) and Eq. (2)]: \nğ¿ğ¶ğ¿ğ¼ğ‘ƒ(ğœƒ) = ğ¸Ì‚ğ‘¡ [min(ğ‘Ÿğ‘¡(ğœƒ)ğ´Ì‚ğ‘¡, ğ‘ğ‘™ğ‘–ğ‘(ğ‘Ÿğ‘¡(ğœƒ), 1 âˆ’ğœ€, 1 + ğœ€)ğ´Ì‚ğ‘¡)]             (1) \nwhere, \nğ‘Ÿğ‘¡(ğœƒ) =\nğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)\nğœ‹ğœƒğ‘œğ‘™ğ‘‘(ğ‘ğ‘¡|ğ‘ ğ‘¡)                                                  (2) \nrepresents the probability ratio between current and previous policies, ğ´Ì‚ğ‘¡denoted the \nadvantage estimate that measures how much better an action is compared to the ex-\npected value at that state, and ğœ€ was the clipping parameter that constrained policy \nupdates within a trust region. \nContext. The context enabled response generation specific to the patient's presenting \nissues, utterances, and emotional states at each dialogue turn. The context space con-\nsisted of structured user input processed through retokenisation. At each generation \nstep, the context for each user utterance was represented by Ci, containing: problem \ntype Pi, user utterance Ui, and user emotional state Ei, denoted by: \nğ¶ğ‘–=< ğ‘ğ‘Ÿğ‘œğ‘ğ‘™ğ‘’ğ‘š> ğ‘ƒğ‘–< ğ‘¢ğ‘ ğ‘’ğ‘Ÿ> ğ‘ˆğ‘–< ğ‘¢ğ‘ ğ‘’ğ‘Ÿğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›> ğ¸ğ‘– \nThe ğ¶ğ‘– is followed by the response prompt <therapist>, creating the complete input \nrepresentation of context as ğ¶ğ‘–< ğ‘¡â„ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–ğ‘ ğ‘¡> \nAction. The model executed actions through structured response generation, produc-\ning both therapist textual responses and corresponding emotional classifications. The \naction aimed to generate contextually appropriate responses recognising context Ci \nwhilst aligning with appropriate emotional classifications. \nThe model executed two generation components: ğ‘1 produced response text, and \nğ‘2 predicted the appropriate therapist emotional stance. Action space ğ´1 consisted of \nall possible response sequences within vocabulary constraints. Action space ğ´2 en-\ncompassed seven discrete emotional categories existing within the dataset: {anger, \nsadness, depression, disgust, fear, joy, neutral}. The complete action was denoted as \nğ‘= (ğ‘1, ğ‘2) âˆˆğ´1 âˆ—ğ´2. Following Ci, structured action generated responses in the \nsequence < ğ‘¡â„ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›> ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›ğ‘¤ğ‘œğ‘Ÿğ‘‘< ğ‘’ğ‘œğ‘ >. \nPolicy. The policy operated by encoding structured input context ğ¶ğ‘–, then generating \ntext by computing probability distributions over vocabulary tokens through decoder \nlayers that maximized ğ‘ƒ(ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’|ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡). The model continued generation be-\nyond text to produce the emotional alignment sequence. Generation employed sam-\npling parameters (top-p=1.0, top-k=0.0) recommended by TRL documentation to \n"}, {"page": 10, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n10 \nensure response stability, with sequence completion triggered by the end-of-sequence \ntoken. \nRewards. Our novel composite reward function [Eq. (3)] addresses the limitation of \nprior RL modelsâ€™ overreliance on lexical metrics through complementary metrics: text \nquality, emotional alignment, contextual relevance, empathetic content, and sentiment \nappropriateness. The ensemble approach further addressed reward hacking, where \nmodels exploit individual reward functions by generating responses that achieve high \nscores through rule-following whilst producing clinically meaningless content [30].  \nText Fluency. The text quality reward function ğ‘“ğ‘ğ‘¢ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¦(Â·) measured response coher-\nence and appropriateness.  \nğ‘Ÿğ‘ = ğ‘“ğ‘ğ‘¢ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¦(ğ‘¡â„ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’) \nImplementation incorporated repetition detection and meaningless pattern identifica-\ntion, penalising responses with excessive n-gram repetition and incoherent content. \nEmotional Alignment. Emotional alignment rewarded suitable emotional classifica-\ntions and penalised misalignment. The reward ğ‘Ÿ_ğ‘’ evaluated emotion prediction accu-\nracy and emotion token presence: \nğ‘Ÿğ‘’ = ğ‘“ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›(ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›, ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›, â„ğ‘ğ‘ _ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›_ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›) \nThis rewarded exact emotional matches and appropriate categories (positive, negative, \nneutral), whilst penalising conflicts between incompatible categories. \nContextual Relevance. Contextual relevance ensured responses addressed specific \npatient concerns rather than generic interventions. Semantic similarity measured con-\ntextual appropriateness: \nğ‘Ÿğ‘Ÿ =  ğ‘ğ‘œğ‘ _ğ‘ ğ‘–ğ‘š(ğ‘’ğ‘šğ‘ğ‘’ğ‘‘(ğ‘¡â„ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–ğ‘ ğ‘¡_ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’), ğ‘’ğ‘šğ‘ğ‘’ğ‘‘(ğ‘¢ğ‘ ğ‘’ğ‘Ÿ_ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡)) \nSemantic embeddings were computed using Sentence-BERT (all-MiniLM-L6-v2) \n(Appendix 1) to measure similarity between responses and user inputs. This model \nwas chosen for its ability to vectorise text for semantic comparison. \nEmpathetic Content. A pre-trained empathy classification model (paragon-\nanalytics/bert_empathy) (Appendix 1) evaluated the empathetic quality of generated \nresponses: \nğ‘Ÿğ‘’ğ‘šğ‘ = ğ‘“ğ‘’ğ‘šğ‘(ğ‘¡â„ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–ğ‘ ğ‘¡_ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’) \n"}, {"page": 11, "text": "11  \n \nE. H.Q. Zhang and J. Ive \nThe empathy model was chosen for its ability to predict empathic language content. \nThis assessed empathetic quality, rewarding responses demonstrating understanding \nand emotional support. \nSentiment Appropriateness. Sentiment ensured responses conveyed appropriately \npositive sentiment, distinct from the discrete emotional categories used elsewhere. \nWhile emotion classification captured specific emotional states, sentiment analysis \nmeasured the overall positive/negative nature of the response \nğ‘Ÿğ‘  = ğ‘“ğ‘ ğ‘’ğ‘›ğ‘¡(ğ‘¡â„ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–ğ‘ ğ‘¡_ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’) \nwhere, ğ‘“ğ‘ ğ‘’ğ‘›ğ‘¡(Â·) employed DistilBERT sentiment classification (distilbert-base-\nuncased-finetuned-sst-2-english) (Appendix 1). \nTotal Reward. The final composite reward function combined all components with \noptimised weighting: \nğ‘Ÿğ‘¡ğ‘œğ‘¡ğ‘ğ‘™= ğ‘¤ğ‘ Ã— ğ‘Ÿğ‘ + ğ‘¤ğ‘’ Ã— ğ‘Ÿğ‘’ + ğ‘¤ğ‘Ÿ Ã— ğ‘Ÿğ‘Ÿ + ğ‘¤ğ‘’ğ‘šğ‘ Ã— ğ‘Ÿğ‘’ğ‘šğ‘ + ğ‘¤ğ‘  Ã— ğ‘Ÿğ‘         (3) \nwhere, text quality (ğ‘¤ğ‘ = 1.1) and emotional alignment (ğ‘¤ğ‘’ = 1.2) received higher \nweighting, contextual understanding (ğ‘¤_ğ‘Ÿ = 1.1) was emphasised, whilst supportive \ncontent components (ğ‘¤ğ‘’ğ‘šğ‘ = 0.7, ğ‘¤ğ‘  = 0.7) received moderate weighting to balance \nclinical appropriateness with empathetic responsiveness. \nWeight selection was based on preliminary experiments evaluating response quali-\nty across different configurations. The weights do not sum to 1.0 (total = 4.8) as this \napproach prioritised absolute reward magnitudes rather than relative proportions, \nwhere stronger gradients were needed to overcome the conservative learning rate \n(1 Ã— 10âˆ’6) used for stability. \n4.6 \nHyperparameter Tuning \nGrid search optimised top_p, top_k, and temperature parameters over combined \nBLEU and ROUGE-1-2-L metrics. This optimisation balanced response diversity and \nclinical appropriateness across the three model variants: SFT (No emotion), SFT \n(With emotion), and RL-SFT (With emotion). Full tuning parameters are provided in \nAppendix 2 (Table 9). \n4.7 \nEvaluation \nEvaluation used automated metrics as follows: LLM-as-a-judge on clinical criteria, \nand human evaluation-ensuring assessment aligns with clinical utility, not just NLP \nperformance [27].  \n \n1. BLEU: precision against reference responses [31]. \n2. ROUGE-1-2-L: N-gram overlap between generated and reference responses [32]. \n"}, {"page": 12, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n12 \n3. METEOR: Automatic metric for machine translation evaluation based on general-\nised unigram matching [33]. \nLLM-as-a-judge assessed all three model variants using GPT-4o-mini via OpenAI \nAPI with prompt engineering (Table 10 in Appendix 3) to evaluate 100 samples from \nthe testing dataset on a 1-5 scale across six emotional support criteria: \n1. Therapeutic Rapport - Emotional connection and trust-building. \n2. Active Understanding - Comprehension and reflection of user's state. \n3. Relevance Focus - Addressing specific problems without going off-topic. \n4. Practical Helpfulness - Providing actionable guidance and direction. \n5. Professional Appropriateness - Maintaining boundaries. \n6. Emotional Validation - Acknowledging and validating user emotions. \nThese criteria were adapted from Yuan et al.â€™s [34] human evaluation metrics to cre-\nate a comprehensive framework tailored for therapeutic communication assessment. \nHuman validation was conducted through manual inspection of a subset of GPT-4o-\nmini evaluations to ensure scoring consistency and appropriateness of the automated \njudgments. \n5 \nResults \n5.1 \nAutomated Evaluation  \nAutomated evaluation results (see Table 4) from baseline GPT-2 to the reinforcement \nlearning fine-tuned model demonstrate substantial improvements across BLEU, \nROUGE-1, ROUGE-2, ROUGE-L, and METEOR measures. Baseline GPT-2 \nachieved minimal scores across BLEU (0.0021) and ROUGE metrics (0.0478, 0.0064, \n0.0408), indicating poor alignment with reference responses. Interestingly, baseline \nGPT-2 achieved the highest METEOR score (0.0670) across all models, likely due to \nMETEOR's emphasis on synonym and stem matching, which may favour more di-\nverse vocabulary generation over precise alignment with references [33]. \nTable 4. Automated evaluation quality metrics \nModel \nBLEU \nROUGE-1 ROUGE-2 ROUGE-L METEOR \nGPT-2 \n0.0021 \n0.0478 \n0.0064 \n0.0408 \n0.0691 \nSFT (No therapist emotions) \n0.0106 \n0.1160 \n0.0159 \n0.1058 \n0.0433 \nSFT (Therapist emotions) \n0.0108 \n0.1164 \n0.0175 \n0.1076 \n0.0485 \nReinforcement Learning - SFT \n0.0111 \n0.1397 \n0.0213 \n0.1317 \n0.0581 \n \nBoth SFT variants demonstrated improvements over baseline across BLEU and \nROUGE metrics, though with reduced METEOR scores. The emotion-inclusive SFT \nmodel outperformed the no-emotion variant across most metrics, with notable im-\nprovements in ROUGE-2 (+10.1%) and ROUGE-L (+1.7%). \n"}, {"page": 13, "text": "13  \n \nE. H.Q. Zhang and J. Ive \nThe RL-aligned model achieved the strongest overall performance, demonstrating \nsubstantial gains in ROUGE-1 (+20.0% over emotion SFT) and ROUGE-L (+22.4% \nover emotion SFT), suggesting enhanced lexical overlap with reference responses. \nTable 5. Automated evaluation quality metrics (ESConv dataset) \nModel \nBLEU ROUGE-1 \nROUGE-2 \nROUGE-L \nMETEOR \nSFT (Therapist emotions) \n0.0050 \n0.1046 \n0.0089 \n0.0925 \n0.0354 \nReinforcement Learning - SFT 0.0042 \n0.1110 \n0.0092 \n0.1002 \n0.0290 \n \nIn Table 5, automated evaluation on the ESConv dataset assessed model generali-\nsation beyond the training domain. The SFT emotion-aware model achieved moderate \ncross-dataset performance. The RL-aligned model demonstrated mixed results, with \nimprovements in ROUGE metrics (+6.1% ROUGE-1, +3.4% ROUGE-2, +8.3% \nROUGE-L) compared to emotion-inclusive SFT, but decreased performance in BLEU \n(-16%) and METEOR (-18.1%). \nTable 6. Emotion token generation accuracy performance \nEmotion Accuracy \nModel \n0.6696 \nGPT-2 \n0.8084 \nSFT (No therapist emotions) \n0.9229 \nSFT (Therapist emotions) \n0.9934 \nReinforcement Learning \nIn Table 6, emotion classification accuracy revealed progressive improvements \nacross model variants. Baseline GPT-2 achieved 66.96% accuracy, whilst the no-\nemotion SFT model reached 80.84%. The emotion-aware SFT model achieved \n95.37%, demonstrating the effectiveness of joint text-emotion training. The RL model \nattained 99.34% accuracy, representing a 4.16% improvement over emotion-aware \nSFT. This high performance demonstrates that the composite reward function effec-\ntively enhanced emotional alignment capabilities. \n5.2 \nLLM-as-a-Judge Evaluation \nIn Table 7, LLM-as-a-judge evaluation (GPT-4o-mini) revealed a clear performance \nhierarchy across all models tested on 100 dialogue samples. The RL model achieved \nthe highest overall average score of 1.718, followed by SFT at 1.655, and baseline \nGPT-2 at 1.062. \nThe RL model demonstrated notable superiority in relevance focus (1.830 vs SFT's \n1.740), representing 5.2% improvement. This aligns with the contextual relevance \ncomponent in the composite reward function, suggesting successful optimisation of \ncontext-aware response generation. In active understanding, the RL model achieved \n1.670 compared to SFT's 1.570 (6.4% improvement), indicating enhanced compre-\nhension of patient concerns. In emotional validation, RL achieved 1.640 versus SFT's \n1.530 (7.2% improvement), validating the emotion-focused reward components. \n"}, {"page": 14, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n14 \nHowever, both advanced models showed similar performance in practical helpfulness \n(RL: 1.490, SFT: 1.480), with only marginal improvement over baseline GPT-2 \n(1.020). \nTable 7. LLM-as-a-judge evaluation results \nMetric \nBaseline GPT-2 \nSFT Model \nRL Model \nTherapeutic Rapport \n1.090 \n1.700 \n1.730 \nActive Understanding \n1.030 \n1.570 \n1.670 \nRelevance Focus \n1.070 \n1.740 \n1.830 \nPractical Helpfulness \n1.020 \n1.480 \n1.490 \nProfessional Appropriateness \n1.100 \n1.910 \n1.950 \nEmotional Validation \n1.060 \n1.530 \n1.640 \n \n5.3 \nHuman Evaluation \nManual inspection (samples in Table 8) revealed that LLM-as-a-judge had under-\nscored around 20 samples. Human evaluation showed that the RL model frequently \ngenerated question-oriented responses rather than providing direct therapeutic advice. \nFor instance (Table 8), when users express intense emotions such as depression re-\ngarding bereavement, the generated responses employ open-ended questioning tech-\nniques that encourage further exploration without emotional reactivity. However, this \nlikely explains the relatively low ROUGE scores, as reference responses were derived \nfrom conversations characterised by natural turn-taking, where not every utterance \ntends to be a question. Consequently, there was lower lexical overlap with reference \nresponses that contained more declarative statements. \nNevertheless, generated responses demonstrated strong contextual relatedness, \nwith every response directly addressing specific keywords in patient utterances rather \nthan providing generic statements. The neutral emotions generated benefit therapeutic \nrapport. According to Gelso and Kanninen [22], effective therapists maintain neutrali-\nty to remain close to the patient's â€œzone of proximal developmentâ€, avoiding manipu-\nlation of patient emotional expression through excessive warmth. The RL model's \ntendency towards neutral emotional stances and seemingly low Emotional Validation \nscores reflects this professional therapeutic approach. \n"}, {"page": 15, "text": "15  \n \nE. H.Q. Zhang and J. Ive \nTable 8. Samples of generated responses from the SFT-RL model \nProblem \nType \nUser Text \nUser \nemotion \nGenerated Therapist \nResponse \nGenerated \nTherapist \nEmotion \nTherapeutic \nRelationship \nDid I offend you? \nneutral \nNo, you didn't offend \nme. You didn't say that \nyou were upset with \nme. \nneutral \nBereavement \nI can't stop thinking \nabout my grandmother \nwho passed away \ndepression \nAnd you wanna tell me \nwhat you wanna do for \nher? \nneutral \nPost-\nTraumatic \nStress Dis-\norder \nI keep having flashbacks \nfrom the accident \nfear \nDo you want to talk \nabout it? \nneutral \n6 \nDiscussion \nThis study presents a proof-of-concept for developing cost-effective, locally deploya-\nble therapeutic dialogue models using structured multimodal input formatting in GPT-\n2 combined with multi-component reward optimisation. The approach addresses criti-\ncal requirements for mental health applications: data sensitivity through local de-\nployment, computational efficiency for resource-constrained environments, and en-\nhanced safety through smaller model parameters that enable better oversight and con-\ntrol [20]. Our results confirm that the proposed approach directly addresses the limita-\ntions of existing methods: \n1. Clinical Empathy Alignment: Unlike SFT models [24] that produce surface em-\npathy, our RL modelâ€™s 99.34% emotion accuracy and strong LLM-as-a-judge \nscores in Emotional Validation demonstrate that integrating clinical-specific re-\nwards (emotional alignment, contextual relevance) generates empathy that is both \ngenuine and boundary-appropriate. This addresses Luo et al.â€™s [13] finding that pa-\ntients prefer human responses-our modelâ€™s empathy is tied to context, not scripted. \n2. Context-Emotion Integration: The RL modelâ€™s superior performance on Rele-\nvance Focus (+5.2% over SFT) and cross-dataset ROUGE improvements confirm \nthat structuring input as unified context-emotion units (vs. separate features) solves \nprior modelsâ€™ context-insensitivity. Unlike Saha et al. [16], our model seeks not to \ngenerate generic empathy phrases â€“ every response addresses specific user key-\nwords. \n3. Reward Function Superiority: Our multi-component reward function outper-\nforms generic metrics (e.g., BLEU, ROUGE) by prioritising clinical utility. The \nRL modelâ€™s higher Professional Appropriateness scores (1.95) demonstrate that \nrewarding therapeutic boundaries avoids the over-empathising trap of prior RL \nmodels [35]. \n"}, {"page": 16, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n16 \nNotably, these responses are generated upon GPT-2 (124M parameters), a decoder-\nonly model with inherent architectural limitations for complex conversational tasks. \nResearchers from Anthropic found that 'alignment tax' is inherent in RL training on \nsmall LLMs (<10B parameters) where NLP metrics performance may degrade after \nRL training [36]. \nComparing against existing empirical evidence, Sharma et al. [15] performed RL \non DialoGPT for empathetic rewriting, transforming low-empathy conversational \nposts to higher empathy. Whilst serving different purposes, it demonstrated RLâ€™s \nfeasibility in improving response generation, achieving a BLEU score of 0.1391. Alt-\nhough this score is significantly higher than our model's, this does not diminish our \nresults as their task differs fundamentally from dialogue generation. Empathetic re-\nwriting resembles machine translation â€“ improving existing text rather than generat-\ning new responses - creating an inherent advantage when using BLEU, a metric de-\nsigned to evaluate machine-translated text against reference translations.  \nEvaluating Saha et al.'s [16] work, which performed RL on GPT-2 using composite \nrewards of BLEU, ROUGE-L, and sentiment scores for empathetic text generation, \nthis study recognises that such metrics may prioritise lexical similarity over contextu-\nal appropriateness and often yield sparse, biased signals [28]. Whilst direct perfor-\nmance comparison is not meaningful due to different datasets and tasks, our approach \ndemonstrates that composite rewards incorporating contextual and emotional appro-\npriateness can achieve competitive lexical scores (ROUGE-L: 0.1317) without direct-\nly optimising for them.  \nAn important aspect of this model is improving current therapist generation ap-\nproaches by enhancing affective empathy, where SFT-emotion outperformed SFT-no-\nemotion across all metrics (BLEU: 0.0108 vs. 0.0106; ROUGE-1: 0.1164 vs. 0.1160; \nROUGE-2: 0.0175 vs. 0.0159; ROUGE-L: 0.1076 vs. 0.1058; METEOR: 0.0485 vs. \n0.0433). This suggests that generating emotion tokens before end-of-sentence tokens \nprovides dual benefits: improving lexical generation whilst demonstrating affective \nempathy, which Rudra et al. [35] emphasised as important for therapists to display \ngenuine emotional engagement rather than surface-level recognition. \nThis work's contribution lies in demonstrating a proof-of-concept for developing \nsmaller, safer therapeutic dialogue models suitable for sensitive healthcare data. By \nrestructuring GPT-2 for contextual-emotional input processing, we present a cost-\neffective multimodal alternative that addresses critical deployment constraints in men-\ntal health applications: data privacy through local deployment, computational effi-\nciency for resource-limited environments, and enhanced oversight capabilities inher-\nent in smaller model architectures. The SFT model's consistent improvements across \nautomated evaluation metrics, validated by LLM-as-a-judge evaluation showing 'Rel-\nevance Focus' as the second-highest metric, demonstrates that structured input effec-\ntively captures context-emotion relationships without requiring large-scale, resource-\nintensive models. This approach offers healthcare institutions a viable pathway for \nimplementing AI-assisted therapeutic tools whilst maintaining stringent data security \nand clinical supervision requirements. \n6.1 \nLimitations \nThis study acknowledges several important limitations. Firstly, significant class im-\nbalance in emotion labels within the training dataset resulted in overfitting towards \n"}, {"page": 17, "text": "17  \n \nE. H.Q. Zhang and J. Ive \nneutral therapist emotion generation. This creates bias where the model defaults to \nemotionally restrained responses, potentially limiting communication flexibility. \nWhilst this reflects realistic professional therapist approaches to maintaining clinical \nboundaries, it may reduce empathy and emotional responsiveness in contexts requir-\ning warmer interactions. This limitation appears in LLM-as-a-judge evaluation re-\nsults, where the model achieved the lowest score for Practical Helpfulness whilst \nrating highest in Professional Appropriateness, suggesting focus on professional be-\nhaviour over empathetic engagement.  \nAnother significant limitation is the absence of empirical hyperparameter optimisa-\ntion for reward weights prior to training. Given limited computational resources and \ntime, systematic optimisation techniques were omitted. Human evaluation reveals the \nrelevance component may be disproportionately weighted, leading to suboptimal \nconversational patterns where the model occasionally uses simplistic paraphrasing \nrather than generating meaningful responses. For instance, when presented with â€œMy \nex keeps texting me, and I don't know how to respondâ€, the model sometimes produc-\nes repetitive responses like â€œSo you're not sure how to respond?â€ This suggests exces-\nsive relevance weighting may encourage surface-level lexical matching rather than \ndeeper engagement. \nSignificant limitations exist in using LLMs for mental health support. First, while \nthese models simulate empathy effectively, they frequently exaggerate it through \nlengthy responses, creating misleading emotional connections and potentially foster-\ning inappropriate user reliance [35] [37]. Second, generic prompt engineering fails to \ncapture contextual awareness, resulting in inconsistent responses that undermine reli-\nability [11] [38]. Third, LLMs lack clinical judgement, making them prone to generat-\ning harmful recommendations in legally sensitive psychological contexts [39]. Fourth, \npre-trained models exhibit racial and demographic biases, producing stereotyped re-\nsponses and hallucinated inaccuracies that misalign with users' values [21] [12]. Fi-\nnally, commercially available LLMs raise privacy concerns regarding API vulnerabil-\nities and data leakage when handling sensitive information [40].  \n6.2 \nFuture Research \nWhilst this study employed a relatively small LLM with only 124M parameters, \nqualitative examples demonstrated GPT-2's capability to produce responses that ef-\nfectively capture context and remain highly relevant to user input. Fine-tuning on \ntherapy session data shows its effectiveness in enabling LLMs to replicate profession-\nalism and appropriate speech tones, potentially addressing limitations of generalised \nLLMs that produce superficial empathetic responses through lengthy, generic outputs. \nThis study highlights the critical need for higher-quality datasets to better align LLMs \nwith human needs and serve vulnerable groups requiring specialised support.  \nThe structured input approach demonstrated success and reveals potential for im-\nproving emotional support CAs to capture multimodal information. Whilst multimo-\ndality here involves preprocessing video and audio signals into text, future research \ncould employ RL on graph-convolutional-network-based multimodal classifiers [41], \nrestructuring input formats for multimodal response generation.  \n"}, {"page": 18, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n18 \n7 \nConclusion \nThis study directly addresses four critical methodological gaps in existing therapeutic \ndialogue generation: 1) SFTâ€™s failure to balance clinical rigor and empathy; 2) RLâ€™s \noverreliance on generic reward metrics; 3) context-emotion decoupling; and 4) de-\nployment barriers. Our innovations â€“ a clinical-specific multi-component reward \nfunction, an RL framework for context-emotion integration, and a lightweight GPT-2 \nimplementation â€“ produce a model that achieves 99.34% emotion accuracy, outper-\nforms baseline and SFT variants on clinical criteria, and enables local deployment. \nThe results demonstrate that small LLMs, when optimised for clinical priorities (not \njust NLP metrics), can serve as valuable assistive tools for therapists â€“ enhancing \naccessibility to mental health support while maintaining clinical safety and privacy. \nThis work paves the way for practical, ethical AI-assisted therapeutic tools that ad-\ndress the global mental health burden. \n8 \nEthics \nAll datasets are open source from published articles. Recognising potential harms of \nLLMs in mental health support [42], including retraumatisation, unreliable advice, \noverreliance, and safety bypass risks, this study positions the model as a clinical assis-\ntance tool rather than a direct patient interface. Implementation follows a tiered ethi-\ncal framework [43]: Level 1 Safeguards (expert-use disclaimers, local deployment), \nLevel 2 Protections (human oversight, transcript review), and Level 3 Responsibility \n(transparent documentation of limitations and capabilities). \nDeclaration on Generative AI. GenAI has been used for supporting drafting and \nlanguage refinement. \nAppendices \nAppendix 1: URLs for Tools \nTensorboard: https://www.tensorflow.org/tensorboard  \nHuggingFace Transformers library https://huggingface.co/docs/transformers/en/index  \nHuggingFace Transformers GPT2 https://huggingface.co/openai-community/gpt2  \nHuggingFace TRL Reinforcement Learning PPO:  \nhttps://huggingface.co/docs/trl/main/en/ppo_trainer  \nhttps://pypi.org/project/trl/0.11.3/  \nWeights & Biases: https://wandb.ai/site/  \nroberta-base-go_emotions:  \nhttps://huggingface.co/SamLowe/roberta-base-go_emotions  \nall-MiniLM-L6-v2: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  \ndistilbert-base-uncased-finetuned-sst-2-english:  \nhttps://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english  \n"}, {"page": 19, "text": "19  \n \nE. H.Q. Zhang and J. Ive \nparagon-analytics/bert_empathy:  \nhttps://huggingface.co/paragon-analytics/bert_empathy \nAppendix 2 \nTable 9. Hyperparameters \nModel \nRange \nGPT-2 \nTOP_P_VALUES = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.00] \nTOP_K_VALUES = [0, 5, 10, 15, 20, 25, 30, 35] \nTEMPERATURE_VALUES = [1.0, 1.1, 1.2, 1.3] \nSFT (With emotions) \nTOP_P_VALUES = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.00] \nTOP_K_VALUES = [0, 5, 10, 15, 20, 25, 30, 35] \nTEMPERATURE_VALUES = [1.0, 1.1, 1.2, 1.3] \nReinforcement Learning \n- SFT (With emotions) \nTOP_P_VALUES = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.00] \nTOP_K_VALUES = [0, 5, 10, 15, 20, 25, 30, 35] \nTEMPERATURE_VALUES = [1.0, 1.1, 1.2, 1.3] \nAppendix 3 \nTable 10. Prompt for LLM-as-a-judge Evaluation \nCriterion \nDescription \nCriteria examples \nTherapeutic \nRapport \nEvaluates emotional connec-\ntion, compassion, active \nlistening skills, and ability to \nprovide affirmation and \ncomfort to build trust with \nthe user \nLook for: Warmth, validation, emotional \nattunement, supportive tone, building \ntrust<br>Poor examples: Dismissive, cold, \njudgmental responses \nActive Under-\nstanding \nMeasures comprehension of \nthe user's emotional state and \nsituation through effective \nparaphrasing, reflection, and \ndemonstration of understand-\ning \nLook for: Accurate reflection of user's \nwords, paraphrasing key points, demon-\nstrating comprehension of the situa-\ntion<br>Poor examples: Misunderstanding, \nignoring context \nRelevance \nFocus \nAssesses how well the re-\nsponse addresses the specific \nproblem or situation men-\ntioned by the user without \ngoing off-topic \nLook for: Directly addressing the stated \nproblem, staying on topic, responding to \nuser's specific concerns<br>Poor examples: \nTangential responses, changing subjects \nPractical Help-\nfulness \nEvaluates whether the re-\nsponse provides practical \nvalue, support, and clear \ntherapeutic direction to help \nthe user progress \nLook for: Actionable guidance, therapeutic \ntechniques, constructive suggestions, \nmovement toward solutions<br>Poor ex-\namples: Vague responses, no direction \nProfessional \nAppropriateness \nMeasures appropriate thera-\npeutic interpretation of \nstatements and judicious use \nof self-disclosure while \nmaintaining professional \nboundaries \nLook for: Proper therapeutic boundaries, \nappropriate interpretations, professional \nlanguage and approach<br>Poor examples: \nOversharing, inappropriate interpretations \nEmotional \nAssesses the therapist's abil-\nLook for: Acknowledging user's feelings, \n"}, {"page": 20, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n20 \nValidation \nity to acknowledge, validate, \nand appropriately respond to \nthe user's current emotional \nstate \nnormalizing emotions, showing understand-\ning of emotional experience<br>Poor ex-\namples: Dismissing feelings, minimizing \nemotions \nprompt = f\"\"\"You are an expert evaluator of therapy chatbot responses. Your task is \nto evaluate a therapist chatbot's response based on the specific criterion provided. \n**Context:** \n1. Problem Type: {problem_type}  # Type of user's psychological problem (e.g., anx-\niety, depression) \n2. User Input: â€œ{user_text}â€  # Original input content from the user \n3. User Emotion: {user_emotion}  # Emotional state of the user (e.g., frustrated, anx-\nious) \n4. Therapist Response: â€œ{therapist_response}â€  # Output from the therapy chatbot \n**Evaluation Criterion: {criteria}** \nDescription: {self.evaluation_criteria.get(criteria, â€œGeneral therapeutic effective-\nnessâ€)}  # Criterion definition from the evaluation table \nEvaluation Focus: {criteria_examples.get(criteria, â€œEvaluate overall therapeutic quali-\nty.â€)}  # Key observation points for the criterion \n**Instructions:** \n1. Evaluate the therapist response on a scale of 1-5 for the given criterion: \n   - 1: Poor - Completely inadequate or inappropriate for this criterion \n   - 2: Below Average - Some issues or limitations in this specific area \n   - 3: Average - Adequate performance in this criterion but room for improvement \n   - 4: Good - Well-executed in this criterion with minor areas for enhancement \n   - 5: Excellent - Outstanding and highly effective in this specific criterion \n2. Provide a focused explanation (2-3 sentences) specifically about this criterion. \n3. Consider how well the response performs in this particular dimension of therapeu-\ntic effectiveness. \n**Response Format:** \nScore: [1-5] \nExplanation: [Your justification focused specifically on this criterion] \n**Your Evaluation:**\"\"\" \nReferences \n1. â€˜Mental Illness - National Institute of Mental Health (NIMH)â€™. Accessed: Sep. 13, 2025. \n[Online]. Available: https://www.nimh.nih.gov/health/statistics/mental-illness \n"}, {"page": 21, "text": "21  \n \nE. H.Q. Zhang and J. Ive \n2. P. Corrigan, â€˜How stigma interferes with mental health care.â€™, Am. Psychol., vol. 59, no. 7, \npp. 614â€“625, Oct. 2004, doi: 10.1037/0003-066X.59.7.614. \n3. B. M. CoÃªlho, G. L. Santana, M. C. Viana, Y.-P. Wang, and L. H. Andrade, â€œI donâ€™t need \nany treatmentâ€ â€“ barriers to mental health treatment in the general population of a megaci-\ntyâ€™, Braz. J. Psychiatry, vol. 43, no. 6, pp. 590â€“598, Dec. 2021, doi: 10.1590/1516-4446-\n2020-1448. \n4. J. Goodwin, E. Savage, and A. Oâ€™Donovan, â€œI Personally Wouldnâ€™t Know Where to Goâ€: \nAdolescentsâ€™ Perceptions of Mental Health Servicesâ€™, J. Adolesc. Res., vol. 39, no. 5, pp. \n1384â€“1412, Sep. 2024, doi: 10.1177/07435584221076056. \n5. S. Li, M. Chen, P. L. Liu, and J. Xu, â€˜Following Medical Advice of an AI or a Human \nDoctor? Experimental Evidence Based on Clinician-Patient Communication Pathway \nModelâ€™, Health Commun., vol. 40, no. 9, pp. 1810â€“1822, Jul. 2025, doi: \n10.1080/10410236.2024.2423114. \n6. E. Jo et al., â€˜Assessing GPT-4â€™s Performance in Delivering Medical Advice: Comparative \nAnalysis With Human Expertsâ€™, JMIR Med. Educ., vol. 10, no. 1, p. e51282, Jul. 2024, \ndoi: 10.2196/51282. \n7. A. FÃ¸lstad et al., â€˜Future directions for chatbot research: an interdisciplinary research \nagendaâ€™, Computing, vol. 103, no. 12, pp. 2915â€“2942, Dec. 2021, doi: 10.1007/s00607-\n021-01016-7. \n8. D. O. Shumway and H. J. Hartman, â€˜Medical malpractice liability in large language model \nartificial intelligence: legal review and policy recommendationsâ€™, J. Osteopath. Med., vol. \n124, no. 7, pp. 287â€“290, Jul. 2024, doi: 10.1515/jom-2023-0229. \n9. C. Raï¬€el et al., â€˜Exploring the Limits of Transfer Learning with a Uniï¬ed Text-to-Text \nTransformerâ€™. \n10. N. Stiennon et al., â€˜Learning to summarize with human feedbackâ€™, in Advances in Neural \nInformation Processing Systems, Curran Associates, Inc., 2020, pp. 3008â€“3021. Accessed: \nApr. \n08, \n2025. \n[Online]. \nAvailable: \nhttps://proceedings.neurips.cc/paper_files/paper/2020/hash/1f89885d556929e98d3ef9b864\n48f951-Abstract.html \n11. Z. Guo, A. Lai, J. H. Thygesen, J. Farrington, T. Keen, and K. Li, â€˜Large Language Model \nfor Mental Health: A Systematic Reviewâ€™, Feb. 18, 2024. doi: 10.2196/preprints.57400. \n12. L. Ouyang et al., â€˜Training language models to follow instructions with human feedbackâ€™. \n13. M. Luo, C. J. Warren, L. Cheng, H. M. Abdul-Muhsin, and I. Banerjee, â€˜Assessing Empa-\nthy in Large Language Models with Real-World Physician-Patient Interactionsâ€™, in 2024 \nIEEE International Conference on Big Data (BigData), Dec. 2024, pp. 6510â€“6519. doi: \n10.1109/BigData62323.2024.10825307. \n14. H. Lang, F. Huang, and Y. Li, â€˜Fine-Tuning Language Models with Reward Learning on \nPolicyâ€™, Mar. 28, 2024, arXiv: arXiv:2403.19279. doi: 10.48550/arXiv.2403.19279. \n15. A. Sharma, I. W. Lin, A. S. Miner, D. C. Atkins, and T. Althoff, â€˜Towards Facilitating \nEmpathic Conversations in Online Mental Health Support: A Reinforcement Learning Ap-\nproachâ€™, in Proceedings of the Web Conference 2021, Ljubljana Slovenia: ACM, Apr. \n2021, pp. 194â€“205. doi: 10.1145/3442381.3450097. \n16. T. Saha, V. Gakhreja, A. S. Das, S. Chakraborty, and S. Saha, â€˜Towards Motivational and \nEmpathetic Response Generation in Online Mental Health Supportâ€™, in Proceedings of the \n45th International ACM SIGIR Conference on Research and Development in Information \nRetrieval, Madrid Spain: ACM, Jul. 2022, pp. 2650â€“2656. doi: 10.1145/3477495.3531912. \n17. R. L. Bashshur, G. W. Shannon, N. Bashshur, and P. M. Yellowlees, â€˜The Empirical Evi-\ndence for Telemedicine Interventions in Mental Disordersâ€™, Telemed. E-Health, vol. 22, \nno. 2, pp. 87â€“113, Feb. 2016, doi: 10.1089/tmj.2015.0206. \n"}, {"page": 22, "text": "Context-Emotion Aware Therapeutic Dialogue Generation   \n22 \n18. E. J. Kraus, B. Nicosia, and D. I. Shalowitz, â€˜A qualitative study of patientsâ€™ attitudes to-\nwards telemedicine for gynecologic cancer careâ€™, Gynecol. Oncol., vol. 165, no. 1, pp. \n155â€“159, Apr. 2022, doi: 10.1016/j.ygyno.2022.01.035. \n19. M. Torniainen-Holm et al., â€˜The effectiveness of email-based exercises in promoting psy-\nchological wellbeing and healthy lifestyle: a two-year follow-up studyâ€™, BMC Psychol., \nvol. 4, no. 1, p. 21, Dec. 2016, doi: 10.1186/s40359-016-0125-4. \n20. Z. Ma, Y. Mei, and Z. Su, â€˜Understanding the Benefits and Challenges of Using Large \nLanguage Model-based Conversational Agents for Mental Well-being Supportâ€™, AMIA. \nAnnu. Symp. Proc., vol. 2023, pp. 1105â€“1114, Jan. 2024. \n21. S. Gabriel, I. Puri, X. Xu, M. Malgaroli, and M. Ghassemi, â€˜Can AI Relate: Testing Large \nLanguage Model Response for Mental Health Supportâ€™, Oct. 07, 2024, arXiv: \narXiv:2405.12021. doi: 10.48550/arXiv.2405.12021. \n22. C. Gelso and K. Kanninen, â€˜Neutrality Revisited: On the Value of Being Neutral Within an \nEmpathic Atmosphereâ€™, J. Psychother. Integr., vol. 27, pp. 330â€“341, Feb. 2017, doi: \n10.1037/int0000072. \n23. H. W. Chung et al., â€˜Scaling Instruction-Finetuned Language Modelsâ€™. \n24. H. Jin, S. Chen, D. Dilixiati, Y. Jiang, M. Wu, and K. Q. Zhu, â€˜PsyEval: A Suite of Mental \nHealth Related Tasks for Evaluating Large Language Modelsâ€™, Jun. 03, 2024, arXiv: \narXiv:2311.09189. doi: 10.48550/arXiv.2311.09189. \n25. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, â€˜Language Models are \nUnsupervised Multitask Learnersâ€™. \n26. Y. Chu, L. Liao, Z. Zhou, C.-W. Ngo, and R. Hong, â€˜Towards Multimodal Emotional \nSupport Conversation Systemsâ€™, Oct. 19, 2024, arXiv: arXiv:2408.03650. doi: \n10.48550/arXiv.2408.03650. \n27. S. Liu et al., â€˜Towards Emotional Support Dialog Systemsâ€™, Jun. 02, 2021, arXiv: \narXiv:2106.01144. doi: 10.48550/arXiv.2106.01144. \n28. A. Anuchitanukul and J. Ive, â€˜SURF: Semantic-level Unsupervised Reward Function for \nMachine Translationâ€™, in Proceedings of the 2022 Conference of the North American \nChapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, Eds, Seattle, United States: As-\nsociation \nfor \nComputational \nLinguistics, \nJul. \n2022, \npp. \n4508â€“4522. \ndoi: \n10.18653/v1/2022.naacl-main.334. \n29. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€˜Proximal Policy Opti-\nmization \nAlgorithmsâ€™, \nAug. \n28, \n2017, \narXiv: \narXiv:1707.06347. \ndoi: \n10.48550/arXiv.1707.06347. \n30. J. Eisenstein et al., â€˜Helping or Herding? Reward Model Ensembles Mitigate but do not \nEliminate \nReward \nHackingâ€™, \nAug. \n16, \n2024, \narXiv: \narXiv:2312.09244. \ndoi: \n10.48550/arXiv.2312.09244. \n31. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, â€˜BLEU: a method for automatic evalua-\ntion of machine translationâ€™, in Proceedings of the 40th Annual Meeting on Association for \nComputational Linguistics  - ACL â€™02, Philadelphia, Pennsylvania: Association for Com-\nputational Linguistics, 2001, p. 311. doi: 10.3115/1073083.1073135. \n32. C.-Y. Lin, â€˜ROUGE: A Package for Automatic Evaluation of Summariesâ€™, in Text Sum-\nmarization Branches Out, Barcelona, Spain: Association for Computational Linguistics, \nJul. \n2004, \npp. \n74â€“81. \nAccessed: \nSep. \n13, \n2025. \n[Online]. \nAvailable: \nhttps://aclanthology.org/W04-1013/ \n33. S. Banerjee and A. Lavie, â€˜METEOR: An Automatic Metric for MT Evaluation with Im-\nproved Correlation with Human Judgmentsâ€™, in Proceedings of the ACL Workshop on In-\ntrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, \n"}, {"page": 23, "text": "23  \n \nE. H.Q. Zhang and J. Ive \nJ. Goldstein, A. Lavie, C.-Y. Lin, and C. Voss, Eds, Ann Arbor, Michigan: Association for \nComputational Linguistics, Jun. 2005, pp. 65â€“72. Accessed: Sep. 13, 2025. [Online]. \nAvailable: https://aclanthology.org/W05-0909/ \n34. A. Yuan, E. Garcia Colato, B. Pescosolido, H. Song, and S. Samtani, â€˜Improving Work-\nplace Well-being in Modern Organizations: A Review of Large Language Model-based \nMental Health Chatbotsâ€™, ACM Trans. Manag. Inf. Syst., vol. 16, no. 1, pp. 1â€“26, Mar. \n2025, doi: 10.1145/3701041. \n35. P. Rudra, W.-T. Balke, T. Kacprowski, F. Ursin, and S. Salloch, â€˜Large language models \nfor surgical informed consent: an ethical perspective on simulated empathyâ€™, J. Med. Eth-\nics, p. jme-2024-110652, Mar. 2025, doi: 10.1136/jme-2024-110652. \n36. Y. Bai et al., â€˜Training a Helpful and Harmless Assistant with Reinforcement Learning \nfrom \nHuman \nFeedbackâ€™, \nApr. \n12, \n2022, \narXiv: \narXiv:2204.05862. \ndoi: \n10.48550/arXiv.2204.05862. \n37. V. Sorin et al., â€˜Large Language Models and Empathy: Systematic Reviewâ€™, J. Med. Inter-\nnet Res., vol. 26, no. 1, p. e52597, Dec. 2024, doi: 10.2196/52597. \n38. F. Farhat, â€˜ChatGPT as a Complementary Mental Health Resource: A Boon or a Baneâ€™, \nAnn. Biomed. Eng., vol. 52, no. 5, pp. 1111â€“1114, May 2024, doi: 10.1007/s10439-023-\n03326-7. \n39. D. Grabb, â€˜The impact of prompt engineering in large language model performance: a psy-\nchiatric exampleâ€™, J. Med. Artif. Intell., vol. 6, no. 0, Oct. 2023, doi: 10.21037/jmai-23-71. \n40. H. R. Lawrence, R. A. Schneider, S. B. Rubin, M. J. MatariÄ‡, D. J. McDuff, and M. J. Bell, \nâ€˜The Opportunities and Risks of Large Language Models in Mental Healthâ€™, JMIR Ment. \nHealth, vol. 11, no. 1, p. e59479, Jul. 2024, doi: 10.2196/59479. \n41. Q. Yang, M. Ye, and B. Du, â€˜EmoLLM: Multimodal Emotional Understanding Meets \nLarge \nLanguage \nModelsâ€™, \nJun. \n29, \n2024, \narXiv: \narXiv:2406.16442. \ndoi: \n10.48550/arXiv.2406.16442. \n42. I. Song, S. R. Pendse, N. Kumar, and M. D. Choudhury, â€˜The Typing Cure: Experiences \nwith Large Language Model Chatbots for Mental Health Supportâ€™, Mar. 06, 2024, arXiv: \narXiv:2401.14362. doi: 10.48550/arXiv.2401.14362. \n43. N. Neveditsin, P. Lingras, and V. Mago, â€˜Clinical Insights: A Comprehensive Review of \nLanguage Models in Medicineâ€™, Jan. 07, 2025, arXiv: arXiv:2408.11735. doi: \n10.48550/arXiv.2408.11735. \n"}]}