{"doc_id": "arxiv:2601.03418", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03418.pdf", "meta": {"doc_id": "arxiv:2601.03418", "source": "arxiv", "arxiv_id": "2601.03418", "title": "PCoA: A New Benchmark for Medical Aspect-Based Summarization With Phrase-Level Context Attribution", "authors": ["Bohao Chu", "Sameh Frihat", "Tabea M. G. Pakull", "Hendrik Damm", "Meijie Li", "Ula Muhabbek", "Georg Lodde", "Norbert Fuhr"], "published": "2026-01-06T21:12:03Z", "updated": "2026-01-06T21:12:03Z", "summary": "Verifying system-generated summaries remains challenging, as effective verification requires precise attribution to the source context, which is especially crucial in high-stakes medical domains. To address this challenge, we introduce PCoA, an expert-annotated benchmark for medical aspect-based summarization with phrase-level context attribution. PCoA aligns each aspect-based summary with its supporting contextual sentences and contributory phrases within them. We further propose a fine-grained, decoupled evaluation framework that independently assesses the quality of generated summaries, citations, and contributory phrases. Through extensive experiments, we validate the quality and consistency of the PCoA dataset and benchmark several large language models on the proposed task. Experimental results demonstrate that PCoA provides a reliable benchmark for evaluating system-generated summaries with phrase-level context attribution. Furthermore, comparative experiments show that explicitly identifying relevant sentences and contributory phrases before summarization can improve overall quality. The data and code are available at https://github.com/chubohao/PCoA.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03418v1", "url_pdf": "https://arxiv.org/pdf/2601.03418.pdf", "meta_path": "data/raw/arxiv/meta/2601.03418.json", "sha256": "01bc8780849e990d098200df31c5791800e82bfa581f46f7c578c4c9ee321c5a", "status": "ok", "fetched_at": "2026-02-18T02:22:54.209014+00:00"}, "pages": [{"page": 1, "text": "PCOA: A New Benchmark for Medical Aspect-Based Summarization With\nPhrase-Level Context Attribution\nBohao Chu1,*\nSameh Frihat1\nTabea M. G. Pakull2\nHendrik Damm2\nMeijie Li4\nUla Muhabbek1\nGeorg Lodde3\nNorbert Fuhr1\n1University of Duisburg-Essen, 2University of Applied Sciences and Arts Dortmund,\n3University Hospital Essen, 4Institute for Artificial Intelligence in Medicine (IKIM)\n*bohao.chu@qq.com\nAbstract\nVerifying system-generated summaries remains\nchallenging, as effective verification requires\nprecise attribution to the source context, which\nis especially crucial in high-stakes medical\ndomains. To address this challenge, we in-\ntroduce PCOA, an expert-annotated bench-\nmark for medical aspect-based summarization\nwith Phrase-level Context Attribution. PCOA\naligns each aspect-based summary with its\nsupporting contextual sentences and contrib-\nutory phrases within them. We further propose\na fine-grained, decoupled evaluation frame-\nwork that independently assesses the quality\nof generated summaries, citations, and con-\ntributory phrases. Through extensive experi-\nments, we validate the quality and consistency\nof the PCOA dataset and benchmark several\nlarge language models on the proposed task.\nExperimental results demonstrate that PCOA\nprovides a reliable benchmark for evaluating\nsystem-generated summaries with phrase-level\ncontext attribution. Furthermore, comparative\nexperiments show that explicitly identifying\nrelevant sentences and contributory phrases be-\nfore summarization can improve overall qual-\nity. The data and code are available at https:\n//github.com/chubohao/PCoA.\n1\nIntroduction\nDocument summarization aims to condense long\ntexts into concise and coherent summaries that pre-\nserve key information (Narayan et al., 2018). Re-\ncent advances have achieved strong performance\nin generating overall summaries (Rush et al., 2015;\nCheng and Lapata, 2016). However, readers of\nthe same article often focus on different aspects\n(Zhang et al., 2023), making a single generic sum-\nmary insufficient. Instead, users frequently prefer\nsummaries tailored to specific aspects of interest.\nAspect-based summaries can better meet these di-\nverse needs and support cross-study comparisons\n(Yang et al., 2023; Takeshita et al., 2024), which\n        DOC:      Patients who have unresectable or metastatic melanoma with \nBRAF V600E or V600K mutation have prolonged progression-free survival \nand overall survival when receiving treatment with BRAF plus MEK inhibitors.  \n     However, long-term clinical outcomes ...      A total of 563 patients were \nrandomly assigned to receive dabrafenib plus trametinib (211 in the COMBI-\nd trial and 352 in the COMBI-v trial).\n1\n2\n7\nSummary:  A total of 563 patients who have unresectable or metastatic \nmelanoma were involved.\nCitations:          Phrases: A total of 563 patients\nunresectable or metastatic...\nAspect: Participants \n1\n7\nLLMs\nFigure 1: Illustrative example from the PCOA bench-\nmark. Given an article and a target aspect, the task is to\ngenerate an aspect-based summary with cited contextual\nsentences and aligned contributory phrases.\nare essential for evidence synthesis and clinical\ndecision-making in the medical domain.\nMost existing work on document summarization\nfocuses on using large language models (LLMs)\nto generate summaries for diverse applications\n(Takeshita et al., 2024). Although state-of-the-art\nLLMs show strong capabilities, they can suffer\nfrom factual inaccuracies (Mallen et al., 2023),\nsuch as hallucinations, which can be particularly\nrisky in clinical decision-making. Attributing gen-\nerated summaries to their source evidence allows\nusers to more easily locate relevant context and\nverify the summarized content, thereby helping to\nmitigate these concerns (Xie et al., 2024). However,\nexisting context attribution methods are typically\ncoarse-grained, often citing entire documents or\nparagraphs, which requires users to sift through\nlarge amounts of text to verify specific statements.\nIn this work, we investigate phrase-level context\nattribution for aspect-based summaries generated\nby LLMs, using a newly developed corpus anno-\ntated by medical experts. This fine-grained context\nattribution enhances the traceability of generated\nsummaries and enables users to efficiently verify\ntheir accuracy by explicitly linking summarized\ncontent to its original context through citations.\nOur main contributions are summarized as follows:\nContribution 1: This work introduces PCOA, a\nnew dataset constructed by annotating 152 random-\narXiv:2601.03418v1  [cs.CL]  6 Jan 2026\n"}, {"page": 2, "text": "ized controlled trial (RCT) articles across sixteen\nkey medical aspects commonly reported in clin-\nical studies. Totally, PCOA dataset consists of\n1, 799 aspect-based summaries, each paired with\nexplicitly cited contextual sentences and aligned\ncontributory phrases, as illustrated in Figure 1. Hu-\nman evaluation results indicate that the annotated\ndataset is of high quality with respect to the com-\npleteness and conciseness of all three parts. In\naddition, context attribution evaluation results sug-\ngest that each summary is appropriately grounded\nin its cited sentences and contributory phrases (§3).\nContribution 2: This work further proposes a fine-\ngrained evaluation framework tailored to this new\ntask, which systematically measures (i) claim-level\nrecall and precision for generated summaries, (ii)\nsentence-level recall and precision for cited con-\ntextual sentences, and (iii) phrase-level recall and\nprecision for contributory phrase alignment (§4).\nContribution 3: This work conducts a comprehen-\nsive evaluation of both open-source LLMs (e.g.,\nLLaMA; Grattafiori et al., 2024, Mistral; Mistral\nAI, 2024, DeepSeek; DeepSeek-AI et al., 2024)\nand proprietary models (e.g., GPT-4o; Hurst et al.,\n2024) on the PCOA dataset. The results show that\nPCOA serves as an effective benchmark for evaluat-\ning phrase-level context attribution in aspect-based\nsummarization. Furthermore, we compare three\ncontext attribution strategies and show that prior\nattribution, which identifies relevant sentences and\nphrases before summarization, consistently outper-\nforms both intrinsic and post-hoc attribution (§5).\n2\nRelated Work\n2.1\nMedical Aspect-Based Summarization\nArticles describing RCTs are typically organized\naround key elements, such as those defined in the\nPICO framework (Participants, Intervention, Com-\nparison, Outcome) (Richardson et al., 1995). Jin\nand Szolovits (2018) introduced a method using\nlong short-term memory (LSTM) networks to iden-\ntify PICO components in medical texts. More re-\ncently, Hu et al. (2023) proposed a section-aware\napproach to extract PICO components from RCT\nabstracts through a two-step natural language pro-\ncessing (NLP) pipeline. Additionally, Joseph et al.\n(2024) developed FACTPICO, a benchmark dataset\nof 345 plain-language summaries of RCT abstracts\ngenerated by LLMs and reviewed by experts for\nfactual accuracy. Building on prior work, we aim\nto advance fine-grained summarization by encom-\npassing sixteen common aspects of RCT articles.\n2.2\nContext Attribution\nIntrinsic Attribution: Intrinsic attribution refers\nto attribution mechanisms integrated into the gen-\neration process, where the model implicitly links\ngenerated summary content to supporting context\nduring generation.\nGao et al. (2023) proposed\na self-citation method in which the model high-\nlights supporting evidence directly from the input.\nMenick et al. (2022) explored fine-tuning LLMs\nfor generating citations during inference. While in-\ntrinsic attribution reduces reliance on external tools,\nit does not explicitly verify whether the cited evi-\ndence genuinely supports the generated statements,\nwhich may result in unreliable attributions.\nPost-Hoc Attribution: Post-hoc attribution as-\nsigns a model’s generated output to its input\nsources after generation. Unlike intrinsic attribu-\ntion, it relies on external alignment techniques ap-\nplied retrospectively, such as saliency maps (Li\net al., 2016), token-level similarity (Do˘gruöz et al.,\n2021), or entailment-based models (Honovich et al.,\n2022).\nAlthough these methods can explicitly\nidentify supporting evidence for generated content,\ntheir reliance on external evaluators makes them\nvulnerable when the generation itself is inaccurate.\nIn such cases, post-hoc citations may become mis-\nleading or uninformative, as they fail to direct users\nto the specific evidence underlying the claims.\nPrior Attribution: Prior attribution refers to incor-\nporating attribution mechanisms before generation\n(Fang et al., 2024; Slobodkin et al., 2024). By\nexplicitly identifying relevant contextual text in ad-\nvance, this approach reduces the risk of models\nrelying on irrelevant or noisy content (Chu et al.,\n2025). As a result, it preserves traceability and im-\nproves the factual accuracy of the generated output.\n3\nDataset Construction\n3.1\nSource Articles\nAs RCTs constitute one of the primary sources\nof evidence in evidence-based medicine (EBM)\n(Joseph et al., 2024), we restricted our dataset\nto RCT articles. A total of 607 abstracts1 from\nRCT articles retrieved from PubMed2 were initially\nscreened, of which 152 were ultimately retained.\n1We focus on abstracts because they are always publicly\naccessible and typically include the key medical aspects.\n2https://pubmed.ncbi.nlm.nih.gov(by Dec. 1, 2025)\n"}, {"page": 3, "text": "Figure 2: Human evaluation results assessing the completeness (top) and conciseness (bottom) of summaries, cited\nsentences, and contributory phrases across sixteen aspects, as measured using a 5-point Likert scale.\nArticles were selected based on the following cri-\nteria: (1) the study focuses on melanoma; (2) the\npublication date falls within the past ten years; (3)\nthe article is written in English; (4) the study is clas-\nsified as an RCT; and (5) the article is published in\na journal ranked Q1 or Q2 according to the Journal\nCitation Reports (JCR) (Clarivate Analytics, 2025).\n3.2\nMedical Aspects\nGrounded in interviews with healthcare profession-\nals and guided by the widely adopted PICO frame-\nwork (Richardson et al., 1995), we define A as a\nset of sixteen key medical aspects commonly re-\nported in clinical research (see Table 3 in §A.1).\nThese aspects capture the core elements typically\ndocumented in RCTs and support structured, com-\nprehensive summarization. To promote clarity and\nconsistency, a minimal reporting requirement is fur-\nther specified for each aspect, highlighted in green\nin Table 3, indicating the essential information to\nbe included whenever the aspect is applicable.\n3.3\nAnnotation Process\nTwo medical students from an in-house annotation\nlab, who were compensated at the local student\nassistant rate, were recruited to carry out the anno-\ntation. All 152 articles were jointly annotated by\nboth annotators. They followed a detailed annota-\ntion protocol comprising three sequential phases\n(I–III), followed by a final quality evaluation phase\n(§3.4). To improve efficiency and ensure annota-\ntion consistency, an online annotation system was\ndeveloped, as shown in Figure 7 in §A.2.\n(Phase I) Sentence Annotation: In the first phase,\nannotators were asked to assign one or more rele-\nvant aspects to each contextual sentence according\nto the definitions in Table 3. A sentence could be\nannotated with multiple aspects or left unlabeled.\n(Phase II) Summary Annotation: In the second\nphase, annotators were asked to write a summary\nfor each aspect based on the corresponding selected\nsentences. Each summary was required to describe\nthe target aspect and, when applicable, incorporate\nthe highlighted information specified in Table 3.\n(Phase III) Contributory Phrase Annotation: In\nthe third phase, annotators were asked to identify\ncontributory phrases corresponding to the given\naspect from the selected sentences. Each contribu-\ntory phrase was required to be incorporated into the\nsummary, either in its original form or in a variant.\n3.4\nDataset Quality Analysis\nHuman Evaluation: To assess the overall qual-\nity of the annotated dataset, a human evaluation\nof completeness and conciseness was conducted\non three components of instances sampled from 50\nrandomly selected articles (seed = 42). Two under-\ngraduate psychology students served as voluntary\nevaluators. Each instance was independently rated\nby both evaluators on a 5-point Likert scale, follow-\ning the detailed guidelines provided in Table 4 in\n§A.3. The final scores were obtained by averaging\nthe two ratings. As shown in Figure 2, all sixteen\nmedical aspects achieved average scores above 4.6\non both metrics, with conciseness rated slightly\nhigher overall. Notably, contributory phrases re-\nceived marginally lower scores than summaries and\ncitations, particularly in terms of completeness.\nInter-Annotator Agreement: To assess inter-\nannotator agreement (IAA), exact match rate,\nwithin-one rate, and mean absolute error3 are re-\nported, following prior work (Attali and Burstein,\n2006; Zhang and Zhou, 2007). The results indicate\na high level of agreement across all metrics, with\na within-one rate of 97.4%, an exact match rate of\n3Further details of these metrics can be found in §A.4.\n"}, {"page": 4, "text": "92.1%, and a mean absolute error of 0.109. These\nfindings demonstrate strong annotator consistency,\nwith only minor scoring variations, suggesting that\nthe annotated dataset is reliable and reproducible.\nContext Attribution Evaluation: To evaluate the\nextent to which cited sentences and contributory\nphrases support the summary, each summary is\nfirst decomposed into a set of subclaims using\nMistral-Large-2411 (Mistral AI, 2024). Subse-\nquently, TRUE (Honovich et al., 2022), a natural\nlanguage inference (NLI) model, is used to assess\nwhether each cited sentence entails at least one sub-\nclaim. As shown in subfigure (a) of Figure 9 in\n§A.3, the ratio of supportive sentences, which is\ndefined as the proportion of cited sentences that en-\ntail at least one subclaim, exceeds 88% for nearly\nall medical aspects, with the exception of aspect\nFU (Funding). In addition, we compute the sub-\nclaim attribution rate, defined as the proportion of\nsubclaims that are supported by at least one cited\nsentence. The subclaim rate is at least 81% for\nmost medical aspects, as illustrated in subfigure (b)\nof Figure 9. Finally, to assess phrase attribution,\nROUGE-1 (Recall-Oriented Understudy for Gist-\ning Evaluation; Lin, 2004) is applied to quantify\nlexical overlap between contributory phrases and\ntheir corresponding summary, yielding an average\nROUGE-1 score of at least 86% across all medical\naspects, as shown in subfigure (c) of Figure 9.\n3.5\nDataset Characteristics\nSource Articles: The Natural Language Toolkit\n(NLTK) (Bird et al., 2009) was used to analyze\nthe included articles. The articles have an average\nlength of 392.81 tokens, ranging from 138 to 814,\nand contain an average of 13.91 sentences, with\ncounts ranging from 4 to 24. The distributions of\narticle lengths and sentence counts are shown in\nsubfigures (a) and (b) of Figure 10 in §A.5.\nData Instances: In 1, 799 human-annotated data\ninstances, the average summary length is 24.63\ntokens, ranging from 2 to 48. Each summary cites\nan average of 1.44 sentences, with counts ranging\nfrom 1 to 6. The distributions of summary lengths\nand cited sentence counts are shown in subfigures\n(c) and (d) of Figure 10. The average length of\ncontributory phrases is 10.78 tokens, ranging from\n1 to 92, as shown in subfigure (e) of Figure 10.\nAspect Coverage in Articles: Among the 152 in-\ncluded articles, more than 150 report information\non the P (Participants; n = 152), I ( Intervention;\nn = 150), O (Outcomes; n = 150), and F (Find-\nings; n = 151) aspects. In contrast, fewer than\n70 articles address the SE (Secondary Endpoints;\nn = 57), B (Blinding; n = 61), and FU (Fund-\ning; n = 42) aspects. The distribution of aspect\ncoverage is shown in subfigure (f) of Figure 10.\nRelative Positions of Information: To examine\nthe structural placement of aspect-based informa-\ntion within articles, we analyzed the positional dis-\ntribution of cited sentences across rhetorical as-\npects. As shown in Figure 11 in §A.5, aspect OB\n(Objective) information predominantly appears at\nthe beginning of articles, whereas aspect F (Find-\nings) typically occurs toward the end of articles.\n4\nPCOA Benchmark\n4.1\nProblem Formalization\nGiven an article, PCOA requires summarization\nsystems to generate a summary for each applicable\naspect in A (§3.2), together with the cited con-\ntextual sentences and contributory phrases. For-\nmally, let the article d = [c1, c2, · · · , cn] be a\nsequence of uniquely indexed sentences, and let\na ∈A denote a target aspect. A summarization\nsystem M(C′, P′, sum′ | d, a) is expected to out-\nput an aspect-specific summary sum′, a set of cited\nsentences C′ = {c′\n1, c′\n2, · · · , c′\nk}, and a set of con-\ntributory phrases P′ = {p′\n1, p′\n2, · · · , p′\nm}, which\nare extracted from the cited sentences in C′ and\nrepresented as token sequences.\n4.2\nEvaluation Metrics\nTo ensure output quality, a summarization system\nis excepted to satisfy the following criteria: (1) the\nsummary should be complete and concise, covering\nall key information of the target aspect without re-\ndundancy or factual errors; (2) the cited sentences\nshould provide clear evidence supporting the sum-\nmary; and (3) the contributory phrases should align\nwith and reflect the core content of the summary.\nGuided by these criteria, separate evaluations are\nconducted for each of the three output components.\nSummary Evaluation: In accordance with crite-\nrion (1) and following the DOCLENS framework\n(Xie et al., 2024), claim recall (C-R) is adopted\nto measure key information coverage, while claim\nprecision (C-P) is used to assess conciseness and\nfactual correctness. As illustrated in the left panel\nof Figure 3, each summary is decomposed into a set\nof atomic subclaims using a claim decomposition\nmodel E (specified in §4.3), where each subclaim\n"}, {"page": 5, "text": "1\n5\nSummary\nSummaryREF\nClaim1\nClaim2\nClaim1REF\nClaim2REF\nClaim3REF\nClaim Recall: \n�\n�    Claim Precision: \n�\n�\nCitationsREF\n1\n5\n7\nCitation Recall: \n�\n�   Citation Precision: \n�\n�\n1\n5\n7\nClaim1\nClaim2\n2\n5\n7\n2\nPhrases\n1\n5\n2\n7\nSummary\nPhrase Recall:\n�\n�   Phrase Precision:\n�\n�\nPhrasesREF\nDecompose\nDecompose\nEntail\nEntail\nE1\nEn\n···\nSplit\nIn\nSplit\nE1REF\nEmREF\n···\nE1\nEk\n···\nE...REF\nE...\nEntailment\nNot Entailment\nInvalid Citation\nWithin\nConsistent Citation\nConsistent Phrase\nFigure 3: Overview of the automatic evaluation framework. Summaries are evaluated using Claim Recall and Claim\nPrecision (left), citations are assessed with Citation Recall and Citation Precision (middle), and contributory phrases\nare evaluated using Phrase Recall and Phrase Precision (right).\ncorresponds to a single factual statement. Let sum\ndenote the reference summary, sum′ the generated\nsummary, Lsum the set of subclaims extracted from\nsum, and L′\nsum the set of subclaims extracted from\nsum′. A entailment evaluation model ϕ (specified\nin §4.3) is employed to determine whether each\nsubclaim l ∈Lsum is entailed by sum′, and whether\neach subclaim l′ ∈L′\nsum is entailed by sum. Based\non these judgments, claim recall and precision are\ncomputed as follows:\nC-R =\n1\n|Lsum|\nX\nl∈Lsum\nI[sum′ |= l],\nC-P =\n1\n|L′sum|\nX\nl′∈L′sum\nI[sum |= l′],\nwhere |= denotes ENTAILMENT, and I[·] is an indi-\ncator function that returns 1 if the entailment holds\nand 0 otherwise.\nCitation Evaluation: Based on criterion (2), we\nintroduce sentence recall (S-R) and sentence preci-\nsion (S-P) to evaluate the quality of cited sentences.\nUnlike prior work (Xie et al., 2024; Gao et al.,\n2023), which deems a citation valid if the cited sen-\ntences collectively support the summary, we evalu-\nate whether each cited sentence independently sup-\nports the generated summary. Specifically, a cited\nsentence is considered valid if it entails at least one\nsubclaim of the generated summary and is included\nin the reference citation set. Formally, sentence\nrecall and precision are defined as follows:\nS-R =\n1\n|C|\nX\nc∈C\nI[c ∈C′ ∧(∃l′ ∈L′\nsum, c |= l′)]\nS-P =\n1\n|C′|\nX\nc′∈C′\nI[c′ ∈C ∧(∃l′ ∈L′\nsum, c′ |= l′)]\nwhere C denotes the set of cited sentences in the\nreference data instance, and C′ denotes the set of\ncited sentences in the system output, respectively.\nPhrase Evaluation: Based on criterion (3), a valid\ncontributory phrase must be extracted from a cited\nsentence, rather than hallucinated or drawn from\nAlgorithm 1: Computation of Evaluation Metrics\nRequire: decomposition model: E, NLI model: ϕ, tokenizer T\nInput: prediction (sum′, C′, P′), reference (sum, C, P)\nOutput: C-R, C-P, S-R, S-P, P-R, P-P\n1:\n{l1, l2, ..., ln} ←E(sum); n ←0;\n2:\nforeach li ∈{l1, l2, ..., ln}\n3:\nif ϕ(sum′, li) == 1 then n++;\n4:\nC-R ←n/|{l1, l2, ..., ln}|\n5:\n{l′\n1, l′\n2, ..., l′\nm} ←E(sum′); n ←0;\n6:\nforeach l′\ni ∈{l′\n1, l′\n2, ..., l′\nm}\n7:\nif ϕ(sum, l′\ni) == 1 then n++;\n8:\nC-P ←n/|{l′\n1, l′\n2, ..., l′\nm}|;\n9:\nn ←0;\n10: foreach c′\ni ∈C′\n11:\nforeach l′\ni ∈{l′\n1, l′\n2, ..., l′\nn}\n12:\nif ϕ(c′\ni, l′\ni) == 1 && c′\ni ∈C then n++; break;\n13: S-R ←n/|C|; S-P ←n/|C′|;\n14: {p′\n1, p′\n2, ..., p′\nk} ←T (P′); n ←0;\n15: foreach p′\ni ∈{p′\n1, p′\n2, ..., p′\nk}\n16:\nif p′\ni ∈T (P) && p′\ni ∈sum′ && p′\ni ∈S C′ then n++;\n17: P-R ←n/|T (P)|; P-P ←n/|T (P′)|;\nan unrelated source, and if it appears in the gener-\nated summary either verbatim or in a semantically\nequivalent variant. Formally, phrase recall (P-R)\nand precision (P-P) are defined as follows:\nP-R =\n1\n|P|\nX\np∈P\nI[p ∈P′ ∧p ∈S C′ ∧p ∈sum′],\nP-P =\n1\n|P′|\nX\np′∈P′\nI[p′ ∈P ∧p′ ∈S C′ ∧p′ ∈sum′],\nwhere P denotes the set of contributory phrases (in\ntokens) in the reference and P′ denotes the set of\ncontributory phrases in the system output.\n4.3\nEvaluation Algorithm\nAcross all experiments, Mistral-Large-2411\n(Mistral AI, 2024) is adopted as the decomposi-\ntion model E, which decomposes both system-\ngenerated and reference summaries into sets of\natomic subclaims. To tokenize contributory phrases\ninto individual words, NLTK (Bird et al., 2009) is\nused as the tokenizer T . For entailment evalua-\ntion, TRUE (Honovich et al., 2022) is employed\n"}, {"page": 6, "text": "Summary Evaluation\nCitation Evaluation\nPhrase Evaluation\nLLM\nC-R\nC-P\nC-F1\nS-R\nS-P\nS-F1\nP-R\nP-P\nP-F1\nLLaMA3.1-70B\n0.694\n0.536\n0.605\n0.775\n0.559\n0.650\n0.669\n0.450\n0.538\nDeepSeek-V3\n0.757\n0.583\n0.659\n0.820\n0.569\n0.672\n0.637\n0.466\n0.539\nMistral-Large\n0.634\n0.668\n0.651\n0.766\n0.572\n0.655\n0.639\n0.522\n0.574\nGPT-4o\n0.695\n0.561\n0.621\n0.805\n0.577\n0.672\n0.621\n0.476\n0.539\nTable 1: Benchmark results of LLMs evaluated on three parts. Bold values indicate the best performance in each\nmetric, underlined values indicate the second-best. *–F1 refers to the corresponding F1-score.\nas the entailment evaluator ϕ. Formally, ϕ(p, h)\ndenotes the output of the evaluator, returning 1 if\nthe premise p entails the hypothesis h, and 0 other-\nwise. The detailed computation procedure for all\nevaluation metrics is presented in Algorithm 1.\n5\nExperiments and Analysis\nIn this section, we conduct experiments to investi-\ngate the following research questions: RQ1: How\ndo open-source and closed-source LLMs compare\nin their performance on aspect-based summariza-\ntion with phrase-level context attribution? RQ2:\nAmong the three context attribution strategies in-\ntroduced in §2.2, which approach is most effective\nin addressing the challenges of aspect-based sum-\nmarization with phrase-level context attribution?\n5.1\nBenchmarking LLMs on PCOA\nTo address RQ1, we evaluate a range of open-\nsource and closed-source instruction-following\nmodels on the PCOA dataset using the evalua-\ntion metrics introduced in §4.2. Specifically, we\nselect LLaMA3.1-70B-Instruct (Meta AI, 2024),\nMistral-Large-2411 (Mistral AI, 2024), and\nDeepSeek-V3-0324 (DeepSeek Inc., 2025) as rep-\nresentative open-source models, and GPT-4o (Ope-\nnAI, 2024) as representative closed-source model.\nExperimental Setting: In this experiment, the in-\ntrinsic context attribution strategy described in §2.2\nis employed. Given an RCT article and a specified\nclinical aspect, the model is prompted to generate\nan aspect-based summary together with the cor-\nresponding supporting sentences and contributory\nphrases extracted from these selected sentences.\nAll models are evaluated in a zero-shot setting, us-\ning a shared prompt template provided in §C.1.\nThe models use a default temperature setting of 0.7.\nDue to the limited computational capacity of our\nlocal machines, we rely on commercial APIs456 to\n4https://deepinfra.com/ (by Dec. 1, 2025)\n5https://docs.mistral.ai/api/ (by Dec. 1, 2025)\n6https://platform.openai.com/ (by Dec. 1, 2025)\nevaluate large language models, with all prompts\nsubmitted via POST requests. The total computa-\ntional cost across all models and evaluation proce-\ndures is approximately $23.6. All experiments are\nconducted on a single NVIDIA A6000 GPU.\nMain Results: Table 1 presents the benchmark\nresults across the three generation components.\nThe following is observed: (1) DeepSeek-V3 and\nGPT-4o achieved the joint best performance in ci-\ntation evaluation. In addition, DeepSeek-V3 at-\ntained the best results in summary evaluation, while\nMistral-Large achieved the highest performance\nin phrase evaluation. (2) Most LLMs exhibit higher\nscores on recall-oriented metrics than on precision-\noriented metrics, which may indicate a tendency to\ngenerate a larger amount of information, including\npotentially redundant or inaccurate content. (3) The\nresults of citation and phrase evaluations suggest\nthat accurately identifying supportive contextual\nsentences and contributory phrases remains a chal-\nlenging problem for current LLMs in the intrinsic\ncontext attribution setting.\nResponse Error Analysis: Compliance with the\nprescribed prompt template format was examined\nfor the participating LLMs. The results indicate\nthat DeepSeek-V3 and GPT-4o consistently pro-\nduced responses that were fully (100%) compliant\nwith the template. In contrast, LLaMA3.1 generated\n3 out of 1, 799 responses that deviated from the ex-\npected format, which were subsequently corrected\nmanually. For Mistral-Large, only 1, 609 out of\n1, 799 responses conformed to the template format.\nIrregular responses were manually reviewed, and 9\ninvalid responses were excluded.\nImpact of Context Complexity: Model perfor-\nmance is examined with respect to context com-\nplexity, as reflected by the length of the three ref-\nerence components. This analysis reports the re-\nsults of DeepSeek-V3, which achieves the highest\noverall performance. As shown in subfigure (a) of\nFigure 4, claim recall decreases as the number of\nsubclaims increases. In contrast, claim precision\n"}, {"page": 7, "text": "(a)\n(b)\n(c)\nFigure 4: Effects of the number of reference subclaims, citations, and contributory phrases on model performance.\nThe x-axis represents the count, while the y-axis indicates the metric scores.\n(a)\n(b)\n(c)\nFigure 5: Context attribution evaluation across sixteen medical aspects. The x-axis represents the sixteen individual\nmedical aspects, while the y-axis indicates the evaluation scores.\npeaks at approximately n ≈11 subclaims and then\ngradually declines as the reference length contin-\nues to increase. For citation evaluation, both recall\nand precision decrease with an increasing number\nof cited sentences, as illustrated in subfigure (b) of\nFigure 4. This trend may be partly influenced by\nthe upper limit of six cited sentences. A similar pat-\ntern is observed in contributory phrase evaluation,\nwhich largely mirrors the trends in claim evalua-\ntion, as shown in subfigure (c) of Figure 4. Overall,\nthese results indicate that greater context complex-\nity is associated with lower model performance.\nAspect-Wise Performance Analysis: To evaluate\nmodel performance across the sixteen medical as-\npects, the evaluation results of DeepSeek-V3 were\ngrouped by aspect, and six standard evaluation met-\nrics were computed for each group, as presented\nin Table 2. The results reveal substantial varia-\ntion in model performance across different medical\naspects. Specifically, aspects O (Outcomes) and\nI (Intervention) yielded lower scores across most\nmetrics, likely due to the higher density of rele-\nvant information within their associated articles,\nwhich increases the difficulty of accurate extraction.\nIn contrast, aspects B (Blinding), FU (Funding),\nand RE (Registration) demonstrated comparatively\nhigher performance, potentially because the rele-\nvant information is typically concise and unambigu-\nous, thereby facilitating more accurate predictions.\nContext Attribution Analysis: Following the con-\ntext attribution evaluation method described in §3.4,\nthe extent to which cited sentences and contributory\nphrases support the generated summaries produced\nby DeepSeek-V3 is assessed. Overall, the rate of\nsupportive citations in the prediction is lower than\nthat in the reference. In particular, the rate of sup-\nportive citations for aspects O (Outcomes), F (Find-\nings), and FU (Funding) is all ≤65%, as shown in\nsubfigure (a) of Figure 5. This observation is con-\nsistent with the findings reported in §Aspect-Wise\nPerformance Analysis. Similarly, the attribution\nrate of predicted subclaims is lower than that of the\nreference summaries, with aspects C (Comparator),\nF (Findings), and FU (Funding) exhibiting attribu-\ntion rates of ≤70%, as illustrated in subfigure (b)\nof Figure 5. Furthermore, the ROUGE-1 scores\nbetween the cited phrases and the corresponding\nsummaries are also lower in the predicted data,\nparticularly for aspects O (Outcomes) and F (Find-\nings), where the scores fall below 63%, as shown\nin subfigure (c) of Figure 5.\n5.2\nPerformance Comparison of Three\nContext Attribution Strategies\nTo address RQ2, DeepSeek-V3 is evaluated on the\nPCOA dataset using the three context attribution\nstrategies described in §2.2, and their performance\nis compared. As the intrinsic context attribution\n"}, {"page": 8, "text": "Claim Evaluation\nSentence Evaluation\nPhrase Evaluation\nAspect\nC-R\nC-P\nC-F1\nS-R\nS-P\nS-F1\nP-R\nP-P\nP-F1\nOB\n0.761\n0.690\n0.724\n0.703\n0.700\n0.702\n:::::\n0.505\n0.551\n0.527\nP\n0.755\n0.614\n0.677\n0.835\n0.739\n0.784\n0.530\n0.431\n0.475\nI\n:::::\n0.616\n0.677\n0.645\n0.937\n0.569\n0.708\n0.579\n0.597\n0.588\nC\n0.763\n:::::\n0.265\n:::::\n0.393\n:::::\n0.698\n:::::\n0.297\n:::::\n0.417\n0.688\n0.417\n0.519\nO\n:::::\n0.606\n0.497\n:::::\n0.546\n0.743\n0.327\n0.454\n:::::\n0.329\n0.342\n:::::\n0.335\nF\n:::::\n0.520\n:::::\n0.302\n:::::\n0.382\n:::::\n0.415\n:::::\n0.136\n:::::\n0.205\n:::::\n0.116\n:::::\n0.077\n:::::\n0.092\nM\n0.832\n0.541\n0.655\n0.904\n:::::\n0.282\n:::::\n0.430\n0.857\n0.582\n0.694\nTD\n0.671\n0.726\n0.697\n0.956\n0.839\n0.894\n0.648\n0.477\n0.549\nPE\n0.873\n0.537\n0.665\n0.846\n0.546\n0.664\n0.921\n0.601\n0.728\nSE\n0.876\n:::::\n0.453\n0.597\n0.860\n0.378\n0.525\n0.773\n0.583\n0.665\nFD\n0.851\n0.711\n0.775\n0.965\n0.752\n0.845\n0.723\n:::::\n0.303\n:::::\n0.427\nAE\n0.750\n0.774\n0.762\n0.900\n0.773\n0.832\n0.575\n0.508\n0.539\nR\n0.825\n0.663\n0.735\n0.972\n0.788\n0.871\n0.874\n:::::\n0.290\n0.436\nB\n0.934\n0.556\n0.697\n1.000\n0.910\n0.953\n0.938\n0.762\n0.841\nFU\n0.992\n0.897\n0.942\n:::::\n0.595\n0.595\n0.595\n0.988\n0.921\n0.954\nRE\n0.981\n0.765\n0.860\n0.977\n0.971\n0.974\n0.923\n0.610\n0.735\nTable 2: Evaluation results of DeepSeek-v3 across sixteen medical aspects. Bold values in each column indicate\nthe top three scores, while:::::::::\nunderlined::::\nwith:a:::::\nwavy::::\nline values highlight the bottom three.\nFigure 6: Evaluation results of three context attribution strategies across claim, citation, and phrase levels.\nstrategy has already been examined in the first ex-\nperiment, this experiment focuses on evaluating the\nprior and post-hoc context attribution strategies.\nExperimental Setting: For prior context attri-\nbution, the LLM is first prompted to retrieve\naspect-related sentences and to extract contributory\nphrases from them. It is then instructed to generate\nan aspect-based summary using only the retrieved\nsentences and extracted phrases as input. The cor-\nresponding prompt template is provided in §C.3.\nIn contrast, post-hoc context attribution begins by\nprompting the LLM to generate an aspect-based\nsummary, followed by an additional prompt that\nretrieves the related sentence(s) and extracts the\nmost contributory phrases. The prompt template\nfor this strategy is presented in §C.4.\nMain Results: Figure 6 presents a comparison\nof the three context attribution strategies. Com-\npared with the intrinsic (S-F1 = 0.67, P-F1 = 0.54)\nand post-hoc strategies (S-F1 = 0.58, P-F1 = 0.48),\nthe prior context attribution strategy (S-F1 = 0.70,\nP-F1 = 0.61) achieves superior performance on\ncitation- and phrase-related metrics, while main-\ntaining comparable performance on claim-related\nmetrics (C-F1 = 0.63, 0.66, and 0.62, respectively).\nThese results suggest that explicitly identifying rel-\nevant sentences and contributory phrases before\nsummarization can enhance the overall quality. In\ncontrast, the post-hoc context attribution strategy\nconsistently underperforms on most metrics. To\nfurther examine the differences among the three\ncontext attribution strategies, a case study is con-\nducted, as shown in Table 5 in §B.1,\n6\nConclusion\nWith the aim of enhancing the verifiability and trust-\nworthiness of system-generated summaries, we in-\ntroduce a new benchmark for medical aspect-based\nsummarization with phrase-level context attribu-\ntion. We further propose a fine-grained evaluation\nframework to assess the quality of generated sum-\nmaries, citations, and contributory phrases. Exper-\nimental results show that PCOA serves as a reli-\nable benchmark for evaluating medial aspect-based\nsummarization with phrase-level context attribu-\ntion. Moreover, comparative experiments show\nthat explicitly identifying relevant sentences and\ncontributory phrases in advance can substantially\nimprove the overall quality of summarization.\n"}, {"page": 9, "text": "Limitations: Our research marks a substantial step\ntoward medical aspect-based summarization with\nphrase-level context attribution. Nonetheless, it has\ncertain limitations: (1) Due to the limited computa-\ntional capacity of our local machines, we rely on\ncommercial APIs to evaluate large language mod-\nels, which incurs a cost of approximately $23.6\nfor assessing all data instances in PCOA. To mit-\nigate this concern and promote transparency, we\nrelease the full set of evaluation data for repro-\nducibility. (2) In our evaluation framework, we em-\nployed Mistral-Large-2411 to decompose sum-\nmaries into individual subclaims. Although this\napproach has been adopted in several prior studies,\nits reliability remains a concern, as the quality of\nthe decomposition directly impacts the final eval-\nuation results. To mitigate this issue, we imple-\nmented two strategies: a) we applied an identical\nprompting strategy across all evaluations to ensure\nconsistency and fairness; b) we performed post-\nprocessing using regular expressions to identify\nanomalous outputs. In each batch of 1, 799 de-\ncomposition results, approximately 10 such cases\nwere found and subsequently corrected manually.\n(3) Currently, our phrase-level evaluation metrics\nare based on surface matching and do not consider\nword order or semantic meaning, similar to the\nROUGE-1 approach. This may cause variant ex-\npressions with the same meaning in the summary\nto be inaccurately evaluated.\nAcknowledgments\nThis project is supported by the project WisPerMed\n(AI for Personalized Medicine), funded by the Ger-\nman Science Foundation (DFG) as RTG 2535.\nReferences\nYigal Attali and Jill Burstein. 2006. Automated essay\nscoring with e-rater® v.2. The Journal of Technology,\nLearning and Assessment, 4(3):1–29.\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\nural Language Processing with Python: Analyzing\nText with the Natural Language Toolkit. O’Reilly\nMedia. https://www.nltk.org/book/ (Accessed:\n2025-12-01).\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 484–494, Berlin, Germany. As-\nsociation for Computational Linguistics.\nBohao Chu, Meijie Li, Sameh Frihat, Chengyu Gu,\nGeorg Lodde, Elisabeth Livingstone, and Norbert\nFuhr. 2025. TracSum: A new benchmark for aspect-\nbased summarization with sentence-level traceability\nin medical domain. In Proceedings of the 2025 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 844–864, Suzhou, China. Associa-\ntion for Computational Linguistics.\nClarivate Analytics. 2025. Journal Citation Reports.\nAccessed: 2025-12-01.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx-\nuan Wang, Bochao Wu, Chengda Lu, Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, et al. 2024.\nDeepseek-v3 technical report.\nDeepSeek Inc. 2025. DeepSeek-V3-0324: Model re-\nlease. Accessed: 2025-12-01.\nA. Seza Do˘gruöz, Sunayana Sitaram, Barbara E. Bul-\nlock, and Almeida Jacqueline Toribio. 2021. A sur-\nvey of code-switching: Linguistic and social per-\nspectives for language technologies. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1654–1666, Online.\nAssociation for Computational Linguistics.\nJinyuan Fang, Zaiqiao Meng, and Craig MacDon-\nald. 2024.\nTRACE the evidence: Constructing\nknowledge-grounded reasoning chains for retrieval-\naugmented generation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2024, pages\n8472–8494, Miami, Florida, USA. Association for\nComputational Linguistics.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6465–6488, Singapore. Associa-\ntion for Computational Linguistics.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, et al. 2024. The llama 3 herd of mod-\nels. arXiv preprint arXiv:2407.21783.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3905–3920, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nYan Hu, Vipina K. Keloth, Kalpana Raja, Yong Chen,\nand Hua Xu. 2023. Towards precise pico extraction\nfrom abstracts of randomized controlled trials using\na section-specific learning approach. Bioinformatics,\n39(9):btad542.\n"}, {"page": 10, "text": "Aaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford,\net al. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nDi Jin and Peter Szolovits. 2018. PICO element de-\ntection in medical text via long short-term memory\nneural networks. In Proceedings of the BioNLP 2018\nworkshop, pages 67–75, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nSebastian Joseph, Lily Chen, Jan Trienes, Hannah\nGöke, Monika Coers, Wei Xu, Byron Wallace, and\nJunyi Jessy Li. 2024. FactPICO: Factuality evalua-\ntion for plain language summarization of medical evi-\ndence. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 8437–8464, Bangkok,\nThailand. Association for Computational Linguistics.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802–9822, Toronto,\nCanada. Association for Computational Linguistics.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nat McAleese.\n2022.\nTeaching language models to support\nanswers with verified quotes.\narXiv preprint\narXiv:2203.11147.\nMeta AI. 2024. Introducing llama 3.1: Our most capa-\nble models to date. Accessed: 2025-12-01.\nMistral AI. 2024. Mistral: Introducing the large lan-\nguage model 2407. Accessed: 2025-12-01.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nOpenAI. 2024. Hello gpt-4o. Accessed: 2025-12-01.\nW. Scott Richardson, Mark C. Wilson, Jim Nishikawa,\nand Robert S. Hayward. 1995. The well-built clinical\nquestion: a key to evidence-based decisions. ACP\nJournal Club, 123(3):A12–A13.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nAviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster,\nand Ido Dagan. 2024. Attribute first, then generate:\nLocally-attributable grounded text generation. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3309–3344, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nSotaro Takeshita, Tommaso Green, Ines Reinig, Kai\nEckert, and Simone Ponzetto. 2024. ACLSum: A\nnew dataset for aspect-based summarization of scien-\ntific publications. In Proceedings of the 2024 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n6660–6675, Mexico City, Mexico. Association for\nComputational Linguistics.\nYiqing Xie, Sheng Zhang, Hao Cheng, Pengfei Liu, Ze-\nlalem Gero, Cliff Wong, Tristan Naumann, Hoifung\nPoon, and Carolyn Rose. 2024. DocLens: Multi-\naspect fine-grained evaluation for medical text gen-\neration. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 649–679, Bangkok,\nThailand. Association for Computational Linguistics.\nXianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang\nWang, Xiaoman Pan, Linda Petzold, and Dong Yu.\n2023. OASum: Large-scale open domain aspect-\nbased summarization. In Findings of the Association\nfor Computational Linguistics: ACL 2023, pages\n4381–4401, Toronto, Canada. Association for Com-\nputational Linguistics.\nMin-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A\nlazy learning approach to multi-label learning. Pat-\ntern Recognition, 40(7):2038–2048.\nYusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong\nChen, Dragomir Radev, Chenguang Zhu, Michael\nZeng, and Rui Zhang. 2023. MACSum: Control-\nlable summarization with mixed attributes. Transac-\ntions of the Association for Computational Linguis-\ntics, 11:787–803.\n"}, {"page": 11, "text": "Appendix\nA\nDataset Construction Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nA.1\nMedical Aspects Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nA.2\nAnnotation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12\nA.3\nDataset Quality Analysis Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nA.4\nInter-Annotator Agreement Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nA.5\nDataset Characteristics Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nB\nExperiments Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\nB.1\nComparative Case Study of Three Context Attribution Strategies . . . . . . . . . . . . . . . . . . . . . . 15\nC\nInstructions And Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16\nC.1\nPrompt For Intrinsic Context Attribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nC.2\nPrompt For Subclaim Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nC.3\nPrompt For Prior Context Attribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nC.4\nPrompt For Post-Hoc Context Attribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n"}, {"page": 12, "text": "Aspect\nExample of Summary\nOBjective (OB)\nTo determine the 5-year survival rates [items] of patients with unresectable or metastatic melanoma\n[diseases] who derive long-term benefit from combination therapy with BRAF and MEK inhibitors\n[treatment].\nParticipants (P)\nA total of 563 [number] previously untreated patients with unresectable or metastatic melanoma and a\nBRAF V600E or V600K mutation [diseases] participated in the study.\nIntervention (I)\nPatients received first-line treatment with the BRAF inhibitor dabrafenib (150 mg twice daily) plus the\nMEK inhibitor trametinib (2 mg once daily) [administration].\nComparator (C)\nThe comparator was ipilimumab monotherapy [name], administered at 3 mg/kg every three weeks for four\ndoses [administration].\nOutcomes (O)\nApproximately one-third of patients experienced long-term benefits, with a 5-year OS rate [endpoint] of\n34% [value].\nFindings (F)\nNivolumab plus ipilimumab or nivolumab alone demonstrated durable, improved clinical outcomes\n[finding] over ipilimumab monotherapy in advanced melanoma.\nMedicines (M)\nUsed medicines were dabrafenib and trametinib [medicine].\nTreatment Duration (TD)\nWeekly administration of ontuxizumab without dose change until disease progression [condition].\nPrimary Endpoints (PE)\nPrimary endpoint was progression-free survival [endpoint].\nSecondary Endpoints (SE)\nSecondary endpoint was long-term survival rates [endpoint].\nFollow-Up Duration (FD)\nThe median follow-up duration was 22 months [duration].\nAdverse Events (AE)\nThe most common adverse events overall were headache [adverse event] (55.3%) [value].\nRandomization (R)\nPatients were randomized in a 2:1 [ratio] to receive either binimetinib or dacarbazine [group].\nBlinding (B)\nThe study was double-blind [blinding].\nFUnding (FU)\nThe study was funded by Bristol-Myers Squibb [sponsor].\nREgistration (RE)\nThe ClinicalTrials.gov number is NCT03698019 [number].\nTable 3: Sixteen commonly reported medical aspects in RCT articles and corresponding summary examples.\nA\nDataset Construction Details\nA.1\nMedical Aspects Definition\nGrounded in interviews with healthcare profes-\nsionals and guided by the widely adopted PICO\nframework (Richardson et al., 1995), we predefine\nsixteen key medical aspects commonly reported\nin clinical research (see Table 3). These aspects\ncapture the core elements typically documented in\nRCTs and support structured, comprehensive sum-\nmarization. To promote clarity and consistency, a\nminimal reporting requirement is further specified\nfor each aspect, highlighted in green in Table 3,\nindicating the essential information to be included\nwhenever the aspect is applicable.\nA.2\nAnnotation Details\nAnnotators: We recruited two medical students\nfrom the in-house annotation lab. They were under-\ngraduate or master’s students at the medical school\nand were compensated according to the local stu-\ndent assistant pay rate. No information was col-\nlected beyond their annotation outputs, and partic-\nipants were informed in advance that the results\nwould be used solely for research purposes.\nAnnotation Protocol: The annotation protocol\nwas designed as a structured, multi-phase workflow\nto ensure consistency and reproducibility. Annota-\ntors were trained with detailed guidelines defining\neach aspect, annotation rules, and representative\nexamples. The protocol first required annotators\nFigure 7: Overview of the manual annotation process across Phases I–III. The left side illustrates citation annotation,\nwhile the right side presents summary and contributory phrase annotation, exemplified for the OB aspect.\n"}, {"page": 13, "text": "Figure 8: Human evaluation interface. The left shows the full articles with highlighted citations (red) and contributory\nphrases (blue), while the right displays the data to be evaluated using six metrics on a 5-point Likert scale.\nto assign one or more relevant aspects to each sen-\ntence according to Table 3, with sentences allowed\nto receive multiple labels or remain unlabeled. An-\nnotators then wrote a summary for each aspect\nbased on the selected sentences, ensuring that the\nsummary described the target aspect and, when ap-\nplicable, incorporated the highlighted information\nspecified in Table 3. Finally, annotators identified\ncontributory phrases for each aspect from the previ-\nously selected sentences, with each phrase required\nto appear in the summary in its original or a mod-\nified form. As shown in Figure 7, all annotations\nwere conducted using a custom online system that\nenforced the protocol and minimized deviations.\nA.3\nDataset Quality Analysis Details\nHuman Evaluation: We conducted a human eval-\nuation of completeness and conciseness on data\ninstances (summaries, cited sentences, and contrib-\nutory phrases) sampled from 50 randomly selected\narticles (seed = 42). Two undergraduate psychol-\nogy students served as evaluators 7. Each instance\nwas independently rated by both evaluators on a\n5-point Likert scale, following the detailed guide-\nlines provided in Table 4. The final scores were\nobtained by averaging the two ratings. As shown\nin Figure 8, all annotations were conducted using\na custom online system that enforced the protocol\nand minimized deviations.\nContext Attribution Evaluation Results: To eval-\nuate the extent to which cited sentences and con-\ntributory phrases support the summary, we first em-\nploy Mistral-Large-2411 (Mistral AI, 2024) to\n7No information was collected beyond their annotation\noutputs, and participants were informed in advance that the\nresults would be used solely for research purposes.\nAspect\nScore\nScore Description\nCompleteness\n★★★★★\nAll key information has been accurately identified.\n★★★★✩\nMost key information has been accurately identified.\n★★★✩✩\nSome key information has been accurately identified.\n★★✩✩✩\nMost key information is missing from the output.\n★✩✩✩✩\nNo key information has been identified.\nConciseness\n★★★★★\nThe entire output is relevant to the given aspect.\n★★★★✩\nMost of the output pertains to the given aspect.\n★★★✩✩\nSome of the output is relevant to the given aspect.\n★★✩✩✩\nMost of the output is irrelevant to the given aspect.\n★✩✩✩✩\nThe output is entirely irrelevant to the aspect.\nTable 4: Evaluation criteria and scoring guidelines\nacross the dimensions of completeness and conciseness.\ndecompose each summary into a set of subclaims.\nWe then apply TRUE (Honovich et al., 2022), a\nnatural language inference (NLI) model, to assess\nwhether each cited sentence entails at least one sub-\nclaim. As shown in subfigure (a) of Figure 9, the\ncitation entailment ratio, which is defined as the\nproportion of cited sentences that entail at least one\nsubclaim, exceeds 88% for nearly all aspects, with\nthe exception of aspect FU (Funding). In addition,\nwe compute the subclaim attribution rate, defined\nas the proportion of subclaims that are supported\nby at least one cited sentence. The subclaim rate is\nat least 81% for most aspects, as illustrated in sub-\nfigure (b) of Figure 9. Finally, to assess phrase attri-\nbution, we apply ROUGE-1 (Lin, 2004) to quantify\nlexical overlap between contributory phrases and\ntheir corresponding summary, yielding an average\nROUGE-1 score of at least 86% across all aspects,\nas shown in subfigure (c) of Figure 9.\nA.4\nInter-Annotator Agreement Details\nTo assess inter-annotator agreement (IAA), we re-\nport exact match rate, within-one rate, and mean\nabsolute error computed between the evaluations\nof the two annotators on a 5-point Likert scale,\n"}, {"page": 14, "text": "(a)\n(b)\n(c)\nFigure 9: Context attribution evaluation across sixteen medical aspects in the PCOA dataset. The x-axis represents\nthe sixteen individual medical aspects, while the y-axis indicates the evaluation scores.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 10: Statistical overview and analysis of annotated data instances in PCOA. Subfigures (a–f) present: (a)\ndistribution of article length (in tokens); (b) distribution of sentence count per article; (c) distribution of summary\nlength (in tokens); (d) number of sentences cited by each summary; (e) number of key phrases (in tokens) cited by\neach summary; and (f) aspect coverage across all articles.\nfollowing prior work (Attali and Burstein, 2006;\nZhang and Zhou, 2007). The definitions of these\nmetrics are as follows:\nExact Match Rate: The proportion of instances\nin which the two annotators assigned exactly the\nsame rating on the 5-point Likert scale.\nWithin-One Rate: The proportion of instances\nin which the absolute difference between the two\nannotators’ ratings is at most one point on the 5-\npoint Likert scale.\nMean Absolute Error: The average of the abso-\nlute differences between the two annotators’ ratings\nacross all instances on the 5-point Likert scale.\nA.5\nDataset Characteristics Details\nSource Articles: The Natural Language Toolkit\n(NLTK) (Bird et al., 2009) was used to analyze\nthe included articles. The articles have an average\nlength of 392.81 tokens, ranging from 138 to 814,\nand contain an average of 13.91 sentences, with\ncounts ranging from 4 to 24. The distributions of\narticle lengths and sentence counts are shown in\nsubfigures (a) and (b) of Figure 10.\nData Instances: In 1, 799 human-annotated data\ninstances, the average summary length is 24.63\ntokens, ranging from 2 to 48. Each summary cites\nan average of 1.44 sentences, with counts ranging\nfrom 1 to 6. The distributions of summary lengths\nand cited sentence counts are shown in subfigures\n(c) and (d) of Figure 10. The average length of\ncontributory phrases is 10.78 tokens, ranging from\n1 to 92, as shown in subfigure (e) of Figure 10.\nAspect Coverage in Articles: Among the 152 in-\ncluded articles, more than 150 report information\non the P (Participants; n = 152), I ( Intervention;\nn = 150), O (Outcomes; n = 150), and F (Find-\nings; n = 151) aspects. In contrast, fewer than\n70 articles address the SE (Secondary Endpoints;\nn = 57), B (Blinding; n = 61), and FU (Fund-\ning; n = 42) aspects. The distribution of aspect\ncoverage is shown in subfigure (f) of Figure 10.\nRelative Positions of Information: To examine\nthe structural placement of aspect-based informa-\ntion within articles, we analyzed the positional dis-\ntribution of cited sentences across rhetorical as-\npects. As shown in Figure 11, aspect OB (Ob-\njective) information predominantly appears at the\nbeginning of articles, whereas aspect F (Findings)\ntypically occurs toward the end of articles.\n"}, {"page": 15, "text": "Figure 11: Relative positions of sentence-level citations in articles for each aspect. The top indicates the beginning\nof the article, and the bottom indicates the end. The x denotes the average position of sentences related to each\naspect within the articles.\nREFERENCE\nSUMMARY: Patients received a low dose of 30,000 and high dose of 60,000 mNAU of intratumoral GEN0101.\nSENTENCES: [2], [4]\nKEY PHRASES: “intratumoral GEN0101 administration”, “low dose of 30,000”, “high dose of 60,000 mNAU”, “GEN0101”\nINTRINSIC CONTEXT ATTRIBUTION\n⇒SUMMARY: The intervention involved intratumoral administration of GEN0101 at doses of 30,000 mNAU (low dose) or 60,000 mNAU (high\ndose) in patients with stage IIIC or IV malignant melanoma.\n⇒SENTENCES: [1] We previously demonstrated that hemagglutinating virus of Japan-envelope (HVJ-E) induced not only direct. (✗) ... [2] We here\nperformed a phase Ia clinical trial of intratumoral GEN0101 administration in six patients with stage IIIC or IV malignant melanoma. (✓) [4] Patients\nwere separated into two groups (n = 3 each) and received a low dose of 30,000 and high dose of 60,000 mNAU of GEN0101. (✓)\n⇒KEY PHRASES: “intratumoral GEN0101 administration”, “low dose of 30,000 mNAU”, “high dose of 60,000 mNAU”\n⇒C-F1: 0.857, S-F1: 0.800, P-F1:1.000\nPRIOR CONTEXT ATTRIBUTION\nSTEP 1. ⇒SENTENCES: [2] We here performed a phase Ia clinical trial of intratumoral GEN0101 administration in six patients with stage IIIC or\nIV malignant melanoma. (✓) [4] Patients were separated into two groups (n = 3 each) and received a low dose of 30,000 and high dose of 60,000\nmNAU of GEN0101. (✓)\nSTEP 2. ⇒KEY PHRASES: “intratumoral GEN0101 administration”, “low dose of 30,000 mNAU”, “high dose of 60,000 mNAU”\nSTEP 3. ⇒SUMMARY: Patients received intratumoral GEN0101 administration at low (30,000 mNAU) or high (60,000 mNAU) doses.\n⇒C-F1: 1.000, S-F1: 1.000, P-F1: 1.000\nPOST-HOC CONTEXT ATTRIBUTION\nSTEP 1. ⇒SUMMARY: The phase Ia clinical trial evaluated intratumoral administration of GEN0101 (HVJ-E) at low (30,000 mNAU) and high\n(60,000 mNAU) doses in six patients with advanced malignant melanoma, demonstrating safety, tolerability, and antitumor effects, including increased\nNK cell activity and interferon-˘03b3 levels.\nSTEP 2. ⇒SENTENCES: [2] We here performed a phase Ia clinical trial of intratumoral GEN0101 administration in six patients with stage IIIC or\nIV malignant melanoma. (✓) [3] The primary aim was to evaluate the safety and tolerability of GEN0101, and the secondary... (✗) [4] Patients were\nseparated into two groups (n = 3 each) and received a low dose of 30,000 and high dose of 60,000 mNAU of GEN0101. (✓) ... [5] (✗) [6] (✗) [9] (✗)\n[10] (✗)\nSTEP 3. ⇒KEY PHRASES: “intratumoral GEN0101 administration”, “low dose of 30,000 and high dose of 60,000 mNAU”, “safety and tolerability”,\n...\n⇒C-F1: 0.364, S-F1: 0.444, P-F1: 0.491\nTable 5: A case analysis of three context attribution strategies (PMID: 34984539. Aspect: Intervention).\nB\nExperiments Details\nB.1\nComparative Case Study of Three\nContext Attribution Strategies\nThe differences among the three context attribu-\ntion strategies are further examined through a case\nstudy. As shown in Table 5, the intrinsic strategy\ngenerates the summary along with cited sentences\nand contributory phrases, but produces an invalid\ncitation (i.e., [1]) that is not supported by the source\ncontent. In contrast, the prior strategy accurately\nidentifies sentences and contributory phrases rel-\nevant to the aspect I (Intervention) (e.g., [2], [4]),\nresulting in a coherent and complete summary. By\ncomparison, the post-hoc strategy first generates\nthe summary; although the claim is correct (C-R\n= 1.000), it includes substantial irrelevant infor-\nmation, leading to many unrelated sentences (e.g.,\n[3], [5], [6], [9], [10]) and phrases being retrieved.\nThese observations indicate that the prior strategy\neffectively filters out irrelevant context, improving\ngeneration precision, while the retrieved sentences\nand phrases naturally serve as valid citations.\n"}, {"page": 16, "text": "C\nInstructions And Demonstration\nC.1\nPrompt For Intrinsic Context Attribution\n#Instruction:\nGiven a medical abstract presented as a list of sentences, perform the following tasks:\n1. Provide a list of indices of the sentences that contain information about the study’s objective.\n2. From those sentences, extract a list of key phrases (e.g., indicators to be measured, target diseases, treatments) that are relevant\nto the objective.\n3. Based on the relevant sentences and extracted key phrases, summarize the objective of the study in one concise sentence.\n#Abstract:\n{abstract}\n#Indices of involved sentences:\n#Key phrases:\n#Summary:\nTable 6: Prompt instructions used for baselines to generate an aspect-based summary, cited sentences, and contribu-\ntory phrases. An example of the OB (Objective) aspect is shown.\nC.2\nPrompt For Subclaim Decomposition\nInstructions:\nPlease decompose the given summary into a list of atomic subclaims, where each subclaim represents a single factual statement\nfrom the summary. Output only a list of sentences, and do not output additional information.\nDemonstration:\nSummary:\nLong-term follow-up showed no new safety concerns, and results were consistent with the known tolerability profile of\nencorafenib plus binimetinib.\nList of subclaims:\n[\"Long-term follow-up was conducted.\", \"No new safety concerns were found.\", \"Results were consistent with the known\ntolerability profile of encorafenib plus binimetinib.\"]\nSummary:\n{summary}\nList of subclaims:\nTable 7: Prompt instructions used for Mistral-Large-2411 to decompose a summary into a list of subclaims.\nC.3\nPrompt For Prior Context Attribution\n#Instructions:\nGiven a medical abstract presented as a list of sentences, perform the following tasks:\n1. Provide a list of indices of the sentences that contain or contribute to the control group description.\n2. From those sentences, extract a list of key phrases (e.g., control group name, mode of administration) that are relevant to the\ncontrol group.\n#Abstract:\n{abstract}\n1. Indices of involved sentences:\n2. Key phrases:\nTable 8: Prompt instructions used in prior context attribution methods to retrieve relevant sentences for a given\nmedical aspect and extract contributory phrases from them. An example of the OB (Objective) aspect is shown.\n"}, {"page": 17, "text": "#Instructions:\nGiven a list of sentences containing information about the study’s objective, and a corresponding list of key phrases (e.g.,\nmeasured outcomes, target diseases, or treatments), summarize the study’s objective in one concise and coherent sentence and try\nto include as much information as possible from the key phrases.\n#Input Sentences:\n{sentences}\n#Input Key Phrases:\n{phrases}\n#Summary:\nTable 9: Prompt instructions used in prior context attribution methods to generate an aspect-based summary from\nretrieved sentences and extracted contributory phrases. An example of the OB (Objective) aspect is shown.\nC.4\nPrompt For Post-Hoc Context Attribution\n#Instructions:\nGiven a medical abstract presented as a list of sentences, summarize the objective of the study in one concise sentence.\n#Abstract:\n{abstract}\n#Summary:\nTable 10: Prompt instructions used in post-hoc context attribution methods to generate an aspect-based summary\nfrom an input abstract. An example of the OB (Objective) aspect is shown.\n#Instructions:\nGiven a medical abstract provided as a list of sentences, along with a summary describing the study’s objective, perform\nthe following tasks:\n1. Identify and return the indices of sentences in the abstract that are relevant to the given summary of the study’s objective.\n2. From the identified sentences, extract a list of key phrases that are pertinent to the study objective (e.g., measured indicators,\ntarget diseases, interventions, or treatments).\n#Abstract:\n{abstract}\n#Summary:\n{summary}\n1. Indices of involved sentences:\n2. Key phrases:\nTable 11: Prompt instructions used in post-hoc context attribution methods to retrieve contextual sentences from the\ninput abstract related to the input summary and to extract contributory phrases from the retrieved sentences. An\nexample of the OB (Objective) aspect is shown.\n"}]}