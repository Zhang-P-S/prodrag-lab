{"doc_id": "arxiv:2601.03023", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03023.pdf", "meta": {"doc_id": "arxiv:2601.03023", "source": "arxiv", "arxiv_id": "2601.03023", "title": "MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models", "authors": ["Lecheng Gong", "Weimin Fang", "Ting Yang", "Dongjie Tao", "Chunxiao Guo", "Peng Wei", "Bo Xie", "Jinqun Guan", "Zixiao Chen", "Fang Shi", "Jinjie Gu", "Junwei Liu"], "published": "2026-01-06T13:56:33Z", "updated": "2026-01-07T02:10:21Z", "summary": "Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03023v2", "url_pdf": "https://arxiv.org/pdf/2601.03023.pdf", "meta_path": "data/raw/arxiv/meta/2601.03023.json", "sha256": "57f2c5cff0fca363c2bf614227a5a9646b98a87fbbc31f83192b0d694eb6e450", "status": "ok", "fetched_at": "2026-02-18T02:23:02.318349+00:00"}, "pages": [{"page": 1, "text": "MEDDIALOGRUBRICS: A COMPREHENSIVE BENCHMARK AND\nEVALUATION FRAMEWORK FOR MULTI-TURN MEDICAL\nCONSULTATIONS IN LARGE LANGUAGE MODELS\nLecheng Gong1, Weimin Fang1, Ting Yang1, Dongjie Tao1, Chunxiao Guo1, Peng Wei1, Bo Xie1, Jinqun Guan1, Zixiao\nChen1, Fang Shi1, Jinjie Gu1, and Junwei Liu1\n1Ant Group\nABSTRACT\nMedical conversational AI (AI) plays a pivotal role in the development of safer and more effective\nmedical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing\nthe information-gathering and diagnostic reasoning abilities of medical large language models (LLMs)\nhave not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel\nbenchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained\nevaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically\ndesigned to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-\nagent system to synthesize realistic patient records and chief complaints from underlying disease\nknowledge without accessing real-world electronic health records, thereby mitigating privacy and\ndata-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical\nfacts and augmented with a dynamic guidance mechanism that continuously detects and corrects\nhallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the\nsimulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-\ngeneration pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject\nsampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a\ncomprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment\ndimensions, current models face substantial challenges. Our results indicate that improving medical\ndialogue will require advances in dialogue management architectures, not just incremental tuning of\nthe base-model.\n1\nIntroduction\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and\ngeneration, catalyzing a paradigm shift in medical informatics. Applications ranging from clinical decision support\nto patient-facing assistants promise to democratize healthcare access and alleviate clinician burnout [1, 2]. However,\nsafe deployment of LLMs in clinical settings requires evaluation frameworks that rigorously mirror the complexity of\nreal-world medical practice.\nIn authentic clinical environments, diagnostic reasoning is not a static question-answering task but a dynamic, multi-turn\nprocess. Clinicians must navigate an open problem space, actively ask for information, formulate hypotheses, and refine\ntheir differential diagnosis through iterative interaction [3, 4]. Despite this, existing benchmarks for medical LLMs\nfocus predominantly on static tasks—such as multiple-choice questions (e.g. MedQA, MedMCQA) or summarization\n[5, 6]. Although these benchmarks evaluate domain knowledge retention, they fail to assess an LLM’s ability to conduct\na structured consultation, manage dialogue flow, or exhibit safety behaviors during information gathering.\nDeveloping a robust evaluation framework for multi-turn medical consultations presents three distinct challenges:\n• Data Scarcity and Privacy: High-quality, multi-turn medical dialogue data is scarce due to strict privacy\nregulations (e.g., HIPAA/GDPR), limiting the scale of reproducibility studies.\narXiv:2601.03023v2  [cs.CL]  7 Jan 2026\n"}, {"page": 2, "text": "Work In Progress\nFigure 1: The landscape of MedDialogRubrics framework\n• Simulation reliability: While agent-based simulation offers an alternative that preserves privacy, stan-\ndard LLM-based patient agents are prone to hallucinations—inventing symptoms or contradicting medical\nlogic—which compromising the validity of the evaluation [7].\n• Evaluation Subjectivity: Unlike multiple-choice questions with a single gold standard, evaluating a dialogue\ntrajectory requires assessing whether \"must-ask\" questions were posed at the right time. This typically relies\non costly human annotation or vague heuristic-based metrics.\nTo address these limitations, we introduce MedDialogRubrics, a comprehensive benchmark and evaluation framework\ndesigned to rigorously assess the diagnostic reasoning and information-gathering capabilities of LLMs. Unlike prior\nworks that rely on unstructured simulations, our framework employs a clinically grounded, multi-agent synthesis\npipeline. We construct a robust Patient Agent anchored to atomic medical facts and augmented with a dynamic guidance\nmechanism to detect and correct hallucinations in real-time. Furthermore, we automate the evaluation process by\ngenerating over 60,000 fine-grained rubrics derived from Evidence-Based Medicine (EBM) guidelines, utilizing reject\nsampling to ensure clinical relevance.\nOur key contributions are summarized as follows:\n• A Novel Benchmark Dataset: We release a dataset of 5,200 synthetically constructed, clinically coherent\npatient cases covering a diverse range of diseases, synthesized without accessing private real-world records.\n• Hallucination-Free Patient Simulation: We propose a controlled Patient Agent architecture that decouples\nmedical knowledge from dialogue generation, ensuring internal consistency and clinical plausibility during\nmulti-turn interactions.\n• EBM-Grounded Evaluation Pipeline: We introduce a structured rubric-generation pipeline that combines\nLLM retrieval with expert refinement, establishing a \"must-ask\" criterion for objective scoring.\n• Comprehensive Analysis: We evaluate state-of-the-art LLMs using our framework, revealing significant\ngaps in current dialogue management architectures and highlighting the necessity for systems that go beyond\nincremental instruction tuning.\n2\n"}, {"page": 3, "text": "Work In Progress\n2\nRelated Work\n2.1\nMedical QA and Clinical Reasoning Benchmarks\nThe evaluation of Large Language Models (LLMs) in the medical domain has evolved from static knowledge retrieval\nto complex, multi-turn clinical reasoning and agentic interactions.[8, 9, 10] Early benchmarks, such as MedMCQA[11],\nestablished a foundation by providing over 194,000 high-quality multiple-choice questions (MCQs) from Indian\nmedical entrance exams, focusing on a model’s ability to reason across diverse healthcare topics in a single-turn format.\nSimilarly, LLM-MedQA[12] leverages the established MedQA dataset to enhance performance through multi-agent\narchitectures and case study generation, emphasizing domain-specific terminology and zero-shot reasoning.\nAs the field progressed toward assessing clinical utility, Med-PaLM 2[13] introduced a rigorous framework for long-\nform question answering, utilizing nine physician-validated evaluation axes and adversarial datasets to probe the\nlimits of model safety and accuracy. To address the limitations of static QA, several benchmarks transitioned to\nmulti-turn dialogue systems. LLM-Mini-CEX[14] adapted the traditional clinical exercise into an automated evaluation\nof diagnostic conversations, employing patient simulators to test both diagnostic capabilities and humanistic qualities.\nLiao et al. further advanced this by reformulating USMLE questions into interactive consultations, requiring models to\nproactively elicit missing patient information[15, 16]. MediQ[9] specifically targets this proactive information-seeking\nbehavior, simulating interactions between a Patient System and an Expert System to ensure reliable clinical reasoning\nunder incomplete context. Recent developments have introduced agentic and large-scale physician-led benchmarks.\nAgentClinic presents a multimodal agent environment that treats clinical decision-making as a sequential task involving\nexternal tools and electronic health records, revealing that interactive diagnosis is significantly more challenging than\nstatic answering. HealthBench[10] represents one of the most comprehensive efforts to date, featuring 5,000 multi-turn\nconversations evaluated against over 48,000 unique rubric criteria validated by 262 physicians. Finally, MAQuE[17]\nfocuses on the nuances of the \"doctor agent,\" evaluating inquiry proficiency and patient experience across 3,000\nsimulated patient agents with diverse emotional and linguistic patterns. Together, these benchmarks reflect a paradigm\nshift toward ensuring LLMs can safely and effectively navigate the dynamic nature of real-world clinical practice. If\nevaluating a doctor were like a driving test, MedMCQA would be the written theory exam, while AgentClinic, MediQ,\nand HealthBench would be the actual road test, requiring the driver to handle unexpected traffic, talk to passengers, and\nnavigate a changing environment safely.\nTable 1: Comparison of existing medical consultation benchmarks.\nBenchmark\nMulti-turn\nSupport\nKey Points\nRubrics\nExpert Validated\nNumber of\n# Rubrics\nMedMCQA\n✗\n✗\n✗\n–\nMed-PaLM 2\n✓\n✗\n✓\n–\nLLM-Mini-CEX\n✓\n✓\n✓\n–\nLiao et al.\n✓\n✗\n✗\n–\nAgentClinic\n✓\n✗\n✗\n–\nMediQ\n✓\n✗\n✗\n–\nLLM-MedQA\n✗\n✗\n✗\n–\nHealthBench\n✓\n✓\n✓\n48,562\nMAQuE\n✓\n✗\n✗\n–\nMEDDIALOGRUBRICS (Ours)\n✓\n✓\n✓\n60,000\nSome recent efforts attempt to incorporate more dynamic reasoning tasks. For example, interactive patient simulators\nhave been used in medical education for trainee evaluation [18], and some LLM studies explore chain-of-thought or\nstepwise diagnostic reasoning [4, 7]. However, existing simulators are either proprietary, manually constructed, or not\ndesigned for large-scale LLM evaluation.\n2.2\nDialogue Simulation and Patient Agents\nDialogue simulators in general NLP research include user simulators for task-oriented dialogue in domains such as\ntravel booking or customer service [19, 20]. However, medical contexts require stricter factual consistency, symptom\nlogic, and safety considerations. Some recent work proposes LLM-driven patient agents [21], but these often lack\ncontrols for reproducibility or clinical correctness, making them unsuitable for standardized evaluation in all models.\n3\n"}, {"page": 4, "text": "Work In Progress\nRecent advancements in Large Language Models (LLM) have shifted the focus of medical AI from static question-\nanswering [22, 23] to autonomous agentic systems capable of simulating complex, multi-turn clinical interactions.[24,\n25] Central to this shift is the development of \"patient agents\"—LLM-based entities designed to replicate the biological,\npsychological, and communicative traits of human patients.[26, 27] While early work relied on simple persona-based\nprompting [28], current research has pivoted toward more sophisticated construction paradigms to enhance behavioral\nrealism and factual grounding.\nA prominent methodological trend is the use of persona-driven frameworks like PatientSim, which models behavior\nalong axes of personality, literacy, and cognitive confusion.[26] To ensure clinical validity, these models are increasingly\naligned using Direct Preference Optimization (DPO), which significantly outperforms Supervised Fine-Tuning (SFT)\nin handling complex reasoning and emotional nuance[29]. Furthermore, the integration of Retrieval-Augmented\nGeneration (RAG) and Multi-Agent Systems (MAS) has enabled agents to interact with simulated Electronic Health\nRecords (EHR) and external diagnostic tools.[24, 30, 22] Frameworks such as AI Hospital [22] and MedAgentSim [31]\nhave established multi-role environments (Doctor, Patient, Examiner) to evaluate diagnostic accuracy within realistic\nclinical workflows.\nThe evaluation of these agents has similarly evolved from linguistic metrics (BLEU/ROUGE) to action-oriented\nbenchmarks like MedAgentBench [32] and MedAgentBoard.[33] These benchmarks challenge agents to execute\nmulti-step clinical tasks in FHIR-compliant environments, highlighting a \"reliability gap\" in current state-of-the-art\nmodels for action-based vs. query-based tasks.Emerging research now targets the \"statelessness\" of current agents,\nproposing Bayesian-inspired memory management systems to maintain longitudinal patient models for chronic disease\nsimulation[25, 34]. Collectively, these developments lay the groundwork for high-fidelity, autonomous patient simulators\nthat bridge the gap between AI-driven medical knowledge and its practical application in dynamic clinical settings.\nTable 2: Comparison of Dialogue Simulation and Patient Agents Framework\nMethod\nImplementation\nCost\nMedical\nAccuracy\nBehavioral\nRealism\nConsistency\nPrompt Engineering\nVery Low\nModerate\nHigh\nLow\nRAG\nModerate\nVery High\nModerate\nHigh\nFine-Tuning\nHigh\nHigh\nVery High\nModerate\nMulti-Agent\nVery High\nHigh\nModerate\nVery High\n2.3\nAutomated Evaluation for Medical LLMs (LLM-as-a-Judge)\nWhile traditional n-gram metrics (e.g. ROUGE, BLEU) remain insufficient for capturing the clinical validity of generated\ntext, the \"LLM-as-a-Judge\" paradigm has emerged as a scalable alternative to costly human expert annotation. Recent\nbenchmarks such as HealthBench[10] and MedHELM[35] have shifted the focus of evaluation from rote knowledge\nrecall to rubric-based behavioral assessment and jury-based consensus, respectively. Specifically, HealthBench utilizes\nover 48,000 granular criteria to assess safety and adherence, whereas MedHELM employs an ensemble of judge\nmodels to mitigate individual model biases. Despite their promise, studies by Maina et al. [36] highlight persistent\nchallenges, including substantial verbosity bias, inconsistency in low-resource languages (e.g., Kinyarwanda), and a\n\"severity gap\" where models like GPT-5 and Gemini exhibit divergent leniency compared to human clinicians. Notably,\nreasoning-enhanced models such as DeepSeek-R1 and GPT-o3-mini have recently demonstrated superior inter-rater\nreliability with human experts compared to standard instruction-tuned models, suggesting that explicit reasoning chains\nare critical for reliable automated clinical evaluation.consultations with formal scoring criteria.\n2.4\nGaps in Current Research\nAcross these areas, three major gaps emerge:\n• The lack of clinically grounded, scalable multi-turn medical-dialogue datasets with controlled patient behavior.\n• The absence of structured benchmarks that evaluate the inquiry strategy and the progression of diagnostic\nreasoning.\n• Limited rigor in applying LLM-as-judge methods to medical reasoning tasks, where errors may have significant\nsafety implications.\n4\n"}, {"page": 5, "text": "Work In Progress\nOur framework directly addresses these gaps by combining synthetic case generation, structured clinical key-point\nannotation, a reproducible patient agent, and a calibrated LLM-as-judge evaluation pipeline. To our knowledge, it is the\nfirst fully integrated benchmark specifically designed for evaluating multi-turn medical consultation competence in\nLLMs.\n3\nMethods\n3.1\nPhase 1: Multi-Agent Patient Record Generation\nTo facilitate scalable and reproducible evaluation of multi-turn diagnostic interactions, we adopt the Patient-Zero\nframework [37]. This approach enables the generation of synthetic patient data without accessing real-world medical\nrecords, thereby strictly preserving patient privacy. The generation pipeline consists of three distinct phases:\n• Disease Knowledge Retrieval: We source comprehensive disease-specific knowledge—including symptoms,\nepidemiology, and complications—from authoritative open medical encyclopedias.\n• Multi-Step Record Generation: A hierarchical multi-agent system constructs the patient record through\nthree progressive steps:\n1. Disease Outline: A high-level summary of the clinical presentation.\n2. Basic Information: Demographics, lifestyle factors, and medical history.\n3. Detailed Clinical Data: Specifics regarding symptom duration, severity, and pertinent negative findings.\n• Chief Complaint Synthesis: Based on the fully generated record, a specialized agent synthesizes the patient’s\n\"Chief Complaint\" (the initial statement presented to the physician) to simulate realistic consultation initiation.\nStage 1: Disease Knowledge Retrieval\nWe begin by selecting the target conditions from a curated taxonomy that\ncovers common primary care presentations, chronic diseases, mental-health conditions, and acute emergencies. For\neach condition, we define:\n• Core symptoms (required for diagnosis)\n• Auxiliary symptoms (optional but common)\n• Red-flag symptoms that signal severe or urgent presentations\n• Risk factors, epidemiological constraints, and typical disease trajectory\n• Differential diagnoses with overlapped symptoms\nThese elements are derived from clinical guidelines and medical knowledge sources (e.g., UpToDate, NICE guidelines,\nstandard textbooks, Baidu Health Encyclopedia1, Kuake Health Encyclopedia). Each disease profile is encoded in a\nstructured template.\nStage 2: Multi-Step Record Generation\nOnce a disease profile is established, we sample distributions based\non demographics, severity of symptoms, onset time, comorbidities, and lifestyle variables. Rules guaranty internal\nconsistency, such as ensuring that the progression of symptoms aligns with the chosen stage of the disease or that\nincompatible comorbid conditions do not coexist.\nA multi-agent system generates the record in stages:\n• Disease Overview: A high-level summary of the presentation.\n• Basic Information: Demographic, lifestyle, and medical history.\n• Detailed Clinical Data: duration, severity, and findings of negative symptoms.\nStage 3: Chief Complaint Synthesis\nBased on the complete record, a separate agent synthesizes the patient’s initial\n\"Chief Complaint\" (the opening statement to the doctor).\nThen We apply automated validators that check for:\n• Missing required symptoms\n1https://expert.baidu.com/dict/pages/main/index\n5\n"}, {"page": 6, "text": "Work In Progress\nFigure 2: Multi-Agent Patient Record Generation Pipeline\n• Contradictory time courses\n• Misaligned demographics and risk factors\n• Overlap between the case description and distractor illnesses\nCases that fail these checks are revised or discarded. The result is a dataset of high-quality diagnostic scenarios.\n3.2\nPhase 2: Patient Agent Design\nA central component of the evaluation framework is the patient agent, which simulates patient behavior during multi-turn\ninteractions. The agent must be realistic enough to challenge the model while remaining deterministic to provide a fair\ncomparison between models.\nAs shown in 3, the core design features of the patient agent include:\n• The Patient Agent is designed to simulate a real patient interacting with the candidate Doctor LLM.\n• Atomic Decomposition: The generated narrative record is decomposed into a set of discrete \"atomic statements\"\n(e.g., Symptom: Headaches, Duration: 2 weeks, Trigger: Bright light). This serves as the agent’s \"ground\ntruth\" memory.\n• Strict Adherence and Inference: The agent is prompted to answer questions strictly based on these atomic facts.\nIf asked about missing details, it provides plausible answers that do not contradict the record (e.g., denying a\nspecific allergy if the record lists \"No known allergies\").\n6\n"}, {"page": 7, "text": "Work In Progress\nFigure 3: Patient Agent & Multi-turn Dialogue Generation Framework\n• Guidance Injection Loop: To prevent hallucinations observed in standard agents, we implement a feedback\nloop. If the Patient Agent generates a response that conflicts with its atomic memory during a turn, the system\ncatches the error, suppresses the output, and injects a \"Guidance Prompt\" (e.g., Warning: Your record states\nyou are a non-smoker. Correct your response.). This guidance persists in the context of preventing repeated\nerrors.\n3.3\nPhase 3: LLM-Based & Expert-Annotation EBM-Driven Rubric Generation\nTo systematically evaluate the multi-turn dialogue, we define a gold-standard list of key inquiry rubrics for each case.\nThese represent the essential information a competent clinician would seek during the diagnostic interview. The key\ninquiry rubrics can be seen in Table 4.\nTo enhance factual grounding and controllability, the key rubrics are derived using a hybrid manual–automated\nmethodology: large language models (LLMs) first generate candidate key points from the case description, after which\nclinical experts systematically review, filter, and annotate these candidates.\n3.3.1\nGeneration of Multi-Agent rubrics with rejection Sampling\nAs illustrated in Figure 4, we propose a multi-agent pipeline with (i) a Rubric Generation Agent that proposes\ncandidate rubrics grounded in an evidence-based knowledge graph, (ii) a Rubric Evaluation Agent that scores\ncandidates with respect to predefined quality criteria, (iii) reject sampling with thresholding to filter low-quality\ncandidates, and (iv) a Rubric Improvement Agent that iteratively refines rejected candidates using structured feedback.\nAccepted and verified rubrics are stored in a rubric repository for downstream human annotation.\nEvidence Grounding via Knowledge Graph.\nWe assume access to an evidence-based knowledge graph G (e.g.,\nguideline-derived diagnostic pathways) that encodes clinically relevant entities and relations. For an input case x, we\nretrieve a subgraph Gx (or a set of relevant triples) to constrain generation and promote coverage of guideline-mandated\ninquiry dimensions.\nRubric Generation Agent.\nThe generation agent parameterized by an LLM produces a candidate rubric set\nR(k) ∼pθ(R | x, Gx) ,\n(1)\n7\n"}, {"page": 8, "text": "Work In Progress\nFigure 4: Generation of Multi-Agent rubrics with rejection Sampling\nwhere we sample K candidates {R(k)}K\nk=1 to encourage diversity and avoid mode collapse. In practice, R(k) may be\ngenerated via temperature-controlled sampling and/or prompting constraints derived from Gx.\nRubric Evaluation Agent.\nAn independent evaluator agent assigns each candidate set R(k) a scalar score s(k) ∈R:\ns(k) = fϕ\n\u0010\nx, Gx, R(k)\u0011\n,\n(2)\nwhere the scoring rubric aggregates criteria commonly required in clinical interviewing, including: (i) relevance to x,\n(ii) coverage/completeness, (iii) non-redundancy, (iv) clarity/actionability, and (v) consistency with evidence in Gx.\nCore Principle Verification.\nBeyond scalar scoring, we enforce a set of hard constraints (“core principles”) via a\nverifier agent v(·):\nVERIFY\n\u0010\nR(k)\u0011\n∈{0, 1},\n(3)\nwhich checks, for example, that guideline-mandated red flags are covered, unsafe/irrelevant inquiries are absent, and\neach rubric is grounded (directly or indirectly) in Gx.\nRubric Improvement Agent.\nFor candidates that are rejected or fail verification, an improvement agent revises\nrubrics conditioned on structured feedback:\neR(k) ∼pψ\n\u0010\nR | x, Gx, R(k), F(k)\u0011\n,\n(4)\nwhere F(k) contains evaluator/verifier feedback (e.g., missing red flags, redundancy, ambiguous phrasing). The refined\nrubrics are re-submitted to evaluation and verification, forming a closed-loop refinement process.\nReject Sampling.\nWe apply reject sampling by accepting a candidate if its evaluation score exceeds a threshold τ:\nACCEPT\n\u0010\nR(k)\u0011\n≜I\nh\ns(k) ≥τ\ni\n.\n(5)\nA candidate is stored only if it is both accepted and passes core principle verification:\nR(k) ∈D ⇐⇒ACCEPT\n\u0000R(k)\u0001\n= 1 ∧VERIFY\n\u0000R(k)\u0001\n= 1.\nwhere D denotes the rubric repository.\n8\n"}, {"page": 9, "text": "Work In Progress\nAlgorithm 1 Multi-Agent Rubric Generation with Reject Sampling\nRequire: Case description x; knowledge graph G; retrieval function RETRIEVE; generator pθ; evaluator fϕ; verifier v;\nimprover pψ; threshold τ; samples K; max refinement steps T.\nEnsure: Verified rubric set(s) added to repository D.\n1: Gx ←RETRIEVE(G, x)\n2: for k = 1 to K do\n3:\nR(k) ∼pθ(R | x, Gx)\n4:\nfor t = 1 to T do\n5:\ns(k) ←fϕ(x, Gx, R(k))\n6:\na ←I[s(k) ≥τ]\n7:\nb ←v(x, Gx, R(k))\n8:\nif a = 1 and b = 1 then\n9:\nD ←D ∪{R(k)} {Store verified rubrics}\n10:\nbreak\n11:\nelse\n12:\nF(k) ←FEEDBACK(s(k), a, b)\n13:\nR(k) ∼pψ(R | x, Gx, R(k), F(k))\n14:\nend if\n15:\nend for\n16: end for\n17: return D\n3.3.2\nExpert Annotation\nAs shown in Figure 5, an independent panel of three domain experts evaluates each candidate rubric in parallel. For\nevery rubric, each expert provides (i) an independent binary vote (Keep vs. Delete) and (ii) free-form textual feedback\ncontaining concrete revision suggestions (e.g., clarifications, scope constraints, wording edits, or edge-case handling).\nAll votes are then aggregated and tallied. A rubric is retained for further processing only if it receives at least two Keep\nvotes (majority agreement; vote count ≥2). Rubrics failing to reach this threshold (< 2 Keep votes) are discarded.\nFor retained rubrics, the system performs organization and merging: overlapping or semantically redundant rubrics are\nconsolidated, and the retained rubrics are refined by incorporating the experts’ textual feedback. This consolidation-\nand-revision stage yields a curated set of Key Rubrics, representing the final expert-validated rubric set produced in this\nphase.\n3.4\nPhase 4: LLM-as-Judge Evaluation Framework\nWe developed a structured LLM-as-judge evaluation framework designed to provide clinically grounded, reproducible,\nand bias-minimized assessments of multi-turn diagnostic dialogues. The framework was informed by empirical\nperformance patterns observed in three independent datasets (7166 matched samples), ensuring that the adjudication\nfaithfully reflects real-world behaviors and safety requirements in medical reasoning.\nEach conversation from the interactions between different doctor agents and simulated patient agents is evaluated\nagainst all the rubric criteria for each case using a model as a grader, in an LLM-as-ajudge setup. The model-based\ngrader outputs binary judgment verdicts for each rubric, which are Satisfied and Not Satisfied. The final case score is\nthe weighted sum of all positive and negative weights, normalized by the sum of the positive weights (the maximum\npossible score that the doctor agent can achieve).\nSk =\nP\nri∈R Irimri\nP\nri∈R Iri\n, mri = Judge(Ck, ri)\n(6)\nmri =\n\u001a1,\nif ri is satisfied,\n0,\nif ri is not satisfied.\n(7)\nwhere Sk is the final case score for the case and the conversation C generated by the doctor agents. R is the set of all\nrubrics for each case k, Iri is the importance score assigned to rubric ri and mri is the binary indicator returned from\nthe model-based judge, Judge(·, ·), representing the llm-as-judge process for rubric ri.\n9\n"}, {"page": 10, "text": "Work In Progress\nFigure 5: Expert Annotation Rubric Generation Workflow\nHuman Consistency Analysis\nSimilar to HealthBench [10], we utilize the Macro F1 score to validate the effectiveness\nof using a model-based grader as a proxy for human judgment. In our setup, we compare the ground truth judgment\nof experts and model-based graders for each task and compute the F1 scores for each of the classes {Satisfied, Not\nSatisfied}.\nF1 = 2 · precision · recall\nprecision + recall,\nprecision =\nTP\nTP + FP , recall =\nTP\nTP + FN ,\n(8)\nwhere TP, FP, and FN are the True Positive, False Positive, and False Negative values, respectively. We also\nperformed ablation studies to isolate the most significant factors in the level of alignment between the model-based\ngrader and human judgments. For more details, see Section 4.5.\n4\nExperiments Results and Analysis\nWe evaluate the medical consultation capabilities of four representative Large Language Models (LLMs)—spanning\nboth proprietary and open-source paradigms—functioning as doctor agents within the MedDialogRubrics framework.\nDistinguishing itself from existing benchmarks, our framework incorporates over 60,000 expert-annotated rubric criteria\nacross more than 4,700 cases. This high-resolution granularity facilitates an atomic-level quality assessment, enabling\nthe precise identification of specific failure modes that remain obscured by traditional, coarse-grained evaluation metrics.\n10\n"}, {"page": 11, "text": "Work In Progress\n4.1\nExperimental Setup\nEvaluated Systems\nWe benchmark two state-of-the-art open-source models (Qwen3-235B-A22B-Instruct-2507[38],\nDeepSeek-R1[39]) and two state-of-the-art proprietary models (GPT-5[40], Gemini-2.5-Pro[41]).\nThis study focuses specifically on assessing the inquiry completeness of large language models (LLMs) in a clinical\ncontext. To this end, we configure each model to conduct multi-turn clinical interviews with a simulated patient agent\nthat generates responses conditioned on an underlying electronic medical record. Using a unified system prompt that\nexplicitly specifies the evaluation dimensions, we instruct each model to adopt the role of a physician and perform a\nmulti-round clinical history-taking process. At each dialogue turn, the model receives the full interaction history and\nmay either continue asking questions or terminate the consultation by outputting the token “End Inquiry.” To avoid\nunbounded interactions, we cap each session at a maximum of 12 turns.\nFor both patient simulation and inquiry quality evaluation, we employ DeepSeek-V3, selected based on its favorable\ntrade-off between performance and computational cost. The detailed prompting schemes used for all components are\ndescribed below.\nEnsemble Strategies and Implementation\nEvaluating models in isolation fails to capture the complexity of medical\ndecision-making. Therefore, we implement robust ensemble strategies using a panel of three advanced LLMs: GPT-5,\nGemini-2.5-Pro, and DeepSeek-V3[42]. The alignment of these judges is rigorously verified against human experts (via\nMacro F1 on 300 cases). To simulate clinical decision boards, we assess three distinct aggregation mechanisms:\n• Majority Voting: Reflects democratic consensus to filter outliers.\n• Unanimous Voting: Enforces a zero-tolerance policy for disagreement to ensure high precision.\n• Liberal Aggregation: Prioritizes recall by flagging a condition if any single agent detects it, mimicking\nsafety-critical screening protocols.\nThese strategies allow us to quantify the reliability gains of deploying multi-LLM systems in diagnostic scenarios.\nEvaluation Pipeline\nThe evaluation of each doctor agent is conducted through the following procedure:\n1. A multi-turn consultation is first generated by interacting with the model and a controlled patient agent.\n2. Inquiry actions and reasoning patterns are then extracted from the resulting dialogue context.\n3. Structured scoring is applied utilizing the MedDialogRubrics LLM-as-a-Judge pipeline.\n4. Consistency verification is performed, with safety penalties enforced where discrepancies arise.\n5. Scores are aggregated at both the per-case and per-dataset granularities.\n6. Finally, classical metrics—including Precision, Recall, Accuracy, and F1-score—are computed to quantify\nperformance.\n4.2\nAnalysis of Information Gathering and Temporal Dynamics\nFigure 6 illustrates the trajectory of diagnostic accuracy relative to the length of the consultation (number of turns). We\nobserve distinct behavioral patterns across the evaluated models:\nEfficiency vs. Exhaustiveness:\nGemini-2.5-pro (green trajectory) demonstrates superior information-seeking effi-\nciency. It achieves a rapid increase in accuracy, peaking at approximately 52% around turns 9-10 before stabilizing.\nThis suggests a highly strategic inquiry policy that prioritizes high-value medical facts early in the conversation.\nThe \"Late-Bloomer\" Phenomenon:\nIn contrast, GPT-5 (red trajectory) exhibits a linear growth pattern. While it\nstarts with a lower match rate compared to Gemini and DeepSeek at turn 4 (< 30%), it surpasses DeepSeek by turn 9\nand continues to improve, achieving its highest performance at 13+ turns. This behavior indicates a more cautious or\nexhaustive reasoning strategy, requiring deeper context to formulate precise inquiries.\nPerformance Plateaus:\nDeepSeek-R1 and Qwen3-235B-A22B-Instruct-2507 show more modest gains. DeepSeek-R1\nshows steady but slow improvement, plateauing near 40%, while Qwen3-235B-A22B-Instruct-2507 struggles to exceed\na 30-35% match rate regardless of dialogue length.\nKey Takeaway:\nThe disparity between the strongest model (Gemini-2.5-pro at ∼52%) and the theoretical maximum\n(100%) highlights the \"substantial challenges\" mentioned in our abstract. Even advanced models miss nearly half of the\ncritical diagnostic criteria defined by experts, underscoring the difficulty of the MedDialogRubrics benchmark.\n11\n"}, {"page": 12, "text": "Work In Progress\nFigure 6: Relationship between the number of consultation turns and rubric match precision for different LLM-based\ndoctor–patient agent pairs. Each curve reports the precision of matching asked consultation questions to the reference\nconsultation key points prior to reaching a final conclusion, illustrating how information coverage and alignment evolve\nas dialogue length increases across models.\n4.3\nAblation Study on Patient Agent Components\nTo validate the effectiveness of the proposed Patient Agent framework, we conducted an ablation study assessing\nthe impact of each component on agent fidelity. Table 3 reports the performance metrics across three progressive\nconfigurations: Basic, Strict Adherence, and the full pipeline with Guidance Injection.The Basic setup, which relies\nsolely on prompt engineering without constraints, exhibits a relatively high hallucination rate (0.129) and suboptimal\nconsistency in behavior (0.689). This suggests that a standard LLM struggles to maintain a consistent patient persona\nover multi-turn dialogues.Incorporating Strict Adherence & Inference significantly enhances the agent’s stability. By\ngrounding responses in decomposed atomic statements, the Behavior score improves dramatically from 0.689 to 1.000,\nand the hallucination rate decreases to 0.076. This indicates that constraining the agent to \"ground truth\" memory is\nessential for simulating consistent patient logic.Finally, the deployment of the Guidance Injection Loop achieves the\nbest performance. This feedback mechanism further attenuates the hallucination rate to 0.049 and boosts Relevance to\n0.992. Notably, under this full configuration, the agent achieves perfect scores (1.000) across all Anthropomorphism\ndimensions (Linguistics, Cognition, and Behavior). These results empirically demonstrate that our dual-mechanism\ndesign (Strict Adherence + Guidance Loop) effectively mitigates the stochastic hallucinations typical of LLMs while\npreserving high-fidelity patient simulation.\n4.4\nHuman-LLM Judge Alignment for Auto-Evaluation\n4.4.1\nEnsemble strategies\nTo ensure the scalability of our benchmark, we validated our automated scoring mechanism. Figure 7 compares the\nalignment of three voting strategies—Majority Voting, Unanimous Voting, and Liberal Strategy—against ground-truth\nscores provided by clinical experts.\n12\n"}, {"page": 13, "text": "Work In Progress\nPatient Agent Component\nHallucination ↓Relevance ↑\nAnthropomorphism\nLinguistics ↑Cognition ↑Behavior ↑\nBasic\n0.129\n0.92\n0.981\n0.971\n0.689\n+ Strict Adherence & Inference\n0.076\n0.951\n0.993\n1.000\n1.000\n+ Guidance Injection Loop\n0.049\n0.992\n1.000\n1.000\n1.000\nTable 3: Ablation study of the Patient Agent. We report the Hallucination rate (↓), Relevance (↑), and Anthropomorphism\nscores across three progressive configurations. The results demonstrate that the combination of Strict Adherence and\nthe Guidance Injection Loop significantly minimizes hallucinations while maximizing behavioral realism.\nFigure 7: Evaluation metics on different doctor models using different judge strategies\nRobustness of Majority Voting:\nAcross all three Doctor Agents (DeepSeek, Gemini, GPT-5), the Majority Voting\nstrategy (blue bars) consistently produces stable agreement with human experts, maintaining F1-scores in the 75–79%\nrange. This confirms that an ensemble of LLM judges can reliably approximate human clinical judgment.\nPrecision-Recall Trade-offs:\nThe Unanimous Voting strategy (purple bars) typically results in lower Recall and\nF1-scores (e.g., dropping to ∼74.7% accuracy for DeepSeek), indicating that it is overly penalizing. In contrast, the\nLiberal Strategy (orange bars) achieves the highest alignment metrics, particularly for GPT-5 (F1-Score ≈79.6%).\nHowever, we adopt Majority Voting for our main leaderboard to balance sensitivity and specificity, mitigating the risk\nof false positives inherent in the Liberal approach.\n4.5\ndiscussion\nThe empirical results from MedDialogRubrics reveal critical insights into the current state of medical AI and the\nvalidity of simulated benchmarks.\nThe \"Inquiry Deficit\" in LLMs.\nA central finding from Figure 6 is that an increase in context length does not\nstrictly guaranty better diagnostic reasoning. Although prompt engineering techniques often focus on context window\nutilization, our results suggest that the bottleneck lies in active inquiry planning. Models like Qwen3-235B-A22B-\nInstruct-2507 and DeepSeek-r1 plateau early, suggesting that they do not dynamically update their differential diagnosis\nto ask the next most relevant question. Only Gemini-2.5-pro demonstrated the ability to \"close the loop\" effectively\nwithin a standard clinical time frame (8-12 turns).\n13\n"}, {"page": 14, "text": "Work In Progress\nBenchmarking Beyond Static QA.\nThe dynamic nature of our results validates the necessity of multi-turn evaluation.\nIn static benchmarks (e.g. MedQA), differences between models like GPT-5 and Qwen3-235B-A22B-Instruct-2507\nmight be marginal. However, our temporal analysis (Figure 6) exposes a significant behavioral gap—up to a 20%\ndifference in rubric coverage—that static snapshots would obscure. This confirms that MedDialogRubrics captures the\nclinical reasoning process, not just the outcome.\nReliability of Automated Evaluation.\nThe high alignment scores in Figure 7 (Acc > 76%) between our automated\npipeline and human experts are crucial. They demonstrate that by employing a multi-agent judging system with voting\nensembles, we can scale medical evaluation without the prohibitive cost of continuous expert annotation. The \"Liberal\nStrategy\" shows high agreement for GPT-5 specifically, suggesting that stronger models may generate more nuanced\nanswers that are harder for a strict \"Unanimous\" judge to validate, but are correctly recognized by a more flexible\naggregation strategy.\n5\nConclusions\nWe present MedDialogRubrics, a benchmark and evaluation framework for rigorously assessing the multi-turn inquiry\nabilities of medical LLMs. Unlike previous work that focused on single-turn QA or final diagnosis accuracy, our\nframework targets fine-grained, human-aligned evaluation of the diagnostic process. Using 5,200 synthetic patient cases\nand more than 60,000 expert-refined rubric criteria, MedDialogRubrics assesses not only diagnostic correctness but also\nthe completeness, logic, and effectiveness of information gathering. A key feature is the use of Evidence-Based Medicine\n(EBM) guidelines to define specific \"must-ask\" questions, exposing capability gaps that aggregate metrics miss and\nseparating conversational fluency from clinical adequacy. A dynamic guidance mechanism reduces hallucinations during\ndata generation, keeping evaluations clinically plausible and coherent. Experiments show that even state-of-the-art\nLLMs struggle under these standards, particularly in strategic information seeking and long-context management. Our\nresults indicate that improving medical conversational AI will require advances in dialogue management architectures,\nnot just incremental tuning of the base-model. MedDialogRubrics, therefore, offers a standardized and challenging\nplatform for reproducible research to advance safer, more comprehensive, and clinically effective doctor agents.\nReferences\n[1] T. Jiang, C. Huang, H. Xia, et al. Large language models in healthcare: Opportunities, challenges, and future\ndirections. npj Digital Medicine, 2023.\n[2] M. Singhal, S. Azizi, T. Tu, et al. Large language models encode clinical knowledge. Nature, 2023.\n[3] J. Kassirer and L. Kopelman. Learning Clinical Reasoning. Lippincott Williams & Wilkins, 1991.\n[4] M. Nezhad, F. Mireshghallah, and M. Sap. Diagnosing large language models: Can llms perform clinical\nreasoning? arXiv preprint arXiv:2309.00060, 2023.\n[5] B. Jin, A. Dhingra, Z. Liu, et al. What disease does this patient have? a large-scale open-domain medical qa\ndataset. In Proceedings of EMNLP, 2020.\n[6] Y. Jin, Y. Zhang, K. Chen, et al. Clinical-longformer and clinical-bert: Transformers for long clinical documents.\nJournal of Biomedical Informatics, 2022.\n[7] J. Nori, C. Ji, C. Fallenstein, et al. Capabilities of gpt-4 in medical reasoning. arXiv preprint arXiv:2303.13375,\n2023.\n[8] Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. AgentClinic: a\nmultimodal agent benchmark to evaluate AI in simulated clinical environments, May 2025. arXiv:2405.07960\n[cs].\n[9] Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S. Ilgen, Emma Pierson, Pang Wei Koh, and\nYulia Tsvetkov. MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning,\nNovember 2024. arXiv:2406.00922 [cs].\n[10] Rahul K Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos\nTsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large\nlanguage models towards improved human health. arXiv preprint arXiv:2505.08775, 2025.\n[11] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. MedMCQA : A Large-scale Multi-Subject\nMulti-Choice Dataset for Medical domain Question Answering, March 2022. arXiv:2203.14371 [cs].\n14\n"}, {"page": 15, "text": "Work In Progress\n[12] Hang Yang, Hao Chen, Hui Guo, Yineng Chen, Ching-Sheng Lin, Shu Hu, Jinrong Hu, Xi Wu, and Xin Wang.\nLLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models, January\n2025. arXiv:2501.05464 [cs].\n[13] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip\nMansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun\nLiu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi\nMatias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards Expert-Level Medical Question\nAnswering with Large Language Models, May 2023. arXiv:2305.09617 [cs].\n[14] Xiaoming Shi, Jie Xu, Jinru Ding, Jiali Pang, Sichen Liu, Shuqing Luo, Xingwei Peng, Lu Lu, Haihong Yang,\nMingtao Hu, Tong Ruan, and Shaoting Zhang. LLM-Mini-CEX: Automatic Evaluation of Large Language Model\nfor Diagnostic Conversation, August 2023. arXiv:2308.07635 [cs].\n[15] Yusheng Liao, Yutong Meng, Hongcheng Liu, Yanfeng Wang, and Yu Wang. An Automatic Evaluation Framework\nfor Multi-turn Medical Consultations Capabilities of Large Language Models, September 2023. arXiv:2309.02077\n[cs].\n[16] Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, and Yu Wang. Automatic interactive\nevaluation for large language models with state aware patient simulator, Jul 2024.\n[17] Linlu Gong, Ante Wang, Yunghwei Lai, Weizhi Ma, and Yang Liu. The Dialogue That Heals: A Comprehensive\nEvaluation of Doctor Agents’ Inquiry Capability, September 2025. arXiv:2509.24958 [cs].\n[18] R. Cook, J. Woods, and T. Bruns. Simulated patients in medical education: A review. Medical Education, 2011.\n[19] J. Schatzmann, K. Weilhammer, M. Stuttle, et al.\nA survey of statistical user simulation techniques for\nreinforcement-learning of dialogue management strategies. Knowledge Engineering Review, 2006.\n[20] A. Budzianowski and P. Vuli’c. Hello, it’s gpt-3: Towards zero-shot task-oriented dialogue systems. In Proceedings\nof SIGDIAL, 2019.\n[21] X. Wang, R. Zhang, Y. Shao, et al. Large language models as patient simulators: Opportunities and challenges.\narXiv preprint arXiv:2305.00050, 2023.\n[22] Zhihao Fan, Lai Wei, Jialong Tang, Wei Chen, Wang Siyuan, Zhongyu Wei, and Fei Huang. AI hospital:\nBenchmarking large language models in a multi-agent medical interaction simulator. In Owen Rambow, Leo\nWanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings\nof the 31st International Conference on Computational Linguistics, pages 10183–10213, Abu Dhabi, UAE, January\n2025. Association for Computational Linguistics.\n[23] Ivan Sviridov, Amina Miftakhova, Tereshchenko Artemiy Vladimirovich, Galina Zubkova, Pavel Blinov, and\nAndrey Savchenko. 3mdbench: Medical multimodal multi-agent dialogue benchmark. In Proceedings of the 2025\nConference on Empirical Methods in Natural Language Processing, pages 26625–26665, 2025.\n[24] Xiaoran Xu and Ravi Sankar. Large language model agents for biomedicine: A comprehensive review of methods,\nevaluations, challenges, and future directions. Information, 16(10), 2025.\n[25] Xiaoquan Zhi, Hongke Zhao, Likang Wu, Chuang Zhao, and Hengshu Zhu. Reinventing Clinical Dialogue:\nAgentic Paradigms for LLM Enabled Healthcare Communication, December 2025. arXiv:2512.01453 [q-bio].\n[26] Daeun Kyung, Hyunseung Chung, Seongsu Bae, Jiho Kim, Jae Ho Sohn, Taerim Kim, Soo Kyung Kim, and\nEdward Choi. Patientsim: A persona-driven simulator for realistic doctor-patient interactions, 2025.\n[27] Ruiyi Wang, Stephanie Milani, Jamie C Chiu, Jiayin Zhi, Shaun M Eack, Travis Labrum, Samuel M Murphy,\nNev Jones, Kate Hardy, Hong Shen, et al. Patient-{\\Psi}: Using large language models to simulate patients for\ntraining mental health professionals. arXiv preprint arXiv:2405.19660, 2024.\n[28] David A Cook. Creating virtual patients using large language models: scalable, global, and low cost. Medical\nteacher, 47(1):40–42, 2025.\n[29] Siyang Liu, Bianca Brie, Wenda Li, Laura Biester, Andrew Lee, James Pennebaker, and Rada Mihalcea. Eeyore:\nRealistic depression simulation via expert-in-the-loop supervised and preference optimization. In Findings of the\nAssociation for Computational Linguistics: ACL 2025, pages 13750–13770, 2025.\n[30] Fnu Neha, Deepshikha Bhati, and Deepak Kumar Shukla. Retrieval-augmented generation (rag) in healthcare: A\ncomprehensive review. AI, 6(9), 2025.\n[31] Mohammad Almansoori, Komal Kumar, and Hisham Cholakkal. Self-evolving multi-agent simulations for\nrealistic clinical interactions, 2025.\n15\n"}, {"page": 16, "text": "Work In Progress\n[32] Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, and Jonathan H. Chen.\nMedagentbench: A realistic virtual ehr environment to benchmark medical llm agents, 2025.\n[33] Yinghao Zhu, Ziyi He, Haoran Hu, Xiaochen Zheng, Xichen Zhang, Zixiang Wang, Junyi Gao, Liantao Ma, and\nLequan Yu. Medagentboard: Benchmarking multi-agent collaboration with conventional methods for diverse\nmedical tasks, 2025.\n[34] Junfeng Lu and Yueyan Li. Dynamic affective memory management for personalized llm agents, 2025.\n[35] Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M Banda, Nikesh Kotecha,\nTimothy Keyes, Yifan Mai, Mert Oez, et al. Medhelm: Holistic evaluation of large language models for medical\ntasks. arXiv preprint arXiv:2505.23802, 2025.\n[36] Gwydion Williams, Samuel Rutunda, Floris Nzabakira, and Bilal A Mateen. Human evaluators vs. llm-as-a-judge:\nToward scalable, real-time evaluation of genai in global health. medRxiv, pages 2025–10, 2025.\n[37] Yunghwei Lai, Weizhi Ma, and Yang Liu. Patient-Zero: A Unified Framework for Real-Record-Free Patient\nAgent Generation, September 2025. arXiv:2509.11078 [cs].\n[38] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu,\nJianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui\nMen, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying\nXu, and Zipeng Zhang. Qwen2.5-1m technical report. arXiv preprint arXiv:2501.15383, 2025.\n[39] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.\n[40] OpenAI. Introducing GPT-5. https://openai.com/gpt-5/, 2025.\n[41] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel\nBlistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning,\nmultimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025.\n[42] DeepSeek-AI. Deepseek-v3 technical report, 2024.\nA\nSupplementary Figures and Tables\nFigure 8: An Example of the Key Rubric matching between Gemini-2.5-pro and Gpt-5\n16\n"}, {"page": 17, "text": "Work In Progress\nCategory\nDescription\nExample\nSymptom\nCharacterization\nSystematically describe the main\nsymptom, including onset and time\ncourse, location/radiation, quality,\nseverity, triggers/relievers, associated\nsymptoms, and response to any self-care.\n\"When did it start—sudden or gradual?\";\n\"Where exactly is it? Does it spread\nanywhere?\"; \"What does it feel like\n(sharp, dull, burning, cramping)? Rate\n0–10.\"; \"What makes it worse or better\n(activity, food, position, rest,\nmedication)?\"\nUrgency/Triage\nAssessment\nScreen for warning signs that suggest\nsevere disease or clinical instability, and\ndetermine whether urgent evaluation,\nemergency referral, or immediate testing\nis needed.\n\"Any chest tightness/pain, sweating,\nshortness of breath, palpitations, or pain\nradiating to the left arm/jaw?\"; \"Any\nconfusion, fainting, slurred speech,\nweakness/numbness, seizures?\"; \"High\nfever that won’t come down, chills/rigors,\npurple rash/petechiae?\"\nExploration of\nDifferential\nDiagnostics\nAsk discriminating questions that help\nconfirm or rule out the most likely and\nthe most dangerous alternative diagnoses\nrelevant to the presentation.\n(Abdominal pain) \"Is the pain localized?\nAny rebound pain or pain that moves?\nWorse after meals or on an empty\nstomach?\"; (Fever/cough) \"Any\nsputum—color/amount? Pleuritic chest\npain? Sick contacts? Vaccination\nstatus?\"; (Dizziness) \"Is it spinning vs\nlightheadedness on standing? Any\ntinnitus or hearing loss?\"\nMedical history &\nIndividual Risk\nContext\nReview medical background and risk\nfactors that influence diagnosis, treatment\nchoices, and medication safety\n(conditions, surgeries, allergies,\nmedications, family history, reproductive\nstatus when relevant).\n\"Any history of hypertension, diabetes,\ncoronary disease, asthma, liver/kidney\ndisease?\"; \"Any surgeries or\nhospitalizations? Recent tests or\nimaging?\"; \"Any medication/food\nallergies? Ever had severe reactions?\";\n\"What medications are you taking\n(dose/frequency)? Any recent\nstarts/stops?\"\nSocial and\nLifestyle Factors\nIdentify exposures and lifestyle factors\nthat change disease likelihood or affect\nprevention and management (occupation,\ntravel, contacts, substances, diet, sexual\nand environmental exposures as\nappropriate).\n\"What do you do for work? Any exposure\nto dust, chemicals, noise, radiation,\nanimals?\"; \"Any recent travel/camping?\nAny mosquito/tick bites?\"; \"Any close\ncontacts with fever/cough/diarrhea?\nRecent hospital/large gatherings?\"\nFunctional\nImpact\nAssess how symptoms affect daily life\nand functioning, and clarify the patient’s\nconcerns, expectations, and preferred\ncare goals to support shared decisions.\n\"How is this affecting work/school—have\nyou needed time off?\"; \"Any impact on\nsleep, appetite, energy? Trouble walking\nor climbing stairs?\"; \"What bothers you\nmost? What are you most worried\nabout?\"\nTable 4: Key-rubric categories with descriptions and examples.\n17\n"}]}