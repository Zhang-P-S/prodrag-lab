{"doc_id": "arxiv:2601.14698", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.14698.pdf", "meta": {"doc_id": "arxiv:2601.14698", "source": "arxiv", "arxiv_id": "2601.14698", "title": "ClaimDB: A Fact Verification Benchmark over Large Structured Data", "authors": ["Michael Theologitis", "Preetam Prabhu Srikar Dammu", "Chirag Shah", "Dan Suciu"], "published": "2026-01-21T06:19:47Z", "updated": "2026-01-21T06:19:47Z", "summary": "Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on \"reading\" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.14698v1", "url_pdf": "https://arxiv.org/pdf/2601.14698.pdf", "meta_path": "data/raw/arxiv/meta/2601.14698.json", "sha256": "e23c08cb6cdd305da9d587f885b85d69ba896b6012568404dec450b4d9f7ea44", "status": "ok", "fetched_at": "2026-02-18T02:20:53.702204+00:00"}, "pages": [{"page": 1, "text": "CLAIMDB: A Fact Verification Benchmark over Large Structured Data\nMichael Theologitis1, Preetam Prabhu Srikar Dammu1, Chirag Shah1, Dan Suciu1\n1University of Washington,\nSeattle, WA, USA\nmthe@cs.washington.edu\nAbstract\nDespite substantial progress in fact-verification\nbenchmarks, claims grounded in large-scale\nstructured data remain underexplored. In this\nwork, we introduce CLAIMDB, the first fact-\nverification benchmark where the evidence for\nclaims is derived from compositions of millions\nof records and multiple tables. CLAIMDB con-\nsists of 80 unique real-life databases covering\na wide range of domains, from governance and\nhealthcare to media, education and the natural\nsciences. At this scale, verification approaches\nthat rely on “reading” the evidence break down,\nforcing a timely shift toward reasoning in exe-\ncutable programs. We conduct extensive exper-\niments with 30 state-of-the-art proprietary and\nopen-source (below 70B) LLMs and find that\nnone exceed 83% accuracy, with more than half\nbelow 55%. Our analysis also reveals that both\nclosed- and open-source models struggle with\nabstention—the ability to admit that there is no\nevidence to decide—raising doubts about their\nreliability in high-stakes data analysis. We re-\nlease the benchmark, code, and the LLM leader-\nboard at https://claimdb.github.io.\n1\nIntroduction\nClaims based on large-scale structured data are ev-\nerywhere. They effectively drive and justify the\nmost important decisions of our times. For ex-\nample, Joe Biden stated on August 29, 2023 in\na speech at the White House that the U.S. infla-\ntion “is now down close to 3, the lowest among the\nworld’s leading economies.” (2023) at the time of\nthe most aggressive cycle of interest-rate hikes in\ndecades (PBS, 2022). Similarly, two years later, on\nAugust 11, 2025, Donald Trump stated that “Wash-\nington, D.C., has 41 homicides per 100,000 people,\nNo. 1 that we can find anywhere in the world.” (The\nWhite House, 2025) to justify the historic decision\nand deployment of the national guard at the U.S.\ncapital (announced later in the same speech).\nIn both cases, these claims are summaries of\nlarge, official, publicly available structured datasets.\nBiden’s inflation claim can be verified against con-\nsumer price indices (CPI) published by the Bureau\nof Labor Statistics in the form of approximately\nten separate Excel tables (U.S. BLS). On the other\nhand, Trump’s crime claim can be fact-checked\nagainst crime records released by the Metropolitan\nPolice Department of D.C. in the form of a single\nCSV file containing crime incidents (MPD, 2025).\nDespite the central role of structured data in real-\nworld decision-making, fact-verification research\nhas largely focused elsewhere. Prior work has\nmade important progress on evidence grounded\nin modalities ranging from text and documents to\nWikipedia tables, info-boxes, and small relational\ntables (e.g., Chen et al., 2020; Schlichtkrull et al.,\n2023). These settings have enabled significant ad-\nvances in fact-checking (e.g., Bazaga et al. 2024),\nbut they share a common simplifying assumption:\nevidence is small—in fact, it is small enough to fit\nwithin an LLM’s context window. As Biden and\nTrump’s examples showcase, this assumption does\nnot hold for many high-stakes, real-world claims.\nIn this work, we propose CLAIMDB, a fact-\nverification benchmark in which claims are\ngrounded in evidence deliberately composed from\nmultiple tables and millions of rows. Successful\nverification in CLAIMDB implicitly requires some\nform of executable reasoning (Chen et al., 2023) as\nthe evidence is too large to naively “read”, consume\nand combine. Each claim is paired with a unique\ndatabase (out of 80) that contains, on average, 11\ntables and 4.5M records, mirroring the scale and\ncomplexity of real-world fact-checking.\n2\nRelated Work\nFact verification started with free-form text. The\nseminal work of Thorne et al. (2018) on FEVER\npairs claims with Wikipedia sentences to de-\n1\narXiv:2601.14698v1  [cs.CL]  21 Jan 2026\n"}, {"page": 2, "text": "BIRD\nBenchmark\n10,962\n6,545\nFilter using SQL's AST\n. . .\n80 Unique DBs\nExecute SQL on DBs\nEntailed \nPrompt\nContr. \nPrompt\nNEI\nPrompt\nNL Ans.\nPanel of LLM Judges\nNL Ans.\nNL SQL\nNL SQL\nIntermediate\nQ/A Pairs\nClaim\nClaim\n13,737\nTotal Claims: 62,018\nEntailed\nClaim\nClaim\n19,450\nContr.\nClaim\nClaim\n28,831\nNEI\nRuling: \nKeep or Reject\nClaim\nClaim\n12,855\nTotal Claims: 53,368\nEntailed\nClaim\nClaim\n16,529\nContr.\nClaim\nClaim\n23,984\nNEI\nClaimDB: Final Benchmark\nFigure 1: Overview of the CLAIMDB construction pipeline. We start from the NL-to-SQL BIRD benchmark\n(Section 3.2), execute each query on its respective real-world database, and filter out low-information pairs using the\nquery AST (Sections 3.3, 3.4). For each remaining Q/A pair, we prompt gpt-5 to generate claims grounded in the\ngold answer—with some additional context for NEI claims (Section 3.5). We then use a panel of LLM judges from\nMistral AI, Microsoft, and xAI to retain only high-quality claims (Section 4.1). Finally, we apply embedding-based\npost-processing to two NEI categories for high-quality sampling (Section 4.2); this step is omitted for space reasons.\ntermine whether they are supported or refuted.\nLIAR (Wang, 2017) is a complementary bench-\nmark of political claims with fine-grained veracity\nlabels. Since then, fact-verification has evolved be-\nyond text to include structured and semi-structured\nevidence. FEVEROUS (Aly et al., 2021) extends\nFEVER with tables, while other works explore\nverification over time-series data (Strong and Vla-\nchos, 2025), Wikipedia info-boxes (Gupta et al.,\n2020), knowledge graphs (Dammu et al., 2024),\nfinancial reports (Zhao et al., 2024), and temporal\ndata (Barik et al., 2024, 2025).\nAs the evidence modality naturally evolves to\nreal-life, large structured sources like tables, the\nreasoning starts to become more symbolic. The\npopular TabFact benchmark (Chen et al., 2020) re-\nquires operations like aggregation (e.g., AVG, SUM,\nMAX) and comparison. Subsequent benchmarks,\nincluding SEM-TAB-FACTS (Wang et al., 2021),\nand SCITAB (Lu et al., 2023) further explored this\nsetting by grounding real-life claims in tables ex-\ntracted from complex scientific articles.\nHowever, existing fact-verification benchmarks\nrely on tables that are small enough to fit within an\nLLM’s context window. Evidence comes almost\nexclusively from Wikipedia or scientific articles,\nwhere tables are inherently small. In CLAIMDB we\nbreak this pattern by designing a benchmark where\nclaims are grounded in databases with millions of\nrecords. At this scale, verification can no longer\nrely on naively “reading” and then reasoning over\nin-context evidence.\n3\nCLAIMDB Benchmark\nIn CLAIMDB, the task is to determine whether\na given claim is 1 entailed, 2 contradicted, or\n19%\n17%\n13%\n13%\n12%\n6%\n6%\n6%\n5%\nDomains\nEconomy (10262)\nEntertainment (9094)\nSports (6862)\nEducation (6830)\nTechnology (6231)\nGastronomy (3277)\nHealth (3260)\nEnvironment (2994)\nTransportation (2438)\nGovernance (1823)\nLabor (297)\nClaim Distribution per Domain\nDomain\nSubdomains\nEconomy\nFinance, World Economies, Retail, Banking\nEntertainment\nMovies, Music, TV Shows, Games, Cartoons\nSports\nBasketball, Olympics, Hockey, F1, Soccer\nTechnology\nSoftware, IT, Blockchain, Vision\nEducation\nUniversity, Academia, Schools, Language, Books\nGastronomy\nFood, Restaurant\nHealth\nHealthcare, Medical, Biology, Chemistry\nEnvironment\nWeather, Geography\nTransportation\nTransit Systems, Airport\nGovernance\nCrime, Law\nLabor\nHuman Resources\nFigure 2: Claim distribution and taxonomy. We group\nthe 80 databases into 11 high-level domains (introduced\nin this work), each comprising multiple subdomains.\nThe subdomains are inherited from Li et al. (2023) with\na few minor modifications.\n3 has not-enough-info (NEI) with respect to the ev-\nidence in a database. Each claim is paired with ex-\nactly one large, multi-table database, which serves\nas the sole source of evidence for verification. The\nworkflow for creating the CLAIMDB benchmark\nis shown in Figure 1. In this section we describe\nthe first half of the workflow—the creation of all\nclaims (Section 4 covers the quality-control half).\n3.1\nOverview\nThe final CLAIMDB benchmark contains 53,368\nclaims paired with databases from the BIRD bench-\n2\n"}, {"page": 3, "text": "Property\nValue\nTotal Claims\n53,368\nLabel distribution\n# Entailed (E)\n12,855\n# Contradicted (C)\n16,529\n# Not Enough Info (NEI)\n23,984\nNEI subcategories\n# Out-of-Schema\n12,644\n# Counterfactual\n5,786\n# Subjective\n5,554\nClaim Context\nDatabases per Claim\n1\nAvg. Tables per Database\n11.6\nAvg. Records per Database\n4.6M\nTotal Databases (DBs)\n80\nTable 1: Summary statistics of CLAIMDB.\nmark (Section 3.2): 12,855 entailed, 16,529 con-\ntradicted, and 23,984 not-enough-info. Table 1\nreports a detailed breakdown of label distributions\nand dataset statistics.\nWhat makes CLAIMDB challenging is the scale\nof the underlying data. Each database contains, on\naverage, 11.6 tables and 4.6M records. At this scale,\ncommon approaches that rely on feeding tables di-\nrectly into an LLM (e.g., Wang et al., 2024c) are no\nlonger viable. Instead, successful systems must rely\non executable programs to carry out the heavy com-\npositional reasoning required by our benchmark—\nfor example, averaging or sorting over millions of\nrecords (Section 3.3). Promising solutions include\nbut are not limited to PoT (Chen et al., 2023) and\ntool-calling agents (Theologitis and Suciu, 2025).\nThe benchmark uses 80 distinct real-life\ndatabases from BIRD (Li et al., 2023), cover-\ning a wide range of domains and subdomains,\nfrom governance and healthcare to media, educa-\ntion, transportation, and the natural sciences. Fig-\nure 2 summarizes the domain taxonomy present in\nCLAIMDB (full details are in Appendix Table 5).\n3.2\nBIRD Benchmark\nOur starting point is the BIRD (Li et al., 2023)\nNL2SQL benchmark. Mapping NL to SQL has\na long history, dating back to early work on NL\ninterfaces (Warren and Pereira, 1982). More recent\nsystems include WikiSQL (Zhong et al., 2017),\nSpider (Yu et al., 2018), and KaggleDBQA (Lee\net al., 2021). BIRD is the first benchmark that\nconsists of realistic databases, with multiple, large\ntables (millions of rows), including “dirty” data\nsuch as null values and inconsistent formatting.\nBIRD provides us with 11k NL-to-SQL ex-\namples (the combined public train and test\nsplits). Each example is paired with the underlying\ndatabase and accompanied by external informa-\ntion specific to each example. For example, in the\ntoxicology database, a label marked with “+” in-\ndicates that a molecule is carcinogenic, while “−”\nindicates the opposite. Throughout all subsequent\ntransformations, we preserve any external info in-\ntact. Importantly for us, the NL-to-SQL pairs have\nbeen carefully curated and vetted by trained, inde-\npendent annotators, and the SQL queries are pro-\nduced via a double-blind annotation process with\nexperts in the loop.\n3.3\nPre-Filtering\nWe start from the 11k NL-to-SQL pairs provided\nby BIRD. As a first filtering step, we retain only\npairs where the SQL query combines substantial in-\nformation in order to answer the NL question. For\nexample, a query that involves a superlative (e.g.,\nMAX) or averaging (AVG) summarizes large portions\nof the database, collapsing millions of values into\none. This is highly desirable for us, because ver-\nifying claims derived from such pairs implicitly\nrequires compositional reasoning over the database.\nOn the other hand, a lookup query that returns a sin-\ngle record (e.g., “What is the name of the mayor of\nSeattle?”) is of no interest: a verifier could answer\nby simply inspecting and seeing the right data.\nCompositional Queries. To identify such cases,\nwe convert every SQL query into an abstract syntax\ntree (AST) and use it to identify the ones that con-\ntain computations over substantial parts of the data.\nMore specifically, a query is retained if and only if\nits AST contains at least one of the following:\n◦Orderings or superlatives, which introduce\ncomparisons over all data (e.g., ORDER BY).\n◦Aggregate Functions, such as AVG and SUM,\nwhich operate over large sets of records.\n◦Window functions, which involve complex\ninformation flow across partitions of the data.\n◦Multi-table joins, where three or more tables\nare combined for the final result.\nThe common theme is that answering the question\n(or, later, verifying the claim) requires combining\ninformation from large portions of the database,\nwhich will far exceed the context size of any LLM.\nFinally, we discard queries whose answers return\nmore than ten records. Answers are fed to an LLM\nduring claim generation (Section 3.5) so this pre-\n3\n"}, {"page": 4, "text": "vents overwhelming the model with bookkeeping.\nAfter these two filtering steps we end up with\nroughly 6.5k NL/SQL pairs, where each SQL query\n“touches” a large subset of its associated database.\n3.4\nQuestion-Answer Pairs over Databases\nNext, we execute each SQL query on its corre-\nsponding database to obtain the exact answer. This\nyields Question/Answer pairs grounded in real-\nworld databases. From this point on, the SQL itself\nis no longer needed and can be discarded.\nThe answer to each SQL query is a table, i.e.\na set of rows, which is not a friendly format for\nLLMs. Following Singha et al. (2023), we convert\neach answer from a table to JSON, a format that\nis friendlier for LLMs. After this step we are left\nwith 6.5k Question/JSON-Answer pairs.\n3.5\nClaim Generation\nEach pair serves as ground truth. Given one such\npair, we prompt gpt-5 to generate claims that are\neither entailed by the answer or contradicted by it\n(we discuss NEI claims later in this section). Fig-\nure 3 shows a concrete example of a single Q/A\npair together with the claims generated from it.\nDomain: California Schools\nQuestion: Which cities have the top 5 lowest enrollment\nnumber for students in grades 1 through 12?\nAnswer: Coulterville; Pinecrest; Shaver Lake; Emigrant\nGap; Hyampom.\nGenerated Claims:\n• Entailed: In California, the five cities with the low-\nest K-12 student enrollment are Coulterville, Pinecrest,\nShaver Lake, Emigrant Gap, and Hyampom.\n• Contradicted: The three California cities with the low-\nest K-12 enrollments are Lee Vining, Trona and Keeler.\n• NEI (Out-of-Schema): Most families in Hyampom\nchoose homeschooling rather than enrolling their chil-\ndren in public schools.\n• NEI (Counterfactual): If Pinecrest had opened a new\nK-12 campus in 2020, its grades 1-12 enrollment would\nhave increased enough that the city would not rank\namong the five lowest in California.\n• NEI (Subjective): Among Coulterville, Pinecrest, Emi-\ngrant Gap, and Hyampom, Emigrant Gap provides the\nmost nurturing learning environment for K–12 students.\nFigure 3: Example of claims generated from a single\nQ/A pair in CLAIMDB (California Schools database).\nIn the generation of NEI claims we also give the\ndatabase schema information along the golden context.\nFor this step we explored a range of prompting\nstrategies (Kotha et al., 2025), including different\ntask instructions, zero-shot versus few-shot, and\nvarying levels of CoT, while keeping track of fail-\nure modes—which we deal with in the next section.\nOur final setup uses a single prompt for each\nlabel (3 in total). All final prompts are available in\nFigures 12, 13 and 14 of the Appendix.\nFor both entailed and contradicted claims, we\nfound that 1-shot prompting with medium reason-\ning works best. Given a single Q/A pair, the model\ngenerates between one and three claims.\nNEI Claims.\nLastly, we generate claims that\nare definitely unanswerable from the database at\nhand. We draw inspiration from the taxonomy pre-\nsented by Kirichenko et al. (2025) and the work of\nAmayuelas et al. (2024). Given our specific setting,\nwe can safely support three claim categories.\nFirst, we generate claims that fall outside the\nconcepts of the database. To do this, we inspect the\ndatabase schema metadata offline—including table\nand column names, and relationships—and provide\nthe full schema information to gpt-5. The model\nhas no trouble using this effectively: for exam-\nple, in Figure 3, the generated out-of-schema claim\nrefers to homeschooling, which is not represented\nanywhere in the California Schools database.\nSecond, we generate subjective claims which ex-\npress opinions or value judgments that cannot be\nobjectively verified. Finally, we generate counter-\nfactual claims. These describe imagined or “what\nif” scenarios that are unanswerable with certainty.\nFor NEI claim generation, we found that zero-\nshot prompting with reasoning works best. Any\nform of in-context learning consistently hurt gener-\nation quality: gpt-5 is already capable of produc-\ning these claims reliably, and few-shot prompting\nmainly ends up constraining its imagination.\nFinally, we end up with roughly 14k, 19k, and\n28k entailed, contradicted, and NEI claims, respec-\ntively (Figure 1). The discrepancy in counts arises\nbecause contradicted claims are not constrained by\nthe answer itself (they can oppose it arbitrarily),\nwhile NEI claims are not constrained by either the\nanswer or even the concepts present in the Q/A pair\nor the database (we address this in Section 4.2).\n4\nQuality Control\nAutomatic claim generation is never perfect. Issues\ncan still slip-in such as claims that reference prior\ncontext—which is opaque to a verifier—or leak\nhelpful information. While outright label errors\nare rare (the claim generation task is trivial), we\ntake conservative steps to filter problematic claims.\n4\n"}, {"page": 5, "text": "This is the second half of the workflow illustrated\nin Figure 1.\n4.1\nPanel of LLM Judges\nManually annotating 64k examples is infeasible,\nso we rely on the now widely adopted LLM-as-a-\nJudge paradigm, where an LLM evaluator is used\nas a proxy for human annotators.\nJudge Panel. It is well-known that LLMs tend\nto prefer answers generated by themselves (Zheng\net al., 2023; Ye et al., 2025). To avoid this self-\nenhancement bias, we therefore exclude OpenAI\nmodels entirely. We further follow Verga et al.\n(2024), who show that replacing a single large eval-\nuator with a panel of judges of smaller LLMs leads\nto more stable results. So, we construct a judge\npanel from three different model families coming\nfrom Microsoft, xAI, and Mistral AI:\n(a) Phi-4 (Abdin et al., 2024)\n(b) grok-3-mini (xAI, 2025)\n(c) mistral-small (Mistral AI, 2025c)\nEach judge independently evaluates every claim,\nand we later combine their judgements for the fi-\nnal ruling—whether a claim is eliminated or not.\nImportantly, we also ask the judges to justify their\nreasoning and verdicts in a few sentences in NL.\nRubrics. Before putting the judge panel to work,\nwe must first clearly define the evaluation criteria.\nEach judge is given the full gold context—namely\nthe NL question, answer, and domain—along with\nthe generated claim and its assigned label. Judges\nthen answer the following binary (“yes” or “no”)\nquestions: 1 Is the label correct?, and 2 Is the\nclaim self-contained? The second question refers\nto claims where there are opaque references to pre-\nvious context (e.g., “the question above”, etc.).\nNEI claims require additional care. During gen-\neration, gpt-5 had access to the schema as part of\nthe gold context. As a result, schema artifacts oc-\ncasionally appear directly in the generated claims,\nwhich unintentionally helps systems evaluated on\nthe benchmark. For these claims, we therefore ask\ntwo additional questions: 3 Is there schema leak-\nage?, and 4 Is the assigned NEI category correct?\nRuling. If any judge flags a claim as problematic\nunder any rubric (e.g., incorrect label, lack of self-\ncontainment, etc.), the claim is discarded. In other\nwords, a single negative judgment is grounds for\nelimination. This is intentionally conservative as\nit gives us confidence that the final benchmark is\nclean—even if that means over-eliminating claims.\nPrompt. The evaluator prompt plays a central role\nin how reliable the judging process is (Ye et al.,\n2025). We already simplify the task as much as\npossible (e.g., binary classification rubrics), and\nwe make one more simplifying decision: we are\nwilling to “unfairly” discard borderline claims.\nWhile most LLM-as-a-Judge setups aim to max-\nimize agreement with human annotators (Thakur\net al., 2024), our goal is slightly different. We\nwant to maximize the recall on claims that humans\nwould eliminate. To that end, the prompt includes\ninstructions such as “If you are unsure, answer no.”\nTo calibrate the prompt, we manually annotated\n300 examples (150 NEI and 150 contradicted or\nentailed) using our rubrics. We shuffle and split\nthese annotated examples in two halves: one for\ncalibrating the prompt, and the other for testing it.\nPrompt Engineering. After calibration, we end\nup with the prompts shown in Figures 15 and 16 of\nthe Appendix. The former is used for entailed and\ncontradicted claims, while the latter for NEI claims\nand includes the two additional evaluation rubrics.\n\"yes\"\n\"yes\"\n\"yes\"\n\"yes\"\n0\n25\n50\n75\n100\nPerc. of Claims (%)\nLabel correct?\nSelf-contained?\nCategory correct? Schema leakage?\n\"NOT ENOUGH INFO\" claims only\nClaim Quality: Human Annotators vs. Panel of LLM-Judges\nPhD Annotators\nPanel of Judges\nFigure 4: Performance of the judge panel on the human-\nannotated test set (75 NEI and 75 C or E). The panel\nachieves 100% recall on the examples that the human an-\nnotators found problematic and behaves conservatively.\nPrompt Results. The judge panel achieves 100%\nrecall on the claims flagged by human annotators;\nthey are all eliminated by the panel as well. We\nalso did not observe any mislabeled claims. This\naligns with our experience during the construction\nof the benchmark—generating claims from correct\nQ/A pairs is an easy task, and gpt-5 is more than\ncapable of doing so reliably. Figure 4 shows the\nbehavior of the panel on the annotated test set.\nFinally, we run the full benchmark through the\njudge panel with all three models evaluated at tem-\nperature zero. Overall, 5.6% of claims are flagged\nfor incorrect labels and 1.7% for not being self-\ncontained. Among NEI claims, 11% show schema\nleakage and 5% have an incorrect category assign-\nment. Taken together, this filtering step removes\napproximately 14% of the benchmark (Figure 1).\n5\n"}, {"page": 6, "text": "4.2\nGrounding NEI Claims\nThe main reason we have a large number of NEI\nclaims is that they are inherently unconstrained by\nthe database concepts. They can be about almost\nanything, especially in the case of out-of-schema\nclaims. However, this freedom often makes them\nvery obvious to classify. The same issue arises with\ncounterfactual claims—given a bit of imagination\n(gpt-5 does not lack this quality), it is easy to go\noverboard. These overboard claims are not bad per\nse; we simply want a way to avoid sampling them\nwhen we construct our main benchmarking test set.\nEssentially, we are looking for a way to rank\nNL claims by how “close” they are to the concepts\nof the underlying data—we can approximate this\nusing the golden context. We capture this “close-\nness” using semantic textual similarity (STS). More\nspecifically, we embed1 the Q/A pair and the gen-\nerated claim, and compute the similarity between\nthem. The resulting score gives us a simple and ef-\nfective way to rank claims by how closely they stay\ngrounded in plausible concepts of the database.\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nSimilarity Score\n0\n1\n2\nNumber of Claims\n×103\nQ1: 0.81\nQ2: 0.84\nQ3: 0.86\nFigure 5: Distribution of smilarity scores between gen-\nerated claims and their gold context for out-of-schema\nclaims. Higher scores indicate that claims stay “closer”\nto the underlying data concepts. The two highlighted\nclaim examples (♣, ⋆) are discussed in Section 4.2.\nFigure 5 shows the distribution of similarity\nscores for out-of-schema claims. To make this con-\ncrete, we look at two claims generated from the\nsame Q/A in the Chicago Crime database, which\nasks about crimes in Chicago’s Central district. Al-\nthough both claims are out-of-schema, they fall into\nvery different parts of the similarity distribution:\n(♣) The commander of Chicago’s Cen-\ntral police district holds a law degree\nfrom an Illinois university.\n(⋆) Crimes recorded in Chicago’s Cen-\ntral district disproportionately involve\n1We use gemini-embedding-001 (Lee et al., 2025) for the\nembeddings; at this time, it ranks 4th on MTEB (2023).\ntourists compared to other districts.\nThe second claim is much closer to the database\nconcepts: validating it would require inspecting the\ndata to see whether victim information (e.g., regis-\ntered residents) is available, which is at least plausi-\nble. The first claim, in contrast, drifts far from what\nthe database could reasonably support. This differ-\nence is captured by the similarity score: the second\nclaim (⋆) falls in the top quartile (0.869), while\nthe first (♣) lies in the lowest quartile (0.798).\n5\nBenchmark Splits\nWe divide CLAIMDB into three disjoint splits:\n1 the training set, 2 a public test set, and 3 a\nprivate test set (the labels are hidden). Both test\nsets contain approximately 1,000 examples each.\nThey are sampled2 uniformly at random per label\nand fully balanced, with roughly 333 examples per\nlabel. For NEI claims, the three subcategories are\nalso balanced with roughly 111 examples each; for\nboth out-of-schema and counterfactual, sampling is\nfurther restricted to the top quartile of the similarity\nscore distribution (Figures 5 and 9).\n6\nEvaluation\n6.1\nSetup\nClaims in CLAIMDB are intentionally derived from\ninsights that combine information across millions\nof records through operations such as averaging,\ngrouping and sorting (Section 3.3). At this scale,\nverification cannot be done by “reading” tables and\nreasoning in-context: the databases contain 4.6M\nrecords on average—which is also far beyond any\nLLM’s context window. As a result, verifiers must\nreason in executable programs that handle the bulk\nof the compositional reasoning that is required.\nOur evaluation primarily focuses on tool-calling\nagents (Wang et al., 2024a; Mialon et al., 2024),\nwhich we view as a very strong and practical solu-\ntion for this setting. We use Google’s MCP toolbox\nfor databases (Buvaraghan and Egan, 2025) and\ngive each agent a single tool: the ability to execute\narbitrary SQL queries against the database associ-\nated with the claim at hand. The only constraint\nwe impose is a generous limit of 20 tool calls; ex-\nceeding this counts as a failed run.\nThere also exist other setups such as coding\nagents (Wang et al., 2024b) that interact with data\n2The test sets are drawn from claims coming from the dev\nsplit of BIRD as it has received substantial cleanup effort in\nprior work (e.g., Liu et al., 2025; Wretblad et al., 2024).\n6\n"}, {"page": 7, "text": "via Python and pandas. However, prior work has\nshown such approaches to be brittle for the smaller\nLLMs (below 70B) we consider in this paper (Sun\net al., 2024). We therefore leave a systematic study\nof potential alternatives—from other tool choices\nto different paradigms altogether—for future work.\nWe use a single prompt (Figure 17 in the Ap-\npendix) and we conduct extensive experiments with\n30 state-of-the-art proprietary and open-source\nLLMs. Information and hyperparameters (e.g., de-\ncoding) are illustrated in Table 4 of the Appendix.\n6.2\nMetrics\nCLAIMDB is a three-way classification benchmark\nwith labels entailed (E), contradicted (C), and not-\nenough-info (NEI). For each label, we compute\nprecision, recall, and F1 by treating that label as the\npositive class and the remaining two as negatives.\nPrecision measures the proportion of predictions\nfor a given label that are correct, while recall mea-\nsures the fraction of true instances of that label that\nare correctly identified. F1 is their harmonic mean.\nWe report all metrics, but we primarily focus on\naccuracy and Macro-F1 = 1\n3(F1E + F1C + F1NEI).\n6.3\nDiscussion\nWe analyze results on the public test set. Results on\nthe private test set are reported in Appendix A.2.\n5\n10\n15\n20\n25\n30\nNumber of Parameters (B)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nqwq\nqwen3\nqwen3\nqwen3\nqwen3\nqwen3\nqwen3-coder\nnemotron-3-nano\nmistral-small\nmistral-nemo\nministral-3\nministral-3\nministral-3\nmagistral\nllama3.2\nllama3.1\ngpt-oss\ndevstral\ndevstral-small-2\ncogito\ncogito\nScaling Trend\nAlibaba\nNVIDIA\nMistral AI\nMeta\nOpenAI\nDeep Cogito\nLog-Linear Fit\nFigure 6: Final accuracy versus model size.\nScaling Trends. Figure 6 plots accuracy against\nmodel size. We also see the familiar pattern of\nlarger models performing better, but with improve-\nments that are marginal and roughly log-linear.\nOpen-Source Models Struggle. Table 2 reports\nresults for all 30 models. Across both accuracy and\nmacro-F1, gpt-5-mini performs best, followed by\nclaude-haiku-4.5 and gemini-3-flash. Out of\nall evaluated models, more than half (17 out of 30)\nstay below 55% accuracy and Macro-F1.\nOpen-source models generally struggle, with the\nexception of gpt-oss-20b which performs on par\nwith top proprietary models. Aside from gpt-oss,\nno other open-source model (out of the 20 remain-\ning) exceeds 68% accuracy or macro-F1. Overall,\nthis highlights that there is room for improvement\nfor open-source models—to catch up—in reason-\ning over large-scale data. The CLAIMDB bench-\nmark is a clear step toward this goal.\nLong sessions hurt. Long LLM-database interac-\ntions have inherent fragility: as they grow longer,\nmodels increasingly lose focus due to the large\namounts of information. In this setting, a single\nbad decision—a single careless query—can flood\nthe model with hundreds of thousands of values.\nFigure 7 plots, for each of the 30 models, the\naverage number of tool calls over the 1,000 test\nexamples. We fit a second-order polynomial and\nobserve that the top-performing models typically\naverage roughly 4–8 tool calls. Longer (or shorter)\nthan that leads to degraded performance. The com-\nplete tool-call distributions are shown in Figure 11.\n0.0\n2.5\n5.0\n7.5\n10.0\nAverage Number of Tool Calls\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nOpenAI\nGoogle\nAnthropic\nAlibaba\nMistral AI\nNVIDIA\nMeta\nDeep Cogito\nPoly (deg=2)\nFigure 7: Average number of tool calls per model on the\npublic test set of CLAIMDB. Performance generally\ndegrades as LLM-database interactions become longer.\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.754\n0.234\n0.012\n0.030\n0.952\n0.018\n0.077\n0.146\n0.777\ngpt-5-mini\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.796\n0.204\n0.000\n0.033\n0.955\n0.012\n0.124\n0.182\n0.694\nclaude-haiku-4-5\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.393\n0.159\n0.447\n0.079\n0.459\n0.462\n0.065\n0.068\n0.866\nqwen3:32b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.656\n0.121\n0.224\n0.126\n0.612\n0.262\n0.139\n0.105\n0.756\nnemotron-3-nano\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted Label\nGround Truth Label\nFigure 8: Confusion matrices (normalized) for represen-\ntative proprietary and open-source models. The primary\ndifference lies in how models handle NEI: proprietary\nmodels are biased against abstention, while open-source\nmodels predict it excessively (full results in Figure 10).\n7\n"}, {"page": 8, "text": "LLM Backbone\nPrecision\nRecall\nF1\nMacro-F1\nAcc.\nTotal\nTokens\nEnt.\nContr. NEI\nEnt.\nContr. NEI\nEnt.\nContr.\nNEI\ngpt-4o-mini (2024)\n0.777 0.666 0.679\n0.577 0.656 0.107\n0.662 0.661 0.185\n0.503\n0.445\n27M\ngpt-4.1-nano (2025a)\n0.647 0.641 0.376\n0.165 0.124 0.952\n0.263 0.208 0.539\n0.337\n0.416\n13M\ngpt-5-nano (2025b)\n0.815 0.710 0.874\n0.742 0.900 0.720\n0.777 0.794 0.790\n0.787\n0.787\n24M\ngpt-5-mini (2025b)\n0.875 0.713 0.963\n0.754 0.952 0.777\n0.810 0.815 0.860\n0.828\n0.827\n14M\ngpt-oss:20b (2025)\n0.851 0.735 0.676\n0.669 0.688 0.862\n0.749 0.710 0.758\n0.739\n0.740\n14M\ngemini-2.5-flash (2025)\n0.841 0.734 0.830\n0.685 0.825 0.869\n0.755 0.777 0.849\n0.793\n0.793\n6M\ngemini-3-flash (2025)\n0.754 0.750 0.962\n0.799 0.934 0.673\n0.776 0.832 0.792\n0.800\n0.801\n10M\nclaude-3-haiku (2024)\n0.572 0.556 0.433\n0.261 0.435 0.756\n0.359 0.488 0.550\n0.466\n0.485\n9M\nclaude-3-5-haiku (2024)\n0.775 0.626 0.661\n0.580 0.743 0.702\n0.663 0.680 0.681\n0.675\n0.675\n11M\nclaude-haiku-4-5 (2025)\n0.836 0.711 0.983\n0.796 0.952 0.682\n0.815 0.814 0.805\n0.811\n0.809\n38M\nqwen3:1.7b (2025)\n0.645 0.533 0.351\n0.060 0.048 0.982\n0.110 0.089 0.518\n0.239\n0.366\n2M\nqwen3:4b (2025)\n0.530 0.554 0.492\n0.450 0.218 0.860\n0.487 0.312 0.626\n0.475\n0.511\n11M\nqwen3:8b (2025)\n0.785 0.617 0.450\n0.306 0.311 0.940\n0.441 0.414 0.608\n0.488\n0.521\n10M\nqwen3:14b (2025)\n0.812 0.744 0.438\n0.285 0.360 0.943\n0.422 0.485 0.599\n0.502\n0.531\n9M\nqwen3:32b (2025)\n0.732 0.667 0.491\n0.393 0.459 0.866\n0.512 0.544 0.626\n0.561\n0.574\n10M\nqwen3-coder:30b (2025)\n0.744 0.625 0.660\n0.646 0.659 0.711\n0.691 0.641 0.685\n0.672\n0.672\n22M\nqwq:32b (2025)\n0.500 0.558 0.472\n0.336 0.508 0.667\n0.402 0.532 0.552\n0.495\n0.504\n9M\nmistral-nemo:12b (2024)\n0.380 0.446 0.381\n0.381 0.211 0.577\n0.381 0.287 0.459\n0.376\n0.391\n8M\nministral-3:3b (2025b)\n0.511 0.500 0.390\n0.276 0.163 0.821\n0.359 0.246 0.529\n0.378\n0.422\n16M\nministral-3:8b (2025b)\n0.633 0.557 0.484\n0.408 0.396 0.792\n0.496 0.463 0.600\n0.520\n0.533\n17M\nministral-3:14b (2025b)\n0.612 0.638 0.624\n0.598 0.580 0.690\n0.605 0.608 0.655\n0.623\n0.623\n22M\nmistral-small:22b (2025c) 0.423 0.526 0.375\n0.222 0.060 0.878\n0.291 0.108 0.525\n0.308\n0.389\n16M\nmagistral:24b (2025a)\n0.682 0.714 0.419\n0.309 0.257 0.905\n0.426 0.378 0.573\n0.459\n0.492\n18M\ndevstral:24b (2025a)\n0.415 0.624 0.399\n0.291 0.221 0.771\n0.342 0.326 0.526\n0.398\n0.429\n19M\ndevstral-small-2 (2025b)\n0.602 0.712 0.521\n0.610 0.447 0.705\n0.606 0.549 0.599\n0.585\n0.588\n15M\nnemotron-3-nano (2025)\n0.714 0.726 0.612\n0.652 0.601 0.747\n0.681 0.658 0.673\n0.671\n0.667\n29M\nllama3.1:8b (2024)\n0.341 0.433 0.337\n0.222 0.079 0.726\n0.269 0.133 0.461\n0.288\n0.344\n7M\nllama3.2:3b (2024)\n0.428 0.412 0.366\n0.372 0.100 0.685\n0.398 0.161 0.477\n0.345\n0.387\n8M\ncogito:14b (2025)\n0.758 0.701 0.384\n0.141 0.184 0.973\n0.238 0.292 0.551\n0.360\n0.435\n8M\ncogito:32b (2025)\n0.797 0.564 0.506\n0.354 0.556 0.792\n0.491 0.560 0.617\n0.556\n0.568\n11M\nTable 2: Results on the public test set. We report per-label precision, recall, and F1, along with macro-F1 and\naccuracy. We also report the total number of tokens (input + output) across the 1,000 experiments for each model.\nVery Expensive. To meaningfully fact-check a\nclaim, a verifier must truly understand the data at\nits disposal. This inevitably requires consuming—\neither intentionally or unintentionally—a lot of in-\nformation. As shown in Table 2, models consume\nand produce on the order of tens of millions of to-\nkens over the full test set. For example, running\nclaude-haiku-4-5 (the cheapest model in the 4.5\nseries) cost us $43 for the public test split alone.\nThis highlights the need to balance performance\nwith cost in real-life settings like CLAIMDB.\nNEI is poorly handled. We observe that both\nclosed- and open-source models exhibit opposite\nbut degenerate behaviors with respect to the NEI\nlabel. Figure 8 shows normalized confusion matri-\nces for two top-performing proprietary models and\ntwo strong open-source alternatives.\nBoth gpt-5-mini and claude-haiku-4.5 are\nstrongly biased against predicting NEI. When the\nground-truth label is entailed or contradicted, they\nalmost never predict NEI (close to 0%), indicat-\ning a strong, inherent reluctance. Even when the\ncorrect label is NEI, they hedge heavily, spreading\nprobability mass across the other two labels.\nBoth qwen3 and nemotron-3-nano display the\nopposite behavior: they over-predict NEI. When\nthe ground truth is entailed or contradicted, NEI is\npredicted roughly half the time for qwen3, with a\nmilder but similar tendency for nemotron-3-nano.\n7\nConclusion\nIn this work, we introduce CLAIMDB, a fact-\nverification benchmark over large-scale structured\ndata. It contains 80 unique databases, each with\nan average of 11 tables and 4.5M records. Each\nclaim is grounded in evidence deliberately com-\nposed of millions of records across multiple tables.\nCLAIMDB is a clear step toward real-world claim\nverification—mirroring the various challenges of\nhigh-stakes verification in our modern world.\n8\n"}, {"page": 9, "text": "8\nLimitations\nReliance on BIRD. Our benchmark is derived from\nNL-to-SQL pairs in the BIRD benchmark (Sec-\ntion 3.2). As a result, any annotation error in BIRD\ndirectly propagates to CLAIMDB. For example, if\nthe NL question is “How old was the oldest mayor\nof Seattle when elected?” but the associated SQL\nquery instead computes the answer for Portland,\nany claims generated from this pair would be in-\ncorrectly labeled. To mitigate this risk for public\nevaluation and model development, we construct\nour test splits exclusively from claims derived\nfrom the dev split of BIRD, which has undergone\nsubstantial subsequent cleanup by prior work (Sec-\ntion 5). Importantly, our dependency on BIRD is\nnot irreversible: as additional errors in BIRD are\nidentified, the corresponding derived claims can be\nremoved from CLAIMDB on-the-fly.\nSingle Evidence Modality. CLAIMDB deliber-\nately focuses on structured data as its primary\nevidence source (which may include numerical,\ncategorical, temporal, and short textual fields).\nWhile real-world fact verification often involves\nmulti-modal evidence—such as free-form text, re-\nports, news articles, charts, and other unstructured\nsources—structured data remains a critical yet\ncomparatively underexplored modality in existing\nbenchmarks. By isolating this setting, CLAIMDB\nenables targeted evaluation of reasoning over com-\nplex structured evidence. We acknowledge that\nthis focus alone does not capture the full breadth\nof real-world verification, and view extension to\nmulti-modal evidence as a natural direction for fu-\nture work.\nSnapshot Validity and Evaluation Scope. En-\ntailment, contradiction, and NEI are assessed with\nrespect to the fixed database snapshot used to con-\nstruct the benchmark.\nBecause the underlying\ndatabases come from established public dataset\nreleases—and are therefore static and potentially\ndated—answers may differ if a system incorporates\nnewer external information. For example, a query\nabout the “bottom-5 enrollment schools” could\nchange, even if the claim was correct in the snap-\nshot used for claim generation. As with many ML\nbenchmarks, evaluation should therefore be inter-\npreted relative to the provided database state (Li\net al., 2023) rather than a live, continuously updated\nworld.\nEvaluation Solely on SQL. Our evaluation uses\nagents equipped with a single tool: the ability to\nexecute SQL queries. Performance therefore de-\npends not only on a model’s reasoning ability, but\nalso on its familiarity with SQL. One of the rea-\nsons we choose SQL is because, as a language, it\nmakes the required data operations—such as filter-\ning, grouping, aggregation, and ordering—explicit,\nisolating the core reasoning challenge. But, some\nmodels may perform worse than others simply be-\ncause they are worse at writing SQL (e.g., due to\ntheir pre-training data).\n9\nEthical Considerations\nData Licensing. CLAIMDB bases all claims on\nNL-to-SQL pairs coming from the BIRD bench-\nmark which is released under CC BY-SA 4.0 li-\ncense. CLAIMDB is released under the same li-\ncense, in accordance with the original terms.\nPrivacy.\nWe do not anonymize any part\nof CLAIMDB. CLAIMDB includes contradicted\nclaims, which are synthetically generated by per-\nturbing facts and are explicitly labeled as false\nwithin the benchmark. These claims are not in-\ntended to introduce new real-world information\nabout entities, but solely to support controlled eval-\nuation of real-life fact-verification systems.\n10\nAcknowledgments\nThis work was supported by NSF III 2507117, NSF\nSHF 2312195, and NSF IIS 2314527. We also\nthank David Alexander for an insightful discussion\non benchmark performance.\nReferences\nMarah I Abdin, Jyoti Aneja, Harkirat S. Behl, Sébastien\nBubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J. Hewett, Mojan Javaheripi, Piero\nKauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li,\nWeishung Liu, Caio C. T. Mendes, Anh Nguyen,\nEric Price, Gustavo de Rosa, Olli Saarikivi, and\n8 others. 2024.\nPhi-4 technical report.\nCoRR,\nabs/2412.08905.\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. FEVEROUS: fact extraction and verifi-\ncation over unstructured and structured information.\nIn Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\n9\n"}, {"page": 10, "text": "Alfonso Amayuelas, Kyle Wong, Liangming Pan,\nWenhu Chen, and William Yang Wang. 2024. Knowl-\nedge of knowledge: Exploring known-unknowns un-\ncertainty with large language models. In Findings of\nthe Association for Computational Linguistics, ACL\n2024, Bangkok, Thailand and virtual meeting, Au-\ngust 11-16, 2024, pages 6416–6432. Association for\nComputational Linguistics.\nAnthropic. 2024. The Claude 3 Model Family: Opus,\nSonnet, Haiku.\nAnthropic. 2025. Introducing Claude Haiku 4.5.\nAnab Maulana Barik, Wynne Hsu, and Mong-Li Lee.\n2024.\nTime matters: An end-to-end solution for\ntemporal claim verification. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing: EMNLP 2024 - Industry Track,\nMiami, Florida, USA, November 12-16, 2024, pages\n657–664. Association for Computational Linguistics.\nAnab Maulana Barik, Wynne Hsu, and Mong-Li Lee.\n2025. Chronofact: Timeline-based temporal fact ver-\nification. In Proceedings of the Thirty-Fourth Inter-\nnational Joint Conference on Artificial Intelligence,\nIJCAI 2025, Montreal, Canada, August 16-22, 2025,\npages 8031–8039. ijcai.org.\nAdrián Bazaga, Pietro Lio, and Gos Micklem. 2024.\nUnsupervised pretraining for fact verification by\nlanguage model distillation. In The Twelfth Inter-\nnational Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net.\nHamsa Buvaraghan and Derek Egan. 2025. MCP Tool-\nbox for Databases: Simplify AI Agent Access to\nEnterprise Data.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2023.\nProgram of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks.\nTrans. Mach.\nLearn. Res., 2023.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact verification. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPreetam Prabhu Srikar Dammu, Himanshu Naidu,\nMouly Dewan, YoungMin Kim, Tanya Roosta, Aman\nChadha, and Chirag Shah. 2024. Claimver: Explain-\nable claim-level verification and evidence attribution\nof text through knowledge graphs. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 13613–13627. Association for Computational\nLinguistics.\nDeep Cogito. 2025. Cogito v1 Preview Introducing IDA\nas a path to general superintelligence.\nTulsee Doshi. 2025. Gemini 3 Flash: frontier intelli-\ngence built for speed. Google Blog.\nSaibo Geng, Martin Josifoski, Maxime Peyrard, and\nRobert West. 2023. Grammar-constrained decod-\ning for structured NLP tasks without finetuning. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2023, Singapore, December 6-10, 2023, pages 10932–\n10952. Association for Computational Linguistics.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\ntra, Archie Sravankumar, Artem Korenev, Arthur\nHinsvark, and 542 others. 2024. The llama 3 herd of\nmodels. Preprint, arXiv:2407.21783.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: inference on tables as\nsemi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 2309–2324. Association for Computational\nLinguistics.\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, Alek-\nsander Madry, Alex Baker-Whitcomb, Alex Beutel,\nAlex Borzunov, Alex Carney, Alex Chow, Alex Kir-\nillov, Alex Nichol, Alex Paino, and 79 others. 2024.\nGpt-4o system card. CoRR, abs/2410.21276.\nPolina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri,\nand Samuel J. Bell. 2025. Abstentionbench: Rea-\nsoning llms fail on unanswerable questions. CoRR,\nabs/2506.09038.\nAnoop Kotha, Julian Lee, and Eric Zakariasson. 2025.\nGpt-5 prompting guide.\nChia-Hsuan Lee, Oleksandr Polozov, and Matthew\nRichardson. 2021. Kaggledbqa: Realistic evalua-\ntion of text-to-sql parsers. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021, pages 2261–2273. Association for\nComputational Linguistics.\nJinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel\nCer, Madhuri Shanbhogue, Iftekhar Naim, Gus-\ntavo Hernández Ábrego, Zhe Li, Kaifeng Chen, Hen-\nrique Schechter Vera, Xiaoqi Ren, Shanfeng Zhang,\nDaniel Salz, Michael Boratko, Jay Han, Blair Chen,\nShuo Huang, Vikram Rao, Paul Suganthan, and 28\nothers. 2025. Gemini embedding: Generalizable em-\nbeddings from gemini. CoRR, abs/2503.07891.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li,\nBowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,\n10\n"}, {"page": 11, "text": "Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang\nLi, Kevin Chen-Chuan Chang, Fei Huang, Reynold\nCheng, and Yongbin Li. 2023. Can LLM already\nserve as A database interface? A big bench for large-\nscale database grounded text-to-sqls. In Advances in\nNeural Information Processing Systems 36: Annual\nConference on Neural Information Processing Sys-\ntems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023.\nXinyu Liu, Shuyu Shen, Boyan Li, Nan Tang, and Yuyu\nLuo. 2025. Nl2sql-bugs: A benchmark for detect-\ning semantic errors in NL2SQL translation. In Pro-\nceedings of the 31st ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, V.2, KDD\n2025, Toronto ON, Canada, August 3-7, 2025, pages\n5662–5673. ACM.\nXinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov,\nand Min-Yen Kan. 2023. SCITAB: A challenging\nbenchmark for compositional reasoning and claim\nverification on scientific tables. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2023, Singapore, De-\ncember 6-10, 2023, pages 7787–7813. Association\nfor Computational Linguistics.\nGrégoire Mialon, Clémentine Fourrier, Thomas Wolf,\nYann LeCun, and Thomas Scialom. 2024. GAIA: a\nbenchmark for general AI assistants. In The Twelfth\nInternational Conference on Learning Representa-\ntions, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net.\nMistral AI. 2024. Mistral NeMo.\nMistral AI. 2025a. Devstral.\nMistral AI. 2025b. Introducing Mistral 3.\nMistral AI. 2025c. Mistral Small 3.1.\nMPD. 2025. Crime Incidents 2025. Accessed: 2026-\n01-01.\nNiklas Muennighoff, Nouamane Tazi, Loïc Magne, and\nNils Reimers. 2023. MTEB: massive text embedding\nbenchmark. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, EACL 2023, Dubrovnik, Croatia,\nMay 2-6, 2023, pages 2006–2029. Association for\nComputational Linguistics.\nNVIDIA, :, Aaron Blakeman, Aaron Grattafiori,\nAarti Basant,\nAbhibha Gupta,\nAbhinav Khat-\ntar, Adi Renduchintala, Aditya Vavre, Akanksha\nShukla, Akhiad Bercovich, Aleksander Ficek, Alek-\nsandr Shaposhnikov, Alex Kondratenko, Alexan-\nder Bukharin, Alexandre Milesi, Ali Taghibakhshi,\nAlisa Liu, Amelia Barton, and 340 others. 2025.\nNvidia nemotron 3: Efficient and open intelligence.\nPreprint, arXiv:2512.20856.\nOpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason\nAi, Sam Altman, Andy Applebaum, Edwin Arbus,\nRahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao,\nBoaz Barak, Ally Bennett, Tyler Bertao, Nivedita\nBrett, Eugene Brevdo, Greg Brockman, Sebastien\nBubeck, and 108 others. 2025. gpt-oss-120b & gpt-\noss-20b model card. Preprint, arXiv:2508.10925.\nOpenAI. 2025a. Introducing GPT-4.1 in the API.\nOpenAI. 2025b. Introducing GPT-5.\nPBS. 2022. Analysis: What the Feds largest interest\nrate hike in decades means for you.\nQwen Team. 2025. QwQ-32B: Embracing the Power of\nReinforcement Learning.\nAbhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle\nBerrada, Guillaume Lample, Jason Rute, Joep Bar-\nmentlo, Karmesh Yadav, Kartik Khandelwal, Khy-\nathi Raghavi Chandu, Léonard Blier, Lucile Saulnier,\nMatthieu Dinot, Maxime Darrin, Neha Gupta, Ro-\nman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan\nWang, and 80 others. 2025a.\nMagistral.\nCoRR,\nabs/2506.10910.\nAbhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexan-\nder H. Liu, Alexandre Sablayrolles, Amélie Héliou,\nAmélie Martin, Anmol Agarwal, Andy Ehrenberg,\nAndy Lo, Antoine Roux, Arthur Darcet, Arthur Men-\nsch, Baptiste Bout, Baptiste Rozière, Baudouin De\nMonicault, Chris Bamford, Christian Wallenwein,\nChristophe Renaudin, and 84 others. 2025b. Devs-\ntral: Fine-tuning language models for coding agent\napplications. Preprint, arXiv:2509.25193.\nMichael Sejr Schlichtkrull, Zhijiang Guo, and Andreas\nVlachos. 2023. Averitec: A dataset for real-world\nclaim verification with evidence from the web. In\nAdvances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Pro-\ncessing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023.\nAnanya Singha, José Cambronero, Sumit Gulwani,\nVu Le, and Chris Parnin. 2023. Tabular represen-\ntation, noisy operators, and impacts on table struc-\nture understanding tasks in LLMs. In NeurIPS 2023\nSecond Table Representation Learning Workshop.\nMarek Strong and Andreas Vlachos. 2025. TSVer: A\nbenchmark for fact verification against time-series\nevidence. In Proceedings of the 2025 Conference on\nEmpirical Methods in Natural Language Processing,\npages 29894–29914, Suzhou, China. Association for\nComputational Linguistics.\nZhihong Sun, Chen Lyu, Bolun Li, Yao Wan, Hongyu\nZhang, Ge Li, and Zhi Jin. 2024. Enhancing code\ngeneration performance of smaller models by distill-\ning the reasoning ability of llms. In Proceedings of\nthe 2024 Joint International Conference on Computa-\ntional Linguistics, Language Resources and Evalua-\ntion, LREC/COLING 2024, 20-25 May, 2024, Torino,\nItaly, pages 5878–5895. ELRA and ICCL.\n11\n"}, {"page": 12, "text": "Gemini Team. 2025. Gemini 2.5: Pushing the frontier\nwith advanced reasoning, multimodality, long con-\ntext, and next generation agentic capabilities. CoRR,\nabs/2507.06261.\nAman Singh Thakur, Kartik Choudhary, Venkat Srinik\nRamayapally, Sankaran Vaidyanathan, and Dieuwke\nHupkes. 2024. Judging the judges: Evaluating align-\nment and vulnerabilities in llms-as-judges. CoRR,\nabs/2406.12624.\nThe White House. 2025. President Trump Holds a Press\nConference, Aug. 11, 2025.\nMichael Theologitis and Dan Suciu. 2025.\nThucy:\nAn LLM-based Multi-Agent System for Claim Ver-\nification across Relational Databases.\nPreprint,\narXiv:2512.03278.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction\nand verification.\nIn Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2018, New\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\n1 (Long Papers), pages 809–819. Association for\nComputational Linguistics.\nU.S. BLS. Consumer Price Index (CPI) Databases. Ac-\ncessed: 2026-01-01.\nPat Verga, Sebastian Hofstätter, Sophia Althammer, Yix-\nuan Su, Aleksandra Piktus, Arkady Arkhangorod-\nsky, Minjie Xu, Naomi White, and Patrick Lewis.\n2024. Replacing judges with juries: Evaluating LLM\ngenerations with a panel of diverse models. CoRR,\nabs/2404.18796.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2024a. Voyager: An open-ended\nembodied agent with large language models. Trans.\nMach. Learn. Res., 2024.\nNancy Xin Ru Wang, Diwakar Mahajan, Marina\nDanilevsky, and Sara Rosenthal. 2021. Semeval-2021\ntask 9: Fact verification and evidence finding for tabu-\nlar data in scientific documents (SEM-TAB-FACTS).\nIn Proceedings of the 15th International Workshop on\nSemantic Evaluation, SemEval@ACL/IJCNLP 2021,\nVirtual Event / Bangkok, Thailand, August 5-6, 2021,\npages 317–326. Association for Computational Lin-\nguistics.\nWilliam Yang Wang. 2017. “Liar, Liar Pants on Fire”:\nA New Benchmark Dataset for Fake News Detection.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers), pages 422–426, Vancouver, Canada.\nAssociation for Computational Linguistics.\nXingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang,\nYunzhu Li, Hao Peng, and Heng Ji. 2024b. Exe-\ncutable code actions elicit better LLM agents. In\nForty-first International Conference on Machine\nLearning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net.\nZilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin\nEisenschlos, Vincent Perot, Zifeng Wang, Lesly Mi-\nculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee,\nand Tomas Pfister. 2024c. Chain-of-table: Evolving\ntables in the reasoning chain for table understanding.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nDavid H. D. Warren and Fernando C. N. Pereira. 1982.\nAn efficient easily adaptable system for interpreting\nnatural language queries. Am. J. Comput. Linguistics,\n8(3-4):110–122.\nWRAL. 2023. Fact check: Biden says U.S. inflation\nrate ’lowest’ of leading economies.\nNiklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas,\nAmin Ahmadi, and Oskar Holmström. 2024. Under-\nstanding the effects of noise in text-to-sql: An exami-\nnation of the bird-bench benchmark. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics, ACL 2024 - Short Papers,\nBangkok, Thailand, August 11-16, 2024, pages 356–\n369. Association for Computational Linguistics.\nxAI. 2025. Grok 3 Beta — The Age of Reasoning\nAgents.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 40\nothers. 2025.\nQwen3 technical report.\nCoRR,\nabs/2505.09388.\nJiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen,\nQihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer,\nChao Huang, Pin-Yu Chen, Nitesh V. Chawla, and\nXiangliang Zhang. 2025. Justice or prejudice? quan-\ntifying biases in llm-as-a-judge. In The Thirteenth In-\nternational Conference on Learning Representations,\nICLR 2025, Singapore, April 24-28, 2025. OpenRe-\nview.net.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, Zilin Zhang, and\nDragomir R. Radev. 2018. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain\nsemantic parsing and text-to-sql task. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 3911–3921.\nAssociation for Computational Linguistics.\nYilun Zhao, Yitao Long, Tintin Jiang, Chengye Wang,\nWeiyuan Chen, Hongjun Liu, Xiangru Tang, Yiming\nZhang, Chen Zhao, and Arman Cohan. 2024. Find-\nver: Explainable claim verification over long and\nhybrid-content financial documents. In Proceedings\n12\n"}, {"page": 13, "text": "of the 2024 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2024, Miami, FL,\nUSA, November 12-16, 2024, pages 14739–14752.\nAssociation for Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena. In\nAdvances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Pro-\ncessing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017.\nSeq2sql:\nGenerating structured queries\nfrom natural language using reinforcement learning.\nCoRR, abs/1709.00103.\nA\nEvaluation\nA.1\nEvaluation Setup\nTable 4 illustrates the hyperparameters of the eval-\nuation setup.\nA.2\nPrivate Test Set\nThe experimental results on the private test set\nare shown in Table 3. To reduce the risk of test-\nset exploitation and benchmark gaming, we report\nonly aggregate metrics (Macro-F1 and accuracy),\nomitting per-label and fine-grained analyses.\nA.3\nStructured Outputs\nA few years ago, getting structured outputs—\nresponses that adhere to strict user-defined JSON\nschemas—back from LLMs was a painful process.\nMaking it work would typically require careful\nprompting and repeated retries to get outputs in the\nright format. At the same time, this feature was\nbecoming increasingly important for any serious\napplication that relied on LLMs.\nThankfully, the situation has improved substan-\ntially. Most major AI providers (e.g., OpenAI, An-\nthropic, Google) now support some form of struc-\ntured outputs. There are multiple ways to imple-\nment this, but one of the most reliable is grammar-\nconstrained decoding (Geng et al., 2023). In this\napproach, the JSON schema is compiled into a\ncontext-free grammar, and during decoding the\nmodel’s logits are masked so that only grammar-\nvalid tokens can be generated.\nConcretely, this means that when a JSON\nschema is provided, the model is guaranteed to\nproduce a syntactically valid response. For ex-\nample, if we request an enum over three possible\nLLM Backbone\nMacro-F1\nAcc.\nTokens\ngpt-5-nano (2025b)\n0.791\n0.793\n20M\ngpt-5-mini (2025b)\n0.828\n0.828\n19M\ngpt-oss:20b (2025)\n0.759\n0.763\n14M\ngemini-2.5-flash (2025)\n0.761\n0.758\n8M\ngemini-3-flash (2025)\n0.807\n0.805\n12M\nclaude-3-5-haiku (2024)\n0.671\n0.673\n11M\nclaude-haiku-4-5 (2025)\n0.799\n0.792\n35M\nqwen3:1.7b (2025)\n0.231\n0.363\n2M\nqwen3:4b (2025)\n0.461\n0.493\n11M\nqwen3:8b (2025)\n0.493\n0.529\n10M\nqwen3:14b (2025)\n0.465\n0.506\n9M\nqwen3:32b (2025)\n0.539\n0.558\n9M\nqwen3-coder:30b (2025)\n0.685\n0.686\n22M\nqwq:32b (2025)\n0.498\n0.513\n8M\nmistral-nemo:12b (2024)\n0.345\n0.354\n8M\nministral-3:3b (2025b)\n0.332\n0.369\n15M\nministral-3:8b (2025b)\n0.543\n0.556\n16M\nministral-3:14b (2025b)\n0.618\n0.618\n23M\nmistral-small:22b (2025c)\n0.296\n0.379\n15M\nmagistral:24b (2025a)\n0.430\n0.468\n18M\ndevstral:24b (2025a)\n0.388\n0.420\n-\ndevstral-small-2 (2025b)\n0.572\n0.576\n16M\nnemotron-3-nano (2025)\n0.660\n0.656\n30M\nllama3.1:8b (2024)\n0.301\n0.354\n7M\nllama3.2:3b (2024)\n0.317\n0.358\n8M\ncogito:14b (2025)\n0.384\n0.450\n8M\ncogito:32b (2025)\n0.548\n0.567\n11M\nTable 3: Private test set results. We report macro-F1\nand accuracy. We also report the total number of tokens\n(input + output) across the 1,000 experiments for each\nmodel. Per-label statistics are omitted to preserve the\nintegrity of the private test set and evaluation.\nverdicts (entailed, contradicted, NEI), the output is\nguaranteed to be one of these values. This greatly\nsimplifies evaluation and removes an entire class\nof failure modes unrelated to model reasoning.\nThis level of control is only possible because\nmodel providers have direct access to their own\ndecoding process. As a result, most proprietary\nAPIs expose structured output functionality di-\nrectly. However, when using agentic frameworks\n(e.g., OpenAI Agent SDK, Pydantic AI, Google\nADK, etc.) that connect to third-party or open-\nsource models through adapters such as LiteLLM,\nthis guarantee often breaks down.\nFor example, some leading agentic frameworks\nsimulate structured outputs by injecting a prompt\nthat forces the model to call a predefined tool for its\nfinal answer3 ,4, rather than enforcing the schema\nat decoding time when the GCD option is available\n3https://github.com/pydantic/pydantic-ai/issues/242\n4https://github.com/openai/openai-agents-python/issues/\n1778#issuecomment-3316092585\n13\n"}, {"page": 14, "text": "Model\nContext\nWindow temp. topK topP\ngpt-4o-mini\n128K\n1.0\n–\n1.0\ngpt-4.1-nano\n1M\n1.0\n–\n1.0\ngpt-5-nano\n400K\n1.0\n–\n1.0\ngpt-5-mini\n400K\n1.0\n–\n1.0\ngpt-oss:20b\n128K\n1.0\n–\n1.0\ngemini-2.5-flash\n1M\n1.0\n64\n0.95\ngemini-3-flash\n1M\n1.0\n64\n0.95\nclaude-3-haiku\n200K\n1.0\n–\n–\nclaude-3-5-haiku\n200K\n1.0\n–\n–\nclaude-haiku-4-5\n200K\n1.0\n–\n–\nqwen3:1.7b\n40K\n0.6\n20\n0.95\nqwen3:4b\n246K\n0.6\n20\n0.95\nqwen3:8b\n40K\n0.6\n20\n0.95\nqwen3:14b\n40K\n0.6\n20\n0.95\nqwen3:32b\n40K\n0.6\n20\n0.95\nqwen3-coder:30b\n256K\n0.7\n20\n0.8\nqwq:32b\n40K\n1\n40\n0.95\nmistral-nemo:12b\n1M\n0.0\n–\n–\nministral-3:3b\n256K\n0.15\n–\n–\nministral-3:8b\n256K\n0.15\n–\n–\nministral-3:14b\n256K\n0.15\n–\n–\nmistral-small:22b\n128K\n0.0\n–\n–\nmagistral:24b\n39K\n0.7\n–\n0.95\ndevstral:24b\n128K\n0.0\n–\n–\ndevstral-small-2:24b\n384K\n0.15\n–\n–\nnemotron-3-nano\n1M\n1.0\n–\n1.0\nllama3.2:3b\n128K\n0.0\n–\n–\nllama3.1:8b\n128K\n0.0\n–\n–\ncogito:14b\n128K\n0.0\n–\n–\ncogito:32b\n128K\n0.0\n–\n–\nTable 4: Extra information and hyperparameters. Un-\navailable or N/A values are shown as “–”. Numbers also\ncome from HuggingFace and Ollama.\n(e.g., on open-source models). Aside from bloat-\ning the model context, this approach is inherently\nbrittle: the LLM can simply ignore the instruction.\nThis creates an evaluation asymmetry. Models\nwithout guaranteed structured decoding may fail\ndue to formatting errors even when their underlying\nprediction is correct, while models with GCD will\nalways produce a valid label (which implies a 33%\naccuracy floor). Since our goal is not to benchmark\nhow well models, vendors and agentic frameworks\nsupport structured outputs, but rather how well can\nmodels reason over large data, directly penalizing\nsuch failures would be unfair.\nOur policy is therefore simple.\nFor models\nwhere structured outputs are not guaranteed—\ntypically due to limitations of the agentic frame-\nwork we use—we re-run the test whenever a re-\nsponse violates the expected schema. This occurs\nin only a small fraction of cases and is usually re-\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nSimilarity Score\n0.0\n0.5\n1.0\n1.5\n2.0\nNumber of Claims\n×103\nOut-of-Schema\nQ1: 0.81\nQ2: 0.84\nQ3: 0.86\n0.70\n0.75\n0.80\n0.85\n0.90\nSimilarity Score\nCounterfactual\nQ1: 0.81\nQ2: 0.83\nQ3: 0.84\nFigure 9: Distribution of similarity scores between gen-\nerated claims and their gold context for both counter-\nfactual and out-of-schema claims. These claims can\n“drift” from the database concepts so we embed them\nand measure semantic similarity between them and the\nalso embedded golden context. Higher scores indicate\nthat claims stay “closer” to the underlying data con-\ncepts. The two highlighted claim examples (♣, ⋆) are\ndiscussed in Section 4.2.\nsolved with one or two re-runs.\nB\nSimilarity Distributions\nWe provide the counterfactual and out-of-schema\nsimilarity distributions (Section 4.2) in Figure 9.\nC\nPrompts\nIn Figures 12, 13 and 14 we showcase the prompts\nused in creating the claims (Section 3.5). In Fig-\nures 15 and 16 we illustrate the two prompts used\nfor the judging process—one for contradicted and\nentailed claims and one for NEI since the latter has\nextra rubrics and extra golden context (Section 4.1).\nLastly, in Figure 17, we show the single, optimized\nverifier prompt for all models in our evaluations.\n14\n"}, {"page": 15, "text": "Ent.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.708\n0.255\n0.037\n0.108\n0.865\n0.028\n0.269\n0.385\n0.346\ngpt-4o-mini\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.165\n0.045\n0.790\n0.066\n0.124\n0.810\n0.024\n0.024\n0.952\ngpt-4.1-nano\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.742\n0.204\n0.054\n0.048\n0.900\n0.051\n0.119\n0.161\n0.720\ngpt-5-nano\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.754\n0.234\n0.012\n0.030\n0.952\n0.018\n0.077\n0.146\n0.777\ngpt-5-mini\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.670\n0.177\n0.153\n0.048\n0.689\n0.263\n0.071\n0.068\n0.860\ngpt-oss:20b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.691\n0.239\n0.070\n0.061\n0.827\n0.112\n0.069\n0.060\n0.872\ngemini-2.5-flash\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.799\n0.180\n0.021\n0.060\n0.934\n0.006\n0.199\n0.128\n0.673\ngemini-3-flash-preview\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.262\n0.214\n0.524\n0.085\n0.435\n0.480\n0.110\n0.131\n0.758\nclaude-3-haiku-20240307\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.580\n0.246\n0.174\n0.066\n0.743\n0.190\n0.101\n0.194\n0.704\nclaude-3-5-haiku-20241022\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.796\n0.204\n0.000\n0.033\n0.955\n0.012\n0.124\n0.182\n0.694\nclaude-haiku-4-5\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.060\n0.033\n0.907\n0.024\n0.048\n0.927\n0.009\n0.009\n0.982\nqwen3:1.7b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.450\n0.108\n0.441\n0.326\n0.218\n0.456\n0.074\n0.065\n0.860\nqwen3:4b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.306\n0.156\n0.538\n0.060\n0.311\n0.628\n0.024\n0.036\n0.940\nqwen3:8b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.285\n0.093\n0.622\n0.039\n0.360\n0.601\n0.027\n0.030\n0.943\nqwen3:14b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.393\n0.159\n0.447\n0.079\n0.459\n0.462\n0.065\n0.068\n0.866\nqwen3:32b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.646\n0.222\n0.132\n0.103\n0.659\n0.239\n0.119\n0.170\n0.711\nqwen3-coder:30b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.278\n0.097\n0.625\n0.152\n0.164\n0.684\n0.113\n0.065\n0.821\nministral-3:3b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.408\n0.198\n0.393\n0.142\n0.396\n0.462\n0.095\n0.113\n0.792\nministral-3:8b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.598\n0.207\n0.195\n0.188\n0.584\n0.228\n0.190\n0.119\n0.690\nministral-3:14b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.381\n0.135\n0.483\n0.323\n0.211\n0.465\n0.298\n0.125\n0.577\nmistral-nemo:12b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.222\n0.033\n0.745\n0.202\n0.060\n0.737\n0.101\n0.021\n0.878\nmistral-small:22b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.310\n0.084\n0.605\n0.070\n0.258\n0.672\n0.075\n0.018\n0.907\nmagistral:24b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.291\n0.093\n0.616\n0.221\n0.221\n0.559\n0.190\n0.039\n0.771\ndevstral:24b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.610\n0.108\n0.282\n0.178\n0.447\n0.375\n0.223\n0.071\n0.705\ndevstral-small-2:24b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.656\n0.121\n0.224\n0.126\n0.612\n0.262\n0.139\n0.105\n0.756\nnemotron-3-nano\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.222\n0.072\n0.706\n0.184\n0.079\n0.737\n0.244\n0.030\n0.726\nllama3.1:8b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.372\n0.054\n0.574\n0.270\n0.100\n0.630\n0.229\n0.086\n0.685\nllama3.2:3b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.141\n0.069\n0.790\n0.027\n0.184\n0.789\n0.018\n0.009\n0.973\ncogito:14b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.354\n0.264\n0.381\n0.042\n0.556\n0.402\n0.048\n0.161\n0.792\ncogito:32b\nEnt.\nContr.\nNEI\nEnt.\nContr.\nNEI\n0.336\n0.264\n0.399\n0.136\n0.508\n0.356\n0.199\n0.134\n0.667\nqwq:32b\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted Label\nGround Truth Label\nFigure 10: Public test set. Confusion matrices (normalized) for all models. One takeaway is that the primary\ndifference in top-performing open- and closed-source models lies in how they handle NEI: proprietary models are\nbiased against abstention, while open-source models predict it excessively.\n15\n"}, {"page": 16, "text": "0\n5\n10\n15\n20\n0\n50\ngpt-4o-mini\n=8.2\n0\n5\n10\n15\n20\n0\n200\n400\ngpt-4.1-nano\n=3.6\n0\n5\n10\n15\n20\n0\n100\ngpt-5-nano\n=6.3\n0\n5\n10\n15\n20\n0\n100\ngpt-5-mini\n=5.2\n0\n5\n10\n15\n20\n0\n50\n100\ngpt-oss:20b\n=5.7\n0\n5\n10\n15\n20\n0\n100\n200\ngemini-2.5-flash\n=3.7\n0\n5\n10\n15\n20\n0\n100\n200\ngemini-3-flash-preview\n=4.5\n0\n5\n10\n15\n20\n0\n100\n200\nclaude-3-haiku-20240307\n=4.2\n0\n5\n10\n15\n20\n0\n100\n200\nclaude-3-5-haiku-20241022\n=4.8\n0\n5\n10\n15\n20\n0\n50\n100\nclaude-haiku-4-5\n=8.2\n0\n5\n10\n15\n20\n0\n200\n400\nqwen3:1.7b\n=0.7\n0\n5\n10\n15\n20\n0\n200\n400\nqwen3:4b\n=1.3\n0\n5\n10\n15\n20\n0\n100\n200\nqwen3:8b\n=3.1\n0\n5\n10\n15\n20\n0\n100\n200\nqwen3:14b\n=2.9\n0\n5\n10\n15\n20\n0\n100\n200\nqwen3:32b\n=2.9\n0\n5\n10\n15\n20\n0\n50\n100\nqwen3-coder:30b\n=8.6\n0\n5\n10\n15\n20\n0\n50\nministral-3:3b\n=10.8\n0\n5\n10\n15\n20\n0\n50\n100\nministral-3:8b\n=8.6\n0\n5\n10\n15\n20\n0\n50\n100\nministral-3:14b\n=11.8\n0\n5\n10\n15\n20\n0\n200\nmistral-nemo:12b\n=3.3\n0\n5\n10\n15\n20\n0\n500\nmistral-small:22b\n=0.1\n0\n5\n10\n15\n20\n0\n100\nmagistral:24b\n=4.5\n0\n5\n10\n15\n20\n0\n50\n100\ndevstral:24b\n=4.8\n0\n5\n10\n15\n20\n0\n50\n100\ndevstral-small-2:24b\n=8.7\n0\n5\n10\n15\n20\n0\n50\n100\nnemotron-3-nano\n=8.0\n0\n5\n10\n15\n20\n0\n200\nllama3.1:8b\n=3.0\n0\n5\n10\n15\n20\n0\n100\n200\nllama3.2:3b\n=3.5\n0\n5\n10\n15\n20\n0\n100\ncogito:14b\n=5.1\n0\n5\n10\n15\n20\n0\n100\ncogito:32b\n=6.1\n0\n5\n10\n15\n20\n0\n100\n200\nqwq:32b\n=2.0\nNumber of Tool Calls\nCount\nFigure 11: Public test set. Distribution of the number of tool calls for each model on the 1,000 examples of the\npublic test set of CLAIMDB. The best-performing model is gpt-5-mini (0.83 acc.) which gives us insight on\nhow the distribution of tools calls should probably look like for a model to do well on our benchmark. Some models\noften decide not to use a tool call at all, which is obviously a losing strategy as they have no idea of the underlying\ndata environment to answer correctly. We try to avoid this by “forcing” the model to inspect the data first, by\nincluding instructions like “Use the available tools to query the database and gather evidence before making a\ndecision” and “You should always start by querying the database for the schema” (Figure 17) which are ignored.\n16\n"}, {"page": 17, "text": "from pydantic import BaseModel, Field\nclass ContradictedClaim(BaseModel):\ncontradicted_claim: str = Field(\n...,\ndescription=(\n\"A contradicted claim.\"\n)\n)\nclass ContradictedClaimCollection(BaseModel):\ncollection: list[ContradictedClaim]\n## Role\nYou are a **misleading** spokesperson\n**in a controlled evaluation\nsetting**.\n## Task\nGiven the following inputs:\n- A question\n- Its correct answer\n- The data domain\n- Optional external knowledge (clarifications)\nYour task is to produce natural language claims that are factually\nincompatible with the provided answer. In other words, any reader who\nknows the correct answer would judge your claim to be false.\n## Requirements\n- Each claim must be self-contained and must not use opaque references\nto earlier context (e.g., \"the answer,\" \"the question,\" \"the earlier\nclaim\", etc.). Instead, any needed context should be stated explicitly\nwithin each claim.\n- Each claim must contradict or be factually incompatible with the\nanswer, directly or indirectly.\n- Do not restate or explain the external knowledge; assume it is\nalready known to the reader.\n- Produce between 1 and 3 claims.\n## Example\n### Input\n{\n\"question\": \"Which three districts recorded the highest graduation\nrates in 2022?\",\n\"answer\": [\n{\n\"DistrictName\": \"Redwood Coast Unified\",\n\"GradRate\": 0.97\n},\n{\n\"DistrictName\": \"Sierra Vista Union\",\n\"GradRate\": 0.96\n},\n{\n\"DistrictName\": \"Mission Creek Unified\",\n\"GradRate\": 0.95\n}\n],\n\"domain\": \"California Schools\",\n\"external-knowledge\": \"GradRate = Number of graduates / Total number\nof eligible seniors\"\n}\n### Output\nRedwood Coast Unified did not lead California's graduation rankings in\n2022 — it was Riverbend Joint Unified that posted the top rate.\nSierra Vista Union is no longer among the highest graduation-rate\ndistricts in 2022.\nFairmont Hills Unified surpassed Redwood Coast Unified with 98% of its\neligible seniors graduating in 2022, according to data in CA.\nFigure 12: Prompt (1-shot) for generating contradicted\nclaims. It is highly optimized for gpt-5 specifically,\nby following the work of Kotha et al. (2025). The out-\nput schema is provided here as python code as we do\nnot know how OpenAI encodes it in the prompt inter-\nnally. Effectively, the model returns a JSON object that\nadheres to ContradictedClaimCollection, i.e., a col-\nlection of contradicted claims. The number of generated\nclaims usually stays between 1-3 as per the instructions.\nfrom pydantic import BaseModel, Field\nclass EntailedClaim(BaseModel):\nentailed_claim: str = Field(\n...,\ndescription=(\n\"An entailed claim.\"\n)\n)\nclass EntailedClaimCollection(BaseModel):\ncollection: list[EntailedClaim]\n## Role\nYou are an **honest** spokesperson **in a controlled evaluation\nsetting**.\n## Task\nGiven the following inputs:\n- A question\n- Its correct answer\n- The data domain\n- Optional external knowledge (clarifications)\nYour task is to produce natural language claims that are consistent\nwith and supported by the provided answer. In other words, any reader\nwho knows the correct answer would judge your claim to be true.\n## Requirements\n- Each claim must be self-contained and must not use opaque references\nto earlier context (e.g., \"the answer,\" \"the question,\" \"the earlier\nclaim\", etc.). Instead, any needed context should be stated explicitly\nwithin each claim.\n- Each claim must follow from or be fully supported by the answer,\ndirectly or indirectly.\n- Do not restate or explain the external knowledge; assume it is\nalready known to the reader.\n- Produce between 1 and 3 claims.\n## Example\n### Input\n{\n\"question\": \"Which three districts recorded the highest graduation\nrates in 2022?\",\n\"answer\": [\n{\n\"DistrictName\": \"Redwood Coast Unified\",\n\"GradRate\": 0.97\n},\n{\n\"DistrictName\": \"Sierra Vista Union\",\n\"GradRate\": 0.96\n},\n{\n\"DistrictName\": \"Mission Creek Unified\",\n\"GradRate\": 0.95\n}\n],\n\"domain\": \"California Schools\",\n\"external-knowledge\": \"GradRate = Number of graduates / Total number\nof eligible seniors\"\n}\n### Output\nRedwood Coast Unified led California's graduation rankings in 2022 with\na 97% rate.\nIn 2022, California's strongest graduation results came from Redwood\nCoast Unified, which saw 97% of its eligible seniors finish high school.\nSierra Vista Union and Mission Creek Unified followed closely, with\ngraduation rates of 96% and 95%, respectively.\nMission Creek Unified achieved a graduation rate of 95% in 2022,\nplacing it among California's top three districts. It ranked just\nbehind Redwood Coast Unified and Sierra Vista Union.\nFigure 13: Prompt (1-shot) for generating entailed\nclaims. It is highly optimized for gpt-5 specifically,\nby following the work of Kotha et al. (2025). The out-\nput schema is provided here as python code as we do not\nknow how OpenAI encodes it in the prompt internally.\nEffectively, the model returns a JSON object that ad-\nheres to EntailedClaimCollection, i.e., a collection\nof entailed claims. The number of generated claims\nusually stays between 1-3 as per the instructions.\n17\n"}, {"page": 18, "text": "from pydantic import BaseModel, Field\nfrom typing import Literal\nclass NoInfoClaim(BaseModel):\nno_info_claim: str = Field(\n...,\ndescription=\"A NOT ENOUGH INFO claim.\"\n)\ncategory: Literal[\n\"Out-of-Schema\", \"Subjective\", \"Counterfactual\"\n] = Field(\n...,\ndescription=\"The category of the NOT ENOUGH INFO claim.\"\n)\nclass NoInfoClaimCollection(BaseModel):\ncollection: list[NoInfoClaim]\n## Role\nYou are a neutral spokespearson **in a controlled evaluation setting**.\n## Task\nGiven the following inputs:\n- A question\n- Its correct answer\n- The data domain\n- The schema of the database\n- Optional external knowledge (clarifications)\nYour task is to produce natural language claims whose truth **cannot**\nbe determined from the database or the given Q/A. That is, even with\nfull access to both the database and the correct answer, these claims\ncannot be definitively verified or falsified.\n## Requirements\n- Each claim must be self-contained and must not use opaque references\nto earlier context (e.g., \"the answer,\" \"the question,\" \"the earlier\nclaim\", etc.). Instead, any needed context should be stated explicitly\nwithin each claim.\n- Each claim must *not* be entailed or contradicted by the answer,\ndirectly or indirectly.\n- Each claim must fall into at least one of these categories:\n1. **Out-of-schema** — involves concepts the database doesn't store\nor represent anywhere in its schema.\n2. **Subjective/evaluative** — expresses opinions or judgments that\ncannot be objectively verified.\n3. **Counterfactual/hypothetical** — describes an imagined or \"what\nif\" situation that is not reflected in the actual data.\n- Produce between 1 and 5 claims.\n- Do not restate or explain the external knowledge; assume it is\nalready known to the reader.\nFigure 14: Prompt (zero-shot) for generating NEI\nclaims. It is highly optimized for gpt-5 specifically,\nby following the work of (Kotha et al., 2025). The out-\nput schema is provided here as python code as we do\nnot know how OpenAI encodes it in the prompt inter-\nnally. Effectively, the model returns a JSON object that\nadheres to NoInfoClaimCollection, i.e., a collection\nof NEI claims. The number of generated claims usually\nstays between 1-5 as per the instructions.\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nclass ClaimQuality(BaseModel):\nlabel_correct: Literal[\"yes\", \"no\"] = Field(\n...,\ndescription=(\n'Is the assigned label of the claim (ENTAILED/CONTRADICTED) '\n'correct given the gold information? Answer \"yes\" if the '\n'label of the claim follows from the gold information; \"no\" '\n'otherwise. If you are unsure, answer \"no\".'\n)\n)\nfree_of_meta_references: Literal[\"yes\", \"no\"] = Field(\n...,\ndescription=(\n'Does the claim avoid meta-references to the question, '\n'answer, or prior text (e.g., \"this question\", '\n'\"the answer above\", \"as mentioned earlier\")? Answer '\n'\"yes\" if it is completely free of meta-references; '\n'\"no\" otherwise. References to provided external '\n'knowledge do not count as meta-references.'\n)\n)\nreasoning: str = Field(\n...,\ndescription=(\n'Brief explanation (1-2 sentences) justifying your '\n'evaluation (especially for \"label_correct\").'\n)\n)\nYour task is to evaluate a natural-language claim across two criteria.\nYou will be given a gold context composed of a question, its answer,\nthe domain, and optional external knowledge. Treat the gold context as\nthe authoritative ground truth.\nYou will be given a claim labeled as either ENTAILED (supported by the\ngold context) or CONTRADICTED (refuted by the gold context). Using this\ngold information, assess whether the claim is correctly labeled, and\nwhether it is free of meta-references. More detailed instructions for\nthe two evaluation criteria are provided along with the JSON schema\nbelow.\nYour answer should be in JSON format, adhering to the following schema:\n<SCHEMA GENERATED HERE>\nFigure 15: Judge prompt used for contradicted and en-\ntailed claims. The prompt consists of two parts: a Py-\ndantic schema that specifies the required JSON output\n(label correctness, self-containment, and a short justi-\nfication), and a natural-language instruction block that\nexplains the judging task and the available gold context.\nThe Python block defines the output schema, which is\ninjected into the prompt at runtime in the placeholder\n<SCHEMA GENERATED HERE>.\n18\n"}, {"page": 19, "text": "from pydantic import BaseModel, Field\nfrom typing import Literal\nclass NEIClaimQuality(BaseModel):\nlabel_correct: Literal[\"yes\", \"no\"] = Field(\n...,\ndescription=(\n'Is the assigned label of the claim (ENTAILED/CONTRADICTED) '\n'correct given the gold information? Answer \"yes\" if the '\n'label of the claim follows from the gold information; \"no\" '\n'otherwise. If you are unsure, answer \"no\".'\n)\n)\nfree_of_meta_references: Literal[\"yes\", \"no\"] = Field(\n...,\ndescription=(\n'Does the claim avoid meta-references to the question, '\n'answer, or prior text (e.g., \"this question\", '\n'\"the answer above\", \"as mentioned earlier\")? Answer '\n'\"yes\" if it is completely free of meta-references; '\n'\"no\" otherwise. References to provided external '\n'knowledge do not count as meta-references.'\n)\n)\ncategory_correct: Literal[\"yes\", \"no\"] = Field(\n...,\ndescription=(\n'Is the assigned category of the claim '\n'(OUT-OF-SCHEMA/COUNTERFACTUAL/SUBJECTIVE) correct given '\n'the gold information? Answer \"yes\" if the category of the '\n'claim follows from the gold information; \"no\" otherwise.'\n)\n)\nschema_leakage: Literal[\"yes\", \"no\"] = Field(\n...,\ndescription=(\n'Does the claim expose database schema details or '\n'technical artifacts? Answer \"yes\" if it exposes '\n'schema details (e.g., table names, column names, etc.); '\n'\"no\" if it does not.'\n)\n)\nreasoning: str = Field(\n...,\ndescription=(\n'Brief explanation (1-2 sentences) justifying your '\n'evaluation (especially for \"label_correct\" and '\n'\"category_correct\").'\n)\n)\nYour task is to evaluate a natural-language claim across several\ncriteria. You will be given a gold context composed of a question, its\nanswer, the domain, optional external knowledge, and the complete\ndatabase schema underlying the gold context. Treat the gold context as\nthe authoritative ground truth.\nYou will be given a claim with an assigned NOT ENOUGH INFO (NEI) label,\nmeaning that its truth cannot be determined from the gold context, even\nwith full access to the database. The claim is also assigned an NEI\ncategory: OUT-OF-SCHEMA (depends on information not stored in the\ndatabase), SUBJECTIVE (expresses opinions or judgments), or\nCOUNTERFACTUAL (describes hypothetical scenarios). Using the gold\ncontext, assess whether the NEI label and category are correct, whether\nthe claim is free of meta-references, and whether it leaks schema\ndetails of the database. More detailed descriptions for each evaluation\ncriterion are provided in the JSON schema below.\nYour answer should be in JSON format, adhering to the following schema:\n<SCHEMA GENERATED HERE>\nFigure 16: Judge prompt used for NEI claims. Com-\npared to the prompt for entailed and contradicted claims,\nthis prompt includes additional evaluation criteria spe-\ncific to NEI cases. In addition to label correctness and\nself-containment, judges assess whether the assigned\nNEI category is correct and whether the claim leaks\ndatabase schema details. The instruction block also\ngives judges access to the full database schema. The\nPython block defines the output schema, which is in-\njected into the prompt at runtime in the placeholder\n<SCHEMA GENERATED HERE>.\nYou are a fact-checking assistant operating over structured data. You\nwill be given a natural-language claim and optional external\ninformation. You will have access to a SQLite database and may execute\narbitrary SQL queries over it using specialized tools.\nYour task is to determine whether the claim is \"ENTAILED\",\n\"CONTRADICTED\", or \"NOT ENOUGH INFO\" based on evidence you obtain from\nthe database. The labels are defined as follows:\n- ENTAILED: The claim is supported by the database.\n- CONTRADICTED: The claim is refuted by the database.\n- NOT ENOUGH INFO: The database does not provide sufficient evidence to\ndecide.\nUse the available tools to query the database and gather evidence\nbefore making a decision. Do not ask the user for clarification or\nadditional information.\nYou should always start by querying the database for the schema (tables\nand columns).\nYour answer should be in JSON format, adhering to the following schema:\n{\n\"properties\": {\n\"verdict\": {\n\"description\": \"Whether the claim is supported, contradicted, or\nundecidable from the database.\",\n\"enum\": [ \"ENTAILED\", \"CONTRADICTED\", \"NOT ENOUGH INFO\"],\n\"title\": \"Verdict\",\n\"type\": \"string\"\n},\n\"justification\": {\n\"description\": \"Brief justification (1-2 sentences) of the\nverdict.\",\n\"title\": \"Justification\",\n\"type\": \"string\"\n}\n},\n\"required\": [\"verdict\", \"justification\"],\n\"title\": \"ClaimVerdict\",\n\"type\": \"object\"\n}\nOutput Example 1:\n{\n\"verdict\": \"ENTAILED\",\n\"justification\": \"The database shows that the population of France\nis 67 million, which supports the claim.\"\n}\nOutput Example 2:\n{\n\"verdict\": \"CONTRADICTED\",\n\"justification\": \"The database indicates that the capital of\nGermany is Berlin, contradicting the claim.\"\n}\nOutput Example 3:\n{\n\"verdict\": \"NOT ENOUGH INFO\",\n\"justification\": \"The database does not contain any information\nabout the population of Sacramento.\"\n}\nFigure 17: Prompt for all verifier models in the experi-\nments (Section 6.1). It has been carefully constructed\nwith many iterations and refinements across the differ-\nent model families. We have tried our best to make the\nmodels perform as good as possible. For example, after\nnoticing some models deciding not to use tools and hal-\nlucinate an answer, we including instructions like “Use\nthe available tools to query the database and gather\nevidence before making a decision” and “You should\nalways start by querying the database for the schema”.\nFurthermore, for a discussion on output format see Ap-\npendix A.3. Notably, for the observant readers, there\nis no explicit explanation of the tool the agent has in\nits disposal because these descriptions are provided by\nthe frameworks internally (most are built this way) by\ninjecting the docstrings and input-output schemas of the\ntools in the prompt using templates.\n19\n"}, {"page": 20, "text": "Domain\nSubdomain\nDatabases\nEntertainment\nMovies\nmovie, movie_3, movie_platform, movies, movies_4, movielens\nMusic\nmusic, music_tracker, music_platform_2\nTV Shows\nlaw_episode\nGames\nvideo_games, card_games\nCartoons\nsimpson_episodes, superhero, disney\nTechnology\nSoftware\ntalkingdata, codebase_community, codebase_comments, social_media, software_company\nIT\npublic_review_platform, app_store\nBlockchain\ncoinmarketcap\nVision\nimage_and_language\nEducation\nUniversity\nstudent_club, university, cs_semester, college_completion, computer_student\nAcademia\nauthors, citeseer, book_publishing_company\nSchools\ncalifornia_schools\nLanguage\nlanguage_corpus\nBooks\nbooks, shakespeare\nHealth\nHealthcare\nsynthea, donor, mental_health_survey\nMedical\nthrombosis_prediction\nBiology\ngenes\nChemistry\ntoxicology\nEconomy\nFinance\nstudent_loan, debit_card_specializing\nWorld Economies\nworld_development_indicators\nRetail\nretail_complains, sales, superstore, car_retails, regional_sales, retails, retail_world, works_cycles\nBanking\nfinancial\nTransportation\nTransit Systems\nbike_share_1, shipping, trains, cars\nAirport\nairline\nGastronomy\nFood\nfood_inspection_2, beer_factory, food_inspection, cookbook, craftbeer\nRestaurant\nrestaurant, menu\nGovernance\nCrime\nchicago_crime, shooting\nLaw\nlegislator\nEnvironment\nWeather\nsales_in_weather\nGeography\nmondial_geo, address, world\nLabor\nHuman Resources\nhuman_resources\nSports\nBasketball\nprofessional_basketball\nOlympics\nolympics\nHockey\nice_hockey_draft, hockey\nF1\nformula_1\nSoccer\neuropean_football_1, european_football_2, soccer_2016\nTable 5: Domain taxonomy with representative databases (80 total across 11 domains and 36 subdomains).\n20\n"}]}