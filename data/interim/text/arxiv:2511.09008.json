{"doc_id": "arxiv:2511.09008", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.09008.pdf", "meta": {"doc_id": "arxiv:2511.09008", "source": "arxiv", "arxiv_id": "2511.09008", "title": "A Neurosymbolic Approach to Natural Language Formalization and Verification", "authors": ["Sam Bayless", "Stefano Buliani", "Darion Cassel", "Byron Cook", "Duncan Clough", "Rémi Delmas", "Nafi Diallo", "Ferhat Erata", "Nick Feng", "Dimitra Giannakopoulou", "Aman Goel", "Aditya Gokhale", "Joe Hendrix", "Marc Hudak", "Dejan Jovanović", "Andrew M. Kent", "Benjamin Kiesl-Reiter", "Jeffrey J. Kuna", "Nadia Labai", "Joseph Lilien", "Divya Raghunathan", "Zvonimir Rakamarić", "Niloofar Razavi", "Michael Tautschnig", "Ali Torkamani", "Nathaniel Weir", "Michael W. Whalen", "Jianan Yao"], "published": "2025-11-12T06:00:37Z", "updated": "2025-11-12T06:00:37Z", "summary": "Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.09008v1", "url_pdf": "https://arxiv.org/pdf/2511.09008.pdf", "meta_path": "data/raw/arxiv/meta/2511.09008.json", "sha256": "4d0744c7abbd5b625644a49a39b22daafca6531a425b525147ddf217686150fb", "status": "ok", "fetched_at": "2026-02-18T02:27:23.163710+00:00"}, "pages": [{"page": 1, "text": "A NEUROSYMBOLIC APPROACH TO NATURAL\nLANGUAGE FORMALIZATION AND VERIFICATION\nSam Bayless1, Stefano Buliani1, Darion Cassel1, Byron Cook1,2, Duncan Clough1, R´emi Delmas1,\nNafi Diallo1, Ferhat Erata1, Nick Feng1, Dimitra Giannakopoulou1, Aman Goel1, Aditya Gokhale1,\nJoe Hendrix1, Marc Hudak1, Dejan Jovanovi´c1, Andrew M. Kent1, Benjamin Kiesl-Reiter1,\nJeffrey J. Kuna1, Nadia Labai1, Joseph Lilien1, Divya Raghunathan1, Zvonimir Rakamari´c1,\nNiloofar Razavi1, Michael Tautschnig1, Ali Torkamani1, Nathaniel Weir1, Michael W. Whalen1,\nJianan Yao3\n1 Amazon Web Services 2 University College London 3 University of Toronto\nABSTRACT\nLarge Language Models perform well at natural language interpretation and rea-\nsoning, but their inherent stochasticity limits their adoption in regulated industries\nlike finance and healthcare that operate under strict policies. To address this limi-\ntation, we present a two-stage neurosymbolic framework that (1) uses LLMs with\noptional human guidance to formalize natural language policies, allowing fine-\ngrained control of the formalization process, and (2) uses inference-time autofor-\nmalization to validate logical correctness of natural language statements against\nthose policies. When correctness is paramount, we perform multiple redundant\nformalization steps at inference time, cross checking the formalizations for se-\nmantic equivalence. Our benchmarks demonstrate that our approach exceeds 99%\nsoundness, indicating a near-zero false positive rate in identifying logical validity.\nOur approach produces auditable logical artifacts that substantiate the verification\noutcomes and can be used to improve the original text.\n1\nINTRODUCTION\nThe content generation and reasoning capabilities of Large Language Models (LLMs) continue\nto advance rapidly, demonstrating unprecedented improvements in coherence and analytical accu-\nracy (Wei et al., 2022; Yao et al., 2023; Lewis et al., 2021). Despite these advances, their proba-\nbilistic nature and tendency to generate plausible but incorrect information (hallucinations, cf. Xu\net al. 2024b) remain barriers to widespread adoption in regulated sectors. Industries such as health-\ncare, financial services, and legal practices have legal and regulatory obligations for accuracy and\nauditability that current LLM technology has yet to meet (Haltaufderheide & Ranisch, 2024).\nCompanies develop institutional policies to ensure compliance with applicable laws and regula-\ntions. Such policies are typically captured in natural language (NL) documents that define rules,\nprocedures, or guidelines. A challenge thus emerges when organizations look to deploy LLMs to\nanswer questions about such documents: can we develop guardrails to ensure that LLM outputs\nconform to institutional policies? Consider an airline implementing a chatbot to assist customer ser-\nvice representatives in navigating refund policies: if the chatbot incorrectly claims that a customer is\neligible for a refund when they are not, this could lead to legal exposure and loss of customer trust.\nAn effective guardrail would help representatives decide if they can rely on a chatbot response\nwithout spending additional human effort to verify it. The key concern would be to ensure that when\nthe guardrail reports an answer is valid, it actually is. Inspired by the concept of soundness in logic,\nwe define soundness as (1 −p), where p is the overall probability of incorrect validity claims. High\nsoundness thus means that across all requests, incorrect approvals are rare. Following established\npractices in safety-critical systems, where reliability is often measured in “nines” (e.g., 99% = “two\nnines,” 99.9% = “three nines”), we target soundness levels of at least 99%, and secondarily focus\non recall to maximize the probability of accepting valid content. While aiming to maximize recall\n1\narXiv:2511.09008v1  [cs.CL]  12 Nov 2025\n"}, {"page": 2, "text": "under this requirement, we also pursue actionable feedback that steers LLMs toward content that a\nconservative guardrail can accept.\nA natural candidate for developing robust compliance guardrails are symbolic reasoning systems, as\nthey leverage formal logic to generate independently verifiable guarantees (Robinson & Voronkov,\n2001). This approach aligns well with policy documents, which rely on logical, rule-like statements\n(e.g., “if a flight is canceled or ..., then passengers are entitled to a refund”). However, symbolic\nmethods are inherently limited in interpreting natural language, which has triggered the develop-\nment of neurosymbolic approaches: hybrid solutions that combine the NL processing capabilities of\nneural networks with the mathematical rigor of symbolic systems (Hitzler & Sarker, 2021).\nThis paper presents AUTOMATED REASONING CHECKS (ARC), a neurosymbolic approach that\nexceeds 99% soundness on datasets that it was not trained on—an assurance threshold unattain-\nable by existing pure neural or neurosymbolic approaches. This high soundness is also reflected in\nconventional metrics such as false positive rate and precision, where ARC outperforms competing\napproaches. In addition, ARC delivers explainable verdicts and provides actionable feedback that\nLLMs can utilize to refine their outputs. The mechanisms enabling 99% soundness establish a robust\nfoundation for future research aimed at pushing assurance boundaries to three nines and beyond.\nARC operates through two complementary components. The first, called POLICY MODEL CRE-\nATOR (PMC), combines LLMs with symbolic reasoning to translate NL policies into formal policy\nmodels expressed in logic. The process begins with an autoformalization phase that generates an\ninitial policy model. This is followed by an optional vetting phase where domain experts review and\nrefine the policy model with assistance from the system. Through this vetting process, domain ex-\nperts can resolve ambiguities and inconsistencies that may exist in the original documents, or correct\npotential omissions and imprecisions from autoformalization. Policy model creation occurs offline,\nwhere its computation cost will be amortized across subsequent verification tasks. Notably, policy\nmodels serve as enduring sources of truth that provide definitive, unambiguous references within\ntheir respective domains.\nThe second component, called ANSWER VERIFIER (AV), implements a guardrail that verifies NL\ncontent against policy models. The AV uses LLMs to translate NL content into individual logical\nclaims over the vocabulary of the policy model. Each claim is analyzed separately and assigned a\nverification result, together with detailed logical explanations and corrective guidance where appli-\ncable. To increase reliability, the AV uses multiple LLMs to simultaneously formalize the same NL\ncontent, then uses symbolic reasoning to compare formalizations and assign confidence scores. The\nAV delivers auditable logical artifacts that substantiate the verification outcomes.\n2\nRELATED WORK\nRecent approaches use LLMs as judges to evaluate factual accuracy (Jacovi et al., 2025), though\nthese rely on the same probabilistic models that introduce errors. MiniCheck (Tang et al., 2024)\nprovides efficient fact-checking by decomposing claims.\nRefChecker (Hu et al., 2024) intro-\nduces knowledge-centric verification against structured knowledge bases. SelfCheckGPT (Manakul\net al., 2023) leverages consistency across multiple responses to detect hallucinations. FactCheck-\nGPT (Wang et al., 2024) provides comprehensive evaluation with fine-grained error categorization.\nWhile promising, these methods operate within the probabilistic paradigm and cannot provide for-\nmal guarantees. Our neurosymbolic framework formally verifies logical validity against explicit\npolicies, achieving near-zero false positives.\nNeurosymbolic systems combine LLMs with symbolic reasoning, typically translating natural lan-\nguage to formal representations that are then solved by external reasoners (Pan et al., 2023; Olaus-\nson et al., 2023; Callewaert et al., 2025; Ryu et al., 2024). Some neurosymbolic approaches enhance\nthis translation process by integrating LLMs’ reasoning capabilities through Chain-of-Thought (Xu\net al., 2024a; Liu et al., 2025; Xiong et al., 2024).\nExamples of neurosymbolic systems include approaches based on Answer Set Programming\n(ASP) (Ishay et al., 2023; Yang et al., 2023; Brewka et al., 2011) that generate ASP representations.\nLINC (Olausson et al., 2023) uses first-order logic with Prover9 (McCune, 2005). Verus-LM (Calle-\nwaert et al., 2025) provides a multi-paradigm framework with IDP-Z3 (Carbonnelle et al., 2022).\n2\n"}, {"page": 3, "text": "Autoformalization\nSelf Refinement\nComposition\nLogical Test Case\nEnumeration\nConflict Detection\nVerification\n(SMT Solver)\nFeedback Computation\n⋮\n⋮\nVerification Results\nUser\nFeedback\nAutoformalization\nSelf Refinement\nPolicy Model \nCreator (PMC)\nAnswer Verifier (AV)\nPolicy\nModel\nLogical Equivalence\nAggregation\nPolicy\nModel\nTest Case 1\nPolicy\nModel\n⋮\n⋮\nTest Case N\nConflict 1\nConflict N\nLLM-Guided Repair\nManual Repair\nNL\nPolicy\nDocument\nPolicy\nModel\nAutoformalization 1\nAutoformalization N\nQuestion & Answer\nConversation\nText \nSpan 1\nText \nSpan N\nFigure 1: End-to-end architecture of ARC\nSAT-LM (Ye et al., 2023) employs declarative prompting with SMT (De Moura & Bjørner, 2008).\nLogic-LM (Pan et al., 2023) supports multiple formalisms with self-refinement.\nAutoformalization has been studied in mathematics (Wang et al., 2018; Szegedy, 2020; Wu et al.,\n2022; Jiang et al., 2022). However, existing neurosymbolic systems focus on single-shot prob-\nlem solving. Unlike autoformalization for mathematical statements with precise semantics, we\nhandle policy ambiguities through a two-stage approach: the PMC resolves ambiguity with hu-\nman guidance during policy creation, while the AV performs redundant translation with multiple\nLLMs to quantify confidence during validation. This separation enables verification of whether\nLLM-generated content logically follows from established policies, which is crucial for regulated\nindustries.\n3\nMETHODOLOGY\nFig. 1 shows our architecture’s two main components: the PMC (§3.1) and the AV (§3.2). We\nillustrate our approach with an example NL policy about park admission fees:\nGeneral admission: The regular admission to the park is $50. The admission fee in the low season is\n75% of the regular admission fee.\nDiscount: Seniors (age greater than 65) qualify for a 40% discount. Whenever a discount applies, there\nwill be a $10 flat discount processing fee.\nCredit: You can use credit for up to 50% of your final admission (1 credit for 1 dollar). However, if\ncredits are used, then the discount rate is capped at 25%. You can purchase credit at a rate of $0.60 per\ncredit. You can only purchase credit in increments of 5 (cost 3$).\nTax: A federal tax of 10% applies to the final expense.\nSuppose a user asks “I am a senior and want to visit the park in the low season, and I have a total\nfund of $35.40. Can I visit the park?”, and we want to verify a chatbot’s answer of “No, $35.40 is\nnot enough.”\nARC tackles the verification problem in two stages. In the first stage, the PMC auto-formalizes the\npolicy into a so-called policy model: a set of logic rules, expressed in SMT-LIB (Barrett et al., 2016),\ntogether with a schema that defines variables with their types and NL descriptions. SMT-LIB is a\nstandardized logic language that uses prefix notation, where operators precede their arguments; e.g.,\n“if x, then y” is written as (=> x y). Fig. 2 shows snippets of the policy model. In the second stage,\nthe AV first auto-formalizes the statement under validation into logic formulas (over the variables\nof the policy model), then uses an SMT solver to verify those formulas against the policy model.\nA snippet of the validation feedback for our example is shown in Fig. 3. The feedback includes\n3\n"}, {"page": 4, "text": "Variable\nType\nDescription\nisLowSeason\nBool\nWhether the designated admission day is in the low season\nadmissionFeeAfterDiscount\nR\nAdmission fee after discounts are applied but before tax\nRule 1: (=> isLowSeason (= admissionFee (* 0.75 baseFee)))\nRule 2: (<= (* 2.0 customerCredits) admissionFeeAfterDiscount)\nFigure 2: Snippets of policy model (top: variable schema; bottom: rules)\nthe logic translation (with a confidence score between 0 and 1), the validation result, and further\nfeedback explaining the result shown in §3.2.\nThe validation result is Satisfiable, meaning the statement is not always valid: the claim (cannot\nvisit the park) is consistent with the premises (the person is a senior, it is low season, and they have\na fund of 35.4$ available) but doesn’t necessarily follow from them. AV provides two assignments\nas feedback: one showing a counter-example where admission is in fact possible, and a case where\nthe person indeed cannot enter the park. The key difference is in the use of credits (creditUnit = 3\nvs. creditUnit = 0, respectively).\nExplanation. In order to be admitted, the person needs to use $15 worth of credits: The admission\nfee in the low season is $37.5 (75% of $50). After applying the 25% senior discount (capped at 25%\nbecause of credit use) and adding the $10 discount processing fee, the actual admission fee becomes\n$38.125. This fee can be paid by combining $15 of credit (cost: $9) with a $23.125 cash payment,\nfor an expense of $32.125. After adding a federal tax of 10%, the final expense becomes $35.3375,\nwhich is within the budget of $35.4. Notably, this problem proved challenging even for advanced\nLLM judges: both Claude Sonnet 3.7 and Opus 4.1 (with reasoning mode) incorrectly classified the\nanswer as valid, providing plausible but flawed reasoning (see Appendix Figs 8 and 9).\n3.1\nPOLICY MODEL CREATOR (PMC)\nThe PMC takes a policy document written in natural language and autoformalizes it into a policy\nmodel (§3.1.1). It also provides an array of utilities to support users in policy vetting (§3.1.2).\n3.1.1\nAUTOFORMALIZING NATURAL LANGUAGE DOCUMENTS\nTo handle the size and complexity of real-world policy documents in the face of known LLM rea-\nsoning limitations around context size and distractors (Rajeev et al., 2025; Levy et al., 2024), the\nPMC takes a divide-and-conquer approach to autoformalize documents into logic (see Fig. 1).\nThe PMC first splits the input document into a set of text spans. These are processed using an\nincremental, refinement-guided autoformalization procedure: A language model processes each\nspan and identifies statements that express coherent, formalizable meaning. For each statement,\nthe LLM translates the semantic content into a list of SMT-LIB datatypes, variables, and logical\nconstraints (rules). The LLM’s context maintains existing declarations within a span to avoid dupli-\ncated or conflicting declarations. The complete formalization of a span is what we call a policy unit.\nIf this process introduces an error (e.g., malformed syntax), we provide the invalid declarations and\ntheir failure causes to the LLM for repair in a refinement loop.\nOnce the PMC has formalized all text spans, it then composes the resulting policy units into a\nsingle policy model. The PMC generates textual embeddings of variables and clusters them using\ncosine similarity. Variables within a cluster are unified, while variables that share the same name but\nare not clustered are renamed. Consistent replacement of original variables with unified variables\nis performed for the rules of each policy unit, then the rules are aggregated, dropping syntactic\nduplicates. The resulting policy model is a structured representation of the document, consisting of\nthree fields: datatypes, variables, and rules. Each variable is associated with an NL description that\nexplains its meaning in terms of the source document, as shown in Fig. 2. This initial policy model\nis then vetted, as described in the next section. We measure the relationship between document size\nand formalized policy size in §A.1.3.\n4\n"}, {"page": 5, "text": "3.1.2\nPOLICY MODEL VETTING\nThe initial policy model that we automatically generate might contain errors, omissions, and im-\nprecisions. Additionally, as shown in §4.2, NL policies often contain ambiguities that only subject\nmatter experts can resolve. We therefore provide users with several methods for vetting of their pol-\nicy models: linting, inspection, and testing (both manual and automatic). We also develop automated\nrepair approaches around these vetting methods.\nLinting.\nWe provide a linter for our policy models that checks integrity and consistency properties\nbeyond the simple malformedness errors caught during autoformalization. First, we perform syntax-\nbased checks. For example, we detect variables that are not used in the logical rules, and report them\nas warnings to the user. Second, we perform semantics checks. For example, we use an SMT solver\nto detect contradictory rules, which would lead to unexpected results from the AV. We show the\nlinting report to the user as a list of errors and warnings, which can then be addressed either directly\n(e.g., by deleting an unused variable) or through more detailed policy inspection and repair.\nInspection.\nManual inspection allows users to review their generated policy model and verify its\ncorrectness, similar to code review in software development. Users can examine the policy variables\nwith their types and descriptions, as well as the logical rules themselves. We provide two views of the\nrules for inspection: SMT-LIB for experts and structured English for non-experts. We generate the\nstructured English mechanically (based on templates like “if ... then ...”) so as to avoid potential\nhallucinations and imprecisions from the use of LLMs for this purpose. If users uncover an issue\nwith a rule, they can provide NL feedback explaining what is wrong and how it should be fixed.\nThis triggers an automatic LLM-based policy repair step that adjusts the policy model based on the\nprovided feedback. Manual inspection provides strong correctness guarantees when all rules are\ncarefully reviewed, but it can be challenging with large numbers of complex rules that have intricate\ninteractions. The PMC therefore also provides testing as an additional policy vetting methodology.\nTesting.\nTesting provides a systematic way to validate policy models through examples. Similar\nto unit tests, test cases in the PMC are either NL question-answer pairs or simple NL statements\nwith their expected outcomes (i.e., findings) provided by the user (e.g., valid, invalid). Test cases\nthemselves can either be provided manually by users or generated automatically. For manually\nprovided test cases, the PMC “executes” them by running the AV to compute the actual findings.\nIf the actual findings do not match the expected ones, there is an error in the policy model or in\nthe AV translation. The PMC also offers automatic, symbolic test-case generation that leverages\nan SMT solver to systematically explore the state space of the policy model. Since such test cases\nare generated symbolically, each comes with its provably-correct actual finding and, unlike manual\ntest cases, we do not need to run the AV to compute it. Hence, if the actual findings do not match\nthe expected ones, there is an error in the policy model. If testing uncovers an error, users can then\nexamine the policy model and/or translation using the provided information (e.g., the logical rules\njustifying the result) to root cause the issue and generate a repair.\n3.2\nANSWER VERIFIER (AV)\nThe AV uses LLMs to translate natural language (typically, a question-answer pair) into premise-\nconclusion pairs, where premises (abbreviated as P) and conclusions (abbreviated as C) are ex-\npressed in the logical vocabulary of the policy model.\nFor example, a statement like “Since\nyou have at least $50, you can enter the park” might be translated into the premise (≥\ntotalAdmissionFund 50) and the conclusion isEntryAllowed. Premises are the contextual facts\nand conditions established by the NL statement (“you have at least $50”). Conclusions are the log-\nical consequences claimed to follow from those conditions (‘you can enter the park”). A given text\nfragment may be translated into multiple premise-conclusion pairs (or none), as needed to represent\ndifferent claims that are asserted to follow from different conditions.\nTo increase translation confidence, the AV redundantly translates the NL statement using k LLMs\n(Alg. 1). It compares the resulting premise-conclusion pairs semantically using an SMT solver, to\nestimate a confidence score for each pair. Intuitively, the confidence score of a premise-conclusion\npair ⟨P, C⟩is the proportion of the k translations that logically entail the implication P ⇒C.\n5\n"}, {"page": 6, "text": "Algorithm 1 AV Redundant Translation\n1: procedure REDUNDANTTRANSLATION(msg, policy, LLMs)\n2:\nfindings ←∅; Ts ←[Translate(msg, policy, llm) for every model llm ∈LLMs] ,\n3:\nfor T ∈Ts for every premise-conclusion pair ⟨P, C⟩∈T do\n4:\nSupports ←{T ′ | T ′ ∈Ts if T ′ |= (P ⇒C) and T ′ ̸|= ¬P}\n5:\nfindings.add(⟨P, C, conf ⟩) where conf = |Supports|/|Ts|\n6:\nreturn findings\nLogic Translation (Confidence: 1.0):\n• Premise:(and (= ageClass SENIOR) isLowSeason (= totalAdmissionFund 35.4))\n• Conclusion:(not isEntryAllowed)\nValidation Result: Satisfiable (not Valid)\nCounter-Example to Validity:creditUnit=3,customerCredits=15.0,creditDollarValue=9.0, cashAmount\n= 23.181, totalPaymentAvailable = 38.181, finalAdmissionFee = 38.125, isEntryAllowed = true, . . .\nSatisfying Assignment: creditUnit = 0, finalAdmissionFee = 35.75, isEntryAllowed = false, . . .\nFigure 3: Snippet of validation feedback\nFor example, consider the text under validation in §3 and the policy model shown in Fig. 2. Suppose\nwe run redundant translation with three LLMs that all produce identical translations containing the\npremise-conclusion pair shown in Fig. 3. The confidence score for this pair is 3/3.\nNow suppose one of the three LLMs produces a premise-conclusion pair with a different conclusion:\nisEntryAllowed. In this case, AV would return two distinct premise-conclusion pairs: the original\npair from Fig. 3, with confidence 2/3, and the other one with confidence 1/3.\nValidation Feedback.\nAfter translating the text into a list of premise-conclusion pairs with con-\nfidence scores, the AV uses the SMT solver Z3 to provide detailed, logically grounded feedback to\nhelp users understand the validation results, and (where appropriate) provide corrective guidance.\nWe validate each translated claim (consisting of premise P, conclusion C, and confidence score)\nagainst the policy model M, and produce one of the following findings:\n• NoTranslations: The LLM is unable to translate the text into the vocabulary of the policy model.\n• TooComplex: Either the text or the translation into SMT-LIB requires too many tokens.\n• TranslationAmbiguous: ⟨P, C⟩has a confidence score below a configurable threshold (default:\n3/3).\n• Impossible (M ⊨¬P): The premises alone contradict the policy model.\n• Invalid (M ∧P ⊨¬C): The conclusion must be false given the policy model and premises.\n• Satisfiable (M ∧P ⊭C and M ∧P ⊭¬C): The conclusion is consistent with, but doesn’t\nnecessarily follow from, the policy model and the premises.\n• Valid (M ∧P ⊨C): The conclusion must be true given the policy model and the premises.\nFor Impossible, Valid or Invalid findings, the findings include the relevant rules (i.e., the rules justi-\nfying the result) from the policy model, extracted from the SMT solver. For Satisfiable findings, the\nfeedback returns satisfying assignments (“scenarios”) demonstrating how the premises could be ex-\ntended to make the finding valid or invalid. For Impossible, Invalid, Valid, and Satisfiable cases, the\nfindings provide sufficient information that an independent third party could use a theorem prover\nto re-derive the finding from the policy model. For TranslationAmbiguous findings, the feedback\npresents two differing translations along with an assignment that is satisfiable in one translation\nbut not the other. NoTranslations findings return the relevant untranslatable text segments. Finally,\nlogic warnings are surfaced if the premises or conclusions are always true or false irrespective of the\npolicy rules.\nConsider again the example shown in Sec. 3, where the AV is used to validate the conversation with\nthe question “I am a senior and want to visit the park in the low season, and I have a total fund of\n$35.40. Can I visit the park?”, and the chatbot answer “No, $35.40 is not enough.” As shown in\nFig. 3, the AV returns a finding of Satisfiable and provides a variable assignment demonstrating a\nconcrete case in which the park admission is possible within the budget constraint (showing that\nthe provided answer can be wrong), along with a variable assignment showing what additional\ninformation would make the answer correct.\n6\n"}, {"page": 7, "text": "Table 1: Comparison of logical accuracy detection performance on CONDITIONALQA-LOGIC (Sun et al.,\n2022). The columns show soundness (S), false-positive rate (FPR), precision (Pr), recall (Re), F1 score (F1),\naccuracy (Ac), and counts of true/false positives/negatives (TP/FP/TN/FN).\nMethod\nS ↑\nFPR ↓\nPr ↑\nRe ↑\nF1 ↑\nAc ↑\nTP ↑\nFP ↓\nTN ↑\nFN ↓\nARC (#3-ensemble, threshold=3/3)\n99.2\n2.5\n92.6\n15.6\n26.7\n42.7\n163\n13\n506\n884\nARC (#3-ensemble, threshold=2/3)\n98.7\n4.0\n91.0\n20.3\n33.3\n45.4\n213\n21\n498\n834\nARC (without redundant translation)\n98.6\n4.2\n93.8\n31.7\n47.4\n52.9\n332\n22\n497\n715\nLLMaJ (#3-ensemble, threshold=3/3)\n98.3\n5.0\n92.1\n29.0\n44.2\n50.9\n304\n26\n493\n743\nLLMaJ (#3-ensemble, threshold=2/3)\n96.3\n11.2\n90.1\n50.1\n64.4\n63.0\n525\n58\n461\n522\nLLMaJ (1x Sonnet3.7)\n94.8\n15.6\n87.4\n53.5\n66.4\n63.7\n560\n81\n438\n487\nLLMaJ (1x Sonnet3.7 w/ extended thinking)\n94.9\n15.4\n88.2\n57.1\n69.3\n66.2\n598\n80\n439\n449\nFG Implicit span-level (Jacovi et al., 2025)\n96.4\n11.0\n90.5\n52.0\n66.0\n64.2\n544\n57\n462\n503\nFG JSON (Jacovi et al., 2025)\n92.5\n22.7\n86.3\n71.2\n78.0\n73.2\n745\n118\n401\n302\nFG Response-level (Jacovi et al., 2025)\n94.4\n17.0\n85.7\n50.5\n63.6\n61.3\n529\n88\n431\n518\nMiniCheck (Labs, 2024)\n90.4\n28.9\n82.9\n69.3\n75.5\n69.9\n726\n150\n369\n321\nRefChecker (Hu et al., 2024)\n84.4\n47.2\n78.9\n87.6\n83.0\n76.1\n917\n245\n274\n130\nSelfCheckGPT (Manakul et al., 2023)\n95.0\n15.0\n90.5\n71.3\n79.7\n75.8\n746\n78\n441\n301\nLogic-LM (Pan et al., 2023)\n97.4\n7.7\n84.0\n20.2\n32.6\n44.1\n212\n40\n479\n835\n4\nEMPIRICAL EVALUATION\nWe evaluate ARC around the following research questions (RQs) to understand how effective ARC\nis as a guardrail and how our design choices contribute to its performance.\nRQ1 (RELIABILITY OF VALIDATING LOGICAL ACCURACY): How reliably does ARC validate\nlogical accuracy compared to alternative baselines?\nRQ2 (IMPACT OF REDUNDANT TRANSLATION): How does redundant translation (§3.2) impact\nARC’s performance?\nRQ3 (IMPACT OF HUMAN POLICY VETTING): Does human policy vetting improve logical accu-\nracy validation enough to justify the additional effort?\nRQ4 (EFFECTIVENESS OF ARC’S FEEDBACK): Is the feedback provided by ARC effective in\ndriving improvement of LLM output?\nMetrics.\nWe frame logical accuracy detection as a binary classification problem: decide whether\nNL statements are Valid or not. All metrics we report reflect this binary setting. We evaluate ARC\nusing standard classification metrics (precision, recall, F1, accuracy), treating Valid as the positive\nclass and all other categories as negative. Our primary objective, however, is to maximize recall\nwhile strictly limiting false positives across the entire pipeline. In safety-critical settings, rejecting\nborderline cases (not-valid) is still favorable: such outputs can either be refined by the answer-\ngenerating LLM using feedback from ARC, or escalated to human experts.\nTo capture this asymmetry, we define the soundness metric as the probability that content classified\nas valid is actually valid, computed over all decisions as 1 −#False Positives\n#Samples\n. High soundness ensures\nthat incorrect content is rarely approved. When comparing alternative methods, Valid recall is used\nas a tie-breaker under the requirement of maintaining high soundness. Note that soundness is notably\ndifferent from the standard performance metrics (e.g., FPR, precision, Fβ) as it operates directly over\nthe entire dataset (it is not conditional on output or label).\n4.1\nEVALUATION OF LOGICAL ACCURACY VALIDATION\nDataset.\nWe extended the ConditionalQA dataset (Sun et al., 2022) beyond its original binary clas-\nsification (valid / not answerable) into a richer evaluation set with several types of “not valid” exam-\nples. We introduce these additional categories to create variety in the dataset. The extended evalua-\ntion set includes the following categories: Valid (logically correct), Invalid (incorrect due to wrong\nconditions), Satisfiable (missing necessary conditions), Impossible (contradictory conditions), and\nNoTranslations (content that cannot be formalized, originally classified as not answerable). These\ncategories were created by systematically manipulating the conditional structure of original answers:\nremoving conditions (Valid →Satisfiable), negating the claim (Valid →Invalid), or merging con-\ntradictory conditions (Valid →Impossible). The extended dataset (CONDITIONALQA-LOGIC) con-\ntains 349 Valid and 173 examples that are not Valid (103 Invalid, 52 Satisfiable, 4 Impossible, and\n14 NoTranslations).\n7\n"}, {"page": 8, "text": "Table 2: Effect of human vetting on logical accuracy detection for RyanAir’s customer service policy.\nMethod\nS ↑\nFPR ↓\nPr ↑\nRe ↑\nF1 ↑\nAc ↑\nTP ↑\nFP ↓\nTN ↑\nFN ↓\nARC with human vetting\n100.0\n0.0\n100.0\n45.5\n62.5\n61.3\n20\n0\n18\n24\nARC without any human vetting\n96.8\n8.7\n84.6\n25.0\n38.6\n43.6\n11\n2\n16\n33\nRQ1: Reliability of Validating Logical Accuracy.\nWe evaluate different validation methods on\ntheir end-to-end ability to predict validation labels for QA pairs about given NL policy documents.\nThis evaluation tests the complete ARC pipeline (PMC plus AV) without human involvement. Ta-\nble 1 reports performance comparison against alternative methods: LLM-as-Judge (LLMaJ) ap-\nproaches with different prompting strategies, FACTS Grounding (FG) variants (Jacovi et al., 2025),\nfine-grained hallucination detection methods (Tang et al., 2024; Labs, 2024; Manakul et al., 2023;\nHu et al., 2024), and the neurosymbolic Logic-LM (Pan et al., 2023).\nOur evaluation shows that ARC consistently achieves the highest soundness, and the lowest false\npositive rate, across all methods. At the most conservative threshold (3/3), it reaches 99.2% sound-\nness with a 2.5% false positive rate while achieving 92.6% precision. Alternative approaches did not\nachieve the required soundness threshold of 99%, with the second best method (in terms of sound-\nness) being LLMaJ (#3-ensemble, threshold=3/3) at 98.3% soundness and 5.0% false positive rate,\ntwice that of ARC. The results also highlight a clear soundness-recall tradeoff: higher soundness\nreduces the proportion of valid content that is accepted. For example, RefChecker has the highest\nrecall of 87.6%, but this comes at the cost of soundness dropping to just 84.4%, the lowest of all\nmethods. ARC’s reliability comes with lower recall (15.6% for the configuration with soundness\nover 99%), but the tradeoff is intentional: in safety-critical domains where false approvals are far\nmore costly than false rejections, conservatism is a necessary design choice. This makes other meth-\nods unsuitable for applications where assurance guarantees are critical. These findings underscore\nthe importance of conservative guardrails in regulated domains, where avoiding false approvals is\nmore valuable than maximizing coverage. The results also highlight how soundness stands apart\nfrom other metrics through its focus on minimizing false positives. For example, inspecting results\nof RefChecker and ARC at a low confidence threshold (1/3), we see that they exhibit higher preci-\nsion, F1 score and accuracy than ARC at the maximum threshold (3/3), but at the cost of 18x and\n1.5x increase in false positives, respectively. In Appendix A.1.1 we supplement this analysis with\nan evaluation of mitigating identified logical inaccuracies with ARC compared to the baselines.\nRQ2: Impact of Redundant Translation.\nARC uses redundant translation (Alg. 1) to detect\nambiguity and increase confidence in NL-to-logic translations. The first 3 rows in Table 1 capture\nthe impact of redundant translation. Redundant translation improves soundness (from 98.6% to\n99.2%) and reduces the false-positive rate from 4.2% to 2.5%. As discussed above, this comes at the\ncost of reduced recall (31.7% down to 15.6%). The tradeoff can be tuned by adjusting the confidence\nlevel “knob” in the AV: while a strict setting of 3/3 yields 99.2% soundness with only 15.6% recall,\nlowering the confidence to 2/3 increases recall to 20.3% at the cost of bringing soundness to 98.7%.\n4.2\nREFINING REAL-WORLD POLICY MODELS AND ANSWERS\nCONDITIONALQA-LOGIC policies are small and self-contained, and our policy autoformalization\napproach (§3.1.1) produces high-quality models without further review, resulting in the high sound-\nness seen in Table 1. Real-world policy documents can be much larger and more complex than\nCONDITIONALQA-LOGIC policies. To understand the applicability of ARC to such real-world\npolicies, we collected a dataset of customer-facing policy documents (ranging from refund policies\nto insurance policies) from six different businesses. These policies typically require human-in-the-\nloop refinement (§3.1.2) to capture their nuances and support sound ARC validation. We explore\nthe impact of human vetting with a case study of one of the policies (RQ3). In a real-world setting,\nusers may deploy LLM-based chatbots to answer questions about these policies, requiring assurance\nof answer correctness. To study the end-to-end impact of ARC in this setting, we evaluate iterated\nself-refinement of LLM answers using feedback from ARC on the dataset (RQ4).\nRQ3: Impact of Human Policy Vetting.\nWe designed a case study to evaluate the support that\nARC provides for policy refinement and its impact on ARC’s performance as a guardrail. In our\nstudy, we create and compare two formalizations of an airline’s refund policy: one using the PMC\n8\n"}, {"page": 9, "text": "to create a policy model without additional vetting, and one that is revised using a human-in-the-\nloop (as described in §3.1.2). To evaluate these two formalizations, we created a test suite balanced\nacross three sources: 1) verbatim statements from the original policy; 2) Q/A pairs generated by an\nLLM; 3) Q/A pairs generated by three different individuals. The expected classification labels were\ndetermined manually.\nWe found that clarification of ambiguities is a task that warrants human input. There were several\nambiguities and edge cases in the study document, such as what happens when a death occurs on\n(as opposed to prior to) the day of travel. The most notable ambiguity is where the policy states:\n“you may be entitled for a refund if your scheduled time of departure is delayed by at least 5 hours”.\nIt is unclear whether one is entitled to such a refund only if they did not travel (a prerequisite the\ndocument clarifies in other cases).\nThe detailed finding categories were helpful in guiding policy revisions. TranslationAmbiguous or\nNoTranslations findings typically indicated that variables needed to be added or revised. Impossible\nfindings, on the other hand, consistently indicated subtle rule inconsistencies and triggered rule\nrevisions. A recurring pattern in this category is one where autoformalization fails to recognize\nvalid exceptions to the rules that may appear in a different section of the document. For example,\nthe document begins with a broad statement that “If your flight operated and you didn’t travel, you’re\nnot entitled to a refund,” but later on lists several special circumstances under which passengers may\nindeed qualify for a refund even if their flight operated. As shown below, ARC correctly flags\nthe displayed question-answer pair as Impossible and identifies the problematic rules in the policy\nmodel.\nQuestion-answer pair:\nQ: My flight operated but I did not travel because I was denied board-\ning. Am I eligible for a refund?\nA: Yes, if you were denied boarding you are eligible for a refund.\nARC interprets the question-answer as follows:\nPremise: (and didFlightOperate\n(not didPassengerTravel)\n(= flightDisruptionReason DENIED_BOARDING))\nConclusion: isRefundEligible\nARC judgment: Impossible\nARC returns two rules to explain this finding:\n1: (=> (and didFlightOperate\n(not didPassengerTravel))\n(not isRefundEligible))\n2: (=> (= flightDisruptionReason\nDENIED_BOARDING) isRefundEligible)\nThe revision process was a non-trivial effort of several person-hours. As illustrated in Table 2,\nrefinement played a central role in creating a policy that is effective as a guardrail. In fact, human\nvetting increased soundness to 100% and the recall to 45.5%. Note that, even though some tests\nwere run during human vetting, about a third of the tests were held-out. Validation inaccuracies\nwere evenly spread across all classes. Since vetted policies can be reused across future validation\ntasks, the cost is amortized over time, making human-in-the-loop vetting a practical and effective\ncomplement to automated formalization.\nFigure 4: ARC validation finding distribution af-\nter k iterations of answer revision using ARC\nfeedback. At k = 0, we plot the finding dis-\ntribution before any revisions.\nRQ4:\nEffectiveness of ARC’s Feedback.\nThe\nformally-grounded feedback that ARC provides (Sec-\ntion 3.2), in addition to being helpful for human vet-\nting and policy refinement as discussed above, can\nalso be used for automated answer revision. In Fig-\nure 4, we prompt an LLM to iteratively revise AV-\njudged non-Valid answers given ARC feedback (#3-\nensemble, threshold=3/3), and plot the relative per-\ncentages of each finding type after each iteration. We\ncan see that after just three iterations of revision, the\nLLM is able to go from 10.8% to 43.9% Valid an-\nswers. Primarily, this comes from a sharp reduction\nin AV-judged Satisfiable answers (where the answer\ncould be true or false depending on additional context\nthat is not stated in the answer). Given ARC’s feed-\nback, which includes a logically-derived scenario that\nillustrates additional context that is needed to make\nthe answer Valid, the LLM is able to effectively re-\nvise these into Valid answers.\n9\n"}, {"page": 10, "text": "ARC’s feedback is less effective in revising TranslationAmbiguous and NoTranslations answers.\nAnalysis of the revision trajectories shows that the correct revision in these cases would have been to\nrevise the policy model, not the answer; for example, the policy model could be missing variables,\nleading to a failure to formalize the answer’s content, or the policy model could have ambiguous\nvariables that overlap in meaning, leading to a failure to generate a consistent formalization of the\nanswer. As discussed in RQ3, these forms of feedback can be effectively handled by human vetting.\nThis leads credence to the possibility of future automation of manual vetting effort by leveraging\nARC’s feedback to revise the policy model as well as the answer.\n5\nCONCLUSIONS, LIMITATIONS, AND FUTURE WORK\nWe presented ARC, a neurosymbolic guardrail that exceeds 99% (our “two nines” target) soundness\nwhen validating LLM answers against policies, an assurance unattainable by existing approaches.\nThis soundness comes at the cost of recall, a tradeoff we believe appropriate for regulated industries.\nSoundness of ARC heavily depends on the quality of the policy model that it uses for validation. For\nthis reason, ARC enables human oversight. Such oversight is particularly relevant when policies are\ncomplex or very long documents. As we have shown, when domain experts refine policy models,\nboth soundness and recall improve significantly. Beyond metrics, our formal representations let ex-\nperts resolve ambiguities in policy documents, which is something no LLM can do, as only humans\nwith authority can provide the definitive interpretation of what was intended.\nWhile ARC achieves strong soundness guarantees, current implementation limitations include:\n• Scalability: Policy models from documents with hundreds of pages can include hundreds-to-\nthousands of rules, making them challenging for human vetting.\n• Document types: Policies with numerical tables, cross-references, or implicit assumptions can be\nchallenging to formalize accurately without human vetting or background knowledge.\n• Computational cost: Redundant translation requires multiple (3) LLM calls, resulting in average\n5-15 second latency and increased API cost per Q/A validation with our current implementation.\n• Autoformalization challenges: Subtle issues like ambiguous pronouns, implicit temporal or con-\nditional scoping, and complex nested clauses or negations can lead to incorrect formalizations that\npropagate through the current pipeline.\n• Human effort: The investment for human vetting of policies, while amortized over time, remains\na significant upfront cost.\nFuture work includes exploring automatic and confidence-aware focused vetting, fine-tuned transla-\ntion models for improving accuracy and latency/costs, and improved logical formalisms to address\ncurrent limitations while targeting three nines soundness and beyond. Our approach directly benefits\nfrom advances in LLMs and generative AI techniques: as models improve, their ability to formalize\nnatural language to logic will too. We are confident ARC will inherit these improvements while\nmaintaining the mathematical guarantees provided by symbolic reasoning.\nREFERENCES\nClark Barrett, Pascal Fontaine, and Cesare Tinelli.\nThe Satisfiability Modulo Theories Library\n(SMT-LIB). www.SMT-LIB.org, 2016.\nGerhard Brewka, Thomas Eiter, and Mirosław Truszczy´nski. Answer set programming at a glance.\nCommunications of the ACM, 54(12):92–103, 2011.\nBenjamin Callewaert, Simon Vandevelde, and Joost Vennekens. Verus-lm: a versatile framework\nfor combining llms with symbolic reasoning. arXiv preprint arXiv:2501.14540, 2025.\nPierre Carbonnelle, Simon Vandevelde, Joost Vennekens, and Marc Denecker. Interactive configu-\nrator with fo (.) and idp-z3. arXiv preprint arXiv:2202.00343, 2022.\nLeonardo De Moura and Nikolaj Bjørner. Z3: an efficient smt solver. In Proceedings of the The-\nory and Practice of Software, 14th International Conference on Tools and Algorithms for the\nConstruction and Analysis of Systems, TACAS’08/ETAPS’08, pp. 337–340, 2008.\n10\n"}, {"page": 11, "text": "Joschka Haltaufderheide and Robert Ranisch. The ethics of chatgpt in medicine and healthcare: a\nsystematic review on large language models (llms). npj Digit. Medicine, 7(1), 2024. doi: 10.1038/\nS41746-024-01157-X. URL https://doi.org/10.1038/s41746-024-01157-x.\nPascal Hitzler and Md. Kamruzzaman Sarker (eds.). Neuro-Symbolic Artificial Intelligence: The\nState of the Art, volume 342 of Frontiers in Artificial Intelligence and Applications. IOS Press,\n2021. ISBN 978-1-64368-244-0. doi: 10.3233/FAIA342. URL https://doi.org/10.\n3233/FAIA342.\nXiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei\nLiu, Yue Zhang, and Zheng Zhang. Knowledge-centric hallucination detection. In Yaser Al-\nOnaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Em-\npirical Methods in Natural Language Processing, pp. 6953–6975, Miami, Florida, USA, Novem-\nber 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.395.\nURL https://aclanthology.org/2024.emnlp-main.395/.\nAdam Ishay, Zhun Yang, and Joohyung Lee. Leveraging large language models to generate answer\nset programs. In Proceedings of the 20th International Conference on Principles of Knowledge\nRepresentation and Reasoning, KR ’23, 2023. ISBN 978-1-956792-02-7. doi: 10.24963/kr.2023/\n37. URL https://doi.org/10.24963/kr.2023/37.\nAlon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas,\nMichelle Liu, Nate Keating, Adam Bloniarz, et al. The facts grounding leaderboard: Bench-\nmarking llms’ ability to ground responses to long-form input. arXiv preprint arXiv:2501.03200,\n2025.\nAlbert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timoth´ee\nLacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem\nprovers with informal proofs. arXiv preprint arXiv:2210.12283, 2022.\nBespoke Labs.\nBespoke-minicheck-7b,\n2024.\nURL https://huggingface.co/\nbespokelabs/Bespoke-MiniCheck-7B.\nMosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length\non the reasoning performance of large language models, 2024. URL https://arxiv.org/\nabs/2402.14848.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL https:\n//arxiv.org/abs/2005.11401.\nTongxuan Liu, Wenjiang Xu, Weizhe Huang, Yuting Zeng, Jiaxing Wang, Xingyu Wang, Hailong\nYang, and Jing Li. Logic-of-thought: Injecting logic into contexts for full reasoning in large\nlanguage models. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025\nConference of the Nations of the Americas Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies (Volume 1: Long Papers), pp. 10168–10185, Albuquerque,\nNew Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-\n6.\ndoi: 10.18653/v1/2025.naacl-long.510.\nURL https://aclanthology.org/2025.\nnaacl-long.510/.\nPotsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource black-box hallu-\ncination detection for generative large language models. In Houda Bouamor, Juan Pino, and Ka-\nlika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 9004–9017, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.emnlp-main.557. URL https://aclanthology.org/2023.\nemnlp-main.557/.\nWilliam McCune. Release of prover9. In Mile high conference on quasigroups, loops and nonasso-\nciative systems, Denver, Colorado, 2005.\n11\n"}, {"page": 12, "text": "Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenen-\nbaum, and Roger Levy. LINC: A neurosymbolic approach for logical reasoning by combining\nlanguage models with first-order logic provers. In Houda Bouamor, Juan Pino, and Kalika Bali\n(eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 5153–5176, Singapore, December 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.emnlp-main.313. URL https://aclanthology.org/2023.\nemnlp-main.313/.\nLiangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering large\nlanguage models with symbolic solvers for faithful logical reasoning. In Houda Bouamor, Juan\nPino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP\n2023, pp. 3806–3824, Singapore, December 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.findings-emnlp.248. URL https://aclanthology.org/2023.\nfindings-emnlp.248/.\nMeghana Rajeev, Rajkumar Ramamurthy, Prapti Trivedi, Vikas Yadav, Oluwanifemi Bamgbose,\nSathwik Tejaswi Madhusudan, James Zou, and Nazneen Rajani. Cats confuse reasoning llm:\nQuery agnostic adversarial triggers for reasoning models, 2025. URL https://arxiv.org/\nabs/2503.01781.\nJohn\nAlan\nRobinson\nand\nAndrei\nVoronkov\n(eds.).\nHandbook\nof\nAutomated\nRea-\nsoning\n(in\n2\nvolumes).\nElsevier\nand\nMIT\nPress,\n2001.\nISBN\n0-444-50813-\n9.\nURL\nhttps://www.sciencedirect.com/book/9780444508133/\nhandbook-of-automated-reasoning.\nHyun Ryu, Gyeongman Kim, Hyemin S Lee, and Eunho Yang. Divide and translate: Composi-\ntional first-order logic translation and verification for complex logical reasoning. arXiv preprint\narXiv:2410.08047, 2024.\nHaitian Sun, William W Cohen, and Ruslan Salakhutdinov. Conditionalqa: A complex reading\ncomprehension dataset with conditional answers. arXiv preprint arXiv:2110.06884, 2021.\nHaitian Sun, William Cohen, and Ruslan Salakhutdinov. ConditionalQA: A complex reading com-\nprehension dataset with conditional answers. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 3627–3637, Dublin, Ireland, May 2022. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.253. URL https:\n//aclanthology.org/2022.acl-long.253/.\nChristian Szegedy. A promising path towards autoformalization and general artificial intelligence.\nIn International Conference on Intelligent Computer Mathematics, pp. 3–20. Springer, 2020.\nLiyan Tang, Philippe Laban, and Greg Durrett. MiniCheck: Efficient fact-checking of LLMs on\ngrounding documents. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Pro-\nceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp.\n8818–8847, Miami, Florida, USA, November 2024. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2024.emnlp-main.499. URL https://aclanthology.org/2024.\nemnlp-main.499/.\nQingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments with neural translation of in-\nformal to formal mathematics. In International Conference on Intelligent Computer Mathematics,\npp. 255–270. Springer, 2018.\nYuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Ruba-\nshevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pil-\nlai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. Factcheck-bench: Fine-grained\nevaluation benchmark for automatic fact-checkers. In Yaser Al-Onaizan, Mohit Bansal, and Yun-\nNung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024,\npp. 14199–14230, Miami, Florida, USA, November 2024. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2024.findings-emnlp.830. URL https://aclanthology.org/\n2024.findings-emnlp.830/.\n12\n"}, {"page": 13, "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837, 2022.\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and\nChristian Szegedy. Autoformalization with large language models. Advances in neural informa-\ntion processing systems, 35:32353–32368, 2022.\nXuyuan Xiong, Simeng Han, Ziyue Zhou, and Arman Cohan.\nHybridmind: Meta selection of\nnatural language and symbolic language for enhanced llm reasoning. 2024. URL https://\napi.semanticscholar.org/CorpusID:273501516.\nJundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. Faithful logi-\ncal reasoning via symbolic chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Sriku-\nmar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pp. 13326–13365, Bangkok, Thailand, August 2024a. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.720. URL https:\n//aclanthology.org/2024.acl-long.720/.\nZiwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. Hallucination is inevitable: An innate limitation\nof large language models. CoRR, abs/2401.11817, 2024b. doi: 10.48550/ARXIV.2401.11817.\nURL https://doi.org/10.48550/arXiv.2401.11817.\nZhun Yang, Adam Ishay, and Joohyung Lee. Coupling large language models with logic program-\nming for robust and general reasoning from text. arXiv preprint arXiv:2307.07696, 2023.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Ad-\nvances in neural information processing systems, 36:11809–11822, 2023.\nXi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Satlm: Satisfiability-aided language models\nusing declarative prompting. In Proceedings of NeurIPS, 2023.\n13\n"}, {"page": 14, "text": "A\nAPPENDIX\nA.1\nADDITIONAL EXPERIMENTS\nA.1.1\nEFFECTIVENESS OF FEEDBACK FOR MITIGATING LOGICAL INACCURACIES\nTable 3 sheds light on how effective is ARC’s feedback compared to existing state-of-the-art meth-\nods for mitigating logical inaccuracies using the CONDITIONALQA-LOGIC dataset. We utilized a\nuniform experimental methodology for each method M: raw feedback from M is incorporated into\nan identical prompt for answer revision through an LLM for at most 10 revision iterations. In each\nrevision iteration k (0 ≤k ≤10), the answer from the previous iteration is evaluated by M, and if\nlabeled as not Valid by M, a new revised answer is generated through an LLM (Claude Sonnet 3.7)\nby incorporating the raw evaluation feedback from M in the Fig. 10 prompt. Columns 2-6 (marked\nunder % Valid) report the percentage of answers classified as Valid by each method M after differ-\nent revision iterations k ∈0, 1, 3, 5, 10. We further evaluated the final revised answers generated\nafter at most 10 revision iterations separately with top 3 judges from Table 1 (in terms of sound-\nness), and report soundness and recall with respect to the corresponding judge J. Additionally, the\nCONDITIONALQA-LOGIC dataset contains 14 questions labeled as not answerable from the source\ntext by human annotators from (Sun et al., 2021), for which we report the count of final revised\nanswers labeled as Valid by method M as false positives counts in the last column.\nTable 3: Effectiveness of different methods in providing feedback to mitigate logical inaccuracies (as detected\nby the same method) on CONDITIONALQA-LOGIC (Sun et al., 2022). For each method, raw feedback is incor-\nporated into an identical prompt for LLM-based refinement (Fig. 10). We report the percentage of responses\nclassified as Valid by the method after at most 10 answer refinement iterations. Columns S & Re report sound-\nness and recall of final revised answers evaluated separately with top 3 judges from Table 1 (w.r.t. soundness)\nas ground truth. Column FPHuman reports false positives in the final revised answers for questions labeled as not\nanswerable by human annotators.\nMethod M\n% Valid\nJudge J to evaluate final revised answers after 10 revision iterations\nNot ans.\nARC (#3-ensemble)\nLLMaJ (#3-ensemble)\nFG Implicit span-level\n@0\n@1\n@3\n@5\n@10\nS\nRe\nS\nRe\nS\nRe\nFPHuman\nARC (#3-ensemble)\n10.5\n15.5\n19.2\n21.3\n23.6\n—\n89.9\n33.3\n93.9\n72.2\n1\nLLMaJ (#3-ensemble)\n23.8\n68.2\n73.4\n75.9\n77.6\n32.2\n30.9\n—\n91.4\n95.5\n9\nFG Implicit span-level\n38.7\n93.1\n99.2\n99.6\n100.0\n9.6\n100.0\n54.2\n100.0\n—\n14\nFG JSON\n53.6\n93.1\n99.2\n99.6\n99.6\n11.3\n96.6\n46.6\n99.2\n78.2\n99.5\n14\nLLMaJ (1x Sonnet3.7)\n42.0\n94.6\n99.0\n99.6\n99.8\n10.3\n98.2\n56.3\n99.7\n80.8\n99.8\n14\nExamining the above results, we observe the following:\n• ARC’s feedback helps drive the count of Valid answers (as evaluated by ARC) from 55 tests ini-\ntially to 123 tests after 10 iterations. ARC remains cautious and conservative when passing revised\nanswers as Valid, and flags answers even in cases with minute/subtle errors or discrepancies.\n• FG Implicit span-level, FG JSON, and LLMaJ (1x Sonnet3.7) methods remain much more liberal,\nquickly reaching to answers evaluated by them as Valid for > 93% tests after just 1 revision\niteration, and for > 99% tests after 10 revision iterations.\n• When comparing final revised answers through different judges, ARC stands out with the highest\noverall soundness across all top 3 judges.\n• As expected, FG Implicit span-level, FG JSON, and LLMaJ (1x Sonnet3.7) methods show near-\nperfect recall. However, the final revised answers from these methods show major soundness gaps\nwhen evaluated with ARC or LLMaJ (#3-ensemble) judges, making them unsuitable for high-\nstakes tasks.\n• For the 14 questions that are annotated as not answerable, ARC showed significantly lower false\npositives compared to other methods.\n• Overall, the soundness-recall tradeoff persists across methods: ARC demonstrates the highest\nsoundness and agreements across judges, but at the cost of lower recall rates. ARC conservative\njudgments and attention to minute and subtle details are well aligned for safety-critical domains\nwhere false approvals are far more costly than false rejections.\n14\n"}, {"page": 15, "text": "A.1.2\nUTILIZING PMC RULES BEYOND AV\nIn this experiment, we utilized rules generated by PMC 3.1 for CONDITIONALQA-LOGIC, and\nused them instead of or in addition to the source document text as in-context information for LLMaJ\nmethod. Table 4 summarizes the key results for: 1) with PMC rules instead of document text, and\n2) with PMC rules in addition to the document text.\nTable 4: Overall logical accuracy detection across types of in-context information for LLM baselines.\nIn-Context Information\nS ↑\nFPR ↓\nPr ↑\nRe ↑\nF1 ↑\nAc ↑\nTP ↑\nFP ↓\nTN ↑\nFN ↓\nLLMaJ (#3-ensemble, threshold=3/3)\n98.3\n5.0\n92.1\n29.0\n44.2\n50.9\n304\n26\n493\n743\n1) with PMC rules, without Doc\n98.9\n3.5\n93.5\n24.7\n39.1\n48.5\n259\n18\n501\n788\n2) with PMC rules, with Doc\n97.6\n7.1\n90.6\n34.2\n49.7\n53.6\n358\n37\n482\n689\nA.1.3\nPMC SCALING\nIn order to examine how PMC scales with respect to policy size, we run it over a large real-world\ndocument consisting of 274 pages of content. Each page consists of approximately 500 tokens.\nFigure 5 measures the number of datatypes, variables, rules with respect to document size.\nFigure 5: Number of datatypes, variables, and rules with respect to number of unique pages of text formalized,\nwhere each page is approximately 500 tokens.\nWe can see that PMC-produced policy size (in terms of counts of datatypes, variables, and rules)\nscales smoothly with document size. The overall document amounts to 600 datatypes, 1968 vari-\nables, and 1467 rules.\nA.2\nEXPERIMENT DETAILS\nDataset Details (Section 4.1).\nThe original ConditionalQA dataset provides binary labels (valid\nor not-answerable) for question-answer pairs, suited for the task of answering complex questions\nover long documents. However, the dataset’s defining characteristic–that answers are only correct\nunder specific stated conditions–presented a unique opportunity for more comprehensive evaluation\nsuited for our logical accuracy detection task. We systematically leveraged this conditional structure\nto generate additional evaluation categories by using the relationship between answers and their\nassociated conditions:\nSatisfiable: Answers with non-empty conditions deliberately removed or dropped–tests whether a\nmethod can identify when necessary conditions are missing, even though the core answer content\nremains logically satisfiable within the document context.\n15\n"}, {"page": 16, "text": "Invalid: Answers with incorrect alternative conditions applied–evaluates a method’s ability to\ndetect when conditions directly contradict the stated answer based on the document (e.g., flipping\nyes/no responses while maintaining the original conditions).\nImpossible: Contradictory yes/no answers with non-empty conditions merged–tests detection of\nlogical impossibilities by combining mutually exclusive responses (yes/no answers) under unified\ncondition sets.\nValid: Original answers with their stated conditions intact–represents the ground truth conditional\nanswers as provided in the dataset.\nNoTranslations: Questions originally marked as not-answerable in the dataset–preserves the\ndataset’s inherent cases for which an answer cannot be given based on the source text.\nThis systematic augmentation transformed the original dataset into a multi-dimensional evaluation\ndataset for logical accuracy detection that tests conditional reasoning capabilities across various\nlogical relationships and edge cases.\nBaseline Details (Section 4.1).\nFor a fair comparison, we evaluated all methods under comparable\nconfigurations:\nLLMaJ: For a comprehensive LLM-as-judge baseline that takes into account different validation\noutput types comparable to ARC, we developed a customized prompt 7 with explicit instructions\nand details about the logical accuracy validation task. When coupled with majority voting as an\nensemble of 3 (i.e., LLMaJ (#-ensemble) in Table 1), we utilized a comparable ensemble con-\nfiguration as utilized in ARC (#-ensemble) (i.e., ARC with redundant translation using 3 LLM\ncalls).\nFACTS Grounding: We utilized the exact same prompts as presented in (Jacovi et al., 2025) using\nClaude Sonnet 3.7 as the LLM.\nMiniCheck: We evaluated the method in its recommended default configuration as presented\nin (Labs, 2024).\nRefChecker: We utilized the accurate context setting from (Hu et al., 2024) (input prompt provided\nwith the document) and evaluated under joint checking of claims using Claude Sonnet 3.7.\nSelfCheckGPT: We configured the method from (Manakul et al., 2023) with 3 samples using\nClaude Sonnet 3.7.\nLogic-LM: We adapted the method from (Pan et al., 2023) and configured with Prover91 as the\nsolver.\nA.3\nIMPLEMENTATION DETAILS\nA.3.1\nFRAGMENT OF SMT-LIB UTILIZED BY ARC\nARC supports the autoformalization of natural language policy documents into quantifier-free\nSMTLIB with non-linear arithmetic (QF NRIA) as shown in Fig. 6. This logical fragment allows\nus to express predicates over integers, real numbers, booleans, and datatypes (enumerated values).\nWe restrict our approach to this logical fragment because regulatory policy documents are typically\nwritten for human consumption and thus lack complex quantification.\nτ := Int | Real | Bool | k\nd := (declare-datatype k (v1 . . . vn)) | (declare-const x τ)\nop := + | −| / | ∗| = | > | < | ≤| ≥\nc := integers | reals | true | false\ne := x | v | c | (and e e) | (or e e) | (not e) | (=> e e) | (op e e)\ns := (assert e)\np := d1 . . . dn; s1 . . . sn\nFigure 6: The fragment of SMTLIB expressed by our formalization.\n1https://www.cs.unm.edu/˜mccune/prover9/\n16\n"}, {"page": 17, "text": "A.4\nPROMPT TEMPLATES\nA.4.1\nLLM-AS-JUDGE OUTPUTS FOR RUNNING EXAMPLE\nSystem prompt:\nYou are a logician that validate answer to question based on a given policy\nUser prompt:\nPlease consider the following policy:\n<policy>\nGeneral admission: The regular admission to the park is $50. The admission fee in the low\nseason is 75% of the regular admission fee.\nDiscount: Seniors (age greater than 65) qualify for 40% discounts. Whenever a discount ap-\nplies, there will be a $10 flat discount processing fee.\nCredit:You can use credit for up to 50% of your final admission (1 credit for 1 dollar). However,\nif credits are used, then the discount rate is capped at 25%. You can purchase credit at a rate of\n$0.60 per credit. You can only purchase credit in increments of 5 (cost 3$).\nTax: A federal tax of 10% applies to the final expense.\n</policy>\nPlease help validate the question and answer pair as valid or not.\n<question> I am a senior citizen visiting park in the low season, I have $35.4 dollars as total\nfund, will I be able to visit the park\n</question>\n<answer> No, $35.4 is not enough </answer>\nFigure 7: LLM-as-Judge Prompt for the example in Sec. 3\nValid.\nReasoning:Let me calculate the cost for a senior citizen visiting the park in the low season:\nBase admission price in the low season:\nRegular admission: $50 low season rate: 75% of regular = $50 × 0.75 = $37.50 Senior discount:\nSeniors get 40% discount = $37.50 × 0.60 = $22.50 Discount processing fee: +$10 Pre-tax\namount: $32.50 Tax calculation:\n10% federal tax = $32.50 × 0.10 = $3.25 Final cost: $32.50 + $3.25 = $35.75 The visitor has\n$35.40, which is $0.35 short of the required $35.75.\nUsing credits wouldn’t help because:\nWith credits, the discount would be capped at 25% (worse than the 40% senior discount) There\nwould be additional costs to purchase the credit.\nThe answer is valid. With $35.40, the senior citizen does not have enough money to visit the\npark in the low season.\nFigure 8: LLM-as-Judge Validation (Claude Sonnet 3.7)\n17\n"}, {"page": 18, "text": "Valid.\nReasoning: Looking at this problem, I need to calculate the minimum cost for a senior citizen\nvisiting in the low season.\nLet me work through the calculation:\nStep 1: Low Season Admission\nRegular admission: $50 low season admission: 75% × $50 = $37.50 Step 2: Senior Discount\nSeniors get 40% discount Discounted admission: $37.50 × (1 - 0.40) = $37.50 × 0.60 = $22.50\nAdd discount processing fee: $22.50 + $10 = $32.50 Step 3: Consider Credits Using credits\nwould cap the discount at 25% instead of 40%, making it more expensive, so the optimal choice\nis NOT to use credits.\nStep 4: Apply Tax\nSubtotal: $32.50 Federal tax (10%): $32.50 × 1.10 = $35.75 Final Cost: $35.75\nSince the visitor has $35.40 and needs $35.75, they are short by $0.35.\nValidation Result: VALID\nThe answer ”No, $35.4 is not enough” is correct. The minimum amount needed is $35.75,\nwhich exceeds the available funds by $0.35.\nFigure 9: LLM-as-Judge Validation (Claude Opus 4.1)\nA.4.2\nPROMPT TEMPLATE UTILIZED FOR LLM-BASED ANSWER REFINEMENT\nYou are an expert in revising answers to questions based on provided feedback.\nGiven a domain, a question, an original answer, and an explanation of why the original answer\nis incorrect, your task is to revise the original answer based on the given explanation. Return\nonly the revised answer without any prefix. Avoid being overly specific and avoid extending the\nrevised answer with your own background knowledge. The revised answer should be consistent\nwith the provided source text.\nDOMAIN: {domain}\nSOURCE TEXT: {source text}\nQUESTION: {question}\nORIGINAL ANSWER: {original answer}\nFEEDBACK: {feedback}\nREVISED ANSWER:\nFigure 10: Prompt template utilized for LLM-based answer refinement.\n18\n"}, {"page": 19, "text": "A.4.3\nPROMPT TEMPLATE UTILIZED FOR LLM-AS-JUDGE (LLMAJ) METHOD\nYou are an expert document validator. Your task is to determine whether a given answer to a question is\ncorrect according to the provided policy document. When a test finishes, you’re provided with a set of\nvalidation results to understand how your Automated Reasoning policy is performing. A test includes the\nfollowing information:\nQuery and Content: A question a user might ask your GenAI application and a possible response. You\ndefine these if you manually create the test. Automated Reasoning defines these if you generated test\nscenarios.\nConfidence threshold: The minimum confidence level for logic validation that you set for your test. This\nthreshold determines how Automated Reasoning handles uncertainty in translating natural language to\nformal logic. Content that meets or exceeds the threshold is considered a high-confidence finding that\ncan be validated with a definitive result (VALID or INVALID). Content that falls below the threshold is a\nlow-confidence finding that’s marked as TRANSLATION AMBIGUOUS, indicating the system detected\nambiguity and chose not to provide a potentially incorrect validation result.\nValidation results:\nExpected result: The result you expect from running the test.\nActual result: The result from running the test.\nExecution result: Indicates whether the test passed. If the expected and actual results align, the test\npassed. If not, the test failed.\nFindings: The output from an Automated Reasoning policy test is a set of findings. Findings represent\nfactual claims contained in your test question and answer. Use these to help you understand why a test\npassed or failed.\nType: Translations can include a combination of claims and premises.\nPremises: Provides context, assumptions, or conditions that affect how a claim should be evaluated. In\nquestion-and-answer formats, the premise is often the question itself. Answers can also contain premises\nthat establish constraints or conditions. For example, in the question, ”What numbers are divisible by\n2?” and answer, ”Even numbers”, the premise is ”numbers divisible by 2”. In the statement, ”When the\ntraffic light turns green, you must go,” the premises is ”traffic light is green”.\nClaims: Factual statements that Automated Reasoning evaluates for accuracy. In a question-and-answer\nformat, the claim is typically the answer. In a standalone statement, the claim is the fact being asserted.\nFor example, in the question, ”What numbers are divisible by 2?” and answer, ”Even numbers”, the claim\nis ”even numbers”.\nResult: Indicates how valid a finding’s claims are. For more information, see Test validation results.\nConfidence: The confidence score (ranging from 0.0 to 1.0) that Automated Reasoning has in the\ntranslation from natural language to formal logic, representing how certain the system is about correctly\ninterpreting the input text. Higher scores indicate greater certainty in the translation. For example, if a\ntranslation has a confidence of ”1.0”, that indicates maximum certainty that the natural language was\naccurately converted to formal logic. Lower confidence scores suggest the system has some uncertainty\nabout the translation that you may want to review.\nAssignments: Variable assignments from your policy that prove the finding is valid or not. Translations\nhave logic statements that show how the natural language was converted to formal logic. These can be\nmore complex when there is nested logic. For example, hasDogHistoryOfAggression is false.\nRules: The extracted logic from your policy that supports the finding. A test provides you with enough\nrelevant rules from your policy to help you understand the finding result.\nFigure 11: Prompt utilized for LLM-as-Judge (LLMaJ) method (Part 1/2)\n19\n"}, {"page": 20, "text": "The following list details possible validation results from an Automated Reasoning policy test:\nVALID The claims in the model’s response are logically consistent with your policy rules and can be\nmathematically proven correct. The response correctly follows all applicable logical constraints and the\nreasoning from premises to conclusions is sound.\nExample: If your policy states ”Employees with 1+ year of service get parental leave” and the model\nresponds ”You qualify for parental leave since you’ve worked here for 18 months,” this would be VALID\nbecause 18 months exceeds the 1-year requirement.\nINVALID The claims in the model’s response contradict or violate your policy rules. The response con-\ntains statements that are mathematically provable as incorrect based on your policy’s formal logic con-\nstraints.\nExample: If your policy states ”Employees with 1+ year of service get parental leave” and the model\nresponds ”You qualify for parental leave even though you’ve only worked here for 3 months,” this would\nbe INVALID because 3 months doesn’t meet the 1-year requirement.\nSATISFIABLE Given the information provided in the policy, whether the claims in the model’s response\nare correct or in violation of policy rules depends on additional information that is not specified in the\nresponse. Without that additional information, the claims can neither be proven correct nor incorrect.\nExample: If your policy states ”Employees need 1+ year of service for parental leave AND must submit\nform HR-101” and the model responds ”You qualify for parental leave since you’ve worked here for 2\nyears,” this would be SATISFIABLE because the response correctly addresses the service requirement\nbut doesn’t mention the form requirement (without contradicting it).\nIMPOSSIBLE Automated Reasoning can’t make a statement about the claims. This can happen if the\npremises are logically incorrect, or if there is a conflict within the Automated Reasoning policy itself.\nExample: If your policy contains contradictory rules like ”All employees get vacation days” and ”No\nemployees get vacation days,” or if the test question contains impossible premises like ”What benefits\ndo employees get if they work negative hours?”, the result would be IMPOSSIBLE because the logical\nfoundation is flawed.\nTRANSLATION AMBIGUOUS Detected an ambiguity in the translation meant it would be unsound\nto continue with validity checking. Additional context or follow-up questions might be needed to get\ntranslation to succeed.\nExample: If your test question is ”Can they take leave?” without specifying who ”they” refers to, or if the\nmodel response uses ambiguous pronouns like ”It depends on their situation” without clear referents, the\nresult would be TRANSLATION AMBIGUOUS because the system cannot reliably translate the vague\nlanguage into formal logic.\nTOO COMPLEX The input contains too much information for Automated Reasoning to process within\nits latency limits.\nExample: If your test includes an extremely long model response with hundreds of interconnected claims\nabout employee benefits, vacation policies, health insurance, retirement plans, and performance reviews\nall in a single response, the result might be TOO COMPLEX because the logical analysis would exceed\nprocessing time limits.\nNO TRANSLATIONS Identifies that some or all of the input prompt wasn’t translated into logic. This\ncan happen if the input isn’t relevant to the Automated Reasoning policy, or if the policy doesn’t have\nvariables to model relevant input. If Automated Reasoning can’t translate anything, you get a single\nNO TRANSLATIONS finding. You might also see a NO TRANSLATIONS (along with other findings)\nif some part of the validation isn’t translated.\nExample: If your HR policy is designed to validate employee benefits but your test question asks ”What’s\nthe weather like today?” or ”How do I cook pasta?”, the result would be NO TRANSLATIONS because\nthe content is completely unrelated to your policy’s domain and variables.\nPOLICY DOCUMENT: {document}\nQUESTION: {question}\nANSWER: {answer}\nBased on the document above, classify this question-answer pair into exactly one of the QA validator\naggregate results\nAnalyze the question and answer carefully against the document. Consider: - Does the answer accurately\nreflect what the document states? - Are there any conditions, exceptions, or edge cases the answer fails to\nmention? - Is the answer always true, sometimes true, or never true according to the document?\nProvide your classification as a single word from the list above in the format <answer>[...]</answer>\nfollowed by a brief explanation.\nCLASSIFICATION:\nFigure 12: Prompt utilized for LLM-as-Judge (LLMaJ) method (Part 2/2)\n20\n"}]}