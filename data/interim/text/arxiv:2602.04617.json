{"doc_id": "arxiv:2602.04617", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04617.pdf", "meta": {"doc_id": "arxiv:2602.04617", "source": "arxiv", "arxiv_id": "2602.04617", "title": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation", "authors": ["Ruixiao Yang", "Yuanhe Tian", "Xu Yang", "Huiqi Li", "Yan Song"], "published": "2026-02-04T14:45:49Z", "updated": "2026-02-04T14:45:49Z", "summary": "Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04617v1", "url_pdf": "https://arxiv.org/pdf/2602.04617.pdf", "meta_path": "data/raw/arxiv/meta/2602.04617.json", "sha256": "dad1885101bb0a5c39cfb9e463acd1ce54fca60dd8e8ea0cce70166ba2bf66de", "status": "ok", "fetched_at": "2026-02-18T02:19:48.231988+00:00"}, "pages": [{"page": 1, "text": "LEAD: Layer-wise Expert-aligned Decoding for\nFaithful Radiology Report Generation\nRuixiao Yang‚ô¶‚ô£, Yuanhe Tian‚ô•‚ô£\nXu Yang‚ô¶\nHuiqi Li‚ô¶\nYan Song‚ô†\n‚ô¶Beijing Institute of Technology\n‚ô£Zhongguancun Academy\n‚ô•Zhongguancun Institute of Artificial Intelligence\n‚ô†University of Science and Technology of China\n‚ô¶{3220245229, pyro yangxu, huiqili}@bit.edu.cn\n‚ô•tianyuanhe@zgci.ac.cn\n‚ô†clksong@gmail.com\nAbstract\nRadiology Report Generation (RRG) aims to pro-\nduce accurate and coherent diagnostics from med-\nical images. Although large vision language mod-\nels (LVLM) improve report fluency and accuracy,\nthey exhibit hallucinations, generating plausible\nyet image-ungrounded pathological details. Ex-\nisting methods primarily rely on external knowl-\nedge guidance to facilitate the alignment between\ngenerated text and visual information. However,\nthese approaches often ignore the inherent decod-\ning priors and vision-language alignment biases\nin pretrained models and lack robustness due to\nreliance on constructed guidance. In this paper,\nwe propose Layer-wise Expert-aligned Decoding\n(LEAD), a novel method to inherently modify the\nLVLM decoding trajectory. A multiple experts\nmodule is designed for extracting distinct patho-\nlogical features which are integrated into each\ndecoder layer via a gating mechanism. This layer-\nwise architecture enables the LLM to consult ex-\npert features at every inference step via a learned\ngating function, thereby dynamically rectifying\ndecoding biases and steering the generation to-\nward factual consistency. Experiments conducted\non multiple public datasets demonstrate that the\nLEAD method yields effective improvements in\nclinical accuracy metrics and mitigates hallucina-\ntions while preserving high generation quality.\n1. Introduction\nRadiology Report Generation (RRG) is a significant cross-\nmodal task that bridges medical imaging and natural lan-\nguage processing, with the aim of automatically produc-\ning accurate, coherent and clinically informative textual\ndescriptions for given radiological images (e.g. chest X-\nrays) (Sloan et al., 2024; Huang et al., 2025; Tian et al.,\nStandard Large VLM\nText\n...\nLLM Decoder\nExisting Methods\nText\nVisual \nEncoder\n...\n...\nLLM Decoder\nKnowledge\nOurs\nText\nVisual \nEncoder\n...\n...\nLLM Decoder\n...\nMLP\nMLP\nVisual \nEncoder\nMultiple\nExperts\nMLP\nNo Finding\nCardiomegaly\nLung Opacity\nCardiomegaly\nCardiomegaly\nHallucination\nFact\nGate Fusion\nFigure 1. Conceptual illustration of Layer-wise Expert-aligned\nDecoding (LEAD). Unlike standard VLMs and existing methods\nthat often succumb to intrinsic language priors (top and middle\nrows), our approach directly intervenes in the internal decoding\nprocess. By adaptively injecting fine-grained visual expert signals\ninto intermediate representations of each decoder layer, LEAD\ndynamically rectifies the generation trajectory to ensure faithful\nalignment with fine-grained medical visual facts (bottom row).\n2025). This technique can substantially reduce radiolo-\ngists‚Äô workload and improve diagnostic efficiency, and can\nalso act as an effective decision-support tool in resource-\nlimited clinical settings (Johnson et al., 2019; Chen et al.,\n2020). With the rapid advancement of deep learning, espe-\ncially Transformer architectures and large language models\n(LLMs), the RRG field has achieved remarkable progress,\nwith the generated report fluency approaching human expert\nlevels (Tu et al., 2024; Liu et al., 2025b). Existing RRG\nmethods still face substantial challenges in achieving high\nclinical accuracy and maintaining factual consistency with\nthe images. Recently, approaches based on vision‚Äìlanguage\nmodels (VLM) have attempted to leverage the strong com-\nmonsense reasoning and language generation capabilities\nof LLMs to improve report quality, and have achieved no-\n1\narXiv:2602.04617v1  [cs.CL]  4 Feb 2026\n"}, {"page": 2, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\ntable performance gains (Li et al., 2023; Tian & Song, 2025;\nTanno et al., 2025). Despite their strong performance in\nreport generation and human acceptability, these methods\nstill suffer from prominent hallucinations, where generated\ndescriptions contradict imaging evidence (Kapadnis et al.,\n2024; Gu et al., 2025; Hou et al., 2025).\nHallucination is a common issue in VLMs, arising primarily\nfrom factors such as unbalanced and insufficient image‚Äìtext\nmatching data distributions, suboptimal cross-modal align-\nment between visual and textual representations, and the\nintrinsic language priors of LLMs (Liu et al., 2024b; Li\net al., 2025). In medical image report generation, this issue\nlargely degrades clinical accuracy and factual consistency.\nSpecifically, hallucinations appear as misalignment between\ngenerated reports and fine-grained pathological findings in\nradiological images, causing factual inaccuracies (Huang\net al., 2023; Gu et al., 2025; Jiang et al., 2025). Although\nexisting studies have attempted to enhance the attention of\nVLMs to visual features and achieve fine-grained image-text\nalignment through various approaches (Liu et al., 2025b;\nSun et al., 2025; Gu et al., 2025; Hou et al., 2025) to improve\nreport accuracy, most adhere to a common paradigm: con-\nstructing knowledge bases from relevant fine-grained visual-\ntextual information, and subsequently utilizing retrieval-\naugmented (RAG) or contrastive learning to provide align-\nment guidance during report generation via optimization at\nthe prompt or token level. However, these approaches over-\nlook addressing hallucinations from their intrinsic causes in\nVLMs. Though such guidance improves output accuracy,\nthe models still suffer from decoding and alignment bi-\nases induced by the inherent prior knowledge of pretrained\nLLMs. Furthermore, this design induces heavy reliance\non constructed guidance, where any bias therein is propa-\ngated to generated reports, ultimately compromising model\nrobustness. Therefore, addressing inherent decoding and\nalignment biases to mitigate hallucinations at the source\nis a crucial research direction, as it enables more accurate\nalignment between fine-grained visual features and textual\nsemantics and leads to more robust and factually consistent\nreport generation models.\nTo address these limitations, we propose Layer-wise Expert-\naligned Decoding (LEAD), which injects fine-grained visual\nexpert signals into the VLM decoding process using multi-\nlabel pathological classification features extracted from med-\nical images. This design aims to mitigate hallucinations and\nenforce tighter alignment between generated reports and\nvisual evidence, as shown in Figure 1. LEAD is inspired\nby recent VLM studies that reduce LLM priors decoding\nbiases by injecting statical perturbations into intermediate\ndecoder representations during inference (Su et al., 2025b;\nWu et al., 2025; Chen et al., 2025). Unlike these methods,\nour approach requires an adaptive injection strategy to in-\ntegrate expert signals which are inherently rich in effective\ninformation and diverse across samples. Our objective is to\ndynamically incorporate these signals into intermediate rep-\nresentations to achieve fine-grained visual alignment. We\nconceptualize the VLM as a knowledgeable yet error-prone\n‚Äústudent‚Äù guided by ‚Äúvisual experts‚Äù, mirroring an itera-\ntive refinement process. Concretely, each decoding layer is\ntreated as a generation step, where projected expert features\nare selectively fused into intermediate representations via\na context-aware gated fusion mechanism. This guides the\ndecoding trajectory toward a low-hallucination trajectory by\nutilizing fine-grained visual cues, thereby suppressing LLM\nprior bias. Furthermore, lightweight unfreezing and fine-\ntuning during this process enhance visual‚Äìtextual semantic\nalignment through direct interaction between the LLM and\nexpert signals. Compared with methods based on fixed\nperturbations or external knowledge bases, LEAD shows\nsuperior adaptability and robustness while effectively sup-\npressing hallucinated content unrelated to visual evidence.\nThis work makes the following contributions:\n‚Ä¢ We propose Layer-wise Expert-aligned Decoding, a\nnovel framework that uses multi-label pathological fea-\ntures as visual expert signals to rectify the intrinsic de-\ncoding priors of LLM. By strategically injecting these\nsignals into each decoder layer, our method ensures the\ngenerated reports adhere closely to visual evidence.\n‚Ä¢ We design a bidirectional integration mechanism that\ntreats report generation as an interactive expert-guided\nprocess, employing a context-aware gated fusion mech-\nanism at each decoding layer to adaptively integrate\nfine-grained expert signals. This facilitates a dynamic\nshift in the decoding trajectory toward visual facts.\n‚Ä¢ Experiments on both the CheXpert Plus and MIMIC-\nCXR datasets demonstrate that our approach substan-\ntially improves clinical accuracy, effectively reducing\nhallucinations while preserving report fluency.\n2. The Approach\nThis work presents a novel layer-wise expert-aligned decod-\ning framework for radiology report generation, built upon\nthe Qwen3-VL architecture. An overview of the proposed\nframework is illustrated in Figure 2. To mitigate halluci-\nnations, LEAD injects fine-grained visual expert signals\ndirectly into the decoding process. By enabling each LLM\nlayer to selectively integrate these signals into intermedi-\nate representations, the framework dynamically steers the\ndecoding trajectory toward visual facts. Specifically, we de-\nsign a visual expert module consisting of pathology-specific\nclassifiers to capture comprehensive fine-grained features.\nThese expert signals undergo distinct projections when in-\njected into different decoder layers to transform into appro-\npriate information for each layer. Meanwhile, the decoder\n2\n"}, {"page": 3, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nVisual \nEncoder\nModality\nConnector\nExpert Classifier 1\nVisual Expert Module\nExpert Classifier 2\nExpert Classifier C\n‚Ä¶...\nVision Tokens\n...\nText Tokens\n...\nConfidence-Aware \nAggregation\nProjector\nùëì1\nùëù1\nùëì2\nùëù2\nùëìùëê\nùëùùëê\nWeighted \nConcatenate\n‡∑©ùëìùëñ= ùëùùëñ‚àôùëìùëñ\nùëí\nExpert \nEmbedding\nQwen3 LLM\nDecoder Layer 0\nCAGF 0\nCAGF 1\nCAGF n\nDecoder Layer 1\n‚Ä¶...\nDecoder Layer n\nContext Adaptive Gate Fusion\nCAGF\nContext Adaptive Gated Fusion\nImage\nMLP\n(1 ‚àíùëîùë°\nùëô) ‚äô‚Ñéùë°\nùëô+ ùëîùë°\nùëô‚äôùëíùëô\nùëíùëô\nHidden \nStates\nConcatenate\nDynamic Gate \nMLP\nùëí\nExpert \nEmbedding\nGenerated Report\nDate Flow\n‚Ñé‚Äôùë°\nùëô\n‚Ñéùë°\nùëô\n‚Ñéùë°\nùëô\nùëîùë°\nùëô\n‚Ñé‚Äôùë°\nùëô\n‚äôHadamard Product\nFigure 2. An overview of the proposed framework.\nemploys a context-aware gated fusion mechanism to adap-\ntively absorb expert information at each layer, effectively\nrectifying decoding biases derived from LLM priors and\nenhancing the clinical accuracy of the generated reports.\n2.1. Backbone\nAn appropriate backbone is essential for the proposed\nmethod, as RRG requires a VLM that combines advanced\nLLM capabilities with strong cross-modal understanding.\nThis backbone not only enables high-quality report genera-\ntion, but also provides a solid foundation to further improve\nimage‚Äìtext alignment and hallucination suppression. We\nadopt Qwen3-VL as the backbone due to its strong multi-\nmodal comprehension and text generation capabilities.\nQwen3-VL is a state-of-the-art VLM that integrates a ViT\nvisual encoder, a modality connector based on multilayer\nperceptron (MLP), and the Qwen3 large language model\n(Bai et al., 2025). Given a radiological image, the visual en-\ncoder extracts a sequence of embeddings, which are aligned\ninto the language token space and concatenated with prompt\ntokens for decoding. To balance training efficiency with\nthe preservation of pretrained knowledge, we employ a hy-\nbrid fine-tuning strategy. We apply Low-Rank Adaptation\n(LoRA) (Hu et al., 2022) to the LLM‚Äôs attention weights\nwhile freezing its core parameters to prevent catastrophic for-\ngetting. Concurrently, the visual encoder and our proposed\nmodules are fully fine-tuned to capture domain-specific med-\nical patterns and facilitate visual-textual alignment.\n2.2. Visual Expert Module\nTo extract pathological visual features and leverage them\nto guide text decoding, we integrate a visual expert branch.\nThe module comprises a set of specialized experts corre-\nsponding one-to-one with target categories, where each is\ninstantiated as a three-layer MLP binary classifier. During\nforward propagation, we replicate feature maps from the fi-\nnal layer of the visual encoder to feed this expert branch. Su-\npervised by pathological labels extracted from reports (e.g.,\ncardiomegaly, pleural effusion), experts learn to capture\ndiscriminative features indicative of specific pathological\nconditions. The intermediate features from each classifier\nare then concatenated to construct a comprehensive expert\nembedding. Subsequently, these signals are incorporated\ninto the LLM decoding process through a fusion mecha-\nnism described in the following section, enabling factually\ngrounded visual evidence to guide hallucination mitigation.\n2.3. Layer-wise Expert-Aligned Decoding Method\nTo effectively bridge the gap between visual evidence and\ntextual generation, we propose the LEAD method. This\nmethod is designed to simulate a continuous ‚Äúconsultation‚Äù\nprocess where the language model like a student actively in-\nteracts with the pathological classifiers like experts at each\nstep of the generation process. Unlike existing methods\nrestricted to input level knowledge injection or fixed pat-\ntern interventions, our approach orchestrates a bidirectional\ninteraction at each decoding layer by adaptively embed-\nding expert guidance and enabling the decoder to perform\na secondary selection of these signals. This mechanism\n3\n"}, {"page": 4, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nsteers intermediate representations closer to visual evidence,\nprogressively correcting decoding deviations to suppress\nhallucinations and ensure factual consistency of reports.\n2.3.1. CONFIDENCE-AWARE EXPERT AGGREGATION\nTo achieve adaptive integration of expert signals tailored to\ndiverse input samples, we employ a confidence-aware fusion\nstrategy. Recognizing that pathological manifestations vary\nbetween different cases, we modulate the feature vector fi\nof each expert by its confidence score pi = œÉ(si), where\nsi denotes the classification logit and œÉ denotes the sig-\nmoid. This operation, formulated as Àúfi = pi ¬∑ fi, effectively\nsuppresses the noise of irrelevant experts. The weighted fea-\ntures are then concatenated and linearly projected to form\nthe global expert embedding e ‚ààRd, where d denotes the\nfeature dimension of the intermediate representations.\n2.3.2. ADAPTIVE EXPERT PROJECTION\nSince different layers of the LLM decoder capture semantic\ninformation at varying levels, the visual expert signals must\nbe adaptively transformed to align with the feature space of\neach layer. For the l-th layer of the decoder, we employ an\nMLP layer to produce the layer-wise expert embedding el:\nel = œïl(e)\nwhere œïl(¬∑) represents the MLP transformation of the l-th\nlayer. This transformation allows the module to adaptively\nemphasize specific pathological features relevant to the cur-\nrent depth of the decoding network architecture.\n2.3.3. CONTEXT-ADAPTIVE GATED FUSION\nMECHANISM\nTo integrate the expert knowledge into the LLM, we intro-\nduce a gated fusion block at each decoder layer. The core\nmotivation behind this design is to empower the decoder\nto selectively absorb visual evidence without disrupting the\ninherent semantic flow of the autoregressive generation. In-\nstead of indiscriminately mixing multimodal features, which\nrisks compromising the pretrained linguistic coherence, we\nemploy a context-aware gated fusion mechanism combined\nwith an interpolation connection. Note that the hidden state\nhere corresponds to the intermediate representation refer-\nenced in our preceding discussion of the LEAD architecture.\nLet Hl = {hl\n1, . . . , hl\nT } denote the sequence of hidden\nstates in the l-th layer. Since the expert embedding el con-\ntains global pathological information, we first expand it to\nmatch the sequence length of Hl. At each step t of the\nsequence, we determine the injection intensity by checking\nboth the current context and the visual signal. We concate-\nnate the current hidden state hl\nt with the expert embedding\nel to compute a dynamic gate gl\nt:\ngl\nt = œÉ\n\u0000œïgate([hl\nt; el])\n\u0001\n(1)\nwhere [¬∑; ¬∑] represents the concatenation operation. The ex-\npert signals is then injected via an interpolation connection:\nh‚Ä≤l\nt = (1 ‚àígl\nt) ‚äôhl\nt + gl\nt ‚äôel\n(2)\nwhere ‚äôrepresents the Hadamard product. This mechanism\nfunctions as a soft selection switch: if the current hidden\nstate hl\nt correlates with the generation of text grounded\nin visual facts, the gate opens to incorporate the relevant\nexpert information; conversely, if hl\nt pertains to content\nunrelated to visual input, the gate value remains suppressed\nto preserve the original linguistic fluency. This effectively\nsteers the intermediate representations toward a decoding\ntrajectory aligned with visual facts, thereby suppressing\nhallucinations induced by the LLM‚Äôs priors and promoting\nalignment between the generated text and visual evidence.\n2.4. Training Strategy\nTraining a robust RRG model requires balancing the reten-\ntion of the LLM‚Äôs linguistic fluency with the acquisition of\ndomain-specific medical visual understanding. To achieve\nthis, we employ a fine-tuning strategy combined with a\nmulti-task objective to optimize the overall performance.\nWe freeze the majority of the LLM parameters to preserve\npre-trained capabilities, while fine-tuning the model via\nLoRA for efficient domain adaptation. Concurrently, to\nensure the extraction of pathological features, we fully\nunfreeze the vision encoder, the modality connector, the\nproposed Visual Expert Module, and all layer-wise expert-\naligned decoding blocks. This allows the visual backbone\nto learn fine-grained pathological representations while the\ninjection modules learn the optimal alignment strategy.\nTraining is supervised by a composite loss function L com-\nbining of the generation loss and the pathological classifi-\ncation loss. The generation task is optimized via standard\nCross-Entropy loss (Lgen) for next-token prediction:\nLgen = ‚àí\nT\nX\nt=1\nlog P(yt|y<t, I)\n(3)\nwhere yt is the target token and I is the image. Simul-\ntaneously, to ensure the expert module provides accurate\nguidance, we apply a multi-label Binary Cross-Entropy loss\n(Lcls) to the predictions of the expert classifiers:\nLcls = ‚àí\nC\nX\ni=1\n[ci log(ÀÜci) + (1 ‚àíci) log(1 ‚àíÀÜci)]\n(4)\nwhere C is the number of pathology categories (experts), ci\nis the ground-truth label extracted from the report, and ÀÜci is\n4\n"}, {"page": 5, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nTable 1. Dataset statistics.\nDATASET\nTRAINING\nVAL\nTESTING\nCHEXPERT PLUS\n40,463\n5,780\n11,562\nMIMIC-CXR\n270,790\n2,130\n3,858\nthe predicted probability. The total objective is a weighted\nsum of these terms to balance generation and classification:\nL = Lgen + ŒªLcls\n(5)\nwhere Œª is a hyperparameter balancing the two tasks. Based\non empirical observations, we set Œª = 4 to achieve a bal-\nance of magnitude between the losses. This configuration\nenforces explicit supervision on the visual experts, thereby\nensuring that the injected signals are factually grounded.\n3. Experiment Settings\n3.1. Datasets\nTo conduct a comprehensive evaluation of our proposed\nframework, we primarily use the CheXpert Plus dataset\n(Chambon et al., 2024), a recently introduced large-scale\nbenchmark for the generation of chest radiograph reports.\nAdditionally, we employ the MIMIC-CXR dataset (Johnson\net al., 2019) to conduct supplementary analytical experi-\nments and assess the model‚Äôs generalization capabilities.\nThe dataset partition is presented in Table 1.\nCheXpert Plus. The dataset comprises approximately 220k\nchest X-rays paired with comprehensive radiology reports.\nReports in CheXpert Plus are parsed into structured sec-\ntions, such as Indication, Findings and Impression, while\nalso including detailed patient demographics and labels for\n14 pathology categories extracted via CheXBert (Smit et al.,\n2020). We adopted the data filtering and partitioning pro-\ntocols as defined in CXPMRG-Bench (Wang et al., 2025a).\nSpecifically, we retained samples with findings sections,\nresulting in a corpus of 57,805 image-report pairs. The\ndataset was then randomly divided into training, validation,\nand testing sets following the 7:1:2 ratio.\nMIMIC-CXR. The MIMIC-CXR dataset serves as a stan-\ndard benchmark in the field, consisting of 377,110 chest\nradiographs and 227,827 free-text radiology reports. In this\nwork, we utilize a subset of the MIMIC-CXR dataset to con-\nduct supplementary analytical experiments. This enables\nus to validate the generalization capability of our method\nacross different clinical data distributions.\n3.2. Baseline\nTo evaluate the effectiveness of LEAD, we conduct a\ncomprehensive comparison on the CheXpert Plus dataset\nwith representative methods spanning the field‚Äôs evolu-\ntion. We benchmark a diverse range of baselines, ranging\nfrom classical Transformer architectures, including R2Gen\n(Chen et al., 2020), R2GenCMN (Chen et al., 2021), and\nR2GenRL (Qin & Song, 2022), to early generative models\nlike CvT2DistilGPT2 (Nicolson et al., 2023). Furthermore,\nwe compare with recent advanced large VLMs, R2GenGPT\n(Wang et al., 2023) equipped with Llama2 (Touvron et al.,\n2023) and Llama3 (Grattafiori et al., 2024), the retrieval-\naugmented R2GenCSR (Wang et al., 2024), and the Mamba-\nbased MambaXray-VL (Wang et al., 2025a).\nTo validate the effectiveness and adaptability of LEAD,\nwe conduct comprehensive internal comparisons using the\nQwen3-VL backbone, which is the latest version of Qwen\nseries models that achieve state-of-the-art performance on\nmany tasks (Liu et al., 2025a; Zhang et al., 2025; Deng &\nMihalcea, 2025; Su et al., 2025a; Bai et al., 2025; Chen et al.,\n2026; Wang et al., 2026) We design controlled experiments\nacross four distinct configurations: fully frozen parameters,\nstandard LoRA fine-tuning (Hu et al., 2022), fine-tuning the\nvision tower only, and a hybrid strategy. In each setting,\nthe corresponding vanilla model serves as the control to\nquantify the incremental performance gains. Furthermore,\nwe investigate scalability by evaluating models with 2B, 4B,\nand 8B parameters for comprehensive validation.\n3.3. Implementation Details\nWe implement our proposed framework using PyTorch and\nthe Hugging Face Transformers library. All experiments are\nconducted on 2 NVIDIA A100 (80GB) GPUs.\nModel Configuration. We employ Qwen3-VL-Instruct as\nthe backbone and conduct experiments across three scales:\n2B, 4B, and 8B. To efficiently adapt the model while preserv-\ning pre-trained knowledge, we apply Low-Rank Adaptation\n(LoRA) to all linear layers within the LLM‚Äôs attention and\nfeed-forward networks (r = 64, Œ± = 128).\nTraining Strategy. The model is trained for 8 epochs with\na total batch size of 16 and a gradient accumulation step\nof 4. We use the AdamW optimizer with a peak learning\nrate of 2e-4. A cosine annealing scheduler is employed for\nlearning rate decay, accompanied by a linear warmup for the\nfirst 3% of training steps. For input processing, all images\nare resized to a resolution of 256√ó256 pixels. During the\ninference phase, we employ greedy decoding to generate\nradiology reports, prioritizing the most probable tokens.\nEvaluation Metrics. We employ a comprehensive set of\nmetrics to assess both the linguistic quality and clinical cor-\nrectness of the generated reports. For Natural Language\nGeneration (NLG) performance, we report ROUGE-L (Lin,\n2004), METEOR (Banerjee & Lavie, 2005), and CIDEr\n(Vedantam et al., 2015), which measure the lexical over-\nlap and semantic similarity between the generated text and\nground-truth reports. To evaluate Clinical Efficacy (CE), we\n5\n"}, {"page": 6, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nTable 2. Comparison with the prior baselines on the CheXpert Plus Dataset. The evaluation metrics include Natural Language Generation\nscores (R-L: ROUGE-L, M: METEOR, C: CIDEr) and Clinical Efficacy scores (P: Precision, R: Recall, F1: F1-score) which are reported\nas macro-averages. Bold indicates the best performance overall, and underlined denotes the second best results.\nMETHOD\nPUBLISH\nDECODER\nR-L\nM\nC\nP\nR\nF1\nR2GEN (CHEN ET AL., 2020)\nEMNLP20\nTRANSFORMER\n0.246\n0.113\n0.077\n0.318\n0.200\n0.181\nR2GENCMN (CHEN ET AL., 2021)\nACL21\nTRANSFORMER\n0.256\n0.127\n0.102\n0.329\n0.241\n0.231\nR2GENRL (QIN & SONG, 2022)\nACL22\nTRANSFORMER\n0.186\n0.101\n0.012\n0.193\n0.229\n0.196\nCVT2DISTILGPT2 (NICOLSON ET AL., 2023)\nAIM23\nGPT2\n0.238\n0.118\n0.101\n0.285\n0.252\n0.246\nR2GENGPT (WANG ET AL., 2023)\nMETA-RAD.23\nLLAMA2-7B\n0.266\n0.145\n0.123\n0.315\n0.244\n0.260\nLLAMA3-8B\n0.220\n0.121\n0.134\n0.306\n0.232\n0.222\nR2GENCSR (WANG ET AL., 2024)\nARXIV24\nLLAMA2-7B\n0.265\n0.146\n0.121\n0.315\n0.247\n0.259\nMAMBAXRAY-VL-B (WANG ET AL., 2025A)\nCVPR25\nLLAMA2-7B\n0.267\n0.149\n0.117\n0.333\n0.264\n0.273\nLAYER-WISE EXPERT-ALIGNED DECODING (LEAD)\nOURS\nQWEN3-2B\n0.188\n0.108\n0.075\n0.282\n0.237\n0.243\nLAYER-WISE EXPERT-ALIGNED DECODING (LEAD)\nOURS\nQWEN3-4B\n0.192\n0.115\n0.080\n0.296\n0.242\n0.250\nLAYER-WISE EXPERT-ALIGNED DECODING (LEAD)\nOURS\nQWEN3-8B\n0.197\n0.125\n0.082\n0.320\n0.267\n0.275\nutilize the standard CheXpert labeler (Irvin et al., 2019) to\nextract labels for 14 prominent pathological observations\nfrom both generated and reference reports. By comparing\nthese extracted labels, we compute Precision, Recall, and\nF1-score to quantify diagnostic accuracy. All CE metrics are\nreported as macro-averages to ensure balanced evaluation\nacross all pathology categories regardless of their frequency.\n4. Result and Analysis\nTo empirically validate the effectiveness of the proposed\nLayer-wise Expert-aligned Decoding (LEAD) framework,\nwe conducted extensive experiments on the CheXpert Plus\nand MIMIC-CXR datasets. In this section, we provide a\ndetailed analysis of performance comparison with state-of-\nthe-art methods, the adaptability of our approach across\ndifferent training configurations, and the contribution of\nindividual components through ablation studies.\n4.1. Comparison with Other Methods\nWe benchmark our proposed LEAD framework with com-\nprehensive baselines, ranging from transformer architectures\nto contemporary large VLMs. The quantitative results on\nthe CheXpert Plus dataset are summarized in Table 2.\nThe primary objective of LEAD is to rectify decoding biases\nand mitigate hallucinations inherent in large VLMs. As\nreported in Table 2, our method using Qwen3-8B achieves\na clinical F1-score of 0.275, surpassing the performance of\nrecent advanced models such as MambaXray-VL and the\nretrieval-augmented approach R2GenCSR.\nCrucially, compared with recent RRG frameworks utilizing\nadvanced LLM backbones, LEAD demonstrates a substan-\ntial margin in clinical accuracy. While existing approaches\noptimize visual encoders or employ instruction tuning, they\noften fail to suppress the LLM‚Äôs intrinsic tendency to gen-\nerate unfounded findings driven by internal priors. This\nevidence suggests that relying solely on external interven-\ntions is insufficient to guarantee medical factuality. Instead,\nour strategy of directly injecting fine-grained expert sig-\nnals into decoding layers proves more effective, steering the\ngeneration trajectory toward visual consistency in real-time.\nRegarding NLG metrics, we observe that LEAD yields\nslightly lower scores compared to other baselines. This\nphenomenon is expected, as standard metrics prioritize lexi-\ncal overlap and often reward models that overfit to generic\nlinguistic templates. However, LEAD encourages the de-\ncoder to adhere to fine-grained visual signals. This guides\nthe model to generate clinically precise descriptions, effec-\ntively relaxing the strict alignment with the stylistic patterns\nof ground truth templates in favor of diagnostic accuracy.\nWe further investigate LEAD‚Äôs scalability across Qwen3\nbackbones (2B, 4B, 8B). As shown in Table 2, we observe a\npositive correlation between model capacity and diagnostic\nperformance, suggesting that larger models better interpret\nand integrate the pathological constraints injected. Notably,\neven our lightweight 2B variant achieves an F1-score of\n0.243, competitive with the larger R2GenGPT-Llama3-8B,\nconfirming that LEAD‚Äôs effectiveness stems from its intrin-\nsic alignment mechanism rather than parameter scaling.\n4.2. Robustness and Generalization Analysis\nTo verify that the performance gains are intrinsic to the\nLEAD framework rather than dependent on specific fine-\ntuning strategies, we conducted controlled experiments\nacross four distinct training configurations. It is impor-\ntant to note that in all settings, the parameters of the pro-\nposed LEAD module are fully trainable to learn the optimal\nalignment, regardless of whether the backbone is frozen or\nunfrozen. The comparative results are presented in Table 3.\nTo verify that performance gains are intrinsic to the LEAD\nframework rather than dependent on specific fine-tuning\nstrategies, we conducted controlled experiments across four\ntraining configurations. The results demonstrate that LEAD\nconsistently improves the F1-score of the backbone model\nacross all settings. It is important to note that in all con-\n6\n"}, {"page": 7, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nTable 3. Effectiveness of LEAD under different settings. We compare the vanilla backbone with the backbone equipped with our LEAD\nmethod across four fine-tuning configurations on two datasets. The symbols √ó and ‚úìdenote frozen and trainable parameters, respectively.\nNote that the parameters of the proposed LEAD modules are fully trainable in all ‚ÄúOurs‚Äù settings.\nMODEL\nSETTINGS\nCHEXPERT PLUS\nMIMIC-CXR\nVISION TOWER\nLLM\nR-L\nM\nC\nP\nR\nF1\nR-L\nM\nC\nP\nR\nF1\nBACKBONE\n√ó\n√ó\n0.114\n0.103\n0.050\n0.223\n0.201\n0.195\n0.112\n0.101\n0.055\n0.219\n0.228\n0.192\nOURS\n√ó\n√ó\n0.188\n0.118\n0.074\n0.241\n0.212\n0.214\n0.190\n0.109\n0.081\n0.237\n0.212\n0.197\nBACKBONE\n‚úì\n√ó\n0.190\n0.112\n0.077\n0.267\n0.235\n0.226\n0.200\n0.114\n0.086\n0.241\n0.230\n0.214\nOURS\n‚úì\n√ó\n0.193\n0.118\n0.077\n0.278\n0.245\n0.241\n0.202\n0.118\n0.093\n0.262\n0.238\n0.225\nBACKBONE\n√ó\n‚úì\n0.192\n0.115\n0.076\n0.265\n0.239\n0.228\n0.198\n0.114\n0.089\n0.248\n0.228\n0.217\nOURS\n√ó\n‚úì\n0.194\n0.123\n0.080\n0.281\n0.242\n0.245\n0.203\n0.120\n0.097\n0.269\n0.246\n0.232\nBACKBONE\n‚úì\n‚úì\n.0191\n0.119\n0.079\n0.270\n0.241\n0.243\n0.202\n0.117\n0.093\n0.280\n0.242\n0.231\nOURS\n‚úì\n‚úì\n0.197\n0.125\n0.082\n0.320\n0.267\n0.275\n0.208\n0.120\n0.104\n0.298\n0.250\n0.247\nTable 4. Ablation study on CheXpert Plus. ‚ÄúExp.‚Äù indicates\nthe presence of the visual expert module. ‚ÄúProj.‚Äù denotes the\nprojection strategy, where ‚ÄúShared‚Äù uses a single projection shared\nacross all layers and ‚ÄúLayer‚Äù uses distinct layer-wise projections.\n‚ÄúFuse.‚Äù represents the fusion mechanism, comparing our context-\nadaptive ‚ÄúGate‚Äù with direct ‚ÄúAdd‚Äù.\nCONFIGURATION\nMETRICS\nEXP.\nPROJ.\nFUSE.\nR-L\nM\nC\nP\nR\nF1\n-\n-\n-\n0.191 0.119 0.079 0.270 0.241 0.243\n‚úì\n-\n-\n0.189 0.118 0.075 0.268 0.244 0.242\n‚úì\nSHARED GATE\n0.192 0.120 0.079 0.275 0.246 0.249\n‚úì\nLAYER\nADD\n0.194 0.123 0.080 0.295 0.253 0.259\n‚úì\nLAYER GATE\n0.197 0.125 0.082 0.320 0.267 0.275\nfigurations, the parameters of the LEAD module are fully\ntrainable to learn the optimal alignment. Specifically, in\nthe most restricted scenario where both the vision tower\nand LLM are frozen, exclusively optimizing our inserted\ncomponents boosts the F1-score from 0.195 to 0.214. This\nconfirms that even when the backbone‚Äôs capacity is locked,\nour experts can extract and inject pathological cues that the\nfrozen model fails to utilize. This advantage is further am-\nplified in the fully optimized hybrid setting, where LEAD\npropels the performance from 0.243 to a peak of 0.275. The\nsignificant margin of +3.2% indicates that our layer-wise\ninjection mechanism provides unique capabilities to contin-\nuously steer the decoding trajectory toward visual facts.\nTo assess the adaptability of the model, we extended the eval-\nuation to the MIMIC-CXR dataset. The results mirror the\ntrends observed on CheXpert Plus, with LEAD consistently\noutperforming the backbone across settings. This stability\nacross datasets confirms that our framework captures robust\npathological-textual alignments rather than overfitting to\nspecific data distributions or training configurations.\n4.3. Ablation Study\nTo validate the necessity of each component within the\nproposed LEAD framework, we conducted an ablation study\non the CheXpert Plus dataset. The results are summarized\nin Table 4. We first evaluate a variant that employs the\nvisual expert module solely as an auxiliary classification task\nwithout injecting features into the decoder. The performance\nof this configuration is comparable to the baseline. This\nresult suggests that merely optimizing a multi-task objective\nis insufficient to correct hallucinations.\nWhen incorporating expert signals, sharing the output of a\nsingle projection across all layers yields a marginal perfor-\nmance improvement. However, a substantial gap remains\nwhen compared to our layer-wise projection strategy. This\nperformance disparity validates our hypothesis that LLM\ndecoding layers at varying depths capture different levels of\nsemantic abstraction, thereby necessitating distinct, layer-\nspecific expert projections rather than a uniform signal.\nFinally, we analyze the fusion mechanism by replacing the\ndynamic gate with a direct ‚ÄúAdd‚Äù operation. This leads to a\nsuboptimal performance. The superior performance of our\nproposed method highlights the critical role of the context-\nadaptive gated fusion. This mechanism acts as a dynamic\n‚Äúsoft switch‚Äù allowing the decoder to selectively integrate\nexpert evidence only when relevant to the current gener-\nation step, thereby maximizing visual alignment without\ndisrupting the linguistic coherence of the backbone.\n4.4. Qualitative Analysis\nTo intuitively evaluate clinical accuracy, Figure 3 presents a\nqualitative comparison between the fully unfrozen hybrid\nfine-tuned baseline and our LEAD framework. As observed,\nthe baseline model frequently suffers from factual incon-\nsistencies and omissions. For example, it fails to detect\npulmonary edema in the first case and hallucinates a ‚Äúnor-\nmal‚Äù heart size in the subsequent cases despite clear visual\nevidence of cardiomegaly. However, LEAD accurately recti-\nfies these decoding biases by leveraging fine-grained expert\nsignals. Our method not only successfully captures fine-\ngrained pathological details missed by the backbone, such\nas pleural effusions in the second case, but also corrects hal-\nlucinated descriptions to accurately diagnose the enlarged\n7\n"}, {"page": 8, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nReference: \n    Unchanged supportive quadrant, including right IJ catheter, right PICC line, and bilateral pigtail drainage catheters. The cardiomediastinal silhouette is \ngrossly unchanged. Decrease in bibasilar opacities. Mild pulmonary edema. Decrease in bilateral small pleural effusions. No visualized pneumothorax.\nBackbone:\n    There is unchanged positioning of supportive medical devices, including an enteric tube which courses below the inferior margin of the radiograph. \nThere are low lung volumes with persistent bibasilar opacities and small bilateral pleural effusions. No pneumothorax is seen. The cardiomediastinal \nsilhouette is unchanged in appearance. No acute bony abnormalities are noted. (Missed: edema)\nOurs (LEAD):\n    Interval removal of the right IJ Swan-Ganz catheter and sheath, otherwise tubes and lines are unchanged. Interval increase in bilateral pleural \neffusions with increased pulmonary edema. No significant change in bibasilar opacities which may represent atelectasis or consolidation.\nReference: \n    PA and lateral view of the chest demonstrates a new moderate-sized right pleural effusion with adjacent opacity at the right base. Heart size is \nmoderately enlarged. No evidence of a pneumothorax. Calcifications at the aortic arch. On the lateral projection, there is the suggestion of a calcified \naneurysm of the abdominal aorta up to 4.1 cm.\nBackbone:\n    The cardiomediastinal silhouette is normal in size. There are bibasilar opacities, left greater than right which may represent atelectasis versus \nconsolidation. The pulmonary vasculature appears prominent with indistinct margins suggesting mild interstitial edema. No acute osseous abnormalities \nidentified. (Missed: pleural effusions, no pneumothorax)\nOurs (LEAD):\n    There is redemonstration of cardiomegaly, with splaying of the carina reflecting left atrial enlargement. There has been interval development of small \nbilateral pleural effusions, right greater than left, as well as interstitial opacities in both lung bases, which may represent atelectasis or early \nconsolidation. No pneumothorax. Visualized osseous structures are unremarkable for age.\nReference: \n    There is new widespread airspace disease in the right lung, particularly in the right upper lobe, with additional bilateral lower lobe opacities, \nworrisome for pneumonia, edema or aspiration. There is cardiomegaly, unchanged in the prior study. The bony thorax shows osteopenia.\nBackbone:\n    The cardiomediastinal silhouette is normal in size. There are diffuse reticular opacities with perihilar prominence and Kerley B lines, consistent with \npulmonary edema. There are small bilateral pleural effusions. The visualized osseous structures are unremarkable.\nOurs (LEAD):\n    There is redemonstration of cardiomegaly with enlargement of the pulmonary arteries bilaterally, compatible with pulmonary hypertension. There has \nbeen interval development of bilateral alveolar opacities in both lungs which may represent interstitial edema versus infection or aspiration. The \nvisualized osseous structures are unremarkable.\nRadiology Image Input\nRadiology Image Input\nRadiology Image Input\nFigure 3. Qualitative comparison of generated reports. ‚ÄúBackbone‚Äù represents the results of the fully unfrozen hybrid fine-tuned backbone.\nGreen text indicates correct pathological findings, red highlights factual errors, and yellow denotes missed findings.\nheart. This qualitative evidence confirms that our layer-wise\ninjection mechanism effectively steers the generation toward\nhigh clinical accuracy and aligns with visual facts.\n5. Related Work\nLarge Vision-Language Models for RRG. The evolution\nof RRG has transitioned from early encoder-decoder archi-\ntectures (Chen et al., 2021; Babar et al., 2021; Wang et al.,\n2025b) to Large Vision-Language Models (LVLMs) that\nleverage the reasoning capabilities of pretrained LLMs (Li\net al., 2023; Tanno et al., 2025). Representative frameworks,\nsuch as R2GenGPT (Wang et al., 2023), employ lightweight\nadapters to align visual features with frozen LLMs. How-\never, by treating the LLM as a black box, these methods re-\nmain vulnerable to intrinsic language priors. Consequently,\nthey frequently generate hallucinations where descriptions\ndeviate from fine-grained visual evidence (Liu et al., 2023;\nTian et al., 2024; Kapadnis et al., 2024; Liu et al., 2024a;\nGu et al., 2025; Hou et al., 2025).\nHallucination Mitigation Strategies. To address hallu-\ncinations, existing approaches primarily rely on external\nknowledge or auxiliary alignment objectives. This category\nincludes Retrieval-augmented (RAG) (Wang et al., 2024;\nLiu et al., 2025b; Sun et al., 2025; Hou et al., 2025) and\ncontrastive learning approaches (Gu et al., 2025; Wang et al.,\n2025b). While these techniques enhance image-text match-\ning, they share a limitation: they intervene largely at the\nprompt or token level via external guidance, overlooking\nthe intrinsic decoding biases within the LLM. Furthermore,\nthis reliance on constructed knowledge compromises model\nrobustness when external guidance is suboptimal.\nDecoding Intervention Methods. In the general domain,\nstrategies such as activation steering have been explored to\nsuppress hallucinations (Su et al., 2025b; Wu et al., 2025;\nChen et al., 2025). However, these generic strategies typi-\ncally rely on static interventions, lacking the fine-grained\nadaptivity required for complex pathological features.\nUnlike previous methods, our LEAD framework introduces\na dynamic, internal intervention mechanism. By integrating\nmulti-label expert signals via layer-wise gated fusion, LEAD\nadaptively rectifies the decoding trajectory to align with\nvisual facts, offering a solution to intrinsic model biases.\n6. Conclusion\nIn this work, we revisit hallucination in vision‚Äìlanguage\nreport generation from the perspective of intrinsic decoding\nbiases in large language models. We propose a Layer-wise\nExpert-aligned decoding framework that integrates struc-\ntured visual expert signals into intermediate representations\nof LLM, enabling adaptive correction of biased generation\ntrajectories. By formulating generation as a step-wise expert-\n8\n"}, {"page": 9, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nguided refinement process and introducing a context-aware\ngated fusion mechanism, our method selectively aligns in-\ntermediate representations with fine-grained visual evidence\nwhile preserving linguistic coherence. Experiments com-\npared with other methods demonstrate effective improve-\nments in factual accuracy, with effective hallucination mit-\nigation across different model scales and training settings.\nThis work demonstrates that internal decoding guidance is\ncritical as external constraints, offering a robust direction\nfor eliminating hallucinations in medical report generation.\nReferences\nBabar, Z., van Laarhoven, T., and Marchiori, E. Encoder-\ndecoder models for chest x-ray report generation perform\nno better than unconditioned baselines. Plos one, 16(11):\ne0259639, 2021.\nBai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z.,\nDeng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z.,\nHuang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li,\nZ., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu,\nJ., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv,\nC., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun,\nY., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang,\nQ., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z.,\nYang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang,\nH., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F.,\nZhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report.\narXiv preprint arXiv:2511.21631, 2025.\nBanerjee, S. and Lavie, A. Meteor: An automatic metric\nfor mt evaluation with improved correlation with human\njudgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization, pp. 65‚Äì72, 2005.\nChambon, P., Delbrouck, J.-B., Sounack, T., Huang, S.-C.,\nChen, Z., Varma, M., Truong, S. Q., Chuong, C. T., and\nLanglotz, C. P. Chexpert plus: Augmenting a large chest\nx-ray dataset with text radiology reports, patient demo-\ngraphics and additional image formats. arXiv preprint\narXiv:2405.19538, 2024.\nChen, J., Tian, J., Zheng, T., Hui, Y., and Li, Z. Qwen-\naec: Instruction-tuning large language models for high-\nperformance building code analysis. Advanced Engineer-\ning Informatics, 71:104268, 2026.\nChen, X., Zhang, Y., Liu, Q., Wu, J., Zhang, F., and Tan, T.\nMixture of decoding: An attention-inspired adaptive de-\ncoding strategy to mitigate hallucinations in large vision-\nlanguage models.\narXiv preprint arXiv:2505.17061,\n2025.\nChen, Z., Song, Y., Chang, T.-H., and Wan, X. Generating\nradiology reports via memory-driven transformer. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 1439‚Äì1449,\n2020.\nChen, Z., Shen, Y., Song, Y., and Wan, X. Cross-modal\nmemory networks for radiology report generation. In\nProceedings of the 59th annual meeting of the association\nfor computational linguistics and the 11th international\njoint conference on natural language processing (volume\n1: long papers), pp. 5904‚Äì5914, 2021.\nDeng, N. and Mihalcea, R. Rethinking table instruction\ntuning. arXiv preprint arXiv:2501.14693, 2025.\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian,\nA., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A.,\nVaughan, A., et al. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783, 2024.\nGu, D., Gao, Y., Zhou, Y., Zhou, M., and Metaxas, D.\nRadalign: Advancing radiology report generation with\nvision-language concept alignment. In International Con-\nference on Medical Image Computing and Computer-\nAssisted Intervention, pp. 484‚Äì494. Springer, 2025.\nHou, W., Cheng, Y., Xu, K., Li, H., Hu, Y., Li, W., and\nLiu, J. Radar: Enhancing radiology report generation\nwith supplementary knowledge injection. arXiv preprint\narXiv:2505.14318, 2025.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., Chen, W., et al. Lora: Low-rank adaptation\nof large language models. ICLR, 1(2):3, 2022.\nHuang, X., Han, Y., Li, R., Wu, P., Zhang, K., et al. Cmeaa:\nCross-modal enhancement and alignment adapter for ra-\ndiology report generation. In Proceedings of the 31st\nInternational Conference on Computational Linguistics,\npp. 8546‚Äì8556, 2025.\nHuang, Z., Zhang, X., and Zhang, S. Kiut: Knowledge-\ninjected u-transformer for radiology report generation. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 19809‚Äì19818, 2023.\nIrvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S.,\nChute, C., Marklund, H., Haghgoo, B., Ball, R., Shpan-\nskaya, K., et al. Chexpert: A large chest radiograph\ndataset with uncertainty labels and expert comparison. In\nProceedings of the AAAI conference on artificial intelli-\ngence, volume 33, pp. 590‚Äì597, 2019.\nJiang, Y., Chen, C., Nguyen, D., Mervak, B. M., and Tan, C.\nGpt-4v cannot generate radiology reports yet. In Findings\nof the Association for Computational Linguistics: NAACL\n2025, pp. 2127‚Äì2154, 2025.\n9\n"}, {"page": 10, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nJohnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum,\nN. R., Lungren, M. P., Deng, C.-y., Mark, R. G., and\nHorng, S. Mimic-cxr, a de-identified publicly available\ndatabase of chest radiographs with free-text reports. Sci-\nentific data, 6(1):317, 2019.\nKapadnis, M., Patnaik, S., Nandy, A., Ray, S., Goyal, P., and\nSheet, D. Serpent-vlm: self-refining radiology report gen-\neration using vision language models. In Proceedings of\nthe 6th Clinical Natural Language Processing Workshop,\npp. 283‚Äì291, 2024.\nLi, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J.,\nNaumann, T., Poon, H., and Gao, J. Llava-med: Training\na large language-and-vision assistant for biomedicine in\none day. Advances in Neural Information Processing\nSystems, 36:28541‚Äì28564, 2023.\nLi, Z., Wu, X., Du, H., Liu, F., Nghiem, H., and Shi, G. A\nsurvey of state of the art large vision language models:\nBenchmark evaluations and challenges. In Proceedings of\nthe Computer Vision and Pattern Recognition Conference,\npp. 1587‚Äì1606, 2025.\nLin, C.-Y. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pp.\n74‚Äì81, 2004.\nLiu, C., Tian, Y., and Song, Y. A systematic review of deep\nlearning-based research on radiology report generation.\narXiv preprint arXiv:2311.14199, 2023.\nLiu, C., Tian, Y., Chen, W., Song, Y., and Zhang, Y. Boot-\nstrapping large language models for radiology report gen-\neration. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 38, pp. 18635‚Äì18643, 2024a.\nLiu, H., Xue, W., Chen, Y., Chen, D., Zhao, X., Wang,\nK., Hou, L., Li, R., and Peng, W. A survey on halluci-\nnation in large vision-language models. arXiv preprint\narXiv:2402.00253, 2024b.\nLiu, J., Tian, Y., and Song, Y. Balanced training data aug-\nmentation for aspect-based sentiment analysis. arXiv\npreprint arXiv:2507.09485, 2025a.\nLiu, K., Ma, Z., Kang, X., Li, Y., Xie, K., Jiao, Z., and\nMiao, Q. Enhanced contrastive learning with multi-view\nlongitudinal data for chest x-ray report generation. In Pro-\nceedings of the Computer Vision and Pattern Recognition\nConference, pp. 10348‚Äì10359, 2025b.\nNicolson, A., Dowling, J., and Koopman, B. Improving\nchest x-ray report generation by leveraging warm starting.\nArtificial intelligence in medicine, 144:102633, 2023.\nQin, H. and Song, Y. Reinforced cross-modal alignment\nfor radiology report generation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022, pp.\n448‚Äì458, 2022.\nSloan, P., Clatworthy, P., Simpson, E., and Mirmehdi, M.\nAutomated radiology report generation: A review of re-\ncent advances. IEEE Reviews in Biomedical Engineering,\n18:368‚Äì387, 2024.\nSmit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A. Y., and\nLungren, M. P. Chexbert: combining automatic label-\ners and expert annotations for accurate radiology report\nlabeling using bert. arXiv preprint arXiv:2004.09167,\n2020.\nSu, C., Tian, Y., Song, Y., and Zhang, Y. Text reinforcement\nfor multimodal time series forecasting. arXiv preprint\narXiv:2509.00687, 2025a.\nSu, J., Chen, J., Li, H., Chen, Y., Qing, L., and Zhang, Z.\nActivation steering decoding: Mitigating hallucination\nin large vision-language models through bidirectional\nhidden state intervention. In Proceedings of the 63rd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 12964‚Äì12974,\n2025b.\nSun, L., Zhao, J. J., Han, W., and Xiong, C. Fact-aware\nmultimodal retrieval augmentation for accurate medical\nradiology report generation. In Proceedings of the 2025\nConference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers), pp.\n643‚Äì655, 2025.\nTanno, R., Barrett, D. G., Sellergren, A., Ghaisas, S.,\nDathathri, S., See, A., Welbl, J., Lau, C., Tu, T., Azizi,\nS., et al. Collaboration between clinicians and vision‚Äì\nlanguage models in radiology report generation. Nature\nMedicine, 31(2):599‚Äì608, 2025.\nTian, Y. and Song, Y. Feature decomposition via shared\nlow-rank matrix recovery for ct report generation. IEEE\nTransactions on Medical Imaging, pp. 1‚Äì1, 2025.\nTian, Y., Xia, F., and Song, Y. Diffusion networks with task-\nspecific noise control for radiology report generation. In\nProceedings of the 32nd ACM International Conference\non Multimedia, pp. 1771‚Äì1780, 2024.\nTian, Y., Yan, Z., Lyu, N., and Song, Y. Extractive radiology\nreporting with memory-based cross-modal representa-\ntions. IEEE Transactions on Medical Imaging, pp. 1‚Äì1,\n2025.\n10\n"}, {"page": 11, "text": "LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nTu, T., Azizi, S., Driess, D., Schaekermann, M., Amin, M.,\nChang, P.-C., Carroll, A., Lau, C., Tanno, R., Ktena, I.,\net al. Towards generalist biomedical ai. Nejm Ai, 1(3):\nAIoa2300138, 2024.\nVedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider:\nConsensus-based image description evaluation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 4566‚Äì4575, 2015.\nWang, X., Li, Y., Wang, F., Wang, S., Li, C., and Jiang,\nB. R2gencsr: Retrieving context samples for large lan-\nguage model based x-ray medical report generation. arXiv\npreprint arXiv:2408.09743, 2024.\nWang, X., Wang, F., Li, Y., Ma, Q., Wang, S., Jiang, B.,\nand Tang, J. Cxpmrg-bench: Pre-training and bench-\nmarking for x-ray medical report generation on chexpert\nplus dataset. In Proceedings of the Computer Vision and\nPattern Recognition Conference, pp. 5123‚Äì5133, 2025a.\nWang, Y., Sun, Y., Tan, T., Hao, C., Cui, Y., Su, X., Xie,\nW., Shen, L., and Yu, Z. Trrg: Towards truthful radiology\nreport generation with cross-modal disease clue enhanced\nlarge language models. In International Conference on\nMedical Image Computing and Computer-Assisted Inter-\nvention, pp. 647‚Äì657. Springer, 2025b.\nWang, Z., Liu, L., Wang, L., and Zhou, L. R2gengpt: Radiol-\nogy report generation with frozen llms. Meta-Radiology,\n1(3):100033, 2023.\nWang, Z., Tian, Y., Wang, H., and Song, Y.\nExplain-\nable multimodal aspect-based sentiment analysis with\ndependency-guided large language model. arXiv preprint\narXiv:2601.06848, 2026.\nWu, J., Ding, Y., Liu, G., Xia, T., Huang, Z., Sui, D., Liu,\nQ., Wu, S., Wang, L., and Tan, T. Sharp: Steering hal-\nlucination in lvlms via representation engineering. In\nProceedings of the 2025 Conference on Empirical Meth-\nods in Natural Language Processing, pp. 14357‚Äì14372,\n2025.\nZhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B.,\nXie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embed-\nding: Advancing text embedding and reranking through\nfoundation models. arXiv preprint arXiv:2506.05176,\n2025.\n11\n"}]}