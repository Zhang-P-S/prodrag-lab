{"doc_id": "arxiv:2602.13272", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.13272.pdf", "meta": {"doc_id": "arxiv:2602.13272", "source": "arxiv", "arxiv_id": "2602.13272", "title": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks", "authors": ["Muyan Weng", "Defu Cao", "Wei Yang", "Yashaswi Sharma", "Yan Liu"], "published": "2026-02-05T01:02:19Z", "updated": "2026-02-05T01:02:19Z", "summary": "It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.13272v1", "url_pdf": "https://arxiv.org/pdf/2602.13272.pdf", "meta_path": "data/raw/arxiv/meta/2602.13272.json", "sha256": "fb58072197ec86939ee8a993705fcc01f90c5c43dde95e931cfc4b09280a6392", "status": "ok", "fetched_at": "2026-02-18T02:19:45.311714+00:00"}, "pages": [{"page": 1, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents\non Contextual and Event-Informed Time Series Tasks\nMuyan Weng\nmuyanwen@usc.edu\nUniversity of Southern California\nLos Angeles, California, USA\nDefu Cao\ndefucao@usc.edu\nUniversity of Southern California\nLos Angeles, California, USA\nWei Yang\nwyang930@usc.edu\nUniversity of Southern California\nLos Angeles, California, USA\nYashaswi Sharma\nyashaswi@usc.edu\nUniversity of Southern California\nLos Angeles, California, USA\nYan Liu\nyanliu.cs@usc.edu\nUniversity of Southern California\nLos Angeles, California, USA\nAbstract\nLarge language model (LLM)–based agents are increasingly applied\nto time-series forecasting tasks, yet existing evaluations primarily\nemphasize numerical accuracy, making it unclear whether strong\nforecasting performance reflects genuine temporal understanding\nor the ability to reason under contextual and event-driven condi-\ntions. We introduce TemporalBench, a multi-domain benchmark\ndesigned to evaluate temporal reasoning behavior under progres-\nsively richer informational settings. TemporalBench adopts a four-\ntier task taxonomy that examines historical structure interpreta-\ntion, context-free forecasting, contextual temporal reasoning, and\nevent-conditioned prediction across four real-world domains: retail,\nhealthcare, energy, and physical systems. By controlling access to fu-\nture targets and contextual information, the benchmark enables a di-\nagnostic analysis of whether models can correctly interpret tempo-\nral patterns, align them with external context, and adapt predictions\nwhen conditions change. Extensive baseline experiments show that\nstrong numerical forecasting accuracy does not reliably translate\ninto robust contextual or event-aware temporal reasoning; instead,\nexisting agent frameworks exhibit fragmented strengths and sys-\ntematic failure modes that remain largely hidden under forecasting-\nonly benchmarks. The TemporalBench dataset is publicly available\nat https://huggingface.co/datasets/Melady/TemporalBench, and we\nadditionally provide a public leaderboard at https://huggingface.co/\nspaces/Melady/TemporalBench_Leaderboard.\nCCS Concepts\n• Computing methodologies →Machine learning.\nKeywords\nTime Series Benchmarking, Temporal Reasoning, Large Language\nModels, Agent-based Systems, Event-aware Forecasting\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym ’XX, Woodstock, NY\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu. 2018.\nTemporalBench: A Benchmark for Evaluating LLM-Based Agents on Con-\ntextual and Event-Informed Time Series Tasks. In Proceedings of Make\nsure to enter the correct conference title from your rights confirmation email\n(Conference acronym ’XX). ACM, New York, NY, USA, 19 pages. https:\n//doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nTime-series data from real-world systems are rarely generated by a\nsingle, stationary process. Instead, they arise from evolving environ-\nments in which external conditions, interventions, and unexpected\nevents repeatedly perturb underlying dynamics.[32, 46] Across do-\nmains such as retail, healthcare and energy, temporal patterns are\nshaped by contextual factors that influence how historical behavior\nshould be interpreted and how future outcomes should be antici-\npated. Consequently, effective time-series forecasting in practice\nrequires reasoning about temporal signals under changing condi-\ntions, rather than simple extrapolation of past observations.\nDespite this inherent complexity, much of the existing time-\nseries evaluation landscape focuses on simplified settings. To en-\nable controlled comparison and scalable benchmarking, widely used\ndatasets often abstract away contextual variation and event-driven\nregime changes, emphasizing numerical prediction accuracy un-\nder relatively stable assumptions.[9, 38, 39] While this paradigm\nhas driven substantial progress in forecasting and foundation mod-\nels [2, 7, 8, 48], it also raises a key concern: strong performance\nunder simplified benchmarks may not reflect temporal understand-\ning that transfers to real-world scenarios. In particular, it remains\nunclear whether forecasting accuracy reflects genuine temporal\nunderstanding or merely effective numerical extrapolation.\nRecent advances in large language models (LLMs) and agent-\nbased systems further amplify this tension. LLM-based agents\nhave demonstrated strong capabilities across a range of complex\ntasks, including software engineering, scientific discovery, robot-\nics planning, and data analysis [5, 11, 22, 37, 40]. By combining\ninstruction following, multi-step reasoning, tool use, and access\nto external knowledge, agent frameworks offer a flexible para-\ndigm for tackling problems that extend beyond single-turn pre-\ndiction [6, 35, 36, 49, 50]. Motivated by these successes, recent work\nhas begun to explore whether agentic approaches can also ben-\nefit time-series analysis and forecasting, particularly in settings\narXiv:2602.13272v1  [cs.AI]  5 Feb 2026\n"}, {"page": 2, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\ninvolving heterogeneous information and more complex decision\nlogic [51–55].\nThis line of work implicitly assumes that agent-based systems\ncan move beyond numerical prediction to reason about time-series\nbehavior under context. However, existing evaluation protocols\nmake this assumption difficult to verify. Forecasting benchmarks\nprimarily assess numerical prediction error, without evaluating\nwhether models correctly interpret historical structure, align con-\ntextual information with temporal segments, or adapt predictions\nwhen conditions change [1, 38]. Conversely, temporal reasoning\nbenchmarks for language models typically operate on textual data\nalone, omitting real numerical time-series signals and the dynamics\nthey encode [14, 34, 44]. As a result, it remains unclear whether\nagent-based systems genuinely improve contextual temporal rea-\nsoning, or whether apparent gains instead reflect evaluation pro-\ntocols that conflate numerical accuracy with broader reasoning\ncapabilities.\nA central challenge in bridging this gap lies in the role of con-\ntext. In real-world time-series applications, contextual information\ndoes not merely refine numerical estimates; it shapes how tem-\nporal patterns should be interpreted. Context determines which\ntemporal segments are comparable, which changes are meaning-\nful, and how historical behavior should generalize to the future.\nEvents such as promotions, policy changes, medical interventions,\nor extreme weather can induce regime shifts that invalidate naive\nextrapolation [42, 45]. Evaluating temporal reasoning in such set-\ntings therefore requires benchmarks that incorporate context and\nevents as first-class components, rather than treating them as aux-\niliary inputs.\nTo address these challenges, we introduce TemporalBench, a\nmulti-domain benchmark for evaluating agent and LLM perfor-\nmance on time-series tasks under progressively richer contextual\nand event-driven settings. TemporalBench spans four real-world\ndomains—retail, healthcare, energy, and physical systems—and\nadopts a diagnostic task-centric design with a four-tier taxonomy\n(T1–T4) that disentangles historical understanding, context-free\nprediction, contextual reasoning, and event-informed forecasting.\nBy controlling access to future targets and contextual information,\nthe benchmark enables systematic analysis of how temporal signals\nare interpreted and reused. Through extensive baseline evaluation,\nwe show that strong forecasting accuracy does not reliably translate\ninto robust contextual or event-aware temporal reasoning.\nOverall, our contributions are as follows:\n• We introduce TemporalBench, a multi-domain benchmark\nfor evaluating agent and LLM performance on time-series\ntasks under contextual and event-informed settings.\n• We propose a four-tier task taxonomy (T1–T4) that disen-\ntangles historical understanding, pure temporal prediction,\ncontextual reasoning, and event-conditioned forecasting.\n• We develop a unified benchmark construction pipeline with\nautomatic task generation, rule-based ground-truth labeling,\nand explicit uncertainty modeling.\n• Through extensive baseline evaluation, we demonstrate that\nstrong forecasting accuracy does not reliably translate into\nrobust contextual or event-aware temporal reasoning.\n2\nTemporalBench\n2.1\nBenchmark Overview\nBenchmark\nTS-Involved\nContext\nReasoning\n#Tasks\nTask Type\nTest of Time [14]\n✗\n✗\n✓\n1\nQA\nTRAM [44]\n✗\n✗\n✓\n1\nQA\nTSI-Bench [13]\n✓\n✗\n✗\n1\nTS Analysis\nTSB-AD [30]\n✓\n✗\n✗\n1\nTS Analysis\nGIFT-Eval [1]\n✓\n✗\n✗\n1\nTS Analysis\nTFB [38]\n✓\n✗\n✗\n1\nTS Analysis\nTime-MMD [28]\n✓\n✗\n✗\n1\nTS Analysis\nCiK [45]\n✓\n✓\n✗\n1\nTS Analysis\nTGTSF [47]\n✓\n✓\n✗\n1\nTS Analysis\nLLM TS Struggle [33]\n✓\n✗\n✓\n2\nQA, TS Analysis\nMTBench [10]\n✓\n✗\n✓\n3\nQA, TS Analysis\nChatTime [41]\n✓\n✗\n✓\n3\nQA, TS Analysis\nTemporalBench(Ours)\n✓\n✓\n✓\n4\nQA, TS Analysis\nTable 1: Comparison of TemporalBench with existing tem-\nporal and time-series related benchmarks. TS-Involved indi-\ncates whether real numerical time-series are included. Con-\ntext indicates whether external contextual information is\nexplicitly incorporated.\nWe introduce TemporalBench, a multi-domain benchmark de-\nsigned to systematically evaluate agent performance on time-series\ntasks that require temporal understanding, prediction, and reason-\ning under contextual and event-driven settings. Rather than treating\ntime-series evaluation as a single forecasting problem, Temporal-\nBench adopts a task-centric design that explicitly probes different\ntemporal competencies through controlled task construction.\nTemporalBench is built upon four real-world time-series datasets,\nspanning retail (FreshRetailNet-50K[43]), healthcare (MIMIC-\nIV[24]), energy (PSML[56]), and physical system domains (Causal\nChambers[16]). Across these heterogeneous sources, we apply a\nunified benchmark construction pipeline that transforms raw nu-\nmerical time-series into a structured set of evaluation tasks. An\noverview of this pipeline is illustrated in Figure 1, which shows how\nevent generation or detection, prompt assembly, and format-aware\ntask construction jointly give rise to the four task families (T1–T4).\nThe resulting benchmark comprises 2,775 evaluation tasks in-\nstantiated over 191 distinct time-series instances. For each instance,\nmultiple tasks are generated to probe complementary aspects of\ntemporal intelligence, including intrinsic structure understand-\ning, future prediction, contextual reasoning, and event-informed\ndecision-making.\nAs summarized in Table 1, TemporalBench differs from prior\ntemporal and time-series benchmarks by jointly incorporating real\nnumerical time-series signals, explicit contextual information, and\nreasoning-oriented task formulations across multiple task types.\nTasks are instantiated as either multiple-choice reasoning questions\nor numerical forecasting objectives, enabling fine-grained analysis\nof agent behavior across distinct temporal reasoning regimes.\nIn the following sections, we describe the task taxonomy and\nbenchmark construction pipeline in detail.\n2.2\nTask Families\nWe organize TemporalBench into four task families (T1–T4) to ex-\namine distinct aspects of temporal reasoning under progressively\nrelaxed assumptions, rather than to incrementally increase task\n"}, {"page": 3, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nWhat is the trend of the series?\nIs there periodic structure?\nAre there any anomalies?\n                  T2 Task — Context-   \n                Free Forecasting\n            Input: Historical time series\n       Goal: Forecast future values under   \n    unchanged conditions\n  T3 Task — Contextual \nUnderstanding\nInput: Historical time series + context\nGoal: Understand how context \naffects temporal \ninterpretation\n  T1 Task — Historical\nUnderstanding\nInput: Historical time series\nGoal: Understand intrinsic temporal \nstructure\n  T4 Task — Event-                       \n Informed Forecasting\n       Input: Historical time series + context   \n             + future events\n                 Goal: Forecast future behavior     \n                     under changed conditions\nTask Family\nUnderstanding Task\nForecasting Task\nTemporal\nStructure\nContextual\nConstraints\n1\nWhat is the future trend?\nWill volatility increase/decrease?\nWill seasonal patterns persist?\nWhich segment is larger?\nDoes the event increase the series?\nWhich period is more volatile?\nAgent\n  Forecasting\n  Reasoning\nAgent\nAgent\nTesting\nCapabilities\nSynthesized Events\nDetected Events\nT1\nData\nSource\n• Promotion\n• Holiday\n• Storm ...\n• Mutation\n• Annotation\n✔️ Time Series: [1.0, 0.5, 0.8 ...]\n ✔️ Format Requirements\n ✔️ Background: This sequence\nrecords the store's daily sales...\n ✔️ Event Description: A\npromotion will happen...\nT2\nT3\nT4\nGenerated Task\nTask Generation\nUnderstanding\n2\n3\nFigure 1: Overview of the task generation pipeline in our benchmark, illustrating how raw time-series data are transformed\ninto T1–T4 tasks through event generation or detection, prompt assembly, and format-aware task construction.\ndifficulty. The four families form a coherent evaluation sequence\nthat probes how temporal understanding and forecasting behav-\nior change as additional sources of information are introduced.\nConcretely, tasks vary along two key factors: whether the focus is\non interpreting historical behavior or predicting future out-\ncomes, and whether reasoning is performed using only numerical\ntime-series signals or additionally conditioned on contextual\nand event information. For each time-series instance, data are\npartitioned into historical and future segments using a predefined\nevent boundary, and a complete set of tasks spanning all four fami-\nlies is generated. T1 and T2 operate solely on numerical time-series\nsignals, while T3 and T4 further incorporate textual context re-\nflecting realistic application scenarios. This unified construction\nenables controlled comparison across task families, ensuring that\nobserved performance differences arise from changes in temporal\nand contextual reasoning requirements rather than from uncon-\ntrolled variation in data or task format.\nT1: Historical Time-Series Understanding. T1 tasks evaluate whether\nan agent can correctly interpret the structure and behavior of a time\nseries based solely on historical observations. Agents are provided\nwith a historical time-series segment and asked multiple-choice\nquestions about intrinsic temporal properties, including trend direc-\ntion, volatility level, seasonality, and the presence of anomalies. By\nisolating historical interpretation from prediction and context, T1\nserves as a probe of an agent’s ability to extract and reason about\ntemporal structure itself.\nT2: Future Time-Series Prediction without Context. T2 tasks focus\non predicting the future segment of a time series using historical ob-\nservations alone. Agents are required to either produce numerical\nforecasts or answer multiple-choice questions concerning antici-\npated trends, volatility changes, or seasonal patterns. By excluding\ncontextual information, T2 aligns with traditional forecasting set-\ntings while enabling direct comparison between numerical accuracy\nand qualitative expectations. Importantly, T1 and T2 do not form an\nincreasing difficulty hierarchy; rather, they isolate complementary\ncompetencies related to interpreting past structure and extrapolat-\ning future behavior from temporal signals alone.\nT3: Contextual Reasoning over Time-Series. T3 tasks introduce a\nfundamental shift from pattern interpretation to contextual tempo-\nral reasoning. In addition to historical time-series data, agents are\nprovided with textual context that grounds the series in domain\nsemantics and real-world conditions. Rather than asking what the\nseries looks like, T3 questions probe whether an agent can explain,\ncompare, and reason about temporal behavior in light of contextual\ndescriptions.\nTo systematically localize reasoning failures, T3 tasks are con-\nstructed along six capability dimensions, which together form a\nminimal basis set for contextual temporal reasoning: C1 Align-\nment (mapping natural-language conditions to temporal fields or\nwindows), C2 Slicing (comparing temporal segments across peri-\nods or states), C3 Difference Judgment (reasoning about distri-\nbutional differences), C4 Lag (identifying delayed or offset effects),\nC5 Structure (recognizing higher-level temporal patterns such as\npeaks and change points), and C6 Interaction Understanding\n(reasoning about joint or interaction effects). For each time-series\ninstance, multiple questions are generated to cover a subset of these\ndimensions, enabling fine-grained attribution of reasoning errors.\nT4: Event-Informed Time-Series Prediction. T4 tasks evaluate event-\nconditioned and counterfactual temporal reasoning by requiring\nagents to predict how future time-series behavior would change un-\nder specified upcoming events. Along with historical observations,\nagents receive background information describing the application\ncontext and textual descriptions of future events expected to influ-\nence the series, such as extreme weather or other domain-specific\ninterventions. Agents must integrate historical patterns, contextual\nsemantics, and event information to produce numerical forecasts or\nqualitative judgments about future trends, volatility, or seasonality.\nT4 therefore probes whether temporal information can be reused\n"}, {"page": 4, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\nfor conditional reasoning, rather than merely extrapolated under\nunchanged assumptions.\nDiagnostic Role. Together, T1–T4 enable TemporalBench to sep-\narate numerical prediction from temporal interpretation, and to\nfurther test whether temporal information can be aligned with\ncontext and used for reasoning under changing conditions. This\ndecomposition allows experimental results to be traced back to\nspecific temporal competencies, a property that is essential for\ndiagnosing failure modes in agentic time-series systems.\n2.3\nDataset Transformation Pipeline\nWe construct TemporalBench through a unified dataset transfor-\nmation pipeline designed to ensure methodological consistency,\nreproducibility, and comparability across heterogeneous time-series\ndomains. The pipeline consists of two key components: (i) event\ninjection or detection to define historical–future boundaries, and\n(ii) ground truth generation to support systematic evaluation across\ntask families.\n2.3.1\nEvent Injection and Detection. Events play a central role in\nTemporalBench by defining the boundary between historical and\nfuture segments of each time series and enabling event-centric\ntask construction. However, the four datasets in our benchmark\ndiffer substantially in how events are recorded or manifested. Some\ndatasets contain dense temporal measurements with sparse or un-\nreliable event annotations, while others naturally exhibit explicit\nevents or intervention signals.\nTo accommodate this heterogeneity within a unified benchmark\ndesign, we adopt two complementary strategies: event injection and\nevent detection. The goal of event injection is not to faithfully simu-\nlate real-world causal mechanisms, but to introduce controlled and\ninterpretable regime boundaries that support systematic evaluation\nof event-aware temporal reasoning. Accordingly, evaluation focuses\non whether agents appropriately condition their predictions and\ndecisions on the presence of an event, rather than on the realism\nor magnitude of the injected effect.\nFreshRetailNet-50K and PSML lack reliable event annotations rel-\native to their dense temporal records. For these datasets, we inject\na salient event at a selected time point and simulate its impact on\nthe subsequent segment. To reduce bias from imperfect simulation,\ninjected events are designed to be coarse and distinctive, and down-\nstream tasks emphasize qualitative changes between historical and\nfuture segments, such as shifts in trend, volatility, or seasonality,\nrather than precise numerical effects.\nIn contrast, MIMIC-IV and Causal Chambers naturally support\nevent detection. MIMIC-IV provides explicit clinical event annota-\ntions, including procedures, medication administration, and patient\ntransfers. Causal Chambers does not include explicit event labels,\nbut interventions in the underlying physical system often induce\nabrupt changes in observed variables, which we detect as change\npoints. Under both strategies, each time series is partitioned into a\nhistorical segment and a future segment by a well-defined event, en-\nabling consistent task construction across heterogeneous datasets.\n2.3.2\nGround Truth Generation. Ground truth labels in Tempo-\nralBench are generated automatically from time-series data us-\ning unified, rule-based procedures, without manual annotation or\nmodel-dependent heuristics. Label generation follows three core\nprinciples: independence from contextual descriptions, robustness\nacross heterogeneous domains, and explicit handling of uncertainty.\nAcross all datasets and task families, labels are computed solely\nfrom historical and future time-series segments. Contextual descrip-\ntions and event narratives are used only to define the historical–\nfuture split and to condition task prompts, and they do not influence\nany labeling rule. This design prevents information leakage from\nevent injection or detection and ensures that models cannot succeed\nby exploiting prompt structure rather than temporal evidence.\nTo ensure robustness across domains and scales, we rely on\nstatistically stable estimators, including medians, median absolute\ndeviation (MAD), interquartile range (IQR), and Theil–Sen slopes.\nLabels are defined primarily in terms of relative changes rather\nthan absolute values. When observed effects fail to meet predefined\nstrength or support criteria, we assign Uncertain labels instead of\nforcing unreliable decisions.\nGround truth generation is task-specific. T1 targets intrinsic\nproperties of historical segments, such as trend, volatility, sea-\nsonality, and anomalies, using robust statistics and window-based\ncomparisons. T2 and T4 focus on future behavior, with numerical\nground truth given by observed future values and auxiliary multiple-\nchoice labels describing qualitative changes relative to history. T3\nemphasizes higher-level temporal reasoning over historical data,\ngenerating labels for questions involving segment comparisons,\nlagged responses, and interaction effects, with only statistically\nreliable instances retained.\nAdditional implementation details and pseudocode for event\ninjection or detection and ground-truth labeling are provided in\nAppendix D.\n3\nExperiments\nThe experiments in this section are designed to systematically eval-\nuate the temporal competencies isolated by the four task families in\nTemporalBench. Rather than treating performance as a single aggre-\ngate score, we analyze agent behavior across T1–T4 to disentangle\ndistinct aspects of temporal intelligence.\nBy evaluating different agent paradigms under identical inputs\nand constraints, our experiments aim to answer three guiding ques-\ntions: (i) whether strong numerical forecasting performance trans-\nlates into reliable temporal interpretation, (ii) how the introduction\nof contextual information alters reasoning behavior over time-series\ndata, and (iii) whether agents can meaningfully adapt their predic-\ntions under explicitly specified future events. Each experiment is\ntherefore diagnostic in nature, with observed failure modes mapped\nback to specific task families and reasoning dimensions rather than\ntreated as isolated errors.\n3.1\nExperiment Setup\nOur experimental design aims to evaluate agentic temporal rea-\nsoning capabilities in a controlled and diagnostic manner, rather\nthan to optimize predictive accuracy. In particular, the setup is de-\nsigned to disentangle the effects of agent architecture, backbone\nlanguage model capacity, and evaluation protocol across different\ntask families in TemporalBench.\n"}, {"page": 5, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nAgent\nDataset\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nTask\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nSingle LLM\nSR\n100%\n100%\n84.66%\n100%\n100%\n100%\n100%\n96%\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n100%\nACC\n63.64%\n52.27%\n2.89%\n13.64%\n67.50%\n20.67%\n34.80%\n36.00%\n13.33%\n27.33%\n35.20%\n26.00%\n46.81%\n21.28%\n36.61%\n29.79%\nTimeSeries\nScientist\nSR\n85.80%\n100.00%\n99.43%\n100.00%\n83.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n86.17%\n100.00%\n100.00%\n100.00%\nACC\n33.52%\n56.82%\n3.41%\n56.82%\n28.00%\n26.67%\n21.60%\n27.33%\n28.67%\n2.67%\n21.60%\n2.67%\n10.11%\n23.40%\n28.87%\n23.40%\nAgentScope\nSR\n100.00%\n100.00%\n96.59%\n100.00%\n100.00%\n98.00%\n100.00%\n98.00%\n75.00%\n100.00%\n99.60%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n62.50%\n12.12%\n13.64%\n18.94%\n66.00%\n24.67%\n27.20%\n35.33%\n12.00%\n46.00%\n44.00%\n32.00%\n44.68%\n21.28%\n23.95%\n22.70%\nMetaGPT\nSR\n100.00%\n100.00%\n94.32%\n100.00%\n100.00%\n98.00%\n100.00%\n100.00%\n75.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n62.50%\n9.09%\n5.11%\n14.39%\n67.50%\n21.09%\n22.00%\n31.33%\n10.67%\n59.33%\n45.20%\n16.00%\n45.74%\n17.02%\n28.97%\n25.53%\nCAMEL\nSR\n100.00%\n100.00%\n92.05%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n75.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n64.20%\n0.76%\n6.25%\n31.06%\n68.50%\n14.00%\n18.40%\n30.67%\n10.00%\n66.00%\n42.00%\n26.67%\n46.81%\n20.57%\n30.14%\n23.40%\n(a) Multi-choice task performance across datasets.\nAgent\nMetric\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nT2\nT4\nT2\nT4\nT2\nT4\nT2\nT4\nSingle LLM\nSR\n97.73%\n79.55%\n14.00%\n38.00%\n44.00%\n34.00%\n91.49%\n93.62%\nMAE / OW_sMAPE\n0.12\n0.34\n0.61\n0.44\n2.48\n2.58\n15.20\n16.86\nsMAPE / OW_RMSSE\n1.27\n1.29\n0.60\n0.37\n2.57E-05\n2.69E-05\n0.55\n0.63\nTimeSeries\nScientist\nSR\n43.18%\n79.55%\n26.00%\n52.00%\n50.00%\n32.00%\n89.36%\n89.36%\nMAE / OW_sMAPE\n0.35\n0.51\n1.53\n0.84\n2.44\n2.94\n15.81\n17.18\nsMAPE / OW_RMSSE\n1.27\n1.40\n0.65\n0.48\n2.53E-05\n3.06E-05\n0.52\n0.64\nAgentScope\nSR\n41.94%\n23.94%\n5.26%\n31.58%\n45.10%\n27.45%\n93.75%\n89.80%\nMAE / OW_sMAPE\n0.12\n0.20\n0.28\n0.35\n2.76\n2.66\n11.05\n12.02\nsMAPE / OW_RMSSE\n126.27\n130.86\n37.38\n30.51\n2.62E-03\n2.46E-03\n0.43\n0.49\nMetaGPT\nSR\n90.91%\n88.64%\n16.00%\n64.00%\n52.00%\n40.00%\n93.62%\n93.62%\nMAE / OW_sMAPE\n0.13\n0.24\n0.34\n0.40\n2.62\n2.76\n14.11\n15.40\nsMAPE / OW_RMSSE\n126.59\n127.22\n24.74\n43.47\n2.72E-03\n2.87E-03\n0.53\n0.63\nCAMEL\nSR\n93.18%\n93.18%\n34.00%\n62.00%\n52.00%\n30.00%\n95.74%\n93.62%\nMAE / OW_sMAPE\n0.13\n0.28\n0.43\n0.45\n2.99\n2.50\n12.02\n15.74\nsMAPE / OW_RMSSE\n126.75\n128.18\n34.89\n35.78\n3.11E-03\n2.60E-03\n0.55\n0.59\n(b) Forecasting performance on T2 and T4 tasks across datasets.\nTable 2: Performance of different agent frameworks across multi-choice and forecasting tasks using gpt-4o as the base model.\n3.1.1\nModels and Agents. We evaluate both time-series–specialized\nagents and general-purpose LLM-based agents to assess their per-\nformance on the proposed benchmark. Given the limited availabil-\nity of agent systems explicitly designed for time-series tasks, we\ninclude TimeSeriesScientist [55] as a representative time-series–\nspecific agent. We emphasize that TimeSeriesScientist is used to\nrepresent domain-specialized, structure-aware agentic approaches,\nrather than as an exhaustive benchmark of time-series modeling\ntechniques.\nIn addition, we evaluate several general-purpose agent frame-\nworks that have been widely adopted for reasoning and decision-\nmaking tasks, including MetaGPT [20], AgentScope [17], and\nCAMEL [26]. These frameworks are not specialized for time-series\ndata and primarily rely on language-based reasoning and tool or-\nchestration, making them suitable baselines for assessing temporal\nreasoning in the absence of domain-specific inductive biases.\nTo isolate the contribution of agent frameworks themselves, we\nfurther include a direct LLM prompting baseline, in which a single\nlarge language model is queried without agent structure, tool use,\nor multi-step interaction. This setting reflects common practice and\nprovides a reference point for comparing agent-based approaches\nagainst raw LLM capabilities.\nAll agents and baselines are instantiated with a common set of\nbackbone language models to ensure controlled comparison. The\nbackbone models include GPT-4o [21], Gemini-2.5-Flash [19],\nClaude-3-7-Sonnet [3], DeepSeek-Chat [27], and Qwen-Plus [4].\nAcross all settings, agents receive identical task inputs, output con-\nstraints, and access to information, ensuring that performance dif-\nferences reflect agent design rather than model capacity.\nWe do not treat standalone forecasting models as primary base-\nlines, as TemporalBench targets agentic temporal reasoning and\ndecision-making rather than pure predictive accuracy. Traditional\nforecasting performance is therefore used as a reference signal\nrather than a proxy for task competence.\n3.1.2\nEvaluation Metrics. We adopt evaluation metrics that re-\nflect the distinct objectives of reasoning-oriented and prediction-\noriented tasks in TemporalBench. For multiple-choice question-\nanswering tasks, including T1, T3, and the qualitative components\nof T2 and T4, we report accuracy as the primary metric. Accuracy\nis chosen to emphasize unambiguous decision correctness, aligning\nwith the benchmark’s goal of identifying discrete reasoning failures\nrather than grading partial or stylistic explanations.\nQuestions labeled as Uncertain or Inconclusive are excluded from\naccuracy computation for all models. This ensures that evaluation\n"}, {"page": 6, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\nfocuses on instances with sufficient statistical support and avoids\npenalizing models on inherently ambiguous cases.\nFor numerical forecasting tasks in T2 and T4, we evaluate predic-\ntion quality using error-based metrics computed between predicted\nand ground-truth future values over the valid forecast horizon.\nFor datasets involving single or low-dimensional target series (e.g.,\nFreshRetailNet, PSML, and Causal Chambers), we report stan-\ndard forecasting metrics including mean absolute error (MAE) and\nsymmetric mean absolute percentage error (sMAPE).\nFor the MIMIC dataset, forecasting involves multiple correlated\nclinical time series with heterogeneous scales. To ensure fair ag-\ngregation across series and avoid domination by high-variance sig-\nnals, we use overall weighted metrics, specifically OW_sMAPE and\nOW_RMSSE. These metrics are standard in multi-series forecasting\nand enable fair comparison across differing scales and prediction\nhorizons.\n3.2\nMain Results\nTable 2 summarizes the performance of different agent frameworks\nand a single-LLM baseline across four task families (T1–T4) and\nfour domains, using gpt-4o as the backbone model.\n3.2.1\nGlobal Performance Patterns Across Task Families. Here we\nanalyze how performance evolves across task families as temporal\nreasoning requirements shift from structural interpretation (T1), to\npure prediction without context (T2), to contextual and event-aware\nreasoning (T3 and T4). Across all evaluated methods, Temporal-\nBench reveals a consistent stratification aligned with this design.\nHistorical understanding tasks (T1) achieve moderate accuracy\nfor most agents, indicating that intrinsic temporal properties such as\ntrend direction or volatility are partially accessible when reasoning\nis confined to historical structure alone. In contrast, reasoning-\nintensive tasks that require grounding temporal patterns in context\n(T3) remain uniformly challenging: accuracy frequently drops to\nsingle-digit or low double-digit percentages in retail and clinical\ndomains, even when the same models perform reasonably well on\nT1. This sharp degradation highlights the difficulty of contextual\ntemporal reasoning beyond structural interpretation.\nPrediction-related multiple-choice tasks (T2 and T4) exhibit sub-\nstantially higher variance across agents than historical understand-\ning tasks. Notably, inferring future behavior in a discrete, quali-\ntative label space proves markedly more difficult than producing\nnumerical forecasts. Across datasets, improvements in numerical\nforecasting accuracy (e.g., lower MAE or sMAPE) do not reliably\ntranslate into higher accuracy on qualitative future-oriented judg-\nments. This decoupling between numerical prediction quality and\nqualitative temporal interpretation directly reflects the distinction\nthat TemporalBench is designed to expose.\nKey Discovery. Temporal reasoning performance degrades\nsharply as tasks move from structural understanding (T1) to\ncontextual reasoning (T3), and strong numerical forecasting\naccuracy does not reliably translate into accurate qualitative\njudgments about future behavior (T2/T4).\n3.2.2\nAgent Frameworks versus Direct LLM Prompting. This sub-\nsection evaluates whether agent orchestration provides systematic\nbenefits over direct LLM prompting across the temporal competen-\ncies isolated by T1–T4. Comparing agent-based methods with direct\nLLM prompting reveals that agent frameworks do not consistently\nimprove temporal reasoning performance.\nOn qualitative tasks that require either historical interpretation\n(T1) or context-free future reasoning (T2), the single-LLM baseline\nfrequently matches or exceeds the accuracy of general-purpose\nagents, particularly on FreshRetailNet and MIMIC. These results\nsuggest that when temporal structure must be inferred directly\nfrom numerical sequences, additional agent steps can introduce\nreasoning noise rather than providing effective decomposition or\nabstraction.\nForecasting tasks present a more nuanced pattern. While the\nsingle LLM achieves competitive MAE and sMAPE on relatively\nstructured domains such as FreshRetailNet and MIMIC, agent-based\nmethods occasionally reduce numerical error on PSML and Causal\nChambers. However, these gains are inconsistent across domains\nand task families, indicating that current agent frameworks lack a\nrobust and general mechanism for exploiting temporal regularities\nbeyond what is already captured by the backbone model.\nKey Discovery. Agent orchestration alone does not guarantee\nimproved temporal reasoning, and can introduce additional\nnoise when temporal structure must be inferred directly from\nnumerical time-series data.\n3.2.3\nTime-Series-Specific Agent versus General-Purpose Agents.\nThis subsection contrasts domain-specific temporal modeling with\ngeneral-purpose agent reasoning across the disentangled competen-\ncies evaluated by TemporalBench. The time-series–specific agent\ndoes not consistently outperform general-purpose agents across\ntask families, but it exhibits a distinct and systematic performance\nprofile.\nOn prediction-oriented tasks (T2 and T4), its error metrics are\ncomparatively stable across domains, remaining within a narrower\nrange than those of general-purpose agents. This stability aligns\nwith its design emphasis on explicit time-series modeling and nu-\nmerical consistency. However, this advantage does not extend to\nqualitative reasoning tasks.\nOn multiple-choice tasks, particularly those requiring contextual\ngrounding and higher-level reasoning (T3), the time-series-specific\nagent offers limited improvement and often underperforms general-\npurpose agents. These results indicate that while specialized tem-\nporal architectures can stabilize numerical prediction, they do not\ndirectly address challenges such as abstraction, comparison across\ntemporal segments, or conditional reasoning under context, which\nare central to T3 and T4.\nKey Discovery. Domain-specific temporal modeling stabilizes\nnumerical prediction (T2/T4) but does not address higher-\nlevel reasoning challenges such as abstraction, comparison, or\nconditional reasoning under context (T3).\n"}, {"page": 7, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\n3.2.4\nDomain-Specific Effects and Event-Aware Reasoning. This sub-\nsection examines how domain characteristics interact with the\nevent-aware reasoning requirements introduced in T4. Performance\nvaries substantially across domains. Physical and semi-synthetic\ndatasets such as PSML and Causal Chambers generally yield higher\naccuracy and more stable forecasting errors, reflecting clearer tem-\nporal structure and stronger signal-to-noise ratios. In contrast,\nFreshRetailNet and MIMIC exhibit consistently lower accuracy, par-\nticularly on reasoning-intensive (T3) and event-aware (T4) tasks,\nhighlighting the challenges posed by noisy dynamics, confounding\nfactors, and heterogeneous event effects.\nEvent-informed prediction tasks further expose limitations in\nhow agents utilize contextual and event information. While incor-\nporating event descriptions occasionally improves numerical fore-\ncasting relative to context-free prediction (T2) in specific domains,\nqualitative accuracy does not consistently benefit from the pres-\nence of event narratives. This pattern suggests that current agents\nstruggle to translate event descriptions into actionable temporal\nconstraints, underscoring the importance of explicitly evaluating\nevent-conditioned temporal reasoning rather than assuming that\ncontextual integration emerges implicitly.\nKey Discovery. Event descriptions are not consistently trans-\nlated into actionable temporal constraints by current agents,\nrevealing a gap between contextual awareness and event-\nconditioned temporal reasoning (T4).\nOverall, these results show that existing agent frameworks ex-\nhibit fragmented strengths across the temporal competencies eval-\nuated by T1–T4. By disentangling historical understanding, pre-\ndiction, contextual reasoning, and event-conditioned forecasting,\nTemporalBench reveals gaps between numerical accuracy, temporal\ninterpretation, and event-aware reasoning that are largely hidden\nby forecasting-only benchmarks.\n4\nAnalysis\nAlthough accuracy on some reasoning-oriented tasks is low, Tem-\nporalBench does not exhibit a uniform floor effect. Instead, per-\nformance varies systematically across task families, domains, and\nagent designs, revealing structured capability differences. In this\nsection, we analyze these patterns to characterize how temporal\ncompetencies emerge and fail under controlled settings.\n4.1\nAnalysis of Temporal Reasoning\nTo further understand agent reasoning behavior beyond aggregate\naccuracy, we analyze performance on T3 tasks by decomposing\nthem into six reasoning capabilities (C1–C6). Figure 2 visualizes\naccuracy across all agents and four backbone models. Rather than\nexhibiting uniform improvements, agents show highly uneven ca-\npability profiles.\nAcross all backbone models, C6 (interaction understanding) and\nC5 (structural pattern recognition) consistently achieve higher accu-\nracy than other capabilities. These tasks primarily rely on recogniz-\ning co-occurring conditions, peak patterns, or regime-level changes,\nwhich are more amenable to surface-level pattern matching and\nstatistical cues. In contrast, C2 (slicing-based comparison) and C3\nC1\nC2\nC3\nC4\nC5\nC6\ngpt-4o\nC1\nC2\nC3\nC4\nC5\nC6\nqwen\nC1\nC2\nC3\nC4\nC5\nC6\ndeepseek\nC1\nC2\nC3\nC4\nC5\nC6\ngemini\nsingle-llm\nagentscope\nmetaGPT\nCAMEL\nTimeSeriesScientist\nFigure 2: Radar plots showing the performance of different\nagents on the six T3 reasoning dimensions (C1–C6) under dif-\nferent base LLMs. Each subplot corresponds to a base model,\nand each curve represents an agent.\n(difference judgment) remain challenging for most agents, indicat-\ning persistent difficulty in reliably comparing temporal segments\nor quantifying relative changes.\nComparing agent types, general-purpose agents such as MetaGPT\nand CAMEL exhibit modest gains over the single-LLM baseline on\nhigher-level reasoning dimensions (e.g., C5 and C6), but often un-\nderperform on lower-level analytical capabilities such as C1 (align-\nment) and C4 (lag detection). This suggests that language-driven\norchestration can help with abstract interpretation but does not\nreliably improve fine-grained temporal analysis.\nThe time-series–specific agent shows a distinct but limited ad-\nvantage profile. While its performance does not dominate across\nall six capabilities, it demonstrates more balanced behavior on C1\nand C4, which require explicit temporal alignment and response-\nwindow reasoning. This pattern is consistent with its design em-\nphasis on structured time-series processing. However, it does not\nsubstantially improve performance on higher-level comparative or\ninteraction-based reasoning, reinforcing that specialized temporal\nmodeling alone is insufficient for complex reasoning tasks.\nOverall, the C1–C6 analysis reveals that current agents possess\nfragmented reasoning capabilities rather than unified temporal in-\ntelligence. Strengths in interaction and structure recognition coexist\nwith weaknesses in comparative and causal reasoning, highlighting\nthe need for benchmarks that disentangle reasoning dimensions\nand for agent designs that explicitly address these gaps.\nInsight. Temporal reasoning in current agents appears non-\nuniform, with structural and interaction reasoning (C5–C6)\nmore accessible than comparative and distributional reasoning\n(C2–C3), suggesting that T3 failures are linked to specific\nreasoning gaps rather than data scarcity or model size.\n"}, {"page": 8, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\n4.2\nThe Role of Temporal Representations in\nAgent Reasoning\nTo further diagnose the limitations of general-purpose agents on\ntemporal tasks, we design a lightweight feature extraction and\nvisualization module and integrate it into a representative general\nagent, AgentScope with gpt-4o as the backbone. In contrast to time-\nseries–specific agents, general agents typically ingest temporal data\nby directly serializing numerical values into long textual prompts,\nwithout mechanisms to explicitly expose temporal structure. Such\nraw serialization obscures relationships among values, making it\ndifficult for LLMs to reason about trends, volatility changes, or\nstructural shifts over time.\nModule Design. The proposed module augments raw time-series\ninput with two complementary representations. First, we extract a\ncompact set of structured numerical features summarizing global\nstatistics and multi-scale temporal dynamics. This includes a re-\nduced subset of interpretable catch22 features [31] capturing distri-\nbutional properties, complexity, and outliers, as well as multi-scale\nstructural features such as short-, mid-, and long-term slopes, recent\nvolatility, and shift indicators. All features are described in natural\nlanguage within the prompt to ensure interpretability. Second, we\ngenerate a visualization of the time series as an additional modality,\nexposing relative changes, peaks, and structural patterns that are\ndifficult to infer from raw numeric tokens alone.\nImpact on Temporal Understanding. We evaluate the effect of this\nmodule on AgentScope by comparing performance under different\ninput representations. Table 3 reports results for raw time-series\ninputs, feature-based augmentation, visualization-based augmen-\ntation, and their combination. Overall, the impact of enhanced\nrepresentations is heterogeneous across domains and task families\nrather than uniformly positive.\nIn domains with clearer temporal structure and stronger signal-\nto-noise ratios, such as Causal Chambers, enhanced representations\nlead to consistent improvements on multi-choice prediction and\nreasoning tasks, particularly for future-related judgments (T2) and\nevent-aware prediction (T4). In contrast, for more complex and\nnoisy real-world domains such as FreshRetailNet and PSML, gains\nare limited and sometimes mixed: while historical understanding\n(T1) often benefits from explicit structural cues, performance on\nreasoning-intensive tasks (T3) and future multi-choice prediction\n(T2) does not consistently improve and may even degrade in some\nsettings.\nThese results suggest that explicitly exposing temporal struc-\nture through features and visualizations can alleviate certain failure\nmodes of general agents, especially in structured domains. However,\nrepresentation-level augmentation alone is insufficient to resolve\nbroader challenges of temporal reasoning and decision-making\nin complex real-world time-series. Importantly, all observed ef-\nfects are obtained without additional training, task-specific tuning,\nor changes to the agent workflow, indicating that representation\nquality is a meaningful but partial factor in agent performance on\ntime-series tasks.\nInsight. Explicit temporal representations can partially alle-\nviate reasoning failures, but their effectiveness is domain- and\ntask-dependent. Improvements emerge primarily in settings\nwith clearer temporal structure, suggesting that representa-\ntion quality is a necessary but insufficient condition for robust\ncontextual and event-aware temporal reasoning.\nFreshRetailNet\nPSML\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nRaw\n62.50%\n12.12%\n13.64%\n18.94%\n66.00%\n24.67%\n27.20%\n35.33%\nFeat.\n64.20%\n16.67% 13.64%\n18.94%\n63.00%\n30.61%\n26.80%\n29.33%\nVis.\n63.64%\n9.09%\n10.80%\n20.45%\n67.50%\n26.67%\n22.00%\n34.67%\nBoth 69.32%\n5.30%\n10.80%\n15.15%\n65.00%\n21.77%\n21.20%\n40.67%\nMIMIC\nCausal Chambers\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nRaw\n44.68%\n21.28%\n23.95%\n22.70%\n12.00%\n46.00%\n44.00%\n32.00%\nFeat.\n43.62%\n25.53%\n24.46%\n21.99%\n13.33%\n36.67%\n42.40%\n30.00%\nVis.\n44.68%\n22.70%\n26.02%\n24.11%\n13.33%\n41.33%\n40.00%\n36.00%\nBoth\n44.15%\n24.11%\n25.43%\n24.82%\n10.67%\n52.67%\n42.00%\n38.67%\nTable 3: Multi-choice accuracy (ACC) of AgentScope with\ngpt-4o under different temporal representations. Feat. de-\nnotes feature-based augmentation, Vis. denotes visualization-\nbased augmentation, and Both indicates the combination of\nfeature and visualization augmentations. Bold indicates the\nbest performance within each dataset and task column.\n5\nConclusion\nThis work introduces TemporalBench, a benchmark for systemat-\nically evaluating agent and LLM performance on time-series tasks\nthat require temporal understanding, prediction, and contextual\nawareness. By combining real-world time-series data with event-\ncentric task construction and a four-tier task taxonomy (T1–T4),\nTemporalBench moves beyond traditional forecasting-only evalua-\ntions and enables structured assessment of temporal competence\nunder diverse informational settings.\nOur empirical results show that strong numerical forecasting\nperformance does not reliably translate into correct decisions when\ntasks require contextual grounding or event-conditioned reasoning.\nPerformance varies substantially across task families and domains,\nindicating that temporal competence is not a single unified capa-\nbility. Improvements in forecasting accuracy alone are therefore\ninsufficient to ensure robust behavior in settings that involve con-\ntextual interpretation or conditional prediction.\nTaken together, these findings suggest that current agentic ap-\nproaches to time-series problems exhibit fragmented strengths\nrather than holistic temporal understanding. While some systems\nperform well on isolated aspects of temporal tasks, none consis-\ntently integrate temporal structure, contextual information, and\ndecision requirements across all evaluated settings.\nFrom a broader perspective, TemporalBench highlights the need\nfor evaluation frameworks that disentangle complementary tempo-\nral competencies instead of collapsing them into a single end-to-end\nobjective. By evaluating agents across multiple task types, domains,\nand informational regimes, the benchmark provides a foundation\nfor more principled analysis of agent capabilities and limitations in\nreal-world time-series applications.\n"}, {"page": 9, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nReferences\n[1] Taha Aksu, Gerald Woo, Juncheng Liu, Xu Liu, Chenghao Liu, Silvio Savarese,\nCaiming Xiong, and Doyen Sahoo. 2024. GIFT Eval: A Benchmark for General\nTime Series Forecasting Model Evaluation. arXiv preprint arXiv:2409.16086 (2024).\n[2] Abdul Fatir Ansari, Caner Türkmen, Lorenzo Stella, Oleksandr Shchur, Funda\nTurkmen, et al. 2024. Chronos: Learning the Language of Time Series. arXiv\npreprint arXiv:2403.07815 (2024).\n[3] Anthropic. 2024. Claude 3.7 Sonnet Model Card. https://www.anthropic.com/\nclaude. Accessed 2025.\n[4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,\nWenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint\narXiv:2309.16609 (2023).\n[5] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. 2023.\nEmergent au-\ntonomous scientific research capabilities of large language models. arXiv preprint\narXiv:2304.05332 (2023).\n[6] Defu Cao, Michael Gee, Jinbo Liu, Hengxuan Wang, Wei Yang, Rui Wang, and Yan\nLiu. 2025. Conversational Time Series Foundation Models: Towards Explainable\nand Effective Forecasting. arXiv preprint arXiv:2512.16022 (2025).\n[7] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and\nYan Liu. 2023. Tempo: Prompt-based generative pre-trained transformer for time\nseries forecasting. arXiv preprint arXiv:2310.04948 (2023).\n[8] Defu Cao and Yan Liu. 2025. TimeDiT: Diffusion Transformers Foundation Model\nfor Time Series Forecasting. (2025).\n[9] Ching Chang, Jeehyun Hwang, Yidan Shi, Haixin Wang, Wen-Chih Peng, Tien-Fu\nChen, and Wei Wang. 2025. Time-IMM: A Dataset and Benchmark for Irregular\nMultimodal Multivariate Time Series. arXiv preprint arXiv:2506.10412 (2025).\n[10] Jialin Chen, Aosong Feng, Ziyu Zhao, Juan Garza, Gaukhar Nurbek, Cheng Qin,\nAli Maatouk, Leandros Tassiulas, Yifeng Gao, and Rex Ying. 2025. Mtbench: A\nmultimodal time series benchmark for temporal reasoning and question answer-\ning. arXiv preprint arXiv:2503.16858 (2025).\n[11] Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Xinyu Ma, Wei Yang, Daiting Shi,\nJiaxin Mao, and Dawei Yin. 2025. Tourrank: Utilizing large language models for\ndocuments ranking with a tournament-inspired strategy. In Proceedings of the\nACM on Web Conference 2025. 1638–1652.\n[12] Zhenrui Chu, Yilun Xu, Yikang Li, Hongming Zhang, and Yue Zhang. 2024.\nTimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\nLarge Language Models. In Proceedings of ACL.\n[13] Wenjie Du, Jun Wang, Linglong Qian, Yiyuan Yang, Zina Ibrahim, Fanxing Liu,\nZepu Wang, Haoxin Liu, Zhiyuan Zhao, Yingjie Zhou, et al. 2024. Tsi-bench:\nBenchmarking time series imputation. arXiv preprint arXiv:2406.12747 (2024).\n[14] Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong\nYim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi. 2024.\nTest of time: A benchmark for evaluating llms on temporal reasoning. arXiv\npreprint arXiv:2406.09170 (2024).\n[15] Austin Feng, Andreas Varvarigos, Ioannis Panitsas, Daniela Fernandez, Jinbiao\nWei, Yuwei Guo, Jialin Chen, Ali Maatouk, Leandros Tassiulas, and Rex Ying.\n2025. TelecomTS: A Multi-Modal Observability Dataset for Time Series and\nLanguage Analysis. arXiv preprint arXiv:2510.06063 (2025).\n[16] Juan L Gamella, Jonas Peters, and Peter Bühlmann. 2024. The causal cham-\nbers: Real physical systems as a testbed for ai methodology. arXiv preprint\narXiv:2404.11341 (2024).\n[17] Dawei Gao et al. 2024. AgentScope: A Flexible yet Robust Multi Agent Platform.\narXiv preprint arXiv:2402.14034 (2024).\n[18] Azul Garza and Renée Rosillo. 2025. TimeCopilot. arXiv preprint arXiv:2509.00616\n(2025).\n[19] Google DeepMind. 2024. Gemini 2.5 Flash. https://ai.google.dev/gemini-api/\ndocs/models/gemini. Accessed 2025.\n[20] Sirui Hong et al. 2024. MetaGPT: Meta Programming for a Multi Agent Collabo-\nrative Framework. arXiv preprint arXiv:2308.00352 (2024).\n[21] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh,\nAidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024.\nGpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).\n[22] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir\nPress, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve\nreal-world github issues? arXiv preprint arXiv:2310.06770 (2023).\n[23] Mingxuan Jin, Haixu Zhang, Wenjie Wang, Yasha Wang, et al. 2024. What Can\nLarge Language Models Tell Us about Time Series Forecasting? arXiv preprint\narXiv:2411.09089 (2024).\n[24] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout,\nSteven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al.\n2023. MIMIC-IV, a freely accessible electronic health record dataset. Scientific\ndata 10, 1 (2023), 1.\n[25] Geon Lee, Wenchao Yu, Kijung Shin, Wei Cheng, and Haifeng Chen. 2025. Time-\ncap: Learning to contextualize, augment, and predict time series events with\nlarge language model agents. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 39. 18082–18090.\n[26] Guohao Li et al. 2023. CAMEL: Communicative Agents for Mind Exploration of\nAI Society. In NeurIPS.\n[27] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng-\ngang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437 (2024).\n[28] Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Prab-\nhakar Kamarthi, Aditya Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen,\nChao Zhang, et al. 2024. Time-mmd: Multi-domain multimodal dataset for time\nseries analysis. Advances in Neural Information Processing Systems 37 (2024),\n77888–77933.\n[29] Penghang Liu, Elizabeth Fons, Svitlana Vyetrenko, Daniel Borrajo, Vamsi Potluru,\nand Manuela Veloso. 2025. TS-Agent: A Time Series Reasoning Agent with\nIterative Statistical Insight Gathering. arXiv preprint arXiv:2510.07432 (2025).\n[30] Qinghua Liu and John Paparrizos. 2024. The elephant in the room: Towards a re-\nliable time-series anomaly detection benchmark. Advances in Neural Information\nProcessing Systems 37 (2024), 108231–108261.\n[31] Carl H Lubba, Sarab S Sethi, Philip Knaute, Simon R Schultz, Ben D Fulcher, and\nNick S Jones. 2019. catch22: CAnonical Time-series CHaracteristics: Selected\nthrough highly comparative time-series analysis. Data mining and knowledge\ndiscovery 33, 6 (2019), 1821–1852.\n[32] Fiammetta Menchetti, Fabrizio Cipollini, and Fabrizia Mealli. 2021. Estimating the\ncausal effect of an intervention in a time series setting: the C-ARIMA approach.\narXiv preprint arXiv:2103.06740 (2021).\n[33] Mike A Merrill, Mingtian Tan, Vinayak Gupta, Thomas Hartvigsen, and Tim\nAlthoff. 2024. Language models still struggle to zero-shot reason about time\nseries. In Findings of the Association for Computational Linguistics: EMNLP 2024.\n3512–3533.\n[34] Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, and Dan Roth.\n2020. TORQUE: A reading comprehension dataset of temporal ordering questions.\narXiv preprint arXiv:2005.00242 (2020).\n[35] Heng Ping, Arijit Bhattacharjee, Peiyu Zhang, Shixuan Li, Wei Yang, Anzhe\nCheng, Xiaole Zhang, Jesse Thomason, Ali Jannesari, Nesreen Ahmed, et al. 2025.\nVeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation. arXiv\npreprint arXiv:2510.27617 (2025).\n[36] Heng Ping, Shixuan Li, Peiyu Zhang, Anzhe Cheng, Shukai Duan, Nikos\nKanakaris, Xiongye Xiao, Wei Yang, Shahin Nazarian, Andrei Irimia, et al.\n2025. Hdlcore: A training-free framework for mitigating hallucinations in llm-\ngenerated hdl. arXiv preprint arXiv:2503.16528 (2025).\n[37] Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten,\nand Kees Joost Batenburg. 2025. Agentic large language models, a survey. arXiv\npreprint arXiv:2503.23037 (2025).\n[38] Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang,\nChenjuan Guo, Aoying Zhou, Christian S Jensen, Zhenli Sheng, and Bin Yang.\n2024. TFB: Towards Comprehensive and Fair Benchmarking of Time Series\nForecasting Methods. Proceedings of the VLDB Endowment 17, 9 (2024), 2363–\n2377.\n[39] Oleksandr Shchur, Abdul Fatir Ansari, Caner Turkmen, Lorenzo Stella, Nick\nErickson, Pablo Guerron, Michael Bohlke-Schneider, and Yuyang Wang. 2025.\nfev-bench: A realistic benchmark for time series forecasting. arXiv preprint\narXiv:2509.26468 (2025).\n[40] Harsh Singh, Rocktim Jyoti Das, Mingfei Han, Preslav Nakov, and Ivan Laptev.\n2024. Malmm: Multi-agent large language models for zero-shot robotics manipu-\nlation. arXiv preprint arXiv:2411.17636 (2024).\n[41] Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu,\nLei Zhang, and Jianxin Liao. 2025. Chattime: A unified multimodal time series\nfoundation model bridging numerical and textual data. In Proceedings of the AAAI\nConference on Artificial Intelligence, Vol. 39. 12694–12702.\n[42] Xinlei Wang, Maike Feng, Jing Qiu, Jinjin Gu, and Junhua Zhao. 2024. From\nnews to forecast: Integrating event analysis in llm-based time series forecasting\nwith reflection. Advances in Neural Information Processing Systems 37 (2024),\n58118–58153.\n[43] Yangyang Wang, Jiawei Gu, Li Long, Xin Li, Li Shen, Zhouyu Fu, Xiangjun\nZhou, and Xu Jiang. 2025. FreshRetailNet-50K: A Stockout-Annotated Censored\nDemand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail.\narXiv preprint arXiv:2505.16319 (2025).\n[44] Yuqing Wang and Yun Zhao. 2024. Tram: Benchmarking temporal reasoning for\nlarge language models. In Findings of the Association for Computational Linguistics:\nACL 2024. 6389–6415.\n[45] Andrew Robert Williams, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi,\nJithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste,\nIrina Rish, Nicolas Chapados, et al. 2024. Context is key: A benchmark for\nforecasting with essential textual information. arXiv preprint arXiv:2410.18959\n(2024).\n[46] Xin Wu, Fei Teng, Xingwang Li, Ji Zhang, Tianrui Li, and Qiang Duan. 2025.\nOut-of-Distribution Generalization in Time Series: A Survey. arXiv preprint\narXiv:2503.13868 (2025).\n[47] Zhijian Xu, Hao Wang, and Qiang Xu. 2024.\nIntervention-Aware Forecast-\ning: Breaking Historical Limits from a System Perspective.\narXiv preprint\n"}, {"page": 10, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\narXiv:2405.13522 (2024).\n[48] Wei Yang, Defu Cao, and Yan Liu. 2025. Foundation Models for Demand Fore-\ncasting via Dual-Strategy Ensembling. arXiv preprint arXiv:2507.22053 (2025).\n[49] Wei Yang, Jiacheng Pang, Shixuan Li, Paul Bogdan, Stephen Tu, and Jesse Thoma-\nson. 2025. Maestro: Learning to Collaborate via Conditional Listwise Policy\nOptimization for Multi-Agent LLMs. arXiv preprint arXiv:2511.06134 (2025).\n[50] Wei Yang and Jesse Thomason. 2025. Learning to deliberate: Meta-policy collab-\noration for agentic llms with multi-agent reinforcement learning. arXiv preprint\narXiv:2509.03817 (2025).\n[51] Wen Ye, Jinbo Liu, Defu Cao, Wei Yang, and Yan Liu. 2025. When LLM Meets\nTime Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference.\narXiv preprint arXiv:2509.01822 (2025).\n[52] Wen Ye, Wei Yang, Defu Cao, Yizhou Zhang, Lumingyuan Tang, Jie Cai, and Yan\nLiu. 2024. Domain-Oriented Time Series Inference Agents for Reasoning and\nAutomated Analysis. arXiv preprint arXiv:2410.04047 (2024).\n[53] Chin-Chia Michael Yeh, Vivian Lai, Uday Singh Saini, Xiran Fan, Yujie Fan, Jun-\npeng Wang, Xin Dai, and Yan Zheng. 2025. Empowering Time Series Forecasting\nwith LLM-Agents. arXiv preprint arXiv:2508.04231 (2025).\n[54] Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K Gupta, and Jingbo Shang. 2024.\nLarge language models for time series: A survey. arXiv preprint arXiv:2402.01801\n(2024).\n[55] Haokun Zhao, Xiang Zhang, Jiaqi Wei, Yiwei Xu, Yuting He, Siqi Sun, and Chenyu\nYou. 2025. TimeSeriesScientist: A General Purpose AI Agent for Time Series\nAnalysis. arXiv preprint arXiv:2510.01538 (2025).\n[56] Xiangtian Zheng, Nan Xu, Loc Trinh, Dongqi Wu, Tong Huang, S Sivaranjani,\nYan Liu, and Le Xie. 2022. A multi-scale time-series dataset with benchmark for\nmachine learning in decarbonized energy grids. Scientific Data 9, 1 (2022), 359.\nA\nMore details\nTask Counts per Tier. We report the task counts for each tier (T1–\nT4) across all datasets. For MIMIC, 47 samples are constructed per\ntier, yielding 188/141/239/141 multiple-choice questions for T1–T4\nrespectively; T2 and T4 additionally include 47 forecasting sam-\nples covering 282 target series. For PSML, each tier contains 50\nsamples, with 200/150/250/150 multiple-choice questions for T1–T4\nand 50 forecasting samples in T2 and T4. For Causal Chambers,\neach tier also includes 50 samples, with 150/150/250/150 multiple-\nchoice questions for T1–T4 and 50 forecasting samples in T2 and\nT4. For FreshRetailNet, 44 samples are used per tier, producing\n176/132/176/132 multiple-choice questions for T1–T4, with 44 fore-\ncasting samples in T2 and T4.\nInput Length Analysis. Table 4 further reports the average in-\nput token length of tasks T1–T4 across datasets, computed using\nthe gpt-4o tokenizer, illustrating the substantial contextual and\ntemporal complexity of the benchmark.\nDataset\nT1\nT2\nT3\nT4\nFreshRetailNet\n23,756\n57,895\n23,968\n58,162\nPSML\n4,507\n78,397\n12,374\n74,980\nMIMIC\n629\n6,557\n2,078\n4,439\nCausal Chambers\n6,963\n56,359\n26,565\n52,828\nTable 4: Average input token length (computed using the\ngpt-4o tokenizer) for tasks T1–T4 across different datasets.\nConceptual Organization. Table 5 summarizes the conceptual\norganization of T1–T4. The four task families form a 2 × 2 de-\ncomposition that separates historical understanding from future\nprediction, and isolates how the presence of context and events\nfundamentally changes the nature of temporal reasoning.\nEach task family isolates a different aspect of temporal intelli-\ngence by selectively controlling the availability of future targets\nand contextual information. Importantly, performance on later task\nfamilies does not subsume earlier ones: success in prediction does\nnot imply understanding, and access to context does not guarantee\neffective reasoning.\nB\nRelated Work\nB.1\nTime-Series Forecasting Benchmarks\nA number of large-scale benchmarks evaluate forecasting and time-\nseries models across diverse domains, primarily through numerical\nprediction or analysis objectives. TFB [38] provides a unified frame-\nwork for fair comparison of statistical, machine learning, and deep\nlearning forecasters. GIFT-Eval [1] focuses on standardized evalua-\ntion protocols for time-series foundation models under pretraining\nand finetuning splits, and Chronos [2] evaluates forecasting models\non a large collection of real-world time series datasets, including\nboth in-domain and zero-shot benchmark settings. Beyond forecast-\ning, several benchmarks target specific time-series analysis capabil-\nities, such as TSI-Bench [13], TSB-AD [30], and Time-MMD [28],\nwhich assess performance on analysis- or distribution-oriented\ntasks over numerical sequences. More recent efforts begin to incor-\nporate contextual cues into time-series settings, including CiK [45]\n"}, {"page": 11, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 5: Conceptual decomposition of temporal competencies in TemporalBench.\nNo Context / Events\nWith Context / Events\nHistorical Focus\nT1: Structural Understanding\nT3: Contextual Temporal Reasoning\nInterpret intrinsic temporal properties\nGround temporal patterns in domain semantics\n(trend, volatility, seasonality, anomalies)\nand reason about conditions and comparisons\nFuture Focus\nT2: Pure Temporal Prediction\nT4: Event-Conditioned Prediction\nExtrapolate future behavior from\nReason about how future outcomes change\nhistorical temporal signals alone\nunder specified upcoming events\nand TGTSF [47], but they still frame evaluation largely as single-task\nnumerical analysis without explicit reasoning-oriented objectives.\nMeanwhile, a few benchmarks explore the intersection of lan-\nguage models and numerical time-series by combining question\nanswering with time-series inputs, such as LLM TS Struggle [33],\nMTBench [10], and ChatTime [41]. While these benchmarks move\nbeyond pure forecasting, they typically do not explicitly incorpo-\nrate external contextual information or provide a unified multi-task\ntaxonomy to disentangle complementary aspects of temporal com-\npetence. Overall, existing time-series benchmarks are largely opti-\nmized for numerical performance, motivating our effort to move\nbeyond prediction error toward understanding- and context-aware\ntemporal reasoning.\nB.2\nTemporal Reasoning Benchmarks for LLMs\nA separate line of work evaluates temporal reasoning in large lan-\nguage models using purely textual problems, focusing on symbolic,\nevent-based, and commonsense temporal understanding. Bench-\nmarks such as TimeBench [12] and other temporal reasoning eval-\nuations probe abilities such as ordering events, reasoning about\nduration, and inferring causal-temporal relations from text. These\ndatasets highlight important deficiencies of LLMs in temporal rea-\nsoning, but they omit real numerical time-series signals and do not\ncapture the numerical temporal dynamics central to forecasting\nand many real-world decision-making settings.\nRecent multi-modal benchmarks begin to bridge this gap by cou-\npling numerical time-series with language supervision and down-\nstream reasoning tasks. For example, TelecomTS [15] introduces\na large-scale observability benchmark derived from real-world\ntelecommunications networks, supporting tasks such as anom-\naly detection, root-cause analysis, and question answering over\ntime-series with associated textual context. Such efforts further\nmotivate the need for benchmarks that jointly evaluate numerical\ntime-series understanding and contextual reasoning, which our\nwork addresses.\nB.3\nAgentic and LLM-Based Methods for\nTime-Series\nRecent work has begun to explore agentic or large language model\n(LLM)–driven approaches for time-series analysis and forecast-\ning. TimeSeriesScientist [55] proposes an agent-based framework\nthat decomposes univariate forecasting into stages such as data\ninspection, model selection, and result interpretation, demonstrat-\ning the potential of agentic workflows for automating parts of the\nforecasting pipeline. Other studies investigate the capabilities and\nlimitations of directly applying LLMs to time-series tasks, including\nnumerical forecasting, trend interpretation, and zero-shot reason-\ning over serialized temporal inputs [23].\nMore recent work extends this line of research by introduc-\ning agentic architectures that combine LLM reasoning with time-\nseries–specific tools or models. TimeCAP [25] leverages LLM agents\nto contextualize and semantically augment time-series data for\ndownstream prediction, while TS-Agent [29] emphasizes iterative\nstatistical reasoning through tool invocation and evidence accumu-\nlation. From a systems perspective, TimeCopilot [18] presents an\nopen-source agentic forecasting framework that automates model\nselection, ensembling, and explanation generation.\nIn parallel, several general-purpose agent frameworks have been\nproposed to support structured reasoning and coordination among\nmultiple LLM agents. MetaGPT [20] organizes agents according\nto software engineering roles (e.g., product manager, architect, de-\nveloper), enabling role-specialized collaboration through explicit\ncommunication protocols. AgentScope [17] provides a flexible plat-\nform for constructing, orchestrating, and evaluating multi-agent\nsystems, with explicit support for tool use, memory, and interaction\nlogging. CAMEL [26] emphasizes autonomous role-playing and\nconversational coordination between agents to elicit complex rea-\nsoning behaviors without heavy task-specific supervision. While\nthese frameworks offer powerful abstractions for agent coordi-\nnation and reasoning, they are largely task-agnostic and do not\nprovide dedicated evaluation settings tailored to time-series under-\nstanding.\nC\nMore Analysis\nC.1\nError Analysis\nFigure 3 presents an aggregated error analysis grouped by agent,\nwhere all systems are instantiated on the same ChatGPT backbone.\nFor clarity, we categorize failures into several broad types: failures\nto produce valid answers for multiple-choice questions, violations\nof required output formats, incorrect forecast horizon lengths, and\nsevere numerical abnormalities reflected by extreme error metrics.\nWhile these categories correspond to the color-coded groups in\nthe figure, we focus our discussion on their semantic implications\nrather than their internal labels.\nOverall trends. A dominant observation across all settings is that\na large fraction of errors arise from output control failures rather\nthan a lack of temporal understanding. In particular, predicting a se-\nquence with an incorrect horizon length is the most frequent failure\n"}, {"page": 12, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\nFigure 3: Distribution of error types across five agents us-\ning gpt-4o as the base LLM, aggregated over all datasets and\ntasks. Each pie chart corresponds to an agent and shows the\nproportion of different failure modes.\nmode for general-purpose agents. This issue accounts for more than\n40% of AgentScope’s errors, nearly two-thirds of MetaGPT’s errors,\nand remains substantial even for CAMEL. The single-LLM base-\nline exhibits a similar pattern, with length violations constituting a\nlarge portion of its failures. These results suggest that maintaining\nstrict control over forecast length is a fundamental challenge when\nLLMs are used to generate structured temporal outputs, regardless\nof whether an agent framework is employed.\nBeyond horizon control, numerical instability emerges as an-\nother prominent source of error. For several agents, a considerable\nnumber of outputs satisfy basic syntactic constraints yet lead to\nextremely large MAE or RMSE values. Such failures indicate that\nmodels may generate sequences that are formally valid but numeri-\ncally implausible, for example due to scale drift, unbounded growth,\nor abrupt regime changes not supported by the input history.\nComparison across agent types. Despite sharing the same base\nmodel, different agent designs exhibit markedly different error pro-\nfiles. General-purpose agents tend to struggle most with low-level\nexecution constraints. MetaGPT, for example, is overwhelmingly\ndominated by horizon-length errors, indicating brittle control over\nstructured generation in multi-step workflows. AgentScope and\nCAMEL show more balanced distributions, but still suffer heavily\nfrom a combination of length violations and numerically unstable\nforecasts, suggesting that orchestration and tool usage alone do not\nresolve these issues.\nThe time-series–specific agent (TimeSeriesScientist) displays a\nqualitatively different pattern. Errors related to selecting invalid\noptions in multiple-choice tasks are largely mitigated, indicating\nbetter alignment with task semantics. However, this advantage\ncomes with a shift toward formatting violations and numerical\nanomalies. In other words, while domain-specific reasoning helps\navoid some semantic errors, it does not guarantee adherence to\nstrict output schemas nor robustness in numerical generation.\nThe single-LLM baseline presents the most extreme form of\noutput unreliability. A majority of its failures stem from producing\noutputs that either do not conform to the required format or do\nnot match the expected horizon length. Metric-related failures are\ncomparatively rare, largely because many responses fail before\nreaching numerical evaluation. This highlights the role of agent\nscaffolding in filtering out the most basic execution errors, even if\ndeeper issues remain.\nImplications for future agent design. Taken together, these re-\nsults suggest that the primary bottlenecks for agentic time-series\nmodeling lie not in temporal reasoning itself, but in enforcing exe-\ncution discipline. Robust agents must treat output constraints as\nfirst-class citizens. This includes explicit mechanisms to guarantee\nhorizon length, enforce output schemas, and validate numerical\nplausibility before accepting a forecast. Without such safeguards,\neven strong language models and sophisticated agent workflows\nremain vulnerable to systematic, repeatable failure modes.\nOverall, the error analysis reveals that many failures are struc-\ntural rather than incidental. Addressing these issues requires re-\nthinking agent design around controllability and validation, rather\nthan solely improving temporal representations or reasoning strate-\ngies.\nInsight. A substantial portion of errors across T2 and T4 stem\nfrom execution and controllability failures rather than incor-\nrect temporal reasoning. This suggests that improving agentic\ntime-series systems requires treating output constraints and\nvalidation as first-class design objectives alongside reasoning\ncapabilities.\nC.2\nLength Sensitivity Study\nFigure 4 studies how truncating the historical input affects perfor-\nmance across datasets and task tiers (T1–T4) for three representative\nsettings: a general-purpose agent (AgentScope), a time-series agent\n(TimeSeriesScientist), and a single-LLM baseline. Overall, history\nlength is not a universally monotonic driver of performance. Its\neffect varies by domain and tier, and longer inputs can introduce\ninstability rather than steady gains. This length sensitivity matters\nin practice because real agent pipelines often face prompt budget\nlimits and must decide whether to include more raw observations,\nto summarize them, or to retrieve only salient segments. The results\nhere therefore highlight a trade-off: longer histories may provide\nmore evidence, but they can also dilute key signals and make struc-\ntured generation harder to control. In several settings we observe\n“sweet spots,” where intermediate context performs as well as (or\nbetter than) the longest input, suggesting that effective utilization\nof additional tokens is itself a non-trivial capability.\nDataset-dependent sensitivity is substantial. The four datasets\nrespond differently as history increases. On PSML, AgentScope and\nthe single-LLM baseline are relatively stable and often improve\nslightly with more context, especially on simpler tasks such as\nT1, suggesting recurring patterns that remain exploitable under\nstraightforward truncation. In contrast, FreshRetailNet exhibits\nweaker and more irregular sensitivity: most tiers fluctuate within\na narrow band as the window grows, indicating limited marginal\n"}, {"page": 13, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\n60%\n70%\n80%\n90%\n100%\n0%\n10%\n20%\n30%\n40%\n50%\nCausal Chambers\n60%\n70%\n80%\n90%\n100%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\nFreshRetailNet\n60%\n70%\n80%\n90%\n100%\n0%\n10%\n20%\n30%\n40%\n50%\nMIMIC\n60%\n70%\n80%\n90%\n100%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\nPSML\nAgent\nAgentScope\nTimeSeriesScientist\nSingle-LLM\nFigure 4: Effect of input time-series length on agent perfor-\nmance across four datasets. Each subplot corresponds to a\ndataset, and multi-choice accuracy is reported for different\nagents under varying input lengths. Lines with the same\ncolor denote the same agent, while darker to lighter shades\nrepresent T1–T4 tasks, respectively.\nevidence from additional retail history. MIMIC shows the clearest\nlength-induced instability—moderate extensions can help, but very\nlong histories sometimes degrade sharply, most noticeably for the\nhardest tier—consistent with long clinical histories introducing\nnoise that is difficult for LLM-driven pipelines to compress and\nuse. Causal Chambers is more consistent than retail and MIMIC,\nthough performance still varies across tiers, indicating that domain\nstructure, not raw length, primarily governs difficulty.\nTask-tier effects reveal diminishing returns and occasional rever-\nsals. Across datasets, T1 benefits most consistently from longer\nhistory, but improvements often saturate once a substantial portion\nof the series is included. For AgentScope, the best T1 performance\nfrequently appears at intermediate-to-long histories rather than at\nthe maximum length. For prediction- and decision-oriented tasks\n(T2, T4), trends are less stable: moderate gains can appear, yet they\nrarely persist, and the longest inputs can even reverse performance.\nThis aligns with the earlier observation that these tiers are limited\nnot only by temporal evidence, but by the agent’s ability to extract\nrelevant signals and maintain controllable, structured outputs as\nprompts grow.\nDifferences across agent types highlight distinct failure modes.\nAgentScope is generally robust but relatively insensitive to his-\ntory length, especially on FreshRetailNet, suggesting that longer\nraw histories are not automatically converted into stronger tem-\nporal reasoning. The single-LLM baseline shows a clearer sweet\nspot: moderate lengths often match or outperform both shorter and\nlonger settings, while the longest prompts can degrade performance,\nreflecting a trade-off between contextual richness and controlla-\nbility. TimeSeriesScientist is more bimodal: in some domain–tier\ncombinations it exploits longer histories effectively (e.g., a sharp\ngain at the longest input on FreshRetailNet for one tier), but it\ncan also drop abruptly elsewhere (e.g., severe degradation at the\nlongest history on MIMIC for a difficult tier). This suggests that\ntime-series–specific tooling can help when extra history contains\nrelevant structure, but becomes brittle when long contexts include\nheterogeneous regimes or are not compressed into stable interme-\ndiate representations.\nImplications for agent design and benchmark use. Two implica-\ntions follow. First, longer history should not be equated with better\ncontext: beyond a moderate length, additional tokens can yield\ndiminishing returns or reversals, so evaluations should not assume\nthat more context is inherently beneficial. Second, the results mo-\ntivate length-aware agents. Instead of truncation alone, systems\nshould add history selection and summarization mechanisms (e.g.,\nretrieving salient subsequences, compressing regimes, extracting\nstructured evidence) to prevent long-input dilution and instability.\nFrom a benchmarking perspective, reporting performance at multi-\nple history lengths remains essential: it reveals whether methods\ntruly leverage extended evidence or rely on short-horizon heuristics,\nand avoids conclusions driven by a single fixed length.\nOverall, the length study suggests that temporal benchmarks\nshould evaluate not only forecasting and reasoning accuracy, but\nalso an agent’s ability to robustly use additional historical evidence\nunder realistic prompt-length and control constraints. This capa-\nbility remains only partially and inconsistently developed across\ncurrent general agents, time-series agents, and single-LLM base-\nlines.\nInsight. The ability to leverage longer historical context is\nfragile and task-dependent. Performance saturation and degra-\ndation at extended input lengths indicate that temporal rea-\nsoning is constrained not only by access to evidence, but by\nan agent’s ability to select, compress, and control relevant\ntemporal information.\nD\nPseudocode for Event Injection / Detection\nand Ground-Truth Labeling\nThe section 2 provides algorithmic descriptions of the dataset trans-\nformation pipeline introduced in Section 3, including event in-\njection or detection and rule-based ground-truth generation. The\npseudocode is intended to make the benchmark construction pro-\ncess fully explicit and reproducible, translating the methodological\nprinciples discussed in the main text into concrete procedures. In\nparticular, these algorithms clarify how event boundaries are de-\nfined independently of label computation, how robust statistical\ncriteria are used to derive ground truth across task families, and\nhow uncertainty is handled when temporal signals are weak or\nambiguous. Together, they serve as a reference implementation of\nthe benchmark design rather than a simulation of real-world causal\ndynamics.\n"}, {"page": 14, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\nAlgorithm 1 Event Generation: Injection or Detection\nRequire: Time series dataset D = {(𝑥(𝑖)\n1:𝑇𝑖,𝑚𝑒𝑡𝑎(𝑖))}𝑁\n𝑖=1, random-\nness seed 𝑠, injection probability 𝑝𝑖𝑛𝑗, injection magnitudes /\npatterns set P, change-point detector 𝐶𝑃(·)\nEnsure: For each series index 𝑖, event time 𝑡(𝑖)\n𝑒𝑣𝑡and history/future\nsplit (𝐻(𝑖), 𝐹(𝑖))\n1: set random seed 𝑠\n2: for each time series 𝑖= 1 . . . 𝑁do\n3:\n𝑥←𝑥(𝑖)\n1:𝑇𝑖\n4:\nif 𝑚𝑒𝑡𝑎(𝑖) contains reliable event annotations then\n5:\n𝑡𝑒𝑣𝑡←recorded event time (choose one if multiple)\n6:\nmark 𝑚𝑜𝑑𝑒←DETECTED\n7:\nelse\n8:\n𝑐𝑝_𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑠←𝐶𝑃(𝑥)\n⊲e.g., ruptures, PELT, or\ndomain-specific detector\n9:\nif 𝑐𝑝_𝑐𝑎𝑛𝑑𝑖𝑑𝑎𝑡𝑒𝑠≠∅∧𝑟𝑎𝑛𝑑𝑜𝑚() < 𝑝𝑑𝑒𝑡then\n10:\n𝑡𝑒𝑣𝑡←select candidate (e.g., largest magnitude)\n11:\nmark 𝑚𝑜𝑑𝑒←DETECTED\n12:\nelse\n13:\nif random() < 𝑝𝑖𝑛𝑗then\n14:\n𝑡𝑒𝑣𝑡←select injection time (e.g., uniform over\nvalid window)\n15:\n𝑝𝑎𝑡𝑡𝑒𝑟𝑛←sample(P)\n16:\n𝑥←InjectEffect(𝑥,𝑡𝑒𝑣𝑡, 𝑝𝑎𝑡𝑡𝑒𝑟𝑛)\n17:\nmark 𝑚𝑜𝑑𝑒←INJECTED\n18:\nelse\n19:\nskip this series (no event defined)\n20:\nend if\n21:\nend if\n22:\nend if\n23:\nDefine history window 𝐻(𝑖) = 𝑥1:𝑡𝑒𝑣𝑡and future window\n𝐹(𝑖) = 𝑥𝑡𝑒𝑣𝑡+1:𝑡𝑒𝑣𝑡+ℎ, where ℎis forecast horizon\n24:\nSave (𝐻(𝑖), 𝐹(𝑖),𝑡𝑒𝑣𝑡,𝑚𝑜𝑑𝑒)\n25: end for\n26: return {(𝐻(𝑖), 𝐹(𝑖),𝑡𝑒𝑣𝑡,𝑚𝑜𝑑𝑒)}𝑖\nNotes / Recommended defaults:\n• 𝑝𝑖𝑛𝑗: injection probability for datasets lacking events, e.g.\n0.3–0.5. Use consistent value across experiments.\n• P (injection patterns): coarse patterns only (shift in level,\nscale change, temporary spike, seasonality phase shift). Avoid\nparameterizing patterns to produce deterministic labels.\n• 𝐶𝑃(·): preferred detectors include PELT or Bayesian change-\npoint detectors; tune sensitivity but report chosen hyperpa-\nrameters.\n• Choose injection times away from boundaries (e.g., avoid\nfirst/last 10% of series) to ensure adequate history and future\nsupport.\n• Log and publish the RNG seed(s) and injection choices to\nensure reproducibility.\nAlgorithm 2 Rule-based Ground-Truth Labeling\nRequire: Partitioned series (𝐻, 𝐹), minimum sample support 𝑛𝑚𝑖𝑛,\neffect-size thresholds 𝜏𝑙𝑒𝑣𝑒𝑙,𝜏𝑣𝑜𝑙, estimators: median, MAD, IQR,\nTheil–Sen\nEnsure: Labels for T1/T2/T3/T4 tasks; possibly UNCERTAIN or\nINCONCLUSIVE\n1: Compute robust estimates on 𝐻: ˜𝜇𝐻←𝑚𝑒𝑑𝑖𝑎𝑛(𝐻); ˜𝜎𝐻←\n𝑀𝐴𝐷(𝐻); seasonal features if applicable\n2: Compute robust estimates on 𝐹: ˜𝜇𝐹←𝑚𝑒𝑑𝑖𝑎𝑛(𝐹); ˜𝜎𝐹←\n𝑀𝐴𝐷(𝐹)\n3: 𝑛𝐻←length(𝐻); 𝑛𝐹←length(𝐹)\n4: if 𝑛𝐻< 𝑛𝑚𝑖𝑛∨𝑛𝐹< 𝑛𝑚𝑖𝑛then\n5:\nMark labels as INCONCLUSIVE\n6:\nreturn\n7: end if\n8: Level change: 𝑑𝑙𝑒𝑣𝑒𝑙←( ˜𝜇𝐹−˜𝜇𝐻)/max(| ˜𝜇𝐻|,𝜖)\n9: Volatility change: 𝑑𝑣𝑜𝑙←( ˜𝜎𝐹−˜𝜎𝐻)/max( ˜𝜎𝐻,𝜖)\n10: Effect sizes: compute effect size (e.g., Cliff’s delta or rank-\nbiserial) for distributions 𝐻vs 𝐹\n11: Significance criterion:𝑠𝑢𝑝𝑝𝑜𝑟𝑡= (|𝑑𝑙𝑒𝑣𝑒𝑙| > 𝜏𝑙𝑒𝑣𝑒𝑙)∨(|𝑑𝑣𝑜𝑙| >\n𝜏𝑣𝑜𝑙) ∨(effect size > 𝜏𝑒𝑠)\n12: if support is False then\n13:\nAssign qualitative label = UNCERTAIN\n14: else\n15:\nAssign qualitative labels:\n16:\nif 𝑑𝑙𝑒𝑣𝑒𝑙> 𝜏𝑙𝑒𝑣𝑒𝑙then\n17:\nlabel level = INCREASE\n18:\nelse if 𝑑𝑙𝑒𝑣𝑒𝑙< −𝜏𝑙𝑒𝑣𝑒𝑙then\n19:\nlabel level = DECREASE\n20:\nelse\n21:\nlabel level = NO_CHANGE\n22:\nend if\n23:\nif 𝑑𝑣𝑜𝑙> 𝜏𝑣𝑜𝑙then\n24:\nlabel volatility = INCREASE\n25:\nelse if 𝑑𝑣𝑜𝑙< −𝜏𝑣𝑜𝑙then\n26:\nlabel volatility = DECREASE\n27:\nelse\n28:\nlabel volatility = NO_CHANGE\n29:\nend if\n30: end if\n31: T1 labels (history-only):\n32: compute windowed comparisons on 𝐻(early vs late) using\nrelative median and dispersion ratios →trend / seasonality /\nanomaly labels (use Theil–Sen for slopes)\n33: apply robust z-scores for anomaly detection: 𝑧𝑡\n= (𝑥𝑡−\n𝑚𝑒𝑑𝑖𝑎𝑛(𝐻))/𝑀𝐴𝐷(𝐻); mark extreme outliers if |𝑧𝑡| > 𝑧𝑡ℎ𝑟𝑒𝑠ℎ\n34: T2 / T4 numeric ground truth: store observed 𝐹values as\nnumeric forecast targets\n35: T3 labels (contextual reasoning):\n36: for candidate contextual queries (e.g., difference between sub-\ngroups), compute effect size and ensure 𝑛𝑠𝑢𝑝𝑝𝑜𝑟𝑡≥𝑛𝑚𝑖𝑛; only\nretain queries with effect size > 𝜏𝑒𝑠\n37: Finalize: attach metadata: (𝑙𝑎𝑏𝑒𝑙𝑠,𝑠𝑢𝑝𝑝𝑜𝑟𝑡_𝑚𝑒𝑡𝑟𝑖𝑐𝑠,𝑚𝑜𝑑𝑒)\nwhere mode indicates DETECTED or INJECTED\n38: return labels and metadata\n"}, {"page": 15, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nLabeling constants and recommended defaults:\n• 𝑛𝑚𝑖𝑛(minimum samples): 10–30 depending on frequency\n(daily vs hourly)\n• 𝜏𝑙𝑒𝑣𝑒𝑙(relative median change): 0.10 (10% relative change) as\nbaseline; report sensitivity analyses\n• 𝜏𝑣𝑜𝑙(relative MAD change): 0.10\n• 𝜏𝑒𝑠(effect size threshold): 0.2 (small-to-medium)\n• 𝑧𝑡ℎ𝑟𝑒𝑠ℎ(robust z-score for anomalies): 3.5\n• 𝜖: small constant to avoid divide-by-zero (e.g., 10−6)\nReproducibility and anti-shortcut measures:\n• Random seeds for injection/detection sampling are fixed\nand recorded. Publish seeds and the exact sampled injection\npatterns in the artifact.\n• Use multiple injection patterns and random magnitudes\nwithin a coarse range to prevent models from learning de-\nterministic templates.\n• Ensure all label decisions are computed only from 𝐻and 𝐹\narrays: explicitly do not use the textual context or injected\npattern metadata in any labeling rule.\n• Record support metrics (effect sizes, sample counts) with\neach label so that downstream analyses can filter or weight\nby label confidence.\nDesign rationale (concise):\n• The pipeline favors controlled, interpretable evaluation signals\nover exact simulation fidelity: event injection is intended\nas a way to create regime splits for probing event-aware\nreasoning, not as a synthetic causal generator.\n• Robust estimators (median/MAD/IQR/Theil–Sen) are used\nto make labels stable across domains and data scales.\n• Uncertain / Inconclusive labels avoid forcing spurious ground\ntruth when data support is weak.\nE\nIllustrative Task Examples Across Domains\nTo make the task design of TemporalBench concrete, we present\none complete example instance for each domain in the appendix.\nEach example illustrates how a single time-series sample is trans-\nformed into four task tiers (T1–T4) under a unified abstraction,\nwhile preserving domain-specific characteristics.\nFigure 5 shows an example from PSML, where the shared context\nconsists of power load and meteorological covariates. The tasks\nrange from basic time-series understanding (T1), to forecasting\nand qualitative judgments (T2), event-based reasoning (T3), and\nevent-conditioned forecasting and decision-making (T4).\nFigure 6 presents a clinical example from MIMIC. The shared\ncontext contains multivariate physiological signals, with heart rate\nserving as a reference variable. In this setting, T3 and T4 are condi-\ntioned on a clinically meaningful event (fever), demonstrating how\nTemporalBench captures medically grounded temporal reasoning\nbeyond pure forecasting.\nFigure 7 illustrates a retail scenario from FreshRetailNet. The\nexample highlights sparse and promotion-driven sales dynamics,\nwhere discount-related events play a central role in T3 and T4.\nDespite the domain shift, the task structure remains consistent\nwith the other datasets.\nDataset: PSML\nShared time-series context:\n• Target variable: load_power\n• History length: 336\n|\nFuture length: 168\n• History: 0.805, 0.779, 0.770, 0.792, ..., 0.975, 0.893, 0.836\n• Future: 0.720, 0.689, 0.703, 0.709, ..., 0.431, 0.509, 0.427\n• Covariates: avg_temperature, time_position_in_day,\nTemperature, Wind Speed, GHI, DHI, DNI, Relative Hu-\nmidity, Solar Zenith Angle\nT1\n• Trend (upward / downward / constant): constant\n• Volatility (increased / decreased / constant): constant\n• Seasonality (fixed / shifting / none): fixed\n• Outliers (sudden_spike / level_shift / stable): sudden_spike\nT2\nForecasting subtask: Predict future values of the target variable\ngiven the shared historical context.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nLower\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): increased\n• Seasonality Shift (fixed / shifting / no / Uncertain): fixed\nT3\nQuestion: A sharp nighttime load rise is observed. Does it\ncoincide with a temperature drop?\nOptions: Yes / No / Uncertain\nAnswer: Yes\nT4\nEvent description: A sharp nighttime load rise accompanied by\na temperature drop is observed in the historical window.\nForecasting subtask: Predict future values conditioned on the\nshared historical context and the above event.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nLower\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): increased\n• Seasonality Shift (fixed / shifting / no / Uncertain): fixed\nFigure 5: A full example instance (PSML) showing tasks T1–\nT4 in TemporalBench. All tiers share the same underlying\ntime-series context, while T4 is additionally conditioned on\nan explicit event description.\nFinally, Figure 8 provides an example from Causal Chambers,\na controlled physical system. The shared context includes system\nmeasurements and operational variables, while T3 and T4 focus\non reasoning under load-regime interventions, emphasizing causal\nand regime-based temporal comparisons.\n"}, {"page": 16, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\nDataset: MIMIC\nShared time-series context:\n• Reference variable: heart_rate\n• History length: 50\n|\nFuture length: 29\n• History: 80.0, 82.0, 82.0, 82.0, ..., 92.0, 92.0, 91.0\n• Future: 91.0, 91.0, 80.0, 85.0, ..., 84.0, 84.0, 86.33\n• Covariates (history): temperature_c, resp_rate, spo2,\nsbp, dbp, time_position_in_day\n• Covariates (future): time_position_in_day\nT1\n• Trend (upward / downward / constant): constant\n• Volatility (increased / decreased / constant): increased\n• Seasonality (fixed / shifting / none): shifting\n• Outliers (sudden_spike / level_shift / stable): sudden_spike\nT2\nForecasting subtask: Predict future multivariate physiological\ntime-series trajectories given the shared historical context,\nwith heart rate serving as the primary reference variable.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nUncertain\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): decreased\n• Seasonality Shift (fixed / shifting / no / Uncertain): shifting\nT3\nQuestion: During fever episodes (Temp ≥37.2◦C), is heart rate\nhigher than normal?\nOptions: Higher / Lower / Similar / Uncertain\nAnswer: Higher\nT4\nEvent description: Fever episodes are observed when body\ntemperature reaches or exceeds 37.2◦C.\nForecasting subtask: Predict future multivariate physiological\ntime-series trajectories conditioned on the shared historical\ncontext and the fever event, with heart rate serving as the\nprimary reference variable.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nUncertain\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): decreased\n• Seasonality Shift (fixed / shifting / no / Uncertain): shifting\nFigure 6: A full example instance (MIMIC) illustrating tasks\nT1–T4 in TemporalBench. All tiers share the same underlying\nmultivariate physiological time-series context, while T4 is\nadditionally conditioned on a clinical event (fever).\nDataset: FreshRetailNet\nShared time-series context:\n• Target variable: sales_censored\n• History length: 480\n|\nFuture length: 112\n• History: 0.0, 0.2, 0.0, 0.3, ..., 0.0, 0.0, 0.0\n• Future: 0.091, 0.0, 0.817, 0.465, ..., 0.336, 0.0, 0.100\n• Covariates: discount, holiday_flag, precipitation,\navg_temperature, time_position_in_day\nT1\n• Trend (upward / downward / constant): constant\n• Volatility (increased / decreased / constant): constant\n• Seasonality (fixed / shifting / none): fixed\n• Outliers (sudden_spike / level_shift / stable): sudden_spike\nT2\nForecasting subtask: Predict future retail sales values given the\nshared historical context.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nUncertain\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): Uncertain\n• Seasonality Shift (fixed / shifting / no / Uncertain): fixed\nT3\nQuestion: Under high-discount periods, is the peak-to-median\nratio higher than under low or no discount?\nOptions: Yes / No / Uncertain\nAnswer: No\nT4\nEvent description: High-discount periods are observed in the\nhistorical window.\nForecasting subtask: Predict future retail sales values condi-\ntioned on the shared historical context and the discount event.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nUncertain\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): Uncertain\n• Seasonality Shift (fixed / shifting / no / Uncertain): fixed\nFigure 7: A full example instance (FreshRetailNet) illustrat-\ning tasks T1–T4 in TemporalBench. All tiers share the same\nunderlying retail time-series context, while T4 is addition-\nally conditioned on a promotional event (high discount).\nAcross all four examples, the same task tiers operate on different\ndomains and data characteristics, illustrating that TemporalBench\nevaluates time-series understanding, forecasting, and event-driven\nreasoning within a single, coherent benchmark framework.\n"}, {"page": 17, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nDataset: Causal Chambers\nShared time-series context:\n• Target variable: pressure_downwind\n• History length: 66\n|\nFuture length: 32\n• History: 96043.98, 96043.74, 96042.94, 96042.76, ..., 96040.94,\n96041.25, 96042.17\n• Future: 96039.68, 96040.11, 96039.83, 96039.54, ..., 96037.50,\n96036.52, 96036.55\n• Covariates:\ntimestamps,\npressure_upwind,\npressure_ambient, speed_in, speed_out, current_in,\ncurrent_out, load_in, load_out, hatch_state\nT1\n• Trend (upward / downward / constant): constant\n• Volatility (increased / decreased / constant): Uncertain\n• Outliers (sudden_spike / level_shift / stable): Uncertain\nT2\nForecasting subtask: Predict future values of the target variable\ngiven the shared historical context.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nSimilar\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): Uncertain\n• Seasonality Shift (fixed / shifting / no / Uncertain): no\nT3\nQuestion: Compare pressure_downwind under high load\n(≥q0.70 load_in) versus low load (≤q0.30). Which regime\nhas the higher median level?\nOptions: Higher / Lower / Similar / Uncertain\nAnswer: Similar\nT4\nEvent description: High-load and low-load regimes are identi-\nfied based on quantiles of load_in.\nForecasting subtask: Predict future values of the target variable\nconditioned on the shared historical context and the load-\nregime event.\nMCQ subtask:\n• Future vs. History (Higher / Lower / Similar / Uncertain):\nSimilar\n• Volatility Change (increased / decreased / constant / Uncer-\ntain): Uncertain\n• Seasonality Shift (fixed / shifting / no / Uncertain): no\nFigure 8: A full example instance (Causal Chambers) illus-\ntrating tasks T1–T4 in TemporalBench. All tiers share the\nsame underlying physical-system time-series context, while\nT4 is additionally conditioned on a load-regime event.\nF\nPerformance with different base LLMs\nThis section analyzes how agent performance varies when differ-\nent large language models are used as the backbone. We compare\nclaude-3.7-sonnet, gemini-2.5-flash, deepseek-chat and qwen\n-plus, and relate their behaviors to the trends observed in the main\ntext where gpt-4o is used as the base model.\nAcross all backbone models, several consistent patterns emerge.\nFirst, agent-based frameworks substantially outperform the single-\nLLM baseline on multi-choice tasks (T1 and T3), particularly on\nreasoning-intensive subtasks. This mirrors the observations under\ngpt-4o, indicating that the gains from agent decomposition and\ntool-mediated reasoning are largely model-agnostic. Even when\nthe underlying LLM is weaker or less stable, agents such as Time-\nSeriesScientist, AgentScope, MetaGPT, and CAMEL maintain high\nexecution success rates, suggesting that structured workflows ef-\nfectively mitigate backbone limitations.\nSecond, the accuracy gap between single LLMs and agent frame-\nworks widens as tasks become more reasoning-heavy. Under all\nbase models, T3 exhibits the largest performance variance, with\nsingle LLMs often collapsing to low accuracy despite high success\nrates, while agent frameworks retain meaningful gains. This trend\nis consistent with the gpt-4o results in the main paper and con-\nfirms that T3 exposes intrinsic reasoning deficiencies that cannot\nbe resolved by prompt-only approaches.\nThird, forecasting tasks (T2 and T4) show stronger dependence\non the backbone model. Compared to gpt-4o, alternative base\nLLMs generally yield lower success rates and higher error metrics,\nespecially on PSML and Causal Chambers. Nevertheless, agent\nframeworks still demonstrate more robust behavior than single\nLLMs, with reduced failure rates and more stable error profiles.\nThis suggests that while numerical forecasting quality is sensitive\nto the backbone model, agent coordination improves reliability even\nwhen absolute accuracy remains constrained by model capacity.\nFinally, differences among base LLMs primarily affect the abso-\nlute performance level rather than the relative ranking of agent\nframeworks. Models with stronger instruction-following and rea-\nsoning capabilities, such as claude-3.7-sonnet, exhibit patterns\nmost similar to gpt-4o, whereas lighter or more compressed models\n(e.g., gemini-2.5-flash) show larger drops on forecasting tasks.\nImportantly, no base model eliminates the advantages of agent-\nbased designs, reinforcing the central claim that temporal reasoning\nbenefits from explicit structure beyond raw model scale.\nOverall, these results demonstrate that the conclusions drawn in\nthe main text using gpt-4o generalize across a diverse set of back-\nbone LLMs. Agent-based temporal reasoning consistently improves\nrobustness and accuracy, while the choice of base model primarily\ninfluences the ceiling rather than the existence of these gains.\n"}, {"page": 18, "text": "Conference acronym ’XX, June 03–05, 2018, Woodstock, NY\nMuyan Weng, Defu Cao, Wei Yang, Yashaswi Sharma, and Yan Liu\nTable 6: Multi-choice (MCQ) task performance across datasets and agent frameworks under different backbone models.\n(a) Base model: claude-3.7-sonnet\nAgent\nDataset\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nTask\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nSingle LLM\nSR\n84.09%\n100.00%\n82.39%\n100.00%\n98.00%\n94.00%\n100.00%\n98.00%\n66.00%\n100.00%\n80.00%\n78.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n50.00%\n8.33%\n67.88%\n16.67%\n65.00%\n19.18%\n21.60%\n33.33%\n8.00%\n59.33%\n28.40%\n44.67%\n28.72%\n36.17%\n42.48%\n32.62%\nTimeSeries\nScientist\nSR\n76.14%\n100.00%\n100.00%\n100.00%\n72.50%\n100.00%\n99.60%\n100.00%\n100.00%\n100.00%\n96.80%\n100.00%\n77.13%\n100.00%\n100.00%\n100.00%\nACC\n26.14%\n56.82%\n3.98%\n56.82%\n22.00%\n26.67%\n13.20%\n27.33%\n28.67%\n2.67%\n23.60%\n2.67%\n25.00%\n23.40%\n27.20%\n23.40%\nAgentScope\nSR\n100.00%\n79.55%\n89.77%\n100.00%\n100.00%\n98.00%\n100.00%\n100.00%\n75.00%\n94.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n50.57%\n7.62%\n35.80%\n15.15%\n67.00%\n19.73%\n26.40%\n30.67%\n11.33%\n65.25%\n44.80%\n63.33%\n30.32%\n33.33%\n26.20%\n27.66%\nMetaGPT\nSR\n100.00%\n100.00%\n82.95%\n100.00%\n100.00%\n86.00%\n92.40%\n96.00%\n75.00%\n98.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n57.95%\n10.61%\n30.68%\n12.12%\n65.00%\n22.46%\n22.80%\n32.64%\n16.67%\n64.00%\n46.00%\n63.33%\n31.38%\n36.17%\n23.59%\n34.04%\nCAMEL\nSR\n100.00%\n79.55%\n89.77%\n100.00%\n100.00%\n98.00%\n100.00%\n100.00%\n75.00%\n94.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n50.57%\n7.62%\n35.80%\n15.15%\n67.00%\n19.73%\n26.40%\n30.67%\n11.33%\n65.25%\n44.80%\n63.33%\n30.32%\n33.33%\n26.20%\n27.66%\n(b) Base model: gemini-2.5-flash\nAgent\nDataset\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nTask\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nSingle LLM\nSR\n100.00%\n0.00%\n76.70%\n9.09%\n100.00%\n74.00%\n100.00%\n82.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n94.66%\n100.00%\nACC\n19.89%\n0.00%\n16.56%\n1.52%\n47.50%\n12.00%\n13.60%\n21.33%\n11.33%\n29.33%\n50.40%\n14.67%\n26.60%\n20.57%\n40.94%\n21.99%\nTimeSeries\nScientist\nSR\n76.14%\n100.00%\n100.00%\n100.00%\n72.50%\n100.00%\n99.60%\n100.00%\n100.00%\n100.00%\n96.80%\n100.00%\n77.13%\n100.00%\n100.00%\n100.00%\nACC\n26.14%\n56.82%\n3.98%\n56.82%\n22.00%\n26.67%\n13.20%\n27.33%\n28.67%\n2.67%\n23.60%\n2.67%\n25.00%\n23.40%\n27.20%\n23.40%\nAgentScope\nSR\n100.00%\n72.73%\n85.80%\n59.09%\n100.00%\n70.00%\n98.40%\n68.00%\n75.00%\n100.00%\n100.00%\n98.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n27.27%\n4.17%\n26.70%\n15.38%\n48.50%\n23.81%\n14.80%\n28.43%\n12.00%\n30.67%\n38.40%\n29.93%\n28.19%\n18.44%\n23.76%\n24.82%\nMetaGPT\nSR\n100.00%\n40.91%\n71.59%\n15.91%\n100.00%\n82.00%\n99.60%\n70.00%\n75.00%\n96.00%\n100.00%\n100.00%\n100.00%\n100.00%\n95.82%\n100.00%\nACC\n18.18%\n7.41%\n20.39%\n4.76%\n47.50%\n16.26%\n15.20%\n23.81%\n14.67%\n30.56%\n37.20%\n22.00%\n27.13%\n16.31%\n23.71%\n19.15%\nCAMEL\nSR\n100.00%\n15.91%\n85.23%\n9.09%\n100.00%\n76.00%\n99.60%\n80.00%\n75.00%\n100.00%\n100.00%\n94.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n25.57%\n9.52%\n25.00%\n41.67%\n48.00%\n20.18%\n10.40%\n24.17%\n16.67%\n28.67%\n45.60%\n29.79%\n23.94%\n20.57%\n24.61%\n20.57%\n(c) Base model: deepseek-chat\nAgent\nDataset\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nTask\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nSingle LLM\nSR\n100.00%\n100.00%\n82.39%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n68.18%\n56.82%\n16.85%\n26.52%\n74.00%\n24.67%\n34.80%\n36.00%\n15.33%\n19.33%\n44.40%\n29.33%\n33.00%\n22.67%\n39.14%\n22.67%\nTimeSeries\nScientist\nSR\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n36.36%\n56.82%\n34.09%\n56.82%\n38.50%\n26.67%\n22.80%\n27.33%\n28.67%\n2.67%\n29.60%\n2.67%\n15.43%\n23.40%\n25.94%\n23.40%\nAgentScope\nSR\n100.00%\n100.00%\n96.02%\n100.00%\n100.00%\n78.00%\n100.00%\n100.00%\n75.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n70.45%\n34.85%\n5.68%\n42.42%\n68.50%\n20.51%\n24.80%\n31.33%\n16.00%\n47.33%\n46.40%\n42.00%\n32.98%\n22.70%\n25.71%\n24.82%\nMetaGPT\nSR\n100.00%\n100.00%\n97.16%\n100.00%\n100.00%\n98.00%\n100.00%\n100.00%\n75.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n69.89%\n41.67%\n5.11%\n37.12%\n71.50%\n19.73%\n28.00%\n30.67%\n15.33%\n22.00%\n46.40%\n40.00%\n32.98%\n22.70%\n25.79%\n30.50%\nCAMEL\nSR\n100.00%\n100.00%\n90.91%\n100.00%\n100.00%\n74.00%\n100.00%\n100.00%\n75.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n67.05%\n8.33%\n19.89%\n29.55%\n65.00%\n18.02%\n25.20%\n30.67%\n12.67%\n65.33%\n46.80%\n48.00%\n31.38%\n24.82%\n25.61%\n31.21%\n(d) Base model: qwen-plus\nAgent\nDataset\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nTask\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nSingle LLM\nSR\n100.00%\n100.00%\n64.20%\n70.45%\n100.00%\n54.00%\n100.00%\n26.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n60.80%\n11.36%\n7.30%\n9.85%\n49.00%\n10.67%\n23.20%\n8.00%\n20.00%\n18.00%\n50.40%\n35.33%\n43.50%\n27.33%\n40.84%\n35.33%\nTimeSeries\nScientist\nSR\n17.61%\n100.00%\n100.00%\n100.00%\n18.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n75.20%\n100.00%\n22.87%\n100.00%\n99.58%\n100.00%\nACC\n7.95%\n56.82%\n23.30%\n56.82%\n12.50%\n26.67%\n14.00%\n27.33%\n28.67%\n2.67%\n20.80%\n2.67%\n5.85%\n23.40%\n28.45%\n23.40%\nAgentScope\nSR\n100.00%\n100.00%\n100.00%\n90.91%\n100.00%\n84.00%\n100.00%\n78.00%\n75.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n58.52%\n7.58%\n39.77%\n8.33%\n54.50%\n19.05%\n27.60%\n26.50%\n24.00%\n42.67%\n49.60%\n42.00%\n44.15%\n25.53%\n28.24%\n29.79%\nMetaGPT\nSR\n100.00%\n22.73%\n100.00%\n56.82%\n100.00%\n86.00%\n100.00%\n88.00%\n75.00%\n100.00%\n100.00%\n96.00%\n100.00%\n100.00%\n100.00%\n97.87%\nACC\n65.34%\n16.67%\n40.34%\n10.67%\n53.50%\n18.60%\n29.20%\n28.03%\n26.00%\n34.00%\n49.20%\n39.58%\n47.34%\n26.24%\n26.95%\n31.88%\nCAMEL\nSR\n100.00%\n100.00%\n15.91%\n100.00%\n100.00%\n75.00%\n100.00%\n75.00%\n74.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\n100.00%\nACC\n64.20%\n12.88%\n35.80%\n12.88%\n75.00%\n44.44%\n32.00%\n44.44%\n22.67%\n33.33%\n48.40%\n42.67%\n41.49%\n25.53%\n27.62%\n34.04%\n"}, {"page": 19, "text": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTable 7: Forecasting performance on T2 and T4 tasks across datasets and agent frameworks under different backbone models.\n(a) Base model: claude-3.7-sonnet\nAgent\nMetric\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nT2\nT4\nT2\nT4\nT2\nT4\nT2\nT4\nSingle LLM\nSR\n61.36%\n47.73%\n34.00%\n40.00%\n48.00%\n46.00%\n0.00%\n97.87%\nMAE\n0.11\n0.14\n0.26\n0.29\n2.31\n2.70\n12.14\nsMAPE\n1.25\n1.22\n0.28\n0.30\n2.40E-05\n2.80E-05\n0.47\nTimeSeries\nScientist\nSR\n36.36%\n43.18%\n22.00%\n14.00%\n54.00%\n30.00%\n89.36%\n89.36%\nMAE\n0.14\n0.16\n0.26\n0.26\n2.32\n2.57\n9.75\n10.54\nsMAPE\n1.29\n1.21\n0.29\n0.24\n2.41E-05\n2.68E-05\n0.34\n0.38\nAgentScope\nSR\n65.91%\n88.64%\n30.00%\n34.00%\n62.00%\n32.00%\n97.87%\n95.74%\nMAE\n0.13\n0.11\n0.27\n0.24\n2.16\n2.79\n9.86\n13.45\nsMAPE\n121.00\n121.96\n26.92\n20.07\n2.25E-03\n2.90E-03\n0.41\n0.48\nMetaGPT\nSR\n29.55%\n70.45%\n14.00%\n26.00%\n50.00%\n36.00%\n97.87%\n97.87%\nMAE\n0.12\n0.14\n0.16\n0.24\n2.07\n2.87\n9.90\n11.36\nsMAPE\n126.50\n120.66\n16.88\n26.33\n2.15E-03\n2.99E-03\n0.41\n0.49\nCAMEL\nSR\n65.91%\n88.64%\n30.00%\n34.00%\n62.00%\n32.00%\n97.87%\n95.74%\nMAE\n0.13\n0.11\n0.27\n0.24\n2.16\n2.79\n9.86\n13.45\nsMAPE\n121.00\n121.96\n26.92\n20.07\n2.25E-03\n2.90E-03\n0.41\n0.48\n(b) Base model: gemini-2.5-flash\nAgent\nMetric\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nT2\nT4\nT2\nT4\nT2\nT4\nT2\nT4\nSingle LLM\nSR\n0.00%\n2.27%\n0.00%\n4.00%\n36.00%\n38.00%\n0.00%\n89.36%\nMAE\n0.06\n0.11\n2.18\n2.90\n11.80\nsMAPE\n1.29\n0.16\n2.26E-05\n3.02E-05\n0.38\nTimeSeries\nScientist\nSR\n36.36%\n43.18%\n22.00%\n14.00%\n54.00%\n30.00%\n89.36%\n89.36%\nMAE\n0.14\n0.16\n0.26\n0.26\n2.32\n2.57\n9.75\n10.54\nsMAPE\n1.29\n1.21\n0.29\n0.24\n2.41E-05\n2.68E-05\n0.34\n0.38\nAgentScope\nSR\n0.00%\n2.33%\n0.00%\n2.04%\n30.19%\n28.30%\n75.47%\n82.35%\nMAE\n0.15\n0.14\n2.32\n2.20\n11.60\n12.06\nsMAPE\n143.19\n16.93\n2.41E-03\n2.29E-03\n0.41\n0.36\nMetaGPT\nSR\n0.00%\n4.55%\n4.00%\n6.00%\n50.00%\n42.00%\n76.60%\n89.36%\nMAE\n0.39\n0.24\n0.20\n2.48\n2.55\n11.26\n10.93\nsMAPE\n124.81\n24.13\n21.58\n2.58E-03\n2.65E-03\n0.40\n0.35\nCAMEL\nSR\n0.00%\n0.00%\n2.00%\n4.00%\n34.00%\n46.00%\n53.19%\n87.23%\nMAE\n0.16\n0.16\n0.16\n2.34\n2.62\n11.08\n11.58\nsMAPE\n22.22\n19.38\n19.38\n2.43E-03\n2.73E-03\n0.39\n0.36\n(c) Base model: deepseek-chat\nAgent\nMetric\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nT2\nT4\nT2\nT4\nT2\nT4\nT2\nT4\nSingle LLM\nSR\n0.00%\n2.27%\n20.00%\n46.00%\n52.00%\n48.00%\n0.00%\n98.00%\nMAE\n0.10\n0.29\n0.38\n1.97\n2.53\n12.12\nsMAPE\n0.97\n0.27\n0.28\n2.05E-05\n2.63E-05\n0.48\nTimeSeries\nScientist\nSR\n34.09%\n25.00%\n68.00%\n60.00%\n64.00%\n40.00%\n89.36%\n89.36%\nMAE\n0.17\n0.12\n0.34\n0.34\n2.11\n2.51\n10.70\n0.34\nsMAPE\n1.30\n1.25\n0.32\n0.27\n2.19E-05\n2.61E-05\n12.34\n0.45\nAgentScope\nSR\n27.54%\n1.15%\n12.36%\n25.00%\n58.00%\n40.00%\n97.87%\n95.74%\nMAE\n0.10\n0.07\n0.29\n0.36\n2.43\n2.72\n10.22\n11.43\nsMAPE\n124.06\n129.35\n27.39\n27.94\n2.52E-03\n2.82E-03\n0.46\n0.39\nMetaGPT\nSR\n38.64%\n0.00%\n22.00%\n38.00%\n58.00%\n44.00%\n97.87%\n97.87%\nMAE\n0.09\n0.31\n0.35\n2.33\n2.70\n10.29\n10.88\nsMAPE\n125.21\n25.15\n29.31\n2.42E-03\n2.81E-03\n0.41\n0.41\nCAMEL\nSR\n45.45%\n18.18%\n20.00%\n30.00%\n56.00%\n38.00%\n97.87%\n97.87%\nMAE\n0.11\n0.22\n0.28\n0.39\n2.37\n2.55\n11.36\n11.32\nsMAPE\n129.65\n132.99\n25.78\n32.22\n2.46E-03\n2.65E-03\n0.47\n0.46\n(d) Base model: qwen-plus\nAgent\nMetric\nFreshRetailNet\nPSML\nCausal Chambers\nMIMIC\nT2\nT4\nT2\nT4\nT2\nT4\nT2\nT4\nSingle LLM\nSR\n0.00%\n4.55%\n6.00%\n12.00%\n46.00%\n46.00%\n0.00%\n92.00%\nMAE\n0.32\n0.32\n0.54\n2.17\n2.83\n17.02\nsMAPE\n1.32\n0.27\n0.34\n2.26E-05\n2.95E-05\n0.54\nTimeSeries\nScientist\nSR\n4.55%\n6.82%\n0.00%\n16.00%\n46.00%\n44.00%\n82.98%\n80.85%\nMAE\n0.06\n0.28\n0.39\n2.43\n2.46\n15.08\n18.80\nsMAPE\n1.30\n1.34\n0.32\n2.52E-05\n2.55E-05\n0.42\n0.58\nAgentScope\nSR\n1.15%\n8.64%\n6.38%\n6.38%\n39.22%\n35.29%\n54.10%\n89.80%\nMAE\n0.23\n0.27\n0.31\n0.66\n2.39\n2.77\n15.67\n18.65\nsMAPE\n135.87\n136.62\n29.69\n40.12\n2.48E-03\n2.88E-03\n0.52\n0.53\nMetaGPT\nSR\n0.00%\n6.82%\n16.00%\n12.00%\n42.00%\n32.00%\n59.57%\n100.00%\nMAE\n0.35\n0.47\n0.34\n2.13\n2.49\n15.97\n17.68\nsMAPE\n134.81\n35.39\n28.81\n2.22E-03\n2.59E-03\n0.46\n0.58\nCAMEL\nSR\n2.27%\n100.00%\n20.00%\n40.00%\n44.00%\n40.00%\n72.34%\n95.74%\nMAE\n0.16\n0.23\n0.08\n0.32\n2.15\n2.54\n14.62\n18.26\nsMAPE\n125.89\n120.09\n7.58\n24.99\n2.24E-03\n2.64E-03\n0.50\n0.56\n"}]}