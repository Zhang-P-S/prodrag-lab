{"doc_id": "arxiv:2511.22402", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.22402.pdf", "meta": {"doc_id": "arxiv:2511.22402", "source": "arxiv", "arxiv_id": "2511.22402", "title": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs", "authors": ["Srivarshinee Sridhar", "Raghav Kaushik Ravi", "Kripabandhu Ghosh"], "published": "2025-11-27T12:26:06Z", "updated": "2025-11-27T12:26:06Z", "summary": "Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.22402v1", "url_pdf": "https://arxiv.org/pdf/2511.22402.pdf", "meta_path": "data/raw/arxiv/meta/2511.22402.json", "sha256": "f5815078cf564b3bee9dac5da94059b099a07fcfed4df1215799b6e7b296f33a", "status": "ok", "fetched_at": "2026-02-18T02:25:57.878701+00:00"}, "pages": [{"page": 1, "text": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs\nSrivarshinee Sridhar1*, Raghav Kaushik Ravi1*, Kripabandhu Ghosh2\n1 Vellore Institute of Technology, Chennai\n2 IISER Kolkata\nAbstract\nLarge Language Models (LLMs) are increasingly used in\nclinical settings, where sensitivity to linguistic uncertainty\ncan influence diagnostic interpretation and decision-making.\nYet little is known about where such epistemic cues are in-\nternally represented within these models. Distinct from un-\ncertainty quantification, which measures output confidence,\nthis work examines input-side representational sensitivity to\nlinguistic uncertainty in medical text. We curate a contrastive\ndataset of clinical statements varying in epistemic modality\n(e.g., “is consistent with” vs. “may be consistent with”) and\npropose Model Sensitivity to Uncertainty (MSU), a layer-\nwise probing metric that quantifies activation-level shifts in-\nduced by uncertainty cues. Our results show that LLMs ex-\nhibit structured, depth-dependent sensitivity to clinical uncer-\ntainty, suggesting that epistemic information is progressively\nencoded in deeper layers. These findings reveal how linguis-\ntic uncertainty is internally represented in LLMs, offering in-\nsight into their interpretability and epistemic reliability.\nCode — https://github.com/varshin5699/Mapping-Clinical-\nDoubt--Locating-Linguistic-Uncertainty-in-LLMs\nIntroduction\nLarge Language Models (LLMs) are increasingly deployed\nin high-stakes domains such as law, medicine, and pub-\nlic policy settings that demand not only accurate outputs\nbut also the ability to reason responsibly under uncer-\ntainty. While previous research has emphasized calibrating\nmodel confidence and evaluating output probabilities, com-\nparatively little is known about how LLMs internally rep-\nresent input-side uncertainty particularly linguistic uncer-\ntainty expressed through epistemic modality (e.g., might,\ncould, probably). Figure 1 shows that even minimal shifts\nin modality, such as replacing ‘should’ with ‘could’, lead\nto consistent differences in model generations, despite all\nother input remaining constant. These variations are not ar-\ntifacts of sampling noise; rather, they indicate a systematic\nand grounded sensitivity to uncertainty cues. This raises\nimportant questions: How are such epistemic signals en-\ncoded across the layers of a model? Are these representa-\ntions stable across model variants? Understanding the inter-\nnal treatment of linguistic uncertainty is critical for build-\n*These authors contributed equally.\nFigure 1: Although the prompt pairs differ only in epis-\ntemic modality (should vs could), the responses vary: those\nprompted with could tend to offer a broader range of medi-\ncal possibilities and are more open-ended compared to those\nending with should. This could imply how the model inter-\nprets linguistic uncertainty.\ning trustworthy, transparent systems, especially in applica-\ntions where appropriately responding to hedged or specula-\ntive language can directly affect outcomes and user trust.\nUncertainty in LLM Outputs\nMuch of the recent work on LLM uncertainty has focused\non model outputs, through calibration techniques (Desai and\nDurrett 2020), truthfulness under uncertainty (Lin, Hilton,\nand Evans 2022), or confidence alignment with ground-truth\n(Ghafouri et al. 2024). While these approaches provide use-\nful diagnostics on model predictions, they do not investigate\nhow input uncertainty is internally represented or whether\nmodels distinguish between certain and uncertain prompts\nat a representational level.\nEpistemics and Modal Reasoning\nSeveral recent studies have examined the role of modal verbs\nin model reasoning. (Holliday, Mandelkern, and Zhang\n2024)) show that LLMs often struggle with logical tasks\ninvolving modal operators, suggesting a lack of systematic\nreasoning with modality.\n(Zhou, Jurafsky, and Hashimoto 2023) analyze use of\nepistemic markers in LLM-generated text, showing large ef-\narXiv:2511.22402v1  [cs.CL]  27 Nov 2025\n"}, {"page": 2, "text": "fects on accuracy depending on whether uncertainty or cer-\ntainty markers are used, although they do not study how\nthese markers are internally represented in neural activa-\ntions. Similarly, (Lee et al. 2025) show that LLM-based eval-\nuators are systematically biased against responses contain-\ning expressions of uncertainty.\nOur work complements these efforts by offering a mech-\nanistic perspective on how epistemic modality is encoded\ninside models, using probing over activation spaces. In con-\ntrast to prior work focusing on usage or output alignment,\nwe provide empirical evidence of internal sensitivity to epis-\ntemic variation.\nProbing Internal Representations\nA growing body of mechanistic interpretability research\nseeks to understand LLM internals by intervening in the ac-\ntivation space. One core technique is activation patching,\nalso known as causal mediation or causal tracing, which sub-\nstitutes hidden activations from a clean forward pass into a\ncorrupted run, thereby identifying components causally re-\nsponsible for specific behaviors (Vig et al. 2020). Building\non this, path patching refines the approach by restricting in-\nterventions to specific computational paths, enabling finer-\ngrained localization of functional subcircuits (Goldowsky-\nDill et al. 2023). Complementary methods include causal\nscrubbing (Chan et al. 2022), which tests whether abstract\ncircuits maintain function across structural perturbations,\nand automated circuit discovery frameworks such as ACDC\n(Conmy et al. 2023).\nIn this work, we pose the question: Are large lan-\nguage models sensitive to epistemic modality in their in-\nput? This question is investigated by contrasting represen-\ntations elicited by semantically similar prompts that differ\nonly in the degree of certainty they convey. As depicted in\nFigure 2, we curate a multiple-choice dataset of 3,114 sen-\ntence pairs that differ in their expression of certainty and\nexamine how these variations influence the model’s internal\nactivation space.\nTo quantify such effects, we introduce a novel metric,\nModel Sensitivity to Uncertainty (MSU), which captures\nthe representational shift induced by epistemic cues at each\nlayer of the model. Through this lens, we assess whether and\nwhere in the model epistemic modality is internally encoded.\nOur key contributions are as follows:\n• We propose a probing framework to assess whether\nLLMs encode epistemic modality through changes in\ntheir activation space.\n• We introduce MSU, a layerwise metric for quantifying\nmodel sensitivity to linguistic uncertainty.\n• We release the dataset of 3,114 sentence pairs depicting\nlinguistic certainty and uncertainty.\nDataset\nUnlike contrastive datasets used in prior work (Rimsky et al.\n2024), data was not constructed to be semantically oppo-\nsite or adversarial. Instead, they vary along a fine-grained\nlinguistic axis of uncertainty, making them well-suited for\nprobing representational sensitivity to uncertainty.\nFigure 2: Paired inputs with linguistic certainty and uncer-\ntainty are passed through the model. Layerwise activation\ndifferences (∆) are used to compute MSU, capturing the\nmodel’s sensitivity to uncertainty across layers.\nThe dataset used in this work is derived from claims in\nthe Anthropic/Persuasion (Durmus et al. 2024) cor-\npus. Sentences containing modal verbs such as “should” and\n“must” were identified and programmatically masked using\nthe pandas (Wes McKinney 2010), NumPy (Harris et al.\n2020), and NLTK (Bird and Loper 2004) libraries. These\nmasked positions were then filled with controlled multiple-\nchoice options representing either certain or uncertain lin-\nguistic modality (e.g., “should” vs. “could”).\nFor\nexample,\nconsider\na\nsample\nfrom\nthe\nAnthropic/Persuasion\ndataset\n(Durmus\net\nal.\n2024):\nExample Prompt\nOriginal: “Governments and technology companies must do\nmore to protect online privacy and security.”\nModified prompt:\n<|im start|>user\nGovernments and technology companies\n[MASK] do more to protect online\nprivacy and security.\nChoose a replacement for the MASK.\nA) Must\nB) Might\n<|im end|>\nOne of the pair would be appended with A, and the other\nwith B, as shown in Figure 2 to form the linguistic certain-\nuncertain pair. The changes made to the dataset ensure the\nfollowing:\nControlled Variation: The only systematic difference\nacross each pair is the use of uncertainty markers (e.g.,\n“might”, “possibly”, “maybe”), ensuring minimal lexical\n"}, {"page": 3, "text": "confounds.\nSemantic Stability: As the core semantics is preserved, any\nvariation in activation vectors can be more confidently at-\ntributed to modality, not content drift. For examples of these\nsome of these changes, refer Appendix ??.\nEach sentence was paired with two variants, one express-\ning certainty and one expressing uncertainty by appending\nthe starting letter of the option at the end of the instruction,\nresulting in 3,114 samples per condition, and a total of 6,228\nexamples.\nEvaluation Setup\nWe\nconduct\nour\nanalysis\non\nthree\nsmall-scale\nlan-\nguage\nmodels:\nQwen2.5-0.5B-Instruct\n(Team\n2024),\nQwen1.5-0.5B-Chat (Bai et al. 2023), and LLaMA-3.2-1B-\nInstruct(Grattafiori et al. 2024).\nModel Specifications\nThese models were selected to cover a range of instruction-\ntuned and chat-oriented variants with varying parameter\ncounts while maintaining manageable computational over-\nhead. All models provide access to internal activations,\nwhich is crucial for our layer-wise representation analysis.\nageable computational overhead. Table 1 details the model\nsizes and number of layers for the language models used\nin our experiments. All internal activations were accessed\nusing the TransformerLens (Nanda and Bloom 2022)\nlibrary.\nModel\nParameters\nLayers\nQwen2.5-0.5B-Instruct\n391M\n24\nQwen1.5-0.5B-Chat\n308M\n24\nLlama-3.2-1B-Instruct\n1.1B\n16\nTable 1: Model sizes, number of layers, and sources for LLM\nvariants used in our analysis.\nCan Linguistic Uncertainty be probed in the\nActivation Space?\nPrincipal Component Analysis (PCA) is applied layer by\nlayer to the model activation vectors to examine whether\nthe uncertainty is encoded linearly separable. For each layer,\nactivations of certain and uncertain examples are projected\nonto the two main components using Scikit-learn (Pedregosa\net al. 2011), allowing visualization of potential clustering\npatterns. This analysis serves as a diagnostic to assess the\nvalidity of the data set to study linguistic uncertainty (refer\nto appendix ).\nResults\nIn all three models, we observe clear clustering patterns\nthat validate the separability of epistemic modalities in the\nmodel’s internal representation space. Interestingly, in the\nlater layers of the Instruct model (Layer 17 and 23), as well\nas in Layers 13 through 15 of the Chat model, we detect a\ngeometric inversion in the PCA projections: the cluster cor-\nresponding to uncertain statements flips position relative to\nthat of certain statements along the primary axes. This inver-\nsion suggests a deeper semantic reorganization in the latent\nspace, potentially signaling a transition from syntactic or\nlexical representation toward task-relevant abstractions of\nlinguistic uncertainty. These structured shifts reinforce our\nhypothesis that uncertainty is not only linearly encoded but\nis also semantically recontextualized in deeper layers, un-\nderscoring the interpretability and representational richness\nof the models under investigation.\nHow does Sensitivity to Linguistic Uncertainty\nchange across layers?\nWe extract activation vectors from all transformer layers us-\ning the TransformerLens library (Nanda and Bloom 2022),\nwhich allows access to cached internal states without mod-\nifying the model architecture. Specifically, for each input\npair, consisting of a certain and an epistemically uncertain\nvariant, we record the residual stream activations at the final\ntoken position, where model output is most strongly influ-\nenced.\nTo quantify the representational shift induced by linguis-\ntic uncertainty, we introduce a metric called Model Sensitiv-\nity to Uncertainty (MSU). This metric captures the average\ndistance between the representations of the certain and un-\ncertain variants of each input sentence pair.\nFormally, for a given model layer ℓ, we define MSU as:\nMSU(ℓ) = 1\nN\nN\nX\ni=1\n\r\r\rh(ℓ,certain)\ni\n−h(ℓ,uncertain)\ni\n\r\r\r\n2\n(1)\nwhere h(ℓ,·)\ni\ndenotes the activation vector obtained from\nlayer ℓfor the i-th input in its certain or uncertain form, and\nN is the total number of input pairs.\nMSU provides a quantitative estimate of how much lin-\nguistic uncertainty perturbs the model’s internal representa-\ntions. Larger MSU values indicate greater sensitivity to un-\ncertainty at that layer.\nResults\nAcross three models, we observe a strikingly consistent\ntrend: the MSU scores increase monotonically with depth,\nindicating that sensitivity to epistemic uncertainty is a pro-\ngressively emerging phenomenon in the transformer stack\n(Figure 5). Later layers exhibit substantially higher sensitiv-\nity to epistemic modals than early layers, indicating that se-\nmantic distinctions introduced by modality are progressively\namplified across depth. This aligns with prior work indicat-\ning that deeper layers are responsible for encoding abstract,\ncompositional semantics and final decision-making(Zhao,\nZiser, and Cohen 2024). Our results suggest that epistemic\nuncertainty is treated as a high-level semantic feature.\nDespite architectural and training differences between\nQwen2.5 and Qwen1.5, both models demonstrate nearly\nidentical MSU profiles, suggesting that this pattern of late-\nemerging sensitivity is robust to model family, scale, and in-\nstruction tuning. This implies that the encoding of linguistic\nuncertainty may be a universal behavior among autoregres-\nsive transformers\n"}, {"page": 4, "text": "Figure 3: PCA plots of the last token activations of layers 10 and 17 for Qwen2.5-0.5B-Instruct model. A geometric inversion\ncan be observed in the Projections for the Uncertain and Certain input activations.\nFigure 4: Layer-wise MSU scores for Qwen2.5-0.5B-\nInstruct, indicate progressively increasing scores across lay-\ners suggesting that later layers are responsible for encoding\nuncertainties in language\nConclusion\nOur investigation reveals that the encoding of epistemic un-\ncertainty is a distributed and emergent property of deep\ntransformer architectures. Rather than residing in isolated\nlayers, epistemic modality unfolds progressively, peaking in\nthe final layers across models and variants alike. By propos-\ning the Mean Sensitivity to Uncertainty (MSU) metric, we\nprovide a targeted lens into this phenomenon. This struc-\ntural consistency underscores a deeper semantic organiza-\ntion within LLMs and opens new pathways for designing\nmodels that are not only more interpretable, but also more\nepistemically aware.\nLimitations\nThis study takes an initial step toward understanding how\nlanguage models encode linguistic uncertainty. Our findings\nare based on a limited set of instruction-tuned models, so\ntheir generality across architectures, sizes, and pretraining\nModel\nAverage MSU\nLLaMA 3.2-1B\n9.361\nQwen1.5-0.5B-Chat\n16.968\nQwen2.5-0.5B-Instruct\n11.520\nTable 2: Average MSU values across transformer layers for\neach model. Qwen1.5-0.5B-Chat comes out to be the most\nsensitive to linguistic uncertainty which is then followed by\nQwen2.5-0.5B-Instruct and LLaMA 3.2-1B, among the 3\ntested models.\nparadigms remains uncertain.\nFuture work should expand to include diverse model\ntypes, like non-instruct, multilingual, domain-specific and\ninputs namely, varied modal verbs, syntax, discourse). A key\ndirection is localizing uncertainty to specific neurons or at-\ntention heads, and examining how internal uncertainty sig-\nnals (e.g., logits, entropy) relate to output confidence and\ncalibration.\nAcknowledgments\nWe thank the developers of TransformerLens (Nanda and\nBloom 2022) for enabling detailed access to model inter-\nnals and interpretability tools. We also acknowledge An-\nthropic for releasing the Persuasion dataset, which supported\nour exploratory evaluation. Additionally, we appreciate the\nQwen team for open-sourcing their instruction-tuned mod-\nels, which served as the basis for this study. This work was\nconducted independently without external funding.\n"}, {"page": 5, "text": "References\nBai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan,\nY.; Ge, W.; Han, Y.; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin,\nJ.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men,\nR.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.;\nWang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang,\nH.; Yang, J.; Yang, S.; Yao, Y.; Yu, B.; Yuan, H.; Yuan, Z.;\nZhang, J.; Zhang, X.; Zhang, Y.; Zhang, Z.; Zhou, C.; Zhou,\nJ.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report.\narXiv preprint arXiv:2309.16609.\nBird, S.; and Loper, E. 2004. NLTK: The Natural Language\nToolkit. In Proceedings of the ACL Interactive Poster and\nDemonstration Sessions, 214–217. Barcelona, Spain: Asso-\nciation for Computational Linguistics.\nChan, L.; Garriga-Alonso, A.; Goldowsky-Dill, N.; Green-\nblatt, R.; Nitishinskaya, J.; Radhakrishnan, A.; Shlegeris, B.;\nand Thomas, N. 2022. Causal Scrubbing: a method for rig-\norously testing interpretability hypotheses.\nConmy, A.; Mavor-Parker, A. N.; Lynch, A.; Heimersheim,\nS.; and Garriga-Alonso, A. 2023. Towards Automated Cir-\ncuit Discovery for Mechanistic Interpretability. In Thirty-\nseventh Conference on Neural Information Processing Sys-\ntems.\nDesai, S.; and Durrett, G. 2020. Calibration of Pre-trained\nTransformers.\nIn Webber, B.; Cohn, T.; He, Y.; and Liu,\nY., eds., Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 295–\n302. Online: Association for Computational Linguistics.\nDurmus, E.; Lovitt, L.; Tamkin, A.; Ritchie, S.; Clark, J.;\nand Ganguli, D. 2024. Measuring the Persuasiveness of Lan-\nguage Models.\nGhafouri, B.; Mohammadzadeh, S.; Zhou, J.; Nair, P.; Tian,\nJ.-J.; Goel, M.; Rabbany, R.; Godbout, J.-F.; and Pelrine, K.\n2024. Epistemic Integrity in Large Language Models. In\nNeurips Safe Generative AI Workshop 2024.\nGoldowsky-Dill, N.; MacLeod, C.; Sato, L.; and Arora,\nA. 2023. Localizing Model Behavior with Path Patching.\narXiv:2304.05969.\nGrattafiori, A.; Dubey, A.; Jauhri, A.; ...; et al. 2024. The\nLlama 3 Herd of Models. https://arxiv.org/abs/2407.21783.\nArXiv:2407.21783.\nHarris, C. R.; Millman, K. J.; van der Walt, S. J.; Gommers,\nR.; Virtanen, P.; Cournapeau, D.; Wieser, E.; Taylor, J.; Berg,\nS.; Smith, N. J.; Kern, R.; Picus, M.; Hoyer, S.; van Kerk-\nwijk, M. H.; Brett, M.; Haldane, A.; del R´ıo, J. F.; Wiebe,\nM.; Peterson, P.; G´erard-Marchant, P.; Sheppard, K.; Reddy,\nT.; Weckesser, W.; Abbasi, H.; Gohlke, C.; and Oliphant,\nT. E. 2020.\nArray programming with NumPy.\nNature,\n585(7825): 357–362.\nHolliday, W. H.; Mandelkern, M.; and Zhang, C. E. 2024.\nConditional and Modal Reasoning in Large Language Mod-\nels. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, 3800–3821. Mi-\nami, Florida, USA: Association for Computational Linguis-\ntics.\nLee, D.; Hwang, Y.; Kim, Y.; Park, J.; and Jung, K. 2025.\nAre LLM-Judges Robust to Expressions of Uncertainty? In-\nvestigating the effect of Epistemic Markers on LLM-based\nEvaluation. In Proceedings of the 2025 NAACL-Long Pa-\npers.\nLin, S.; Hilton, J.; and Evans, O. 2022. Teaching models to\nexpress their uncertainty in words. Transactions on Machine\nLearning Research, 2022.\nNanda, N.; and Bloom, J. 2022. TransformerLens. https:\n//github.com/TransformerLensOrg/TransformerLens.\nPedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.;\nThirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss,\nR.; Dubourg, V.; Vanderplas, J.; Passos, A.; Cournapeau, D.;\nBrucher, M.; Perrot, M.; and Duchesnay, E. 2011. Scikit-\nlearn: Machine Learning in Python.\nJournal of Machine\nLearning Research, 12: 2825–2830.\nRimsky, N.; Gabrieli, N.; Schulz, J.; Tong, M.; Hubinger,\nE.; and Turner, A. 2024. Steering Llama 2 via Contrastive\nActivation Addition. In Ku, L.-W.; Martins, A.; and Sriku-\nmar, V., eds., Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 15504–15522. Bangkok, Thailand: Association for\nComputational Linguistics.\nTeam, Q. 2024. Qwen2.5: A Party of Foundation Models.\nVig, J.; Gehrmann, S.; Belinkov, Y.; Qian, S.; Nevo, D.;\nSinger, Y.; and Shieber, S. 2020. Investigating Gender Bias\nin Language Models Using Causal Mediation Analysis. In\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and\nLin, H., eds., Advances in Neural Information Processing\nSystems, volume 33, 12388–12401. Curran Associates, Inc.\nWes McKinney. 2010. Data Structures for Statistical Com-\nputing in Python. In St´efan van der Walt; and Jarrod Mill-\nman, eds., Proceedings of the 9th Python in Science Confer-\nence, 56 – 61.\nZhao, Z.; Ziser, Y.; and Cohen, S. B. 2024.\nLayer by\nLayer: Uncovering Where Multi-Task Learning Happens in\nInstruction-Tuned Large Language Models. In Al-Onaizan,\nY.; Bansal, M.; and Chen, Y.-N., eds., Proceedings of the\n2024 Conference on Empirical Methods in Natural Lan-\nguage Processing, 15195–15214. Miami, Florida, USA: As-\nsociation for Computational Linguistics.\nZhou, K.; Jurafsky, D.; and Hashimoto, T. 2023. Navigating\nthe Grey Area: How Expressions of Uncertainty and Over-\nconfidence Affect Language Models. https://arxiv.org/abs/\n2302.13439. ArXiv:2302.13439.\n"}, {"page": 6, "text": "Appendix\nLayer-wise Sensitivity to Linguistic Uncertainty\nTransformer-based language models adopt a deep, autore-\ngressive architecture in which representations are refined\nlayer by layer gradually building from lexical cues to nu-\nanced semantic abstraction. This layered structure raises a\nkey question: at which depth is linguistic uncertainty, espe-\ncially that conveyed by epistemic modals (e.g., must, might,\nshould, could), most saliently represented? To investigate\nthis, we analyze the Mean Sensitivity to Uncertainty (MSU)\nacross transformer layers for three Qwen2.5-0.5B variants\nChat, Instruct, and Base as well as the LLaMA 3.2-1B-\nInstruct model.\nAcross all models, a consistent pattern emerges: early lay-\ners (e.g., Layers 0-5) demonstrate low sensitivity to uncer-\ntainty (MSU approx. 2.5-6), reflecting a focus on surface-\nlevel representations. In contrast, later layers (e.g., Layers\n13-23) show a sharp rise in MSU, often surpassing 30, sug-\ngesting that deeper layers increasingly encode and amplify\nepistemic cues. For instance, in LLaMA 3.2-1B-Instruct,\nMSU climbs from 2.67 at Layer 0 to 31.96 at Layer 15.\nAmong the Qwen variants, the Chat model displays the high-\nest overall sensitivity, with elevated MSU values throughout\nthe stack, indicating a heightened responsiveness to modal\nuncertainty. These findings support the hypothesis that the\nsemantic abstraction required to capture epistemic nuance is\na late-stage phenomenon within the transformer hierarchy.\nFigure 5: Layer-wise MSU scores for Qwen2.5-0.5B-Chat.\nAmong all variants, the Chat model exhibits the highest\nsensitivity to linguistic uncertainty, with a steep increase in\nMSU across deeper layers.\nPCA-analysis\nWe observe a noticeable shift in the principal components\nof internal representations in the deeper layers of all mod-\nels (Figures: 10a). Specifically, while earlier and mid-level\nlayers exhibit stable projection patterns, the final layers dis-\nplay a reorientation in the direction of PC1 and PC2. This\nstructural transition likely reflects a late-stage reorganiza-\ntion of semantic or epistemic features, where uncertainty-\nrelated signals become more linearly separable or concen-\ntrated. Such emergent behavior may indicate that LLMs pro-\ngressively consolidate abstract modality cues toward the fi-\nnal layers, where decision-critical information is encoded.\nThis suggests that the geometry of representations not just\ntheir magnitude may carry functional signals related to epis-\ntemic reasoning.\nFigure 6: Layer-wise MSU scores for LLaMA 3.2-1B-\nInstruct, illustrating a steady increase in sensitivity to epis-\ntemic uncertainty across layers. The scores begin modestly\nin early layers (e.g., 2.68 at Layer 0) and rise sharply in\ndeeper layers, peaking at 31.96 in Layer 15. This trend sup-\nports the hypothesis that later layers in autoregressive trans-\nformers are more attuned to modeling linguistic uncertainty.\nDataset Examples\nFigure 7: Example from dataset used to probe\n"}, {"page": 7, "text": "Figure 8: Example from dataset used to probe\n(a) Layer 0\n(b) Layer 10\n(c) Layer 15\nFigure 9: PCA projections of internal representations at dif-\nferent layers of the LLaMA 3.2 1B model. While early and\nmid layers exhibit relatively stable clustering patterns, the\nfinal layer shows a notable shift in the orientation of the\nprincipal components, suggesting a reorganization of the\nrepresentational space. This directional change in PC1 vs.\nPC2 may reflect the model’s transition from encoding gen-\neral contextual features to more task-specific or decision-\nrelevant information.\n"}, {"page": 8, "text": "(a) Layer 0\n(b) Layer 13\n(c) Layer 23\nFigure 10: PCA projections of internal representations at different layers of the Qwen1.5-0.5B-Chat model. Most layers display\na consistent structure in the representational space; however, a marked shift in the direction of PC1 vs. PC2 emerges between\nlayers 13 and 23. This transition suggests that epistemic information becomes reorganized or amplified in deeper layers, aligning\nwith the model’s increasing sensitivity to uncertainty.\n"}]}