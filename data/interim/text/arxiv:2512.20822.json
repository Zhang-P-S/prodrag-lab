{"doc_id": "arxiv:2512.20822", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.20822.pdf", "meta": {"doc_id": "arxiv:2512.20822", "source": "arxiv", "arxiv_id": "2512.20822", "title": "MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs", "authors": ["Zhan Qu", "Michael Färber"], "published": "2025-12-23T22:52:24Z", "updated": "2025-12-23T22:52:24Z", "summary": "Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.20822v1", "url_pdf": "https://arxiv.org/pdf/2512.20822.pdf", "meta_path": "data/raw/arxiv/meta/2512.20822.json", "sha256": "c2e42e7b1562562df9684df1287adfc39e67e6dddc65aa489e8822678190be06", "status": "ok", "fetched_at": "2026-02-18T02:23:51.829472+00:00"}, "pages": [{"page": 1, "text": "MediEval: A Unified Medical Benchmark for Patient-Contextual and\nKnowledge-Grounded Reasoning in LLMs\nZhan Qu and Michael Färber\nTU Dresden and ScaDS.AI, Germany\n{zhan.qu, michael.faerber}@tu-dresden.de\nAbstract\nLarge Language Models (LLMs) are increas-\ningly applied to medicine, yet their adoption is\nlimited by concerns over reliability and safety.\nExisting evaluations either test factual medi-\ncal knowledge in isolation or assess patient-\nlevel reasoning without verifying correctness,\nleaving a critical gap. We introduce MediEval,\na benchmark that links MIMIC-IV electronic\nhealth records (EHRs) to a unified knowledge\nbase built from UMLS and other biomedical vo-\ncabularies. MediEval generates diverse factual\nand counterfactual medical statements within\nreal patient contexts, enabling systematic evalu-\nation across a 4-quadrant framework that jointly\nconsiders knowledge grounding and contextual\nconsistency. Using this framework, we identify\ncritical failure modes, including hallucinated\nsupport and truth inversion, that current propri-\netary, open-source, and domain-specific LLMs\nfrequently exhibit.\nTo address these risks,\nwe propose Counterfactual Risk-Aware Fine-\ntuning (CoRFu), a DPO-based method with\nan asymmetric penalty targeting unsafe con-\nfusions. CoRFu improves by +16.4 macro-F1\npoints over the base model and eliminates truth\ninversion errors, demonstrating both higher ac-\ncuracy and substantially greater safety.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable capabilities across diverse do-\nmains, with medicine being among the most high-\nimpact areas of application. In clinical contexts,\nLLMs have been explored for tasks such as summa-\nrizing electronic health records (EHRs), generat-\ning discharge instructions, providing clinical deci-\nsion support, and answering medical questions (Ra-\njpurkar et al., 2022; Singhal et al., 2025; Moor et al.,\n2023; Singhal et al., 2023; Pal et al., 2022). Their\nappeal lies in the ability to integrate unstructured\ntext with medical knowledge, potentially reducing\ndocumentation burden and assisting clinicians in\ndecision-making.\nTranslating research prototypes into real-world\ndeployment hinges critically on reliability and\nsafety. Unlike generic NLP tasks, medical reason-\ning requires not only factual correctness but also\ncontextual grounding in patient-specific data while\nadhering to verified medical knowledge. Errors\nin this setting are not mere degradations in per-\nformance but risks that can directly translate into\npatient harm (Thirunavukarasu et al., 2023; Yang\net al., 2023; Haltaufderheide and Ranisch, 2024).\nA critical challenge is that LLMs often fail to\napply medical knowledge consistently within the\nheterogeneous and noisy context of patient records\n(Zhou et al., 2025b). For example, a model may\nstate that metformin, a therapy for type 2 diabetes,\nis contraindicated in severe renal impairment, yet\nfail to apply this knowledge when the condition\nappears in a noisy and heterogeneous patient record.\nThe cause of such errors may be that current models\nare trained to recall facts in isolation rather than\nto integrate them with diverse patient information.\nSuch inconsistencies expose a critical gap between\nknowing medical facts and using them safely.\nExisting evaluation paradigms only partially ad-\ndress this gap. Medical data are uniquely challeng-\ning because patient records are heterogeneous with\nfree-text clinical notes and tabular entries coded in\ndifferent systems for diagnoses, procedures, and\nmedications. Medical knowledge is hierarchical\nand ontology-driven (e.g., UMLS, SNOMED CT,\nRxNorm), but its large scale, noise, and limited\ncross-vocabulary connectivity make consistent rea-\nsoning difficult. Benchmarks based on EHRs as-\nsess the ability to extract or reason over struc-\ntured data, but often reduce the task to retrieval\nor serialization without verifying medical sound-\nness (Lovón-Melgarejo et al., 2025). In contrast,\nknowledge-based evaluations test whether LLMs\ncan handle logical transformations of medical facts\n(Zhou et al., 2025a; Sung et al., 2021), but do not\nconnect reasoning to real patient contexts. The field\n1\narXiv:2512.20822v1  [cs.CL]  23 Dec 2025\n"}, {"page": 2, "text": "Figure 1: Overview of the current work with a real example; texts in blue indicate the extracted sample.\nthus lacks a unified framework that probes whether\nLLMs can (i) remain faithful to medical knowledge\nand (ii) apply it consistently to individual patient\nrecords.\nIn this paper, we address this gap with MediEval\n(Figure 1), a benchmark and evaluation framework\nthat links real patient records (MIMIC-IV) (John-\nson et al., 2023) with a unified biomedical knowl-\nedge base built from UMLS (Bodenreider, 2004),\nSNOMED CT (Donnelly et al., 2006), and RxNorm\n(Liu et al., 2005; Nelson et al., 2011). To ensure rig-\norous construction, MediEval constructs evaluation\nstatements by applying graph-guided substitutions\nand recombinations within biomedical ontologies,\nwith plausibility checks to ensure that true cases\nremain clinically valid while false ones are realis-\ntic and challenging. These statements are embed-\nded in real patient contexts, enabling evaluation\nof quadrant-level reasoning, defined as True/False\nwith respect to medical knowledge and Support-\ned/Unsupported with respect to the patient record.\nTo capture safety-critical errors, we introduce two\nnew metrics: Hallucinated Support Rate (HSR)\nand Truth Inversion Rate (TIR). We then conduct\na comprehensive evaluation of proprietary, open-\nsource (general-purposes), and biomedical LLMs\nunder this unified protocol, revealing consistent\nweaknesses across quadrants.\nTo mitigate the identified risks, we intro-\nduce Counterfactual Risk-Aware Fine-tuning\n(CoRFu), a DPO-based method that uses quadrant-\nstructured preference pairs and an asymmetric\npenalty to push models to prefer “True+Supported”\nresponses over safety-critical confusions (e.g.,\n“False+Supported”). This risk-aware, counterfac-\ntual approach targets precisely the failure modes\nthat conventional preference optimization over-\nlooks. Our code and data are publicly available1 .\nOur key contributions are:\n• MediEval Benchmark. A unified benchmark\nlinking EHRs with biomedical ontologies to\ntest both factual grounding and patient consis-\ntency, with novel safety evaluation metrics.\n• Comprehensive Evaluation. Cross-model\nevaluation of proprietary, open-source, and\nbiomedical LLMs under identical settings, re-\nvealing systematic quadrant-level weaknesses\nand safety-critical error patterns.\n• Counterfactual Risk-Aware Fine-Tuning\n(CoRFu). A risk-aware extension of DPO\nwith asymmetric penalties and quadrant-\naware approach that reduces safety-critical\nerrors in medical reasoning tasks.\n2\nRelated Work\nA growing body of work investigates whether\nLLMs can handle complex information in EHRs.\nOne study focused on data serialization and re-\ntrieval, with performance sensitive to prompt de-\nsign and feature selection (Lovón-Melgarejo et al.,\n2025). Another work has explored long-context\n1https://anonymous.4open.science/r/MediEval-5FA5/\n2\n"}, {"page": 3, "text": "modeling, showing some architectures can pro-\ncess entire patient timelines with >10k events and\nachieve greater robustness to irregular temporal pat-\nterns (Wornow et al., 2025). Other approaches inte-\ngrate structured and unstructured data, for instance,\nby using small auxiliary models as “knowledge\ntriggers” to support tabular prediction (Yan et al.,\n2025) or by designing code-aware representations\nthat capture semantic and temporal structure in di-\nagnosis prediction (Tan et al., 2025). These studies\ndemonstrate progress in surfacing and encoding\npatient information, but whether model outputs are\nconsistent with established clinical knowledge re-\nmains underexplored.\nAnother line of research investigates the relia-\nbility of the medical knowledge encoded in LLMs.\nDynamic probing methods show that models often\nachieve low joint accuracy when facts are rephrased\n(Zhou et al., 2025a), while multifaceted evaluations\nreveal brittle performance on tasks requiring com-\nparison, verification, or rectification (Zhou et al.,\n2024). Broader studies also report steep declines\nwhen moving from factual recall to scenario-based\nreasoning as cognitive complexity increases (Zhou\net al., 2025b). However, even when scenarios are\nconsidered, these evaluations do not disentangle\nwhether errors arise from factual misunderstanding,\nfailures of contextual grounding, or both.\nSeveral approaches have sought to improve the\nfactual reliability of LLMs by grounding them in\nstructured biomedical knowledge. These include\ntriplet generation with knowledge graph verifica-\ntion (Su et al., 2025), alignment with biomedical\nknowledge graph embeddings (Sakhovskiy and Tu-\ntubalina, 2025), and graph-based retrieval augmen-\ntation for medical question-answering (Wu et al.,\n2025). Scholars have also critiqued the evaluation\nparadigms: some argue that multiple-choice ques-\ntions reward shallow pattern recognition rather than\ngenuine medical knowledge (Griot et al., 2025),\nwhile others stress that medical benchmarks must\nprioritize construct validity to remain clinically\nmeaningful (Alaa et al., 2025). Automatic eval-\nuation frameworks, such as AutoMedEval (Zhang\net al., 2025), further highlight the need for scalable\nyet reliable assessment pipelines.\n3\nMethodology\n3.1\nMediEval Framework: Data Construction\nMediEval is a benchmark constructed from the\nMIMIC-IV database (Medical Information Mart\nfor Intensive Care IV) (Johnson et al., 2023) in\ncombination with biomedical ontologies such as\nUMLS (Bodenreider, 2004). It is designed to eval-\nuate whether large language models (LLMs) can\nperform clinical inference by verifying whether\nmedical statements are factually correct and prop-\nerly grounded in patient records. MediEval inte-\ngrates structured EHR tables, unstructured clinical\nnotes, and ontology-derived knowledge into a uni-\nfied dataset of context–statement pairs annotated\nwith correctness and support labels (see Figure 1).\nElectronic Health Records (EHRs) as Founda-\ntion.\nMIMIC-IV is a large-scale publicly avail-\nable collection of de-identified electronic health\nrecords (EHR) comprising 546,028 hospital admis-\nsions for 223,452 unique individuals. Two modali-\nties are utilized: (i) structured tabular data (lists of\ndiagnoses, procedures, and medications recorded\nwith ICD-CM, ICD-PCS, and NDC codes), and (ii)\nunstructured discharge summaries, which typically\ncontain about 22 sections such as Brief Hospital\nCourse, History of Present Illness, Major Surgical\nor Invasive Procedures, and Discharge Diagnoses.\nFor each admission a, we define the structured\nevent set and the discharge summary section set as\nSa = { Zdiag\nia , Zproc\nja , Zmed\nka },\n(1)\nDSa = { s(1)\na , s(2)\na , . . . , s(ma)\na\n}.\n(2)\nHere Zdiag\nia , Zproc\nja , and Zmed\nka\ndenote the lists of\ndiagnosis, procedure, and medication codes for\nadmission a, containing ia, ja, and ka elements\nrespectively, while DSa represents the set of ma\nsegmented sections from the discharge summary.\nSee Figure 1 for a real sample from MIMIC-IV.\nStep 1: Context Extraction.\nDischarge sum-\nmaries are divided into multiple sections, not all\nof which are equally informative. For MediEval,\nwe define the evaluation context Ca ⊂DSa as\nthe subset of sections that best capture patient tra-\njectory and clinical decision-making. In practice,\nwe select sections covering diagnoses, procedures,\ntreatments, medical history, and hospital course,\nas these jointly provide the most comprehensive\nevidence. Importantly, the reasoning that connects\nthese entities is not explicitly structured but em-\nbedded in free-text narratives, requiring models to\ninfer relations such as diagnosis–medication from\nnatural language.\n3\n"}, {"page": 4, "text": "Step 2: Semantic Normalization of Medical\nCodes.\nElectronic health records employ hetero-\ngeneous coding systems: diagnoses and procedures\nin MIMIC-IV are stored using ICD-9/10, while\nmedications are represented with National Drug\nCodes (NDC). To reason across modalities, these\ncodes must be projected into a unified semantic\nspace.\nWe achieve this by leveraging the Uni-\nfied Medical Language System (UMLS) (Boden-\nreider, 2004), which integrates over 60 families\nof biomedical vocabularies and provides explicit\nmappings between them. Specifically, ICD codes\nare aligned with SNOMED CT concepts, NDC\ncodes are aligned with RxNorm drug identifiers,\nand all are normalized into Concept Unique Identi-\nfiers (CUIs) that serve as universal nodes for cross-\nsystem integration. Formally,\nf : z 7→CUI(z), Ua = {f(z) | z ∈Sa}.\n(3)\nHere, z denotes a raw code that is being mapped\nto a CUI(z), and Ua is the set of normalized CUIs\nfor admission a. This normalization ensures that\ndiagnoses, procedures, and medications from dif-\nferent coding systems can be compared, linked, and\nreasoned over in a unified space.\nStep 3: Ontology Graph Construction and Rela-\ntion Extraction.\nFrom the normalized concepts\nacross the cohort, we construct a semantic graph\nG = (V, E), where V are CUIs and E are relations\ndrawn from the UMLS Metathesaurus, including\ntreated_by, has_associated_procedure, and is_a\netc. A dictionary mapping synonyms and abbrevi-\nations to CUIs supports robust entity matching in\ntext. Many clinically valid associations are not en-\ncoded as direct links but instead mediated through\nintermediate nodes such as therapeutic classes,\nanatomical structures, or related conditions. To\ncapture these, we allow multi-hop traversal in G,\nlimited to paths of length at most three in order to\npreserve clinical plausibility.\nFormally, for concepts h, t ∈V , let d(h, t) be\ntheir hop distance; we define their relations as\nR(h, t) = { r | (h, r, t) ∈G, d(h, t) ≤3 }\n(4)\nDirect relations. The semantic graph captures\nclinically meaningful associations as single-edge\nlinks. For instance, it encodes that the antihyper-\ntensive drug Lisinopril (CUI C0023861) is linked\nto the condition Hypertension (CUI C0020538)\nthrough a treated_by relation. Similarly, it rep-\nresents that the procedure Coronary Angioplasty\n(CUI C0001979) is associated with the diagnosis\nCoronary Artery Disease (CUI C0010054) via a\nhas_associated_procedure relation.\nMulti-hop inference. Not all clinically valid rela-\ntions are encoded as direct edges between specific\nentities in UMLS. For example, many drug–disease\nlinks are only represented at the therapeutic class\nlevel rather than for individual drugs. As a result,\nthe relation between Lisinopril and Hypertension\nmay absent as a direct edge. Instead, the graph\nencodes that Lisinopril (CUI C0023861) belongs\nto the class Angiotensin-Converting Enzyme In-\nhibitors (CUI C0003028) via an is_a relation, and\nthat this class as a whole is linked to Hypertensive\ndisease (CUI C0020538) via a treats relation:\n(C0023861) is_a\n−−→(C0003028) treats\n−−−→(C0020538).\nBy traversing this two-hop path, the system can\ninfer the treatment relation that is missing at the\nleaf level. Such multi-hop reasoning is essential\nfor capturing clinically valid associations that are\nindirectly encoded in UMLS.\nIn summary, Ua provides the admission-specific\nset of normalized concepts, while G encodes both\ndirect and inferred relations among these concepts.\nTogether they form the foundation for structured\nfact extraction in MediEval.\nStep 4: Quadrant-based Statement Construc-\ntion.\nFor each fact (h, r, t), we generate a set of\nsamples covering the four MediEval quadrants:\nYa(h, r, t) = {yQ1, yQ2, yQ3, yQ4}.\n(5)\nQuadrants are defined by the cross of factual cor-\nrectness (true vs. false) and contextual grounding\n(supported vs. unsupported), as summarized in Fig-\nure 1. This design ensures that evaluation covers\nboth straightforward comprehension and more sub-\ntle failure modes such as plausible contradictions,\nrelational mismatch, and erroneous information.\nThe quadrants are constructed as follows. Q1\n(True–Supported, Supported Fact) represents the\nbaseline case: a correct fact directly attested in the\npatient context Ca. Q2 (True–Unsupported, Plau-\nsible Contradiction) is generated by replacing an\nentity with a closely related alternative from the\nontology graph G (e.g., another entity sharing the\nsame parent concept) that is absent from Ca, yield-\ning a statement that is medically correct in general\nbut unsupported for this patient (see Figure 2 for an\nexample). Q3 (False–Supported, Relational Mis-\nmatch) recombines entities from distinct true facts,\n4\n"}, {"page": 5, "text": "creating a false relation even though all entities\nappear in Ca, thereby testing whether a model can\nresist misleading but superficially plausible associa-\ntions. Finally, Q4 (False–Unsupported, Erroneous\nInformation) introduces a semantically distant dis-\ntractor entity from G, producing a statement that is\nboth false and unsupported.\nThis procedure requires ontology-guided sub-\nstitutions, recombinations, and plausibility checks\nto guarantee that true statements remain clinically\nvalid while false ones are both plausible and chal-\nlenging. The resulting dataset integrates curated\nnarrative context, ontology-based code normaliza-\ntion, and graph reasoning into a unified framework\nfor rigorous evaluation of factual reliability and\ncontext adherence. See Figure 2 and Appendix A\nfor real samples of each quadrant.\nFigure 2: Example of statement verification against\npatient records (Quadrant 2: True-Unsupported). The\nstatement is medically correct, yet in this case, GERD is\ntreated with omeprazole, a different class of medication,\nso the statement is not supported by the context.\n3.2\nMediEval Framework: LLM Evaluation\nQuadrant Formulation as a Four-Way NLI Task.\nGiven a patient context c and a candidate medical\nstatement y, the evaluation task is to assign y to one\nof four quadrants q ∈{Q1, Q2, Q3, Q4}. This for-\nmulation extends natural language inference (NLI)\ninto a clinically grounded setting: rather than only\njudging whether a statement is correct, the model\nmust also decide whether it is grounded in the pa-\ntient record. By framing correctness and support\njointly in a 2×2 structure, MediEval captures safety-\ncritical confusions that cannot be identified when\nthese dimensions are assessed separately. Although\nmotivated by medicine, this setup provides a gen-\neral template for domains where factual validity\nand contextual grounding are both essential. To\nensure a fair comparison, all non-proprietary mod-\nels are fine-tuned under an identical supervised\nsetup with LoRA adapters and a classification head\non the four quadrants, ensuring that differences in\nperformance reflect model capabilities rather than\nvariations in training.\nBaseline Metrics.\nWe evaluate models on a test\nset Dtest = {(ci, yi, qi)} with gold quadrant labels\nqi and predictions ˆqi. Overall accuracy is defined\nas\nAccuracy =\n1\n|Dtest|\nX\ni\nI(ˆqi = qi).\n(6)\nIn addition, we report macro-averaged Precision,\nRecall, and F1 across quadrants, together with per-\nquadrant F1:\nF1(Qj) = 2 Prec(Qj) Rec(Qj)\nPrec(Qj) + Rec(Qj),\nj = 1, . . . , 4.\n(7)\nThese metrics capture overall discrimination\nability and expose potential biases, such as over-\npredicting “supported” statements.\nCritical Error Rates.\nBeyond global perfor-\nmance, clinical safety requires avoiding specific\nmisclassifications. A model may state a correct\nmedical fact but wrongly claim it is supported by\nthe patient record, or elevate an incorrect statement\nto seemingly valid evidence. To capture these en-\ntangled risks, we introduce two error rates:\nHallucinated Support Rate (HSR). The fraction\nof true but unsupported statements (Q2) misclassi-\nfied as true and supported (Q1):\nHSR =\nP\ni I(qi = Q2 ∧ˆqi = Q1)\nP\ni I(qi = Q2)\n.\n(8)\nA high HSR indicates that the model hallucinates\nevidence, presenting correct medical knowledge as\nif it were grounded in the record.\nTruth Inversion Rate (TIR). The proportion of\nfalse but superficially supported statements (Q3)\nmisclassified as true and supported (Q1):\nTIR =\nP\ni I(qi = Q3 ∧ˆqi = Q1)\nP\ni I(qi = Q3)\n.\n(9)\nA high TIR indicates that the model elevates incor-\nrect or unsafe claims to seemingly valid evidence,\na critical failure mode in clinical contexts.\n5\n"}, {"page": 6, "text": "Interpretation of Metrics.\nThese metrics disen-\ntangle complementary aspects of model behavior:\nOverall accuracy and macro-F1 capture balanced\nclassification performance. Per-quadrant F1 identi-\nfies systematic weaknesses (e.g., under-recognition\nof unsupported truths). HSR and TIR directly target\nthe most safety-critical confusions between quad-\nrants: hallucinating support and inverting truth.\n3.3\nCounterfactual Risk-Aware Fine-Tuning\nLarge language models are typically fine-tuned in\ntwo broad ways. The most direct approach is super-\nvised fine-tuning (SFT), where the model is trained\nto predict gold-standard labels with a cross-entropy\nloss. While simple and effective, SFT treats each\nlabel independently. In contrast, preference-based\noptimization exploits comparisons between a pre-\nferred and a dispreferred output. A prominent ex-\nample is Direct Preference Optimization (DPO),\nwhich updates the model to increase the relative\nlikelihood of preferred responses while staying\nclose to a reference model (Rafailov et al., 2023).\nBuilding on these foundations and the supervised\nfine-tuning results established in our evaluation\ntask, we propose CoRFu (Counterfactual Risk-\naware Fine-tuning), a contrastive fine-tuning strat-\negy tailored to the clinical safety challenges re-\nvealed by MediEval. Training proceeds on pref-\nerence triplets (c, yw, yl), where c is the clinical\ncontext, yw is a factually supported statement, and\nyl is a statement drawn from the counterfactual\nquadrants of MediEval. The key innovation is the\nCoRFu loss function, which augments the stan-\ndard DPO objective with an asymmetric penalty\nthat more severely punishes safety-critical errors.\nLet S denote the DPO preference margin:\nS(c; yw, yl) = β log[πθ(yw|c) πref(yl|c)\nπref(yw|c) πθ(yl|c)].\n(10)\nand define the CoRFu loss as:\nLCoRFu = −E\n\u0002\nlog σ(S)\n\u0003\n+ λ · E\n\u0002\nI(S < 0) · S2\u0003\n.\n(11)\nThe first term is the standard DPO objective,\nwhich encourages the model πθ to prefer yw over yl\nrelative to a reference policy πref. The second term\nintroduces an asymmetric penalty that activates\nonly when the model ranks yl above yw (S < 0).\nScaling this penalty quadratically in S dispropor-\ntionately punishes high-confidence mistakes, align-\ning optimization pressure with clinical safety by\ndiscouraging unsafe misinterpretations of evidence.\n4\nExperimental Setup\n4.1\nDataset\nThe final MediEval dataset is derived from MIMIC-\nIV v3.1 and consists of 2,015 unique hospital ad-\nmissions, linked to 8,350 medical statements and\nyielding 37,144 samples (see Appendix B for distri-\nbution of relation types). The context length varies\nfrom 340 to 2,827 tokens. To prevent data leak-\nage across splits, we retain only one admission per\npatient, since each patient may have multiple hos-\npitalizations. The dataset is partitioned into train-\ning, validation, and test sets in an 80/10/10 ratio,\nbalanced across the four quadrants. To ensure cor-\nrectness, 200 test samples were randomly selected\nfor human evaluation (details in Appendix C).\n4.2\nLarge Language Models and Baselines\nTo enable systematic comparison across model\nsizes and training regimes, we selected 15 LLMs\nspanning proprietary, open-source, and biomedi-\ncal domain-specific families. These include GPT\n(OpenAI) (OpenAI et al., 2024), LLaMA (Meta)\n(Grattafiori et al., 2024), Mistral (Mistral AI)\n(Jiang et al., 2024), Qwen (Alibaba) (Yang et al.,\n2025), and the biomedical models Meditron (Chen\net al., 2023), Med42 (Christophe et al., 2024), and\nClinicalCamel (Toma et al., 2023). For implement-\ning CoRFu, we used the Llama-3.1–8B-Instruct\nand Qwen3-8B models, with direct preference op-\ntimization (DPO) serving as a baseline.\n4.3\nTraining Setups and Evaluation Metrics\nLLM evaluation with supervised fine-tuning.\nAll non-proprietary models are fine-tuned under\nan identical supervised setup to ensure compara-\nbility. Each model is equipped with a sequence\nclassification head and trained on the four Me-\ndiEval quadrants using weighted cross-entropy loss.\nLoRA adapters are applied for parameter-efficient\nfine-tuning, with uniform hyperparameters across\nmodels (batch size, learning rate, and number of\nepochs). Proprietary models are evaluated in their\nbase zero-shot generative form without fine-tuning,\nand their results are shown in Appendix E.\nEvaluation of CoRFu.\nWe construct preference\npairs where Q1 (supported truth) is always the pre-\nferred output. We explore three regimes:\n• Pairwise: contrast Q1 with a specific error\ntype (Q2, Q3, or Q4).\n6\n"}, {"page": 7, "text": "• Mixed: contrast Q1 against a pooled set of\nQ2/3/4 in a single stage.\n• Curriculum: train sequentially on Q1 vs. Q2,\nthen Q1 vs. Q3, and finally Q1 vs. Q4, reusing\nthe updated model at each stage.\nThe mixed setup exposes the model to diverse error\ntypes simultaneously, while the curriculum regime\nintroduces them in stages of increasing difficulty,\nreducing interference from heterogeneous signals.\nBy encoding MediEval’s safety perspective directly\ninto both the loss and the training setups, CoRFu\naligns model fine-tuning not only with preference\ncorrectness but also with clinical reasoning.\nEvaluation Metrics.\nPerformance is measured\nby accuracy, macro-precision, macro-recall, macro-\nF1, and per-quadrant F1. Furthermore, critical error\nrates are quantified by the hallucinated support rate\n(HSR) and the truth inversion rate (TIR), for which\nlower values indicate better safety.\n5\nResults and Discussion\n5.1\nMediEval Benchmarking Results\nOverall Performance.\nTable 1 reports macro\nmetrics (accuracy, precision, recall, F1) and safety\nscores (HSR, TIR) on MediEval.\nOverall per-\nformance is limited given the clinical reason-\ning requirements, with accuracies ranging from\n59.3% to 73.9% and macro F1 from 50.3% to\n70.7% across base models. The strongest model\nis Llama-3.3-70B-Instruct, which achieves the\nhighest overall results in accuracy (73.9%), pre-\ncision (73.3%), recall (73.2%), and F1 (70.7%).\nAmong smaller models, Llama-3.1-8B-Instruct\nis competitive, reaching 61.5% macro F1.\nAl-\nthough pretrained on clinical corpora, biomedical\nLLMs do not dominate. These results suggest that\nlexical familiarity and domain exposure alone are\ninsufficient; fine-grained reasoning to disentangle\nfactual correctness from contextual grounding re-\nquires explicit supervision.\nPer-Quadrant Performance.\nPerformance is un-\neven across the four quadrants, but consistent with\ntheir expected difficulty. Q1 (Supported Fact) is\neasiest (best 86.7 by Llama-3.3-70B), consistent\nwith models’ strength on recognized truthful pat-\nterns in-context. Q2 (Plausible Contradiction) is\nharder, with the best model below 70%, showing\nthat LLMs often treat true but irrelevant statements\nas supported. Q3 (Relational Mismatch) stresses\nrelation-level reasoning; the best score reaches only\n69.3, showing limited resistance to superficially co-\nherent recombinations. Domain-specific models ex-\ncel on Q4 (Erroneous Information) (74.9 to 76.4%),\nlikely because clinical pretraining gives them\nstronger priors for rejecting clearly false medical\nstatements, though their broader reasoning across\nquadrants remains limited. Notably, even within\na single model (e.g., Llama-3.1-8B-Instruct),\nperformance varies sharply across quadrants, show-\ning that strength in one type of reasoning does\nnot transfer to others. This unevenness highlights\npersistent weaknesses in how models combine fac-\ntuality with patient grounding.\nSafety-Critical Errors.\nSafety-critical metrics\nexpose risks not captured by aggregate F1.\nMixtral-8x7B consistently achieves the lowest er-\nror rates, with HSR (Q2→Q1) of 20.5% and TIR\n(Q3→Q1) of 15.3%, whereas Llama-3.2-3B ex-\nhibits the highest risk on both metrics (HSR 40.9%,\nTIR 31.6%). This indicates that Mixtral is the safest\nmodel, even though it does not attain the highest\nmacro F1. In contrast, Llama-3.3-70B achieves\nthe best macro F1 (70.7%) yet fails to minimize\nHSR/TIR (21.2/21.1), demonstrating that higher ac-\ncuracy does not necessarily translate into safer clin-\nical reasoning. Overall, the divergence between F1\nand safety metrics shows that models can perform\nwell on average while still making risky errors.\n5.2\nCoRFu Evaluation Results\nOverall Effects.\nAcross both backbones, CoRFu\noutperforms the base and DPO models, but the\nstrongest gains concentrate in a single pairing. On\nLlama-3.1-8B-Instruct, Q1 vs. Q2 achieves the\nbest results at 76.8% accuracy and 77.9% macro\nF1 (+16.4 F1 over base), while other variants are\nlower (e.g., Q1 vs. Q3 at 72.9% F1, Q1 vs. mix\nat 69.6%, curriculum at 70.6%). For Qwen3-8B,\nthe pattern is sharper: only Q1 vs. Q2 yields a\nsubstantial aggregate improvement (70.7% Acc,\n71.0% F1; +11.1 F1 over base), whereas Q1 vs. Q3,\nQ1 vs. Q4, and curriculum underperform on macro\nmetrics. This shows that contrasting supported\nwith unsupported truths provides the most reliable\nsignal for improving overall reasoning, while other\npairings mainly target specific weaknesses.\nPer-Quadrant Improvements.\nCoRFu lifts per-\nquadrant F1 scores, but the biggest gains come\nfrom targeted pairings rather than curriculum. With\nthe Llama-3.1-8B-Instruct backbone, Q1 vs.\n7\n"}, {"page": 8, "text": "Table 1: MediEval benchmarking results (best results underlined) and CoRFu results (best results in bold).\nOverall Performance\nPer-Quadrant F1-Scores\nCritical Error Rates\nType\nModel\nAcc.\nPrec.\nRec.\nF1\nF1_Q1\nF1_Q2\nF1_Q3\nF1_Q4\nHSR\nTIR\nOpen-source\nLlama-3.1-8B-Instruct\n67.8\n66.4\n66.8\n61.5\n83.6\n64.0\n38.2\n60.0\n28.2\n21.1\n(with SFT)\nLlama-3.2-1B-Instruct\n64.1\n57.6\n64.3\n50.3\n80.9\n0.0\n68.1\n52.2\n27.3\n15.8\nLlama-3.2-3B-Instruct\n64.1\n61.8\n63.8\n61.6\n77.1\n53.8\n44.8\n70.9\n40.9\n31.6\nLlama-3.3-70B-Instruct\n73.9\n73.3\n73.2\n70.7\n86.7\n70.0\n65.9\n60.0\n21.2\n21.1\nMixtral-8x7B-Instruct-v0.1\n65.4\n69.6\n66.0\n63.8\n51.4\n68.3\n69.3\n66.4\n20.5\n15.3\nMistral-7B-Instruct-v0.3\n60.5\n61.0\n60.7\n59.7\n67.8\n55.6\n46.2\n69.2\n29.1\n26.3\nVicuna-13B-v1.5\n59.3\n59.3\n59.2\n59.3\n62.6\n56.1\n66.8\n51.6\n22.7\n15.5\nQwen3-4B\n61.7\n61.3\n61.7\n61.4\n71.7\n49.0\n63.3\n61.6\n31.8\n26.3\nQwen3-8B\n63.7\n64.4\n63.7\n59.9\n70.0\n58.6\n50.0\n60.8\n28.2\n31.1\nQwen3-32B\n62.9\n64.6\n63.8\n62.1\n72.9\n62.6\n51.4\n61.3\n28.2\n21.1\nDomain-specific\nMeditron-70B\n64.4\n62.3\n65.2\n68.0\n66.2\n64.3\n66.7\n74.9\n24.8\n26.3\n(with SFT)\nMed42-70B\n65.6\n63.3\n65.3\n62.8\n58.8\n55.8\n58.5\n76.4\n23.9\n25.6\nClinicalCamel-70B\n62.0\n61.7\n62.6\n62.0\n55.4\n54.3\n63.3\n75.0\n24.8\n25.6\nLlama-3.1-8B-Ins.\n+ DPO (Q1 vs. Q2)\n65.6\n59.7\n64.2\n59.5\n53.5\n55.1\n56.7\n72.7\n32.7\n27.9\n+ CoRFu (Q1 vs. Q2)\n76.8\n77.2\n77.0\n77.9\n73.2\n76.6\n78.9\n79.9\n18.2\n0.0\n+ CoRFu (Q1 vs. Q3)\n72.7\n66.8\n74.2\n72.9\n84.2\n69.1\n71.1\n67.1\n20.1\n11.5\n+ CoRFu (Q1 vs. Q4)\n64.5\n74.7\n63.6\n69.0\n70.6\n67.4\n70.0\n68.1\n19.9\n17.7\n+ CoRFu (Q1 vs. mix)\n69.1\n70.2\n69.9\n69.6\n67.7\n68.6\n72.1\n71.7\n23.1\n12.9\n+ CoRFu (Curriculum)\n64.5\n75.0\n63.6\n70.6\n70.6\n70.5\n72.3\n68.8\n19.1\n3.5\nQwen3-8B\n+ DPO (Q1 vs. Q2)\n51.0\n51.6\n50.9\n51.1\n51.2\n49.1\n56.7\n47.4\n22.7\n4.4\n+ CoRFu (Q1 vs. Q2)\n70.7\n71.5\n71.1\n71.0\n65.1\n66.7\n78.0\n74.3\n21.8\n0.0\n+ CoRFu (Q1 vs. Q3)\n61.1\n52.0\n59.9\n53.0\n52.9\n55.3\n49.3\n54.4\n22.3\n8.3\n+ CoRFu (Q1 vs. Q4)\n60.3\n55.2\n59.6\n50.7\n54.3\n50.1\n51.3\n46.7\n18.9\n7.6\n+ CoRFu (Q1 vs. mix)\n59.3\n60.7\n60.3\n59.3\n53.3\n52.4\n68.9\n62.7\n19.3\n13.3\n+ CoRFu (Curriculum)\n48.2\n46.0\n48.1\n47.7\n40.8\n45.3\n41.1\n49.2\n17.3\n14.5\nQ2 achieves the best balance across quadrants\n(Q1/Q2/Q3/Q4 = 73.2/76.6/78.9/79.9), improving\nboth factual grounding (Q2) and overall accuracy.\nTargeted objectives produce predictable effects:\ntraining on (Q1 vs. Q2) substantially boosts Q2 F1\n(+12.6 over base), while (Q1 vs. Q3) increases Q3\nF1. These results show that targeted pairings not\nonly correct specific weaknesses but also provide\ncomplementary improvements across quadrants.\nSafety-Critical Reductions.\nImprovements in\naccuracy do not automatically translate into safety,\nbut CoRFu reduces both HSR and TIR compared to\nbase and DPO. On Llama-3.1-8B, the (Q1 vs. Q2)\nrun achieves the lowest HSR at 18.2% and elimi-\nnates TIR entirely. On Qwen-8B, curriculum yields\nthe lowest HSR at 17.3%, while (Q1 vs. Q2) again\nminimizes TIR to 0.0. These results show that dif-\nferent contrastive objectives selectively mitigate\ndifferent risks, confirming that aggregate accuracy\nand safety-critical errors must be disentangled and\naddressed with tailored supervision.\nAblation Study.\nWe conducted an ablation study\nfor the value of λ. When λ = 0, the method re-\nduces to vanilla DPO, with performance reported in\nTable 1. Values between 0.5 and 1.0 yield the best\nmacro-F1 while significantly lowering HSR and\nTIR. For λ > 1.0, over-penalization occurs, lead-\ning to a decrease in macro-F1 and an increase in\nerror rates. See Appendix F & G for more analyses.\nOverall Insight.\nTaken together, the results show\nthat contrastive objectives shape model behavior in\ncomplementary ways. Q1–Q2 training provides the\nstrongest overall gains, improving accuracy, macro\nF1, and grounding while driving HSR to its lowest\nlevels. Q1–Q3 supervision instead strengthens sup-\nported reasoning and sharply lowers TIR, though\nwith smaller aggregate benefits. Curriculum is less\ncompetitive on headline metrics but can reduce\nHSR further for some backbones. Overall, these\nresults demonstrate that contrastive fine-tuning can\nbe directed to improve specific weaknesses, and\nthat MediEval makes such effects visible by disen-\ntangling accuracy from safety-critical errors.\n6\nConclusion\nWe introduced MediEval, a benchmark that jointly\nevaluates factual verification and patient-contextual\nreasoning by linking biomedical ontologies with\nreal-world patient records. Our results show that\ncurrent LLMs struggle with quadrant-level reason-\ning and make safety-critical errors not reflected in\naggregate accuracy. To mitigate these risks, we pro-\nposed Counterfactual Risk-Aware Fine-tuning\n(CoRFu), which reduces unsafe misclassifications\nthrough asymmetric preference optimization.\nFor future work, MediEval could be extended\nto other clinical tasks such as question answer-\ning, adapted to safety-critical domains beyond\nmedicine, and integrated with methods such as\nretrieval-augmented generation or models designed\nfor temporal patient reasoning. We hope this work\nprovides a foundation for more rigorous and risk-\naware evaluation of LLMs.\n8\n"}, {"page": 9, "text": "Limitations\nDataset size and coverage.\nMediEval is con-\nstructed from approximately 2k admissions in\nMIMIC-IV, yielding around 37k statements. This\nscale reflects a deliberate design choice to prior-\nitize clinical plausibility, careful quality control,\nand strict prevention of patient-level information\nleakage, which are particularly important in safety-\ncritical medical evaluation. The dataset construc-\ntion pipeline is fully automated and released with\nthe codebase, making MediEval readily extensible\nto larger cohorts, additional institutions, or alterna-\ntive clinical subsets as future work.\nAnnotation and validation. A subset of the\ntest set was independently reviewed by medically\ntrained annotators, with GPT-5 used as an auxiliary\nreference to inspect disagreements. High agree-\nment suggests that the ontology-grounded construc-\ntion leads to largely unambiguous labels. While\nbroader expert validation could further strengthen\nconfidence, the reliance on structured biomedical\nknowledge and deterministic generation reduces\nsubjectivity compared to fully manual annotation.\nData and ontology noise. As with any bench-\nmark grounded in real-world clinical data and large\nbiomedical ontologies, MediEval inherits noise\nand inconsistencies from MIMIC-IV and resources\nsuch as UMLS, SNOMED CT, and RxNorm. We\nmitigate these effects through ontology-guided\nplausibility checks, constrained multi-hop traversal,\nand explicit grounding in patient context. Further\nadvances in ontology curation and clinical data\nquality are likely to directly benefit benchmarks\nsuch as MediEval.\nReferences\nAhmed Alaa, Thomas Hartvigsen, Niloufar Golchini,\nShiladitya Dutta, Frances Dean, Inioluwa Deborah\nRaji, and Travis Zack. 2025. Position: Medical large\nlanguage model benchmarks should prioritize con-\nstruct validity. In Forty-second International Confer-\nence on Machine Learning Position Paper Track.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas\nKöpf, Amirkeivan Mohtashami, and 1 others. 2023.\nMeditron-70b: Scaling medical pretraining for large\nlanguage models. arXiv preprint arXiv:2311.16079.\nClément Christophe, Praveen K Kanithi, Prateek Mun-\njal, Tathagata Raha, Nasir Hayat, Ronnie Ra-\njan, Ahmed Al-Mahrooqi, Avani Gupta, Muham-\nmad Umar Salman, Gurpreet Gosal, and 1 others.\n2024. Med42–evaluating fine-tuning strategies for\nmedical llms: full-parameter vs. parameter-efficient\napproaches. arXiv preprint arXiv:2404.14779.\nKevin Donnelly and 1 others. 2006.\nSnomed-ct:\nThe advanced terminology and coding system for\nehealth. Studies in health technology and informat-\nics, 121:279.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\nMaxime Griot, Jean Vanderdonckt, Demet Yuksel, and\nCoralie Hemptinne. 2025.\nPattern recognition or\nmedical knowledge?\nthe problem with multiple-\nchoice questions in medicine. In Proceedings of the\n63rd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5321–5341.\nJoschka Haltaufderheide and Robert Ranisch. 2024.\nThe ethics of chatgpt in medicine and healthcare:\na systematic review on large language models (llms).\nNPJ digital medicine, 7(1):183.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lam-\nple, Lélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, and 7 others. 2024. Mixtral of experts.\nPreprint, arXiv:2401.04088.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nSimon Liu, Wei Ma, Robin Moore, Vikraman Gane-\nsan, and Stuart Nelson. 2005. Rxnorm: prescription\nfor electronic drug information exchange. IT profes-\nsional, 7(5):17–23.\nJesús Lovón-Melgarejo, Martin Mouysset, Jo Olei-\nwan, Jose G Moreno, Christine Damase-Michel, and\nLynda Tamine. 2025. Evaluating llm abilities to un-\nderstand tabular electronic health records: A compre-\nhensive study of patient data extraction and retrieval.\nIn European Conference on Information Retrieval,\npages 153–168. Springer.\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein\nAbad, Harlan M Krumholz, Jure Leskovec, Eric J\nTopol, and Pranav Rajpurkar. 2023. Foundation mod-\nels for generalist medical artificial intelligence. Na-\nture, 616(7956):259–265.\n9\n"}, {"page": 10, "text": "Stuart J Nelson, Kelly Zeng, John Kilbourne, Tammy\nPowell, and Robin Moore. 2011. Normalized names\nfor clinical drugs: Rxnorm at 6 years.\nJournal\nof the American Medical Informatics Association,\n18(4):441–448.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, Jeff Belgum, and\n6 others. 2024. Gpt-4 technical report. Preprint,\narXiv:2303.08774.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: your language\nmodel is secretly a reward model. In Proceedings of\nthe 37th International Conference on Neural Infor-\nmation Processing Systems, pages 53728–53741.\nPranav Rajpurkar, Emma Chen, Oishi Banerjee, and\nEric J Topol. 2022. Ai in health and medicine. Na-\nture medicine, 28(1):31–38.\nAndrey Sakhovskiy and Elena Tutubalina. 2025. Bali:\nEnhancing biomedical language representations\nthrough knowledge graph and language model align-\nment. In Proceedings of the 48th International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, pages 1152–1164.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\nand 1 others. 2023. Large language models encode\nclinical knowledge. Nature, 620(7972):172–180.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, and\n1 others. 2025. Toward expert-level medical ques-\ntion answering with large language models. Nature\nMedicine, 31(3):943–950.\nXiaorui Su, Yibo Wang, Shanghua Gao, Xiaolong\nLiu, Valentina Giunchiglia, Djork-Arné Clevert, and\nMarinka Zitnik. 2025. KGARevion: An AI agent for\nknowledge-intensive biomedical QA. In The Thir-\nteenth International Conference on Learning Repre-\nsentations.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4723–4734,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nYanchao Tan, Hang Lv, Yunfei Zhan, Guofang Ma,\nBo Xiong, and Carl Yang. 2025. BoxLM: Unify-\ning structures and semantics of medical concepts for\ndiagnosis prediction in healthcare. In Forty-second\nInternational Conference on Machine Learning.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nAugustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G\nKrishnan, Barry B Rubin, and Bo Wang. 2023. Clin-\nical camel: An open expert-level medical language\nmodel with dialogue-based knowledge encoding.\narXiv preprint arXiv:2305.12031.\nMichael Wornow, Suhana Bedi, Miguel Angel Fuentes\nHernandez, Ethan Steinberg, Jason Alan Fries,\nChristopher Re, Sanmi Koyejo, and Nigam Shah.\n2025. Context clues: Evaluating long context mod-\nels for clinical prediction tasks on EHR data. In\nThe Thirteenth International Conference on Learn-\ning Representations.\nJunde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min\nXu, Filippo Menolascina, Yueming Jin, and Vicente\nGrau. 2025. Medical graph RAG: Evidence-based\nmedical large language model via graph retrieval-\naugmented generation. In Proceedings of the 63rd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 28443–\n28467, Vienna, Austria. Association for Computa-\ntional Linguistics.\nJiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yao-\njun Hu, Jimeng Sun, and Jian Wu. 2025. Small mod-\nels are LLM knowledge triggers for medical tabular\nprediction. In The Thirteenth International Confer-\nence on Learning Representations.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 41\nothers. 2025.\nQwen3 technical report.\nPreprint,\narXiv:2505.09388.\nRui Yang, Ting Fang Tan, Wei Lu, Arun James\nThirunavukarasu, Daniel Shu Wei Ting, and Nan\nLiu. 2023. Large language models in health care:\nDevelopment, applications, and challenges. Health\nCare Science, 2(4):255–263.\nXiechi Zhang, Zetian Ouyang, Linlin Wang, Gerard\nDe Melo, Zhu Cao, Xiaoling Wang, Ya Zhang, Yan-\nfeng Wang, and Liang He. 2025. AutoMedEval: Har-\nnessing language models for automatic medical capa-\nbility evaluation. In Proceedings of the 63rd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6272–6285,\nVienna, Austria. Association for Computational Lin-\nguistics.\n10\n"}, {"page": 11, "text": "Yuxuan Zhou, Xien Liu, Chen Ning, and Ji Wu. 2024.\nMultifaceteval: multifaceted evaluation to probe llms\nin mastering medical knowledge. In Proceedings of\nthe Thirty-Third International Joint Conference on\nArtificial Intelligence, pages 6669–6677.\nYuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, and\nJi Wu. 2025a. Reliable and diverse evaluation of\nLLM medical knowledge mastery. In The Thirteenth\nInternational Conference on Learning Representa-\ntions.\nYuxuan Zhou, Xien Liu, Chenwei Yan, Chen Ning, Xiao\nZhang, Boxun Li, Xiangling Fu, Shijin Wang, Guop-\ning Hu, Yu Wang, and Ji Wu. 2025b. Evaluating\nLLMs across multi-cognitive levels: From medical\nknowledge mastery to scenario-based problem solv-\ning. In Forty-second International Conference on\nMachine Learning.\nAppendices\nA\nReal Samples for Each Quadrant\nThis appendix provides representative examples of\nMediEval samples from each of the four quadrants\n(see Figure 2 for Q2). Each example consists of\na patient context extracted from a real MIMIC-IV\ndischarge summary, a constructed medical state-\nment, and its corresponding quadrant label.\nFigure 3: Example of statement verification against pa-\ntient records (Quadrant 1: True-Supported). The state-\nment is medically correct, and GERD is indeed treated\nwith omeprazole, making it true and supported.\nFigure 4: Example of statement verification against\npatient records (Quadrant 3: False-Supported). The\ncrafted statement is medically incorrect, as Atenolol\nis indicated for hypertension. Since both GERD and\nAtenolol appear in the patient context, the statement\nmay seem supported, even though it is false.\n11\n"}, {"page": 12, "text": "Figure 5: Example of statement verification against\npatient records (Quadrant 4; False-Unsupported). The\nstatement is medically incorrect, since insulin is not\nindicated for GERD. Moreover, insulin does not appear\nin the patient context, making the statement false and\nunsupported.\nB\nDistribution of Relation Types\nThis appendix reports the distribution of biomedi-\ncal relation types used in MediEval. Relations are\nderived from UMLS and associated vocabularies\nafter semantic normalization and ontology-guided\ntraversal. The distribution reflects the clinical di-\nversity of the benchmark, covering treatment, diag-\nnostic, pharmacologic, and associative relations.\nTable 2: Distribution of relation types in the dataset\nRelation Type\nCount\ntreats\n17,540\nmay_be_treated_by\n4,552\nrelated_to\n4,008\nclassified_as\n3,774\ncontraindicated_class_of\n1,250\nsubset_includes\n1,568\nhas_pharmacologic_class\n854\ndiagnoses\n738\nprevents\n748\ncauses\n808\nassociated_with\n588\nauthorized_value\n282\nco-occurs_with\n130\ndefining_characteristic_of\n230\nmanifestation_of\n56\nsign_or_symptom_of\n12\nmay_be_diagnosed_by\n4\npart_of\n2\nTotal\n37,144\nC\nHuman Evaluation\nWe conducted human validation on 200 balanced\nsamples. Two medically trained annotators labeled\neach item independently. In cases of disagreement,\nGPT-5 was consulted as an auxiliary signal for\nsanity checking, but final labels were determined\nby the human annotators. Agreement was 97%\n(194/200), with Cohen’s kappa ≈0.96, indicat-\ning strong agreement. All disagreements occurred\nbetween Q2 and Q3, which is one of the subtle dis-\ntinctions that the benchmark is designed to probe.\nBecause our extraction pipeline is fully ontology-\ndriven and deterministic, the errors stem from noise\nor inconsistencies in the underlying ontologies or\nEHR data.\nD\nImplementation Details\nAll training was performed on an NVIDIA L40S\nGPU with 46 GB memory. All reported results are\naveraged over 3 random seeds (42, 43, 44). For\nthe MediEval supervised baselines, we fine-tuned\nmodels for 3 epochs with per-device batch size\n4, gradient accumulation 8, maximum sequence\nlength 4096, learning rate 2 × 10−5, weight decay\n0.01, and warmup ratio 0.03. When enabled, LoRA\nused rank r = 8, α = 16, and dropout 0.05. For\nthe CoRFu experiments, we fine-tuned with maxi-\nmum sequence length 4096, per-device batch size\n1, gradient accumulation 16 (effective batch size\n16), and trained for 1 epoch. We used the AdamW\noptimizer with learning rate 1 × 10−4, β1 = 0.9,\nβ2 = 0.999, weight decay 0.01, warmup ratio 0.05,\nand gradient clipping at 1.0. LoRA was configured\nwith rank r = 16, α = 32, and dropout 0.1. The\nCoRFu loss used β = 0.1 and λ = 0.5.\nE\nEvaluation of Proprietary GPT Models\nGPT-4o and GPT-5 are evaluated under a realis-\ntic clinical RAG setting, following their typical\nusage: zero-shot inference without task-specific\nfine-tuning. This reflects practical deployment con-\nstraints, where adapting proprietary models is often\ninfeasible. Our goal is not a direct head-to-head\ncomparison, but rather to highlight two complemen-\ntary observations: (1) the task remains challenging\neven for frontier models without adaptation, and\n(2) smaller models equipped with targeted supervi-\nsion (SFT/CoRFu) can outperform general-purpose\nLLMs on safety-critical dimensions.\nFor proprietary models, the prompt consists of\nthe following components: (1) context, (2) state-\n12\n"}, {"page": 13, "text": "Table 3: MediEval benchmarking results of Proprietary GPT Models.\nOverall Performance\nPer-Quadrant F1-Scores\nCritical Error Rates\nModel\nAcc.\nPrec.\nRec.\nF1\nF1_Q1\nF1_Q2\nF1_Q3\nF1_Q4\nHSR\nTIR\nGPT-5\n45.7\n36.4\n46.7\n30.4\n68.3\n53.3\n0.0\n0.0\n19.3\n-\nGPT-4o\n49.3\n31.6\n50.0\n29.1\n66.3\n50.0\n0.0\n0.0\n46.7\n-\nment, (3) definitions of the four quadrants, (4) an\ninstruction to select exactly one label, and (5) the\nlist of candidate labels.\nOverall, proprietary models struggle on this task.\nGPT models achieve only 30.4% (GPT-4o) and\n29.1% (GPT-5) macro-F1. Since both models are\nevaluated in their base generative form without\ntask-specific adaptation, this performance reflects\nboth the difficulty of MediEval and the lack of\nalignment to the evaluation protocol.\nIn terms of safety errors, HSR (Q2→Q1) ranges\nfrom 19.3% for GPT-5 to 46.7% for GPT-4o. No-\ntably, proprietary models never predict Q3, result-\ning in zero TIR. This behavior indicates miscalibra-\ntion rather than genuine safety awareness.\nF\nAblation Study\nFigure 6 illustrates the effect of the regularization\ncoefficient λ on performance and error rates. For\nboth models, moderate values of λ (around 0.5–\n1.0) achieve the best trade-off: macro-F1 is maxi-\nmized, while HSR and TIR are substantially re-\nduced compared to λ = 0, which corresponds\nto vanilla DPO. As λ increases further, perfor-\nmance degrades and error rates rise, indicating over-\npenalization of negative preference margins. This\nablation is necessary because λ directly controls\nthe strength of the corrective penalty introduced by\nCoRFu, balancing preference optimization against\nerror suppression. In contrast, we fix the DPO tem-\nperature β, since it only rescales the preference\nmargin and its effect can be absorbed into λ.\nG\nQualitative Evaluation\nWe further analyze whether model behavior cor-\nrelates with input characteristics such as context\nlength or relation type, across all evaluated set-\ntings, including SFT and CoRFu. We observe no\nconsistent correlation between any of the reported\nevaluation metrics and context length, nor system-\natic performance differences across relation types\nfor any model. These results indicate that the ob-\nserved performance trends are not driven by input\ncomplexity or relation semantics, but instead reflect\nFigure 6: Effect of λ on F1, HSR, and TIR for Qwen3-\n8B and Llama3.2-8B.\ndifferences in training objectives and supervision\nstrategies.\nInstead, errors predominantly arise from confu-\nsions between Q2 and Q1, indicating difficulties in\ndistinguishing between partially incorrect and fully\nincorrect cases. We provide a qualitative analysis\nof these failure modes below.\nQualitative Example - hadm_id: 24480054\nStatement. Gastroesophageal reflux disease\nmay be treated by aluminum hydroxide, which\nis a type of antacid.\nContext.\n13\n"}, {"page": 14, "text": "History of Present Illness:\n___ is a G4P3 female with a history of\nthrombophilia and cerebrovascular accident\nwho presented to an outside hospital with\nworsening epigastric pain without overt\nsigns of labor. She was admitted in early\nlabor and subsequently delivered her baby on\nthe morning of ___. However, her abdominal\npain continued to worsen during\nhospitalization.\nShe has a history of gastroesophageal reflux\ndisease previously treated effectively with\npantoprazole, later transitioned to\nomeprazole due to insurance changes, with\nrecurrence of symptoms. During the first\ntrimester of pregnancy, she experienced\nworsening epigastric pain with significant\nnausea and vomiting. After the first\ntrimester, nausea and vomiting improved, and\nher heartburn and epigastric pain also\nimproved. In the week prior to delivery, her\nsymptoms worsened again, with pain primarily\nlocalized to the epigastrium and exacerbated\nby movement. She attempted over-the-counter\nTums and Pepcid without relief.\nAt the outside hospital, she underwent\nabdominal ultrasound and CT imaging\ndemonstrating pancreatitis. She was noted to\nhave leukocytosis and received imipenem\nprior to transfer. She received at least 4~L\nof normal saline and voided at least 350~cc.\nShe reported bilious emesis but denied\nongoing nausea.\nHer delivery was uncomplicated. Her newborn\nson, ___, is healthy. No estimated blood\nloss was recorded, and delivery was vaginal.\nShe continued to experience suprapubic pain.\nOn arrival to the MICU, vital signs were:\ntemperature 97.9$^\\circ$F, blood pressure\n156/97, heart rate 107, respiratory rate 16,\nand SpO$_2$ 97\\% on nasal cannula. She\nreported abdominal pain that was tolerable\nand improved with hydromorphone. She denied\nshortness of breath. She had not passed gas,\nand her last bowel movement was ___.\nPast Medical History:\n- Stroke associated with Depo-Provera,\nwithout residual deficits\n- Thrombophilia (MTHFR deficiency)\n- Hepatitis C (cleared virus; viral load\nundetectable, antibody positive; no IVDU;\nhistory of blood transfusion ___)\n- Ruptured appendix ___\n- Morbid obesity\n- Gastroesophageal reflux disease\n- Pregnancy in ___ with possible fetal\nalcohol syndrome, ADHD, and bipolar\ndisorder; no issues in ___ or ___ pregnancies\nBrief Hospital Course:\nSevere Acute Pancreatitis: The patient was\nadmitted to the MICU and kept NPO. MRCP\ndemonstrated greater than 30\\% pancreatic\nnecrosis with a hemorrhagic component. A\nnasojejunal tube was placed, and tube feeds\nwere initiated. Initial intolerance improved\nwith cycling. No drainable fluid collections\nwere identified. Gastroenterology followed\nthroughout admission. She was discharged on\na clear liquid diet with plans to advance to\na low-fat diet. Persistent left upper\nquadrant pain gradually improved.\nThrombophilia: The patient has a history of\nMTHFR deficiency and prior stroke. A new\nnon-occlusive portal vein thrombosis was\nidentified during admission. Therapeutic\nenoxaparin was initiated after stabilization\nof hemorrhagic pancreatitis, with a planned\nduration of ___ months and hematology\nfollow-up.\nPostpartum Status: The patient had an\nuncomplicated vaginal delivery of a healthy\ninfant on ___. Social work was involved due\nto limited home support.\nAcute Kidney Injury: An acute kidney injury\nwas present on admission, likely secondary\nto pancreatitis, and resolved with\nintravenous fluid resuscitation.\nIleus and Diarrhea: An initial ileus was\ntreated conservatively. The patient later\ndeveloped tube-feed--associated diarrhea.\n\\textit{Clostridioides difficile} testing\nwas negative. Symptoms improved with\nas-needed loperamide.\nNew-Onset Diabetes Mellitus: Hyperglycemia\ndeveloped after initiation of tube feeds,\nlikely secondary to severe pancreatitis. The\npatient was started on an insulin sliding\nscale. Due to clinical complexity, she was\ndischarged on a Humalog sliding scale only\nand requires outpatient reassessment.\nInactive Issues:\nGastroesophageal reflux disease: continue\npantoprazole.\nDischarge Diagnoses:\n- Acute severe pancreatitis\n- Hyperglycemia, likely new-onset diabetes\nmellitus\n- Portal vein thrombosis\n- Acute kidney injury, resolved\nLabel. True-Supported\nThe statement is medically correct in general;\nhowever, the specific treatment (aluminum hydrox-\nide) is not mentioned in the clinical context. While\nGERD is documented and treated with pantopra-\nzole and other antacids, the generated statement\nintroduces an unsupported medication, and is there-\nfore labeled as Q2.\nAlmost all models fail on this example due to\na combination of lexical bias and treatment-level\n14\n"}, {"page": 15, "text": "over-generalization. The presence of strong sur-\nface cues such as GERD, reflux, and references\nto antacid use biases models toward conceptual\nmatching, leading them to infer support even when\nthe specific medication (aluminum hydroxide) is\nnot mentioned.\nIn addition, models tend to collapse distinc-\ntions within therapeutic classes, treating different\nantacids as interchangeable. This error is further\nexacerbated by clinical complexity rather than con-\ntext length: the note is dominated by severe acute\npancreatitis and post-partum complications, while\nGERD appears as a secondary issue, making fine-\ngrained verification of drug-level evidence partic-\nularly challenging. Together, these factors reveal\na persistent difficulty in jointly verifying medical\ncorrectness and strict contextual grounding.\n15\n"}]}