{"doc_id": "pdf:054231038b7dd4c5", "source": "zh", "lang": "zh", "pdf_path": "data/raw/zh/pdf/基于深度学习的无监督医学图像配准算法研究_徐奥博.pdf", "meta": {"doc_id": "pdf:054231038b7dd4c5", "source": "local_pdf", "title": "基于深度学习的无监督医学图像", "title_from": "pdf_meta|first_page|filename", "language": "mixed_or_unknown", "sha256": "054231038b7dd4c5c546f4e266bb1e87db2845a54c3caa27ab1ca2fd711ef217", "file_name": "基于深度学习的无监督医学图像配准算法研究_徐奥博.pdf", "page_count": 73, "pdf_metadata": {"format": "PDF 1.6", "title": "", "author": "CNKI", "subject": "", "keywords": "", "creator": "ReaderEx_DIS 2.5.0 Build 4088", "producer": "TTKN", "creationDate": "D:20250926150227-08'00'", "modDate": "", "trapped": "", "encryption": "Standard V4 R4 128-bit AES"}, "created_at": "2026-02-18T05:22:55.265235+00:00", "status": "ok"}, "pages": [{"page": 1, "text": "基于深度学习的无监督医学图像\n配准算法研究\n二○二五年五月\n硕士专业\n学位论文\n"}, {"page": 2, "text": "分类号\nTP391\n学校代码\n10593\n密级\n公开\n学号\n2213391013\n硕士专业学位论文\n基于深度学习的无监督医学图像\n配准算法研究\nRESEARCH ON UNSUPERVISED MEDICAL\nIMAGE REGISTRATION METHOD BASED\nON DEEP LEARNING\n作者姓名：\n徐奥博\n指导教师：\n张学军\n合作导师：\n吴东波\n专业名称：\n新一代电子信息技术\n研究方向：\n医学图像配准\n所在学院：计算机与电子信息学院\n论文答辩日期2025 年5 月26 日\n学位授予日期2025 年6 月20 日\n答辩委员会主席\n陈海强\n"}, {"page": 3, "text": "I\n基于深度学习的无监督医学图像配准算法研究\n摘要\n医学图像配准在手术导航和影像引导诊断等临床应用中具有重要作\n用。然而，传统卷积神经网络受限于卷积核大小主要聚焦局部特征，难以\n建模长距离体素依赖关系；基于自注意力机制的Transformer 架构虽能捕\n获全局相关性，但其二次计算复杂度在处理高分辨率三维医学影像时面临\n巨大计算开销，且其空间局部性较弱依赖大规模训练数据和庞大模型参数，\n难以满足临床低资源部署需求。此外，传统U 型网络的下采样与上采样策\n略易导致空间细节丢失，影响配准精度。针对上述挑战，本文提出两种创\n新架构HybridMorph 与WaveMorph，分别从轻量化全局建模和多尺度特征\n优化角度突破技术瓶颈。\nHybridMorph 架构聚焦于解决Transformer 的高计算成本问题，通过\n融合结构化状态空间对偶性（State Space Duality, SSD）框架与深度可分离\n卷积操作，构建残差混合模块（Residual Hybrid Module, RHM）与并行通\n道特征聚合器（Parallel Channel Feature Aggregator, PCFA）。RHM 利用结\n构化状态空间对偶性（SSD）框架的线性复杂度优势，在引入极少量参数\n的前提下，实现局部空间特征与长距离依赖关系的高效整合使其在准确理\n解移动图像和固定图像之间的非线性空间关系方面展现出显著的优势和\n效率。PCFA 通过轻量化设计，使HybridMorph 相比TransMorph 减少4.1\n倍计算量和7.3 倍参数量，显著提升特征提取效率。该架构提供三种不同\n参数量版本，适配不同计算资源场景。\nWaveMorph 架构针对传统下采样与上采样的信息丢失问题，将Haar\n小波无损分解特性与ConvNeXt 架构相结合，提出了一种新颖的多尺度小\n波特征融合下采样模块，该模块使用多尺度卷积核从八个频率子图像中提\n取和融合特征，避免关键解剖结构细节在下采样过程中丢失。此外，在解\n"}, {"page": 4, "text": "II\n码器端引入了一个轻量级的动态上采样模块，基于图像局部特征自适应调\n整策略，精准恢复高频细节，改善配准过程中的拓扑结构失真。WaveMorph\n将卷积神经网络（CNN）的归纳偏差与Transformer 的优势相结合，有效\n地缓解了由于空间信息丢失而导致的拓扑失真问题，将推理速度提升至\n0.072 秒/图像，兼顾高精度与实时性。\n关键词：可变形医学图像配准\n无监督深度学习\n结构化状态空间对偶\nHaar 小波\nConvNeXt\n轻量化模型\n"}, {"page": 5, "text": "III\nRESEARCH ON UNSUPERVISED MEDICAL\nIMAGE REGISTRATION METHOD\nBASED ON DEEP LEARNING\nABSTRACT\nMedical image registration is crucial in clinical applications such as\nsurgical\nnavigation\nand\nimage-guided\ndiagnosis.\nHowever,\ntraditional\nconvolutional neural networks (CNNs) are limited by the size of convolution\nkernels, focusing mainly on local features and struggling to model long-range\nvoxel\ndependencies.\nBased\non\nself-attention\nmechanisms,\ntransformer\narchitectures can capture global correlations but face significant computational\ncosts due to their quadratic complexity when processing high-resolution 3D\nmedical images. Moreover, Transformers’ weak spatial locality requires\nlarge-scale training data and substantial model parameters, making them\nunsuitable\nfor\nlow-resource\nclinical\ndeployment.\nTraditional\nU-Net\narchitectures\nsuffer\nfrom\ninformation\nloss\nduring\ndown-sampling\nand\nup-sampling, affecting registration accuracy. To address these challenges, this\npaper introduces two innovative architectures: HybridMorph and WaveMorph,\neach targeting different aspects of technical limitations through lightweight\nglobal modeling and multi-scale feature optimization.\nThe\nHybridMorph\narchitecture\nfocuses\non\nmitigating\nthe\nhigh\ncomputational cost of Transformers by integrating the Structured State Space\nDuality (SSD) framework with depthwise separable convolutions to construct\nResidual Hybrid Module (RHM) and Parallel Channel Feature Aggregator\n(PCFA). The RHM leverages the linear complexity advantage of the SSD\nframework to efficiently integrate local spatial features with long-range\ndependencies,\ndemonstrating\nsignificant\nadvantages\nand\nefficiency\nin\naccurately understanding the nonlinear spatial relationships between moving\nand fixed images. The lightweight design of PCFA enables HybridMorph to\nachieve a 4.1-fold reduction in computation and a 7.3-fold reduction in\nparameters\ncompared\nto\nTransMorph,\nsignificantly\nenhancing\nfeature\nextraction efficiency. This architecture offers three versions with varying\n"}, {"page": 6, "text": "IV\nparameter sizes to accommodate different computational resource scenarios.\nThe WaveMorph addresses the issue of information loss during traditional\ndown-sampling\nand\nup-sampling\nby\ncombining\nHaar\nwavelet\nlossless\ndecomposition\nwith\nthe\nConvNeXt\narchitecture,\nintroducing\na\nnovel\nmulti-scale wavelet feature fusion down-sampling module. This module\nextracts and fuses features from eight frequency sub-images using multi-scale\nconvolution kernels, preventing the loss of critical anatomical details during\ndown-sampling. Additionally, a lightweight dynamic up-sampling module is\nintroduced at the decoder end, employing an adaptive strategy based on local\nimage features to precisely recover high-frequency details and improve\ntopological distortion during registration. By merging the inductive biases of\nCNNs with the strengths of Transformers, WaveMorph effectively alleviates\ntopological distortions caused by spatial information loss, achieving an\ninference speed of 0.072 seconds per image while balancing high precision and\nreal-time performance. This dual approach not only enhances registration\naccuracy but also meets the stringent latency requirements of clinical\napplications.\nKEY WORDS: Deformable Medical Image Registration; Unsupervised\nDeep Learning; Structured State Space Duality; Haar wavelet; ConvNeXt;\nLight-weight Model\n"}, {"page": 7, "text": "目  录 \n第一章 绪论............................................................................................... 1 \n1.1 研究背景及意义 ............................................................................... 1 \n1.2 国内外研究现状 ............................................................................... 2 \n1.2.1 传统医学图像配准方法 ............................................................. 2 \n1.2.2 基于深度学习的医学图像配准方法 ........................................ 3 \n1.3 本文主要研究内容 ........................................................................... 4 \n1.4 本文组织结构 ................................................................................... 6 \n第二章 相关基础知识 .............................................................................. 7 \n2.1 医学图像配准理论基础................................................................... 7 \n2.1.1 医学图像配准定义 ..................................................................... 7 \n2.1.2 医学图像配准基础框架 ............................................................. 7 \n2.2 深度学习网络模型 ......................................................................... 10 \n2.2.1 卷积神经网络 ........................................................................... 10 \n2.2.2 U-Net 网络模型 ......................................................................... 11 \n2.2.3 ConvNeXt 网络模型 ................................................................ 12 \n2.2.4 结构化状态空间对偶 ............................................................... 13 \n2.2.5 小波变换 ................................................................................... 15 \n2.3 损失函数与评价指标 ..................................................................... 16 \n2.3.1 损失函数 ................................................................................... 16 \n2.3.2 评价指标 ................................................................................... 16 \n2.4 空间变换网络 ................................................................................. 17 \n2.5 本章小结 ......................................................................................... 17 \n第三章 轻量级混合Mamba-2 的无监督医学图像配准方法.............. 18 \n3.1 引言 ................................................................................................. 18 \n3.2 轻量级混合Mamba-2 的无监督医学图像配准 .......................... 19 \n3.2.1 可变形配准框架 ....................................................................... 19 \n3.2.2 残差混合模块 ........................................................................... 21 \n3.2.3 并行通道特征聚合模块 ........................................................... 23 \n3.3 实验结果与分析 ............................................................................. 25 \n3.3.1 数据集与处理 ........................................................................... 25 \n3.3.2 实验设置 ................................................................................... 25 \n3.3.3 模型复杂度研究 ....................................................................... 26 \n3.3.4 实验分析 ................................................................................... 26 \n3.4 本章小结 ......................................................................................... 32 \n"}, {"page": 8, "text": "第四章 小波引导的多尺度 ConvNeXt 的无监督医学图像配准 ....... 34 \n4.1 引言 ................................................................................................. 34 \n4.2 基于小波引导的多尺度 ConvNeXt 方法的无监督医学图像配\n准 .......................................................................................................... 36 \n4.2.1 可变形配准框架 ....................................................................... 36 \n4.2.2 多尺度小波特征融合模块 ....................................................... 37 \n4.2.3 瓶颈模块 ................................................................................... 40 \n4.2.4 轻量化动态上采样模块 ........................................................... 41 \n4.3 实验结果与分析 ............................................................................. 41 \n4.3.1 数据集与预处理 ....................................................................... 41 \n4.3.2 实验设置 ................................................................................... 42 \n4.3.3 实验分析 ................................................................................... 43 \n4.3.4 消融实验 ................................................................................... 47 \n4.3.5 讨论 ........................................................................................... 49 \n4.4 本章小结 ......................................................................................... 52 \n第五章 结论与展望 ................................................................................ 54 \n5.1 结论 ................................................................................................. 54 \n5.2 创新点 ............................................................................................. 54 \n5.3 展望 ................................................................................................. 55 \n参考文献 ................................................................................................... 56 \n"}, {"page": 9, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n1\n第一章绪论\n1.1 研究背景及意义\n随着医学成像技术的快速发展，多模态成像技术已成为现代临床诊疗的核心工具，\n为临床医生提供更客观全面的诊断和治疗参考依据。医学图像分析在计算机辅助诊断、\n手术导航、跟踪疾病进展等环节发挥着无可替代的作用，能够缓解临床实践中由于临\n床医生的知识差异、疲劳以及医疗资源缺乏等负面因素。然而，由于不同的数据采集\n模态、不同时间、不同个体或者是不同维度的医学图像在空间分辨率、功能信息表达\n等方面存在显著差异，仅通过临床医生肉眼观察难以进行综合研判，失败或异常的对\n应关系会影响诊断、治疗计划和疾病进展监测。因此，医学图像分析中的许多工作流\n程要求图像处于标准坐标系中，以便进行比较、分析和可视化。\n医学图像配准技术由此在许多临床医疗应用中发挥着无可替代的作用，旨在将不\n同影像上的解剖点，或具有诊断意义的点以及感兴趣区域（Region of Interesting, ROI）\n建立密集的非线性空间对应关系，以提高图像分析的准确性和可靠性。医学图像配准\n常用于捕获和量化不同解剖结构的生物力学和动力学，包括心肌运动跟踪[1, 2]，以及放\n射治疗中器官运动的跟踪等[3, 4]。在神经成像领域，医学图像配准通过将患病患者和解\n剖模板（Atlas）在标准坐标系中进行解剖学对应有助于识别和定位异常，如肿瘤，病\n变或萎缩[5]。\n医学图像配准可以根据组织特性分为应用于刚性组织的刚性配准和应用于柔软组\n织的非刚性配准（或称为可变形配准，除特殊说明，下文将采用可变形配准进行论述）。\n在过去很长一段时间，大多数图像配准都是由医生手动完成的，而手动校准的准确性\n在很大程度上依赖于医生的能力，这可能在临床上影响某些配准任务的质量[6]。\n随着计算机技术的快速发展，由计算机辅助的自动配准逐渐克服了手动图像配准\n可能存在的一些缺点。相对于较为简单的刚性配准，传统计算机辅助的可变形配准通\n常基于实例优化预定义的形变模型和目标函数，逐像素（或体素）优化来实现非线性\n空间映射。然而，实例优化的特性使其无法利用先验知识（例如，解剖标志、专家注\n"}, {"page": 10, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n2\n释或标签图），这导致了高昂的计算成本和时间消耗，限制了其在不同临床任务下的\n通用性[7]。同时传统方法的缓慢的收敛性和复杂的参数调优过程使其难以部署到临床\n实时应用中，如手术导航等需要快速响应的场景。深度学习的复兴改变了图像配准研\n究的背景，为克服这些挑战的提供了新路径，逐渐成为近年研究热点。与传统医学图\n像配准方法[8-10]相比，基于深度学习的方法大幅度提高了配准效率和准确性。这些方\n法通过迭代学习全局优化函数，取替了传统方法的实例优化，从而实现了在未见过的\n医学图像对上的快速配准。\n1.2 国内外研究现状\n医学图像配准技术可以根据不同的维度进行划分，具体来说，从配准方式可分为\n传统基于优化的配准、基于深度学习的配准；从输入图像的数据模态可以分为单模态\n配准、多模态配准；从输入图像维度可以分为二维图像配准、三维图像配准，以及跨\n维度配准；从组织特性可以分为刚性配准、可变形配准。\n1.2.1 传统医学图像配准方法\n近些年，图像配准已从临床医生手动完成过渡到由计算机辅助的自动配准方法，\n基于此，本文将以自动配准方法为主，不再赘述手动配准的过程。通过优化策略估计\n的参数对应于变形模型的自由度，在刚性转换情况下一般具有六个自由度，在仿射转\n换情况下一般具有十二个自由度，对齐图像的每个像素位置的变换都是用参数位移场\n加一个恒等变换的形式获得。但当考虑非参数密集变换（即可变形转换）时其数量将\n跃升至几百万，同时伴随着模型复杂性和迭代时间的大幅度增加，在临床应用中，传\n统基于优化的医学图像配准方法的选择通常是计算效率和配准精度的权衡。\n传统可变形图像配准方面已有大量的工作[15-17]，传统算法涉及三个主要组成部分，\n变形模型、目标能量函数，以及优化方法。一些研究优化了位移矢量场空间[11, 12, 16-18]，\nBajcsy 等人[11]提出了一种将三维解剖脑图谱与X 射线、CT 扫描数据进行匹配的配准\n方法，通过无撕裂折叠的弹性变形策略，在全局对齐基础上逐步细化提高局部相似性\n与整体一致性，实现了对不同个体脑形态差异的有效建模与配准。Glocker 等人[12]提\n出了一种无需代价函数导数的高效稠密图像配准方法，将配准问题建模为离散的马尔\n可夫随机场优化，通过少量控制点插值实现密集变形场，并结合多尺度增量策略与线\n性规划求解，在处理大形变与高分辨率图像配准中展现出优异性能。而一些工作则探\n"}, {"page": 11, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n3\n讨了具有保持拓扑结构的微分同胚配准方法[19-21]，Avants 等人[8]提出了一种新的对称\n图像归一化方法（SyN），基于最大化互相关实现保微分同胚变换优化。Vercauteren 等\n人[9]在Demons 算法的理论基础上提出了一种高效的非参数微分同胚图像配准算法，将\n其优化过程迁移至微分同胚变换空间，实现了更快收敛与更平滑、符合黄金标准的配\n准结果。Modat 等人[13]结合全局块匹配算法与经过GPU 优化和收敛改进的自由形变\n（FFD）局部配准方法，实现了高效且准确的肺部图像配准。Beg 等人[14]系统推导了\n大形变微分同胚度量映射（Large Deformation Diffeomorphic Metric Mapping，LDDMM）\n问题的Euler-Lagrange 方程，通过半拉格朗日方法实现粒子流追踪，求解图像间由微\n分同胚变换连接的最优速度场，并计算形变路径上的度量距离，用以捕捉多个解剖结\n构之间的大型形变。然而，传统的医学图像配准方法通常通过迭代优化每对图像的能\n量函数，独立估计两幅图像之间的密集非线性关系，迭代方法具有缓慢的收敛性，它\n们的性能受到图像强度的保真度的限制。即使一些研究将配准算法移植到GPU 运行，\n但对于给定的图像对，这些方法仍需要大量的时间和计算资源。\n1.2.2 基于深度学习的医学图像配准方法\n基于深度学习的方法通过从训练数据集中优化能量函数，学习图像配准的全局表\n示，对于未见过的医学图像，网络能够直接输出给定图像对的变形场函数及对齐图像。\n下面将根据训练模式的不同，划分为有监督模型和无监督模型两个类别详细论述。\n有监督模型[22-25]常通过经典方法获得与待配准图像对应的真实的标准变形场标签\n数据，并使模型经过训练后重现变形场。Rohé等人[23]基于一个全卷积神经网络（FCN）\n架构，通过引入了一种基于感兴趣区域（ROI）分割配准的训练策略，以合成参考变\n形场进行监督学习进而从配准图像中确定性地预测变换参数。Sokooti 等人[24]避免了传\n统方法中对预定义相似性度量的迭代优化过程，采用卷积神经网络结构设计了一种名\n为RegNet 的端到端非刚性图像配准框架，并使用人工合成的位移向量场（Displacement\nVector Field, DVF）数据集进行有监督训练，能够直接从输入图像对中预测DVF。但\n此类有监督模型的性能往往高度依赖于标签的准确性，而获得真实变形场的成本通常\n较高。\n最近，使用真实标签进行有监督的神经网络训练方法逐渐被不依赖于真实标签的\n无监督方法[26,29,30]所取代成为了研究的焦点。其大幅度减轻了相关人员进行数据标注\n"}, {"page": 12, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n4\n的工作量，并在运行速度和精度上优于传统配准方法[33-35]。无监督(或称“自监督”)\n模型的训练过程不依赖于真实标签，网络通过优化图像对之间的对比度差异进行训练,\n并由空间变换网络[36]扭曲移动图像。在一些情况下，无监督学习比有监督学习表现得\n更好。例如，以VoxelMopth[27]为代表的单流端到端配准方法，通过将移动图像和固定\n图像两个n 维图像拼接为一个输入图像，经过卷积神经网络输出移动图像到固定图像\n的空间映射，实现配准任务的高效预测。与传统逐对优化方法相比，VoxelMorph 显著\n提升了配准速度，并在无监督与弱监督两种训练策略下均取得了接近或优于现有方法\n的准确性。Kim 等人[28]基于生成对抗网络[37]（Generative adversarial nets, GAN）的思\n想提出了一种周期一致的可变形图像配准方法CycleMorph，以VoxelMorph 的模型作\n为配准网络骨干结合循环一致性通过提供隐式正则化以在变形期间优化拓扑保持性来\n增强图像配准性能。Chen 等人[32]提出了一种结合Transformer 与卷积网络的混合架构\nTransMorph，通过利用Swin Transformer[38]的大感受野优势有效改善卷积神经网络的有\n限感受野缺乏对图像中的远程空间关系建模的限制，增强配准中远程空间对应关系的\n建模能力。此外，一些研究提出使用不同分辨率下的多尺度医学图像配准框架[38]。\n综上所述，基于深度学习的医学图像配准大量研究证明深度学习可用于增强图像\n配准的性能，同时由于构建具有优质真实标签数据集的困难，研究人员对开发用于端\n到端的无监督框架的兴趣越来越高。但当前的方法仍存在一些问题，以标准卷积为网\n络骨干的方法在计算效率和局部特征提取方面具有显著优势，但传统卷积核（3×3×3）\n有限的感受野在处理配准图像对远程体素的空间关系的局限性限制了可变形配准的精\n度[39, 40]。以TransMorph 为代表的方法，通过将transformer 引入图像配准领域，借助\n自注意机制[41]的优势弥补了标准卷积在建立远程空间对应能力方面的不足，但其二次\n复杂度带来的高计算代价制约了临床部署可行性，无法满足医疗任务中低计算负载和\n快速推理的必要性，例如手术导航系统常用的嵌入式设备对算力的限制。同时，\nTransformer 结构复杂，其注意力权重分配的决策过程难以追踪，而高可解释性对于临\n床应用非常重要[42]。\n1.3 本文主要研究内容\n结合以上背景与研究现状，基于深度学习的无监督医学图像配准的主要问题是难\n以平衡高配准性能与低计算开销，限制了其在计算资源受限的实际医疗环境中的部署。\n"}, {"page": 13, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n5\n基于现有的医学图像配准方法无法精确捕获局部空间特征与长距离体素依赖关系的同\n时满足临床场景对计算效率和实时性的要求的问题，本文提出了轻量级混合Mamba-2\n的医学图像配准方法和小波引导的多尺度ConvNeXt 的医学图像配准方法。为了方便\n理解，本文以三维图像输入进行方法的描述，具体研究内容如下：\n(1) 基于Transformer 的模型依赖自注意力机制的全局感受野在一些图像配准任务\n展现出了优异的配准性能。然而，其自注意力机制的二次计算复杂度导致了巨大的计\n算开销。为了解决在资源受限的医疗环境中部署的挑战，进一步提高医学图像配准的\n效率和准确性，提出了用于医学图像配准的轻量级混合Mamba-2 架构HybridMorph。\n本文提出了一种残差混合模块（RHM），该模块基于卷积和Mamba-2 为医学图像配\n准任务重建了一个特征提取模块，采用结构化状态空间对偶性（SSD）框架来解决\nTransformer 的高计算成本问题，其全局感受野的设计和线性计算复杂度使其在准确理\n解移动图像和固定图像之间的非线性空间关系方面展现出显著的优势和效率。同时还\n提出了一种名为并行通道特征聚合器（PCFA）的新型轻量级方法，用于提取更丰富的\n特征。在本研究中还引入了三个不同参数数量的HybridMorph 版本，以适配不同计算\n资源场景。\n(2) 医学图像配准的实时推理在诸如图像引导诊断和手术导航等临床实践中至关\n重要。基于标准卷积的深度学习方法虽然能够满足低计算负载和计算效率，但受限于\n性能瓶颈。基于Transformer 的架构在非刚性配准任务中表现出了更高的准确性。然\n而，其较弱的空间局部先验条件需要大规模的训练数据集和大量的参数，这与临床工\n作流程中有限的标注数据和实时需求相冲突。此外，传统的下采样和上采样总是会降\n低诸如组织边界或小病灶等高频解剖特征的质量。为了解决这个问题，本文开发了\nWaveMorph，这是一种基于小波引导的多尺度ConvNeXt 方法，用于无监督医学图像\n配准。本文通过将ConvNeXt 架构与Haar 小波无损分解相结合，提出了一种新颖的\n多尺度小波特征融合下采样模块，该模块使用多尺度卷积核从八个频率子图像中提取\n和融合特征，以实现高效的下采样。此外，在解码器端引入了一个轻量级的动态上采\n样模块，以增强高频细节的重建效果。WaveMorph 将卷积神经网络（CNN）的归纳偏\n差与Transformer 的优势相结合，有效地缓解了由于空间信息丢失而导致的拓扑失真问\n题，并支持实时推理。\n"}, {"page": 14, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n6\n1.4 本文组织结构\n本文共分为五章，各章节内容安排如下：\n第一章：绪论。首先，首先阐述医学图像配准的研究背景与核心意义，说明其在\n临床诊断、治疗规划等领域的关键作用；继而梳理国内外研究现状，涵盖传统基于优\n化的方法（如基于位移矢量场空间优化、基于微分同胚优化的配准技术）及基于深度\n学习的前沿进展（包括有监督模型、无监督模型）；最后，说明本文的核心研究内容，\n包括当前方法存在问题及提出的创新方法，并介绍全文的章节组织结构。\n第二章：相关基础知识。首先，介绍了医学图像配准的概念、理论基础以及配准\n框架；其次，基于深度学习的无监督医学图像配准方法中所使用的相关深度学习模型，\n包括卷积神经网络、医学图像处理领域代表性的U-Net 网络模型、以及本文主要研究\n内容所涉及的相关技术，包括结构化状态空间对偶架构、ConvNeXt 网络结构、小波变\n换等。最后，介绍了医学图像配准任务中常用的空间变换网络、损失函数以及评价指\n标。\n第三章：轻量级混合Mamba-2 的无监督医学图像配准方法。本章提出了用于医学\n图像配准的轻量级混合Mamba-2 架构HybridMorph。首先，介绍了HybridMorph 可变\n形配准的总体框架；其次，介绍了HybridMorph 网络中的两个创新点：基于卷积和\nMamba-2 重构了适用于医学图像配准任务的残差混合模块和新型轻量化的方法并行通\n过特征聚合模块；最后，通过在多个脑部MRI 数据集上开展对比实验与消融分析，\n验证了HybridMorph 在保持线性复杂度的同时，有效捕捉了固定图像与浮动图像间的\n非线性变形关系，展现了状态空间模型在医学图像配准模型轻量化中的应用潜力。\n第四章：小波引导的多尺度ConvNeXt 的无监督医学图像配准方法。本章提出了\nWaveMorph，这是一种基于小波引导的多尺度ConvNeXt 方法，用于无监督医学图像\n配准。首先，介绍了WaveMorph 可变形配准的总体框架；其次，介绍了WaveMorph\n网络中的两个创新点：通过结合Haar 小波无损分解和ConvNeXt 架构的多尺度小波特\n征融合模块和轻量化动态上采样模块；最后，通过在多个脑部MRI 数据集上开展对比\n实验与消融分析，验证了WaveMorph 在配准精度、推理效率等方面的优越性。\n第五章：结论与展望。首先，对本文的研究内容进行了简要总结；其次，阐述了\n本文工作的主要创新点；最后，对未来的研究工作进行了展望。\n"}, {"page": 15, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n7\n第二章相关基础知识\n2.1 医学图像配准理论基础\n2.1.1 医学图像配准定义\n医学图像配准任务通常可以简单分为图谱配准、患者间配准，以及患者内配准（不\n同时期）。在一个配准图像对中，其中一个通常称为源图像或移动图像，而另一个称\n为目标图像或固定图像。在本文中，移动图像（Moving Image）使用�: Ω →�表示，\n固定图像（Fixed Image）使用�: Ω →�表示，其中Ω代表图像域，令∅: ��→��代表\n从移动图像到固定图像坐标映射的变形场。可变形医学图像的目标函数可以写成：\nsim\nreg\nˆ\nargmin ( ,\n)\nargmin\n( ,\n)\n( )\nf m\nf m\n\n\n\n\n\n\n\n\n\n\nL\nL\nL\n(2-1)\n其中，(�∘∅)是�通过变形场∅进行空间变换后的图像，ℒ���⋅, ⋅是图像间的相似度\n测量，ℒ�ℯℊ∙是正则化项，�是正则化项的平衡参数，且�> 0，用于在图像相似性和\n平滑正则化之间取得平衡，以保证高空间规则性水平上实现良好的配准。\n对于∅෡中，ℒ���⋅, ⋅的常见选择一般可分为单模态和多模态两类，对于对比度相\n近的单模态图像，常采用均方误差（MSE）[14]、归一化互相关（NCC）[8]，对于对比\n度差异过大或来自于不同成像设备的多模态图像，常采用互信息（MI）[43]、归一化互\n信息（NMI）[44]、模态独立邻域描述子（MIND）[45]等。正则化项ℒ�ℯℊ∙约束变形场∅\n的空间平滑性，以拟合图像间映射的拓扑结构并防止物理上不合理的折叠。常见的正\n则化选项包括平滑正则项（常用二阶平滑）[27]、弯曲能量正则项[46]、总变差正则项[47]。\n2.1.2 医学图像配准基础框架\n传统图像配准是一种基于迭代优化的程序，涉及提取必要的特征并确定相似性度\n量（以评估注册质量），选择转换模型和最后的搜索优化机制[48]。如图2-1 所示，在\n完整的传统图像配准流程中，一个配准图像对（移动图像和固定图像），将馈送到已\n根据对应配准任务完成配准参数定义的配准模型中；配准模型通过将根据固定图像的\n特征或强度扭曲移动图像，并根据预先定义的目标函数（即相似性度量）量化扭曲后\n"}, {"page": 16, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n8\n的移动图像与固定图像之间的相似度；搜索优化机制决定了迭代优化过程中最大化或\n最小化目标函数，并通过正则化变换施加特定于当前图像配准任务的约束，以解决与\n当前任务间可能存在的不适性的困难。转换模型由搜索优化机制不断迭代更新参数并\n置于移动图像上，以产生具有改善的对齐图像，如果满足配准任务的终止要求，则迭\n代过程将结束，否则，启动医学图像配准算法的新迭代。直到无法获得更好的配准或\n满足某些预定要求之前，移动图像将在每个周期中提高其与固定图像的空间对应关系\n[49]。\n图2-1 传统医学图像配准框架流程图\nFigure 2-1 Traditional image registration framework flowchart for the medical images\n在基于深度学习的医学图像配准方法中，有监督训练是各类配准模型的基础，其\n根据模型训练过程中的监督程度可分为完全监督和弱监督方法。如图2-2 所示，展示\n了完全监督和弱监督方法的工作示意图，对于完全监督的方法常利用由传统医学图像\n配准方法生成的真实位移向量场来显式监督神经网络的训练过程，对于弱监督的方法\n通常不使用显式监督，而是使用解剖结构轮廓标签隐式监督神经网络的训练过程。弱\n监督方法中使用的解剖结构标签的获取成本远低于完全监督方法中使用的真实位移向\n量场。而在损失函数的选取上常使用基于强度的自动配准方法中的图像相似性度量函\n"}, {"page": 17, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n9\n数的扩展。\n图2-2 基于深度学习的有监督和弱监督医学图像配准框架示意图\nFigure 2-2 Schematic diagram of a supervised and weakly supervised registration framework for\nthe medical images based on deep learning\n尽管基于深度学习的弱监督方法相对完全监督方法已一定程度上降低了真实标签\n的创建和获取的成本，但仍是一个耗时耗力的过程。由于在医学图像配准任务中移动\n图像和固定图像间线性或非线性的空间映射是深度学习模型需要学习的全部内容，因\n此基于无监督（或自监督）的医学图像配准成为了解决监督方法标签或数据稀缺问题\n的方法。如图2-3 所示，展示了无监督方法的完整流程，移动图像和固定图像被馈送\n入对应的人工神经网络模型中，并由网络通过提取配准图像对间的空间映射关系预测\n一个包含了每个像素或体素位移量的非线性变形场（位移场），移动图像根据变形场\n的位移参数由空间变换网络[36]扭曲为配准后的变形图像。随后，网络通过配准后图像\n与固定图像最小化损失函数中的显式相似性约束项，并引入隐性的平滑性约束项防止\n不符合解剖学结构的形变，并通过梯度下降等方式反向传播更新网络模型的参数来优\n化预测的变形场。在经过多轮迭代优化后，网络模型将学习到最佳的通用配准参数，\n进而达到快速将未见过配准图像对中的移动图像扭曲为与固定图像解剖对齐的状态，\n且整个训练和推理过程无需依赖人工标注的真实位移向量场或解剖分割标签。\n"}, {"page": 18, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n10\n图2-3 基于深度学习的无监督医学图像配准框架示意图\nFigure 2-3 Schematic diagram of the unsupervised registration framework for the medical images\nbased on deep learning\n2.2 深度学习网络模型\n2.2.1 卷积神经网络\n卷积神经网络（Convolutional Neural Network, CNN）是一种专门用来处理具有类\n似网络结构的数据的神经网络，例如图像数据[50]。完整的CNN 结构一般由输入层、\n卷积层、池化层、全连接层以及输出层构成[51]。卷积是一种特殊的线性运算，在一个\n卷积神经网络架构中至少一层使用卷积运算来代替矩阵乘法运算[50]。CNN 最早可以追\n溯到1980 年Fukushima 等人[52]提出的神经认知机，但直到2012 年的ImageNet 竞赛中\n由Krizhevsky 等人[53]提出的AlexNet 取得突破性成果，CNN 才重新回到研究者的视\n野并重塑了人工智能和计算机视觉的发展轨迹，为深度学习革命奠定了基础。\nCNN 的输入层接收原始数据（如图像的像素矩阵），其维度由数据类型决定，例\n如三维医学图像是仅有灰度的高度×宽度×深度的三维张量。卷积层是CNN 的核心，\n通过滑动可学习的卷积核（或滤波器）在输入数据上进行局部运算，此时卷积核的权\n重参数是固定不变的，同一个卷积核在对输入数据的不同位置进行卷积运算时，使用\n的是相同的一组权重，即参数共享。卷积神经网络的有效感受野受限于卷积核的大小，\n因此浅层卷积层仅连接输入图像的局部区域（如3×3 卷积核）适合提取边缘、纹理\n等低级特征，通过多层卷积的叠加结合池化操作，有效感受野呈指数级扩大，深层的\n卷积层提取整体轮廓、局部组合等全局结构甚至抽象概念等高级特征。\n池化层紧随卷积层之后，主要功能是降维和特征筛选，常见的池化操作是最大池\n化（Max Pooling）和平均池化（Average Pooling）[54]。最大池化通过在特征图的局部\n"}, {"page": 19, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n11\n区域选择最大的像素值，保留关键信息同时减少空间维度（或称图像分辨率），这有\n助于缓解过拟合并增强对平移的鲁棒性。平均池化通过在特征图的局部区域选择所有\n像素的平均值来减少空间维度，其相对于最大池化对图像的背景信息更加敏感，更适\n用于网络获取全局上下文继而做出相应决策。此外，池化层通过降低特征图的分辨率\n大幅提高卷积有效感受野的同时可以使其获得具有空间不变性的特征，并且通过减少\n参数数量降低了后续计算的复杂度。\n卷积层与池化层的交替堆叠构成CNN 的特征提取模块，而全连接层则位于网络\n末端，负责将高层特征映射到最终输出（如分类标签）。在全连接层中通过将前一层\n的特征图展平为一维向量，并与所有神经元建立连接，因此被称为全连接层。在前向\n传播中，由于卷积和全连接操作本质上是线性变换，因此如果仅为线性模型其对复杂\n数据的表达能力将严重受限，研究人员通过引入激活函数进行非线性变换，让网络能\n过学习逼近任意非线性的复杂函数，并使网络具备区分图像中不同形状、边缘、纹理\n等特征的能力。CNN 的训练依赖于反向传播算法，通过梯度下降等算法最小化损失函\n数调整卷积核、偏置和全连接层的权重。\n2.2.2 U-Net 网络模型\nU-Net 网络是一种专门为图像分割任务设计的深度学习框架，名称源于网络结构\n呈现对称的U 型结构[55]。网络的核心架构有编码器、瓶颈层、解码器以及跳跃连接四\n个部分组成，因其结构对称、优越的端到端训练效果等特点，在医学图像处理领域广\n受欢迎，已成为众多医学图像配准、医学图像分割等模型的基础框架。\n编码器部分通过连续的卷积和池化操作逐步降低特征图的空间分辨率，同时不断\n增加通道数，从而逐层提取更抽象的语义特征。卷积核为3×3 的标准卷积层、批归一\n化层[56]、ReLU 激活层[57]被整合为一个特征提取模块，编码器的每一层包含两个上述\n的特征提取模块，随后进行步长为2 的最大池化下采样操作，是特征图空间尺寸减半\n而特征维度加倍。瓶颈层位于网络的最底层，此时特征图具有最小的空间尺寸和最大\n的通道数也代表其具有最丰富的全局语义信息，瓶颈层负责捕获输入图像的全局上下\n文信息，是连接编码器和解码器的关键枢纽。\n解码器部分的目标是将编码器提取的高层语义特征与底层细节特征结合，通过上\n采样操作（如转置卷积或最近邻插值）逐步恢复空间分辨率，最终生成与输入图像尺\n"}, {"page": 20, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n12\n寸一致的分割掩码。具体而言，每个解码器层通过上采样操作将输入的特征图的空间\n尺寸翻倍、通道数减半，然后与解码器对应层级的特征图在通道维度进行拼接（即跳\n跃连接）形成具有语义信息和空间细节的融合特征，随后由同编码器一致的特征提取\n模块进一步细化特征，逐步恢复至原始图像分辨率。\n在U-Net 中，跳跃连接是其核心创新点，巧妙地解决了传统分割方法中语义信息\n与空间细节难以兼顾的难题，使得网络在分割边界和小目标上具有更好的感知能力。\n编码器的下采样操作会丢失浅层空间位置信息、边缘纹理等细节，而解码器的上采样\n操作仅依靠深层语义特征容易导致分割边界模糊、伪影等细节缺失，跳跃连接的创新\n将编码器中对应层级的浅层特征直接传递至解码器的对称层级，使编码器能够提取包\n含全局图像内容理解的高层特征，同时保留边缘和局部结构的精确位置。这种跨层级\n的多尺度特征融合尤其适用于医学图像中常见的小样本问题，确保模型在复杂背景中\n准确区分目标区域，一定程度上缓解因下采样导致的信息损耗，最终实现更精细、完\n整的分割结果。\n图2-4 U-Net 网络结构[55]\nFigure 2-4 U-Net network struture[55]\n2.2.3 ConvNeXt 网络模型\nConvNeXt 是Liu 等人[58]提出的一种纯卷积神经网络架构，旨在将Transformer[41]\n的先进设计理念融入卷积神经网络，从而在保持卷积神经网络低参数量和强归纳偏置\n"}, {"page": 21, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n13\n的优势的同时提升其性能。ConvNeXt 针对网络的宏观结构和基本模块在ResNet[59]的\n基础上进行了系统性的现代化改进；具体而言，宏观结构上保留了多阶段设计，每个\n阶段包含了多层堆叠的卷积特征提取模块，并逐步降低特征图的空间分辨率、增加通\n道数；基本模块上采用更大的卷积核（7×7 卷积核）代替传统的小卷积核（3×3 卷积\n核）扩大卷积有效感受野以更好地捕捉长距离依赖特征；同时在卷积特征提取块中使\n用GELU[60] 代替传统的ReLU 激活函数增强非线性表达能力，并参考Vision\nTransformer[61]减少了激活函数的数量，并引入了层归一化（Layer Normalization, LN）\n代替批归一化（Batch Normalization, BN），从而提升了模型的训练稳定性和泛化能力。\nConvNeXt 在每个基本的特征提取单元中使用深度可分离卷积[62]代替传统的标准\n卷积，即使用深度卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）\n进行高效的特征提取，并参考Transformer 中的MLP 模块和MobileNetV2[63]的设计思\n想引入了逆瓶颈结构，使纯卷积网络在保持局部感受野优势的同时，也能获得更强的\n建模能力。目前，ConvNeXt 已被广泛应用于图像分割、视频理解等任务，在处理高分\n辨率图像（例如三维医学图像）时，纯卷积的局部计算特性能够带来更高的计算效率。\n图2-5 ConvNeXt 网络结构\nFigure 2-5 ConvNeXt network struture\n2.2.4 结构化状态空间对偶\n结构化状态空间模型（Structured State Space Sequence Models, SSMs）[64, 65]是最近\n一类与传统的状态空间模型（State Space Models）[66]高度相关的用于深度学习的系列\n模型，可以被看做递归神经网络（Recurrent Neural Network, RNN）[67]和卷积神经网络\n（CNN）的结合，并具有线性或者接近线性的复杂度。通过将参数归一化为对角结构，\n"}, {"page": 22, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n14\nSSMs 有效解决了计算和内存需求高的问题，展示了处理长距离信息建模问题的潜力。\n具体来说，SSMs 模型由四个参数（�, �, �, �）定义，这些参数决定了序列到序列的转\n换：\n\n\n\nh t\nAh t\nBx t\n\n\n\n(2-2)\n\n\ny t\nCh t\n\n(2-3)\n\n\n,\n,\n,\n,\nk\nK\nCB CAB\nCA B\n\n\n\n(2-4)\ny\nx\nK\n\n\n(2-5)\n通过如公式2-6 定义的零阶保持（zero-order hold, ZOH）将连续参数�, �, �, �转换\n为离散参数�, �后，可以进行有效的可并行计算。但SSMs 通常比等效的CNN 需要\n更多的内存，这使得其在序列建模中的进一步应用受到了阻碍。\n\n\n1\nexp(\n)\n(\n)\nexp(\n)\nA\nA\nB\nA\nA\nI\nB\n\n\n\n\n\n\n\n(2-6)\n最近，Mamba[68]在SSMs 的基础上引入选择性机制构建了选择性状态空间模型\n（Selective Structured State Space Sequence Models with a Scan, S6），它允许从输入中\n进行有效的过滤。同时，开发了一种硬件感知算法，通过扫描递归的计算模型，使其\n在硬件上比过往的方法更快。Mamba 在序列长度线性放缩和显著的提高计算效率的同\n时，实现了Transformer 的建模能力。\nMamba 对SSMs 的改进使其在信息密集型数据上的表现优于传统的线性时不变的\nSSMs，但由于缺乏GPU 等现代硬件专用的矩阵乘法单元，S6 虽然开发了专门的硬件\n感知算法以提升计算效率，但其仍只能在递归模式下计算，而不是卷积模式，因此无\n法利用GPU 等现代硬件专用的矩阵乘法单元，效率仍然低于CNN 和Transformer 等硬\n件友好模型。Mamba-2[69]在S6 的基础上揭示了选择性SSM 和注意力机制之间的更深\n层次关系并，基于半可分矩阵的块分解和二次对偶形式，构建了链接结构化SSMs 和\n注意力机制的框架，称为结构化状态空间对偶（Structured State Space Duality, SSD）。\nMamba-2 允许使用矩阵乘法单元，并通过SSD 来显著提高SSMs 的训练速度，同时支\n持更大的循环状态大小。SSD 的对偶形式可以看做为选择性SSMs 的特例，并且是与\n注意力机制[41]密切相关，其二次计算定义为：\n1\nif \n(\n)\n0\nif \ni\nj\nij\na\na\ni\nj\nL QK\nV\nL\ni\nj\n\n\n\n\n\n\n\n\n\n\n\n，\nF\n(2-7)\n"}, {"page": 23, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n15\n其中，��是有界于[0, 1]上的输入相关标量。SSD 与标准的softmax 注意力相比去\n掉了softmax，同时注意力矩阵按元素乘以一个额外的1-半可分（1-semiseparable）下\n三角掩码矩阵�，因此可以看做是自回归注意力的一种变种。在SSD 算法中使用高效\n的半可分矩阵进行结构化矩阵乘法来代替S6 的递归计算，同时掩码矩阵�可以被视为\n用不同的数据依赖位置掩码替换Transformer 的启发式位置嵌入，从而控制跨时间传递\n的信息流并结合结构化矩阵来加速计算，其半可分矩阵的块分解通过低秩且分块的方\n法比纯线性或二次形式注意力获得更好的效率权衡。\n2.2.5 小波变换\n离散小波变换（Discrete Wavelet Transformation, DWT）[41]作为一种强大的信号处\n理工具，已被广泛应用于图像的多尺度分析。最近，一些研究将DWT 引入神经网络\n架构中，来改善图像分类、超分辨率、去噪等各种任务中的特征表示能力。Xu G 等人\n[71]提出一种简易的小波下采样，应用Haar 小波变换来降低特征图的空间分辨率，同时\n尽可能多地保留信息。Fujieda 等人[72]提出了通过小波变换补充多分辨率分析的损失部\n分，并将它们作为整个架构中的附加组件。如Luo 等人[73]所述，借助一种新的小波合\n成网络架构，能够快速形成高分辨率的视差图。然而，小波理论在基于深度学习的图\n像配准中的应用尚不广泛。这主要是因为图像在经过小波变换后会分解为多个频段的\n子带图像可能会增加配准过程的复杂性，例如，三维医学图像经过小波分解将得到八\n个包含不同空间域和频域信息的子带图，如何高效整合低频（全局结构）与高频（边\n界、细节）子带信息，以指导配准过程中的特征提取与空间变换，仍是开放性问题。\n同时，目前由CNN 或Transformer 构成的模型在处理图像时以空间域为主，如何更好\n的将小波变换的频域特征与空间特征相融合仍有待研究。\n目前主流的基于深度学习的医学图像配准方法（如VoxelMopth、TransMorph），\n更关注于网络主干或模型架构上的创新，通常采用标准的下采样操作（例如，最大池\n化[74]、跨步卷积[75]），虽计算复杂度较低，但忽略了下采样过程中局部邻域特征聚集\n导致高频信息（如组织边界、微小解剖结构）不可逆地丢失对模型性能的影响，最重\n要的是模型输入的失真可能导致最终扭曲图像上拓扑折叠（Topological Folding）等失\n真现象。\n"}, {"page": 24, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n16\n2.3 损失函数与评价指标\n2.3.1 损失函数\n网络训练的损失函数依旧遵循传统方法的优化函数主要由两部分组成，计算输入\n体积与配准域间的图像相似性度量，以及规范变形场空间平滑性的正则化项。\n图像相似性度量，在实验中，本文应用两个广泛使用的相似性度量用于无监督训\n练模型的评估，第一个是均方误差，计算体素差异的平方和的平均值，适用于移动图\n像和固定图像的对比度、强度分布相近的情况，其中��是体素位置，Ω是图像域：\n\n\n2\nwarpe\n1\n(\n)\n(\n)\nfixed\ni\nd\ni\ni\nMSE\nI\np\nI\np\n\n\n\n\n(2-8)\n第二个是局部归一化互相关，计算图像体积间局部窗口相似度，对强度和对比度\n更具鲁棒性：\n\n\n\n\n2\n2\n( (\n)\n) ([\n](\n)\n)\n,\n( (\n)\n)\n([\n](\n)\n)\ni\ni\ni\ni\nf\ni\nm\np\ni\nf\ni\nm\np\np\nf p\nm\np\nNCC f\nm\nf p\nm\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(2-9)\n其中μ�和μ�∘φ表示以体素�为中心、大小为�3的局部窗口内的平均体素值。本文在\n实验中使用�= 9。\n2.3.2 评价指标\n本研究采用多维度量化指标系统评估配准性能。具体来说，首先基于解剖学分割\n的一致性，本文计算30 个（IXI 数据集）/35 个（OASIS 数据集）脑区结构的Dice 相\n似系数（Dice Similarity Coefficient, DSC）[76]，其定义如下：\n2|\n|\nDSC\n|\n|\n|\n|\nX\nY\nX\nY\n\n\n\n(2-10)\n其中�和�分别表示配准后移动图像与固定图像的二值化分割掩膜。针对配准图像\n对，首先计算各解剖结构的个体DSC 值，随后通过层次化统计方法获得组水平均值和\n标准差。\n其次，为评估变形场的生物力学合理性，引入雅可比行列式拓扑分析指标：对形\n变场进行微分运算，计算其雅可比矩阵的行列式值分布。特别地，定义非正值体素占\n比（Folding Ratio, FR）来量化形变场的规律性。\n"}, {"page": 25, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n17\n1\nFR\n(|\n( ) | 0) 100%\nv V\nJ v\nN\n\n\n\n\n\n\n(2-11)\n其中�表示整个三维脑区体素集合，�为总体素数，�… 为指示函数。该指标量化\n变形场的局部不可逆折叠现象，值越低表明拓扑保持性越优。\n2.4 空间变换网络\n本文通过使�∘∅得到的配准后图像与固定图像之间的差异最小化来得到最优参\n数值。本文使用空间变换网络[36]来计算�∘∅以获得配准后图像。对于医学图像可以被\n定义为在每个体素的八个临近方位的体素值进行线性插值：\n{ , , }\n(\n)\n( )\n( )\n(1 |\n|)\nd\nd\nd\nx y z\nm\nm\n\n\n\n\n\n\n\n\n\n\n\nq\np\np\nq\np\nq\nZ\n(2-12)\n其中�' = �+ ��，对于每个体素�，��是由网络计算出的位移，��' 是�'八个\n临近方位的体素。\n2.5 本章小结\n本章主要介绍了医学图像配准的相关理论基础。首先，介绍了医学图像配准的理\n论，包括医学图像配准的定义及不同模态下常用的相似性度量和正则化项；其次，介\n绍了传统医学图像配准方法和基于深度学习的医学图像配准方法的框架；随后，介绍\n了与本文研究相关的深度学习模型，如卷积神经网络、经典的U-Net 网络架构、\nConvNeXt 网络结构、结构化状态空间对偶以及小波变换；最后，介绍了本文在基于深\n度学习的无监督医学图像配准任务中所使用的空间变换网络、损失函数以及评价指标。\n"}, {"page": 26, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n18\n第三章轻量级混合Mamba-2 的无监督医学图像配准方法\n3.1 引言\n目前，许多用于医学图像配准的深度神经网络架构基于卷积神经网络（CNNs），\n即使采用具有能够共享权重的跳跃连接的U 型网络结构[55]，由于卷积操作的有效感受\n野受限于卷积核大小，CNN 依然更擅长捕获局部特征，而难以建模长距离信息的相关\n性。为了赋予模型理解全局信息的能力，一些研究深入拓展了Transformer 在各领域的\n应用，例如目前各项视觉任务的主干网络Vision Transformer[61]和Swin Transformer[38]。\n鉴于CNN 和Transformer 的互补特性，诸多研究探索通过将Transformer[41]与CNN 结\n合，形成混合网络架构，例如TransUnet[77]、SwinUNETR[78]。Transformer 中的自注意\n力机制通过将图像视为连续的块序列能够更好的捕获图像长距离信息的相关性，但该\n计算机制具有与图像大小相关的二次计算复杂度导致大量计算开销，尤其对于三维图\n像这种信息密集型任务（例如医学图像配准）。这忽视了实际医疗环境中的计算约束，\n无法满足医疗任务中低参数和低计算负载的必要性。近期，基于结构化状态空间模型\n（SSMs）[64, 65]被证明在序列建模中存在潜力，Mamba[68]通过硬件感知算法结合参数化\nSSM 选择机制，在多个领域取得了超越Transformer 模型的性能，且保持了线性复杂\n度。近期，一些研究将SSMs 模型拓展到计算机视觉领域，并取得了令人欣喜的结果。\nMamba-2[69]在选择性状态空间模型的基础上引入状态空间对偶性(SSD)框架，解决了\nMamba 无法进行张量并行性和序列并行性训练和推理的问题，并允许更大的循环状态\n大小，相较于Transformer 在处理长距离信息相关性方面具有显著优势和效率。\n基于这些事实，本文开发了一种新颖的医学图像配准混合架构HybridMorph，该\n架构能够有效捕获医学图像体积中的局部空间特征和长距离体素依赖关系，并保持与\n输入大小相关的线性计算复杂度，展示了SSD 框架在医学图像配准任务中促进轻量化\n方面的潜力。在HybridMorph 通过创新的轻量化混合架构，在降低参数和计算成本的\n同时，实现了在多个数据集上最先进的性能。这项工作的主要贡献如下：\n1.本文探索了SSD 架构在医学图像配准的潜在应用。提出了一种基于卷积和SSD\n"}, {"page": 27, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n19\n架构融合的新型轻量化神经网络HybridMorph，能够更好的平衡计算复杂度与性能，\n适用于无监督可变形医学图像配准。本文提供了三种不同参数量的HybridMorph 版本，\n适配不同计算资源场景。\n2.本文提出了一个残差混合模块（Residual Hybrid module, RHM），RHM 基于卷\n积和Mamba-2 重构了适用于医学图像配准任务的特征提取模块。RHM 在引入少量参\n数和计算开销，利用残差连接和调整因子，进一步促进了卷积和Mamba-2 在图像中对\n局部信息和全局信息的整合特征提取能力。\n3.为解决实际医疗环境中计算资源限制带来的挑战，本文提出了一种新型轻量化\n的方法并行通道特征聚合模块（Parallel Channel Feature Aggregator, PCFA）。使用PCFA\n块的HybridMorph 相比于TransMorph 减少了4.1 倍的计算量，7.3 倍的参数量。PCFA\n能够在较低计算开销下，提取更加丰富的特征表达。同时PCFA 模块可以轻松集成于\n其他网络结构中，为轻量化优化策略提供了更多的选择。\n4.本文在患者间配准和图谱到患者脑MRI 配准数据集(包含1000 多个用于测试和\n训练的脑MRI 图像)上广泛验证了所提出的配准模型。与各种基准配准方法比较，所\n提出的模型展示了最先进的性能。\n3.2 轻量级混合Mamba-2 的无监督医学图像配准方法\n3.2.1 可变形配准框架\n图3-1 概述了所提出的无监督图像配准方法，移动图像�与固定图像�作为输入馈\n送至HybridMorph 网络，并输出变形场∅。通过空间变换函数将移动图像�扭曲为配准\n后图像�∘∅，然后评估�和�∘∅的相似性。图3-2 显示了所提出的HybridMorph 的网\n络结构。尽管本文的方法实现与维度无关，但为了方便理解，下文以三维医学图像输\n入进行方法的描述。本文假设�, �在数据预处理中是已完成放射对齐，因此网络将更\n关注于体素间的非线性空间对应。\nHybridMorph 遵循编码器-解码器网络结构，给定输入图像�, �: Ω →��×�×�×�，\n其中�、�、�和�分别代表三维医学图像的通道（单通道灰度数据）、高度、宽度和\n切片数。如图3-2 所示，在编码器阶段，首先将配准图像对�, �拼接为单输入，采用\n两层深度可分离卷积（DWConv）通过共享特征形成隐式强度分布差异(非线性映射关\n系)感知，生成浅层特征图��∈���×�\n�×�\n�×�\n�。随后，HybridMorph 集成了四个连续的并\n"}, {"page": 28, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n20\n行通道特征聚合模块(Parallel Channel Feature Aggregator Block, PCFA)，其核心的混合\n残差模块（Residual Hybrid module, RHM）以有效的方式整合局部特征和全局特征。\n经过每个PCFA 块，特征图的通道数加倍而分辨率减半，因此，编码器末级输出深层\n特征图尺寸为��∈���×��× �\n��×�\n��× �\n��。解码器由连续的上采样层和残差卷积块组成，如\n图3-2 灰色块所示，残差卷积块由深度可分离卷积和逐点卷积组成并引入两级残差连\n接，其中第一级残差连接中引入权重来调整不同路径信息的贡献比例，第二级的深度\n可分离卷积将输出维度拓展了3 倍并由逐点卷积压缩至输出维度。残差卷积块有效地\n平衡了计算效率和特征表达能力，在实现参数量和计算量优化的同时增强了特征提取\n的深度和丰富度。在解码器阶段，每个上采样的特征图通过跳跃连接与来自编码器对\n应特征图级联，然后经过残差卷积块（Residual Convolution module, ResConv）最终传\n播到生成变形场∅的层。\n如图3-2 所示，为了减少参数量和计算量，HybridMorph 使用深度卷积提取表层\n特征（由深蓝色块表示），因此，可能捕捉复杂特征、跨通道信息以及空间依赖性的\n能力较弱。为了解决这一缺点，在浅层特征图的跳跃连接中，受三向注意力[79]的启发，\n本文将其拓展到适用于三维医学图像，引入了几乎无参数的四向注意力模块，以捕捉\n维度交互，细化基础空间信息。最后，使用空间变换函数将网络生成的变形场∅应用\n于移动图像�，从而实现精确的解剖对齐。\n图3-1 基于混合Conv.- SSD 的图像配准模型的总体框架\nFigure 3-1 The overall framework of Hybrid Conv.-SSD based image registration model\n"}, {"page": 29, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n21\n图3-2 HybridMorph 配准网络架构图\nFigure 3-2 The architecture of HybridMorph registration network\n3.2.2 残差混合模块\n三维医学图像的体素数据分布于三维空间，无固定处理顺序，邻域关系由空间几\n何决定，模型需捕捉局部与全局空间信息的非线性关系。卷积神经网络（CNN）可以\n有效提取局部几何特征，如边缘、曲率和纹理，使其在处理局部非刚性变形时表现出\n色，但卷积核有限感受野限制了对全局形变的建模能力，难以直接捕捉长距离空间信\n息依赖关系或复杂的全局几何变形。为了解决这个问题，本文提出的RHM 将CNN 与\n基于SSD 的Mamba-2 结构结合，以提升全局感知力和对非规则结构的适应性，同时\n避免了Transformer 在密集预测任务上会导致计算量呈二次方增长的问题，从而更好地\n满足非刚性配准的需求。\n如图3-3 所示，首先，使用1×1 卷积将输入在通道维度翻倍，随后在通道维度将\n输入分割成两个大小相同的子输入，分别馈送到两个分支模块。在卷积分支中，本文\n使用一个深度卷积和两个逐点卷积的逆瓶颈层的结构，并引入了全局响应归一化\n（Global Response Normalization, GRN）[80]以解决卷积分支可能存在的特征崩溃问题并\n增强通道间的特征竞争。在Triplet Mamba-2 分支(下面将具体描述)，本文采用了三向\nMamba 块用于模拟不同尺度的全局信息，分支输入在经过Layernorm[81]层后，沿着三\n个不同的遍历方式将图像展开成序列，使用单独的Mamba-2 块并行处理每个序列，随\n"}, {"page": 30, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n22\n后在残差连接中利用调整因子增强残差学习的灵活性。随后，triplet Mamba-2 分支使\n用Layernorm 标准化特征，使用线性层交叉融合结果序列，并通过重塑和简单残差连\n接形成分支特征图。一个完整的RHM 可以被表示为:\n,\n(\n)\n(\n)\nc\nm\nin\nF F\nSplit Conv F\n\n(3-1)\n2(\n(\n))\n(\n)\ncF\nGRN Conv\nDWConv Fc\n\n(3-2)\na\n(Mamba-2(\n(\n))\n))\n(\nm\nm\nm\nm\nF\nLine r LN\nLN F\ns F\nF\n\n\n\n(3-3)\nout\nc\nm\nin\nF\nF\nF\nF\n\n\n\n\n\n\n\n\n\n(3-4)\n其中��, ��\n' 分别表示卷积分支的输入和输出特征，��, ��\n' 分别表示Triplet Mamba-2\n分支的输入和输出特征。对于两个分支输出的特征图在末端的特征融合，我们引入可\n学习参数γ, δ来改进普通的特征加权融合并给予更灵活的权重调整范围，在训练过程中，\n模型自动调整局部信息和全局信息的重要性差异。\nTriplet Mamba-2:与序列数据不同，三维医学图像具有空间位置的非顺序性、旋转\n和平移不变性、非欧几里得特性等一系列视觉特性，体素之间的关系并不遵循明确的\n因果顺序，且存在多维度的空间依赖性。如公式2-7 所示，SSD 可以视作因果（自回\n归）注意力的特例，因此其具有因果注意力的部分特性，即在处理序列数据时仅关注\n其之前的位置元素，无法关注后续位置的元素，这使得适用于具有顺序性和因果关系\n的自然语言序列的单一扫描过程很难同时捕获不同方向的依赖信息。这使得适用于因\n果性的自然语言序列的单一扫描过程很难同时捕获不同方向的依赖信息。为此，本文\n构建了Triplet Mamba-2 模块，引入了高度、宽度、深度三个方向的扫描机制，从而更\n好地适应三维医学图像的非因果性。此策略可确保每个体素整合来自不同方向的空间\n信息，并利用残差连接和调整因子在训练阶段自适应调整权重分配，在几乎不引入新\n参数和计算复杂度的情况下进一步增强长距离空间信息相关性的建模能力。\n"}, {"page": 31, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n23\n图3-3 残差混合模块架构示意图\nFigure 3-3 The architecture of Residual Hybrid Module\n3.2.3 并行通道特征聚合模块\n在基于SSD 的Mamba-2 模型中，本研究观察到随着通道数的增加，模型的参数\n量和计算复杂度呈爆炸性增长，这对模型的效率和实际应用带来了挑战。为了解决这\n一问题，本文受到分组卷积[82]思想的启发，提出了一种新型轻量化的方法，命名为并\n行通道特征聚合模块（Parallel Channel Feature Aggregator, PCFA），如图3-4 所示。该\n方法通过将特征图沿通道维度进行分组处理，在保持通道数不变的情况下，能够显著\n减少计算量和参数量，同时有效保留关键特征。相比未使用轻量化方法，PCFA 能够\n在较低计算开销下，提取更加丰富的特征表达。\n单纯在通道维度进行分组处理虽然能够减少计算复杂度，但会导致通道维度特征\n共享的减少继而导致模型的表达能力下降，尤其是在处理复杂任务时。PCFA 在单独\n的分支中应用RHM 核心层来提取融合特征，引入Crossnorm [83]归一化，该层仅在训\n练阶段通过增强特征的标准化过程结合统计数据，以丰富模型对图像中结构性信息的\n学习。\n随后，PCFA 采用了逆瓶颈结构进一步恢复了增强特征的通道维度提取细化特征，\n并压缩维度去除不必要的特征信息，从而减少冗余计算量。在多分支输出特征馈送入\n"}, {"page": 32, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n24\nSelfnorm[83]归一化层使用对多分支统计数据进行重新校准，确保模型仅关注关键信息，\n减少了标准化特征和统计信息混合的多样性。\n此外，为了加强不同组间通道信息的融合并减少通道间的依赖性，PCFA 采用了\n通道混洗（Channel Shuffle）[84]操作和Layer Scale[58]层。通过轻量级的操作完成了跨\n组通道信息的有效交换，并为通道维度添加可学习的缩放因子，促进了通道间的信息\n竞争，保证了各个分支间信息的充分交流及自适应调整，提升训练的稳定性和模型的\n泛化能力。具体为：\n/\n/\n/\nin\n1\n2\n{\n,\n,\n,\n}\n \nC\nC G\nC G\nC G\nG\nF\nF\nF\nF\n\n\n(3-5)\n/\n2\n/\n1 1\nConv\nCN(RHM(\n))\n(\n)\nC G\nC G\ni\ni\nF\nF\n\n\n\n(3-6)\n/\nm\n1\nSN Cat{\n}\n(\n)\nC\nC G\nG\ni\ni\nF\nF\n\n\n\n(3-7)\nout\nm\nin\nL\n \nS CS(\n)\n(\n)\nC\nC\nC\nF\nF\nF\n\n\n(3-8)\n其中，�in\n�是输入特征图，{�1\n�/�, �2\n�/�, …, ��\n�/�}表示将输入特征图按通道维度分成�\n组。��\n'�/�是通过RHM层及CrossNorm层的增强特征在经过逆瓶颈操作后的特征；LS 表\n示Layer Scale 层；CS 表示Channel Shuffle 层；�out\n�是最终输出的特征图。\n图3-4 并行通道特征聚合模块架构示意图\nFigure 3-4 The architecture of Parallel Channel Feature Aggregator block\n"}, {"page": 33, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n25\n3.3 实验结果与分析\n3.3.1 数据集与处理\n为了彻底验证所提出的方法，本文使用了包含1000 多个脑MRI 图像的两个数据\n集。首先，本文使用公开的图像信息提取（IXI）数据集，评估所提出的模型与图谱到\n患者脑部MRI 配准任务中的表现。(IXI)数据集包含来自正常健康受试者的600 张\nMRI 脑部图像的集合，其中本文选取了576 张T1 加权脑MRI 图像作为固定图像，并\n使用从TransMorph[32]获得的图谱脑MRI 作为移动图像。数据集按403，58，115（7:1:2）\n作为训练，验证和测试集。\n对于患者间脑部MRI 图像配准数据集，本文使用了来自开放获取影像研究系列\n（Open Access Series of Imaging Studies, OASIS）的414 张T1 加权脑部MRI 图像。由\n于测试集未公开，本文使用一个验证集作为测试集，数据集分为394 个用于训练，20\n个用于验证，这与TransMorph 使用的划分相同。每个图像体积被用做移动图像，与训\n练集中的其他图像体积（即固定图像）随机匹配形成一个图像对，然后将移动图像和\n固定图像反转以形成另外一个图像对，从而形成两个配准对。\n所有数据集的预处理遵循VoxelMorph 相同的标准步骤，包括使用Freesurfer[85]进\n行重采样、仿射空间归一化、头骨剥离，并将图像体积裁剪到160192224 的大小。\n本文使用Freesurfer 对测试数据进行解剖分割，对IXI 数据集使用30 个解剖结构的标\n签图，对OASIS 数据集使用35 个解剖结构的分割图，这些分割图仅用于评估配准性\n能。\n3.3.2 实验设置\n本文将HybridMorph 与之前表现出最先进配准性能的各种配准方法进行了比较，\n这包括了基于传统迭代优化的方法和基于深度学习的方法。所有方法的超参数根据相\n关工作和经验进行了设置，以平衡配准精度和运行时间。\nHybridMorph 使用Pytorch 框架，在配备NVIDIA RTX 4090 GPU 的PC 上实现。\n所有模型都使用Adam 优化算法器训练500 个epochs，学习率为1 × 10−4，batchsize\n为1。在训练期间，图谱到患者脑部MRI 配准通过随机方向上的翻转进行数据增强，\n而患者间脑部MRI 配准则没有应用数据增强。在损失函数方面，图谱到患者配准使用\n归一化互信息（NCC）损失，而患者间MRI 配准使用均方误差（MSE）损失。正则化\n"}, {"page": 34, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n26\n项使用扩散正则化，正则化超参数λ分别设置为1（图谱到患者配准）和0.02（患者间\n配准）。为了更好地展示HybridMorph 的架构优势，本文尽量将实验参数和损失函数\n与TransMorph 保持一致。\n3.3.3 模型复杂度研究\n本文还研究了模型复杂度对配准性能的影响。表3-1 列出了所提出的HybridMorph\n模型的三个变体的参数设置和可训练参数的数量。在基本模型HybridMorph 中，初始\n维度�被设置为64，并且编码器的各层级中PCFA 块的数量分别被设置为2，2，4，2。\n此外，本文引入了HybridMorph-small，HybridMorph-large，它们分别是HybridMorph\n模型大小的1/4 倍和2 倍，在表3-1 中将HybridMorph 及其变体与由Transformer 及其\n变体构成的配准模型进行了比较，并在表3-4 中报告了不同参数量变体在多个脑数据\n集上的配准性能。\n表3-1 消融研究中使用的HybridMorph 模型的结构超参数。\nTable 3-1 The architecture hyperparameters of the HybridMorph models used in the ablation study\n模型\n初始维度\n编码器块数量\n参数量（M）\nGMACs\nTransMorph\n96\n2,2,4,2\n46.77\n686.8\nTransMorph-large\n128\n2,2,12,2\n108.34\n1084.9\nTransMorph-ViT\n96\n2,2,4,2\n110.60\n406.4\nVoxelMorph-large\n-\n-\n63.25\n3656.2\nHybridMorph\n64\n2,2,4,2\n6.43\n167.9\nHybridMorph-small\n32\n2,2,4,2\n1.69\n101.2\nHybridMorph-large\n96\n2,2,4,2\n14.23\n261.2\nHybridMorph-nopcfa\n64\n2,2,4,2\n21.65\n193.4\n3.3.4 实验分析\n3.3.4.1 配准结果\n在本实验中，本文训练了HybridMorph 进行图谱到患者间和患者间脑MRI 图像配\n准。本文选择在验证集上Dice 分数优秀的网络参数，并在测试集上报告结果。如图\n3-5 中所示，展示了图谱到患者间脑部MRI 配准的样本切片的定性配准结果，所提出\n的模型HybridMorph 对微分同胚没有显示约束，但与其他方法的定性比较中，生成了\n"}, {"page": 35, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n27\n更平滑的位移场（如图3-5 的第二行与第三行所示）。\n此外，如图3-5 中最后一行所示，我们对配准前及不同基线方法下的扭曲图像与\n固定图像之间的绝对差值进行了可视化。为增强视觉辨识度，我们将绝对误差归一化\n至区间[−0.5,0.5]，并映射至RGB 色域进行展示，颜色越浅代表配准误差越小。从视\n觉结果来看，我们的方法具有最低的绝对误差，表现出优越的定性配准效果，同时也\n取得了更高的Dice 分数。\n定量评价如表3-2 和表3-3 所示，结果表明，所提出的方法HybridMorph 在图谱\n到患者间脑部MRI 配准中获得了最高的平均Dice 评分0.781。相比之下，较TransMorph\n提高了2.9%。在患者间脑MRI 配准中HybridMorph 也以0.825 的平均Dice 评分取得\n了最佳成绩，且在折叠体素更少的情况下，其Dice 分数提高了1.6%。值得注意的是，\n这个Dice 分数是在可训练参数量减少了7.3 倍（如图3-7 所示）、计算复杂度降低了\n4.1 倍（如图3-8 所示）的条件下获得的，相较于TransMorph。如表3-4 所示，\nHybridMorph-large 在两个脑数据集中相较于HybridMorph 平均Dice 分数提高了0.2%，\n但其可训练参数量和计算复杂度仍远低于TransMorph。这表明HybridMorph 在配准精\n度、计算复杂性和模型参数方面均优于其他同类方法。\n如图3-6 所示，箱线图显示了使用所提出的HybridMorph 和基线配准方法的不同\n脑MRI 子结构的Dice 分数。本文的方法在关键解剖结构的配准精度上呈现系统性优\n势：对于壳核（Putamen）、苍白球（Pallidum）和脉络膜丛（Choroid-Plexus）等毫米\n级解剖结构上获得了较高的Dice 分数，HybridMorph 显著提高了小解剖结构的配准精\n度，表明其在细粒度特征保留和恢复方面的有效性；而对于脑干（Brain-Stem）、丘\n脑（Thalamus）、小脑皮层（Cerebellum-Cortex）和脑脊液（CSF）等大体积的解剖结\n构上同样获得了较高的Dice 分数，则证实了HybridMorph 同样具有较好的大形变建模\n能力。此外，从不同解剖区域的四分位距（interquartile range, IQR）来看，HybridMorph\n的IQR 显著小于其他方法，表明HybridMorph 具有更优的鲁棒性和泛化能力。\n"}, {"page": 36, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n28\n表3-2 不同配准方法在IXI 数据集上的定量比较\nTable 3-2 Quantitative comparison of the various registration methods on the IXI dataset\n模型\nDSC\nFR\nAffine\n0.406±0.035\n-\nSyN\n0.645±0.152\n<0.0001\nNiftyReg\n0.645±0.167\n0.020±0.046\nLDDMM\n0.733±0.126\n<0.0001\nVoxelMorph\n0.729±0.129\n1.590±0.339\nCycleMorph\n0.737±0.029\n1.719±0.382\nTransMorph-ViT\n0.732±0.030\n1.554±0.270\nTransMorph\n0.752±0.029\n1.440±0.303\nHybridMorph\n0.781±0.014\n1.355±0.307\n表3-3 不同配准方法在OASIS 数据集上的定量比较\nTable 3-3 Quantitative comparison of the various registration methods on the OASIS dataset\n模型\nDSC\nFR\nAffine\n0.571±0.053\n-\nSyN\n0.769±0.028\n<0.0001\nNiftyReg\n0.762±0.034\n0.020±0.04\nLDDMM\n0.733±0.126\n<0.0001\nVoxelMorph\n0.787±0.026\n1.290±0.319\nCycleMorph\n0.793±0.025\n1.219±0.362\nTransMorph-ViT\n0.808±0.023\n1.224±0.348\nTransMorph\n0.809±0.022\n0.390±0.328\nHybridMorph\n0.825±0.021\n0.204±0.047\n"}, {"page": 37, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n29\n图3-5 不同配准方法在脑磁共振图谱配准任务中的定性比较\nFigure 3-5 Qualitative comparison of various registration methods on the atlas-to-patient brain\nMR registration task\n图3-6 不同配准方法在脑磁共振图谱配准任务中的定量比较\nFigure 3-6 Quantitative comparison of thon methods on the atlas-to-patient brain MR\nregistration task\n"}, {"page": 38, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n30\n3.3.4.2 计算复杂度\n如图3-8 所示，本文比较了基于深度学习的配准模型间的计算复杂度。该实验使\n用的图像尺寸参考本工作中脑部MRI 数据集图像的大小。尽管所有基于卷积的配准模\n型计算复杂度与Transformer 相当，且可训练参数量均少于1M，但它们的配准性能明\n显较差。与此相比，基于Transformer 的模型因其二次计算复杂度而产生了大量的计算\n开销，通常具有超过30M 的参数量。所提出的HybridMorph 模型在计算复杂度上表现\n出较大优势，具有167.9 个GMAGs，显著低于所有基于Transformer 构建的配准模型\n和CycleMorph，仅为传统基于卷积网络的配准模型Voxelmorph 的42%计算复杂度。\n本文提出的HybridMorph 网络架构在平衡计算复杂度与性能方面表现突出。相比\n于TransMorph，HybridMorph 在参数量减少7.3 倍、计算复杂度降低4.1 倍的情况下，\n依然在所有评估的配准任务中表现出明显的优势。HybridMorph 不仅能够容纳更多的\n参数，而且具有更小的计算复杂度和更优秀的性能，充分证明了基于SSD 的模型在计\n算效率上的优势。本文的轻量化结构进一步强化了这一优势。\n此外，本文还对参数大小（即模型复杂性）对配准性能的影响进行了实验，定量\n结果如表3-4 所示。实验结果表明，HybridMorph-small 在两个验证集上的平均Dice\n得分最低，分别为0.778，0.809。尽管HybridMorph-large 取得了最佳的Dice 评分并展\n现了更快的模型收敛速度，其在Dice 评分上仅有0.2%微小提升，但计算成本几乎翻\n倍，因此适用于无计算资源负担同时需要快速收敛的情况。当模型复杂度为唯一变量\n时，计算复杂度与模型复杂度之间呈强相关性。随着模型复杂度的增加，计算复杂度\n也随之增加，且Dice 分数同样有所提升。\n在推理速度方面相比之下，所提出的HybridMorph 模型在保持较低模型参数量与\n计算复杂度的同时，展现出优越的效率表现。在周期训练过程中，其训练速度为5.88\n分钟/每轮，仅次于VoxelMorph；而在推理阶段，HybridMorph 以0.214 秒/图像的速\n度显著优于其他方法，相较于在配准精度上表现最佳的TransMorph，其推理速度提升\n约1.54 倍，充分体现了所提出方法在保证性能的同时具备更高的推理效率。\n"}, {"page": 39, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n31\n图3-7 基于深度学习的模型参数的数量比较（参数值以百万为单位）\nFigure 3-7 The number of parameters in each deep-learning-based model(The\nvalues are in units of millions of parameters)\n图3-8 以千兆乘积累加操作表示的模型计算复杂度比较\nFigure 3-8 Model computational complexity comparisons represented in Giga\nmultiply–accumulate operations (GMACs)\n3.3.5 消融实验\n本文进行了大量的消融实验，以验证所提出各个模块的有效性。在图谱到患者间\n和患者间脑MRI 数据集中，本文分别评估了浅层特征提取层的注意力块、PCFA 块以\n及跳跃连接的贡献。\n首先，当从浅层特征提取层的跳跃连接中移除注意力块时，图谱到患者配准的平\n"}, {"page": 40, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n32\n均Dice 得分为0.774，而患者间配准的平均Dice 得分为0.819。接着，当同时移除注\n意力块和跳跃连接时，图谱到患者的平均Dice 得分下降至0.750，患者间配准的平均\nDice 得分也降至0.777。本文观察到，相较于移除注意力块，移除跳跃连接对配准性\n能的影响更为显著，但即使如此，移除跳跃连接后的性能仍小幅优于TransMorph 移除\n跳跃连接后的得分。\n接下来，本文评估了移除PCFA 块的情况，仅使用HRM 块作为深度特征提取层。\n在这种设置下，图谱到患者配准的平均Dice 得分为0.78，患者间配准的平均Dice 得\n分为0.823。模型参数量上涨至21.65M，计算复杂度上涨至193.46GMACs 的同时性能\n出现了微小的降低。在训练过程中，本文观察到，移除PCFA 块后，在验证集上的Dice\n得分相比于标准HybridMorph 表现出更大幅度的波动，这进一步验证了本文提出的\nPCFA 轻量化模块对稳定性和性能的重要贡献。\n表3-4 HybridMorph 模型的消融实验\nTable 3-4 The Ablation Study of HybridMorph models\n模型\nIXI Dataset\nOASIS Dataset\nDSC\nFR\nDSC\nFR\nw/o attention\n0.774±0.015\n1.395±0.319\n0.819±0.017\n0.238±0.068\nw/o skip\n0.750±0.012\n0.242±0.136\n0.777±0.014\n0.0250±0.014\nw/o PCFA\n0.780±0.013\n1.458±0.316\n0.823±0.018\n0.216±0.054\nHybridMorph\n0.781±0.014\n1.555±0.307\n0.825±0.021\n0.204±0.047\nHybridMorph-small\n0.778±0.123\n1.219±0.382\n0.809±0.020\n0.225±0.074\nHybridMorph-large\n0.783±0.021\n1.579±0.328\n0.827±0.018\n0.227±0.058\n3.4 本章小结\n在本文中，本文介绍了HybridMorph，一种用于无监督可变形图像配准的轻量化\n混合模型。HybridMorph 基于卷积和SSD 框架重构了核心特征提取模块，相较于\nTransformer，Mamba-2 在处理长距离信息相关性方面表现出显著的优势与效率，同时\n保持线性复杂度，这使得HybridMorph 成为解决实际医疗环境中计算资源限制的有力\n候选者。本文评估了HybridMorph 在患者间配准和图谱到患者间脑MRI 配准任务中的\n"}, {"page": 41, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n33\n表现，结果表明，HybridMorph 相比多种传统方法和基于学习的方法实现了更高的配\n准精度，证明了其在可变形医学图像配准中的有效性。\n本文的工作存在一些局限性。首先，由于训练基线方法的时间和可用GPU 资源有\n限，本文通过经验或基线方法建议的超参数值来进行设定，而未使用广泛的网格搜索\n进行优化。此外，鉴于本研究引入了一种通用的轻量化图像配准网络结构，本文更专\n注于在结构层面平衡计算复杂度与性能，而非深入优化损失函数或采用复杂的训练方\n法来选择最优配置。所提出的网络结构可以轻松结合多尺度、周期一致性（GAN）等\n方法进行训练调整，并且能够与任何配准损失函数一起使用。\n在未来的研究中，本文计划在以下三个方向进行扩展。其一，利用图像生成策略\n增强训练数据或替代损失函数（例如互信息）来扩展所提出方法在多模态配准任务中\n的潜力；其二，选择合适的正则化器（例如逆一致性），使所提出的方法获得更平滑\n且通常微分同胚的变换。其三，进一步训练HybridMorph 在其他器官（例如肺、肝脏、\n心脏）上的配准效果以验证其在多器官上的模型泛化性，并验证其对于噪点、伪影或\n不同扫描仪源图像的模型鲁棒性。\n"}, {"page": 42, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n34\n第四章小波引导的多尺度ConvNeXt 的无监督医学图像配准\n4.1 引言\n现有的医学图像配准深度神经网络架构主干多以卷积神经网络和Transformers 架\n构为主。Transformers 随着Vision Transformer[61]，swin transformer[38]的引入在一系列\n计算机视觉任务中实现了先进的性能使其取代标准卷积成为视觉任务主要网络骨干，\n也包括医学图像配准领域。其自注意力机制通过将图像视为连续的块序列能够更好的\n捕获图像长距离信息的相关性是Transformer 架构在视觉任务中的主要优势之一。但是，\n由于全局自注意力机制的计算复杂度与输入尺寸呈二次方关系，在处理高分辨率三维\n医学影像数据（如MRI/CT）时，Transformer 架构会产生极高的计算开销和参数激增\n问题，这与临床实际部署场景中的计算资源限制存在根本性冲突。同时，由于其有限\n的归纳偏差，Transformers 的性能优势更依赖于大型数据集和更大的模型。这些大型\n数据集在自然图像中很常见，但医学图像数据集通常很难轻易的获得大量的数据。最\n近，完全由卷积模块构建的ConvNeXt[58]通过利用Transformer 的优势架构进行改进保\n留卷积固有的归纳偏置的同时保持了标准卷积神经网络的效率和参数量。ConvNeXt\n在准确性、可扩展性和鲁棒性方面与Transformer 相媲美，使其存在成为医学图像配准\n任务的主干网络的潜力。\n基于深度学习的医学图像配准方法多以采用具有能够共享权重的跳跃连接的U 型\n结构（U-Net）[55]，U-Net 通过下采样和上采样模块使卷积扩大感受野捕获全局上下文\n和局部细节以实现体素级别的对齐。但是传统的下采样（例如最大池化[74]、跨步卷积[75]）\n在减少图像分辨率会造成图像空间信息（尤其是小结构和边缘信息）的丢失，而上采\n样（例如三线性插值[86]、转置卷积[87]）虽然可以恢复图形尺寸，但是缺乏对高频细节\n信息的建模能力无法精确恢复下采样时丢失的细节，从而影响配准精度。小波变换是\n一种从时域分析中建立的工具，它将信号分解为不同频率和时间尺度的成分来分析信\n号的特征[88]。小波变换的理论和实践在图像处理工作中得到了广泛的研究，例如图像\n分解[89]、图像压缩[90]等方面，其无损分解的特性使其应用于下采样对于空间信息保留\n"}, {"page": 43, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n35\n具有天然优势。将小波无损分解应用于下采样能够在保留高频细节的同时降低空间分\n辨率，避免传统下采样导致的信息丢失，提升对组织边界、解剖结构的感知能力，高\n频子带的显式保留将使网络在无显性微分同胚的约束条件下，拥有更好的拓扑保持特\n性。动态上采样能够根据图像的局部特征和任务需求自适应地调整上采样策略，相比\n传统上采样方法的固定采样策略，能够更精准地采样局部细节信息，更好地恢复医学\n图像的拓扑结构。\n基于上述分析，当前的方法仍存在几个问题：第一，传统下采样在降低空间分辨\n率过程中会丢弃部分信息可能导致网络难以配准关键解剖结构细节，上采样时又因固\n定策略无法精确恢复高频信息，从而影响配准精度；其次Transformer 类方法虽然具有\n捕获长距离依赖的优势，但其高昂的计算成本难以满足实时性要求，而传统的标准卷\n积类方法虽高效但配准精度较低。基于此，本文利用小波变换多尺度无损分解信号的\n优势结合ConvneXt 的架构优势提出了一种新颖的医学图像配准架构WaveMorph，该\n架构进一步引入了轻量化动态上采样模块，旨在最大程度地减少编解码阶段下采样和\n上采样期间关键解剖结构的失真，从而改善配准精度和实时性需求。WaveMorph 在由\n基于Transformer，CNN 和传统方法组成的基线上实现了最先进的性能。这项工作的贡\n献有四个方面：\n1.WaveMorph：本文提出了一种基于融合小波变换和ConvNeXt 结构的新型神经网\n络WaveMorph，用于无监督可变形医学图像配准，提供了频域-空域协同优化的新范式。\n2.本文提出了一个多尺度小波特征融合（Multi-Scale Wavelet Feature Fusion,\nMSWF）下采样模块，通过结合Haar 小波无损分解和ConvNeXt 架构，利用多尺度卷\n积核提取并融合来自8 种不同频率子图的特征，从而实现高效的下采样。\n3.本文创新性地将应用于图像超分辨率的轻量化动态上采样模块引入医学图像配\n准领域，解决传统上采样方法在配准过程中易导致关键解剖结构的模糊或失真问题，\n大量的实验结果表明动态上采样模块有效提升配准精度和鲁棒性。\n4.本文在患者间配准和图谱到患者脑MRI 配准数据集上广泛验证了所提出的配准\n模型。相比传统方法及深度学习基线方法，所提出的模型展示了最先进的性能，分别\n达到了0.779±0.015 和0.824±0.021 的Dice 系数，同时推理速度上实现了大幅优化，\n仅0.072s/image，验证了该模型在医学图像配准任务中的有效性和实时性。\n"}, {"page": 44, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n36\n4.2 小波引导的多尺度ConvNeXt 的无监督医学图像配准方法\n4.2.1 可变形配准框架\nConvNeXt 通过保留卷积神经网络的归纳偏置特性，使其在较小样本医学数据上具\n有比Transformer 更好的泛化能力，同时卷积操作的局部性先验和平移不变性使其更适\n合处理三维医学图像的多维度空间依赖性，同时通过非顺序性、旋转和平移不变性等\n视觉特性建模，显著提升对解剖结构的空间关系解析能力。尽管ConvNeXt 的远程建\n模能力弱于Transformer 的全局注意力机制，但其通过采用更大的卷积核（7×7×7）\n有效扩展了感受野，间接增强了跨区域特征关联的建模能力。与传统CNN 中依赖批\n量统计信息的BatchNorm 不同，ConvNeXt 引入的LayerNorm 通过逐层归一化策略，\n避免了对批量大小的敏感性，从而在小批量（Small-batch）医学图像配准任务中具有\n更好的鲁棒性和泛化能力。\n图4-1 概述了所提出的单流无监督图像图像配准网络结构，WaveMorph，其中移\n动图像�与固定图像�在数据预处理中已完成仿射对齐，因此网络将更关注于体素间的\n非线性空间映射。WaveMorph 遵循编码器-解码器的U 型网络结构，m，f 代表单通道\n三维医学图像，首先�，�在通道维度拼接为单输入，即�: Ω →���×�×�×�，其中�、�、\n�和�分别代表通道、高度、宽度和切片数，由一个标准卷积混合通过共享特征形成\n隐式强度分布差异(非线性映射关系)感知。随后WaveMorph 集成了四个连续的多尺度\n小波特征融合层，其中包含了高效多尺度小波特征融合下采样模块（MSWF）和\nConvNeXt 结构的特征提取模块，ConvBlock。经过每个MSWF 块，特征图的通道数加\n倍而分辨率减半，因此，编码器末级输出深层特征图尺寸为��∈��×��× �\n��×�\n��× �\n��。在\n此之后，WaveMorph 采用瓶颈快来建模长距离空间相关性，此时特征图的大小不变。\n之后，WaveMorph 集成了四个连续的解码器卷积块（Decoder Convolution, DecConv），\n由连续的动态上采样层，带有残差连接的不同卷积核大小的标准3D 卷积和ConvBlock\n组成，用于特征解码和图像分辨率恢复。\n在解码器阶段，WaveMorph 遵循U-Net 的设计，每个上采样的特征图通过跳跃连\n接与来自解码器特征图拼接，为解码器提供多级特征映射。不同于一般的跳跃连接，\n我在跳跃连接中添加了由多尺度卷积增强后的逆离散小波变换后的图像，弥补了传统\n跳跃连接中仅传递单尺度特征的局限性。最后，采用一个标准3D 卷积输出与原始图\n"}, {"page": 45, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n37\n像相同分辨率的映射特征图（即变形场∅），表示三个正交轴上的体素方向位移。通\n过空间变换函数将移动图像�扭曲为配准后图像�∘∅，然后通过公式2-1 评估�和�∘\n∅的相似性，从而实现精确的解剖对齐。\n图4-1 WaveMorph 图像配准模型的总体框架\nFigure 4-1 The overall framework of WaveMorph image registration model\n图4-2 编码器卷积模块的框架示意图\nFigure 4-2 The framework of Decoder Convolution block\n4.2.2 多尺度小波特征融合模块\n在3D 离散小波变换（DWT）中，使用由低通（L）和高通（H）滤波器组成的八\n个滤波器（即����, ���ℎ, ��ℎ�, ��ℎℎ, �ℎ��, �ℎ�ℎ, �ℎℎ�, �ℎℎℎ）将图像�沿着�，�和�三个方向正交\n分解为分辨率减半的八个包含不同空间域和频域信息的子带图像。因此，�将被分解为\n纯低频分量����、纯高频分量�ℎℎℎ以及包含高频和低频混合频带的六个分量\n（���ℎ, ��ℎ�, ��ℎℎ, �ℎ��, �ℎ�ℎ, �ℎℎ�）。在本研究中，本文使用高效简单的Haar 小波作为作为基\n函数，但不局限于此，其他小波基函数同样适用，但计算成本会有所增加。由于DWT\n"}, {"page": 46, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n38\n的正交特性，即使经过卷积的线性变换后也可以通过逆小波变换(IDWT)精确地重建。\n图4-3 Haar 小波变换分解的8 个子带图\nFigure 4-3 The eight subband diagrams decomposed by Haar wavelet transform\nChen 等人[91]提出通过最初将图像分离为高频分量和低频分量来对输入执行卷积\n并在两者之间执行信息交换，尽管实现方式与小波变换无关，但验证了在输入的低频\n分量上与高频分量分开执行卷积的好处，即获得更有信息量的特征图。受此启发，本\n文创新性地结合离散小波变换（DWT）特性提出了多尺度小波特征融合下采样模块。\n对于如图4-3 所展示的经过小波变换的八个子带图，根据子带特征所携带的不同信息\n性质实施差异化特征增强策略。如图4-4 所示，对于包含全局信息的纯低频子图（LLL），\n本文使用大卷积核（大小为7×7×7）的卷积块进行特征提取。大小为7 的卷积核具\n有较大的感受野，能够捕捉到图像的整体结构和全局信息，有助于在配准过程中增强\n全局形变的建模能力。对于包含局部解刨细节的纯高频分量子图（HHH），本文使用\n小卷积核（大小为3×3×3）进行特征提取。大小为3 的卷积核具有较强的局部特征\n提取能力，能够有效地捕捉到图像中的细节和边缘信息，使其在处理局部非刚性变形\n时表现出色。对于包含全局信息和局部信息分量的六个子图\n"}, {"page": 47, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n39\n（���、���、���、���、���、���），本文使用分别使用三个尺度卷积核（大小\n分别为1，3，5）进行特征提取。这些子带图包含了图像在不同维度上的混合频率信\n息，多尺度卷积核能够捕捉到不同尺度下的特征。对于每个子图通过将三个尺度的特\n征图在通道维度拼接，并经过点卷积融合多尺度特征，提取出这些子图中丰富的特征\n信息，以获得包含丰富全局感知力与对非规则结构的适应性的特征图。对于经过特征\n增强的八个字带图，MSWF 根据是否输出下采样的特征图划分了两个策略，如图4-4\n中绿色框所示。对于输出原分辨率的策略，本文将在4.2.3 小节中详细论述。\n对于输出下采样特征的策略，MSWF 通过三个差异化特征增强策略获得八个增强\n特征����−ℎℎℎ，将这八个增强特征在通道维度拼接后，由一个点卷积将通道维度压缩至\n输出维度。由于对小波变换后的八个子带图分开执行特征增强，卷积特征共享优势仅\n存在于独立的子带图中而缺乏跨自带图的信息共享不利于模型的表达能力提升。因此，\nMSWF 引入了四分支结构的跨通道交互的注意力层(CDA)，加强不同子图间通道信息\n和空间信息的融合，保证了各频率信息的充分交流及自适应调整，提升训练的稳定性\n和模型的泛化能力。一个完整的MSWF 可以被表示为：\n\n\n\n\n,\n,\n,\n,\n,\n,\n,\n \n \nin\nLLL LLH LHL LHH HLL HLH HHL HHH\nDWT F\n\n(4-1)\n\n\n7\n \nlll\nF\nConv\nLLL\n\n(4-2)\n\n\n3\n \n \n \nhhh\nF\nConv\nHHH\n\n(4-3)\n(\n(\n)) ,\n{1,3,5}\n(\n)\nllh hhl\nk\nF\nPConv Concat Conv LLH\nHHL\nk\n\n\n\n\n(4-4)\n(\n(\n,\n,\n,\n,\n,\n,\n,\n))\n(\n)\nout\nlll\nllh\nlhl\nlhh\nhll\nllh\nhhl\nhhh\nF\nCDA PConv Concat F\nF\nF\nF\nF\nF\nF\nF\n\n(4-5)\n其中，���，����是输入和输出特征图，DWT 代表三维可分离小波变换，[LLL-HHH]\n表示经过小波变换后分辨率减半的八个子带图像，Conv 是ConvNeXt 模块，其中下角\n标为卷积核的大小，����−ℎℎℎ表示经过ConvNeX 模块的增强特征，PConv 表示点卷积，\nCDA 表示跨维度注意力层。\n"}, {"page": 48, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n40\n图4-4 多尺度小波特征融合模块架构示意图\nFigure 4-4 The architecture of Multi-Scale Wavelet Feature Fusion Module(MSWF)\n4.2.3 瓶颈模块\n在传统U-Net 结构在编码器端通过下采样逐渐减小图像的分辨率后进行特征提取，\n这样做的好处，一方面降低计算量和显存占用，一方面对于固定卷积核大小的卷积能\n够在更大图像范围做特征提取从而扩大感受野。本文在编码器末端引入了一个创新的\n瓶颈块来扩大这些优势，通过低计算成本的最大化感受野，进一步建模空间长距离依\n赖性，WaveMorph 通过合并两个连续的输出原分辨率特征图的MSWF 模块。具体来\n说，如图4-4 绿色块未下采样分支中所示，输入特征图在经过小波变换后分辨率减半\n的八个子带图经过并行的多尺度ConvBlock，得到特征增强的子带图，再由逆小波变\n换恢复与输入特征图相同的分辨率，这些特征通过注意力机制进行融合再经过大内核\n的ConvBlock 进一步特征提取，确保全局信息在网络中得到充分利用，最终得到输出\n特征图。整个瓶颈块可以被视作两个微型的U-Net 结构，瓶颈块能够在理论感受野最\n大的情况下多个尺度上精细地提取和融合图像特征，尤其是全局形变的相关特征，在\n这些瓶颈区域内，特征通道的数量和分辨率保持不变。对于输出原分辨率特征图的\nMSWF 模块的实现如图4-4 粉色块所示保持与公式4-1 到4-4 相同的操作，获得八个\n经过卷积增强的特征。\n(\n,\n,\n,\n,\n,\n,\n,\n)\nidwt\nlll\nllh\nlhl\nlhh\nhll\nllh\nhhl\nhhh\nF\nIDWT F\nF\nF\nF\nF\nF\nF\nF\n\n(4-6)\n7\n(\n(\n))\n(\n)\nout\nidwt\nF\nConv\nCDA PConv F\n\n(4-7)\n"}, {"page": 49, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n41\n其中，IDWT 代表三维小波逆变换，Conv_7 代表卷积核为7 的ConvNeXt 模块。\nMSWF 将通过公式4-6，公式4-7，将八个经过卷积增强的特征恢复原分辨率，并进行\n进一步的特征细化。\n4.2.4 轻量化动态上采样模块\n在基于深度学习的医学图像配准方法中，上采样模块一般位于解码器端，不仅用\n于恢复图像分辨率，更重要的是在每个尺度上提取到更多的空间特征，其精度和效率\n直接影响配准结果的准确性及计算资源消耗。传统上采样方法（如三线性插值和最近\n邻插值）依赖固定插值规则，难以适应医学图像中复杂的解剖结构（如细微的解剖学\n结构或肿瘤边缘），易导致配准后边缘模糊或锯齿状伪影，降低配准精度。为了缓解\n此类问题，并适用于计算开销受限的医学场景，本文将图像超分辨领域的轻量化动态\n上采样器DySample[92]引入了医学图像配准任务中。其核心思想是绕过传统的动态卷积，\n通过动态采样点建模几何信息，无需高分辨率引导特征和复杂子网络，仅需引入极少\n量参数（千级参数增量）即可实现动态采样。具体来说，DySample 通过线性投影生成\n采样偏移量，结合PyTorch 内置的grid_sample 函数进行重采样。首先，采用双线性初\n始化调整初始采样位置，确保零偏移时输出与双线性插值一致，避免初始分布不均；\n其次，通过动态范围因子（Dynamic Scope Factor）约束偏移量范围，抑制采样点重叠\n导致的边界预测误差；最后，通过分组上采样将特征通道划分为多组独立生成偏移，\n增强对不同语义区域的自适应能力。如表4-2 和表4-3 所示，DySample 的引入，显著\n提升了医学图像的配准精度，且对GPU 内存占用和训练时间的影响可忽略不计。\n4.3 实验结果与分析\n4.3.1 数据集与预处理\n为全面验证所提方法的有效性，本文使用了两个数据集来评估在医学图像配准中\n常用的两项任务，分别是图谱到患者的配准任务和患者间的配准任务，包含1000 多个\n脑MRI 的T1 图像。首先，对于图谱到患者脑部MRI 配准任务，采用公开的图像信\n息提取（IXI）数据集来评估所提出的模型的表现，IXI 数据集由600 张来自正常健\n康受试者的MRI 脑部图像构成，基于IXI 数据集筛选576 例健康受试者的T1 加权脑\nMRI 影像作为固定图像，并使用从CycleMorph 获得的图谱脑MRI 作为移动图像，按\n照7:1:2 比例划分为训练集（403 例）、验证集（58 例）和测试集（115 例）。\n"}, {"page": 50, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n42\n针对更具临床挑战性的患者间配准任务，采用OASIS（Open Access Series of\nImaging Studies）数据集的414 例T1 影像。依据TransMorph 基准实验协议划分为394\n例训练集和20 例测试集（因官方未公开独立测试集），通过随机选择训练集样本作为\n固定图像并与移动图像形成配准对，同时应用图像角色反转策略（即交换移动/固定图\n像）生成双重训练样本（即394 例配准对）以增强模型泛化能力。\n所有MRI 影像数据均经过标准化预处理流程，包括采用Freesurfer[85]进行各向同\n性重采样（1×1×1mm³），基于AC-PC 平面对齐的仿射空间归一化，应用BET（Brain\nExtraction Tool）算法的颅骨剥离，以及统一裁剪至160（矢状）×192（冠状）×224\n（轴向）体素尺寸。配准性能评估采用基于Freesurfer 自动解剖分割的金标准，其中\nIXI 测试集采用涵盖皮质和白质分区的30 个精细解剖结构标签，OASIS 测试集则扩展\n至包含海马、杏仁核等深部核团的35 个解剖结构，这些分割图仅用于评估配准性能。\n该实验设计严格遵循TransMorph 的标准化预处理流程，确保了跨方法比较的公平性。\n4.3.2 实验设置\n本文将WaveMorph 与之前表现出最先进配准性能的各种配准方法进行了比较，这\n包括了三个基于传统迭代优化的方法和四个基于深度学习的方法。所有方法的超参数\n根据相关工作和经验进行了设置，以平衡配准精度和运行时间。\nWaveMorph 使用Pytorch 2.3.1 框架和CUDA 11.8 版本，在配备NVIDIA RTX 4090\nGPU的PC上实现。所有模型优化器统一使用Adam 算法，训练周期固定为500个epochs，\n学习率为1 × 10−4，batchsize 为1。在训练期间，针对不同配准任务的特性，实验设\n置差异化配置，在图谱到患者脑部MRI 配准任务中数据增强采用随机三轴翻转（概率\np=0.5）以提升模型对解剖对称性的泛化能力，相似度度量选用归一化互相关损失\n（NCC），通过局部窗口（9×9×9 体素）计算强度协方差以增强鲁棒性。在患者间\n配准任务中，禁用空间变换增强以保留原始解剖拓扑关系，采用均方误差损失（MSE）\n最大化图像对齐精度。两类任务均采用扩散正则化项，正则化超参数λ分别设置为1（图\n谱到患者配准）和0.02（患者间配准），前者强化形变场平滑性以抑制图谱噪声干扰，\n后者弱化约束以保留个体解剖变异。为了更好地展示WaveMorph 的架构优势，实验严\n格将实验参数和损失函数与TransMorph 保持一致。\n"}, {"page": 51, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n43\n4.3.3 实验分析\n4.3.3.1 配准结果\n本研究在脑MRI 图像配准的两个关键任务（图谱-患者配准与患者间配准）中对\nWaveMorph 进行了系统性评估。本文选择在验证集上Dice 分数优秀的网络，并在测\n试集上报告结果。如图4-5 中所示，展示了图谱到患者间脑部MRI 配准的样本切片的\n定性结果，其中，第一行展示了不同方法配准后的扭曲图像，第二行和第三行分别是\n变形场的可视化（将三个维度上的位移量分别映射到RGB 颜色通道）和将变形场作用\n于标准网格的图像。尽管未显式施加微分同胚约束，WaveMorph 生成的位移场展现出\n卓越的拓扑保持性，与其他方法的定性比较中，生成了更平滑的位移场(如图4-5 中第\n三行和第四行)。如图4-5 中最后一行，本文可视化了配准前和不同基线方法下扭曲图\n像和固定图像之间的绝对差值，并将绝对差值归一化到[-0.5，0.5]后映射到RGB 色域，\n颜色越浅意味着配准误差值越低，从视觉结果来看，本文的方法具有最低的绝对误差，\n表现出了更好的定性结果。\n定量评价如表4-1 所示，结果表明，所提出的方法WaveMorph 在两项关键医学图\n像配准任务中均取得了最好的Dice 分数，和更少的折叠体素相比其他的非显性微分同\n胚约束的深度学习方法。在图谱到患者间脑部MRI 配准中获得了最高的平均Dice 评\n分0.779。相比之下，较TransMorph 提高了2.7%。在患者间脑MRI 配准中WaveMorph\n也以0.824 的平均Dice 评分取得了最佳成绩，其Dice 分数提高了1.5%。如表4-4 所\n示，WaveMorph 在推理速度（0.072s/image）方面显著优于现有方法，较第二名的\nViT-V-Net（0.197s/样本）提升2.73 倍。如图4-6 所示，箱线图显示了使用所提出的\nWaveMorph 和现有MIR 方法的不同脑MRI 子结构的Dice 分数。本文的方法在关键解\n剖结构的配准精度上呈现系统性优势：对于第三脑室（Third-Ventricle）、第四脑室\n（Fourth-Ventricle）和脉络膜丛（Choroid-Plexus）等毫米级解剖结构上获得了较高的\nDice 分数，其配准精度提升验证了MSWF 和Dysample 模块在细粒度特征保留和恢复\n方面的有效性；而对于大脑皮层（Cerebral-Cortex）、小脑皮质（Cerebellum-Cortex）\n和丘脑（Thalamus）等大体积的解剖结构上同样获得了较高的Dice 分数，则证实了\nConvNeXt 架构的大形变建模能力。值得注意的是，WaveMorph 在各解剖指标上的四\n分位距（IQR）较对比方法显著缩小，表明WaveMorph 具有更优的鲁棒性和泛化性。\n"}, {"page": 52, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n44\n综合模型推理效率（4.6 倍于TransMorph），参数量（仅0.7M），鲁棒性及配准精度\n等维度，WaveMorph 展现出显著的临床适用性优势。\n表4-1 不同配准方法在图谱-患者（IXI）和患者内（OASIS）脑MRI 配准任务上的定量比较\nTable 4-1 Quantitative comparison of the various registration methods on the Atlas-to-patient (IXI)\nand Inter-patient (OASIS) brain MRI registration task\n模型\nAtlas-to-Patient MRI\nInter-Patient MRI\nDSC\nFR\nDSC\nFR\nAffine\n0.406±0.035\n-\n0.571±0.053\n-\nSyN\n0.645±0.152\n<0.0001\n0.769±0.028\n<0.0001\nNiftyReg\n0.645±0.167\n0.020±0.046\n0.762±0.034\n0.020±0.04\nLDDMM\n0.733±0.126\n<0.0001\n0.733±0.126\n<0.0001\nVoxelMorph\n0.729±0.129\n1.590±0.339\n0.787±0.026\n1.290±0.319\nCycleMorph\n0.737±0.029\n1.719±0.382\n0.793±0.025\n1.219±0.362\nViT-V-Net\n0.732±0.030\n1.554±0.270\n0.808±0.023\n1.224±0.348\nTransMorph\n0.752±0.029\n1.440±0.303\n0.809±0.022\n0.390±0.328\nWaveMorph\n0.779±0.015\n1.310±0.313\n0.824±0.021\n0.204±0.047\n图4-5 不同配准方法在脑磁共振图谱配准任务中的定性比较\nFigure 4-5 Qualitative comparison of various registration methods on the atlas-to-patient brain\nMR registration task\n"}, {"page": 53, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n45\n图4-6 不同配准方法在脑磁共振图谱配准任务中的定量比较\nFigure 4-6 Quantitative comparison of the various registration methods\non the atlas-to-patient brain MR registration task\n4.3.3.2 计算与模型复杂度\n如图4-7 和图4-8 所示，本文比较了基于深度学习的配准模型间的模型复杂度和\n计算复杂度。该实验使用的图像尺寸参考本工作中脑部MRI 数据集图像的大小。基于\nTransformer 的模型因其二次计算复杂度而产生了大量的计算开销，通常具有超过30M\n的参数量。基于标准卷积的配准模型计算复杂度与Transformer 相当，可训练参数量均\n少于1M，但它们的配准性能明显较差。与此相比，所提出的WaveMorphh 模型在计\n算复杂度上介于基于标准卷积和Transformer 构建的配准模型介于，具有534.71 个\nGMAGs，显著低于CycleMorph。在模型复杂度（即，可训练参数量）上与基于标准\n卷积构建的配准模型相当仅有0.7M，显著低于TransMorph 的46.8M，但在所有评估\n的配准任务中表现出明显的性能优势。WaveMorphh 的架构优势，使其具有比\nTransformer 更小的计算和模型复杂度和更优秀的性能，充分满足了实际医疗环境中医\n疗任务中低参数和低计算负载的必要性。\n"}, {"page": 54, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n46\n图4-7 基于深度学习的模型参数的数量比较（参数值以百万为单位）\nFigure 4-7 The number of parameters in each deep-learning-based model\n(The values are in units of millions of parameters)\n图4-8 以千兆乘积累加操作表示的模型计算复杂度比较\nFigure 4-8 Model computational complexity comparisons represented\nin Giga multiply–accumulate operations (GMACs)\n"}, {"page": 55, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n47\n4.3.4 消融实验\n本文进行了大量的消融实验，以验证所提出各个模块的有效性。在图谱到患者间\n和患者间脑MRI 数据集中，本文分别评估了MSWF 下采样模块与Dysample 上采样模\n块的独立贡献。\n如表表4-2 和表4-3 所示，本文选取三种下采样策略，分别是在多数任务中常用\n的最大池化（Maxpooling），Swin Transformer 中报告的PatchMerging 下采样策略，以\n及Xu 等人[71]报告的使用Haar 小波进行下采样的策略Wavesample，由于其本身是应用\n于二维自然图像分割，本文将其拓展至适用于三维图像便于进行实验。同样，本文选\n取了两种上采样策略，分别是最邻近插值（Nearest）和三线性插值（Trilinear）。结合\nWaveMorph 中的MSWF 下采样与Dysample 上采样，共形成了12 种组合分别在两种\n图像配准任务中测试以验证下采样和上采样策略对图像配准性能的影响。在网络架构\n方面统一使用WaveMorph 框架，编码器使用ConvneXt 模块作为特征提取层，解码器\n使用由标准卷积和残差连接构成的DecConv 模块，在未使用MSWF 模块的瓶颈块中，\n本文使用两个连续的ConvneXt 模块代替。实验结果表明，当固定上采样模块为\nDysample 时，MSWF 相较PatchMerging，Dice 系数在图谱到患者间配准任务上提升了\n2.7%，在患者间配准任务上提升了0.5%，在固定下采样模块为MSWF 的条件下，\nDysample 较最近邻插值（Nearest），Dice 系数在图谱到患者间配准任务上提升了2.1%，\n在患者间配准任务上提升了0.9%。MSWF+Dysample 组合在两个配准任务中达到最优\n精度，分别是在图谱到患者间配准任务上平均Dice 系数为0.779，在在患者间配准任\n务上平均Dice 系数为0.824。本文观察到，在WaveMorph 框架下仅使用Dysample 上\n采样模块，依旧取得了接近或超越TransMorph 的性能表现，而在仅使用MSWF 下采\n样模块和采用非下采样MSWF 模块作为瓶颈块的构成时，在两个配准任务中均取得了\n优于TransMorph 的性能表现，这进一步验证了本文提出的MSWF 模块对稳定性和性\n能的重要贡献。\n"}, {"page": 56, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n48\n表4-2 不同配准方法在图谱-患者（IXI）脑MRI 配准任务上的定性比较\nTable 4-2 Quantitative comparison of the various registration methods\non the Atlas-to-patient (IXI) brain MRI registration task\n上采样\n下采样\nNearest\nTrilinear\nDysample\nAtlas-to-Patient MRI\nDSC\nFR\nMaxpooling\n\n\n\n0.747±0.030\n1.581±0.329\n\n\n\n0.748±0.029\n1.578±0.348\n\n\n\n0.756±0.028\n1.591±0.355\nPatchmerging\n\n\n\n0.748±0.030\n1.511±0.321\n\n\n\n0.748±0.030\n1.540±0.346\n\n\n\n0.752±0.028\n1.536±0.361\nWavesample\n\n\n\n0.749±0.022\n1.531±0.325\n\n\n\n0.751±0.030\n1.542±0.337\n\n\n\n0.754±0.028\n1.539±0.341\nMSWF\n\n\n\n0.758±0.018\n1.386±0.337\n\n\n\n0.764±0.020\n1.411±0.324\n\n\n\n0.779±0.015\n1.310±0.313\n表4-3 不同配准方法在患者内（OASIS）脑MRI 配准任务上的定性比较\nTable 4-3 Quantitative comparison of the various registration methods\non Inter-patient (OASIS) brain MRI registration task\n上采样\n下采样\nNearest\nTrilinear\nDysample\nPatient-to-Patient MRI\nDSC\nFR\nMaxpooling\n\n\n\n0.809±0.017\n0.167±0.056\n\n\n\n0.811±0.019\n0.174±0.055\n\n\n\n0.815±0.018\n0.222±0.062\nPatchmerging\n\n\n\n0.811±0.017\n0.171±0.055\n\n\n\n0.812±0.017\n0.174±0.058\n\n\n\n0.818±0.018\n0.213±0.054\nWavesample\n\n\n\n0.812±0.018\n0.170±0.057\n\n\n\n0.814±0.017\n0.175±0.054\n\n\n\n0.819±0.018\n0.211±0.055\nMSWF\n\n\n\n0.815±0.019\n0.178±0.050\n\n\n\n0.819±0.018\n0.184±0.055\n\n\n\n0.824±0.021\n0.204±0.047\n"}, {"page": 57, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n49\n4.3.5 讨论\n4.3.5.1 信息重要性分析\n在深度学习模型中，可解释性的一大挑战在于信息损失不可追踪，尤其是常规池\n化操作（如最大池化、平均池化）会丢弃部分信息，从而影响网络对输入数据原始特\n性的学习能力。相比于传统的黑箱式降维方法，小波变换提供了一种数学可逆的下采\n样方式，小波变换清晰地提供了各个频带的信息，使研究者和临床医生能够追踪模型\n在不同频率层面所提取的特征，从而提升网络训练过程的可解释性。具体而言，小波\n变换能够将输入图像分解为多个频率分量：其中，低频分量保留主要的结构信息，与\n传统下采样结果相似；高频分量则分别捕捉图像在水平、垂直以及对角方向上的边缘\n和纹理特征。这些高频分量为组织边界、小解剖结构等细粒度信息的表达提供了补充。\n在医学图像配准任务中，这种多尺度的频域信息增强了模型对边缘、轮廓和纹理特征\n的感知能力，有效减缓了卷积神经网络（CNN）因局部特征提取而导致的配准不稳定\n问题。\n基于CNN 的方法，例如VoxelMorph，CycleMorph，在下采样阶段通常采用最大\n池化操作，通过仅保留领域中最大值的方法来降低分辨率，在这个过程中丢失的信息\n即无法被网络利用也无法还原，这将影响配准的精确性，如表4-2 中MSWF 模块相较\n于MaxPooling 使Dice 系数提升2.3%。基于Transformer 的方法，例如ViT-V-Net，\nTransMorph，通过PatchMerging的方法优化了MaxPooling操作中存在的信息丢失问题，\n其思想是将图像分割为多个Patch 后通过拼接和压缩来实现分辨率的减低，虽然在自\n然图像中这种方法所造成信息混杂的影响不大，但在医学图像配准任务中信息的混杂\n会对解剖结构的拓扑保持造成较大影响，进而影响到配准性能，如表4-2 中MSWF 模\n块相较于PatchMerging 使Dice 系数提升2.7%。WaveMorph 的成功可归因于其独特的\n架构设计。MSWF 模块通过Haar 小波变换对输入图像进行无损多尺度分解，分离出\n八个频率的子图，其中包含低频全局结构和高频局部细节。结合不同感受野下的\nConvNeXt 的层级特征提取能力，实现了编码器多层级频域与空间域信息的互补增强，\n有效减少传统U-Net 下采样过程中的信息丢失。低频子图采用大卷积核（7×7×7）捕\n捉全局形变特征，包含高低频子图采用不同大小的卷积核实现局部与全局特征的融合\n增强，而高频子图通过3×3×3 卷积提取边缘细节，这种差异化策略能够提取多粒度\n"}, {"page": 58, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n50\n特征，增强对全局大位移形变和局部精细形变的适应性，同时避免单一卷积核导致的\n特征偏向性和单一信息可能导致的模型脆弱性（即模型泛化性差）。\n基于深度学习的单流非刚性医学图像配准方法，其最终变形场的生成主要依赖于\n解码器通过上采样层和卷积逐步恢复特征图的空间分辨率，将编码器压缩后的低维特\n征映射到与输入图像相同尺寸的变形场。其中，上采样层是否能够精确还原医学图像\n中复杂的解剖结构，很大程度上影响了最终图像对非线性空间对齐的精确度。现有的\n方法在上采样层中采用具有固定采样策略的上采样方法，过去常采用最近邻插值方法，\n通过将距离所映射位置最近的输入像素的灰度值作为待采样点的像素值，最近在三维\n图像领域，现有方法采用三线性插值来改善最近邻插值的块效应（即，锯齿或马赛克\n现象）和空间连续性差的问题，其思想是通过三个轴方向依次进行线性插值，结合8\n个最近邻体素的值加权计算目标体素的值，但三线性插值的采用策略仅基于局部邻域\n的线性插值，无法恢复原图中因分辨率不足而丢失的细节，仅能相对于最邻近插值平\n滑现有数据。图像超分辨率旨在通过对低分辨率图像进行处理，恢复出更多的高频细\n节，以提升图像的视觉质量（例如，重建高分辨的MRI 能够清晰的显示肿瘤的边界及\n与周围组织的关系），这与医学图像精确配准的需求相匹配。轻量化的Dysample 上采\n样模块通过动态偏移量自适应采样位置以满足不同解剖区域的需求，在网络训练过程\n中与损失函数协同优化，从而在恢复分辨率的同时，更好的保留医学图像的细节信息\n和解剖结构特征，进一步减少块效应，改善三线性插值可能造成的高频细节模糊问题\n（尤其在复杂结构，如脑部MRI 中的灰质/白质边界）。如表4-2 中Dysample 的引入\n使Dice 系数最邻近差值和三线性插值分别提升2.1%和1.5%，说明其对图像细节特征\n的增强在配准精度提升中起到了关键作用。\n4.3.5.2 训练收敛与推理速度\n相较于动辄40M 以上模型可学习参数量的Transformer 类模型，由ConvNeXt 构\n建的网络能够以小于1M 的可学习参数量规模获得更优的配准精度，同时得益于高效\n的深度卷积计算带来的低计算开销使其具有更快速的推理速度，更适合于临床环境中\n部署的低计算负载和低延迟的需求。\n在模型训练期间，与其他基于深度学习的方法相比，所提出的WaveMorph 在50\n个epochs 内Dice 分数实现了接近峰值性能，而TransMorph 需要在接近250 个epochs\n"}, {"page": 59, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n51\n才能接近峰值性能，表明信息在整个网络中的无损流通使它比竞争模型更快的学习图\n像对之间的空间对应关系。这意味着在深度学习模型的两个阶段训练曲线[71]中，\nWaveMorph 具有更短的“瞬态”阶段，即更快的识别了局部最小值的邻域，而在后续\n的训练周期中网络进入“最小化”阶段，在该邻域内寻找局部最小值。快速收敛的特\n性意味着可以节省训练时长，而且节省计算资源和成本。值得注意的是，WaveMorph\n使用了更现代化的纯卷积结构，使其在微小的计算量和参数量的提升条件下，整个训\n练周期始终优于其他基于标准卷积的模型。这意味着ConvNeXt 结构比标准卷积更有\n效，从而显著提升了WaveMorph 的配准性能。\n表4-4 比较了现有的传统方法的推理时间和基于深度学习的基线方法的训练时间\n（min/epoch）和推理时间(sec/image)。所有的方法都是在IXI 数据集中使用相同的训\n练集和测试集在GPU 上运行（部分传统方法基于CPU 运行）。最耗时和第二耗时的\n训练方法是基于GAN 的CycleMorph 和基于Transformer 的TransMorph，分别需要大\n约8 天和3 天的训练时间。CycleMorph 虽然网络架构由标准卷积构成，但由于周期一\n致性训练需要在一个训练周期中同时训练多个网络导致了大量的耗时。TransMorph 的\n耗时是因为其参数量约为卷积模型的70 倍，Adam 优化器的显存占用约为参数量的2\n倍（优化器额外显存消耗约为卷积模型的140 倍），巨大的可学习参数量和显存压力\n显著拖慢了训练速度。所提出的WaveMorph 具有卷积极低的参数量优势结合内核优化\n技术具有最快的周期训练速度和仅有0.072s 的卓越的推理速度，相较于现有基线方法\n中推理速度最块的ViT-V-Net 方法提升2.7 倍，相较于配准精度最优的TransMorph 方\n法的推理速度提升4.6 倍。在实际手术导航流程中，以神经外科手术为例，医生在操\n作过程中需要实时了解手术器械与周围神经、血管等重要组织的位置关系，实时性是\n保障手术精准与安全的关键要素。目前深度学习模型在GPU 加速下需要0.2-0.5s 仍存\n在显著延迟，WaveMorph 将配准延迟控制在0.1s 以内，能够确保导航图像与患者解剖\n结构的实时同步，避免视觉与运动的失调，更好的满足实时交互需求。\n"}, {"page": 60, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n52\n表4-4 不同配准方法的平均训练和推理时间比较\nTable 4-4 Comparison of the various registration methods\non the average training and inference time\n模型\n训练速度\n（min/epooch）\n推理速度\n（s/image）\nSyN\n-\n192.140\nNiftyReg\n-\n30.723\nLDDMM\n-\n66.829\nVoxelMorph\n4.93\n0.430\nCycleMorph\n21.99\n0.281\nViT-V-Net\n4.83\n0.197\nTransMorph\n7.56\n0.329\nWaveMorph\n3.88\n0.072\n4.4 本章小结\n在本文中，本文介绍了WaveMorph，一种用于无监督可变形图像配准的纯卷积模\n型。WaveMorph 是融合了小波变换和ConvNeXt 结构，通过频域与空域的协同优化显\n著提升了特征的表征能力，动态上采样技术引入进一步改善了高频细节建模不足的问\n题，相较于Transformer，在计算和模型复杂度方面表现出显著的优势与效率，这使得\nWaveMorph 成为解决实际医疗环境中计算资源限制和临床实时应用的有力候选者。本\n文评估了WaveMorph 在患者间配准和图谱到患者间脑MRI 配准任务中的表现，结果\n表明，WaveMorph 相比多种传统方法和基于学习的方法实现了更高的配准精度，证明\n了其在可变形医学图像配准中的有效性。\n本文的工作存在一些局限性。首先，由于训练基线方法的时间和可用GPU 资源有\n限，本文通过经验或基线方法建议的超参数值来进行设定，而未使用广泛的网格搜索\n进行优化。其次，本研究的小波变换目前基于固定的小波基（Haar），无法通过梯度\n下降动态学习最优变换。此外，本研究建议将小波分解后的多尺度子带图根据其频带\n所携带的信息采取差异化特征提取策略，MWFS 虽然在两项基准测试中表现良好，但\n未充分考虑不同频带信息贡献度的动态变化。\n此外，鉴于本研究引入了一种通用的纯卷积单流图像配准网络结构，本文更专注\n于在结构层面上的创新使纯卷积的网络在医学图像配准领域重新具有竞争性能，而非\n"}, {"page": 61, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n53\n深入优化损失函数或采用复杂的训练方法来选择最优配置。所提出的网络结构可以轻\n松结合多尺度、周期一致性(GAN)等方法进行训练调整，并且能够与任何配准损失函\n数一起使用。\n在未来的研究中，本文将重点探索：其一，进一步评估WaveMorph 对其他器官（例\n如肺部，心脏，腹部等）的配准效果以验证通用性;其二，利用图像生成策略增强训练\n数据或替代损失函数(例如互信息)来扩展所提出方法在多模态配准任务中的潜力；其\n三，设计可学习的小波基函数，使其能自动适应数据分布以提升表征能力，实现频域\n表征与空间域形变的协同优化;其四，进一步轻量化改进模型并尝试部署在实际临床环\n境中的低功耗医疗设备（如移动端PACS 系统、边缘设备）。\n"}, {"page": 62, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n54\n第五章结论与展望\n5.1 结论\n医学图像配准作为医学影像分析的核心任务，旨在实现不同模态或不同时间点图\n像的空间对齐，对疾病诊断、手术规划等临床应用至关重要。然而，现有基于深度学\n习的配准方法在全局依赖建模、计算效率与细节保留等方面仍面临显著挑战。针对这\n些挑战，本文围绕医学图像配准的轻量化、高效性和高精度需求，提出了两种创新性\n的网络架构——HybridMorph 和WaveMorph，分别从轻量化全局依赖建模和多尺度特\n征融合的角度突破现有技术瓶颈，并在理论与应用层面取得重要进展。HybridMorph 与\nWaveMorph 的成功表明，结合不同技术范式的优势（如状态空间模型的全局建模能力、\n小波变换的多尺度分解特性）是突破现有瓶颈的有效途径。未来研究将进一步探索跨\n模态配准、微分同胚形变及更高效的无监督学习策略，推动医学图像配准技术在复杂\n临床场景中的广泛应用。\n5.2 创新点\n本文主要创新点总结如下：\n（1）本文探索了SSD 架构在医学图像配准的潜在应用，提出了一种基于卷积和\nSSD 架构融合的新型轻量化神经网络HybridMorph，解决了Transformer 的二次计算开\n销问题，能够更好的平衡计算复杂度与性能。本文提出了一个残差混合模块(RHM)，\nRHM 基于卷积和Mamba-2 重构了适用于医学图像配准任务的特征提取模块，提升对\n局部信息和全局信息的整合特征提取能力。为解决实际医疗环境中计算资源限制带来\n的挑战，本文提出了一种新型轻量化的方法并行通道特征聚合模块（PCFA），为轻量\n化优化策略提供了更多的选择。\n（2）本文提出了一种基于融合小波变换和ConvNeXt 结构的新型神经网络\nWaveMorph，用于无监督可变形医学图像配准，WaveMorph 提供了频域-空域协同优化\n的新范式。本文提出了一个多尺度小波特征融合（MSWF）模块，以实现高效的无损\n下采样。此外，在解码器端引入了一个轻量级的动态上采样模块，以增强高频细节的\n"}, {"page": 63, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n55\n重建效果。WaveMorph 将卷积神经网络（CNN）的归纳偏差与Transformer 的优势相\n结合，有效地缓解了传统采样操作中由于空间信息丢失而导致的拓扑失真问题，并支\n持实时推理。\n（3）本文在患者间配准和图谱到患者脑MRI 配准数据集(包含1000 多个用于测\n试和训练的脑MRI 图像)上广泛验证了所提出的配准模型。相比传统方法及深度学习\n基线方法，所提出的两个模型均展示了最先进的性能。\n5.3 展望\n本文主要基于脑部MRI 图像对常见医学图像配准任务进行了研究，有效解决了当\n前基于深度学习的医学图像配准方法在资源受限的医疗环境中训练和推理问题，并进\n一步推进了无监督学习条件下的医学图像配准性能和推理速度。鉴于本研究更专注于\n通用的轻量化图像配准网络结构，因此在多模态配准、多器官配准、微分同胚等方面\n仍存在一些局限性，后续的工作展望如下：\n（1）由于训练基线方法的时间和可用GPU 资源有限，本文通过经验或基线方法\n建议的超参数值来进行设定，未来将使用广泛的网格搜索进行优化以寻找最优超参数\n组合。\n（2）利用图像生成策略增强训练数据或深入优化损失函数来扩展所提出方法在多\n模态配准任务中的潜力；选择合适的正则化器（例如逆一致性），使所提出的方法获\n得更平滑且通常微分同胚的变换。\n（3）进一步评估所提出的方法在其他器官（例如肺、肝脏、心脏）上的配准效果\n以验证其在多器官上的模型泛化性，并验证其对于噪点、伪影或不同扫描仪源图像的\n模型鲁棒性。\n（4）对于WaveMorph 模型，设计可学习的小波基函数，使其能自动适应数据分\n布以提升表征能力，实现频域表征与空间域形变的协同优化;\n（5）所提出的网络结构可以轻松结合多尺度、周期一致性（GAN）等方法进行\n训练调整，并且能够与任何配准损失函数一起使用。未来将结合训练方法进一步轻量\n化改进模型并尝试部署在实际临床环境中的低功耗医疗设备（如移动端PACS 系统、\n边缘设备）。\n"}, {"page": 64, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n56\n参考文献\n[1]\nBai W, Suzuki H, Huang J, et al. A population-based phenome-wide association\nstudy of cardiac and aortic structure and function[J], Nature Medicine, 2020, 26(10):\n1654.\n[2]\nQin C, Wang S, Chen C, et al. Generative myocardial motion tracking via latent\nspace exploration with biomechanics-informed prior[J], Medical Image Analysis,\n2023, 83: 102682.\n[3]\nBrock K K, Mutic S, McNutt T R, et al. Use of image registration and fusion\nalgorithms and techniques in radiotherapy: Report of the AAPM Radiation Therapy\nCommittee Task Group No. 132[J], Medical Physics, 2017, 44(7): e43-e76.\n[4]\nOh S, Kim S, Deformable image registration in radiation therapy[J], Radiation\nOncology Journal, 2017, 35(2): 101.\n[5]\nJena R, Sethi D, Chaudhari P, et al. Deep learning in medical image registration:\nMagic or mirage?[J], Advances in Neural Information Processing Systems, 2024, 37:\n108331-108353.\n[6]\nHaskins G, Kruger U, Yan P, Deep learning in medical image registration: a\nsurvey[J], Machine Vision and Applications, 2020, 31(1): 8.\n[7]\nKlein A, Andersson J, Ardekani B A, et al. Evaluation of 14 nonlinear deformation\nalgorithms applied to human brain MRI registration[J], Neuroimage, 2009, 46(3):\n786-802.\n[8]\nAvants B B, Epstein C L, Grossman M, et al. Symmetric diffeomorphic image\nregistration with cross-correlation: evaluating automated labeling of elderly and\nneurodegenerative brain[J], Medical Image Analysis, 2008, 12(1): 26-41.\n[9]\nVercauteren T, Pennec X, Perchant A, et al. Diffeomorphic demons: Efficient\nnon-parametric image registration[J], NeuroImage, 2009, 45(1): S61-S72.\n"}, {"page": 65, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n57\n[10]\nModat M, Ridgway G R, Taylor Z A, et al. Fast free-form deformation using graphics\nprocessing units[J], Computer Methods and Programs in Biomedicine, 2010, 98(3):\n278-284.\n[11]\nBajcsy R, Kovačič S, Multiresolution elastic matching[J], Computer Vision, Graphics,\nand Image Processing, 1989, 46(1): 1-21.\n[12]\nGlocker B, Komodakis N, Tziritas G, et al. Dense image registration through MRFs\nand efficient linear programming[J], Medical Image Analysis, 2008, 12(6): 731-741.\n[13]\nModat M, McClelland J, Ourselin S, Lung registration using the NiftyReg package[J],\nMedical Image Analysis for the Clinic-a Grand Challenge, 2010, 2010: 33-42.\n[14]\nBeg M F, Miller M I, Trouvé A, et al. Computing large deformation metric mappings\nvia geodesic flows of diffeomorphisms[J], International Journal of Computer Vision,\n2005, 61: 139-157.\n[15]\nAshburner J, A fast diffeomorphic image registration algorithm[J], Neuroimage, 2007,\n38(1): 95-113.\n[16]\nDalca A V, Bobu A, Rost N S, et al. Patch-based discrete registration of clinical brain\nimages, Springer, 201660-67.\n[17]\nDavatzikos C, Spatial transformation and registration of brain images using\nelastically deformable models[J], Computer Vision and Image Understanding, 1997,\n66(2): 207-222.\n[18]\nRueckert D, Sonoda L I, Hayes C, et al. Nonrigid registration using free-form\ndeformations: application to breast MR images[J], IEEE Transactions on Medical\nImaging, 1999, 18(8): 712-721.\n[19]\nCao Y, Miller M I, Winslow R L, et al. Large deformation diffeomorphic metric\nmapping of vector fields[J], IEEE Transactions on Medical Imaging, 2005, 24(9):\n1216-1230.\n[20]\nHernandez M, Bossa M N, Olmos S, Registration of anatomical images using paths\nof diffeomorphisms parameterized with stationary vector field flows[J], International\nJournal of Computer Vision, 2009, 85: 291-306.\n"}, {"page": 66, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n58\n[21]\nOishi K, Faria A, Jiang H, et al. Atlas-based whole brain white matter analysis using\nlarge deformation diffeomorphic metric mapping: application to normal elderly and\nAlzheimer's disease participants[J], Neuroimage, 2009, 46(2): 486-499.\n[22]\nCao X, Yang J, Zhang J, et al. Deformable image registration based on\nsimilarity-steered CNN regression[C]//Medical Image Computing and Computer\nAssisted Intervention−MICCAI 2017: 20th International Conference, Quebec City,\nQC, Canada, September 11-13, 2017, Proceedings, Part I 20. Springer International\nPublishing, 2017: 300-308.\n[23]\nRohé M M, Datar M, Heimann T, et al. SVF-Net: learning deformable image\nregistration using shape matching[C]//Medical Image Computing and Computer\nAssisted Intervention−MICCAI 2017: 20th International Conference, Quebec City,\nQC, Canada, September 11-13, 2017, Proceedings, Part I 20. Springer International\nPublishing, 2017: 266-274.\n[24]\nSokooti H, De Vos B, Berendsen F, et al. Nonrigid image registration using\nmulti-scale 3D convolutional neural networks[C]//Medical Image Computing and\nComputer Assisted Intervention −\nMICCAI 2017: 20th International Conference,\nQuebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20. Springer\nInternational Publishing, 2017: 232-239.\n[25]\nYang X, Kwitt R, Styner M, et al. Quicksilver: Fast predictive image registration–a\ndeep learning approach[J], NeuroImage, 2017, 158: 378-396.\n[26]\nDe Vos B D, Berendsen F F, Viergever M A, et al. End-to-end unsupervised\ndeformable image registration with a convolutional neural network[C]//International\nWorkshop on Deep\nLearning in Medical\nImage Analysis. Cham: Springer\nInternational Publishing, 2017: 204-212.\n[27]\nBalakrishnan G, Zhao A, Sabuncu M R, et al. Voxelmorph: a learning framework for\ndeformable medical image registration[J], IEEE Transactions on Medical Imaging,\n2019, 38(8): 1788-1800.\n[28]\nKim B, Kim D H, Park S H, et al. CycleMorph: cycle consistent unsupervised\n"}, {"page": 67, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n59\ndeformable image registration[J], Medical Image Analysis, 2021, 71: 102036.\n[29]\nDe Vos B D, Berendsen F F, Viergever M A, et al. A deep learning framework for\nunsupervised affine and deformable image registration[J], Medical Image Analysis,\n2019, 52: 128-143.\n[30]\nBalakrishnan G, Zhao A, Sabuncu M R, et al. An unsupervised learning model for\ndeformable medical image registration[C]//Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. 2018: 9252-9260.\n[31]\nChen J, He Y, Frey E C, et al. Vit-v-net: Vision transformer for unsupervised\nvolumetric medical image registration[J], arXiv preprint arXiv:2104.06468, 2021.\n[32]\nChen J, Frey E C, He Y, et al. Transmorph: Transformer for unsupervised medical\nimage registration[J], Medical Image Analysis, 2022, 82: 102615.\n[33]\nShi J, He Y, Kong Y, et al. Xmorpher: Full transformer for deformable medical\nimage registration via cross attention[C]//International Conference on Medical Image\nComputing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland,\n2022: 217-226.\n[34]\nEppenhof K A, Lafarge M W, Veta M, et al. Progressively trained convolutional\nneural networks for deformable image registration[J], IEEE Transactions on Medical\nImaging, 2019, 39(5): 1594-1604.\n[35]\nMok T C W, Chung A C S. Large deformation diffeomorphic image registration with\nlaplacian pyramid networks[C]//Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\n– 8, 2020, Proceedings, Part III 23. Springer International Publishing, 2020:\n211-221.\n[36]\nJaderberg M, Simonyan K, Zisserman A, Spatial transformer networks[J], Advances\nin Neural Information Processing Systems, 2015, 28.\n[37]\nGoodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[J],\nAdvances in Neural Information Processing Systems, 2014, 27.\n[38]\nLiu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using\n"}, {"page": 68, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n60\nshifted windows[C]//Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 2021: 10012-10022.\n[39]\nLuo W, Li Y, Urtasun R, et al. Understanding the effective receptive field in deep\nconvolutional neural networks[J], Advances in Neural Information Processing\nSystems, 2016, 29.\n[40]\n高威威.深度卷积神经网络的有效感受野研究[D].哈尔滨工业大学,2021.\n[41]\nVaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J], Advances in\nNeural Information Processing Systems, 2017, 30.\n[42]\nJain\nS,\nWallace\nB\nC,\nAttention\nis\nnot\nexplanation[J],\narXiv\npreprint\narXiv:1902.10186, 2019.\n[43]\nViola P, Wells III W M, Alignment by maximization of mutual information[J],\nInternational Journal of Computer Vision, 1997, 24(2): 137-154.\n[44]\nSengupta D, Gupta P, Biswas A, A survey on mutual information based medical\nimage registration algorithms[J], Neurocomputing, 2022, 486: 174-188.\n[45]\nHeinrich M P, Jenkinson M, Bhushan M, et al. MIND: Modality independent\nneighbourhood descriptor for multi-modal deformable registration[J], Medical Image\nAnalysis, 2012, 16(7): 1423-1435.\n[46]\nJohnson H J, Christensen G E, Consistent landmark and intensity-based image\nregistration[J], IEEE Transactions on Medical Imaging, 2002, 21(5): 450-461.\n[47]\nVishnevskiy V, Gass T, Szekely G, et al. Isotropic total variation regularization of\ndisplacements in parametric image registration[J], IEEE Transactions on Medical\nImaging, 2016, 36(2): 385-395.\n[48]\nOliveira F P, Tavares J M R, Medical image registration: a review[J], Computer\nMethods in Biomechanics and Biomedical Engineering, 2014, 17(2): 73-93.\n[49]\nBharati S, Mondal M, Podder P, et al. Deep learning for medical image registration:\nA comprehensive review[J], arXiv preprint arXiv:2204.11341, 2022.\n[50]\nGoodfellow I, Bengio Y, Courville A, et al. Deep learning[M]. Cambridge: MIT\npress, 2016.\n"}, {"page": 69, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n61\n[51]\n周飞燕, 金林鹏, 董军, 卷积神经网络研究综述[J], 计算机学报, 2017, 40(06),\n1229-1251.\n[52]\nFukushima K. Neocognitron: A self-organizing neural network model for a\nmechanism of pattern recognition unaffected by shift in position[J]. Biological\nCybernetics, 1980, 36(4): 193-202.\n[53]\nKrizhevsky A, Sutskever I, Hinton G E, Imagenet classification with deep\nconvolutional neural networks[J], Advances in Neural Information Processing\nSystems, 2012, 25.\n[54]\nBieder F, Sandkühler R, Cattin P C, Comparison of methods generalizing max-and\naverage-pooling[J], arXiv preprint arXiv:2103.01746, 2021.\n[55]\nRonneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical\nimage\nsegmentation[C]//Medical\nimage\ncomputing\nand\ncomputer-assisted\nintervention – MICCAI 2015: 18th international conference, Munich, Germany,\nOctober 5-9, 2015, proceedings, part III 18. Springer international publishing, 2015:\n234-241.\n[56]\nIoffe S, Szegedy C. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift[C]//International conference on machine learning.\npmlr, 2015: 448-456.\n[57]\nNair\nV,\nHinton\nG\nE.\nRectified\nlinear\nunits\nimprove\nrestricted\nboltzmann\nmachines[C]//Proceedings of the 27th international conference on machine learning\n(ICML-10). 2010: 807-814.\n[58]\nLiu Z, Mao H, Wu C Y, et al. A convnet for the 2020s[C]//Proceedings of the\nIEEE/CVF\nconference\non\ncomputer\nvision\nand\npattern\nrecognition.\n2022:\n11976-11986.\n[59]\nHe\nK,\nZhang\nX,\nRen\nS,\net\nal.\nDeep\nresidual\nlearning\nfor\nimage\nrecognition[C]//Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 2016: 770-778.\n[60]\nHendrycks D, Gimpel K, Gaussian error linear units (gelus)[J], arXiv preprint\n"}, {"page": 70, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n62\narXiv:1606.08415, 2016.\n[61]\nDosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale[J], arXiv preprint arXiv:2010.11929,\n2020.\n[62]\nChollet\nF.\nXception:\nDeep\nlearning\nwith\ndepthwise\nseparable\nconvolutions[C]//Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 2017: 1251-1258.\n[63]\nSandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted residuals and linear\nbottlenecks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. 2018: 4510-4520.\n[64]\nGu A, Goel K, Ré C. Efficiently modeling long sequences with structured state\nspaces[J]. arXiv preprint arXiv:2111.00396, 2021.\n[65]\nGu A, Johnson I, Goel K, et al. Combining recurrent, convolutional. and\ncontinuous-time models with linear state space layers[J], Advances in Neural\nInformation Processing Systems, 2021, 34: 572-585.\n[66]\nKalman R E, A new approach to linear filtering and prediction problems[J], 1960.\n[67]\nRumelhart D E, Hinton G E, Williams R J, Learning representations by\nback-propagating errors[J], Nature, 1986, 323(6088): 533-536.\n[68]\nGu A, Dao T, Mamba: Linear-time sequence modeling with selective state spaces[J],\narXiv preprint arXiv:2312.00752, 2023.\n[69]\nDao T, Gu A, Transformers are ssms: Generalized models and efficient algorithms\nthrough structured state space duality[J], arXiv preprint arXiv:2405.21060, 2024.\n[70]\nStollnitz E J, DeRose A D, Salesin D H, Wavelets for computer graphics: a primer.\n1[J], IEEE Computer Graphics and Applications, 1995, 15(3): 76-84.\n[71]\nXu G, Liao W, Zhang X, et al. Haar wavelet downsampling: A simple but effective\ndownsampling module for semantic segmentation[J], Pattern Recognition, 2023, 143:\n109819.\n[72]\nFujieda S, Takayama K, Hachisuka T, Wavelet convolutional neural networks[J],\n"}, {"page": 71, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n63\narXiv preprint arXiv:1805.08620, 2018.\n[73]\nLuo C, Li Y, Lin K, et al. Wavelet synthesis net for disparity estimation to synthesize\ndslr calibre bokeh effect on smartphones[C]//Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 2020: 2407-2415.\n[74]\nZafar A, Aamir M, Mohd Nawi N, et al. A comparison of pooling methods for\nconvolutional neural networks[J], Applied Sciences, 2022, 12(17): 8643.\n[75]\nLin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object\ndetection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. 2017: 2117-2125.\n[76]\nDice L R. Measures of the amount of ecologic association between species[J].\nEcology, 1945, 26(3): 297-302.\n[77]\nChen J, Lu Y, Yu Q, et al. Transunet: Transformers make strong encoders for\nmedical image segmentation[J], arXiv preprint arXiv:2102.04306, 2021.\n[78]\nHatamizadeh A, Nath V, Tang Y, et al. Swin unetr: Swin transformers for semantic\nsegmentation of brain tumors in mri images[C]//International MICCAI brainlesion\nworkshop. Cham: Springer International Publishing, 2021: 272-284.\n[79]\nMisra D, Nalamada T, Arasanipalai A U, et al. Rotate to attend: Convolutional triplet\nattention\nmodule[C]//Proceedings\nof\nthe\nIEEE/CVF\nWinter\nConference\non\nApplications of Computer Vision. 2021: 3139-3148.\n[80]\nWoo S, Debnath S, Hu R, et al. Convnext v2: Co-designing and scaling convnets\nwith masked autoencoders[C]//Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2023: 16133-16142.\n[81]\nLei Ba J, Kiros J R, Hinton G E, Layer normalization[J], ArXiv e-prints, 2016: 1607.\n[82]\nXie S, Girshick R, Dollár P, et al. Aggregated residual transformations for deep\nneural networks[C]//Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 2017: 1492-1500.\n[83]\nTang Z, Gao Y, Zhu Y, et al. Crossnorm and selfnorm for generalization under\ndistribution shifts[C]//Proceedings of the IEEE/CVF International Conference on\n"}, {"page": 72, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n64\nComputer Vision. 2021: 52-61.\n[84]\nMa N, Zhang X, Zheng H T, et al. Shufflenet v2: Practical guidelines for efficient cnn\narchitecture design[C]//Proceedings of the European Conference on Computer Vision\n(ECCV). 2018: 116-131.\n[85]\nFischl B, FreeSurfer[J], Neuroimage, 2012, 62(2): 774-781.\n[86]\nFadnavis S, Image interpolation techniques in digital image processing: an\noverview[J], International Journal of Engineering Research and Applications, 2014,\n4(10): 70-73.\n[87]\nDumoulin V, Visin F, A guide to convolution arithmetic for deep learning[J], arXiv\npreprint arXiv:1603.07285, 2016.\n[88]\nFinder S E, Amoyal R, Treister E, et al. Wavelet convolutions for large receptive\nfields[C]//European Conference on Computer Vision. Cham: Springer Nature\nSwitzerland, 2024: 363-380.\n[89]\nLiu\nP,\nZhang\nH,\nZhang\nK,\net\nal.\nMulti-level\nwavelet-CNN\nfor\nimage\nrestoration[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops. 2018: 773-782.\n[90]\nSong J, He J, Feng M, et al. High frequency matters: Uncertainty guided image\ncompression with wavelet diffusion[J], arXiv preprint arXiv:2407.12538, 2024.\n[91]\nChen Y, Fan H, Xu B, et al. Drop an octave: Reducing spatial redundancy in\nconvolutional neural networks with octave convolution[C]//Proceedings of the\nIEEE/CVF International Conference on Computer Vision. 2019: 3435-3444.\n[92]\nLiu\nW,\nLu\nH,\nFu\nH,\net\nal.\nLearning\nto\nupsample\nby\nlearning\nto\nsample[C]//Proceedings of the IEEE/CVF International Conference on Computer\nVision. 2023: 6027-6037.\n[93]\nIm D J, Tao M, Branson K, An empirical analysis of deep network loss surfaces[J],\n2016.\n[94]\nSutskever I, Martens J, Dahl G, et al. On the importance of momentum and\ninitialization in deep learning[C]//30th International Conference on Machine\n"}, {"page": 73, "text": "广西大学硕士专业学位论文\n基于深度学习的无监督医学图像配准算法研究\n65\nLearning. 2013: 404-439.\n"}]}