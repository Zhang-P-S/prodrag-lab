{"doc_id": "arxiv:2512.03634", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.03634.pdf", "meta": {"doc_id": "arxiv:2512.03634", "source": "arxiv", "arxiv_id": "2512.03634", "title": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment", "authors": ["Ahmad Aghaebrahimian"], "published": "2025-12-03T10:14:31Z", "updated": "2025-12-03T10:14:31Z", "summary": "Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.03634v1", "url_pdf": "https://arxiv.org/pdf/2512.03634.pdf", "meta_path": "data/raw/arxiv/meta/2512.03634.json", "sha256": "b2c25c467603f37338eb6bfaa6f3764b62a9f175a985c4fcf8643230473e8dc3", "status": "ok", "fetched_at": "2026-02-18T02:25:37.484153+00:00"}, "pages": [{"page": 1, "text": "AlignCheck: a Semantic Open-Domain Metric for Factual Consistency\nAssessment\nAhmad Aghaebrahimian\nInstitute of Computational Life Sciences,\nDepartment of Life Sciences and Facility Management,\nZurich University of Applied Sciences, 8820 Waedenswil, Switzerland\nSwiss Institute of Bioinformatics, 1015 Lausanne, Switzerland\nahmad.aghaebrahimian@zhaw.ch\nAbstract\nLarge Language Models have significantly ad-\nvanced natural language processing tasks, but\nremain prone to generating incorrect or mis-\nleading but plausible arguments. This issue,\nknown as hallucination, is particularly concern-\ning in high-stakes domains like clinical appli-\ncations, where factual inaccuracies can have se-\nvere consequences. Existing evaluation metrics\nfail to adequately assess factual consistency and\nlack interpretability, making diagnosing and\nmitigating errors difficult. We propose an inter-\npretable framework for factual consistency as-\nsessment for in-domain and open-domain texts\nto address these limitations. Our approach de-\ncomposes text into atomic facts and introduces\na flexible, schema-free methodology. Unlike\nprevious methods with an absolute metric, we\nincorporate a weighted metric to enhance fac-\ntual evaluation. Additionally, we propose a\nmechanism to control assessment complexity in\nintricate domains. We benchmark our approach\non popular general and clinical datasets and\nrelease our code to support fact-aware model\ntraining in future research.\n1\nIntroduction\nLarge Language Models (LLMs) have revolution-\nized various natural language generation tasks, in-\ncluding question answering (Deutsch et al., 2021),\ntext summarization (Goyal et al., 2023), and dia-\nlogue systems (Shuster et al., 2021). Despite their\nimpressive capabilities, LLMs are prone to a phe-\nnomenon known as hallucination, where they gen-\nerate incorrect or misleading arguments with high\nconfidence. This issue is particularly critical in\nhigh-stakes domains such as clinical and medical\napplications, where factual inaccuracies can have\nsevere consequences.\nTo mitigate these risks, it is mandatory to val-\nidate the factual consistency of LLM-generated\ncontent. Although numerous methods have been\nproposed to assess factual accuracy (Lee et al.,\n2022; Min et al., 2023; Goodrich et al., 2019),\nmany suffer from key limitations. Specifically, ex-\nisting approaches often lack interpretability, offer-\ning only numerical (Deutsch et al., 2021) or bi-\nnary (Tang et al., 2023) metrics without indicating\nwhere within the generated text errors occur. This\nabsence of granular insight makes it difficult to\ndiagnose and correct inaccuracies effectively. As\nhighlighted by Luo et al. (Luo et al., 2025), current\nmethods for factual consistency checking fall short,\nparticularly for clinical texts. Additionally, current\ntechniques do not provide sufficient flexibility to\naccount for different classes of facts, such as facts\nabout a patient’s demography or health journey,\nlimiting their applicability in diverse real-world\nscenarios. This highlights the need for more robust,\ninterpretable, and adaptable evaluation methods for\nfactual consistency checking that align better with\nhuman judgments and task-specific requirements.\nIn this paper, we propose an interpretable frame-\nwork for factual consistency checking on general\nand clinical texts, showcasing the application of\nthe framework for summarization. Similar to ear-\nlier studies (Goyal et al., 2023), we advocate de-\ncomposing texts into atomic facts for more gran-\nular analysis. However, in contrast to most meth-\nods, which focus on the sentence-level fact assess-\nment (Goyal et al., 2023), we adopt a general ap-\nproach to broaden the scope of consistency check-\ning to the global text, similar to Min et al (Min et al.,\n2023). In contrast to Min et al. (Min et al., 2023),\nthough, our approach is schema-free and does not\ndepend on an external knowledge base or schema.\nTo accommodate schema-free factuality evaluation,\nwe introduce a weighted metric inspired by the F1\nscore, combined with BERTScore (Zhang et al.,\n2020b). Furthermore, we suggest a flexible mecha-\nnism to control the complexity of factual evaluation\nwhen dealing with intricate domains.\narXiv:2512.03634v1  [cs.CL]  3 Dec 2025\n"}, {"page": 2, "text": "We release the code1 and suggest integrating\nthe score into the objective function for training a\nfact-aware model in further studies.\nIn summary, our contribution consists of\n• AlignCheck, a new F1 score formulation for\nfactual consistency, complemented with the\ncodebase,\n• a granular algorithm for different levels of\nfact-checking,\n• and benchmarking the factuality score for pop-\nular factual datasets.\n2\nRelated Work\nEvaluating the quality of synthetic text, such as\nthe output of machine translation or summariza-\ntion algorithms, has always been an issue due to\nthe inherent subjectivity and complexity of lan-\nguage. Popular metrics such as BLEU (Bilingual\nEvaluation Understudy) (Papineni et al., 2002) and\nROUGE (Recall Oriented Understudy for Gisting\nEvaluation) (Lin, 2004) focus on surface-level in-\nformation, such as precision and recall over n-\ngram overlap, thus fail to capture deeper seman-\ntic fluency and coherence in the generated text.\nBERTScore (Zhang et al., 2020b) suggested a se-\nmantic score to address these shortcomings. How-\never, BERTScore and other semantic-based metrics\ndo not account for factual accuracy, meaning highly\ncoherent text can still correspond to factually incon-\nsistent outputs (Li et al., 2024; Zhou et al., 2022).\nAccording to recent studies (Goodrich et al.,\n2019; Falke et al., 2019), up to 30% of synthetic\nsummaries contain factual consistency problems.\nTherefore, improving factual consistency check-\ning has been an active field in recent years (Nori\net al., 2023; Shuster et al., 2021). Several studies\nsuggested model-based factuality evaluation where\na model is trained to evaluate the consistency for\ngenerated text by formulating it as a binary clas-\nsification task (Kryscinski et al., 2020), question\nanswering (Deutsch et al., 2021), or Natural Lan-\nguage Inference (NLI) (Laban et al., 2022) through\ntextual entailment assessment.\nDecomposing text into constituents such as\nentities (Lee et al., 2022) or triples (subject,\npredicate, object) is exercised in several other\nworks (Goodrich et al., 2019; Thorne et al., 2018;\n1https://github.com/Soshaince/AlignCheck\nMin et al., 2023) as a direct and model-free alter-\nnative to validate factual consistency of generated\ntexts. Our approach fits in this category and is sim-\nilar to (Goodrich et al., 2019); however, we do not\nassume a fixed schema for the task. Instead, we\nadopt a flexible approach by relaxing this assump-\ntion, thus broadening the domain of the application.\n3\nMethods and Data\nWe run a basic Named Entity Recognition (NER)\ntagger over the source (i.e., ground truth summary)\nand target (i.e., predicted summary) texts to get the\nsets of source Es and target entities Et. We utilized\ntwo datasets, AgreeFact (Tang et al., 2023) and\nMIMIC-IV-Ext-BHC (Aali et al., 2024). The de-\ntails of the datasets are provided below. For general\ntexts in AgreeFact and clinical texts in MIMIC-IV-\nExt-BHC, we used Spacy and MedCat, respectively.\nThe NER tagger extracts named entities with their\ntypes from both source and target texts. In general\ndomains, these types might be Person, Time, Orga-\nnization, etc, and in the clinical context, they might\nbe Diagnosis, Prognosis, Treatment, etc. We treat\nthe output of NER annotators as a bag of types,\nthus assigning TD-IDF (Term Frequency-Inverse\nDocument Frequency) weights to each type. De-\npending on the granularity of the intended factual\nassessment, we can decide how many top-n types\nT we want to include in the assessment.\nGiven each source text, we define Fs as the set\nof all facts fs = (ss, ps, os) where ss is a sub-\nject named entity ss ∈Es whose type st ∈T\nand which is associated with os, a type-free ob-\nject named entity by the schema-less predicate ps.\nLikewise, we define Ft as the set of factsft =\n(st, pt, ot), st ∈Et extracted from each target text.\nWe also define SOs and SOt as a set of unique\n(ss, os) and (st, ot) accordingly. Given Fs and Ft\nas sets of triples, we can define True Positive(TP)\nas shared triples between Fs and Ft, False Nega-\ntives(FN) as Fs −(Fs ∩Ft) and False Positive(FP)\nas Ft −(Fs ∩Ft).\nFP and FN both negatively impact the factual\nconsistency. While an increase in the number of\nFNs reduces Recall(R) and signals factual incon-\nsistency, an increase in the number of FPs reduces\nPrecision(P) and might signal hallucination.\nThe F1 score, F1 = 2PR\nP+R, as the harmonic mean\nof precision and recall, is a popular metric in many\nexperiments with imbalanced labels. In schema-\nbased systems (Goodrich et al., 2019), it is easy\n"}, {"page": 3, "text": "to treat Fs and Ft as atomic facts with constant\nnamed entities and predicates, thus computing the\nF1 score as described above is straightforward. In\nschema-less systems, though, there might be incon-\nsistency in predicates, thus, a fact with the same\nsemantics, but not matching the predicate in Fs will\nbe considered as FN or FP. To address this issue,\nwe use BERTScore as a means of soft similarity\nestimation. BERTScore computes the similarity\nbetween two strings as the sum of the cosine sim-\nilarities between their token embeddings, thereby\nenabling paraphrase detection.\nAlgorithm 1 Weighted scoring algorithm\nTP ←0\nFP ←0\nFN ←0\nfor fs ∈Fs do\nif (ss, os) /∈SOt then\nFN ←FN + 1\nelse\nfor ft ∈Ft do\nif ss == st then\nif os == ot then\nTP ←TP + BERTScore(ps, pt)\nend if\nend if\nend for\nend if\nend for\nfor ft ∈Ft do\nif (st, ot) /∈SOs then\nFP ←FP + 1\nend if\nend for\nAs indicated in the Algorithm 1, every missing\n(ss, os) in SOt is a full instance of FN. Conversely,\neach lacking instance of (st, ot) in SOs increases\nFP by one full unit. TP is impacted when ss == st\nand os == ot. In this case, TP increments or\ndecrements based on BERTScore(ps, pt), which\nis in the range (−1, 1). This means that as the\ntarget predicate becomes semantically closer to the\nsource predicate, the value of TP increases and\ndecreases as the semantic distance grows.\nTo showcase the application of our proposed\nmetric, we selected two datasets, AgreeFact (Tang\net al., 2023) in general and MIMIC-IV-Ext-\nBHC (Aali et al., 2024) in the clinical domain.\nAgreeFact (Tang et al., 2023) is a benchmark for\nassessing factual consistency as a binary classifica-\ntion. It consists of 9 annotated factuality datasets\nstratified according to the deployed summarization\nmodel. We employ samples of the dataset, consist-\ning of 2353 texts, summarized with one or more\nof four different summarization models, including\nBART (Lewis et al., 2020), Pegasus and Pegasus-\nDynamic (Zhang et al., 2020a), and T5 (Raffel\net al., 2020). The dataset consists of 1726 unique\nsamples. Since we intend to compare the factuality\nof different models, we ignored 1313 samples that\nwere summarized using only one model.\nMIMIC-IV-Ext-BHC (Aali et al., 2024) is a\ndataset of Brief Hospital Course (BHC) summaries.\nBHCs are clinical documents that summarize a\npatient’s hospital stay.\nMIMIC-IV-Ext-BHC is\nextracted from MIMIC-IV-Note (Johnson et al.,\n2023), which is a raw collection of 331,794 de-\nidentified discharge summaries from 145,915 pa-\ntients admitted to the Beth Israel Deaconess Medi-\ncal Center. To demonstrate the application of our\nproposed metric, we sampled 10000 BHCs for fine-\ntuning and 300 BHCs for testing. We finetuned an\ninstance of Llama 3-13B with different strategies\nas described below.\nThe LM_ep2 strategy involves post-tuning the\nLLM on 10,000 BHCs plain texts for two epochs,\nwhile LM_ep5 extends this process to five epochs.\nIn contrast, the Instruction_ep2 strategy uses\ninstruct-tuning, where the LLM is trained on\n10,000 BHCs along with their summaries for two\nepochs, and Instruction_ep5 increases the number\nof epochs for this instruct-tuning process to five.\nThese strategies highlight different approaches to\nfine-tuning, varying both in training data usage and\nthe number of epochs.\nEmploying each model, we summarized the 300\ntest samples using the fine-tuned models.\n4\nResults\nWe employed the AlignCheck to estimate the fac-\ntual overlap between each summarization model’s\nprediction and the ground-truth summary text. Ta-\nble 2 summarizes the number of times each model\nperforms the best with respect to the others on the\nAgreeFact and MIMIC-IV-Ext-BHC datasets.\nThen we used the Friedman test to check if there\nis a difference between models characterized by\nthe AlignCheck score. The Friedman test is a non-\nparametric statistical test used to detect differences\nin performance across more than 3 models evalu-\nated on multiple data points, where each sample has\na score for each model, and there is no normality\nassumption. The p-values p = 0.000 estimated by\nthe Friedman test for both datasets demonstrate that\nthe models are statistically significantly different.\nThe Friedman test tells if there’s a difference, but\n"}, {"page": 4, "text": "Models\nBART\nPegasus\nPegasusDynamic\nT5\nBART\n1.0000\n1.110223e-16\n0.0000\n0.0000\nPegasus\n1.110223e-16\n1.0000\n0.0000\n1.566292e-11\nPegasusDynamic\n0.0000\n0.0000\n1.0000\n0.0000\nT5\n0.0000\n1.566292e-11\n0.0000\n1.0000\nModels\ninstruction_ep2\ninstruction_ep5\nlm_ep2\nlm_ep5\ninstruction_ep2\n1.0\n8.728525e-01\n9.882236e-06\n0.001026\ninstruction_ep5\n0.872852\n1.0\n1.712586e-07\n0.000039\nlm_ep2\n0.000010\n1.712586e-07\n1.0\n0.723773\nlm_ep5\n0.001026\n3.905937e-05\n7.237732e-01\n1.0\nTable 1: First table: Friedman posthoc test on AgreeFact (Tang et al., 2023). Second table: Friedman post hoc test\non MIMIC-IV-Ext-BHC (Aali et al., 2024). p-values lower than 0.05 show a statistically significant difference.\nModel (AgreeFact)\nFirst rank\nBART\n93.7%\nPegasus\n3.6%\nPegasusDynamic\n2.3%\nT5\n0.4%\nModel (MIMIC-IV-Ext-BHC)\nFirst rank\ninstruction_ep2\n23.7%\ninstruction_ep5\n26.3%\nlm_ep2\n37.4%\nlm_ep5\n12.6%\nTable 2: Top ranks. Number of times each model gets\nthe highest score on each dataset\ndoes not say which models differ. After the Fried-\nman test, a follow-up with post-hoc tests reported\nin Table 1 shows which models are statistically sig-\nnificantly different as expected by their number of\nparameters and training protocols.\n5\nConclusion and Future Work\nIn high-stakes domains such as clinical contexts,\npreserving critical facts, particularly of certain\ntypes, is essential when doing text processing tasks\nsuch as summarization. In this study, we intro-\nduced a novel schema-free methodology and scor-\ning algorithm for assessing factual consistency in\nboth in-domain and open-domain contexts. Our\napproach involves decomposing source and target\ntexts into atomic facts and softly quantifying the\ndegree of factual overlap between them. To refine\nthis algorithm, we introduced a granular mecha-\nnism based on TF-IDF weights that adjusts the\nlevel of fact extraction based on the involved en-\ntity types. In addition to the score, which shows\nthe semantic overall based on tangible facts, the\nconstituents of the score, including FP and FN, pro-\nvide interpretable insights of where the model went\nwrong by fabricating new facts enumerated in FP\nor ignoring necessary facts enumerated in FN.\nThis work represents a step toward enabling gen-\nerative models to produce more fact-aware outputs.\nAs a next step, we aim to integrate our scoring\nmethod into training pipelines, allowing models\nto better adhere to factual content. One potential\ndirection is to incorporate the score as a soft con-\nstraint within the objective function. Alternatively,\nwe may leverage TNs and FPs as negative samples,\nand TPs as positive samples, and use them to train\na contrastive learning algorithm. We plan to further\nexplore these directions in future work.\nLimitations\nWhile our proposed methodology provides a novel\nframework for assessing factual consistency, sev-\neral limitations should be acknowledged. First,\nour evaluation primarily focused on controlled\ndatasets, and the robustness of the scoring algo-\nrithm in highly noisy, real-world clinical or open-\ndomain scenarios remains to be validated. Second,\nalthough the TF-IDF–based weighting mechanism\nallows for some granularity, it may not fully capture\nthe nuanced importance of domain-specific entity\ntypes, especially in contexts where subtle distinc-\ntions carry significant implications (e.g., between\nsimilar medical conditions or treatments). Third,\nthe decomposition of text into atomic facts, while\nuseful for consistency measurement, can introduce\nsubjectivity or error depending on the quality of the\nfact extraction process. Finally, our approach has\nnot yet been integrated into end-to-end generative\nmodel training, and thus its impact on actual model\noutputs and downstream task performance remains\nto be empirically demonstrated.\n"}, {"page": 5, "text": "References\nAsad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Ja-\nson Hom, Christian Bluethgen, Eduardo Pontes Reis,\nSergios Gatidis, Namuun Clifford, Joseph Daws,\nArash S Tehrani, Jangwon Kim, and Akshay S Chaud-\nhari. 2024. A dataset and benchmark for hospital\ncourse summarization with adapted large language\nmodels. Journal of the American Medical Informat-\nics Association, 32(3):470–479.\nDaniel Deutsch, Tania Bedrax-Weiss, and Dan Roth.\n2021. Towards question-answering as an automatic\nmetric for evaluating the content quality of a sum-\nmary. Transactions of the Association for Computa-\ntional Linguistics, 9:774–789.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019. Rank-\ning generated summaries by correctness: An interest-\ning but challenging application for natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2214–2220, Florence, Italy. Association for\nComputational Linguistics.\nBen Goodrich, Vinay Rao, Peter J. Liu, and Mohammad\nSaleh. 2019. Assessing the factual accuracy of gener-\nated text. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery\n& Data Mining, KDD ’19, page 166–175, New York,\nNY, USA. Association for Computing Machinery.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023.\nNews summarization and evaluation in the era of\ngpt-3. Preprint, arXiv:2209.12356.\nAlistair E. W. Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J. Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow,\nLi wei H. Lehman, Leo A. Celi, and Roger G. Mark.\n2023. Mimic-iv, a freely accessible electronic health\nrecord dataset. Scientific Data, 10(1):1.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. SummaC: Re-visiting NLI-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-\ncale Fung, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2022. Factuality enhanced language models for\nopen-ended text generation. In Proceedings of the\n36th International Conference on Neural Information\nProcessing Systems, NIPS ’22, Red Hook, NY, USA.\nCurran Associates Inc.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880. Association for Computational\nLinguistics.\nZhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu,\nYuxuan Lai, Chongyang Tao, and Shuai Ma. 2024.\nLeveraging large language models for NLG evalua-\ntion: Advances and challenges. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 16028–16045, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2025. Factual consistency evaluation of summariza-\ntion in the era of large language models. Preprint,\narXiv:2402.13758.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12076–12100, Singa-\npore. Association for Computational Linguistics.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabilities\nof gpt-4 on medical challenge problems. Preprint,\narXiv:2303.13375.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n"}, {"page": 6, "text": "Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe La-\nban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscin-\nski, Justin Rousseau, and Greg Durrett. 2023. Un-\nderstanding factual errors in summarization: Errors,\nsummarizers, datasets, error detectors. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 11626–11644, Toronto, Canada. Association\nfor Computational Linguistics.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020a.\nPegasus: pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning, ICML’20. JMLR.org.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b.\nBertscore:\nEvaluating text generation with bert.\nPreprint,\narXiv:1904.09675.\nKaitlyn Zhou, Su Lin Blodgett, Adam Trischler, Hal\nDaumé III, Kaheer Suleman, and Alexandra Olteanu.\n2022. Deconstructing NLG evaluation: Evaluation\npractices, assumptions, and their implications. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 314–324, Seattle, United States. Association\nfor Computational Linguistics.\n"}]}