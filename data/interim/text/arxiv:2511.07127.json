{"doc_id": "arxiv:2511.07127", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.07127.pdf", "meta": {"doc_id": "arxiv:2511.07127", "source": "arxiv", "arxiv_id": "2511.07127", "title": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks", "authors": ["Linna Wang", "Zhixuan You", "Qihui Zhang", "Jiunan Wen", "Ji Shi", "Yimin Chen", "Yusen Wang", "Fanqi Ding", "Ziliang Feng", "Li Lu"], "published": "2025-11-10T14:12:35Z", "updated": "2025-11-13T16:00:18Z", "summary": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.07127v3", "url_pdf": "https://arxiv.org/pdf/2511.07127.pdf", "meta_path": "data/raw/arxiv/meta/2511.07127.json", "sha256": "2aca79af6e86931348c51f26cb4e46b3f863d129d120fe5b52fa8aa6cbfd5b0b", "status": "ok", "fetched_at": "2026-02-18T02:27:29.960740+00:00"}, "pages": [{"page": 1, "text": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal\nFeatures in Clinical Prognostic Tasks\nLinna Wang1*, Zhixuan You1*, Qihui Zhang2*, Jiunan Wen1, Ji Shi1, Yimin Chen3, Yusen Wang1,\nFanqi Ding1, Ziliang Feng1, Li Lu1†\n1Sichuan University\n2 Peking University\n3 The Second Affiliated Hospital of Kunming Medical University\nlenawang@stu.scu.edu.cn, youzhixuan@stu.scu.edu.cn, zqhui scu@foxmail.com, jiunanwen@stu.scu.edu.cn,\n2024141520215@stu.scu.edu.cn, 20241105@kmmu.edu.cn, 2024141520213@stu.scu.edu.cn, dingfanqi@stu.scu.edu.cn,\nfengziliang@scu.edu.cn, luli@scu.edu.cn\nAbstract\nLarge Language Models (LLMs) and causal learning each\nhold strong potential for clinical decision making (CDM).\nHowever, their synergy remains poorly understood, largely\ndue to the lack of systematic benchmarks evaluating their in-\ntegration in clinical risk prediction. In real-world healthcare,\nidentifying features with causal influence on outcomes is cru-\ncial for actionable and trustworthy predictions. While recent\nwork highlights LLMs’ emerging causal reasoning abilities,\nthere lacks comprehensive benchmarks to assess their causal\nlearning and performance informed by causal features in clin-\nical risk prediction. To address this, we introduce REACT-\nLLM, a benchmark designed to evaluate whether combin-\ning LLMs with causal features can enhance clinical prognos-\ntic performance and potentially outperform traditional ma-\nchine learning (ML) methods. Unlike existing LLM-clinical\nbenchmarks that often focus on a limited set of outcomes,\nREACT-LLM evaluates 7 clinical outcomes across 2 real-\nworld datasets, comparing 15 prominent LLMs, 6 traditional\nML models, and 3 causal discovery (CD) algorithms. Our\nfindings indicate that while LLMs perform reasonably in clin-\nical prognostics, they have not yet outperformed traditional\nML models. Integrating causal features derived from CD al-\ngorithms into LLMs offers limited performance gains, pri-\nmarily due to the strict assumptions of many CD methods,\nwhich are often violated in complex clinical data. While the\ndirect integration yields limited improvement, our benchmark\nreveals a more promising synergy: LLMs serve effectively as\nknowledge-rich collaborators for identifying and optimizing\ncausal features. Additionally, in-context learning improves\nLLM predictions when prompts are tailored to the task and\nmodel. Different LLMs show varying sensitivity to structured\ndata encoding formats, for example, open-source models per-\nform better with JSON, while smaller models benefit from\nnarrative serialization. These findings highlight the need to\nmatch prompts and data formats to model architecture and\npretraining.\nCode —\nhttps://github.com/LinnaWang-Lena/REACT LLM\nExtended version — https://arxiv.org/abs/2511.07127\n*These authors contributed equally.\n†Corresponding author.\nIntroduction\nIn clinical environments such as the Intensive Care Unit\n(ICU), timely and accurate risk assessment is critical for en-\nabling early interventions and improving patient outcomes\n(Alizadehsani et al. 2021; Placido et al. 2023; Fihn et al.\n2024; Yeh et al. 2024). The widespread adoption of Elec-\ntronic Health Records (EHRs) has provided access to rich,\nstructured clinical data, opening new opportunities for data-\ndriven decision support (Nguyen et al. 2021; Khalifa, Al-\nbadawy, and Iqbal 2024; Wang et al. 2025). One key applica-\ntion is prognostic modeling (Van Smeden et al. 2021), which\nestimates the risk of patients developing specific conditions\nover time. In this context, machine learning (ML) models\noutperform traditional statistical methods in terms of scala-\nbility and predictive performance. However, ML models of-\nten depend on high-dimensional features and remain vulner-\nable to input variability and spurious correlations (Rajpurkar\net al. 2022; Wang et al. 2025). This has motivated growing\ninterest in applying causal discovery (CD) to clinical down-\nstream tasks, as advances in CD methods (Zhou and Chen\n2022; Lagemann et al. 2023; Zanga, Ozkirimli, and Stella\n2022; Feuerriegel et al. 2024) enable the extraction of mean-\ningful causal relationships from observational data.\nRecently, Large Language Models (LLMs), known for\ntheir impressive capabilities across a wide range of natural\nlanguage processing tasks (Van Veen et al. 2024; Van Sons-\nbeek et al. 2023), have been increasingly applied to clinical\napplications (Thirunavukarasu et al. 2023; Ferdush, Begum,\nand Hossain 2024; Singhal et al. 2025; Van Veen et al. 2024;\nLiu et al. 2023; Sandmann et al. 2024; Kafkas et al. 2025;\nKang et al. 2025). Leveraging strategies such as fine-tuning\n(Ben Shoham and Rappoport 2024; Wang et al. 2024a),\nretrieval-augmented generation (RAG) (Bedi, Thukral, and\nDhiman 2025), and task-specific prompting (Zheng et al.\n2025), LLMs have shown promise in clinical risk prediction.\nFor example, (Ben Shoham and Rappoport 2024) fine-tuned\na pre-trained LLM to predict diagnoses and hospital read-\nmissions, achieving state-of-the-art performance.\nAlthough LLMs and causal learning each show strong\npotential for CDM, their potential synergy remains largely\n"}, {"page": 2, "text": "unexplored. This is primarily due to the lack of a system-\natic benchmark for evaluating their integrated application in\nclinical risk prediction tasks. Moreover, evidence on LLM\nperformance in this domain is not uniformly positive, with\nsome studies concluding that LLMs are not yet prepared for\nautonomous CDM (Hager et al. 2024; Brown et al. 2025).\nA recent benchmark, ClinicalBench (Chen et al. 2024), sys-\ntematically evaluated general-purpose and medical-specific\nLLMs against traditional ML models on 3 prognostic tasks.\nThe study revealed that despite variations in model scale\nand the use of different prompting or fine-tuning strategies,\nLLMs consistently underperformed ML models. Therefore,\nthe predictive capabilities of LLMs in clinical risk prediction\nrequire further investigation. A comprehensive benchmark\nthat integrates LLMs, causal learning, and clinical predic-\ntion is urgently needed. Motivated by this, this study aims to\nrigorously evaluate whether incorporating causal knowledge\ncan improve the performance of LLMs on critical clinical\nrisk prediction tasks. Unlike prior work limited to narrow\nclinical endpoints, we extend the evaluation to a broader set\nof prognostic outcomes to address the central question: Can\nthe integration of LLMs with causal features enhance per-\nformance in clinical prognostic tasks and potentially out-\nperform traditional ML models?\nREACT-LLM Design. To answer this question, we\npresent REACT-LLM (Risk Evaluation and Causal features\nTest with LLMs), a novel benchmark for assessing how ef-\nfectively LLMs can serve as inference and error-correction\nexperts that augment CD methods in uncovering causal fea-\ntures of clinical risk outcomes (Figure 1). Using structured\ndata from 2 real-world datasets, MIMIC-III(Johnson et al.\n2016) and MIMIC-IV(Johnson et al. 2023), we investigate 7\nrepresentative prognostic tasks: (1) In-hospital mortality (In-\nHospDeath), (2) 30-day hospital readmission (Readmit30),\n(3) Multiple ICU stays during a single hospitalization (Mul-\ntiICU), (4) Sepsis during ICU stay (SepsisICU), (5) Acute\nkidney injury during ICU stay (AKIICU), (6) Prolonged\nhospital stay (LOS), and (7) Early ICU admission (Early-\nICU). Here are the main tasks:\n▷Baseline evaluation: Benchmark all MLs/LLMs on 7\noutcomes across 2 datasets using complete feature sets.\n▷Prompt engineering evaluation: Beyond direct prompt-\ning, 4 representative prompt engineering strategies (Chain-\nof-Thought (CoT), Self-Reflection (SR), Role-Playing (RP),\nand In-Context Learning (ICL)) are employed to assess\nLLMs performance across 7 tasks.\n▷Input format sensitivity evaluation: Assess LLMs per-\nformance across 5 formats for structured patient data: Row-\nColumn Format (RCF), JSON, LaTeX, Template-Based\nNatural Language (TBNL) and Narrative Serialization (NS).\n▷Causal feature evaluation: Use 3 representative CD\nmethods to identify features with direct or indirect causal\nrelationships to each outcome. Evaluate LLMs using only\nthese causal features as input.\n▷LLM-assisted causal feature editing evaluation: Prompt\nLLMs to optimize the causal feature sets derived from 3 CD\nmethods, and separately, to generate causal feature sets for\neach outcome relying solely on their internal knowledge.\nOverall, the contributions can be summarized as follows:\n• O Benchmark: Evaluate 3 CD algorithms, 6 ML models,\nand 15 LLMs (spanning diverse model sizes and archi-\ntectures) for predicting 7 clinical outcomes.\n• õ Two Datasets: We curate the MIMIC-III and MIMIC-\nIV datasets, encompassing 6 categories of clinical in-\nformation and 7 prognostic outcome labels, providing\na comprehensive foundation for evaluating LLM-based\nmedical risk prediction models.\n• \u0011 Actionable Insights & Nuanced Findings: (1) LLMs\nhave yet to outperform traditional ML models in clin-\nical outcome prediction. (2) Engineering prompts of-\nfer slight gains under imbalanced EHR conditions. (3)\nLLMs refine CD-derived features effectively, enabling\na human–AI synergy for causal feature engineering.\n(4) Different LLMs exhibit varying sensitivity to struc-\ntured EHR encoding formats. RCF benefits proprietary\nlarge models, JSON consistently enhances performance\nin open-source models, while NS proves effective for\nsmaller models. LLMs can infer causal relations even\namong variables with ambiguous or coded names, such\nas ICD codes.\nREACT-LLM Construction\nGoals\n• Causal Feature Identification. Recover the causal\nstructure from observational data using 3 CD methods by\nassigning each outcome to its direct and indirect causes.\n• Clinical Binary Outcome Prediction. Estimate the\nprobability of binary outcomes.\n• LLM Evaluation Tasks. Evaluate LLMs across 5 groups\nof experiments to address the following questions:\n❑Q1: Can LLM outperform traditional ML in clinical\nprognosis? →Based on baseline evaluation.\n❑Q2: Can prompt engineering boost LLM performance\nin clinical risk prediction? →Based on prompt engineer-\ning evaluation.\n❑Q3: How does the encoding format of structured EHR\ndata affect LLM performance? →Based on input format\nsensitivity evaluation.\n❑Q4: Can CD improve LLM predictions by identifying\ncausal features? →Based on causal feature evaluation.\n❑Q5: Can LLM validate or identify causal features in\nclinical contexts? →Based on LLM-assisted causal fea-\nture editing evaluation.\nDatasets Preprocessing\nStudy Population. This study uses 2 public datasets1:\nMIMIC-III (v1.4) (Johnson et al. 2016) and MIMIC-IV\n(v3.1) (Johnson et al. 2023), which contain de-identified\nEHRs from ICU and emergency department admissions at\nBeth Israel Deaconess Medical Center in Boston. MIMIC-\nIII contains ICU records from 2001 to 2012 and is widely\nused in critical care research, while MIMIC-IV extends cov-\nerage through 2022 with an updated schema and improved\n1https://physionet.org\n"}, {"page": 3, "text": "Figure 1: REACT-LLM framework diagram. The Data Preprocessing Pipeline outlines data processing steps, 7 outcome labels,\nand dataset splits. The MIMIC-LLM table presents our curated data, showing the proportion of positive samples for each\noutcome. LLM/ML Clinical Outcome Prediction presents 2 prediction tasks and a prompt sample. Feed Data & Causal Features\nto LLM illustrates 3 evaluation tasks.\ndata quality. We include only adult patients (age ≥18), re-\ntained the first ICU stay per admission, and excluded ICU\nstays shorter than one day to ensure clinical relevance.\nData collection. We extract 5 categories of features to\ncharacterize each ICU admission: (1) demographics and ad-\nmission details (age, gender, admission type, initial ICU\nunit); (2) 65 high-frequency admission diagnoses from\nMIMIC-III/IV, encoded as binary indicators (present/ab-\nsent); (3) 27 high-frequency clinical procedures (binary en-\ncoded); (4) 55 medications administered within the first 24\nhours (total dosage per drug); and (5) 115 vital signs and\nlaboratory results from the first 24 hours (median summa-\nrized). The 24-hour window supports early prediction while\ncapturing key clinical information from the initial phase of\nICU care.\nExperiment Setup\nPrediction Tasks. We define 7 outcome labels (not mu-\ntually exclusive): (1) InHospDeath: in-hospital mortality;\n(2) Readmit30: a binary indicator set to 1 if a subsequent\nhospital admission occurs within 30 days of discharge; (3)\nMultiICU: more than one ICU stay associated with the\nsame hospital admission; (4) SepsisICU: identified using\nthe database-provided label based on the Sepsis-3 definition\n(Singer et al. 2016), which requires a SOFA score ≥2 and\nclinical suspicion of infection; (5) AKIICU: defined accord-\ning to KDIGO criteria (Khwaja 2012), using creatinine or\nurine output when available, with baseline creatinine taken\nas the lowest value within the prior 7 days; (6) LOS: length\nof hospital stay exceeding 14 days; and (7) EarlyICU: ICU\nadmission occurring within 12 hours of hospital admission.\nDataset Division. To manage the computational cost of\nquerying LLMs, we apply stratified random sampling to en-\nsure proportional representation of positive cases for each\nlabel, selecting 8,000 samples from each datasets, resulting\nin a combined cohort of 16,000 patients. For traditional ML\nmodels, the combined dataset is split into training (6,600),\nvalidation (800), and test (600) sets. The test sets from both\ndatasets are shared with LLM evaluations to ensure fair and\nconsistent comparisons.\nMetrics. This paper uses AUROC, AUPRC and F1 score.\nWe report 95% confidence intervals based on 1,000 boot-\nstrap samples for ML models and 5-run results for LLMs.\nMethod\nCausal Discovery Models: We evaluate 3 representa-\ntive CD approaches: (1) functional-based (DirectLiNGAM\n(Shimizu et al. 2011)), (2) score-based (GES (Chicker-\ning 2002)), and (3) gradient-based (CORL (Wang et al.\n2021)). All methods were implemented using the gCastle\ntoolkit2, an open-source causal discovery library developed\nby Noah’s Ark Lab (Zhang et al. 2021). We apply each CD\nmethod 5 times on the full dataset to identify direct and in-\ndirect causes for each outcome. Features appearing in more\n2https://github.com/huawei-noah/trustworthyAI/tree/master/\ngcastle\n"}, {"page": 4, "text": "than 2 runs are retained to form the causal feature set. The\nparameter Settings are shown in the Appendix A.2.\nBenchmarked ML Models: We include 6 common used\nbaseline models: AdaBoost, Decision Tree (DT), Logistic\nRegression (LR), Random Forest (RF), Support Vector Ma-\nchine (SVM) and XGBoost. All models are run with default\nhyperparameters to ensure fairness and reproducibility, al-\nlowing us to examine whether the LLM can outperform un-\noptimized traditional ML models.\nPrompt Protocols. Following predefined clinical cate-\ngories (patient demographics, diagnoses, procedures, med-\nications, and laboratory results including vital signs), we\nevaluate 5 prevalent encoding strategies for transforming\nstructured EHR data into formats suitable for LLM input:\n(1) RCF: Encodes records in a flat, CSV-like format using\ncomma-separated values. (2) JSON Encoding: Represents\ndata as hierarchical key-value pairs organized by clinical\ncategories. (3)LaTeX Table Format: Structures EHR data\ninto formatted LaTeX-style tables by category. (4) TBNL:\nConverts each clinical category into sentence-level descrip-\ntions using predefined templates to enhance textual fluency.\n(5) NS: Converts tabular EHR data into coherent, human-\nreadable narratives, structured by the predefined clinical cat-\negories. We proceed as follows:\n• Baseline Evaluation: Applies the Direct Prompting ap-\nproach using LLMs on input formatted with NS.\n• Prompt Engineering Evaluation: Extends beyond Direct\nPrompting by incorporating strategies CoT, SR, RP and\nICL, all under the NS format.\n• Input Format Sensitivity Evaluation: Tests the Direct\nPrompting approach across all 5 input formats (RCF,\nJSON, LaTeX, TBNL, NS) to assess the impact of en-\ncoding on performance.\n• Causal Feature Evaluation: Uses only the feature subsets\nidentified by CD methods, test under the Direct Prompt-\ning setting with NS input.\nThe prompt protocols for these 4 groups follow a consis-\ntent design and are detailed in the Appendix C and E.\nFor the LLM-assisted causal feature editing evaluation,\nwe design 2 protocols (details are in the Appendix D):\n• LLM edited CD feature sets: For each clinical outcome,\nfeature sets generated by a CD algorithm are provided\nto the LLM, which was prompted to act as an expert in\nintensive care medicine and refine the list. The LLM’s\noutput was used as the optimized feature set.\n• LLM-Generated causal feature sets: The LLM indepen-\ndently generates a causal feature set based solely on its\ninternalized clinical knowledge.\nLLM Implement details: Experiments are conducted on\n3 NVIDIA RTX A800 GPUs. We benchmark a diverse set of\nLLMs spanning different architectures and scales. To ensure\nreproducibility, all inferences use greedy decoding (temper-\nature = 0, do sample = False). Proprietary models included\nadvanced reasoning models (GPT-o1, GPT-o3 mini, Claude-\n3.7-Sonnet, Gemini-2.5-Pro, Gemini-2.5-Flash), large mod-\nels (GPT-4o, Claude-4, Claude-3.5-Haiku), and small mod-\nels (GPT-4o-mini). Open-source models included a top\nreasoning model (DeepSeek-R1), large models (Llama-\n3.1-405b, DeepSeek-V3, Qwen3-235b), and small models\n(Qwen3-8b, Qwen3-14b). Details are in Appendix A.1.\nEmpirical Results and Analysis\nBaseline evaluation\nWe evaluate 15 LLMs with direct prompting across 7 clin-\nical prediction tasks on 2 datasets, using 3 metrics (com-\nplete results in the Appendix B.1 and B.2). As shown in\nTable 1, among traditional ML models, XGBoost, RF, and\nLR consistently achieve top performance across clinical out-\ncomes. In the open-source LLM category, larger models\n(e.g., DeepSeek-V3, Llama-3.1-405b) outperform smaller\nand thinking models more frequently. While thinking mod-\nels rarely lead on individual tasks, they exhibit more stable\nperformance than smaller models overall. A similar trend\nholds for proprietary LLMs: larger models generally out-\nperform other ones. Notably, Gemini-2-Flash achieves the\nhighest AUPRC on the EarlyICU task (0.8540), exceeding\nthe best ML model (RF, 0.8473). Thinking models in this\ngroup also show more consistent performance than smaller\nvariants. Overall, proprietary LLMs outperform open-source\nones more often across tasks.\nHowever, with the exception of Gemini-2-Flash on the\nEarlyICU AUPRC metric, none of the LLMs surpass tra-\nditional ML models on any clinical prediction task. In most\ncases, LLM performance lags behind ML baselines by a sub-\nstantial margin, typically around 10–20%.\nBased on this, we select 6 representative small, large\nand thinking models from both open-source and proprietary\nLLMs for subsequent experiments.\n✔Finding for Q1: Current LLMs remain immature and\nunreliable for clinical prognostic decision support. Among\nLLMs, proprietary and large-scale models tend to offer rel-\natively better performance.\nPrompt engineering evaluation\nWe evaluate 6 LLMs with 5 prompting strategies across 7\nclinical prediction tasks(complete results in Appendix B.3).\nAcross all tasks and metrics, ICL surpasses the baseline 41\ntimes and ranks best in 22. SR also performs well, exceed-\ning the baseline in 38 cases and leading in 19. Role-Playing\nand CoT follow. As shown in Figure 2, ICL outperforms\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nInHospDeath\nReadmit30\nMultilICU\nSepsisICU\nAKIICU\nLOS\nEarlyICU\n LLMs with Different Prompt Engineerings\nCHAIN-OF-THOUGHT\nSELF-REFELECTION\nRole-playing\nIn-Context-Learning\nGemini-2-Flash\nGemini-2-Pro\nGPT-o3-mini\nQwen3-8b\nQwen3-235b\nDeepSeek-R1\nFigure 2: AUROC of LLMs with Different Engineering\nStrategies on MIMIC-III across 7 Labels. Bars indicate base-\nline scores obtained via direct prompting.\n"}, {"page": 5, "text": "Outcome\nInHospDeath\nReadmit30\nMultiICU\nSepsisICU\nAKIICU\nLOS\nEarlyICU\nPositive Ratio\n27.0%\n13.6%\n17.7%\n46.9%\n55.0%\n29.1%\n69.5%\nMetric\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nMachine Learning Models\nSVM\n0.8949 0.7707 0.6668 0.2393 0.7467 0.4141 0.8059 0.7956 0.8221 0.8558 0.8499 0.6949 0.7336 0.8304\nLR\n0.9079 0.8047 0.7326 0.2912 0.7555 0.4419 0.8059 0.7856 0.8285\n0.865\n0.8552 0.7022 0.6884 0.8406\nDT\n0.7022 0.6354 0.5323 0.2535 0.5617 0.3227\n0.659\n0.7265 0.6499 0.7815 0.6482 0.5699 0.5885 0.8347\nRF\n0.8936 0.7806 0.6222 0.2122 0.7109 0.3507 0.8229 0.8059 0.8323 0.8576 0.8521 0.6917 0.7263 0.8473\nAdaboost\n0.8694 0.7231 0.6837\n0.253\n0.7418 0.3532 0.8111\n0.795\n0.8318 0.8655 0.8404 0.6736 0.7180 0.8443\nXGboost\n0.9088 0.8148 0.6863 0.2469 0.7199 0.3737 0.8183 0.7964 0.8332 0.8606 0.8516 0.7099 0.7312 0.8319\nOpen-source Large Language Models\nQwen3-14b\n0.8075 0.6544 0.4502 0.1002 0.5719 0.1472 0.6848 0.6528 0.7092 0.7679 0.6219 0.3143 0.6179\n0.739\nQwen3-8b\n0.8076 0.6537 0.4511 0.0999 0.5711 0.1476 0.6866 0.6546 0.7089 0.7676 0.6172 0.3129\n0.615\n0.7377\nLlama-3.1-405b\n0.7012 0.4658 0.4929 0.1304 0.5545 0.1681 0.5687 0.7139 0.6988 0.7969 0.5355 0.6419 0.5389 0.7477\nQwen3-235b\n0.8194 0.6878 0.4580 0.0964 0.5615 0.1695 0.6894 0.6604 0.7221 0.7409 0.6425 0.5511 0.5641 0.7197\nDeepSeek-V3\n0.8370 0.6873 0.4478 0.0796 0.5845 0.2126 0.6756 0.6384 0.7413 0.7862 0.6531 0.4192 0.5567 0.7796\nDeepSeek-R1\n0.8363 0.6649 0.4285 0.1030 0.5793 0.2104 0.6883 0.6487 0.7270 0.7667 0.6096 0.3732 0.5663 0.7660\nProprietary Large Language Models\nGPT-4o-mini\n0.8080 0.7036 0.4712\n0.091\n0.5538 0.1748 0.6673 0.6479 0.7104 0.7487 0.6158 0.4195 0.5450 0.7855\nGemini-2-Flash\n0.8475 0.6817 0.4881 0.1209 0.5634 0.1759 0.6653 0.6703 0.7045 0.7507 0.6799 0.4592 0.6455 0.8540\nGPT-4o\n0.8223 0.6961 0.4780 0.2133 0.5790 0.1878 0.6751 0.6298 0.7069 0.7687 0.6618 0.4375 0.5513 0.7676\nClaude-3.5-Haiku 0.8149 0.7020 0.5227 0.0992 0.5517 0.1616 0.6586 0.5482 0.7085 0.7501 0.6172 0.4412 0.5970 0.7640\nClaude-4\n0.8252 0.7039 0.4500 0.0860 0.5689 0.2167 0.6707 0.6765 0.6846 0.7449 0.6354 0.5465 0.5128 0.7559\nGemini-2-Pro\n0.8424 0.6904 0.4848 0.1177 0.6135 0.2300 0.6806 0.6373 0.7214 0.7625 0.7122 0.5089 0.5748 0.7722\nGPT-o1\n0.8396 0.6695 0.4742 0.1163 0.6194 0.2269 0.6605 0.5984 0.7127 0.7558 0.6331 0.3691 0.5931 0.7675\nGPT-o3-mini\n0.8585 0.6831 0.5015 0.1224 0.6109 0.1955 0.6758 0.6585 0.7095 0.7433 0.6712 0.4735 0.5408 0.8013\nClaude-3.7-Sonnet 0.8699 0.7365 0.4665 0.1146 0.5815 0.1999 0.6530 0.6101 0.7139 0.7588 0.6891 0.4883 0.5867 0.7544\nTable 1: Baseline performance of LLMs and traditional ML models across 7 clinical prediction tasks on MIMIC-III. Results\non MIMIC-IV are provided in Appendix B.1 and B.2. ‘Positive Ratio’ refers to the proportion of samples labeled as 1. ‘ROC’\ndenotes AUROC, and ‘PRC’ denotes AUPRC. Bold scores indicate the best performance within ML models and LLMs category.\ndirect prompting in 18 cases and achieves the highest AU-\nROC in 13. Prompting strategies yield clear improvements\non the Readmit30 task. ICL enhances performance: Gemini-\n2-Pro’s AUROC increases by 0.077, GPT-o3-mini’s AU-\nROC and F1 improve by 0.072 and 0.084, respectively, and\nQwen3-235B gains 0.059 in AUROC. DeepSeek-R1 bene-\nfits more from CoT prompting, with a 0.0271 increase in\nAUPRC and a 0.0912 gain in AUROC. However, despite\nthese gains, none of the prompting strategies enable LLMs\nto surpass traditional ML models across any outcome.\n✔Finding for Q2: ICL can enhance LLM performance\nin clinical prediction tasks. In highly imbalanced settings,\nprompting strategies show some benefits. However, no sin-\ngle strategy consistently improves results across all tasks and\nmodels, suggesting their effectiveness depends on the clini-\ncal context and model capacity.\nInput format sensitivity evaluation\nWe analyze the performance of 4 LLMs across 3 metrics\nusing 5 input format strategies (complete results in the Ap-\npendix B.4). As shown in Figure 3, the proprietary large\nmodel GPT-o3-mini responds best to RCF, with weaker per-\nformance on NS and TBNL. The open-source large model\nQwen3-235b performs best with JSON, leading across all\nmetrics and outperforming RCF by up to 0.064, but responds\nleast effectively to RCF. Among smaller models, the pro-\nprietary Gemini-2-Flash performs best with NS, achieving\nits highest AUROC and AUPRC, surpassing RCF by 0.051\nand 0.026, respectively. The open-source Qwen3-8b shows\nstrong results with both JSON and NS.\nGemini-2-Flash\nGPT-o3-mini\nQwen3-8b\nQwen3-235b\nGemini-2-Flash\nGPT-o3-mini\nQwen3-8b\nQwen3-235b\nGemini-2-Flash\nGPT-o3-mini\nQwen3-8b\nQwen3-235b\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nScore\nF1\nROC\nPRC\nLLMs with Different Input Formats\nRCF\nJSON\nLATEX\nTBNL\nNS\nFigure 3: Average performance of LLMs with 5 Input For-\nmats on MIMIC-III. All 3 scores represent the mean values\nacross 7 outcomes.\n"}, {"page": 6, "text": "✔Finding for Q3: Different LLMs show varying sensitiv-\nity to structured EHR encoding formats. RCF works well for\nproprietary large models. JSON consistently improves per-\nformance for open-source models. LaTeX tables offer lim-\nited benefit across models. TBNL produces mixed results,\nindicating rigid templates require task-specific tuning. NS is\nparticularly effective for smaller models.\nWhy do different types of LLMs prefer different input\nformats? This preference likely stems from the interplay\nbetween pretraining data, model architecture, and capacity.\nProprietary large models tend to favor RCF, possibly due to\nexposure to structured documents during pretraining. This\naligns with previous study (Li et al. 2024), which suggest\nLLMs exhibit a bias toward formal data formats. In contrast,\nopen-source large models prefer JSON, as their training\ndata often include public datasets rich in JSON-formatted\ntemplates, API references, and structured records, common\non platforms like Hugging Face. JSON’s key-value struc-\nture closely matches the data distribution these models en-\ncounter, and a study (Zhu et al. 2024) shows that reinforcing\nJSON format improves hierarchical parsing. Smaller mod-\nels perform better with NS, which converts inputs into flu-\nent narrative text. This format reduces reliance on structure\nparsing and better matches the natural language data these\nmodels are trained on, aligning with a previous study show-\ning that smaller models benefit from training-aligned data\ndistributions (Yam and Paek 2024).\nCausal feature evaluation\nIn this section, we investigate our core hypothesis (Q4):\ncan causally selected features improve LLM prediction?\nWe evaluate the performance of 6 LLMs using 3 CD fea-\nture sets (complete results in the Appendix B.5). As shown\nin Figure 4, across all LLMs, CD-derived features often\nlead to performance degradation. Compared to the base-\nline, DirectLiNGAM leads to the most significant drop.\nHowever, CORL yields slight improvements on Gemini-2-\nPro, DeepSeek-R1 and Qwen3-235b. GES shows consis-\ntent gains on open-source models including DeepSeek-R1,\nQwen3-8b and Qwen3-235b.\nContrary to expectations, although some improvements\nBase\nLLM Gen\nCORL\nCROL Opt\nDL\nDL Opt\nGES\nGES Opt\nGemini-2-Pro\nGemini-2-Flash\nGPT-o3-mini\nDeepSeek-R1\nQwen3-235b\nQwen3-8b\n0.6511\n0.6489\n0.6568\n0.6523\n0.6165\n0.6284\n0.6411\n0.6506\n0.6501\n0.6370\n0.6422\n0.6428\n0.6101\n0.6264\n0.6406\n0.6297\n0.6592\n0.6148\n0.6328\n0.6393\n0.6027\n0.6150\n0.6546\n0.6406\n0.6306\n0.6338\n0.6348\n0.6308\n0.6157\n0.5969\n0.6395\n0.6227\n0.6375\n0.6483\n0.6393\n0.6359\n0.5935\n0.6247\n0.6415\n0.6468\n0.6380\n0.6410\n0.6313\n0.6324\n0.5844\n0.6078\n0.6493\n0.6499\nAUROC Score Performance of LLM-Assisted Causal Feature Editing\n0.59\n0.60\n0.61\n0.62\n0.63\n0.64\n0.65\nAUROC Value\nFigure 4: Average AUROC of LLMs across 7 outcomes\nover MIMIC-III and MIMIC-IV. ’Base’ denotes perfor-\nmance using all features. ’LLM Gen’ refers to the causal\nfeature set generated by the LLM itself. ’DL’ indicates Di-\nrectLiNGAM. ’Opt’ indicates the optimized feature set re-\nfined from the CD outputs by the corresponding LLM.\nare observed, LLMs leveraging CD features do not consis-\ntently outperform the baseline. However, this does not re-\nflect the quality of the CD methods themselves, as ground-\ntruth causal graphs are rarely available in clinical data for\nvalidation. In clinical risk prediction, strict assumptions in\nCD algorithms may result in a limited set of outcome-related\ncausal features, while non-causal but highly correlated fea-\ntures still provide strong predictive signals. Additionally,\nmost CD methods lack prior knowledge and domain-specific\nconstraints, making it difficult to recover complex causal\nstructures from high-dimensional clinical data.\n✔Finding for Q4: Causal feature subsets derived from\ncommonly used CD algorithms did not consistently im-\nprove LLMs prediction. This may be due to strict assump-\ntions inherent in many CD methods (e.g., the Causal Faith-\nfulness assumption or the absence of hidden confounders),\nwhich are often violated in complex, high-dimensional clini-\ncal data. However, this highlights the potential of further ex-\nploring whether integrating LLM-derived prior knowledge\ninto CD outcomes can enhance performance (Zhou et al.\n2024; Ban et al. 2025).\nLLM-assisted causal feature editing evaluation\nBuilding on the findings from Q4, we further explore\nwhether LLM-assisted causal features could enhance LLM\nperformance. We evaluate 2 strategies: (1) optimizing causal\nfeature sets derived from CD methods using different LLMs,\nand (2) allowing LLMs to directly generate causal feature\nsets based on clinical knowledge. In both cases, the corre-\nsponding LLM is used to predict the outcome using its re-\nspective feature set (complete results in the Appendix B.5).\nFigure 4 shows the average AUROC performance of each\nmodel on both MIMIC-III and MIMIC-IV datasets. LLM\noptimization is key for DirectLiNGAM feature sets. Ex-\ncept for DeepSeek-R1, the optimized DirectLiNGAM fea-\ntures consistently outperform their unoptimized counter-\nparts, though still fall short of the baseline. For CORL and\nGES, optimization leads to mixed results, with both im-\nprovements and declines. Notably, optimized GES achieves\nfurther gains on Qwen3-8b and Qwen3-235b, surpassing the\nbaseline. While LLM-optimized CD features do not always\nMIMIC III\nMIMIC IV\nModel\nAUROC\nF1\nAUROC\nF1\nGemini-2-Pro\nBase\n0.6571\n0.5471\n0.6451\n0.5426\nGES Opt\n0.6403\n0.5142\n0.6420\n0.5479\nLLM Gen\n0.6559\n0.5474\n0.6453\n0.5673\nQwen3-8b\nBase\n0.6368\n0.5214\n0.6392\n0.5456\nGES Opt\n0.6449\n0.4732\n0.6537\n0.5240\nLLM Gen\n0.6518\n0.4696\n0.6481\n0.5288\nTable 2: Average performance of LLMs across 7 outcomes\non MIMIC-III and MIMIC-IV, respectively. ’Base’ denotes\nperformance using all features. ’LLM Gen’ denotes the\ncausal feature set generated by the LLM itself, while ’Opt’\nrefers to the GES causal features optimized by the LLM.\n"}, {"page": 7, "text": "Baseline\nGES Opt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPredicted Probability\nLLM Gen\nPredicted Probability Distributions of Gemini-2-Flash\nLOS\nInHospDeath\nReadmit30\nMultiICU\nLOS\nInHospDeath\nReadmit30\nMultiICU\nSepsisICU\nAKIICU\nEarlyICU\nFigure 5: Predicted probability distributions of Gemini-\n2-Flash across 7 outcomes using 3 different feature sets\non MIMIC III: Baseline (all features), Optimized GES\n(causal features optimized by Gemini-2-Flash), and LLM-\nGenerated (features generated by Gemini-2-Flash).\nexceed baseline performance, they generally improve upon\nraw CD outputs. LLM-generated causal features often out-\nperform those from CORL and DirectLiNGAM, and show\ncompetitive performance with GES. To further analyze this\nperformance, Table 2 provides a breakdown by dataset for\nrepresentative proprietary and open-source LLMs, showing\nthat LLM-generated causal feature sets and LLM-optimized\nGES feature sets yield comparable results.\nFigure 5 shows the predicted probability distributions of\nGemini-2-Flash using the baseline features, optimized GES\nfeatures and LLM-generated causal features. For outcomes\nlike Readmit30, MultiICU, and InHospDeath, the LLM-\ngenerated features exhibit prediction preferences similar to\nthe baseline, tending toward more severe outcomes. Across\nall 3 feature sets, the prediction distributions for probabili-\nties above 0.5 are largely consistent.\n✔Finding for Q5: LLMs show potential in optimizing\nand identifying causal features in clinical settings. This high-\nlights a valuable synergy: CD algorithms provide a struc-\ntured foundation, while LLMs contribute prior knowledge\nto refine and enhance feature selection (Darvariu, Hailes,\nand Musolesi 2024; Zhou et al. 2024). Our results further\ndemonstrate that LLMs can serve as a complementary tool\nfor causal feature selection in clinical prediction tasks, align-\ning with recent studies showing that LLMs are capable of\ninferring causal relationships from clinical data (Naik et al.\n2024; Kiciman et al. 2023).\nRelated works\nClinical decision support is rapidly advancing, driven by\nthe growth of medical data and the shift toward precision\nmedicine. In ICUs, robust and interpretable ML models like\nLR, XGBoost, and RF are widely used for tasks such as pre-\ndicting in-hospital mortality (Wang et al. 2025), readmis-\nsion (Fathy, Emeriaud, and Cheriet 2025), AKI (Lin, Shi,\nand Kong 2025), and sepsis (Gao et al. 2024), enhancing di-\nagnostic efficiency and enabling early intervention for high-\nrisk patients (Hyland et al. 2020).\nRecently, LLMs have shown promise by directly process-\ning unstructured text, preserving critical clinical details of-\nten lost in manual feature extraction (Xu et al. 2024; Hager\net al. 2024; Wang et al. 2024b; Ahmed et al. 2025). Through\nfine-tuning and RAG, LLMs integrate hospital-specific data\nwith general medical knowledge, improving decision sup-\nport (Ansari et al. 2025; Jin et al. 2024). Current benchmarks\nfocus on clinical question answering, multimodal integra-\ntion, and real-world evaluation frameworks (Liu et al. 2024;\nJin et al. 2021; Bae et al. 2023; Budler et al. 2025; Esteitieh,\nMandal, and Laliotis 2025).\nMeanwhile, causal learning enhances clinical prediction\ninterpretability at both model and data levels. ML may over-\nfit spurious correlations, resulting in unreliable predictions\n(Huang et al. 2025). In contrast, causal learning aims to un-\ncover the true causal structures, offering more robust and\ninterpretable insights (Richens, Lee, and Johri 2020; Feuer-\nriegel et al. 2024; Zhou and Chen 2022; Zanga, Ozkirimli,\nand Stella 2022). Major causal discovery methods include\nconstraint-based (e.g., PC (Spirtes, Glymour, and Scheines\n2000), score-based (e.g., GES (Chickering 2002)), func-\ntional causal models (e.g., DirectLiNGAM (Shimizu et al.\n2011)), and optimization-based approaches (e.g., CORL\n(Wang et al. 2021)). Among them, constraint-based meth-\nods are limited by reliance on the faithfulness assumption\nand inability to determine causal directions within Markov\nequivalence classes(Zhou and Chen 2022).\nIntegrating LLMs with causality advances clinical predic-\ntion by producing reliable causal chains, enhancing counter-\nfactual reasoning, and translating complex causal relation-\nships into clinically meaningful insights (Zeng et al. 2025;\nKiciman et al. 2023; Kweon et al. 2024). However, large-\nscale evaluations of LLM and causal methods specifically\nfor clinical risk prediction remain scarce. Detailed related\nwork can be found in Appendix F.\nConclusion\nThis study evaluates LLMs with causal features in clinical\nprognostic tasks. While integrating CD features into LLMs\ndid not yield notable gains in clinical risk prediction, this\ndoes not diminish the inherent value of CD. The modest\nperformance is largely due to limitations of current CD al-\ngorithms in clinical settings: the strict assumptions that of-\nten result in sparse or incomplete feature sets. These chal-\nlenges highlight the need for caution when applying CD out-\nputs directly to LLM-based tasks without further refinement.\nNonetheless, our findings suggest a promising synergy: CD\nprovides a data-driven foundation for causal feature iden-\ntification, while LLMs contribute rich domain knowledge\nto enhance feature selection. This aligns with growing ev-\nidence that LLMs can assist in uncovering causal relation-\nships in complex biomedical contexts. Moving forward, in-\ncorporating LLM-guided priors into the CD process, or us-\ning LLMs to refine CD-derived features post hoc, offers a\npromising path toward more robust clinical prediction mod-\nels. We also find that ICL improves LLM performance in\nclinical prediction, especially when prompts are tailored to\n"}, {"page": 8, "text": "the task and model. Input format also matters: proprietary\nmodels prefer RCF, open-source models perform better with\nJSON, and smaller models benefit from NS inputs. These\nresults highlight the need to align prompts and data formats\nwith model architecture, capacity, and pretraining.\nReferences\nAhmed, A.; Saleem, M.; Alzeen, M.; Birur, B.; Fargason,\nR. E.; Burk, B. G.; Harkins, H. R.; Alhassan, A.; and Al-\nGaradi, M. A. 2025.\nLeveraging Large Language Mod-\nels to Enhance Machine Learning Interpretability and Pre-\ndictive Performance: A Case Study on Emergency Depart-\nment Returns for Mental Health Patients.\narXiv preprint\narXiv:2502.00025.\nAlizadehsani, R.; Alizadeh Sani, Z.; Behjati, M.; Roshanza-\nmir, Z.; Hussain, S.; Abedini, N.; Hasanzadeh, F.; Khosravi,\nA.; Shoeibi, A.; Roshanzamir, M.; et al. 2021. Risk factors\nprediction, clinical outcomes, and mortality in COVID-19\npatients. Journal of medical virology, 93(4): 2307–2320.\nAnsari, M. S.; Khan, M. S. A.; Revankar, S.; Varma, A.; and\nMokhade, A. S. 2025. Lightweight Clinical Decision Sup-\nport System using QLoRA-Fine-Tuned LLMs and Retrieval-\nAugmented Generation. arXiv preprint arXiv:2505.03406.\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\nOh, J.; Ji, L.; Chang, E.; Kim, T.; et al. 2023. Ehrxqa: A\nmulti-modal question answering dataset for electronic health\nrecords with chest x-ray images. Advances in Neural Infor-\nmation Processing Systems, 36: 3867–3880.\nBan, T.; Chen, L.; Lyu, D.; Wang, X.; Zhu, Q.; and Chen,\nH. 2025. Llm-driven causal discovery via harmonized prior.\nIEEE Transactions on Knowledge and Data Engineering.\nBedi, P.; Thukral, A.; and Dhiman, S. 2025. XLR-KGDD:\nleveraging LLM and RAG for knowledge graph-based ex-\nplainable disease diagnosis using multimodal clinical infor-\nmation. Knowledge and Information Systems, 1–21.\nBen Shoham, O.; and Rappoport, N. 2024. Cpllm: Clini-\ncal prediction with large language models. PLOS Digital\nHealth, 3(12): e0000680.\nBrown, K. E.; Yan, C.; Li, Z.; Zhang, X.; Collins, B. X.;\nChen, Y.; Clayton, E. W.; Kantarcioglu, M.; Vorobeychik,\nY.; and Malin, B. A. 2025. Large language models are less\neffective at clinical prediction tasks than locally trained ma-\nchine learning models. Journal of the American Medical\nInformatics Association, 32(5): 811–822.\nBudler, L. C.; Chen, H.; Chen, A.; Topaz, M.; Tam, W.; Bian,\nJ.; and Stiglic, G. 2025. A Brief Review on Benchmarking\nfor Large Language Models Evaluation in Healthcare. Wi-\nley Interdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 15(2): e70010.\nChen, C.; Yu, J.; Chen, S.; Liu, C.; Wan, Z.; Bitterman, D.;\nWang, F.; and Shu, K. 2024.\nClinicalBench: Can LLMs\nBeat Traditional ML Models in Clinical Prediction? arXiv\npreprint arXiv:2411.06469.\nChickering, D. M. 2002.\nOptimal structure identification\nwith greedy search. Journal of machine learning research,\n3(Nov): 507–554.\nDarvariu, V.-A.; Hailes, S.; and Musolesi, M. 2024. Large\nlanguage models are effective priors for causal graph dis-\ncovery. arXiv preprint arXiv:2405.13551.\nEsteitieh, Y.; Mandal, S.; and Laliotis, G. 2025.\nTo-\nwards metacognitive clinical reasoning: Benchmarking md-\npie against state-of-the-art llms in medical decision-making.\nmedRxiv, 2025–01.\nFathy, W.; Emeriaud, G.; and Cheriet, F. 2025. A compre-\nhensive review of ICU readmission prediction models: From\nstatistical methods to deep learning approaches. Artificial\nIntelligence in Medicine, 103126.\nFerdush, J.; Begum, M.; and Hossain, S. T. 2024. ChatGPT\nand clinical decision support: scope, application, and limita-\ntions. Annals of Biomedical Engineering, 52(5): 1119–1124.\nFeuerriegel, S.; Frauen, D.; Melnychuk, V.; Schweisthal, J.;\nHess, K.; Curth, A.; Bauer, S.; Kilbertus, N.; Kohane, I. S.;\nand van der Schaar, M. 2024.\nCausal machine learning\nfor predicting treatment outcomes. Nature Medicine, 30(4):\n958–968.\nFihn, S. D.; Berlin, J. A.; Haneuse, S. J.; and Rivara, F. P.\n2024. Prediction Models and Clinical Outcomes—A Call\nfor Papers. JAMA Network Open, 7(4): e249640–e249640.\nGao, J.; Lu, Y.; Ashrafi, N.; Domingo, I.; Alaei, K.; and Pish-\ngar, M. 2024. Prediction of sepsis mortality in ICU patients\nusing machine learning methods. BMC Medical Informatics\nand Decision Making, 24(1): 228.\nHager, P.; Jungmann, F.; Holland, R.; Bhagat, K.; Hubrecht,\nI.; Knauer, M.; Vielhauer, J.; Makowski, M.; Braren, R.;\nKaissis, G.; et al. 2024. Evaluation and mitigation of the\nlimitations of large language models in clinical decision-\nmaking. Nature medicine, 30(9): 2613–2622.\nHuang, L.; Dou, Z.; Fang, F.; Zhou, B.; Zhang, P.; and\nJiang, R. 2025.\nPrediction of mortality in intensive care\nunit with short-term heart rate variability: Machine learning-\nbased analysis of the MIMIC-III database. Computers in\nBiology and Medicine, 186: 109635.\nHyland, S. L.; Faltys, M.; H¨user, M.; Lyu, X.; Gumbsch, T.;\nEsteban, C.; Bock, C.; Horn, M.; Moor, M.; Rieck, B.; et al.\n2020. Early prediction of circulatory failure in the intensive\ncare unit using machine learning. Nature medicine, 26(3):\n364–373.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and\nSzolovits, P. 2021. What disease does this patient have? a\nlarge-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14): 6421.\nJin, M.; Yu, Q.; Shu, D.; Zhang, C.; Fan, L.; Hua, W.; Zhu,\nS.; Meng, Y.; Wang, Z.; Du, M.; et al. 2024. Health-llm:\nPersonalized retrieval-augmented disease prediction system.\narXiv preprint arXiv:2402.00746.\nJohnson, A. E.; Bulgarelli, L.; Shen, L.; Gayles, A.; Sham-\nmout, A.; Horng, S.; Pollard, T. J.; Hao, S.; Moody, B.; Gow,\nB.; et al. 2023. MIMIC-IV, a freely accessible electronic\nhealth record dataset. Scientific data, 10(1): 1.\nJohnson, A. E.; Pollard, T. J.; Shen, L.; Lehman, L.-w. H.;\nFeng, M.; Ghassemi, M.; Moody, B.; Szolovits, P.; An-\nthony Celi, L.; and Mark, R. G. 2016. MIMIC-III, a freely\naccessible critical care database. Scientific data, 3(1): 1–9.\n"}, {"page": 9, "text": "Kafkas, S¸.; Abdelhakim, M.; Althagafi, A.; Toonsi, S.; Al-\nghamdi, M.; Schofield, P. N.; and Hoehndorf, R. 2025. The\napplication of Large Language Models to the phenotype-\nbased prioritization of causative genes in rare disease pa-\ntients. Scientific Reports, 15(1): 15093.\nKang, Y.; Yang, M.; Peng, Y.; Cai, J.; Zhao, L.; Gao, Z.; Li,\nN.; and Pu, B. 2025. LLM-DG: Leveraging large language\nmodel for enhanced disease prediction via inter-patient and\nintra-patient modeling. Information Fusion, 121: 103145.\nKhalifa, M.; Albadawy, M.; and Iqbal, U. 2024. Advancing\nclinical decision support: The role of artificial intelligence\nacross six domains. Computer Methods and Programs in\nBiomedicine Update, 5: 100142.\nKhwaja, A. 2012. KDIGO clinical practice guidelines for\nacute kidney injury.\nNephron Clinical Practice, 120(4):\nc179–c184.\nKiciman, E.; Ness, R.; Sharma, A.; and Tan, C. 2023. Causal\nreasoning and large language models: Opening a new fron-\ntier for causality. Transactions on Machine Learning Re-\nsearch.\nKweon, S.; Kim, J.; Kwak, H.; Cha, D.; Yoon, H.; Kim, K.;\nYang, J.; Won, S.; and Choi, E. 2024. Ehrnoteqa: An llm\nbenchmark for real-world clinical practice using discharge\nsummaries.\nAdvances in Neural Information Processing\nSystems, 37: 124575–124611.\nLagemann, K.; Lagemann, C.; Taschler, B.; and Mukherjee,\nS. 2023. Deep learning of causal structures in high dimen-\nsions under data limitations. Nature Machine Intelligence,\n5(11): 1306–1316.\nLi, J.; Cao, Y.; Huang, S.; and Chen, J. 2024. Formality is\nFavored: Unraveling the Learning Preferences of Large Lan-\nguage Models on Data with Conflicting Knowledge. arXiv\npreprint arXiv:2410.04784.\nLin, Y.; Shi, T.; and Kong, G. 2025. Acute kidney injury\nprognosis prediction using machine learning methods: a sys-\ntematic review. Kidney Medicine, 7(1): 100936.\nLiu, F.; Li, Z.; Zhou, H.; Yin, Q.; Yang, J.; Tang, X.; Luo,\nC.; Zeng, M.; Jiang, H.; Gao, Y.; et al. 2024. Large language\nmodels in the clinic: a comprehensive benchmark.\narXiv\npreprint arXiv:2405.00716.\nLiu, S.; Wright, A. P.; Patterson, B. L.; Wanderer, J. P.;\nTurer, R. W.; Nelson, S. D.; McCoy, A. B.; Sittig, D. F.;\nand Wright, A. 2023. Using AI-generated suggestions from\nChatGPT to optimize clinical decision support.\nJournal\nof the American Medical Informatics Association, 30(7):\n1237–1245.\nNaik, N.; Khandelwal, A.; Joshi, M.; Atre, M.; Wright, H.;\nKannan, K.; Hill, S.; Mamidipudi, G.; Srinivasa, G.; Bifulco,\nC.; et al. 2024. Applying large language models for causal\nstructure learning in non small cell lung cancer. In 2024\nIEEE 12th International Conference on Healthcare Infor-\nmatics (ICHI), 688–693. IEEE.\nNguyen, A. T.; Jeong, H.; Yang, E.; and Hwang, S. J. 2021.\nClinical risk prediction with temporal probabilistic asym-\nmetric multi-task learning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, 9081–9091.\nPlacido, D.; Yuan, B.; Hjaltelin, J. X.; Zheng, C.; Haue,\nA. D.; Chmura, P. J.; Yuan, C.; Kim, J.; Umeton, R.; An-\ntell, G.; et al. 2023. A deep learning algorithm to predict\nrisk of pancreatic cancer from disease trajectories. Nature\nmedicine, 29(5): 1113–1122.\nRajpurkar, P.; Chen, E.; Banerjee, O.; and Topol, E. J. 2022.\nAI in health and medicine. Nature medicine, 28(1): 31–38.\nRichens, J. G.; Lee, C. M.; and Johri, S. 2020. Improving the\naccuracy of medical diagnosis with causal machine learning.\nNature communications, 11(1): 3923.\nSandmann, S.; Riepenhausen, S.; Plagwitz, L.; and Vargh-\nese, J. 2024. Systematic analysis of ChatGPT, Google search\nand Llama 2 for clinical decision support tasks. Nature com-\nmunications, 15(1): 2050.\nShimizu, S.; Inazumi, T.; Sogawa, Y.; Hyvarinen, A.; Kawa-\nhara, Y.; Washio, T.; Hoyer, P. O.; Bollen, K.; and Hoyer,\nP. 2011. DirectLiNGAM: A direct method for learning a\nlinear non-Gaussian structural equation model. Journal of\nMachine Learning Research-JMLR, 12(Apr): 1225–1248.\nSinger, M.; Deutschman, C. S.; Seymour, C. W.; Shankar-\nHari, M.; Annane, D.; Bauer, M.; Bellomo, R.; Bernard,\nG. R.; Chiche, J.-D.; Coopersmith, C. M.; et al. 2016. The\nthird international consensus definitions for sepsis and septic\nshock (Sepsis-3). Jama, 315(8): 801–810.\nSinghal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn, E.;\nAmin, M.; Hou, L.; Clark, K.; Pfohl, S. R.; Cole-Lewis, H.;\net al. 2025. Toward expert-level medical question answering\nwith large language models. Nature Medicine, 31(3): 943–\n950.\nSpirtes, P.; Glymour, C. N.; and Scheines, R. 2000. Causa-\ntion, prediction, and search. MIT press.\nThirunavukarasu, A. J.; Ting, D. S. J.; Elangovan, K.;\nGutierrez, L.; Tan, T. F.; and Ting, D. S. W. 2023. Large lan-\nguage models in medicine. Nature medicine, 29(8): 1930–\n1940.\nVan Smeden, M.; Reitsma, J. B.; Riley, R. D.; Collins, G. S.;\nand Moons, K. G. 2021. Clinical prediction models: diagno-\nsis versus prognosis. Journal of clinical epidemiology, 132:\n142–145.\nVan Sonsbeek, T.; Derakhshani, M. M.; Najdenkoska, I.;\nSnoek, C. G.; and Worring, M. 2023. Open-ended medi-\ncal visual question answering through prefix tuning of lan-\nguage models. In International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention, 726–\n736. Springer.\nVan Veen, D.; Van Uden, C.; Blankemeier, L.; Delbrouck,\nJ.-B.; Aali, A.; Bluethgen, C.; Pareek, A.; Polacin, M.; Reis,\nE. P.; Seehofnerov´a, A.; et al. 2024. Adapted large language\nmodels can outperform medical experts in clinical text sum-\nmarization. Nature medicine, 30(4): 1134–1142.\nWang, H.; Gao, C.; Dantona, C.; Hull, B.; and Sun, J.\n2024a.\nDRG-LLaMA: tuning LLaMA model to predict\ndiagnosis-related group for hospitalized patients. NPJ digi-\ntal medicine, 7(1): 16.\n"}, {"page": 10, "text": "Wang, J.; Ahn, S.; Dalal, T.; Zhang, X.; Pan, W.; Zhang, Q.;\nChen, B.; Dodge, H. H.; Wang, F.; and Zhou, J. 2024b. Aug-\nmented Risk Prediction for the Onset of Alzheimer’s Dis-\nease from Electronic Health Records with Large Language\nModels. arXiv preprint arXiv:2405.16413.\nWang, L.; Guo, X.; Shi, H.; Ma, Y.; Bao, H.; Jiang, L.; Zhao,\nL.; Feng, Z.; Zhu, T.; and Lu, L. 2025. CRISP: A causal\nrelationships-guided deep learning framework for advanced\nICU mortality prediction.\nBMC Medical Informatics and\nDecision Making, 25(1): 165.\nWang, X.; Du, Y.; Zhu, S.; Ke, L.; Chen, Z.; Hao, J.; and\nWang, J. 2021. Ordering-based causal discovery with rein-\nforcement learning. arXiv preprint arXiv:2105.06631.\nXu, X.; Yao, B.; Dong, Y.; Gabriel, S.; Yu, H.; Hendler, J.;\nGhassemi, M.; Dey, A. K.; and Wang, D. 2024. Mental-llm:\nLeveraging large language models for mental health predic-\ntion via online text data. Proceedings of the ACM on In-\nteractive, Mobile, Wearable and Ubiquitous Technologies,\n8(1): 1–32.\nYam, H. M.; and Paek, N. J. 2024. What should baby models\nread? Exploring sample-efficient data composition on model\nperformance. arXiv preprint arXiv:2411.06672.\nYeh, Y.-C.; Kuo, Y.-T.; Kuo, K.-C.; Cheng, Y.-W.; Liu, D.-\nS.; Lai, F.; Kuo, L.-C.; Lee, T.-J.; Chan, W.-S.; Chiu, C.-\nT.; et al. 2024. Early prediction of mortality upon intensive\ncare unit admission. BMC Medical Informatics and Deci-\nsion Making, 24: 394.\nZanga, A.; Ozkirimli, E.; and Stella, F. 2022. A survey on\ncausal discovery: theory and practice. International Journal\nof Approximate Reasoning, 151: 101–129.\nZeng, H.; Yin, C.; Chai, C.; Wang, Y.; Dai, Q.; and Sun,\nH. 2025.\nCancer gene identification through integrating\ncausal prompting large language model with omics data–\ndriven causal inference. Briefings in Bioinformatics, 26(2).\nZhang, K.; Zhu, S.; Kalander, M.; Ng, I.; Ye, J.; Chen, Z.;\nand Pan, L. 2021. gcastle: A python toolbox for causal dis-\ncovery. arXiv preprint arXiv:2111.15155.\nZheng, X.; Ji, S.; Sun, J.; Chen, R.; Gao, W.; and Srivas-\ntava, M. 2025.\nProMind-LLM: Proactive Mental Health\nCare via Causal Reasoning with Sensor Data. arXiv preprint\narXiv:2505.14038.\nZhou, W.; and Chen, Q. 2022. A survey on causal discov-\nery. In China Conference on Knowledge Graph and Seman-\ntic Computing, 123–135. Springer.\nZhou, Y.; Wu, X.; Huang, B.; Wu, J.; Feng, L.; and Tan,\nK. C. 2024.\nCausalbench: A comprehensive benchmark\nfor causal learning capability of llms.\narXiv preprint\narXiv:2404.06349.\nZhu, T.; Dong, D.; Qu, X.; Ruan, J.; Chen, W.; and Cheng,\nY. 2024. Dynamic data mixing maximizes instruction tuning\nfor mixture-of-experts. arXiv preprint arXiv:2406.11256.\n"}, {"page": 11, "text": "Appendix\nContents\nA Reproducibility Statement\n1\nA.1\nLLM Implement Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\nA.2\nCausal Discovery Implement Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\nB\nExperiment Results\n2\nB.1\nBaseline performance - Results of ML Models on MIMIC-III & MIMIC-IV . . . . . . . . . . . . . . . . . . . .\n2\nB.2\nBaseline performance - Results of Directly Prompting LLMs on MIMIC-III & MIMIC-IV\n. . . . . . . . . . . .\n7\nB.3\nPrompt engineering - Results of Directly LLMs with Prompt engineering on MIMIC-III & MIMIC-IV . . . . . .\n17\nB.4\nInput format sensitivity - Results of Directly Prompting LLMs on MIMIC-III & MIMIC-IV . . . . . . . . . . . .\n19\nB.5\nCausal feature - Causal Discovery Features, LLM-Assisted Causal Features . . . . . . . . . . . . . . . . . . . .\n19\nC Examples of Different Input format\n22\nC.1\nNarrative Serialization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.2\nRow-Column Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.3\nJSON Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.4\nLaTeX Table Format\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.5\nTemplate-Based Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nD Examples of LLM-Assisted Causal Feature Editing\n27\nD.1\nLLM edited CD feature sets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nD.2\nLLM-Generated causal feature sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nE\nExamples of LLM-Based Prediction\n35\nE.1\nDirectly Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nE.1.1\nLength-of-hospital-stay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nE.1.2\nICU admission within 12 hours of hospital admission . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nE.2\nChain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nE.2.1\nMultiple ICU admissions during a single hospitalization\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nE.3\nSelf-Reflection Prompting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nE.3.1\nSepsis onset during the ICU stay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nE.4\nRole-Playing Prompting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nE.4.1\nAKI during the ICU stay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nE.5\nIn-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nE.5.1\n30-day hospital readmission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nE.5.2\nIn-hospital mortality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nF\nRelated Work\n42\n"}, {"page": 12, "text": "A\nReproducibility Statement\nA.1\nLLM Implement Details\nOur experimental setup utilized 3 NVIDIA RTX A800 GPUs. Our code is built upon the HuggingFace Transformers framework\n(https://huggingface.co/docs/transformers/en/index). To ensure deterministic outcomes, all Large\nLanguage Model (LLM) inferences were performed using Greedy Decoding (i.e., temperature=0, do sample=False). For full\ntransparency and to facilitate replication, our source code and complete results are publicly available at our project website\nhttps://anonymous.4open.science/r/REACT_LLM-5DD1.\nOur benchmark evaluates 14 general-purpose LLMs, including Qwen3-8B, Qwen3-14B, Qwen3-235B [37], Llama-3.1-405b\n[13], DeepSeek-R1 [14], DeepSeek-V3 [24], Gemini-2-Flash, Gemini-2-Pro [28], GPT-4o-mini [26], GPT-o1 [19], GPT-4o [17],\nClaude-4 [5], GPT-o3-mini [27], Claude-3.5-Haiku [3], Claude-3.7-Sonnet [4]. We obtained the model weights for all publicly\navailable models from the Hugging Face repository (https://huggingface.co/). For the proprietary model commercial\nmodels in our benchmark accessible only via API, we specify the official provider and the precise model identifier used for\nour calls to maintain transparency and facilitate future replication. The specific download links and their corresponding official\nsources are as follows:\n• Qwen3-8B: https://huggingface.co/Qwen/Qwen3-8B\n• Qwen3-14B: https://huggingface.co/Qwen/Qwen3-14B\n• Qwen3-235B: https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\n• Llama-3.1-405B: https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct\n• DeepSeek-R1: https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\n• DeepSeek-V3: https://huggingface.co/deepseek-ai/DeepSeek-V3\n• Gemini-2-Pro & Gemini-2-Flash: https://ai.google.dev/models/gemini\n• GPT-o1: https://openai.com/zh-Hans-CN/o1/\n• GPT-o3-mini: https://platform.openai.com/docs/models/o3-mini\n• GPT-4o: https://platform.openai.com/docs/models/chatgpt-4o-latest\n• GPT-4o-mini: https://platform.openai.com/docs/models/gpt-4o-mini\n• Claude Models (Claude-4, 3.5-Haiku, 3.7-Sonnet): https://docs.anthropic.com/en/docs/about-claud\ne/models/overview\nA.2\nCausal Discovery Implement Details\ngCastle (https://gcastle.readthedocs.io/en/latest/index.html) is a causal discovery toolkit developed by\nHuawei Noah’s Ark Lab. We utilized its Function-based DirectLiNGAM algorithm, Score-based GES algorithm, and Gradient-\nbased CORL algorithm for our analysis.\nThe GES algorithm was configured with the parameters criterion=’bic’ and method=’scatter’. For the CORL\nalgorithm, we used the settings encoder name=’transformer’, decoder name=’lstm’, reward mode=’episodic’,\nreward regression type=’LR’, batch size=len(feature), input dim=len(feature), embed dim=64,\nand device type=’gpu’. The DirectLiNGAM algorithm was applied with its default parameters.\n1\n"}, {"page": 13, "text": "B\nExperiment Results\nB.1\nBaseline performance - Results of ML Models on MIMIC-III & MIMIC-IV\nOutcome\nInHospDeath\nReadmit30\nMultiICU\nSepsisICU\nAKIICU\nLOS\nEarlyICU\nPositive Ratio\n27.0%\n14.6%\n22.0%\n62.4%\n47.4%\n31.3%\n65.3%\nMetric\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nMachine Learning Models\nSVM\n0.9368 0.8726 0.7120 0.2524 0.7580 0.4992 0.8680 0.9129 0.7778 0.7535 0.8051 0.7035 0.6353 0.7309\nLR\n0.9452 0.8836 0.7565 0.2746 0.7533 0.4981 0.8720 0.9137 0.7879 0.7760 0.8269 0.7220 0.6435 0.7616\nDT\n0.8029 0.7624 0.5484 0.2747 0.5587 0.3805 0.7416 0.8735 0.6268 0.6971 0.6333 0.5794 0.6190 0.8240\nRF\n0.9419 0.8780 0.7084 0.2737 0.6970 0.4068 0.8943 0.9337 0.7967 0.7728 0.7928 0.6707 0.7607 0.8416\nAdaboost\n0.9207 0.8373 0.7028 0.2330 0.7146 0.4345 0.8676 0.9143 0.7778 0.7540 0.7673 0.6361 0.7066 0.8137\nXGboost\n0.9490 0.8843 0.7359 0.2698 0.6985 0.4474 0.8907 0.9212 0.8025 0.7871 0.8127 0.6931 0.7584 0.8412\nOpen-source Large Language Models\nQwen3-8b\n0.8747 0.7458 0.4258 0.1044 0.5477 0.2153 0.7338 0.8356 0.7396 0.6885 0.6072 0.3760 0.5455 0.6738\nQwen3-235b\n0.8807 0.7674 0.3639 0.0944 0.5546 0.2234 0.7608 0.8494 0.7488 0.6962 0.5996 0.2657 0.5603 0.7859\nDeepSeek-R1\n0.8860 0.7445 0.3875 0.1021 0.5471 0.2285 0.7341 0.8390 0.7300 0.6924 0.5891 0.3534 0.5189 0.7198\nProprietary Large Language Models\nGemini-2-Pro\n0.9167 0.8186 0.3883 0.0984 0.5393 0.2185 0.7568 0.8288 0.7517 0.7296 0.6616 0.4532 0.5016 0.6611\nGemini-2-Flash 0.8975 0.7578 0.4210 0.1044 0.5697 0.2558 0.7267 0.8312 0.7301 0.6792 0.6494 0.4808 0.5128 0.7886\nGPT-o3-mini\n0.9012 0.7581 0.5131 0.1333 0.5800 0.2405 0.7314 0.8334 0.7302 0.6969 0.6098 0.3906 0.5942 0.7888\nTable 1: Baseline performance of LLMs and traditional ML models across 7 clinical prediction tasks on MIMIC-IV. Bolded\nscores indicate the best performance within each model category.\n2\n"}, {"page": 14, "text": "Figure 1: AUROC performance of MLs across 7 clinical prediction tasks on MIMIC-III.\n3\n"}, {"page": 15, "text": "Figure 2: AUPRC performance of MLs across 7 clinical prediction tasks on MIMIC-III.\n4\n"}, {"page": 16, "text": "Figure 3: AUROC performance of MLs across 7 clinical prediction tasks on MIMIC-IV.\n5\n"}, {"page": 17, "text": "Figure 4: AUPRC performance of MLs across 7 clinical prediction tasks on MIMIC-IV.\n6\n"}, {"page": 18, "text": "B.2\nBaseline performance - Results of Directly Prompting LLMs on MIMIC-III & MIMIC-IV\nFigure 5: AUROC performance of LLMs across 7 clinical prediction tasks on MIMIC-III.\n7\n"}, {"page": 19, "text": "Figure 6: AUPRC performance of LLMs across 7 clinical prediction tasks on MIMIC-III.\n8\n"}, {"page": 20, "text": "Figure 7: F1 performance of LLMs across 7 clinical prediction tasks on MIMIC-III.\n9\n"}, {"page": 21, "text": "Figure 8: Confusion Matrix performance of LLMs across 7 clinical prediction tasks on MIMIC-III.\n10\n"}, {"page": 22, "text": "Figure 9: Predictive probability performance of LLMs across 7 clinical prediction tasks on MIMIC-III.\n11\n"}, {"page": 23, "text": "Figure 10: AUROC performance of LLMs across 7 clinical prediction tasks on MIMIC-IV.\n12\n"}, {"page": 24, "text": "Figure 11: AUPRC performance of LLMs across 7 clinical prediction tasks on MIMIC-IV.\n13\n"}, {"page": 25, "text": "Figure 12: F1 performance of LLMs across 7 clinical prediction tasks on MIMIC-IV.\n14\n"}, {"page": 26, "text": "Figure 13: Confusion Matrix performance of LLMs across 7 clinical prediction tasks on MIMIC-IV.\n15\n"}, {"page": 27, "text": "Figure 14: Predictive probability performance of LLMs across 7 clinical prediction tasks on MIMIC-IV.\n16\n"}, {"page": 28, "text": "B.3\nPrompt engineering - Results of Directly LLMs with Prompt engineering on MIMIC-III &\nMIMIC-IV\nFigure 15: AUPRC of LLMs with different Prompt Engineering on MIMIC-III.\nFigure 16: F1 of LLMs with different Prompt Engineering on MIMIC-III.\n17\n"}, {"page": 29, "text": "Figure 17: F1 of LLMs with different Prompt Engineering on MIMIC-IV.\nFigure 18: AUROC of LLMs with different Prompt Engineering on MIMIC-IV.\n18\n"}, {"page": 30, "text": "B.4\nInput format sensitivity - Results of Directly Prompting LLMs on MIMIC-III & MIMIC-IV\nFigure 19: Average Performance of LLMs with Different Input Formats on MIMIC-IV.\nB.5\nCausal feature - Causal Discovery Features, LLM-Assisted Causal Features\nFigure 20: Average F1 of LLMs across 7 outcomes on MIMIC-III and MIMIC-IV.\n19\n"}, {"page": 31, "text": "Figure 21: AUROC of LLMs across 7 outcomes on MIMIC-III.\nFigure 22: F1 of LLMs across 7 outcomes on MIMIC-III.\n20\n"}, {"page": 32, "text": "Figure 23: AUROC of LLMs across 7 outcomes on MIMIC-III.\nFigure 24: F1 of LLMs across 7 outcomes on MIMIC-III.\n21\n"}, {"page": 33, "text": "C\nExamples of Different Input format\nC.1\nNarrative Serialization\nExample of LLMs with Narrative Serialization for 30-day hospital readmission\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis feature-list>\nN Procedures information: <Procedures feature-list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage feature-list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS feature-list>\n® Your Task:\nWill the patient die in hospital because of the above situation?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 25: Prompt template for the Narrative Serialization method. This approach converts structured Electronic Health Record\n(EHR) data into a human-readable, semi-structured text format for clinical prediction tasks.\n22\n"}, {"page": 34, "text": "C.2\nRow-Column Format\nExample of LLMs with Row-Column Format for 30-day hospital readmission\nGENDER, ADMISSION TYPE, FIRST CAREUNIT, AGE, Arterial Thrombotic Events, AIDS\nDiagnosis, Abnormalities of heart beat, ...<other clinical features>\nMale, EW EMER., Medical Intensive Care Unit (MICU), 53.0, 1.0, 1, 1.0, ...\n<other values matching the header structure>\n® Your Task:\nWill the patient be readmitted to hospital within 30 days after discharge?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 26: Prompt template illustrating the Row-Column serialization format. This method represents patient data in a compact,\ntwo-row tabular structure, with the first row defining features and the second containing corresponding values.\n23\n"}, {"page": 35, "text": "C.3\nJSON Encoding\nExample of LLMs with JSON Encoding for 30-day hospital readmission\n{\n\"basic_information\": {\n\"gender\": \"Male\",\n\"admission_type\": \"EW EMER.\",\n\"first_careunit\": \"Medical Intensive Care Unit (MICU)\",\n\"age\": 53.0\n},\n\"diagnoses\": {\n\"Arterial Thrombotic Events\": {\n\"value\": 1,\n\"original_feature\": \"Diag_ATE_filtered\"\n},\n...<other blocks of clinical features and values>\n}\n}\n® Your Task:\nWill the patient be readmitted to hospital within 30 days after discharge?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 27: Prompt template for the JSON Encoding method. This format serializes clinical data into a structured JSON object,\ntesting the LLM’s ability to parse and reason over machine-readable data structures.\n24\n"}, {"page": 36, "text": "C.4\nLaTeX Table Format\nExample of LLMs with LaTeX Table Format for 30-day hospital readmission\n  Patient Basic information\nGENDER\nADMISSION TYPE\nFIRST CAREUNIT\nAGE\nGENDER\nADMISSION TYPE\nFIRST CAREUNIT\nAGE\nÁ Diagnosis information:\nArterial Thrombotic Events\nAIDS Diagnosis\nAbnormalities of heart beat\n...\n1\n1\n1\n...\nN Procedures information:\nFluoroscopy of Multiple Coronary Arteries using Other Contrast\n...\n1\n...\n Medications administered during the first 24 hours after ICU admission:\nAcetaminophen Usage\nAspirin Usage\nCalcium Gluconate Usage\nMean Blood Pressure\n...\n<value>\n<value>\n<value>\n<value>\n...\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\nHeart Rate\nSystolic Blood Pressure\nDiastolic Blood\nMean Blood Pressure\n...\n<value>\n<value>\n<value>\n<value>\n...\n® Your Task:\nWill the patient be readmitted to hospital within 30 days after discharge?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 28: Prompt template for the LaTeX Table serialization method. Patient features are encoded using LaTeX syntax, evalu-\nating the model’s proficiency in handling complex markup languages for data interpretation.\n25\n"}, {"page": 37, "text": "C.5\nTemplate-Based Natural Language\nExample of LLMs with Template-Based Natural Language for 30-day hospital readmission\nA <Age> -year-old <Gender> patient was admitted to the <First careunit> as <Admission type> . The\npatient is diagnosed with <Diagnosis feature-list containing values> . The patient has undergone\n<Procedure feature-list containing values> . During the first 24 hours after ICU admission, the pa-\ntient received <Medicine Usage feature-list containing values> . Laboratory tests and vital signs\nrecorded during the first 24 hours showed <TS feature-list containing values> .\n® Your Task:\nWill the patient be readmitted to hospital within 30 days after discharge?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 29: Prompt template for the Template-Based Natural Language approach. This method synthesizes structured EHR data\ninto a cohesive clinical narrative, presenting the information to the LLM in a fluent, paragraph-based format.\n26\n"}, {"page": 38, "text": "D\nExamples of LLM-Assisted Causal Feature Editing\nD.1\nLLM edited CD feature sets\nExamples of GPT-o3-mini edited CD feature sets\n  Input\nYou are a clinical expert with extensive experience in intensive care medicine. Your task is to optimize the causal\nfeature sets for various clinical prognostic outcomes generated by the GES causal discovery algorithm, based on\nestablished clinical knowledge and clinical experience.\n1. All Available Features ( <amount of all features> features): <feature-list>\n2. Feature Sets to be Optimized: <GES-based feature-list>\n3. Your Task and Output Requirements:\nTarget Outcome Clinical Context: <label> : <description>\nPlease consider the specific pathophysiology and clinical risk factors associated with <label> when making your\noptimization decisions.\nComplete the following tasks:\n1. Add features that you believe are clinically causally related to <label> but were missed.\n2. Remove features that you believe are not clinically causally related to <label> .\n3. Return ONLY the changes (additions and removals) in the following JSON format. You must provide clear,\nconcise clinical explanations for each decision.\n{\n\"<label>\": {\n\"added_features\": [\n{\n\"feature\": \"Suggested_Addition_Feature_Description\",\n\"reason\": \"Clinical explanation: Why this feature is crucial ...\"\n}\n],\n\"removed_features\": [\n{\n\"feature\": \"Suggested_Removal_Feature_Description\",\n\"reason\": \"Clinical explanation: Why this feature should be ...\"\n}\n]\n}\n}\nPlease ensure your response is valid JSON format and includes clinical reasoning for all modifications.\nÁ Output\n27\n"}, {"page": 39, "text": "{\n\"DIEINHOSPITAL\": {\n\"added_features\": [\n{\n\"feature\": \"Age\",\n\"reason\": \"Clinical explanation: Age is a well-established risk ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Metronidazole Usage\",\n\"reason\": \"Clinical explanation: Antibiotic usage such as ...\"\n},\n...\n}\n{\n\"FirstICU24_AKI_ALL\": {\n\"added_features\": [\n{\n\"feature\": \"Chronic Kidney Disease Group\",\n\"reason\": \"Patients with established chronic kidney disease ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Amiodarone Usage\",\n\"reason\": \"Amiodarone is used primarily for arrhythmia management...\"\n},\n...\n}\n{\n\"Multiple_ICUs\": {\n\"added_features\": [\n{\n\"feature\": \"Charlson Comorbidity Index\",\n\"reason\": \"Clinical explanation: A higher comorbidity burden ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Vancomycin Usage\",\n\"reason\": \"Clinical explanation: Although vancomycin is used in...\"\n},\n...\n}\n28\n"}, {"page": 40, "text": "{\n\"Readmission_30\": {\n\"added_features\": [\n{\n\"feature\": \"Charlson Comorbidity Index\",\n\"reason\": \"Clinical explanation: This index summarizes overall ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Hereditary factor VIII deficiency\",\n\"reason\": \"Clinical explanation: This is a rare condition with ...\"\n},\n...\n}\n{\n\"LOS_Hospital\": {\n\"added_features\": [\n{\n\"feature\": \"White Blood Cell Count\",\n\"reason\": \"Clinical explanation: Elevated WBC counts indicate ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Acetaminophen Usage\",\n\"reason\": \"Clinical explanation: As a common symptomatic ...\"\n},\n...\n}\n{\n\"sepsis_all\": {\n\"added_features\": [\n{\n\"feature\": \"White Blood Cell Count\",\n\"reason\": \"Clinical explanation: White Blood Cell Count is a key ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Non-ionized Calcium\",\n\"reason\": \"Clinical explanation: While electrolytes can be ...\"\n},\n...\n}\n29\n"}, {"page": 41, "text": "{\n\"ICU_within_12hr_of_admit\": {\n\"added_features\": [\n{\n\"feature\": \"Acute Respiratory Failure Diagnosis\",\n\"reason\": \"Patients presenting with acute respiratory failure ...\"\n},\n...\n],\n\"removed_features\": [\n{\n\"feature\": \"Multi-Drug Resistant Organisms before ICU\",\n\"reason\": \"While important in infection management, a history of ...\"\n},\n...\n}\nFigure 30: Prompt template for LLM-assisted causal feature set editing. The LLM is instructed to act as a clinical expert to refine\na pre-existing feature set (generated by the GES algorithm) by adding or removing features and providing clinical justifications.\nD.2\nLLM-Generated causal feature sets\nExamples of GPT-o3-mini-Generated causal feature sets for Length-of-hospital-stay\n  Input\nYou are a clinical expert with extensive experience in intensive care medicine. Your task is to identify features that\nhave DIRECT or INDIRECT causal relationships with the outcome based on established clinical knowledge and\nclinical experience.\n1. All Available Features (organized by category):\nDiag features ( <amount of diagnosis features> features):\n<Diagnosis feature-list>\nProc features ( <amount of procedure features> features):\n<Procedure feature-list>\nMed features ( <amount of medicine features> features):\n<Medicine Usage feature-list containing values>\nTS features ( <amount of TS features> features): <TS feature-list containing values>\n2.\nYou need to analyze the above features to identify those that have direct or indirect causal relationships with\n<label description> .\nConsider the following:\n- DIRECT causal relationships: Features that directly cause or strongly predict the outcome\n- INDIRECT causal relationships: Features that are part of the causal pathway, represent underlying pathophysiol-\nogy, or are established risk factors\n- Include features that reflect disease severity, organ dysfunction, or treatment interventions related to the outcome\n- EXCLUDE features that are purely correlational without established causal basis\n- EXCLUDE features that are consequences rather than causes of the outcome\n3. Selection Requirements:\n- Select features that have direct or indirect causal relationships with <label description>\n- MINIMUM feature count per category:\n* Diag (Diagnosis): At least 0 features (10% of <amount of diagnosis features> )\n30\n"}, {"page": 42, "text": "* Proc (Procedures): At least 0 features (10% of <amount of procedure features> )\n* Med (Medications): At least 0 features (10% of <amount of medicine features> )\n* TS (Time Series): At least 0 features (10% of <amount of TS features> )\n- You may select more than the minimum if clinically relevant\n4. Output Requirements:\nReturn your analysis in the following JSON format with feature descriptions EXACTLY matching those provided in\nthe feature list:\n{\n\"<label>\": {\n\"selected_features\": {\n\"Diag\": [\"feature_description_1\", \"feature_description_2\", ...],\n\"Proc\": [\"feature_description_1\", \"feature_description_2\", ...],\n\"Med\": [\"feature_description_1\", \"feature_description_2\", ...],\n\"TS\": [\"feature_description_1\", \"feature_description_2\", ...],\n},\n},\n}\nIMPORTANT:\n- Use EXACT feature descriptions as provided in the feature list\n- Organize selections by category (Diag, Proc, Med, TS)\n- Meet minimum requirements for each category unless providing valid exemption\n- Base selections on established clinical evidence and pathophysiology\n- Provide valid JSON format without additional explanations\nÁ Output\n{\n\"DIEINHOSPITAL\": {\n\"selected_features\": {\n\"Diag\": [\n\"AKI Related Group\",\n...\n],\n\"Proc\": [\n\"\"Continuous invasive mechanical ventilation for 96...\"\n...\n],\n\"Med\": [\n\"Furosemide Usage\",\n...\n],\n\"TS\": [\n\"Blood Oxygen Saturation\",\n...\n]\n},\n},\n}\n31\n"}, {"page": 43, "text": "{\n\"Readmission_30\": {\n\"selected_features\": {\n\"Diag\": [\n\"Chronic Kidney Disease Group\",\n...\n],\n\"Proc\": [\n\"Central venous catheter placement with guidance\"\n...\n],\n\"Med\": [\n\"Norepinephrine Usage\",\n...\n],\n\"TS\": [\n\"Heart Rate\",\n...\n]\n},\n},\n}\n{\n\"Multiple_ICUs\": {\n\"selected_features\": {\n\"Diag\": [\n\"Acute Respiratory Failure Diagnosis\",\n...\n],\n\"Proc\": [\n\"Hemodialysis\"\n...\n],\n\"Med\": [\n\"Norepinephrine Usage\",\n...\n],\n\"TS\": [\n\"Heart Rate\",\n...\n]\n},\n},\n}\n32\n"}, {"page": 44, "text": "{\n\"sepsis_all\": {\n\"selected_features\": {\n\"Diag\": [\n\"Urinary Tract Infection Group\",\n...\n],\n\"Proc\": [\n\"Central venous catheter placement with guidance\"\n...\n],\n\"Med\": [\n\"Prednisone Usage\",\n...\n],\n\"TS\": [\n\"Heart Rate\",\n...\n]\n},\n},\n}\n{\n\"FirstICU24_AKI_ALL\": {\n\"selected_features\": {\n\"Diag\": [\n\"AKI Related Group\",\n...\n],\n\"Proc\": [\n\"Extracorporeal circulation auxiliary to open heart surgery\"\n...\n],\n\"Med\": [\n\"Vancomycin Usage\",\n...\n],\n\"TS\": [\n\"Heart Rate\",\n...\n]\n},\n},\n}\n33\n"}, {"page": 45, "text": "{\n\"LOS_Hospital\": {\n\"selected_features\": {\n\"Diag\": [\n\"Acute Respiratory Failure Diagnosis\",\n...\n],\n\"Proc\": [\n\"Hemodialysis\"\n...\n],\n\"Med\": [\n\"Norepinephrine Usage\",\n...\n],\n\"TS\": [\n\"Heart Rate\",\n...\n]\n},\n},\n}\n{\n\"ICU_within_12hr_of_admit\": {\n\"selected_features\": {\n\"Diag\": [\n\"Acute Respiratory Failure Diagnosis\",\n...\n],\n\"Proc\": [\n\"Central venous catheter placement with guidance\"\n...\n],\n\"Med\": [\n\"Norepinephrine Usage\",\n...\n],\n\"TS\": [\n\"Heart Rate\",\n...\n]\n},\n},\n}\nFigure 31: Prompt template for causal feature generation by an LLM. Acting as a clinical expert, the LLM selects a causally\nrelevant feature set from a comprehensive list of available clinical variables based on provided guidelines.\n34\n"}, {"page": 46, "text": "E\nExamples of LLM-Based Prediction\nE.1\nDirectly Prompting\nE.1.1\nLength-of-hospital-stay\nExample of LLMs with Directly Prompting for Length-of-hospital-stay\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis feature-list>\nN Procedures information: <Procedures feature-list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage feature-list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS feature-list>\n® Your Task:\nWill the patient have a prolonged hospital stay (longer than average)?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 32: An example of the Direct Prompting technique, here shown for the length-of-stay prediction task. This baseline\napproach presents serialized patient data followed by a direct query for the outcome.\n35\n"}, {"page": 47, "text": "E.1.2\nICU admission within 12 hours of hospital admission\nExample of LLMs with Directly Prompting for ICU admission within 12 hours of hospital admission\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis feature-list>\nN Procedures information: <Procedures feature-list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage feature-list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS feature-list>\n® Your Task:\nWill the patient be admitted to ICU within 12 hours of hospital admission?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 33: An example of the Direct Prompting technique, in this case applied to predicting ICU admission within 12 hours. This\nbaseline approach presents serialized patient data followed by a direct query for the outcome.\n36\n"}, {"page": 48, "text": "E.2\nChain-of-Thought Prompting\nE.2.1\nMultiple ICU admissions during a single hospitalization\nExample of LLMs with Chain-of-Thought Prompting for Multiple ICU admissions during a single hospitalization\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis feature-list>\nN Procedures information: <Procedures feature-list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage feature-list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS feature-list>\n® Your Task:\nWill the patient require multiple ICU stays during this hospitalization?\nI know you are not a medical professional, but you are forced to make this prediction.\nPlease provide your concise reasoning steps for the prediction(no more than 3 steps), and finally answer with the\nprobability as a number between 0 and 1.\nFigure 34: An example of the Chain-of-Thought (CoT) prompting strategy, here applied to predicting multiple ICU admissions.\nThis technique instructs the model to provide explicit reasoning steps prior to its final answer.\n37\n"}, {"page": 49, "text": "E.3\nSelf-Reflection Prompting\nE.3.1\nSepsis onset during the ICU stay\nExample of LLMs with Self-Reflection Prompting for Sepsis onset during the ICU stay\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis feature-list>\nN Procedures information: <Procedures feature-list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage feature-list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS feature-list>\n® Your Task:\nWill the patient develop sepsis during this hospitalization?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. First answer with a number. Then conduct a concise\nreflection. Finally output your answer again with a number.\nFigure 35: An example of the Self-Reflection prompting technique, in this case for sepsis onset prediction. This method requires\nthe model to generate an initial answer, reflect on its reasoning, and then provide a final, potentially revised, answer.\n38\n"}, {"page": 50, "text": "E.4\nRole-Playing Prompting\nE.4.1\nAKI during the ICU stay\nExample of LLMs with Role-Playing Prompting for AKI during the ICU stay\nImagine that you are a doctor. Today, you’re seeing a patient with the following profile:\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis feature-list>\nN Procedures information: <Procedures feature-list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage feature-list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS feature-list>\n® Your Task:\nWill the patient develop acute kidney injury (AKI) within the first 24 hours of ICU admission?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 36: An example of the Role-Playing prompting technique, here demonstrated for Acute Kidney Injury (AKI) prediction.\nThis approach assigns a specific persona, such as a ”doctor,” to the LLM to contextualize the prediction task.\nE.5\nIn-Context Learning\nE.5.1\n30-day hospital readmission\nExample of In-Context Learning for 30-day hospital readmission\n  Patient Basic information\nGender: Male\nAdmission type: EMERGENCY\nFirst careunit: Medical Intensive Care Unit (MICU)\nAge: 46.0\nÁ Diagnosis information: Arterial Thrombotic Events, AIDS Diagnosis, Abnormalities of\nheart beat, Hereditary factor VIII deficiency, Mild Liver Disease Diagnosis,\nEssential (primary) hypertension, Alcoholic liver disease, Other disorders of\nfluid, electrolyte and acid-base balance\nN Procedures information: Not available\n Medications administered during the first 24 hours after ICU admission:\nAcetaminophen Usage = 1000.0, Aspirin Usage = 650.0, Calcium Gluconate Usage =\n8.0, Magnesium Sulfate Usage = 8.0, Norepinephrine Usage = 8.0\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n39\n"}, {"page": 51, "text": "Heart Rate=120.0; Systolic Blood Pressure=111.0; Diastolic Blood Pressure=65.0;\nMean Blood Pressure=75.0; Respiratory Rate=25.5; Temperature (Celsius)=37.165;\nOxygen Flow Rate=2.0; ...\nAnswer: 1.0\n<Case 2>\n<Case 3>\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis features list>\nN Procedures information: <Procedures features list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage features list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS features list>\n® Your Task:\nWill the patient be readmitted to hospital within 30 days after discharge?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 37: An example of the In-Context Learning (ICL) strategy, here applied to the 30-day readmission task. This few-shot\ntechnique provides the model with several complete examples (case-answer pairs) within the prompt to guide its prediction.\nE.5.2\nIn-hospital mortality\nExample of In-Context Learning for In-hospital mortality\n  Patient Basic information\nGender: Male\nAdmission type: EMERGENCY\nFirst careunit: Medical Intensive Care Unit (MICU)\nAge: 46.0\nÁ Diagnosis information: Arterial Thrombotic Events, AIDS Diagnosis, Abnormalities of\nheart beat, Hereditary factor VIII deficiency, Mild Liver Disease Diagnosis,\nEssential (primary) hypertension, Alcoholic liver disease, Other disorders of\nfluid, electrolyte and acid-base balance\nN Procedures information: Not available\n40\n"}, {"page": 52, "text": " Medications administered during the first 24 hours after ICU admission:\nAcetaminophen Usage = 1000.0, Aspirin Usage = 650.0, Calcium Gluconate Usage =\n8.0, Magnesium Sulfate Usage = 8.0, Norepinephrine Usage = 8.0\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\nHeart Rate=120.0; Systolic Blood Pressure=111.0; Diastolic Blood Pressure=65.0;\nMean Blood Pressure=75.0; Respiratory Rate=25.5; Temperature (Celsius)=37.165;\nOxygen Flow Rate=2.0; ...\nAnswer: 0.0\n<Case 2>\n<Case 3>\n  Patient Basic information\nGender: <Gender>\nAdmission type: <Admission type>\nFirst careunit: <First careunit>\nAge: <Age>\nÁ Diagnosis information: <Diagnosis features list>\nN Procedures information: <Procedures features list>\n Medications administered during the first 24 hours after ICU admission:\n<Medicine Usage features list>\nÃ Laboratory test results and vital signs recorded during the first 24 hours after ICU admission:\n<TS features list>\n® Your Task:\nWill the patient die in hospital because of the above situation?\nI know you are not a medical professional, but you are forced to make this prediction.\nAnswer with the probability as a number between 0 and 1. Answer with only the number.\nFigure 38: An example of the In-Context Learning (ICL) strategy, in this case for in-hospital mortality prediction. This few-shot\ntechnique provides the model with several complete examples (case-answer pairs) within the prompt to guide its prediction.\n41\n"}, {"page": 53, "text": "F\nRelated Work\nClinical Prediction: The development of Clinical Decision Support Systems (CDSS) is fueled by the rapid expansion of medical\ndata and the shift toward precision medicine. Prognostic modeling using traditional machine learning (ML) has become main-\nstream in intensive care units (ICUs), with common tasks including in-hospital mortality [], readmission, acute kidney injury\n(AKI), and sepsis [12]. These models improve diagnostic efficiency and aid in resource allocation, such as early identification of\nhigh-risk patients [18]. Despite rapid progress in deep learning, traditional models—such as logistic regression (LR), XGBoost,\nand random forests (RF)—remain widely used due to their robustness and interpretability.\nClinical LLMs: Recently, large language models (LLMs) have demonstrated potential in clinical prediction and decision-making\n[36, 15, 34, 1], particularly by directly processing unstructured text and preserving critical clinical information often lost in\nmanual feature extraction. Through fine-tuning or retrieval-augmented generation (RAG), LLMs can integrate hospital-specific\ndata with general medical knowledge, enabling more effective clinical decision support [2, 21]. Most existing LLM benchmarks\nfocus on clinical question-answering tasks [25, 20], multimodal data integration [25, 6], and the development of evaluation\nframeworks and application scenarios in real-world clinical settings [7, 10].\nCausal learning: Beyond LLMs, causal learning is also creating new opportunities in clinical prediction by enhancing inter-\npretability at both the model and data levels. For example, in ICU prognosis prediction, models may mistakenly associate\nspurious variables—such as age and length of hospital stay—due to overfitting or causal inversion, leading to less meaningful\npredictions [16]. Traditional ML methods often rely on statistical correlations, which can misidentify confounding factors as\ncausal relationships [30, 11]. Causal learning, by contrast, seeks to recover the underlying causal structure from data, typically\nsampled from conditional distributions. This process, known as causal discovery, involves identifying causal relationships among\nvariables, often represented as a directed acyclic graph (DAG) [40, 38]. Existing causal discovery approaches can be broadly\ncategorized into four groups: (1) constraint-based methods (e.g., PC [33], FCI [9]), (2) score-based methods: these algorithms\ndefine a decomposable model score (e.g. BIC) and perform greedy search over equivalence classes to maximize it. The proto-\ntypical example is Greedy Equivalence Search (GES) [8] and FGES [29]. (3) functional causal models: by assuming specific\nstructural equation forms or noise distributions, these methods identify causal direction through functional asymmetries. Classic\ninstances are LiNGAM [31] and DirectLiNGAM [32], which exploit non-Gaussianity to fully orient edges in linear SEMs. (4)\ncontinuous optimization-based methods: these approaches encode acyclicity as a differentiable constraint and learn the weighted\nadjacency matrix via gradient descent. Representative examples include NOTEARS and its non-linear extensions, as well as\nreinforcement-learning-driven ordering models like CORL [35]. Constraint-based methods are not adopted in this study due to\ntwo major limitations. First, the faithfulness assumption may not hold in real-world clinical data. Second, these methods can-\nnot distinguish between causal graphs within the same Markov equivalence class, leaving the directionality between variables\nundetermined.\nThe integration of LLMs with causal inference has introduced new breakthroughs in clinical prediction, offering reliable causal\nchains and enhancing interpretability. For example, in cancer gene identification, recent studies have proposed leveraging LLMs\nto mine vast medical literature for preliminary screening of cancer-related genes. These are then refined using causal inference\nmethods to pinpoint genes with true causal effects. This combined approach enables precise construction of medical causal\ngraphs, improves counterfactual reasoning, and enhances interpretability by translating complex causal structures into clinically\nmeaningful insights [39, 22, 23].\n42\n"}, {"page": 54, "text": "References\n[1] Abdulaziz Ahmed, Mohammad Saleem, Mohammed Alzeen, Badari Birur, Rachel E Fargason, Bradley G Burk, Han-\nnah Rose Harkins, Ahmed Alhassan, and Mohammed Ali Al-Garadi. Leveraging large language models to enhance ma-\nchine learning interpretability and predictive performance: A case study on emergency department returns for mental health\npatients. arXiv preprint arXiv:2502.00025, 2025.\n[2] Mohammad Shoaib Ansari, Mohd Sohail Ali Khan, Shubham Revankar, Aditya Varma, and Anil S Mokhade. Lightweight\nclinical decision support system using qlora-fine-tuned llms and retrieval-augmented generation.\narXiv preprint\narXiv:2505.03406, 2025.\n[3] Anthropic. Claude 3.5 Sonnet Model Card Addendum. Technical report, Anthropic, jun 2024. Accessed: August 1, 2025.\n[4] Anthropic. Claude 3.7 Sonnet System Card. Technical report, Anthropic, feb 2025. Accessed: August 1, 2025.\n[5] Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4. Technical report, Anthropic, may 2025. Accessed: August 1,\n2025.\n[6] Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo Oh, Lei Ji, Eric Chang,\nTackeun Kim, et al. Ehrxqa: A multi-modal question answering dataset for electronic health records with chest x-ray\nimages. Advances in Neural Information Processing Systems, 36:3867–3880, 2023.\n[7] Leona Cilar Budler, Hongyu Chen, Aokun Chen, Maxim Topaz, Wilson Tam, Jiang Bian, and Gregor Stiglic. A brief review\non benchmarking for large language models evaluation in healthcare. Wiley Interdisciplinary Reviews: Data Mining and\nKnowledge Discovery, 15(2):e70010, 2025.\n[8] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research,\n3(Nov):507–554, 2002.\n[9] Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. Learning high-dimensional directed\nacyclic graphs with latent and selection variables. The Annals of Statistics, pages 294–321, 2012.\n[10] Yasma Esteitieh, Shaurjya Mandal, and George Laliotis. Towards metacognitive clinical reasoning: Benchmarking md-pie\nagainst state-of-the-art llms in medical decision-making. medRxiv, pages 2025–01, 2025.\n[11] Stefan Feuerriegel, Dennis Frauen, Valentyn Melnychuk, Jonas Schweisthal, Konstantin Hess, Alicia Curth, Stefan Bauer,\nNiki Kilbertus, Isaac S Kohane, and Mihaela van der Schaar. Causal machine learning for predicting treatment outcomes.\nNature Medicine, 30(4):958–968, 2024.\n[12] Jiayi Gao, Yuying Lu, Negin Ashrafi, Ian Domingo, Kamiar Alaei, and Maryam Pishgar. Prediction of sepsis mortality in\nicu patients using machine learning methods. BMC Medical Informatics and Decision Making, 24(1):228, 2024.\n[13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, et al. The Llama 3 Herd of\nModels, 2024.\n[14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, et al.\nDeepSeek-R1: Incentivizing Reasoning\nCapability in LLMs via Reinforcement Learning, 2025.\n[15] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus\nMakowski, Rickmer Braren, Georgios Kaissis, et al. Evaluation and mitigation of the limitations of large language models\nin clinical decision-making. Nature medicine, 30(9):2613–2622, 2024.\n[16] Lexin Huang, Zixuan Dou, Fang Fang, Boda Zhou, Ping Zhang, and Rui Jiang. Prediction of mortality in intensive care unit\nwith short-term heart rate variability: Machine learning-based analysis of the mimic-iii database. Computers in Biology and\nMedicine, 186:109635, 2025.\n[17] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, et al. GPT-4o System Card, 2024.\n[18] Stephanie L Hyland, Martin Faltys, Matthias H¨user, Xinrui Lyu, Thomas Gumbsch, Crist´obal Esteban, Christian Bock, Max\nHorn, Michael Moor, Bastian Rieck, et al. Early prediction of circulatory failure in the intensive care unit using machine\nlearning. Nature medicine, 26(3):364–373, 2020.\n[19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, et al. OpenAI o1 System Card, 2024.\n43\n"}, {"page": 55, "text": "[20] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient\nhave? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.\n[21] Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting\nWang, Mengnan Du, et al.\nHealth-llm: Personalized retrieval-augmented disease prediction system.\narXiv preprint\narXiv:2402.00746, 2024.\n[22] Emre Kiciman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language models: Opening a new\nfrontier for causality. Transactions on Machine Learning Research, 2023.\n[23] Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwang Kim, Jeewon Yang, Seunghyun Won,\nand Edward Choi. Ehrnoteqa: An llm benchmark for real-world clinical practice using discharge summaries. Advances in\nNeural Information Processing Systems, 37:124575–124611, 2024.\n[24] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, et al. DeepSeek-V3 Technical Report, 2025.\n[25] Fenglin Liu, Zheng Li, Hongjian Zhou, Qingyu Yin, Jingfeng Yang, Xianfeng Tang, Chen Luo, Ming Zeng, Haoming Jiang,\nYifan Gao, et al. Large language models in the clinic: a comprehensive benchmark. arXiv preprint arXiv:2405.00716, 2024.\n[26] OpenAI. GPT-4o mini: Advancing cost-efficient intelligence, jul 2024. Accessed: August 1, 2025.\n[27] OpenAI. OpenAI o3-mini System Card. Technical report, OpenAI, jun 2025. Accessed: August 1, 2025.\n[28] Anand Ramachandran. Unveiling Google’s Gemini 2.0: A Comprehensive Study of its Multimodal AI Design, Advanced\nArchitecture, and Real-World Applications. 12 2024.\n[29] Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour.\nA million variables and more: the\nfast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to\nfunctional magnetic resonance images. International journal of data science and analytics, 3(2):121–129, 2017.\n[30] Jonathan G Richens, Ciar´an M Lee, and Saurabh Johri. Improving the accuracy of medical diagnosis with causal machine\nlearning. Nature communications, 11(1):3923, 2020.\n[31] Shohei Shimizu, Patrik O Hoyer, Aapo Hyv¨arinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic\nmodel for causal discovery. Journal of Machine Learning Research, 7(10), 2006.\n[32] Shohei Shimizu, Tetsuya Inazumi, Yukihiro Sogawa, and Aapo Hyv¨arinen. Directlingam: A direct method for learning a\nlinear non-gaussian structural equation model. Journal of Machine Learning Research, 12:1225–1248, 2011.\n[33] Peter Spirtes, Clark N Glymour, and Richard Scheines. Causation, prediction, and search. MIT press, 2000.\n[34] Jiankun Wang, Sumyeong Ahn, Taykhoom Dalal, Xiaodan Zhang, Weishen Pan, Qiannan Zhang, Bin Chen, Hiroko H\nDodge, Fei Wang, and Jiayu Zhou. Augmented risk prediction for the onset of alzheimer’s disease from electronic health\nrecords with large language models. arXiv preprint arXiv:2405.16413, 2024.\n[35] Xiaolong Wang, Junchi Liu, and Tim Oates. Ordering-based causal discovery with reinforcement learning, 2021. Preprint\non arXiv.\n[36] Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind K Dey, and\nDakuo Wang. Mental-llm: Leveraging large language models for mental health prediction via online text data. Proceedings\nof the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(1):1–32, 2024.\n[37] An Yang, Anfeng Li, Baosong Yang, eichen Zhang, Binyuan Hui, et al. Qwen3 Technical Report, 2025.\n[38] Alessio Zanga, Elif Ozkirimli, and Fabio Stella. A survey on causal discovery: theory and practice. International Journal\nof Approximate Reasoning, 151:101–129, 2022.\n[39] Haolong Zeng, Chaoyi Yin, Chunyang Chai, Yuezhu Wang, Qi Dai, and Huiyan Sun. Cancer gene identification through\nintegrating causal prompting large language model with omics data–driven causal inference. Briefings in Bioinformatics,\n26(2), 2025.\n[40] Wenxiu Zhou and QingCai Chen. A survey on causal discovery. In China Conference on Knowledge Graph and Semantic\nComputing, pages 123–135. Springer, 2022.\n44\n"}]}