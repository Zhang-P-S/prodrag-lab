{"doc_id": "arxiv:2512.23090", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.23090.pdf", "meta": {"doc_id": "arxiv:2512.23090", "source": "arxiv", "arxiv_id": "2512.23090", "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients", "authors": ["Armin Berger", "Manuela Bergau", "Helen Schneider", "Saad Ahmad", "Tom Anglim Lagones", "Gianluca Brugnara", "Martha Foltyn-Dumitru", "Kai Schlamp", "Philipp Vollmuth", "Rafet Sifa"], "published": "2025-12-28T21:57:42Z", "updated": "2026-01-02T18:25:09Z", "summary": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.23090v2", "url_pdf": "https://arxiv.org/pdf/2512.23090.pdf", "meta_path": "data/raw/arxiv/meta/2512.23090.json", "sha256": "66200f04a27a32fbd7f2e5b4637c7cb73470fb7c18018f4cd1643fbf54795894", "status": "ok", "fetched_at": "2026-02-18T02:23:40.050199+00:00"}, "pages": [{"page": 1, "text": "Benchmark Success, Clinical Failure:\nWhen Reinforcement Learning Optimizes for\nBenchmarks, Not Patients\nArmin Berger∗1,2,3, Manuela Bergau∗1,2,3, Helen Schneider1, Saad\nAhmad1, Tom Anglim Lagones4,5, Gianluca Brugnara6, Martha\nFoltyn-Dumitru6, Kai Schlamp6, Philipp Vollmuth6, and Rafet\nSifa1,2\n1Fraunhofer IAIS, Germany\n2University of Bonn , Germany\n3Lamarr Institute, Germany\n4Department of Health Queensland, Australia\n5Griffith University, Australia\n6University Hospital Bonn, Germany\nDecember 2025\nAbstract\nRecent Reinforcement Learning (RL) advances for Large Language\nModels (LLMs) have improved reasoning tasks, yet their resource-constrained\napplication to medical imaging remains underexplored.\nWe introduce\nChexReason, a vision-language model trained via R1-style methodology\n(SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL sam-\nples, and a single A100 GPU. Evaluations on CheXpert and NIH bench-\nmarks reveal a fundamental tension: GRPO recovers in-distribution per-\nformance (23% improvement on CheXpert, macro-F1 = 0.346) but de-\ngrades cross-dataset transferability (19% drop on NIH). This mirrors high-\nresource models like NV-Reason-CXR-3B, suggesting the issue stems from\nthe RL paradigm rather than scale. We identify a generalization para-\ndox where the SFT checkpoint uniquely improves on NIH before opti-\nmization, indicating teacher-guided reasoning captures more institution-\nagnostic features. Furthermore, cross-model comparisons show structured\nreasoning scaffolds benefit general-purpose VLMs but offer minimal gain\nfor medically pre-trained models. Consequently, curated supervised fine-\ntuning may outperform aggressive RL for clinical deployment requiring\nrobustness across diverse populations.\n∗These authors contributed equally and share first authorship.\n1\narXiv:2512.23090v2  [cs.AI]  2 Jan 2026\n"}, {"page": 2, "text": "1\nIntroduction\nRecent work demonstrates that reinforcement learning (RL) can substantially\nimprove large language model performance, particularly in settings with a clear\nreward signal and automatically verifiable outcomes (e.g., mathematics and code\ngeneration; see, e.g., DeepSeek-R1). However, it remains less clear how reliably\nthese gains transfer to problems with weaker or more subjective supervision,\nsuch as free-form natural language generation and multimodal inputs. In this\nwork, we investigate whether R1-style training, which combines supervised fine-\ntuning (SFT) with Group Relative Policy Optimization (GRPO), can enhance\nmultilabel chest X-ray classification in small vision-language models under se-\nvere resource constraints. We focus on chest X-ray diagnosis because it rep-\nresents a clinically critical task where radiologists value both hard diagnostic\nlabels for rapid assessment and accompanying reasoning traces to establish trust\nin model outputs. Moreover, chest X-rays benefit from large publicly available\ndatasets with multilabel annotations that provide natural reward signals for\nreinforcement learning. While recent work has explored R1-style reasoning for\nmedical visual question answering, multilabel chest X-ray classification remains\nless studied. A notable exception is NVIDIA’s NV-Reason-CXR-3B, which uti-\nlizes extensive synthetic data and compute. Our work contrasts with this high-\nresource approach by examining R1-style training under extreme constraints: 50\ntimes less training data and 4 times less compute. This setting is particularly\nrelevant for practitioners who lack large-scale annotation pipelines or extensive\ninfrastructure but still seek to leverage reasoning-guided training for improved\ndiagnostic performance. Our work makes three primary contributions.\n• Low-Resource R1-Style Training: We present ChexReason, trained with\nonly 2,000 SFT and 1,000 RL samples on a single A100 GPU, demonstrat-\ning that R1-style training is feasible without extensive resources.\n• Instruction Format Sensitivity: Cross-model analysis reveals that optimal\ninstruction format depends on medical pre-training: structured medically\ninformed reasoning scaffolds benefit general-purpose VLMs while provid-\ning minimal gain for domain-specialized models.\n• Benchmark-Transferability Trade-off: GRPO improves CheXpert perfor-\nmance (+23%) but degrades NIH transferability (−19%), mirroring NV-\nReason-CXR-3B failures and suggesting a paradigm-level issue.\n• Generalization Paradox: The SFT checkpoint uniquely improves on out-\nof-distribution data, indicating teacher-guided traces capture more gener-\nalizable features than reward-optimized outputs.\n2\nRelated Work\nRecent advancements in large language models have spurred significant interest\nin applying reinforcement learning (RL) and chain-of-thought (CoT) reasoning\n2\n"}, {"page": 3, "text": "to medical vision-language models (VLMs), a trend motivated by the success\nof general-domain approaches like DeepSeek-R1. Consequently, several studies\nhave explored R1-style reasoning recipes for medical visual question answering\n(VQA). For instance, MedVLM-R1 [27] utilizes GRPO to improve VQA accu-\nracy significantly across MRI, CT, and X-ray benchmarks. Similarly, the intro-\nduction of Med-R1 [18] demonstrated that RL can enhance generalization and\nreliability across eight imaging modalities, notably outperforming much larger\nmodels. To address computational constraints, RARL [29] employs Low-Rank\nAdaptation (LoRA) to improve reasoning on VQA tasks, while other works such\nas GMAI-VL-R1 [31] have harnessed RL for multimodal reasoning on large-scale\ndatasets. Furthermore, recent research has explored eliciting medical reasoning\nfrom base models using verifiable rewards without explicit supervision, yielding\nimprovements in out-of-distribution generalization [39].\nWhile VQA-focused approaches have proliferated, the application of R1-\nstyle training to multilabel chest X-ray classification remains less explored. A\npioneering effort in this specific domain is NV-Reason-CXR-3B [24], which ap-\nplies reasoning with GRPO to the CheXpert ontology. This model produces\nradiologist-style stepwise reasoning traces using a two-stage training pipeline\non a Qwen2.5-VL-3B-Instruct backbone, scaling supervision via synthetic data\nderived from radiology reports. This paper by Nvidia, despite its similarity to\nour approach, was developed independently from the research of this paper and\npublished weeks earlier. Complementing this methodology, ChestX-Reasoner\n[8] leverages process supervision mined from clinical reports and introduces\nRadRBench-CXR, a benchmark designed to evaluate reasoning quality in chest\nX-ray interpretation.\nWithin the broader chest X-ray domain, various approaches have been de-\nveloped to tackle diagnosis and report generation.\nInterpretability has been\na key focus; X-Ray-CoT [25], for example, proposes a framework for diagno-\nsis using CoT reasoning that achieves high balanced accuracy on the CORDA\ndataset. In the realm of conversational models, RadVLM [28] excels in mul-\ntitask capabilities including classification, localization, and captioning. Other\nunified frameworks include MedRAX [7], an agentic framework that integrates\nstate-of-the-art analysis tools with multimodal LLMs.\nReport generation has also benefited from targeted technical improvements\ndesigned to enhance clinical accuracy. To address the issue of hallucinations,\nparticularly regarding prior exams, Direct Preference Optimization (DPO) has\nbeen successfully employed to reduce hallucinated lines significantly [3]. Ad-\nditionally, joint localization and classification have been explored using combi-\nnations of Llama-2-Chat and BiomedCLIP [19]. These modeling advances are\nsupported by evolving evaluation frameworks, such as CXPMRG-Bench [36]\nfor report generation and ReXrank [38], which establishes standardized metrics\nacross multiple datasets.\nBeyond reasoning-based approaches, classification and structured prediction\nhave been addressed through diverse strategies. Multi-task networks like CLN\n[26] have been developed for simultaneous localization and classification, while\nother studies have utilized small LLMs to automate the transformation of un-\n3\n"}, {"page": 4, "text": "structured radiology reports into structured labels [1]. Collaboration between\nclinicians and VLMs has also been investigated to enhance report generation\n[33]. Furthermore, competitive results on validation sets have been achieved\nthrough federated learning [22], CLIP-based zero-shot learning with text em-\nbeddings [4], and contrastive learning with partial label overlap loss [16]. Finally,\nRL fine-tuning continues to be refined for medical VQA, with recent works focus-\ning on visual grounding [20], the consistent benefits of GRPO [42], and aligning\nmodels with clinical preferences [41].\nThese medical-specific developments are deeply informed by broader ad-\nvances in visual reasoning. Innovations such as vision-guided RL for human-free\nalignment [37] and the optimization of visual reasoning via CoT responses [32]\nprovide the foundational techniques that are currently being adapted to enhance\nperformance in medical imaging tasks.\n3\nMethodology\nThis section describes our methodology for training a vision-language model\nto perform multilabel classification of chest X-ray images with accompanying\nchain-of-thought reasoning traces. Our approach consists of two main stages:\nsupervised fine-tuning (SFT) using high-quality reasoning traces, followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nfurther refine performance. A central goal is to elicit deliberate reasoning be-\nhaviour, what might be termed “level 2 thinking,” rather than shallow pattern\nmatching, through the reinforcement learning process. For our experiments, we\nemployed MedGemma-4B, a multimodal vision-language model that processes\nimages normalized to 896 × 896 pixels into 256 image tokens. We selected this\nmodel for its extensive pretraining on biomedical data and its domain-adapted\nCLIP encoder for visual processing. To evaluate generalization across different\narchitectures, we also conducted comparative experiments using Qwen2.5-VL-\n3B-Instruct. All training runs were executed on a single NVIDIA A100 80GB\nGPU.\n3.1\nDataset\nFor training, we utilized the MIMIC-CXR-JPG dataset [17], which contains\n377,110 chest X-ray images associated with 227,835 imaging studies from 65,379\npatients [13, 14, 9]. We selected only datapoints with antero-posterior or postero-\nanterior views and considered only positive CheXpert labels.\nThe selection\nprocess employed a penalty-based greedy iterative sampler designed to score\ncandidates by favoring images that reduce deficits for under-represented labels\nwhile heavily penalizing choices that would exacerbate the over-representation\nof abundant labels. This strategy ensures each label is present in at least 5%\nof the samples. Using this approach, we generated 2,000 samples for the Super-\nvised Fine-Tuning (SFT) step and 1,000 samples for the Reinforcement Learning\n(RL) step, with no data overlap between the two subsets.\n4\n"}, {"page": 5, "text": "For the reasoning traces dataset leveraged in the SFT stage, each of the 2,000\ninstruction-following examples was generated using Gemini 2.5 as a teacher\nmodel. The teacher model was provided with ground-truth labels during gen-\neration but was instructed to produce rationales as if reasoning independently.\nWe developed this dataset in collaboration with doctors and radiologists from\nthe University Clinic Bonn (UKB) and Queensland Health to mirror the medical\nreasoning process radiologists undergo when examining chest X-rays. Our de-\nvelopment process involved sampling 100 datapoints from the 2,000 selected for\nSFT training and generating corresponding examples using medically informed\nreasoning. These datapoints were reviewed by radiologists at the UKB, and\nthe prompting strategy was incrementally refined to deliver medically sound\nreasoning traces.\nFurthermore, we extracted an additional 500 samples at random from the\nsame dataset [17] to serve as a validation set.\nUsing this validation set, we\nevaluated the performance of different prompting strategies and their respec-\ntive effects during SFT training. We also used this set to cross-compare the\nresponse to training of two student models: MedGemma-4B and Qwen2.5-VL-\n3B-Instruct.\nFor evaluation, we employed two out-of-distribution test sets. The first is\na subset of the CheXpert dataset [15], a large-scale collection from Stanford\nHospital comprising 224,316 chest radiographs from 65,240 patients. These are\nlabeled for 14 common observations using an automated rule-based labeler that\nexplicitly captures uncertainty from radiology reports. The second is a subset\nof the NIH Chest X-ray dataset [2], which contains 112,120 frontal-view X-\nrays from 30,805 unique patients at the NIH Clinical Center, annotated with\n14 disease labels derived via natural language processing of associated reports.\nUnlike CheXpert, the NIH Chest X-ray 14 dataset does not explicitly label\nuncertainty. From these datasets, we utilized 518 observations from CheXpert\nand 488 observations from NIH for testing. We reduced the NIH label set to\nnine pathology categories to match a subset of the CheXpert labels: Atelectasis,\nCardiomegaly, Consolidation, Edema, Lung Lesion, No Finding, Pleural Other,\nPneumonia, and Pneumothorax.\n3.2\nTraining\nWe divide training into two stages: supervised fine-tuning (SFT) followed by\nGroup Relative Policy Optimization (GRPO). Throughout this section, we use\nthe following notation: input X-ray images are denoted as x, generated text\nsequences (comprising reasoning and predictions) as t, ground truth labels as\nY , and predicted labels as ˆY from the 14-label CheXpert lexicon. The model\noutputs text in a structured format {analysis:\n..., conclusion:\n...},\nfrom which we extract predicted labels using a rule-based parser.\n5\n"}, {"page": 6, "text": "3.2.1\nSupervised Fine-Tuning\nIn the SFT stage, we train the model to generate teacher-annotated reasoning\ntraces from the 2,000-sample training dataset.\nThe objective minimizes the\nnegative log-likelihood of expert traces t∗given input X-ray images x:\nLSFT(θ) = −E(x,t∗)∼DSFT\n\n\n|t∗|\nX\nj=1\nlog πθ(t∗\nj | x, t∗\n<j)\n\n,\n(1)\nwhere t∗\nj denotes the j-th token in the expert trace, and each trace was generated\nby Gemini 2.5 provided with ground-truth labels but instructed to produce ra-\ntionales as if reasoning independently, resulting in clinician-validated reasoning\npatterns.\nWe performed SFT on two vision-language models: MedGemma-4B [10]\nand Qwen2.5-VL-3B-Instruct [34], using the Accelerate library [11]. To enable\nparameter-efficient training, we employed Low-Rank Adaptation (LoRA) [12]\nwith rank 16, scaling factor 32, and dropout rate 0.1, applied exclusively to the\nlanguage model’s attention and feed-forward projection layers, while keeping the\nvision encoder frozen throughout training. Both models were trained for up to 6\nepochs with early stopping, patience of 2 epochs based on validation loss, using\nthe AdamW optimizer with a learning rate of 5×10−5, cosine annealing schedule\nwith 5% warmup, weight decay of 0.01, and gradient clipping at norm 5.0. We\nused an effective batch size of 8 achieved through gradient accumulation with a\nper-device batch size of 1, and reserved 10% of the data for validation. Training\nwas conducted in BFloat16 precision with gradient checkpointing enabled for\nmemory efficiency. Critically, we applied assistant-only loss masking, where loss\ncomputation was restricted to the assistant’s response tokens by masking the\nuser prompt, image placeholder tokens, and padding tokens. For visual token\nprocessing, we controlled image resolution by constraining visual tokens to the\nrange of 256–512 tokens per image via the processor’s minimum and maximum\npixels parameters.\n3.2.2\nGroup Relative Policy Optimization\nFollowing SFT, we fine-tune the model using Group Relative Policy Optimiza-\ntion (GRPO) [30] on the 1,000-sample RL dataset. GRPO obviates the need for\nan additional value function approximation and instead uses the average reward\nof multiple sampled outputs as the baseline. Specifically, for each input X-ray x,\nGRPO samples a group of text completions {t1, t2, · · · , tG} from the old policy\nπθold and optimizes the policy model by maximizing the following objective:\n6\n"}, {"page": 7, "text": "JGRP O(θ) = Ex∼DRL, {ti}G\ni=1∼πθold(t|x)\n\"\n1\nG\nG\nX\ni=1\n1\n|ti|\n|ti|\nX\nj=1\nmin\n\u0002\nri,j(θ) ˆAi,\nclip(ri,j(θ), 1−ε, 1+ε) ˆAi\n\u0003\n−βDKL(πθ∥πref)\n#\n,\n(2)\nwhere ri,j(θ) =\nπθ(ti,j|x,ti,<j)\nπθold(ti,j|x,ti,<j) is the token-level importance ratio, ˆAi is\nthe sequence-level advantage calculated based on relative rewards within each\ngroup, and ε and β are hyper-parameters controlling the clipping bounds and\nKL divergence penalty, respectively. This group-relative approach aligns well\nwith the comparative nature of reward models, which are typically trained on\ncomparison datasets. Furthermore, instead of adding a KL penalty to the re-\nward, GRPO regularizes by directly adding the KL divergence term between\nthe trained policy and the reference policy πref, the SFT checkpoint, to the\nobjective [30].\nStabilizing Training Against Mode Collapse. In early training runs,\nwe observed training instability characterized by sharp drops in policy entropy,\nrepetitive outputs, and rapid performance degradation on validation metrics,\nsymptoms consistent with mode collapse in RL fine-tuning of LLMs [40]. To ad-\ndress this, we incorporated several stabilization mechanisms informed by recent\nwork on stable RL training for language models. Specifically, our formulation\nincludes: (1) token-level importance sampling correction via the ratio πθ/πθold\nto account for policy updates between rollout and training steps, (2) clipped\npolicy updates to constrain policy staleness and prevent aggressive parameter\nchanges, and (3) explicit KL divergence regularization against the reference\npolicy to maintain proximity to the SFT checkpoint [40]. These modifications\neliminated the collapse behavior observed in preliminary experiments across all\ntraining runs.\nFor each training sample, we generated 4 completions at temperature 0.8\nwith nucleus sampling (top-p = 0.95). The optimization balances reward max-\nimization against a KL divergence penalty (β = 0.15) to prevent excessive de-\nviation from the SFT checkpoint. We employed the Dr. GRPO loss normal-\nization [21] to eliminate length bias, with asymmetric PPO clipping bounds\n[0.15, 0.22] for stable policy updates.\nReward Functions. We developed two alternative reward functions with\ncontrasting design philosophies and evaluated their performance in separate\ntraining runs.\nThe hard reward enforces strict format compliance combined\nwith label prediction accuracy:\nrhard(t, Y ) =\n\n\n\n\n\nJ(Y, ˆY ) −λlen\nif t is valid and |t| < 250\nJ(Y, ˆY )\nif t is valid and |t| ≥250\n0\notherwise\n(3)\nwhere valid outputs must contain exactly one <think>...</think> reasoning\n7\n"}, {"page": 8, "text": "block followed by a <solution>...</solution> block, J(Y, ˆY ) = |Y ∩ˆY |/|Y ∪ˆY |\nis the Jaccard similarity score between predicted and ground truth labels, and\nλlen is a penalty coefficient for short responses.\nThe nuanced reward implements a multi-component scoring mechanism de-\nsigned to provide a more granular reward signal during training:\nrnuanced(t, Y ) = rmatch + rpartial −rFP −rcollapse −rformat,\n(4)\nwhere exact matches receive +100 points, partial credit is scaled by recall (30\npoints per correct label relative to ground truth size) and precision (20 points\nper correct label relative to prediction size), and various penalties discourage\nundesirable behaviors.\nCritically, we incorporated frequency-weighted penal-\nties for false positives: commonly occurring labels such as “No Finding” and\n“Support Devices” incur higher penalties when incorrectly predicted than rare\npathologies, discouraging shotgun predictions. To prevent mode collapse, we\nmonitor a sliding window of the most recent 100 predictions and apply cumu-\nlative penalties (−30 per excess repetition, −50 for mode collapse) when any\nsingle label exceeds 70% dominance.\nAdditional penalties discourage invalid\nCheXpert labels (−100 each), duplicate predictions (−25 each), and extraneous\ntext outside the designated blocks. All predicted labels are filtered to the 14\nvalid CheXpert labels before evaluation.\n4\nResults and Evaluation\n4.1\nEffect of Prompting\nTo identify the optimal instruction format for multilabel CheXpert classifica-\ntion, we evaluated nine prompt variants on MedGemma-4B, selected as the\nprimary ablation target due to its superior base performance among available\nmodels. Each variant implemented a distinct reasoning strategy, ranging from\nstructured, stepwise protocols to free-form narratives and explicit verification\nmechanisms (Table 1). We initially prioritized a structured approach, denoted\nas Reasoning A,\nwhich aimed to mimic radiological best practices developed\nby experts from the\nUniversity Clinic Bonn and Queensland Health.\nThis\nprompt instructed the LLM to perform a mandatory 12-step chest X-ray anal-\nysis within <think> tags, followed by a strict list of derived CheXpert labels\nwithin <solution> tags. Contrary to expectations, the less constrained Rea-\nsoning Narrative prompt achieved the highest overall performance (micro-F1 =\n0.524, macro-F1 = 0.270), followed by Reasoning Self-Check (micro-F1 = 0.514,\nmacro-F1 = 0.253). Both outperformed the structured Reasoning A baseline\n(micro-F1 = 0.498, macro-F1 = 0.245). The performance differential was driven\nprimarily by recall rather than precision. Reasoning Narrative and Reasoning\nSelf-Check attained substantially higher overall recall (0.599 and 0.558, respec-\ntively) compared to Reasoning A (0.504), suggesting that these prompt framings\nenhanced the model’s sensitivity to positive findings. Conversely, certain com-\nplex prompts introduced significant decoding failures, most notably Reasoning\n8\n"}, {"page": 9, "text": "C (failure rate 0.482) and Reasoning F (0.180), which severely hindered end-\nto-end performance by producing unparseable outputs. Prompts that imposed\noverly rigid formatting constraints (Reasoning C, Reasoning F) or required ex-\nplicit differential comparisons for every finding (Reasoning F) suffered from both\ndecoding failures and reduced classification quality. In contrast, free-form nar-\nrative reasoning (Reasoning Narrative) and structured self-checking (Reasoning\nSelf-Check) balanced interpretability with output reliability, yielding micro-F1\ngains of +0.026 and +0.016 over the baseline.\nNotably, these results reflect\nMedGemma-4B’s medical pre-training; as we show in the SFT analysis, the\nstructured Reasoning A format, which underperformed here, becomes the opti-\nmal choice for general-purpose models lacking domain-specific knowledge.\nTable 1: Prompt ablation on base MedGemma-4B (CheXpert validation).\nPrompt Configuration\nCheXpert Validation Metrics\nMicro-F1\nMacro-F1\nFail Rate\nReasoning Narrative\n0.524\n0.270\n0.002\nReasoning Self-Check\n0.514\n0.253\n0.010\nReasoning Hypothesis\n0.357\n0.188\n0.022\nReasoning A (baseline)\n0.498\n0.245\n0.000\nReasoning B\n0.427\n0.170\n0.000\nReasoning C\n0.260\n0.132\n0.482\nReasoning D\n0.329\n0.170\n0.000\nReasoning E\n0.404\n0.218\n0.000\nReasoning F\n0.269\n0.108\n0.180\nKey design cues:\nReasoning Narrative: Free-form expert narrative with evidence aggregation\nReasoning Self-Check: Region-wise scan then explicit verification pass\nReasoning Hypothesis: Hypothesis-driven comparison of candidate findings\nReasoning A: Structured 12-step checklist with strict format (Baseline)\nReasoning B: Zonal lung review with pathology differentiation\nReasoning C: Evidence-heavy report with label-confidence matrix\nReasoning D: Definition-guided labeling with strict criteria\nReasoning E: Second-look pass over anatomic blind spots\nReasoning F: Differential diagnosis required for each abnormality\n4.2\nEffect of Supervised Fine-Tuning\nTo investigate whether instruction format effectiveness depends on medical pre-\ntraining, we fine-tuned two vision-language models, MedGemma-4B (medically\npre-trained) and Qwen2.5-VL-3B-Instruct (general-purpose, no medical train-\ning), on four distinct instruction formats: Reasoning Narrative (free-form radi-\nologic narrative), Reasoning A (structured 12-step reasoning), Free Reasoning\n(concise analysis followed by labels), and Only Label (direct label output with\n9\n"}, {"page": 10, "text": "no rationale).\nMedGemma-4B Results. For the medically pre-trained model, a strik-\ning trade-off emerged between micro-F1 and macro-F1 performance (Table 2).\nThe Only Label variant achieved the highest micro-F1 (0.461), while Free Rea-\nsoning obtained the best macro-F1 (0.253). This divergence reflects label fre-\nquency effects: direct label prediction excels at high-support conditions like\n“No Finding” and “Support Devices” that dominate micro-averaged metrics,\nbut struggles with rare pathologies that carry equal weight in macro-averages.\nNotably, Reasoning A underperformed significantly across both metrics (micro-\nF1 = 0.293, macro-F1 = 0.139), suggesting that explicit structured reasoning\nprovides little benefit when the model already possesses domain-specific feature\nrepresentations.\nQwen2.5-VL-3B-Instruct Results. The general-purpose model exhib-\nited a strikingly different pattern (Table 3). Reasoning A, the structured 12-\nstep reasoning format, achieved the best performance by a substantial margin\n(micro-F1 = 0.371, macro-F1 = 0.208), outperforming all other variants. In\ncontrast, Only Label, which succeeded on MedGemma, dropped to mediocre\nperformance (micro-F1 = 0.249, macro-F1 = 0.080). Free Reasoning also strug-\ngled (micro-F1 = 0.200, macro-F1 = 0.131), while Reasoning Narrative showed\nmoderate scores but high failure rates (17%).\nCross-Model Interpretation. This complete ranking reversal reveals a\nfundamental insight: medical pre-training encodes domain-specific feature rep-\nresentations that enable direct label mapping, whereas general-purpose VLMs\nrequire explicit structured reasoning scaffolds to compensate for missing domain\nknowledge. The 12-step clinical analysis in Reasoning A provides necessary in-\nterpretive guidance for Qwen, directing attention through systematic examina-\ntion of anatomical structures and pathological indicators. However, this same\nstructure appears redundant for MedGemma, which has already internalized\nthese reasoning patterns through medical pre-training. The failure of free-form\nreasoning on Qwen (but moderate success on MedGemma) further supports\nthis hypothesis: without either pre-trained medical representations or explicit\nstructural guidance, the model lacks the necessary inductive biases for clinical\ninterpretation.\nDecoding Reliability and Training Dynamics. Across both models,\ndecoding reliability correlated inversely with rationale length, with Only Label\nproducing zero format violations while longer formats increased malformation\nrisk.\nThe training dynamics in Figure 1 show that syntax-constrained vari-\nants (Only Label, Reasoning A) converge rapidly to high token accuracy, while\nFree Reasoning converges gradually and saturates lower. We hypothesize this\nreflects output entropy rather than learning quality: templated formats have\npredictable token distributions that inflate accuracy metrics but encourage pat-\ntern memorization, whereas free-form traces force the model to learn semantic\nrelationships rather than surface shortcuts.\n10\n"}, {"page": 11, "text": "Given its superior macro-F1 performance and balanced handling of both\ncommon and rare pathologies on the medically pre-trained model, we selected\nthe MedGemma-4B Free Reasoning variant as the initialization checkpoint for\nsubsequent GRPO training.\nTable 2: Supervised fine-tuning variants on MedGemma-4B (CheXpert valida-\ntion).\nMicro\nMacro\nFail\nVariant\nP\nR\nF1\nP\nR\nF1\nOnly Label\n0.596\n0.375\n0.461\n0.332\n0.214\n0.241\n0.000\nFree Reasoning\n0.480\n0.365\n0.415\n0.358\n0.227\n0.253\n0.032\nReasoning Narrative\n0.565\n0.281\n0.376\n0.345\n0.141\n0.180\n0.210\nReasoning A\n0.405\n0.230\n0.293\n0.381\n0.143\n0.139\n0.004\nTable 3:\nSupervised fine-tuning variants on Qwen/Qwen2.5-VL-3B-Instruct\n(CheXpert validation).\nMicro\nMacro\nFail\nVariant\nP\nR\nF1\nP\nR\nF1\nOnly Label\n0.333\n0.199\n0.249\n0.153\n0.122\n0.080\n0.000\nFree Reasoning\n0.200\n0.200\n0.200\n0.150\n0.142\n0.131\n0.056\nReasoning Narrative\n0.396\n0.198\n0.264\n0.122\n0.076\n0.075\n0.170\nReasoning A\n0.322\n0.436\n0.371\n0.194\n0.265\n0.208\n0.002\n11\n"}, {"page": 12, "text": "0\n10\n20\n30\n40\n50\nGlobal Step\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nMean Token Accuracy\nFree Reasoning\nReasoning Narrative\nReasoning A\nOnly Label\n0\n10\n20\n30\n40\n50\nGlobal Step\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nLoss\nTraining Loss\nFree Reasoning\nReasoning Narrative\nReasoning A\nOnly Label\nFigure 1: Comparative training dynamics across all four supervised fine-tuning\nvariants. The right panel shows training loss convergence, while the left panel\ndisplays mean token-level prediction accuracy. Notably, syntax-constrained vari-\nants (Only Label, Reasoning A, Reasoning Narrative) converge rapidly to higher\ntoken accuracy saturation, whereas Free Reasoning exhibits slower convergence\nand lower saturation levels.\nThis pattern reflects fundamental differences in\noutput entropy rather than learning quality, with Free Reasoning’s diverse, un-\nstructured traces requiring more nuanced semantic learning compared to the\npredictable template patterns of structured formats.\n4.3\nEffect of Group Relative Policy Optimization\nWe evaluated GRPO training using both reward functions on the Free Rea-\nsoning SFT checkpoint (Table 4). The simpler hard reward function achieved\nmarginally better validation performance (micro-F1 = 0.391, macro-F1 = 0.258)\ncompared to the complex nuanced reward (micro-F1 = 0.387, macro-F1 =\n0.257), suggesting that explicit format enforcement combined with Jaccard sim-\nilarity provided sufficient signal for policy optimization. Notably, both reward\nfunctions improved macro-F1 relative to the SFT checkpoint (0.253 →0.258),\nindicating enhanced performance on low-prevalence, diagnostically challenging\nconditions, though micro-F1 declined slightly (0.415 →0.391), reflecting mod-\nest performance degradation on high-support labels that dominate the micro-\naveraged metric.\n12\n"}, {"page": 13, "text": "0\n10000\n20000\n30000\n40000\n50000\n60000\nGlobal Step\n400\n350\n300\n250\n200\n150\n100\n50\n0\nReward Value\nTotal Train Reward\nRaw\nSmoothed (EMA=0.95)\n0\n10000\n20000\n30000\n40000\n50000\n60000\nGlobal Step\n400\n350\n300\n250\n200\n150\n100\n50\n0\nReward Value\nReward Function Mean (Component)\nRaw\nSmoothed (EMA=0.95)\n0\n10000\n20000\n30000\n40000\n50000\n60000\nGlobal Step\n200\n300\n400\n500\n600\nTokens\nMean Completion Length\nRaw\nSmoothed (EMA=0.95)\nFigure 2: Evolution of training metrics during the GRPO training process using\nnuanced rewards. The panels display (left) the total training reward, (center)\nthe mean reward function component, and (right) the mean completion length\nin tokens. Faint lines represent raw data recorded at each global step, while bold\nlines indicate an exponential moving average (EMA) with a smoothing factor\nof 0.95 to highlight the underlying training trends.\n0\n10000\n20000\n30000\n40000\n50000\n60000\nGlobal Step\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward Value\nTotal Train Reward\nRaw\nSmoothed (EMA=0.95)\n0\n10000\n20000\n30000\n40000\n50000\n60000\nGlobal Step\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward Value\nReward Function Mean (Component)\nRaw\nSmoothed (EMA=0.95)\n0\n10000\n20000\n30000\n40000\n50000\n60000\nGlobal Step\n200\n300\n400\n500\n600\nTokens\nMean Completion Length\nRaw\nSmoothed (EMA=0.95)\nFigure 3: Evolution of training metrics during the GRPO training process using\nhard rewards. The panels display (left) the total training reward, (center) the\nmean reward function component, and (right) the mean completion length in\ntokens. Faint lines represent raw data recorded at each global step, while bold\nlines indicate an exponential moving average (EMA) with a smoothing factor\nof 0.95 to highlight the underlying training trends.\nOn the in-distribution CheXpert test set (Table 5), GRPO training suc-\ncessfully recovered the performance lost during supervised fine-tuning.\nThe\nChexReason model (SFT + GRPO) achieved a macro-F1 of 0.346, represent-\ning a 23% improvement over the SFT checkpoint (0.282) and nearly matching\nthe MedGemma baseline (0.362). This recovery was particularly pronounced\nfor categories such as Cardiomegaly (0.442 →0.664), Lung Opacity (0.161 →\n0.743), and Support Devices (0.728 →0.818), where the reward signal effectively\nguided the model toward more accurate label predictions.\n13\n"}, {"page": 14, "text": "The generalization characteristics of GRPO training reveal a more nuanced\npicture when evaluated across two out-of-distribution test sets. On the CheX-\npert test set (Table 5), which originates from Stanford Hospital but shares the\nsame 14-label CheXpert schema as the MIMIC-CXR-JPG training data, the\nChexReason model achieves strong performance (macro-F1 = 0.346), substan-\ntially improving over the SFT checkpoint (0.282 →0.346, +23%). However,\non the NIH Chest X-ray test set (Table 6), which employs a distinct labeling\nmethodology reduced to nine CheXpert-compatible categories, the ChexRea-\nson model regresses to baseline levels (macro-F1 = 0.243), representing a 19%\ndegradation relative to the SFT checkpoint (0.299 →0.243) and matching the\npretrained MedGemma baseline performance.\nThis divergent behavior across out-of-distribution datasets suggests that\nGRPO optimization aligns the model to the semantic structure of the CheXpert\nlabeling convention rather than to generalizable radiologic features. The shared\nlabel schema between MIMIC-CXR-JPG (training) and CheXpert (test), en-\ncompassing identical pathology definitions, labeling granularity, and uncertainty\nencoding, enables the reward signal to exploit label-specific patterns that trans-\nfer within this ecosystem but fail to generalize to alternative labeling method-\nologies. Paradoxically, the SFT checkpoint achieves its best performance on the\nNIH dataset (macro-F1 = 0.299) despite exhibiting the weakest scores on the\nCheXpert test set (0.282), suggesting that teacher-generated reasoning traces\ncapture visual-semantic relationships that transcend specific label taxonomies,\neven as they degrade performance on the validation distribution used to guide\ntraining.\nThese findings reveal a fundamental tension in reinforcement learning for\nsmall vision-language models in medical imaging: reward-based optimization\nexcels at recovering performance within a specific labeling framework but ap-\npears to fail eliciting the kind of generalizable diagnostic reasoning that transfers\nacross institutional conventions and annotation methodologies. The fact that\nthe simpler hard reward performs comparably to the carefully engineered nu-\nanced reward further supports this interpretation. Both reward functions pro-\nvide sufficient supervision to align the model to CheXpert label semantics, but\nneither addresses the underlying challenge of learning institution-agnostic radio-\nlogic representations. This suggests that improving cross-dataset generalization\nmay require architectural modifications, multi-dataset training curricula, or re-\nward formulations that explicitly penalize overfitting to labeling conventions\nrather than post-hoc RL fine-tuning on single-institution data.\n14\n"}, {"page": 15, "text": "Table 4: Reinforcement learning using GRPO in various setups (CheXpert val-\nidation). All variants use the MedGemma-4B Free-Reasoning SFT model\ncheckpoint.\nMicro\nMacro\nReward\nFunction\nTraining Steps\nP\nR\nF1\nP\nR\nF1\nFail\nNuanced\nBest F1-Score\n0.342\n0.445\n0.387\n0.248\n0.311\n0.257\n0.020\nMaximum\n0.335\n0.424\n0.374\n0.228\n0.271\n0.232\n0.030\nHard\nBest F1-Score\n0.343\n0.455\n0.391\n0.250\n0.328\n0.258\n0.028\nMaximum\n0.340\n0.418\n0.375\n0.243\n0.279\n0.242\n0.018\n4.4\nGeneralization Analysis\nThe evolution of SFT training metrics in Figure 1 indicates that the Free Rea-\nsoning variant successfully masters the structured output format with stable\nconvergence.\nHowever, this optimization masks complex generalization chal-\nlenges revealed during out-of-distribution testing.\nA striking pattern emerges in cross-dataset transfer. High-performing mod-\nels like NV-Reason (macro-F1 = 0.755 on CheXpert) and ChexReason (0.346)\nexhibit dramatic degradation on the NIH dataset, dropping 61% and 30% re-\nspectively (Tables 5 and 6). This parallel failure suggests aggressive optimiza-\ntion for CheXpert-specific patterns trades generalization for benchmark perfor-\nmance. Empirical evidence supports this: Compton et al. [6] demonstrated that\nmodels often rely on deep-seated dataset artifacts like hospital source discrim-\nination. In contrast, the SFT checkpoint uniquely improves on NIH (0.282 →\n0.299) despite weaker CheXpert scores, whereas the MedGemma baseline drops\n33%.\nWe hypothesize this stems from the susceptibility of small vision-language\nmodels (3-4B parameters) to spurious patterns. Murali et al. [23] demonstrated\nthat spurious features are learned early and preferentially when they contain\nhigh “usable information,” a process entrenched by subsequent optimization\non benchmark datasets. However, our teacher-guided SFT process appears to\ndisrupt this overfitting without re-introducing benchmark-specific pressures.\nAligned with knowledge distillation literature, Boland et al. [5] showed that\ndistillation from unbiased teachers significantly reduces spurious feature learn-\ning.\nBy forcing alignment with Gemini-generated reasoning traces that em-\nphasize diagnostic principles rather than shortcuts, our SFT approach acts as\nimplicit knowledge distillation. These relearned, institution-agnostic principles\nprove less effective on the artifact-heavy CheXpert distribution but transfer\nmore reliably to the NIH dataset.\nThe observation that NV-Reason and ChexReason lose this generalization\n15\n"}, {"page": 16, "text": "advantage when optimized for higher CheXpert scores supports the notion\nthat current benchmark-driven development practices may inadvertently dis-\nfavor models with broader real-world viability. This interpretation aligns with\nVogt-Lowell et al. [35], who report that end-to-end fine-tuning can compromise\nout-of-distribution robustness. Collectively, these findings imply that the delib-\nerate constraint of supervised fine-tuning with reasoning-aligned guidance may\nhelp preserve institution-agnostic generalization capabilities by disrupting the\nlearned shortcuts often associated with competitive benchmark performance.\nTable 5: Comparative F1-Score performance across five models on the out-\nof-sample original Chexpert dataset, including overall aggregated performance.\nThe best performing model for each category is highlighted in bold.\nF1-Score\nCategory\nNV-Reason\nMedGemma\nQwen2.5\nMedGemma\n(SFT)\n(ChexReason)\nAtelectasis\n0.758\n0.288\n0.354\n0.229\n0.240\nCardiomegaly\n0.853\n0.627\n0.385\n0.442\n0.664\nConsolidation\n0.188\n0.000\n0.097\n0.205\n0.000\nEdema\n0.807\n0.174\n0.128\n0.385\n0.132\nEnlarged Cardiomediastinum\n0.879\n0.016\n0.407\n0.075\n0.000\nFracture\n0.750\n0.143\n0.009\n0.150\n0.200\nLung Lesion\n0.875\n0.300\n0.042\n0.211\n0.133\nLung Opacity\n0.961\n0.748\n0.464\n0.161\n0.743\nNo Finding\n0.775\n0.625\n0.318\n0.579\n0.607\nPleural Effusion\n0.822\n0.634\n0.264\n0.464\n0.625\nPleural Other\n0.667\n0.000\n0.026\n0.000\n0.000\nPneumonia\n0.429\n0.400\n0.031\n0.143\n0.400\nPneumothorax\n0.842\n0.308\n0.033\n0.179\n0.286\nSupport Devices\n0.970\n0.808\n0.637\n0.728\n0.818\nOverall Average (Macro F1)\n0.755\n0.362\n0.228\n0.282\n0.346\nTable 6: Comparative F1-Score performance across five models on the out-of-\ndistribution dataset, including overall aggregated performance. The best per-\nforming model for each category is highlighted in bold.\nF1-Score\nCategory\nNV-Reason\nMedGemma\nQwen2.5\nMedGemma\n(SFT)\n(ChexReason)\nAtelectasis\n0.422\n0.300\n0.000\n0.264\n0.260\nCardiomegaly\n0.543\n0.482\n0.000\n0.440\n0.312\nConsolidation\n0.056\n0.029\n0.109\n0.200\n0.178\nEdema\n0.273\n0.522\n0.000\n0.526\n0.429\nLung Lesion\n0.397\n0.195\n0.115\n0.113\n0.179\nNo Finding\n0.283\n0.275\n0.215\n0.343\n0.250\nPleural Other\n0.152\n0.180\n0.000\n0.248\n0.082\nPneumonia\n0.000\n0.202\n0.117\n0.138\n0.182\nPneumothorax\n0.552\n0.000\n0.000\n0.416\n0.317\nOverall Average (Macro F1)\n0.297\n0.243\n0.062\n0.299\n0.243\n16\n"}, {"page": 17, "text": "5\nConclusion\nThis study examined whether R1-style training, combining supervised fine-\ntuning with Group Relative Policy Optimization, enhances multilabel chest\nX-ray classification in small vision-language models under low-resource condi-\ntions. We trained MedGemma-4B and Qwen2.5-VL-3B-Instruct using teacher-\ngenerated reasoning traces from Gemini 2.5, employing only 2,000 SFT and\n1,000 RL samples on a single A100 GPU. Our ChexReason model achieved a 23%\nimprovement over the SFT checkpoint on the standard CheXpert benchmark\n(macro-F1 = 0.346), despite training on out-of-distribution MIMIC-CXR-JPG\ndata.\nHowever, this success severely compromised cross-dataset generalization.\nOn the NIH Chest X-ray dataset, which uses different labeling methodology,\nChexReason performance degraded by 19%, reverting to baseline levels (macro-\nF1 = 0.243). This mirrors the high-resource NV-Reason-CXR-3B model, which\nalso saw massive drops on NIH data despite state-of-the-art CheXpert scores.\nParadoxically, our SFT checkpoint uniquely improved on NIH (0.282 →0.299\nmacro-F1) while showing weaker CheXpert performance, suggesting teacher-\nguided reasoning traces capture more generalizable visual-semantic relationships\nthan reward-optimized outputs. Our cross-model comparison further revealed\nthat instruction format effectiveness depends on medical pre-training. Qwen2.5-\nVL-3B, lacking domain-specific representations, performed best with explicit\n12-step structured reasoning (macro-F1 = 0.208), while MedGemma-4B per-\nformed best with direct label prediction (macro-F1 = 0.253).\nThis suggests\nstructured reasoning scaffolds can compensate for missing domain knowledge,\nbut become redundant, or even detrimental, when medical pre-training has al-\nready internalized clinical reasoning patterns. These findings reveal a tension in\nreinforcement learning for small medical VLMs: reward signals recover bench-\nmark performance by exploiting dataset-specific semantics, but potentially de-\ngrade transferability across institutions. Since both the high-resource NVIDIA\nmodel and our low-resource ChexReason model failed similarly, the issue likely\nstems from the RL fine-tuning paradigm itself when applied to small models\non standardized benchmarks. Consequently, R1-style training on hard labels\nsuch as CheXpert may be counterproductive for clinical deployment requiring\nrobustness across diverse populations. Practitioners under resource constraints\nmay be better served by curated, supervised fine-tuning rather than aggressive\nbenchmark optimization.\nReferences\n[1] Abdullah Abdullah and Seong Tae Kim.\nAutomated Radiology Report\nLabeling in Chest X-Ray Pathologies: Development and Evaluation of a\nLarge Language Model Framework. JMIR Medical Informatics, 13:e68618–\ne68618, March 2025.\n17\n"}, {"page": 18, "text": "[2] BahaaEldin0.\nNih-chest-xray-14.\nhttps://huggingface.co/datasets/\nBahaaEldin0/NIH-Chest-Xray-14, 2024. Accessed: 2025-12-25.\n[3] Oishi Banerjee, Hong-Yu Zhou, Subathra Adithan, Stephen Kwak, Kay\nWu, and Pranav Rajpurkar.\nDirect preference optimization for sup-\npressing hallucinated prior exams in radiology report generation, 2024.\narXiv:2406.06496.\n[4] Prakhar Bhardwaj, Sheethal Bhat, and Andreas Maier. Enhancing zero-\nshot learning in medical imaging: integrating clip with advanced techniques\nfor improved chest x-ray analysis, March 2025. arXiv:2503.13134 [cs].\n[5] Christopher Boland, Sotirios A. Tsaftaris, and Sonia Dahdouh. Preventing\nShortcut Learning in Medical Image Analysis through Intermediate Layer\nKnowledge Distillation from Specialist Teachers.\nMachine Learning for\nBiomedical Imaging, 3, 2025.\n[6] Rhys Compton, Lily Zhang, Aahlad Puli, and Rajesh Ranganath. When\nmore is less: Incorporating Additional Datasets Can Hurt Performance By\nIntroducing Spurious Correlations.\nIn Proceedings of Machine Learning\nResearch, volume 219, pages 1–24. PMLR, 2023.\n[7] Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, and Bo Wang.\nMedRAX:\nMedical\nReasoning\nAgent\nfor\nChest\nX-ray,\nMay\n2025.\narXiv:2502.02673 [cs].\n[8] Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi\nXie. ChestX-Reasoner: Advancing Radiology Foundation Models with Rea-\nsoning through Step-by-Step Verification, May 2025. arXiv:2504.20930 [cs].\n[9] Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff,\nPlamen C. Ivanov, Roger G. Mark, et al. PhysioBank, PhysioToolkit, and\nPhysioNet: Components of a new research resource for complex physio-\nlogic signals. Circulation, 101(23):e215–e220, 2000. Circulation [Online].\nRRID:SCR 007345.\n[10] Google DeepMind. Medgemma: Medical generative multimodal architec-\nture. https://huggingface.co/google/medgemma-4b-it, 2024. Hugging\nFace model card.\n[11] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary\nMueller, and Sourab Mangrulkar.\nAccelerate:\nTraining and inference\nat scale made simple, efficient and adaptable.\nhttps://github.com/\nhuggingface/accelerate, 2022.\n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of\nlarge language models. In International Conference on Learning Represen-\ntations, 2022.\n18\n"}, {"page": 19, "text": "[13] X. Hu, L. Gu, K. Kobayashi, L. Liu, M. Zhang, T. Harada, R. Summers,\nand Y. Zhu. Medical-CXR-VQA dataset: A Large-Scale LLM-Enhanced\nMedical Dataset for Visual Question Answering on Chest X-Ray Images,\n2025. Version 1.0.0. RRID:SCR 007345.\n[14] Xinyue Hu, Lin Gu, Kazuma Kobayashi, Liangchen Liu, Mengliang Zhang,\nTatsuya Harada, Ronald M. Summers, and Yingying Zhu. Interpretable\nmedical image visual question answering via multi-modal relationship graph\nlearning. Medical Image Analysis, 97:103279, October 2024.\n[15] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-\nIlcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie\nShpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Matthew P.\nLungren, Andrew Y. Ng, and Curtis P. Langlotz. Chexpert: A large chest\nradiograph dataset with uncertainty labels and expert comparison.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence, volume 33,\npages 590–597, 2019.\n[16] Jongseong\nJang,\nDaeun\nKyung,\nSeung\nHwan\nKim,\nHonglak\nLee,\nKyunghoon Bae, and Edward Choi. Significantly improving zero-shot X-\nray pathology classification via fine-tuning pre-trained image-text encoders.\nScientific Reports, 14(1):23199, October 2024.\n[17] Alistair E. W. Johnson et al. MIMIC-CXR-JPG, a large publicly available\ndatabase of labeled chest radiographs, 2019. arXiv:1901.07042.\n[18] Yuxiang Lai et al. Med-r1: Reinforcement learning for generalizable med-\nical reasoning in vision-language models, March 2025. arXiv:2503.13939\n[cs.CV].\n[19] Khai Le-Duc, Ryan Zhang, Ngoc Son Nguyen, Tan-Hanh Pham, Anh Dao,\nBa Hung Ngo, Anh Totti Nguyen, and Truong-Son Hy. Litegpt: Large\nvision-language model for joint chest x-ray localization and classification\ntask, 2024. arXiv:2407.12064.\n[20] Bo Liu, Xiangyu Zhao, Along He, Yidi Chen, Huazhu Fu, and Xiao-Ming\nWu. Gemex-rmcot: An enhanced med-vqa dataset for region-aware multi-\nmodal chain-of-thought reasoning, 2025. arXiv:2506.17939.\n[21] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao\nDu, Wee Sun Lee, and Min Lin. Understanding R1-Zero-like training: A\ncritical perspective. arXiv preprint arXiv:2503.20783, 2025.\n[22] Mahshad Lotfinia, Arash Tayebiarasteh, Samaneh Samiei, Mehdi Joodaki,\nand Soroosh Tayebi Arasteh. Boosting multi-demographic federated learn-\ning for chest radiograph analysis using general-purpose self-supervised\nrepresentations.\nEuropean Journal of Radiology Artificial Intelligence,\n3:100028, September 2025.\n19\n"}, {"page": 20, "text": "[23] Nihal Murali, Aahlad Puli, Ke Yu, Rajesh Ranganath, and Kayhan Bat-\nmanghelich. Beyond Distribution Shift: Spurious Features Through the\nLens of Training Dynamics. Transactions on Machine Learning Research,\n2023.\n[24] Andriy Myronenko, Dong Yang, Baris Turkbey, Paul Salama, et al. Reason-\ning visual language model for chest x-ray analysis, 2025. arXiv:2510.23968\n[cs.CV].\n[25] Chee Ng, Liliang Sun, and Shaoqing Tang. X-Ray-CoT: Interpretable Chest\nX-ray Diagnosis with Vision-Language Models via Chain-of-Thought Rea-\nsoning, August 2025. arXiv:2508.12455 [cs].\n[26] Gabriel Iluebe Okolo, Stamos Katsigiannis, and Naeem Ramzan.\nCLN:\nA multi-task deep neural network for chest X-ray image localisation and\nclassification.\nExpert Systems with Applications, 288:128162, September\n2025.\n[27] Jiazhen Pan et al. Medvlm-r1: Incentivizing medical reasoning capability of\nvision-language models (vlms) via reinforcement learning, February 2025.\narXiv:2502.19634 [cs.CV].\n[28] Chantal Pellegrini,\nNicolas Deperrois,\net al.\nRadvlm:\nA multi-\ntask conversational vision-language model for radiology, February 2025.\narXiv:2502.03333 [cs.CV].\n[29] Tan-Hanh Pham and Chris Ngo. Rarl: Improving medical vlm reasoning\nand generalization with reinforcement learning and lora under data and\nhardware constraints, 2025. arXiv:2506.06600.\n[30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao\nBi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo.\nDeepseekmath: Pushing the limits of mathematical reasoning in open lan-\nguage models. 2024.\n[31] Yanzhou Su, Tianbin Li, Jiyao Liu, et al.\nGmai-vl-r1:\nHarnessing\nreinforcement learning for multimodal medical reasoning, April 2025.\narXiv:2504.01886 [cs.CV].\n[32] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang,\nZhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-\ntuning for visual reasoning, 2025. arXiv:2503.20752.\n[33] Ryutaro Tanno, David G. T. Barrett, Andrew Sellergren, Sumedh Ghaisas,\nSumanth Dathathri, Abigail See, Johannes Welbl, Charles Lau, Tao Tu,\nShekoofeh Azizi, Karan Singhal, Mike Schaekermann, Rhys May, Roy Lee,\nSiWai Man, Sara Mahdavi, Zahra Ahmed, Yossi Matias, Joelle Barral,\nS. M. Ali Eslami, Danielle Belgrave, Yun Liu, Sreenivasa Raju Kalidindi,\nShravya Shetty, Vivek Natarajan, Pushmeet Kohli, Po-Sen Huang, Alan\n20\n"}, {"page": 21, "text": "Karthikesalingam, and Ira Ktena.\nCollaboration between clinicians and\nvision–language models in radiology report generation. Nature Medicine,\n31(2):599–608, February 2025.\n[34] Qwen Team.\nQwen2.5-vl-3b-instruct.\nhttps://huggingface.co/Qwen/\nQwen2.5-VL-3B-Instruct, 2025.\n[35] Kevin Vogt-Lowell, Noah Lee, Theodoros Tsiligkaridis, and Marc Vaillant.\nRobust Fine-Tuning of Vision-Language Models for Domain Generaliza-\ntion. In Proceedings of the IEEE High Performance Extreme Computing\nConference, 2023.\n[36] Xiao Wang, Fuling Wang, Yuehang Li, Qingchuan Ma, Shiao Wang,\nBo Jiang, Chuanfu Li, and Jin Tang. CXPMRG-Bench: Pre-training and\nBenchmarking for X-ray Medical Report Generation on CheXpert Plus\nDataset, October 2024. arXiv:2410.00379 [cs].\n[37] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang,\nMing Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment\nin large vision-language models via vision-guided reinforcement learning,\n2025. arXiv:2503.18013.\n[38] Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Juli´an N.\nAcosta, Josh Miller, Ouwen Huang, and Pranav Rajpurkar. ReXrank: A\nPublic Leaderboard for AI-Powered Radiology Report Generation, Novem-\nber 2024. arXiv:2411.15122 [cs].\n[39] Zijian Zhao et al. Med-rlvr: Emerging medical reasoning from a 3b base\nmodel via reinforcement learning, 2025. arXiv:2502.19655.\n[40] Chujie Zheng, Kai Dang, Bowen Yu, Mingze Li, Huiqiang Jiang, Junrong\nLin, Yuqiong Liu, An Yang, Jingren Zhou, and Junyang Lin. Stabilizing\nreinforcement learning with llms: Formulation and practices, 2025.\n[41] Kangyu Zhu, Peng Xia, Yun Li, Hongtu Zhu, Sheng Wang, and Huaxiu\nYao. Mmedpo: Aligning medical vision-language models with clinical-aware\nmultimodal preference optimization, 2025. arXiv:2412.06141, ICML 2025.\n[42] Wenhui Zhu, Xuanzhao Dong, Xin Li, Peijie Qiu, Xiwen Chen, Abolfazl\nRazi, Aris Sotiras, Yi Su, and Yalin Wang.\nToward effective reinforce-\nment learning fine-tuning for medical vqa in vision-language models, 2025.\narXiv:2505.13973.\n21\n"}]}