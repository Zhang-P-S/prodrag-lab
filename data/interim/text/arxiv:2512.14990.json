{"doc_id": "arxiv:2512.14990", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.14990.pdf", "meta": {"doc_id": "arxiv:2512.14990", "source": "arxiv", "arxiv_id": "2512.14990", "title": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent", "authors": ["Mehil B Shah", "Mohammad Masudur Rahman", "Foutse Khomh"], "published": "2025-12-17T00:50:58Z", "updated": "2026-01-26T19:02:00Z", "summary": "Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.14990v2", "url_pdf": "https://arxiv.org/pdf/2512.14990.pdf", "meta_path": "data/raw/arxiv/meta/2512.14990.json", "sha256": "da8f2f439cfb29a716afafb8e965f34df2f4e842c221e2f245024bb9dfee2aa9", "status": "ok", "fetched_at": "2026-02-18T02:24:17.829077+00:00"}, "pages": [{"page": 1, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an\nIntelligent Agent\nMehil B Shah\nDalhousie University\nHalifax, Canada\nshahmehil@dal.ca\nMohammad Masudur Rahman\nDalhousie University\nHalifax, Canada\nmasud.rahman@dal.ca\nFoutse Khomh\nPolytechnique Montreal\nMontreal, Canada\nfoutse.khomh@polymtl.ca\nAbstract\nDespite their wide adoption in various domains (e.g., healthcare,\nfinance, software engineering), Deep Learning (DL)-based applica-\ntions suffer from many bugs, failures, and vulnerabilities. Reproduc-\ning these bugs is essential for their resolution, but it is extremely\nchallenging due to the inherent nondeterminism of DL models and\ntheir tight coupling with hardware and software environments. Ac-\ncording to recent studies, only about 3% of DL bugs can be reliably\nreproduced using manual approaches. To address these challenges,\nwe present RepGen, a novel, automated, and intelligent approach\nfor reproducing deep learning bugs. RepGen constructs a learning-\nenhanced context from a project, develops a comprehensive plan\nfor bug reproduction, employs an iterative generate-validate-refine\nmechanism, and thus generates such code using an LLM that re-\nproduces the bug at hand. We evaluate RepGen on 106 real-world\ndeep learning bugs and achieve a reproduction rate of 80.19%, a\n19.81% improvement over the state-of-the-art measure. A developer\nstudy involving 27 participants shows that RepGen improves the\nsuccess rate of DL bug reproduction by 23.35%, reduces the time to\nreproduce by 56.8%, and lowers participants‚Äô cognitive load.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚ÜíSoftware testing and debug-\nging.\nKeywords\nDeep learning bugs, deep learning bug reproduction, automated de-\nbugging, LLM-powered agents, code generation, machine learning\nsystems, software testing and debugging\nACM Reference Format:\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh. 2026.\nImitation Game: Reproducing Deep Learning Bugs Leveraging an Intelli-\ngent Agent. In 2026 IEEE/ACM 48th International Conference on Software\nEngineering (ICSE ‚Äô26), April 12‚Äì18, 2026, Rio de Janeiro, Brazil. ACM, New\nYork, NY, USA, 13 pages. https://doi.org/10.1145/3744916.3787795\n1\nIntroduction\nArtificial Intelligence (AI) has been widely adopted in many applica-\ntion domains, including software engineering [45, 46], autonomous\nvehicles [27], healthcare [64], finance [10], and cybersecurity [16].\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nNoDerivatives 4.0 International License.\nICSE ‚Äô26, Rio de Janeiro, Brazil\n¬© 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2025-3/2026/04\nhttps://doi.org/10.1145/3744916.3787795\nThe global market share of AI software reached $34.8 billion in 2023\nand is projected to grow up to $360 billion by 2030 [26]. Over 67% of\ntop-performing companies have incorporated AI in their business\nsolutions, and 97% of Fortune 500 companies have invested in AI\ntechnologies [58], indicating their significance. However, software\napplications empowered by Deep Learning (DL), the underlying\ntechnology behind current AI systems, remain prone to bugs, faults,\nand vulnerabilities, which could lead to major consequences (e.g.,\nsystem crashes) and catastrophic failures (e.g., autonomous vehicle\naccidents) [69]. Unlike the bugs in traditional, developer-written\nsoftware, the bugs in DL software are inherently challenging due\nto several factors. First, they are often non-deterministic due to ran-\ndomness in model training, i.e., random weight initialization of the\nmodel layers [52]. Second, DL models perform high-dimensional\ntensor operations and suffer from a lack of interpretability, making\ntheir encountered bugs opaque [46]. Finally, these bugs also have\nmulti-faceted dependencies on hardware (e.g., GPU), underlying\nframeworks (e.g., PyTorch, TensorFlow) [81], and data pipelines,\nmaking them highly complex [62].\nTo resolve DL bugs, software developers must first systematically\nreproduce them on their local machines. Without a reproduction,\nthey cannot confirm the presence of a bug or diagnose its root cause.\nHowever, reproduction of DL bugs can be effort-intensive, time-\nconsuming, and frustrating due to various technical challenges.\nThey include intricate data pipelines, hardware dependencies, and\nvariations in software frameworks and library versions. Even when\na bug is reproducible, developers frequently need to engage in\ntrial-and-error, carefully tune environmental settings, and reason\nabout the contextual factors that may influence the behaviour of\nDL programs, all of which can be tedious and error-prone [54].\nDevelopers also face the challenge of missing or incomplete infor-\nmation when attempting to reproduce bugs from issue reports [71].\nReports may lack crucial details of a bug and omit relevant data\nor code snippets. In such cases, even experienced developers must\nspend substantial time reconstructing the missing detail of a bug,\nthe target environment, and iteratively testing hypotheses to re-\nproduce the erroneous behaviour [32, 82]. These difficulties make\nthe manual process of reproducing DL bugs highly inefficient and\nerror-prone. Thus, there is a strong need for techniques that can\naccelerate and systematize the reproduction of DL bugs, reducing\ndeveloper effort while increasing the reliability of DL systems.\nOver the last two decades, there have been many techniques to\nsupport developers in localizing [20, 47, 60], reproducing [40, 53],\nand correcting software bugs [17, 44, 78]. However, they are not\nsufficient since they were not designed to tackle deep learning\nspecific challenges. They might fail to effectively reproduce DL\nbugs or may do so inefficiently due to the following limitations:\narXiv:2512.14990v2  [cs.SE]  26 Jan 2026\n"}, {"page": 2, "text": "ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh\n(a) Limited contextual understanding: Existing techniques\n(e.g., RecDroid [84], ReBL [73], and AdbGPT [24]) analyze bug re-\nports and GUI applications and leverage UI interactions or code\nexecution paths to reproduce software bugs. In contrast, DL bugs\ngo beyond source code and can originate from multiple artifacts\nscattered across different parts of the ML pipeline, including model\narchitectures, training datasets, preprocessing scripts, hyperparam-\neters, underlying frameworks, and hardware (e.g., GPU) [63]. Col-\nlecting and correlating information from these scattered artifacts\nsystematically is challenging, as each may contribute in subtle ways\nto the manifestation of the bug. Existing techniques lack mecha-\nnisms to identify or utilize these DL-specific contextual clues, even\nwhen they are present in bug reports or code, which makes these ap-\nproaches inefficient or insufficient for reliably reproducing DL bugs.\n(b) Heavy reliance on GUI-specific elements: Many exist-\ning techniques‚ÄîRecDroid [84], ReBL [73], AdbGPT [24], ReAct-\nDroid [33], LLMDroid [70]‚Äîcapture well-defined states of GUI-\nbased applications using record-and-replay methods [31] and gen-\nerate deterministic event sequences to reproduce software faults.\nHowever, DL bugs originate from various stochastic processes such\nas model training, hyperparameter optimization, and data pipeline\nexecution, which do not involve any GUI interactions [63]. Thus,\nexisting techniques relying on GUI interactions (e.g., mouse clicks,\nor screen capture) are fundamentally misaligned with DL-based\nsystems, which makes them less effective for reproducing DL bugs.\n(c) Challenges with ‚Äúsilent‚Äù bugs: Traditional techniques often\nverify the reproduction of a software bug by checking for crashes or\nundesirable program states [53, 66]. However, DL bugs can manifest\nthemselves in non-conventional ways, such as increased losses dur-\ning model training or extremely poor performance during model\ninference. Bugs with such symptoms are called silent bugs in liter-\nature [67]. According to a recent study [67], 72.8% of developers\nreported difficulty in resolving silent bugs, which highlights the per-\nsistent challenges of these bugs. Thus, existing techniques [53, 66]\nrelying on crashes or state-specific symptoms might be inadequate\nfor accurately reproducing silent bugs from DL systems.\nGiven these limitations of existing techniques, we present Rep-\nGen, an automated, intelligent approach for reproducing DL bugs,\nwith a specific focus on accelerating and systematizing their re-\nproduction. Unlike the existing techniques, our approach lever-\nages a learning-enhanced context for code generation and approx-\nimates fault symptoms of the code using LLMs, thereby improv-\ning the speed, consistency, and reliability of DL bug reproduction.\nFirst, our technique builds a learning-enhanced context against\na DL bug by capturing relevant code containing model training\nloops and their dependencies from a project. Second, using the con-\ntext, RepGen generates a comprehensive plan for bug reproduction,\nwhich captures the environment setup, model training mechanism,\nand inference pipeline. Finally, both the context and the plan are\npassed to the LLM-powered agent, which runs an iterative gener-\nate‚Äìvalidate‚Äìrefine cycle to generate code that can reproduce the\nbug at hand. Our agent not only evaluates the code using standard\nmechanisms (e.g., compiler feedback, static analysis, relevance feed-\nback) [61, 68, 75] but also refines the code and validates its fault\nsymptoms using an LLM (e.g., Qwen2.5-Coder-7B) to enhance its\neffectiveness in reproducing DL bugs. To the best of our knowledge,\nFigure 1: Bug #228 in X-Transformer; red boxes indicate incorrect or\nmissing parameters in the user-provided snippet.\nRepGen is the first technique to offer a unified, intelligent solution\nto systematically reproduce DL bugs, making our work novel.\nWe evaluated RepGen using a comprehensive dataset of 106 real-\nworld deep learning bugs collected from 16 GitHub projects. Our\ntechnique successfully reproduced 85 (80.19%) bugs and outper-\nformed eight LLM-only baselines (e.g., GPT 4.1, Llama3, DeepSeek-\nR1, Qwen3, Qwen2.5 and their variants), indicating its superiority.\nA controlled developer study with 27 participants further validated\nour results, showing that participants can reproduce ‚âà23.35% more\nbugs when assisted by RepGen and can save ‚âà56.8% time during bug\nreproduction. All these findings suggest a strong potential of our\ntechnique for supporting the systematic and efficient reproduction\nof deep learning bugs.\nWe thus make the following contributions in this paper:\n(a) A novel, intelligent technique - RepGen - to support the\nreproduction of deep learning bugs, emphasizing the acceleration\nand systematization of bug reproduction.\n(b) A novel agentic workflow that builds a comprehensive, learning-\nenhanced context, develops a targeted reproduction plan, and lever-\nages a feedback-driven reproduction agent that can generate high-\nquality code capable of reproducing DL bugs efficiently.\n(c) A comprehensive experiment comprising an empirical evalu-\nation of RepGen, an ablation study targeting 13 major components,\ncomparisons with eight baselines, and a developer study involving\n27 participants, demonstrating the benefits of our technique.\n(d) A carefully curated benchmark dataset containing 106 bug\nreports and associated artifacts (e.g., code examples, configuration\nfiles) from 16 real-world deep learning projects.\n(e) A replication package [9] containing our implementation,\ndataset, and experimental results to enable future research.\n2\nMotivating Example\nTo demonstrate the effectiveness of our technique, let us consider\nthe bug in Fig. 1 (Bug #228, X-transformer). The bug occurs in\nthe TransformerWrapper class when two specific parameters ‚Äì\nattn_num_mem_kv and attn_one_kv_head ‚Äì are initialized simul-\ntaneously. In particular, when the model uses a single shared key-\nvalue head (attn_one_kv_head = True) and simultaneously allo-\ncates the memory for additional key-value pairs (attn_num_mem_kv\n= 20), it leads to an incompatible configuration for the attention\nmodel, which causes a ValueError.\nAs shown in Fig. 1, the bug report provides a partially complete\ncode snippet and lacks necessary details (e.g., required libraries). Re-\nproducing such bugs manually is possible, but it requires developers\nto identify missing dependencies, understand parameter interde-\npendencies, configure the target environment, and iteratively test\nfor the reported faulty behaviour, which can be time-consuming\n"}, {"page": 3, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent\nICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nFigure 2: Reproduction code generated by RepGen, with green boxes\nhighlighting additions that make the script verifiable and executable.\nand error-prone. Existing techniques for bug reproduction - RecD-\nroid [84], AdbGPT [24] - rely heavily on GUI-based interactions\nand event sequences, making them non-applicable to DL models\noperating through training data, model structure, hardware con-\nfigurations, and underlying frameworks. Hence, they naturally\nfail to reproduce such bugs. On the other hand, ChatGPT, a high-\nperforming LLM, accepts the bug report (Fig. 1) and generates a\nnon-compilable code snippet1 with incorrect import statements.\nIn contrast, our technique, RepGen, systematizes the bug re-\nproduction process and provides a code example (Fig. 2) that suc-\ncessfully reproduces the bug. First, RepGen constructs a learning-\nenhanced context by combining the code retrieved from the Trans-\nformerWrapper and Decoder classes with the useful information\nfrom the bug report (e.g., erroneous behaviors). Second, RepGen\nleverages the context and formulates an appropriate plan to re-\nproduce the bug. Finally, the context and the plan are passed to\nour reproduction agent. The reproduction agent then generates\nan initial version of code that misses required import statements,\nas detected by PyLint. Using PyLint‚Äôs feedback, our reproduction\nagent generates a more complete and relevant code snippet, but\nit still lacks an important configuration needed to reproduce the\nbug (e.g., both attn_num_mem_kv and attn_one_kv_head should\nbe initialized simultaneously). This issue was detected by our re-\nproduction agent, suggesting that the code missed the symptoms\nmentioned in the bug report. By incorporating this feedback into\nthe regeneration process, RepGen then generates a complete code\nexample that can reproduce the bug (Fig. 2).\nIn short, by leveraging a learning-enhanced context, comprehen-\nsive planning, multi-phase feedback, and iterative refinement, our\ntechnique enables the automated and structured reproduction of DL\nbugs that would otherwise require substantial manual effort. Rep-\nGen completes this process in ‚âà5 minutes, demonstrating its ability\nto accelerate and systematize bug reproduction. This also highlights\nour technique‚Äôs potential for improving developer productivity and\nensuring consistent, verifiable reproduction of DL bugs.\n3\nRepGen\nFig. 3 shows the schematic diagram of our proposed technique ‚Äì\nRepGen‚Äì for DL bug reproduction. We discuss the major steps of\nour methodology as follows.\n3.1\nConstruction of Learning-Enhanced Context\nSince we use an LLM to generate code capable of reproducing bugs\nfrom real-life projects, we need project-specific context. We thus\n1http://bit.ly/462AOlj\nleverage a hybrid retrieval framework and construct a learning-\nenhanced context against a bug report as follows (Step 1a, Fig. 3).\n3.1.1\nPre-Retrieval. Before performing any retrieval, we apply a\nseries of preprocessing steps to the codebase as follows.\n(a) Semantic chunking: We break each source code file into\nsmaller, semantically meaningful segments through code chunk-\ning [43]. It is essential since DL codebases often contain long, com-\nplex files spanning hundreds of lines. We employ Abstract Syntax\nTree (AST) parsing, identify natural partitioning points (e.g., method\nboundary, class declaration), and break each file into smaller chunks\nthat are structurally correct and semantically relevant.\n(b) Indexing: After splitting the code into chunks, we index\nthem for efficient search and real-time analysis. We construct two\ncomplementary indices for efficient retrieval of relevant code snip-\npets: a sparse retrieval index that can be leveraged by the BM25\nranking function [59] and a dense retrieval index that can be lever-\naged for embedding-based semantic retrieval.\n3.1.2\nCode Snippet Retrieval. To retrieve relevant code against a\nbug report, we design a hybrid retrieval approach leveraging the\nabove indices as follows.\n(a) BM25 search: Best Matching 25 (BM25) is a probabilistic al-\ngorithm for similarity matching that incorporates document length\nnormalization and term frequency saturation. It can identify code\nchunks that contain exact matches of API calls, error messages, or\nspecific identifiers found in a bug report. Given a query ùëÑderived\nfrom the bug description, we compute the BM25 score for a code\nchunk ùê∑[59]. We also normalize the BM25 scores across all the\nchunks for an effective analysis [79].\n(b) ANN search: Approximate Nearest Neighbour (ANN) search\nleverages semantic proximity between two items within the seman-\ntic space. We use an ANN search to capture the conceptual or se-\nmantic relationship between code chunks and bug reports that may\nnot be evident from their contents. Using a code-language model2,\nwe encode each bug report and code chunk into dense vectors and\nnormalize them. Then, to measure the similarity between the bug\nreport (q) and code (d), we compute their angular similarity [1].\n(c) Hybrid scoring: To leverage the complementary strengths\nof both approaches, we combine the normalized scores from the\nBM25 Search and ANN Search using the formula below:\nscoreh(ùê∑) = (1 ‚àíùõº) ¬∑ BM25norm(ùëÑ, ùê∑) + ùõº¬∑ simangular(q, d)\nAfter iterative hyperparameter tuning, we set ùõºto 0.55. We then\nrank the code snippets by their hybrid scores and select the top K\n(e.g., 20) for further re-ranking.\n(d) Reranking: After retrieving a short list of code snippets\nabove, we employ a cross-encoder3 to re-rank them. The ms-marco-\nMiniLM-L12-v2 cross-encoder demonstrates a strong capability in\nunderstanding texts and code, making it a suitable choice for our\napproach. First, MiniLMv2, distilled from the BERT and RoBERTa\narchitectures, inherits their strong performance on code-specific\ntasks [22, 41, 72]. Second, its fine-tuning on the MS Marco dataset [14],\nderived from Bing search queries, provides a comprehensive under-\nstanding of human-generated natural language. Unlike the above\nretrieval methods, which rely on lexical or semantic matching, the\n2http://bit.ly/3IwN0kk\n3http://bit.ly/454NWFo\n"}, {"page": 4, "text": "ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh\nBug Report\nCode \nRepository\nConstruction\nof Learning-\nEnhanced Context\nBug Report\nRestructuring\nGeneration \nof Plans\n2\nInitial Code \nGeneration\nStructural \nFeedback\nStatic Analysis \nFeedback\nRelevance \nFeedback\nRuntime\nFeedback\nReproduction\nAgent\n3\nReproduction Agent\nif, syntactic errors\nif, missing imports, function definitions, etc.\nif, non-relevant code, update\nthe context and plan\nif, low likelihood of \nreproduction\n1a\n1b\n3a\n3b\n3c\n3d\n3e\nReproducible\nCode Snippet\nInput\nOutput\nplan +\ncontext\nFigure 3: Schematic diagram of our proposed technique - RepGen\ncross-encoder jointly encodes both the bug report and code snippet,\nallowing its attention mechanisms to detect their exact relation-\nships. It can also identify subtle but important connections that the\nsimpler retrieval methods might miss.\nGiven a bug report ùëÑand code snippet ùê∑, the cross-encoder\ncomputes a relevance score, as follows:\nscorecross(ùëÑ, ùê∑) = CrossEncoder([CLS] ‚äïùëÑ‚äï[SEP] ‚äïùê∑‚äï[SEP])\nwhere ‚äïdenotes concatenation with special tokens (e.g., CLS, SEP),\nand the model analyzes cross-attention between all tokens to pro-\nduce a score between 0 and 1. Based on the cross-encoder scores,\nwe re-rank the top K snippets (e.g., 20) for further processing.\n(e) Capturing dependencies: Deep learning pipeline involves\nmultiple components, and their bugs could be multifaceted. For\nexample, a bug in a model training loop might depend on the cus-\ntom layer‚Äôs definition, data preprocessing functions, and custom\nloss functions defined in separate modules. Therefore, for each re-\ntrieved snippet ùê∑ùëñ, we parse its AST and identify the corresponding\ndependencies (e.g., imported modules, referenced variables). The\ndependency resolution ensures that all necessary code components\nare captured for subsequent analysis.\n3.1.3\nContext Construction. Once relevant code snippets and their\ndependencies are captured, we construct a unified context that\ncan help LLMs generate appropriate code reproducing deep learn-\ning bugs. To achieve this, we perform several analyses using the\nretrieved code and bug report as follows.\n(a) Module-centric partitioning & retrieval: We organize the\nretrieved code snippets based on their source modules. Our idea\nwas to select the modules containing the most relevant code to the\nbug. We thus map the code snippets to their corresponding system\npaths and organize them in modules:\nùëÄùëò= ResolveModule(ùê∑ùëñ),\nùëò‚àà{1, ..., ùëÅ}, ùëÅ‚â§5\nwhere ùê∑ùëñrefers to an individual code snippet, and ResolveModule\nis the function that assigns it to a specific module, ùëÄùëò. In this step,\nwe cluster the code snippets according to their respective modules\nand determine each module‚Äôs priority based on its members‚Äô maxi-\nmum score from the re-ranking step. In other words, a module with\na single highly relevant snippet will be prioritized over one with\nnumerous less relevant snippets. After determining the module\npriority, we select the top five modules for subsequent analysis.\n(b) AST-driven training loop extraction: Through a com-\nprehensive analysis of TensorFlow and PyTorch documentation,\nwe identified eight framework-specific heuristics for detecting\nmodel training loops. They include method calls like model.fit(),\noptimizer.step(), and custom training loop patterns. These heuris-\ntics are available in our replication package [9]. Using these heuris-\ntics and Abstract Syntax Tree (AST)-based parsing, we capture the\npotential training loops from each module:\nLùëò=\n√ò\nùê∑ùëñ‚ààùëÄùëò\nExtractLoops(AST(ùê∑ùëñ))\nThis extraction systematically identifies various components of a\ntraining loop, including forward/backward passes, gradient com-\nputations, optimizer steps, loss calculations, and data loader in-\nteractions. These specific components are crucial because they\naccurately distinguish actual training loops from other code snip-\npets (e.g., model definitions, preprocessing scripts). The identified\ntraining loops are then used for subsequent steps.\n(c) Training loop ranking: Given that a module may contain\nmultiple training loops, we need to identify those most relevant\nto a reported bug. We employ the same cross-encoder4 as the re-\nranking step to assess the relevance of each training loop to the bug\nreport, leveraging its strong understanding of both code and natural\nlanguage text. Given a training loop ùêøùëó, and the bug report query\n(Q), the cross-encoder computes the relevance score as follows:\nscoreloop(ùêøùëó) = CrossEncoder([CLS] ‚äïùëÑ‚äï[SEP] ‚äïùêøùëó)\nwhere ‚äïdenotes concatenation with special tokens (e.g., CLS, SEP).\nFrom each module, only one loop, specifically the one with the\nhighest relevance score, is selected for the subsequent steps.\n(d) Construction of learning-enhanced code context: Once\ntraining loops are collected, we construct the code context for every\nmodule by combining its training loop, relevant code snippets, and\ndependencies as follows:\n4http://bit.ly/454NWFo\n"}, {"page": 5, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent\nICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nùê∂ùëò=\nùêøùëò\n|{z}\ntraining loop\n‚à™\n√ò\nùê∑ùëñ‚ààTop5 (ùëÄùëò)\nùê∑ùëñ\n|           {z           }\nrelevant code snippets\n‚à™DepGraph(ùê∑ùëñ)\n|             {z             }\ndependencies\nCout = [ùê∂1, ...,ùê∂ùêæ]\nHere, ùê∂ùëòrepresents each of the top K (e.g., 5) contexts con-\nstructed. Our reproduction agent leverages each of these contexts\nuntil it succeeds in generating the code capable of reproducing a DL\nbug. By combining the most relevant training loops, code snippets,\nand their dependencies, we thus construct a learning-enhanced\ncode context that offers useful information to our LLM-based re-\nproduction agent (Step 1a, Fig. 3).\n3.2\nBug Report Restructuring\nUsing an LLM, we restructure each bug report and extract its useful\ninformation (e.g., erroneous behaviours) (Step 1b, Fig. 3), as follows.\nFirst, we analyze the report title and opening paragraphs to ex-\ntract the core problem statement about a bug. Second, we capture\nthe technical details, including stack traces, error messages, per-\nformance metrics, and resource utilization patterns, to form the\nObserved Behaviour section. Third, we also capture the intended\nsystem behaviour from report text, API specifications, and baseline\nmetrics, and construct a detailed Expected Behaviour section. Fi-\nnally, RepGen synthesizes all gathered information into a detailed\nsequence of steps that specifies environment configurations, hard-\nware requirements, dataset specifications, and training parameters,\nand constructs the steps for reproducing the bug. If any steps are\nexplicitly mentioned in a bug report, they are leveraged by the LLM.\nOtherwise, the LLM reasons through the gathered information and\ngenerates a set of candidate steps. Then all three items are used by\nthe LLM to restructure and enhance the bug report.\n3.3\nPlan Generation\nTo reproduce any deep learning bug, we need a comprehensive plan.\nTo construct the prompt for generating plans, we utilize the existing\nguidelines for self-planning [38], which have demonstrated good\nperformance in code generation. Self-planning has been shown\nin existing literature [38] to generate plans that produce more\ncorrect, reliable, and robust code. Using the prompt and an LLM,\nRepGen generates a plan that breaks the task of bug reproduction\n(Step 2, Fig. 3) into small, manageable steps. The plan guides our\nagent through code generation, ensuring all necessary components,\nconfigurations, and validation steps are considered. We adopt the\nfollowing steps to generate our plan:\nFirst, we identify the key components (e.g., training loops, mod-\nule dependencies) and their interactions within each context using\nthe LLM. Second, we capture the reproduction steps and numerical\nparameters from the bug report. Third, we structure the plan into\nmodular, verifiable stages to enable systematic validation and con-\ntrol during bug reproduction attempts. In particular, it incorporates\noutput assertions, resource monitoring, and error verification steps\ninto the plan. Finally, we capture the plan as a structured sequence of\nsteps to ensure seamless integration within the workflow. As a part\nof this step, we generate five plans (one for each context) and then\npass the enriched context and its associated plan to the reproduction\nagent. Fig. 4 shows an example plan generated by our approach.\nFigure 4: Plan generated by RepGen\n3.4\nCode Generation using LLM\nTo execute the above plan and generate code capable of reproducing\nDL bugs, we introduce a specialized LLM-powered agent (Step 3,\nFig. 3). To avoid hallucinations, our agent combines LLMs with an\nenriched context and a robust, multi-stage validation framework.\nSuch a framework enables an iterative refinement process, guided\nby comprehensive feedback, ensuring that the generated code ac-\ncurately and reliably reproduces a bug. We discuss the workflow of\nour bug reproduction agent below.\nCandidate Code Generation: Our agent generates code by\nconsidering the restructured bug report, available code context, and\nour generated plan (Step 3a, Fig. 3). This initial code is likely to\ncontain the critical components (e.g., required import statements,\nmodel architecture, training pipeline). It serves as a foundation that\ncan be evolved to generate the final code snippet reproducing a bug.\nCapturing feedback on generated code: To ensure syntactic\ncorrectness of our generated code, we employ AST-based structural\nvalidation (Step 3b, Fig. 3). It serves as the first line of defence against\nsyntactic errors, ensuring adherence to language grammar rules.\nCode that fails syntactic checks undergoes regeneration, ensuring\nthat only syntactically sound code proceeds to subsequent stages.\nWe employ PyLint-based static analysis (Step 3c, Fig. 3) to identify\npotential vulnerabilities or quality issues in the syntactically cor-\nrect code. It also looks for common programming errors, including\nmissing variable definitions, incorrect import statements, invalid\nfunction calls, and type inconsistencies. The detailed feedback from\nthis step triggers further refinement of the code by the LLM, ensur-\ning adherence to Python best practices and minimizing potential\nruntime failures. Beyond syntactic correctness and completeness,\nwe further incorporate mechanisms to ensure the relevance and\neffectiveness of the generated code. To avoid hallucinations and\nensure relevance to the original bug report, we capture intelligent\nrelevance feedback against the generated code by leveraging an\nexisting method [61] (Step 3d, Fig. 3). The mechanism evaluates the\nsemantic alignment between the generated code and the bug report.\nIf the intelligent relevance feedback indicates the irrelevance of the\ncode, we switch to the next available context. This step is crucial\ngiven the tendency of LLMs to generate plausible but irrelevant\ncode. Besides static feedback mechanisms above, we introduce a\nnovel runtime feedback mechanism that evaluates the likelihood of\nour generated code successfully reproducing a reported bug (Step\n3e, Fig. 3). We capture such feedback through a systematic, multi-\nstep process as follows. First, we approximate the final output and\nstate of the generated code by leveraging the code understanding\nand reasoning capability of LLMs [21]. Second, we leverage the\nLLM to map the program state (e.g., high loss values) to known\ncategories of DL bugs from an established taxonomy (e.g., incorrect\n"}, {"page": 6, "text": "ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh\nloss function), which we injected into the context [34]. If a direct\nmatch is not found in the existing taxonomy, we approximate the\ncategory of the bug based on its pre-trained knowledge. Third, the\nLLM leverages the taxonomy of symptoms [19, 67] provided in the\nprompt to derive the symptoms of the potential bugs in the code. If\nthe symptoms are not explicitly covered in the existing taxonomy,\nLLM approximates the symptoms based on its pre-trained knowl-\nedge. Finally, the symptoms of the generated code are compared to\nthose in the bug report using an LLM, which provides a similarity\nscore. A high similarity score indicates a strong likelihood of suc-\ncessful bug reproduction, whereas a low score triggers an iterative\nrefinement process. In the case of a low score, the LLM regener-\nates the code by incorporating the feedback on the symptoms of\nthe code and the bug report. This dynamic feedback loop ensures\ncontinuous improvement of the generated code and its chance of\nreproducing the bug. Furthermore, if the agent fails to generate\nbug-reproducing code within five attempts, the system switches\nto the next available code context. Our choice of five attempts is\ninspired by relevant literature [74]. In short, our approach system-\natizes DL bug reproduction by delivering a comprehensive code\nsnippet that quickly and reliably reproduces the bug.\n3.5\nImplementation Details\nWe implement RepGen using carefully selected open-source LLMs\nthat meet three important requirements: demonstrated excellence\nin code-related tasks, support for efficient quantization, and robust\ncommunity maintenance. We leverage Qwen2.5-7B for text process-\ning tasks such as bug report restructuring, and its specialized variant\nQwen2.5-Coder-7B for code generation and analysis. They meet the\nabove three criteria. We also adopt the recommended generation\nparameters [3] for the Qwen2.5-7B and Qwen2.5-Coder-7B models\nin our experiment. Our hybrid retrieval framework combines both\nsparse and dense retrieval methods to maximize relevance. The\nsparse retrieval component uses BM25 (k1 = 1.2, b = 0.75) for lexical\nmatching, while the dense retrieval component employs an efficient\nnearest-neighbour search (i.e., 50 trees). All experiments were con-\nducted on machines equipped with Nvidia RTX 3050 GPUs (8GB\nvRAM) in a local environment. On average, each bug report was\nprocessed in ‚âà4-5 minutes, with a range between 2 and 7 minutes,\ndepending on the complexity of the report and size of the codebase.\n4\nExperiment\nWe curate a benchmark dataset of 106 bugs and evaluate our tech-\nnique ‚Äì RepGen ‚Äì for automated bug reproduction using standard\nmetrics [24]. To contextualize our work, we compare our technique\nagainst eight state-of-the-art LLMs of varying sizes. We also ana-\nlyze the impact of various components of our approach through an\nablation study and investigate the cases for which our approach\nfails. Finally, we validate the effectiveness of our generated code by\ninvolving 27 real developers and AI engineers. In our experiments,\nwe thus answer four research questions as follows:\nRQ1 - Effectiveness: How effectively does RepGen reproduce\ndeep learning bugs compared to state-of-the-art LLMs?\nRQ2 - Ablation Study: How different components of RepGen\ncontribute to its bug reproduction capability?\nRQ3 - Failure Analysis: What are the current limitations of\nRepGen, and does increasing model size improve RepGen‚Äôs ability\nto reproduce deep learning bugs?\nRQ4 - Usefulness: Do developers find our generated code prac-\ntically useful for bug reproduction?\n4.1\nDataset Construction\nTo construct the dataset for our experiment, we followed a system-\natic approach inspired by prior works [50]. We select GitHub, the\nlargest resource for open-source repositories, and use its GitHub\nSearch API [2] for repository selection. We first limit our search to\nPython repositories due to its frequent use in DL systems [5]. Next,\nwe identify the repositories built on three frameworks: TensorFlow,\nKeras, and PyTorch, the most widely used DL frameworks [4]. We\ncollected 223,485 repositories from GitHub after this step. Then, we\nmake sure that each repository has at least one push to the main\nbranch after January 2023, indicating their recent maintenance [50].\nAfter this filtering, we retained 83,649 repositories for the sub-\nsequent analysis. Finally, we select such repositories that have a\nminimum of 10 watchers and 500 stars, a criterion used in existing\nliterature [56] to exclude obsolete or low-quality repositories. After\napplying these criteria, we collected 653 repositories from GitHub.\nAfter the preliminary filtering above, we removed repositories with\nfewer than 10 closed bugs, ensuring that the selected repositories\nhave a sufficient history of reported and resolved bugs. After this fil-\ntering step, we retained 392 repositories for manual analysis. Since\nour primary focus was on bugs in DL systems, we manually ana-\nlyzed the 392 repositories and excluded non-DL, toy or educational\nrepositories, leading to a total of 54 repositories for subsequent\nanalysis. We then collect all the bugs that were resolved from each\nof the 54 repositories in the last two years (April 1 2023 - April 1\n2025). We also carefully examined each bug report, fix commits, and\nnecessary instructions and attempted to reproduce the bug in an\nisolated environment. If we could reproduce the bug within 1 hour,\nwe would include this bug in the dataset. Bugs which could not be\nreproduced within an hour were marked as non-reproducible and\nexcluded from further analysis. Through this process, we identified\n106 reproducible bugs from 16 distinct repositories. We spent ‚âà190\nperson-hours on dataset construction.\n4.2\nVerification of Bug Reproduction\nTo verify whether our generated code accurately reproduces re-\nported deep learning bugs, we employ a systematic approach in-\nspired by prior work [63]. For explicit bugs [63], we confirm the\nbug reproduction by matching: (a) the error type (e.g., ValueError,\nCUDA OOM), (b) key diagnostic information (e.g., tensor shape\nmismatches), and (c) the execution context (e.g., training vs. infer-\nence phase). We execute our code five times with different random\nseeds and match their output with that of the bug report to confirm\na successful reproduction. For silent bugs [67], we also execute the\ngenerated code five times with different random seeds to account\nfor training non-determinism. The mean evaluation metrics (ac-\ncuracy, loss, memory usage) from these runs are then compared\nagainst the values reported in the bug report. We consider a bug\nsuccessfully reproduced if the mean metric falls within 5% error\nmargin of the reported value, following established practices from\n"}, {"page": 7, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent\nICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nprior work [12, 57, 63]. We also verify the behavioural equivalence\nby examining whether the output of the generated code demon-\nstrates the same failure patterns (e.g., NaN loss, excessive memory\nusage or performance degradation) as mentioned in the bug report.\n4.3\nBaseline Selection\nTo rigorously validate our approach, RepGen, we systematically\nchose eight open-source and closed-source LLMs as our baseline\nmethods. We chose Llama-3 (8B, 70B), which are open-source,\ndecoder-only transformer models with strong instruction-following\ncapabilities and widely adopted by the research community. We also\nincluded DeepSeek-R1 (7B, 685B) and Qwen3 (8B) for their advanced\nreasoning capabilities. Furthermore, we included Qwen2.5-7B, as\nit was the base model for RepGen. Finally, we chose GPT-4.1, a\nhighly capable proprietary model with its long context window\nand advanced code generation abilities. We employed three dis-\ntinct prompting techniques for every LLM (zero-shot, few-shot, and\nchain-of-thought), and used DSPy to optimize our prompts. For\nsmaller models (up to 8B), we conducted experiments on a local\nmachine using Ollama, while for larger models (70B and above),\nwe leveraged their respective APIs [6‚Äì8]. We generated the code\nusing the recommended generation parameters (e.g., temperature,\ntop_p, top_k) for the local models. To mitigate the effects of LLM\nnon-determinism, we generated the code five times and evaluated\neach generation following our verification protocol (Section 4.2).\nDuring our baseline selection, we also identified a few existing\ntechniques. We observed that LIBRO failed to reproduce any DL\nbugs because it relies on high-quality, project-specific information.\nOther methods such as Otter(++) [11] and AEGIS [74] were also\nexcluded as they do not provide replication packages, making their\nevaluation infeasible. Similarly, as discussed earlier (Section 1), GUI-\nbased methods do not apply to our case as well. Thus, we did not\nselect existing bug reproduction methods for comparison due to\ntheir inapplicability to our problem scenario.\n4.4\nDeveloper Study\nTo assess the benefits of RepGen in a real-life setting, we conduct a\nuser study involving 27 software developers and AI engineers. We\ndiscuss our study setup, including questionnaire preparation, study\ndesign, and participant selection, as follows:\nIntroduction: In our questionnaire, we first provide necessary\nbackground context about the reproducibility of deep learning bugs\nand discuss the purpose of our study. Next, we provide a set of\ninstructions and outline the structure of the subsequent sections.\nDemographic Information: After the introduction, we gather\na participant‚Äôs background data through multiple-choice and nu-\nmeric input questions. This includes years of professional software\ndevelopment experience, DL experience, and familiarity with frame-\nworks like TensorFlow, PyTorch, Keras, and JAX. We also collect\ninformation about participants‚Äô prior experience debugging deep\nlearning systems and the types of issues they commonly encounter.\nBug Reproduction: Each participant receives two bug reproduc-\ntion tasks with varying difficulty levels. For each bug, participants\nin the experimental group received the bug report along with the\ngenerated code from RepGen, while the control group received only\nthe bug report; the order of the reproduction tasks was randomized\nfor each participant to prevent any ordering effects. Participants\ndocument their reproduction process, including root cause analysis,\ntime spent, and detailed steps taken. We measure their success in\nbug reproduction through yes/no responses and their execution logs\n(if provided). We also collect qualitative data about their followed\napproach through open-ended questions.\nWorkload Assessment: Using NASA TLX [29] dimensions, we\nevaluate participants‚Äô experience through 5-point Likert scales that\nmeasure their mental demand, temporal demand, performance, ef-\nfort, and frustration. This standardized assessment helps us quantify\ntheir cognitive load during bug reproduction.\nCode Evaluation: The final section of our study focuses on eval-\nuating the utility of our generated code. Through a combination\nof Likert scales and multiple-choice questions, participants rate the\noverall usefulness of the code and identify its valuable aspects for\nreproducibility of the bug, such as error handling, data preparation,\nand model configuration. Open-ended questions capture sugges-\ntions for further improvement or any complementary information.\nParticipant Selection: We first conducted a pilot study with\ntwo researchers and two developers to validate our questionnaire\ndesign. Based on their feedback, we refined the questionnaire by\nclarifying the ambiguous questions. We then recruited 27 profes-\nsional developers and AI engineers through direct correspondence,\norganizational mailing lists, and professional networks. Participants\nwere randomly assigned to either the experimental group (n = 14)\nor the control group (n = 13). The experimental group, which re-\nceived detailed reproduction code, had a median experience of 5\nyears in software development and 2 years in deep learning devel-\nopment. Similarly, the control group also had a median of 5 years\nin software development and 2 years in deep learning. Both groups\nreported similar debugging experiences and framework expertise,\nwith 92.57% having prior experience in debugging DL systems, and\n100% having experience with at least one of the DL frameworks.\nThus, the two groups possess equivalent relevant experience, miti-\ngating any confounding factors in our study.\n4.5\nMetrics\nTo assess the effectiveness of our technique in bug reproduction, we\nemploy the following metrics, as per the existing literature [24, 83]\nSuccess Rate (SR) measures the percentage of deep learning bugs\nreproduced by a technique as follows.\nùëÜùëÖ= Number of successfully reproduced bugs\nTotal number of bugs in the dataset\n√ó 100%\nTime to Reproduce (TTR) captures the time required to success-\nfully reproduce a bug, measured from the start of the reproduction\nattempt until the bug is confirmed.\n4.6\nResults\n4.6.1\nAnswering RQ1 ‚Äì Effectiveness. Table 1 shows the effec-\ntiveness (e.g., success rate) of RepGen and eight other baseline tech-\nniques in reproducing deep learning bugs. Our technique, RepGen,\nachieved an 80.19% success rate in bug reproduction, surpassing\nthe other baselines powered by various LLMs. While prompting\nhas the potential to enhance an LLM‚Äôs built-in reasoning abilities,\nit might fall short in reproducing DL bugs as follows.\n"}, {"page": 8, "text": "ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh\nTable 1: Effectiveness of RepGen for DL Bug Reproduction\nModel\nZero-Shot\nFew-Shot\nCoT\nRepGen\n80.19% ¬±1.51%\nLlama-3-8B\n18.87% ¬±3.29% 33.02% ¬±2.82% 28.30% ¬±2.67%\nLlama-3-70B\n44.34% ¬±2.15% 17.92% ¬±2.57% 17.92% ¬±1.41%\nDeepSeek-R1-7B\n36.79% ¬±3.66% 40.57% ¬±2.56% 22.64% ¬±3.66%\nDeepSeek-R1-685B\n49.06% ¬±1.92% 60.38% ¬±1.94% 42.45% ¬±1.64%\nQwen3-8B\n23.58% ¬±3.07% 30.19% ¬±2.56% 34.91% ¬±3.25%\nGPT-4.1\n42.45% ¬±1.51% 53.77% ¬±1.62% 40.57% ¬±0.75%\nQwen2.5-7B\n21.70% ¬±2.39% 28.30% ¬±3.61% 30.19% ¬±2.06%\nQwen2.5-Coder-7B 25.47% ¬±3.78% 33.02% ¬±2.92% 28.30% ¬±1.94%\nNote: RepGen does not use any in-context learning or CoT reasoning.\nFirst, zero-shot prompting [42] allows LLMs to respond based on\ntheir pre-trained knowledge. They cannot access relevant codebase\ninformation, which is critical for reproducing DL bugs. As shown by\nLlama-3-8B‚Äôs 18.87% and Qwen2.5-7B‚Äôs 21.64% reproduction rates,\nzero-shot prompting results in poor performance. Even larger mod-\nels, such as Llama-3-70B and GPT-4.1, struggle significantly and\ndeliver a maximum of 49% success rate. All these findings suggest\nthat zero-shot prompting might not be sufficient for reproducing\nDL bugs. It cannot integrate project-specific context, limiting the\ncapabilities of the LLMs. On the other hand, RepGen solves such a\nknowledge gap by constructing a learning-enhanced context and\nequipping the LLM agent with appropriate information (e.g., train-\ning pipelines, dependencies) for code generation.\nSecond, few-shot prompting [18] provides an LLM with a small\nset of example inputs and outputs to show desired behaviours and\nto guide the LLM‚Äôs response. However, its usefulness during DL bug\nreproduction might be limited. While some models, like Qwen2.5-\nCoder-7B, show a small improvement to 33.02% from their zero-shot\nperformance, other models, such as Llama-3-70B, surprisingly per-\nform worse, degrading to 17.92% compared to their zero-shot perfor-\nmance. DeepSeek-R1-685B, improves significantly and reaches to\n60.38% with few-shot learning, but this is still lower than RepGen‚Äôs\nperformance. Furthermore, as shown in the Venn diagram in Fig. 5,\nRepGen reproduced 35 unique bugs that DeepSeek-R1 + Few-Shot\ncould not reproduce. Thus, our findings suggest that a limited set of\nexamples cannot possibly overcome all the challenges of DL bugs,\nsuch as the variety of bug types, non-determinism, architectural se-\ntups, and environmental settings. Furthermore, few-shot prompting\nis restricted by the fixed limits of the model‚Äôs context window. On\nthe other hand, RepGen overcomes these limitations by providing a\ndynamically constructed, enriched context tailored to specific bugs,\nwhich helps our agent, powered by the LLM, to understand the\nproblem better and generate code capable of reproducing bugs.\nFinally, chain of thought (CoT) [77] prompting encourages LLMs\nto explain their intermediate reasoning steps before generating a\nfinal response. While helpful for general-purpose tasks, its use did\nnot result in a high performance when generating code capable of\nreproducing deep learning bugs. Models like Llama-3-70B (17.92%)\nand DeepSeek-R1-7B (22.64%) perform worse with CoT than with\nzero-shot prompting. Even where small gains are seen (e.g., Qwen3-\n8B), the overall success in bug reproduction is still low (34.91%).\nThus, the reasoning steps of LLMs based on incomplete information\nor only pre-trained knowledge might not lead to an appropriate\nplan for a complex problem scenario like ours. More importantly,\nFigure 5: RepGen v DeepSeek-R1\nCoT has no built-in way to check if the generated code is syntacti-\ncally correct, shares symptoms with a bug report, or demonstrates\nthe erroneous runtime behaviour. On the other hand, our plan is\ngenerated by our LLM based on its reasoning capabilities, the re-\nfined bug report and the learning-enhanced context. Furthermore,\nwe employ a multi-stage feedback mechanism, with an iterative\ncycle of generating, validating, and refining, which leads to code\ncapable of reproducing the DL bugs.\nWe also investigated whether integrating RepGen‚Äôs artifacts en-\nhances the reproduction capabilities of the best-performing LLM\n(DeepSeek-R1-685B). Using RepGen‚Äôs plan alone and context alone,\nDeepSeek-R1-685B reproduced 63.21% and 68.87% of bugs, respec-\ntively. Combining the learning-enhanced context with the plan\nincreased this to 71.70%. Similarly, when DeepSeek-R1-685B was\nprovided a restructured bug report along with the context and plan,\nit achieved 65.04%. This performance further improved to 72.64%\nwhen run with an agentic setup that enhances the context, plan,\nand restructured report with lightweight PyLint and AST feedback.\nStatistical Significance Tests: To demonstrate RepGen‚Äôs effec-\ntiveness, we conducted paired comparisons between RepGen and\neach of the 24 baseline configurations using McNemar‚Äôs test on\nthe 2 √ó 2 contingency tables for the 106 shared bugs. McNemar‚Äôs\ntest [49] is appropriate for paired binary outcomes and for assessing\nwhether the proportion of bugs reproduced by RepGen differs sig-\nnificantly from each baseline counterpart. For each comparison, we\nreport the ùëù-values and the Haldane‚ÄìAnscombe adjusted odds ratio\n(hereafter, ‚Äúodds ratio‚Äù) [28]. To account for multiple comparisons,\nwe applied the Benjamini‚ÄìHochberg correction [15] to these tests,\nyielding BH-adjusted ùëù-values (q-values). All 24 comparisons re-\nmained statistically significant after BH correction (ùëû‚â§3.80√ó10‚àí3),\nwith odds ratios ranging from 2.45 to 19.86. For example, when\ncomparing RepGen to DeepSeek-R1-685B (few-shot), there were\nùëè= 35 cases where RepGen succeeded and the baseline failed, ver-\nsus ùëê= 14 cases in the opposite direction, giving an odds ratio of\n2.45 and a BH-adjusted q-value of 3.80 √ó 10‚àí3. We also calculated\n95% confidence intervals for the difference in mean success rates for\nbug reproduction over five runs between RepGen and the baseline\nusing the Newcombe hybrid score method [55], finding that the\ndifference is 19.8% with a 95% CI of 7.0%‚Äì29.7%, indicating that\nRepGen reproduces between 7% and 30% more of the shared bugs\nthan DeepSeek-R1-685B. Note that this statistic differs from the\nodds ratio, which summarizes the relative odds of reproduction.\nOverall, these results demonstrate that RepGen outperforms all\nbaselines in bug reproduction with statistical significance.\n4.6.2\nAnswering RQ2 ‚Äì Ablation Study. Table 2 shows the results\nof our ablation study. RepGen has six major components across\ndifferent steps of its workflow. We disable each of them and assess\n"}, {"page": 9, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent\nICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nTable 2: Impact of Components on Bug Reproduction Success\nConfig.\nComponents\nSuccess Rate\nR1 R2 R3 R4 R5 R6\nNo Relevance F.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n17.92% ¬± 1.62%\nNo Planning\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n21.70% ¬± 2.18%\nNo Restructuring\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n22.64% ¬± 2.18%\nNo Compilation F.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n33.96% ¬± 2.01%\nNo Static Analysis F.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n41.51% ¬± 2.15%\nNo Runtime F.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n46.23% ¬± 1.62%\nComplete System\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n80.19% ¬± 1.33%\nR1=BR Restructuring, R2=Planning, R3=Compilation F., R4=Static Analysis F.,\nR5=Relevance F., R6=Runtime F., ‚óè=Component Present\nTable 3: Impact of Retrieval Components on Reproduction\nConfig.\nComponents\nSuccess Rate\nC1 C2 C3 C4 C5 C6 C7\nNo ANN\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n26.42% ¬± 2.83%\nNo BM25\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n24.53% ¬± 1.89%\nNo Dependency Ext.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n34.91% ¬± 0.94%\nNo Module Part.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n23.58% ¬± 2.45%\nNo Reranker\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n30.19% ¬± 1.32%\nNo Training Loop Ext.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n28.30% ¬± 2.11%\nNo Training Loop Rank.\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n20.75% ¬± 1.63%\nComplete System\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n‚óè\n80.19% ¬± 1.33%\nC1=ANN, C2=BM25, C3=Dependency Extraction, C4=Module Partitioning,\nC5=Reranker, C6=Training Loop Extraction, C7=Training Loop Ranking\nRepGen‚Äôs code generation capability towards DL bug reproduction\nas follows. First, we see that the removal of the relevance-checking\ncomponent has the most significant impact, with performance drop-\nping to 17.92%. The relevance estimation between code and the bug\nreport helps RepGen discard noisy or irrelevant code and trigger a\nre-generation of code. Second, the removal of the planning compo-\nnent reduces the success rate in bug reproduction to 21.70%. Such a\ndecrease indicates that breaking down bug reproduction tasks into\nsmaller, manageable sub-tasks (a.k.a., planning) equips the LLM\nwith a structured way to generate appropriate code and reproduce\nbugs. Third, the removal of the bug report restructuring component\nalso has a major impact on RepGen and reduces the success rates to\n22.64%. This impact indicates that the restructuring of a bug report\nenhances its clarity and precision and thereby improves the LLM‚Äôs\nability to understand and reproduce issues. Fourth, the removal\nof compilation checks in RepGen reduced the success rate in bug\nreproduction to 33.96%, underscoring the importance of syntactic\ncorrectness in generated code. Finally, the removal of static analy-\nsis feedback and runtime behaviour approximation had significant\nimpacts, reducing success rates in bug reproduction to 46.23% and\n41.51%, respectively. These components serve as important verifi-\ncation steps, ensuring that the code has relevant symptoms and is\nlikely to reproduce the reported bugs.\nWe also analyze the impact of the seven components used in our\ncontext construction step (Step 1a, Fig. 3), with results summarized\nin Table 3. Removal of the training loop ranking component caused\nthe largest performance drop, reducing the success rate to 20.75%.\nThis underscores its critical role in identifying the training loop\nmost relevant to the reported bug, ensuring that the constructed\ncontext focuses on the correct portion of the codebase. Similarly,\nomitting module partitioning reduced the success rate to 23.58%,\nhighlighting that prioritizing modules (e.g., components or classes)\nbased on the relevance of the code snippets rather than the sheer\nnumber of snippets is essential for building an informative con-\ntext. Our hybrid retrieval mechanism also proved vital for context\nconstruction. Disabling BM25 search, which captures exact lexical\nmatches, reduced RepGen‚Äôs success rate to 24.53%, while removing\nANN search, responsible for retrieving semantically related code,\nresulted in a 26.42% success rate. These drops demonstrate that\nboth lexical and semantic retrieval are complementary and nec-\nessary for constructing a comprehensive context. Removal of the\ntraining loop extraction component lowered RepGen‚Äôs performance\nto 28.30%, indicating that incomplete loops without core training\nlogic can weaken the context. Removing reranking, which refines\nthe relevance of retrieved snippets, decreased success to 30.19%,\nshowing that careful prioritization of candidate code is important\nfor context quality. Finally, omitting dependency extraction, re-\nsponsible for capturing imports, helper functions, and referenced\nvariables, reduced RepGen‚Äôs success rate to 34.91%, highlighting\nthe need for a complete and executable context. These findings\ncollectively show that each element of our context construction\npipeline is vital for the high performance of RepGen. Our findings\nabove align with earlier work [63], suggesting that successful bug\nreproduction requires a multi-faceted approach. The strong perfor-\nmance of RepGen (80.19%) compared to its ablated versions also\nsuggests that each component addresses a specific challenge and\ncontributes significantly when reproducing DL bugs.\n4.6.3\nAnswering RQ3 ‚Äì Failure Analysis. To understand difficulty\nof reproducing certain deep learning bugs, we manually analyzed\nthe 21 bugs that RepGen failed to reproduce. Two researchers, the\nlead author and an independent collaborator, each with over five\nyears of experience in deep learning and software engineering, la-\nbelled these bugs using multiple established DL bug taxonomies [34,\n35, 51] and identified their bug types, symptoms, and root causes.\nWe also examined each non-reproduced bug‚Äôs report and associated\ndeveloper discussion to extract its symptoms and root causes, and\nmapped them to the closest taxonomy-defined type. The inter-rater\nagreement, measured using Cohen‚Äôs kappa was 0.849, which in-\ndicates an almost perfect agreement [48]. All disagreements were\nsubsequently resolved through discussion between the two re-\nsearchers. Based on the labelled types, we then grouped the bugs\ninto three higher-level categories: (a) API/Dependency bugs (10/21),\n(b) environment-dependent bugs (5/21) and (c) data-dependent bugs\n(6/21), as shown in Table 4. Reproducing these categories of bugs\nis challenging primarily due to their reliance on specific external\nconditions. For example, RepGen struggles with newer APIs and\nupdates since they were released after the knowledge cutoff of the\nbase model, Qwen2.5-Coder-7B. Environment-dependent bugs are\nalso difficult to reproduce because of complex distributed systems,\nresource allocation across multiple GPUs, and precise hardware\nrequirements. Similarly, data-dependent bugs are challenging due\nto reliance on specific datasets, file system structures, and particular\ndata formats and paths. Moreover, we also found that each failure\ntype aligns with distinctive pipeline signals in RepGen. API or de-\npendency mismatches result in low BM25 or ANN relevance scores\nand unresolved dependency graphs. Environment-dependent bugs\nexhibit incomplete context coverage or unresolvable hardware-\nspecific requirements, as indicated by runtime feedback showing\nthat the bug cannot be reproduced in the environment created by\n"}, {"page": 10, "text": "ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh\nTable 4: Non-Reproducible Bug Categories\nCategory\nDefinition\nExample\nAPI/Dependency Bug\nAn error caused by an incompatibility between the\ncode and its dependencies (e.g., libraries), often due to\nversion changes.\nA model fails during inference with a TypeError because the code passes an argument\n(‚Äôend_token_id‚Äô) to a function from the KerasNLP library that no longer accepts it in its\nupdated version.\nEnvironment-\nDependent Bug\nA bug that appears in a specific hardware or software\nconfiguration, such as a multi-GPU or distributed com-\nputing setup.\nA training job using TensorFlow‚Äôs multi_worker_mirrored strategy on a two-node clus-\nter fails to show any performance increase, indicating a scaling problem specific to the\ndistributed environment.\nData-Dependent Bug\nAn error caused by input data issues, such as missing\nfiles, incorrect paths, or improper formatting.\nA model training script crashes during the first epoch with a NotFoundError because it\ncannot locate a file (17e00d.jpg) that is listed in the dataset but is not present at the path.\nthe generated code. Data-dependent bugs produce early failures in\nstatic analysis or at runtime due to missing datasets or paths. These\nconsistent signals explain why some bugs cannot be reproduced\nand provide future directions for improving RepGen‚Äôs workflow.\nExperiment with GPT-4.1: Given that our technique, empow-\nered by Qwen2.5-Coder-7B, reproduced 85 out of 106 bugs (80.19%),\nwe further investigated whether a larger model could reproduce the\n21 remaining bugs. We thus employed GPT-4.1, one of the most ca-\npable models, and conducted experiments using these bugs. RepGen,\nempowered with GPT-4.1, reproduced 8 out of the 21 bugs (38.10%),\ndemonstrating that larger models can tackle some of the challeng-\ning cases. This improvement is primarily due to GPT-4.1‚Äôs enhanced\nability to identify and resolve API or dependency mismatches, re-\nproducing 5 of 10 such bugs (50.0% ¬± 15.8%). For instance, GPT-4.1\nsuccessfully reproduced pip install failures caused by invalid\nresolver options or ImportError issues stemming from incompati-\nble Keras versions, reflecting a stronger understanding of evolving\nlibrary ecosystems. Additionally, GPT-4.1‚Äôs improved code analy-\nsis enabled the reproduction of two environment-dependent bugs\n(40.0%¬±21.9%) where resolution involved code modifications rather\nthan complex infrastructure changes, such as correcting tensor di-\nmension mismatches during training. Data-dependent bugs, such\nas missing image files, were largely unreproducible (16.7% ¬± 15.2%)\nbecause automated techniques cannot create or manage external\nfile system content. Conversely, RepGen, even when powered by\nGPT-4.1, failed to reproduce the remaining 13 bugs due to their\ndependence on highly specific external conditions [37]. Many of\nthese were environment-dependent bugs requiring control over\ndistributed systems, hardware configurations, or intricate frame-\nwork interactions that exceed the capabilities of current LLMs.\nExamples include multi-node scaling issues in TensorFlow and\nModuleNotFoundError for specialized libraries (e.g., DeepSpeed).\nGiven the evidence above, model size is a factor, but not the\nsole determinant of success in reproducing DL bugs. This finding\nreinforces that the primary challenge of the automated DL bug\nreproduction is the lack of awareness of the bug‚Äôs external context.\nWhile a larger model can resolve more issues based on its broader\nknowledge, it still fails when the reproduction requires precise\nenvironment-related context that it cannot access. Thus, future ap-\nproaches should focus on improving the quality of the context pro-\nvided to LLM agents, rather than relying on increased model sizes.\n4.6.4\nAnswering RQ4 ‚Äì Usefulness. To evaluate the usefulness of\nour generated code in a practical setting, we conducted a controlled\nstudy involving 27 professional developers and AI engineers (n=14\nexperimental, n=13 control). To ensure that our developer study\nhad sufficient statistical power to detect meaningful differences be-\ntween two experimental settings, we conducted an a priori power\nanalysis. This analysis estimates whether the planned sample size\ncan reliably detect effects of practical significance. The results indi-\ncate that our study is adequately powered (80% power, ùõº=0.05) to\ndetect large effects (Cliff‚Äôs ùõø‚â•0.47) in continuous measures (e.g.,\ntime) and substantial differences in success rates. This aligns with\ntypical effect sizes observed in software engineering studies on tool\nevaluation and is appropriate for detecting meaningful, practical\nimprovements in bug reproduction [23, 39]. To analyze the results,\nwe employed the same strategy as in prior work [63].\nFigure 6: Developer Study Results\nEach participant was assigned two bug reports discussing deep\nlearning bugs from our dataset. The control group (i.e, did not\nreceive our generated code) achieved a 73.07% success rate in re-\nproducing bugs, whereas the experimental group (i.e., received our\ngenerated code) achieved a 96.42% success rate, indicating an im-\nprovement of 23.35% ¬± 18.39%, which show that our generated\ncode substantially aids developers in reproducing deep learning\nbugs. The analysis of participants‚Äô reported reproduction process\nrevealed that the experimental group was able to understand the\nbugs better. For example, one participant explained their under-\nstanding as follows: ‚ÄúShape mismatch during commit loss calculation\ndue to incorrect tensor reshaping when mask is applied‚Äù. On the\nother hand, control group participants often expressed uncertainty:\n‚ÄúDidn‚Äôt understand how masking affects training‚Äù. Our provided code\nalso helped reduce the average time to reproduce by 56.8% ¬± 16.8%,\nfrom 25.73 minutes for the control group to 11.11 minutes for the\nexperimental group. For example, one participant from the exper-\nimental group explained their reproduction approach as follows:\n‚ÄúRan provided code, confirmed TF version, observed expected error‚Äù.\nOn the other hand, a participant from the control group faced\nchallenges: ‚ÄúTried multiple inputs; all failed due to unrelated tensor\nshape issues‚Äù. Further details about the participants‚Äô experiences\nare available in our replication package [9].\nTo determine whether the presence of our generated code (i.e.,\ncategorical variable) significantly influenced the bug reproduction\nsuccess of the two groups (i.e, categorical variable), we performed\na ùúí2 test of independence. We found that there was a statistically\nsignificant difference (ùëù= 0.0423, ùúô= 0.276) in success rates when\n"}, {"page": 11, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent\nICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nthe code was present or absent. We also performed a Mann-Whitney\nùëàtest to assess the difference in the time taken by the participants\nfrom the two groups, and found out that there is a significant\ndifference (ùëù= 0.000, Cliff‚Äôs ùõø= 0.84). Thus, the above findings\nsuggest a significant impact of RepGen in a practical setting on the\nsuccess rate and time taken to reproduce deep learning bugs.\nWe also assessed participants‚Äô cognitive load using the NASA\nTask Load Index (NASA-TLX). As shown in Fig. 6, the experimen-\ntal group reported significantly lower cognitive burden across all\ndimensions, including a 43.2% ¬± 28.2% reduction in frustration and\na 25.6% ¬± 23.1% reduction in mental demand. Furthermore, partic-\nipants in the experimental group also reported higher performance\n(4.21 vs. 3.23), corresponding to a 30.34% ¬± 24.46% improvement\nrelative to the control group. Based on our collected responses,\nthe experimental group also rated the overall usefulness of our\ngenerated code at 4.57 out of 5, with 92.30% rating it as \"Useful\"\nor \"Highly Useful\" (>4). All participants in the experimental group\nwould also prefer the inclusion of the generated code in bug reports.\nThus, all the findings above suggest that RepGen‚Äôs generated code\noffers significant benefits by improving bug-reproduction success\nrates, reducing time spent, and lowering developers‚Äô cognitive load.\n5\nThreats To Validity\nThreats to internal validity relate to experimental errors and bi-\nases. Replication of baseline techniques could pose a threat. To\nmitigate this, we used the official versions of the eight models and\nemployed three widely used prompting techniques. We further\nmitigated bias by repeating our experiments five times and com-\nparing performance across these trials [13]. Threats to construct\nvalidity concern the rigour of our evaluation methodology [65].\nDuring dataset construction, we implemented a stringent verifica-\ntion protocol involving the replication of operating environment,\nerror manifestation, and time spent for reproduction (e.g., one-\nhour) to simulate real-world constraints [63]. For silent bugs, we\naccepted the metrics that fall within 5% margin of reported val-\nues, following established practices from prior work [57, 63]. Thus,\nour systematic approach ensured that only verifiably reproducible\nbugs were included. Another threat could arise from the evaluation\nmetrics. To mitigate this threat, we used the evaluation metrics\nfrom the existing literature [24, 63]. Thus, the threats to construct\nvalidity might be mitigated. Threats to external validity relate to\ngeneralizability [25]. Our dataset contains bugs from 16 actively\nmaintained projects spanning three frameworks, with a median of\n8,282 stars and 258K LOC, indicating their strong real-world rele-\nvance. Our Python-centric implementation (e.g., PyLint integration)\nmay warrant adaptation for non-Python DL ecosystems. However,\nthis threat is mitigated by RepGen‚Äôs modular design, which permits\nframework-specific adaptations. We have also released our replica-\ntion package [9] to support the reproducibility of our findings.\n6\nRelated Work\nLocalization and repair of deep learning bugs: Over the last\ntwo decades, several techniques have been proposed to localize and\nrepair bugs in deep learning (DL) systems [19, 30, 36, 76, 80]. For ex-\nample, DeepLocalize [76] uses dynamic analysis (e.g., monitoring\nof model parameters) to detect and localize faults in neural network\ntraining, while DeepFD [19] applies runtime feature analysis (e.g.,\ngradients, weights) to classify and locate faults. AutoTrainer [80]\nmonitors DL training to detect issues like vanishing gradients and\napplies built-in fixes. More recently, Harzevili et al. [30] study input\nvalidation faults (a.k.a, checker bugs) in PyTorch and TensorFlow\nand leverage retrieval-augmented LLM prompts to generate their\nfixes. However, these methods assume the bug has already been\nreproduced and do not automate the reproduction of DL bugs. In\ncontrast, we focus on automating DL bug reproduction to support\ndownstream tasks such as fault diagnosis and repair.\nAutomated bug reproduction: Traditional approaches for bug\nreproduction employ GUI-based testing, program analysis, and\nnatural language processing. For example, ReCDroid+ [83] syn-\nthesizes replayable event sequences from Android issue reports,\nwhereas recent techniques like ReBL [24] and AdbGPT [73] use\nLLMs to extract and execute reproduction steps. These methods\ntypically assume deterministic execution and well-defined oracles\nsuch as crashes or exceptions. Unfortunately, these techniques are\nnot suitable for DL systems due to their non-determinism (e.g., ran-\ndom initialization) and complex, silent failure modes (e.g., accuracy\ndegradation) [34, 63]. A recent work, AEGIS [74], automates bug\nreproduction using LLM agents and external tools but overlooks\nDL-specific challenges such as non-determinism, hardware and\ndataset dependencies, weak oracles, and incomplete issue descrip-\ntions, limiting reliability. Similarly, Otter++ [11] retrieves generic\ncode and generates test cases. In contrast, RepGen addresses these\nlimitations via three key innovations: it constructs a DL-specific\nlearning-enhanced context; integrates runtime-feedback-driven val-\nidation to estimate reproduction success; and generates complete,\nexecutable code for DL bug reproduction.\n7\nConclusion & Future Work\nDeep learning bugs are difficult to reproduce because of the inher-\nent nondeterminism of DL models and their tight coupling with\nspecific hardware (e.g., GPU) and software environments (e.g., un-\nderlying frameworks). In this paper, we present RepGen, a novel\napproach for automatically reproducing deep learning bugs. Our\ntechnique accelerates and systematizes the reproduction of deep\nlearning bugs ‚Äì by integrating a learning-enhanced context, plan-\nning and a generate-validate-refine loop with multiple verification\nmechanisms. Evaluation on 106 real-world bugs demonstrated Rep-\nGen‚Äôs superior performance, achieving an 80.19% reproduction rate\n‚Äî 19.81% higher than the best baseline. Furthermore, a controlled\ndeveloper study confirmed the practical usefulness of RepGen by im-\nproving the reproduction success rates by 23.35%, reducing average\ntime to reproduce by 56.8%, and substantially lowering developers‚Äô\ncognitive load. Future work includes extending support for dis-\ntributed training bugs and integrating with existing tools localizing\nor repairing deep learning bugs. Thus, by enabling reliable bug\nreproduction, RepGen advances the reliability and trustworthiness\nof DL systems.\nAcknowledgments: This work was supported by the Natural Sci-\nences and Engineering Research Council of Canada (Discovery\nGrant RGPIN-03236), the Fonds de recherche du Qu√©bec (FRQ), and\nthe Canadian Institute for Advanced Research (CIFAR).\n"}, {"page": 12, "text": "ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\nMehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh\nReferences\n[1] 2017. Cosine Distance, Cosine Similarity, Angular Cosine Distance, Angular\nCosine Similarity. https://www.itl.nist.gov/div898/software/dataplot/refman2/\nauxillar/cosdist.htm Accessed: 2025-07-07.\n[2] 2022. GitHub REST API Documentation. https://docs-internal.github.com/en/\nrest?apiVersion=2022-11-28 Accessed: 2025-07-02.\n[3] 2024.\nGeneration\nConfiguration\nfor\nQwen2.5-Coder-32B-Instruct.\nhttps://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/\ngeneration_config.json Accessed: 2025-07-12.\n[4] 2024.\nML Engineer comparison of Pytorch, TensorFlow, JAX, and\nFlax. https://softwaremill.com/ml-engineer-comparison-of-pytorch-tensorflow-\njax-and-flax/ Accessed: 2025-06-28.\n[5] 2024.\nWhat‚Äôs the best programming language for machine learn-\ning?\nhttps://www.techtarget.com/searchenterpriseai/tip/Whats-the-best-\nprogramming-language-for-machine-learning Accessed: 2025-07-04.\n[6] 2025. DeepSeek API Documentation. https://api-docs.deepseek.com/ Accessed:\n2025-07-17.\n[7] 2025. GroqCloud. https://console.groq.com Accessed: 2025-07-17.\n[8] 2025. OpenAI API. https://openai.com/api/ Accessed: 2025-07-17.\n[9] 2025. Replication Package for RepGen. https://github.com/mehilshah/ICSE26-\nRepGen\n[10] Peter Martey Addo, Dominique Guegan, and Bertrand Hassani. 2018. Credit risk\nanalysis using machine and deep learning models. Risks 6, 2 (2018), 38.\n[11] Toufique Ahmed, Jatin Ganhotra, Rangeet Pan, Avraham Shinnar, Saurabh Sinha,\nand Martin Hirzel. 2025. Otter: Generating Tests from Issues to Validate SWE\nPatches. arXiv preprint arXiv:2502.05368 (2025).\n[12] Saeed S Alahmari, Dmitry B Goldgof, Peter R Mouton, and Lawrence O Hall.\n2020. Challenges for the repeatability of deep learning models. IEEE Access 8\n(2020), 211860‚Äì211868.\n[13] Andrea Arcuri and Lionel Briand. 2014. A hitchhiker‚Äôs guide to statistical tests\nfor assessing randomized algorithms in software engineering. Software Testing,\nVerification and Reliability 24, 3 (2014), 219‚Äì250.\n[14] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu,\nRangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. 2016.\nMs marco: A human generated machine reading comprehension dataset. arXiv\npreprint arXiv:1611.09268 (2016).\n[15] Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate: a\npractical and powerful approach to multiple testing. Journal of the Royal statistical\nsociety: series B (Methodological) 57, 1 (1995), 289‚Äì300.\n[16] Daniel S Berman, Anna L Buczak, Jeffrey S Chavis, and Cherita L Corbett. 2019.\nA survey of deep learning methods for cyber security. Information 10, 4 (2019),\n122.\n[17] Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. 2024. Repairagent: An\nautonomous, llm-based agent for program repair. arXiv preprint arXiv:2403.17134\n(2024).\n[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[19] Jialun Cao, Meiziniu Li, Xiao Chen, Ming Wen, Yongqiang Tian, Bo Wu, and\nShing-Chi Cheung. 2022. Deepfd: Automated fault diagnosis and localization for\ndeep learning programs. In Proceedings of the 44th international conference on\nsoftware engineering. 573‚Äì585.\n[20] Partha Chakraborty, Mahmoud Alfadel, and Meiyappan Nagappan. 2024. Rlocator:\nReinforcement learning for bug localization. IEEE Transactions on Software\nEngineering (2024).\n[21] Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. 2024.\nReasoning Runtime Behavior of a Program with LLM: How Far Are We?. In 2025\nIEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE\nComputer Society, 140‚Äì152.\n[22] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimil-\niano Di Penta, and Gabriele Bavota. 2021. An Empirical Study on the Usage of\nBERT Models for Code Completion. In 2021 IEEE/ACM 18th International Confer-\nence on Mining Software Repositories (MSR). 108‚Äì119. doi:10.1109/MSR52588.2021.\n00024\n[23] Francisco Gomes de Oliveira Neto, Richard Torkar, Robert Feldt, Lucas Gren,\nCarlo A Furia, and Ziwei Huang. 2019. Evolution of statistical analysis in empirical\nsoftware engineering research: Current state and steps forward. Journal of\nSystems and Software 156 (2019), 246‚Äì267.\n[24] Sidong Feng and Chunyang Chen. 2024. Prompting is all you need: Automated an-\ndroid bug replay with large language models. In Proceedings of the 46th IEEE/ACM\nInternational Conference on Software Engineering. 1‚Äì13.\n[25] Michael G Findley, Kyosuke Kikuta, and Michael Denly. 2021. External validity.\nAnnual review of political science 24, 1 (2021), 365‚Äì393.\n[26] Grand View Research. 2024. Deep Learning Market Size & Share Report, 2024-\n2030. Market Research Report (2024).\nhttps://www.grandviewresearch.com/\nindustry-analysis/deep-learning-market\n[27] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. 2020.\nA survey of deep learning techniques for autonomous driving. Journal of field\nrobotics 37, 3 (2020), 362‚Äì386.\n[28] JB Haldane. 1956. The estimation and significance of the logarithm of a ratio of\nfrequencies. Annals of human genetics 20, 4 (1956), 309‚Äì311.\n[29] Sandra G Hart. 2006. NASA-task load index (NASA-TLX); 20 years later. In\nProceedings of the human factors and ergonomics society annual meeting, Vol. 50.\nSage publications Sage CA: Los Angeles, CA, 904‚Äì908.\n[30] Nima Shiri Harzevili, Mohammad Mahdi Mohajer, Jiho Shin, Moshi Wei, Gias\nUddin, Jinqiu Yang, Junjie Wang, Song Wang, Zhen Ming, Jiang, and Nachiappan\nNagappan. 2024. Checker Bug Detection and Repair in Deep Learning Libraries.\narXiv:2410.06440 [cs.SE] https://arxiv.org/abs/2410.06440\n[31] Nima Honarmand and Josep Torrellas. 2014. Replay debugging: Leveraging\nrecord and replay for program debugging. ACM SIGARCH Computer Architecture\nNews 42, 3 (2014), 445‚Äì456.\n[32] Kaifeng Huang, Bihuan Chen, Susheng Wu, Junming Cao, Lei Ma, and Xin Peng.\n2023. Demystifying dependency bugs in deep learning stack. In Proceedings of\nthe 31st ACM Joint European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering. 450‚Äì462.\n[33] Yuchao Huang, Junjie Wang, Zhe Liu, Mingyang Li, Song Wang, Chunyang\nChen, Yuanzhe Hu, and Qing Wang. 2025. One Sentence Can Kill the Bug: Auto-\nReplay Mobile App Crashes From One-Sentence Overviews. IEEE Transactions\non Software Engineering (2025).\n[34] Nargiz Humbatova, Gunel Jahangirova, Gabriele Bavota, Vincenzo Riccio, An-\ndrea Stocco, and Paolo Tonella. 2020. Taxonomy of real faults in deep learning\nsystems. In Proceedings of the ACM/IEEE 42nd international conference on software\nengineering. 1110‚Äì1121.\n[35] Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A\nComprehensive Study on Deep Learning Bug Characteristics (ESEC/FSE 2019).\nAssociation for Computing Machinery, New York, NY, USA, 510‚Äì520. doi:10.\n1145/3338906.3338955\n[36] Sigma Jahan, Mehil B Shah, Parvez Mahbub, and Mohammad Masudur Rahman.\n2025. Improved Detection and Diagnosis of Faults in Deep Neural Networks\nUsing Hierarchical and Explainable Classification. arXiv preprint arXiv:2501.12560\n(2025).\n[37] Sigma Jahan, Mehil B Shah, and Mohammad Masudur Rahman. 2024. Towards\nunderstanding the challenges of bug localization in deep learning systems. arXiv\npreprint arXiv:2402.01021 (2024).\n[38] Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi\nJin, and Wenpin Jiao. 2024. Self-Planning Code Generation with Large Language\nModels. ACM Trans. Softw. Eng. Methodol. 33, 7, Article 182 (Sept. 2024), 30 pages.\ndoi:10.1145/3672456\n[39] Vigdis By Kampenes, Tore Dyb√•, Jo E Hannay, and Dag IK Sj√∏berg. 2007. A\nsystematic review of effect size in software engineering experiments. Information\nand Software Technology 49, 11-12 (2007), 1073‚Äì1086.\n[40] Sungmin Kang, Juyeon Yoon, and Shin Yoo. 2023. Large language models are few-\nshot testers: Exploring llm-based general bug reproduction. In 2023 IEEE/ACM\n45th International Conference on Software Engineering (ICSE). IEEE, 2312‚Äì2323.\n[41] Anjan Karmakar and Romain Robbes. 2021. What do pre-trained code models\nknow about code?. In 2021 36th IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE). IEEE, 1332‚Äì1336.\n[42] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. Advances in\nneural information processing systems 35 (2022), 22199‚Äì22213.\n[43] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in neural information processing systems 33 (2020), 9459‚Äì9474.\n[44] Yi Li, Shaohua Wang, and Tien N Nguyen. 2022. Dear: A novel deep learning-\nbased approach for automated program repair. In Proceedings of the 44th interna-\ntional conference on software engineering. 511‚Äì523.\n[45] Parvez Mahbub and Mohammad Masudur Rahman. 2024. Predicting line-level\ndefects by capturing code contexts with hierarchical transformers. In 2024 IEEE In-\nternational Conference on Software Analysis, Evolution and Reengineering (SANER).\nIEEE, 308‚Äì319.\n[46] Parvez Mahbub, Ohiduzzaman Shuvo, and Mohammad Masudur Rahman. 2023.\nExplaining software bugs leveraging code structures in neural machine trans-\nlation. In 2023 IEEE/ACM 45th International Conference on Software Engineering\n(ICSE). IEEE, 640‚Äì652.\n[47] Junayed Mahmud, Nadeeshan De Silva, Safwat Ali Khan, Seyed Hooman\nMostafavi, SM Hasan Mansur, Oscar Chaparro, Andrian Marcus, and Kevin\nMoran. 2024. On using gui interaction data to improve text retrieval-based bug\nlocalization. In Proceedings of the 46th IEEE/ACM International Conference on\nSoftware Engineering. 1‚Äì13.\n[48] Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia\nmedica 22, 3 (2012), 276‚Äì282.\n[49] Quinn McNemar. 1947. Note on the sampling error of the difference between\ncorrelated proportions or percentages. Psychometrika 12, 2 (1947), 153‚Äì157.\n"}, {"page": 13, "text": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent\nICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil\n[50] Mohammad Mehdi Morovati, Amin Nikanjam, Foutse Khomh, and Zhen\nMing (Jack) Jiang. 2023. Bugs in Machine Learning-Based Systems: A Faultload\nBenchmark. Empirical Softw. Engg. 28, 3 (apr 2023), 33 pages. doi:10.1007/s10664-\n023-10291-1\n[51] Mohammad Mehdi Morovati, Amin Nikanjam, Florian Tambon, Foutse Khomh,\nand Zhen Ming Jiang. 2024. Bug characterization in machine learning-based\nsystems. Empirical Software Engineering 29, 1 (2024), 14.\n[52] Prabhat Nagarajan, Garrett Warnell, and Peter Stone. 2018. The impact of nonde-\nterminism on reproducibility in deep reinforcement learning. (2018).\n[53] Mathieu Nayrolles, Abdelwahab Hamou-Lhadj, Sofi√®ne Tahar, and Alf Larsson.\n2015. JCHARMING: A bug reproduction approach using crash traces and directed\nmodel checking. In 2015 IEEE 22nd International Conference on Software Analysis,\nEvolution, and Reengineering (SANER). IEEE, 101‚Äì110.\n[54] MM Abid Naziri, Aman Kumar Singh, Benjamin Wu, Feiran Qin, Saikat Dutta,\nand Marcelo d‚ÄôAmorim. 2025. BugsInDLLs: A Database of Reproducible Bugs in\nDeep Learning Libraries to Enable Systematic Evaluation of Testing Techniques.\nIn Proceedings of the 34th ACM SIGSOFT International Symposium on Software\nTesting and Analysis. 61‚Äì65.\n[55] Robert G Newcombe. 1998. Improved confidence intervals for the difference\nbetween binomial proportions based on paired data. Statistics in medicine 17, 22\n(1998), 2635‚Äì2650.\n[56] Nafiseh Nikeghbal, Amir Hossein Kargaran, Abbas Heydarnoori, and Hinrich\nSch√ºtze. 2023.\nGirt-data: Sampling github issue report templates. In 2023\nIEEE/ACM 20th International Conference on Mining Software Repositories (MSR).\nIEEE, 104‚Äì108.\n[57] Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan\nRosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems and\nopportunities in training deep learning software systems: An analysis of vari-\nance. In Proceedings of the 35th IEEE/ACM international conference on automated\nsoftware engineering. 771‚Äì783.\n[58] PwC. 2023. AI Predictions Survey. Technical Report. PricewaterhouseCoopers.\nhttps://www.pwc.com/us/en/tech-effect/ai-analytics/ai-predictions.html\n[59] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends¬Æ in Information Retrieval\n3, 4 (2009), 333‚Äì389.\n[60] Ripon K Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E Perry. 2013.\nImproving bug localization using structured information retrieval. In 2013 28th\nIEEE/ACM International Conference on Automated Software Engineering (ASE).\nIEEE, 345‚Äì355.\n[61] Asif Mohammed Samir and Mohammad Masudur Rahman. 2025.\nIm-\nproved IR-based Bug Localization with Intelligent Relevance Feedback.\narXiv:2501.10542 [cs.SE] https://arxiv.org/abs/2501.10542\n[62] Mehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh. 2024. Towards\nUnderstanding the Impact of Data Bugs on Deep Learning Models in Software\nEngineering. arXiv preprint arXiv:2411.12137 (2024).\n[63] Mehil B Shah, Mohammad Masudur Rahman, and Foutse Khomh. 2025. Towards\nenhancing the reproducibility of deep learning bugs: an empirical study. Empirical\nSoftware Engineering 30, 1 (2025), 23.\n[64] Dinggang Shen, Guorong Wu, and Heung-Il Suk. 2017. Deep learning in medical\nimage analysis. Annual review of biomedical engineering 19 (2017), 221‚Äì248.\n[65] Gregory T Smith. 2005. On construct validity: issues of method and measurement.\nPsychological assessment 17, 4 (2005), 396.\n[66] Mozhan Soltani, Pouria Derakhshanfar, Annibale Panichella, Xavier Devroey,\nAndy Zaidman, and Arie van Deursen. 2018. Single-objective versus multi-\nobjectivized optimization for evolutionary crash reproduction. In Search-Based\nSoftware Engineering: 10th International Symposium, SSBSE 2018, Montpellier,\nFrance, September 8-9, 2018, Proceedings 10. Springer, 325‚Äì340.\n[67] Florian Tambon, Amin Nikanjam, Le An, Foutse Khomh, and Giuliano Antoniol.\n2024. Silent bugs in deep learning frameworks: an empirical study of keras and\ntensorflow. Empirical Software Engineering 29, 1 (2024), 10.\n[68] Yuriy Tymchuk, Mohammad Ghafari, and Oscar Nierstrasz. 2018. JIT feedback:\nWhat experienced developers like about static analysis. In Proceedings of the 26th\nConference on Program Comprehension. 64‚Äì73.\n[69] Daisuke Wakabayashi. 2018. Self-driving uber car kills pedestrian in Arizona,\nwhere Robots Roam. https://www.nytimes.com/2018/03/19/technology/uber-\ndriverless-fatality.html Accessed on December 17, 2023.\n[70] Chenxu Wang, Tianming Liu, Yanjie Zhao, Minghui Yang, and Haoyu Wang. 2025.\nLLMDroid: Enhancing Automated Mobile App GUI Testing Coverage with Large\nLanguage Model Guidance. Proceedings of the ACM on Software Engineering 2,\nFSE (2025), 1001‚Äì1022.\n[71] Di Wang, Matthias Galster, and Miguel Morales-Trujillo. 2024. A systematic\nmapping study of bug reproduction and localization. Information and Software\nTechnology 165 (2024), 107338.\n[72] Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and\nXiangke Liao. 2022. Bridging pre-trained models and downstream tasks for\nsource code understanding. In Proceedings of the 44th international conference on\nsoftware engineering. 287‚Äì298.\n[73] Dingbang Wang, Yu Zhao, Sidong Feng, Zhaoxu Zhang, William GJ Halfond,\nChunyang Chen, Xiaoxia Sun, Jiangfan Shi, and Tingting Yu. 2024. Feedback-\ndriven automated whole bug report reproduction for android apps. In Proceedings\nof the 33rd ACM SIGSOFT International Symposium on Software Testing and Anal-\nysis. 1048‚Äì1060.\n[74] Xinchen Wang, Pengfei Gao, Xiangxin Meng, Ruida Hu, Chao Peng, Yun Lin, and\nCuiyun Gao. 2024. AEGIS: An Agent-based Framework for Bug Reproduction\nfrom Issue Descriptions. (2024).\n[75] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao\nWu, Xin Jiang, and Qun Liu. 2022. Compilable Neural Code Generation with\nCompiler Feedback. In Findings of the Association for Computational Linguistics:\nACL 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.).\nAssociation for Computational Linguistics, Dublin, Ireland, 9‚Äì19. doi:10.18653/\nv1/2022.findings-acl.2\n[76] Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. Deeplocalize: Fault local-\nization for deep neural networks. In 2021 IEEE/ACM 43rd International Conference\non Software Engineering (ICSE). IEEE, 251‚Äì262.\n[77] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824‚Äì24837.\n[78] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated\nprogram repair in the era of large pre-trained language models. In 2023 IEEE/ACM\n45th International Conference on Software Engineering (ICSE). IEEE, 1482‚Äì1494.\n[79] Linghao Zhang, Hongyi Zhang, Chong Wang, and Peng Liang. 2024. RAG-\nEnhanced Commit Message Generation. arXiv:2406.05514 [cs.SE] https://arxiv.\norg/abs/2406.05514\n[80] Xiaoyu Zhang, Juan Zhai, Shiqing Ma, and Chao Shen. 2021. Autotrainer: An\nautomatic dnn training problem detection and repair system. In 2021 IEEE/ACM\n43rd International Conference on Software Engineering (ICSE). IEEE, 359‚Äì371.\n[81] Yuhao Zhang, Yifan Chen, Shing-Chi Cheung, Yingfei Xiong, and Lu Zhang. 2018.\nAn empirical study on tensorflow program bugs. In ISSTA. 129‚Äì140.\n[82] Yuhao Zhang, Luyao Ren, Liqian Chen, Yingfei Xiong, Shing-Chi Cheung, and\nTao Xie. 2020. Detecting numerical bugs in neural network architectures. In Pro-\nceedings of the 28th ACM joint meeting on european software engineering conference\nand symposium on the foundations of software engineering. 826‚Äì837.\n[83] Yu Zhao, Ting Su, Yang Liu, Wei Zheng, Xiaoxue Wu, Ramakanth Kavuluru,\nWilliam GJ Halfond, and Tingting Yu. 2022. Recdroid+: Automated end-to-end\ncrash reproduction from bug reports for android apps. ACM Transactions on\nSoftware Engineering and Methodology (TOSEM) 31, 3 (2022), 1‚Äì33.\n[84] Yu Zhao, Tingting Yu, Ting Su, Yang Liu, Wei Zheng, Jingzhi Zhang, and\nWilliam GJ Halfond. 2019. Recdroid: automatically reproducing android applica-\ntion crashes from bug reports. In 2019 IEEE/ACM 41st International Conference on\nSoftware Engineering (ICSE). IEEE, 128‚Äì139.\n"}]}