{"doc_id": "arxiv:2511.20956", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.20956.pdf", "meta": {"doc_id": "arxiv:2511.20956", "source": "arxiv", "arxiv_id": "2511.20956", "title": "BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model", "authors": ["Rawa Mohammed", "Mina Attin", "Bryar Shareef"], "published": "2025-11-26T01:22:29Z", "updated": "2025-11-26T01:22:29Z", "summary": "Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.20956v1", "url_pdf": "https://arxiv.org/pdf/2511.20956.pdf", "meta_path": "data/raw/arxiv/meta/2511.20956.json", "sha256": "a660ddc6ad0019045159e37193f9b1681a0dab4e2e1a80b47b0de5e6453b1f85", "status": "ok", "fetched_at": "2026-02-18T02:26:14.468521+00:00"}, "pages": [{"page": 1, "text": "BUSTR: BREAST ULTRASOUND TEXT REPORTING WITH A\nDESCRIPTOR-AWARE VISION–LANGUAGE MODEL\nRawa Mohammed, Mina Attin, Bryar Shareef\nUniversity of Nevada, Las Vegas\nABSTRACT\nAutomated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of\npaired image–report datasets and the risk of hallucinations from large language models. We propose\nBUSTR, a multitask vision–language framework that generates BUS reports without requiring paired\nimage–report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS,\npathology, histology) and radiomics features, learns descriptor-aware visual representations with a\nmulti-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns\nvisual and textual tokens via a dual-level objective that combines token-level cross-entropy with a\ncosine-similarity alignment loss between input and output representations. We evaluate BUSTR on\ntwo public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors.\nAcross both datasets, BUSTR consistently improves standard natural language generation metrics\nand clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology.\nOur results show that this descriptor-aware vision model, trained with a combined token-level and\nalignment loss, improves both automatic report metrics and clinical efficacy without requiring paired\nimage–report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR\nKeywords Breast Ultrasound · Breast Cancer · Medical Report Generation · BI-RADS · Large Language Models ·\nZero-Shot Learning · Multimodal · Domain Aware · Radiology Report Generation\n1\nIntroduction\nBreast ultrasound (BUS) is a vital complementary imaging tool for early tumor detection, particularly due to its\nincreased sensitivity in dense breast tissue, where mammography tends to be less effective [1]. However, increasing\ndemand for radiological services has created a bottleneck that results in delayed diagnoses, increased workloads, and a\nhigher risk of errors, all of which can negatively impact patient care [2].\nPrevious BUS computer-aided diagnosis (CAD) systems have mainly focused on lesion segmentation [3, 4] and\nclassification [5], demonstrating the value of curated benchmarks and tumor-aware architectures for downstream\nanalysis [6]. Beyond single-task models, multitask architectures that jointly model tumor classification with auxiliary\nobjectives have shown further gains in representation quality and decision support for BUS [7, 8, 9]. However, these\nsystems mainly produce quantitative outputs (e.g., segmentation masks, class labels or probabilities, and BI-RADS\ncategories) rather than full narrative reports. While such outputs are useful for triage and risk stratification, they\nprovide limited insight into how specific imaging findings support a given decision. In contrast, narrative BUS reports\ncan improve interpretability and clinical explainability by explicitly linking visual descriptors to the final assessment\nrather than returning only a score or category. To address the growing demand, reporting burden, and need for more\ninterpretable outputs, researchers have therefore explored automated report generation methods to assist radiologists\nand improve diagnostic efficiency.\nAutomated radiology report generation (RRG) has evolved from early template-based and retrieval-based approaches\n[10, 11] to deep learning and transformer-based models [12, 13, 14] and, more recently, large language models (LLMs)\n[15]. Although template-based and retrieval-based approaches helped standardize reporting, they lacked flexibility\nbeyond predefined patterns. With deep learning [12], early systems paired convolutional neural networks (CNNs) for\nimage feature extraction with recurrent neural networks (RNNs), including long short-term memory (LSTM) networks,\narXiv:2511.20956v1  [cs.CV]  26 Nov 2025\n"}, {"page": 2, "text": "Breast Ultrasound Text Reporting\nfor descriptive text generation. These approaches improved report generation but still struggled to capture long-range\ndependencies and maintain contextual coherence, and their outputs often remain fragmented and lack the depth needed\nfor clinical interpretation.\nRecently, large language models (LLMs) have significantly influenced radiology report generation [16, 15]. They\nimprove text generation, contextual understanding, and multimodal learning, allowing for more coherent reports.\nNevertheless, LLM-based systems still struggle with medical image interpretation, sometimes producing incorrect\ndescriptions, irrelevant details, or hallucinated findings [17, 18]. Recent work has begun to address factual consistency\nby explicitly aligning visual concepts with diagnostic findings [19] or by optimizing report quality and diversity via\nreinforcement learning and text augmentation [20], but these approaches still rely on large paired image–report corpora.\nIn breast ultrasound, multimodal vision–language approaches have also been explored, for example using text prompts\nderived from clinical descriptors to guide segmentation [21], which further motivates leveraging structured textual\ninformation for BUS report generation.\nA major limitation for BUS report generation is the lack of large, well-annotated paired image–report datasets, which\nmakes it difficult to train robust deep learning models. Additionally, manual annotation of BUS images requires\nsignificant time and resources, further limiting the availability of such paired data. Although Breast Imaging-Reporting\nand Data System (BI-RADS) provides a structured, standardized way to describe lesions and supports structured\nreporting and data mining in clinical practice [22, 23], it does not capture the full diagnostic reasoning process of\nradiologists. In addition, BUS images are affected by operator dependency, speckle noise, low contrast, and artifacts,\nwhich further complicate reliable report generation.\nSeveral studies have explored AI-driven BUS computer-aided diagnosis and report generation [24, 25, 26, 27, 28, 29].\nQin et al. [27] proposed a deep learning-based tumor classification model, but their approach focuses on lesion\ncategorization rather than full-text report generation. Lo and Chen [26] introduced a metadata-driven reporting system\nthat integrates multiple imaging features, yet the resulting outputs remain relatively shallow and lack detailed narrative\ndescriptions. Li et al. [25] developed a cross-modality feature alignment method to enhance text–image coherence,\nthough its reliance on private datasets limits broader applicability. Azhar et al. [28] presented an AI-powered pipeline\nthat synthesizes structured multimodal BUS reports by combining radiologist annotations with deep learning analysis.\nHuh et al. [29] integrated multiple BUS analysis tools within a LangChain-based LLM framework to generate clinically\nmeaningful breast ultrasound reports. Similarly, Ge et al. [24] incorporated BI-RADS descriptors into structured BUS\nreporting, but their system mainly produces template-style outputs rather than fully narrative reports.\nIn this study, we propose BUSTR, a unified multimodal framework for BUS report generation without paired im-\nage–report data. The method constructs a fact-grounded vision encoder from structured descriptors with multitask\ntraining across heterogeneous datasets, and aligns visual tokens with word embeddings as an instruction prompt into a\nfrozen language model to produce narrative reports. A two-level objective encourages accurate token prediction and\nsentence-level semantic agreement. Our contributions are as follows:\n1. We propose BUSTR, a single framework for BUS report generation without paired image–report supervision.\n2. We design a descriptor-aware vision–language alignment module, trained with configuration-specific multitask\nlearning, that supports joint training across multiple BUS datasets with partially overlapping descriptor sets.\n3. We introduce dual-level training objective functions that couple token-level cross-entropy with sentence-level\nsemantic agreement, aiming to reduce hallucinations.\n2\nProposed Method\n2.1\nOverview\nThe proposed framework, BUSTR, consists of three main components: (1) zero-shot report generation, where structured\ndescriptors and radiomics features are converted into BUS reports; (2) multitask BUS descriptor classification, where a\nvision encoder is trained to predict multiple lesion descriptors and auxiliary targets; and (3) report generation, where\nvision tokens and prompts are jointly processed to generate clinically meaningful reports using a combination of\ncross-entropy and cosine-similarity alignment loss. The same framework is applied to both datasets: any structured,\ntextual, or metadata associated with a BUS image is treated as text and used to construct supervisory reports. The\noverall architecture is shown in Fig. 1.\n2\n"}, {"page": 3, "text": "Breast Ultrasound Text Reporting\nFigure 1: Overall BUSTR architecture. (a) Zero-shot construction of reports from BUS descriptors and radiomics\nfeatures. (b) Multitask training of a descriptor-aware Swin vision encoder. (c) Report generation, where visual tokens\nand prompts are fused in a frozen LLM to produce BUS reports.\n2.2\nSupervisory Report Construction from Structured Descriptors\nIn the absence of narrative radiology reports for breast ultrasound (BUS) images, we utilize a Llama-based model [30]\nto generate reports from structured, fact-based BUS descriptors. These descriptors may include BI-RADS information\n(e.g., BI-RADS class, shape, margin, echogenicity, posterior features), histopathology annotations (e.g., histology and\npathology), and radiomics features extracted from tumor masks (e.g., size, intensity statistics). Depending on the image,\nonly a subset of these descriptors may be available. In what follows, we refer to these automatically generated texts\nsimply as reports.\nThe key design principle is that supervisory reports should not introduce any new findings beyond what is present in the\nstructured fields. Unlike generic synthetic reports, our reports are constrained to restate validated descriptor values and\nradiomics-derived attributes, avoiding unsupported or hallucinated content. Later, we evaluate how well the generated\nreports preserve these facts using clinical efficacy metrics such as F1-score (F1), precision (P), and sensitivity (S).\nFor a given BUS image, we collect all available descriptor–value pairs into\nˆydescriptors = {(d, ˆyd)},\nwhere each d denotes a descriptor type (e.g., BI-RADS class, shape, margin, pathology) and ˆyd is its corresponding\nvalue. We denote the associated radiomics features by\nR = frad(M),\nwhere M is the tumor mask and frad(·) is a fixed radiomics feature extractor.\n3\n"}, {"page": 4, "text": "Breast Ultrasound Text Reporting\nWe define a formatting function Fformat that converts the available descriptors and radiomics features into an instruction-\nstyle prompt:\nP = Fformat(ˆydescriptors, R),\n(1)\nwhere P is a natural language description of the lesion constructed solely from the structured information for that image.\nThe formatted prompt P is then provided to the frozen LLM to generate the report:\nRgen = FLLM(P).\n(2)\nThis construction process is applied identically to all images; the only variation is which descriptor–value pairs (d, ˆyd)\nappear in ˆydescriptors, depending on the dataset and available annotations.\n2.3\nMulti-head Vision Encoder via Multitask Descriptor Classification\nWe design a multitask model that uses a pre-trained Swin Transformer [31] as a vision encoder with multiple classifica-\ntion heads and a single regression head. Let I ∈RH×W be an input ultrasound image, and let\nE = fenc(I) ∈RP ×E\ndenote the encoder output, where P is the number of image patches (vision tokens) and E is the embedding dimension.\nThe Swin encoder is followed by four descriptor-specific branches for shape, margin, posterior features, and echogenicity,\nas well as a separate branch for tumor size. For each branch, we apply adaptive average pooling over the patch dimension\nto obtain a fixed-length hidden state (e.g., hshape, hmargin, hposterior, hecho), which is shared across the corresponding\nprediction heads.\nBI-RADS prediction is implemented as a descriptor-aware classifier that takes the concatenation of all four descriptor\nhidden states:\nˆyBI-RADS = hBI-RADS\n\u0000concat(hshape, hmargin, hposterior, hecho)\n\u0001\n,\nmirroring the ACR guidelines in which BI-RADS categories are derived from combinations of lesion descriptors.\nIn addition, we define individual classification heads for shape, margin, posterior features, and echogenicity, and a\nregression head for tumor size:\nˆyshape = hshape(hshape),\nˆymargin = hmargin(hmargin),\nˆyposterior = hposterior(hposterior),\nˆyecho = hecho(hecho),\nˆysize = hsize(hsize),\n(3)\nwhere ˆysize is a normalized scalar (we scale tumor size by the maximum value during training, consistent with the\nimplementation).\nDuring inference, the tumor size head provides a quantitative size estimate that is inserted into the generated report\nas a scalar value. Empirically, adding tumor size as an extra vision token for the language model did not improve\nperformance, so it is not included in the visual token sequence.\nMultitask Loss\nFor each supervised target (e.g., BI-RADS category, tumor size, shape, margin, posterior features, echogenicity),\nwe define a separate loss term. Categorical descriptors are optimized with cross-entropy losses, while tumor size is\noptimized with an L1 loss (mean absolute error) on the normalized size:\nLBI-RADS = CE(ˆyBI-RADS, yBI-RADS),\nLshape = CE(ˆyshape, yshape),\nLmargin = CE(ˆymargin, ymargin),\nLposterior = CE(ˆyposterior, yposterior),\nLecho = CE(ˆyecho, yecho),\nLsize = L1(ˆysize, ysize).\n(4)\nSome descriptors are composite. In particular, margin includes a main class (circumscribed vs. non-circumscribed)\nand four non-circumscribed sub-types (indistinct, angular, spiculated, microlobulated). Let LMargin denote the loss\n4\n"}, {"page": 5, "text": "Breast Ultrasound Text Reporting\nfor the main margin class, and let Lmargin,j denote the loss for subtype j, where j ∈{1, . . . , 4} indexes the four\nnon-circumscribed sub-types. The combined margin loss is then defined as\nLCombined Margin = 0.5 LMargin + 0.5\n4\n4\nX\nj=1\nLmargin,j,\n(5)\nwhich matches the implementation where the main margin loss and the mean of the four subtype losses are averaged\nwith equal weights.\nLet T (cfg) denote the set of supervised tasks used in a given training configuration (e.g., for one dataset, T (cfg) may\ninclude BI-RADS, tumor size, shape, margin, posterior features, and echogenicity; for another dataset, only a subset of\nthese descriptors is available). Let Lt be the per-task loss, with LCombined Margin used for margin. The overall multitask\nvision loss is defined as the average over the active tasks:\nL(cfg)\nVision =\n1\n|T (cfg)|\nX\nt∈T (cfg)\nLt.\n(6)\nIn the fully annotated configuration, this reduces to the simple average of the six main task losses (tumor size, BI-RADS,\nshape, combined margin, posterior features, and echogenicity), consistent with the implemented loss.\n2.4\nReport Generation\nThe model generates radiology reports, Rfinal, by combining vision embeddings from the pre-trained multi-head vision\nencoder with word embeddings of the report and an instructional prompt. The word embedding sequence is produced\nby the LLM’s embedding model ELLM and defined as\nEword = ELLM(T, P),\nwhere T is the (partially) generated or ground-truth report text used during training, and P is the instruction-style\nprompt.\nMeanwhile, the vision encoder produces visual tokens Ev from the ultrasound image, capturing features relevant to the\ndescriptor classification tasks. These visual tokens are projected into LLaMA’s [30] embedding space and concatenated\nwith the word embeddings:\nRfinal = FLLM\n\u0000concat(Ev, Eword)\n\u0001\n.\n(7)\nIn BUS reports, clinical terms such as BI-RADS descriptors, tumor characteristics (e.g., size and location), and\nhistopathology terms are more important than general words. Many of these terms are split into multiple sub-tokens by\nstandard tokenizers, which increases the chance of hallucination or partial word generation. To address this, relevant\nclinical terms are added to the tokenizer as independent tokens before training, so the LLM embedding matrix is resized\naccordingly.\nDuring the report generation stage, the vision encoder is fine-tuned starting from the multitask weights, while the\nlanguage model remains frozen due to the limited dataset size. Training is optimized by combining two loss functions\ncomputed from the LLM output and the report: a cross-entropy loss over tokens and a cosine-similarity alignment loss\nbetween input and output representations.\nThe token-level cross-entropy loss is\nLCE = −\nL\nX\ni=1\nlog p(xi),\n(8)\nwhere p(xi) represents the predicted probability of token xi at position i in a sequence of length L.\nLet H ∈RL×D denote the last hidden state of the LLM and Z ∈RL×D the corresponding input embeddings (including\nvision tokens, prompt tokens, and report tokens). We define a cosine-similarity alignment loss as\nLAlign = 1 −1\nL\nL\nX\ni=1\nHi · Zi\n∥Hi∥∥Zi∥,\n(9)\nwhich encourages the final LLM representations to stay aligned with the image-conditioned input embeddings.\nThe final training objective for the report generation stage is a weighted sum of the two losses:\nL = λCELCE + λAlignLAlign,\n(10)\nwith λCE = λAlign = 0.5 in our experiments. This joint objective helps the model match the token-level content while\nalso maintaining a consistent alignment between visual–textual inputs and the generated report representations.\n5\n"}, {"page": 6, "text": "Breast Ultrasound Text Reporting\nTable 1: BrEaST case distribution per descriptor category.\nDescriptor\nCategories\ncases\nDescriptor\nCategories\ncases\nBI-RADS\n2\n30\nMargin\nCircumscribed\n115\n3\n37\nNon-Circumscribed:\n137\n4A\n44\nAngular\n42\n4B\n46\nIndistinct\n115\n4C\n49\nMicrolobulated\n36\n5\n46\nSpiculated\n33\nShape\nOval\n97\nEchogenicity\nAnechoic\n15\nRound\n15\nHypoechoic\n148\nIrregular\n140\nHyperechoic\n9\nPosterior Features\nNone\n159\nIsoechoic\n12\nEnhancement\n36\nHeterogeneous\n57\nShadowing\n50\nComplex Cystic/Solid\n11\nCombined Features\n7\nTable 2: BUS-BRA descriptor distribution\nDescriptor\nCategories\n# of cases\nBI-RADS\n2\n562\n3\n463\n4\n693\n5\n157\nPathology\nBenign\n1268\nMalignant\n607\nHistology\nTop 10 of 28\nFibroadenoma\n835\nInvasive Ductal Carcinoma\n520\nCyst\n142\nFibrocystic Changes\n106\nInvasive Lobular Carcinoma\n42\nIntraductal Papilloma\n41\nSclerosing Adenosis\n37\nHyperplasia\n31\nLipoma\n17\nPhyllodes Tumor\n13\n3\nExperiments\n3.1\nDatasets and preprocessing\nWe evaluate BUSTR on two publicly available breast ultrasound datasets: BrEaST [32] and BUS-BRA [33]. Both\ndatasets provide BUS images and structured descriptors that can be used as input to the zero-shot supervisory report\nconstructed from descriptors and radiomics.\nBrEaST contains 256 BUS images with detailed BI-RADS descriptors, including BI-RADS category, lesion shape,\nmargin (with sub-types), echogenicity, and posterior features. The dataset also includes curated lesion masks that allow\nradiomics feature extraction. However, the number of cases is relatively small, and some descriptor categories are\nhighly imbalanced. For report generation, we focus on five descriptors: BI-RADS class, shape, margin (including non-\ncircumscribed sub-types), echogenicity, and posterior features. Cases with BI-RADS category 1 (four images) are\nexcluded from the analysis. The full descriptor distribution is summarized in Table 1.\nBUS-BRA contains 1,875 BUS images with fewer descriptors but a larger number of cases. For each image, BI-RADS\ncategory, pathology (benign vs. malignant), and histology are available. All three are used in the zero-shot report\ngeneration and in the multitask training. The descriptor distribution for BUS-BRA is given in Table 2.\n6\n"}, {"page": 7, "text": "Breast Ultrasound Text Reporting\nTable 3: Overall performance using NLG metrics. Bold indicates the best, and underlined indicates the second best.\nApproaches\nBLEU-1↑\nBLEU-2↑\nBLEU-3↑\nBLEU-4↑\nROUGE-L↑\nMETEOR↑\nCIDEr↑\nBrEaST\nR2GenGPT\n0.586\n0.465\n0.383\n0.324\n0.442\n0.290\n0.159\nR2GenCMN\n0.635\n0.520\n0.439\n0.379\n0.498\n0.306\n0.418\nLi\n0.628\n0.516\n0.439\n0.381\n0.497\n0.303\n0.378\nTSGET\n0.633\n0.523\n0.445\n0.386\n0.508\n0.307\n0.438\nR2Gen\n0.639\n0.528\n0.450\n0.391\n0.504\n0.310\n0.492\nOurs\n0.668\n0.554\n0.475\n0.415\n0.557\n0.336\n0.688\nBUS-BRA\nR2GenGPT\n0.705\n0.595\n0.517\n0.458\n0.593\n0.359\n1.673\nR2GenCMN\n0.722\n0.617\n0.541\n0.483\n0.611\n0.370\n1.414\nLi\n0.721\n0.619\n0.543\n0.484\n0.617\n0.369\n1.302\nTSGET\n0.718\n0.610\n0.533\n0.475\n0.602\n0.371\n1.484\nR2Gen\n0.715\n0.607\n0.531\n0.472\n0.606\n0.365\n1.298\nOurs\n0.738\n0.642\n0.572\n0.516\n0.656\n0.398\n2.110\nFor both datasets, images are padded to obtain square slices and then resized to 224 × 224 pixels. To obtain robust\nestimates and avoid imbalanced splits, we perform five-fold cross-validation. For each fold, the training set is further\nsplit into 80% training and 20% validation. No data augmentation is applied.\n3.2\nTraining, Inference, and Implementation Details\n3.2.1\nTraining stages.\nTraining consists of two stages:\n1. Multi-head vision encoder. We first train the Swin Transformer encoder with multiple descriptor heads\nas defined in Sec. II-C. For each dataset, the active descriptor set follows Sec. III-A (e.g., shape, margin,\nechogenicity, posterior features, and BI-RADS for BrEaST; BI-RADS, pathology, and histology for BUS-\nBRA). The encoder is trained for 100 epochs with a batch size of 8 and a learning rate of 1 × 10−4 using the\nmultitask loss in Eq (6).\n2. Report generation. In the second stage, we freeze the LLaMA language model while fine-tuning the vision\nencoder starting from the multitask weights obtained in Stage 1. The report generation model is trained for\n25 epochs on BrEaST and 35 epochs on BUS-BRA, with a batch size of 8 and the same learning rate of\n1 × 10−4. The training objective is the combined loss in Eq. (10), based on token-level cross-entropy and a\ncosine-similarity alignment loss.\nFor both stages, the same five-fold partitions are used to prevent data leakage between training, validation, and test\nsets. At inference time, BUSTR receives only a BUS image as input and generates a narrative report without access to\nstructured descriptors.\n3.2.2\nImplementation details.\nAll experiments are conducted on a single machine with an Intel Xeon Gold 6326 CPU (2.90 GHz) and NVIDIA A100\n80 GB GPUs. Training all five folds requires approximately 4 hours for BrEaST and 16 hours for BUS-BRA.\n3.3\nEvaluation Metrics\nWe evaluate BUSTR with two groups of metrics: Natural Language Generation (NLG) metrics and Clinical Efficacy\n(CE) metrics.\n3.3.1\nNLG metrics.\nTo measure the similarity between generated reports and ground truth reports, we use BLEU-1 to BLEU-4 [34], ROUGE-\nL [35], METEOR [36], and CIDEr [37]. These metrics capture n-gram overlap, longest common subsequences, semantic\ncoverage, and consensus-based similarity.\n7\n"}, {"page": 8, "text": "Breast Ultrasound Text Reporting\nTable 4: Overall Performance using CE metrics for BrEaST dataset. Bold indicates the best, and underlined refers the\nsecond best performance.\nDescriptor\nR2GenGPT\nR2GenCMN\nLi\nR2Gen\nTSGET\nBUSTR\nBI-RADS\n↑P\n0.166\n0.178\n0.105\n0.329\n0.218\n0.356\n↑S\n0.115\n0.235\n0.171\n0.250\n0.206\n0.258\n↑F1\n0.123\n0.182\n0.114\n0.244\n0.165\n0.250\nShape\n↑P\n0.490\n0.559\n0.602\n0.689\n0.655\n0.685\n↑S\n0.457\n0.592\n0.595\n0.654\n0.638\n0.706\n↑F1\n0.447\n0.529\n0.577\n0.647\n0.613\n0.690\nEchogenicity\n↑P\n0.348\n0.384\n0.344\n0.415\n0.422\n0.351\n↑S\n0.560\n0.568\n0.556\n0.528\n0.568\n0.568\n↑F1\n0.428\n0.445\n0.424\n0.453\n0.449\n0.433\nMargin\n↑P\n0.631\n0.612\n0.645\n0.757\n0.726\n0.752\n↑S\n0.523\n0.612\n0.639\n0.710\n0.678\n0.714\n↑F1\n0.496\n0.559\n0.629\n0.711\n0.666\n0.724\nPosterior Features\n↑P\n0.401\n0.451\n0.401\n0.556\n0.427\n0.402\n↑S\n0.592\n0.631\n0.631\n0.635\n0.572\n0.631\n↑F1\n0.478\n0.517\n0.490\n0.556\n0.477\n0.491\nTable 5: Overall Performance using CE metrics for BUS-BRA dataset. Bold indicates the best, and underlined refers\nthe second best performance.\nDescriptor\nR2GenGPT\nR2GenCMN\nLi\nTSGET\nR2Gen\nBUSTR\nBI-RADS\n↑P\n0.472\n0.511\n0.333\n0.562\n0.534\n0.565\n↑S\n0.468\n0.516\n0.428\n0.540\n0.473\n0.547\n↑F1\n0.462\n0.494\n0.320\n0.537\n0.454\n0.534\nPathology\n↑P\n0.715\n0.819\n0.790\n0.808\n0.797\n0.842\n↑S\n0.726\n0.797\n0.767\n0.781\n0.791\n0.842\n↑F1\n0.717\n0.798\n0.766\n0.786\n0.793\n0.838\nHistology\n↑P\n0.401\n0.472\n0.419\n0.456\n0.421\n0.508\n↑S\n0.509\n0.589\n0.570\n0.576\n0.576\n0.543\n↑F1\n0.432\n0.509\n0.477\n0.498\n0.484\n0.510\n3.3.2\nCE metrics.\nTo assess the correctness of clinically relevant content, we compute precision (P), sensitivity (S), and F1-score (F1) for\nkey lesion descriptors and BI-RADS category. For BrEaST, we report CE metrics for BI-RADS, shape, echogenicity,\nmargin, and posterior features. For BUS-BRA, we evaluate BI-RADS, pathology, and histology. These metrics quantify\nhow accurately the generated reports recover the underlying structured facts and, together with qualitative examples\n(e.g., Fig. 2), provide a simple form of post-hoc interpretability by verifying whether the generated text correctly\nrecovers key lesion descriptors and BI-RADS categories.\n3.4\nOverall Performance\n3.4.1\nNatural language generation.\nWe compare BUSTR with five state-of-the-art (SOTA) models: R2Gen [38], R2GenCMN [39], TSGET [40], and\nR2GenGPT [15], originally developed for chest X-ray report generation, and Li [25], which is designed for ultrasound\nimaging. Table 3 summarizes the NLG performance.\nBUSTR consistently outperforms all baselines on both datasets. On BrEaST, it achieves the best BLEU-4, ROUGE-L,\nMETEOR, and CIDEr scores (0.415, 0.557, 0.336, and 0.688, respectively). On BUS-BRA, BUSTR further improves\nperformance, reaching 0.516 BLEU-4, 0.656 ROUGE-L, 0.398 METEOR, and 2.110 CIDEr. These gains indicate that\nintegrating a vision encoder on structured descriptors, radiomics features, and the proposed dual-level loss significantly\nenhances the quality and consistency of the generated BUS reports compared to existing approaches.\n8\n"}, {"page": 9, "text": "Breast Ultrasound Text Reporting\nFigure 2: Qualitative comparison between top-performing models. Different colors are used to show each predicted\ndescriptor. Highlighted texts are incorrect predictions.\n3.4.2\nClinical efficacy.\nTables 4 and 5 report CE metrics for descriptor prediction. For BrEaST, BUSTR achieves the highest precision,\nsensitivity, and F1-score for the BI-RADS category, and the best F1-score for margin classification (F1 = 0.724), while\nalso providing the strongest overall balance across shape, echogenicity, and posterior features. Some baselines, such\nas R2Gen and TSGET, obtain slightly higher precision or F1-scores on individual descriptors (e.g., echogenicity or\nposterior features), but their improvements are localized and do not extend across all metrics.\nFor BUS-BRA, BUSTR consistently outperforms all baselines in precision for BI-RADS, pathology, and histology, and\nachieves the highest F1-scores for pathology and histology. For BI-RADS, BUSTR attains the best precision and a\nnear-best F1-score that is comparable to TSGET. Overall, these results indicate that BUSTR improves both textual\nquality and clinical faithfulness of the generated reports, especially for clinically critical targets such as BI-RADS and\npathology.\n3.5\nStatistical Analysis\nTo assess whether BUSTR’s performance gains are robust across folds, we perform paired two-sided t-tests on the\nfive-fold test scores for the automatic text metrics (BLEU-4, ROUGE-L, METEOR, and CIDEr). For each dataset,\nBUSTR is compared with the strongest CIDEr baseline (R2Gen for BrEaST and R2GenGPT for BUS-BRA). On\nBrEaST, BUSTR improves ROUGE-L, METEOR, and CIDEr over R2Gen with statistically significant differences\n(p < 0.05), while the gain in BLEU-4 is positive but not significant (p ≈0.14). On BUS-BRA, BUSTR achieves\nstatistically significant improvements over R2GenGPT for all four metrics (p < 0.01 in each case). These tests indicate\nthat the improvements reported in Table 3 are consistent across folds rather than due to random variation.\n9\n"}, {"page": 10, "text": "Breast Ultrasound Text Reporting\nTable 6: The performance of each contribution of the model and other different approaches using NLG metrics. Bold\nindicates the best, and underlined indicates the second best.\nApproaches\nBLEU-1↑\nBLEU-2↑\nBLEU-3↑\nBLEU-4↑\nROUGE-L↑\nMETEOR↑\nCIDEr↑\nBrEaST\nBase Model\n0.634\n0.521\n0.444\n0.387\n0.487\n0.330\n0.537\nVision Only\n0.659\n0.545\n0.465\n0.405\n0.548\n0.331\n0.566\nLoss Only\n0.658\n0.542\n0.459\n0.398\n0.541\n0.328\n0.553\nCE + Cos\n0.669\n0.556\n0.476\n0.416\n0.556\n0.337\n0.664\nCE * Cos\n0.663\n0.549\n0.467\n0.407\n0.551\n0.332\n0.552\nmean(CE, Cos)\n0.668\n0.554\n0.475\n0.415\n0.557\n0.336\n0.688\nmax(CE, Cos)\n0.651\n0.533\n0.452\n0.393\n0.541\n0.325\n0.546\nBUS-BRA\nBase Model\n0.717\n0.615\n0.540\n0.483\n0.608\n0.392\n1.924\nVision Only\n0.724\n0.623\n0.549\n0.494\n0.619\n0.398\n1.920\nLoss Only\n0.728\n0.628\n0.555\n0.498\n0.643\n0.392\n2.087\nCE + Cos\n0.736\n0.639\n0.568\n0.512\n0.655\n0.398\n2.155\nCE * Cos\n0.728\n0.631\n0.561\n0.505\n0.649\n0.392\n1.974\nmean(CE, Cos)\n0.738\n0.642\n0.572\n0.516\n0.656\n0.398\n2.110\nmax(CE, Cos)\n0.730\n0.638\n0.570\n0.517\n0.657\n0.394\n1.952\n3.6\nAblation Study\nTo quantify the contribution of each component, we conduct ablation experiments summarized in Table 6. We consider\nthe following variants:\n• Base Model: a basic vision encoder (pre-trained Swin Transformer) with a single linear projection to the word\nembedding space and standard cross-entropy loss.\n• Vision Only: the base model where the vision encoder is replaced by our descriptor-aware multi-head encoder,\nwhile the loss function remains standard cross-entropy.\n• Loss Only: the base vision encoder with the combined objective based on token-level cross-entropy and\ncosine-similarity alignment loss between the LLM’s final hidden states and the input embeddings.\n• Full BUSTR: our complete model using both the descriptor-aware multi-head vision encoder and the dual-level\nloss. We further evaluate different choices for the combination function F (sum, product, mean, max) over\nLCE and LAlign.\nThe results show that both the vision encoder and the loss design contribute meaningfully to performance. On BrEaST,\nreplacing the base encoder with our multi-head encoder (Vision Only) improves BLEU-4 from 0.387 to 0.405, while\nusing only the combined loss (Loss Only) yields BLEU-4 of 0.398. When both contributions are combined, BUSTR\nachieves BLEU-4 of 0.415 and the highest CIDEr score (0.688). Similar trends are observed on BUS-BRA, where the\nfull model reaches 0.516 BLEU-4 and 2.110 CIDEr.\nAcross combination functions, taking the mean of LCE and LAlign provides a good balance between token-level accuracy\nand global consistency of the hidden representations, and is adopted as the default choice for BUSTR.\n4\nDiscussion\nOur results suggest that supervisory reports derived from structured descriptors are a viable supervision signal when\nhuman-written paired reports are not available. BUSTR outperforms all baselines in BLEU-4, ROUGE-L, METEOR,\nand CIDEr on both datasets (Table 3). Paired two-sided t-tests on the five-fold test scores indicate that, on BrEaST,\nthe gains in ROUGE-L, METEOR, and CIDEr over R2Gen are statistically significant (p < 0.05), whereas the\nimprovement in BLEU-4 is smaller and not significant (p ≈0.14). On BUS-BRA, BUSTR achieves statistically\nsignificant improvements over R2GenGPT for all four metrics (p < 0.01). A likely reason is that these supervisory\nreports provide a clean and consistent training signal: they are deterministically constructed from structured descriptors\nand radiomics and avoid some of the stylistic variability and omissions present in narrative reports. The multi-head\nvision encoder and multitask loss further contribute to these gains. By predicting multiple lesion descriptors through\nseparate heads and optimizing their joint loss, the encoder learns descriptor-aware visual features. The ablation study\n10\n"}, {"page": 11, "text": "Breast Ultrasound Text Reporting\n(Table 6) shows that replacing the base encoder with the multi-head encoder (Vision Only) improves BLEU-4 and\nCIDEr, suggesting that explicit descriptor supervision produces visual representations that are better aligned with the\nstructured information used to build the reports.\nThe dual-level objective function also improves alignment between generated reports and ground truth reports. The\nLoss Only variant, which uses the combined token-level cross-entropy and cosine-similarity alignment loss with a base\nencoder, already improves NLG metrics over the base model, and the full BUSTR (multi-head encoder plus dual loss)\nachieves the best performance on all NLG metrics across both datasets (Table 6). Taking the mean of LCE and LAlign\noffers a good balance between local token accuracy and global consistency of the hidden representations. Cross-entropy\nencourages correct next-token prediction given the image-conditioned context, while the alignment term encourages the\nfinal LLM hidden states to remain close to the input embedding trajectory formed by visual tokens, prompts, and ground\ntruth reports. Empirically, this leads to outputs that are both fluent and better grounded in the underlying structured\nfacts.\nFrom a clinical perspective, the CE results highlight both strengths and remaining challenges. On BrEaST, BUSTR\nachieves the highest precision, sensitivity, and F1-score for BI-RADS category and the best F1-score for margin, while\nmaintaining strong performance on shape (Table 4). On BUS-BRA, it attains the best precision for BI-RADS, pathology,\nand histology, and the best F1-scores for pathology and histology, with BI-RADS F1 comparable to the best baseline\n(Table 5). These patterns suggest that BUSTR is particularly effective for clinically critical targets such as BI-RADS\nand pathology, which are directly and strongly supervised both in the multi-head encoder and in the reports. Qualitative\nexamples in Fig. 2 show that BUSTR produces more coherent sentences and more structured tumor descriptions, with\nfewer misclassifications of critical information than the baselines. Tumor size estimation remains more difficult, likely\nbecause it is a continuous quantity that is sensitive to noise and scaling; we therefore use a dedicated regression head at\ninference time and insert the predicted size into the report text instead of relying on the language model to infer it.\nDespite these promising results, our study has two main limitations. First, the evaluation is based on two specific BUS\ndatasets, BrEaST and BUS-BRA, which have limited sample sizes and incomplete annotations. BrEaST is relatively\nsmall and imbalanced, and neither dataset provides real narrative radiology reports; instead, we rely on structured\ndescriptors and, when available, lesion masks to construct reports. As a result, reporting style diversity and coverage of\nrare findings are constrained by the available labels, and the performance and generalizability of BUSTR on larger,\nmore heterogeneous datasets remain to be established. Second, our assessment is entirely algorithmic: we use automatic\nNLG and CE metrics, but we do not yet include radiologists in the loop. There are no reader studies to evaluate clinical\nusefulness, trust, or impact on workflow, or to measure whether BUSTR’s narrative outputs improve interpretability\ncompared to conventional quantitative predictions (e.g., BI-RADS scores alone). Incorporating expert feedback and\nprospective radiologist-in-the-loop evaluation will be essential to understand how the proposed system behaves in real\nclinical practice, how its narrative reports are perceived by end-users, and how it might be integrated into existing\nreporting pipelines.\n5\nConclusion\nWe presented BUSTR, a framework for breast ultrasound (BUS) report generation that does not require paired\nimage–report data. BUSTR constructs reports from descriptors and radiomics features, learns descriptor-aware visual\nrepresentations with a multi-head Swin encoder trained under a multitask loss, and aligns vision and language using a\ndual-level objective that combines token-level cross-entropy with a cosine-similarity alignment term. Experiments on\nthe BrEaST and BUS-BRA datasets show that BUSTR outperforms recent baselines on standard NLG metrics and\nimproves clinical efficacy for key targets such as BI-RADS category and pathology, indicating that descriptor- and\nradiomics-based supervision can provide a viable training signal when narrative reports are unavailable.\nIn addition to improving prediction performance, BUSTR generates narrative reports that explicitly connect lesion\ndescriptors and radiomics-derived attributes to the final assessment, offering a potential path toward more interpretable\nBUS AI systems. Future work will evaluate the framework on larger, more diverse datasets and include radiologist-in-\nthe-loop studies to assess clinical usefulness, trust, and perceived interpretability in real-world workflows.\nReferences\n[1] Thomas M Kolb, Jacob Lichy, and Jeffrey H Newhouse. Comparison of the performance of screening mammog-\nraphy, physical examination, and breast us and evaluation of factors that influence them: an analysis of 27,825\npatient evaluations. Radiology, 225(1):165–175, 2002.\n[2] IHS Markit. The complexities of physician supply and demand: Projections from 2015 to 2030. Technical report,\n2017.\n11\n"}, {"page": 12, "text": "Breast Ultrasound Text Reporting\n[3] Dengdi Sun, Changxu Dong, Yuchen Yan, Bo Jiang, Yayang Duan, Zhengzheng Tu, and Chaoxue Zhang.\nChallenge-aware u-net for breast lesion segmentation in ultrasound images. Pattern Recognition, page 111851,\n2025.\n[4] Bryar Shareef, Aleksandar Vakanski, Phoebe E. Freer, and Min Xian. Estan: Enhanced small tumor-aware network\nfor breast ultrasound image segmentation. Healthcare, 10(11):2262, 2022.\n[5] TR Mahesh, Surbhi Bhatia Khan, Kritika Kumari Mishra, Saeed Alzahrani, and Mohammed Alojail. Enhancing\ndiagnostic precision in breast cancer classification through efficientnetb7 using advanced image augmentation and\ninterpretation techniques. International Journal of Imaging Systems and Technology, 35(1):e70000, 2025.\n[6] Y. Zhang, M. Xian, H.D. Cheng, B. Shareef, J. Ding, F. Xu, K. Huang, B. Zhang, C. Ning, and Y. Wang. Busis: A\nbenchmark for breast ultrasound image segmentation. Healthcare (Basel), 10(4):729, 2022.\n[7] Ruili Li, Ruiyu Li, Eichi Takaya, Zizhen Lin, Tomoya Kobayashi, Nanako Mtsuda, and Takuya Ueda. Bcs-net:\nMulti-task breast cancer screening network enhanced by multi-modality attention. In ICASSP 2025 - 2025 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5, 2025.\n[8] Haoyuan Chen, Yonghao Li, Jiadong Zhang, Long Yang, Yiqun Sun, Yaling Chen, Shichong Zhou, Zhenhui\nLi, Xuejun Qian, Qi Xu, and Dinggang Shen. An alignment and imputation network (ainet) for breast cancer\ndiagnosis with multimodal multi-view ultrasound images. IEEE Transactions on Medical Imaging, pages 1–1,\n2025.\n[9] Bryar Shareef, Min Xian, Aleksandar Vakanski, and Haotian Wang. Breast ultrasound tumor classification using a\nhybrid multitask cnn–transformer network. In Medical Image Computing and Computer-Assisted Intervention\n(MICCAI), 2023.\n[10] Tatiana Tommasi, Francesco Orabona, and Barbara Caputo. An svm confidence-based approach to medical image\nannotation. In Workshop of the Cross-Language Evaluation Forum for European Languages, pages 696–703.\nSpringer, 2008.\n[11] Julien Gobeill, Patrick Ruch, and Xin Zhou. Query and document expansion with medical subject headings terms\nat medical imageclef 2008. In Evaluating Systems for Multilingual and Multimodal Information Access: 9th\nWorkshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17-19, 2008,\nRevised Selected Papers 9, pages 736–743. Springer, 2009.\n[12] Mehreen Sirshar, Muhammad Faheem Khalil Paracha, Muhammad Usman Akram, Norah Saleh Alghamdi, Syeda\nZainab Yousuf Zaidi, and Tatheer Fatima. Attention based automated radiology report generation using cnn and\nlstm. Plos one, 17(1):e0262209, 2022.\n[13] Xinyi Wang, Grazziela Figueredo, Ruizhe Li, Wei Emma Zhang, Weitong Chen, and Xin Chen. A survey of deep\nlearning-based radiology report generation using multimodal data. arXiv preprint arXiv:2405.12833, 2025.\n[14] Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou. METransformer: Radiology report generation by\ntransformer with multiple learnable expert tokens. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 16302–16312, 2023.\n[15] Wang, Z. and Liu, L. and Wang, L. and Zhou, L. R2gengpt: Radiology report generation with frozen llms.\nMeta-Radiology, 1(3):100033, 2023.\n[16] Zhaoyi Sun, Hanley Ong, Patrick Kennedy, Liyan Tang, Shirley Chen, Jonathan Elias, Eugene Lucas, George Shih,\nand Yifan Peng. Evaluating gpt-4 on impressions generation in radiology reports. Radiology, 307(5):e231259,\n2023.\n[17] Ankit Pal and Malaikannan Sankarasubbu. Gemini goes to med school: exploring the capabilities of multimodal\nlarge language models on medical challenge problems & hallucinations. In Proceedings of the 6th Clinical Natural\nLanguage Processing Workshop, pages 21–46, 2024.\n[18] Vignav Ramesh, Nathan A Chi, and Pranav Rajpurkar. Improving radiology report generation systems by removing\nhallucinated references to non-existent priors. In Machine Learning for Health, pages 456–473. PMLR, 2022.\n[19] Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, and Dimitris Metaxas. RadAlign: Advancing radiology report\ngeneration with vision-language concept alignment. arXiv preprint arXiv:2501.07525, 2025.\n[20] Daniel Parres, Alberto Albiol, and Roberto Paredes. Improving radiology report generation quality and diversity\nthrough reinforcement learning and text augmentation. Bioengineering, 11(4):351, 2024.\n[21] Raja Mallina and Bryar Shareef. Xbusnet: Text-guided breast ultrasound segmentation via multimodal vision–\nlanguage learning. Diagnostics, 15(22):2849, 2025.\n12\n"}, {"page": 13, "text": "Breast Ultrasound Text Reporting\n[22] Boyu Zhang, Aleksandar Vakanski, and Min Xian. Bi-rads-net-v2: a composite multi-task neural network for\ncomputer-aided diagnosis of breast cancer in ultrasound images with semantic and quantitative explanations. IEEE\nAccess, 11:79480–79494, 2023.\n[23] Laurie R. Margolies, Gaurav Pandey, Eliot R. Horowitz, and David S. Mendelson. Breast imaging in the era of\nbig data: Structured reporting and data mining. AJR American Journal of Roentgenology, 206(2):259–264, 2016.\n[24] Shuang Ge, Qiongyu Ye, Wenquan Xie, Desheng Sun, Huabin Zhang, Xiaobo Zhou, and Kehong Yuan. Ai-assisted\nmethod for efficiently generating breast ultrasound screening reports. Current Medical Imaging, 19(2):149–157,\n2023.\n[25] Jun Li, Tongkun Su, Baoliang Zhao, Faqin Lv, Qiong Wang, Nassir Navab, Ying Hu, and Zhongliang Jiang.\nUltrasound report generation with cross-modality feature alignment via unsupervised guidance. IEEE Transactions\non Medical Imaging, 2024.\n[26] Chung-Ming Lo and Hui-Ru Chen. Automated breast imaging report generation based on the integration\nof multiple image features in a metadata format for shared decision-making.\nHealth informatics journal,\n30(3):14604582241288460, 2024.\n[27] Haojun Qin, Lei Zhang, and Quan Guo. Computer-aided diagnosis system for breast ultrasound reports generation\nand classification method based on deep learning. Applied Sciences, 13(11):6577, 2023.\n[28] Khadija Azhar, Byoung-Dai Lee, Shi Sub Byon, Kyu Ran Cho, and Sung Eun Song. Ai-powered synthesis of\nstructured multimodal breast ultrasound reports integrating radiologist annotations and deep learning analysis.\nBioengineering, 11(9):890, 2024.\n[29] Jaeyoung Huh, Hyun Jeong Park, and Jong Chul Ye. Breast ultrasound report generation using LangChain. arXiv\npreprint arXiv:2312.03013, 2023.\n[30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 10012–10022, 2021.\n[32] Anna Pawłowska, Anna ´Cwierz-Pie´nkowska, Agnieszka Domalik, Dominika Jagu´s, Piotr Kasprzak, Rafał\nMatkowski, Łukasz Fura, Andrzej Nowicki, and Norbert ˙Zołek. Curated benchmark dataset for ultrasound based\nbreast lesion analysis. Scientific Data, 11(1):148, 2024.\n[33] Wilfrido Gómez-Flores, Maria Julia Gregorio-Calas, and Wagner Coelho de Albuquerque Pereira. Bus-bra: a\nbreast ultrasound dataset for assessing computer-aided diagnosis systems. Medical Physics, 51(4):3110–3123,\n2024.\n[34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,\npages 311–318, 2002.\n[35] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out,\npages 74–81, 2004.\n[36] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation\nwith human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for\nmachine translation and/or summarization, pages 65–72, 2005.\n[37] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566–4575,\n2015.\n[38] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology reports via memory-driven\ntransformer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1439–1449, 2020.\n[39] Zhihong Chen, Yaling Shen, Yan Song, and Xiang Wan. Cross-modal memory networks for radiology report\ngeneration. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5904–5914,\n2021.\n[40] Xiulong Yi, You Fu, Ruiqing Liu, Hao Zhang, and Rong Hua. Tsget: Two-stage global enhanced transformer for\nautomatic radiology report generation. IEEE Journal of Biomedical and Health Informatics, 28(4):2152–2162,\n2024.\n13\n"}]}