{"doc_id": "arxiv:2511.06738", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.06738.pdf", "meta": {"doc_id": "arxiv:2511.06738", "source": "arxiv", "arxiv_id": "2511.06738", "title": "Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights", "authors": ["Hyunjae Kim", "Jiwoong Sohn", "Aidan Gilson", "Nicholas Cochran-Caggiano", "Serina Applebaum", "Heeju Jin", "Seihee Park", "Yujin Park", "Jiyeong Park", "Seoyoung Choi", "Brittany Alexandra Herrera Contreras", "Thomas Huang", "Jaehoon Yun", "Ethan F. Wei", "Roy Jiang", "Leah Colucci", "Eric Lai", "Amisha Dave", "Tuo Guo", "Maxwell B. Singer", "Yonghoe Koo", "Ron A. Adelman", "James Zou", "Andrew Taylor", "Arman Cohan", "Hua Xu", "Qingyu Chen"], "published": "2025-11-10T06:00:12Z", "updated": "2025-11-10T06:00:12Z", "summary": "Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.06738v1", "url_pdf": "https://arxiv.org/pdf/2511.06738.pdf", "meta_path": "data/raw/arxiv/meta/2511.06738.json", "sha256": "530efd50b5929953f8567ae3289eb0c8fc45240a234a8904548c22a147f01e45", "status": "ok", "fetched_at": "2026-02-18T02:28:02.127439+00:00"}, "pages": [{"page": 1, "text": "Rethinking Retrieval-Augmented Generation for Medicine:\nA Large-Scale, Systematic Expert Evaluation and\nPractical Insights\nHyunjae Kim1, Jiwoong Sohn2, Aidan Gilson3, Nicholas Cochran-Caggiano4, Serina Applebaum1, Heeju Jin5,\nSeihee Park5, Yujin Park5, Jiyeong Park5, Seoyoung Choi5, Brittany Alexandra Herrera Contreras1,6,\nThomas Huang1, Jaehoon Yun7, Ethan F. Wei1,8, Roy Jiang1, Leah Colucci1, Eric Lai1, Amisha Dave1,\nTuo Guo1, Maxwell B. Singer1, Yonghoe Koo9, Ron A. Adelman1, James Zou10, Andrew Taylor11,\nArman Cohan12, Hua Xu1, and Qingyu Chen1,*\n1Yale School of Medicine, Yale University, New Haven, CT, USA\n2Department of Biosystems Science and Engineering, ETH Zurich, Zurich, Switzerland\n3Massachusetts Eye and Ear, Harvard Medical School, Boston, MA, USA\n4Geisel School of Medicine at Dartmouth, Hanover, NH, USA\n5Seoul National University College of Medicine, Seoul, Republic of Korea\n6San Juan Bautista School of Medicine, Caguas, PR, USA\n7Hanyang University College of Medicine, Seoul, Republic of Korea\n8PA Leadership Charter School, West Chester, PA, USA\n9Asan Medical Center, University of Ulsan College of Medicine, Seoul, Republic of Korea\n10Stanford University School of Medicine, Stanford, CA, USA\n11University of Virginia School of Medicine, Charlottesville, VA, USA\n12Yale School of Engineering & Applied Science, Yale University, New Haven, CT, USA\n*qingyu.chen@yale.edu\nABSTRACT\nLarge language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with\nrapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has\nbeen widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably\nachieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen\nmedical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across\n200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components—(i) evidence\nretrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and\ncompleteness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant,\nevidence selection remained weak (precision 41–43%, recall 27–49%), and factuality and completeness dropped by up to 6% and 5%,\nrespectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to\nthe overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation,\nsubstantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These\nfindings call for re-examining RAG’s role in medicine and highlight the importance of stage-aware evaluation and deliberate system design\nfor reliable medical LLM applications.\n1\nIntroduction\nLarge language models (LLMs) have gained significant traction in medicine1–3, with growing efforts focused on their deploy-\nment across a range of tasks, including medical question answering4,5, disease diagnosis6,7, and treatment planning8,9. Yet,\ndespite these advances, critical challenges persist in ensuring their safe and effective use in clinical settings. First, medical\nknowledge such as clinical guidelines and drug information is frequently revised as new evidence emerges10,11. To ensure\nclinical reliability, LLMs must continuously incorporate such updates to produce accurate and clinically relevant responses.\nA systematic evaluation of six leading LLMs—spanning the GPT12, Gemini13, and Llama families14—revealed marked lim-\nitations in addressing questions about newly approved drugs, with performance remaining consistently low10. Second, in\narXiv:2511.06738v1  [cs.CL]  10 Nov 2025\n"}, {"page": 2, "text": "medicine, providing a decision or recommendation alone is rarely sufficient, and what often matters more is the ability to sup-\nport that output with credible evidence15,16. Healthcare professionals routinely consult authoritative sources, particularly in\nuncertain or high-stakes scenarios where accountability and transparency are paramount17,18. Current LLMs, however, lack\nexplicit mechanisms for verification and typically generate responses without grounding in reliable references19–22. Earlier\ngenerations of LLMs frequently hallucinated in medical applications19,20, and despite some progress, even the latest models\nstill struggle to produce accurate citations or verifiable evidence21,22.\nRetrieval-augmented generation (RAG) has emerged as a promising paradigm to address these challenges23–25. RAG is\ndesigned to help models access up-to-date information and improve the factuality and trustworthiness of their responses, by\nincorporating external evidence at inference time24,26. A standard RAG pipeline comprises three main stages25. First, domain-\nspecific documents (e.g., clinical guidelines, biomedical publications, institutional protocols) are selected by practitioners, split\ninto passages, and encoded into vector representations to build a searchable database. Second, given a user query, the system\nretrieves the top-k most relevant passages (typically using cosine similarity between vector embeddings) and appends them\nto the input prompt. Third, the LLM integrates the retrieved passages with the query to generate the final response. This\narchitecture offers notable advantages: LLMs can access recent information without retraining, and users can provide domain-\nspecific and authoritative knowledge sources from which the model retrieves evidence to support its outputs. Owing to these\nstrengths, RAG has attracted growing attention27,28 and now is widely adopted in medical applications, such as clinical ques-\ntion answering, trial screening, triage and diagnosis, and disease management29–34.\nHowever, despite its growing adoption, few studies have systematically examined how RAG performs in practice and\nwhether it meaningfully addresses challenges related to factuality and verifiability. Most existing studies in medicine treat the\nRAG framework as a black box, applying it directly and evaluating only end-task performance, without analyzing intermediate\nsteps such as retrieval quality or evidence usage—a gap also highlighted as a pressing need in recent reviews35,36. Previous\nefforts have laid important groundwork but often remain constrained to pilot-scale studies (e.g., 30 queries in a specific medical\nspecialty37) or examine only specific aspects such as citation relevance38. Furthermore, recent studies have reported mixed\nresults, with some suggesting that RAG may even reduce accuracy in downstream medical tasks39,40. These observations\nmotivate a systematic investigation into how RAG interacts with LLMs in medical contexts. Several key aspects warrant in-\ndepth examination: (1) whether the retriever retrieves evidence that is relevant to the query, since irrelevant documents may\ndegrade response quality; (2) whether the LLM effectively identifies and uses the retrieved information; and (3) how retrieval-\naugmented responses compare to those generated without retrieval, particularly in medical decision-support scenarios.\nWe conducted a large-scale, expert-driven evaluation to systematically assess the effectiveness of RAG in medicine. 18 med-\nical professionals manually evaluated and contributed a total of 80,502 expert annotations across four model configurations—\nGPT-4o12 and Llama-3.1-8B14, each with and without RAG. The evaluation covered 200 medical queries, comprising 100\nreal free-text patient queries and 100 complex, context-rich scenarios modeled after the United States Medical Licensing Ex-\namination (USMLE). As shown in Fig. 1, annotators evaluated RAG-LLMs across three stages: (I) Evidence retrieval: We\nassessed the relevance and coverage of the retrieved passages by determining whether they provided sufficient information to\nlogically infer the key elements necessary for a correct answer. (II) Evidence selection: We evaluated whether LLMs effectively\nincorporated retrieved passages into their responses. Specifically, we measured the proportion of retrieved documents cited in\nthe responses and how many of these were judged relevant in the evidence retrieval stage. (III) Response generation: We com-\npared the final outputs of LLMs with and without RAG in head-to-head evaluations, focusing on factuality and completeness.\nEvaluations were performed at both the response level (i.e., the overall factuality and completeness of the full answer) and the\nstatement level (i.e., the factuality and completeness of each individual statement).\nOur findings reveal substantial limitations in the use of RAG for medical applications. Although RAG is widely assumed\nto improve response quality, it does not consistently enhance factuality or evidence usage, and in some cases, it may even\ndiminish the overall quality of the generated responses. (I) Evidence retrieval: The top-retrieved passages were often irrelevant\nand provided limited coverage of medical queries. On average, only 22% of the top-16 passages were judged relevant, and\nthe proportion was even lower for USMLE queries (15%). In terms of coverage, the top-16 passages together contained\ninformation for only 33% of must-have statements, dropping to 26% for USMLE queries. (II) Evidence selection: Even\nwhen relevant passages were retrieved, LLMs often failed to incorporate them effectively. Both precision and recall for selecting\nrelevant evidence remained low. For GPT-4o, precision was 41% and recall 49%; for Llama-3.1, precision was 43% and recall\nonly 28%. As a result, the already limited coverage of must-have statements from retrieval (33%) dropped by nearly 10%\nafter evidence selection. At the same time, irrelevant passages were frequently included, with their frequency nearly twice that\nof relevant passages. (III) Response generation: RAG may not improve LLM outputs. At both the response and statement\nlevels, factuality and completeness were consistently lower for GPT-4o and Llama-3.1 with RAG than without. Response-\nlevel factuality dropped by up to 6% for GPT-4o, while statement-level completeness declined by more than 5% for Llama 3.1.\n2/34\n"}, {"page": 3, "text": "Database\nPassage 1\nQuery (n=200): “Benadryl makes me super sleepy. Can I take Claritin during the day instead?”\nLLM\n(GPT-4o and \nLlama-3.1-8B)\nModel Statements (n = 15,970)\nGold\nResponse\nModel\nResponse\nEvidence Selection\nWhether each cited \nreference is:\n✅ Relevant\n❌ Irrelevant\nI. Evidence \nRetrieval\n(n = 30,800)\nII. Evidence \nSelection\n(n = 26,032)\nRetrieval\nModel Response and Supporting References (n = 1,627)\nDiphenhydramine, the active ingredient in Benadryl, is a\nfirst-generation H1 histamine receptor antagonist that is\ncommonly associated with drowsiness or sleepiness as a\nside effect [1][2].\nDue to these sedative properties,\nBenadryl is often used as a sleep aid, but it can cause\ndaytime sleepiness, grogginess, and an increased risk of\nfalls [2]. Claritin (loratadine) is a second-generation …\n### References\n1. Mohan S, et al. A Case of Massive Diphenhydramine and \nNaproxen Overdose .…\n2. Benadryl. Wikipedia. Accessed October 14, 2023 …\nIII. Response \nGeneration\n(n = 23,670)\nDiphenhydramine is the active ingredient in Benadryl.\nDiphenhydramine is a first-generation H1 histamine \nreceptor antagonist.\nBenadryl has sedative properties.\nDiphenhydramine (Benadryl) is a \nfirst-generation antihistamine.\nA first-generation antihistamine \nfrequently causes sedation.\nLoratadine (Claritin) is a second-\ngeneration antihistamine\n…\n…\nMust-have Statements (n = 1,925)\nFactuality Whether each \nmodel statement is:\n✅ Factually Accurate\n❌ Inaccurate\nCompleteness\nWhether \nessential information is:\n✅ Supported\n✔ Partially Supported\n❌ Unsupported\n…\nPassage 16\nRetrieved\nContent \n+\nQuery\nRelevance Whether each \nretrieved passage is:\n✅Relevant\n❌Irrelevant\n80,502 \nAnnotations\nMust-have \nstatements\n(n=1,925)\nTop-16 Passages\nResponses from \nfour diﬀerent \nsystems\n…\nCross-reference\nFigure 1. Study design and evaluation framework. Our fine-grained framework decomposes the RAG pipeline into three\ncomponents, evidence retrieval, evidence selection, and response generation, enabling systematic evaluation of each stage.\nFirst, retrieved passages are annotated for relevance (evidence retrieval; n =30,800). Next, model responses are evaluated to\ndetermine whether they are grounded in relevant evidence (evidence selection; n =26,032). Finally, responses are broken\ndown into individual statements and assessed for correctness (factuality; n =15,970) and coverage of required information\n(completeness; n =7,700). In total, our framework comprises 80,502 expert annotations, enabling rigorous error\nattribution across the full RAG pipeline.\nThese degradations were more pronounced when the RAG system incorporated irrelevant passages or failed to retrieve relevant\nones. For instance, Llama-3.1’s factuality dropped by over 8% when irrelevant passages were cited, compared to outputs\ngrounded in relevant content. Similarly, completeness declined by over 6% when relevant information was not retrieved at all.\nWe proposed two simple and practical strategies designed to directly mitigate the observed issues. First, evidence filtering\nremoves irrelevant passages from the retrieved set, which is a necessary step given the high proportion of irrelevant content\nand the models’ tendency to mistakenly incorporate it. Second, query reformulation rewrites the initial query to guide retrieval\ntoward more relevant evidence, thereby improving the precision and coverage of the retrieval step. We evaluated each strategy\n3/34\n"}, {"page": 4, "text": "independently and in combination across five medical question-answering benchmarks. In line with earlier findings, the RAG\nmodels did not consistently improve performance and in some cases reduced accuracy. In contrast, combining evidence filtering\nand query reformulation yielded substantial gains on more challenging datasets: Llama-3.1 improved by +12% on MedMCQA\nand +8.2% on MedXpertQA, while GPT-4o achieved gains of +3.4% and +6.6% on the same datasets.\nOur study motivates a re-examination of RAG in the medical domain. Despite its promise, our study shows that RAG\npipelines can introduce new sources of failure, including the retrieval of irrelevant information, failure to integrate relevant\nevidence, and reductions in both the factual accuracy and completeness of model outputs. By systematically evaluating each\nstage of the RAG process with a large number of expert annotations, we identify critical bottlenecks that have been largely\noverlooked in previous studies. Importantly, we demonstrate that targeted interventions, such as evidence filtering and query\nreformulation, substantially improve performance on challenging medical tasks. These findings suggest that the path forward\nlies not in applying RAG as a default solution but in rethinking both its system design and its evaluation.\n2\nResults\n2.1\nAnnotation Summary\nWe briefly summarize the annotation process (Fig. 1) and associated statistics below; full details, including stage-specific\nguidelines, annotator instructions, and interface design, are provided in the Methods section.\n2.1.1\nModel Selection\nWe conducted a comprehensive survey of current RAG practices in medicine, summarized in Supplementary Table 1, covering\nretrievers, LLMs, and knowledge sources. Our review includes 29 papers published between June 2023 and August 2025.\nAmong the retrievers, MedCPT41, an open-source, domain-specialized retriever for biomedicine, was the most commonly\nused (28%). Given its scalability and strong domain relevance, we selected MedCPT as the retriever for our pipeline. Further\ndiscussion on alternative retrievers is provided in the Discussion section. For LLMs, the majority of studies (over 65%) relied\non proprietary models from the GPT family (GPT-3.5, GPT-4, and GPT-4o), while a smaller subset employed open-source\nmodels, most notably variants of Llama. In our study, we adopted both proprietary and open-source LLMs, GPT-4o and\nLlama-3.1-8B, respectively, to ensure a balanced evaluation across model types. For the retrieval corpora, we included a broad\nrange of widely used medical sources, including PubMed, Wikipedia, clinical guidelines, StatPearls, and medical textbooks.\n2.1.2\nData Curation\nWe randomly sampled 100 patient queries from the K-QA dataset42 and 100 USMLE-style questions from the MedBullets\ndataset43. Both datasets provide gold-standard responses annotated by human experts. In K-QA, each gold response is divided\ninto “must-have” and “nice-to-have” statements, both verified by human annotators. Note that these annotations were created\nindependently of our study and originate from the original datasets. The must-have statements capture the essential factual\ninformation required for a correct answer, whereas the nice-to-have statements provide supportive but non-essential details.\nIn contrast, the MedBullets dataset does not provide annotated statements. To address this, we prompted GPT-4o to extract\nstatements from each gold response and classify them as must-have or nice-to-have, following prior work demonstrating the\nstrong performance of LLMs in this task38. We used only the must-have statements for evaluating evidence retrieval and\ncompleteness scores, yielding a total of 1,925 statements across the two datasets.\nFor a total of 200 questions, we generated 800 responses using four model configurations: GPT-4o, GPT-4o with RAG,\nLlama-3.1, and Llama-3.1 with RAG. For the RAG-based models, the top-k retrieved passages were provided as input context.\nWe set k = 16 to limit the annotation workload to a manageable level while still ensuring sufficient context for model reasoning.\nEach response was accompanied by a list of supporting references, that is, source materials or citations the model cited as\nevidence for its answer. For annotation, we exclusively used the 1,627 references produced by the RAG models. Each response\nwas then segmented into individual statements using GPT-4o to enable statement-level factuality assessment. During this\nprocess, any inline citations (e.g., [1][2]) provided by the model were also separated along with each statement to enable later\ntracing of which reference supported which statement. Non-factual claims that could not be objectively verified were excluded,\nresulting in a final total of 15,970 model-generated statements.\n4/34\n"}, {"page": 5, "text": "2.1.3\nAnnotation Procedure and Statistics\nWe conducted a multi-stage annotation process using a curated dataset comprising 200 queries, their top-16 retrieved passages,\n1,925 must-have statements, 15,970 model-generated statements from 800 responses, and 1,627 references. The paragraphs\nbelow describe how the full set of 80,502 annotations was derived by aggregating results from the three evaluation stages.\nEvidence retrieval\nFor each of the 200 queries, the retriever returned the top-16 passages. Annotators then evaluated each\npassage by comparing it to the corresponding must-have statements, labeling it as relevant or irrelevant. A passage was labeled\nas relevant if it fully (or partially) supported at least one of the must-have statements associated with the query; otherwise,\nit was labeled irrelevant. This process yielded a total of 30,800 passage–statement pairs, computed as 1,925 must-have\nstatements (aggregated across 200 queries) each evaluated against the top-16 retrieved passages for its corresponding query\n(1,925 x 16 = 30,800).\nEvidence selection\nAnnotators compared the 1,627 references generated by the two RAG models against each of the top-16\nretrieved passages per query, identifying which passages corresponded to the sources actually cited by the model. A single\nreference could be matched to multiple retrieved passages when appropriate, as multiple passages might originate from the\nsame document or from different documents containing overlapping content. This involved checking metadata (e.g., title,\nsource name, URL) and reviewing the citation context in the model’s response to confirm alignment between the cited reference\nand the retrieved passage. For example, as shown in Fig. 1, if the model cited a PubMed article titled “A Case of Massive\nDiphenhydramine and Naproxen Overdose,” annotators assessed whether any retrieved passage matched that article based on both\nmetadata and textual content. Once the alignments were established, this provided a basis for evaluating whether each reference\nwas grounded in a relevant or irrelevant source, using relevance labels from the previous stage. This process produced a total\nof 26,032 passage–reference alignment annotations, calculated as 1,627 references evaluated against 16 retrieved passages\nper query (1,627 x 16 = 26,032).\nResponse generation\nThis stage was evaluated along two axes: factuality and completeness. The factuality task assessed the\ncorrectness of individual statements generated by all four models, yielding a total of 15,970 statement-level annotations cor-\nresponding to all model-generated statements across queries. The completeness task evaluates whether each model response\nadequately captures the essential content of the gold-standard response. For each of the 1,925 must-have statements, an-\nnotators judged whether the model’s response fully supported, partially supported, or failed to support it, producing 7,700\nresponse-statement annotations (1,925 statements x 4 models = 7,700).\n2.1.4\nInter-annotator Agreement\nAnnotation reliability was assessed using Krippendorff’s α (nominal), with 10,000 bootstrap samples for confidence intervals.\nEach sample in this analysis was independently labeled by two annotators. For evidence retrieval, support and partial support la-\nbels were merged into a single category to address class imbalance during agreement estimation. This yielded 1,280 annotated\npassage-statement pairs, with a resulting Krippendorff’s α of 0.757 (95% CI: 0.638–0.852), indicating substantial agree-\nment. For evidence selection, a total of 416 reference-passage pairs were evaluated, resulting in a Krippendorff’s α of 0.926\n(95% CI: 0.906–0.944), indicating substantial inter-annotator consistency. For the factuality criterion, 972 statements gen-\nerated across the four model conditions were evaluated, yielding moderate inter-annotator agreement with a Krippendorff’s α\nof 0.512 (95% CI: 0.372–0.636). Although factuality judgments are grounded in objective evidence, they may still vary\ndepending on each clinician’s background knowledge and experience. Importantly, each annotator assessed all model out-\nputs for a given query, ensuring internal consistency within each case. Thus, while the absolute agreement level was moderate,\nrelative comparisons between models (e.g., RAG vs. non-RAG) remain reliable. For the completeness criterion, 328 response–\nstatement pairs were evaluated. As with evidence retrieval, partial support labels were merged with support labels. This yielded\na Krippendorff’s α of 0.742 (95% CI: 0.666–0.812), again indicating substantial inter-annotator agreement.\n2.2\nEvidence Retrieval\nWe employed three complementary metrics, all computed based on expert annotations of the top-16 retrieved passages per\nquery. (1) Precision@k measures the proportion of relevant passages among the top-k retrieved results. Here, a passage is con-\nsidered relevant if it partially or fully supports at least one of the must-have statements associated with the query. (2) Miss@k\n5/34\n"}, {"page": 6, "text": "0\n0.1\n0.2\n0.3\n0.4\n0.5\nk=1\n4\n8\n16\n0\n0.1\n0.2\n0.3\n0.4\nk=1\n4\n8\n16\nPrecision@k\nCoverage@k\nAverage\nUSMLE-style Queries\nPatient Queries\na\nc\n0.217\n0.281\n0.153\n0.328\n0.256\n0.400\n0\n0.2\n0.4\n0.6\n0.8\n1\nk=1\n4\n8\n16\nMiss@k\nb\n0.310\n0.250\n0.370\nFigure 2. Evidence retrieval performance across different evaluation metrics and query types. a, Precision@k: proportion\nof relevant passages among the top-k; higher is better. b, Miss@k: proportion of queries with no relevant passage in the\ntop-k; lower is better. c, Coverage@k: proportion of must-have statements supported by the top-k; higher is better.\nreflects the proportion of queries for which no relevant passage was retrieved within the top-k. (3) Coverage@k quantifies\nthe proportion of must-have statements that were supported by at least one of the top-k retrieved passages.\nRetrieval performance was markedly limited, with most retrieved passages failing to provide relevant support. (1) Preci-\nsion@k remained low across all top-k values (Fig. 2a). At k = 16, precision was 0.217, indicating that only ~22% of pas-\nsages were identified as relevant. Performance was even lower for USMLE-style queries (0.153) compared to patient queries\n(0.281). This discrepancy reflects the dense contextual details in USMLE-style queries, such as medical history and symptom\ndescriptions, which can hinder retrieval accuracy. (2) Miss@k revealed that a substantial fraction of queries failed to retrieve\nany relevant evidence, even at high top-k (Fig. 2b). Specifically, 31% of queries had zero relevant passages among the top-16\nretrieved results, highlighting the challenge of surfacing useful external information in complex clinical scenarios. The failure\nrate was even higher for USMLE-style queries (37%) than for patient queries (25%). Notably, the issue was more pronounced\nat lower top-k, for instance, at top-1, 82% of USMLE-style queries and 63% of patient queries returned no relevant evidence.\n(3) Coverage@k of essential content was also limited (Fig. 2c). Here, essential information refers to must-have statements,\nwhich are manually annotated parts of the gold-standard answer required for a correct response. The score was 0.328, indi-\ncating that only ~33% of must-have statements were supported by the top-16 retrieved passages. While coverage gradually\nimproved with larger top-k, a substantial portion of essential content remained unretrieved. Once again, performance diverged\nby query type: USMLE-style queries achieved only 0.256 coverage, compared to 0.400 for patient queries.\n2.3\nEvidence Selection\nWe evaluated whether models effectively cited relevant information from the top-16 retrieved passages, which included a mix\nof relevant and irrelevant content. On average, GPT-4o generated 4.9 references per query, while Llama-3.1 generated 4.5\n(Fig. 3a). Each reference was either retrieval-based—drawn directly from the top-16 passages—or self-generated using the\nmodel’s internal knowledge. The majority of GPT-4o’s references (89.8%, 4.4 per query) were retrieval-based, compared to\nLlama-3.1 (62.2%, 2.8 per query), indicating a higher proportion of self-generated references in the latter. We computed\nmicro-averaged precision and recall by comparing all retrieval-based references against expert relevance annotations. Self-\ngenerated references were excluded from this analysis, as annotations were available only for the retrieved passages.\nModels struggled to selectively incorporate relevant evidence from the retrieved passages, as reflected in the low precision\nand recall scores across both models (Fig. 3b). GPT-4o achieved a precision of 0.412 and a recall of 0.486, while Llama-3.1\nachieved a similar precision of 0.430 but a substantially lower recall of 0.275. The low precision indicates that models fre-\nquently incorporated irrelevant content. On average, GPT-4o cited 2.6 irrelevant passages per query out of 4.9 total references,\nwhile Llama-3.1 cited 1.6 out of 4.5 (Fig. 3a). This indicates that even high-performing models like GPT-4o often treat irrele-\nvant content as valid evidence, reflecting limitations in their ability to distinguish useful information from misleading content.\nIn contrast, the low recall highlights the models’ failure to effectively incorporate relevant information, even when it was readily\navailable. Based on the earlier precision@16 of 0.217, each query included approximately 3.5 relevant passages among the\ntop-16 retrieved. GPT-4o cited 1.8 of these on average, while Llama-3.1 cited just 1.2, indicating that both models failed to\nmake full use of available evidence, with Llama-3.1 omitting nearly two-thirds of the relevant passages. In both models, the\n6/34\n"}, {"page": 7, "text": "0\n0.1\n0.2\n0.3\n0.4\n0.5\nPrecision\nRecall\nGPT-4o\nLlama-3.1-8B\n0.412\n0.430\n0.486\n0.275\nEvidence Selection Performance\nb\nRetrieval-based\nSelf-generated\n0\n1\n2\n3\n4\n5\nGPT-4o\nLlama-3.1-8B\nAverage Number of References per Query by Type\n1.8\n4.9\n4.5\n2.6\n0.5\n(10.2%)\n1.7\n(37.8%)\n1.6\n1.2\n4.4\n(89.8%)\n2.8\n(62.2%)\n+\nRetrieval-based \n(Relevant)\nRetrieval-based \n(Irrelevant)\na\nFigure 3. Analysis of citation types and evidence selection performance. a, Average number of references per query,\ncategorized by evidence source (retrieval-based vs. self-generated); retrieval-based references are further broken down by\nrelevance. Self-generated references refer to citations produced by the RAG model itself that do not appear among the\nretrieved passages. b, Precision and recall for identifying relevant evidence among retrieved passages.\nnumber of relevant passages cited per query was lower than the number of irrelevant ones: 1.8 vs. 2.6 for GPT-4o and 1.2 vs.\n1.6 for Llama-3.1.\nVerifiability of self-generated references\nBeyond retrieved content, both models also produced references not found among\nthe top-16 retrieved passages. These self-generated references made up 10.2% of all citations in GPT-4o and 37.8% in Llama-\n3.1 (Fig. 3a). To verify their authenticity, we manually checked self-generated references from GPT-4o and Llama-3.1 to\ndetermine whether they could be located online using the generated citation details. A small fraction (13.3%) of GPT-4o’s\nself-generated references were unverifiable, whereas a much larger proportion (77.2%) of those from Llama-3.1 could not be\nconfirmed, often containing fabricated metadata such as non-existent titles, authors, or publication sources. This discrepancy\nunderscores a key limitation of small models: while they more frequently produce self-generated citations, they are also more\nprone to hallucinating plausible-sounding but non-existent references, raising concerns about the verifiability of their outputs.\n2.4\nResponse Generation\nWe evaluated model outputs using two key criteria: factuality, which measures the correctness of the presented informa-\ntion, and completeness, which assesses whether essential information is fully conveyed. All scores were manually determined\nthrough expert annotation. For the RAG models, responses were generated using the top-16 retrieved passages.\n2.4.1\nFactuality\nWe evaluated factuality at two levels: response and statement. At the response level, each answer was assigned a binary score—\n1 if the entire response was factually correct, and 0 if any part contained a factual error. At the statement level, each individual\nstatement within the response was evaluated independently as true or false, and we computed the proportion of factual state-\nments for each query. The response-level and statement-level scores were both averaged across all queries.\nFactuality performance declined under the RAG setting for both models (Fig. 4a). At the response level, GPT-4o exhibited\na 6.0% drop in factuality, while Llama-3.1 showed a smaller decrease of 1.0%. At the statement level, both models experienced\ncomparable declines: 1.6% for GPT-4o and 1.9% for Llama-3.1. These results suggest that RAG negatively affects models\nacross different scales and capabilities. Notably, GPT-4o showed a sharper decline at the response level, indicating that factual\nerrors emerged more broadly across queries. However, since GPT-4o started from a much higher baseline (68.0%) compared\nto Llama-3.1 (39.5%), the smaller drop observed in Llama-3.1 does not necessarily indicate greater robustness.\nTo better understand how evidence retrieval and selection affect model performance, we analyzed factuality at the statement\nlevel by grouping each statement based on the type of evidence cited by the model. Specifically, for every statement produced\nby a RAG model, we identified whether it was supported by (1) a retrieved relevant passage (True Positive), (2) a retrieved\nbut irrelevant passage (False Positive), (3) a self-generated source without retrieval grounding (Self-generated), or (4) no\nreference at all (No Reference). As shown in Fig. 1 (see II. Evidence Selection), references were explicitly marked within the\n7/34\n"}, {"page": 8, "text": "96.2\n90.9\n0\n20\n40\n60\n80\n100\nGPT-4o\nLlama-3.1-8B\nBase LLM\nRAG\n60.8\n52.2\n0\n20\n40\n60\n80\n100\nGPT-4o\nLlama-3.1-8B\nBase LLM\nRAG\n94.6\n(-1.6)\n89.0 (-1.9)\n60.4 (-0.4)\n46.8 (-5.4)\n0\n20\n40\n60\n80\n100\nGPT-4o\nLlama-3.1-8B\nBase LLM\nRAG\n30.5\n23.0\n0\n20\n40\n60\n80\n100\nGPT-4o\nLlama-3.1-8B\nBase LLM\nRAG\n38.5 (-1.0)\n28.0 (-2.5)\n20.5 (-2.5)\n39.5\n68.0\nStatement-level\nResponse-level\n62.0 (-6.0)\nFactuality Score (%)\na\nStatement-level\nResponse-level\nCompleteness Score (%)\nc\n75\n80\n85\n90\n95\n100\nGPT-4o + RAG\nLlama-3.1 + RAG\nTrue Positive\nFalse Positive\nSelf-generated\nNo Reference\n97.1\nStatement-level Analysis (Factuality)\nb\n96.4\n93.7\n95.2\n93.8\n90.8\n85.5\n85.0\n40\n50\n60\n70\n80\nGPT-4o\nGPT-4o + RAG\nLlama-3.1\nLlama-3.1 + RAG\nSupported & Referenced\nSupported but Missed\nUnsupported\nd\n71.7\n62.1\n60.6\n68.2\n65.3\n64.2\n50.5\n57.2\n51.3\n56.9\n44.8\n56.0\nStatement-level Analysis (Completeness)\nFigure 4. Factuality and completeness of model responses. a, Average factuality scores at the response and statement levels\nacross queries. b, Statement-level factuality broken down by the type of evidence cited: relevant retrieved (True Positive),\nirrelevant retrieved (False Positive), self-generated, or none. c, Average completeness scores at the response and statement\nlevels, based on coverage of must-have statements. d, Completeness for must-have statements, categorized by whether the\nsupporting evidence was retrieved and cited (Supported & Referenced), retrieved but not cited (Supported but Missed), or\nnot retrieved at all (Unsupported).\nmodel’s response using numbered citation-style indicators (e.g., [1], [2]), each corresponding to a retrieved passage. By link-\ning each statement to its cited references, and using our manual relevance annotations for the retrieved passages, we assigned\nevery statement to one of the four evidence categories.\nFactuality was highest when statements were grounded in relevant evidence, and declined when models relied on irrelevant\nor self-generated sources (Fig. 4b). When citing true positive passages, GPT-4o achieved a factuality of 97.1%, while Llama-\n3.1 reached 93.8%. When citing false positives, GPT-4o maintained relatively high performance with 95.2%, whereas Llama-\n3.1 dropped more substantially to 85.5%. This suggests that GPT-4o is more robust to noisy or off-topic evidence, often\ngenerating factually accurate content even when the cited material is not directly relevant. In contrast, Llama-3.1 appears more\nsensitive to the quality of retrieved content. When models relied on self-generated evidence without any retrieved support, the\nlowest performance was observed. GPT-4o again maintained a high factuality score of 96.4%, while Llama-3.1 fell to 85.0%,\nindicating Llama-3.1’s greater vulnerability to hallucination in the absence of external grounding.\n2.4.2\nCompleteness\nAt the response level, each response was assigned a score of 1 if it successfully addressed all must-have statements defined in the\ngold-standard reference; otherwise, it received a score of 0. At the statement level, each response was assessed for how many\nof the must-have statements it explicitly covered, and the proportion of addressed statements was calculated. Both scores were\naveraged across all queries.\nCompleteness performance declined under the RAG setting for both models (Fig. 4c). At the response level, GPT-4o and\n8/34\n"}, {"page": 9, "text": "Llama-3.1 each showed a 2.5% decrease. At the statement level, GPT-4o exhibited a minimal decline of 0.4%, while Llama-3.1\nexperienced a more pronounced drop of 5.4%. These results suggest that incorporating retrieved evidence does not guarantee\nimproved coverage of essential content and may even hinder completeness, particularly in smaller or less capable models.\nWe further analyzed model performance at the statement level by categorizing must-have statements according to the\nquality of the retrieved evidence and whether it was cited in the response. Specifically, each must-have statement was grouped\ninto one of three categories: (1) Supported and Referenced: The statement was supported by one of the top-16 retrieved\npassages and explicitly referenced in the model’s response. (2) Supported but Missed: The statement was supported by a\nretrieved passage but was not cited by the model. (3) Unsupported: The statement was not supported by any of the retrieved\npassages.\nCompleteness was highest when models correctly referenced relevant evidence, and declined when such evidence was ei-\nther missed or not retrieved. (Fig. 4d). In the Supported and Referenced category, both models improved under the RAG\nsetting: GPT-4o increased from 68.2% to 71.7%, and Llama-3.1 from 50.5% to 56.9%. However, when models failed to\ncite available supporting evidence (Supported but Missed), GPT-4o’s score decreased from 65.3% to 60.6%, and Llama-3.1\nalso experienced a slight decrease from 57.2% to 56.0%. These results suggest that access to high-quality evidence alone is\ninsufficient, and effective evidence selection remains critical for achieving high completeness. In the Unsupported category,\ncompleteness declined across both models with RAG: GPT-4o fell from 64.2% to 62.1%, and Llama-3.1 from 51.3% to\n44.8%. This indicates that even with retrieval augmentation, models may struggle to compensate when key content is absent\nfrom the retrieved documents, especially in the case of Llama-3.1, which showed the steepest drop.\nAnswer accuracy\nWe evaluated answer accuracy on the full dataset from which the USMLE-style queries were originally sam-\npled. In line with our earlier findings on factuality and completeness, both GPT-4o and Llama-3.1 showed clear performance\ndegradation after applying standard RAG at top-k = 16, with accuracy dropping from 71.1% to 70.5% and from 49.4% to\n45.8%, respectively. These results suggest that the declines in factuality and completeness are not isolated errors, but reflect\nbroader impairments in overall response quality.\n2.5\nEnhanced RAG Pipeline\nWe evaluated the impact of retrieval augmentation using five held-out QA datasets on GPT-4o and Llama-3.1. Four configura-\ntions were tested: (i) standard RAG, which retrieves and appends passages to the input; (ii) RAG with evidence filtering, which\nremoves irrelevant passages to suppress retrieval noise; (iii) RAG with query reformulation, which addresses low coverage in\nthe initial retrieval by using the model’s first response as a rewritten query; and (iv) a combined pipeline integrating both fil-\ntering and reformulation. The filtering module was fine-tuned on our expert-annotated relevance dataset using Llama-3.1-8B\nas the backbone, while the reformulation component adopted a rationale-driven querying strategy to generate more targeted\nfollow-up queries (see Methods for details). Accuracy was automatically computed, and confidence intervals were estimated\nvia 10,000 bootstrap replicates sampled with replacement at the same size as each test set. Detailed results are provided in the\nSupplementary Table 2.\nStandard RAG produced inconsistent effects across datasets and retrieval depths (Fig. 5a, i). On Llama-3.1, standard RAG\nled to accuracy gains in some cases (e.g., MedMCQA) but performance drops in others (e.g., MedQA). Even within the same\ndataset, outcomes varied by top-k: for example, MMLU-Pro showed degradation from k = 1 to k = 16 and improvement only\nat k = 32. This instability highlights the sensitivity of standard RAG to retrieval noise and passage overload. Additionally,\nstandard RAG on GPT-4o also exhibited inconsistent effects, though performance degradation was even more pronounced\n(Fig. 5b, i). On datasets where GPT-4o already achieved strong baseline accuracy, such as MedQA (0.922), MMLU (0.930),\nand MMLU-Pro (0.864), standard RAG led to a decline in performance across all top-k settings. This suggests that for high-\nperforming LLMs, the additional context can introduce more distraction than utility. As with Llama-3.1, MedXPertQA was a\nnotable exception, where standard RAG yielded some gains at higher top-k values.\nCombining evidence filtering and query reformulation yielded consistent and significant improvements across all bench-\nmarks (Fig. 5a, ii-iv). Individually, both filtering and reformulation led to performance improvements in some settings, but\nfailed to produce consistent gains across datasets. When integrated, however, the two components complemented each other—\nfiltering reduced noise from irrelevant passages, while reformulation compensated for low coverage in retrieved evidence—\nresulting in stable and monotonic gains from k = 4 to k = 32. The combined pipeline outperformed the non-RAG baseline by\nup to +9.0% (MedQA), +6.2% (MMLU), +1.1% (MMLU-Pro), +12.0% (MedMCQA), and +8.2% (MedXpertQA), with all\nimprovements statistically significant (McNemar exact test, p < 0.001 for most and p = 0.0022 for MMLU) except MMLU-\nPro. For GPT-4o, the combined pipeline led to notable gains on the more challenging datasets, MedMCQA (+3.4%) and\n9/34\n"}, {"page": 10, "text": "0.720\n0.678\n0.652\n0.650\nk=1\n0.708\n0.682\n0.650\n0.658\n2\n0.710\n0.706\n0.670\n0.652\n4\n0.720\n0.708\n0.690\n0.650\n8\n0.762\n0.734\n0.720\n0.660\n16\n0.740\n0.730\n0.702\n0.636\n32\n0.766\n0.748\n0.748\n0.730\n0.780\n0.776\n0.736\n0.722\n0.782\n0.756\n0.750\n0.746\n0.796\n0.782\n0.752\n0.736\n0.786\n0.792\n0.758\n0.764\n0.800\n0.772\n0.760\n0.738\n0.604\n0.570\n0.591\n0.556\n0.570\n0.556\n0.553\n0.545\n0.628\n0.599\n0.599\n0.588\n0.626\n0.564\n0.615\n0.588\n0.628\n0.628\n0.583\n0.583\n0.631\n0.618\n0.602\n0.642\n0.586\n0.566\n0.564\n0.552\n0.606\n0.566\n0.604\n0.560\n0.632\n0.612\n0.646\n0.592\n0.634\n0.584\n0.618\n0.560\n0.622\n0.604\n0.636\n0.578\n0.648\n0.630\n0.634\n0.620\n0.136\n0.136\n0.136\n0.136\n0.178\n0.150\n0.144\n0.160\n0.168\n0.148\n0.182\n0.128\n0.170\n0.164\n0.184\n0.160\n0.206\n0.176\n0.184\n0.162\n0.218\n0.186\n0.172\n0.180\na\nBase LLM: 0.672\nBase LLM: 0.738\nBase LLM: 0.620\nBase LLM: 0.528\nBase LLM: 0.136\nMedQA\nMMLU\nMMLU-Pro\nMedMCQA\nMedXPertQA\n0.894\n0.896\n0.886\n0.902\nk=1\n0.912\n0.902\n0.888\n0.898\n2\n0.908\n0.904\n0.886\n0.908\n4\n0.918\n0.914\n0.896\n0.904\n8\n0.916\n0.914\n0.902\n0.900\n16\n0.906\n0.920\n0.902\n0.922\n32\n0.790\n0.778\n0.742\n0.782\n0.760\n0.774\n0.754\n0.770\n0.778\n0.780\n0.758\n0.768\n0.768\n0.778\n0.772\n0.752\n0.796\n0.780\n0.770\n0.792\n0.814\n0.786\n0.790\n0.804\n0.900\n0.912\n0.886\n0.904\n0.902\n0.904\n0.884\n0.902\n0.900\n0.898\n0.902\n0.900\n0.912\n0.896\n0.900\n0.914\n0.908\n0.890\n0.910\n0.910\n0.910\n0.908\n0.912\n0.912\n0.834\n0.834\n0.824\n0.826\n0.837\n0.840\n0.829\n0.850\n0.840\n0.816\n0.829\n0.845\n0.858\n0.856\n0.842\n0.840\n0.850\n0.850\n0.840\n0.840\n0.840\n0.840\n0.842\n0.832\n0.288\n0.318\n0.294\n0.310\n0.300\n0.290\n0.294\n0.288\n0.324\n0.324\n0.308\n0.300\n0.340\n0.320\n0.310\n0.314\n0.354\n0.332\n0.310\n0.314\n0.340\n0.342\n0.318\n0.322\nBase LLM: 0.922\nBase LLM: 0.930\nBase LLM: 0.864\nBase LLM: 0.780\nBase LLM: 0.288\nb\nRelative Gain (%)\n+\n-\nRelative Gain (%)\n+\n-\n(i)\n(ii) \n(iii)\n(iv)\nMedQA\nMMLU\nMMLU-Pro\nMedMCQA\nMedXPertQA\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\n(i)\n(ii) \n(iii)\n(iv)\nFigure 5. Performance of RAG variants and non-RAG baselines across five QA datasets. MedQA44, MMLU45,\nMMLU-Pro46, MedMCQA47, and MedXPertQA48 were used. a, Results using Llama-3.1-8B as the base model. b, Results\nusing GPT-4o as the base model. Cell colors indicate the magnitude and direction of accuracy changes relative to the base\nLLM: sky-blue denotes performance gains, while red indicates performance drops. Four RAG configurations are compared:\n(i) standard RAG (retrieval-only), (ii) retrieval + evidence filtering, (iii) retrieval + query reformulation, and (iv) retrieval +\nboth evidence filtering and query reformulation, evaluated at different top-k settings (k = 1,2,4,8,16,32). Each cell displays\nthe accuracy of the model, while the color represents the relative gain compared to the base LLM. Positive gains (blue)\nindicate improved performance over the base LLM, while negative values (red) indicate performance drops. Base LLM\naccuracies are shown beneath each block for reference. The best scores are highlighted in bold.\nMedXpertQA (+6.6%) (p = 0.013 and 0.027), whereas no improvement was observed on MedQA, MMLU, and MMLU-Pro,\nwhere where baseline accuracy was already high and left limited room for further gains (Fig. 5b, ii–iv).\n3\nDiscussion\nThis study presents the first large-scale, fine-grained evaluation of widely used medical RAG frameworks to elucidate their\nbehavior throughout the end-to-end pipeline. We surveyed the literature to understand which RAG configurations are most\ncommonly used in medical applications. Based on these representative frameworks, we evaluated two LLMs (GPT-4o and\nLlama-3.1-8B) on 200 queries (real patient inquiries and USMLE-style questions) across evidence retrieval, evidence selection,\nand response generation. Eighteen medical experts contributed over 80,000 expert annotations. Such an extensive, expert-\nannotated analysis has not been previously achieved in the medical RAG literature. Whereas prior efforts, such as the MIRAGE\nbenchmark39, focused primarily on answer accuracy across large question sets, these efforts offered limited insight into how\nand where failures emerge within the RAG pipeline. In contrast, our fully manual, component-level evaluation dissects the\nend-to-end process and highlights system weaknesses, directly addressing recent calls for more nuanced and clinically relevant\nRAG assessment25.\nFirst and importantly, our evaluation shows that standard RAG may not improve and can even degrade LLM performance\non medical tasks. In head-to-head comparisons with their non-RAG counterparts, GPT-4o and Llama-3.1-8B both exhibited\nmodest drops in factuality and more pronounced declines in completeness, with the latter decreasing by over five percent-\nage points at the statement level. This pattern held across both query types: for patient queries, GPT-4o’s factuality and\ncompleteness dropped by approximately 2% and 8%, respectively; for USMLE-style queries, the decreases were 1% and 3%.\nWhile many prior studies have highlighted RAG’s ability to improve factuality and mitigate hallucinations in medical applica-\ntions29,42,49,50, others–particularly in the general domain–have reported that irrelevant or noisy retrieved content can distract\nmodels or sustain hallucinations51–53. This aligns with our findings: when the model incorporated irrelevant rather than rel-\nevant passages (i.e., the “False Positive” category in Fig. 4), statement-level factuality scores declined, with the effect being\nespecially pronounced for the smaller model, Llama-3.1, which exhibited a drop of over 8%. Completeness is another critical\n10/34\n"}, {"page": 11, "text": "dimension where retrieval is generally expected to fill knowledge gaps and broaden the scope of model responses. However,\nprior evidence remains mixed. In long-form clinical QA tasks, one study reported improved completeness over non-RAG\nLLMs based on manual evaluation29, whereas others observed declines–for example, a drop from 3.47 to 3.27 on a five-point\nscale in one study37, and a roughly 5% decrease reported in another42. Our results indicate a consistent shortfall: when no\nsupporting passage was retrieved for a must-have statement (i.e., Unsupported), models often overlooked key information\nthey had previously included without retrieval, leading to substantial performance degradation, with over 6% for Llama-3.1.\nInterestingly, when a relevant passage was retrieved but not cited by the model (i.e., Supported but Missed), completeness\nstill dropped by 1-5%, suggesting that the model may have been distracted by other retrieved passages and failed to integrate\ncritical evidence.\nThrough our stage-wise analyses, we highlight two primary bottlenecks underlying these declines. First, evidence retrieval\nis often inadequate: only 22% of the top-16 passages were judged relevant, and 31% of queries lacked any relevant passage at\nall. Prior studies have largely examined how irrelevant or conflicting retrieved content influences end-task performance51–57;\nhowever, the retrieval accuracy for medical queries itself has rarely been evaluated in a standardized, expert-grounded manner.\nOur manual evaluation addressed this gap, showing that current retrievers frequently mishandled clinical questions. Moreover,\nthe retrieved content covered only 33% of must-have statements overall and just 26% for USMLE-style queries. This indicates\nthat even with RAG, LLMs often receive limited knowledge support and must rely primarily on their internal knowledge to\naddress medical queries, highlighting the persistent challenge of providing sufficient context58,59. Importantly, our notion of\nrelevance differs from conventional definitions in information retrieval systems60–65. Rather than requiring an explicit answer\nspan, lexical overlap, or topical relatedness, we adopt a coverage-based criterion that evaluates whether a passage supplies the\nnecessary medical information to support key statements. This yields a more clinically grounded and functionally meaningful\nview of retrieval quality.\nThe second bottleneck is evidence selection, where LLMs struggle to identify and integrate relevant information even when\nit is retrieved. Both GPT-4o and Llama-3.1 struggled in selecting relevant evidence, achieving only 41-43% precision and 27-\n49% recall. GPT-4o tended to cite a larger number of retrieved passages (90% of citations vs. 62% for Llama-3.1), indicating\na stronger inclination to incorporate retrieved content into its responses. This behavior presents a potential advantage: when\nprovided with high-quality, relevant evidence, GPT-4o appears better equipped to integrate it effectively. However, this respon-\nsiveness comes at a cost—its limited evidence selection precision leads to a greater number of irrelevant references, averaging\n2.6 per query compared to 1.6 for Llama-3.1. In contrast, Llama-3.1 cited fewer retrieved passages and more frequently missed\nrelevant content, selecting only 34% of the relevant evidence available. It also relied more heavily on self-generated references\n(37.8% of all citations), the majority of which (77.2%) could not be verified and often included fabricated titles, authors, or\npublication sources. These findings highlight complementary failure modes between model scales: while larger models like\nGPT-4o risk over-incorporating retrieved information without sufficient discrimination, smaller models like Llama-3.1 may\nunder-utilize retrieved evidence and compensate by hallucinating references. Our evidence selection analysis is closely related\nto the actively growing body of research on evidence attribution and citation generation, both of which aim to evaluate and\nimprove the verifiability of LLM responses38,66–74. Consistent with these studies, our findings confirm that current LLMs\nstill exhibit notable limitations in reliably using available evidence and grounding their responses appropriately. Additionally,\nwe positioned evidence selection as a connecting layer within the medical RAG pipeline, enabling us to trace how retrieved\ncontent contributes to model responses at the statement level. This represents a novel analytical perspective in the context of\nmedical RAG, bridging retrieval and generation more explicitly than in previous studies.\nTo better understand why RAG underperformed compared to its non-RAG counterparts, we conducted a qualitative error\nanalysis focusing on cases where RAG responses demonstrated lower statement-level factuality. We observed that RAG often\nintroduced errors by anchoring responses to misleading numerical references in the retrieved passages. For instance, in one\nexample, a prolactin level of 28 ng/mL was initially judged normal by GPT-4o, but the RAG model flagged it as abnormal.\nThis change appears to stem from retrieved information that defined the upper limit as 25 ng/mL; although this information\nwas contextually misaligned with the original question, the model incorrectly adopted the reference range and judged 28\nng/mL as abnormal. A similar issue emerged in response to the question, “What is the safest amount of Advil to take at one\ntime?” The GPT-4o-based RAG model responded that 400 mg is the safest dose–an overstatement likely driven by the fact\nthat multiple retrieved studies consistently referenced 400 mg as the standard single adult dose in bioequivalence and safety\nevaluations, though these studies were situated in different clinical or experimental contexts. Another recurrent failure mode\ninvolved lexical ambiguity or synonym mismatches in retrieval. When asked “Is poison ivy contagious between people?”, the\nretriever surfaced content about the character Poison Ivy from DC Comics and the 1992 film of the same name rather than\nabout the allergenic plant. Consequently, Llama-3.1 generated a response describing the fictional character, entirely missing\nthe medical intent of the question. This error illustrates both the retriever’s failure to disambiguate terms and the model’s\n11/34\n"}, {"page": 12, "text": "inability to recover from such retrieval mistakes. We also found that RAG models were more prone to misinterpreting user\nintent, especially when retrieved content introduced a competing frame. In response to the question “How long does this year’s\nflu usually last?”, both non-RAG models correctly addressed symptom duration at the individual level. However, once RAG was\nintroduced, the models instead focused on the epidemiological length of flu seasons, which is an error likely caused by retrieved\npassages discussing historical influenza trends and population-level data. These examples collectively demonstrate that errors\noriginating in the retrieval stage act as noise, impairing the accuracy and reliability of the model’s generated responses.\nBeyond evaluation, we proposed augmenting the standard RAG pipeline with two lightweight and model-agnostic com-\nponents: evidence filtering and query reformulation. Unlike approaches that require retraining either the retriever or the\nLLM75–77, our methods are readily applicable to any retrieval systems and LLMs. For evidence filtering, we first evaluated\nzero-shot performance using our 3,200 expert-annotated samples spanning 200 queries and their corresponding top-16 re-\ntrieved passages. Zero-shot filtering proved insufficient, yielding limited performance for both Llama-3.1 (precision = 0.483,\nrecall = 0.566, F1 = 0.521) and GPT-4o (precision = 0.697, recall = 0.324, F1 = 0.442). After fine-tuning Llama-3.1 with\nfive-fold cross-validation, performance improved substantially (precision = 0.592, recall = 0.657, F1 = 0.623). These results\nhighlight that filtering is effective but cannot be reliably achieved in a zero-shot setting. Supervised signal remains important,\nunderscoring the value of our large-scale expert annotations as a reusable training resource for future modules. For query\nreformulation, we drew inspiration from prior work showing that models capable of articulating explicit rationales tend to\ngenerate more focused and contextually appropriate queries40,78,79. Building on this rationale-guided formulation approach,\nwe prompted the model to first produce an intermediate response to the initial query and then used that response itself as a\nreformulated query. To assess the effect of reformulation, we compared retrieval outcomes between initial and reformulated\nqueries using the MedQA dataset. For each query, we retrieved 256 passages and applied our fine-tuned evidence filtering\nmodel to assess relevance. On average, reformulated queries yielded substantially more passages deemed relevant than the\noriginal queries (32% vs. 13%), indicating that reformulation enhanced both the precision and contextual alignment of the\nretrieval stage.\nBuilding on the same evaluation framework, we also explored whether stronger retrievers could further improve perfor-\nmance. We evaluated two representative approaches: BM2561, a classical lexical baseline widely used for its simplicity, and\nQwen3-Embedding-0.6B80, an open-source LLM-based embedding model that outperforms commercial API-based models\n(text-embedding-ada-002, text-embedding-3-large) on the MTEB leaderboard81 (https://huggingface.co/spaces/mteb/\nleaderboard). As shown in Supplementary Table 2, no single retriever consistently outperformed others across the five medi-\ncal QA datasets. Performance varied by dataset, and even the most advanced model, Qwen3-Embedding-0.6B, did not achieve\nthe highest accuracy overall. These results indicate that retriever replacement alone cannot overcome the intrinsic limitations\nof the RAG architecture. In contrast, our proposed modules (evidence filtering and query reformulation) yielded more robust\nand generalizable improvements across both BM25 and Qwen3 retrievers, consistent with our MedCPT-based findings.\nLooking forward, we recommend that future RAG development in medicine move beyond end-task performance metrics\nand adopt a stage-wise perspective that explicitly examines retrieval, selection, and generation behaviors. Since RAG pipelines\ndo not consistently lead to performance gains across tasks, a safer and more reliable approach is to incorporate lightweight mod-\nules such as evidence filtering and query reformulation, which can offer practical improvements without requiring retriever or\nmodel retraining. Further research should explore how these interventions interact with different retrievers, search databases,\nmedical subdomains, and LLM architectures, as well as how they influence factual grounding and reasoning consistency. We\nalso observed that the benefits of RAG augmentation tend to diminish when the underlying model already demonstrates strong\nperformance. In such cases, engineers may need to determine whether RAG should be applied using a small validation set. In\nthe longer term, an adaptive architecture that selectively invokes RAG only when the model’s internal knowledge appears\ninsufficient could offer a promising and efficient solution. Finally, establishing and sharing standardized expert-annotated\ndatasets, such as those introduced in this study, will be essential for advancing medical RAG. While expert-driven evaluation\nremains critical, it should be complemented by more scalable strategies, including community-based review82 and automated\nassessment83. These efforts will support more reliable and sustainable evaluation across diverse medical settings.\n12/34\n"}, {"page": 13, "text": "4\nMethods\n4.1\nAnnotation\n4.1.1\nData Curation\nPatient queries\nWe used a sample of 100 queries from the K-QA dataset42 This dataset originally comprises 201 free-form\nclinical questions sourced from real-world patient–physician conversations on an online consultation platform. The queries,\nsubmitted by patients, span a broad range of topics, including ear, nose, and throat (ENT), dermatology, mental and emotional\nhealth, and vision and eye care. Each query is paired with a long-form, natural language response written by a physician. In\naddition, the dataset includes manual annotations identifying must-have and nice-to-have statements within the physician\nresponses.\nUSMLE-stytle queries\nWe used a sample of 100 multiple-choice questions from the MedBullets dataset43, which is curated\nfor USMLE Step 2 and Step 3 preparation. Unlike other datasets that provide only questions and answers44,45, MedBullets\nincludes detailed, expert-authored explanations, offering richer context and rationale.\nStatement extraction\nAs the MedBullets dataset does not include statement-level annotations, we curated must-have state-\nments by using GPT-4o to segment each explanation into individual statements. We employed a few-shot prompting strategy,\nusing a mixture of general- and medical-domain examples adapted from prior work42,84. The full prompt with examples is\nshown in Supplementary Table 3. The same procedure was also applied to segment model responses into individual statements\nfor downstream evaluation.\nMust-have statement identification\nThe full set of extracted statements, along with the question and gold answer, was then\npassed to GPT-4o using a few-shot prompt to identify which statements were essential for answering the question, guiding\nthe model to select only the most critical information. The model produced a list of binary labels indicating whether each\nstatement was considered must-have. The full prompt with examples is shown in Supplementary Table 4.\nResponse generation\nWe generated model responses using both RAG and non-RAG variants. We used identical prompts\nto generate responses from both GPT-4o and Llama-3.1-8B. However, different sampling parameters were applied to ac-\ncommodate each model’s behavior. Specifically, GPT-4o was run with temperature = 0.8, while Llama-3.1-8B used\ntemperature = 1.0 and top_p = 0.9. We intentionally avoided greedy decoding, as deterministic outputs often failed\nto comply with formatting requirements, particularly for citation generation. Instead, we introduced controlled randomness\nand, if necessary, performed multiple inference attempts to obtain outputs that conformed to the expected structure. The full\nprompt used is shown in Supplementary Table 5.\nModel statement filtering\nSome model statements could not be reliably assessed for factuality, as they lacked verifiable factual\ncontent. For instance, they simply rephrased the question, provided generic commentary, or included procedural language. To\nexclude such statements, we applied a GPT-4o classifier prompt to identify and filter out non-distinctive content. A statement\nwas considered distinctive only if it introduced new clinical reasoning, factual knowledge, or a definitive judgment not already\nstated or implied in the question. The prompt used for this classification is shown in Supplementary Table 6.\nStatement-citation alignment\nWe aligned inline citations with the individual statements extracted from each response. Any\ninline citations were preserved alongside their corresponding statements, ensuring that each statement remained linked to\nits supporting reference (or none, if no citation was originally provided). This process enabled us to later trace which refer-\nence was intended to support each claim. The alignment was performed using GPT-4o, with the prompt details provided in\nSupplementary Table 7.\n4.1.2\nAnnotation Procedure\nOverview\nAll annotations were conducted using a custom interface built on Label Studio (https://labelstud.io/), de-\nsigned to provide annotators with a streamlined and user-friendly labeling workflow. Annotators were given written anno-\ntation guidelines describing each task, along with example cases, and we held synchronous meetings prior to the annotation\nphase to clarify task definitions and resolve ambiguities. A total of 18 annotators participated in this study. Of these, nine\n13/34\n"}, {"page": 14, "text": "were assigned to the evidence retrieval and evidence selection tasks, and the remaining nine were assigned to the response\ngeneration tasks (factuality and completeness). All annotators involved in the response generation tasks were either medical\nresidents or clinical fellows, ensuring sufficient clinical expertise to assess the accuracy and completeness of model-generated\nresponses. Details on each annotator’s clinical training stage, medical specialty, and institutional affiliation at the time of an-\nnotation are provided in Supplementary Table 8. We ultimately collected over 80,502 annotations across all stages of the\npipeline, representing the largest human-labeled dataset for fine-grained evaluation of medical RAG systems.\nEvidence Retrieval\nThis subtask evaluates the relevance of retrieved passages with respect to the given query, defined by\nwhether a passage contains information essential to support the answer. Annotators are presented with a query, the top-\n16 passages retrieved by the system, and a set of must-have statements identified from the gold response. Each passage is\npaired with each must-have statement to form individual annotation units. For every passage-statement pair, annotators as-\nsess whether the passage provides sufficient information to support the given statement. Support is defined as follows. First,\na statement is considered fully supported if the passage contains all the necessary information to infer the statement directly\nand unambiguously. Second, partial support is assigned when the passage includes content that is thematically or contextually\nrelated to the statement, but additional assumptions or reliance on external knowledge are required to complete the inference.\nFinally, a passage is labeled as providing no support if it fails to offer any relevant information for the statement, or if it con-\ntains content that contradicts it. Illustrative examples for each support category are provided in Supplementary Table 9. These\nexamples were also included in the official annotation guidelines and used during annotator training to ensure consistency and\nshared understanding of the labeling criteria.\nEvidence Selection\nThis subtask evaluates the source attribution of model-generated references. Annotators are given (1) a\nquery, (2) the top-16 passages retrieved by the system, and (3) the references produced by the model. For each reference, they\ndetermine which passage(s) served as its source of information. A passage is considered a source if its metadata and content\ntogether provide sufficient information to reconstruct or justify the reference. When multiple passages jointly contribute to\na single reference, annotators are instructed to select all relevant passages. If none of the retrieved passages offer adequate\nsupport, the reference is labeled as self-generated, indicating that the content originated from the model rather than from\nretrieved evidence.\nResponse Generation\nThis subtask evaluates the quality of model-generated responses along two axes: factuality and com-\npleteness. The factuality task assesses the accuracy of individual statements produced by the model. Annotators are provided\nwith the patient query, the model-generated response, and the corresponding reference response for context. Each model state-\nment is labeled as true if it aligns with the reference response or can reasonably be considered accurate based on the annotator’s\nclinical expertise or authoritative medical sources. Conversely, a statement is labeled as false if it contradicts the reference,\nconflicts with established medical knowledge, or introduces implausible or unverifiable claims. The completeness task exam-\nines whether the model response adequately captures the essential content of the reference response. Each reference response\nis decomposed into a set of must-have statements, i.e., statements deemed clinically important and expected to appear in a\ncomplete answer. For each must-have statement, annotators evaluate whether the model response fully supports it, partially\nsupports it, or fails to support it. This task adopts the same three-way support classification scheme as the evidence retrieval\ntask, enabling consistent, stage-aligned evaluation across the pipeline (see Supplementary Table 9 for examples).\n4.2\nRAG Implementation\nTo design our RAG configuration, we conducted an extensive review of prior studies, particularly focusing on the choice of\nretrieval corpora and retriever (i.e., search engine) architectures. Rather than adopting a custom corpus or architecture, we\nopted for a widely adopted pipeline structure commonly reported in the literature. A summary of this review is provided in\nSupplementary Table 1. Building on this baseline RAG system, we incorporated two key components–query formulation and\nevidence filtering–to enhance performance. All implementation was based on the MedRAG toolkit (https://github.com/\nTeddy-XiongGZ/MedRAG), with additional functionalities newly developed for this study. The resulting pipeline is illustrated\nin Fig. 6.\nRetrieval Corpora\nWe adopted the multi-source retrieval setup proposed by MedRAG39, an early framework for medical-\ndomain RAG, and further augmented it with clinical guidelines. Our final retrieval corpus comprises five complementary\nsources: (1) PubMed: A widely used biomedical article repository containing 23.9 million articles, covering diverse topics\n14/34\n"}, {"page": 15, "text": "Retrieval\nReformulated\nQuery\nQuery Reformulation\nEvidence Filtering\nResponse Generation\nInitial Query: “Benadryl makes \nme super sleepy. Can I take \nClaritin during the day instead?”\nReformulated Query: \n“Diphenhydramine, the active \ningredient in Benadryl, is a first-\ngeneration H1 histamine receptor \nantagonist …”\n…\nLLM\nFiltering Model\nRetriever\nVector\nDatabase\nCandidate Passages\n…\n…\nRefined Evidence Set\nInitial\nQuery\nCandidate Passages\n…\n…\nLLM\nRefined Evidence Set\nResponse: “Yes, you can take \nClaritin (loratadine) during the \nday as it is a non-drowsy \nantihistamine. Benadryl \n(diphenhydramine) is  …”\nInitial\nQuery\nFigure 6. Enhanced RAG pipeline. This introduces two lightweight components–query reformulation and evidence\nfiltering–to improve the quality of retrieved evidence before generation. Given an input question, the LLM first reformulates\nit to better suit retrieval (Query Reformulation). The retriever then searches external knowledge sources such as PubMed or\nWikipedia to gather candidate passages (Retrieval). A filtering model removes irrelevant evidence to retain informative\nevidence (Evidence Filtering). Finally, the LLM generates a response conditioned on the refined evidence set (Response\nGeneration).\nacross the biomedical and life sciences literature. (2) StatPearls: A structured educational corpus comprising 301.2k passages\nfrom 9.3k peer-reviewed clinical articles, designed primarily for medical board exam preparation and continuing medical edu-\ncation. Each document follows a consistent format with sections such as epidemiology, pathophysiology, clinical presentation,\nand management, making it a valuable resource for evidence-grounded summarization and retrieval. (3) Wikipedia: A broad-\ncoverage knowledge source with 29.9 million passages from 6.5 million documents, covering both general-domain and medi-\ncal topics. (4) Medical textbooks: A set of 18 licensed medical textbooks spanning foundational medical disciplines, including\nanatomy, histology, neurology, pathology, physiology, biochemistry, immunology, obstetrics and gynecology, pediatrics, psy-\nchiatry, cell biology, internal medicine, pharmacology, and surgery. This corpus yielded 125.8k expert-authored snippets.\n(5) Clinical guidelines: we additionally curated 723K passages from guidelines issued by nine leading health organizations\nand online clinical sources, including: Cancer Care Ontario (CCO), Centers for Disease Control and Prevention (CDC), Cana-\ndian Medical Association (CMA), International Committee of the Red Cross (ICRC), National Institute for Health and Care\nExcellence (NICE), PubMed Clinical Queries, Strategy for Patient-Oriented Research (SPOR), World Health Organization\n(WHO), and WikiDoc.\nRetriever\nWe used MedCPT41, a dual-encoder retriever pretrained on PubMed search logs. It comprises two components: a\nquery encoder and an article encoder. The query encoder transforms an input query into a dense vector at inference time, while\nthe article encoder pre-encodes all passages from the retrieval corpora into dense vector representations, which are stored in a\nvector database. During inference, the query vector is matched against the pre-encoded passage vectors using vector similarity\nmetrics, and the top-k most similar passages are retrieved.\nEvidence Filtering\nFor the filtering model, we used Llama-3.1-8B to maintain architectural consistency with the base LLM.\nThe model is designed to estimate p(ˆy|q,d), where it takes a query q and a candidate passage d as input and predicts a binary\nlabel ˆy ∈{0,1}, indicating whether the passage is relevant to the query. We constructed a dataset of 3,200 query–passage\npairs with expert-provided annotations serving as ground truth labels. Given a query and a candidate passage, the model was\nprompted to generate one of two tokens: “Yes” if the passage contained supporting evidence for the query, or “No” otherwise\n(see Supplementary Table 10 for the detailed prompt). The model was trained using standard causal language modeling\nloss (cross-entropy loss) over the next token. Model hyperparameters were tuned through five-fold cross-validation. The\nfinal configuration used a learning rate of 2e-6, a batch size of 8, and 3 training epochs. Training was performed using the\nLLaMA-Factory repository85 on a single 80GB A100 GPU.\n15/34\n"}, {"page": 16, "text": "Query Reformulation\nIn medical QA, the formulation of user queries plays a critical role in retrieval performance. Overly\ndetailed queries, such as those containing lengthy patient narratives or exhaustive symptom descriptions, may overwhelm\nthe retriever with peripheral information, obscuring the core diagnostic clues. Conversely, overly concise queries often lack\nsufficient clinical context, making it difficult to identify relevant evidence. To address these challenges, we generate rationale\nqueries that reflect the model’s internal reasoning process for solving the clinical task. This rationale serves as an optimized\nquery representation: for verbose inputs, it helps filter out irrelevant details and highlights diagnostically salient information;\nfor underspecified inputs, it fills in missing but medically necessary context that would be expected in a clinician’s reasoning.\nGiven an input question q, an LLM M is prompted to produce a rationale r as follows:\nRespond to the following clinical decision-making task using the provided patient information, in a step-by-step fash-\nion. Output your explanation and single option from the given options as the final answer.\nDuring retrieval, only the rationale r is provided to the retriever. Preliminary analysis showed that including both the\noriginal question and the rationale often exceeds the retriever’s input length limit and may introduce redundant or conflicting\nsignals. We retrieve the top-k passages using r, and the same model M is then used to generate the final response.\n4.3\nHeld-out Test Sets\nWe used five medical-domain QA benchmarks that consist of exam-style multiple-choice questions and are widely used as\nstandard testbeds in this field. These datasets primarily provide answer keys without long-form explanations, and our evalu-\nation focused solely on answer matching (i.e., accuracy). (1) MedQA44: A dataset of four-option multiple-choice questions\ncollected from medical question banks and exam preparation sites. (2) MedMCQA47: A large-scale dataset of Indian post-\ngraduate medical entrance exam questions (AIIMS, NEET-PG), covering a wide range of medical subjects. (3) MMLU45: A\npopular, comprehensive multi-domain multiple-choice benchmark comprising 57 subjects. We selected a subset of MMLU\nfocused on clinical knowledge, college medicine, and professional medicine. (4) MMLU-Pro46: An enhanced and more chal-\nlenging version of MMLU that increases the number of answer choices per question (from 4 to 10), filters out trivial or noisy\nitems, and emphasizes reasoning complexity. We likewise selected subsets of MMLU-Pro concentrated on clinical knowledge,\ncollege medicine, and professional medicine. (5) MedXpertQA48: A highly challenging medical QA benchmark comprising\nmultiple-choice questions across 17 specialties and 11 body systems. It incorporates specialty board–style questions with ex-\ntensive filtering, augmentation, and expert review. Even experts achieve only around 42% accuracy, making it one of the most\ndifficult medical QA benchmarks to date. Due to inference cost constraints, we randomly sampled up to 500 examples per\ndataset when the original size exceeded this threshold.\nReferences\n1. Tian, S. et al. Opportunities and challenges for chatgpt and large language models in biomedicine and health. Briefings\nBioinforma. 25 (2023).\n2. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat. medicine 29, 1930–1940 (2023).\n3. Liu, F. et al. Application of large language models in medicine. Nat. Rev. Bioeng. 1–20 (2025).\n4. Lucas, M. M., Yang, J., Pomeroy, J. K. & Yang, C. C. Reasoning with large language models for medical question\nanswering. J. Am. Med. Informatics Assoc. 31, 1964–1975 (2024).\n5. Yang, R. et al. Integrating umls knowledge into large language models for medical question answering. arXiv preprint\narXiv:2310.02778 (2023).\n6. Zhou, S. et al. Large language models for disease diagnosis: A scoping review. npj Artif. Intell. 1, 9 (2025).\n7. Liu, X. et al. A generalist medical language model for disease diagnosis assistance. Nat. medicine 31, 932–942 (2025).\n8. Kim, K. et al. Llm-guided multi-modal multiple instance learning for 5-year overall survival prediction of lung cancer.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention, 239–249 (Springer, 2024).\n9. Kim, H. et al. Leme: Open large language models for ophthalmology with advanced reasoning and clinical validation.\narXiv e-prints arXiv–2410 (2024).\n16/34\n"}, {"page": 17, "text": "10. Wu, E., Wu, K. & Zou, J. Limitations of learning new and updated medical knowledge with commercial fine-tuning\nlarge language models. NEJM AI 2, AIcs2401155 (2025).\n11. Li, Q. et al. Reviewing clinical knowledge in medical large language models: Training and beyond. arXiv preprint\narXiv:2502.20988 (2025).\n12. Hurst, A. et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).\n13. Team, G. et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530 (2024).\n14. Grattafiori, A. et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).\n15. Hager, P. et al. Evaluation and mitigation of the limitations of large language models in clinical decision-making. Nat.\nmedicine 30, 2613–2622 (2024).\n16. Li, J. et al. Benchmarking large language models in evidence-based medicine. IEEE J. Biomed. Heal. Informatics (2024).\n17. Kell, G. et al. Question answering systems for health professionals at the point of care—a systematic review. J. american\nmedical informatics association 31, 1009–1024 (2024).\n18. Gallifant, J. et al. The tripod-llm reporting guideline for studies using large language models. Nat. medicine 31, 60–69\n(2025).\n19. Kim, Y. et al.\nMedical hallucinations in foundation models and their impact on healthcare.\narXiv preprint\narXiv:2503.05777 (2025).\n20. Chen, Q. et al. Benchmarking large language models for biomedical natural language processing applications and rec-\nommendations. Nat. communications 16, 3280 (2025).\n21. Press, O. et al. Citeme: Can language models accurately cite scientific claims? Adv. Neural Inf. Process. Syst. 37, 7847–\n7877 (2024).\n22. Wang, X. et al. Medcite: Can language models generate verifiable text for medicine? arXiv preprint arXiv:2506.06605\n(2025).\n23. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. neural information processing\nsystems 33, 9459–9474 (2020).\n24. Gupta, S., Ranjan, R. & Singh, S. N. A comprehensive survey of retrieval-augmented generation (rag): Evolution,\ncurrent landscape and future directions. arXiv preprint arXiv:2410.12837 (2024).\n25. Amugongo, L. M., Mascheroni, P., Brooks, S., Doering, S. & Seidel, J. Retrieval augmented generation for large language\nmodels in healthcare: A systematic review. PLOS Digit. Heal. 4, e0000877 (2025).\n26. Jin, Q., Yang, Y., Chen, Q. & Lu, Z. Genegpt: Augmenting large language models with domain tools for improved access\nto biomedical information. Bioinformatics 40, btae075 (2024).\n27. Fan, W. et al. A survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the\n30th ACM SIGKDD conference on knowledge discovery and data mining, 6491–6501 (2024).\n28. Gargari, O. K. & Habibi, G. Enhancing medical ai with retrieval-augmented generation: A mini narrative review. Digit.\nhealth 11, 20552076251337177 (2025).\n29. Zakka, C. et al. Almanac—retrieval-augmented language models for clinical medicine. Nejm ai 1, AIoa2300068 (2024).\n30. Unlu, O. et al. Retrieval-augmented generation–enabled gpt-4 for clinical trial screening. NEJM AI 1, AIoa2400181\n(2024).\n31. Kresevic, S. et al. Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval\naugmented generation-based framework. NPJ digital medicine 7, 102 (2024).\n32. Wada, A. et al. Retrieval-augmented generation elevates local llm quality in radiology contrast media consultation. npj\nDigit. Medicine 8, 395 (2025).\n33. Ke, Y. H. et al. Retrieval augmented generation for 10 large language models and its generalizability in assessing medical\nfitness. npj Digit. Medicine 8, 187 (2025).\n34. Gaber, F. et al. Evaluating large language model workflows in clinical decision support for triage and referral and diag-\nnosis. npj Digit. Medicine 8, 263 (2025).\n17/34\n"}, {"page": 18, "text": "35. Yu, H. et al. Evaluation of retrieval-augmented generation: A survey. In CCF Conference on Big Data, 102–120 (Springer,\n2024).\n36. Singh, A., Ehtesham, A., Kumar, S. & Khoei, T. T. Agentic retrieval-augmented generation: A survey on agentic rag.\narXiv preprint arXiv:2501.09136 (2025).\n37. Gilson, A. et al. Enhancing large language models with domain-specific retrieval augment generation: A case study on\nlong-form consumer health question answering in ophthalmology. arXiv preprint arXiv:2409.13902 (2024).\n38. Wu, K. et al. An automated framework for assessing how well llms cite relevant medical references. Nat. Commun. 16,\n3615 (2025).\n39. Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking retrieval-augmented generation for medicine. In Findings of the\nAssociation for Computational Linguistics ACL 2024, 6233–6251 (2024).\n40. Sohn, J. et al. Rationale-guided retrieval augmented generation for medical question answering. In Proceedings of the\n2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), 12739–12753 (2025).\n41. Jin, Q. et al. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical\ninformation retrieval. Bioinformatics 39, btad651 (2023).\n42. Manes, I. et al. K-qa: A real-world medical q&a benchmark. In Proceedings of the 23rd Workshop on Biomedical Natural\nLanguage Processing, 277–294 (2024).\n43. Chen, H., Fang, Z., Singla, Y. & Dredze, M. Benchmarking large language models on answering and explaining chal-\nlenging medical questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1: Long Papers), 3563–3599 (2025).\n44. Jin, D. et al. What disease does this patient have? a large-scale open domain question answering dataset from medical\nexams. Appl. Sci. 11, 6421 (2021).\n45. Hendrycks, D. et al. Measuring massive multitask language understanding. In International Conference on Learning\nRepresentations (2021).\n46. Wang, Y. et al. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Adv. Neural\nInf. Process. Syst. 37, 95266–95290 (2024).\n47. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Medmcqa: A large-scale multi-subject multi-choice dataset for medical\ndomain question answering. In Conference on health, inference, and learning, 248–260 (PMLR, 2022).\n48. Zuo, Y. et al. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. In Forty-second International\nConference on Machine Learning (2025).\n49. Miao, J., Thongprayoon, C., Suppadungsuk, S., Garcia Valencia, O. A. & Cheungpasitporn, W. Integrating retrieval-\naugmented generation with large language models in nephrology: advancing practical applications. Medicina 60, 445\n(2024).\n50. Luo, M.-J. et al. Development and evaluation of a retrieval-augmented large language model framework for ophthalmol-\nogy. JAMA ophthalmology 142, 798–805 (2024).\n51. Ding, H., Pang, L., Wei, Z., Shen, H. & Cheng, X. Retrieve only when it needs: Adaptive retrieval augmentation for\nhallucination mitigation in large language models. arXiv preprint arXiv:2402.10612 (2024).\n52. Niu, C. et al. Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models. In\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 10862–\n10878 (2024).\n53. Sun, Z. et al. Redeep: Detecting hallucination in retrieval-augmented generation via mechanistic interpretability. In The\nThirteenth International Conference on Learning Representations (2025).\n54. Cuconasu, F. et al. The power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, 719–729 (2024).\n55. Yoran, O., Wolfson, T., Ram, O. & Berant, J. Making retrieval-augmented language models robust to irrelevant context.\nIn The Twelfth International Conference on Learning Representations (2024).\n18/34\n"}, {"page": 19, "text": "56. Wu, S. et al. How easily do irrelevant inputs skew the responses of large language models? In First Conference on Language\nModeling (2024).\n57. Amiraz, C., Cuconasu, F., Filice, S. & Karnin, Z. The distracting effect: Understanding irrelevant passages in RAG. In\nChe, W., Nabende, J., Shutova, E. & Pilehvar, M. T. (eds.) Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 18228–18258 (Association for Computational Linguistics, Vienna,\nAustria, 2025).\n58. Joren, H. et al. Sufficient context: A new lens on retrieval augmented generation systems. In The Thirteenth International\nConference on Learning Representations (2025).\n59. Xie, K., Laban, P., Choubey, P. K., Xiong, C. & Wu, C.-S. Do rag systems cover what matters? evaluating and optimizing\nresponses with sub-question coverage. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 5836–5849 (2025).\n60. Wei, X. & Croft, W. B. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th annual international\nACM SIGIR conference on Research and development in information retrieval, 178–185 (2006).\n61. Robertson, S., Zaragoza, H. et al. The probabilistic relevance framework: Bm25 and beyond. Foundations Trends Inf.\nRetr. 3, 333–389 (2009).\n62. Lavrenko, V. & Croft, W. B. Relevance-based language models. In ACM SIGIR Forum, vol. 51, 260–267 (ACM New\nYork, NY, USA, 2017).\n63. Karpukhin, V. et al. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), 6769–6781 (2020).\n64. Khattab, O. & Zaharia, M. Colbert: Efficient and effective passage search via contextualized late interaction over bert.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, 39–48\n(2020).\n65. Zhao, W. X., Liu, J., Ren, R. & Wen, J.-R. Dense text retrieval based on pretrained language models: A survey. ACM\nTransactions on Inf. Syst. 42, 1–60 (2024).\n66. Rashkin, H. et al. Measuring attribution in natural language generation models. Comput. Linguist. 49, 777–840 (2023).\n67. Gao, T., Yen, H., Yu, J. & Chen, D. Enabling large language models to generate text with citations. In Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Processing (Association for Computational Linguistics, 2023).\n68. Liu, N. F., Zhang, T. & Liang, P. Evaluating verifiability in generative search engines. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, 7001–7025 (2023).\n69. Yue, X. et al. Automatic evaluation of attribution by large language models. In Findings of the Association for Computational\nLinguistics: EMNLP 2023, 4615–4635 (2023).\n70. Huang, C., Wu, Z., Hu, Y. & Wang, W. Training language models to generate text with citations via fine-grained\nrewards. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n2926–2949 (2024).\n71. Malaviya, C. et al. Expertqa: Expert-curated questions and attributed answers. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers), 3025–3045 (2024).\n72. Asai, A. et al.\nOpenscholar:\nSynthesizing scientific literature with retrieval-augmented lms.\narXiv preprint\narXiv:2411.14199 (2024).\n73. Zhang, J. et al. LongCite: Enabling LLMs to generate fine-grained citations in long-context QA. In Che, W., Nabende,\nJ., Shutova, E. & Pilehvar, M. T. (eds.) Findings of the Association for Computational Linguistics: ACL 2025, 5098–5122\n(Association for Computational Linguistics, Vienna, Austria, 2025).\n74. Wang, X. et al. MedCite: Can language models generate verifiable text for medicine? In Che, W., Nabende, J., Shutova, E.\n& Pilehvar, M. T. (eds.) Findings of the Association for Computational Linguistics: ACL 2025, 18891–18913 (Association\nfor Computational Linguistics, Vienna, Austria, 2025).\n75. Asai, A., Wu, Z., Wang, Y., Sil, A. & Hajishirzi, H. Self-rag: Learning to retrieve, generate, and critique through self-\nreflection. In International Conference on Learning Representations (2024).\n19/34\n"}, {"page": 20, "text": "76. Jeong, M., Sohn, J., Sung, M. & Kang, J. Improving medical reasoning through retrieval and self-reflection with retrieval-\naugmented large language models. Bioinformatics 40, i119–i129 (2024).\n77. Jin, B. et al. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint\narXiv:2503.09516 (2025).\n78. Wang, L., Yang, N. & Wei, F. Query2doc: Query expansion with large language models. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, 9414–9423 (2023).\n79. Jagerman, R., Zhuang, H., Qin, Z., Wang, X. & Bendersky, M. Query expansion by prompting large language models.\narXiv preprint arXiv:2305.03653 (2023).\n80. Zhang, Y. et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint\narXiv:2506.05176 (2025).\n81. Muennighoff, N., Tazi, N., Magne, L. & Reimers, N. Mteb: Massive text embedding benchmark. In Proceedings of the\n17th Conference of the European Chapter of the Association for Computational Linguistics, 2014–2037 (2023).\n82. Chiang, W.-L. et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International\nConference on Machine Learning (2024).\n83. Ru, D. et al. Ragchecker: A fine-grained framework for diagnosing retrieval-augmented generation. Adv. Neural Inf.\nProcess. Syst. 37, 21999–22027 (2024).\n84. Min, S. et al. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing, 12076–12100 (2023).\n85. Zheng, Y., Zhang, R., Zhang, J., YeYanhan, Y. & Luo, Z. Llamafactory: Unified efficient fine-tuning of 100+ language\nmodels. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System\nDemonstrations), 400–410 (2024).\n86. Li, Y. et al. Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical\ndomain knowledge. Cureus 15 (2023).\n87. Rau, A. et al. A context-based chatbot surpasses radiologists and generic chatgpt in following the acr appropriateness\nguidelines. Radiology 308, e230970 (2023).\n88. Vector embeddings - OpenAI API.\n89. Russe, M. F. et al. Performance of chatgpt, human radiologists, and context-aware chatgpt in identifying ao codes from\nradiology reports. Sci. Reports 13, 14215 (2023).\n90. Lozano, A., Fleming, S. L., Chiang, C.-C. & Shah, N. Clinfo. ai: An open-source retrieval-augmented large language\nmodel system for answering medical questions using scientific literature. In Pacific Symposium on Biocomputing 2024,\n8–23 (World Scientific, 2023).\n91. National Center for Biotechnology Information. Entrez Programming Utilities Help. National Library of Medicine (US),\nBethesda, MD (2023).\n92. Guo, Y., Qiu, W., Leroy, G., Wang, S. & Cohen, T. Retrieval augmentation of large language models for lay language\ngeneration. J. Biomed. Informatics 149, 104580 (2024).\n93. Ferber, D. et al.\nGpt-4 for information retrieval and comparison of medical oncology guidelines.\nNEJM AI 1,\nAIcs2300235 (2024).\n94. Chen, X. et al. Evaluating and enhancing large language models’ performance in domain-specific medicine: development\nand usability study with docoa. J. medical Internet research 26, e58158 (2024).\n95. Izacard, G. et al. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118\n(2021).\n96. Cohan, A., Feldman, S., Beltagy, I., Downey, D. & Weld, D. SPECTER: Document-level representation learning using\ncitation-informed transformers. In Jurafsky, D., Chai, J., Schluter, N. & Tetreault, J. (eds.) Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, 2270–2282 (Association for Computational Linguistics, Online,\n2020).\n97. Alkhalaf, M., Yu, P., Yin, M. & Deng, C. Applying generative ai with retrieval augmented generation to summarize and\nextract key clinical information from electronic health records. J. Biomed. Informatics 156, 104662 (2024).\n20/34\n"}, {"page": 21, "text": "98. Benfenati, D., De Filippis, G. M., Rinaldi, A. M., Russo, C. & Tommasino, C. A retrieval-augmented generation applica-\ntion for question-answering in nutrigenetics domain. Procedia Comput. Sci. 246, 586–595 (2024). 28th International\nConference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024).\n99. Li, Z. et al. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281\n(2023).\n100. Bora, A. & Cuayáhuitl, H. Systematic analysis of retrieval-augmented generation-based llms for medical chatbot appli-\ncations. Mach. Learn. Knowl. Extr. 6, 2355–2374 (2024).\n101. Chen, X. et al. Eyegpt for patient inquiries and medical education: Development and validation of an ophthalmology\nlarge language model. J. Med. Internet Res. 26, e60063 (2024).\n102. Reimers, N. & Gurevych, I. Sentence-bert: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang,\nJ., Ng, V. & Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 3982–3992 (Association for\nComputational Linguistics, Hong Kong, China, 2019).\n103. Xiong, G. et al. Improving retrieval-augmented generation in medicine with iterative follow-up questions. In Biocom-\nputing 2025: Proceedings of the Pacific Symposium, 199–214 (World Scientific, 2024).\n104. Services, A. W. Titan text embeddings v2 (amazon.titan-embed-text-v2:0). Embedding model via Amazon Bedrock\n(2024). Supports up to 8192 tokens, output dimensions 256/512/1024, multilingual 100+ languages.\n105. Cui, L. et al. Bailicai: A domain-optimized retrieval-augmented generation framework for medical applications. Big\nData Min. Anal. (2025).\n106. Bai, Y. et al. Qwen: Open foundation and instruction models by alibaba cloud. arXiv preprint arXiv:2309.16609 (2023).\n107. Woo, J. J. et al. Custom large language models improve accuracy: comparing retrieval augmented generation and artificial\nintelligence agents to noncustom models for evidence-based medicine. Arthrosc. The J. Arthrosc. & Relat. Surg. 41, 565–\n573 (2025).\n108. Zhang, G. et al. Leveraging long context in retrieval augmented language models for medical question answering. npj\nDigit. Medicine 8, 239 (2025).\n109. Shi, Y. et al. Mkrag: Medical knowledge retrieval augmented generation for medical question answering. In AMIA...\nAnnual Symposium proceedings. AMIA Symposium, vol. 2024, 1011–1020 (2025).\n110. Chiang, W.-L. et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023) 2, 6 (2023).\n111. Yang, Q. et al. Dual retrieving and ranking medical large language model with retrieval augmented generation. Sci.\nReports 15, 18062 (2025).\n112. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C. & Zaharia, M. ColBERTv2: Effective and efficient retrieval\nvia lightweight late interaction. In Carpuat, M., de Marneffe, M.-C. & Meza Ruiz, I. V. (eds.) Proceedings of the 2022\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\n3715–3734 (Association for Computational Linguistics, Seattle, United States, 2022).\n113. Wu, J. et al. Medical graph RAG: Evidence-based medical large language model via graph retrieval-augmented genera-\ntion. In Che, W., Nabende, J., Shutova, E. & Pilehvar, M. T. (eds.) Proceedings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), 28443–28467 (Association for Computational Linguistics, Vi-\nenna, Austria, 2025).\n114. Tayebi Arasteh, S. et al. Radiorag: Online retrieval–augmented generation for radiology question answering. Radiol.\nArtif. Intell. 7 (2025).\n115. Wada, A. et al. Retrieval-augmented generation elevates local llm quality in radiology contrast media consultation. npj\nDigit. Medicine 8, 395 (2025).\n116. Chen, Z. et al. Towards omni-RAG: Comprehensive retrieval-augmented generation for large language models in med-\nical applications. In Che, W., Nabende, J., Shutova, E. & Pilehvar, M. T. (eds.) Proceedings of the 63rd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), 15285–15309 (Association for Computational\nLinguistics, Vienna, Austria, 2025).\n21/34\n"}, {"page": 22, "text": "117. Wang, Z., Khatibi, E. & Rahmani, A. M. Medcot-RAG: Causal chain-of-thought RAG for medical question answering.\nIn IEEE-EMBS International Conference on Body Sensor Networks 2025 (2025).\nAcknowledgements\nThis study is supported by the National Institutes of Health National Library of Medicine under Award Number R01LM014604.\nAuthor Contributions Statement\nH.K. and Q.C. contributed to study design. H.K., H.J., S.P., Y.P., J.P., and S.C. contributed to drafting and refining the\nannotation guidelines. H.K. and J.S. configured the annotation interface. S.A., H.J., S.P., Y.P., J.P., S.C., B.A.H.C., T.H., and\nJ.Y. contributed to data annotation (evidence retrieval and evidence selection). A.G., N.C., R.J., L.C., E.L., A.D., T.G., M.B.S.,\nand Y.K. contributed to data annotation (factuality and completeness). A.G., N.C., S.A., and R.J. contributed to qualitative\nanalysis and case study. H.K., J.S., and E.F.W. contributed to model development. H.K. and Q.C. drafted the manuscript.\nR.A.A., J.Z., A.T., A.C., and H.X. provided critical feedback on the manuscript and study design. All authors read and approved\nthe final version of the manuscript.\nData/Code Availability\nModels and code are available at https://github.com/Yale-BIDS-Chen-Lab/medical-rag, and the full set of expert\nannotations will be released upon publication acceptance.\nCompeting Interests\nThe authors declare no competing interests.\n22/34\n"}, {"page": 23, "text": "Supplementary Information\nSupplementary Table 1. Summary of existing medical RAG frameworks. Frameworks are organized chronologically by\npublication date, summarizing their retriever and LLM configurations, underlying knowledge sources, and downstream\nmedical applications. Only peer-reviewed studies published up to August 2025 were included, focusing specifically on\nmedical and healthcare domains while excluding general biomedical NLP applications. Studies lacking sufficient\nimplementation details were omitted. Each framework utilizes an LLM released after November 2022 (i.e., post-GPT-3.5).\nDescriptions are kept at a high level; although some frameworks draw from similar knowledge sources (e.g., clinical\nguidelines), their specific implementations and scopes vary across studies.\nPublication\nDate\nFramework\nRetriever(s)\nLLM(s)\nKnowledge Sources\nApplication(s)\nJun 2023\nChatDoctor86\nLexical matching\nLlama\nWikipedia, MedlinePlus\nQA\nJul 2023\naccGPT87\nText-embedding-ada-\n00288\nGPT-3.5\nClinical guidelines\nImaging recommendation\nAug 2023\nFraCChat89\nText-embedding-ada-002\nGPT-3.5, GPT-4\nAO/OTA\nFracture\nand\nDis-\nlocation\nClassification\nCom-\npendium (2018)\nAO code identification\nDec 2023\nClinfo.ai90\nBM25, Entrez API91\nGPT-3.5, GPT-4\nPubMed\nQA, Summarization\nJan 2024\nAlmanac29\nText-embedding-ada-002\nGPT-4\nPubMed,\nClinical\nguidelines,\netc.\nQA\nJan 2024\nRALL92\nDPR63, etc.\nGPT-4, Llama-2\nWikipedia, UMLS\nLay language generation\nMay 2024\nFerber et al.93\nText-embedding-ada-002\nGPT-4\nClinical guidelines\nQA\nJun 2024\nRECTIFIER30\nText-embedding-ada-002\nGPT-3.5, GPT-4\nClinical guidelines\nClinical trial screening\nJul 2024\nSelf-BioRAG76\nMedCPT\nLlama-2\nPubMed, PMC, Clinical guide-\nlines, Textbooks\nQA\nJul 2024\nDocOA94\nNot specified\nGPT-3.5\nPubMed, Clinical guidelines\nOsteoarthritis management\nAug 2024\nMedRAG39\nBM25,\nMedCPT,\nCon-\ntriever95, SPECTER96\nGPT-3.5,\nGPT-4,\nLlama-2,\nMixtral-\n8x7B, etc.\nPubMed, StatPearls, Textbooks,\nWikipedia\nQA\nAug 2024\nAlkhalaf et al.97\nBM25\nLlama-2\nEHR\nSummarization\nSep 2024\nChatZOC50\nBM25\nBaichuan\nClinical guidelines, FAQs\nOpthalmology QA\nSep 2024\nBenfenati et al.98\nBM25, GTE99\nGPT-3.5, Mistral\nNutrigenetic\npolymorphism\ndataset,\nPubMed-derived\nnutrition–gene studies\nNutrigenetics QA\nOct 2024\nBora\nand\nCuayáhuitl100\nGTR-T5-Large\nLlama-2, Mistral\nTextbooks, Journals\nQA\nDec 2024\nEyeGPT101\nall-MiniLM-L6-v2102\nLlama-2\nTextbooks, Custom Database\nOphthalmology QA\nJan 2025\ni-MedRAG103\nMedCPT\nGPT-4\nStatPearls, Textbooks\nQA\nJan 2025\nAzimi et al.100\nTitan\nText\nEmbeddings\nV2104\nGPT-4o,\nClaude\n3.5 Sonnet\nRegistered Dietitian (RD) exam\nquestions across four nutrition\ndomains (Academy of Nutrition\nand Dietetics guidelines and ref-\nerences)\nDietetic QA\nFeb 2025\nBailicai105\nMedCPT\nQwen106, Llama-2\nPubMed, StatPearls, Textbooks,\nWikipedia\nQA\nMar 2025\nWoo et al.107\nInternal retriever\nGPT-3.5,\nGPT-4,\nClaude 3, Llama-3,\nMistral-8×7B\nClinical guidelines\nClinical decision support\nApr 2025\nRAG squared40\nMedCPT\nGPT-4o, Llama-3,\netc.\nPubMed, PMC, Clinical guide-\nlines, Textbooks\nQA\nMay 2025\nBriefContext108\nBM25, MedCPT\nGPT-4o,\nLlama-\n3.1\nPubMed, Textbooks\nSummarization\nMay 2025\nMKRAG109\nContriever, etc.\nVicuna-7B110\nDisease Database\nKnowledge graph QA\nMay 2025\nYang et al.111\nElasticsearch,\nCol-\nBERTv2112\nGPT-4, Claude 3\nInternal, expert-reviewed hospi-\ntal documents\nClinical decision support\nJul 2025\nMedGraphRAG113\nGraph-based retrieval\nGPT-4,\nGemini\n1.0 Pro, Llama-2,\nLlama-3\nMIMIC-IV, FakeHealth, Pub-\nHealth, UMLS Graph\nQA\n23/34\n"}, {"page": 24, "text": "Table 1 (continued)\nPublication\nDate\nFramework\nRetriever(s)\nLLM(s)\nKnowledge Sources\nApplication(s)\nJul 2025\nRadioRAG114\nText-embedding-ada-002\nGPT-4o mini\nRadiopaedia\nCase retrieval, Report genera-\ntion\nJul 2025\nWada et al.115\nText-embedding-3-\nlarge88, Lexical matching\nGPT-4o\nClinical guidelines (ACR Man-\nual, ESUR Guidelines, etc.)\nSafety consultation\nJul 2025\nMedOmniKB116\nMedCPT, Graph-based re-\ntrieval\nGPT-4, Gemini 1.5\nPubMed,\nClinical\nguidelines,\nTextbooks, Wikipedia, UMLS,\nDrugBank\nQA\nAug 2025\nMedCoT-RAG117\nMedCPT\nLlama-3.1\nPubMed, StatPearls, Textbooks,\nWikipedia\nQA\n24/34\n"}, {"page": 25, "text": "Supplementary Table 2. Accuracy of RAG configurations using different retrievers with Llama-3.1-8B, with or without\nevidence filtering and query reformulation. Each cell reports accuracy with 95% confidence intervals. The non-RAG\nbaseline is shown at the top. Blue and red shading denote gains and drops from the non-RAG baseline, respectively.\nRAG Components\nBenchmarks\nRetrieval\nFiltering\nQuery Reform.\nMedQA\nMedMCQA\nMMLU\nMMLU-Pro\nMedXpertQA\n\u0017 (Non-RAG)\n\u0017\n\u0017\n0.672\n0.528\n0.738\n0.620\n0.136\n(0.630, 0.712)\n(0.484, 0.572)\n(0.700, 0.776)\n(0.572, 0.668)\n(0.106, 0.166)\nMedCPT\n\u0017\n\u0017\n0.660\n0.578\n0.764\n0.583\n0.179\n(k=16)\n(0.618, 0.702)\n(0.534, 0.620)\n(0.726, 0.800)\n(0.535, 0.634)\n(0.146, 0.214)\n\u0017\n\u0013\n0.720\n0.636\n0.758\n0.583\n0.172\n(0.680, 0.758)\n(0.594, 0.678)\n(0.718, 0.796)\n(0.532, 0.634)\n(0.140, 0.204)\n\u0013\n\u0017\n0.734\n0.604\n0.792\n0.628\n0.186\n(0.696, 0.772)\n(0.562, 0.648)\n(0.756, 0.828)\n(0.580, 0.679)\n(0.152, 0.220)\n\u0013\n\u0013\n0.762\n0.622\n0.786\n0.628\n0.218\n(0.724, 0.800)\n(0.580, 0.664)\n(0.750, 0.822)\n(0.578, 0.676)\n(0.182, 0.254)\nMedCPT\n\u0017\n\u0017\n0.636\n0.620\n0.738\n0.642\n0.162\n(k=32)\n(0.594, 0.678)\n(0.578, 0.662)\n(0.698, 0.776)\n(0.594, 0.690)\n(0.130, 0.194)\n\u0013\n\u0017\n0.702\n0.634\n0.760\n0.602\n0.191\n(0.660, 0.742)\n(0.592, 0.676)\n(0.722, 0.798)\n(0.551, 0.652)\n(0.158, 0.226)\n\u0017\n\u0013\n0.730\n0.630\n0.772\n0.618\n0.186\n(0.690, 0.768)\n(0.586, 0.674)\n(0.734, 0.808)\n(0.570, 0.666)\n(0.152, 0.220)\n\u0013\n\u0013\n0.740\n0.648\n0.800\n0.631\n0.206\n(0.702, 0.778)\n(0.606, 0.690)\n(0.764, 0.834)\n(0.583, 0.679)\n(0.170, 0.242)\nBM25\n\u0017\n\u0017\n0.610\n0.552\n0.704\n0.564\n0.150\n(k=16)\n(0.568, 0.654)\n(0.508, 0.596)\n(0.662, 0.744)\n(0.513, 0.612)\n(0.120, 0.182)\n\u0013\n\u0017\n0.642\n0.574\n0.748\n0.626\n0.188\n(0.598, 0.684)\n(0.530, 0.618)\n(0.710, 0.786)\n(0.578, 0.674)\n(0.154, 0.222)\n\u0017\n\u0013\n0.692\n0.560\n0.765\n0.687\n0.158\n(0.652, 0.732)\n(0.516, 0.604)\n(0.728, 0.802)\n(0.639, 0.733)\n(0.126, 0.190)\n\u0013\n\u0013\n0.716\n0.610\n0.744\n0.671\n0.166\n(0.676, 0.754)\n(0.568, 0.652)\n(0.704, 0.782)\n(0.623, 0.719)\n(0.134, 0.200)\nBM25\n\u0017\n\u0017\n0.648\n0.574\n0.749\n0.593\n0.158\n(k=32)\n(0.606, 0.690)\n(0.532, 0.618)\n(0.712, 0.788)\n(0.543, 0.642)\n(0.126, 0.190)\n\u0013\n\u0017\n0.676\n0.584\n0.752\n0.591\n0.200\n(0.634, 0.716)\n(0.540, 0.628)\n(0.712, 0.790)\n(0.543, 0.639)\n(0.166, 0.236)\n\u0017\n\u0013\n0.718\n0.588\n0.790\n0.653\n0.154\n(0.678, 0.756)\n(0.546, 0.630)\n(0.752, 0.824)\n(0.604, 0.703)\n(0.122, 0.186)\n\u0013\n\u0013\n0.744\n0.616\n0.780\n0.663\n0.174\n(0.706, 0.782)\n(0.574, 0.658)\n(0.742, 0.814)\n(0.615, 0.711)\n(0.142, 0.208)\nQwen3-Emb-0.6B\n\u0017\n\u0017\n0.654\n0.596\n0.738\n0.612\n0.170\n(k=16)\n(0.612, 0.694)\n(0.554, 0.638)\n(0.700, 0.776)\n(0.561, 0.660)\n(0.138, 0.202)\n\u0013\n\u0017\n0.698\n0.620\n0.764\n0.639\n0.182\n(0.658, 0.736)\n(0.576, 0.662)\n(0.726, 0.802)\n(0.588, 0.687)\n(0.148, 0.216)\n\u0017\n\u0013\n0.748\n0.604\n0.760\n0.668\n0.174\n(0.710, 0.786)\n(0.562, 0.646)\n(0.722, 0.798)\n(0.620, 0.717)\n(0.142, 0.206)\n\u0013\n\u0013\n0.734\n0.630\n0.768\n0.663\n0.184\n(0.694, 0.772)\n(0.588, 0.672)\n(0.730, 0.806)\n(0.615, 0.711)\n(0.150, 0.218)\nQwen3-Emb-0.6B\n\u0017\n\u0017\n0.696\n0.588\n0.746\n0.602\n0.184\n(k=32)\n(0.656, 0.736)\n(0.544, 0.630)\n(0.708, 0.784)\n(0.553, 0.652)\n(0.150, 0.218)\n\u0013\n\u0017\n0.702\n0.614\n0.758\n0.660\n0.178\n(0.662, 0.742)\n(0.572, 0.656)\n(0.720, 0.796)\n(0.612, 0.709)\n(0.144, 0.212)\n\u0017\n\u0013\n0.726\n0.600\n0.764\n0.676\n0.196\n(0.686, 0.764)\n(0.556, 0.642)\n(0.726, 0.802)\n(0.628, 0.725)\n(0.162, 0.230)\n\u0013\n\u0013\n0.760\n0.630\n0.750\n0.655\n0.192\n(0.726, 0.796)\n(0.586, 0.672)\n(0.712, 0.788)\n(0.607, 0.703)\n(0.158, 0.226)\n25/34\n"}, {"page": 26, "text": "Supplementary Table 3. Prompt for statement extraction. The {input} placeholder is intended to be replaced with the\nactual explanation or model response during prompting. We used GPT-4o with temperature set to 0 for deterministic output.\nPrompt Template\nPlease breakdown the given text into independent facts. Review the examples provided below to gain a clearer understanding of the task requirements\nand the expected output format.\nInput: He made his acting debut in the film The Moon is the Sun’s Dream (1992), and continued to appear in small and supporting roles throughout\nthe 1990s. Output: [“He made his acting debut in the film.”, “He made his acting debut in The Moon is the Sun’s Dream.”, “The Moon is the Sun’s\nDream is a film.”, “The Moon is the Sun’s Dream was released in 1992.”, “After his acting debut, he appeared in small and supporting roles.”, “After his\nacting debut, he appeared in small and supporting roles throughout the 1990s.”]\nInput: He is also a successful producer and engineer, having worked with a wide variety of artists, including Willie Nelson, Tim McGraw, and Taylor\nSwift. Output: [“He is successful.”, “He is a producer.”, “He is an engineer.”, “He has worked with a wide variety of artists.”, “illie Nelson is an artist.”,\n“He has worked with Willie Nelson.”, “Tim McGraw is an artist.”, “He has worked with Tim McGraw.”, “Taylor Swift is an artist.”, “He has worked with\nTaylor Swift.”]\nInput: Possible causes for right lower abdominal pain in a young female are Appendicitis, Inflammatory bowel disease, Diverticulitis, Kidney stone,\nurinary tract infection, Ovarian cyst or torsion, Ectopic pregnancy, Pelvic inflammatory disease, endometriosis. Output: [“Possible cause for right\nlower abdominal pain in a young female: Appendicitis.”, “Possible cause for right lower abdominal pain in a young female: Inflammatory bowel\ndisease.”, “Possible cause for right lower abdominal pain in a young female: Diverticulitis.”, “Possible cause for right lower abdominal pain in a young\nfemale: Kidney stone.”, “Possible cause for right lower abdominal pain in a young female: urinary tract infection.”, “Possible cause for right lower\nabdominal pain in a young female: Ovarian cyst or torsion.”, “Possible cause for right lower abdominal pain in a young female: Ectopic pregnancy.”,\n“Possible cause for right lower abdominal pain in a young female: Pelvic inflammatory disease.”,“Possible cause for right lower abdominal pain in a\nyoung female: endometriosis.”]\nInput: Hep A IgM refers to a specific type of antibody called Immunoglobulin M (IgM) against the virus hepatitis A. When infected with hepatitis\nA, these antibodies are detectable at symptom onset and remain detectable for approximately three to six months. These antibodies might also be\ndetectable in the first month after hepatitis A vaccination. A negative or non-reactive result means no IgM antibodies against hepatitis A found in\nyour serum, meaning the absence of an acute or recent hepatitis A virus infection. Output: [“Hep A IgM refers to a specific type of antibody called\nImmunoglobulin M (IgM) against the virus hepatitis A.”, “When infected with hepatitis A, these antibodies are detectable at the time of symptom\nonset.”, “When infected with hepatitis A, these antibodies remain detectable for approximately three to six months after infection.”, “These antibodies\nmight also be detectable in the first month after hepatitis A vaccination.”, “The absence of IgM antibodies against hepatitis A in your serum indicates the\nabsence of an acute or recent hepatitis A virus infection.”, “A negative or non-reactive result means that there were no IgM antibodies against hepatitis\nA found in your serum.”]\nInput: methotrexate (Otrexup, Rasuvo, RediTrex) and thalidomide (Contergan, Thalomid) are both considered contraindicated for treatment of\nUC in pregnancy. possible treatment for UC during pregnancy include low-risk drugs such as aminosalicylates (sulfasalazine and mesalamine),\nimmunomodulators (azathioprine, cyclosporine A ,6-mercaptopurine) and corticosteroids. Biological agents such as Infliximabl, Adalimumab,\nVedolizumab and Ustekinumab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. Output: [“Methotrexate\n(Otrexup, Rasuvo, RediTrex) is contraindicated for treatment of ulcerative colitis in pregnancy.”, “Thalidomide (Contergan, Thalomid) is contraindi-\ncated for treatment of ulcerative colitis in pregnancy.”, “Aminosalicylates (sulfasalazine and mesalamine) are considered low-risk drugs for treatment\nof ulcerative colitis during pregnancy.”, “Immunomodulators (azathioprine, cyclosporine A, 6-mercaptopurine) are considered low-risk drugs for\ntreatment of ulcerative colitis during pregnancy.”, “Corticosteroids are considered low-risk drugs for treatment of ulcerative colitis during pregnancy.”,\n“Treatment for ulcerative colitis during pregnancy with biological agents such as Adalimumab is generally avoided during pregnancy as their safety\nin pregnancy is not well established yet.”, “Treatment for ulcerative colitis during pregnancy with biological agents such as Vedolizumab is generally\navoided during pregnancy as their safety in pregnancy is not well established yet.”, “Treatment for ulcerative colitis during pregnancy with biological\nagents such as Infliximab is generally avoided during pregnancy as their safety in pregnancy is not well established yet.”, “Treatment for ulcerative\ncolitis during pregnancy with biological agents such as Ustekinumab is generally avoided during pregnancy as their safety in pregnancy is not well\nestablished yet.”]\n# YOUR TASK\nInput: {input}\nOutput:\n26/34\n"}, {"page": 27, "text": "Supplementary Table 4. Prompt for must-have statement identification. The {query}, {answer}, and {statements}\nplaceholders are intended to be replaced with the actual question, gold answer, and corresponding list of statements during\nprompting. We used GPT-4o with temperature set to 0 for deterministic output.\nPrompt Template\nPlease categorize the provided statements as either “must-have” or “nice-to-have”.\n*Must-have*: essential independent facts required to provide a complete answer to the given query.\n*Nice-to-have*: supplementary or additional information that is useful but not essential.\nReview the examples provided below to gain a clearer understanding of the task requirements and the expected output format.\nQuery: I am a 33 years old female with right lower abdominal pain , what could it be?\nAnswer: Possible causes for right lower abdominal pain in a young female are Appendicitis, Inflammatory bowel disease, Diverticulitis, Kidney stone,\nurinary tract infection, Ovarian cyst or torsion, Ectopic pregnancy, Pelvic inflammatory disease, endometriosis.\nStatement: [“Possible cause for right lower abdominal pain in a young female: Appendicitis.”, “Possible cause for right lower abdominal pain in a young\nfemale: Inflammatory bowel disease.”, “Possible cause for right lower abdominal pain in a young female: Diverticulitis.”, “Possible cause for right lower\nabdominal pain in a young female: Kidney stone.”, “Possible cause for right lower abdominal pain in a young female: urinary tract infection.”, “Possible\ncause for right lower abdominal pain in a young female: Ovarian cyst or torsion.”, “Possible cause for right lower abdominal pain in a young female:\nEctopic pregnancy.”, “Possible cause for right lower abdominal pain in a young female: Pelvic inflammatory disease.”, “Possible cause for right lower\nabdominal pain in a young female: endometriosis.”]\nOutput: [“Must-have”, “Must-have”, “Must-have”, “Must-have”, “Must-have”, “Must-have”, “Must-have”, “Must-have”, “Must-have”]\nQuery: So what does the non reactive mean for the hep a igm\nAnswer: Hep A IgM refers to a specific type of antibody called Immunoglobulin M (IgM) against the virus hepatitis A. When infected with hepatitis\nA, these antibodies are detectable at symptom onset and remain detectable for approximately three to six months. These antibodies might also be\ndetectable in the first month after hepatitis A vaccination. A negative or non-reactive result means no IgM antibodies against hepatitis A found in your\nserum, meaning the absence of an acute or recent hepatitis A virus infection.\nStatements: [“Hep A IgM refers to a specific type of antibody called Immunoglobulin M (IgM) against the virus hepatitis A.”, “When infected with\nhepatitis A, these antibodies are detectable at the time of symptom onset.”, “When infected with hepatitis A, these antibodies remain detectable for\napproximately three to six months after infection.”, “These antibodies might also be detectable in the first month after hepatitis A vaccination.”, “The\nabsence of IgM antibodies against hepatitis A in your serum indicates the absence of an acute or recent hepatitis A virus infection.”, “A negative or\nnon-reactive result means that there were no IgM antibodies against hepatitis A found in your serum.”]\nOutput: [“Must-have”, “Nice-to-have”, “Nice-to-have”, “Nice-to-have”, “Must-have”, “Must-have”]\nQuery: What medications are contraindicated for a pregnant woman with ulcerative colitis?\nAnswer: methotrexate (Otrexup, Rasuvo, RediTrex) and thalidomide (Contergan, Thalomid) are both considered contraindicated for treatment of\nUC in pregnancy. possible treatment for UC during pregnancy include low-risk drugs such as aminosalicylates (sulfasalazine and mesalamine),\nimmunomodulators (azathioprine, cyclosporine A ,6-mercaptopurine) and corticosteroids. Biological agents such as Infliximabl, Adalimumab,\nVedolizumab and Ustekinumab is generally avoided during pregnancy as their safety in pregnancy is not well established yet. Treatment for ulcerative\ncolitis during pregnancy should be tailored by your OBGYN and gastroenterologist.\nStatements: [“Methotrexate (Otrexup, Rasuvo, RediTrex) is contraindicated for treatment of ulcerative colitis in pregnancy.”, “Thalidomide (Conter-\ngan, Thalomid) is contraindicated for treatment of ulcerative colitis in pregnancy.”, “Aminosalicylates (sulfasalazine and mesalamine) are considered\nlow-risk drugs for treatment of ulcerative colitis during pregnancy.”, “Immunomodulators (azathioprine, cyclosporine A, 6-mercaptopurine) are con-\nsidered low-risk drugs for treatment of ulcerative colitis during pregnancy.”, “Corticosteroids are considered low-risk drugs for treatment of ulcerative\ncolitis during pregnancy.”, “Treatment for ulcerative colitis during pregnancy with biological agents such as Adalimumab is generally avoided during\npregnancy as their safety in pregnancy is not well established yet.”, “Treatment for ulcerative colitis during pregnancy with biological agents such as\nVedolizumab is generally avoided during pregnancy as their safety in pregnancy is not well established yet.”, “Treatment for ulcerative colitis during\npregnancy with biological agents such as Infliximab is generally avoided during pregnancy as their safety in pregnancy is not well established yet.”,\n“Treatment for ulcerative colitis during pregnancy with biological agents such as Ustekinumab is generally avoided during pregnancy as their safety in\npregnancy is not well established yet.”, “Treatment for ulcerative colitis during pregnancy should be tailored by your OBGYN and gastroenterologist.”]\nOutput: [“Must-have”, “Must-have”, “Nice-to-have”, “Nice-to-have”, “Nice-to-have”, “Nice-to-have”, “Nice-to-have”, “Nice-to-have”, “Nice-to-have”,\n“Must-have”]\n# YOUR TASK\nRespond only with a list containing either ‘Must-have’ or ‘Nice-to-have’ for each statement. No other responses are required.\nQuery: {query}\nAnswer: {answer}\nStatements: {statements}\nOutput:\n27/34\n"}, {"page": 28, "text": "Supplementary Table 5. Prompt for response generation. Each prompt was constructed by combining a base query type\n(either patient queries or USMLE-style questions) with one or more template components shown in the table. For instance,\nthe complete prompt for the RAG configuration with patient queries consists of (Patient Query) + (RAG) + (Input Query).\nPrompt Type\nTemplate\nPatient Queries\nRespond to the provided clinical inquiry. Your response must be accurate, clear, and include all essential information while omitting\nextraneous details.\nUSMLE-style\nQueries\nAnswer to the provided multiple-choice question about medical knowledge in a step-by-step fashion. Output your explanation and\nsingle option from the given options as the final answer.\nNon-RAG\nAdditionally, distinguish between statements within your response that require authoritative references and those that do not. Here,\na “statement” refers to an atomic or foundational piece of information, which may not necessarily form a complete sentence. For\nstatements requiring references, include citations immediately after the respective statements, with reference numbers enclosed in\nsquare brackets (e.g., [1][2][3]). Citations may also be placed within the sentence, rather than just at the end, when appropriate.\nAt the end of your response, provide a consolidated list of references, formatted according to AMA guidelines. Label the section as\n“### References,” and number the references sequentially (1, 2, 3, etc.), with each reference on a separate line. Every reference\nlisted MUST be appropriately cited within the response using square brackets. Please double-check thoroughly to ensure this\nrequirement is met without exception.\nRAG\nAdditionally, distinguish between statements within your response that require authoritative references and those that do not. Here,\na ”statement” refers to an atomic or foundational piece of information, which may not necessarily form a complete sentence. For\nstatements requiring references, include citations immediately after the respective statements, with reference numbers enclosed in\nsquare brackets (e.g., [1][2][3]). Citations may also be placed within the sentence, rather than just at the end, when appropriate.\nThe provided document snippets are retrieved through a search engine and may be used as references to support your response.\nExternal references not included in the provided materials may also be incorporated. At the end of your response, provide a consoli-\ndated list of references, formatted according to AMA guidelines. Label the section as “### References,” and number the references\nsequentially (1, 2, 3, etc.), with each reference on a separate line. Every reference listed MUST be appropriately cited within the\nresponse using square brackets. Please double-check thoroughly to ensure this requirement is met without exception.\n- Document -\n{passage_1}\nMetadata: {metadata_1}\n- Document -\n{passage_2}\nMetadata: {metadata_2}\n...\n- Document -\n{passage_16}\nMetadata: {metadata_16}\nInput Query\n### Input Query\n{query}\n28/34\n"}, {"page": 29, "text": "Supplementary Table 6. Prompt for model statement filtering. The {question} and {sentence} placeholders are\nintended to be replaced with the actual question or model statement during prompting. We used GPT-4o with temperature\nset to 0 for deterministic output.\nPrompt Template\nYou are given a single sentence. Your task is to decide whether this sentence is distinctive or non-distinctive.\nA sentence is Distinctive only if:\nIt does not simply restate or paraphrase the question stem, and\nIt introduces either:\n(a) clinical reasoning or inference (e.g., ”this pattern suggests a mechanical cause”, ”these findings are consistent with TSC”), or\n(b) definitions or factual information that go beyond what is stated in the question, or\n(c) a final judgment or answer sentence (e.g., ”the most accurate test is ”, ”the answer is ”)\nA sentence is Non-Distinctive if it falls into any of the following:\nIt restates or rewords content already found in the question\nIt presents a raw observation or finding from the question (e.g., age, vitals, exam result)\nIt provides a definition or fact that is already included or implied in the question\nIt is procedural, meta, or instructional (e.g., “Let’s analyze…” or “We need to consider…”)\nYou may refer to the list below when deciding. All of these are non-distinctive sentences:\n- To determine the most strongly associated condition for the patient described, we need to analyze the clinical information provided step-by-step.\n- To determine the correct answer, let’s analyze the given information step-by-step.\n- To determine the most strongly associated condition with the patient’s symptoms, we need to analyze the information provided.\n- To address the query, we will analyze the patient’s behavior and symptoms step-by-step to determine the most appropriate psychological defense\nmechanism being demonstrated.\n- The patient’s behavior will be analyzed to match the most appropriate psychological defense mechanism from the given options.\n- To answer this question, we need to analyze the patient’s behavior and determine which coping mechanism he is exhibiting.\n- To approach this question, let’s analyze the patient’s behavior and the options provided.\n- To determine the most accurate test for the condition described in the question, we need to consider the patient’s symptoms.\n- We need to consider physical examination findings.\n- We need to consider laboratory test results in the context of common clinical scenarios.\n- Step-by-Step Analysis: Patient Presentation.\n- Step-by-Step Analysis: Laboratory Findings.\nQuestion: A 1-year-old girl is brought to a neurologist due to increasing seizure frequency over the past 2 months. She recently underwent a neurology\nevaluation which revealed hypsarrhythmia on electroencephalography (EEG) with a mix of slow waves, multifocal spikes, and asynchrony. Her parents\nhave noticed the patient occasionally stiffens and spreads her arms at home. She was born at 38-weeks gestational age without complications. She has\nno other medical problems. Her medications consist of lamotrigine and valproic acid. Her temperature is 98.3°F (36.8°C), blood pressure is 90/75\nmmHg, pulse is 94/min, and respirations are 22/min. Physical exam reveals innumerable hypopigmented macules on the skin and an irregularly\nshaped, thickened, and elevated plaque on the lower back. Which of the following is most strongly associated with this patient’s condition?\nFor the given question, all of these are non-distinctive sentences (not limited to the followings):\n- The patient is a 1-year-old girl.\n- The patient has a history of increasing seizure frequency.\n- The EEG reveals hypsarrhythmia.\n- The patient has numerous hypopigmented macules.\n- The patient exhibits occasional stiffening and spreading of arms.\n- The patient has an irregularly shaped, thickened, and elevated plaque on the lower back.\n- The patient is currently on lamotrigine.\n- The patient is currently on valproic acid.\n- The patient was born at term without complications.\n- No other medical problems were reported for the patient.\n- And more ...\n# Question\n{question}\n# Target sentence (Do not output anything other than ”Distinctive” or ”Non-distinctive”.)\n{sentence}\n29/34\n"}, {"page": 30, "text": "Supplementary Table 7. Prompt for statement-citation alignment. The {model_response}, {model_statements}, and\n{references} placeholders are intended to be replaced with the actual model response, corresponding list of statements,\nand references during prompting. We used GPT-4o with temperature set to 0 for deterministic output.\nPrompt Template\nYou are given a model-generated response that includes statements with inline citations (e.g., [1], [2], etc.), along with a list of references correspond-\ning to those citations.\nYour task is to identify the alignment between each statement and the reference(s) it is associated with, based solely on the position of the citation\nmarkers in the response.\nInstructions:\n1. For each statement in the response, determine whether it is linked to one or more references by locating citation markers (e.g., [1]) near or within\nthe statement.\n2. For each linked reference, assign it to the corresponding statement.\n3. Do not assess the factual accuracy or content of the reference. Only use the citation markers and their position in the response to make the alignment.\n4. Consider a “statement” to be a single sentence or a semantically independent clause.\n### Input:\n- **Model Response**: {model_response}\n- **Statements**: {model_statements}\n- **References**: {references}\n### Output format: Your output should be organized in JSON, without any other responses, using the following format:\n{ “#1”: { “statement”: “<copy of the statement>”, “refs”: [1, 3] or [] if no references are associated }, “#2”: { “statement”: “...”, “refs”: [...] } }\n### Output:\n30/34\n"}, {"page": 31, "text": "Supplementary Table 8. Profiles of annotators. This table lists all clinicians and medical trainees who contributed to the\nexpert annotation of evidence retrieval, selection, and response evaluation tasks, along with their affiliations and specialties.\nAffiliations are listed as of the project’s start and may have changed since then.\nName\nAffiliation\nSpecialty (if applicable)\nResidents / Fellows\nNicholas Cochran-Caggiano\nYale School of Medicine (EMS Fellowship)\nEmergency Medicine\nLeah Colucci\nYale School of Medicine\nEmergency Medicine\nTuo Guo\nYale School of Medicine\nEmergency Medicine\nRoy Jiang\nYale School of Medicine\nDermatology\nEric Lai\nYale School of Medicine\nOphthalmology\nAmisha Dave\nYale School of Medicine\nOphthalmology\nMaxwell B. Singer\nYale School of Medicine\nOphthalmology\nAidan Gilson\nMassachusetts Eye and Ear, Harvard Medical School\nOphthalmology\nYonghoe Koo\nAsan Medical Center, University of Ulsan College of Medicine\nPhysical Medicine and Rehabilitation\nMD Candidates and Graduates\nSerina Applebaum\nYale School of Medicine\n–\nThomas Huang\nYale School of Medicine\n–\nBrittany Alexandra Herrera Contreras\nYale School of Medicine, San Juan Bautista School of Medicine\n–\nHeeju Jin\nSeoul National University College of Medicine\n–\nSeihee Park\nSeoul National University College of Medicine\n–\nYujin Park\nSeoul National University College of Medicine\n–\nSeoyoung Choi\nSeoul National University College of Medicine\n–\nJiyeong Park\nSeoul National University College of Medicine\n–\nJaehoon Yun\nHanyang University College of Medicine\n–\n31/34\n"}, {"page": 32, "text": "Supplementary Table 9. Examples used in annotator training for support label decisions. These training samples illustrate\nhow annotators were instructed to evaluate whether a given context supports the associated statement.\nField\nContent\nContext\nIndividuals with diabetes are recommended to limit their consumption of sweets to one or two times per week. It is also suggested\nbeing selective with desserts and to focus on foods with a low glycemic index, such as high fiber foods like whole grains and legumes,\nas well as certain lower sugar fruits like berries, melons, and apples.\nStatement\nIt is recommended that diabetics avoid sweets.\nExplanation\nThe context mentions to limit their consumption of sweets, which contrasts with the suggestion to avoid their consumption.\nLabel\nNo Support\nContext\nRight lower abdominal pain in a 25-year-old female could be caused by a variety of medical conditions. Some potential causes\ninclude: Ovarian cyst: a fluid-filled sac on the ovary—Ectopic pregnancy: a pregnancy that occurs outside the uterus.\nStatement\nPossible cause for right lower abdominal pain in a young female can be Appendicitis.\nExplanation\nInsufficient information.\nLabel\nNo Support\nContext\nMites and insects Ivermectin is also used to treat infection with parasitic arthropods. Scabies – infestation with the mite Sarcoptes\nscabiei – is most commonly treated with topical permethrin or oral ivermectin. For most scabies cases, ivermectin is used in a two\ndose regimen: a first dose kills the active mites, but not their eggs. Over the next week, the eggs hatch, and a second dose kills the\nnewly hatched mites. For severe “crusted scabies”, the U.S. Centers for Disease Control and Prevention (CDC) recommends up to\nseven doses of ivermectin over the course of a month, along with a topical antiparasitic. Both head lice and pubic lice can be treated\nwith oral ivermectin, an ivermectin lotion applied directly to the affected area, or various other insecticides. Ivermectin is also used\nto treat rosacea and blepharitis, both of which can be caused or exacerbated by Demodex folliculorum mites.\nStatement\nPermethrin cream (Elimite) is a topical treatment for scabies.\nLabel\nFull Support\nContext\nOverall, the heart rate is decreased while stroke volume is increased, resulting in a net increase in blood pressure, leading to increased\ntissue perfusion. This causes the myocardium to work more efficiently, with optimized hemodynamics and an improved ventricular\nfunction curve. Other electrical effects include a brief initial increase in action potential, followed by a decrease as the K+ conductance\nincreases due to increased intracellular amounts of Ca2+ ions. The refractory period of the atria and ventricles is decreased, while it\nincreases in the sinoatrial and AV nodes. A less negative resting membrane potential is made, leading to increased irritability. The\nconduction velocity increases in the atria, but decreases in the AV node. The effect upon Purkinje fibers and ventricles is negligible.\nAutomaticity is also increased in the atria, AV node, Purkinje fibers, and ventricles.\nStatement\nA calcium channel blocker would not change the relative velocities of conduction in Purkinje fibers, atria, and ventricles.\nExplanation\nThe context compares the pharmacological mechanism of action, action potential, and relative velocity after digoxin administration.\nHowever, it does not provide an explanation of conduction velocity when a calcium channel blocker is administered. While it is\nrelated to the statement in that it discusses conduction velocity after drug administration, it does not mention calcium channel\nblockers. Please be careful not to annotate it as partial support incorrectly.\nLabel\nNo Support\nContext\nJunctional rhythm describes an abnormal heart rhythm resulting from impulses coming from a locus of tissue in the area of the\natrioventricular node, the “junction” between atria and ventricles. Under normal conditions, the heart’s sinoatrial node determines\nthe rate by which the organ beats – in other words, it is the heart’s “pacemaker”. The electrical activity of sinus rhythm originates\nin the sinoatrial node and depolarizes the atria. Current then passes from the atria through the atrioventricular node and into the\nbundle of His, from which it travels along Purkinje fibers to reach and depolarize the ventricles. This sinus rhythm is important\nbecause it ensures that the heart’s atria reliably contract before the ventricles.\nStatement\nConduction through the AV node is slow to allow the ventricles enough time to fill with blood.\nExplanation\nInsufficient information.\nLabel\nNo Support\nContext\nThe Purkinje fibers (; Purkinje tissue or subendocardial branches) are located in the inner ventricular walls of the heart, just beneath\nthe endocardium in a space called the subendocardium. The Purkinje fibers are specialized conducting fibers composed of electrically\nexcitable cells. They are larger than cardiomyocytes with fewer myofibrils and many mitochondria. They conduct cardiac action\npotentials more quickly and efficiently than any other cells in the heart. Purkinje fibers allow the heart’s conduction system to create\nsynchronized contractions of its ventricles, and are essential for maintaining a consistent heart rhythm. Histology Purkinje fibers\nare a unique cardiac end-organ. Further histologic examination reveals that these fibers are split in ventricles walls. The electrical\norigin of atrial Purkinje fibers arrives from the sinoatrial node. Given no aberrant channels, the Purkinje fibers are distinctly shielded\nfrom each other by collagen or the cardiac skeleton.\nStatement 1\nConduction through the Purkinje system is the fastest within the heart.\nLabel\nFull Support\n32/34\n"}, {"page": 33, "text": "Field\nContent\nStatement 2\nThe conduction velocity through the heart in order of speed is Purkinje fibers > atria > ventricles > AV node.\nExplanation\nFrom the statement, “They conduct cardiac action potentials more quickly and efficiently than any other cells in the heart,” it is partially\ninferable that Purkinje fibers > others in terms of conduction speed and efficiency.\nLabel\nPartial Support\nContext\nFunction The AV node receives two inputs from the right atrium: posteriorly, via the crista terminalis, and anteriorly, via the inter-\natrial septum. Contraction of heart muscle cells requires depolarization and repolarization of their cell membranes. Movement of\nions across cell membranes causes these events. The cardiac conduction system (and AV node part of it) coordinates myocyte me-\nchanical activity. A wave of excitation spreads out from the sinoatrial node through the atria along specialized conduction channels.\nThis activates the AV node. The atrioventricular node delays impulses by approximately 0.09s. This delay in the cardiac pulse is\nextremely important: It ensures that the atria have ejected their blood into the ventricles first before the ventricles contract. This\nalso protects the ventricles from excessively fast rate response to atrial arrhythmias.\nStatement\nThe conduction velocity of the structures of the heart are in the following order: Purkinje fibers > atria > ventricles > AV node.\nExplanation\nInsufficient information. Although there is some mention of “delay,” the context does not provide information to compare the\nconduction speeds of the AV node, Purkinje fibers, atria, and ventricles. Therefore, it should be annotated as no support.\nLabel\nNo Support\nContext\nLexapro, also known by its generic name escitalopram, is a selective serotonin reuptake inhibitor (SSRI) commonly prescribed for\nthe treatment of major depressive disorder (MDD) and generalized anxiety disorder (GAD) [1]. It works by increasing the levels\nof serotonin, a neurotransmitter associated with mood regulation, in the brain [2].\nDosage: The typical starting dose for adults with MDD or GAD is 10 mg once daily, which may be increased to a maximum of 20\nmg per day depending on the patient’s response and tolerability [4].\nSide Effects: Common side effects include nausea, insomnia, fatigue, dry mouth, increased sweating, and sexual dysfunction [5].\nSerious side effects can include serotonin syndrome, which is characterized by symptoms such as agitation, hallucinations, and\nrapid heart rate [6].\nContraindications: Lexapro should not be used in patients who are taking monoamine oxidase inhibitors (MAOIs) or have a known\nhypersensitivity to escitalopram or citalopram [7].\nWarnings and Precautions:\nSuicidality: Antidepressants, including Lexapro, may increase the risk of suicidal thoughts and behaviors in children, adolescents,\nand young adults [8].\nSerotonin Syndrome: Risk increases when used in combination with other serotonergic drugs [6].\nQT Prolongation: Lexapro can cause dose-dependent QT interval prolongation, which can lead to serious cardiac arrhythmias [9].\nStatement 1\nEscitalopram is sold under the brand names Lexapro and Cipralex.\nExplanation\nFrom the statement “Lexapro, also known by its generic name escitalopram,” it is clear that Lexapro is the brand name for escitalopram.\nHowever, since there is no mention of Cipralex, it should be annotated as partial support.\nLabel\nPartial Support\nStatement 2\nSide effects of Escitalopram include GI symptoms such as nausea, diarrhoea, constipation.\nExplanation\nThe context lists various side effects, but since there is no mention of diarrhea or constipation, it should be annotated as partial\nsupport.\nLabel\nPartial Support\nStatement 3\nSide effects of Escitalopram include insomnia.\nLabel\nFull Support\nStatement 4\nThe FDA had published a black box warning regarding Escitalopram.\nExplanation\nSince “The FDA had published a black box warning regarding Escitalopram” is not mentioned, Partial Support would be a more appro-\npriate annotation.\nLabel\nPartial Support\nStatement 5\nLexapro is not approved for use in pediatric patients less than 12 years of age.\nExplanation\nThe context states, “Antidepressants, including Lexapro, may increase the risk of suicidal thoughts and behaviors in children.” However,\nthis does not indicate that the drug is not approved for use. Therefore, the appropriate annotation is no support.\nLabel\nNo Support\nContext\nOral ivermectin is an effective option for the treatment of scabies. It is often used in a two-dose regimen, with the first dose killing\nthe active mites and the second dose, administered one week later, targeting the newly hatched mites [1]. Ivermectin is particularly\nuseful for treating crusted scabies and is sometimes prescribed in combination with a topical agent [2]. Although it is not FDA-\napproved for scabies treatment in the United States, it is widely used off-label for this purpose [3]. Studies have shown that oral\nivermectin is effective and generally well-tolerated in children and infants, although its use in children weighing less than 15 kg is\noff-label [4, 5].\n33/34\n"}, {"page": 34, "text": "Field\nContent\nStatement\nIvermectin (Stromectol) is not safe for children under 15kg.\nExplanation\nOff-label use cannot always be considered unsafe. Therefore, this statement is not necessarily supported.\nLabel\nNo Support\nSupplementary Table 10. Prompt for evidence filtering.\nPrompt Template\nGiven a query and a text passage, determine whether the passage contains supporting evidence for the query. Supporting evidence means that the\npassage provides clear, relevant, and factual information that directly backs or justifies the answer to the query.\nRespond with one of the following labels:\n“Yes” if the passage contains supporting evidence for the query.\n“No” if the passage does not contain supporting evidence.\nYou should respond with only the label (Yes or No) without any additional explanation.\nQuestion: {question}\nPassage: {passage}\n34/34\n"}]}