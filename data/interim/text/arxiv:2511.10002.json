{"doc_id": "arxiv:2511.10002", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.10002.pdf", "meta": {"doc_id": "arxiv:2511.10002", "source": "arxiv", "arxiv_id": "2511.10002", "title": "PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models", "authors": ["Shivam Sharma", "Riya Naik", "Tejas Gawas", "Heramb Patil", "Kunal Korgaonkar"], "published": "2025-11-13T06:12:12Z", "updated": "2025-11-14T07:47:21Z", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like content. This has revolutionized various sectors such as healthcare, software development, and education. In education, LLMs offer potential for personalized and interactive learning experiences, especially in regions with limited teaching resources. However, adapting these models effectively to curriculum-specific content, such as the National Council of Educational Research and Training (NCERT) syllabus in India, presents unique challenges in terms of accuracy, alignment, and pedagogical relevance. In this paper, we present the framework \"PustakAI\"\\footnote{Pustak means `book' in many Indian languages.} for the design and evaluation of a novel question-answering dataset \"NCERT-QA\" aligned with the NCERT curriculum for English and Science subjects of grades 6 to 8. We classify the curated QA pairs as Factoid, Inferential, and Others (evaluative and reasoning). We evaluate the dataset with various prompting techniques, such as meta-prompt, few-shot, and CoT-style prompting, using diverse evaluation metrics to understand which approach aligns more efficiently with the structure and demands of the curriculum. Along with the usability of the dataset, we analyze the strengths and limitations of current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B) as AI-based learning tools in formal education systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.10002v2", "url_pdf": "https://arxiv.org/pdf/2511.10002.pdf", "meta_path": "data/raw/arxiv/meta/2511.10002.json", "sha256": "4f0a6c795b9b2b96f9fa3cd2ca1cc55e97a61f384c0b56a01bfb2f62849e4db7", "status": "ok", "fetched_at": "2026-02-18T02:27:13.834440+00:00"}, "pages": [{"page": 1, "text": "Pustak AI: Curriculum-Aligned and Interactive\nTextbooks Using Large Language Models\nShivam Sharma, Riya Naik, Tejas Gawas, Heramb Patil, and Kunal Korgaonkar\nDepartment of Computer Science and Information Systems,\nBITS Pilani K K Birla Goa Campus, Goa, India\nEmail: kunalk@goa.bits-pilani.ac.in\nAbstract. Large Language Models (LLMs) have demonstrated remark-\nable capabilities in understanding and generating human-like content.\nThis has revolutionized various sectors such as healthcare, software de-\nvelopment, and education. In education, LLMs offer potential for per-\nsonalized and interactive learning experiences, especially in regions with\nlimited teaching resources. However, adapting these models effectively to\ncurriculum-specific content, such as the National Council of Educational\nResearch and Training (NCERT) syllabus in India, presents unique chal-\nlenges in terms of accuracy, alignment, and pedagogical relevance. In this\npaper, we present the framework \"PustakAI\"1 for the design and evalu-\nation of a novel question-answering dataset \"NCERT-QA\" aligned with\nthe NCERT curriculum for English and Science subjects of grades 6 to\n8. We classify the curated QA pairs as Factoid, Inferential, and Others\n(evaluative and reasoning). We evaluate the dataset with various prompt-\ning techniques, such as meta-prompt, few-shot, and CoT-style prompt-\ning, using diverse evaluation metrics to understand which approach aligns\nmore efficiently with the structure and demands of the curriculum. Along\nwith the usability of the dataset, we analyze the strengths and limitations\nof current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-\nmini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B)\nas AI-based learning tools in formal education systems.\nKeywords: Large Language Model · QA Systems · Educational AI.\n1\nIntroduction\nThe idea of a machine that could answer questions, write essays, or translate nat-\nural language like humans seemed fiction. But with the advent of Large Language\nModels (LLMs), that vision has become reality. LLMs built on a vast amount\nof data learn using transformer architectures. As they evolved, LLMs have been\nused to assist in various tasks such as composing emails, writing code, scientific\ndiscovery, and tutoring. It is the last category that is of interest to us. Trained\non a diverse range of subjects and answering styles, LLMs can assist students\n1 Pustak means ‘book’ in many Indian languages.\narXiv:2511.10002v2  [cs.CL]  14 Nov 2025\n"}, {"page": 2, "text": "2\nAuthors Suppressed Due to Excessive Length\nby answering questions, explaining complex concepts, and even offering feed-\nback on their submissions. On the other hand, for educators, it can help create\nlesson plans, automate learning tasks such as creating balanced questionnaires,\nand also explain concepts with additional research. LLMs have the potential\nto benefit education by extending learning beyond standard teaching-learning\nand help bridge the educational gap. To facilitate this, increasing integration\nof language models into educational contexts has prompted a wave of research\nexploring their capabilities, limitations, and impact.\nResearchers and developers are utilizing LLMs to create interactive learn-\ning platforms that can adapt to student needs. These models have been used\nto generate practice questions, summarize complex topics, provide coding help,\nand translate languages, thereby supporting diverse learners across disciplines.\nHowever, the challenge of efficiently training LLMs for educational purposes re-\nmains. This is largely due to the quality of training data and the inference meth-\nods employed. Recent progress in dataset development has focused on general\neducational content, but for practical use in institutional settings, the data must\nbe tailored to specific curriculam. Therefore, it is equally important to improve\nLLMs for educational question answering by introducing more effective inference\nmethods and creating high-quality, curriculum-aligned datasets. This will enable\nthe fine-tuning of both LLMs and traditional language models, leading to more\naccurate and contextually relevant educational responses.\nOur framework PustakAI aims to contribute as follows:\n– Present the NCERT-QA dataset to implement a curriculum-aligned Q&A\nsystem. This study validates the NCERT-QA dataset as a foundation for\nbuilding a curriculum-aligned Q&A system. We develop a QA dataset de-\nrived directly from curriculum content and conduct a comprehensive evalu-\nation of its effectiveness through various inference prompting techniques.\n– Demonstrate the unique challenge posed by our curriculum-specific dataset\nby baselining it against a general-domain benchmark such as SQuAD.\n– Perform an in-depth analysis to identify the optimal models and prompting\nstrategies for the NCERT-QA task, including a comparison between subjects\n– Analyze the practical trade-offs between performance and efficiency to make\na case for cost-effective deployment in real-world school settings.\nThe rest of the paper is organized as follows. Section 2 summarizes existing ed-\nucational datasets, Sections 3 and 4 elaborate the dataset curation steps and\ndataset analysis, Section 5 describes the implementation of the pipeline to eval-\nuate the dataset and inference strategies, and Section 6 summarizes the exper-\nimental evaluation and results. Finally, Section 7 summarizes the conclusions\ndrawn.\n2\nBackground and Related Work\nThe application of LLMs to question answering has evolved from general bench-\nmarks like SQuAD [8] to high-stakes domains like education, where the risks of\n"}, {"page": 3, "text": "PustakAI\n3\n\"hallucination\" demand high factual accuracy and pedagogical alignment. This\nhas driven research in two key areas: the creation of curriculum-aligned datasets\nand the development of methodologies like advanced prompting to ensure model\noutputs are faithful to reliable sources.\nEarly educational datasets focused on broad reasoning, such as ARC for sci-\nence [4] and FairytaleQA for narrative comprehension [16]. A move toward direct\ncurriculum alignment was marked by datasets like RACE, sourced from student\nexaminations [8]. This trend has intensified with resources tightly coupled to spe-\ncific textbook content, such as CK12-QA for science [1] and PeerQA for scientific\nreviews [2]. Concurrently, the focus on evaluation has sharpened, with bench-\nmarks like SyllabusQA introducing fact-checking metrics [6] and TruthfulQA\ntesting models against common falsehoods [11]. Despite this progress, a signif-\nicant gap remains for a large-scale dataset aligned with a major non-Western\ncurriculum like India’s NCERT, a gap our work aims to fill. A comparative\nanalysis of these datasets is provided in (Table 1).\nAligning LLMs to be pedagogically sound is a key challenge, as general-\npurpose models are not inherently suited for the classroom. Frameworks like\nCOGENT demonstrate how to generate grade-appropriate content by providing\nstructured guidance on learning objectives and readability [12]. This has also\nprompted architectural debates, contrasting large unified models with more effi-\ncient Mixture-of-Experts (MoE) architectures tailored to specific curricula [13].\nOur evaluation of a wide spectrum of models contributes directly to this investi-\ngation, providing empirical data on the performance-cost trade-offs for deploying\npractical AI tools in school systems.\nIn education, ensuring faithfulness (grounding answers in trusted sources)\nis non-negotiable. The standard approach combines Retrieval-Augmented Gen-\neration (RAG) for its architecture [9] with advanced prompting to control the\nmodel’s reasoning process. While Chain-of-Thought (CoT) was an early break-\nthrough for eliciting reasoning [7], the efficacy of popular frameworks like ReAct\nhas been challenged. A recent critical evaluation found that ReAct’s performance\ngains stem from exemplar similarity rather than genuine reasoning, revealing a\nfailure to generalize [3]. This critique highlights the need for robust alterna-\ntives like meta-prompting, which uses high-level, structural guidance to enforce\na faithful reasoning process [17]. Our work directly investigates if this structural\napproach is more effective than the content-based guidance of few-shot or CoT\nprompts in a curriculum-aligned context.\n3\nDataset Curation\nOur NCERT-QA data curation process includes three major steps: collection of\nNCERT text documents, data extraction, and answer mapping. Each of the steps\nis detailed below and is visualized in Fig. 1(a). Our objective is to collect high-\nquality, authentic data. To achieve this, we gathered 35 documents (chapters)\nfrom the English curriculum and 48 documents from the Science curriculum for\nclasses 6 to 8. Each document’s textual content was meticulously extracted from\n"}, {"page": 4, "text": "4\nAuthors Suppressed Due to Excessive Length\nTable 1: Comparative Analysis of Key Question-Answering Datasets.\nDataset\nFocus\nTarget\nAge/Grade\nSubjects\nLang.\nCurriculum\nAlignment\nKey Features / Relevance\nSQuAD\nExtractive\nQA\nGeneral Adult General\n(Wikipedia)\nEnglish\nNone\nFoundational extractive QA bench-\nmark.\nRACE\nReading\nComprehen-\nsion\nGrades\n7-12\n(Ages 12-18)\nEnglish\nEnglish\nHigh\n(Chinese\nExaminations)\nPrecedent\nfor\ncurriculum-aligned\nQA from exams.\nARC\nScience\nRea-\nsoning\nGrades 3-9\nScience\nEnglish\nLoose\n(Grade-\nlevel science)\nBenchmark for complex science rea-\nsoning.\nSciQ\nScience QA\nGeneral\nScience\n(Physics,\nChem, Bio)\nEnglish\nLoose\n(General\nscience topics)\nProvides supporting evidence text\nwith questions.\nTruthfulQA\nFactual Faith-\nfulness\nGeneral Adult General\n(38\ncategories)\nEnglish\nN/A\nMeasures model’s ability to avoid\ncommon falsehoods.\nFairytaleQA\nNarrative\nComprehen-\nsion\nGrades K-8\nReading/ Sto-\nries\nEnglish\nNone\nExpert-generated\nquestions\nfor\nyounger students.\nCK12-QA\nMultimodal\nTextbook QA\nMiddle School Science\nEnglish\nHigh\n(CK-12\nTextbooks)\nDirect parallel for RAG on science\ntextbooks.\nSyllabusQA\nCourse Logis-\ntics QA\nUniversity\nGeneral\n(36\nmajors)\nEnglish\nHigh (University\nSyllabi)\nIntroduces Fact-QA metric for fac-\ntual accuracy.\nPeerQA\nScientific\nDocument\nQA\nGraduate+\nSTEM/NLP\nEnglish\nN/A\n(Scientific\nPapers)\nExpert-generated\nquestions\nfrom\nauthentic sources.\nNCERT-QA Curriculum-\naligned QA\nGrades 6-8\nScience, En-\nglish\nEnglish High\n(Indian\nNCERT)\nAddresses the gap for a major\nnon-Western curriculum.\nPDFs, with all images, captions, and tables removed to ensure the purity of the\ntext. Our overall refined corpus comprised a total of 83 documents.\n3.1\nData Extraction\nIn each chapter, we systematically extracted the chapter text as context and\nthe questions provided in the exercises of the respective chapter. Specifically,\na total of 451 questions were extracted for English documents, while 288 ques-\ntions were obtained from the science documents, bringing the dataset to a to-\ntal of 739 question-answer pairs. These extracted questions were subsequently\ncategorized into three distinct types: Factoid, Inferential, and a third category\ndenoted as Others. The Factoid category consists of questions whose answers\ncan be directly extracted from the passage, identified as specific spans of text.\nThe Inferential category includes questions that necessitate logical reasoning\nand inferential thinking to develop a comprehensive response based on the pas-\nsage content. The Other category is distinct from the first two; it encompasses\nquestions that are boolean in nature or require answers extracted from multiple\nparagraphs, among other traits. Examples for each category are illustrated in\nTable 2(a)\n3.2\nAnswer Mapping\nIn the final phase of dataset curation, we execute the process of correlating\nanswers to the questions extracted from the exercises. We utilize the solutions\n"}, {"page": 5, "text": "PustakAI\n5\n(a)\n(b)\nFig. 1: (a) NCERT-QA dataset curation process. NCERT textbooks are parsed\nto extract chapters as context and the respective questions. Answers are retrieved\nfrom various authentic public online sources and aligned based on chapter and\nquestion indices. These answers are used as ground truth. The resulting QA\ndataset is structured as a collection of context-question-answer tuples. (b) LLM\nprompting and evaluation pipeline. LLM is presented with chapter and its cor-\nresponding question by employing a variety of prompting strategies. Model then\ngenerates a response to the question which is compared against ground truth\nusing various evaluation matrices.\nTable 2: (a) Examples of different question categories with corresponding an-\nswers; (b) Distribution of Question Types in the NCERT-QA Dataset.\nCategory Question\nAnswer\nFactoid\nWhat\ndid\nPatrick\nthink\nhis\ncat\nwas\nplaying\nwith?\nWhat\nwas\nit\nre-\nally?\nPatrick\nthought\nhis cat was playing\nwith a doll, but it\nwas actually a tiny\nman.\nInferential Why did the little\nman grant Patrick\na wish?\nBecause\nPatrick\nsaved\nhim\nfrom\nthe cat and the elf\nwanted\nto\nreturn\nthe favor.\nOthers\nIn\nwhat\nway\ndid\nthe\nshopkeeper\nmake\na\nfool\nof\nRasheed?\nThe\nshopkeeper\npretended Rasheed\ncould\nwin\nprizes,\nbut\ntricked\nhim\nwith cheap goods.\nQuestion Category Count Percentage\nFactoid\n405\n55%\nInferential\n258\n35%\nOther\n76\n10%\nTotal\n739\n100%\n"}, {"page": 6, "text": "6\nAuthors Suppressed Due to Excessive Length\nto each specified question sourced from authentic public online sources. The\nmapping of questions is achieved by utilizing the chapter index and question\nnumber as reference points. Subsequently, we manually assess the accuracy and\nappropriateness of answer mappings to ensure their correctness.\n4\nExploratory Dataset Analysis\nIn this section, we conduct an in-depth analysis of the NCERT-QA dataset\nto better understand its defining characteristics and underlying structure. Our\nprimary focus lies in examining the diversity of the data and the formulation\nof question-answer pairs. This analysis is crucial for evaluating the dataset’s\nsuitability for building robust educational QA systems and for highlighting how\nits features can be used to test different model capabilities.\n4.1\nData Diversity\nThe NCERT-QA dataset is intentionally diverse, covering two distinct subjects\nEnglish and Science across three consecutive grade levels (6, 7, and 8). This\nsubject diversity introduces a variety of text complexities and styles. The En-\nglish texts are primarily narrative and literary, requiring comprehension of plot,\ncharacter, and thematic elements. In contrast, the Science texts are descrip-\ntive and explanatory, demanding an understanding of concepts, processes, and\nfactual information. This is evident from the Q&A length distribution shown\nin Fig 2. The data indicate that Science subject documents tend to feature\nmore elaborate questions and answers compared to those in English. Most En-\nglish questions fall within the 5–10 word range, while Science questions typically\nrange from 10–20 words, reflecting their more conceptually driven nature. Simi-\nlarly, the answer length distribution highlights the explanatory style of Science\nresponses, with answer lengths commonly falling between 10-40 words. This di-\nchotomy provides a comprehensive testbed for evaluating an LLM’s adaptability\nto different linguistic domains and reasoning types within a single, coherent ed-\nucational framework. Although the two subjects differ in the structure and flow\nof their texts, they also exhibit notable similarities. As shown in Fig 2, both\nEnglish and Science texts frequently reference entities such as PERSON, CAR-\nDINAL, DATE, and ORG. This suggests that while English and Science differ\nin textual structure and cognitive demands, they converge in their reference to\ncertain key real-world entities which may aid in the development of cross-domain\nentity recognition and information extraction capabilities in language models.\n4.2\nQuestion-Answer Formulation\nThe formulation of the question-answer pairs is designed to mirror real-world\neducational exercises and assess a range of cognitive skills. As introduced in\nSection 3, questions are categorized as Factoid, Inferential, and Other. A quan-\ntitative breakdown of the 739 questions in our dataset reveals the distribution\n"}, {"page": 7, "text": "PustakAI\n7\nFig. 2:\nSubject-wise\nanalysis\nof\nthe\nNCERT-QA\ndataset\nshowing\nques-\ntion&answer length distributions and top named entities, highlighting differences\nin complexity and focus between English and Science.\nshown in Table 2(b). The prevalence of Factoid questions (55%) ensures a solid\nbaseline for testing a model’s core reading comprehension and information re-\ntrieval abilities. A significant portion of Inferential questions (35%) is critical\nfor evaluating deeper reasoning, requiring models to connect ideas and make\nlogical deductions that are not explicitly stated in the text. The Other category\n(10%) includes more complex questions that often require synthesizing infor-\nmation from multiple paragraphs, providing a challenge for advanced reasoning.\nThis balanced distribution ensures that our evaluation rigorously tests models on\na spectrum of tasks, from simple lookup to complex synthesis, which is essential\nfor a versatile educational assistant.\n5\nMethodology\nIn this study, we establish a pipeline founded on LLMs to exhibit the uti-\nlization of the NCERT-QA dataset. A comparative analysis is conducted on\nthe performance of various LLMs, which include (gemma3:1b, llama3.2:3b, and\nnemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B-16E-Instruct and\ndeepseek-r1-distill-llama-70b) with the objective of evaluating the enhancement\nin performance facilitated by this dataset, as shown in the Fig 1(b).\nEach input to the LLM consists of a question paired with its corresponding\ncontext. We wrap this combination into a prompt to guide the model through\na reasoning process. Prompt is stitched in a way that the model first deter-\nmines the type of question—whether it is factoid, inferential, or others. Based\non the identified question type, the model is then expected to follow appropriate\nreasoning steps using the given context to arrive at an accurate answer.\nAfter the LLM generates an answer, we evaluate the answer against the\nground truth answers provided in the NCERT-QA dataset. We employ a range\nof evaluation metrics to assess the quality of the responses. These metrics go\n"}, {"page": 8, "text": "8\nAuthors Suppressed Due to Excessive Length\nbeyond simple word overlap measures like ROUGE-L[10] to contextual relevance\nusing BERTScore-f1[18].\nWe extend the comparison by evaluating the answer quality in terms of faithful-\nness and semantic relevance. We calculate semantic relevance as cosine similarity\nbetween sentence embeddings using SentenceTransformer models[14]:\nFaithfulness = |tokensanswer ∩tokenscontext|\n|tokensanswer|\nSemanticSim =\neref · egen\n∥eref∥∥egen∥\nSemanticSim (SS) measures how well the generated answer aligns with the\nintended meaning of the reference answer, regardless of exact word matches.\nUnlike traditional lexical metrics such as ROUGE, semantic similarity evaluates\nconceptual coherence between responses using cosine similarity between sentence\nembeddings.\nFaithfulness (FF) measures the extent to which generated answers remain\ngrounded in the provided source context, critical for educational applications\nwhere accuracy is paramount. This multilayered evaluation ensures a robust\nassessment of the LLM’s ability to understand, reason, and respond accurately\nto NCERT-QA.\n5.1\nPrompt Strategies\nWe implement the framework detailed in 5 using diverse state-of-the-art prompt-\ning strategies [15, 5]. This implementation is designed to evaluate the underlying\ninference process utilizing our dataset.\nThe prompting strategies used are as follows:\n1. Shot-based: We provide one example of each question category for the LLM\nto understand the answering pattern before asking the model to perform an-\nswer retrieval. This helps guide the model by showing the format and logic\nneeded to come up with a response, without overloading it with lots of in-\nterleaved instructions. We extend this prompt to 3 and 5 shots by increasing\nthe number of examples such that LLM can learn the diverse spectrum of\neach category.\n2. CoT: In this strategy, we provide a method that encourages the model to\nreason step by step rather than jumping straight to the answer. We uti-\nlize this method to solve complex problems that require logical reasoning,\nintermediate steps, or multi-hop reasoning within the context.\n3. Meta Prompt (MP): Our approach involves employing the large language\nmodel, Claude-Sonnet, to construct a prompt or template that is used in our\nsubsequent NCERT question and answer task. The goal of incorporating a\nmeta_prompt is to evaluate whether Instructions by a meta-language model\ncan enhance generating responses from the answering LLM, as opposed to\nrelying solely on human-devised instructions.\n4. Meta One-shot (MP-1S): In this work, we utilize a dual approach by inte-\ngrating a meta prompt with one illustrative example of question types. To\n"}, {"page": 9, "text": "PustakAI\n9\nconstruct the prompt, we begin by drawing upon instructions sourced from\na meta-level language model and subsequently embed an instance for each\ncategory of question within the prompt as examples.\n6\nResults and Analysis\nOur experimental results are presented in three parts. We first establish the\nunique value of the NCERT-QA dataset, then analyze model and prompt perfor-\nmance on our task, and finally discuss the practical implications for deployment.\nPart 1: The Unique Challenge of Curriculum-Aligned QA: To demon-\nstrate that answering curriculum-specific questions is a distinct challenge that\ngeneral-purpose models cannot solve from pre-trained knowledge alone, we con-\nducted a baseline experiment. We evaluated Llama4-Scout-17B on the well-\nknown SQuAD dataset and on our NCERT-QA dataset without providing the\ntextbook context. This setup forces the model to rely solely on its internal knowl-\nedge.\nThe results in Table 3 are unequivocal. The model performs exceptionally well\non SQuAD, a general-knowledge benchmark and possibly used for LLM training,\nbut its performance collapses on NCERT-QA questions when deprived of the\ntextbook context. The F1 score plummets, indicating an inability to generate\nprecise answers. This is because NCERT questions are deeply tied to the specific\nphrasing, narratives, and vocabulary of the curriculum, which is not adequately\nrepresented in the model’s general training data. This experiment confirms our\ncore hypothesis: a specialized dataset and prompting with the right context are\nnot just helpful but essential for building a reliable educational assistant.\nTable 3: Baseline performance of Llama-4-Scout-17B on SQuAD vs. NCERT-\nQA (no context), showing the performance drop and the need for a curriculum-\nspecific, context-aware approach.\nMetric\nSQuAD NCERT QA (Eng) NCERT QA (Sci) NCERT-QA (Overall)\nF1\n0.894\n0.32\n0.47\n0.395\nSemantic Sim.\n0.920\n0.78\n0.88\n0.830\nWe further assessed the models using each prompt type with and without\nincorporating contextual information on NCERT-QA. As shown in Tables 4a\nand 4b, there is a noticeable improvement in metric scores when context is\nincluded during retrieval. This highlights the critical role of external knowledge\nin enhancing model performance and underscores the significance of our proposed\ndataset.\nPart 2: In-depth Analysis of Model and Prompt Performance: Hav-\ning established the need for our approach of prompting with the right context,\nwe now analyze the performance of various models and prompting strategies on\n"}, {"page": 10, "text": "10\nAuthors Suppressed Due to Excessive Length\nthe NCERT-QA dataset with the full context provided. As shown in the Ta-\nble 4b, there is a clear correlation between model size and performance. The\nlarger models, Llama4-Scout-17B and DeepSeek-70B, significantly outperform\ntheir smaller, open-source counterparts. Between the two models, Llama4-Scout-\n17B shows higher performance, and among the small open-sourced counterparts,\nLlama3.2-3B leads largely in F1 and faithfulness.\nThe choice of prompting strategy also had a profound impact. The meta\noneshot prompt emerged as the most consistently effective strategy, delivering\nthe best F1 scores for most models (See Table 4b) in both English and Science\nsubsets. This suggests that a high-quality, machine-generated instruction com-\nbined with a single, clear example offers an optimal balance of guidance. Con-\nversely, the Chain-of-Thought strategy yielded surprisingly poor results, partic-\nularly on the Faithfulness metric across models(e.g., 0.532 for Llama4-Scout; See\nAppendix A). This indicates that encouraging detailed step-by-step reasoning for\nthis task caused the models to \"hallucinate\" details beyond the provided con-\ntext. Comparing performance across subjects, models consistently scored slightly\nhigher on the Science dataset than the English dataset. For instance, Llama4-\nScout achieved a faithfulness score of 0.87 on Science, while its performance on\nEnglish was closer to 0.85. This suggests that the factual, descriptive nature of\nthe science texts is more amenable to prompting with context than the inferential\nand narrative complexities of the English literary texts.\nTable 4: Comparison of Small (S) and Large (L) models: (a) Performance without\ncontextual data; (b) Best model and prompt type on English and Science with\ncontext.\n(a)\nNCERT Model\nF1\nB-F1 R-L\nSS\nEnglish\nLlama4 (L)\n0.32\n0.81\n0.27 0.78\nDS (L)\n0.13\n0.78\n0.07 0.77\nGemma3 (S)\n0.26\n0.79\n0.22 0.77\nNemo (S)\n0.24\n0.79\n0.19 0.77\nLlama3.2 (S) 0.18\n0.76\n0.14 0.70\nScience\nLlama4 (L)\n0.47\n0.81\n0.41 0.88\nDS (L)\n0.46\n0.88\n0.40 0.89\nGemma3 (S)\n0.27\n0.81\n0.23 0.79\nNemo (S)\n0.25\n0.89\n0.20 0.79\nLlama3.2 (S) 0.19\n0.78\n0.15 0.72\n(b)\nNCERT Model\nPrompt\nF1\nB-F1 R-L\nSS\nFF\nEnglish\nLlama4 (L)\nMP-1S\n0.46\n0.86\n0.40 0.86 0.85\nDS (L)\nMP\n0.45\n0.86\n0.39 0.87 0.79\nLlama3.2 (S) MP-1S\n0.40\n0.84\n0.35 0.83 0.81\nNemo (S)\nMP\n0.36\n0.83\n0.28 0.82 0.76\nGemma3 (S)\nMP-1S\n0.35\n0.83\n0.31 0.81 0.80\nScience\nLlama4 (L)\nMP-1S\n0.47\n0.88\n0.41 0.88 0.87\nDS (L)\nMP-1S\n0.46\n0.87\n0.40 0.89 0.81\nLlama3.2 (S) MP-1S\n0.41\n0.86\n0.35 0.85 0.83\nNemo (S)\nMP-1S\n0.37\n0.85\n0.29 0.84 0.78\nGemma3 (S)\nMP-1S\n0.36\n0.85\n0.32 0.83 0.82\nPart 3: Practical Implications for Real-World Deployment While larger\nmodels deliver higher performance, a practical educational tool must also be\nefficient and cost-effective. In this section, we analyze the trade-off between per-\nformance and inference speed to identify the most viable model for deployment\nin a school setting. Fig: 3 (a) and (b) compares our two top-performing models.\nWhile DeepSeek-70B holds a slight edge in semantic similarity, Llama4-Scout-\n17B achieves a better F1 score and is significantly more faithful, all while being\nover 6 times faster. An average inference time of 2 seconds is well within the\n"}, {"page": 11, "text": "PustakAI\n11\nacceptable range for a real-time interactive assistant, whereas a 13 second wait\nis likely too slow for an engaging student experience. This analysis demonstrates\nthat Llama4-Scout-17B is not just a compromise but arguably the superior choice\nfor this application. It delivers state-of-the-art results on the metrics that mat-\nter most (accuracy and faithfulness) with the efficiency required for practical,\ncost-effective deployment in schools, where computational resources may be lim-\nited. Additionally as can be seen from Fig. 3 (c), among the smaller open-source\nmodels, Gemma3-1B and Llama3.2-3B, exhibit lower latency, with Llama3.2-3B\noffering a more optimal trade-off between latency and overall performance.\nFig. 3: Performance–efficiency trade-off: (a,b) Llama-4-Scout matches DeepSeek-\n70B performance at far lower inference time. (c) Average runtime per prompt\ntype shows meta and meta-one-shot with reduced latency; Llama models and\nGemma3-1B are fastest, DeepSeek-70B slowest.\n7\nConclusion\nIn this study, we introduce the NCERT-QA dataset to bridge the gap between\ncurriculum content and educational Q&A systems. Our categorization of ques-\ntion types highlights the diversity present in the dataset, ensuring a comprehen-\nsive representation of curriculum-based queries. This work marks an initial step\ntoward expanding the dataset across additional grades and subjects. Through\nextensive evaluations across multiple models and prompting strategies, we em-\nphasize the vital role of high-quality, curriculum-aligned data in enhancing the\naccuracy and relevance of responses. Our findings demonstrate that incorporat-\ning contextual knowledge significantly improves model performance, reinforcing\nthe importance of structured retrieval and carefully curated datasets such as\nNCERT-QA. Our analysis of models of varying scales demonstrates the potential\nof smaller open-source models for practical deployment in resource-constrained\nenvironments. Our dataset and pipeline, PustakAI, establishes a foundation for\nbuilding more robust and scalable educational AI systems that are closely aligned\n"}, {"page": 12, "text": "12\nAuthors Suppressed Due to Excessive Length\nwith academic curriculum. More detailed observations and exhaustive analysis\nwill be provided on the ArXiv version of this paper.\nAcknowledgments. We express our gratitude towards Bebras Challenge for making\nthe practice question available in the public domain for practice. We acknowledge BITS\nPilani’s OPERA and New Faculty Seed Grant and the CSIS department’s infrastruc-\ntural support. ACM for bringing this challenge to India and promoting it.\nA\nAppendix: Comparison of Model Performance Under\nContextual and Non-Contextual Prompts\nComparative performance of models visualized across all prompting strategies.\nFig. 4: Performance of models on contextual vs. non-contextual prompts. Faith-\nfulness metric is absent for non-contextual prompts since faithfulness measures\nthe match between the provided context and the LLM’s generated answer, which\ncannot be computed without context.\n"}, {"page": 13, "text": "PustakAI\n13\nReferences\n1. Alawwad, H.A., Zafar, A., Jamal, A., Alhothali, A., Alharbi, A., Kawsar, F.: Eval-\nuating multimodal large language models on educational textbook question an-\nswering. arXiv preprint arXiv:2506.21596 (2025)\n2. Baumgärtner, T., Briscoe, T., Gurevych, I.: PeerQA: A scientific question answer-\ning dataset from peer reviews. In: Proceedings of the 2025 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies (2025)\n3. Bhambri, S., Verma, M., Kambhampati, S.: Do think tags really help LLMs plan?\na critical evaluation of ReAct-style prompting. Transactions on Machine Learning\nResearch (2025)\n4. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord,\nO.: Think you have solved question answering? try ARC, the AI2 reasoning chal-\nlenge. arXiv preprint arXiv:1803.05457 (2018)\n5. Dang, H., Mecke, L., Lehmann, F., Goller, S., Buschek, D.: How to prompt? oppor-\ntunities and challenges of zero-and few-shot learning for human-ai interaction in\ncreative applications of generative models. arXiv preprint arXiv:2209.01390 (2022)\n6. Fernandez, N., Scarlatos, A., Lan, A.: SyllabusQA: A course logistics question\nanswering dataset. In: Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers). pp. 10344–10369 (2024)\n7. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models\nare zero-shot reasoners. In: Advances in Neural Information Processing Systems.\nvol. 35, pp. 22199–22213 (2022)\n8. Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.: RACE: Large-scale reading com-\nprehension dataset from examinations. In: Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing. pp. 785–794 (2017)\n9. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H.,\nOtt, M., Chen, D., Yih, W.t., et al.: Retrieval-augmented generation for knowledge-\nintensive NLP tasks. In: Advances in Neural Information Processing Systems.\nvol. 33, pp. 9459–9474 (2020)\n10. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: Text\nSummarization Branches Out. pp. 74–81. Association for Computational Linguis-\ntics, Barcelona, Spain (Jul 2004), https://aclanthology.org/W04-1013/\n11. Lin, S., Hilton, J., Evans, O.: TruthfulQA: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 (2021)\n12. Liu, Z., Yin, S.X., Goh, D.H.L., Chen, N.F.: COGENT: A curriculum-oriented\nframework for generating grade-appropriate educational content. In: Proceedings\nof the 20th Workshop on Innovative Use of NLP for Building Educational Appli-\ncations (2025)\n13. Razafinirina, M.A., et al.: Pedagogical alignment of large language models (LLM)\nfor personalized learning: A survey, trends and challenges. Journal of Intelligent\nLearning Systems and Applications 16(04), 448 (2024)\n14. Reimers, N., Gurevych, I.: Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In: Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP). pp. 3982–3992 (2019)\n15. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824–24837 (2022)\n"}, {"page": 14, "text": "14\nAuthors Suppressed Due to Excessive Length\n16. Xu, Y., Yao, B., Wu, T., Zhang, Z., Yu, M., Ma, X., Wang, D., Hou, Y., Peng,\nN., Li, T.J.J., et al.: Fantastic questions and where to find them: FairytaleQA\n– an authentic dataset for narrative comprehension. In: Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). pp. 835–853 (2022)\n17. Zhang, C., et al.: Meta-prompting: A structure-oriented approach for large lan-\nguage models. arXiv preprint (2024)\n18. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Evaluating\ntext generation with bert. arXiv preprint arXiv:1904.09675 (2019)\n"}]}