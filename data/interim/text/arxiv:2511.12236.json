{"doc_id": "arxiv:2511.12236", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.12236.pdf", "meta": {"doc_id": "arxiv:2511.12236", "source": "arxiv", "arxiv_id": "2511.12236", "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts", "authors": ["Raavi Gupta", "Pranav Hari Panicker", "Sumit Bhatia", "Ganesh Ramakrishnan"], "published": "2025-11-15T14:33:02Z", "updated": "2025-11-15T14:33:02Z", "summary": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.12236v1", "url_pdf": "https://arxiv.org/pdf/2511.12236.pdf", "meta_path": "data/raw/arxiv/meta/2511.12236.json", "sha256": "777684d00146bef381743b83203c88638d5cbee45f120d1384379b7c9c218854", "status": "ok", "fetched_at": "2026-02-18T02:27:00.123106+00:00"}, "pages": [{"page": 1, "text": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text\nBy Checking Inconsistencies About Key Facts\nRaavi Gupta1∗, Pranav Hari Panicker2∗, Sumit Bhatia3, Ganesh Ramakrishnan2\n1Columbia University, 2IIT Bombay\n3Media and Data Science Research (MDSR) Lab, Adobe\nraavi.g@columbia.edu, {pranavhp, ganesh}@cse.iitb.ac.in, sumit.bhatia@adobe.com\nAbstract\nLarge language models (LLMs), despite their\nremarkable text generation capabilities, often\nhallucinate and generate text that is factually\nincorrect and not grounded in real-world knowl-\nedge. This poses serious risks in domains like\nhealthcare, finance, and customer support. A\ntypical way to use LLMs is via the APIs pro-\nvided by LLM vendors where there is no ac-\ncess to model weights or options to fine-tune\nthe model. Existing methods to detect hallu-\ncinations in such settings where the model ac-\ncess is restricted or constrained by resources\ntypically require making multiple LLM API\ncalls, increasing latency and API cost. We in-\ntroduce CONFACTCHECK, an efficient halluci-\nnation detection approach that does not lever-\nage any external knowledge base and works on\nthe simple intuition that responses to factual\nprobes within the generated text should be con-\nsistent within a single LLM and across different\nLLMs. Rigorous empirical evaluation on mul-\ntiple datasets that cover both the generation of\nfactual texts and the open generation shows that\nCONFACTCHECK can detect hallucinated facts\nefficiently using fewer resources and achieves\nhigher accuracy scores compared to existing\nbaselines that operate under similar conditions.\nOur code is available here.\n1\nIntroduction\nLarge Language Models (LLMs) are the go-to tools\nfor NLP applications given their excellent text\ngeneration capabilities (Zhao et al., 2023). How-\never, despite recent developments in model archi-\ntecture and training, even state-of-the-art models\nsuch as GPT-4 (Achiam et al., 2023) and PALM-\n540B (Chowdhery et al., 2023) often generate text\nthat appears plausible, but is factually incorrect or\nnon-sensical – a phenomenon termed hallucina-\ntion (Huang et al., 2023). A formal analysis by\nXu et al. (2024) shows that LLMs cannot learn all\n∗Equal contribution.\npossible computational functions, and hence, by\ndesign, will always hallucinate, albeit to different\ndegrees. Consequently, detecting when the LLM\nhallucinates is imperative to take corrective action\nand minimize misinformation from reaching users.\nSuch model hallucinations can be either intrinsic\nor extrinsic (Ji et al., 2023). Intrinsic hallucinations\narise when model outputs contradict the input or\nin-context instructions and can often be detected\nby checking input-output consistency (Huang et al.,\n2023). Extrinsic hallucinations, on the other hand,\noccur when the model output is factually incorrect\nand is not grounded on the pre-training data (Huang\net al., 2023). Given the volume of pre-training data\nand that it is typically inaccessible by the users,\nextrinsic hallucinations pose a greater challenge\ndue to their unverifiable nature (Ji et al., 2023).\nHallucinations in LLMs are typically addressed\nby either (i) improving factual accuracy via train-\ning or fine-tuning (Tian et al., 2023; Azaria and\nMitchell, 2023a; Chuang et al., 2023), or (ii) ver-\nifying model outputs using external knowledge\nsources (Cheng et al., 2024). However, in many\npractical cases, end-users or developers lack access\nto model weights or external verification sources.\nRecent approaches circumvent this by repeatedly\nquerying the LLM (Manakul et al., 2023; Zhang\net al., 2023a; Liu et al., 2022) to thoroughly verify\nresponses or sample large number of outputs to\nestimate output probability distributions, leading to\nsignificantly increased cost and latency. To address\nthese limitations, we propose CONFACTCHECK,\na lightweight method for hallucination detection\nthat relies solely on the LLM’s internal knowl-\nedge. CONFACTCHECK is based on a simple idea:\nan LLM’s understanding of a topic can be eval-\nuated by asking related questions and measuring\nconsistency. This recursive probing strategy has\nalso been used in testing question-answering sys-\ntems (Chen et al., 2021). As illustrated in Fig-\nure 1, CONFACTCHECK identifies key entities/tags\narXiv:2511.12236v1  [cs.CL]  15 Nov 2025\n"}, {"page": 2, "text": "Figure\n1:\nKey\nfact-based\nhallucination\ndetec-\ntion through the Fact Alignment check of our\nCONFACTCHECK pipeline. Each fact is used to gener-\nate a question, and the fact is regenerated by prompting\nthe question to the LLM. The regenerated facts are com-\npared with the original extracted key facts to check for\ntheir consistency.\n(using NER/POS tagging) in the generated output\nand then formulates contextually relevant questions\naround these entities. We term these entities/tags as\n‘key facts’, as these contain essential factual infor-\nmation in sentences. The LLM’s answers to these\nquestions are checked for consistency with the orig-\ninal response, with high consistency indicating that\nthe output is grounded in the model’s pre-training\ndata (reflective of the world knowledge).\nWe evaluate CONFACTCHECK on four dif-\nferent\ndatasets\nspanning\nquestion-answering\n(NQ_Open (Kwiatkowski et al., 2019), Hot-\npotQA (Yang et al., 2018), WebQA (Berant et al.,\n2013)) and open-ended generation tasks where\ninputs to the LLM lack any additional context (Wik-\niBio (Manakul et al., 2023)). CONFACTCHECK\noutperforms recent state-of-the-art self-check or\nself-consistency-based baselines (Manakul et al.,\n2023; Zhang et al., 2023a; Liu et al., 2022) along\nwith baselines relying on the internal states of\nmodels (Chen et al., 2024) for LLMs of different\nmodel families. CONFACTCHECK achieves this\noutperformance while being significantly faster\nand requiring a lower number of LLM calls (c.f.,\nTable 2). We also report the results of various\nablation studies guiding our design choices\nand conclude by discussing the strengths and\nlimitations of CONFACTCHECK.\n2\nRelated Work\nLLMs are inherently prone to hallucinations (Xu\net al., 2024; Ji et al., 2023), a phenomenon also\nobserved in visual and multi-modal models (Bai\net al., 2024; Liu et al., 2024). This has led to ex-\ntensive research on hallucination detection and mit-\nigation (Huang et al., 2023; Zhang et al., 2023b;\nTonmoy et al., 2024). Existing methods fall broadly\ninto two categories: self-checking, prompt-based\napproaches and those that require access to model\nweights or external knowledge sources.\nMethods Requiring Access to Model Weights\nand External Sources: Tian et al. (2023) demon-\nstrate that fine-tuning with factuality preferences\nimproves output correctness. Azaria and Mitchell\n(2023b) use internal LLM activations passed\nthrough a classifier to estimate truthfulness. IN-\nSIDE (Chen et al., 2024) uses internal sentence\nembeddings and analyzes their covariance eigen-\nvalues to detect hallucinations. Various decoding\nstrategies (Chuang et al., 2023; Shi et al., 2024)\nhave also been developed that utilize token proba-\nbilites at various layers to detect and mitigate hal-\nlucinations. Some approaches such as HaluAgent\n(Cheng et al., 2024) use additional tools such as\nweb search engines, code interpreters etc for text\nand code-based detection of hallucinations respec-\ntively.\nSelf-Checking and Prompt-Based Methods:\nZhang et al. (2023a) propose Semantic-Aware\nCross-Check Consistency (SAC3), a sampling-\nbased method that checks for self-consistency\nacross multiple generations. Similarly, SelfCheck-\nGPT (Manakul et al., 2023) samples diverse out-\nputs and scores their similarity to the original to es-\ntimate confidence. InterrogateLLM (Yehuda et al.,\n2024), focuses on regenerating the original query\nfor a generated answer by reversing few-shot QA\npairs to few-shot AQ pairs to self-check for model\nconfidence during regeneration. These self-refining\napproaches often rely on the target LMs them-\nselves, which is also demonstrated in Self-Refine\n(Madaan et al., 2023), an iterative mitigation-based\napproach for hallucinations. Mündler et al. (2023)\nexplore self-contradictions using two LLMs – one\nfor generation and one for contradiction analysis.\nTRUE (Honovich et al., 2022) evaluates factual\nconsistency using a range of metrics (n-gram, NLI,\nmodel-based) on the FEVER dataset (Thorne et al.,\n2018). Liu et al. (2022) propose a reference-free,\ntoken-level method for detecting hallucinations and\nalso present the Hallucination Detection dataset\n(HaDes), with raw web text being perturbed and\nthen annotated by humans to design it for hallu-\ncination detection as a classification task. Cohen\net al. (2023) present a cross-checking prompt-based\nmethod with 2 LLMs in a dialogue setting for\n"}, {"page": 3, "text": "evaluating hallucinations. Yang et al. (2023) em-\nploy a reverse validation method for self-checking\nvia \"databases\" (i.e the same LLMs), by prompt-\ning specific fact-based information to the models.\nFactScore (Min et al., 2023) breaks outputs into\natomic facts, and verifies them using reliable exter-\nnal knowledge sources. We also utilize the notion\nof atomic facts in CONFACTCHECK , however, in-\nstead of leveraging external sources, we check for\nconsistency in LLM outputs about the atomic facts.\n3\nThe CONFACTCHECK Approach\nFigure 2 summarizes our proposed hallucination\ndetection approach comprising of two main steps\n– (i) a fact alignment check where key facts in the\noutput are compared with facts obtained by targeted\nprobing of the LLM; and (ii) a uniform distribution\ncheck that filters out the low confidence predictions.\nWe now describe the overall pipeline in detail.\n3.1\nFact Alignment Check\nExtracting Key Facts: To check whether a piece\nof text, A, generated by an LLM M is hallucinated,\nwe start with the assumption that the generated text\nis correct. We then generate questions targeting\neach key fact in A, such that they can be answered\nsolely using the content of A. Subsequently, we\nemploy the LLM to answer the questions and see\nif the answers match the information in A, a mis-\nmatch indicating hallucinations. The initial step\nis to identify the factual components within a sen-\ntence. According to Kai et al. (2024), factual infor-\nmation in a sentence is typically conveyed through\nspecific parts of speech, viz., nouns, pronouns, car-\ndinal numbers, and adjectives. We highlight tags\nwith such information as key facts that are to be\nextracted. Min et al. (2023) use a similar concept,\nwhere they classify short sentences in text (obtained\nby InstructGPT generation and human annotation)\nas atomic facts. However, the key facts we discuss\nare extracted NER/POS tags containing factual in-\nformation, and hence are different. Key facts can be\nextracted by performing part-of-speech (POS) tag-\nging or Named Entity Recognition (NER) on the\nsentence. Given an LLM output A, we perform\ncoreferencing and decompose A into sentences\nS1, S2, . . . , SN, where N is the total number of\nsentences, such that A = {S1, S2 . . . , SN}. Each\nsentence is tagged to extract key facts aij, where\ni ∈{1, . . . , N}, and j depends on the number\nof tagged entities in a sentence. The tagging can\nbe either POS-based or NER-based, as discussed\nin Section 5.4.3. For example, given the origi-\nnal sentence “Argentina won the World Cup in the\nyears, 1978, 1986 and 2006.”, in Figure 1, the\nkey facts consist of a = [a11 = Argentina, a12 =\nWorld Cup, a13 = 1978, 1986 and 2006].\nTargeted Question Generation: After identify-\ning key facts, the next step involves verifying\nwhether each fact is hallucinated within the context\nof the sentence. Unlike previous methodologies\nthat assign a hallucination score to each sentence,\nCONFACTCHECK focuses on key facts, thereby\nenhancing explainability by pinpointing the ex-\nact parts of a sentence that are hallucinated and\nproviding reasons for this determination, as de-\ntailed in Section 5.5. Specifically, for each key\nfact aij given sentence Si, a corresponding ques-\ntion qij is generated (using a T5-based model that\nis specifically finetuned for this task of question\nregeneration), with aij as the target answer and\nSi as the context, expressed as qij = Q(aij|Si),\nwhere Q represents the question generation module.\nIn Figure 1, each key fact provides one question\nq = [q11 = Question 1, q12 = Question 2, q13 =\nQuestion 3]. LLM M′ is then used to evaluate\nthese questions at a low temperature to ensure re-\nsponse consistency, as it enables the LLM to gen-\nerate high-quality and deterministic outputs. Each\nindividual key fact-based question is answered by\nthe LLM with greater precision and therefore helps\nto better identify whether the fact is correct or incor-\nrect (Dhuliawala et al., 2024). Note that M′ may\nor may not be the same as M, as another LLM can\nbe used to evaluate the responses of LLM M.\nConsistency Checking The responses from M′\nyield regenerated facts fij, which are subsequently\nchecked for consistency with aij. To check for\nthe similarity between fij and aij, we follow the\nLLM-as-a-judge paradigm (Zheng et al., 2023),\nby querying GPT4.1-mini using few-shot prompt-\ning to assess whether each pair is aligned or not.\nFor instance, the set f for Figure 1 being f =\n[f11 = Argentina, f12 = FIFA World Cup, f13 =\n1978, 1986 and 2022.], and original key facts be-\ning a = [a11 = Argentina, a12 = World Cup, a13\n= 1978, 1986 and 2006].\nIn this case, facts\nf13 and a13 are non-aligned; whereas, the pairs\n< f11, a11 > and < f12, a12 > are aligned as\nper the judge’s output. For each aligned and non-\naligned pairs, we assign the score of 0 and 1 re-\nspectively. Note that since the number of extracted\nfacts varies based on the sentence, the number of\n"}, {"page": 4, "text": "Figure 2: Pipeline of the CONFACTCHECK approach, with NER tagging of outputs followed by the first comparison-\nbased check (Fact Alignment Check) and the secondary KS test-based probability check (Uniform Distribution\nCheck) for rechecking the classfied non-hallucinations, result in the final tagging of hallucinations.\nquestions generated per sentence also varies. The\nconsistency checking step, thus, enables the de-\ncomposition of sentence-level information into dis-\ncrete factual elements and leverages and operates\nunder the assumption that the LLM’s responses\nwill remain consistent for factual information when\nsampled at a low temperature.\n3.2\nUniform Distribution Check\nAfter the fact-alignment step, we perform a subse-\nquent step to check if the facts were regenerated\nwith high confidence. The underlying intuition be-\nhind this step is that if the LLM is confident in\nregenerating a fact correctly, the probability dis-\ntribution of the generated tokens will be skewed,\nwith the selected tokens having significantly higher\nprobabilities than the other possible tokens. This\nresults in a non-uniform distribution of token prob-\nabilities. Conversely, if the LLM is uncertain, even\nthough the generated tokens may have the highest\nrelative probability, their values will be closer to\nthose of alternative tokens (closer to a uniform dis-\ntribution) and indicating less confidence in LLM\nprediction. To quantify this effect, we apply the\nKolmogorov–Smirnov (K–S) test to the top five to-\nkens associated with each regenerated fact fij. The\ntest is conducted using a standard significance level\nof 0.05. A p-value below this threshold leads to the\nrejection of the null hypothesis (i.e., the top tokens\nare drawn from a uniform distribution) implying\nthat the LLM exhibits confidence in its generation.\nIf the test indicates a non-uniform distribution, the\nLLM is deemed confident in regeneration, and orig-\ninal fact aij is classified as non-hallucinated. How-\never, if the token probabilities follow a uniform\ndistribution, it is concluded that the particular fact\nis hallucinated, reflecting the LLM’s lack of confi-\ndence. The final hallucination score for a sentence\nSi is calculated by averaging the individual scores\nof aij present in it to give a probability of how\nlikely a sentence has been hallucinated.\n4\nExperimental Protocol\n4.1\nTask and Datasets\nWe consider two common task settings – question\nanswering (QA) and text summarization. In the QA\nsetting, LLMs are particularly susceptible to factual\nhallucinations, especially when no external context\nor information is provided with the input questions.\nThe summarization task is a representative of the\nlong-form text generation tasks where the output\nis not limited to be a short answer (a phrase or a\nsentence), and hence enables us to evaluate the abil-\nity of various methods to detect hallucinations in\nlonger pieces of text. Further, this setting also tests\nthe ability of the LLM to generate text that is faith-\nful to the input context (text to be summarized).\nWe use the following datasets for evaluation,\n(with the validation/test sets for QA):\n1. Natural Questions (NQ)-open (Kwiatkowski\net al., 2019) is an open-domain QA benchmark\nderived from the Natural Questions dataset (Lee\net al., 2019). The validation split of this dataset\nconsists of 3,610 open-domain question-answer\npairs covering a wide range of topics..\n2. HotpotQA (Yang et al., 2018) is a QA dataset\nthat features complex questions requiring multi-\nhop reasoning.\n3. WebQA (Berant et al., 2013) dataset is a factoid\n"}, {"page": 5, "text": "QA dataset where the questions are derived from\nthe Freebase knowledge base.\n4. WikiBio (Manakul et al., 2023) is a halluci-\nnation detection dataset derived from Wikipedia\nbiographies. It consists of 238 randomly selected\narticles from among the longest 20% Wikipedia\narticles. It also provides synthetic text generated by\nGPT-3 for each of the original articles, along with\nlabels for factual correctness of the sentences.\n4.2\nBaselines\nWe use following four representative self-check\nand self-consistency based hallucination detection\nmethods as baselines.\nHaDes (Liu et al., 2022) is an external reference-\nfree method that leverages various token-level fea-\ntures such as POS tags, average word probability,\nmutual information, and TF-IDF scores to identify\nif a token is hallucinated or not.\nSelfCheckGPT (Manakul et al., 2023) is a sam-\npling based approach built upon the intuition that\nfor hallucinated responses, stochastically sampled\nresponses for the same input are likely to diverge.\nSAC3 (Zhang et al., 2023a), another sampling-\nbased approach that generates responses to multiple\nsemantically similar inputs to the original input and\nchecks for consistency in the generated outputs.\nINSIDE (Chen et al., 2024) detects hallucinations\nusing the EigenScore metric, calculated using the\neigenvalues of the covariance matrix of the re-\nsponses to measure the semantic consistency/diver-\nsity in the dense embedding space of the generated\noutputs.\n4.3\nImplementation details\nModels Used. We use LLaMA3.1-8B-Instruct\nand Qwen2.5-7B-Instruct as the base LLMs for\ncomparing CONFACTCHECK and various base-\nlines. For the QA task, initial responses are also\ngenerated using these base LLMs, with a tempera-\nture of 1. Further, we use different models of Phi-3\nfamily to study how well CONFACTCHECK per-\nforms with LLMs of varying scale (Section 5.3).\nWe present ablations that guided our design choices\nin Sections 5.4.2 and 5.4.3. We use the official\nimplementation of HaDes1 for our experiments.\nFor SAC3 (Zhang et al., 2023a), we compute the\nquestion-level consistency SAC3-Q score and em-\nploy predetermined thresholds to discern the pres-\nence of hallucinated outputs.\n1https://github.com/microsoft/HaDes\nMetrics for Analysis:\nWe consider hallucina-\ntion detection as a binary classification task where\nthe text generated by the LLM is either halluci-\nnated or not. For QA datasets, we assign labels of\n1 for hallucination and 0 for non-hallucination to\nthe original outputs by comparing them with the\ngolden answers in the QA datasets using GPT4.1-\nmini as a judge LLM. For WikiBio, each sentence-\nlevel golden label is provided in the dataset itself.\nWe compare the baselines with our approach (see\nTable 1) and report the AUC-PR scores on the 3\nopen-domain QA datasets, as well as the WikiBio\nsummarization dataset. Note that the SelfCheck-\nGPT baseline is applicable on the WikiBio dataset,\nas the others deal with only the QA task and require\nquestions as part of their input.\n5\nEmpirical Results\n5.1\nCONFACTCHECK for Hallucination\nDetection\nTable 1 summarizes the results of different meth-\nods for the four datasets and across two LLM\nbackbones (LLaMA3.1-8b and Qwen2.5-7B). We\nobserve that CONFACTCHECK outperforms most\nbaselines on the QA datasets and the two LM\nbackbones.\nOnly the Selfcheck-Prompt base-\nline outdoes our approach in few settings, and\neven in all such cases, CONFACTCHECK is the\nsecond-best performing method (the second-best\napproach in each column is underlined). Selfcheck-\nPrompt achieves the second-best performance on\nthree other QA settings, while SAC3 and INSIDE\nachieve the second-best performance in 2 different\nsettings. Thereby, CONFACTCHECK demonstrates\nconsistency by either being the best or second-best\nperforming method in all settings. Further, only\nSelfCheckGPT can be used for detecting halluci-\nnations in free-form text (WikiBio dataset), as the\nother baselines are designed for detecting halluci-\nnations in QA tasks and need questions as part of\ntheir input. CONFACTCHECK, on the other hand,\ncan detect hallucinations in QA as well as free-\nform text settings and achieves strong performance\nacross all settings. Such strong performance of\nCONFACTCHECK can be attributed to the fact that\nit identifies the key factual tokens in the generated\ntext and probes the LLM regarding its knowledge\naround these tokens.\n"}, {"page": 6, "text": "Model\nNQ Open\nHotpotQA\nWebQA\nWikiBio\nLLaMA3.1\nQwen2.5\nLLaMA3.1\nQwen2.5\nLLaMA3.1\nQwen2.5\nLLaMA3.1\nQwen2.5\nHaDes (Liu et al., 2022)\n0.54\n0.67\n0.68\n0.69\n0.46\n0.48\nN/A\nN/A\nSAC3 (Zhang et al., 2023a)\n0.59\n0.71\n0.68\n0.59\n0.63\n0.55\nN/A\nN/A\nSelfCheck-MQAG (Manakul et al., 2023)\n0.58\n0.75\n0.76\n0.78\n0.50\n0.62\n0.83\n0.83\nSelfCheck-Prompt (Manakul et al., 2023)\n0.76\n0.80\n0.86\n0.82\n0.54\n0.68\n0.92\n0.90\nINSIDE (Chen et al., 2024)\n0.61\n0.54\n0.56\n0.60\n0.58\n0.68\nN/A\nN/A\nCONFACTCHECK\n0.73\n0.80\n0.83\n0.84\n0.66\n0.71\n0.86\n0.85\nTable 1: AUC-PR scores for NQ Open, HotpotQA, WebQA, and WikiBio datasets. We compare ConFactCheck in\nthe same settings as the baselines, using LLaMA3.1-8B-Inst and Qwen2.5-7B-Inst as the base models. Settings\nfor CONFACTCHECK results use beam decoding on the whole pipeline (this yields best possible scores). The best\nperforming method in a given column is in bold and the second best performing model is underlined.\n5.2\nComputational Efficiency of Different\nMethods\nRecall from discussions in Section 1 that self-check\nor self-refinement style methods suffer from high\nlatencies due to the need to query the LLM repeat-\nedly to estimate the output probability distributions\nor for a thorough verification of the generated out-\nput. CONFACTCHECK, on the other hand, identi-\nfies key facts in the generated output and gener-\nates targeted questions around these facts, thereby\ngreatly reducing the number of LLM calls. Fur-\nther, CONFACTCHECK relies on lightweight com-\nparisons and statistical operations (Section 3) to\ncheck if the answers to targeted questions align\nwith the original output.\nTable 2 presents the\naverage number of LLM calls made and the av-\nerage inference time for different methods. We\nnote from the table that CONFACTCHECK achieves\nfast inference times for both the LLaMA3.1 and\nQwen2.5 backbones.\nINSIDE is slightly faster\nthan CONFACTCHECK, however our pipeline of-\nfers up to ≈1.4x speedup compared to SelfCheck-\nPrompt (Manakul et al., 2023) (9.51s vs. 13.35s\nfor LLaMA3.1) and ≈1.5x and ≈3x when compared\nto SAC3 (on the 2 LLMs respectively). Note also\nthat in the case of CONFACTCHECK the number\nof calls being made to the LLM is equivalent to\nthe average number of key facts extracted per input\nin the dataset plus one additional call to the judge-\nLLM for Fact Alignment. In Table 2), we report the\nlatency numbers for SelfCheckGPT and SAC3 with\n20 and 5 LLM samples per question, and INSIDE\nwith 10 LLM samples as recommended by the re-\nspective papers. Also note that the performance\nnumbers for SelfCheckGPT and SAC3 in Table 1\nare with these optimal number of LLM calls (20\nand 5 respectively, while they can be lower) to ex-\nhibit their best performance with efficiency. All\nexperiments on CONFACTCHECK and the base-\nlines as reported were run using NVIDIA A6000\nGPUs, using the mentioned open-source LLMs for\nquerying and execution.\nMethod\n# LLM calls/samples\nLLaMA3.1\nQwen2.5\nSelfCheck-MQAG\n20\n58.9 s\n47.94 s\nSelfCheck-Prompt\n20\n13.35 s\n12.16 s\nSAC3\n5\n15.46 s\n29.37 s\nINSIDE\n10\n4.89 s\n5.68 s\nCONFACTCHECK\n3.8\n9.51 s\n9.03 s\nTable 2:\nAverage inference time (in seconds) for\nCONFACTCHECK and the baselines (which have con-\nfigurable amount of LLM calls) over the samples of the\nNQ_Open dataset while using LLaMA3.1 and Qwen2.5\nmodels. CONFACTCHECK offers significant speedups\nover the self-checking baselines.\n5.3\nCONFACTCHECKwith LLMs of Varying\nScale\nWe\nnow\nstudy\nhow\nthe\nperformance\nof\nCONFACTCHECK\nvaries\nwith\nthe\nscale\nof\nthe underlying LLM. We use the Phi-3-Instruct\nfamily (Abdin et al., 2024) of models for this\npurpose and chose models of 3 sizes – 3.8B, 7B,\nand 13B. Table 3 summarizes the results for the\nthree Phi-3 models on the three QA datasets. In\naddition to the AUC-PR of hallucination detection,\nwe also report the percentage of hallucinated\noutputs in each setting to understand the severity\nof hallucinations at different model scales. We\nnote from the table that for these datasets, there\nis a decent amount of hallucinated outputs,\nwhich wavers from the 3.8B to 13B models.\nThis shows that just increasing the model size\nmay not eliminate hallucinations. We also note\nthat the ability of CONFACTCHECK to detect\nhallucinations is similar and consistent across\ndifferent model sizes. While the Phi3-7B slightly\n"}, {"page": 7, "text": "outperforms on NQ-open, the increasing model\nsizes show moderate gains for the HotpotQA and\nWebQA datasets.\nModel\nNQ Open\nHotpotQA\nWebQA\nAUC\n%Hall.\nAUC\n%Hall.\nAUC\n%Hall.\nPhi-3-4b\n0.69\n0.65\n0.74\n0.69\n0.63\n0.49\nPhi-3-7b\n0.73\n0.58\n0.74\n0.60\n0.62\n0.46\nPhi-3-13b\n0.71\n0.54\n0.76\n0.64\n0.65\n0.50\nTable 3: Performance of CONFACTCHECK for different\nsize models of the Phi-3 family. We report AUC-PR of\nhallucination detection and percentage of hallucinated\noutputs (Hall.) for the 3.8B, 7b, and 13B models for the\nthree QA datasets.\n5.4\nAblation Studies\nWe\nnow\ndescribe\ndifferent\nablation\nstud-\nies that guided different design choices for\nCONFACTCHECK.\nWe report the impact of\nfact-alignment and uniform distribution check\nsteps in the pipeline (Section 3). We also describe\nthe effects of different decoding strategies and\nmethods for detecting key facts in the input.\n5.4.1\nRole of Different Components in\nCONFACTCHECK\nRecall\nthat\nthere\nare\ntwo\nmain\nsteps\nin\nCONFACTCHECK – fact alignment and uniform\ndistribution check.\nThe fact alignment step at-\ntempts to regenerate the key facts in the generated\noutput by querying the LLM with targeted ques-\ntions. The regenerated facts are then compared\nwith the original output for consistency. The sub-\nsequent uniform distribution check acts as another\nverification layer by relying on the model’s confi-\ndence in the generation of regenerated key facts. Ta-\nble 4 summarizes the hallucination detection scores\nachieved by just the fact-alignment step along with\nthe improvements achieved by performing the sub-\nsequent uniform distribution check (the complete\npipeline). We note from the table that the uniform\ndistribution step plays a crucial role in the overall\nperformance of CONFACTCHECK with maximum\ngains of up to 18%.\n5.4.2\nEffect of Decoding Strategies\nRegardless of how the original response, subject to\nhallucination assessment, was generated, we exam-\nine the variations in regenerated factual responses\nwhen decoding strategies are varied. The following\ndecoding strategies were utilized:\nComponent\nLLM\nNQ Open\nHotpotQA\nWebQA\nFact Alignment\nLLaMA3.1\n0.66\n0.79\n0.56\n+ Distribution Check\nLLaMA3.1\n0.73\n0.83\n0.66\n% gain\n11%\n5%\n18%\nFact Alignment\nQwen2.5\n0.79\n0.82\n0.68\n+ Distribution Check\nQwen2.5\n0.8\n0.84\n0.71\n% gain\n1%\n2%\n5%\nTable 4: AUC-PR scores achieved by the two major com-\nponents of CONFACTCHECK. A uniform distribution\ncheck after the fact alignment step leads to significant\nperformance gains.\n• Greedy Decoding: Greedy decoding involves\nselecting the token from the vocabulary V\nwith the highest conditional probability. This\nsuggests prioritizing key facts for which the\nmodel has the highest immediate confidence.\n• Beam Decoding:\nBeam decoding repre-\nsents an enhancement over greedy decoding.\nIn Beam decoding, a parameter known as\nbeam_size determines the number of tokens\nwith the highest conditional probabilities con-\nsidered at each time step t. For our experi-\nments, we considered the beam size to be 5.\nModel\nNQ Open\nHotpotQA\nWebQA\nWikiBio\nLLaMA3.1 (Greedy)\n0.70\n0.81\n0.62\n0.86\nLLaMA3.1 (Beam)\n0.73\n0.83\n0.66\n0.86\nQwen2.5 (Greedy)\n0.79\n0.82\n0.66\n0.85\nQwen2.5 (Beam)\n0.80\n0.84\n0.71\n0.85\nTable 5: The AUC-PR scores of CONFACTCHECK with\nLLaMA3.1-8B-Inst and Qwen2.5-7b-Inst models us-\ning different decoding strategies for fact regeneration\non the QA datasets. Beam decoding (beam size = 5)\noutperforms Greedy Decoding in most of the settings.\nBeam decoding improves the detection of hal-\nlucinations during fact regeneration compared to\ngreedy search. This advantage likely arises because\nbeam decoding explores multiple possible answer\npaths before selecting the most likely one. Beam\ndecoding also implicitly mitigates hallucinations\nby preferring sequences with higher cumulative\nconfidence, which are more likely to reflect con-\nsistent factual patterns across generations. As a\nresult, when regenerating key facts, beam decoding\nensures a more informed selection of entities, and\nthe results in Table 5 show its improvements. Chen\net al. (2018) further corroborate this by indicating\nthat beam decoding generally outperforms greedy\ndecoding. By maintaining multiple candidate gen-\nerations, beam decoding reduces the likelihood of\n"}, {"page": 8, "text": "factual errors, ensuring the correct regeneration of\nfacts. However, this decoding strategy does involve\na trade-off with computational efficiency compared\nto greedy decoding.\n5.4.3\nTagging of key-facts\nIdentifying of key facts in the generated text is a\ncrucial step in CONFACTCHECK as they are used\nto probe the LLM in a targeted fashion. Hence,\nthe choice of method used for identifying key facts\nin the generated text can have significant impact\non the overall performance. Kai et al. (2024) sug-\ngests that factual information in a sentence can be\nidentified using POS tagging, specifically ’NNP’\nor ’NNPS’. Building on this, we selected the tags\n’NNP’, ’NNPS’, ’CD’, and ’RB’ to be considered\nkey facts. As an alternative, we also evaluated us-\ning NER tagging and considering identified named\nentities as key facts. We used Stanford’s Stanza (Qi\net al., 2020) library for NER and POS tagging. Ad-\nditionally, we also sampled random tokens from\nthe sentence and used them as key facts, ensur-\ning that the number of sampled tokens equaled the\nnumber of NER tags present. Table 6 summarizes\nthe results for the three strategies and reveals that\nthough the results are similar, NER outperforms\nboth POS tagging and random token sampling in\nmore settings to identify which tokens contribute\nto the factuality of a sentence or paragraph.\nTagging\nNQ Open\nHotpotQA\nWebQA\nLLaMA3.1\nQwen2.5\nLLaMA3.1\nQwen2.5\nLLaMA3.1\nQwen2.5\nRandom\n0.72\n0.78\n0.82\n0.83\n0.68\n0.69\nPOS\n0.71\n0.81\n0.82\n0.83\n0.66\n0.7\nNER\n0.73\n0.8\n0.83\n0.84\n0.66\n0.71\nTable 6: The AUC-PR scores while using different tag-\nging strategies on LLaMA3.1-8B-Inst and Qwen2.5-7B-\nInst for identifying key facts in the sentence. NER is\nobserved to perform slightly better in more cases over\nthese three QA datasets.\n5.5\nKey Strengths of CONFACTCHECK\nWe\nnow\ndiscuss\nthe\nmajor\nstrengths\nof\nCONFACTCHECK which are summarized as\nfollows.\nTraining-Free Operation: Our generic approach\nrequires only the LLM-generated output for fact-\nalignment check stage of the pipeline and does not\nnecessitate dataset- or task-specific training. The\nnumber of generated questions is determined by\nthe factual content within the generated sentence,\navoiding heuristic selection. During fact regenera-\ntion, CONFACTCHECK leverages the output token\nprobabilities for the probability check, which is\nprovided by most open-source LLMs. However, in\nfew cases of LLMs accessed via APIs it is possible\nthat the access to output probabilities is not avail-\nable (see Limitations). Even in cases where such\nAPI-based LLMs are used for generation, their out-\nputs can be passed through open-source LLMs to\nperform both checks with token probabilities.\nEase of Implementation: CONFACTCHECK does\nnot require access to model weights or underlying\ntraining data. Requiring only the model’s output\nand the LLM used for response generation, our\nmethod can be deployed on the same device as\nthe response generation process, whether through\na web interface, API, or a locally executed model.\nEven for the use of KS test, we require only the\noutput token probabilities of the top-5 generations,\nwhich can be directly stored during LLM genera-\ntion.\nConsistent Sample Scoring: Unlike stochastic\nhallucination detection methods such as SelfCheck-\nGPT (Manakul et al., 2023), CONFACTCHECK op-\nerates deterministically by probing factual tokens\nat temperature 0. While similar consistency can\nbe achieved in other methods using fixed seeds,\nCONFACTCHECK offers this behavior by default,\nresulting in stable sample scoring without addi-\ntional tuning. This also modestly reduces computa-\ntional overhead by avoiding multiple generations\nper query.\nInterpretability: CONFACTCHECK provides key-\nfact-level scoring, enabling users to identify\nspecific hallucinated facts.\nFor instance, in\nthe running example of Figure 1, in addition\nto classifying the output text as hallucinated,\nCONFACTCHECK explicitly identifies that the fact\na21\n=\n{1978, 1986 and 2006} is hallucinated\n(non-aligned).\nOperating on fine-grained facts\nrather than entire sentences, our pipeline offers\na greater degree of explainability than previous ap-\nproaches like SAC (Zhang et al., 2023a), clarifying\nthe rationale behind a hallucination classification.\n6\nConclusions\nWe presented CONFACTCHECK, a novel fact-\nbased hallucination detection pipeline evaluated\nit using four factuality measurement datasets and\ncompared with multiple strong baselines.\nOur\nfindings reveal that despite being less computa-\ntionally expensive and not requiring any training,\nCONFACTCHECK performs on par with other ap-\n"}, {"page": 9, "text": "proaches while being significantly faster.\n7\nLimitations\nDespite the high performance, ease of use, and\nefficiency offered by CONFACTCHECK, it is not\nwithout limitations. We analyze and present repre-\nsentative examples of failure cases to highlight its\nshortcomings and possible future areas of improve-\nment.\nEffect of incorrect tags on correct outputs:\nConsider the following example from HotpotQA:\nWhich of the office buildings used to staff the White\nHouse used to be known as the State, War, and Navy\nBuilding? For this question, the answer provided\nby an LLM is the following. The office building\nused to staff the White House that was once known\nas the State, War, and Navy Building is now known\nas the Eisenhower Executive Office Building. This\nbuilding was constructed in 1952 and was named\nafter President Dwight D. Eisenhower.\nAlthough Eisenhower Executive Office Building\nis factually correct, our pipeline categorizes the\nparagraph as hallucinated. This discrepancy arises\nbecause our model identifies the fact ‘1952’ as hal-\nlucinated because of the building’s actual construc-\ntion period between 1871 and 1888. This contrasts\nwith the golden output from HotpotQA, which does\nnot flag the answer as hallucinated (when the judge\nLLM is used on the original output and golden an-\nswer to get the golden label). However, due to the\npresence of other hallucinated facts, our pipeline as-\nsigns a hallucinated tag to the paragraph. Similarly,\nwhile the model correctly identifies the building as\nthe Eisenhower Executive Office Building, it erro-\nneously states the construction year as 1952 (actual:\n1871–1888). As a result, CONFACTCHECK tags\nthis factual mismatch, leading to a hallucination\nscore for the entire paragraph.\nInefficiency in question generation:\nThe generated questions extracted key facts are\ndone by the T5-based finetuned model. While it is\nefficient in generating pinpointing questions with\nthe extracted fact as answer with original output as\ncontext, some ambigious questions such as “Who\nwas the building named after?” can be generated.\nThis ambiguity can result in inaccuracies when re-\ngenerating facts. For this, using a much larger LLM\ncan be useful, however it would be computationally\nexpensive and time-inefficient while not providing\nsignificant improvements.\nLanguage-based limited usecases:\nIn addition, we also note that the proposed\nCONFACTCHECK has only been tested for English\nlanguage and LLMs trained mostly on English data.\nAlthough the framework is theoretically language-\nagnostic, its reliance on NER/POS tools constrains\napplicability in low-resource languages lacking ro-\nbust NLP pipelines. Further, the performance of\nCONFACTCHECK depends crucially on intermedi-\nate steps requiring NER and POS tagging, which\nmay not always be available for low-resource lan-\nguages.\nUnavailability of output token probabilities in\nAPI-based LLMs:\nWhile using CONFACTCHECK, fact regeneration is\nperformed and subsequently the output token prob-\nabilities of the regenerated facts are required for the\nUniform Distribution check to gauge the LLM’s\nconfidence in generation. However, it is possible\nthat for multiple API-based LLMs or closed source\nmodels, output generation probabilities cannot be\nstored or utilized. Hence, CONFACTCHECK can-\nnot be accessed by such LLMs and specifically\nrequires open-source LLMs for the two checks in\nthe pipeline.\nReferences\nMarah I Abdin, Sam Ade Jacobs, Ammar Ahmad\nAwan, Jyoti Aneja, Ahmed Awadallah, Hany Has-\nsan Awadalla, Nguyen Bach, Amit Bahree, Arash\nBakhtiari, Harkirat Behl, Alon Benhaim, Misha\nBilenko, Johan Bjorck, Sébastien Bubeck, Martin\nCai, Caio César Teodoro Mendes, Weizhu Chen,\nVishrav Chaudhary, Parul Chopra, Allie Del Giorno,\nGustavo de Rosa, Matthew Dixon, Ronen Eldan,\nDan Iter, Abhishek Goswami, Suriya Gunasekar, Em-\nman Haider, Junheng Hao, Russell J. Hewett, Jamie\nHuynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann,\nNikos Karampatziakis, Dongwoo Kim, Mahmoud\nKhademi, Lev Kurilenko, James R. Lee, Yin Tat\nLee, Yuanzhi Li, Chen Liang, Weishung Liu, Xi-\nhui (Eric) Lin, Zeqi Lin, Piyush Madan, Arindam\nMitra, Hardik Modi, Anh Nguyen, Brandon Norick,\nBarun Patra, Daniel Perez-Becker, Thomas Portet,\nReid Pryzant, Heyang Qin, Marko Radmilac, Corby\nRosset, Sambudha Roy, Olli Saarikivi, Amin Saied,\nAdil Salim, Michael Santacroce, Shital Shah, Ning\nShang, Hiteshi Sharma, Xia Song, Olatunji Ruwase,\nXin Wang, Rachel Ward, Guanhua Wang, Philipp\nWitte, Michael Wyatt, Can Xu, Jiahang Xu, Weijian\nXu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu,\nChengruidong Zhang, Cyril Zhang, Jianwen Zhang,\nLi Lyna Zhang, Yi Zhang, Yunan Zhang, and Xiren\nZhou. 2024. Phi-3 technical report: A highly capable\nlanguage model locally on your phone. Technical\nReport MSR-TR-2024-12, Microsoft.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\n"}, {"page": 10, "text": "Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\nArXiv preprint, abs/2303.08774.\nAmos Azaria and Tom Mitchell. 2023a. The internal\nstate of an LLM knows when it’s lying. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 967–976, Singapore. Associa-\ntion for Computational Linguistics.\nAmos Azaria and Tom Mitchell. 2023b. The internal\nstate of an LLM knows when it’s lying. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 967–976, Singapore. Associa-\ntion for Computational Linguistics.\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He,\nZongbo Han, Zheng Zhang, and Mike Zheng Shou.\n2024. Hallucination of multimodal large language\nmodels: A survey. ArXiv preprint, abs/2404.18930.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu,\nMingyuan Tao, Zhihang Fu, and Jieping Ye. 2024.\nINSIDE: llms’ internal states retain the power of hal-\nlucination detection. In The Twelfth International\nConference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024. OpenReview.net.\nSongqiang Chen, Shuo Jin, and Xiaoyuan Xie. 2021.\nTesting your question answering software via ask-\ning recursively. In 2021 36th IEEE/ACM Interna-\ntional Conference on Automated Software Engineer-\ning (ASE), pages 104–116.\nYun Chen, Victor O.K. Li, Kyunghyun Cho, and Samuel\nBowman. 2018. A stable and effective learning strat-\negy for trainable greedy decoding. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 380–390, Brussels,\nBelgium. Association for Computational Linguistics.\nXiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi\nZhang, Fuzheng Zhang, Di Zhang, Kun Gai, and\nJi-Rong Wen. 2024. Small agent can also rock! em-\npowering small language models as hallucination\ndetector. ArXiv preprint, abs/2406.11277.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1–113.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James Glass, and Pengcheng He. 2023. Dola:\nDecoding by contrasting layers improves factual-\nity in large language models.\nArXiv preprint,\nabs/2309.03883.\nRoi Cohen, May Hamri, Mor Geva, and Amir Glober-\nson. 2023. LM vs LM: Detecting factual errors via\ncross examination. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 12621–12640, Singapore. Associ-\nation for Computational Linguistics.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. 2024. Chain-of-verification reduces\nhallucination in large language models. In Findings\nof the Association for Computational Linguistics ACL\n2024, pages 3563–3578, Bangkok, Thailand and vir-\ntual meeting. Association for Computational Linguis-\ntics.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the Second\nDialDoc Workshop on Document-grounded Dialogue\nand Conversational Question Answering, pages 161–\n175, Dublin, Ireland. Association for Computational\nLinguistics.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.\nA survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.\nArXiv preprint, abs/2311.05232.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12).\nJushi Kai, Tianhang Zhang, Hai Hu, and Zhouhan\nLin. 2024. Sh2: Self-highlighted hesitation helps\nyou decode more truthfully.\nArXiv preprint,\nabs/2401.05930.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\n"}, {"page": 11, "text": "Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,\nXiutian Zhao, Ke Wang, Liping Hou, Rongjun Li,\nand Wei Peng. 2024.\nA survey on hallucination\nin large vision-language models.\nArXiv preprint,\nabs/2402.00253.\nTianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,\nZhifang Sui, Weizhu Chen, and Bill Dolan. 2022.\nA token-level reference-free hallucination detection\nbenchmark for free-form text generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 6723–6737, Dublin, Ireland. Association\nfor Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. In Advances in\nNeural Information Processing Systems 36: Annual\nConference on Neural Information Processing Sys-\ntems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023.\nPotsawee Manakul, Adian Liusie, and Mark Gales. 2023.\nSelfCheckGPT: Zero-resource black-box hallucina-\ntion detection for generative large language models.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n9004–9017, Singapore. Association for Computa-\ntional Linguistics.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12076–12100, Singa-\npore. Association for Computational Linguistics.\nNiels Mündler, Jingxuan He, Slobodan Jenko, and Mar-\ntin Vechev. 2023. Self-contradictory hallucinations\nof large language models: Evaluation, detection and\nmitigation. ArXiv preprint, abs/2305.15852.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A Python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations.\nWeijia Shi, Xiaochuang Han, Mike Lewis, Yulia\nTsvetkov, Luke Zettlemoyer, and Wen-tau Yih. 2024.\nTrusting your evidence: Hallucinate less with context-\naware decoding. In Proceedings of the 2024 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 2: Short Papers), pages\n783–791, Mexico City, Mexico. Association for Com-\nputational Linguistics.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nKatherine Tian, Eric Mitchell, Huaxiu Yao, Christo-\npher D Manning, and Chelsea Finn. 2023.\nFine-\ntuning language models for factuality.\nArXiv\npreprint, abs/2311.08401.\nS. M Towhidul Islam Tonmoy, S M Mehedi Zaman,\nVinija Jain, Anku Rani, Vipula Rawte, Aman Chadha,\nand Amitava Das. 2024. A comprehensive survey of\nhallucination mitigation techniques in large language\nmodels. Preprint, arXiv:2401.01313.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli.\n2024. Hallucination is inevitable: An innate lim-\nitation of large language models. ArXiv preprint,\nabs/2401.11817.\nShiping Yang, Renliang Sun, and Xiaojun Wan. 2023.\nA new benchmark and reverse validation method for\npassage-level hallucination detection. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 3898–3908, Singapore. Associ-\nation for Computational Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nYakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan\nWeill, Royi Ronen, and Noam Koenigstein. 2024.\nInterrogateLLM: Zero-resource hallucination detec-\ntion in LLM-generated answers.\nIn Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 9333–9347, Bangkok, Thailand. Association\nfor Computational Linguistics.\nJiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Ma-\nlin, and Sricharan Kumar. 2023a. SAC3: Reliable\nhallucination detection in black-box language models\nvia semantic-aware cross-check consistency. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 15445–15458, Singapore.\nAssociation for Computational Linguistics.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023b. Siren’s song\nin the ai ocean: A survey on hallucination in large\nlanguage models. Preprint, arXiv:2309.01219.\n"}, {"page": 12, "text": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. Preprint,\narXiv:2303.18223.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nA\nModels and Implementations\nA.1\nSelfCheckGPT (Manakul et al., 2023)\nOne of the first papers to counter zero-resource hal-\nlucination detection, we compare the SelfCheck-\nPrompt variant using LLaMA3.1-8B-Instruct and\nQwen2.5-7B-Instruct which is the best perform-\ning approach in their paper. Additionally we com-\npute the SelfCheck-MQAG scores as well (which\nis the QA-based variant). These are presented in\nTable 1. We set the number of stochastic samples\nto be generated as 20 (as mentioned in the original\npaper). The scoring method for SelfCheck-MQAG\nselected was Bayes with Alpha. Both β1 and β2\nwere set to 0.95.\nA.2\nSAC3 (Zhang et al., 2023a)\nAs discussed above, for using SAC3 as one of\nthe baselines, we evaluate it using the instruc-\ntion finetuned model version of LLaMA3.1-8B and\nQwen2.5-7B. We calculate the question-level con-\nsistency score (SAC3-Q) which is highlighted in\nthe original study as a score describing the cross-\ncheck consistency between 2 types of QA pairs,\ni) the original question and generated answer as a\npair and ii) a number of semantically similar gen-\nerated questions along with their answers as pairs.\nFor feasibility in accordance with our available\ncomputational resources, we experimented with\n2 generated perturbated QA pairs. This number\ncan be increased or varied to check for different\ncomparisons, but Zhang et al. (2023a) suggest that\nusing between 2 to 5 perturbed questions per data\nsample yields similar quantitative results.\nA.3\nHaDes (Liu et al., 2022)\nHaDeS is a novel token-free hallucination detec-\ntion dataset for free-form text generation. For the\ndataset creation, raw text from web data is per-\nturbed with out-of-box BERT model. Human an-\nnotators are then employed to assess whether the\nperturbed text spans are hallucinations given the\noriginal text. The final model is a binary classifier\nfor detecting hallucinated/non-hallucinated text.\nA.4\nINSIDE\n(Chen et al., 2024) INSIDE is a hallucination de-\ntection method which deals with the interal states\nof LLMs during generation to detect for hallucina-\ntions in outputs. Their approach utilizes the layer\nof sentence embedding outputs and exploits the\neigenvalues of the covariance matrix of outputs to\nmeasure consistency in the dense embedding space.\nThe define a particular score known as EigenScore,\nwhich is the logarithmic determinant of the covari-\nance matrix between a certain K number of outputs’\nsentence embeddings (to check for the consistency\nin the relationship of those K outputs’ embeddings).\nUsing it as a baseline, we implement it with our\nsettings with LLaMA3.1-8B and Qwen2.5-7B as the\nLLMs on the 3 QA datasets and calculate the AUC-\nPR scores.\nB\nUsage of ConFactCheck on datasets\nB.1\nOpen-Domain Question Answering\nThree datasets are used for this particular task, as\nshown above. We use ConFactCheck on the origi-\nnally generated outputs for each of the questions in\nthe datasets, to check for whether the LLMs gen-\nerating the original answers have hallucinated or\nnot. ConFactCheck is applied on a sentence-level\nbasis, where the outputs are split into sentences,\nfollowing which key facts are extracted and Con-\nFactCheck begins the checking mechanism.\nB.2\nText-based Summarization\nFor this particular task, we use the WikiBio dataset\nwhich contains summaries of individuals collected\nfrom Wikipedia, along with synthetic GPT3 gen-\nerated summaries of the same. ConFactCheck is\napplied as a sentence-level detector on the respec-\ntive sentences of each of the provided synthetic\nsummaries, which have be annotated with their hal-\nlucination labels at the said sentence-level as part\nof the dataset. We obtain sentence level halluci-\nnation scores and compare those with the golden\nannotate labels per sentence, and for passage-level\nhallucinations, we average over the sentence-level\nscores to get overall scores for passages.\n"}, {"page": 13, "text": "C\nF1-Score based Matching\nIn our primary pipeline, factual alignment is deter-\nmined using an LLM-as-a-judge approach. Specif-\nically, we query OpenAI’s GPT-4.1-mini via the\nAPI to compare extracted and regenerated facts and\nassign binary alignment labels. While this method\nyields strong performance, it requires reliable ac-\ncess to the OpenAI API and incurs associated com-\nputational and cost overheads.\nTo support use cases where API access is restricted\nor an external LLM judge is unavailable, we also\nexplore an alternative matching strategy based on\nsimple lexical overlap using F1-score. In this vari-\nant, alignment between fact pairs is determined by\ncomputing the F1-score of their token overlap, and\npairs exceeding a predefined threshold are marked\nas aligned. The table below presents the AUC-\nPR scores across three datasets using this heuristic\nmethod at various F1-score thresholds, where the\nM′ is LLaMA3.1-8B-Instruct (used for the fact\nregeneration). For this scoring, we split the extract\nand regenerated facts into lists of individual words,\nand compute the F1-scores on these lists. Different\nthresholds are used (as shown in Table 7 below) to\nassign 0/1 labels for similar/dissimilar facts.\nAlthough this approach is less semantically robust\nthan LLM-based judgment, it offers a lightweight,\nfully offline alternative that still provides reason-\nable scores that are close to the main scores in\nour pipeline, especially in resource-constrained set-\ntings.\nF1-score\nLLaMA3-NQopen\nLLaMA3-Hotpot\nLLaMA3-WebQA\n0.4\n0.640\n0.791\n0.550\n0.5\n0.648\n0.795\n0.556\n0.6\n0.659\n0.796\n0.556\n0.7\n0.662\n0.798\n0.562\n0.8\n0.664\n0.800\n0.570\nTable 7: F1-score based matching with different thresh-\nolds in fact alignment (ranging from 0.4 to 0.8)\nD\nPrompting Format\nPrompt Templates Used in the Pipeline\n1. Fact Regeneration Prompt (Manually\nConstructed Chat Format):\nThis prompt is used to generate fact-based\nquestions from the given sentence. The prompt\nfollows a constructed chat format, to be man-\nually customized for the model in use (e.g.,\nLLaMA3.1, Qwen2.5). It is used for each of\nthe questions generated by the T5-finetuned\nmodel on the extract key facts.\ni) Example format for LLaMA3-8B-Instruct:\n'''<|begin_of_text |><|\nstart_header_id|>system <|\nend_header_id|>\nYou are a Question -answering\nassistant , only answer the\nquestion.\n<|eot_id|><| start_header_id|>user <|\nend_header_id|>\nQuestion: <insert question here >\n<|eot_id|><| start_header_id|>\nassistant <| end_of_header_id|>'''\n2. Fact Alignment Prompt (used with the\njudge LLM):\nFew-Shot prompt used to check for align-\nment between extract and regenerated facts\nusing LLM-as-a-judge. This prompt is well-\nstructured to give the judge LLM complete un-\nderstanding of how to generate the alignment\noutput for the pairs of facts that it is applied\non.\n”’You are a fact comparison expert. Your task\nis to determine whether pairs of extracted and\nregenerated facts refer to the same real-world\nentity, concept, or meaning.\nFor each pair:\n- Return ‘0‘ if the two facts refer to the\nsame thing, even if the wording, specificity,\nor structure is different.\n- Return ‘1‘ if the two facts do not refer to\nthe same thing, or if their meanings conflict.\nGuidelines:\n- Minor differences in wording, grammar, or\ncapitalization should be ignored.\n- Partial vs full names (e.g., \"Vancouver\" vs\n\"Vancouver, British Columbia\") should match\nif they refer to the same entity.\n- Aliases and synonyms (e.g., \"Roger Pirates\"\nvs \"Roger crew\") should count as a match.\n- Abbreviations (e.g., \"UCLA\" vs \"University\nof California, Los Angeles\") are also matches.\n- Return ‘1‘ only if clearly unrelated or\nambiguous.\nFormat:\nReturn\na\nPython-style\nlist\nof\nexactly\n{n}\nbinary values (0 or 1), corresponding to each\nfact pair in order.\nDo not output anything else. If unsure, still\nreturn a complete list.\nExamples:\n• \"President Donald J. Trump\" vs \"Donald\nTrump\" →0\n• \"Vancouver,\nBritish\nColumbia\"\nvs\n\"Vancouver\" →0\n• \"five\" vs \"5 seasons\" →0\n"}, {"page": 14, "text": "• \"UCLA\" vs \"University of California, Los\nAngeles\" →0\n• \"Microsoft\" vs \"Apple\" →1\nNow judge the following fact pairs: {pairs}\nOutput: ”’\nFigure 3: Prompting templates used for Fact Regen-\neration and Fact Alignment in the CONFACTCHECK\npipeline. Note that the alignment prompt uses few-shot\nprompting.\nE\nAnnotation Performance of\nLLM-as-a-judge\nTo demonstrate the reliability of our LLM-based\njudging, we conducted a small-scale human evalu-\nation. We engaged two human annotators to label\n150 samples each across the three QA datasets.\nWhen comparing these human annotations with\nthe GPT-4o labels, we observed overlap scores\n(between GPT and Human annotators) ranging\nfrom 82.6% to 93%, indicating that the LLM is\ncapable of reliably generating accurate labels.\nFurthermore, we’ve calculated inter-annotator\nagreement metrics among the human annotators as\nwell. The Cohen’s Kappa scores range from 0.76\nto 0.91, which highlights substantial agreement\nand further corroborates the quality of our labels.\nWe will be adding these details in the appendix of\nthe submitted paper.\nDataset\nMetric\nValue\nNQOpen\nGPT Overlap (with Annotator 1)\n84%\nGPT Overlap (with Annotator 2)\n82.60%\nInter-Annotator Overlap\n96%\nInter-Annotator Cohen’s Kappa\n91.17%\nHotpotQA\nGPT Overlap (with Annotator 1)\n93%\nGPT Overlap (with Annotator 2)\n91%\nInter-Annotator Overlap\n91%\nInter-Annotator Cohen’s Kappa\n78%\nWebQ\nGPT Overlap (with Annotator 1)\n89%\nGPT Overlap (with Annotator 2)\n84%\nInter-Annotator Overlap\n88%\nInter-Annotator Cohen’s Kappa\n76%\nTable 8: GPT-human and inter-annotator overlap scores\nfor three QA datasets (150 samples).\nF\nCross-evaluation with different LLMs\nCONFACTCHECK provides additional flexibility\nwhen it comes to the usage of LLMs for de-\ntection in the pipeline.\nThe original LLM\nused to generate the initial output can be\nused for the Fact Alignment check in a self-\ncheck-based setting.\nHowever, while using\nCONFACTCHECK on the outputs of a particular\nbase LLM (eg.\nLLaMA3.1-8b-Instruct), we\ncan employ usage of another LLM for cross-\nevaluation during fact regeneration (eg: using\nQwen2.5-7b-Instruct on the initial LLaMA-\n3.1 outputs).\nWe provide experimental results\nto demonstrate the efficacy of cross-evaluation\nwhile using the 2 LLMs Qwen2.5-7b-Instruct\nand LLaMA3.1-8b-Instruct on two of the QA\ndatasets.\nMethod\nNQOpen\nWebQ\nQwen on LLaMA3 Outputs\n0.71\n0.63\nLLaMA3 as Self-Evaluator\n0.73\n0.66\nLLaMA3 on Qwen Outputs\n0.81\n0.70\nQwen as Self-Evaluator\n0.80\n0.71\nTable 9: AUC-PR scores comparing evaluator setups on\nNQOpen and WebQ datasets.\nG\nComparison of Selfcheck with varying\nnumber of samples\nThe SelfCheckGPT baseline methods provide con-\nfigurable flexibility in terms of the number of\nstochastic samples that are generated to provide\ntheir final scores. The authors suggest that the sam-\nples’ count can vary from 5 samples to 20 and\nprovide similarly comparable results. We have\nused 20 samples for generation using Selfcheck\nin the Table 1 of our paper. Here, we provide a\ndemonstration of results on the WebQA dataset\nwith LLaMA3.1-8b-Instruct as the base LLM,\nwhen samples are varied between 5 and 20 for the\nSelfcheck methods, and compare their AUC-PR\nscores and computational time metrics with each\nother and CONFACTCHECK.\nH\nJudge LLM vs Human evaluation in\nFact Alignment\nWe evaluate the reliability of LLM-based compar-\nison in the Fact Alignment Check of the pipeline\n"}, {"page": 15, "text": "Approach\nSample Size\nTime (s)\nAUC-PR\nSelfCheck-MQAG\n5 samples\n29.15\n0.51\n20 samples\n61.59\n0.50\nSelfCheck-Prompt\n5 samples\n8.4\n0.54\n20 samples\n14.1\n0.54\nConFactCheck\n2.8 avg facts + 1 API call\n8.77\n0.66\nTable 10: Performance comparison of Selfcheck meth-\nods with 5 and 20 samples each, along with latency\ncomparisons of these approaches with ConFactCheck.\nusing GPT-4.1-mini as the judge LLM. Each ex-\ntracted and regenerated fact pair is scored (0 for\naligned, 1 for not aligned) by the LLM. To validate\nits accuracy, we randomly sampled 25 instances\nfrom each of three QA datasets (75 in total) and\nobtained equivalent 0/1 judgments from a human\nevaluator for each individual facts within each of\nthe instances. Comparing the two sets of scores\nrevealed strong agreement, with accuracy between\n89.7%–93.2% and Cohen’s κ > 0.79. These re-\nsults offer strong evidence of the efficacy of the\nLLM-based comparison in our use case.\nDataset\nAgreement (%)\nCohen’s Kappa\nNQ-Open\n89.7\n0.783\nHotpotQA\n93.2\n0.845\nWebQA\n89.9\n0.799\nTable 11: Model (LLaMA-3) vs. human agreement\nscores on the evaluation of Fact Alignment samples\nacross three datasets.\nI\nStep-by-Step CONFACTCHECK\nExample\nExample: Question and Answer Processing\nStep-by-Step\nInput:\nQuestion: Who won the FIFA World\nCup in 2022?\nAnswer: The FIFA World Cup in 2022\nwas won by Argentina.\nStep 1: Extract sentences from the origi-\nnal answer\n• The sentence splitter extracts: \"The FIFA\nWorld\nCup\nin\n2022\nwas\nwon\nby\nArgentina.\"\nStep 2: Extract Key facts using NER\n• Named entities detected: “FIFA World\nCup”, “Argentina”, “2022”.\n• Generated questions using T5-finetuned\nmodel for each key fact:\n– FIFA World Cup →Q1: Which tour-\nnament did Argentina win in 2022?\n– Argentina →Q2: Who won the\nFIFA World Cup in 2022?\n– 2022 →Q3: When did Argentina\nwin the FIFA World Cup?\nStep 3: Generate pinpointed answers\n• Using the LM to answer the generated\nquestions:\nAnswers = [\"FIFA\nWorld\nCup\", \"Argentina\", “1978, 1986 and\n2022”]\nStep 4: Compare original and regener-\nated answers\n• Use\nHuggingface\nQA\npipeline\nto\nextract\nshortened\npinpointed\nanswers\nfrom\noriginal\nand\nregenerated contexts.\n• Judge if answers match (0 = match, 1 =\nhallucination):\nInitial hallucination flags = [0,\n0, 1]\nStep 5: Final hallucination check with\nprobability\n• Use token-level probabilities and\nKS-test to confirm hallucination.\n• Final hallucination flags remain:\n[0, 1, 1]\nFigure 4: Hypothetical step-by-step example explaining\nthe methodology of CONFACTCHECK\nJ\nPseudocode for the algorithm proposed\nThe hallucination detection algorithm is designed\nas a two-step process applied at the sentence level\nfor a generated answer. Given a generated answer\nA and a model M′, the goal is to produce a score\nfor each sentence indicating the likelihood of hal-\nlucination.\nIn the first step as highlighted in Algorithm 1, the\ngenerated answer is split into sentences, and each\n"}, {"page": 16, "text": "sentence is analyzed to extract atomic facts using\nNamed Entity Recognition (NER). For each key\nfact aij in sentence Si, a corresponding question qij\nis generated. The model M′ then provides an an-\nswer fij to this question. A separate Align function\n(which uses a judge LLM for fact pair comparison)\nevaluates whether the fact aij is consistent with the\nanswer fij. If aligned, the fact is marked as consis-\ntent (score 0), otherwise as hallucinated (score 1).\nThis step yields an initial binary score list for all\nfacts.\nIn Algorithm 2, for each fact marked as consistent\n(score 0) in Step 1, we compute the logit scores of\nthe top k tokens in the model’s answer fij. These\nscores are converted into a probability distribution.\nWe then perform a Kolmogorov–Smirnov (KS) test\nto statistically compare this empirical distribution\nagainst a uniform distribution. If the KS test yields\na p-value less than a significance threshold (typ-\nically 0.05), the null hypothesis — that the two\ndistributions are the same — is rejected. This indi-\ncates that the distribution is significantly different\nfrom uniform, and the fact remains marked as con-\nsistent (score 0). However, if the p-value is greater\nthan or equal to 0.05, the distribution is consid-\nered close to uniform, signaling high uncertainty\nin the model’s response, and in such case the fact\nis reclassified as hallucinated (score 1).\nAlgorithm 1 : Fact Alignment Check\n1: Input: Generated Answer A, Model M′\n2: Output: Initial Score List [sij] for all facts aij\n3: // Step 1: Sentence splitting and fact\nextraction\n4: Perform coreference resolution on A and split\ninto sentences {S1, S2, . . . , SN}\n5: for all sentence Si in A do\n6:\nExtract atomic facts {aij} from Si using\nNER\n7:\nfor all fact aij do\n8:\nGenerate question qij ←Q(aij | Si)\n9:\nGet answer fij ←M′(qij)\n10:\nif Align(fij, aij) then\n11:\nSet sij ←0\n▷Fact is consistent\n12:\nelse\n13:\nSet sij ←1 ▷Fact is hallucinated\n14:\nend if\n15:\nend for\n16: end for\n17: return [sij]\nAlgorithm 2 : Uniformity Check Phase (via KS\nTest)\n1: Input: Initial Score List [sij], Corresponding\nAnswer Logits sijk\n2: Output:\nFinal\nSentence\nScores\n[Score(S1), . . . , Score(SN)]\n3: for all sentence Si do\n4:\nInitialize Score(Si) ←0\n5:\nfor all fact aij in Si do\n6:\nif sij == 0 then\n7:\nCompute normalized probabilities:\np(wijk) =\nesijk\nPk\nm=1 esijm\n8:\n//\nCompare\nwith\nuniform\ndistribution\n9:\nPerform KS test between p(wijk)\nand uniform distribution\n10:\nif p-value ≥0.05 then\n11:\nSet sij ←1\n▷Mark as\nhallucinated\n12:\nend if\n13:\nend if\n14:\nAdd sij to Score(Si)\n15:\nend for\n16:\nNormalize: Score(Si) ←Score(Si)\n#facts in Si\n17: end for\n18: return [Score(S1), . . . , Score(SN)]\n"}]}