{"doc_id": "arxiv:2511.06215", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.06215.pdf", "meta": {"doc_id": "arxiv:2511.06215", "source": "arxiv", "arxiv_id": "2511.06215", "title": "Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease", "authors": ["Puzhen Su", "Yongzhu Miao", "Chunxi Guo", "Jintao Tang", "Shasha Li", "Ting Wang"], "published": "2025-11-09T04:01:45Z", "updated": "2025-11-09T04:01:45Z", "summary": "Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.06215v1", "url_pdf": "https://arxiv.org/pdf/2511.06215.pdf", "meta_path": "data/raw/arxiv/meta/2511.06215.json", "sha256": "1ef1f33e34a096cc462b8af7a0e010f25c793db803bd60c11991346a6f673dc5", "status": "ok", "fetched_at": "2026-02-18T02:28:07.670388+00:00"}, "pages": [{"page": 1, "text": "Explicit Knowledge-Guided In-Context Learning for\nEarly Detection of Alzheimer‚Äôs Disease\nPuzhen Su, Yongzhu Miao, Chunxi Guo, Jintao Tang*, Shasha Li*, Ting Wang*\nCollege of Computer Science and Technology, National University of Defense Technology, Changsha, China\n{supuzhen, miaoyz, chunxi, tangjintao, shashali, tingwang}@nudt.edu.cn\nAbstract‚ÄîDetecting Alzheimer‚Äôs Disease (AD) from narrative\ntranscripts remains a challenging task for large language models\n(LLMs), particularly under out-of-distribution (OOD) and data-\nscarce conditions. While in-context learning (ICL) provides a\nparameter-efficient alternative to fine-tuning, existing ICL ap-\nproaches often suffer from task recognition failure, suboptimal\ndemonstration selection, and misalignment between label words\nand task objectives, issues that are amplified in clinical domains\nlike AD detection. We propose Explicit Knowledge In-Context\nLearners (EK-ICL), a novel framework that integrates struc-\ntured explicit knowledge to enhance reasoning stability and task\nalignment in ICL. EK-ICL incorporates three knowledge com-\nponents: confidence scores derived from small language models\n(SLMs) to ground predictions in task-relevant patterns, parsing\nfeature scores to capture structural differences and improve\ndemo selection, and label word replacement to resolve semantic\nmisalignment with LLM priors. In addition, EK-ICL employs\na parsing-based retrieval strategy and ensemble prediction to\nmitigate the effects of semantic homogeneity in AD transcripts.\nExtensive experiments across three AD datasets demonstrate that\nEK-ICL significantly outperforms state-of-the-art fine-tuning and\nICL baselines. Further analysis reveals that ICL performance\nin AD detection is highly sensitive to the alignment of label\nsemantics and task-specific context, underscoring the importance\nof explicit knowledge in clinical reasoning under low-resource\nconditions.\nIndex\nTerms‚ÄîAlzheimer‚Äôs\nDisease,\nContextual\nLearning,\nLLMs, Explicit Knowledge.\nI. INTRODUCTION\nAlzheimer‚Äôs disease (AD) is a global health crisis, causing\nprofound cognitive decline and significant socio-economic\nimpact [1]. Early detection is critical for effective intervention,\nyet traditional diagnostic methods like cerebrospinal fluid\n(CSF) analysis [2] and neuroimaging [3] are invasive and\ncostly. In response, non-invasive, text-based approaches [4],\nthat analyze linguistic patterns have emerged as a promising\nalternative.\nCurrent AD detection methods predominantly rely on adapt-\ning small language models (SLMs) for specific tasks. While\neffective, SLMs methods are limited by their dependency\non extensive task-specific data, leading to over-specialization\nand pre-trained knowledge forgetting [5], which hampers\ngeneralization to unseen data, especially in out-of-distribution\n(OOD) scenarios. While data augmentation (DA) techniques\n[6] improve performance, they still rely heavily on data align-\nment and introduce bias due to random operation. In contrast,\nlarge language models (LLMs) such as LLaMA and GPT-4,\n*Corresponding author.\nUnable to Recognize \nOOD Task\nLLM\nü§Ø\nüòÑ\nTask Recognized\nTries to Replicate \nlabels from demos\nUtilizes Pre-Trained \nKnowledge \nTest #i\nRetrieved Semantic \nSimilar demos\nA\nAlzheimer\nB\nControl\nA\nB\nA\nCase #1\nCase #2\nCase #1:  Alzheimer's Disease Detection Task\nWhich one to replicate? \nThey look like the same!ü§î\nA\nLet's take a guess.\nB\nüòµüí´\nGround-Truth \nLabel\nLabel Words\nFig. 1. Schematic illustration of reasoning failure in ICL under out-of-\ndistribution (OOD) task scenarios.\nwith their remarkable in-context learning (ICL) capabilities,\noffer a promising solution. ICL enables LLMs to adapt to\nnew tasks with minimal data, using in-context demos without\nany parameter updates. However, existing ICL methods have\nunderperformed in AD detection, with even advanced models\nlike GPT-4 [7] failing to surpass traditional SLMs approaches.\nAs illustrated in Fig. 1, when LLMs encounter tasks they\nhave not been pre-trained for, the absence of semantic contrast\namong retrieved demos combined with semantically unrelated\nlabel words leads to unreliable reasoning, prompting LLMs to\nreplicate demo labels rather than engage in context-informed\ndecision making.\nSuch shortfall can be attributed to two interrelated factors,\ndemos quality and task recognition [8], which are keys to\neffective ICL. First, the quality of retrieved demos is limited\nin AD detection due to transcript-specific characteristics. Most\nICL methods construct input contexts by retrieving demos\neither randomly or based on semantic similarity. However,\nthese strategies struggle due to the semantic homogeneity of\ntranscripts, where both AD and control participants describe\nthe same picture. Additionally, as shown in Fig. 2, different\ncategories of transcripts exhibit distinct parsing structures,\nyet, conventional ICL methods fail to leverage such parsing\nvariation, limiting their ability to retrieve informative demos.\nSecond, effective task recognition depends on how well the\ntask objective is semantically aligned with both transcript\nand label words. In AD detection, the transcripts are de-\nscriptive narratives rather than direct indicators of cognitive\nimpairment, making it difficult to establish a strong semantic\nconnection with the task objective. Furthermore, the semantic\nmisalignment between label words and task can lead to severe\nperformance drops in ICL [9]. Thus, when conventional label\nwords fail to align with the nuanced linguistic features of AD\narXiv:2511.06215v1  [cs.CL]  9 Nov 2025\n"}, {"page": 2, "text": "Actions\nObjects\nFiller\nLocations\nSubject\nPronoun\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nAvg. Category Frequency\nTraining Set\nTotal\nAlzheimer\nControl\nActions\nObjects\nFiller\nLocations\nSubject\nPronoun\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nAvg. Category Frequency\nTesting Set\nTotal\nAlzheimer\nControl\nActions\nObjects\nFiller\nLocations\nSubject\nPronoun\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nAvg. Category Frequency\nPitt Set\nTotal\nAlzheimer\nControl\nActions\nObjects\nFiller\nLocations\nSubject\nPronoun\n0\n5\n10\n15\n20\nAvg. Category Frequency\nLu Set\nTotal\nAlzheimer\nControl\nActions\nObjects\nFiller\nLocations\nSubject\nPronoun\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nAvg. Category Frequency\nTotal Set\nTotal\nAlzheimer\nControl\nFig. 2. Parsing category distributions across datasets in AD detection. This figure compares the average frequency of six syntactic categories‚ÄîActions,\nObjects, Fillers, Locations, Subjects, and Pronouns‚Äîacross the ADReSS challenge-Train, ADReSS challenge-Test, Lu, Pitt, and Total datasets.\ntranscripts, the difficulty of task recognition exacerbates.\nTo address the challenges of existing ICL methods, we\npropose EK-ICL (Explicit Knowledge In-Context Learners),\na novel framework designed to enhance LLMs reasoning in\nAD detection tasks. EK-ICL integrates three forms of explicit\nknowledge: 1) Confidence scores, derived from SLM logits,\nwhich provide reliable insights into prediction and stabilize\ntask recognition; 2) Parsing feature scores, which highlight\nlinguistic differences between samples and serve as the basis\nof improving demo retrieval quality that captures structural\ndistinctions; and 3) In-distribution (ID) label words, which\nhave been investigated for this task, and replacing traditional\ntask-specific label words with ID labels to better align with\nthe pre-trained knowledge of LLMs. Additionally, EK-ICL\nemploys an ensemble-based ICL strategy that aggregates pre-\ndictions through majority voting to mitigate label conflict. We\ndemonstrate that EK-ICL effectively addresses the limitations\nof existing ICL methods and outperforms SOTA SLMs and\nLLMs methods across three AD detection datasets. Our con-\ntributions can be summarized as follows:\n‚Ä¢ We propose EK-ICL, a novel ICL framework for AD\ndetection that integrates explicit knowledge to enhance\ntask recognition and demo selection.\n‚Ä¢ EK-ICL captures AD-specific linguistic patterns via\nSLM-derived\nconfidence\nand\nparsing-based\nfeature\nscores, improving reasoning under OOD and low-\nresource settings.\n‚Ä¢ We conduct a systematic study on label word configu-\nrations, revealing that task-label alignment is critical for\nrobust ICL in clinical AD detection.\nII. METHODS\nWe propose EK-ICL, a framework that enhances AD detec-\ntion by integrating explicit knowledge into ICL. As shown in\nFig. 3, EK-ICL improves task recognition, demo quality, and\nlabel-task alignment by incorporating three forms of explicit\nknowledge obtained from explicit knowledge initialization.\nFirst, confidence scores (Sconf) are derived from SLMs logits\nand align the transcript with the task, addressing task recog-\nnition issues. Second, parsing feature scores (Sfeat) capture\nsyntactic differences between samples, which improves demo\nquality and task alignment by prioritizing structural similar-\nParsing \nSearch\nEK-Initialized \nTest Transcript\nEK-Initialized \nTraining Dataset\nTest\ndemos\nICL Pair \nExplicit Knowledge\nConstructing \nICL Pairs\nEnsemble\nStep 1:  \nExplicit Knowledge Initialization\nTraining \nDataset\nSLM \nAssessor\nSupervised \nKnowledge\nExtract \nModule\nTrained \nModule\nConstruct\nParsing \nDecomposer\nFeature Score\nS\nLogits\nL\nLabel Replacement\nR\nFeature Score\nS\nLogits\nL\nLabel Replacement\nR\nüî•\nStep 2:  \nEK-Initialized demo Retrieval\nStep 3:  \nReasoning\nLearners \nVoting\nFig. 3. Overview of the proposed EK-ICL framework.\nity. Finally, ID label replacement substitutes traditional task-\nspecific labels (i.e., Alzheimer/Control), with in-distribution\nlabels (i.e., Good/Bad), ensuring better alignment with the\ntask. These components are further complemented by pars-\ning search, which enhances demo selection by leveraging\nparsing similarity, and ensemble in-context learners, which\naggregates predictions from multiple learners using majority\nvoting, addressing label conflict and stabilizing predictions.\nThe following sections describe each component in detail.\nA. Explicit Knowledge Initialization\na) SLM Assessor (EK-I Contribution Scores): To quan-\ntify model confidence and stabilize reasoning, we incorporate\na lightweight SLM with two modules: a noise Injector that\nperturbs token representations and a contribution Judger that\nevaluates token-level contributions.\nGiven a token representation ei in transcript Tj, the Injector\nproduces a noise representation e‚Ä≤\ni using the Gumbel-Softmax\ntrick [10].\n"}, {"page": 3, "text": "The Judger then assigns a token-specific contribution\nweight pi via a linear probe with sigmoid activation:\npi = œÉ\n\u0010\nw‚ä§ei + b\n\u0011\n,\npi ‚àà(0, 1).\n(1)\nFinally, the perturbed token is a convex combination of the\noriginal and noise representations:\nÀúei = pi ei + (1 ‚àípi) e‚Ä≤\ni.\n(2)\nThe final text representation, ÀúE = {Àúe1, Àúe2, ¬∑ ¬∑ ¬∑ , Àúen}, is\naggregated using a global max-pooling operation. The logits\nof the SLM, used as confidence scores Sconf, are computed as:\nz = GlobalMaxPooling(ÀúE),\n(3)\nSconf = Sigmoid(MLP(z)),\n(4)\nwhere MLP is the multi-layer perceptron. The trained Judger\noutputs are also used to compute feature scores Sfeat and\nparsing similarities.\nb) Parsing Decomposer (EK-II Feature Scores): The\nparsing decomposer, extracts syntactic structure-based knowl-\nedge from AD transcripts, providing explicit parsing feature\nscores Sfeat. Differs from previous retrieval approaches that\nrely on semantic similarity, our method quantifies the con-\ntribution of different parsing categories to improve structural\ndifferentiation between AD and HC groups. To achieve this,\nwe define six key parsing categories, including Subject, Object,\nAction, Location, Filler, and Pronoun.\nGiven a transcript Tj with token-level representations Ej =\n{e1, e2, ¬∑ ¬∑ ¬∑ , en}, we first compute the parsing contribution\nweight œâk\nj for each category Catk:\nœâk\nj =\nn\nX\ni=1\npi ¬∑ I(Cat(ei) = Catk),\n(5)\nwhere I(¬∑) is the indicator function, determining if token ei\nbelongs to category Catk, and pi is the output of the Judger\nmodule, indicating the token‚Äôs contribution.\nNext, we define the contribution array R(Tj) for each\ntranscript Tj:\nRj = {œâa\nj , ¬∑ ¬∑ ¬∑ , œâb\nj|œâa\nj > œâb\nj}.\n(6)\nTo establish a global ranking, we compute the standard\ncontribution array Rstd across the training dataset D:\nRstd = {¬Øœâa, ¬∑ ¬∑ ¬∑ , ¬Øœâb | ¬Øœâa > ¬Øœâb},\n¬Øœâk =\n1\n|D|\nX\nx‚ààD\nnx\nX\ni=1\npi ¬∑ I (Cat(ei) = Catk) ,\n(7)\nand then, we can derive the standard category rank:\nCstd = arg sort(Rstd).\n(8)\nFinally, the feature scores Sfeat for transcript Tj is computed\nas a weighted sum:\nSfeat = Rstd(Tj) ¬∑ Softmax(Rstd),\n(9)\nhere, Rstd(Tj) represents the category-wise contribution array\nof Tj, ordered according to Cstd.\nBy computing these parsing-based feature scores Sfeat, we\nprovide explicit structural knowledge that are later used in\nparsing search to improve demo selection. These scores help\ndistinguish between AD and HC transcripts by emphasizing\nparsing variations rather than relying on semantic similarity.\nc) In-Distribution Labels Replacement (EK-III ID Label\nWords): Noticing label sensitivity in OOD tasks, we replace\ndefault labels (i.e., Alzheimer/Control) with ID labels (i.e.,\nGood/Bad), aligning with the LLMs‚Äô pre-trained knowledge.\nThis alignment enables LLMs to better leverage their pre-\ntrained understanding of universal concepts, mitigating per-\nformance degradation caused by mismatched label semantics\nin OOD scenarios.\nB. Parsing Search and Ensemble Prediction\na) Parsing Search: Unlike current ICL methods that\nretrieve demos based on semantic similarity, we introduce\nparsing similarity, which better captures structural differ-\nences in transcripts. Specifically, for a given target tran-\nscript Tj and a retrieved candidate Tk, we define their cat-\negory rank as C(Tj) = {cj,1, cj,2, ¬∑ ¬∑ ¬∑ , cj,6} and C(Tk) =\n{ck,1, ck,2, ¬∑ ¬∑ ¬∑ , ck,6}, where each element represents the fre-\nquency of a parsing category in the respective transcript.\nThe parsing similarity between Tj and Tk is then computed\nas:\nSim(Tj, Tk) = Œª1 ¬∑ (1 ‚àíND(C(Tj), C(Tk)))\n+ Œª2 ¬∑ Cos(R(Tj), Rj(Tk)),\n(10)\nwhere Cos(¬∑, ¬∑) is the cosine similarity, ND(¬∑, ¬∑) is the nor-\nmalized position distance and Œª1 = Œª2 = 0.5 are weighting\ncoefficients. The order of Rj(Tk) is aligned with rank C(Tj):\nND(Tj, Tk) =\n6\nX\ni=1\n|(œÑ(C(Tj), i) ‚àíœÑ(C(Tk), i)|\ni\n,\n(11)\nwhere the index function œÑ : C ‚ÜíR+ assigns an index value\nto each element c ‚ààC, œÑ(C(Tj), i) = 7‚àíi and œÑ(C(Tk), i) =\n7 ‚àít, where t s.t. ck,i = cj,t.\nb) Ensemble In-Context Learners: To further enhance\nreasoning stability and mitigate label conflict, we introduce\nan ensemble learning strategy. Each learner processes a single\ndemo retrieved via parsing similarity, and the final prediction\nis obtained via 3 learners majority voting.\nIII. EXPERIMENT\nA. Setup\n1) Datasets: We evaluated the proposed EK-ICL frame-\nwork on three standard AD detection datasets: ADReSS chal-\nlenge dataset [11], Lu [12] and Pitt corpora [13].\nAll datasets consist of transcripts formatted according to the\nCHAT protocol [14], ensuring consistency in pre-processing\nand facilitating cross-study comparisons.\n2) Baselines: To comprehensively evaluate the performance\nof EK-ICL, we compared it with a range of baselines1.\n1SLM methods are based on bert-base-uncased, and LLM methods are\nbased on llama3.1-8B-instruct. All experiments are conducted on one NVIDIA\nQuadro RTX 8000 48G GPU.\n"}, {"page": 4, "text": "TABLE I\nMAIN RESULTS ON AD DETECTION ACROSS THREE DATASETS. PERFORMANCE COMPARISON OF VARIOUS FINE-TUNING, ICL, AND OUR EK-ICL\nMETHOD ON THE TEST, LU, AND PITT. METRICS INCLUDE ACCURACY (ACC), PRECISION (PRE), RECALL (REC), AND F1-SCORE.\nMethod\nShot\nTest (%)\nLu (%)\nPitt (%)\nAcc\nPre\nRec\nF1\nAcc\nPre\nRec\nF1\nAcc\nPre\nRec\nF1\nBERTc\nN/A\n81.25\n94.11\n66.67\n78.05\n83.33\n85.71\n88.89\n87.27\n70.12\n82.87\n58.49\n68.58\nBERTf\nN/A\n81.25\n82.61\n79.17\n80.85\n80.95\n80.64\n92.59\n86.21\n77.05\n85.16\n71.24\n77.58\nSPZ\nN/A\n91.66\n95.45\n87.50\n91.30\n83.33\n83.33\n92.59\n87.72\n77.96\n88.07\n69.93\n77.96\nLoRAcls\nN/A\n60.40\n55.81\n100\n71.64\n64.29\n65.00\n96.29\n77.61\n57.92\n57.19\n97.39\n72.07\nLoRAgen\nN/A\n41.67\n43.33\n54.17\n48.15\n47.62\n60.87\n51.85\n56.00\n51.18\n57.19\n49.35\n52.98\nICLV an\n0\n52.08\n55.55\n20.83\n30.30\n38.09\n54.54\n22.22\n31.58\n47.36\n56.30\n24.84\n34.47\n1\n64.58\n73.33\n45.83\n56.41\n45.23\n70.00\n25.93\n37.84\n50.27\n61.38\n29.08\n39.47\n2\n58.33\n61.11\n45.83\n52.38\n42.86\n58.82\n37.04\n45.45\n48.63\n56.90\n32.35\n41.25\n3\n52.08\n52.63\n41.66\n46.51\n59.52\n75.00\n55.56\n63.83\n55.74\n65.52\n43.46\n52.26\nICLSem\n1\n50.00\n50.00\n37.50\n42.86\n42.86\n63.64\n25.30\n36.84\n45.36\n51.63\n31.05\n38.78\n2\n45.83\n44.44\n33.33\n38.09\n38.09\n53.33\n29.63\n38.09\n46.08\n51.96\n43.14\n47.14\n3\n37.50\n39.29\n45.83\n42.31\n42.85\n57.14\n44.44\n50.00\n49.73\n55.55\n49.02\n52.08\nICLLog\n0\n52.08\n100.00\n4.17\n8.00\n40.48\n75.00\n11.11\n19.35\n49.73\n100.00\n9.80\n17.86\n1\n52.08\n100.00\n4.17\n8.00\n38.10\n100.00\n3.70\n7.14\n44.44\n51.85\n4.58\n8.41\n2\n52.08\n60.00\n12.05\n20.69\n40.48\n66.67\n14.81\n24.24\n47.72\n67.27\n12.09\n20.50\n3\n56.25\n64.29\n37.50\n47.37\n52.38\n70.59\n44.44\n54.55\n53.37\n65.62\n34.31\n45.06\nICLEns\n0\n45.83\n40.00\n16.67\n23.53\n45.24\n70.00\n25.93\n37.84\n45.54\n52.29\n26.14\n34.86\n1\n52.08\n54.54\n24.00\n34.28\n40.48\n62.50\n18.51\n28.57\n47.18\n54.49\n31.69\n40.08\n2\n43.75\n42.28\n37.50\n40.00\n33.33\n47.37\n33.33\n39.13\n43.53\n49.15\n37.90\n42.80\n3\n47.92\n48.15\n54.17\n50.98\n33.33\n47.62\n37.03\n41.67\n53.55\n59.62\n51.63\n55.34\nOurs (EK-ICL)\n1\n93.75\n100.00\n87.50\n93.33\n88.09\n92.31\n88.89\n90.57\n80.51\n94.61\n68.95\n79.77\nTABLE II\nABLATION RESULTS FOR THE EK-ICL ACROSS TEST DATASET, LU, AND PITT CORPORA. THE SYMBOL (-) INDICATES THAT RECALL AND F1 COULD\nNOT BE COMPUTED DUE TO A DIVISION-BY-ZERO SCENARIO.\nMethod\nTest\nLu\nPitt\nAcc\nPre\nRec\nF1\nAcc\nPre\nRec\nF1\nAcc\nPre\nRec\nF1\nOurs\n93.75\n100\n87.50\n93.33\n88.09\n92.31\n88.89\n90.57\n80.51\n94.61\n68.95\n79.77\nw/o Confidence Scores\n50.00\n100\n-\n-\n38.09\n100\n3.70\n7.14\n44.26\n50.00\n0.60\n1.29\nw/o Features Scores\n87.50\n100\n75.00\n85.71\n76.19\n84.00\n77.78\n80.77\n71.76\n91.26\n54.57\n68.30\nw/o Parsing Search\n83.33\n100\n66.67\n80.00\n83.33\n91.67\n81.48\n86.27\n71.22\n91.57\n53.26\n67.35\na) Fine-tuning Methods: 1) BERT (coarse-grained) [15]:\nUtilizes the CLS token for classification; 2) BERT (fine-\ngrained) [15]: Extracts representations via GlobalMaxPooling;\n3) SPZ [6]: Employs semantic perturbations and zonal-mixing\nto enhance robustness; 4) LoRA (CLS) [16]: Applies low-rank\nadaptation for parameter-efficient fine-tuning on LLM under\nclassification task; 5) LoRA (GEN) [16]: Applies low-rank\nadaptation for parameter-efficient fine-tuning on LLM under\ntext generation task.\nb) ICL Methods: 6) Vanilla ICL: Basic ICL using ran-\ndomly selected demonstrations; 7) Semantic ICL: Incorporates\nsemantically similar demonstrations to improve prediction\naccuracy; 8) Logits ICL [17]: Utilizing the logits of SLM to\nassist LLM; 9) Ensemble ICL [18]: Combines multiple in-\ncontext learners and use the majority vote to obtain the final\nclassification.\n3) Evaluation Metrics: We employed standard classifica-\ntion metrics (Accuracy, Precision, Recall, and F1-Score) to\nevaluate model performance. Results are reported across both\nbalanced and imbalanced datasets to assess robustness and\ngeneralization.\nB. Main Results\nWe evaluate EK-ICL for AD detection in both data-scarce\nand OOD scenarios, against FPFT, PEFT, and multiple ICL\nparadigms.\nFPFT on SLMs consistently achieve strong performance,\nbut PEFT on LLMs cannot. FPFT approaches such as\nSPZ yield top-tier results: 91.66%/83.33%/77.96% accuracy\non Test, Lu, and Pitt respectively. This demonstrates the\ncapacity of SLMs, when fully fine-tuned, to capture fine-\ngrained linguistic cues crucial for AD detection. By contrast,\nPEFT methods on LLMs (LoRAcls) perform markedly worse\n(e.g., 60.40% accuracy on Test), and text generation, ori-\nented PEFT (LoRAgen) further degrades. These results reveal\nthe limitations of parameter-efficient tuning for subtle, low-\nresource clinical tasks.\nStandard ICL methods struggle in this domain. The best\nvanilla ICL (shot = 1) achieves only 64.58%/50.00%/50.27%\naccuracy on Test, Lu, and Pitt, with ICLSem (semantic re-\ntrieval) performing even worse due to the semantic homogene-\nity of AD transcripts. Extensions leveraging logits (ICLLog) or\nensemble voting (ICLEns) bring no consistent gains, confirm-\ning that isolated optimization (logits, ensembles, or semantic\n"}, {"page": 5, "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\nShot\n40\n45\n50\n55\n60\n65\nAccuracy (%)\nTest Dataset - Accuracy\nVanilla ICL - Accuracy (OOD)\nVanilla ICL - Accuracy (ID)\nSemantic ICL - Accuracy (OOD)\nSemantic ICL - Accuracy (ID)\n0\n1\n2\n3\n4\n5\n6\n7\n8\nShot\n40\n45\n50\n55\n60\n65\n70\n75\nPrecision (%)\nTest Dataset - Precision\nVanilla ICL - Precision (OOD)\nVanilla ICL - Precision (ID)\nSemantic ICL - Precision (OOD)\nSemantic ICL - Precision (ID)\n0\n1\n2\n3\n4\n5\n6\n7\n8\nShot\n20\n30\n40\n50\n60\n70\n80\nRecall (%)\nTest Dataset - Recall\nVanilla ICL - Recall (OOD)\nVanilla ICL - Recall (ID)\nSemantic ICL - Recall (OOD)\nSemantic ICL - Recall (ID)\n0\n1\n2\n3\n4\n5\n6\n7\n8\nShot\n30\n35\n40\n45\n50\n55\n60\nF1 Score (%)\nTest Dataset - F1 Score\nVanilla ICL - F1 (OOD)\nVanilla ICL - F1 (ID)\nSemantic ICL - F1 (OOD)\nSemantic ICL - F1 (ID)\n(a)\n0\n1\n2\n3\nShot\n0\n20\n40\n60\n80\n100\nF1 Score (%)\nOOD Labels\nLog-Test\nEns-Test\nLog-Lu\nEns-Lu\nLog-Pitt\nEns-Pitt\n0\n1\n2\n3\nShot\n0\n20\n40\n60\n80\n100\nID Labels\n0\n1\n2\n3\nShot\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nOOD Labels\n0\n1\n2\n3\nShot\n0\n20\n40\n60\n80\n100\nID Labels\n(b)\nFig. 4.\nComparative performance of ICL baselines under OOD/ID label-word settings. (a) ICLVan and ICLSem on the Test dataset. (b) ICLLog and\nICLEns on three datasets.\nAbnormal/Normal\nAD/Non-AD\nNegative/Positive\nRelated/Unrelated\nTerrible/Great\nAlzheimers/Healthy\nCogDem/CogCtrl\nCogImp/CogCtrl\nDementia/Non-Dem\nEarlyAD/AgeCtrl\nGood/Bad\nGood/Abnormal\nGood/Terrible\nGood/Negative\nGood/Unrelated\nGood/AD\nGood/Alzheimers\nGood/CogDem\nGood/CogImp\nGood/Dementia\nGood/EarlyAD\nBad/Good\nBad/Normal\nBad/Great\nBad/Positive\nBad/Related\nBad/Non-AD\nBad/Control\nBad/Healthy\nBad/CogCtrl\nBad/Non-Dem\nBad/AgeCtrl\n0\n20\n40\n60\n80\n100\nScore (%)\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nAligned\nFixed Good\nFixed Bad\nAcc\nPre\nRec\nF1\nFig. 5. Performance of EK-ICL on the Test set across 30 label word pairs categorized into three configurations: Aligned, Fixed Good, and Fixed\nBad. Gray hatched bars indicate values that are not applicable due to division by zero.\nretrieval alone) is insufficient for robust OOD adaptation.\nEK-ICL\novercomes\nthese\nlimitations\nby\nintegrat-\ning explicit knowledge into ICL. Our framework uni-\nfies SLM-derived confidence scores, parsing feature scores,\nand\nID\nlabel\nreplacement\n(e.g.,\nGood/Bad),\nreinforced\nwith a parsing-based demo retrieval and ensemble majority\nvoting. This multi-faceted design ensures both task‚Äìlabel\nand context‚Äìtask alignment. 93.75%/88.09%/80.51% accuracy\n(93.33%/90.57%/79.77% F1) on Test, Lu, and Pitt. These\nresults highlight the necessity of simultaneously integrating\nexplicit knowledge, structured retrieval, and label alignment\nfor reliable LLM reasoning in AD detection. Our ablation\nconfirms that none of these components alone, or in naive\npairwise combination, can yield comparable gains.\nC. Ablation Study\nOur ablation study (shown Tab. II), conducted under ID\nlabel conditions, reveals several key findings regarding the\nroles of SLM-logits based and parsing based EK modules in\nEK-ICL:\nSLM logits are indispensable. Removing the confidence\nscores leads to a dramatic drop in accuracy, highlighting their\ndecisive and foundational role in calibrating LLM predictions.\nNevertheless, as observed in the main results for ICLLog,\nconfidence scores alone do not yield stable improvements as\ndemo counts increases.\nParsing-based feature scores and search provide critical\nauxiliary benefits. While not as dominant as logits, parsing\nfeature scores help the model capture fine-grained structural\ncues, and parsing-based demo search further improves retrieval\nquality by prioritizing syntactic diversity over semantic sim-\nilarity. Removing either module results in clear performance\ndegradation, confirming their additive and synergistic contri-\nbutions.\nThe effectiveness of EK is fundamentally constrained\nby label word selection. However, even with optimal EK\nintegration, model performance is highly sensitive to the\nchoice and alignment of label words. This is especially evident\nfrom the instability of ICLLog (see Tab. I) in conventional label\nsetting. These observations motivate our subsequent in-depth\nanalysis: a systematic investigation of how different ID and\nOOD label word combinations, both aligned and non-aligned,\naffect EK-ICL performance.\nD. In-depth Analysis of Label Word Semantics\nTo further understand how different ICL methods behave\nacross diverse label words, we conducted extensive compar-\native analyses on the Test dataset (see Fig. 4 and\n5). Our\nanalyses yield three key insights:\nID label words alone are insufficient without EK. As\nillustrated in Fig. 4a, simply adopting ID-aligned labels (e.g.,\nGood/Bad) does not significantly enhance the performance\nof vanilla and semantic ICL methods in the absence of EK.\n"}, {"page": 6, "text": "These methods continue to struggle, indicating that semantic\nalignment with pre-trained LLM knowledge alone cannot\nadequately address the complex linguistic and structural chal-\nlenges inherent to AD detection.\nLogits and ensemble methods show limited effectiveness\nunder OOD labels. ICLLog and ICLEns methods under\nOOD label conditions exhibit consistently poor and stagnant\nperformance, even with increasing demo counts (see Fig. 4b).\nUnder ID conditions, while the ensemble method remains\nineffective, logits-based ICL initially achieves notable per-\nformance improvements at low demo counts. However, this\nadvantage diminishes as more demos are introduced. This\nbehavior underscores that confidence scores alone, without\nthe additional structured parsing-based features and retrieval\nprovided by EK-ICL, lack the stability needed for robust ICL\nperformance in OOD and resource-scarce settings.\nEK-ICL performance hinges on LLM-informed label\npair alignment. As shown in Fig. 5, EK-ICL achieves its\nstrongest performance with the Good/Bad label pair, while\nother semantically aligned or clinically reasonable pairs (e.g.,\nAD/Non-AD) perform inconsistently or collapse. This suggests\nthat effective label design is not merely about logical polarity\nor domain relevance, but about aligning with LLMs‚Äô pre-\ntrained semantic biases. In the Aligned and Fixed Good\nsettings, performance is often dominated by the positive label,\nindicating LLMs‚Äô preference for positive labels. In contrast,\nthe Fixed Bad setting yields more stable and discriminative re-\nsults across multiple pairings (e.g., Bad/Healthy, Bad/Control),\nimplying that negative anchors like Bad may provide stronger\ntask-relevant separation in AD detection. These results high-\nlight that label effectiveness in OOD ICL settings depends\non compatibility with LLM priors, which may deviate from\nhuman judgment. A model-aware, empirically guided approach\nto label selection is therefore essential.\nIV. CONCLUSION\nWe propose EK-ICL, a novel in-context learning framework\ntailored for AD detection in data-scarce and OOD settings.\nEK-ICL explicitly injects three types of structured knowledge\nto overcome task recognition failure: (1) confidence scores de-\nrived from SLMs provide stable alignment between transcript\nand task objective; (2) parsing feature scores capture struc-\ntural variations across transcripts, guiding informative demon-\nstration selection; and (3) label word replacement aligns label\nsemantics with LLM priors to mitigate label-task mismatch.\nExtensive experiments on three AD corpora show that EK-\nICL outperforms both fine-tuning and standard ICL baselines.\nFurther analysis reveals that robust AD detection requires joint\nalignment across label, context, and structure‚Äîdependencies\nthat cannot be satisfied by any single knowledge source alone.\nOur findings underscore the importance of explicit knowledge\nin enabling LLMs to reason reliably in clinical OOD scenarios.\nACKNOWLEDGMENT\nThis work was supported by the Key Research and De-\nvelopment Project of Hunan Province (No. 2025JK2119),\nFoundation of NUDT (HQKYZH2025KD004).\nREFERENCES\n[1] B. J. Kelley and R. C. Petersen, ‚ÄúAlzheimer‚Äôs disease and mild cognitive\nimpairment,‚Äù Neurologic Clinics, vol. 25, no. 3, pp. 577‚Äì609, 2007.\n[2] L. M. Shaw, H. Vanderstichele, M. Knapik-Czajka, and et al., ‚ÄúCere-\nbrospinal fluid biomarker signature in alzheimer‚Äôs disease neuroimaging\ninitiative subjects,‚Äù Annals of Neurology, vol. 65, no. 4, pp. 403‚Äì413,\n2009.\n[3] R. C. Petersen, P. S. Aisen, L. A. Beckett, and et al., ‚ÄúAlzheimer‚Äôs dis-\nease neuroimaging initiative (adni) clinical characterization,‚Äù Neurology,\nvol. 74, no. 3, pp. 201‚Äì209, 2010.\n[4] A.\nRoshanzamir,\nH.\nAghajan,\nand\nM.\nSoleymani\nBaghshah,\n‚ÄúTransformer-based\ndeep\nneural\nnetwork\nlanguage\nmodels\nfor\nalzheimer‚Äôs disease risk assessment from targeted speech,‚Äù BMC\nMedical Informatics and Decision Making, vol. 21, pp. 1‚Äì14, 2021.\n[5] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,\nG. Slabaugh, and T. Tuytelaars, ‚ÄúA continual learning survey: Defying\nforgetting in classification tasks,‚Äù IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 44, no. 7, pp. 3366‚Äì3385, 2021.\n[6] F. Li, C. Huang, P. Su, and et al., ‚ÄúSpz: A semantic perturbation-\nbased data augmentation method with zonal-mixing for alzheimer‚Äôs\ndisease detection,‚Äù in Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\n2024, pp. 15 429‚Äì15 439.\n[7] B. B. T and J.-M. Chen, ‚ÄúPerformance assessment of chatgpt vs bard\nin detecting alzheimer‚Äôs dementia,‚Äù arXiv, 2024. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2402.01751\n[8] A.\nZhao,\nF.\nYe,\nJ.\nFu,\nand\nX.\nShen,\n‚ÄúUnveiling\nin-context\nlearning: A coordinate system to understand its working mechanism,‚Äù\narXiv\npreprint\narXiv:2407.17011,\noct\n2024.\n[Online].\nAvailable:\nhttps://doi.org/10.48550/arXiv.2407.17011\n[9] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen\net al., ‚ÄúLarger language models do in-context learning differently,‚Äù\nArXiv\npreprint,\nvol.\nabs/2303.03846,\n2023.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2303.03846\n[10] E.\nJang,\nS.\nGu,\nand\nB.\nPoole,\n‚ÄúCategorical\nreparameterization\nwith gumbel-softmax,‚Äù in 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nOpenReview.net, 2017. [Online].\nAvailable: https://openreview.net/forum?id=rkE3y85ee\n[11] S. Luz, F. Haider, S. de la Fuente, D. Fromm, and B. MacWhinney,\n‚ÄúAlzheimer‚Äôs dementia recognition through spontaneous speech: The\nadress challenge,‚Äù in Interspeech 2020, 21st Annual Conference of\nthe International Speech Communication Association, Virtual Event,\nShanghai, China, 25-29 October 2020, H. Meng, B. Xu, and T. F.\nZheng, Eds.\nISCA, 2020, pp. 2172‚Äì2176. [Online]. Available:\nhttps://doi.org/10.21437/Interspeech.2020-2571\n[12] A. M. Lanzi, A. K. Saylor, D. Fromm, H. Liu, B. MacWhinney,\nand M. Cohen, ‚ÄúDementiabank: Theoretical rationale, protocol, and\nillustrative analyses,‚Äù American Journal of Speech-Language Pathology,\n2023.\n[13] J. T. Becker, F. Boiler, O. L. Lopez, J. Saxton, and K. L. McGonigle,\n‚ÄúThe natural history of alzheimer‚Äôs disease: description of study cohort\nand accuracy of diagnosis,‚Äù Archives of Neurology, vol. 51, no. 6, pp.\n585‚Äì594, 1994.\n[14] B. MacWhinney, The CHILDES project: The database.\nPsychology\nPress, 2000, vol. 2.\n[15] A. Balagopalan, B. Eyre, J. Robin, F. Rudzicz, and J. Novikova,\n‚ÄúComparing pre-trained and feature-based models for prediction of\nalzheimer‚Äôs disease based on speech,‚Äù Frontiers in aging neuroscience,\nvol. 13, p. 635945, 2021.\n[16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, ‚ÄúLora: Low-rank adaptation of large language models,‚Äù\nin International Conference on Learning Representations, 2022.\n[17] M.\nM.\nMojarradi,\nL.\nYang,\nR.\nMcCraith,\nand\nA.\nMahdi,\n‚ÄúImproving in-context learning with small language model ensembles,‚Äù\nArXiv\npreprint,\nvol.\nabs/2410.21868,\n2024.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2410.21868\n[18] S. Min, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, ‚ÄúNoisy channel\nlanguage model prompting for few-shot text classification,‚Äù in Proceed-\nings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nDublin, Ireland: Association for\nComputational Linguistics, 2022, pp. 5316‚Äì5330.\n"}]}