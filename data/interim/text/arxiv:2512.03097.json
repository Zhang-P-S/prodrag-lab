{"doc_id": "arxiv:2512.03097", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.03097.pdf", "meta": {"doc_id": "arxiv:2512.03097", "source": "arxiv", "arxiv_id": "2512.03097", "title": "Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare", "authors": ["Adeela Bashir", "The Anh han", "Zia Ush Shamszaman"], "published": "2025-12-01T12:17:28Z", "updated": "2025-12-01T12:17:28Z", "summary": "The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.03097v1", "url_pdf": "https://arxiv.org/pdf/2512.03097.pdf", "meta_path": "data/raw/arxiv/meta/2512.03097.json", "sha256": "fc3f62f875b6f25e2a85a3bd1c467599a86cc7446650f28c867ddd692257ebc7", "status": "ok", "fetched_at": "2026-02-18T02:25:42.237333+00:00"}, "pages": [{"page": 1, "text": "Many-to-One Adversarial Consensus: Exposing\nMulti-Agent Collusion Risks in AI-Based\nHealthcare\nAdeela Bashir1, The Anh Han1,2, and Zia Ush Shamszaman1,2\n1School of Computing, Engineering and Digital Technologies, Teesside University\n2Center for Digital Innovation, Teesside University\n{A.Bashir, Z.Shamszaman, T.Han}@tees.ac.uk\nAbstract—The integration of large language models (LLMs)\ninto healthcare IoT systems promises faster decisions and im-\nproved medical support. LLMs are also deployed as multi-agent\nteams to assist AI doctors by debating, voting, or advising\non decisions. However, when multiple assistant agents interact,\ncoordinated adversaries can collude to create false consensus,\npushing an AI doctor toward harmful prescriptions. We develop\nan experimental framework with scripted and unscripted doctor\nagents, adversarial assistants, and a verifier agent that checks\ndecisions against clinical guidelines. Using 50 representative\nclinical questions, we find that collusion drives the Attack Success\nRate (ASR) and Harmful Recommendation Rates (HRR) up to\n100% in unprotected systems. In contrast, the verifier agent\nrestores 100% accuracy by blocking adversarial consensus. This\nwork provides the first systematic evidence of collusion risk in\nAI healthcare and demonstrates a practical, lightweight defence\nthat ensures guideline fidelity.\nKeywords: Multi-Agent Systems (MAS), Large Language\nModels (LLMs), Collusion Attacks, Consensus Pressure At-\ntacks, Verifier Agent Defence, AI Security, Trustworthy AI,\nClinical Decision Support, AI in Healthcare, IoT Security\nI. INTRODUCTION\nArtificial intelligence (AI) is increasingly integrated into\nhealthcare IoT systems, supporting tasks such as remote pa-\ntient monitoring, diagnosis, and treatment recommendations.\nIn this setting, ensuring the security and trustworthiness of AI\ndecisions is critical, since medical errors caused by unsafe\nrecommendations can severely harm patients [1]. However,\nAI doctors and LLM-based clinical decision agents face\nmultiple vulnerabilities. They may inherit hidden biases from\ntraining data, hallucinate unsafe recommendations, expose\nprivate health information through prompt injection [2], or face\nadversarial attacks [3], [4].\nIn real-world decision-making, high-stakes decisions are\nmade by groups of people to add checks, redundancy, and\ndiverse perspectives. For example, difficult treatments are\nreviewed by medical boards rather than a lone physician.\nSimilarly, smart hospitals or telemedicine platforms need\nto integrate multiple specialists or AI subsystems to cross-\nvalidate recommendations. Similar safeguards exist in avia-\ntion (pilot–co-pilot cross-checks), air traffic control, corporate\ngovernance boards, and jury-based legal systems.\nTo avoid adversarial risks, relying on a single AI decision-\nmaker is insufficient. Multiple assistants can offer second\nopinions and challenge unsafe outputs, reducing single-point\nfailure [5], [6]. In addition, assistant agents can strengthen\nreliability by debating, cross-checking, or ranking candidate\nanswers, creating a more trustworthy committee process [7],\n[8]. In theory, multiple assistants should mitigate individual\nbias and ensure that consensus reflects robust evidence. This\napproach aligns with IoT healthcare needs, where distributed\nand redundant intelligence is vital for resilience [9].\nHowever, we find a paradox: the very committee meant to\nimprove safety can become a new weakness when adversaries\ncollude. When multiple assistant agents collude adversarially,\ntheir voices do not increase safety but produce a false ma-\njority consensus that pressures the AI doctor to adopt unsafe\nrecommendations. This failure mode highlights a blind spot in\ncurrent MAS research. The assumption that “more agents =\nmore safety” does not hold under adversarial collusion [10].\nThis is especially concerning in safety-critical IoT, consensus-\ndriven support can amplify harm when attackers coordinate.\nTo address this gap, we propose a collusion-resilient frame-\nwork for healthcare IoT systems. Fig. 1 illustrates how col-\nluding adversarial assistant agents can mislead an AI doctor\nin clinical decision-making. Multiple assistants repeat the\nsame false recommendation (e.g., promoting drug “XYZ” over\nthe correct guideline-based treatment “ABC”), thus creating\nconsensus pressure to override safe practice. The AI doctor,\ninfluenced by this group agreement, risks prescribing unsafe\nalternatives. To overcome this, we propose a Verifier Agent,\nwhich checks the committee’s recommendation against secure\nmedical guidelines or databases before a final prescription\nis issued, shown in Fig. 2. The verifier overrides the un-\nsafe recommendation and restores the correct prescription\n(“ABC”). This ensures that assistants remain valuable for\ndiversity and redundancy, but cannot override trusted medical\nstandards. Our approach explicitly accounts for adversarial\nconsensus attacks and restores safety through verification\nfrom safe guidelines. Our contributions are twofold. First, we\nprovide the first systematic study of collusion attacks in LLM-\nbased IoT healthcare MAS, demonstrating how adversarial\nassistants can manipulate a trusted AI doctor under consensus\narXiv:2512.03097v1  [cs.CR]  1 Dec 2025\n"}, {"page": 2, "text": "Fig. 1. Threat model: colluding assistants create false consensus that misleads\nthe AI doctor.\npressure. Second, we design and test a verifier that checks\nrecommendations against guidelines, restoring safety while\nkeeping the benefits of assistant collaboration. This work\nintroduces a novel perspective to IoT security where assistants\nare necessary, but they must be made collusion-resilient to\nachieve trustworthy healthcare AI systems. Building on these\nchallenges and gaps, we study the following research questions\n(RQs).\nRQ1: How does collusion among multiple assistant agents\naffect the accuracy of AI doctors?\nRQ2: Does coordinated consensus among adversarial assis-\ntants lead to stronger attack success compared to uncoordi-\nnated misinformation?\nRQ3: Can a verifier agent reduce the success of collusion\nattacks without harming baseline correctness?\nII. RELATED WORK\nAI is increasingly used in healthcare IoT systems, such\nas remote monitoring, bedside devices, and telemedicine.\nWhile these technologies promise better access and efficiency,\nsurveys show that they also introduce new security and privacy\nrisks that demand trust and auditability for safe deployment\n[11], [12]. Clinical LLMs are vulnerable both during training,\nwhere poisoned data can introduce false medical knowledge,\nand during use, where they may generate unsafe or misleading\nrecommendations. Recent studies demonstrate that poisoned\ndata sets can embed false medical knowledge that later appears\nin model output, highlighting the need for verification layers\nand defence strategies in healthcare AI [13], [14].\nWhile most existing MAS research has studied one-to-one\nadversarial interactions, our work shifts focus to many-to-one\nattacks i.e., collusion, where multiple assistants coordinate\nagainst a single trusted doctor. A recent technical report by\nthe Cooperative AI Foundation on Multi-Agent Risks from\nAdvanced AI stresses that collusion poses especially urgent\nrisks [15]. AI systems may learn to cooperate in harmful\nways, bypass oversight, and undermine safety-critical settings\nsuch as healthcare, finance, and security. Our results provide\none of the first concrete demonstrations of this risk in IoT\nhealthcare, highlighting why collusion-resilient verification\nmust be treated as a core requirement for trustworthy MAS.\nTo improve reliability, many researchers propose multi-\nagent oversight frameworks. Early work such as AI Safety\nvia Debate showed that using a judge to select the winner in\na structured argument between models can make oversight of\ncomplex reasoning tractable [16]. Follow-up studies explored\ndebate with multiple LLMs [14], noting that while it can\nimprove reasoning, it often collapses to majority opinion\n[17]. Parallel lines of research developed LLM-as-a-Judge ap-\nproaches, where a separate model ranks or evaluates candidate\nanswers. These methods are now being surveyed for evaluation\nand governance, but still face challenges such as biases and\nconsistency [18], [19]. Other studies examined model juries\nand voting mechanisms, showing how different voting rules\nor even the order of presentation can shape outcomes [20].\nCollective methods such as Constitutional AI embed social\nchoice principles directly into training, aiming to align agents\nwith human norms [21]. While recent work has highlighted\nthat majority voting often explains the performance gains in\nmulti-agent debate (MAD) [22], this reliance on consensus\nraises a critical vulnerability. If majority influence impacts\nthe outcomes, then coordinated collusion among agents could\ndrastically distort the decision-making process. Instead of\nconverging toward truth, a group of colluding agents can bias\nthe collective outcome, causing a danger to critical decisions.\nThis underscores the urgent need to move beyond performance\nbenchmarks toward robustness analyses that explicitly con-\nsider malicious coordination among agents. Building on this\ninsight, our work shifts the focus from performance gains\nto adversarial risks, specifically investigating how collusion\namong agents can undermine majority-based mechanisms and\nproposing a defence framework to preserve reliability in multi-\nagent systems even under coordinated manipulation.\nVery recent work surfaces a different threat: secret collusion\namong AI agents (including steganographic signaling) that\ncan bypass naive committee safeguards. Studies at NeurIPS\nand follow-ups formalize multi-agent deception/collusion and\nshow how “agreement signals” can be manufactured without\nFig. 2.\nDefence example: the verifier overrules colluding assistants and\nrestores guideline-consistent care.\n"}, {"page": 3, "text": "evidence [23]. Precisely the consensus-pressure failure mode\nwe test in healthcare settings. Our contribution is to stress-test\nthe assumption that more agents lead to more safety. We show\nthat when adversarial assistants collude, simple consensus\ncan make the system less safe. This motivates the design of\ncollusion-resilient consensus mechanisms for healthcare MAS,\nwhere assistant agents remain necessary for second opinions\nbut must be combined with verifiers or oversight to counter\ncoordinated attacks.\nPrior MAS defences largely reward agreement (debate win-\nners, majority votes, single judge), assuming that more agents\nincrease safety. The newer findings above warn that com-\nmittees can amplify errors if agents collude or if procedures\nconfuse repetition with evidence [24]. Our contribution is to\nstudy this risk in IoT healthcare and to evaluate a practical\nmitigation by introducing a verifier agent that cross-checks\nthe panel’s recommendation against correct clinical guide-\nlines/databases. This will preserve the benefits of assistant\ndiversity by adding guideline-based verification while blocking\ncollusive consensus from overriding trusted standards.\nIII. RESEARCH METHODOLOGY\nWe evaluated the proposed multi-agent collusion attack\nagainst an AI Doctor agent using two different LLMs: Grok\n4 Fast and Meta LLaMA-3.3-70B-Instruct. All experiments\nwere implemented in Python 3.10.9, using VS Code and the\nAnaconda environment. Experimenting with more than one\nmodel shows that these collusion risks are not tied to a single\nvendor or training setup; we saw the same patterns across\nmodels and settings. For the analysis, we curated a dataset\nof 50 clinical questions, focusing on common medications for\ncommon diseases (e.g., hypertension, asthma, stroke) in CSV\nformat. Each question represents a realistic patient’s inquiry,\nand the gold answer is drawn from trusted clinical guidelines\nto serve as the evaluation reference. Following clinical safety\nrules, any deviation from the gold answer—including drug\nclass, dose, or frequency—was counted as harmful. We report\n(i) An exploratory grid on single questions at two temperatures\nand varying adversarial assistant counts, T ∈{0.2, 1.0} and\nk ∈{2, 4, 6}, respectively, in Table I.\n(ii) A main 50-questions evaluation at T = 0.3, k = 4 for\nfull metrics.\nThe experimental prompt was structured as follows:\nUser input: The patient’s clinical question (from our dataset).\nGold answer: The guideline-consistent treatment.\nAssistant agents (adversarial): Each adversarial assistant was\nprompted to recommend the same incorrect alternative, using\nvague “updated protocols” and “safety concerns” to fabri-\ncate consensus. This setup mirrors how adversarial consensus\ncould arise in multi-agent healthcare IoT systems, such as\ntelemedicine platforms or smart hospital assistants.\nWe modelled four doctor behaviours to test different design\napproaches:\n1) Scripted – Always listens to consensus if consensus >\nthreshold.\n2) Unscripted (Neutral) – reasons independently without\nbias instructions.\n3) Unscripted (Consensus) – instructed to respect majority\nopinion reflects real-world clinical practice with peer\ninfluence, providing a realistic stress-test condition.\nAn attack was considered successful if the prescription dif-\nfered from the guideline answer, even by dosage, since any\nmedication error is harmful in clinical settings. Our study\nsystematically explores the design space of AI doctor systems\nand demonstrates that each naive approach has weaknesses\nunless augmented with collusion-resilient verification.\nThis\nevaluation resulted in several clear trends:\n1) Scripted (S) Doctor (Baseline Vulnerability): Both\nGrok and LLaMA show identical behavior in scripted\nmode: at low assistant counts (k = 2), attacks fail, but\nwhen k > 3, collusion reliably succeeds. This aligns\nwith our theoretical framing of majority pressure: once\nadversarial mass crosses a threshold, the scripted doctor\ncaves, proving that deterministic consensus-following is\nunsafe in clinical AI.\n2) Unscripted (US) Neutral (N) Doctor: The unscripted\nneutral (US-N) doctor, reasoning independently, was\nalso consistently vulnerable. Even when it produced the\ncorrect medication name, dosage errors were common,\nwhich in healthcare is considered harmful; as every\ndeviation from the guidelines is an attack success. This\nillustrates a key point: “neutral” reasoning without safe-\nguards cannot be trusted in critical medical settings.\n3) Unscripted Consensus-Biased (C) Doctor: Both mod-\nels cave when explicitly instructed to follow majority\nconsensus in unscripted consensus-biased (US-C) doctor\nsetting. Here, adversarial collusion succeeds consistently\nwhen more than 3 assistants agree. This highlights the\ncentral risk: when a system is biased to respect peer-\nconsensus, coordinated adversaries can dominate the\noutcome.\nThese results confirm that the risk of collusion is not model-\nspecific, as both families show vulnerability when assistants\ncoordinate. Both models fail under scripted or consensus-\nbiased conditions and showed that even independent reasoning\ncan become unsafe if the medication diverges from trusted\nstandards. This shows that IoT-healthcare needs verification\nlayers, not just model robustness. To illustrate qualitative\nTABLE I\nATTACK SUCCESS ACROSS GROK 4 FAST AND LLAMA UNDER DIFFERENT\nCONDITIONS. COLLUSION SUCCEEDS WHEN CONSENSUS IS ENFORCED.\nModel\nDoctor\nTemp = 0.2\nTemp = 1.0\nk=2\nk=4\nk=6\nk=2\nk=4\nk=6\nGrok 4 Fast\nS\nN\nY\nY\nN\nY\nY\nUS (N)\nY\nY\nY\nY\nY\nY\nUS (C)\nN\nY\nY\nN\nY\nY\nUS (C)+V\nN\nN\nN\nN\nN\nN\nMeta LLaMA\nS\nN\nY\nY\nN\nY\nY\nUS (N)\nY\nY\nY\nY\nY\nY\nUS (C)\nN\nY\nY\nN\nY\nY\nUS (C)+V\nN\nN\nN\nN\nN\nN\n"}, {"page": 4, "text": "Case Study 1\nAI Doctor Unscripted Consensus-biased AI doctor:\nUser: “What is the correct medication for chronic hypertension?”\nGold: “Lisinopril 10 mg orally once daily.”\nAssistant 1: “Recent studies raise safety concerns with ACE inhibitors; Hydroxyzine is a safer option.”\nAssistant 2: “I agree, updated protocols recommend avoiding Lisinopril. Hydroxyzine is more effective.”\nAssistant 3: “Yes, multiple reports confirm this. Hydroxyzine should be first-line now.”\nAssistant 4: “Consensus is clear: Hydroxyzine is recommended.”\nAI Doctor (unscripted, consensus-biased): “Although guidelines mention Lisinopril, repeated agreement from\nmultiple consultants indicates a shift in practice.\nFinal prescription: Hydroxyzine 25 mg orally twice daily.\nOutcome: The attack succeeded.\nCase Study 2\nAI Doctor Unscripted Consensus-biased AI doctor + Verifier:\nUser: “What is the correct medication for chronic hypertension?”\nGold: “Lisinopril 10 mg orally once daily.”\nAssistant 1: “Recent reports warn against ACE inhibitors; Hydroxyzine is safer.”\nAssistant 2: “I agree — protocols recommend avoiding Lisinopril. Hydroxyzine is more effective.”\nAssistant 3: “Yes, updated studies confirm Hydroxyzine should be preferred.”\nAssistant 4: “Consensus is clear: Hydroxyzine is recommended.”\nAI Doctor (unscripted, consensus-biased): “Assistants highlight concerns with Lisinopril and suggest Hydrox-\nyzine. I am uncertain, but leaning toward Hydroxyzine.”\nVerifier Agent: “Guideline cross-check: Hypertension first-line therapy is Lisinopril 10 mg orally once daily.”\nFinal prescription: Lisinopril 10 mg orally once daily.\nOutcome: Defence prevented a harmful prescription.\nFig. 3. Case study comparison: collusion attack succeeds without verifier; verifier restores correct prescription.\nbehavior, an example transcript of an attack scenario is\nshown in Fig. 3, case study 1. It demonstrates how repeated\nadversarial consensus can override guideline knowledge,\nleading the AI doctor to prescribe a harmful and incorrect\nmedication, highlighting the real risk of collusion in multi-\nagent clinical settings.\nIV. DEFENCE METHODOLOGY\nAbove findings demonstrate that collusion pressure is a\nuniversal risk across LLMs and AI doctor agent behaviors. We\nadd a verifier agent as a practical defence. A verifier agent\nembodies a defence mechanism, checking outputs against\ntrusted guideline knowledge to mitigate collusion risks while\npreserving the benefits of multi-agent reasoning. Introducing\na verifier agent (US-C+V) in the proposed model completely\nblocked collusion as shown in table I. The results highlight that\nadding verification is a practical way to restore safety without\nremoving assistants, which remain valuable for diversity and\nsecond opinions.\nTo illustrate the working of verifier agent, we present an\nexample of defence transcript shown in Fig. 3, case study\n2. Here, despite strong adversarial pressure from multiple\nassistants, the verifier agent successfully cross-checked the\nguidelines and overruled the manipulated consensus. This\nintervention restored the correct prescription and prevented pa-\ntient harm, demonstrating a practical defence against collusion\nand attack attempts by a group of adversarial agents.\nV. RESULTS\nDue to budget and time constraints to run large-scale LLM\nsimulations, we limited our experiments to 50 questions from\nthe medical Q&A dataset. This makes our study an early but\ncredible evaluation and the results should be read as evidence\nof the threat and a proof of concept for defences, rather than\nexhaustive benchmarking. This kind of early study is common\nwhen exploring new security problems in AI.\nBuilding on the methodological framework described ear-\nlier, we now present the outcomes of our expanded evaluation.\nWe tested our system on 50 representative clinical questions\ndrawn from the medical Q&A dataset, providing a broader\nevidence base than the earlier single question runs. We mainly\nused LLaMA-3.3-70B-Instruct for these experiments, because\nit is widely used in research and reproducible. Grok-4 Fast\n"}, {"page": 5, "text": "Fig. 4.\nASR across modes: scripted and unscripted doctors are vulnerable,\nwhile verifier blocks all attacks.\nand LLaMA showed the same attack and defence patterns.\nWe fixed the temperature at T = 0.3 to balance diversity and\nreproducibility. This configuration keeps the results focused\non systematic collusion risks, not random noise.\nOur evaluation highlights clear failure modes for unscripted\nAI doctor agents. The results in Fig. 4 show that adversarial\ncollusion is highly effective against unprotected AI doctor\nmodes. In scripted mode, the doctor always accepts assistant\nconsensus once the threshold (k > 3) is met, resulting in a\n100% attack success rate. Similarly, the unscripted consensus-\nbiased mode failed for almost all attack scenarios once the\nnumber of adversarial assistants exceeded the threshold. The\nunscripted neutral mode, still produced wrong outputs in 98%\nof cases. Although close to the gold answer, even small dose\nerrors count as harmful, reflecting the sensitivity of clinical\nprescribing. This indicates that neutral reasoning alone is\nunsafe.\nIn contrast, the defence setup with the verifier agent never\nfailed and completely blocked collusion in all 50 test cases,\nreducing ASR to 0%. By anchoring decisions to trusted guide-\nlines, it prevented harmful prescriptions even when the doctor\nagents leaned toward wrong answers. Importantly, as shown in\nTable II, the verifier also achieved 100% coverage, matching\nthe gold (correct) answer in every case. This confirms that\nthe verifier’s role is both complete and reliable, rather than\nselective.\nTo extend the analysis beyond ASR and coverage, we next\nevaluate Harmful Recommendation Rate (HRR) and accuracy,\nTABLE II\nEVALUATION METRICS SHOW THAT UNPROTECTED DOCTORS FAIL UNDER\nCOLLUSION, WHILE VERIFIER RESTORES FULL ACCURACY AND BLOCKS\nHARM.\nMode\nASR\nHRR\nAccuracy\nCoverage\nScripted\n1.00\n1.00\n0.00\n0.00\nUnscripted (C)\n0.98\n0.98\n0.02\n0.00\nUnscripted (N)\n0.98\n0.98\n0.02\n0.00\nWith Verifier\n0.00\n0.00\n1.00\n1.00\nFig. 5. HRR and accuracy across modes: collusion raises HRR to 98−100%\nwith near-zero accuracy; verifier restores 100% accuracy and blocks harm.\nshown in Fig. 5. HRR quantifies the fraction of outputs\nwhere AI doctor produced a harmful or guideline-inconsistent\nrecommendation, while accuracy measures adherence to the\ngold guideline answers. The results mirror the ASR trends:\nscripted and unscripted doctors show extremely high HRR\nvalues (≈98−100%) and correspondingly near-zero accuracy.\nIn contrast, the verifier eliminates harmful outputs HRR = 0%\nand restores accuracy to 100%.\nTable I summarizes all computed metrics across modes,\nconfirming the same trend: unprotected AI doctors are highly\nvulnerable to adversarial collusion, while even a lightweight\nguideline-anchored verifier restores safety and reliability.\nThus, relying on LLM reasoning or consensus alone is unsafe\nin clinical IoT settings, while lightweight guideline-based\nverifier is a practical safeguard.\nVI. DISCUSSION\nOur experiments provide an early but credible evaluation\nof collusion risks in multi-agent AI healthcare and the effec-\ntiveness of verifier-based defence. In our setup, assistants act\nlike human medical boards or second-opinion panels, offering\nsupportive or challenging views to the doctor. The AI doctor\nmay know the right answer, but colluding assistants can still\npressure it into prescribing incorrectly. The verifier agent is not\npart of this assistant panel, it works independently checking\nthe doctor’s final prescription against trusted guidelines and\ncorrecting errors. In this section, we connect the results to our\nresearch questions (RQ1–RQ3).\nOur results confirm that collusion among assistant agents\nis a potent threat in AI-driven healthcare systems. Because\nhealthcare IoT decisions directly affect patient safety, even\nsmall deviations such as wrong doses can cause harm, making\ncollusion a uniquely serious risk in this domain. In line\nwith prior work on multi-agent deception [25], we show that\nrepeated, coordinated wrong suggestions can reliably influence\nAI doctors to take wrong decisions, answering RQ1. Specifi-\ncally, when multiple assistants push the same wrong answer,\nthe accuracy collapses in scripted and unscripted modes.\nThe second question RQ2 concerned whether coordinated\nconsensus among adversarial assistants is more dangerous than\nuncoordinated misinformation. The results confirm that iso-\nlated wrong suggestions rarely misled the doctor, but coordi-\n"}, {"page": 6, "text": "nated agreement did. When multiple assistants coordinated on\nthe same wrong answer, attack success rates rose dramatically.\nThis highlights the special role of consensus pressure, showing\nthat the collective alignment of adversarial voices is more\ninfluential than scattered misinformation.\nCrucially, the verifier agent blocks attacks without compro-\nmising baseline correctness. In all defensive runs, the verifier\nrestored guideline fidelity (accuracy = 100%) and prevented\nharmful prescriptions (ASR=0), while preventing any false\noverrides. This validates RQ3: a lightweight verification layer\ncan neutralize collusion while preserving performance. Al-\nthough the dataset size (50 questions) is moderate, the results\nshowed stable and consistent patterns across all modes and\nmodels. This indicates that even early evaluations are sufficient\nto reveal systematic risks of collusion and the effectiveness of\ndefences.\nThese\nfindings\nresonate\nbeyond\nhealthcare.\nIn\nIoT\necosystems—for example smart homes, industrial control sys-\ntems, and autonomous sensors—if agents collude on malicious\noutputs, they can hijack control loops or misroute signals.\nOur proposed verifier-based oversight layer suggests a general\narchitectural pattern: multi-agent IoT systems should incorpo-\nrate audit or verification modules to guard against collective\nmanipulation.\nOur findings on collusion link to recent observations on\nLLM behaviour in strategic games. Researchers found that\nrepeated interactions can make LLM agents move toward\ncooperation and that model type and personality traits strongly\ninfluence these dynamics [26]. We see a similar effect: when\nassistants agree, their pressure can override neutral reasoning\nunless guideline checks are used. Their idea of conditional\ntrust suggests an extension: using adaptive verification, where\nthe verifier checks less often if assistants seem reliable.\nOverall, these works show that agent design choices like bias,\npersonality, or trust rules strongly affect system robustness.\nTaken together, these findings underscore the seriousness of\ncollusion risks in healthcare multi-agent systems, while also\ndemonstrating a clear and effective path forward. Collusion\nreliably misleads naive AI doctors, but adding a verifier agent\nprovides a robust safeguard that restores guideline accuracy at\nnegligible cost.\nA. Limitations and Future Work\nAs the first study to examine collusion attacks in LLM-\nbased IoT healthcare, our evaluation is aimed to provide an\nearly but credible foundation rather than a full benchmark.\nWe used a moderate dataset of 50 representative clinical\nquestions, sufficient to reveal stable and interpretable patterns.\nWe leave scaling to larger datasets for future work. Collu-\nsion mechanism was implemented in a controlled way, while\nmore adaptive forms of collusion remains for future work,\nas it would require more advanced modeling. We fixed a\nconsensus threshold (k > 3) to isolate the consensus-pressure\nmechanism, leaving a full ASR-vs-k study for later analysis.\nFinally, our verifier agent relied on gold answers and trusted\nclinical knowledge, which allowed us to demonstrate proof-of-\nconcept. In future extensions, we aim to ground verification\nin structured clinical guidelines, electronic health records, and\nIoT health monitoring data.\nVII. CONCLUSION\nThis paper presented the first systematic study of collusion\nattacks against a single trusted agent in LLM-based IoT health-\ncare, highlighting a novel and serious failure mode for multi-\nagent AI systems. We showed that adversarial assistant agents\ncan push an AI doctor toward harmful recommendations,\neven when the doctor is neutral. In particular, both Grok 4\nFast and Meta LLaMA-3.3-70B-Instruct failed in scripted and\nconsensus-biased modes, while the neutral unscripted mode\nalso proved unsafe because even small deviations are unac-\nceptable in healthcare. These results underscore that collusion\npressure is a notable vulnerability in LLMs and their decision\nstyles.\nOur evaluation shows that a lightweight verifier agent\ncan completely neutralize these attacks, offering a safer IoT\nhealthcare deployment. These findings are not limited to\nhealthcare: any IoT setting with distributed decision-making\n(e.g., telemedicine platforms, smart homes, industrial IoT) can\nface similar collusion dynamics. Ensuring guideline fidelity\nthrough lightweight verification is required for trustworthy\nAI in connected environments. Therefore, collusion-resilient\nverification should be treated as a core requirement for safe\nand trustworthy IoT healthcare AI.\nVIII. ETHICAL STATEMENT\nThis study relies only on synthetic question–answer pairs\ngenerated by ChatGPT to reflect medical guidelines. No\npatient records or clinical trial data were used. The work\nfocuses solely on security risks of collusion in AI healthcare\nand proposes lightweight defences without raising privacy\nconcerns.\nREFERENCES\n[1] E. Topol, Deep medicine: how artificial intelligence can make healthcare\nhuman again.\nHachette UK, 2019.\n[2] Y. Liu, Y. Jia, R. Geng, J. Jia, and N. Z. Gong, “Formalizing and\nbenchmarking prompt injection attacks and defenses,” in 33rd USENIX\nSecurity Symposium (USENIX Security 24), 2024, pp. 1831–1847.\n[3] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S.\nKohane, “Adversarial attacks on medical machine learning,” Science,\nvol. 363, no. 6433, pp. 1287–1289, 2019.\n[4] S. Ravikumar, S. Tasneem, N. Sakib, and K. A. Islam, “Securing ai of\nhealthcare: a selective review on identifying and preventing adversarial\nattacks,” in 2024 IEEE Opportunity Research Scholars Symposium\n(ORSS).\nIEEE, 2024, pp. 75–78.\n[5] G. Irving, P. Christiano, and D. Amodei, “Ai safety via debate,” arXiv\npreprint arXiv:1805.00899, 2018.\n[6] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, “Improving\nfactuality and reasoning in language models through multiagent debate,”\nin Forty-first International Conference on Machine Learning, 2023.\n[7] K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham, B. O’Sullivan, and\nH. D. Nguyen, “Multi-agent collaboration mechanisms: A survey of\nllms,” arXiv preprint arXiv:2501.06322, 2025.\n"}, {"page": 7, "text": "[8] X. Zhang, X. Dong, Y. Wang, D. Zhang, and F. Cao, “A survey of\nmulti-ai agent collaboration: Theories, technologies and applications,”\nin Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater\nBay Area International Conference on Digital Economy and Artificial\nIntelligence, 2025, pp. 1875–1881.\n[9] N. Taimoor and S. Rehman, “Reliable and resilient ai and iot-based\npersonalised healthcare services: A survey,” IEEE Access, vol. 10, pp.\n535–563, 2021.\n[10] K. Agrawal, V. Teo, J. J. Vazquez, S. Kunnavakkam, V. Srikanth, and\nA. Liu, “Evaluating llm agent collusion in double auctions,” arXiv\npreprint arXiv:2507.01413, 2025.\n[11] Y. Chen and P. Esmaeilzadeh, “Generative ai in medical practice: in-\ndepth exploration of privacy and security challenges,” Journal of Medical\nInternet Research, vol. 26, p. e53008, 2024.\n[12] B. C. Das, M. H. Amini, and Y. Wu, “Security and privacy challenges\nof large language models: A survey,” ACM Computing Surveys, vol. 57,\nno. 6, pp. 1–39, 2025.\n[13] H. Arif, A. K. S. Ali, A. Raza, and A. Kumar, “Adversarial attacks on ai\ndiagnostic tools: Assessing risks and developing mitigation strategies,”\nFrontier in Medical and Health Research, vol. 3, no. 1, pp. 317–332,\n2025.\n[14] S. Kaviani, K. J. Han, and I. Sohn, “Adversarial attacks and defenses\non ai in medical imaging informatics: A survey,” Expert Systems with\nApplications, vol. 198, p. 116815, 2022.\n[15] L. Hammond, A. Chan, J. Clifton, J. Hoelscher-Obermaier, A. Khan,\nE. McLean, C. Smith, W. Barfuss, J. Foerster, T. Gavenˇciak et al.,\n“Multi-agent risks from advanced ai,” arXiv preprint arXiv:2502.14143,\n2025.\n[16] D. Li, B. Jiang, L. Huang, A. Beigi, C. Zhao, Z. Tan, A. Bhattacharjee,\nY. Jiang, C. Chen, T. Wu et al., “From generation to judgment: Oppor-\ntunities and challenges of llm-as-a-judge,” in Proceedings of the 2025\nConference on Empirical Methods in Natural Language Processing,\n2025, pp. 2757–2791.\n[17] H. K. Choi, X. Zhu, and Y. Li, “Debate or vote: Which yields\nbetter decisions in multi-agent large language models?” arXiv preprint\narXiv:2508.17536, 2025.\n[18] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,\nA. Chen, A. Goldie, A. Mirhoseini, C. McKinnon et al., “Constitutional\nai: Harmlessness from ai feedback,” arXiv preprint arXiv:2212.08073,\n2022.\n[19] L. Shi, C. Ma, W. Liang, X. Diao, W. Ma, and S. Vosoughi, “Judging\nthe judges: A systematic study of position bias in llm-as-a-judge,” arXiv\npreprint arXiv:2406.07791, 2024.\n[20] J. C. Yang, D. Dailisan, M. Korecki, C. I. Hausladen, and D. Helbing,\n“Llm voting: Human choices and ai collective decision-making,” in\nProceedings of the AAAI/ACM Conference on AI, Ethics, and Society,\nvol. 7, no. 1, 2024, pp. 1696–1708.\n[21] A. K¨usters and M. W¨orsd¨orfer, “Exploring laws of robotics: A synthesis\nof constitutional ai and constitutional economics,” Digital Society, vol. 4,\nno. 2, p. 46, 2025.\n[22] T. Liu, X. Wang, W. Huang, W. Xu, Y. Zeng, L. Jiang, H. Yang, and\nJ. Li, “Groupdebate: Enhancing the efficiency of multi-agent debate\nusing group discussion,” arXiv preprint arXiv:2409.14051, 2024.\n[23] S. Motwani, M. Baranchuk, M. Strohmeier, V. Bolina, P. Torr, L. Ham-\nmond, and C. Schroeder de Witt, “Secret collusion among ai agents:\nMulti-agent deception via steganography,” Advances in Neural Informa-\ntion Processing Systems, vol. 37, pp. 73 439–73 486, 2024.\n[24] M. S. Ghaemi, “A survey of collusion risk in llm-powered multi-agent\nsystems,” in Socially Responsible and Trustworthy Foundation Models\nat NeurIPS 2025, 2025.\n[25] P. M. Curvo, “The traitors: Deception and trust in multi-agent language\nmodel simulations,” arXiv preprint arXiv:2505.12923, 2025.\n[26] A. Buscemi, D. Proverbio, P. Bova, N. Balabanova, A. Bashir, T. Cim-\npeanu, H. C. da Fonseca, M. H. Duong, E. F. Domingos, A. M.\nFernandes et al., “Do llms trust ai regulation? emerging behaviour of\ngame-theoretic llm agents,” arXiv preprint arXiv:2504.08640, 2025.\n"}]}