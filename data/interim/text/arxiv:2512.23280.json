{"doc_id": "arxiv:2512.23280", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.23280.pdf", "meta": {"doc_id": "arxiv:2512.23280", "source": "arxiv", "arxiv_id": "2512.23280", "title": "Chinese Morph Resolution in E-commerce Live Streaming Scenarios", "authors": ["Jiahao Zhu", "Jipeng Qiang", "Ran Bai", "Chenyu Liu", "Xiaoye Ouyang"], "published": "2025-12-29T08:04:48Z", "updated": "2025-12-29T08:04:48Z", "summary": "E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.23280v1", "url_pdf": "https://arxiv.org/pdf/2512.23280.pdf", "meta_path": "data/raw/arxiv/meta/2512.23280.json", "sha256": "8c512dad705b9d06e2f5df5caf58e9ce2a253f263e7ebb4a7b523b09ffb4d0e7", "status": "ok", "fetched_at": "2026-02-18T02:23:37.184335+00:00"}, "pages": [{"page": 1, "text": "Chinese Morph Resolution in E-commerce Live Streaming Scenarios\nJiahao Zhu1 , Jipeng Qiang1*, Ran Bai2, Chenyu Liu2, Xiaoye Ouyang2\n1 School of Information Engineering, Yangzhou University, China\n2 China Academy of Electronic and Information Technology, China\nmz120231031@stu.yzu.edu.cn, jpqiang@yzu.edu.cn\n{bairan, liuchenyu, ouyangxiaoye }@cetc.com.cn\nAbstract\nE-commerce live streaming in China, particu-\nlarly on platforms like Douyin, has become a\nmajor sales channel, but hosts often use morphs\nto evade scrutiny and engage in false advertis-\ning. This study introduces the Live Auditory\nMorph Resolution (LiveAMR) task to detect\nsuch violations. Unlike previous morph re-\nsearch focused on text-based evasion in social\nmedia and underground industries, LiveAMR\ntargets pronunciation-based evasion in health\nand medical live streams. We constructed the\nfirst LiveAMR dataset with 86,790 samples and\ndeveloped a method to transform the task into\na text-to-text generation problem. By leverag-\ning large language models (LLMs) to generate\nadditional training data, we improved perfor-\nmance and demonstrated that morph resolution\nsignificantly enhances live streaming regula-\ntion.\n1\nIntroduction\nE-commerce live streaming has become an im-\nmensely popular and influential sales channel in\nChina. For example, one short video platform\nDouyin hosted over 9 million live broadcasts each\nmonth, selling more than 10 billion items through\nthere sessions (Center, 2022). To increase sales\nand attract customers, hosts engage in practices\nsuch as using morphs to evade scrutiny and con-\nducting false advertising. As shown in the Figure\n1, morphs are used in promotional language that\nsuggests the product has medicinal effects in or-\nder to evade scrutiny. Detecting violations during\nthe live commerce process is crucial for protecting\nconsumer rights and promoting industry standard-\nization (Xiao, 2024; Xu, 2024).\nTo detect violations in live commerce, resolv-\ning morphs used in the live content is intuitively\nimportant. Previous morph research has primarily\n* Corresponding authors.\nMorph: 这个产品是k糖的，糖友放心拍，以后就不用去药什么店了。\nOrigin: 这个产品是抗糖的，糖尿病患者放心拍，以后就不用去药店了。\nThis product is anti-glycemic, so diabetic patients can buy it with confidence and they don’t have to go to the pharmacy anymore.\nk糖 --> 抗糖\nk-sugar-->anti-glycemic\n糖友 --> 糖尿病患者\nsugar friends -->diabetic\n  \n  药什么店 -->药店\nwhat drug store-->pharmacy\nHomophone\nSynonyms\nTransformation\nFigure 1: Example of morph used in the live streaming\nscenarios\nfocused on social media commentary and under-\nground industries (Sha et al., 2017; You et al., 2018;\nWang et al., 2024). There are two main differences\nbetween their research and this paper.\n(1) Different purposes for morphing: Their fo-\ncus is on making the written text appear different\nto evade keyword recognition (You et al., 2018;\nWang et al., 2024), whereas the live streaming field\nfocuses on differences in pronunciation to evade\nvoice censorship. For example, in visual scenar-\nios, characters with a left-right structure are often\nsplit into two words, such as “胡” (hú)->“古月”(gˇu\nyùe). In the live streaming field, a very common sit-\nuation is inserting some meaningless words , like\n‘’某”(mˇou, some) or “什么”(shén m¯e, what) can\nhelp maintain the rhythm of speech without inter-\nfering with the listener’s understanding of the infor-\nmation, such as “手术”(shˇou shù, surgery)->“手某\n术”(shˇou mˇou shù, surgery)\".\n(2) Different subjects of interest: Social media\ncommentary focuses on current affairs and politics\n(You et al., 2018), and underground industries focus\non illegal gambling and the sex industry (Wang\net al., 2024), while our study focuses on the health\nand medical industry.\nIn this paper, we focus on auditory-based morph\nresolution task in live screaming scenarios, denoted\nas LiveAMR task. Voice censorship is first pro-\ncessed using automatic speech recognition (ASR)\ntechnology (Wang et al., 2023a), which converts\nspeech into text. By observation, we can find that\narXiv:2512.23280v1  [cs.CL]  29 Dec 2025\n"}, {"page": 2, "text": "the LiveAMR task is similar to the grammar cor-\nrection task (Kobayashi et al., 2024). In this way,\nwe can train a text generation model to convert the\ninput text with morph words into normal text. This\nstudy produces two main contributions toward the\ndevelopment and evaluation of LiveAMR methods.\nOur contributions are listed below:\n(1) To the best of our knowledge, there is no\nexisting work on LiveAMR. We constructed a\nLiveAMR dataset containing 86,790 samples, in-\ncluding 2,688 different morphs. In live streaming\nscenarios, considering the noise in the live envi-\nronment and the variations in presenters’ expres-\nsions, the results of different ASR systems vary\ngreatly. We re-annotated the second test set, se-\nlecting different live streaming rooms and different\nASR methods which includes 400 positive and 400\nnegative samples. This approach allows us to com-\nprehensively assess the model’s performance and\nadaptability under different conditions.\n(2) We transform LiveAMR task into a type of\ntext-to-text generation task. By training the T5\nmodel using the constructed morph dataset, we\nachieved F1 scores of 94% and 82% on Test Set\n1 and Test Set 2, surpassing the performance of\nother models respectively. Considering the effi-\nciency of manual annotation is relatively low, we\npropose an innovative solution that leverages large\nlanguage modeling to generate LiveAMR exam-\nples, thereby improving the scale of LiveAMR\ntraining set. Experimental results show that in-\ncorporating the dataset generated by LLM into the\ntraining process also improved the performance of\nLiveAMR methods. Additionally, we investigated\nthe peformance of morph resolution in detecting\nviolations. We also verify that morph resolution\ncan significantly improve the model’s accuracy in\nthe live streaming regulation. The dataset and code\nis available at github 1.\n2\nRelated Work\nThere has been extensive research on morph resolu-\ntion across different language backgrounds includ-\ning English (Ji and Knight, 2018; Li et al., 2022;\nWang et al., 2023b; Qiang et al., 2023c), and Chi-\nnese (Huang et al., 2017, 2019; Qiang et al., 2023a),\netc. In this paper, we only focus on morph resolu-\ntion in Chinese. Because Chinese is a pictographic\nlanguage, methods for identifying morph words\nin other languages cannot be applied to Chinese.\n1https://github.com/loopback00/LiveAMR\nExisting research on Chinese morphs primarily fo-\ncuses on social media and underground industries.\nInitially, it was considered a filtering problem,\nwith researchers using statistical and rule-based\nmatching methods to identify problematic text\n(Wang et al., 2013; Choudhury et al., 2007; Qiang\net al., 2023b; Yoon et al., 2010). Subsequently, Sha\net al. (Sha et al., 2017) proposed incorporating\nradicals into Chinese characters to enhance their\nfeatures and improve morphs resolution. You et\nal.(You et al., 2018) further extracted actual con-\ntextual information and enhanced embedded rep-\nresentations by integrating transformed mentions\nor target candidates with their relevant context into\nan AutoEncoder. Recently, addressing the charac-\nteristics of morph words in underground industries,\nWang et al.(Wang et al., 2024) introduced a morph\nparsing algorithm based on machine translation\nmodels.\nHowever, existing research on morphs mainly\nfocuses on social media and underground indus-\ntries, with studies on morph resolution in the emerg-\ning context of live streaming still being relatively\nscarce.\n3\nTask Definition\nIn the research context of this paper, ’morph’ refers\nto the process where live streamers avoid plat-\nform censorship by replacing sensitive or restric-\ntive words during product promotion, while en-\nsuring that the audience can easily understand the\noriginal meaning conveyed by the transformation.\nHere, we formally define the auditory-based morph\nresolution task in live screaming scenarios as the\nLiveAMR task. By analyzing thousands of videos,\nthe main types of transformations can be catego-\nrized into three major types (transformation, homo-\nphones, and synonyms), as shown in Table 1.\nSuppose one example is \"咱们一些<小糖\n人>都是一样可以放心去喝，也不用去找<白\n褂褂>了。\" (Some diabetes patients can safely\ndrink without needing to consult a doctor.) with\ntwo morphs “小糖人”(sugar doll)->“糖尿病患\n者”(diabetic) and “白褂褂”(people with white)-\n>“医生”( doctor). The correct output by LiveAMR\nmethod should be “咱们一些<糖尿病患者>都是\n一样可以放心去喝，也不用去找<医生>了。”.\n4\nDataset Construct\nIn this section, we describe the whole process of\nconstructing a LiveAMR dataset.\n"}, {"page": 3, "text": "Type\nCharacteristic\nExamples\nTransformation\nInsert meaningless characters into words, or change the structure\nwhile keeping the sound similar to the original words.\n某医某院:医院(hospital)\nmˇou y¯ı mˇou yuàn:y¯ı yuàn\n祛什么斑:祛斑(spot removal)\nq¯u shén m¯e b¯an:q¯u b¯an\n小问小题:问题(problem)\nxiˇao wèn xiˇao tí:wèn tí\nHomophone\nUse symbols to replace Chinese characters\nk糖:抗糖(anti glycemic)\nk táng: kàng táng\nk老:抗老(anti aging)\nk lˇao: kàng lˇao\nSynonyms\nUse words that are highly related or synonymous with the target\nword\n白大褂:医生\n(people in white:doctor)\n心灵之窗:眼睛\n(windows to the soul:eyes)\nTable 1: The three types of transformations in LiveAMR. For the two types of morphs, transformation and\nhomophone, we have additionally annotated their pinyin below them.\nData Collection: We crawled videos from four\ndomains in Douyin website 2: health supplements,\npharmaceuticals, medical devices, and cosmetics.\nThese areas are chosen due to their unique risks and\nchallenges in live streaming. As products aimed at\nimproving health, they have a large market size and\ndiverse categories. However, due to their specific\nnature, consumers often face significant informa-\ntion asymmetry regarding their efficacy and safety.\nThis asymmetry creates opportunities for false ad-\nvertising and misleading marketing, particularly in\nthe highly interactive and instant-feedback environ-\nment of live streaming (Auronen, 2003).\nFrom the four domains, we carefully selected\n25 live streaming channels as data sources. These\nchannels are well-known on the platform and have\nhigh sales, ensuring they are representative. We\ncrawled a total of 7,812 live video clips, each lim-\nited to 60 seconds. This duration ensures sufficient\ninformation capture while reducing data processing\ncomplexity to some extent, providing rich material\nfor subsequent data annotation.\nASR Process: We first need to convert the audio\ninformation into text format. We tested the tran-\nscription performance of mainstream ASR tools\nin this scenario, with FunASR (Gao et al., 2023)\nachieving the best recognition results, followed by\nKaldi (Ravanelli et al., 2019) and Whisper (Rad-\nford et al., 2023). We employed this FunASR to\nperform ASR, converting the spoken content in the\ncrawled videos into text for subsequent morph an-\nnotation. A total of 86,750 speech statements were\ntranscribed.\nThis process of converting video to text not only\n2https://www.douyin.com/\nadds a new modality to the research but also makes\nthe form of morphs more flexible and varied. In the\nvideo context, morph words themselves are very\ndifficult to distinguish by ASR. Additionally, other\nfactors such as the host’s colloquial expressions,\nfast speaking pace, and background noise can lead\nto inaccuracies in ASR recognition results, result-\ning in a more diverse range of extracted morph\nforms.\nLabel Suggestions via LLMs: Recently, LLMs\nhave been widely used for data annotation (Zhang\net al., 2023). Despite the challenges posed by the\npresence of grammatical morphs in the annotation\nof morphs, LLMs with their powerful contextual\nlearning capabilities, can still identify some stan-\ndard morphs and provide the correct original terms.\nTherefore, we provided the annotation suggestions\nfrom the LLMs to human annotators as a refer-\nence, assisting them in the annotation process to\nenhance both efficiency and accuracy. Whether\nsome morphs recommended by the LLMs actually\nexist in the original document, annotators can more\nquickly locate the variant words. To specifically il-\nlustrate the performance of LLMs in LiveAMR\ntask, we selected three representative LLMs as\nbaselines to comparison.\nHuman Annotation: In order to make it eas-\nier for annotators to label, we created a website\nfor annotation. We provided corresponding videos\nand LLM annotation suggestions as auxiliary infor-\nmation, with video support being essential. When\nwe attempted annotation without referencing the\nvideos, annotators reported that many words could\nnot be clearly understood. We recruited three in-\nterns with bachelor’s degrees with annotation expe-\n"}, {"page": 4, "text": "rience and an understanding of morph characteris-\ntics as annotators.\nThe unique research scenarios required anno-\ntators to process multiple modalities of informa-\ntion, enhancing the quality and accuracy of the\nannotations. Prior to formal annotation, detailed\ntraining was provided, including explanations of\nguidelines and procedures, along with trial anno-\ntations to ensure understanding and adherence to\nthe tasks. Each annotator needs to undergo train-\ning before starting their annotation work, and they\ncan only begin once they have passed the training.\nAs a result, the annotation process yielded 6,853\npositive sentences containing morphs and 90,137\nnegative sentences without morphs.\nData Filtering: Despite manually annotating\nmorph words, we found that a small number of\nvariant words were still not annotated. Therefore,\nwe further adopted a process of human-machine\ncollaboration for secondary annotation to achieve\nthe goal of constructing a high-quality dataset.\nFirst, we use the corpus manually annotated in\nthe previous step to build a morph resolution model,\nemploying both rule-based method and pre-trained\nlanguage model based method. Second, we auto-\nmatically annotate the manually annotated corpus\nfrom the previous step using the trained method.\nThird, we manually verify the correctness of the\nmachine’s automatic annotation results, retaining\ncorrect annotations and discarding incorrect ones.\nFinally, the morphs corresponding to each original\ndocument are the combination of the results from\nthe previous manual annotation and this step of\ncollaborative annotation.\n(1) Rule-based method: Using the corpus man-\nually annotated in the previous step, we constructed\na morph dictionary D whic contains 430 original\nwords and their corresponding 2,688 morphs. Each\nentry in the dictionary contains one original word\nalong with their multiple morph words, where the\nrelationship between original word and morphs is\none-to-many.\nDuring the annotated process automatically, we\nsearch each instance of the manually annotated cor-\npus to find the morphs in the dictionary. If a match\nis found, this instance and the identified morph\nword will undergo further manual verification\n(2) Pre-trained language model based method:\nUsing the manually annotated corpus, we fine-\ntuned the pre-trained language model Mengzi-T5\n(Zhang et al., 2021). The details of the method is\nshown in section 5.1. During the annotated pro-\ncess automatically, each instance is input into the\nfine-tuned model, and the model’s input and output\nwere compared. If the input and output differed,\nit indicated that there might be omitted morph in\nthe sample. These samples were further examined,\nand upon confirmation, they were appropriately\nannotated.\nPositive&Negative\nMorph Num\nTrain\n6,236/76,554\n7,301\nValid\n800/800\n1,025\nTest1\n800/800\n1,081\nTest2\n400/400\n548\nTable 2: The statistics of the constructed Chinese morph\ndataset.\nData Analysis: Since the dataset construction is\nhighly dependent on ASR outputs, the same speech\ninput may produce different ASR results when pro-\ncessed by different ASR models. For example, the\nmorph form “白某障”(bái mˇou zhàng) for “白内\n障”(bái nèi zhàng, cataract) could be transcribed\nas “白母障”(bái mˇu zhàng), “白某张”(bái mˇou\nzh¯ang), “白某章”(bái mˇou zh¯ang) by different\nASR models.\nTo conduct a more comprehensive evaluation,\nWe re-annotated the second test set (denoted Test2),\nselecting both different live streaming rooms and\ndifferent ASR method. The Test2 includes 400\npositive and negative instances.\nFollowing the above process, we constructed a\nhigh-quality and comprehensive morph dataset, as\nshown in Table 2. Dataset consists of 8,236 positive\nsamples and 78,554 negative samples. The dataset\nincludes a total of 431 original words and their\ncorresponding 2,688 morphs forms, in which each\nword has nearly 7 morph words on average.\n5\nMethods\nLiveAMR method: Existing morph resolution\nmethods generally use non-autoregressive language\nmodel MacBERT, a corrective masked language\nmodel pre-training task was added to the BERT\nmodel (Wang et al., 2024). In the LiveAMR task,\nsince the length of the variant words does not equal\nthe length of the original word, we will use a text-\nto-text pre-trained model as a backbone, such as\nBART (Lewis, 2019) and Mengzi-T5 (Zhang et al.,\n2021). Below are the steps involved in this process.\nThe created dataset consists of source-target\npairs (X and Y ), where: X is the input text ( live\nstream transcript), Y is the desired output text (the\n"}, {"page": 5, "text": "normal text without morph words). The goal of the\nmodel is to learn a mapping from X to Y .\nThe pre-trained model M is a transformer-based\nsequence-to-sequence architecture, which is typi-\ncally structured as: (1) Encoder: Takes the input\nsequence X and encodes it into hidden states; (2)\nDecoder: Takes the encoder’s hidden states and\ngenerates the target sequence Y .\nDuring training, the model aims to minimize the\nloss, which is typically the Cross-Entropy Loss\nfor text generation tasks. The formula for Cross-\nEntropy Loss is:\nL = −\nT\nX\ni=1\nV\nX\nv=1\nˆyi,v log p(yi,v|X)\nwhere T is the length of the target sequence, V\nis the size of the vocabulary, ˆyi,v is a one-hot en-\ncoding of the true token at position i in the target\nsequence, and p(yi,v|X) is the predicted probabil-\nity of token yi at position i given the input X.\nDuring training, the model minimizes the loss\nfunction L with respect to the model parameters θ\nover multiple iterations (epochs):\nθ⋆= arg min\nθ\nE[L(X, Y ; θ)]\nWhere E denotes the expectation over the training\ndata, L(X, Y ; θ) is the loss function dependent on\nthe input X, the target Y , and the model parameters\nθ.\nAfter fine-tuning, the model generates new out-\nputs for unseen inputs. This is done by feeding\nthe input Xinput through the model to obtain the\npredicted sequence Ypred:\nYpred = M(Xinput)\nWhere Ypred is the generated sequence, which can\nbe decoded back into text.\nData Augmentation via LLMs: Some studies\nsuggest that LLMs can be used to generate training\ndatasets (Ding et al., 2023). Although manual an-\nnotation can yield morph data from the real world,\nit comes at a high cost and may contain some re-\ndundancy, limiting the scale and diversity of the\ndataset. Therefore, we aim to leverage LLMs to\ngenerate more morph data to supplement manually\nannotated data and enhance the model’s generaliza-\ntion ability.\nHowever, given the complexity of morph forms\nand the limitations of LLMs in understanding them,\nwe did not directly ask the LLMs to generate sen-\ntences containing morphs. To this end, we propose\na more reliable construction strategy that combines\nthe annotated morphs lexicon with LLM capabili-\nties. The specific steps are as follows:\n(1) We randomly select a positive example from\nthe training set and extract the corresponding\nmorph words WS. There may be one or more\nmorph words.\n(2) Based on the morph dictionary D, we obtain\nthe original word WO for WS.\n(3) We had the LLM simulate a live commerce\nscenario to generate 5 different sentences contain-\ning WO.\n(4) According to the morph dictionary D, we\nreplace the original word WO with different morph\nwords to construct a set of sentences containing\ndifferent morph words.\nThrough this approach, we constructed a manu-\nally created morph dataset containing 11,280 posi-\ntive samples and 2,155 negative samples. Addition-\nally, each positive sample generated by the LLM\naverages 2.87 morphs. This data effectively supple-\nments the manually annotated data, increasing the\nscale and diversity of the model’s training data. In\nTable 6, show some specific examples.\n6\nExperiment\n6.1\nExperimental Setup\nMetrics. We expect the model to modify only the\nmorphs in the target sentences without altering any\nother parts. A strict sentence-level assessment is\napplied: a positive sample is considered success-\nfully predicted only when all morphs are correctly\nrestored. For negative samples, a negative sample\nis deemed successfully predicted only if the model\nmakes no modifications at all.\nBaselines. The following models were selected\nas the baseline for comparison:\n(1)LLMs: To explore the morphs resolution ca-\npabilities of LLMs, we chose three representative\nmodels in the field of Chinese language understand-\ning: GPT-3.5-turbo 3, Deepseek -V24, and GLM4-\nPlus5. We manually selected 8 examples from the\ntraining set, including 6 positive samples and 2 neg-\native samples, to be added as context to the prompt.\nThe temperature was uniformly set to 0.7.\n3https://openai.com/\n4https://platform.deepseek.com/\n5https://chatglm.cn/\n"}, {"page": 6, "text": "Test1\nTest2\nMethod\nAcc\nPre\nRecall\nF1\nAcc\nPre\nRecall\nF1\nGPT\n0.405\n0.421\n0.320\n0.364\n0.496\n0.494\n0.441\n0.466\nDeepseek\n0.605\n0.660\n0.529\n0.587\n0.677\n0.667\n0.626\n0.646\nGLM\n0.451\n0.484\n0.515\n0.499\n0.532\n0.525\n0.649\n0.580\nKenlm\n0.583\n0.607\n0.372\n0.537\n0.516\n0.515\n0.513\n0.514\nSeq2Edit\n0.651\n0.968\n0.361\n0.526\n0.702\n0.987\n0.408\n0.588\nConvseq2seq\n0.740\n0.978\n0.527\n0.685\n0.687\n0.898\n0.421\n0.573\nBART\n0.708\n0.701\n0.767\n0.738\n0.656\n0.670\n0.611\n0.639\nT5\n0.893\n0.989\n0.801\n0.888\n0.760\n0.968\n0.536\n0.690\n+Aug\n0.928\n0.937\n0.927\n0.932\n0.863\n0.929\n0.787\n0.852\nTable 3: The results of different methods, where “+Aug” indicates fine-tuned the model using data augmentation via\nLLM.\n(2)Seq2seq Model: We selected two Seq2seq\nmodels Convseq2seq (Gehring et al., 2017) and\nBART (Lewis, 2019) as backbone, and fine-tune\nthe model on the constructed training datset.\n(3)Others: To better illustrate that seq2seq is\nmore suitable for the morph resolution task, we\nchose to analyze the statistical language model\nKenlm (Heafield, 2011) and BERT-based model\nSeq2Edit (Omelianchuk et al., 2020).\n(4) Our method: It is based on T5 (mengzi-T5\n(Zhang et al., 2021)). This model adopts the T5\ntraining paradigm and has been retrained on large-\nscale Chinese corpora.\n6.2\nImplementation Details\nIt is based on T5 (mengzi-T5 (Zhang et al., 2021)).\nThe Mengzi T5 model includes an encoder and de-\ncoder, where each consisting of 12 layers of Trans-\nformer layers. This model adopts the T5 training\nparadigm and has been retrained on large-scale\nChinese corpora.\nDuring the training process, the maximum length\nof the input sequence is set to 128, and the initial\nlearning rate is set to 1e-4. We train the model for\n20 epochs on a 24GB Nvidia 3090Ti GPU with the\nbatch size set to 32. We use the AdamW optimizer,\nand the model employs a cosine annealing learning\nrate schedule.\n6.3\nExperimental Results\nThe experimental results, presented in Table 3, re-\nveal that character-level correction methods like\nSeq2Edit and the statistical language model Kenlm\nare inadequate for addressing morphs in live\nstreaming scenarios. In contrast, Seq2seq mod-\nels (Convseq2seq, BART, and T5) perform better\nat managing inconsistencies in output length. No-\ntably, the T5 model achieved the highest F1 score\nacross both test sets, demonstrating its effective-\nness for this task.\nFor T5 method, the results via data augmenta-\ntion improved the F1 scores of T5 model by 4.95%\non Test1; on Test2, the improvements was 23.47%.\nOur method shows stable performance across dif-\nferent test sets due to its contextual learning capa-\nbilities. On Test1, its performance is slightly lower\nthan the baseline model, likely because the baseline\nexcels with data similar to the training set. How-\never, on Test2, which uses data from a different\nASR model, the LLM’s performance matches that\nof fine-tuned Seq2seq models, demonstrating its\ngeneralization ability with varied data distributions.\n6.4\nUsefulness of Morph Resolution\nTo investigate the role of morph resolution in de-\ntecting violations in e-commerce live streaming\nscenarios, we conducted a simple usability experi-\nment.\nSetup. We selected 4,641 live streaming clips\nfor ASR processing and annotated the transcription\nresults for each clip. After thorough consultation\nwith market regulators, we have categorized the\nidentification of violations in live-streaming sales\nvideos into three types: compliance, suspected vi-\nolation, and serious violation. Specifically, the\n\"compliance\" category refers to content that fully\nadheres to relevant regulations and platform rules,\nwithout any violation. The \"suspected violation\"\ncategory covers content that may potentially in-\nvolve violation behaviors but requires further verifi-\ncation, such as suspected acts of inducing irrational\nconsumption. The \"serious violation\" category per-\ntains to actions that are explicitly prohibited by the\nplatform or regulations, such as promoting health-\ncare products as drugs.\nWe annotated a total of 4,447 instances including\n2,430 compliances, 1,305 suspected violations, and\n712 serious violations. We divided them into a\n"}, {"page": 7, "text": "Table 4: Statistical information on dataset.\nClass\nNumber\nTraining set\nCompliance\n2,250\nSuspected violation\n557\nSerious Violation\n1,150\nValidation Set\nCompliance\n130\nSuspected violation\n130\nSerious Violation\n130\nTest set\nCompliance\n50\nSuspected violation\n25\nSerious Violation\n25\ntraining set, a validation set, and test set. The test\nset includes 100 samples, and the validation set\ncontains 390 samples. The statistical information\nof the constructed CLiveSVD dataset is presented\nin Table 4.\nMethod\nCat.\nAcc\nPre\nRecall F1\nDefalut\n0\n0.81\n0.917\n0.88\n0.89\n1\n0.81\n0.77\n0.68\n0.72\n2\n0.91\n0.66\n0.80\n0.72\nMorph\n0\n0.90\n0.96\n0.96\n0.96\n1\n0.90\n0.77\n0.84\n0.80\n2\n0.90\n0.91\n0.84\n0.87\nTable 5: Comparison of experimental results. \"Default\"\nindicates that the ASR results of the video are not pro-\ncessed. \"Morph\" refers to the processing of the ASR\nresults for morph resolution. \"0\" represents compliant\ncategories, \"1\" indicates suspected violation categories,\nand \"2\" denotes serious violation categories.\nImplements. It is important to note that in the\ndefault method, neither the training set nor the test\nset undergoes any changes, while in the compari-\nson method, both the training set and the test set are\nprocessed with morph resolution. The BERT (Ken-\nton and Toutanova, 2019) model was fine-tuned for\nclassification task.\nResults. As shown in Table 5, after resolution\nmorphs in the original ASR results, the F1 scores\nfor the compliant, suspected violation, and serious\nviolation categories increased by approximately\n6.91%, 11.76%, and 20.36%, respectively, com-\npared to the unprocessed results. This demonstrates\nthat morph resolution can significantly improve the\nmodel’s accuracy in detecting v.\n6.5\nAblation Study\nWe explored the impact of data augmentation quan-\ntity on model performance. As shown in Section 5,\nFigure 2: Performance with different number of training\nsamples.\nwe controlled the data augmentation by setting the\nnumber of sentences generated for each original\nword. The sentence counts were set to 1, 2, 3, 4,\n5, and 6, resulting in data volumes of 2,693, 5,373,\n8,058, 10,744, 14,405, and 16,116, respectively.\nIn Figure 2, the experimental results show that\ndata augmentation has a significant positive impact\non model performance. At the same time, when the\nvariable is set to 5, the number of augmented sam-\nples reaches 14,405, and the model’s performance\ntends to stabilize.\n7\nConclusion\nThis study introduces the task of morph resolu-\ntion in live streaming scenarios, termed LiveAMR.\nA LiveAMR dataset was created through human-\nLLM collaboration, comprising 7,836 positive and\n91,119 negative samples. The study analyzed task\ncharacteristics and utilized a text-to-text model ar-\nchitecture for morph resolution. Given the imprac-\nticality of manually constructing large-scale train-\ning corpora, an efficient data augmentation method\nbased on LLMs was proposed, leveraging exist-\ning annotated data. Experimental results show that\nthis augmentation method enhances model perfor-\nmance compared to baselines. The findings also\nindicate that morph resolution can contribute posi-\ntively to streaming regulation.\nLimitations\nWe only annotated the live streaming domain where\nmorphs are frequently used to evade censorship,\nwithout covering all topics in the live streaming\nfield. Additionally, we validated the effectiveness\nof our proposed data augmentation method on only\n"}, {"page": 8, "text": "three models. In the future, we plan to expand\nthis dataset and continue exploring the linguistic\nphenomenon of morphs.\nEthics Statement\nAll data was collected from publicly available\nsources on the Douyin platform, ensuring no vi-\nolation of privacy or data protection laws. Our aim\nis to address false advertising in health and medical\nlive streams, contributing to consumer protection\nand industry standardization. Furthermore, this\nwork serves the dual purposes of addressing moral\nconcerns and navigating political censorship.\nHuman annotation was conducted by trained an-\nnotators who followed ethical guidelines, and we\nused large language models to enhance annotation\naccuracy. No personal or sensitive information was\nused, and all data was anonymized to prevent mis-\nuse.\nOur findings support the development of tools\nto combat deceptive practices in e-commerce live\nstreaming, ultimately benefiting consumers. The\ndataset and code will be made publicly available\nfollowing ethical guidelines to encourage further\nresearch.\nAcknowledgement\nThis research is partially supported by the Na-\ntional Language Commission of China (ZDI145-\n71), the National Natural Science Foundation of\nChina (62076217), the Blue Project of Jiangsu and\nYangzhou University, and the Top-level Talents\nSupport Program of Yangzhou University.\nReferences\nLauri Auronen. 2003. Asymmetric information: theory\nand applications. In Seminar of Strategy and Interna-\ntional Business as Helsinki University of Technology,\nvolume 167, pages 14–18. Citeseer.\nCINI Center. 2022. The 50th statistical report on china’s\ninternet development. Beijing2022.\nMonojit Choudhury, Rahul Saraf, Vijit Jain, Animesh\nMukherjee, Sudeshna Sarkar, and Anupam Basu.\n2007. Investigation and modeling of the structure of\ntexting language. International Journal of Document\nAnalysis and Recognition (IJDAR), 10:157–174.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin,\nShengding Hu, Zhiyuan Liu, Maosong Sun, and\nBowen Zhou. 2023. Enhancing chat language models\nby scaling high-quality instructional conversations.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3029–3051.\nZhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian\nShi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao\nDu, Zhangyu Xiao, and Shiliang Zhang. 2023. Fu-\nnasr: A fundamental end-to-end speech recognition\ntoolkit. In INTERSPEECH.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional se-\nquence to sequence learning. In International confer-\nence on machine learning, pages 1243–1252. PMLR.\nKenneth Heafield. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pages\n187–197.\nLongtao Huang, Ting Ma, Junyu Lin, Jizhong Han,\nand Songlin Hu. 2019. A multimodal text match-\ning model for obfuscated language identification in\nadversarial communication? In The World Wide Web\nConference, pages 2844–2850.\nLongtao Huang, Lin Zhao, Shangwen Lv, Fangzhou Lu,\nYue Zhai, and Songlin Hu. 2017. Kiem: a knowledge\ngraph based method to identify entity morphs. In\nProceedings of the 2017 ACM on conference on in-\nformation and knowledge management, pages 2111–\n2114.\nHeng Ji and Kevin Knight. 2018. Creative language\nencoding under censorship. In Proceedings of the\nFirst Workshop on Natural Language Processing for\nInternet Freedom, pages 23–33.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of naacL-HLT, volume 1, page 2. Min-\nneapolis, Minnesota.\nMasamune Kobayashi, Masato Mita, and Mamoru Ko-\nmachi. 2024. Revisiting meta-evaluation for gram-\nmatical error correction. Transactions of the Associa-\ntion for Computational Linguistics, 12:837–855.\nM Lewis. 2019.\nBart:\nDenoising sequence-to-\nsequence pre-training for natural language genera-\ntion, translation, and comprehension. arXiv preprint\narXiv:1910.13461.\nGengsong Li, Hongmei Li, Yu Pan, Xiang Li, Yi Liu,\nQibin Zheng, and Xingchun Diao. 2022. Name dis-\nambiguation based on entity relationship graph in big\ndata. In International Conference on Data Mining\nand Big Data, pages 319–329. Springer.\nKostiantyn Omelianchuk, Vitaliy Atrasevych, Artem\nChernodub, and Oleksandr Skurzhanskyi. 2020.\nGector–grammatical error correction:\nTag, not\nrewrite. In Proceedings of the Fifteenth Workshop\non Innovative Use of NLP for Building Educational\nApplications, pages 163–170.\n"}, {"page": 9, "text": "Jipeng Qiang, Yang Li, Chaowei Zhang, Yun Li, Yi Zhu,\nYunhao Yuan, and Xindong Wu. 2023a. Chinese\nidiom paraphrasing. Transactions of the Association\nfor Computational Linguistics, 11:740–754.\nJipeng Qiang, Kang Liu, Ying Li, Yun Li, Yi Zhu,\nYun-Hao Yuan, Xiaocheng Hu, and Xiaoye Ouyang.\n2023b. Chinese lexical substitution: Dataset and\nmethod. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 29–42.\nJipeng Qiang, Kang Liu, Yun Li, Yunhao Yuan, and\nYi Zhu. 2023c. Parals: Lexical substitution via pre-\ntrained paraphraser. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 3731–\n3746.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In International conference on machine\nlearning, pages 28492–28518. PMLR.\nMirco Ravanelli, Titouan Parcollet, and Yoshua Bengio.\n2019. The pytorch-kaldi speech recognition toolkit.\nIn ICASSP 2019-2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6465–6469. IEEE.\nYing Sha, Zhenhui Shi, Rui Li, Qi Liang, and Bin\nWang. 2017.\nResolving entity morphs based on\ncharacter-word embedding. Procedia Computer Sci-\nence, 108:48–57.\nAobo Wang, Min-Yen Kan, Daniel Andrade, Takashi\nOnishi, and Kai Ishikawa. 2013. Chinese informal\nword normalization: an experimental study. In Pro-\nceedings of the Sixth International Joint Conference\non Natural Language Processing, pages 127–135.\nNannan Wang, Cheng Huang, Junren Chen, and Lingzi\nLi. 2024. Cmright: Chinese morph resolution based\non end-to-end model combined with enhancement\nalgorithms. Expert Systems with Applications, page\n124294.\nQingyu Wang, Tielin Zhang, Minglun Han, Yi Wang,\nDuzhen Zhang, and Bo Xu. 2023a. Complex dy-\nnamic neurons improved spiking transformer net-\nwork for efficient automatic speech recognition. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, pages 102–109.\nWenxuan Wang, Jen-tse Huang, Weibin Wu, Jianping\nZhang, Yizhan Huang, Shuqing Li, Pinjia He, and\nMichael R Lyu. 2023b. Mttm: Metamorphic test-\ning for textual content moderation software. In 2023\nIEEE/ACM 45th International Conference on Soft-\nware Engineering (ICSE), pages 2387–2399. IEEE.\nPinghui Xiao. 2024.\nThe rise of livestreaming e-\ncommerce in china and challenges for regulation:\nA critical examination of a landmark case occurring\nduring covid-19 pandemic. Computer Law & Secu-\nrity Review, 52:105955.\nYing Xu. 2024. Research on legal regulation of false\npropaganda behavior in online live streaming sales in\nchina. Open Journal of Legal Science, 12:3338.\nTaijin Yoon, Sun-Young Park, and Hwan-Gue Cho.\n2010. A smart filtering system for newly coined pro-\nfanities by using approximate string alignment. In\n2010 10th IEEE International Conference on Com-\nputer and Information Technology, pages 643–650.\nIEEE.\nJirong You, Ying Sha, Qi Liang, and Bin Wang. 2018.\nMorph resolution based on autoencoders combined\nwith effective context information. In Computational\nScience–ICCS 2018: 18th International Conference,\nWuxi, China, June 11–13, 2018 Proceedings, Part III\n18, pages 487–498. Springer.\nRuoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou,\nand Lei Zou. 2023. LLMaAA: Making large lan-\nguage models as active annotators. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 13088–13103, Singapore. Association\nfor Computational Linguistics.\nZhuosheng Zhang, Hanqing Zhang, Keming Chen,\nYuhang Guo, Jingyun Hua, Yulong Wang, and Ming\nZhou. 2021. Mengzi: Towards lightweight yet inge-\nnious pre-trained models for chinese. arXiv preprint\narXiv:2110.06696.\nA\nThe annotation Website\nWe have built a website based on Vue+FastAPI for\nannotators’ labeling work, as shown in Figure 3.\nDue to the unique nature of the research scenarios,\nthe annotators needed to process multiple modal-\nities of information, which enhanced the quality\nand accuracy of the annotation results. At the same\ntime, this is a time-consuming task, and we extend\nour sincerest gratitude to the annotators for their\nefforts.\nB\nPrompt templates in this paper\nChatGPT-Generate Sentences. The prompting\ntemplate of ChatGPT-Generate sentences include\ntargets words is shown in Figure 4.\nC\nMore Examples\nHere, we randomly some samples from morph\ndataset in Table 6.\n"}, {"page": 10, "text": "Method\nSentence\nReal\nBC组合在三号选项三宝贝那维生c呢孩子，我们自己老年人免某粒特别弱，经常被其他人连带，经\n常阿秋阿秋的。\nThe BC combination in option three significantly impacts children. Older adults have particularly weak\nimmunity and often catch colds from others.\n免某粒(miˇan mˇou lì:Free of certain pills):免疫力(miˇan yì lì, immunity)\n阿秋阿秋(¯a qi¯u ¯a qi¯u,Aqiu Aqiu):感冒(gˇan mào,catarrh)\n都知道用小蓝帽什么意思吧，对不对？\nYou all know what the little blue hat means, right?\n小蓝帽(xiˇao lán mào,small blue hat):保健食品标志(bˇao jiàn shí pˇin bi¯ao zhì,Health Supplement Approval\nMark)\n我们一号链接三百一十八米，两桶。\nOur link number one is 318 yuan, for two barrels.\n米(mˇi,rice)元(yuán,yuan)\nLLM\n想要改某善身体某平某衡？试试我们的新品，今天下单有特别优惠，立减50米！\nWant to improve your balance? Try our new product, order today for a special discount of 50 yuan off!\n改某善(gˇai mˇou shàn,improvement):改善(gˇai shàn,improvement)\n某平某衡(mˇou píng mˇou héng,balance):平衡(píng héng,balance)\n米(mˇi,rice)元(yuán,yuan)\n我们的产品专为孕妈妈设计，能够帮助控制糖高，减轻身体猛副某用，让孕期更加轻松。\nOur products are designed specifically for pregnant women to help control hyperglycemia and relieve certain\nbody effects, making pregnancy easier.\n孕妈妈(yùn m¯a m¯a,Pregnant mother):孕妇(yùn fù,pregnant)\n糖高(táng g¯ao,high in sugar):高血糖(g¯ao xuè táng,hyperglycemia)\n猛副某用(mˇeng fù mˇou yòng,side effect):副作用(fù zuò yòng,side effect)\n运和动不仅有助于心血管健康，还能减少某血某栓形成的风险，百大褂也经常强调这一点。\nExercise not only helps cardiovascular health, but also reduces the risk of thrombus, which doctors often\nemphasize.\n运和动(yùn hé dòng,movement and motion):运动(yùn dòng,exercise)\n某血某栓(mˇou xuè mˇou shu¯an,thrombus)：血栓(xuè shu¯an,thrombus)\n百大褂(bˇai dà guà,people in white):医生(y¯ı sh¯eng,doctor)\nTable 6: Morph sample display: The first row contains sentences with morphs, the second row is the translation, and\nthe third row shows the morph annotation results. \"Real\" indicates that the data source is real data, not synthetic\ndata. \"LLM\" indicates data synthesized using an LLM-based method, shown in 5.\nvideo\nLLM label suggestions\nASR result\nHuman annotation\nFigure 3: Screenshot of an annotation example on the\nannotation Website. The red text indicates added com-\nments.\nYour role is that of a live-streaming host promoting products.\nYou need to generate five promotional sentences that include\nthe target words. Here are some real promotional sentences\nfor you to mimic. The sentences should not have repeated\nmeanings. The target word should remain unchanged. The\nlength of the sentences should be as consistent as possible\nwith the examples provided.\nTarget Words:\n[Target Words]\nExamples:\n[Examples]\nGenerated Sentences:\nFigure 4: The prompting template of generating sen-\ntences. Generate context-appropriate sentences that\ncontain the specified vocabulary and meet the required\nquantity.\n"}]}