{"doc_id": "arxiv:2512.14604", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.14604.pdf", "meta": {"doc_id": "arxiv:2512.14604", "source": "arxiv", "arxiv_id": "2512.14604", "title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts", "authors": ["Prasanjit Dubey", "Aritra Guha", "Zhengyi Zhou", "Qiong Wu", "Xiaoming Huo", "Paromita Dubey"], "published": "2025-12-16T17:14:10Z", "updated": "2025-12-16T17:14:10Z", "summary": "Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.14604v1", "url_pdf": "https://arxiv.org/pdf/2512.14604.pdf", "meta_path": "data/raw/arxiv/meta/2512.14604.json", "sha256": "4d7a4d1f10009be95a002cb8fe6ba8f4fc9f019082f2f934cd6c9b0e3a2ec3d9", "status": "ok", "fetched_at": "2026-02-18T02:24:19.045000+00:00"}, "pages": [{"page": 1, "text": "LLmFPCA-detect: LLM-powered Multivariate Functional\nPCA for Anomaly Detection in Sparse Longitudinal Texts\nPrasanjit Dubey1,∗\nAritra Guha2\nZhengyi Zhou2\nQiong Wu2\nXiaoming Huo1\nParomita Dubey3\n1H. Milton Stewart School of Industrial and Systems Engineering,\nGeorgia Institute of Technology\n2AT&T Chief Data Office\n3Department of Data Sciences and Operations, Marshall School of Business,\nUniversity of Southern California\nAbstract\nSparse longitudinal (SL) textual data arises when individuals generate text repeatedly over\ntime (e.g., customer reviews, occasional social media posts, electronic medical records across\nvisits), but the frequency and timing of observations vary across individuals. These complex\ntextual data sets have immense potential to inform future policy and targeted recommenda-\ntions. However, because SL text data lack dedicated methods and are noisy, heterogeneous,\nand prone to anomalies, detecting and inferring key patterns is challenging. We introduce\nLLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with func-\ntional data analysis to detect clusters and infer anomalies in large SL text datasets. First,\nLLmFPCA-detect embeds each piece of text into an application-specific numeric space us-\ning LLM prompts. Sparse multivariate functional principal component analysis (mFPCA)\n∗Corresponding author. 755 Ferst Dr NW, Atlanta, GA 30332, USA. Email: pdubey31@gatech.edu\n1\narXiv:2512.14604v1  [stat.ML]  16 Dec 2025\n"}, {"page": 2, "text": "conducted in the numeric space forms the workhorse to recover primary population charac-\nteristics, and produces subject-level scores which, together with baseline static covariates,\nfacilitate data segmentation, unsupervised anomaly detection and inference, and enable other\ndownstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling\nguided by the data segments and anomalies discovered by LLmFPCA-detect, and we show\nthat cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing\npipelines, help boost prediction performance. We support the stability of LLmFPCA-detect\nwith experiments and evaluate it on two different applications using public datasets, Ama-\nzon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating\nutility across domains and outperforming state-of-the-art baselines.\n1\nIntroduction\nIn modern machine learning, it is common to encounter datasets comprising of N subjects, where\neach subject i is associated with a sequence of textual observations {Ki(Ti1), Ki(Ti2), . . . , Ki(TiNi)}\nrecorded at sparse and irregular time points {Ti1, Ti2, . . . , TiNi} ⊂R. Despite LLMs having spurred\nmany advancements in analysis of text data, current methods are not well adapted to sparse\nlongitudinal (SL) designs—time-evolving texts observed at irregular, subject-specific times—so\nthese are frequently discarded or collapsed across time, ignoring the dynamic patterns in the\ntexts. In this paper, we propose a novel framework for the analysis of SL text data that yields\nrepresentations suitable for straightforward integration into unsupervised and supervised learning\npipelines. The proposed methodology is applicable to a wide range of domains that generate SL text\ndata, such as, electronic medical records in healthcare [Ford et al., 2016], consumer interactions\nthrough service channels in business [Cavique et al., 2022], activity logs from online learning\nplatforms in education [Yang and Kang, 2020], user posts and comments on social media Hutto\net al. [2013], Valdez et al. [2020], Kelley and Gillan [2022] and many more.\nA major challenge with SL text datasets is that observations are unstructured and noisy, het-\nerogeneous across subjects, and may contain outliers. The first step in making such data amenable\nfor downstream supervised or unsupervised learning tasks, including prediction and inference, is\n2\n"}, {"page": 3, "text": "to extract parsimonious feature representations of the longitudinal texts that capture the lead-\ning modes of variation.\nIn this work, we propose LLmFPCA-detect, which starts from noisy\nSL texts and produces learned representations, accounting for heterogeneity and providing type-I-\nerror–controlled outlier screening. LLmFPCA-detect begins by embedding text into an application\nspecific numeric space using LLMs. In this numeric space, sparse multivariate functional principal\ncomponent analysis(mFPCA) Happ and Greven [2018], Yao et al. [2005] is used to model the lon-\ngitudinal text embeddings as noisy observations of an underlying smooth trajectory. The method\nfirst clusters the preliminary FPC scores, augmented with baseline subject-level covariates, and\nthen screens for outliers; a novel calibration step yields the final set of anomalies with statistical\nsignificance guarantees. We illustrate this new approach on two datasets: the Amazon review\ncorpus and the Wikipedia talk- page comment stream, where LLmFPCA-detect reveals insightful\nfindings from SL text data.\nRelated Works\nModeling SL data Beginning with the seminal parametric random-effects for-\nmulation Laird and Ware [1982], the field of longitudinal data analysis has undergone extensive\ndevelopment over the decades; see Verbeke et al. [2014] for a review on multivariate longitudinal\ndata analysis. Functional data analysis (FDA) provides a nonparametric framework for SL data—\nvia principal components through conditional expectation Yao et al. [2005], Happ and Greven\n[2018]—to predict subject-specific smooth trajectories even from one or a few observations. While\nthis line of work has expanded to include dynamic Hao et al. [2024], Zhou and Mueller [2024]\nand covariate-dependent Kim et al. [2023] extensions, and has led to methods for clustering and\nunsupervised anomaly detection Schmutz et al. [2020], Wu et al. [2023], Castrillón-Candás and\nKon [2022], and supervised tasks such as regression and classification Müller [2005], none of these\nmethods extend directly to heterogeneous, complex SL text data paired with baseline covariates\nand containing outliers.\nText time series versus SL texts An SL design differs from a time series; instead of a sin-\ngle, regularly spaced sequence of observations, it comprises many subjects, each with its own\ntrajectory recorded at irregular, subject-specific times where per-subject sampling is sparse, and\nbetween-subject heterogeneity could be substantial. While text time-series modeling has advanced\n3\n"}, {"page": 4, "text": "considerably O’Connor et al. [2010], Blei and Lafferty [2006], Wang and McCallum [2006], Bamler\nand Mandt [2017], Dodds et al. [2011], Griffiths and Steyvers [2004], Yurochkin et al. [2019], these\napproaches rely on dense, uniformly spaced observations and are not suited to SL texts.\nAnomaly detection Text clustering and anomaly detection are central NLP tasks, used to flag\nharmful content, phishing, and spam. With pretrained language models (e.g., BERT Devlin et al.\n[2019], RoBERTa Liu et al. [2019], GPT Brown et al. [2020]), embedding-based detectors have\nproliferated alongside other approaches Yin and Wang [2016], Cao et al. [2025], Ruff et al. [2019],\nSubakti et al. [2022], Dhillon and Modha [2001], Liu et al. [2008], Kannan et al. [2017]. Yet three\nlimitations persist: (i) most methods lack type-I error control for flagged anomalies; (ii) time\nseries anomaly detectors Blázquez-García et al. [2021], Zamanzadeh Darban et al. [2024], Xu et al.\n[2022] can be adapted to unstructured texts via embeddings, but only assuming dense, regularly\nsampled streams; and (iii) these methods do not support SL designs with subject-specific, irregular\nobservation times and evolving trajectories, hence missing on the individual level dynamic trends in\nthe anomalies. Functional data analysis methods for SL anomaly detection exist [Sun and Genton,\n2011, Dai and Genton, 2018, Hubert et al., 2015, Gervini, 2009], but they operate on structured\nnumeric functions rather than unstructured text and likewise lack formal false-positive guarantees.\nAs a result, there is no end-to-end solution that transforms SL texts into trajectory-aware feature\nrepresentations and detects anomalies with explicit type-I error control.\nOur Contributions\nWe introduce LLmFPCA-detect, a novel framework that combines LLM-\nbased embeddings with sparse mFPCA to enable covariate-informed data segmentation and type-I\nerror controlled anomaly detection in sparsely observed, longitudinal, heterogeneous text data,\nyielding feature representations suitable for incorporating SL texts in a wide range of downstream\ntasks. LLmFPCA-detect is broadly applicable to settings involving subjects with time-stamped\ntext records that arrive irregularly over time. While we focus on sparsely sampled scenarios, the\nmethodology can be readily adapted to densely observed data. We demonstrate the effectiveness\nof LLmFPCA-detect through its application to the Amazon Reviews dataset (Amazon data) and\nthe Wikipedia talk-page comment streams (Wiki data). The key components of the framework,\nas illustrated in Figure 1, are:\n4\n"}, {"page": 5, "text": "1. Representation We derive domain-appropriate LLM embeddings for each time-stamped text.\nFor the Amazon Reviews dataset, we embed the texts using emotion scores based on Plutchik’s\nWheel of EmotionsPlutchik [1980], which identifies eight primary emotions as the foundation\nfor all others. For the Wikipedia request–comment stream, we obtain toxicity and aggression\nscores using GPT for each comment to compare against findings from human-annotated scores.\n2. Learning trajectory representations and detection with guarantees The numeric tra-\njectories form multivariate SL data, which are processed using the mFPCA pipeline to obtain\nmultivariate functional principal component (mFPC) scores. These scores, combined with base-\nline covariates, are used for covariate-informed clustering. Anomalies are then detected in an\nunsupervised manner by: i) screening points in the tails of the cluster-specific mFPC score\ndistributions, and ii) statistically testing the screened points while controlling for multiple\ncomparisons. The identified anomalies are further analyzed to localize time window specific\ndeviations in the population.\n3. Interpretability and insights We use LLMs to extract keywords from texts associated with\neach cluster and flagged window, revealing dynamic, human-interpretable signals that explain\nwhy the flagged discovery matters.\nOrganization\nThe rest of the paper is organized as follows. Section 2 provides the motivation for\nthe clustering and anomaly detection steps of LLmFPCA-detect. Section 3 outlines the methods,\nestimation procedures, and algorithms that make up the different steps in LLmFPCA-detect.\nSections 4.1 and 4.2 demonstrates the application of LLmFPCA-detect to customer journey data\nfrom Amazon reviews and to Wikipedia request–comment streams, illustrating its cross-domain\napplicability. Additional details and experiments are provided in the Appendix.\n2\nMotivation and Framework\nIn this section, we present the foundational framework underlying LLmFPCA-detect.\n5\n"}, {"page": 6, "text": "Multivariate functional data representation\nFor each subject i = 1, . . . , N, a random func-\ntion Xi ∈L2(T )p is observed on a discrete, potentially irregular and sparse time grid {Tij}Ni,N\nj,i=1\nalong with baseline covariates Zi ∈Rq where (X, Z) ∼P, with P being the joint distribu-\ntion of (X, Z). The population mean function is defined as µ(t) = E(X(t)), and the covari-\nance surface for s, t ∈T is given by C(s, t) = E{(X(s) −µ(s)) ⊗(X(t) −µ(t))} with entries\nCij(s, t) = Cov(X(i)(s), X(j)(t)) is assumed to satisfy the conditions of Proposition 2 in Happ and\nGreven [2018]. Then, X admits a multivariate Karhunen–Loève expansion (Propositions 3 and 4\nin Happ and Greven [2018])\nX(t) = µ(t) +\n∞\nX\nj=1\nρmψm(t)\nwhere ρm = ⟨X(t) −µ(t), ψm(t)⟩with Cov(ρm, ρn) = λmI{m = n}, and λ1 ≥λ2 ≥· · · ≥0 are the\neigenvalues of the covariance operator associated with C. The corresponding eigenfunctions ψm,\nm ∈N serve as the multivariate functional principal components, with ρm being the associated\nmFPC scores. If X admits a finite expansion with M principal components, Proposition 5 in\nHapp and Greven [2018] establishes how mFPCA of X relates to univariate functional principal\ncomponent analysis (uFPCA) of each component X(d)(·) ∈L2(T ) for d = 1, . . . , p.\nFigure 1: Proposed framework.\nData heterogeneity and anomalies\nSuppose the trajec-\ntories {Xi}N\ni=1 belong to K distinct clusters, denoted by\nC1, . . . , CK, with S\nk Ck = {1, . . . , n} and Ck ∩Cj = ∅for j ̸= k.\nObservations in Ck are generated according to the distribu-\ntion Pk, yielding the overall mixture P = PK\nk=1 πkPk with\n(π1, . . . , πK) denoting cluster proportions. For i ∈Ck, assume\nthat Xi admits a finite multivariate Karhunen–Loève expan-\nsion Xi(t) = µk(t) + PM\nm=1 ρimψm(t),\ni ∈Ck, where µk ∈\nL2(T )p is the cluster-specific mean function, and ψm ∈L2(T )p\nare shared eigenfunctions across clusters. To incorporate pos-\n6\n"}, {"page": 7, "text": "sible measurement errors and anomalies, we observe\nY i(t) = Xi(t) + ηi(t) + ai(t),\nwhere ηi, ai ∈L2(T )p capture the measurement errors and anomalies respectively.\nThese are\nassumed to be jointly independent of Xi, i = 1, . . . , n, with E(ηi(t)) ≡0 for all t ∈T , and\nCov(η(j)(s), η(k)(t)) = σ2\nηIs=t for all j, k ∈{1, . . . , p}.\nThe term ai ≡0 almost surely for all\ni ∈AC\n0 , where A0 ⊂{1, . . . , N} denotes the set of anomalous subjects. For each i ∈A0, we\nassume ai(t) ̸= 0 for some t ∈T0 ⊂T almost surely. We employ trimmed k-means to recover\nthe clusters accurately despite being contaminated with outliers; for details on cluster recovery see\nSection D.1in Appendix D.\nCalibrating the anomalies\nAfter the clusters are recovered, the anomalous observations in\nA0 are assigned to one of the clusters C1, . . . , CK. To detect A0 in an unsupervised manner, we\nperform a screening step within each cluster by examining the tails of the FPC score distribution,\napproximating Ck ∩A0 by Ak,ϵ\n0\n⊂Ck (see Appendix E).\nThe distribution of FPC scores in the clean subset Ck ∩AC\n0 is then used to recover Ck ∩A0 with\nconfidence. In practice, each cluster Ck is randomly split into two subsets, and the non-screened\nportion is used to calibrate the anomaly detection procedure; see Theorem E.1 in Appendix E\nfor theoretical guarantees.\nFinally, based on the detected anomalous set A0, we analyze the\ncorresponding keywords across different time windows.\nThe foregoing framework outlines a pipeline for obtaining cluster-specific feature representa-\ntions and type-I controlled anomaly detection in fully observed multivariate functional trajectories\nwith possible measurement errors. In SL settings, each subject is observed at random time points\nTij for i = 1, . . . , N, j = 1, . . . , Ni, with Tij ∈T . These time points Ti1, . . . , TiNi are assumed i.i.d.\nand independent of Xi and ηi for all i. The number of measurements Ni is random, reflecting\nsparse and irregular designs, and Ni, for i = 1, . . . , N, are assumed i.i.d. and independent of all\nother random elements.\nIn practice, we observe Yi(Tij), j = 1, . . . , Ni, i = 1, . . . , N, and all relevant quantities must be\n7\n"}, {"page": 8, "text": "estimated from these noisy observations. Section 3 outlines the estimation details and algorithms\nfor this pipeline, including steps for incorporating the underlying textual data.\n3\nMethods: Pipeline and Estimation\nFrom SL Texts to Numeric Embeddings\nThe first step maps each time-stamped text Ki(Tij)\nto a p-dimensional vector via a fixed embedding\nΦ : X −→Rp,\nY i(Tij) = Φ\n\u0000Ki(Tij)\n\u0001\n,\n(3.1)\nwhere Φ, is implemented via LLM prompting, held constant across subjects, and deterministic (the\nsame text yields the same vector). For subject i this yields the multivariate trajectory {Y i(Tij)}Ni\nj=1,\nwhose coordinates are modeled jointly using mFPCA (e.g. Plutchik emotion embeddings for Ama-\nzon reviews; see Sections C and 4.1). Each subject also has baseline, time-invariant covariates\nZi ∈Rq (e.g. average rating, review length, engagement duration).\nAlgorithm 1 Multivariate Functional Principal Component Analysis (mFPCA)\nInput: SL data: {Y i(Tij)}Ni\nj=1 for i = 1, . . . , N.\n1: ({ˆξ(d)\nik }N,Kd\ni=1,k=1, ˆµ(d)(t), {ˆϕ(d)\nk (t)}Kd\nk=1) ←uFPCA({(Tij, Y (d)\ni\n(Tij))}i,j) for each dimension d =\n1, . . . , p.\n▷Algorithm 4; only scores are used below\n2: ˆΞi ←(ˆξ(1)\ni1 , . . . , ˆξ(1)\niK1, . . . , ˆξ(p)\ni1 , . . . , ˆξ(p)\niKp), i = 1, . . . , N\n▷Stack univariate FPC scores\n3: Define matrix ˆΞ ∈RN×M with rows ˆΞi where M = Pp\nd=1 Kd.\n4: ˆCΞ ←\n1\nN−1 ˆΞ⊤ˆΞ .\n▷Compute covariance matrix\n5: Perform eigen-decomposition of ˆCΞ to obtain eigenvalues {ˆλm}M\nm=1 and eigenvectors {ˆvm}M\nm=1.\n6: ˆψ(d)\nm (t) ←PKd\nk=1 ˆv(d)\nm,k ˆϕ(d)\nk (t), d = 1, . . . , p and m = 1, . . . , M.\n▷Multivariate eigenfunctions\n7: ˆρim ←ˆΞ\n⊤\ni ˆvm for i = 1, . . . , N and m = 1, . . . , M\n▷Compute mFPC scores\nOutput: Tuple of estimated mFPC scores, eigenfunctions and mean curves: {ˆρim, ˆψm, ˆµ}N,M\ni,m=1.\nDynamic Trajectory Representations using mFPCA\nAlgorithm 1 details the estimation\nsteps of the mFPCA setup outlined in Section 2. Starting from {Y i(Tij)}Ni\nj=1, we estimate the\nmFPC scores ˆρim by building on the univariate functional principal component analysis (uFPCA)\nof each {Y (d)\ni\n(Tij)}Ni,N\nj,i=1 for d = 1, . . . , p. The algorithm follows the approach in Happ and Greven\n8\n"}, {"page": 9, "text": "[2018], using estimated quantities from uFPCA including the mean functions ˆµ(d)(t), eigenfunctions\nˆϕ(d)(t) and univariate FPC scores ˆξ(d)\nik ; for details see Algorithm 4 in Section C.2 and Yao et al.\n[2005].\nAlgorithm 2 Detecting anomalous subjects within a cluster ˆC\nInput: Subject cluster ˆC; data {Y i(Tij) : i ∈ˆC}; significance levels α1, α (where α1 > α).\n1: Obtain mFPC scores {ˆρ ˆC\nim}i∈ˆC,m=1,...,B corresponding to the top B cluster-specific mFPC com-\nponents using Algorithm 6 applied to {Y i(Tij) : i ∈ˆC}.\n▷B: number of top mFPC components based on prop. of variance explained\n2: Randomly partition ˆC into disjoint sets I1, I2 of equal size.\n3: (G1, Gc\n1) ←ScreenPotentialOutliers(I1, {ˆρ ˆC\nim : j ∈I1}, B, α1).\n▷Algorithm 7\n4: (G2, Gc\n2) ←ScreenPotentialOutliers(I2, {ˆρ ˆC\nim : j ∈I2}, B, α1).\n5: Initialize A(1) ←∅.\n▷Set of confirmed outliers for cluster ˆC\n6: A(1)\nG1 ←ConfirmAnomalies(G1, Gc\n2, {ˆρ ˆC\nim : j ∈G1 ∪Gc\n2}, B, α).\n▷Algorithm 8\n7: A(1)\nG2 ←ConfirmAnomalies(G2, Gc\n1, {ˆρ ˆC\nim : j ∈G2 ∪Gc\n1}, B, α).\n8: A(1) ←A(1)\nG1 ∪A(1)\nG2.\nOutput: Set of confirmed anomalous subjects A(1) = {(i, Si) : i ∈ˆC is an outlier, Si ̸= ∅}.\nClustering and Anomaly Detection using mFPC Scores and Covariates\nWe segment\nsubjects by clustering their estimated mFPC scores jointly with static covariates (Algorithm 5,\nAppendix D). For each estimated cluster ˆCk we re-fit mFPCA using only its members (Algorithm 1;\nAlgorithm 6), yielding cluster-specific means ˆµk(t), eigenfunctions ˆψ\nk\nm(t), updated scores ˆρk\nim and\nand reconstructed trajectories (Equation (D.4)).\nAlgorithm 3 Dynamic temporal profiling of anomalous subjects\nInput: Type 1 anomalies A(1) (from Alg. 2 for cluster ˆC); data {Y j(Tjk) : j ∈ˆC}; cluster means\n{ˆµ(d)\nˆC (t)} (from Alg. 6); Clean held-out sets Gc\n1, Gc\n2 & split info I1, I2 for ˆC (from Alg. 2); time\nwindows {(aw, bw]}W\nw=1; significance level α.\n1: ({¯µ(w)\nˆC }W\nw=1, {D(w)\nj\n}j∈Gc\n1∪Gc\n2,w=1,...,W) ←ComputeWindowDeviations({Y j(Tjk) : j ∈Gc\n1 ∪\nGc\n2}, {ˆµ(d)\nˆC (t)}, {(aw, bw]}W\nw=1).\n▷Compute scores for clean held-out set Alg. 9\n2: A(2) ←IdentifyAnomalousWindows(A(1), {Y i(Tij) : i s.t. (i, _) ∈A(1)},\n{¯µ(w)\nˆC }, {D(w)\nj\n}, I1, I2, Gc\n1, Gc\n2,\n{(aw, bw]}W\nw=1, α).\n▷Identify anomalous windows for subjects (Alg. 10)\nOutput: Set of subject-indexed anomalous temporal windows A(2) = {(i, Wi) : i ∈A(1), Wi ̸= ∅}.\nGlobally anomalous subjects will still be assigned to one of the K clusters unless explicitly\n9\n"}, {"page": 10, "text": "screened—a difficult task in heterogeneous data. To detect such cases post-assignment, we apply\nAlgorithm 2 (with Algorithms 7 and 8; Appendix E). The procedure tests whether a subject’s\nmultivariate FPC scores deviate from the typical pattern of its assigned cluster ˆC, using sample\nsplitting and data-driven calibration to control multiplicity across principal components. It outputs\nflagged subjects A(1) = (i, Si), where Si records the outlying FPC directions—information that\nthen guides localized anomaly analysis (Algorithm 3).\nFigure 2: Plutchik’s wheel of emotions.\nSubjects flagged by Algorithm 2 (set A(1)) may be\nanomalous only over portions of their trajectories. Al-\ngorithm 3 localizes these periods by comparing each\nsubject’s raw segments to the cluster mean, with data-\ndriven calibration (Algorithm 9); implementation details\nare in Appendix E (Algorithms 9, 10).\nThe output is\nA(2) = (i, Wi), where Wi denotes the time windows in\nwhich subject i’s trajectory departs from a clean cohort\nwithin that window. This step pinpoints atypical inter-\nvals and enables per-window anomaly flags, which feed\ninto the final dynamic keyword profiling stage.\nDynamic Keyword Profiling\nFinally, we describe intent extraction from anomalous reviews.\nFor each subject i, let Si, be the anomalous reviews. Challenges include lexical variation for similar\nsemantics, shared stylistic drift across users, and scalability for large number of anomalous reviews.\nWe maintain a time-ordered intent list I(t−)\ni\nfrom reviews before time t. At time t, an LLM receives\nI(t−)\ni\n, top global intents observed before t, and the current review, and either matches an existing\nintent or proposes a new one. Full details appear in Algorithm 11 (Appendix F).\n10\n"}, {"page": 11, "text": "4\nReal Data Applications\n4.1\nModeling Dynamic Emotions in Amazon Customer Reviews\nWe use the Amazon Reviews corpus Hou et al. [2024], which includes 1,946 users and 22,032\nreviews over five years, focusing on Automobile for the main analysis; Beauty & Personal Care\nand Sports & Outdoors supply user-level covariates (e.g., cross-category purchase share). Each\nreview includes a user ID, timestamp, product title, text, and a 1–5 rating, with users posting over\nmultiple years.\nEmotion embedding for text transcripts\nPlutchik’s wheel of emotions provides a struc-\ntured framework for mapping emotional states along opposing pairs, capturing both intensity and\npolarity (see Fig. 2 Semeraro et al. [2021]).\nWe convert each transcript into four real-valued\nscores—joy–sadness, trust–disgust, fear–anger, and surprise–anticipation—on a continuous [−1, 1]\nscale, where −1 and 1 denote the extremes of each pole (e.g., grief vs. ecstasy), and intermediate\nvalues encode moderate intensity. A zero-shot GPT-3.5-Turbo prompt returns one scalar per axis\n(details and validation in Section C.1). Stacking these over time yields a 4-D timestamped embed-\nding per subject, which serves as the input to LLmFPCA-detect for mFPCA and the subsequent\nsteps.\nRating\nReview\nJoy–Sadness\nTrust–Disgust\nFear–Anger\nSurprise–Anticipation\n5\nI use this great oil in all of my 150cc Scooters (was told to by a Scooter\nmechanic) and I’ve never had an engine problem. But this price is thru\nthe roof, $17.50 for a single quart is STUPID...wally world sells it for\n$4.99...but its kinda funny that all of Amazon’s oils are priced thru the\nroof\n-0.8\n-0.6\n-0.8\n-1\n1\nReceived this today and went to put it on my 3/8 extension for an oil\nfilter change. The machining is pretty, but measurements are so poor I\ncannot get it on the extension to use. Absolute junk! I should have paid\nmore attention to the negative review.\n-0.77\n-0.75\n-0.5\n-0.7\nTable 1: Amazon customer reviews with emotion scores across four Plutchik dimensions.\nTable 1 illustrates how emotion embeddings reveal customer pain points that are not captured\nby 5-star ratings alone. In the first example, a 5-star review shows strong sadness (–0.8), disgust\n(–0.6), anger (–0.8), and surprise (–1), indicating frustration with pricing despite overall satis-\nfaction. The third example, also rated 1 star, shows high sadness (–0.77), disgust (–0.75), and\nsurprise (–0.7), pointing to severe frustration over usability issues.\n11\n"}, {"page": 12, "text": "Cluster 1\nCluster 2\nCluster 3\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.0\n0.2\n0.4\nNormalized Time (t)\nMean Emotion Score (µt)\nJoy −Sadness\nTrust −Disgust\nFear −Anger\nSurprise −Anticipation\nFigure 3: Mean emotion trajectories across the three user clusters. Curves represent mean scores\nfor the Joy–Sadness, Trust–Disgust, Fear–Anger, and Surprise–Anticipation emotion dimensions.\nEmotion mFPCA scores (Algorithm 1) improve predictive power over product ratings\nWe test whether review text improves forecasting of adverse outcomes (e.g., sudden rating drops)\nin Amazon Reviews. A “rating drop” is defined as the extreme percentile of each user’s maximum\ngap between consecutive ratings. We compare two optimally tuned random-forest models on a\nclass-balanced sample with identical baseline covariates—cluster labels from Algorithms 1–5 and\npurchase mix across categories.\nModel A summarizes past behavior by the mean Automobile\nrating; Model B replaces that single aggregate with emotion mFPC scores, capturing time-varying\ntextual signals. On the test set, Model A: accuracy 0.542, precision 0.538, recall 0.596, F1 0.565,\nROC–AUC 0.534.\nModel B improves all metrics—accuracy 0.609 (+12.4%), precision 0.610\n(+13.4%), recall 0.603 (+1.2%), F1 0.606 (+7.3%), ROC–AUC 0.645 (+20.8%)–showing that\ncompact emotion-trajectory features capture predictive signal beyond coarse star-rating averages.\nClustering dynamics and case studies\nFigure 3 plots mean emotion trajectories for the\nthree clusters from purchasing proportions in Automobiles, Beauty & Personal Care, and Sports\n& Outdoors).\nCluster 1 has the highest baseline across emotions–consistently stronger affect.\nCluster 3 follows a similar temporal shape but is uniformly lower (milder affect). Cluster 2 departs\nmost, with elevated sadness and anger, indicating sharper pain points. Because anomalies are\nscored relative to each cluster’s mean, even upward shifts in positive emotion within Cluster 2 can\nregister as anomalous. Section D.3 of the Appendix reports bootstrap analysis confirming cluster\n12\n"}, {"page": 13, "text": "fear−anger\njoy−sadness\nsurprise−anticipation\ntrust−disgust\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.3\n0.4\n0.5\n0.13\n0.15\n0.17\n0.1\n0.2\n0.025\n0.050\n0.075\nTime\nEmotion Score\nClean Cohort\nAnomaly\nMean\nFigure 4: Mode of variation plot for a user along the fourth FPC (outlying) from cluster 3\nstability.\nThrough mode-of-variation plots (see Section C.2for details) and corresponding review ex-\ncerpts in the flagged time window, we show that the detected anomalies capture customer pain\npoints. Figure 4 shows a user’s emotional trajectory relative to Cluster 3. The user’s emotions\nare consistently shifted from the cluster mean along the fourth eigenfunction in Cluster 3, with a\npronounced spike in the fear–anger petal and a sharp drop in joy–sadness during the final time\nwindow—signaling a clear pain point. Review texts from this period reveal issues with mismatched\nparts, specifically a replacement door-handle cover with incorrect keyhole cut-outs. The dominant\ncomplaints relate to product fit and quality control. These insights suggest actionable interven-\ntions, such as enforcing compatibility checks at purchase and improving final-stage quality control\nby the seller.\nFigure 5 shows a user exhibiting a dip–recovery emotional pattern along the second eigen-\nfunction. Early in the timeline, all four emotion petals remain well below the cluster baseline.\nDuring the anomalous time window, there is a sharp rise in fear and surprise, driven by issues\nrelated to poor product quality. The user expresses frustration and regret, suggesting loss of brand\ntrust. Key pain points include the failure of a critical component and confusion caused by missing\ndocumentation.\n13\n"}, {"page": 14, "text": "fear−anger\njoy−sadness\nsurprise−anticipation\ntrust−disgust\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.25\n0.30\n0.35\n0.40\n0.45\n0.10\n0.15\n0.20\n0.25\n0.30\n0.250\n0.275\n0.300\n0.325\n0.350\n0.01\n0.02\n0.03\n0.04\nTime\nEmotion Score\nClean Cohort\nAnomaly\nMean\nFigure 5: Mode of variation plot for a user along the second FPC (outlying) from cluster 1\nKeyword profiling\nAfter detecting anomalies, we perform keyword profiling (Algorithm 11 in\nSection F of the Appendix) to each flagged instance. Table 2 summarizes the keywords associated\nwith anomalous points in each cluster. A quick glance shows that users in Cluster 2 tend to express\nbroadly negative emotions, while Cluster 3 highlights more specific issues—such as missing cables\nand poor documentation—reflecting the more descriptive and varied nature of reviews in that\ngroup. Table 6 illustrates dynamic profiling of keywords; see Section F in the Appendix for details.\nCluster\nKeywords\nCluster 1\nas described, good quality, perfect product, poor value for money, wrong size, poor\nfit\nCluster 2\npoor quality, poor value for money\nCluster 3\nas described, bulky design, good quality, good value for money, good design, leaks\nfuel, missing cable, quantity issue, poor documentation, wrong size\nTable 2: Group-level pain points detected across clusters\n4.2\nTracking Toxicity and Aggression in Wikipedia request–comment\nstream\nWe evaluate LLmFPCA-detect on the English Wikipedia request–comment stream to demonstrate\ncross-domain applicability. For each comment, we record the text, timestamp, structured user\n14\n"}, {"page": 15, "text": "covariates, and crowdsourced ground-truth toxicity/aggression scores.\nThis corpus exemplifies\nsparse longitudinal text: users post at irregular, infrequent intervals. The dataset was collected\nvia the Wikipedia API, restricted to the user–talk and article–talk namespaces, and sourced from\nWiki data. We retain comments from 2010–2015 authored by 925 pseudonymized users.\nMethod\nTW1\nTW2\nTW3\nTW4\nTW5\nLLmFPCA-detect (gpt-4o-mini)\n0.58\n0.58\n0.46\n0.37\n0.32\nIsolation Forest (BERT)\n0.41\n0.33\n0.25\n0.23\n0.39\nIsolation Forest (gpt-4o-mini)\n0.41\n0.33\n0.25\n0.23\n0.39\nTable 3: F1 scores for anomalies detected by LLmFPCA-detect versus ground truth, compared\nwith Isolation Forest on GPT-derived scores and a BERT baseline (segregated by time windows).\nComparison with state-of-the-art\nWe assess anomaly detection on Wikipedia by treating\nhuman-annotated toxicity/aggression as surrogate ground truth and extracting GPT-derived tox-\nicity/aggression scores from text via prompts.\nAs a content-agnostic baseline, we use BERT\nembeddings (no explicit toxicity cues). We partition the timeline into five windows and, within\neach, define pseudo–ground-truth anomalies using Isolation Forest on the human scores plus user\ncovariates (comment count, median inter-comment gap). We then run Isolation Forest on (i) GPT-\nderived scores and (ii) BERT embeddings (each with the same covariates) as baselines. Finally, we\napply LLmFPCA-detect to the GPT-derived trajectories with the same covariates to flag anomalies\nacross the five windows and compare against these baselines (Table 3).\nCluster dynamics\nLLmFPCA-detect flags not only one-off vandalism or brief flare-ups by other-\nwise well-behaved contributors, but also sustained problematic behavior and its mode of deviation.\nFor example, Cluster 1 outliers tend to post unusually high volumes or engage in extended policy\ndisputes, whereas Cluster 2 outliers show short, intense bursts of toxic language. Table 4 presents\nrepresentative cases with brief excerpts and the corresponding anomalous time window. In Clus-\nter 1, the dominant pattern is procedural friction—disagreements about process (e.g., whether a\nproposed mentorship program requires further consensus) rather than direct attacks. By contrast,\nCluster 2 features overt hostility, where procedural disagreements escalate into personal or con-\n15\n"}, {"page": 16, "text": "Cluster User ID\nComment excerpt (abridged)\nLabel\n1\n10783082\n“. . . If that’s how you want it. I will talk to this to ANI if necessary\n. . . ”\n1\n1\n10756369\n“=== Adopt Me === Here is a proposal for a new mentorship\nprocess . . . ”\n1\n2\n2305952\n“OK, maybe I was wrong. I’m sorry, but don’t try me again . . . ”\n5\n2\n2305952\n“No, that’s irrelevant. Your source is garbage, stop spamming it.”\n5\nTable 4: Examples from the Wikipedia comment stream where detected anomalies match crowd-\nsourced annotations, showing cluster ID, anonymized user ID, excerpt, and toxicity/aggression\nlabel.\nCluster 1\n(Window)\nTop keywords (LLmFPCA-detect)\nTheme\nW1\nconsensus, policy, “WP: ANI”\nPolicy enforcement friction\nW2\ncivility, manners, please, courtesy\nSoft-skills reminders\nW3\nbacklog, deadline, stall, formalise\nProcedural urgency\nCluster 2\n(Window)\nTop keywords (LLmFPCA-detect)\nTheme\nW4\nnonsense, garbage-source,\nstop-spamming\nDirect hostility\nW5\nrevert, vandal, warning, block, “3RR”\nConflict over content\nW5\nwasting-time, already-explained\nModerator fatigue\nTable 5: Dynamic–keyword profiling makes each anomaly legible. In this Wikipedia setting, instead\nof an opaque outlier score, the moderator sees the top keywords that drove the statistical flag.\nfrontational language. Additionally, Appendix D.3 reports bootstrap analyses confirming stability\nof the obtained clusters.\nKeyword profiling\nDynamic keyword profiling makes each anomaly interpretable (Table 5).\nRather than an opaque outlier score, moderators see the top terms that triggered the flag, revealing\nthe concerns underlying anomalous behavior. In this corpus, Cluster 1 anomalies are predominantly\nprocedural—e.g., disputes over which venue (WP:ANI, etc.) should adjudicate. Cluster 2, by\ncontrast, exhibits explicit antagonism: personal attacks, contempt for sources (“garbage-source\"),\nand edit-war jargon. The exasperation lexicon (“wasting time,\" “already explained\") further signals\nmoderator fatigue—an operational risk that steady-state toxicity metrics would miss.\nIdentifying peak–hostility windows (e.g., Window 5) with LLmFPCA-detect enables proactive\n16\n"}, {"page": 17, "text": "moderation, such as temporarily throttling edits. In Cluster 1, the dominant issue is procedural\nfriction, suggesting policy fixes like clearer closure rules or targeted sanctions. Keyword profiling\npinpoints specific, time-bounded situations where light-touch actions can prevent rule violations\nand burnout. Linking time windows to salient terms reveals root causes and supports proportion-\nate, domain-specific responses instead of one-size-fits-all bans.\n5\nConclusion\nLLmFPCA-detect provides an end-to-end framework for sparse longitudinal (SL) text by inte-\ngrating LLM-embeddings with functional data analysis. LLmFPCA-detect tackles key challenges\nin such datasets—including sparsity, irregularity, noise, and semantic complexity—by embedding\ntext into meaningful numeric representations, followed by mFPCA which is used for user seg-\nmentation, anomaly detection, and dynamic intent profiling across large SL text datasets, a set-\nting that remains largely unaddressed in the literature. Applied to Amazon customer reviews,\nLLmFPCA-detect successfully uncovers emotion dynamics and identifies critical pain points in the\ncustomer journey, offering valuable insights for consumer analytics. We demonstrate the utility\nof LLmFPCA-detect on English Wikipedia request–comment stream to detect toxic comments,\nwhere the detected anomalies align well with crowdsourced human annotations. The flexibility of\nLLmFPCA-detect makes it applicable to other domains such as healthcare, education, and social\nmedia where SL text data is routine. Future work includes establishing theoretical guarantees based\non mFPCA estimates rather than fully observed trajectories, and extending LLmFPCA-detect to\nother supervised and unsupervised tasks on SL text datasets.\nReferences\nHussam Alhuzali and Sophia Ananiadou. Spanemo: Casting multi-label emotion classification as\nspan-prediction. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1571–1585,\n2021.\n17\n"}, {"page": 18, "text": "Robert Bamler and Stephan Mandt. Dynamic word embeddings. In International conference on\nMachine learning, pages 380–389. PMLR, 2017.\nAne Blázquez-García, Angel Conde, Usue Mori, and Jose A Lozano. A review on outlier/anomaly\ndetection in time series data. ACM computing surveys (CSUR), 54(3):1–33, 2021.\nD. M. Blei and J. D Lafferty. Dynamic topic models. In Proceedings of the 23rd International\nConference on Machine Learning, pages 113–120, 2006.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models\nare few-shot learners. In Advances in Neural Information Processing Systems, volume 33, 2020.\nURL https://arxiv.org/abs/2005.14165.\nFabio Calefato, Filippo Lanubile, and Nicole Novielli. Emotxt: A toolkit for emotion recognition\nfrom text. In Proceedings of the Seventh International Conference on Affective Computing and\nIntelligent Interaction (ACII), pages 79–85, 2017.\nYang Cao, Sikun Yang, Chen Li, Haolong Xiang, Lianyong Qi, Bo Liu, Rongsheng Li, and Ming\nLiu. Tad-bench: A comprehensive benchmark for embedding-based text anomaly detection.\narXiv preprint arXiv:2501.11960, 2025.\nJulio E Castrillón-Candás and Mark Kon. Anomaly detection: A functional analysis perspective.\nJournal of Multivariate Analysis, 189:104885, 2022.\nMariana Cavique, Antónia Correia, Ricardo Ribeiro, and Fernando Batista. What are airbnb hosts\nadvertising? a longitudinal essay in lisbon. Consumer Behavior in Tourism and Hospitality, 17\n(3):312–325, 2022.\nWenlin Dai and Marc G Genton. Functional boxplots for multivariate curves. Stat, 7(1):e190,\n2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n18\n"}, {"page": 19, "text": "bidirectional transformers for language understanding, 2019. URL https://arxiv.org/abs/\n1810.04805.\nInderjit S Dhillon and Dharmendra S Modha. Concept decompositions for large sparse text data\nusing clustering. Machine learning, 42(1):143–175, 2001.\nPeter Sheridan Dodds, Kameron Decker Harris, Isabel M Kloumann, Catherine A Bliss, and\nChristopher M Danforth. Temporal patterns of happiness and information in a global social\nnetwork: Hedonometrics and twitter. PloS one, 6(12):e26752, 2011.\nElizabeth Ford, John A Carroll, Helen E Smith, Donia Scott, and Jackie A Cassell. Extracting\ninformation from the text of electronic medical records to improve case detection: a systematic\nreview. Journal of the American Medical Informatics Association, 23(5):1007–1015, 2016.\nDaniel Gervini. Detecting and handling outlying trajectories in irregularly sampled functional\ndatasets. The Annals of Applied Statistics, pages 1758–1775, 2009.\nThomas L Griffiths and Mark Steyvers. Finding scientific topics. PNAS, 101(suppl. 1):5228–5235,\n2004.\nSiteng Hao et al. Dynamic modeling for multivariate functional and longitudinal data. Journal of\nEconometrics, 239(2):105573, 2024. ISSN 0304-4076. doi: https://doi.org/10.1016/j.jeconom.\n2023.105573.\nClara Happ and Sonja Greven.\nMultivariate functional principal component analysis for data\nobserved on different (dimensional) domains. Journal of the American Statistical Association,\n113(522):649–659, 2018.\ndoi: 10.1080/01621459.2016.1273115.\nURL https://doi.org/10.\n1080/01621459.2016.1273115.\nYupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley.\nBridging\nlanguage and items for retrieval and recommendation. arXiv preprint arXiv:2403.03952, 2024.\nMia Hubert, Peter J Rousseeuw, and Pieter Segaert. Multivariate functional outlier detection.\nStatistical Methods & Applications, 24(2):177–202, 2015.\n19\n"}, {"page": 20, "text": "Clayton J Hutto, Sarita Yardi, and Eric Gilbert. A longitudinal study of follow predictors on\ntwitter. In Proceedings of the sigchi conference on human factors in computing systems, pages\n821–830, 2013.\nRamakrishnan Kannan, Hyenkyun Woo, Charu C Aggarwal, and Haesun Park. Outlier detection\nfor text data. In Proceedings of the 2017 siam international conference on data mining, pages\n489–497. SIAM, 2017.\nSean W Kelley and Claire M Gillan. Using language in social media posts to study the network\ndynamics of depression longitudinally. Nature communications, 13(1):870, 2022.\nMinhee Kim et al. Covariate dependent sparse functional data analysis. INFORMS Journal on\nData Science (IJDS) (Online), 2, 2023.\nNan M Laird and James H Ware. Random-effects models for longitudinal data. Biometrics, pages\n963–974, 1982.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019. URL https://arxiv.org/abs/1907.11692.\nYu-Bao Liu, Jia-Rong Cai, Jian Yin, and Ada Wai-Chee Fu. Clustering text data streams. Journal\nof computer science and technology, 23(1):112–128, 2008.\nYu Lu and Harrison H Zhou. Statistical and computational guarantees of lloyd’s algorithm and\nits variants. arXiv preprint arXiv:1612.02099, 2016.\nSaif M. Mohammad.\nPractical and ethical considerations in the effective use of emotion and\nsentiment lexicons. In Proceedings of the 12th Language Resources and Evaluation Conference,\npages 4543–4549, 2020.\nSaif M. Mohammad and Felipe Bravo-Marquez. Emotion intensities in tweets. Proceedings of\nNAACL-HLT, pages 1–6, 2018.\n20\n"}, {"page": 21, "text": "Hans-Georg Müller. Functional modelling and classification of longitudinal data. Scandinavian\nJournal of Statistics, 32(2):223–240, 2005.\nBrendan O’Connor, Ramnath Balasubramanyan, Bryan Routledge, and Noah Smith. From tweets\nto polls: Linking text sentiment to public opinion time series. In Proceedings of the international\nAAAI conference on web and social media, volume 4, pages 122–129, 2010.\nRobert Plutchik. Emotion: Theory, research, and experience. Theories of emotion, 1, 1980.\nLukas Ruff, Yury Zemlyanskiy, Robert Vandermeulen, Thomas Schnake, and Marius Kloft. Self-\nattentive, multi-context one-class classification for unsupervised anomaly detection on text. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n4061–4071, 2019.\nAmandine Schmutz, Julien Jacques, Charles Bouveyron, Laurence Cheze, and Pauline Martin.\nClustering multivariate functional data in group-specific functional subspaces. Computational\nStatistics, 35(3):1101–1131, 2020.\nSix Seconds. Plutchik’s wheel of emotions: Feelings wheel, 2025. URL https://www.6seconds.\norg/2025/02/06/plutchik-wheel-emotions/. Accessed 2025-05-21.\nAlfonso Semeraro, Salvatore Vilella, and Giancarlo Ruffo. Pyplutchik: Visualising and comparing\nemotion-annotated corpora. PLOS ONE, 16(9):e0256503, September 2021. ISSN 1932-6203. doi:\n10.1371/journal.pone.0256503. URL http://dx.doi.org/10.1371/journal.pone.0256503.\nAlvin Subakti, Hendri Murfi, and Nora Hariadi. The performance of bert as data representation\nof text clustering. Journal of big Data, 9(1):15, 2022.\nYing Sun and Marc G Genton.\nFunctional boxplots.\nJournal of computational and graphical\nstatistics, 20(2):316–334, 2011.\nDanny Valdez, Marijn Ten Thij, Krishna Bathina, Lauren A Rutter, and Johan Bollen. Social\nmedia insights into us mental health during the covid-19 pandemic: longitudinal analysis of\ntwitter data. Journal of medical Internet research, 22(12):e21418, 2020.\n21\n"}, {"page": 22, "text": "Geert Verbeke, Steffen Fieuws, Geert Molenberghs, and Marie Davidian. The analysis of multi-\nvariate longitudinal data: a review. Statistical methods in medical research, 23(1):42–59, 2014.\nXuerui Wang and Andrew McCallum. Topics over time: a non-markov continuous-time model of\ntopical trends. KDD ’06, page 424–433, New York, NY, USA, 2006. Association for Computing\nMachinery. ISBN 1595933395. URL https://doi.org/10.1145/1150402.1150450.\nShushan Wu, Luyang Fang, Jinan Zhang, TN Sriram, Stephen J Coshatt, Feraidoon Zahiri, Alan\nMantooth, Jin Ye, Wenxuan Zhong, Ping Ma, et al. Unsupervised anomaly detection and diag-\nnosis in power electronic networks: Informative leverage and multivariate functional clustering\napproaches. IEEE Transactions on Smart Grid, 15(2):2214–2225, 2023.\nJiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series\nanomaly detection with association discrepancy, 2022. URL https://arxiv.org/abs/2110.\n02642.\nKenneth CC Yang and Yowei Kang. What can college teachers learn from students’ experien-\ntial narratives in hybrid courses?: A text mining method of longitudinal data. In Theoretical\nand practical approaches to innovation in higher education, pages 91–112. IGI Global Scientific\nPublishing, 2020.\nFang Yao, Hans-Georg Müller, and Jane-Ling Wang. Functional data analysis for sparse longi-\ntudinal data. Journal of the American Statistical Association, 100(470):577–590, 2005. doi:\n10.1198/016214504000001745. URL https://doi.org/10.1198/016214504000001745.\nJianhua Yin and Jianyong Wang. A model-based approach for text clustering with outlier detection.\nIn 2016 IEEE 32nd International Conference on Data Engineering (ICDE), pages 625–636.\nIEEE, 2016.\nMikhail Yurochkin, Zhiwei Fan, Aritra Guha, Paraschos Koutris, and XuanLong Nguyen. Scalable\ninference of topic evolution via models for latent geometric structures. In Advances in Neural\nInformation Processing Systems, 32, 2019.\n22\n"}, {"page": 23, "text": "Zahra Zamanzadeh Darban, Geoffrey I Webb, Shirui Pan, Charu Aggarwal, and Mahsa Salehi.\nDeep learning for time series anomaly detection: A survey. ACM Computing Surveys, 57(1):\n1–42, 2024.\nYidong Zhou and Hans-Georg Mueller. Dynamic modelling of sparse longitudinal data and func-\ntional snippets with stochastic differential equations. Journal of the Royal Statistical Society\nSeries B: Statistical Methodology, page qkae116, 2024.\n23\n"}, {"page": 24, "text": "Appendix\nThe appendix contains supplementary text, figures, and additional results that support the main\npaper.\nA\nTemporal Dynamics of Customer Intents Across Clusters\nand the Structure of Technical Proofs\nThis section highlights the evolving patterns of customer intents across the four time windows for\neach cluster (Table 6) and directs readers to the Technical Appendices D.1 and E.1 for the formal\nproofs underpinning our clustering and anomaly-screening methods.\nTable 6 presents the distribution of dominant customer intents—such as product quality, fit,\nand value perceptions—across the four sequential time windows for each of the three clusters. For\neach cluster, we list the most frequent intent labels along with their occurrence counts, highlighting\nhow user concerns and satisfaction indicators evolve over the course of their interaction trajectory.\nThis breakdown reveals distinct temporal patterns in user feedback: Cluster 1 users progressively\nemphasize product perfection in later windows, Cluster 2 maintains a consistently low volume of\nquality complaints, and Cluster 3 displays a broad diversity of intents, including both positive and\nnegative evaluations, particularly in the final window.\n24\n"}, {"page": 25, "text": "Table 6: Summary of Intents and Counts by Cluster and Time Window\nCluster\nTime Window 1\nTime Window 2\nTime Window 3\nTime Window 4\nCluster 1\npoor value for\nmoney (4), wrong\nfit (1)\ngood product\nquality (2), perfect\nproduct (50), poor\nproduct quality\n(1), wrong fit (1)\nas described (6),\ngood product\nquality (10),\nperfect product\n(21), poor product\nquality (2), poor\nvalue for money\n(7), wrong fit (1)\ngood product\nquality (1), perfect\nproduct (31), poor\nvalue for money\n(2), wrong fit (1)\nCluster 2\npoor product\nquality (1)\npoor product\nquality (3), poor\nvalue for money (1)\npoor product\nquality (1)\npoor product\nquality (2)\nCluster 3\npoor product\ndesign (3), poor\nproduct quality (3)\ngreat value for\nmoney (1), poor\nproduct quality (2)\npoor product\ndesign (1), poor\nproduct quality (1)\nas described (3),\ngood product\nquality (16), great\nbargain (3), great\ndesign (1), great\nvalue for money\n(10), missing\ncharging cable (1),\nperfect product (1),\npoor product\ndesign (14), poor\nproduct quality\n(18), poor value for\nmoney (8), size\nvariation issue (1),\ntoo big and bulky\n(2), wrong fit (4)\nTechnical Appendices\nAppendix D.1 quantifies how well Lloyd’s k-means recovers the true\nclusters when the data contain a small fraction of anomalous points. Theorem D.1 shows that, if\nthe cluster means are sufficiently separated (∆) and the normalized signal-to-noise ratio rk exceeds\na computable threshold, the mis-clustering rate of the clean data drops to exp(−∆2/16σ2) after\nat most 4 log N iterations, starting from a modestly accurate initialization.\nAppendix E.1 supplies the statistical basis for the screening and calibration steps used after\nclustering. It models clean mFPC scores as draws from a baseline law Lk\nm and treats anomalies\nas score shifts by an independent random effect.\nTheorem E.1 proves that, when the within-\n25\n"}, {"page": 26, "text": "cluster anomaly fraction πa,k = o(1), a simple tail test on the empirical score distribution always\nincludes the anomalous subjects with high probability. These two results jointly justify the clus-\ntering–then–screening strategy used throughout the paper.\nB\nExperimental Setup, Simulations and Supplementary Re-\nsults\nB.1\nComputational Environments and Tasks\nAll emotion scoring and related language-model tasks were performed on setup 1: the Standard\nNC4as T4 v3 VM with a Tesla T4 GPU (16 GB), 4 vCPUs, 28 GB RAM, Linux (x64, Gen 1). All\nsubsequent work—fitting algorithms, analyzing Amazon reviews, other experiments, simulations\netc. —was carried out on setup 2: MacBook M3 Pro with 36 GB memory, and setup 3: 13 inch\n2020 MacBook Pro (2.3 GHz Quad-Core i7, 32 GB LPDDR4X, macOS Sequoia 15.0.1). The full\nresearch project did not require more compute than the experiments reported in the paper.\nB.2\nSimulations\nFor each cluster ℓ∈{1, 2, 3}, we assess the stability of our anomaly-detection pipeline under real-\nistic sparsity by performing a simulation study with S = 50 replicates on the Amazon Automobile\nreview data (Section 4.1). Let ˆCℓbe the set of users in cluster ℓ(from Algorithm 5), and let\nA(ℓ) = { i ∈ˆCℓ: A(ℓ)\ni\n= 1} denote the “base” anomalies originally flagged by Algorithm 2. We first\nobtain cluster-specific fitted emotion trajectories\nˆX(j)\ni, ˆCℓ(t),\ni ∈ˆCℓ, j = 1, . . . , 4,\nvia the mFPCA procedure of Section 3 (Algorithms 1, 6). In replicate s, for each user i we draw\na truncated Poisson subsample size\nK(s)\ni\n∼Pois(10) truncated to {5, . . . , 15},\n26\n"}, {"page": 27, "text": "then uniformly select K(s)\ni\ntimepoints from the common grid {t1, . . . , tT} to form\nLt(s)\ni\n= {t(s)\ni1 , . . . , t(s)\niK(s)\ni },\nLy(j),(s)\ni\n=\n\b ˆX(j)\ni, ˆCℓ(t) : t ∈Lt(s)\ni\n\t\n.\nWe apply univariate Sparse FPCA independently to each (Lt(s)\ni , Ly(j),(s)\ni\n) (Algorithm 4), stack\nthe resulting scores into the matrix H(ℓ,s), and then perform multivariate FPCA (Algorithm 1)\nto obtain mFPC scores ˆρ(ℓ,s)\ni,m . We then re-run the exact screening-calibration anomaly detection\n(Algorithm 2), yielding binary flags\nA(ℓ,s)\ni\n∈{0, 1},\ni ∈ˆCℓ.\nwhere\nA(ℓ,s)\ni\n=\n\n\n\n\n\n\n\n1,\nuser i flagged as anomalous in replicate s,\n0,\nnot flagged in replicate s.\nBy comparing {A(ℓ,s)\ni\n}S\ns=1 against the base set A(ℓ), we compute per-user detection frequencies—i.e.\nthe proportion of replicates in which originally flagged anomalies remain detected—thus quantify-\ning the robustness of our pipeline to the sparse, irregular sampling patterns inherent in customer\nreview trajectories.\n0.4\n0.6\n0.8\nCluster 1\nCluster 2\nCluster 3\nRecall\nFigure 6: Distribution of recall across the clusters beginning from cluster 1 on the\n27\n"}, {"page": 28, "text": "Figure 6 plots, for each cluster ℓ, the empirical distribution of\nRecall(ℓ)\ns\n=\n\f\f{ i ∈A(ℓ) : A(ℓ,s)\ni\n= 1}\n\f\f\n|A(ℓ)|\n,\nover S = 50 subsampling replicates s. Here\nA(ℓ) = { i ∈ˆCℓ: A(ℓ)\ni\n= 1}\nis the set of “base” anomalies—i.e., the users flagged by Algorithm 2 on the full (unsparsified)\ntrajectories—so |A(ℓ)| is the total number of those original anomalies in cluster ℓ.\nIn Cluster 1 and Cluster 3 (the two larger clusters), the median recall exceeds 0.80 and the\ninterquartile ranges are tight (appx. 0.78–0.92 and 0.82–0.94, respectively), indicating that over\n80% of the base anomalies survive even when only 5–15 timepoints per user are observed. By\ncontrast, Cluster 2 (the smallest cluster) has a markedly lower median recall (appx. 0.45) and a\nmuch wider spread (0.30–0.55), revealing that nearly half of its base anomalies are missed in many\nsubsampled runs.\nThese differences reflect intrinsic heterogeneity in the temporal signatures of anomalous users.\nClusters 1 and 3 appear to harbor anomalies whose deviations from the cluster mean are pro-\nnounced and sustained over time, so they survive aggressive subsampling. In Cluster 2, however,\nthe outlying behavior is more localized or subtler, making detection highly sensitive to which\nwindows are sampled. This suggests that, in practice, anomaly detection for Cluster 2 may ben-\nefit from (a) collecting additional observations around key time periods or (b) combining Type-1\nscore-based flags with complementary time-window tests (Algorithm 3) to recover those more\nfragile anomalies.\nFigure 7 displays the per-user detection probability—i.e. the fraction of the 50 subsampling\nreplicates in which each base anomaly is recovered—separately for the three clusters. In Cluster 1\n(left panel), the density is strongly concentrated near 1.0, with a steep rise above 0.8, indicating\nthat nearly all anomalies in this group are detected almost every time. Cluster 3 (right panel)\nexhibits a similar pattern, with a prominent mode around 0.9–1.0 and only a small tail below 0.7,\n28\n"}, {"page": 29, "text": "Cluster 1\nCluster 2\nCluster 3\n0.00\n0.25\n0.50\n0.75\n1.000.00\n0.25\n0.50\n0.75\n1.000.00\n0.25\n0.50\n0.75\n1.00\n0\n1\n2\n3\nDetection Probability\nDensity\nFigure 7: Detection probability across the clusters\nconfirming that its anomalies are likewise robust under sparse sampling. By contrast, Cluster 2’s\ndistribution (center panel) is centered around 0.3–0.4 and is much flatter, revealing that most of\nits anomalies are recovered in fewer than half of the replicates.\nThese density profiles mirror our recall findings: anomalies in Clusters 1 and 3 produce large,\nsustained deviations from their cluster means and thus enjoy high, stable detection probabilities,\nwhereas the subtler, more localized deviations in the smaller Cluster 2 lead to low and highly vari-\nable hit-rates. Together, these results underscore the need for enhanced detection strategies—such\nas targeted time-window testing (Algorithm 3)—to reliably capture the more fragile anomalies in\nCluster 2.\nRecall Comparison Across Clusters: Significance Testing\nIn order to formally evaluate\nwhether the observed differences in recall across clusters are statistically significant, we employed\na Kruskal–Wallis rank-sum test, a nonparametric test of significance for comparing more than two\nindependent groups. Accounting for the imbalance in cluster sizes—Clusters 1 and 3 each contain\napproximately 800 users, whereas Cluster 2 contains only about 200—the null hypothesis H0 : “all\nthree clusters have the same recall distribution” was tested at the α = 0.05 level. We obtained\nχ2(2) = 100.28,\np-value < 2 × 10−16,\nstrongly rejecting H0. To identify which clusters differ, we conducted pairwise Wilcoxon rank-sum\ntests with Benjamini–Hochberg adjustment for multiple comparisons. The adjusted p-values for\n29\n"}, {"page": 30, "text": "Cluster 2 vs. Cluster 1 and Cluster 2 vs. Cluster 3 were both\npadj < 2 × 10−16,\nindicating highly significant lower recall in the small Cluster 2, while the comparison between\nClusters 1 and 3 yielded\npadj = 0.20,\nconsistent with no significant difference at the 5% level. Finally, to account for the paired nature\nof the 50 simulation runs, we applied a Friedman test, another nonparametric test of significance\nfor related samples, which yielded\nχ2(2) = 75.16,\np-value < 2 × 10−16.\nTogether, these statistical tests of significance confirm that Clusters 1 and 3 share equivalently high\nand stable recall under sparse sampling, whereas the much smaller Cluster 2 exhibits a significantly\nand substantially weaker recall performance.\nSupplemental details regarding experiments perform in Section 4.1:\nFor Cluster 2\nwe applied the two-stage anomaly pipeline with screening threshold α1 = 0.15 and Bonferroni-\ncorrected confirmation α = 0.10 across B = 4 mFPC loadings, followed by window-wise testing at\nα = 0.20 across W = 4 fixed intervals. All confirmed outliers i ∈A(2) were then summarized in\nthe Excel file cluster2_pvalues.xlsx.\nThis spreadsheet has nine columns:\n• user_id: the unique subject identifier i.\n• p_comp1, . . . , p_comp4: the Bonferroni-adjusted empirical p-values ˆpemp\ni,m from the Type-1\ntest on the mth mFPC loading (m = 1, . . . , 4).\n• p_win1, . . . , p_win4: the Bonferroni-adjusted empirical p-values ˆp(w)\ni\nfrom the Type-2 test\nin the wth time window (w = 1, . . . , 4).\n30\n"}, {"page": 31, "text": "Entries marked NA in the p_win* columns indicate windows with no observations for subject\ni, while numeric values record the corresponding adjusted p-value. This table therefore provides,\nfor each of the 56 Cluster 2 anomalies, a complete vector of significance measures across both the\nprincipal-component and time-window analyses.\nFor Clusters 1 and 3, we applied the two-stage anomaly pipeline with screening threshold\nα1 = 0.1 and Bonferroni-corrected confirmation α = 0.05 across B = 4 mFPC loadings, followed\nby window-wise testing at α = 0.10 across W = 4 fixed intervals. Similar details are available for\nclusters 1 and 3 in files cluster1_pvalues.xlsx and cluster3_pvalues.xlsx respectively.\nThe Amazon reviews data used for both experiments and simulations is sourced from Amazon\ncustomer reviews datasets and selecting the appropriate item categories as mentioned in the paper.\nPrompts for emotion-scoring implementation using Plutchik’s wheel is provided in Section C.\nFurther methodological specifics—including clustering and anomaly-detection algorithms, the-\noretical underpinnings, subroutine descriptions, keyword profiling procedures, and emotion-scoring\nvalidation—are detailed in Appendix Sections C through G.\nC\nSupplemental details for Emotion scoring and validation\nand FPCA Methodology\nOur primary analysis centers on the Automobile category of the Amazon reviews dataset, which\nincludes 1,946 users and 22,032 reviews over five years. Reviews from the other two categories are\nused to construct user-level covariates, such as the proportion of purchases across categories. Each\nreview includes a user ID, timestamp, product title, review text, and a 1–5 star rating. Users post\nreviews at irregular intervals over multiple years.\nC.1\nEmotion scoring and validation based on Plutchik’s wheel of emo-\ntions\nThe extraction of emotions from customer reviews and interaction transcripts is critical for orga-\nnizations seeking to move beyond coarse sentiment analysis and gain actionable insights into the\n31\n"}, {"page": 32, "text": "nuanced affective states of their users. Idnetification and quantification of emotions embedded\nin textual feedback allows businesses to tailor their responses, segment customers, and proac-\ntively address emerging issues. Among various emotion detection frameworks, Plutchik’s Wheel\nof Emotions is frequently chosen due to its structured and psychoevolutionary foundation, which\ncaptures both the complexity and gradation of human affect Plutchik [1980], Seconds [2025], Se-\nmeraro et al. [2021]. In natural language processing, researchers have utilized Plutchik’s model\nto relabel emotion datasets, enabling more nuanced detection and classification of emotions in\ntext, which leads to improved performance in emotion recognition tasks, especially for complex\nor subtle affective states Mohammad and Bravo-Marquez [2018], Alhuzali and Ananiadou [2021].\nBeyond text, Plutchik’s model also informs emotion AI applications in facial recognition and user\nexperience design, helping systems identify, interpret, and appropriately respond to the emotional\nstates of users, thereby enhancing personalization and emotional intelligence in human-computer\ninteraction Calefato et al. [2017]. Unlike continuous dimensional models or basic discrete emotion\nsets, Plutchik’s wheel organizes eight core emotions—joy, trust, fear, surprise, sadness, anticipa-\ntion, anger, and disgust—along with their intensities and combinations, enabling a richer and\nmore interpretable taxonomy for emotion detection Seconds [2025], Semeraro et al. [2021]. This\nis represented in 4 petals with opposing extremes: joy-sadness,trust-fear, surprise-anticipation,\nanger-disgust. The wheel’s compositional structure also facilitates algorithmic implementation,\nallowing AI systems to process negations and blends of emotions through arithmetic and logical\noperations, which is particularly advantageous for analyzing customer feedback, social media, and\nconversational data Mohammad [2020]. Leveraging Plutchik’s model thus ensures a comprehen-\nsive and interpretable framework for mapping textual cues to specific emotional categories and\nintensities, facilitating more robust and actionable emotion analytics than alternative approaches.\nIn this work, we use in-context learning with to prompt GPT-3.5-Turbo to map the customer\nreviews to each of the 4 dimensions corresponding to Plutchik’s Wheel of Emotions\nListing 1 and table 7 outline the scores and the prompt used for “joy–sadness” petal. Emotions\nare mapped to a continuous scale from −1 to 1, where positive values represent varying intensities of\njoy (serenity, joy, ecstasy) and negative values represent varying intensities of sadness (pensiveness,\n32\n"}, {"page": 33, "text": "sadness, grief). Similar prompts and scores are used for other petals as well. The rubric is as\nfollows:\nEmotion\nIntensity\nScore Range\nEcstasy\nIntense\n(2\n3, 1]\nJoy\nModerate\n(1\n3, 2\n3]\nSerenity\nMild\n(0, 1\n3]\nPensiveness\nMild\n[0, −1\n3)\nSadness\nModerate\n[−1\n3, −2\n3)\nGrief\nIntense\n[−1, −2\n3)\nTable 7: Emotion scoring rubric based on Plutchik’s joy–sadness petal.\nThe following prompt is used to guide the scoring process for each review:\nListing 1: Prompt for Plutchik-based Emotion Scoring\nYou are an expert at honestly classifying and scoring emotion from text.\nThis task involves analyzing user’s review of a product, consisting of a review_title\nand review_text.\nThe objective is to quantitatively measure emotions, and score them based on a\nstructured framework of emotions on a scale of -1 and 1.\nEmotions can be represented using Plutchik’s wheel of emotion using 4 pairs of petals\nand opposing petal combinations.\nFor this task, focus on one petal and its opposing pair. The petal includes: Ecstacy,\njoy and serenity.\nThe opposing petal includes: Grief, sadness and pensiveness. The emotions on each petal\nare arranged on the basis of their intensities.\nEcstacy is the most intense form of emotion and its opposing pair is grief. Joy is a\nmoderate form and its opposing pair is sadness.\nSimilarly Serenity is a mild form, and pensiveness is its opposing pair.\n--- INSTRUCTIONS ---\nCarefully read the user’s review, including both the review_title and review_text. Based\n33\n"}, {"page": 34, "text": "on the emotional cues present,\nuse the structured framework of the emotions, described below, along with their\nrespective intensities, to guide your analysis,\nand score the review, on a scale of -1 and 1.\nThe emotion score should lie between 0 and 1 if the review suggests ecstacy, joy or\nserenity, while it should lie between 0 and -1\nif the review suggests grief, sadness or pensiveness. The score should be determined\nbased on the intensity of the emotion. For e.g.\nsince ecastacy is the most intense form of emotion it should be scored between 2/3 and\n1.\nSimilarly Grief being the most intense opposing pair, it should be scored on a scale of\n-2/3 and -1. Similarly if joy is detected it\nshould be scored on a scale of 1/3 and 2/3, and if its opposing pair sadness is detected\nit should be scored on a scale of -1/3 and -2/3.\nSimilarly if Serenity is detected it should be scored on a scale of 0 and 1/3 and if its\nopposing pair pensiveness is detected it should\nbe scored on a scale of 0 and 1/3. Absence of these emotions defaults to a score of 0.\nThe final score should be a single number between -1 and 1, reflecting the emotion and\nits intensity.\nPetal Dynamics:\nJoy: Indicates happiness or pleasure derived from product satisfaction.\n- Serenity (Mild)\n- Joy (Moderate)\n- Ecstasy (Intense)\nOpposing Petal Dynamics:\nSadness: Reveals disappointment or sorrow due to unmet product expectations.\n- Pensiveness (Mild)\n34\n"}, {"page": 35, "text": "- Sadness (Moderate)\n- Grief (Intense)\n--- TASK ---\nFor each user’s review_title and review_text, follow the instructions to score the\nemotion expressed\non a scale of -1 and 1. Ensure the final score is a single number between -1 and 1 based\non petal or opposing petal dynamics, without any additional explanation.\nin the format: ’Score= ’\nScoring Emotions of Amazon reviews based on Plutchik’s Wheel of Emotions, and\nMode of Variation plots\nThroughout the empirical study we specialise to p = 4 by letting\nΦ return continuous scores along the four opposing-petal pairs of Plutchik’s Wheel of Emotions.\nConcretely,\nYi(Tij) =\n\u0000e(1)\nij , e(2)\nij , e(3)\nij , e(4)\nij\n\u0001\n,\n(C.1)\nwhere, for instance, e(1)\nij\n= +1 encodes intense Joy and e(1)\nij\n= −1 encodes intense Sadness,\nwith proportionate grading in between. Appendix G reports the prompt template and a man-\nually–annotated validation set showing an average match rate of approximately 56 %.\nLet each customer be indexed by i ∈{1, . . . , N}, and let xij denote the textual datapoint (e.g.,\na review, comment, or message) associated with the j-th interaction of customer i, recorded at\ntimestamp Tij ∈Ti ⊂R, where j ∈{1, . . . , mi} and mi is the number of observed interactions for\ncustomer i. Each xij is mapped to a four-dimensional vector of continuous emotion scores:\neij =\n\u0010\ne(1)\nij , e(2)\nij , e(3)\nij , e(4)\nij\n\u0011\n∈[−1, 1]4,\n(C.2)\nwhere each component e(d)\nij for d ∈{1, 2, 3, 4} corresponds to an opposing-petal pair in Plutchik’s\nWheel of Emotions: Joy–Sadness, Anger–Fear, Trust–Disgust, and Anticipation–Surprise, respec-\ntively.\nThe value e(d)\nij = +1 indicates strong expression of the first emotion in the pair (e.g., Joy), −1\n35\n"}, {"page": 36, "text": "denotes strong expression of the second (e.g., Sadness), and 0 corresponds to emotional neutrality.\nEmotion scores are generated by querying GPT-3.5 Turbo leveraging prompt engineering with\na fixed prompt that defines each Plutchik pair and instructs the model to output four scalar\nintensities representing the emotional content of xij on the [−1, 1] scale. This pipeline produces a\ndeterministic mapping from each textual interaction to an interpretable, low-dimensional emotional\nrepresentation. Although each dimension is extracted independently, and therefore modeled as\nmarginally uncorrelated at this stage, we do not assume that the underlying emotional processes\nare statistically independent. In particular, we later capture their joint evolution using multivariate\nsparse functional models (Section 3).\nValidation\nTo validate our numeric embedding scheme, we randomly sampled 30 reviews from\nacross categories and obtained expert human labels for the 24 emotion categories across the four\nPlutchik petal and opposing petal combinations.\nWe then ran a GPT prompt 2 on those re-\nviews, performed classification, and computed exact-match accuracy (19/33 = 57.6 %). Accuracy\nwas measured as the proportion of reviews for which GPT’s emotion classification matched the\nconsensus human annotation (see Appendix G, Table 8).\nFormally, for each customer i and emotion dimension d, we define the observed emotion signal\nas a sparse, irregular time series:\nE(d)\ni\n=\nn\u0010\nTij, e(d)\nij\n\u0011\n: j = 1, . . . , mi\no\n,\nd = 1, . . . , 4.\n(C.3)\nThe full sequence {eij}mi\nj=1 defines a multivariate, irregularly sampled emotional trajectory for cus-\ntomer i, which serves as the input to the functional data analysis pipeline developed in subsequent\nsections.\nEvaluation of the emotion scoring method, including prompt design and accuracy computed\nagainst manually annotated examples, is described in Appendix G.\nMode of variation plots\nThis routine produces a series of faceted line plots that contrast each\nconfirmed anomalous subject’s component-driven trajectory deviations with both the cluster mean\n36\n"}, {"page": 37, "text": "and a reference cohort along a given anomalous eigenfunction direction ψm.\nFor a subject i ∈C flagged by Algorithm 2 and each anomalous direction m ∈Si, the following\ncurves are drawn for each embedding dimension d = 1, . . . , p:\n1. Cluster Mean: ˆµ(d)\nˆC (t), as estimated in Algorithm 6.\n2. Clean Cohort trajectories: ˆµ(d)\nˆC (t) + ˆρjm, ˆC ˆψ(d)\nm, ˆC(t) for each reference subject j ∈Gc\ns (the\nclean held-out set from Algorithm 7).\n3. Subject trajectory: ˆµ(d)\nˆC (t) + ˆρim, ˆC ˆψ(d)\nm, ˆC(t).\nOnly interior time-points are displayed to avoid boundary artifacts. If window-level anomalies\nWi ⊂{1, . . . , W} were identified by Algorithm 10, the corresponding intervals on the time-axis are\nlightly shaded. Line styling distinguishes the cohort (thin solid), the anomalous subject (thicker\nsolid), and the mean (dashed). The outcome is a list of B plots—one per anomalous compo-\nnent—each facetted by dimension d, offering a clear visual summary of how the subject’s trajectory\ndeparts from typical variation.\nCluster 2 examples\nOn top of the existing results in Section 4.1 we provide additional exam-\nples leveraging mode of variation plots for users in cluster 2. Users in cluster 2 generally have\npredominantly lower scores on the joy and trust petal as evident from figure 3. Figure 8 represents\na user who is no different, and has even more extreme emotion trajectory resulting in being flagged\nas an outlier. The trajectory is decisively negative—missing parts, poor cleaning performance, and\ncheaply made accessories drive Fear–Anger and Surprise–Anticipation to their highest levels while\nTrust–Disgust turns consistently downward. The final window of the journey remains dominated\nby pain points that describe products falling off, tearing during installation, or arriving fused or\nbroken; correspondingly, all four emotion curves sit well below the cluster mean, and the user is\nflagged as an outlier for extreme negativity. Potential actionable insights include tightening vendor\nquality control and shipping inspection for fragile SKUs, adding explicit fit-compatibility checks\nat checkout, and prioritizing fast replacement for early-life failures—measures that directly target\nthe defects driving the user’s strongest negative reactions.\n37\n"}, {"page": 38, "text": "fear−anger\njoy−sadness\nsurprise−anticipation\ntrust−disgust\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n0.25\n0.50\n0.75\n−0.2\n−0.1\n0.0\n0.1\n0.2\n0.3\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\n−0.3\n−0.2\n−0.1\n0.0\n0.1\n0.2\n−0.10\n−0.05\n0.00\nTime\nEmotion Score\nClean Cohort\nAnomaly\nMean\nFigure 8: Mode of variation plot for a user along the first FPC (outlying) from cluster 2\nGiven that the cluster has predominantly negative emotions, one would expect a user within\nthis cohort experiencing positive product experiences at some point in their journey to be an\noutlier. This is exactly what we observe for the user depicted by the emotion customer journey\ncurves, as in the figure 9. This further showcases the validity of our anomaly detection algorithm\nfear−anger\njoy−sadness\nsurprise−anticipation\ntrust−disgust\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.2\n0.4\n0.6\n0.8\n0.0\n0.1\n0.2\n0.3\n0.00\n0.05\n0.10\n−0.1\n0.0\n0.1\n0.2\n−0.125\n−0.100\n−0.075\n−0.050\n−0.025\n0.000\nTime\nEmotion Score\nClean Cohort\nAnomaly\nMean\nFigure 9: Mode of variation plot for a user along the second FPC (outlying) from cluster 2\nwhere the outlying customers are detected with respect to cluster means.\nIt can be observed\nthat the joy petal increases significantly towards the later half of the customer’s journey.\nIn\nthe first half of the timeline, the user echoes the cluster mood: adhesive patches fail, and an\noverpriced organiser proves hard to fold. Fear–Anger and Surprise–Anticipation rise sharply here,\n38\n"}, {"page": 39, "text": "while Joy–Sadness and Trust–Disgust sink, mirroring these early disappointments. Beginning at\nroughly the midpoint, the pattern reverses: successive five-star reviews report magnetic trays that\nsimplify sewing, door-edge guards installed from a wheelchair without scratches, and a series of\neasy-to-use safety and cleaning accessories. All positive emotion curves swing upward, marking\nthe user as a positive outlier against an otherwise pessimistic cohort.\nC.2\nMethodological framework for univariate and multivariate FPCA\nUnivariate FPCA models each embedding dimension separately, yet the coordinates of a subject’s\nnumeric trajectory often exhibit strong dependencies. To capture their joint variation, we employ\nMultivariate Sparse Functional PCA following the framework of Happ et al. (2018). Let ˆξ(d)\nik denote\nthe demeaned univariate FPC scores for subject i on the kth basis of dimension d = 1, . . . , p. We\nassemble these into the score matrix\nΞ ∈RN×M,\nwith rows Ξi =\n\u0000ˆξ(1)\ni1 , . . . , ˆξ(1)\niK1, . . . , ˆξ(p)\ni1 , . . . , ˆξ(p)\niKp\n\u0001\n,\nwhere M = Pp\nd=1 Kd. The sample covariance of the stacked scores is\nbZ =\n1\nN −1 Ξ⊤Ξ ∈RM×M,\nand its eigen-decomposition yields eigenpairs (ˆνm, ˆcm) for m = 1, . . . , M. The resulting multivariate\nFPC scores for subject i are\nˆρim = Ξi · ˆcm,\ni = 1, . . . , N.\nTo reconstruct smooth, low-dimensional trajectories, we project each univariate eigenfunction\nˆϕ(d)\nn (t) onto the multivariate basis:\nˆψ(d)\nm (t) =\nKd\nX\nn=1\nˆc(d)\nm,n ˆϕ(d)\nn (t),\n39\n"}, {"page": 40, "text": "where ˆc(d)\nm,n denotes the entry of ˆcm corresponding to dimension d. The centered reconstruction on\ndimension d is then\ne\nX(d)\ni (t) =\nM\nX\nm=1\nˆρim ˆψ(d)\nm (t),\nand adding back the univariate mean ˆµ(d)(t) yields the final multivariate embedding\nb\nX(d)\ni (t) = ˆµ(d)(t) +\nM\nX\nm=1\nˆρim ˆψ(d)\nm (t),\nd = 1, . . . , p.\nThis procedure delivers a smooth, joint summary of each subject’s p-variate trajectory while pre-\nserving the original scale of the numeric embeddings.\nAfter extracting multivariate FPC scores ˆρim for m = 1, . . . , M, we form the score vector\nρi = (ˆρi1, . . . , ˆρiM) for each subject i. Time-invariant covariates (e.g. average rating, review length)\nare collected in zi ∈Rq. We concatenate these into joint features\nwi = (ρi, zi) ∈RM+q,\nthen center and whiten them by computing\n¯w = 1\nN\nN\nX\ni=1\nwi,\nWc =\n\n\nw1 −¯w\n...\nwN −¯w\n\n\n,\nΣ =\n1\nN −1W ⊤\nc Wc,\n˜wi = Σ−1/2(wi −¯w),\nso that { ˜wi} have zero mean and identity covariance. Clustering is then performed on { ˜wi} yields\ngroups {Cℓ}L\nℓ=1 of subjects with similar joint profiles using algorithm 5 in section 6 below.\nUnivariate Functional Principal Component Analysis (uFPCA) Subroutine\nFrom Sec-\ntion C.2, we have that we model each subject’s latent, continuous embedding trajectory in dimen-\nsion d = 1, . . . , p by\nX(d)\ni (t),\nt ∈T , i = 1, . . . , N.\n40\n"}, {"page": 41, "text": "Each observed embedding\nY (d)\ni\n(Tij) = X(d)\ni (Tij) + ε(d)\nij ,\nε(d)\nij ∼N(0, σ2\nd),\nthus forms sparse functional data due to the irregular and limited sampling times {Tij}Ni\nj=1.\nTo recover the main modes of variation in each dimension separately, we apply the univariate\nsparse FPCA of Yao et al. [2005] using algorithm 4 below.\nSubroutine algorithm 4 (uFPCA): This subroutine implements the univariate sparse functional\nprincipal component analysis on irregularly sampled observations. Starting from the set of pairs\n{(Tij, Yij)}, it first estimates the mean trajectory ˆµ(t) (for example, via local polynomial smooth-\ning) and then constructs a smoothed covariance surface ˆG(s, t). An eigen-decomposition of ˆG yields\nthe principal component functions {ˆϕk(t)} and their associated variances {ˆλk}. The number of\ncomponents K is chosen—typically by the fraction of variance explained—and the subject-specific\nFPC scores {ˆξik} are estimated (for example, using the PACE algorithm), with centering to en-\nsure mean zero. The outputs comprise the centered scores, the estimated mean function, and the\neigenfunctions.\nAlgorithm 4 uFPCA: Univariate Functional Principal Component Analysis (uFPCA) Subroutine.\nInput: Univariate sparse functional data {(Tij, Yij) : i = 1, . . . , N, j = 1, . . . , Ni}.\n1: Estimate mean function ˆµ(t).\n▷e.g., via local polynomial smoothing\n2: Estimate covariance surface ˆG(s, t).\n▷e.g., via smoothing of raw covariances\n3: Perform eigen-decomposition of ˆG(s, t) to get eigenfunctions ˆϕk(t) and eigenvalues ˆλk.\n4: Determine number of components K (if not pre-specified).\n▷e.g., using Fraction of Variance\nExplained (FVE)\n5: Estimate FPC scores ˆξik for k = 1, . . . , K, ensuring they are centered.\n▷e.g., via PACE Yao\net al. [2005]\nOutput:\nTuple:\n(Centered FPC scores {ˆξik}N,K\ni=1,k=1, mean function ˆµ(t), eigenfunctions\n{ˆϕk(t)}K\nk=1).\nFor d = 1, . . . , p, we approximate\nX(d)\ni (t) ≈µ(d)(t) +\nKd\nX\nk=1\nξ(d)\nik ϕ(d)\nk (t),\n41\n"}, {"page": 42, "text": "where µ(d)(t) is the population mean function, {ϕ(d)\nk (t)} are the principal component functions,\nand {ξ(d)\nik } are the corresponding FPC scores for subject i. The truncation level Kd is chosen by\nthe proportion of variance explained.\nDenoting the estimates by ˆµ(d), ˆϕ(d)\nk , and ˆξ(d)\nik , the fitted trajectory for subject i in dimension d\nbecomes\nˆX(d)\ni (t) = ˆµ(d)(t) +\nKd\nX\nk=1\nˆξ(d)\nik ˆϕ(d)\nk (t).\nThis provides a smooth, low-dimensional summary of each subject’s p-variate embedding trajectory\nin its original scale.\nD\nFundamentals of Clustering and the Corresponding Algo-\nrithmic Framework\nD.1\nRecovery of Clusters\nTo analyze cluster recovery, we condition on ai and treat it as deterministic. Assume the trajec-\ntories Y i are fully observed for all i = 1, . . . , N. Then, for non-anomalous subjects i ∈Ck ∩AC\n0 , ,\nwe have E(Y i(t)) = µk(t) for all t ∈T , whereas for anomalous subjects i ∈Ck ∩A0, E(Y i(t)) =\nµk(t) + ai(t) for all t ∈T . Define the overall population mean of Xi as µ⋆(t) = PK\nk=1 πkµk(t), for\nall t ∈T with πk being the cluster proportions. The corresponding population mean of Y i is then\ngiven by µ(t) = µ⋆(t) + ¯a(t) where ¯a(t) =\n1\nN\nPN\ni=1 ai(t) for all t ∈T . Since the eigenfunctions\nψm ∈L2(T )p are shared across clusters, performing mFPCA on Xi −µ⋆using the pooled sample\nyields the same eigenfunctions ψm, m = 1, . . . , M, as the cluster-specific mFPCA. This is equiv-\nalent to performing mFPCA on Y i −µ, after adjusting the covariance operator by removing the\ndiagonal noise component σ2\nη; see [Yao et al., 2005] for details. The mFPC scores of ρim of Y i −µ\nare then given by ρim = ⟨Y i −µ, ψm⟩. One can decompose the mFPC scores as\nρim = ⟨Y i −µ, ψm⟩= ⟨Xi −µk, ψm⟩+ ⟨µk −µ⋆, ψm⟩+ ⟨ηi, ψm⟩+ ⟨ai −¯a, ψm⟩.\n42\n"}, {"page": 43, "text": "For i ∈Ck ∩AC\n0 , E(ρim) = ⟨µk −µ⋆, ψm⟩as for i ∈Ck ∩AC\n0 , ai ≡0, and E(⟨Xi −µk, ψm⟩) = 0\nand E(⟨ηi, ψm⟩) = 0 for all i ∈Ck. Similarly, for all i ∈Ck ∩A0, E(ρim) = ⟨µk −µ⋆, ψm⟩+ ⟨ai −\n¯a, ψm⟩. Thus, mFPCA applied to all trajectories Y i produces scores whose expectations differ\nacross clusters and reflect anomalies when present. This motivates modeling each subject’s scores\njointly with their static covariates Zi via wi = (ρi1, . . . , ρiM, Z1, . . . , Zq) ∈RM+q as\nwi = θk + ϵi + αi,\nfor i ∈Ck,\nwhere θk ∈RM+q is the cluster mean of the uncorrupted points in Ck, ϵi ∈RM+q is a sub-Gaussian\nrandom vector with covariance Σ ∈RM+q×M+q such that for σ > 0, E[ϵi] = 0,\nand for all v ∈\nRM+q,\nE [exp (⟨v, ϵi⟩)] ≤exp\n\u0010\nσ2∥v∥2\n2\n\u0011\n. The term αi represents perturbations due to anomalies,\nwith αi = 0 for i ∈AC\n0 and αi ̸= 0 otherwise.\nSince we are interested in anomalies in the\nfunctional trajectories, we assume that αi ̸= 0 only in the first M coordinates for i ∈A0, i.e.\nαij = 0 for all j ∈{M + 1, . . . , M + q}. Let E(Zi) = γk for i ∈Ck.\nLet ∆= mink̸=ℓ∥θk −θℓ∥denote the signal strength (separation between the means of the\nuncorrupted data across clusters). In terms of FPC scores, θkm = ⟨µk −µ⋆, ψm⟩for m = 1, . . . , M,\nand θkm = γk for m ≥M + 1. Let Nk = |Ck|, and let N a\nk = |Ck ∩A0| denote the number of\nanomalies in cluster Ck. Write Na = |A0|, N ⋆= N −Na, and ε = Na/N. We run trimmed k-means\nwith trimming rate τ and then apply Lloyd’s algorithm to the retained observations to estimate\ncluster assignments. The following argument makes the retained set essentially free of anomalies\nbefore Lloyd.\nLet d = M + q. Fix δ ∈(0, 1/2) and define the inlier (1 −δ)-radius R1−δ by\nPr\n\u0000∥wi −θk∥≤R1−δ\n\f\f i ∈Ck ∩AC\n0\n\u0001\n≥1 −δ\nfor each k.\nUnder the sub-Gaussian assumption on ϵi, there exists a universal constant c > 0 such that\nR1−δ ≤c σ\np\nd + log(1/δ) .\n43\n"}, {"page": 44, "text": "Let the initialization error be Λ0 :=\n1\n∆maxk∈{1,...,K}\n\r\rˆθ\n(0)\nk\n−θk\n\r\r and assume that\nR1−δ <\n\u0010\n1\n2 −Λ0\n\u0011\n∆.\n(D.1)\nCondition (D.1) implies that, with probability at least 1 −δ for each k,\n∥wi −ˆθ\n(0)\nk ∥< ∥wi −ˆθ\n(0)\nℓ∥\nfor all ℓ̸= k,\ni ∈Ck ∩AC\n0 .\n(D.2)\nIn particular ∥wi −ˆθ\n(0)\nk ∥< 1\n2 ∆and ∥wi −ˆθ\n(0)\nl ∥≥1\n2 ∆for l ̸= k.\nChoose the trimming rate to cover true anomalies plus the inlier tail,\nτ ≥ε + δ,\n(D.3)\nwith ε = Na/N. After the initial trimmed assignment that keeps the closest (1−τ)N observations\nto their nearest initialized center, all anomalous points are trimmed and at most a δ fraction of\nnon-anomalous points are discarded. Hence the set passed to Lloyd’s algorithm coincides with\nS\nk(Ck ∩AC\n0 ) up to at most δN ⋆losses. Denote zi = k as the true cluster assignment for each\nnon-anomalous point i ∈Ck ∩AC\n0 , and let ˆzs\ni be the assignment at iteration s = 0, 1, 2, . . . . Define\nthe mis-clustering rate on non-anomalous points as\nAs := 1\nN ⋆\nX\ni∈AC\n0\nI{ˆzs\ni ̸= zi}.\nFor the recovery guarantee, let πmin = mink πk with πk = (Nk −N a\nk )/N ⋆−δ and\nrK = ∆\nσ\nr\nπmin\n1 + Kd/{(1 −δ)N ⋆}\nand\nΛs = 1\n∆\nmax\nk∈{1,...,K}\n\r\rˆθ\n(s)\nk −θk\n\r\r.\nTheorem D.1. Assume (1 −δ)N ⋆π2\nmin ≥CK log{(1 −δ)N ⋆} and rK ≥C\n√\nK for a sufficiently\nlarge constant C. Given any (data-dependent) initialization satisfying Λ0 ≤1/2 −4/√rK with\n44\n"}, {"page": 45, "text": "probability 1 −ν, we have\nAs ≤exp\n\u0012\n−∆2\n16σ2\n\u0013\n+ δ\nfor all s ≥4 log N ⋆\nwith probability at least 1 −ν −4/{(1 −δ)N⋆} −2 exp(−∆/σ).\nProof. The result follows by applying Theorem 3.2 of Lu and Zhou [2016] to the retained obser-\nvations obtained after the trimming step, under the stated initialization conditions.\nD.2\nAlgorithm framework for Subject Segmentation and Cluster-Specific\nmFPCA\nAlgorithm 5 segments subjects into K groups by combining their multivariate functional principal\ncomponent (mFPC) embeddings with static covariates. First, for each subject i, the vector of\nits top M mFPC scores is concatenated with its q-dimensional covariate vector to form a joint\nfeature vector wi ∈RM+q. The entire collection {wi}N\ni=1 is then mean–centered and whitened—i.e.\nmultiplied by the inverse square root of its empirical covariance—so that the transformed features\nhave zero mean and identity covariance. Finally, any off-the-shelf clustering algorithm (for example,\nK-means) is applied to the whitened feature set { ˜wi} to produce the final partition { ˆCk}K\nk=1.\nAlgorithm 5 Clustering Subjects using mFPC Scores and Covariates\nInput: mFPC scores {ˆρim} from Algorithm 1, and subject-level covariates {Zi ∈Rq}.\n1: For i = 1, . . . , N, get the mFPC score vector ˆρi = (ˆρi1, . . . , ˆρiM)\n▷Algorithm1.\n2: Construct joint feature vectors wi = (ˆρi, Zi) ∈R1×(M+q) for i = 1, . . . , N.\n3: Standardize {wi} to obtain whitened features { ˜wi}:\n▷Standardisation\na. Compute mean ¯w = 1\nN\nP wi and centered features wi,c = wi −¯w.\nb. Compute covariance ˆΣw =\n1\nN−1\nP w⊤\ni,cwi,c and its inverse square root ˆΣ\n−1/2\nw\n.\nc. ˜wi = wi,c ˆΣ\n−1/2\nw\n.\n▷Features have zero mean, identity covariance\n4: Apply trimmed K-means to { ˜wi}N\ni=1 to obtain K clusters { ˆCk}K\nk=1.\nOutput: A set of K clusters { ˆCk}K\nk=1 .\nWithin each cluster Cℓ, we re-apply the Multivariate Sparse FPCA procedure to obtain cluster-\nspecific mean functions ˆµ(d)\nCℓ(t), eigenfunctions ˆψ(d)\nm,Cℓ(t), and updated scores ˆρim,Cℓusing algorithm\n6 below.\n45\n"}, {"page": 46, "text": "Algorithm 6 revisits each cluster identified in Algorithm 5 and applies the multivariate sparse\nFPCA pipeline to the subjects within that cluster. For each cluster ˆCk, it collects the raw SL\ntrajectories, invokes the mFPCA routine (Algorithm 1), and produces cluster-specific FPC scores,\neigenfunctions, and mean curves that capture the principal modes of joint variation within that\ngroup.\nAlgorithm 6 Cluster-Specific mFPCA\nInput: Data {Y i(Tij)}N\ni=1, and the set of K clusters { ˆCk}K\nk=1 from Algorithm 5.\n1: for k = 1, . . . , K do\n▷For each cluster\n2:\nLet Y ˆCk = {Y i(Tij) : i ∈ˆCk}.\n▷Data for subjects in cluster ˆCk\n3:\n({ˆρk\nim}i∈ˆCk, { ˆψ\nk\nm}, {ˆµk}) ←mFPCA(Y ˆCk)\n▷Algorithm 1\n4: end for\nOutput: For each cluster ˆCk (k = 1, . . . , K): cluster-specific mFPC scores {ˆρk\nim}i∈ˆCk, mean func-\ntion {ˆµk(t)}, and eigenfunctions {ˆψ\nk\nm}.\nThe final subject-specific p-variate trajectory, incorporating cluster-specific structure, ˆXi, ˆCℓ(t)\nfor subject i ∈ˆCℓ, is then given by:\nb\nX(d)\ni,Cℓ(t) = ˆµ(d)\nCℓ(t) +\nM\nX\nm=1\nˆρim,Cℓˆψ(d)\nm,Cℓ(t),\nd = 1, . . . , p,\n(D.4)\nthereby aligning each subject’s fitted trajectory with the structural characteristics of its assigned\ncluster.\nD.3\nCluster Stability\nThough clustering is fully unsupervised and lacks ground truth labels, several validation metrics\nexist to assess cluster quality. In our implementation of LLmFPCA-detect across both domains, we\nselected the optimal number of clusters based on the average silhouette width. LLmFPCA-detect\ncan, in general, accommodate any clustering method without altering the overall workflow. For\nthe Amazon Reviews dataset, we assessed cluster stability using the bootstrapped Jaccard index,\na common metric in the clustering literature. Across 200 bootstrap samples, the mean Jaccard\nindices for Clusters 1–3 were 77%, 91%, and 76%, respectively. The scores being ≥75% confirm the\n46\n"}, {"page": 47, "text": "reproducibility and stability of the clusters. A similar validation was performed on the Wikipedia\ndataset, yielding 94% for Cluster 1 and 87% for Cluster 2, further demonstrating consistent cluster\nrecovery under resampling.\nE\nPrinciples of Anomaly Detection with Subroutine Details\nfor Algorithms 2 and 3 in Section 3\nE.1\nPrinciples of Anomaly Detection\nFor i ∈A0 suppose that the random variable ⟨ai, ψm⟩is distributed according to a continuous\nlaw Lm,a with compact support [amin, amax] where amin, amax > 0. Once the clusters have been ob-\ntained, one can extract the cluster-specific mFPCA of the trajectories and recompute the mFPCA\nscores as\nρk\nim =⟨Y i −µk, ψm⟩= ⟨Xi −µk, ψm⟩+ ⟨ηi, ψm⟩+ αim\nwhere αim = ⟨ai, ψm⟩.\nFor i ∈Ck ∩AC\n0 , define Lk\nm as the law of ρk\nim = ⟨Y i −µk, ψm⟩=\n⟨Xi −µk, ψm⟩+ ⟨ηi, ψm⟩as αim = 0 in this case. Let ρk\nm ∼Lk\nm, and assume that ρk\nm has a\ncontinuous distribution. For the anomalous points i ∈Ck ∩A0, the scores ρk\nim = ⟨Y i −µk, ψm⟩=\n⟨Xi −µk, ψm⟩+ ⟨ηi, ψm⟩+ αim are assumed to be independent, with αim\niid\n∼Lm,a. Thus, the law\nof ρk\nim corresponds to that of ρa,k\nm = ρk\nm + αm, where αm ∼Lm,a is independent of ρk\nm. Let ˜ρk\nm\nbe a random variable drawn from the empirical distribution of ρk\nim over i ∈Ck. For any given\nϵ > 0, define the set Ak,ϵ\n0\nas Ak,ϵ\n0\n= {i ∈Ck : P(SM\nm=1{|˜ρk\nm| > |ρk\nim|}) < ϵ}. Theorem E.1 states\nthat one can screen for the anomalous points in Ck by using the tail of the distribution of ˜ρk\nm.\nWhile this may not recover Ck ∩A0 exactly, observe that for i ∈Ak,ϵ\n0\n\\ {Ck ∩A0}, i ∈Ck ∩AC\n0\nand therefore αim = 0 which implies that P(|ρk\nm| > |ρk\nim|) = 0.5 for any m ∈{1, . . . , M}. Hence\nP(SM\nm=1 |ρk\nm| > |ρk\nim|) ≥0.5, which inspires the calibration step of the screened points in Ak,ϵ\n0\nto\nrecover the set {Ck ∩A0} accurately.\nTheorem E.1. Assume that for any k = 1, . . . , K, πa,k = o(1) as N →∞. Then, there exists ϵ > 0\n47\n"}, {"page": 48, "text": "such that for any k ∈{1, . . . , K}, Ck ∩A0 ⊂Ak,ϵ\n0 .\nProof. Let i ∈Ck ∩A0. Observe that\nP\n M\n[\nm=1\n{|˜ρk\nm| > |ρk\nim|}\n!\n≤\nM\nX\nm=1\nP(|˜ρk\nm| > |ρk\nim|)\n=\nM\nX\nm=1\n\b\n(1 −πa,k)P(|ρk\nm| > |ρk\nim|) + πa,kP(|ρa,k\nm | > |ρk\nim|)\n\t\n≤(1 −πa,k)\nM\nX\nm=1\nP(|ρk\nm| > |ρk\nm,2 + amin|) + πa,kM/2\nwhere the second term follows as for any i ∈Ck ∩A0, ρa,k\nm and ρk\nim are identically distributed and\nfor the first term, observe that for any i ∈Ck ∩A0, P(|ρk\nm| > |ρk\nim|) ≤P(|ρk\nm| > |ρk\nm,2 + amin|) with\nρk\nm,2 being an i.i.d copy of ρk\nm as ρk\nim has the same distribution as ρk\nm,2 + αm and αm ≥amin almost\nsurely. Let ˜ϵ = maxm∈1,...,M\n\b\nP(|ρk\nm| > |ρk\nm,2 + amin|)\n\t\n> 0. Then,\nP\n M\n[\nm=1\n{|˜ρk\nm| > |ρk\nim|}\n!\n≤(1 + M)˜ϵ\nwhich completes the proof by taking ϵ = (1 + M)˜ϵ.\nE.2\nSubroutine Details for Algorithm 2\nSubroutine algorithm 7 (ScreenPotentialOutliers): This routine identifies subjects whose mul-\ntivariate functional principal component (mFPC) scores lie in the extreme tails of their cluster-\nspecific distribution. Given a set of subjects Is and their scores on B mFPC components, we\nfirst build, for each component m, the empirical cumulative distribution function bFm,Is. We then\nextract the lower and upper cutoff points at probabilities α1/(2B) and 1 −α1/(2B), respectively.\nA subject i is flagged as a potential outlier if any of its component scores falls below the lower\ncutoff or above the upper cutoff. All such flagged indices comprise the set Gs, while the remaining\n“clean” subjects form Gc\ns = Is \\ Gs. By focusing on extreme quantiles of each component’s score\n48\n"}, {"page": 49, "text": "distribution, this screening step ensures that only those subjects with unusually large or small\nloadings proceed to the subsequent confirmation stage.\nAlgorithm 7 ScreenPotentialOutliers: Screening for Potential Outliers in a Subject Set\nInput: Set of subject indices Is ⊂ˆC; cluster-specific mFPC scores {ˆρjm, ˆC : j ∈Is, m = 1, . . . , B};\nnumber of components B; screening level α1.\n1: Initialize Gs ←∅.\n▷Potential outliers in Is\n2: for m = 1, . . . , B do\n3:\nLet bFm,Is be the empirical CDF of scores {ˆρjm, ˆC : j ∈Is}.\n4:\nqlo\nm,Is ←bF −1\nm,Is(α1/(2B)); qhi\nm,Is ←bF −1\nm,Is(1 −α1/(2B)).\n▷Empirical quantiles\n5: end for\n6: for each subject i ∈Is do\n7:\nif ∃m ∈{1, . . . , B} s.t. (ˆρim, ˆC < qlo\nm,Is or ˆρim, ˆC > qhi\nm,Is) then\n8:\nAdd i to Gs.\n9:\nend if\n10: end for\n11: Gc\ns ←Is \\ Gs.\n▷Clean held-out set from Is\nOutput: Set of potential outliers Gs; clean held-out set Gc\ns.\nSubroutine algorithm 8 (ConfirmAnomalies): This routine takes as input a candidate set of\nsubjects Itest, a calibration set Icalib, cluster-specific mFPC scores for B components, and a global\nsignificance level α. Its goal is to confirm which candidates exhibit unusually large scores relative\nto the calibration group.\nFor each subject i ∈Itest, we initialize an empty index set Si to record the components in\nwhich i may be anomalous.\nFor each component m = 1, . . . , B, we compute an empirical p-\nvalue by counting how many calibration scores exceed |ˆρim|, adding one to both numerator and\ndenominator to guard against zero counts. We then compare this p-value to the adjusted threshold\nα/B; if p < α/B, component m is flagged and added to Si. After all components are tested, any\nsubject with Si ̸= ∅is declared a confirmed anomaly, and the pair (i, Si) is appended to the output\nset Aconfirmed.\nThe subroutine returns Aconfirmed, the list of all subjects whose functional scores deviate sig-\nnificantly from the calibration distribution, together with the specific components in which each\ndeviation occurs.\n49\n"}, {"page": 50, "text": "Algorithm 8 ConfirmAnomalies: Confirming Anomalous Subjects using Calibration\nInput: Set of potential outlier indices Itest; calibration set Icalib; cluster-specific mFPC scores\n{ˆρjm, ˆC : j ∈Itest ∪Icalib, m = 1, . . . , B}; number of components B; significance level α.\n1: Initialize Aconfirmed ←∅.\n2: for each subject i ∈Itest do\n3:\nSi ←∅.\n▷Captures outlying FPC components for subject i\n4:\nfor m = 1, . . . , B do\n5:\npemp\nim ←\n1+#{j∈Icalib:|ˆρjm, ˆ\nC|≥|ˆρim, ˆ\nC|}\n1+|Icalib|\n.\n▷Compute empirical p-value\n6:\nif pemp\nim < α/B then\n▷Bonferroni correction; other testing methods (e.g., BH)\napplicable\n7:\nAdd m to Si.\n8:\nend if\n9:\nend for\n10:\nif Si ̸= ∅then\n11:\nAdd (i, Si) to Aconfirmed.\n12:\nend if\n13: end for\nOutput:\nSet of confirmed anomalous subjects from Itest with their deviating components\nAconfirmed = {(i, Si)}.\nE.3\nSubroutine Details for Algorithm 3\nSubroutine algorithm 9 (ComputeWindowDeviations): This routine prepares the baseline deviation\nmeasures that will later be used to identify time-localized anomalies.\nIt accepts as input the\nobserved trajectories of all calibration subjects, the estimated cluster-level mean functions, and a\npredefined collection of time windows.\nFor each window, the algorithm first computes the average value of the cluster mean over that\ninterval. It then, for each calibration subject, calculates the subject’s own average observation in\nthe same interval and measures the deviation as the maximum absolute difference between the\nsubject’s window average and the cluster mean average. Each deviation score is recorded in a\nlookup table indexed by subject and window.\nOn completion, the subroutine outputs two sets of results: (1) the window-specific cluster aver-\nages and (2) the matrix of deviation scores for every calibration subject across all windows. These\nprecomputed quantities serve as the reference distribution when testing whether any subject’s\nwindowed behavior departs significantly from the cohort norm.\nSubroutine algorithm 10 (IdentifyAnomalousWindows): This routine takes the set of subjects\n50\n"}, {"page": 51, "text": "Algorithm 9 ComputeWindowDeviations: Precompute Window Deviations for Calibration\nInput: Data {Y j(Tjk) : j ∈Gc\n1 ∪Gc\n2} for calibration subjects; cluster means {ˆµ(d)\nˆC (t)}; time\nwindows {(aw, bw]}W\nw=1.\n1: Initialize map Dcalib ←∅for storing D(w)\nj\nvalues.\n2: Initialize map MC ←∅for storing ¯µ(w)\nˆC\nvalues.\n3: for each window w = 1, . . . , W do\n4:\n¯µ(w)\nˆC\n←(avg(ˆµ(1)\nˆC (t) in (aw, bw]), . . . , avg(ˆµ(p)\nˆC (t) in (aw, bw])).\n5:\nStore ¯µ(w)\nˆC\nin MC indexed by w.\n6:\nfor each subject j ∈Gc\n1 ∪Gc\n2 do\n7:\n¯e(w)\nj\n←average of {Y j(Tjk) where Tjk ∈(aw, bw]}.\n8:\nD(w)\nj\n←∥¯e(w)\nj\n−¯µ(w)\nˆC ∥∞.\n9:\nStore D(w)\nj\nin Dcalib indexed by (j, w).\n10:\nend for\n11: end for\nOutput:\nWindowed cluster means {¯µ(w)\nˆC }W\nw=1 (via map MC); Calibration deviation scores\n{D(w)\nj\n}j∈Gc\n1∪Gc\n2,w=1,...,W (via map Dcalib).\nalready confirmed as anomalous, their observed trajectories, the precomputed windowed cluster\nmeans, and the calibration deviation scores from the two held-out groups. For each flagged subject,\nwe choose the calibration group that does not include that subject. We then break the subject’s\ntrajectory into W contiguous time windows.\nWithin each window, we compute the subject’s\naverage observation and measure its maximum absolute difference from the corresponding cluster\nmean.\nThis difference is compared against the calibration scores for that window to form an\nempirical p-value—namely, the fraction of calibration subjects whose deviation equals or exceeds\nthe subject’s own, with a small offset in numerator and denominator to avoid zero counts. After\napplying a Bonferroni correction across all W windows, any window with p < α/W is marked as\nanomalous. The subroutine outputs the final list of subjects together with the specific windows in\nwhich each one deviates significantly from the cohort norm.\nF\nSupplemental details for Dynamic Keyword Profiling\nTo extract intent keywords from anomalous reviews, we develop a dynamic and scalable algorithm\nthat leverages a large language model (LLM) to identify and update user intent lists over time.\n51\n"}, {"page": 52, "text": "Algorithm 10 IdentifyAnomalousWindows: Identify Anomalous Windows for Subjects\nInput: Anomalous subjects A(1); data {Y i(Tij) : i s.t. (i, _) ∈A(1)}; windowed cluster means\n{¯µ(w)\nˆC }; calibration deviation scores {D(w)\nj\n}; calibration split info I1, I2, Gc\n1, Gc\n2; time windows\n{(aw, bw]}W\nw=1; significance level α.\n1: Initialize A(2) ←∅.\n2: for each subject i such that (i, _) ∈A(1) do\n3:\nDetermine Gc\nnull: if i ∈I1, Gc\nnull ←Gc\n2; else Gc\nnull ←Gc\n1.\n4:\nInitialize Wi ←∅.\n▷Anomalous windows for subject i\n5:\nfor each window w = 1, . . . , W do\n6:\n¯e(w)\ni\n←average of {Y i(Tij) where Tij ∈(aw, bw]}.\n7:\nD(w)\ni\n←∥¯e(w)\ni\n−¯µ(w)\nˆC ∥∞.\n8:\np(w)\ni\n←\n1+#{j∈Gc\nnull:D(w)\nj\n≥D(w)\ni\n}\n1+|Gc\nnull|\n.\n▷Uses precomputed D(w)\nj\n9:\nif p(w)\ni\n< α/W then\n▷Bonferroni correction\n10:\nAdd w to Wi.\n11:\nend if\n12:\nend for\n13:\nif Wi ̸= ∅then\n14:\nAdd (i, Wi) to A(2).\n15:\nend if\n16: end for\nOutput: Set A(2) = {(i, Wi) : i ∈A(1), Wi ̸= ∅}.\nThe goal is to cluster semantically similar review content into concise intent phrases, even when\nexpressed using varied vocabulary and writing styles.\nEach user j ∈U is associated with a set of anomalous reviews Sj, where each review may\ninclude a descriptive title.\nAt any time t, we maintain a user-specific, time-ordered sequence\nof extracted intents I(t−)\nj\nrepresenting the intents derived from user j’s prior reviews. For each\nnew review r(t)\nj , the algorithm compares its content to the top k prior intents for user j, denoted\nK(t−)\nj\n= Topk(I(t−)\nj\n), as well as the top k intents across all other users, K(t−)\n−j\n= S\nl̸=j K(t−)\nl\n.\nThe combined set {I(t−)\nj\n, K(t−)\n−j , r(t)\nj } is fed into an LLM. If the LLM determines that the review\nmatches an existing intent from either the user’s own history or from others, it is assigned to that\nintent. Otherwise, the model generates a new intent phrase tailored to the content of the review,\nwhich is then appended to I(t)\nj . This process is outlined formally below.\n52\n"}, {"page": 53, "text": "Algorithm 11 Intent Keyword Extraction from Anomalous Reviews\nInput:\n• Users U and their anomalous reviews Sj.\n• For each user j: prior intent list I(t−)\nj\n.\n• For each time t: new review r(t)\nj .\n• Top k intents from user j: K(t−)\nj\n.\n• Top k intents from others: K(t−)\n−j .\n1: for each user j ∈U do\n2:\nfor each time t with anomalous review r(t)\nj\ndo\n3:\nRetrieve I(t−)\nj\n4:\nCompute K(t−)\nj\n= Topk(I(t−)\nj\n)\n5:\nCompute K(t−)\n−j\n= S\nl̸=j K(t−)\nl\n6:\nQuery LLM with {I(t−)\nj\n, K(t−)\n−j , r(t)\nj }\n7:\nif LLM matches r(t)\nj\nto an intent i∗∈I(t−)\nj\n∪K(t−)\n−j\nthen\n8:\nAssign r(t)\nj\nto i∗\n9:\nelse\n10:\nGenerate new intent inew = LLM(r(t)\nj )\n11:\nUpdate I(t)\nj\n←I(t−)\nj\n∪{inew}\n12:\nend if\n13:\nUpdate K(t)\nj\n= Topk(I(t)\nj )\n14:\nend for\n15: end for\nOutput:\n• Updated time-ordered intent lists Ij for each user.\n• Top k intent keywords Kj for each user.\n• Mapping from reviews to associated intent keywords.\nTo illustrate the types of intents extracted and their temporal distribution, Table 6 summa-\nrizes keyword frequencies by cluster across four time windows. Clusters reflect coherent intent\nthemes—such as product quality, value for money, or fit—and demonstrate the algorithm’s abil-\nity to maintain semantic consistency while adapting to newly emerging patterns. See Table 6 in\nSection A for details.\n53\n"}, {"page": 54, "text": "G\nEmotion scoring and validation\nEmotion labels are generated for each Amazon review using GPT-4-32k according to Plutchik’s\nwheel (see Prompt 2). To validate our embedding, we selected 30 reviews and had humans anno-\ntate each on the 24 primary and opposing petals. We then applied the same GPT prompt (Prompt\n2) to these examples and compared its output to the human consensus. The exact-match accu-\nracy—defined as the fraction of reviews where GPT’s predicted emotion agreed with the expert\nlabel—was 19/33 (57.6%).\nListing 2: Prompt for Emotion Category Classification\n--- CONTEXT ---\nYou are an expert at honestly classifying emotion from text. This task involves\nanalyzing user reviews of products, each consisting of a review_title and\nreview_text. The objective is to identify the predominant emotion expressed based on\na structured framework based on 8 main emotions, each with three levels of\nintensity, leading to a total of 24 distinct emotional categories.\n--- INSTRUCTIONS ---\nCarefully read each user review, including both the review_title and review_text. Based\non the emotional cues present,\nuse the structured framework of 8 primary emotions, described below, along with their\nrespective intensities, to guide your analysis,\nand classify the review into one of the 24 distinct emotional categories.\n1. Joy: Indicates happiness or pleasure derived from product satisfaction.\n- Serenity (Mild)\n- Joy (Moderate)\n- Ecstasy (Intense)\n2. Trust: Suggests reliability or confidence in the product.\n- Acceptance (Mild)\n54\n"}, {"page": 55, "text": "- Trust (Moderate)\n- Admiration (Intense)\n3. Fear: Shows worry or concern about the product or its effects.\n- Apprehension (Mild)\n- Fear (Moderate)\n- Terror (Intense)\n4. Surprise: Reflects astonishment or unexpected reactions towards the product.\n- Distraction (Mild)\n- Surprise (Moderate)\n- Amazement (Intense)\n5. Sadness: Reveals disappointment or sorrow due to unmet product expectations.\n- Pensiveness (Mild)\n- Sadness (Moderate)\n- Grief (Intense)\n6. Disgust: Demonstrates revulsion or strong disapproval of the product.\n- Boredom (Mild)\n- Disgust (Moderate)\n- Loathing (Intense)\n7. Anger: Exhibits frustration or anger towards the product or service.\n- Annoyance (Mild)\n- Anger (Moderate)\n- Rage (Intense)\n8. Anticipation: Expresses hopeful expectation or eagerness about the product.\n- Interest (Mild)\n55\n"}, {"page": 56, "text": "- Anticipation (Moderate)\n- Vigilance (Intense)\n--- TASK ---\nFor each user review_title and review_text, follow the instructions to categorize the\nemotion expressed in the review\ninto one of the 24 emotion categories. Ensure your classification is presented as a\nsingle word in Python string format.\nBelow we present the table of human annotations and GPT-predicted Plutchik dimensions for a\nset of representative Amazon reviews. Experts labeled each review according to one of the twenty\nfour possible emotion categories as per the eight primary petals of Plutchik’s Wheel (four opposing\nemotion pairs), and we then applied our GPT-4 prompt (Listing 2) to generate a corresponding\nprediction. Table 8 showcases these results, highlighting both concordant and discordant cases,\nwhich form the basis for our exact-match accuracy metric reported in Section 4.1.\nTable 8: Comparison of human-annotated vs. GPT-predicted Plutchik dimensions.\nTitle\nReview text\nEmotion (human)\nEmotion (GPT)\nPerfect\nFit\nfor Polaris Product as advertised, and\nit fit perfectly in my Polaris ATV.\njoy\nJoy\nGood\nfilter\nwrench\nWorks great for Mercedes V6 Diesel en-\ngine oil change. Matter of fact, can’t\nchange the oil filter without it.\nBit\npricey for what it is, but works well.\nacceptance/submission\nTrust\n(continued on next page)\n56\n"}, {"page": 57, "text": "Title\nReview text\nEmotion (human)\nEmotion (GPT)\nSame\nas\nname\nbrand\nbut\ncost\nless\nGreat aftermarket and non-name-brand\ninsect screen for your RV furnace ex-\nhaust.\nSame product that comes in\nfancy packaging, but less expensive. I\nwould buy this again.\njoy\nTrust\n100%\nlambs\nwool\nProduct does an OK job. Not sure if\nit’s that much better than a synthetic\ncleaning tool for an RV. Clearly not\nsure about value received for dollars\nspent. Will have to use more as time\nprogresses. Get the smaller unit as the\nbigger one is probably way too hard.\napprehension\nApprehension\nEasy\nto\nuse\nEasy to see waste water from holding\ntanks so you know when they are empty\nand clean, mainly black water tank.\njoy\nTrust\nGood\nvalue\nDecently made and works ok.\nacceptance\nAcceptance\nWell\nmade\nproduct\nReally happy with this purchase to use\nwith our motorhome.\nJust the right\nheight at lowest level and has many\nmore adjustments if needed for uneven\nground. It is well made, all the welds\nare clean and even, and it has non-slip\ntape attached. I would recommend.\njoy\nJoy\n(continued on next page)\n57\n"}, {"page": 58, "text": "Title\nReview text\nEmotion (human)\nEmotion (GPT)\nGreat\ncustomer\nservice\nPurchased this to use for my mo-\ntorhome and the 22.5′′ tires that are\nrunning at 100 psi. The unit is quiet,\nwell made, with the exception of the\nplastic screw-on cap for the filter as it is\ncast plastic. I did have a problem with\nthe air chuck that came.\ntrust\nTrust\nSimple to\ninstall\nEasy to install and setup, once you\nwatch the videos made by TST. We\nuse this in our Class A motorhome\nand towed vehicle. The color display/-\nmonitor is nice and easy to see and\nread.\nTST provides both a movable\ndash mount or window suction mount.\nadmiration\nTrust\nDoes the\nJob\nJust a gas can that has been engineered\nto prevent dummies from spilling gas.\nacceptance\nAcceptance\nOEM\noil\nfilter\nYou’d\nthink\nMercedes\nwould\nhave\nmoved to a canister style filter rather\nthan the old school paper. But it works.\nUsed on a MBS Sprinter Class C mo-\ntorhome.\nacceptance\nAcceptance\nOEM Hy-\ndraulic\nOil\nUsed as a backup for my HWH hy-\ndraulic\nsystem\non\nmy\nmotorhome.\nGood, OEM product.\ntrust\nTrust\n(continued on next page)\n58\n"}, {"page": 59, "text": "Title\nReview text\nEmotion (human)\nEmotion (GPT)\nOEM\npart\nGreat price on OEM part for my car.\nWorks great. Very happy with part.\njoy\nJoy\nGood\nreplace-\nment\nitem\nGreat price on air filter for car. Half the\nprice that shop wanted for air filter.\njoy\nJoy\nAppear\nwellmade\nHaving just received these, and con-\nfirmed they do fit my 22.5′′ tires on\nmy class A, I would have to say that\nthese appear to be a well made product.\nThey are easy to install and I appreci-\nate the retainer strap that goes behind\nthe tire to secure the cover.\ntrust\nTrust\nGenuine\nparts\nBasic Onan generator filter.\nacceptance\nAcceptance\nDo\nthe\njob, at a\ncost\nThese crush washers work for the oil\ndrain plug on my Subaru but was sur-\nprised at the cost for two.\nacceptance\nSurprise\n(continued on next page)\n59\n"}, {"page": 60, "text": "Title\nReview text\nEmotion (human)\nEmotion (GPT)\nSimple to\nuse\nSeems an accurate device as compared\nto the Milton gauge I have been using.\nOn my motorhome, this fits the valve\nstems better than the Milton, thereby\nnot losing air pressure while reading tire\npressure. Overall, happy with the de-\nvice.\njoy\nJoy\nIt’s a fuse\nPriced well and hope I don’t have to use\nit.\nacceptance\nAnticipation\nGood\nfit\nfor gener-\nator\nAppears well made and machined. Will\nfit an Onan 8000 KW diesel generator\nin our motorhome.\nFor the price, it\nshould also come with the plastic cover\nto keep debris out of the drain hole.\nacceptance\nAcceptance\nGood\npurchase\nPerfect fit for Onan QD8000 generator\nand priced well.\njoy\nTrust\nDecent\nWax\nIt’s a wax, it works.\nacceptance\nAcceptance\nWorks\nOK\nDecent wax for boats.\nacceptance\nAcceptance\nGood size\nWorks\nwell\nfor\nDiesel\nengine\noil\nchanges.\njoy\nTrust\n(continued on next page)\n60\n"}, {"page": 61, "text": "Title\nReview text\nEmotion (human)\nEmotion (GPT)\nGood\ntowels\nWork well for use on motorhome win-\ndows or waxing.\njoy\nAcceptance\nDecent\nwash nut\nDoes an OK job of washing vehicles\nwithout abrasive issues.\nacceptance\nAcceptance\nDirect\nReplace-\nment\nThis light fixture is a direct replacement\nfor my lights in my motorhome closets\nand storage bays.\nacceptance\nAcceptance\nAbsolutely\nJunk! Do\nnot buy\nReceived this today and went to put it\non my 3/8′′ extension for an oil filter\nchange. The machining is pretty, but\nmeasurements are so poor I cannot get\nit on the extension to use.\nAbsolute\njunk! I should have paid more attention\nto the negative review.\nrage\nRage\nOEM Fil-\nter\nAlways want to use the OEM filter on\nmy PSD.\ntrust\nTrust\nSimple\nInstalla-\ntion\nEasy to install, and should be a breeze\nfor cleaner drains. The only downside\nI can see is that it will take longer to\ndrain and will require staying under the\ntruck to get to second container for final\ndrain. I installed this on an F350 with\na 6.4 L PSD.\ninterest\nTrust\n(continued on next page)\n61\n"}, {"page": 62, "text": "Title\nReview text\nEmotion (human)\nEmotion (GPT)\nGood\nProduct\nfor\nRV\nuse\nProduct appears to be well made if\nbrass and stainless steel. Just using it\nnow for the first time and it is hold-\ning water pressure consistently at the\ncampsite.\nMakes me wonder why I\nwaited so long to upgrade my old, in-\nline regulator, which will now be.\njoy\nTrust\nSimplifies\noil change\nSimple to install. Does take longer to\ndrain, as it is a smaller opening the oil\ndrains through. I use an adapter and\ntube to drain into 1 gallon jugs, so no\nreal cleanup involved.\nacceptance\nTrust\nReliable\nproduct\nWorks well to ensure toilet seal remains\npliable.\nacceptance\nTrust\nH\nLLM usage\nIn preparing this manuscript, we employed large language model (LLM) solely as an assistive tool\nto aid in polishing and refining the writing. Specifically, LLM was used to improve the grammar,\nclarity, and readability of the text.\n62\n"}, {"page": 63, "text": "I\nTable of Notations\nAdditionally Table 9 compiles all key symbols and their definitions for easy reference. Each entry\nlists the mathematical notation, its interpretation in the context of our customer journey analysis,\nand the section where it is first introduced.\nThis comprehensive glossary ensures clarity and\nconsistency across the various methodological components described in this paper.\n63\n"}, {"page": 64, "text": "Table 9: Table for a complete list of symbols and their meanings as used in this paper.\nNotation\nInterpretation\nLocation defined\ni ∈{1, . . . , N}\nSubject index, total of N users\nSec. 3\nNi\nNumber of observations for subject i\nSec. 3\nTij\nObservation time of the jth record for user i\nSec. 3\nKi(Tij)\nRaw text transcript at time Tij\nSec. 3\nΦ: X →Rp\nEmbedding map from text to p-dim numeric vector\nSec. 3\nY i(Tij)\np-variate embedding at time Tij\nSec. 3\nˆµ(d)(t), ˆϕ(d)\nk (t), ˆξ(d)\nik\nUnivariate mean, eigenfunctions, and scores\nAlg. 4\nˆΞ ∈RN×M\nStacked univariate FPC scores\nAlg. 1\nˆλm, ˆvm\nEigenvalues and eigenvectors of ˆCΞ\nAlg. 1\nˆψ(d)\nm (t)\nMultivariate eigenfunction, component m, dimension d\nAlg. 1\nˆρim\nmFPCA score for user i, component m\nAlg. 1\nb\nX(d)\ni,Cℓ(t)\nReconstructed cluster specific p-variate trajectory\nSec.. D.2\nˆCℓ\nCluster ℓvia K-means\nSec. D\nI1, I2\nRandom half-splits for screening\nAlg. 2\nα1, α\nScreening and final significance levels\nAlg. 2\nA(1)\nConfirmed anomalies w.r.t. cohort\nAlg. 2\n(aw, bw]\nFixed time window w\nAlg. 3\nD(w)\nj\nDeviation score for user j in window w\nAlg. 9\nA(2)\nConfirmed anomalous windows\nAlg. 3\n∆\nMinimum separation between true cluster centroids\nApp. D (Thm. D.1)\nπk = Nk/N\nProportion of cluster Ck\nApp. D (Thm. D.1)\nπmin = mink πk\nSmallest cluster proportion\nApp. D (Thm. D.1)\nNa, N ⋆\nTotal anomalies, total non-anomalous\nApp. D (Thm. D.1)\nrk\nNormalized SNR for cluster k\nApp. D (Thm. D.1)\nΛs\nMax. normalized centroid error at iteration s\nApp. D (Thm. D.1)\nαmax\nMax. anomaly-effect norm\nApp. D (Thm. D.1)\nA0\nTrue anomalous subject set\nSec. 3\nρk\nim\nCluster-k mFPCA score, component m\nApp. E\nLk\nm\nNull distribution of ρk\nim in cluster k\nApp. E (Thm. E.1)\n˜ρk\nm\nBootstrap draw from Lk\nm\nApp. E (Thm. E.1)\nAk,ϵ\n0\nScreened candidates in cluster k at level ϵ\nApp. E (Thm. E.1)\nIs, Gs\nScreening split and potential outliers set\nAlg. 7\npemp\nim\nEmpirical p-value for subject i, component m\nAlg. 8\nWi\nAnomalous time-window set for subject i\nAlg. 10\n¯µ(w)\nCluster mean averaged over window w\nAlg. 9\nα/W\nWindow-wise Bonferroni threshold\nAlg. 10\n64\n"}]}