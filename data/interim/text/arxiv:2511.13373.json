{"doc_id": "arxiv:2511.13373", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13373.pdf", "meta": {"doc_id": "arxiv:2511.13373", "source": "arxiv", "arxiv_id": "2511.13373", "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs", "authors": ["Prakrit Timilsina", "Anuj Nepal", "Rajan Kadel", "Robin Doss"], "published": "2025-11-17T13:47:27Z", "updated": "2025-11-17T13:47:27Z", "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13373v1", "url_pdf": "https://arxiv.org/pdf/2511.13373.pdf", "meta_path": "data/raw/arxiv/meta/2511.13373.json", "sha256": "e480aec75ad667b612e43f7042d7207bca0c217feb3085268bbd5e41f73b855b", "status": "ok", "fetched_at": "2026-02-18T02:26:50.034734+00:00"}, "pages": [{"page": 1, "text": "A Novel Hierarchical Integration Method for\nEfficient Model Merging in Medical LLMs\nPrakrit Timilsina\nThink for Tech\nKathmandu, Nepal\nprakrittimilsina@thinkfortech.com\nAnuj Nepal\nDeakin Cyber Research and Innovation Centre\nDeakin University / Universal Higher Education\nMelbourne, Australia\nanepal@deakin.edu.au\nRajan Kadel\nSchool of IT and Engineering\nMelbourne Institute of Technology\nMelbourne, Australia\nrkadel@mit.edu.au\nRobin Doss\nDeakin Cyber Research and Innovation Centre\nDeakin University\nGeelong, Australia\nrobin.doss@deakin.edu.au\nThis work has been submitted to the IEEE for possible publication.\nAbstract—Medical Large Language Models (LLMs) face sig-\nnificant challenges in distributed healthcare, including consol-\nidating specialized domain knowledge across institutions while\nmaintaining privacy, reducing computational overhead, and\npreventing catastrophic forgetting during model updates.This\npaper presents a systematic evaluation of six parameter-space\nmerging techniques applied to two architecturally compatible\nmedical LLMs derived from the Mistral-7B base model. We\nintroduce a novel hierarchical method that combines selective\nOptimal Transport (OT) alignment for attention layers with\ncosine similarity-weighted interpolation, designed to address per-\nmutation variance while minimizing computational overhead for\nedge deployment scenarios. Our study evaluates Task Arithmetic,\nLinear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our\nHierarchical approach across five medical benchmarks. Results\ndemonstrate that architecturally compatible models benefit sig-\nnificantly from simple averaging methods, with Task Arithmetic\nachieving 45.80% accuracy on MedQA, outperforming complex\npruning-based approaches. These findings offer critical insights\nfor the deployment of distributed medical AI in resource-\nconstrained IoT environments, where computational efficiency\nand model compatibility are paramount. Our work establishes\nthat for architecturally compatible models, simple averaging\nprovides a robust and computationally efficient baseline for\nknowledge consolidation, offering a pragmatic path forward for\nscalable medical AI systems.\nIndex Terms—Model Merging, Medical AI, Large Language\nModels, Parameter Efficiency, Domain Adaptation, Healthcare\nInformatics, Biomedical Computing, IoT Healthcare.\nI. INTRODUCTION\nLarge Language Models (LLMs) have transformed the field\nof Natural Language Processing (NLP), exhibiting exceptional\nperformance capabilities across diverse computational tasks.\nRecent developments emphasize efficiency, specialization, and\ndomain adaptation, enabling LLMs to support complex tasks\nin sectors such as healthcare, finance, and law.\nIn the medical domain, this shift toward specialization is\nespecially vital, as LLMs demonstrate potential in automating\nCorresponding author: anepal@deakin.edu.au\nclinical documentation, synthesizing biomedical literature, and\nsupporting diagnostic decision-making [1], [2]. In distributed\nhealthcare networks enabled by IoT infrastructure, these ca-\npabilities become even more critical as medical data and\nexpertise are increasingly fragmented across edge devices and\ninstitutional boundaries. However, deploying LLMs in clinical\nsettings presents challenges due to the need for linguistic\nsophistication, in-depth clinical knowledge, regulatory com-\npliance, and strict safety protocols.\nBuilding and training comprehensive medical LLMs from\nscratch is computationally expensive and requires massive\ndomain-specific datasets, while covering the full range of med-\nical specialties often exceeds the resources of most healthcare\norganizations [3]. To address this, researchers typically adapt\ngeneral-purpose LLMs to healthcare via fine-tuning. Notable\nefforts include Google’s Med-PaLM [4] and the open-source\nBioMistral [5], which demonstrate competitive performance on\nmedical Question Answering (QA) and reasoning benchmarks.\nHowever, these models are typically fine-tuned independently\nfor each task or specialty, leading to fragmented capabilities\nand duplicated effort.\nAn alternative strategy is model merging, which involves\ncombining two or more pre-trained models into a unified\nsystem without additional gradient-based training. This ap-\nproach enables consolidation of diverse medical expertise\nwhile reducing retraining costs and avoiding catastrophic for-\ngetting inherent in sequential fine-tuning [6]. This approach is\nparticularly relevant for IoT-enabled healthcare, where band-\nwidth and compute constraints make retraining or ensembling\nimpractical.\nPrior research has explored merging strategies ranging from\nsimple weight averaging, known as “model soups” [7], to\nmore advanced methods based on task-vector arithmetic [8],\nFisher information weighting [9], and interference reduction\n[10]. However, their effectiveness in specialized domains,\nsuch as medicine, particularly when models share a common\narchitectural foundation, remains underexplored.\narXiv:2511.13373v1  [cs.LG]  17 Nov 2025\n"}, {"page": 2, "text": "We hypothesize that simple merging methods can outper-\nform complex strategies when applied to architecturally com-\npatible medical LLMs. The key contributions of this research\ninclude:\n• We perform a systematic comparative analysis of six\nmodel merging techniques on compatible medical LLMs,\ndemonstrating that simple averaging methods consis-\ntently outperform complex approaches across five medi-\ncal benchmarks.\n• We introduce a novel Hierarchical Cosine-OT-LERP\nmethod that integrates task-vector similarity with se-\nlective attention head alignment, achieving competitive\nperformance while addressing permutation variance.\n• We provide practical deployment guidelines for medical\nAI practitioners, showing that architectural compatibility\nis the primary determinant of merging success, with Task\nArithmetic achieving 45.80% accuracy on MedQA.\nII. RELATED WORK\nThis section reviews prior research in two key areas: the\ndevelopment of specialized LLMs for the medical domain and\nthe evolution of parameter-space model merging techniques.\nA. Medical LLM Development\nThe development of specialized LLMs for healthcare has\naccelerated in recent years, driven by the growing availability\nof medical corpora and the success of general-purpose LLMs.\nEarly work established that general LLMs, such as GPT-4, en-\ncode substantial clinical knowledge [4]. The application of AI\nin healthcare has expanded across various medical specialties,\nwith particular attention to ethical considerations in sensitive\ndomains such as mental health and psychiatry [11]. Building\non this, models such as Med-PaLM and BioMistral [5] have\nbeen created by fine-tuning or continually pre-training large\nmodels on domain-specific medical datasets. Other notable\nlarge clinical models include GatorTron, trained on over 82\nbillion words from clinical and biomedical texts, and Clinical\nCamel, a LLaMA-2-based model that demonstrates strong\nperformance on medical benchmarks.\nTo\nreduce\nthe\ncomputational\nburden\nof\nfull\nfine-\ntuning, parameter-efficient fine-tuning (PEFT) techniques have\nemerged. Methods including Low-Rank Adaptation (LoRA)\n[12] and more recent variants like QLoRA [13] and AdaLoRA\n[14] have also been used to adapt LLMs with fewer trainable\nparameters [15]. Despite these advances, these approaches\ncan introduce compatibility challenges when merging models,\nmotivating the exploration of alternative strategies such as\nmodel merging.\nB. Model Merging Techniques\nParameter merging of pre-trained models combines spe-\ncialized capabilities without retraining. Foundational methods\ninclude Linear Averaging (“model soups”) [7] and “task\nvector” arithmetic [8]. More recent work addresses parame-\nter interference: TIES-Merging and DARE use sparsity and\nsign consensus to resolve conflicts [10], [16], while DELLA\nemploys adaptive pruning [17]. While most evaluations focus\non general NLP, our work provides a needed comparison of\ncompatible medical domain models.\nOur work differs from prior approaches in three key aspects:\n(1) focus on the specialized medical domain rather than\ngeneral NLP tasks, (2) systematic evaluation of architecturally\ncompatible models sharing identical base initialization, and\n(3) introduction of a hybrid method combining task-vector\nsimilarity weighting with selective optimal transport alignment\nspecifically designed for attention mechanisms in transformer\narchitectures.\nC. Evaluation in Specialized Domains\nEvaluating medical LLMs requires standardized bench-\nmarks that assess both clinical knowledge and reasoning. We\nuse five well-established tasks in our evaluation:\n• MedQA [19]: A multiple-choice QA dataset modeled on\nmedical board exams.\n• PubMedQA [20]: A QA dataset based on biomedical\nresearch abstracts.\n• MMLU (Professional Medicine) [21]: The medical sub-\nset of the Massive Multitask Language Understanding\nbenchmark.\n• MedMCQA [22]: An extensive multiple-choice question-\nanswering dataset derived from Indian medical admission\nexaminations.\n• HellaSwag [23]: Included to assess whether domain-\nspecific merging leads to degradation (catastrophic for-\ngetting) of general commonsense reasoning.\nD. Alignment in Model Merging\nWhen merging two neural networks, a hidden challenge\narises from permutation variance: even if two models perform\nthe same function, their internal neurons or attention heads\nmay be arranged differently across models. Directly averaging\nweights can mismatch equivalent components and degrade\nperformance.\nTo address this, previous researches apply alignment tech-\nniques before merging as follows:\n• Permutation-based alignment (e.g., Git Re-Basin) finds\na discrete permutation matrix P that reorders neurons\n(or heads) in model B to best match model A, allowing\nbetter-weighted averaging.\n• Optimal Transport (OT) treats neurons (e.g., attention\nheads) as distributions and computes a soft matching to\nmaximize similarity, typically using cosine similarity.\nOur hierarchical hybrid approach builds on this foundation\nby combining layer-wise interpolation weights derived from\nthe cosine similarity of task vectors, with selective OT align-\nment applied only to multi-head attention components. This\nhybrid strategy offers a middle path—retaining precise align-\nment where it’s most impactful while avoiding the overhead\nof full OT on all layers.\n"}, {"page": 3, "text": "TABLE I\nCOMPARATIVE OVERVIEW OF MODEL MERGING STUDIES\nStudy / Method\nModel Compatibility\nPrimary Technique\nEvaluation Domain(s)\nKey Contribution(s)\nModel Soups [7]\nSame Base, Diff. Seeds\nLinear Averaging\nGeneral Computer Vi-\nsion (CV) & NLP\nImproves model robustness and generalization.\nTask Arithmetic [8]\nSame Base Model\nVector\nArithmetic\n(Addi-\ntion/Negation)\nGeneral NLP & Vision\nEnables compositional editing of model skills\nby manipulating task vectors.\nDARE-TIES [10]\nSame Base Model\nSparsity + Sign Consensus\nGeneral NLP (T5)\nReduces interference when merging many task\nvectors by resolving sign conflicts.\nDARE [16]\nSame Base Model\nRandom Pruning + Rescal-\ning\nGeneral NLP (Llama)\nProvides a simpler and effective alternative\nsparsity method to DARE-TIES.\nDELLA [17]\nSame Base Model\nAdaptive Pruning\nGeneral NLP (Llama-2)\nImproves on DARE by using adaptive density\nfor pruning based on layer similarity.\nGit Re-Basin [18]\nDifferent\nArchitectures\nPossible\nPermutation-based\nAlignment\nGeneral NLP & Vision\nAligns functionally similar neurons before av-\neraging, enabling deeper merges.\nProposed Paper\nSame Base Model (Full\nFine-Tunes)\nComparative (6 Methods)\nSpecialized\n(Medical\nQA)\nShows simple averaging outperforms com-\nplex merges for compatible models in a\nspecialized domain.\nE. AI and IoT in Distributed Healthcare\nIn smart healthcare systems, AI models are often trained\non distributed data from IoMT sensors and edge devices [24].\nFederated Learning (FL) enables collaborative training without\ncentralizing sensitive data, but still requires strong privacy\nsafeguards against leakage through model updates [25]. Model\nmerging faces similar risks, where secure aggregation is es-\nsential to prevent data exposure or poisoning when combining\ninstitutional models. Deploying large models on constrained\nedge devices adds further challenges [26], underscoring the\nneed for efficient consolidation methods. Our approach pro-\nvides a lightweight merging strategy that integrates medical\nexpertise while remaining practical for edge deployment.\nIII. METHODOLOGY\nWe present a rigorous and reproducible methodology for\ncomparing model merging techniques, encompassing model\nselection, algorithm implementation, the formulation of our\nproposed method, and a standardized evaluation protocol. Our\nmethodology follows a systematic three-stage approach as\nillustrated in Fig. 1. The process begins with model selection\nand compatibility assessment, proceeds through various merg-\ning algorithms, and concludes with standardized evaluation\nprotocols. This structured pipeline ensures reproducibility and\nenables fair comparison across different merging techniques.\nA. Model Selection and Compatibility Assessment\nModel compatibility is a critical prerequisite for successful\nparameter space merging between the models. This includes\nidentical base architectures, layer shapes, and initialization\nschemes. As shown in the top layer of Fig. 1, our selection\nprocess ensures architectural compatibility between parent\nmodels before merging. We selected two medical fine-tuned\nLLMs based on three key criteria: (1) shared base architecture\n(Mistral-7B-Instruct-v0.1 [27]), (2) complementary medical\nspecializations, and (3) availability as complete parameter\ncheckpoints. Initial experiments merging thoroughly fine-tuned\nmodels with adapter-only models confirmed this necessity, as\nincompatible architectures resulted in significant performance\ndegradation. The selected compatible models are:\n• Parent A (BioMistral): Specialized on biomedical liter-\nature and research [5].\n• Parent\nB\n(TachyHealth):\nFine-tuned\nfor\nmedical\nquestion-answering tasks [28].\nBoth models were verified to contain complete parameter sets\nthrough checkpoint analysis.\nB. Merging Algorithms\nThe middle layer of Fig. 1 illustrates the six implemented\nmerging techniques, including the proposed one, called Hier-\narchical Cosine-OT-LERP Method, each representing a differ-\nent approach to resolving parameter conflicts during integra-\ntion. The six implemented merging techniques are summarized\nin Table II. These methods represent different approaches\nto parameter space integration. These methods were selected\nfor their relevance in recent literature and their diversity in\nhandling parameter conflicts. We also attempted to implement\nother popular methods, such as SLERP and SCE; however,\nthese failed in our environment due to persistent configuration\nerrors and loading incompatibilities, respectively, highlighting\nthe practical challenges of using varied merging tools.\nMathematical Formulation: The core mathematical foun-\ndations underlying these merging techniques can be expressed\nas follows. Let θA and θB be the parameter vectors of two\nfine-tuned models derived from a common base model with\nparameters θbase.\nLinear Average: The parameters are merged via simple\ninterpolation:\nθmerged = (1 −α)θA + αθB\n(1)\n"}, {"page": 4, "text": "Fig. 1. Three-stage workflow architecture for healthcare model merging.\nTABLE II\nIMPLEMENTED PARAMETER-SPACE MERGING METHODS\nMethod\nApproach\nLinear Average\nWeighted interpolation of parameters\nTask Arithmetic\nArithmetic operations on task vectors\nDARE-TIES\nSparsity (density=0.6) + Sign Consensus\nDELLA-Merging\nAdaptive Pruning (density=0.6, ϵ = 0.05)\nModel\nBreadcrumbs\nDual-Threshold Pruning (density=0.9, γ\n=\n0.01)\nHierarchical\nLayer-wise OT-based alignment of attention\nheads guided by task vector similarity\nTask Arithmetic: This method operates on task vectors\n(∆), which represent the changes from the base model. The\nmerge is then:\nθmerged = θbase + α∆A + (1 −α)∆B\n(2)\nwhere ∆A = θA −θbase and ∆B = θB −θbase. The symbols\nare defined in Table III.\nTABLE III\nMATHEMATICAL NOTATION\nSymbol\nDefinition\nθA, θB\nParameter vectors of parent models\nθbase\nBase model parameters\n∆A, ∆B\nTask vectors (fine-tuning deltas)\nα\nGlobal interpolation weight ∈[0, 1]\nwl\nLayer-wise similarity weight for layer l\nHierarchical\nCosine-OT-LERP\nMethod: To create a\nmerged model that intelligently combines the specialized\nknowledge of the parent models, we propose a hierarchical\nCosine-OT-LERP method. This technique operates on a layer-\nby-layer basis, using a multi-stage process designed to align\nfunctionally similar components before interpolating them\nbased on their similarity of change.\nThe process begins by computing the delta vectors (also\nknown as “task vectors”) for each parent model relative to\nthe common base model. The cosine similarity of these delta\nvectors for each layer l is then used to determine a dynamic\ninterpolation weight wl, as defined in Equation 3. This weight\nreflects how similarly the two parent models diverged from\nthe base for a given layer.\nwl = max\n\u0012\n0,\n∆A,l · ∆B,l\n∥∆A,l∥2∥∆B,l∥2\n\u0013\n(3)\nwhere ∆A,l = θA,l −θbase,l and ∆B,l = θB,l −θbase,l are the\ndelta vectors for layer l of models A and B, respectively.\nFor the crucial attention projection layers (queries, keys,\nvalues, and output), a simple interpolation can be suboptimal\ndue to permutation variance, where functionally equivalent\nheads may exist at different indices. To mitigate this, we\nemploy OT [29] to find a permutation that aligns the attention\nheads of parent model B with those of parent model A based\non maximal cosine similarity. The aligned heads are then\ncombined using linear interpolation with the computed weight\nwl. Non-attention layers, which are less susceptible to such\npermutation variance, are combined directly using the same\nlayer-wise weight.\nThe\ncomplete\nprocedure\nis\nformalized\nin\nAlgo-\nrithms 1 and 2. This approach ensures that we align\n"}, {"page": 5, "text": "functionally similar subspaces within attention layers before\nperforming a similarity-weighted merge, thereby preserving\nspecialized\nknowledge\nwhile\nminimizing\ndestructive\ninterference between the parent models.\nAlgorithm 1 Hierarchical Cosine-OT-LERP Model Merging\nRequire: Base model θbase, Parent models θA, θB\nEnsure: Merged model θmerged\n1: θmerged ←copy(θbase)\n2: for each layer l in model do\n3:\npA,l ←θA[l], pB,l ←θB[l], pbase,l ←θbase[l]\n4:\n∆A,l ←pA,l −pbase,l\n▷Compute task vectors\n5:\n∆B,l ←pB,l −pbase,l\n6:\nwl ←max\n\u0010\n0,\n∆A,l·∆B,l\n∥∆A,l∥2∥∆B,l∥2\n\u0011\n▷Cosine similarity\nweight\n7:\nif l is attention projection layer then\n8:\npaligned\nB,l\n←ALIGNHEADS(pA,l, pB,l)\n9:\nθmerged[l] ←(1 −wl)pA,l + wlpaligned\nB,l\n10:\nelse\n11:\nθmerged[l] ←(1 −wl)pA,l + wlpB,l\n12: return θmerged\nAlgorithm 2 Attention Head Alignment Function\n1: function ALIGNHEADS(PA, PB)\n2:\nReshape PA, PB into head matrices {hA,i}, {hB,j}\n3:\nfor i = 1 to H do\n4:\nfor j = 1 to H do\n5:\nCij ←1 −\nhA,i·hB,j\n∥hA,i∥2∥hB,j∥2 ▷Cost matrix entry\n6:\nπ ←LINEARSUMASSIGNMENT(C)\n▷Optimal\nTransport (OT) solution\n7:\nP aligned\nB\n←PERMUTE(PB, π)\n8:\nreturn P aligned\nB\nC. Computational Complexity Analysis\nThe computational complexity varies significantly across\nmerging methods:\n• Linear Average & Task Arithmetic: O(P) where P is\nthe number of parameters, requiring only element-wise\noperations.\n• DARE-TIES & DELLA: O(P log P) due to per-layer\nmagnitude-based pruning operations, where sorting is\nperformed within each layer for threshold selection.\n• Hierarchical Method: O(L · H3 + P) where H is the\nnumber of attention heads and L is the number of layers.\nThe H3 term arises from the Hungarian algorithm in\nlinear sum assignment for optimal transport, applied to\nQ, K, V, O projections per layer.\n• Model Breadcrumbs: O(P log P) for dual-threshold\npruning requiring percentile computation across param-\neter magnitudes.\nSimple methods offer significant computational advantages,\nrequiring 10-100x fewer operations than alignment-based ap-\nproaches.\nD. Evaluation Protocol\nThe bottom layer of Fig. 1 depicts the evaluation proto-\ncol. All models were evaluated using the Language Model\nEvaluation Harness [30]. The evaluation was conducted on\nan NVIDIA A100 GPU with 40GB of VRAM, ensuring\nsufficient memory for high-fidelity, full-precision testing.\nWe used native bfloat16 precision with 5-shot prompt-\ning across the five benchmarks, executed with the follow-\ning command-line flags: --model hf --num_fewshot\n5 --batch_size auto. For the relevant merge tech-\nniques, key hyperparameters were set to standard or recom-\nmended values: DARE-TIES used a density of 0.6, and\nlinear methods used a balanced weight of α = 0.5. The end-\nto-end experimental workflow is depicted in Fig 1.\nIV. RESULTS\nThis section presents the empirical outcomes of our compar-\native study. We analyze performance across five benchmarks,\ncompare merged models relative to the base model, and\nconclude with a profile analysis of top-performing methods.\nA. Hyperparameter Sensitivity Analysis\nOur evaluation used recommended default hyperparameters:\nDARE-TIES (density=0.6), DELLA (density=0.6, ϵ = 0.05),\nand Breadcrumbs (density=0.9, γ = 0.01). The significant\nunderperformance of DARE-TIES suggests these defaults may\nbe suboptimal for highly compatible medical models. Prelimi-\nnary experiments with higher density values (0.8-0.95) showed\nimproved performance, indicating that aggressive pruning de-\nstroys valuable medical knowledge.\nB. Primary Performance Analysis\nThe evaluation across five medical and general reason-\ning benchmarks revealed significant performance differences\namong the merging techniques. The detailed results, including\naccuracy and standard error for each model, are presented in\nTable IV.\nThe most significant finding is the strong performance of\nsimple merging techniques. Task Arithmetic [8] and Linear\nAveraging [7] consistently emerged as the top-performing\nmethods, surpassing both parent models on key medical QA\ntasks and even exceeding the base model on several metrics.\nTo summarize the overall performance, Figure 2 ranks\neach model by its average accuracy across all five evalu-\nated benchmarks. This visualization clearly shows that Task\nArithmetic and Linear Averaging achieve the highest overall\nscores, while DARE-TIES [16] performs the worst among the\nsuccessful merges. Our proposed Hierarchical method places\ncompetitively within the top group but does not surpass the\nsimpler averaging techniques.\nTask Arithmetic achieved the highest scores on MedQA\n(45.80%) and PubMedQA (77.20%), while Linear Averaging\nachieved the highest score on HellaSwag (80.57%). These re-\nsults contrast sharply with the performance of Uniform DARE-\nTIES (density = 0.6), which significantly underperformed.\nFor example, Task Arithmetic’s MedQA score represents\n"}, {"page": 6, "text": "TABLE IV\nPERFORMANCE COMPARISON ACROSS MEDICAL BENCHMARKS (5-SHOT, BFLOAT16 PRECISION, ACCURACY % ± STDERR %)\nModel\nMedQA\nPubMedQA\nMMLU Prof. Med.\nMedMCQA\nHellaSwag\nBase (Mistral) [27]\n42.97 ± 1.39\n75.80 ± 1.92\n59.56 ± 2.98\n42.60 ± 0.76\n75.12 ± 0.43\nParent A (BioMistral) [5]\n44.93 ± 1.39\n73.60 ± 1.97\n54.41 ± 3.03\n44.01 ± 0.77\n78.50 ± 0.41\nParent B (TachyHealth) [28]\n42.89 ± 1.39\n74.00 ± 1.96\n53.31 ± 3.03\n42.31 ± 0.76\n77.17 ± 0.42\nTask Arithmetic [8]\n45.80 ± 1.40\n77.20 ± 1.88\n56.99 ± 3.01\n46.00 ± 0.77\n80.48 ± 0.40\nLinear Average [7]\n45.80 ± 1.40\n77.00 ± 1.88\n56.99 ± 3.01\n46.00 ± 0.77\n80.57 ± 0.39\nHierarchical (Proposed)\n45.40 ± 1.40\n73.40 ± 1.98\n54.78 ± 3.02\n43.89 ± 0.77\n78.46 ± 0.41\nModel Breadcrumbs\n41.95 ± 1.38\n73.60 ± 1.97\n50.74 ± 3.04\n42.67 ± 0.76\n78.03 ± 0.41\nDELLA-Merging [17]\n41.79 ± 1.38\n75.00 ± 1.94\n47.79 ± 3.03\n41.36 ± 0.76\n76.59 ± 0.42\nDARE-TIES [16]\n36.45 ± 1.35\n68.60 ± 2.08\n44.85 ± 3.02\n34.64 ± 0.74\n70.09 ± 0.46\nFig. 2.\nAverage accuracy ranking of all models across the five evaluated\nbenchmarks.\nan improvement of +9.35 percentage points over Uniform\nDARE-TIES (36.45%), and its MedMCQA score (46.00%)\nwas +11.36 points higher than DARE-TIES (34.64%). This\nhighlights the substantial performance penalty incurred by the\ndefault pruning strategy in this context.\nWhile simple averaging proved effective overall, it is no-\ntable that no merge method surpassed the original base model’s\nperformance on MMLU Professional Medicine (59.56%). This\nsuggests that while simple averaging effectively consolidates\nknowledge from compatible models, it does not universally\ncreate a “super-model” that exceeds all baselines on every task.\nC. Performance Relative to Base Model\nLightweight merge methods can preserve and enhance med-\nical QA performance. As shown in Fig. 3, Task Arithmetic\nand Linear Averaging improved accuracy by 2.83 points on\nMedQA and 3.40 points on MedMCQA versus the unmerged\nMistral baseline, demonstrating that simple merging boosts\naccuracy without additional fine-tuning.\nFig. 3. Accuracy Point Difference vs. Base Model.\nD. Model Performance Profiles\nFig 4 illustrates the normalized performance profiles of\nkey models across all benchmarks. Task Arithmetic demon-\nstrated the most balanced profile, maintaining competitive\nscores across all tasks and showing particular strength on the\nQA benchmarks. The proposed Hierarchical method shows a\nwell-rounded profile, closely tracking the best parent model\n(Parent A) on HellaSwag and outperforming the Base model\non MedQA. However, its performance on PubMedQA and\nMMLU Prof. Med. did not reach Task Arithmetic levels,\nsuggesting that while OT alignment successfully prevented\ncatastrophic interference, the overall merge was more con-\nservative and did not unlock the same synergy as simple\naveraging for these benchmarks.\nV. DISCUSSION AND IMPLICATIONS\nOur results show that lightweight methods, such as Linear\nAveraging and Task Arithmetic, consistently outperform com-\nplex algorithms across medical benchmarks. We analyze the\nreasons and implications for the deployment of medical AI.\nA. Critical Analysis: Why Complex Methods Underperformed\nThe central question arising from our results is: Why did\nLinear Averaging and Task Arithmetic, the simplest methods,\noutperform more sophisticated techniques, such as DARE-\nTIES, DELLA, and our Hierarchical-OT merge? We posit\nseveral interconnected reasons:\nHigh Model Compatibility and Low Parameter Conflict:\nThe primary reason for this success is likely the high degree of\n"}, {"page": 7, "text": "MedQA\nPubMedQA\nMMLU_PM\nMedMCQA\nHellaSwag\n25%\n50%\n75%\n100%\nModel Comparison (Normalized Scores)\nModels\nBase (Mistral)\nParent A (BioMistral)\nTask Arithmetic\nHierarchical (Proposed)\nFig. 4. Normalized performance profile comparing key models (Best=1.0 on\neach axis)\ncompatibility between the parent models. Both BioMistral\nand TachyHealth are fine-tuned versions of the same\nMistral-7B-Instruct-v0.1 base, trained on related\nmedical domains, and occupy closely related, well-behaved\nregions within the same loss basin. This architectural and do-\nmain alignment likely minimized parameter conflicts, allowing\nsimple averaging to consolidate knowledge effectively. The\nadvanced methods, designed primarily to resolve high degrees\nof conflict from diverse task vectors, may be unnecessary or\neven counterproductive.\nDetrimental Effects of Aggressive Pruning (DARE-\nTIES): The significant performance degradation of Uniform\nDARE-TIES (density=0.6) is a critical finding. This method\nprunes 40% of the delta parameters before merging. In high-\nquality fine-tunes, small-magnitude changes in the delta are not\nnoise but rather crucial, subtle adjustments. Aggressively prun-\ning these parameters likely destroyed valuable information,\ncausing the model to regress even below the performance of\nits parents. This suggests that sparsity-based methods require\ncareful tuning of the density hyperparameter, potentially\nsetting it much higher (e.g., > 0.9) when merging compatible,\nhigh-performing models.\nLoss Basin Proximity and Regularization Effects: Models\nfine-tuned from the same base typically reside within the\nsame or adjacent loss basins. Simple averaging performs im-\nplicit regularization by combining beneficial adaptations while\nmitigating overfitting artifacts specific to individual training\nruns. Complex merging techniques that attempt to resolve\nnon-existent conflicts may inadvertently disrupt this natural\nregularization effect.\nB. Implications for Medical AI\nThese findings have practical implications for medical LLM\ndeployment. For architecturally compatible models, simple\naveraging methods are remarkably effective, with sophisti-\ncated conflict-resolution mechanisms providing limited benefit\ndue to minimal parameter interference. Unlike inference-time\nensembles that double computational costs, merged models\nmaintain single-model efficiency while consolidating knowl-\nedge, making them ideal for resource-constrained clinical en-\nvironments. Future gains could be achieved by merging models\nfrom truly disjoint medical specialties (e.g., radiology and\ngenomics), where advanced alignment strategies may become\nessential. This underscores the importance of tailoring merging\nstrategies to model compatibility and diversity.\nC. Practical Guidelines for Practitioners\nBased on our findings, we recommend:\n1) Verify Model Compatibility: Confirm parent models\nare full-checkpoint models with identical architectures,\nnot PEFT/LoRA adapters. Attempting to merge incom-\npatible formats is a common pitfall.\n2) Start Simple: Begin with Linear Averaging [7] or Task\nArithmetic [8] for compatible models. These methods\nare computationally efficient and provide robust base-\nlines.\n3) Tune Hyperparameters Carefully: Advanced methods\nlike DARE-TIES [16] are sensitive to hyperparameters.\nFor high-quality fine-tunes, consider densities above 0.9\nto avoid destructive pruning.\nVI. LIMITATIONS AND FUTURE WORK\nThis evaluation highlights the limitations of generalizability\nin model merging for medical AI, while identifying key\ndirections for the safe integration of healthcare.\nA. Limitations\nOur evaluation focused exclusively on Mistral-7B fine-tuned\nmodels with high architectural and domain compatibility. The\nclear superiority of simple averaging methods may not gener-\nalize to more heterogeneous merging scenarios. The findings\nare limited by:\n1) Architectural Homogeneity: All models share identi-\ncal architecture. Merging different model families (e.g.,\nLlama 2, Falcon) would introduce incompatible weight\nmatrices requiring sophisticated alignment techniques.\n2) Domain Proximity: Both parents operate within the\nmedical domain. Merging truly disjoint domains (e.g.,\nmedicine and finance) would likely introduce significant\nparameter conflicts, potentially favoring advanced inter-\nference mitigation techniques like DARE-TIES [16] over\nsimple averaging.\n3) Model Format: Our study was restricted to full-\ncheckpoint models. Integrating PEFT/LoRA modules\nrequires different techniques beyond direct parameter-\nspace merging.\n"}, {"page": 8, "text": "B. Future Directions\nFuture research should prioritize three key areas. Scalability\nand generalization efforts should replicate findings across\ndifferent architectures (Llama 2/3, Qwen) and scales (13B,\n70B parameters) with automated hyperparameter optimization.\nSafety-aware merging strategies must incorporate safety objec-\ntives through alignment-aware loss functions and lightweight\ncompatibility metrics (such as weight orthogonality and acti-\nvation correlation) to predict model “mergeability”. Finally,\nclinical deployment evaluation should assess merged mod-\nels in healthcare IoT environments, including edge devices\nand federated learning networks, with real-time performance\nmonitoring to ensure practical viability in resource-constrained\nmedical settings.\nVII. CONCLUSION\nThis study demonstrated that simple weight averaging\nconsistently outperformed more sophisticated parameter-space\nmerging algorithms when applied to architecturally compatible\nmedical LLMs. Our evaluation across five medical bench-\nmarks showed that basic averaging achieved up to 45.80%\naccuracy on MedQA, compared to 36.45% for more complex\napproaches such as DARE-TIES.\nOur work paved the way for paradigm shifts in medical\nLLM deployment by enabling healthcare organizations to\nmerge lightweight models from disparate departments and\nlocations. This approach democratized access to advanced\nmedical knowledge while underscoring the need for rigor-\nous safety validation protocols. By favoring strategic model\nmerging over monolithic training, the medical AI community\ncan redefine how clinical knowledge is encoded, shared, and\ndeployed worldwide.\nREFERENCES\n[1] K.\nZ.\net\nal.,\n“Revolutionizing\nhealth\ncare:\nThe\ntransformative\nimpact of large language models in medicine,” Journal of Medical\nInternet Research, vol. 27, p. e59069, 2025. [Online]. Available:\nhttps://doi.org/10.2196/59069\n[2] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez,\nT. F. Tan, and D. S. W. Ting, “Large language models in medicine,”\nNat. Med., vol. 29, no. 8, pp. 1930–1940, 2023. [Online]. Available:\nhttps://doi.org/10.1038/s41591-023-02448-8\n[3] K. H. et al., “A survey of large language models for healthcare:\nFrom\ndata,\ntechnology,\nand\napplications\nto\naccountability\nand\nethics,” Inf. Fusion, vol. 118, p. 102963, 2025. [Online]. Available:\nhttps://doi.org/10.1016/j.inffus.2025.102963\n[4] K. S. et al., “Large language models encode clinical knowledge,”\nNature, vol. 620, no. 7972, pp. 172–180, 2023. [Online]. Available:\nhttps://doi.org/10.1038/s41586-023-06291-2\n[5] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and\nR. Dufour, “BioMistral: A collection of open-source pretrained large\nlanguage models for medical domains,” in Findings of the Association\nfor Computational Linguistics: ACL 2024, 2024, pp. 5848–5864.\n[Online]. Available: https://aclanthology.org/2024.findings-acl.348\n[6] R. M. French, “Catastrophic forgetting in connectionist networks,”\nTrends Cogn. Sci., vol. 3, no. 4, pp. 128–135, 1999. [Online]. Available:\nhttps://doi.org/10.1016/S1364-6613(99)01294-2\n[7] M. W. et al., “Model soups: Averaging weights of multiple fine-tuned\nmodels improves accuracy without increasing inference time,” in\nProc. 39th Int. Conf. Mach. Learn. (ICML), ser. Proc. Mach. Learn.\nRes., vol. 162.\nPMLR, 2022, pp. 23 965–23 998. [Online]. Available:\nhttps://proceedings.mlr.press/v162/wortsman22a.html\n[8] G. I. et al., “Editing models with task arithmetic,” in Proc. of the\nInternational Conference on Learning Representations (ICLR).\nIEEE,\n2023.\n[9] M. Matena and C. Raffel, “Merging models with fisher-weighted aver-\naging,” in Proc. 36th Int. Conf. Neural Information Processing Systems\n(NeurIPS).\nNew Orleans, LA, USA: Curran Associates Inc., 2022, pp.\n1287–1300.\n[10] P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal, “TIES-\nMerging: Resolving interference when merging models,” in Advances in\nNeural Information Processing Systems 36 (NeurIPS), 2023. [Online].\nAvailable: https://dl.acm.org/doi/10.5555/3666122.3666432\n[11] U. Poudel, S. Jakhar, P. Mohan, and A. Nepal, “Ai in mental\nhealth: A review of technological advancements and ethical issues\nin psychiatry,” Issues Ment. Health Nurs., 2025. [Online]. Available:\nhttps://www.tandfonline.com/doi/full/10.1080/01612840.2025.2502943\n[12] E. J. H. et al., “LoRA: Low-rank adaptation of large language\nmodels,” in Proc. 10th Int. Conf. Learn. Representations (ICLR),\nVirtual Event, Apr. 2022. [Online]. Available: https://openreview.net/\nforum?id=nZeVKeeFYf9\n[13] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:\nEfficient finetuning of quantized llms,” in Proc. 37th Int. Conf. Neural\nInf. Process. Syst. (NeurIPS), ser. NIPS ’23.\nNew Orleans, LA, USA:\nCurran Associates, Inc., 2023, pp. Art. no. 441, 28 pp.\n[14] Q. Z. et al., “Adaptive budget allocation for parameter-efficient\nfine-tuning,” in Proc. Int. Conf. Learning Representations (ICLR), 2023.\n[Online]. Available: https://openreview.net/pdf?id=lq62uWRJjiY\n[15] N. J. P. et al, “Parameter-efficient fine-tuning of large language models\nusing semantic knowledge tuning,” Sci. Rep., vol. 14, 2024. [Online].\nAvailable: https://www.nature.com/articles/s41598-024-75599-4\n[16] L. Yu, B. Yu, H. Yu, F. Huang, and Y. Li, “Language models are super\nmario: Absorbing abilities from homologous models as a free lunch,” in\nProc. Int. Conf. Mach. Learn. (ICML), 2024.\n[17] P. Huang, J. Koco´n, M. Piasecki, and A. Janz, “DELLA-Merging:\nReducing interference in model merging via magnitude-based sampling,”\narXiv preprint arXiv:2406.11617, 2024.\n[18] S. K. Ainsworth, J. Hayase, and S. Srinivasa, “Git re-basin: Merging\nmodels modulo permutation symmetries,” in Proc. Int. Conf. Learning\nRepresentations (ICLR), 2023.\n[19] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits,\n“What disease does this patient have? a large-scale open domain question\nanswering dataset from medical exams,” Appl. Sci., vol. 11, no. 14, p.\n6421, 2021. [Online]. Available: https://doi.org/10.3390/app11146421\n[20] Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu, “PubMedQA: A\ndataset for biomedical research question answering,” in Proc. Conf.\nEmpirical Methods in Nat. Lang. Process. (EMNLP) & Int. Joint\nConf. Nat. Lang. Process. (IJCNLP), 2019, pp. 2567–2577. [Online].\nAvailable: https://doi.org/10.18653/v1/D19-1259\n[21] D. H. et al., “Measuring massive multitask language understanding,” in\nProc. Int. Conf. Learn. Representations (ICLR). OpenReview.net, 2021.\n[Online]. Available: https://openreview.net/forum?id=d7KBjmI3GmQ\n[22] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “MedMCQA: A\nlarge-scale multi-subject multi-choice dataset for medical domain\nquestion answering,” in Proc. Conf. Health, Inference, and Learn.\n(CHIL), vol. 174.\nPMLR, 2022, pp. 248–260. [Online]. Available:\nhttps://proceedings.mlr.press/v174/pal22a.html\n[23] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “HellaSwag:\nCan a machine really finish your sentence?” in Proc. 57th Annu.\nMeeting Assoc. Comput. Linguistics (ACL), 2019, pp. 4791–4800.\n[Online]. Available: https://aclanthology.org/P19-1472/\n[24] S. A. et al., “IoT-Based Healthcare-Monitoring System towards Improv-\ning Quality of Life: A Review,” Healthcare, vol. 10, no. 10, p. 1993,\nOct. 2022.\n[25] S. R. Abbas, Z. Abbas, A. Zahir, and S. W. Lee, “Federated learning\nin smart healthcare: A comprehensive review on privacy, security,\nand predictive analytics with iot integration,” Healthcare, vol. 12,\nno. 24, p. 2587, 2024. [Online]. Available: https://doi.org/10.3390/\nhealthcare12242587\n[26] S. Hadish, V. Bojkovi´c, M. Aloqaily, and M. Guizani, “Language models\nat the edge: A survey on techniques, challenges, and applications,” in\n2024 2nd International Conference on Foundation and Large Language\nModels (FLLM), 2024, pp. 262–271.\n[27] A. Q. Jiang et al., “Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\n"}, {"page": 9, "text": "[28] TachyHealthResearch,\n“Mistral-7B-Medical-\nFinetune QA Choices,”\nhttps://huggingface.co/TachyHealthResearch/\nMistral-7B-Medical-Finetune QA Choices, 2025.\n[29] S. P. Singh and M. Jaggi, “Model fusion via optimal transport,” in\nProc. 33rd Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 33.\nCurran\nAssociates, Inc., 2020. [Online]. Available: https://proceedings.neurips.\ncc/paper/2020/file/fb2697869f56484404c8ceee2985b01d-Paper.pdf\n[30] L. Gao, J. Tow, S. Biderman et al., “A framework for few-shot\nlanguage model evaluation,” Zenodo, Software Release, 2021. [Online].\nAvailable: https://doi.org/10.5281/zenodo.5371629\n"}]}