{"doc_id": "arxiv:2511.17938", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.17938.pdf", "meta": {"doc_id": "arxiv:2511.17938", "source": "arxiv", "arxiv_id": "2511.17938", "title": "SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization", "authors": ["Jianghao Wu", "Yasmeen George", "Jin Ye", "Yicheng Wu", "Daniel F. Schmidt", "Jianfei Cai"], "published": "2025-11-22T06:32:34Z", "updated": "2025-11-22T06:32:34Z", "summary": "Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.17938v1", "url_pdf": "https://arxiv.org/pdf/2511.17938.pdf", "meta_path": "data/raw/arxiv/meta/2511.17938.json", "sha256": "fba7d15e235fe804d745e83d20e828faa8b8c5921dc5b6a06a4a46d18bf659e9", "status": "ok", "fetched_at": "2026-02-18T02:26:30.077729+00:00"}, "pages": [{"page": 1, "text": "SPINE: Token-Selective Test-Time Reinforcement Learning\nwith Entropy-Band Regularization\nJianghao Wu1*, Yasmeen George1, Jin Ye1, Yicheng Wu2, Daniel F. Schmidt1, Jianfei Cai1\n1Monash University\n2Imperial College London\nAbstract\nLarge language models (LLMs) and multimodal LLMs\n(MLLMs) excel at chain-of-thought reasoning but face dis-\ntribution shift at test-time and a lack of verifiable supervi-\nsion. Recent test-time reinforcement learning (TTRL) meth-\nods derive label-free pseudo-rewards from self-consistency\nvoting over sampled trajectories, yet they often collapse:\nthe majority-vote reward prevails, responses shorten, and\nPass@1 declines. We trace this to uniform sequence up-\ndates in which most tokens are low-entropy followers,\nwhile a small high-entropy subset determines the reasoning\nbranches. Thus we propose SPINE, a token-selective test-\ntime reinforcement learning framework that (i) updates only\nforking tokens, the high-entropy branch points identified\nfrom forward-pass statistics, and (ii) applies an entropy-\nband regularizer at those tokens to sustain exploration when\nentropy is too low and to suppress noisy supervision when\nit is too high. SPINE plugs into GRPO-style objectives,\noptionally with a KL anchor, and requires no labels or re-\nward models. Across ten benchmarks spanning multimodal\nVQA, general and expert QA, mathematical reasoning, and\nmedical QA, SPINE consistently improves Pass@1 over\nTTRL while avoiding response-length collapse and yield-\ning more stable training dynamics on both LLM and MLLM\nbackbones.\nThese results indicate that aligning updates\nwith chain-of-thought branch points is a simple and label-\nfree mechanism for stable and effective test-time adapta-\ntion in reasoning models. Code is available at https:\n//github.com/JianghaoWu/SPINE.\n1. Introduction\nLarge-scale foundation models, including both language\nmodels (LLMs) and multimodal large language models\n(MLLMs), exhibit impressive chain-of-thought (CoT) rea-\nsoning across a wide range of general-domain tasks [8,\n14, 48].\nYet real-world deployment faces two persistent\npressures: distribution shift at test-time [11, 31, 59] and\n*Corresponding author. Email: jianghao.wu@monash.edu\nthe scarcity of verifiable supervision [3, 40]. Reinforce-\nment learning with verifiable rewards (RLVR) can unlock\nstronger reasoning [22, 35], but it presupposes dense labels\nor high-quality reward models that many domains lack, e.g.,\nmathematical problem solving [27], clinical decision sup-\nport [42], and scientific QA [12]. These constraints mo-\ntivate improving models directly on unlabeled test inputs\nrather than waiting for new annotations.\nTest-Time Training (TTT) adapts models on incom-\ning unlabeled data, typically via pseudo-labels or self-\nsupervised signals [1, 2, 38, 39].\nHowever, recent ev-\nidence indicates that reinforcement learning generalizes\nmore robustly than supervised fine-tuning (SFT) on rea-\nsoning tasks, where SFT often imitates surface patterns\nrather than improving deductive behavior [28, 52]. Building\non this, Test-Time Reinforcement Learning (TTRL) [67]\nand unsupervised post-training for MLLMs [49] sample\nmultiple reasoning paths and derive pseudo-rewards via\nself-consistency voting (Fig. 1a), yielding substantial gains\nwithout labels or reward models. Nevertheless, in practice\nstandard TTRL quickly develops a characteristic collapse\nmode. As updates proceed, the majority-vote reward keeps\nincreasing while responses become shorter and Pass@1\neventually drops (Fig. 1b, top). This behavior suggests that\nthe policy is optimizing agreement among sampled trajec-\ntories rather than correctness, converging to a small set of\nshort, self-consistent but often incorrect answers. This col-\nlapse stems from learning on noisy pseudo-rewards: uni-\nform sequence updates implicitly treat self-consistency as a\nfaithful surrogate for correctness, thereby exposing a struc-\ntural mismatch between the proxy signal and the true ob-\njective. These failure modes in turn suggest that updating\nall tokens uniformly may be fundamentally misaligned with\nwhere the reasoning actually branches.\nRecent analyses of token-entropy patterns in CoT from\nRLVR reveal a highly skewed distribution: the vast major-\nity of tokens are generated with low entropy, while only a\nsmall minority in the top quantiles (≈20%) exhibit high en-\ntropy (Fig. 1c). The same study [46] shows that these high-\nentropy tokens act as branch points that steer the chain-\nof-thought, whereas low-entropy tokens mainly carry local\narXiv:2511.17938v1  [cs.CL]  22 Nov 2025\n"}, {"page": 2, "text": "(a)\no1\no2\no3\n…\nPseudo\nlabel\nr1\nr2\nr3\n…\nResponses\nRewards\nMajority Voting\nLLM,\nMLLM\nEstimate Advantage, Update Policy\nToken count\nAll token entropy\nTop 2  forking tokens\nTop 2   Token entropy\nToken count\n(b)\nMa ority vote re ard\nResponse length \nPass 1 accuracy\n    \n    \n    \n    \n    \n    \nTTRL\nOurs\n(c)\nerror CoT\nCatastrophic \n forgetting\nfixed CoT\nOverfitting\nforking \ntokens\nflowing \ntokens\nPseudo label = GT\nPseudo labe ≠GT\n(d)\nFigure 1. Motivation of SPINE. (a) TTRL: sample multiple responses, majority vote forms a pseudo-label, then update with GRPO. (b)\nTTRL is unstable with shrinking outputs. (c) Entropy is skewed; the top 20% high-entropy tokens mark forking decisions. (d) SPINE\nupdates only forking tokens and applies an entropy band, stabilizing adaptation and mitigating overfitting and forgetting.\ncontinuity. In our TTRL setting, we leverage this entropy-\nbased view to a qualitatively different regime: label-free\ntest-time RL for reasoning driven by noisy self-consistency\npseudo-rewards. Rather than updating all tokens uniformly,\nwe recast TTRL as exploration in the CoT decision space:\nhigh-entropy positions define a small set of forking to-\nkens that form the reasoning “spine” along which adapta-\ntion should occur, whereas low-entropy follower tokens are\nbetter left unchanged to preserve stable local realizations.\nThis token-selective view turns test-time optimization from\nfull-sequence updates into sparse updates on forking tokens\n(Fig. 1d), concentrating gradients at actual decision points\nto prevent collapse, length shrinkage, and over-dispersion\nof the chain-of-thought under noisy pseudo-rewards.\nWe therefore propose SPINE (Selective Policy Improve-\nments at Nodes of Entropy), a selective test-time rein-\nforcement learning framework. (i) Token-selective updates.\nSPINE identifies forking tokens based on token entropy and\napplies GRPO-style policy updates only at these positions\nwhile preserving low-entropy follower tokens. (ii) Entropy-\nband regularization. It introduces a lightweight band-pass\nobjective at forking tokens that increases entropy when too\nlow to sustain exploration and decreases it when too high to\nsuppress noisy supervision, thus preventing premature col-\nlapse. SPINE reuses forward-pass statistics (log probabili-\nties and entropies) and can include a KL anchor, requiring\nno labels or reward models.\nOur main contributions can be summarized as follows.\n• A token-selective TTRL paradigm that aligns updates\nwith reasoning branch points (forking tokens) rather than\nwith all tokens, preserving flowing segments.\n• A band-pass entropy regularizer at forking tokens that\njointly curbs collapse and over exploration, improving\nboth stability and accuracy under noisy pseudo-rewards.\n• Across ten benchmarks, SPINE consistently improves\nPass@1 over TTRL on both LLM and MLLM backbones,\nwhile delivering more stable and reliable label-free test-\ntime adaptation.\n2. Related Work\n2.1. Reasoning in LLMs and MLLMs\nReasoning in LLMs has been advanced mainly by su-\npervised and self-supervised training that teach chain-of-\nthought (CoT), self-consistency, and reflection [47, 48].\nEarly LLMs internalize stepwise patterns via supervised\nfine-tuning (SFT) on (prompt, trace, answer) triplets, as in\nFlan-PaLM–style mixtures that enable zero-shot CoT [6,\n60].\nThis SFT on CoT paradigm was then adopted by\nmultimodal models. A common recipe first aligns vision\n"}, {"page": 3, "text": "Policy \nModel\nRollout\no2\no1\no4\no5\no3\nMajority\nVoting\nPseudo\nlabel\nBase\nRe ard\no6\no2\no1\no4\no5\no3\no6\nSame Ans er  + Re ard\nDifferent Ans er   /o Re ard\nToken level \npolicy gradient\nforking tokens:\nKeep gradient\nflowing tokens :\nStop gradient \nKeep pseudo training stable:\nKeep chain-of-thought active:\nFigure 2. SPINE pipeline. The model samples responses, majority voting produces a pseudo-label, and rewards are assigned. Gradients\nupdate only forking tokens, while flowing tokens are frozen. An entropy band further stabilizes training and preserves reasoning diversity.\nand language through visual instruction tuning, for ex-\nample, LLaVA and MiniGPT-4, and then performs SFT\non multimodal chain-of-thought data to elicit stronger vi-\nsual reasoning, for example, LLaVA CoT and related sys-\ntems [24, 54, 65]. These methods report consistent gains\nacross visual math and chart understanding benchmarks.\nDespite its effectiveness, SFT behaves like behavior cloning\nof a single reasoning path, which can limit generalization\nand requires costly, high-quality reasoning traces [4, 7, 41].\nThese limitations motivate outcome-based reinforcement\nlearning on tasks with verifiable solutions, where rewards\nare computed automatically by unit tests, checkers, or ver-\nifiers [20, 22]. Within this line, GRPO is a practical ob-\njective for long chain-of-thought: it is critic-free, estimates\nadvantages from groups of rollouts, and yields a stable low-\nvariance signal [35].\nBuilding on GRPO, large reason-\ning models such as DeepSeekMath and the R1 series show\nstrong gains on mathematical reasoning [8, 35], and related\nwork extends outcome-based RL to multimodal settings to\ncouple perception with stepwise reasoning [50, 61]. Over-\nall, reasoning benefits from data-centric SFT to bootstrap\nstepwise behavior and from outcome-based RL to move be-\nyond imitation. However, many approaches still depend on\nreward models or human labels [22, 32], which are costly in\nspecialized domains and hard to maintain after deployment,\nmotivating methods that learn from verifiable signals with\nminimal supervision at test time.\n2.2. Test-Time Scaling\nTest-time scaling increases the compute budget at infer-\nence without updating model parameters. Prior work sug-\ngests that for many reasoning tasks, allocating extra com-\npute at test-time can be more sample efficient than scaling\npretraining compute [18, 26, 36]. Two common forms are\nparallel generation and sequential generation [51]. Paral-\nlel generation draws multiple candidates or decision paths\nand then aggregates them, e.g., self-consistency and best\nof N [5, 30, 37, 47], Monte Carlo Tree Search for discrete\ndecisions [53, 63], or token-level search, such as reward-\nguided sampling [19, 33]. Aggregation may rely on sim-\nple voting or reward models [22, 44, 62]. Sequential gen-\neration allocates more steps to a single response through\nreflection and chain-of-thought prompting [29, 48]. While\nthese strategies improve accuracy, their gains are ultimately\nbounded by the base model and by the cost and latency of\nlarge-scale sampling. Beyond scaling inference-time sam-\npling, test-time training (TTT) updates parameters on unla-\nbeled inputs via pseudo-labels or self-supervision [1, 45].\nTest-Time RL (TTRL) [67] instead uses majority-vote self-\nconsistency as a verifiable reward, with MM-UPT extend-\ning to multimodal models [49]. ETTRL [25] reshapes roll-\nouts and advantages via response-level entropy. Compute\nas Teacher (CaT) remains label-free but introduces an ex-\nternal teacher/judge (e.g., GPT-4o) to synthesize and verify\nanswers [15], thus replacing self-consistency with auxiliary\nmodel feedback. EVOL-RL is likewise label-free yet re-\nlies on an external embedding model to score novelty and\nperforms full-sequence policy updates guided by embed-\nding similarity [64]. In contrast, our method avoids exter-\nnal teachers and embedders and operates at the token level.\nWe update only high-entropy forking tokens and apply an\nentropy band with a masked KL at those positions, isolat-\ning the gains of selective token updates under a matched\nGRPO-style TTRL setup.\n3. Methodology\nSetting and notation.\nWe study label-free test-time re-\ninforcement learning for autoregressive reasoning mod-\nels, including both text-only LLMs and multimodal LLMs\n"}, {"page": 4, "text": "(MLLMs). Given an input x, the policy πθ(y | x) gener-\nates y = (a1, . . . , aT ) autoregressively, where each token\nis sampled as at ∼πθ(· | st) and st denotes the decoder\nstate before emitting token t (i.e., a summary of x and a<t).\nAll subsequent quantities (e.g., rewards, advantages, and\nentropies) are defined on the model’s output distributions\nand do not depend on the input modality.\nAt test-time, the model receives only unlabeled in-\nputs and must improve its reasoning behavior without\nany ground-truth supervision.\nTo this end, we leverage\nreinforcement learning with self-consistency rewards and\ngrouped advantage estimation.\nOur key idea is to per-\nform token-selective test-time reinforcement learning: we\nidentify high-entropy forking tokens, where the chain-of-\nthought branches, and apply an entropy-band constraint that\nsuppresses overly high entropy while preventing collapse,\nthereby stabilizing updates under noisy pseudo-rewards.\nThis selective update mechanism, integrated with self-\nconsistency rewards and the GRPO objective, forms the\ncore of our SPINE.\n3.1. Self-consistency reward and GRPO objective\nWithout ground-truth labels, we derive a verifiable super-\nvision signal through self-consistency. For each input x,\nwe draw N candidate responses {yi}N\ni=1 ∼πθold(· | x) and\naggregate them into a consensus output y⋆(e.g., majority\nvoting over extracted answers). Each sampled response yi\nthen receives a rule-based reward\nri = r(yi, y⋆) ∈[0, 1]\n(1)\nsuch as exact agreement or a task-specific verifiable score\n(e.g., unit tests for code). When noted, we adopt a leave-\none-out variant y⋆\n−i to mitigate self-inclusion bias.\nThis\nself-consistency reward encourages the model to prefer sta-\nble, high-consensus reasoning paths without relying on ex-\nternal supervision.\nTo optimize the policy using these rewards, we adopt\nGrouped Relative Policy Optimization (GRPO), a stable on-\npolicy reinforcement learning algorithm that replaces ex-\nplicit value estimation with group-wise normalized advan-\ntages. Within each group of N samples for the same input x,\nthe standardized advantage for the i-th sample is computed\nas\nˆA i = ri −mean({rj}N\nj=1)\nstd({rj}N\nj=1) + ϵ\n,\ni = 1, . . . , N\n(2)\nand the token-level PPO ratio is\nρ(i)\nt (θ) =\nπθ\n\u0010\na(i)\nt\n| s(i)\nt\n\u0011\nπθold\n\u0010\na(i)\nt\n| s(i)\nt\n\u0011.\n(3)\nThe clipped surrogate objective is then\nℓPPO =min\nh\nρ(i)\nt\nˆA i, clip(ρ(i)\nt , 1−ϵ, 1+ϵ) ˆA ii\n(4)\nwhere the clip operator truncates the ratio to the interval\n[1 −ϵ, 1 + ϵ].\nTo ensure stable adaptation without over-regularizing\nnon-forking positions, we apply a token-level KL anchor\nonly on forking tokens. Concretely, we define a masked,\nsize-normalized KL term\nℓfork\nKL = E(i,t)∈B\n\u0002\nm(i)\nt\nDKL\n\u0000πθ(· | s(i)\nt ) ∥πref(· | s(i)\nt )\n\u0001\u0003\nE(i,t)∈B[ m(i)\nt\n] + ϵ\n,\n(5)\nwhere πref denotes the fixed reference policy, set to the\npre-adaptation base model πθ0 and kept frozen throughout\nadaptation. Here m(i)\nt\n∈{0, 1} masks the top-20% entropy\nforking tokens. This choice preserves flexibility on low-\nentropy, non-branching tokens while anchoring updates at\nhigh-uncertainty forks where policy drift is most likely.\n3.2. Selecting forking tokens by entropy\nReward improvements in reasoning models are mainly con-\ncentrated at branching points of the chain-of-thought. We\ntherefore identify such forking tokens through their token-\nlevel entropy, defined as\nHt = −\nX\nv∈V\nπθ(v | st) log πθ(v | st),\n(6)\nwhich measures the uncertainty of the predictive distribu-\ntion at step t. For each sampled output yi, we compute\n{H(i)\nt }Ti\nt=1 and select the top 20% tokens with the highest\nentropy as the set of forking tokens Si ⊂{1, . . . , Ti}. A\nbinary mask m(i)\nt\n∈{0, 1} marks these positions, and we\nensure at least one token is always selected to avoid degen-\nerate cases. These high-entropy tokens correspond to the\nmodel’s reasoning forks, where adaptation yields the largest\nexpected reward gain.\n3.3. Entropy-band regularization via quantiles\nBuilding on the forking tokens Si defined above, we reg-\nularize their entropy distribution to avoid instability dur-\ning adaptation.\nUniform entropy maximization can lead\nto over-exploration, while entropy collapse prematurely\nprunes reasoning branches. We therefore shape token-level\nuncertainty only at forking points by constraining entropies\nto remain within a per-sample, data-driven band defined by\nfixed quantiles.\nFor each sample i, we collect the entropies of forking\ntokens {H(i)\nt\n: t ∈Si} and compute two fixed quantiles:\nH(i)\nlow = Quantile10%\n\b\nH(i)\nt\n: t ∈Si\n\t\n,\nH(i)\nhigh = Quantile50%\n\b\nH(i)\nt\n: t ∈Si\n\t\n.\n(7)\nThe quantile band adapts automatically to each sample’s en-\ntropy distribution without requiring any task-specific tuning\n"}, {"page": 5, "text": "Table 1. Performance of SPINE on multimodal reasoning and medical QA benchmarks. SPINE consistently improves over TTRL and the\nbase model across both multimodal VQA tasks (MathVision, SLAKE, MedXpertQA-MM) and medical QA (MedQA, PubMedQA).\nName\nMathVision\nSLAKE\nMedXpertQA-MM\nMedQA\nPubMedQA\nAvg\nBase Model\nQwen2.5-VL-3B-Instruct\nQwen3-1.7B\nNo adaptation\n19.65\n26.17\n17.17\n30.40\n68.00\n32.28\nSelf-Consistency\n19.20\n25.84\n19.47\n39.62\n68.42\n34.51\nw/ LMSI\n9.21\n9.05\n22.01\n50.47\n71.20\n32.39\nw/ SEALONG\n10.36\n12.32\n6.51\n49.69\n71.00\n29.98\nw/ TTRL\n22.73\n30.00\n22.61\n51.88\n71.50\n39.74\nw/ SPINE\n27.26\n38.66\n23.92\n55.40\n76.20\n44.29\n∆\n+4.5\n+8.7\n+1.3\n+3.5\n+4.7\n+4.6\nor scale parameters. It provides a simple, robust mechanism\nto prevent entropy collapse while allowing flexible explo-\nration across reasoning paths.\nThe band statistics are treated as constants within each\nupdate; we denote the stop-gradient operator by sg(·) and\nwrite\nH\n(i)\nlow := sg\n\u0000H(i)\nlow\n\u0001\n,\nH\n(i)\nhigh := sg\n\u0000H(i)\nhigh\n\u0001\n.\n(8)\nViolations are penalized by hinge losses:\nℓhigh\nt\n= max\n\u00000, H(i)\nt\n−H\n(i)\nhigh\n\u0001\n,\nℓlow\nt\n= max\n\u00000, H\n(i)\nlow −H(i)\nt\n\u0001\n.\n(9)\nThe overall band regularization aggregates these penal-\nties only over forking tokens:\nRband(θ) = E(i,t)∈B\nh\u0000βℓℓlow\nt\n+ βu ℓhigh\nt\n\u0001\nm(i)\nt\ni\n.\n(10)\nwhere ℓhigh\nt\nand ℓlow\nt\ndenote per-token penalties for exces-\nsive and insufficient entropy respectively, and m(i)\nt\nmasks\nforking tokens.\n3.4. Final objective\nWe optimize the core loss over mini-batches of prompts,\nsamples, and tokens:\nLcore =−E(i,t)∈B\n\u0002\nm(i)\nt\nℓPPO\n\u0003\n+ λKL ℓfork\nKL ,\n(11)\nand add the entropy-band regularizer Rband defined in\nEq. (10), whose strength is controlled by βℓand βu. The\nfinal objective is\nL=Lcore + Rband.\n(12)\nTest-time update loop.\nFor each unlabeled input x: (i)\nsample N responses with the behavior policy πθold; (ii) ag-\ngregate to a consensus y⋆and compute rule-based rewards\n{ri}; (iii) compute grouped, standardized advantages { ˆA i}\nby Eq. (2); (iv) compute token entropies {H(i)\nt }, select fork-\ning tokens (top-20% per sample) to obtain masks m(i)\nt , and\ncompute the per-sample quantile thresholds by Eq. (7); (v)\nupdate θ by minimizing Eq. (12). Repeat the loop for a\nsmall number of iterations over the test split.\n4. Experiments\n4.1. Experimental Setup\nModels\nWe evaluate SPINE on a compact, representative\nsuite spanning multimodal and text-only LLMs, covering\nmodel type (MLLM vs. LLM), specialization (generalist\nvs. math-focused), and scale (1.5–3B). Concretely, we use\nQwen2.5-VL-3B-Instruct for multimodal reasoning [55],\nQwen3-1.7B as a general-purpose text-only LLM [57], and\nQwen2.5-Math-1.5B as a math-specialized LLM [56]. Ex-\nperiments initialize from publicly released checkpoints.\nBenchmarks\nWe evaluate SPINE across four task fami-\nlies. For multimodal VQA, we consider MathVision [43]\nfor diagram-based mathematical reasoning, as well as\nSLAKE [23] and MedXpertQA-MM [66] for clinical im-\nage understanding. For general and expert knowledge QA,\nwe include GPQA [34] and MMLU [9]. For mathematical\nreasoning, we use AIME 2025, AMC, and the MATH-500\nsubset of MATH [10]. For medical text-only QA, we adopt\nMedQA (USMLE) [16] and PubMedQA [17].\nBaselines\nWe use standard TTRL (GRPO with a KL an-\nchor and majority-vote self-consistency) [67] as our pri-\nmary baseline, and additionally report No adaptation and\nSelf-Consistency (majority vote without updates).\nFor\nbroader comparison, we include two self-improvement\nbaselines: LMSI [13], which generates high-confidence\nCoT pseudo-labels via self-consistency and performs su-\npervised fine-tuning; and SEALONG [21], which samples\nmultiple long-context trajectories, scores them via MBR-\nstyle consensus, and fine-tunes on the top outputs.\nImplementation Details and Evaluation Protocol\nWe\nimplement SPINE with GRPO on all benchmarks. Dur-\ning adaptation, for each prompt we sample N=8 rollouts\nwith temperature 0.7 and top-p=0.95, aggregate a pseudo-\nlabel via self-consistency (majority vote), and update the\npolicy using GRPO with an optional KL anchor to the base\n"}, {"page": 6, "text": "Table 2. Performance of SPINE on mathematical, general, and expert reasoning benchmarks. SPINE consistently outperforms both TTRL\nand the base model across mathematical tasks (AIME 2025, AMC, MATH-500) and general or expert benchmarks (GPQA, MMLU).\nName\nAIME 2025\nAMC\nMATH-500\nGPQA\nMMLU\nAvg\nBase Model\nQwen2.5-Math-1.5B\nNo adaptation\n10.00\n28.91\n30.20\n4.06\n-\n18.29\nSelf-Consistency\n3.33\n31.02\n45.37\n6.15\n-\n21.47\nw/ LMSI\n6.67\n26.50\n31.50\n19.19\n-\n20.97\nw/ SEALONG\n6.67\n25.30\n32.60\n18.69\n-\n20.82\nw/ TTRL\n16.67\n49.88\n66.42\n25.38\n-\n39.59\nw/ SPINE\n20.00\n59.03\n77.00\n30.96\n-\n46.75\n∆\n+3.3\n+9.2\n+10.6\n+5.6\n-\n+7.2\nBase Model\nQwen3-1.7B\nNo adaptation\n10.00\n38.55\n71.60\n9.09\n58.16\n37.48\nSelf-Consistency\n10.83\n33.13\n66.07\n8.75\n65.39\n36.83\nw/ LMSI\n16.67\n34.94\n62.40\n18.18\n71.74\n40.78\nw/ SEALONG\n20.00\n40.96\n61.00\n13.64\n69.36\n40.99\nw/ TTRL\n26.67\n53.01\n79.86\n29.94\n71.19\n52.13\nw/ SPINE\n36.67\n61.46\n81.40\n36.04\n72.66\n57.65\n∆\n+10.0\n+8.5\n+1.5\n+6.1\n+1.5\n+5.5\nTable 3. Cross-Task Generalization of SPINE based on Qwen3-\n1.7B. Each model is adapted on one dataset and evaluated across\nall four benchmarks to measure generalization and forgetting.\nTraining\nAIME 2025\nAMC\nMATH-500\nGPQA\nAvg\nQwen3-1.7B\n10.00\n38.55\n71.60\n9.09\n32.31\nAIME 2025\n36.67\n48.19\n78.60\n24.36\n46.96\n∆\n+26.7\n+9.6\n+7.0\n+15.3\n+14.7\nAMC\n20.00\n61.46\n76.40\n22.84\n45.18\n∆\n+10.0\n+22.9\n+4.8\n+13.8\n+12.9\nMATH-500\n23.33\n49.39\n81.40\n18.78\n43.23\n∆\n+13.3\n+10.8\n+9.8\n+9.7\n+10.9\nGPQA\n10.00\n40.96\n70.00\n36.04\n39.25\n∆\n+0.0\n+2.4\n−1.6\n+27.0\n+6.9\nmodel. At evaluation, we use greedy decoding (tempera-\nture 0) and report Pass@1 accuracy (a single greedy out-\nput). Predictions are matched to references after a standard\nnormalization pipeline, case folding, whitespace cleanup,\nUnicode/LaTeX canonicalization (including symbol map-\nping), unit-word removal, mixed-number handling, and al-\ngebraic equivalence checking via sympy when applicable;\nfull rules follow the grade answer implementation. We\nset the maximum output length to 3072 tokens for LLM\ntasks and 2048 tokens for multimodal tasks, and keep sam-\npling filters and stopping criteria identical across meth-\nods. For multimodal models, following MM-UPT [49], we\ndo not freeze the vision tower during training. All runs\nuse a fixed seed for reproducibility and are conducted on\n4×NVIDIA A100 80GB GPUs. For fair comparison, all ex-\nperiments are conducted using the EasyR1 framework [58]\nunder a unified configuration. Except for the hyperparam-\neters specific to our method, all other training settings are\nkept identical across experiments. With matched compute\nbudgets, SPINE exhibits broadly comparable efficiency to\nTTRL.\n4.2. Main Results\nResults on multimodal VQA tasks.\nTable 1 shows that\nTTRL improves the no-adaptation baseline on MathVision,\nSLAKE, and MedXpertQA-MM from 19.65/26.17/17.17\nto 22.73/30.00/22.61.\nBuilding on this, SPINE reaches\n27.26/38.66/23.92, which is +4.5, +8.7, and +1.3 over\nTTRL. In contrast, recent SFT-based methods such as LMSI\nand SEALONG do not exhibit clear improvements and even\nlead to declines on certain benchmarks, highlighting the\nlimited generalization of supervised fine-tuning under un-\nseen multimodal distributions.\nResults on mathematical reasoning tasks.\nTable 2 re-\nports results on three mathematical benchmarks.\nOn\nQwen2.5-Math-1.5B, TTRL improves the no-adaptation\nbaseline from 10.00 to 16.67 on AIME 2025, from 28.91\nto 49.88 on AMC, and from 30.20 to 66.42 on MATH-\n500. SPINE further increases the scores to 20.00, 59.03, and\n77.00, giving additional gains of 3.3, 9.2, and 10.6. A sim-\nilar pattern appears on Qwen3-1.7B, where SPINE reaches\n36.67 on AIME 2025, 61.46 on AMC, and 81.40 on MATH-\n500, improving over TTRL by 10.0, 8.5, and 1.5. In con-\ntrast, SFT-based approaches such as LMSI and SEALONG\nshow mild improvements over the baseline.\n"}, {"page": 7, "text": "Table 4. Ablation study of SPINE. FT: selective update on Forking\nTokens; EB: Entropy-Band regularization. Both modules consis-\ntently improve TTRL across benchmarks.\nMethod\nMathVision\nSLAKE\nGPQA\nAvg\nBase\n19.65\n26.17\n9.09\n18.30\n+TTRL\n22.73\n30.00\n29.94\n27.56\n∆vs. Base\n+3.1\n+3.8\n+20.9\n+9.3\n+TTRL+FT\n25.12\n32.66\n34.51\n30.76\n∆vs. Base\n+5.5\n+6.5\n+25.4\n+12.5\n+TTRL+FT+EB\n27.26\n38.66\n36.04\n33.99\n∆vs. Base\n+7.6\n+12.5\n+27.0\n+15.7\n(a) Low entropy-band\n(c) Forking token ratio\nAccuracy (Pass@1)\n70\n66\n64\n68\n62\n60\n72\n74\n10%\n20% 30%\n40%\n50%\n40%\n50% 60%\n70%\n90% 10%\n20% 30%\n40%\n50%\n(b) High entropy-band\nFigure 3. Hyperparameter sensitivity: (a–b) show the effect of\nvarying the lower and upper quantiles of the entropy band, while\n(c) varying the forking-token ratio on MMLU.\nResults on medical and general reasoning tasks.\nTa-\nble 1 and Table 2 also show the results on medical QA and\ngeneral reasoning benchmarks. On domain-specific medi-\ncal tasks, TTRL improves over the no-adaptation baseline,\nand SPINE further increases accuracy to 55.40 on MedQA\nand 76.20 on PubMedQA, adding 3.5 and 4.7 points over\nTTRL. On general and expert reasoning tasks, SPINE also\nbrings consistent gains. With Qwen3-1.7B, it raises GPQA\nfrom 29.94 to 36.04 and MMLU from 71.19 to 72.66, cor-\nresponding to improvements of 6.1 and 1.5. These results\nindicate that selective token updates with entropy-band reg-\nularization yield more stable and accurate adaptation than\nTTRL.\nSPINE generalizes well beyond the target task.\nTo as-\nsess whether SPINE overfits the adaptation dataset or loses\ngeneralization on unseen domains, we conduct a cross-task\nevaluation on four benchmarks. Table 3 shows that adapt-\ning on AIME 2025 raises the average Pass@1 from 32.31\nto 46.96, with consistent gains on AMC, MATH-500, and\nGPQA. Adapting on AMC and MATH-500 shows a similar\npattern, yielding improvements across tasks with average\ngains of +12.9 and +10.9, respectively. Adapting on GPQA\nleads to a strong in-domain improvement of 27.0 and stable\nresults on AIME 2025 and AMC, with only a small decline\nof 1.6 on MATH-500. These findings show that SPINE gen-\neralizes well across tasks under label-free adaptation.\n(a)Rollout number N\n(b) Max response length\n50\n20\n60\n70\n10\n0\n80\n30\n40\n50\n20\n60\n70\n10\n0\n80\n30\n40\nAccuracy (Pass@1)\nAccuracy (Pass@1)\n4\n6\n8\n12\n16\n24\n1K\n2K\n3K\n4K\n5K\n6K\nFigure 4.\nSensitivity to scaling-related hyperparameters, with\nlarger rollout N and longer response lengths providing consistent\nimprovements on the challenging AIME 2025.\n4.3. Ablation Study\nComponent Analysis.\nTable 4 reports the ablation results\nof SPINE. Starting from TTRL, which improves the base\nmodel by 9.3 on average, adding the Forking Token mod-\nule brings further gains, increasing the average accuracy\nfrom 27.56 to 30.76. Entropy-Band regularization provides\nadditional stability and raises the average to 33.99.\nTo-\ngether, FT and EB achieve the best results, with an over-\nall improvement of 15.7 over the base model and 6.4 over\nTTRL. These findings show that selective token updates and\nentropy-aware regularization reinforce each other and sub-\nstantially enhance adaptation quality.\nHyperparameter Sensitivity Analysis.\nSince SPINE op-\nerates at test time, practical deployments typically cannot\ntune hyperparameters for each input. We therefore adopt a\nsingle configuration across all experiments and assess ro-\nbustness to key hyperparameters. Figure 3 reports results\non MMLU, varying (a) the lower quantile of the entropy\nband (with the upper bound fixed to the median), (b) the up-\nper quantile of the entropy band, and (c) the forking-token\nratio. Performance shows limited variation across settings,\nwith a mild preference for a 0.10 lower quantile, a 0.50 up-\nper quantile, and a forking-token ratio near 20%, confirming\nthe robustness of SPINE under different configurations.\nTo further assess scaling behavior, Figure 4 varies the\nrollout number N and the maximum response length on the\nchallenging AIME 2025. Accuracy improves steadily as\nmore test-time compute is allocated, while avoiding marked\ndegradation under modest compute budgets.\n5. Analysis and Discussions\n5.1. Training Dynamics\nBeyond the failure cases in Fig. 1b, we analyze TTRL’s in-\nstability in Fig. 5. As training proceeds, the majority-vote\nreward of TTRL quickly saturates at 1.0, indicating overfit-\nting to pseudo-consensus and loss of reasoning diversity. In\ncontrast, SPINE shows a slower, bounded increase, main-\ntaining multiple reasoning trajectories. Likewise, TTRL’s\n"}, {"page": 8, "text": "3.0\n2.5\n0.5\n1.0\n1.5\n0.0\n2.0\n100\n200\n300\n400\n500\n100\n200\n300\n400\n500\n0.2\n0.4\n0.6\n0.0\n0.8\n1.0\n100\n200\n300\n400\n500\n0.2\n0.3\n0.4\n0.1\n0.0\n1500\n2000\n2500\n1000\n100\n200\n300\n400\n500\n500\n3000\n(a) Majority-vote reward\n(b) Response length\n(c) Mean token entropy\n(d) Pass@1 accuracy\nFigure 5. Training dynamics comparison between SPINE (blue)\nand TTRL (red) on GPQA. (a) Majority-vote reward, (b) response\nlength, (c) mean token entropy, and (d) Pass@1 accuracy.\nToken entropy\nTok n coun \n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTok n coun \nToken entropy\n(a)\n(b)\n(c)\nTo 20%  ok n \nTo 20%  ok n \nFigure 6. Token entropy analysis before (a) and after (b) adapta-\ntion with SPINE. Panel (c) highlights their semantic differences:\nthe upper word cloud shows high-entropy tokens, while the lower\none corresponds to low-entropy tokens.\nresponse length drops sharply, while SPINE shows only\nmild shortening, suggesting its reasoning chains remain ac-\ntive. Token-level entropy in Fig. 5c shows that TTRL’s un-\ncertainty grows erratically, whereas SPINE keeps it nearly\nsteady, reflecting more stable reasoning. Finally, Fig. 5d\nshows that SPINE achieves consistently higher and more\nstable Pass@1 accuracy as training continues.\n5.2. Entropy Distribution and Forking Tokens\nWe examine token-level entropy before and after adaptation\nin Fig. 6. At initialization (a), most tokens are low-entropy\nwith a small high-entropy tail. After adapting with SPINE\n(b), this shape remains, indicating stable uncertainty with-\nout collapse or over-dispersion. We further visualize the top\n20% high-entropy tokens (c); despite some overlap with fre-\nquent words, they more often correspond to branching cues\nthat steer diverse reasoning, supporting our use of entropy-\nguided updates.\n5.3. Why Does SPINE Work?\nSelf-consistency offers a verifiable test-time reward: draw-\ning multiple rollouts raises the chance that at least one is\ncorrect, and small online updates steer the policy toward\nconsensus without labels. SPINE improves both where and\nhow to update. First, Forking-Token (FT) selective updates\nfocus gradients on decision-critical tokens identified by en-\ntropy and branch structure, so learning targets tokens that\nchange the final answer rather than boilerplate, reducing\nreward overfitting and preserving alternative paths. Sec-\nond, Entropy-Band (EB) regularization keeps token-level\nuncertainty within a target band, preventing low-entropy\ncollapse and high-entropy drift, thereby stabilizing trajec-\ntory distributions. Empirically, these choices yield bounded\nreward growth, milder response-length shrinkage, stable\ntoken-entropy, and higher Pass@1 than uniform TTRL up-\ndates.\n5.4. Limitations and Failure Modes\nAlthough SPINE improves accuracy even when the initial\nmajority-vote accuracy is below 0.5, it can still struggle un-\nder severe domain mismatch where the model’s prior is sys-\ntematically wrong and the consensus is consistently incor-\nrect. In such cases, the pseudo-reward provides little correc-\ntive signal and adaptation may reinforce spurious reason-\ning patterns. Moreover, when the underlying task requires\nstructured or multi-hop reasoning that substantially deviates\nfrom the model’s pretraining distribution, the benefits of se-\nlective updates diminish because the branching structure it-\nself becomes unreliable. Finally, test-time training intro-\nduces additional computational overhead, which may limit\ndeployment in strict latency-constrained environments.\n6. Conclusion\nIn this paper, we introduced SPINE, a selective test-time\nreinforcement learning framework that aligns updates with\ndecision-critical (forking) tokens and stabilizes adaptation\nvia an entropy-band constraint. Built on self-consistency\nrewards and GRPO, SPINE requires no ground-truth labels\nand retains comparable efficiency to standard TTRL under\nmatched compute budgets. Across ten benchmarks span-\nning multimodal VQA, general and expert QA, mathemati-\ncal reasoning, and medical QA, SPINE consistently outper-\nforms TTRL and no-adaptation baselines in practice while\nrobustly avoiding majority-vote saturation, response-length\ncollapse, and entropy drift. Empirically, analyses of train-\ning dynamics and token-entropy distributions indicate that\nSPINE preserves diverse reasoning trajectories and concen-\ntrates learning where it most affects the answer.\nReferences\n[1] Ekin Aky¨urek, Mehul Damani, Linlu Qiu, Han Guo, Yoon\nKim, Andreas, and Jacob. The surprising effectiveness of\ntest-time training for abstract reasoning.\narXiv preprint,\n2024. 1, 3\n[2] Ali Behrouz, Peilin Zhong, Mirrokni, and Vahab.\nTitans:\nLearning to memorize at test time. arXiv preprint, 2024. 1\n[3] Stephen\nCasper,\nXander\nDavies,\nClaudia\nShi,\nThomas Krendl Gilbert, J´er´emy Scheurer, Javier Rando,\nRachel Freedman, Tomasz Korbak, David Lindner, Pedro\n"}, {"page": 9, "text": "Freire, et al.\nOpen problems and fundamental limitations\nof reinforcement learning from human feedback.\narXiv\npreprint arXiv:2307.15217, 2023. 1\n[4] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jian-\nnan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te\nGao, and Wanxiang Che. Towards reasoning era: A survey of\nlong chain-of-thought for reasoning large language models.\narXiv preprint arXiv:2503.09567, 2025. 3\n[5] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao,\nPengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi\nWang, Zhou, and Denny. Universal self-consistency for large\nlanguage model generation. arXiv preprint, 2023. 3\n[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,\nYi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\nDehghani, Siddhartha Brahma, et al.\nScaling instruction-\nfinetuned language models. Journal of Machine Learning\nResearch, 25(70):1–53, 2024. 2\n[7] Dylan J Foster, Adam Block, and Dipendra Misra. Is behav-\nior cloning all you need? understanding horizon in imitation\nlearning. Advances in Neural Information Processing Sys-\ntems, 37:120602–120666, 2024. 3\n[8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, and et al. Deepseek-r1: Incentivizing rea-\nsoning capability in llms via reinforcement learning. arXiv\npreprint, 2025. 1, 3\n[9] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\nsuring massive multitask language understanding.\narXiv\npreprint arXiv:2009.03300, 2020. 5\n[10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, Steinhardt,\nand Jacob. Measuring mathematical problem solving with\nthe math dataset. arXiv preprint, 2021. 5\n[11] Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao\nShuai, Wei Luo, Bin Xiao, Yuanqing Li, and Mingkui Tan.\nTest-time learning for large language models. arXiv preprint\narXiv:2505.20633, 2025. 1\n[12] Ming Hu, Chenglong Ma, Wei Li, Wanghan Xu, Jiamin\nWu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu,\nYingzhou Lu, et al. A survey of scientific large language\nmodels: From data foundations to agent frontiers.\narXiv\npreprint arXiv:2508.21148, 2025. 1\n[13] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi\nWang, Hongkun Yu, and Jiawei Han. Large language mod-\nels can self-improve. In Proceedings of the 2023 conference\non empirical methods in natural language processing, pages\n1051–1068, 2023. 5\n[14] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney, and et al. Openai o1 sys-\ntem card. arXiv preprint, 2024. 1\n[15] Dulhan Jayalath, Shashwat Goel, Thomas Foster, Parag\nJain, Suchin Gururangan, Cheng Zhang, Anirudh Goyal,\nand Alan Schelten.\nCompute as teacher: Turning infer-\nence compute into reference-free supervision. arXiv preprint\narXiv:2509.14234, 2025. 3\n[16] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits.\nWhat disease does this\npatient have? A large-scale open domain question answering\ndataset from medical exams. Applied Sciences, 11(14):6421,\n2021. 5\n[17] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu.\nPubMedQA: A dataset for\nbiomedical research question answering.\narXiv preprint\narXiv:1909.06146, 2019. 5\n[18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 3\n[19] Maxim Khanov, Jirayu Burapacheep, Li, and Yixuan. Args:\nAlignment as reward-guided search. arXiv preprint, 2024. 3\n[20] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi.\nCoderl: Mastering\ncode generation through pretrained models and deep rein-\nforcement learning. Advances in Neural Information Pro-\ncessing Systems, 35:21314–21328, 2022. 3\n[21] Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo\nYu, Yujiu Yang, and Wai Lam.\nLarge language models\ncan self-improve in long-context reasoning. arXiv preprint\narXiv:2411.08147, 2024. 5\n[22] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Ed-\nwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. Let’s verify step by step.\nIn The Twelfth International Conference on Learning Repre-\nsentations, 2023. 1, 3\n[23] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and\nXiao-Ming Wu. Slake: A semantically-labeled knowledge-\nenhanced dataset for medical visual question answering. In\n2021 IEEE 18th international symposium on biomedical\nimaging (ISBI), pages 1650–1654. IEEE, 2021. 5\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36:34892–34916, 2023. 3\n[25] Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang,\nFeiYang Shen, and ShaoGuo Liu.\nEttrl:\nBalancing\nexploration and exploitation in llm test-time reinforce-\nment learning via entropy mechanism.\narXiv preprint\narXiv:2508.11356, 2025. 3\n[26] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li,\nBiqing Qi, Wanli Ouyang, Zhou, and Bowen. Can 1b llm sur-\npass 405b llm? rethinking compute-optimal test-time scal-\ning. arXiv preprint, 2025. 3\n[27] Yixin Liu, Avi Singh, C Daniel Freeman, John D Co-\nReyes, and Peter J Liu.\nImproving large language model\nfine-tuning for solving math problems.\narXiv preprint\narXiv:2310.10047, 2023. 1\n[28] Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang,\nYing Sun, and Jun Wang. Othink-mr1: Stimulating multi-\nmodal generalized reasoning capabilities via dynamic rein-\nforcement learning. arXiv preprint arXiv:2503.16081, 2025.\n1\n[29] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-\nlinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\n"}, {"page": 10, "text": "Shrimai Prabhumoye, Yiming Yang, and et al. Self-refine:\nIterative refinement with self-feedback. Advances in Neural\nInformation Processing Systems, 36:46534–46594, 2023. 3\n[30] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, et al.\nWebgpt:\nBrowser-assisted question-answering with human feedback.\narXiv preprint arXiv:2112.09332, 2021. 3\n[31] Changdae Oh, Zhen Fang, Shawn Im, Xuefeng Du, and Yix-\nuan Li. Understanding multimodal llms under distribution\nshifts: An information-theoretic approach. arXiv preprint\narXiv:2502.00577, 2025. 1\n[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, and et al.\nTraining\nlanguage models to follow instructions with human feed-\nback. Advances in neural information processing systems,\n35:27730–27744, 2022. 3\n[33] Haikang Deng Raffel and Colin. Reward-augmented decod-\ning: Efficient controlled text generation with a unidirectional\nreward model. arXiv preprint, 2023. 3\n[34] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson\nPetty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael,\nBowman, and Samuel R. Gpqa: A graduate-level google-\nproof q&a benchmark.\nIn First Conference on Language\nModeling, 2024. 5\n[35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao\nSong, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of math-\nematical reasoning in open language models. arXiv preprint\narXiv:2402.03300, 2024. 1, 3\n[36] Charlie Snell, Jaehoon Lee, Kelvin Xu, Kumar, and Aviral.\nScaling llm test-time compute optimally can be more effec-\ntive than scaling model parameters. arXiv preprint, 2024.\n3\n[37] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,\nRyan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,\nChristiano, and Paul F. Learning to summarize with human\nfeedback. Advances in neural information processing sys-\ntems, 33:3008–3021, 2020. 3\n[38] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A\nEfros, Hardt, and Moritz.\nTest-time training for out-of-\ndistribution generalization. Arxiv, 2019. 1\n[39] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram,\nGenghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang,\nSanmi Koyejo, and et al. Learning to (learn at test time):\nRnns with expressive hidden states. arXiv preprint, 2024. 1\n[40] David Silver Sutton and Richard S. Welcome to the era of\nexperience. Google AI, 2025. 1\n[41] Miles Turpin, Julian Michael, Ethan Perez, and Samuel\nBowman.\nLanguage models don’t always say what they\nthink: Unfaithful explanations in chain-of-thought prompt-\ning. Advances in Neural Information Processing Systems,\n36:74952–74965, 2023. 3\n[42] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong\nZhao, Bing Qin, and Ting Liu.\nHuatuo: Tuning llama\nmodel with chinese medical knowledge.\narXiv preprint\narXiv:2304.06975, 2023. 1\n[43] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing\nRen, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Mea-\nsuring multimodal mathematical reasoning with math-vision\ndataset.\nAdvances in Neural Information Processing Sys-\ntems, 37:95095–95169, 2024. 5\n[44] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei\nLi, Deli Chen, Yu Wu, Sui, and Zhifang. Math-shepherd:\nVerify and reinforce llms step-by-step without human anno-\ntations. arXiv preprint, 2023. 3\n[45] Renhao Wang, Yu Sun, Arnuv Tandon, Yossi Gandelsman,\nXinlei Chen, Alexei A Efros, and Xiaolong Wang. Test-time\ntraining on video streams. Journal of Machine Learning Re-\nsearch, 26(9):1–29, 2025. 3\n[46] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shix-\nuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang,\nZhenru Zhang, et al. Beyond the 80/20 rule: High-entropy\nminority tokens drive effective reinforcement learning for\nllm reasoning. arXiv preprint arXiv:2506.01939, 2025. 1\n[47] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed\nChi, Sharan Narang, Aakanksha Chowdhery, Zhou, and\nDenny. Self-consistency improves chain of thought reason-\ning in language models. arXiv preprint, 2022. 2, 3\n[48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 1, 2, 3\n[49] Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong,\nWeiran Huang, and Lichao Sun. Unsupervised post-training\nfor multi-modal llm reasoning via grpo.\narXiv preprint\narXiv:2505.22453, 2025. 1, 3, 6\n[50] Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang,\nLinghe Kong, Lichao Sun, and Weiran Huang. Advancing\nmultimodal reasoning via reinforcement learning with cold\nstart. arXiv preprint arXiv:2505.22334, 2025. 3\n[51] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hai-\nley Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov,\nHarchaoui, and Zaid.\nFrom decoding to meta-generation:\nInference-time algorithms for large language models. arXiv\npreprint, 2024. 3\n[52] Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng,\nXinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan\nYang, and Xu Yang.\nOn the generalization of sft: A re-\ninforcement learning perspective with reward rectification.\narXiv preprint arXiv:2508.05629, 2025. 1\n[53] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan,\nTimothy P Lillicrap, Kenji Kawaguchi, Shieh, and Michael.\nMonte carlo tree search boosts reasoning via iterative prefer-\nence learning. arXiv preprint, 2024. 3\n[54] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song,\nLichao Sun, and Li Yuan. Llava-cot: Let vision language\nmodels reason step-by-step. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 2087–\n2098, 2025. 3\n[55] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo\nZheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang,\nHaoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei\n"}, {"page": 11, "text": "Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin,\nKai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei\nLi, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin,\nTianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren,\nYang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu,\nZeyu Cui, Zhenru Zhang, Qiu, and Zihan. Qwen2.5 technical\nreport. arXiv preprint, 2024. 5\n[56] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen\nYu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren\nZhou, Junyang Lin, et al. Qwen2. 5-math technical report:\nToward mathematical expert model via self-improvement.\narXiv preprint arXiv:2409.12122, 2024. 5\n[57] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, et al. Qwen3 technical report. arXiv\npreprint arXiv:2505.09388, 2025. 5\n[58] Shenzhi\nWang\nZhangchi\nFeng\nDongdong\nKuang\nYuwen Xiong Yaowei Zheng, Junting Lu.\nEasyr1: An\nefficient, scalable, multi-modality rl training framework.\nhttps://github.com/hiyouga/EasyR1, 2025. 6\n[59] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao,\nFangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. Revisiting out-of-distribution robustness in\nnlp: Benchmarks, analysis, and llms evaluations. Advances\nin Neural Information Processing Systems, 36:58478–58507,\n2023. 1\n[60] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.\nStar: Bootstrapping reasoning with reasoning. Advances in\nNeural Information Processing Systems, 35:15476–15488,\n2022. 2\n[61] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu,\nXikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learn-\ning to reason with multimodal large language models via\nstep-wise group relative policy optimization. arXiv preprint\narXiv:2503.12937, 2025. 3\n[62] Kaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu,\nErmo Hua, Xingtai Lv, Ning Ding, Biqing Qi, and Bowen\nZhou. Openprm: Building open-domain process-based re-\nward models with preference trees. In The Thirteenth Inter-\nnational Conference on Learning Representations, 2025. 3\n[63] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Hao-\nhan Wang, Wang, and Yu-Xiong. Language agent tree search\nunifies reasoning acting and planning in language models.\narXiv preprint, 2023. 3\n[64] Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kis-\nhan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang,\nHaitao Mi, and Dong Yu. Evolving language models without\nlabels: Majority drives selection, novelty promotes variation.\narXiv preprint arXiv:2509.15194, 2025. 3\n[65] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 3\n[66] Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu,\nErmo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou.\nMedxpertqa: Benchmarking expert-level medical reasoning\nand understanding. arXiv preprint arXiv:2501.18362, 2025.\n5\n[67] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu\nCui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long,\nErmo Hua, et al.\nTtrl: Test-time reinforcement learning.\narXiv preprint arXiv:2504.16084, 2025. 1, 3, 5\n"}]}