{"doc_id": "arxiv:2511.05968", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.05968.pdf", "meta": {"doc_id": "arxiv:2511.05968", "source": "arxiv", "arxiv_id": "2511.05968", "title": "DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities", "authors": ["Nagur Shareef Shaik", "Teja Krishna Cherukuri", "Adnan Masood", "Dong Hye Ye"], "published": "2025-11-08T11:08:27Z", "updated": "2025-11-08T11:08:27Z", "summary": "The integration of medical images with clinical context is essential for generating accurate and clinically interpretable radiology reports. However, current automated methods often rely on resource-heavy Large Language Models (LLMs) or static knowledge graphs and struggle with two fundamental challenges in real-world clinical data: (1) missing modalities, such as incomplete clinical context , and (2) feature entanglement, where mixed modality-specific and shared information leads to suboptimal fusion and clinically unfaithful hallucinated findings. To address these challenges, we propose the DiA-gnostic VLVAE, which achieves robust radiology reporting through Disentangled Alignment. Our framework is designed to be resilient to missing modalities by disentangling shared and modality-specific features using a Mixture-of-Experts (MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained optimization objective enforces orthogonality and alignment between these latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder then uses these disentangled representations to generate reports efficiently. On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4 scores of 0.266 and 0.134, respectively. Experimental results show that the proposed method significantly outperforms state-of-the-art models.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.05968v1", "url_pdf": "https://arxiv.org/pdf/2511.05968.pdf", "meta_path": "data/raw/arxiv/meta/2511.05968.json", "sha256": "d91e819e43d654a79f703494b4a9d2b19bc36805555d071aeb74011605e345e5", "status": "ok", "fetched_at": "2026-02-18T02:28:08.952815+00:00"}, "pages": [{"page": 1, "text": "DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language\nVariational AutoEncoder for Robust Radiology Reporting with Missing Modalities\nNagur Shareef Shaik1, Teja Krishna Cherukuri1, Adnan Masood2, Dong Hye Ye1,\n1Department of Computer Science, Georgia State University, Atlanta, GA, USA; 2UST, Aliso Viejo, CA, USA.\nnshaik3@student.gsu.edu, tcherukuri1@student.gsu.edu, amasood@amp207.hbs.edu, dongye@gsu.edu\nAbstract\nThe integration of medical images with clinical context is es-\nsential for generating accurate and clinically interpretable ra-\ndiology reports. However, current automated methods often\nrely on resource-heavy Large Language Models (LLMs) or\nstatic knowledge graphs and struggle with two fundamental\nchallenges in real-world clinical data: (1) missing modali-\nties, such as incomplete clinical context , and (2) feature\nentanglement, where mixed modality-specific and shared\ninformation leads to suboptimal fusion and clinically un-\nfaithful hallucinated findings. To address these challenges,\nwe propose the DiA-gnostic VLVAE, which achieves robust\nradiology reporting through Disentangled Alignment. Our\nframework is designed to be resilient to missing modalities\nby disentangling shared and modality-specific features using\na Mixture-of-Experts (MoE) based Vision-Language Varia-\ntional Autoencoder (VLVAE). A constrained optimization ob-\njective enforces orthogonality and alignment between these\nlatent representations to prevent suboptimal fusion. A com-\npact LLaMA-X decoder then uses these disentangled repre-\nsentations to generate reports efficiently. On the IU X-Ray\nand MIMIC-CXR datasets, DiA has achieved competetive\nBLEU@4 scores of 0.266 and 0.134, respectively. Experi-\nmental results show that the proposed method significantly\noutperforms state-of-the-art models.\nIntroduction\nRadiology report generation (RRG) is a critical task in med-\nical imaging that aims to produce accurate and comprehen-\nsive reports from scans, which can help lessen the burden on\nradiologists. Despite progress in computer vision and natural\nlanguage processing, RRG remains a significant challenge\ndue to the need for precise clinical insight and coherent\nreport synthesis. This is often complicated by imbalanced\ndatasets where rare conditions are underrepresented, which\ncan compromise diagnostic reliability (Yu et al. 2025).\nEarly models, such as R2Gen (Chen et al. 2020) and\nCvT2Dis (Nicolson et. al 2023), relied exclusively on\nimage features, using transformers and contrastive learn-\ning to refine visual representations. However, this image-\ncentric approach has difficulty capturing nuanced diseases\nand integrating clinical reasoning. Subsequent efforts fo-\ncused on improving vision-language integration. For ex-\nample, XProNet utilized cross-modal prototypes for align-\nment (Wang, Bhalerao, and He 2022), while METrans-\nformer used multiple learnable expert tokens to enhance tex-\ntual consistency (Wang et al. 2023). Still, these models’ re-\nliance on image-centric patterns can lead to semantic dis-\ncrepancies and clinical errors, especially when radiographic\nfeatures of different diseases overlap, due to a lack of con-\ntextual grounding.\nTo address these limitations, recent models have begun\nto incorporate diagnostic context, such as disease pseudo-\nlabels, knowledge graphs, or prior findings. Knowledge-\ndriven approaches like MKSG (Yang et al. 2022) and\nM2KT (Yang et al. 2023) use medical knowledge graphs\nto improve factual accuracy. Context-aware models such as\nKiUT (Huang, Zhang, and Zhang 2023), DCL (Li et al.\n2023b), EKAGen (Bu et al. 2024), and PromptMRG (Jin\net al. 2024) have also integrated expert knowledge and prior\nreports through graphs and prompts. While these methods\nenhance the clinical relevance of the generated reports, they\nhave several technical constraints. For instance, they often\nlack explicit disentanglement, making it difficult to separate\nmodality-specific knowledge from shared information. Con-\nsequently, the absence of context can lead to incomplete re-\nports due to inefficient multi-modal alignment. Additionally,\nprompt-based models often depend on templates constructed\nfrom pseudo-diagnoses, which limits their adaptability and\ncan significantly increase computational overhead due to\ntheir use of Large Language Models (LLMs).\nRetrieval-augmented methods like SEI (Liu et al. 2024)\nhave advanced this area by extracting “factual entities” from\na study, retrieving similar past cases, and using them to\nguide a cross-modal fusion decoder. However, this approach\nhas its own issues. The entity-extraction and retrieval stages\ncan be brittle, and the fusion network does not enforce ex-\nplicit modality disentanglement or probabilistic feature gat-\ning. This leaves the model vulnerable to feature interference\nwithin what the authors term an “unstable fusion space”.\nFurthermore, when contextual information is missing, these\nmodels often fall back on deterministic rules instead of\na principled probabilistic strategy, which can cause errors\nfrom earlier stages to propagate.\nTo tackle these challenges, we introduce the DiA-gnostic\nVLVAE, designed for robust radiology reporting by lever-\naging the principle of Disentangled Alignment. To handle\nmissing modalities and dynamic patient states, the frame-\nwork uses real-time clinical data, including demographics,\narXiv:2511.05968v1  [cs.CV]  8 Nov 2025\n"}, {"page": 2, "text": "symptoms, and prior history, as dynamic context. Its core\nis a Vision-Language Variational Autoencoder (Mao et al.\n2023) that disentangles modality-specific and shared latent\nrepresentations, ensuring consistent vision-language align-\nment even when context is incomplete. This is supported by\na Vision-Language Representation Learning module using\nGuided Context Attention (Cherukuri, Shaik, and Ye 2024)\nand a Modality Abstractor (Vaswani et al. 2017) for effec-\ntive cross-modal feature fusion. Finally, a compact and effi-\ncient LLaMA-X decoder generates clinically precise reports,\navoiding the template rigidity of prompt-based models (Jin\net al. 2024) while outperforming more resource-intensive al-\nternatives in adaptability and computational efficiency.\nRelated Work\nFusion of Heterogeneous Medical Data\nFusing hetero-\ngeneous medical data, such as EHR, clinical notes, and\nvarious medical imaging types (Venugopalan et al. 2021;\nMohsen et al. 2022), has shown significant potential for\nimproving clinical tasks like prognosis prediction (Kline\net al. 2022; Cheerla and Gevaert 2019), phenotyping (Hayat,\nGeras, and Shamout 2022), and medical image segmentation\n(Huang et al. 2020b). This integration of diverse data sources\nis a clear trend aimed at building more comprehensive and\naccurate clinical models (Huang et al. 2020a).\nHandling Missing Modality\nIn practice, some clinical\ndata modalities are inevitably missing (Huang et al. 2020a).\nA common solution is late fusion, where predictions from\nindependently modeled modalities are aggregated at the de-\ncision level (Yoo et al. 2019; Steyaert et al. 2023). However,\nthis approach can be suboptimal as it fails to capture the in-\nteractions between modalities (Huang et al. 2020a). More re-\ncent research has explored generative methods to impute or\nreconstruct missing data at the feature or instance level (Ma\net al. 2021; Zhang et al. 2022; Sharma and Hamarneh 2019).\nThese techniques may use a Bayesian meta-learning frame-\nwork (Ma et al. 2021) or impute features in the latent space\nwith auxiliary information (Zhang et al. 2022). Despite these\nadvances, results from generated data may not be robust (Li\net al. 2023a; Yao et al. 2024a), and handling missing data\nin highly heterogeneous settings like image-and-text fusion\nremains an open challenge (Yao et al. 2024a).\nDisentangled Representation Learning\nA promising ap-\nproach for handling both missing data and modal inconsis-\ntency is to disentangle shared and modality-specific infor-\nmation (Yao et al. 2024a; Liu et al. 2025; Robinet et al.\n2024). The goal is to learn representations that separate com-\nmon, patient-related information from unique, modality-\nspecific details (Robinet et al. 2024). This is often achieved\nby imposing explicit constraints on the latent space. Com-\nmon techniques include enforcing orthogonality between\nshared and specific representations to minimize redundancy\n(Braman et al. 2021; Yao et al. 2024a) or minimizing\ntheir mutual information, often via an adversarial objective\n(Sanchez, Serrurier, and Ortner 2020; Liu et al. 2025; Robi-\nnet et al. 2024). Concurrently, the alignment of shared rep-\nresentations is enforced using methods like Jensen-Shannon\ndivergence (JSD) (Yao et al. 2024a) or contrastive objec-\ntives (Robinet et al. 2024). While most prior work focused\non more homogeneous modalities like different MRI scans\n(Chen et al. 2019; Shen and Gao 2019), DiA introduces a\nprobabilistic tri-factor decomposition that leverages a Vi-\nsion–Language VAE with a shared-gate Mixture-of-Experts\nand a unified Disentangled-Alignment constraint, enabling\nrobust radiology reporting from highly heterogeneous inputs\nwith missing modalities.\nMethodology\nThe DiA-gnostic VLVAE is a principled probabilistic ap-\nproach for robust radiology reporting designed to be re-\nsilient to missing modalities such as incomplete clinical\ncontext. The framework is built on the principle of Dis-\nentangled Alignment, which it achieves by learning a tri-\nfactor latent space that explicitly separates modality-specific\n(vision, language) features from shared cross-modal seman-\ntics. To handle missing data, the shared latent is inferred\nvia a Mixture-of-Experts (MoE) posterior, a theoretically\ngrounded method that allows the model to marginalize an\nabsent expert while preserving inferential integrity. This fac-\ntorization is guided by a dual-consistency constraint: an\northogonality term disentangles the latent factors, while\na contrastive alignment term ensures the shared space is\npredictive of each modality, leading to robust and faith-\nful generation. This disentangled structure is learned by\nour novel Vision-Language Mixture-of-Experts Variational\nAuto-Encoder (VL-MoE-VAE) module and is used to drive\nreport generation through an efficient LLaMA-X decoder.\nProblem Formulation\nLet our dataset be D = {(Vi, Li, Ri)}N\ni=1, where for each\nsubject i, Vi ∈RH×W ×C represents a medical image (e.g.,\nChest X-Ray), Li = {li,k}\nKi\ni=1 captures clinical indications\n(e.g., patient demographics, symptoms, prior history) with\nK elements, and Ri = {ri,t}\nTi\nt=1 is the corresponding radi-\nology report. Our primary objective is to learn a conditional\ngenerative model p(R | V, L) that maximizes the likelihood\nof producing the correct report R given the image V and\nthe accompanying clinical context L. A critical principle for\nachieving robust reporting is modality resilience: the frame-\nwork must remain effective even when one modality is ab-\nsent, particularly the clinical context L. Consequently, the\nframework must also support principled inference for the\nmarginal scenario p(R | V ).\nFeature Extraction and Fusion\nBefore probabilistic modeling, we transform the raw, high-\ndimensional inputs into a unified, semantically rich feature\nspace. This stage serves as a powerful feature extraction\nbaseline, complementing DiA.\nVision & Language Feature Extractor\nWe leverage a\npre-trained convolutional neural network, EfficientNetB0\n(Tan and Le 2019), to extract high-level features from in-\nput image V . To capture clinically relevant global pat-\nterns that are often missed by local receptive fields, we\naugment the backbone with a Guided Context Attention\n"}, {"page": 3, "text": "Figure 1: Architecture of DiA: Extracts vision features using EfficientNetB0 with Guided Context Attention and language\nfeatures via a Transformer Encoder, fused by a Modality Abstractor; learns modality-specific latents (Zv, Zl) using VAEs\n(VGG16 and Transformer) and shared latent (Zs) through a Mixture-of-Experts Shared Encoder, disentangled via Lorth, aligned\nwith Lalign; generate reports using LlaMA-X Decoder.\n(GCA) (Cherukuri, Shaik, and Ye 2024) mechanism. This\nmodule produces a spatially-aware feature map that is pro-\njected into the final vision feature, FV ∈RSV ×E, where SV\ncaptures spatial dimensions and E is the number of feature\nchannels. The clinical context L is tokenized and processed\nby a standard Transformer encoder (Vaswani et al. 2017)\nto capture complex semantic relationships, producing a se-\nquence of contextualized embeddings FL ∈RSL×E, where\nSL is the maximum sequence length.\nModality Abstractor\nTo align and integrate these het-\nerogeneous features, we use a Modality Abstractor based\non bidirectional cross-attention (Vaswani et al. 2017). First,\nthe vision features FV and language features FL are pro-\njected into query (Q), key (K), and value (V) representations\nusing learnable weight matrices. The module then allows\nfeatures from each modality to query the other, dynami-\ncally highlighting visually-grounded clinical terms and text-\nrelevant image regions. This process computes both vision-\nto-language FV 2L and language-to-vision FL2V representa-\ntions via multi-head attention:\nFV 2L = FV + Softmax\n\u0012QV · K⊤\nL\n√dk\n\u0013\n· VL\n(1)\nFL2V = FL + Softmax\n\u0012QL · K⊤\nV\n√dk\n\u0013\n· VV\n(2)\nwhere dk is the key vector’s dimension. The resulting fea-\ntures are concatenated to form a unified multi-modal rep-\nresentation FV L, integrating complementary features for\ndownstream VLVAE module.\nVision-Language Mixture-of-Experts VAE\nWe formulate DiA’s probabilistic framework using a Multi-\nmodal Variational Autoencoder (MVAE) (see Fig. 1) that\nlearns a Tri-factor Latent Decomposition. This decompo-\nsition is designed to disentangle the sources of variation in\nvision-language data into three distinct latent variables: a\nvision-specific latent Zv, a language-specific latent Zl, and\na shared, cross-modal latent Zs. As the true posterior over\nthe latents, pθ(Zv, Zl, Zs|V, L), is intractable, we introduce\na variational approximation with a specific factorization:\nqϕ(Zv, Zl, Zs|V, L) ∼qϕv(Zv|V )·qϕl(Zl|L)·qϕs(Zs|V, L).\nHere, qϕv and qϕl are encoders for the modality-specific la-\ntents, while qϕs is a joint encoder for the shared latent, which\nuses a Mixture-of-Experts (MoE) strategy to ensure robust-\nness against missing modalities.\nModality-Specific Latent Inference\nThe model’s struc-\nture is guided by its generative process, which assumes that\neach observed modality is generated independently from its\ncorresponding specific latent variable. For the vision modal-\nity, a latent variable Zv is sampled from a prior distribution\np(Zv), and the image is generated by a decoder pθv(V | Zv),\nparameterized by θv. Similarly, the language latent Zl is\nsampled from its prior p(Zl) to generate the clinical con-\ntext via pθl(L | Zl), with parameters θl. This design intro-\nduces a critical inductive bias: all information necessary to\n"}, {"page": 4, "text": "reconstruct a modality must be encoded in its specific latent\nvariable, which enforces representational independence and\nfacilitates disentangled learning.\nTo learn the parameters, we need to infer the values of the\nlatent variables from the data. This requires computing the\ntrue posterior distributions, pθv(Zv | V ) and pθl(Zl | L),\nwhich are intractable to compute directly. To overcome this,\nwe employ variational inference, introducing encoder net-\nworks to approximate these true but intractable posteriors.\nThe vision encoder, qϕv(Zv | V ), uses a pre-trained VGG16\nnetwork (Simonyan and Zisserman 2014) followed by a\nfully connected layer to produce the Gaussian parameters\n(µv, σ2\nv) for the approximate posterior over Zv. The lan-\nguage encoder, qϕl(Zl | L), is a Transformer-based en-\ncoder (Liu and Liu 2019) that outputs (µl, σ2\nl ) for the ap-\nproximate posterior over the language-specific latent Zl.\nShared Latent Inference via Mixture-of-Experts\nTo\nmodel the shared latent variable Zs, DiA employs a Mixture-\nof-Experts (MoE) strategy (Shi et al. 2019) via a dedicated\nshared encoder. This approach contrasts with Product-of-\nExperts (PoE) approaches (Wu and Goodman 2018), which\ncan produce overconfident posterior estimates and degrade\nsignificantly when a modality is missing. The MoE formu-\nlation provides a more robust alternative for learning from\npartially observed data.\nThe shared encoder approximates the posterior over Zs as\na weighted combination of unimodal expert posteriors. For\neach modality M ∈{V, L}, the encoder outputs parameters\n(µs, σ2\ns) and corresponding mixture weights πM. The over-\nall MoE posterior is then defined as:\nqϕs(Zs | V, L) =\nX\nM∈{V,L}\nπM · qϕs(Zs | M),\n(3)\nwhere the mixture coefficients πM are non-negative and sum\nto one. This allows the model to adaptively the contribution\nof each modality to the shared representation.\nLearning Objective\nThe overall learning objective for the\nproposed VL-MoE-VAE is to maximize the Evidence Lower\nBound (ELBO) (Mao et al. 2023) on the marginal log-\nlikelihood. The ELBO balances accurate reconstruction with\nstructured regularization over the latent space to enforce the\ndesired disentangled alignment across Zv, Zl, and Zs. The\nfull objective is defined as:\nLELBO = Eqϕs(Zs|V,L)\nh\nEqϕv (Zv|V ) [log pθv(V |Zv)]\n+ Eqϕl(Zl|L) [log pθl(L|Zl)]\ni\n−\nh\nDKL(qϕv(Zv|V )∥p(Zv)) + DKL(qϕl(Zl|L)∥p(Zl))\ni\n−JSD(qϕs(Zs|V, L) , p(Zs))\n(4)\nThis objective function evaluates the model’s ability to re-\nconstruct the input modalities (V, L) from their respective\nspecific latents Zv and Zl, conditioned on a shared latent\nvariable Zs. It also encourages the modality-specific poste-\nriors qϕv(Zv | V ) and qϕl(Zl | L) to remain close to stan-\ndard Gaussian priors N(0, I) via a Kullback-Leibler (KL)\ndivergence penalty.\nA key aspect of our Mixture-of-Experts (MoE) for-\nmulation\nis\nthe\nuse\nof\nJensen-Shannon\nDivergence\n(JSD) (Men´endez et al. 1997) to regularize the shared latent\nZs. Unlike the standard KL divergence, which can lead to\ncomponent collapse where only one expert contributes to the\nposterior (Minka et al. 2005), the symmetric and bounded\nnature of JSD is more suitable for mixture distributions. It\nencourages the entire mixture to align with the prior, pro-\nmoting stability and ensuring all experts contribute mean-\ningfully to the shared latent representation, a choice consis-\ntent with recent findings in multimodal generative model-\ning (Sutter, Daunhawer, and Vogt 2020; Yao et al. 2024b).\nDisentangled Alignment Constraint\nThe ELBO objective alone does not guarantee that the la-\ntent factors are either semantically meaningful or disentan-\ngled. To explicitly enforce the desired properties of disentan-\nglement between shared and modality-specific factors, and\nstrong alignment within the shared space, we introduce a\nnovel Disentangled Alignment Constraint, which combines\ntwo regularization terms detailed below.\nOrthogonality for Disentanglement\nTo promote statis-\ntical independence between modality-specific and shared\nlatent representations, we introduce an orthogonality con-\nstraint on the latent space, a technique demonstrated to be\neffective in structured representation learning (Bousmalis\net al. 2016). Specifically, we enforce uncorrelatedness be-\ntween the latent variables Zv, Zl, and Zs by first applying\na whitening transformation to each, resulting in zero-mean,\nunit-covariance representations denoted as\n\u0000 ˜Zv, , ˜Zl, , ˜Zs\n\u0001\n.\nThis is implemented via a batch normalization layer applied\nto each latent subspace. The orthogonality loss is then for-\nmulated as the sum of squared Frobenius norms of the pair-\nwise cross-covariance matrices:\nLorth = ∥˜Z⊤\ns ˜Zv∥2\nF + ∥˜Z⊤\ns ˜Zl∥2\nF + ∥˜Z⊤\nv ˜Zl∥2\nF\n(5)\nMinimizing Lorth penalizes any statistical correlation be-\ntween the latent subspaces, thereby encouraging disentan-\nglement. This uncorrelation, when combined with whiten-\ning, approximates statistical independence under the as-\nsumption of non-Gaussianity, a core principle underlying\nIndependent Component Analysis (ICA) (Hyv¨arinen, Hurri,\nand Hoyer 2001).\nContrastive Alignment of the Shared Space\nWhile or-\nthogonality promotes statistical independence, it does not in-\nherently guarantee the semantic relevance of the shared rep-\nresentation Zs. To address this, we introduce a contrastive\nalignment objective based on the InfoNCE loss (Rusak et al.\n2024), which aligns Zs with the modality-specific latents Zv\nand Zl. This objective encourages Zs to exhibit higher sim-\nilarity with its corresponding modality-specific latent while\ntreating the other as a negative sample. Formally, the align-\nment loss is defined as:\nLalign = −Eq(Zv,Zs)\n\"\nlog\nexp(sim(Zs, Zv)/τ)\nP\nZ′∈{Zv,Zl} exp(sim(Zs, Z′)/τ)\n#\n−Eq(Zl,Zs)\n\"\nlog\nexp(sim(Zs, Zl)/τ)\nP\nZ′∈{Zv,Zl} exp(sim(Zs, Z′)/τ)\n#\n(6)\n"}, {"page": 5, "text": "where sim(·) denotes cosine similarity, and τ is a tem-\nperature parameter. This formulation ensures that Zs re-\nmains semantically coherent with both modalities. From an\ninformation-theoretic perspective, minimizing Lalign effec-\ntively maximizes the mutual information between the shared\nand specific latents (I(Zs; Zv) and I(Zs; Zl)), ensuring that\nthe shared latent Zs captures semantic information common\nto both modalities (Poole et al. 2019).\nWhen combined, the orthogonality and alignment objec-\ntive enable the model to learn latent spaces that are both sta-\ntistically disentangled and semantically rich. This dual con-\nstraint is crucial for improving the model’s generalization,\nrobustness, and interpretability in multi-modal settings.\nLlaMA-X Decoder\nThe final report is generated by the LLaMA-X Decoder,\nwhich is trained to model the dependencies between the re-\nport text and the fused multi-modal representations from the\npreceding modules. The entire DiA freamework is optimized\nend-to-end with a composite loss function.\nThe LLaMA-X Decoder is a compact adaptation of\nLLaMA (Touvron et al. 2023). It uses a GPT-derived Cross-\nAttention (Brown 2020) to condition the report generation\non the fused multi-modal representations from both the\nModality Abstractor (FV L) and VL-MoE-VAE (Zv, Zl, Zs).\nThe architecture incorporates several optimizations for ef-\nficiency and performance: (1) Rotary Positional Encod-\nings (RoPE) which embed relative positional information\nvia rotation matrices in the query and key vectors to effi-\nciently handle long sequence lengths; (2) Grouped Query\nAttention which partitions queries into groups and lever-\nages Key-Value (KV) caching to minimize redundant com-\nputations during inference; (3) SwiGLU Feed-Forward Net-\nwork (FFN) that is defined as SwiGLU(x) = (xW1) ⊙\nσ(xW2)W3, with SiLU activation σ(·) to enhance feature\ntransformation and mitigate the vanishing gradient prob-\nlem; (4) RMS Pre-Normalization that is defined as x′ =\nx/\np\nmean(x2) + ϵ to stabilize the inputs to the attention and\nfeed-forward layers.\nThe decoder is trained by optimizing a standard cross-\nentropy loss, LCE = −PN\ni=1\nPT\nj=1 rij log(ˆrij) to align\npredicted reports ˆr with ground-truth r over T tokens. The\noverall objective for the DiA framework integrates this gen-\neration loss with previously defined objectives for the VL-\nMoE-VAE and the Disentangled Alignment Constraint. The\ntotal loss is a weighted sum:\nLtotal = LCE + LELBO + λ1Lorth + λ2Lalign,\n(7)\nwhere λ1 and λ2 are hyperparameters that balance the con-\ntributions of the orthogonality and alignment losses, re-\nspectively. This composite objective ensures that the model\nlearns to generate accurate reports while maintaining a ro-\nbust, disentangles latent structure.\nInference with Missing Context\nA key advantage of the DiA framework is its inherent ro-\nbustness to missing modalities, a common scenario in clini-\ncal workflows where the image V is present but the clinical\ncontext L may be absent. This resilience is a direct conse-\nquence of using a Mixture-of-Experts (MoE) posterior to in-\nfer the shared latent Zs. At inference time, if a modality L\nis unavailable, a designated “null” token is passed to cor-\nresponding expert. As the MoE router was exposed to the\nsame token during training, it learns to down-weight the un-\navailable modality automatically, i.e. πL ≈0 and πV ≈1\nin Eq. (3). This allows the posterior to gracefully reduce to\nbeing conditioned only on the available data qϕs(Zs | V )\nwithout requiring any imputation or architectural changes.\nThis process is theoretically sound. By substituting the\nreduced posterior into the training objective in eq. (4) and\ndiscarding terms involving the missing modality L, the ob-\njective becomes a marginal ELBO.\nL(V )\nELBO = Eqϕv (Zv|V )\n\u0002\nlog pθv(V | Zv)\n\u0003\n(8)\n−DKL\n\u0000qϕv(Zv | V ) ∥p(Zv)\n\u0001\n−JSD\n\u0000qϕs(Zs | V ), p(Zs)\n\u0001\nThis new objective L(V )\nELBO remains a valid lower bound on\nthe marginal log-likelihood of the observed data ( L(V )\nELBO ≤\nlog pθ(V )), ensuring the learning procedure is principled for\nany subset of modalities.\nThe model’s effective performance in this scenario stems\nfrom the contrastive alignment term Lalign applied during\ntraining. By maximizing the mutual information between\nthe shared latent and each specific modality I(Zs; Zv) and\nI(Zs; Zl), the shared latent Zs learns to encode salient\ncross-modal semantics. Consequently, even when inferred\nfrom a single modality, Zs still provides the LLaMA-X de-\ncoder with sufficient information to generate clinically faith-\nful reports, leading to a graceful degradation in performance\nrather than a catastrophic failure.\nExperiments\nExperimental Settings\nDatasets and Preprocessing\nWe evaluate DiA on two\nstandard radiology report generation benchmarks: IU\nX-Ray\n(Demner-Fushman\net\nal.\n2016)\nand\nMIMIC-\nCXR (Johnson et al. 2019), both comprising paired chest\nX-ray images, free-text reports, and structured clinical meta-\ndata, enabling assessment under both complete and missing\nmodality conditions.\nIU X-Ray, consists of 7,470 frontal-view X-ray images\nand 3,955 reports. We adopt a 70%/10%/20% train/valida-\ntion/test split and use a 1,000 word vocabulary. Approxi-\nmately 2% of the test samples in this dataset are missing\nclinical context, providing a controlled setting to test for\nmodality resilience. MIMIC-CXR is a much larger dataset\nwith 473,057 images and 206,563 reports across 64,588 pa-\ntients. We use the official split from (Chen et al. 2020),\ncomprising 270,790 training, 2,130 validation, and 3,858\ntest samples. Reports are tokenized, lower-cased, and fil-\ntered to remove non-alphabetic tokens; words appearing <\n4 are discarded, resulting in a vocabulary of 4,000 tokens.\nThis dataset presents a more significant challenge for model\nrobustness, as approximately 45% of its test samples have\nmissing clinical indications.\n"}, {"page": 6, "text": "Table 1: Performance comparison of our proposed DiA with state-of-the-art models on the IU X-Ray and MIMIC-CXR\ndatasets, reporting NLG and CE metrics; Methods grouped as Image (Img), Knowledge-Guided (KG), & Context-Aware (CA).\nType\nModel\nIU X-Ray\nMIMIC-CXR\nB@1\nB@4\nR-L\nF1\nB@1\nB@4\nR-L\nF1\nImg\nR2Gen (Chen et al. 2020)\n0.470\n0.165\n0.371\n-\n0.353\n0.103\n0.277\n-\nCvT2Dis (Nicolson et. al 2023)\n0.473\n0.175\n0.376\n-\n0.392\n0.127\n0.285\n0.384\nKG\nMETransformer (Wang et al. 2023)\n0.483\n0.172\n0.380\n-\n0.386\n0.124\n0.291\n0.311\nClinical BERT(Yan and Pei 2022)\n0.495\n0.170\n0.376\n-\n0.383\n0.106\n0.275\n0.415\nM2KT (Yang et al. 2023)\n0.497\n0.174\n0.399\n-\n0.386\n0.111\n0.274\n0.352\nMKSG (Yang et al. 2022)\n0.496\n0.178\n0.381\n-\n0.363\n0.115\n0.284\n0.371\nXProNet (Wang, Bhalerao, and He 2022)\n0.525\n0.199\n0.411\n-\n0.344\n0.105\n0.279\n-\nCA\nPromptMRG (Jin et al. 2024)\n0.401\n0.098\n0.281\n0.211\n0.398\n0.112\n0.268\n0.476\nKiUT (Huang, Zhang, and Zhang 2023)\n0.525\n0.185\n0.409\n-\n0.393\n0.113\n0.285\n0.321\nEKAGen (Bu et al. 2024)\n0.526\n0.203\n0.404\n-\n0.411\n0.119\n0.217\n0.499\nSEI (Liu et al. 2024)\n-\n-\n-\n-\n0.382\n0.135\n0.299\n0.460\nOurs\nDiA\n0.616\n0.266\n0.516\n0.298\n0.415\n0.134\n0.369\n0.497\nTable 2: Ablation Study: Incremental effects of VL-MoE-VAE (LELBO) and Disentangled Alignment (DA) (Lorth + Lalign)\nacross with-context (✓) and missing-context (✗) scenarios\nContext\nBaseline\nVL-MoE-VAE\nDA\nIU X-Ray\nMIMIC-CXR\nB@1\nB@4\nR-L\nF1\nB@1\nB@4\nR-L\nF1\n✓\n✓\n✗\n✗\n0.602\n0.262\n0.435\n0.358\n0.386\n0.114\n0.260\n0.446\n✓\n✓\n✗\n0.655\n0.319\n0.548\n0.381\n0.423\n0.140\n0.343\n0.551\n✓\n✓\n✓\n0.691\n0.357\n0.624\n0.396\n0.447\n0.158\n0.399\n0.621\n✗\n✓\n✗\n✗\n0.276\n0.079\n0.185\n0.166\n0.295\n0.049\n0.176\n0.219\n✓\n✓\n✗\n0.365\n0.174\n0.374\n0.204\n0.356\n0.093\n0.315\n0.394\n✓\n✓\n✓\n0.387\n0.198\n0.421\n0.213\n0.371\n0.104\n0.350\n0.438\nImplementation and Training Details\nDiA was imple-\nmented in PyTorch and trained for 25 epochs on an NVIDIA\nA40 GPU using the AdamW optimizer (Loshchilov and\nHutter 2017) with a learning rate of 1e-4 and a weight decay\nof 1e-5. We used a batch size of 4 and set the maximum re-\nport length of 50 words. The model’s compact architecture\nis defined by an embedding dimension E of 1024, a latent\ndimension for (Zv, Zl, Zs) of 256, 6 Transformer encoder-\ndecoder layers, 8 attention heads, and 2 key-value (KV)\nheads. A dropout rate of 0.1 was used to mitigate overfitting,\nwhile the loss term coefficients were set to λ1, λ2 = 0.3.\nThese values were determined empirically from a search\nrange of 0.1 to 0.5. To ensure consistent results, the Trans-\nformer’s weight initialization was controlled by setting a\nrandom seed. We assess model performance using natural\nlanguage generation (NLG) metrics including BLEU (Pap-\nineni et al. 2002) and ROUGE-L (Lin 2004), and a clini-\ncal efficacy (CE) metric such as F1 score. Following (Nicol-\nson et. al 2023), the F1 score is calculated by converting the\ngenerated reports into 14 disease classification labels using\nthe CheXbert labeler (Smit et al. 2020).\nEvaluation\nComparison with State-of-the-Art Methods\nAs shown\nin Table 1, DiA demonstrates superior performance com-\npared to state-of-the-art (SOTA) methods on both IU X-Ray\nand MIMIC-CXR datasets. The evaluation spans Image-\nspecific (Img), Knowledge-Guided (KG), and Context-\nAware (CA) approaches, with DiA excelling in both natu-\nral language generation (NLG) and clinical efficacy (CE)\nmetrics. On IU X-Ray, DiA achieves a BLEU@4 score of\n0.266, surpassing the best KG model (XProNet) by 0.067,\nwhile an F1 sccore of 0.298, outperming the best CA model\n(PromptMRG) by 0.087. On the more challenging MIMIC-\nCXR dataset, DiA’s performance is highly competitive;\nwhile SEI shows a marginal lead in BLEU@4 (0.135 vs.\n0.134), DiA’s higher ROUGE-L score indicates enhanced re-\nport coherence. Its F1 score of 0.497 nearly matches the top\nperformer, EKAGen (0.499). These results highlight DiA’s\nadept integration of vision-language contexts, surpassing ad-\nvanced CA methods that struggle with longer reports.\nAblation Study: Impact of Core Components\nTable 2\npresents an ablation study quantifying the impact of DiA’s\ncore components, the VL-MoE-VAE and the Disentangled\nAlignment (DA) constraint-under both complete (✓) and\nmissing (✗) context scenarios. When clinical context is avail-\nable, adding the VL-MoE-VAE to the baseline significantly\nboosts performance, improving the F1 score on MIMIC-\nCXR by 0.105 and the BLEU@4 on IU X-ray by 0.057,\nwhich demonstrates the benefit of modeling a shared latent\nstructure. Incorporating the DA constraint (Lorth+Lalign) fur-\n"}, {"page": 7, "text": "Figure 2: Comparison of actual and generated reports with chest X-rays and attention maps. Purple highlights key findings in\nthe actual report, green indicates matched findings in the report, and amber marks mismatches / additional generated findings.\nTable 3: Comparison of encoder-decoder variants on\nMIMIC-CXR. RAD-DINO + CXR-BERT replaces DiA’s\ncustom feature extractor and latent encoder; decoder across\nall variants is LLaMA-X unless otherwise noted.\nVariant\nParams\nFLOPs\nB@4\nF1\nRAD+CXR-BERT\n568.7\n81.1\n0.121\n0.441\nTransformer\n591.2\n80.6\n0.126\n0.479\nGPT-2\n746.9\n86.4\n0.116\n0.419\nDiA LLaMA-X\n589.7\n51.1\n0.134\n0.497\nther enhances performance, with full DiA model achieving\nthe highest scores across all metrics (e.g., MIMIC-CXR: F1\n0.621, ROUGE-L 0.399).\nUnder missing context, DiA shows remarkable resilience.\nWhile the baseline’s F1 score on MIMIC-CXR drops by\n0.227, with image only input, while DiA drops by only\n0.183, outperforming the baseline by a margin of +0.219 in\nthis challenging setting. The resilience is also evident on IU\nX-Ray, where DiA’s BLEU@4 remains more than 2× higher\nthan baseline’s (0.198 vs. 0.079). Comparing the start-to-end\ngains on MIMIC-CXR, the full DiA model improves over\nthe baseline by 0.175 on the F1 score with context and by\n0.219 without context, demonstrating even greater relative\nbenefit in the challenging incomplete-input scenario.\nThese findings confirm that DiA’s latent structure ef-\nfectively infers missing semantics, establishing DiA as a\nmodality-resilient, high-performance report generator.\nAnalysis of Architectural Choices and Efficiency\nTa-\nble 3 summarizes a comparison of encoder and decoder\nvariants on MIMIC-CXR to validate DiA’s architectural de-\nsign. For encoder variants, we compared DiA’s custom fea-\nture extraction pipeline against a pre-trained RAD-DINO\n+ CXR-BERT setup. (Perez-Garcia et al. 2025; Boeck-\ning et al. 2022) Despite using powerful pre-trained mod-\nels, the RAD-DINO + CXR-BERT configuration achieved\nlower performance (BLEU@4 = 0.121, F1 = 0.441) and\nincurred higher computational cost (81.1 GFLOPs). DiA’s\nend-to-end learned encoder proved more effective and effi-\ncient (BLEU@4 = 0.134, F1 = 0.497 at 51.1 GFLOPs). For\ndecoder variants, the LLaMA-X architecture outperformed\nstandard Transformer (BLEU@4 = 0.126, F1 = 0.479 at\n80.6 GFLOPs) and GPT-2 decoders (BLEU@4 = 0.116, F1\n= 0.419 at 86.4 GFLOPs) in both accuracy and efficiency.\nThese results demonstrate that DiA’s lightweight yet expres-\nsive components offer superior performance-to-cost trade-\noff. DiA’s efficiency is demonstrated by its training and in-\nference times on an NVIDIA A40 GPU. Training on IU X-\nRay takes 2.8 hours, with a 0.15-second inference time. For\nthe larger MIMIC-CXR dataset, training takes 79.8 hours\nwith a 0.18-second inference time. With 589.7M parameters\nand a computational cost of 51.14 GFLOPs, DiA maintains\nconsistent computational efficiency.\nQualitative Visual Inspection\nAs shown in Figure 2, vi-\nsual inspection of the model’s attention maps reinforces its\nstrengths. The heatmaps highlight that DiA focuses on key\nclinical regions in the chest X-rays, both with and with-\nout the presence of clinical context in the input. The strong\nalignment between the generated reports and the ground-\ntruth reports underscores the effective synergy of all of DiA’s\ncomponents.\nConclusion\nThis research introduces DiA, a cutting-edge framework that\nadvances radiology report generation by effectively integrat-\ning medical scans with real-time clinical indications. The\ncore of DiA is its ability to disentangle and align modality-\nspecific and shared latent representations, enabling the gen-\neration of coherent reports even with incomplete context. As\na result, DiA outperforms state-of-the-art methods on the IU\nX-Ray and MIMIC-CXR datasets. This proven robustness\nin handling missing data underscores DiA’s potential to en-\nhance diagnostic accuracy and support radiologists in real-\nworld clinical scenarios. Overall, DiA significantly advances\nautomating radiology reporting, promising to improve effi-\nciency and reliability of medical imaging workflows.\n"}, {"page": 8, "text": "References\nBoecking, B.; Usuyama, N.; Bannur, S.; Castro, D. C.;\nSchwaighofer, A.; Hyland, S.; Wetscherek, M.; Naumann,\nT.; Nori, A.; Alvarez-Valle, J.; et al. 2022. Making the most\nof text semantics to improve biomedical vision–language\nprocessing. In European conference on computer vision, 1–\n21. Springer.\nBousmalis, K.; Trigeorgis, G.; Silberman, N.; Krishnan,\nD.; and Erhan, D. 2016.\nDomain separation networks.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 343–351.\nBraman, N.; Gordon, J. W. H.; Goossens, E. T.; Willis, C.;\nStumpe, M. C.; and Venkataraman, J. 2021. Deep Orthogo-\nnal Fusion: Multimodal Prognostic Biomarker Discovery In-\ntegrating Radiology, Pathology, Genomic, and Clinical Data.\nIn Medical Image Computing and Computer Assisted Inter-\nvention – MICCAI 2021, 667–677.\nBrown, T. B. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nBu, S.; Li, T.; Yang, Y.; and Dai, Z. 2024.\nInstance-\nlevel expert knowledge and aggregate discriminative atten-\ntion for radiology report generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 14194–14204.\nCheerla, A.; and Gevaert, O. 2019. Deep learning with mul-\ntimodal representation for pan-cancer prognosis prediction.\nBioinformatics, 35(14): i446–i454.\nChen, C.; Dou, Q.; Jin, Y.; Chen, H.; Qin, J.; and Heng, P.-\nA. 2019. Robust multimodal brain tumor segmentation via\nfeature disentanglement and gated fusion. In Medical Image\nComputing and Computer Assisted Intervention – MICCAI\n2019, 447–456. Springer.\nChen, Z.; Song, Y.; Chang, T.-H.; and Wan, X. 2020. Gen-\nerating radiology reports via memory-driven transformer.\narXiv preprint arXiv:2010.16056.\nCherukuri, T. K.; Shaik, N. S.; and Ye, D. H. 2024.\nGuided Context Gating: Learning to Leverage Salient Le-\nsions in Retinal Fundus Images.\nIn Proceedings of the\nIEEE International Conference on Image Processing (ICIP).\nAbu Dhabi, United Arab Emirates: IEEE. ArXiv preprint\narXiv:2406.13126.\nDemner-Fushman, D.; Kohli, M. D.; Rosenman, M. B.;\nShooshan, S. E.; Rodriguez, L.; Antani, S.; Thoma, G. R.;\nand McDonald, C. J. 2016. Preparing a collection of radiol-\nogy examinations for distribution and retrieval. Journal of\nthe American Medical Informatics Association, 23(2): 304–\n310.\nHayat, N.; Geras, K. J.; and Shamout, F. E. 2022. MedFuse:\nMulti-modal fusion with clinical time-series data and chest\nX-ray images. In Proceedings of the 7th Machine Learning\nfor Healthcare Conference, volume 182, 479–503. PMLR.\nHuang, S.-C.; Pareek, A.; Seyyedi, S.; Banerjee, I.; and Lun-\ngren, M. P. 2020a.\nFusion of medical imaging and elec-\ntronic health records using deep learning: a systematic re-\nview and implementation guidelines. NPJ Digital Medicine,\n3(1): 136.\nHuang, S.-C.; Pareek, A.; Zamanian, R.; Banerjee, I.; and\nLungren, M. P. 2020b. Multimodal fusion with deep neural\nnetworks for leveraging CT imaging and electronic health\nrecord: a case-study in pulmonary embolism detection. Sci-\nentific Reports, 10(1): 22147.\nHuang, Z.; Zhang, X.; and Zhang, S. 2023.\nKiut:\nKnowledge-injected u-transformer for radiology report gen-\neration.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 19809–19818.\nHyv¨arinen, A.; Hurri, J.; and Hoyer, P. O. 2001. Independent\ncomponent analysis. In Natural Image Statistics: A Proba-\nbilistic Approach to Early Computational Vision, 151–175.\nSpringer.\nJin, H.; Che, H.; Lin, Y.; and Chen, H. 2024. Promptmrg:\nDiagnosis-driven prompts for medical report generation. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 38, 2607–2615.\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\nM. P.; Deng, C.-y.; Peng, Y.; Lu, Z.; Mark, R. G.; Berkowitz,\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\nlicly available database of labeled chest radiographs. arXiv\npreprint arXiv:1901.07042.\nKline, A.; Wang, H.; Li, Y.; Dennis, S.; Hutch, M.; Xu, Z.;\nWang, F.; Cheng, F.; and Luo, Y. 2022. Multimodal machine\nlearning in precision health: A scoping review. NPJ Digital\nMedicine, 5(1): 171.\nLi, L.; Ding, W.; Huang, L.; Zhuang, X.; and Grau, V. 2023a.\nMulti-modality cardiac image computing: A survey. Medi-\ncal Image Analysis, 85: 102869.\nLi, M.; Lin, B.; Chen, Z.; Lin, H.; Liang, X.; and Chang,\nX. 2023b.\nDynamic graph enhanced contrastive learn-\ning for chest x-ray report generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3334–3343.\nLin, C.-Y. 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, C.; Huang, Z.; Chen, Z.; Tang, F.; Tian, Y.; Xu, Z.; Luo,\nZ.; Zheng, Y.; and Meng, Y. 2025.\nIncomplete Modality\nDisentangled Representation for Ophthalmic Disease Grad-\ning and Diagnosis. arXiv preprint arXiv:2502.11724.\nLiu, D.; and Liu, G. 2019. A transformer-based variational\nautoencoder for sentence generation. In 2019 International\nJoint Conference on Neural Networks (IJCNN), 1–7. IEEE.\nLiu, K.; Ma, Z.; Kang, X.; Zhong, Z.; Jiao, Z.; Baird, G.; Bai,\nH.; and Miao, Q. 2024. Structural Entities Extraction and\nPatient Indications Incorporation for Chest X-Ray Report\nGeneration. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, 433–443.\nSpringer.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nMa, M.; Ren, J.; Zhao, L.; Tulyakov, S.; Wu, C.; and Peng,\nX. 2021. SMIL: Multimodal learning with severely missing\nmodality. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 35, 2302–2310.\n"}, {"page": 9, "text": "Mao, Y.; Zhang, J.; Xiang, M.; Zhong, Y.; and Dai, Y.\n2023.\nMultimodal variational auto-encoder based audio-\nvisual segmentation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 954–965.\nMen´endez, M. L.; Pardo, J. A.; Pardo, L.; and Pardo, M.\nd. C. 1997. The jensen-shannon divergence. Journal of the\nFranklin Institute, 334(2): 307–318.\nMinka, T.; et al. 2005. Divergence measures and message\npassing. Technical Report MSR-TR-2005-173.\nMohsen, F.; Ali, H.; El Hajj, N.; and Shah, Z. 2022. Artificial\nintelligence-based methods for fusion of electronic health\nrecords and imaging data. Scientific Reports, 12(1): 17981.\nNicolson et. al, A. 2023. Improving chest X-ray report gen-\neration by leveraging warm starting. Artificial intelligence\nin medicine, 144: 102633.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation.\nIn Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nPerez-Garcia, F.; Sharma, H.; Bond-Taylor, S.; Bouzid,\nK.; Salvatelli, V.; Ilse, M.; Bannur, S.; Castro, D. C.;\nSchwaighofer, A.; Lungren, M. P.; et al. 2025. Exploring\nscalable medical image encoders beyond text supervision.\nNature Machine Intelligence, 7(1): 119–130.\nPoole, B.; van den Oord, A.; Hjelm, R. D.; Maaløe, L.;\nDhariwal, P.; Kingma, D. P.; and Alemi, A. A. 2019. Varia-\ntional Inference with Mutual Information Constraints. arXiv\npreprint arXiv:1907.00030.\nRobinet,\nL.;\nBerjaoui,\nA.;\nKheil,\nZ.;\nand\nCohen-\nJonathan Moyal, E. 2024. DRIM: Learning Disentangled\nRepresentations from Incomplete Multimodal Healthcare\nData. arXiv preprint arXiv:2409.17055.\nRusak, E.; Reizinger, P.; Juhos, A.; Bringmann, O.; Zim-\nmermann, R. S.; and Brendel, W. 2024. InfoNCE: Identi-\nfying the Gap Between Theory and Practice. arXiv preprint\narXiv:2407.00143.\nSanchez, E. H.; Serrurier, M.; and Ortner, M. 2020. Learning\nDisentangled Representations via Mutual Information Esti-\nmation. In Computer Vision – ECCV 2020, 205–221.\nSharma, A.; and Hamarneh, G. 2019. Missing MRI pulse se-\nquence synthesis using multi-modal generative adversarial\nnetwork. In IEEE Transactions on Medical Imaging, vol-\nume 39, 1170–1183.\nShen, Y.; and Gao, M. 2019. Brain tumor segmentation on\nMRI with missing modalities. In Information Processing\nin Medical Imaging: 26th International Conference, IPMI\n2019, 417–428. Springer.\nShi, Y.; Paige, B.; Torr, P.; et al. 2019. Variational mixture-\nof-experts autoencoders for multi-modal deep generative\nmodels. Advances in neural information processing systems,\n32.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSmit, A.; Jain, S.; Rajpurkar, P.; Pareek, A.; Ng, A.; and Lun-\ngren, M. 2020. Combining Automatic Labelers and Expert\nAnnotations for Accurate Radiology Report Labeling Using\nBERT. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 1500–\n1519. Association for Computational Linguistics.\nSteyaert, S.; Qiu, Y. L.; Zheng, Y.; Mukherjee, P.; Vogel, H.;\nand Gevaert, O. 2023. Multimodal deep learning to predict\nprognosis in adult and pediatric brain tumors. In Communi-\ncations Medicine, volume 3, 1–15.\nSutter, T.; Daunhawer, I.; and Vogt, J. 2020. Multimodal\ngenerative learning utilizing jensen-shannon-divergence.\nAdvances in neural information processing systems, 33:\n6100–6110.\nTan, M.; and Le, Q. 2019. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nconference on machine learning, 6105–6114. PMLR.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nVenugopalan, J.; Tong, L.; Hassanzadeh, H. R.; and Wang,\nM. D. 2021.\nMultimodal deep learning models for early\ndetection of Alzheimer’s disease stage. Scientific reports,\n11(1): 3254.\nWang, J.; Bhalerao, A.; and He, Y. 2022. Cross-modal pro-\ntotype driven network for radiology report generation. In\nComputer Vision–ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23-27, 2022, Proceedings, Part\nXXXV, 563–579. Springer.\nWang, Z.; Liu, L.; Wang, L.; and Zhou, L. 2023.\nMe-\ntransformer: Radiology report generation by transformer\nwith multiple learnable expert tokens.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 11558–11567.\nWu, M.; and Goodman, N. 2018.\nMultimodal generative\nmodels for scalable weakly-supervised learning. Advances\nin neural information processing systems, 31.\nYan, B.; and Pei, M. 2022. Clinical-bert: Vision-language\npre-training for radiograph diagnosis and reports generation.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 36, 2982–2990.\nYang, S.; Wu, X.; Ge, S.; Zheng, Z.; Zhou, S. K.; and Xiao,\nL. 2023. Radiology report generation with a learned knowl-\nedge base and multi-modal alignment. Medical Image Anal-\nysis, 86: 102798.\nYang, S.; Wu, X.; Ge, S.; Zhou, S. K.; and Xiao, L. 2022.\nKnowledge matters: Chest radiology report generation with\ngeneral and specific knowledge. Medical image analysis,\n80: 102510.\nYao, W.; Yin, K.; Cheung, W. K.; Liu, J.; and Qin, J. 2024a.\nDrFuse: Learning Disentangled Representation for Clinical\nMulti-Modal Fusion with Missing Modality and Modal In-\nconsistency. In The Thirty-Eighth AAAI Conference on Ar-\ntificial Intelligence (AAAI-24).\n"}, {"page": 10, "text": "Yao, W.; Yin, K.; Cheung, W. K.; Liu, J.; and Qin, J. 2024b.\nDrFuse: Learning Disentangled Representation for Clinical\nMulti-Modal Fusion with Missing Modality and Modal In-\nconsistency. In The Thirty-Eighth AAAI Conference on Ar-\ntificial Intelligence (AAAI-24).\nYoo, Y.; Tang, L. Y.; Li, D. K.; Metz, L.; Kolind, S.; Tra-\nboulsee, A. L.; and Tam, R. C. 2019. Deep learning of brain\nlesion patterns and user-defined clinical and MRI features\nfor predicting conversion to multiple sclerosis from clini-\ncally isolated syndrome. In Computer Methods in Biome-\nchanics and Biomedical Engineering: Imaging & Visualiza-\ntion, volume 7, 250–259.\nYu, T.; Lu, W.; Yang, Y.; Han, W.; Huang, Q.; Yu, J.; and\nZhang, K. 2025.\nAdapter-Enhanced Hierarchical Cross-\nModal Pre-training for Lightweight Medical Report Gener-\nation. IEEE Journal of Biomedical and Health Informatics.\nZhang, C.; Chu, X.; Ma, L.; Zhu, Y.; Wang, Y.; Wang, J.;\nand Zhao, J. 2022. M3Care: Learning with missing modal-\nities in multimodal healthcare data. In Proceedings of the\n28th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, 2418–2428.\n"}, {"page": 11, "text": "Supplementary Material\nDerivation of the Evidence Lower Bound (ELBO)\nIn this section, we provide a detailed derivation of the Ev-\nidence Lower Bound (ELBO) objective optimized by our\nDiA-gnostic VLVAE. Our model leverages a tri-factor la-\ntent space consisting of modality-specific variables Zv, Zl\nand a shared latent variable Zs, constrained via disentangle-\nment and alignment regularizers. Importantly, the posterior\nover Zs is formulated as a Mixture-of-Experts (MoE), which\nenables robust inference under missing modalities. The gen-\nerative model assumes the following factorized structure:\npθ(V, L, Zv, Zl, Zs) =pθ(V | Zv) pθ(L | Zl)\np(Zv) p(Zl) p(Zs)\nHere, Zv and Zl are modality-specific latent variables for\nvision V and language L, respectively. The shared latent\nZs ∼N(0, I) encodes cross-modal semantics, and while\nit is not used directly in the decoders, it is regulated via aux-\niliary constraints. This simplifies decoding while allowing\nZs to influence training through alignment objectives and\ncross-modal supervision.\nWe seek to maximize the marginal log-likelihood\nlog pθ(V, L), which is lower bounded via:\nlogpθ(V, L) = LELBO+\nDKL(qϕ(Zv, Zl, Zs | V, L)∥pθ(Zv, Zl, Zs | V, L))\nThis implies:\nLELBO = Eqϕ\n\u0014\nlog pθ(V, L, Zv, Zl, Zs)\nqϕ(Zv, Zl, Zs | V, L)\n\u0015\nThe approximate posterior factorizes as:\nqϕ(Zv, Zl, Zs | V, L) = qϕv(Zv | V ) qϕl(Zl | L) qϕs(Zs | V, L)\nwhere qϕs(Zs\n| V, L) is implemented as a mixture of\nmodality-specific experts. Substituting the factorized distri-\nbutions, we expand LELBO as:\nLELBO = Eqϕv ,qϕl,qϕs\n\u0002\nlog pθ(V | Zv) + log pθ(L | Zl)\n\u0003\n+ Eqϕv [log p(Zv) −log qϕv(Zv | V )]\n+ Eqϕl [log p(Zl) −log qϕl(Zl | L)]\n+ Eqϕs [log p(Zs) −log qϕs(Zs | V, L)]\nGrouping terms yields:\nLELBO = Eqϕs(Zs|V,L)\nh\nEqϕv (Zv|V ) log pθv(V | Zv)\ni\n+ Eqϕs(Zs|V,L)\nh\nEqϕl(Zl|L) log pθl(L | Zl)\ni\n−DKL(qϕv(Zv | V )∥p(Zv))\n−DKL(qϕl(Zl | L)∥p(Zl))\n−DKL(qϕs(Zs | V, L)∥p(Zs))\nIn our formulation, the shared posterior qϕs(Zs | V, L) is a\nmixture of unimodal experts:\nqϕs(Zs | V, L) = πvqϕs(Zs | V ) + πlqϕs(Zs | L)\nwhere πv, πl are data-dependent mixture weights. This mix-\nture distribution may not be absolutely continuous with re-\nspect to p(Zs), which causes instability when computing the\nKL divergence. Following prior work in multimodal VAEs,\nwe replace the KL term with the Jensen-Shannon Diver-\ngence:\nJSD(qϕs(Zs | V, L) ∥p(Zs))\nThis leads to the final training objective as in eq. (4):\nLELBO = Eqϕs(Zs|V,L)\nh\nEqϕv (Zv|V ) log pθv(V | Zv)\ni\n+ Eqϕs(Zs|V,L)\nh\nEqϕl(Zl|L) log pθl(L | Zl)\ni\n−DKL(qϕv(Zv | V )∥p(Zv))\n−DKL(qϕl(Zl | L)∥p(Zl))\n−JSD(qϕs(Zs | V, L) ∥p(Zs))\nThis ELBO objective is used during training to learn a dis-\nentangled, semantically aligned latent representation across\nmodalities. Although Zs is not directly used in the recon-\nstruction paths, it plays a vital role in enforcing global se-\nmantic consistency and enables robust inference under miss-\ning modality conditions.\nDisentangled Alignment Constraints\nTo encourage a semantically structured latent space, DiA-\ngnostic VLVAE introduces two complementary regulariza-\ntion losses: an orthogonality constraint to enforce statistical\ndisentanglement, and a contrastive alignment loss to ensure\ncross-modal consistency.\nProposition 1 (Disentanglement via Orthogonality)\nLet\n( ˜Zs, ˜Zv, ˜Zl) be whitened latent vectors with zero mean and\nunit variance. If the decoder is locally linear in latent space,\nthen minimizing the following orthogonality loss:\nLorth = ∥˜Z⊤\ns ˜Zv∥2\nF + ∥˜Z⊤\ns ˜Zl∥2\nF + ∥˜Z⊤\nv ˜Zl∥2\nF\nencourages all latent factors to be mutually uncorrelated.\nUnder the assumptions of Independent Component Analy-\nsis (ICA), such uncorrelatedness implies statistical indepen-\ndence.\nProof Sketch.\nThe Frobenius norm ∥X⊤Y ∥2\nF measures\nthe sum of squared pairwise covariances between compo-\nnents of X and Y . For whitened vectors (zero mean and unit\nvariance), these terms reduce to:\n∥˜Z⊤\ns ˜Zv∥2\nF ∝\nX\ni,j\nCov2( ˜Zs,i, ˜Zv,j)\nand analogously for the other pairs. Minimizing Lorth to zero\nenforces that all pairwise covariances vanish, i.e., that Zs,\nZv, and Zl are mutually uncorrelated. Under ICA assump-\ntions, this guarantees statistical independence of the latent\ncomponents.\n"}, {"page": 12, "text": "Figure 3: t-SNE projections of latent variables for IU X-Ray. Each subfigure shows distributions of language-specific (Zl, blue),\nvision-specific (Zv, red), and shared (Zs, green) representations under four settings: (a) Base VLVAE, (b) with Lorth, (c) with\nLalign, and (d) with both constraints.\nFigure 4: t-SNE projections of latent variables for MIMIC-CXR under the same settings as in Fig. 3. The plots illustrate how\nthe latent space evolves across training objectives and datasets.\nProposition 2 (Alignment via Contrastive Loss)\nLet Zs\nbe the shared latent representation and Zv, Zl the modality-\nspecific latents. Then minimizing the contrastive alignment\nloss:\nLalign = L(v)\nalign + L(l)\nalign\nwhere each term is defined using the InfoNCE objective, e.g.,\nL(v)\nalign = −E\n\"\nlog\nexp(sim(Zs, Zv)/τ)\nP\nZ′v exp(sim(Zs, Z′v)/τ)\n#\n,\nmaximizes a variational lower bound on the mutual infor-\nmation I(Zs; Zv) and I(Zs; Zl), respectively.\nProof Sketch.\nThe InfoNCE objective with K−1 negative\nsamples satisfies:\nI(X; Y ) ≥log K −LNCE\nTherefore, minimizing L(v)\nalign increases a lower bound on\nI(Zs; Zv), encouraging the shared latent to retain relevant\nmodality-specific semantics. The same argument applies to\nL(l)\nalign.\nRemark.\nTogether, Lorth and Lalign enable the model to\nlearn a disentangled yet semantically grounded latent rep-\nresentation that generalizes across modality configurations.\nLatent Structure Visualization\nLatent Disentanglement and Alignment Analysis.\nWe\nvisualize the latent distributions of modality-specific (Zv,\nZl) and shared (Zs) representations using t-SNE on IUXRay\n(Figure 3) and MIMIC-CXR (Figure 4) to assess the impact\nof the disentangled alignment constraints Lorth and Lalign.\nWithout any constraints, the base model exhibits heavy en-\ntanglement across all latents, indicating poor separation of\nmodality-specific and shared semantics. Introducing only\nLorth yields clear separation among Zv, Zl, and Zs, ef-\nfectively disentangling modality-specific features. However,\nthe shared latent remains misaligned, lacking semantic co-\nherence. Conversely, Lalign alone collapses all representa-\ntions into a semantically aligned cluster but compromises\ndisentanglement by blurring modality-specific distinctions.\nWhen both constraints are applied jointly, the resulting\nlatent structure achieves optimal balance: Zv and Zl form\nwell-separated clusters, while Zs aligns closely with both,\nindicating successful capture of shared semantics without\nsacrificing modality identity. This structured organization\nconfirms that the proposed disentangled alignment not only\nenforces statistical independence via orthogonality but also\nencourages semantic consistency through contrastive align-\nment. The consistency of this effect across both datasets\nhighlights DiA’s generalization capability and supports its\ncore contribution: learning modality-resilient, interpretable\nlatent spaces for robust cross-modal report generation.\n"}, {"page": 13, "text": "Table 4: Architectural Specifications of DiA Components.\nComponent\nBase Model / Type\nDetails\nVision Feature Extractor\nEfficientNetB0\nPre-trained on ImageNet; appended with a\nGlobal Context Attention (GCA) module.\nOutput dim: 1024.\nLanguage Feature Extractor\nTransformer Encoder\n6 layers, 8 heads, FF dim 2048, GELU,\ndropout 0.1.\nModality Abstractor\nBidirectional Cross-Attention\n2 layers, 8 heads\nVL-MoE-VAE Encoders\nVision-Specific (qϕv)\nVGG16 + MLP\nPre-trained on ImageNet; final conv fea-\ntures fed to 2-layer MLP for µv, σv.\nLanguage-Specific (qϕl)\nTransformer Encoder\n4 layers, 8 heads, FF dim 1024; outputs\nµl, σl.\nShared Encoder (qϕs)\nMLP (MoE)\nTwo 2-layer expert MLPs (vision/lan-\nguage), hidden size 512; outputs µs, σs.\nVL-MoE-VAE Decoders\nVision Decoder (pθv)\nTransposed CNN\n5-layer transposed conv network.\nLanguage Decoder (pθl)\nTransformer Decoder\n4 layers, 8 heads.\nLLaMA-X Decoder\nTransformer Decoder\n6 layers, 8 heads, 2 KV heads, SwiGLU,\nRoPE positional encoding.\nMarginal ELBO under Missing Language Context\nA critical feature of the DiA framework is its ability to han-\ndle incomplete data, a common scenario in clinical settings\nwhere textual context L may be unavailable during infer-\nence. The Mixture-of-Experts (MoE) design provides a prin-\ncipled way to manage this by allowing the model to fall\nback on unimodal inference from the available vision data.\nThis section details the derivation of the marginal Evidence\nLower Bound (ELBO) that justifies this process, demon-\nstrating that the framework remains theoretically sound even\nwith partial inputs.\nWhen the language modality L is missing (e.g., passed\nas a NULL token), the MoE router learns to down-weight\nthe corresponding expert, effectively conditioning the shared\nposterior only on the vision input. Our goal is to show\nthat the learning objective remains a valid lower bound\non the marginal log-likelihood of the observed vision data,\nlog pθ(V ). The derivation begins with the definition of the\nmarginal log-likelihood and its relationship to the ELBO:\nlog pθ(V ) = log\nZZ\npθ(V, Zv, Zs) dZv dZs\n≥Eqϕ(Zv,Zs|V )\n\u0014\nlog pθ(V, Zv, Zs)\nqϕ(Zv, Zs | V )\n\u0015\nAssuming the posterior factorizes as qϕ(Zv, Zs | V ) =\nqϕv(Zv | V ) qϕs(Zs | V ) and using the generative factor-\nization pθ(V, Zv, Zs) = pθ(V | Zv) p(Zv) p(Zs), we ex-\npand the objective. This expansion yields the final marginal\nELBO for the vision-only case:\nL(V )\nELBO = Eqϕv (Zv|V )\n\u0002\nlog pθ(V | Zv)\n\u0003\n−DKL\n\u0000qϕv(Zv | V ) ∥p(Zv)\n\u0001\n−JSD\n\u0000qϕs(Zs | V ) ∥p(Zs)\n\u0001\nIn this case, the posterior over Zs reduces to the vision-\nspecific expert, i.e., qϕs(Zs | V ), since πl = 0 and πv = 1\nin the MoE formulation:\nqϕs(Zs | V, NULL) = qϕs(Zs | V )\nTherefore, even in the absence of language modality, the\nDiA framework yields a valid and optimized ELBO for uni-\nmodal input. This marginal ELBO retains semantic structure\nthrough Zs and enables modality-resilient report generation\nwithout requiring explicit imputation or retraining.\nImplementation and Architectural Details\nAll models were implemented in PyTorch, and the source\ncode has been provided for full reproducibility. The follow-\ning sections detail the model architectures, training hyperpa-\nrameters, and dataset statistics, with specific configurations\nsummarized in the corresponding tables.\nModel Architectures\nThe detailed configurations of the\ncore DiA components are specified in Table 4. The frame-\nwork uses pre-trained backbones for initial feature extrac-\ntion, including an EfficientNetB0 for the vision extractor and\na 6-layer Transformer for the language extractor. For the\nprobabilistic VL-MoE-VAE module, the modality-specific\nencoders consist of a VGG16+MLP for vision and a 4-layer\nTransformer for language. The final report generation uses a\n6-layer LLaMA-X decoder, which is optimized with features\nlike SwiGLU activation and RoPE positional encodings.\nTraining Hyperparameters\nThe training hyperparame-\nters and computational environment are summarized in Ta-\nble 5. All models were trained for 25 epochs with a batch\nsize of 4 using the AdamW optimizer. We used a learning\nrate of 1 × 10−4 with a linear warmup for the first 10% of\ntraining steps. The latent and embedding dimensions are 256\nand 1024, respectively. The loss term coefficients λ1 (Or-\nthogonality) and λ2 (Alignment), were both set to 0.3 after a\nsearch over the set 0.1, 0.3, 0.5. The experiments were con-\nducted on a single NVIDIA A40 GPU using PyTorch 2.1.\n"}, {"page": 14, "text": "Table 5: Training hyperparameters and computational environment.\nParameter\nValue / Description\nOptimizer\nAdamW (β1 = 0.9, β2 = 0.999, ϵ = 10−8)\nLearning Rate\n1 × 10−4 with linear warmup\nWeight Decay\n1 × 10−5 (excluding bias and LayerNorm)\nBatch Size\n4\nEpochs\n25\nLatent Dim (Zv, Zl, Zs)\n256\nEmbedding Dim (E)\n1024\nλ1 (Orthogonality)\n0.3 (search: {0.1, 0.3, 0.5})\nλ2 (Alignment)\n0.3 (search: {0.1, 0.3, 0.5})\nTemperature (τ)\n0.07 (InfoNCE loss)\nGPU\n1x NVIDIA A40 (48GB)\nSoftware\nPyTorch 2.1, CUDA 12.1\nTable 6: Statistics for IU X-Ray and MIMIC-CXR datasets.\nDataset\nTrain\nVal\nTest\nVocab Size\nAvg. Len.\n% Missing (Test)\nIU X-Ray\n5,229\n747\n1,501\n∼1,000\n33\n∼2%\nMIMIC-CXR\n270,790\n2,130\n3,858\n∼4,000\n58\n∼45%\nDataset Statistics\nTable 6 provides the key statistics for\nthe IU X-Ray and MIMIC-CXR datasets used in our ex-\nperiments. The statistics include the train, validation, and\ntest splits, as well as the vocabulary size and average report\nlength for each dataset. Notably, the table highlights the dif-\nference in data scarcity between the two benchmarks, with\nthe MIMIC-CXR test set having a significantly higher rate\nof missing clinical context (∼45%) compared to IU X-Ray\n(∼2%).\n"}]}