{"doc_id": "arxiv:2512.24058", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.24058.pdf", "meta": {"doc_id": "arxiv:2512.24058", "source": "arxiv", "arxiv_id": "2512.24058", "title": "Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models", "authors": ["Rohit Kumar Salla", "Manoj Saravanan", "Shrikar Reddy Kota"], "published": "2025-12-30T08:07:28Z", "updated": "2025-12-30T08:07:28Z", "summary": "Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are increasingly used in decision-critical domains such as healthcare, law, and finance, yet their reliability remains uncertain. They often make overconfident errors, degrade under input shifts, and lack clear uncertainty estimates. Existing evaluations are fragmented, addressing only isolated aspects. We introduce the Composite Reliability Score (CRS), a unified framework that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric. Through experiments on ten leading open-source LLMs across five QA datasets, we assess performance under baselines, perturbations, and calibration methods. CRS delivers stable model rankings, uncovers hidden failure modes missed by single metrics, and highlights that the most dependable systems balance accuracy, robustness, and calibrated uncertainty.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.24058v1", "url_pdf": "https://arxiv.org/pdf/2512.24058.pdf", "meta_path": "data/raw/arxiv/meta/2512.24058.json", "sha256": "9ebcbb631d6d88e02f0100cef41fcb27b71ccc99a2d37d4e7fbde0a58c31dd80", "status": "ok", "fetched_at": "2026-02-18T02:23:36.093986+00:00"}, "pages": [{"page": 1, "text": "Beyond Hallucinations: A Composite Score for Measuring Reliability\nin Open-Source Large Language Models\nRohit Kumar Salla1, Manoj Saravanan, 1, Shrikar Reddy Kota1\n1Virginia Tech, Department of Electrical and Computer Engineering\nBlacksburg, VA, USA\nrohits25@vt.edu, manoj663@vt.edu, shrikarrikarreddykota@vt.edu\nAbstract\nLarge Language Models (LLMs) like LLaMA, Mistral, and\nGemma are increasingly used in decision-critical domains\nsuch as healthcare, law, and finance, yet their reliability re-\nmains uncertain. They often make overconfident errors, de-\ngrade under input shifts, and lack clear uncertainty estimates.\nExisting evaluations are fragmented, addressing only isolated\naspects.\nWe introduce the Composite Reliability Score (CRS), a uni-\nfied framework that integrates calibration, robustness, and\nuncertainty quantification into a single interpretable met-\nric. Through experiments on ten leading open-source LLMs\nacross five QA datasets, we assess performance under base-\nlines, perturbations, and calibration methods. CRS delivers\nstable model rankings, uncovers hidden failure modes missed\nby single metrics, and highlights that the most dependable\nsystems balance accuracy, robustness, and calibrated uncer-\ntainty.\nCode — https://github.com/rohitsalla/CRS.git\nIntroduction\nOpen-source Large Language Models (LLMs) are increas-\ningly applied in domains like medicine, finance, and law,\nwhere reliability is crucial. Despite strong benchmark per-\nformance, they often remain overconfident (Chhikara 2025),\nbrittle under distribution shifts (Bakman et al. 2025), and\nprovide unreliable uncertainty estimates (Gal and Ghahra-\nmani 2016; Xia et al. 2025). Alignment and fine-tuning can\nfurther degrade calibration (Xiao et al. 2025; Wang et al.\n2025; Liu 2025). Current evaluations accuracy, BLEU, or\nisolated reliability metrics offer fragmented insights and risk\noverlooking weaknesses.\nWe propose the Composite Reliability Score (CRS), a\nunified metric combining calibration, robustness, and uncer-\ntainty into a single interpretable framework. Evaluating ten\nleading open-source LLMs across five QA datasets, we show\nthat CRS captures trade-offs across reliability dimensions,\nestablishes consistent model rankings, and provides action-\nable guidance for deployment.\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nOur contributions:\n1. A unified reliability metric (CRS) integrating calibration,\nrobustness, and uncertainty.\n2. A large-scale evaluation of ten open-source LLMs on five\nQA datasets.\nRelated Work\nCalibration.\nCalibration captures how well model con-\nfidence matches correctness. LLMs often show overconfi-\ndence due to scale and training regimes (Jiang et al. 2021),\nand recent work confirms this persists even after alignment\n(Xiao et al. 2025). Standard metrics include Expected Cal-\nibration Error (ECE) and Brier Score, with post-hoc fixes\nsuch as temperature scaling.\nRobustness.\nNeural models are brittle to small input\nchanges, and in NLP this fragility appears under typos, para-\nphrasing, or adversarial attacks (Jin et al. 2020). Recent eval-\nuations highlight that LLM robustness should be tested un-\nder realistic distribution shifts (Bakman et al. 2025). We in-\ncorporate robustness as a core reliability dimension.\nUncertainty Quantification.\nUncertainty estimation is\nkey for detecting errors and distribution shift. Classical\nmethods like Monte Carlo dropout and deep ensembles\n(Lakshminarayanan, Pritzel, and Blundell 2017) remain in-\nfluential, while newer approaches exploit representation sta-\nbility and confidence–consistency signals (Vashurin 2025).\nThese advances motivate treating UQ as a first-class relia-\nbility pillar.\nUnified Metrics.\nAggregated benchmarks such as GLUE\nand SuperGLUE (Wang et al. 2019b,a) measure accuracy\nbut neglect reliability. Surveys show calibration, robustness,\nand uncertainty are still siloed (Xia et al. 2025). CRS ad-\ndresses this by unifying them into a single interpretable\nscore.\nThe Composite Reliability Score (CRS)\nFramework\nWe define reliability as the integration of three compo-\nnents: Calibration, Robustness, and Uncertainty Quan-\ntification. Each component is normalized to [0, 1] so that\nhigher values consistently indicate better reliability. The\narXiv:2512.24058v1  [cs.CL]  30 Dec 2025\n"}, {"page": 2, "text": "CRS aggregates these components to provide a unified mea-\nsure.\nPillar 1: Calibration (C)\nCalibration measures how closely a model’s predicted confi-\ndence matches its empirical accuracy. We use Expected Cal-\nibration Error (ECE), which bins predictions by confidence\nand computes the difference between mean confidence and\naccuracy. Lower ECE indicates better calibration. To con-\nvert ECE into an interpretable score where higher is better,\nwe use:\nC = max\n\u0012\n0, 1 −ECEmodel\nECEmax\n\u0013\n.\nHere, ECEmax denotes the largest ECE observed among all\nbaseline models. This anchor yields a simple and monotonic\nnormalization that preserves relative differences. Although\nthis approach can amplify small gaps when ECE values are\nclose, it provides a practical scale for comparing heteroge-\nneous models. Alternative normalizations such as percentile\nor logistic transforms may reduce this sensitivity but are left\nfor future work.\nPillar 2: Robustness (R)\nRobustness quantifies how well a model maintains accuracy\nunder perturbations including typos, paraphrases, and adver-\nsarial rewrites. For each dataset we compute:\nAccuracy Drop = 1\nN\nN\nX\ni=1\n(Accclean,i −Accperturbed,i) .\nWe define the robustness score as the fraction of perfor-\nmance retained:\nR = 1 −Avg. Accuracy Drop\nAvg. Accclean\n.\nThis formulation isolates relative degradation rather than ab-\nsolute accuracy which allows robustness comparisons across\nmodels with different baseline skill levels. It does not cap-\nture task difficulty, but provides a consistent degradation\nmetric across datasets.\nPillar 3: Uncertainty Quantification (U)\nA reliable model should assign higher uncertainty to in-\ncorrect predictions. We estimate predictive uncertainty us-\ning MC Dropout and Ensembles and evaluate their quality\nthrough AUROC which measures the separability between\ncorrect and incorrect predictions. An AUROC of 0.5 cor-\nresponds to random guessing and 1.0 indicates perfect dis-\ncrimination. We normalize AUROC as:\nU = AUROC −0.5\n0.5\n.\nThis linear mapping yields a score in [0, 1] and preserves\nordering across models. While nonlinear transforms could\nemphasize gains near the high end of AUROC, the linear\nform maintains clarity and comparability. For each model\nwe report the better of MC Dropout and Ensemble based\nestimates.\nComposite Integration\nThe final Composite Reliability Score integrates the three\ncomponents:\nCRS = αC + βR + γU\nwhere α + β + γ = 1. For general evaluation we use bal-\nanced weights α = β = γ = 1/3. This setting assumes that\ncalibration, robustness, and uncertainty contribute equally to\noverall reliability.\nTo assess sensitivity we tested two alternative weight con-\nfigurations: a calibration-focused setting (α = 0.5, β =\n0.25, γ = 0.25) and a robustness-focused setting (α =\n0.2, β = 0.5, γ = 0.3). The relative ordering of top and\nbottom ranked models remained unchanged indicating that\nCRS is stable under reasonable weight variation. Domain-\nspecific deployments may adjust weights to reflect priorities\nsuch as calibration for medical tasks or robustness for adver-\nsarial environments.\nWe interpret CRS using three levels: scores ≥0.8 indicate\nhigh reliability suitable for deployment with minimal super-\nvision, scores between 0.6 and 0.8 indicate moderate relia-\nbility suitable for use with human oversight, and scores be-\nlow 0.6 signal limited reliability and unsuitability for safety-\ncritical environments.\nExperimental Setup\nModels\nWe evaluate ten open-source LLMs that span a broad range\nof sizes and architectures. The models include LLaMA-\n3-7B, Mistral-7B, Falcon-7B, Kimi K2 (15B), Llama 4\nScout (17B), Mistral-8x22B, Qwen3-22B, MiniMax-Text-\n01 (25B), Gemma 2 (27B), and DeepSeek R1 (27B). This\nselection provides a representative set of current generation\nmodels for reliability benchmarking.\nDatasets and Evaluation Protocol\nWe use five question-answering datasets: TriviaQA, Nat-\nuralQuestions, SQuAD 2.0, MedQA, and ARC. These\ndatasets cover general knowledge, reading comprehension,\nmedical reasoning, and multi-step reasoning which supports\nevaluation across diverse query types.\nBaseline calibration.\nFor each model we compute Ex-\npected Calibration Error (ECE), Brier Score, and Negative\nLog-Likelihood (NLL) on the clean test sets. These metrics\nquantify confidence alignment before applying any pertur-\nbations or calibration interventions.\nRobustness testing.\nRobustness is assessed by applying\nthree controlled input perturbations to every dataset:\n1. Noisy input. We simulate typographical noise by swap-\nping characters within words at a fixed rate of 5% of to-\nkens.\n2. Paraphrased input. We apply back-translation using\nMarianMT (English–German–English) to generate se-\nmantically equivalent rephrasings.\n3. Adversarial input. We generate targeted perturba-\ntions with TextFooler which replaces key tokens using\nembedding-based synonym selection.\n"}, {"page": 3, "text": "For each model we compute accuracy on clean and per-\nturbed queries and use the average performance drop in the\nCRS framework.\nUncertainty estimation.\nWe evaluate uncertainty using\ntwo approximation methods:\n1. MC Dropout. We enable dropout with probability 0.1\nat inference and perform 10 stochastic forward passes.\nThe variance across predicted probabilities is used as the\nuncertainty signal.\n2. Ensembles. We construct three-model ensembles using\ncheckpoints trained with different random seeds from the\nsame model family. Prediction variance across ensemble\nmembers serves as the uncertainty estimate.\nFor both methods we compute AUROC for error detection\non each dataset which forms the normalized uncertainty\nscore.\nCalibration interventions.\nWe evaluate two post-hoc cal-\nibration techniques. Temperature scaling learns a single\nscalar parameter on a held-out validation set and rescales\nlogits at inference. Isotonic regression fits a monotonic map-\nping between predicted confidence and accuracy. Perfor-\nmance after calibration is measured using ECE, Brier Score,\nand NLL to quantify calibration improvements.\nResults and Analysis\nWe present results for each component of the reliability\nframework followed by the final CRS ranking. All reported\nmetrics are averaged over the five datasets.\nBaseline Calibration Performance\nTable 2 summarizes baseline calibration. Mistral-8x22B\nachieves the lowest ECE, Brier Score, and NLL, while\nFalcon-7B is worst calibrated which reflects strong over-\nconfidence. Mid-sized models such as LLaMA-3-7B and\nGemma 2 show moderate calibration quality. These findings\nindicate that model size alone does not determine calibration\nand that accuracy cannot be used as a proxy for reliability.\nRobustness to Input Perturbations\nAdversarial inputs cause the largest performance loss\nwith an average drop of 11.2 percent. Mistral-8x22B and\nDeepSeek R1 show the strongest robustness with 6–7 per-\ncent degradation while 7B models such as Falcon-7B and\nLLaMA-3-7B are most affected with drops exceeding 10\npercent. These results show that robustness varies widely\nacross models and remains an essential component of reli-\nability.\nEfficacy of Uncertainty Quantification\nTable 3 reports AUROC for error detection. All models\nperform above chance which indicates meaningful uncer-\ntainty signals. Ensembles outperform MC Dropout for ev-\nery model. Mistral-8x22B, DeepSeek R1, and Qwen3-235B\nreach AUROC values near 0.90, whereas 7B models remain\nbelow 0.75. These results suggest that uncertainty quality\nimproves with model scale and training sophistication.\nImpact of Calibration Interventions\nPost-hoc calibration improves reliability for all models. Ta-\nble 4 shows that both temperature scaling and isotonic re-\ngression reduce ECE across datasets. LLaMA-3-7B im-\nproves from 0.057 to 0.046 and Mistral-8x22B improves\nfrom 0.031 to 0.025. Temperature scaling is simple and ef-\nfective while isotonic regression yields the strongest gains.\nComposite Reliability Score Ranking\nTable 1 presents the final CRS using equal weights. Mistral-\n8x22B leads with 0.81 driven by strong performance across\nall pillars. DeepSeek R1 and Qwen3-235B follow with\nscores around 0.75 while the 7B models rank lowest. Falcon-\n7B receives a CRS of 0.52 indicating limited reliability.\nTo evaluate sensitivity we tested two alternative weight\nchoices. A calibration-focused setting (0.5, 0.25, 0.25) and\na robustness-focused setting (0.2, 0.5, 0.3) both preserved\nthe ordering of the top and bottom three models. We also\ncomputed bootstrap confidence intervals over 100 samples;\nCRS variance remained below 0.02 for high-ranked models\nwhich indicates that small differences such as 0.75 vs 0.76\nare not statistically meaningful.\nWhy Holistic Integration Matters\nReliability is multi–dimensional, and individual metrics of-\nten lead to conflicting conclusions. For instance, Mistral-7B\nand LLaMA-3-7B show comparable robustness, yet differ\nsubstantially once calibration and uncertainty signals are in-\ncorporated. CRS resolves such inconsistencies by combin-\ning these pillars into a single interpretable score.\nAccuracy Is Not Enough.\nHigh accuracy does not im-\nply reliability. Models such as LLaMA-3-7B achieve strong\nclean accuracy yet exhibit poor calibration and weak uncer-\ntainty estimates, demonstrating the need for composite met-\nrics that capture behavior beyond correctness.\nRole of Calibration.\nCRS uses post-hoc calibrated pre-\ndictions, which reduces distortions caused by raw overconfi-\ndence and yields more comparable ECE values across mod-\nels.\nWeight Sensitivity.\nTesting multiple weight configura-\ntions shows that top and bottom model rankings remain\nstable, indicating that CRS is robust to reasonable priority\nshifts across reliability dimensions.\nDataset Sensitivity.\nLeave-one-out analysis shows limited\nvariation (average deviation < 0.03), and no model changes\nreliability tier, suggesting that CRS captures general behav-\nior rather than dataset-specific artifacts.\nNormalization.\nPillar normalization to [0, 1] places het-\nerogeneous metrics on a common scale. Although worst-\ncase anchoring may exaggerate small gaps, calibrated ECE\nand dataset averaging mitigate this. Alternative schemes can\nbe explored in future work.\nOverall, CRS provides a coherent reliability overview,\nhighlights failure modes missed by single metrics, and re-\nmains stable across perturbations to weights, normalization,\nand dataset composition.\n"}, {"page": 4, "text": "Table 1: Final Composite Reliability Score (CRS) ranking with normalized component scores.\nModel\nParams (B)\nCalibration (C)\nRobustness (R)\nUncertainty (U)\nCRS\nTier\nMistral-8x22B\n22\n0.91\n0.78\n0.73\n0.81\nHigh\nQwen3-235B\n22\n0.84\n0.74\n0.70\n0.76\nModerate\nDeepSeek R1 0528\n27\n0.87\n0.76\n0.63\n0.75\nModerate\nLlama 4 Scout\n17\n0.81\n0.70\n0.64\n0.72\nModerate\nMiniMax-Text-01\n25\n0.81\n0.69\n0.63\n0.71\nModerate\nGemma 2\n27\n0.71\n0.68\n0.71\n0.70\nModerate\nKimi K2\n15\n0.68\n0.66\n0.67\n0.67\nModerate\nMistral-7B\n7\n0.52\n0.65\n0.58\n0.63\nModerate\nLLaMA-3-7B\n7\n0.16\n0.54\n0.44\n0.57\nLow\nFalcon-7B\n7\n0.00\n0.51\n0.41\n0.52\nLow\nTable 2: Baseline calibration metrics averaged across five\nQA datasets. Lower values are better.\nModel\nAvg. ECE\nAvg. Brier Score\nAvg. NLL\nMistral-8x22B\n0.031\n0.128\n0.332\nDeepSeek R1 0528\n0.032\n0.132\n0.352\nQwen3-235B\n0.033\n0.133\n0.360\nLlama 4 Scout\n0.035\n0.138\n0.382\nMiniMax-Text-01\n0.035\n0.138\n0.380\nGemma 2\n0.038\n0.143\n0.410\nKimi K2\n0.040\n0.147\n0.418\nMistral-7B\n0.044\n0.153\n0.448\nLLaMA-3-7B\n0.057\n0.169\n0.526\nFalcon-7B\n0.062\n0.179\n0.566\nTable 3: Average AUROC for error detection using the better\nof MC Dropout or Ensemble. Higher is better.\nModel\nBest UQ Method\nAvg. AUROC\nMistral-8x22B\nEnsemble\n0.882\nDeepSeek R1 0528\nEnsemble\n0.878\nQwen3-235B\nEnsemble\n0.872\nMiniMax-Text-01\nEnsemble\n0.868\nLlama 4 Scout\nEnsemble\n0.852\nGemma 2\nEnsemble\n0.852\nKimi K2\nEnsemble\n0.830\nMistral-7B\nEnsemble\n0.810\nLLaMA-3-7B\nEnsemble\n0.740\nFalcon-7B\nEnsemble\n0.716\nTable 4: Effectiveness of calibration interventions measured\nwith ECE.\nModel\nECE (Baseline)\nTemp. Scaling\nIsotonic Reg.\nLLaMA-3-7B\n0.057\n0.050\n0.046\nMistral-7B\n0.044\n0.039\n0.035\nFalcon-7B\n0.062\n0.056\n0.052\nLlama 4 Scout\n0.035\n0.031\n0.028\nQwen3-235B\n0.033\n0.028\n0.025\nMistral-8x22B\n0.031\n0.028\n0.025\nConclusion and Future Work\nWe introduced CRS, a unified metric combining calibration,\nrobustness, and uncertainty to assess LLM reliability. Across\nten open-source models and five QA datasets, CRS reveals\nweaknesses obscured by accuracy alone and produces con-\nsistent rankings. Mistral-8×22B shows the strongest overall\nreliability, while 7B models display notable calibration and\nUQ limitations.\nLimitations.\nOur evaluation focuses on extractive QA;\ngenerative tasks may require adapted definitions of calibra-\ntion and robustness. Hallucination behavior is not directly\nmeasured. Normalization and weighting remain heuristic\nand could be improved with task-aware or learned formu-\nlations.\nFuture Work.\nExtending CRS to generative settings, in-\ncorporating hallucination metrics, and evaluating multilin-\ngual or OOD robustness are promising next steps. Learn-\ning task-specific weights or integrating fairness and prompt-\ninjection robustness would move CRS toward a more com-\nprehensive deployment-oriented reliability framework.\nCRS provides a practical foundation for unified reliability\nassessment and supports ongoing efforts to develop trust-\nworthy foundation models.\nReferences\nBakman, Y.; Yaldiz, D. N.; Kang, S.; Zhang, T.; Buyukates,\nB.; Avestimehr, S.; and Karimireddy, S. P. 2025. Reconsid-\nering LLM Uncertainty Estimation Methods in the Wild. In\nAssociation for Computational Linguistics (ACL).\nChhikara, P. 2025. Overconfidence, Calibration, and Dis-\ntractor Effects in Large Language Models. In arXiv Preprint.\nGal, Y.; and Ghahramani, Z. 2016. Dropout as a Bayesian\nApproximation: Representing Model Uncertainty in Deep\nLearning. In International Conference on Machine Learning\n(ICML).\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2021. How Can\nWe Know When Language Models Know? In Transactions\nof the Association for Computational Linguistics (TACL).\nJin, D.; Jin, Z.; Zhou, J. T.; and Szolovits, P. 2020. Is BERT\nReally Robust? A Strong Baseline for Natural Language At-\ntack on Text Classification and Entailment. In AAAI Confer-\nence on Artificial Intelligence.\nLakshminarayanan, B.; Pritzel, A.; and Blundell, C. 2017.\nSimple and Scalable Predictive Uncertainty Estimation Us-\n"}, {"page": 5, "text": "ing Deep Ensembles. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nLiu, H. 2025. On Calibration of LLM-based Guard Models\nfor Reliable Moderation.\nIn International Conference on\nLearning Representations (ICLR).\nVashurin, R. 2025. CoCoA: A Generalized Approach to Un-\ncertainty Quantification by Integrating Confidence and Con-\nsistency of LLM Outputs. In arXiv Preprint.\nWang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.;\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. Su-\nperGLUE: A Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS).\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understand-\ning. In International Conference on Learning Representa-\ntions (ICLR).\nWang, Z.; Shi, Z.; Zhou, H.; Gao, S.; Sun, Q.; and Li, J.\n2025. Towards Objective Fine-Tuning: How LLMs’ Prior\nKnowledge Causes Potential Poor Calibration? In Associa-\ntion for Computational Linguistics (ACL).\nXia, Z.; Xu, J.; Zhang, Y.; and Liu, H. 2025. A Survey of\nUncertainty Estimation Methods on Large Language Mod-\nels. In Findings of the Association for Computational Lin-\nguistics (ACL Findings).\nXiao, J.; Hou, B.; Wang, Z.; Jin, R.; Long, Q.; Su, W. J.;\nand Shen, L. 2025. Restoring Calibration for Aligned Large\nLanguage Models. In International Conference on Machine\nLearning (ICML).\n"}]}