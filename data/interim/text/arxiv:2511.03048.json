{"doc_id": "arxiv:2511.03048", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.03048.pdf", "meta": {"doc_id": "arxiv:2511.03048", "source": "arxiv", "arxiv_id": "2511.03048", "title": "ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment", "authors": ["Anthony Hevia", "Sanjana Chintalapati", "Veronica Ka Wai Lai", "Thanh Tam Nguyen", "Wai-Tat Wong", "Terry Klassen", "Lucy Lu Wang"], "published": "2025-11-04T22:45:06Z", "updated": "2025-11-04T22:45:06Z", "summary": "We present ROBOTO2, an open-source, web-based platform for large language model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2 streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process via an interactive interface that combines PDF parsing, retrieval-augmented LLM prompting, and human-in-the-loop review. Users can upload clinical trial reports, receive preliminary answers and supporting evidence for ROB2 signaling questions, and provide real-time feedback or corrections to system suggestions. ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and data released to foster reproducibility and adoption. We construct and release a dataset of 521 pediatric clinical trial reports (8954 signaling questions with 1202 evidence passages), annotated using both manually and LLM-assisted methods, serving as a benchmark and enabling future research. Using this dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into current model capabilities and ongoing challenges in automating this critical aspect of systematic review.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.03048v1", "url_pdf": "https://arxiv.org/pdf/2511.03048.pdf", "meta_path": "data/raw/arxiv/meta/2511.03048.json", "sha256": "f19eb5168e3ee744c85a575a4123c429fe99519eb0dff19986329ba0893e2bec", "status": "ok", "fetched_at": "2026-02-18T02:28:26.535615+00:00"}, "pages": [{"page": 1, "text": "ROBOTO2: An Interactive System and Dataset for LLM-assisted\nClinical Trial Risk of Bias Assessment\nAnthony Hevia1∗\nSanjana Chintalapati1∗\nVeronica Ka Wai Lai2\nThanh Tam Nguyen3\nWai-Tat Wong4\nTerry Klassen5\nLucy Lu Wang1\n1University of Washington\n2The Hospital for Sick Children\n3University of Bologna\n4The Chinese University of Hong Kong\n5University of Saskatchewan\n{hevia, lucylw}@uw.edu\nAbstract\nWe present ROBOTO2, an open-source, web-\nbased platform for large language model\n(LLM)-assisted risk of bias (ROB) assessment\nof clinical trials. ROBOTO2 streamlines the tra-\nditionally labor-intensive ROB v2 (ROB2) an-\nnotation process via an interactive interface that\ncombines PDF parsing, retrieval-augmented\nLLM prompting, and human-in-the-loop re-\nview. Users can upload clinical trial reports,\nreceive preliminary answers and supporting ev-\nidence for ROB2 signaling questions, and pro-\nvide real-time feedback or corrections to sys-\ntem suggestions. ROBOTO2 is publicly avail-\nable at https://roboto2.vercel.app/, with code\nand data released to foster reproducibility and\nadoption. We construct and release a dataset of\n521 pediatric clinical trial reports (8954 signal-\ning questions with 1202 evidence passages), an-\nnotated using both manually and LLM-assisted\nmethods, serving as a benchmark and enabling\nfuture research. Using this dataset, we bench-\nmark ROB2 performance for 4 LLMs and pro-\nvide an analysis into current model capabilities\nand ongoing challenges in automating this crit-\nical aspect of systematic review.1\n1\nIntroduction\nClinical trials, especially when aggregated in sys-\ntematic reviews, provide the highest quality of ev-\nidence for clinical care. While many steps in the\nsystematic review pipeline have seen increasing\nautomation (Marshall and Wallace, 2019; Khalil\net al., 2021; Alshami et al., 2023), especially with\nthe advent of LLMs and associated technology, as-\nsessing the quality of evidence in individual trials,\nspecifically evaluating risk of bias (ROB), remains\na critical and time-consuming bottleneck.\nThe Cochrane Risk of Bias tool version 2\n(ROB2)2 standardizes evaluation by asking 22 sig-\n∗denotes equal contribution\n1Dataset and code at https://github.com/larchlab/ROBoto2\n2https://methods.cochrane.org/bias/resources/rob-2-\nrevised-cochrane-risk-bias-tool-randomized-trials\nnaling questions over 5 domains and computing\nan overall judgment about risk of bias. However,\napplying ROB2 is time-consuming, taking trained\nreviewers 30+ minutes per clinical trial report. This\nlimits scalability for large systematic reviews syn-\nthesizing hundreds or thousands of trials.\nPrevious systems such as RobotReviewer (Mar-\nshall et al., 2016) and others (Marshall et al., 2014)\nexplored automating an earlier version of the ROB\nassessment (ROB) via supervised models, but prac-\ntical, high-quality automation for ROB2 remains\nelusive. We therefore introduce ROBOTO2, a web-\nbased platform supporting human-AI collabora-\ntive ROB2 assessment. ROBOTO2 integrates PDF\nparsing, within-document evidence retrieval, LLM\nprompting, and ROB2 logic to provide initial an-\nswers and rationales for each signaling question.\nExperts can accept, modify, or override sugges-\ntions, with feedback captured for future improve-\nment. Using ROBOTO2, our medical collaborators\nconducted ROB2 assessments on 521 pediatric clin-\nical trials—245 via fully manual review and 276\nusing the LLM-assisted workflow—yielding a new\ndataset for benchmarking and research.\nWe evaluate retrieval methods and four LLMs\n(Llama-3.3-70B-Instruct, GPT-3.5-Turbo, GPT-4o,\nand Claude 3.5-Sonnet) on the 245 manual assess-\nments subset, finding that LLMs remain overly con-\nservative compared to human reviewers, frequently\nopting for high-risk or “No Information” judgments\neven when evidence is present. Larger context win-\ndows and more retrieved evidence somewhat miti-\ngate these tendencies, but fully automated, accurate\nROB2 assessment remains challenging.\nTo summarize, we contribute the following:\n• We introduce the ROBOTO2 system, a public\nweb tool (code and API available) that supports\na human-AI collaborative pipeline for clinical\ntrial ROB2 assessment; the system integrates\ndocument preprocessing, passage retrieval, LLM\nprompting, and interactive expert review;\narXiv:2511.03048v1  [cs.CL]  4 Nov 2025\n"}, {"page": 2, "text": "Figure 1: ROBOTO2 system pipeline. Given a clinical trial PDF as input, ROBOTO2 first preprocesses the document\nto extract and embed paragraphs. Then, a QA module iterates through all of the questions of the ROB2 assessment\nto identify evidence passages and prompt GPT3.5 to answer the question based on the retrieved evidence.\n• We release a dataset of 521 ROB2 assessments\n(8954 questions; 1202 evidence passages), in-\ncluding both manual and LLM-assisted annota-\ntions by medical experts, conducted in the con-\ntext of an ongoing, real-world systematic review\nof pediatric clinical trial literature;\n• We benchmark retrieval strategies and 4 LLMs\non this dataset, providing the first evaluation of\nLLM-assisted ROB2 assessment. Our analysis\nhighlights current model limitations and direc-\ntions for future improvement.\n2\nRelated Work\nAutomating systematic review\nPrior work on\nautomating systematic reviews have investigated\nways to automate the retrieval of relevant papers\non a review topic (Choong et al., 2014; Portenoy\nand West, 2020; van de Schoot et al., 2021), gaug-\ning the quality of clinical trials via risk of bias\nassessment (Marshall et al., 2014, 2016; Suster\net al., 2021), extracting PICO (population, inter-\nvention, comparator, outcome) elements (Wallace\net al., 2016; Nye et al., 2018; Jin and Szolovits,\n2018; Hu et al., 2023), extracting numerical re-\nsults (Yun et al., 2024; Naik et al., 2024), classify-\ning the direction of evidence, also called evidence\ninference (Lehman et al., 2019; DeYoung et al.,\n2020), as well as synthesizing and summarizing re-\nsults across different studies (Wallace et al., 2020;\nDeYoung et al., 2021; Wang et al., 2022; Sanchez-\nGraillet et al., 2022; Shaib et al., 2023). Our work\nbuilds upon this prior work, especially towards as-\nsessing the quality of trials via LLM-assisted risk\nof bias analysis, extending to v2 of the ROB tool.\nROB analysis\nThe ROB assessment question-\nnaire from Higgins et al. (2011) and Sterne et al.\n(2019) can be used to determine the extent to which\nrandomized control trials are at risk of bias. Suster\net al. (2021) provide quality ratings for bodies of ev-\nidence, and found that some risk factors for quality\nhave good accuracy when automatically assessed,\nwhile others do not due to data scarcity. RobotRe-\nviewer (Marshall et al., 2016) introduced a system\nthat automatically assigns ROB categorizations to\nrandomized control trials using a trained language\nmodel. We extend this work by (i) introducing a\ndataset corresponding to the newer and more reli-\nable version of the ROB tool (ROB2) (Sterne et al.,\n2019), (ii) creating an annotation system geared\ntowards supporting a researcher in the loop (Jardim\net al., 2022), which leverages in-document retrieval\nand LLMs to answer signaling questions and iden-\ntify rationales from the source articles, and (iii) con-\nducting experiments and analysis demonstrating\nthe performance and limitations of current LLMs\nin supporting this task.\n3\nBackground\nWe measure risk of bias of randomized trials using\nthe Cochrane ROB2 tool.3 The ROB2 tool assesses\nrisk along five domains that can introduce bias into\nthe results of a randomized trial:\nD1: Randomization process\nD2: Deviations from intended interventions\nD3: Missing outcome data\nD4: Measurement of the outcome\nD5: Selection of the reported result\nEach domain consists of 3-7 signaling questions,\nwhich help gather information and contribute to\nthe final risk classification. For example, this D2\nquestion assesses bias due to unblinded treatment\n3ROB2 replaces its predecessor ROB after a formal eval-\nuation identified areas for improvement (Sterne et al., 2019).\nROB2 includes questions measuring newly identified ways\nthat bias arise in randomized trials.\n"}, {"page": 3, "text": "assignment: “Were participants aware of their as-\nsigned intervention during the trial?” There are five\nresponse options for each signaling question: (1)\nYes; (2) Probably yes; (3) Probably no; (4) No; and\n(5) No information. All questions in App. A.\nThe ROB2 assessment is hierarchical. Signaling\nquestion responses for each domain first contribute\nto domain-level judgments for risk of bias, then\ndomain-level judgments provide the basis for an\noverall risk of bias judgment. The tool provides\nflowcharts for computing the risk of each domain\nbased on the answers to signaling questions (e.g.,\nFigure 3 in App. A) as well as for computing over-\nall risk. Domain-level and overall risk are assessed\nas either low risk, some concerns, or high risk.\nIn ROBOTO2, we model the ROB2 assessment\nas a document-level question-answering (QA) task.\nWe use each signaling question as a query to re-\ntrieve relevant evidence passages from the trial re-\nport, then generate an answer based on the retrieved\nevidence. Answers are validated by a user who is\nconducting the assessment. The final risk assess-\nment is produced by implementing the flowchart\nlogic provided by the ROB2 tool.\n4\nROBOTO2 System Pipeline\nFigure 1 shows the ROBOTO2 pipeline. A user up-\nloads a PDF of a clinical trial report. We (i) prepro-\ncess it to extract paragraphs of text; (ii) embed each\nparagraph using a document embedding model and\nindex them for within-document retrieval; and then,\nfor each signaling question from the ROB2 assess-\nment tool, we (iii) embed the signaling question,\nretrieve the top-k similar paragraphs from the pa-\nper, and prompt an LLM to answer the question\nusing the top-k paragraphs as context. We also ex-\nperiment with providing all passages of text (full\npaper) as input for models with large input context\nwindows. Details follow.\nPreprocessing PDFs\nWe convert each PDF into\nstandardized JSON format using the S2ORC-\ndoc2json library (Lo et al., 2020).4 The output\nJSON contains a list of paragraphs in the paper,\ntheir section headers, and metadata elements such\nas the paper’s title, authors, and abstract.\nEmbedding paragraphs for retrieval\nWe com-\npute embeddings for each paragraph in the up-\nloaded paper using Sentence-Transformers all-\nMiniLM-L6-v2 (Reimers and Gurevych, 2019) and\n4https://github.com/allenai/s2orc-doc2json\nconstruct a key-value store for retrieval. Evaluation\nof the retriever and alternate methods is described\nin App. B. For each signaling question, we embed\nthe question text using the same model and use\ncosine similarity to identify the top-k paragraphs\nto use as context for the QA module.\nAnswering signaling questions\nWe then prompt\nan LLM to answer each ROB2 signaling question\nusing an instruction prompt based on the ROB2\nquestionnaire and with the retrieved evidence para-\ngraphs as context. Prompt templates and a com-\nplete example are given in App. C.\nCollecting user feedback\nWhen conducting as-\nsessments with ROBOTO2, users can modify and\nprovide feedback on all aspects of the assessment.\nWhile we show the top-3 retrieved paragraphs by\ndefault, users can add further paragraphs by select-\ning from the JSON parse. They can also provide\nfeedback on the accuracy of retrieved passages via\nup- or downvotes, and modify the model-predicted\nanswers and rationales (called “Explanation” in\nROBOTO2). ROBOTO2 retains the original LLM\nresponses and rationales, as well as the versions\nconfirmed or edited by expert users. We include\nthese user modifications as part of our dataset.\nDomain-level and overall judgments\nWe imple-\nment the logic provided by the ROB2 flowcharts\n(e.g., Figure 3 in App. A) to produce domain-level\nand overall risk of bias judgments. We visualize\nthese at the end of each domain section and as a\nsummative visualization when users complete the\nROB2 assessment, e.g., three high risk domain-\nlevel judgments yields a “High Risk” overall rating\nvisualized as follows:\nSystem implementation\nPart of the ROBOTO2\nworkflow is shown in Figure 2 (web app at\nhttps://roboto2.vercel.app/). The web interface is\nwritten in React and Typescript. It leverages Trans-\nformers.js for client-side embedding and retrieval,\nand a back-end API in Python and FastAPI for\ndocument parsing and calls to LLM services.\n"}, {"page": 4, "text": "Figure 2: Screenshot of ROBOTO2 assisting with a question from Domain 2. The user can modify the model-\nprovided answer and explanation and rate reference paragraphs.\n5\nAnnotated Evaluation Dataset\nOur dataset consists of 521 ROB2 assessments\n(245 completed using the Cochrance ROB2 Excel\ntool and 276 with LLM assistance via ROBOTO2).\nThese ROB2 assessments were conducted as part\nof an independent research project aiming to sys-\ntematically evaluate the risk of bias of all child\nhealth clinical trials; we re-purpose the data in this\nwork to explore the role and feasibility of LLMs in\nsupporting this aspect of systematic review.\nAnnotation procedure\nAn initial corpus of child\nhealth clinical trial reports was constructed by\nsearching the Cochrane Central Register of Con-\ntrolled Trials, filtering for pediatric clinical trials\nbased on the procedures described in Boluyt et al.\n(2008), and identifying 2334 matching clinical trial\nreports published 1991-2020. We sampled trial\nreports from this corpus for annotation.\nFor a subset of 245 reports, a group of expert\nraters completed assessments manually using the\nCochrane tool, an Excel sheet with macros imple-\nmenting the logic of the ROB2 assessment. To\nsupport judgments, annotators identified evidence\npassages manually from paper PDFs for a subset\nof questions and copied these into the Excel sheet.\nFor each clinical trial, the data consists of the pa-\nper PDF for the trial report, as well as judgments\nfor each signaling question, evidence passages ex-\ntracted from the paper for a subset of questions,\ndomain-level judgments, and the overall risk as-\nsessment score. Five expert annotators participated\nin annotations, and all annotators have graduate\ndegrees in public health, epidemiology, medical\nsciences, or clinical practice, as well as experience\nconducting systematic reviews. This set of 245\n"}, {"page": 5, "text": "Low risk\nSome concerns\nHigh risk\nDomain 1\n234\n243\n44\nDomain 2\n287\n171\n63\nDomain 3\n450\n35\n35\nDomain 4\n406\n60\n54\nDomain 5\n332\n272\n34\nPaper-level\n64\n301\n156\nTable 1: Distribution of domain- and paper-level risk of\nbias judgments in our dataset.\npapers, annotated using the current gold-standard\nROB2 review process, is used for all LLM evalua-\ntion and comparison reported in this paper.\nAn additional 276 papers were annotated sep-\narately by two of the five annotators using\nROBOTO2, with LLM assistance. The version of\nROBOTO2 used for annotations (collected during\nearly 2024) used retrieval and GPT-3.5 (gpt-3.5-\nturbo-0125) as the answer model.5 Because this\nsample of 276 papers was annotated with LLM\nassistance, we withheld them from the final evalu-\nation of ROBOTO2 as described in Section 6, but\ninclude them in the published dataset to support\nfuture work.\nDataset statistics\nThe distribution of domain-\nlevel and overall risk of bias judgments in the full\ndataset are provided in Table 1. Distributions of\nanswered signaling questions and evidence para-\ngraphs in the manually-annotated subset are shown\nat the top of Table 2.\nInter-rater reliability\nTo assess inter-rater relia-\nbility, 20 papers (totaling 440 signaling questions)\nare independently annotated by two annotators us-\ning ROBOTO2. We aggregate answers into the\nfollowing classes: Yes/Probably Yes, No/Probably\nNo, No Information, and N/A, when a question is\nskipped by ROB logic. Four-class Cohen’s Kappa\nis 0.40, indicating fair to moderate agreement. This\nis consistent with prior work showing slight to mod-\nerate agreement (Fleiss’ Kappa of 0.45 at the do-\nmain level) among experienced raters (Minozzi\net al., 2020, 2021); it reflects well-documented\nchallenges of applying the complex ROB2 tool\n(Nejadghaderi et al., 2024), as reviewers can differ\nin their interpretation of ambiguous scenarios and\ntheir thresholds for assigning risk levels. Further\ncommentary in App. D.\n5In experiments, other LLMs and full-text context demon-\nstrate better performance, but these were reasonable config-\nurations at the time of annotation. Our public web interface\nsupports the use of alternate LLMs.\n6\nExperimental Settings\nWe evaluate 4 models: GPT-3.5-Turbo, GPT-4o,\nClaude 3.5-Sonnet, and Llama-3.3-70B-Instruct.\nAll models receive the same prompt and inputs\n(App. C). For all experiments, we aggregate labels\nand outputs into three classes: Yes/Probably Yes\n(Y/PY), No/Probably No (N/PN), and No Informa-\ntion (NI), and report micro-F1 at each domain level\nalong with micro- and macro-averages across all\nsignaling questions (Table 2).\nWithin-document retrieval\nWe evaluate two\nretrieval methods:\nBM25 (Robertson et al.,\n1994) and paragraph embeddings using Sentence-\nTransformers (Reimers and Gurevych, 2019). Each\nsignaling question has a max of one gold evidence\npassage in the dataset; we report recall@k for\nk=1,3,5,10 for all retrieval methods (Table 4). De-\ntailed results and evaluation of the retrieval meth-\nods can be found in App B. In all cases, we use the\nquestions from the ROB2 assessment as the query,\nand paragraphs from the clinical trial paper as the\ndocuments to retrieve. In the publicly available\nversion of ROBOTO2, all-MiniLM-L6-v2 and k=3\nwere selected as these settings achieved competi-\ntive performance at low cost.\nPrompting LLMs for QA\nWe evaluate all mod-\nels in a zero-shot setting with oracle evidence (pro-\nviding the human-labeled evidence passage), as\nwell as with the top-k retrieved evidence (with\nk=1,3,5), and the full paper setting for models\nwith sufficient input context window sizes (all but\nGPT-3.5-Turbo). In the ROB2 assessment, each\nsignaling question includes elaboration text that\nexpands on when each answer should be chosen\nfor that question; we provide this elaboration in the\ninstructions for all prompting settings (example in\nApp. C). We also conduct several experiments with\nin-context learning (Brown et al., 2020), which sug-\ngested minimal gains from the zero-shot setting;\nthese results are reported in App E.\n7\nResults & Discussion\nResults for all experimental settings are provided\nin Table 2. We analyze the model results and user\nstatistics collected during ROBOTO2 annotations\nbelow (full statistics in App. G).\nRoom for improvement\nThe best performing\nmodel (Claude 3.5-Sonnet with the full paper as\ncontext) achieved a micro-F1 of 0.71, highlighting\nconsiderable room for improvement. All evaluated\n"}, {"page": 6, "text": "Model\nRetrieval\nD1\nD2\nD3\nD4\nD5\nMicro-Avg\nMacro-Avg\nn-oracle\n-\n197\n124\n37\n73\n11\n-\n-\nn-total\n-\n750\n1278\n598\n1027\n750\n-\n-\nBaseline w/ oracle evidence paragraphs\nLlama-3.3-70B-Instruct\nOracle\n0.67\n0.55\n0.35\n0.81\n0.45\n0.62\n0.67\nGPT-3.5-Turbo\nOracle\n0.81\n0.66\n0.67\n0.82\n0.57\n0.61\n0.71\nGPT-4o\nOracle\n0.75\n0.78\n0.68\n0.92\n0.54\n0.64\n0.73\nClaude 3.5-Sonnet\nOracle\n0.75\n0.81\n0.66\n0.92\n0.59\n0.66\n0.75\nRetrieved evidence paragraphs\nLlama-3.3-70B-Instruct\nk=1\n0.83\n0.61\n0.42\n0.80\n0.38\n0.49\n0.61\nGPT-3.5-Turbo\nk=1\n0.81\n0.73\n0.69\n0.74\n0.66\n0.58\n0.73\nGPT-4o\nk=1\n0.68\n0.65\n0.58\n0.81\n0.75\n0.55\n0.69\nClaude 3.5-Sonnet\nk=1\n0.69\n0.68\n0.66\n0.87\n0.76\n0.60\n0.73\nLlama-3.3-70B-Instruct\nk=3\n0.87\n0.69\n0.66\n0.81\n0.35\n0.55\n0.68\nGPT-3.5-Turbo\nk=3\n0.82\n0.69\n0.68\n0.73\n0.59\n0.55\n0.70\nGPT-4o\nk=3\n0.75\n0.72\n0.73\n0.82\n0.72\n0.60\n0.75\nClaude 3.5-Sonnet\nk=3\n0.75\n0.75\n0.76\n0.87\n0.77\n0.65\n0.78\nLlama-3.3-70B-Instruct\nk=5\n0.87\n0.72\n0.70\n0.81\n0.28\n0.55\n0.69\nGPT-3.5-Turbo\nk=5\n0.82\n0.69\n0.64\n0.71\n0.56\n0.53\n0.68\nGPT-4o\nk=5\n0.78\n0.74\n0.73\n0.83\n0.69\n0.62\n0.75\nClaude 3.5-Sonnet\nk=5\n0.78\n0.79\n0.78\n0.86\n0.77\n0.67\n0.80\nFull paper as input\nLlama-3.3-70B-Instruct\nFull Paper\n0.88\n0.81\n0.79\n0.79\n0.32\n0.61\n0.72\nGPT-4o\nFull Paper\n0.80\n0.82\n0.77\n0.85\n0.66\n0.66\n0.78\nClaude 3.5-Sonnet\nFull Paper\n0.81\n0.84\n0.80\n0.88\n0.77\n0.71\n0.82\nTable 2: For all settings, we report micro-averaged domain-level F1 along with micro- and macro-averaged F1\nacross all signaling questions. The n-oracle is the number of instances where annotators identified an evidence\npassage, while n-total is the total number of signaling questions answered for that domain.\nmodels achieve strong results in D1, where ques-\ntions are more likely to be answerable based on\ntext in the trial reports. Performance in D2 and\nD3 is mixed, as these may require interpreting nu-\nmerical data (e.g., calculating attrition rates from\nrecruitment and result numbers). D5 is similarly\nchallenging for models as these questions may re-\nquire knowledge of external clinical resources and\nguidelines.\nIncreasing context generally improves perfor-\nmance for most models while reducing accuracy\nfor GPT-3.5-Turbo. In some cases, models with\nretrieval can surpass oracle retrieval performance,\nlikely due to incomplete evidence labeling in our\ndataset, indicating that relevant information ex-\nists beyond annotator-selected passages. Few-shot\nprompting does not appear to outperform zero-shot\nprompting in our experimental results (App E).\nLimited utility for fully-automated ROB assess-\nment\nModel performance cannot be substituted\nfor human judgment in ROB assessments, and\nwe recommend that humans remain in the loop.\nConservative question-level model judgments com-\npound to conservative domain- and paper-level\njudgments, where the fully-automated pipeline\njudges most papers as having “some concerns” or\n“high risk” even when human raters did not. Human\nraters assessed 47 of 276 trials as high risk while\nthe LLM-only pipeline assessed 101 as high risk.\nError analysis (App. F) reveals that the strongest\nmodels tend to over-select “No Information,” which\nmay reflect cautiousness gained from safety and\nalignment training.\nROBOTO2 supports human review and editing\nWe compute detailed metrics for the 276 ROB2\nassessments annotated using ROBOTO2, including\nthe number of times annotators accept the model’s\nanswers and explanations directly versus change\nthem, and the number of retrieved evidence pas-\nsages marked as good (offering evidence to support\nan answer) versus bad (irrelevant). Annotators pro-\nvide their own answer (42.4%) and edit rationales\n(28.7%) around half the time, rather than use the\nanswer (57.6%) and rationales (71.3%) provided\nby the model (Table 7). For evidence passages,\n615 total up/downvotes are collected (out of 3370\nretrieved passages), of which 78.0% are positive\nfeedback. We provide all feedback in our dataset\nto support future model development. Detailed\nstatistics can be found in App. G.\n"}, {"page": 7, "text": "8\nConclusion\nAssessing the quality of clinical trials is an im-\nportant step to weighing their evidence in clinical\ndecision-making. To support this, we introduce the\nROBOTO2 system to assist researchers in conduct-\ning risk of bias assessment for clinical trials with\nLLM support, along with associated code for run-\nning the web interface. We also release a dataset\nof 521 complete ROB analyses (8954 signaling\nquestions with 1202 evidence passages) of child\nhealth clinical trial reports. We hope our system\nand dataset will promote better LLM applications\nfor risk of bias assessment, and that access to this\nassisted annotation tool can enable quicker comple-\ntion of ROB assessments and reduce the labor and\ncosts around systematic literature reviews.\n9\nLimitations\nViewing model outputs could potentially bias\nannotations\nROBOTO2 is designed to expose\nall intermediate and final model outputs, and al-\nlows expert annotators to change any part of the\nmodel output. While we can compute the number\nof changes made, we cannot guarantee that see-\ning model outputs does not influence annotator re-\nsponses. Prior work has shown that human annota-\ntors may demonstrate anchoring bias when exposed\nto LLM assistance during annotation (Choi et al.,\n2024), leading to discrepancies in downstream la-\nbel distributions. We leave the measurement of this\nbias in the ROB setting to future work.\nDataset imbalance\nThough it reflects real-world\nROB2 assessments, our dataset is unbalanced. The\nmajority of papers are assessed as having some con-\ncerns, with fewer papers of low or high risk. This\nlikely biases the evaluation of our system, similar to\nwhat was observed by Suster et al. (2021). Related,\nsome signaling questions have very sparse anno-\ntations (especially those that depend on cascading\nlogic) or are biased in terms of answer distribution\n(almost always one of the answer labels).\nAmong manually conducted reviews, annotator-\nprovided evidence paragraphs are only available for\na small portion of signaling questions, unbalanced\nacross domains; D1 has the most signaling ques-\ntions with evidence passages, while D5 has very\nfew. Our retrieval methods as well as Oracle results\nare only reported on this biased subset, and may\nnot accurately represent performance on sparsely\nannotated questions and domains.\nPotential gains in quality or efficiency\nWe hy-\npothesize that ROBOTO2 may either help to save\ntime or improve the quality of ROB assessments.\nQualitatively, the annotation team reported that\nROBOTO2 offers an opportunity to enhance ev-\nidence and rationale coverage, as the time savings\non the ROB assessment itself were repurposed to\njudge and retrieve relevant evidence. However, the\nimpact of such repurposing of effort on the qual-\nity of the resulting assessments was not explicitly\nmeasured in our system and should be confirmed\nand studied in future work.\nDiversity of language models in experiments\nOur experiments do not include any reasoning mod-\nels such as OpenAI’s o3, Anthropic’s Claude 4-\nSonnet-Thinking, and DeepSeek-R1 (DeepSeek-\nAI, 2025). Future work could explore whether\nthese models improve current performance in terms\nof both answer classification accuracy and gener-\nated rationales.\nAcknowledgements\nWe thank the following individuals for producing\nparts of the ROB dataset we use for evaluation:\nBernadette Zakher, Shannon Sim, Michele Dyson,\nBanke Oketola, and Aneet Saran, from the Uni-\nversity of Victoria, University of Alberta, and Uni-\nversity of Manitoba. This work is supported in\npart by the University of Washington Information\nSchool Strategic Research Fund and the University\nof Washington eScience Institute’s Azure Cloud\nCredits for Research and Teaching.\nReferences\nAhmad Alshami, Moustafa Elsayed, Eslam Ali, Abdel-\nrahman E. E. Eltoukhy, and Tarek M. Zayed. 2023.\nHarnessing the power of chatgpt for automating sys-\ntematic review process: Methodology, case study,\nlimitations, and future directions. Syst., 11:351.\nNicole Boluyt, Lisa Tjosvold, Carol Lefebvre, Terry P\nKlassen, and Martin Offringa. 2008. Usefulness of\nsystematic review search strategies in finding child\nhealth systematic reviews in medline. Archives of\npediatrics & adolescent medicine, 162 2:111–6.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAlexander S. Choi, Syeda Sabrina Akter, JP Singh, and\nAntonios Anastasopoulos. 2024. The LLM effect:\n"}, {"page": 8, "text": "Are humans truly using LLMs, or are they being in-\nfluenced by them instead?\nIn Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 22032–22054, Miami,\nFlorida, USA. Association for Computational Lin-\nguistics.\nMiew Keen Choong, Filippo Galgani, Adam G. Dunn,\nand Guy Tsafnat. 2014. Automatic evidence retrieval\nfor systematic reviews. Journal of Medical Internet\nResearch, 16.\nDeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-\nsoning capability in llms via reinforcement learning.\nJay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey\nKuehl, and Lucy Lu Wang. 2021.\nMSˆ2: Multi-\ndocument summarization of medical studies. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 7494–\n7513, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nJay DeYoung, Eric Lehman, Benjamin Nye, Iain Mar-\nshall, and Byron C. Wallace. 2020. Evidence infer-\nence 2.0: More data, better models. In Proceedings\nof the 19th SIGBioMed Workshop on Biomedical Lan-\nguage Processing, pages 123–132, Online. Associa-\ntion for Computational Linguistics.\nJulian PT Higgins, Douglas G Altman, Peter C\nGotzsche, Peter Juni, David Moher, Andrew D\nOxman, Jelena Savovic, Kenneth F Schulz, Laura\nWeeks, and Jonathan AC Sterne. 2011. The cochrane\ncollaboration’s tool for assessing risk of bias in ran-\ndomised trials. BMJ, 343.\nYan Hu, Vipina Kuttichi Keloth, Kalpana Raja, Yong\nChen, and Hua Xu. 2023. Towards precise pico ex-\ntraction from abstracts of randomized controlled tri-\nals using a section-specific learning approach. Bioin-\nformatics, 39.\nPatricia Sofia Jacobsen Jardim, Christopher James Rose,\nHeather Melanie R Ames, J. Meneses Echavez,\nStijn Van de Velde, and Ashley Elizabeth Muller.\n2022. Automating risk of bias assessment in system-\natic reviews: a real-time mixed methods comparison\nof human researchers to a machine learning system.\nBMC Medical Research Methodology, 22.\nDi Jin and Peter Szolovits. 2018. Pico element detection\nin medical text via long short-term memory neural\nnetworks. In Workshop on Biomedical Natural Lan-\nguage Processing.\nHanan Khalil, Daniel Ameen, and A. Zarnegar. 2021.\nTools to support the automation of systematic re-\nviews: A scoping review. Journal of clinical epidemi-\nology.\nEric Lehman, Jay DeYoung, Regina Barzilay, and By-\nron C. Wallace. 2019. Inferring which medical treat-\nments work from reports of clinical trials. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 3705–3717, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rod-\nney Michael Kinney, and Daniel S. Weld. 2020.\nS2orc: The semantic scholar open research corpus.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics.\nIain J Marshall, Joel Kuiper, and Byron C Wallace.\n2016. Robotreviewer: evaluation of a system for\nautomatically assessing bias in clinical trials. Jour-\nnal of the American Medical Informatics Association,\n23(1):193–201.\nIain James Marshall, Joël Kuiper, and Byron C. Wallace.\n2014. Automating risk of bias assessment for clin-\nical trials. IEEE Journal of Biomedical and Health\nInformatics, 19:1406–1412.\nIain James Marshall and Byron C. Wallace. 2019. To-\nward systematic review automation: a practical guide\nto using machine learning tools in research synthesis.\nSystematic Reviews, 8.\nSilvia Minozzi, Michela Cinquini, Silvia Gianola,\nMarien González-Lorenzo, and Rita Banzi. 2020.\nThe revised cochrane risk-of-bias tool for randomised\ntrials (rob 2) showed low inter-rater reliability and\nchallenges in its application. Journal of clinical epi-\ndemiology.\nSilvia Minozzi, Kerry Dwan, Francesca Borrelli, and\nGraziella Filippini. 2021. Reliability of the revised\ncochrane risk-of-bias tool for randomised trials (rob2)\nimproved with the use of implementation instruction.\nJournal of clinical epidemiology.\nAakanksha Naik, Bailey Kuehl, Erin Bransom, Doug\nDowney, and Tom Hope. 2024. CARE: Extracting ex-\nperimental findings from clinical literature. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2024, pages 4580–4596, Mexico City,\nMexico. Association for Computational Linguistics.\nSeyed Aria Nejadghaderi, Maryam Balibegloo, and\nNima Rezaei. 2024. The cochrane risk of bias as-\nsessment tool 2 (rob 2) versus the original rob: A\nperspective on the pros and cons. Health Science\nReports, 7.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang,\nIain Marshall, Ani Nenkova, and Byron Wallace.\n2018. A corpus with multi-level annotations of pa-\ntients, interventions and outcomes to support lan-\nguage processing for medical literature. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 197–207, Melbourne, Australia. Association\nfor Computational Linguistics.\nJason Portenoy and Jevin D. West. 2020. Constructing\nand evaluating automated literature review systems.\nScientometrics, 125:3233 – 3251.\n"}, {"page": 9, "text": "Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nStephen E. Robertson, Steve Walker, Susan Jones,\nMicheline Hancock-Beaulieu, and Mike Gatford.\n1994. Okapi at trec-3. In Text Retrieval Conference.\nOlivia Sanchez-Graillet, Christian Witte, Frank Grimm,\nSteffen Grautoff, Basil Ell, and Philipp Cimiano.\n2022.\nSynthesizing evidence from clinical trials\nwith dynamic interactive argument trees. Journal\nof Biomedical Semantics, 13.\nChantal Shaib, Millicent Li, Sebastian Joseph, Iain\nMarshall, Junyi Jessy Li, and Byron Wallace. 2023.\nSummarizing, simplifying, and synthesizing medical\nevidence using GPT-3 (with varying success). In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1387–1407, Toronto, Canada.\nAssociation for Computational Linguistics.\nJonathan A. C. Sterne, Jelena Savovi´c, Matthew J. Page,\nRoy G Elbers, Natalie S Blencowe, Isabelle Boutron,\nChristopher J Cates, Hung-Yuan Cheng, Mark S.\nCorbett, Sandra Eldridge, Jonathan R. Emberson,\nMiguel A. Hernán, Sally Hopewell, Asbjørn Hrõb-\njartsson, Daniela R. Junqueira, Peter Juni, Jamie J.\nKirkham, Toby J Lasserson, Tianjing Li, Alexandra\nMcAleenan, Barnaby C. Reeves, Sasha Shepperd, Ian\nShrier, Lesley A Stewart, Kate Tilling, Ian R. White,\nPenny F. Whiting, and Julian P. T. Higgins. 2019.\nRob 2: a revised tool for assessing risk of bias in\nrandomised trials. BMJ, 366.\nSimon Suster, Timothy Baldwin, Jey Han Lau, Anto-\nnio Jimeno Yepes, David Martinez Iraola, Yulia Ot-\nmakhova, and Karin M. Verspoor. 2021. Automating\nquality assessment of medical evidence in systematic\nreviews: Model development and validation study.\nJournal of Medical Internet Research, 25.\nRens van de Schoot, Jonathan de Bruin, Raoul Schram,\nParisa Zahedi, Jan de Boer, Felix Weijdema, Bianca\nKramer, Martijn Huijts, Maarten Hoogerwerf, Ger-\nbrich Ferdinands, Albert Harkema, Joukje E Willem-\nsen, Yongchao Ma, Qixiang Fang, Lars G Tummers,\nand Daniel L. Oberski. 2021. Asreview: Active learn-\ning for systematic reviews.\nByron C. Wallace, Joel Kuiper, Aakash Sharma, Mingxi\nZhu, and Iain James Marshall. 2016. Extracting pico\nsentences from clinical trial reports using supervised\ndistant supervision. Journal of machine learning\nresearch (JMLR), 17.\nByron C. Wallace, Sayantani Saha, Frank Soboczenski,\nand Iain James Marshall. 2020. Generating (factual?)\nnarrative summaries of rcts: Experiments with neu-\nral multi-document summarization. AMIA Annual\nSymposium proceedings, 2021:605–614.\nLucy Lu Wang, Jay DeYoung, and Byron Wallace. 2022.\nOverview of MSLR2022: A shared task on multi-\ndocument summarization for literature reviews. In\nProceedings of the Third Workshop on Scholarly Doc-\nument Processing, pages 175–180, Gyeongju, Repub-\nlic of Korea. Association for Computational Linguis-\ntics.\nHye Sun Yun, David Pogrebitskiy, Iain James Mar-\nshall, and Byron C. Wallace. 2024. Automatically\nextracting numerical results from randomized con-\ntrolled trials with large language models.\nArXiv,\nabs/2405.01686.\nA\nROB2 Assessment Tool\nThe Signaling Questions for the Cochrane ROB2\nTool for Randomized Trials are given in Table 3.\nNote that some questions are cascading, and are\nonly answered if previous questions in the domain\nare answered in a pre-specified way.\nFigure 3 reproduces a flowchart from the ROB2\ntool that indicates how signaling questions con-\ntribute to a domain-level judgment for Domain\n4. Based on how these questions are answered,\nthe domain-level judgment can be low risk, some\nconcerns, or high risk. Flowcharts are also pro-\nvided for the other four domains and are available\nat https://methods.cochrane.org/risk-bias-2.\nB\nRetriever Evaluation\nWe experiment with a sparse retriever, BM25\n(Robertson\net\nal.,\n1994),\nand\nSentence-\nTransformers (Reimers and Gurevych, 2019). We\nassess retriever performance by varying k, the\nnumber of passages retrieved and provided to the\nQA reader module. For models with large context\nwindows, we also experiment with providing the\nentire paper as context.\nAll methods are validated using gold evidence\nparagraphs identified by the annotators in our\ndataset. Each signaling question has a maximum of\none gold evidence passage in the dataset; we report\nrecall@k for k=1,3,5,10 for all retrieval methods\n(Table 4).\nFor within-document retrieval, we find that S-\nBERT successfully retrieves the gold evidence pas-\nsage at a higher rate than BM25 at comparable k\n(Table 4). However, prompting models with the\nfull paper achieves the highest overall F1-scores\n(Table 2). ROBOTO2 uses S-BERT for its balance\nbetween performance, speed, and enabling models\nwith smaller context windows to be used in the web\ninterface.\n"}, {"page": 10, "text": "Domain\nQuestion\nDomain 1: Risk of bias arising from the randomization process\n1.1\nWas the allocation sequence random?\n1.2\nWas the allocation sequence concealed until participants were enrolled and assigned to interventions?\n1.3\nDid baseline differences between intervention groups suggest a problem with the randomization\nprocess?\nDomain 2: Risk of bias due to deviations from the intended interventions\n2.1\nWere participants aware of their assigned intervention during the trial?\n2.2\nWere carers and people delivering the interventions aware of participants’ assigned intervention\nduring the trial?\n2.3\nIf Y/PY/NI to 2.1 or 2.2: Were there deviations from the intended intervention that arose because of\nthe trial context?\n2.4\nIf Y/PY to 2.3: Were these deviations likely to have affected the outcome?\n2.5\nIf Y/PY/NI to 2.4: Were these deviations from intended intervention balanced between groups?\n2.6\nWas an appropriate analysis used to estimate the effect of assignment to intervention?\n2.7\nIf N/PN/NI to 2.6: Was there potential for a substantial impact (on the result) of the failure to analyse\nparticipants in the group to which they were randomized?\nDomain 3: Risk of bias due to missing outcome data\n3.1\nWere data for this outcome available for all or nearly all participants randomized?\n3.2\nIf N/PN/NI to 3.1: Is there evidence that the result was not biased by missing outcome data?\n3.3\nIf N/PN to 3.2: Could missingness in the outcome depend on its true value?\n3.4\nIf Y/PY/NI to 3.3: Is it likely that missingness in the outcome depended on its true value?\nDomain 4: Risk of bias in measurement of the outcome\n4.1\nWas the method of measuring the outcome inappropriate?\n4.2\nCould measurement or ascertainment of the outcome have differed between intervention groups?\n4.3\nIf N/PN/NI to 4.1 and 4.2: Were outcome assessors aware of the intervention received by study\nparticipants?\n4.4\nIf Y/PY/NI to 4.3: Could assessment of the outcome have been influenced by knowledge of\nintervention received?\n4.5\nIf Y/PY/NI to 4.4: Is it likely that assessment of the outcome was influenced by knowledge of\nintervention received?\nDomain 5: Risk of bias in selection of the reported result\n5.1\nWere the data that produced this result analysed in accordance with a pre-specified analysis plan that\nwas finalized before unblinded outcome data were available for analysis?\n5.2\nIs the numerical result being assessed likely to have been selected, on the basis of the results, from\nmultiple eligible outcome measurements (e.g., scales, definitions, time points) within the outcome\ndomain?\n5.3\nIs the numerical result being assessed likely to have been selected, on the basis of the results, from\nmultiple eligible analyses of the data?\nTable 3: Signaling Questions in the Cochrane Risk of Bias Tool for Randomized Trials (ROB2).\nModel\nR@1\nR@3\nR@5\nR@10\nBM25\n0.140\n0.272\n0.367\n0.533\nS-BERT\n0.268\n0.455\n0.519\n0.678\nTable 4: Recall@k for our tested retrieval methods.\nC\nPrompting\nThe prompt is formatted as follows:\n<instruction>\n<signaling_question>\n<elaboration>\n<retrieved_paragraph_1>\n...\n<retrieved_paragraph_k>\nAfter an instruction to answer the question, we pro-\nvide the signaling question itself from the ROB2\nassessment, as well as additional elaboration text\nexplaining all answer options and when they should\nbe used. These elaborations are adapted from ex-\nplanations given in the ROB2 tool, and we further\naugment them such that all possible answer options\nare represented—not all answers are represented in\nelaborations from the original ROB2 tool, which\nwe found may bias models towards answers that\nwere. Retrieved context paragraphs are then ap-\npended. We instruct the model to make a prediction\nand generate a rationale for its prediction.\nA full example prompt for signaling question 1\nin Domain 1 is reproduced below:\nYou are an expert scientific researcher.\n"}, {"page": 11, "text": "Figure 3: Flowchart for how answers to signaling questions contribute to a domain-level judgment for Domain 4 in\nthe ROB2 tool. Reproduced from https://sites.google.com/site/riskofbiastool/welcome/rob-2-0-tool.\nYou will be given a passage from\na scientific paper reporting on a\nrandomized controlled trial along\nwith a question and elaboration of\nthe question. Your task is to return\nthe answer to the question out of the\nfollowing set of answers: \"yes\", \"no\",\n\"probably yes\", \"probably no\", \"no\ninformation\". You should use the given\npassage to answer the question.\nQuestion: \"Was the allocation sequence\nrandom?\"\nElaboration: \"Answer ‘Yes’ if a random\ncomponent was used in the sequence\ngeneration process. Examples include\ncomputer-generated random numbers;\nreference to a random number table;\ncoin tossing; shuffling cards or\nenvelopes; throwing dice; or drawing\nlots. Minimization is generally\nimplemented with a random element (at\nleast when the scores are equal), so an\nallocation sequence that is generated\nusing minimization should generally be\nconsidered to be random.\nAnswer ‘No’ if no random element was\nused in generating the allocation\nsequence or the sequence is predictable.\nExamples include alternation; methods\nbased on dates (of birth or admission);\npatient record numbers; allocation\ndecisions made by clinicians or\nparticipants; allocation based on the\navailability of the intervention; or any\nother systematic or haphazard method.\nAnswer ‘No information’ if the only\ninformation about randomization methods\nis a statement that the study is\nrandomized.\nIn some situations a judgment may\nbe made to answer ‘Probably no’ or\n‘Probably yes’. For example, in the\ncontext of a large trial run by an\nexperienced clinical trials unit,\nabsence of specific information\nabout generation of the randomization\nsequence, in a paper published in a\njournal with rigorously enforced word\ncount limits, is likely to result in\na response of ‘Probably yes’ rather\nthan ‘No information’. Alternatively,\nif other (contemporary) trials by the\nsame investigator team have clearly\nused non-random sequences, it might be\nreasonable to assume that the current\nstudy was done using similar methods.\"\nPassage(s):\n<retrieved_paragraph_1>\n...\n<retrieved_paragraph_k>\n"}, {"page": 12, "text": "D\nFurther Commentary on Inter-Rater\nReliability Analysis\nTwo independent reviewers independently assessed\nrisk of bias for 20 trials using the revised Cochrane\nROB2 tool. These assessments were used to com-\npute IAA as reported in the main paper. Following\nthese annotations, the two reviewers conducted a\nconsensus meeting to better understand discrep-\nancies arising in their annotations. The discussion\nprocess involved revisiting the Cochrane Handbook\nand the official ROB2 guidance document (Higgins\net al., 2011; Sterne et al., 2019) to ensure alignment\nwith recommended best practices.\nNotable discrepancies emerged in this meeting,\nclassified into four main categories: disagreement\nat the signaling question level, disagreement at the\ndomain level, differences in judgments between\nYes and Probably Yes, and No and Probably No.\nOne reviewer tended to adopt a more conservative\napproach and tended to opt for “some concerns” or\n“high risk” judgments whereas the other reviewer\nmore frequently opted for “low risk” ratings when\nthe available information appeared sufficient. This\ndivergence was typical of what has been described\nin prior research on ROB2, which has noted that\neven experienced reviewers may differ in how they\ninterpret the level of concern warranted by ambigu-\nous or incomplete reporting (Minozzi et al., 2020).\nFollowing this consensus meeting, the review-\ners were able to reach full agreement across all\nsignaling questions and domains. This calibration\nprocess is useful for achieving subsequent consis-\ntent application of the ROB2 assessment tool.\nE\nFew-Shot Prompting Results\nResults from few-shot prompting experiments are\nshown in Table 5. The few-shot prompt is created\nby sampling one example for each class from the\ngold label annotations, using the same prompt tem-\nplate in App C. The oracle paragraph is provided\nfor each example, the elaboration for the signaling\nquestions is removed (due to token constraints), and\nthe answer is appended to the end of the prompt in\nthe form Answer:<Label>. Few shot examples\nsampled are removed from the evaluation set for\nmodels. No substantial differences are observed be-\ntween the zero- and few-shot settings when models\nare provided the same number of context passages.\nF\nLLM Error Analysis\nThe 4 LLMs we evaluated exhibit different pat-\nterns of answers, though the evaluation metrics are\ncomparable. Performance across models according\nto micro- and macro-averaged F1 across domains\nsuggests similar performance, qualitative perfor-\nmance is very different between models. We plot\nerror counts in Figure 4, showing true positives\n(TP), alongside each of the two types of false posi-\ntives (FPs) and false negatives (FNs). Here, class\n1 FP/FN errors are those considered to be less se-\nvere (e.g., Y/PY wrongly classified as NI is not as\nsevere as Y/PY wrongly classified as N/PN). We\nalso provide raw counts of these errors in Table 6.\nAs seen in the figure, GPT-4o is more likely than\nother LLMs to answer “No Information” (large\nnumber of FP in the first column) or “No/Probably\nNo”. Llama-3.3-70B-Instruct, Claude-3.5-Sonnet,\nand GPT-4o most often predicted “No Informa-\ntion” for signaling questions where the true label\nwas No/Probably No. On the other hand, GPT-\n3.5-Turbo almost never abstains with a “No Infor-\nmation” prediction, leading to more false positive\nerrors for the N/PN and Y/PY classes. Claude\n3.5-Sonnet was the best performing model evalu-\nated and has fairly comparable false positive rates\nacross “No/Probably No” and “Yes/Probably Yes”.\nLlama-3.3-70b-Instruct demonstrates a similar an-\nswer distribution to Claude 3.5-Sonnet, but with\na consistent false positive rate across all 3 classes,\nbut higher false negatives, with “No/Probably No”\nbeing the highest.\nThese observed behaviors for over-predicting\n“No Information” or “No/Probably No” could stem\nfrom safety mechanisms learned during model post-\ntraining, which might explain GPT-3.5-Turbo’s\nextreme bias towards “No/Probably No” and\n“Yes/Probably Yes”. Some domains have ques-\ntions phrased in a way that requires interpreting\nnumerical data (D2 & D3) or understanding cur-\nrent best practices in the field (D5); stronger mod-\nels tend to abstain in these cases and select “No\nInformation”. However, in the context of ROB2 as-\nsessments, these cautious predictions lead to over-\nconservative domain- and paper-level labels (high\nrisk judgments) for LLM-supported assessments.\nG\nROBOTO2 Usage Statistics\nAcceptance rates of model answers and rationales\nversus user-corrected rates are provided in Table 7.\nOur annotation interface allows users to rate the 3\n"}, {"page": 13, "text": "Model\nRetrieval\nD1\nD2\nD3\nD4\nD5\nMicro-avg\nMacro-avg\nn-total\n-\n750\n1278\n598\n1027\n750\n-\n-\nZero-shot setting\nLlama-3.3-70B-Instruct\nk=1\n0.83\n0.61\n0.42\n0.80\n0.38\n0.49\n0.61\nGPT-3.5-Turbo\nk=1\n0.81\n0.73\n0.69\n0.74\n0.66\n0.58\n0.73\nGPT-4o\nk=1\n0.68\n0.65\n0.58\n0.81\n0.75\n0.55\n0.69\nClaude 3.5-Sonnet\nk=1\n0.69\n0.68\n0.66\n0.87\n0.76\n0.60\n0.73\nFew-shot setting\nLlama-3.3-70B-Instruct (FS)\nk=1\n0.83\n0.61\n0.43\n0.76\n0.38\n0.49\n0.61\nGPT-3.5-Turbo (FS)\nk=1\n0.81\n0.70\n0.65\n0.80\n0.71\n0.60\n0.73\nGPT-4o (FS)\nk=1\n0.70\n0.64\n0.60\n0.83\n0.70\n0.55\n0.69\nClaude 3.5-Sonnet (FS)\nk=1\n0.69\n0.68\n0.65\n0.87\n0.77\n0.60\n0.73\nTable 5: Micro-averaged domain-level F1 along with micro- and macro-averaged F1 across signaling questions for\nfew shot retrieval. Zero-shot k=1 results from Table 2 are reproduced here for reference.\nFigure 4: Stacked bar chart showcasing the aggregate true positive (TP) classifications versus false positive/negative\n(FP/FN) errors made by each model. FPs and FNs are each broken down into two classes, where class 1 (lighter\ncolor) are milder errors than class 2 (darker color) (e.g., misclassifying NI and N/PN or Y/PY is less severe than\nmisclassifying N/PN as Y/PY or vice versa). Counts less than 3 have their numbers hidden for chart readability, and\nfull counts are available in Table 6.\nretrieved paragraphs with a good/bad rating and/or\nadd their own paragraphs from the paper as con-\ntext. Feedback statistics for retrieved evidence para-\ngraphs are given in Table 8.\n"}, {"page": 14, "text": "Model\nClass\nTrue Positives\nFP (class 1)\nFP (class 2)\nFN (class 1)\nFN (class 2)\nLlama-3.3-70B-Instruct\nNI\n29.9\n36.1\n19.7\n29.0\n20.2\nN/PN\n68.7\n29.0\n17.9\n36.1\n25.2\nY/PY\n46.3\n20.2\n25.2\n19.7\n17.9\nGPT-3.5-turbo\nNI\n1.8\n2.8\n1.6\n45.8\n44.4\nN/PN\n104.9\n45.8\n29.5\n2.8\n52.9\nY/PY\n70.0\n44.4\n52.9\n1.6\n29.5\nGPT-4o\nNI\n61.9\n71.1\n34.5\n14.3\n15.2\nN/PN\n62.6\n14.3\n8.3\n71.1\n24.5\nY/PY\n56.5\n15.2\n24.5\n34.5\n8.3\nClaude-3.5-Sonnet\nNI\n50.5\n42.6\n27.3\n23.5\n17.5\nN/PN\n100.0\n23.5\n10.5\n42.6\n16.1\nY/PY\n61.7\n17.5\n16.1\n27.3\n10.5\nTable 6: Confusion-matrix summary for the LLMs when answering ROB2 signaling questions. For each response\ncategory Yes/Probably Yes (Y/PY), No/Probably No (N/PN), and No Information (NI), we list the number of true\npositives (TP) and the false positive (FP) and false negative (FN) counts accrued against the two alternative classes\n(“class 1” and “class 2”). Higher TP and lower FP/FN values reflect better agreement with the gold label. All values\nare normalized averages across run configurations.\nPredictions\nRationales\nDomain\nModel (%)\nExpert (%)\nModel (%)\nExpert (%)\n1\n377 (49.2%)\n390 (50.8%)\n430 (56.1%)\n337 (43.9%)\n2\n853 (59.2%)\n588 (40.8%)\n1117 (77.5%)\n324 (22.5%)\n3\n432 (65.2%)\n231 (34.8%)\n494 (74.5%)\n169 (25.5%)\n4\n591 (64.6%)\n325 (35.4%)\n717 (78.3%)\n199 (21.7%)\n5\n368 (48.2%)\n396 (51.8%)\n485 (63.5%)\n279 (36.5%)\nTotal\n2621 (57.6%)\n1930 (42.4%)\n3243 (71.3%)\n1308 (28.7%)\nTable 7: Counts and percentages of model-originated versus expert-corrected predictions and explanations across\ndomains for ROB assessments completed using ROBOTO2.\nDomain\nDownvotes\nUpvotes\nUser Added Paragraphs\n1\n43\n207\n74\n2\n40\n120\n84\n3\n12\n48\n24\n4\n22\n64\n112\n5\n18\n41\n62\nTotal\n135\n480\n356\nTable 8: Feedback on evidence passages provided by users by domain at the question level, for the ROBOTO2 subset,\ni.e., each number corresponds to the number of questions in that domain for which a user provided a downvote, an\nupvote, or added paragraph, as opposed to the total number of downvotes or upvotes etc.\n"}]}