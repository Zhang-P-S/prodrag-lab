{"doc_id": "arxiv:2511.07044", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.07044.pdf", "meta": {"doc_id": "arxiv:2511.07044", "source": "arxiv", "arxiv_id": "2511.07044", "title": "Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data", "authors": ["Mihael Arcan", "David-Paul Niland"], "published": "2025-11-10T12:38:26Z", "updated": "2025-12-19T20:21:51Z", "summary": "Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.07044v2", "url_pdf": "https://arxiv.org/pdf/2511.07044.pdf", "meta_path": "data/raw/arxiv/meta/2511.07044.json", "sha256": "55488f40bbe70684d52cca38b9bc97910844951c72ff6e32505ce9555aae1c68", "status": "ok", "fetched_at": "2026-02-18T02:28:01.424341+00:00"}, "pages": [{"page": 1, "text": "Evaluating Large Language Models for Anxiety, Depression, and\nStress Detection: Insights into Prompting Strategies and Synthetic\nData\nMihael Arcan1 and David-Paul Niland1\n1Lua Health, Galway, Ireland\nDecember 23, 2025\nAbstract\nMental health disorders affect over one-fifth of adults globally, yet detecting such conditions\nfrom text remains challenging due to the subtle and varied nature of symptom expression. This\nstudy evaluates multiple approaches for mental health detection, comparing Large Language\nModels (LLMs) such as Llama and GPT with classical machine learning and transformer-based\narchitectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of\nclinical interviews, we fine-tuned models for anxiety, depression, and stress classification and ap-\nplied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa\nachieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ\ntasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-\nBasic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness\nof transformer-based models and highlight the value of synthetic data in improving recall and\ngeneralization. However, careful calibration is required to prevent precision loss. Overall, this\nwork emphasizes the potential of combining advanced language models and data augmentation\nto enhance automated mental health assessment from text.\nMental health challenges impose a substantial burden on individuals and communities globally,\nwith recent data indicating that over 20% of adults will experience at least one mental disorder dur-\ning their lifetime [20]. Disorders such as depression and anxiety alone account for an estimated $1\ntrillion in annual global economic productivity losses [6]. These challenges are further exacerbated\nby the subtle and complex ways in which mental health conditions manifest in language, making\ntheir detection in text particularly challenging. Symptoms of stress, anxiety, and depression are of-\nten conveyed through nuanced and varied expressions, reflecting a wide spectrum of emotional and\npsychological states that traditional text-based models struggle to interpret and classify accurately.\nAs mental health challenges continue to escalate globally, there is an increasing need for effective\nmethods of early detection and intervention, particularly in everyday contexts such as workplaces\n[33]. The stigma surrounding mental health remains a significant barrier, often leading to under-\nreporting and delayed care, which further exacerbates the societal impact [29]. In this context,\ninnovative approaches leveraging technology, such as machine learning (ML) and natural language\nprocessing (NLP), hold promise in improving the identification and management of mental health\nconditions, providing more timely and personalized support to those in need.\nOver the past decade, research in NLP and computational social science has explored methods\nfor identifying mental ill-health issues using textual data, including social media content [22]. Much\n1\narXiv:2511.07044v2  [cs.CL]  19 Dec 2025\n"}, {"page": 2, "text": "of this work has focused on building specialized ML models for tasks such as stress detection or\ndepression prediction.\nWhile transformer-based models like BERT and XLNet have improved\nperformance, these require fine-tuning for specific tasks, limiting flexibility. Multi-task approaches\nhave also been explored but are often constrained to predefined task sets.\nAnother promising\narea of research has explored the use of chatbots in mental health services, offering potential for\nincreased accessibility and support. However, many of these tools remain rule-based and lack the\nsophistication of more advanced AI models, which poses a risk of delivering incorrect interventions\nthat could lead to significant negative outcomes for vulnerable individuals [4].\nChatGPT1 has transformed how people interact with AI, fueling widespread adoption across\nindustries such as customer service, education, and mental health detection. Built on the GPT AI\nmodel family, ChatGPT offers a user-friendly conversational interface that simplifies engagement\nwith Large Language Models (LLMs) for individuals and businesses alike. With ongoing advance-\nments in LLMs like OpenAI’s GPT and Meta’s Llama models, the potential for AI to address\ncritical challenges, such as mental health detection, is rapidly expanding. LLMs are capable of gen-\nerating text that closely mirrors real-world data, enabling more accurate identification of mental\nhealth conditions by capturing the diverse ways symptoms are expressed [39, 31]. These models\nhave the potential to recognize subtle linguistic cues and patterns that traditional diagnostic tools\nmight miss, providing more nuanced insights into individuals’ mental health. Additionally, syn-\nthetic data generation provides a valuable tool for augmenting datasets, addressing issues such as\ndata scarcity and class imbalance. By introducing artificial examples that replicate the characteris-\ntics of real data, synthetic data can enhance model performance and robustness, particularly when\naccess to large, diverse datasets is limited. However, it is critical to ensure that synthetic examples\nmaintain the authenticity and relevance of human expressions to preserve data integrity and avoid\npotential negative impacts on model performance. Despite these advancements, significant gaps\nremain in fully understanding the potential of LLMs in the mental health domain, particularly in\ntheir ability to comprehend and accurately classify mental health conditions expressed in natural\nlanguage.\nFurthermore, the integration of synthetic data into mental health detection systems\nintroduces challenges, including the need to balance diversity and authenticity to ensure effective\nmodel training.\nThis study aims to bridge these gaps by evaluating the performance of LLMs, including Ope-\nnAI’s GPT-3.5 Turbo and Metas’s Llama 3 8B [11], against classical ML and transformer-based\nmodels using the DAIC-WOZ and the Stress Detection dataset. Additionally, we investigate the\nimpact of synthetic data on model performance, exploring how different prompting strategies and\ntraining dataset sizes affect classification accuracy. By addressing these questions, we seek to con-\ntribute to the development of more reliable and accurate systems for mental health assessment from\ntextual data, ultimately advancing the application of AI in this critical field.\n1\nRelated Work\nThis section reviews advancements in mental health diagnostics and interventions through AI and\nML, with a focus on depression, anxiety, stress, workplace toxicity, and large language models\n(LLMs). Additionally, we explore the role of synthetic data in addressing data scarcity and ethical\nchallenges.\nTo address Major Depressive Disorder (MDD), [9] proposed a passive diagnostic system com-\nbining clinical psychology, ML, and conversational systems. Utilizing sequence-to-sequence neural\nnetworks, the system enables real-time dialogue with users while monitoring conversations for\n1https://chatgpt.com/\n2\n"}, {"page": 3, "text": "depression symptoms through specialized classifiers. Despite challenges with small datasets, the\nstudy demonstrates the potential for human-like chatbots in real-time mental health support. Ad-\nditionally, [8] introduced a deep neural network predicting PHQ-4 scores (assessing depression\nand anxiety) from text. The model leverages the Universal Sentence Encoder and Transformers,\nincorporating psycholinguistic features to enhance predictions. Though promising, the work high-\nlights challenges related to generalizability and domain-specific applications, especially for social\nmedia data. [21] focused on detecting MDD using natural language processing (NLP) to create\nneural classifiers. By analyzing speech transcripts, the approach predicts individual symptoms us-\ning symptom network analysis, achieving competitive results in binary diagnosis and depression\nseverity prediction.\nIn workplace settings, [3] addressed toxic communication through ToxiScope, a taxonomy for\ndetecting toxic language patterns in emails. Annotation tasks and ML models revealed the interplay\nof implicit and explicit toxicity with workplace power dynamics.\nFuture directions emphasize\nrefining detection techniques and investigating biases.\n[15] explored multimodal biomarkers for psychiatric disorders, utilizing behavioral and physio-\nlogical signals from remote interviews. Features such as facial expressions, speech, and cardiovascu-\nlar modulation were analyzed across four tasks, including MDD and self-rated depression detection.\nResults showed multimodal approaches outperforming unimodal methods, with AUROCs ranging\nfrom 0.72 to 0.82, underscoring the value of multimodal biomarkers for scalable mental health\nassessments.\nLLMs, such as GPT-3, GPT-4, and Google’s PaLM, have demonstrated transformative po-\ntential in mental health. [30] outlined a roadmap for integrating clinical LLMs in psychotherapy,\nemphasizing technical, ethical, and practical considerations. The study advocates responsible de-\nvelopment and public education on AI’s risks and benefits. [7] examined challenges in using LLMs\nfor psychological counseling, such as hallucination, interpretability, and privacy. The authors pro-\npose enhancements in fine-tuning and regulatory scrutiny to ensure practical deployment in mental\nhealthcare. Similarly, [12] evaluated Med-PaLM 2 for predicting psychiatric functioning, achieving\nnear-human performance in depression score prediction across diverse clinical tasks. Social media-\nbased mental health analysis also benefits from LLM advancements. [37] introduced MentaLLaMA,\nan interpretable mental health model trained on the IMHI dataset, achieving state-of-the-art per-\nformance. [35] further highlighted the effectiveness of instruction fine-tuning for mental ill-health\nprediction, positioning LLMs like Mental-FLAN-T5 and Mental-Alpaca as viable alternatives to\ntraditional methods.\nRecent research has also explored the use of transformer-based architectures for predicting clin-\nical assessment outcomes from unstructured patient text. For example, [25] investigated whether\ntransformer models such as BERT and RoBERTa could predict standardized mental health scores—including\nPHQ-9, GAD-7, PANSS, and the Calgary Depression Scale—using weekly written diaries from pa-\ntients experiencing a first episode of psychosis. The models achieved high performance on PHQ-9\n(F1 = 0.921) and GAD-7 (F1 = 0.945), with mean average errors for PANSS and GAF compara-\nble to clinician interrater variability. These results underscore the capacity of transformer-based\nmodels to approximate clinical assessments from natural language, highlighting their potential for\nearly relapse detection and continuous digital monitoring in psychiatric care.\nSynthetic data addresses data scarcity and privacy challenges in mental health. [24] demon-\nstrated the utility of SynthNotes in generating realistic, privacy-preserving mental health datasets.\nSimilarly, [17] and [2] developed GAN-based methods for creating statistically accurate synthetic\ndatasets, facilitating robust AI training while maintaining ethical compliance. [19] showcased GPT-\n3.5 for generating annotated clinical notes to enhance depression diagnostics. Meanwhile, [23] used\nsynthetic data to analyze demographic biases and stress triggers, revealing the potential of AI for\n3\n"}, {"page": 4, "text": "equitable research.\nData-centric approaches further enhance mental health diagnostics. [34] demonstrated the ef-\nfectiveness of synthetic data in early mental health prediction, using balancing and augmentation\nstrategies to outperform traditional methods. In clinical NLP tasks, models like Distil-RoBERTa\nand LLMs, like OpenAI’s GPT family have excelled in zero-shot tasks, with enhanced prompting\nstrategies improving prediction accuracy [36, 1]. Synthetic data generation remains integral to men-\ntal health research, with advancements in frameworks such as MATE-KD [26] and the development\nof task-specific models like SynthNotes contributing to scalable AI solutions.\n2\nMethodology\nIn this section, we describe the methodology employed for data preprocessing and model training,\nwith a focus on detecting depression and stress across two distinct datasets. Depression detection\nwas conducted using the DAIC-WOZ dataset, while stress detection utilized a combined corpus of\nReddit and Twitter data, forming a comprehensive stress detection dataset.\nWe present the data preprocessing techniques, predictive model training procedures, and prompt-\ning strategies designed to enhance the performance of both traditional machine learning models\nand large language models (LLMs), ensuring each approach is optimized for its respective dataset\nand task.\n2.1\nDepression Classification\nIn this section we focus on our approach to leverage the DAIC-WOZ dataset (see Section 3.1) to\nclassify depression and anxiety.\n2.1.1\nData Preprocessing\nOur initial dataset is the DAIC-WOZ, which comprises transcribed clinical interviews collected\nusing a Wizard-of-Oz approach for 142 patients. The interviews, centered on general conversation\ntopics, were conducted entirely within the United States. For each patient, the dataset includes\na transcript of their interview and corresponding PHQ-8 scores, with bot statements excluded to\nretain only patient responses. PHQ-8 scores were mapped to PHQ-2 scores, and GAD-2 scores\nwere inferred using the methodology described by [16].\nFrom these data, we derived PHQ-4 scores for each participant, comprising four variables corre-\nsponding to the four items in the PHQ-4 questionnaire. Generalized Anxiety Disorder items 1 and 2\n(GAD-1, GAD-2) assess anxiety, while Patient Health Questionnaire items 1 and 2 (PHQ-1, PHQ-\n2) assess depression. The PHQ-4 scores classify participants into one of four output categories:\nnone, mild, moderate, or severe. Each participant is assigned a single overall PHQ-4 score, broken\ndown into the four categories: GAD-1, GAD-2, PHQ-1, and PHQ-2, regardless of the number of\nmessages in the dataset.\nTo enhance the contextual information associated with each participant’s PHQ-4 score, we con-\ncatenated individual messages from the same participant, limiting each observation to a maximum\nof 50 words. Importantly, only messages from the same individual were combined; no messages\nfrom different participants were merged.\nAfter completing the concatenation process and removing duplicates and missing values, the\ndataset consisted of 28,186 observations in the training set and 8,710 in the test set.\n4\n"}, {"page": 5, "text": "2.1.2\nTransformer-based Models and XGBoost Training\nBased on the preprocessing of the DAIC-WOZ dataset described above, we fine-tuned several\ntransformer-based models, including BERT, DistilBERT, RoBERTa, DistilRoBERTa, and XLNet\n(see Section 3.2). These models were selected for their proven ability to capture complex linguistic\npatterns and contextual information, which are essential for natural language understanding tasks\nsuch as mental health detection. The fine-tuning process involved initializing each model with pre-\ntrained weights and adapting them to the specific task of predicting psychological distress through\noptimization on the DAIC-WOZ dataset.\nThe DAIC-WOZ dataset, which comprises clinical interviews aimed at assessing psychological\ndistress, provided a robust and contextually rich training corpus. Each input text, corresponding\nto an individual’s responses, was mapped to their PHQ-4 score, enabling supervised learning.\nWeighted F1 score was used as the primary metric to evaluate model performance.\nFor the XGBoost model, the DAIC-WOZ dataset was first converted into dense vector repre-\nsentations using a pre-trained Transformer model. The model training involved hyperparameter\noptimization through a grid search within a five-fold cross-validation framework.\nKey parame-\nters, such as the number of trees, maximum depth, and learning rate, were tuned to maximize\nperformance. The best-performing XGBoost model was selected based on the weighted F1 score,\nensuring robustness across imbalanced classes. To ensure reproducibility, all models were trained\non the same dataset splits, and random seeds were set to control variability in training outcomes.\n2.1.3\nPrediction Using LLM Prompting\nTo evaluate and compare the predictive capabilities of LLMs, like GPT and Llama, against transformer-\nbased models and XGBoost, we explored various prompting strategies designed to elicit specific\nresponses. We prompted Llama 3 8B (see Section 3.2.4) with five distinct queries and assessed\ntheir performance based on the generated outputs. Variations in the prompts included differences\nin lexicalized outputs, definitions, and prompt lengths. These strategies were specifically tailored\nto address the PHQ or GAD questions, resulting in a total of 20 unique prompts. By comparing the\nperformance of these strategies with the predictions made by transformer models and XGBoost,\nwe aimed to evaluate the relative strengths and weaknesses of each approach in predicting mental\nhealth outcomes.\nThe differences between prompt versions were minor, such as variations in phrasing like ”on a\nscale of zero to three” versus ”on a scale of 0 to 3,” or in prompt length, with some being brief (e.g.,\nVersion 3 in Table 1) and others more detailed (e.g., Version 5 in Table 2), including examples of\nmessages evaluated based on specific GAD or PHQ questions.\nThe goal was for the LLM to\nreturn a number corresponding to the relevant PHQ or GAD question, and regular expressions\nwere employed to handle the outputs. This involved extracting the first digit from the string or\nconverting numerically expressed words (e.g., ”zero”) into integers. For instance, if the chatbot\nresponse was ”This message looks like it is showing zero levels of anxiety” our system would return\n”0,” facilitating the analysis and evaluation process.\nFurthermore, we utilized the prompting structure that proved most effective for Llama 3 8B,\nknown as Version 3 (Table 1), for GPT-3.5 Turbo evaluation. This allowed us to assess the relative\nperformance of both models under consistent conditions. All other evaluation aspects, including\nthe methods for information extraction from chatbot responses, remained consistent across both\nGPT and Llama.\n5\n"}, {"page": 6, "text": "Version 1\nGAD-1\nOn a scale of zero to three, estimate how anxious this person is with zero being a little anxious and\nthree being incredibly anxious:+TEXT\nGAD-2\nOn a scale of zero to three, estimate how worried this person is with zero being a little worried and\nthree being incredibly worried:+TEXT\nPHQ-1\nOn a scale of zero to three, estimate how depressed this person is with zero being a little depressed\nand three being incredibly depressed:+TEXT\nPHQ-2\nOn a scale of zero to three, estimate if this person is expressing no interest in previously enjoyed\nactivities with zero being a little and three being incredibly:+TEXT\nVersion 2\nGAD-1\nOn a scale of 0 to 3, estimate how anxious this person is with 0 being a little anxious and 3 being\nincredibly anxious:+TEXT\nGAD-2\nOn a scale of 0 to 3, estimate how worried this person is with 0 being a little worried and 3 being\nincredibly worried:+TEXT\nPHQ-1\nOn a scale of 0 to 3, estimate how depressed this person is with 0 being a little depressed and 3 being\nincredibly depressed:+TEXT\nPHQ-2\nOn a scale of 0 to 3, estimate if this person is expressing no interest in previously enjoyed activities\nwith 0 being a little and 3 being incredibly:+TEXT\nVersion 3\nGAD-1\nOn a scale of zero to three, rate the anxiety of this message:+TEXT\nGAD-2\nOn a scale of zero to three, rate the worry in this message:+TEXT\nPHQ-1\nOn a scale of zero to three, rate the depression in this message:+TEXT\nPHQ-2\nOn a scale of zero to three, rate the interest in previously enjoyed activities in this message:+TEXT\nVersion 4\nGAD-1\nGeneralised anxiety disorder is a mental health illness that is defined by people having\nfeelings of excessive anxiety. On a scale of zero to three, rate the anxiety in this message:+TEXT\nGAD-2\nGeneralised anxiety disorder is a mental health illness that is defined by people having\nfeelings of excessive worry. On a scale of zero to three, rate the anxiety in this message:+TEXT\nPHQ-1\nDepression, or major depressive disorder is a mental health illness that is categorised\nby people feeling down, depressed or hopeless. On a scale of zero to three, rate the depression\nin this message:+TEXT\nPHQ-2\nDepression, or major depressive disorder is a mental health illness that is categorised\nby people having little interest or pleasure in doing things. On a scale of zero to three, rate\nthe depression in this message:+TEXT\nTable 1: Examples for different prompting versions for GAD and PHQ questions.\n6\n"}, {"page": 7, "text": "GAD-1\nGeneralised anxiety disorder is a mental health illness that is defined by people having feelings of\nexessive anxiety. The anxiety in this example is rated zero: ”like uh a a a guy that likes to\nsee different sights and go different places uh like different i’m a big movie person so i like going to\ntheatres stuff like that try to free myself from the situation to clear my mind usually take a walk or\nsomething” The anxiety in this example is rated one: ”got stuck out here then i had my baby\nand i just stayed be doing a little bit of everything a good night’s sleep i grew up not getting to know\nwho my mom was out there just on the food basis they like to give you a lotta food” The anxiety\nin this example is rated two: ”it was it was great and now it’s just okay i’m hardworking i’m uh i\ncare about everyone i yep i suppose going on a trip to canada with my daughter was very memorable\ni love la i love the traffic on the four o five i love the beach” The anxiety in this example is rated\nthree: ”regret right now is um that’s just how she is value we were always what what’s the word\ni’m looking for um also the weather was just it was absolutely awful there and it was just it was very\ndifficulties in my life she’s very touchy i would say in.” On a scale of zero to three, rate the anxiety\nin this message:+TEXT\nGAD-2\nGeneralised anxiety disorder is a mental health illness that is defined by people having feelings of\nexcessive worry. The worry in this example is rated zero: ”i’ve added that to my life she’s\na great woman and uh that started a future in in athletics and that’s what i wanted to do so i\naccomplished my goals so at the time it was a great a great thing and it still is a great thing it’s”\nThe worry in this example is rated one: ”lot of stress every day just everyday stress uh yeah\npretty much that’s what triggers it play like games like i like to play video games those are fun\nstuff like that okay thanks eh my family it’s just eh we’re not like too too close we kinda argue a”\nThe worry in this example is rated three: ”guess that and not and then there’s you i don’t\ncompletely lose it for days at a time when i can get really into that and sort of shut out the rest of\nthe world um the music and the the thoughts just kind of it really makes me a” On a scale of zero to\nthree, rate the anxiety in this message:+TEXT\nPHQ-1\nDepression, or major depressive disorder is a mental health illness that is categorised by people feeling\ndown, depressed or hopeless. The depression in this example is rated zero: ”like uh a a a guy\nthat likes to see different sights and go different places uh like different i’m a big movie person so i\nlike going to theatres stuff like that try to free myself from the situation to clear my mind usually\ntake a walk or something” The depression in this example is rated one: ”of my friend i wish i\nwould’ve handled his sister a little differently as far as the dirt it has been yes it was uh it’s very close\num no i have not we’ve always maintained our friendship um i can’t recall one off hand no problem\nand that’s all” The depression in this example is rated two: ”fight or anything or yeah any of\nthat my grandma she’s always giving me encouragement and um she’s a therapist a licensed therapist\nso she’s always you know made it really really um yeah i’m okay yeah um i just try to stay positive i\ntry to think like okay” The depression in this example is rated three: ”to my son my son and\ndaughter-in-law and my daughter and i went out to have hawaiian food got together it’s always fun\nto get together we got together at my house on christmas i enjoyed just getting together with them\ni guess the newness wore off that just have” On a scale of zthat’sero to three, rate the depression in\nthis message:+TEXT\nPHQ-2\nDepression, or major depressive disorder is a mental health illness that is categorised by people having\nlittle interest or pleasure in doing things. The depression in this example is rated zero: ”like\nuh a a a guy that likes to see different sights and go different places uh like different i’m a big movie\nperson so i like going to theatres stuff like that try to free myself from the situation to clear my mind\nusually take a walk or something” The depression in this example is rated one: ”of my friend\ni wish i would’ve handled his sister a little differently as far as the dirt it has been yes it was uh it’s\nvery close um no i have not we’ve always maintained our friendship um i can’t recall one off hand\nno problem and that’s all” The depression in this example is rated two: ”um feel uninhibited\nuninhibited and i was better prepared about three years ago um i was happy that he was safe try not\nto she has a house i mean a roof over her head to resort to the situation that he was in um because\nand to have been” The depression in this example is rated three: ”i don’t sleep well um well i\nstart to like cry a lot and i start to get really irritable um i argued with my mom and sister yesterday\nit was just something stupid over like yeah i was like okay well this is the problem and it i just”\n<< /SY S >> On a scale of zero to three, rate the depression in this message:+TEXT\nTable 2: Examples for version 5 prompting for the GAD and PHQ questions.\n7\n"}, {"page": 8, "text": "Binary PHQ-4\nRate the anxiety and depression of this message with a score of 0 for no anxiety\nor depression or 1 for anxiety or depression. Only output a 0 or a 1: +TEXT\nMulticlass PHQ-4\nOn a scale of 0 to 3, rate the severity of anxiety and depression in this message:\n+TEXT\nTable 3: Examples of the adapted prompting versions for GPT 3.5 Turbo fine-tuning.\nPrompt\nShots\n# Prompts\nZero-Shot-Basic\n0\n5\nFew-Shot-Basic\n5\n5\nFew-Shot-Extended\n5\n15\nFew-Shot-Enhanced\n7\n5\nTable 4: Prompting structure for depression.\nPrompt\nShots\n# Prompts\nTopics\nZero-Shot-Basic\n0\n5\n-\nFew-Shot-Basic\n5\n5\n-\nGeneral\n0\n1\n5\nSpecific\n0\n1\n43\nTable 5: Prompting structure for stress.\n2.1.4\nLLM Fine-Tuning\nIn addition to comparing the default GPT and Llama 3 LLMs, we conducted a separate experiment\nto assess the impact of fine-tuning GPT-3.5 Turbo. This experiment comprised two sub-tasks. The\nfirst sub-task involved a multiclass classification task based on the PHQ-4 questionnaire, with four\noutput classes: 0 for none, 1 for mild, 2 for moderate, and 3 for severe. The second sub-task involved\na binary classification of PHQ-4, where non-symptomatic and mildly symptomatic instances were\ngrouped into Class 0, and moderate to severe instances were assigned to Class 1 (see Table 3). We\nfine-tuned GPT 3.5 turbo on the DAIC-WOZ dataset, whereby the training set consisted of 28,186\nobservations and the test set contained 8,710.\n2.2\nSynthetic Data Generation and Training\nIn contrast to training transformer-based models and XGBoost models on the existing DAIC-WOZ\ndataset, we investigate the use of generative LLMs to create synthetic datasets aimed at enhancing\nclassification models for depression and stress detection. This approach addresses common chal-\nlenges in mental health datasets, such as data scarcity and class imbalance [27]. Tables 4 and 5\npresent the prompt configurations and example setups used for synthetic data generation.\n2.2.1\nData Preprocessing\nSimilarly, we utilize the DAIC-WOZ dataset for synthetic data generation, incorporating individu-\nals’ conversations in few-shot scenarios when prompting GPT-3.5 with real-world examples. This\napproach aims to enhance the model’s ability to generalize to diverse conversational patterns.\nIn our focus on stress detection, we leverage the Stress Detection dataset (see Section 3.1.2),\nwhich includes both text and metadata from Reddit posts, in combination with the Twitter Full\ndataset, to create a comprehensive stress detection corpus. The combined dataset was then par-\ntitioned into 9,249 training examples and 2,313 test examples, providing a robust foundation for\nevaluation.\n8\n"}, {"page": 9, "text": "Zero-Shot-Basic (for Depression)\n1. Depression is a major problem affecting many people around the world. Generate some examples\nof messages from someone who is likely to be suffering from depression:\n2. Depression is a common and often unnoticed disorder. Generate example messages from someone\nwho could be suffering from depression:\n3. Generate messages from the first person from someone who is experiencing depression:\n4. Generate messages that suggest that the person who wrote them might be suffering from major\ndepressive disorder:\n5. Depression is often categorized as feelings of sadness, hopeless and without interest in previously\nenjoyed activites. Generate examples from the first person of someone who might be experiencing\ndepression:\nFew-Shot-Basic (for Depression)\n1. Here are some examples of messages from someone who is suffering from depression: 5 Examples\n2. Here are some examples messages of someone who is likely to be suffering from major depressive\ndisorder: 5 Examples ...\n3. This person is probably suffering from depression: 5 Examples ...\n4. These are messages from someone who is likely showing moderate or severe levels of depression: 5\nExamples ...\n5. Major depressive disorder (MDD), also known as clinical depression, is a mental disorder charac-\nterized by at least two weeks of pervasive low mood, low self-esteem, and loss of interest or pleasure\nin normally enjoyable activities. Here are some messages from someone who is likely suffering from\nderpession: 5 Examples ...\nFew-Shot-Extended (Depression)\n1. Depression is a major problem affecting many people around the world. Here are some examples\nof messages from people who are likely suffering from depression: 5 Examples ...\n2. Depression is a common and often unnoticed disorder. Here are some examples of people who could\nbe suffering from depression: 5 Examples ...\n3. These messages are from someone who is experiencing depression: 5 Examples ...\n4. These messages suggest that the person might be suffering from major depressive disorder:\n5. These messages are from a person who is experiencing low mood: 5 Examples ...\n6. In these messages, there are indications that the individual is likely experiencing symptoms of\nmajor depressive disorder: 5 Examples ...\n7. These messages reflect someone’s experience of feeling overwhelmed by sadness and hopelessness:\n5 Examples ...\n8. The author of these messages likely feels sad, hopeless and without interest in previously enjoyed\nactivites: 5 Examples ...\n9. These texts suggest that the person is experiencing a loss of interest in activities they used to enjoy,\na common symptom of depression: 5 Examples ...\n10. These messages indicate persistent feelings of worthlessness or guilt, often associated with depres-\nsion: 5 Examples ...\n11. These texts suggest that the person is experiencing changes in appetite or weight, which can be\nsymptoms of depression: 5 Examples ...\n12. These messages indicate that the individual is experiencing significant changes in sleep patterns,\nwhich are often linked to depression: 5 Examples ...\n13. Persistently expressing thoughts of low mood or lack of interest in doing previously enjoyed ac-\ntivitites, indicate moderate or severe depression. The person who wrote these messages could be\nsuffering from depression: 5 Examples ...\n14. The following messages are examples of someone who might be having difficulties in concentration,\ndecision-making, or memory, common cognitive symptoms of depression: 5 Examples ...\n15. These texts suggest that the person is withdrawing from social interactions, a common symptom\nof depression: 5 Examples ...\nTable 6: Examples used for synthetic data generation for depression.\n9\n"}, {"page": 10, "text": "2.2.2\nPrompting Strategies for Depression\nTo generate data tailored for both depression and stress contexts, we employed a range of prompting\nstrategies. These strategies illustrated in Table 6 aimed to capture diverse aspects of mental health\nthrough varied approaches.\nTo enhance data diversity without using examples from the base\ndataset, we applied a Zero-Shot-Basic Prompt strategy. This involved five prompts designed\nto generate data for depression and non-depression scenarios without any specific examples from\nthe dataset, aiming to capture a wider range of expressions.\nThe Few-Shot-Basic Prompts\nstrategy involved designing five distinct prompts, each with five examples, to illustrate scenarios\nof depression and non-depression. The prompts were carefully crafted to guide the generation of\nrepresentative messages for each context. Building on the Few-Shot-Basic approach, the Few-Shot-\nExtended Prompts introduced ten additional prompts. This expansion aimed to increase the\ndiversity of generated data by providing a broader range of examples, further refining the context\nof both depression and non-depression. The Few-Shot-Enhanced Prompts were an extension\nof the Few-Shot-Basic strategy, incorporating a larger number of examples from the DAIC-WOZ\ndataset. This approach aimed to refine the generated messages by offering more detailed guidance\nto the language model.\n2.2.3\nPrompting Strategies for Stress\nSimilar to the synthetic data generation for depression, we leverage a Zero-Shot and Few-Shot\nprompting strategy (Table 7). In addition to these strategies outlined above, we employed the\nGeneral Prompt strategy, which incorporated a broad range of stress-related topics for generating\nsynthetic data aimed at stress detection. This strategy encompassed major life events, daily hassles,\nand organizational stressors, integrating these topics into the prompts for the LLMs to provide\ninsights into the psychological states linked to various stressors. Specifically, the prompts addressed\ncrises, microstressors, ambient stressors, and organizational stressors, offering a comprehensive\nperspective on the psychological impact of different types of stress.\nSimilar to the general prompts, the Specific Prompts were based on the Holmes and Rahe\nStress Scale [14] and focused on 43 stress-inducing life events. These included significant events\nsuch as the death of a spouse and divorce, offering a targeted approach to generating stress-related\ndata.2\nAs with the general prompts, these topics were incorporated into the LLM prompts to\nensure focused and relevant data generation.\n2.2.4\nTraining\nWe first trained the depression and stress models using only the baseline datasets, i.e., the DAIC-\nWOZ and Stress Detection datasets, without any synthetic data, in order to establish baseline\nperformance. Synthetic data were then generated to augment these baseline datasets.\nFor depression, we created four synthetic data versions with 10,000 generated examples each,\ntwo versions with 100,000 examples, and two versions with 500,000 examples. From the 10,000-\nexample datasets, the top two prompting strategies were selected based on F1 performance and\nextended to the 100,000 and 500,000 datasets. Training cycles were conducted to combine the\nfour 10,000-example datasets into a single 40,000-example dataset, in order to assess the combined\nvalue of all prompting strategies. Similarly, the two 100,000-example datasets were merged into a\n200,000-example synthetic dataset, and the two 500,000-example datasets were consolidated into 1\nmillion synthetic data points.\n2Full list of Holmes and Rahe Stress Scale:\nhttps://www.stress.org/wp-content/uploads/2024/02/Holmes-Rahe-Stress-inventory.pdf\n10\n"}, {"page": 11, "text": "Zero-Shot-Basic (Stress)\nStress is a major problem affecting many people around the world. Generate some examples of messages\nfrom someone who is likely to be suffering from stress:\nStress is a common and often unnoticed disorder. Generate example messages from someone who could\nbe suffering from stress:\nGenerate messages from the first person from someone who is experiencing stress:\nGenerate messages that suggest that the person who wrote them might be suffering from stress:\nFew-Shot-Basic (Stress)\nHere are some examples of messages from someone who is suffering from stress: 5 Examples ...\nHere are some examples messages of someone who is likely to be suffering from stress: 5 Examples ...\nThis person is probably suffering from stress: 5 Examples ...\nThese are messages from someone who is likely showing moderate or severe levels of stress: 5 Examples\n...\nGeneral Topic Prompts\nThese are the criteria of different stress risk level: Risk Level=No Stress: I do not see evidence that this\nperson is suffering from stress. Risk Level=Stress: I believe this person is at high risk of suffering from\nstress.\nYour task is to generate a text for each of the following ”topics” with different Risk levels.\n1-\nCrises/catastrophes, 2-Major life events, 3-Daily hassles/microstressors, 4-Ambient stres-\nsors, 5-Organisational stressors,\nProvide the answers with the following columns: text, topic, risk level. Risk level criteria: ”Stress”\nSpecific Topic Prompts\nThese are the criteria of different stress risk level: Risk Level=No Stress: I do not see evidence that\nthis person is suffering from stress. Risk Level=Stress: I believe this person is at high risk of suffering\nfrom stress. Your task is to generate a text for each of the following ”topics” with different Risk levels.\n1-Death of a spouse, 2-Divorce, 3-Marital separation, 4-Imprisonment, 5-Death of a close\nfamily member, 6-Personal injury or illness, 7-Marriage, 8-Dismissal from work, 9-Marital\nreconciliation, 10-Retirement, 11-Change in health of family member, 12-Pregnancy, 13-\nSexual difficulties, 14-Gain a new family member, 15-Business readjustment, 16-Change in\nfinancial state, 17-Death of a close friend, 18-Change to different line of work, 19-Change\nin frequency of arguments, 20-Major mortgage, 21-Foreclosure of mortgage or loan, 22-\nChange in responsibilities at work, 23-Child leaving home, 24-Trouble with in-laws, 25-\nOutstanding personal achievement, 26-Spouse starts or stops work, 27-Begin or end school,\n28-Change in living conditions, 29-Revision of personal habits, 30-Trouble with boss, 31-\nChange in working hours or conditions, 32-Change in residence, 33-Change in schools,\n34-Change in recreation, 35-Change in church activities, 36-Change in social activities, 37-\nMinor mortgage or loan, 38-Change in sleeping habits, 39-Change in number of family\nreunions, 40-Change in eating habits, 41-Vacation, 42-Minor violation of law\nProvide the answers with the following columns: text, topic, risk level. Risk level criteria: ”Stress”\nTable 7: Examples used for synthetic data generation for stress.\n11\n"}, {"page": 12, "text": "For stress, we used two training sets generated from the original prompting strategies, i.e.,\nZero-Shot and Few-Shot strategies, each containing 100,000 examples, as well as two additional\nsets generated from general and specific strategies, also with 100,000 examples each. Consistent\nwith the depression approach, these datasets were merged to create 200,000 examples for each\nstrategy grouping. All generated data were integrated with the Stress Detection training set.\n3\nExperimental Setup\nThis section provides insights at the datasets and models utilized in our work. Additionally, it\ndetails the evaluation metrics employed to present the outcomes effectively.\n3.1\nDatasets\n3.1.1\nDAIC-WOZ\nWithin this comparison, we leveraged the DAIC-WOZ dataset [13], which comprises clinical inter-\nviews aimed at aiding the assessment of psychological distress conditions like anxiety, depression,\nand post-traumatic stress disorder. These interviews were gathered as part of a broader initiative to\ndevelop a computer-based system that conducts interviews with individuals and recognises verbal\nand nonverbal cues associated with mental health issues. Specifically, it encompasses data from\nWizard-of-Oz interviews, where an animated virtual interviewer named Ellie, under the control of\na human interviewer in a separate location, conducted the interviews. The data, which consists of\n189 interaction sessions, each lasting between 7 to 33 minutes, has been originally transcribed and\nannotated to encompass a range of verbal and non-verbal characteristics.\n3.1.2\nStress Detection dataset\nThe Stress Detection dataset3 integrates Reddit and Twitter data to analyze stress-related content.\nThe data, labeled as stress-positive or stress-negative, was collected using the Reddit API and\nTwitter API V2 from September 2019 to September 2021, capturing trends during the COVID-19\npandemic. Reddit data was collected from subreddits like r/Stressed and r/Depression for stress-\npositive examples, and r/Happy and r/Wholesome for stress-negative examples. Twitter data was\ngathered using hashtags such as #Stress and #FeelingStressed for stress-positive examples, and\n#Happiness and #Joy for stress-negative examples.\nReddit data was detailed and structured,\nwhile Twitter data was shorter and less grammatically structured.\n3.2\nModels\n3.2.1\nTransformer Models\nFor a comparison to LLMs, we leverage the Transformer models [32], which rely on a self-attention\nmechanism, allowing it to capture contextual dependencies in input sequences efficiently. The model\nconsists of an encoder-decoder structure, with multi-head self-attention layers enabling parallelised\nprocessing of input tokens. Positional encoding is used to provide information about the token’s\nposition in the sequence. The Transformer’s attention mechanism facilitates capturing long-range\ndependencies, making it highly effective for tasks requiring context understanding. Within this\nwork, we compare the BERT and Roberta Transformer models and their distilled versions, i.e. Dis-\ntilBert and Distil-Roberta. In addition to that we leverage the XLNet models as well. The BERT\n3https://www.kaggle.com/datasets/tihsrahly/stress-detection-dataset\n12\n"}, {"page": 13, "text": "model [10] is pre-trained on large corpora and can then be fine-tuned for specific natural language\nprocessing (NLP) tasks, such as text classification, named entity recognition, and question answer-\ning, among others. BERT embeddings have been widely adopted and have significantly improved\nthe state-of-the-art performance in various NLP applications. DistilBERT [28] is a distilled version\nof BERT, offering a more compact and faster alternative for tasks in natural language process-\ning (NLP). Despite having fewer parameters, DistilBERT embeddings can be utilised in various\nNLP applications, providing a balance between computational efficiency and model performance.\nRoBERTa [18], or Robustly optimised BERT approach, is a variant of the BERT (Bidirectional\nEncoder Representations from Transformers) model, uses dynamic masking during pre-training,\nremoving the Next Sentence Prediction (NSP) objective, and training with larger mini-batches and\nlearning rates. These modifications result in a more robust and efficient model. XLNet [38] is a\nTransformer-based language model, which combines ideas from autoregressive language modeling\n(as seen in models like GPT) and autoencoding (as in BERT) to capture bidirectional context and\nmaintain long-term dependencies in sequences. Instead of predicting the next word in a sentence,\nXLNet is trained to predict a permutation of the words. This approach allows the model to consider\nbidirectional context while preventing it from seeing the entire context during training, enhancing\nits ability to capture dependencies. For all models, we leverage the base version of the transformer\nmodels.\n3.2.2\nXGBoost\nXGBoost (Extreme Gradient Boosting) [5] is an ML algorithm used for both classification and\nregression tasks. XGBoost is based on the gradient boosting framework, which is an ensemble\nlearning technique. It builds an ensemble of decision trees sequentially, where each tree corrects\nthe errors made by the previous ones.\nThe model uses a customizable objective function that\nneeds to be optimised during training. For regression tasks, the objective is often mean squared\nerror (MSE), while for classification tasks, it can be log loss (binary or multiclass). To prevent\noverfitting, XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization techniques into the\nobjective function.\n3.2.3\nOpenAI GPT\nWithin this work, we leveraged the GPT-3.5 Turbo model, a large language model based on the\ndecoder-only Transformer architecture, employing attention mechanisms to focus on the most rel-\nevant segments of input text. It supports a context length of up to 2048 tokens and features 175\nbillion parameters, enabling exceptional zero-shot and few-shot learning capabilities across diverse\ntasks. The model was trained on a vast corpus of text, primarily sourced from a filtered version of\nCommon Crawl (60% of the pre-training dataset), alongside datasets such as WebText2, Books1,\nBooks2, and Wikipedia, comprising a total of 410 billion byte-pair-encoded tokens. Additionally,\nGPT-3.5 demonstrates proficiency in programming languages, including CSS, JSX, and Python.\nThe model’s adaptability is enhanced through fine-tuning, allowing customization for specific do-\nmains and ensuring greater standardization and control over outputs. GPT-3.5 has been employed\nfor two key tasks: classification and synthetic data generation.\n3.2.4\nLLama 3\nLlama 3 [11] is a large language model designed to perform well in both benchmark tests and prac-\ntical applications. It introduces a high-quality human evaluation set to assess its capabilities across\n12 diverse tasks, including coding, reasoning, and summarization, while implementing measures\n13\n"}, {"page": 14, "text": "to prevent overfitting. Key features include a tokenizer with 128K tokens, pretraining on over 15\ntrillion tokens, and an optimized scaling strategy for efficient training. Additionally, the model\nincorporates advanced safety mechanisms, such as Llama Guard 2 and CyberSecEval 2, for secure\ndeployment.\n3.3\nEvaluation Metrics\nBesides analysing the widely used metrics, i.e., weighted precision, recall and F1 for our experi-\nments, we extend our metrics with further metrics in the field of statistics and medicine. Weighted\nspecificity describes the accuracy of a test that reports the presence or absence of a medical con-\ndition. It can be useful for ”ruling in” disease since the test rarely gives positive results in healthy\npatients. A test with a specificity of 1.0 will recognise all patients without the disease by testing\nnegative, therefore a positive test result would rule in the presence of the disease. Nevertheless, a\nnegative result from a test with high specificity is not necessarily useful for ”ruling out” a disease.\nAs an example, a test that always returns a negative test result will have a specificity of 1.0 because\nspecificity does not consider false negatives. A test like that would return negative for patients with\nthe disease, making it useless for ”ruling out” the disease.\nFurthemore, we leverage the Hamming loss and the AUC-ROC Curve. The Hamming loss\nis a metric used in multi-label classification to quantify the accuracy of predictions by measuring\nthe fraction of incorrectly predicted labels across all instances.\nIt is calculated as the average\nfraction of incorrectly predicted labels per instance, with a score of 0 indicating perfect predictions\nand 1 indicating complete misclassification. The Hamming loss accounts for both false positives\nand false negatives in the predicted label sets, making it a valuable measure for evaluating the\noverall performance of multi-label classification models. The AUC-ROC (Area Under the Receiver\nOperating Characteristic Curve) is a graphical representation of a binary classification model’s\nperformance across various threshold settings.\nIt plots the true positive rate against the false\npositive rate, illustrating the trade-off between sensitivity and specificity. The AUC-ROC value\nquantifies the model’s ability to distinguish between classes, with a higher AUC indicating better\noverall performance.\n4\nResults\nWithin this section, we provide the insights on evaluating different prompting strategies, as well\nas how Llama 3 8B and GPT 3.5 Turbo perform compared to XGBoost and different Transformer\nmodels.\n4.1\nAssessment of Anxiety and Depression Prediction Models\nIn this section, we explore the performance of transformer models compared to LLMs on the PHQ\nand GAD questions, providing a detailed analysis of their effectiveness.\n4.1.1\nAnxiety and Depression Prediction with Transformer Models\nIn our investigation, we first compared the performance of various baseline transformer models\nfine-tuned on the DAIC-WOZ dataset (Table 8). Notably, Distil-RoBERTa achieved the highest\nF1 scores for both GAD-1 and GAD-2. For GAD-1, BERT, RoBERTa, and XLNet excelled in\nspecificity, while DistilBERT and XLNet showed the best specificity for GAD-2. In PHQ assess-\nments, XLNet delivered the highest precision, recall and F1 score for PHQ-1. Additionally, XLNet\n14\n"}, {"page": 15, "text": "GAD-1\nGAD-2\nPrec Rec\nF1\nSpec HammL AUC-ROC Prec Rec\nF1\nSpec HammL AUC-ROC\nBERT 0.56 0.56 0.53 0.44\n0.59\n0.66\n0.67 0.69 0.67 0.31\n0.62\n0.52\nDistilBERT 0.59 0.58 0.56 0.42\n0.63\n0.72\n0.65 0.67 0.65 0.33\n0.58\n0.52\nRoBERTa 0.55 0.56 0.55 0.44\n0.62\n0.70\n0.67 0.70 0.65 0.30\n0.56\n0.48\nDistil-RoBERTa 0.57 0.58 0.56 0.42\n0.63\n0.70\n0.69 0.70 0.68 0.30\n0.62\n0.52\nXLNet 0.55 0.56 0.54 0.44\n0.61\n0.70\n0.64 0.67 0.62 0.33\n0.56\n0.45\nPHQ-1\nPHQ-2\nPrec Rec\nF1\nSpec HammL AUC-ROC Prec Rec\nF1\nSpec HammL AUC-ROC\nBERT 0.50 0.52 0.49 0.48\n0.59\n0.68\n0.53 0.54 0.52 0.46\n0.61\n0.71\nDistilBERT 0.53 0.54 0.52 0.46\n0.60\n0.69\n0.57 0.57 0.54 0.43\n0.61\n0.72\nRoBERTa 0.56 0.56 0.54 0.44\n0.61\n0.69\n0.57 0.57 0.56 0.43\n0.64\n0.73\nDistil-RoBERTa 0.55 0.55 0.53 0.45\n0.61\n0.71\n0.55 0.57 0.54 0.43\n0.62\n0.73\nXLNet 0.58 0.58 0.55 0.42\n0.64\n0.70\n0.58 0.58 0.56 0.42\n0.63\n0.73\nTable 8: Insights on weighted precision, recall, F1, specificity, Hamming loss (HammL) and AUC-\nROC (Area Under the Receiver Operating Characteristic Curve for different Transformer models\n(bold scores represent best result for each metric).\noutperformed other models in precision, recall, and F1 score for PHQ-2 inquiries. These results\nhighlight the effectiveness of transformer models in capturing the nuanced aspects of anxiety and\ndepression, demonstrating their potential in mental health evaluation.\n4.1.2\nAnxiety and Depression Prediction with Llama\nTable 9 presents an analysis of the targeted metrics for different prompting variants obtained by\nLlama 3 8B across GAD-1, GAD-2, PHQ-1, and PHQ-2. For GAD-1, Version 3 prompting (On a\nscale of zero to three, rate the anxiety of this message) exhibited the highest performance across\nmultiple metrics, including weighted F1, precision, recall, Hamming Loss, and AUC-ROC. However,\nVersion 1 (On a scale of zero to three, estimate how anxious this person is with zero being a little\nanxious and three being incredibly anxious) displayed better specificity.\nConversely, for GAD-\n2, various prompting versions yielded similar high F1 scores, with the best specificity achieved\nby Version 3 and the highest AUC-ROC by Version 1. Regarding PHQ-1, Version 1 prompting\nresulted in the best weighted F1 score, while Version 3 yielded the highest precision. Similarly, for\nPHQ-2, Version 3 prompting demonstrated the best performance in terms of F1 score, precision,\nand specificity. These findings suggest the importance of prompt formulation in optimizing model\nperformance for anxiety and depression assessment tasks.\n4.1.3\nAnxiety and Depression Prediction Comparison\nIn our comprehensive analysis, we consolidate the optimal strategies derived from using Llama\n3’s prompting Version 3 and the Distil-RoBERTa transformer model, both of which exhibited the\nbest or most competitive performance across GAD and PHQ inquiries. Additionally, we evaluate\nthe predictive capabilities of XGBoost and OpenAI’s GPT-3.5 Turbo model. As shown in Table\n10, XGBoost consistently outperforms other models in terms of the Hamming loss metric. When\ncomparing Llama 3 with GPT-3.5 Turbo, we observe a slight advantage for GPT-3.5 Turbo, par-\nticularly in performance on GAD-2, PHQ-1, and PHQ-2 inquiries. Finally, when comparing the\n15\n"}, {"page": 16, "text": "GAD-1\nGAD-2\nPrec\nRec\nF1\nSpec HammL AUC-ROC\nPrec\nRec\nF1\nSpec HammL AUC-ROC\nVersion 1\n0.31\n0.29\n0.22\n0.69\n0.71\n0.49\n0.53\n0.52\n0.50\n0.48\n0.48\n0.51\nVersion 2\n0.33\n0.32\n0.21\n0.66\n0.68\n0.50\n0.44\n0.66\n0.53\n0.33\n0.34\n0.50\nVersion 3 0.38 0.33 0.33\n0.68\n0.67\n0.52\n0.56\n0.23\n0.27\n0.78\n0.77\n0.50\nVersion 4\n0.11\n0.33\n0.16\n0.67\n0.67\n0.50\n0.44\n0.67 0.53\n0.33\n0.33\n0.50\nVersion 5\n0.11\n0.33\n0.16\n0.67\n0.67\n0.50\n0.44\n0.67 0.53\n0.33\n0.33\n0.50\nPHQ-1\nPHQ-2\nPrec\nRec\nF1\nSpec HammL AUC-ROC\nPrec\nRec\nF1\nSpec HammL AUC-ROC\nVersion 1\n0.37\n0.36\n0.32\n0.64\n0.64\n0.50\n0.29\n0.35\n0.26\n0.64\n0.65\n0.50\nVersion 2\n0.21\n0.46\n0.29\n0.54\n0.55\n0.50\n0.17\n0.41\n0.24\n0.59\n0.59\n0.50\nVersion 3 0.43\n0.29\n0.30\n0.76\n0.71\n0.53\n0.34\n0.34\n0.32 0.65\n0.66\n0.49\nVersion 4\n0.21\n0.46\n0.29\n0.54\n0.54\n0.50\n0.17\n0.41\n0.24\n0.59\n0.59\n0.50\nVersion 5\n0.21\n0.46\n0.29\n0.54\n0.54\n0.50\n0.17\n0.41\n0.24\n0.59\n0.59\n0.50\nTable 9: Insights on weighted precision, recall, F1, specificity, Hamming loss (HammL) and AUC-\nROC (Area Under the Receiver Operating Characteristic Curve for different prompting variant\n(bold scores represent best result for each metric).\nGAD-1\nGAD-2\nPrec Rec\nF1\nSpec HammL AUC-ROC Prec Rec\nF1\nSpec HammL AUC-ROC\nXGBoost 0.45 0.55 0.48\n0.64\n0.45\n0.56\n0.63 0.69 0.60 0.34\n0.31\n0.51\nDistil-RoBERTa 0.57 0.58 0.56\n0.42\n0.63\n0.70\n0.69 0.70 0.68 0.30\n0.62\n0.52\nLlama 3 8B (v3) 0.38 0.33 0.33\n0.68\n0.67\n0.52\n0.56 0.23 0.27 0.78\n0.77\n0.50\nGPT-3.5 Turbo 0.36 0.29 0.31\n0.71\n0.71\n0.51\n0.54 0.37 0.44 0.62\n0.63\n0.53\nPHQ-1\nPHQ-2\nPrec Rec\nF1\nSpec HammL AUC-ROC Prec Rec\nF1\nSpec HammL AUC-ROC\nXGBoost 0.51 0.54 0.48\n0.61\n0.46\n0.55\n0.52 0.53 0.48 0.66\n0.47\n0.56\nDistil-RoBERTa 0.55 0.55 0.53\n0.45\n0.61\n0.71\n0.55 0.57 0.54 0.43\n0.62\n0.73\nLlama 3 8B (v3) 0.43 0.29 0.30\n0.76\n0.71\n0.53\n0.34 0.34 0.32 0.65\n0.66\n0.49\nGPT-3.5 Turbo 0.38 0.39 0.39\n0.64\n0.61\n0.52\n0.37 0.31 0.33 0.71\n0.69\n0.50\nTable 10: Comparison on weighted precision, recall, F1, specificity, Hamming loss (HammL) and\nAUC-ROC (Area Under the Receiver Operating Characteristic Curve for XGBoost, Llama 3 8B,\nGPT-3.5 Turbo and Distil-RoBERTa (bold scores represent best result for each metric).\n16\n"}, {"page": 17, "text": "Binary PHQ-4\nPrec\nRec\nF1\nSpec\nHammD\nAUC-ROC\nDistil-RoBERTa\n0.82\n0.85\n0.82\n0.32\n0.15\n0.58\nGPT-3.5 Turbo\n0.78\n0.48\n0.55\n0.67\n0.52\n0.57\nGPT-3.5 Turbo fine-tuned\n0.80\n0.77\n0.78\n0.49\n0.23\n0.63\nMulticlass PHQ-4\nPrec\nRec\nF1\nSpec\nHammL\nAUC-ROC\nDistil-RoBERTa\n0.57\n0.59\n0.57\n0.69\n0.41\n0.61\nGPT-3.5 Turbo\n0.39\n0.39\n0.39\n0.62\n0.61\n0.51\nGPT-3.5 Turbo fine-tuned\n0.40\n0.45\n0.38\n0.58\n0.55\n0.51\nTable 11: Comparison on weighted precision, recall, F1, specificity, Hamming loss (HammL) or\nHamming distance (HammD) for binary and AUC-ROC (Area Under the Receiver Operating Char-\nacteristic Curve) for Distil-RoBERTa, GPT-3.5 Turbo and fine-tuned GPT-3.5 Turbo (bold scores\nrepresent best result for each metric).\nDistil-RoBERTa transformer model to Llama 3 8B and GPT-3.5 Turbo, our analysis reveals that\nDistil-RoBERTa outperforms all models across all GAD and PHQ inquiries in terms of weighted\nprecision, recall, and F1 scores.\n4.1.4\nPHQ-4 Prediction with fine-tuned GPT-3.5 Turbo\nTable 11 presents a comparison of Distil-RoBERTa, GPT-3.5 Turbo, and fine-tuned GPT-3.5\nTurbo models. For the binary PHQ-4 classification, Distil-RoBERTa outperforms others in preci-\nsion (0.82), recall (0.85), and F1 score (0.82), with fine-tuned GPT-3.5 Turbo achieving the best\nAUC-ROC score (0.63). Conversely, in the multiclass PHQ-4 classification, Distil-RoBERTa again\ndemonstrates best performance, achieving the highest precision (0.57), recall (0.59), F1 score (0.57),\nspecificity (0.69), and AUC-ROC (0.61).\n4.2\nAssessment of Depression and Stress with Synthetic Data\nBuilding on experiments with transformer-based models and large language models (LLMs), we next\ninvestigate generating synthetic data using GPT-3.5 Turbo. The evaluation of various prompting\nstrategies for depression detection, applied to the DAIC-WOZ test set, illustartes nuanced insights\n(Table 12). The baseline model, referred to as the DAIC-WOZ (DW) Baseline, achieved a preci-\nsion of 0.785, recall of 0.805, F1 score of 0.792, and ROC AUC of 0.634. Among the strategies\nusing 10,000 generated examples, the DW+Zero-Shot-Basic strategy achieved the highest recall\nof 0.814 and a ROC AUC of 0.598. Additionally, the DW+Few-Shot-Basic strategy demonstrated\ncomparable performance to the baseline, achieving a precision of 0.780 and an F1 score of 0.787.\nWith larger training datasets, the DW+Zero-Shot-Basic strategy with 100,000 examples per-\nformed well, achieving a precision of 0.785 and a recall of 0.813, with a ROC AUC of 0.614. The\ncombined DW+Few-Shot-Basic and Zero-Shot-Basic strategy with 200,000 examples maintained\nbalanced performance with a precision of 0.779 and a recall of 0.813. At the largest dataset sizes of\n500,000 and 1 million examples, performance remained generally stable, though precision and recall\nvalues slightly decreased. Specifically, the DW+Zero-Shot-Basic strategy with 500,000 examples\nachieved a precision of 0.768 and a ROC AUC of 0.595, while the combined Few-Shot-Basic and\nZero-Shot-Basic strategy at 1 million examples resulted in a precision of 0.777, recall of 0.808,\n17\n"}, {"page": 18, "text": "Prompting Strategy\n# Synt. Precision Recall\nF1\nSpecificity Hamming ROC AUC\nDW (DAIC-WOZ) Baseline\n-\n0.785\n0.805 0.792\n0.463\n0.195\n0.634\nDW+Zero-Shot-Basic\n10k\n0.782\n0.814\n0.785\n0.383\n0.186\n0.598\nDW+Few-Shot-Basic\n10k\n0.780\n0.806\n0.787\n0.424\n0.194\n0.615\nDW+Few-Shot-Extended\n10k\n0.772\n0.799\n0.780\n0.416\n0.201\n0.607\nDW+Few-Shot-Enhanced\n10k\n0.779\n0.809\n0.784\n0.397\n0.191\n0.603\nDW+all\n40k\n0.783\n0.811\n0.788\n0.410\n0.189\n0.611\nDW+Zero-Shot-Basic\n100k\n0.785\n0.813\n0.790\n0.415\n0.187\n0.614\nDW+Few-Shot-Basic\n100k\n0.782\n0.815 0.782\n0.365\n0.185\n0.590\nDW+Zero-Shot-Basic+Few-Shot-Basic\n200k\n0.779\n0.813\n0.781\n0.370\n0.187\n0.591\nDW+Zero-Shot-Basic\n500k\n0.768\n0.800\n0.776\n0.390\n0.200\n0.595\nDW+Few-Shot-Basic\n500k\n0.775\n0.810\n0.778\n0.364\n0.190\n0.587\nDW+Zero-Shot-Basic+Few-Shot-Basic\n1M\n0.777\n0.808\n0.783\n0.396\n0.192\n0.600\nTable 12:\nPerformance Metrics for DAIC-WOZ Dataset with Various Prompting Strategies\nand Generated Data (DW all = Zero-Shot-Basic+Few-Shot-Basic+Few-Shot-Extended+Zero-Shot-\nBasic+Few-Shot-Enhanced)\nPrompting Strategy\n# Synt. Precision Recall\nF1\nSpecificity Hamming ROC AUC\nSD (Stress Detection) Baseline\n-\n0.904\n0.904 0.904\n0.904\n0.096\n0.904\nSD+Zero-Shot-Basic\n100k\n0.887\n0.883\n0.884\n0.889\n0.117\n0.886\nSD+Few-Shot-Basic\n100k\n0.884\n0.884\n0.884\n0.874\n0.116\n0.879\nSD+Zero-Shot-Basic+Few-Shot-Basic\n200k\n0.884\n0.884\n0.883\n0.871\n0.116\n0.878\nSD+General Prompts\n100k\n0.882\n0.882\n0.882\n0.874\n0.118\n0.878\nSD+Specific Prompts\n100k\n0.869\n0.866\n0.867\n0.870\n0.134\n0.868\nSD+General+Specific Prompts\n200k\n0.860\n0.860\n0.860\n0.852\n0.140\n0.856\nTable 13: Performance Metrics for Stress Detection Models with Generated Data Integration.\nand an ROC AUC of 0.600. These results highlight the effectiveness of different prompting strate-\ngies and dataset sizes, with Zero-Shot-Basic 100k and its combinations showing strong overall\nperformance compared to the DW baseline.\nTable 13 illustrates the performance of various prompting strategies applied to the Stress Detec-\ntion (SD) dataset, evaluated across different metrics including precision, recall, F1 score, specificity,\nHamming distance, and ROC AUC. The baseline model, i.e., SD Baseline, serves as the reference\npoint with notably high performance metrics: precision, recall, F1 score, specificity, and ROC AUC\n(all achieving 0.904). This model also recorded a Hamming distance of 0.096, setting a benchmark\nfor comparison with other prompting strategies.\nFor the 100,000 generated examples, several prompting strategies were evaluated. The strat-\negy named SD+Zero-Shot-Basic achieved a precision of 0.887, recall of 0.883, F1 score of 0.884,\nROC AUC of 0.886, and Hamming distance of 0.117. Similarly, the SD+Few-Shot-Basic strategy\nproduced a precision, recall, and F1 score of 0.884, specificity of 0.874, ROC AUC of 0.879, and\nHamming distance of 0.116. When combining the Few-Shot-Basic and Zero-Shot-Basic strate-\ngies with 200,000 examples, comparable performance was observed, with a precision and recall of\n0.884, F1 score of 0.883, ROC AUC of 0.878, and Hamming distance of 0.116.\nAdditionally, the general and specific prompting strategies were evaluated. The SD+General\nPrompts strategy, using 100,000 generated examples, achieved a precision, recall, and F1 score of\n18\n"}, {"page": 19, "text": "0.882, specificity of 0.874, ROC AUC of 0.878, and Hamming distance of 0.118. The SD+Specific\nPrompts strategy, also with 100,000 generated examples, showed slightly lower performance, with\na precision of 0.869, recall of 0.866, F1 score of 0.867, specificity of 0.870, ROC AUC of 0.868,\nand Hamming distance of 0.134. Combining both General+Specific Prompts with 200,000 ex-\namples resulted in a slight performance decline, recording a precision, recall, and F1 score of 0.860,\nspecificity of 0.852, ROC AUC of 0.856, and Hamming distance of 0.140. In summary, while the\nbaseline model for stress detection achieved the highest performance across most of metrics, the\nvarious prompting strategies effectively contributed to stress detection, with Zero-Shot-Basic and\ncombinations of prompting strategies generally demonstrating the best results. The performance\nof general and specific prompts, though significant, was somewhat lower compared to the most\neffective strategies.\n5\nConclusions\nIn conclusion, mental ill-health remains a global challenge, affecting a significant portion of the\npopulation and highlighting the urgent need for effective interventions. Despite the advancements\nin Large Language Models across various NLP tasks, a notable research gap persists regarding\ntheir application and optimization in the mental health domain. This study addresses this gap\nby evaluating the performance of Llama 3 8B and GPT-3.5 Turbo models, comparing them with\ntraditional machine learning and deep learning models. Our exploration of different prompting\nstrategies for GAD and PHQ questions reveals that transformer-based models such as BERT and\nXLNet outperform LLMs with larger parameter sets. Notably, Distil-RoBERTa consistently out-\nperforms all models in terms of weighted precision, recall, and F1 score for both GAD and PHQ\nquestions. These findings provide valuable insights for the future development of language models\nto better address mental health challenges.\nMoreover, our study highlights the nuanced impact of incorporating synthetic data into baseline\nstress and depression classifiers. While the baseline models achieved high performance, especially for\nstress detection, the integration of the synthetically generated data did not always lead to improved\nresults.\nFor stress detection, prompting strategies like Zero-Shot-Basic and its combinations\nyielded better performance metrics, particularly in precision, recall, and ROC AUC, surpassing\nother prompting approaches (e.g. Few-Shot-Basic) in several cases. In contrast, for depression\ndetection, while some strategies showed improvements in recall and ROC AUC with larger datasets,\nthe inclusion of generated data often led to a decrease in precision and recall, suggesting that\nthe synthetically generated data, while diversifying the dataset, may dilute the classifier’s focus.\nThese findings underscore the need for further exploration of diverse language models and carefully\ncontrolled experimental setups, including external, out-of-domain datasets, to better understand\nthe implications of generated data on classifier performance.\nAs we continue to investigate the potential of large language models in the mental health space,\naddressing challenges such as biases in training data and the dynamic nature of mental health will\nbe critical in achieving unbiased and comprehensive model performance.\nReferences\n[1] Mihael Arcan, David-Paul Niland, and Fionn Delahunty. An assessment on comprehending\nmental health through large language models. arXiv preprint arXiv:2401.04592, 2024.\n19\n"}, {"page": 20, "text": "[2] Theodoros N Arvanitis, Sean White, Stuart Harrison, Rupert Chaplin, and George Despo-\ntou. A method for machine learning generation of realistic synthetic datasets for validating\nhealthcare applications. Health Informatics Journal, 28(2):14604582221077000, 2022. PMID:\n35414269.\n[3] Meghana Moorthy Bhat, Saghar Hosseini, Ahmed Hassan Awadallah, Paul N. Bennett, and\nWeisheng Li. Say ’yes’ to positivity: Detecting toxic language in workplace communications.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,\nFindings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November, 2021, pages 2017–2029. Association for\nComputational Linguistics, 2021.\n[4] Mirko Casu, Sergio Triscari, Sebastiano Battiato, Luca Guarnera, and Pasquale Caponnetto.\nAi chatbots for mental health: A scoping review of effectiveness, feasibility, and applications.\nApplied Sciences, 14(13), 2024.\n[5] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of\nthe 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nKDD ’16, pages 785–794, New York, NY, USA, 2016. ACM.\n[6] Parth Chodavadia, Irene Teo, Daniel Poremski, Daniel Shuen Sheng Fung, and Eric Andrew\nFinkelstein.\nPrevalence and economic burden of depression and anxiety symptoms among\nsingaporean adults: results from a 2022 web panel. BMC psychiatry, 23(1):104, 2023.\n[7] Neo Christopher Chung, George Dyer, and Lennart Brocki.\nChallenges of large language\nmodels for mental health counseling. arXiv preprint arXiv:2311.13857, 2023.\n[8] Fionn Delahunty, Robert Johansson, and Mihael Arcan. Passive Diagnosis Incorporating the\nPHQ-4 for Depression and Anxiety. In Proceedings of the Social Media Mining for Health\nApplications (SMM4H) Workshop, Florence, Italy, 2019.\n[9] Fionn Delahunty, Ian D. Wood, and Mihael Arcan. First Insights on a Passive Major Depressive\nDisorder Prediction System with Incorporated Conversational Chatbot. In Irish Conference\non Artificial Intelligence and Cognitive Science, Dublin, Ireland, 2018.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran,\nand Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics.\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3\nHerd of Models. arXiv preprint arXiv:2407.21783, 2024.\n[12] Isaac R. Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, and Matteo\nMalgaroli. The capability of large language models to measure psychiatric functioning. arXiv\npreprint arXiv:2308.01834, 2023.\n[13] Jonathan Gratch, Ron Artstein, Gale Lucas, Giota Stratou, Stefan Scherer, Angela Nazarian,\nRachel Wood, Jill Boberg, David DeVault, Stacy Marsella, David Traum, Skip Rizzo, and\n20\n"}, {"page": 21, "text": "Louis-Philippe Morency. The distress analysis interview corpus of human and computer inter-\nviews. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Mae-\ngaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceed-\nings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),\npages 3123–3128, Reykjavik, Iceland, May 2014. European Language Resources Association\n(ELRA).\n[14] Thomas H. Holmes and Richard H. Rahe. The social readjustment rating scale. Journal of\nPsychosomatic Research, 11(2):213–218, 1967.\n[15] Zifan Jiang, Salman Seyedi, Emily Griner, Ahmed Abbasi, Ali Bahrami Rad, Hyeokhyen\nKwon, Robert O. Cotes, and Gari D. Clifford. Multimodal mental health assessment with\nremote interviews using facial, vocal, linguistic, and cardiovascular patterns. medRxiv, 2023.\n[16] Robert Johansson, Per Carlbring, ˚Asa Heedman, Bj¨orn Paxling, and Gerhard Andersson.\nDepression, anxiety and their comorbidity in the swedish general population: point prevalence\nand the effect on health-related quality of life. PeerJ, 1:e98, 2013.\n[17] Marcos Lacasa, Ferran Prados, Jos´e Alegre, and Jordi Casas-Roma. A synthetic data gen-\neration system for myalgic encephalomyelitis/chronic fatigue syndrome questionnaires. SCI-\nENTIFIC REPORTS, 13(1), December 2023. Publisher Copyright: © 2023, Springer Nature\nLimited.\n[18] Zhuang Liu, Wayne Lin, Ya Shi, and Jun Zhao. A robustly optimized bert pre-training ap-\nproach with post-training. In Chinese Computational Linguistics: 20th China National Con-\nference, CCL 2021, Hohhot, China, August 13–15, 2021, Proceedings, page 471–484, Berlin,\nHeidelberg, 2021. Springer-Verlag.\n[19] Isabelle Lorge, Dan W. Joyce, Niall Taylor, Alejo Nevado-Holgado, Andrea Cipriani, and An-\ndrey Kormilitzin. Detecting the clinical features of difficult-to-treat depression using synthetic\ndata from large language models. arXiv preprint arXiv:2402.07645, 2024.\n[20] John J McGrath, Ali Al-Hamzawi, Jordi Alonso, Yasmin Altwaijri, Laura H Andrade, Eve-\nlyn J Bromet, Ronny Bruffaerts, Jos´e Miguel Caldas de Almeida, Stephanie Chardoul, Wai Tat\nChiu, Louisa Degenhardt, Olga V Demler, Finola Ferry, Oye Gureje, Josep Maria Haro, Elie G\nKaram, Georges Karam, Salma M Khaled, Viviane Kovess-Masfety, Marta Magno, Maria Elena\nMedina-Mora, Jacek Moskalewicz, Fernando Navarro-Mateu, Daisuke Nishi, Oleguer Plana-\nRipoll, Jos´e Posada-Villa, Charlene Rapsey, Nancy A Sampson, Juan Carlos Stagnaro, Dan J\nStein, Margreet ten Have, Yolanda Torres, Cristian Vladescu, Peter W Woodruff, Zahari\nZarkov, Ronald C Kessler, Sergio Aguilar-Gaxiola, Ali Al-Hamzawi, Jordi Alonso, Yasmin A.\nAltwaijri, Laura Helena Andrade, Lukoye Atwoli, Corina Benjet, Evelyn J. Bromet, Ronny\nBruffaerts, Brendan Bunting, Jos´e Miguel Caldas de Almeida, Gra¸ca Cardoso, Stephanie\nChardoul, Alfredo H. C´ıa, Louisa Degenhardt, Giovanni De Girolamo, Oye Gureje, Josep Maria\nHaro, Meredith G. Harris, Hristo Hinkov, Chi yi Hu, Peter De Jonge, Aimee N. Karam,\nElie G. Karam, Georges Karam, Alan E. Kazdin, Norito Kawakami, Ronald C. Kessler, An-\ndrzej Kiejna, Viviane Kovess-Masfety, John J. McGrath, Maria Elena Medina-Mora, Jacek\nMoskalewicz, Fernando Navarro-Mateu, Daisuke Nishi, Marina Piazza, Jos´e Posada-Villa,\nKate M. Scott, Juan Carlos Stagnaro, Dan J. Stein, Margreet Ten Have, Yolanda Torres,\nMaria Carmen Viana, Daniel V. Vigo, Cristian Vladescu, David R. Williams, Peter Woodruff,\nBogdan Wojtyniak, Miguel Xavier, and Alan M. Zaslavsky. Age of onset and cumulative risk\n21\n"}, {"page": 22, "text": "of mental disorders: a cross-national analysis of population surveys from 29 countries. The\nLancet Psychiatry, 10(9):668–681, 2023.\n[21] Kirill Milintsevich, Kairit Sirts, and Ga¨el Dias. Towards automatic text-based estimation of\ndepression through symptom prediction. Brain Informatics, 10(1):4, 2023.\n[22] Arturo Montejo-R´aez, M. Dolores Molina-Gonz´alez, Salud Mar´ıa Jim´enez-Zafra, Miguel ´Angel\nGarc´ıa-Cumbreras, and Luis Joaqu´ın Garc´ıa-L´opez. A survey on detecting mental disorders\nwith natural language processing: Literature review, trends and challenges. Computer Science\nReview, 53:100654, 2024.\n[23] Shinka Mori, Oana Ignat, Andrew Lee, and Rada Mihalcea. Towards algorithmic fidelity: Men-\ntal health representation across demographics in synthetic vs. human-generated data. arXiv\npreprint arXiv:2403.16909, 2024.\n[24] Hajra Murtaza, Musharif Ahmed, Naurin Farooq Khan, Ghulam Murtaza, Saad Zafar, and\nAmbreen Bano. Synthetic data generation: State of the art in health care domain. Comput.\nSci. Rev., 48(C), may 2023.\n[25] A. Periˇsa, M. Arcan, D.P. Niland, F. Delahunty, D. Polˇsek, A. Savi´c, P. Breˇci´c, and J. Vuko-\njevi´c. Prediction of clinical tools scores using transformer-based models in patients with first-\nepisode psychosis. Neuroscience Applied, 3:105182, 2024. Abstracts of the 37th ECNP Congress\n2024.\n[26] Ahmad Rashid, Vasileios Lioutas, and Mehdi Rezagholizadeh. MATE-KD: Masked adversar-\nial TExt, a companion to knowledge distillation.\nIn Chengqing Zong, Fei Xia, Wenjie Li,\nand Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1062–1071, Online, August 2021. Association for\nComputational Linguistics.\n[27] Mabrouka Salmi, Dalia Atif, Diego Oliva, Ajith Abraham, and Sebastian Ventura. Handling\nimbalanced medical datasets: review of a decade of research. Artificial Intelligence Review,\n57(10):273, 2024.\n[28] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2020.\n[29] Toni Sawma, Souheil Hallit, Chloe Joy Younis, Jad Jaber, and Myriam El Khoury-Malhame.\nCultural stigma, psychological distress and help-seeking: Moderating role of self-esteem and\nself-stigma. medRxiv, 2024.\n[30] Elizabeth C Stade, Shannon Wiltsey Stirman, Lyle H Ungar, Cody L Boland, H Andrew\nSchwartz, David B Yaden, Jo˜ao Sedoc, Robert J DeRubeis, Robb Willer, and Johannes C\nEichstaedt. Large language models could change the future of behavioral healthcare: a proposal\nfor responsible development and evaluation. NPJ Mental Health Research, 3(1):12, 2024.\n[31] Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying Chen, Yifan Yang,\nQingyu Chen, Won Kim, Donald C Comeau, Rezarta Islamaj, Aadit Kapoor, Xin Gao, and\nZhiyong Lu. Opportunities and challenges for chatgpt and large language models in biomedicine\nand health. Briefings in Bioinformatics, 25(1):bbad493, 01 2024.\n22\n"}, {"page": 23, "text": "[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in\nNeural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n[33] David W Vinson, Mihael Arcan, David-Paul Niland, and Fionn Delahunty. Towards sustainable\nworkplace mental health: A novel approach to early intervention and support. arXiv preprint\narXiv:2402.01592, 2024.\n[34] Alex X. Wang, Stefanka S. Chukova, Colin R. Simpson, and Binh P. Nguyen. Data-centric\nai to improve early detection of mental illness. In 2023 IEEE Statistical Signal Processing\nWorkshop (SSP), pages 369–373, 2023.\n[35] Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh\nGhassemi, Anind K. Dey, and Dakuo Wang. Mental-llm: Leveraging large language models\nfor mental health prediction via online text data. arXiv preprint arXiv:2307.14385, 2023.\n[36] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou.\nTowards interpretable mental health analysis with large language models.\narXiv preprint\narXiv:2304.03347, 2023.\n[37] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Sophia Ananiadou, and Jimin Huang.\nMentallama: Interpretable mental health analysis on social media with large language models.\narXiv preprint arXiv:2309.13567, 2023.\n[38] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Proceedings\nof the 33rd International Conference on Neural Information Processing Systems, Red Hook,\nNY, USA, 2019. Curran Associates Inc.\n[39] Mingze Yuan, Peng Bao, Jiajia Yuan, Yunhao Shen, Zifan Chen, Yi Xie, Jie Zhao, Quanzheng\nLi, Yang Chen, Li Zhang, Lin Shen, and Bin Dong.\nLarge language models illuminate a\nprogressive pathway to artificial intelligent healthcare assistant. Medicine Plus, 1(2):100030,\n2024.\n23\n"}]}