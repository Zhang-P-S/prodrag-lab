{"doc_id": "arxiv:2512.09867", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.09867.pdf", "meta": {"doc_id": "arxiv:2512.09867", "source": "arxiv", "arxiv_id": "2512.09867", "title": "Hierarchy-Aware Multimodal Unlearning for Medical AI", "authors": ["Fengli Wu", "Vaidehi Patil", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "published": "2025-12-10T17:55:06Z", "updated": "2026-01-23T02:43:59Z", "summary": "Pretrained Multimodal Large Language Models (MLLMs) are increasingly used in sensitive domains such as medical AI, where privacy regulations like HIPAA and GDPR require specific removal of individuals' or institutions' data. This motivates machine unlearning, which aims to remove the influence of target data from a trained model. However, existing unlearning benchmarks fail to reflect the hierarchical and multimodal structure of real-world medical data, limiting their ability to properly evaluate unlearning in practice. Therefore, we introduce MedForget, a hierarchy-aware multimodal unlearning benchmark that models hospital data as a nested structure, enabling fine-grained evaluation of multimodal unlearning across retain and forget splits. Experiments with current unlearning methods show that existing approaches struggle to achieve effective hierarchy-aware forgetting without degrading downstream medical utility. To address this limitation, we propose Cross-modal Hierarchy-Informed Projection for unlearning (CHIP), a training-free, hierarchy-aware multimodal unlearning method that deletes information by selectively removing target-specific weight subspaces while preserving sibling-shared information. Experiments show that CHIP achieves the highest forget-retain performance gap across all hierarchy levels while maintaining competitive downstream utility compared to existing methods. Overall, MedForget provides a practical, HIPAA-aligned benchmark for evaluating structured multimodal unlearning for medical data, and CHIP offers an effective and general solution for hierarchy-aware forgetting that balances deletion with utility.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.09867v3", "url_pdf": "https://arxiv.org/pdf/2512.09867.pdf", "meta_path": "data/raw/arxiv/meta/2512.09867.json", "sha256": "7a7c5ffbe1462760b2c4b9d6fbe36a9539787f7775d8b285a8586d7b6d94f41f", "status": "ok", "fetched_at": "2026-02-18T02:24:30.052276+00:00"}, "pages": [{"page": 1, "text": "Hierarchy-Aware Multimodal Unlearning for Medical AI\nFengli Wu1,*\nVaidehi Patil1,∗\nJaehong Yoon2\nYue Zhang1\nMohit Bansal1\n1UNC Chapel Hill\n2Nanyang Technological University\nAbstract\nPretrained Multimodal Large Language Mod-\nels (MLLMs) are increasingly used in sensi-\ntive domains such as medical AI, where pri-\nvacy regulations like HIPAA and GDPR re-\nquire specific removal of individuals’ or institu-\ntions’ data. This motivates machine unlearning,\nwhich aims to remove the influence of target\ndata from a trained model. However, existing\nunlearning benchmarks fail to reflect the hierar-\nchical and multimodal structure of real-world\nmedical data, limiting their ability to properly\nevaluate unlearning in practice. Therefore, we\nintroduce MedForget, a hierarchy-aware multi-\nmodal unlearning benchmark that models hos-\npital data as a nested structure, enabling fine-\ngrained evaluation of multimodal unlearning\nacross retain and forget splits. Experiments\nwith current unlearning methods show that ex-\nisting approaches struggle to achieve effective\nhierarchy-aware forgetting without degrading\ndownstream medical utility. To address this\nlimitation, we propose Cross-modal Hierarchy-\nInformed Projection for unlearning (CHIP), a\ntraining-free, hierarchy-aware multimodal un-\nlearning method that deletes information by\nselectively removing target-specific weight sub-\nspaces while preserving sibling-shared informa-\ntion. Experiments show that CHIP achieves the\nhighest forget-retain performance gap across\nall hierarchy levels while maintaining compet-\nitive downstream utility compared to existing\nmethods. Overall, MedForget provides a practi-\ncal, HIPAA-aligned benchmark for evaluating\nstructured multimodal unlearning for medical\ndata, and CHIP offers an effective and general\nsolution for hierarchy-aware forgetting that bal-\nances deletion with utility1.\n1\nIntroduction\nModern healthcare increasingly relies on multi-\nmodal large language models (MLLMs) that com-\n*Equal contribution.\n1Our data and code are available at: https://github.\ncom/fengli-wu/MedForget\nbine medical images and text to support diagnosis,\nreport generation, and clinical decision-making (Li\net al., 2023; Moor et al., 2023; Yu et al., 2025).\nTraining such models requires aggregating large\nvolumes of sensitive patient data, which raises pri-\nvacy, compliance, and bias concerns. Machine\nunlearning, which aims to remove the influence\nof specific data from a trained model without re-\ntraining, has therefore become essential for han-\ndling consent withdrawal, data corrections, and\nregulatory requirements. Despite recent progress\nin medical unlearning (Nasirigerdeh et al., 2024;\nDeng et al., 2024; Hardan et al., 2025), most exist-\ning multimodal unlearning benchmarks (shown in\nFigure 1(a)) adopt a flat data assumption, treating\nexamples as independent instances.\nHowever, real-world medical data is inherently\nhierarchical and multimodal: records are organized\ninto nested structures (see Figure 1(b)), and each\ndatapoint couples images, text, and metadata. Ef-\nfective unlearning must therefore respect hierarchy,\nremoving a target node without harming siblings,\nwhile also coordinating deletion across modalities.\nFor instance, forgetting a specific clinical record\n(e.g., one study under a patient) should remove all\ninformation associated with that node without af-\nfecting sibling records from the same patient or\ninstitution. In addition, because each record is mul-\ntimodal, forgetting it must jointly remove the corre-\nlation between the medical image and its associated\nreport, while preserving diagnostic performance on\nunrelated studies (e.g., other patients or images).\nTo bridge this gap, we introduce MEDFORGET,\na Hierarchical Multimodal Medical Unlearning\nBenchmark, which represents the first benchmark\ndesigned to evaluate hierarchy-aware unlearning in\nMLLMs. In contrast to prior benchmarks that treat\nsamples as flat and independent, MEDFORGET\nexplicitly captures the nested multimodal struc-\nture of real-world clinical data (Institution ⊃Pa-\ntient ⊃Study ⊃Section). The benchmark com-\n1\narXiv:2512.09867v3  [cs.CV]  23 Jan 2026\n"}, {"page": 2, "text": "October 5.                                   Target forgotten\n        What does the record say for this chest X-ray?\nPneumonia detected.                       Child leaked\nOctober 5.                                                Target forgotten\n        What does the record say for this chest X-ray?\nLungs are clear.                                          Child forgotten\n        What is the study date for this chest X-ray?\nCurrent Unlearning\nMedForget (Ours)\nFlat structure\nHierarchical structure\n        What is the study date for this chest X-ray?\nUnlearning Request:\n“Forget John Doe's\nchest X-ray study\nfrom January 15        ”\n        What is the study date for this chest X-ray?\nMarch 20.                                   Sibling damaged\n        What is the study date for this chest X-ray?\nFebruary 21.                                           Sibling preserved\nStudy Level\nJan 15 Study \nFeb 21 Study\nSection Level\n Lab\nMedication Record\nE.g., Pneumonia detected\nInstitution Level\nABC Hospital\nPatient Level\nJohn Doe\nGender: Male\nBirth date: 07/09/1993\nUnlearning \ntarget\nDefinitions\n(a)\n(b)\nFigure 1: Flat versus hierarchical multimodal unlearning. Images with red borders belong to the study specified in the\nunlearning request. Flat unlearning (a) may forget the target but risk leaking child-level content or damaging sibling\nstudies. MEDFORGET hierarchical unlearning (b) aims to remove target-specific information while preserving child\nconfidentiality and sibling integrity. The right panel shows the four-level hierarchy in our benchmark.\nprises 3,840 multimodal image–question–answer\ninstances across three task types: generation, cloze,\nand classification. Each hierarchical level intro-\nduces distinct challenges, thereby enabling system-\natic evaluation across different granularities.\nUsing MEDFORGET, we systematically eval-\nuate representative multimodal unlearning meth-\nods and observe that existing approaches strug-\ngle to achieve effective hierarchy-aware forget-\nting without degrading medical utility.\nMoti-\nvated by these observations, we propose Cross-\nmodal Hierarchy-Informed Projection for unlearn-\ning (CHIP), a training-free method that exploits the\nhierarchical structure of the data to isolate and sup-\npress target-specific weight subspaces across both\nthe language backbone and the vision–language\nfusion layers in an MLLM while preserving se-\nmantically adjacent (“sibling”) information. Un-\nlike prior approaches that model samples indepen-\ndently, CHIP treats each unlearning request as the\nremoval of a node from a semantic hierarchy, and\ncomputes removal directions that are differential to\nits siblings. This enables targeted forgetting while\nminimizing collateral damage by preventing loss\nof “sibling” knowledge.\nWe evaluate hierarchy-aware multimodal un-\nlearning across four levels of medical structure\nand across three tasks in MEDFORGET: genera-\ntion, cloze completion and classification evaluated\nusing generation quality, cloze reasoning, and clas-\nsification accuracy respectively. Results reveal a\nconsistent deletion–utility trade-off among existing\nmethods: training-based approaches preserve retain\nand general medical performance but leave substan-\ntial residual memorization on the forget set, while\naggressive training-free methods achieve stronger\ndeletion at the cost of significant utility degrada-\ntion. In contrast, CHIP consistently achieves the\nlargest forget–retain performance gap across hier-\narchies, yielding the lowest forget-set scores while\nmaintaining competitive retain performance. Even\nwhen a specific item is unlearned, the remaining\ncorrelated context can enable implicit reconstruc-\ntion of the deleted information. Flat benchmarks\nfail to expose such leakage pathways and therefore\noverestimate unlearning effectiveness, highlighting\nthe need for structure-aware multimodal unlearning\nevaluation frameworks and methods, particularly in\nregulated domains such as healthcare under HIPAA\n(Act, 1996) and GDPR (Regulation, 2016). Our\nresults show that CHIP is substantially more robust\nto such hierarchical reconstruction attacks, exhibit-\ning lower leakage even when adversaries exploit\nrich contextual identifiers. Across tasks, CHIP also\ndemonstrates more uniform forgetting under com-\nparable generation quality constraints, particularly\n2\n"}, {"page": 3, "text": "for cloze and classification tasks. Together, these re-\nsults show that hierarchy-aware unlearning is both\nnecessary and challenging in medical MLLMs, and\nthat CHIP provides a more effective and robust bal-\nance between privacy protection and downstream\nutility than existing approaches.\n2\nRelated Work\nMultimodal machine unlearning benchmarks.\nMachine unlearning aims to remove the influ-\nence of specific data subsets from trained mod-\nels without retraining from scratch. Existing un-\nlearning benchmarks primarily focus on unimodal\ndata (Thudi et al., 2022; Patil et al., 2024a, 2025),\nleaving multimodal unlearning largely unexplored.\nText-based datasets such as TOFU (Maini et al.,\n2024) and MUSE (Shi et al., 2025) evaluate selec-\ntive forgetting in LLMs but do not capture cross-\nmodal dependencies. There are works (Choi et al.,\n2024; Dahal and Xiong, 2025) showing that ex-\nisting LLM unlearning methods fail to reliably re-\nmove structured, multi-hop knowledge, whether\nthrough indirect reasoning chains or knowledge\ngraph relations, highlighting that flat unlearning\nis insufficient. A few recent efforts explore mul-\ntimodal unlearning (Liu et al., 2025a; Patil et al.,\n2024b), but they primarily evaluate flat unlearning\nwithout modeling the hierarchical relationships be-\ntween data sources or tasks, which fail to reflect\nhow deletions naturally arise in clinical workflows.\nPrivacy\nin\nthe\nmedical\ndomain.\nRegula-\ntions (Act, 1996; Regulation, 2016) mandate strict\ncontrol over identifiable health information and\nsupport the \"right to be forgotten,\" motivating tech-\nniques that can selectively remove private data\nfrom trained models. Although large-scale datasets\nlike MIMIC-CXR (Johnson et al., 2019) have cat-\nalyzed progress in multimodal learning for radiol-\nogy (Hartsock and Rasool, 2024; Li et al., 2023),\nthey also expose risks of patient re-identification\nand data leakage through inference or inversion\nattacks (Shokri et al., 2017; Jagielski et al., 2020).\nPrior defenses such as anonymization, differential\nprivacy, and federated learning (McMahan et al.,\n2017) mitigate but do not eliminate these risks. Re-\ncent research in machine unlearning (Nguyen et al.,\n2025; Jang et al., 2023; Zhang et al., 2023b; Trippa\net al., 2024; Liu et al., 2025a) offers a more targeted\nsolution by enabling models to forget specific data\nwithout retraining. Existing datasets overlook the\nmultimodal, hierarchical structure of medical data,\nwhere forgetting at one level induces cascading\neffects. We address this with a hierarchy-aware\nunlearning framework.\n3\nBenchmark: MEDFORGET\nHere, we describe the hierarchical structure and\nunlearning design in Section 3.1. Section 3.2 then\nwe talk about the dataset construction, including\ndata collection and task design. In Section 3.3, we\nexplain the dataset partitioning strategy. Finally,\nwe present key dataset statistics in Section 3.4.\n3.1\nHierarchical Design and Unlearning\nFormulation\nHierarchy Structure.\nWe define a clinically\ngrounded hierarchy as: Institution ⊃Patient ⊃\nStudy ⊃Section, which is consistent with standard\nrepresentations used in medical informatics (John-\nson et al., 2019; Irvin et al., 2019; Banerjee et al.,\n2023), where clinical data are routinely organized\nalong institution–patient–study–section hierarchies\nto capture diagnostic context and data provenance.\nSpecifically, an Institution represents a healthcare\nprovider or site encompassing multiple patients.\nEach Patient includes one or more Studies, corre-\nsponding to distinct imaging visits or diagnostic\nepisodes. Each Study comprises multimodal data\n(radiographs paired with textual reports), while\neach Section denotes a semantic component of the\nreport (e.g., Findings, Impression). These entities\nform a nested structure where information flows up-\nward: sections collectively describe a study, studies\nsummarize a patient’s trajectory, and patients are\ngrouped under institutions.\nUnlearning Formulation. Existing unlearning\nmethods typically assume flat data with indepen-\ndent samples. However, clinical data exhibits hier-\narchical dependencies: forgetting a patient requires\nremoving all associated studies and sections, while\nforgetting a single section should preserve sibling\nsections under the same study. We formalize this\nby defining unlearning at each hierarchy level: At\nthe institution level, unlearning should remove all\npatients, studies, and sections associated with tar-\nget institutions. At the patient level, all studies and\nsections belonging to target patients should be for-\ngotten, while study-level unlearning should remove\nall sections within target studies. Finally, section-\nlevel unlearning should target only the specified\nsections, leaving all other hierarchical entities in-\ntact. At each level, non-target entities sharing the\n3\n"}, {"page": 4, "text": "Hierarchy\nExample Question\nExample Answer\nTask Type\nForget Scope\nInstitution\nFrom which institution does this medical\nimaging originate?\nElm Medical Foundation\nHierarchy\nInstitution\nPatient\nWhat patient’s medical record does this im-\nage belong to?\nAndrew Lewis\nHierarchy\nPatient, Institution\nStudy\nWhat is the identifier for this imaging\nstudy?\nstudy_chest_xray_001\nHierarchy\nStudy, Patient, Institu-\ntion\nSection\nWhat is documented in the examination sec-\ntion?\nThe exam included PA and\nlateral chest views.\nGeneration\nSection, Study, Pa-\ntient, Institution\nGiven this chest X-ray image, complete:\nThe examination included [blank] and lat-\neral chest views.\nPA\nCloze\nSection, Study,\nPatient, Institution\nIn the context of this radiograph, what type\nof chest X-ray was performed?\nA) AP\nportable only B) Chest PA and lateral C)\nDecubitus views\nB\nClassification\nSection, Study, Pa-\ntient, Institution\nTable 1: Illustration of the hierarchical structure, task types, and forgetting scopes in MEDFORGET. Each hierarchy\nlevel is associated with representative QA tasks. Forgetting at a given level removes VQA pairs from that level and\nall subordinate levels, while the retain set consists of the remaining instances.\nForget Set\nQuestion: Looking at this\nmedical image, what\ndoes the indication\nsection indicate?\nAnswer: woman s/p rsxn\nof large lung mass //\ninterval CXR\nForget Rephrase Set\nQuestion: Based on this\nmedical image, what\ninformation is contained within\nthe indication section?\nAnswer: woman s/p rsxn of\nlarge lung mass // interval CXR\nRetain Set\nQuestion: In the context\nof this radiograph,\ndescribe the impression\nsection for this patient?\nAnswer: Resolution of\nright multi focal\npneumonia. Unchanged\nleft pleural effusion.\nGeneral Med Set\nQuestion: Can you\ndetermine the type of\nfracture?\nAnswer: Comminuted\ntransverse fracture\nForget Image\nRotated −15°\nForget Image\nRetain Image\nGeneral Med\nImage\nEvaluation Sets\n(a)\n(b)\n(c)\n(d)\nFigure 2: Examples of data subsets in MEDFORGET. Each subset serves a distinct evaluation purpose: the Forget Set\n(a) contains target information to be unlearned, the Forget Rephrase Set (c) tests generalization through paraphrased\nquestions and augmented views, the Retain Set (b) evaluates preservation of medical knowledge that should not\nbe forgotten, and the General Med Set (d) assesses retention of general medical capabilities on an independent\nbenchmark. All examples show medical images paired with questions and ground truth answers tailored to their\nevaluation objectives. Additional data examples are provided in Appendix Figure 9.\nsame parent are termed siblings, which should be\npreserved during unlearning. This sibling-aware\nformulation enables evaluation of both forgetting\ncompleteness and collateral damage. Concrete par-\ntition strategies are detailed in Section 3.3.\n3.2\nData Construction Pipeline\nData Collection. We build MEDFORGET on top\nof MIMIC-CXR (Johnson et al., 2019), a large-\nscale de-identified multimodal medical dataset con-\ntaining paired chest X-rays and radiology reports\nfor over 65,000 patients. Each patient has one or\nmore imaging studies, and each report typically\nincludes four standardized sections, Examination,\nIndication, Findings, and Impression, which form a\nnatural patient–study–section hierarchy. We lever-\nage this inherent structure to first retain only stud-\nies that contain all four standard report sections,\nand then filter patients who have at least four such\nstudies. For each remaining patient, we randomly\nsample four studies (or retain all if exactly four).\nSimilarly, for each selected study, we randomly\nsample four report sections (retaining all if exactly\nfour) for downstream processing. To simulate a\nrealistic multi-institutional environment, we group\nevery eight eligible patients into a single synthetic\ninstitution, producing multiple distinct institutions.\nTask Design. Building upon the hierarchical struc-\nture defined above, we design task types that evalu-\nate how well models preserve or forget information\nacross different semantic levels. While hierarchical\nunlearning determines what to forget, these tasks\ndetermine how forgetting impacts multimodal rea-\nsoning and comprehension. Specifically, for each\nreport section, we define three complementary eval-\nuation tasks, generation, classification and cloze-\nstyle completion (see examples in Table 1), that\n4\n"}, {"page": 5, "text": "Institution Level\nPatient Level\nStudy Level\nSection Level\nFigure 3: Illustration of the forget–retain partition at each hierarchy level, where approximately 25% of entities (red)\nform the forget set and the remainder constitute the retain set. This setup supports controlled multi-level unlearning\nexperiments, capturing how forgetting propagates across hierarchically related data.\ncapture distinct reasoning skills such as factual\nconsistency, contextual inference, and cross-modal\ngrounding. We synthetically generate task prompts\nand responses using the DeepSeek-V3 (DeepSeek-\nAI et al., 2025) model, grounded in authentic ra-\ndiology text–image pairs, to enable efficient and\nsemantically coherent medical text generation.\n3.3\nDataset Partition\nAfter generating the full set of multimodal question-\nanswer pairs, we organize the data into structured\nsubsets to support both fine-tuning and targeted\nunlearning experiments across different hierarchy\nlevels. Below, we first describe the fine-tuning set\nused to obtain vanilla (pre-unlearning) MLLM per-\nformance, followed by the forget–retain partitions\nand evaluation sets. We provide more examples\nfrom the dataset in Figure 2.\nFine-tuning Set.\nWe construct question-answer\npairs that explicitly encode both hierarchical con-\ntext and section-level content. For the Institution,\nPatient, and Study levels, questions test entity iden-\ntification (e.g., “From which institution does this\nmedical image originate?”) to encourage cross-\nlevel understanding. At the Section level, we in-\nclude all three task types.\nForget-Retain Partitions.\nTo enable selective\nunlearning, we construct hierarchical forget–retain\npartitions (Figure 3) by designating 25% of entities\nas forget targets at each level of the hierarchy (as\ndefined in Section 3.1) while the remaining enti-\nties under the same parent form the retain set. The\nForget Set contains direct queries that the model\nshould completely unlearn after the requested for-\ngetting operation (e.g., “Looking at this medical\nimage, what does the indication section indicate?”\npaired with sensitive content such as “woman s/p\nrxsxn of large lung mass // interval CXR”). The\nRetain Set (Figure 2 (b)) consists of examples that\nmust be preserved for the same forgetting request,\ntypically medical knowledge from other patients,\n23.7%\n20.1%\n19.6%\n17.1%\n13.4%\n2.9%\n3.2%\nImpression\nFindings\nTechnique\nIndication\nExamination\nHistory\nOther (11 categories)\nFigure 4: Distribution of section types in MEDFORGET.\nstudies, or sections. A typical retain example asks\nthe model to describe findings on an unrelated chest\nX-ray (e.g., “Resolution of right multifocal pneu-\nmonia. Unchanged left pleural effusion”).\nEvaluation Sets.\nTo assess both forgetting perfor-\nmance and downstream clinical utility, apart from\nthe forget and retain splits used during fine-tuning\nand unlearning, we evaluate models on following\ntwo dedicated held-out sets. This structured eval-\nuation protocol enables fine-grained analysis of\nwhether models truly eliminate memorized infor-\nmation at the specified hierarchy level and retain\ntheir broader diagnostic reasoning abilities.\n• Forget rephrase set uses the same underly-\ning ground-truth answers as the Forget set\nbut introduces paraphrased questions (gener-\nated using DeepSeek-V3) and visual augmen-\ntations (one of four SV-DRR angular varia-\ntions: −30◦, −15◦, +15◦, +30◦). These mod-\nifications test whether the model still recalls\nmemorized information when queried with re-\nworded prompts or slightly altered viewpoints,\npreventing trivial exact-match forgetting eval-\nuation and exposing residual memorization.\n• General med set is an independent medical\nVQA benchmark (PMC-VQA (Zhang et al.,\n2023a)) used to measure the preservation of\noverall clinical utility. The questions and im-\nages are entirely unrelated to any institution,\npatient, study, or section involved in the forget\n5\n"}, {"page": 6, "text": "−\n�’\nUnlearned MLLM\n��= �forget,�−�retain,�\nTop �% → �\n=\n�\n�\n�       ·                  ·          ��\n�\n�siblings\n�target\nVision Tokens\n Language Tokens\nActivations\n�%\n×\n�×|�|\n�×�\n�×�\n�×|�|\n|�|×�\nLLM Layers 22-27\nVL Merger\nQA \nPairs\n�−\n��T\n�\n+\nBasis �\nNeurons �\nOn selected \nneurons\n�1 �2 . . . ��\nHierarchical\nMultimodal\nData\nFine-tuned\nMLLM\n(a)\n(d)\n(c)\n(b)\n×�\n×(1−α)\n×�\nTop �\n(� var)\nΣ\nActivation Collection\nNeuron Selection\nDirection Computation & SVD Aggregation\nProjection\nFigure 5: Illustration of CHIP. The method (a) collects cross-modal activations, (b) identifies target-related neurons,\n(c) constructs hierarchy-aware directions that isolate sibling-differential information, and (d) applies a training-free\nsubspace projection to remove these directions from both language and vision–language weights.\nor retain sets, for example, identifying fracture\ntypes on radiographs from unseen sources.\n3.4\nDataset Statistics\nMEDFORGET contains 8 institutions, 64 patients,\n256 studies, and 1,024 report sections, yielding\n3,840 multimodal VQA pairs. We apply a 25%\nforget ratio at each hierarchy level, resulting in 960\nforget and 2,880 retain VQA pairs. The dataset in-\ncludes 3,072 section-level tasks (generation, cloze,\nclassification) and 768 hierarchical identity tasks\nevenly split across institution, patient, and study\nlevels. At the section level, the dataset reflects ra-\ndiology report composition: Impression (23.7%),\nFindings (20.1%), Technique (19.6%), Indication\n(17.1%), and Examination (13.4%) are most fre-\nquent, while clinically rare sections (e.g., Clinical\nHistory, Notification) occur infrequently (full dis-\ntribution shown in Figure 4). See Section A.8 for\ndetailed dataset statistics.\n4\nMethod: CHIP\nIn this section, we discuss our proposed method\nCHIP (illustrated in Figure 5), a training-free ap-\nproach for hierarchy-aware multimodal unlearning.\nOur method is motivated by a key observation: in\nhierarchical data, sibling nodes (i.e., nodes sharing\na common parent) encode shared information re-\nflecting their common context and node-specific\ninformation is unique to each. Since the shared\ncomponent is encoded across all siblings while\nthe node-specific component is localized to each\nnode, removing only the sibling-differential rep-\nresentations while preserving the sibling-shared\ncomponent enables targeted forgetting with mini-\nmal collateral damage.\nCross-modal Activation Collection.\nIn multi-\nmodal models, node information is encoded across\nboth the language backbone and cross-modal fu-\nsion layers. Accordingly, we extract activations\nfrom language layers and vision–language merger\nlayers, aggregating vision and language token rep-\nresentations separately, then combining them as:\na(l) = α · a(l)\nvision + (1 −α) · a(l)\nlang,\n(1)\nwhere α controls the vision weight. For merger\nlayers, we apply global average pooling over the\nspatial dimension to get the activations. This vision-\nlanguage separation ensures both modalities con-\ntribute appropriately to the computed directions\n(see Figure 5 (a)).\nNeuron Selection.\nNot all neurons contribute\nequally to storing forget-specific information. In-\nspired by Liu et al. (2025b), we identify neurons\nmost relevant to forgetting via importance scores.\nLet I(l)\nforget and I(l)\nretain denote mean absolute activa-\ntions over forget and retain samples at layer l. We\ncompute s(l)\ni\n= I(l)\nforget,i −I(l)\nretain,i and select the\ntop k% neurons with the highest scores as set S(l).\nThese selected neurons form the subspace where\nsurgery will be applied (see Figure 5 (b)).\nDirection Computation and SVD Aggrega-\ntion.\nWe formalize this intuition using sibling-\ndifferential directions. Let G = (V, E) denote a\nhierarchical graph. For a target node ntarget with sib-\nlings sharing the same parent, let µ(l)\nn be the mean\nactivation of samples associated with node n at\nlayer l. We compute the sibling-differential direc-\ntion: d = normalize\n\u0010\nµ(l)\ntarget −µ(l)\nsiblings\n\u0011\n, where\nµ(l)\nsiblings denotes the mean activation over retained\nsibling nodes. By subtracting the sibling mean, the\nshared hierarchical structure is canceled, isolating\ntarget-specific representations. When direct sib-\nlings are unavailable, retained nodes at the same hi-\nerarchy level serve as a proxy. For non-leaf targets\n(e.g., institutions or patients), we further decom-\npose the target into its child nodes and compute\n6\n"}, {"page": 7, "text": "separate directions for each child against retained\nnodes at the child level. This multi-direction de-\ncomposition captures heterogeneous information\ndistributed across different parts of the hierarchy,\nyielding multiple directions per target that we sub-\nsequently aggregate (see Figure 5 (c)).\nWe aggregate multiple directions from all tar-\nget nodes in the forget set across the hierarchy us-\ning singular value decomposition (SVD) to obtain\na low-rank subspace capturing dominant forget-\nspecific variance. Let D(l) ∈Rm×|S(l)| be the ma-\ntrix of m stacked directions for layer l, restricted\nto selected neurons. We compute D(l) = UΣV⊤\nand retain the top r right singular vectors from\nV that cumulatively explain at least τ (e.g., 95%)\nof the total variance, forming the projection basis\nQ(l) ∈R|S(l)|×r.\nWeight Subspace Multimodal Projection.\nWe\nthen update the weights corresponding to the se-\nlected neurons via orthogonal projection.\nMo-\ntivated by Belrose et al. (2023); Kodge et al.\n(2024), we perform the weight update as: W(l)\nS,: ←\n\u0000I −Q(l)(Q(l))⊤\u0001\nW(l)\nS,:, where W(l)\nS,: denotes the\nrows of the weight matrix corresponding to selected\nneurons.\nThis projection removes components\naligned with the forget-specific subspace while pre-\nserving orthogonal information (see Section A.2\nfor detailed derivations). Unlearning restricted to\na single modality risks leaving recoverable traces\nin other modality pathways. CHIP therefore ap-\nplies surgery jointly to upper language and vision-\nlanguage merger layers, ensuring that both uni-\nmodal and cross-modal representations related to\nthe target2 are removed (see Figure 5 (d)). We\nchoose layers for this weight projection empirically\nand provide the ablation result in Appendix A.7.\n5\nExperimental Results\n5.1\nExperimental Setup\nEvaluation Metrics.\nWe evaluate unlearning per-\nformance using three complementary metrics that\ncapture both forgetting completeness and retain\nset utility: (1) Generation Score (Gen Score) for\ngeneration tasks, computed as a weighted average\nof 75% factuality score and 25% ROUGE-L (Lin,\n2004). The factuality score is obtained by using\nDeepSeek-V3 (DeepSeek-AI et al., 2025) as LLM-\nas-a-judge (Gu et al., 2024) to rate clinical accuracy\n2We use the terms “forget set” and “target set” interchange-\nably in this paper.\non a 1–10 scale, following prior work (Sun et al.,\n2023; Yu et al., 2024; Zheng et al., 2023); (2) Cloze\nAccuracy (Cloze Acc), measured by exact string\nmatch in cloze-style completion tasks; and (3) Clas-\nsification Accuracy (Class. Acc), computed as the\nproportion of correct multiple-choice predictions\ngiven the question and chest X-ray image. Evalua-\ntion metric details are provided in Appendix A.3.\nCompared Unlearning Methods.\nWe bench-\nmark four representative unlearning methods on\nMEDFORGET:\n(1) Gradient Difference (Grad-\nDiff) (Yao et al., 2024) (2) Negative Preference\nOptimization (NPO) (Zhang et al., 2024) (3)\nModality-Aware Neuron Unlearning (MANU) (Liu\net al., 2025b), and (4) KL Minimization (KL\nMin) (Nguyen et al., 2020). We provide more de-\ntails in Appendix A.1.\nUnlearning Pipeline.\nWe evaluate unlearning\nmethods using a multi-stage pipeline on MEDFOR-\nGET. First, we finetune Lingshu (Xu et al., 2025)\n(7B), a Qwen2.5-VL–based medical MLLM, on\nthe full hierarchical training set to obtain a vanilla\nmodel. For each hierarchy level, we construct cor-\nresponding forget and retain splits (Section 3.3),\nand apply each unlearning method independently\nto the vanilla checkpoint, producing four unlearned\nvariants per method (one per hierarchy level). For-\ngetting at higher hierarchy levels subsumes all sub-\nordinate nodes. We provide more details about the\nunlearning pipeline in Appendix A.4.\n5.2\nMain Results\nComparison of Deletion-Utility Trade-off with\nBaselines.\nAs shown in Table 2 and Figure 7, ex-\nisting unlearning methods struggle to balance effec-\ntive forgetting with utility preservation. Training-\nbased methods such as KL Min and NPO maintain\nstrong retain performance (e.g., retain Gen Scores\nof 51.5–50.8 at the Institution level) but leave sub-\nstantial residual memorization on the forget set (for-\nget Gen Scores > 40). MANU achieves stronger\ndeletion (e.g., forget Gen Score 41.5 at the Insti-\ntution level) but incurs larger drops in retain and\ngeneral medical performance. In contrast, CHIP\nconsistently achieves the largest Forget-Retain per-\nformance gap across hierarchies. For example, at\nthe Institution level, it attains the lowest forget Gen\nScore (39.4) while preserving a retain Gen Score of\n52.0, yielding the highest F/R Diff (12.6 vs. 10.8\nfor KL Min). These results show CHIP’s strength\n7\n"}, {"page": 8, "text": "Hierarchy Method\nForget Set ↓\nRetain Set ↑\nF/R Diff ↑\nEvaluation Sets\nForget Rephrase Set ↓\nGeneral Med Set ↑\nGen Score\nClass. Acc\nCloze Acc Gen Score\nClass. Acc\nCloze Acc\nGen Score\nClass. Acc\nCloze Acc Gen Score\nClass. Acc\nCloze Acc\nSection\nVanilla\n99.84\n100.00\n100.00\n99.23\n100.00\n100.00\n-0.61\n58.47\n98.71\n87.18\n68.52\n95.19\n82.31\nMANU\n44.81\n99.39\n86.12\n47.81\n99.97\n86.07\n3.00\n28.13\n94.94\n77.21\n54.07\n85.89\n65.87\nGrad. Diff.\n45.24\n96.90\n97.85\n48.38\n97.62\n99.64\n3.14\n27.61\n96.95\n75.11\n57.27\n85.34\n72.41\nKL Min.\n43.53\n98.52\n97.68\n53.52\n98.86\n99.81\n9.99\n27.00\n96.54\n74.97\n57.25\n83.55\n73.00\nNPO\n44.62\n96.88\n98.65\n50.66\n96.72\n99.68\n6.04\n34.45\n96.78\n77.42\n59.17\n75.36\n74.85\nCHIP\n40.64\n93.34\n75.72\n47.20\n97.11\n78.57\n6.56\n21.94\n95.06\n69.63\n55.19\n88.17\n67.83\nStudy\nVanilla\n99.47\n100.00\n100.00\n99.27\n100.00\n100.00\n-0.20\n58.38\n98.65\n87.21\n68.47\n95.21\n82.29\nMANU\n43.75\n98.14\n82.93\n47.45\n98.74\n83.12\n3.70\n27.64\n94.84\n73.02\n56.28\n85.54\n64.96\nGrad. Diff.\n44.92\n95.34\n97.23\n49.83\n95.58\n99.61\n4.91\n26.98\n96.76\n73.34\n55.90\n85.00\n71.82\nKL Min.\n42.86\n98.49\n97.66\n45.70\n99.16\n99.47\n2.84\n24.44\n96.34\n74.92\n55.70\n82.19\n72.65\nNPO\n44.20\n96.62\n98.47\n50.08\n98.89\n99.94\n5.88\n29.76\n96.58\n75.86\n56.98\n74.77\n74.41\nCHIP\n40.28\n92.45\n74.87\n50.14\n96.31\n78.77\n9.86\n25.23\n95.05\n69.18\n55.29\n87.79\n65.75\nPatient\nVanilla\n98.83\n100.00\n100.00\n99.65\n100.00\n100.00\n0.82\n58.61\n98.73\n87.32\n68.63\n95.28\n82.41\nMANU\n39.98\n96.25\n81.57\n44.84\n96.55\n80.44\n4.86\n26.31\n94.45\n71.85\n57.08\n85.07\n63.84\nGrad. Diff.\n39.52\n95.22\n95.17\n46.45\n95.54\n97.14\n6.93\n26.11\n96.34\n73.21\n56.05\n81.27\n71.59\nKL Min.\n43.19\n94.41\n97.04\n51.49\n95.03\n99.00\n8.30\n22.74\n94.26\n74.72\n56.17\n79.11\n71.68\nNPO\n44.81\n96.40\n98.21\n50.72\n98.81\n99.96\n5.91\n25.83\n96.35\n74.50\n55.85\n71.80\n74.81\nCHIP\n39.01\n92.36\n69.58\n48.12\n97.46\n75.55\n9.11\n23.83\n94.83\n69.11\n53.66\n87.37\n64.88\nInstitution\nVanilla\n99.25\n100.00\n100.00\n98.97\n100.00\n100.00\n-0.28\n58.42\n98.68\n87.15\n68.55\n95.17\n82.25\nMANU\n41.48\n96.18\n79.38\n47.24\n96.82\n80.46\n5.76\n22.89\n94.34\n69.98\n57.25\n74.66\n62.19\nGrad. Diff.\n40.47\n94.67\n94.34\n50.27\n94.94\n96.38\n9.80\n22.22\n96.14\n72.79\n53.97\n78.66\n71.28\nKL Min.\n40.68\n98.10\n96.81\n51.49\n98.56\n98.92\n10.81\n24.25\n96.06\n74.68\n55.92\n75.72\n71.17\nNPO\n42.80\n99.38\n98.18\n50.79\n99.77\n99.92\n7.99\n25.27\n96.03\n73.02\n56.09\n71.17\n74.14\nCHIP\n39.36\n91.33\n67.03\n51.96\n97.06\n73.16\n12.60\n18.27\n94.71\n68.09\n53.58\n87.20\n63.37\nTable 2: Performance across hierarchical unlearning splits and tasks. All values are percentages (%). F/R Diff\ndenotes the difference between Retain Set and Forget Set Gen Scores, measuring the unlearning-utility gap. ↓\nindicates lower is better for unlearning (forget and its rephrase sets), ↑indicates higher is better for utility preservation\n(retain, F/R Diff, and general sets). Best unlearning and utility performance are shown in bold and underlined.\nSec.\nStu.\nPat.\nInst.\n32\n35\n38\n41\n44\n47\n50\nGen Score\nInstitution-Level Context\nSec.\nStu.\nPat.\nInst.\n32\n35\n38\n41\n44\n47\n50\nPatient-Level Context\nSec.\nStu.\nPat.\nInst.\n32\n35\n38\n41\n44\n47\n50\nStudy-Level Context\nMANU\nGrad. Diff.\nKL Min.\nNPO\nCHIP\nFigure 6: Section-level leakage across unlearning granularities with hierarchical prompts. Lower Gen Scores\nindicate better forgetting of Section-level information, while higher scores indicate more leakage.\nin achieving a superior deletion–utility balance un-\nder hierarchy-aware multimodal unlearning.\nImpact of Hierarchical Granularity.\nOur re-\nsults in Table 2 show that unlearning effectiveness\nvaries systematically with hierarchical granularity.\nCoarse-grained deletion enables stronger forget-\nting but induces larger utility loss: at the Insti-\ntution level, forget Gen Scores drop to 39.4-41.5\nacross methods, while retain Gen Scores decline\nto 50.8-52.0. At finer granularity, forgetting be-\ncomes harder, but utility is better preserved: at the\nSection level, retain Gen Scores remain high (53.3–\n54.0), while forget Gen Scores rise to 40.6–42.4,\nindicating residual memorization. Across all hier-\narchies, CHIP achieves favorable trade-offs, attain-\ning the lowest or near-lowest forget Gen Scores\nat coarse levels (e.g., 39.4 at Institution) while\nmaintaining competitive retain performance. It\nalso preserves utility at the Section level (retain\n54.0) with only modest increases in forget scores\n(40.6). The results highlight a hierarchy-dependent\nprivacy-utility trade-off and show CHIP’s robust-\nness across deletion granularities.\nPerformance Across Tasks.\nTable 2 shows that\nachieving consistent forgetting across heteroge-\nneous tasks is challenging. Classification and cloze\nreasoning are more tightly coupled to core repre-\nsentations than free-form generation, making them\nharder to unlearn without degrading retained gener-\nation quality. We therefore tune all methods to keep\nretain-set generation scores above 0.4; under this\nconstraint, most baselines still exhibit high forget-\nset classification accuracy, indicating incomplete\nforgetting. In contrast, at a comparable generation\nscore (≈0.4), CHIP achieves substantially lower\nforget-set classification and cloze accuracy across\nhierarchies while maintaining competitive retain\nand general medical performance. This demon-\n8\n"}, {"page": 9, "text": "strates CHIP’s more task-robust unlearning, with\nimproved suppression of task-specific memoriza-\ntion beyond surface-level generative degradation.\nResistance to Hierarchical Reconstruction At-\ntacks.\nWe evaluate whether unlearned models\nremain vulnerable to reconstruction when an adver-\nsary exploits hierarchical context. We simulate a\ncumulative reconstruction attack that progressively\nadds identifiers (Institution, Patient, Study) to the\nprompt while attempting to regenerate forgotten\nSection-level content. Leakage is measured us-\ning the Gen Score, where higher values indicate\ngreater recovery risk. As shown in Figure 6, mod-\nels unlearned only at fine granularity (Section or\nStudy) remain highly vulnerable, often regenerat-\ning forgotten content with high fidelity (Gen Score\nbetween 38-49) under strong contextual prompts.\nIn contrast, higher-granularity unlearning (Patient\nor Institution) substantially improves robustness,\nreducing Gen Score to 33-42. Across all methods\nand hierarchical levels, CHIP shows lower leakage\ncompared to the baselines, demonstrating robust-\nness to hierarchical reconstruction attacks.\n6\nConclusion\nThis work exposes the limitations of flat unlearning\nin multimodal medical models and motivates the\nneed for hierarchy-aware unlearning. We introduce\nMEDFORGET, the first hierarchical benchmark for\nmedical unlearning, which reveals privacy–utility\ntrade-offs unaddressed by existing methods. We\nfurther propose CHIP, a training-free approach\nthat selectively removes target-specific information\nwhile preserving shared representations, achieving\nstronger forgetting across hierarchies and improved\nrobustness to reconstruction attacks. Together, our\nbenchmark and method advance the deployment of\ntrustworthy medical MLLMs.\nAcknowledgments\nWe would like to thank Elias Stengel-Eskin for his\nfeedback on a draft of the paper. This work was\nsupported by National Institutes of Health (NIH)\nunder other transactions 1OT2OD038045-01, ARO\nAward W911NF2110220, and ONR Grant N00014-\n23-1-2356. The views contained in this article are\nthose of the authors and not of the funding agency.\nReferences\nAct. 1996. Health insurance portability and accountabil-\nity act of 1996. Public law, 104:191.\nImon Banerjee, Kamanasish Bhattacharjee, John L\nBurns, Hari Trivedi, Saptarshi Purkayastha, Laleh\nSeyyed-Kalantari, Bhavik N Patel, Rakesh Shiradkar,\nand Judy Gichoya. 2023. “shortcuts” causing bias in\nradiology artificial intelligence: causes, evaluation,\nand mitigation. Journal of the American College of\nRadiology, 20(9):842–851.\nNora Belrose, David Schneider-Joseph, Shauli Ravfo-\ngel, Ryan Cotterell, Edward Raff, and Stella Bider-\nman. 2023. LEACE: Perfect linear concept erasure\nin closed form. In Advances in Neural Information\nProcessing Systems, volume 36.\nMinseok Choi, ChaeHun Park, Dohyun Lee, and Jaegul\nChoo. 2024.\nBreaking chains:\nUnraveling the\nlinks in multi-hop knowledge unlearning.\nCoRR,\nabs/2410.13274.\nChahana Dahal and Zuobin Xiong. 2025. How well do\nllms unlearn facts?-a knowledge graph perspective.\nIn Women in Machine Learning Workshop@ NeurIPS\n2025.\nDeepSeek-AI, Aixin Liu, Bei Feng, and 1 others.\n2025.\nDeepseek-v3 technical report.\nPreprint,\narXiv:2412.19437.\nZhipeng Deng, Luyang Luo, and Hao Chen. 2024. En-\nable the right to be forgotten with federated client\nunlearning in medical imaging. In International Con-\nference on Medical Image Computing and Computer-\nAssisted Intervention, pages 240–250. Springer.\nAlexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin,\nBoris Mikheev, Denis Bobkov, Aibek Alanov, Oleg\nRogov, Ivan Oseledets, and Elena Tutubalina. 2025.\nClear: Character unlearning in textual and visual\nmodalities. In Findings of the Association for Compu-\ntational Linguistics: ACL 2025, pages 20582–20603.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,\nXuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,\nShengjie Ma, Honghao Liu, and 1 others. 2024. A\nsurvey on llm-as-a-judge. The Innovation.\nShahad Hardan, Darya Taratynova, Abdelmajid Essofi,\nKarthik Nandakumar, and Mohammad Yaqub. 2025.\nForget-mi: Machine unlearning for forgetting mul-\ntimodal information in healthcare settings. In Inter-\nnational Conference on Medical Image Computing\nand Computer-Assisted Intervention, pages 204–213.\nSpringer.\nIryna Hartsock and Ghulam Rasool. 2024.\nVision-\nlanguage models for medical report generation and\nvisual question answering: A review. Frontiers in\nartificial intelligence, 7:1430984.\n9\n"}, {"page": 10, "text": "Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Mark-\nlund, Behzad Haghgoo, Robyn Ball, Katie Shpan-\nskaya, and 1 others. 2019. Chexpert: A large chest\nradiograph dataset with uncertainty labels and expert\ncomparison. In Proceedings of the AAAI conference\non artificial intelligence, volume 33, pages 590–597.\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea.\n2020. Auditing differentially private machine learn-\ning: How private is private sgd? Advances in Neural\nInformation Processing Systems, 33:22205–22216.\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,\nMoontae Lee, Lajanugen Logeswaran, and Minjoon\nSeo. 2023. Knowledge unlearning for mitigating\nprivacy risks in language models. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 14389–14408.\nAlistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-\nwei H. Lehman, Mengling Feng, Marzyeh Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G. Mark. 2019. Mimic-cxr, a de-identified\npublicly available database of chest radiographs with\nfree-text reports. Scientific Data, 6(1):317.\nSangamesh Kodge, Gobinda Saha, and Kaushik Roy.\n2024. Deep unlearning: Fast and efficient gradient-\nfree class forgetting. Transactions on Machine Learn-\ning Research.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-\nmed: Training a large language-and-vision assistant\nfor biomedicine in one day. Advances in Neural In-\nformation Processing Systems, 36:28541–28564.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nZheyuan Liu, Guangyao Dou, Mengzhao Jia, Zhaoxuan\nTan, Qingkai Zeng, Yongle Yuan, and Meng Jiang.\n2025a. Protecting privacy in multimodal large lan-\nguage models with mllmu-bench. In Proceedings of\nthe 2025 Conference of the Nations of the Americas\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), pages 4105–4135.\nZheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chun-\nhui Zhang, Zhaoxuan Tan, and Meng Jiang. 2025b.\nModality-aware neuron pruning for unlearning in\nmultimodal large language models. arXiv preprint\narXiv:2502.15910.\nPratyush Maini,\nZhili Feng,\nAvi Schwarzschild,\nZachary Chase Lipton, and J Zico Kolter. 2024.\nTOFU: A task of fictitious unlearning for LLMs. In\nFirst Conference on Language Modeling.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Aguera y Arcas. 2017.\nCommunication-efficient learning of deep networks\nfrom decentralized data. In Artificial intelligence and\nstatistics, pages 1273–1282. PMLR.\nMichael Moor, Qian Huang, Shirley Wu, Michihiro\nYasunaga, Yash Dalmia, Jure Leskovec, Cyril Za-\nkka, Eduardo Pontes Reis, and Pranav Rajpurkar.\n2023. Med-flamingo: a multimodal medical few-shot\nlearner. In Machine Learning for Health (ML4H),\npages 353–367. PMLR.\nReza Nasirigerdeh, Nader Razmi, Julia A Schnabel,\nDaniel Rueckert, and Georgios Kaissis. 2024. Ma-\nchine unlearning for medical imaging. arXiv preprint\narXiv:2407.07539.\nQuoc Phong Nguyen, Bryan Kian Hsiang Low, and\nPatrick Jaillet. 2020. Variational bayesian unlearn-\ning. In Advances in Neural Information Processing\nSystems, volume 33, pages 16025–16036. Curran As-\nsociates, Inc.\nThanh Tam Nguyen, Thanh Trung Huynh, Zhao Ren,\nPhi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin,\nand Quoc Viet Hung Nguyen. 2025. A survey of ma-\nchine unlearning. ACM Transactions on Intelligent\nSystems and Technology, 16(5):1–46.\nVaidehi Patil, Peter Hase, and Mohit Bansal. 2024a.\nCan sensitive information be deleted from LLMs?\nobjectives for defending against extraction attacks.\nIn The Twelfth International Conference on Learning\nRepresentations.\nVaidehi Patil, Elias Stengel-Eskin, and Mohit Bansal.\n2025.\nUpcore:\nUtility-preserving coreset se-\nlection for balanced unlearning.\narXiv preprint\narXiv:2502.15082.\nVaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tian-\nlong Chen, and Mohit Bansal. 2024b. Unlearning\nsensitive information in multimodal LLMs: Bench-\nmark and attack-defense evaluation. Transactions on\nMachine Learning Research.\nProtection Regulation. 2016. Regulation (eu) 2016/679\nof the european parliament and of the council. Regu-\nlation (eu), 679(2016):10–13.\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Mal-\nladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke\nZettlemoyer, Noah A. Smith, and Chiyuan Zhang.\n2025. MUSE: Machine unlearning six-way evalua-\ntion for language models. In The Thirteenth Interna-\ntional Conference on Learning Representations.\nReza Shokri, Marco Stronati, Congzheng Song, and Vi-\ntaly Shmatikov. 2017. Membership inference attacks\nagainst machine learning models. In 2017 IEEE sym-\nposium on security and privacy (SP), pages 3–18.\nIEEE.\n10\n"}, {"page": 11, "text": "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,\nChunyuan Li, Yikang Shen, Chuang Gan, Liang-\nYan Gui, Yu-Xiong Wang, Yiming Yang, and 1\nothers. 2023.\nAligning large multimodal mod-\nels with factually augmented rlhf. arXiv preprint\narXiv:2309.14525.\nAnvith Thudi, Gabriel Deza, Varun Chandrasekaran,\nand Nicolas Papernot. 2022. Unrolling sgd: Under-\nstanding factors influencing machine unlearning. In\n2022 IEEE 7th European Symposium on Security and\nPrivacy (EuroS&P), pages 303–319. IEEE.\nDaniel Trippa, Cesare Campagnano, Maria Sofia Bu-\ncarelli, Gabriele Tolomei, and Fabrizio Silvestri.\n2024.\n$\\nabla \\tau$:\nGradient-based and task-\nagnostic machine unlearning. In ICML 2024 Next\nGeneration of AI Safety Workshop.\nWeiwen Xu, Hou Pong Chan, Long Li, Mahani Alju-\nnied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao,\nGuizhen Chen, Chaoqun Liu, Zhaodonghui Li, and\n1 others. 2025. Lingshu: A generalist foundation\nmodel for unified multimodal medical understanding\nand reasoning. arXiv preprint arXiv:2506.07044.\nYuanshun Yao, Xiaojun Xu, and YangLiu. 2024. Large\nlanguage model unlearning. In Advances in Neural\nInformation Processing Systems, volume 37, pages\n105425–105475. Curran Associates, Inc.\nShoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon,\nand Mohit Bansal. 2025. Mexa: Towards general\nmultimodal reasoning with dynamic multi-expert ag-\ngregation. In Conference on Empirical Methods in\nNatural Language Processing.\nTianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng\nHan, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao\nZheng, Maosong Sun, and 1 others. 2024. Rlhf-v:\nTowards trustworthy mllms via behavior alignment\nfrom fine-grained correctional human feedback. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition.\nRuiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024.\nNegative preference optimization: From catastrophic\ncollapse to effective unlearning. In First Conference\non Language Modeling.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weix-\niong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. 2023a. Pmc-vqa: Visual instruction tuning for\nmedical visual question answering. arXiv preprint\narXiv:2305.10415.\nYi Zhang, Gengchen Li, Chen Li, Cheng Xia, Jia Sun,\nRuisi Kong, Siyu Chen, Xin Wang, Yichen Wang,\nand Yu Liu. 2023b. A unified method to revoke the\nprivate data of patients in intelligent healthcare with\naudit to forget. Nature Communications, 14(1):6257.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n2023. Judging llm-as-a-judge with mt-bench and\nchatbot arena. In Advances in Neural Information\nProcessing Systems.\n11\n"}, {"page": 12, "text": "A\nAppendix\nA.1\nCompared Unlearning Methods\nWe benchmark multiple unlearning strategies span-\nning prompt-based and gradient-based paradigms.\nSpecifically, we evaluate: Gradient Difference, KL\nMinimization, NPO, MANU.\nGradient Difference (Yao et al., 2024): It is a ver-\nsion of gradient ascent that optimizes the forget and\nretain objectives. Grad-Diff introduces a balanced\nobjective that simultaneously decreases loss on the\nretain set. The objective is given by\nLdiff = −L(SF , w) + L(SR, w),\nwhere SF and SR denote the forget and retain sets,\nrespectively. We interleave samples from both sets\nfor computational efficiency while ensuring that\ndeletion does not damage model utility on the retain\ndata.\nKL Minimization (Nguyen et al., 2020): It an\nunlearning technique that is guided by divergence\nand aims for targeted forgetting on the forget set\nSF . It simultaneously aims to anchor the model’s\noutputs on the retain set SR to those of the original\nfine-tuned model. Its approach aims to maximize\nthe cross-entropy loss on SF for deletion and aims\nto minimize theKL divergence on SR to get high\nretain set performance. The composite objective is\ngiven by\nLKL = −L(SF , w)\n+\n1\n|SR|\nX\ns∈SR\nKL(po(s) ∥pc(s)),\nwhere po and pc denote the output distributions\nof the original and current models, respectively,\nand w represents the model parameters. By inter-\nleaving batches from both sets during optimization,\nthis method achieves efficient unlearning with pre-\nserved utility on unrelated data.\nNegative Preference Optimization (NPO) (Zhang\net al., 2024): It is a preference optimization method\ndesigned to reduce the instability in gradient ascent.\nNPO treats forget set samples as negative prefer-\nences, encouraging the model to downweigh their\ninfluence relative to a reference policy derived from\nthe retain set. The loss is\nLNPO = 2\nβ E(x,y)∼SF\n\u0002\nlog\n\u00001 + r(x, y)β\u0001\u0003\n,\nwhere r(x, y) = πθ(y|x)/πref(y|x), with β = 0.9\ncontrolling the optimization curvature and πref\nas the retain-only reference model. This yields\nsmoother parameter updates, averting the perfor-\nmance collapse typical of unregularized ascent\nmethods.\nModality-Aware Neuron Unlearning (MANU)\n(Liu et al., 2025b): It is a neuron-level pruning\nfor unlearning framework for MLLMs that selec-\ntively removes modality-specific knowledge con-\ntributions. It works in two stages: (1) important\nneuron selection, where it computes an importance\nscore I(D, n) for each neuron n across both modal-\nities using four metrics, absolute (Iabs), frequency\n(Ifreq), variance (Ivar), and root mean square (Irms),\ndefined on activation deviations between textual\nand multimodal subsets of D; and (2) selective\npruning, which ranks neurons by the ratio score\nSn = I(Df, n)/(I(Dr, n) + ϵ) and sets weights\nto zero for the top α% most forget-associated neu-\nrons. By disentangling modality-specific activa-\ntions, it aims to enable targeted forgetting across\nboth modalities, achieving balanced unlearning.\nA.2\nOrthogonal Projection Derivation\nMathematical Formulation.\nGiven the projec-\ntion basis Q(l) ∈R|S(l)|×r obtained from SVD,\nwhere columns of Q(l) are orthonormal (i.e.,\n(Q(l))⊤Q(l) = Ir), the matrix P = Q(l)(Q(l))⊤is\nthe orthogonal projection matrix onto the subspace\nspanned by the columns of Q(l).\nThe complementary projection P⊥\n= I −\nQ(l)(Q(l))⊤projects onto the orthogonal comple-\nment of this subspace. Applying P⊥to the weight\nmatrix rows:\nW(l)\nS,: ←\n\u0010\nI −Q(l)(Q(l))⊤\u0011\nW(l)\nS,:\n(2)\nremoves all components of the weight vectors that\nlie within the forget-specific subspace, while pre-\nserving components orthogonal to it.\nConnection to Prior Work.\nThis orthogonal pro-\njection technique for removing specific information\nfrom neural network parameters has been explored\nin prior work. Belrose et al. (2023) introduced\nLEACE for closed-form concept erasure in repre-\nsentation space, proving that projecting onto the\nnullspace of concept-related directions prevents\nlinear classifiers from recovering the erased con-\ncept. Kodge et al. (2024) applied a similar principle\ndirectly to weight matrices for class unlearning, us-\ning SVD to identify class-discriminatory subspaces\nand projecting weights onto their orthogonal com-\nplement.\n12\n"}, {"page": 13, "text": "Dataset\nData Structure\nContext Type\nTask Types\nUnlearning Relevance\nMLLMU-\nBench\n(Liu et al.,\n2025a)\nSingle image;\nsingle long con-\ntext\nPerson profile or\ncaption\nGeneration, clas-\nsification,\nand\ncloze-style tasks\nHigh — tests multimodal\nmemorization and recall,\nbut lacks hierarchical or\ncompositional structure.\nCLEAR\n(Dontsov\net al., 2025)\nMultiple images;\nmultiple short con-\ntexts\nImage captions\nName\nrecogni-\ntion;\nentity prediction\nMedium\n—\ncaptures\nentity-level leakage,\nbut\nno\nhierarchical\nor\nmultimodal reasoning.\nUnLOK-\nVQA\n(Patil et al.,\n2024b)\nSingle image;\nsingle question\nPretrained\nknowledge only\nVQA-style\nen-\ntity prediction\nLow — focuses on object-\nlevel forgetting,\nwithout modeling dataset-\nscale deletions.\nMEDFORGET\n(Ours)\nHierarchical:\nInstitution →Pa-\ntient →Study →\nSection\nMultimodal:\nClinical images\n+ text reports\nGeneration,\nclassification,\nand cloze-style\ntasks\nacross hierarchy\nlevels\nHigh — supports hierar-\nchical unlearning that mir-\nrors real-world medical\ndeletion requests across\nmultiple granularity lev-\nels.\nTable 3: Comparison of multimodal unlearning benchmarks. Prior multimodal unlearning datasets study shallow or\nflat structures with limited contextual dependencies. MEDFORGET introduces clinically grounded, hierarchically\nstructured data that spans institutions, patients, and studies. It combines multimodal (image–text) reasoning with\nhierarchical unlearning challenges, enabling realistic assessment of unlearning performance at different granularities.\nOur method builds upon this foundation but dif-\nfers in the construction of the projection basis:\nrather than using simple forget-vs-retain activation\ndifferences, we compute sibling-differential direc-\ntions that exploit hierarchical graph structure to\nisolate target-specific representations while cancel-\ning shared semantic information. Additionally, we\napply selective neuron surgery and jointly operate\nacross language and vision-language fusion layers.\nA.3\nEvaluation Metrics\nWe evaluate all unlearning methods using three\ncomplementary metrics, each tailored to assess for-\ngetting completeness and utility preservation in\nmedical MLLMs:\nGeneration Score (Gen Score) for generation\ntasks, computed as a weighted average of 75%\nfactuality score and 25% ROUGE-L score (Lin,\n2004). The factuality score is obtained by prompt-\ning DeepSeek-V3 (DeepSeek-AI et al., 2025) to\nrate the factual accuracy and medical consistency\nof each generated clinical text against the ground\ntruth on a 1–10 scale (1 = nonsensical or clini-\ncally incorrect; 10 = fully accurate and consis-\ntent), following approaches in (Sun et al., 2023;\nYu et al., 2024; Zheng et al., 2023). The complete\nevaluation prompt template is provided in Table 7.\nThe ROUGE-L score captures the longest-common-\nsubsequence overlap between generated and refer-\nence texts, reflecting both precision and recall in\nclinical narrative generation.\nCloze Accuracy (Cloze Acc) for cloze-style tasks,\ncalculated via exact string matching between the\nmodel’s filled-in blank response and the ground-\ntruth clinical detail, evaluating reliance on memo-\nrized content under partial context.\nClassification Accuracy (Class. Acc) for clas-\nsification tasks, determined as the proportion of\ncorrect multiple-choice predictions on key clinical\nattributes, where the model selects the option with\nthe highest probability as the prediction given the\ninput question and chest X-ray image.\nFor all metrics, lower scores on the forget and\nrephrased forget sets indicate better forgetting,\nwhile higher scores on the retain set and exter-\nnal PMC-VQA benchmark reflect preserved diag-\nnostic utility. Evaluations are conducted indepen-\ndently across hierarchical levels {Institution, Pa-\n13\n"}, {"page": 14, "text": "Attack Level\nExample Prompt (Hierarchical Reconstruction Attack)\nContext Scope\nOriginal (No Context)\nReferring to this image, please generate the Impression section in detail.\n—\nStudy\nFor study study_chest_xray_481: Referring to this image, please gener-\nate the Impression section in detail.\nStudy\nPatient\nFor patient Jason Martinez, study study_chest_xray_481: Referring to\nthis image, please generate the Impression section in detail.\nPatient + Study\nInstitution\nFor institution Elm Medical Foundation, patient Jason Martinez, study\nstudy_chest_xray_481: Referring to this image, please generate the\nImpression section in detail.\nInstitution + Patient + Study\nTable 4: Examples of the cumulative hierarchical reconstruction attack used to evaluate the resistance of unlearned\nmodels. The attack starts from the Study-level context (since section-level alone lacks hierarchical identifiers) and\nprogressively prepends higher-level identifiers (Patient, Institution) to a section-level generation task (here generating\nthe Impression section). Higher unlearning granularity provides stronger protection against these increasingly\nspecific attacks.\nSec.\nStu.\nPat.\nInst.\n33\n36\n39\n42\n45\n48\nGen Score\nForget Set\nSec.\nStu.\nPat.\nInst.\n42\n45\n48\n51\n54\n57\nRetain Set\nSec.\nStu.\nPat.\nInst.\n15\n18\n21\n24\n27\n30\nForget Rephrase Set\nSec.\nStu.\nPat.\nInst.\n48\n51\n54\n57\n60\n63\nGeneral Med Set\nMANU\nGrad. Diff.\nKL Min.\nNPO\nCHIP\nFigure 7: Performance comparison of unlearning methods on the Forget Set, Retain, Forget Rephrase, and General\nMed sets across different hierarchy levels. The x-axis labels denote the following hierarchy levels: Inst. (Institute),\nPat. (Patient), Stu. (Study), and Sec. (Section). The Forget Rephrase and General Med sets are evaluation subsets.\ntient, Study, Section} × {Method}.\nA.4\nUnlearning Pipeline\nWe provide additional details on the unlearning\npipeline introduced in Section 5.1.\nFinetuning Stage.\nDuring finetuning on the hier-\narchical training set (see Section 3.1 and Table 1),\nthe model learns to capture hierarchical relation-\nships, associating imaging features with their insti-\ntutional and patient information, study metadata,\nand corresponding section-level text. The resulting\nmodel serves as the pre-unlearning (vanilla) base-\nline for all subsequent unlearning experiments.\nHierarchical Unlearning Stage.\nSince the hier-\narchy is cumulative, forgetting at a higher level\n(e.g., institution) subsumes all subordinate data\n(patients, studies, sections). This design enables\ncontrolled evaluation of how unlearning granular-\nity affects both forgetting completeness and utility\npreservation across different levels of the medical\ndata hierarchy.\nA.5\nQualitative Examples\nWe present qualitative comparisons to illustrate\nthe practical differences between hierarchical and\nnon-hierarchical unlearning in medical MLLMs.\nFigure 8 shows model outputs on forget set sam-\nples across all hierarchy levels after unlearning\nwith MEDFORGET (hierarchical) versus a flattened\ndataset (non-hierarchical). Hierarchical unlearning\nachieves comprehensive erasure across the entire\nsubtree beneath the targeted level, as evidenced by\nthe complete removal of institution names, patient\nidentifiers, and study details at their respective hier-\narchy levels. In contrast, non-hierarchical unlearn-\ning exhibits incomplete forgetting: broader identi-\nfiers (Institution, Patient) are frequently preserved\nwhile finer-grained information (Study ID formats,\nSection indications) is only partially erased. These\nqualitative observations align with our quantitative\nfindings and underscore the necessity of explicit\nhierarchical modeling for reliable medical data re-\nmoval.\n14\n"}, {"page": 15, "text": "Forget Set ↓\nRetain Set ↑\nHierarchy Variant\nF/R Diff ↑ROUGE\nFact.\nGen Score\nClass. Acc\nCloze Acc ROUGE\nFact.\nGen Score\nClass. Acc\nCloze Acc\nSection\nVanilla\n-0.61\n99.96\n99.80\n99.84\n100.00\n100.00\n99.92\n99.00\n99.23\n100.00\n100.00\nw/o Sibling\n2.25\n17.44\n16.20\n16.51\n99.79\n45.18\n19.24\n18.60\n18.76\n100.00\n48.60\nw/o VL Merger\n3.47\n40.70\n36.70\n37.70\n99.41\n73.72\n43.18\n40.50\n41.17\n100.00\n74.12\nw/o Vision-Text Sep\n1.99\n25.54\n21.50\n22.51\n100.00\n61.43\n25.40\n24.20\n24.50\n99.93\n57.16\nLang Only\n1.67\n25.84\n23.80\n24.31\n99.81\n58.95\n26.82\n25.70\n25.98\n99.84\n56.35\nZero-out Pruning\n-4.07\n24.42\n27.70\n26.88\n99.97\n62.70\n21.04\n23.40\n22.81\n99.80\n58.45\nCHIP (Full)\n6.56\n43.16\n39.80\n40.64\n93.34\n75.72\n47.50\n47.10\n47.20\n97.11\n78.57\nStudy\nVanilla\n-0.20\n99.98\n99.30\n99.47\n100.00\n100.00\n99.78\n99.10\n99.27\n100.00\n100.00\nw/o Sibling\n3.39\n16.14\n16.50\n16.41\n100.00\n46.91\n19.20\n20.00\n19.80\n99.39\n46.50\nw/o VL Merger\n8.74\n39.56\n36.20\n37.04\n100.00\n73.33\n46.62\n45.50\n45.78\n99.67\n74.52\nw/o Vision-Text Sep\n3.58\n24.90\n21.50\n22.35\n100.00\n61.14\n26.92\n25.60\n25.93\n99.60\n58.70\nLang Only\n4.15\n25.12\n22.80\n23.38\n99.94\n59.95\n27.92\n27.40\n27.53\n100.00\n56.11\nZero-out Pruning\n-2.41\n23.16\n26.80\n25.89\n99.52\n65.27\n21.92\n24.00\n23.48\n100.00\n60.34\nCHIP (Full)\n9.86\n40.82\n40.10\n40.28\n92.45\n74.87\n50.56\n50.00\n50.14\n96.31\n78.77\nPatient\nVanilla\n0.82\n98.92\n98.80\n98.83\n100.00\n100.00\n99.80\n99.60\n99.65\n100.00\n100.00\nw/o Sibling\n3.54\n15.52\n15.80\n15.73\n99.75\n42.41\n20.98\n18.70\n19.27\n100.00\n43.82\nw/o VL Merger\n7.40\n36.52\n34.60\n35.08\n99.44\n66.80\n44.82\n41.70\n42.48\n100.00\n69.40\nw/o Vision-Text Sep\n4.34\n23.26\n20.10\n20.89\n99.40\n54.32\n26.52\n24.80\n25.23\n100.00\n53.29\nLang Only\n3.49\n24.36\n21.80\n22.44\n100.00\n56.34\n26.02\n25.90\n25.93\n100.00\n51.27\nZero-out Pruning\n-2.91\n22.38\n26.50\n25.47\n100.00\n61.10\n21.84\n22.80\n22.56\n99.59\n57.80\nCHIP (Full)\n9.11\n39.64\n38.80\n39.01\n92.36\n69.58\n49.98\n47.50\n48.12\n97.46\n75.55\nInstitution\nVanilla\n-0.28\n99.40\n99.20\n99.25\n100.00\n100.00\n99.18\n98.90\n98.97\n100.00\n100.00\nw/o Sibling\n5.02\n16.30\n15.70\n15.85\n100.00\n40.64\n21.38\n20.70\n20.87\n100.00\n43.21\nw/o VL Merger\n10.39\n38.82\n35.30\n36.18\n100.00\n66.67\n49.78\n45.50\n46.57\n100.00\n68.21\nw/o Vision-Text Sep\n5.66\n23.30\n20.70\n21.35\n99.61\n53.88\n28.54\n26.50\n27.01\n99.38\n52.86\nLang Only\n6.11\n24.28\n22.60\n23.02\n100.00\n54.34\n30.12\n28.80\n29.13\n99.69\n52.14\nZero-out Pruning\n-1.05\n22.72\n26.60\n25.63\n100.00\n57.53\n23.02\n25.10\n24.58\n100.00\n56.43\nCHIP (Full)\n12.60\n40.44\n39.00\n39.36\n91.33\n67.03\n53.64\n51.40\n51.96\n97.06\n73.16\nTable 5: Ablation study results across all hierarchy levels. All variants within the same level use identical\nhyperparameters except for the ablated component. F/R Diff (Retain −Forget Gen Score) serves as the primary\nmetric, as lower Forget scores may result from indiscriminate knowledge destruction rather than selective forgetting.\nThe best F/R Diff within each level is shown in bold. All values are percentages (%).\nA.6\nExtended Experiment Results\nIn this section, we provide comprehensive exper-\nimental results that complement the main paper\nby presenting detailed generation quality metrics\nacross all hierarchical unlearning splits. While the\nmain paper focuses on the Gen Score metric for\nconciseness, here we report the individual compo-\nnents: ROUGE-L scores and Factuality scores (see\nAppendix A.3 for metric definitions), alongside the\ncomposite Gen Score values.\nTable 6 presents the full results across all four\nevaluation splits (Forget Set, Retain Set, Forget\nRephrase Set, and General Med Set) for each hier-\narchical level (Institution, Patient, Study, and Sec-\ntion). These detailed metrics enable a nuanced\nunderstanding of the trade-offs between unlearning\neffectiveness and utility preservation.\nA.7\nAblation Study\nWe conduct comprehensive ablation studies to val-\nidate the design choices in CHIP across all four\nhierarchy levels: Section, Study, Patient, and In-\nstitution. This enables examination of how each\ncomponent contributes to selective unlearning from\nfine-grained to coarse-grained scenarios.\nEvaluation Protocol.\nTo ensure fair comparison,\nall variants within the same hierarchy level share\nidentical hyperparameters (layer selection, neuron\nratio, SVD threshold), with only the ablated com-\nponent modified. We adopt F/R Diff (Retain Gen\nScore minus Forget Gen Score) as the primary eval-\nuation metric. This choice is motivated by a key\nobservation: some ablated variants achieve lower\nForget scores (seemingly better forgetting) but at\nthe cost of drastically reduced Retain performance.\nIn such cases, the model has lost utility indiscrim-\ninately rather than achieving selective forgetting.\nF/R Diff captures this trade-off by measuring the\ngap between retained and forgotten knowledge,\nwhere higher values indicate more selective un-\nlearning.\nA.7.1\nSibling Contrast\nThe core innovation of CHIP is the sibling-\ncontrastive direction d = µtarget −µsiblings, which\nisolates target-specific information while preserv-\ning sibling-shared knowledge. We compare against\na variant (w/o Sibling) that uses only the target\nmean activation without sibling subtraction.\nAs shown in Table 5, removing the sibling con-\n15\n"}, {"page": 16, "text": "Hierarchy\nQuestion\nGround Truth\nOutput from Model\nUnlearned on MedForget\nOutput from Model\nUnlearned on Non-\nHierarchical Datasets\nExplanation\nInstitution\nWhich medical\ninstitution is\nresponsible for\nthis radiograph?\nElm Medical\nFoundation\nTrinity Health System\n(Wrong institution name –\nUnlearned)\nElm Medical Foundation\n(Correct institution name – Not\nunlearned)\nMedForget correctly\nforgets institution name;\nnon-hierarchical still leaks\nit\nPatient\nWho is the\npatient in this\nmedical image?\nPatricia Baker\nDorothy Ramirez\n(Wrong patient name –\nUnlearned)\nPatricia Baker\n(Correct patient name – Not\nunlearned)\nMedForget erases patient\nname completely; non-\nhierarchical retains PHI\nStudy\nWhat is the\nidentifier for\nthis imaging\nstudy?\nstudy_chest_xra\ny_021\nstudy_chest_xray_243.jpeg\n(Wrong study ID and wrong\nformat – Unlearned)\nstudy_chest_xray_031\n(Wrong study ID, yet correct\nformat – Partially not\nunlearned)\nMedForget additionally\nremoves filename\npattern; non-hierarchical\npreserves format clue\nSection\nLooking at this\nmedical image,\nwhat does the\nindication\nsection\nindicate?\nWorsening\ndyspnea.\nw SOB // Eval for pulmonary\nedema, effusion, or other acute\nprocess\n(Wrong indication – Unlearned)\nw // Please eval for pulmonary\nedema, r/o acute process\n(Wrong indication – Unlearned)\nBoth forget the original\nindication\nInput Image\nFigure 8: Qualitative comparison of MEDFORGET vs. non-hierarchical datasets across different hierarchy levels.\nFor each level, we compare model outputs after unlearning on hierarchically-organized data (MEDFORGET) versus\nflattened data. Green text indicates successfully unlearned information, orange text indicates partially retained\ninformation, and red text indicates information not unlearned.\ntrast leads to a consistent pattern across all hierar-\nchy levels: while Forget scores decrease substan-\ntially (appearing as “better” forgetting), the Retain\nscores collapse even more severely. At Institution\nlevel, Retain Gen Score drops from 51.96 to 20.87\n(a 60% reduction), causing F/R Diff to fall from\n12.60 to 5.02. This pattern persists across finer\ngranularities, with Section level showing F/R Diff\ndegradation from 6.56 to 2.25.\nThis finding reveals the critical role of sibling\ncontrast: without subtracting µsiblings, the com-\nputed direction captures not only target-specific\ninformation but also shared hierarchical patterns\n(e.g., institutional formatting, common imaging\ncharacteristics). Projecting out this conflated di-\nrection removes both target and sibling knowledge\nindiscriminately, resulting in catastrophic utility\nloss. The sibling-contrastive formulation isolates\nwhat makes the target different from its siblings, en-\nabling selective forgetting while protecting shared\nrepresentations.\nA.7.2\nMultimodal Fusion\nCHIP incorporates two multimodal fusion designs:\n(1) VL Merger Modification, which projects out\nforget-specific directions in the vision-language\nmerger layers, and (2) Vision-Lang Separation,\nwhich separately aggregates activations from visual\nand textual tokens during activation collection us-\ning weighted combination: a = α · avision + (1 −\nα) · alang. We also include a Lang Only baseline\nthat modifies only language layers without any mul-\ntimodal intervention.\nResults in Table 5 demonstrate the contribution\nof each component:\n(1) VL Merger modification enables cross-\nmodal forgetting. Removing VL Merger modi-\nfication (w/o VL Merger) leads to F/R Diff degra-\ndation across all hierarchy levels: from 6.56 to 3.47\nat Section level, 9.86 to 8.74 at Study level, 9.11\nto 7.40 at Patient level, and 12.60 to 10.39 at In-\nstitution level. This confirms that modifying the\nvision-language binding layer is necessary for dis-\nrupting the cross-modal associations that encode\nforgotten information.\n(2) Vision-Lang Separation improves direc-\ntion precision. Removing Vision-Lang Separa-\ntion (w/o Vision-Lang Sep) also degrades F/R Diff\nacross all levels: from 6.56 to 1.99 at Section level,\n9.86 to 3.58 at Study level, 9.11 to 4.34 at Patient\nlevel, and 12.60 to 5.66 at Institution level. This\ndemonstrates that separately aggregating visual and\nlanguage activations produces more precise forget\ndirections than naive averaging.\n(3) Lang Only baseline confirms the neces-\nsity of multimodal intervention. The Lang Only\nvariant, which applies projection only to language\nlayers without multimodal components, achieves\n16\n"}, {"page": 17, "text": "Forget Set\nQuestion: Looking at this\nmedical image, what\ndoes the indication\nsection indicate?\nAnswer: woman s/p rsxn\nof large lung mass //\ninterval CXR\nForget Rephrase Set\nQuestion: Based on this\nmedical image, what\ninformation is contained within\nthe indication section?\nAnswer: woman s/p rsxn of\nlarge lung mass // interval CXR\nRetain Set\nQuestion: In the context\nof this radiograph,\ndescribe the impression\nsection for this patient?\nAnswer: Resolution of\nright multi focal\npneumonia. Unchanged\nleft pleural effusion.\nGeneral Med Set\nQuestion: Can you\ndetermine the type of\nfracture?\nAnswer: Comminuted\ntransverse fracture\nForget Image\nRotated −15°\nForget Image\nRetain Image\nGeneral Med\nImage\nEvaluation Sets\n(a)\n(b)\n(c)\n(d)\nQuestion: Given this\nchest X-ray, summarize\nthe key points in the\nfindings section?\nAnswer: There is\npersistent right lower\nlobe consolidation with\nfiducial marker\ncompatible with\npatient's known\nadenocarcinoma. \nForget Image\nForget Image\nRotated −15°\nQuestion: Based on the provided\nchest X-ray, what are the\nprincipal observations to include\nin the findings section?\nAnswer: Persistent consolidation\nis noted in the right lower lobe,\nwhich contains a fidibucial\nmarker and is consistent with\nthe patient's known\nadenocarcinoma. \nQuestion: What organ\nappears abnormal in\nthis radiograph?\nAnswer: Gallbladder\nGeneral Med\nImage\nQuestion: Referring to\nthis image, explain the\nexamination section in\ndetail\nAnswer: CHEST (PA AND\nLAT) \nForget Image\nQuestion: In the context\nof this radiograph,\ndescribe the impression\nsection for this patient\nAnswer: Resolution of\nright multi focal\npneumonia. Unchanged\nleft pleural effusion.\nForget Image\nRotated −30°\nQuestion: Based on the\nprovided image, provide a\ncomprehensive explanation of\nthe examination section.\nAnswer: CHEST\n(POSTEROANTERIOR AND\nLATERAL VIEWS)\nForget Image\nForget Image\nRotated −30°\nQuestion: In the context\nof this radiograph,\ndescribe the indication\nsection for this patient\nAnswer: Altered mental\nstatus with slurred\nspeech.\nQuestion: Based on the\nfindings in this radiograph,\nwhat are the clinical\nindications documented for\nthis patient?\nAnswer: The patient presented\nwith a change in mental status\nand dysarthria.\nRetain Image\nRetain Image\nQuestion: Given this\nchest X-ray, summarize\nthe key points in the\nfindings section\nAnswer: An aortic stent\nis in place within the\ntortuous thoracic aorta\nwith a known aneurysm\narising from the aortic\narch. \nRetain Image\nQuestion: Referring to\nthis image, explain the\nclinical history section of\nthis patient\nAnswer: male with chest\npain\nGeneral Med\nImage\nQuestion: The patient\nshown below most\nlikely has?\nAnswer: Osteoarthritis\nQuestion: This patient\ncame in complaining of\nsevere abdominal pain.\nWhy is this an\nemergency?\nAnswer: A ruptured\nabdominal aortic\naneurysm.\nGeneral Med\nImage\nFigure 9: More examples of data samples in MEDFORGET.\nlower F/R Diff than the full model at all levels\n(1.67 at Section, 3.20 at Study, 3.49 at Patient, 5.20\nat Institution). This confirms that language-layer\nmodification alone is insufficient for multimodal\nunlearning, and both VL Merger modification and\nVision-Lang Separation contribute to the overall\neffectiveness.\n(4) Components exhibit synergistic effects.\nThe full model consistently achieves the high-\nest F/R Diff across all levels, indicating that VL\nMerger modification and Vision-Lang Separation\naddress complementary aspects of multimodal\nmemorization.\nA.7.3\nWeight Modification\nWe compare our subspace projection approach\nagainst the zero-out pruning strategy. While both\nmethods modify weights based on importance\nscores, projection removes only the forget-specific\nsubspace components, whereas zero-out eliminates\nselected neuron contributions.\nAs shown in Table 5, zero-out pruning produces\npathological behavior across all hierarchy levels:\n(1) Zero-out yields negative F/R Diff at all lev-\nels. The F/R Diff values are consistently negative:\n−4.07 (Section), −2.41 (Study), −2.91 (Patient),\nand −1.05 (Institution). This indicates that Retain\nperformance falls below Forget performance, i.e.,\nthe model performs worse on data it should retain\nthan on data it should forget.\n(2) Subspace projection preserves orthogonal\ninformation.\nOur projection-based approach\n(Wnew = (I −QQ⊤)Wold) removes only the\ncomponents aligned with forget directions while\npreserving orthogonal information. In contrast,\nzero-out completely eliminates selected neurons,\ndestroying both forget-aligned and retain-aligned\ncomponents encoded in those neurons.\n(3) The performance gap is substantial across\nall granularities. The gap between CHIP and zero-\nout ranges from 10.63 pts at Section level to 13.65\npts at Institution level. This consistent advantage\nshows that subspace projection is fundamentally\nmore suitable for selective unlearning than zero-out\npruning, regardless of the hierarchy granularity.\n17\n"}, {"page": 18, "text": "Hierarchy Method\nForget Set ↓\nRetain Set ↑\nF/R Diff ↑\nEvaluation Sets\nForget Rephrase Set ↓\nGeneral Med Set ↑\nROUGE\nFact.\nGen Score ROUGE\nFact.\nGen Score\nROUGE\nFact.\nGen Score ROUGE\nFact.\nGen Score\nSection\nVanilla\n99.96\n99.80\n99.84\n99.92\n99.00\n99.23\n-0.61\n58.89\n58.33\n58.47\n67.10\n68.99\n68.52\nMANU\n46.34\n44.30\n44.81\n50.56\n46.90\n47.82\n3.01\n29.41\n27.70\n28.13\n56.98\n53.10\n54.07\nGrad. Diff.\n48.37\n44.20\n45.24\n50.41\n47.70\n48.38\n3.14\n31.24\n26.40\n27.61\n59.26\n56.60\n57.27\nKL Min.\n44.84\n43.10\n43.54\n53.86\n53.40\n53.52\n9.98\n28.80\n26.40\n27.00\n57.39\n57.20\n57.25\nNPO\n47.06\n43.80\n44.62\n54.12\n49.50\n50.66\n6.04\n36.99\n33.60\n34.45\n61.20\n58.50\n59.18\nCHIP\n43.17\n39.80\n40.64\n47.49\n47.10\n47.20\n6.56\n22.06\n21.90\n21.94\n58.17\n54.20\n55.19\nStudy\nVanilla\n99.98\n99.30\n99.47\n99.78\n99.10\n99.27\n-0.20\n57.71\n58.60\n58.38\n67.64\n68.75\n68.47\nMANU\n44.49\n43.50\n43.75\n49.08\n46.90\n47.45\n3.70\n28.98\n27.20\n27.64\n58.94\n55.40\n56.28\nGrad. Diff.\n48.30\n43.80\n44.92\n49.91\n49.80\n49.83\n4.91\n30.24\n25.90\n26.98\n58.59\n55.00\n55.90\nKL Min.\n45.75\n41.90\n42.86\n49.02\n44.60\n45.70\n2.84\n28.16\n23.20\n24.44\n55.68\n55.70\n55.70\nNPO\n46.31\n43.50\n44.20\n53.63\n48.90\n50.08\n5.88\n31.13\n29.30\n29.76\n57.23\n56.90\n56.98\nCHIP\n40.82\n40.10\n40.28\n50.57\n50.00\n50.14\n9.86\n25.63\n25.10\n25.23\n58.87\n54.10\n55.29\nPatient\nVanilla\n98.92\n98.80\n98.83\n99.80\n99.60\n99.65\n0.82\n59.32\n58.37\n58.61\n69.16\n68.45\n68.63\nMANU\n41.13\n39.60\n39.98\n45.54\n44.60\n44.84\n4.86\n26.63\n26.20\n26.31\n59.41\n56.30\n57.08\nGrad. Diff.\n42.56\n38.50\n39.52\n49.58\n45.40\n46.45\n6.93\n28.53\n25.30\n26.11\n57.69\n55.50\n56.05\nKL Min.\n45.24\n42.50\n43.19\n52.05\n51.30\n51.49\n8.30\n24.35\n22.20\n22.74\n59.07\n55.20\n56.17\nNPO\n46.94\n44.10\n44.81\n54.37\n49.50\n50.72\n5.91\n26.82\n25.50\n25.83\n59.02\n54.80\n55.86\nCHIP\n39.63\n38.80\n39.01\n50.00\n47.50\n48.12\n9.11\n23.93\n23.80\n23.83\n56.53\n52.70\n53.66\nInstitution\nVanilla\n99.40\n99.20\n99.25\n99.18\n98.90\n98.97\n-0.28\n59.60\n58.03\n58.42\n67.31\n68.96\n68.55\nMANU\n44.70\n40.40\n41.48\n50.66\n46.10\n47.24\n5.76\n25.86\n21.90\n22.89\n59.80\n56.40\n57.25\nGrad. Diff.\n43.68\n39.40\n40.47\n51.10\n50.00\n50.27\n9.80\n23.47\n21.80\n22.22\n57.19\n52.90\n53.97\nKL Min.\n40.92\n40.60\n40.68\n54.16\n50.60\n51.49\n10.81\n26.52\n23.50\n24.26\n57.18\n55.50\n55.92\nNPO\n44.41\n39.60\n40.80\n52.55\n50.20\n50.79\n9.99\n28.79\n24.10\n25.27\n57.84\n55.50\n56.09\nCHIP\n40.45\n39.00\n39.36\n53.64\n51.40\n51.96\n12.60\n18.46\n18.20\n18.27\n55.60\n52.90\n53.58\nTable 6: Extended performance metrics across hierarchical unlearning splits. All values are percentages (%). Gen\nScore is computed as 0.25 × ROUGE + 0.75 × Fact. F/R Diff denotes the difference between Retain Set and Forget\nSet Gen Scores. ↓indicates lower is better for unlearning, ↑indicates higher is better for utility preservation. Best\nunlearning and utility performance are shown in bold and underlined.\nA.8\nDataset Statistics\nWe provide detailed breakdowns of the forget-\nretain partitions at each hierarchy level. At the in-\nstitution level, we designate 2 out of 8 institutions\nas forget targets, with the remaining 6 as retain. At\nthe patient level, 16 out of 64 patients are assigned\nto the forget set (2 per institution on average). At\nthe study level, 64 out of 256 studies are forgotten\n(4 per patient on average). At the section level, 256\nout of 1,024 sections belong to the forget set (4\nper study on average). For task-level statistics, the\n3,072 section-level VQA pairs are evenly divided\namong three task types: 1,024 generation, 1,024\ncloze, and 1,024 classification instances. Within\neach task type, 256 instances belong to the forget\nset and 768 to the retain set, maintaining the 25%\nforget ratio. The 768 hierarchical identity tasks\ncomprise 256 institution-level, 256 patient-level,\nand 256 study-level instances, each with 64 forget\nand 192 retain samples. This controlled partition-\ning ensures that unlearning effectiveness and utility\npreservation can be evaluated consistently across\nall hierarchy levels and task types.\nFactuality Score Evaluation Prompt\nYou will evaluate the factuality of the “generated_answer”\nagainst the “ground_truth” for medical image analysis\nquestions. Assess how well the response captures the\nKEY MEDICAL INFORMATION and assign a score\n(1-10).\nEvaluation Principles:\n1. Medical Terminology Accuracy\n2. Core Clinical Content (anatomical structures, find-\nings, diagnostic information)\n3. Partial Credit for key concepts\n4. Context Sensitivity\nScoring Rubric:\n10–9: Fully correct; all key findings present\n8–7: Mostly correct; minor omissions\n6–5: Key terms present but missing context\n4–3: Some terminology but misses critical findings\n2–1: Mostly incorrect or irrelevant\nInput:\nQuestion:\n{question};\nGenerated:\n{generated}; Ground Truth: {ground_truth}\nOutput: {\"score\": <1-10>, \"reasoning\": \"...\"}\nTable 7: Prompt template for factuality evaluation.\n18\n"}]}