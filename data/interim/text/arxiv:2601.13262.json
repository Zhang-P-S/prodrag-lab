{"doc_id": "arxiv:2601.13262", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13262.pdf", "meta": {"doc_id": "arxiv:2601.13262", "source": "arxiv", "arxiv_id": "2601.13262", "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning", "authors": ["Eric Onyame", "Akash Ghosh", "Subhadip Baidya", "Sriparna Saha", "Xiuying Chen", "Chirag Agarwal"], "published": "2026-01-19T17:51:00Z", "updated": "2026-01-19T17:51:00Z", "summary": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13262v1", "url_pdf": "https://arxiv.org/pdf/2601.13262.pdf", "meta_path": "data/raw/arxiv/meta/2601.13262.json", "sha256": "018a317febcdb46563fbeef7365945b6ccd61e04f54bb6c4829f2b9e9464c809", "status": "ok", "fetched_at": "2026-02-18T02:21:07.958156+00:00"}, "pages": [{"page": 1, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for\nMultilingual Medical Reasoning\nEric Onyame∗\nUniversity of Virginia\nAkash Ghosh∗\nIIT-Patna\nSubhadip Baidya\nIIT-Patna\nSriparna Saha\nIIT-Patna\nXiuying Chen\nMBZUAI\nChirag Agarwal\nUniversity of Virginia\nAbstract\nWhile large language models (LLMs) have shown to perform well on monolingual mathematical\nand commonsense reasoning, they remain unreliable for multilingual medical reasoning appli-\ncations, hindering their deployment in multilingual healthcare settings. We address this by first\nintroducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-\nended reasoning queries with a single verifiable answer, spanning thirteen languages, including\nunderrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we\npropose CURE-MED, a curriculum-informed reinforcement learning framework that integrates\ncode-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly\nimprove logical correctness and language stability. Across thirteen languages, our approach consis-\ntently outperforms strong baselines and scales effectively, achieving 85.21% language consistency\nand 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04%\nlogical correctness at 32B parameters. These results support reliable and equitable multilingual\nmedical reasoning in LLMs. The code and dataset are available at cure_med.\n1\nIntroduction\nRecent progress in large language models (LLMs) and reasoning-oriented systems has produced strong performance\nin mathematical reasoning and code generation [1–4]. While these advances suggest LLMs can learn structured\nsolution strategies beyond pattern completion, medical reasoning remains challenging [5, 6] because it requires domain\nknowledge, careful use of context, and reasoning that clinicians can inspect [7, 8].\nPrior work shows promising medical QA results, yet reliable medical reasoning still depends on reasoning-centric\ndata and evaluations that test reasoning behavior rather than answer plausibility [9–11]. Without such resources, models\nmay generate fluent, credible-sounding outputs without dependable reasoning. The problem is amplified in multilingual\nsettings: progress remains English-centered, leaving mid- and low-resource languages underrepresented and reliability\nuneven across communities. Despite cross-lingual transfer, open-ended medical reasoning often exhibits two recurring\nfailures: reduced logical accuracy and unstable language behavior [12, 13]. For clinical use, these failures erode\ninterpretability and trust, since clinicians and patients must understand not only what a system concludes, but how\nit arrives there [14].\nWhile recent efforts attempt to strengthen medical capability through domain-specific supervision [15, 16], bench-\nmarks primarily remain monolingual and rely on closed-form settings, providing limited visibility into multilingual\nreasoning quality and language fidelity [17]. As LLMs increasingly support clinical education and decision-making,\nsystematic evaluation of multilingual reasoning and language consistency becomes essential for fairness, reliability, and\ngeneralization [9, 12].\nIn this work, We study multilingual medical reasoning across 13 high-, mid-, and low-resource languages. We introduce\nCUREMED-BENCH, an open-ended benchmark where each query has a single verifiable answer, enabling independent\nevaluation of logical accuracy and language consistency and analysis of cross-lingual generalization under clinically\ngrounded constraints. Next, we propose CUREMED, a two-stage training framework (see Figure 1) for multilingual\nmedical reasoning. We apply code-switching-aware supervised fine-tuning (SFT) to stabilize language usage during\n∗Equal Contribution. Correspondence Authors: Eric Onyame and Akash Ghosh\narXiv:2601.13262v1  [cs.AI]  19 Jan 2026\n"}, {"page": 2, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nA.\nB.\nC.\nStage 0: Curating a multilingual data \nusing clinically-validated sources like \nMedlinePlus for training CURE-MED\nStage 1: Supervised fine-tuning the \nmultilingual Qwen model on the code-\nswitched medical reasoning data\nStage 2: GRPO-guided curriculum learning, \nstructuring the training progressively from high- \nto medium- and finally low-resource languages\nCURE-MED, our \nmultilingual medical \nreasoning model\n<step1> The question describes une \nfemme de 34 ans presenting with \nsevere abdominal pain in de l’hypoc-\nhondre droit, … … la cholécystite \naiguë as the diagnosis.</step4>\nFigure 1: The CURE-MED pipeline for multilingual medical reasoning. The framework progresses through three\nstages: (A) curation of clinically validated multilingual data from sources like MedlinePlus to enable cross-lingual\nreasoning; (B) supervised fine-tuning of the Qwen2.5-Instruct backbone on code-switched reasoning traces; and (C)\nGRPO-guided curriculum reinforcement learning, progressively training from high- to mid- and low-resource languages\nto enhance logical correctness and language consistency.\nintermediate reasoning steps and perform curriculum-informed GRPO to improve logical correctness and language\nfidelity. Our contributions are: 1) We present a systematic evaluation of multilingual medical reasoning of LLMs\nusing verifiable medical queries, enabling reliable measurement of logical accuracy and language consistency across\nlanguages; 2) We introduce CUREMED-BENCH, a large-scale multilingual medical reasoning dataset spanning 13\nlanguages across high-, mid-, and low-resource settings; 3) We propose CURE-MED, a two-stage training framework\nfor multilingual medical reasoning that combines code-switching-aware SFT with curriculum-informed reinforcement\nlearning (RL) to jointly optimize logical correctness and linguistic fidelity; and 4) Through extensive automatic\nand human evaluations, we show that CURE-MED achieve state-of-the-art performance on CUREMED-BENCH and\ndemonstrate improved out-of-distribution generalization, including improved robustness in low-resource languages and\nstronger performance on unseen medical questions and languages.\n2\nRelated Work\nThis work lies at the intersection of medical reasoning with LLMs and multilingual reasoning. We summarize key\ngaps in prior work and position CURE-MED as a unified response.\nLarge Medical Reasoning Models. LLMs have been widely studied for medical QA, clinical retrieval, and diagnostic\ntasks [10, 15, 18, 19]. Domain-specific pretraining and instruction tuning can improve factuality, yet benchmark gains\noften do not translate to reliable medical reasoning [11, 20], with models producing fluent but clinically unsound\nexplanations [14]. A core issue is evaluation: many medical benchmarks are closed-form (e.g., multiple-choice),\nwhich hides intermediate reasoning and limits verification of logical validity [17, 20]. Recent open-ended evaluations\nexist, but are largely monolingual or limited to a few high-resource languages, leaving multilingual medical reasoning\nunderexplored [17, 21].\nWe address these gaps by introducing open-ended medical queries with single verifiable answers across 13 diverse\nlanguages, enabling independent assessment of reasoning correctness.\nMultilingual Reasoning and Language Fidelity. Prior work shows CoT prompting can enable cross-lingual inference\ntransfer [3, 22, 23], but evaluations mostly target general-domain math/symbolic tasks and skew toward high-resource\nlanguages [4, 12, 13, 24, 25]. In medical settings, models often exhibit degraded accuracy, language drift, and weak\ncross-lingual generalization [17, 21]. Methods such as language mixing and supervised reasoning distillation can\nimprove fluency, but are typically studied in limited bilingual settings or overfit high-resource languages [26–30]. RL\nhas also been used to promote structured reasoning, but remains largely English-centric and general-domain [31–35].\n2\n"}, {"page": 3, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nCURE-MED differs from prior work by optimizing language fidelity and reasoning correctness jointly. We evaluate\nacross high-, mid-, and low-resource languages, and integrate code-switching-aware supervision with curriculum-\ninformed RL for robust multilingual medical reasoning.\n3\nMethodology\nHere, we describe the construction of CUREMED-BENCH (Sec. 3.1), including dataset collection and human verification.\nNext, we present CURE-MED: cold-start initialization (Sec. 3.2), reward design (Sec. 3.3), and GRPO-guided curriculum\nreinforcement learning (Sec. 3.4).\n3.1\nDataset Collection\nWe construct CUREMED-BENCH, a multilingual medical reasoning dataset of 15,774 open-ended QA instances across\n13 languages spanning Africa, Asia, and Europe, enabling evaluation under diverse linguistic conditions (including\nAfrican languages such as Hausa, Yoruba, and Swahili). A breakdown by language and language family is provided in\nAppendix C.\nSource Material and Question Generation. CUREMED-BENCH is grounded in MedlinePlus, a clinically validated\nmedical resource curated by U.S. federal health agencies. Following tool-assisted synthetic data generation [36–41], we\nuse GPT-4o to retrieve MedlinePlus content and draft closed-ended multiple-choice questions in each target language.\nEach item is anchored to the source, includes four options with exactly one correct answer, and provides clinically\ngrounded supervision prior to conversion to open-ended prompts.\nFiltering for Reasoning Difficulty. Following Chen et al. [42], we apply multi-stage filtering to retain questions\nrequiring substantive medical reasoning. We remove trivial items by discarding questions answered correctly by all\nthree compact LLMs: Qwen2.5-3B/7B [43] and LLaMA-3.1-8B [44]. We further exclude under-specified or ambiguous\nquestions, retaining samples with a single, unambiguous correct answer and consistent cross-lingual interpretation;\nGPT-4o is used to identify cases with multiple valid answers or cross-lingual inconsistency.\nConversion to Open-Ended Problems. We convert each remaining item into an open-ended prompt x using GPT-4o,\nand generate an explicit reasoning chain r with a free-form ground-truth answer y∗. This removes multiple-choice cues\nand yields open-ended instances with supervised reasoning, enabling direct evaluation of reasoning quality and answer\ncorrectness. We define the dataset as D = {(x, r, y∗)}, where each instance has a single clinically grounded solution\nsupported by an explicit reasoning trace. As summarized in Table 1, CUREMED-BENCH contains 15,774 instances\nacross 13 languages, including low-resource languages, extending prior benchmarks that are largely multiple-choice\nand/or linguistically limited.\nHuman Verification and Ethical Review. All samples are verified by native speakers and medical experts (physicians,\nadvanced medical students, and nursing PhD candidates). Reviewers assess clinical correctness, linguistic fidelity,\nand cultural appropriateness, revising culture-specific terminology and removing translation artifacts or medically\ninappropriate content. Across 13 languages, user studies report an average rating of 4.89/5, supporting clinical validity\n(Appendix Table 5). All procedures were approved by an Institutional Review Board for social and behavioral sciences\nand followed established ethical research standards. Additional details are provided in Appendix D.\nDataset\nLang.\nSize\nOpen-ended?\nReasoning-Supervision\nLow-resource?\nMMedBench\n6\n8.5k\n✗\n✓\n✗\nMedQA\n3\n13k\n✗\n✗\n✗\nMedExpQA\n4\n2,488\n✗\n✓\n✗\nPubMedQA\n1\n211k\n✗\n✓\n✗\nMedQAUSMLE\n1\n11.4k\n✗\n✗\n✗\nMedMCQA\n1\n193k\n✗\n✓\n✗\nOphthaLingua\n7\n1,184\n✗\n✗\n✓\nMCMLE\n1\n270k\n✗\n✗\n✗\nXMedBench\n4\n8,280\n✗\n✗\n✗\nWorldMedQA\n4\n568\n✗\n✗\n✗\nHealthSearchQA\n1\n3,173\n✓\n✓\n✗\nCURE-MED-\nBench\n13\n15,774\n✓\n✓\n✓\nTable 1: Comparison of medical domain benchmarks.\n3\n"}, {"page": 4, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nExample: Cold-Start Code-Switched Reasoning (French)\nQuestion. Une femme de 34 ans se présente aux urgences avec des douleurs abdominales sévères localisées dans l’hypochondre droit, associées à des nausées et des\nvomissements. Étant donné ses antécédents de calculs biliaires, quel est le diagnostic le plus probable pour son état?\nReasoning (code-switched CoT). <thinking> <step1> The question describes une femme de 34 ans presenting with severe abdominal pain in the région de\nl’hypochondre droit, along with des nausées et des vomissements. These are classic upper abdominal symptoms. </step1> <step2> Ses symptômes suggèrent\nune atteinte possible de la vésicule biliaire, especially given her history of calculs biliaires. This could indicate a gallbladder-related condition. </step2> <step3>\nThe presence of fièvre à 38,5°C and tenderness upon palpation of the quadrant supérieur droit indicates an inflammatory process — likely une infection. </step3>\n<step4> In this context, on peut fortement envisager la cholécystite aiguë (acute cholecystitis) as the most probable diagnosis given the presentation and risk factors.\n</step4> </thinking>\nAnswer. <answer> Le diagnostic le plus probable est la cholécystite aiguë </answer>\nLanguage: French\nType: Cold-start code-switched CoT sample\nFigure 2: An example from the cold-start multilingual dataset showing CoT reasoning in French. The reasoning\ncombines English-based clinical terms and local-language expressions, reflecting code-switching in medical contexts.\n3.2\nCold-Start Initialization via Supervised Fine-Tuning (SFT)\nWe initialize multilingual reasoning with a cold-start SFT stage on code-switched long CoT trajectories. This stage\nstabilizes multi-step reasoning in the base model before we introduce stricter language-consistency constraints in later\ntraining. Given an input query x in the target language ℓ, we construct a multi-step reasoning trajectory that allows\ncontrolled code-switching in intermediate steps (see Figure 2 for a French subset example). Each trajectory contains\nreasoning steps r = {r1, . . . , rT }, where step rt may be written in language ℓt ∈L, followed by a final answer y∗\nwritten in the target language ℓ.\nWe fine-tune the model by maximizing the likelihood of the reasoning trajectory and final answer conditioned on\nthe input: LSFT = −log pθ(r, y∗| x), training the model to produce multi-step reasoning before generating the final\nresponse. Code-switching in r allows the LLM use the most effective language for intermediate inference while keeping\nthe final answer in ℓ. The resulting language-adaptive reasoning behavior provides a strong initialization for RL stages\nthat enforce language consistency without degrading logical accuracy.\n3.3\nReward Design\nWe train CURE-MED with a weighted reward that promotes clinical correctness, language fidelity, and adherence\nto a structured output format. We use a closed-source multilingual reward model that performs competitively on\nRewardBench [45]. To mitigate same-model judge bias, we use a separate model for LLM-as-a-judge verification\n[46, 47].\nCorrectness Reward. Following Zheng et al. [48], we use GPT-4.1 as a verifier to score semantic and clinical\nequivalence between the model output (y), and reference answer (y∗). The verifier returns a continuous score in [0, 1]:\nRacc(y | x, y∗) = vacc(x, y, y∗) ∈[0, 1].\n(1)\nWe use exact-match scoring for closed-ended questions. For open-ended questions, the verifier assigns partial credit\nwhen the response reaches the correct conclusion via clinically valid reasoning, even under paraphrase [49], providing\nsmoother learning signals.\nLanguage Consistency Reward. We enforce strict output-language fidelity by scoring whether y is written entirely in\nthe query language ℓ:\nRlang(y | ℓ) =\n\u001a1\nif the language of y matches ℓ\n0\notherwise.\n(2)\nFormat Reward. A parser checks compliance with the required structure (<thinking>, numbered <step n>, and\n<answer> tags):\nRfmt(y) =\n\u001a1\nif the required format is followed\n0\notherwise.\n(3)\nThe final composite reward is defined as:\nR(y | x, y∗, ℓ) = λaccRacc(y | x, y∗) + λlangRlang(y | ℓ) + λfmtRfmt(y)\n(4)\n4\n"}, {"page": 5, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nExample: Baseline vs. CURE-Med (Spanish)\nQuestion. Un paciente presenta congestión nasal y tos leve desde hace dos días. No tiene fiebre ni dificultad para respirar. ¿Cuál es la causa más probable?\nBaseline model (incorrect)\nReasoning (flawed). El cuadro parece un resfriado común, pero la ausencia de\nfiebre podría indicar que no es viral y la tos podría ser señal de algo más serio\ncomo una infección pulmonar temprana. La congestión nasal podría ser un\nsíntoma inicial de una patología más grave.\nAnswer. Podría tratarse de una infección pulmonar temprana. ✗\nCURE-Med (correct)\nReasoning (code-switched CoT). <step1> The symptoms are mild, lo que\ncoincide con un resfriado leve. </step1> <step2> No fever, lo que\nreduce la probabilidad de neumonía. </step2> <step3> Lo más\nprobable es un resfriado viral leve. </step3>\nAnswer. Lo más probable es un resfriado viral leve. ✓\nFigure 3: Qualitative Spanish medical-reasoning example comparing a baseline Qwen2.5-7B-Instruct model and\nCURE-MED-7B. The baseline model produces fluent but clinically flawed reasoning (red) and an incorrect diagnosis,\nwhereas CURE-MED generates a structured, code-switched CoT (blue) and arrives at the correct diagnosis (green).\n3.4\nGRPO-guided curriculum reinforcement learning\nAfter SFT, we fine-tune the model with curriculum-guided GRPO [34, 50] for optimizing the reasoning policy under\nthe multilingual verifier-driven reward described in Sec. 3.3.\nCurriculum Design. We design the curriculum around language resource availability rather than problem complexity.\nThis is motivated by the observation that models achieve higher reasoning accuracy in high-resource languages,\nproviding more stable reward signals early in reinforcement learning. We therefore treat languages as tasks of increasing\ndifficulty and progress from high→medium→low-resource tiers. Based on baseline performance, we define three tiers:\nhigh- (French, Japanese, Spanish, Vietnamese), medium- (Korean, Thai, Turkish, Bengali), and low-resource (Amharic,\nYoruba, Hausa, Hindi, Swahili). We start GRPO on the high-resource and progressively expand training to lower-\nresource tiers. To reduce catastrophic forgetting, we retain a fixed fraction of samples from the previous phase when\nintroducing a new tier. Formally, curriculum phase Ci draws samples from languages in tier Li ∈{high, medium, low}.\nTraining Procedure. While following prior works [34, 50, 51], we apply GRPO without modifying the optimization\nrule, the training was designed in curriculum phases. When reward improvements plateau within a tier, we expand\nsampling to include the next tier while mixing in data from the previous phase to preserve earlier capabilities. At phase\ni, we sample batches from: Di = α Di−1 + (1 −α) DLi, where DLi denotes data from tier Li, Di−1 is the retained\ndata from phase i −1, and α=0.85 controls the retention ratio. This retention-aware curriculum supports incremental\ntransfer to low-resource languages while maintaining performance.\n4\nExperiments\nNext, we outline the experimental setup, baseline models, training and evaluation procedures used to address key\nresearch questions: RQ1) Does CURE-MED improve multilingual medical reasoning over instruction-tuned baselines\nand their vanilla variants? RQ2) What is the performance trade-off between language fidelity and medical reasoning\naccuracy?\nRQ3) How does curriculum-guided learning affect performance across model scales?\nRQ4) Does\nCURE-MED generalize to unseen medical questions and languages under out-of-distribution evaluation?\n4.1\nExperimental Setup\nDataset and Splits.\nAll experiments are conducted on CUREMED-BENCH, where the dataset is partitioned into\n80% train and 20% held-out test set. The train set is further divided into 80% for supervised fine-tuning and 20% for\nreinforcement fine-tuning. Dataset construction and filtering procedures are described in Sec. 3.\nBaselines. We benchmark CURE-MED against 28 baseline models comprising i) general-purpose, including Qwen\n2.5-Instruct [52], LLaMA [53], Gemma [54], Mistral [55], Apollo2 [56], and Ministral [57]; and ii) medical-specific,\nincluding MedAlpaca [58], Meditron [59], UltraMedical [60], HuatuoGPT [61], OpenBioLLM [62], BioMistral [63],\nand MMed-LLaMA [17]. All models are evaluated in a zero-shot setting across three independent runs.\nModel Training and Evaluation. We use Qwen-2.5-{1.5B,3B,7B,14B,32B} instruction-tuned models as backbones.\nTraining is performed on eight NVIDIA A100 GPUs in two stages: i) SFT on the multi-step cold-switched dataset\nfor three epochs and ii) language-resource-aware curriculum fine-tuning with GRPO. Reinforcement progresses from\nhigh- to low-resource languages, retaining 85% of data from earlier stages to mitigate catastrophic forgetting. See\nAppendix C.1 for additional details on our high-/low-resource language definition and the criteria used to assign\nlanguages to each group.\n5\n"}, {"page": 6, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nModel\nConsistency (↑)\nAccuracy (↑)\nSmall Models (≤3B)\nLLaMA-3.2-3B\n23.69±0.36\n10.41±0.38\nQwen2.5-Instruct-1.5B\n3.84±0.25\n6.20±0.24\nQwen2.5-Instruct-3B\n8.39±0.42\n10.83±0.60\nCURE-MED-Qwen2.5-1.5B\n57.60±0.65\n28.32±0.35\nCURE-MED-Qwen2.5-3B\n74.28±0.60\n42.93±0.60\nMedium Models (7–9B)\nBioMistral-7B\n7.10±0.90\n4.80±0.95\nGemma-7B\n0.37±0.25\n1.23±0.80\nMedAlpaca-7B\n3.50±0.90\n2.47±0.95\nMeditron-7B\n0.43±0.40\n2.50±1.10\nMistral-7B\n18.70±1.30\n15.23±1.20\nApollo2-7B\n25.63±1.35\n15.93±1.35\nQwen2.5-Instruct-7B\n25.44±0.36\n29.56±0.42\nLLaMA-3.1-Instruct-8B\n36.56±0.31\n18.91±0.18\nHuatuoGPT-o1-8B\n67.30±0.14\n46.86±0.09\nOpenBioLLM-Llama3-8B\n1.47±0.45\n36.62±0.72\nMMed-Llama-3-8B\n21.38±0.56\n28.09±0.62\nUltraMedical LLaMA-3-8B\n47.03±1.03\n35.29±1.10\nMinistral-8B\n46.93±0.45\n42.87±0.21\nLLaMA-3-8B\n31.58±0.12\n28.93±0.42\nGemma-9B\n23.22±1.14\n36.97±1.03\nCURE-MED-Qwen2.5-7B\n85.21±0.63\n54.35±0.50\nLarge Models (≥14B)\nMedAlpaca-13B\n0.10±0.17\n0.07±0.12\nQwen2.5-Instruct-14B\n35.57±0.38\n41.79±0.39\nQwen2.5-Instruct-32B\n41.51±0.38\n49.69±0.40\nQwen2.5-Instruct-72B\n70.73±1.10\n58.80±1.20\nLLaMA-3.1-70B\n75.68±1.01\n54.65±0.31\nLLaMA-3.3-Instruct-70B\n79.66±0.32\n60.80±0.72\nHuatuoGPT-o1-70B\n86.79±0.44\n66.67±0.24\nOpenBioLLM-Llama3-70B\n70.30±0.43\n51.22±0.41\nMeditron-70B\n0.21±0.55\n4.54±0.59\nMMed-LLaMA-3.1-70B\n26.49±0.36\n37.85±0.76\nCURE-MED-Qwen2.5-14B\n90.27±0.31\n63.74±0.43\nCURE-MED-Qwen2.5-32B\n94.96±0.40\n70.04±0.04\nTable 2: Mean results across 13 languages on 28 baseline models and CURE-MED. We observe that CURE-MED\nmodels outperform models across all parameter scales. Consistency denotes language consistency and Accuracy\ndenotes logical accuracy. Best overall results are bold, best baselines are underlined.\nFollowing Chen et al. [42], we evaluate on the held-out test set using an LLM-as-a-judge framework, with GPT-4o\nused to match each model output to the known ground-truth answer. We assess logical accuracy (LA), defined as the\nclinical accuracy of the final answer, and language consistency (LC), defined as whether the final answer is produced\nin the question’s corresponding target language. Figure 3 provides a representative Spanish example, illustrating how\ncurriculum-guided reinforcement improves accuracy while maintaining language consistency compared to a fluent\nbut incorrect baseline. See Appendix B for Additional implementation details.\n5\nResults\nHere, we report results that answer RQ1–RQ4 from Sec. 4. We compare CURE-MED to instruction-tuned baselines\nand analyze language-reasoning trade-offs, scaling under curriculum-guided reinforcement, and out-of-distribution\ngeneralization.\nRQ1) CURE-MED outperforms baselines. Table 2 compares CURE-MED to three baseline families: general-purpose\ninstruction-tuned LLMs, medical-domain instruction-tuned models, and medical-specialized LLMs. Across scales,\nCURE-MED improves both logical accuracy and target-language consistency. At ≤3B, baselines show low correctness\nand frequent language violations, while CURE-MED reaches 42.93% logical correctness and 74.28% consistency (3B).\nAt 7–9B, CURE-MED improves over the best baseline in logical correctness (54.35% vs. 46.86%) while maintaining\n85.21% consistency. At ≥14B, CURE-MED remains best, reaching 70.04% logical correctness and 94.96% consistency.\n6\n"}, {"page": 7, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nCureMed-3B\nCureMed-7B\nCureMed-14B\nCureMed-32B\nMistral-7B\nApollo2-7B\nQwen2.5-Instruct-14B\nHuatuo-o1-8B\nOpenBioLLM-8B\nMedAlpaca-13B\nQwen2.5-Instruct-32B\nHuatuo-o1-70B\nMeditron-70B\nLLaMa-70B\nQwen2.5-Instruct-72B\nCureMed-1.5B\n\u0007\n\t\u0007\n\u000b\u0007\n\r\u0007\n\u000f\u0007\n\b\u0007\u0007\n\u0012\u0013\u0019\u0016\u001e\u0013\u0016\u0015\u0003\u0011\u001a\u0019\u001c\u0017\u001c\u001d\u0015\u0019\u0014\u001f\u0003\u0005\u0004\u0006\n\u0007\n\b\u0007\n\t\u0007\n\n\u0007\n\u000b\u0007\n\f\u0007\n\r\u0007\n\u000e\u0007\n\u000f\u0007\n\u0012\u001a\u0016\u0017\u0014\u0013\u0018\u0003\u0010\u0014\u0014\u001e\u001b\u0013\u0014\u001f\u0003\u0005\u0004\u0006\nCURE-MED\nBaselines\n70B\n32B\n14B\n7B\n1.5B\nFigure 4: Trade-off performance between logical of multilingual medical reasoning models, where each point\nrepresents a model instance with bubble size reflecting model scale. Baseline and CURE-MED models are shown as\n○and ⋆, respectively. CURE-MED shifts performance toward the upper-right, indicating consistent gains in language\nconsistency and logical accuracy.\n1.5B\n3B\n7B\n14B\n32B\n0\n20\n40\n60\n80\n100\nLanguage Consistency(%)\n4.0%\n10.8%\n25.4%\n35.6%\n41.5%\n57.6%\n74.3%\n85.2%\n90.3%\n95.0%\n1.5B\n3B\n7B\n14B\n32B\n0\n20\n40\n60\n80\n100\nLogical Accuracy(%)\n6.3%\n8.2%\n29.6%\n41.8%\n49.7%\n28.3%\n42.9%\n54.4%\n63.7%\n70.0%\nBase\nCURE-Med (ours)\nFigure 5: Scaling performance of CURE-MED vs. base across Qwen2.5-Instruct variants on language consistency (left)\nand logical accuracy (right). Our method (solid red line) consistently outperforms the base model (dashed blue line),\nwith performance gaps widening at larger model scales, highlighting the effectiveness of CURE-MED for multilingual\nmedical reasoning.\nNotably, our 32B model is competitive with closed-source systems and outperforms several proprietary models on\nCUREMED-BENCH (See Appendix E.3, E.4; Tables 8, 9, 10).\nRQ2) CURE-MED achieves better language and reasoning trade-offs. Figure. 4 shows that while baselines exhibit\na weak trade-off between language consistency and logical correctness, CURE-MED shifts this in the upper-right corner,\nhighlighting that CURE-MED improves medical reasoning without sacrificing target-language fidelity, addressing a key\nfailure mode of prior multilingual medical systems. We observe that CURE-MED-1.5B outperform several baselines\nranging from 7B to 70B and our CURE-MED-32B model outperform all 28 baseline models.\nRQ3) Scaling Trends of CURE-MED.\nFig. 5 shows that CURE-MED smoothly scale language consistency\n(57.6%@1.5B →95.0%@32B) and logical correctness (28.3%→70.0%). By comparison, instruction-tuned baselines\nexhibit only modest gains in language consistency as scale increases, remaining unreliable even at larger scale.\nTables 6-7 in App. E report per-language results, showing that CURE-MED consistently improves performance across\nlanguages and scales effectively. These trends indicate that curriculum-guided reinforcement fundamentally alters\nscaling behavior by coupling reasoning optimization with language fidelity.\n7\n"}, {"page": 8, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nRQ4) Out-of-distribution cross-lingual generalization. We evaluate transfer to held-out medical benchmarks:\nMMedBench [17], MedExpQA [64], and MedQA [65]. Across all three benchmarks, CURE-MED improves accuracy\nover the Qwen2.5 backbones in the majority of language–scale settings, with the clearest gains for smaller models.\nOn MMedBench (Table 4), the 1.5B backbone increases from 6.00→24.00 and from 20.00→57.50 on representative\nlanguages, demonstrating strong transfer under limited capacity. MedExpQA (Table 11) shows a similar large jump\nat 1.5B, rising from 1.40→44.80, while MedQA (Table 12) improves from 21.00→59.50 at 1.5B on Chinese variants.\nThese gains remain at larger scales, indicating that curriculum-guided RL transfers beyond in-domain training to\nunseen questions and language variants.\nModel size\nBase\nNaïve SFT CURE-MED (w/o RL) Naïve RFT CURE-MED (w/ RL)\nQwen2.5-Instruct — Language consistency (↑)\n1.5B\n3.84±0.25\n8.60±1.23\n53.67±0.38 (+45.07)\n8.81±0.34\n57.60±0.65 (+48.79)\n3B\n8.39±0.42 13.07±0.33\n72.68±0.38 (+59.61)\n13.28±0.57 74.28±0.60 (+61.00)\n7B\n25.44±0.36 37.11±0.44\n83.46±0.36 (+46.35)\n38.99±0.68 85.21±0.63 (+46.22)\n14B\n35.57±0.38 37.20±0.33\n84.28±0.35 (+47.08)\n39.10±1.05 90.27±0.31 (+51.17)\n32B\n35.57±0.38 43.00±0.27\n90.29±0.21 (+47.29)\n45.10±1.12 94.96±0.40 (+49.86)\nQwen2.5-Instruct — Logic accuracy (↑)\n1.5B\n6.20±0.24\n4.61±0.36\n22.97±0.57 (+18.36)\n8.80±0.47\n28.32±0.35 (+19.52)\n3B\n10.83±0.60 9.50±0.38\n39.13±0.53 (+29.63)\n10.06±0.45 42.93±0.60 (+32.87)\n7B\n29.56±0.42 30.05±1.10\n50.03±0.48 (+19.98)\n38.50±0.38 54.35±0.50 (+15.85)\n14B\n41.79±0.39 43.10±0.13\n61.91±0.45 (+18.81)\n45.20±0.55 63.74±0.43 (+18.54)\n32B\n49.69±0.40 51.21±0.15\n66.34±0.43 (+15.13)\n53.40±0.49 70.04±0.04 (+16.64)\nTable 3: Ablation study of CURE-MED. Results are averaged over three runs and reported as mean ± standard\ndeviation and green columns denote CURE-MED variants.\nModel\nFrench\nJapanese\nRussian\nSpanish\nQwen2.5-1.5B\n6.00\n11.06\n20.00\n20.00\n→\n|\nCURE-MED\n24.00\n35.18\n57.50\n44.50\nQwen2.5-3B\n6.50\n24.62\n22.50\n23.00\n→\n|\nCURE-MED\n42.00\n37.69\n60.50\n56.00\nQwen2.5-7B\n42.00\n51.76\n53.50\n63.00\n→\n|\nCURE-MED\n50.00\n46.73\n66.00\n64.00\nQwen2.5-14B\n61.00\n57.29\n63.00\n71.50\n→\n|\nCURE-MED\n64.00\n65.83\n75.50\n78.00\nQwen2.5-32B\n69.50\n67.84\n72.00\n29.50\n→\n|\nCURE-MED\n78.50\n77.29\n80.00\n82.50\nTable 4: OOD accuracy on MMedBench. CURE-MED improves reasoning performance across all model sizes,\nshowing strong cross-lingual generalization to unseen medical questions and languages. See Tables 11-12 for results on\nMedExpQA and MedQA datasets.\n6\nAblation Study\nHere, we ablate CURE-MED’s key components and measure their impact on logical accuracy. We also assess robustness\nby evaluating CURE-MED across multiple multilingual medical QA benchmarks and strong medical-domain LLM\nbaselines.\nEffect of Codeswitched Supervised Fine-Tuning. We isolate the effect of code-switched supervision during SFT\nby contrasting the base model, naïve SFT trained on multilingual long-CoT data, and CURE-MED SFT without\nreinforcement learning. Naïve SFT yields small and sometimes unstable improvements: language consistency rises\nfrom 8.39%→13.07% at 3B, yet logic accuracy decreases from 10.83%→9.50%, indicating that multilingual instruction\ntuning does not consistently strengthen medical reasoning as shown in Table 3. In contrast, code-switched SFT in\nCURE-MED produces large, consistent gains across model scales. At 1.5B, language consistency increases from\n3.84%→53.67% and logic accuracy from 6.20%→22.97%. These improvements persist as scale increases, reaching\n90.29% language consistency and 66.34% logic accuracy at 32B. In summary, the results show that structured\n8\n"}, {"page": 9, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\ncode-switching during SFT drives the strongest gains, while naïve multilingual SFT remains insufficient for reliable\nmultilingual medical reasoning.\nEffect of GRPO-guided curriculum reinforcement learning. We assess whether RL adds value beyond SFT by\ncomparing naïve single stage GRPO based RFT against the curriculum and language resource-aware RL used in\nCURE-MED, with results summarized in Table 3. Naïve RFT yields limited and uneven gains, especially at smaller\nscales, suggesting that uniform reinforcement signals do not consistently shape multilingual behavior. In contrast,\nCURE-MED applies RL after code switched SFT and delivers reliable improvements in both language consistency\nand logical accuracy across all model sizes. These results show that curriculum and resource-aware RL stabilizes\noptimization and strengthens multilingual medical reasoning beyond naïve GRPO.\nCURE-MED vs. Medical LLM baselines across Benchmarks. We evaluate CURE-MED against strong medical-\ndomain LLM baselines across four multilingual medical benchmarks (see Fig. 6). CURE-MED remains consistent,\nwith CURE-MED-32B achieving the best performance on CUREMED-BENCH (70.04%) and MMed-Bench (79.57%),\nand remains competitive on MedQA and MedExpQA, where HuatuoGPT-70B leads narrowly. CURE-MED-14B also\nprovides strong results across all benchmarks, while other medical baselines lag behind more substantially, highlighting\nCURE-MED’s robustness across diverse evaluation settings.\n\u0012\u001e\u001c\u0014\u0018\u0014\u0013\u0007\u0011\"% #\n\u0018\"!\u001b\u0010\n\u0018\"!\u0014+'\u001b\u0010\n\u0018\u0018\"!\u0007\u0011\"% #\n\b\n\n\b\n\f\b\n\r\b\n\u0010  *(\u001f ,\u0003\u0005\u0004\u0006\n\u0018\"!$)(&%\u0007\u000e\b\u0011\n\u0018\u0018\"!\u0007\u0017\u0017\u001f\u0018\u0010\u0007\u000e\b\u0011\n\u0019'\"%\u0011$&\u0017\u0017\u0018\u0007\u000e\b\u0011\n\u0016*\u001f)*&\u0015\u001a\u001d\u0007\u000e\b\u0011\n\u0012\u001e\u001c\u0014\u0007\u0018\u0014\u0013\u0007\t\f\u0011\n\u0012\u001e\u001c\u0014\u0007\u0018\u0014\u0013\u0007\u000b\n\u0011\nFigure 6: CURE-MED vs. medical LLM baselines across four multilingual medical QA benchmarks. Results show logi\ncal accuracy, highlighting CURE-MED’s consistent across diverse evaluation settings.\n7\nConclusion\nWe introduce CUREMED-BENCH, a multilingual medical reasoning benchmark of open-ended questions with explicit\nreasoning traces and a single verifiable answer across 13 languages, including low-resource settings. Using CUREMED-\nBENCH, we propose CURE-MED, which combines cold-start code-switched initialization, structured supervised\nfine-tuning, and language-resource-aware curriculum-RL to improve reasoning while preserving target-language fidelity.\nAcross languages, datasets, and model scales, CURE-MED improves logical correctness and language consistency\nover strong baselines; ablations show supervised and RL stages provide complementary gains for stable multilingual\nreasoning.\n8\nLimitations\nCUREMED-BENCH is constrained by the availability of clinically reliable source material across languages, which\nlimits coverage and can create uneven difficulty between high- and low-resource settings. Our benchmark targets\nopen-ended questions with a single verifiable answer and thus does not capture longitudinal care trajectories, multi-visit\ndecision-making, or multimodal clinical evidence. In addition, parts of our pipeline rely on API-based models (e.g., for\ngeneration and/or verification), which can be costly and may hinder reproducibility for some researchers; a practical\ndirection is to replace these components with smaller open-source models trained for the same roles and to release\nprompts, code, and verifier alternatives to reduce dependence on paid APIs. Future work will expand language coverage,\nbroaden clinical settings and modalities, and further reduce reliance on proprietary APIs.\n9\n"}, {"page": 10, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\n9\nEthical Considerations\nThis work supports the evaluation and training of multilingual medical reasoning systems by measuring reasoning\ncorrectness and target-language fidelity across diverse languages. CUREMED-BENCH is derived from publicly available,\nclinically curated sources and contains no patient records or personally identifiable information. Native speakers and\nmedical experts reviewed all samples for clinical correctness, linguistic fidelity, and cultural appropriateness under\nIRB-approved procedures, and we report per-language results to surface reliability differences across resource levels.\nAcknowledgements\nThe authors thank all members of the Aikyam Lab for their insightful discussions and valuable feedback. We also thank\nthe native speakers and medical experts across the 13 languages studied in this work for their support with the data\nverification procedures. C.A. is supported, in part, by grants from Capital One, LaCross Institute for Ethical AI in\nBusiness, the UVA Environmental Institute, OpenAI Researcher Program, Thinking Machine’s Tinker Research Grant,\nand Cohere. The views expressed are those of the authors and do not reflect the official policy or the position of the\nfunding agencies.\nReferences\n[1] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James\nKeeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378\n(6624):1092–1097, 2022. 1\n[2] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,\nChenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.\n[3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 2\n[4] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. arXiv preprint\narXiv:2212.10403, 2022. 1, 2\n[5] Farah Magrabi, Elske Ammenwerth, Jytte Brender McNair, Nicolet F De Keizer, Hannele Hyppönen, Pirkko\nNykänen, Michael Rigby, Philip J Scott, Tuulikki Vehko, Zoie Shui-Yee Wong, et al. Artificial intelligence in\nclinical decision support: challenges for evaluating ai and practical implications. Yearbook of medical informatics,\n28(01):128–134, 2019. 1\n[6] William W Stead. Clinical implications and challenges of artificial intelligence and deep learning. Jama, 320(11):\n1107–1108, 2018. 1\n[7] Vimla L Patel, José F Arocha, and Jiajie Zhang. Thinking and reasoning in medicine. The Cambridge handbook\nof thinking and reasoning, 14:727–750, 2005. 1\n[8] Jose F Arocha, Dongwen Wang, and Vimla L Patel. Identifying reasoning strategies in medical decision making:\na methodological guide. Journal of biomedical informatics, 38(2):154–171, 2005. 1\n[9] Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can large language\nmodels reason about medical questions? Patterns, 5(3), 2024. 1\n[10] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark,\nStephen R Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language\nmodels. Nature Medicine, 31(3):943–950, 2025. 2\n[11] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on\nmedical challenge problems. arXiv preprint arXiv:2303.13375, 2023. 1, 2\n[12] Samuel Cahyawijaya, Holy Lovenia, and Pascale Fung. Llms are few-shot in-context low-resource language\nlearners. arXiv preprint arXiv:2403.16512, 2024. 1, 2\n[13] Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty, and Lidong Bing. Democratizing llms for low-resource\nlanguages by leveraging their english dominant abilities with linguistically-diverse prompts. arXiv preprint\narXiv:2306.11372, 2023. 1, 2\n[14] Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, Vince I Madai, and Precise4Q Consortium.\nExplainability for artificial intelligence in healthcare: a multidisciplinary perspective. BMC medical informatics\nand decision making, 20(1):310, 2020. 1, 2\n10\n"}, {"page": 11, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\n[15] Lei Liu, Xiaoyan Yang, Junchi Lei, Yue Shen, Jian Wang, Peng Wei, Zhixuan Chu, Zhan Qin, and Kui Ren. A\nsurvey on medical large language models: Technology, application, trustworthiness, and future directions. arXiv\npreprint arXiv:2406.03712, 2024. 1, 2\n[16] Zhang Shengyu, Dong Linfeng, Li Xiaoya, Zhang Sen, Sun Xiaofei, Wang Shuhe, Li Jiwei, Runyi Hu, Zhang\nTianwei, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792,\n2023. 1\n[17] Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and\nWeidi Xie. Towards building multilingual language model for medicine. Nature Communications, 15(1):8384,\n2024. 1, 2, 5, 8\n[18] Quan Guo, Shuai Cao, and Zhang Yi. A medical question answering system using large language models and\nknowledge graphs. International Journal of Intelligent Systems, 37(11):8548–8564, 2022. 2\n[19] Akash Ghosh, Debayan Dutta, Sriparna Saha, and Chirag Agarwal. A survey of multilingual reasoning in language\nmodels. Findings of the Association for Computational Linguistics: EMNLP, 2025:8920–8936, 2025. 2\n[20] Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering\nand explaining challenging medical questions. In Proceedings of the 2025 Conference of the Nations of the\nAmericas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1:\nLong Papers), pages 3563–3599, 2025. 2\n[21] Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei,\nJason Eshraghian, Peter Abadir, and Rama Chellappa. Addressing cognitive bias in medical language models.\narXiv preprint arXiv:2402.08113, 2024. 2\n[22] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv\npreprint arXiv:2210.03057, 2022. 2\n[23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models\nare zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022. 2\n[24] Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. Breaking language barriers in\nmultilingual mathematical reasoning: Insights and observations. arXiv preprint arXiv:2310.20246, 2023. 2\n[25] Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. Mapo: Ad-\nvancing multilingual reasoning through multilingual alignment-as-preference optimization.\narXiv preprint\narXiv:2401.06838, 2024. 2\n[26] Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindˇrich Libovick`y, Constantin A Rothkopf, Alexan-\nder Fraser, and Kristian Kersting. Speaking multiple languages affects the moral bias of language models. arXiv\npreprint arXiv:2211.07733, 2022. 2\n[27] Haneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, and Hwaran Lee. Code-switching curriculum learning for\nmultilingual transfer in llms. arXiv preprint arXiv:2411.02460, 2024.\n[28] Yubin Ge, Devamanyu Hazarika, Yang Liu, and Mahdi Namazifar. Supervised fine-tuning of large language\nmodels on human demonstrations through the lens of memorization. 2023.\n[29] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe\nYuan, and Pengfei Liu. O1 replication journey–part 2: Surpassing o1-preview through simple distillation, big\nprogress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024.\n[30] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning.\narXiv preprint arXiv:2502.03387, 2025. 2\n[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human\nfeedback. Advances in neural information processing systems, 35:27730–27744, 2022. 2\n[32] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[33] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.\n[34] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948, 2025. 5\n11\n"}, {"page": 12, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\n[35] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with\nreinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. 2\n[36] Aaron Parisi, Yao Zhao, and Noah Fiedel.\nTalm: Tool augmented language models.\narXiv preprint\narXiv:2205.12255, 2022. 3\n[37] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.\n[38] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\nYu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:\n55006–55021, 2023.\n[39] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunk-\numar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022.\n[40] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.\nAdvances in Neural Information Processing Systems, 36:68539–68551, 2023.\n[41] Akash Ghosh, Srivarshinee Sridhar, Raghav Kaushik Ravi, Muhsin Muhsin, Sriparna Saha, and Chirag Agarwal.\nClinic: Evaluating multilingual trustworthiness in language models for healthcare. arXiv, 2025. 3\n[42] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou\nWang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. 3,\n6, 13\n[43] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai\nDang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 3\n[44] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024. 3\n[45] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi\nChandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models\nfor language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, pages\n1755–1797, 2025. 4\n[46] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky,\nMinjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with a\npanel of diverse models. arXiv preprint arXiv:2404.18796, 2024. 4\n[47] Hritik Bansal, John Dang, and Aditya Grover. Peering through preferences: Unraveling feedback acquisition for\naligning large language models. arXiv preprint arXiv:2308.15812, 2023. 4\n[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural\ninformation processing systems, 36:46595–46623, 2023. 4\n[49] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward\nbridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. 4\n[50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\narXiv preprint arXiv:2402.03300, 2024. 5\n[51] Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, and\nPaul Pu Liang. Learn globally, speak locally: Bridging the gaps in multilingual reasoning. arXiv preprint\narXiv:2507.05418, 2025. 5, 16\n[52] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei\nHuang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu,\nRui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint\narXiv:2412.15115, 2024. 5\n[53] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages\narXiv–2407, 2024. 5\n12\n"}, {"page": 13, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\n[54] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language\nmodels at a practical size. arXiv preprint arXiv:2408.00118, 2024. 5\n[55] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\nMistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. 5\n[56] Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, and Benyou Wang. Efficiently democratizing\nmedical llms for 50 languages via a mixture of language family experts, 2024. URL https://arxiv.org/abs/\n2410.10626. 5\n[57] Mistral AI Team. Un ministral, des ministraux, October 2024. URL https://mistral.ai/news/ministraux.\nAccessed: 2025-12-24. 5\n[58] Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexei Figueroa,\nAlexander Löser, Daniel Truhn, and Keno K. Bressem. Medalpaca – an open-source collection of medical\nconversational ai models and training data, 2025. URL https://arxiv.org/abs/2304.08247. 5\n[59] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi,\nMatteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical\npretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. 5\n[60] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui,\nBiqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. Advances in Neural\nInformation Processing Systems, 37:26045–26081, 2024. 5\n[61] H Zhang, J Chen, F Jiang, F Yu, Z Chen, J Li, G Chen, X Wu, Z Zhang, Q Xiao, et al. Huatuogpt, towards taming\nlanguage model to be a doctor. arxiv (2023). arXiv preprint arXiv:2305.15075. 5, 16\n[62] Saama AI Labs. Openbiollm: Llama3-based biomedical large language model. https://huggingface.co/\naaditya/Llama3-OpenBioLLM-70B, 2024. Model card. Paper in preparation. 5\n[63] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour.\nBiomistral: A collection of open-source pretrained large language models for medical domains, 2024. URL\nhttps://arxiv.org/abs/2402.10373. 5\n[64] Iñigo Alonso, Maite Oronoz, and Rodrigo Agerri. Medexpqa: Multilingual benchmarking of large language\nmodels for medical question answering. Artificial intelligence in medicine, 155:102938, 2024. 8\n[65] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this\npatient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11\n(14):6421, 2021. 8\nAppendix\nA\nLLM-as-a-Judge Verification Protocol\nInspired by [42], We employ an LLM-as-a-judge framework to automatically evaluate the correctness of model-\ngenerated responses. In this setup, GPT-4o acts as a verifier that compares a model’s prediction against a reference\nanswer and determines whether the response is logically correct and linguistically valid. The verifier outputs a binary\ndecision, returning True when the response aligns with the reference and False otherwise. Fig. 7 shows the exact\nprompt used for verification.\nB\nTraining and Verification Protocols\nThis section documents the prompts, reward verification procedures, and training hyperparameters used for supervised\nand reinforcement fine-tuning. Together, these components define the optimization signals and structured supervision\nunderlying the proposed framework.\n13\n"}, {"page": 14, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nPrompt for the LLM-as-a-Judge Evaluator\n<Model Response>\n{Model Response}\n</Model Response>\n<Reference Answer>\n{Ground-truth Answer}\n</Reference Answer>\nYou are given a model-generated response and a reference answer. Determine whether the model response is correct with respect\nto the reference. Output \"True\" if the response is correct and \"False\" otherwise.\nFigure 7: Prompt used for LLM-as-a-judge verification.\nB.1\nReward Verification and Weighting\nWe design a composite reward that jointly enforces clinical correctness, language fidelity, and output format compliance.\nThe final reward is defined as\nR = 0.65 × Raccuracy + 0.30 × Rlanguage + 0.05 × Rformat.\nThis weighting prioritizes medical correctness while explicitly penalizing language drift and format violations.\nB.2\nVerifier Models and Prompts\nBoth correctness and language rewards are scored using gpt-4.1 with temperature=0.0 and max_tokens=10. For each\nprompt, we generate 16 candidate responses to estimate stable reward signals.\nB.3\nAccuracy Verifier.\nYou are an expert multilingual medical evaluator. Score the generated response for correctness and medical validity on a\ncontinuous scale from 0.0 to 1.0. Give 1.0 if the reasoning is clinically sound and semantically correct, even if phrased differently\nfrom the reference. Focus on factual and clinical accuracy rather than wording.\nQuestion: {question}\nGround truth answer: {ground_truth}\nGenerated response: {generated}\nOutput only a float between 0.0 and 1.0.\nB.3.1\nLanguage Consistency Verifier Prompt\nYou are an expert multilingual medical evaluator. Determine whether the model response is written entirely in the same language\nas the question.\nQuestion language: {language}\nGenerated response: {generated}\nOutput 1.0 if the language matches exactly; otherwise output 0.0.\nB.3.2\nFormat Reward\nWe apply a deterministic rule-based check requiring exactly one <thinking> block and one <answer> block, imple-\nmented using regular expressions with re.DOTALL. This constraint ensures consistent structure during reinforcement\nlearning.\nB.4\nTraining Hyperparameters\nB.4.1\nSupervised Fine-Tuning.\n• Optimizer: AdamW (β1 = 0.9, β2 = 0.999)\n• Learning rate: 1 × 10−5 (cosine scheduler, 10% warmup)\n14\n"}, {"page": 15, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\n0\n2\n4\n6\n8\n10\n12\n14\n16\nDistribution (%)\nBengali\nTurkish\nSpanish\nHindi\nVietnamese\nThai\nAmharic\nKorean\nSwahili\nJapanese\nYoruba\nHausa\nFrench\n2.9%\n3.5%\n6.1%\n7.0%\n7.0%\n7.0%\n7.2%\n8.1%\n8.7%\n9.4%\n9.7%\n9.9%\n13.5%\nLanguage Family\nLanguages\nAfroasiatic\nAmharic (Am), Hausa (Ha)\nNiger–Congo\nSwahili (Sw), Yoruba (Yo)\nIndo-European\nBengali (Bn), Hindi (Hi), French\n(Fr), Spanish (Es)\nTurkic\nTurkish (Tr)\nAustroasiatic\nVietnamese (Vi)\nTai–Kadai\nThai (Th)\nJaponic\nJapanese (Ja)\nKoreanic\nKorean (Ko)\nFigure 8: Language and family composition of CUREMED-BENCH. Left: Number of dataset instances per language\nacross the 13 languages. Right: Assignment of languages to eight language families with standard abbreviations.\n• Epochs: 3\n• Effective batch size: 32\n• Max sequence length: 4096\n• Precision: bf16\n• Optimization: DeepSpeed ZeRO-3 with gradient checkpointing\nB.4.2\nReinforcement Fine-Tuning.\n• Algorithm: GRPO\n• Learning rate: 1 × 10−6 (cosine scheduler, warmup ratio 0.1)\n• Weight decay: 0.1\n• Effective batch size: 16\n• Generations per prompt: 16\n• Max training steps: 500\n• Max prompt / completion length: 1024 / 1024\nC\nDataset Details\nThis appendix characterizes the linguistic composition of CUREMED-BENCH. Figure 8 shows the per-language instance\ndistribution, with French contributing the largest share (13.5%) and Bengali the smallest (2.9%), and most languages\noccupying a mid-range band of roughly 7–10% of the data. The figure also groups the 13 languages into eight language\nfamilies, spanning Afroasiatic and Niger–Congo as well as Indo–European, Turkic, Austroasiatic, Tai–Kadai, Japonic,\nand Koreanic. Together, these statistics highlight both the dataset’s uneven language coverage and its broad typological\ndiversity.\nC.1\nLanguage-based Curriculum Tiers\nWe construct our curriculum by defining difficulty along the linguistic axis rather than by question complexity. To\noperationalize this design, we use Qwen2.5-14B-Instruct as a reference model and estimate baseline reasoning accuracy\nseparately for each language. The model performs best on high-resource languages and degrades as linguistic resources\nand model familiarity decrease, so we treat high-resource languages as easier tasks and progressively introduce more\nchallenging languages during training. This curriculum aims to transfer reasoning competence learned in high-resource\nsettings to underrepresented languages while maintaining language fidelity.\nBased on the baseline accuracy ranking, we partition languages into three tiers. The high-resource tier includes French,\nJapanese, Spanish, and Vietnamese. The medium-resource tier includes Korean, Thai, Turkish, and Bengali. The\nlow-resource tier includes Amharic, Yoruba, Hausa, Hindi, and Swahili. This tiering reflects the reference model’s initial\nproficiency distribution and provides a structured progression from easier to harder multilingual reasoning conditions.\n15\n"}, {"page": 16, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nD\nData Curation\nThe following prompt was used to generate the initial pool of medically grounded multiple-choice questions across\n13 languages. Inspired by the approach of Hwang et al. [51] and Zhang et al. [61], we adapted their template and\ninstructed GPT-4o to query MedlinePlus directly and independently construct questions in each target language rather\nthan translating from a shared source. This ensures linguistic naturalness, cultural appropriateness, and strong domain\ngrounding across all languages.\nPrompt for Generating Multilingual Medical Multiple-Choice Questions\nTask: You are an expert medical content generator. Generate {num_questions} high-quality, medically accurate multiple-choice\nquestions (MCQs) based strictly on content from MedlinePlus by searching and curating from the website.\nYou must independently compose each question in ALL of the following languages: Amharic, Bengali, French, Hausa, Hindi,\nJapanese, Korean, Spanish, Swahili, Thai, Turkish, Vietnamese, Yoruba.\nRequirements:\n1. Medical Grounding: All information must be sourced from MedlinePlus, covering symptoms, causes, risk factors,\ndiagnostics, treatments, or prevention strategies.\n2. Independent Composition: Each language version must be originally written (not translated) using natural phrasing and\nmedically appropriate terminology for that language.\n3. Clinical Reasoning Depth: Questions must require genuine clinical reasoning beyond trivial fact recall. Each question\nshould have exactly one unambiguous correct answer.\n4. Format: 4-option MCQ (A/B/C/D) with one correct answer.\nOutput Format: Return valid JSON array:\n[\n\\{\"question_id\": \"<id>\", \"source_concept\": \"<MedlinePlus_topic>\",\n\"mcq_items\": [\\{\"language_code\": \"<lang>\", \"question\": \"<text>\",\n\"option_A\": \"<text>\", \"option_B\": \"<text>\", \"option_C\": \"<text>\", \"option_D\": \"<text>\",\n\"correct_answer\": \"<A|B|C|D>\"\\}, ...]\\}\n]\nIMPORTANT: Return ONLY valid JSON without explanations, formatting, or additional text. Ensure all special characters are\nproperly escaped.\nFigure 9: Prompt for Stage 1 multilingual MCQ generation. Here, {num_questions} specifies the number of questions\nto generate, and GPT-4o queries MedlinePlus directly to construct clinically grounded questions independently in each\nof the 13 target languages.\nD.1\nHuman Verification Protocol and Rater Instructions\nThis section documents the human verification procedures used to validate the quality of our synthetic data. We provide\nthe exact instructions used by medical professionals who assessed the clinical correctness of question-answer pairs and\nby native speakers who evaluated the language’s correctness and fidelity in the target language, as shown in Figures 10\nand 11. These materials specify the task setup, scoring rubric, and optional comment guidelines used throughout our\nverification pipeline.\nD.2\nHuman Verification Scores by Language\nWe report per-language human verification scores from two rater groups. Medical professionals score clinical\ncorrectness of each question–answer pair, while native speakers score target-language quality and fidelity. Table 5\nsummarizes both scores on a 1–5 scale, where higher values indicate better quality.\nE\nPer-Language Model Performance\nThis section provides a fine-grained analysis of multilingual medical reasoning performance broken down by language.\nWe compare CURE-MED with instruction-tuned baselines across all 13 languages in CUREMED-BENCH, enabling a\ndetailed examination of logical correctness and language consistency under diverse linguistic and resource conditions.\nThis per-language view complements aggregate results by revealing where gains are most pronounced and where\nchallenges remain.\n16\n"}, {"page": 17, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nParticipant Instructions: Verification Task\nTask Overview\nYou will review synthetically generated medical question–answer pairs based on public sources such as MedlinePlus. These\npairs are generated synthetically and do not involve real patient data. Your role is to assess medical correctness and accuracy.\nWhat You Will Do\nFor each question–answer pair:\n• Read the question and the provided answer.\n• Check for medical correctness: ensure the information is accurate, logically sound, and aligned with standard medical\nknowledge.\n• Assign a score from 1 to 5:\n– 1: Completely inaccurate or misleading.\n– 2: Mostly inaccurate with major errors.\n– 3: Partially accurate but with notable issues.\n– 4: Mostly accurate with minor issues.\n– 5: Fully accurate and reliable.\n• (Optional) Provide a brief comment if necessary (e.g., explain errors, suggest corrections, or note cultural/language\nspecifics). Comments are optional but helpful.\nYou will receive batches of 50–100 pairs via an online survey. The task takes approximately 1–2 hours and can be completed\nremotely at your convenience. You may skip any pair or stop at any time.\nFigure 10: Instructions provided to medical professional annotators for verifying clinical correctness of synthetic\nquestion–answer pairs.\nParticipant Instructions: Language Verification Task\nTask Overview\nYou will review synthetically generated medical question–answer pairs written in one of the following target languages:\nAmharic, Bengali, French, Hausa, Hindi, Japanese, Korean, Spanish, Swahili, Thai, Turkish, Vietnamese, and Yoruba. These\npairs are generated synthetically and do not include real patient data. Your role is to verify whether the question and answer are\nwritten correctly and naturally in the target language.\nWhat You Will Do\nFor each question–answer pair:\n• Read the question and the provided answer.\n• Verify language correctness and fidelity:\n– The text is in the requested target language (no switching to another language).\n– The wording is grammatical and understandable for a native speaker.\n– The phrasing is natural and appropriate for medical communication.\n– Medical terms are expressed in an acceptable way for the target language (including common loanwords, when\nappropriate).\n• Assign a score from 1 to 5:\n– 1: Not in the target language or largely unintelligible.\n– 2: Major language errors; difficult to understand.\n– 3: Understandable but with noticeable errors or unnatural phrasing.\n– 4: Mostly correct and natural with minor issues.\n– 5: Fully correct, natural, and clearly in the target language.\n• (Optional) Provide a brief comment to note issues (e.g., incorrect language, grammar problems, unnatural phrasing, or\nbetter word choices).\nYou will receive batches of 50–100 pairs via an online survey. The task takes approximately 1–2 hours and can be completed\nremotely at your convenience. You may skip any pair or stop at any time.\nFigure 11: Instructions provided to native-speaker annotators for verifying language correctness and target-language\nfidelity of synthetic question–answer pairs.\n17\n"}, {"page": 18, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nLanguage\nMedical correctness\nLanguage quality\nAmharic\n4.45\n4.45\nBengali\n4.92\n4.96\nFrench\n5.00\n5.00\nHausa\n4.96\n5.00\nHindi\n5.00\n4.92\nJapanese\n4.96\n4.96\nKorean\n5.00\n5.00\nSpanish\n5.00\n5.00\nSwahili\n5.00\n4.96\nThai\n4.70\n4.70\nTurkish\n4.60\n4.60\nVietnamese\n4.95\n4.95\nYoruba\n5.00\n5.00\nTable 5: Per-language human verification scores (1–5) from medical professionals (clinical correctness) and native\nspeakers (language quality). Higher is better.\nLanguage Logic (Base) Logic (CURE-MED)\n∆\nLang. (Base) Lang. (CURE-MED)\n∆\nAmharic\n0.95\n17.14\n+16.19\n0.00\n64.76\n+64.76\nBengali\n10.00\n60.00\n+50.00\n2.14\n91.43\n+89.29\nFrench\n67.86\n77.86\n+10.00\n71.43\n96.43\n+25.00\nHausa\n5.06\n43.04\n+37.98\n0.00\n77.22\n+77.22\nHindi\n4.48\n48.51\n+44.03\n5.97\n90.30\n+84.33\nJapanese\n68.57\n77.14\n+8.57\n60.00\n94.29\n+34.29\nKorean\n41.33\n52.00\n+10.67\n26.67\n84.00\n+57.33\nSpanish\n62.86\n72.38\n+9.52\n60.95\n96.19\n+35.24\nSwahili\n0.00\n35.71\n+35.71\n0.00\n67.14\n+67.14\nThai\n51.02\n59.18\n+8.16\n37.76\n86.73\n+48.97\nTurkish\n12.50\n43.75\n+31.25\n3.57\n75.89\n+72.32\nVietnamese\n66.67\n70.48\n+3.81\n61.90\n94.29\n+32.39\nYoruba\n0.00\n40.86\n+40.86\n0.00\n77.42\n+77.42\nTable 6: Per-language performance of Qwen2.5-7B-Instruct (Base) and the CURE-MED 7B variant on CUREMED-\nBENCH. We report logical correctness and language accuracy, along with absolute gains ∆(CURE-MED −Base).\nE.1\nPer-Language Results for Qwen2.5-7B\nTable 6 reports per-language performance for the Qwen2.5-7B-Instruct baseline and its CURE-MED variant. Across\nall 13 languages, CURE-MED substantially improves both logical accuracy and language consistency. Gains are\nespecially large in low-resource languages such as Amharic, Hausa, Swahili, and Yoruba, where the baseline frequently\nfails to produce correct or language-faithful responses. In higher-resource languages such as French, Japanese, and\nSpanish, CURE-MED yields more moderate but consistent improvements, indicating that GRPO-guided curriculum\nRL enhances reasoning robustness without degrading performance in well-resourced settings. Overall, these results\nshow that CURE-MED improves multilingual medical reasoning uniformly while significantly narrowing performance\ndisparities across languages.\nE.2\nPer-Language Results for Qwen2.5-3B\nTable 7 shows that CURE-MED consistently improves the 3B model across all evaluated languages in both logical\ncorrectness and language accuracy. The baseline 3B model exhibits extremely low performance for several languages,\nincluding Amharic, Hausa, Swahili, and Turkish, whereas the CURE-MED variant achieves large absolute gains,\noften exceeding 40–80 percentage points. Even in languages where the base model is already relatively stronger,\nsuch as French, Japanese, Spanish, and Vietnamese, CURE-MED delivers clear and reliable improvements. These\nresults demonstrate that curriculum-guided reinforcement is particularly effective for small models, enabling robust\nmultilingual medical reasoning despite limited model capacity.\n18\n"}, {"page": 19, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nLanguage Logic (Base) Logic (CURE-MED)\n∆\nLang. (Base) Lang. (CURE-MED)\n∆\nAmharic\n0.95\n14.29\n+13.34\n0.00\n40.95\n+40.95\nBengali\n2.86\n55.71\n+52.85\n0.00\n85.00\n+85.00\nFrench\n12.14\n70.71\n+58.57\n22.14\n95.71\n+73.57\nHausa\n2.53\n27.85\n+25.32\n0.00\n64.56\n+64.56\nHindi\n5.97\n28.36\n+22.39\n0.00\n83.58\n+83.58\nJapanese\n23.81\n62.86\n+39.05\n26.67\n89.52\n+62.85\nKorean\n8.00\n36.00\n+28.00\n2.67\n76.00\n+73.33\nSpanish\n17.14\n62.86\n+45.72\n23.81\n94.29\n+70.48\nSwahili\n0.00\n17.86\n+17.86\n0.00\n51.43\n+51.43\nThai\n10.20\n58.16\n+47.96\n0.00\n73.47\n+73.47\nTurkish\n1.79\n28.57\n+26.78\n0.00\n53.57\n+53.57\nVietnamese\n44.76\n69.52\n+24.76\n79.05\n80.00\n+0.95\nYoruba\n6.45\n17.20\n+10.75\n0.00\n69.89\n+69.89\nTable 7: Per-language performance of the 3B Base model and its CURE-MED variant on CUREMED-BENCH. We\nreport logical correctness and language accuracy, along with absolute gains ∆(CURE-MED −Base).\nModel\nLang. Consistency (↑) Logical Acc. (↑)\nGPT-5-nano\n69.11\n73.24\nGPT-5-mini\n75.33\n80.57\nGemini 2.5 Flash\n48.01\n54.79\nGemini 2.5 Pro\n4.33\n10.62\nClaude 3 Haiku\n93.43\n73.31\nTable 8: Inference-only performance of proprietary models on CUREMED-BENCH (averaged across 13 languages).\nE.3\nProprietary Model Performance on CUREMED-BENCH\nTable 8 summarizes inference-only performance of frontier models on CUREMED-BENCH, reporting language consis-\ntency and logical accuracy averaged over 13 languages. While some models maintain strong target-language adherence\n(e.g., Claude 3 Haiku), results reveal substantial brittleness: GPT-5-nano exhibits notably weaker language consistency,\nand the Gemini 2.5 family degrades sharply in both language control and reasoning quality (with Gemini 2.5 Pro nearly\ncollapsing). These averages also conceal larger failures in low-resource languages, where models more frequently drift\nfrom the target language and show steeper drops in logical accuracy (see Appendix E.4). Overall, CUREMED-BENCH\nexposes a reliability gap for proprietary LLMs: strong performance in some settings does not ensure robust multilingual\nreasoning or consistent target-language adherence.\nE.4\nPer-language Performance of Closed-source Models\nWe analyze proprietary models on CUREMED-BENCH at the per-language level using logical accuracy (Table 9) and\nlanguage consistency (Table 10). Even among the five evaluated systems (GPT-5-nano, GPT-5-mini, Gemini 2.5\nFlash/Pro, and Claude 3 Haiku), strong aggregate scores mask substantial cross-lingual brittleness. Across higher-\nresource languages, performance is comparatively stable: French and Spanish achieve high logical accuracy (often\n≥90%) and strong language adherence, and we observe similarly consistent behavior in Japanese, Korean, Thai, Turkish,\nand Vietnamese, where language consistency typically remains high alongside solid reasoning performance. In contrast,\nlow-resource languages expose clear failure modes. Amharic exhibits severe target-language breakdown for several\nmodels (e.g., GPT-5-nano and Gemini 2.5 Flash/Pro), where language consistency collapses despite non-trivial logical\naccuracy for some settings; Claude 3 Haiku is more robust, maintaining high language adherence and stronger accuracy.\nHausa shows a different dissociation: multiple models drift from the target language even when logical accuracy remains\nmoderate to high, indicating that medical reasoning does not imply reliable language control under inference-only\nprompting. Yoruba is the most challenging overall: language adherence is often low (notably for GPT-5 and Gemini\n2.5 variants), and logical accuracy drops sharply across models, revealing compounding failures in both reasoning\nand language control. Overall, these results underscore a persistent reliability gap in proprietary LLMs and motivate\nevaluating multilingual medical reasoning with joint measures of correctness and target-language fidelity.\n19\n"}, {"page": 20, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nLanguage\nGPT-5-nano\nGPT-5-mini\nGemini 2.5 Flash\nGemini 2.5 Pro\nClaude 3 Haiku\nAmharic\n5.71\n41.90\n24.76\n0.95\n70.48\nBengali\n65.00\n73.57\n38.57\n9.29\n62.86\nFrench\n89.29\n93.57\n75.71\n23.57\n90.71\nHausa\n78.48\n89.87\n43.04\n3.80\n55.70\nHindi\n78.36\n78.36\n62.69\n14.93\n76.12\nJapanese\n84.76\n84.76\n69.52\n13.33\n87.62\nKorean\n78.67\n80.00\n62.67\n6.67\n73.33\nSpanish\n89.52\n94.29\n74.29\n18.10\n88.57\nSwahili\n84.29\n86.43\n48.57\n7.14\n77.14\nThai\n85.71\n90.82\n64.29\n6.12\n75.51\nTurkish\n79.46\n84.82\n53.57\n6.25\n70.54\nVietnamese\n88.57\n88.57\n63.81\n18.10\n84.76\nYoruba\n35.48\n56.99\n25.81\n2.15\n25.81\nTable 9: Logical accuracy (%) of proprietary models on CUREMED-BENCH across 13 languages under inference-only\nprompting. We report accuracy against the single ground-truth answer.\nLanguage\nGPT-5-nano\nGPT-5-mini\nGemini 2.5 Flash\nGemini 2.5 Pro\nClaude 3 Haiku\nAmharic\n1.90\n24.76\n12.38\n1.90\n92.38\nBengali\n39.29\n65.71\n34.29\n1.43\n95.71\nFrench\n92.86\n98.57\n67.86\n5.00\n98.57\nHausa\n56.96\n43.04\n35.44\n1.27\n73.42\nHindi\n53.73\n73.88\n58.96\n8.21\n97.76\nJapanese\n80.00\n88.57\n56.19\n5.71\n96.19\nKorean\n92.00\n88.00\n56.00\n4.00\n97.33\nSpanish\n97.14\n98.10\n71.43\n5.71\n91.43\nSwahili\n82.86\n77.86\n32.86\n2.86\n98.57\nThai\n94.90\n88.78\n59.18\n9.18\n97.96\nTurkish\n90.18\n93.75\n52.68\n7.14\n96.43\nVietnamese\n89.52\n91.43\n60.00\n0.95\n99.05\nYoruba\n27.96\n32.26\n23.66\n2.15\n67.74\nTable 10: Language consistency (%) of proprietary models on CUREMED-BENCH across 13 languages under inference-\nonly prompting. We report the fraction of outputs that adhere to the requested target language.\nModel\nEnglish\nFrench\nItalian\nSpanish\nQwen2.5-1.5B\n1.40\n6.40\n4.80\n6.80\n→\n|\nCURE-MED\n44.80\n47.20\n24.00\n32.80\nQwen2.5-3B\n24.8\n12.00\n13.60\n13.60\n→\n|\nCURE-MED\n48.00\n50.60\n36.80\n48.80\nQwen2.5-7B\n54.40\n44.00\n34.40\n48.00\n→\n|\nCURE-MED\n53.60\n56.80\n47.20\n57.60\nQwen2.5-14B\n61.60\n54.40\n46.40\n60.00\n→\n|\nCURE-MED\n66.40\n64.40\n64.80\n68.00\nQwen2.5-32B\n72.80\n73.60\n64.80\n70.40\n→\n|\nCURE-MED\n72.20\n73.00\n72.60\n76.20\nTable 11: OOD accuracy on MedExpQA across four languages. CURE-MED improves reasoning performance across\nmodel sizes, showing cross-lingual generalization to unseen medical questions and languages.\n20\n"}, {"page": 21, "text": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning\nModel\nEnglish\nSimplified Chinese\nTraditional Chinese\nQwen2.5-1.5B\n18.50\n21.00\n16.00\n→\n|\nCURE-MED\n37.80\n59.50\n47.50\nQwen2.5-3B\n32.50\n55.00\n36.00\n→\n|\nCURE-MED\n41.00\n68.00\n54.00\nQwen2.5-7B\n50.50\n73.00\n60.00\n→\n|\nCURE-MED\n51.50\n70.00\n57.00\nQwen2.5-14B\n56.00\n80.50\n69.50\n→\n|\nCURE-MED\n59.50\n75.00\n70.00\nQwen2.5-32B\n63.00\n84.00\n71.00\n→\n|\nCURE-MED\n64.00\n81.00\n76.00\nTable 12: OOD accuracy on MedQA across English and Chinese. CURE-MED improves reasoning performance across\nmodel sizes, demonstrating robustness across unseen languages.\n21\n"}]}