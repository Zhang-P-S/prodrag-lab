{"doc_id": "arxiv:2512.14732", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.14732.pdf", "meta": {"doc_id": "arxiv:2512.14732", "source": "arxiv", "arxiv_id": "2512.14732", "title": "INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT", "authors": ["Idan Tankel", "Nir Mazor", "Rafi Brada", "Christina LeBedis", "Guy ben-Yosef"], "published": "2025-12-10T23:28:26Z", "updated": "2025-12-10T23:28:26Z", "summary": "Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.   We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.14732v1", "url_pdf": "https://arxiv.org/pdf/2512.14732.pdf", "meta_path": "data/raw/arxiv/meta/2512.14732.json", "sha256": "dbc6331b0e441dbfbf93975215345b7eb25f220c3981820e5d4ed52d126c278c", "status": "ok", "fetched_at": "2026-02-18T02:24:29.046286+00:00"}, "pages": [{"page": 1, "text": "INFORM-CT: INtegrating LLMs and VLMs\nFOR Incidental Findings Management in\nAbdominal CT\nIdan Tankel∗1, Nir Mazor†1, Rafi Brada1, Christina LeBedis2, and\nGuy Ben-Yosef1\n1GE Healthcare Technology and Innovation Center, Niskayuna,\nNY, USA\n2Boston Medical Center, Boston, MA, USA\nitankel@outlook.com\nDecember 18, 2025\nAbstract\nIncidental findings in CT scans, though often benign, can have sig-\nnificant clinical implications and should be reported following established\nguidelines. Traditional manual inspection by radiologists is time-consuming\nand variable.\nThis paper proposes a novel framework that leverages\nlarge language models (LLMs) and foundational vision-language models\n(VLMs) in a plan-and-execute agentic approach to improve the efficiency\nand precision of incidental findings detection, classification, and report-\ning for abdominal CT scans.\nGiven medical guidelines for abdominal\norgans, the process of managing incidental findings is automated through\na planner-executor framework.\nThe planner, based on LLM, generates\nPython scripts using predefined base functions, while the executor runs\nthese scripts to perform the necessary checks and detections, via VLMs,\nsegmentation models, and image processing subroutines.\nWe demonstrate the effectiveness of our approach through experiments\non a CT abdominal benchmark for three organs, in a fully automatic\nend-to-end manner. Our results show that the proposed framework out-\nperforms existing pure VLM-based approaches in terms of accuracy and\nefficiency.\nKeywords: Incidental Findings Detection, Abdominal CT,\nVision-Language Models, Planner-Executor Framework, Clinical\nGuidelines\n∗Equal contribution. Corresponding author.\n†Equal contribution.\n1\narXiv:2512.14732v1  [cs.LG]  10 Dec 2025\n"}, {"page": 2, "text": "1\nIntroduction\nIncidental findings on abdominal CT scans are common and may have impor-\ntant clinical implications. Therefore, it is crucial to report these findings in\nan actionable manner, adhering to established guidelines. Virtually every scan\nreveals incidental findings, making it essential to distinguish significant findings\nfrom background noise. This paper aims to address the clinical concern of man-\naging the overwhelming number of findings, especially in older individuals where\nincidental findings are prevalent. We propose a novel framework based on LLM\ncombined with VLM in an agent framework, particularly of a plan-and-execute\nstyle, to improve the efficiency and precision of automatic incidental findings\nanalysis in abdominal CT imaging, adhered to medical guidelines.\nTraditional methods for incidental findings detection on abdominal CT rely\non manual inspection by radiologists, which can be time-consuming and prone\nto variability [4]. In the past decade, deep learning-based medical anomaly de-\ntection has emerged as a relevant approach. These methods often aim to learn\nthe distribution of normal patterns from healthy subjects and detect anoma-\nlous ones as outliers, for instance, via autoencoders or generative adversarial\nnetworks (e.g., [22, 17, 1, 18, 2]). Other relevant models target segmentation\nor detection of specific types of incidental findings (e.g. liver mass [13]). How-\never, none of these methods propose a general-purpose approach for detecting\nmultiple incidental findings across various organs, such as in abdominal imaging.\nVision-language multimodal approaches have shown promise in enhancing\nthe detection of pathologies by leveraging both visual and textual information.\nCLIP [16] efficiently learns visual concepts from natural language supervision,\nenabling zero-shot transfer capabilities. For medical 3D inputs, CT-CLIP [9]\nand BIMVC [7] focus on chest CT volumes, pairing them with radiology text re-\nports to improve diagnostic accuracy. MERLIN [5], designed for abdominal CT,\nintegrates textual and 3D visual data to provide comprehensive insight into ab-\ndominal imaging. These models collectively advance the field of medical imaging\nby combining visual and textual information, improving zero-shot classification\ntasks without additional annotations. However, these models still struggle to\nperform complex diagnostic tasks, and as we show here, they can be signifi-\ncantly augmented when paired with LLMs and computer vision sub-routines in\nan agent-based framework.\nPlanner-executor systems automate complex tasks by generating and exe-\ncuting code based on pre-defined instructions. Recent advances in plan-and-\nexecute frameworks have paved the way for the integration of LLM-powered\nagents. These agents can plan and perform actions, enhancing the overall effi-\nciency and accuracy of task execution. The majority of computer vision work for\nsuch systems focuses on visual question answering (VQA). For example, models\nsuch as [19, 12, 8] leverage code-generation models as well as vision-language\nmodels such as CLIP into subroutines, producing results for any query by gener-\nating and executing Python code. More advanced methods integrate a planner,\nreinforcement learning agent, and reasoner for reliable reason (e.g., [11]) or use\na multi-turn conversation and feedback (e.g., [21, 14]). In the context of in-\n2\n"}, {"page": 3, "text": "cidental findings detection, such systems can ensure the adherence to clinical\nprotocols and improve the efficiency of the inspection process. To our knowl-\nedge, we are the first to apply this approach of code generation and execution for\nCT diagnosis, providing a novel and interpretable solution for medical imaging\nanalysis.\nWhile prior VQA-based approaches typically rely on a small set of base\nfunctions (e.g., object detectors, CLIP) and produce short programs, our set-\nting requires substantially more complex programs together with low-level image\nprocessing primitives (e.g., size, edge, intensity), which motivates a careful de-\nsign of the underlying plan-and-executor architecture tailored to the medical\nimaging domain.\nTo conclude, our contributions in this paper are as follows:\n(i) We are the first to propose an incidental findings pipeline for the entire\nabdominal region, based on an LLM and VLM agentic approach. This pipeline is\ngeneral, automatically created, and adheres to clinical protocols and guidelines.\n(ii) We propose a plan-and-execute program generation method, which starts\nfrom a PDF, and automatically generates and executes a robust Python program\nwith multiple visual subroutines (base functions) that predict clinical recommen-\ndations.\n(iii) We introduce a benchmark and a new method to create test examples\nfor incidental-finding recommendations, based on Abdominal-CT reports.\n2\nMethod\nThe proposed method aims to automate the management of incidental findings\non abdominal CT scans for multiple abdominal organs, based on PDFs of medi-\ncal guidelines. This entire process is performed end-to-end automatically using\nour planner-executor framework. The framework utilizes the parsed guidelines\n(stored in a JSON file) and available protocols to generate and execute the neces-\nsary code for inspection. An overview of the full pipeline is shown in Figure 1.\n2.1\nParsing Guidelines\nWe begin by parsing the medical guidelines, which often come in PDF format,\ninto decision trees that include multiple checks and detections leading to rec-\nommendations. For this parsing stage, we used LLM (GPT-4o [15]), and the\nLangChain framework [6], to analyze figures, tables, cross-references, footnotes,\nand PDF text, converting them into JSON formats applicable for later stages.\nAn example of a PDF and the parsed tree is shown in Figure 2.\nThe parsed\nJSON file contains structured information extracted from the guidelines, includ-\ning checks, detections, measurements, and recommendations. This structured\nformat allows for easy integration into the planner-executor framework.\n3\n"}, {"page": 4, "text": "Figure 1: Overview of INFORM-CT pipeline. The framework consists of three\ncomponents: (i) Dataset Processing & IF Label Extraction (see Section 3.1.1):\nstructured metadata is derived from radiology reports through LLM-based pars-\ning, enabling the extraction of organ-specific incidental findings (IFs) labels; (ii)\nGuideline Parsing & Program Synthesis (see Section 2) medical guideline are\nparsed into decision-tree structures, which are combined with a set of prede-\nfined base functions to generate an executable inspection program via a plan-\nner–executor architecture; (iii) Program Execution: The program operates on\nCT scans, invoking base functions to produce IF predictions. These predictions\nare evaluated against IFs labels to compute accuracy measures.\n2.2\nPlanner-Executor Framework\nUsing the parsed guidelines (stored in a JSON file) and available protocols, we\nimplemented a planner-executor framework:\n• Planner: The planner, a ReAct [21] agent set up on Claude 3.5 [3] (selected\nfor its strong code generation capabilities), generates a Python script using\na set of predefined base functions. It utilizes the parsed guidelines to create\nthe script.\n• Executor: The executor runs the generated Python script, triggering the\ninner base functions.\nAt the core of this framework is the code generation of a Python script\ndesigned to inspect incidental findings based on medical guidelines. The chal-\nlenge lies in the complex structure of the decision trees from Section 2.1 and\nthe variety of visual subroutines involved in this inspection. For instance, a\nsingle program might include the detection of a tumor mask, the calculation\nof its diameter (in mm), the measurement of its border thickness, tumor gray-\nlevel evaluation (in Hounsfield units), and the presence of higher-level attributes\nassessed by a CLIP classifier — all in addition to the logical options inherent\nin the Python script itself. These complex requirements demand extensions of\n4\n"}, {"page": 5, "text": "Figure 2:\nGuideline Parsing Process Demonstration. The left panel presents\na section of the original guideline document in PDF format, while the right\npanel displays the corresponding parsed JSON structure, which encapsulates\nkey nodes, verifications, and risk assessments, facilitating integration into the\nplanner-executor framework.\nexisting plan-and-execute methods into more sophisticated programs with an\nexpanded set of base functions.\nA representative example of a synthesized program derived from the ACR\nliver guidelines is shown in Algorithm 1 (Appendix A), illustrating the type of\nclinical logic produced by our planner–executor framework.\n2.2.1\nBase Functions.\nThe base functions are built on existing methods, models, and detectors for\nsegmentation and detection of CT organs, such as abdomen CT segmentation\nmodels, abdomen CLIP models, and image processing procedures. These func-\ntions include:\n• Organ Segmentation: Segmenting organs in the CT scan. Based on To-\ntalSegmentor [20], and nnUNet [10] frameworks. These include multiple\ndifferent segmentation models that cover a wide range of tasks, including\norgan and tumor segmentation in the abdomen.\n• Mass and Tumor Segmentation: Detecting and segmenting masses and\ntumors. Based on [10, 20] as well.\n• Measuring tumor diameter: An image processing procedure to measure\nthe diameter (in cm or mm) of a tumor based on a mask of pixels and\nmetadata from the CT resolution. Includes a few estimation methods.\n• Measuring gray-level intensity: An image processing procedure to measure\nthe gray-level intensity (HU) of a tumor based on a mask of pixels and\nmetadata from the CT scan file.\n• Measuring border thickness: Measuring the thickness of organ or lesion\nborders using the Hausdorff distance.\n5\n"}, {"page": 6, "text": "• Labeler: A labeler module is integrated to automate the classification of\nhigher-level fine-grained attributes using a vision-language model.\nFor\nexample, the labeler can tag a lesion as ”benign”, ”suspicious”, or ”flash-\nfilling” according to a list of sub-features.\nOur labeler is implemented\nusing the MERLIN model [5], which is currently the state-of-the-art 3D\nmodel for abdominal CT. It was trained on paired 3D CT volumes and\ncorresponding text reports, enabling it to generate accurate labels for seg-\nmented regions on these scans.\n2.2.2\nIncidental Findings Code Generation.\nCode generation models such as [8, 19] have demonstrated the successful use\nof creating programs as a description of complex decision-making and analysis\nprocesses. However, clinical detection of incidental findings in abdominal CT\nscans involves a more challenging task. This process must account for multiple\ncritical factors that are not typically used for normal images, including com-\nputation of size and grey-level intensity in specified regions, considering scan\ndetails such as contrast phase, and often incorporating patient medical history.\nThese complexities necessitate a robust and adaptable code generation approach\nto ensure accurate and efficient analysis.\nTo generate a code representation of each incidental findings management\nprocedure, we provided a detailed description of the API available for each\nbase function, along with simple examples that demonstrate their proper usage.\nIn addition, we included a comprehensive overview of the problem, the clini-\ncal pipeline, the parsed tree from Sec. 2.1 as well as other relevant details to\neffectively instruct the code generation process.\nThe generation process works in an interactive manner, involving a multi-\nturn conversation with the LLM. The steps are as follows:\n1. Initial Draft Generation: The agent generates a preliminary draft of the\nprogram out of the decision tree.\n2. Execution and Feedback: After executing the draft, the system generates\nfeedback and evaluates a STOP criterion to decide whether to continue.\nThis criterion assesses both syntactic correctness and semantic validity of\nthe generated code.\n3. Iterative Refinement: If the STOP criterion is not met, another call to the\nLLM is made to regenerate the code based on the feedback. The method\nthen returns to step 2, iterating through these steps until the final program\nis produced.\n6\n"}, {"page": 7, "text": "3\nExperiments\n3.1\nMulti-Organ Benchmark from Clinical Abdominal CT\nTo simulate the real-world management of incidental findings, where multiple\norgans need to be checked according to various guidelines (in PDFs) and adhering\nto hospital protocols, we conducted a benchmark using American College of\nRadiology (ACR) guidelines for three different organs: Liver, Pancreas, and\nKidney. The experiments are based on our internal dataset, which consists of\na large set of abdominal CT scans paired with radiology reports. This dataset\nwas used to develop a procedure for collecting test data for our method. The\ndata includes thousands of abdominal CT scans from 6,366 unique patients.\nThe scans were made in various phases, including venous and arterial phases.\nFor liver scans, we restricted our investigation to venous phase CT scans,\nallowing existing segmentation models to effectively detect lesions. To ensure\na balanced set of recommendations, we included scans both with and without\nliver lesion detections. This approach allowed us to cover the different paths in\nthe decision tree comprehensively. Consequently, we included scans that were\nconducted for liver inspection, where liver lesions are more likely to be found.\nUltimately, we gathered 168 scans, providing a good balance of the possible\ndecision tree paths.\nWe applied the same type of filtering for the pancreas and kidney.\nFor\nthe pancreas, we selected venous phase CT scans, while for the kidney, we chose\narterial phase CT scans. This approach ensured that lesions in these organs were\ndetectable by existing segmentation models, allowing us to create a balanced and\ncomprehensive dataset for each organ. Specifically, we gathered 168 scans for\nthe liver, 188 scans for the pancreas, and 98 scans for the kidney.\nTo ensure our method adhered to established clinical guidelines, we selected\nguidelines from the ACR website and collected PDFs for each organ. We then\nused the parsed tree procedure described in Section 2.1 to parse these guide-\nlines. This process involved converting the PDF text and figures into JSON\nformats, which included structured information such as checks, detections, mea-\nsurements, and recommendations.\n3.1.1\nExtracting ”correct” recommendations from reports.\nFor each scan, our method aims to generate recommendations based on guide-\nlines for the management of incidental findings in a specific organ. To evaluate\nthe predicted recommendations, we built a procedure to also obtain ”correct”\nrecommendations extracted from reports. First, we note that the radiologist’s\nreport for each scan includes detailed observations and patient background in-\nformation, which can be used to infer the recommendation and its explanation\nas reflected by a trajectory in the parsed tree. Next, we generated a list of all\npossible paths in the decision tree by traversing it, with each path representing\na sequence of checks and decisions leading to a specific recommendation. We\nthen used an LLM (GPT-4o,\n[15]) to review the radiology report and select\n7\n"}, {"page": 8, "text": "the best tree path that matches the report. This selected path is considered the\n”correct” recommendation for the scan (the leaf includes the recommendation,\nwhile the rest of the path can be considered its ”explanation”), which we use to\ntest both baseline and our model.\n3.2\nEvaluation of INFORM-CT\n3.2.1\nImplementation details.\nWe utilized the CLAUDE 3.5 LLM [3] to intelligently translate the logic de-\nscribed in the guidelines into code. For example, the guidelines might specify\nthat a mass is ”Homogeneous (thin or imperceptible wall, no mural nodule,\nsepta, or calcification).” This description needs to be converted into code that\noperates the base functions. The logic of ”or” and ”not”, as well as the compu-\ntation of attributes such as ”thin”, ”thick”, and ”calcification” are all handled\nby the base functions and orchestrated by the program. We implemented most\nof the image processing functions using standard Python libraries. For running\nMERLIN as a labeler to obtain higher-level attributes, we computed the cosine\nsimilarity of the entire 3D scan matched to a set of labels representing all po-\ntential attribute values. We were limited in this implementation by the variety\nof available strong segmentation models for abdominal organ lesions. All seg-\nmentation models were implemented using nnUNET and taken from its GitHub\nrepository [10, 20].\nAll programs were automatically generated from a PDF containing guide-\nlines. Our experiments included guidelines from the ACR, but any adjustment\nof these guidelines according to specific hospital protocols, as well as guide-\nlines from different radiology organizations (e.g., the ESR - European Society\nof Radiology), can be accommodated.\n3.2.2\nBaseline Evaluation Using MERLIN Model.\nOur model was evaluated against the MERLIN baseline [5], a Vision Language\nModel similar to CLIP. To evaluate MERLIN, we listed all the possible paths\nin the decision tree, concatenating the text in the nodes.\nWe also included\nthe patient background information (such as age and risk factor), as it was\nprovided to INFORM-CT, to ensure a fair comparison. For each decision path\ncombined with the background information, we computed the cosine similarity\nand selected the path with the highest score as the prediction of the MERLIN\nmodel.\n3.2.3\nAnalysis.\nThe results of matching the recommendation predictions of our models, as well\nas the MERLIN model (as mentioned above), are shown in Table 1.\nFigure 3\nillustrates the process, showing link to the guideline PDF, pieces from the gen-\nerated code, the execution of the code via base functions, and the execution\noutput for liver and kidney (renal) incidental findings management over two\n8\n"}, {"page": 9, "text": "Abdominal Organ\nRandom prediction\nPure MERLIN\nINFORM-CT\nLiver\n10.0 / –\n12.5 / 14.33\n63.09 / 61.38\nRenal\n16.67 / –\n48.8 / 14.0\n60.0 / 61.0\nPancreas\n7.14 / –\n11.17/ 12.96\n41.48 / 46.32\nTable 1: Accuracy (left, in percentage) and weighted F1 scores (right) for pre-\ndicting the correct recommendation in managing incidental findings across se-\nlected organs using the INFORM-CT and MERLIN models.\nsample scans. The results indicate that our method can effectively handle the\nautomatic management of incidental findings for different abdominal organs.\nThe accuracy of the final recommendation predictions is relatively high, typi-\ncally much higher than applying a pure VLM approach (”Pure Merlin”) on a\nreal-world clinical benchmark. We also evaluated the correctness of decisions\nmade along the way, namely the path in the decision tree that yielded the rec-\nommendation. This is the explainable part of our model, and this evaluation\nsheds light on how explainable the model is and how well it matches the correct\nexplanation computed from the report (as explained in Section 3.1). The re-\nsults, as shown in Table 2, indicate that the model explanation matches those\nprovided in the report for the majority of cases, while MERLIN provides limited\nexplanatory capability.\nFinally, we turn to assess the contribution of the internal components of\nthe model on performance.\nSpecifically, we evaluate the contribution of the\nsegmentation base functions through an ablation study. In Table 2, we present\nthe recommendation accuracy (in percentage) of an ablated INFORM-CT model\nin which segmentation tasks are converted to text and are also performed by\nMERLIN. Comparing this ablated model to the full INFORM-CT reveals that\nthe segmentation component is critical to the success of the model and cannot\nbe replaced by the VLM. However, the VLM and image processing routines\nare also crucial components of INFORM-CT, leading to the conclusion that the\nwhole is greater than the sum of its parts.\nExplanation Evaluation\nINFORM-CT\nPure MERLIN\nAcc\n54.76\n4.76\nRecommendation Evaluation\nFull\nAblated\nAcc\n63.09\n20.45\nTable 2:\nAdditional evaluation for the incidental finding management of the\nliver.\nThe explanatory part of the model is shown on the left, displaying\nthe accuracy of the decision trajectory for obtaining the final recommendation\nmatched to the reasons provided in the report. On the right is a comparison\nof the full and ablated model, where the segmentation base routine is removed\nand replaced by the VLM.\n9\n"}, {"page": 10, "text": "(A)\n(B)\nFigure 3:\nPredictions of the INFORM-CT model for scans, adhering to ACR\nguidelines for the management of incidental findings. Process and results are\nshown for the liver (A), renal (B). Selected tree trajectory is shown as output,\nand final recommendation is marked magenta color.\n4\nDiscussion\nIn this paper, we have demonstrated the effectiveness of the INFORM-CT agen-\ntic framework in managing incidental findings on abdominal CT scans by lever-\naging advanced LLM, VLM, and segmentation models. While INFORM-CT can\neffectively handle the logic and image processing operations needed for inciden-\ntal findings, it is still limited by the capabilities of the underlying segmentation\nand VLM models used as base functions. Specifically, we expect that a VLM\ncapable of better labeling fine details against local scan regions will further\nimprove recommendation performance.\nReferences\n[1] Akcay, S., Atapour-Abarghouei, A., Breckon, T.P.:\nGanomaly:\nSemi-\nsupervised anomaly detection via adversarial training. In:\nComputer\nVision–ACCV 2018: 14th Asian Conference on Computer Vision, Perth,\nAustralia, December 2–6, 2018, Revised Selected Papers, Part III 14. pp.\n622–637. Springer (2019)\n[2] Almeida, S.D., L¨uth, C.T., Norajitra, T., Wald, T., Nolden, M., J¨ager, P.F.,\nHeussel, C.P., Biederer, J., Weinheimer, O., Maier-Hein, K.H.: coopd: re-\nformulating copd classification on chest ct scans as anomaly detection using\ncontrastive representations. In: International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention. pp. 33–43. Springer\n(2023)\n[3] Anthropic:\nClaude 3.5 sonnet (2024), https://www.anthropic.com/\nclaude/sonnet\n10\n"}, {"page": 11, "text": "[4] Berland, L.L., Silverman, S.G., Gore, R.M., Mayo-Smith, W.W., Megibow,\nA.J., Yee, J., Brink, J.A., Baker, M.E., Federle, M.P., Foley, W.D., et al.:\nManaging incidental findings on abdominal ct: white paper of the acr in-\ncidental findings committee. Journal of the American College of Radiology\n7(10), 754–773 (2010)\n[5] Blankemeier, L., Cohen, J.P., Kumar, A., Van Veen, D., Gardezi, S.J.S.,\nPaschali, M., Chen, Z., Delbrouck, J.B., Reis, E., Truyts, C., et al.: Merlin:\nA vision language foundation model for 3d computed tomography. arXiv\npreprint arXiv:2406.06512 (2024)\n[6] Chase, H.: LangChain (Oct 2022), https://github.com/langchain-ai/\nlangchain\n[7] Chen, Y., Liu, C., Liu, X., Arcucci, R., Xiong, Z.: Bimcv-r: A landmark\ndataset for 3d ct text-image retrieval. In: International Conference on Med-\nical Image Computing and Computer-Assisted Intervention. pp. 124–134.\nSpringer (2024)\n[8] Gupta, T., Kembhavi, A.: Visual programming: Compositional visual rea-\nsoning without training. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 14953–14962 (2023)\n[9] Hamamci, I.E., Er, S., Almas, F., Simsek, A.G., Esirgun, S.N., Dogan, I.,\nDasdelen, M.F., Durugol, O.F., Wittmann, B., Amiranashvili, T., et al.:\nDeveloping generalist foundation models from a multimodal dataset for 3d\ncomputed tomography. arXiv preprint arXiv:2403.17834 (2024)\n[10] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-\nnet: a self-configuring method for deep learning-based biomedical image\nsegmentation. Nature methods 18(2), 203–211 (2021)\n[11] Ke, F., Cai, Z., Jahangard, S., Wang, W., Haghighi, P.D., Rezatofighi,\nH.: Hydra: A hyper agent for dynamic compositional visual reasoning. In:\nEuropean Conference on Computer Vision. pp. 132–149. Springer (2024)\n[12] Khan, Z., BG, V.K., Schulter, S., Fu, Y., Chandraker, M.: Self-training\nlarge language models for improved visual program synthesis with visual\nreinforcement. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 14344–14353 (2024)\n[13] Lyu, F., Xu, J., Zhu, Y., Wong, G.L.H., Yuen, P.C.: Superpixel-guided\nsegment anything model for liver tumor segmentation with couinaud seg-\nment prompt. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. pp. 678–688. Springer (2024)\n[14] Min, J., Buch, S., Nagrani, A., Cho, M., Schmid, C.: Morevqa: Exploring\nmodular reasoning models for video question answering. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 13235–13245 (2024)\n11\n"}, {"page": 12, "text": "[15] OpenAI: Gpt-4o (2023), https://www.openai.com/gpt-4o\n[16] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,\nSastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable\nvisual models from natural language supervision. In: International confer-\nence on machine learning. pp. 8748–8763. PMLR (2021)\n[17] Schlegl, T., Seeb¨ock, P., Waldstein, S.M., Langs, G., Schmidt-Erfurth, U.:\nf-anogan: Fast unsupervised anomaly detection with generative adversarial\nnetworks. Medical image analysis 54, 30–44 (2019)\n[18] Shvetsova, N., Bakker, B., Fedulova, I., Schulz, H., Dylov, D.V.: Anomaly\ndetection in medical imaging with deep perceptual autoencoders. IEEE\nAccess 9, 118571–118583 (2021)\n[19] Sur´ıs, D., Menon, S., Vondrick, C.: Vipergpt: Visual inference via python\nexecution for reasoning. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision. pp. 11888–11898 (2023)\n[20] Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter,\nA.W., Heye, T., Boll, D.T., Cyriac, J., Yang, S., et al.: Totalsegmentator:\nRobust segmentation of 104 anatomic structures in ct images. Radiology:\nArtificial Intelligence 5(5) (2023)\n[21] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.:\nReact: Synergizing reasoning and acting in language models (2023), https:\n//arxiv.org/abs/2210.03629\n[22] Zhang, Y., Lu, D., Ning, M., Wang, L., Wei, D., Zheng, Y.: A model-\nagnostic framework for universal anomaly detection of multi-organ and\nmulti-modal images. In: International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention. pp. 232–241. Springer (2023)\nAppendix\nA\nExample Generated Program\nAlgorithm 1 provides an illustrative example of the type of clinical logic exe-\ncuted by INFORM-CT. Starting from the segmented liver masses, the algorithm\ncomputes lesion-level attributes—including diameter, radiological features, and\npatient-specific risk factors—and applies the decision rules derived from the\nACR guidelines to produce per-lesion recommendations. These are then aggre-\ngated into a patient-level follow-up recommendation.\nWhile simplified for clarity, this example captures the core reasoning steps\nsynthesized automatically by the planner–executor framework, which generates\nsimilar structured programs for all organs and guideline pathways.\n12\n"}, {"page": 13, "text": "Algorithm 1: Assessment of Liver Lesions\nThis illustrative pseudocode shows the structure of a synthesized\nprogram generated by the planner–executor framework for the liver\nACR guidelines.\nInput: x: abdominal CT scan\nInput: b: patient background / clinical data\nOutput: rec: patient-level follow-up recommendation\nM ←mass segmentator(x, organ=”liver”)\nR ←[ ]\n// list of mass-level recommendations\nforeach m ∈M do\nd ←calc mass diameter cm(m, x)\n// mass diameter in cm\nr ←assess patient liver risk(b)\n// patient-level risk\n(Low / High)\nif d ≤1.0 then\nif r = Low then\nrm ←“Benign; no further follow-up.”\nelse\nrm ←“Liver MRI in 3–6 months.”\nelse if 1.0 < d ≤1.5 then\nϕm ←calc mass imaging features(m, x)\n// lesion’s\nimaging features\nif ϕm = suspicious then\nFurther logic...\nelse\nFurther other logic...\nelse\n“Further logic for large lesions...“\n// larger than 1.5 cm\nappend rm to R\nrec ←agg recommendations(R) // aggregate into patient-level\nrecommendation\nreturn rec\n13\n"}]}