{"doc_id": "arxiv:2601.09280", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.09280.pdf", "meta": {"doc_id": "arxiv:2601.09280", "source": "arxiv", "arxiv_id": "2601.09280", "title": "ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering", "authors": ["Chaerin Lee", "Sohee Park", "Hyunsik Na", "Daseon Choi"], "published": "2026-01-14T08:33:14Z", "updated": "2026-01-14T08:33:14Z", "summary": "Recent studies in medical question answering (Medical QA) have actively explored the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. We argue that the core challenge lies not in expanding access to knowledge, but in identifying and reasoning over the appropriate subset of evidence for each query. ReGraM is a region-first knowledge graph reasoning framework that addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence aware modes. By focusing inference on only the most relevant portion of the KG, ReGraM departs from the assumption that all relations are equally useful an assumption that rarely holds in domain-specific medical settings. Experiments on seven medical QA benchmarks demonstrate that ReGraM consistently outperforms a strong baseline (KGARevion), achieving an 8.04% absolute accuracy gain on MCQ, a 4.50% gain on SAQ, and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses further show that aligning region construction with hop-wise reasoning is the primary driver of these improvements. Overall, our results highlight region-first KG reasoning as an effective paradigm for improving factual accuracy and consistency in medical QA.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.09280v1", "url_pdf": "https://arxiv.org/pdf/2601.09280.pdf", "meta_path": "data/raw/arxiv/meta/2601.09280.json", "sha256": "84d19914d50525f2095d37cea27db3e660523539d5edeb2900b65bb6f82d0e02", "status": "ok", "fetched_at": "2026-02-18T02:21:34.959268+00:00"}, "pages": [{"page": 1, "text": "ReGraM: Region-First Knowledge Graph Reasoning for Medical Question\nAnswering\nChaerin Lee∗\nSohee Park†\nHyunsik Na†\nDaseon Choi‡\nDepartment of Software, Soongsil University, Seoul, Republic of Korea\n{chaerin1112, sosohi, rnrud7932}@soongsil.ac.kr\nsunchoi@ssu.ac.kr\nAbstract\nRecent studies in medical question answering\n(Medical QA) have actively explored the in-\ntegration of large language models (LLMs)\nwith biomedical knowledge graphs (KGs) to\nimprove factual accuracy. However, most exist-\ning approaches still rely on traversing the entire\nKG or performing large-scale retrieval, which\nintroduces substantial noise and leads to unsta-\nble multi-hop reasoning. We argue that the core\nchallenge lies not in expanding access to knowl-\nedge, but in identifying and reasoning over the\nappropriate subset of evidence for each query.\nReGraM is a region-first knowledge graph rea-\nsoning framework that addresses this challenge\nby constructing a query-aligned subgraph and\nperforming stepwise reasoning constrained to\nthis localized region under multiple evidence-\naware modes. By focusing inference on only\nthe most relevant portion of the KG, ReGraM\ndeparts from the assumption that all relations\nare equally useful—an assumption that rarely\nholds in domain-specific medical settings. Ex-\nperiments on seven medical QA benchmarks\ndemonstrate that ReGraM consistently outper-\nforms a strong baseline (KGARevion), achiev-\ning an 8.04% absolute accuracy gain on MCQ,\na 4.50% gain on SAQ, and a 42.9% reduction\nin hallucination rate. Ablation and qualitative\nanalyses further show that aligning region con-\nstruction with hop-wise reasoning is the pri-\nmary driver of these improvements. Overall,\nour results highlight region-first KG reasoning\nas an effective paradigm for improving factual\naccuracy and consistency in medical QA.\n1\nIntroduction\nMedical question answering (Medical QA) increas-\ningly requires multi-hop reasoning that connects\nsymptoms, mechanisms, diagnoses, treatments,\nand molecular interactions (Xie et al., 2024; Sing-\nhal et al., 2025; Wu et al., 2025).\nDespite this growing demand, most existing\nLLM-based Medical QA systems still rely on shal-\nlow, retrieval-based reasoning. Even methods de-\nsigned for multi-hop inference often lack explicit\nstructural constraints, causing reasoning chains to\ndrift across unrelated biomedical concepts. As a\nresult, clinical evaluations frequently report medi-\ncal hallucinations—plausible but unsupported state-\nments that pose serious safety risks in real-world\napplications (Saab et al., 2024; Williams et al.,\n2024). To address these issues, recent work has\nintegrated large language models with biomedical\nKGs through KG-based QA, KG-RAG, and graph-\nconstrained prompting (Zeng et al., 2025; Sui et al.,\n2025; Shi et al., 2025; Long et al., 2024). Methods\nsuch as KGARevion further apply post-hoc veri-\nfication of LLM-generated triplets against a KG\n(Jeong et al., 2024). However, most KG-based ap-\nproaches still treat the KG as a flat search space\nspanning heterogeneous biomedical subdomains\nand relation types (Chandak et al., 2023). Because\ntypical queries require only a small, task-relevant\nsubset of the KG, global traversal introduces sub-\nstantial retrieval noise that accumulates over multi-\nhop reasoning and destabilizes inference (Su et al.,\n2024; Jeong et al., 2024; Wu et al., 2025).\nWe propose a different design principle: rather\nthan exploring the entire KG prior to reasoning,\nwe first construct a semantically aligned subgraph\ntailored to the query and constrain all subsequent\nreasoning within that region. Unlike post-hoc verifi-\ncation or soft context filtering, this region is treated\nas a hard structural boundary throughout multi-\nhop inference, preventing semantically irrelevant\nrelations from intervening once reasoning begins.\nWe instantiate this principle in a framework called\nReGraM, which performs query typing, region se-\nlection, evidence-aware reasoning, and verification\nentirely within a localized KG region. This design\nenforces early restriction of the reasoning space\nand promotes factual consistency by structurally\nexcluding irrelevant relations.\nWe evaluate ReGraM on PrimeKG using the\n1\narXiv:2601.09280v1  [cs.CL]  14 Jan 2026\n"}, {"page": 2, "text": "same entity set and relation schema as KGARevion\nto ensure a controlled comparison. To improve\nregion construction, we refine the relation descrip-\ntion map used for retrieval and prompting, without\nmodifying the underlying KG.\nAlthough evaluated in the medical domain, the\nregion-first principle generalizes to other structured\ndomains with high relational complexity. Our main\ncontributions are as follows:\n1. We propose ReGraM, a region-first KG reason-\ning framework that constructs a query-aligned\nsubgraph and performs reasoning via multi-hop\ndecomposition within it.\nThis design struc-\nturally constrains the reasoning space and miti-\ngates semantic drift by excluding irrelevant rela-\ntions.\n2. We demonstrate substantial empirical gains: an\n8.04% improvement in MCQ accuracy, a 4.50%\nimprovement in SAQ accuracy, a 32–48% reduc-\ntion in hallucination rates, and a 46% reduction\nin inference latency across seven medical QA\nbenchmarks and multiple hallucination testbeds.\n3. We conduct ablation and error analyses showing\nthat region selection and multi-hop decomposi-\ntion are critical to factual consistency, and we\nvalidate the contribution of each module to over-\nall system stability.\n2\nRelated Work\n2.1\nLLM-based Reasoning and Multi-hop\nInference\nMedical question answering (MedQA) requires\nmulti-hop reasoning that connects diverse biomedi-\ncal concepts in a stepwise manner, which is chal-\nlenging for single-step retrieval-based approaches.\nTo address this challenge, a wide range of LLM-\nbased reasoning methods have been proposed, in-\ncluding Chain-of-Thought and its variants, ReAct,\nReflexion, SelfCheck, and STaR (Wei et al., 2022;\nYao et al., 2022; Shinn et al., 2023; Manakul et al.,\n2023; Zelikman et al., 2024). However, these meth-\nods typically rely on the LLM’s internal knowl-\nedge without external structural constraints, often\nleading to reasoning drift, cumulative errors, and\nfactual hallucinations in complex biomedical sce-\nnarios (Bang et al., 2023; Huang et al., 2025).\nIn particular, many MedQA tasks require con-\nnecting multiple entities across relational chains.\nLLMs alone struggle to maintain consistency\nacross these hops, highlighting the need to incorpo-\nrate external structured knowledge—such as knowl-\nedge graphs (KGs)—to provide relational ground-\ning and guide multi-hop inference (Chen et al.,\n2019; Luo et al., 2023; Sakarvadia, 2024).\n2.2\nKG-based Reasoning for Medical QA\nTo improve factual grounding, a variety of mod-\nels have explored knowledge graph-based QA in\nthe biomedical domain, including QAGNN (Ya-\nsunaga et al., 2021), JointLK (Sun et al., 2022),\nKagNet (Lin et al., 2019), MedReason (Wu et al.,\n2025), and ontology-guided KGQA (Liu et al.,\n2025). While effective in incorporating KG struc-\nture, these methods typically operate over the en-\ntire graph without explicitly tailoring the reasoning\nspace to the query context.\nIn large-scale biomedical KGs such as PrimeKG,\nfull-graph traversal introduces irrelevant relations,\npropagates retrieval noise, and fragments evidence\ndue to sparsity (Zhai et al., 2024; Mavi et al., 2023;\nChandak et al., 2023). Some recent methods ad-\ndress this issue by incorporating verification mech-\nanisms (e.g., KGARevion (Su et al., 2024)) or by\norganizing retrieval contexts using graph structure\n(e.g., GraphRAG (Xiao et al., 2025) and Med-\nGraphRAG (Wu et al., 2025)). However, these\napproaches often treat KG retrieval as a preprocess-\ning step, leaving the subsequent reasoning process\nunconstrained beyond the retrieved context. As a\nresult, retrieval noise and semantically irrelevant re-\nlations can still influence downstream reasoning, es-\npecially in multi-hop and generative settings. This\nissue is particularly pronounced in generative QA,\nwhere early retrieval errors can propagate and com-\npound across reasoning steps.\nIn contrast, our work imposes an explicit struc-\ntural boundary before reasoning begins. Rather\nthan applying KG pruning post hoc or relying on\nsoft context filtering, we define a query-specific\nsubgraph as the primary reasoning space, ensur-\ning that multi-hop inference remains structurally\ngrounded throughout the entire process.\n3\nMethod\nFigure 1 presents an overview of ReGraM. Re-\nGraM is a region-first knowledge graph reason-\ning framework that constructs a query-aligned sub-\ngraph and performs multi-hop reasoning under a\nhard region constraint.\nThis section describes the overall framework and\nits core components. Additional implementation\ndetails, prompt templates, and qualitative examples\n2\n"}, {"page": 3, "text": "Figure 1: Overview of the ReGraM framework. Given a medical query, ReGraM constructs a query-aligned local knowledge\ngraph region and performs reasoning exclusively within the selected region to generate the final answer.\nare provided in the appendix.\n3.1\nQuery Typing and Decomposition\nReGraM begins by analyzing the structure of\nthe input query to guide subsequent region\nconstruction.\nGiven an input query Q, the\nmodel first classifies it into one of five high-level\nmedical domains (GENE/PROTEIN, DRUG/THERAPY,\nDISEASE/SYMPTOM,\nPATHWAY/METABOLISM,\nor\nINTEGRATED) using an LLM-based classification\nprompt. The predicted domain serves as a semantic\nprior for adjusting relation importance during KG\nregion selection.\nThe query is then decomposed into up to three\nsub-questions using a one-shot CoT prompt, align-\ning the reasoning process with the underlying multi-\nhop structure (Appendix A and Figure 3).\n3.2\nRegion Selection\nFor each sub-question qi, ReGraM constructs a\nlocalized KG region rather than retrieving from the\nfull graph. Medical entities are extracted using the\nUMLS linker in SciSpaCy (Neumann et al., 2019;\nBodenreider, 2004) and expanded into an enriched\nentity set Eexp via UMLS synonyms, custom alias\ndictionaries curated from PrimeKG node attributes,\nand fuzzy string matching (similarity ≥90) with\nRapidFuzz (?).\nCandidate triplets Tcand connected to Eexp\nare collected from the KG and reweighted us-\ning domain-specific relation weights wr\n∈\n{0.5, 0.8, 1.0, 1.2, 1.5}, which encode the rele-\nvance of each relation type to the predicted query\ndomain. To prioritize both semantic relevance and\nevidence diversity, triplets are re-ranked using a\nweighted Maximal Marginal Relevance (MMR)\ncriterion (Carbonell and Goldstein, 1998):\nMMR(t) = λ Sim(qi, t) wr\n−(1 −λ) max\nt′∈S Sim(t, t′)\n(1)\nHere,\nSim(·, ·)\ndenotes\nthe\ncosine\nsimi-\nlarity\nbetween\nSentence-BERT\nembeddings\n(all-MiniLM-L6-v2) of the sub-question qi and\nthe textual representation of a triplet t (Reimers\nand Gurevych, 2019). The second term penalizes\nredundancy by discouraging selection of triplets\nthat are semantically similar to previously selected\nones, where S denotes the set of triplets already\nchosen for the current region. The hyperparameter\nλ = 0.7 balances relevance and diversity during\n3\n"}, {"page": 4, "text": "region construction.\nThe top-K ranked triplets (with K = 15) consti-\ntute the query-specific KG region Gq = (Vq, Eq),\nwhere Eq is the selected triplet set and Vq con-\ntains all head and tail entities appearing in Eq. All\nsubsequent knowledge access operations during\nreasoning are restricted to this region, formally\ndenoted as Lookup(qi) ⊆Gq. This restriction\nenforces region-constrained reasoning by prevent-\ning the model from accessing relations outside the\nquery-aligned subgraph (see Appendix B).\nReGraM does not modify the underlying KG;\nperformance gains stem from structurally bounding\ninference within a query-aligned subgraph. This de-\nsign enables the model to more effectively interpret\nand utilize the semantic structure already encoded\nin PrimeKG, for example by distinguishing clini-\ncally relevant relations from generic associations\nduring region construction.\n3.3\nEvidence-aware Dynamic Reasoning\nReGraM adapts its reasoning behavior based on the\namount of retrieved evidence. We unify the notion\nof hop count across modules by fixing the maxi-\nmum number of reasoning steps to H = 3, imple-\nmented as up to three decomposed sub-questions.\nNo additional graph-walk or in-region traversal\ndepth is introduced beyond these steps; all reason-\ning is organized within this fixed hop budget.\nLet Nfacts denote the number of region triplets\navailable for a given sub-question. Based on Nfacts,\nthe model dynamically switches among three rea-\nsoning modes. Importantly, even when hypothesis\ngeneration is permitted, reasoning remains con-\nstrained. Across all modes, ReGraM operates un-\nder a closed-world assumption: entities must be-\nlong to Vq, and relations must be selected from a\npredefined schema set R.\nKG-STRICT relies solely on region evidence\nwhen sufficient facts are available, while LLM-\nGUESS falls back to parametric knowledge when\nno region evidence exists. HYBRID performs\nconstrained hypothesis completion using only in-\nregion entities and schema-allowed relations, fol-\nlowed by Reviewer verification, enabling limited\ngeneralization when KG evidence is partial but non-\nempty. All reasoning steps are executed strictly\nwithin the selected region Gq, with the total num-\nber of reasoning steps fixed to H = 3 for stability.\n3.4\nVerify & Revise Loop\nIn HYBRID mode, ReGraM employs a verifica-\ntion mechanism to control hallucinated or incon-\nsistent hypotheses. Generated triplets are evalu-\nated by a LoRA-tuned Llama-3.1-8B binary classi-\nfier, referred to as the Reviewer (Hu et al., 2022;\nGrattafiori et al., 2024). The Reviewer assigns a\nscore sθ to each triplet and accepts it as valid if\nsθ ≥0.5.\nTriplets rejected by the Reviewer are not dis-\ncarded immediately; instead, they are passed to\na revision prompt that attempts to correct seman-\ntic or schema-level inconsistencies. The revised\ntriplets are then re-evaluated by the Reviewer. This\nverification–revision loop runs for up to two iter-\nations, and only Reviewer-approved triplets are\nincorporated into the reasoning evidence (see Ap-\npendix A and Figure 5).\n3.5\nAnswer Synthesis\nThroughout reasoning, ReGraM stores intermedi-\nate sub-answers and supporting evidence in an\nEvidence Map. Final answer generation is per-\nformed using mode-specific synthesis templates:\nKG-STRICT produces concise factual answers\ngrounded solely in region evidence; HYBRID gen-\nerates stepwise explanations based on Reviewer-\nverified triplets; and LLM-GUESS explicitly in-\ndicates missing KG evidence while providing a\nplausible answer.\nThe Answer Synthesizer integrates hop-level\noutputs into a coherent final response grounded in\nthe accumulated evidence (see Appendix A, Fig-\nures 6–9). Taken together, ReGraM’s region-first\ndesign separates where to reason (region construc-\ntion) from how to reason (evidence-aware infer-\nence), allowing reasoning behavior to adapt to evi-\ndence availability.\n4\nExperiments\nThis section analyzes how region-first KG reason-\ning affects medical QA accuracy, hallucination ro-\nbustness, and reasoning efficiency. All experiments\ncompare ReGraM with the baseline KGARevion\nunder identical settings. Both methods operate on\nthe same biomedical knowledge graph, PrimeKG,\nwith the same entity set and relation schema.\n4.1\nExperimental Setup\nBoth ReGraM and KGARevion are based on\nLlama-3.1-8B and operate on the same biomedical\n4\n"}, {"page": 5, "text": "Task\nMetric\nModel\nBasic\nInter.\nExpert\nMedQA\nAfrimed\nMMLU\nPubMed\nAvg\nMCQ\nACC ↑\nKGARevion\n38.8\n35.0\n33.8\n51.7\n49.9\n62.8\n54.8\n46.2\nReGraM\n47.1\n45.0\n45.9\n54.4\n56.5\n65.7\n64.9\n54.2\n∆\n+8.3\n+9.9\n+12.1\n+2.7\n+6.6\n+2.9\n+10.1\n+8.0\nSAQ\nACC ↑\nKGARevion\n10.2\n13.9\n14.8\n36.5\n26.1\n65.7\n53.5\n31.5\nReGraM\n11.3\n13.9\n20.8\n51.0\n47.1\n65.5\n44.7\n36.0\n∆\n+1.1\n0.0\n+6.0\n+14.5\n+21.0\n-0.3\n-8.8\n+4.5\nIS ↓\nKGARevion\n57.5\n53.8\n57.6\n52.9\n23.7\n41.8\n36.0\n46.2\nReGraM\n30.5\n32.0\n30.1\n28.2\n16.1\n23.8\n24.4\n26.4\n∆IS\n27.0\n40.5\n47.7\n46.7\n32.1\n43.1\n32.2\n42.9\nTable 1: Performance of ReGraM and KGARevion on seven medical QA benchmarks. ACC denotes accuracy (%), and IS de-\nnotes the hallucination inconsistency score evaluated only for SAQ (lower is better). ∆indicates absolute accuracy improvement\n(percentage points), and ∆IS denotes relative reduction for IS (%), computed as (ISKGARevion −ISReGraM)/ISKGARevion × 100.\nknowledge graph, PrimeKG, under identical train-\ning and inference settings. No additional knowl-\nedge graphs, entities, relations, or edges are intro-\nduced in ReGraM.\nThe key difference lies in how relational knowl-\nedge is utilized. ReGraM extends the relation de-\nscription map originally used in KGARevion to\nmore comprehensively cover the diverse relation\ntypes present in PrimeKG. This extension does\nnot alter the knowledge available to the model,\nbut ensures that domain-relevant relations are not\nsystematically underweighted when the reasoning\nspace is explicitly constrained. As a result, re-\ngion construction more accurately aligns retrieved\ntriplets with the query intent, while preserving a\nfully controlled KG setting. Performance gains in\nReGraM are not attributable to increased knowl-\nedge coverage, as the underlying knowledge graph\nremains unchanged. Instead, improvements stem\nfrom semantically interpreting and structurally con-\nstraining existing PrimeKG relations during region\nconstruction and reasoning.\nTraditional KG-QA models such as QAGNN,\nJointLK, and KagNet assume supervised QA train-\ning and differ fundamentally from our generative\nKG-alignment objective. ReGraM is positioned as\na structural extension of KGARevion under identi-\ncal training and inference settings; accordingly, we\nfocus on a controlled comparison to isolate the ef-\nfect of region-first bounding. Broader comparisons\nto KG-RAG or GraphRAG-style baselines are left\nto future work. Experiments were conducted on\nseven medical QA benchmarks: MedDDx-Basic,\nMedDDx-Intermediate, MedDDx-Expert, MedQA,\nPubMedQA, MMLU-Medical, and AfrimedQA.\nFor readability, we refer to the three MedDDx\nvariants as Basic, Intermediate, and Expert in ta-\nbles and figures. Overall, region-first reasoning\nimproves stability under the semantic diversity of\nPrimeKG relations.\n4.2\nEvaluation Protocols\nWe evaluate model performance along two axes:\naccuracy and hallucination robustness. Standard\nmedical QA benchmarks are assessed under both\nmultiple-choice (MCQ) and short-answer (SAQ)\nsettings, while hallucination robustness is evaluated\nusing the MedHallu and Med-HALT benchmarks.\nStandard Medical QA Benchmarks (MCQ &\nSAQ)\nIn the MCQ setting, each model receives\na question with candidate options and outputs an\nanswer choice, and we report exact-match accu-\nracy. In the SAQ setting, the model generates an\nopen-ended answer. Because embedding similar-\nity alone does not reliably capture biomedical cor-\nrectness, we employ an LLM-as-a-Judge protocol\n(Badshah and Sajjad, 2025; Chang et al., 2024) us-\ning GPT-4o to assess semantic alignment between\ngenerated and reference answers. Following com-\nmon practice, outputs with a similarity score of at\nleast 0.8 are treated as correct, and the same judge\nand threshold are used for both methods to ensure\na controlled comparison. Evaluation prompts are\nprovided in Appendix A and Figure 10.\nHallucination Benchmarks (MedHallu, Med-\nHALT)\nMedHallu (Huang et al., 2025) measures\nfactual accuracy, hallucination rate, and reasoning\nerror rate in free-form generation. Med-HALT (Wu\net al., 2025), on the other hand, evaluates model\nstability under adversarial conditions through three\nsub-tests: Fake Question Test (FQT), None-of-the-\nAbove (NOTA), and False Confidence Test (FCT).\nOur LLM-based inconsistency analysis and Med-\nHallu evaluation follow the SAQ (open-ended) set-\nting, while Med-HALT adopts the MCQ (multiple-\n5\n"}, {"page": 6, "text": "Domain\nKG Coverage (%)\nMean IS\nDrug_Therapy\n68.14\n5.08\nGene_Protein\n60.30\n5.97\nDisease_Symptom\n15.66\n7.16\nPathway_Metab.\n26.31\n5.42\nIntegrated\n83.52\n6.83\nTable 2: Domain-wise hallucination inconsistency score (IS)\nvs KG coverage. Higher coverage generally leads to lower IS.\nchoice) discriminative format. Evaluation prompts\nand parsing rules are provided in Appendix A and\nFigure 12.\nFor MedHallu’s semantic alignment score (MH-\nSim), we use GPT-4o to measure the similarity\nbetween the generated answer and the ground-truth\nanswer on a 0–100 scale, and report the average\nscore across samples.\n4.3\nMain Results on Medical QA Performance\nTable 1 summarizes the comparative performance\nof ReGraM and KGARevion across seven med-\nical QA benchmarks under both multiple-choice\n(MCQ) and short-answer (SAQ) settings. We report\nabsolute accuracy improvements (∆, percentage\npoints) as well as relative reductions in inconsis-\ntency score (∆IS, percentage). Overall, ReGraM\nachieves consistent gains over the baseline, improv-\ning MCQ accuracy by 8.04% and SAQ accuracy by\n4.50%, with particularly strong improvements on\ndatasets requiring intensive multi-hop reasoning.\nReGraM shows the largest gains on the MedDDx\nbenchmarks (Basic, Intermediate, and Expert), all\nof which involve compositional reasoning across\nmultiple biomedical entities.\nIn the SAQ setting, ReGraM yields substan-\ntial improvements on MedQA (14.50%) and\nAfrimedQA (21.04%), both of which involve com-\nplex clinical scenarios that demand precise evi-\ndence alignment. These results indicate that in-\ntegrating semantic region selection with dynamic\nreasoning modes effectively improves open-ended\nanswer generation.\nA performance trade-off is observed on Pub-\nMedQA (-8.85%), where KG evidence is sparse\nor weakly aligned with question semantics. This\nbehavior reflects a design trade-off rather than a\nfailure case: in datasets with limited KG cover-\nage, aggressive region restriction and conservative\nverification can under-retrieve relevant evidence\nneeded for answer generation.\nResults in Table 1 are averaged over three runs\n(mean ± std). Observed improvements consistently\nexceed baseline variance, supporting the effective-\nMetric\nKGARevion\nReGraM\nMH-ACC\n10.00\n43.50\nMH-Sim\n0.00\n54.70\nMH-Hallu\n90.00\n89.00\nMH-RError\n100.00\n72.00\nTable 3: MedHallu results under SAQ. MH-ACC (↑): accu-\nracy on a 0–100 scale. MH-Sim (↑): semantic similarity to\nthe ground-truth answer on a 0–100 scale. MH-Hallu (↓) and\nMH-RError (↓): hallucination and reasoning error rates (%).\nness of the region-first design in enhancing accu-\nracy and compositional reasoning across diverse\nmedical QA tasks.\n4.4\nHallucination Evaluation Results\nWe evaluate hallucination robustness using IS\n(SAQ), MedHallu (generative), and Med-HALT\n(adversarial MCQ).\nDataset-wise Hallucination Reduction (IS)\nTa-\nble 1 shows factual hallucination inconsistency\nscores (IS) across datasets under the SAQ setting,\nscored using an LLM-based inconsistency evalua-\ntor (see Appendix A). ReGraM reduces the average\nIS from 46.2 to 26.4, achieving a 42.9% reduc-\ntion overall. The largest reductions are observed\nin multi-hop–intensive datasets such as MedDDx-\nBasic (–47.0%), MedDDx-Intermediate (–40.5%),\nand MedDDx-Expert (–47.8%), demonstrating that\nReGraM is particularly effective where multi-step\nreasoning is required. This indicates that region\nbounding reduces drift, and the Reviewer sup-\npresses unsupported steps.\nDomain-wise Hallucination Robustness\nTa-\nble 2 reports mean hallucination inconsistency\nscores (IS) across five biomedical domains under\nthe SAQ setting. Overall, higher KG coverage\ntends to correspond to lower hallucination rates;\nfor instance, the Drug_Therapy domain exhibits\nthe lowest IS. However, this relationship is not uni-\nform across domains. Notably, the Integrated\ndomain shows relatively high IS despite having the\nhighest KG coverage.\nWe further conduct a finer-grained analysis in\nAppendix C.7, demonstrating that hallucination ro-\nbustness also depends on additional structural fac-\ntors, including relation granularity, prompt design,\nand evidence redundancy.\nMedHallu: Free-form Generative Hallucination\nTable 3 presents results on the MedHallu bench-\nmark. While KGARevion hallucinates in nearly\nall generated answers, ReGraM significantly im-\nproves factual accuracy, semantic similarity, and\n6\n"}, {"page": 7, "text": "Dataset\nType\nReGraM\nw/o Domain Prior\n∆(prior)\nw/o Multihop\n∆(multihop)\nMedDDx-Expert\nMCQ\n45.88\n39.34\n+6.54\n41.41\n+3.03\nMedQA\nMCQ\n54.38\n56.48\n−2.10\n55.38\n−1.00\nMMLU-Med\nMCQ\n65.72\n67.77\n−2.05\n67.95\n−2.23\nPubMedQA\nMCQ\n64.86\n67.30\n−2.44\n67.30\n−5.01\nMedDDx-Expert\nSAQ\n20.81\n17.18\n+3.63\n18.43\n+2.38\nMedQA\nSAQ\n51.03\n2.91\n+48.12\n19.80\n+29.28\nMMLU-Med\nSAQ\n65.45\n5.33\n+60.12\n31.86\n+33.59\nPubMedQA\nSAQ\n44.65\n41.90\n+2.75\n42.30\n+2.35\nTable 4: Ablation results for ReGraM (MCQ and SAQ). Domain prior and multi-hop reasoning are both essential for maintaining\nfactual consistency.\nMetric\nKGARevion\nReGraM\nFQT-ACC\n22.00\n24.80\nFQT-Hallu\n78.00\n76.50\nNOTA-ACC\n22.00\n19.67\nNOTA-Hallu\n78.00\n80.33\nFCT-ACC\n22.00\n25.19\nFCT-Hallu\n78.00\n74.81\nTable 5: MedHALT results. ACC (↑): accuracy. Hallu (↓):\nhallucination rate.\nreasoning reliability. These results demonstrate Re-\nGraM’s effectiveness in suppressing hallucinations\nunder free-form generative settings.\nMed-HALT: Adversarial Discriminative Hallu-\ncination\nTable 5 presents performance on Med-\nHALT, which probes hallucination under adversar-\nial conditions through Fake-Question (FQT), None-\nof-the-Above (NOTA), and False-Confidence Test\n(FCT). ReGraM outperforms KGARevion in the\nFCT sub-test (+3.19 ACC), indicating stronger fac-\ntual discrimination under confidence-sensitive eval-\nuation. However, performance slightly degrades\non FQT and NOTA. This can be attributed to Re-\nGraM’s region-first design: while it discourages\nunsupported extrapolation, it also biases the model\ntoward selecting the most plausible answer within\nthe retrieved region. As a result, in adversarial\nsettings where the correct response is to abstain\n(e.g., NOTA), ReGraM may over-commit to weak\nin-region evidence. Overall, these results highlight\na fundamental trade-off between hallucination sup-\npression and abstention under adversarial inputs.\n4.5\nEfficiency Analysis\nReGraM reduces MCQ latency by 46% via region-\nfirst design. SAQ latency slightly increases due to\nverification. Full results are in Appendix C.5.\n4.6\nAblation Studies\nTo analyze how each structural component con-\ntributes to performance, we conducted ablation\nMetric\nReGraM\nw/o Reviewer\nSAQ Accuracy (avg.) ↑\n36.03\n33.17\nMedHallu Accuracy ↑\n43.50\n0.00\nMedHallu Similarity ↑\n54.70\n0.00\nMedHallu Hallucination ↓\n89.0\n100.0\nMedHallu Reasoning Error ↓\n72.0\n100.0\nMed-HALT FQT Hallu ↓\n74.81\n97.67\nMed-HALT FCT Accuracy ↑\n25.19\n33.67\nTable 6: Reviewer ablation. ReGraM vs. w/o Reviewer on QA\naccuracy and hallucination benchmarks. Metrics are computed\nunder our GPT-4o–based automatic evaluation protocol on a\n0–100 scale (higher is better for accuracy/similarity; lower is\nbetter for hallucination/error rates, ↓).\nstudies targeting the region selector, multi-hop de-\ncomposition, region refinement (MMR), reasoning\ndepth, and their interactions.\nEffect of Domain Prior and Multi-hop Reason-\ning\nTable 4 compares the impact of removing\ntwo core components of the region-first design: the\ndomain prior and multi-hop decomposition. Both\nablation settings retain the region selector but deac-\ntivate either the semantic domain prior used during\nregion construction or the hop-by-hop decomposi-\ntion used in reasoning.\nUnder the MCQ setting, removing either com-\nponent results in relatively mild changes. In con-\ntrast, under the SAQ setting, both ablations lead\nto substantial performance degradation. In particu-\nlar, w/o Domain Prior constructs regions without\ndomain-level semantic guidance, which weakens\nrelation reweighting during region selection and al-\nlows semantically irrelevant triplets to dominate the\nselected region. As a result, the evidence presented\nto the reasoner becomes noisy and internally incon-\nsistent, leading to sharp accuracy drops on MedQA\n(51.03 →2.91) and MMLU-Med (65.45 →5.33).\nSimilarly, the w/o Multihop setting shows pro-\nnounced degradation on multi-step reasoning tasks\nsuch as MedQA (51.03 →19.80) and MMLU-Med\n(65.45 →31.86), suggesting that single-step rea-\nsoning fails to reconstruct compositional dependen-\n7\n"}, {"page": 8, "text": "cies across medical concepts.\nOverall, these results indicate that the factual\nstability and reasoning accuracy of ReGraM are\njointly supported by (1) the domain prior, which\nstabilizes region construction by suppressing se-\nmantically irrelevant evidence, and (2) the multi-\nhop reasoning mechanism, which enables struc-\ntured inference over interdependent medical con-\ncepts.\nReviewer Ablation: Accuracy vs.\nStability\nTrade-off\nTable 6 summarizes the effect of re-\nmoving the Reviewer. SAQ Accuracy (avg.) is\naveraged over four SAQ benchmarks (MedDDx-\nExpert, MedQA, MMLU-Med, and PubMedQA).\nWithout the Reviewer, ReGraM experiences catas-\ntrophic failure on MedHallu (0% accuracy, 100%\nhallucination), while performance on Med-HALT\nbecomes inconsistent, hallucination increases, but\nsome discriminative accuracy improves (e.g., FCT\naccuracy). These results highlight a central insight:\nthe Reviewer enforces conservative generation be-\nhavior that reduces hallucination, but may under-\nanswer when KG evidence is limited. Such dis-\ncriminative accuracy gains come at the cost of sub-\nstantially increased hallucination rates, underscor-\ning the trade-off between discriminative confidence\nand factual reliability.\nInteraction Between Domain Prior and Multi-\nhop Reasoning\nRemoving either the domain\nprior or multi-hop decomposition significantly re-\nduces accuracy, and removing both leads to catas-\ntrophic failure, as shown in Appendix C.5.\nRegion Refinement and Hop-depth\nAblation\nresults on MMR and hop-depth variation are pro-\nvided in Appendix C.2. The region refinement\nimproves SAQ stability by reducing redundancy,\nand a hop depth of 3 yields optimal factuality-\nperformance trade-offs.\n5\nDiscussion\nReGraM suggests that instability in medical QA is\nnot only a modeling issue but also a structural one.\nIn particular, how and when the reasoning space\nis bounded can be as important as the choice of\nthe underlying language model. Reasoning over\na heterogeneous KG without explicit boundaries\nallows irrelevant relations to intervene, amplifying\nsemantic drift in multi-hop and generative settings.\nBy constructing a query-aligned region before rea-\nsoning, ReGraM effectively bounds the reasoning\nspace and improves robustness under both MCQ\nand SAQ settings.\nWe observe a tension between precision and\ncompleteness: strict region bounding may lead to\nunder-answering in low-coverage settings, whereas\ndense domains benefit more consistently.\nThis\ntrade-off is also reflected in adversarial settings\n(e.g., NOTA), where strict in-region evidence can\nbias the model toward a plausible in-region option\nunless an explicit abstention mechanism is intro-\nduced. These limitations and broader evaluation\ncaveats are discussed in Section 7.\n6\nConclusion\nThis work introduced ReGraM, a region-first\nKG reasoning framework for improving stability\nand factuality in medical QA. ReGraM constructs\nquery-aligned local KG regions and performs rea-\nsoning within a bounded region, mitigating ineffi-\nciency and semantic drift in heterogeneous biomed-\nical KGs. Across seven medical QA benchmarks,\nReGraM improves accuracy over KGARevion by\n8.04 points in MCQ and 4.50 points in SAQ, while\nalso reducing hallucinations and improving infer-\nence efficiency. Notably, these gains are achieved\nusing a lightweight 8B-scale language model with-\nout task-specific fine-tuning, highlighting the prac-\ntical efficiency and deployability of region-first rea-\nsoning in resource-constrained medical settings.\nReGraM remains robust under free-form genera-\ntion and empirically suggests improved scalability\nby maintaining a bounded reasoning space during\ninference, although formal guarantees are left for\nfuture work. By treating the KG as a structural\ncomponent that shapes reasoning, ReGraM enables\nmore controllable inference; future work will ex-\ntend region-first reasoning to better handle answer-\nabsent queries via explicit abstention strategies.\n8\n"}, {"page": 9, "text": "7\nLimitations\nDespite the strong empirical performance of Re-\nGraM, several limitations remain and warrant care-\nful consideration.\nReliance on automatic evaluation protocols\nOur evaluation primarily relies on LLM-based au-\ntomatic judges rather than clinician-centered as-\nsessment. While this choice enables scalable and\nconsistent benchmarking across diverse datasets, it\ncannot fully capture clinical appropriateness, safety,\nor decision validity in real-world medical contexts.\nMoreover, the reported inconsistency score (IS) is\ncomputed using a fixed scorer prompt with deter-\nministic decoding, and its absolute value may vary\ndepending on the evaluator model or prompt de-\nsign. Accordingly, IS should be interpreted primar-\nily as a relative comparison under a fixed evaluation\nprotocol. In future work, we plan to incorporate\nclinician-in-the-loop evaluation and scenario-based\nexpert review to more comprehensively assess clin-\nical reliability (Livingston et al., 2025).\nDependence on relation semantic representa-\ntions\nReGraM relies on PrimeKG as its sole\nknowledge base and does not introduce new entities\nor relations. The effectiveness of region selection\ntherefore depends on how well relation semantics\nare captured by the relation description map. Al-\nthough we expand and refine this map to better\nreflect PrimeKG’s relational diversity, some fine-\ngrained or context-dependent relations may still\nbe underrepresented. This limitation highlights an\nopen challenge in representing domain semantics\nwhen reasoning is explicitly constrained to a lo-\ncalized region. Improving automatic or learned\nrelation semantic representations remains an im-\nportant direction for future work.\nChallenges in evidence aggregation at the de-\ncision stage\nEven when ReGraM successfully\naccumulates relevant evidence through region se-\nlection and multi-hop reasoning, final answer selec-\ntion can occasionally become misaligned with the\nsupporting evidence (Appendix C.9, Example 2),\nparticularly in discrete option selection scenarios.\nThis observation suggests that robustness in med-\nical QA depends not only on retrieving and veri-\nfying high-quality evidence, but also on how such\nevidence is aggregated, calibrated, and translated\ninto final decisions. Improving evidence-aware\naggregation and decision calibration remains an\nimportant direction for future research (Wen et al.,\n2025).\nLimited scalability analysis\nReGraM empiri-\ncally reduces inference latency by reasoning over\ncompact, query-aligned regions rather than the full\nknowledge graph. However, we do not provide a\nformal asymptotic analysis with respect to KG size.\nConsequently, observed efficiency gains may vary\ndepending on KG structure, domain distribution,\nand reasoning depth. A more rigorous theoretical\nand system-level analysis of scalability is left for\nfuture work (Chen et al., 2025).\n8\nEthics Statement\nThis work aims to improve the factual consistency\nof medical question answering systems by con-\nstraining reasoning over a biomedical knowledge\ngraph. All experiments were conducted on publicly\navailable benchmark datasets using automatic eval-\nuation protocols, and no private or patient-specific\ndata were used.\nThe generated outputs are not intended for clin-\nical use without human oversight, and the system\nshould not be deployed in real-world diagnostic\nsettings without expert review, as medical inaccu-\nracies may still occur.\nAn anonymized version of the code and\nadditional materials is available at https://\nanonymous.4open.science/r/ReGraM-6B41/,\nand will be fully released upon acceptance.\nReferences\nSher Badshah and Hassan Sajjad. 2025. Reference-\nguided verdict: Llms-as-judges in automatic eval-\nuation of free-form qa. In Proceedings of the 9th\nWidening NLP Workshop, pages 251–267.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\n9\n"}, {"page": 10, "text": "Jaime Carbonell and Jade Goldstein. 1998. The use of\nmmr, diversity-based reranking for reordering doc-\numents and producing summaries. In Proceedings\nof the 21st Annual International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, SIGIR ’98, page 335–336, New York, NY,\nUSA. Association for Computing Machinery.\nPranjal Chandak, Kexin Huang, and Marinka Zitnik.\n2023. Building a knowledge graph to enable preci-\nsion medicine. Scientific Data, 10:67.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, and 1 others. 2024.\nA survey on evaluation of large language models.\nACM transactions on intelligent systems and technol-\nogy, 15(3):1–45.\nJialin Chen, Houyu Zhang, Seongjun Yun, Alejandro\nMottini, Rex Ying, Xiang Song, Vassilis N Ioannidis,\nZheng Li, Qingjun Cui, and Yale University Amazon.\n2025. Gril: Knowledge graph retrieval-integrated\nlearning with large language models. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2025, pages 16306–16319.\nJifan Chen, Shih-ting Lin, and Greg Durrett. 2019.\nMulti-hop question answering via reasoning chains.\narXiv preprint arXiv:1910.02610.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, and 1 others. 2022. Lora: Low-rank\nadaptation of large language models. ICLR, 1(2):3.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2025. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions. ACM Trans. Inf. Syst., 43(2).\nMinbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jae-\nwoo Kang. 2024.\nImproving medical reasoning\nthrough retrieval and self-reflection with retrieval-\naugmented large language models. Bioinformatics,\n40(Supplement_1):i119–i129.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nRunxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming\nLiu, Dayong Wu, Shijin Wang, and Bing Qin. 2025.\nOntology-guided reverse thinking makes large lan-\nguage models stronger on knowledge graph question\nanswering. In Proceedings of the 63rd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 15269–15284, Vi-\nenna, Austria. Association for Computational Lin-\nguistics.\nLeah\nLivingston,\nAmber\nFeatherstone-Uwague,\nAmanda Barry, Kenneth Barretto, Tara Morey,\nDrahomira Herrmannova, and Venkatesh Avula.\n2025. Reproducible generative artificial intelligence\nevaluation for health care: a clinician-in-the-loop\napproach. JAMIA Open, 8(3):ooaf054.\nCui Long, Yongbin Liu, Chunping Ouyang, and Ying\nYu. 2024. Bailicai: A domain-optimized retrieval-\naugmented generation framework for medical appli-\ncations. arXiv preprint arXiv:2407.21055.\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\nShirui Pan. 2023. Reasoning on graphs: Faithful and\ninterpretable large language model reasoning. arXiv\npreprint arXiv:2310.01061.\nPotsawee Manakul, Adian Liusie, and Mark Gales. 2023.\nSelfcheckgpt: Zero-resource black-box hallucination\ndetection for generative large language models. In\nProceedings of the 2023 conference on empirical\nmethods in natural language processing, pages 9004–\n9017.\nVaibhav Mavi, Abulhair Saparov, and Chen Zhao.\n2023. Retrieval-augmented chain-of-thought in semi-\nstructured domains. In Proceedings of the Natural\nLegal Language Processing Workshop 2023, pages\n178–191, Singapore. Association for Computational\nLinguistics.\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\nAmmar. 2019. ScispaCy: Fast and robust models\nfor biomedical natural language processing. In Pro-\nceedings of the 18th BioNLP Workshop and Shared\nTask, pages 319–327, Florence, Italy. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno,\nDavid Stutz, Ellery Wulczyn, Fan Zhang, Tim\nStrother, Chunjong Park, Elahe Vedadi, and 1 others.\n2024. Capabilities of gemini models in medicine.\narXiv preprint arXiv:2404.18416.\nMansi Sakarvadia. 2024. Towards interpreting language\nmodels: A case study in multi-hop reasoning. arXiv\npreprint arXiv:2411.05037.\nYucheng Shi, Shaochen Xu, Tianze Yang, Zhengliang\nLiu, Tianming Liu, Xiang Li, and Ninghao Liu. 2025.\nMkrag: Medical knowledge retrieval augmented gen-\neration for medical question answering. In AMIA\nAnnual Symposium Proceedings, volume 2024, page\n1011.\n10\n"}, {"page": 11, "text": "Noah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. Advances in Neural Information Process-\ning Systems, 36:8634–8652.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, and\n1 others. 2025. Toward expert-level medical ques-\ntion answering with large language models. Nature\nMedicine, 31(3):943–950.\nXiaorui Su, Yibo Wang, Shanghua Gao, Xiaolong\nLiu, Valentina Giunchiglia, Djork-Arné Clevert, and\nMarinka Zitnik. 2024. Kgarevion: an ai agent for\nknowledge-intensive biomedical qa. arXiv preprint\narXiv:2410.04660.\nYuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang,\nand Bryan Hooi. 2025. Fidelis: Faithful reasoning in\nlarge language models for knowledge graph question\nanswering. In Findings of the Association for Com-\nputational Linguistics: ACL 2025, pages 8315–8330.\nYueqing Sun, Qi Shi, Le Qi, and Yu Zhang. 2022.\nJointLK: Joint reasoning with language models and\nknowledge graphs for commonsense question answer-\ning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 5049–5060, Seattle, United States. Asso-\nciation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\nand 1 others. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. Advances\nin neural information processing systems, 35:24824–\n24837.\nBingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu,\nYulia Tsvetkov, Bill Howe, and Lucy Lu Wang. 2025.\nKnow your limits: A survey of abstention in large\nlanguage models. Transactions of the Association for\nComputational Linguistics, 13:529–556.\nChristopher YK Williams, Brenda Y Miao, Aaron E\nKornblith, and Atul J Butte. 2024. Evaluating the\nuse of large language models to provide clinical rec-\nommendations in the emergency department. Nature\ncommunications, 15(1):8236.\nJunde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min\nXu, Filippo Menolascina, Yueming Jin, and Vicente\nGrau. 2025.\nMedical graph rag: evidence-based\nmedical large language model via graph retrieval-\naugmented generation. In Proceedings of the 63rd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 28443–\n28467.\nYilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qian-\nwen Zhang, Di Yin, Xing Sun, and Xiao Huang. 2025.\nGraphrag-bench: Challenging domain-specific rea-\nsoning for evaluating graph retrieval-augmented gen-\neration. arXiv preprint arXiv:2506.02404.\nYunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang,\nBingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang\nXie, and Yuyin Zhou. 2024. A preliminary study of\no1 in medicine: Are we closer to an ai doctor? arXiv\npreprint arXiv:2409.15277.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. In The eleventh international conference on\nlearning representations.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosse-\nlut, Percy Liang, and Jure Leskovec. 2021.\nQa-\ngnn: Reasoning with language models and knowl-\nedge graphs for question answering. arXiv preprint\narXiv:2104.06378.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D\nGoodman. 2024. Star: Self-taught reasoner boot-\nstrapping reasoning with reasoning. In Proc. the 36th\nInternational Conference on Neural Information Pro-\ncessing Systems, volume 1126.\nZefan Zeng, Qing Cheng, Xingchen Hu, Yan Zhuang,\nXinwang Liu, Kunlun He, and Zhong Liu. 2025.\nKosel: Knowledge subgraph enhanced large lan-\nguage model for medical question answering.\nKnowledge-Based Systems, 309:112837.\nWeihe Zhai, Arkaitz Zubiaga, Bingquan Liu, Cheng-Jie\nSun, and Yalong Zhao. 2024. Towards faithful knowl-\nedge graph explanation through deep alignment in\ncommonsense question answering. In Proceedings\nof the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 18920–18930.\n11\n"}, {"page": 12, "text": "A\nPrompt Templates Used in ReGraM\nReGraM performs prompt-based reasoning across\nmultiple stages, including query decomposition,\nregion selection, reasoning, answer generation, and\nverification. This appendix visualizes the main\nprompts used at each stage of execution. Each\nprompt is designed according to its functional role,\nand curly brackets {} denote dynamically filled\ninput slots at runtime.\nA.1\nReasoning Prompts\nPrompts for query decomposition, KG interaction,\nand hop-level synthesis.\nPrompt 1. Query Domain Classification Prompt\nSystem: You are an expert biomedical query classifier.\nYour task is to choose the single most relevant category\nfor the given query from the list below. The query may\nspan multiple domains; select the category that best\nrepresents the main thrust of the question.\nCategory Options:\n• \"GENE_PROTEIN\": Questions about genes, proteins,\ninteractions, expression.\n• \"DRUG_THERAPY\": Questions about drugs, therapies,\ntreatments, side effects, indications.\n• \"DISEASE_SYMPTOM\":\nQuestions about diseases,\nsymptoms, phenotypes, diagnoses.\n• \"PATHWAY_METABOLISM\":\nQuestions about path-\nways, metabolism, or processes.\n• \"INTEGRATED\": Complex questions spanning multi-\nple categories.\nQuery: {user_question}\nOutput (JSON only):\n«JSON_START»\n{\"category\": \"DRUG_THERAPY\"}\n«JSON_END»\nFigure 2: Query Domain Classification Prompt. Classifies\na question into one of five high-level biomedical domains\n(GENE_PROTEIN, DRUG_THERAPY, etc.). The predicted\ncategory is later used for relation weighting and KG region\nselection\nPrompt 2. Multi-hop CoT Decomposition Prompt\nSystem: You are a world-class medical reasoning en-\ngine. Your task is to decompose a complex medi-\ncal question into up to three logical, answerable sub-\nquestions (hops). First, provide your reasoning process\n(Analysis), then output the sub-questions as JSON.\nExample:\nUser\nQuery:\n\"What\ndrug\nthat\ntargets\nthe\nFLT4\ngene\nalso\nsuppresses inflammation?\"\nAnalysis:\n1. The primary entity is \"FLT4\".\n2.\nThe\nsecondary\ncondition\nis\n\"inflammation\".\n3.\nThe task is to find a drug\nconnecting them.\nOutput (JSON):\n{\"hops\":\n[\"Hop 1:\n...\", \"Hop 2:\n...\", \"Hop 3: ...\"]}\nTask: User Query: «<{user_question}»> Your Re-\nsponse: Provide both “Analysis” and “Final JSON”.\nFigure 3: Multi-hop CoT Decomposition Prompt. Decom-\nposes a query into up to three logical sub-questions (hops)\nusing a one-shot Chain-of-Thought prompt. Serves as the core\nstep for constructing ReGraM’s reasoning path\nPrompt 3. Hypothetical Triplet Generation Prompt\n(HYBRID Mode)\nSystem: You are a cautious biomedical reasoning ex-\npert. Your goal is to generate logical triplets to answer\nthe given sub-question.\nSub-question: {hop_question}\nKnown\nFacts\n(from\nKG,\nmay\nbe\nempty):\n{known_facts}\nRules:\n1. Use only the information in “Known Facts”.\n2. If insufficient, generate general, high-level plausi-\nble triplets.\n3. Do not invent new entities or overly specific rela-\ntions.\n4. All generated triplets must use only entities present\nin Vq and relations from the allowed schema set R.\nOutput (JSON only):\n«JSON_START»\n{\"Triplets\":\n[[\"entity_A\",\n\"has_function\",\n\"function_X\"],\n...]}\n«JSON_END»\nFigure 4: Hypothetical Triplet Generation Prompt (HYBRID\nMode). Used when insufficient KG evidence is retrieved.\nThe LLM conservatively generates triplets using only allowed\nentities and relations while avoiding new entity creation\nPrompt 4.\nTriplet Revision Prompt (Reviewer\nLoop)\nSystem: You are a fact-checker verifying the factuality\nand usefulness of the given triplet.\nConstraints:\n• Relation MUST be one of: {allowed_relations}.\n• Use only entities present in the input.\nTriplet: {t}\nQuestion (context): {q}\nOutput (JSON only):\n«JSON_START»\n{\"Revised_Triplets\":\n[[\"Head\",\n\"relation\", \"Tail\"]]}\n«JSON_END»\nFigure 5: Triplet Revision Prompt (Reviewer Loop). Used\nwhen a generated triplet is judged as false by the Reviewer. En-\ncourages correcting schema-violating relations and removing\nunnecessary hallucinated content\n12\n"}, {"page": 13, "text": "Prompt 5.\nHop Answer Synthesis Prompt —\nKG_STRICT Mode\nSystem: You are an expert medical reasoner. Based\nonly on the “Verified Facts,” provide a direct, concise\nanswer to the sub-question.\nSub-question: {hop_question}\nVerified Facts: {verified_triplets}\nConcise Answer:\nFigure 6: Hop Answer Synthesis Prompt — KG_STRICT\nMode. Used when ≥10 KG facts are available; the LLM\ngenerates answers solely from verified evidence without using\nits internal knowledge\nPrompt 6. Hop Answer Synthesis Prompt — HY-\nBRID Mode\nSystem: You are a knowledgeable analyst. Derive a fi-\nnal answer by reasoning step-by-step using the verified\nfacts.\nSub-question: {hop_question}\nVerified Facts: {verified_triplets}\nReasoning Steps:\n1. Fact: ...\n2. Fact: ...\n3. Connection: ...\nFinal Answer (1 sentence):\nFigure 7: Hop Answer Synthesis Prompt — HYBRID Mode.\nActivated when 1–9 KG facts exist; the LLM performs step-\nwise reasoning using partial evidence to derive a final answer\nPrompt 7.\nHop Answer Synthesis Prompt —\nLLM_GUESS Mode\nSystem: You are an expert biomedical researcher. No\nspecific facts were found in the KG. Based on general\nknowledge, provide a plausible answer beginning with\n“Based on general knowledge,”.\nSub-question: {hop_question}\nPlausible Answer:\nFigure 8: Hop Answer Synthesis Prompt — LLM_GUESS\nMode. Applied when no relevant KG facts are retrieved; the\nLLM relies on parametric knowledge and explicitly expresses\nuncertainty in its answer\nPrompt 8. Final Answer Synthesis Prompt\nSystem: You are a lead researcher writing a final,\nconclusive answer for a user. Synthesize the reasoning\ncontained in the Evidence Map to answer the Original\nQuestion.\nOriginal Question: {original_query}\nEvidence Map: {evidence_map}\nFinal Conclusive Answer:\nFigure 9: Final Answer Synthesis Prompt. Integrates hop-\nlevel reasoning results from the Evidence Map into a coherent\nfinal answer, ensuring logical consistency across hops\nA.2\nEvaluation Prompts\nPrompts for automatic evaluation, hallucination\ndetection, and inconsistency scoring.\nPrompt 9. SAQ Judge Prompt (LLM-as-a-Judge)\nSystem: You are an expert evaluator grading a submit-\nted answer. Compare the “Ground Truth Answer” with\nthe “Model’s Answer.”\nTask:\n1. Assess correctness.\n2. Provide a similarity score (0.0–1.0).\n3. Output JSON including:\n• \"reasoning\"\n• \"similarity_score\"\n• \"is_correct\" (true if score ≥0.8)\nQuestion: {query}\nGround Truth Answer: {ground_truth_answer}\nModel’s Answer: {model_response}\nFigure 10: SAQ Judge Prompt (LLM-as-a-Judge). Evalu-\nates semantic alignment between a model-generated free-form\nanswer and the ground truth using GPT-4o; responses with\nsimilarity ≥0.8 are considered correct\nPrompt 10. Hallucination Judge Prompt (Med-\nHallu)\nSystem: You are an expert medical fact-checker. De-\ntermine whether the model’s answer contains halluci-\nnation (unsupported claim, contradiction, fabrication)\ngiven the knowledge context.\nQuestion: {query}\nKnowledge Context: {reference_context}\nModel’s Answer: {model_answer}\nOutput (JSON only):\n–\nsupported\n/\nunsupported\n/\ncontradicted\nFigure 11: Hallucination Judge Prompt (MedHallu). Used in\nMedHallu evaluation to determine whether a model response\ncontains hallucination (e.g., unsupported claims or fabricated\nfacts) given reference context\nPrompt 11. Inconsistency Scorer Prompt (IS)\nSystem: You are an evaluator that measures factual\ninconsistency (hallucination) between two answers to\nthe same medical question.\nTask:\nGiven a Question, a Reference Answer\n(ground truth), and a Model Answer, output a JSON\nobject with:\n• \"is\": a number in [0,1], where 1 indicates max-\nimally inconsistent / hallucinated and 0 indicates\nfully consistent.\n• \"rationale\": a brief explanation (1–2 sentences).\nConstraints:\n• Output JSON only.\n• Use deterministic evaluation (temperature=0) in our\nsetup.\nQuestion: {query}\nReference Answer: {ground_truth}\nModel Answer: {model_answer}\n13\n"}, {"page": 14, "text": "Figure 12: Inconsistency Scorer Prompt (IS). Computes a\nfactual inconsistency score in [0,1] between the model answer\nand the reference answer under SAQ. We use deterministic\ndecoding (temperature=0) and report dataset-level averages\n(Table 1)\nB\nImplementation Details\nTo enhance reproducibility, this appendix details\nthe implementation settings of ReGraM, includ-\ning model configurations, inference hyperparame-\nters, KG preprocessing pipeline, relation weighting\nschema, and full KG statistics.\nB.1\nModel and Inference Settings\nBackbone Language Model\nThe core rea-\nsoning modules of ReGraM are based on\nLlama-3.1-8B-Instruct (query decomposition,\ntriplet generation, hop-by-hop reasoning, synthe-\nsis).\nWe applied 4-bit quantization (NF4, via\nbitsandbytes) and used torch.bfloat16 preci-\nsion.\n• Model:\nmeta-llama/Meta-Llama-3.1-8B-\nInstruct\n• Quantization: 4-bit NF4\n• Precision: torch.bfloat16\nReviewer Model (Binary Verifier)\nLoRA-tuned\nLlama-3.1-8B-Instruct; entity/relation embed-\ndings initialized via RotatE on PrimeKG.\n• Base:\nmeta-llama/Meta-Llama-3.1-8B-\nInstruct\n• Embedding Init: RotatE (PrimeKG)\n• LoRA: rank r = 64, α = 16, modules = q_proj,\nv_proj, dropout=0.1\nInference Settings\n• Temperature = 0.3, Top-p = 0.9\n• Max tokens: Classification = 256, Reasoning =\n512, Synthesis = 1024\nEvaluation Phase:\n• Model: GPT-4o (OpenAI), Temperature = 0.0\nExpanded Relation Description Map\nReGraM\nextends the relation description map used in\nKGARevion by providing richer natural-language\ndescriptions, domain annotations, and domain-\naware weights for PrimeKG relation types. This\nexpansion is designed to improve region-aware re-\ntrieval accuracy rather than to introduce new knowl-\nedge. All mappings operate strictly over existing\nPrimeKG relations.\nB.2\nKnowledge Graph Preprocessing and\nRetrieval\nThis subsection details the implementation of the\nRegion Selection module (Step (2) in Figure 1),\nwhich constructs a query-aligned KG region for\neach sub-question. As shown in Figure 1, region\nselection consists of (i) entity linking and expan-\nsion, (ii) domain-aware relation weighting, and (iii)\nweighted MMR re-ranking to select Top-K triplets\nthat form the localized region Gq.\nEntity Linking and Expansion (Figure 1-(2),\n“Entity Linking”)\nTo convert natural language\nqueries into structured, KG-aligned representations,\nwe adopt a hybrid entity linking pipeline that com-\nbines symbolic linking with fuzzy matching and\nsynonym expansion. This step corresponds to the\n“Entity Linking” component in the Region Selec-\ntion block of Figure 1.\n• Primary linker: scispaCy using the UMLS\nKnowledge Base linker (en_core_sci_sm).\n• Fuzzy Matching: rapidfuzz (Levenshtein ra-\ntio ≥90) to recover surface-form variations.\n• Synonym Expansion:\nA domain-specific\nalias dictionary (SYNONYM_MAP) curated from\nPrimeKG node attributes (e.g., “Tylenol” →\n“Acetaminophen”).\nDomain-Specific Relation Weighting (Figure 1-\n(2), “Domain-Aware Relation Weighting”)\nRe-\nGraM assigns domain-specific weights wr to rela-\ntion types during region construction. This corre-\nsponds to the “Domain-Aware Relation Weighting”\ncomponent in Figure 1-(2), and it biases region\nselection toward relations that are semantically rel-\nevant to the predicted query domain. Table 7 sum-\nmarizes the full weighting scheme.\nWeighted MMR Re-ranking and Region Forma-\ntion (Figure 1-(2), “Weighted MMR”)\nGiven\ncandidate triplets connected to the expanded entity\nset, we re-rank them to balance semantic relevance\nand diversity using weighted Maximal Marginal\nRelevance (MMR). This step corresponds to the\n“Weighted MMR” component in Figure 1-(2) and\nselects Top-K triplets to form the localized region\nGq.\n• Embedding: sentence-transformers/\nall-MiniLM-L6-v2 (384-d)\n• MMR diversity factor: λ = 0.7\n14\n"}, {"page": 15, "text": "Relation\nGene_Protein\nDrug_Therapy\nDisease_Symptom\nPathway_Metabolism\nIntegrated\nInteracts_with\n1.5\n0.8\n0.6\n1.0\n1.2\nTargets\n0.8\n1.5\n0.8\n1.0\n1.3\nTreats\n0.6\n1.5\n1.2\n0.8\n1.1\nCauses\n0.5\n0.7\n1.5\n0.8\n1.0\nExpressed_in\n1.3\n0.7\n0.5\n1.0\n1.1\nAssociated_with\n1.0\n1.0\n1.3\n0.9\n1.2\nRegulates\n1.4\n0.8\n0.7\n1.5\n1.3\nOccurs_in\n0.9\n0.8\n1.0\n1.2\n1.1\nTable 7: Relation weighting matrix used in KG region construction Weights wr are applied during region selection based on\nthe predicted domain (rows: relations, columns: query domains). Higher weights prioritize semantically aligned relations during\nretrieval\nDataset\nN\nAfrimedQA (MCQ)\n3,910\nAfrimedQA (SAQ)\n1,236\nMedQA (USMLE)\n1,273\nMedDDx\n1,769\nMedHallu\n1,000\nPubMedQA\n1,000\nMedHalt (FCT)\n300\nMedHalt (NOTA)\n300\nMedHalt (Fake)\n300\nTable 8: Number of evaluation instances used in our experi-\nments.\nDataset\nType\nReGraM\nw/o MMR\n∆\nMedDDx-Expert\nMCQ\n44.44\n43.06\n+1.38\nMedQA\nMCQ\n54.38\n54.80\n−0.42\nMMLU-Med\nMCQ\n65.72\n65.66\n+0.06\nMedDDx-Expert\nSAQ\n20.81\n27.60\n−6.79\nMedQA\nSAQ\n51.03\n25.30\n+25.73\nMMLU-Med\nSAQ\n65.45\n42.30\n+23.15\nTable 9: Effect of removing MMR on MCQ and SAQ ac-\ncuracy MMR reduces redundancy in KG retrieval, especially\nin SAQ, where excessive overlap between triplets amplifies\nreasoning drift\n• Region size: K = 15\nOverall, the above pipeline operationalizes Step (2)\nin Figure 1 by transforming domain-aware, query-\nlinked triplet candidates into a compact, query-\naligned region used as the reasoning space in sub-\nsequent modules.\nB.3\nDataset Statistics\nTable 8 reports the number of evaluation instances\nused for each dataset in our experiments. For large-\nscale datasets, we follow prior work and report\nresults on a randomly sampled subset.\nH=1\nH=3\nH=5\nH=7\nH=10\nMCQ\n33.82\n44.44\n42.33\n41.67\n39.33\nSAQ\n14.84\n20.81\n20.33\n21.00\n18.00\nTable 10: Impact of reasoning hop-depth on MCQ and\nSAQ performance (MedDDx-Expert) The best results are\nobtained at Hop 3, while performance drops at larger hop\ndepths due to over-expansion and noise accumulation.\nC\nAdditional Experiments\nThis appendix presents additional ablation results\nand qualitative reasoning examples that comple-\nment the main paper. These findings further il-\nlustrate ReGraM’s reasoning stability, structural\neffects, and observed failure modes, providing in-\nsights for future analyses.\nC.1\nEffect of Removing MMR (Extended\nResults)\nReGraM employs weighted MMR-based triplet re-\nranking to reduce redundancy within the KG region\nand to construct query-centered, refined subgraphs.\nTable 9 reports extended results showing the per-\nformance change when MMR is removed. In the\nMCQ setting, MMR removal has minimal impact,\nwhile in SAQ, redundant KG candidates cause in-\ncreased reasoning drift and reduced consistency.\nC.2\nNumber of reasoning steps(H) Analysis\nReasoning hop depth is a key factor for evaluating\nhow multi-hop decomposition affects the accuracy\nand stability of KG reasoning. Table 10 presents\nresults for MedDDx-Expert, measuring accuracy\nat hop depths of 1, 3, 5, 7, and 10. Performance\npeaks at Hop 3, while deeper paths cause noise\npropagation and increased hallucination frequency.\n15\n"}, {"page": 16, "text": "Categorization\nMultihop\nACC\n∆\nO\nO\n51.03\n+0.00\nX\nO\n2.91\n−46.17\nO\nX\n19.80\n−29.28\nX\nX\n12.17\n−36.91\nTable 11: Ablation of region selector and multi-hop decom-\nposition on MedQA (SAQ). Disabling either module leads\nto severe degradation, and removing both causes the largest\nperformance drop.\nC.3\nInteraction Between Region Selector and\nMulti-hop Decomposition\nReGraM’s region-first architecture achieves maxi-\nmum stability when semantic region selection and\nstepwise reasoning alignment operate complemen-\ntarily. To evaluate their interaction, we conduct an\nablation on MedQA (SAQ) by selectively disabling\neach component.\nTable 11 presents the result. Removing either mod-\nule causes a significant drop in accuracy, while\nremoving both leads to a catastrophic performance\ncollapse—confirming their complementary role in\nregion-first reasoning.\nC.4\nEffect of Reviewer Module (Extended\nResults)\nGroup\nDataset / Setting\nReGraM\nw/o Reviewer\nSAQ Acc\nMedDDx-Expert\n20.81\n27.67\nMedQA\n51.03\n24.33\nMMLU-Med\n65.45\n41.67\nPubMedQA\n44.65\n39.00\nMedHallu\nACC / Sim\n43.50 / 54.70\n0.00 / 0.00\nHallu / RError\n0.8900 / 0.7200 1.0000 / 1.0000\nMed-HALT\nFQT (ACC / Hallu)\n0.0233 / 0.7481 0.0233 / 0.9767\nFCT (ACC / Hallu)\n0.2519 / 0.7481 0.3367 / 0.6833\nNOTA (ACC / Hallu) 0.1967 / 0.8033 0.2100 / 0.7867\nTable 12: Reviewer ablation across QA and hallucina-\ntion benchmarks SAQ Acc. reports exact-match accuracy.\nMedHallu reports ACC, semantic similarity (Sim; 0–100),\nhallucination rate (Hallu), and reasoning error rate (RError).\nMed-HALT reports ACC and hallucination rate (lower is bet-\nter)\nTo assess the contribution of the Reviewer mod-\nule in hallucination suppression and factual consis-\ntency, we conducted ablation experiments where\nthe module was removed from ReGraM. Table 12\nsummarizes performance across QA and hallucina-\ntion benchmarks.\nWhile MCQ results show mixed changes—with\nslight accuracy increase in MedQA (+2.29) and\nminor drops elsewhere—the impact in SAQ set-\ntings is substantial. Removing the Reviewer causes\nsevere degradation in MedQA (−26.70), MMLU-\nMed (−23.78), and PubMedQA (−5.65). Con-\nversely, MedDDx-Expert (SAQ) shows a surpris-\ning accuracy increase, suggesting that conservative\nverification can introduce a trade-off between fac-\ntual precision and answer completeness when KG\nevidence is sufficiently dense.\nTo further evaluate the Reviewer’s effectiveness,\nwe ablated it from ReGraM and evaluated the\nmodel on MedHallu and Med-HALT hallucina-\ntion benchmarks.\nAs shown in Table 12, the\nmodel without the Reviewer fails catastrophically\non MedHallu (ACC=0, hallucination=100%), un-\nderscoring the role of verification as a support-\ning mechanism for enhancing hallucination robust-\nness—particularly when KG evidence is sparse or\nambiguous. While not the primary driver of factual\naccuracy, the verification layer contributes to stabi-\nlizing generation under uncertain contexts within\nthe region-first structure.\nC.5\nEfficiency Analysis (Extended Results)\nSetting\nKGARevion (s/item)\nReGraM (s/item)\nMCQ\n1.85\n1.00\nSAQ\n3.26\n3.90\nTable 13: Average inference time per item under MCQ and\nSAQ settings\nC.6\nInteraction Between Domain Prior and\nMulti-hop Reasoning\nDomain Prior\nMultihop\nACC\n∆\nO\nO\n51.03\n0.00\nX\nO\n2.91\n-48.12\nO\nX\n19.80\n-31.23\nX\nX\n12.17\n-38.86\nTable 14: Interaction between domain prior and multi-hop\nreasoning on MedQA (SAQ). Removing either component\nsignificantly reduces performance, and removing both causes\nthe most severe degradation\nC.7\nDomain-wise Hallucination Trends\nTo further investigate the relationship between KG coverage\nand hallucination robustness, we report in Table 15 the\ndomain-wise inconsistency scores (IS) for each dataset under\nthe SAQ setting. While domains such as Drug_Therapy and\nGene_Protein (with relatively high KG coverage) generally\nexhibit lower hallucination rates, this pattern does not hold\nuniformly—e.g., the Integrated domain has the highest KG\ncoverage but a relatively high mean IS. This suggests that\nalthough KG coverage may contribute to consistency, it is not\nthe sole determining factor; factors such as evidence\nredundancy, relation granularity, or prompt design may also\nplay important roles.\n16\n"}, {"page": 17, "text": "Dataset\nDrug_Therapy\nGene_Protein\nDisease_Symptom\nPathway_Metab\nIntegrated\nMedDDx-B\n0.0887\n0.0455\n0.0769\n0.1429\n0.0784\nMedDDx-I\n0.0370\n0.0679\n0.0520\n0.0400\n0.0000\nMedDDx-E\n0.0000\n0.1198\n0.0803\n0.0526\n0.0347\nMedQA\n0.1014\n0.0833\n0.0628\n0.0000\n0.2500\nAfrimedQA\n0.0806\n–\n0.0899\n–\n0.0294\nMMLU\n0.0167\n0.0417\n0.0883\n0.0750\n0.0800\nPubMedQA\n0.0310\n0.0000\n0.0509\n0.0147\n0.0054\nKG Coverage (%)\n68.14\n60.30\n15.66\n26.31\n83.52\nTable 15: Domain-wise hallucination IS vs KG coverage (SAQ). Higher KG coverage correlates with lower hallucination rates,\nwhile sparsely represented domains exhibit greater instability.\nC.8\nDataset-wise Hallucination Trends\nFigure 13: Reduction in hallucination inconsistency score (IS)\nacross datasets under SAQ setting. ReGraM shows consistent\nimprovement across all benchmarks\nC.9\nQualitative Examples (Strengths and a\nKey Bottleneck)\nWe present two step-by-step examples to illustrate (i) when\nReGraM succeeds end-to-end in both MCQ and SAQ, and (ii)\na representative bottleneck case where multi-hop evidence is\ncorrectly accumulated but the final decision step becomes\ninconsistent with the evidence.\nExample 1 (index=50): End-to-end success in both MCQ\nand SAQ.\nQuery. What are the diseases associated with the NGLY1\ngene, and what clinical outcomes does this link entail?\nOptions. (A) ALG1-CDG (B) lipoyl transferase 1 defi-\nciency (C) NGLY1-deficiency (D) aminoacylase 1 defi-\nciency. Gold: C\nStep 1: Query typing and decomposition. Domain:\nGENE_PROTEIN.\nHop decomposition: single hop (fall-\nback run).\nStep 2:\nRegion selection (Top verified triplets).\nVerified KG triplets (subset): (1) (NGLY1-deficiency,\nphenotype present, Developmental regression);\n(2)\n(NGLY1-deficiency,\nphenotype\npresent,\nCerebral\natrophy);\n(3)\n(NGLY1-deficiency,\nphenotype present, Alacrima).\nStep 3: Hop-level sub-answer. Sub-answer1: NGLY1\nis associated with NGLY1-deficiency, with neurologi-\ncal/systemic phenotypes such as developmental regres-\nsion and cerebral atrophy.\nStep 4: Final outputs. MCQ: Pred C (correct).\nSAQ:\nMapped to option C and judged correct under the SAQ\nprotocol.\nTakeaway. ReGraM can solve the same clinical-genetics\nquery end-to-end across both MCQ and SAQ by ground-\ning outcomes in a compact, query-aligned region.\nExample 2 (index=49): Evidence is correctly accumulated,\nbut the final decision is inconsistent.\nQuery. What are potential diagnoses for chronic cystitis\nsymptoms with reduced bladder capacity and frequent\nurination?\nOptions (MCQ). (A) cystitis cystica (B) chronic cystitis\n(C) cystitis (D) interstitial cystitis. Gold: D\nStep 0: Hop decomposition (Adaptive_MultiHop).\n• Hop 1: candidate diagnoses for chronic cystitis symp-\ntoms\n• Hop 2: diagnoses linked to reduced capacity and fre-\nquent urination\n• Hop 3: final diagnosis matching the combined profile\nStep 1: Region evidence (verified triplets; subset).\n• (interstitial\ncystitis, phenotype\npresent,\nFunctional abnormality of the bladder)\n• (interstitial\ncystitis, phenotype\npresent,\nPollakisuria)\n• (interstitial\ncystitis, phenotype\npresent,\nNocturia)\n• (interstitial\ncystitis, phenotype\npresent,\nUrinary bladder inflammation)\nStep 2: Hop-level sub-answers (compressed).\n• Sub-answer1: Candidate diagnoses include intersti-\ntial cystitis and related cystitis subtypes.\n• Sub-answer2: Evidence links interstitial cystitis to\nfrequent urination and bladder functional abnormality\n(reduced capacity).\n• Sub-answer3: The combined symptom profile is most\nconsistent with interstitial cystitis (Option D).\nFinal output (MCQ). Predicted option: B (incorrect;\nGold D).\nTakeaway. Region-first multi-hop reasoning success-\nfully concentrates evidence toward the correct diagnosis\n17\n"}, {"page": 18, "text": "(D) at the hop level, but the final discrete decision can\nstill become inconsistent with the accumulated evidence,\nmotivating stronger evidence-aware aggregation and cal-\nibrated selection.\nD\nRegion-Constrained Multi-Hop\nReasoning Algorithm\nAlgorithm 1: Region-Constrained Multi-Hop Reason-\ning in ReGraM\nInput. User query Q, knowledge graph G, language\nmodel M\nOutput. Final answer A, evidence map E\nPhase 1: Query Processing\n• D ←DOMAINCLASSIFICATION(Q)\n• {q1, . . . , qk} ←QUERYDECOMPOSITION(Q) (k ≤\n3)\n• C ←∅\n(context: hop-level Q/A memory)\n• E ←∅\n(evidence map)\nPhase 2: Region Construction & Hop-wise Reasoning\n(for each sub-question qi)\n• Esci ←ENTITYLINKUMLS(qi)\n• Efuzzy ←FUZZYMATCH(qi, G)\n• Eexp ←SYNONYMEXPAND(Esci ∪Efuzzy)\n• Tcand ←EXTRACTTRIPLETS(Eexp, G)\n• Eq ←WEIGHTEDMMR(Tcand, D, K = 15)\n(Top-\nK triplets)\n• Vq ←{ v | v appears in a head/tail of some t ∈\nEq }\n• Gq ←(Vq, Eq)\n(query-aligned reasoning region)\n• Fi ←TEXTIFY(Eq)\n(facts shown to M)\n• Nfacts ←|Eq|\nand determine mode:\n– If Nfacts ≥10, then KG-STRICT\n– If 0 < Nfacts < 10, then HYBRID\n– Else, LLM-GUESS\n• (ai, Ti) ←REASON(qi, Fi, C, mode; Gq)\n• Hard constraint: all KG lookups / entity expansion\ninside REASON satisfy\nLOOKUP(·) ⊆Gq and generated entities must be\nin Vq.\n• If\nmode\n=\nHYBRID:\n(ai, Ti)\n←\nREVIEWERVERIFY(qi, ai, Ti; Vq, R)\n(R: allowed\nrelations)\n• C ←C ∪{(qi, ai)},\nE ←E ∪Ti\nPhase 3: Final Synthesis\n• A ←SYNTHESIZE(Q, C, E)\n• return A, E\n18\n"}]}