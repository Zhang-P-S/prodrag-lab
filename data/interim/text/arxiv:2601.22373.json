{"doc_id": "arxiv:2601.22373", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.22373.pdf", "meta": {"doc_id": "arxiv:2601.22373", "source": "arxiv", "arxiv_id": "2601.22373", "title": "Stability-Aware Prompt Optimization for Clinical Data Abstraction", "authors": ["ArinbjÃ¶rn Kolbeinsson", "Daniel Timbie", "Sajjan Narsinghani", "Sanjay Hariharan"], "published": "2026-01-29T22:30:35Z", "updated": "2026-01-29T22:30:35Z", "summary": "Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.22373v1", "url_pdf": "https://arxiv.org/pdf/2601.22373.pdf", "meta_path": "data/raw/arxiv/meta/2601.22373.json", "sha256": "234fde093898030a7e0ab018922b6b4b0150519c24bdb09f5b1a4b78ad6f7977", "status": "ok", "fetched_at": "2026-02-18T02:20:07.167478+00:00"}, "pages": [{"page": 1, "text": "Stability-Aware Prompt Optimization\nfor Clinical Data Abstraction\nArinbjÃ¶rn Kolbeinsson\nCentury Health\narinbjorn.kolbeinsson@century.health\nDaniel Timbie\nCentury Health\ndaniel.timbie@century.health\nSajjan Narsinghani\nCentury Health\nsajjan.narsinghani@century.health\nSanjay Hariharan\nCentury Health\nsanjay.hariharan@century.health\nAbstract\nLarge language models used for clinical abstraction are sensitive to\nprompt wording, yet most work treats prompts as fixed and studies\nuncertainty in isolation. We argue these should be treated jointly.\nAcross two clinical tasks (MedAlign applicability/correctness and\nMS subtype abstraction) and multiple open and proprietary mod-\nels, we measure prompt sensitivity via flip rates and relate it to\ncalibration and selective prediction. We find that higher accuracy\ndoes not guarantee prompt stability, and that models can appear\nwell-calibrated yet remain fragile to paraphrases. We propose a\ndual-objective prompt optimization loop that jointly targets accu-\nracy and stability, showing that explicitly including a stability term\nreduces flip rates across tasks and models, sometimes at modest\naccuracy cost. Our results suggest prompt sensitivity should be an\nexplicit objective when validating clinical LLM systems.\n1\nIntroduction\nLarge language models (LLMs) are increasingly used for clinical\nabstraction over electronic health record (EHR) text, including phe-\nnotype classification, applicability judgements, and response cor-\nrectness assessment. This work draws on our experience operating\nCHARM, a production clinical abstraction system deployed at Cen-\ntury Health for phenotype classification and chart review tasks.\nIn deployed systems, however, the model is only one part of the\nstack. The same underlying model is wrapped in different prompts:\nvendor defaults, institution-specific templates, and local tweaks by\nteams trying to â€œmake it behave.â€ In this setting, both predictive\nuncertainty and prompt sensitivity, i.e., how much model behaviour\nchanges under semantically equivalent prompt edits, become cen-\ntral to safety and trust. A system that appears well calibrated under\none prompt but becomes erratic under another is difficult to validate\nand harder to monitor once deployed.\nMost work on medical LLMs has focused on accuracy and, in-\ncreasingly, on calibration or abstention mechanisms, but largely\ntreats the prompt as a fixed design choice. Existing studies charac-\nterise calibration and uncertainty on clinical QA or summarisation\nbenchmarks, or explore selective prediction policies that abstain\nwhen confidence is low, yet they do so under a single prompt tem-\nplate and a single backend. As a result, we know relatively little\nabout how uncertainty estimates behave when prompts change, or\nwhether seemingly â€œwell calibratedâ€ models remain reliable once\nthe surface form of the instruction is perturbed. This is problematic\nin realistic pipelines, where prompts differ between vendors, are\niteratively tweaked by domain experts, or must be adapted to fit\nwithin product constraints.\nIn this paper we argue that, for clinical abstraction tasks where\ncorrectness is judged against unstructured evidence such as note\nexcerpts or visit summaries, prompting and uncertainty should be\ntreated as a joint problem rather than as separate concerns. Con-\ncretely, we study two grounded datasets and ask three questions:\n(i) how sensitive are different open and proprietary backends to\nprompt wording at the instance level on these tasks; (ii) how do\nstandard uncertainty signals (probabilities, conformal sets) behave\nunder prompt perturbations; and (iii) can we exploit sensitivity\ninformation to drive a dual-objective prompt optimisation loop that\ntrades off task performance against prompt stability.\nFigure 1: Stability-aware prompt optimization loop. Starting from an initial prompt, we evaluate accuracy and flip rate (measured\nacross paraphrased variants), identify failure cases, and use an LLM to generate candidate prompts conditioned on these failures.\nCandidates are scored on a joint objective balancing performance and stability, and the best-scoring prompt is selected for the\nnext iteration.\narXiv:2601.22373v1  [cs.CL]  29 Jan 2026\n"}, {"page": 2, "text": "ArinbjÃ¶rn Kolbeinsson, Daniel Timbie, Sajjan Narsinghani, and Sanjay Hariharan\nOur contributions are threefold.\nâ€¢ We characterise prompt sensitivity across clinical abstrac-\ntion tasks and multiple backends, measuring flip rates and\ncalibration under prompt perturbations for both open-weight\nand proprietary models.\nâ€¢ We analyse the relationship between prompt stability and\nmodel uncertainty, finding that stable predictions tend to\nbe more confident, though uncertainty alone does not fully\npredict sensitivity.\nâ€¢ We propose a dual-objective prompt optimisation loop that\nexplicitly balances accuracy and stability, demonstrating\nthat joint optimisation reduces flip rates across tasks and\nmodels.\n2\nRelated work\n2.1\nPrompt sensitivity and multi-prompt\nevaluation\nA growing body of work shows that LLM behaviour is highly sen-\nsitive to seemingly minor changes in prompt wording. Zhuo et\nal. introduce ProSA, a framework for prompt sensitivity analysis\nthat operates at the instance level, defining PromptSensiScore to\nquantify disagreement across semantically equivalent prompts and\nrelating this to decoding confidence, model size, and task difficulty\n[16]. ProSA demonstrates that larger models are not uniformly\nmore robust and that subjective, judge-based evaluations are par-\nticularly vulnerable to prompt perturbations on complex reasoning\ntasks.\nAt the task and benchmark level, Mizrahi et al. argue that single-\nprompt evaluation is fundamentally unreliable, and call for multi-\nprompt LLM evaluation [10]. They construct large sets of para-\nphrased instructions for existing benchmarks and show that both\nabsolute scores and model rankings can change dramatically across\ntemplates. They propose metrics such as maximum performance,\naverage performance and a combined score (CPS) to summarise\nbehaviour over a prompt set and highlight that current â€œstate-of-\nthe-artâ€ claims are highly prompt-dependent. Together, ProSA and\nmulti-prompt evaluation provide complementary evidence that\nprompt formulation is a major, under-controlled source of vari-\nance, but they stop at measurement rather than proposing concrete\noptimisation procedures.\nOther work analyses prompt sensitivity in more specialised set-\ntings (e.g. prompt engineering for natural language interfaces in\nsoftware visualisation or multimodal models), but these typically\neither rely on small, domain-specific prompt sets or treat sensitivity\nqualitatively rather than building an optimisation objective around\nit [1]. None of these studies connect prompt sensitivity to uncer-\ntainty estimation or selective prediction, and none use sensitivity\nsignals to drive where to invest optimisation effort.\n2.2\nPrompt optimisation and robustness\nA large literature targets automatic prompt optimisation for per-\nformance, usually on standard NLP tasks, under a fixed train/test\ndistribution. Representative methods include RL-based optimisation\nof discrete prompts (e.g. RLPrompt) and search-based or LLM-in-\nthe-loop rewriting approaches, which treat prompts as a policy\nto be tuned against a task-specific reward. These methods almost\nexclusively optimise a single metric such as accuracy, BLEU or task\nreward, and ignore stability across paraphrases or distributional\nshifts.\nMore recently, several lines of work explicitly study robust\nprompt optimisation. Li et al. formulate robust prompt optimisation\nunder distribution shift, arguing that prompts tuned on a labelled\nsource group often fail on an unlabelled target group; their Gener-\nalised Prompt Optimisation (GPO) framework incorporates unla-\nbelled target data to improve worst-group performance while main-\ntaining source performance [5]. DRO-InstructZero treats instruc-\ntion optimisation as a distributionally robust optimisation problem,\nmaximising worst-case expected utility over an f-divergence ball\naround the evaluation distribution to obtain prompts that are more\nreliable under real-world uncertainty [6]. Closely related work in\nsafety focuses on robust optimisation of system prompts against\njailbreaking or prompt injection attacks, where the robustness objec-\ntive is defined in terms of adversarial success rather than semantic\nstability (e.g., BlackDAN: 14; AEGIS: 8).\nThere is also an emerging strand of multi-objective prompt opti-\nmisation. SoS (â€œSurvival of the Safestâ€) introduces an evolutionary\nframework that optimises prompts for both performance and safety\n[12], while ParetoPrompt formulates prompt optimisation as a\nmulti-objective reinforcement-learning problem, exploring a Pareto\nfront of prompts that trade off multiple metrics [15]. These meth-\nods show that multi-objective formulations are useful in prompt\noptimisation, but they typically focus on performance vs safety or\nefficiency, not on semantic prompt stability.\nClosest to our focus, Chen et al. introduce Prompt Stability Mat-\nters, which argues that automatic prompt generators for general-\npurpose multi-agent systems should account for prompt stability,\ndefined as consistency of model responses across repeated execu-\ntions for the same prompt [2]. They propose semantic stability as\na criterion, train a LLaMA-based evaluator to score it, and build a\nstability-aware prompt generation system that uses stability feed-\nback to improve both accuracy and output consistency. However,\ntheir notion of stability is primarily about stochastic reproducibility\nof a fixed prompt, rather than sensitivity to semantically equiva-\nlent prompt variants for the same input, and their experiments are\nconducted on general-purpose multi-agent workflows rather than\nclinical abstraction tasks.\nFinally, error-driven prompt refinement methods such as ProTeGi\nand PO2G use feedback on model mistakes to update prompts, but\nstill optimise a single performance metric without explicit stability\nterms [7, 11]. We bridge these lines of work by using instance-level\nsensitivity signals to guide prompt optimisation on clinical abstrac-\ntion tasks, connecting prompt stability to uncertainty estimation\nin a way that existing frameworks do not.\n3\nMethod\n3.1\nProblem setup and optimisation objective\nIn deployed clinical systems such as CHARM, prompt optimisation\ntypically involves iterative refinement with human feedback. Anno-\ntators review model outputs, provide corrections, and the prompt\nis updated to reduce errors on a labelled evaluation set. This pro-\ncess optimises for task accuracy but does not explicitly account for\n"}, {"page": 3, "text": "Stability-Aware Prompt Optimization\nfor Clinical Data Abstraction\nprompt stabilityâ€”the extent to which predictions remain consistent\nunder semantically equivalent prompt rephrasings.\nWe extend this approach by formulating prompt optimisation\nas a dual-objective problem. Let ğ¹(ğ‘ƒ) denote the task performance\n(e.g., accuracy) of prompt ğ‘ƒ, and let ğ‘†(ğ‘ƒ) denote a stability measure\n(e.g., negative flip rate). We optimise:\nğ½(ğ‘ƒ) = ğœ†perf Â· ğ¹(ğ‘ƒ) + ğœ†stab Â· ğ‘†(ğ‘ƒ)\n(1)\nwhere ğœ†perf and ğœ†stab control the trade-off between performance\nand stability. Setting ğœ†stab = 0 recovers accuracy-only optimisation;\nsetting both weights equal gives balanced joint optimisation.\n3.2\nOptimisation loop\nWe implement an LLM-in-the-loop optimisation procedure that\niteratively refines the prompt to improve the joint objective. Each\niteration proceeds as follows.\nEvaluation. Given the current prompt ğ‘ƒ, we run inference on a\nheld-out evaluation set to compute accuracy. To estimate stability,\nwe generate ğ¾paraphrased variants of ğ‘ƒusing an LLM and measure\nthe flip rateâ€”the proportion of examples whose predictions change\nbetween the base prompt and its paraphrases.\nFailure identification. We identify two types of failure cases: (i)\nhigh-flip examples, where predictions are unstable across prompt\nvariants, and (ii) misclassified examples, where the base prompt\nprediction is incorrect. These cases inform the next round of prompt\ngeneration.\nCandidate generation. An LLM proposes ğ‘candidate prompts\nconditioned on the current prompt, its performance metrics, and\nconcrete failure examples. This targeted feedback encourages edits\nthat address specific error patterns rather than generic rephrasings.\nSelection. Each candidate is scored on the joint objective ğ½(ğ‘ƒ). If\nthe best candidate improves over the current prompt, it is accepted;\notherwise, the iteration makes no update. The loop terminates after\na fixed number of iterations or when no improvement is found for\nseveral consecutive rounds.\nIn our experiments, we compare accuracy-only optimisation\n(ğœ†stab = 0) against joint optimisation (ğœ†perf = ğœ†stab = 0.5) to isolate\nthe effect of the stability term.\n3.3\nUncertainty signals and metrics\nWe extract uncertainty signals differently depending on model\naccess. For local HuggingFace models, we compute class proba-\nbilities from per-token logprobs by taking a softmax over label\nscores, using the top-class probability as a confidence estimate. For\nBedrock-hosted models, which provide only predicted labels, we\nrely on agreement across prompt variants: flip rate serves as a proxy\nfor prediction uncertainty.\nFor models with probability access, we construct conformal pre-\ndiction sets following standard split conformal inference. We use\na nonconformity score of 1 âˆ’ğ‘ƒ(label), set the coverage level to\nğ›¼= 0.1, and partition the held-out set into calibration and evalua-\ntion subsets (50/50 split). We report empirical coverage and average\nset size.\nWe evaluate models along four dimensions: (i) performanceâ€”\naccuracy and F1 by task, with log-loss and Brier score when proba-\nbilities are available; (ii) calibrationâ€”reliability diagrams, expected\ncalibration error (ECE), and maximum calibration error (MCE);\n(iii) sensitivityâ€”flip rate between base and variant prompts, and\nJensenâ€“Shannon divergence between prompt-induced distributions\nfor models with probability access; and (iv) selective predictionâ€”\ncoverageâ€“accuracy curves using probability thresholds or confor-\nmal set size, abstaining when set size exceeds one or confidence\nfalls below a threshold.\n3.4\nStability-Conditioned Uncertainty Analysis\nTo investigate whether prompt sensitivity manifests in model uncer-\ntainty estimates, we analyze the relationship between per-example\nflip rates and conformal prediction set sizes. This addresses a natural\nquestion: do examples that are unstable under prompt perturbation\nalso exhibit higher predictive uncertainty?\nPer-example flip indicator. For each example ğ‘¥ğ‘–in the evaluation\nset, let Ë†ğ‘¦(0)\nğ‘–\ndenote the prediction under the base prompt and Ë†ğ‘¦(ğ‘˜)\nğ‘–\nthe prediction under the ğ‘˜-th paraphrased variant. We define the\nbinary flip indicator and continuous flip rate as:\nflipğ‘–= 1\nh\nâˆƒğ‘˜: Ë†ğ‘¦(ğ‘˜)\nğ‘–\nâ‰ Ë†ğ‘¦(0)\nğ‘–\ni\n,\nflip_rateğ‘–= 1\nğ¾\nğ¾\nâˆ‘ï¸\nğ‘˜=1\n1\nh\nË†ğ‘¦(ğ‘˜)\nğ‘–\nâ‰ Ë†ğ‘¦(0)\nğ‘–\ni\n(2)\nwhere ğ¾is the number of prompt variants.\nConformal set size. For models providing class probabilities, we\nconstruct conformal prediction sets using the base prompt pre-\ndictions. Following standard split conformal inference [13], we\npartition the held-out set into calibration and evaluation subsets\n(50/50 split). For each evaluation example ğ‘¥ğ‘–, the conformal set Cğ‘–\ncontains all labels ğ‘¦such that the nonconformity score 1âˆ’ğ‘ƒ(ğ‘¦| ğ‘¥ğ‘–)\nfalls below the calibrated threshold at level ğ›¼= 0.1. We record the\nset size |Cğ‘–| and whether the true label is covered, ğ‘¦ğ‘–âˆˆCğ‘–.\nStratified analysis. We partition examples into stable (flipğ‘–= 0)\nand unstable (flipğ‘–= 1) groups and report: (i) mean conformal set\nsize per group, (ii) empirical coverage per group, and (iii) Spearman\nrank correlation between flip_rateğ‘–and |Cğ‘–|.\nA positive correlation would indicate that model uncertainty,\nas captured by conformal set size, reflects prompt sensitivity, sug-\ngesting that standard uncertainty quantification partially accounts\nfor instability. Conversely, low or no correlation would imply that\ncalibration and prompt stability measure distinct phenomena, rein-\nforcing the need to optimize for both objectives independently.\nThis analysis is restricted to models with access to output proba-\nbilities (local HuggingFace models). For API-based models provid-\ning only predicted labels, we report group-level flip rates without\nconformal conditioning.\n4\nData\nWe study prompt sensitivity and uncertainty on two chart-grounded\nclinical abstraction settings: MedAlign applicability and correctness,\nand an internal multiple sclerosis (MS) subtype abstraction task\nderived from the internal note corpus. In both cases, models must\nreason over unstructured clinical text (visit notes, summaries) to\n"}, {"page": 4, "text": "ArinbjÃ¶rn Kolbeinsson, Daniel Timbie, Sajjan Narsinghani, and Sanjay Hariharan\nproduce labels that reflect clinician judgement rather than surface\nform alone.\n4.1\nMedAlign\nMedAlign is a clinician-generated benchmark for instruction fol-\nlowing on electronic medical records (EHRs) [3]. The dataset con-\ntains 983 natural-language instructions written by 15 practising\nclinicians across 7 specialties, paired with 276 longitudinal EHR\ntimelines. For 303 instruction-EHR pairs, clinicians provided refer-\nence responses and ranked evaluations of six large language model\noutputs. MedAlign is released as a test-only benchmark under a\nresearch data use agreement and is designed to capture realistic\ninformation-seeking and documentation needs rather than exam-\nstyle question answering.\nWe work with MedAlign v1.3, which includes two additional\nannotation tables that we use to define our tasks. First, an applicabil-\nity table provides an is_applicable field indicating whether a given\ninstruction is actually supported by the information present in\nthe chart. Second, a clinician-reviewed responses table contains bi-\nnary_correct labels indicating whether a particular model response\nis judged correct or incorrect given the same instruction and chart\ncontext.\nFrom these tables we derive two binary classification tasks:\nMedAlign applicability: â€œIs the instruction supported by the\nchart?â€, with labels {Yes, No} taken directly from is_applicable.\nMedAlign correctness: â€œDoes the model response align with\nclinician judgement for this instruction and chart?â€, with labels\n{Correct, Incorrect} mapped from binary_correct âˆˆ{0, 1}.\nFor each record we assemble a textual input consisting of the\ninstruction, the chart evidence, the clinician reference summary\nwhen available, and the model response (for correctness). Chart\nevidence is prefixed with a â€œChart evidence:â€ header and separated\nfrom the instruction and other sections by blank lines. When a\nresponse is missing we insert the literal string â€œ<empty response>â€\nso that true blanks can be distinguished from parsing errors. All text\nis decoded as UTF-8 and we preserve original casing and punctua-\ntion. We drop rows with invalid or missing labels and deduplicate\nrecords by instruction identifier, model name and annotator, where\napplicable.\nBecause MedAlign is explicitly test-only, we do not use it for\nany parameter tuning or prompt pre-training. Instead, we cap the\nevaluation subset at 100 examples for applicability and 200 for\ncorrectness. For each task we draw a single subset of labelled rows\nand use the same subset and random 50/50 calibrationâ€“evaluation\nsplit across all models and prompt variants. This design keeps\ncomparisons fair while keeping computational cost and manual\ninspection effort manageable.\n4.2\nMS subtype\nOur second dataset is an internal EHR-derived corpus focusing\non MS subtype abstraction from neurology notes. Clinically, MS\nis commonly described in terms of course phenotypes such as\nrelapsingâ€“remitting MS (RRMS), primary progressive MS (PPMS)\nand secondary progressive MS (SPMS), with clinically isolated syn-\ndrome and â€œnot documentedâ€ or â€œunspecifiedâ€ categories also used\nin practice [4, 9]. In routine care, this information is often encoded\ninconsistently across structured fields and free-text documentation,\nwhich makes it a natural target for LLM-based abstraction.\nThe MS subtype dataset is constructed from de-identified notes\nfor patients with a confirmed MS diagnosis. Each note, or visit-level\nbundle of notes, is paired with a target label reflecting the docu-\nmented subtype at that time. For this study we formulate a six-way\nclassification problem with the label set: Clinically Isolated Syn-\ndrome (CIS), Relapsing-Remitting MS (RRMS), Primary Progressive\nMS (PPMS), Secondary Progressive MS (SPMS), Not Documented,\nand Unknown. Labels are derived from a combination of structured\nphenotype modifiers and manual review by domain experts. We do\nnot use imaging or laboratory data, and we do not attempt to infer\nlatent phenotype from longitudinal trajectories; the task is strictly\nto abstract what is documented in the note.\nAll internal data notes are passed through an institutional de-\nidentification pipeline that removes direct identifiers (names, dates\nof birth, contact details) and obvious quasi-identifiers. We retain\nsection headings and paragraph breaks but do not perform addi-\ntional clinical concept normalisation. As with MedAlign, we treat\nthe dataset as a held-out evaluation corpus for this study. To keep\nexperiments comparable in scale, we cap the internal data note\nevaluation subset at 100 notes per run. We again use a single held-\nout set shared across all models and prompt variants, and apply a\nrandom 50/50 calibrationâ€“evaluation split for conformal prediction.\n4.3\nTask construction and edge cases\nAcross both datasets we focus on tasks where correctness is judged\nwith respect to unstructured evidence, not purely against pre-\nexisting structured labels. In MedAlign applicability, the ground-\ntruth label reflects whether the requested operation is supported\nby the chart as presented. Some instructions are inherently non-\napplicable, for example, tooling prompts or future protocol re-\nquests that cannot be resolved from the current EHR snapshot,\neven though their phrasing appears reasonable. These cases intro-\nduce a degree of annotation noise and highlight that applicability\njudgements depend on chart coverage, not only on linguistic plau-\nsibility.\nIn MedAlign correctness, labels reflect clinician assessments\nof particular model outputs. Responses may be partially correct,\nover-confident or hallucinated; the binary label encodes clinical ac-\nceptability rather than exact string match to a reference. This makes\nthe task distribution different from generic question answering and\ncloser to real-world audit of model behaviour.\nIn the MS subtype, the â€œNot documentedâ€ class is intended to\ncapture notes where no explicit subtype is present. In practice, some\nnotes contain ambiguous or implicit descriptions of disease course.\nWe retain these borderline examples but flag them for sensitivity\nanalyses, as they are precisely the cases where over-interpretation\nby a model could be harmful.\nFor all tasks we retain basic metadata such as anonymised patient\nor visit identifiers, instruction identifiers and model names (for\nMedAlign correctness). This enables stratified analyses by visit,\ninstruction type or source model, and supports future work on\ntemporal drift and per-patient aggregation.\n"}, {"page": 5, "text": "Stability-Aware Prompt Optimization\nfor Clinical Data Abstraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFlip rate (across variants)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMargin\nFigure 2: Prediction margin versus flip rate for MedGemma\n4B on MedAlign applicability. Stable examples (flip rate = 0)\nhave high margins, indicating confident predictions. As flip\nrate increases, margins decrease and become more variable,\nsuggesting that uncertain predictions are more prone to flip-\nping under prompt variation.\n4.4\nEthical and privacy considerations\nBoth tasks involve sensitive clinical information. MedAlign is de-\nidentified and provided under a research data use agreement by its\noriginal authors, who explicitly position it as a test-only benchmark\n[3]. We follow this guidance and do not attempt to reconstruct\nor augment the underlying EHR data. The MS subtype corpus is\nderived from an internal EHR system and is de-identified under\nthe host institutionâ€™s governance processes. Access is restricted\nunder an internal data-use protocol that prohibits re-identification\nattempts and external sharing of raw notes. All experiments in\nthis work are observational and retrospective; we do not generate\npatient-facing outputs, and we use the datasets solely to study\nmodel behaviour, uncertainty and prompt sensitivity.\n5\nExperiments and results\n5.1\nE1: Stability and Model Certainty\nTo test whether prompt stability relates to model certainty, we\nanalyze the relationship between per-example flip rates and pre-\ndiction margin (the difference between top-two class probabilities).\nFor each example, we compute the flip rate across ğ¾= 3 prompt\nvariants and the margin from the base promptâ€™s output distribution.\nFigure 2 shows results for MedGemma 4B on MedAlign appli-\ncability as box plots grouped by flip rate. Examples that never flip\n(flip rate = 0) have margins concentrated near 1.0, indicating high\nconfidence. As flip rate increases, median margin decreases and\nvariability grows. This pattern supports the intuition that stable\npredictions are more trustworthy: when the model is certain, it is\nalso robust to prompt variation.\n5.2\nE2: Accuracy Does Not Guarantee Stability\nA natural question is whether improving accuracy automatically\nimproves prompt stability. To test this, we generate multiple seman-\ntically equivalent prompt variants for a single task and model, eval-\nuate each on accuracy and flip rate, and examine whether higher-\naccuracy prompts are also more stable.\nFigure 3 shows results for Llama 3 70B on MedAlign applica-\nbility. Each point represents a candidate prompt; the x-axis shows\naccuracy and the y-axis shows mean flip rate across ğ¾= 3 para-\nphrased variants. If accuracy implied stability, we would expect\na negative correlation with points concentrated in the upper-left\n(low accuracy, high flip) and lower-right (high accuracy, low flip)\nregions.\nInstead, we observe substantial vertical spread: prompts with\nsimilar accuracy can have very different flip rates. Some high-\naccuracy prompts are unstable (upper-right region), while some\nlower-accuracy prompts are relatively stable. This decoupling sug-\ngests that optimizing for accuracy alone does not guarantee prompt\nrobustness, motivating explicit stability objectives. We observe sim-\nilar patterns across tasks and models.\nFigure 3: Accuracy vs flip rate for MedAlign applicability\nusing Llama 3 70B. Each point is one candidate prompt; the\ny-axis is the mean flip rate across three paraphrased vari-\nants. The vertical spread at similar accuracy levels illustrates\nthat improving accuracy alone does not guarantee prompt\nstability.\n5.3\nE3: Joint Optimization Improves Stability\nWe run the stability-aware prompt optimizer across three clini-\ncal tasks (MedAlign applicability, MedAlign correctness, and MS\nsubtype) using three Bedrock models (Claude Haiku 4.5, Llama-\n4 Maverick, GPT-OSS-20B). Each setting is repeated with three\nrandom seeds on a fixed stratified subset (ğ‘= 50). We compare\naccuracy-only optimization (ğœ†perf = 1, ğœ†stab = 0) against joint op-\ntimization (ğœ†perf = ğœ†stab = 0.5). Table 1 reports mean Â± std over\nseeds for end-of-run accuracy and flip rate. Figure 4 visualizes the\noptimization trajectories across all tasks and models.\nAcross the nine model-task combinations, joint optimization\nimproves stability in eight cases. The largest gains occur when\n"}, {"page": 6, "text": "ArinbjÃ¶rn Kolbeinsson, Daniel Timbie, Sajjan Narsinghani, and Sanjay Hariharan\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nAccuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nFlip rate\nMedAlign Applicability\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAccuracy\nMedAlign Correctness\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nMS Subtype\nHaiku 4.5\nGPT-OSS-20B\nLlama-4 Maverick\nAcc-only\nJoint\nFigure 4: Summary of E3 optimizer sweeps across tasks and models. Each segment shows the mean startâ†’end trajectory (3 seeds)\nin accuracyâ€“flip space; endpoints include error bars. Color encodes model; line style/marker encodes objective (accuracy-only\nvs joint). Adding a stability term systematically shifts endpoints toward lower flip rates, often with modest accuracy trade-offs.\nTask / Model\nSetting\nAcc. (end)\nFlip (end)\nMedAlign Applicability\nHaiku 4.5\nAcc-only\n0.827 Â± 0.047\n0.078 Â± 0.017\nHaiku 4.5\nJoint\n0.787 Â± 0.025\n0.047 Â± 0.025\nLlama-4 Maverick\nAcc-only\n0.633 Â± 0.025\n0.309 Â± 0.098\nLlama-4 Maverick\nJoint\n0.527 Â± 0.009\n0.036 Â± 0.014\nGPT-OSS-20B\nAcc-only\n0.653 Â± 0.041\n0.249 Â± 0.067\nGPT-OSS-20B\nJoint\n0.647 Â± 0.047\n0.242 Â± 0.025\nMedAlign Correctness\nHaiku 4.5\nAcc-only\n0.680 Â± 0.059\n0.089 Â± 0.044\nHaiku 4.5\nJoint\n0.653 Â± 0.062\n0.018 Â± 0.006\nLlama-4 Maverick\nAcc-only\n0.607 Â± 0.019\n0.229 Â± 0.090\nLlama-4 Maverick\nJoint\n0.573 Â± 0.034\n0.169 Â± 0.031\nGPT-OSS-20B\nAcc-only\n0.580 Â± 0.043\n0.453 Â± 0.043\nGPT-OSS-20B\nJoint\n0.533 Â± 0.009\n0.262 Â± 0.041\nMS Subtype\nHaiku 4.5\nAcc-only\n0.833 Â± 0.025\n0.073 Â± 0.048\nHaiku 4.5\nJoint\n0.840 Â± 0.016\n0.082 Â± 0.030\nLlama-4 Maverick\nAcc-only\n0.767 Â± 0.062\n0.220 Â± 0.000\nLlama-4 Maverick\nJoint\n0.680 Â± 0.028\n0.196 Â± 0.047\nGPT-OSS-20B\nAcc-only\n0.873 Â± 0.009\n0.171 Â± 0.017\nGPT-OSS-20B\nJoint\n0.880 Â± 0.016\n0.156 Â± 0.003\nTable 1: E3 sweep results: accuracy-only vs joint optimization\n(mean Â± std over 3 seeds). Joint optimization consistently\nreduces flip rate, with variable accuracy trade-offs depending\non model and task.\nbaseline stability is poor: for Llama-4 Maverick on MedAlign appli-\ncability, flip rate drops from 0.309 to 0.036, and for GPT-OSS-20B\non MedAlign correctness, from 0.453 to 0.262. The accuracy trade-\noff varies: in some cases (GPT-OSS-20B on MS subtype) accuracy\nslightly improves under joint optimization, while in others (Llama-4\nMaverick on applicability) it decreases. The one exception is Haiku\non MS subtype, where baseline stability is already high and joint\noptimization shows no additional benefit.\n6\nDiscussion\nOur results support three main findings about prompt stability in\nclinical LLM systems, which we summarize below and then discuss\nin terms of their practical implications.\nAccuracy and stability are decoupled at the prompt level. E2 demon-\nstrates that within a single model, prompts with similar accuracy\ncan have very different flip rates. This is distinct from the model-\nlevel trend (Appendix, Figure 7) where larger models tend to be\nboth more accurate and more stable. The implication is that select-\ning a capable model does not guarantee prompt-level robustness;\nprompt engineering must explicitly account for stability.\nStability correlates with model certainty. E1 shows that stable\nexamples, those whose predictions do not flip under prompt para-\nphrases, tend to have higher prediction margins (Spearman ğœŒ=\n0.204 between margin and stability). This supports the intuition\nthat confident predictions are more trustworthy: when the model\nis certain, it is also more robust to prompt variation. However, the\ncorrelation is modest, indicating that certainty alone is not a reliable\nproxy for stability.\nStability is optimizable. E3 demonstrates that adding an explicit\nstability term to the optimization objective improves stability in\n8 of 9 model-task combinations, sometimes with modest accuracy\ntrade-offs. This suggests that prompt stability should be treated as\na first-class objective rather than assumed to follow from accuracy\nimprovements.\nBridging prompt sensitivity and prompt optimization. Prior work\non prompt sensitivity [10, 16] establishes that LLM behavior varies\nsubstantially across paraphrased instructions, but stops at mea-\nsurement. Separately, prompt optimization methods [5, 11] focus\non improving accuracy without accounting for stability. Our work\nbridges these literatures by using sensitivity signals to drive opti-\nmization, showing that stability can be explicitly targeted rather\nthan merely observed.\nA complementary signal for clinical AI validation. Current best\npractices for validating clinical LLM systems emphasize accuracy,\ncalibration, and selective prediction [3]. Our findings suggest prompt\nstability should be added to this checklist. A system may appear\nwell-calibrated under its development prompt yet behave erratically\n"}, {"page": 7, "text": "Stability-Aware Prompt Optimization\nfor Clinical Data Abstraction\nwhen that prompt is paraphrased by a downstream team. Stability\ntesting provides a complementary validation signal that existing\nmetrics do not capture.\nImplications for multi-team deployment. In realistic clinical de-\nployments, the same underlying model is often wrapped in differ-\nent prompts by different stakeholders: vendor defaults, institution-\nspecific templates, and local modifications by clinical informaticists.\nOur results suggest this practice carries hidden risk. A prompt vali-\ndated by one team may become unstable when another team makes\nseemingly innocuous edits. Explicitly optimizing for stability, or\nat minimum measuring it, could reduce unexpected failures when\nprompts drift across teams or over time.\nStability as a trust signal. The correlation between stability and\nmodel certainty (E1) suggests a practical heuristic: predictions that\nare robust to prompt variation may be more trustworthy than those\nthat flip. This aligns with intuitions from ensemble methods, where\nagreement across models signals reliability. Here, agreement across\nprompts serves an analogous role. While the correlation is modest,\nit provides initial evidence that stability could inform selective\nprediction policies, abstaining on examples that are both uncertain\nand prompt-sensitive.\nIntegration into production. These findings have informed ongo-\ning development of the CHARM system at Century Health, where\nstability metrics are now tracked alongside accuracy during prompt\nvalidation. When prompts are updated or new models are evaluated,\nwe measure flip rates across paraphrased variants as part of the\nrelease process, flagging prompts that show high instability for\nadditional review before deployment.\n6.1\nLimitations\nScale. Our experiments use capped evaluation sets (50â€“200 ex-\namples per task) and only ğ‘˜= 3 prompt paraphrases for flip rate es-\ntimation. The stability-calibration correlation (ğœŒ= 0.204, ğ‘= 0.056)\nis borderline significant, and larger samples may reveal stronger or\nweaker relationships. The E3 sweeps use ğ‘= 50 examples with 3\nseeds, which limits statistical power for detecting small effects.\nParaphrase generation. Flip rates depend on the paraphrases used.\nWe generate variants via LLM rewriting, which may not reflect\nthe full range of prompt variations encountered in deployment\n(e.g., vendor templates, institution-specific formatting, non-native\nspeaker phrasings). The stability we measure is relative to our\nparaphrase distribution, not absolute.\nStability-calibration analysis restricted to HF models. The E1 anal-\nysis joining flip rates with conformal set sizes is only possible for\nmodels providing output probabilities (HuggingFace). For Bedrock\nmodels (Claude, Llama, Qwen), we can measure flip rates but cannot\nassess the stability-calibration relationship. This limits generaliz-\nability to API-based deployments.\nSingle optimization method. We evaluate one LLM-in-the-loop\noptimizer architecture. We did not compare against alternative\nprompt optimization methods (evolutionary search, gradient-based,\nrandom search), so we cannot claim our specific approach is optimal,\nonly that incorporating a stability term into some optimization\nobjective is beneficial.\nNo downstream clinical validation. We show that joint optimiza-\ntion reduces flip rates, but we do not demonstrate that this translates\nto improved clinical outcomes or reduced harm. The link between\nprompt stability and patient safety remains indirect.\nTask scope. Our experiments cover two clinical abstraction set-\ntings (MedAlign and MS subtype). Generalization to other clinical\nNLP tasks, such as entity extraction, temporal reasoning, or multi-\ndocument summarization, remains untested.\nReferences\n[1] Daniel Atzberger, Adrian Jobst, Maksim Tytarenko, Willy Scheibel, JÃ¼rgen\nDÃ¶llner, and Tobias Schreck. Analyzing the sensitivity of prompt engineer-\ning techniques in natural language interfaces for 2.5d software visualization.\nIn WWW Companion â€™25: Companion Proceedings of the ACM Web Confer-\nence 2025, pages 1591â€“1595. Association for Computing Machinery, 2025. doi:\n10.1145/3701716.3717813. Appears in the WWW Companion proceedings (work-\nshop context: Prompt Engineering for Pre-Trained Language Models).\n[2] Ke Chen, Yufei Zhou, Xitong Zhang, and Haohan Wang. Prompt stability matters:\nEvaluating and optimizing auto-generated prompt in general-purpose systems.\narXiv, 2025.\n[3] Scott L. Fleming, Alejandro Lozano, William J. Habber, Avinash S. Jindal, Ed-\nuardo P. Reis, Rahul Thapa, Louis Blankemeier, Julian Z. Genkins, Ethan Stein-\nberg, Ashwin Nayak, Birju Patel, Chung-Cheng Chiang, Alison Callahan, Zepeng\nHuo, Sergios Gatidis, Scott J. Adams, Oluwadamilola Fayanju, Shreya J. Shah,\nThomas Savage, Emily Goh, Akshay S. Chaudhari, Nima Aghaeepour, Christo-\npher Sharp, Michael A. Pfeffer, Percy Liang, Jonathan H. Hsieh, Chris Piech,\nand Nigam H. Shah. MedAlign: A clinician-generated dataset for instruction\nfollowing with electronic medical records. Proceedings of the AAAI Conference on\nArtificial Intelligence, 38(20):22021â€“22030, 2024. doi: 10.1609/aaai.v38i20.30205.\n[4] Sylvia Klineova and Fred D. Lublin.\nClinical course of multiple sclerosis.\nCold Spring Harbor Perspectives in Medicine, 8(9):a028928, 2018. doi: 10.1101/\ncshperspect.a028928.\n[5] Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, and Tat-Seng Chua.\nRobust prompt optimization for large language models against distribution shifts.\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP 2023), pages 1539â€“1554. Association for Computational Lin-\nguistics, 2023. doi: 10.18653/v1/2023.emnlp-main.95.\n[6] Yangyang Li. DRO-InstructZero: Distributionally robust prompt optimization\nfor large language models. arXiv, 2025.\n[7] A. J. Lieander, H. Wang, and K. Rafferty. Prompt optimization with two gradients\nfor classification in large language models. AI, 6(8):182, 2025. doi: 10.3390/\nai6080182.\n[8] Ting-Chun Liu, Ching-Yu Hsu, Kuan-Yi Lee, Chi-An Fu, and Hung-yi Lee. AEGIS:\nAutomated co-evolutionary framework for guarding prompt injections schema.\narXiv, 2025.\n[9] Fred D. Lublin, Stephen C. Reingold, Jeffrey A. Cohen, Gary R. Cutter, Per Soel-\nberg SÃ¸rensen, Alan J. Thompson, Jerry S. Wolinsky, Laura J. Balcer, Brenda\nBanwell, Frederik Barkhof, et al. Defining the clinical course of multiple scle-\nrosis: The 2013 revisions. Neurology, 83(3):278â€“286, 2014. doi: 10.1212/WNL.\n0000000000000560.\n[10] Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel\nStanovsky. State of what art? a call for multi-prompt LLM evaluation. Trans-\nactions of the Association for Computational Linguistics, 12:933â€“949, 2024. doi:\n10.1162/tacl_a_00681.\n[11] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.\nAutomatic prompt optimization with â€œgradient descentâ€ and beam search. In\nProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP 2023), pages 7957â€“7968. Association for Computational Lin-\nguistics, 2023. doi: 10.18653/v1/2023.emnlp-main.494.\n[12] Ankita Sinha, Wendi Cui, Kamalika Das, and Jiaxin Zhang. Survival of the\nsafest: Towards secure prompt optimization through interleaved multi-objective\nevolution. In Proceedings of the 2024 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track, pages 1016â€“1027. Association for Computa-\ntional Linguistics, 2024. doi: 10.18653/v1/2024.emnlp-industry.76.\n[13] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning\nin a random world. Springer, 2005.\n[14] Xinyuan Wang, Victor Shea-Jay Huang, Renmiao Chen, Hao Wang, Chengwei\nPan, Lei Sha, and Minlie Huang. BlackDAN: A black-box multi-objective ap-\nproach for effective and contextual jailbreaking of large language models. arXiv,\n2024.\n"}, {"page": 8, "text": "ArinbjÃ¶rn Kolbeinsson, Daniel Timbie, Sajjan Narsinghani, and Sanjay Hariharan\n[15] Guang Zhao, Byung-Jun Yoon, Gilchan Park, Shantenu Jha, Shinjae Yoo, and\nXiaoning Qian. Pareto prompt optimization. In International Conference on\nLearning Representations (ICLR) 2025, 2025. URL https://proceedings.iclr.cc/\npaper_files/paper/2025/hash/13b45b44e26c353c64cba9529bf4724f-Abstract-\nConference.html. Conference proceedings paper.\n[16] Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, and\nKai Chen. ProSA: Assessing and understanding the prompt sensitivity of LLMs.\nIn Findings of the Association for Computational Linguistics: EMNLP 2024, pages\n1950â€“1976. Association for Computational Linguistics, 2024. doi: 10.18653/v1/\n2024.findings-emnlp.108.\nAppendix\nPSS Validation\nTo validate our anchor-based flip-rate metric against the published\nProSA framework, we compute the ProSA-style instance-level prompt\nsensitivity score (PSS) on a small subset using a base prompt plus\nğ‘˜= 3 paraphrases. Figure 5 shows a strong correlation between\nflip-rate and PSS (5 prompts, MedAlign applicability with Haiku),\nindicating our simpler anchor-based metric tracks the symmetric\nPSS measure.\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\n0.11\n0.12\nFlip rate (anchor-based)\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\nPSS (pairwise)\nFigure 5: Flip-rate vs. ProSA-style PSS on a 50-example sub-\nset (5 prompts, ğ‘˜= 3 paraphrases). The strong correlation\nvalidates our anchor-based flip-rate as a practical proxy for\nthe symmetric PSS metric.\nOptimizer Trajectories\nFigure 6 shows representative optimizer trajectories in accuracyâ€“\nflip-rate space, comparing accuracy-only and joint optimization\nsettings across tasks. Each panel traces the sequence of accepted\nprompts, while faint grey candidate points and connectors illustrate\nthe local search space explored at each iteration. Rows correspond\nto tasks and columns to the optimization setting (accuracy-only\nvs. joint accuracy-stability). These trajectories visualize how the\noptimization dynamics differ under the two objectives: accuracy-\nonly runs tend to move horizontally (improving accuracy without\nregard for stability), while joint optimization traces paths that also\npush downward in flip-rate.\nModel-Level Sensitivity and Calibration\nWhile our main experiments focus on the relationship between\nstability and uncertainty at the example level, we also examined\nmodel-level patterns. Figure 7 shows flip rate versus accuracy across\nmodels on MedAlign applicability. Larger models tend to occupy the\nhigh-accuracy, low-flip region, while smaller models cluster in the\nlow-accuracy, high-flip corner. This model-level trend suggests that\ncapacity improvements benefit both metrics, but does not guarantee\nprompt-level stability within a given model (see E2).\nFigure 8 shows conformal prediction coverage and set size trade-\noffs across HuggingFace models. Models achieve similar coverage\ntargets (âˆ¼0.90â€“0.97) with varying abstention costs (set sizes), illus-\ntrating that calibration quality differs substantially across backends.\n"}, {"page": 9, "text": "Stability-Aware Prompt Optimization\nfor Clinical Data Abstraction\nFigure 6: Optimizer trajectories for Claude Haiku 4.5 (seed 1). Each panel shows accepted prompts (solid line) and candidate\nproposals (faint grey) in accuracyâ€“flip-rate space. Left column: accuracy-only optimization; right column: joint optimization.\nRows: MedAlign applicability (top), MedAlign binary (middle), MS subtype (bottom).\n"}, {"page": 10, "text": "ArinbjÃ¶rn Kolbeinsson, Daniel Timbie, Sajjan Narsinghani, and Sanjay Hariharan\nFigure 7: Flip rate vs accuracy trade-off across models on\nMedAlign applicability. Larger models tend to be both more\naccurate and more stable at the model level, though this does\nnot guarantee prompt-level stability.\nFigure 8: Conformal prediction results on MedAlign applica-\nbility across HuggingFace models. Coverage targets can be\nmet with very different abstention costs and retained accu-\nracy.\n"}]}