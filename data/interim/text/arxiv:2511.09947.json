{"doc_id": "arxiv:2511.09947", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.09947.pdf", "meta": {"doc_id": "arxiv:2511.09947", "source": "arxiv", "arxiv_id": "2511.09947", "title": "EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models", "authors": ["Sha Zhao", "Mingyi Peng", "Haiteng Jiang", "Tao Li", "Shijian Li", "Gang Pan"], "published": "2025-11-13T04:27:06Z", "updated": "2025-11-27T14:49:19Z", "summary": "Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.09947v2", "url_pdf": "https://arxiv.org/pdf/2511.09947.pdf", "meta_path": "data/raw/arxiv/meta/2511.09947.json", "sha256": "8c8f783fbab5678d726e3a50017ec06a89d677883e4689c12b8e4a148daaf65b", "status": "ok", "fetched_at": "2026-02-18T02:27:17.906904+00:00"}, "pages": [{"page": 1, "text": "EEGAgent: A Unified Framework for Automated EEG Analysis Using Large\nLanguage Models\nSha Zhao1,2, mingyi peng1,2, Haiteng Jiang3,4,1, Tao Li3,4,1, Shijian Li1,2 Gang Pan1,2,4\n1State Key Laboratory of Brain-machine Intelligence, Zhejiang University\n2College of Computer Science and Technology, Zhejiang University\n3Department of Neurobiology, Affiliated Mental Health Center & Hangzhou\nSeventh People’s Hospital, Zhejiang University School of Medicine\n4MOE Frontier Science Center for Brain Science and Brain-machine Integration, Zhejiang University\nAbstract\nScalable and generalizable analysis of brain activity is es-\nsential for advancing both clinical diagnostics and cogni-\ntive research. Electroencephalography (EEG), a non-invasive\nmodality with high temporal resolution, has been widely used\nfor brain states analysis. However, most existing EEG mod-\nels are usually tailored for individual specific tasks, limiting\ntheir utility in realistic scenarios where EEG analysis often\ninvolves multi-task and continuous reasoning. In this work,\nwe introduce EEGAgent, a general-purpose framework that\nleverages large language models (LLMs) to schedule and\nplan multiple tools to automatically complete EEG-related\ntasks. EEGAgent is capable of performing the key functions:\nEEG basic information perception, spatiotemporal EEG ex-\nploration, EEG event detection, interaction with users, and\nEEG report generation. To realize these capabilities, we de-\nsign a toolbox composed of different tools for EEG prepro-\ncessing, feature extraction, event detection, etc. These capa-\nbilities were evaluated on public datasets, and our EEGAgent\ncan support flexible and interpretable EEG analysis, high-\nlighting its potential for real-world clinical applications.\nIntroduction\nBrain diseases are increasingly recognized as a major global\nhealth challenge, often leading to profound functional im-\npairments and progressive cognitive decline (Steinmetz et al.\n2024). To support early diagnosis and effective intervention,\nthe ability to accurately and efficiently assess brain function\nis of paramount importance. Among existing neurophysi-\nological techniques, electroencephalography (EEG) stands\nout as a widely accessible, non-invasive tool that offers high\ntemporal resolution and low operational cost. By capturing\nelectrical signals generated by neuronal activity via scalp\nelectrodes, EEG provides rich, multi-channel time-series\ndata that encode dynamic information about both physiolog-\nical processes and pathological changes. Despite its value,\nEEG analysis presents substantial challenges. The signals\nare highly non-stationary, exhibit considerable inter-subject\nvariability, and contain complex waveform patterns that are\ndifficult to interpret (Shen and Lin 2019). Manual analysis\nby domain experts remains the standard in clinical prac-\ntice, but this approach is time-consuming, labor-intensive,\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nand inherently subjective. These constraints severely limit\nthe scalability and efficiency of EEG applications, es-\npecially in settings requiring continuous monitoring or\nmulti-faceted analysis.\nRecent advances in machine learning (ML) and deep\nlearning (DL) have enabled more automated and scalable ap-\nproaches to EEG analysis(Zhou et al. 2025b). Modern data-\ndriven models can learn meaningful representations directly\nfrom raw EEG signals and achieve high accuracy across\ntasks such as seizure detection, sleep staging, and neurolog-\nical disorder diagnosis (Li et al. 2025). These approaches\nhave led to notable improvements in analysis speed, ac-\ncuracy, and reproducibility. However, most existing EEG\nmodels remain task-specific and are optimized for iso-\nlated objectives. This leads to the so-called task isolation\nproblem, wherein each model addresses only a narrow slice\nof EEG analysis without considering the broader context\nor the interdependence of concurrent tasks. In clinical sce-\nnarios, EEG analysis often involves multi-task, continuous\nreasoning—for example, detecting epileptiform discharges\nwhile also assessing cognitive states or sleep transitions\n(Zhao et al. 2024). Most current task-specific EEG models\nfail to meet this requirement, necessitating an urgent de-\nvelopment of a unified and flexible framework to address\nthis critical need. Fortunately, recent breakthroughs in large\nlanguage models (LLMs) offer a promising pathway toward\novercoming these limitations (Zhang et al. 2024). With their\npowerful capabilities in task planning, contextual reasoning,\nand tool integration, LLM-powered agents can manage com-\nplex, multi-step workflows. They dynamically interpret user\nintent and interface with external modules, making them\nideal candidates for scheduling multi-task EEG analysis in a\ncoordinated and intelligent manner.\nInspired by this potential, we propose EEGAgent, a\ngeneral-purpose, LLM-enhanced intelligent framework for\nEEG analysis. EEGAgent is designed to bridge the gap\nbetween single-task models and multiple EEG analysis\ndemands in the real world. It achieves this by integrating\nperception, task understanding, and dynamic model schedul-\ning into a unified system. EEGAgent brings together tra-\nditional feature engineering, multiple deep learning back-\nbones, and an LLM-based policy planning engine to en-\nable end-to-end coordination and execution of diverse EEG\ntasks. Through this integration, EEGAgent performs mod-\narXiv:2511.09947v2  [cs.LG]  27 Nov 2025\n"}, {"page": 2, "text": "ular and adaptive scheduling, dynamically selecting appro-\npriate pipelines based on the characteristics of the input\ndata and the goals of the analysis. Furthermore, the system\nis equipped with domain-specific knowledge bases and the\nability to interface with external information sources, en-\nhancing its contextual awareness and interpretability. With\nthis architecture, EEGAgent supports the entire workflow,\nfrom signal preprocessing and feature extraction to classi-\nfication, decision reasoning, and interactive reporting. By\nproviding an intelligent, multi-task-capable EEG analysis\nplatform, our framework aims to advance the field to-\nward greater automation, adaptability, and clinical readiness,\nhelping unlock the full potential of EEG as a diagnostic and\nmonitoring modality. Our key contributions are as follows:\n• We propose the EEGAgent framework, which enables\nunified scheduling and automated execution of multi-\ntask EEG analysis by integrating traditional and deep\nlearning-based methods, thereby improving overall sys-\ntem efficiency and adaptability. To our best knowledge,\nwe are the first to design an agent for EEG analysis.\n• EEGAgent incorporates key EEG-specific capabilities\nsuch as context awareness, flexible spatiotemporal analy-\nsis, accurate event localization, and automated interactive\nreporting, facilitating comprehensive and adaptive EEG\nanalysis and interpretation.\n• We evaluate the capability of EEGAgent on public\ndatasets, demonstrating its clinical potential through the\nsupport for interactive workflows and automated report\ngeneration, which can enhance diagnostic efficiency and\nassist clinical decision-making.\nRelated works\nEEG Analysis\nElectroencephalography (EEG) is widely used in sleep stag-\ning, emotion recognition, seizure detection, and cognitive\nworkload assessment, all of which require models capable\nof extracting informative patterns from noisy brain signals.\nExisting methods fall into two major categories: traditional\nmachine learning with handcrafted features and deep learn-\ning models that learn directly from raw data.\nTraditional ML approaches remain useful where effi-\nciency or interpretability is important. Naive Bayes has\nbeen applied to seizure and drowsiness detection (Ashok\nand Purushothaman 2016), Random Forests to mental work-\nload and emotion recognition (Messaoud and Chavez 2021),\nand Linear Discriminant Analysis (LDA) to motor im-\nagery–based BCIs (Fu et al. 2020; Santos, San Martin, and\nFraga 2023). Other classical classifiers such as SVM, KNN,\nand Logistic Regression are frequently used as baselines or\nin hybrid systems (Saeidi et al. 2021). Deep learning meth-\nods have shown strong performance by automatically ex-\ntracting spatial–temporal features from EEG signals (Zhou\net al. 2025a; Wang et al. 2024). CNNs excel in sleep stag-\ning and motor imagery (Efe and Ozsen 2023; Khademi,\nEbrahimi, and Kordy 2022), RNNs (e.g., LSTM, GRU) per-\nform well in sequential tasks such as seizure and emotion de-\ntection (Mekruksavanich and Jitpattanakul 2023; Zhao et al.\n2025; Wang et al. 2025b), and Transformer-based models\nfurther improve the modeling of long-range dependencies\n(Sun, Xie, and Zhou 2021; Wei et al. 2023). However, most\nexisting EEG models are designed for isolated tasks under\ncontrolled conditions, which limits their practical applica-\nbility. In real-world scenarios, tasks such as artifact removal,\nevent detection, and signal classification often need to be\nperformed together, underscoring the need for a unified,\ncontext-aware framework that can flexibly support multi-\ntask EEG analysis.\nLLM-Enhanced Agent Framework\nRecent advances in large language models (LLMs) have\nenabled the creation of intelligent agents that can per-\nform complex reasoning and manage multi-step tasks(Wang\net al. 2025a). Originally designed for next-token prediction,\nLLMs now demonstrate impressive capabilities in language\nunderstanding, planning, and decision-making (Brown et al.\n2020; Radford et al. 2019). When augmented with mod-\nules for planning, memory, perception, and tool use, they\ncan coordinate workflows, invoke external tools, and synthe-\nsize information to support high-level decision-making (Yao\net al. 2023b; Shinn et al. 2023). Techniques such as Chain of\nThought (CoT) (Wei et al. 2022) and Tree of Thought (ToT)\n(Yao et al. 2023a) enhance LLM reasoning via stepwise de-\ncomposition and hypothesis exploration. Planning modules\nhelp break down goals into subtasks, while memory mecha-\nnisms enable both contextual tracking and long-term knowl-\nedge retention (Huang et al. 2022; Wang et al. 2023; Wu\net al. 2022). Retrieval-Augmented Generation (RAG) further\nimproves reasoning by allowing access to external knowl-\nedge bases such as clinical guidelines (Lewis et al. 2020).\nHowever, directly applying LLM agents to EEG analy-\nsis poses unique challenges. EEG signals are highly variable\nand non-stationary, and they require domain-specific inter-\npretation. Tailoring LLMs to accommodate these complex-\nities is essential to ensure their utility in EEG-related ap-\nplications. While most existing EEG models remain task-\nspecific, recent trends in LLM-driven agent frameworks of-\nfer a new direction. These agents leverage planning, mem-\nory, and tool integration to coordinate analytic workflows\nand generate structured outputs. Extending such capabilities\nto EEG analysis could enable more flexible, multi-task solu-\ntions that align with clinical workflows.\nMethod\nThe EEGAgent is an intelligent system designed to au-\ntonomously plan and execute EEG analysis tasks. As illus-\ntrated in Fig. 1, this framework leverages a series of tools\nthat automate the execution of these tasks, thereby forming\nseveral distinctive capabilities tailored to diverse EEG anal-\nysis scenarios. Specifically, the agent demonstrates the fol-\nlowing core competencies:\n• Perception: The EEGAgent is aware of the EEG data\ncontext, including subject metadata, available channels,\nand the temporal span of the recording.\n• Exploration: The EEGAgent supports in-depth analysis\nwithin user-specified or system-identified spatiotempo-\n"}, {"page": 3, "text": "�  Perception\nName: Jack\nAge: 22\nSex: Male\n...\nEnvironment\nI. Patient Info\nII. Record Info\nIII. Impression\n    1. background\n    2. findings\n    3. provocative\nIV. Conclusion\nReport\nTemplate\nTools\nParametric tools\nNon-parametric tools\nEEG-related knowledge\nLibrary\nThought\nPlanning\nAction\nContext\nQuestion\nExploration\nDetection\nReporting\nFree Analysis：\nStart  to  End(s) :\nStatistical Features:\nAbnormal Findings:\nSummary ：\nBasic Information\nName: \nSex: \nAge: \nElectrode Montage: \nRecording Duration:\n... \nFind out Event\nEvent Type:\nStart  to End :\nBrain Region:\nStart  to End :\nBrain Region:\n...\nEEG Report\nPatient Info:\nRecord Info:\nBackground:\nAbnormal Findings:\n...\nFigure 1: EEGAgent framework\nral ranges. It flexibly schedules appropriate tools—such\nas frequency analysis, waveform extraction, or statisti-\ncal characterization—to examine targeted EEG segments\nfrom multiple perspectives.\n• Detection: The EEGAgent can accurately identify the\ntime segments and brain regions where specific EEG\nevents occur and further decode brain states, enabling ef-\nficient retrieval and matching.\n• Interaction and Reporting: The EEGAgent supports in-\nteraction with users and automatically generates struc-\ntured EEG reports based on predefined templates.\nTo support these capabilities, the EEGAgent integrates a\nmodular toolbox of parametric models and statistical feature\nextractors. Guided by a large language model, it dynami-\ncally plans and sequences tool usage based on task needs and\ncontext. This design enables flexible, context-aware analy-\nsis—from broad perception to precise localization—along\nwith automated and structured report generation.\nEEGAgent Architecture\nThe EEGAgent is designed to operate in a complex envi-\nronment consisting of subject information, EEG recording\nmetadata, raw signals, and report templates. It is built on\nQwen3-235B (Yang et al. 2025), a large language model\nknown for its strong reasoning capabilities and seamless in-\ntegration with external tools, hereafter referred to as Qwen.\nTo provide domain-specific expertise, the Agent incorpo-\nrates an EEG knowledge base. When a new task arrives,\nthe task description is embedded into a high-dimensional\nsemantic vector using Qwen3-Embedding-8B (Zhang et al.\n2025), and relevant knowledge entries are retrieved from the\nknowledge base via similarity search. The retrieved informa-\ntion is then used to enrich the context for Qwen. The Agent’s\ntoolbox contains two types of tools: parametric tools based\non deep learning models and non-parametric tools that uti-\nlize handcrafted statistical features. Upon receiving a task,\nQwen combines the enriched context and the current EEG\ndata to analyze the problem, formulate a plan, and select\nthe appropriate tools. These tools are then executed to ex-\ntract the necessary information, which is fed back to Qwen\nfor further reasoning. This iterative cycle continues until the\ntask is satisfactorily completed. By integrating knowledge\nretrieval, memory, and a versatile toolbox under the con-\ntrol of Qwen, the EEGAgent provides a flexible and in-\nterpretable framework capable of handling diverse EEG\nanalysis tasks in an automated manner.\nToolbox Design for Capability Support\nTo realize the core capabilities of the EEGAgent, the toolbox\nis organized along three independent dimensions: paramet-\nric vs. non-parametric nature, temporal granularity, and spa-\ntial granularity. These dimensions characterize each tool’s\nfunctional attributes and computational cost, enabling flexi-\nble scheduling and intelligent composition. Parametric deep\nlearning tools capture high-level clinical semantics for com-\nplex tasks, whereas non-parametric statistical and signal-\nprocessing tools provide efficient extraction of fundamen-\n"}, {"page": 4, "text": "Tool Name\nType\nTime Granularity\nSpace Granularity\nDescription\nnormalAbnormal\nParametric Tool\nFull EEG\nWhole Channel\nEstimates probability of pathological normality/abnor-\nmality for the entire EEG.\neyemMuscle\nParametric Tool\nSingle Second\nSingle Channel\nClassifies eye movement and muscle artifacts within 1-\nsecond window per channel.\nseizArtiBckg\nParametric Tool\nSingle Second\nSingle Channel\nClassifies seizure, artifact, and background within 1-\nsecond window per channel.\nseizNormal\nParametric Tool\nSingle Second\nSingle Channel\nDetects seizure vs. non-seizure within 1-second win-\ndow per channel.\nslowSeizBckg\nParametric Tool\n10 Seconds\nWhole Channel\nClassifies slow waves, epileptic and background activity\nin 10-second windows.\nbaseInfo\nNon-parametric\nTool\nFull EEG\nWhole Channel\nExtract structured information such as patient demo-\ngraphics and recording metadata from the EEG.\ncompute amplitude\nNon-parametric\nTool\n≤60 Seconds\nWhole Channel\nComputes\namplitude\nfeatures\n(mean\nabs,\nRMS,\nmax/min) for selected channels.\ncompute psd\nNon-parametric\nTool\n≤60 Seconds\nWhole Channel\nCalculates power spectral density across frequency\nbands for selected channels.\ncompute symmetry\nNon-parametric\nTool\n≤60 Seconds\nLeft-Right Channel\nPair\nCalculates Pearson correlation for left-right channel\npairs to assess symmetry.\nTable 1: Tools and their time-space granularity.\ntal features. Temporal granularity ranges from coarse long-\nwindow analysis to fine short-segment detection, and spatial\ngranularity spans global multi-channel assessment to single-\nchannel localization.\nThe Agent flexibly schedules and combines these tools\nto meet task requirements. For instance, non-parametric\ntools such as baseInfo support macro-level percep-\ntion, while parametric tools such as seizNormal and\nslowSeizBckg enable precise event localization. By dy-\nnamically integrating tools of different natures and granular-\nities, the Agent achieves multi-perspective analyses and bal-\nances accuracy with efficiency. The resulting multidimen-\nsional information is further fused to drive interaction and\nreport generation, ultimately producing structured and clini-\ncally meaningful EEG outputs. A summary of the tools and\ntheir temporal–spatial characteristics is shown in Table 1.\nEEG Perception Ability: Comprehending the\nfundamental information of EEG signals\nPerception capability refers to that the EEGAgent can un-\nderstand the EEG data environment in which it operates.\nGiven the specific characteristics of EEG data, the EEGA-\ngent can not only access basic information but also extract\ncritical advanced features, enabling it to build a comprehen-\nsive perceptual model of EEG recordings. EEG data inher-\nently contains rich contextual information, such as electrode\nconfiguration, recording duration, and subject status. To en-\nhance the Agent’s understanding of such background details,\nwe developed a compact knowledge module encapsulated\nas the baseInfo tool, which extracts key metadata upon\nEEG loading. This tool is automatically triggered during the\ninitial loading phase (i.e., environment initialization), help-\ning the Agent to construct a foundational perceptual view.\nWithin this module, the system extracts essential subject\ninformation (e.g., name, gender, age), the total recording du-\nration, and the electrode montage. Among these, age plays\na particularly crucial role in EEG interpretation. To improve\nthe Agent’s sensitivity to age-related factors, we incorpo-\nrate summarized insights from clinical literature regarding\nnormative EEG patterns across developmental stages (Liu\n2011), allowing the Agent to apply age-appropriate refer-\nence standards. Besides, the spatial organization of elec-\ntrodes and their mapping to anatomical brain regions are pre-\nsented in a structured tabular format, enabling the Agent to\nunderstand spatial relationships among channels, including\nsymmetry and brain regional affiliation. We also provide a\nconcise summary of typical frequency bands and their asso-\nciated pathological features to serve as cognitive references.\nSince foundational information such as age-related factors\nand electrode configurations is independent of specific tasks,\nit can be preloaded before any analysis begins. This enables\nthe EEGAgent to form an initial expectation of EEG pat-\nterns, effectively simulating the anticipatory cognition that\nhuman experts develop prior to interpreting EEG signals.\nEEG Exploration Ability: Analyzing EEG\nsegments from multiple perspectives\nThe EEGAgent’s exploration ability enables it to con-\nduct flexible and systematic analysis of any task-specified\ntime interval within EEG recordings. To model the EEGA-\ngent’s exploration capability, we consider a temporal inter-\nval [Tstart, Tend], which is partitioned into N non-overlapping\nsegments of length ∆t:\nXi = [Tstart + (i−1)∆t, Tstart + i∆t],\ni = 1, . . . , N. (1)\nFor each segment Xi, the Agent selects an analysis plan Ti\nfrom the tool library via a Qwen-based controller πθ, con-\nditioned on contextual information C (such as patient age,\nprior events, modality priors):\nTi = πθ(Xi, C) ⊆Tp ∪Tnp,\n(2)\nwhere Tp and Tnp represent sets of parametric and non-\nparametric tools, respectively. Each tool T ∈Ti is treated\n"}, {"page": 5, "text": "as a function:\nT : Xi 7→RT\ni ,\nRT\ni ∈RdT or V∗,\n(3)\nwhere RT\ni denotes the tool’s output, such as feature vectors,\nclassification scores, or symbolic descriptors. To synthesize\ntool outputs for each segment, a fusion function ffuse inte-\ngrates intermediate results:\nRi = ffuse({RT\ni }; ϕ),\nϕ = fusion parameters.\n(4)\nFinally, a summarization function fsummarize aggregates re-\nsults across all segments into a comprehensive semantic\nsummary S:\nS = fsummarize({Ri}N\ni=1; C),\n(5)\nwhich includes global assessments, rhythmic patterns, and\nlocalized event characterizations. The entire exploration\nworkflow is coordinated by a large language model. It han-\ndles every step, from selecting tools to fusing outputs and\ngenerating summaries. Rather than relying on fixed rules,\nthe Agent interprets tool results based on context. This ap-\nproach resolves representational mismatches and allows the\nsystem to generate high-level insights through language-\nbased reasoning. As a result, the EEGAgent can explore\nEEG data like a human expert, with a balance between in-\nterpretability and efficiency across different analysis goals.\nEEG Detection Ability: Recognizing Targeted\nSpatiotemporal EEG Event\nThe EEGAgent is capable of automatically identifying spe-\ncific clinically relevant EEG event types during review,\nand precisely localizing these events in both temporal and\nspatial domains throughout the recording. Given the non-\nstationary nature of EEG signals and the dynamic variation\nof their statistical properties over time, analyses conducted\nat a single temporal scale are insufficient to accurately cap-\nture event features. To address this challenge, we adopt a\nmulti-granularity analysis strategy that improves detec-\ntion accuracy and efficiency while reasonably controlling\ncomputational costs. In the temporal dimension, the EEGA-\ngent utilizes two time scales: 10 seconds and 1 second. The\n10-second window corresponds to the typical duration used\nin clinical review and serves as a coarse screening tool to\ndetermine whether a target event occurs within that period.\nUpon detection of a potential event, the EEGAgent further\nswitches to a finer 1-second scale to conduct detailed anal-\nysis, achieving more precise spatiotemporal localization. In\nthe spatial dimension, the EEGAgent employs both single-\nchannel and multi-channel analyses, enabling it to focus on\nlocal signal variations at individual electrodes as well as in-\ntegrate information across multiple channels to enhance ro-\nbustness in event detection.\nNotably, the entire analysis process is based on a slid-\ning window mechanism and is dynamically planned and\nscheduled by Qwen. It adjusts the analysis strategy accord-\ning to task requirements and intermediate results, balanc-\ning the trade-off between analysis granularity and compu-\ntational cost. This flexible guidance enables the EEGAgent\nto progressively refine its analysis from coarse to fine gran-\nularity. As a result, event detection and localization are per-\nformed efficiently and economically without compromising\naccuracy.\nAlgorithm 1: EEG Report Generation\nInput: Raw EEG data D\nParameter: Tool list L\nOutput: Final EEG report R\n1: Initialize report template T\n2: S ←Segment(D, 10s) {Divide EEG into 10s win-\ndows}\n3: for each segment si ∈S do\n4:\nrcoarse\ni\n←Analyze(si, coarse) {Coarse-level anal-\nysis}\n5:\nif Qwen Decide(rcoarse\ni\n) = fine then\n6:\nfor each subsegment fi,j ∈Segment(si, 1s) do\n7:\nrfine\ni,j ←Analyze(fi,j, fine)\n8:\nT ←T ∪rfine\ni,j\n9:\nend for\n10:\nelse\n11:\nT ←T ∪rcoarse\ni\n12:\nend if\n13: end for\n14: R ←GenerateReport(T ) {Qwen or template-based\nsynthesis}\n15: return R\nInteraction and Reporting Ability: Interacting\nwith Users and Generating Clinical EEG Reports\nEEGAgent enables lightweight user interaction by remem-\nbering context, allowing users to ask follow-up questions,\nrequest specific analyses, or clarify ambiguous results. This\nconversational ability improves interpretability and aligns\nwith clinical workflows. Building on this interaction, EE-\nGAgent can automatically generate structured reports to as-\nsist neurologists, summarizing both global patterns and lo-\ncal abnormalities through a hierarchical, multi-scale analy-\nsis pipeline. To support this goal, EEGAgent adopts a mod-\nular reporting strategy based on the ACNS standardized\ntemplate (Tatum et al. 2016), decomposing the report into\ncomponents such as background activity, abnormal events,\nand clinical impressions. This design ensures consistency\nand interpretability while accommodating variability across\nclinical cases. As shown in Algorithm 1, the input EEG is\nfirst segmented into 10-second windows, aligned with stan-\ndard clinical practice. Each segment is analyzed coarsely to\nidentify salient events. A lightweight Qwen controller de-\ntermines whether finer 1-second-scale analysis is warranted,\nallowing the system to focus on diagnostically relevant re-\ngions without overreacting to noise or isolated outliers. The\nresulting observations from both granularities are compiled\ninto a structured template and converted into final text using\neither template-based generation or Qwen-assisted summa-\nrization. This unified workflow enables EEGAgent to pro-\nduce clinically aligned reports that are robust, interpretable,\nand capable of supporting multi-level reasoning over com-\n"}, {"page": 6, "text": "plex EEG signals.\nExperiments\nDataset\nWe evaluate the EEGAgent on three widely used bench-\nmark datasets from the Temple University Hospital (TUH)\nEEG corpus. The TUH EEG corpus includes several subsets,\neach targeting distinct analysis tasks and annotation gran-\nularities. TUH Abnormal EEG Corpus (TUAB) (L´opez\net al. 2015) is designed for binary classification of EEG\nrecordings as normal or abnormal. It contains 2,993 record-\nings from 2,383 unique subjects, with roughly equal propor-\ntions of 1,521 normal and 1,472 abnormal samples. Each\nrecording spans approximately 20 minutes and follows the\nstandard 10–20 electrode montage, with a typical sampling\nrate of 250 Hz. Annotations are provided at the recording\nlevel, making TUAB suitable for global assessment. TUH\nEEG Event Corpus (TUEV) (Obeid and Picone 2016) fo-\ncuses on fine-grained event classification. It comprises 518\nrecordings annotated with six event types: periodic lateral-\nized epileptiform discharges (PLED), generalized periodic\ndischarges (GPED), spikes and sharp waves (SPSW), eye\nmovements (EYEM), artifacts (ARTF), and background ac-\ntivity (BCKG). Annotations are provided at 1-second res-\nolution on a per-channel basis, enabling precise temporal\nand spatial localization. TUEV supports various classifica-\ntion tasks, including binary seizure detection, artifact dis-\ncrimination, and multi-class event recognition. We evaluate\nthe detection capability of the EEGAgent on this dataset.\nTUH EEG Slowing Corpus (TUSL) (Obeid and Picone\n2016) targets the differentiation of epileptiform slowing\nfrom background activity and seizure events. It includes 300\nlabeled segments, each 10 seconds long, from 75 recording\nsessions involving 38 patients. All data are recorded with\nthe TCP-REF montage at 256 Hz. Annotations are provided\nat the segment level across all channels. Due to its limited\nsize, ensemble strategies such as soft voting are often used\nto improve evaluation robustness. Together, these datasets\nprovide a comprehensive testbed for multi-scale, multi-task\nEEG analysis, supporting both global and localized objec-\ntives under varying levels of annotation granularity.\nsystem: You are an assistant specialized in EEG interpretation. [...]\n{'patient_id': 'aaaaabuv', 'sex': 'F', 'age': 55, 'start_date': '17.09.12', 'start_time': '08.35.40', 'data_duration': '[0 - 1385]s'} \n[...] \n'available_channel': \n'Hemisphere  | Prefrontal            | Frontal           | Temporal                    | Central    | Parietal | Occipital\nLeft     \n | FP1-F3, FP1-F7 | F3-C3, F7-T3 | T3-T5, T3-C3, A1-T3 | C3-CZ     | C3-P3   | T5-O1, P3-O1\nRight  \n | FP2-F4, FP2-F8 | F4-C4, F8-T4 | T4-T6, C4-T4, T4-A2 | CZ-C4     | C4-P4   | T6-O2, P4-O2', \n[...]\n'Age factor': {'min_age': 20.0, 'max_age': 60.0, 'features': ['Occipital α: average 10 Hz, low amplitude', 'Mu rhythm: θ response not \nobvious', 'Frontal θ: not obvious', 'Hyperventilation: δ response not obvious', 'Photic driving: photic driving gradually disappears']}  \n[...]\nHuman:  Tell me your understanding of this EEG recording.\nAssistant:  This EEG recording belongs to a 55-year-old female patient, with a start date of September 17, 2012, and a start \ntime of 08:35:40. The total duration of the recording is 1385 seconds (approximately 23 minutes). \nThe recording likely captures a broad overview of the patient's cerebral activity during this time, and based on the patient's age, \nwe can expect certain EEG characteristics, such as low-amplitude alpha waves (around 8-9 Hz) in the occipital regions and \npotentially an increase in slow-wave activity in the frontal and temporal regions.\nFigure 2: The EEGAgent perceives the environment con-\nstructed from EEG data. Solid lines represent processes vis-\nible to the user.\nEEG Perception Experiment\nWe evaluate the perceptual capability of the EEGAgent on\nTUAB. This experiment is designed to assess whether the\nAgent can accurately extract contextual metadata and inte-\ngrate prior neurophysiological knowledge into its interpre-\ntation process. As shown in Figure 2, the Agent is provided\nwith structured information, including patient demograph-\nics (e.g., sex and age), recording parameters (e.g., start date,\nstart time, and total duration), and spatial channel mappings\n(e.g., FP1-F3 and F3-C3 corresponding to the left frontal re-\ngion).\nThe evaluation focuses on the Agent’s ability to extract\nkey metadata such as the subject’s age and the record-\ning duration, and subsequently apply these contextual cues\nwhen interpreting the EEG background activity. In this case,\nthe patient’s age informs the Agent’s expectations regard-\ning typical EEG characteristics, including slowing of back-\nground rhythms, reduced fast activity, the presence of low-\namplitude occipital alpha rhythms, and a possible increase\nin frontal and temporal slow-wave activity. This reasoning\nprocess leverages well-established age-related EEG patterns\n(Liu 2011). The objective of this evaluation is to determine\nwhether the EEGAgent can demonstrate context-driven per-\nceptual capability by combining demographic information,\nspatial channel knowledge, and neurophysiological priors to\nguide its understanding of the EEG environment.\nHuman:  Please analyze the EEG condition from minute 5 to 6.\nAssistant:  [...] To efficiently assess this 60-second segment, I should: 1. First evaluate the overall pathological \nnormality vs. abnormality probability across this interval. 2. Analyze background rhythm characteristics. [..]\n<FUNCTION> slowSeizBckg\n<ARGS> {\"start\": 300, \"end\": 360}\nAssistant:  [...] To further characterize the nature of the slow waves: - Assess amplitude and distribution across \nregions -Check for posterior dominant rhythm (alpha rhythm)  to assess baseline arousal.  [...] \nTool:'return': {'FP1-F7': {'mean_abs': '8.27e-06', 'rms': '1.22e-05', 'max_val': '4.71e-05', 'min_val': '-8.18e-05'}, ...}\nAssistant:  [...] From 5 to 6 minutes, the EEG shows persistent generalized slow wave activity (GRDA pattern) \nwith no seizure or significant asymmetry. This finding warrants clinical correlation for possible metabolic, \ntoxic, or structural etiologies, especially in an elderly patient.\nTool:  'return': [{'duration': '300s-310s', 'Prob': {'bckg': 0.04, 'slow': 0.96, 'seiz': 0.0}}, ... ]\n<FUNCTION> compute_amplitude\n<ARGS> {\"name\": [\"FP1-F7\", \"F7-T3\", \"T3-T5\", \"T5-O1\", \"FP2-F8\", \"F8-T4\", \"T4-T6\", \"T6-O2\"], \"start\": \n300,\"end\": 360}\nFigure 3: Exploration of EEG segments by the EEGAgent.\nSolid lines represent processes visible to the user.\nEEG Segment Exploration Experiment\nTo evaluate the Agent’s exploratory capability, we present a\ncase study from the TUAB corpus in response to the non-\nspecific query: “Please analyze the EEG condition from\nminute 5 to 6.” As shown in Figure 3, the Agent au-\ntonomously executes a structured, multi-stage workflow.\nFirst, it deploys the parametric tool slowSeizBckg for\na rapid assessment, which detects slow-wave activity with\na high probability. Based on this preliminary result, the\nAgent initiates a deeper analysis using the non-parametric\ntool compute amplitude to precisely characterize the\nspatial distribution of the detected waves. In the final stage,\nthe Agent synthesizes multiple sources of evidence: the\n"}, {"page": 7, "text": "high-level classification (slow wave), low-level quanti-\ntative features (indicating a generalized pattern), and rele-\nvant patient context (elderly). This integrative reasoning\nproduces a clinically meaningful report identifying persis-\ntent generalized slow-wave activity, which warrants further\nclinical correlation. This case study highlights the Agent’s\nability to schedule a coarse-to-fine sequence of analytical\ntools, effectively bridging the gap between raw EEG data\nand expert-level interpretation.\nHuman:  Was there an epileptic seizure in the previous minute? \nAssistant:  To determine whether epileptic discharges can be observed within the first minute of the EEG recording, we need to analyze \nthe data for epileptiform activity in that time window. [...]\nThe most appropriate tool is `seizNormalModel_OneSecond`. However, calling this tool 60 times would be inefficient. Alternatively, \nthe slowSeizBckg tool analyzes 10-second windows. This is more efficient for scanning the first 60 seconds.\n<FUNCTION> slowSeizBckg\n<ARGS> {\"start\": 0, \"end\": 60}\nAssistant: there is a notable probability of epileptiform activity in the initial 10-second interval (0s–10s). [...]\nGiven the elevated seizure probability in the **0–10 second** window, I will now analyze this specific interval using a higher-resolution tool. \nI will use the seizNormal tool to analyze key channels in the temporal and frontal regions .\n<FUNCTION> seizNormal\n<ARGS> {\"name\": [\"FP1-F3\", \"F3-C3\", \"C3-P3\", \"P3-O1\", \"FP2-F4\", \"F4-C4\", \"C4-P4\", \"P4-O2\"], \"start\": 0, \"end\": 10}\nTool:''return': [{'duration': '0s-1s', 'FP1-F3': {'Non-seiz': 1.0, 'seiz': 0.0},..., 'F4-C4': {'Non-seiz': 0.32, 'seiz': 0.68}, 'C4-P4': {'Non-seiz': 0.98, \n'seiz': 0.02}, 'P4-O2': {'Non-seiz': 0.97, 'seiz': 0.03}}, {'duration': '1s-2s', ...}...]\nAssistant: The **right frontal to central region (F4-C4)** shows the highest seizure probability (68%) at 0–1s, suggesting a focal \nonset in the **right frontal lobe**. Later, at 7–8s, there is a possible spread or independent activity in the **right posterior region (P4-\nO2)** [...]\nTool:'return': [{'duration': '0s-10s', 'Prob': {'bckg': 0.57, 'slow': 0.02, 'seiz': 0.42}},..., {'duration': '50s-60s', 'Prob': {'bckg': 0.86, 'slow': 0.0, \n'seiz': 0.14}}]}\nFigure 4: Detection of Epileptic Discharges by the EEGA-\ngent. Solid lines represent processes visible to the user.\nEEG Event Detection Experiment\nDetection ability involves not only identifying the temporal\nintervals of EEG events but also localizing their spatial dis-\ntribution. Because many clinically relevant events are tran-\nsient and spatially variable, EEGAgent adopts a hierarchical\nmulti-scale strategy to balance accuracy and efficiency. As\nshown in Figure 4, the system first performs a coarse-grained\nscan (e.g., 60-s windows) to quickly locate suspicious inter-\nvals, and then progressively focuses on these intervals using\nhigher-resolution tools to refine temporal and spatial local-\nization. In the example, an initial elevated seizure probabil-\nity at 0–10 s was further resolved at the channel level, ulti-\nmately pinpointing the right fronto-central region (F4–C4)\nat 0–1 s as the likely onset site.\nAdditionally, we evaluated this detection framework on\nthe TUEV test set, focusing on a seizure detection task. An-\nnotations were preprocessed by merging adjacent events less\nthan 1 s apart and defining SPSW, GPED, and PLED as\npositive classes. Predictions were considered correct when\ntheir IoU with the reference events exceeded 0.7 on the same\nchannel. Under this criterion, the framework achieved a hit\nrate of 69.30% and a false rate of 44.77%, demonstrating\nthat the stepwise localization strategy enables efficient and\nprecise detection of transient epileptiform events.\nEEG Report Generation Experiment\nWe conducted a feasibility evaluation of the EEGAgent’s ex-\npression capability using the TUAB dataset, a large-scale\nclinical EEG dataset with normal and abnormal labels that\nreflects the information distribution typically encountered in\nreal-world clinical settings. Due to the lack of paired EEG\nrecordings and standardized reports, we designed an auto-\nmatic report generation task based on ACNS standards\n(Tatum et al. 2016) to assess the system’s ability to trans-\nlate signal analysis results into clinical language.\nIn this task, the EEGAgent receives a full EEG recording\nand generates a structured report containing basic informa-\ntion, background activity, abnormal events, and diagnostic\nconclusions. As shown in Figure 5, the Agent accurately ex-\ntracts patient metadata (e.g., age, recording duration, elec-\ntrode configuration), identifies background rhythms and\ntheir spatial distribution, and detects abnormal events at a\ncoarse level. For example, in the illustrated case, the system\ndescribed occipital alpha slowing, diffuse slow waves, and\nleft fronto-central epileptiform discharges, and synthesized\nthese findings into a clinically coherent conclusion. This ex-\nperiment demonstrates that the EEGAgent is capable of\ngenerating clinically interpretable EEG reports, cover-\ning key information elements and supporting abnormal-\nity detection and localization. Although quantitative evalu-\nation remains challenging, the TUAB-based validation sug-\ngests that the proposed approach is feasible for automated\nEEG interpretation.\nEEG Report generated by EEGAgent\nPatient ID\naaaaabuv\nAge\n55\nSex\nFemale\nRecording Date\n2017/9/12\nRecording Duration\n1385 seconds\nReport Date\n-\nMedication History\n-\nConsciousness\nawake\nMontage\n10-20 electrode system\nBackground Activity\nAlpha Activity\nPosterior dominant rhythm: 10 Hz alpha with low amplitude, consistent with normal aging.\nFast Waves\nSlow Waves\nBackground interrupted by diffuse slowing episodes.\nWaveform Characteristics\nModerate asymmetry in posterior rhythms.\nAbnormal EEG Findings\nPersistent diffuse theta-delta slowing at multiple intervals, slow wave probability >80%. Focal epileptiform\ndischarges at left fronto-central region at 500s, seizure probability 72%. Brief intermittent epileptiform activity\nat other times, low seizure probability. No electrographic seizures detected.\nConclusion\nDiffuse slowing and left fronto-central epileptiform discharges present without seizures recorded.\nReported by\nEEGAgent\nReviewed by\nReview Date\nFigure 5: EEG report generated by the EEGAgent\nConclusion\nIn this work, we present EEGAgent, the first framework\nto integrate intelligent agent systems with EEG analy-\nsis. To handle the long duration and non-stationary na-\nture of EEG signals, we design a flexible toolbox operat-\ning across multiple temporal and spatial granularities for\nadaptive and context-aware analysis. We showcase its core\ncapabilities—perception, exploration, detection, interaction\nand report generation—and evaluate it on public datasets.\nEEGAgent improves analysis efficiency, scalability, and\ninterpretability, highlighting the potential of LLM-driven\nagents to automate EEG interpretation and support clinical\ndecision-making.\nReferences\nAshok, S.; and Purushothaman, G. 2016.\nDWT based\nEpileptic Seizure Detection from EEG Signals using Na¨ıve\nBayes/k-NN Classifiers. IEEE Access, 4: 1–1.\n"}, {"page": 8, "text": "Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language models\nare few-shot learners. In Proceedings of the 34th Interna-\ntional Conference on Neural Information Processing Sys-\ntems, NIPS ’20. Red Hook, NY, USA: Curran Associates\nInc. ISBN 9781713829546.\nEfe, E.; and Ozsen, S. 2023. CoSleepNet: Automated sleep\nstaging using a hybrid CNN-LSTM network on imbalanced\nEEG-EOG datasets. Biomedical Signal Processing and Con-\ntrol, 80: 104299.\nFu, R.; Han, M.; Tian, Y.; and Shi, P. 2020. Improvement\nmotor imagery EEG classification based on sparse common\nspatial pattern and regularized discriminant analysis. Jour-\nnal of Neuroscience Methods, 343: 108833.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage Models as Zero-Shot Planners: Extracting Action-\nable Knowledge for Embodied Agents. arXiv:2201.07207.\nKhademi, Z.; Ebrahimi, F.; and Kordy, H. M. 2022. A trans-\nfer learning-based CNN and LSTM hybrid deep learning\nmodel to classify motor imagery EEG signals. Computers\nin biology and medicine, 143: 105288.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨aschel,\nT.; Riedel, S.; and Kiela, D. 2020.\nRetrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks.\nIn\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and\nLin, H., eds., Advances in Neural Information Processing\nSystems, volume 33, 9459–9474. Curran Associates, Inc.\nLi, J.; Chen, X.; Shen, F.; Chen, J.; Liu, Y.; Zhang, D.; Yuan,\nZ.; Zhao, F.; Li, M.; and Yang, Y. 2025. Deep Learning-\nPowered Electrical Brain Signals Analysis: Advancing Neu-\nrological Diagnostics. arXiv:2502.17213.\nLiu, X. Y. 2011. Clinical Electroencephalography Train-\ning Tutorial. Beijing, China: People’s Medical Publishing\nHouse, 1st edition. Nov. 2011.\nL´opez, S.; Suarez, G.; Jungreis, D.; Obeid, I.; and Picone,\nJ. 2015. Automated identification of abnormal adult EEGs.\nIn 2015 IEEE Signal Processing in Medicine and Biology\nSymposium (SPMB), 1–5.\nMekruksavanich, S.; and Jitpattanakul, A. 2023. Effective\ndetection of epileptic seizures through EEG signals using\ndeep learning approaches. Machine Learning and Knowl-\nedge Extraction, 5(4): 1937–1952.\nMessaoud, R. B.; and Chavez, M. 2021.\nRandom forest\nclassifier for EEG-based seizure prediction. arXiv preprint\narXiv:2106.04510.\nObeid, I.; and Picone, J. 2016. The Temple University Hos-\npital EEG Data Corpus. Frontiers in neuroscience, 10: 196.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019.\nLanguage Models are Unsupervised\nMultitask Learners.\nSaeidi, M.; Karwowski, W.; Farahani, F. V.; Fiok, K.; Taiar,\nR.; Hancock, P. A.; and Al-Juaid, A. 2021. Neural decoding\nof EEG signals with machine learning: a systematic review.\nBrain sciences, 11(11): 1525.\nSantos, E.; San Martin, R.; and Fraga, F. 2023. Compari-\nson of subject-independent and subject-specific EEG-based\nBCI using LDA and SVM classifiers. Medical & biological\nengineering & computing, 61.\nShen, Y.-W.; and Lin, Y.-P. 2019. Challenge for Affective\nBrain-Computer Interfaces: Non-stationary Spatio-spectral\nEEG Oscillations of Emotional Responses. Frontiers in hu-\nman neuroscience, 13: 366.\nShinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and\nYao, S. 2023. Reflexion: language agents with verbal re-\ninforcement learning. In Oh, A.; Naumann, T.; Globerson,\nA.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in\nNeural Information Processing Systems, volume 36, 8634–\n8652. Curran Associates, Inc.\nSteinmetz, J. D.; Seeher, K. M.; Schiess, N.; Nichols, E.;\nCao, B.; Servili, C.; Cavallera, V.; Cousin, E.; Hagins, H.;\nMoberg, M. E.; et al. 2024. Global, regional, and national\nburden of disorders affecting the nervous system, 1990–\n2021: a systematic analysis for the Global Burden of Disease\nStudy 2021. The Lancet Neurology, 23(4): 344–381.\nSun, J.; Xie, J.; and Zhou, H. 2021. EEG Classification with\nTransformer-Based Models. In 2021 IEEE 3rd Global Con-\nference on Life Sciences and Technologies (LifeTech), 92–\n93.\nTatum, W. O.; et al. 2016. ACNS Guideline 7: Guidelines\nfor EEG Reporting. Journal of Clinical Neurophysiology,\n33(4): 328–332.\nWang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.;\nZhu, Y.; Fan, L.; and Anandkumar, A. 2023. Voyager: An\nOpen-Ended Embodied Agent with Large Language Mod-\nels. In NeurIPS 2023 Foundation Models for Decision Mak-\ning Workshop.\nWang, J.; Zhao, S.; Jiang, H.; Li, S.; Li, T.; and Pan, G. 2024.\nGeneralizable Sleep Staging via Multi-Level Domain Align-\nment. Proceedings of the AAAI Conference on Artificial In-\ntelligence, 38(1): 265–273.\nWang, J.; Zhao, S.; Luo, Z.; Zhou, Y.; Jiang, H.; Li, S.; Li, T.;\nand Pan, G. 2025a. CBraMod: A Criss-Cross Brain Founda-\ntion Model for EEG Decoding. arXiv:2412.07236.\nWang, Y.; Zhao, S.; Jiang, H.; Li, S.; Li, T.; and Pan, G.\n2025b. M-MDD: A multi-task deep learning framework for\nmajor depressive disorder diagnosis using EEG. Neurocom-\nputing, 636: 130008.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; ichter, b.;\nXia, F.; Chi, E.; Le, Q. V.; and Zhou, D. 2022. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels.\nIn Koyejo, S.; Mohamed, S.; Agarwal, A.; Bel-\ngrave, D.; Cho, K.; and Oh, A., eds., Advances in Neural\nInformation Processing Systems, volume 35, 24824–24837.\nCurran Associates, Inc.\nWei, Y.; Liu, Y.; Li, C.; Cheng, J.; Song, R.; and Chen,\nX. 2023.\nTC-Net: A Transformer Capsule Network for\n"}, {"page": 9, "text": "EEG-based emotion recognition. Computers in biology and\nmedicine, 152: 106463.\nWu, Y.; Rabe, M. N.; Hutchins, D.; and Szegedy, C. 2022.\nMemorizing Transformers. arXiv:2203.08913.\nYang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.;\nYu, B.; Gao, C.; Huang, C.; Lv, C.; Zheng, C.; Liu, D.; Zhou,\nF.; Huang, F.; Hu, F.; Ge, H.; Wei, H.; Lin, H.; Tang, J.;\nYang, J.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Zhou,\nJ.; Lin, J.; Dang, K.; Bao, K.; Yang, K.; Yu, L.; Deng, L.;\nLi, M.; Xue, M.; Li, M.; Zhang, P.; Wang, P.; Zhu, Q.; Men,\nR.; Gao, R.; Liu, S.; Luo, S.; Li, T.; Tang, T.; Yin, W.; Ren,\nX.; Wang, X.; Zhang, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang,\nY.; Zhang, Y.; Wan, Y.; Liu, Y.; Wang, Z.; Cui, Z.; Zhang,\nZ.; Zhou, Z.; and Qiu, Z. 2025. Qwen3 Technical Report.\narXiv:2505.09388.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao, Y.;\nand Narasimhan, K. 2023a. Tree of Thoughts: Deliberate\nProblem Solving with Large Language Models. In Oh, A.;\nNaumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and\nLevine, S., eds., Advances in Neural Information Process-\ning Systems, volume 36, 11809–11822. Curran Associates,\nInc.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK. R.; and Cao, Y. 2023b. ReAct: Synergizing Reasoning\nand Acting in Language Models. In The Eleventh Interna-\ntional Conference on Learning Representations.\nZhang, X.; Chowdhury, R. R.; Gupta, R. K.; and Shang, J.\n2024.\nLarge language models for time series: A survey.\narXiv preprint arXiv:2402.01801.\nZhang, Y.; Li, M.; Long, D.; Zhang, X.; Lin, H.; Yang, B.;\nXie, P.; Yang, A.; Liu, D.; Lin, J.; Huang, F.; and Zhou, J.\n2025. Qwen3 Embedding: Advancing Text Embedding and\nReranking Through Foundation Models. arXiv:2506.05176.\nZhao, S.; Li, W.; Wang, X.; Foglia, S.; Tan, H.; Zhang, B.;\nHamoodi, A.; Nelson, A.; and Gao, Z. 2024. A Systematic\nReview of Machine Learning Methods for Multimodal EEG\nData in Clinical Application. arXiv:2501.08585.\nZhao, S.; Yi, S.; Zhou, Y.; Pan, J.; Wang, J.; Xia, J.; Li, S.;\nDong, S.; and Pan, G. 2025. Wearable Music2Emotion: As-\nsessing Emotions Induced by AI-Generated Music through\nPortable EEG-fNIRS Fusion. In Proceedings of the 33rd\nACM International Conference on Multimedia, MM ’25,\n5627–5636. New York, NY, USA: Association for Comput-\ning Machinery. ISBN 9798400720352.\nZhou, Y.; Zhao, S.; Wang, J.; Jiang, H.; Li, S.; Li, T.; and\nPan, G. 2025a.\nBrainUICL: An unsupervised individual\ncontinual learning framework for EEG applications. In The\nThirteenth International Conference on Learning Represen-\ntations.\nZhou, Y.; Zhao, S.; Wang, J.; Jiang, H.; Li, S.; Li, T.;\nand Pan, G. 2025b.\nSPICED: A Synaptic Homeostasis-\nInspired Framework for Unsupervised Continual EEG De-\ncoding. arXiv preprint arXiv:2509.17439.\n"}]}