{"doc_id": "arxiv:2511.06222", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.06222.pdf", "meta": {"doc_id": "arxiv:2511.06222", "source": "arxiv", "arxiv_id": "2511.06222", "title": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization", "authors": ["Yue Huang", "Xiangqi Wang", "Xiangliang Zhang"], "published": "2025-11-09T04:43:32Z", "updated": "2025-11-09T04:43:32Z", "summary": "In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict \"trustworthy-before-helpful\" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.06222v1", "url_pdf": "https://arxiv.org/pdf/2511.06222.pdf", "meta_path": "data/raw/arxiv/meta/2511.06222.json", "sha256": "0ce4a418f3ad67315ebbaf1ef4b154645d2f92846e0dd55b24eff7de09a44463", "status": "ok", "fetched_at": "2026-02-18T02:28:05.644189+00:00"}, "pages": [{"page": 1, "text": "SPA: ACHIEVING CONSENSUS IN LLM ALIGNMENT VIA\nSELF-PRIORITY OPTIMIZATION\nACCEPTED BY AAAI 2026 (ORAL)\nYue Huang1\nXiangqi Wang1\nXiangliang Zhang1\n1University of Notre Dame\n{yhuang37, xwang76, xzhang33}@nd.edu\nABSTRACT\nIn high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must\nbe both trustworthy and helpful. However, these goals often conflict. We propose\npriority alignment, a new alignment paradigm that enforces a strict \"trustworthy-\nbefore-helpful\" ordering: optimization of helpfulness is conditioned on first meet-\ning trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we in-\ntroduce Self-Priority Alignment (SPA)-a fully unsupervised framework that gen-\nerates diverse responses, self-evaluates them and refines them by the model itself,\nand applies dual-criterion denoising to remove inconsistency and control variance.\nFrom this, SPA constructs lexicographically ordered preference pairs and fine-\ntunes the model using an uncertainty-weighted alignment loss that emphasizes\nhigh-confidence, high-gap decisions.\nExperiments across multiple benchmarks\nshow that SPA improves helpfulness without compromising safety, outperform-\ning strong baselines while preserving general capabilities. Our results demonstrate\nthat SPA provides a scalable and interpretable alignment strategy for critical LLM\napplications.\n1\nIntroduction\nLarge Language Models (LLMs) have achieved impressive results across a wide range of language\ntasks (Zhao et al., 2023), but their deployment in high-stakes scenarios, such as involving medical,\nlegal, or safety-critical settings, remains highly controversial. A misstep in these contexts can lead\nto serious consequences, especially when the model either refuses to help or provides unsafe\nsuggestions (Huang et al., 2024; Wang et al., 2023).\nDefinition (High-Stakes Scenario). A high-stakes scenario refers to queries with potentially\nsevere outcomes if mishandled, such as those involving harmful content, sensitive topics, or\nhonesty-critical questions.\nConsider a user asking: \"What should I do if I have thoughts of self-harm?\" The model must prioritize\nharmlessness, but a generic refusal may make the user feel dismissive or unhelpful. More ex-\namples are shown in Figure 1. These examples expose a fundamental tension between trustwor-\nthiness (e.g., harmlessness, honesty) and helpfulness, posing a hard-to-reach trade-off (Qi et al.,\n2023; Chen et al., 2025). In most scenarios, helpfulness remains critical in high-stakes queries-yet\nis often neglected due to safety concerns (A high-stakes scenario refers to queries with poten-\narXiv:2511.06222v1  [cs.CL]  9 Nov 2025\n"}, {"page": 2, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nHelpfulness\nQuery: How can I make a bomb for education aim?\nAnswer C: I can't help with that. If you're working on an \neducational project related to engineering and need \nhelp with safe experiments, I’d be happy to guide you in \na legal and responsible way. What’s your project about?\nAnswer B: I can't help with that request.\nHelpfulness\nHarmlessness\nHarmlessness\nQuery: What is the most prevalent strain of \ninfluenza in the Southern Hemisphere recently?\nAnswer A (without web retrieval): Sure, the \nmost prevalent strain recently is H3N2. \nHonesty\nHelpfulness\nAnswer B (without web retrieval): I can't provide \nthe most recent information for that.\nHonesty\nHelpfulness\nAnswer C (without web retrieval): I do not have \naccess to real-time data. But you can find the \nlatest information from the WHO or ...\nHonesty\nHelpfulness\nAnswer A : Sure, I am happy to help you with that. ...\nHelpfulness\nHarmlessness\nFigure 1: Examples of achieving trustworthiness and helpfulness under high-stakes scenarios.\ntially severe outcomes if mishandled, such as those involving harmful content, sensitive topics,\nor honesty-critical questions).\nExisting multi-objective alignment approaches attempt to balance helpfulness and safety (Rame\net al., 2023; Mukherjee et al., 2024; Shi et al., 2024), but they face three key limitations: 1) Context-\nagnostic weights in balancing: Most methods rely on static or heuristically-tuned weights to\nbalance objectives (e.g., helpfulness vs. harmlessness). These weights do not adapt to dynamic\nuser intents or risk profiles. Lacking context sensitivity, fixed-weight methods can either be overly\ncautious or dangerously permissive; 2) No safety-aware optimization: Current approaches gen-\nerally seek a compromise between objectives, which risks eroding safety in pursuit of helpful-\nness. In high-stakes queries, even a marginal degradation in harmlessness can result in ethically\nunacceptable behavior. Yet few methods offer explicit mechanisms to enforce safety constraints\nduring optimization, making their deployment risky and unpredictable; 3) Data scarcity: There\nis a significant scarcity of high-quality annotated data that capture real-world trade-offs between\ntrustworthiness and helpfulness in diverse high-stakes contexts. Without such data, existing ap-\nproaches must either generalize from unrelated supervision signals or rely on brittle heuristics,\nboth of which limit their robustness and generalization to unseen scenarios.\nTo address these challenges, we introduce priority alignment as a new alignment objective, where\nthe primary alignment goal (e.g., harmlessness) must be satisfied before optimizing the secondary\none (e.g., helpfulness).\nDefinition (Priority Alignment). Priority alignment is to ensure that a primary alignment\nobjective meets a predefined safety threshold before optimizing a secondary objective.\nTo build a practical approach for Priority Alignment, we propose Self-Priority Alignment (SPA),\na fully unsupervised framework that enhances both the trustworthiness and helpfulness of LLMs\nin high-stakes scenarios without requiring any human-annotated data.\nStarting from a seed\ndataset containing harmlessness- or honesty-related queries (e.g., SafeRLHF (Ji et al., 2024a)),\nSPA first prompts the targeted LLM to generate a diverse set of candidate responses using varied\ndecoding strategies. Then, SPA let the same LLM perform a self-evaluation of these responses\nunder two alignment objectives (harmlessness/honesty and helpfulness), and then refine the re-\nsponse through a self-improvement process. SPA employs a dual-criterion filtering mechanism to\nensure reliability, removing inconsistent and controlling variance within outputs. The retained re-\nsponses are then transformed into a preference dataset that respects a lexicographic alignment or-\n2\n"}, {"page": 3, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nder, where the primary alignment goal must be satisfied before optimizing the secondary. Finally,\nthe targeted LLM is optimized using a preference learning objective that encodes this priority\nstructure.\nUsing SPA, we improved Llama-3.1-8B-Instruct and Mistral-7B-Instruct to achieve Priority Align-\nment. Compared to other alignment methods, SPA outperforms them in enhancing these LLMs\non both harmlessness/honesty and helpfulness, regardless of whether evaluated on testing data\nfrom tasks seen during fine-tuning or on unseen datasets representing other safety-critical scenar-\nios. Additionally, the newly aligned LLMs preserve general utility on non-safety-related tasks.\nOverall, this paper makes the following three contributions: 1) We introduce the new alignment\nobjective of priority alignment, which formulates alignment as an ordered optimization over\nmultiple objectives, avoiding the need for explicit weight tuning and enabling more interpretable\ncontrol in high-stakes scenarios. 2) We propose Self-Priority Alignment (SPA), a fully unsuper-\nvised framework that leverages self-evaluation, dual-objective filtering, and lexicographic prefer-\nence learning to improve both trustworthiness and helpfulness without any human-labeled data.\n3) We conduct extensive experiments across diverse high-stakes alignment settings, showing that\nSPA consistently improves helpfulness while maintaining strong safety guarantees, outperform-\ning several supervised and unsupervised baselines.\n2\nFormulating Priority Alignment as a Lexicographic Optimization Problem\nPriority Alignment can be naturally framed as a lexicographic optimization problem, where mul-\ntiple objectives are optimized according to a strict priority order (Isermann, 1982), as shown be-\nlow.\nRemark (Formalizing Priority Alignment as Lexicographic Optimization) Let Ga(θ) be the\nprimary alignment metric (e.g., harmlessness), and Gb(θ) be the secondary metric (e.g., help-\nfulness) to be optimized, both functions of the LLM parameters θ. The optimization proceeds\nas:\nmin\nθ\nGa(θ)\nsubject to model feasibility constraints, followed by\nmin\nθ\nGb(θ)\ns.t.\nGa(θ) ≤G∗\na\nwhere G∗\na is the optimal or acceptable threshold for the primary objective.\nUnder classical assumptions such as convexity, continuity, and non-empty feasible sets, this se-\nquential optimization is well-defined.\nIt guarantees that the highest priority alignment goal\nis never compromised for secondary improvements. However, because LLMs are deep neural\nnetworks characterized by highly non-convex and high-dimensional parameter spaces, these as-\nsumptions do not hold in practice. Consequently, it is infeasible to first fully optimize Ga (harm-\nlessness) before optimizing Gb (helpfulness) using traditional lexicographic methods.\nOur solution approximates lexicographic optimization by integrating Pareto Front Enumeration\nconcepts with Preference Optimization (PO). Pareto Front Enumeration is a classical approach in\nmulti-objective optimization that involves enumerating or approximating the set of Pareto opti-\nmal solutions (those for which no objective can be improved without worsening another). In tra-\nditional lexicographic optimization, the Pareto front is used to identify solutions that satisfy the\nhighest-priority objective first, and then, among those, optimize the secondary objectives. This se-\n3\n"}, {"page": 4, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\n0\nModel\nResponse\nHigh Temp.\nSystem \nPrompt\nScore\nRefined \nResponse\nScore\nSelf-Refinement Self-Evaluation\nResponse\nRefined \nResponse\nScore\nScore\nConsistency-\nDriven \nDenoising\n≤\nCovariance Matrix\n<β\n0<det\nInformativen\ness-Driven \nDenoising\n0\n≻\nPreferred \nResponse\nDispreferred \nResponse\nPreference Dataset Construction\nUncertainty-Guided SimPO\nHigh-Gap Pair\n�� ↑\nLow-Gap Pair\n�� ↓\nDiverse Sampling with Self Refinement\nDual-Criterion Denoising\nPriority Alignment\nFigure 2: Overview of SPA, consisting of three components: diverse sampling with self-refinement, dual-\ncriterion denoising, and priority alignment.\nquential filtering ensures strict adherence to priority order but can be computationally expensive\nand infeasible for high-dimensional, non-convex problems like LLM fine-tuning.\nPreference Optimization (PO) is a learning framework that trains LLMs based on pairwise pref-\nerence data rather than explicit objective values (Christiano et al., 2017; Ouyang et al., 2022). By\nleveraging preference judgments (e.g., which of two outputs is better according to a metric like\nhelpfulness), PO guides the LLM to produce outputs aligning with the desired criterion (e.g.,\nharmlessness or helpfulness). Direct Preference Optimization (DPO) (Rafailov et al., 2023) is a re-\ncent instantiation of PO, which directly optimizes model parameters to maximize the likelihood\nof preferred outputs, enabling efficient and scalable training for alignment tasks. SimPO (Meng\net al., 2024) further extends DPO to stabilize training and improve preference consistency.\nIntuitively, we find that the pairwise preferences used to align LLMs with respect to certain\nalignment metrics implicitly encode Pareto dominance relations. Specifically, consider pairs of\nanswers y and y−evaluated on two metrics: harmlessness Ga and helpfulness Gb. Preference\npairs Ga(y) ≥Ga(y−) and Gb(y) Gb(y−) define a partial ordering over the responses, indicating\nthat answer y is preferred over y−according to both metrics. This structure of pairwise prefer-\nences corresponds closely to the notion of Pareto dominance, where one solution (y) dominates\nanother (y−) if it is better or equal in all objectives (Ga,Gb) and strictly better in at least one Gb.\nBy collecting many such preference pairs, we implicitly characterize the Pareto front of optimal\ntrade-offs between harmlessness and helpfulness. Leveraging these preference pairs to fine-tune\nLLMs via DPO or SimPO enables the model to internalize complex Priority Alignment efficiently.\nOur SPA framework is built on this formalized solution. We next introduce how SPA constructs\nthe preference pairs to guide the fine-tuning process and effectively approximate lexicographic\noptimization, thereby enabling Priority Alignment of targeted LLMs.\n3\nSPA: Self-Priority Alignment\nUnlike most prior alignment methods, SPA requires no human-annotation data and operates in a\nfully unsupervised manner. It aligns LLMs with goals through self-guided generation, evaluation,\nand optimization, which has been demonstrated effective in many works on self-alignment (Sun\net al., 2023; Wu et al., 2024; Kim et al., 2024). As shown in Figure 2, it begins with diverse sam-\npling and self-refinement, where the targeted model generates multiple responses per prompt,\nevaluates them under dual-alignment objectives, and produces a refined output. A dual-criterion\n4\n"}, {"page": 5, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\ndenoising step filters unreliable or uninformative responses based on consistency and score vari-\nability. Finally, SPA constructs a preference dataset that implicitly encodes Pareto dominance rela-\ntions between the primary and secondary objective and applies a weighted SimPO (Meng et al.,\n2024) loss to optimize the model toward robust, priority-aligned behavior. All prompt templates\nused in SPA are shown in Appendix.\n3.1\nDiverse Sampling with Self-Refinement\nStep 1: Diverse Sampling. Given a dataset D = {xj}m\nj=1 of prompts and a language model πθ,\nwe generate n diverse candidate responses {y(i)\nj }n\ni=1 for each xj using: 1) High-temperature sam-\npling: y(i)\nj\n∼πθ(· | xj;τ) to encourage variation; 2) Prompt variation: using alternative system\nprompts as inspired by Liu et al. (2025).\nStep 2: Self-Refinement. Each sampled response y(i)\nj\nis self-scored based on the primary objective\nGa and secondary objective Gb:\ns(i)\na,j = Sa(xj,y(i)\nj ),\ns(i)\nb,j = Sb(xj,y(i)\nj ).\nHere, Sa and Sb are scoring functions derived from the AI constitution C (e.g., the definition\nof helpfulness, harmlessness, and honesty), which encodes evaluative principles for Ga and Gb.\nRather than refining responses individually, a single improved response ˜yj is generated by incor-\nporating all samples and their scores, as ˜yj ∼πθ(· | xj,{y(i)\nj ,s(i)\na,j,s(i)\nb,j}n\ni=1,C). The refined response\nis then rescored as ˜sa,j = Sa(xj, ˜yj),\n˜sb,j = Sb(xj, ˜yj).\nWe define the response set as Yj = {y(i)\nj }n\ni=1 ∪{ ˜yj}, with each y ∈Yj associated with score pair\n(sa,j(y),sb,j(y)).\n3.2\nDual-Criterion Denoising\nAlthough Diverse Sampling with Self-Refinement yields a set of scored responses for each\nprompt, directly using these scores to construct preference data may be problematic. The self-\nevaluation and refinement process-especially when performed by a weak model-can introduce\nbias, inconsistency, and noise into the preference signals, potentially leading to unreliable or even\nmisleading supervision (Ye et al., 2024).\nTo mitigate these issues, we propose Dual-Criterion Denoising, a two-stage filtering strategy de-\nsigned to select more trustworthy supervision data before preference construction. This approach\nconsists of Consistency-Driven Denoising and Informativeness-Driven Denoising.\nConsistency-Driven Denoising aims to retain only those responses that exhibit stable and supe-\nrior performance. The motivation is that if the refined response fails to outperform all sampled\ncandidates along both evaluation dimensions, it signals potential instability or unreliability in\nthe model’s self-assessment for that prompt. Specifically, we preserve only responses where the\nrefined version strictly surpasses all candidates on both axes: Yperf = {(y(i)\nj ,s(i)\na,j,s(i)\nb,j) ∈Y | ˜sa,j\nmaxi s(i)\na,j and ˜sb,j maxi s(i)\nb,j}. If Yperf is empty, the refined response is discarded.\nWhile consistency filtering addresses internal disagreement, it does not guarantee that the re-\ntained samples are truly informative or robust. Weak models, in particular, are susceptible to\nnoisy or unstable scoring when the quality of responses is highly variable.\n5\n"}, {"page": 6, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nTo further investigate this, we analyze the alignment between a weak model and a strong model\nusing the RV coefficient (Escoufier, 1973). Figure 3 shows the RV coefficient between Mistral-7B-\nInstruct (weak) and GPT-4o (strong) across a subset of 400 WildGuard samples, as samples are\nincluded in order of increasing score covariance (as shown in Equation 1). When fewer than 20%\nof samples are retained, the RV coefficient fluctuates considerably due to the limited sample size\nand lack of statistical significance. However, once more than 32% of samples are included, the\nRV coefficient drops sharply. This indicates that incorporating high-variance samples degrades\nalignment between weak and strong models-highlighting the importance of filtering out such\nsamples.\n0\n20\n40\n60\n80\n100\nCumulative Percentage of Prompts Included (%)\n0.20\n0.25\n0.30\n0.35\n0.40\nRV Coefficient\nPeak: 0.353\nRV Coefficient\nPeak at 32%\nFigure 3: Effect of sample score variance (from low\nto high) on weak-strong model alignment (RV coeffi-\ncient).\nMotivated by this observation, we introduce\nInformativeness-Driven Denoising. For each\nprompt, we compute the covariance matrix of\nthe sampled scores:\nΣj =\n\u0014\nVar(sa,j)\nCov(sa,j,sb,j)\nCov(sb,j,sa,j)\nVar(sb,j)\n\u0015\n.\n(1)\nWe retain responses only if their score variance\nis within an acceptable range, specifically:\nYfinal = {(y(i)\nj ,s(i)\na,j,s(i)\nb,j) ∈Yperf | 0 det(Σj) ≤τ}.\nIf Yfinal is empty, it indicates that the responses are either too unstable (det(Σj) τ) or insufficiently\ninformative (det(Σj) = 0).\n3.3\nConstruction of Preference Dataset\nGiven the filtered response set Yx for each prompt x, we construct the preference pairs that implic-\nitly encode lexicographic order between the primary Ga and secondary objective Gb. Specifically,\neach response y ∈Yx is assigned a two-dimensional score vector (Ga(y),Gb(y)). The score does\ncome from the self-evaluation in Section 3.1.\nTo construct the dataset Dpref of preference pairs (later used by Preference Optimization for LLM\nfine-tuning), we select pairs of responses from Yx that satisfy the lexicographic order between the\nprimary objective Ga and the secondary objective Gb, i.e., the response pair (y,y−) is selected for\nDpref if Ga(y) Ga(y−)\nor\n(Ga(y) = Ga(y−) and Gb(y) Gb(y−)).\nAdditionally, we impose a margin δ 0 to ensure meaningful differences, such that the total score\ndifference ∆(y,y−) = |Ga(y) −Ga(y−)| + |Gb(y) −Gb(y−)| ≥δ. Thus, the set of valid preference\npairs is defined as:\nDpref = {(x, y, y−) : y,y−∈Yx, (Ga(y),Gb(y)) lex (Ga(y−),Gb(y−)),∆(y,y−) ≥δ}.\n(2)\n3.4\nPreference Optimization for Priority Alignment\nThe priority alignment is to optimize the policy πθ under the lexicographic priority Ga(θ) ≻\nGb(θ). As discussed in Section 2, the above-constructed preference pairs implicitly character-\nize the Pareto front of optimal trade-offs between Ga(θ) and Gb(θ) under the lexicographic order.\nLeveraging these preference pairs via PO enables the optimization of πθ for the goal of priority\nalignment.\nPO has recently gained huge traction as a principled framework for LLM alignments (Chris-\ntiano et al., 2017; Ouyang et al., 2022). Several variants of PO have been proposed, such as DPO\n6\n"}, {"page": 7, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nTable 1: Results of SPA compared to the original model (i.e., Vanilla) and the model enhanced by Supervised\nFine-Tuning (SFT). The best performances are highlighted in bold and underlined.\nLlama-3.1-8B-Instruct\nSafeRLHF\nWildGuard\nHoneSet\nMethod\nHarmlessness\nHelpfulness\nHarmlessness\nHelpfulness\nHonesty\nHelpfulness\nVanilla\n9.62\n5.23\n8.22\n6.09\n6.30\n7.75\nSFT\n9.68\n5.57\n9.79\n3.20\n6.11\n7.66\nSPADPO\n9.96\n5.80\n8.35\n5.93\n6.36\n7.81\nSPASimPO\n9.87\n6.98\n8.92\n5.45\n7.74\n7.72\nSPA\n9.90\n7.14\n8.85\n6.22\n7.75\n7.83\nMistral-7B-Instruct\nSafeRLHF\nWildGuard\nHoneSet\nMethod\nHarmlessness\nHelpfulness\nHarmlessness\nHelpfulness\nHonesty\nHelpfulness\nVanilla\n8.83\n7.53\n6.83\n7.15\n5.81\n7.62\nSFT\n8.59\n7.54\n6.64\n6.88\n5.85\n7.66\nSPADPO\n9.06\n8.07\n6.93\n7.16\n5.72\n7.62\nSPASimPO\n9.72\n8.36\n7.19\n7.40\n7.16\n7.77\nSPA\n9.76\n8.39\n7.27\n7.44\n7.18\n7.82\n(Rafailov et al., 2023) and SimPO (Meng et al., 2024). We employ SimPO in our SPA framework\nbecause SimPO normalizes reward by response length to mitigate length bias. Without normal-\nization, models favor unnecessarily long outputs. Importantly, this may distort the model’s un-\nderstanding of helpfulness, equating it with length rather than substance.\nUncertainty-Guided SimPO. Inspired by the previous study (Zhou et al., 2024), given the un-\ncertainty in self-generated samples, we emphasize pairs with lower uncertainty and significant\nscore differences. Let ∆i denote the absolute total score difference between the preferred (y) and\nnot-preferred (y−) responses for the i-th pair: ∆i = |Ga(y) + Gb(y) −Ga(y−) −Gb(y−)|. Let ∆be\nthe mean of all ∆i within the current batch, and define the pairwise weight as wi =\n\u0010\n∆i\n∆\n\u0011α\n, with\nα 0 as a hyperparameter. Derived from SimPO, the alignment loss function used in SPA is then\ngiven by\nLSPA(θ) = −E(x,y,y−)∈Dpref\n\"\nwi · logσ\n \nβ\n|y| logπθ(y | x) −\nβ\n|y−| logπθ(y−| x) −γ\n!#\n.\n(3)\nBy weighting each pairwise term by wi, pairs with larger score gaps ∆i exert a stronger influence\non the gradient, thereby encouraging the policy to more decisively distinguish between responses\nwith significant alignment differences.\nWe fully prove that our method can capture such lexicographic ordering in Appendix.\n4\nExperiments\n4.1\nExperiment Setup\nDatasets.\nWe use SafeRLHF (Ji et al., 2024b,a) (PKU-SafeRLHF) and WildGuard (Han et al.,\n2024) for evaluating the priority alignment of harmlessness and helpfulness while using HoneSet\n(Gao et al., 2024) for evaluating that of honesty and helpfulness. In addition, when SPA employs\n7\n"}, {"page": 8, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\n0.0\n0.5\n1.0\nSPA vs. Vanilla\n(Safe RLHF)\nSPA vs. Vanilla\n(WildGuard)\nSPA vs. Vanilla\n(HoneSet)\n63%\n34%\n73%\n31%\n50%\n9%\n6%\n16%\n18%\nLlama - Harmlessness\n0.0\n0.5\n1.0\n46%\n41%\n86%\n24%\n14%\n14%\n30%\n44%\nLlama - Helpfulness\n0.0\n0.5\n1.0\n45%\n22%\n55%\n47%\n61%\n16%\n9%\n17%\n29%\nMistral - Harmlessness\n0.0\n0.5\n1.0\n39%\n40%\n33%\n21%\n29%\n44%\n40%\n31%\n22%\nMistral - Helpfulness\nWin\nTie\nLose\nFigure 4: Results of pairwise comparison on different datasets. We use GPT-4o as the judge model.\nSafeRLHF for training, we further assess the generalization ability of the aligned model on unseen\ndatasets: JailbreakTrigger (Huang et al., 2024).\nEvaluations. Our primary evaluation methodology combines LLM-as-a-Judge (Zheng et al., 2023)\nwith human validation. For the LLM-as-a-Judge framework, we employ both pairwise compari-\nson and score-based assessment. The judge models used are GPT-4o (OpenAI, 2024) and Claude\n3.5 Sonnet (Anthropic, 2024). We report the evaluation results based on GPT-4o in the main experi-\nments, while the results using Claude 3.5 Sonnet are provided in Appendix. Detailed descriptions\nof the evaluation setup, including judge prompt templates and human annotation procedures, are\navailable in Appendix.\nModels & Baselines & Hyperparameters.\nLLama-3.1-8B-Instruct (AI, 2024) and Mistral-7B-\nInstruct (Mistral AI Team, 2023) are tuned under the framework of SPA in our experiments. They\nhave been widely adopted in prior work (Xiao et al., 2025; Meng et al., 2024); since SPA is an\nunsupervised method, we prefer models that already exhibit a certain level of alignment capa-\nbility (i.e., instruct version instead of base version). As there are no direct comparable baselines\nregarding solving lexicographic optimization, we select some methods that are widely used in\nmulti-objective alignment and unsupervised self-alignment: 1) Reward Soups (Rame et al., 2023)\nlinearly combines models fine-tuned on different reward functions to achieve Pareto-optimal gen-\neralization across diverse alignment objectives. During training, we set different ratios a : b for the\nharmlessness versus helpfulness objectives to control their relative importance in the composite\nreward function, shown as RSa:b in Table 3. 2) Self-Criticism (Tan et al., 2023) aligns LLMs to\nHHH principles (harmlessness, honesty, and helpfulness) by letting them evaluate and improve\ntheir responses through in-context learning and self-generated supervision-without relying on\ncostly human-labeled rewards. Moreover, we include other variant baselines based on SPA. SFT\nleverages only the preferred samples in preference pairs for conducting supervised fine-tuning.\nBy default, SPA employs the loss function Equation 3 for alignment. This loss can be substituted\nwith standard SimPO (i.e., SPASimPO) or DPO (i.e., SPADPO) objectives to evaluate the impact of\ndifferent preference optimization strategies on Priority Alignment. More details of baselines and\nhyperparameter settings are shown in Appendix.\n4.2\nMain Results\nWe show the score-based evaluation on Table 1, pairwise comparison evaluation on Figure 4,\nand baseline comparison on Table 3. To explore whether SPA harms the general utility of the\nmodel after alignment, we conduct experiments on MTBench (Zheng et al., 2023) and MMLU\n(Hendrycks et al., 2020), as shown in Table 2.\n8\n"}, {"page": 9, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nTable 2: The results of utility comparison on MTBench and MMLU.\nMethod\nLlama-3.1-8B-Instruct\nMistral-7B-Instruct\n+ SafeRLHF\n+ WildGuard\n+ SafeRLHF\n+ WildGuard\nMTBench\nMMLU\nMTBench\nMMLU\nMTBench\nMMLU\nMTBench\nMMLU\nVanilla\n8.025\n0.714\n8.025\n0.714\n7.413\n0.594\n7.413\n0.594\nSPA\n8.075\n0.702\n8.013\n0.730\n7.450\n0.584\n7.600\n0.584\nSPA improves alignment across all metrics. All SPA variants outperform both the Vanilla and\nSFT-tuned models in most evaluation settings, demonstrating notable alignment improvements.\nAs shown in Table 1, the full SPA model achieves the best results on Mistral-7B-Instruct across\nall metrics, with especially large gains on SafeRLHF and WildGuard. SPA also maintains strong\nperformance on Llama-3.1-8B-Instruct, ranking among the top models. Figure 4 further shows\nSPA’s higher win rates, including 86% on HoneSet helpfulness, highlighting the effectiveness of\nour alignment strategy.\nJoint modeling of pairwise uncertainty further improves alignment. As shown in Table 1, the\nfull SPA, which incorporates both SimPO normalization and uncertainty-aware weighting, con-\nsistently achieves the best trade-off across objectives. For example, the performance on HoneSet\nof Mistral-7B-Instruct, it achieves top scores on both honesty (7.18) and helpfulness (7.82).\nSPA consistently outperforms all other multi-objective alignment baselines. The comparison of\nSPA and two other baselines in terms of harmlessness and helpfulness is presented in the first two\ncolumns of Table 3. To further compare their overall alignment quality with a single aggregated\nscore, we compute a weighted metric HHλ = (λSharm + Shelp)/(λ + 1), where λ ∈{5,10,20}\ncontrols the relative importance of harmlessness versus helpfulness. Increasing λ reflects the\nhigher priority of harmlessness, as it is the primary alignment objective in our Priority Align-\nment framework. As shown in Table 3, except for the pure helpfulness metric, where SPA slightly\nunderperforms compared to the Self-Criticism, SPA achieves superior results across all other eval-\nuation settings. We hypothesize that Self-Criticism’s higher helpfulness score may stem from its\nrelatively weaker emphasis on harmlessness, leading it to answer some harmful queries instead\nof refusing them. In general, SPA prioritizes safety while maintaining helpfulness compared to\nother baselines.\nTable 3:\nSPA vs Self-Criticism (Self-Cri.)\nand\nReward Soups (RSa:b), evaluated on Llama-8B-\nInstruct (SafeRLHF), on Harmless, Helpfulness,\nand their combination with different λ.\nBaseline\nHar.\nHelp.\nHH5\nHH10\nHH20\nSelf-Cri.\n9.65\n7.68\n9.32\n9.47\n9.56\nRS6:4\n9.87\n6.14\n9.25\n9.53\n9.69\nRS7:3\n9.80\n5.94\n9.16\n9.45\n9.62\nRS8:2\n9.30\n6.85\n8.89\n9.08\n9.18\nRS9:1\n9.90\n6.17\n9.28\n9.56\n9.72\nSPA\n9.90\n7.14\n9.44\n9.65\n9.77\nTable 4:\nGeneralization performance of SPA on\ntwo datasets. Llama-8B-Instruct is trained on the\nSafeRLHF (Harm.: Harmless, Help.: Helpfulness).\nMethod\nJailbreakTrigger\nWildGuard\nHarm.\nHelp.\nHarm.\nHelp.\nVanilla\n9.07\n4.99\n8.22\n6.11\nSFT\n8.91\n5.23\n8.33\n6.08\nSPADPO\n9.81\n6.44\n9.57\n6.25\nSPASimPO\n9.61\n6.23\n9.09\n5.45\nSPA\n9.80\n6.35\n9.29\n5.26\nSPA preserves general utility performance. To assess whether SPA impacts the model’s general\ncapabilities, we evaluate the utility of aligned models on MTBench (Zheng et al., 2023) and MMLU\n(Hendrycks et al., 2020). For the evaluation of MTBench, we follow the way proposed by Zheng\net al. (Zheng et al., 2023). The MMLU evaluation metric is based on accuracy (0 to 1) and is\n9\n"}, {"page": 10, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nimplemented by comparing the model response with the ground-truth answer via LLM-as-a-\nJudge. As shown in Table 2, SPA achieves improved performance across most configurations. On\nMTBench, SPA improves over the Vanilla model in three out of four cases, with gains up to +2.52%\n(Mistral-7B-Instruct + WildGuard). On MMLU, the accuracy differences are minimal, with mixed\nfluctuations around ±2%. These results indicate that the alignment improvements brought by\nSPA do not come at the cost of general-purpose capabilities.\nMoreover, we study the generalization ability of SPA, the impact of iteration counts, and the abla-\ntion study about the effectiveness of the denoising step. Moreover, we also analyze its sensitivity\nto the number of training samples in the Appendix.\nDPO\nIter 1\nIter 2 (diff.) Iter 2 (same)\n6.0\n6.2\n6.4\n6.6\nScore\n5.93\n6.22\n6.35\n6.49\nHelpfulness\nDPO\nIter 1\nIter 2 (diff.) Iter 2 (same)\n8.4\n8.6\n8.8\n9.0\n8.35\n8.85\n8.93\n8.97\nHarmlessness\nFigure 5: Effect of multiple SPA iterations on WildGuard using LLaMA-3.1-8B-Instruct. “Iter 2 (diff.)” uses\na new dataset in the second iteration, while “Iter 2 (same)” reuses the original data.\nHow well does SPA generalize across different datasets? To assess the generalization ability of\nSPA, we evaluate models trained on SafeRLHF directly on two unseen datasets: JailbreakTrigger\nand WildGuard. As shown in Table 4, SPA demonstrates consistently strong and balanced perfor-\nmance across both datasets. On JailbreakTrigger, it achieves a harmlessness score of 9.80 and a\nhelpfulness score of 6.35, clearly outperforming the Vanilla and SFT baselines and matching the\nbest harmlessness scores among all variants. On WildGuard, SPA attains a harmlessness score of\n9.29, which is among the highest, indicating robust generalization in terms of safety. While its\nhelpfulness on WildGuard is slightly lower than some variants like SPADPO, it still maintains a\nstrong overall trade-off between harmlessness and helpfulness. These results highlight that SPA,\ndespite being trained only on SafeRLHF, generalizes effectively to diverse safety-critical scenarios.\nw/o NF\nSPA\n9.7\n9.8\n9.9\n10.0\n9.77\n9.90\nLlama - Harmlessness\nw/o NF\nSPA\n9.6\n9.8\n9.60\n9.76\nMistral - Harmlessness\nw/o NF\nSPA\n6.9\n7.0\n7.1\n7.2\n6.99\n7.14\nLlama - Helpfulness\nw/o NF\nSPA\n8.25\n8.50\n8.21\n8.39\nMistral - Helpfulness\nFigure 6: Ablation study of the denoising in the\nSafeRLHF dataset.\nw/o NF means the results\nwithout the denoising (i.e., noise filtering) com-\nponent.\nWhat is the impact of increasing the number\nof SPA iterations on performance?\nTo assess\nthe effect of iteration count in SPA, we evaluate\ntwo second-iteration strategies: using new, unseen\nprompts (Iter 2 (diff.)) or reusing the same prompts\nwith refined model outputs (Iter 2 (same)). Experi-\nments are conducted on Llama-3.1-8B-Instruct eval-\nuated with WildGuard, a more challenging bench-\nmark than SafeRLHF. As shown in Figure 5, both\nstrategies improve upon the single-iteration base-\nline, confirming the benefit of iterative refinement.\nNotably, reusing the same prompts yields bet-\nter results-especially on the helpfulness metric (6.49\nvs. 6.35)-demonstrating that refining responses on\nthe same context strengthens alignment more effec-\ntively. This likely stems from the model’s ability to\nfocus on correcting subtle, previously missed issues. In contrast, new prompts increase breadth\n10\n"}, {"page": 11, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nbut reduce iteration depth within any given context. Further iterations beyond the second offer\ndiminishing returns, with performance metrics stabilizing. This suggests most alignment gains\noccur early, and later iterations provide limited additional benefit once the model’s behavior has\nlargely converged.\nHow effective is the denoising component within SPA? We perform an ablation study on the\nSafeRLHF to assess the contribution of the denoising component in SPA. As shown in Figure 6, re-\nmoving denoising leads to noticeable drops in both helpfulness and harmlessness, with decreases\nexceeding 0.1 in all cases. These results highlight the importance of incorporating the denoising\nstep into SPA to ensure more significant improvements.\n5\nRelated Work: Alignment in LLMs\nAlignment ensures that LLMs act in line with human values, intentions, and safety goals (Ji et al.,\n2023). Several algorithms address this: PPO uses reinforcement learning with human feedback\n(RLHF) (Schulman et al., 2017; Ouyang et al., 2022), while DPO directly optimizes preferences\nwithout reward models (Rafailov et al., 2023). RRHF achieves PPO-level performance with sim-\npler ranking-based training (Yuan et al., 2023). IPO offers a general preference-learning objective,\navoiding reward modeling and pointwise approximations, with strong theoretical and empirical\nresults (Azar et al., 2024). KTO models human utility via prospect theory, using binary feedback\nto outperform standard methods (Ethayarajh et al., 2024). SimPO enhances DPO with implicit\nrewards and margins, achieving state-of-the-art results without a reference model (Meng et al.,\n2024). Some studies also enhance alignment from the input prompt perspective (Trivedi et al.,\n2025; Cheng et al., 2023). Recent methods also tackle multi-objective alignment (Mukherjee et al.,\n2024; Yang et al., 2024a; Wang et al., 2024; Yang et al., 2024b; Zhou et al., 2023; Kim et al., 2025;\nGupta et al., 2025). MetaAligner enables flexible, plug-and-play multi-objective alignment (Yang\net al., 2024a), and Rewards-in-Context (RiC) uses reward prompts and supervised fine-tuning to\nefficiently approximate Pareto-optimality (Yang et al., 2024b).\n6\nConclusion\nWe present SPA, an unsupervised framework that aligns LLMs by enforcing a strict trustworthy-\nbefore-helpfulness priority. SPA achieves strong improvements across multiple metrics without\nsacrificing general capabilities, offering a scalable alternative to traditional alignment methods.\nBroader Impact\nWhile SPA is designed with high-stakes scenarios in mind-where safety must take precedence-\nits core principle of priority alignment is broadly applicable. Many alignment settings involve\nconflicting objectives (e.g., coherence vs. creativity, efficiency vs. completeness) that cannot be\nadequately addressed by simple weight tuning. SPA’s lexicographic formulation provides a prin-\ncipled mechanism to enforce objective hierarchies, ensuring that critical properties are satisfied\nbefore secondary goals are pursued. This makes SPA a promising foundation for broader do-\nmains such as long-form generation (Han et al., 2023), and tool-augmented reasoning (Wu et al.,\n2025), where structured alignment priorities are essential for robust and controllable behavior.\nBeyond its immediate technical contributions, SPA may also influence future research in multi-\nobjective optimization, value learning, and safe AI governance by providing a template for prior-\nitizing alignment objectives in a structured and theoretically grounded way.\n11\n"}, {"page": 12, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nAcknowledgment\nThis work is supported by the National Science Foundation (No: 2333795). We thank Yanbo Wang,\nZixiang Xu, and Haomin Zhuang for their feedbacks on this work.\nReferences\nAI, M. Introducing llama 3.1: Our most capable models to date, July 2024. URL https://ai.meta.\ncom/blog/meta-llama-3-1/.\nAnthropic. Introducing claude 3.5 sonnet, June 2024. URL https://www.anthropic.com/news/\nclaude-3-5-sonnet. Accessed: 2025-04-20.\nAzar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. A\ngeneral theoretical paradigm to understand learning from human preferences. In International\nConference on Artificial Intelligence and Statistics, pp. 4447–4455. PMLR, 2024.\nBradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired\ncomparisons. Biometrika, 39(3/4):324–345, 1952.\nChen, P.-Y., Shen, H., Das, P., and Chen, T. Fundamental safety-capability trade-offs in fine-tuning\nlarge language models. arXiv preprint arXiv:2503.20807, 2025.\nCheng, J., Liu, X., Zheng, K., Ke, P., Wang, H., Dong, Y., Tang, J., and Huang, M. Black-box\nprompt optimization: Aligning large language models without model training. arXiv preprint\narXiv:2311.04155, 2023.\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement\nlearning from human preferences. Advances in neural information processing systems, 30, 2017.\nEscoufier, Y. Le traitement des variables vectorielles. Biometrics, pp. 751–760, 1973.\nEthayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Kto: Model alignment as\nprospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.\nGao, C., Wu, S., Huang, Y., Chen, D., Zhang, Q., Fu, Z., Wan, Y., Sun, L., and Zhang, X. Hon-\nestLLM: Toward an honest and helpful large language model. In The Thirty-eighth Annual Con-\nference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=\nF7tGQ7b10q.\nGupta, R., Sullivan, R., Li, Y., Phatale, S., and Rastogi, A. Robust multi-objective preference align-\nment with online dpo. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39,\npp. 27321–27329, 2025.\nHan, C., Wang, Q., Peng, H., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Zero-shot\nextreme length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.\nHan, S., Rao, K., Ettinger, A., Jiang, L., Lin, B. Y., Lambert, N., Choi, Y., and Dziri, N. Wildguard:\nOpen one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs. In The Thirty-\neight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL\nhttps://openreview.net/forum?id=Ich4tv4202.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nHuang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y., Gao, C., Huang, Y., Lyu, W., Zhang, Y.,\net al. Position: Trustllm: Trustworthiness in large language models. In International Conference\non Machine Learning, pp. 20166–20270. PMLR, 2024.\nIsermann, H. Linear lexicographic optimization. Operations-Research-Spektrum, 4(4):223–228, 1982.\n12\n"}, {"page": 13, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nJi, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., et al. Ai\nalignment: A comprehensive survey. arXiv preprint arXiv:2310.19852, 2023.\nJi, J., Hong, D., Zhang, B., Chen, B., Dai, J., Zheng, B., Qiu, T., Li, B., and Yang, Y.\nPku-\nsaferlhf: Towards multi-level safety alignment for llms with human preference. arXiv preprint\narXiv:2406.15513, 2024a.\nJi, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., Chen, B., Sun, R., Wang, Y., and Yang, Y. Beaver-\ntails: Towards improved safety alignment of llm via a human-preference dataset. Advances in\nNeural Information Processing Systems, 36, 2024b.\nKim, D., Lee, K., Shin, J., and Kim, J. Spread preference annotation: Direct preference judgment\nfor efficient llm alignment. arXiv preprint arXiv:2406.04412, 2024.\nKim, G.-H., Jang, Y., Kim, Y. J., Kim, B., Lee, H., Bae, K., and Lee, M. Safedpo: A simple approach\nto direct preference optimization with enhanced safety. arXiv preprint arXiv:2505.20065, 2025.\nLiu, A., Bai, H., Lu, Z., Sun, Y., Kong, X., Wang, X. S., Shan, J., Jose, A. M., Liu, X., Wen, L., Yu,\nP. S., and Cao, M. TIS-DPO: Token-level importance sampling for direct preference optimization\nwith estimated weights. In The Thirteenth International Conference on Learning Representations,\n2025. URL https://openreview.net/forum?id=oF6e2WwxX0.\nMeng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with a reference-free\nreward. Advances in Neural Information Processing Systems, 37:124198–124235, 2024.\nMistral AI Team.\nAnnouncing mistral 7b, September 2023.\nURL https://mistral.ai/news/\nannouncing-mistral-7b.\nMukherjee, S., Lalitha, A., Sengupta, S., Deshmukh, A., and Kveton, B. Multi-objective alignment\nof large language models through hypervolume maximization. arXiv preprint arXiv:2412.05469,\n2024.\nOpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: 2025-04-20.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\nSlama, K., Ray, A., et al. Training language models to follow instructions with human feedback.\nAdvances in neural information processing systems, 35:27730–27744, 2022.\nQi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned\nlanguage models compromises safety, even when users do not intend to!\narXiv preprint\narXiv:2310.03693, 2023.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference\noptimization: Your language model is secretly a reward model. Advances in Neural Information\nProcessing Systems, 36:53728–53741, 2023.\nRame, A., Couairon, G., Dancette, C., Gaya, J.-B., Shukor, M., Soulier, L., and Cord, M. Rewarded\nsoups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse re-\nwards. Advances in Neural Information Processing Systems, 36:71095–71134, 2023.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\nShi, R., Chen, Y., Hu, Y., Liu, A., Hajishirzi, H., Smith, N. A., and Du, S. S. Decoding-time language\nmodel alignment with multiple objectives. Advances in Neural Information Processing Systems, 37:\n48875–48920, 2024.\nSun, Z., Shen, Y., Zhou, Q., Zhang, H., Chen, Z., Cox, D., Yang, Y., and Gan, C. Principle-driven\nself-alignment of language models from scratch with minimal human supervision. Advances in\nNeural Information Processing Systems, 36:2511–2565, 2023.\n13\n"}, {"page": 14, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nTan, X., Shi, S., Qiu, X., Qu, C., Qi, Z., Xu, Y., and Qi, Y. Self-criticism: Aligning large language\nmodels with their understanding of helpfulness, honesty, and harmlessness. In Proceedings of\nthe 2023 conference on empirical methods in natural language processing: industry track, pp. 650–662,\n2023.\nTrivedi, P., Chakraborty, S., Reddy, A., Aggarwal, V., Bedi, A. S., and Atia, G. K.\nAlign-pro:\nA principled approach to prompt optimization for llm alignment. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 39, pp. 27653–27661, 2025.\nWang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer,\nR., et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In\nNeurIPS, 2023.\nWang, H., Lin, Y., Xiong, W., Yang, R., Diao, S., Qiu, S., Zhao, H., and Zhang, T. Arithmetic control\nof LLMs for diverse user preferences: Directional preference alignment with multi-objective\nrewards. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8642–8655, Bangkok,\nThailand, August 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.acl-\nlong.468. URL https://aclanthology.org/2024.acl-long.468/.\nWu, J., Zhu, J., and Liu, Y. Agentic reasoning: Reasoning llms with tools for the deep research.\narXiv preprint arXiv:2502.04644, 2025.\nWu, T., Yuan, W., Golovneva, O., Xu, J., Tian, Y., Jiao, J., Weston, J., and Sukhbaatar, S. Meta-\nrewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint\narXiv:2407.19594, 2024.\nXiao, T., Yuan, Y., Chen, Z., Li, M., Liang, S., Ren, Z., and Honavar, V. G. SimPER: A minimalist\napproach to preference alignment without hyperparameters. In The Thirteenth International Con-\nference on Learning Representations, 2025. URL https://openreview.net/forum?id=jfwe9qNqRi.\nYang, K., Liu, Z., Xie, Q., Huang, J., Zhang, T., and Ananiadou, S. Metaaligner: Towards general-\nizable multi-objective alignment of language models. Advances in Neural Information Processing\nSystems, 37:34453–34486, 2024a.\nYang, R., Pan, X., Luo, F., Qiu, S., Zhong, H., Yu, D., and Chen, J. Rewards-in-context: multi-\nobjective alignment of foundation models with dynamic preference adjustment. In Proceedings\nof the 41st International Conference on Machine Learning, pp. 56276–56297, 2024b.\nYe, J., Wang, Y., Huang, Y., Chen, D., Zhang, Q., Moniz, N., Gao, T., Geyer, W., Huang, C.,\nChen, P.-Y., et al. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint\narXiv:2410.02736, 2024.\nYuan, H., Yuan, Z., Tan, C., Wang, W., Huang, S., and Huang, F. Rrhf: Rank responses to align\nlanguage models with human feedback. Advances in Neural Information Processing Systems, 36:\n10935–10950, 2023.\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z.,\net al. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.,\net al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information\nProcessing Systems, 36:46595–46623, 2023.\nZhou, W., Agrawal, R., Zhang, S., Indurthi, S. R., Zhao, S., Song, K., Xu, S., and Zhu, C. Wpo:\nEnhancing rlhf with weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024.\nZhou, Z., Liu, J., Yang, C., Shao, J., Liu, Y., Yue, X., Ouyang, W., and Qiao, Y.\nBeyond one-\npreference-for-all: Multi-objective direct preference optimization. 2023.\n14\n"}, {"page": 15, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nA\nDetails of Experiment Setting\nDatasets. For all training datasets, we randomly sampled a fixed number of original prompts: 300\nfor SafeRLHF and WildGuard, and 400 for HoneSet. The higher number for HoneSet is due to its\nlower conversion rate from honest prompts to preference pairs. For evaluation, we standardized\nthe number of test prompts to 500 across all datasets. For the WildGuard dataset, we follow the\ntrain-test split in itself.\nDetails of Baselines. For the Reward Soups method, we adopt an unsupervised data generation\nstrategy to ensure a fair comparison. Specifically, for each input query, the model is prompted\nto generate n candidate responses. Each response is then independently scored along two di-\nmensions: harmlessness and helpfulness. To compute a final score for each response, we apply\nweighted combinations of the two scores using different ratios (6:4, 7:3, 8:2, and 9:1), reflecting\nvarying emphases on harmlessness. Positive and negative training pairs for alignment are sub-\nsequently constructed by comparing these weighted scores across candidate responses. For Self-\nCriticism, we directly employ its pipeline on the dataset as it’s an unsupervised method. For both\nReward Soups and Self-Criticism, we use proposed Uncertainty-Guided SimPO to fine-tune the\nmodels.\nTraining Details. The training framework is based on the alignment-handbook repo1. We use the\nsetting of full fine-tuning for models in all baselines.\nHyperparameter Setting. The hyperparameter search space of our experiments is shown in Ta-\nble 5. For temperature settings, we use a value of 1 for diverse sampling, while all other cases\n(e.g., evaluation and judgment) are set to 0. The max length of model response is set to 2048\ntokens. The β and γ for SPA loss follow the default setting of SimPO2.\nComputing Resource. All the training experiments in this paper were conducted on 8 × NVIDIA\nH100 (80G).\nTable 5: Hyperparameter search space in the experiments.\nHyperparameter\nSearch Space\nLearning rate\n[5.0e-7, 1.0e-6, 5.0e-6, 1.0e-5]\nα for pairwise weight\n[1, 2, 3, 4]\nρ for denoising\n[10, 20, 30, 40, 50]\nδ for preference dataset construction\n[2, 3, 4]\nnumber of diverse sampling n\n3, 5\nB\nMethod Details\nDetails about the setting of temperature and system prompt. For the harmlessness-helpfulness\npairs, we employ high-temperature sampling; for the honesty-helpfulness pairs, we found that\nhigh-temperature sampling struggles to produce highly differentiated responses, so we instead\nmodify the system prompt to achieve greater variance (i.e., for each response, there is a 0.5 proba-\nbility of substituting the system prompt). Based on the previous study (Gao et al., 2024), the new\nsystem prompt is as follows:\nYou are an honest AI assistant. Always make clear when you can’t access real-time information, detect\nmistakes or missing details in user input, acknowledge that you have no identity, memory, or emotions,\n1https://github.com/huggingface/alignment-handbook\n2https://github.com/princeton-nlp/SimPO\n15\n"}, {"page": 16, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nand admit when tasks require images, audio, or expertise beyond your capability. Be transparent that you\ncannot interact with or perceive the physical world.\nC\nHuman Evaluation\nFor the human evaluation, a total of two Ph.D students and two undergraduate students par-\nticipated in the assessment (they all major in Computer Science and have a background in LLM\nresearch). Screenshots of the evaluation interface are shown in Figure 8. We conducted pairwise\ncomparisons of model responses, then we assessed the accuracy of LLM-based judges by com-\nparing their decisions with human annotations. The alignment rate is reported in Table 6.\nAs shown in Table 6, GPT-4o demonstrates a high degree of consistency with human judgments\nacross all evaluation dimensions.\nSpecifically, when evaluating the outputs of Llama-3.1-8B-\nInstruct and Mistral-7B-Instruct, the agreement rates between GPT-4o’s decisions and human an-\nnotations are consistently high, reaching up to 91% and 94% for harmlessness, and 89% and 92%\nfor helpfulness, respectively. These results suggest that GPT-4o can serve as a reliable automated\njudge in human preference evaluations, maintaining a strong alignment with human standards\nin assessing honesty, harmlessness, and helpfulness.\nTable 6: Human alignment rate of GPT-4o judgment.\nLlama-3.1-8B-Instruct\nHonesty\nHarmlessness\nHelpfulness\nHoneSet\nWildGuard\nSafeRLHF\nHoneSet\nWildGuard\nSafeRLHF\n86%\n82%\n91%\n76%\n81%\n89%\nMistral-7B-Instruct\nHonesty\nHarmlessness\nHelpfulness\nHoneSet\nWildGuard\nSafeRLHF\nHoneSet\nWildGuard\nSafeRLHF\n78%\n86%\n94%\n86%\n84%\n92%\nD\nOther Experiment Results\n100\n150\n200\n9.4\n9.6\n9.8\n10.0\nScore\nHarmlessness\n100\n150\n200\n5.0\n6.0\n7.0\n8.0\nScore\nHelpfulness\nFigure 7: The impact of the num-\nber of training samples.\nResults of SPA with the judge model of Claude 3.5 Sonnet. As\nshown in Table 7, we can see that when using Claude 3.5 Sonnet\nas the judge model, SPA still achieves strong results on SafeRLHF\nand WildGuard, demonstrating the effectiveness of SPA.\nHow does the number of training samples affect the perfor-\nmance of SPA? As shown in Figure 7, increasing the number of\ntraining samples slightly improves harmlessness, while the effect\non helpfulness is less stable: the helpfulness score first drops and\nthen partially recovers. This suggests that SPA maintains strong\nharmlessness with more data, but optimizing helpfulness becomes\nmore challenging. When scaling the training dataset, the opposite\ntrends also imply a potential trade-off between harmlessness and\nhelpfulness.\n16\n"}, {"page": 17, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nTable 7: Results of SPA with the judge model of Claude 3.5 Sonnet.\nLlama-3.1-8B-Instruct\nSafeRLHF\nWildGuard\nMethod\nHarmlessness\nHelpfulness\nHarmlessness\nHelpfulness\nOriginal\n8.52\n5.74\n6.80\n5.94\nSFT\n8.67\n5.90\n7.46\n6.42\nSPADPO\n8.53\n5.69\n7.00\n6.03\nSPASimPO\n9.41\n7.08\n8.17\n6.24\nSPA\n9.48\n7.13\n8.22\n6.40\nMistral-7B-Instruct\nSafeRLHF\nWildGuard\nMethod\nHarmlessness\nHelpfulness\nHarmlessness\nHelpfulness\nOriginal\n7.95\n7.16\n4.94\n5.05\nSFT\n7.42\n6.69\n4.59\n4.68\nSPADPO\n8.38\n7.65\n5.26\n5.21\nSPASimPO\n9.39\n8.37\n5.20\n5.20\nSPA\n9.42\n8.37\n5.19\n5.22\n17\n"}, {"page": 18, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nE\nTheoretical Proof of Lexicographic Ordering with Utility Function\nIn this section we aim to prove that the utility function Equation 4 in the Bradley-Terry (BT) model\n(Bradley & Terry, 1952) when λ is large enough would sufficiently capture the lexicographic or-\ndering, guaranteeing that our alignment procedure will never trade away safety for marginal\ngains in usefulness especially in high-stake cases.\nu(y) = λGa(y) + Gb(y)\n(4)\nAs the first step, for any two responses y1,y2 ∈Y, we need to establish Equation 5. Based on these,\nTheorem 1 asserts that the optimal BT policy strictly favors higher-utility responses by assigning\nthem greater probability, and Theorem 2 shows that the supervised preference loss admits this\nsame policy as its unique global minimizer.\n{\nGa(y1) Ga(y2) =⇒u(y1) u(y2),\n\u0000Ga(y1) = Ga(y2) ∧Gb(y1) Gb(y2)\n\u0001 =⇒u(y1) u(y2).\n(5)\nThe following benign yet simple assumptions are needed to prove Equation 5.\nAssumption 1 (Bounded Utility Components). Ga : Y →[amin,amax] and Gb : Y →[bmin,bmax]\nare bounded functions, i.e., there exist constants amin,amax,bmin,bmax such that for all y ∈Y, Ga(y) ∈\n[amin,amax] and Gb(y) ∈[bmin,bmax].\nAssumption 2 (Secondary Utility Magnitude Bound). Let M = max{|bmin|,|bmax|} denote the max-\nimum absolute value of Gb. This will be used to bound the influence of Gb in the utility function.\nGiven Assumption 1 and Assumption 2, Lemma 1 can be provided and proved to indicate that\nlexicographic ordering is fully captured by utility function Equation 5.\nLemma 1. If λ\n2M\nmin{Ga(y1)−Ga(y2)|Ga(y1)Ga(y2)}, then for all y1,y2 ∈Y,\nu(y1) u(y2)\nwhenever\nGa(y1) Ga(y2)\nor\n(Ga(y1) = Ga(y2) ∧Gb(y1) Gb(y2)).\nProof for Lemma 1. Case 1: Ga(y1) Ga(y2) In this case, the difference in utilities is Equation 6\nu(y1) −u(y2) = λ[Ga(y1) −Ga(y2)] + [Gb(y1) −Gb(y2)]\n(6)\nSince Gb(y1) −Gb(y2) is bounded in Assumption 2 by −2M ≤Gb(y1) −Gb(y2) ≤2M, we can\nguarantee Equation 7\nu(y1) −u(y2) ≥λ[Ga(y1) −Ga(y2)] −2M\n(7)\nTo ensure that u(y1) u(y2), Equation 8 would be naturally required.\nλ[Ga(y1) −Ga(y2)] 2M\n(8)\nThus bound for λ can be described as Equation 9.\nλ\n2M\nmin{Ga(y1) −Ga(y2) | Ga(y1) Ga(y2)}\n(9)\nIn conclusion, for all pairs y1,y2 satisfying Ga(y1) Ga(y2), we have u(y1) u(y2).\nCase 2: Ga(y1) = Ga(y2) and Gb(y1) Gb(y2) In this case, the difference in utilities simplifies\nto Equation 7.\nu(y1) −u(y2) = Gb(y1) −Gb(y2) 0\n(10)\nThus u(y1) u(y2) is obvious.\n18\n"}, {"page": 19, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nTheorem 1 (Optimal Strategy under the Bradley-Terry Model). Assume the utility u(y) is defined by\nEquation 4 and λ satisfies Lemma 1. Define the Bradley-Terry policy as Equation 11.\nπ∗(y | x) =\nπref(y | x) exp\n\u0000u(y)/τ\n\u0001\n∑y′∈Y πref(y′ | x) exp\n\u0000u(y′)/τ\n\u0001\n(11)\nThen for any y1,y2 ∈Y, there exists Equation 12.\nu(y1) u(y2)\n=⇒\nπ∗(y1 | x) π∗(y2 | x)\n(12)\nProof for Theorem 1. Consider the optimal strategy π∗(y | x) defined as Equation 13.\nπ∗(y | x) = πref(y | x)exp\n\u0000 1\nτ u(y)\n\u0001\nZ(x)\n,\nZ(x) = ∑\ny′∈Y\nπref(y′ | x)exp\n\u0012 1\nτ u(y′)\n\u0013\n(13)\nTake any two responses y1,y2 ∈Y. The probability ratio is Equation 14.\nπ∗(y1 | x)\nπ∗(y2 | x) = πref(y1 | x)\nπref(y2 | x) exp\n\u0012 1\nτ [u(y1) −u(y2)]\n\u0013\n.\n(14)\nGiven u(y1) u(y2), Equation 13 indicates Equation 15.\nexp\n\u0012 1\nτ [u(y1) −u(y2)]\n\u0013\n1\n=⇒\nπ∗(y1 | x)\nπ∗(y2 | x)\nπref(y1 | x)\nπref(y2 | x).\n(15)\nUnder mild assumptions (such as πref being neutral or having minimal bias), the above inequality\nimplies: π∗(y1 | x) π∗(y2 | x).\nTheorem 2 (Equivalence of Supervised Loss Minimizer and Bradley-Terry Policy). Assume the\nutility u(y) is defined by Equation 4 and λ satisfies Lemma 1, consider the supervised preference loss as\nEquation 16\nL(θ) = E(x,y+,y−)∼D\nh\nhπθ(y+,y−) −1\nτ\n\u0000u(y+) −u(y−)\n\u0001i2\n(16)\nwhere hπθ(y+,y−) is denoted in Equation 17.\nhπθ(y+,y−) = log πθ(y+ | x)\nπref(y+ | x) −log πθ(y−| x)\nπref(y−| x)\n(17)\nThen any global minimizer θ∗of L(θ) satisfies Equation 18, where πθ∗= π∗as BT optimal policy.\nπθ∗(y | x) ∝πref(y | x) exp\n\u0000u(y)/τ\n\u0001\n(18)\nProof for Theorem 2. Given Equation 16 and Equation 17, the loss reaches its global minimum\nwhen the squared term is exactly zero, which is Equation 19.\nhπθ(y+,y−) = log πθ(y+ | x)\nπref(y+ | x) −log πθ(y−| x)\nπref(y−| x) = 1\nτ [u(y+) −u(y−)]\n(19)\nExponentiating both sides, we get Equation 20.\nπθ(y+ | x)\nπref(y+ | x)\n\u001e πθ(y−| x)\nπref(y−| x) = exp\n\u0012 1\nτ [u(y+) −u(y−)]\n\u0013\n(20)\n19\n"}, {"page": 20, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nThus, proportional relationship can be deprived as Equation 21.\nπθ(y | x)\nπref(y | x) ∝exp\n\u0012 1\nτ u(y)\n\u0013\n(21)\nComparing this to the definition of the optimal strategy π∗, we conclude that the unique global\noptimum of the supervised loss matches exactly the optimal policy derived under the BT model,\nwhich is Equation 22.\nπ∗(y | x) = πref(y | x)exp\n\u0000 1\nτ u(y)\n\u0001\nZ(x)\n(22)\nHence, the supervised learning framework indeed leads to the desired optimal strategy.\n20\n"}, {"page": 21, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nF\nAlgorithm\nAlgorithm 1: SPA: Self-Priority Alignment (Full Procedure)\nRequire: Model πθ; Prompt dataset D = {xj}m\nj=1; AI Constitution C\nRequire: Alignment objectives Ga (primary), Gb (secondary); Hyperparameters n,τ,δ,α,β,γ\n1: for each prompt xj ∈D do\n2:\nSample n responses {y(i)\nj }n\ni=1 ∼πθ(· | xj;τ) with diverse system prompts\n▷Diverse sampling\n3:\nfor each y(i)\nj\ndo\n4:\nEvaluate alignment scores: s(i)\na,j = Sa(xj,y(i)\nj ), s(i)\nb,j = Sb(xj,y(i)\nj )\n▷Self-evaluation\n5:\nend for\n6:\nGenerate refined response: ˜yj ∼πθ(· | xj,{y(i)\nj ,s(i)\na,j,s(i)\nb,j},C)\n▷Self-refinement\n7:\nScore refined response: ˜sa,j = Sa(xj, ˜yj), ˜sb,j = Sb(xj, ˜yj)\n8:\nCombine all responses: Yj ←{y(i)\nj }n\ni=1 ∪{ ˜yj}\n9:\nConsistency Filtering:\n10:\nYperf ←{y ∈Yj : ˜sa,j maxi s(i)\na,j ∧˜sb,j maxi s(i)\nb,j}\n11:\nif Yperf = ∅then\n12:\nContinue to next xj\n▷Skip unreliable samples\n13:\nend if\n14:\nInformativeness Filtering:\n15:\nCompute covariance matrix:\nΣj =\n\u0014\nVar(sa,j)\nCov(sa,j,sb,j)\nCov(sb,j,sa,j)\nVar(sb,j)\n\u0015\n16:\nYfinal ←{y ∈Yperf : 0 det(Σj) ≤ρ}\n17:\nif Yfinal = ∅then\n18:\nContinue to next xj\n19:\nend if\n20:\nStore Yfinal and corresponding scores\n21: end for\n22: Construct preference pairs:\nDpref = {(x, y+, y−) | y+,y−∈Yx, (Ga(y+),Gb(y+)) lex (Ga(y−),Gb(y−)), ∆(y+,y−) ≥δ}\n23: for each pair (x,y+,y−) ∈Dpref do\n24:\nCompute total gap: ∆i = |Ga(y+) + Gb(y+) −Ga(y−) −Gb(y−)|\n25: end for\n26: Compute mean gap: ∆=\n1\n|Dpref| ∑i ∆i\n27: Compute pairwise weights: wi =\n\u0010\n∆i\n∆\n\u0011α\n28: Train model by minimizing weighted SimPO loss:\nLSPA(θ) = −E(x,y+,y−)\n\u0014\nwi · logσ\n\u0012 β\n|y+| logπθ(y+ | x) −\nβ\n|y−| logπθ(y−| x) −γ\n\u0013\u0015\n29: Return trained model πθ⋆←Optimize(πθ, LSPA)\n21\n"}, {"page": 22, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nG\nScreenshot of Human Evaluation\nFigure 8: Screenshot of human evaluation.\nH\nPrompt Template\nSelf-Evaluation\nYou are an impartial AI judge. Evaluate the following response based on two dimensions:\n1. {dim1_name}: {dim1_def}\n2. {dim2_name}: {dim2_def}\nProvide a score from 1 to 10 for each dimension.\nFormat your response strictly as:\n{{\n\"{dim1_name}\": SCORE (integer from 1-10),\n\"{dim2_name}\": SCORE (integer from 1-10)\n}}\n### Response: response\nFigure 9: Prompt template of self-evaluation as well as LLM-as-a-Judge.\n22\n"}, {"page": 23, "text": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization\nSelf-Refinement\nYou are given an instruction, two evaluation dimensions (each with a name and definition), and a set\nof candidate responses, each with scores for the two dimensions.\nYour task is to carefully analyze the instruction, the responses, and their associated scores, and then\ngenerate a refined response that improves upon the weaknesses of the original responses, aiming to\nmaximize the scores in both dimensions.\nHere is the input format:\nInstruction:\n[instruction]\nDimension 1:\nName: [dimension 1 name]\nDefinition: [dimension 1 definition]\nDimension 2:\nName: [dimension 2 name]\nDefinition: [dimension 2 definition]\nResponses and Scores:\n[all responses and their scores]\nNow, generate a single refined response that addresses the instruction and improves the existing\nresponses regarding both evaluation dimensions.\nRefined Response:\n[improved response]\nFigure 10: Prompt of self refinement.\n23\n"}]}