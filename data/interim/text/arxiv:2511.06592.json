{"doc_id": "arxiv:2511.06592", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.06592.pdf", "meta": {"doc_id": "arxiv:2511.06592", "source": "arxiv", "arxiv_id": "2511.06592", "title": "MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making", "authors": ["Zhi Rui Tam", "Yun-Nung Chen"], "published": "2025-11-10T00:44:37Z", "updated": "2025-11-10T00:44:37Z", "summary": "As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.06592v1", "url_pdf": "https://arxiv.org/pdf/2511.06592.pdf", "meta_path": "data/raw/arxiv/meta/2511.06592.json", "sha256": "41a18bf0c1f9e6edcb4a5973ad2d31e04e564f3b204a3d38e7ad0a823ae49135", "status": "ok", "fetched_at": "2026-02-18T02:28:03.287792+00:00"}, "pages": [{"page": 1, "text": "2025-11-11\nMedVoiceBias: A Controlled Study of Audio LLM\nBehavior in Clinical Decision-Making\nZhi Rui Tam1 and Yun-Nung Chen1\n1National Taiwan University, Taipei, Taiwan\nAs large language models transition from text-based interfaces to audio interactions in clinical settings, they\nmight introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on\n170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age,\ngender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs\nvaried by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer\nrecommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices,\nwhich persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully\neliminated gender bias, the impact of emotion was not detected due to poor recognition performance.\nThese results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient‚Äôs\nvoice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We\nconclude that bias-aware architectures are essential and urgently needed before the clinical deployment of\nthese models.\n1. Introduction\nThe rapid deployment of large language models (LLMs) in healthcare presents both a promising frontier\nand a critical concern. Among medical decisions, surgical recommendations are particularly challenging\nfor bias assessment. While these decisions should be driven by clinical factors, research shows that implicit\nbiases often lead to the substitution of demographic proxies, such as using a patient‚Äôs age instead of their\nfrailty as a decisive factor [15]. Disentangling such biases from legitimate clinical variations, like the\nhigher surgery rates for females due to conditions like cholecystitis [3], requires controlled experimental\ndesigns that isolate demographic factors.\nRecent evidence has shown that LLMs exhibit similar patterns of bias. A large-scale analysis of nine\nmodels across 500 emergency department vignettes found that marginalized groups were significantly\nmore likely to receive recommendations for urgent and invasive procedures [18]. This strong correlation\nsuggests that surgical decisions serve as sensitive indicators for algorithmic bias, especially given the\nambiguity inherent in the process.\nThe emergence of audio LLMs introduces additional complexity. Unlike traditional text-based LLMs,\naudio LLMs handle continuous audio signals that carry paralinguistic information, including perceived\ndemographics encoded in accent, pitch, and prosody. These voice characteristics could trigger similar\nsubstitution patterns, leading to biased recommendations based on how a patient sounds rather than\nwhat they say [16].\nTo address this gap, we present a comprehensive framework for assessing demographic bias in audio\nLLMs‚Äô medical decision-making. Our approach focuses on surgical recommendation tasks, leveraging\nCorresponding author(s): y.v.chen@ieee.org\narXiv:2511.06592v1  [cs.CL]  10 Nov 2025\n"}, {"page": 2, "text": "their documented susceptibility to demographic substitution effects. Through controlled experiments\nusing synthesized speech, we isolate the effects of voice characteristics while holding clinical content\nconstant. This methodology allows us to determine whether audio LLMs perpetuate the problematic\npattern of using demographic proxies instead of clinical indicators. Our contributions are 3-fold:\n‚Ä¢ We present the first systematic evaluation of voice-based bias in audio LLMs through binary surgery\ndecisions, revealing how paralinguistic features influence high-stakes medical recommendations.\n‚Ä¢ We create MedVoiceBias with 170 clinical cases from DDXPlus with 36 synthesized speaker profiles\n(age, gender, emotional expressions) to benchmark controlled bias assessment. 1\n‚Ä¢ We demonstrate severe modality-dependent biases (up to 34.9pp deviation between text/audio),\nwith age disparities persisting under chain-of-thought (CoT) while gender bias is eliminated.\n2. Related Work\nThe intersection of speech AI and bias in healthcare is understudied but critical field. We categorize\nexisting research into three key areas: automatic speech recognition (ASR) bias, behavioral bias in audio\nLLMs, and bias in medical AI systems.\nASR and Audio LLM Bias. Speech recognition systems exhibit systematic performance disparities across\ndemographic groups. Koenecke et al. [12] first documented significantly higher word error rates for Black\nspeakers compared to White speakers. Harris et al. [9] demonstrated that intersectional bias patterns\ncompound these disparities, with the Fair-Speech dataset [22] revealing performance gaps exceeding\n40% across demographic groups. Beyond ASR performance, Spoken StereoSet [13] evaluated behavioral\nbiases in speech language models, finding minimal bias scores close to 50% for general stereotype tasks\ninvolving gender and age demographics. However, domain-specific applications, particularly high-stakes\nmedical contexts, may reveal different bias patterns as clinical decision-making involves complex reasoning\nbeyond simple stereotype association.\nBias in Medical AI Systems. Medical AI systems exhibit systematic demographic biases in clinical\ndecision-making. Omar et al. [18] demonstrated that across 432,000 responses from nine language\nmodels on emergency department scenarios, marginalized groups were significantly more likely to receive\nrecommendations for invasive procedures. This reflects demographic substitution patterns in human med-\nical practice, where easily observable characteristics inappropriately influence clinical recommendations\ndespite evidence-based guidelines.\nResearch Gap and Contribution. To the best of our knowledge, no prior work has systematically\ninvestigated how voice characteristics influence medical reasoning in audio LLMs. Surgical recom-\nmendations are an especially suitable testbed for bias evaluation, as they involve binary, high-stakes\ndecisions with documented demographic disparities [1], are governed by clinical guidelines that should\nbe demographic-agnostic, and contain inherent uncertainty that allows bias to surface [6].\nOur work makes the first systematic contribution in this area by leveraging controlled speech synthesis to\nisolate the impact of voice characteristics on surgical decision-making. Through this design, we reveal\nhow the speech modality can introduce previously overlooked bias vectors in medical AI systems.\n1Dataset available at https://hf.co/datasets/theblackcat102/MedVoiceBias\n2\n"}, {"page": 3, "text": "I can't breathe at all, it\nfeels like ¬†...¬†\nDo I need surgery to cure it?\nYes\nNo\nNo\n( Sesame CSM )\nAge : Old, Young\nGender: Male, Female\n6 Emotions\nPatient profile\nPatient\nprofile with\naudio\nprofile\nNo\nYes\nYes\nNo\nNo\nYes\nNo\nYes\nYes\nMisaligned\nAdding persona\nno effect\nFigure 1 ‚à£The framework for evaluating audio LLM bias. A patient‚Äôs text profile is converted to speech\nvia a TTS model, with voices systematically varied by characteristics to assess for implicit bias.\n3. Methodology\n3.1. Motivation\nTo investigate bias in medical reasoning, we focus on two fundamental components of clinical decision-\nmaking: structured diagnostic reasoning and binary treatment recommendations.\nFor structured diagnostic reasoning, we employ the DDXPlus dataset [8], which provides differential\ndiagnoses: a list of 49 plausible conditions aligned with a patient‚Äôs symptoms and widely regarded as\na cornerstone of physician reasoning. By prompting audio LLMs to generate differential diagnoses, we\nmove beyond surface-level accuracy metrics and evaluate whether vocal characteristics systematically\ninfluence the quality and clinical soundness of reasoning.\nFor treatment recommendations, we employ yes/no surgery decisions as clear, high-stakes binary classi-\nfication tasks.2 Binary classification provides a well-established framework for evaluating algorithmic\nbias, yielding unambiguous outcomes that facilitate straightforward fairness measurement [7]. Surgical\ndecisions are particularly consequential, as historical evidence shows that human clinical judgment has\nbeen influenced by patient demographics, resulting in disparities in access to major surgery [4, 11]. By\nrequiring a definitive yes/no recommendation, we create a clear and interpretable signal for bias. This\ndesign enables precise quantification of how vocal profiles may skew audio LLMs‚Äô recommendations,\noffering a critical mechanism to detect and mitigate potential harms before such systems are deployed in\nreal-world clinical practice.\n3.2. MedVoiceBias Dataset Construction\nTo systematically evaluate the influence of audio characteristics, we constructed a dataset designed to\nbenchmark bias assessment in audio LLMs. This dataset construction is detailed below.\n2Our implementation allowed models to output ‚Äúyes,‚Äù ‚Äúmaybe,‚Äù or ‚Äúno,‚Äù but for analysis we treated only ‚Äúyes‚Äù as a positive\ndecision and grouped the others together.\n3\n"}, {"page": 4, "text": "Table 1 ‚à£Statistics of our created MedVoiceBias data. ùëÅrepresents the number of unique voice profile.\nCategory\nCohort\nWER (%)\nLength (s)\nN\nAge\nYoung\n6.1\n34.0\n10\nOld\n8.9\n42.4\n6\nGender\nFemale\n6.6\n35.7\n10\nMale\n7.3\n37.3\n10\nExpression\nHappy\n5.1\n31.0\n4\nLaughing\n5.3\n33.7\n4\nSad\n5.6\n33.7\n4\nConfused\n5.6\n36.5\n4\nEnunciated\n6.3\n38.9\n4\nWhisper\n7.8\n37.8\n4\n3.2.1. Speaker Demographics\nWe sourced utterances from the Common Voice repository [2] (release cv-corpus-22.0-2025-06-20).\nUsing the provided metadata, we focused on two age strata: (i) speakers self-reporting ages 20‚Äì29\n(young) and (ii) speakers self-reporting ages ‚â•60 (old). Within each cohort, we retained only those\nspeakers whose perceived age and gender are unambiguous to human raters. Manual validation was\nessential, as a systematic comparison of 200 randomly sampled profiles revealed discrepancies between\nself-reported and acoustically perceived demographics in 23% of cases (age: 18%, gender: 5%). From\nthis process, we selected 12 speakers, balanced across both age and gender (6M/6F; 6 young/6 old),\nwith demographic classifications confirmed by consensus among three annotators.\n3.2.2. Emotional Variability\nTo examine whether paralinguistic cues influence downstream bias, we augmented the corpus with\ncontrolled emotional renderings. Specifically, we used the Expresso dataset [17], selecting six affective\nconditions: happy, laughing, sad, confused, enunciated, and whisper. Expresso includes two male and two\nfemale speakers self-identified as young, and we incorporated all four speakers expressing each of the six\nemotions available in the dataset.\n3.2.3. Voice Cloning\nIn total, we constructed 36 unique voice profiles. For each selected speaker profile (demographic or\nemotional), we used Sesame-1B [20] to synthesize patient speech from DDXPlus [8] clinical contexts,\nadapted to match the target voice characteristics. Because input length constraints degraded synthesis\nquality, we segmented the original patient profiles at the sentence level. For each sentence, we generated\nthree candidate synthesis samples per voice profile and applied Whisper-v3 [19] to perform ASR on\nall outputs. The sample with the lowest word error rate (WER) was selected as the final audio input\nfor downstream experiments, yielding an average WER of 6.4% across all profiles. The statistics of our\nMedVoiceBias data is detailed in Table 1.\nTo further validate synthesis quality, we applied the MOSA-Net+ [24] automatic speech quality assessment\nmodel. The generated samples achieved average PESQ and intelligibility scores of 3.6/5.0 and 0.97,\nrespectively, indicating consistently high-quality audio for subsequent evaluation.\n4\n"}, {"page": 5, "text": "3.3. Bias Evaluation Metrics\nWe evaluate bias by systematically comparing a model‚Äôs clinical recommendations from voice-based\ninputs against those generated from a text-only control baseline. This approach allows us to isolate and\nquantify the influence of paralinguistic and demographic cues present in the synthesized audio.\nOur primary metric is the surgery recommendation rate, defined as the proportion of cases in which a\nmodel recommends a surgical intervention. For each model and prompting strategy, we first establish\nthis rate using the text-only baseline. We then calculate the recommendation rate for each distinct\ndemographic cohort in our dataset (age, gender, and emotion), enabling a direct comparison of the\nmodel‚Äôs behavior with and without audio cues.\nAll statistical comparisons utilize Fisher‚Äôs exact test [21] to compare surgery recommendation rates\nbetween audio-based cohorts and the text-only baseline. We chose this test over Chi-squared to avoid\nissues with low expected cell counts in our demographic subgroups, ensuring precise statistical inference\nand exact p-values. A difference is considered statistically significant if the p-value is less than 0.05. To\nfocus on clinically meaningful effects, we additionally require an absolute difference of 2% or greater in\nthe surgical recommendation rate to highlight a finding.\nPrior to the main experiments, we performed two preliminary evaluations. First, we conducted a basic\nsanity check on the text-only patient profiles to ensure all models could correctly perform the fundamental\ntask of providing a surgical recommendation. Second, we fed audio from a Common Voice profile to each\nmodel to verify its ability to distinguish age and gender. This evaluation confirmed that the models could\naccurately recognize gender from voice, a key prerequisite for our bias analysis. This entire process was\nrepeated for every model and prompting strategy to ensure a robust and comprehensive analysis.\n4. Experiments\nTo investigate implicit bias in audio LLMs for medical decision-making, we designed a two-phase ex-\nperimental protocol. In the first phase, we validated each model‚Äôs ability to (1) identify demographic\ncharacteristics from voice and (2) generate accurate surgical recommendations from text. In the second\nphase, we compared surgical recommendation rates across different input modalities (text vs. audio)\nand demographic groups.\n4.1. Setting\nWe evaluated six state-of-the-art audio LLMs: DeSTA2.5-Audio 8B [14], Qwen2.5-Omni 3B and 7B [23],\nGemini Flash (2.0, 2.5) [5], and GPT-4o-mini-audio [10]. Before examining potential bias, we first\nconfirmed that these models could reliably distinguish age, gender, and emotions from audio, as well as\nachieve performance above random chance when making surgical recommendations from the original\ntext.\nResults in Table 2 show varying levels of demographic detection across models. Gender prediction\naccuracy ranged from 96.1% to 99.9% for most models (with the exception of GPT-4o-mini at 0%),\nwhile age prediction exhibited wider variance (32.6% to 85.7%). This demographic detection ability is a\nprerequisite for analyzing bias, as models are capable of perceiving demographic cues and then exhibit\ndifferential behavior.\n5\n"}, {"page": 6, "text": "Table 2 ‚à£Audio model performance in text-mode surgery accuracy, voice-based age/gender/emotion\nidentification accuracy (%).\nModel\nSurgery\nAge\nGender\nEmotion\ngpt-4o-mini\n76.2\n0.0\n0.0\n0.0\ngemini-2.0-flash\n68.3\n66.0\n99.5\n0.2\ngemini-2.5-flash\n55.5\n57.4\n99.9\n17.0\nQwen2.5-Omni-3B\n63.9\n66.1\n96.1\n12.2\nQwen2.5-Omni-7B\n60.3\n66.1\n97.5\n16.9\nDeSTA2.5\n57.8\n65.4\n99.5\n40.5\nTable 3 ‚à£Surgery recommendation rates without emotional expressions (%). Bold fonts indicate statisti-\ncally significant differences (p <0.05) compared to Text baseline.\nDirect Answer (DA)\nChain-of-Thought (CoT)\nModel\nText\nText+Profile\nASR\nAudio\nText\nText+Profile\nASR\nAudio\ngpt-4o-mini\n26.5\n26.5\n19.4\n5.3\n14.7\n14.7\n11.2\n12.4\ngemini-2.0-flash\n0.0\n0.0\n14.1\n0.6\n7.6\n7.6\n6.5\n6.5\ngemini-2.5-flash\n27.6\n27.6\n21.2\n31.8\n6.7\n6.7\n23.5\n18.2\nQwen2.5-Omni-3B\n97.6\n97.6\n14.8\n75.3\n31.8\n31.8\n15.4\n35.9\nQwen2.5-Omni-7B\n11.2\n11.2\n5.3\n20.6\n22.7\n22.7\n26.5\n27.6\nDeSTA2.5\n53.9\n53.9\n26.5\n88.8\n26.8\n26.8\n28.3\n28.5\nFor each model, we conducted experiments under two conditions: direct answer (DA) and diagnose-then-\ndecide chain-of-thought (CoT). In the DA setting, the model received the full patient profile (either audio\nor ASR transcripts) and was asked to directly output a binary decision regarding surgical necessity. In\nthe CoT setting, the model was first prompted to infer the possible disease and then determine whether\nsurgery is required.\n4.2. Modality-Induced Bias: Text vs. Audio\nTable 3 reveals substantial modality-dependent bias in surgical recommendations. Under the DA condition,\n66.7% (4 of 6) evaluated models exhibited statistically significant differences between text and audio\ninputs, with recommendation rate shifts ranging from -22.2% for Qwen2.5-3B to +34.9% for DeSTA2.5.\nA striking example is GPT-4o-mini, whose recommendation rate dropped from 26.5% with text to just\n5.3% with audio, a relative reduction of 80%, highlighting strong susceptibility to paralinguistic cues.\nThe ASR condition, which uses transcripts without paralinguistic features, showed intermediate levels of\nbias. For example, Qwen2.5-3B produced recommendation rates of 14.8% (ASR) versus 97.6% (text)\nand 75.3% (audio), suggesting that both transcription errors and vocal characteristics contribute to\ndisparities. Despite relatively strong ASR accuracy (average WER = 6.4%), the ASR condition still\nproduced significant deviations in 3 of 6 models under the DA setting, highlighting how even minor\ntranscription errors can cascade into clinically meaningful differences in decision-making.\n6\n"}, {"page": 7, "text": "Table 4 ‚à£Age and gender effects in audio surgery recommendation rate without emotional expressions\n(%). Bold fonts indicate statistically significant differences (p < 0.05) between groups.\nDirect Answer (DA)\nChain-of-Thought (CoT)\nModel\nYoung\nOld\nMale\nFemale\nYoung\nOld\nMale\nFemale\ngpt-4o-mini\n3.6\n3.6\n3.9\n2.6\n8.4\n5.4\n5.0\n5.0\ngemini-2.0-flash\n0.7\n0.6\n0.6\n0.5\n6.0\n3.7\n3.7\n3.5\ngemini-2.5-flash\n25.3\n17.9\n19.7\n18.8\n16.1\n8.5\n10.1\n9.4\nQwen2.5-Omni-3B\n85.3\n73.5\n76.7\n73.2\n23.7\n28.2\n30.0\n28.1\nQwen2.5-Omni-7B\n16.8\n14.9\n14.3\n15.7\n25.8\n22.6\n22.8\n22.4\nDeSTA2.5\n87.6\n90.1\n93.5\n83.7\n22.6\n20.9\n20.9\n18.9\n4.3. Demographic-Aware Effects\nWe investigate how demographic cues influence the audio LLMs‚Äô clinical decisions to determine if these\nmodels are sensitive to profiles (e.g., more conservative for elderly patients).\n4.3.1. Age-Related Disparities\nTable 4 presents the surgical recommendation rates stratified by patient age. Contrary to our hypothesis\nthat explicit reasoning would compel models to focus solely on the disease, CoT prompting revealed\npersistent and sometimes amplified age-related disparities.\nIn the DA condition, 4 of 6 models showed recommendations that significantly varied by age. Qwen2.5-3B\nshowed the largest difference, recommending surgery for 85.3% of young vs. 73.5% of elderly patients.\nThis 11.8% gap is a clinically meaningful finding that could lead to the systematic under-treatment of\nelderly patients.\nUnexpectedly, CoT prompting increased the prevalence of age-related differences, with five of six models\nshowing significant variations. While the magnitude of these differences slightly decreased on average\n(mean absolute difference: DA = 4.9%, CoT = 3.7%), their increased consistency across models suggests\nthat explicit reasoning may activate shared yet problematic clinical heuristics about age and surgical\nrisk. Interestingly, models like DeSTA2.5 and Qwen2.5-3B reversed their recommendation patterns\nbetween the DA and CoT conditions, indicating that the reasoning pathways fundamentally alter how\nage cues influence a model‚Äôs decisions. The findings imply that current audio LLMs are not yet equipped\nto effectively process and appropriately handle paralinguistic signals.\n4.3.2. Gender-Related Bias\nGender bias patterns differed markedly from age-related disparities, as shown in Table 1. In the Direct\nAnswer (DA) condition, only half of the models exhibited significant gender bias. The absolute differences\nin recommendation rates (ranging from 1.9% to 8.0%) were substantially smaller than those observed\nfor age-related disparities. DeSTA2.5 showed the largest gender gap, recommending surgery for 92.5%\nof male vs. 84.5% of female patients.\nCoT prompting eliminated gender bias across all models, with no significant differences observed. This\ncomplete mitigation contrasts sharply with the age-related effects under CoT. This finding suggests\n7\n"}, {"page": 8, "text": "Table 5 ‚à£Positive rate across all expressions in the DA setting (%).\nModel\nConf\nEnun\nHap\nLau\nSad\nWhi\nRef\ngpt-4o-mini\n3.8\n4.6\n4.2\n4.8\n3.6\n3.8\n26.5\ngemini-2.0\n0.8\n0.8\n1.8\n0.5\n0.5\n0.3\n0.0\ngemini-2.5\n29.2\n27.8\n27.0\n29.5\n29.7\n27.8\n27.6\nQwen2.5-3B\n92.0\n91.2\n92.3\n91.3\n91.8\n89.8\n97.6\nQwen2.5-7B\n17.3\n16.8\n20.3\n17.5\n16.8\n18.2\n11.2\nDeSTA2.5\n90.3\n87.4\n84.7\n87.8\n92.5\n87.9\n53.9\nthat models may encode and process age and gender information through fundamentally different\nmechanisms, and that they are better equipped to handle gender cues than age cues.\n4.4. Emotional Expression Effect\nWe evaluated whether emotional expression in speech affected surgery recommendations across six\nemotion categories. Table 5 shows the percentage of ‚ÄúYes‚Äù recommendations for each emotion in the\nDA setting. Most models exhibited relatively consistent behavior across emotions, with only two models\nshowing significant differences: gemini-2.0 and DeSTA2.5. For gemini-2.0, the ‚Äúhappy‚Äù emotion (1.8%)\nshowed a notably higher recommendation rate compared to other emotions (0.3% to 0.8%).\nHowever, the minimal variation in surgery recommendations across emotional expressions should be\ninterpreted with caution. Given that most models demonstrated extremely low emotion detection accuracy\n(below 17%), the observed consistency likely reflects their inability to perceive emotional cues rather\nthan deliberate emotional robustness. Therefore, only results from models with demonstrated emotion\ndetection capabilities, such as DeSTA 2.5, can provide meaningful insights into genuine emotion-based\nbias.\n5. Conclusion\nThis study provides the first systematic evaluation of voice-based bias in audio LLMs for medical decision-\nmaking. We created MedVoiceBias, which uses 170 clinical cases and 36 synthesized voice profiles to\nenable a controlled bias assessment. Our findings reveal that audio LLMs exhibit significant instability,\nwith surgery recommendation rates deviating a lot from text-only baselines. We demonstrate that\nthese biases manifest across all demographic groups, with emotional expressions further amplifying the\neffect. While explicit reasoning from the models mitigates these biases, it does not eliminate them. This\nhighlights a fundamental architectural challenge: the inability to reliably disentangle a patient‚Äôs medical\ninformation from the paralinguistic features of their voice. These results have critical implications for\nthe deployment of speech-enabled AI in healthcare. Audio LLMs currently pose unacceptable risks of\ncreating disparities based on how patients sound rather than on their medical needs. We conclude that\nbias-aware training and architectural innovations are imperative before clinical deployment to ensure\nthat decisions are driven by medical evidence, not by a patient‚Äôs voice.\n8\n"}, {"page": 9, "text": "References\n[1] Omar M Al Jammal, Shane Shahrestani, Arash Delavar, Nolan J Brown, Julian L Gendreau, Brian V\nLien, Ronald Sahyouni, Luis Daniel Diaz-Aguilar, Omar S Shalakhti, and Martin H Pham. Demo-\ngraphic predictors of treatments and surgical complications of lumbar degenerative diseases: an\nanalysis of over 250,000 patients from the national inpatient sample. Medicine, 101(11):e29065,\n2022.\n[2] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben\nMorais, Lindsay Saunders, Francis Tyers, and Gregor Weber.\nCommon Voice: A massively-\nmultilingual speech corpus.\nIn Proceedings of the Twelfth Language Resources and Evaluation\nConference, pages 4218‚Äì4222, 2020.\n[3] Mark C Bicket, Kao-Ping Chua, Pooja Lagisetty, Yi Li, Jennifer F Waljee, Chad M Brummett, and\nThuy D Nguyen. Prevalence of surgery among individuals in the united states. Annals of Surgery\nOpen, 5(2):e421, 2024.\n[4] Charles E Binkley, David S Kemp, and Brandi Braud Scully. Should we rely on ai to help avoid bias\nin patient selection for major surgery? AMA Journal of Ethics, 24(8):773‚Äì780, 2022.\n[5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier\nwith advanced reasoning, multimodality, long context, and next generation agentic capabilities.\narXiv preprint arXiv:2507.06261, 2025.\n[6] Arabella Dill-Macky, Chiu-Hsieh Hsu, Leigh A Neumayer, Valentine N Nfonsam, and Alexandra P\nTurner. The role of implicit bias in surgical resident evaluations. Journal of Surgical Education, 79\n(3):761‚Äì768, 2022.\n[7] Hossein Estiri, Zachary H Strasser, Sina Rashidian, Jeffrey G Klann, Kavishwar B Wagholikar,\nThomas H McCoy Jr, and Shawn N Murphy. An objective framework for evaluating unrecognized\nbias in medical ai models predicting covid-19 outcomes. Journal of the American Medical Informatics\nAssociation, 29(8):1334‚Äì1341, 2022.\n[8] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. DDXPlus: A new\ndataset for automatic medical diagnosis. Advances in neural information processing systems, 35:\n31306‚Äì31318, 2022.\n[9] Camille Harris, Chijioke Mgbahurike, Neha Kumar, and Diyi Yang. Modeling gender and dialect\nbias in automatic speech recognition. In Findings of the Association for Computational Linguistics:\nEMNLP 2024, pages 15166‚Äì15184, 2024.\n[10] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint\narXiv:2410.21276, 2024.\n[11] Dani Kiyasseh, Jasper Laca, Taseen F Haque, Maxwell Otiato, Brian J Miles, Christian Wagner,\nDaniel A Donoho, Quoc-Dien Trinh, Animashree Anandkumar, and Andrew J Hung. Human visual\nexplanations mitigate bias in ai-based assessment of surgeon skills. NPJ Digital Medicine, 6(1):54,\n2023.\n9\n"}, {"page": 10, "text": "[12] Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor\nToups, John R Rickford, Dan Jurafsky, and Sharad Goel. Racial disparities in automated speech\nrecognition. Proceedings of the national academy of sciences, 117(14):7684‚Äì7689, 2020.\n[13] Yi-Cheng Lin, Wei-Chih Chen, and Hung-yi Lee. Spoken stereoset: on evaluating social bias toward\nspeaker in speech large language models. In 2024 IEEE Spoken Language Technology Workshop\n(SLT), pages 871‚Äì878. IEEE, 2024.\n[14] Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang,\nChee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, et al. Desta2. 5-audio: Toward\ngeneral-purpose large audio language model with self-generated cross-modal alignment. arXiv\npreprint arXiv:2507.02768, 2025.\n[15] Isacco Montroni, Nicole M Saur, Armin Shahrokni, Pasithorn A Suwanabol, and Tyler R Chesney.\nSurgical considerations for older adults with cancer: a multidimensional, multiphase pathway to\nimprove care. Journal of Clinical Oncology, 39(19):2090, 2021.\n[16] Paarth Neekhara, Shehzeen Hussain, Subhankar Ghosh, Jason Li, and Boris Ginsburg. Improving\nrobustness of LLM-based speech synthesis by learning monotonic alignment. In Proc. Interspeech\n2024, pages 3425‚Äì3429, 2024.\n[17] Tu Anh Nguyen, Wei-Ning Hsu, Antony d‚ÄôAvirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal\nRemez, Jade Copet, Gabriel Synnaeve, Michael Hassid, et al. Expresso: A benchmark and analysis\nof discrete expressive speech resynthesis. arXiv preprint arXiv:2308.05725, 2023.\n[18] Mahmud Omar, Shelly Soffer, Reem Agbareia, Nicola Luigi Bragazzi, Donald U Apakama, Carol R\nHorowitz, Alexander W Charney, Robert Freeman, Benjamin Kummer, Benjamin S Glicksberg, et al.\nSociodemographic biases in medical decision making by large language models. Nature Medicine,\npages 1‚Äì9, 2025.\n[19] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In International conference on machine\nlearning, pages 28492‚Äì28518. PMLR, 2023.\n[20] Johan Schalkwyk, Ankit Kumar, Dan Lyth, Sefik Emre Eskimez, Zack Hodari, Cinjon Resnick, Ramon\nSanabria, Raven Jiang, and the Sesame team. Sesame CSM-1B: Conversational speech model.\nhttps://csm1b.com/, 2024. Accessed: 2025-09-02.\n[21] Graham JG Upton. Fisher‚Äôs exact test. Journal of the Royal Statistical Society: Series A (Statistics in\nSociety), 155(3):395‚Äì402, 1992.\n[22] Irina-Elena Veliche, Zhuangqun Huang, Vineeth Ayyat Kochaniyan, Fuchun Peng, Ozlem Kalinli,\nand Michael L Seltzer. Towards measuring fairness in speech recognition: Fair-speech dataset.\narXiv preprint arXiv:2408.12734, 2024.\n[23] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang\nFan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical\nreport. arXiv preprint arXiv:2503.20215, 2025.\n10\n"}, {"page": 11, "text": "[24] Ryandhimas E Zezario, Yu-Wen Chen, Szu-Wei Fu, Yu Tsao, Hsin-Min Wang, and Chiou-Shann\nFuh. A study on incorporating whisper for robust speech assessment. In 2024 IEEE International\nConference on Multimedia and Expo (ICME), pages 1‚Äì6. IEEE, 2024.\n11\n"}]}