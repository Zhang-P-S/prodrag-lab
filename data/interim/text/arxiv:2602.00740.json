{"doc_id": "arxiv:2602.00740", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.00740.pdf", "meta": {"doc_id": "arxiv:2602.00740", "source": "arxiv", "arxiv_id": "2602.00740", "title": "ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement", "authors": ["Ziyan Xiao", "Yinghao Zhu", "Liang Peng", "Lequan Yu"], "published": "2026-01-31T14:12:05Z", "updated": "2026-01-31T14:12:05Z", "summary": "Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns \"how to revise\" rather than just \"what to revise\". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.00740v1", "url_pdf": "https://arxiv.org/pdf/2602.00740.pdf", "meta_path": "data/raw/arxiv/meta/2602.00740.json", "sha256": "ecfde3a960108f4cd91d4d2f6ffdcd1fda48ba36ac96c5da1ac27052b7a3cca9", "status": "ok", "fetched_at": "2026-02-18T02:20:06.333763+00:00"}, "pages": [{"page": 1, "text": "ExperienceWeaver: Optimizing Small-sample Experience Learning for\nLLM-based Clinical Text Improvement\nZiyan Xiao1, Yinghao Zhu1, Liang Peng1, Lequan Yu1*\n1School of Computing and Data Science, The University of Hong Kong\nlqyu@hku.hk\nAbstract\nClinical text improvement is vital for healthcare\nefficiency but remains difficult due to limited\nhigh-quality data and the complex constraints\nof medical documentation. While Large Lan-\nguage Models (LLMs) show promise, current\napproaches struggle in small-sample settings:\nsupervised fine-tuning is data-intensive and\ncostly, while retrieval-augmented generation\noften provides superficial corrections without\ncapturing the reasoning behind revisions. To\naddress these limitations, we propose Expe-\nrienceWeaver, a hierarchical framework that\nshifts the focus from data retrieval to expe-\nrience learning. Instead of simply recalling\npast examples, ExperienceWeaver distills noisy,\nmulti-dimensional feedback into structured, ac-\ntionable knowledge. Specifically, error-specific\nTips and high-level Strategies. By injecting this\ndistilled experience into an agentic pipeline,\nthe model learns \"how to revise\" rather than\njust \"what to revise\". Extensive evaluations\nacross four clinical datasets demonstrate that\nExperienceWeaver consistently improves per-\nformance, surpassing state-of-the-art models\nsuch as Gemini-3 Pro in small-sample settings.\n1\nIntroduction\nThe clinical domain generates large volumes of un-\nprocessed but informative text. Processing such\ndata requires both professional expertise and adapt-\nability to diverse reader needs. Recent reviews\nhighlight the heavy medical burden of such tasks\nin hospitals (Arndt et al., 2017; Sinsky et al.,\n2016; Tai-Seale et al., 2017). As a result, Clin-\nical Text Improvement, which enhances the pre-\ncision, clarity, and compliance of medical notes,\nhas become essential for improving healthcare effi-\nciency (Perkins et al., 2024; Bergomi et al., 2024).\nWhile Large Language Models (LLMs) show\npromise for medical assistance (Sandmann et al.,\n*Corresponding author.\n2025; Moor et al., 2023), reliable text improve-\nment remains challenging due to data scarcity and\nheterogeneous expectations. Unlike general writ-\ning, clinical documentation is shaped by implicit,\ncontext-specific constraints often defined by lim-\nited examples (Beldi et al., 2022).\nExisting\napproaches\nrely\non\nSupervised\nFine-Tuning (SFT) (Ouyang et al., 2022) or\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020), but both struggle with sparse and\nnoisy feedback. SFT is computationally expensive\nand ill-suited to small samples, while standard\nretrieval often fails to capture the reasoning behind\nrevisions.\nTo address these limitations, we advocate for a\nparadigm shift from data retrieval to experience\ncuration, optimization and retrieval. In this study,\nwe propose ExperienceWeaver, a hierarchical ex-\nperience distillation framework designed to learn\nand optimize experience from multi-dimensional,\nunprocessed feedback. We design a two layer dis-\ntillation process, and eventually formulate concrete,\nerror-specific Tips and high-level functional Strate-\ngies. To validate the performance, we conducted\nlarge scale simulated experiments and real-world\nvalidation. Our contributions are summarized as:\n‚Ä¢ Insights: We analyze the mechanisms of experi-\nence abstraction and distillation, validating their\neffectiveness in downstream task\n‚Ä¢ Method:\nWe\npresent\nExperienceWeaver,\na\ntraining-free\nframework\nthat\norganizes\nmulti-dimensional feedback into a hierarchy of\nTips and Strategies, enabling precise experience\ninjection and adaptability in small-sample\nscenarios.\n‚Ä¢ Experiment: Extensive experiments across clin-\nical datasets show that ExperienceWeaver out-\nperforms strong baselines, including Gemini-3\nPro (+5.93%), GPT-5.1 (+6.35%), and standard\n1\narXiv:2602.00740v1  [cs.CL]  31 Jan 2026\n"}, {"page": 2, "text": "Agent-Evolving Pipeline\nRevision \nAgent\nRaw Clinical Text\n(b) Retrieval-based Generation from Memory\n(c) ExperienceWeaver (Ours)\nEvaluation & \nFeedback\nImproved Clinical Text\nMemory \nStorage\nSearch\nRetrieval\nCritic\nEvolving\nHow to improve the efficiency of In-Context Learning?\nPlease revise {RAW INPUT}. \nHere are retrieved similar cases:\n {SIMILAR CASES}\nRetriever\nMemory\nSimilarity\nPlease revise {RAW INPUT}. \nHere are revision strategies:\n {REVISION STRATEGIES}\nHere are detailed tips:\n{TIPS with CASES}\nMemory\nRetriever\nStrategies\nRedundancy in \nStorage \nSparse Information\nafter Retrieval\nDense Information\nfor Retrieval\nReasoning stored \nin Memory \nTips & Cases\nFigure 1: Project Motivation. Improving clinical text with evolving agents relies heavily on the efficiency of\nin-context learning. Current RAG-based methods often fail to provide sufficiently dense information and introduce\nredundancy in memory storage. The proposed method addresses these limitations by weaving experience from\nstored feedback, enabling more effective reasoning within memory.\nRAG, improving text quality with small-sample\nmulti-dimensional feedback.\n2\nRelated Work\n2.1\nLLM-empowered Medical Text Editing\nSince the emergence of large language models\n(LLMs), their application in enhancing text quality\nhas been extensively investigated. Research has\nfrequently benchmarked LLM performance across\na range of medical text processing tasks, including\nstructured reporting (Bergomi et al., 2024; Sacoran-\nsky et al., 2024), translation (Guerreiro et al., 2024),\nand information extraction (Le Guellec et al., 2024;\nTang et al., 2024). However, incorporating feed-\nback is essential to align outputs with clinicians‚Äô\nexpectations. This presents a challenge, as human\nfeedback is often limited in quantity and irregular\nin form. Consequently, there is a need for methods\ncapable of analyzing and extracting hidden infor-\nmation from sparse, unstructured labels to better\nsupport clinical text improvement.\n2.2\nSelf-evolving Agent\nRecent research has increasingly focused on en-\nhancing LLM performance through the develop-\nment of self-evolving agent systems. These ap-\nproaches can be broadly categorized into three di-\nrections: policy training (Beldi et al., 2022), tool\ncreation and utilization (Yao et al., 2023; Qin et al.,\n2023; Schick et al., 2023), and experience-driven\nin-context learning (Wang et al., 2024b; Zhao et al.,\n2024; Qu et al., 2024; Zhang et al., 2025; Zheng\net al., 2025). Policy training typically involves\npost-training an LLM with a reward model to cap-\nture user preferences and guide generation toward\ndesired outputs. Tool creation and utilization ex-\ntend LLM capabilities by integrating or developing\nexternal resources, such as web search, code gener-\nation, and mathematical computation.\nExperience-driven in-context learning plays a\nparticularly important role in small-sample sce-\nnarios. It emphasizes the accumulation of expe-\nrience, supported by critique and summarization\nmechanisms (Qu et al., 2024; Zhao et al., 2024;\nZheng et al., 2025), as well as effective memory\ncontrol (Wang et al., 2024b). These processes en-\nable precise knowledge extraction that enhances\ninference. Notably, this direction is especially well-\nsuited to text improvement tasks, as it facilitates\nefficient adaptation to limited, fine-grained user\nfeedback. Building on this foundation, this study\ninvestigates the construction of an effective experi-\nence system for clinical text improvement.\n3\nMethodology\nThis section presents the methodology of the pro-\nposed ExperienceWeaver and the agentic frame-\nwork for text improvement. The overall pipeline is\nillustrated in Figure 2.\n3.1\nExperience Weaver\nEffective in-context learning requires precise and\ninformative cues within a limited context window.\nTo address this, we propose ExperienceWeaver,\n2\n"}, {"page": 3, "text": "Metric\nScore\nReasoning\nCorrectness\n4/5\nIt correctly detects‚Ä¶\nUsefulness\n3/5\nThe change is meaningful‚Ä¶\n‚ãÆ\n‚ãÆ\n‚ãÆ\nRaw Text\nImproved Text\nUnit in Memory\nRecord 1\nRaw Text \nImproved Text\nFeedbacks\nRecord N\nRaw Text \nImproved Text\nFeedbacks\nMemory Storage\nExperience Pool\nStrategies Pool\nTips Pool\nStage 1: Weaving for Experience\nStage 2: Weaving for Retrieval\n(a) Experience Abstraction\n(b) Combination\nAbstract experiences to \nmeet the evaluation \nexpectation\nRecord 1 \nRecord 2 \nRecord 3 \nRecord N \nRecord N-1 \nRecord N+1 \nLLM\nLLM\nAbstraction Group \nLLM\nLLM\nLLM\nExperience Abstraction\nHierarchical Combination\nMetric 1 Weaving\nMetric 2\nIn stage 1, weave for each metric, and formulate an experience pool\nMetric 1\nMetric 2\nMetric 3 ‚Ä¶\nLLM\nExperience\nComposite \nExperience\nDistilled Experience\nRetriever\nHere are revision strategies:\n {STRATEGIES}\nHere are detailed tips:\n{TIPS with SAMPLES}\nError Detection\nText Revision\nSelf-Critic\nStrategies, ‚Ä¶\nStrategies, ‚Ä¶\nStrategies, ‚Ä¶\nError \nType 1\nError \nType N\nWeaving based \non Error Type\nLLM\nWeaving based \non Functions\n(c) Layered Experience Weaving\nRevision\nDetection\nSelf-Critic\nEvaluation\nStorage\nInput\nOverview:\nOutput\nExperience\nAgents\nScores, reasonings\nMetric 1\nMetric 2\nMetric 3\n‚ãÆ\nContain scores \nand reasoning \nfor each metric\nFigure 2: The pipeline of proposed ExperienceWeaver. The pipeline consists of two stages of weaving: first, a\nhierarchical auto-combination and distillation for experience abstracted from feedback; second, a re-weaving step to\nformulate error-specific tips and functional strategy layers. The goal is to inject distilled multi-layered experience\ninto the three phases in the agentic pipeline: error detection, text revision, and self-critique.\nwhich distills actionable experience prior to re-\ntrieval. The design optimizes two stages: (1) dis-\ntillation, where feedback is compressed into a con-\ndensed form, and (2) retrieval, where abstracted\nexperience is re-structured for practical use.\nThe ExperienceWeaver framework consists of\nthree components, including two stages of experi-\nence weaving. In Stage 1: Weaving for Experi-\nence, raw heterogeneous feedback is transformed\ninto a structured experience pool.\nIn Stage 2:\nWeaving for Retrieval, this pool is reorganized\ninto error-specific tips and functional strategies. Fi-\nnally, a customized retriever integrates the layered\nexperiences into the agentic pipeline. Details of\neach stage and the retriever design are provided in\nthe following subsections.\n3.1.1\nStage 1: Weaving for Experience\nStage 1 follows a ‚Äúdiscover‚Äìsimplify‚Äù paradigm\nand consists of two main steps: Experience Ab-\nstraction and Experience Combination.\nExperience Abstraction The first step is to\ntransform raw feedback into structured experience\nthat is useful to the system. Typically, feedback\ncontains descriptions of revision strengths, weak-\nnesses, or insufficiencies, which cannot directly\nformulate task-related experience. Thus, we define\nan LLM-based Experience Abstractor LLMabstract\nthat distills feedback, such as evaluation results and\ncomments, into experience units.\nFormally, given a set of raw feedback records\n{f1, f2, . . . , fn}, we define an abstraction func-\ntion:\nEa\ni = LLMabstract(fi),\ni = 1, . . . , n\nEach Ea\ni is then grouped by evaluation metric\nm ‚ààM, producing an initial set of experiences\n{Ea\ni,m} for each record and metric. In total, there\nwill be Nrecords √ó Nm sets of experience, each\ntypically contains 5 to 8 experience of 50 words.\nAlthough these experiences capture finite details,\nmany records may generate overlapping distilled\nexperiences. This motivates the second step: com-\nbination.\nExperience Combination The next step is to\nmerge similar experiences across records using a\nhierarchical method. We introduce a hyperparam-\neter, the group size NG. For each iteration, NG\nexperiences are grouped together and passed to a\ncombination function:\nEc\nj = LLMcombine({Ea\ni1, Ea\ni2, . . . , Ea\niNG}),\nwhere LLMcombine, and j = groups in tree. This\nstep merges overlapping experiences while pre-\nserving concrete details. This process follows a\ntree-based format, continuing until the remaining\nnodes are fewer than NG.\nThe final output is a set of unique and informa-\ntive experiences tailored to each feedback dimen-\nsion. In practice, this ends up with approximately\n3\n"}, {"page": 4, "text": "10 distilled experiences per metric, each ranging\nfrom 100 to 300 words.\n3.1.2\nStage 2: Weaving for Retrieval\nFrom an agentic perspective, experience should\nnot only reflect evaluation metrics but also sup-\nport text improvement phases such as error detec-\ntion, revision, and self-critique. In Stage 2, Experi-\nenceWeaver re-weaves distilled experience across\nmetrics into two layers: error-specific tips and func-\ntional strategies.\nError-Specific Tips with Supporting Cases\nError detection is fundamental to text improvement,\nrequiring tips that specify what is incorrect and\nhow to revise it. This step aggregates experiences,\ndenoted as Ec, across metrics. Formally,\nTp,e = LLMtips(Ec, phase = p, error = e)\nwhere p ‚àà{Detection, Revision, Self-Critique}\nand e denotes the error type.\nTo balance completeness and conciseness, we\nintroduce a hyperparameter œÑe controlling the min-\nimum error frequency for weaving. For instance,\nœÑe = 0.1 includes only errors occurring in more\nthan 10% of records. In practice, this produces 5‚Äì8\ndistilled experiences per error type (50‚Äì100 words\neach) across 20‚Äì30 distinct error types.\nFunctional Strategies Beyond detailed tips, a\nhigher-level layer is added to provide concise strate-\ngies that guide the preferred style of generation\nacross the three phases. Formally:\nSp = LLMstrategy(\nX\ne\nTp,e, phase = p)\nwhere Sp denotes the functional strategy for phase\np.\nTypically, this process produces 2‚Äì4 strategies\nper phase, each approximately 50 words in length.\nTogether, the error-specific tips and functional\nstrategies form a layered retrieval structure that\nsupports error detection, revision, and self-critique.\n3.1.3\nExperience retriever\nAs illustrated in Figure 3, the experience context\nis retrieved according to the current phase p, and\nthe error type e, being processed. This ensures that\nthe agent receives guidance at multiple levels of\ngranularity.\nTo regulate the amount of information provided,\nwe introduce a hyperparameter œÑt, which controls\nthe maximum number of tips injected into the con-\ntext. Adjusting œÑt balances informativeness against\nconciseness, allowing the system to avoid redun-\ndancy while still delivering sufficient detail.\nInput:\n‚Ä¢ For phase ùíë‚àà{ùë´ùíÜùíïùíÜùíÑùíïùíäùíêùíè, ùëπùíÜùíóùíäùíîùíäùíêùíè, ùë∫ùíÜùíçùíá‚àíùë™ùíìùíäùíïùíäùííùíñùíÜ} \n‚Ä¢ For detected error type ùíÜ\n========Strategy for Quality Control ========\n1. Strategy 1 for phase ùíë\n2. Strategy 2 for phase ùíë\n3. ‚Ä¶\n==Detailed Tips (Retrieved 3 tips from Layer 2) ==\nSearched Tips for detected error type  ùíÜ\n1. Tips 1 for processing ùíÜ in phase ùíë\n2. Tips 2 for processing ùíÜ in phase ùíë\n3. ‚Ä¶\nExample\nSample Strategies\n‚Ä¢ Prioritize the use of standar\ndized medical terminology,\navoid colloquial or vague   \nexpressions.\nSample Tips\n[Improper Terminology]\n‚Ä¢ Tips: \nFor negative findings, use\n‚Äúno obvious abnormal sign‚Äù\ninstead of  ‚Äúno obvious abn\normalities‚Äùto comply with s\ntandardized  medical termin\nology.\nFormat of Injected Experience\nNumber of Tips \ncontrolled by ùùâùíï \nFigure 3: Sample contexts formulated by Experi-\nenceWeaver.\n3.2\nEvolving Agent System\n3.2.1\nAgentic Framework for Text\nImprovement\nBeyond the details of ExperienceWeaver, the sys-\ntem adopts an agentic framework with multi-agent\ndesign for both revision and evaluation. All agents\nand the orchestrator share the same base model.\nFollowing the ReAct paradigm, the orchestrator\nautomatically determines when to activate detec-\ntion and revision agents.\nOn the revision side, three specialized agents are\ncoordinated by an independent orchestrator, each\nconnected to memory, RAG, and experience re-\ntrieval: (1) Error Detection, which identifies mis-\ntakes and specifies error types; (2) Revision, which\ngenerates improved text incorporating detected er-\nrors; and (3) Self-Critique, which evaluates re-\nvisions and regenerates outputs if the score falls\nbelow 0.6, up to three iterations.\n3.2.2\nMulti-dimensional Feedback as a\nTeacher\nWe also adopt agentic settings on the evaluation\nside, creating a feedback environment that serves\nas a teacher for the evolving agent system. The\naim is to support multi-dimensional improvements\nrather than one-dimensional gains.\nFour feedback dimensions are considered: (1)\nCorrectness, whether revisions are accurate and\nconsistent with identified errors; (2) Formatting,\nadherence to predefined rules ensuring standard-\n4\n"}, {"page": 5, "text": "ized evaluation; (3) Meaningfulness, whether revi-\nsions convey clinically relevant information with-\nout unnecessary additions; and (4) Readability,\nwhether clarity and usability for practitioners are\nimproved.\nAll feedback is generated by the same underly-\ning models with dimension-specific prompts. De-\ntailed prompt designs and examples are provided\nin Appendix A.7.\n4\nExperimental Setups\n4.1\nDatasets\nWe used four clinical text datasets in English and\nChinese, covering both structured and unstructured\nformats: (1) MIMIC Radiology Report (Johnson\net al., 2024), 200 English chest X-ray reports from\na benchmark database; (2) MIMIC Discharge (El-\ngaar et al., 2024), 200 English discharge summaries\nrepresenting semi-structured patient outcome doc-\numents; (3) MIMIC Free Text (Johnson et al.,\n2023), 200 English unstructured clinical notes of\nvariable length; and (4) In-house Radiology Re-\nport, 200 Chinese abdominal CT reports collected\nin 2025 from a hospital in China.\n4.2\nEvaluation Protocol and LLM Judge\nReliability\nWe conduct the preliminary experiments on validat-\ning the LLM-as-a-Judge. Due to the vast amount\nof evaluation required, as it is necessary and in-\nevitable to introduce LLM evaluation. Accordingly,\nwe design a preliminary experiment for assessing\nthe stability and reliability of LLMs as evaluators.\nSpecifically, we define our observation model:\nYi,j(T, a) = ¬µa + Œ≥T (a) + Œ±i(a) + Œ≤j + ri,j(T, a)\nwhere, Yi,j(T) denotes the observed score, ¬µa the\nglobal mean for attribute a, Œ≥T (a) the text effect,\nŒ±i(a) the evaluator effect, and Œ≤j the iteration ef-\nfect. The residual ri,j(T, a) captures generation-\nlevel fluctuations.\nThe full statistical model is described in Ap-\npendix A.1. Validation experiments on 40 samples\nconfirmed that iteration effects are statistically in-\ndistinguishable from zero. Based on preliminary\nresults, we selected optimal base models as eval-\nuators, allowing Œ±i and Œ≤j to be omitted from the\nobservation model.\nQuality improvement is measured as the score\ndifference between texts generated by the base\nmodel and those produced in the experiment. We\nintroduce a binary feedback indicator F ‚àà{0, 1},\nwhere Yi,j(T; F = 1) is the score after system evo-\nlution with feedback, and Yi,j(T; F = 0) is the\nscore before evolution. We adopt pairwise differ-\nence as the main indicator of model performance:\nDi,j = Yij(T; F = 1) ‚àíYij(T; F = 0)\nwhere Di,j = 0 means no changes, and positive\nvalues means positive improvement.\n4.3\nSetups of Main experiments\nBaselines methods.\nFor the main experiments,\nwe adopted Deepseek-chat (DeepSeek, 2025a) as\nthe base model and evaluated ExperienceWeaver\nagainst two categories of baselines.\nFirst, to assess the impact of the experience\nweaver, we compared with state-of-the-art LLMs,\nincluding Deepseek R1 Search (Guo et al., 2025),\nDeepseek Thinking Mode (DeepSeek, 2025b),\nGemini-3 Pro (Deepmind, 2025), GPT-5.1 (Ope-\nnAI, 2025), and Claude-4.5 Pro (Anthropic., 2025).\nThese models directly revised raw clinical text\nwithout retrieval-augmented generation (RAG),\nself-critique, or experience injection.\nSecond, we included in-context learning base-\nlines, such as RAG with unprocessed memory, re-\ntrieval based solely on error similarity, and intrinsic\nself-critic modules performing iterative revision us-\ning internal knowledge.\nImplementation details.\nWe selected 200 sam-\nples from each of the four data sources and split\neach dataset 50:50 into training and testing sub-\nsets. Both subsets were processed through the\nbase model; training outputs populated memory for\nRAG-based models and ExperienceWeaver, while\ntesting outputs served as baselines for comparison.\nSystem hyperparameters were set to control Ex-\nperienceWeaver: weaving group size NG = 4, min-\nimum error frequency threshold œÑe = 4, and max-\nimum retrieved tips œÑt = 5. Ablation studies on\nhyperparameter sensitivity and performance impact\nare presented in Section 5.4.\n4.4\nSetups of Real-world Validation\nExperiments\nIn addition to the main simulation experi-\nments, we conducted a validation study involving\nhuman-labeled error detection to ensure real-world\nreliability. The dataset comprised 1,000 radiology\nreport errors, annotated in a blinded, head-to-head\nmanner by two medical experts (Dr. A with 10\n5\n"}, {"page": 6, "text": "years of experience and Dr. B with 6 years), serv-\ning as the ground truth. This dataset was split\ninto 500 training and 500 testing cases. During\nthe training phase, we selected Œ≥1 correctly and\nŒ≥2 incorrectly detected cases per error type, set-\nting Œ≥1 = Œ≥2 = 10 with a fixed woven base of 10.\nExperienceWeaver generated reflections on these\ninstances and applied hierarchical experience weav-\ning, producing distilled strategies and actionable\ntips for application to the test set.\nIn testing,\nwe evaluated three base mod-\nels,\nDeepSeek-chat\n(DeepSeek,\n2025a),\nMistral-large (Mistral, 2025) and GPT-5.1 (Ope-\nnAI, 2025) on 500 errors. To study the effect of\nexperience length, we introduced a hyperparameter\nœÑ ‚àà{1, 3, 5}, specifying the maximum number of\ntips per error type. Performance was measured us-\ning Accuracy, Macro-Precision, and Macro-Recall.\nA case study on Error Detection further illustrates\nhow ExperienceWeaver operates in practice.\n5\nExperimental Results\nThe experiments were designed to address the fol-\nlowing research questions:\n‚Ä¢ RQ1: How can an evaluation framework be con-\nstructed using LLM-as-a-Judge, and how can its\nconsistency and reliability be validated?\n‚Ä¢ RQ2: How can ExperienceWeaver effectively\ntransform multi-dimensional, extended feedback\ninto actionable experience, and to what extent\ndoes the learned experience enhance clinical text\nimprovement?\n‚Ä¢ RQ3: How effective is ExperienceWeaver in\npractical small-sample clinical text processing\ntasks?\n‚Ä¢ RQ4: What factors influence the effectiveness\nof ExperienceWeaver, particularly regarding the\nvolume of feedback and the granularity of weav-\ning?\n5.1\nRQ1: Evaluator Validation\nTo ensure the reliability of the LLM-as-a-Judge\nframework, we defined a statistical model and con-\nducted hypothesis tests on 40 samples. Full details\nare provided in Appendix A.1 and results in Ap-\npendix A.3. The analysis revealed two key find-\nings: (1) the LLM evaluator is sensitive to dif-\nferences across clinical texts, and (2) the average\nscore remains consistent when controlling for text\nand evaluator effects. These results form the basis\nfor subsequent experiments.\nWe then developed metrics to quantify bias, sta-\nbility, precision, skewness, and differentiability of\ngenerated scores. Definitions are provided in Ap-\npendix A.4. A diverse set of base models was eval-\nuated, including Mistral (Mistral, 2025), GPT-5\nMini, GPT-5.1 (OpenAI, 2025), Claude-4.5 Son-\nnet (Anthropic., 2025), GLM-Flash (Zeng et al.,\n2025), Gemini-3 Pro (Deepmind, 2025), Qwen-2.5\n7B, Grok-4,\nand DeepSeek-Chat (DeepSeek,\n2025a).\nFigure A.2 shows score distributions\nacross metrics and models.\nFigure 4 ranks model performance across quality\ndimensions. Mistral and GPT-5 Mini consistently\nplaced in the top four, with average ranks of 2.2 and\n2.8, compared to Claude-4.5 at 4.4. While rankings\nmay differ from ground-truth alignment, under our\nno-ground-truth setting Mistral and GPT-5 Mini\ndemonstrated superior consistency and resilience,\nsupporting the validity of our framework. Both\nalso lie on the cost‚Äìperformance frontier, as shown\nin Figure 9 in Appendix A.4.\nBias\nStability\nPrecision\nSkewness\nDifferentiability\nFigure 4: Performance Rank of base models as LLM-\nas-a-Judge.\n5.2\nRQ2: Comparative Results\nOverall performance.\nTable 1 compares Expe-\nrienceWeaver with baseline agentic methods and\nSOTA LLMs. The weaving model consistently im-\nproved text quality, outperforming strong baselines\nsuch as GPT-5.1 and Gemini-3 Pro. Average en-\nhancements were 0.794 for English Chest X-ray Re-\nports, 0.456 for Chinese Abdominal Reports, 0.412\nfor Discharge Reports, and 0.137 for Clinical Free\nText, corresponding to gains of 19.85%, 11.40%,\n10.30%, and 3.43% on a 5-point Likert scale.\n6\n"}, {"page": 7, "text": "Data Type\nRadiology Report\nDischarge Report\nFree Text\nData Source\nMIMIC Chest X-ray\nIn house\nMIMIC Discharge\nMIMIC Free Text\nEvaluator\nMistral\nGPT5-mini\nAvg\nMistral\nGPTMini\nAvg\nMistral\nGPT5-mini\nAvg\nMistral\nGPT5-mini\nAvg\nSection 1: In-Context Learning Methods\nRAG\n0.225\n0.225\n0.225\n0.200\n0.888\n0.544\n0.625\n0.262\n0.444\n0.300\n0.300\n0.300\nSelf-Critic\n0.175\n0.425\n0.300\n0.525\n0.788\n0.656\n0.538\n0.300\n0.419\n0.262\n0.475\n0.369\nSection 2: Direct SOTA LLMs\nDeepseek R1 Search\n0.137\n0.488\n0.312\n0.175\n0.700\n0.438\n0.475\n0.300\n0.388\n-0.025\n0.088\n0.031\nDeepseek Thinking\n0.075\n0.538\n0.306\n0.650\n0.762\n0.706\n0.425\n0.188\n0.306\n0.350\n0.350\n0.350\nGemini-3 Pro\n0.420\n0.450\n0.435\n0.680\n0.750\n0.715\n-0.470\n0.200\n0.200\n-0.470\n-0.080\n-0.275\nGPT-5.1\n0.140\n0.280\n0.210\n0.440\n0.680\n0.560\n-0.400\n0.320\n0.320\n-0.400\n0.270\n-0.065\nClaude-4.5 Sonnet\n0.760\n0.510\n0.635\n0.790\n1.060\n0.925\n-0.350\n0.110\n0.110\n-0.350\n0.240\n-0.055\nSection 3: Weaving Experience\nWeaving in Detection\n0.138\n0.488\n0.312\n0.375\n0.750\n0.562\n0.575\n0.688\n0.631\n0.375\n0.188\n0.281\nWeaving in Revision\n0.462\n0.400\n0.431\n0.188\n0.875\n0.531\n0.588\n0.400\n0.494\n0.125\n0.288\n0.206\nWeaving in Self-Critic\n0.150\n0.350\n0.250\n0.425\n0.688\n0.556\n0.500\n0.337\n0.419\n0.238\n0.388\n0.312\nWeaving in Total\n0.775\n0.812\n0.794\n0.338\n0.575\n0.456\n0.525\n0.300\n0.412\n-0.025\n0.300\n0.137\nTable 1: Comparative experimental results across data sources and evaluation metrics. Scores are reported as the\naverage pairwise difference between baseline revison by base model (DeepSeek-chat), and the listed methods. Bold\nvalues indicate the top two performances.\nComparison across data sources.\nPerformance\nwas most reliable on English Chest X-ray, Chi-\nnese Abdominal CT, and English Discharge Re-\nports. Improvements in Clinical Free Text were\nmodest due to its unstructured format and length;\nhere, self-critic mechanisms proved most effective.\nComparison across baselines.\nBuilt on the\nchat-version of DeepSeek, ExperienceWeaver con-\nsistently outperformed R1 Search, Deep Thinking,\nand advanced models such as Gemini-3 Pro and\nGPT-5.1. On challenging datasets like Discharge\nReports and MIMIC Free Text, it demonstrated\ngreater resilience than these SOTA systems.\n‚ÄùRecord ID\": ‚Äùrecord_id\",\n‚ÄùErrors in the Report\": \"The \ndiagnostic opinion mentions C3-6 \nlevel spinal canal stenosis \",\n‚ÄùGround Truth\": ‚ÄùOmission\",\n‚ÄùErrors Specification\": ‚ÄùThis is not \nclearly described in the examination \nfindings, which only mention \nintervertebral foramen stenosis \",\nGround Truth\n‚ÄùLLM Labelled Error Type\": \n‚ÄùConflicts between findings and \nexaminations\",\n‚ÄùLLM Reasoning with Detection\": ‚ÄùThe \nerror description points out that \nthe diagnostic opinion refers to \n'C3-6 level spinal canal stenosis,' \nwhile the examination findings only \ndescribe \n'intervertebral \nforamen \nstenosis.' These differ on a key \nexamination \nitem \n(spinal \ncanal \nstenosis), \nresulting \nin \ninconsistency. This corresponds to \nerror type b, 'contradiction between \nexamination \nitems \nand \nfindings,' \nmeaning \nthat \nthe \ndiagnostic \nconclusion conflicts with or lacks \nsupport \nfrom \nthe \nexamination \ndescription.\"\nBase Model Generation\nDistilled Experience to Tackle the Mistakes\nThe key to distinguishing \"Omission\" \nfrom \"Contradiction\" lies in the logical \nrelationship.\nModels often misclassify \n\"diagnostic opinions not \nmentioning examination \nfindings\" as \"Contradiction.\" \nHowever, \"Omission\" is \nessentially missing information \n(something that should have \nbeen written but was not), \nwhereas \"Contradiction\" \nrequires a logical conflict \n(such as opposite \ndescriptions). For improvement, \ndefinitions must be strictly \nfollowed: if the issue is only \nmissing information without \nconflict, it should be \nclassified as Omission; only \nwhen the content is \ninconsistent (e.g., the \ndiagnostic opinion mentions \nsomething that the examination \nfindings do not describe) \nshould it be classified as \nContradiction.\nSummarize \ncommon \nmistakes\nProviding concise highlights\nOffer \nconcrete \ndetails\nFigure 5: Case Study to explain how experience im-\nproves detection accuracy.\n5.3\nRQ3: Real-world Validation Experiments\nof Error Detection\nTable 2 shows the results of woven and injected\nexperience on the validation task. Across all three\nbaseline models, incorporating experience yielded\nsubstantial gains, with accuracy, precision, and re-\ncall improving by up to 20.4%, 16.3%, and 28.6%,\nrespectively. A moderate level of detail (œÑ = 3)\nprovided the most stable performance.\nThis\nexperiment\nillustrates\nhow\nExperi-\nenceWeaver operates in scenarios where strong\nbaseline models fail to perform reliably on small\ntasks. Such models may struggle in specific small-\nsample cases due to misalignment between their\nintrinsic knowledge and the unique correctness\ndistribution of the task.\nExperience weaving\naddresses this limitation by distilling fine-grained\nreflections on individual errors into precise,\nhigh-level, actionable knowledge. This directly\nimproves detection accuracy. The effectiveness\nof this mechanism is particularly notable given\nthat the experience was derived from only 10 true\ncases and 10 false cases. A case study in figure5\nis provided to further demonstrate how woven\nexperience influences in practice.\n5.4\nRQ4: Further Analysis\nThe ExperienceWeaver framework includes several\nhyperparameters that shape its performance. We\nfocus on two key ones: (1) feedback group size\nin Stage 1 (weaving for experience) and (2) the\nminimal error frequency threshold for weaving in\nretrieval. Experiments were conducted on MIMIC\n7\n"}, {"page": 8, "text": "Models\nDeepSeek-chat (DeepSeek, 2025a)\nMistral-Large (Mistral, 2025)\nGPT 5.1 (OpenAI, 2025)\nMetric\nAccuracy\nPrecision\nRecall\nAccuracy\nPrecision\nRecall\nAccuracy\nPrecision\nRecall\nBase\n0.692\n0.706\n0.629\n0.726\n0.720\n0.692\n0.702\n0.714\n0.645\nBase + Experience (œÑ = 1)\n0.834\n0.805\n0.736\n0.851\n0.815\n0.890\n0.724\n0.734\n0.689\nBase + Experience (œÑ = 3)\n0.826\n0.821\n0.747\n0.874\n0.831\n0.813\n0.818\n0.794\n0.728\nBase + Experience (œÑ = 5)\n0.846\n0.814\n0.743\n0.840\n0.807\n0.748\n0.812\n0.773\n0.726\nImprovement (%)\n+19.5%\n+16.3%\n+18.7%\n+20.4%\n+15.4%\n+28.6%\n+16.5%\n+11.2%\n12.9%\nTable 2: Performance on the Error Detection task, comparing the base model and the model with experience. œÑ\ndenotes the maximum number of tips per error injected. Bold numbers indicate the optimal performance. Percentage\nimprovements are calculated using the optimal score relative to the base model.\nChest X-ray reports under the main study settings,\nwith GPT-5 Mini and Mistral-Large as evaluators.\nAt the core lies a dual trade-off: preserving con-\ncrete details versus efficient abstraction in weaving,\nand ensuring sufficient injected experience versus\navoiding redundancy in retrieval.\nFigure 6: Performance across different group size\nFigure 7: Performance across different minimal errors\nComparing different weaving group size.\nThe\nsize of weaving groups determines number of unit\nfeedback are combined for abstraction in the first\nstage . It is expected that smaller groups preserve\nfiner detail, while larger groups may improve effi-\nciency in weaving.\nAs shown in Figure 6, increasing group size\nleads to diminishing improvement for the GPT-5\nMini evaluator, likely due to a loss of concrete-\nness when too much feedback is processed at once.\nIn terms of cost and time, token usage does not\ndecrease significantly with larger groups, which\nmight because input feedback remains constant\nand dominates the token. The primary advantage\nof larger groups lies in reduced processing time,\nwhich drops substantially as group size increases.\nTaken together, experience weaving is more ef-\nfective in hierachical structure than a parallel one,\nwith acceptable trade-offs in processing time.\nComparing different minimal error frequency\nto weave.\nThe minimal error frequency is a hyper-\nparameter in the second stage to set up threshold on\nfrequency for errors to be included. For example,\na threshold of 8 means that only errors occurring\nat least eight times are added to the tips dictionary.\nLower thresholds produce a broader pool but may\nreduce precision in retrieval. As shown in Figure\n7, higher thresholds tend to improve performance\nunder GPT-5 Mini as the evaluator. The results\nsuggest that focusing on highly frequent errors is\nmore effective and efficient.\n6\nConclusion\nIn this study, we introduced ExperienceWeaver, a\nhierarchical experience distillation and optimiza-\ntion paradigm for multi-dimensional text improve-\nment in small-sample scenarios.\nExperiments\ndemonstrated significant performance gains, with\nresults in some cases surpassing SOTA models.\nReal-world validation further clarified the mech-\nanisms of experience weaving, underscoring its\npractical value. We believe these findings support\nmore efficient adoption of LLMs and advance agen-\ntic approaches to text improvement.\n7\nLimitations\nWhile our study demonstrates the effectiveness of\nthe proposed framework, several limitations should\nbe acknowledged.\nVariation across LLM-as-a-Judge models.\nDif-\nferent LLM base models, when used as judges,\n8\n"}, {"page": 9, "text": "exhibit distinct evaluation behaviors due to vari-\nations in architecture and training. Although we\nmitigated bias by selecting models with relatively\nstable and consistent patterns, residual variation\nremains. We conducted large scale preliminary\nexperiments on generated score, and hypothesis\ntesting and selected the optimal LLM as the base\nmodel.\nLimited sample size.\nOur experiments relied on\napproximately 200 training and testing samples per\ndata source, which is modest compared to typi-\ncal LLM studies. While in-context learning meth-\nods are well suited to small-sample scenarios, this\nconstraint limits generalizability. The results sug-\ngest that weaving-based methods can outperform\nmore complex model variants even under limited\ndata, but future work should explore scalability\nwith larger datasets to confirm robustness.\nPotential risks.\nEeploying LLMs for clinical text\nimprovement entails inherent risks. The primary\nconcern is the potential for ‚Äúhallucination‚Äù, where\nthe model might generate plausible but factually in-\ncorrect medical details, or conversely, omit critical\nclinical findings during the revision process. Addi-\ntionally, there is a risk of over-correction, where the\nmodel might alter the physician‚Äôs specific stylistic\nnuances or intent in favor of standardized phras-\ning. To mitigate these risks, this system is designed\nstrictly as an assistive tool to support, rather than\nreplace, human judgment. We emphasize that final\nverification by qualified medical professionals re-\nmains essential before any automated revisions are\nfinalized.\n8\nEthical Considerations\nThis study was conducted in strict adherence to\nestablished ethical guidelines and regulatory stan-\ndards for medical research. We strictly complied\nwith the data use agreements and specific licensing\nterms for all datasets employed, including the cre-\ndentialed access requirements for MIMIC-IV and\nMIMIC-CXR. For the in-house dataset, all clini-\ncal texts underwent rigorous de-identification to\nremove Protected Health Information (PHI) prior\nto processing, ensuring no private data was ex-\nposed to external Large Language Models. The\nresearch protocol received approval from the In-\nstitutional Review Board (IRB). Due to the retro-\nspective nature of the study using anonymized data,\nthe requirement for individual patient consent was\nwaived. Regarding the human validation study, we\nobtained written informed consent from all partic-\nipating medical experts, who were fairly compen-\nsated for their professional time and contributions.\nReferences\nAnthropic. 2025.\nIntroducing Claude Sonnet 4.5\n‚Äî anthropic.com. https://www.anthropic.com/\nnews/claude-sonnet-4-5.\n[Accessed 05-01-\n2026].\nBrian G Arndt, John W Beasley, Michelle D Watkinson,\nJonathan L Temte, Wen-Jan Tuan, Christine A Sinsky,\nand Valerie J Gilchrist. 2017. Tethered to the ehr:\nprimary care physician workload assessment using\nehr event log data and time-motion observations. The\nAnnals of Family Medicine, 15(5):419‚Äì426.\nAmal Beldi, Salma Sassi, and Abedrazzek Jemai. 2022.\nLearn2sum: A new approach to unsupervised text\nsummarization based on topic modeling. In Proceed-\nings of the 14th international conference on manage-\nment of digital ecosystems, pages 136‚Äì143.\nLaura Bergomi, Tommaso M Buonocore, Paolo Anton-\nazzo, Lorenzo Alberghi, Riccardo Bellazzi, Lorenzo\nPreda, Chandra Bortolotto, and Enea Parimbelli.\n2024. Reshaping free-text radiology notes into struc-\ntured reports with generative question answering\ntransformers.\nArtificial Intelligence in Medicine,\n154:102924.\nGoogle Deepmind. 2025.\nGemini 3 ‚Äî deep-\nmind.google.\nhttps://deepmind.google/\nmodels/gemini/. [Accessed 05-01-2026].\nDeepSeek. 2025a. DeepSeek ‚Äî deepseek.com. https:\n//www.deepseek.com/en. [Accessed 05-01-2026].\nDeepSeek. 2025b.\nThinking Mode | DeepSeek\nAPI\nDocs\n‚Äî\napi-docs.deepseek.com.\nhttps://api-docs.deepseek.com/guides/\nthinking_mode. [Accessed 05-01-2026].\nMohamed Elgaar, Jiali Cheng, Nidhi Vakil, Hadi Amiri,\nand Leo Anthony Celi. 2024.\nMeddec: Medical\ndecisions for discharge summaries in the mimic-iii\ndatabase.\nNuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa\nCoheur, Pierre Colombo, and Andr√© FT Martins.\n2024. xcomet: Transparent machine translation eval-\nuation through fine-grained error detection. Transac-\ntions of the Association for Computational Linguis-\ntics, 12:979‚Äì995.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\n9\n"}, {"page": 10, "text": "Alistair Johnson, Tom Pollard, Roger Mark, Seth\nBerkowitz, and Steven Horng. 2024.\nMimic-cxr\ndatabase. PhysioNet10, 13026:C2JT1Q.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nBastien Le Guellec, Alexandre Lef√®vre, Charlotte\nGeay, Lucas Shorten, Cyril Bruge, Lotfi Hacein-\nBey, Philippe Amouyel, Jean-Pierre Pruvo, Gregory\nKuchcinski, and Aghiles Hamroun. 2024. Perfor-\nmance of an open-source large language model in ex-\ntracting information from free-text radiology reports.\nRadiology: Artificial Intelligence, 6(4):e230364.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt√§schel, and 1 others. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances\nin neural information processing systems, 33:9459‚Äì\n9474.\nMistral. 2025. Introducing Mistral 3 | Mistral AI ‚Äî mis-\ntral.ai.\nhttps://mistral.ai/news/mistral-3.\n[Accessed 05-01-2026].\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein\nAbad, Harlan M Krumholz, Jure Leskovec, Eric J\nTopol, and Pranav Rajpurkar. 2023. Foundation mod-\nels for generalist medical artificial intelligence. Na-\nture, 616(7956):259‚Äì265.\nOpenAI.\n2025.\nIntroducing\nGPT-5\n‚Äî\nope-\nnai.com.\nhttps://openai.com/index/\nintroducing-gpt-5/. [Accessed 05-01-2026].\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, and 1\nothers. 2022. Training language models to follow in-\nstructions with human feedback. Advances in neural\ninformation processing systems, 35:27730‚Äì27744.\nScott W Perkins, Justin C Muste, Taseen Alam, and\nRishi P Singh. 2024. Improving clinical documen-\ntation with artificial intelligence: A systematic re-\nview. Perspectives in health information manage-\nment, 21(2):1d.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, and 1 others. 2023. Toolllm: Facilitating\nlarge language models to master 16000+ real-world\napis. arXiv preprint arXiv:2307.16789.\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai,\nShuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong\nWen. 2024.\nFrom exploration to mastery:\nEn-\nabling llms to master tools via self-driven interac-\ntions. arXiv preprint arXiv:2410.08197.\nEthan Sacoransky, Benjamin YM Kwan, and Donald\nSoboleski. 2024. Chatgpt and assistive ai in struc-\ntured radiology reporting: A systematic review. Cur-\nrent Problems in Diagnostic Radiology, 53(6):728‚Äì\n737.\nSarah Sandmann, Stefan Hegselmann, Michael Fujarski,\nLucas Bickmann, Benjamin Wild, Roland Eils, and\nJulian Varghese. 2025.\nBenchmark evaluation of\ndeepseek large language models in clinical decision-\nmaking. Nature Medicine, pages 1‚Äì1.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. 2023.\nToolformer: Language models can teach themselves\nto use tools. Advances in Neural Information Pro-\ncessing Systems, 36:68539‚Äì68551.\nChristine Sinsky, Lacey Colligan, Ling Li, Mirela\nPrgomet, Sam Reynolds, Lindsey Goeders, Johanna\nWestbrook, Michael Tutty, and George Blike. 2016.\nAllocation of physician time in ambulatory practice:\na time and motion study in 4 specialties. Annals of\ninternal medicine, 165(11):753‚Äì760.\nMing Tai-Seale, Cliff W Olson, Jinnan Li, Albert S\nChan, Criss Morikawa, Meg Durbin, Wei Wang, and\nHarold S Luft. 2017. Electronic health record logs\nindicate that physicians split time evenly between\nseeing patients and desktop medicine. Health affairs,\n36(4):655‚Äì662.\nY Tang, Z Xiao, X Li, Q Zhang, EW Chan, IC Wong,\nand 1 others. 2024. Large language model in medical\ninformation extraction from titles and abstracts with\nprompt engineering strategies: A comparative study\nof gpt-3.5 and gpt-4. Arxiv.\nAman Singh Thakur, Kartik Choudhary, Venkat Srinik\nRamayapally, Sankaran Vaidyanathan, and Dieuwke\nHupkes. 2025. Judging the judges: Evaluating align-\nment and vulnerabilities in llms-as-judges. In Pro-\nceedings of the Fourth Workshop on Generation,\nEvaluation and Metrics (GEM2), pages 404‚Äì430.\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,\nBinghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu,\nTianyu Liu, and 1 others. 2024a. Large language\nmodels are not fair evaluators. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n9440‚Äì9450.\nZora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and\nGraham Neubig. 2024b. Agent workflow memory.\narXiv preprint arXiv:2409.07429.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nAohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin\nChen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao\n10\n"}, {"page": 11, "text": "Zeng, Jiajie Zhang, and 1 others. 2025. Glm-4.5:\nAgentic, reasoning, and coding (arc) foundation mod-\nels. arXiv preprint arXiv:2508.06471.\nGenghan Zhang, Weixin Liang, Olivia Hsu, and Kunle\nOlukotun. 2025.\nAdaptive self-improvement llm\nagentic system for ml library development. arXiv\npreprint arXiv:2502.02534.\nAndrew Zhao, Daniel Huang, Quentin Xu, Matthieu\nLin, Yong-Jin Liu, and Gao Huang. 2024. Expel:\nLlm agents are experiential learners. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 19632‚Äì19642.\nBoyuan Zheng, Michael Y Fatemi, Xiaolong Jin,\nZora Zhiruo Wang, Apurva Gandhi, Yueqi Song,\nYu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neu-\nbig, and 1 others. 2025. Skillweaver: Web agents can\nself-improve by discovering and honing skills. arXiv\npreprint arXiv:2504.07079.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, and 1 others.\n2023. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in neural information pro-\ncessing systems, 36:46595‚Äì46623.\nA\nAppendix\nA.1\nStatistical Model in Evaluator Score\nGeneration\nEvaluator determines the reliability of the exper-\niment results.\nThere are common doubts that\nLLM-as-a-Judge has intrinsic instability or biases\n(Thakur et al., 2025; Wang et al., 2024a). How-\never, \"Just-Ask-GPT-4\" (Zheng et al., 2023) still\nis a common paradigm in practice because of its\nconvenience and scalability.\nOur experiment allow certain fluctuations in the\nLLM evaluators. The mean thing is to make sure it\ncan generate stable output, with little fluctuations\nand random noise. To validate this, we design the\nfollowing statistical model and experiment.\nIn our scenarios, we build the following model\nto model the evaluation.\nThere are NT text samples, T1, T2, ..., TNT . For\neach text T, we evaluate it based on several at-\ntributes A = {ai : i = 1, 2, ..., Na}. For instance,\nA = {Readability, Formating, ...}. Each text T\nhas a unknown true attribute vector,\nŒ∏(T) = (scorea1, scorea2, ..., scoreNa) ‚ààRNa.\nTo evaluate the attribute, we introduce Ne eval-\nuators. In this experiment, we repeat k times for\nevaluators on each text T. Therefore, we define\nthe observation Yi,j(T) be the evaluation results\nfor evaluator i, in iteration j.\nWe define our observation model:\nYi,j(T) = Œ∏(T) + bi(T) + Œµi,j(T)\nwhere, Yi,j(T) is the observed score, bi(T) is the\nevaluator-specific bias (may depend on T), and\nŒµi,j(T) is zero-mean noise with variance œÉ2\ni (T).\nWe assume the following models to construct\nthe residual,\nYi,j(T, a) = ¬µa + Œ≥T (a) + Œ±i(a) + ri,j(T, a)\nwhere, ¬µa denotes global mean for attribute a,\nŒ≥T (a) denotes text fixed effect, and Œ±i(a) denotes\nthe fixed effect of evaluators. Then the estimated\nerror term becomes\nÀÜŒµi,j(T, a) := ri,j(T, a)\nA.2\nEmpirical Distribution of Generated\nScores\nFigure 8 presents the empirical score distribution\nacross four evaluation metrics. It illustrates the\nmean scores, standard deviations, and the relative\ncontributions of key factors to the overall score\nvariability, including text effect, rater (evaluator)\neffect, iteration effect, and residual variance.\nThe figure reveals that text content is the domi-\nnant source of score variation, which is desirable,\nas evaluation scores should primarily reflect dif-\nferences in the input text. An effective evaluator\nshould exert minimal influence on the score itself.\nFor instance, Qwen 2.5 7B under the Correctness\nmetric serves as a counterexample, where the base\nmodel alone substantially lowers the output score,\nindicating evaluator bias. Notably, the effect of it-\neration is minimal, suggesting that models produce\nconsistent scores across repeated evaluations. This\nconsistency is critical for ensuring score stability\nand reliability. Lastly, residual variance highlights\ndiffering levels of fluctuation across base models.\nIn our model selection process, we selected the\nbase models that minimize such residual fluctua-\ntions to ensure robust and interpretable evaluation\noutcomes.\nA.3\nHypothesis Testing for the Consistency\nand Reliability of Scores\nTherefore, we design the following hypothesis test-\ning procedures to verify the simulated evaluators‚Äô\n11\n"}, {"page": 12, "text": "Figure 8: Score distribution across LLM models as base model.\nconsistency and reliability. The hypothesis are con-\nstucted with the following questions.\nIs the generated score stable? In our experi-\nment, we introduce multiple runs to validate the sta-\nbility of generation. In the above statistical model,\nit is equal to validate the effects of iteration Œ≤j.\nAs we expected iteration to be a random influence\nto the model, so we use random-effect model as\nŒ≤jN(¬µŒ≤, œÉ2\nŒ≤)\nH0 : ¬µŒ≤ = 0.\nWe use t-test for the above model. Second, we\ncan also check whether the residual have equal\nmeans across runs. We can consider the run-wise\nmean residuals ¬Ørj =\n1\nNT Ne\nP\ni,T ri,j,T , and con-\nstruct a hypothesis testing as as follows\nH0 : E[¬Ørj] = 0, ‚àÄj\nIs the generated score sensitive enough? An-\nother critical fundamental to ensure that the score\ncan represent the text quality is that text (T) is a\ndeterminant of score. Therefore, we can construct\nthe following test on whether texts differs in their\nmean scores.\nH0 : Œ≥T = 0\n‚àÄT\nwhich can be tested using an F-test.\nHow does the base model affects the sensitiv-\nity and reliability of the score? To verify whether\nbase model will impact the score, we can construct\nthe following test\nH0 : Œ±i = 0\n‚àÄi\nH0 : V ar(Œµi,u,T |i) = œÉ2, ‚àÄi\nFor the first test, we can leverage the F-test and\nfor the second test We can use Levene‚Äôs test on\ngrouped by base model.\nWith consideration of consistency and sensitivity\nof the generated model, we will find two base mod-\nels that is at the top of cost-performance frontier.\nA.3.1\nResults of Hypothesis Testing\nTable 3 and Table 4 summarize the statistical out-\ncomes. Based on the testing results, we can draw\nthe following conclusions with statistical support:\n(1) Text has a significant effect on average scores\nand contributes substantially to between-text\nvariance.\nThis indicates that the generated\nscores are sensitive to differences in text con-\ntent.\n(2) The choice of evaluator significantly affects\nboth the mean and variance of scores. This\ndemonstrates that selecting different base mod-\nels as evaluators inevitably introduces hetero-\ngeneity.\n12\n"}, {"page": 13, "text": "Score Type\nText Mean\nText Variance\nRater Mean\nRater Variance\nRun Bias\nRun Stability\nRun Trend\nResidual Variance\nUsefulness\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\nCorrectness\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\nReadability\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\nFormat\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n‚úì\n‚úì\nTable 3: Summary of Hypothesis Testing Results Across Score Types. ‚úódenotes to reject H0 and ‚úìdenotes to accept\nH0\nScore Type\nText Mean\nText Variance\nRater Mean\nRater Variance\nRun Bias\nRun Stability\nRun Trend\nResidual Variance\nUsefulness\n2.92e-166\n0.000\n1.88e-46\n9.21e-20\n0.671\n0.122\n0.002\n0.004\nCorrectness\n2.44e-13\n0.000\n0.000\n1.39e-103\n0.499\n0.108\n0.001\n0.000\nReadability\n7.49e-294\n0.000\n1.24e-218\n7.28e-39\n0.996\n0.075\n0.000\n0.048\nFormat\n0.000\n0.000\n1.19e-79\n1.32e-52\n0.895\n0.783\n0.912\n0.619\nTable 4: P-values from Hypothesis Testing Across Score Types\n(3) There is no evidence that iteration effects shift\nthe average away from zero. This suggests that\nevaluators are consistent across repeated evalu-\nations of the same text under identical settings.\nThe evaluation itself remains stable despite\nthe influence of text and evaluator, highlight-\ning the potential of the mean score generated\nby simulated evaluators as a consistent and re-\nliable metric.\n(4) Residuals across the four metrics exhibit het-\nerogeneity. This shows that the four metrics\ncapture distinct dimensions of evaluation, even\nwhen assessed by the same simulated LLM\nevaluator.\nA.4\nPerformance of Different LLMs as\nEvaluator\nA.4.1\nMetric Definition\nTo rigorously compare the performance of different\nbase LLM models as evaluators, we introduce a set\nof diagnostic metrics derived from the proposed\nstatistical framework. These metrics are computed\nfrom the same experiment to the hypothesis testing.\nSubsequently, model comparison and selection is\nconducted by ranking models, and Figure 4 sum-\nmarize the ranking results.\nThe metrics are defined as follows:\nBias. Bias measures systematic deviations in the\nmean attribute values introduced by different base\nmodels. An ideal evaluator should align closely\nwith the collective consensus with values closer to\nzero.\nBias =\n\f\fŒ±i(a)\n\f\f\nStability. Stability measures the consistency of\nevaluations across repeated iterations. We there-\nfore define stability measured by the variance of\nthe iteration effect, with smaller values indicating\ngreater consistency.\nStability = Var(Œ≤j)\nPrecision.\nPrecision measures the extent of\nrandom fluctuations in the residual of the model.\nTherefore, precision is measured by the standard\ndeviation of the residual term, with values closer\nzero denoting greater precision.\nPrecision = SD\n\u0000ri,j(T, a)\n\u0001\nSkewness. A balanced evaluation should avoid\nsystematic leaning toward either extreme of the\nscale. We therefore compute skewness, with values\ncloser to zero indicating balanced distribution.\nSkewness = Skew\n\u0000Yi,j(T, a)\n\u0001\nDifferentiability. Differentiability measures the\nextent to which a model distributes scores across\nthe available scale levels. We adopt the entropy of\nthe score distribution, with higher values reflecting\ngreater differentiation of scores.\nH = ‚àí\nX\ni\npi log(pi)\nwhere pi denotes the proportion of scores as-\nsigned to scale level i.\nA.4.2\nPerformance Statistics\nTable 5 reports the quantitative evaluation of nine\nbase LLM models across five diagnostic metrics:\nbias, stability, precision, skewness, and entropy. To\nsynthesize these findings, Table 6 presents the rank\nordering of models within each metric, together\nwith a total rank score obtained by summing across\nall dimensions. Notably, GPT-5 mini and Mistral\nachieve the lowest total rank, indicating balanced\n13\n"}, {"page": 14, "text": "performance across metrics and suggesting their\nsuitability as robust evaluators. Furthermore, we\nalso plot the cost-performance frontier of the nice\nbase models, as shown in Figure 9. GPT-5 mini\nand Mistral also shows their advantages in showing\nthe optimal cost-performanc balances.\nTable 5: Evaluation Metrics for Base Models\nModel\nBias\nStability\nPrecision\nSkewness\nEntropy\nDeepseek\n0.239\n0.011\n0.706\n1.112\n1.923\nClaude-4.5\n0.188\n0.004\n1.083\n0.484\n2.168\nGemini-3Pro\n0.176\n0.003\n0.946\n0.743\n1.957\nGLM-Flash\n0.070\n0.005\n0.881\n0.736\n2.021\nGPT-5 mini\n0.114\n0.003\n0.752\n0.513\n2.081\nGPT-5.1\n0.183\n0.005\n0.890\n0.771\n2.050\nGrok-4\n0.244\n0.087\n0.796\n0.783\n2.073\nMistral\n0.059\n0.005\n0.676\n0.589\n2.183\nQwen 2.5 7B\n0.878\n0.087\n0.696\n0.013\n1.932\nTable 6: Ranked Evaluation Metrics Across Base Mod-\nels\nModel\nBias\nStability\nPrecision\nSkewness\nEntropy\nTotal\nMistral\n1\n4\n1\n4\n1\n11\nGPT-5 mini\n3\n1\n4\n3\n3\n14\nClaude-4.5\n6\n3\n9\n2\n2\n22\nGLM-Flash\n2\n4\n6\n5\n6\n23\nGemini-3Pro\n4\n2\n8\n6\n7\n27\nGPT-5.1\n5\n4\n7\n7\n5\n28\nQwen 2.5 7B\n9\n8\n2\n1\n8\n28\nGrok-4\n8\n8\n5\n8\n4\n33\nDeepseek\n7\n7\n3\n9\n9\n35\nFigure 9: Cost performance frontier for bases models as\nevaluator.\n14\n"}, {"page": 15, "text": "A.5\nWhat Experience being Woven?\nFigures 10 and 11 illustrate representative out-\nputs from the ExperienceWeaver process. Stage 1\ndemonstrates how raw feedback is abstracted\nand distilled into structured experiences, while\nStage 2 shows the subsequent re-weaving into\nerror-specific tips and functional strategies.\nA.6\nSample of ExperienceWeaver Retrieval\nPlease evaluate the following revised version\nof the medical report:\nOriginal Report Findings: No obvious ab-\nnormalities in both lungs. Heart size and shape\nare normal. No obvious abnormalities in the\nmediastinum. Diaphragm surface is smooth,\ncostophrenic angles are sharp.\nImpression: No obvious abnormalities in\nboth lungs.\nRevised Report Findings: No obvious ab-\nnormal signs in both lungs. Heart size and\nshape are normal. No obvious abnormal signs\nin the mediastinum. Diaphragm surface is\nsmooth, costophrenic angles are sharp.\nImpression: No obvious abnormal signs in\nboth lungs.\nRelevant Historical Experience (from Ex-\nperience Weaver)\n=== Strategy for Quality Control ===\n‚Äî Prioritize the use of standardized med-\nical terminology, avoid colloquial or vague\nexpressions, and ensure professionalism and\naccuracy of terms.\n‚Äî Maintain consistency in terminology us-\nage; the same concept should use the same\nstandardized term throughout the report.\n=== Detailed Tips (Retrieved 3 tips from\nLayer 2) ===\n[Improper Terminology Usage] Errors:\n‚Äî For negative findings, use ‚Äúno obvious\nabnormal signs‚Äù instead of ‚Äúno obvious abnor-\nmalities,‚Äù to comply with standardized medi-\ncal terminology.\n‚Äî Terminology should remain professional,\navoiding colloquial expressions.\n‚Äî When revising terminology, pay attention\nto context to ensure the revised terms align\nwith the overall style of the report.\nPlease refer to these historical experiences\nto evaluate the current revision.\nEvaluation Requirements 1. Provide a\nquality score between 0‚Äì1 (1 = perfect, 0 =\ncompletely unacceptable). 2. List specific is-\nsues (issues). 3. List strengths (strengths).\n4. Give a recommendation: accept (accepted,\ncan be directly submitted to the evaluation\nsystem), revise (needs further revision), or\nreject (rejected, needs rewriting). 5. Provide\na brief reasoning.\n15\n"}, {"page": 16, "text": "\"formatting\": [\n{\n\"record_id\": ‚Äùrecord_id\",\n\"format_score\": \"2\",\n\"reasoning\": \"Overall, this report provides detailed clinical information and \nincludes multiple necessary elements (medical history, physical examination, \nimportant test results, etc.). It uses many professional medical terms and \npresents a structured approach to clinical recording. However, the formatting \ndoes not meet key requirements of the task and contains many issues. The \nmost important problem is the absence of clearly labeled mandatory sections: \nFINDINGS (detailed examination/imaging/lab findings) and IMPRESSION \n(summary/imaging or diagnostic interpretation), which makes the format \nnon-compliant. In addition, the text contains numerous placeholders ([ ]), \ninconsistent labeling (some under Chinese headings, others as English \nabbreviations or marked with asterisks...\",\n\"strengths\": [\n\"Comprehensive content covering patient information, chief complaint, \npresent illness, past history, physical examination, and key \nlaboratory/microbiology results\",\n\"Extensive use of professional medical terminology, with an overall \nappropriate clinical tone\",\n\"Structured format (e.g., chief complaint, past history, physical examination, \nimportant test results) facilitates clinical review and information retrieval\"\n],\n\"issues\": [\n\"Does not include clearly labeled mandatory sections 'FINDINGS' and \n'IMPRESSION', failing structural requirements\",\n\"Contains numerous placeholders (e.g., [patient name], [date], [x]), affecting \ncompleteness and readability of the format\",\n\"Laboratory and microbiology results are inconsistently formatted, with \nunclear use of asterisks, inconsistent units, and incomplete/broken \ninformation (e.g., antimicrobial susceptibility tables not fully presented)\",\n\"Terminology is mostly correct but inconsistent and mixed (Chinese and \nEnglish, parentheses/asterisks combined), lacking unified labeling \nstandards\",‚Ä¶\n]\n}\nMemory Storage\nStage 1: Weaving for Experience\n\"record_id\": [\n{\n‚Äùinitial_experience\": [\n\"Did not provide clearly labeled FINDINGS and IMPRESSION section titles as\nrequired (this is mandatory, and the absence is a major issue)\",\n\"Report contains numerous placeholders '___' that were not replaced, affecting\nreadability and completeness\",\n\"Terminology, abbreviations, and capitalization are not consistent (e.g., TVH/TVT-O\nSling vs. TVT-O sling, mixed formats for Hgb/Hct/MCV)\",\n\"Formatting of test results, vital signs, and fluid intake/output data is chaotic,\nlacking standardized itemization and unit alignment\",\n\"Medication and usage descriptions are not standardized, with repetition and\ninconsistencies (dosage, formulation, frequency descriptions mixed together)\",\n\"Discharge\nfollow-up\nand\ncontact\ninformation\nare\nincomplete\n(Followup\nInstructions are empty), with some key dates missing\",\n\"Punctuation and formatting are not neat (e.g., multiple line breaks, extra periods,\ninconsistent bullet points)\"\n]\n}\n]\n(a) Experience Abstraction\n(b) Combination\n\"formatting_experience\": [\n\"```json\",\n\"[\",\n\"\\\"1. Report Structure and Section Titles: The report must contain clear, independent, \nand consistently formatted top-level sections. It must use fully capitalized and bold \n'FINDINGS' and 'IMPRESSION' as standard titles, respectively for collecting all detailed \nexamination findings (such as physical exam, laboratory, imaging, vital signs) and all \ndiagnostic summaries, clinical interpretations, and follow-up recommendations. \nAvoid using any other equivalent titles (such as 'Pertinent Laboratory and Imaging \nResults', 'Brief Hospital Course', 'Discharge Diagnoses', 'Important Examination \nResults', 'Impression', 'Summary', 'Examination Findings', 'Discharge Diagnosis', \n'Imaging Findings', 'Diagnostic Opinion', 'Conclusion', or any Chinese titles). Ensure \nthat the 'FINDINGS' and 'IMPRESSION' sections are independent and complete; for \nexample, imaging entries should separately include detailed findings and medical \nopinion summaries, and all scattered impressions should be consolidated into the \n'IMPRESSION' section.\\\",\",\n\"\\\"2. Remove Placeholders and Complete Content: Thoroughly remove all \nplaceholders, empty items, unfinished content, illustrative notes, editorial comments, \nand content unrelated to the main report (such as '___', '[date]', '[patient name]', \n'[fill in]', '[to be added]', 'each ___ tablet', '?', '*', Markdown marks, Chinese \ncomments, 'rewrite notes', 'improvement summary'). In the final report, all \nplaceholders must be replaced with specific, complete, and authentic information. ‚Ä¶\n\"]\"\n],\nFigure 10: Sample of feedback and weaved experience in stage 1.\nStage 1: Weaving for Experience\nStage 2: Weaving for Retrieval\n\"Description/Expression Errors\": [\n\"When describing physical examination or imaging findings, professional and \nprecise terminology must be used, avoiding colloquial or vague wording. For \nexample, when describing a murmur, use standard auscultation locations (such as \nthe right sternal border at the second intercostal space) and radiation direction; \nwhen describing negative imaging results, use standardized expressions such as \n'no evidence of...' or 'no acute... identified' instead of 'did not show a...'.\",\n\"For descriptions of the size of devices, anatomical structures, or lesions, units \nmust be consistent and conform to medical conventions. For example, vascular \nstent size should be clearly labeled as 'diameter (mm) x length (cm)', avoiding \nomission of units or use of inappropriate units (such as mistakenly writing \ndiameter in cm). Check whether values fall within reasonable physiological or \npathological ranges.\",\n\"In different parts of the report (such as medical history, clinical course, diagnosis), \ndescriptions of the same key diagnosis must be strictly consistent. For example, if \nthere is a contradiction in aortic dissection classification (Type A/Type B), it should \nbe cross-checked and corrected based on treatment history (such as surgical site) \nand contextual logic, to avoid major clinical information errors caused by clerical \nmistakes.\",‚Ä¶\n]\nError-Specific Tips \n\"revision_strategy\": [\n\"Ensure the standardization and consistency of medical terminology and \nabbreviations, prioritize the use of complete terms recommended by the latest \nguidelines, and avoid non-standard abbreviations and outdated expressions.\",\n\"Improve the clarity and precision of the text, eliminate vague, colloquial, or \nredundant expressions, and replace them with specific, objective professional \ndescriptions.\",\n\"Maintain structural integrity and logical rigor of the document, check and correct \ngrammatical errors, list formatting, tense consistency, and contradictions in key \ninformation.\",\n\"Strengthen the standardization of data and units, ensure all values are accompanied \nby clear units, maintain uniform formatting, and verify clinical plausibility to avoid \nambiguity.\"\n]\nFunctional Strategies\nFigure 11: Sample of weaved experience in stage 2.\n16\n"}, {"page": 17, "text": "A.7\nPrompts\nA.7.1\nPrompts in Weaving Experience\nPrompts for LLM experience abstraction.\nBased on the following evaluation feedback,\nextract the key formatting revision sugges-\ntions. The suggestions should be concise, spe-\ncific, detailed, include examples, and be dis-\ntinct from each other.\nFeedback data:\n{feedback text}\nPlease extract 3‚Äì5 of the most important for-\nmatting revision suggestions. Each suggestion\nshould: 1. Be concise and clear 2. Be specific\nand actionable 3. Include concrete examples 4.\nBe different from the others Return the output\nin JSON array format, where each element is\na string suggestion.\nPrompts for LLM experience combination.\nPlease merge the following two sets of re-\nvision suggestions, removing duplicates and\nkeeping only unique and important sugges-\ntions.\nFirst set of experience:\n{original experience set 1}\nSecond set of experience:\n{original experience set 2}\nThird / Fourth ... set of experience depend-\ning on the group size.\nWhen merging these suggestions, the re-\nquirements are: 1. Remove duplicate or simi-\nlar suggestions 2. Keep all unique and impor-\ntant suggestions\nIf two suggestions are similar, merge them\ninto a more complete suggestion. Keep the\nsuggestions concise, specific, and detailed\nPrompts for error-specific tips distillation.\nFrom the following experiences, extract sug-\ngestions related to {phase description} {error\ntype} errors.\nRelevant Layer 1 experience: {relevant ex-\nperience list}\nEach suggestion should: 1. Be specific and\nactionable 2. Be based on the evaluation sys-\ntem‚Äôs preferences 3. Be applicable to similar\nsituations Return the suggestions in JSON ar-\nray format.\nPrompts for functional strategy distillation.\nFrom the following detailed suggestions, ex-\ntract high-level strategies related to {phase\ndescription}, highlighting the evaluation sys-\ntem‚Äôs preferences and focus areas.\nDetailed suggestions (total {number of\ntips}): { all tips list }\nStrategy requirements: 1. Show the evalu-\nation system‚Äôs accumulated preferences and\nfocus areas 2. Provide concise but specific\nstrategic directions (not detailed steps) 3. Be\nspecific enough to guide decision-making 4.\nReflect the evaluation system‚Äôs requirements\nfor { phase description } Return the strategies\nin JSON array format.\nA.7.2\nPrompts in Simulated Evaluators\nPrompts for evaluating the formatting score.\nYou will evaluate the formatting quality of the\nfollowing medical report. Please assess it ac-\ncording to the following formatting standards:\nScoring criteria (1‚Äì5 points):\n1: Format completely does not meet stan-\ndards, with serious formatting issues\n2: Format basically does not meet standards,\nwith many formatting issues\n3: Format partially meets standards, with\nsome formatting issues\n4: Format basically meets standards, with\nonly a few formatting issues\n5: Format fully meets standards, formatting\nis excellent\nPlease evaluate from the following aspects:\nEvaluation aspects: {categories list} Struc-\ntural requirements: {structure requirements}\nTerminology requirements: {terminology re-\nquirements} Style requirements: {style re-\nquirements}\nPlease output in JSON format:\nPrompts for evaluating the correctness.\nYou will examine the correctness of the fol-\nlowing error.\nThe possible output options\nare:\n{correctness order}; where 1 means\n‚Äôcompletely incorrect‚Äô, 2 means ‚Äôincorrect‚Äô,\n3 means ‚Äôpartially correct‚Äô, 4 means ‚Äôcorrect‚Äô,\nand 5 means ‚Äôcompletely correct‚Äô.\nPlease output in JSON format: {\"label\": \"1\"\nor \"2\" or \"3\" or \"4\" or \"5\", \"reasoning\": \"your\n17\n"}, {"page": 18, "text": "evaluation reasoning\"}\nCorrectness refers to whether the detected\nerror matches the error type. If the detected\nerror does not match the error type, it is incor-\nrect; if it matches, it is correct.\nPlease score according to the following stan-\ndards:\n1: Completely incorrect, the detected error\ndoes not match the error type at all\n2: Incorrect, the detected error basically\ndoes not match the error type\n3: Partially correct, the detected error par-\ntially matches the error type\n4: Correct, the detected error basically\nmatches the error type\n5: Completely correct, the detected error\nfully matches the error type\nPlease provide a score and explain the rea-\nsoning. The following is the input medical\nreport error: error. The following is the de-\ntected error type: error type. Please directly\noutput in JSON format.\nPrompts for evaluating the meaningfulness.\nYou will examine the necessity of the follow-\ning error.\nThe possible output options are: {mean-\ningfulness order}; where 1 means ‚Äôvery un-\nnecessary‚Äô, 2 means ‚Äôunnecessary‚Äô, 3 means\n‚Äôaverage‚Äô, 4 means ‚Äônecessary‚Äô, and 5 means\n‚Äôvery necessary‚Äô.\nPlease output in JSON format: {\"label\": \"1\"\nor \"2\" or \"3\" or \"4\" or \"5\", \"reasoning\": \"your\nevaluation reasoning\"}\nNecessity refers to whether the error is re-\nlated to the medical diagnosis process. If the\nerror affects diagnosis, it is necessary; if it\ndoes not affect subsequent diagnosis by the\ndoctor, it is unnecessary. Please score accord-\ning to the following standards:\n1: Very unnecessary, completely does not\naffect diagnosis\n2: Unnecessary, has very little impact on\ndiagnosis\n3: Average, has some impact on diagnosis\n4: Necessary, has important impact on diag-\nnosis\n5: Very necessary, has decisive impact on\ndiagnosis\nPlease provide a score and explain the rea-\nsoning.\nThe following is the input medical report\nerror: error. Please directly output in JSON\nformat.\"\nPrompts for evaluating the readability.\nYou will evaluate the readability of the fol-\nlowing report. The possible output options\nare:\"1\",\"2\",\"3\",\"4\",\"5\"; where 1 means ‚Äôvery\nhard to read‚Äô, 2 means ‚Äôhard to read‚Äô, 3 means\n‚Äôaverage‚Äô, 4 means ‚Äôeasy to read‚Äô, and 5 means\n‚Äôvery easy to read‚Äô.\nPlease output in JSON format: \"label\": \"1\"\nor \"2\" or \"3\" or \"4\" or \"5\", \"reasoning\": \"your\nevaluation reasoning\"\nReadability refers to whether the report is\neasy to understand and read. Please score ac-\ncording to the following standards:\n1: Very hard to read, structure is chaotic,\ndifficult to understand\n2: Hard to read, many obstacles to under-\nstanding\n3: Average, basically readable but not clear\nenough\n4: Easy to read, clear structure, easy to un-\nderstand\n5: Very easy to read, excellent structure,\nvery clear and easy to understand\nPlease provide a score and explain the rea-\nsoning. The following is the input medical\nreport error: error. Please directly output in\nJSON format.\n18\n"}]}