{"doc_id": "arxiv:2601.13330", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13330.pdf", "meta": {"doc_id": "arxiv:2601.13330", "source": "arxiv", "arxiv_id": "2601.13330", "title": "RegCheck: A tool for automating comparisons between study registrations and papers", "authors": ["Jamie Cummins", "Beth Clarke", "Ian Hussey", "Malte Elson"], "published": "2026-01-19T19:10:45Z", "updated": "2026-01-19T19:10:45Z", "summary": "Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13330v1", "url_pdf": "https://arxiv.org/pdf/2601.13330.pdf", "meta_path": "data/raw/arxiv/meta/2601.13330.json", "sha256": "7ee2158fe7203cdaa4d88adc4983171ee6ff42fe9299b8aa10e56ffa8e8c9636", "status": "ok", "fetched_at": "2026-02-18T02:21:06.294271+00:00"}, "pages": [{"page": 1, "text": "RegCheck: A tool for automating comparisons\nbetween study registrations and papers\nJamie Cummins* 1,2, Beth Clarke1, Ian Hussey1, and Malte Elson1\n1Institute of Psychology, University of Bern\n2Bennett Institute of Applied Data Science, University of Oxford\n*Corresponding author: jamie.cummins@unibe.ch\nJanuary 21, 2026\nAbstract\nAcross the social and medical sciences, researchers recognize that spec-\nifying planned research activities (i.e., ’registration’) prior to the com-\nmencement of research has benefits for both the transparency and rigour\nof science. Despite this, evidence suggests that study registrations fre-\nquently go unexamined, minimizing their effectiveness.\nIn a way this\nis no surprise: manually checking registrations against papers is labour-\nand time-intensive, requiring careful reading across formats and expertise\nacross domains. The advent of generative language models unlocks new\npossibilities in facilitating this activity. We present RegCheck, an open-\nsource, modular LLM-assisted tool designed to help researchers, reviewers,\nand editors from across scientific disciplines compare study registrations\nwith their corresponding papers. Importantly, RegCheck keeps human ex-\npertise and judgement in the loop by (i) ensuring that users are the ones\nwho determine which features should be compared, and (ii) presenting the\nmost relevant text associated with each feature to the user, facilitating\n(rather than replacing) human discrepancy judgements. RegCheck also\ngenerates shareable reports with unique RegCheck IDs, enabling them to\nbe easily shared and verified by other users. RegCheck is designed to be\nadaptable across scientific domains, as well as registration and publication\nformats. In this paper we provide an overview of the motivation, work-\nflow, and design principles of RegCheck, and we discuss its potential as\nan extensible infrastructure for reproducible science with an example use\ncase.\nKeywords:\nStudy registration; large language models; embeddings; re-\nsearch trustworthiness assessment; open source software\n1\narXiv:2601.13330v1  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "1\nIntroduction\nThe importance of registering study designs, materials, hypotheses, outcomes,\nand processing/analysis plans for the transparency and rigour of research has\nseen increasing recognition in the medical and social sciences in the last 20 years.\nIn clinical trials, registration is a legal requirement according to the regulations\ngoverning many countries such as the United States (NIH Policy on the Dis-\nsemination of NIH-Funded Clinical Trial Information | Grants & Funding, n.d.)\nand Europe (Regulation (EU) No 536/2014 of the European Parliament and of\nthe Council of 16 April 2014 on clinical trials on medicinal products for hu-\nman use, and repealing Directive 2001/20/EC Text with EEA relevance, 2014).\nEven when not legally required, registration sees use across the sciences. In\ngeneral medical research, the creation of registrations (or publication of study\nprotocols) is growing in prevalence (Tan et al., 2019); the same trend can also\nbe seen in economics, with pre-analysis plans (Ofosu & Posner, 2023), and in\npsychology, with preregistration (and more recently, Registered Reports, Cham-\nbers and Tzavella, 2021; Hardwicke et al., 2024). Although different fields have\ndifferent labels, norms, and literatures discussing these approaches, for the sake\nof consistency here, we refer to all of these using the term \"registration\".\nIn principle, these registrations improve the transparency of decision making\nand plans associated with research programs. This transparent record allows re-\nsearchers to evaluate whether planned analytic choices were followed or not, thus\nfacilitating the trustworthiness of scientific research more broadly (Hardwicke\n& Wagenmakers, 2023). Crucially, registrations enable the reader to distinguish\nwhich analytic decisions were made before and after observing the data, which\ncan have implications for the veracity and severity of the reported statistical\ntests (Simmons, Nelson, & Simonsohn, 2011). This provides researchers with\ncritical information that they can use to evaluate the robustness of the results\nfor themselves: in other words, researchers can more accurately assess the risk\nof bias and calibrate their confidence in the results accordingly (Hardwicke &\nWagenmakers, 2023)\nIn practice, however, study registration is often more complex. Researchers\nmay need to deviate from their registration for a variety of reasons (see Lakens,\n2024, for a discussion on when and how researchers might consider deviating\nfrom their registration). Although some deviations could be unproblematic or\ntrivial, others can seriously impact the severity of the test(s) of the predic-\ntions (Lakens, 2024). This is far from a distant concern — several meta-science\nstudies across a variety of fields have found that deviations from registrations\nare common (Bakker et al., 2020; Claesen, Gomes, Tuerlinckx, & Vanpaemel,\n2021; Goldacre et al., 2019; Hahn et al., 2025; Heirene et al., 2024; Li et al.,\n2018; Ofosu & Posner, 2023; Poole, Linden, Sedgewick, Allchin, & Hobson,\n2025; TARG Meta-Research Group & Collaborators, 2023; Van den Akker et\nal., 2024, 2023). Even more concerning is that these deviations are often not re-\nported in the final paper (TARG Meta-Research Group & Collaborators, 2023;\n2\n"}, {"page": 3, "text": "Van den Akker et al., 2024, 2023)1.\nWe might expect such issues to be caught in the peer review process. Yet,\nclearly, this is often not the case (Van den Akker et al., 2024).\nSo how do\nthese deviations make it past peer review? The answer is probably quite sim-\nple. Checking registrations against the eventually-published paper is a time-\nintensive and laborious process. Given peer-reviewers are already overburdened\n(Adam, 2025), the fairly substantial additional task of checking studies against\ntheir registrations seems unlikely to be done.\nIndeed, one study of articles\npublished at PLOS suggests that very few editors and reviewers engage with\nregistrations during the peer review process (Syed, 2025, see also Spitzer and\nMueller, 2023b). In a survey of reviewers for medical journals, only 34.3% indi-\ncated they had examined the information on a trial registry with the past two\nyears (Mathieu, Chan, & Ravaud, 2013). This may also serve to explain why\ndeviations are not reported by authors themselves: at least in some cases, it is\nplausible that authors are unaware of such deviations, given that checking for\nthem is labourious.\nWhile study registration offers a promising solution in principle to the risk of\nbias that can result from researcher degrees of freedom, in practice, the effective-\nness of registration is seriously undermined by the work involved in conducting\nregistration-paper comparisons. If registrations are never examined or evalu-\nated, then we risk assuming that the risk of bias has been reduced when this\nmay not be the case at all.\nThe recent advent of generative large language models (LLMs) offers the\npossibility of automating portions of the work involved in registration-paper\ncomparisons, assisting reviewers in a task that they currently seem to avoid\n(note that we refer to ’registration-paper’ or ’registration-publication’ compar-\nisons throughout this manuscript, but of course such processes can be done with\ndrafts of manuscripts prior to publication or submission to a journal). Indeed,\ndoing this is already accessible to the average researcher: the simplest way to\ncompare registrations and publications is to upload them to a commercial LLM\nand prompt it to compare them. However, this ad hoc approach raises a host of\nissues. First, the dimensions of comparison may vary across calls if not explicitly\nspecified - on one call the LLM may compare materials based on sample size,\nstatistical models, and inference criteria, while on another call it may compare\nthe very same materials based on hypothesis tests and primary outcome defi-\nnitions. While users can specify these dimensions, they may still receive very\ndifferent outputs depending on their definitions of these dimensions. Addition-\nally, the specific content of prompts will necessarily be unstandardized between\nresearchers, which can introduce a substantial degree of variability in the qual-\nity and accuracy of output (Cummins, 2025; Sclar, Choi, Tsvetkov, & Suhr,\n2023). More generally, the act of verifying the judgements of consistency made\nby the model may eventually be as laborious as doing the manual comparison\nin the first place. Most critically, the importance of expert human judgement is\n1Another serious issue is that registrations are often underspecified and do not provide\nsufficient details to deviate from to begin with (Bakker et al., 2020; Hahn et al., 2025; Ofosu\n& Posner, 2023; Poole et al., 2025; Van den Akker et al., 2024).\n3\n"}, {"page": 4, "text": "displaced by the model’s (sometimes erroneous or lack of) judgement.\nWhat is needed instead is an approach which ensures consistency in prompt\ncontent and comparison logic across calls and users, while also ensuring that\nexpert knowledge is applied by (i) allowing researchers to specify which di-\nmensions should be examined, and (ii) facilitating researchers’ assessment of\ndiscrepancies, rather than entirely outsourcing this to the model.\nTo meet these needs, we have developed RegCheck: a tool which automati-\ncally extracts the most relevant text from registrations and papers as they relate\nto various user-specified dimensions, easily facilitating comparison between these\nsources. It provides a pipeline that parses registration and publication texts,\nlets users specify what dimensions they want to check, and produces structured,\nauditable reports which are shareable via unique links. By lowering the difficulty\nbarrier for checking registrations without removing expert oversight, RegCheck\naims to make consistency checks a routine part of scientific workflows.\n2\nThe RegCheck Workflow\nRegCheck’s backend is organised into six stages:\n1. Ingestion. The user specifies a registration (through upload, or provid-\ning a ClinicalTrials.gov identifier) and paper (through upload). Papers are\nparsed using specialized models of the user’s choice (currently with GRO-\nBID; GROBID, 2025, and DPT-2; Huang, 2025, as offered models) while\nregistrations are either parsed using standard libraries (PyMuPDF), XML\nencoding and extraction, or API interactions with trial registries, depen-\ndent on content.\n2. Extraction. References sections are stripped, and layout is normalised.\nIf the paper is indicated to consist of multiple studies, then the paper is\nfurther processed with irrelevant studies removed, and only the general\nintroduction, the relevant study, and general discussion are retained. In\ninstances where the specific study makes reference to previous studies\n(e.g., \"Our procedure was the same as the previous experiment\") then an\nLLM flow is implemented to attempt to identify and extract those relevant\ndetails from the other studies.\n3. Embedding.\nAll content from both documents is chunked based on\ntoken-based, sentence-aware method (200 tokens per chunk, full sentences\nretained, overlap of 30 tokens between chunks). Embeddings are computed\nfor these text chunks to support retrieval-augmented generation and text\nextraction using OpenAI’s text-embeddings-3-large model. The embed-\ndings form the basis of the extraction process for direct quotes from the\nmanuscript and registration.\n4. Definition. Users either select from pre-set dimensions for comparison or\ndefine their own by providing a label for the dimension and a definition.\n4\n"}, {"page": 5, "text": "Definitions are used for targeted retrieval (for text extraction) and better-\nspecified prompting (for LLM judgements).\n5. Analysis. For each dimension, RegCheck:\n• Retrieves the most relevant evidence excerpts from each source using\ndense embedding retrieval. It forms a single query from the dimen-\nsion label plus its definition (user-provided or a built-in default), em-\nbeds this query, and ranks the pre-embedded registration and paper\nchunks for relevance (based on cosine similarity). The top candidates\nare then re-ranked with an additional cosine-based scoring step and\ntruncated to the final top-k.\n• Sends the retrieved excerpts to a user-selected LLM, which produces\nconcise summaries of the dimension-relevant registration and paper\ncontent (referencing the excerpt IDs).\n• Produces a deviation judgement from the LLM using the same evi-\ndence, encoded as yes (deviation), no (no deviation), or missing (in-\nsufficient evidence), plus a brief explanation in deviation_information.\n6. Reporting. RegCheck renders a structured results page (and CSV ex-\nport) keyed by a task ID. For each dimension it displays the retrieved\nevidence excerpts from the registration and paper (with excerpt IDs and\nrelevance scores), the LLM summaries, and the deviation judgement plus\nsupporting explanation. Figure 1 provides an illustration of the current\nappearance of the RegCheck report.\n3\nDesign Principles and Philosophy\n• Pragmatism. RegCheck is a software tool which provides a standard-\nized framework for checking registrations against papers, with the goal\nof increasing the frequency of registration-paper comparisons (which is\nnot currently done regularly). All steps of its research and development\nare conducted with the aim of evaluating and improving its extraction\naccuracy, informativeness, and ease of use.\n• Human-in-the-loop. RegCheck facilitates human experts; it does not\nreplace them.\nThe system highlights candidate discrepancies, and also\nprovides indicative LLM-based judgements, but it is not aimed at out-\nsourcing human expertise.\n• Non-prescriptive. It is not the role of RegCheck to make determina-\ntions about which dimensions matter when comparing a registration to a\npaper: this can vary by user, use case, and field/discipline norms. There is\ncurrently no cross-discipline consensus, that we know of, on the selection\nof dimensions which do or do not matter for study registration. RegCheck\nprovides default dimensions based on frequently-examined features (e.g.,\n5\n"}, {"page": 6, "text": "Figure 1: A screenshot of an example RegCheck report. Each row represents\na comparison between the registration and paper along a specific, pre-specified\ndimension (noted in the ’Dimension’ column). Under the ’Preregistration’ and\n’Paper’ columns, users can toggle between displaying direct quotes from the re-\nspective document, or LLM-generated summaries of the content of these quotes.\nThe color-coding of the row indicates whether a deviation between the materials\nwas found (red), no deviation was found (blue), or that there was insufficient\ninformation in one or both sources to render a deviation judgement (yellow).\nThe ’Deviation Information’ column also provides commentary on the similari-\nties and differences between the two sets of materials.\n6\n"}, {"page": 7, "text": "sample size, primary outcomes), but also provides free-text fields for users\nto add their own for this reason. The current default dimensions are most\nrelevant to psychology, but future iterations of the software will offer rec-\nommended default dimensions which vary based on the discipline of the\nto-be-compared paper.\n• Discipline-agnostic and discipline-aware. Although it has origins in\npsychological research, RegCheck is an abstract workflow aimed at being\nuseful for researchers across a range of scientific disciplines. At the same\ntime, we recognize that RegCheck’s accuracy and usefulness may vary\nacross fields and disciplines; for this reason, we are investigating variation\nin its accuracy in a range of different areas.\n4\nPrivacy and Confidentiality\nRegCheck does not request, log, or store any text uploaded by users, with the\nexception of extracted quotes from the manuscript during the creation of the\nRegCheck report (which are stored to facilitate the creation and maintenance\nof the persistent and shareable RegCheck report pages). Additionally, no user\ninformation is logged by RegCheck.\nWe present users with optional, anonymous surveys upon running a RegCheck\nreport, which we use to improve user experience and provide us with informa-\ntion about the purposes for which the platform is used (e.g., for peer review,\nself-review, etc.). However, these data are not identifiably linked to the content\nuploaded by those users.\nRegCheck provides users with the option to decide which generative LLM\nis used in the creation of the RegCheck report (by default, ChatGPT-5 is se-\nlected). Some of these models (e.g., DeepSeek, Llama) are open-weights models\nwhich are hosted on third-party inference API providers (e.g., the Groq ser-\nvice). Others (e.g., ChatGPT) are closed-source models which are hosted by\nthose commercially-invested in that model (e.g., OpenAI serves the ChatGPT\nmodel). OpenAI states in their documentation that text served to its models via\nits API service is not retained nor used for training. However, we of course have\nno control over the enforcement of, and adherence to, these privacy policies,\nnor do we have the capacity to know whether such policies may change in the\nfuture. We therefore provide the open-weights models as alternative options,\nwhich are hosted by third-party providers who are not commercially invested\nin the development of the models they host. In the future, we have plans to\nmigrate our services to an internal university server, which will also allow us to\ndeploy university-hosted open-weights models in order to provide more definitive\nguarantees around privacy, confidentiality, and future reproducibility.\nUsage of RegCheck should be constrained by the same standards of con-\nfidentiality and privacy around academic materials that apply in any other\nprofessional context. In other words: just as one should not share confiden-\ntial materials with unauthorised colleagues, one should not share upload such\n7\n"}, {"page": 8, "text": "materials to RegCheck. More generally, RegCheck should categorically not be\nused for content that users do not have permission to share (e.g., materials\nwhich users have acquired during the process of peer review which should not\nbe shared beyond reviewers themselves).\n5\nIs RegCheck Available To Use?\nYes, RegCheck is available to use. The cost of running the RegCheck processes\nvaries by the size of registrations and papers and the selected model, but costs\non average about 40 cents per report using ChatGPT-5 with medium reason-\ning effort. RegCheck can be used either through our point-and-click web app\n(described below) or run locally (at one’s own cost) using the open-source code,\navailable on Github. When using the public app, the cost of usage is currently\ncovered by our research group - in other words, using RegCheck is entirely free\nfor users.\nRegCheck’s point-and-click production application is currently available at\nhttps://regcheck.app.\nGiven that systematic, formal evaluation is currently\nunder way, we currently do not provide any guarantees about the fidelity of\nRegCheck’s output. Indeed, we strongly advise that users manually verify all\noutput for accuracy. With this said, our impressions of RegCheck, based on\nextensive-but-unsystematic testing, are that it is generally very accurate in its\nextractions and is quite adept at catching genuine deviations between registra-\ntions and papers where these deviations exist.\nRegCheck frequently catches\ndeviations that members of our research team missed, including subtle differ-\nences in registered vs. reported numeric inclusion criteria thresholds, changes\nin planned analyses, and instances of outcome-switching in clinical trials.\n6\nWhen and How Should I Use RegCheck?\nThe most obvious use case for RegCheck is for editors and reviewers who are\nassessing a manuscript where one or more studies were registered (where they\nhave permission to share and upload those materials; see Privacy and Confiden-\ntiality). Similarly, research synthesists might use it when assessing the trustwor-\nthiness of studies included in a systematic review or meta-analysis. Of course,\nRegCheck is also useful for authors who want to check their own manuscripts\nfor any deviations they should report (e.g., RegCheck could help to spot regis-\ntration deviations that might be missing in a registration deviation table, like\nthe one suggested by Willroth & Atherton, 2024). Regardless of whether the\nuser is using RegCheck to assess their own paper or someone else’s, the process\nis the same.\nRegCheck is designed to be intuitive and easy to use. First, users navigate\nto regcheck.app (currently in beta). The user can then open the \"Tools\" menu\nof the navigation bar and navigate to \"Registration-Paper Comparisons\". The\nuser may then choose to either use our recommended default settings, or decide\n8\n"}, {"page": 9, "text": "for themselves which settings they wish to use. If users opt to choose settings\nfor themselves, they are provided with several parameters to decide on:\n1. Parser: Users may decide which engine is used for parsing the uploaded\npaper.\nOptions currently provided are for GROBID (GROBID, 2025),\nwhich is best-suited for papers already typeset to journal standards, and\nDPT-2 (Huang, 2025), which is best-suited for non-typeset manuscripts\n(e.g., drafts of manuscripts). We recommend GROBID as the default.\n2. Language model: Users may select the large language model which is\nused for rendering judgments with regard to deviations in the manuscript.\nCurrently we recommend ChatGPT-5 on medium reasoning effort. There\nis currently no option for users to select the embeddings model used for\nRAG: this is, by default, text-embeddings-3-large from OpenAI (OpenAI,\nn.d.).\n3. Comparison dimensions: Users may determine which dimensions the reg-\nistration and paper should be compared along. The default dimensions\nwe suggest are based on our understanding of those factors which many\nresearchers who conduct such comparisons are interested in (e.g., outcome-\nswitching, sample-size specification, hypothesis specification).\n4. Append outputs: Users may determine whether the evaluation of a specific\ndimension should include the content of previously-evaluated dimensions\nor not. One of the crucial advantages of RegCheck is that it allows for dif-\nferent comparison dimensions to be evaluated separately in separate calls\nto the LLM. This means that the quality of output for each dimension\ncan be evaluated on an individual basis, since the calls made are inde-\npendent2. It is also possible that one may wish for the software to be\ncontext-aware of the content of response(s) to previous dimensions when\nevaluating another dimension. For example, for clinical trials, knowing\nwhat output was previously given for primary outcome definitions might\nhelp the aoftware give more accurate output related to secondary out-\ncomes, as well as reduce the likelihood that it confuses a primary outcome\nfor a secondary outcome (i.e., since it is now provided with the context\nthat certain outcomes have already previously been defined as primary).\nOur recommended default is to allow for this contextual awareness.\nAfter providing this information, users can provide both the registration (either\nby uploading a Microsoft Word docx or a pdf, or by providing a ClinicalTri-\nals.gov identifier) and the paper (docx or pdf). PDF documents will usually\nwork best for this, as they can be parsed more easily. Users can then simply\nclick ’Compare’.\n2By contrast, if multiple dimensions were evaluated based on a single call to the model, this\nwould mean that individual dimension results were necessarily affected by the output of other\ndimensions, due to the nature of next-token prediction in language models; by extension, there\nwould be an inherent dependency among outputs for each dimension, which is suboptimal for\nevaluation.\n9\n"}, {"page": 10, "text": "RegCheck then generates a report. Depending on the model selected and\nnumber of dimensions specified, this usually takes anywhere from 1 to 10 min-\nutes. The RegCheck report will populate iteratively as each dimension is eval-\nuated and output is provided. A status message is provided to users specifying\nhow many dimensions have been evaluated thus far, and how many dimensions\nremain. The final, completed report extracts direct quotes from both the regis-\ntration and paper with regard to the dimension compared, as well as a judgment\nof whether the registration and paper match on this dimension (alternatively,\nRegCheck flags if the relevant information is missing/ not reported), and details\nrelated to this decision. This output serves an orienting function to users: where\nbefore, one would need to scan each line of a registration and paper to compare\ndimensions, RegCheck provides direct quotes from each document which can\nbe used as search terms to easily identify the relevant content in-context in the\nmanuscript (e.g., by using Ctrl+F in the manuscript PDF and searching for the\nextracted quote text).\nRegCheck’s provided judgement of deviations, and deviation information,\ncan additionally be useful for spotting deviations that may have been over-\nlooked. However, RegCheck is not intended to replace human judgment. Users\nmust apply their own discretion when evaluating registration-paper consistency,\nand they should be aware that RegCheck can make both false positive errors\n(cases where RegCheck flags that there is a deviation when there is not a devi-\nation; also keep in mind that very minor deviations may be flagged) and false\nnegative errors (cases where RegCheck flags that there is no deviation when\nthere is a deviation). We are currently in the process of assessing when, and\nhow frequently, RegCheck makes these errors, as well as validating RegCheck\nmore broadly.\n7\nHow Accurate Is RegCheck?\nAs mentioned, the anecdotal testing of RegCheck among our research team\nand close colleagues gives us the impression that RegCheck is adept at flagging\n(in)consistencies between registrations and papers. However, anecdotal testing\nis not a substitute for formal evaluation. Our research team is thus currently\nworking on several different projects aimed at quantifying and evaluating the\nperformance of RegCheck for comparisons of registrations and papers in psy-\nchology, medicine, and economics. Evaluating RegCheck comes with challenges.\nImportantly, there is little consensus on how one can assess such performance in\nhumans, let alone automated tools. This is largely due to two issues. First, there\nis no agreed-upon framework on how to evaluate registration-paper consistency\n- different researchers disagree about what should be specified in a registration,\nwhat dimensions along which a registration should be compared to a paper, and\nwhat ultimately counts as a deviation or not. Second, some aspects of compar-\nison are inherently ambiguous - whereas it can be relatively straightforward to\nget agreement on a prespecified sample size, other features of comparison (e.g.,\nprespecified hypotheses or sufficiency of prespecified outcome descriptions) can\n10\n"}, {"page": 11, "text": "elicit more variation in the evaluations of humans.\nPut simply: in most cases here, establishing a ground truth among human\ncomparators is incredibly difficult, and we lack systematic frameworks for mak-\ning such comparisons in the first place. In evaluating RegCheck, we acknowledge\nthis fact, and use it to inform our evaluation approach. Specifically, rather than\ntrying to evaluate RegCheck as a software aimed at approximating a known\nground-truth value, we instead treat and evaluate it as an accompanying re-\nviewer. In practice, this means one of our core evaluation metrics of interest is\nthe degree of agreement with other human comparators, and the establishment\nof relative non-inferiority to observed human-human agreement. In other words:\nfor us to consider RegCheck accurate, it should, in general, show agreement with\nhuman raters to at least the same extent as humans show agreement with one\nanother. More concretely, our goals in evaluating RegCheck are in terms of in-\nterrater agreement on (i) extracted text from registrations/papers (e.g., planned\nresearch questions, planned statistical tests), and (ii) judgements of consistency\nbetween the registration and paper. To do this for a given research area, we\nspecify and define a set of dimensions along which registrations and papers\nshould be compared, solicit multiple human comparators to provide these com-\nparisons, and examine agreement among these human evaluations and those of\nRegCheck. In cases of disagreement, we will also examine whether RegCheck is\nhallucinating or making mistakes, or capturing legitimate details which humans\ntend to miss.\nAlthough our primary investigations focus on the aspect of RegCheck-human\nconsistency, we additional plan to examine RegCheck’s accuracy through artificially-\ngenerating registration and paper texts using a generative language model which\ncontain specific injected inconsistencies. Such an approach is more amenable to\nconventional means of evaluation, given that we are in control of the ground-\ntruth consistency in these cases. This approach also provides us with opportuni-\nties to conduct evaluation at greater scales: whereas solicitling human compara-\ntors to review registration-paper consistency takes a substantial deal of time,\nevaluating RegCheck’s performance on artificially-generated examples can be\ndone much more quickly and efficiently.\nOf course, the drawback to this is\nthat the artificial texts may be relatively generic, and written in \"LLM-speak\"\nwhich will likely not be representative of the diversity of writing styles present\nin the true academic literature. Consequently, these diverging approaches (in-\nterrater agrement with human comparators, artificially-generated texts with\nknown (in)consistencies) provide us with multiple perspectives on the accuracy\nof RegCheck: both its conventional accuracy in an artificial context, as well as\nits accuracy in more realistic settings.\nOf course, in principle RegCheck could still serve a pragmatic function even\nif it exhibits lower interrater agreement than that observed for human-human\nagreement. After all, the baseline rate of conducting registration-paper compar-\nisons in typical human reviewing is low, and this is largely due to the fact that\ndoing such comparisons is time-consuming and laborious. Checking the accu-\nracy of an existing RegCheck report, by contrast, is much less time-consuming.\nIn this sense, RegCheck may serve an orienting function to authors and review-\n11\n"}, {"page": 12, "text": "ers. Even if its extractions are at times inaccurate, it may functionally increase\nthe likelihood of these checks being done in the first place, where currently the\nbaseline rate of this behavior is very low. However, this is obviously not optimal:\nour goal with RegCheck is to provide a refined software whose text extractions\nand consistency judgements reach the same standard of agreement with humans\nas those another human.\n8\nFuture Agenda\nRegCheck is best seen as infrastructure for reproducible science. By making\nregistration–paper comparison faster, more consistent, and more transparent, we\nhope to increase the likelihood that these checks become routine in author’s own\nworkflows as well as in editorial workflows, peer review, and post-publication\nauditing. Its modularity allows adaptation across disciplines and integration\ninto larger ecosystems for research integrity. The research arm of RegCheck’s\ndevelopment focuses on concretely addressing:\n1. The accuracy of RegCheck’s text extraction, and whether this accuracy\nvaries across domains, disciplines, and model hyperparameters;\n2. Whether providing RegCheck to authors can improve the quality of registration-\npaper comparisons upon submission of papers to journals;\n3. Whether RegCheck improves in the efficiency and frequency of registration-\npaper comparisons.\nWe welcome engagement and input from researchers, journals, and funders in-\nterested in using RegCheck in diverse research areas. We are currently working\nto develop extensions of RegCheck, including the capacity to (i) assess multiple\npapers at once for use in meta-science or meta-analysis research, and (ii) assess\nconsistencies and deviations between study analysis code and descriptions of\nplanned analyses.\nReferences\nAdam, D. (2025, August). The peer-review crisis: how to fix an overloaded\nsystem. Nature, 644(8075), 24–27. Retrieved 2025-11-24, from https://\nwww.nature.com/articles/d41586-025-02457-2\ndoi: 10.1038/d41586\n-025-02457-2\nBakker, M., Veldkamp, C. L. S., Van Assen, M. A. L. M., Crompvoets, E. A. V.,\nOng, H. H., Nosek, B. A., . . . Wicherts, J. M. (2020, December). Ensuring\nthe quality and specificity of preregistrations.\nPLOS Biology, 18(12),\ne3000937. Retrieved 2025-02-18, from https://dx.plos.org/10.1371/\njournal.pbio.3000937 doi: 10.1371/journal.pbio.3000937\nChambers, C. D., & Tzavella, L. (2021, November). The past, present and\nfuture of Registered Reports.\nNature Human Behaviour, 6(1), 29–42.\n12\n"}, {"page": 13, "text": "Retrieved 2025-02-13, from https://www.nature.com/articles/s41562\n-021-01193-7 doi: 10.1038/s41562-021-01193-7\nClaesen, A., Gomes, S., Tuerlinckx, F., & Vanpaemel, W. (2021, October). Com-\nparing dream to reality: an assessment of adherence of the first generation\nof preregistered studies. Royal Society Open Science, 8(10), 211037. Re-\ntrieved 2025-05-02, from https://royalsocietypublishing.org/doi/\n10.1098/rsos.211037 doi: 10.1098/rsos.211037\nCummins, J. (2025). The threat of analytic flexibility in using large language\nmodels to simulate human data: A call to attention. arXiv. Retrieved 2025-\n11-21, from https://arxiv.org/abs/2509.13397 (Version Number: 2)\ndoi: 10.48550/ARXIV.2509.13397\nGoldacre, B., Drysdale, H., Dale, A., Milosevic, I., Slade, E., Hartley, P.,\n. . . Mahtani, K. R.\n(2019, December).\nCOMPare: a prospective co-\nhort study correcting and monitoring 58 misreported trials in real time.\nTrials, 20(1), 118. Retrieved 2025-10-16, from https://trialsjournal\n.biomedcentral.com/articles/10.1186/s13063-019-3173-2\ndoi: 10\n.1186/s13063-019-3173-2\nGROBID. (2025, September). GROBID Documentation. Retrieved 2025-11-24,\nfrom https://grobid.readthedocs.io/en/latest/\nHahn, L., Glöckner, A., Gollwitzer, M., Hellmann, J., Lange, J., Schindler, S.,\n& Sassenberg, K. (2025, July). A Cross-Sectional Study of the Complete-\nness of Preregistrations by Psychological Authors From German-Speaking\nInstitutions.\nAdvances in Methods and Practices in Psychological Sci-\nence, 8(3), 25152459251357568.\nRetrieved 2025-08-27, from https://\njournals.sagepub.com/doi/10.1177/25152459251357568\ndoi:\n10\n.1177/25152459251357568\nHardwicke,\nT. E.,\nThibault,\nR. T.,\nClarke,\nB.,\nMoodie,\nN.,\nCrüwell,\nS., Schiavone, S. R., . . .\nVazire, S.\n(2024, October).\nPrevalence\nof Transparent Research Practices in Psychology:\nA Cross-Sectional\nStudy of Empirical Articles Published in 2022.\nAdvances in Meth-\nods and Practices in Psychological Science, 7(4), 25152459241283477.\nRetrieved 2025-01-21,\nfrom https://journals.sagepub.com/doi/10\n.1177/25152459241283477 doi: 10.1177/25152459241283477\nHardwicke, T. E., & Wagenmakers, E.-J. (2023, January). Reducing bias, in-\ncreasing transparency and calibrating confidence with preregistration. Na-\nture Human Behaviour, 7(1), 15–26. Retrieved 2025-01-24, from https://\nwww.nature.com/articles/s41562-022-01497-2\ndoi: 10.1038/s41562\n-022-01497-2\nHeirene, R., LaPlante, D., Louderback, E., Keen, B., Bakker, M., Serafi-\nmovska, A., & Gainsbury, S.\n(2024, July).\nPreregistration speci-\nficity and adherence:\nA review of preregistered gambling studies and\ncross-disciplinary comparison. Meta-Psychology, 8. Retrieved 2025-08-\n27, from https://open.lnu.se/index.php/metapsychology/article/\nview/2909 doi: 10.15626/MP.2021.2909\nHuang, B.\n(2025, September).\nDocument Pre-trained transformer-2.\nRe-\ntrieved 2025-11-24, from https://landing.ai/developers/document\n13\n"}, {"page": 14, "text": "-pre-trained-transformer-2\nLakens, D.\n(2024, May).\nWhen and How to Deviate From a Preregis-\ntration.\nCollabra:\nPsychology, 10(1), 117094.\nRetrieved 2025-10-10,\nfrom https://online.ucpress.edu/collabra/article/10/1/117094/\n200749/When-and-How-to-Deviate-From-a-Preregistration\ndoi: 10\n.1525/collabra.117094\nLi, G., Abbade, L. P. F., Nwosu, I., Jin, Y., Leenus, A., Maaz, M., . . . Tha-\nbane, L.\n(2018, December).\nA systematic review of comparisons be-\ntween protocols or registrations and full reports in primary biomedical re-\nsearch. BMC Medical Research Methodology, 18(1), 9. Retrieved 2025-10-\n17, from https://bmcmedresmethodol.biomedcentral.com/articles/\n10.1186/s12874-017-0465-7 doi: 10.1186/s12874-017-0465-7\nMathieu, S., Chan, A.-W., & Ravaud, P.\n(2013, April).\nUse of Trial Reg-\nister Information during the Peer Review Process.\nPLoS ONE, 8(4),\ne59910.\nRetrieved 2026-01-16, from https://dx.plos.org/10.1371/\njournal.pone.0059910 doi: 10.1371/journal.pone.0059910\nNIH\nPolicy\non\nthe\nDissemination\nof\nNIH-Funded\nClinical\nTrial\nIn-\nformation\n|\nGrants\n&\nFunding.\n(n.d.).\nRetrieved\n2025-11-\n21,\nfrom\nhttps://grants.nih.gov/policy-and-compliance/policy\n-topics/clinical-trials/reporting/nih-policy\nOfosu, G. K., & Posner, D. N.\n(2023, March).\nPre-Analysis Plans:\nAn\nEarly Stocktaking.\nPerspectives on Politics, 21(1), 174–190.\nRe-\ntrieved 2025-08-27, from https://www.cambridge.org/core/product/\nidentifier/S1537592721000931/type/journal_article doi: 10.1017/\nS1537592721000931\nOpenAI.\n(n.d.).\nOpenAI API Documentation.\nRetrieved 2025-11-24, from\nhttps://platform.openai.com\nPoole, D., Linden, A., Sedgewick, F., Allchin, O., & Hobson, H.\n(2025,\nJune). A systematic review of pre-registration in autism research jour-\nnals. Autism, 29(6), 1390–1402. Retrieved 2025-08-28, from https://\ndoi.org/10.1177/13623613241308312\n(Publisher: SAGE Publications\nLtd) doi: 10.1177/13623613241308312\nRegulation (EU) No 536/2014 of the European Parliament and of the Coun-\ncil of 16 April 2014 on clinical trials on medicinal products for human\nuse, and repealing Directive 2001/20/EC Text with EEA relevance. (2014,\nApril). Retrieved 2025-11-21, from http://data.europa.eu/eli/reg/\n2014/536/oj (Legislative Body: EP, CONSIL)\nSclar, M., Choi, Y., Tsvetkov, Y., & Suhr, A. (2023). Quantifying Language\nModels’ Sensitivity to Spurious Features in Prompt Design or: How I\nlearned to start worrying about prompt formatting. arXiv. Retrieved 2025-\n11-21, from https://arxiv.org/abs/2310.11324 (Version Number: 2)\ndoi: 10.48550/ARXIV.2310.11324\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychol-\nogy: Undisclosed flexibility in data collection and analysis allows present-\ning anything as significant. Psychological science, 22(11), 1359–1366. doi:\n10.1177/0956797611417632\n14\n"}, {"page": 15, "text": "Spitzer, L., & Mueller, S.\n(2023a, March).\nRegistered report:\nSurvey on\nattitudes and experiences regarding preregistration in psychological re-\nsearch.\nPLOS ONE, 18(3), e0281086.\nRetrieved 2025-08-27, from\nhttps://dx.plos.org/10.1371/journal.pone.0281086\ndoi: 10.1371/\njournal.pone.0281086\nSpitzer,\nL., & Mueller,\nS.\n(2023b,\nAugust).\nStage 1 Registered Re-\nport: Restriction of researcher degrees of freedom through the Psycho-\nlogical Research Preregistration-Quantitative (PRP-QUANT) Template.\nRetrieved 2025-08-28, from https://www.psycharchives.org/jspui/\nhandle/20.500.12034/8644 (Publisher: PsychArchives) doi: 10.23668/\nPSYCHARCHIVES.13151\nSyed, M. (2025, April). Some data indicating that editors and reviewers do\nnot check preregistrations during the review process.\nJournal of Trial\nand Error. Retrieved 2025-05-02, from https://journal.trialanderror\n.org/pub/check-preregistrations doi: 10.36850/e5ce-4cc5\nTan, A. C., Jiang, I., Askie, L., Hunter, K., Simes, R. J., & Seidler, A. L.\n(2019, September). Prevalence of trial registration varies by study char-\nacteristics and risk of bias. Journal of Clinical Epidemiology, 113, 64–\n74.\nRetrieved 2025-10-16, from https://linkinghub.elsevier.com/\nretrieve/pii/S0895435619300435 doi: 10.1016/j.jclinepi.2019.05.009\nTARG Meta-Research Group & Collaborators. (2023, October). Estimating the\nprevalence of discrepancies between study registrations and publications:\na systematic review and meta-analyses.\nBMJ Open, 13(10), e076264.\nRetrieved 2025-08-27, from https://bmjopen.bmj.com/lookup/doi/10\n.1136/bmjopen-2023-076264 doi: 10.1136/bmjopen-2023-076264\nVan den Akker, O. R., Bakker, M., Van Assen, M. A. L. M., Pennington,\nC. R., Verweij, L., Elsherif, M. M., . . . Wicherts, J. M. (2024, October).\nThe potential of preregistration in psychology: Assessing preregistration\nproducibility and preregistration-study consistency. Psychological Meth-\nods.\nRetrieved 2025-08-29, from https://doi.apa.org/doi/10.1037/\nmet0000687 doi: 10.1037/met0000687\nVan den Akker, O. R., Van Assen, M. A. L. M., Enting, M., De Jonge, M.,\nOng, H. H., Rüffer, F., . . . Bakker, M. (2023, July). Selective Hypothesis\nReporting in Psychology: Comparing Preregistrations and Correspond-\ning Publications.\nAdvances in Methods and Practices in Psychological\nScience, 6(3), 25152459231187988. Retrieved 2025-08-27, from http://\njournals.sagepub.com/doi/10.1177/25152459231187988\ndoi:\n10\n.1177/25152459231187988\nWillroth, E. C., & Atherton, O. E.\n(2024, January).\nBest Laid Plans: A\nGuide to Reporting Preregistration Deviations.\nAdvances in Methods\nand Practices in Psychological Science, 7(1), 25152459231213802.\nRe-\ntrieved 2025-06-02, from http://journals.sagepub.com/doi/10.1177/\n25152459231213802 doi: 10.1177/25152459231213802\n15\n"}]}