{"doc_id": "arxiv:2601.02123", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.02123.pdf", "meta": {"doc_id": "arxiv:2601.02123", "source": "arxiv", "arxiv_id": "2601.02123", "title": "DeCode: Decoupling Content and Delivery for Medical QA", "authors": ["Po-Jen Ko", "Chen-Han Tsai", "Yu-Shao Peng"], "published": "2026-01-05T13:54:38Z", "updated": "2026-01-20T09:31:31Z", "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.02123v2", "url_pdf": "https://arxiv.org/pdf/2601.02123.pdf", "meta_path": "data/raw/arxiv/meta/2601.02123.json", "sha256": "08d03e022b8c46266203e0e2044644cd0fe88b52cb17c944996e1e6b67e7a722", "status": "ok", "fetched_at": "2026-02-18T02:23:16.685225+00:00"}, "pages": [{"page": 1, "text": "DeCode: Decoupling Content and Delivery for Medical QA\nPo-Jen Ko*\nNational Taiwan University\nb11902138@ntu.edu.tw\nChen-Han Tsai\nHTC DeepQ\nmaxwell_tsai@htc.com\nYu-Shao Peng\nHTC DeepQ\nys_peng@htc.com\nAbstract\nLarge language models (LLMs) exhibit strong\nmedical knowledge and can generate factu-\nally accurate responses.\nHowever, existing\nmodels often fail to account for individual pa-\ntient contexts, producing answers that are clini-\ncally correct yet poorly aligned with patients’\nneeds. In this work, we introduce DeCode\n(Decoupling Content and Delivery), a training-\nfree, model-agnostic framework that adapts ex-\nisting LLMs to produce contextualized answers\nin clinical settings. We evaluate DeCode on\nOpenAI HealthBench, a comprehensive and\nchallenging benchmark designed to assess clin-\nical relevance and validity of LLM responses.\nDeCode improves the previous state-of-the-art\nfrom 28.4% to 49.8%, corresponding to a 75%\nrelative improvement.\nExperimental results\nsuggest the effectiveness of DeCode in improv-\ning clinical question answering of LLMs.\n1\nIntroduction\nLarge language models (LLMs) have recently\nachieved strong performance on a variety of medi-\ncal natural language processing tasks, most notably\nmedical question answering (QA), where models\nare evaluated on their ability to generate correct\nresponses to clinically relevant questions (Singhal\net al., 2025; Nori et al., 2023). This progress has\nbeen demonstrated across a growing collection of\nmedical QA benchmarks, spanning multiple-choice\nand generative settings, professional examination-\nstyle questions, and open-domain clinical knowl-\nedge assessments (Jin et al., 2021; Pal et al., 2022).\nCollectively, results on these evaluations suggest\nthat contemporary LLMs exhibit substantial medi-\ncal knowledge and reasoning capability under stan-\ndardized testing conditions (Saab et al., 2024; Ope-\nnAI, 2024).\nExisting medical QA benchmarks, however, are\npredominantly designed to measure answer cor-\n*Work done during internship at HTC DeepQ\nrectness or reasoning accuracy, often via exact-\nmatch, multiple-choice selection, or expert-graded\nfactual validity. While these metrics are well-suited\nfor assessing knowledge recall and clinical rea-\nsoning, they provide only a partial characteriza-\ntion of model behavior in patient-facing or clini-\ncal communication settings (Gong et al., 2025; Tu\net al., 2025). In particular, such evaluations do not\ncapture whether model responses are understand-\nable, appropriately calibrated to patient context, or\naligned with norms of safe and empathetic medical\ncommunication.\nThis limitation motivates the need for evaluation\nframeworks that extend beyond accuracy-based\nmetrics. OpenAI HealthBench was introduced to\naddress this gap by evaluating medical LLM out-\nputs along multiple qualitative dimensions, includ-\ning context seeking, emergency referrals, and re-\nsponding under uncertainty, in addition to factual\ncorrectness (Arora et al., 2025). Unlike prior medi-\ncal QA datasets, which typically assume a single\ncorrect answer independent of delivery style or\naudience, HealthBench explicitly models the in-\nteractional aspects of medical responses, enabling\na more fine-grained analysis of clinically relevant\nresponse quality.\nEmpirical results on HealthBench further show\nthat models with comparable accuracy on tradi-\ntional medical QA benchmarks can exhibit sub-\nstantial variation across other non-accuracy dimen-\nsions, revealing a misalignment between standard-\nized QA performance and patient-centered context\nawareness (Arora et al., 2025). Together, these find-\nings suggest that accuracy alone is insufficient as a\nproxy for real-world clinical readiness and under-\nscore the importance of multidimensional evalua-\ntion for medical LLMs.\nIn this work, we introduce the Decoupling Con-\ntent and Delivery (DeCode) framework, a modu-\nlar approach for generating patient-specific medical\nresponses from clinical conversations. DeCode de-\narXiv:2601.02123v2  [cs.CL]  20 Jan 2026\n"}, {"page": 2, "text": "composes an existing clinical interaction into mul-\ntiple complementary analytical perspectives, each\nimplemented via a specialized LLM module. The\noutputs of these modules are subsequently synthe-\nsized to produce a final response that accounts for\nboth medical correctness and patient context.\nImportantly, DeCode operates in a training-free\nparadigm, orchestrating the generation process\nthrough explicit clinical formulation and structured\ndiscourse constraints. Empirically, we demonstrate\nthat DeCode substantially improves performance\non HealthBench, increasing the prior state-of-the-\nart score from 28.4% to 49.8%. Furthermore, we\nshow that DeCode generalizes consistently across\nmultiple leading LLMs, suggesting that the frame-\nwork captures model-agnostic principles for per-\nsonalized medical response generation.\nThe remainder of this paper is organized as fol-\nlows. Related works are introduced in Section 2.\nThe proposed method is presented in Section 3.\nExperimental setup and results are provided in Sec-\ntion 4 and Section 5, respectively. Finally, Sec-\ntion 6 concludes the paper.\n2\nRelated Work\nEarly evaluations of large language models (LLMs)\nin medical question answering have primarily\nfocused on standardized multiple-choice bench-\nmarks, including MedQA (Jin et al., 2021), MedM-\nCQA (Pal et al., 2022), and PubMedQA (Jin et al.,\n2019). These benchmarks have catalyzed substan-\ntial research on assessing and improving medical\nknowledge in LLMs (Singhal et al., 2025; Nori\net al., 2023; Saab et al., 2024; Jeong et al., 2024;\nLi et al., 2024; Wu et al., 2025a). However, such\nevaluations remain inherently static and accuracy-\ncentric, limiting their ability to assess communica-\ntive competence, contextual sensitivity, and patient-\ncentered delivery beyond factual correctness (Gong\net al., 2025).\nHealthBench (Arora et al., 2025) introduces a\nmultidimensional evaluation framework for med-\nical QA based on open-ended, multi-turn clinical\nconversations. Unlike traditional multiple-choice\nbenchmarks, HealthBench employs physician-\nauthored rubrics to assess behavioral dimensions\nsuch as clinical accuracy, communication quality,\nand contextual awareness, enabling a more compre-\nhensive evaluation of medical QA systems beyond\nfactual correctness.\nMuSeR (Zhou et al., 2025) targets HealthBench\nby proposing a self-refinement framework in which\na student LLM is guided by high-quality responses\nfrom a reference teacher model. In its original\nformulation, MuSeR relies on data synthesis and\nsupervised training: the student generates an ini-\ntial response, performs structured self-assessment\nacross multiple dimensions, and produces a refined\nfinal answer. While this training-based pipeline\nis computationally intensive and primarily appli-\ncable to trainable, open-source LLMs, the core\nself-refinement procedure can also be applied at in-\nference time, enabling response refinement without\nmodel distillation or fine-tuning.\nIn parallel, multi-agent frameworks have been\nproposed to address complex medical QA by\ndecomposing reasoning across specialized roles.\nMedAgents (Tang et al., 2024) employs role-\nplaying specialists for debate-based hypothesis re-\nfinement, while MDAgents (Kim et al., 2024) dy-\nnamically configures expert teams based on query\ncomplexity. More recent approaches further ex-\ntend this paradigm: KAMAC (Wu et al., 2025b) in-\ntroduces on-demand expert recruitment to address\nknowledge gaps during generation, and AI Hos-\npital (Fan et al., 2025) evaluates agent-based sys-\ntems in interactive patient simulation environments.\nHowever, these methods primarily emphasize diag-\nnostic reasoning and accuracy on traditional bench-\nmarks, often overlooking how complex reasoning\noutcomes are translated into clear, user-aligned re-\nsponses.\nBuilding on these observations, we introduce\nDeCode, a modular framework that explicitly de-\ncouples medical content reasoning from response\ndelivery. Unlike training-based or agent-centric ap-\nproaches, DeCode requires no additional training\nand is model-agnostic, while emphasizing struc-\ntured generation that supports contextualized and\nuser-aligned medical responses. We present our\nimplementation in the following section.\n3\nMethod\nMedical question answering with LLMs can be\nmodeled as a form of conditional text generation\nP(R | H), where R denotes the response and\nH the conversation history. In practice, H con-\ntains rich patient-specific information—such as\nsymptoms, risk factors, and health indicators—\ndistributed across multiple dialogue turns. How-\never, LLMs are typically trained to model this dis-\ntribution directly, without mechanisms to explic-\n"}, {"page": 3, "text": "Profiler\nFormulator\nPain for 68 year old\nadvanced cancer\nAre there any\noptions nearby?\nNo dedicated\nhospital nearby...\nHistory\nBackground\nNeeds\nClinical Indicators\nStrategist\nSynthesizer\nResponse\nDeCode Framework\nPos. Directives\nNeg. Constraints\nFigure 1: The DeCode Framework Pipeline. Given the conversation history H, the system first employs the\nProfiler and Formulator to extract user context (B, N) and clinical indicators C. These components are then\nsynthesized by the Strategist to generate tailored directives S (consisting of positive strategies S+ and negative\nconstraints S−). Finally, the Synthesizer constructs the response based on C and S, ensuring both medical accuracy\nand user adaptability.\nitly aggregate these dispersed signals. As a result,\nspecific patient details are frequently overlooked\nduring response generation.\nTo address this limitation, we introduce DeCode,\na framework that structures the generation process\nthrough four intermediate textual representations:\nuser background B, user needs N, clinical indi-\ncators C, and discourse strategy S. As illustrated\nin Figure 1, these representations are orchestrated\nby four corresponding modules: Profiler Mprof,\nFormulator Mform, Strategist Mstrat, and Syn-\nthesizer Msyn. By disentangling content from de-\nlivery, DeCode enables independent optimization\nof medical accuracy and communicative quality.\nThe inference process is formalized as a sequential\nchain:\nR = Msyn(S, C, H)\n|\n{z\n}\nSynthesis\n◦Mstrat(B, N, C, H)\n|\n{z\n}\nStrategy\n◦{Mprof(H), Mform(H)}\n|\n{z\n}\nExtraction\nwhere M denotes the LLM modules tailored for\nspecific sub-tasks. In the following sections, we\ndetail the design of each module.\n3.1\nProfiler: User Context Disentanglement\nMedical advice varies significantly across indi-\nviduals. The same symptom may imply differ-\nent risks depending on the user’s background and\nlifestyle. To capture this nuance beyond surface-\nlevel queries, the Profiler Mprof extracts the user’s\nspecific context from the conversation history H.\nWe formalize this extraction as:\n(B, N) = Mprof(H).\nThe user background B encapsulates critical at-\ntributes such as age, occupation, and living con-\nditions that constrain actionable advice. Concur-\nrently, the user needs N identifies the user’s core\nintent by synthesizing the conversation history H.\nBy decoupling the user information B and N from\nthe history H, we allow improved clarity in identi-\nfying user-specific constraints during response for-\nmulation. The user background B and user needs\nN are then sent to the Strategist module.\n3.2\nFormulator: Clinical Distillation\nA critical challenge in medical dialogue is that di-\nagnostic cues are often dispersed throughout the\nconversation history H, making it difficult to verify\nif the response covers all relevant medical aspects.\nTo address this, the Formulator Mform functions\nas a clinical information distiller. It extracts and\naggregates a structured set of clinical indicators C\n(e.g., symptoms, possible causes, and potential red\nflags) from the user statements in H. We formalize\nthis process as\nC = Mform(H).\nCrucially, this module operates purely on a fac-\ntual level, decoupling the medical substance from\nthe delivery style. By explicitly manifesting C as\nan intermediate representation, the system provides\na rigorous checklist for the downstream modules.\nThis ensures that the final response is grounded\nin verified medical evidence and that high-stakes\nsafety indicators are duly addressed, regardless of\nthe chosen empathy level or conversation tone.\n3.3\nStrategist: Discourse Orchestration\nBeyond factual accuracy, effective medical dia-\nlogue requires determining the optimal delivery\nstrategy tailored to the user’s cognitive and emo-\ntional context. The Strategist Mstrat addresses\nthis gap by synthesizing the conversation history\n"}, {"page": 4, "text": "H, extracted user profile (B, N) and clinical indi-\ncators (C) into a coherent strategy. We formalize\nthis process as:\nS = {S+, S−} = Mstrat(B, N, C, H).\nThe resulting discourse strategy S comprises\ntwo complementary sets. Positive directives S+\nprescribe the prioritization of clinical content and\nestablish the appropriate level of technical de-\ntail, crucially instructing the model to actively\nseek clarification when information is insufficient.\nConversely, negative constraints S−serve as be-\nhavioral guardrails, preventing counterproductive\nstyles (e.g., overly academic tones) and filtering\nout content that may be overwhelming or poten-\ntially misleading for the specific user. By enforcing\nthese strategies, the module ensures that the final\nresponse is not only medically grounded but also\nempathetic and strictly aligned with the user’s pref-\nerences.\n3.4\nSynthesizer: Controlled Generation\nFinally, the Synthesizer Msyn generates the re-\nsponse R by integrating the clinical indicators C\nwith the discourse strategy S. We formalize this\nprocess as:\nR = Msyn(S, C, H).\nBy separating content formulation from delivery\nplanning, the Synthesizer operates as a constrained\ngenerator. It articulates the verified information\nin C while adhering to the directives defined in S.\nThis ensures that the generation process focuses\non realization rather than reasoning, producing out-\nputs that are clinically accurate and contextually\nappropriate.\nTaken together, the Profiler, Formulator, Strate-\ngist, and Synthesizer form a coherent generation\npipeline that transforms the conversation history\nH into a personalized and clinically grounded re-\nsponse R. Each module addresses a distinct stage\nof the reasoning–generation process, enabling ex-\nplicit control over user understanding, clinical con-\ntent, discourse planning, and surface realization.\nFor reproducibility and clarity, the prompts corre-\nsponding to each module are provided in the ap-\npendix.\n4\nExperiments\n4.1\nDataset and Evaluation\nWe evaluate on OpenAI HealthBench (Arora et al.,\n2025), which contains 5,000 simulated multi-turn\npatient–clinician conversations ending in a user\nquery. Each conversation is annotated by medical\nprofessionals and assigned to one of seven themes:\nemergency referrals, context seeking, global health,\nhealth data tasks, complex responses, hedging, and\ncommunication.\nHealthBench additionally pro-\nvides physician-authored rubrics per conversation,\ngrouped into five evaluation axes: accuracy, com-\npleteness, communication quality, context aware-\nness, and instruction following. For more details\nregarding the conversation themes and evaluation\naxes, please refer to the original paper (Arora et al.,\n2025).\nMetric.\nWe follow the official HealthBench pro-\ntocol (Arora et al., 2025): each conversation is\ngraded using its rubric and scored by GPT-4.1 (Ope-\nnAI, 2025a). Reported numbers are the mean nor-\nmalized score over the evaluated set.\nSplits.\nWe report results on the full HealthBench\ndataset for the primary evaluation. Owing to the\ncomputational cost, all subsequent experiments are\nconducted on the Hard subset of 1,000 challenging\nconversations.\n4.2\nImplementation Details\nBase LLMs.\nWe use OpenAI o3 (OpenAI,\n2025c) as the primary base model and addi-\ntionally evaluate GPT-5.2 (OpenAI, 2025b),\nClaude-Sonnet\n4.5\n(Anthropic,\n2025),\nand\nDeepSeek R1 (DeepSeek-AI et al., 2025) to assess\ngeneralization across model families.\nComparison Methods.\nWe compare against:\n(i) Zero-shot, which directly prompts the base\nLLM using the conversation history; (ii) MDA-\ngents (Kim et al., 2024), which recruits specialized\nexperts based on query complexity and aggregates\ntheir responses; (iii) KAMAC (Wu et al., 2025b),\nwhich dynamically recruits experts during genera-\ntion; and (iv) MuSeR (Zhou et al., 2025), which\napplies a self-refinement generation strategy. For\nKAMAC, the model selects the initial number of\nexperts and the discussion is fixed to two rounds.\nFor MuSeR, we use the authors’ self-refinement\nprompts at inference time, without model distil-\nlation or fine-tuning. A detailed cost and latency\nbreakdown for all frameworks is included in the\nappendix.\n"}, {"page": 5, "text": "Table 1: Comparison between zero-shot prompting and DeCode. Both methods use OpenAI o3 as the base\nLLM and are evaluated on the full HealthBench dataset and its hard subset. DeCode consistently outperforms\nthe zero-shot baseline across most conversation themes and evaluation axes. Performance on the hard subset is\nsubstantially lower than on the full set, highlighting the increased difficulty of this evaluation setting. Deltas in\nDeCode columns are computed relative to the zero-shot baseline within the same split; deltas greater than 2 points\nare highlighted.\nFull Set\nHard Subset\nMetric (↑, %)\nZero-shot\nDeCode (Ours)\nZero-shot\nDeCode (Ours)\nOverall Score\n57.8\n67.8 (+10.0)\n28.4\n49.8 (+21.4)\nThemes\nEmergency Referrals\n69.2\n80.3 (+11.1)\n27.0\n59.1 (+32.1)\nContext Seeking\n51.2\n67.0 (+15.8)\n30.0\n58.3 (+28.3)\nGlobal Health\n52.7\n65.2 (+12.5)\n31.8\n49.8 (+18.0)\nHealth Data Tasks\n44.3\n56.7 (+12.4)\n17.0\n35.6 (+18.6)\nCommunications\n67.9\n74.4 (+6.5)\n29.2\n43.8 (+14.6)\nHedging\n59.6\n69.5 (+9.9)\n30.9\n54.6 (+23.7)\nComplex Responses\n55.1\n52.6 (-2.5)\n24.4\n41.3 (+16.9)\nAxes\nAccuracy\n66.4\n72.5 (+6.1)\n45.6\n54.3 (+8.7)\nCompleteness\n59.5\n74.0 (+14.5)\n30.7\n58.8 (+28.1)\nCommunication Quality\n68.1\n61.9 (-6.2)\n55.5\n54.2 (-1.3)\nContext Awareness\n41.7\n53.4 (+11.7)\n4.0\n40.5 (+36.5)\nInstruction Following\n61.2\n59.4 (-1.8)\n45.8\n46.5 (+0.7)\n5\nResults and Analysis\n5.1\nMain Results\nIn this experiment, we compare DeCode with a\nzero-shot baseline built on the same underlying\nLLM, OpenAI o3. Both methods are evaluated\non the full HealthBench dataset as well as its hard\nsubset, with results summarized in Table 1.\nA clear performance gap emerges between the\nfull dataset and the hard subset. On the full set,\nthe zero-shot baseline performs weakest on health\ndata tasks. Performance further degrades on the\nhard subset, where the baseline struggles across\nnearly all conversation themes; the highest score\nachieved is only 31.8% under the global health\ntheme, highlighting the increased difficulty of this\nsplit.\nIn contrast, DeCode improves response quality\nacross all conversation themes on the full set, with\nthe exception of the complex responses category.\nFurther analysis suggests that DeCode occasionally\ngenerates overly detailed responses for relatively\nsimple or straightforward queries. While this be-\nhavior can enrich informational content, it may\nnegatively affect perceived communication qual-\nity, contributing to the observed performance drop\nalong this evaluation axis.\nOn the hard subset, DeCode yields substantial\ngains in overall performance. Notably, the lowest-\nscoring health data theme improves to 35.6%,\nwhile all remaining themes exceed 40%. These\nresults underscore the effectiveness of DeCode in\nenhancing both the content and delivery of med-\nical question answering under more challenging\nevaluation conditions.\n5.2\nGeneralizability Across Backbone Models\nIn this experiment, we examine the generalizabil-\nity of DeCode across different base LLMs. A key\nadvantage of the proposed framework is its model-\nagnostic design, which allows it to be applied to\na wide range of base LLMs while consistently im-\nproving medical question answering performance.\nWe evaluate DeCode on the hard subset using sev-\neral leading LLMs from different providers, with\nresults reported in Table 2.\nBased on the zero-shot performance of the base\nLLMs, health data tasks emerge as a particularly\nchallenging category for GPT-5.2, OpenAI o3, and\nDeepSeek R1. In contrast, Claude-4.5 exhibits\n"}, {"page": 6, "text": "Table 2: DeCode performance across diverse base LLMs. Comparison between zero-shot (ZS) and DeCode-\nenhanced performance on the HealthBench hard subset across multiple base LLMs. Inline deltas in the DeCode\ncolumns are computed relative to the ZS baseline for the same model; deltas larger than 2 points are highlighted.\nGPT-5.2\nOpenAI o3\nClaude 4.5\nDeepSeek R1\nMetric (↑, %)\nZS\nDeCode\nZS\nDeCode\nZS\nDeCode\nZS\nDeCode\nOverall Score\n36.6\n56.0 (+19.4)\n28.4\n49.8 (+21.4)\n12.4\n40.0 (+27.6)\n14.8\n25.7 (+10.9)\nThemes\nEmergency Ref.\n53.6\n65.8 (+12.2)\n27.0\n59.1 (+32.1)\n18.5\n50.2 (+31.7)\n19.9\n33.0 (+13.1)\nContext Seeking\n40.7\n62.6 (+21.9)\n30.0\n58.3 (+28.3)\n12.1\n46.8 (+34.7)\n15.0\n32.0 (+17.0)\nGlobal Health\n31.9\n51.8 (+19.9)\n31.8\n49.8 (+18.0)\n7.3\n33.9 (+26.6)\n15.6\n22.3 (+6.7)\nHealth Data Tasks\n34.0\n52.1 (+18.1)\n17.0\n35.6 (+18.6)\n15.0\n35.4 (+20.4)\n3.1\n22.0 (+18.9)\nCommunications\n33.8\n55.7 (+21.9)\n29.2\n43.8 (+14.6)\n16.6\n39.8 (+23.2)\n16.1\n18.0 (+1.9)\nHedging\n37.8\n60.7 (+22.9)\n30.9\n54.6 (+23.7)\n13.1\n48.7 (+35.6)\n18.3\n34.1 (+15.8)\nComplex Resp.\n35.1\n44.0 (+8.9)\n24.4\n41.3 (+16.9)\n15.2\n27.5 (+12.3)\n14.8\n15.9 (+1.1)\nAxes\nAccuracy\n49.4\n62.6 (+13.2)\n45.6\n60.9 (+8.7)\n28.6\n49.5 (+20.9)\n30.5\n32.6 (+2.1)\nCompleteness\n24.9\n56.6 (+31.7)\n30.7\n58.8 (+28.1)\n3.7\n44.1 (+40.4)\n15.6\n27.8 (+12.2)\nComm. Quality\n68.0\n54.8 (-13.2)\n55.5\n54.2 (-1.3)\n67.3\n53.5 (-13.8)\n60.9\n58.4 (-2.5)\nCont. Awareness\n33.0\n50.7 (+17.7)\n4.0\n40.5 (+36.5)\n1.5\n30.9 (+29.4)\n0.0\n19.1 (+19.1)\nInst. Following\n56.8\n48.0 (-8.8)\n45.8\n46.5 (+0.7)\n45.7\n43.6 (-2.1)\n44.8\n42.5 (-2.3)\nits weakest performance on global health tasks.\nAcross evaluation axes, context awareness and com-\npleteness are especially challenging: OpenAI o3,\nClaude-4.5, and DeepSeek R1 all record single-\ndigit scores on these dimensions in certain cases,\nindicating systematic deficiencies in handling com-\nplex contextual and informational requirements.\nConsistent with the observations in Section 5.1,\nDeCode delivers substantial improvements over\nthe corresponding zero-shot baselines across all\ntested models.\nNotably, Claude-4.5, which at-\ntains an initial overall score of 12.4%, improves\nto 40.0% when integrated with DeCode. Simi-\nlarly, the strongest baseline model, GPT-5.2, im-\nproves from 36.6% to 56.0%. Importantly, in all\nexperiments the underlying base LLM remains un-\nchanged. By explicitly decoupling content from\ndelivery, DeCode systematically enhances the per-\nformance of diverse base LLMs across nearly all\nmedical QA scenarios. These results demonstrate\nthat the benefits of DeCode are robust and largely\nLLM-agnostic, extending across a wide range of\nmodel architectures and providers.\n5.3\nComparison with LLM Frameworks\nIn this experiment, we evaluate representative\nLLM-based medical QA frameworks on the Health-\nBench hard subset using OpenAI o3 as the base\nLLM. Specifically, we consider MDAgents (Kim\net al., 2024), KAMAC (Wu et al., 2025b), and\nMuSeR (Zhou et al., 2025) with results summa-\nrized in Table 3.\nRelative to the zero-shot baseline, MDAgents\ndemonstrates consistent improvements across all\nconversation themes and four of the five evalua-\ntion axes. Notably, it achieves the strongest per-\nformance on instruction following among all com-\npared methods. This behavior can be attributed to\nits complexity-driven orchestration strategy: MDA-\ngents first estimates the difficulty of a given medi-\ncal query and determines whether it can be handled\nby a single agent or requires a coordinated team\nof specialized experts. Once the team composition\nis selected, it remains fixed throughout the gen-\neration process. This upfront complexity assess-\nment enables stable role assignment and coherent\nmulti-agent collaboration, which appears particu-\nlarly effective for instruction-heavy medical QA\nscenarios.\nIn contrast, KAMAC does not yield consistent\ngains over the zero-shot baseline and, in several\ncases, exhibits notable degradations in context\nawareness, communication quality, and instruc-\ntion following. Unlike MDAgents, KAMAC fol-\nlows a knowledge-driven strategy that dynamically\nrecruits new specialists during the generation pro-\n"}, {"page": 7, "text": "Table 3: Comparison with leading LLM-based frameworks. We evaluate MDAgents (Kim et al., 2024),\nKAMAC (Wu et al., 2025b), and MuSeR (Zhou et al., 2025), together with standard zero-shot prompting and\nDeCode, all using OpenAI o3 on the HealthBench hard subset. Inline deltas are reported relative to the zero-shot\nbaseline, with improvements greater than 2 points highlighted.\nZero-Shot\nMDAgents\nKAMAC\nMuSeR\nDeCode\nMetric (↑, %)\n(Kim et al., 2024)\n(Wu et al., 2025b)\n(Zhou et al., 2025)\n(Ours)\nOverall Score\n28.4\n36.2 (+7.8)\n27.4 (-1.0)\n47.1 (+18.7)\n49.8 (+21.4)\nThemes\nEmergency Referrals\n27.0\n36.3 (+9.3)\n33.1 (+6.1)\n53.3 (+26.3)\n59.1 (+32.1)\nContext Seeking\n30.0\n34.4 (+4.4)\n27.4 (-2.6)\n60.1 (+30.1)\n58.3 (+28.3)\nGlobal Health\n31.8\n43.0 (+11.2)\n30.2 (-1.6)\n44.4 (+12.6)\n49.8 (+18.0)\nHealth Data Tasks\n17.0\n19.1 (+2.1)\n7.7 (-9.3)\n37.2 (+20.2)\n35.6 (+18.6)\nCommunications\n29.2\n38.0 (+8.8)\n32.9 (+3.7)\n40.4 (+11.2)\n43.8 (+14.6)\nHedging\n30.9\n39.2 (+8.3)\n29.9 (-1.0)\n51.2 (+20.3)\n54.6 (+23.7)\nComplex Responses\n24.4\n31.6 (+7.2)\n28.5 (+4.1)\n37.7 (+13.3)\n41.3 (+16.9)\nAxes\nAccuracy\n45.6\n52.9 (+7.3)\n43.8 (-1.8)\n53.5 (+7.9)\n54.3 (+8.7)\nCompleteness\n30.7\n45.8 (+15.1)\n36.1 (+5.4)\n49.1 (+18.4)\n58.8 (+28.1)\nCommunication Quality\n55.5\n49.5 (-6.0)\n46.7 (-8.8)\n54.1 (-1.4)\n54.2 (-1.3)\nContext Awareness\n4.0\n4.6 (+0.6)\n0.0 (-4.0)\n46.0 (+42.0)\n40.5 (+36.5)\nInstruction Following\n45.8\n53.5 (+7.7)\n36.2 (-9.6)\n46.2 (+0.4)\n46.5 (+0.7)\ncess when existing agents identify missing domain\nknowledge. While this adaptive recruitment mecha-\nnism is intended to enhance coverage, our analysis\nsuggests that introducing new experts mid-stream\ncan disrupt conversational coherence. Specifically,\nthe newly added agents often generate responses\nthat overlap with existing contributions or shift\nthe discussion focus, leading to redundancy and\ntask-level confusion. These effects are amplified in\nlonger, multi-round discussions, ultimately degrad-\ning response quality.\nAmong the compared LLM-based frameworks,\nMuSeR’s self-refinement approach yields the\nlargest performance gains, particularly on context\nseeking and health data tasks, and achieves the\nstrongest improvement in context awareness. This\nis consistent with MuSeR’s design, where struc-\ntured self-assessment guides targeted response re-\nfinement to recover missing contextual signals. Our\nresults show that such self-refinement remains ef-\nfective even when applied purely at inference time.\nHowever, MuSeR does not explicitly disentangle\nclinical content from discourse strategy, limiting its\nability to independently optimize medical accuracy\nand communicative quality—an aspect directly ad-\ndressed by DeCode.\nTaken together, these results indicate that multi-\nagent medicalQA performance depends critically\non how contextual information and communica-\ntive intent are structured during generation. While\nfixed-team orchestration and self-refinement im-\nprove context sensitivity, dynamically introducing\nnew experts can incur coordination overhead that\ndegrades coherence. Explicitly separating clinical\ncontent from discourse strategy provides a more\nstable alternative, enabling balanced optimization\nof medical accuracy and communication quality, as\ndemonstrated by DeCode.\n5.4\nAblation Study\nWe conduct an ablation study to assess the individ-\nual contribution of each component in the DeCode\nframework. Specifically, we independently remove\nthe Profiler, Formulator, and Strategist modules and\ncompare their performance against the full DeCode\nmodel on the HealthBench hard subset. The results\nare summarized in Table 4.\nImpact of the Profiler\nThe Profiler module is de-\nsigned to extract the user’s background and under-\nlying needs, enabling personalized response gen-\neration. Removing this component is therefore ex-\npected to reduce personalization in scenarios that\nrequire a deeper understanding of the user. As\nshown in Table 4, the absence of the Profiler leads\nto notable performance drops in communications\nand complex responses. These degradations align\nwith our expectations, as removing the Profiler lim-\nits the model’s ability to infer the appropriate level\n"}, {"page": 8, "text": "Table 4: Ablation study on the HealthBench hard subset. DeCode denotes the complete framework. Each\nablation independently removes a single component (Profiler, Formulator, or Strategist), while keeping the remaining\nmodules unchanged. Values report absolute scores, with inline deltas indicating changes relative to DeCode. Deltas\nlarger than 2 points are highlighted.\nMetric (↑, %)\nDeCode\nw/o Profiler\nw/o Formulator\nw/o Strategist\nOverall Score\n49.8\n49.3 (-0.5)\n39.7 (-10.1)\n49.4 (-0.4)\nThemes\nEmergency Referrals\n59.1\n61.3 (+2.2)\n52.9 (-6.2)\n57.9 (-1.2)\nContext Seeking\n58.3\n57.6 (-0.7)\n44.2 (-14.1)\n59.4 (+1.1)\nGlobal Health\n49.8\n50.7 (+0.9)\n39.6 (-10.2)\n51.5 (+1.7)\nHealth Data Tasks\n35.6\n34.2 (-1.4)\n32.1 (-3.5)\n40.0 (+4.4)\nCommunications\n43.8\n41.4 (-2.4)\n33.6 (-10.2)\n43.0 (-0.8)\nHedging\n54.6\n55.5 (+0.9)\n43.9 (-10.7)\n54.1 (-0.5)\nComplex Responses\n41.3\n35.4 (-5.9)\n30.8 (-10.5)\n30.3 (-11.0)\nAxes\nAccuracy\n54.3\n54.1 (-0.2)\n47.5 (-6.8)\n52.6 (-1.7)\nCompleteness\n58.8\n58.0 (-0.8)\n43.9 (-14.9)\n59.7 (+0.9)\nCommunication Quality\n54.2\n54.4 (+0.2)\n59.0 (+4.8)\n47.7 (-6.5)\nContext Awareness\n40.5\n39.6 (-0.9)\n29.6 (-10.9)\n44.2 (+3.7)\nInstruction Following\n46.5\n47.9 (+1.4)\n49.2 (+2.7)\n44.2 (-2.3)\nof detail and tailor responses to user-specific con-\ntexts.\nImpact of the Formulator\nThe Formulator mod-\nule is responsible for identifying and structuring\nsalient clinical indicators from the conversation his-\ntory. Without this module, the model must rely\non unstructured context, which can hinder coher-\nent reasoning over clinical details. Consistent with\nthis intuition, removing the Formulator results in\nsubstantial declines in completeness and context\nawareness, along with a modest reduction in accu-\nracy. These findings highlight the importance of\nthe Formulator in organizing clinical information\nand ensuring that relevant conditions are explicitly\naddressed during medical question answering.\nImpact of the Strategist\nThe Strategist module\ngoverns response delivery by shaping tone, fram-\ning, and discourse strategy. Its removal primarily\naffects how information is communicated rather\nthan what information is presented. As observed\nin our results, ablating the Strategist leads to a\npronounced drop in communication quality and a\nsmaller but consistent decline in instruction follow-\ning. Both axes reflect how effectively responses\nengage with and adapt to user expectations. These\nresults underscore the role of the Strategist in en-\nsuring that medically relevant content is conveyed\nin an appropriate, user-receptive manner.\n6\nConclusion\nIn this work, we introduce Decoupling Content\nand Delivery (DeCode), a modular framework for\ncontextualized medical question answering. De-\nCode adapts a base LLM into four specialized\ncomponents—Profiler, Formulator, Strategist,\nand Synthesizer—that jointly structure response\ngeneration by explicitly separating medical content\nreasoning from discourse and delivery. This design\nenables the model to produce medically accurate\nresponses while remaining sensitive to user context\nand communication needs.\nExperiments on the OpenAI HealthBench bench-\nmark demonstrate that DeCode consistently outper-\nforms a zero-shot baseline and remains competitive\nwith leading multi-agent frameworks across both\nfull and hard evaluation settings. Moreover, evalua-\ntions across multiple base LLMs show that DeCode\ngeneralizes well across different model families\nand architectures, highlighting its model-agnostic\nnature.\nFuture work may explore mechanisms for\ncaching and updating patient-specific information\nacross multi-round interactions. Additionally, ex-\ntending the DeCode paradigm beyond medical QA\nto other user-centered domains represents a promis-\ning direction. Taken together, these results suggest\nthat DeCode provides a principled foundation for\nadvancing contextualized medical QA.\n"}, {"page": 9, "text": "7\nLimitations\nOur evaluation is conducted on simulated patient–\nclinician conversations, which may not fully reflect\nthe complexity, uncertainty, and risk profiles of\nreal-world clinical settings. Although DeCode im-\nproves response quality without additional training,\noutputs generated by large language models may\nstill contain errors or omissions and should not\nbe used as a substitute for professional medical\njudgment. Validation and safeguards are necessary\nbefore deploying systems in clinical practice.\nReferences\nAnthropic. 2025. Claude 4.5 sonnet. https://www.\nanthropic.com/news/claude-sonnet-4-5.\nRahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Pre-\nston Bowman, Joaquin Quiñonero-Candela, Foivos\nTsimpourlas, Michael Sharman, Meghan Shah, An-\ndrea Vallone, Alex Beutel, Johannes Heidecke, and\nKaran Singhal. 2025. HealthBench: Evaluating large\nlanguage models towards improved human health.\nPreprint, arXiv:2505.08775.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning. Preprint,\narXiv:2501.12948.\nZhihao Fan, Lai Wei, Jialong Tang, Wei Chen, Wang\nSiyuan, Zhongyu Wei, and Fei Huang. 2025. AI\nhospital: Benchmarking large language models in a\nmulti-agent medical interaction simulator. In Pro-\nceedings of the 31st International Conference on\nComputational Linguistics, pages 10183–10213, Abu\nDhabi, UAE. Association for Computational Linguis-\ntics.\nEun Jeong Gong, Chang Seok Bang, Jae Jun Lee, and\nGwang Ho Baik. 2025. Knowledge-Practice Per-\nformance Gap in Clinical Large Language Models:\nSystematic review of 39 benchmarks. Journal of\nMedical Internet Research, 27:e84120.\nMinbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jae-\nwoo Kang. 2024.\nImproving Medical Reasoning\nthrough Retrieval and Self-Reflection with Retrieval-\nAugmented Large Language Models. Bioinformatics,\n40(Supplement 1):i119–i129.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What Disease\ndoes this Patient Have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567–2577.\nYubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu\nChan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee,\nMarzyeh Ghassemi, Cynthia Breazeal, and Hae Won\nPark. 2024. Mdagents: an adaptive collaboration of\nllms for medical decision-making. In Proceedings\nof the 38th International Conference on Neural In-\nformation Processing Systems, NIPS ’24, Red Hook,\nNY, USA. Curran Associates Inc.\nJunkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng\nZhang, Xinhui Kang, Siyu Wang, Peng Li, Ya-Qin\nZhang, Weizhi Ma, and Yang Liu. 2024. Agent Hos-\npital: A simulacrum of hospital with evolvable med-\nical agents. In Proceedings of the 32nd ACM Inter-\nnational Conference on Multimedia, pages 53–62.\nACM.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan,\nRichard Edgar, Nicolo Fusi, Nicholas King, Jonathan\nLarson, Yuanzhi Li, Weishung Liu, Renqian Luo,\nScott Mayer McKinney, Robert Osazuwa Ness, Hoi-\nfung Poon, Tao Qin, Naoto Usuyama, Chris White,\nand Eric Horvitz. 2023. Can generalist foundation\nmodels outcompete special-purpose tuning? case\nstudy in medicine. Preprint, arXiv:2311.16452.\nOpenAI. 2024. Openai o1 system card. Technical re-\nport, OpenAI.\nOpenAI. 2025a.\nGpt 4.1 system card.\nhttps://\nopenai.com/index/gpt-4-1/.\nOpenAI. 2025b.\nGpt-5.2 system card.\nhttps://\nopenai.com/index/introducing-gpt-5-2.\nOpenAI.\n2025c.\nOpenai\no3\nsystem\ncard.\nhttps://openai.com/index/\nintroducing-o3-and-o4-mini/.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan\nSankarasubbu. 2022. Medmcqa: A large-scale multi-\nsubject multi-choice dataset for medical domain ques-\ntion answering. In Proceedings of the Conference\non Health, Inference, and Learning, volume 174 of\nProceedings of Machine Learning Research, pages\n248–260. PMLR.\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno,\nDavid Stutz, Ellery Wulczyn, Fan Zhang, Tim\nStrother, Chunjong Park, Elahe Vedadi, Juanma Zam-\nbrano Chaves, Szu-Yeu Hu, Mike Schaekermann,\nAishwarya Kamath, Yong Cheng, David G. T. Bar-\nrett, Cathy Cheung, Basil Mustafa, Anil Palepu, and\n48 others. 2024. Capabilities of gemini models in\nmedicine. Preprint, arXiv:2404.18416.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\n"}, {"page": 10, "text": "Clark, Stephen R. Pfohl, Heather Cole-Lewis, Dar-\nlene Neal, Qazi Mamunur Rashid, Mike Schaeker-\nmann, Amy Wang, Dev Dash, Jonathan H. Chen,\nNigam H. Shah, Sami Lachgar, Philip Andrew Mans-\nfield, and 16 others. 2025. Toward expert-level medi-\ncal question answering with large language models.\nNature Medicine, 31(3):943–950.\nXiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming\nLi, Yilun Zhao, Xingyao Zhang, Arman Cohan, and\nMark Gerstein. 2024. MedAgents: Large language\nmodels as collaborators for zero-shot medical rea-\nsoning.\nIn Findings of the Association for Com-\nputational Linguistics: ACL 2024, pages 599–621,\nBangkok, Thailand. Association for Computational\nLinguistics.\nTao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab,\nJan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li,\nMohamed Amin, Yong Cheng, Elahe Vedadi, Nenad\nTomašev, Shekoofeh Azizi, Karan Singhal, Le Hou,\nAlbert Webson, Kavita Kulkarni, S. Sara Mahdavi,\nChristopher Semturs, and 7 others. 2025. Towards\nconversational diagnostic artificial intelligence. Na-\nture, 642(8067):442–450.\nSean Wu, Michael Koo, Fabien Scalzo, and Ira Kurtz.\n2025a. AutoMedPrompt: A new framework for opti-\nmizing LLM medical prompts using textual gradients.\nPreprint, arXiv:2502.15944.\nXiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Yanyuan\nQiao, Imran Razzak, and Yutong Xie. 2025b.\nA\nknowledge-driven adaptive collaboration of LLMs\nfor enhancing medical decision-making. In Proceed-\nings of the 2025 Conference on Empirical Methods in\nNatural Language Processing, pages 33483–33500,\nSuzhou, China. Association for Computational Lin-\nguistics.\nYuxuan Zhou, Yubin Wang, Bin Wang, Chen Ning,\nXien Liu, Ji Wu, and Jianye Hao. 2025. Enhanc-\ning the medical context-awareness ability of llms\nvia multifaceted self-refinement learning. Preprint,\narXiv:2511.10067.\n"}, {"page": 11, "text": "A\nCost & Latency Analysis\nIn this section, we analyze the cost and latency of\nthe DeCode framework. In addition, we include the\ncost and latency of MDAgents (Kim et al., 2024),\nKAMAC (Wu et al., 2025b), and MuSeR (Zhou\net al., 2025). To ensure fair comparison, we assume\nan output token rate of 70 tokens/second to avoid\nlatency fluctuations due to API re-routing. The\ndetails of the DeCode framework is listed in Table\n5. Comparisons between the various LLM-based\nmedical QA frameworks are provided in Table 6.\nB\nPrompt Templates\nThe prompts for each module of DeCode are pro-\nvided below. The Profiler module uses two inde-\npendent prompts to extract the user background B\nand user needs N. The prompt to extract the user\nbackground B and user needs N is provided in\nFigure 2 and Figure 3. The Formulator prompt to\nextract the clinical indicators C is listed in Figure 4.\nThe Strategist prompt is given in Figure 5. Finally,\nthe Synthesizer prompt is presented in Figure 6.\n"}, {"page": 12, "text": "Table 5: Token usage and latency breakdown of the DeCode framework. We report average input tokens, output\ntokens, and estimated latency for each DeCode module. Latency is computed assuming a standardized throughput\nof 70 tokens/s. Since the Profiler and Formulator modules execute in parallel, their combined latency is attributed to\nthe Formulator module.\nComponent\nAvg. Input Tok.\nAvg. Output Tok.\nAvg. Latency (s)\nBackground (Profiler)\n426.71\n229.51\n3.29\nUser Needs (Profiler)\n360.71\n213.31\n3.04\nClinical Indicators (Formulator)\n515.71\n876.54\n12.53\nDiscourse Strategy (Strategist)\n1,420.37\n848.34\n12.11\nResponse (Synthesizer)\n1,502.16\n1,457.63\n20.83\nDeCode Total (parallelized)\n4,225.66\n3,625.33\n45.47\nTable 6: Comparison of computational cost and latency across methods. We report average input tokens, output\ntokens, estimated cost, and latency per sample. Latency is computed assuming a standardized generation throughput\nof 70 tokens/s. Bold denotes the best-performing method in each column (lowest cost or lowest latency).\nMethod\nAvg. Input Tok.\nAvg. Output Tok.\nAvg. Cost ($)\nAvg. Latency (s)\nMDAgents\n44,444\n4,909\n0.128\n70.13\nKAMAC\n574,438.50\n29,610.58\n1.386\n423.01\nMuSeR\n6,205.93\n2437.13\n0.040\n48.96\nDeCode (Ours)\n4,225.66\n3,625.33\n0.037\n45.47\nUser Background (B)\nYou are a medical intake specialist. Analyze the following conversation and extract the user’s\nbackground information.\nCONVERSATION:\n{conversation_history}\nExtract and infer the following information about the user (if available in the conversation):\n- Age or age group\n- Career/Occupation\n- Economic condition (inferred from context)\n- Living place/location\n- Living situation (alone, with family, etc.)\n- Any other relevant personal context\nIMPORTANT: Only include information that can be reasonably inferred from the conversation. Do NOT\nmake up information.\nRespond in this EXACT format:\nAGE: [age or age group, or \"Not specified\"]\nCAREER: [occupation, or \"Not specified\"]\nECONOMIC_CONDITION: [economic status inferred from context, or \"Not specified\"]\nLIVING_PLACE: [location/region, or \"Not specified\"]\nLIVING_SITUATION: [living arrangement, or \"Not specified\"]\nOTHER_CONTEXT: [any other relevant information, or \"None\"]\nBe concise and factual. If information is not available, write \"Not specified\" or \"None\".\nFigure 2: Prompt template for the User Background extraction.\n"}, {"page": 13, "text": "User Need (N)\nYou are analyzing a medical conversation to understand what the user needs.\nCONVERSATION:\n{conversation_history}\nIdentify what the user explicitly asks for or clearly needs. Be conservative - only include needs\nthat are:\n1. Explicitly stated by the user\n2. Clearly implied by the user’s questions or concerns\nDO NOT include:\n- Things the user might need but didn’t mention\n- General medical advice that wasn’t requested\n- Assumptions about what the user should want\nRespond in this EXACT format:\nNEEDS:\n1. [First explicit need]\n2. [Second explicit need]\n3. [Third explicit need]\n...\nIf the user doesn’t clearly state what they want, respond with:\nNEEDS:\nNone specified\nBe strict and conservative.\nFigure 3: Prompt template for the User Need identification.\n"}, {"page": 14, "text": "Clinical Indicators (C)\nYou are a clinical safety and completeness planner.\nYour ONLY job is to identify the medically important content that MUST be covered\nfor this case to be safe, accurate, and reasonably complete. You are NOT deciding tone or style.\nYou are optimizing for clinical accuracy and completeness, not brevity.\nCONVERSATION:\n{conversation_history}\nCreate a numbered list of key clinical content items that the final answer should try to cover,\nsuch as:\n- Important symptom details or history that should be addressed or clarified\n- Key possible causes or differentials (described in a cautious, non-diagnostic way)\n- Red-flag or emergency warning signs that should be mentioned if relevant\n- What the user can monitor or do at home (if appropriate)\n- When and how urgently they should seek in-person care\n- Any important limitations or uncertainties of online advice\nRules:\n- Focus on clinical content ONLY (WHAT to cover), not HOW to phrase it.\n- Err on the side of including any clinically important point that might affect safety.\n- Each item should be 1–2 sentences max.\n- Avoid repeating the same content in multiple items.\n- Do not invent new symptoms; only build on what is in the conversation.\n- It is acceptable to mention reasonable possible causes or scenarios even if the user did not use\nthose exact words, as long as they logically follow from the described symptoms.\nRespond in this EXACT format:\n1. [Clinical content item]\n2. [Clinical content item]\n3. [Clinical content item]\n...\nFigure 4: Prompt template for the Formulator module (Mform).\n"}, {"page": 15, "text": "Discourse Strategy (S)\nYou are a response-strategy planner for a medical assistant.\nYou receive:\n- The original conversation\n- A brief user background profile\n- A list of what the user clearly needs\n- A clinical content checklist (what should be covered for safety/completeness)\nYour job is to design HOW the assistant should answer for THIS user: what to prioritize, how deep\nto go, what style and structure to use, and what to avoid.\nCONVERSATION:\n{conversation_history}\nUSER BACKGROUND PROFILE:\n{user_profile}\nUSER NEEDS (what the user clearly wants):\n{needs_formatted}\nCLINICAL CONTENT CHECKLIST (what should be covered):\n{content_formatted}\nPay particular attention to:\n- Whether the user’s needs are clearly stated or vague/unspecified.\n- Whether there is sufficient information available for a safe medical assessment.\n- When needs or information are unclear, the plan should usually include a brief strategy for\nclarifying key gaps (e.g., 1–2 focused questions), while still guiding the assistant to give the\nbest possible provisional answer based only on what is already known.\nIMPORTANT:\n- The assistant MUST still give concrete, practical, medically useful information even when\ninformation is incomplete. Use conditional language (e.g., \"If X..., then Y...\") rather than\nrefusing to say anything.\n- Do NOT tell the assistant to avoid discussing possible causes or next steps entirely.\n- Clarification questions should be few (0–2 of the most important ones) and should not dominate\nthe answer.\nDesign a plan with TWO sections:\n1. WHAT TO DO/COVER (TO DO):\n- How the assistant should prioritize and present the content for THIS user.\n- What level of technical detail is appropriate for this user.\n- Whether to keep the answer short vs. more detailed.\n- Whether to explicitly ask clarification questions (0–2 key questions only), and if so, in what\nstyle and at what point (usually after giving main guidance).\n- Which content items from the checklist are highest priority to cover explicitly.\n- How to adapt the response to the user’s apparent role, location, and constraints.\n2. WHAT NOT TO DO/COVER (NOT TO DO):\n- Things that would likely confuse, overwhelm, or frustrate THIS user.\n- Styles to avoid (e.g., too technical, too casual, too vague, overly long).\n- Types of content to avoid (e.g., extremely long, low-yield lists of differential diagnoses;\nstrong reassurance when red flags are possible; rigid instructions when access is limited).\n- Any ways of answering that would clearly conflict with the user’s instructions.\nYou are NOT writing the final medical answer. You are only writing the plan.\nRespond in this EXACT format:\nTO DO:\n1. [Response strategy / priority tailored to user]\n2. [Another response strategy / priority]\n3. [Continue as needed]\nNOT TO DO:\n1. [Specific thing to avoid for this user]\n2. [Another thing to avoid]\n3. [Continue as needed]\nFigure 5: Prompt template for the Strategist module (Mstrat).\n"}, {"page": 16, "text": "Controlled Generation (R)\nYou are an experienced medical professional providing personalized advice.\nYour highest priorities are:\n1) Clinical accuracy and completeness of the information you provide.\n2) Clear, practical guidance for the user.\n3) Safe and appropriate communication.\nORIGINAL CONVERSATION:\n{conversation_history}\nPRESENTATION GUIDELINES (HOW TO ANSWER):\nTO DO:\n{to_do_formatted}\nNOT TO DO:\n{not_to_do_formatted}\nCONTENT CHECKLIST (WHAT YOU MUST COVER CLINICALLY):\n{content_formatted}\nYour task:\n1. Cover ALL items in the CONTENT CHECKLIST as clearly and concretely as possible. Aim for at\nleast one explicit sentence or short paragraph addressing each item.\n2. Follow the TO DO / NOT TO DO guidelines for how to present the information in a way that fits\nTHIS user’s background and needs.\n3. Be explicit about uncertainty and information gaps, but still give the BEST POSSIBLE DIRECT\nANSWER based only on the conversation.\n- Use conditional language (e.g., \"If X..., then Y...\") rather than refusing to answer.\n4. You may ask up to 1–2 of the most important clarification questions, but they should be placed\nnear the end and should NOT replace giving guidance.\n5. Keep the response user-centered and practical, and explain what the user can do next (e.g.,\nmonitor, self-care, when/where to seek in-person care).\n6. End with a brief reminder that this information does not replace an in-person medical\nevaluation and that the user should seek care if they are worried or if concerning symptoms arise.\nProvide your response:\nFigure 6: Prompt template for the Synthesizer module (Msyn).\n"}]}