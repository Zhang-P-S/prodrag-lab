{"doc_id": "arxiv:2511.17559", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.17559.pdf", "meta": {"doc_id": "arxiv:2511.17559", "source": "arxiv", "arxiv_id": "2511.17559", "title": "SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering", "authors": ["Gyubok Lee", "Woosog Chay", "Edward Choi"], "published": "2025-11-13T06:35:29Z", "updated": "2025-12-21T16:56:46Z", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.17559v2", "url_pdf": "https://arxiv.org/pdf/2511.17559.pdf", "meta_path": "data/raw/arxiv/meta/2511.17559.json", "sha256": "76a634bd35c906b1bfddfbd95ccb96fec3e8f5b0582bbf87f1da4abf502c0437", "status": "ok", "fetched_at": "2026-02-18T02:27:11.929073+00:00"}, "pages": [{"page": 1, "text": "Proceedings of Machine Learning Research 297, 2025\nMachine Learning for Health (ML4H) 2025\nSCARE: A Benchmark for SQL Correction and Question\nAnswerability Classification for Reliable EHR Question Answering\nGyubok Lee∗†\ngyubok.lee@kaist.ac.kr\nWoosog Chay∗†\nbenchay@kaist.ac.kr\nEdward Choi†\nedwardchoi@kaist.ac.kr\n† Korea Advanced Institute of Science & Technology, South Korea\n∗Co-first authors\nAbstract\nRecent advances in Large Language Models\n(LLMs) have enabled the development of text-\nto-SQL models that allow clinicians to query\nstructured data stored in Electronic Health\nRecords (EHRs) using natural language. How-\never, deploying these models for EHR question\nanswering (QA) systems in safety-critical clin-\nical environments remains challenging: incor-\nrect SQL queries—whether caused by model er-\nrors or problematic user inputs—can undermine\nclinical decision-making and jeopardize patient\ncare. While prior work has mainly focused on\nimproving SQL generation accuracy or filtering\nquestions before execution, there is a lack of a\nunified benchmark for evaluating independent\npost-hoc verification mechanisms (i.e., a com-\nponent that inspects and validates the gener-\nated SQL before execution), which is crucial for\nsafe deployment. To fill this gap, we introduce\nSCARE, a benchmark for evaluating methods\nthat function as a post-hoc safety layer in EHR\nQA systems. SCARE evaluates the joint task\nof (1) classifying question answerability (i.e.,\ndetermining whether a question is answerable,\nambiguous, or unanswerable) and (2) verify-\ning or correcting candidate SQL queries. The\nbenchmark comprises 4,200 triples of questions,\ncandidate SQL queries, and expected model\noutputs, grounded in the MIMIC-III, MIMIC-\nIV, and eICU databases. It covers a diverse set\nof questions and corresponding candidate SQL\nqueries generated by seven different text-to-\nSQL models, ensuring a realistic and challeng-\ning evaluation. Using SCARE, we benchmark\na range of approaches—from two-stage methods\nto agentic frameworks. Our experiments reveal\na critical trade-off between question classifica-\n∗These authors contributed equally\ntion and SQL error correction, highlighting key\nchallenges and outlining directions for future re-\nsearch.\nKeywords: Text-to-SQL, EHR QA, Database\nQA, Reliable QA, Benchmarks and Datasets\nData and Code Availability\nData and code\nare publicly available on our GitHub repository at\nhttps://github.com/glee4810/SCARE.\nInstitutional Review Board (IRB)\nIRB ap-\nproval is not required for this work.\nThe patient\nrecords used in this work are from the PhysioNet\nwebsite and licensed under the Open Data Commons\nOpen Database License v1.01.\n1. Introduction\nElectronic Health Records (EHRs) store a wide range\nof patient data, such as hospital admissions, diag-\nnoses, procedures, and prescriptions, making them\nessential for healthcare practice and research.\nAd-\nvances in Large Language Models (LLMs) have en-\nabled natural language interaction with EHRs, with\ntext-to-SQL models converting clinicians’ questions\ninto SQL queries to retrieve patient data without re-\nquiring SQL expertise (Wang et al., 2020; Lee et al.,\n2022; Bardhan et al., 2024).\nHowever, deploying these systems in clinical set-\ntings carries significant risks.\nFor example, an in-\ncorrect SQL query could miss a patient’s penicillin\nallergy or miscalculate a medication dosage, lead-\ning to potentially catastrophic outcomes. To ensure\nsafe deployment, question answering (QA) systems\nover EHRs must generate accurate queries that re-\nflect user intent or reject unsuitable ones. However,\n1. https://opendatacommons.org/licenses/odbl/1-0/\n© 2025 G. Lee, W. Chay & E. Choi.\narXiv:2511.17559v2  [cs.CL]  21 Dec 2025\n"}, {"page": 2, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nachieving such reliability is hindered by two key chal-\nlenges:\n(1) Clinicians, often unfamiliar with SQL\nor database systems, may pose problematic ques-\ntions (Lee et al., 2022).\nThese include unanswer-\nable queries (e.g., requesting physician data not in\nthe schema) and ambiguous ones (e.g., “BP?”) that\nmust be clarified rather than translated directly into\nSQL. (2) Even well-posed (answerable) questions can\nresult in incorrect SQL due to limitations in text-to-\nSQL models (Tarbell et al., 2023; Lee et al., 2024b;\nShi et al., 2024).\nTo\naddress\nthese\nchallenges,\nwe\nintroduce\nSCARE2, a benchmark designed to evaluate a\npost-hoc\nreliability\nlayer\nin\nEHR\nQA\nsystems.\nUnlike existing benchmarks that focus solely on\neither SQL correction (Pourreza and Rafiei, 2024;\nWang et al., 2023b; Askari et al., 2025) or question\nanswerability classification (Zhang et al., 2020; Wang\net al., 2023a), SCARE is the first to evaluate an\nintegrated post-hoc layer that handles both tasks,\nensuring more reliable QA for EHRs. To construct\nSCARE, we first source QA data compatible with\nthree major EHR databases—MIMIC-III (Johnson\net al., 2016), MIMIC-IV (Johnson et al., 2023), and\neICU (Pollard et al., 2018)—and augment it with\nmanually annotated ambiguous and unanswerable\nquestions to capture a diverse range of problematic\nuser queries.\nWe then generate SQL queries from\nthese answerable,\nambiguous,\nand unanswerable\nquestions using a diverse set of text-to-SQL mod-\nels, from lightweight options to advanced agentic\nframeworks, and pair them with the corresponding\nanswers (expected model outputs), yielding 4,200\nquestion–SQL–answer tuples.\nUsing SCARE as\na testbed, we compare various types of methods.\nOur experiments reveal significant challenges faced\nby\nexisting\nmodels\nin\nhandling\nthe\nintegrated\ntask, particularly in balancing the need to correct\nflawed queries while preserving already-correct ones,\nand in accurately identifying nuanced ambiguities.\nThese findings highlight the difficulty of robust\npost-hoc verification and underscore the necessity of\nSCARE for driving future research toward the safe\ndeployment of clinical QA systems.\n2. A\nbenchmark\nfor\nSQL\nCorrection\nand\nQuestion\nAnswerability Classification for Reliable EHR question\nanswering\n2. Related Work\n2.1. EHR QA for Structured Data\nEHR question answering (QA) involves answering\nclinically relevant questions by querying patient data\nstored in EHRs.\nThese tasks may require clini-\ncal knowledge or time-based reasoning.\nMIMIC-\nSQL (Wang et al., 2020) and EHRSQL (Lee et al.,\n2022), for example, employ text-to-SQL modeling\nto support question answering over structured EHR\ndatabases, demonstrating the potential for querying\nlarge-scale datasets using natural language.\nSimi-\nlarly, Raghavan et al. (2021) uses a method that first\nconverts natural language questions into logical forms\nand then translates them into SQL. Alternative ap-\nproaches have used SPARQL (Park et al., 2021) or\nPython-based agents (Shi et al., 2024) to perform\nEHR QA. However, text-to-SQL methods remain the\nmost widely used technology due to their efficiency\nand scalability for large-scale databases like EHRs.\n2.2. Reliability in Text-to-SQL\nAlongside advances in text-to-SQL models—from\nfew-shot prompting (Rajkumar et al., 2022; Chang\nand Fosler-Lussier) to advanced task decomposition\n(Pourreza and Rafiei, 2024) and multi-agent frame-\nworks (Wang et al., 2023b)—a stream of research has\nfocused on enabling more reliable deployment of these\nmodels, which can be broadly categorized into three\nareas.\nSQL Error Correction.\nThis line of work en-\nhances reliability by reducing model errors in SQL\ngeneration, aiming to improve overall performance.\nThe task typically involves taking an initial SQL\nquery as input and producing a corrected version as\noutput (Gong et al., 2025; Askari et al., 2025). Al-\nthough vital for boosting accuracy, these approaches\nassume that all input questions are convertible to\nSQL (i.e., answerable), limiting their applicability in\nreal-world clinical scenarios where user inputs are un-\nconstrained.\nQuestion Answerability Classification.\nA sec-\nond strand of research focuses exclusively on iden-\ntifying problematic user questions, typically before\nSQL generation. Benchmarks like TriageSQL (Zhang\net al., 2020) and DTE (Wang et al., 2023a) focus on\nclassifying questions as answerable or one of several\nnon-answerable categories. Although this tackles an\nessential layer of input validation, these methods act\n2\n"}, {"page": 3, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nFigure 1: Overview of the SCARE benchmark for evaluating a post-hoc verification layer. The task assumes\na candidate SQL query has already been generated by an upstream (and potentially black-box) text-to-SQL\nmodel. The layer then takes both the user’s question and the candidate SQL as input to decide one of\nfour actions: (1) for an answerable question with correct SQL, preserve the query; (2) for an answerable\nquestion with incorrect SQL, correct the query; (3) for an ambiguous question, identify the ambiguity for\nuser clarification; and (4) for an unanswerable question, reject the query and inform the user, thus ensuring\nthe overall reliability and safety of the EHR QA system.\nas pre-hoc filters and do not handle SQL correction,\nresulting in a gray area where neither task fully cov-\ners the combined challenge of question filtering and\nquery verification.\nSQL Generation with Abstention.\nMost closely\nrelated to our goal are works that integrate relia-\nbility into the end-to-end generation process.\nLee\net al. (2022, 2024b,a); Somov and Tutubalina (2025)\nevaluate models on their ability to generate accurate\nSQL while abstaining if the query is deemed incor-\nrect, where the concept of incorrect SQL covers both\nmodel errors for answerable questions and intrinsic\nfailure due to invalid user input such as ambiguous\nor unanswerable questions. While these approaches\nbridge the gap by incorporating caution directly into\nthe pipeline, their evaluation focuses on an all-or-\nnothing outcome: produce a perfect query or abstain.\nThis conflates the SQL generator’s capabilities with\nthe system’s verification mechanism, making it diffi-\ncult to assess their interplay. Furthermore, this all-\nor-nothing approach limits nuanced user interactions,\nsuch as noting that a question is ambiguous or that\na request is impossible to fulfill using SQL given the\ndatabase schema.\n3. Problem Formulation in SCARE\nWe define the task of post-hoc verification, performed\nby an independent safety layer within a safety-critical\nEHR QA system.\nThis layer audits the output of\nan upstream text-to-SQL model—treated as a black\nbox for modularity—before execution. The task re-\nquires three inputs: a natural language question q,\nthe database schema S, and a candidate SQL query\nˆy. Crucially, ˆy is provided for all question types to\nreflect a real-world deployment scenario where an up-\nstream model might still produce SQL for flawed in-\nputs.\nThe layer must therefore decide whether to\npreserve the candidate, correct it, or reject it by in-\nforming the user.\nThe SCARE benchmark contains four categories\naligned with Fig. 1:\n• answerable-correct (qans, ˆycor, y∗): qans is an\nanswerable question and ˆycor is the correct can-\n3\n"}, {"page": 4, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nFigure 2: Overview of the SCARE benchmark con-\nstruction pipeline.\ndidate SQL. The correctness is determined by\ncomparing its execution result to that of the\nground-truth SQL, y∗.\n• answerable-incorrect (qans, ˆyinc, y∗) :\nThe\ncandidate SQL query is incorrect.\n• ambiguous (qamb, ˆyinc, lamb): qamb is an ambigu-\nous question and the ground-truth label lamb is\nthe string “ambiguous”.\n• unanswerable (quna, ˆyinc, luna) :\nquna is an\nunanswerable question and the ground-truth la-\nbel luna is the string “unanswerable”.\nFor both ambiguous and unanswerable, any candi-\ndate ˆy is considered incorrect because these questions\neither require clarification for accurate SQL transla-\ntion or lie outside the SQL functionalities given the\nprovided schema.\nGiven (q, S, ˆy), the model f is expected to output\na verified result o∗∈{y′, ℓamb, ℓuna}:\nf(q, S, ˆy) =\n\n\n\n\n\ny′\nif q is answerable\nlamb\nif q is ambiguous\nluna\nif q is unanswerable,\n(1)\nwhere y′ is the output SQL (either the preserved ˆy or\na corrected SQL query) such that its execution result\nmatches that of the ground-truth SQL y∗.\n4. Benchmark Construction\nThis section describes the construction process of\nthe SCARE benchmark in three main stages: (1)\nannotating a diverse set of questions, including an-\nswerable, ambiguous, and unanswerable ones, across\nthree EHR databases; (2) generating candidate SQL\nqueries using multiple text-to-SQL models; and (3)\nconducting stratified sampling to create a balanced\nevaluation set. An overview of the process is shown\nin Figure 2.\n4.1. Question Annotation\nEHR Databases.\nOur work uses three major pub-\nlicly available EHR databases: MIMIC-III (Johnson\net al., 2016), MIMIC-IV (Johnson et al., 2023), and\neICU (Pollard et al., 2018). For MIMIC-III, we adopt\nthe schema from the MIMICSQL (Wang et al., 2020)\ndataset. For MIMIC-IV and eICU, we follow the pre-\nprocessing procedure used in EHRSQL (Lee et al.,\n2022).\nThe resulting schema sizes are: MIMIC-III\n(5 tables, 50 columns), MIMIC-IV (7 tables, 112\ncolumns), and eICU (10 tables, 72 columns).3. Using\nthese databases as the foundation for our benchmark,\nwe create a pool of answerable questions, followed by\nunanswerable and ambiguous ones.\n4.1.1. Answerable Question Creation\nFor\nquestions\ncompatible\nwith\nMIMIC-III,\nwe\nuse 1,000 question–SQL pairs from MIMICSQL\n(mimicsql_natural_v2). For MIMIC-IV and eICU,\nhowever, we cannot directly reuse the EHRSQL\npairs, because their value-shuffled patient data do\nnot match our databases.\nInstead, we construct\nnew data from scratch using the EHRSQL ques-\ntion templates (e.g., “What is the route of ad-\nministration of {drug_name}?”\nwhere the actual\nvalue for {drug_name} is later sampled from the\ndatabase). The construction proceeds in three steps:\n(1) sample valid values from the databases, (2)\ngenerate new ground-truth SQL queries, and (3)\nparaphrase the questions using OpenAI’s GPT-4o.\nThis process results in 450 pairs for MIMIC-IV and\n432 pairs for eICU. These SQL queries are consid-\nered the ground-truth answers for the answerable\nportion of our dataset (answerable-correct and\nanswerable-incorrect).\n4.1.2. Ambiguous and Unanswerable\nQuestion Annotation\nDuring the deployment of EHR QA systems, users\noften pose unanswerable or ambiguous questions for\nwhich no corresponding SQL exists. To address this\n3. We use the demo versions of MIMIC-IV and eICU to avoid\nprivacy alterations such as value shuffling, which perturbs\npatient data distributions.\nThe demos share the same\ndatabase schema structures as the full versions.\n4\n"}, {"page": 5, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 1: Sample data from the SCARE benchmark. For answerable-correct, the model is expected to\noutput the same SQL, as it is correct. For answerable-incorrect, the candidate SQL is flawed because it\nfails to use DISTINCT when counting patients, so the model is expected to correct the error in the SQL. For\nambiguous, the model is expected to output “ambiguous,” as the question contains the vague word “enough.”\nFor unanswerable, the model is expected to output “unanswerable,” as “family visitation” goes beyond the\ninformation stored in the schema.\nType\nUser Question\nCandidate SQL\nAnswer\nanswerable-\ncorrect\nCount the ICU visits of\npatient 10007058 since\n2100.\nSELECT COUNT(*) FROM icustays WHERE\nsubject_id = 10007058 AND intime >=\n‘2100-01-01 00:00:00’ AND intime <=\n‘2100-12-31 23:59:00’\nSELECT COUNT(*) FROM\nicustays WHERE subject_id\n= 10007058 AND intime\n>= ‘2100-01-01 00:00:00’\nAND intime <= ‘2100-12-31\n23:59:00’\nanswerable-\nincorrect\nHow many people were\nadmitted to the hospi-\ntal?\nSELECT count(subject_id) FROM\nadmissions\nSELECT count(DISTINCT\nsubject_id) FROM\nadmissions\nambiguous\nHow many patients were\nadministered divalproex\n(delayed release) in\nenough doses since 2100?\nSELECT count(DISTINCT subject_id)\nFROM prescriptions WHERE drug\n= ‘divalproex’ AND stoptime >\n‘2100-01-01’\n‘ambiguous’\nunanswerable\nDid patient 10007795\nhave family visitation\nduring their first ICU\nstay?\nSELECT ce.charttime AS\nvisitation_time FROM chartevents\nce JOIN d_items di ON ce.itemid =\ndi.itemid JOIN icustays icu ...\n‘unanswerable’\nissue, we curate such questions by annotating new in-\nstances into six categories: three types of ambiguity\nand three types of unanswerability, derived from prior\ntext-to-SQL literature and from frequently unanswer-\nable cases included in EHRSQL Lee et al. (2022).\nTo ensure a diverse and comprehensive set of prob-\nlematic questions, newly annotated questions are first\ngenerated using GPT-4o based on their target cate-\ngories, followed by human validation to ensure proper\nalignment between each question and its assigned cat-\negory, while filtering out semantically similar queries.\nDescriptions of these categories are provided below,\nwith further annotation details and examples in Ap-\npendix E.\nAmbiguous Questions.\nAmbiguous questions are\nthose that require clarification before SQL generation\n(the model answer to these questions is “ambiguous”).\nThese include vague-question (VQ) instances, such\nas short, phrasal questions (e.g., “Patient status?”)\n(Radhakrishnan et al., 2020; Wang et al., 2023a);\nvague-word (VW) instances with imprecise terms\n(e.g., “Recent high blood pressure cases”) (He et al.,\n2024); and ambiguous-reference (AR) instances\ninvolving unclear entities (e.g., “The patient from last\nweek”) (Yu et al., 2019; Zhu et al., 2024). For reli-\nable EHR QA systems, it is crucial to classify such\nquestions as “ambiguous” and notify users that fur-\nther clarification is needed to ensure accurate query\nprocessing.\nUnanswerable Questions.\nUnanswerable ques-\ntions cannot be resolved via SQL queries given the\nprovided database schema (the model answer to\nthese questions is “unanswerable”).\nThese include\nsmall-talk (ST), encompassing casual talk unre-\nlated to EHR (e.g., “What’s the weather like today?”)\n(Zhang et al., 2020); out-of-scope (OS), where re-\nquests go beyond SQL capabilities (e.g., “Predict fu-\nture patient outcomes”) (Zhang et al., 2020; Wang\net al., 2023a); and missing-column, including newly\ncreated examples (MC′) and those adapted from\nEHRSQL (MC′′), where questions reference non-\nexistent columns (e.g., “Query the blood_type col-\numn,” but it doesn’t exist) (Zhang et al., 2020; Wang\net al., 2023a; Lee et al., 2022).\nFor reliable EHR\nQA systems, it is crucial to classify such questions\nas “unanswerable” and inform users that the request\ncannot be fulfilled within the scope of the text-to-SQL\ntask. This notification is important for guiding users\nto refine their next input to align with the system’s\ncapabilities.\n5\n"}, {"page": 6, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 2: The dataset statistics of the SCARE bench-\nmark.\nDatabase\nanswerable-\ncorrect\nanswerable-\nincorrect\nambiguous unanswerable Total\nMIMIC-III\n350\n350\n350\n350\n1,400\nMIMIC-IV\n350\n350\n350\n350\n1,400\neICU\n350\n350\n350\n350\n1,400\nTotal\n1,050\n1,050\n1,050\n1,050\n4,200\n4.2. Candidate SQL Generation\nBuilding on the questions, we generate candidate\nSQL queries to simulate real-world model outputs\nthat could be implemented within the EHR QA sys-\ntem.\n4.2.1. Generating Candidate SQL\nTo generate a diverse pool of candidate SQL queries\n(ˆy), we utilize a variety of text-to-SQL models, rang-\ning from fine-tuned SQL-specialized models to agen-\ntic frameworks.\nThis diverse model set enables\nstress-testing of error-correction frameworks against\nvaried ˆy given q distributions, accommodating di-\nverse question types such as answerable, ambiguous,\nand unanswerable queries for EHR QA. The mod-\nels include: LLM-SQL, a few-shot baseline utiliz-\ning GPT-4o for SQL generation (Chang and Fosler-\nLussier); CodeS-15B, a 15B parameter model pre-\ntrained for SQL and fine-tuned on the BIRD dataset\n(Li et al., 2024); DIN-SQL, an advanced in-context\nlearning approach using GPT-4o with task decom-\nposition and self-correction (Pourreza and Rafiei,\n2024); MAC-SQL, a multi-agent framework em-\nploying GPT-4o for iterative SQL query refinement\n(Wang et al., 2023b); Deepseek R1-70B, a 70B\nparameter general-purpose reasoning model (Guo\net al., 2025); o4-mini, an advanced reasoning model\nfrom OpenAI; and Qwen3-32B, a 32B parameter\ngeneral-purpose reasoning model (Yang et al., 2025).\nTable 7 reports the execution accuracy of these mod-\nels.\n4.3. Stratified Sampling and Final Dataset\nCreation\nBased on the pool of generated SQL candidates, we\nconstruct the final benchmark through stratified sam-\npling, dividing the data into subgroups (strata) to\nensure a balanced distribution across different sce-\nnarios.\nFor each of the three databases, we select\n1,400 instances, evenly balanced across four strata\n(350 instances each): answerable-correct, where\nthe candidate SQL returns the correct answer, match-\ning the ground-truth result; answerable-incorrect,\nwhere the candidate SQL returns an incorrect an-\nswer, often due to issues like wrong column selection\nor invalid operations; ambiguous, where instances are\nderived from ambiguous questions, making any can-\ndidate SQL inherently invalid; and unanswerable,\nwhere instances are derived from unanswerable ques-\ntions, making any candidate SQL inherently invalid.\nAfter sampling from each database independently, we\ncombine these selections across the three databases\nto form the complete benchmark, resulting in a\nwell-balanced dataset of 4,200 instances overall (3\ndatabases × 1,400 instances each). Rather than re-\nflecting a naturally skewed real-world distribution of\nuser queries, this stratified, balanced split ensures\nthat each of the four key safety scenarios across dif-\nferent EHR databases is equally tested, which is es-\nsential for the diagnostic evaluation of post-hoc ver-\nification models in EHR QA systems. Sample data\nand data statistics are shown in Tables 1 and 2.\n5. Experiments\n5.1. Metrics\nFor the two answerable categories, we use three\nmetrics.\nFirst,\nwe measure Coverage\n(Cov),\nthe proportion of instances where the model pro-\nvides a SQL output rather than a classification la-\nbel (i.e., “ambiguous”, or “unanswerable”).\nThen,\nwe measure the Preservation\nRate\n(PR) on\nanswerable-correct inputs and the Correction\nRate\n(CR)\non\nanswerable-incorrect\ninputs.\nBoth metrics are defined as the final execution ac-\ncuracy, calculated over the total number of instances\nin their respective categories. This ensures that mod-\nels are penalized not only for errors in SQL genera-\ntion but also for incorrectly classifying an answerable\nquestion as ambiguous or unanswerable.\nFor the unanswerable and ambiguous categories,\nwe evaluate the model’s ability to correctly classify\nquestions into their respective categories. We report\nthe per-class Precision, Recall, and F1-score for\nboth the “unanswerable” and “ambiguous” labels.\n5.2. Methods\nTo establish comprehensive baselines on SCARE, we\nevaluate seven methods that represent distinct strate-\n6\n"}, {"page": 7, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 3: Baseline performance on the SCARE benchmark.\nMethods classify questions as answerable,\nambiguous, or unanswerable, and handle SQL queries for answerable questions by preserving correct queries\nor fixing incorrect ones. Metrics are defined in Section 5.1. Higher values indicate better performance.\nMethod\nanswerable-\ncorrect\nanswerable-\nincorrect\nambiguous\nunanswerable\nPR\nCov\nCR\nCov\nPrec\nRec\nF1\nPrec\nRec\nF1\nTwo-Stage\n80.4\n81.3\n42.5\n77.7\n72.5\n36.4\n48.4\n58.2\n93.0\n71.6\nSingle-Turn\n97.9\n99.5\n49.6\n97.6\n84.5\n31.6\n46.0\n84.1\n70.7\n76.8\nSingle-Turn-Veri\n98.1\n99.5\n52.1\n98.1\n89.0\n32.4\n47.5\n84.0\n72.5\n77.8\nMulti-Turn-SelfRef\n97.2\n99.4\n51.4\n98.1\n89.4\n32.9\n48.1\n83.5\n72.7\n77.7\nSingle-Turn-Cls\n95.5\n97.6\n53.0\n94.2\n87.3\n39.1\n54.0\n75.4\n86.6\n80.6\nSingle-Turn-Veri-Cls\n95.9\n97.8\n53.8\n94.0\n87.0\n39.0\n53.9\n75.7\n86.8\n80.8\nMulti-Turn-SelfRef-Cls\n97.8\n99.4\n53.0\n98.1\n88.0\n32.9\n47.9\n83.3\n72.7\n77.6\ngies for the joint task of question answerability classi-\nfication and SQL correction. These include four base\nmethods and three hybrid variants built upon them.\n• Two-Stage: This method follows a modular,\ndivide-and-conquer strategy. It first employs a\ndedicated classifier to determine if a question is\nanswerable, ambiguous, or unanswerable. Only\nif the question is deemed answerable does it pro-\nceed to a second stage, where a separate module\nverifies the candidate SQL and corrects it if nec-\nessary.\n• Single-Turn:\nThis method adopts an inte-\ngrated, end-to-end approach.\nIn a single gen-\nerative pass, the model is tasked with jointly an-\nalyzing the question and candidate SQL to si-\nmultaneously handle answerability classification\nand SQL correction, directly outputting either a\nfinal SQL query or a classification label.\n• Single-Turn-Veri:\nBuilding on the end-to-\nend approach, this method introduces a simple\niterative verification loop.\nIt extends Single-\nTurn by having an internal verifier to check the\ngenerated output. If the verifier flags an error,\nthe model performs multiple retries to generate\na correct answer.\n• Multi-Turn-SelfRef: This method involves\nan iterative refinement strategy in a multi-turn\nsetting. It generates an initial answer, produces\nfeedback on its own output, and then uses this\nfeedback to guide the next refinement attempt.\nThe three variant methods—Single-Turn-Cls,\nSingle-Turn-Veri-Cls,\nand\nMulti-Turn-\nSelfRef-Cls—are designed as hybrid approaches.\nThese aim to combine the strengths of the modular\nand integrated strategies by using the output from\nthe specialized classifier in Two-Stage (i.e., the\nclassification result and its reasoning) as an explicit\nguiding signal for the integrated models. We provide\nthe detailed prompts used to implement the four\nbase methods in Appendix F.\nFor the backbone LLMs, we use Gemini-2.5-Flash\n(Comanici et al., 2025) as the main LLM through-\nout the main experiments. Additional results for two\nother closed-source models (GPT-5 mini (OpenAI,\n2025) and Gemini-2.0-Flash (Google, 2025)) and two\nopen-source models (Llama-3.3-70B (Meta, 2024) and\nQwen3-32B (Yang et al., 2025)) are provided in Ap-\npendix A, all of which exhibit similar performance\ntrends when used to implement the methods.\n5.3. Results\nTable 3 shows the performance of various baseline\nmethods on the SCARE benchmark.\nTrade-off between question classification and\nSQL correction.\nA critical challenge lies in bal-\nancing the need to preserve correct SQL queries (PR)\nwhile accurately identifying ambiguous or unanswer-\nable questions.\nThe Two-Stage approach, which\ndecouples these tasks, achieves the highest recall for\nunanswerable (93.0%). However, this comes at a sig-\nnificant cost to Cov, as the initial classification stage\n7\n"}, {"page": 8, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nfrequently misidentifies answerable questions. Con-\nversely, Single-Turn excels at PR (97.9%) by in-\ntegrating the question classification and SQL correc-\ntion tasks, but its ability to detect ambiguity is no-\ntably weak (Recall 31.6%). This suggests an inherent\ntension where maximizing preservation often leads to\noverlooking problematic inputs, and vice versa. We\nprovide a qualitative analysis of the error cases in\nSection B.\nIterative\nrefinement\nimproves\ncorrection,\nbut\nlimitations\nremain.\nIterative approaches\n(Single-Turn-Veri and Multi-Turn-SelfRef)\ndemonstrate improvements in CR without compro-\nmising PR. Multi-Turn-SelfRef achieves a CR\nof 51.4%, compared to 49.6% for the basic Single-\nTurn, while maintaining a high PR (97.2%). How-\never, the overall correction capability is still limited.\nA detailed breakdown of correction outcomes by SQL\nerror type in Table 4 (with explanations provided in\nSection D) shows that, although the methods perform\nreasonably well on localized errors such as Table/-\nColumn (T/C) references (55.1% CR), they struggle\nconsiderably with Other Global (OG) errors, which\nrequire substantial structural changes to SQL queries\n(30.8% CR).\nHybrid approaches yield the best balance.\nThe most effective strategies leverage the strengths\nof both decoupled classification and integrated re-\nfinement.\nThe -Cls variants, which incorporate\nthe reasoning output from the Two-Stage classi-\nfier, significantly enhance overall performance. No-\ntably, Single-Turn-Veri-Cls achieves the highest\nCR (53.8%) and the best F1 score for unanswerable\n(80.8%), while maintaining a strong PR (95.9%).\nThis hybrid approach effectively mitigates the trade-\noffs observed in the base methods, pointing towards\nthe necessity of integrating explicit answerability rea-\nsoning into the verification process.\nNuanced ambiguity remains highly challeng-\ning.\nMethods\nconsistently\nstruggle\nto\nidentify\nambiguous, achieving a maximum F1 score of only\n54.0%.\nAs detailed in Table 5, detection rates for\nvague-question (VQ) and vague-word (VW) are\nparticularly poor, often remaining below 35%. This\nindicates that while models can easily identify overt\nissues like small-talk (ST, >94% recall), they lack\nthe sensitivity required to detect subtle linguistic am-\nbiguities. This deficiency poses a significant risk, as\nTable 4: Detailed correction rates by SQL error types\nfor answerable-incorrect. T/C, J/G, PV, OL, and\nOG denote table/column reference errors, JOIN/-\nGROUP BY errors, predicate value errors, other local\nerrors, and other global errors, respectively.\nCR\nT/C\nJ/G\nPV\nOL\nOG\nTwo-Stage\n45.4\n48.5\n39.7\n31.4\n17.8\nSingle-Turn\n52.4\n54.4\n50.7\n40.4\n28.0\nSingle-Turn-Veri\n55.1\n56.4\n50.7\n41.0\n29.0\nMulti-Turn-SelfRef\n55.1\n58.8\n50.7\n42.0\n30.8\nTable 5: Recall for correctly identifying ambiguous\nand unanswerable questions by granular question cat-\negories.\nSee Section 4.1.2 for question type defini-\ntions.\nRec\nVQ VW AR\nST\nOS\nMC′ MC′′\nTwo-Stage\n30.1\n22.1 56.9 100.0 99.2 79.1 94.7\nSingle-Turn\n32.6\n29.1\n33.1\n98.0\n74.1\n53.6\n60.3\nSingle-Turn-Veri\n33.7\n29.4\n34.0\n98.0\n77.0\n54.0\n64.0\nMulti-Turn-SelfRef\n34.8\n29.9\n33.7\n98.0\n77.0\n54.8\n64.0\nSingle-Turn-Cls\n32.0 32.3 53.1\n95.6\n92.1\n72.6\n87.0\nSingle-Turn-Veri-Cls\n32.6\n30.5\n54.0\n95.6\n91.2\n74.1\n87.0\nMulti-Turn-SelfRef-Cls 37.4 31.7\n52.3\n94.8\n91.6\n74.5\n87.7\nundetected ambiguities can lead to the execution of\nincorrect SQL queries.\n5.4. Qualitative Analysis of Failure Modes\nTo better understand the challenges highlighted by\nour benchmark, further qualitative analysis of model\nfailures is provided in Appendix B. As suggested by\nquantitative results, the most salient errors fall into\ntwo categories: failing to detect nuanced ambiguity\nand failing to correct global SQL errors.\nFailures in ambiguous Classification.\nModels\nconsistently fail to identify ambiguity when a ques-\ntion contains vague expressions (e.g., vague-word,\nvague-question) and the candidate SQL either ig-\nnores or misinterprets them.\nFor example, models\noften incorrectly approve a candidate SQL query for\na question like “Find patients with sufficient data”\nbecause the query appears syntactically valid, fail-\ning to recognize that “sufficient data” is unresolv-\nable without clarification. In other cases, for phrasal\nquestions like “Sodium?”, models incorrectly approve\n8\n"}, {"page": 9, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\na degenerate candidate SQL such as SELECT label\nFROM d_labitems WHERE label = ‘Sodium’ in-\nstead of classifying the question as ambiguous. These\nfailures persist even with frontier LLMs. We hypoth-\nesize that this is due to the lack of joint consideration\nof linguistic vagueness handling and SQL generation\nwhen building LLMs.\nAs a result, the models are\nbiased towards generating any SQL that seems fit,\nrather than assessing the semantic answerability of\nthe question itself.\nFailures in answerable-incorrect Correction.\nFor answerable questions, models struggle most with\nOther Global (OG) errors, which require substantial\nlogical or structural corrections to the candidate SQL.\nThese failures mostly stem from two main issues: lim-\nited ability to follow instructions and overreliance on\nparametric knowledge. First, models violate explicit\ntextual guidelines provided in the prompt. For ex-\nample, when instructed to use the earliest diagnosis\ntime if multiple records exist, a model often overlooks\nthis instruction and preserves incorrect SQL candi-\ndate queries. Second, models hallucinate SQL logic\nbased on their internal knowledge instead of using\nthe database schema provided in the context window.\nThese include cases where models invent non-existent\nICD codes or table relationships. These cases show\nthe limitations of current LLMs in strictly following\ncomplex instructions and leveraging provided context\nover internal parametric knowledge.\n6. Conclusion\nThe safe deployment of EHR question answering\nsystems in clinical environments demands reliabil-\nity mechanisms that go beyond standard text-to-\nSQL generation accuracy. In this work, we introduce\nSCARE, the first benchmark specifically designed to\nevaluate a unified post-hoc safety layer tasked with\nthe joint challenge of SQL correction and question an-\nswerability classification.\nGrounded in open-source\nEHR databases, SCARE incorporates diverse sce-\nnarios derived from various text-to-SQL models.\nOur comprehensive evaluation reveals critical lim-\nitations in current approaches. We uncover a stark\ntrade-off between preserving correct queries and accu-\nrately identifying problematic questions (either am-\nbiguous or unanswerable). Furthermore, our experi-\nmental results reveal that methods severely struggle\nto detect nuanced ambiguities commonly posed dur-\ning EHR QA. While hybrid approaches combining\niterative refinement with explicit classification sig-\nnals show promise, significant advancements are still\nneeded before these systems can be reliably deployed\nin safety-critical clinical applications. SCARE pro-\nvides an essential tool for the community to drive fu-\nture research toward developing robust and auditable\nverification methods, ultimately facilitating the safe\nintegration of LLMs into clinical workflows.\n7. Limitations and Future Work\nThe SCARE benchmark has several limitations\nrooted in its specific design choices.\nFirst, its bal-\nanced data distribution is intentionally designed for\na diagnostic stress test, ensuring each of the four key\nscenarios is evenly evaluated. However, this design\nin turn does not reflect the natural distribution of\nqueries during real-world clinical deployment. Simi-\nlarly, the benchmark is grounded in academic EHR\nschemas (MIMIC-III, MIMIC-IV, and eICU), which\nare smaller than many production systems.\nThis\nchoice was made to foster transparent and repro-\nducible research, though generalization to larger, pro-\nprietary schemas remains an important future chal-\nlenge.\nFinally, SCARE evaluates user-system in-\nteractions in a single-turn setting. This design en-\nables a controlled evaluation of a model’s ability to\ndetect various problematic question-candidate SQL\npairs without the confounding effects of dialogue his-\ntory.\nWe recognize that the resolution of ambigu-\nity, as opposed to its mere detection, is often best\nhandled through multi-turn interaction. Future re-\nsearch can extend the SCARE framework to the ac-\ntual production-level EHR systems and a multi-turn\nsetting.\nAcknowledgments\nThis work was supported by the Institute of In-\nformation\n&\nCommunications\nTechnology\nPlan-\nning\n&\nEvaluation\n(IITP)\ngrants\n(No.RS-2019-\nII190075, No.RS-2025-02304967) and National Re-\nsearch Foundation of Korea (NRF) grants (NRF-\n2020H1D3A2A03100945), funded by the Korea gov-\nernment (MSIT).\nReferences\nArian Askari, Christian Poelitz, and Xinye Tang.\nMagic: generating self-correction guideline for in-\n9\n"}, {"page": 10, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\ncontext text-to-sql. In Proceedings of the Thirty-\nNinth AAAI Conference on Artificial Intelligence\nand Thirty-Seventh Conference on Innovative Ap-\nplications of Artificial Intelligence and Fifteenth\nSymposium on Educational Advances in Artificial\nIntelligence,\nAAAI’25/IAAI’25/EAAI’25. AAAI\nPress, 2025.\nISBN 978-1-57735-897-8.\ndoi: 10.\n1609/aaai.v39i22.34511. URL https://doi.org/\n10.1609/aaai.v39i22.34511.\nJayetri Bardhan, Kirk Roberts, and Daisy Zhe Wang.\nQuestion answering for electronic health records:\nScoping review of datasets and models. Journal of\nMedical Internet Research, 26:e53636, 2024.\nShuaichen Chang and Eric Fosler-Lussier.\nHow to\nprompt llms for text-to-sql:\nA study in zero-\nshot, single-domain, and cross-domain settings. In\nNeurIPS 2023 Second Table Representation Learn-\ning Workshop.\nGheorghe Comanici, Eric Bieber, Mike Schaeker-\nmann, and et al. Gemini 2.5: Pushing the fron-\ntier with advanced reasoning, multimodality, long\ncontext, and next generation agentic capabilities,\n2025. URL https://arxiv.org/abs/2507.06261.\nYue Gong, Chuan Lei, Xiao Qin, Kapil Vaidya,\nBalakrishnan Narayanaswamy, and Tim Kraska.\nSqlens:\nAn end-to-end framework for error de-\ntection and correction in text-to-sql, 2025. URL\nhttps://arxiv.org/abs/2506.04494.\nGoogle.\nGemini\n2.0:\nFlash,\nflash-lite\nand\npro.\nhttps://developers.googleblog.com/en/\ngemini-2-family-expands/, Feb 2025. Accessed:\n2025-09-05.\nDaya Guo,\nDejian Yang,\nHaowei Zhang,\nJunx-\niao Song,\nRuoyu Zhang,\nRunxin Xu,\nQihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948, 2025.\nXinyi He, Mengyu Zhou, Xinrun Xu, Xiaojun Ma,\nRui Ding, Lun Du, Yan Gao, Ran Jia, Xu Chen,\nShi Han, et al. Text2analysis: A benchmark of ta-\nble question answering with advanced data analysis\nand unclear queries. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 38,\npages 18206–18215, 2024.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-\nwei H Lehman, Mengling Feng, Mohammad Ghas-\nsemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G Mark. Mimic-iii, a freely\naccessible critical care database. Scientific data, 3\n(1):1–9, 2016.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen,\nAlvin Gayles, Ayad Shammout, Steven Horng,\nTom J Pollard, Sicheng Hao, Benjamin Moody,\nBrian Gow, et al. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):\n1, 2023.\nGyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu\nKwon, Woncheol Shin, Seongjun Yang, Minjoon\nSeo, Jong-Yeup Kim, and Edward Choi. Ehrsql:\nA practical text-to-sql benchmark for electronic\nhealth records.\nAdvances in Neural Information\nProcessing Systems, 35:15589–15601, 2022.\nGyubok Lee, Woosog Chay, Seonhee Cho, and Ed-\nward Choi. Trustsql: Benchmarking text-to-sql re-\nliability with penalty-based scoring. arXiv preprint\narXiv:2403.15879, 2024a.\nGyubok Lee, Sunjun Kweon, Seongsu Bae, and Ed-\nward Choi. Overview of the EHRSQL 2024 shared\ntask on reliable text-to-SQL modeling on elec-\ntronic health records. In Tristan Naumann, Asma\nBen Abacha, Steven Bethard, Kirk Roberts, and\nDanielle Bitterman, editors, Proceedings of the 6th\nClinical Natural Language Processing Workshop,\npages 644–654, Mexico City, Mexico, June 2024b.\nAssociation for Computational Linguistics.\ndoi:\n10.18653/v1/2024.clinicalnlp-1.62.\nURL https:\n//aclanthology.org/2024.clinicalnlp-1.62.\nHaoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xi-\naokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan,\nCuiping Li, and Hong Chen.\nCodes:\nTowards\nbuilding open-source language models for text-to-\nsql.\nProceedings of the ACM on Management of\nData, 2(3):1–28, 2024.\nMeta.\nLlama 3.3:\nModel cards and prompt for-\nmats, 2024. URL https://www.llama.com/docs/\nmodel-cards-and-prompt-formats/llama3_3/.\nOpenAI. Introducing gpt-5. https://openai.com/\nindex/introducing-gpt-5/, Aug 2025. Accessed:\n2025-09-05.\n10\n"}, {"page": 11, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nJunwoo Park, Youngwoo Cho, Haneol Lee, Jaegul\nChoo, and Edward Choi. Knowledge graph-based\nquestion answering with electronic health records.\nIn Machine Learning for Healthcare Conference,\npages 36–53. PMLR, 2021.\nTom J Pollard, Alistair EW Johnson, Jesse D Raffa,\nLeo A Celi, Roger G Mark, and Omar Badawi.\nThe eicu collaborative research database, a freely\navailable multi-center database for critical care re-\nsearch. Scientific data, 5(1):1–13, 2018.\nMohammadreza Pourreza and Davood Rafiei. Din-\nsql: Decomposed in-context learning of text-to-sql\nwith self-correction. Advances in Neural Informa-\ntion Processing Systems, 36, 2024.\nKarthik\nRadhakrishnan,\nArvind\nSrikantan,\nand\nXi Victoria Lin. Colloql: Robust text-to-sql over\nsearch queries. In Proceedings of the First Work-\nshop on Interactive and Executable Semantic Pars-\ning, pages 34–45, 2020.\nPreethi Raghavan, Jennifer J Liang, Diwakar Maha-\njan, Rachita Chandra, and Peter Szolovits. emrk-\nbqa: A clinical knowledge-base question answering\ndataset.\nAssociation for Computational Linguis-\ntics, 2021.\nNitarshan Rajkumar, Raymond Li, and Dzmitry\nBahdanau.\nEvaluating the text-to-sql capabili-\nties of large language models.\narXiv preprint\narXiv:2204.00498, 2022.\nWenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu\nZhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl\nYang, and May Dongmei Wang. Ehragent: Code\nempowers large language models for few-shot com-\nplex tabular reasoning on electronic health records.\nIn Proceedings of the 2024 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n22315–22339, 2024.\nOleg Somov and Elena Tutubalina. Confidence esti-\nmation for error detection in text-to-sql systems.\narXiv preprint arXiv:2501.09527, 2025.\nRichard Tarbell, Kim-Kwang Raymond Choo, Glenn\nDietrich, and Anthony Rios. Towards understand-\ning the generalization of medical text-to-sql mod-\nels and datasets, 2023. URL https://arxiv.org/\nabs/2303.12898.\nBing Wang,\nYan Gao,\nZhoujun Li,\nand Jian-\nGuang Lou.\nKnow what I don’t know:\nHan-\ndling ambiguous and unknown questions for text-\nto-SQL.\nIn Anna Rogers, Jordan Boyd-Graber,\nand Naoaki Okazaki, editors, Findings of the Asso-\nciation for Computational Linguistics: ACL 2023,\npages 5701–5714, Toronto, Canada, July 2023a.\nAssociation for Computational Linguistics.\ndoi:\n10.18653/v1/2023.findings-acl.352.\nURL https:\n//aclanthology.org/2023.findings-acl.352.\nBing Wang, Changyu Ren, Jian Yang, Xinnian Liang,\nJiaqi Bai, Qian-Wen Zhang, Zhao Yan, and Zhou-\njun Li. Mac-sql: Multi-agent collaboration for text-\nto-sql. arXiv preprint arXiv:2312.11242, 2023b.\nPing Wang, Tian Shi, and Chandan K Reddy. Text-\nto-sql generation for question answering on elec-\ntronic medical records. In Proceedings of The Web\nConference 2020, pages 350–361, 2020.\nAn Yang, Zihan Qiu, et al. Qwen3 technical report,\n2025. URL https://arxiv.org/abs/2505.09388.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, et al. Cosql: A conversational text-\nto-sql challenge towards cross-domain natural lan-\nguage interfaces to databases. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1962–1979, 2019.\nYusen Zhang, Xiangyu Dong, Shuaichen Chang, Tao\nYu, Peng Shi, and Rui Zhang. Did you ask a good\nquestion? a cross-domain question intention classi-\nfication benchmark for text-to-sql. arXiv preprint\narXiv:2010.12634, 2020.\nXiaohu Zhu, Qian Li, Lizhen Cui, and Yongkang Liu.\nLarge language model enhanced text-to-sql gener-\nation: A survey. arXiv preprint arXiv:2410.06011,\n2024.\n11\n"}, {"page": 12, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nAppendix A. Full Performance\nResults\nTable 6 reports the full results for GPT-5 mini,\nGemini-2.0-Flash, Llama-3.3-70B, and Qwen3-32B.\nAppendix B. Qualitative Analysis\nThrough a manual review of incorrect model out-\nputs, we identified several recurring error patterns\nthat highlight the key challenges posed by our bench-\nmark. The primary failure mode for each category is\ndetailed below:\n• answerable-correct: The most common type\nof failure was an attempt to modify an already-\ncorrect candidate SQL query, which resulted in\nan incorrect version. It was infrequent for the\nmodel to misclassify these questions as not an-\nswerable.\n• answerable-incorrect:\nThe most common\nfailure was the inability to correct the provided\nincorrect SQL query.\n• ambiguous: the most common failure was classi-\nfying the question as answerable and generating\na SQL query without realizing the question’s am-\nbiguity. For example, when given the question\nis “Albumin?” and a candidate SQL provided an\nempty result, a model may decide to fix the query\nto not have an empty result rather than classify-\ning the question as ambiguous. This could result\nin a final output like “SELECT DISTINCT label\nFROM d_labitems WHERE label = ’albumin”’.\nMisclassifying an ambiguous question as “unan-\nswerable” was infrequent.\n• unanswerable:\nMisclassifying the question as\n“ambiguous” was as common as misclassifying it\nas answerable and attempting to provide a SQL\nquery. This was mainly due to models not taking\nthe database schema (even though it was given\nas an input) into consideration when determin-\ning answerability. For example, for the question\n“What was the name of the diagnosis for pa-\ntient 10039997 in other departments?”, where no\ncolumns regarding departments exist, a model\nmight misclassify the question as ambiguous be-\ncause of the phrase “other departments.”\nThis\nindicates the model failed to incorporate the pro-\nvided knowledge of the database schema when\nclassifying the question.\nAppendix C. Performance of\nText-to-SQL Models for\nCandidate SQL\nGeneration\nTable 7 presents the performance comparison of seven\ntext-to-SQL models across three medical datasets:\nMIMIC-IV, eICU, and MIMIC-III (MIMICSQL). No-\ntably, OpenAI’s o4-mini achieves the highest accu-\nracy, with Qwen3-32B ranking second, while CodeS-\n15B yields the lowest performance.\nAppendix D. SQL Error Types and\nDefinitions\nTable 8 presents the examples of error types present\nin the candidate SQL query from the answerable\nquestions of the sCARE benchmark. In the table,\nT/C, J/G, PV, OL, and OG denote table/column\nreference errors, JOIN/GROUP BY errors, predicate\nvalue errors, other local errors, and other global er-\nrors, respectively.\nAppendix E. Details of Ambiguous\nand Unanswerable\nQuestion Annotation\nWe provide further details on the generation and an-\nnotation process for the six categories of ambigu-\nous and unanswerable questions introduced in Sec-\ntion 4.1.2. Table 9 shows samples for each category\nalongside an answerable example.\nE.1. Ambiguous Questions\nThe following details the generation process for the\nthree types of ambiguous questions.\n• vague-question: We prompt GPT-4o to cre-\nate overly vague, keyword-based questions (Rad-\nhakrishnan et al., 2020) conditioned on a hospital\ndomain, simulating user queries that lack specific\nintent (e.g., “Weight?”, “Symptoms?”).\n• vague-word: To generate questions with impre-\ncise filtering conditions, we prompt GPT-4o to\nstrategically insert ambiguous terms (e.g., “com-\nmon,” “long,” “more than usual”) into otherwise\nanswerable questions.\n12\n"}, {"page": 13, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 6: Full baseline performance on the SCARE benchmark. Higher values indicate better performance.\nMethod\nanswerable-\ncorrect\nanswerable-\nincorrect\nambiguous\nunanswerable\nPR\nCov\nCR\nCov\nPrec\nRec\nF1\nPrec\nRec\nF1\nGPT-5 mini\nTwo-Stage\n82.4\n85.6\n55.1\n83.9\n68.1\n73.7\n70.8\n77.7\n88.4\n82.7\nSingle-Turn\n95.7\n99.3\n62.6\n98.4\n75.5\n44.3\n55.8\n87.7\n64.6\n74.4\nSingle-Turn-Veri\n96.0\n99.4\n65.0\n99.1\n80.8\n46.0\n58.6\n85.5\n70.7\n77.4\nMulti-Turn-SelfRef\n95.7\n99.5\n67.8\n98.7\n81.8\n46.1\n59.0\n83.5\n72.3\n77.5\nSingle-Turn-Cls\n88.4\n91.7\n59.0\n86.6\n74.6\n75.8\n75.2\n79.0\n88.0\n83.2\nSingle-Turn-Veri-Cls\n88.8\n92.7\n63.7\n90.1\n77.5\n75.5\n76.5\n80.2\n88.1\n84.0\nMulti-Turn-SelfRef-Cls\n93.2\n97.9\n68.1\n95.9\n84.9\n68.5\n75.8\n79.6\n89.0\n84.0\nGemini-2.0-Flash\nTwo-Stage\n78.6\n80.0\n27.5\n75.5\n77.3\n35.6\n48.8\n54.3\n95.7\n69.3\nSingle-Turn\n98.8\n99.5\n29.4\n94.7\n69.2\n13.2\n22.2\n78.5\n68.5\n73.1\nSingle-Turn-Veri\n98.9\n99.4\n30.7\n94.2\n68.6\n15.0\n24.6\n78.7\n69.8\n74.0\nMulti-Turn-SelfRef\n97.7\n98.8\n34.2\n91.5\n68.8\n18.9\n29.6\n74.1\n76.3\n75.2\nSingle-Turn-Cls\n96.3\n97.6\n30.1\n89.9\n84.9\n33.2\n47.8\n70.6\n92.6\n80.1\nSingle-Turn-Veri-Cls\n96.3\n97.7\n31.1\n89.1\n85.8\n34.0\n48.7\n70.4\n92.9\n80.1\nMulti-Turn-SelfRef-Cls\n95.0\n96.4\n34.0\n87.1\n84.8\n35.5\n50.1\n67.7\n93.2\n78.4\nLlama-3.3-70B\nTwo-Stage\n69.0\n73.1\n29.9\n68.1\n71.4\n47.6\n57.1\n52.9\n99.0\n68.9\nSingle-Turn\n60.4\n98.5\n20.8\n90.4\n48.8\n18.6\n26.9\n75.6\n69.6\n72.5\nSingle-Turn-Veri\n97.1\n99.2\n38.1\n92.3\n57.6\n25.3\n35.2\n74.5\n76.5\n75.5\nMulti-Turn-SelfRef\n95.1\n97.4\n38.9\n88.1\n60.0\n31.8\n41.6\n72.0\n77.6\n74.7\nSingle-Turn-Cls\n74.1\n77.0\n29.5\n69.6\n74.9\n49.4\n59.6\n54.1\n99.1\n70.0\nSingle-Turn-Veri-Cls\n75.0\n77.6\n31.1\n69.0\n74.0\n49.3\n59.2\n54.3\n98.9\n70.1\nMulti-Turn-SelfRef-Cls\n72.7\n75.3\n33.0\n67.3\n72.1\n49.1\n58.4\n53.6\n99.0\n69.5\nQwen3-32B\nTwo-Stage\n73.6\n75.0\n22.6\n71.7\n60.4\n51.4\n55.6\n58.9\n94.1\n72.5\nSingle-Turn\n51.0\n93.4\n18.5\n84.7\n60.4\n25.9\n36.3\n71.1\n68.6\n69.8\nSingle-Turn-Veri\n97.8\n99.2\n31.8\n88.5\n65.9\n31.2\n42.4\n71.5\n74.0\n72.7\nMulti-Turn-SelfRef\n91.0\n95.0\n40.9\n89.7\n69.5\n40.7\n51.3\n68.4\n79.3\n73.5\nSingle-Turn-Cls\n78.5\n81.0\n27.9\n74.1\n60.9\n50.0\n54.9\n61.4\n94.2\n74.4\nSingle-Turn-Veri-Cls\n78.8\n81.0\n29.9\n73.3\n61.5\n51.7\n56.2\n61.6\n93.8\n74.3\nMulti-Turn-SelfRef-Cls\n86.1\n89.9\n41.0\n83.9\n71.7\n51.5\n59.9\n65.8\n92.0\n76.7\n13\n"}, {"page": 14, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 7: Execution accuracy of seven different text-\nto-SQL models on MIMIC-IV, eICU, and MIMIC-III\n(MIMICSQL).\nMIMIC-IV\neICU\nMIMICSQL\nLLM-SQL\n61.1\n60.2\n74.4\nCodeS-15B\n24.0\n15.1\n62.0\nDIN-SQL\n59.8\n56.3\n76.9\nMAC-SQL\n66.0\n59.5\n75.7\nDeepseek R1-70B\n46.7\n54.2\n76.0\nQwen3-32B\n69.3\n62.3\n86.9\nOpenAI o4-mini\n72.4\n69.0\n85.9\nOn Average\n40.1\n38.2\n65.8\n• ambiguous-reference:\nWe create questions\nwith unresolved references by prompting GPT-\n4o to modify answerable questions, incorporating\nreferentially ambiguous words like “this,” “that,”\nor “them.”\nE.2. Unanswerable Questions\nWe create three types of unanswerable questions to\ntest a system’s ability to recognize queries beyond its\nscope.\n• small-talk: We use GPT-4o to generate con-\nversational questions unrelated to EHR data\n(e.g., “Did you grab coffee before rounds to-\nday?”), with instructions to explicitly avoid ref-\nerencing the database schema.\n• out-of-scope: We generate these by prompting\nGPT-4o to transform existing answerable ques-\ntions into analytical tasks that extend beyond\nSQL’s capabilities, such as predictive modeling.\n• missing-column: In addition to using examples\nfrom EHRSQL, we generate a new set of more\ndifficult questions for this category. We increase\nthe difficulty by designing questions that refer-\nence columns that are plausible within the EHR\ncontext but do not exist in the database schema.\nQuality Check\nTo ensure the reliability and con-\nsistency of our annotations, all generated questions\nundergo a rigorous review process.\nThe process is\nconducted by three annotators (two authors and one\nhired external annotator), all of whom are com-\nputer science graduate students proficient in SQL.\nEach annotator independently evaluates whether the\nquestions fit their designated categories.\nWe mea-\nsure inter-annotator agreement using Cohen’s kappa,\nwhich ranges from 85.8% to 90.9%, and Fleiss’ kappa,\nwhich is 87.8%, indicating a high level of agreement.\nOnly questions that receive unanimous approval from\nall three annotators are included in the final dataset.\nAppendix F. Baseline Method\nImplementation\nWe present the prompts used to implement our four\nbase methods: Two-Stage, Single-Turn, Single-\nTurn-Veri, and Multi-Turn-SelfRef. Note that\nthe {evidence} part in the prompt refers to assump-\ntions made during SQL annotation that are not ex-\nplicitly stated in the questions (e.g., use SQLite for\nSQL query generation; use DENSE_RANK() only\nwhen ranking is explicitly specified).\nF.1. Two-Stage\nPrompt 1: Prompt used for the classifica-\ntion stage in Two-Stage.\nYour task is to classify a natural language\nquestion into one of the following three\ncategories, based on whether it can be\nanswered using SQL over the given database\nschema.\nClassification Categories:\n1. **\"answerable\"** - The question can be\nclearly answered with the given database\nschema. All required information (tables,\ncolumns, relationships) exists in the schema.\n2. **\"ambiguous\"** - The question is unclear,\nambiguous, or requires clarification. This\nincludes:\n- Questions with unclear references (\"it\",\n\"that\", \"the previous one\")\n- Questions with multiple possible\ninterpretations without further\nclarification\n- Questions that are too vague to\nunderstand the intent\n3. **\"unanswerable\"** - The question cannot\nbe answered with the given database schema.\nThis includes:\n- Questions requiring information not\navailable in the database\n- Questions that are completely out of\nscope for SQL operations\n14\n"}, {"page": 15, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\n- Questions requiring functionality\noutside of SQL operations\n- Questions that are general conversation\nor small talk\n# Input\n- Database Schema\n- Question\n# Output Format\nRespond with a single JSON object:\n{{\n\"reasoning\": \"<reasoning behind your\ndecision>\",\n\"answer\": \"<one of the three categories: \"\nanswerable\", \"ambiguous\", or \"unanswerable\n\">\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion: {question}\nPrompt 2: Prompt used for the SQL cor-\nrection stage in Two-Stage.\nYour task is to check whether the given SQL\nquery is correct according to the schema, the\nquestion, and the SQL guideline.\nIf it is incorrect, provide a corrected query\n. If it is correct, return it unchanged.\n# Input\n- Database Schema\n- Question\n- Evidence (SQL guideline)\n- Current SQL Query\n- Execution Result\n# Output Format\nRespond with a single JSON object:\n{{\n\"reasoning\": \"<reasoning behind your\ndecision>\",\n\"answer\": \"the corrected SQL query if\nincorrect, otherwise the original query\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion:\n{question}\nEvidence:\n{evidence}\nCurrent SQL Query:\n{sql}\nExecution Result:\n{exec}\nF.2. Single-Turn\nPrompt 3:\nThe prompt used in Single-\nTurn.\nYour task is to determine whether the\npredicted SQL is correct, or whether the\nquestion is intrinsically ambiguous or\nunanswerable for SQL generation, given the\nquestion and database schema. Follow the\ninstructions below:\n- If the question is **answerable** and the\nSQL is **correct**, output the same SQL.\n- If the question is **answerable** but the\nSQL is **incorrect**, output the fixed SQL.\n- If the question is **ambiguous** (requires\nclarification before SQL translation), output\n`\"ambiguous\"`. Ambiguity types include:\n1. vague-question: Extremely brief or\nunclear questions (e.g., \"BP?\", \"Patient?\")\n.\n2. vague-word: Questions containing vague\nwords (e.g., \"How many patients meet the *\nhigh risk* criteria?\").\n3. referential-ambiguity: Questions\ncontaining unclear referents (e.g., \"When\nwas the first time *it* happened in the\nmost recent hospital visit?\").\n- If the question is **unanswerable** (cannot\nbe converted into valid SQL), output `\"\nunanswerable\"`. Unanswerable types include:\n1. small-talk: Casual queries unrelated to\nthe data (e.g., \"Did you grab coffee?\").\n2. out-of-scope: Requests that cannot be\nhandled by SQL (e.g., \"Can you *cluster*\npatients based on medication patterns?\").\n15\n"}, {"page": 16, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\n3. missing-column: References to non-\nexistent columns (e.g., asking for an *\naddress* field not present in the schema).\n# Important\n- Do not assume anything that is not\nexplicitly stated in the input.\n# Input\n- Database Schema\n- Question\n- Evidence (SQL guideline)\n- Predicted SQL\n- Execution Result\n# Output Format\nRespond with a single JSON object:\n{{\n\"reasoning\": \"<reasoning behind your\ndecision>\",\n\"answer\": \"<either the original SQL, fixed\nSQL, \"ambiguous\", or \"unanswerable\">\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion:\n{question}\nEvidence:\n{evidence}\nPredicted SQL:\n{sql}\nExecution Result:\n{exec}\nF.3. Single-Turn-Veri\nPrompt 4:\nThe SQL correction prompt\nused in Single-Turn-Veri.\nYour task is to determine whether the\npredicted SQL is correct, or whether the\nquestion is intrinsically ambiguous or\nunanswerable for SQL generation, given the\nquestion and database schema. Follow the\ninstructions below:\n- If the question is **answerable** and the\nSQL is **correct**, output the same SQL.\n- If the question is **answerable** but the\nSQL is **incorrect**, output the fixed SQL.\n- If the question is **ambiguous** (requires\nclarification before SQL translation), output\n`\"ambiguous\"`. Ambiguity types include:\n1. vague-question: Extremely brief or\nunclear questions (e.g., \"BP?\", \"Patient?\")\n.\n2. vague-word: Questions containing vague\nwords (e.g., \"How many patients meet the *\nhigh risk* criteria?\").\n3. referential-ambiguity: Questions\ncontaining unclear referents (e.g., \"When\nwas the first time *it* happened in the\nmost recent hospital visit?\").\n- If the question is **unanswerable** (cannot\nbe converted into valid SQL), output `\"\nunanswerable\"`. Unanswerable types include:\n1. small-talk: Casual queries unrelated to\nthe data (e.g., \"Did you grab coffee?\").\n2. out-of-scope: Requests that cannot be\nhandled by SQL (e.g., \"Can you *cluster*\npatients based on medication patterns?\").\n3. missing-column: References to non-\nexistent columns (e.g., asking for an *\naddress* field not present in the schema).\n# Important\n- Do not assume anything that is not\nexplicitly stated in the input.\n# Input\n- Database Schema\n- Question\n- Evidence (SQL guideline)\n- Predicted SQL\n- Execution Result\n# Output Format\nRespond with a single JSON object:\n{{\n\"reasoning\": \"<reasoning behind your\ndecision>\",\n\"answer\": \"<either the original SQL, fixed\nSQL, \"ambiguous\", or \"unanswerable\">\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion:\n16\n"}, {"page": 17, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\n{question}\nEvidence:\n{evidence}\nPredicted SQL:\n{sql}\nExecution Result:\n{exec}\nPrompt 5:\nThe verifier prompt used in\nSingle-Turn-Veri.\nYour task is to verify whether the model has\ncorrectly followed the task instructions for\nSQL prediction. Carefully evaluate the\npredicted SQL in relation to the database\nschema, the question, the evidence (SQL\nguideline), and the execution result.\nFollow the instructions below:\n- If the predicted SQL is correct (or if the\nquestion is not answerable and the label is \"\nambiguous\" or \"unanswerable\"), start your\nfeedback with the phrase \"the predicted SQL\nis correct\".\n- If the predicted SQL is incorrect, start\nyour feedback with the phrase \"the predicted\nSQL is incorrect\". Then explain clearly why\nit is wrong (e.g., incorrect column, wrong\njoin, missing condition, misclassification of\nambiguity/unanswerability, etc.).\nAmbiguity Types\n1. vague-question: Extremely brief or\nunclear questions (e.g., \"BP?\", \"Patient?\")\n.\n2. vague-word: Questions containing vague\nwords (e.g., \"How many patients meet the *\nhigh risk* criteria?\").\n3. referential-ambiguity: Questions\ncontaining unclear referents (e.g., \"When\nwas the first time *it* happened in the\nmost recent hospital visit?\").\nUnanswerable Types\n1. small-talk: Casual queries unrelated to\nthe data (e.g., \"Did you grab coffee?\").\n2. out-of-scope: Requests that cannot be\nhandled by SQL (e.g., \"Can you *cluster*\npatients based on medication patterns?\").\n3. missing-column: References to non-\nexistent columns (e.g., asking for an *\naddress* field not present in the schema).\n# Important\n- Your role is to provide evaluation feedback\nonly, not to generate or fix SQL\n- Your feedback should be precise and\ngrounded in the given schema and instruction.\n- Do not assume anything that is not\nexplicitly stated in the input.\n# Input\n- Database Schema\n- Question\n- Question Explanation\n- Evidence (SQL guideline)\n- Predicted SQL\n- SQL Explanation\n- Execution Result\n# Output Format\nRespond with a single JSON object:\n{{\n\"feedback\": \"<detailed feedback on your\ndecision>\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion:\n{question}\nEvidence:\n{evidence}\nPredicted SQL:\n{sql}\nExecution Result:\n{exec}\nF.4. Multi-Turn-SelfRef\nPrompt 6: Prompt used for the SQL cor-\nrection stage in Multi-Turn-SelfRef.\nYour task is to determine whether the\npredicted SQL is correct, or whether the\nquestion is intrinsically ambiguous or\nunanswerable for SQL generation, given the\n17\n"}, {"page": 18, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nquestion and database schema. Follow the\ninstructions below:\n- If the question is **answerable** and the\nSQL is **correct**, output the same SQL.\n- If the question is **answerable** but the\nSQL is **incorrect**, output the fixed SQL.\n- If the question is **ambiguous** (requires\nclarification before SQL translation), output\n`\"ambiguous\"`. Ambiguity types include:\n1. vague-question: Extremely brief or\nunclear questions (e.g., \"BP?\", \"Patient?\")\n.\n2. vague-word: Questions containing vague\nwords (e.g., \"How many patients meet the *\nhigh risk* criteria?\").\n3. referential-ambiguity: Questions\ncontaining unclear referents (e.g., \"When\nwas the first time *it* happened in the\nmost recent hospital visit?\").\n- If the question is **unanswerable** (cannot\nbe converted into valid SQL), output `\"\nunanswerable\"`. Unanswerable types include:\n1. small-talk: Casual queries unrelated to\nthe data (e.g., \"Did you grab coffee?\").\n2. out-of-scope: Requests that cannot be\nhandled by SQL (e.g., \"Can you *cluster*\npatients based on medication patterns?\").\n3. missing-column: References to non-\nexistent columns (e.g., asking for an *\naddress* field not present in the schema).\n# Important\n- Do not assume anything that is not\nexplicitly stated in the input.\n# Input\n- Database Schema\n- Question\n- Evidence (SQL guideline)\n- Predicted SQL\n- Execution Result\n# Output Format\nRespond with a single JSON object:\n{{\n\"reasoning\": \"<reasoning behind your\ndecision>\",\n\"answer\": \"<either the original SQL, fixed\nSQL, \"ambiguous\", or \"unanswerable\">\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion:\n{question}\nEvidence:\n{evidence}\nPredicted SQL:\n{sql}\nExecution Result:\n{exec}\nPrompt 7: Prompt used for the feedback\nstage in Multi-Turn-SelfRef.\nYour task is to review whether the model has\nfollowed the task instructions correctly for\nSQL prediction. Carefully examine the\ndatabase schema, the question, the evidence (\nSQL guideline), the predicted SQL, and the\nexecution result.\nFollow the instructions below:\n- If the predicted SQL is correct (or if the\nquestion is not answerable and the label is \"\nambiguous\" or \"unanswerable\"), start your\nfeedback with the phrase \"the predicted SQL\nis correct\".\n- If the predicted SQL is incorrect, start\nyour feedback with the phrase \"the predicted\nSQL is incorrect\". Then explain clearly why\nit is wrong (e.g., incorrect column, wrong\njoin, missing condition, misclassification of\nambiguity/unanswerability, etc.).\nAmbiguity Types\n1. vague-question: Extremely brief or\nunclear questions (e.g., \"BP?\", \"Patient?\")\n.\n2. vague-word: Questions containing vague\nwords (e.g., \"How many patients meet the *\nhigh risk* criteria?\").\n3. referential-ambiguity: Questions\ncontaining unclear referents (e.g., \"When\nwas the first time *it* happened in the\nmost recent hospital visit?\").\nUnanswerable Types\n1. small-talk: Casual queries unrelated to\nthe data (e.g., \"Did you grab coffee?\").\n18\n"}, {"page": 19, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\n2. out-of-scope: Requests that cannot be\nhandled by SQL (e.g., \"Can you *cluster*\npatients based on medication patterns?\").\n3. missing-column: References to non-\nexistent columns (e.g., asking for an *\naddress* field not present in the schema).\n# Important\n- Your role is to provide evaluation feedback\nonly, not to generate or fix SQL\n- Your feedback should be precise, grounded\nin the given schema and instruction.\n- Do not assume anything that is not\nexplicitly stated in the input.\n# Input\n- Database Schema\n- Question\n- Question Explanation\n- Evidence (SQL guideline)\n- Predicted SQL\n- SQL Explanation\n- Execution Result\n# Response Format\n{{\n\"feedback\": \"<detailed feedback on your\ndecision>\"\n}}\n# Input\nDatabase Schema:\n{database_schema}\nQuestion:\n{question}\nEvidence:\n{evidence}\nPredicted SQL:\n{sql}\nExecution Result:\n{exec}\nPrompt 8: Prompt used for the refinement\nstage in Multi-Turn-SelfRef.\nAll previous SQL queries were found to be\nproblematic. Based on the feedback, determine\nonce again whether the question is\nanswerable, ambiguous, or unanswerable. If it\nis answerable, correct the SQL.\nPredicted SQL:\n{sql}\nExecution Result:\n{exec}\n19\n"}, {"page": 20, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 8: Examples of different SQL error types from the answerable questions in the SCARE benchmark.\nEach row shows a specific error in the candidate query: T/C, the query references a wrong column name\nin the SELECT clause; J/G, the query incorrectly joins the wrong table; PV, the query uses an incorrect\nvalue for the long_title column; OL, the query unnecessarily uses a ROUND() function; OG, the query\nlogic is too simplistic and misses the core user intent.\nType\nUser Question\nGround-truth SQL\nCandidate SQL\nT/C\nWhat was the name of\nthe specimen test that\nwas last given to patient\n10027602 since 12/2100?\nSELECT microbiologyevents.\nspec_type_desc FROM\nmicrobiologyevents WHERE\nmicrobiologyevents.hadm_id IN\n( SELECT admissions.hadm_id\nFROM admissions WHERE\nadmissions.subject_id\n= 10027602 ) AND\nstrftime(’%Y-%m’,microbiologyevents.\ncharttime) >= ’2100-12’ ORDER BY\nmicrobiologyevents.charttime DESC\nLIMIT 1\nSELECT test_name FROM\nmicrobiologyevents WHERE subject_id\n= 10027602 AND datetime(charttime)\n>= datetime(’2100-12-31\n23:59:00’,’start of month’,’-0\nmonth’) ORDER BY charttime DESC\nLIMIT 1\nJ/G\nWhat was patient\n10015860’s last value\nof anion gap in 10/this\nyear?\nSELECT labevents.valuenum FROM\nlabevents WHERE labevents.hadm_id\nIN ( SELECT admissions.hadm_id\nFROM admissions WHERE\nadmissions.subject_id = 10015860\n) AND labevents.itemid IN\n( SELECT d_labitems.itemid\nFROM d_labitems WHERE\nd_labitems.label = ’anion gap’ ) AND\ndatetime(labevents.charttime,’start\nof year’) = datetime(’2100-12-31\n23:59:00’,’start of\nyear’,’-0 year’) AND\nstrftime(’%m’,labevents.charttime)\n= ’10’ ORDER BY labevents.charttime\nDESC LIMIT 1\nSELECT ce.valuenum FROM chartevents\nce JOIN d_items di ON ce.itemid\n= di.itemid WHERE ce.subject_id =\n10015860 AND di.label = ’anion gap’\nAND datetime(ce.charttime,’start\nof year’) = datetime(’2100-12-31\n23:59:00’,’start of year’,’-0 year’)\nAND strftime(’%m’, ce.charttime)\n= ’10’ ORDER BY ce.charttime DESC\nLIMIT 1\nPV\nHow many people were\ngiven procedure on sin-\ngle vessel two or more\ntimes?\nSELECT COUNT(T1.subject_id) FROM\n( SELECT admissions.subject_id,\nCOUNT(*) AS C1 FROM\nprocedures_icd JOIN admissions\nON procedures_icd.hadm_id\n= admissions.hadm_id WHERE\nprocedures_icd.icd_code = (\nSELECT d_icd_procedures.icd_code\nFROM d_icd_procedures WHERE\nd_icd_procedures.long_title =\n’procedure on single vessel’ ) GROUP\nBY admissions.subject_id ) AS T1\nWHERE T1.C1 >= 2\nSELECT COUNT(DISTINCT\nprocedures_icd.subject_id) FROM\nprocedures_icd JOIN d_icd_procedures\nON procedures_icd.icd_code =\nd_icd_procedures.icd_code WHERE\nd_icd_procedures.long_title\n= ’single vessel’ GROUP BY\nprocedures_icd.subject_id HAVING\nCOUNT(*) >= 2\nOL\nWhat was the dura-\ntion in days of patient\n10018845’s last stay in\nthe icu?\nSELECT strftime(’%J’,\nicustays.outtime) - strftime(’%J’,\nicustays.intime) FROM icustays\nWHERE icustays.hadm_id IN ( SELECT\nadmissions.hadm_id FROM admissions\nWHERE admissions.subject_id =\n10018845 ) AND icustays.outtime IS\nNOT NULL ORDER BY icustays.intime\nDESC LIMIT 1\nSELECT ROUND((JULIANDAY(outtime)\n- JULIANDAY(intime)), 0) AS\nduration_days FROM icustays WHERE\nsubject_id = 10018845 ORDER BY\nintime DESC LIMIT 1\nOG\nCan you show me the\ntop three most frequent\nlab tests in 2100?\nSELECT d_labitems.label FROM\nd_labitems WHERE d_labitems.itemid\nIN ( SELECT T1.itemid FROM ( SELECT\nlabevents.itemid, DENSE_RANK()\nOVER ( ORDER BY COUNT(*) DESC\n) AS C1 FROM labevents WHERE\nstrftime(’%Y’,labevents.charttime)\n= ’2100’ GROUP BY labevents.itemid )\nAS T1 WHERE T1.C1 <= 3 )\nSELECT T1.label, COUNT(*) as\nfrequency FROM d_labitems T1 INNER\nJOIN labevents T2 ON T1.itemid =\nT2.itemid\n20\n"}, {"page": 21, "text": "SQL Correction and Question Answerability Classification for Reliable EHR QA\nTable 9: Examples of diverse question types in SCARE.\nType\nUser Question\nReason\nanswerable\nWhat are the five commonly ordered medications for patients aged 60 or above?\nClear and answerable\nvague-question\nBP?\nToo short, unclear intent\nvague-word\nHow many current patients meet the high-risk criteria?\nTerm high-risk not defined\nambiguous-reference\nWhen was the first time it happened in this hospital visit?\nPronoun it is ambiguous\nsmall talk\nDid you grab coffee?\nNot relevant to EHR data\nout-of-scope\nCan you cluster patients based on their medication patterns?\nBeyond text-to-SQL tasks\nmissing-column\nWhat is the address of patient 10016742?\nAddress not stored in MIMIC-IV/eICU\n21\n"}]}