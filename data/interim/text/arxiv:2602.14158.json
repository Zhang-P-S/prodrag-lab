{"doc_id": "arxiv:2602.14158", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.14158.pdf", "meta": {"doc_id": "arxiv:2602.14158", "source": "arxiv", "arxiv_id": "2602.14158", "title": "A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing", "authors": ["Naeimeh Nourmohammadi", "Md Meem Hossain", "The Anh Han", "Safina Showkat Ara", "Zia Ush Shamszaman"], "published": "2026-02-15T14:17:27Z", "updated": "2026-02-15T14:17:27Z", "summary": "Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.14158v1", "url_pdf": "https://arxiv.org/pdf/2602.14158.pdf", "meta_path": "data/raw/arxiv/meta/2602.14158.json", "sha256": "354ede2c0c8149441c7e699654e4c295b56cf9afeb85e200afc0b45f2cf60894", "status": "ok", "fetched_at": "2026-02-18T02:19:16.390505+00:00"}, "pages": [{"page": 1, "text": "A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT,\nLLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical\nQuery Processing\nNaeimeh Nourmohammadi1, Md Meem Hossain1,2, The Anh Han1,2, Safina Showkat Ara3, and Zia Ush\nShamszaman1,2 ⋆\n1 Department of Computing and Games, Teesside University, Middlesbrough, United Kingdom Naominourr@gmail.com,\nmdmeemhossain@gmail.com,t.han@tees.ac.uk, z.shamszaman@tees.ac.uk\n2 Centre for Digital Innovation, Teesside University, Middlesbrough, United Kingdom\n3 Faculty of Business & Technology, University of Sunderland, Sunderland, United Kingdom\nsafina.ara@sunderland.ac.uk\nAbstract. Large language models (LLMs) show promise for healthcare question answering, but clinical use is\nlimited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose\na multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty\nestimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three\nrepresentative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+\nquestion–answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves\nthe strongest scores (ROUGE-1 0.536 ± 0.04; ROUGE-2 0.226 ± 0.03; BLEU 0.098 ± 0.018) and substantially\noutperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a mod-\nular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explana-\ntions, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement\nagent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered\nfor high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based\nuncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses.\nIn evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation\nreduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 sec-\nonds under the reported configuration. Overall, the results indicate that agent specialisation and verification\nlayers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based\nand bias-aware medical AI.\n1\nIntroduction\nWhile large language models (LLMs) have shown promise in healthcare applications, significant challenges remain to\nensure their reliability, evidence-based validation, and appropriate contextual response for clinical use [24]. Single-\nmodel approaches often lack robust verification mechanisms and may generate responses that, while technically\naccurate, lack proper clinical context or evidence backing [39]. These limitations present substantial barriers to\nthe adoption of LLM in critical healthcare settings where reliability and evidence-based practice are essential [26].\nThis research addresses these challenges through a novel multi-agent architecture that leverages the complementary\nstrengths of different LLM architectures. In particular, we examine three architectural approaches: GPT’s traditional\ntransformer architecture, LLaMA’s efficiency-focused design, and DeepSeek R1’s innovative optimisation framework,\nimplementing a hybrid system that overcomes the limitations of single-model implementations [39]. While we focus\non these three models for our implementation, this selection represents a diverse cross-section of available LLMs with\nvarying capabilities and architectures. Our multi-agent framework is designed to be model-agnostic and can be readily\nadapted to incorporate other models, such as Mistral, Gemini, and other emerging LLMs. The approach demonstrated\nhere establishes a methodology that extends beyond these specific models to the broader LLM ecosystem. Our\napproach integrates specialised agents for clinical reasoning validation, evidence retrieval, and response refinement\nto create a more robust and reliable medical AI system.\nOur research makes four primary contributions: (1) a comparative analysis of GPT, LLaMA, and DeepSeek R1\nfor medical applications, revealing key performance differences and architectural advantages; (2) a novel multi-agent\narchitecture combining chain-of-thought reasoning with evidence-based validation to enhance response reliability;\n(3) implementation of uncertainty quantification and bias detection mechanisms that provide transparency in AI-\ngenerated medical information; and (4) an optional human expert validation component that maintains professional\noversight while enabling efficient automation.\nThe multi-agent approach represents a significant advancement over single-model implementations by addressing\ncore limitations in medical AI [28]: the need for reasoning transparency, evidence integration, uncertainty communi-\ncation, and bias mitigation. Our implementation demonstrates that such an approach achieves an 87% accuracy for\nmedical queries with relevance scores of 0.80, offering meaningful improvements in the delivery of clinical information.\n⋆Corresponding author: Zia Ush Shamszaman, z.shamszaman@tees.ac.uk/z.u.shamszaman@gmail.com\narXiv:2602.14158v1  [cs.CL]  15 Feb 2026\n"}, {"page": 2, "text": "The adaptive response tailoring of the system adjusts the complexity of the content based on user expertise, ensuring\nappropriate communication at all levels of healthcare interaction.\nOur primary contribution lies in the architectural paradigm rather than individual model performance alone.\nWe demonstrate that systematic integration of specialised agents combining clinical reasoning validation, evidence\nretrieval, and response refinement produces more reliable medical AI outputs than single-model approaches. This\nmodular architecture enables flexible deployment across varying resource constraints whilst maintaining essential\nsafeguards for clinical use through uncertainty quantification and bias detection mechanisms. The approach represents\na shift from computational scaling towards architectural innovation for medical AI reliability.\nMoreover, to promote transparency and reproducibility, we release our fine-tuning and multi-agent system im-\nplementation as open-source. Before delving into our specific approach, it’s important to understand the unique\nchallenges that make healthcare applications of LLMs particularly demanding and why a multi-agent framework\noffers advantages in addressing these challenges.\n1.1\nResearch Objectives\nThis research pursues four primary objectives to address the critical limitations in current medical language model\nimplementations. Firstly, we aim to conduct a comprehensive comparative analysis of GPT, LLaMA, and DeepSeek\nR1 architectures for medical domain adaptation, examining their training dynamics, convergence patterns, and per-\nformance characteristics when applied to healthcare applications. Secondly, we seek to assess the resource efficiency\nand computational requirements of each architecture, analysing memory utilisation, processing demands, and opti-\nmisation strategies during both fine-tuning and inference phases for medical tasks. Thirdly, we endeavour to design\nand implement a novel multi-agent framework that strategically combines the complementary strengths of differ-\nent language model architectures, integrating evidence retrieval capabilities, uncertainty quantification mechanisms,\nand bias detection systems to enhance medical query processing reliability. Finally, we aim to evaluate the clinical\nperformance and practical deployment considerations of our multi-agent approach compared to traditional single-\nmodel implementations, assessing accuracy, relevance, response reliability, and integration potential for healthcare\norganisations seeking to implement language models in clinical settings.\n1.2\nChallenges in Healthcare Adaptation\nImportantly, applying LLMs in the healthcare domain requires extra attention. The medical field comprises numerous\nspecialities, each with its distinct terminology, abbreviations, and contextual meanings. These unique linguistic\npatterns often require targeted training data and model fine-tuning to ensure accurate interpretation and response.\nHealthcare applications demand unprecedented accuracy and reliability, as patient care may be compromised by\neven minor errors [32]. This necessitates robust verification mechanisms that single-model approaches often lack.\nAdditionally, patient privacy is a critical concern, which forces us to demonstrate extra caution as health data is\nprotected by privacy laws [31]. Furthermore, in healthcare, decision preferences are often established by an open and\nhonest exchange between AI and the patient, where the manufacturer engages with the doctor to provide the most\nvaluable treatment options.\nChallenges in Adapting General-Purpose LLMs to Medical Tasks With the rise of medical-based artificial\nintelligence, adapting general-purpose LLMs to healthcare applications has introduced a range of complex challenges\nthat go beyond standard domain adaptation. These challenges stem from the unique nature of medical data, the\ncritical demands of clinical settings, and strict privacy requirements. LLM applications in healthcare primarily aim\nto automate routine tasks to reduce staff workload, leverage existing clinical data within privacy constraints, and\nprovide decision support while respecting clinician and patient preferences.\nDataset Specialisation Issues The scarcity and fragmentation of healthcare data highlight the challenges of\napplying LLMs effectively in the medical domain. One of the main obstacles is data quality and representation;\ncommon medical conditions are sometimes under-represented or entirely missing from the datasets used for model\ntraining. This lack of comprehensive data introduces biases and limits the model’s predictive accuracy. Furthermore,\ninconsistencies in documentation standards and formats across healthcare institutions complicate data preparation\nand model training. Addressing these discrepancies is crucial to achieving standardised coverage and reliable format\nconsistency. Quality control is also of paramount importance to ensure the accuracy and reliability of medical training\ndata, which must be validated by medical experts. Medical data annotation presents its own set of challenges, as it\ndemands highly specialised expertise and significant resources. Notably, variability in annotations is a common issue,\nwith differing levels of agreement among annotators. This is especially problematic in complex medical scenarios where\ninterpretations may vary based on context. Additionally, healthcare data often includes both structured formats (like\nlaboratory results) and unstructured formats (like clinical notes). For LLMs to be effective, they must be capable of\nmanaging and integrating both types of information seamlessly.\n"}, {"page": 3, "text": "Technical Adaptation Challenges The adaptation of LLMs to healthcare applications presents significant techni-\ncal challenges that span multiple dimensions of implementation and training. Medical language models must handle\ncomplex technical terminology, intricate reasoning chains, and domain-specific knowledge while maintaining both\naccuracy and reliability. The primary challenge lies in effectively processing and representing medical terminology,\nwhich includes diverse jargon, complex nomenclature systems, and specialised vocabularies that are rarely present\nin general text corpora. These models must also manage context-dependent interpretations of medical terms, where\nthe same term may carry different implications across various medical specialities.\nTraining dynamics pose another significant challenge, as medical domain adaptation requires a careful balance be-\ntween maintaining general language understanding capabilities while acquiring specialised medical knowledge. Models\nmust learn to handle various document formats ranging from structured lab reports to unstructured clinical notes,\neach requiring different processing approaches. The prevention of catastrophic forgetting becomes crucial, as models\nneed to retain their base capabilities while adapting to medical tasks. Resource optimisation presents an ongoing\nchallenge, with different architectures offering varying trade-offs between memory efficiency and computational de-\nmands. Some approaches excel in memory utilisation but require significant computational resources, while others\nmay be computationally efficient but memory-intensive.\nFurthermore, medical data often involves long-context dependencies and complex temporal relationships that\nstandard attention mechanisms may struggle to capture effectively. The adaptation process must address these\nchallenges while ensuring the model can maintain coherence across extended medical narratives and complex clinical\nreasoning chains. These technical challenges are compounded by the need for robust validation mechanisms and the\nrequirement to handle out-of-distribution cases effectively, as medical applications often encounter rare or previously\nunseen conditions that the model must process reliably.\nPerformance and Reliability Requirements In LLMs, the performance and reliability are exceptionally high,\nwhich are crucial for successful clinical applications, especially healthcare. The accuracy rates requested by the clinical\nstandards are quite high, but sometimes, some tasks have a larger error tolerance. The necessity of confidence scores to\nbe accurate in predicting models becomes essential in clinical decision-making. Besides, the robustness of the model\nmust provide advanced out-of-distribution capabilities for unknown diseases to prevent the model from reaching\nunrecognisable medical situations. The prevalence of biased data in the machines can affect the care quality, causing\nseveral problems for the patients. Furthermore, being able to maintain logical correctness in medical reasoning is the\nmost significant thing to do because, otherwise, contradictions or gaps would have innumerable potential consequences\nfor patients.\nIntegration with Clinical Workflows The convenient integration of LLMs into clinical workflows brings along\npractical hurdles that transcend model performance. Requirements for real-time performance vary across different\nmedical activities, thereby necessitating careful optimisation of model inference. Designing the interface while con-\nsidering the needs and workflows of medical professionals is essential, with the end goal of presenting model outputs\nin an actionable and intuitive form. Adjusting existing clinical processes to accommodate model outputs demands\nappropriate consideration of workflow efficiency and user acceptance. Standards for validation vary in stringency in\nmedical settings; such validation requires extensive testing in real clinical environments and the development of rele-\nvant evaluation techniques. Continuous monitoring systems become indispensable to ensure performance evaluation\nand control.\n1.3\nEthical and Legal Considerations\nThe deployment of AI systems in healthcare environments raises significant ethical and regulatory challenges that\nmust be addressed through thoughtful system design and implementation.\nComprehensive Security Implementation In healthcare applications, privacy and security considerations are\nparamount. Our system implements comprehensive security measures at multiple levels. The optional human expert\nvalidation feature includes secure authentication protocols, ensuring that only authorised medical professionals can\naccess and validate responses.\nLegal Frameworks and Compliance Medical AI deployment must navigate complex regulatory landscapes across\njurisdictions. Our implementation incorporates data minimisation principles, processing only what is necessary for\neach specific task. The uncertainty quantification mechanisms provide important transparency for GDPR Article 14\nrequirements regarding meaningful information about algorithmic logic.\nBias Mitigation and Fairness Bias in medical AI systems can manifest in multiple forms, potentially exacer-\nbating existing healthcare disparities and compromising equitable care delivery. Our architecture implements several\ntargeted mechanisms to identify and mitigate potential biases, representing a significant advancement over existing\n"}, {"page": 4, "text": "approaches that often treat bias as a training data issue rather than an operational concern [13]. The lexical bias\ndetection component maintains a comprehensive database of potentially problematic medical terminology and phras-\ning, developed through collaboration with medical ethics researchers and diverse healthcare practitioners. When\npotentially biased language is detected, the system automatically triggers a reassessment before content delivery.\nBeyond lexical analysis, our implementation employs distributional bias detection to identify systematic differences\nin response patterns across demographic groups or medical conditions.\nOur bias mitigation strategy extends beyond detection to active intervention through the DeepSeek refinement\npipeline. When potential bias is identified, the system automatically reformulates responses to use more inclusive\nlanguage, provides balanced treatment options, and acknowledges potential knowledge limitations. This approach\nensures that bias mitigation operates as a continuous process rather than a one-time training consideration. Most\nimportantly, our architecture’s optional human expert validation component provides a critical safeguard against\nundetected biases, allowing professional oversight, particularly for novel or complex cases. This collaborative ap-\nproach acknowledges the limitations of automated bias detection while providing a practical framework for ensuring\nequitable and fair medical information delivery. Through these comprehensive security and fairness mechanisms, our\nmulti-agent architecture addresses critical ethical considerations in medical AI deployment. By integrating these con-\nsiderations directly into the system’s operational design rather than treating them as external constraints, we establish\na framework for responsible medical AI that balances technical performance with essential ethical requirements.\n2\nBackground and Related Work\nThis section reviews the historical development of language models in healthcare contexts and examines the archi-\ntectural characteristics of current general-purpose models with relevance to medical applications.\n2.1\nEvolution of Language Models in Healthcare\nThe application of language models in healthcare has undergone a remarkable transformation, evolving from basic\nrule-based systems to today’s sophisticated neural architectures. While early systems focused primarily on structured\ndata processing, contemporary approaches have moved toward more complex, multi-agent architectures that can\nhandle nuanced medical reasoning and evidence-based validation [4]. The development landscape encompasses two\nmain trajectories: first, the adaptation of general-purpose models such as GPT, LLaMA, and DeepSeek to medical\ntasks, and second, specialised models like BioBERT and ClinicalBERT designed solely for healthcare applications.\nRecent innovations have particularly focused on the integration of evidence-based reasoning and uncertainty quan-\ntification. The emergence of chain-of-thought prompting techniques has enabled more transparent clinical reasoning\nprocesses, while advances in retrieval-augmented generation have facilitated the real-time integration of medical\nevidence into model responses. These developments have laid the groundwork for more sophisticated multi-agent\narchitectures that can combine multiple specialised components for improved medical decision support.\n2.2\nGeneral-Purpose Language Models\nGPT (Generative Pre-trained Transformers) has played a significant role in advancing language models, offering\nbroad adaptability in the medical domain. As shown in Figure 1, GPT relies on transformer decoder architecture with\nlearned positional embeddings and layer normalisation [41], employing causal self-attention mechanisms and GeLU\nactivation functions [14]. While GPT has demonstrated strong capabilities in clinical applications, its effectiveness\nin specialised medical domains often requires extensive fine-tuning or domain adaptation strategies [38].\nRecent advancements in medical AI have introduced more specialised models like LLaMA and DeepSeek, which\nincorporate optimisations that enhance medical domain adaptation. LLaMA integrates RMSNorm for improved\ntraining stability, Flash Attention to reduce memory constraints, and Rotary Positional Embeddings (RoPE) [35],\nmaking it more efficient for handling large medical texts (Figure 1, right) [40]. DeepSeek further refines parameter-\nefficient fine-tuning techniques, enabling better processing of complex medical terminology and reasoning tasks.\nDeepSeek R1, a more recent addition to general-purpose language models, introduces a revolutionary architectural\napproach through its Mixture-of-Experts (MoE), an architecture that uses a routing mechanism to selectively activate\nonly a subset of the model’s parameters for each input) framework and multi-head latent attention design [9].\nThe architecture (Figure 2) employs a novel Router Network that selectively activates specific experts while\nkeeping others dormant, enabling efficient processing by utilising only 21B of 236B parameters per token. This selective\nactivation mechanism, combined with an advanced multi-head latent attention system, allows the model to handle\nextensive context windows of up to 128,000 tokens. The model’s distinct features include expert aggregation, efficient\nKV cache implementation, latent vector compression, and sophisticated attention processing mechanisms. These\narchitectural choices, enhanced by the Unsloth framework and 4-bit quantisation, enable DeepSeek R1 to achieve\nsuperior performance in medical tasks while maintaining computational efficiency. Research has shown that while\nstandard GPT models usually require substantial training data for medical domain adaptation, newer architectures\nlike LLaMA and DeepSeek can achieve superior performance with more efficient training approaches [6]. This efficiency\n"}, {"page": 5, "text": "gain becomes particularly important in multi-agent systems where multiple model components need to work together\nseamlessly.\nIn the domain-specific realm, several specialised language models have been developed. BioBERT adapts to\nbiomedical applications with pre-training on PubMed abstracts and full-text articles, implementing expanded vocab-\nulary for biomedical terms and modified tokenisation [19]. ClinicalBERT focuses on clinical notes from MIMIC-III,\nfeaturing specialised vocabulary and modified attention patterns designed for clinical document structure [16] [2].\nPubMedBERT takes a different approach by training from scratch on biomedical literature, employing custom vo-\ncabulary and domain-adaptive pre-training [12]).\nFig. 1. Transformer Architecture Comparison: (Left) GPT decoder-based architecture processes input through learned posi-\ntional embeddings combined with token embeddings, applies masked multi-head self-attention (preventing attention to future\ntokens), followed by feedforward layers with GeLU activation. Layer normalisation (epsilon 1e-5) ensures training stability.\n(Right) LLaMA architecture employs RMSNorm for improved stability, Rotary Positional Embeddings (RoPE) for efficient\nposition encoding, Flash Attention for memory optimization, and SwiGLU activation, optimised for efficiency in large-scale\nmedical language modelling [41,27].\nIn healthcare multi-agent architectures, IBM Watson Health represented an early attempt at applying natural\nlanguage processing (NLP) to healthcare decision support [18], showing promise in oncology domains but limited\ngeneralisability due to its rule-based components. Google’s Med-PaLM 2 [37]achieved expert-level performance on\nmedical licensing exams but employs a primarily monolithic approach with limited mechanisms for evidence inte-\ngration. Our multi-agent architecture builds upon these foundations while addressing the key limitations of existing\napproaches. Unlike Watson’s predominantly rule-based components, our system employs fine-tuned LLMs with so-\nphisticated uncertainty quantification mechanisms. In contrast to Med-PaLM’s monolithic approach, our architecture\nexplicitly separates clinical reasoning, evidence retrieval, and response refinement into distinct agents, enhancing both\nexplainability and performance. Most distinctively, our architecture incorporates an optional human expert valida-\ntion component, providing a flexible framework for professional oversight that can be calibrated based on query\ncomplexity and clinical criticality.\nA significant challenge in healthcare applications is managing complex medical terminology, which includes di-\nverse jargon from Latin and Greek, complex nomenclature systems, context-dependent abbreviations, and interdis-\nciplinary semantic complexity. Privacy and security considerations are also paramount, addressed through compre-\nhensive measures including secure API connections, encrypted data transmissions, HIPAA compliance, automated\nde-identification processes, and secure authentication protocols for human expert validation.\n"}, {"page": 6, "text": "Fig. 2. DeepSeek Architecture Overview: This figure illustrates the DeepSeek model’s Mixture-of-Experts (MoE) framework,\nwhere a Router Network selectively activates a subset of expert models per token, optimising computational efficiency[11].\n2.3\nPrivacy and Security Considerations in Medical AI\nIn healthcare applications, privacy and security considerations are paramount. Our system implements comprehen-\nsive security measures at multiple levels. The Evidence Retrieval System employs secure API connections when\naccessing medical databases, with all data transmissions encrypted using industry-standard protocols. Patient data\nhandling follows strict HIPAA compliance guidelines, with automated de-identification processes integrated into the\nsystem’s workflow. The architecture incorporates an optional human expert validation feature that includes secure\nauthentication protocols, ensuring that only authorised medical professionals can access and validate responses.\nThroughout the implementation, we prioritise data protection by employing standard security practices relevant\nto medical information systems. The multi-agent architecture keeps privacy considerations at the forefront, with each\ncomponent designed to process only necessary information while maintaining appropriate security boundaries. Our\napproach acknowledges the sensitive nature of healthcare data and implements safeguards aligned with established\nmedical privacy standards. The system’s design reflects current best practices for securing medical AI applications,\nbalancing the need for accessible clinical information with rigorous data protection requirements. By integrating se-\ncurity considerations into the fundamental architecture rather than as an afterthought, our implementation addresses\nkey privacy concerns that typically arise in medical AI deployments.\n2.4\nFine-tuning Methods\nAdapting general language models to the medical domain requires specialised training approaches that balance ex-\npertise with computational efficiency. Various fine-tuning methods have evolved to address this challenge. The imple-\nmentation described demonstrates a novel approach combining traditional fine-tuning with specialised optimisation\ntechniques through the Unsloth framework. Parameter-efficient fine-tuning, particularly using LoRA techniques, has\nproven crucial in medical applications, maintaining high performance standards whilst addressing computational\nhurdles[15]. Selective layer fine-tuning leverages the understanding that different layers in language models serve\ndistinct functions, with knowledge hierarchically organised across transformer model layers[35]. This approach re-\nduces computational requirements without sacrificing performance[12]. Soft prompt tuning has established a foun-\ndation for practical model adaptation in medicine by using learnable continuous vectors at the beginning of input\nsequences, achieving performance comparable to complete fine-tuning whilst preserving the model’s general capa-\nbilities. Domain-adaptive pre-training employs an intermediate pre-training phase on domain-specific data before\ntask-specific fine-tuning, showing impressive results with models like LLaMA on specialised medical tasks. Finally,\n"}, {"page": 7, "text": "hybrid fine-tuning strategies integrate parameter-efficient techniques with prompt-based approaches to exploit the\nstrengths of both paradigms, achieving strong performance without substantial computational overhead [21]. Con-\ncurrently, parameter-efficient tuning methods, some of which were examined by [30], [29] and [15], allow their models\nto adapt to new tasks using a minimum number of parameter updates. These collective advances suggest promising\ndirections for addressing domain-specific challenges in medical natural language processing.\n3\nMethodology\nOur methodological approach consists of two distinct phases: initial fine-tuning of Large Language Models (GPT,\nLLaMA, and DeepSeek R1) for medical domain adaptation, followed by the development of a multi-agent system\nleveraging the most effective models. This methodology addresses the significant challenges in medical information\nprocessing while capitalising on recent advances in language model architectures.\nIn the first phase, we conducted fine-tuning experiments across all three model architectures. While GPT provided\nvaluable baseline performance with stable training dynamics, LLaMA and DeepSeek R1 demonstrated superior\nperformance in medical domain tasks. This comparative analysis informed our subsequent architectural choices, with\nLLaMA and DeepSeek R1 selected for final system implementation due to their enhanced performance metrics and\narchitectural advantages.\nThe second phase focused on developing a multi-agent architecture with specialised components for clinical rea-\nsoning, evidence retrieval, and response refinement. Our approach incorporates three critical areas: advanced data\npreprocessing for medical content, architectural integration of multiple specialised agents, and response generation\nwith validation mechanisms. Throughout implementation, we prioritised both the accurate handling of complex med-\nical terminology and efficient resource utilisation through various optimisation techniques. The system employs an\nEvidence Retrieval Agent that interfaces with PubMed to gather real-time medical evidence and a Clinical Reason-\ning Agent that implements chain-of-thought prompting to ensure logical consistency in medical explanations. These\ncomponents work in concert with a DeepSeek refinement pipeline that enhances response quality through specialised\nparameter-efficient fine-tuning. We further incorporate uncertainty detection through Monte Carlo dropout sampling\nand bias detection through lexical and sentiment analysis to ensure response reliability. The subsequent sections detail\nour implementation process, beginning with data preparation for fine-tuning, proceeding through model architecture\nmodifications, and concluding with the development of our multi-agent system for medical query processing.\n3.1\nFine-Tuning Strategy\nDataset Description This study employed multiple medical question-answering datasets derived from the MedQuAD\n(Medical Question Answering Dataset) repository [1], comprising over 20,000 medical question-answer pairs sourced\nfrom twelve NIH websites. The datasets were obtained from Kaggle and encompass ten domain-specific collections\ncovering diverse medical specialities. These include cancer-focused questions and answers, diabetes and digestive and\nkidney diseases, disease control and prevention information, genetic and rare diseases, heart, lung, and blood condi-\ntions, general medical queries, neurological disorders and stroke, miscellaneous medical topics, senior health concerns,\nand growth hormone receptor-related endocrine queries. Each dataset entry contains structured question-answer pairs\nwith additional annotations specifying question type, clinical focus, synonyms, and UMLS (Unified Medical Language\nSystem) Concept Unique Identifiers where available, facilitating precise medical entity recognition and categorisation\nacross different healthcare domains.\nWhile MedQuAD provides comprehensive coverage across multiple medical domains, we acknowledge several\nlimitations. The dataset primarily focuses on general medical knowledge and common conditions, with potential\nunder-representation of rare diseases and emerging therapies. Additionally, the question-answer pairs are derived\nfrom patient-oriented NIH resources, which may not fully capture the complexity of specialist clinical reasoning or\nnuanced diagnostic scenarios encountered in advanced practice settings. Future work should validate our approach\non broader clinical benchmarks, including case-based reasoning datasets and real-world clinical documentation.\nData Preprocessing Following dataset compilation, we implemented a comprehensive preprocessing pipeline to\nensure data quality and compatibility across model architectures. At the heart of the implementation process is a\nsharded data loading system which is specific to large-scale medical datasets and can work within memory constraints.\nThe system with sharding brings memory management and data access efficiency as a gain, it keeps the support for\ndistributed processing as the main feature and produces sequence-length weighing that is consistently the same for\nboth model architectures. Figure 3 illustrates this comprehensive data processing pipeline. This capability enables\nthe application to scale across multiple processing units while maintaining data integrity and processing efficiency.\nTokenisation Approaches Our implementation employs distinct tokenisation strategies for each architecture to\noptimise medical text processing. GPT utilises the tiktoken library with a vocabulary size of 50,257 tokens and im-\nplements Byte-Pair Encoding (BPE) for tokenisation. This approach tackles medical terminology through subword\ndecomposition, whereby complex medical terms are broken down into meaningful subunits. For example, \"cardiomy-\nopathy\" might be tokenised into components like \"cardio,\" \"myo,\" and \"pathy,\" enabling the model to understand\n"}, {"page": 8, "text": "Fig. 3.\nData Processing Workflow: Medical question-answer datasets undergo memory-efficient loading in chunks (shards),\nfollowed by tokenisation using different methods for each model (GPT uses Byte-Pair Encoding; LLaMA and DeepSeek use\nSentencePiece). Text sequences are padded to consistent lengths, then organised into training batches. This pipeline ensures\nefficient processing of large medical datasets whilst maintaining compatibility across all three model architectures.\nnovel medical terms by recognising common prefixes, roots, and suffixes even when specific combinations weren’t\npresent in training data. The GPT tokenisation particularly excels at handling Latin and Greek-derived medical\nterminology due to its subword approach, though it occasionally struggles with extremely rare medical acronyms and\nhighly specialised terminology that doesn’t follow standard morphological patterns. This approach provides effective\nhandling of medical terminology through subword tokenisation and uses specialised tokens such as <|endoftext|> for\nsequence boundaries.\nLLaMA implements a custom tokeniser built on the SentencePiece framework, offering a different approach to\nmedical text processing. The LLaMA tokeniser employs unigram language model-based tokenisation, which demon-\nstrates superior handling of domain-specific medical vocabulary compared to BPE approaches. This is particularly\nevident in how it processes complex medical terminology with greater coherence, keeping meaningful medical units\ntogether more consistently. Our analysis showed that LLaMA’s tokenisation preserved the semantic integrity of tech-\nnical terms like \"electroencephalography\" and \"hepatosplenomegaly\" better than GPT’s approach. The tokeniser\nwas specially configured with a larger vocabulary allocation for medical terms, allowing direct representation of com-\nmon medical entities without excessive decomposition. This implementation efficiently handles medical vocabulary\nand explicitly manages BOS and EOS tokens. The system demonstrates particular strength in processing medical\nterminology, creating nuanced representations while maintaining consistent sequence lengths.\nDeepSeek R1’s approach to medical terminology represents a significant advancement in our implementation,\nbuilding upon previous architectures with specialised adaptations. Its tokenisation mechanism leverages the Senten-\ncePiece framework while incorporating the Unsloth optimisation pipeline, which provides enhanced processing of\nspecialised medical vocabulary. By retaining the hierarchical structure of medical terminology, DeepSeek R1 achieves\na more efficient representation of complex medical terms compared to GPT. The model demonstrates particular\nstrength in handling technical medical terminology, maintaining semantic coherence for terms like \"electrocardiogra-\nphy\" and \"hepatocellular carcinoma\" that might otherwise be fragmented. The 4-bit quantisation further optimises\nprocessing while preserving terminological precision, enabling the model to maintain high performance across various\nmedical specialities despite reduced computational requirements. This efficient tokenisation approach contributes\nsignificantly to DeepSeek R1’s superior performance metrics in medical domain tasks. Figure 4 demonstrates how the\nsame medical term (e.g., \"cardiomyopathy\") would be tokenised differently by GPT (BPE), LLaMA (SentencePiece),\nand DeepSeek R1 (SentencePiece with Unsloth optimisation).\nFig. 4. Comparison of tokenisation approaches across architectures\nSequence Length Management A careful balance is maintained in the sequence length management system\nbetween model capacity and computational efficiency. The architectures have different sequence length capabilities:\nGPT uses a fixed length of 1024 tokens, while LLaMA and DeepSeek R1 support up to 2048 tokens. DeepSeek\nR1 specifically leverages the Unsloth framework for efficient sequence processing, using a batch size of 2 with four\ngradient accumulation steps. The batch processing system for GPT and LLaMA utilises a basic batch size of 16 with\na gradient accumulation of 8 steps, processing 16,384 tokens per batch while maintaining memory efficiency. This\napproach enables consistent performance across varying medical text lengths.\nQuality Assurance Measures Our data preprocessing pipeline implements comprehensive quality assurance mech-\nanisms throughout the processing workflow. The system begins with header validation to ensure data integrity through\nversion compatibility checks and format verification. We maintain sequence integrity by validating token numerical\n"}, {"page": 9, "text": "values and boundary correctness, with explicit verification that each data shard contains sufficient tokens for the\nrequested batch size and sequence length. Memory management is handled through an advanced system utilising\nshard loading and position tracking, ensuring efficient resource utilisation while maintaining processing consistency.\nThe handling of complex medical terminology requires specialised approaches across different architectures. GPT\nemploys subword tokenization for unfamiliar medical terms, providing effective handling of technical terminology\nand abbreviations through token assignment for common medical entities. LLaMA enhances this capability through\nits SentencePiece-based tokenization, which combines rare term handling with specialised embeddings for medical\nentities. This approach enables the processing of a wide range of medical terminology while maintaining system effi-\nciency. DeepSeek R1 further extends these capabilities through its optimised implementation, providing particularly\nstrong performance in specialised medical vocabulary processing.\nPerformance Optimisation Strategies Our implementation incorporates several critical optimisation techniques\nto maximise efficiency while maintaining high data quality standards. Memory efficiency is achieved through care-\nful tensor management and type optimisation, with precise control over data precision and device placement. The\nsystem implements computational optimisations through high-precision matrix multiplication and strategic resource\nallocation across processing units. These techniques are particularly important for medical applications, where both\naccuracy and computational efficiency are essential. Our multi-agent system extends these optimisations through\ncontext managers for GPU memory handling and automatic resource clean-up procedures, ensuring consistent per-\nformance even under complex processing requirements.\n3.2\nTraining Process Implementation\nOur fine-tuning approach employs architecture-specific optimisation strategies that carefully balance adaptation\nspeed with model stability, accounting for the unique characteristics of each language model architecture.\nLearning Rate Strategy and Optimisation The fine-tuning methodology employs carefully calibrated learning\nrate strategies to balance model adaptation and training stability across architectures. For the GPT architecture, we\nimplement a maximum learning rate of 6×10−4 with a minimum set at 10% (6×10−5) based on experimental evidence\nshowing that moderately higher learning rates effectively enable knowledge acquisition while maintaining stability.\nThe training schedule begins with a linear warm-up period of 715 steps, followed by cosine decay to the minimum over\n19,073 steps, approximately one epoch on our medical dataset. This careful timing prevents catastrophic forgetting,\nensuring the model maintains its general language understanding while acquiring specialised medical knowledge.\nThe LLaMA implementation takes a more conservative approach with a learning rate of 1e-5, reflecting the\narchitecture’s parameter sensitivity. While this lower rate necessitates more training steps to reach convergence, it\nenables the model to preserve its intricate attention mechanisms and specialised embeddings during medical domain\nadaptation. For DeepSeek R1, we leverage the Unsloth framework with an intermediate learning rate of 2e-4 and\nlinear scheduling through the AdamW 8-bit optimiser. This configuration, combined with a micro-batch size of 2 and\n4 gradient accumulation steps, optimises both performance and memory efficiency. All implementations benefit from\nthe AdamW optimiser’s ability to handle sparse gradients typical in medical text processing while providing effective\nweight decay regularisation.\nWeight Decay and Regularisation Weight decay configuration implements controlled regularisation across archi-\ntectures, with a value of 0.1 selectively applied to specific parameter groups based on dimensionality. This strategy\nenables fine-grained control over regularisation, allowing specialised medical weights to adapt while maintaining\noverall model robustness.\ndecay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\nnodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\noptim_groups = [\n{’params’: decay_params, ’weight_decay’: weight_decay},\n{’params’: nodecay_params, ’weight_decay’: 0.0}\nThe selective application proves particularly valuable for medical domain adaptation as it preserves general language\nunderstanding while enabling specialised terminology acquisition. Parameter groups with different dimensionality\nreceive varied regularisation treatment, with careful attention to maintaining the balance between adaptation and\noverfitting prevention.\nGradient Accumulation and Batch Processing To address memory constraints while maintaining effective\ntraining, we implement a sophisticated gradient accumulation approach across architectures. For GPT and LLaMA,\nthe system processes micro-batches of 16 samples and accumulates gradients through 8 steps, achieving an effective\nbatch size of 16,384 tokens without exceeding memory limitations. This method enables robust training even on\nhardware with limited resources while capturing the benefits of large-batch training.\n"}, {"page": 10, "text": "AccumulationCounts =\ntotal batch size\nB × T × ddp-world-size,\nwhere B is the micro-batch size per GPU, T is the sequence length, and ddp-world-size is the number of distributed\ndata parallel (DDP) processes.\nAdditionally, we leverage learning rate scheduling to enhance training stability and convergence speed. The learn-\ning rate follows an increasing schedule in the early training phase before gradually decaying, as depicted in Figure 5.\nThis strategy helps GPT and LLaMA achieve efficient optimisation while minimising instability during large-batch\ntraining. The impact of gradient accumulation on loss trends is illustrated in Figure 5, where both GPT and LLaMA\nexhibit a smooth decline in loss. The comparison highlights the benefits of our gradient accumulation strategy in ac-\ncelerating convergence while maintaining training stability. The coordinated interaction of these parameters ensures\nefficient resource utilisation while preserving training stability, a critical consideration for medical domain adaptation.\nDeepSeek R1, through its optimised architecture, achieves comparable performance with smaller batch sizes, using\ntwo samples per micro-batch with four gradient accumulation steps.\nFig. 5. Training Dynamics of GPT and LLaMA: The left plot illustrates the learning rate schedule across training steps for\nGPT (blue) and LLaMA (red), showing a peak followed by a gradual decrease.\n3.3\nArchitecture-Specific Optimisations\nEach architecture employs distinct optimisation strategies to enhance performance in medical tasks. The GPT design\nuses LayerNorm with an epsilon value of 1e-5 to ensure training stability, complemented by causal masking for\nautoregressive generation and a carefully crafted learning schedule combining warmup periods with cosine decay.\nLLaMA leverages architectural innovations, including RMSNorm with an epsilon value of 1e-6, Flash Attention\nfor efficient computation, and a KV cache system that significantly improves inference performance [8]. Rotary\npositional embeddings enable the effective handling of variable-length medical texts, a critical capability for processing\ndiverse medical documentation. DeepSeek R1 extends these optimisations through the Unsloth framework and 4-bit\nquantisation, achieving superior performance while minimising resource requirements. These architectural choices\nresult in distinct performance profiles across medical tasks, with DeepSeek R1 demonstrating particularly strong\nresults (ROUGE-1: 0.536, ROUGE-2: 0.226) compared to other architectures. The architectural differences between\nthese models are summarised in Table 1.\nTable 1. Comparison of GPT, LLaMA, and DeepSeek R1\nFeature\nGPT Model\nLLaMA Model\nDeepSeek R1 Model\nNormalisation\nLayerNorm\nRMSNorm\nRMSNorm\nPositional Embeddings Learnable\nRoPE\nRoPE\nAttention Mechanism\nStandard Attention\nFlash Attention\nFlash Attention\nFeedforward Activation GeLU\nSwiGLU\nSwiGLU\nInference Optimisation None\nKV Caching\nUnsloth + KV Cache\nTraining Efficiency\nModerate\nHighly\noptimised\nfor\ndis-\ntributed setups\nHighly optimised with 4-bit\nquant\n"}, {"page": 11, "text": "3.4\nSelective Fine-tuning and Hyperparameter Optimisation\nOur comparative analysis reveals that hyperparameter selection significantly impacts medical domain adaptation\nacross architectures. While we covered basic learning rate strategies in the previous section, a deeper examination\nshows distinct optimisation patterns with meaningful performance implications. The GPT implementation’s relatively\nhigh learning rate (6×10−4 to 6×10−5) enables more aggressive knowledge acquisition, while LLaMA’s conservative\napproach (1e-5) favours stability over speed. DeepSeek R1’s intermediate strategy (2e-4) with linear scheduling\nthrough the Unsloth framework achieves an optimal balance between adaptation speed and parameter stability.\nBeyond learning rates, we discovered that selective fine-tuning approaches yield superior results compared to full\nmodel adaptation. The GPT architecture benefits from parameter-specific optimisation, with decay rates calibrated\nto parameter dimensionality. This sophisticated approach prioritises the adaptation of key parameter groups while\nminimising disruption to others, enabling efficient medical knowledge acquisition even with limited domain data.\nThe LLaMA architecture extends this concept by specifically targeting normalisation layers and query projections,\ndemonstrating that architectural complexity requires targeted adaptation rather than uniform parameter updates\n[25].\nDeepSeek R1’s implementation represents the most advanced approach, combining specialised LoRA targeting\nof key projection layers with 4-bit quantisation. This highly selective adaptation strategy achieves remarkable per-\nformance improvements (ROUGE-1: 0.536, ROUGE-2: 0.226) while maintaining minimal computational overhead.\nThese findings suggest that the future of medical domain adaptation lies not in comprehensive model retraining but\nin increasingly sophisticated selective adaptation of architecturally significant components.\n3.5\nMulti-agent System Implementation\nSystem Architecture Overview Following the fine-tuning experiments across different language model architec-\ntures, we developed a comprehensive multi-agent system specifically designed for medical domain applications. Our\nmulti-agent architecture represents a significant advancement in medical language model implementation through\nits modular, component-based design. As illustrated in Figure 6, the system comprises three primary components\nworking in concert to process medical queries: a Clinical Reasoning Validator, an Evidence Retrieval System, and an\noptional Human Expert Review component. These specialised agents coordinate through a central orchestrator that\nmanages information flow [28] and processing sequences to deliver reliable, evidence-based medical responses.\nThe architecture follows a sequential processing pipeline whilst allowing for dynamic feedback loops based on\nuncertainty detection. Initial queries are first processed by the uncertainty detection mechanism, which determines\nwhether additional evidence or reasoning is required. The Clinical Reasoning Validator then applies chain-of-thought\nprompting through the fine-tuned LLaMA model to generate structured medical reasoning. Concurrently, the Evi-\ndence Retrieval System gathers relevant medical literature from authoritative sources, which is then integrated into\nthe reasoning process.\nThe orchestrator monitors performance metrics throughout processing, including relevance scores and uncertainty\nmeasures. When uncertainty thresholds are exceeded, the system automatically triggers additional verification steps,\nwhich may include re-querying with enhanced evidence context or, when available, initiating human expert review.\nThe architecture’s bias detection module provides an additional safeguard, identifying potentially problematic content\nbefore delivery to users. This multi-layered approach ensures responses maintain appropriate medical neutrality whilst\nproviding transparent confidence indicators.\nA distinctive feature of our implementation is its memory-efficient design, with models transitioning between\nGPU and CPU resources as needed through context managers. This approach optimises resource utilisation, which\nis particularly important in resource-constrained healthcare environments. The architecture’s modular design fur-\nther allows for flexible deployment configurations based on available computational resources and specific clinical\nrequirements.\nClinical Reasoning Validator (Agent 1) The Clinical Reasoning Validator represents a cornerstone of our\nmulti-agent architecture, implementing an innovative approach to medical query processing through chain-of-thought\nreasoning. This agent employs the fine-tuned LLaMA model to produce step-by-step medical analyses that explicitly\ndemonstrate the reasoning process, enhancing the transparency and verifiability of generated responses.\nWe implement this validator through a structured prompting system that guides the model through a systematic\nanalytical process:\nprompt = f\"Question: {question}\\n\"\nif evidence:\nprompt += f\"Evidence:\\n{evidence}\\n\"\nprompt += \"\"\"Let’s reason through the problem step by step:\nStep 1: Analyse the question and evidence\nStep 2: Identify key medical concepts\nStep 3: Form a structured response\n"}, {"page": 12, "text": "Fig. 6. Multi-Agent Response Generation Workflow: Medical queries flow through three specialised components working to-\ngether. First, the Evidence Retrieval Agent searches PubMed for relevant medical research. Second, the Clinical Reasoning\nAgent generates structured medical explanations using the fine-tuned LLaMA model. Third, the DeepSeek Refinement Agent\nenhances response quality. Throughout this process, uncertainty detection (measuring confidence) and bias detection (identi-\nfying potentially problematic language) monitor response reliability, with optional expert review available for high-uncertainty\ncases.\nBased on the above steps, here is the final answer:\\n\"\"\"\nThis explicit reasoning structure serves multiple purposes. First, it enhances response accuracy by encouraging a\nthorough analysis of medical concepts rather than immediate answer generation. Second, it provides transparency\nby making reasoning steps visible, allowing users to understand how conclusions were reached. Third, it facilitates\nerror detection by exposing potential logical inconsistencies in the reasoning chain.\nWe incorporate few-shot prompting with selected medical examples, achieving significantly higher accuracy than\nzero-shot approaches. The validator further implements automatic error recovery mechanisms, monitoring for incom-\nplete reasoning chains and regenerating responses when necessary. Performance analysis demonstrates the Clinical\nReasoning Validator achieves 87% accuracy on medical queries with high relevance scores (0.80), indicating strong\nalignment between questions and generated responses.\nEvidence Retrieval System (Agent 2) The Evidence Retrieval System enhances response reliability through\nthe real-time integration of medical literature from authoritative sources [7]. This agent implements a sophisticated\nretrieval-augmented generation approach that grounds responses in current medical evidence, addressing a critical\nlimitation of traditional language models that rely solely on pre-trained knowledge.\nThe Evidence Retrieval System interfaces with PubMed through NCBI’s E-utilities API, specifically using the\nESearch service to identify relevant medical literature. The system sends user queries directly as search terms to\nthe PubMed database via API calls to the NCBI ESearch endpoint, retrieving up to three relevant PubMed article\nidentifiers per query. The implementation processes JSON responses from the ESearch API and returns lists of\nPubMed articles that correspond to relevant literature, with fallback messaging when no articles are found. Retrieved\nPubMed articles are formatted with sequential reference numbering to enable clear citation tracking within generated\nresponses. The implementation includes robust error handling for network failures and API limitations, falling back\nto model-generated responses when evidence retrieval is unsuccessful.\nOur evaluation demonstrates that evidence-augmented responses show significantly lower uncertainty scores (per-\nplexity: 4.13) compared to base responses, confirming the value of evidence integration in enhancing response con-\nfidence. Users also reported higher satisfaction with evidence-backed responses, particularly appreciating the trans-\nparent citation of authoritative sources.\nThe Evidence Retrieval System’s implementation enables real-time literature access without requiring constant\nretraining of the underlying models, ensuring that responses reflect current medical knowledge. This approach is\nparticularly valuable in rapidly evolving medical fields where pre-trained knowledge may quickly become outdated.\nHuman Expert Integration (Agent 3) The architecture incorporates a flexible human expert integration mech-\nanism, providing an optional validation layer for critical medical information. The Human Expert Review Agent\nimplements an interactive review process.\n"}, {"page": 13, "text": "This implementation enables the seamless integration of expert feedback while maintaining system automation\nfor routine queries. The expert review process includes structured feedback collection, response refinement through\nDeepSeek, and additional evidence augmentation when necessary. The implementation includes sophisticated uncer-\ntainty reassessment after refinement, ensuring that expert-guided improvements maintain high-quality standards.\nAgent Orchestration and System Integration The complete system is coordinated through an Agent Orchestra-\ntor that manages interaction between components and ensures cohesive operation. The orchestrator implementation\nhandles query processing, evidence retrieval, reasoning generation, and response refinement. The implementation\nincludes comprehensive error handling, detailed logging, and result persistence through JSON serialisation with\ntimestamped filenames. This ensures traceability and enables system performance analysis across multiple query\nsessions.\nThrough this sophisticated implementation, our multi-agent architecture provides reliable, evidence-based med-\nical information with appropriate uncertainty indication and bias mitigation, which significantly advances medical\nlanguage model applications.\nMemory Management and Optimisation Our implementation incorporates sophisticated memory management\nstrategies to ensure efficient operation across varying hardware configurations. We implement context-based GPU\nmemory handling through a custom context manager that automatically transitions models between CPU and GPU\nas needed:\n@contextmanager\ndef use_gpu(model):\nmodel.to(\"cuda\")\ntry:\nyield\nfinally:\nmodel.to(\"cpu\")\nclear_gpu_memory()\nThis approach, combined with explicit GPU memory clean-up procedures, prevents memory leaks and optimises\nresource utilisation across processing stages. For model loading, we implement memory-efficient techniques, includ-\ning map-location specification and weights-only loading. During inference, we leverage PyTorch’s automatic mixed\nprecision (AMP) capabilities through torch.cuda.amp.autocast() contexts, enabling half-precision computation where\nappropriate while maintaining numerical stability for critical operations.\nThe system’s batch processing implementation balances throughput with memory constraints, dynamically ad-\njusting batch configurations based on available resources. For DeepSeek R1 integration, we implement additional\noptimisations, including device mapping controls and low CPU memory usage flags, ensuring efficient operation even\nwith this larger model.\n3.6\nUncertainty Quantification & Bias Detection\nEnsuring the reliability of AI-generated medical information necessitates a robust approach to uncertainty quan-\ntification and bias detection. In healthcare applications, incorrect or misleading responses can have severe conse-\nquences, making it essential to assess confidence levels and mitigate potential biases before delivering information.\nOur multi-agent system implements a multi-layered approach to uncertainty estimation, combining Monte Carlo\ndropout sampling and perplexity-based confidence scoring. In parallel, our bias detection framework leverages lexical\nanalysis and sentiment assessment to identify and mitigate potential sources of misinformation or unfair language\npatterns. These mechanisms work in tandem to ensure that medical responses remain accurate, transparent, and\nethically responsible.\nMonte Carlo Dropout for Uncertainty Estimation To assess uncertainty in generated responses, our system\nemploys Monte Carlo dropout (MC Dropout), a probabilistic method that enables the estimation of model confi-\ndence by generating multiple response samples for the same query. This approach operates by introducing random\ndropout masks during inference, effectively simulating an ensemble of slightly varied models. By generating multiple\nresponses under these conditions, we calculate pairwise similarity metrics across outputs, where higher variability\nsignals increased uncertainty. The similarity is computed as:\nScos(A, B) =\nA · B\n∥A∥∥B∥,\nwhere A is the embedding vector of the input query and B is the embedding vector of the generated response.\n"}, {"page": 14, "text": "In medical applications, a low standard deviation across generated responses suggests a high degree of confidence,\nwhereas a high standard deviation indicates areas of potential ambiguity or lack of sufficient knowledge. This technique\nenables our system to flag potentially unreliable answers, triggering re-evaluation mechanisms such as additional\nevidence retrieval or human expert review. Unlike traditional deterministic model outputs, Monte Carlo dropout\nprovides a more nuanced and robust confidence estimate, crucial for ensuring safety in medical AI applications.\nPerplexity-Based Confidence Scoring Beyond MC Dropout, our system incorporates perplexity-based confidence\nscoring to quantify uncertainty at a token level. Perplexity (PPL) is a widely recognised metric in NLP that assesses\nhow well a language model predicts a given sequence. In the context of medical AI, higher perplexity scores indicate\nthat the model is less certain about its generated response, suggesting possible inaccuracy or knowledge gaps.\ndef compute_uncertainty_augmented_answer(response_text):\ntry:\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\nwith use_gpu(model):\nwith torch.cuda.amp.autocast():\ninputs = tokenizer(response_text, return_tensors=\"pt\").to(\"cuda\")\nwith torch.no_grad():\noutputs = model(**inputs, labels=inputs.input_ids)\nloss = outputs.loss.item()\nuncertainty_score = np.exp(loss)\nreturn uncertainty_score\nexcept Exception as e:\nprint(f\"Error in compute_uncertainty_augmented_answer: {str(e)}\")\nreturn float(\"inf\")\nThe perplexity is defined as:\nPPL = exp\n \n−1\nN\nN\nX\ni=1\nlog P(wi)\n!\n,\nwhere N is the number of words in the generated response, and P(wi) is the model-assigned probability of the i-th\nword.\nOur implementation utilises the OPT-1.3B model to compute perplexity scores for each generated response.\nThis allows us to establish confidence thresholds, where responses exceeding a predefined perplexity threshold (e.g.,\nPPL > 10.0) are flagged as potentially unreliable. When combined with MC Dropout sampling, perplexity scoring\nenhances response validation by identifying complex medical queries that may require additional expert intervention.\nFurthermore, perplexity-based filtering prevents the delivery of responses that appear syntactically correct but lack\nmedical validity, ensuring a higher standard of accuracy in clinical AI applications.\nLexical Bias Detection Implementation Bias in AI-generated medical content can arise due to training data\nimbalances, systematic errors in language representation, or unintended reinforcement of stereotypes [44]. To coun-\nteract this, our system implements lexical bias detection, a rule-based approach that scans responses for problematic\nlanguage patterns commonly associated with medical misinformation or biased phrasing. Our bias detection module\nmaintains a predefined lexicon of sensitive medical terms and phrases that may introduce implicit biases. When a\npotential lexical bias is detected, the system automatically triggers response refinement by either regenerating the\noutput with alternative phrasing or escalating the response for expert validation. This proactive approach ensures\nthe AI system maintains medical neutrality and avoids reinforcing harmful biases in healthcare recommendations.\nSentiment Analysis for Bias Mitigation To further strengthen bias detection, our system integrates sentiment\nanalysis as a complementary mechanism to lexical scanning. While medical responses should ideally maintain a neutral\nand factual tone, AI-generated text may sometimes introduce emotional bias, potentially misrepresenting the severity\nof conditions or the effectiveness of treatments. Our implementation employs fine-tuned sentiment classification\nmodels that assess whether a generated response exhibits overly positive, negative, or alarmist sentiment. When\na sentiment imbalance is detected, the system automatically reprocesses the response, adjusting its tone towards\nneutrality while preserving factual accuracy. In cases where a strong emotional tone is necessary (such as in warnings\nabout critical medical conditions), expert validation can override bias mitigation to maintain clinical appropriateness.\nWhile our sentiment analysis and lexical bias detection provide important safeguards against certain types of\nbiased content, we acknowledge that our current implementation lacks a comprehensive evaluation of group-level\n"}, {"page": 15, "text": "biases across demographic attributes such as gender, race, or socioeconomic factors. This represents a significant area\nfor improvement, as medical AI systems must ensure equitable performance across diverse patient populations. Future\nwork should systematically evaluate the system’s outputs for potential differential performance across demographic\nvariables and medical conditions.\nFurthermore, while we utilise LIME and SHAP for explainability, we have not fully contextualised these techniques\nfor clinical utility. In medical settings, these tools could potentially enhance practitioner trust and decision-making by\nproviding transparency into how the system integrates evidence and generates conclusions. Ideally, these explanations\nwould identify which symptoms, laboratory values, or patient history elements most strongly influenced the system’s\nresponses, enabling clinicians to critically evaluate outputs against their medical judgment. Future research should\nexplore optimising these explainability approaches specifically for medical contexts to support appropriate reliance\non AI assistance while preserving clinician autonomy in decision-making.\nWhile we utilise LIME and SHAP for explainability, we acknowledge that our current implementation lacks full\ncontextualisation for clinical utility. In medical settings, these tools could potentially enhance practitioner trust and\ndecision-making by providing transparency into how the system integrates evidence and generates conclusions. Ideally,\nthese explanations would identify which symptoms, laboratory values, or patient history elements most strongly\ninfluenced the system’s responses, enabling clinicians to critically evaluate outputs against their medical judgment.\nFuture research should explore optimising these explainability approaches specifically for medical contexts, potentially\nintegrating clinical knowledge graphs to highlight causal influences of specific medical entities in the visualisations,\nsupporting appropriate reliance on AI assistance while preserving clinician autonomy in decision-making.\nClinical knowledge graph integration for LIME/SHAP visualisations requires access to comprehensive medical\nontologies (such as UMLS or SNOMED CT) and sophisticated entity linking mechanisms to map model outputs\nto structured medical concepts. Developing this integration represents a substantial undertaking requiring medi-\ncal informatics expertise and ontology licensing agreements beyond this study’s scope. Our current LIME/SHAP\nimplementation provides foundational explainability infrastructure, with clinical ontology integration planned for\nsubsequent iterations once appropriate medical knowledge resources and expert collaboration are secured.\nResponse Refinement Pipeline The response refinement pipeline is designed to enhance the accuracy and re-\nliability of AI-generated medical responses by integrating uncertainty quantification, bias detection, and evidence\nvalidation. This multi-stage process dynamically adjusts responses based on confidence scores, detected biases, and\nexternal medical references, ensuring outputs are both medically sound and contextually appropriate.\nInitially, the system generates responses using a fine-tuned LLaMA model, which employs few-shot prompting to\nproduce structured and relevant medical explanations. To assess reliability, Monte Carlo Dropout sampling generates\nmultiple response variations, while perplexity scoring evaluates prediction confidence. Responses with high uncertainty\nor inconsistencies are flagged for further review.\nTo strengthen factual accuracy, the evidence retrieval agent queries trusted medical sources such as PubMed\nand clinical databases. Retrieved references are seamlessly integrated into responses, providing verifiable support\nfor medical claims. Simultaneously, bias detection mechanisms analyse language for potential overgeneralisations\nor misleading statements, while sentiment analysis ensures responses maintain a neutral and professional tone. If\na response is flagged due to low confidence or detected biases, it is refined through DeepSeek R1, which adjusts\nphrasing, incorporates additional evidence, and ensures coherence with validated medical knowledge. In cases where\nuncertainty persists, human expert validation is triggered, allowing professionals to verify and refine the content\nbefore finalisation. Verified responses are then stored in a knowledge graph, enabling continuous improvement of the\nsystem’s knowledge base. By combining uncertainty estimation, real-time evidence retrieval, and bias correction, this\nstructured pipeline minimises misinformation risks while enhancing the clinical applicability of AI-generated medical\ncontent. The result is a trustworthy, evidence-backed system that supports medical professionals and improves the\nreliability of AI-driven healthcare solutions.\nBias Detection using LIME and SHAP To enhance our bias detection framework beyond lexical analysis and\nsentiment assessment, we integrated advanced explainability techniques: Local Interpretable Model-agnostic Expla-\nnations (LIME) [34] and SHapley Additive exPlanations (SHAP) [22]. These approaches provide crucial transparency\ninto potential sources of bias within our medical AI responses.\nLIME works by creating simplified, interpretable models that locally approximate our system’s predictions, gen-\nerating explanations for individual responses. By perturbing input features and observing how predictions change,\nLIME identifies which terms or phrases most heavily influence the model’s outputs in potentially biased directions.\nThis proves particularly valuable in the medical domain, where terminology can carry implicit judgments that might\nnot be captured by simple lexical scanning.\nSHAP complements this approach by computing Shapley values, drawn from cooperative game theory, to deter-\nmine each feature’s contribution to predictions. This technique distributes the \"credit\" for particular outcomes across\ninput features, enabling us to identify which medical terms disproportionately influence responses across demographic\ngroups or conditions. SHAP’s global interpretability offers valuable insights into systematic biases that might remain\nhidden with single-response analysis techniques.\n"}, {"page": 16, "text": "Our implementation integrates these methods within the grouped query pipeline to create interpretable visu-\nalisations of potential bias sources, enabling healthcare professionals to understand not only what bias might be\npresent but why it occurs. When flagging potentially biased content, the system provides LIME and SHAP explana-\ntions highlighting specific problematic terms or phrases, facilitating more targeted refinement through the DeepSeek\npipeline.\nThis interpretability directly supports clinical decision-making by enabling healthcare professionals to understand\nwhich specific medical concepts, symptoms, or patient characteristics most heavily influenced the system’s response.\nFor instance, in a query about differential diagnosis, LIME explanations might reveal that terms like \"fever\" and\n\"fatigue\" disproportionately drove the model toward infectious disease conclusions, allowing clinicians to critically\nevaluate whether this reasoning aligns with the full clinical picture. By providing transparent attribution of model\nreasoning, these explainability techniques enhance appropriate clinician reliance on AI assistance supporting rather\nthan replacing professional medical judgment.\n3.7\nHardware Configuration and Performance Analysis\nOur experimental evaluation was conducted using Google Colab’s A100 GPU environment, equipped with NVIDIA\nA100 GPU (40GB memory), 85GB RAM, and 16 vCPU cores. This configuration provided reliable testing conditions\nfor both our fine-tuning experiments and multi-agent system implementation.\nResource utilisation analysis revealed distinct performance patterns across model architectures. During fine-\ntuning, the GPT implementation demonstrated a stable memory profile with consistent gradient handling and uni-\nform VRAM usage. The model maintained approximately 90-92k token/sec throughput with average step times\naround 180ms, indicating efficient resource allocation without memory consumption spikes. The LLaMA implemen-\ntation exhibited different resource patterns, with its RMSNorm and complex attention mechanisms (including rotary\npositional embeddings and grouped query attention) creating varied computational demands. While these architec-\ntural choices increased computational complexity in some areas, optimisations, including KV cache implementation,\nsignificantly reduced memory requirements during inference stages.\nDeepSeek R1 demonstrated exceptional efficiency through the Unsloth framework and 4-bit quantisation, achiev-\ning superior performance with smaller batch sizes while maintaining a reduced memory footprint.\nOur multi-agent implementation uses context managers to transition models between GPU and CPU after task\ncompletion, preventing memory fragmentation and allowing each agent to maximise GPU resources without inter-\nference. The Evidence Retrieval Agent operates primarily with CPU resources during API interactions, while the\nClinical Reasoning Agent and DeepSeek refinement stages utilise GPU acceleration during their respective processing\nwindows. This orchestrated resource management strategy enables the system to process complex medical queries\nefficiently, even under the constrained GPU memory environments typical in clinical deployment scenarios.\nOur monitoring infrastructure tracks performance across all stages using component-specific metrics, ensuring\nreliable operation and providing insights for future optimisations.\n4\nResults and Analysis\nThis section presents quantitative and qualitative findings from our model implementations, comparing performance\ncharacteristics across architectures and evaluating the effectiveness of our multi-agent approach for medical applica-\ntions.\n4.1\nTraining Dynamics and Performance Characteristics\nThe fine-tuning experiments across architectures revealed distinctive training dynamics with important implications\nfor medical domain adaptation. GPT demonstrated remarkable stability throughout its fine-tuning phase, with train-\ning loss progressively decreasing from 4.0 to 0.2 and validation loss maintaining a consistent gap that indicates strong\ngeneralisation abilities. LLaMA exhibited more variable training curves characterised by increased fluctuations in\nlosses compared to GPT; however, this variability reflects its aggressive optimisation process rather than instability,\nshowing significant performance improvements in later training stages. DeepSeek R1, through the Unsloth framework,\ndemonstrated exceptional convergence characteristics with stable training and rapid loss reduction, utilising linear\nlearning rate scheduling and 4-bit quantisation.\nFigure 7 illustrates the training and validation loss curves for GPT-2 over multiple training steps. The training loss\n(blue) shows a consistent decline, indicating successful model optimisation. The validation loss (orange) maintains a\nhigher value throughout but follows a similar decreasing pattern, suggesting good generalisation performance. The\nred dashed line represents the baseline loss for comparison.\nFigure 8 represents the variation in training loss over time (hours). Unlike a smooth decline, the loss exhibits\nfluctuations, which could indicate an unstable training process or periodic adjustments in the learning rate. These\nvariations should be analysed further to determine their impact on model convergence.\nFigure 9 presents the reduction in training loss as the global training step increases. The loss starts at approxi-\nmately 2.0 and decreases to around 1.0 by step 60, demonstrating effective training progress.\n"}, {"page": 17, "text": "Fig. 7. GPT-2 Training and Validation Loss: This figure presents the loss trends during GPT-2 training.\nFig. 8. LLaMA Training Loss Fluctuations Over Time: This figure illustrates the variation in training loss over four hours.\nFig. 9. DeepSeek Training Loss Reduction Across Global Steps: This figure illustrates the decline in training loss over increasing\nglobal steps, indicating the model’s progressive learning.\n"}, {"page": 18, "text": "Our implementation of these fine-tuned models within the multi-agent architecture produced compelling results\nin real-world medical query processing. When processing queries about early signs of Alzheimer’s disease, the system\ndemonstrated effective reasoning capabilities with high relevance scores. As illustrated in our experimental logs, the\nbase LLaMA response achieved a relevance score of 0.80, indicating strong alignment between query and response. The\nMonte Carlo dropout sampling technique generated five distinct responses (Figure 10) with remarkable consistency\n(average pairwise similarity: 0.95, standard deviation: 0.018), suggesting high confidence in the generated content.\nThis uncertainty quantification mechanism proved particularly valuable for medical applications where response\nreliability is paramount.\nFig. 10. Sample Outputs from the Multi-Agent System: This figure presents example text generations produced by our\nmulti-agent system. Each sample demonstrates the model’s ability to generate coherent and contextually relevant responses\nregarding early signs of Alzheimer’s disease. The system integrates multiple expert models to ensure accurate, fluent, and\nmedically relevant outputs.\n4.2\nResource Utilisation and System Performance\nThe analysis revealed distinct resource utilisation patterns across model architectures. GPT maintained a constant\nand effective GPU memory profile throughout training, with optimised gradient accumulation ensuring consistent\nthroughput (approximately 90-92k tokens/sec with ˜180ms average step time) while avoiding memory spikes. LLaMA\nemployed a more complex resource profile featuring rotary positional embeddings and grouped query attention, while\nKV cache and Flash Attention optimised inference efficiency, training required increased computational resources.\nDeepSeek R1 achieved optimal resource efficiency through innovative approaches, including the Unsloth framework,\n4-bit quantisation, and specialised LoRA targeting, enabling superior performance with reduced computational over-\nhead.\nThe multi-agent implementation demonstrated effective resource management during query processing. Our con-\ntext manager approach for transitioning models between GPU and CPU after task completion proved particularly\nvaluable in constrained environments. The system successfully processed complex medical queries with manageable\nmemory consumption as shown in Figure 11, though we observed occasional resource limitations with larger models.\nAs evidenced in our experimental logs, when processing the Alzheimer’s disease query, the DeepSeek refinement\nstage occasionally encountered GPU memory constraints with the warning \"GPU memory exceeded. Try reducing\ninput length or parameters.\" This highlights the importance of careful resource planning in deployment scenarios,\nparticularly when incorporating multiple large models within a single processing pipeline.\nFig. 11. : GPU memory utilisation chart during multi-stage processing\n4.3\nModel Evaluation and Performance Metrics\nWe quantitatively assessed the performance of the three fine-tuned models on medical question-answering tasks\nthrough standard NLP evaluation metrics, with results presented in Table 2.\nTable 2. Performance Metrics with Confidence Intervals\nMetric\nDeepSeek R1\nLLaMA Model\nGPT Model\nROUGE-1 0.53 ±0.04\n0.18 ±0.03\n0.16 ±0.03\nROUGE-2 0.22 ±0.03\n0.12 ±0.02\n0.08 ±0.02\nROUGE-L 0.21 ±0.03\n0.14 ±0.02\n0.13 ±0.02\nBLEU\n0.098 ±0.018\n0.0003 ±0.0001\n0.0002 ±0.0001\nThese results, calculated through bootstrap resampling across 1,000 iterations with 95% confidence intervals,\ndemonstrate statistically significant performance differences across architectures. As shown in Figure 12, DeepSeek\nR1 consistently outperforms both LLaMA and GPT models across all metrics. The substantial advantage in ROUGE-\n1 scores indicates DeepSeek R1’s superior ability to capture relevant medical terminology and concepts. Similarly,\n"}, {"page": 19, "text": "the higher ROUGE-2 scores demonstrate enhanced capability in maintaining medical phrase coherence and complex\nconceptual relationships. While all models show comparable ROUGE-L performance relative to their other metrics,\nDeepSeek R1 maintains its advantage, suggesting better preservation of sequence information in longer medical nar-\nratives. The dramatic difference in BLEU scores further confirms DeepSeek R1’s superior performance in constructing\ncoherent and accurate medical responses.\nFig. 12. Comparison of GPT, LLaMA, and DeepSeek R1 on Text Generation Metrics: This figure presents the performance\nevaluation of three models (GPT, LLaMA, and DeepSeek R1) using ROUGE-1, ROUGE-2, ROUGE-L, and BLEU scores.\nThe multi-agent architecture enhanced these base model capabilities through evidence integration and refinement\nstages. Our experiments demonstrated effective evidence retrieval from PubMed (Figure 13), with the system auto-\nmatically incorporating relevant research findings into responses. The augmented responses showed lower uncertainty\nscores (perplexity: 4.13) compared to base responses, confirming the value of evidence integration in medical infor-\nmation delivery. As shown in Figure 14, the relevance scores of responses improved across different stages, with MC\nDropout and augmented responses achieving higher alignment with the input query compared to the base LLaMA\nmodel. This demonstrates the effectiveness of uncertainty-based refinement in generating more contextually accu-\nrate medical responses. The implementation of bias detection proved valuable in identifying potentially problematic\ncontent, with the system correctly flagging responses containing absolutist language or inappropriate sentiment, as\nshown in our logs: \"Bias detected in the refined response. Expert review is recommended.\"\nFig. 13. Evidence-Augmented Response with Retrieved PubMed Reference: This figure presents an AI-generated response\nenriched with evidence from the scientific literature.\nDespite these limitations, our multi-agent architecture demonstrated effective medical query processing with\nappropriate uncertainty indication, evidence integration, and bias detection, representing a significant advancement\nin medical language model applications.\n4.4\nComparison with Specialised Medical Language Models\nTo contextualise our results within the broader landscape of medical language models, we evaluated BioGPT [23], a\nspecialised domain-specific model pre-trained on biomedical literature, on our test set in a zero-shot setting without\ntask-specific fine-tuning. BioGPT represents the current state-of-the-art in generative specialised medical language\nmodels, having been specifically pre-trained on PubMed abstracts and full-text articles. Table 3 presents the com-\nparative results.\n"}, {"page": 20, "text": "Fig. 14. Relevance Score Across Architectures: This figure compares the relevance scores of different model architectures,\nincluding LLaMA, DeepSeek R1, and Augmented Response.\nTable 3. Comparison with Specialised Medical Language Model\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU\nSpecialised Medical Model (Zero-Shot)\nBioGPT\n0.084 ± 0.024\n0.028 ± 0.009\n0.070 ± 0.019\n0.0003 ± 0.0003\nOur Fine-Tuned Models\nGPT-2 (Fine-tuned)\n0.16 ± 0.03\n0.08 ± 0.02\n0.13 ± 0.02\n0.0002 ± 0.0001\nLLaMA (Fine-tuned)\n0.18 ± 0.03\n0.12 ± 0.02\n0.14 ± 0.02\n0.0003 ± 0.0001\nDeepSeek R1 (Fine-tuned) 0.53 ± 0.04\n0.22 ± 0.03\n0.21 ± 0.03\n0.098 ± 0.018\nWhilst BioGPT benefits from extensive biomedical pre-training, our fine-tuned general-purpose models achieved\nsubstantially superior performance on the medical question-answering task. DeepSeek R1 demonstrates particularly\nstrong results, with ROUGE-1 scores of 0.53 compared to BioGPT’s 0.084, representing a statistically significant\nimprovement (p < 0.05) based on bootstrap hypothesis testing with 1,000 iterations. LLaMA, which serves as the\nClinical Reasoning Agent in our multi-agent architecture, also substantially outperforms the specialised baseline with\nROUGE-1 scores of 0.18. Even GPT-2, which we evaluated for comparative purposes but did not incorporate into the\nfinal multi-agent system due to architectural considerations detailed in Section 3, exceeds BioGPT’s performance.\nThis comparison reveals that specialised pre-training on biomedical literature does not automatically translate to\nsuperior performance on clinical question-answering tasks. Our approach demonstrates that general-purpose models,\nwhen appropriately fine-tuned with medical question-answer pairs, can achieve substantially stronger performance.\nThe superior performance of LLaMA and DeepSeek R1 informed our architectural choices for the multi-agent system,\nwhere these models serve as the foundation for clinical reasoning and response refinement respectively.\n4.5\nStatistical Significance Testing\nTo rigorously evaluate the statistical significance of performance differences between model architectures, we con-\nducted bootstrap hypothesis testing based on 1,000 bootstrap iterations with 95% confidence intervals. Table 4\npresents the comparative analysis of performance differences between model pairs across all evaluation metrics.\nDeepSeek R1 demonstrates statistically significant performance improvements over both GPT and LLaMA architec-\ntures across all metrics (p < 0.05), as evidenced by the non-overlapping confidence intervals. Specifically, DeepSeek\nR1 shows substantial improvements in ROUGE-1 scores compared to GPT (difference: 0.37) and LLaMA (difference:\n0.35), with both differences being statistically significant. Similar patterns emerge for ROUGE-2, with DeepSeek R1\noutperforming GPT (difference: 0.14) and LLaMA (difference: 0.10), and for ROUGE-L with improvements of 0.08\nand 0.07 over GPT and LLaMA respectively. The most dramatic differences appear in BLEU scores, where DeepSeek\nR1 exceeds both alternatives by approximately two orders of magnitude. In contrast, the performance differences be-\ntween LLaMA and GPT models are not statistically significant across any metrics, as indicated by their overlapping\nconfidence intervals. These results provide statistical confirmation that the architectural advantages of DeepSeek\n"}, {"page": 21, "text": "R1, particularly its Mixture-of-Experts framework and optimised implementation through the Unsloth framework,\ntranslate to meaningful and statistically significant performance improvements in medical domain tasks.\nTable 4. Results of Bootstrap Hypothesis Testing\nComparison\nMetric\nDifference\nSignificant (p < 0.05)\nDeepSeek vs. GPT\nROUGE-1\n0.37\nYes\nDeepSeek vs. GPT\nROUGE-2\n0.14\nYes\nDeepSeek vs. GPT\nROUGE-L\n0.08\nYes\nDeepSeek vs. GPT\nBLEU\n0.0978\nYes\nDeepSeek vs. LLaMA ROUGE-1\n0.35\nYes\nDeepSeek vs. LLaMA ROUGE-2\n0.10\nYes\nDeepSeek vs. LLaMA ROUGE-L\n0.07\nYes\nDeepSeek vs. LLaMA BLEU\n0.0977\nYes\nLLaMA vs. GPT\nROUGE-1\n0.02\nNo\nLLaMA vs. GPT\nROUGE-2\n0.04\nNo\nLLaMA vs. GPT\nROUGE-L\n0.01\nNo\nLLaMA vs. GPT\nBLEU\n0.0001\nNo\n4.6\nError Analysis and Failure Cases\nDespite the system’s strong overall performance, our analysis identified several recurring failure patterns that highlight\nareas for further refinement. These errors were most prominent in queries involving rare conditions, specialised\nmedical terminology, and complex multi-step reasoning, revealing limitations in both language model comprehension\nand evidence integration.\nOne of the most common failure types was terminology misinterpretation, where models incorrectly processed\nspecialised medical terms. GPT exhibited the highest error rate in this category, often conflating similarly structured\nterms. For example, in queries related to \"myasthenic crisis,\" the system occasionally confused it with \"myocardial\ncrisis,\" leading to completely inaccurate responses. DeepSeek R1 performed best in this regard, likely due to its more\nefficient tokenisation and handling of domain-specific medical vocabulary.\nThese findings emphasise the need for continued optimisation to enhance the system’s reliability in real-world\nclinical applications.\n4.7\nMulti-Agent System Overall Performance\nThe complete multi-agent system achieved 87% accuracy for medical queries with relevance scores of 0.80, demon-\nstrating significant improvement over single-model implementations. The adaptive response tailoring functionality\nsuccessfully adjusted content complexity based on simulated user expertise levels, with readability scores appropri-\nately varying from 8th-grade level for patient-oriented responses to professional-level for clinician-targeted content.\nResource utilisation analysis revealed acceptable performance even in resource-constrained environments, though\nDeepSeek R1 components occasionally encountered GPU memory limitations with the warning \"GPU memory ex-\nceeded. Try reducing input length or parameters.\" The context manager approach for transitioning models between\nGPU and CPU after task completion proved particularly valuable in managing these constraints.\n4.8\nLatency Benchmarks and Confidence Intervals\nPerformance evaluation of medical AI systems must consider not only accuracy metrics but also response latency,\nwhich is critical for real-world clinical applications. Our comprehensive latency analysis reveals significant differences\nacross model architectures and processing stages (Table 5).\nTable 5. Latency Benchmarks Across Architectures and Processing Stages\nModel/Component\nAverage Latency\n(seconds)\n95% Confidence\nInterval\nMaximum Latency\n(seconds)\nLLaMA Base Response\n25.94\n±0.38\n37.68\nDeepSeek R1 Base Response\n10.31\n±0.24\n26.42\nDeepSeek Response Refinement\n15.24\n±0.29\n36.50\nComplete Pipeline (without expert review) 36.50\n±0.94\n46.11\nDeepSeek R1 demonstrates notably faster base response generation (10.31 seconds) compared to LLaMA (25.94\nseconds), representing a 60% reduction in processing time. This efficiency advantage is particularly important in\ntime-sensitive healthcare contexts where rapid information access can impact decision-making. The complete multi-\nagent pipeline requires 36.50 seconds on average, with 95th percentile measurements reaching 46.11 seconds under\nhigh-demand conditions. These latency patterns provide crucial guidance for deployment planning across different\nhealthcare environments, from emergency departments requiring rapid responses to research contexts where process-\ning thoroughness may outweigh speed considerations. The narrow confidence intervals for base models (±0.24-0.38\n"}, {"page": 22, "text": "seconds) indicate consistent performance across queries, while the wider interval for the complete pipeline (±0.94\nseconds) reflects the increased variability inherent in more complex processing pathways.\n5\nDiscussion and Implications\nThis section examines the implications of our findings, analysing architectural trade-offs, performance characteristics,\nand the broader impact of our multi-agent approach on medical AI applications.\n5.1\nTheoretical Contributions and Differentiation from Existing Work\nThe findings presented in this study contribute to medical AI research in several distinctive ways that differentiate our\napproach from existing methodologies. Unlike monolithic systems such as Google’s Med-PaLM 2 [37], which employs\na single large language model for medical query processing, our multi-agent architecture theoretically advances the\nfield by demonstrating that specialised agent collaboration yields superior performance compared to scaling individual\nmodels. Whilst Med-PaLM 2 achieved expert-level performance on medical licensing examinations through exten-\nsive parameter scaling, our approach achieves comparable accuracy (87%) through architectural innovation rather\nthan computational brute force, representing a paradigm shift towards efficiency-driven medical AI. Our theoretical\nframework introduces novel uncertainty quantification mechanisms that extend beyond traditional confidence scoring,\nimplementing Monte Carlo dropout sampling specifically calibrated for medical applications where response reliability\ndirectly impacts patient safety. Furthermore, our bias detection framework advances current approaches by integrat-\ning lexical analysis with sentiment assessment and explainability techniques, moving beyond post-hoc bias correction\nto real-time bias prevention during response generation. The architectural combination of LLaMA’s efficiency with\nDeepSeek R1’s refinement capabilities represents a theoretical advancement in demonstrating that complementary\nmodel strengths can be systematically leveraged rather than pursuing singular architectural optimisation.\n5.2\nPractical Implications for Healthcare\nThe practical implications of this research extend significantly beyond academic advancement, offering healthcare\norganisations concrete deployment strategies for medical AI implementation across diverse clinical settings. Our\nresource efficiency analysis demonstrates that effective medical AI systems need not require the substantial com-\nputational infrastructure typically associated with large-scale language models, making advanced AI capabilities\naccessible to resource-constrained healthcare environments such as community clinics, rural medical centres, and\ndeveloping healthcare systems. The modular architecture enables graduated deployment strategies where healthcare\norganisations can implement components based on their specific needs and available infrastructure, from simple\nquery processing in primary care settings to comprehensive evidence-based decision support in specialist hospitals.\nFor instance, a district general hospital could implement the evidence retrieval component initially, then gradually\nadd uncertainty quantification and bias detection as computational resources allow.\nFor healthcare systems such as the NHS, these findings offer particular significance given ongoing pressures to\nimprove efficiency whilst maintaining care quality. The system’s ability to process medical queries with quantified\nuncertainty provides a framework for supporting clinical decision-making without replacing professional judgement,\npotentially reducing consultation times for routine inquiries whilst flagging complex cases requiring specialist atten-\ntion. Real clinical deployment implications include integration with existing electronic health record systems, support\nfor clinical documentation workflows, and potential applications in medical education and training programmes. The\nintegrated evidence retrieval system addresses critical practical limitations by ensuring responses remain current with\nevolving medical knowledge without requiring continuous model retraining, particularly valuable for maintaining up-\nto-date guidance across large healthcare networks.\nThe cost-effectiveness implications are substantial for publicly funded healthcare systems, where our approach’s\nreduced computational requirements could enable widespread AI deployment without significant infrastructure invest-\nment. Our uncertainty quantification mechanisms provide healthcare professionals with explicit confidence indicators,\nenabling appropriate reliance on AI assistance whilst preserving clinical autonomy. The bias detection components\naddress practical concerns about AI perpetuating healthcare disparities, offering real-time safeguards that can be\ncustomised for different patient populations and clinical contexts, particularly relevant for diverse healthcare systems\nserving multicultural populations.\n5.3\nDeployment Considerations in Resource-Constrained Environments\nThe multi-agent architecture introduces computational overhead that warrants consideration for deployment in\nresource-constrained healthcare settings. Our latency analysis reveals that the complete pipeline requires approx-\nimately 36.50 seconds average processing time, with 95th percentile reaching 46.11 seconds. For small and medium-\nsized hospitals or clinics with limited computational infrastructure, several deployment strategies could mitigate\nthese constraints: implementing asynchronous processing for non-urgent queries, deploying only essential agents\n(Clinical Reasoning Validator without DeepSeek refinement) for time-sensitive scenarios, utilising edge computing or\n"}, {"page": 23, "text": "cloud-based processing to distribute computational load, and caching common query responses to reduce redundant\nprocessing. The modular architecture enables graduated deployment where healthcare organisations can implement\ncomponents based on available resources and specific clinical requirements, making the system accessible across\ndiverse healthcare environments including resource-limited settings.\n5.4\nValidation Against Specialised Medical Models\nOur comparison with BioGPT validates the effectiveness of our fine-tuning approach for medical domain adaptation.\nWhilst specialised models leverage extensive biomedical pre-training, our results demonstrate that targeted fine-\ntuning of general-purpose models can achieve superior performance for clinical question-answering tasks. Among our\nfine-tuned models, DeepSeek R1 demonstrates particularly strong performance (ROUGE-1: 0.53 vs BioGPT’s 0.084),\nwhilst LLaMA also substantially exceeds the specialised baseline (ROUGE-1: 0.18 vs 0.084). These results suggest\nthat domain-specific fine-tuning on medical question-answer pairs provides substantial benefits beyond specialised\npre-training alone.\nThis finding has important implications for resource-constrained healthcare organisations, particularly within the\nNHS and similar publicly funded health systems. Our results suggest that effective medical AI components need\nnot require specialised models trained on vast biomedical corpora, which often demand substantial computational\nresources and may be subject to licensing restrictions. The strong performance of fine-tuned LLaMA and DeepSeek\nR1 models informed our multi-agent architecture design, where LLaMA serves as the Clinical Reasoning Agent and\nDeepSeek R1 provides response refinement. Whilst individual component performance suggests promising potential\nfor the integrated system, comprehensive multi-agent pipeline evaluation requires computational resources beyond\nthe scope of this initial validation.\n5.5\nLimitations and Considerations\nOur system’s strong performance on common conditions masks limitations in specialised domains. The training\ndata’s concentration in cardiology, oncology, and general medicine creates potential blind spots for rare diseases and\nemerging therapies. While the Evidence Retrieval Agent provides real-time literature access, the Clinical Reasoning\nValidator may struggle with unfamiliar domain-specific patterns. This reflects broader medical NLP challenges where\ndata scarcity creates systematic disparities [43]. Our modular architecture offers mitigation through selective com-\nponent enhancement as speciality datasets become available. The parameter-efficient fine-tuning demonstrated with\nDeepSeek R1 enables rapid adaptation using minimal domain examples. Immediate deployment in specialised settings\nrequires supplementary strategies: enhanced uncertainty thresholds for unfamiliar domains, explicit acknowledgement\nof knowledge limitations, and mandatory expert review for queries outside well-represented specialities.\nWhile our evaluation demonstrates strong performance on the MedQuAD dataset, we acknowledge important\nlimitations in the evaluation scope. MedQuAD, though comprehensive across multiple NIH domains, primarily cap-\ntures patient-oriented medical knowledge and may not fully represent the complexity of specialist clinical reasoning\nor diagnostic scenarios in advanced practice settings. Validation on additional clinical benchmarks such as MedQA\n(USMLE-style questions), PubMedQA (biomedical research questions), and case-based reasoning datasets would fur-\nther strengthen generalisability claims and provide insights into performance across different question formats and\nmedical contexts. Evaluation across diverse benchmarks representing various clinical scenarios and question types\nremains important future work.\nOur evaluation employs standard NLP metrics (ROUGE, BLEU) to enable systematic comparison with existing\napproaches and ensure reproducibility. However, these metrics, whilst valuable for assessing textual similarity, do\nnot fully capture clinical utility dimensions such as diagnostic accuracy, treatment appropriateness, or reasoning\nquality. Future work should incorporate structured clinical evaluation protocols with domain experts, including\nassessment of medical accuracy, appropriateness of recommendations, and quality of clinical reasoning. Such expert-\ndriven evaluation would provide complementary validation of system outputs across dimensions specific to medical\ndecision support, ensuring responses meet the nuanced requirements of clinical practice beyond textual correspondence\nwith reference answers.\nWe acknowledge that more specialised clinical metrics, such as BLEU-Clinical, expert clinical ratings, and medica-\ntion appropriateness scores, would provide additional validation dimensions. However, implementing these metrics re-\nquires access to clinical expert panels and structured evaluation protocols with appropriate ethical oversightresources\nunavailable within this study’s timeframe. Our choice of standard NLP metrics enables systematic comparison with\nexisting approaches while providing reproducible baselines. Clinical validation using specialised medical evaluation\nframeworks represents essential next-phase work, with findings from this architectural study providing the foundation\nfor comprehensive clinical trials.\nOur uncertainty quantification currently employs Monte Carlo dropout and perplexity-based scoring. Whilst these\nmethods provide useful confidence indicators, alternative approaches, including ensemble-based methods, conformal\nprediction, and more sophisticated Bayesian approximation techniques, may offer enhanced uncertainty estimation\nwith calibrated confidence intervals. Investigation of these advanced uncertainty quantification methods represents\nimportant future work, particularly for safety-critical clinical applications where precise confidence bounds are es-\nsential.\n"}, {"page": 24, "text": "We did not implement ensemble-based methods or conformal prediction in this study due to substantial compu-\ntational overhead: ensemble methods would require training and maintaining multiple model variants, multiplying\ninference costs beyond our GPU allocation, whilst conformal prediction necessitates extensive calibration datasets\nwith ground-truth uncertainty labels unavailable for medical question-answering. Our Monte Carlo dropout and\nperplexity-based approaches provide functional uncertainty quantification with manageable computational require-\nments, establishing baselines for more sophisticated techniques in future deployments with greater computational\nresources.\nOur bias detection mechanisms, while addressing lexical and sentiment-level issues, lack systematic fairness eval-\nuation across demographic attributes such as gender, race, or socioeconomic status. This represents a critical gap for\nmedical AI systems that must ensure equitable performance across diverse patient populations.\nFuture work should implement group-level fairness metrics and conduct disparity analysis to verify equitable sys-\ntem behaviour across protected attributes. We note that systematic fairness evaluation across demographic groups\nrequires demographically annotated medical datasets, which remain extremely limited in public availability due to\nlegitimate privacy protections under GDPR and HIPAA regulations. The few available datasets with demographic\nannotations (such as MIMIC-III) require extensive institutional review board approval and controlled access agree-\nments that were beyond this study’s scope. Our current bias detection framework provides foundational safeguards,\nwhilst demographic fairness evaluation remains a priority for future work, contingent upon appropriate data access\nand ethical approvals.\nWhile we employ LIME and SHAP for explainability, our current implementation lacks full contextualisation\nfor clinical decision support. Future research should optimise these approaches for medical contexts, potentially\nintegrating clinical knowledge graphs to highlight causal influences of specific symptoms, laboratory values, and\npatient history elements, thereby supporting appropriate clinician reliance while preserving professional autonomy.\nMedical AI deployment demands rigorous privacy protection with architecture-specific considerations. GPT’s sim-\npler design may facilitate privacy-preserving modifications, while LLaMA and DeepSeek R1’s optimised architectures\nrequire careful analysis to maintain privacy without compromising efficiency. Our bias detection module highlights\nthe need for continuous ethical oversight, as medical AI must address systematic biases that could exacerbate health-\ncare disparities [5]. The human validation component serves dual purposes - ensuring content appropriateness while\nidentifying emergent biases. Healthcare organisations must implement comprehensive governance frameworks ad-\ndressing data protection, algorithmic transparency, and clinical accountability, with privacy-preserving techniques\n[42] integrated at architectural levels rather than post-hoc additions.\nOur modular architecture enables diverse clinical applications beyond question-answering. The Clinical Reasoning\nValidator’s chain-of-thought approach naturally extends to EHR summarisation, while combined evidence retrieval\nand uncertainty quantification provide frameworks for presenting differential diagnoses with explicit confidence assess-\nments. Consider a practical scenario: a patient with fatigue, weight loss, and fever triggers our differential diagnosis\npipeline where the Clinical Reasoning Validator generates structured hypotheses, Evidence Retrieval gathers sup-\nporting literature, DeepSeek R1 refines recommendations, and bias detection ensures balanced presentation. This\ndemonstrates architectural component synergy supporting complex clinical reasoning, though real-world validation\nremains essential.\nOur specialised model comparison was conducted with BioGPT in zero-shot settings to provide baseline per-\nformance benchmarks that reflect the model’s capabilities without task-specific adaptation. Whilst this comparison\ndemonstrates the effectiveness of our fine-tuning approach, we acknowledge that specialised models fine-tuned on\nthe same task-specific data might show different performance characteristics. Additionally, our evaluation focused on\nquestion-answering performance using standard NLP metrics; specialised models may demonstrate particular advan-\ntages in other biomedical NLP tasks such as entity recognition or literature mining for which they were specifically\ndesigned. Furthermore, comprehensive evaluation of our complete multi-agent system pipeline (including evidence\nretrieval, reasoning validation, and refinement stages) on large test sets requires substantial GPU resources that\nwere unavailable during the revision period. Whilst our component-level evaluations and architectural design pro-\nvide strong evidence for the approach’s validity, full-scale system evaluation with adequate computational resources\nremains important future work.\nRegarding other specialised medical models such as PubMedBERT and Clinical-T5: PubMedBERT is an encoder-\nonly BERT architecture designed for classification and named entity recognition tasks, not generative question-\nanswering, making direct comparison using generative metrics methodologically inappropriate. Clinical-T5, whilst\ngenerative, was not included due to practical constraints: the model’s closed-weight distribution and limited LoRA\nadapter support made parameter-efficient fine-tuning infeasible within our computational budget. BioGPT repre-\nsents the most relevant generative baseline for our question-answering task, providing meaningful comparison whilst\nacknowledging that comprehensive evaluation across all specialised architectures remains valuable future work.\n5.6\nBroader Impact on Medical AI Field\nThis research establishes several important precedents that may influence the trajectory of medical AI development\nbeyond the immediate scope of question-answering systems. The demonstrated effectiveness of multi-agent architec-\ntures over monolithic approaches suggests that the field may benefit from moving towards collaborative AI systems\nrather than pursuing ever-larger individual models, potentially reshaping resource allocation strategies across the\n"}, {"page": 25, "text": "medical AI research community. Our evidence-based validation framework provides a template for integrating real-\ntime medical literature into AI systems, addressing the persistent challenge of knowledge currency that affects all\nmedical AI applications from diagnostic support to treatment recommendation systems. The uncertainty quantifica-\ntion methodologies developed here may inform broader medical AI applications where confidence assessment proves\ncrucial, including diagnostic imaging interpretation, drug interaction prediction, and clinical trial patient selection.\nThe bias detection framework contributes to growing recognition that medical AI systems require proactive fairness\nmechanisms rather than reactive bias correction, potentially influencing regulatory frameworks and deployment stan-\ndards across the healthcare technology sector. Furthermore, our resource efficiency findings challenge assumptions\nabout computational requirements for effective medical AI, potentially democratising access to advanced AI capa-\nbilities across healthcare systems with varying technological infrastructures. These contributions collectively suggest\nthat effective medical AI advancement may depend more on architectural innovation and systematic validation than\non computational scaling, offering sustainable pathways for continued development in resource-conscious healthcare\nenvironments.\n6\nConclusion and Future Directions\nThis research advances medical AI through comprehensive architectural comparison and multi-agent system devel-\nopment. Our analysis of GPT, LLaMA, and DeepSeek R1 reveals distinct trade-offs: GPT’s stability suits critical\napplications despite computational costs; LLaMA balances efficiency with performance for resource-constrained set-\ntings; DeepSeek R1 achieves superior metrics while maintaining efficiency through innovative optimisations. Our\nmulti-agent implementation successfully addresses single-model limitations with the Clinical Reasoning Validator\nachieving 87% accuracy and high relevance scores, while Evidence Retrieval enhanced reliability through literature\nintegration. Uncertainty quantification and bias detection provide essential safeguards for medical information deliv-\nery.\nKey challenges remain including terminology misinterpretation, evidence integration failures, and complex reason-\ning limitations, highlighting ongoing research needs. Our findings emphasise leveraging complementary architectural\nstrengths rather than pursuing single-model solutions. DeepSeek R1’s performance suggests promising directions for\nefficient attention mechanisms in medical adaptation, while the multi-agent approach validates component specialisa-\ntion over monolithic designs. Most significantly, human-AI collaboration frameworks emerge as critical for balancing\nautomation benefits with professional oversight. This work establishes foundations for more reliable, transparent,\nand clinically valuable medical AI systems. Through continued architectural innovation, enhanced evaluation frame-\nworks, and strengthened human-AI collaboration, these technologies can meaningfully support healthcare delivery\nwhile maintaining essential safeguards for patient care.\n6.1\nFuture Research Directions\nBuilding on our implementation combining LLaMA and DeepSeek R1, future research should explore dynamic task\nallocation [17] based on query characteristics and enhanced orchestration selecting model combinations based on\ncomplexity, uncertainty, and resources. Evidence retrieval expansion beyond PubMed to include clinical guidelines,\ndrug databases, and imaging repositories would create comprehensive knowledge integration. While Monte Carlo\ndropout provides valuable insights, exploring Bayesian approaches [10] or medical-specific ensemble methods could\nyield nuanced uncertainty metrics [20]. Bias detection requires medical-specific lexical databases and fairness metrics\ncalibrated for healthcare applications.\nOur optional expert validation represents initial steps toward sophisticated human-AI collaboration [33]. Fu-\nture research should optimise labour division between automated systems and professionals, developing intuitive\ninterfaces for expert input and efficient feedback incorporation. Different interaction modes warrant investigation,\nsynchronous review for critical applications, and asynchronous feedback for routine tasks. Comprehensive medical AI\nevaluation requires multidimensional frameworks beyond traditional NLP metrics, incorporating clinical accuracy,\nevidence integration quality, reasoning transparency, resource efficiency, uncertainty communication, and bias mitiga-\ntion. Critically, clinician-centred evaluation through formal usability studies must assess practical workflow value [3].\nSuch evaluation should include qualitative feedback on response quality, relevance to clinical decision-making, and\nintegration with existing clinical processes. Expert scoring of system outputs across various medical specialities would\nprovide valuable insights into performance variability and help identify priority areas for improvement. Additionally,\nobservational studies of clinicians interacting with the system could reveal usability challenges and workflow integra-\ntion opportunities that are not apparent from computational evaluations alone. Real-world deployment challenges\nnecessitate research into containerisation, model compression, and efficient architectures accommodating infrastruc-\nture limitations [36]. Integration with EHR systems requires standardised APIs and protocols, while governance\nframeworks ensuring responsible deployment remain essential for widespread adoption.\nDeclaration of generative AI and AI-assisted technologies in the writing process:\nDuring the preparation of this work, the author(s) used Claude.ai for minor writing corrections and language refine-\nment. After using this tool, the author(s) reviewed and edited the content as needed and take full responsibility for\nthe content of the publication.\n"}, {"page": 26, "text": "References\n1. Abacha, A.B., Demner-Fushman, D.: A question-entailment approach to question answering. BMC Bioinformatics 20\n(2019)\n2. Alsentzer, E., Murphy, J.R., Boag, W., Weng, W.H., Jin, D., Naumann, T., McDermott, M.B.A.: Publicly available clinical\nbert embeddings. ArXiv abs/1904.03323 (2019)\n3. Beede, E., Baylor, E.E., Hersch, F., Iurchenko, A., Wilcox, L., Ruamviboonsuk, P., Vardoulakis, L.M.: A human-centered\nevaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy. Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems (2020)\n4. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\nA., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C.,\n..., Amodei, D.: Language models are few-shot learners. In: Advances in Neural Information Processing Systems. vol. 33,\npp. 1877–1901. Curran Associates, Inc. (2020)\n5. Buscemi, A., Proverbio, D., Bova, P., Balabanova, N., Bashir, A., Cimpeanu, T., da Fonseca, H.C., Duong, M.H., Domingos,\nE.F., Fernandes, A., Krellner, M., Ogbo, N.B., Powers, S.T., Santos, F.P., Ush-Shamszaman, Z., Song, Z., Stefano, A.D.,\nAnh, H.T.: Do llms trust ai regulation? emerging behaviour of game-theoretic llm agents. ArXiv abs/2504.08640 (2025)\n6. Buscemi, A., Proverbio, D., Stefano, A.D., Han, T.A., Castignani, G., Lio, P.D.: Fairgame: a framework for ai agents bias\nrecognition using game theory. ECAI 2025 (In press), ArXiv abs/2504.14325 (2025)\n7. Colombo, A., Bernasconi, A., Ceri, S.: An llm-assisted etl pipeline to build a high-quality knowledge graph of the italian\nlegislation. Information Processing & Management 62 (2025)\n8. Dao, T., Fu, D.Y., Ermon, S., Rudra, A., R’e, C.: Flashattention: Fast and memory-efficient exact attention with io-\nawareness. ArXiv abs/2205.14135 (2022)\n9. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X.,\nYu, X., Wu, Y., Wu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao, Z., ..., Zhang, Z.: Deepseek-r1: Incentivizing reasoning capability\nin llms via reinforcement learning. ArXiv abs/2501.12948 (1 2025). https://doi.org/10.48550/arXiv.2501.12948\n10. Gal, Y., Ghahramani, Z.: Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In:\nInternational Conference on Machine Learning. pp. 1050–1059 (2015)\n11. GeeksForGeeks:\nDeepSeek-R1:\nTechnical\noverview\nof\nits\narchitecture\nand\ninnovations\n(2025),\nhttps://www.\ngeeksforgeeks.org/deepseek-r1-technical-overview-of-its-architecture-and-innovations/\n12. Gu, Y., Tinn, R., Cheng, H., Lucas, M.R., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H.: Domain-specific\nlanguage model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare\n(HEALTH) 3, 1 – 23 (2020)\n13. Han, T.A., Pandit, D., Joneidy, S., Hasan, M.M., Hossain, J., Tania, M.H., Hossain, M.A., Nourmohammadi, N.: An\nexplainable ai tool for operational risks evaluation of ai systems for smes. In: 2023 15th International Conference on\nSoftware, Knowledge, Information Management and Applications (SKIMA). pp. 69–74. IEEE (12 2023). https://doi.\norg/10.1109/SKIMA59232.2023.10387301\n14. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv: Learning (2016)\n15. Hu, J.E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Chen, W.: Lora: Low-rank adaptation of large language\nmodels. ArXiv abs/2106.09685 (2021)\n16. Huang, K., Altosaar, J., Ranganath, R.: Clinicalbert: Modeling clinical notes and predicting hospital readmission. ArXiv\nabs/1904.05342 (2019)\n17. Isern, D., Moreno, A.: A systematic literature review of agents applied in healthcare. Journal of Medical Systems 40, 43\n(2 2016). https://doi.org/10.1007/s10916-015-0376-2\n18. Kohn, M.S., Sun, J., Knoop, S.E., Shabo, A., Carmeli, B., Sow, D.M., Syed-Mahmood, T., Rapp, W.: Ibm’s health analytics\nand clinical decision support. Yearbook of Medical Informatics 23, 154 – 162 (2014)\n19. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: Biobert: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics 36, 1234–1240 (2 2020). https://doi.org/10.1093/bioinformatics/\nbtz682\n20. Leibig, C., Allken, V., Ayhan, M.S., Berens, P., Wahl, S.: Leveraging uncertainty information from deep neural networks\nfor disease detection. Scientific Reports 7, 17816 (12 2017). https://doi.org/10.1038/s41598-017-17876-z\n21. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. ACM Computing Surveys 55, 1–35 (9 2023). https://doi.org/10.\n1145/3560815\n22. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions. In: Neural Information Processing Systems.\npp. 4765–4774 (2017)\n23. Luo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu, T.Y.: Biogpt: generative pre-trained transformer for biomedical\ntext generation and mining. Briefings in Bioinformatics 23, bbac409 (11 2022). https://doi.org/10.1093/bib/bbac409\n24. Meskó, B., Görög, M.: A short guide for medical professionals in the era of artificial intelligence. npj Digital Medicine\n3(1), 126 (2020). https://doi.org/10.1038/s41746-020-00333-z\n25. Milakov, M., Gimelshein, N.: Online normalizer calculation for softmax. ArXiv abs/1805.02867 (2018)\n26. Nazi, Z.A., Peng, W.: Large language models in healthcare and medical domain: A review. Informatics 11, 57 (2023)\n27. NVIDIA: Accelerating a hugging face llama 2 and llama 3 models with transformer engine (2025), https:\n//docs.nvidia.com/deeplearning/transformer-engine-releases/release-1.11/user-guide/examples/te_llama/\ntutorial_accelerate_hf_llama_with_te.html\n28. Ojuri, S., Han, T.A., Chiong, R., Stefano, A.D.: Optimizing text-to-sql conversion techniques through the integration\nof intelligent agents and large language models. Information Processing & Management 62, 104136 (9 2025). https:\n//doi.org/10.1016/j.ipm.2025.104136\n29. Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., Gurevych, I.: Adapterfusion: Non-destructive task composition for transfer\nlearning. ArXiv abs/2005.00247 (2020)\n"}, {"page": 27, "text": "30. Pfeiffer, J., Rücklé, A., Poth, C.A., Kamath, A., Vulic, I., Ruder, S., Cho, K., Gurevych, I.: Adapterhub: A framework for\nadapting transformers. ArXiv abs/2007.07779 (2020)\n31. Price, W.N., Cohen, I.G.: Privacy in the age of medical big data. Nature Medicine 25, 37–43 (1 2020). https://doi.org/\n10.1038/s41591-018-0272-7\n32. Rajkomar, A., Dean, J., Kohane, I.: Machine learning in medicine. New England Journal of Medicine 380, 1347–1358 (4\n2019). https://doi.org/10.1056/NEJMra1814259\n33. Rajpurkar, P., Chen, E., Banerjee, O., Topol, E.J.: Ai in health and medicine. Nature Medicine 28, 31–38 (1 2022).\nhttps://doi.org/10.1038/s41591-021-01614-0\n34. Ribeiro, M.T., Singh, S., Guestrin, C.: “why should i trust you?”: Explaining the predictions of any classifier. Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016)\n35. Rogers, A., Kovaleva, O., Rumshisky, A.: A Primer in BERTology: What We Know About How BERT Works. Transactions\nof the Association for Computational Linguistics 8, 842–866 (12 2020). https://doi.org/10.1162/tacl_a_00349\n36. Sendak, M.P., D’Arcy, J., Kashyap, S., Gao, M., Nichols, M., Corey, K.M., Ratliff, W., Balu, S., Sendak, D., Gao, D., Dr,\nD.B., Nichols: A path for translation of machine learning products into healthcare delivery. EMJ Innovations (2020)\n37. Singhal, K., Azizi, S., Tu, T., Mahdavi, S., Wei, J., Chung, H.W., Scales, N., Tanwani, A.K., Cole-Lewis, H.J., Pfohl,\nS.J., Payne, P.A., Seneviratne, M.G., Gamble, P., Kelly, C., Scharli, N., Chowdhery, A., Mansfield, P.A., y Arcas, B.A.,\nWebster, D.R., Corrado, G.S., Matias, Y., Chou, K.H.L., Gottweis, J., Tomašev, N., Liu, Y., Rajkomar, A., Barral, J.K.,\nSemturs, C., Karthikesalingam, A., Natarajan, V.: Large language models encode clinical knowledge. Nature 620, 172 –\n180 (2022)\n38. Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Amin, M., Hou, L., Clark, K., Pfohl, S.R., Cole-Lewis, H.,\nNeal, D., Rashid, Q.M., Schaekermann, M., Wang, A., Dash, D., Chen, J.H., Shah, N.H., Lachgar, S., Mansfield, P.A.,\nPrakash, S., Green, B., Dominowska, E., y Arcas, B.A., Tomašev, N., Liu, Y., Wong, R., Semturs, C., Mahdavi, S.S.,\nBarral, J.K., Webster, D.R., Corrado, G.S., Matias, Y., Azizi, S., Karthikesalingam, A., Natarajan, V.: Toward expert-\nlevel medical question answering with large language models. Nature Medicine 31, 943–950 (3 2025). https://doi.org/\n10.1038/s41591-024-03423-7\n39. Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W.: Large language models in\nmedicine. Nature Medicine 29, 1930–1940 (8 2023). https://doi.org/10.1038/s41591-023-02448-8\n40. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient foundation language models. ArXiv\nabs/2302.13971 (2 2023). https://doi.org/10.48550/arXiv.2302.13971\n41. Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all\nyou need. In: Neural Information Processing Systems. pp. 5998–6008 (2017)\n42. Vayena, E., Blasimme, A., Cohen, I.G.: Machine learning in medicine: Addressing ethical challenges. PLOS Medicine 15,\ne1002689 (11 2018). https://doi.org/10.1371/journal.pmed.1002689\n43. Wang, Y., Wang, L., Rastegar-Mojarad, M., Moon, S., Shen, F., Afzal, N., Liu, S., Zeng, Y., Mehrabi, S., Sohn, S., Liu, H.:\nClinical information extraction applications: A literature review. Journal of Biomedical Informatics 77, 34–49 (1 2018).\nhttps://doi.org/10.1016/j.jbi.2017.11.011\n44. Wei, S., Wang, Z., Li, M., Liu, X., Wu, B.: Dccma-net: Disentanglement-based cross-modal clues mining and aggregation\nnetwork for explainable multimodal fake news detection. Information Processing & Management 62(4), 104089 (2025).\nhttps://doi.org/10.1016/j.ipm.2025.104089\n"}]}