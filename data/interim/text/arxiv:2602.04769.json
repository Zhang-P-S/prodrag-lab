{"doc_id": "arxiv:2602.04769", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04769.pdf", "meta": {"doc_id": "arxiv:2602.04769", "source": "arxiv", "arxiv_id": "2602.04769", "title": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Image", "authors": ["Yan Chen", "Jie Peng", "Moajjem Hossain Chowdhury", "Tianlong Chen", "Yunmei Liu"], "published": "2026-02-04T17:06:38Z", "updated": "2026-02-09T03:38:37Z", "summary": "Accurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention, yet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large language models (LLMs) show promise in handling neural signals across diverse patients, but two significant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially across EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a massive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the novel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided Channel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of Neuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual representations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact visual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas across multiple seizure detection datasets, demonstrating a significant improvement of 20% in F1 score and reductions of 88% in inference latency. These results highlight NeuroCanvas as a scalable and effective solution for real-time and resource-efficient seizure detection in clinical practice.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04769v2", "url_pdf": "https://arxiv.org/pdf/2602.04769.pdf", "meta_path": "data/raw/arxiv/meta/2602.04769.json", "sha256": "76b9aff0b33a9c18d6a5768fe83b33f52308b6966cd710c77d359f185bf45323", "status": "ok", "fetched_at": "2026-02-18T02:19:46.053204+00:00"}, "pages": [{"page": 1, "text": "February 10, 2026\nNeuroCanvas: VLLM-Powered Robust Seizure Detection\nby Reformulating Multichannel EEG as Image\nYan Chen* 1 , Jie Peng* 2 , Moajjem Hossain Chowdhury* 1 , Tianlong Chen2 , Yunmei Liu1\n1 MINDxAI Lab, Industrial & Systems Engineering Department, University of Louisville\n2 UNITES Lab, University of North Carolina at Chapel Hill\n* Equal contribution\nAbstract\nAccurate and timely seizure detection from Electroencephalography (EEG) is critical for clinical intervention,\nyet manual review of long-term recordings is labor-intensive. Recent efforts to encode EEG signals into large\nlanguage models (LLMs) show promise in handling neural signals across diverse patients, but two signifi-\ncant challenges remain: (1) multi-channel heterogeneity, as seizure-relevant information varies substantially\nacross EEG channels, and (2) computing inefficiency, as the EEG signals need to be encoded into a mas-\nsive number of tokens for the prediction. To address these issues, we draw the EEG signal and propose the\nnovel NeuroCanvas framework. Specifically, NeuroCanvas consists of two modules: (i) The Entropy-guided\nChannel Selector (ECS) selects the seizure-relevant channels input to LLM and (ii) the following Canvas of\nNeuron Signal (CNS) converts selected multi-channel heterogeneous EEG signals into structured visual rep-\nresentations. The ECS module alleviates the multi-channel heterogeneity issue, and the CNS uses compact\nvisual tokens to represent the EEG signals that improve the computing efficiency. We evaluate NeuroCanvas\nacross multiple seizure detection datasets, demonstrating a significant improvement of 20% in F1 score and\nreductions of 88% in inference latency. These results highlight NeuroCanvas as a scalable and effective so-\nlution for real-time and resource-efficient seizure detection in clinical practice. The code will be released at\nhttps://github.com/Yanchen30247/NeuroCanvas.git.\n1. Introduction\nAs one of the most prevalent neurological conditions, epilepsy poses a significant public health challenge (Singh & Sander,\n2020). It is characterized by the predisposition to generate recurrent seizures and impacts the lives of approximately\n50 million people worldwide (WHO, 2024). Consequently, accurate and timely seizure detection is vital for immediate\nmedical intervention (Devinsky et al., 2016). Scalp electroencephalography (EEG) plays an important role in seizure\ndetection. Clinically, the standard for identifying seizure activity involves the visual interpretation of long-term EEG\nrecordings by specialized physicians or neurologists. However, this manual review process is labor-intensive and time-\nconsuming (Ramgopal et al., 2014).\nTo address these limitations, automated seizure detection systems based on deep learning have been proposed. These\nsystems leverage advanced algorithms to enable continuous monitoring without the constraints of human fatigue (Rasheed\net al., 2020). For instance, Tang et al. (2021) employed self-supervised graph neural networks (GNNs), while Afzal et al.\n(2024) utilized efficient recurrent update mechanisms to achieve impressive detection accuracy. Latest Evobrain (Kotoge\net al., 2025) further integrated a two-stream architecture within a time-graph framework to explicitly model the dynamic\nevolution of brain networks. Despite these advancements, two critical challenges persist: (1) seizure events are temporally\nsparse, occurring far less frequently than non-seizure events, and (2) EEG signals vary significantly across patients, limiting\nthe generalization of deep learning models in practical scenarios.\nPreprint. Under review.\n1\narXiv:2602.04769v2  [cs.LG]  9 Feb 2026\n"}, {"page": 2, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nRecently, large language models (LLMs) have shown remarkable potential in addressing these challenges due to their\nstrong generalization capabilities derived from pretraining. Pioneering works like NeuroLM (Jiang et al., 2024), Uni-\nMind (Lu et al., 2025), and EEG-GPT (Kim et al., 2024) have explored novel approaches to tokenizing EEG signals or\ntranslating EEG features into verbal representations, enabling chat-style LLMs to perform prediction and detection tasks.\nThese methods demonstrate promise in leveraging LLMs for seizure detection by handling both the scarcity of seizure\nevents and patient-specific signal differences. However, integrating LLMs for EEG-based seizure detection is not without\nchallenges. Two critical issues remain unresolved: (1) Multi-channel heterogeneity, where the importance of different\nEEG channels varies significantly across patients and environments, and indiscriminate use of multi-channel signals may\nintroduce irrelevant noise that decrease prediction accuracy. (2) Computing inefficiency, as existing methods require\nmassive tokenization of multi-channel signals, resulting in excessive computational demands that are incompatible with\nreal-time seizure detection requirements. Addressing these challenges is crucial for developing scalable, efficient, and\naccurate seizure detection systems for practical clinical settings.\nMotivated by the observations above and the limitations in seizure detection, we propose NeuroCanvas, a novel frame-\nwork that innovatively transforms EEG signals into visual representations, enabling robust seizure detection using vision-\nbased large language models (VLLMs). Specifically, NeuroCanvas comprises two key components: (i) The Entropy-guided\nChannel Selector (ECS): This module uses channel-wise entropy to rank electrodes based on their informativeness, retain-\ning only the most relevant channels for seizure detection. By filtering out irrelevant channels, the ECS module reduces\nnoise and improves task accuracy, addressing the issue of multi-channel heterogeneity. (ii) The CNS: This module trans-\nforms multi-channel EEG windows into compact intensity maps, where pixel values encode normalized signal activity.\nSeizure-related bursts appear as salient spatiotemporal motifs, which can be effectively captured by pretrained visual mod-\nels. The CNS module enhances computational efficiency by reducing the input tokens while preserving critical information.\nFinally, we fine-tune a pretrained VLLM to adapt its general-purpose visual representations for accurate seizure detection.\nIn summary, our contributions are as follows:\n• EEG-to-Image Encoding: We introduce an innovative strategy to encode multi-channel EEG signals into compact\nintensity maps, enabling pretrained visual models to effectively capture seizure-related patterns.\n• VLLM-Driven Framework: We propose a robust seizure detection framework that combines entropy-guided chan-\nnel selection with prompt-guided VLLM adaptation, ensuring efficient inference and maintaining reliability by filter-\ning irrelevant EEG channels.\n• Experimental Validation: Extensive experiments on the TUSZ and CHB-MIT datasets demonstrate the superiority\nof NeuroCanvas, achieving a binary F1-score of 0.501, over 20% higher than the best previous model. Furthermore,\nNeuroCanvas remains robust under severe channel reduction, achieving accuracy of 0.8487 with only two EEG chan-\nnels retained, and provides low-latency inference with 19ms per sample.\n2. Related Work\nDeep Learning Models for EEG Seizure Detection. Deep learning have become the dominant method for seizure detec-\ntion (Rasheed et al., 2020). Classical methods widely used Convolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN) to capture spatiotemporal dependencies in EEG signals (Acharya et al., 2018; Emami et al., 2019; O’Shea\net al., 2020; Talathi, 2017; Tsiouris et al., 2018; Zhang et al., 2022). To further capture the non-Euclidean nature of brain\nconnectivity, GNNs, such as DCRNN (Tang et al., 2021), REST (Afzal et al., 2024) and EvoBrain (Kotoge et al., 2025),\nwere proposed to model topological relationships. However, these models struggle with fixed channel configurations and\npoor generalization ability. Despite the architectural advancements of GNN-based models, they remain constrained by\nfixed channel configurations and poor generalization ability (Afzal et al., 2024; Tang et al., 2021). Thus, their performance\ndecreases in clinical EEG signals characterized by heterogeneity and incompleteness.\nEEG–LLM Alignment and Tokenization. Recently, the rapid development of foundations models raises the potential\nto solve these limitations in the seizure detection. These approaches typically align EEG signals with the semantic space\nof LLMs to leverage their robust generalization capabilities (Jiang et al., 2024; Lu et al., 2025). However, application\nof text-based LLMs to high-dimensional EEG signals presents critical bottlenecks: token inefficiency and morphological\ninformation loss. As EEG is continuous and high dimensional, tokenization often yields long sequences that consume a\ndisproportionate amount of the LLM’s context window (Liu et al., 2025; Merrill et al., 2024). Moreover, discrete tokens\noften fail to preserve fine-grained morphological features, such as the precise geometry of spikes or subtle rhythm changes,\nPreprint. Under review.\n2\n"}, {"page": 3, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nwhich are more naturally preserved in visual representations (Zhang et al., 2025; Liu et al., 2025). VLLM based approaches,\nthus, represent a promising paradigm which takes advantage of the inherent generalization of VLLM foundation models\nwhile preserving the morphological integrity(Zeng et al., 2025; Liu et al., 2025).\nVision-Based Encoding for Time-Series. To align physiological data with the pre-trained VLLMs, recent frameworks\nhave explored directly rendering time-series as waveform line plots with different color representing different channels\nZhang et al. (2025); Liu et al. (2025); He et al. (2025). However, such images are dominated by background, which\nmakes Vision Transformers to inefficiently allocate attention resources to redundant pixels rather than features (Liu et al.,\n2024; Marchetti et al., 2025) Distinct from sparse waveform plots, dense encoding strategies map normalized signal am-\nplitudes directly into pixel intensity grids (Ni et al., 2025; Chen et al., 2024). Such representations preserve the intrinsic\ntime–channel topology while reducing non-informative background regions, resulting in more compact visual inputs. Un-\nder fixed visual token budgets, this increased information density enables more efficient use of visual patches by reducing\nattention allocated to background regions (Endo et al., 2025).\n3. Background\nCharacteristics of EEG Seizure Data. In clinical environments, seizure detection is characterized by extreme class\nimbalance. Previous deep learning methods typically employ the balancing strategy to artificially equilibrate the training\nset (Zhang et al., 2022; Tang et al., 2021; Afzal et al., 2024). However, in previous deep learning models, this strategy\nintroduces a prior shift between training and inference which biases the model toward the positive class. Consequently,\nwhen deployed for test recordings where non-seizure segments are overwhelmingly dominant, such models are prone\nto excessive false positive rates(Lipton et al., 2018; Ingolfsson et al., 2024). Compounding this issue is the challenge\nof channel heterogeneity as clinical EEG recordings frequently feature inconsistent or missing channels as seen in EEG\ndatasets (Shah et al., 2018; Guttag, 2010).\nVisual Representations of EEG Signals. Classical visual representation of EEG is time-frequency transforms such as the\nshort-time Fourier transform (STFT) or Continuous Wavelet Transform (CWT) (Peng et al., 2022; Li et al., 2018; Faust\net al., 2015). While this representation effectively renders non-stationary neural oscillations as distinct visual textures ac-\ncessible to models like CNN, it faces severe scalability issues in clinical settings. Specifically, since STFT or CWT operate\non a single-channel basis, processing a multi-channel EEG recording requires generating independent high-resolution two-\ndimensional images for every channel. Concatenating these spectrograms results in a massive input tensor with excessive\nspatial redundancy (Allen & MacKinnon, 2010). Moreover, the high computational complexity required to compute time-\nfrequency decompositions for multiple channels causes substantial latency, thereby obstructing the deployment of such\nsystems for real-time seizure prediction (Allen & MacKinnon, 2010).\nThis work introduces NeuroCanvas, a novel EEG representation that maps multichannel EEG signals into information-\ndense intensity grids, enabling efficient and morphology-preserving alignment with vision–language models. We begin by\nformally defining the seizure detection task and providing an overview of the framework (Section 3.1). Next, we introduce\nthe Entropy-guided Channel Selector (ECS) module, which identifies the most informative EEG channels for the task based\non their entropy (Section 3.2). Finally, we present our novel CNS module, which visualizes the selected EEG channels on\nthe “canvas” for vision large language mdoel training (Section 3.3). Detailed description about the base model architecture\nis in appendix B\n3.1. Problem Formulation and Framework Overview\nProblem formulation. We formulate the seizure detection task as a binary classification problem over multi-channel EEG\nsignals. D = {(X(i), y(i))}N\ni=1 represents a dataset that includes N EEG segments. Each input X(i) ∈RCi×T represents\na EEG clip with Ci channels and T time steps. The corresponding label y(i) ∈{0, 1} indicates the annotation, where\ny(i) = 1 donates the presence of seizure event and y(i) = 0 donates the normal background (non-seizure).\nFramework overview. The proposed NeuroCanvas operates in three parts. (a) Raw EEG signals are first received by the\nECS module. Spectral entropy is calculated to find the most discriminative EEG channels for seizure detection. The top\nK channels selected by ECS is then fed to the CNS module(Figure 1(a)). (b) The CNS module functions as a universal\nadapter for heterogeneous EEG signals. The selected signals are first normalized and clipped then they are mapped and\nchromatically encoded to generate a ”Visual Canvas”, donated as Ic ∈RH×W ×3, where H and W represent the resolution\nand 3 corresponds to the RGB color channels (Figure 1(b)). (c) Subsequently, the generated ”Visual Canvas” will be input\nPreprint. Under review.\n3\n"}, {"page": 4, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nto a pretrained VLLM for seizure prediction (Figure 1(c)). In this stage, a visual encoder Ev first extract spatial embeddings\nZv = Ev(Ic). These visual embeddings are concatenated with the token embeddings of a text prompt. Eventually, the LLM\nbase model M predicts the seizure detection result.\n3.2. Entropy-guided Channel Selection\nIntensity-Map Construction\nSeizure\nNon-Seizure\n...\nTop-K\nChannels\n(a) Entropy-guided Channel Selector  (ECS)\nRank\nSelected Channels\ng: [0,1] -> [0,1]3\nChannels\nTime\n(Low)\n0\n1\n(High)\nNormalized Activity\nNormalization & Clipping\nIc,t=g(Zc,t)\nCompact CNS Output\nZc,t= \nclip(Xc,t,-A,A)+A\n2A\n[0,1]\n(b) Canvas of Neuron Signal\n...\nVision Encoder\nProjector\nLarge Language Model\nInput image is a 19-Channel EEG\nsignal intensity map ...\n :Prediction\n(c) Network Architecture\nIntensity Map\nFigure 1. Overview of the NeuroCanvas framework.\nChannel selection module identifies and retains a fixed\nsubset of the top-K most discriminative channels based\non the statistical divergence of their spectral entropy be-\ntween seizure and non-seizure states. For each channel\nc in the training set, we first compute the Power Spectral\nDensity (PSD), donated as Pc(f), using the Fast Fourier\nTransform over a 0.5-70 Hz frequency band. The PSD\nis normalized to form a probability distribution ˆPc(f).\nThe spectral entropy Hc is then donated as:\nHc = −\nX\nf\nˆPc(f) log2 ˆPc(f)\n(1)\nTo determine which channels are the most informative,\nwe compute a discriminative score Sc for each chan-\nnel. This score quantifies the separation power of the\nchannel by comparing the distribution of its entropy val-\nues across the seizure and non-seizure labels. Specif-\nically, we calculate the means (µc,seizure, µc,normal) and\nvariance (σ2\nc,seizure, σ2\nc,normal) of the spectral entropy for\neach channel across the training set. The discriminative\nscore Sc is then calculated by a variance-pooled effect\nsize metric:\nSc =\n|µc,seizure −µc,normal|\nr\n1\n2\n\u0010\nσ2\nc,seizure + σ2\nc,normal\n\u0011,\n(2)\nChannels are ranked in order of Sc and We effectively\napply a global mask to retain only the top-K channels\nwith the highest scores. This channel selection ensures\nminimizing inference latency by reducing the visual token budget and directs the encoding focus exclusively toward the\nmost informative channels, thereby facilitating robust detection across heterogeneous EEG inputs.\n3.3. Canvas of Neuron Signal\nDirect tokenization of multi-channel signal like EEG (X ∈RC×T ) represents a scaling challenge. Using numeric embed-\nding layers for LLMs often result in a sequence length that is very large (Liu et al., 2025). Using waveform plots as an\ninput works in some cases (Liu et al., 2025) but the input image is inherently sparse with large amounts of white space.\nFurthermore, as number of channels increases, the overlapping amplitudes make it difficult to distinguish distinct channel.\nAs an alternative to sparse time-domain visualization, frequency domain via Mel Spectrogram construction was proposed\nfor robust audio encoding (Radford et al., 2023). While this approach aims to feed a more rich time-frequency represen-\ntation to the model, it still has some constraints. Firstly, the approach introduces computational overheads of converting\ntime-series data to time-frequency (O(T log T)). Secondly, it still does not solve the issue of feeding multiple channels of\ndata to a LLM or VLLM in an efficient manner. In this method we will need C images for C channels.\nThus, our approach of using CNS attempts to address this issue. The intensity maps will operate in the time domain and\nwill stack multiple channels together, ensuring compact signal representations. Each pixel value will thus directly represent\nthe normalized signal activity.\nNormalization and clipping. Clipping of the EEG signals will allow us to supress outliers while normalizing it will\nstabilize the amplitude distribution. Given a EEG clip X, we apply amplitude clipping and normalization with a fixed\nPreprint. Under review.\n4\n"}, {"page": 5, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nbound:\n˜Xc,t = clip(Xc,t, −A, A), Zc,t =\n˜Xc,t + A\n2A\n∈[0, 1]\n(3)\nHere, Z ∈[0, 1]C×T is a bounded representation whose values can be mapped to pixel intensities.\nIntensity-map construction. We convert Z into an image where the vertical axis corresponds to channels and the hori-\nzontal axis corresponds to time. We first get the grayscale encoding using:\nPc,t = ⌊255 · Zc,t⌉∈{0, . . . , 255}\n(4)\nTo exploit the “third-dimension” capacity of images, we then map each scalar intensity to an RGB triplet via a colormap\ng : [0, 1] →[0, 1]3:\nIc,t,: = g(Zc,t) ∈[0, 1]3,\n(5)\nyielding a compact image I ∈RC×T ×3. To represent the compactness of the information, we define the Information\nDensity (ρ) as the ratio of non-zero feature pixels to the total pixel area H × W. In waveform plots, ρ ≪0.1 due to the\ninherent sparsity of line drawings. In contrast, our Intensity Map achieves ρ ≈1.0. Thus, allowing for a C-fold increase\nin channel capacity without expanding the model’s input dimensionality.\n3.4. Network Architecture.\nThe intensity map from our novel CNS module is encoded by the vision encoder which is then trained using instruction\ntuning according to the system prompt in Appendix C. The frozen LLM decoder then predicts the seizure status.\n4. Experiments\n4.1. Experiment Setup\nWe evaluated NeuroCanvas on Widely used scalp EEG-seizure dataset: TUSZ (Shah et al., 2018) and CHB-MIT (Guttag,\n2010). The details of preprocessing and the datasets is in Appendix D.\nMetrics. To show model’s performance under extreme class imbalance scenario, we report accuracy, precision, recall and\nbinary F1-score (Th¨olke et al., 2023). Notably, we prioritize the binary F1-score over the weighted F1-score often cited in\nprior literature (Afzal et al., 2024; Tang et al., 2021; Zhang et al., 2022). Since the weighted F1-score is heavily dominated\nby the majority class, and this can mask poor performance on the minority seizure class.\nBaselines. We compared NeuroCanvas with SOTA seizure detection models, including REST (Afzal et al., 2024) and\nDCRNN (Tang et al., 2021). Except these latest models, we also included more traditional used deep learning models\nwhich are also widely used for seizure detection: ResNet-LSTM (Lee et al., 2022), CNN-LSTM (Ahmedt-Aristizabal\net al., 2020), Transformer (Vaswani et al., 2017), and GRU (Cho et al., 2014). Additionally, we included a text-augmented\ntime series tasks finetuned LLM: Time-LLM (Jin et al., 2023) to compare our model with the numeric input LLM. To\nensure the rigor and validity of our comparative analysis, we restricted our baselines to methods which are replicable.\nModel training. We implemented NeuroCanvas using the Qwen2.5-VL-7B-Instruct architecture as the backbone founda-\ntion model (Bai et al., 2025). To adapt the visual-linguistic capabilities of the pre-trained model to the domain of EEG\nsignal analysis, we adopted a fine-tuning strategy: the weights of the LLM decoder were frozen, while the vision encoder\nand the cross-modal projector (merger) were kept trainable. Optimization was performed using AdamW with a cosine\ndecay learning rate scheduler. The training process utilized a per-device batch size of 2 with 8 gradient accumulation steps\nand was executed with BF16 precision on 2 NVIDIA H100 GPUs. More details in Appendix.\n4.2. Superior Performance\nTUSZ (Table 1): We observe improvements of {0.4944 - 0.8306, 0.1207 - 0.3580, 0.2128 - 0.5022} in accuracy, precision,\nand binary F1 scores respectively. These improvements underscore the increased seizure detection accuracy of our model.\nThe slightly lower recall score compared to baseline is mitigated by the fact that models with higher recall (CNN-LSTM)\nhave very poor precision. Thus, our model has better precision-recall balance. CHB-MIT(Table 2): The improvements of\nthe model are also showcased in the CHB-MIT dataset with improvements of {0.6623 - 0.9337, 0.0993 - 0.6270, 0.3622\n- 0.7425, 0.1555 - 0.5351} in accuracy, precision, and binary F1 scores respectively. We can see the same pattern for this\nPreprint. Under review.\n5\n"}, {"page": 6, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nTable 1. Seizure detection performance and efficiency on TUSZ dataset.\nModel Performance\nModel Efficiency\nModel\nAccuracy\nPrecision\nRecall\nBinary F1\nInference time (ms)\nParameter Count\nGRU\n0.6434 ± 0.0521\n0.1793 ± 0.0211\n0.6969 ± 0.0596\n0.2845 ± 0.0273\n3.9701 ± 0.0406\n763K\nCNN-LSTM\n0.6528 ± 0.4327\n0.1207 ± 0.0194\n0.9297 ± 0.0504\n0.2128 ± 0.0295\n4.0706 ± 0.0344\n349K\nTransformer\n0.7966 ± 0.0128\n0.2702 ± 0.0138\n0.5896 ± 0.0272\n0.3702 ± 0.0141\n3.918 ± 0.0014\n490K\nResNet-LSTM\n0.7349 ± 0.0448\n0.2564 ± 0.0327\n0.8326 ± 0.0198\n0.3910 ± 0.0359\n2.9263 ± 0.0211\n3M\nDCRNN\n0.7905 ± 0.0588\n0.2643 ± 0.0463\n0.8376 ± 0.0431\n0.4074 ± 0.0546\n7.8728 ± 0.0086\n150K\nREST\n0.7457 ± 0.0358\n0.2418 ± 0.0166\n0.7687 ± 0.0355\n0.3643 ± 0.0150\n1.3480 ± 0.0004\n10K\nTime-LLM\n0.4944 ± 0.1608\n0.1595 ± 0.0358\n0.8805 ± 0.0217\n0.2686 ± 0.0517\n92.031 ± 10.440\n1.1B\nOur Model (19Ch DV)\n0.7961 ± 0.0358\n0.3068 ± 0.0124\n0.8080 ± 0.0225\n0.4438 ± 0.0137\n1054.3 ± 23.71\n7B\nOur Model (19Ch CNS)\n0.8306 ± 0.0212\n0.3580 ± 0.0061\n0.8225 ± 0.0324\n0.5022 ± 0.0119\n126.75 ± 1.251\n7B\nOur Model (8Ch CNS)\n0.7984 ± 0.0183\n0.3182 ± 0.0083\n0.7544 ± 0.0416\n0.4505 ± 0.0142\n50.112 ± 0.8413\n7B\nOur model (4Ch CNS)\n0.7542 ± 0.0141\n0.2734 ± 0.0092\n0.7973 ± 0.0391\n0.4059 ± 0.0125\n32.171 ± 0.6931\n7B\nOur model (2Ch CNS)\n0.7139 ± 0.0098\n0.2414 ± 0.0107\n0.7880 ± 0.0231\n0.3686 ± 0.0104\n19.324 ± 0.6577\n7B\nTable 2. Seizure detection performance and efficiency on CHB-MIT dataset.\nModel Performance\nModel Efficiency\nModel\nAccuracy\nPrecision\nRecall\nBinary F1\nInference time (ms)\nParameter Count\nGRU\n0.6818 ± 0.1057\n0.1830 ± 0.0401\n0.7967 ± 0.0586\n0.2951 ± 0.0501\n3.8462 ± 0.0151\n763K\nCNN-LSTM\n0.7505 ± 0.0834\n0.2410 ± 0.0328\n0.8817 ± 0.0174\n0.3742 ± 0.0450\n4.4932 ± 0.0891\n349K\nTransformer\n0.7606 ± 0.0195\n0.2173 ± 0.0113\n0.6450 ± 0.0569\n0.3318 ± 0.0151\n4.2726 ± 0.0262\n490K\nResNet-LSTM\n0.7583 ± 0.0134\n0.2145 ± 0.0046\n0.7561 ± 0.0047\n0.3342 ± 0.0051\n2.2490 ± 0.0753\n3M\nDCRNN\n0.8574 ± 0.005\n0.3288 ± 0.008\n0.7462 ± 0.002\n0.4564 ± 0.008\n0.9004 ± 0.1812\n150K\nREST\n0.6623 ± 0.1312\n0.1658 ± 0.0492\n0.7028 ± 0.1522\n0.2608 ± 0.0471\n19.736 ± 6.0947\n10K\nTime-LLM\n0.6838 ± 0.0392\n0.0993 ± 0.0056\n0.3622 ± 0.0385\n0.1555 ± 0.0040\n34.496 ± 1.2309\n1.1B\nOur Model (16Ch DV)\n0.8710 ± 0.0084\n0.3636 ± 0.0039\n0.6612 ± 0.0091\n0.4692 ± 0.0059\n752.44 ± 4.012\n7B\nOur Model (16Ch CNS)\n0.8885 ± 0.0072\n0.3954 ± 0.0056\n0.7372 ± 0.0103\n0.5146 ± 0.0059\n107.75 ± 1.114\n7B\nOur Model (8Ch CNS)\n0.9337 ± 0.0130\n0.6270 ± 0.0067\n0.4282 ± 0.0142\n0.5089 ± 0.092\n57.91 ± 0.9934\n7B\nOur model (4Ch CNS)\n0.8965 ± 0.0207\n0.4183 ± 0.0042\n0.7425 ± 0.0183\n0.5351 ± 0.092\n31.65 ± 0.7826\n7B\nOur model (2Ch CNS)\n0.8487 ± 0.0286\n0.2884 ± 0.0075\n0.6043 ± 0.0204\n0.3905 ± 0.123\n19.41 ± 0.6137\n7B\ndataset as well: baseline models with higher recall have very poor precision while our model maintains a better precision-\nrecall balance.\n4.3. Indepth Model Analysis\nImportance of CNS. We encode the raw waveforms into tokens in the Time-LLM framework(Jin et al., 2023). Using CNS\nimage representations, instead of encoded raw waveforms (Jin et al., 2023) lead to an improvement of binary F1 score of\n87% and 231% in TUSZ and CHB-MIT dataset respectively. This shows that the CNS encoding offers a rich, informative\nvisual input that pretrained vision models can easily understand, unlike token sequences. Furthermore, the proposed CNS\nimage representation produces much better results than a direct time-series image representation of the EEG. Compared\nto using time-series image approach (Liu et al., 2025), our model achieved a relative improvement of 13% and 10% in\nTUSZ and CHB-MIT datasets respectively. This highlights the effectiveness of using CNS instead of a lineplot as an input\nto VLLM.\nTable 3. Ablation experiment results on the TUSZ dataset.\nModel\nAcc.\nPrec.\nRec.\nF1\nNeuroCanvas (8 highest ch, HT)\n0.7984 ± 0.0183\n0.3182 ± 0.0083\n0.7544 ± 0.0416\n0.4505 ± 0.0142\nNeuroCanvas (8 lowest ch, HT)\n0.7705 ± 0.0129\n0.2990 ± 0.0048\n0.8319 ± 0.0732\n0.4353 ± 0.0126\nNeuroCanvas (19Ch HT, qwen2.5-7B)\n0.8306 ± 0.0212\n0.3580 ± 0.0061\n0.8225 ± 0.0324\n0.5022 ± 0.0119\nNeuroCanvas (19Ch HT, qwen2.5-3B)\n0.8287 ± 0.0153\n0.3473 ± 0.0054\n0.7909 ± 0.0218\n0.4827 ± 0.0096\nPreprint. Under review.\n6\n"}, {"page": 7, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nLayer-wise Pearson correlation coefficients\nChannel-wise Attention Comparison\nLayer Index\nPearson R\nAttention Score\nEEG Channel\n(a)\n(b)\nFigure 3. (a) Layer-wise Pearson correlation coefficients between ”CNS” and Direct Time-series. (b) Comparison of average attention\nscores across 19 EEG channels.\nCNS Intensity Map\nDirect Visualization\nModel Input\nAttention Heatmap\nFigure 2. Attention heatmap on CNS intensity map and direct time series\nvisualization figure.\nEffect of ECS. In realistic scenarios, not all EEG\nchannels may be available or helpful. Thus, we\ntested our model with fewer channels, using the\nECS to choose the most informative electrodes.\nWith only the top 8 channels, our model achieved\nbinary F1 score higher than feeding lineplot to\nthe VLLM. Even with 2 channels, our method\nachieved 73% of the peak binary F1 score. Thus\nshowcasing that carefully selected channels can\nbe used to effectively detect seizure. On top of\nthat, using just 4 channels even gave the best\nresult in CHB-MIT. This outcome may be at-\ntributed to the bipolar EEG montage employed\nin the dataset, which emphasizes localized differ-\nential activity and reduces common-mode noise,\nthereby allowing a smaller set of channels to re-\nmain highly discriminative.\nModel Efficiency. Even with over six times more\nparameters, our model achieved better inference time than Time-LLM at 8ch (TUSZ) and 4ch (CHB-MIT). Despite Time-\nLLM using patch tokenizer to encode the input signals, CNS and ECS allowed our model to outperform it.\nAttention heatmap visualization. To validate that our our CNS enhances the model’s ability to more efficiently capture\nuseful information in the EEG representation, we visualize the attention score heatmap maps of the VLLM’s vision encoder\n(Figure 2). In the direct visualization (right), the visual attention weights are sparse and frequently scattered across non-\nPreprint. Under review.\n7\n"}, {"page": 8, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\ninformative background regions, as circled in the red blanket (Bad attention). We attribute this failure to the inherent visual\nsparsity of waveform line plots, normal Vision Transformers processes images via patch embeddings, which struggles to\nextract robust semantic features from patches dominated by whitespace with only minor line signals (Dosovitskiy, 2020).\nThe lack of textural coherency prevents the model from anchoring its attention on the physiological signal, leading to\nstochastic focus. In contrast, the CNS representation (left) effectively reformulates the seizure detection task into a visual\nobject recognition problem. By encoding signal amplitude into chromatic intensity, CNS transforms transient seizure events\ninto salient spatiotemporal representations. As observed in the heatmap, the VLLM’s visual attention is accurately directed\ntoward signal regions strongly correlated with seizure detection.\n-95.56%\nInput Token Comparision\nFigure 4. Number of input tokens across EEG representation\npipelines. Compared with numeric and direct visualization in-\nputs, CNS achieves up to a 95.56% reduction in input tokens.\nLayer-wise Pearson correlation analysis. To find out where\nalong the vision encoder the two EEG representation meth-\nods begin to induce qualitatively different channel evidence,\nwe compute a layer-wise Pearson R bar plot between CNS\nand direct visualization (Figure 3a). The resulting trend pro-\nvides evidence that CNS changes not only what regions are\nattended, but also when representation-dependent channel pri-\noritization emerges in the encoder hierarchy. Specifically, the\npositive correlations in earlier layers suggest that both ren-\nderings share a common, representation agnostic stage where\nthe model recovers coarse channel localization and allocates\nattention in broadly consistent ways.\nHowever, later lay-\ners show near-zero or negative correlations, indicating a re-\nranking of channel importance where CNS and visualization\ninputs emphasize different channels, consistent with Figure 2.\nThis layer-dependent divergence is consistent with the gen-\neral principle that earlier layers tend to encode more generic,\ntransferable features, whereas deeper layers become increas-\ningly specialized to input statistics and task-relevant abstractions (Li et al., 2024; Raghu et al., 2021). Importantly, the\nnegative tail bars argues against interpreting CNS as a cosmetic reparameterization. Instead, it functions as an inductive\nbias that reorganizes how channel evidence is composed downstream. While attention-based explanations must be inter-\npreted cautiously, attention flow is explicitly designed to trace how saliency propagates through transformer depth (Abnar\n& Zuidema, 2020), making the observed late-layer inversion a meaningful signal of representational bifurcation rather than\na visualization artifact.\nAttention score analysis. We further quantify the attention distribution on a channel-level. Figure 3(b) shows that CNS\nproduces a consistently higher and more discriminative attention allocation in most of the 19 channels. This indicates\nthat CNS helps the model spend its limited visual capacity on channel-discriminative evidence rather than distributing\nattention uniformly over visually redundant regions. Additionally, visual attention of CNS concentrates on a smaller subset\nof channels (e.g. FP1, FP2), and this concentration indicates improved channel selectivity, suggesting that the model is\nless likely to diffuse its capacity across weakly informative channels and more likely to leverage discriminative channels.\nFrom a neuro signal perspective, this behavior is also consistent with the fact that seizure events can behave in spatially\nnon-uniform patterns across channels, where only part of the image provides strong discriminative evidence at a certain\ntime (Fisher et al., 2017).\nToken compression efficiency. Beyond improving attention allocation, CNS substantially reduces the effective input\nlength presented to the VLLM. As shown in Figure 4 direct visualization consumes 9,000 input visual tokens due to over\nlarge, mostly redundant canvases. In contrast, CNS compresses the input by converting the multi-channel window into a\ncompact, information-dense image, reducing token usage to ∼3,000 for 19 channels and further to 1,350/700/400 tokens\nwhen retaining 8/4/2 channels, respectively up to a 95.56% input token reduction.\nPreprint. Under review.\n8\n"}, {"page": 9, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nChannel\nAttention Score\nAttention Score\nAttention Score\nAttention Score\nTP v.s. TP\nTP v.s. FN\nFN v.s. TP\nFN v.s. FN\nFigure 5. Why CNS helps or fails, the attention distribution analysis. The “TP” denotes the true positive, and the “FN” means the false\npositive. The blue represents the Direct Visualization, and the red denotes CNS.\nAttention distribution analysis for CNS.\nTo better understand how the CNS module contributes to predictions, we analyze the attention weights assigned to each\nEEG channel by CNS, which converts EEG signals into intensity maps, and compare them to the Direct Visualization\nmethod, which converts EEG signals into individual sub-figures. As shown in Figure 5, the analysis is conducted across\nfour scenarios: “TP v.s. TP”, “TP v.s. FN”, “FN v.s. TP”, and “FN v.s. FN”. ❶TP v.s. TP: CNS consistently achieves\nhigher attention scores than the Direct Visualization method. This indicates that CNS improves the signal-to-background\nratio, stabilizing attention allocation while maintaining the underlying channel preference. ❷TP v.s. FN: In cases where\nCNS correctly predicts while the Direct Visualization method fails, CNS allocates lower attention weights to specific chan-\nnels, enabling it to avoid failure modes caused by waveform plots inducing attention toward irrelevant regions. ❸FN v.s.\nPreprint. Under review.\n9\n"}, {"page": 10, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nTP: CNS struggles to capture critical channels, such as FP1, in waveform morphology-dependent samples. This suggests\nthat, for samples relying heavily on waveform shape, the intensity map compresses essential information. Further work is\nneeded to enhance the representation of such channels within the intensity map. ❹FN v.s. FN: The attention weight dis-\ntribution for this scenario resembles the pattern observed in ”TP v.s. TP.” This behavior may be attributed to issues in data\ndistribution or label noise, which warrant additional investigation. Overall, this analysis highlights the strengths of CNS in\nimproving attention allocation and avoiding failure modes, while identifying limitations in handling waveform-dependent\nsamples. Future research should explore strategies to better preserve critical channel information in intensity maps.\n4.4. Ablation Experiment\nEffectiveness of Entropy-Guided Channel Selection. To verify the effectiveness of ECS, we compared our proposed\nstrategy (selecting the Top-8 channels with the highest discriminative spectral entropy scores) against a counter-factual\nbaseline that selects the 8 channels with the lowest scores. As shown in Table 3, the model employing the Top-8 channels\nwith the highest spectral entropy secures superior performance, achieving relative improvements of 3.62% in accuracy and\n3.49% in binary F1-score. This result indicates that spectral entropy acts as an effective metric for channel discriminabil-\nity. Prioritizing high-entropy channels preserves more informative spatiotemporal dynamics for CNS, which translates into\nbetter overall detection quality. Notably, using low spectral entropy score channels attains higher recall but but substan-\ntially lower precision, suggesting that selecting low-entropy channels biases the model toward a more permissive decision\nboundary and increases false alarms.\nImpact of Base Model. We further investigate the impact of the base model by deploying NeuroCanvas with Qwen2.5-\nVL-7B and Qwen2.5-VL-3B under the same 19-channel CNS setting (Table 3). Both settings achieve comparable accuracy,\nindicating that our CNS representation are not strongly reliant on model scale. Nevertheless, the 7B model consistently\nprovides stronger precision and a higher binary F1 score, reflecting improved calibration and fewer false positives at\nsimilar recall. These findings suggest that while a larger base model relatively elevates the performance, the performance\nimprovements from NeuroCanvas are largely driven by the proposed CNS and will remain robust even when the base model\nis downsized.\n5. Conclusion and Future Work\nIn this work, we introduced NeuroCanvas, a novel framework detecting seizure by transforming multi-channel EEG signals\ninto information-dense intensity map visual representations tailored for VLLMs. Our approach includes two key compo-\nnents: CNS and ECS to address the critical challenges of multi-channel heterogeneity and computational inefficiency in\nseizure detection. Extensive experiments on the TUSZ and CHB-MIT datasets demonstrate the superiority of NeuroCan-\nvas. Our model achieves a binary F1-score of 0.5022 on the challenging TUSZ benchmark, outperforming SOTA baselines\nby over 20%. It also reduces 88% of inference latency compare to traditional EEG visual representations. Future work\nwill focus on optimizing NeuroCanvas for deployment on resource-constrained medical devices. Additionally, we plan to\nleverage reinforcement learning to further enhance the model’s reasoning capabilities and diagnostic precision.\nPreprint. Under review.\n10\n"}, {"page": 11, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nImpact Statement\nThe datasets utilized in this research, the Temple University Hospital Seizure Corpus (TUSZ) and the CHB-MIT Scalp\nEEG Database, are anonymized and publicly accessible resources that adhere to ethical standards for patient privacy (Shah\net al., 2018; Guttag, 2010). The authors declare no conflicts of interest, and the methodology presented does not generate\nharmful insights. NeuroCanvas demonstrates a significant reduction in inference latency and improved performance in the\nseizure detection domain. This research highlights the potential of foundation models to democratize access to high-quality\nneurological monitoring across diverse clinical settings.\nAcknowledgement\nThis research was partially funded by the National Institutes of Health (NIH) under award 1R01EB03710101. The views\nand conclusions contained in this document are those of the authors and should not be interpreted as representing the\nofficial policies, either expressed or implied, of the NIH.\nReferences\nAbnar, S. and Zuidema, W. Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928, 2020.\nAcharya, U. R., Oh, S. L., Hagiwara, Y., Tan, J. H., and Adeli, H. Deep convolutional neural network for the automated\ndetection and diagnosis of seizure using eeg signals. Computers in biology and medicine, 100:270–278, 2018.\nAfzal, A., Chrysos, G., Cevher, V., and Shoaran, M. Rest: Efficient and accelerated eeg seizure analysis through residual\nstate updates. arXiv preprint arXiv:2406.16906, 2024.\nAhmedt-Aristizabal, D., Fernando, T., Denman, S., Petersson, L., Aburn, M. J., and Fookes, C. Neural memory networks\nfor seizure type classification. In 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &\nBiology Society (EMBC), pp. 569–575. IEEE, 2020.\nAllen, D. P. and MacKinnon, C. D. Time–frequency analysis of movement-related spectral power in eeg during repetitive\nmovements: A comparison of methods. Journal of neuroscience methods, 186(1):107–115, 2010.\nBai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical\nreport. arXiv preprint arXiv:2502.13923, 2025.\nChen, M., Shen, L., Li, Z., Wang, X. J., Sun, J., and Liu, C. Visionts: Visual masked autoencoders are free-lunch zero-shot\ntime series forecasters. arXiv preprint arXiv:2408.17253, 2024.\nCho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase\nrepresentations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nDevinsky, O., Hesdorffer, D. C., Thurman, D. J., Lhatoo, S., and Richerson, G. Sudden unexpected death in epilepsy:\nepidemiology, mechanisms, and prevention. The Lancet Neurology, 15(10):1075–1088, 2016.\nDosovitskiy, A.\nAn image is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\nEmami, A., Kunii, N., Matsuo, T., Shinozaki, T., Kawai, K., and Takahashi, H. Seizure detection by convolutional neural\nnetwork-based analysis of scalp electroencephalography plot images. NeuroImage: Clinical, 22:101684, 2019.\nEndo, M., Wang, X., and Yeung-Levy, S. Feather the throttle: Revisiting visual token pruning for vision-language model\nacceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22826–22835, 2025.\nFaust, O., Acharya, U. R., Adeli, H., and Adeli, A. Wavelet-based eeg processing for computer-aided seizure detection and\nepilepsy diagnosis. Seizure, 26:56–64, 2015.\nFisher, R. S., Cross, J. H., French, J. A., Higurashi, N., Hirsch, E., Jansen, F. E., Lagae, L., Mosh´e, S. L., Peltola, J.,\nRoulet Perez, E., et al. Operational classification of seizure types by the international league against epilepsy: Position\npaper of the ilae commission for classification and terminology. Epilepsia, 58(4):522–530, 2017.\nPreprint. Under review.\n11\n"}, {"page": 12, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nGuttag, J. Chb-mit scalp eeg database (version 1.0. 0). PhysioNet, 2010.\nHe, Z., Alnegheimish, S., and Reimherr, M. Harnessing vision-language models for time series anomaly detection. arXiv\npreprint arXiv:2506.06836, 2025.\nIngolfsson, T. M., Benatti, S., Wang, X., Bernini, A., Ducouret, P., Ryvlin, P., Beniczky, S., Benini, L., and Cossettini,\nA. Minimizing artifact-induced false-alarms for seizure detection in wearable eeg devices with gradient-boosted tree\nclassifiers. Scientific reports, 14(1):2980, 2024.\nJiang, W.-B., Wang, Y., Lu, B.-L., and Li, D. Neurolm: A universal multi-task foundation model for bridging the gap\nbetween language and eeg signals. arXiv preprint arXiv:2409.00101, 2024.\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., et al. Time-llm: Time\nseries forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.\nKim, J. W., Alaa, A., and Bernardo, D. Eeg-gpt: exploring capabilities of large language models for eeg classification and\ninterpretation. arXiv preprint arXiv:2401.18006, 2024.\nKotoge, R., Chen, Z., Kimura, T., Matsubara, Y., Yanagisawa, T., Kishima, H., and Sakurai, Y. Evobrain: Dynamic\nmulti-channel eeg graph modeling for time-evolving brain networks. arXiv preprint arXiv:2509.15857, 2025.\nLangley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207–1216, Stanford, CA, 2000. Morgan Kaufmann.\nLee, K., Jeong, H., Kim, S., Yang, D., Kang, H.-C., and Choi, E. Real-time seizure detection using eeg: a comprehensive\ncomparison of recent approaches under a realistic setting. arXiv preprint arXiv:2201.08780, 2022.\nLi, A. C., Tian, Y., Chen, B., Pathak, D., and Chen, X. On the surprising effectiveness of attention transfer for vision\ntransformers. Advances in Neural Information Processing Systems, 37:113963–113990, 2024.\nLi, Y., Cui, W., Luo, M., Li, K., and Wang, L. Epileptic seizure detection based on time-frequency images of eeg signals\nusing gaussian mixture model and gray level co-occurrence matrix features. International journal of neural systems, 28\n(07):1850003, 2018.\nLipton, Z., Wang, Y.-X., and Smola, A. Detecting and correcting for label shift with black box predictors. In International\nconference on machine learning, pp. 3122–3130. PMLR, 2018.\nLiu, H., Liu, C., and Prakash, B. A. A picture is worth a thousand numbers: Enabling llms reason about time series via\nvisualization. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 7486–7518, 2025.\nLiu, Y., Gehrig, M., Messikommer, N., Cannici, M., and Scaramuzza, D. Revisiting token pruning for object detection and\ninstance segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.\n2658–2668, 2024.\nLu, W., Song, C., Wu, J., Zhu, P., Zhou, Y., Mai, W., Zheng, Q., and Ouyang, W. Unimind: Unleashing the power of llms\nfor unified multi-task brain decoding. arXiv preprint arXiv:2506.18962, 2025.\nMarchetti, M., Traini, D., Ursino, D., and Virgili, L. Efficient token pruning in vision transformers using an attention-based\nmultilayer network. Expert Systems with Applications, 279:127449, 2025.\nMerrill, M. A., Tan, M., Gupta, V., Hartvigsen, T., and Althoff, T. Language models still struggle to zero-shot reason about\ntime series. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 3512–3533, 2024.\nNi, J., Zhao, Z., Shen, C., Tong, H., Song, D., Cheng, W., Luo, D., and Chen, H. Harnessing vision models for time series\nanalysis: A survey. arXiv preprint arXiv:2502.08869, 2025.\nO’Shea, A., Lightbody, G., Boylan, G., and Temko, A. Neonatal seizure detection from raw multi-channel eeg using a\nfully convolutional architecture. Neural Networks, 123:12–25, 2020.\nPreprint. Under review.\n12\n"}, {"page": 13, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nPeng, P., Song, Y., Yang, L., and Wei, H. Seizure prediction in eeg signals using stft and domain adaptation. Frontiers in\nNeuroscience, 15:825434, 2022.\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via large-scale\nweak supervision. In International conference on machine learning, pp. 28492–28518. PMLR, 2023.\nRaghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A. Do vision transformers see like convolutional\nneural networks? Advances in neural information processing systems, 34:12116–12128, 2021.\nRamgopal, S., Thome-Souza, S., Jackson, M., Kadish, N. E., Fern´andez, I. S., Klehm, J., Bosl, W., Reinsberger, C.,\nSchachter, S., and Loddenkemper, T. Seizure detection, seizure prediction, and closed-loop warning systems in epilepsy.\nEpilepsy & behavior, 37:291–307, 2014.\nRasheed, K., Qayyum, A., Qadir, J., Sivathamboo, S., Kwan, P., Kuhlmann, L., O’Brien, T., and Razi, A. Machine\nlearning for predicting epileptic seizures using eeg signals: A review. IEEE reviews in biomedical engineering, 14:\n139–155, 2020.\nShah, V., Von Weltin, E., Lopez, S., McHugh, J. R., Veloso, L., Golmohammadi, M., Obeid, I., and Picone, J. The temple\nuniversity hospital seizure detection corpus. Frontiers in neuroinformatics, 12:83, 2018.\nSingh, G. and Sander, J. W. The global burden of epilepsy report: Implications for low-and middle-income countries.\nEpilepsy & Behavior, 105:106949, 2020.\nTalathi, S. S. Deep recurrent neural networks for seizure detection and early seizure detection systems. arXiv preprint\narXiv:1706.03283, 2017.\nTang, S., Dunnmon, J. A., Saab, K., Zhang, X., Huang, Q., Dubost, F., Rubin, D. L., and Lee-Messer, C. Self-supervised\ngraph neural networks for improved electroencephalographic seizure analysis. arXiv preprint arXiv:2104.08336, 2021.\nTh¨olke, P., Mantilla-Ramos, Y.-J., Abdelhedi, H., Maschke, C., Dehgan, A., Harel, Y., Kemtur, A., Berrada, L. M.,\nSahraoui, M., Young, T., et al. Class imbalance should not throw you off balance: Choosing the right classifiers and\nperformance metrics for brain decoding with imbalanced data. NeuroImage, 277:120253, 2023.\nTsiouris, K. M., Pezoulas, V. C., Zervakis, M., Konitsiotis, S., Koutsouris, D. D., and Fotiadis, D. I. A long short-term\nmemory deep learning network for the prediction of epileptic seizures using eeg signals. Computers in biology and\nmedicine, 99:24–37, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all\nyou need. Advances in neural information processing systems, 30, 2017.\nWHO.\nEpilepsy, February 2024.\nURL https://www.who.int/news-room/fact-sheets/detail/\nepilepsy. Fact sheet.\nZeng, Z., Cai, Z., Cai, Y., Wang, X., Chen, J., Wang, R., Liu, Y., Cai, S., Wang, B., Zhang, Z., et al. Wavemind: Towards a\nconversational eeg foundation model aligned to textual and visual modalities. arXiv preprint arXiv:2510.00032, 2025.\nZhang, J., Feng, L., Guo, X., Wu, Y., Dong, Y., and Xu, D. Timemaster: Training time-series multimodal llms to reason\nvia reinforcement learning. arXiv preprint arXiv:2506.13705, 2025.\nZhang, Y., Yao, S., Yang, R., Liu, X., Qiu, W., Han, L., Zhou, W., and Shang, W. Epileptic seizure detection based on\nbidirectional gated recurrent unit network. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:\n135–145, 2022.\nPreprint. Under review.\n13\n"}, {"page": 14, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nAppendix\nA. Entropy scores of channels\nFigure 6. Entropy scores calculted for (a) TUSZ (b) CHB-MIT dataset\nFigure 6 shows the entropy scores calculated for TUSZ and CHB-MIT dataset. Interestingly, the frontopolar channels\n(FP1, FP2) are quite discriminative for TUSZ. On the other hand, the bipolar recordings of CHB-MIT show that ”FP1-F7”\nand ”F7-T7” are the most discriminative.\nB. Model Architecture\nFor different visual representations, they have different resolutions. In direct visualization, multichannel EEG is rendered\nas waveform plot. To preserve readability, the input image is scaled with the number of channels and the desired amplitude.\nPreprint. Under review.\n14\n"}, {"page": 15, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nIn CNS canvas, since the amplitude of the signal has been encoded as color, the resolution of the CNS canvas is only related\nto the number of input channels. Detailed resolutions of each types of visual representation are in Appendix (Table 4).\nTable 4. Resolution comparison between Direct Visualization and CNS across different channel settings.\nVisual Representation\nResolution (Width × Height)\nTUSZ\nDirect Visualize 19 Channels\n1538 × 4650\nCNS 19 Channels\n1464 × 860\nCNS 8 Channels\n1464 × 379\nCNS 4 Channels\n1464 × 204\nCNS 2 Channels\n1464 × 111\nCHB-MIT\nDirect Visualize 16 Channels\n1538 × 3914\nCNS 16 Channels\n1464 × 729\nCNS 8 Channels\n1464 × 379\nCNS 4 Channels\n1464 × 204\nCNS 2 Channels\n1464 × 111\nNeuroCanvas is deployed on a pretrained base VLLM with three standard blocks: (i) a vision encoder Ev that converts\nan input image into a sequence of visual tokens, (ii) a cross-modal projector P that maps visual features into the LLM\nhidden space, and (iii) a LLM decoder M that performs conditional generation given visual and text tokens. Given a\ncanvas image Ic, the vision encoder extracts spatial embeddings Zv = Ev(Iv), which are then projected into the language\nspace ˜Zv = P(Zv). In parallel, a task prompt p (Details in Appendix C) is tokenized and embedded as Zt = Emb(p). The\ndecoder then consumes the concatenation of cross-modal tokens and predicts the output.\nC. Prompt for training\nWe cast seizure detection as prompt-guided binary decision. Concretely, the model is prompted to generate a short answer\ntoken from a constrained label set {Seizure, Non-seizure}).\nExample Prompt: ”Question: Input image is a 19-Channel EEG signal intensity map, the color refers to the amplitude of\nthe EEG, is the image represents Seizure or Non-seizure? \\n Options: \\n 1. Seizure \\n 2. Non-seizure”.\nD. Details of Preprocessing and Dataset Splitting\nTUSZ. In v2.0.3 release, TUSZ contains 315 subjects and 1643 recording sessions. All recordings utilize 19 channels in\nstandard 10-20 system. Importantly, TUSZ reflects real clinical imbalance: the corpus includes over 504 hours of annotated\nEEG, with seizure activity comprising about 36 hours (around 7%) CHB-MIT. This dataset contains recordings from 22\nsubjects, each having between 9 and 42 sessions. Due to inconsistencies in the available channels for some subjects, we\nutilized the 16 channels in standard 10-20 system that are present in all subjects.\nPreprocessing. To stay consistent with previous studies (Tang et al., 2021; Afzal et al., 2024; Zhang et al., 2022), we\nresampled the EEG signals into 200Hz for TUSZ dataset and 256Hz for CHB-MIT dataset. The continuous EEG recordings\nare then segmented into non-overlapping clips with a fixed duration of T = 12 seconds. We selected this 12-second window\nsize based on prior empirical benchmarks, which demonstrate that it provides the optimal trade-off between capturing\nsufficient temporal context for seizure detection and latency for the model inference (Tang et al., 2021; Afzal et al., 2024).\nDataset splitting. For the TUSZ dataset, we followed the officially defined data partitions. The original provided training\nset was randomly split into training and validation subsets with a 90/10 ratio, while the official evaluation set served as\nthe standardized test set. For the CHB-MIT dataset, since no predefined splits are provided, we randomly divided the data\ninto approximately 80% for training, 10% for validation, and 10% for testing. To ensure robust evaluation, this splitting\nwas performed on a subject level, preventing the model from being tested on patients included in the training set. Detailed\nstatistics regarding the data distribution across splits are provided in the Appendix. For training set of both TUSZ and\nCHB-MIT, we employed random under-sampling on the non-seizure class so that the number of non-seizure clips matched\nPreprint. Under review.\n15\n"}, {"page": 16, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nthe number of seizure clips. The test sets remained imbalanced to reflect real imbalanced scenarios.\nE. Computing Layer-wise Pearson Correlations\nFor each layer ℓ, we derive an attention-induced saliency over visual tokens using attention rollout up to ℓ, then project\ntoken scores back to a two-dimensional map using the model’s image placeholder mask and the image grid metadata. We\nsubsequently aggregate the saliency map into an attention vector by averaging within each channel’s spatial band. Finally,\nwe compute Pearson’s r between the two attention vectors (CNS vs. direct visualization) per layer and report the mean\nacross paired samples.\nF. Further Analysis of Case Study\nTo better understand how CNS changes model behavior beyond aggregate metrics, we conduct a case study over four\noutcome conditions defined by the detection correctness under CNS versus direct visualization: (1) TP/TP, (2) TP/FN, (3)\nFN/FN, and (4) FN/TP. To ensure statistical representativeness, we randomly sampled 100 instances for each outcome\ncondition and conducted the analysis based on their averaged attention profiles. For each condition, we inspected the mean\nattention score per layer (averaged across channels), and the attention distribution on each channel. We use these analyses\nas a diagnostic lens to characterize the routing of representation-dependent evidence through the vision encoder.\nCondition 1: TP/TP. In condition where both models correctly detect the seizure, the channel-wise attention distribution\nis strongly aligned (Pearson’s r = 0.732, p < 0.001). This correlation indicates that both representations prioritize a\nsimilar subset of channels; however, CNS consistently assigns higher attention magnitudes across most channels. Layer-\nwise analysis corroborates this, as CNS maintains a higher mean attention score, whereas the direct pipeline remains lower\nthroughout. The combination of high cross-representation alignment and stronger attention suggests that in this condition,\nCNS acts as an amplifier rather than altering the fundamental evidence path. It does not change what evidence is used\nwhen the waveform already exposes discriminative cues, but rather increases the saliency of that shared evidence, making\nit accessible earlier in the hierarchy. Practically, this implies that CNS improves the signal-to-background ratio, stabilizing\nattention allocation without shifting the underlying channel preference that drives correct decisions.\nCondition 2: TP/FN. In condition where CNS succeeds but the direct visualization fails is characterized by a systematic\ndisagreement in channel prioritization (Pearson’s r = −0.405, p = 0.085). The two representations induce partially\ninverted channel-wise attention distribution, indicating CNS effectively performs evidence re-ranking. Notably, the direct\nvisualization allocates substantial attention mass to a broad set of channels, which often emphasizes posterior or midline\nregions. However, it fails to detect the seizure. In contrast, CNS corrects this representation-induced bias by concentrating\nattention on a distinct subset of channels. Layer-wise analysis show that direct visualization maintains relatively high\nmean attention across many mid-layers. This supports that direct visualization method faces misallocated search without\ncorrectly deetcting the seizure. On the other hand, CNS exhibits very strong early-layer attention followed by a rapid\ndecline, consistent with the earlier localization of discriminative evidence. This indicates that CNS can avoid failure modes\nwhere waveform plots induce attention toward visually salient but diagnostically unhelpful structures.\nCondition 3: FN/TP. In condition where CNS fails while the direct visualization succeeds, the channel profiles are nearly\nuncorrelated (Pearson’s r = −0.064, p = 0.796), indicating their reliance on fundamentally different channel preferences.\nAlthough CNS again shows substantially higher mean attention across early layers, this increased focus does not translate\ninto a correct detection, demonstrating that greater attention magnitude is not equivalent to better evidence quality. Accord-\ning to the distribution of attention on channels, the CNS strongly allocates attention to central channels, while the direct\nvisualization places relatively more attention on front channels. This divergence suggests that the discriminative evidence\nfor these samples depends on waveform morphology (e.g. transient sharpness or brief rhythmic evolution), that remains\nvisible in line plots but becomes distorted during amplitude-to-color encoding.\nCondition 4:FN/FN. In condition where both CNS and direct visualization fail to detect the seizure, channel-wise attention\nshows relatively strong agreement between CNS and direct visualization (Pearson’s r = 0.418, p = 0.075). While\nboth representations shows weak concentration on certain channels, CNS spreads high attention across a broad range of\nelectrodes. Layer-wise, CNS maintains consistent high attention, whereas direct visualization remains lower and flatter.\nThis pattern reflects a condition that neither representation provides sufficiently separable evidence within the signal clip.\nThis might be caused by signal-noise ratio, weak seizure morphology, or strong non-seizure confounds. In this context,\nattention fails to concentrate on a discriminative channel subset. The broader and higher attention score observed in CNS\nPreprint. Under review.\n16\n"}, {"page": 17, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nLayer Index\nAttention Score\nAttention Score\nAttention Score\nAttention Score\nTP v.s. TP\nTP v.s. FN\nFN v.s. TP\nFN v.s. FN\nFigure 7. Why CNS helps or fails, the attention distribution analysis in Layer-wise. The “TP” denotes the true positive, and the “FN”\nmeans the false positive. The blue represents the Direct Visualization, and the red denotes CNS.\nPreprint. Under review.\n17\n"}, {"page": 18, "text": "NeuroCanvas: VLLM-Powered Robust Seizure Detection by Reformulating Multichannel EEG as Vision\nis consistent with the model trying hard but failing to resolve a clear decision boundary, highlighting that representation\nimprovements alone cannot compensate for segments with intrinsically low evidence or label noise.\nPreprint. Under review.\n18\n"}]}