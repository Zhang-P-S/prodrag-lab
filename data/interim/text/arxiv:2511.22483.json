{"doc_id": "arxiv:2511.22483", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.22483.pdf", "meta": {"doc_id": "arxiv:2511.22483", "source": "arxiv", "arxiv_id": "2511.22483", "title": "Enhancing Trustworthiness with Mixed Precision: Benchmarks, Opportunities, and Challenges", "authors": ["Guanxi Lu", "Hao Mark Chen", "Zhiqiang Que", "Wayne Luk", "Hongxiang Fan"], "published": "2025-11-27T14:17:43Z", "updated": "2025-11-27T14:17:43Z", "summary": "Large language models (LLMs) have shown promising performance across various tasks. However, their autoregressive decoding process poses significant challenges for efficient deployment on existing AI hardware. Quantization alleviates memory and compute pressure by compressing weights, activations, and KV caches to low precisions while preserving generation quality. However, existing quantization frameworks typically focus on perplexity or classification accuracy, often omitting critical trustworthiness metrics. This gap introduces risks when applying quantized LLMs to downstream high-stakes domains such as finance and healthcare. In this work, we systematically investigate the impact of quantization on four trustworthiness metrics (adversarial robustness, fairness, machine ethics, and out-of-distribution robustness) and identify the instability across compression ratios and quantization methods. Building on these observations, we develop a novel precision-ensemble voting approach that leverages predictions from mixed-precision variants of the same model and consistently improves performance by up to $5.8\\%$ on trustworthiness metrics. Our results highlight the importance of considering trustworthiness when developing model compression techniques and point to research opportunities at the intersection of compression and trustworthiness for safety-critical applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.22483v1", "url_pdf": "https://arxiv.org/pdf/2511.22483.pdf", "meta_path": "data/raw/arxiv/meta/2511.22483.json", "sha256": "93a6147f26cbd115376576282362a30c6e75fe0b13cd435b2d6a342e5afc66ff", "status": "ok", "fetched_at": "2026-02-18T02:25:57.137716+00:00"}, "pages": [{"page": 1, "text": "Enhancing Trustworthiness with Mixed Precision:\nBenchmarks, Opportunities, and Challenges\nGuanxi Lu\nDepartment of Electrical and\nElectronic Engineering\nImperial College London\nguanxi.lu22@imperial.ac.uk\nHao (Mark) Chen\nDepartment of Computing\nImperial College London\nhao.chen20@imperial.ac.uk\nZhiqiang Que\nDepartment of Computing\nImperial College London\nz.que@imperial.ac.uk\nWayne Luk\nDepartment of Computing\nImperial College London\nw.luk@imperial.ac.uk\nHongxiang Fan\nDepartment of Computing\nImperial College London\nhongxiang.fan@imperial.ac.uk\nAbstract—Large language models (LLMs) have shown promis-\ning performance across various tasks. However, their autoregres-\nsive decoding process poses significant challenges for efficient\ndeployment on existing AI hardware. Quantization alleviates\nmemory and compute pressure by compressing weights, ac-\ntivations, and KV caches to low precisions while preserving\ngeneration quality. However, existing quantization frameworks\ntypically focus on perplexity or classification accuracy, often\nomitting critical trustworthiness metrics. This gap introduces\nrisks when applying quantized LLMs to downstream high-stakes\ndomains such as finance and healthcare. In this work, we\nsystematically investigate the impact of quantization on four\ntrustworthiness metrics (adversarial robustness, fairness, ma-\nchine ethics, and out-of-distribution robustness) and identify the\ninstability across compression ratios and quantization methods.\nBuilding on these observations, we develop a novel precision-\nensemble voting approach that leverages predictions from mixed-\nprecision variants of the same model and consistently improves\nperformance by up to 5.8% on trustworthiness metrics. Our\nresults highlight the importance of considering trustworthiness\nwhen developing model compression techniques and point to\nresearch opportunities at the intersection of compression and\ntrustworthiness for safety-critical applications.\nIndex Terms—large language models, model quantization,\nmixed precision, model compression, natural language process-\ning, low-bit inference, post-training quantization.\nI. INTRODUCTION\nLarge language models (LLMs) [1], [2] have witnessed\nrapid advancements, demonstrating remarkable capabilities\nacross a broad range of natural language processing tasks.\nHowever, these capabilities come with a huge demand\nfor memory and compute, posing significant challenges in\nresource-constrained settings. Low-bit quantization entails re-\nducing the bit-width of tensors, thereby decreasing mem-\nory footprint and easing computational requirements, while\npreserving generation quality and emergent capabilities such\nas in-context learning and instruction-following. Quantization\nmethods are commonly grouped into post-training quantization\n(PTQ) and quantization-aware training (QAT), with the former\nwidely adopted when further training is infeasible.\nExisting PTQ frameworks focus on reducing the precision\nof weights [3], activations [4], and key–value (KV) caches [5].\nAlthough these frameworks often preserve perplexity and\naccuracy on multi-domain tasks, they typically overlook trust-\nworthiness metrics. This neglect introduces risks when deploy-\ning quantized LLMs to downstream applications, potentially\nleading to unfair, non-robust, or even harmful behaviors.\nThis work highlights the necessity to consider trustworthi-\nness when compressing LLMs for deployment, using weight\nquantization as a representative example. We begin by inves-\ntigating quantized models on both multi-domain tasks and\nfour trustworthiness-focused metrics (adversarial robustness,\nfairness, machine ethics, and out-of-distribution robustness).\nConsistent with prior work [6], we find that quantization\nframeworks typically preserve performance at 8-bit. When fur-\nther compressed to 3-bit and 4-bit, models often maintain ac-\ncuracy on multi-domain tasks, but perform divergently across\nquantization methods and trustworthiness metrics. We further\nobserve that although low-precision models can outperform\nnon-compressed dense models on certain dimensions, their\nperformance is less stable, suffers from high refusal rates, and\ncan exhibit abrupt failures.\nTo improve the robustness of low-precision models, we\nintroduce a novel precision-ensemble voting approach uti-\nlizing multi-precision LLMs. Featuring refusal filtering and\nmajority voting, our approach achieves stable and desirable\nperformance using low-precision models, obtaining superior\nperformance to large dense models by up to 5.8%. Our study\nunderscores the importance of considering trustworthiness\nmetrics under model compression and outlines challenges and\nopportunities for future research on robust, efficient LLMs.\nII. BACKGROUND\nA. Low-Precision LLM Inference\nQuantization is a widely adopted model compression tech-\nnique that represents tensors in low-precision number for-\nmats, thereby reducing both computational cost and memory\narXiv:2511.22483v1  [cs.LG]  27 Nov 2025\n"}, {"page": 2, "text": "footprint. Quantization has been extensively applied to tra-\nditional neural networks (e.g., CNNs/RNNs) [7]–[10], but in\nthe era of transformer-based LLMs [11], self-attention and\nlayer normalization pose new challenges. Contemporary quan-\ntization workflows comprise post-training quantization (PTQ)\nand quantization-aware training (QAT) [12]. PTQ compresses\na pre-trained model without retraining and thus introduces\nminimal overhead. QAT considers quantization error during\nthe training, optimizes parameters for low-bit representations,\nand typically achieves higher accuracy than PTQ. For LLMs,\nresearch has applied quantization to weights, activations, and\nkey–value (KV) caches.\n1) Weight-Only Quantization: Weight-only quantization ap-\nplies lower precision to weights while keeping activations in\ntheir original precision. In this setting, GPTQ [13] conducts\nblockwise second-order optimization, adjusting per-weight\nrounding to efficiently minimize layer-output reconstruction\nerror. AWQ [3] rescales activation-informed channels to pre-\nserve salient directions before applying symmetric per-channel\nweight quantization offline. SqueezeLLM [14] employs a\ndense–sparse decomposition, isolating outlier channels while\nquantizing the remaining dense weights more aggressively.\nAnyPrecisionLLM [15] uses bit-sliced weights to enable\nruntime-selectable precisions, adapting to diverse hardware\nbudgets and workloads. For ultra-low bitwidths, QTIP and\nAQLM propose codebook- or entropy-aware schemes to pre-\nserve accuracy and stability. Furthermore, PDMD [16] adopts\nan adaptive decoding strategy that progressively reduces pre-\ncision as generation proceeds. We focus on how weight-only\nPTQ frameworks affect model trustworthiness in this work.\n2) Weight-Activation\nQuantization:\nWeight–activation\nquantization\ncompresses\nboth\nweights\nand\nactivations,\nenabling low-bit matrix multiplications (e.g., W8A8: 8-bit\nweights and 8-bit activations). In this setting, ZeroQuant [4]\nfirst explores weight–activation quantization for LLMs using\ngroup-wise weight quantization and token-wise activation\nquantization to enable W8A8 inference. SmoothQuant [17]\nmigrates activation quantization difficulty to the weights\nvia\nper-channel\nrescaling,\nsmoothing\nactivation\noutliers\nfor training-free W8A8 quantization. RPTQ [18] reorders\nactivation channels into range-homogeneous clusters and\nfuses the resulting permutations into adjacent layers, enabling\nrobust low-bit activation quantization.\n3) KV Cache Quantization: In LLM inference, the size\nof the KV cache scales rapidly as batch size and sequence\nlength increase, motivating the compression of the stored KV\npairs. In this setting, KVQuant [5] employs per-channel key\nquantization, pre-RoPE key quantization, layer-sensitive non-\nuniform datatypes, and per-vector dense–sparse handling to\nachieve sub-4-bit KV caches. KIVI [19] provides a tuning-\nfree asymmetric 2-bit scheme that quantizes keys per-channel\nand values per-token with a streaming- and hardware-friendly\nimplementation. WKVQuant [20] jointly quantizes model\nweights and the past-only KV cache to low bitwidths, im-\nproving attention efficiency while preserving stability.\nType\nFramework (bits)\nWeight-Only\nGPTQ (3–8); AWQ (3–8); QTIP (2–4); AQLM\n(2–4); SqueezeLLM (2–8)\nWeight+Activation ZeroQuant\n(W8A8);\nSmoothQuant\n(W8A8);\nRPTQ (W4A16 / W4A8 / W4A4)\nKV Cache\nKVQuant (2–4); KIVI (2); WKVQuant (4)\nTABLE I: Summary of low-precision LLM post-training quan-\ntization frameworks with typical bit settings.\nB. Trustworthiness of Low-Precision LLMs\nLLMs are increasingly integrated into daily applications.\nHowever, they pose risks to users, including generating biased\ncontent and disclosing sensitive information. These risks are\nparticularly consequential in safety-critical domains such as\nhealthcare [10], [21]–[23] and finance [24]. Numerous studies\nhave evaluated the trustworthiness of LLMs [6], [25], [26]: De-\ncodingTrust [6] evaluates the trustworthiness of LLMs in eight\ndifferent trustworthiness metrics; TrustLLM [25] provides an\nopen evaluation suite and taxonomy that measure robustness,\nfairness, privacy, and transparency. As LLMs are adopted in\nbroader applications, trustworthiness research also becomes\napplication-specific. In agentic systems, [27] proposes gover-\nnance controls and runtime monitors for autonomous agents,\naddressing provenance, policy compliance, and oversight. In\nhealthcare applications, [28] synthesizes evaluation protocols\nand safeguards for clinical LLMs, emphasizing reliability and\nharm mitigation.\nCurrent quantization frameworks focus on evaluating per-\nplexity on pretraining datasets, as well as zero-shot and few-\nshot accuracy on classification and reasoning tasks. However,\nthese metrics may not fully capture model capabilities such\nas instruction-following, nor behaviors related to hallucina-\ntion [29]. In the trustworthiness domain, [30] highlights the\nimpact of model compression and reports that quantization\ntypically results in less degradation across multiple trustwor-\nthiness metrics compared with pruning. For specific dimen-\nsions, [31] benchmarks the robustness of quantized models in\ncode generation, [32] evaluates the harmfulness of quantized\nmodels, and [33] assesses the truthfulness of quantized models.\nExisting studies [6], [31] further observe that dense model size,\nthe chosen quantization framework, the degree of quantization,\nand the selected trustworthiness metrics all influence the\nperformance of quantized models. Building on these insights,\nwe investigate opportunities to improve the trustworthiness of\nlow-precision LLMs.\nIII. BENCHMARKING TRUSTWORTHINESS OF QUANTIZED\nLLMS\nBefore investigating how trustworthiness can be enhanced\nin mixed-precision settings, we first evaluate the effect of\nquantization on model trustworthiness. In this section, we\nexplore 1) how quantization impacts trustworthiness across\nmultiple dimensions; 2) how trustworthiness varies with the\nchosen quantization framework; and 3) how the compression\nratio influences robustness of the trustworthiness.\n"}, {"page": 3, "text": "A. Models and Quantization Frameworks\nWe evaluate trustworthiness on LLaMA-2-Chat [34], an\ninstruction-tuned variant of LLaMA-2 optimized for multi-\nturn dialogue. We consider two dense LLMs, 7B and 13B,\nas baselines for evaluation. Both configurations are popular\nfor latency-constrained or on-device deployments.\nFollowing prior work [30], [35], we apply AWQ [3] and\nGPTQ [13], two representative post-training weight-only quan-\ntization frameworks to LLaMA-2-13B-Chat. AWQ performs\nactivation-aware weight quantization by estimating channel\nsalience from calibration activations and selecting scales to\npreserve layer responses. GPTQ formulates weight quantiza-\ntion as a blockwise quadratic reconstruction problem, using an\napproximate Hessian to compute error-compensated rounding\nthat minimizes output distortion. We adopt three different bit-\nwidths: 8-bit, 4-bit, and 3-bit. Prior work [30] also investigates\nstructured pruning, another popular model compression tech-\nnique, reporting that quantization constitutes a more reliable\nmethod to preserve trustworthiness than pruning at similar\ncompression ratios.\nB. Evaluation Metrics\nFollowing the configuration in [30], we leverage three\nmetrics to investigate the questions in Sec. III: 1) Accuracy on\nmulti-domain tasks. We report average accuracy on Massive\nMultitask Language Understanding (MMLU) [36], a bench-\nmark covering 57 tasks including elementary mathematics,\nU.S. history, computer science, and law; 2) Trustworthiness.\nWe evaluate four trustworthiness metrics, introduced below;\nand 3) Refusal rate. For each trustworthiness metric, we\nmeasure the refusal rate. Refusal rate characterizes the fre-\nquency that LLMs refuse to provide an explicit answer, by\nanswering “I don’t know”, or giving neutral responses. In most\ntrustworthiness metrics, refusals contribute to inaccuracy.\nTrustworthiness is multifaceted and resists a single quan-\ntified definition. Following TrustLLM [25] which describes\ntrustworthiness in LLMs as the accurate representation of\ninformation, facts, and results, prior work has proposed tax-\nonomies spanning safety, robustness, fairness, toxicity, reli-\nability, privacy, machine ethics, and explainability [6], [25],\n[26]. In this work, we evaluate four representative dimensions:\nadversarial robustness, fairness, machine ethics, and out-of-\ndistribution (OOD) robustness. We leverage the evaluation\nframework of DecodingTrust [6], and score each dimension\non a 0 ∼100 scale.\n1) Adversarial Robustness: Adversarial robustness charac-\nterizes a model’s stability under adversarially perturbed inputs,\nusing AdvGLUE [37] and AdvGLUE++ [6]. AdvGLUE\napplies 14 textual adversarial attack methods to GLUE tasks;\nDecodingTrust further introduces AdvGLUE++, an extension\nthat generates adversarial texts using LLMs. We report accu-\nracy on three representative and challenging tasks: Sentiment\nAnalysis (SST-2), Duplicate Question Detection (QQP), and\nMulti-Genre Natural Language Inference (MNLI), as well as\nthe refusal rate.\n2) Fairness: Fairness characterizes the robustness of model\npredictions with respect to sensitive attributes. To evaluate\nfairness, we use the Adult dataset [38], which includes at-\ntributes such as age, sex, and race to predict whether a person’s\nincome exceeds $50k per year. The fairness score is aggregated\nusing demographic parity difference (DPD) and equalized\nodds difference (EOD). DPD captures disparities in the rate\nof positive predictions between different sensitive attribute\nvalues; EOD incorporates ground-truth labels by comparing\ngroups on both true positive rate and false positive rate. For\nboth metrics, larger values indicate greater unfairness, and zero\nindicates parity.\n3) Machine Ethics: Machine ethics characterizes common-\nsense moral judgments aligned with principles that humans\nintuitively accept. We evaluate using 2,109 short samples\nfrom the ETHICS dataset [39] under zero-shot and few-shot\nsettings, strengthened with jailbreaking and evasive prompts.\nModels are expected to recognize immoral actions and remain\nrobust to evasive wording or jailbreak attempts. The final score\nrepresents the model’s ability to identify immoral actions.\n4) Out-of-Distribution\nRobustness:\nOut-of-distribution\n(OOD) robustness characterizes performance on inputs that\ndeviate substantially from the training distribution. The\nevaluation covers style-transformed paraphrases and out-of-\nscope knowledge, assessing whether the model identifies\nOOD scenarios and attains high accuracy on inputs it does\nnot refuse. The final score aggregates the refusal rate and\nmeaningful accuracy, where meaningful accuracy (MACC)\ndenotes accuracy conditional on non-refusal.\nC. Performance on Multi-domain Tasks and Trustworthiness\nWe evaluate accuracy on multi-domain tasks and on multiple\ntrustworthiness metrics, as depicted in Fig. 1. Although all\nevaluations can be cast as classification after post-processing,\nquantization exhibits a different impact on trustworthiness\nmetrics than on general-task accuracy. We analyze aggregated\nperformance score and refusal rate across all metrics, thereby\naddressing the three questions in Sec. III.\n1) MMLU: We evaluate the models on MMLU, which\nspans 57 subjects in a multiple-choice format to represent\nthe performance in multi-domain tasks. All questions were\nanswered, yielding a zero refusal rate. In terms of accuracy,\nthe change relative to the 13B dense baseline is negligible\nfor both 8-bit and 4-bit quantization. Further compressing\nthe model to 3-bit leads to a noticeable performance drop;\nhowever, accuracy for most configurations remains higher\nthan that of the 7B dense baseline. These results indicate\nthat low-precision quantization can largely preserve multi-\ndomain task accuracy compared to dense models and can\noutperform smaller-parameter dense models. Comparing the\ntwo quantization methods, both AWQ and GPTQ preserve\naccuracy at 8-bit, while AWQ is more robust at 4-bit and 3-bit.\n2) Adversarial Robustness: Adversarial robustness is as-\nsessed on the AdvGLUE++ variants of three GLUE tasks\n(SST-2, QQP, and MNLI). At 8-bit, both AWQ and GPTQ\nachieve accuracy close to the 13B dense baseline. Under lower\n"}, {"page": 4, "text": "16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n20\n40\n60\n80\n100\nScore\nMMLU\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n20\n40\n60\n80\n100\nAdvGLUE++\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n20\n40\n60\n80\n100\nFairness\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n20\n40\n60\n80\n100\nMachine Ethics\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n20\n40\n60\n80\n100\nOOD\n16bit (7B)\n16bit (13B)\nAWQ (13B)\nGPTQ (13B)\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n0\n10\n20\n30\n40\n50\nRefusal Rate\nAdvGLUE++\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n0\n10\n20\n30\n40\n50\nFairness\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n0\n10\n20\n30\n40\n50\nMachine Ethics\n16bit\n(7B)\n3bit\n4bit\n8bit\n16bit\n(13B)\n0\n10\n20\n30\n40\n50\nOOD\nFig. 1: Accuracy and refusal rate for multi-domain tasks (MMLU) and trustworthiness metrics. For MMLU, the refusal rate is\nzero and therefore omitted. On multi-domain tasks, AWQ and GPTQ match the 13B dense baseline at 8-bit and remain close\nat 4-bit, with a larger degradation at 3-bit. On trustworthiness metrics, both methods are comparable to the dense baseline at\n8-bit; at lower precisions (4- and 3-bit), AWQ is more robust than GPTQ.\nprecisions, AWQ exhibits improved robustness at 4-bit and 3-\nbit, whereas GPTQ degrades by more than 10%, at 4-bit and\n3-bit. Accuracy is computed over the valid label set; refusals\nand outputs that cannot be mapped to task labels are counted\nas errors. Consistent with this definition, we observe that AWQ\nlow-bit models have low refusal rates, while GPTQ at higher\ncompression yields more label-inconsistent (“contradictory”)\nanswers rather than explicit abstentions, resulting in refusals.\nThis pattern suggests that aggressive GPTQ compression may\nimpair calibration and label consistency.\nComparing dense baselines, the 7B model outperforms the\n13B model on AdvGLUE++ by 9.3% and yields a lower re-\nfusal rate. One possible explanation is that smaller models are\nless sensitive to spurious lexical cues introduced by adversarial\nperturbations, whereas larger models are more easily misled\nby minor word changes. The observed correlation between\nlower refusal rates and higher accuracy holds for both dense\nand quantized variants.\n3) Fairness: Fairness is assessed on the Adult dataset [38],\nwith scores aggregated from two metrics: demographic parity\ndifference (DPD) and equalized odds difference (EOD). Unlike\nthe other trustworthiness metrics, the fairness score does not\ncompletely rely on correctly predicting the category. Both\ndense and quantized models achieve high fairness, with scores\nclose to or above 80% (higher is better under our normalized\nfairness score); all configurations also achieve low refusal\nrates, since the requests are not ambiguous or misleading.\nFor AWQ, the fairness score remains similar at 8-bit and 4-\nbit, while at 3-bit the performance decreases; GPTQ attains\nhigh fairness at 8-bit and 3-bit, with a decrease at 4-bit.\nAdditionally, the breakdown results show that the magnitude of\nDPD is typically lower than that of EOD, indicating that EOD\ncontributes more to the overall unfairness; for high-fairness\nconfigurations, the difference between EOD and DPD is small.\nComparing dense baselines, the 7B model also outperforms\nthe 13B model, as well as the quantized 13B variants. The per-\nformance gap is mainly associated with cases showing extreme\nbase-rate imbalance, where the label distribution is highly\nskewed. In such cases, larger models appear more sensitive\nto the imbalance and make more biased classifications.\n4) Machine Ethics: Machine ethics is assessed on the\nETHICS dataset [39], reporting accuracy together with a false\npositive ratio (FPR) that penalizes false positive responses to\njailbreaking and evasive inputs. At 8-bit, both quantization\nmethods achieve performance similar to the 13B dense base-\nline. At lower bit-widths (4- and 3-bit), AWQ shows improved\nperformance relative to the dense baseline, whereas GPTQ\nexhibits a significant drop. A breakdown indicates that the true\npositive rate on the immoral class is similar for all models; the\n4- and 3-bit AWQ-quantized models benefit from a lower FPR\non evasive sentences, while low-bit GPTQ-quantized models\nfrequently produce invalid or abstaining outputs under few-\nshot, yielding a higher refusal rate. This highlights the model’s\ndegradation of in-context learning capability.\nComparing dense baselines, the 7B dense model scores\nlower than the 13B dense model and lags behind the AWQ-\nquantized variants. Although the 7B model’s accuracy is\napproximately 10% lower, its lower refusal rate narrows the\ngap with the 13B dense model.\n5) Out-of-Distribution Robustness: OOD robustness is as-\nsessed on both style-shifted inputs and out-of-scope knowledge\nqueries. For style-shifted inputs (e.g., Shakespearean phras-\ning), robustness means maintaining high accuracy with a low\nrefusal rate. For out-of-scope queries, robustness requires de-\ntecting that the query is beyond the model’s knowledge and ab-\nstaining appropriately, while answering in-scope queries cor-\nrectly, achieving high meaningful accuracy (MACC). AWQ-\nquantized models maintain OOD performance relative to the\n13B dense baseline, whereas GPTQ-quantized models degrade\nsignificantly at 4- and 3-bit. A breakdown attributes the 3-\nbit degradation primarily to elevated refusal rates, and the 4-\nbit degradation to low accuracy on out-of-scope knowledge\ntasks, indicating a failure mode similar to that observed in the\nmachine-ethics dimension.\nComparing dense baselines, the 7B model outperforms the\n"}, {"page": 5, "text": "13B model and the quantized variants on OOD robustness.\nSpecifically, in the zero-shot out-of-scope evaluation, the 7B\nmodel achieves nearly double the accuracy while exhibiting\na higher refusal rate, indicating a more conservative behavior\non knowledge-unknown queries.\n6) Observations and Insights: The above observations pro-\nvide insights into the three questions in Sec. III. 1) Quanti-\nzation methods have heterogeneous impacts on multi-domain\ntasks and on different trustworthiness metrics. This difference\nis often missed by standard evaluations of quantized models.\n2) For both AWQ and GPTQ, 8-bit quantization largely\npreserves performance. AWQ is more robust at 4- and 3-\nbit, whereas low-precision GPTQ can substantially degrade\nfew-shot adherence and in-context learning in the LLaMA-2\nseries. 3) Smaller models, whether quantized or trained with\nfewer parameters, can outperform on certain trustworthiness\ndimensions by being less sensitive to ambiguous phrasing,\nwhere larger models are more likely to err. These insights\nmotivate our attempt to enhance quantized models’ robustness\nin trustworthiness, as described in Sec. IV.\nIV. ENHANCING TRUSTWORTHINESS USING\nPRECISION-ENSEMBLE VOTING\nA. Motivation\nIn Sec. III, we examine the performance of quantized\nmodels at different compression ratios on multi-domain tasks\nand trustworthiness metrics. As shown in Fig. 1, models\nquantized to low bit-widths can surpass dense models on\ncertain trustworthiness metrics while maintaining comparable\naccuracy on multi-domain tasks. However, a critical bottleneck\nof low-precision quantization is reduced robustness: quantized\nmodels are more vulnerable to high refusal rates and can\nsuffer abrupt performance drops in specific scenarios. Prior\nwork [30] also reports performance instability across model\nfamilies and quantization methods. These observations moti-\nvate mechanisms that enable low-precision models to provide\nrobust and consistent predictions.\nWe address this bottleneck by leveraging the idea of test-\ntime optimization [40], [41], which invests additional inference\ncompute to enhance performance. To this end, we propose\na simple voting-based precision-ensemble that aggregates the\npredictions of multi-precision variants quantized from the\nsame dense LLM.\nB. Precision-Ensemble Voting\nWe pursue robust quantized models via precision-ensemble\nvoting, illustrated in Fig. 2. The procedure comprises four\nstages, quantization, generation, filtering, and voting, as de-\ntailed in Algorithm 1. In the quantization stage (lines 1–3), the\ndense backbone (e.g., 13B) is quantized to multiple bit-widths\nor the same bit-width using different seeds. In the generation\nstage (lines 4–5), each quantized LLM generates a response\nin parallel, and predictions are mapped to discrete labels.\nMany trustworthiness benchmarks can be cast as classification,\nenabling the use of voting to aggregate results. Labels can be\nextracted using heuristics or an LLM-as-a-Judge [42].\nIn the filtering stage (lines 6–9), we discard invalid or\ncontradictory outputs (e.g., empty, unparseable, or multi-label\ngenerations) and mark refusals. For standard classification\nmetrics, refusals are removed. For OOD robustness tasks that\nevaluate out-of-scope queries, explicit refusals (e.g., “I don’t\nknow”) are retained as a dedicated label. In the final voting\nstage, the remaining candidates are aggregated via unweighted\nmajority voting. In the event of a tie, we select the highest-\nprecision model’s prediction.\nAlgorithm 1 Precision-ensemble voting\nRequire: Dense model Mfp, precisions B = {N1, . . . , Nm},\nprompt x, decoding params θ, refusal filter f\nEnsure: Final label ˆy\n1: for all Ni ∈B do\n2:\nMi ←Quantize(Mfp, Ni)\n3: end for\n4: Generate:\nG ←{Generate(Mi, x; θ)}m\ni=1\n5: Map to labels:\nC ←{Postprocess(g) | g ∈G}\n6: Filter refused candidates:\nC′ ←{c ∈C | ¬f(c)}\n7: if C′ = ∅then\n8:\nreturn REFUSED\n9: end if\n10: Aggregation: ˆy ←Maj.Voting(C′)\n11: return ˆy\nC. Effectiveness of Precision-Ensemble Voting\n1) Evaluation Setup: We quantize the dense LLaMA-2-\n13B-Chat model to three precisions: 3-, 4-, and 8-bit. Each\nquantized model and the precision-ensemble are evaluated\non multi-domain tasks (MMLU) and on three trustworthiness\nmetrics: adversarial robustness, machine ethics, and out-of-\ndistribution robustness. The fairness dimension is excluded\nin this evaluation because it quantifies inter-group disparities\nbetween predictions rather than absolute accuracy, and is\ntherefore not directly comparable under this setup. For each\nmetric, we compare the ensemble prediction against the 7B and\n13B dense baselines and against the best-performing single-\nprecision result for each quantization method.\n2) Effectiveness on Multi-Domain Tasks:\nAccording to\nFig. 3, precision-ensemble voting achieves accuracy that is\nclose to both the best single-precision results and the 13B\ndense baseline, and it outperforms the 7B dense model. The\ngains on MMLU are modest because low-bit quantization\nalready preserves accuracy for these models. Nevertheless,\nthe ensemble effectively mitigates instability observed in low-\nprecision GPTQ-quantized models.\n3) Effectiveness on Trustworthiness Metrics: Fig. 3 shows\nthat precision-ensemble voting delivers robust, desirable per-\nformance. Compared with dense baselines, the ensemble re-\nsults consistently outperform the 13B dense model by up\nto 5.8%, but fail to match the 7B dense baseline, as small\nmodels often benefit from being less sensitive to spurious\nlexical cues. Relative to the best single-precision results, the\n"}, {"page": 6, "text": "Created by Lucas Rathgeb\nfrom the Noun Project\nCreated by Lucas Rathgeb\nfrom the Noun Project\nCreated by Lucas Rathgeb\nfrom the Noun Project\nCreated by Lucas Rathgeb\nfrom the Noun Project\nQuantization\nGeneration\nDense LLM\nQuantized\nLLMs\nFiltering\nVoting\nCandidates\nN2 bit\nN1 bit\nNm bit\nFiltered\nCandidates\nBit-ensemble\nResult\nFig. 2: Workflow of precision-ensemble voting. A dense LLM is\nquantized to multiple precisions; each quantized model generates its\nown response. After response filtering, the remaining responses are\naggregated via unweighted majority voting.\n7B\nAWQ\nGPTQ\n13B\n0\n20\n40\n60\n80\n100\nMMLU\n7B\nAWQ\nGPTQ\n13B\n0\n20\n40\n60\n80\n100\nAdvGLUE++\n7B\nAWQ\nGPTQ\n13B\n0\n20\n40\n60\n80\n100\nMachine Ethics\n7B\nAWQ\nGPTQ\n13B\n0\n20\n40\n60\n80\n100\nOOD\nScore\n16bit (7B)\n16bit (13B)\nAWQ-Best\nAWQ-Voting\nGPTQ-Best\nGPTQ-Voting\nFig. 3: The precision-ensemble voting mechanism\nmaintains MMLU accuracy while consistently im-\nproving performance on the trustworthiness bench-\nmarks.\nprecision-ensemble voting approach performs better in most\ntrustworthiness metrics and for both quantization frameworks.\nWe attribute these gains to two factors: 1) the filtering\nstage (for standard classification metrics) removes refusals,\nthereby lowering the measured refusal rate and preserving\nmeaningful labels for aggregation; and 2) unweighted majority\nvoting improves stability by reducing variance across bit-\nwidths: when a quantized model fails on a specific instance,\nother bit-widths do not tend to fail as well, and the aggregation\nexploits this partial error diversity.\nV. CHALLENGES AND OPPORTUNITIES\nA. Mixed Precision for Multi-Modal Trustworthiness\nChallenges: The rapid development and increasing deploy-\nment of multi-modal LLMs, integrating vision, speech, action,\nand language, introduces a more complex trustworthiness\nproblem than in text-only models. In particular, embodied AI\nsystems, which rely on cross-modal reasoning to interact with\nthe real world, demand heightened attention to trustworthiness\nacross all modalities.\nOpportunities: Multi-modal setting opens new opportu-\nnities for modality-aware mixed-precision quantization that\njointly considers efficiency and trustworthiness. Different bit-\nensemble strategies and precision scheduling can be adaptively\ntuned based on modality-specific information collected at\nruntime. Our findings in the single-modality case lay the\ngroundwork for future exploration in multi-modal scenarios.\nB. Joint Compression and Trust-Aware Optimization\nChallenges: While mixed-precision quantization has proven\neffective in preserving or even enhancing trustworthiness,\nreal-world deployments often combine it with other com-\npression techniques such as pruning, tensor decomposition,\nand knowledge distillation. However, the interplay between\nthese methods and their joint effect on trustworthiness remains\nunderexplored. In particular, naive combinations may intro-\nduce unpredictable degradation in ethical behavior, adversarial\nrobustness, or fairness, especially under low-bit regimes.\nOpportunities: We identify a significant opportunity to\nunify compression design with trustworthiness objectives. One\npromising direction is to formulate model compression as\nan automated sparsity search problem, where trustworthiness\nmetrics are directly embedded in the optimization objective.\nThis would enable the development of trust-aware auto-\ncompression pipelines capable of jointly tuning bit-width,\nsparsity, and decomposition rank for maximal efficiency while\nenhancing trustworthiness.\nC. Algorithm-System-Hardware Co-Design\nChallenges: Efficient system and hardware support for\nmixed-precision and bit-ensemble execution is critical for real-\nworld deployment. At the hardware level, designing a unified\ncompute unit that can efficiently and concurrently support\noperations at multiple precisions remains a major challenge,\ndue to inherent trade-offs among performance, power, and area\n(PPA). At the system level, effective scheduling and execution\npolicies are required to manage precision diversity, particularly\nin multi-batch and multi-tenant deployment settings.\nOpportunities: The hardware and system design choices\nspan a large configuration space. When jointly considered\nwith algorithmic parameters, this forms a vast co-design\nspace where algorithmic performance and hardware/system ef-\nficiency can be optimized together [43]. This joint optimization\ncan be tackled using techniques such as Bayesian optimization\nor reinforcement learning to efficiently explore the co-design\nspace and derive Pareto-optimal solutions.\nVI. CONCLUSION\nThis work analyzes the impact of quantization on trustwor-\nthiness, highlights the stability bottleneck with low precision,\nand proposes a precision-ensemble voting approach to improve\nrobustness. We underscore the importance of considering\ntrustworthiness when compressing LLMs for deployment, us-\ning weight quantization as a representative case study with\nquantitative experimental results. Future directions include\nextending trustworthiness analysis to multi-modal settings, ex-\nploring joint compression and trust-aware optimization frame-\nworks, and pursuing end-to-end algorithm–system–hardware\nco-design for trustworthy and efficient LLM deployment.\n"}, {"page": 7, "text": "REFERENCES\n[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4\ntechnical report,” arXiv preprint arXiv:2303.08774, 2023.\n[2] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao,\nC. Huang, C. Lv et al., “Qwen3 technical report,” arXiv preprint\narXiv:2505.09388, 2025.\n[3] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao,\nX. Dang, C. Gan, and S. Han, “Awq: Activation-aware weight quanti-\nzation for on-device llm compression and acceleration,” Proceedings of\nmachine learning and systems, vol. 6, pp. 87–100, 2024.\n[4] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He,\n“Zeroquant: Efficient and affordable post-training quantization for large-\nscale transformers.”\n[5] C. Hooper, S. Kim, H. Mohammadzadeh, M. W. Mahoney, Y. S. Shao,\nK. Keutzer, and A. Gholami, “Kvquant: Towards 10 million context\nlength llm inference with kv cache quantization,” Advances in Neural\nInformation Processing Systems, vol. 37, pp. 1270–1303, 2024.\n[6] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu,\nZ. Xiong, R. Dutta, R. Schaeffer et al., “Decodingtrust: A comprehensive\nassessment of trustworthiness in gpt models,” 2023.\n[7] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen,\nand T. Blankevoort, “A white paper on neural network quantization,”\narXiv preprint arXiv:2106.08295, 2021.\n[8] H. Fan, H.-C. Ng, S. Liu, Z. Que, X. Niu, and W. Luk, “Reconfigurable\nacceleration of 3d-cnns for human action recognition with block floating-\npoint representation,” in 2018 28th International Conference on Field\nProgrammable Logic and Applications (FPL).\nIEEE, 2018, pp. 287–\n2877.\n[9] H. Fan, G. Wang, M. Ferianc, X. Niu, and W. Luk, “Static block floating-\npoint quantization for convolutional neural networks on fpga,” in 2019\nInternational Conference on Field-Programmable Technology (ICFPT).\nIEEE, 2019, pp. 28–35.\n[10] H. Fan, S. Liu, Z. Que, X. Niu, and W. Luk, “High-performance\nacceleration of 2-d and 3-d cnns on fpgas using static block floating\npoint,” IEEE Transactions on Neural Networks and Learning Systems,\nvol. 34, no. 8, pp. 4473–4487, 2021.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[12] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer,\n“A survey of quantization methods for efficient neural network infer-\nence,” in Low-power computer vision.\nChapman and Hall/CRC, 2022,\npp. 291–326.\n[13] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “Gptq: Accurate\npost-training quantization for generative pre-trained transformers,” arXiv\npreprint arXiv:2210.17323, 2022.\n[14] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Ma-\nhoney, and K. Keutzer, “Squeezellm: dense-and-sparse quantization,” in\nProceedings of the 41st International Conference on Machine Learning,\n2024, pp. 23 901–23 923.\n[15] Y. Park, J. Hyun, S. Cho, B. Sim, and J. W. Lee, “Any-precision llm:\nlow-cost deployment of multiple, different-sized llms,” in Proceedings\nof the 41st International Conference on Machine Learning, 2024, pp.\n39 682–39 701.\n[16] H. M. Chen, F. Tan, A. Kouris, R. Lee, H. Fan, and S. Venieris,\n“Progressive mixed-precision decoding for efficient llm inference,” in\nThe Thirteenth International Conference on Learning Representations.\n[17] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\n“Smoothquant: accurate and efficient post-training quantization for large\nlanguage models,” in Proceedings of the 40th International Conference\non Machine Learning, 2023, pp. 38 087–38 099.\n[18] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y. Shang, G. Sun, Q. Wu,\nJ. Wu, and B. Wu, “Rptq: Reorder-based post-training quantization for\nlarge language models,” arXiv preprint arXiv:2304.01089, 2023.\n[19] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen,\nand X. Hu, “Kivi: a tuning-free asymmetric 2bit quantization for kv\ncache,” in Proceedings of the 41st International Conference on Machine\nLearning, 2024, pp. 32 332–32 344.\n[20] Y. Yue, Z. Yuan, H. Duanmu, S. Zhou, J. Wu, and L. Nie, “Wkvquant:\nQuantizing weight and key/value cache for large language models gains\nmore,” CoRR, 2024.\n[21] Asgari et al., “A framework to assess clinical safety and hallucination\nrates of llms for medical text summarisation,” npj Digital Medicine,\nvol. 8, no. 1, p. 274, 2025.\n[22] H. Fan, M. Chen, L. Castelli, Z. Que, H. Li, K. Long, and W. Luk,\n“When monte-carlo dropout meets multi-exit: Optimizing bayesian\nneural networks on fpga,” in 2023 60th ACM/IEEE Design Automation\nConference (DAC).\nIEEE, 2023, pp. 1–6.\n[23] H. Fan, M. Ferianc, and W. Luk, “Enabling fast uncertainty estimation:\naccelerating bayesian transformers via algorithmic and hardware opti-\nmizations,” in Proceedings of the 59th ACM/IEEE Design Automation\nConference, 2022, pp. 325–330.\n[24] T. Hu, T. Hu, L. Bai, Y. Zhao, A. Cohan, and C. Zhao, “Fintrust:\nA comprehensive benchmark of trustworthiness evaluation in finance\ndomain,” in Proceedings of the 2025 Conference on Empirical Methods\nin Natural Language Processing, 2025, pp. 10 110–10 139.\n[25] Y.\nHuang\net\nal.,\n“Trustllm:\nTrustworthiness\nin\nlarge\nlanguage\nmodels,” 2024. [Online]. Available: https://openreview.net/forum?id=\nbWUU0LwwMp\n[26] Y. Liu et al., “Trustworthy llms: a survey and guideline for evaluating\nlarge language models’ alignment,” arXiv preprint arXiv:2308.05374,\n2023.\n[27] S. Raza, R. Sapkota, M. Karkee, and C. Emmanouilidis, “Trism for\nagentic ai: A review of trust, risk, and security management in llm-\nbased agentic multi-agent systems,” arXiv preprint arXiv:2506.04133,\n2025.\n[28] M. Aljohani, J. Hou, S. Kommu, and X. Wang, “A comprehensive survey\non the trustworthiness of large language models in healthcare,” arXiv\npreprint arXiv:2502.15871, 2025.\n[29] J. Lee, S. Park, J. Kwon, J. Oh, and Y. Kwon, “Exploring the trade-offs:\nQuantization methods, task difficulty, and model size in large language\nmodels from edge to giant,” arXiv preprint arXiv:2409.11055, 2024.\n[30] J. Hong, J. Duan, C. Zhang, Z. Li, C. Xie, K. Lieberman et al.,\n“Decoding compressed trust: Scrutinizing the trustworthiness of efficient\nllms under compression,” arXiv:2403.15447, 2024.\n[31] S. Fang, W. Ding, A. Mastropaolo, and B. Xu, “Smaller= weaker?\nbenchmarking robustness of quantized llms in code generation,” arXiv\npreprint arXiv:2506.22776, 2025.\n[32] Y. Belkhiter, G. Zizzo, and S. Maffeis, “Harmlevelbench: Evaluating\nharm-level compliance and the impact of quantization on model align-\nment,” arXiv preprint arXiv:2411.06835, 2024.\n[33] Y. Fu, X. Long, R. Li, H. Yu, M. Sheng, X. Han, Y. Yin, and P. Li,\n“Quantized but deceptive? a multi-dimensional truthfulness evaluation\nof quantized llms,” arXiv preprint arXiv:2508.19432, 2025.\n[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei\net al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv\npreprint arXiv:2307.09288, 2023.\n[35] A.\nKharinaev,\nV.\nMoskvoretskii,\nE.\nShvetsov,\nK.\nStudenikina,\nB. Mikhail, and E. Burnaev, “Investigating the impact of quantization\nmethods on the safety and reliability of large language models,” arXiv\npreprint arXiv:2502.15799, 2025.\n[36] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, “Measuring massive multitask language understanding,”\narXiv preprint arXiv:2009.03300, 2020.\n[37] B. Wang, C. Xu, S. Wang, Z. Gan, Y. Cheng, J. Gao et al., “Adversarial\nglue: A multi-task benchmark for robustness evaluation of language\nmodels,” in Advances in Neural Information Processing Systems, 2021.\n[38] B. Becker and R. Kohavi, “Adult,” UCI Machine Learning Repository,\n1996, DOI: https://doi.org/10.24432/C5XW20.\n[39] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and\nJ. Steinhardt, “Aligning ai with shared human values,” in International\nConference on Learning Representations.\n[40] C. Snell, J. Lee, K. Xu, and A. Kumar, “Scaling llm test-time compute\noptimally can be more effective than scaling model parameters,” arXiv\npreprint arXiv:2408.03314, 2024.\n[41] H. M. Chen, G. Lu, Y. Okoshi, Z. Mo, M. Motomura, and H. Fan,\n“Rethinking optimal verification granularity for compute-efficient test-\ntime scaling,” arXiv preprint arXiv:2505.11730, 2025.\n[42] D. Li et al., “From generation to judgment: Opportunities and challenges\nof llm-as-a-judge,” in Proceedings of the 2025 Conference on Empirical\nMethods in Natural Language Processing, 2025, pp. 2757–2791.\n[43] H. Fan, M. Ferianc, Z. Que, S. Liu, X. Niu, M. R. D. Rodrigues,\nand W. Luk, “Fpga-based acceleration for bayesian convolutional neural\nnetworks,” IEEE Transactions on Computer-Aided Design of Integrated\nCircuits and Systems, vol. 41, no. 12, pp. 5343–5356, 2022.\n"}]}