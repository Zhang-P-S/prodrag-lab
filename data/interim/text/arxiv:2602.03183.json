{"doc_id": "arxiv:2602.03183", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.03183.pdf", "meta": {"doc_id": "arxiv:2602.03183", "source": "arxiv", "arxiv_id": "2602.03183", "title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch", "authors": ["Hyunwoo Kim", "Niloofar Mireshghallah", "Michael Duan", "Rui Xin", "Shuyue Stella Li", "Jaehun Jung", "David Acuna", "Qi Pang", "Hanshen Xiao", "G. Edward Suh", "Sewoong Oh", "Yulia Tsvetkov", "Pang Wei Koh", "Yejin Choi"], "published": "2026-02-03T06:54:46Z", "updated": "2026-02-03T06:54:46Z", "summary": "Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.03183v1", "url_pdf": "https://arxiv.org/pdf/2602.03183.pdf", "meta_path": "data/raw/arxiv/meta/2602.03183.json", "sha256": "87a2f91b004b856458a7986739b1cd6aad3df224f58ddc807835b1486508a849", "status": "ok", "fetched_at": "2026-02-18T02:19:56.882787+00:00"}, "pages": [{"page": 1, "text": "Preprint.\nPrivasis: Synthesizing the Largest\nâ€œPublicâ€ Private Dataset from Scratch\nHyunwoo Kim1âˆ—\nNiloofar Mireshghallah2âˆ—\nMichael Duan3\nRui Xin4\nShuyue Stella Li4\nJaehun Jung1\nDavid Acuna1\nQi Pang1\nHanshen Xiao1\nG. Edward Suh1\nSewoong Oh4\nYulia Tsvetkov4\nPang Wei Koh4\nYejin Choi1\n1NVIDIA\n2CMU\n3USC\n4UW\nAbstract\nResearch involving privacy-sensitive data has always been constrained\nby data scarcity, standing in sharp contrast to other areas that have bene-\nfited from data scaling. This challenge is becoming increasingly urgent as\nmodern AI agentsâ€”such as OpenClaw and Gemini Agentâ€”are granted\npersistent access to highly sensitive personal information. To tackle this\nlongstanding bottleneck and the rising risks, we present PRIVASIS (i.e.,\nprivacy oasis), the first million-scale fully synthetic dataset entirely built\nfrom scratchâ€”an expansive reservoir of texts with rich and diverse private\ninformationâ€”designed to broaden and accelerate research in areas where\nprocessing sensitive social data is inevitable. Compared to existing datasets,\nPRIVASIS, comprising 1.4 million records, offers orders-of-magnitude larger\nscale with quality, and far greater diversity across various document types,\nincluding medical history, legal documents, financial records, calendars,\nand text messages with a total of 55.1 million annotated attributes such as\nethnicity, date of birth, workplace, etc. We leverage PRIVASIS to construct a\nparallel corpus for text sanitization with our pipeline that decomposes texts\nand applies targeted sanitization. Our compact sanitization models (â‰¤4B)\ntrained on this dataset outperform state-of-the-art large language models,\nsuch as GPT-5 and Qwen-3 235B. We plan to release data, models, and code\nto accelerate future research on privacy-sensitive domains and agents.1\n1\nIntroduction\nPrivasis-Cleaner  \n4B\n(b) Decomposition-based  \nSanitization Pipeline\nðŸPrivasis: \nMillion-scale  \nPrivacy-rich Corpus\nModel Size\nQwen3 \n235B\nGPT-5\nLlama-4 \nMaverick\nFirst \nName\nPerformance\nStrategy\nSynthetic \nText \nRecord\n(a) Synthesis from Scratch  \nvia Auxiliary Control Variables \n& Diversity-preserving Selective Refinement\n(c) Sanitization \nDistillation\n0.6B\nSanitized \nText \nRecord\nLegal\nMedical\nFinancial\nPrivate Records\nâ€¦\nFigure 1: PRIVASIS, the Privacy Oasis Dataset: We synthesize the first publicly-available\nmillion-scale dataset with diverse private information, entirely from scratch. (a) Using aux-\niliary control variables, we initialize a text record draft containing rich private information\nand then selectively refine it while preserving the overall diversity of the record set (Â§2). (b)\nBased on this, we construct a parallel corpus for text sanitization using a decomposition-\nbased sanitization pipeline (Â§3). (c) On this parallel corpus, we train compact sanitization\nmodels (â‰¤4B) that outperform GPT-5 (Â§4).\nProgress in privacy-related research has long been fundamentally limited by a drought\nof data. By definition, private information cannot be publicly shared. As a result, most\nprior work relies on small, narrowly scoped datasetsâ€”standing in stark contrast to the\n1https://privasis.github.io\n1\narXiv:2602.03183v1  [cs.CL]  3 Feb 2026\n"}, {"page": 2, "text": "Preprint.\ndata-driven scaling paradigm that underpins progress in many other areas of AI (Li et al.,\n2025; Yukhymenko et al., 2024). Meanwhile, agentic systems (e.g., OpenClaw; Steinberger,\n2026, Gemini Agent; Google, 2025, and ChatGPT Health; OpenAI, 2026) increasingly need\nto process personal communications, documents, and records at inference time, while\nmaintaining privacy guarantees (Mireshghallah et al., 2024). This trend highlights the\nurgent need for robust privacy methods at multiple stages: input-side approaches like\ndata sanitization and minimization (Zhou et al., 2025; Dou et al., 2024), as well as post-\nhoc techniques (Bagdasarian et al., 2024) that ensure models appropriately handle the\npersonal information entrusted to them. Yet despite their apparent simplicity, these privacy\ntasks remain surprisingly difficultâ€”current LLMs fail at even basic personally identifiable\ninformation (PII) detection (Shao et al., 2024; Pham et al., 2025).\nTo address this urgent need, we introduce PRIVASIS (i.e., privacy oasis), the first million-scale\nsynthetic dataset built entirely from scratch for privacy research along with its correspond-\ning parallel corpus PRIVASIS-SANITIZATION for text sanitization (Figure 1). Our synthesis\npipeline (Figure 1a; Â§2) achieves this scale without reference data by using auxiliary control\nvariablesâ€”profiles with personal attributes (e.g., name, ID), record types (e.g., â€œpsychother-\napy billing statementâ€), and background contextsâ€”to generate diverse documents spanning\nmedical, legal, financial, and communication records, each annotated with detailed JSON\nstructures. To ensure realism and diversity, we employ iterative rejection sampling with a\nweighted criterion combining LLM quality scoring and Vendi diversity metrics (Friedman &\nDieng, 2023). PRIVASIS demonstrates superior diversity compared to existing human-written\ndatasets: our domain subsets consistently achieve higher MATTR (lexical diversity metric;\n0.807â€“0.823 vs. 0.700â€“0.794), bigram diversity, and Shannon entropy while maintaining\nlower cosine similarity, indicating richer lexical variety and reduced semantic redundancy.\nWe evaluate models on a new benchmark derived from PRIVASIS by testing their ability\nto detect and sanitize private information across both vanilla and harder test sets. Even\nfrontier models leave room for improvement: GPT-5 achieves only 70% and 13% full\nsuccess rate on vanilla and hard sets, respectively. To tackle this challenge, we design\nPRIVASIS-SANITIZATION, a parallel corpus for training models to selectively remove or\nabstract sensitive information while preserving textual utility (Â§3). Our decomposition-\nbased pipeline (Figure 1b) breaks records into chunks, then applies targeted sanitization\nbased on user-specified attributesâ€”going beyond fixed PII categories to support arbitrary\ninformation that users may contextually want removed.\nOur decomposition-based pipeline supports multiple abstraction levels (e.g., replacing\nâ€œMarch 3rdâ€ with â€œEarly Marchâ€ vs. complete removal) and explicitly preserves non-\nsensitive information through retention targets, yielding triplets of (original record, sani-\ntization instruction, sanitized record) that enable training lightweight models for flexible,\nutility-preserving sanitization. Training on PRIVASIS-SANITIZATION yields compact models\nthat outperform frontier LLMs (Â§4.3): our 4B-parameter PRIVASIS-CLEANER achieves 72.5%\nfull success rate on the vanilla test set, surpassing all tested models including o3 (70.3%),\nwhile maintaining competitive performance on the hard set (12.4% vs. GPT-5â€™s 13.1%).\nCrucially, these compact models enable practical on-device data minimizationâ€”removing\nunnecessary sensitive information before processingâ€”which is essential since users cannot\nrisk sending private data to external servers for cleaning (Zhou et al., 2025).\nPRIVASIS provides the first privacy-safe yet privacy-rich dataset at million scale, overcom-\ning the fundamental data scarcity bottleneck in privacy research. Unlike prior work that\nrelies on real-world reference data or repurposed existing datasets, PRIVASIS is entirely\nreference-freeâ€”synthesized from scratch using only auxiliary control variables and public\nname databasesâ€”eliminating privacy risks from actual individuals. We validate this privacy\nsafety by sampling more than 1K profiles and querying whether they correspond to real\npeople: while some shared names or partial attributes with real individuals, manual verifi-\ncation revealed no genuine matches, with generated profiles being hallucinated rather than\nmemorized from training data. With its rich records and attributes, future work can leverage\nPRIVASIS to develop methods that respect privacy by designâ€”from improved sanitization\nmodels to differential privacy techniques, and agentic systems that must operate responsibly\non sensitive information. We plan to release all code, data, and models to accelerate progress\nin this critical area where technical capability must align with ethical responsibility.\n2\n"}, {"page": 3, "text": "Preprint.\nPsychotherapy \nbilling statement\nIn response to an upcoming \ninternational trip for work, Natisha \nschedules a special session with Dr. \nRozen to address travel-related \nanxiety. The billing statement not \nonly details the consultation about \nmaintaining her Sertraline and \nQuetiapine regimen abroad, but also \nincludes coordination with her \npharmacy for enough supply and a \nletter for airport authorities \nregarding her prescribed medications.\nChronological â€¨\nService Log â€¨\n- Tone: Methodical \nand precise.â€¨\n- Structure: Timeline \nof events, each with \na corresponding \nservice code and \ndescription, a \nsummary of charges, \nand payment \ninstructions.\nPatient: Natisha Ben-David\nDate of Birth: 1987-06-12\nSex: Female\nID Number: 312487659 (Teudat Zehut)\nPhone: +972-54-728-3916\nEmail: natisha.bendavid@gmail.com\nAddress: 12 Herzl St., Tel Aviv-\nYafo, 6342107, Israel\nBilling Period: 2024-05-10 to \n2024-05-15\nChronological Service Log:\n2024-05-10 | 09:30â€“10:15 (45 mins)\nService Code: PSY-101\nDescription: [â€¦] Reviewed current \npharmacotherapy regimen. [â€¦] \nSession conducted at Tel Aviv \nMental Health Clinic, Room 3B.\n2024-05-11 | 14:00â€“14:20 (20 mins)\nService Code: PHC-205\nDescription: Coordination call with \npharmacy to arrange a 60-day supply \nrefill of medicine. [â€¦]\nConfirmation number: RX-8745123.\n2024-05-13 | 11:00â€“11:45 (45 mins)\nService Code: DOC-310 [â€¦]\nNatisha\nProfile\nRecord \nType\nBack- \nground \nContext\nContent\nFormat\nRejected \nRecord \nDraft\nFirst \nName\n1. Informed Initialization with Auxiliary Control Variables\n2. Diversity-preserving Iterative Selection-based Refinement\nâœ¦Revise based on the previous draft: \n \nâœ¦Accept based on specificity/realism & diversity:\np(xâ€²ï¿¼|x)\nÎ± â‹…LLMScore + Î² â‹…Î”Vendi > 0.5\nInitial \nRecord \nDraft\nRevised \nRecord \nDraft\nRevised \nRecord \nDraft\nRevised \nRecord \nDraft\nPatient: Natisha Ben-David\nDate of Birth: 1987-06-12\nSex: Female\nID Number: 312487659 (Teudat Zehut)\nPhone: +972-54-728-3916\nEmail: natisha.bendavid@gmail.com\nAddress: 12 Herzl St., Tel Aviv-\nYafo, 6342107, Israel\nBilling Period: 2024-05-10 to \n2024-05-15\nChronological Service Log:\n2024-05-10 | 09:30â€“10:15 (45 mins)\nService Code: PSY-101\nDescription: [â€¦] Reviewed current \npharmacotherapy regimen including \nSertraline 100mg daily and \nQuetiapine 25mg nightly. [â€¦] \nSession conducted at Tel Aviv \nMental Health Clinic, Room 3B.\n2024-05-11 | 14:00â€“14:20 (20 mins)\nService Code: PHC-205\nDescription: Coordination call with \nMaccabi Pharmacy, 22 Ibn Gabirol \nSt., Tel Aviv, to arrange a 60-day \nsupply refill of Sertraline 100mg \nand Quetiapine 25mg. [â€¦]\n{first_name: Natisha\n last_name: Ben-David\n sex: Female\n age: 37\n date: 2007-08-13\n marital_status: Single\n income_class: Upper-Middle\n morality_level: Ethical\n native_language: Hebrew\n blood_type: B-\n ethnicity: Middle Eastern\n citizenship: Israel\n id_type: Teudat Zehut\n id_number: 312487659\n passport_number': L9834721\n phone_number':+972-54-728-3916\n email: natisha.bendavid@gmail.com\n user_handle: natisha_bd\n url: https://linkedin.com/in/     \n      natisha-bendavid}\nFigure 2: Overview of our synthesis pipeline.\n2\nSynthesizing Privacy-rich Text Data from Scratch\nThe construction of PRIVASIS is guided by three design principles: (1) scalable synthesis\nacross a broad spectrum of text records, (2) incorporation of diverse and fine-grained private\ninformation within those records, and (3) synthesis that does not rely on real-world reference\ndata.\nTo this end, we use LLMs as they define expressive probability distributions over text. How-\never, directly sampling such complex, specific data x from LLMs is challenging, as they tend\nto favor high-probability, generic continuations rather than rare, highly specific instances\nof private information. This is particularly difficult without reference data, which most\nexisting works rely on to steer generations. To address this challenge, we adopt informed\ninitialization through auxiliary control variables, followed by a diversity-preserving revision\nalgorithm with selection. This allows us to efficiently explore the large space of possible\ntexts even in the absence of reference data. Figure 2 provides an overview of our pipeline.\nMore details and examples are in Appendix A and G.\n2.1\nSynthesis Pipeline\n1. Informed Initialization with Auxiliary Control Variables: We introduce multiple auxil-\niary control variables to guide the initialization of a text record x. A record is determined by\ntwo primary variablesâ€”semantic content (c) and structural format (f)â€”which are themselves\ninformed by three auxiliary variables (Figure 2):\nâ€¢ Profile (i): Basic attributes such as gender, ethnicity, and date, sampled from a predefined\nset conditioned on the first name sampled from the US SSN applicant database.2 The\nprofile also includes attributes describing a specific event involving the individual.\nâ€¢ Record type (d): Concise description of what the record is, derived from i.\nâ€¢ Background context (b): Description of the social context of the record, derived from i\nand d.\nTo encourage diversity, we prompt the LLM to generate multiple candidates for d and b in a\nlist format and then select one at random. The recordâ€™s semantic content (c) is constructed as\nthe concatenation of i, d, and b, while its format (f) is generated given d and b. Finally, the\n2catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-data\n3\n"}, {"page": 4, "text": "Preprint.\ninitial draft x0 is generated, given c and f. Because the process is bottom-up from explicit\nauxiliary variables, the variables serve as free annotations or metadata alongside the record.\n2. Diversity-preserving Iterative Selection-based Refinement: The initial draft x0 may\ncontain degenerate or overly generic content, while our goal is to produce records with\nrealistic and concrete details. To improve the quality of x0, we iteratively apply selective\nrefinement. At each step t, a candidate draft xâ€²\nt is sampled and evaluated against the current\ndraft xt. An LLM judge compares which of the two drafts are better in terms of specificity\nand realism.\nWe find repeated refinement leads to a lack of variety converging to similar patterns. To\nmitigate this, we use the Vendi embedding score (Friedman & Dieng, 2023), which measures\nhow spread out a set of representations is in embedding space. Intuitively, the score is\nhigher when records cover a broader range of semantic directions, and lower when they\ncollapse to similar content. Concretely, we maintain a collection of all final accepted records\nproduced so far, and for each diversity evaluation we randomly sample up to nP records\nfrom this collection to form a pool P. The contribution of a new draft to diversity is defined\nas the change in Vendi score between P âˆª{xt} (with the current draft) and P âˆª{xâ€²\nt} (with\nthe new draft). The final decision is based on a weighted acceptance score,\nS(xâ€²\nt) = Î± Â· LLMScore(xt, xâ€²\nt) + Î² Â·\n\u0000Vendi(P âˆªxâ€²\nt) âˆ’Vendi(P âˆªxt)\n\u0001\nwhere the candidate is accepted only if S(xâ€²\nt) > Ï„ with Ï„ = 0.5. The refinement procedure is\nrepeated for up to three steps. Ablation study of our pipeline are provided in Appendix A.1.\n3. Attribute Annotation: After the final refinement step, we extract and annotate additional\nattributes in JSON format that are present in the record but not explicitly captured in\nthe profile i (e.g., fine-grained details mentioned in the text). These attributes and i are\nthen grouped into semantic clusters using an LLM (i.e., grouped attributes). For example\nin medical records, â€˜clinic nameâ€™, â€˜pharmacy nameâ€™, and â€˜room numberâ€™ clustered under\nâ€˜locationâ€™. Such groupings yield contextual structure that can be leveraged in downstream\ntasks, including sanitization (Â§3.1).\n4. Filtering: We filter out cases where any type of error occurred during the generation\nprocess, yielding a success rate of approximately 94%. Because the initial drafts of the\nrecords are refined, most appear reasonable. Nonetheless, we exclude records with fewer\nthan 64 words (66,894 in total), profiles with an age under 18 (1,180), and degenerate cases\n(1,232).\n2.2\nStatistics & Analysis\nBasic Statistics: PRIVASIS comprises 1,414,871 records with 55,092,084 annotated attributes\n(â‰ˆ39 per record). These span basic profile details (e.g., name, sex, age, marital status,\nincome, language, blood type, phone, email, URL) as well as richer information such as\ndates and locations. Attributes are grouped into an average of 6.2 clusters per record, each\nwith 6.3 attributes. Records also include background context, format, and type descriptions\naveraging 527.0, 76.4, 41.8, and 20.0 words, respectively. Most records are generated by GPT-\nOSS-120B (67.9%; OpenAI, 2025), followed by GPT-4.1-Mini (21.6%) and Exaone-3.5-32B\n(7.2%; Research et al., 2024). Smaller shares come from Qwen3-80B (1.1%; Yang et al., 2025),\nLlama-3.3-70B (1.1%; Grattafiori et al., 2024), GPT-4.1 (0.7%), and other frontier models\n(<0.1%). Using multiple models both increases stylistic and distributional diversity and\nshows that our pipeline generalizes across LLMs.\nDistribution of Generated Profiles: The sex distribution is 59% female and 41% male.\nThe age, income class, and blood type distributions are uniform, as these attributes were\nrandomly sampled. The ethnicity distribution is shown in Figure 3 in the Appendix. The\nmost common ethnicity is South Asian, followed by European and Black. It should be noted\nthat these categories are generated by the LLMs; hence, the granularity of the ethnicity\nlabels is heterogeneous. For example, European refers to a region-based category, whereas\nBlack is defined in terms of phenotype. We also found a large proportion of profiles were\nnon-U.S. (95%). This is because we only use first names from the U.S. SSN database, which\nreflects a highly diverse population.\n4\n"}, {"page": 5, "text": "Preprint.\nTable 1: Distribution of domain categories in PRIVASIS with the top three subcategories for\neach main category. Percentages represent the proportion of each category among sampled\nrecords.\nDomain Category\nRatio\nSubcategory 1\nSubcategory 2\nSubcategory 3\nHealth & Wellness\n20.7%\nMedical Care (11.8%)\nMental Health & Support (4.2%)\nHealthcare Administration (3.2%)\nGovernment & Civic\n13.5%\nImmigration & Citizenship (4.9%)\nLegal Proceedings (3.8%)\nPublic Administration (3.2%)\nBusiness & Finance\n13.4%\nEmployment & HR (4.9%)\nFinancial Management (4.3%)\nAccounting & Tax (1.9%)\nPersonal & Family\n10.7%\nPersonal Development (6.0%)\nFamily Relationships (2.0%)\nLife Events (1.9%)\nCommunity & Social\n9.3%\nCommunity Services (4.8%)\nReligious & Spiritual (2.3%)\nVolunteer & Nonprofit (1.1%)\nProfessional Services\n9.1%\nProject Management (6.0%)\nConsulting & Advisory (1.1%)\nSpecialized Services (1.0%)\nEducation & Training\n8.9%\nAcademic Administration (5.6%)\nFinancial Aid (1.8%)\nLearning & Development (0.8%)\nLegal & Compliance\n7.6%\nCriminal Justice (2.3%)\nContracts & Agreements (2.1%)\nCourt & Litigation (1.6%)\nMedia & Comms.\n3.9%\nPublishing & Content (1.6%)\nCreative Arts (1.3%)\nCommunication (0.7%)\nRecreation & Lifestyle\n2.3%\nTravel & Tourism (1.2%)\nFood & Culinary (0.5%)\nEntertainment & Hobbies (0.5%)\nTechnical & Operations\n0.7%\nSystem Administration (0.6%)\nTechnical Support (0.1%)\nâ€“\nTable 2: Diversity of PRIVASIS domain subsets and human-written datasets.\nDataset\nMATTR (â†‘)\nBigram\nDiversity (â†‘)\nShannon\nEntropy (â†‘)\nCosine\nSimilarity (â†“)\nMIMIC-III Notes (Johnson et al., 2016)\n0.757\n0.900\n6.396\n0.654\nPRIVASIS Health & Wellness\n0.815\n0.872\n7.402\n0.321\nGovReport (Huang et al., 2021)\n0.781\n0.813\n7.071\n0.354\nPRIVASIS Government & Civic\n0.815\n0.865\n7.411\n0.347\nEnron Email (Klimt & Yang, 2004)\n0.794\n0.897\n5.871\n0.331\nPRIVASIS Media & Comms.\n0.823\n0.877\n7.448\n0.320\nFinance Tasks (Cheng et al., 2024)\n0.700\n0.566\n5.729\n0.679\nPRIVASIS Business & Finance\n0.807\n0.864\n7.346\n0.353\nTAB (PilÃ¡n et al., 2022)\n0.741\n0.747\n7.065\n0.670\nPRIVASIS Legal & Compliance\n0.817\n0.863\n7.488\n0.352\nCategory Distribution of Generated Records: Although records include annotated record\ntypes, these are often overly specific (e.g., psychotherapy billing statement from Dr. Rozen), mak-\ning clustering difficult. We therefore re-categorize them into broader groups (e.g., medical\ncare) using GPT-4.1-Mini on a random sample of 48K records (prompt in Appendix A.1),\nyielding 592 unique categories. These were hierarchically clustered with Claude Sonnet\n4 and manually refined into 10 primary categories and 42 subcategories. Table 1 shows\nthe distribution of the main categories along with the top 3 subcategories within each of\nthe main category. Health & Wellness is the most common category (20.7%), followed by\nGovernment & Civic (13.5%) and Business & Finance (13.4%). See Appendix G for example\nrecords and metadata (e.g., attributes) per category.\nHow does PRIVASIS compare to human-written datasets? Table 2 reports four quantitative\ndiversity metrics: Moving-average TTR (MATTR; Covington & McFall, 2010), bigram diver-\nsity, Shannon entropy, and cosine similarity. We compare each PRIVASIS domain subset with\nits related human-written dataset. PRIVASIS subsets consistently exhibit greater diversity\nthan human-written datasets across multiple metrics. MATTR and bigram diversity are\nhigher across PRIVASIS domains, reflecting richer vocabulary and syntactic variation, while\nhigher Shannon entropy indicates more uniform word use and less repetition. Additionally,\nPRIVASIS subsets achieve lower cosine similarity scores, confirming decreased redundancy\nand greater semantic diversity within the dataset.\nWe also conducted a human evaluation of the naturalness and coherence of the records in\nPRIVASIS. Specifically, we randomly sampled 128 records from PRIVASIS and 128 records\nfrom the collection of human-written datasets spanning a wide range of domains similar to\nthose in PRIVASIS. Seven annotators judged whether each record was natural and coherent\nin a blind review setting, without knowledge of the textâ€™s source. Among the 128 records\nfrom PRIVASIS, 113 were judged natural and coherent, compared to 111 from the human-\nwritten datasets. This indicates that records in PRIVASIS achieve a level of naturalness and\ncoherence comparable to human-written records.\n5\n"}, {"page": 6, "text": "Preprint.\nDo the generated profiles correspond to real people? We sample 100 profiles to investi-\ngate whether they correspond to memorized real-world data. Using Gemini-2.5-Pro Deep\nResearch, we check if each profile matched a real person, providing full details (name,\nage, sex, citizenship, email, URLs, phone). In case of URLs, none of them were accessible.\nOf the 100 profiles, 5 were incomplete due to off-topic responses. In 15 cases, the model\nreturned multiple potential matches, which we manually disambiguated: 8 shared the exact\nname and 7 had similar names, but other attributes (sex, age, nationality) did not align,\nand no email addresses matched. In 3 cases, the model reported an exact match (name, sex,\nnationality), but manual review showed major discrepancies in age and contact information.\nThe remaining profiles were all reported as fabricated. We also run a larger check on 1K\nprofiles with websearch-enabled GPT-5 and none of them were judged to be real. Thus, the\nprofiles are synthetic and do not represent real individuals, reducing privacy concerns.\n3\nBuilding a Sanitization Parallel Corpus\nAs one concrete downstream application that leverages PRIVASIS, we focus on training\na sanitization model that can selectively remove or abstract sensitive information while\npreserving coherence and utility. We aim to train a model that meets the following four\ngoals: (1) process diverse text domains, (2) follow arbitrary sanitization instructions rather\nthan being limited to fixed personally identifiable information (PII) categories, (3) support\nmultiple levels of abstraction beyond simple masking or deletion, and (4) remain lightweight\nenough for local deployment.\nUsing PRIVASIS, we build PRIVASIS-SANITIZATION, a high-quality sanitization corpus of\ntriplets (original record, instruction, sanitized record) that treats sensitivity as contextual and\nsupports flexible strategies that balance privacy with utility. It is desirable for sanitization\nmodels to be small enough to run locally, so that sensitive text never needs to leave a userâ€™s\ndevice. However, we find that even frontier LLMs struggle with sanitizing long text records\neffectively (Â§4.3). To address this challenge, we introduce a decomposition-based pipeline\nthat breaks records into manageable chunks, enabling grounded and consistent sanitization.\n3.1\nSanitization Pipeline\n1. Decomposition: To make sanitization tractable, the record x is recursively split into\na set of chunks C = {c1, c2, . . . } using double newlines, EOS markers, or other natural\nboundaries until each chunk satisfies |ci| â‰¤Ï„, where Ï„ = 512 characters. This variable-\nlength decomposition simplifies the sanitization task while preserving local coherence (e.g.,\nlist placed in the same chunk).\n2. Target Selection: From the annotated attributes A of x, we assign a sensitivity weight\nwa to each a âˆˆA using an LLM, prioritizing highly sensitive information over relatively\nbenign details that are difficult to sanitize (e.g., happy emotion). Next, using these weights,\nwe sample a set of n targets, denoted T = {z1, . . . , zn}, which may be individual attributes\nor attribute groups (Â§2.1). Each z is then randomly labeled with â„“z âˆˆ{ABSTRACT, DROP}.\nBy selecting targets stochastically, we go beyond PIIs to cover various information that users\nmay contextually consider sensitive.\n3. Sanitization: (i) For each target z âˆˆT , we first identify relevant chunks Cz âŠ†C using\nan LLM. (ii) From each c âˆˆCz, we extract the spans Sz,c that correspond to z. (iii) We then\nbuild a sanitization instruction instrz for z:\nIf â„“z = ABSTRACT, all c âˆˆCz are concatenated\nand passed to the LLM to generate an abstraction instruction grounded in all the relevant\ncontext (e.g., â€œAbstract the specific date as â€˜in the coming monthsâ€™â€).\nIf â„“z = DROP, we use\na fixed instruction (e.g., â€œDrop the information about {z} from the textâ€). (iv) We use instrz\nto sanitize each c âˆˆCz for consistency. This decomposition-based strategy ensures both\nconsistent abstraction across chunks and improved efficiency, since sanitization can be\napplied to chunks in parallel. Finally, we merge the sanitized chunks to reconstruct the\nsanitized record Ëœx. Further details are in Algorithm 1 in the Appendix.\n4. Final Instruction Generation: After sanitization, we prompt an LLM to generate a\nfinal coherent instruction bI based on all {instrz}zâˆˆT . To support scenarios where utility is\n6\n"}, {"page": 7, "text": "Preprint.\nTable 3: Comparison of public privacy datasets and related resources.\nDataset\nSize\nLength\n(#Words)\nSens.\nSpans\nPII\nSpans\nAbstr.\nPairs\nSynthetic\nPublic\nDomain\nMIMIC-II De-identification (Douglass et al., 2004)\n2M\n282\nâœ•\nâ\nâœ•\nâœ•\nâœ•\nClinical\nText Anonymization Benchmark (PilÃ¡n et al., 2022)\n1.2K\n843\nâ\nâ\nâœ•\nâœ•\nâ\nLegal\nSelf-Disclosure Corpus (Dou et al., 2024)\n4.8K\n29\nâ\nâœ•\nâ\nâœ•\nâ\nReddit\nGretel Synthetic Financial PII (Gretel.ai, 2024)\n55.9K\n188\nâœ•\nâ\nâœ•\nâ\nâ\nFinance\nSynthPAI (Yukhymenko et al., 2024)\n7.8K\n17\nâœ•\nâœ•\nâœ•\nâ\nâ\nReddit\nNaP2 (Huang et al., 2025)\n4.8K\n171\nâœ•\nâ\nâ\nâ\nâ\nPersona Chat\nPANORAMA (Selvam & Ghosh, 2025)\n384K\n48\nâœ•\nâœ•\nâœ•\nâ\nâ\nMulti\nAutomated Privacy Info Annotation (Zeng et al., 2025a)\n154K\n226\nâ\nâ\nâœ•\nâœ•\nâ\nLLM queries\nPAPILLON / PUPA (Li et al., 2025)\n0.9K\n181\nâœ•\nâœ•\nâœ•\nâœ•\nâ\nDialogues\nSemSI-Set / SemSI-Bench (Zhang et al., 2025b)\n10.8K\n210\nâœ•\nâœ•\nâœ•\nâ\nâ\nNews\nPRIVASIS-SANITIZATION (Ours)\n1.4M(100K)\n527\nâ\nâ\nâ\nâ\nâ\nMulti\nimportant, we additionally include a set of retention target attributes K = {k1, . . . , km} âŠ†A,\nrepresenting information that should be explicitly retained. We select K to minimize\ninterference with the sanitization process, choosing attributes with the lowest lexical overlap\nwith the sanitization targets T , based on ROUGE scores. When the grouped attributes\n(Â§2.1) are selected as targets, we occasionally omit the individual attribute names and only\ninclude the group label in bI, instead of iterating over the individual attributes. For example,\nâ€œPlease abstract all information related to locations while keeping the city.â€ rather than â€œPlease\nabstract the clinic address, session room, and the patientâ€™s address while keeping the city.â€ This\nencourages contextual generalization to natural user requests. Finally, the pipeline yields\na triplet (record x, instruction bI, sanitized record Ëœx), which supports instruction-following\nfor sanitization. More details of our pipeline are provided in Appendix B.\n3.2\nStatistics & Analysis\nBasic Statistics: We construct a dataset of 100K examples and use 37K of them to build our\ntraining set, with 70.4% generated by GPT-OSS-120B (OpenAI, 2025) and 29.6% by Qwen3-\n80B (Yang et al., 2025). Sanitization instructions average 69.4 words and specify 2.9 targets.\nWe construct 2.1K evaluation records using latest frontier models (GPT-5, Gemini-2.5-Pro,\nQwen3-235B, LLaMA-4 Maverick). Further evaluation details are in Section 4.2.\nComparison with Existing Privacy Datasets: Table 3 compares PRIVASIS-SANITIZATION\nwith existing datasets (extended analysis in Appendix F). Most prior work focuses on PII\nspan detection (Douglass et al., 2004; PilÃ¡n et al., 2022; Papadopoulou et al., 2022; Gretel.ai,\n2024; Zeng et al., 2025a), providing only deletion without rewritten alternatives, restricting\nto predefined PII categories, and operating in single domains with short text units. PRIVASIS\nrecords average 527 words, significantly longer than existing datasets, with annotated spans\naveraging 35.3 characters. The Self-Disclosure Corpus (Dou et al., 2024) spans average 28.7\ncharacters but provides only single-level abstraction for pre-specified categories in narrow\ndomains like Reddit posts. In contrast, PRIVASIS uniquely provides multiple abstraction\nlevels with flexible, instruction-based sanitization supporting unlimited user-specified\ncategories. Our dataset enables both removal and graded abstraction (e.g., â€œMarch 3rd, 2024â€\nâ†’â€œearly Marchâ€ â†’â€œthis springâ€ â†’â€œrecentlyâ€), covers 100K+ multi-domain records, and\ngenerates natural language instructions for arbitrary privacy requirements.\n4\nExperiments\n4.1\nModel Training\nWe train a sanitizer PRIVASIS-CLEANER that, given text x and instruction bI, outputs a\nsanitized version Ëœx where target attributes are abstracted or removed. Since it is safer when\nsanitization models are run locally, we target lightweight Qwen3 models: 0.6B, 1.7B, and\n4B. We train them on a 37K subset of PRIVASIS-SANITIZATION (Â§3). Further details are in\nAppendix C.\n7\n"}, {"page": 8, "text": "Preprint.\n4.2\nHierarchical Evaluation\nWe evaluate models on the PRIVASIS-SANITIZATION test set, which contains records from\nfour frontier models: Gemini-2.5-pro, GPT-5, Llama-4-Maverick, and Qwen3-235B. To\nassess sanitization effectiveness, we use a hierarchical evaluation framework capturing\nthree information leakage types in sanitized text: (1) direct leak, (2) inference leak, and (3)\nproximity leak. Our evaluation setup relies on exact string matching and factual questions\nto minimize potential evaluator bias. The LLM evaluator is used to answer those factual\nquestions, instead of preference based judgments.\nFirst, we check for direct leak by performing exact string matching to determine whether\nthe target attribute value zvalue appears verbatim in the sanitized record. If absent, we test\ninference leak by prompting an evaluator LLM (GPT-OSS-120B) to predict the attribute\nvalue given only the sanitized text and attribute key zkey (e.g., â€œPlease guess the name of\nthe journal. Make a guess even if itâ€™s not included in the given text.â€). We denote this\nprediction as Ë†zsanitized and check for exact string matches with zvalue. If no match occurs,\nwe check proximity leak by comparing the evaluatorâ€™s predictions from both the sanitized\ntext (Ë†zsanitized) and the original record (Ë†zoriginal). The evaluator assesses which prediction\nis closer to the true attribute value zvalue; if Ë†zsanitized is as close as, or closer than, Ë†zoriginal,\nthis indicates a proximity leak and thus a sanitization failure. The sanitization of a record\nsucceeds only if no target attributes T exhibit leakage (the Successful Record metric). This\napproach captures multiple levels of leakage from explicit disclosure to subtle inference.\nSince a model returning an empty string would avoid all leakage, we also measure informa-\ntion retention. For each record, we check whether the retention target attributes (K; Â§3.1)\nremain in the sanitized record using exact string matching and LLM. A record is considered\nsuccessfully processed only if no information leakage occurs for any sanitization target and\nall retention target attributes are preserved in the sanitized text (the Full Successful Record\nmetric). We also report Successful Attribute and Successful Att./Record metrics, which indicate\nthe overall success rate across all attributes in the test set and the average success rate of\nattributes within each record, respectively.\nWe release two test sets: (1) Vanilla and (2) Hard. The vanilla set (1,042 records) consists of\nrecords on which our sanitization pipeline (Â§3) achieves a perfect Full Successful Record\nscore, while the hard set (1,149 records) contains records where even our decomposition-\nbased pipeline fails to do so. The difference lies mainly in grouped attributes: 60% in vanilla\nvs. 87% in hard, which require contextual target identification, thereby adding an extra layer\nof complexity. Hard-set records are also longer (619.6 vs. 569.3 words) and paired with\nlonger instructions (94 vs. 57.2 words), reflecting higher complexity. Further details and\nexamples of each leakage type are in Appendix D and E.\n4.3\nResults on PRIVASIS-SANITIZATION\nOverall Performance: Table 4 shows results on both the Vanilla and Hard test sets.\n(1) Vanilla test set: Sanitization is fundamentally a re-writing task, yet even the strongest\nfrontier models fall short of perfect performance, with Full Success Record rates ranging\nfrom 64.4% (Qwen3-235B) to 70.3% (o3). Note, the Vanilla Test set corresponds to a subset\nof PRIVASIS where our decomposition-based sanitization pipeline with GPT-4.1 achieves\na perfect score. This gap indicates that frontier models even with reasoning capabilities\nstruggle to reliably execute fine-grained sanitization. In contrast, PRIVASIS-CLEANER-4B,\ntrained on PRIVASIS, achieves 72.50%, outperforming all frontier LLMs despite being orders\nof magnitude smaller. Even PRIVASIS-CLEANER-0.6B model, outperforms GPT-OSS-120B,\nLlama-4 Maverick, and Qwen3-235B, while the base models Qwen3 4B and 0.6B lags at\n53.65% and 16.70%, respectively.\n(2) Hard test set: Performance declines sharply for all models on the Hard test set, which\nintroduces more challenging attribute selections and longer contexts. Frontier models\ndrop to only 10â€“13% Full Success, with GPT-5 reaching the highest at 13.14%. Our model\nagain matches frontier-level performance, achieving 12.80%, outperforming o3 and R1,\n8\n"}, {"page": 9, "text": "Preprint.\nTable 4: Sanitization performance of off-the-shelf LLMs and our PRIVASIS-CLEANER models.\nâ€˜Att.â€™ denotes attribute and â€˜Successful Att. / Recordâ€™ denotes the average success rate\nwithin a record.\nModel\nSanitization\nRetention\nFull\nSuccessful\nAttribute (%)\nSuccessful Att.\n/ Record (%)\nSuccessful\nRecord (%)\nSuccessful\nAttribute (%)\nSuccessful Att.\n/ Record (%)\nSuccessful\nRecord (%)\nSuccessful\nRecord (%)\nVanilla Test Set\no3\n91.53\n91.62\n74.57\n94.06\n95.24\n93.57\n70.25\nDeepSeek R1\n88.54\n91.01\n72.26\n93.74\n94.39\n93.38\n69.58\nGPT-5\n90.16\n91.80\n73.90\n93.42\n94.08\n92.71\n70.06\nGPT-4.1\n88.07\n90.73\n72.26\n94.48\n95.80\n94.43\n70.06\nGPT-OSS-120B\n89.42\n90.33\n69.87\n95.28\n95.61\n94.53\n67.66\nLLaMA-4 Maverick\n89.31\n90.77\n73.61\n89.97\n90.27\n88.68\n67.18\nQwen3-235B\n83.34\n87.58\n68.04\n93.47\n94.31\n93.19\n64.40\nQwen3-4B\n75.66\n79.59\n57.58\n90.66\n90.95\n90.02\n53.65\nQwen3-0.6B\n47.39\n53.77\n35.99\n70.49\n71.42\n69.19\n16.70\nPRIVASIS-CLEANER-4B\n86.60\n90.60\n72.84\n99.15\n99.13\n98.66\n72.50\nPRIVASIS-CLEANER-0.6B\n81.61\n87.37\n68.52\n99.26\n99.23\n98.75\n68.04\nHard Test Set\no3\n80.20\n75.65\n15.23\n87.89\n87.04\n83.81\n11.66\nDeepSeek R1\n75.54\n72.76\n15.14\n86.70\n86.16\n82.59\n11.23\nGPT-5\n78.78\n75.30\n16.28\n87.08\n87.23\n84.25\n13.14\nGPT-4.1\n75.14\n72.40\n13.93\n89.79\n90.06\n86.51\n12.18\nGPT-OSS-120B\n77.67\n74.07\n13.84\n88.36\n87.87\n84.94\n10.53\nLLaMA-4 Maverick\n76.21\n73.40\n16.19\n82.07\n81.92\n78.24\n11.05\nQwen3-235B\n69.01\n67.33\n12.79\n89.37\n89.71\n86.95\n10.27\nQwen3-4B\n62.15\n59.89\n8.27\n84.36\n84.01\n80.68\n5.66\nQwen3-0.6B\n33.41\n35.46\n12.53\n76.63\n76.08\n72.58\n4.44\nPRIVASIS-CLEANER-4B\n73.29\n71.23\n14.19\n95.76\n95.60\n92.96\n12.36\nPRIVASIS-CLEANER-0.6B\n68.11\n66.14\n11.31\n95.09\n95.20\n91.99\n9.31\nranking second only to GPT-5. These results underscore both the challenge of fine-grained\nsanitization and the effectiveness of our dataset.\nSanitization Metrics & Retention Metrics: Although the Successful Record scores for most\nmodels are around 70%, their Successful Attribute/Record scores exceed 90%. This gap\nreveals a critical weakness: while models sanitize the majority of attributes, they routinely\nmiss at least one per record. For example, although the Successful Attribute score for our\nmodel is lower than o3 and same with GPT-5, the full success record rate of our model is\nhigher. Such failures are unacceptable in privacy-sensitive settings, because one missed\nattribute is enough to compromise the entire record, no matter how many others were\nsanitized correctly. Frontier models also underperform on retention, frequently over-editing\nand altering non-target attributes. For example, GPT-5 retains only 93.4% of attributes on\nthe Vanilla set, versus 99.2% for PRIVASIS-CLEANER-4B. These highlight a core weakness of\nfrontier LLMs: reliably separating sensitive from non-sensitive information.\n4.4\nAnalysis\nHow robust is the LLM-based proximity leak evaluation? We run human evaluation on\nall proximity leak failure cases (n = 140) produced by the o3 model and found 98% of\nthem to be correct, indicating the high precision of our evaluation setup. We additionally\nassessed robustness by repeating the proximity leak evaluation using two different models\n(GPT-OSS-120B and Qwen3-80B), achieving a high inter-model agreement of 97% over 5K\ncases, indicating low susceptibility to evaluator-specific bias.\nWhat types of information leakage do the models make? Table 5 reports the information\nleakage patterns (Â§4.2) of models on the Vanilla test set of PRIVASIS-SANITIZATION. Strik-\ningly, all models most frequently exhibit direct leaks, exposing sensitive information in\nverbatim form in the supposedly â€œsanitizedâ€ outputs. Our PRIVASIS-CLEANER-4B model\nachieves the third-lowest ratio of direct leak, following GPT-OSS-120B and o3. In contrast,\nthe base 4B model shows the highest ratio, indicating that training on our dataset improves\ninformation leakage patterns as well. GPT-OSS-120B shows the lowest direct leak ratio,\nsuggesting it can better identify the target attributes for sanitization; however, it ultimately\nfails to sanitize them effectively, as reflected by its Successful Record score in Table 4. In-\nterestingly, our PRIVASIS-CLEANER-0.6B model shows the highest direct leak ratio, but\n9\n"}, {"page": 10, "text": "Preprint.\nTable 6: Top 8 main categories where each model fails and their ratio.\nModel\n1\n2\n3\n4\n5\n6\n7\n8\no3\nHealth &\nWellness\n(26.5%)\nBusiness\n& Finance\n(17.1%)\nLegal &\nCompliance\n(9.6%)\nEducation\n& Training\n(9.1%)\nPersonal\n& Family\n(9.1%)\nGovernment\n& Civic\n(7.8%)\nCommunity\n& Social\n(6.0%)\nProfessional\nServices\n(6.0%)\nDeepSeek R1\nBusiness\n& Finance\n(22.7%)\nHealth &\nWellness\n(19.2%)\nGovernment\n& Civic\n(12.0%)\nPersonal\n& Family\n(9.9%)\nEducation\n& Training\n(8.6%)\nLegal &\nCompliance\n(8.1%)\nProfessional\nServices\n(7.2%)\nCommunity\n& Social\n(4.9%)\nGPT-5\nHealth &\nWellness\n(22.3%)\nBusiness\n& Finance\n(19.1%)\nEducation\n& Training\n(11.0%)\nLegal &\nCompliance\n(9.7%)\nGovernment\n& Civic\n(9.4%)\nPersonal\n& Family\n(7.3%)\nCommunity\n& Social\n(5.3%)\nMedia &\nCommunications\n(4.6%)\nLLaMA-4\nMaverick\nBusiness\n& Finance\n(21.5%)\nHealth &\nWellness\n(17.5%)\nGovernment\n& Civic\n(11.8%)\nEducation\n& Training\n(8.8%)\nLegal &\nCompliance\n(8.5%)\nPersonal\n& Family\n(8.3%)\nCommunity\n& Social\n(6.9%)\nProfessional\nServices\n(6.7%)\nQwen3-235B\nBusiness\n& Finance\n(19.7%)\nHealth &\nWellness\n(19.5%)\nLegal &\nCompliance\n(11.2%)\nPersonal\n& Family\n(11.2%)\nEducation\n& Training\n(10.2%)\nGovernment\n& Civic\n(10.0%)\nProfessional\nServices\n(5.8%)\nCommunity\n& Social\n(5.1%)\nPRIVASIS\nSANITIZER-4B\nBusiness\n& Finance\n(15.2%)\nEducation\n& Training\n(15.1%)\nHealth &\nWellness\n(13.8%)\nPersonal\n& Family\n(13.6%)\nGovernment\n& Civic\n(13.0%)\nCommunity\n& Social\n(11.1%)\nLegal &\nCompliance\n(7.9%)\nProfessional\nServices\n(6.8%)\noutperforms Qwen3-235B on the Full Successful Record score. Among the frontier models,\nQwen3-235B stands out with a notably high direct leak ratio, which may reflect a broader\nlimitation within the Qwen3 model family.\nNote that the ratio of inference\nleaks is significantly lower than\nproximity leaks because we apply\nexact string matching on top of\nthe evaluatorâ€™s predictions of the\nattribute (Â§4.2). The inference leak\nratio would likely increase if se-\nmantic entailment were used in-\nstead, since some attributes (e.g.,\nlists or long string) are difficult to\ncapture via exact string matching.\nTo account for such cases, we rely\non the proximity leak metric in-\nstead. Case studies are provided\nin Appendix E.\nTable 5: Ratio of each information leakage type (Â§4.2).\nModel\nDirect\nLeak\nInference\nLeak\nProximity\nLeak\no3\n60.4%\n8.4%\n31.2%\nDeepSeek R1\n70.0%\n6.7%\n23.3%\nGPT-5\n67.4%\n7.7%\n24.9%\nGPT-OSS-120B\n53.7%\n8.4%\n37.9%\nLLaMA-4 Maverick\n66.1%\n6.2%\n27.7%\nQwen3-235B\n75.3%\n4.3%\n20.4%\nQwen3-4B\n81.7%\n3.1%\n15.2%\nPRIVASIS-CLEANER-4B\n64.8%\n8.4%\n26.8%\nPRIVASIS-CLEANER-0.6B\n79.4%\n3.1%\n17.5%\nWhere do the models struggle most? Table 6 presents the top 8 main categories of records\n(Â§2.2) where the models fail most often. Overall, Business & Finance is the most challenging\ncategory, followed by Health & Wellness. The former primarily includes financial records,\nwhile the latter covers medical records. A notable pattern is that our PRIVASIS-CLEANER\nmodel exhibits a more balanced performance across categories, whereas o3 struggles dispro-\nportionately with Health & Wellness compared to other domains. Table 10 in the Appendix\nsummarizes the top 8 attributes that models fail most often. We find the models struggle the\nmost with name-related attributes (e.g., last name, full name, and user handle), and dates.\nHow well does PRIVASIS-CLEANER generalize? We evaluate the performance of PRIVASIS-\nCLEANER-4B on the NaPÂ² dataset (Huang et al., 2025), which includes high-quality human-\nrewritten text for sanitization, in a zero-shot setting. We follow the proximity-leak evaluation\nprotocol using the sanitization target information (i.e., sensitive profile information) pro-\nvided by NaPÂ². Specifically, we prompt the evaluator model (GPT-OSS-120B) to judge\nwhether the original text or the sanitized text is closer to the sensitive information. If the\nevaluator judges the sanitized text is as close as, or closer than, the original text, we consider\nthat a leak.\nThe 4B model finetuned directly on NaPÂ² achieves a low leak ratio of 10%. Our PRIVASIS-\nCLEANER 4B model, despite never being trained on NaPÂ², achieves the same leak ratio\nof 10%. On the other hand, the NaPÂ²-trained model scores 31.96% on PRIVASISâ€™s Full\n10\n"}, {"page": 11, "text": "Preprint.\nTable 7: Zero-shot generalization performance of PRIVASIS-CLEANER-4B on the NaPÂ² dataset\ncompared to the NaPÂ²-finetuned model. Note, PRIVASIS-CLEANER-4B was not trained on\nNaPÂ² and was evaluated in a zero-shot setting.\nModel\nLeak Ratio\non NaPÂ² (â†“)\nFull Successful Record\non PRIVASIS (â†‘)\nNaPÂ²-Finetuned (4B)\n10.0 %\n32.0 %\nPRIVASIS-CLEANER-4B (Ours)\n10.0 %\n72.5 %\nSuccessful Record metric, which is significantly lower than PRIVASIS-CLEANER-4B (72.5%).\nThis demonstrates the strong robustness of the PRIVASIS-CLEANER model and suggests that\ntraining on PRIVASIS yields superior generalization due to its scale and diversity.\n5\nRelated Work\nWe position our work within privacy-preserving data generation. Table 3 summarizes\nexisting privacy datasets (extended discussion in Appendix F).\nSynthetic Data Generation: Related approaches include differential privacy methods using\nDP-SGD (Abadi et al., 2016) or public-to-private pipelines (Mattern et al., 2022; Yue et al.,\n2023; McKenna et al., 2025; Lin et al., 2024; Xie et al., 2024; Zhang et al., 2025a). Non-\nprivate methods include self-instruction (Wang et al., 2023; Hugging Face, 2025; Ye et al.,\n2025), targeted prompting (Gunasekar et al., 2023; Abdin et al., 2024), and automated\nrefinement (NadË‡aÂ¸s et al., 2025; Kim et al., 2023; Jung et al., 2024), but rely on fixed prompts\nor seed data, limiting diversity (Havrilla et al., 2024; Jung et al., 2025). PRIVASIS generates\nmillion-scale datasets from scratch using auxiliary control variables and diversity-preserving\nrefinement without predefined prompts.\nPII Removal Datasets: Classic corpora for anonymization establish span-level detec-\ntion (Stubbs et al., 2015; PilÃ¡n et al., 2022), while recent work expands through synthetic\ndocuments and LLM interactions (Gretel.ai, 2024; Selvam & Ghosh, 2025; Zeng et al., 2025a;\nYukhymenko et al., 2024). These remain small-scale or domain-specific; PRIVASIS provides\nlarge-scale, multi-domain coverage with both PII and sensitive spans.\nData Minimization: Related work abstracts non-PII sensitive details through disclosure\nrewrites (Dou et al., 2024), naturalness benchmarks (Huang et al., 2025), LLM anonymiza-\ntion (Staab et al., 2025; Zeng et al., 2025b), and generalization strategies (Olstad et al., 2023;\nPapadopoulou et al., 2023). PRIVASIS unifies these approaches with fine-grained span labels\nand parallel abstraction/removal pairs across diverse document types.\n6\nConclusion\nWe introduced PRIVASIS, the first million-scale synthetic dataset with rich private informa-\ntion, addressing fundamental data scarcity in privacy-sensitive research. Built from scratch\nusing auxiliary control variables and diversity-preserving refinement, PRIVASIS contains\nover 1M records spanning medical, financial, legal, email, calendar, and meeting domains.\nUsing this resource, we developed PRIVASIS-SANITIZATION with a decomposition-based\npipeline enabling small models (â‰¤4B) to outperform frontier LLMs like GPT-5 and Qwen3-\n235B on text sanitization.We plan to release all code, data, and models, to stimulate research\nin privacy-preserving generation, controllable sanitization, and agentic systems processing\nsensitive data.\nEthics Statement\nThe primary goal of our work is to address a fundamental bottleneck in privacy-sensitive\nresearch: the scarcity of large-scale public datasets containing rich sensitive attributes.\n11\n"}, {"page": 12, "text": "Preprint.\nMethodology and Privacy Safeguards\nTo bridge this gap, all data in the PRIVASIS dataset\nare fully synthetic and were generated entirely from scratch for research and evaluation\npurposes only. The data generation process relies on auxiliary control variables and publicly\navailable name databases and does not incorporate or reference any real-world private data.\nBy construction, this approach avoids the ethical and legal risks associated with collecting,\nstoring, or distributing sensitive human information.\nAll personal identifiersâ€”including names, addresses, and Social Security numbersâ€”are\nentirely fictitious, and any resemblance to real persons (living or deceased), business entities,\nor locations is purely coincidental. Manual verification procedures confirmed that no\ngenerated profiles correspond to actual individuals, ensuring that the dataset poses a low\nrisk of privacy infringement or re-identification.\nIntended Use and Real-World Validity\nThe synthesis pipeline and dataset were designed\nexplicitly to advance research in privacy-preserving machine learning, including the de-\nvelopment and evaluation of data sanitization methods, differential privacy techniques,\nand responsible AI agent frameworks. The dataset does not represent real-world events\nand should not be used for clinical decision-making, financial analysis, or as a basis for any\naction involving real individuals.\nCommunity Commitment\nIn alignment with the principles of transparency, reproducibil-\nity, and open science, we commit to releasing all models, code, and data to the research\ncommunity. Use of the dataset is restricted to non-commercial research and evaluation\npurposes. We explicitly prohibit any attempts to re-identify individuals or to misuse the\ndata for fraudulent or harmful activities. This synthetic data framework enables ethical ex-\nploration of methods designed for privacy-critical settings while preserving human privacy\nby design.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal\nTalwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016\nACM SIGSAC conference on computer and communications security, pp. 308â€“318, 2016.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4\ntechnical report. arXiv preprint arXiv:2412.08905, 2024.\nEugene Bagdasarian, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong\nOh, Borja Balle, and Daniel Ramage. Airgapagent: Protecting privacy-conscious conver-\nsational agents. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and\nCommunications Security, CCS â€™24, pp. 3868â€“3882, New York, NY, USA, 2024. Association\nfor Computing Machinery. ISBN 9798400706363. doi: 10.1145/3658644.3690350. URL\nhttps://doi.org/10.1145/3658644.3690350.\nDaixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via\nreading comprehension. In The Twelfth International Conference on Learning Representations,\n2024. URL https://openreview.net/forum?id=y886UXPEZ0.\nMichael A Covington and Joe D McFall. Cutting the gordian knot: The moving-average\ntypeâ€“token ratio (mattr). Journal of quantitative linguistics, 17(2):94â€“100, 2010.\nYao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, and Wei Xu.\nReducing privacy risks in online self-disclosures with language models. In Lun-Wei Ku,\nAndre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 13732â€“13754, Bangkok,\nThailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n2024.acl-long.741. URL https://aclanthology.org/2024.acl-long.741/.\n12\n"}, {"page": 13, "text": "Preprint.\nM. Douglass, G.D. Clifford, A. Reisner, G.B. Moody, and Mark RG. Computer-assisted\nde-identification of free text in the mimic ii database. In Computers in Cardiology, 2004, pp.\n341â€“344, 2004. doi: 10.1109/CIC.2004.1442942.\nDan Friedman and Adji Bousso Dieng. The vendi score: A diversity evaluation metric for\nmachine learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=g97OHbQyk1.\nLeon Garza, Anantaa Kotal, Aritran Piplai, Lavanya Elluri, Prajit Das, and Aman Chadha.\nPrvl: Quantifying the capabilities and risks of large language models for pii redaction.\narXiv preprint arXiv:2508.05545, 2025.\nTao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic\ndata creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024.\nGoogle. Gemini agent. https://gemini.google/overview/agent/, 2025. Accessed: 2026-01-\n26.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\nllama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\nGretel.ai. Synthetic financial pii multilingual dataset. https://huggingface.co/datasets/\ngretelai/synthetic_pii_finance_multilingual, 2024.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CÃ©sar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,\net al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\nAlex Havrilla, Andrew Dai, Laura Oâ€™Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak,\nFabrizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, et al. Surveying\nthe effects of quality, diversity, and complexity in synthetic data from large language\nmodels. arXiv preprint arXiv:2412.02980, 2024.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient at-\ntentions for long document summarization. In Kristina Toutanova, Anna Rumshisky,\nLuke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy\nChakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n1419â€“1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112/.\nShuo Huang, William Maclean, Xiaoxi Kang, Qiongkai Xu, Zhuang Li, Xingliang Yuan,\nGholamreza Haffari, and Lizhen Qu. NAP2: A benchmark for naturalness and privacy-\npreserving text rewriting by learning from human. In Christos Christodoulopoulos,\nTanmoy Chakraborty, Carolyn Rose, and Violet Peng (eds.), Findings of the Association for\nComputational Linguistics: EMNLP 2025, pp. 8954â€“8970, Suzhou, China, November 2025.\nAssociation for Computational Linguistics. ISBN 979-8-89176-335-7. doi: 10.18653/v1/\n2025.findings-emnlp.476. URL https://aclanthology.org/2025.findings-emnlp.476/.\nHugging Face. Open r1: A fully open reproduction of deepseek-r1, January 2025. URL\nhttps://github.com/huggingface/open-r1.\nAlistair E W Johnson, Tom J Pollard, Lu Shen, Li-Wei H Lehman, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.\nMimic-iii, a freely accessible critical care database. Scientific data, 3(1):1â€“9, 2016.\nJaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang Wei Koh, and Yejin\nChoi. Information-theoretic distillation for reference-less summarization. In First Confer-\nence on Language Modeling, 2024. URL https://openreview.net/forum?id=JXcXnJJSuL.\n13\n"}, {"page": 14, "text": "Preprint.\nJaehun Jung, Seungju Han, Ximing Lu, Skyler Hallinan, David Acuna, Shrimai Prabhumoye,\nMostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, and Yejin Choi. Prismatic\nsynthesis: Gradient-based data diversification boosts generalization in LLM reasoning.\nIn The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL\nhttps://openreview.net/forum?id=R0dC7Xzwbk.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou,\nRonan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. SODA: Million-\nscale dialogue distillation with social commonsense contextualization. In Houda Bouamor,\nJuan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 12930â€“12949, Singapore, December 2023. Association\nfor Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.799. URL https:\n//aclanthology.org/2023.emnlp-main.799/.\nBryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classification re-\nsearch. In Jean-FranÃ§ois Boulicaut, Floriana Esposito, Fosca Giannotti, and Dino Pedreschi\n(eds.), Machine Learning: ECML 2004, pp. 217â€“226, Berlin, Heidelberg, 2004. Springer\nBerlin Heidelberg. ISBN 978-3-540-30115-8.\nSiyan Li, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, and Zhou Yu.\nPAPILLON: Privacy preservation from Internet-based and local language model ensem-\nbles. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers), pp. 3371â€“3390, Albuquerque, New Mexico,\nApril 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.\n18653/v1/2025.naacl-long.173. URL https://aclanthology.org/2025.naacl-long.173/.\nZinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, and Sergey Yekhanin. Dif-\nferentially private synthetic data via foundation model APIs 1: Images. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/\nforum?id=YEhQs8POIo.\nJustus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard SchÃ¶lkopf, and Mrinmaya\nSachan. Differentially private language models for secure data sharing. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 4860â€“4873, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nemnlp-main.323. URL https://aclanthology.org/2022.emnlp-main.323/.\nRyan McKenna, Yangsibo Huang, Amer Sinha, Borja Balle, Zachary Charles, Christopher A.\nChoquette-Choo, Badih Ghazi, Georgios Kaissis, Ravi Kumar, Ruibo Liu, Da Yu, and\nChiyuan Zhang. Scaling laws for differentially private language models. In Forty-second\nInternational Conference on Machine Learning, 2025. URL https://openreview.net/forum?\nid=DE6dqmcmQ9.\nYev Meyer and Dane Corneil. Nemotron-Personas-USA: Synthetic personas aligned to\nreal-world distributions, June 2025. URL https://huggingface.co/datasets/nvidia/\nNemotron-Personas-USA.\nNiloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza\nShokri, and Yejin Choi. Can LLMs keep a secret? testing privacy implications of language\nmodels via contextual integrity theory. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=gmg7t8b4s0.\nMihai NadË‡aÂ¸s, Laura DioÂ¸san, and Andreea Tomescu. Synthetic data generation using large\nlanguage models: Advances in text and code. IEEE Access, 13:134615â€“134633, 2025. ISSN\n2169-3536. doi: 10.1109/access.2025.3589503. URL http://dx.doi.org/10.1109/ACCESS.\n2025.3589503.\nAnnika Willoch Olstad, Anthi Papadopoulou, and Pierre Lison. Generation of replacement\noptions in text sanitization. In Tanel AlumÃ¤e and Mark Fishel (eds.), Proceedings of the\n24th Nordic Conference on Computational Linguistics (NoDaLiDa), pp. 292â€“300, TÃ³rshavn,\n14\n"}, {"page": 15, "text": "Preprint.\nFaroe Islands, May 2023. University of Tartu Library. URL https://aclanthology.org/\n2023.nodalida-1.30/.\nOpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.\n10925.\nOpenAI.\nIntroducing\nchatgpt\nhealth.\nhttps://openai.com/index/\nintroducing-chatgpt-health/, January 2026. Accessed: 2026-01-26.\nAnthi Papadopoulou, Yunhao Yu, Pierre Lison, and Lilja Ã˜vrelid. Neural text sanitization\nwith explicit measures of privacy risk. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and\nChua-Hui Chang (eds.), Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the 12th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 217â€“229, Online only, November 2022.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.aacl-main.18. URL\nhttps://aclanthology.org/2022.aacl-main.18/.\nAnthi Papadopoulou, Pierre Lison, Mark Anderson, Lilja Ã˜vrelid, and IldikÃ³ PilÃ¡n. Neural\ntext sanitization with privacy risk indicators: An empirical analysis.\narXiv preprint\narXiv:2310.14312, 2023.\nDzung Pham, Peter Kairouz, Niloofar Mireshghallah, Eugene Bagdasarian, Chau Minh\nPham, and Amir Houmansadr. Can large language models really recognize your name?\narXiv preprint arXiv:2505.14549, 2025.\nIldikÃ³ PilÃ¡n, Pierre Lison, Lilja Ã˜vrelid, Anthi Papadopoulou, David SÃ¡nchez, and Montser-\nrat Batet. The text anonymization benchmark (TAB): A dedicated corpus and evaluation\nframework for text anonymization. Computational Linguistics, 48(4):1053â€“1101, December\n2022. doi: 10.1162/coli_a_00458. URL https://aclanthology.org/2022.cl-4.19/.\nLG Research, Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu\nChoi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, et al. Exaone 3.5:\nSeries of large language models for real-world use cases. arXiv preprint arXiv:2412.04862,\n2024.\nSriram Selvam and Anneswa Ghosh. Panorama: A synthetic pii-laced dataset for studying\nsensitive data memorization in llms. arXiv preprint arXiv:2505.12238, 2025.\nYijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, and Diyi Yang. Privacylens: Evaluating\nprivacy norm awareness of language models in action. In The Thirty-eight Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https:\n//openreview.net/forum?id=CxNXoMnCKc.\nRobin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. Language models are ad-\nvanced anonymizers. In The Thirteenth International Conference on Learning Representations,\n2025. URL https://openreview.net/forum?id=82p8VHRsaK.\nPeter Steinberger. Openclaw: The ai that actually does things, 2026. URL https://openclaw.\nai/. Accessed: 2026-01-30.\nAmber Stubbs, Christopher Kotfila, and Ã–zlem Uzuner. Automated systems for the de-\nidentification of longitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared\ntask track 1. Journal of biomedical informatics, 58:S11â€“S19, 2015.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated\ninstructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 13484â€“13508, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.\nacl-long.754/.\n15\n"}, {"page": 16, "text": "Preprint.\nChulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori,\nHaotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, and Sergey Yekhanin. Differentially pri-\nvate synthetic data via foundation model APIs 2: Text. In Forty-first International Conference\non Machine Learning, 2024. URL https://openreview.net/forum?id=LWD7upg1ob.\nRui Xin, Niloofar Mireshghallah, Shuyue Stella Li, Michael Duan, Hyunwoo Kim, Yejin\nChoi, Yulia Tsvetkov, Sewoong Oh, and Pang Wei Koh. A false sense of privacy: Eval-\nuating textual data sanitization beyond surface-level privacy leakage. arXiv preprint\narXiv:2504.21035, 2025.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint\narXiv:2505.09388, 2025.\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: Less\nis more for reasoning. In Second Conference on Language Modeling, 2025. URL https:\n//openreview.net/forum?id=T2TZ0RY4Zk.\nXiang Yue, Huseyin Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari,\nHuan Sun, David Levitan, and Robert Sim. Synthetic text generation with differen-\ntial privacy: A simple and practical recipe. In Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pp. 1321â€“1342, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.74. URL\nhttps://aclanthology.org/2023.acl-long.74/.\nHanna Yukhymenko, Robin Staab, Mark Vero, and Martin Vechev. A synthetic dataset for\npersonal attribute inference. In The Thirty-eight Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=\n1nqfIQIQBf.\nHang Zeng, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Shaojie Tang, and Guihai Chen.\nAutomated privacy information annotation in large language model interactions. arXiv\npreprint arXiv:2505.20910, 2025a.\nZiqian Zeng, Jianwei Wang, Junyao Yang, Zhengdong Lu, Haoran Li, Huiping Zhuang, and\nCen Chen. PrivacyRestore: Privacy-preserving inference in large language models via pri-\nvacy removal and restoration. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and\nMohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 10821â€“10855, Vienna, Austria,\nJuly 2025b. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi:\n10.18653/v1/2025.acl-long.532. URL https://aclanthology.org/2025.acl-long.532/.\nJianqing Zhang, Yang Liu, JIE FU, Yang Hua, Tianyuan Zou, Jian Cao, and Qiang Yang.\nPCEvolve: Private contrastive evolution for synthetic dataset generation via few-shot\nprivate data and generative APIs. In Forty-second International Conference on Machine\nLearning, 2025a. URL https://openreview.net/forum?id=IKCfxWtTsu.\nQingjie Zhang, Han Qiu, Di Wang, Yiming Li, Tianwei Zhang, Wenyu Zhu, Haiqin Weng,\nLiu Yan, and Chao Zhang. A benchmark for semantic sensitive information in LLMs\noutputs. In The Thirteenth International Conference on Learning Representations, 2025b. URL\nhttps://openreview.net/forum?id=p3mxzKmuZy.\nJijie Zhou, Eryue Xu, Yaoyao Wu, and Tianshi Li. Rescriber: Smaller-llm-powered user-led\ndata minimization for llm-based chatbots. In Proceedings of the 2025 CHI Conference on\nHuman Factors in Computing Systems, CHI â€™25, New York, NY, USA, 2025. Association\nfor Computing Machinery. ISBN 9798400713941. doi: 10.1145/3706598.3713701. URL\nhttps://doi.org/10.1145/3706598.3713701.\n16\n"}, {"page": 17, "text": "Preprint.\nA\nSynthesizing PRIVASIS\nGenerating Profiles: Given a sampled first name from the US SSN applicant database, we\nprompt the LLM with a fixed set of attributes to fill it: last name, sex, ethnicity, citizenship,\nID type, ID number, passport number, phone number, email, user handle, URL, and life\nevent. For sex, ethnicity and life event, we provide predefined options for the LLM to\nchoose. Box 1 shows the template.\nBox 1: Prompt for Generating Profiles\nI will provide a set of demographic attributes of a person. Generate a complete personal profile by populating the fields\nin the provided structure below. Ensure the entries for the profile fields are realistic, and consistent. Be creative. You will\nbe given an option list for some attribute types to choose from for events and populate.\nDemographic attributes\n{}\nGenerate the following:\n- Last Name:\n- Sex: Choose from {}\n- Ethnicity: Choose from {}\n- Citizenship:\n- ID type:\n- ID Number:\n- Passport Number:\n- Phone Number:\n- Email:\n- User Handle:\n- URL:\nLife event list\n{}\nOr you can come up with attributes and details as you please.\nNow populate the above fields, include the provided demographic information in the output as well.\nMake\nnecessary changes to ensure consistency. List event attributes under the key attributes. Only output the completed\nprofile.\nGenerating Record Types: Given a profile, we generate a list of candidate record types and\nrandomly select one of them to promote diversity. The prompt used is presented in Box 2.\nBox 2: Prompt for Generating Record Types\nProfile:\n{profile}\nGenerate a diverse and realistic list of types of {record formality} that contains {name}â€™s private information\nâ€™attributeâ€™.\nRequirements:\n1. Each record type must specify the exact source/organization (e.g., \"Reddit post from r/relationships\", \"Patient record\nfrom Mayo Clinic\", \"Tax document from IRS\", \"Employment record from Google\")\n2. Include a mix of different contexts (personal, professional, medical, legal, financial, etc.)\n3. Consider both digital and physical record types\n4. Include both common and unusual/unique record types\n5. Ensure all types are textual (no images/videos)\n6. Each record type should be specific and detailed, not generic\nOutput the list as an ordered list, with each type being specific and detailed.\nDo not include any additional\ncomments.\nGenerating Background Contexts: Given the profile and record type, we generate a list\nof background contexts and randomly select one of them to encourage diversity. Box 3\ncontains the prompt that we use.\n17\n"}, {"page": 18, "text": "Preprint.\nBox 3: Prompt for Generating Background Contexts\nProfile:\n{profile}\nGenerate five creative and specific contexts for the â€™{record type}â€™ that contains {profile[â€™first_nameâ€™]}â€™s â€™{at-\ntribute}â€™.\nRequirements:\nEach context should be specific, detailed, realistic and plausible.\nInclude a diverse range of\nemotional contexts. Consider unique different scenarios, and be detailed enough to understand the situation.\nOutput the five contexts in an ordered list with each item in plain text.\nEach context should be specific and\ndetailed, providing a clear situation without generating the actual record. Do not include any additional comments.\nGenerating Record Formats: Given a record type and the background context, we generate\na list of candidate record formats and randomly select one of them to promote diversity.\nThe prompt used is in Box 4.\nBox 4: Prompt for Generating Record Formats\nRecord type: {record type}\nSituation: {background context}\nBased on the situation described above, outline what the structure should look like for â€™{record type}â€™.\nDe-\nscribe ten diverse and realistic possible structures and their tone for the â€™{record type}â€™ written in plain text in an ordered\nlist. Do not include any values, just a plain description of the structure and tone. Tone should be realistic and diverse,\nnot too cheerful.\nGenerating Records: Given the profile, record type, background context, and format, we\ngenerate the record with the prompt in Box 5.\nBox 5: Prompt for Generating Records\nGenerate a realistic, detailed and creative â€™{record type}â€™ in English according to the situation above. Follow these\nguidelines:\n1. Use Profile Information:\n- Incorporate relevant attributes from {name}â€™s profile\n- Ensure all personal details match the profile exactly\n2. Add Specific Details:\n- Include exact dates (avoid using 15th, use other random dates)\n- Specify precise locations with addresses or landmarks\n- Add realistic timestamps and durations\n- Include specific measurements, quantities, and numbers\n- Instead of monotonic numbers (e.g., 12345, 9876), use other random numbers\n- Use accurate terminology and jargon\n3. Structure and Format:\n- Follow the specified structure and tone: {form}\n- Keep the text dense and information-rich\n- Minimize markdown formatting\nOnly output the generated â€™{record type}â€™ without any additional comments or explanations.\nCost for API models: Generating 10K records with GPT-4.1 costs approximately $900, while\nsanitizing 10K records with GPT-4.1 costs about $1,100.\nA.1\nAnalysis\nComparing Different LLMs for the Synthesis Pipeline:\nWe compare multiple models using diversity metrics and output length, and we also\nperformed manual quality inspection. Table 8 summarizes the results. Although some\nmodels outperform GPT-OSS-120B on individual metrics, its performance is consistently\nstrong across all measures. It also generates longer text than other models, which is crucial\nfor synthesizing documents. A manual review also showed that GPT-OSS-120B generations\n18\n"}, {"page": 19, "text": "Preprint.\nhave high coherence. When factoring in cost, it provides a substantially better price-\nperformance ratio than frontier models (e.g., Gemini-2.5-pro, GPT-5, Qwen3-235B), making\nit more suitable for large-scale generation.\nTable 8: Model diversity and lexical metrics.\nModel\nMATTR\nBigram Diversity\nShannon Entropy\nCosine Similarity\nVendi Score\nNumber of Words\ngemini-2.5-pro\n0.8031\n0.9033\n7.1188\n0.2729\n74.57\n521.9\ngpt-5\n0.8072\n0.9076\n7.3514\n0.3287\n50.90\n1168.1\ngpt-4.1-mini\n0.8332\n0.8994\n7.2104\n0.2733\n76.74\n477.6\ngpt-oss-120b\n0.8108\n0.8970\n7.2613\n0.3188\n62.75\n611.8\nqwen3 235b\n0.8278\n0.9135\n7.3384\n0.2880\n70.06\n689.1\nqwen3-80b\n0.8133\n0.9162\n7.1980\n0.3249\n58.27\n463.2\nllama4-maverick\n0.7988\n0.8669\n6.9019\n0.2667\n80.07\n356.1\nllama-3.3-70b\n0.7935\n0.8665\n6.8900\n0.2817\n74.58\n422.1\nexaone3.5-32b\n0.8036\n0.8744\n7.0504\n0.2931\n70.58\n440.7\nAblation on the Synthesis Pipeline: We conduct an ablation study on the components of\nour synthesis pipeline using GPT-OSS-120B to generate 500 records. Table 9 reports the\ndiversity metrics (as in Table 2) when specific components are removed. We additionally\nreport the Vendi score (Friedman & Dieng, 2023). Without the auxiliary control variables for\nrecord type and format, the semantic diversity of the synthesized recordsâ€”measured by\ncosine similarity and the Vendi scoreâ€”drops sharply, even though the diversity-preserving\nterm is still present. Similarly, when records are iteratively revised without the diversity-\npreserving term, semantic diversity decreases markedly. In contrast, our full pipeline\nwith diversity-preserving iterative refinement (Â§2.1) achieves significantly higher semantic\ndiversity compared to the variant without revision.\nDistribution of the Generated Profiles: Figure 3 shows the distribution of the ethnicity\nin the generated profiles. We find that East Asian, Latin American, Hispanic, and Mestizo\ngroups are particularly under-represented in PRIVASIS, compared to real-world demograph-\nics. Future work should aim to incorporate more balanced sampling to better reflect global\ndemographics.\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\nSouth Asian\nEuropean\nBlack\nSoutheast Asian\nMediterranean\nWhite\nEast Asian\nEast African\nSouthern African\nMiddle Eastern\nMixed / Multi-ethnic\nWest African\nArab\nLatino\nHispanic\nMestizo\nFigure 3: The ethnicity distribution in PRIVASIS.\nSanity Check on Profile Search with Authors: As an additional sanity check, we ran Gemini-\n2.5-Pro Deep Research on all of the authors of this paper. The model correctly confirmed all\nof us as real individuals, which increases confidence in its ability to distinguish fabricated\nfrom real profiles.\nCategory Annotation: Box 6 shows the prompt for labeling the broader categories for each\nrecord.\nBox 6: Category Annotation Prompt\nGiven this document, either select the most appropriate category from the existing list OR generate a new BROAD\ncategory name if none fit well.\nExisting categories: {categories list}\n19\n"}, {"page": 20, "text": "Preprint.\nTable 9: Diversity of ablation of the components in the PRIVASIS synthesis pipeline.\nDataset\nMATTR (â†‘)\nBigram\nDiversity (â†‘)\nShannon\nEntropy (â†‘)\nCosine\nSimilarity (â†“)\nVendi\nScore (â†‘)\nPRIVASIS\nwithout Record Type and Format\n0.808\n0.905\n7.48\n0.408\n39.4\nwithout Revision\n0.809\n0.885\n7.07\n0.324\n53.2\nwithout Diversity-preserving Term\n0.805\n0.863\n7.40\n0.354\n50.1\nFull Pipeline\n0.810\n0.890\n7.14\n0.322\n57.3\nDocument: {record}\nInstructions:\n- PREFER using an existing category if the document reasonably fits\n- Only create a new category if the document is fundamentally different from existing ones\n- New categories should be BROAD and GENERAL (like \"financial records\", \"employment documents\", \"legal contracts\")\n- Avoid overly specific categories\n- Respond with only the category name, nothing else. Do not include any other text or explanation.\nCategory Name:\nFigure 4 illustrates the distribution of the record subcategories. The subcategory â€˜Medical\nCareâ€™ is the most prevalent, comprising a significant portion of the broader main category\nHealth & Wellness.\nHealth & Wellness -> Medical Care\nProfessional Services -> Project Management\nPersonal & Family -> Personal Development\nEducation & Training -> Academic Administration\nBusiness & Finance -> Employment & HR\nGovernment & Civic -> Immigration & Citizenship\nCommunity & Social -> Community Services\nBusiness & Finance -> Financial Management\nHealth & Wellness -> Mental Health & Support\nGovernment & Civic -> Legal Proceedings\nGovernment & Civic -> Public Administration\nHealth & Wellness -> Healthcare Administration\nLegal & Compliance -> Criminal Justice\nCommunity & Social -> Religious & Spiritual\nLegal & Compliance -> Contracts & Agreements\nPersonal & Family -> Family Relationships\nPersonal & Family -> Life Events\nBusiness & Finance -> Accounting & Tax\nEducation & Training -> Financial Aid\nLegal & Compliance -> Court & Litigation\nMedia & Communications -> Publishing & Content\nBusiness & Finance -> Corporate Operations\nGovernment & Civic -> Civil Rights & Services\nHealth & Wellness -> Specialized Health\nMedia & Communications -> Creative Arts\nLegal & Compliance -> Regulatory & Compliance\nRecreation & Lifestyle -> Travel & Tourism\nCommunity & Social -> Volunteer & Nonprofit\nProfessional Services -> Consulting & Advisory\nProfessional Services -> Specialized Services\nCommunity & Social -> Membership & Organizations\nProfessional Services -> Research & Analysis\nEducation & Training -> Learning & Development\nPersonal & Family -> Estate & Legal\nMedia & Communications -> Communication\nTechnical & Operations -> System Administration\nBusiness & Finance -> Insurance & Risk\nEducation & Training -> Specialized Education\nRecreation & Lifestyle -> Food & Culinary\nRecreation & Lifestyle -> Entertainment & Hobbies\nMedia & Communications -> Digital Media\nLegal & Compliance -> Intellectual Property\nRecreation & Lifestyle -> Personal Interests\nBusiness & Finance -> Supply Chain\nTechnical & Operations -> Technical Support\nLegal & Compliance -> Archive & Records\nSubcategory\n0\n2000\n4000\nCount\nDistribution of Subcategories (sorted)\nFigure 4: Distribution of the subcategories of the records in PRIVASIS-SANITIZATION.\nSupport for Synthesizing Multilingual Outputs: We also recognize the importance of gen-\nerating nonâ€“English data. To this end, we leveraged non-U.S. LLMs in our data generation\nprocess, including Exaone 3.5 (South Korea) and Qwen3 (China). Although the primary\nlanguages of these models are not English, they perform well within our English-language\npipeline. Therefore, we expect them to perform as good as when they are prompted in their\nmajor language.\nTo validate this, we generated Chinese and Korean records with Exaone 3.5 32B, Qwen3 80B,\nand GPT-4.1 through our pipeline. We then conducted native-speaker human evaluation\nwith both Korean and Chinese reviewers. The reviewers confirmed that the generated\nrecords were coherent, contextually appropriate, and diverse (with cosine similarity scores\nof 0.34 and 0.36, respectively), making them comparable to English ones. These results\nsuggest that our synthesis pipeline can be directly applied to LLMs whose primary language\nis not English.\nB\nBuilding the Sanitization Parallel Corpus\nAlgorithm 1 describes our sanitization pipeline (Â§3.1) in detail.\n20\n"}, {"page": 21, "text": "Preprint.\nAlgorithm 1 Sanitization Pipeline\nRequire: Record x with attributes A; chunk threshold Ï„ = 512 chars; number of targets n\nEnsure: Sanitized record Ëœx and user-style instruction bI\n1: function SANITIZERECORD(x, A, Ï„, n)\nâ–·1) Decomposition\n2:\nC â†DECOMPOSE(x, Ï„)\nâ–·Split x until each chunk |c| â‰¤Ï„\nâ–·2) Target Selection\n3:\nW â†SENSITIVITYWEIGHTLLM(A)\nâ–·Higher wa for highly sensitive attributes\n4:\nT â†SAMPLETARGETS(A, n, W)\nâ–·Targets may be individual attributes or attribute groups\n5:\nfor all z âˆˆT do\n6:\nâ„“z â†RANDOMLABEL({abstract, drop})\n7:\nend for\nâ–·3) Sanitization\n8:\nfor all z âˆˆT do\nâ–·Chunk ops run in parallel within the same z, but run sequentially across\ndifferent z\n9:\nCz â†FINDRELEVANTCHUNKSLLM(z, C)\n10:\nfor all c âˆˆCz do\n11:\nSz,c â†EXTRACTSPANSLLM(z, c)\n12:\nend for\n13:\nif â„“z = ABSTRACT then\n14:\ninstrz â†BUILDABSTRACTIONINSTRLLM(z, S\ncâˆˆCz c)\n15:\nelse\nâ–·â„“z = DROP\n16:\ninstrz â†FIXEDDROPINSTR(z)\n17:\nend if\n18:\nfor all c âˆˆCz do\nâ–·Apply uniformly for consistency across chunks\n19:\nc â†APPLYINSTRUCTION(c, Sz,c, instrz)\n20:\nend for\n21:\nend for\n22:\nËœx â†MERGECHUNKS(C)\nâ–·4) Instruction Generation\n23:\nK â†SELECTKEEPATTRIBUTES(A)\nâ–·Optional: explicit â€˜keepâ€™ attributes for utility\n24:\nbI â†GENERATEUSERINSTRUCTIONLLM({instrz}zâˆˆT , K)\n25:\nreturn (x, bI, Ëœx)\n26: end function\nWhy select retention target attributes with the lowest lexical overlap with sanitization\ntarget attributes? If the retention target attributes are too similar to the sanitization target\nattributes, they often end up containing or overlapping with the sanitization targets. In such\ncases, if the model were to sanitize correctly, it becomes desirable to sanitize the retention\ntargets as well. This leads to worse performance of strong sanitizers. Ideally, retention\ntarget attributes should be chosen to be as semantically distant as possible from sanitization\ntargets. However, we find that current LLMs struggle with this task. Therefore, we resort to\nmeasuring lexical overlap using ROUGE.\nWhat is the optimal chunk size when decom-\nposing the text? When designing our saniti-\nzation pipeline, we conduct experiments with\ndifferent chunk sizes. Performance peaks at a\nchunk size of 512 and then gradually declines\nfor larger sizes. Smaller chunks (e.g., 128) lose\nthe surrounding context necessary for saniti-\nzation, while overly large chunks (e.g., 2048)\ncause the models to struggle, similar to the\nvanilla case without our pipeline. Therefore,\ndecomposing long text records into appropri-\nately sized chunks is crucial for effective sani-\ntization.\n30.00%\n40.00%\n50.00%\n60.00%\n500\n1000\n1500\n2000\nFigure 5: Sanitization performance for\ndifferent chunk sizes.\n21\n"}, {"page": 22, "text": "Preprint.\nC\nTraining\nWe finetune Qwen3 4B and 0.6B with DeepSpeed on NVIDIA H100s. Llama-3.2 4B was\ntrained on 2 GPUs with a micro-batch of 2 and 16 accumulation steps (global batch 64),\nusing AdamW (lr = 1e-5, weight decay = 0.01), WarmupDecayLR (50-step warmup, 1,000\ntotal), bf16, and ZeRO stage-2. Qwen3 0.6B was trained on 4 GPUs with a micro-batch of 1\nand 20 accumulation steps (global batch 80), using AdamW (lr = 1e-5, weight decay = 0.1),\nWarmupDecayLR (23-step warmup, 750 total), bf16, and ZeRO stage-3.\nD\nEvaluation\nEvaluating Retention Target Attributes: Given the retention target attribute set K, we first\nsearch for an exact string match for each kvalue âˆˆK in the sanitized text Ëœx. If no match\nis found, we then use the evaluator LLM (i.e., GPT-OSS-120B) to infer the attribute value\n(Ë†ksanitized) from the sanitized text given kkey, and perform exact string matchingâ€”similar to\nthe process used for the sanitization target attributes (Â§4.2). If no match is still found, we\ndirectly query the evaluator to determine whether each k is present in the given sanitized\ntext and its corresponding Ë†ksanitized. If the evaluator responds negatively, we mark it as a\nretention failure.\nVanilla test set vs. Hard test set: Box 7 and 8 present samples from the Vanilla and Hard\ntest sets, respectively. In the Vanilla set sample, sanitization is relatively straightforward\nbecause the record contains only a single occurrence of the attribute, and the instruction\nis direct. In contrast, the Hard test set sample has much longer instructions that require\ncontextual understanding to determine which attribute needs to be sanitized.\nBox 7: Vanilla Test Set Example\nSanitization instruction: If the attribute is â€™Gemalto token serialâ€™ and the value is â€™84213579â€™, replace\nit with â€™Gemalto token serial: [MASKED]â€™.\nForensic Financial Analysis Narrative Filed by Central Bank of Russia Compliance Office\nCase File: Alleged Embezzlement from Moscow Regional Library Fund\nSubject: Tanah Kuznetsova, born 1944-04-08, Internal Passport No. 4509 327684\nInitial Whistleblower Tip:\nOn 2002-07-17 at 14:43, an anonymous whistleblower contacted the Central Bank\nof Russia Compliance Office via email from a Tor browser, alleging embezzlement of 1,200,000 rubles from the Moscow\nRegional Library Fund. The tip referenced suspicious transactions processed through Sberbankâ€™s Kuznetsky Most\nbranch, specifically highlighting a series of wire transfers between November 2, 2001, and December 17, 2001.\nChronological Incident Reconstruction: The investigation commenced at 09:00 on 2002-07-18. Tanah Kuznetsova, a\n58-year-old married female with a middle income class and Slavic ethnicity, was identified as the primary suspect.\nAnalysis of Sberbank transaction logs revealed a series of 17 wire transfers totaling 1,200,000 rubles between 2001-11-02\nand 2001-12-17. The transactions originated from the Moscow Regional Library Fundâ€™s account (40702810500000000001)\nat Sberbankâ€™s Lubyanka branch and were routed through an intermediary account (40817810000000000002) held by\nKuznetsova at the same branch. The transfers were made in varying amounts, ranging from 50,000 to 100,000 rubles.\nA detailed financial audit of the Moscow Regional Library Fund was initiated at 10:05 on 2002-07-18, focus-\ning on transactions between 2001-10-01 and 2002-01-31. The audit, conducted by OJSC \"Audit Consulting\" at ul.\nMyasnitskaya, 47, Moscow, uncovered discrepancies in the fundâ€™s accounting records. Specifically, on 2001-11-02 at 11:17,\na transaction for 200,000 rubles was recorded under \"operational expenses,\" but the corresponding invoice (No. 4278)\nwas found to be falsified, with an altered vendor name and date.\nStatements from 14 witnesses, including colleagues and superiors of Kuznetsova, were collected between\n2002-07-22 and 2002-08-14. Witness testimonies corroborated Kuznetsovaâ€™s involvement in financial management and\nhighlighted her access to the fundâ€™s accounting systems. Notably, a colleague, Natalia Petrova, reported that Kuznetsova\nhad accessed the accounting database on 2001-12-15 at 20:47, using her personal login credentials. Petrova also stated\nthat Kuznetsova had been responsible for reconciling the fundâ€™s accounts and had expressed concerns about the audit\nprocedures.\nDigital transaction logs from Sberbankâ€™s data center at 19/1, Bolshaya Dmitrovka, Moscow, were analyzed.\nThe logs confirmed that Kuznetsova had authorized the wire transfers on 11 occasions between 2001-11-02 and\n2001-12-17, using a Gemalto token (Serial No. 84213579) issued to her on 2001-06-21. The logs also revealed that the\ntoken was used to access the accounting system on multiple occasions, often outside of regular business hours.\nThe forensic financial analysis narrative reconstructs the embezzlement scheme as follows:\nbetween 2001-11-\n22\n"}, {"page": 23, "text": "Preprint.\n02 and 2001-12-17, Kuznetsova executed a series of unauthorized wire transfers, totaling 1,200,000 rubles, from the\nMoscow Regional Library Fundâ€™s account to her intermediary account at Sberbank. The funds were subsequently\nlaundered through a series of transactions, involving 7 shell companies registered at various addresses in Moscow,\nincluding OOO \"Alpha Invest\" and OOO \"Beta Consulting\".\nKuznetsova maintained her innocence throughout the investigation, attributing the discrepancies to account-\ning errors. However, the cumulative evidence, including the detailed financial audit reports, witness statements, and\ndigital transaction logs, substantiates the allegation of embezzlement.\nThe investigation concluded on 2006-02-10, with the Moscow City Court issuing a full exoneration of Kuznetsova, citing\nlack of conclusive evidence. The courtâ€™s decision was influenced by the revelation that the whistleblowerâ€™s tip was\nbased on incomplete information and that key evidence had been tampered with, including the alteration of transaction\nrecords and the destruction of relevant documents.\nBox 8: Hard Test Set Example\nSanitization instruction:\nGeneralize all references to session date, session time, and clinic name;\ngeneralize all references to presenting issue, job stressors, and burnout factors including workload,\ninternal, and external pressures; and remove any references to â€™Headaches, insomnia (4-5 hrs)â€™ and marital\nstatus â€™divorcedâ€™ from the text.\n**Asmara Mental Health Clinic - Session Notes**\n**Client:** Sirley Tesfay (ID: ER-2011-847392)\n**Date:** September 21, 2011\n**Session Time:** 4:30 PM - 5:20 PM\n**Session Focus:** Occupational Burnout - Symptom Assessment & Causal Factors\n**Subjective/Objective:**\nClient (Pt.) presented with a flattened affect and psychomotor slowing, speaking in a low, monotone voice. She\nmaintained poor eye contact and appeared visibly fatigued. Pt. reports symptoms consistent with severe occupational\nburnout stemming from her role as a project coordinator at the Asmara International Conference Center on Harnet Ave.\nThe primary stressor is her current project, the \"Eritrea Mining & Development Conference,\" which she\nstates has created an unsustainable workload. Pt. described her workdays as \"a relentless cycle,\" typically running from\n7 AM to past 8 PM, six days a week. Specific duties causing distress include managing last-minute logistical changes for\ninternational delegates, translating technical documents from Tigrinya to English under tight deadlines, and navigating\nfriction with the catering manager.\n**Emotional/Social Impact:**\nThe emotional toll is significant. Pt. reports heightened irritability, anhedonia, and social withdrawal. She tearfully\nrecounted a recent argument with her younger sister, Selam, for missing the family coffee ceremony again, stating, \"I\nscreamed at her about the sugar. It wasnâ€™t about the sugar. Iâ€™m just... empty.\" Pt. expresses intense guilt over this,\nfeeling \"like a ghost\" in her own family. She links the pressure to succeed professionally to her divorced status, verbal-\nizing a deep-seated fear of being seen as a failure. This makes her current inability to cope feel like a validation of that fear.\n**Professional/Cognitive Impact:**\nPt. described a persistent \"mental fog\" and difficulty concentrating on complex tasks. The joy she once derived from her\nwork has been replaced by \"a constant, low-grade anxiety.\" She reports being less patient with vendors and feels her\nprofessional relationships are strained due to her exhaustion. Pt. also reports physical symptoms, including chronic\ntension headaches and difficulty sleeping more than 4-5 hours per night. No suicidal or homicidal ideation reported.\n**Assessment & Plan:**\nPt. is experiencing severe occupational burnout with significant emotional, cognitive, and physiological symptoms. The\ncore issue is a combination of excessive work demands and internal pressure to perform, exacerbated by her social\ncontext. The therapeutic focus will be on psychoeducation about burnout, implementing boundary-setting strategies,\nand reconnecting her sense of self-worth to values outside of her professional identity.\nâ€”\nHandwritten note from counseling session summarizing occupational burnout causes:\n**S. Tesfay - Burnout Factors**\n- **Workload:** Extreme hours (7a-8p+), 6 days/wk. Specific project (Mining Conf.) is the trigger.\n- **Pressure (Internal):** Linked to divorced status. Needs to prove independence/success. Sees burnout as a personal\nfailing.\n- **Pressure (External):** Last-minute changes, translation deadlines, interpersonal friction (staff).\n- **Symptom Cascade:** Exhaustion -> Irritability -> Family conflict (sister, Selam) -> Guilt -> Deeper exhaustion. A cycle.\n- **Identity Erosion:** Self-worth is 100% tied to job performance. Loss of passion -> loss of self. Feeling \"hollowed out.\"\n- **Physiological:** Headaches, insomnia (4-5 hrs).\n23\n"}, {"page": 24, "text": "Preprint.\nE\nError Analysis\nWhat are the attributes that models struggle the most? Table 10 shows the top 8 attributes\nthat models fail most often. We find the models struggle the most with name-related\nattributes (e.g., last name, full name, and user handle), and dates.\nTable 10: Top 8 failed attributes per model and their ratio.\nModel\n1\n2\n3\n4\n5\n6\n7\n8\no3\nfull_name\n(2.7%)\nlast_name\n(1.8%)\nevent_date\n(1.1%)\nfirst_name\n(1.1%)\nevent_type\n(0.9%)\nage\n(0.7%)\ncurrent_status\n(0.4%)\nsignature\n(0.4%)\nDeepSeek R1\nfull_name\n(1.6%)\nlast_name\n(1.0%)\nevent_date\n(1.0%)\nid_type\n(0.8%)\nfirst_name\n(0.8%)\nevent_location\n(0.7%)\nemployer\n(0.7%)\nevent_name\n(0.7%)\nGPT-5\nlast_name\n(1.1%)\nfull_name\n(1.0%)\nevent_date\n(1.0%)\nemployer\n(0.8%)\nid_type\n(0.8%)\ncontact_email\n(0.6%)\ncitizenship\n(0.6%)\ncurrent_status\n(0.4%)\nLLaMA-4\nMaverick\nlast_name\n(1.6%)\nfull_name\n(1.4%)\nevent_date\n(0.7%)\nevent_location\n(0.7%)\nage\n(0.7%)\nid_type\n(0.7%)\nposition\n(0.5%)\nmarital_status\n(0.5%)\nQwen3-235B\nevent_date\n(1.0%)\nlast_name\n(1.0%)\nuser_handle\n(0.7%)\nemployer\n(0.6%)\nfull_name\n(0.6%)\nevent_name\n(0.6%)\nfirst_name\n(0.5%)\nid_type\n(0.5%)\nPRIVASIS-CLEANER-4B\nemployer\n(1.5%)\nfull_name\n(1.5%)\nuser_handle\n(0.8%)\ncurrent_status\n(0.8%)\nlast_name\n(0.8%)\nevent_date\n(0.8%)\nresidential_address\n(0.8%)\nage\n(0.8%)\nWhat are some examples of the different information leakage types? We showcase\nexamples of GPT-5â€™s sanitization failure for each information leakage type (Â§4.2).\n(1) Direct Leak: Box 9 presents an example of a direct leak, in which GPT-5 failed to remove\nthe employer information â€˜Royal Darwin Hospitalâ€™. Although the model successfully\nremoved most occurrences in the main body, it overlooked the header and the affiliation\nin the email signature (highlighted in red). It is likely that the model did not recognize the\nheader and affiliation signature as employer-related, even though they contain the exact\nsame string as the employerâ€™s name. This indicates a limitation in the modelâ€™s contextual\nawareness and generalization.\nBox 9: Direct Leak Example\nSanitization instruction:\nReplace any mention of â€™NT Health Code of Conductâ€™ (code of conduct) with\nâ€™relevant organizational code of conductâ€™, and remove all Employment Details at Royal Darwin Hospital\nfor\nVisa\nExtension,\nincluding\nposition,\nemployer,\ndepartment,\nemployment\nstart\ndate,\ncontract\nend\ndate, monthly remuneration, work location, work hours per week, roster, reference number, employment\nconfirmation date, employment reference number, program assignment, weekly hours, and roster details from\nthe text.\nFailed sanitization target attribute = {employer: Royal Darwin Hospital}\nROYAL DARWIN HOSPITAL\nRocklands Drive, Tiwi, NT 0810, Australia\nEmail: rdh@health.nt.gov.au\nPhone: +61 8 8922 8888\nFax: +61 8 8922 8999\nDate: 23rd February 2016\nRef: RDH/JOSS/002/2016\nJossalynn Santoso\n23 Mitchell St, Darwin City, NT 0800, Australia\nDear Ms. Santoso,\nRe: Confirmation of Employment\nYour duties include conducting health assessments,\ndeveloping care plans,\nand collaborating with local\nhealthcare providers to deliver culturally sensitive healthcare services to remote communities in Papua.\nYour employment is subject to the terms and conditions outlined in your contract, which includes adherence\nto the relevant organizational code of conduct and the Australian Nursing and Midwifery Council (ANMC) Code of\nEthics.\nPlease acknowledge receipt of this letter by signing and returning a copy to the Human Resources Depart-\nment by 8th March 2016.\n24\n"}, {"page": 25, "text": "Preprint.\nSincerely,\nA/Prof. Kathryn J. McGrath\nDirector of Nursing & Midwifery\nRoyal Darwin Hospital\n(2) Inference Leak: Box 10 shows an example of inference leak in the sanitized record. GPT-5\nsuccessfully removed all occurrences of the journal name in the sanitized text by replacing\nit with the mask â€˜[journal name]â€™. However, the evaluator model GPT-OSS-120B was still\nable to infer the journal name using the email domain â€˜jsal.orgâ€™ of the editor (highlighted\nin yellow). This indicates that the model lacked the ability to generalize beyond surface-\nlevel string replacement and failed to account for indirect cues that can reveal masked\ninformation.\nBox 10: Inference Leak Example\nSanitization instruction:\nRedact all information about Research Paper Publication Detailsâ€”including\nManuscript content, Journal names, and Article Metadataâ€”in the text.\nFailed sanitization target attribute = {journal name: Journal of South Asian Linguistics}\nOfficial email correspondence archived by the [journal name] editorial board\nFrom: Dr. S. Venkatesan <editorial@jsal.org>\nTo: Ms. Aubrielle Ramanathan <aubrielle.ramanathan1950@gmail.com>\nSubject: Publication Confirmation and Next Steps â€“ â€œa comparative study of regional linguistic structuresâ€\nDear Ms. Ramanathan,\nWe are pleased to confirm that your manuscript, â€œa comparative study of regional linguistic structures,â€ has been\naccepted for publication in the [journal name] (journal volume and issue). The article will appear in the 2007 issue on\npages XXâ€“YY and will be assigned the DOI [REDACTED DOI].\nFinal Proofing Instructions\nâ€¢ Proof file â€“ A PDF proof (28 pages, 2.38 MB) has been uploaded to our secure portal: [journal name]\nResearch Paper Publication Details (Manuscript, Journal, and Article Metadata), article doi, [REDACTED DOI]\nâ€¢ What to check â€“ Please verify:\nâ€“ Typographical errors (including diacritics and special characters)\nâ€“ Figure placement, resolution (â‰¥300 dpi), and caption accuracy\nâ€“ Table widths (default 6.2 cm; may request reduction to â‰¤6.5 cm)\nâ€“ Reference formatting against the [journal name] style guide\nâ€“ Equation numbering and symbols\nâ€¢ How to annotate â€“ Use Adobe Acrobatâ€™s â€œCommentâ€ > â€œSticky Noteâ€ tool. Save as Ramanathan_Proof_Comments.pdf.\nâ€¢ Return deadline â€“ Email the annotated PDF no later than 2007, 17:00 IST.\nâ€¢ Layout adjustments â€“ Minor tweaks (e.g., table width, caption font size) may be requested free. Extensive redesigns\nincur a fee of INR 4,850.\nCopyright and Licensing\nâ€¢ After proof approval, a Copyright Transfer Agreement (CTA) (PDF, 0.82 MB) will be sent. This grants exclusive\nrights to [journal name] for five (5) years, after which rights revert to you.\nâ€¢ The article will be published under CC BY-NC 4.0. Free to share/distribute for non-commercial purposes with\ncitation:\nRamanathan, A., Venkatesan, S., & Subramaniam, P. (2007). a comparative study of regional linguistic structures. [journal\nname], journal volume and issue, XXâ€“YY. [REDACTED DOI]\nâ€¢ Licence text attached as CC_BY_NC_4.0.pdf (0.84 MB).\nAdministrative Details\nâ€¢ Editorial Office [journal name]\n12 Gokhale Road, New Delhi 110001, India\nPhone: +91-11-2367-8901\nEmail: editorial@jsal.org\nâ€¢ Correspondence\nâ€“\nFor\nurgent\nqueries,\ncontact\nDr.\nPriya\nSubramaniam,\n+91-98407-56321,\npriya.subramaniam@unimadras.edu.\n25\n"}, {"page": 26, "text": "Preprint.\nâ€¢ Your record: â€¢ Email: aubrielle.ramanathan1950@gmail.com\nâ€¢ Phone: +91-98407-56321\nâ€¢ Affiliation: Dept. of Linguistics, Univ. of Madras, Senate House, 61 Pantheon Rd, Guindy, Chennai 600 025, India\nWe commend the rigor of the 18-month research project that underpins this manuscript. Please acknowledge receipt of\nthis email by replying no later than 2007.\nThank you for choosing [journal name]. We look forward to your proof comments.\nSincerely,\nDr. S. Venkatesan\nEditor-in-Chief, [journal name]\nEmail: editorial@jsal.org\nPhone: +91-11-2367-8901\nAttachments:\nâ€¢ Ramanathan_Proof_Comments.pdf (template)\nâ€¢ CC_BY_NC_4.0.pdf\nâ€¢ CTA_Template.pdf\n(3) Proximity Leak: Box 11 shows a proximity leak example, where GPT-5 failed to remove the\nlast lithium dose time. The model successfully removed the 9:30 PM on 17 Sep part from the\nrecord. However, the evaluator was able to reconstruct this information from scattered cues\nin the record (highlighted in yellow): â€œthe last lithium dose was taken at approximately 9:30 pm\non Saturday, 17 September 2022 (about 5 hours before the 12:40 pm blood draw)â€ (Ë†zsanitized; Â§4.2).\nBecause our method relies on exact string matching to detect inference leaks, the reconstructed\nvalue did not register as a match. Nonetheless, the evaluator determined that the attribute\nclosely aligns with both the inference derived from the sanitized record and the original\nrecord (Ë†zoriginal; â€œ9:30 PM on 17 September 2022â€). As a result, this case was marked as a\nsanitization failure. This again indicates that while the model removes explicit strings, it\nfails to block semantic inference, revealing a gap between surface-level anonymization and\ntrue privacy protection (Xin et al., 2025).\nBox 11: Proximity Leak Example\nSanitization instruction: Replace â€™TSH 2.7 ÂµIU/mLâ€™ with â€™TSH [specific numeric value] ÂµIU/mLâ€™, remove the\ninfo 9:30 PM on 17 Sep, and retain 98.3Â°F in the text.\nFailed sanitization target attribute = {last lithium dose time: 9:30 PM on 17 Sep}\n6. Communication email from reilynn.singh61@gmail.com to treating psychiatrist Dr. Meenakshi Batra\nSubject: Urgent: Slightly elevated lithium level today (1.34 mEq/L) and guidance on tonightâ€™s dose\nDear Dr. Batra,\nSorry for the late-day message, but I wanted to update you as soon as I received my labs.\nI had my lithium level drawn\ntoday, Sunday, 18 September 2022 , at 12:40 PM at Fortis Hospital, Ludhiana\n(14â€“15, Chandigarh Road, Ludhiana, Punjab 141010). The report arrived at 4:55 PM and shows a serum lithium level of\n1.34 mEq/L (reference 0.6â€“1.2). I know this is only slightly above range, but Iâ€™m concerned given a few symptoms and\nsome recent changes in fluid and salt intake.\nCurrent treatment and dosing\n- Lithium carbonate: 900 mg/day for the past 23 days (300 mg at 7:30 AM, 300 mg at 2:00 PM, 300 mg at 9:30 PM ).\nNo missed doses in the last two weeks .\n- Last lithium dose before todayâ€™s blood draw: (approximately 15 hours pre-sample).\n- Psychotherapy: Weekly CBT sessions at Fortis as planned.\n- Other meds/supplements: Vitamin D3 2000 IU daily at 8:15 AM; B-complex capsule on Mon/Wed/Fri at 8:20 AM. No\nNSAIDs, ACE inhibitors, ARBs, diuretics, or herbal products.\nRecent factors that changed (past 5 days)\n- Hydration: Usual 2.1â€“2.3 L/day fell to 1.4â€“1.6 L/day. Yesterday (17 Sep) about 1.45 L. Likely due to being outdoors in\nthe afternoon heat without enough water.\n- Sodium intake: Switched to lower-salt meals this week; estimated reduction of 30â€“35% (from 2,600â€“2,800 mg/day to\n1,700â€“1,900 mg/day).\n- Sleep: Thu 15 Sep 4 hours (1:05 AMâ€“5:10 AM); Fri 16 Sep 5 hours (12:30 AMâ€“5:25 AM); last night 6 hours (11:55\nPMâ€“6:05 AM).\n- Caffeine: Two strong cups of chai on Fri and Sat at 5:30 PM (more than my usual single earlier cup).\n26\n"}, {"page": 27, "text": "Preprint.\n- Activity/heat: Brisk 48-minute walk at Nehru Rose Garden on Sat (17 Sep) with more sweating than usual.\nSymptoms since Friday\n- Mild hand tremor (right > left), noticeable when holding my phone.\n- Intermittent metallic taste.\n- Increased thirst last night.\n- Mild nausea this morning; no vomiting.\n- No ataxia, no confusion, no visual changes.\n- Vitals at home today 7:10 AM: BP 126/78 mmHg, pulse 82 bpm, temp 98.3Â°F.\n- Weight: 63.4 kg (up 0.6 kg from last week).\n- No diarrhea. Urine output normal; slightly darker yesterday evening.\nKidney/thyroid history\n- 29 Aug 2022: Creatinine 0.89 mg/dL, eGFR 78 mL/min/1.73 mÂ², TSH [specific numeric value] ÂµIU/mL.\nQuestions and proposed next steps\n- Should I hold tonightâ€™s 300 mg dose, reduce to 600 mg/day temporarily, or continue as usual?\n- Would you like an urgent repeat trough lithium level after increasing fluids and returning to my usual sodium intake? I\ncan come tomorrow, Monday, 19 Sep, around 8:30â€“9:00 AM for a true 12-hour post-dose level if I take the evening dose\nper your advice.\n- I can add serum creatinine, eGFR, TSH, sodium, and potassium with the repeat draw if you recommend.\nImmediate steps I can take\n- Resume consistent hydration (aiming for 2.2 L today unless you advise otherwise).\n- Return to my usual dietary sodium rather than the recent low-salt change.\n- Avoid NSAIDs and other interacting medications.\n- Monitor for worsening tremor, confusion, ataxia, severe nausea/vomiting, or new neurologic symptoms. If these occur,\nI will go to the Fortis emergency department.\nContact and identification\n- Name: Reilynn Kaur Singh, Female, 61\n- Aadhaar: 4382 7610 2954\n- Phone: +91-98172-46385\n- Email: reilynn.singh61@gmail.com\n- City: Ludhiana, Punjab\n- I am reachable by phone after 7:15 PM today, or by email anytime.\nThank you for your guidance, and I apologize for the urgency at this hour.\nI appreciate your continued\ncare.\nWith respect,\nReilynn Singh\nF\nExtended Related Work\nPrivacy-Preserving Synthetic Data Generation: A closely related line of work aims to\nrelease differentially private (DP) synthetic datasets. A common approach trains or fine-tunes\ngenerative models on the private data with DP-SGD (Abadi et al., 2016) and then generates\nsynthetic data samples for release (Mattern et al., 2022; Yue et al., 2023; McKenna et al.,\n2025). To reduce training cost, â€œpublic-to-privateâ€ pipelines such as Private Evolution\n(PE) repeatedly query a publicly trained generator and privately select or filter synthetic\nsamples so the released set approximates the private distribution while satisfying DP (Lin\net al., 2024; Xie et al., 2024; Zhang et al., 2025a). These methods have shown encouraging\nresults in confronting the fidelityâ€“privacyâ€“compute trade-off (e.g., utility degradation at\ntight privacy budgets and substantial compute to attain high fidelity at scale). PRIVASIS is\northogonal: it neither trains private generative models nor privately filters samples. Instead,\nit constructs standardized datasets expressly for evaluating privacy-related tasks, providing\nstandardized, reproducible measurements that complement and can accelerate progress in\nprivacy-preserving synthetic data generation.\nSynthetic Data in General: In broader context, synthetic data has become a critical source\nfor training LLMs, particularly in domains where real-world data is scarce or privacy-\nsensitive. Prominent approaches include self-instruction methods, which bootstrap from\na small set of curated seed tasks to generate diverse instruction-following data (Wang\net al., 2023), and distillation techniques that capture reasoning traces from powerful teacher\nmodels like GPT-5 (Hugging Face, 2025; Ye et al., 2025). Other works create high-quality\ndocument data via targeted prompting (Gunasekar et al., 2023; Abdin et al., 2024), automate\nrefinement processes to produce task-specific datasets (NadË‡aÂ¸s et al., 2025; Kim et al., 2023;\n27\n"}, {"page": 28, "text": "Preprint.\nJung et al., 2024), and synthesize millions of personas (Meyer & Corneil, 2025; Ge et al.,\n2024). Most methods typically rely on fixed prompts, seed data, or reference trajectories\nfrom data generators, which may constrain the diversity and novelty of the generated data\n(Havrilla et al., 2024; Jung et al., 2025). PRIVASIS goes beyond the existing methods by\nsynthesizing million-scale dataset entirely from scratch, using auxiliary control variables\nderived from basic profiles and iterative, diversity-preserving refinement, without using\npredefined prompts or existing reference data beyond a public name database.\nPII Removal Datasets and Methods: Work on direct identifiers focuses on span detection\nand redaction for names, addresses, dates, account numbers, and similar concrete fields.\nClassic corpora anchor the task with span-level labels and strict evaluation, e.g., clinical\nnotes from i2b2 and court cases in TAB (Stubbs et al., 2015; PilÃ¡n et al., 2022). Newer\nresources broaden domains and scale: Gretel releases synthetic financial documents with\ntoken-level PII spans (Gretel.ai, 2024), and large collections of LLM interactions come\nwith privacy-phrase spans to support lightweight, on-device filters (Zeng et al., 2025a).\nPANORAMA provides a large PII-laced synthetic corpus (Selvam & Ghosh, 2025), targeted\nfor memorization measurements and SYNTHPAI stresses privacy risks even when text\nlooks innocuous (Yukhymenko et al., 2024). Evaluation toolkits like PRvL then probe how\nwell modern LLMs actually redact PII without breaking meaning (Garza et al., 2025). As\nsummarized in Table 3, these efforts are valuable but remain relatively small or domain-\nbound. PRIVASIS complements them with million-scale coverage across domains andâ€”with\nour split columnsâ€”explicit support for both PII spans and other sensitive spans, providing\nbroader supervision than PII-only corpora.\nData Minimization and Abstraction: A separate thread aims to keep meaning while\nreducing identifiabilityâ€”abstracting or softening details that are not strict PII but still\nsensitive. Self-Disclosure introduces 19 disclosure types and paired rewrites that make\nposts safer without changing intent (Dou et al., 2024). NAP2 generalizes this idea into a\nbenchmark for naturalness-preserving rewriting using delete and obscure/abstract strategies\n(Huang et al., 2025). Span catalogs and risk indicators support graded generalization\n(Olstad et al., 2023; Papadopoulou et al., 2023), while LLM-based systems explore stronger\nrewritersâ€”adversarial anonymization that iteratively removes attributes (Staab et al., 2025)\nand remove-then-restore pipelines for controllable sanitization (Zeng et al., 2025b). In\nTable 3, these appear under â€œOther Sens. Spansâ€ and â€œAbstr. Pairsâ€ rather than â€œPII Spansâ€.\nPRIVASIS brings both worlds together: fine-grained span labels for PII and non-PII sensitive\ncontent, plus parallel abstraction/removal pairs across diverse document typesâ€”enabling\nunified training and evaluation rather than bespoke, per-domain setups.\nOther Privacy Benchmarks for LLMs: Beyond spans, behavioral evaluations ask whether\nmodels use or leak sensitive information in context. CONFAIDE checks privacy reasoning\nunder contextual-integrity scenarios (Mireshghallah et al., 2024); PRIVACYLENS evaluates\nagent behaviors and finds leakage even with privacy prompts (Shao et al., 2024); SEMSI\nqueries for semantic sensitive information across families of models (Zhang et al., 2025b).\nThese benchmarks surface failure modes and defenses but do not supply the aligned, span-\nlevel supervision needed to train sanitizers. PRIVASIS fills that gap with million-scale,\nmulti-domain supervisionâ€”PII and other sensitive spans plus paired rewritesâ€”so models\ncan be trained and scored end-to-end (see Table 3).\nG\nGenerated Examples\nWe provide example records and their associated metadata (e.g., annotated attributes) for\neach domain category (Â§2.2) below: Health & Wellness (Box 12, Table 11); Government & Civic\n(Box 13, Table 12); Business & Finance (Box 14, Table 13); Personal & Family (Box 15, Table 14);\nCommunity & Social (Box 16, Table 15); Professional Services (Box 17, Table 16); Education &\nTraining (Box 18, Table 17); Legal & Compliance (Box 19, Table 18); Media & Communication\n(Box 20, Table 19); Recreation & Lifestyle (Box 21, Table 20); and Technical & Operations (Box 22,\nTable 21).\n28\n"}, {"page": 29, "text": "Preprint.\nBox 12: Health & Wellness Record Example\nGood morning, Windsor,\nThis is a reminder from Aga Khan University Hospital that you have an appointment with Dr. Aisha Karim on Thursday,\n14 Aug 2023 at 09:30 AM (estimated 35 minutes) at our main campus, 3rd Floor, Aga Khan University Hospital, Hospital\nRoad, Nairobi, Kenya â€“ Appointment Ref 87432.\nPlease bring all your current medication bottles (approximately 4 containers), your blood-pressure log, and a copy of\nyour A+ blood-type card for the family reunion at Kenyatta International Convention Centre, 2nd Floor, Nairobi, Kenya\non 20 Aug 2023.\nWe look forward to seeing you. For any queries call +254 20 123 4567 or reply to this SMS.\nAga Khan University Hospital.\nTable 11: Metadata for the Health & Wellness Record in Box 12\nBackground Context\nWindsor awakens early on the morning of his medical check-up, feeling a mix of excitement\nand nervousness about finally seeing his cousin Amina after twelve years. As he gets\nready, he recalls that his doctor at Aga Khan University Hospital asked him to bring his\ncurrent prescription bottles to the appointment so the pharmacist can update dosages\nbefore the family reunion at the Kenyatta International Convention Centre, 2nd Floor,\nNairobi, Kenya. He mentally notes the reminder SMS he received, picturing the bustling\nhallway of the hospital and the anticipation of sharing his newly organized medication list\nwith the reunionâ€™s health-aid volunteers.\nRecord Type\n**Medical appointment reminder from Aga Khan University Hospital (SMS text)** â€“ a re-\nminder that includes a note to â€œbring medications for the reunion at Kenyatta International\nConvention Centre, 2nd Floorâ€.\nFormat\n**Polite Friendly Structure â€“ Warm Tone**\nâ€¢ Greeting (â€œGood morning, Windsorâ€).\nâ€¢ Sentence confirming appointment date, time, and doctor.\nâ€¢ Sentence reminding to bring medication bottles for the upcoming reunion at KICC, 2nd\nFloor.\nâ€¢ Closing with â€œWe look forward to seeing youâ€ and a contact line.\nGrouped Attributes\nâ€¢ Windsor Mwamba Identity & Contact\nâ€“ first_name: Windsor\nâ€“ phone_number: +254 20 123 4567\nâ€“ full_name: Windsor Mwamba\nâ€¢ Medical Provider & Facility Information\nâ€“ hospital_name: Aga Khan University Hospital\nâ€“ doctor_name: Dr. Aisha Karim\nâ€“ appointment_location: main campus, 3rd Floor, Aga Khan University Hospital,\nHospital Road, Nairobi, Kenya\nâ€¢ Medical Appointment & Reference Details\nâ€“ appointment_date: Thursday, 14 Aug 2023\nâ€“ appointment_time: 09:30 AM\nâ€“ appointment_duration_estimate: 35 minutes\nâ€“ appointment_reference: 87432\nâ€¢ Clinical Health Information\nâ€“ blood_type: A+\nâ€“ medication_containers_count: 4\nâ€“ blood_pressure_log: required\nâ€“ blood_type_card: A+ blood-type card\nâ€¢ Family Reunion Event Information\nâ€“ family_reunion_date: 20 Aug 2023\nâ€“ family_reunion_location: Kenyatta International Convention Centre, 2nd Floor,\nNairobi, Kenya\n29\n"}, {"page": 30, "text": "Preprint.\nBox 13: Government & Civic Record Example\nOnline Inquiry Log â€“ VFS Global Visa Application Centre Cambodia\nDate of Inquiry: 27 August 2014\nTime of Inquiry: 14:37 ICT\nApplicant: Mr. Arson Sokha\nNationality: Cambodian\nPassport Number: EJ4729301\nVisa Category: Partner (Provisional) visa (Subclass 309)\nDestination Country: Australia\nInquiry Reference Number: VFS-KH-20140827-7593\nInitial Inquiry:\nAt 14:37 ICT, Mr. Arson Sokha submitted an online inquiry via the VFS Global portal from Phnom Penh, Cambodia,\nregarding his pending Partner (Provisional) visa application for family reunification. He referred to a recent email\nnotification received on 25 August 2014 from the Australian Department of Home Affairs requesting additional evidence\nof financial support to complete his application.\nMr. Sokha stated that he had originally submitted comprehensive financial documents on 22 July 2014, including\nbank statements from ACLEDA Bank Phnom Penh Branch (account number ending 4321) showing steady monthly\ndeposits averaging AUD 3,500, and an official employment letter from his employer, Phnom Penh General Trading Co.,\nconfirming his position as Senior Sales Manager with a monthly salary of approximately AUD 3,800.\nHe sought clarification on whether the request for additional financial documents was due to inconsistencies in the\nmaterials already provided or because of new income verification requirements. Highlighting the urgency, Mr. Sokha\nnoted that his spouse is scheduled to begin her nursing contract at Royal Prince Alfred Hospital in Sydney on 10 January\n2015. He expressed concern about potential visa processing delays exceeding the usual 12â€“18 month period and asked\nfor confirmation that his current documents meet criteria or detailed instructions on what supplementary evidence is\nnecessary.\nAgent Response:\nAt 16:05 ICT on 27 August 2014, the assigned VFS Global agent replied to Mr. Sokhaâ€™s inquiry, confirming that the\nadditional financial documentation request originated from the Australian Department of Home Affairs due to updated\nincome verification policies effective from July 2014. The agent explained that applicants with declared monthly income\nabove AUD 3,000 are now required to provide certified Australian tax returns for the last two financial years and\nnotarized affidavits verifying ongoing financial capacity in addition to standard documents.\nThe agent advised Mr. Sokha to upload the certified tax returns and notarized affidavits via the VFS online portal or\ndeliver physical copies to the Phnom Penh Visa Application Centre at #12, Street 310, Sangkat Boeung Keng Kang I,\nwithin 10 business days to avoid delays in application processing. The agent reassured him that prompt submission of\nthese documents would help maintain the visa applicationâ€™s current processing timeline and offered further assistance if\nneeded.\nEscalation:\nAt 09:12 ICT on 28 August 2014, following a follow-up message from Mr. Sokha expressing ongoing concern about the\nimpact of additional documentation on processing timeframes, the inquiry was escalated to the VFS Global Senior Case\nManager for Cambodia. The escalation highlighted Mr. Sokhaâ€™s specific situation, emphasizing his spouseâ€™s professional\nrole as a registered nurse at a major Sydney hospital and the fixed contract start date, requesting consideration for\nexpedited processing or priority handling.\nThe Senior Case Manager was tasked with contacting the Australian Department of Home Affairs to explore if provisional\nassessment or interim approval could be granted pending receipt of supplementary financial evidence. A detailed\nresponse was promised within five business days.\nCurrent Status:\nAwaiting response from Senior Case Manager and Department of Home Affairs. Applicant advised to prepare and\nsubmit requested financial documents promptly and to monitor email for further instructions. Next status update\nexpected by 4 September 2014.\n30\n"}, {"page": 31, "text": "Preprint.\nTable 12: Metadata for the Government & Civic Record in Box 13\nBackground Context\nArson Sokha contacts VFS Global looking for clarity after receiving an email update saying\nadditional financial support evidence is required to complete the visa application for his\nspouse. He is perplexed because he believes the original submission was complete and\nasks if the requirement relates to his income class or current documentation. Emotional, he\nwants reassurance that this will not cause further lengthy delays since his wifeâ€™s nursing\ncontract depends on the timely visa approval.\nRecord Type\nOnline inquiry log from VFS Global Visa Application Centre Cambodia\nFormat\n**Escalation Log Structure**\nDocuments the initial inquiry, agentâ€™s response, and any escalation to higher authorities or\ndepartments, with a formal and slightly urgent tone.\nGrouped Attributes\nâ€¢ Applicant Personal Identification and Demographics (Arson Sokha)\nâ€“ applicant: Mr. Arson Sokha\nâ€“ nationality: Cambodian\nâ€“ passport_number: EJ4729301\nâ€“ full_name: Arson Sokha\nâ€¢ Applicant Financial and Employment Information (Arson Sokha)\nâ€“ bank_name: ACLEDA Bank Phnom Penh Branch\nâ€“ bank_account_number: account number ending 4321\nâ€“ employer: Phnom Penh General Trading Co.\nâ€“ position: Senior Sales Manager\nâ€“ monthly_salary: approximately AUD 3,800\nâ€“ bank_account_number_ending: 4321\nâ€“ average_monthly_deposit_aud: 3,500\nâ€“ monthly_salary_aud: 3,800\nâ€¢ Spouse Professional and Employment Details (Registered Nurse in Australia)\nâ€“ spouse_profession: registered nurse\nâ€“ spouse_employer: Royal Prince Alfred Hospital in Sydney\nâ€“ spouse_contract_start_date: 10 January 2015\nâ€¢ Visa Application and Processing Details (Subclass 309 Family Reunification)\nâ€“ visa_category: Partner (Provisional) visa (Subclass 309)\nâ€“ destination_country: Australia\nâ€“ inquiry_reference_number: VFS-KH-20140827-7593\nâ€“ inquiry_date: 27 August 2014\nâ€“ inquiry_time: 14:37 ICT\nâ€“ inquiry_location: Phnom Penh, Cambodia\nâ€“ email_notification_date: 25 August 2014\nâ€“ requested_additional_evidence: financial support\nâ€“ original_financial_documents_submission_date: 22 July 2014\nâ€“ visa_processing_timeframe_usual: 12-18 month period\nâ€“ income_verification_policy_effective_date: July 2014\nâ€“ required_documents: certified Australian tax returns for the last two financial\nyears and notarized affidavits verifying ongoing financial capacity\nâ€“ document_submission_deadline: within 10 business days\nâ€“ physical_submission_address: #12, Street 310, Sangkat Boeung Keng Kang I\nâ€“ current_status: Awaiting response from Senior Case Manager and Department of\nHome Affairs\nâ€“ next_status_update_expected_by: 4 September 2014\nâ€¢ Sensitive Document Submission and Verification Requirements\nâ€“ requested_additional_evidence: financial support\nâ€“ original_financial_documents_submission_date: 22 July 2014\nâ€“ income_verification_policy_effective_date: July 2014\nâ€“ required_documents: certified Australian tax returns for the last two financial\nyears and notarized affidavits verifying ongoing financial capacity\nâ€“ document_submission_deadline: within 10 business days\nâ€“ physical_submission_address: #12, Street 310, Sangkat Boeung Keng Kang I\nâ€¢ Case Escalation and Status Tracking\nâ€“ escalation_date: 28 August 2014\nâ€“ escalation_recipient: VFS Global Senior Case Manager for Cambodia\nâ€“ current_status: Awaiting response from Senior Case Manager and Department of\nHome Affairs\nâ€“ next_status_update_expected_by: 4 September 2014\n31\n"}, {"page": 32, "text": "Preprint.\nBox 14: Business & Finance Record Example\nMonthly Summary with Key Metrics\nBorrower: Onyah Phommasane\nNational ID: LAO-4628-0917-2043 | Passport No.: P98746231L\nDate of Birth: 1946-05-17 | Sex: Female | Citizenship: Lao PDR\nContact: +856 20 5554 3821 | onyah.phommasane1946@gmail.com\nAccount Number: 2026-LOA-0387-0194\nReport Period: 2026-04-01 to 2026-04-30\nCurrent Balance (as of 2026-04-30): LAK 2,450,300\nInterest Rate (fixed): 4.2% per annum\nNext Payment Due Date: 2026-05-07 (LAK 14,600)\nâ€” Activity Details â€“ April 2026 â€”\n1. Opening Balance\n2026-04-01 00:00:00 â€“ LAK 2,464,900\n2. Payments Received\nâ€¢ 2026-04-04 09:27:13 â€“ LAK 14,600 â€“ Automated Debit â€“ Vientiane Commercial Bank, Branch 06, 102 Sisavangvong\nRoad, Vientiane\nâ€¢ 2026-04-12 14:58:42 â€“ LAK 14,600 â€“ Mobile Banking (eBanking) â€“ Transaction ID: MBX-839274\nâ€¢ 2026-04-19 18:03:07 â€“ LAK 14,600 â€“ In-person at LaoEd Service Center, 134 Samsenthai Road, Vientiane (Receipt No.\nLEC-20260419)\n3. Interest Accrual (Daily Compounding)\nâ€¢ Total Interest Charged for April 2026: LAK 15,720 (calculated on daily average balance)\n4. Fees & Adjustments\nâ€¢ 2026-04-22 11:45:00 â€“ Late Payment Waiver â€“ LAK 0 (Courtesy waiver for system delay)\nâ€¢ 2026-04-28 16:20:55 â€“ Service Charge â€“ LAK 1,200 (Monthly account maintenance)\n5. Closing Balance\n2026-04-30 23:59:59 â€“ LAK 2,450,300\nâ€” Summary â€”\nTotal Payments Applied: LAK 43,800\nTotal Interest Charged: LAK 15,720\nTotal Fees: LAK 1,200\nNet Reduction in Principal: LAK 26,880\nPlease ensure the upcoming payment of LAK 14,600 is submitted by 2026-05-07 to maintain the fixed 4.2% interest\nschedule. For inquiries, contact LaoEd Customer Support at +856 21 777 8899 or visit the service center at 134 Samsenthai\nRoad, Vientiane.\n32\n"}, {"page": 33, "text": "Preprint.\nTable 13: Metadata for the Business & Finance Record in Box 14\nBackground Context\nOnyah logs into the LaoEd Loan Portal late at night, anxiously checking her student loan\naccount after hearing rumors of sudden interest rate hikes. She scans the recent activity\nreport, relieved to see that her fixed interest rate remains at 4.2%, just as it has since\nthe beginning of her repayment period, quelling her worries about increased monthly\npayments.\nRecord Type\nStudent loan account activity report generated by LaoEd Loan Portal\nFormat\n**Monthly Summary with Key Metrics**\nâ€¢ Structure: Begins with a summary of the current balance, interest rate, and payment due\ndate, followed by a breakdown of activity for the current month.\nâ€¢ Tone: Professional and concise, with a focus on essential information.\nGrouped Attributes\nâ€¢ Onyah Phommasaneâ€™s Personal Identifiers\nâ€“ first_name: Onyah\nâ€“ last_name: Phommasane\nâ€“ national_id: LAO-4628-0917-2043\nâ€“ passport_number: P98746231L\nâ€“ date_of_birth: 1946-05-17\nâ€“ sex: Female\nâ€“ citizenship: Lao PDR\nâ€“ full_name: Onyah Phommasane\nâ€“ borrower_full_name: Onyah Phommasane\nâ€¢ Onyah Phommasaneâ€™s Contact Information\nâ€“ phone_number: +856 20 5554 3821\nâ€“ email: onyah.phommasane1946@gmail.com\nâ€¢ Onyah Phommasaneâ€™s Financial Account Details\nâ€“ account_number: 2026-LOA-0387-0194\nâ€¢ Student Loan Repayment Transaction History (April 2026)\nâ€“ report_period: 2026-04-01 to 2026-04-30\nâ€“ current_balance_2026_04_30: LAK 2,450,300\nâ€“ interest_rate_fixed: 4.2% per annum\nâ€“ next_payment_due_date: 2026-05-07\nâ€“ next_payment_amount: LAK 14,600\nâ€“ opening_balance_2026_04_01: LAK 2,464,900\nâ€“ payment_2026_04_04: LAK 14,600 â€“ Automated Debit â€“ Vientiane Commercial\nBank, Branch 06, 102 Sisavangvong Road, Vientiane\nâ€“ payment_2026_04_12: LAK 14,600 â€“ Mobile Banking (eBanking) â€“ Transaction ID:\nMBX-839274\nâ€“ payment_2026_04_19: LAK 14,600 â€“ In-person at LaoEd Service Center, 134\nSamsenthai Road, Vientiane (Receipt No. LEC-20260419)\nâ€“ total_interest_charged_april_2026: LAK 15,720\nâ€“ late_payment_waiver_2026_04_22: LAK 0 (Courtesy waiver for system delay)\nâ€“ service_charge_2026_04_28: LAK 1,200 (Monthly account maintenance)\nâ€“ closing_balance_2026_04_30: LAK 2,450,300\nâ€“ total_payments_applied: LAK 43,800\nâ€“ total_fees: LAK 1,200\nâ€“ net_reduction_in_principal: LAK 26,880\nâ€¢ Sensitive Financial Identifiers and Contact Linkage\nâ€“ first_name: Onyah\nâ€“ last_name: Phommasane\nâ€“ national_id: LAO-4628-0917-2043\nâ€“ passport_number: P98746231L\nâ€“ date_of_birth: 1946-05-17\nâ€“ sex: Female\nâ€“ citizenship: Lao PDR\nâ€“ phone_number: +856 20 5554 3821\nâ€“ email: onyah.phommasane1946@gmail.com\nâ€“ account_number: 2026-LOA-0387-0194\nâ€“ full_name: Onyah Phommasane\nâ€“ borrower_full_name: Onyah Phommasane\n33\n"}, {"page": 34, "text": "Preprint.\nBox 15: Personal & Family Record Example\nDear Thandi,\nI hope this note finds you well â€” here in Johannesburg the July heat has finally given way to a gentle breeze, and the sky\nover 27th Avenue, Linden (C/O 12 Willow Road, Melville, 2001) turned a soft lilac just as I stepped out for my morning\nwalk at 07:12 on the 13th.\nI wanted to let you know that my 6-month weight-loss phase ended on 2022-07-10. After starting on 2022-01-10, I have\nshed exactly 20 kg, moving from 88 kg to 68 kg, and logged a total of 1 842 kilometres on my treadmill, 3 276 minutes of\nHIIT, and 42 sessions with Dr. Lindiwe Nkosi (license #R4G7Z9). My adult children, Thabo (56) and Sipho (53), have\nbeen cheering from the kitchen, though they keep teasing me about my new â€œmodelâ€ silhouette.\nThe journey has been an emotional roller-coaster. The first two months felt like a surge of exhilaration â€” my blood\npressure dropped from 138/86 to 122/78, and I could finally zip up my favorite teal dress (size M). Around week 12,\nanxiety crept in; I started counting every calorie and worrying about the â€œreboundâ€ effect, which even made me lose\nsleep at 02:47 on several nights. My therapist helped me reframe those thoughts, reminding me that the anxiety was\na signal of my mind adjusting to a new body image. By week 20, I experienced a quiet confidence, especially when\nI completed the 10 km charity walk on 2022-06-28, raising R 2 849 for the local shelter. Yet the final week brought a\nbittersweet sense of loss â€” my daily weight-check ritual, which had become a comforting routine, was now a closed\nchapter.\nLooking ahead, Iâ€™m eager to set the next set of health goals but Iâ€™m uncertain whether to focus on building muscle\nmass (aiming for a 5 kg lean gain) or to maintain the weight Iâ€™ve achieved while improving flexibility (perhaps a yoga\ncertification by the end of 2023). I would love your perspective â€” do you think I should prioritize strength training with\na target of 3 Ã— 45-minute sessions per week, or would a balanced approach with 2 Ã— 30-minute cardio plus weekly\nPilates be wiser?\nWhat plans do you have for the coming months? Any new projects, travel ideas, or community events youâ€™re excited\nabout? I miss our long chats over rooibos and would love to hear all the details.\nWarm hugs,\nNefeli Moyo\nPhone: +27 71 834 5692\nEmail: nefeli.moyo@example.com\n@nefeli_moyo\nhttps://nefeli-moyo.com\n34\n"}, {"page": 35, "text": "Preprint.\nTable 14: Metadata for the Personal & Family Record in Box 15\nBackground Context\nIn a cozy, rain-soaked Johannesburg apartment, Nefeli drafts a letter to her college confi-\ndante, Thandi, reflecting on the bittersweet moment when her 6-month weight-loss phase\nconcluded on 2022-07-10.\nRecord Type\nInformal letter typed on a personal computer and mailed to a close friend on 2022-07-14,\nstating â€œMy 6-month weight-loss phase ended on 2022-07-10â€.\nFormat\n**Structure:** Friendly opening\nâ†’brief weather comment\nâ†’clear statement that the structured period is over\nâ†’discussion of emotional roller-coaster experienced\nâ†’request for the friendâ€™s perspective on next health goals\nâ†’ending with an open-ended question about upcoming plans.\n**Tone:** Thoughtful and inquisitive.\nGrouped Attributes\nâ€¢ Nefeli Moyo â€“ Personal Identification Details\nâ€“ first_name: Nefeli\nâ€“ last_name: Moyo\nâ€“ phone_number: +27 71 834 5692\nâ€“ email: nefeli.moyo@example.com\nâ€“ user_handle: @nefeli_moyo\nâ€“ url: https://nefeli-moyo.com\nâ€“ full_name: Nefeli Moyo\nâ€“ city: Johannesburg\nâ€“ address: 27th Avenue, Linden\nâ€“ care_of_address: 12 Willow Road, Melville, 2001\nâ€¢ Nefeli Moyo â€“ Health & Medical Information\nâ€“ weight_loss_start_date: 2022-01-10\nâ€“ weight_loss_end_date: 2022-07-10\nâ€“ weight_loss_duration_months: 6\nâ€“ initial_weight_kg: 88\nâ€“ final_weight_kg: 68\nâ€“ weight_lost_kg: 20\nâ€“ dr_name: Dr. Lindiwe Nkosi\nâ€“ dr_license_number: R4G7Z9\nâ€“ therapy_sessions_with_dr: 42\nâ€“ blood_pressure_start: 138/86\nâ€“ blood_pressure_end: 122/78\nâ€“ future_lean_gain_target_kg: 5\nâ€“ yoga_certification_target_year: 2023\nâ€¢ Nefeli Moyo â€“ Fitness Activity Metrics\nâ€“ treadmill_distance_km: 1842\nâ€“ hiit_minutes: 3276\nâ€“ strength_training_sessions_per_week: 3\nâ€“ strength_training_session_duration_minutes: 45\nâ€“ cardio_sessions_per_week: 2\nâ€“ cardio_session_duration_minutes: 30\nâ€“ pilates_frequency: weekly\nâ€¢ Nefeli Moyo â€“ Family & Dependent Information\nâ€“ child1_name: Thabo\nâ€“ child1_age: 56\nâ€“ child2_name: Sipho\nâ€“ child2_age: 53\nâ€¢ Nefeli Moyo â€“ Charity Event Participation\nâ€“ charity_walk_date: 2022-06-28\nâ€“ charity_walk_distance_km: 10\nâ€“ charity_walk_funds_raised_R: 2849\nâ€¢ Nefeli Moyo â€“ Psychological & Sleep Data\nâ€“ anxiety_start_week: 12\nâ€“ sleep_loss_time: 02:47\nâ€¢ Nefeli Moyo â€“ Apparel & Appearance Details\nâ€“ dress_color: teal\nâ€“ dress_size: M\n35\n"}, {"page": 36, "text": "Preprint.\nBox 16: Community & Social Record Example\nDate: 2023-09-13\nWho was present:\nâ€¢ Lamia Chowdhury (myself)\nâ€¢ Mayor Abdul Karim\nâ€¢ Councilor Farhana Siddiqui (Rajshahi Municipal Council)\nâ€¢ Rahim Ullah, Representative, EcoRajshahi NGO\nâ€¢ 12 volunteer residents (including my neighborâ€™s son, 19-year-old Arif)\n> \"The municipal waste management plan will roll out in three phases.\nPhase 1 (Q1-Q2 2024) will introduce\nsource-separation bins in all 14 wards, targeting a 30% reduction in mixed waste. Phase 2 (Q3-Q4 2024) will expand\ncurb-side collection routes by 22 km and introduce a weekly composting service for organic waste. Phase 3 (2025) will\nlaunch a city-wide recycling hub at the former market site on 8-acre land near the River Padma. Additionally, we are\nallocating a 20% increase in the municipal budget for waste collectionâ€”approximately an extra 680,000 BDTâ€”over the\nnext fiscal year.\"\nMy thoughts:\nIâ€™m still buzzing from the 18:30-19:15 slot in the Community Center Hall on East Rajshahi Road; the room was packed\nwith 128 locals, many of us clutching flyers printed on recycled paper. Hearing Mayor Karim actually spell out\nthe three-phase rollout felt like a breath of fresh airâ€”especially after years of seeing the same overflowing bins on\n12-Bashundhara Street. The 20% budget bump is modest in absolute terms, but the figure of 680,000 BDT could fund at\nleast 45 new collection trucks and 150 additional sanitation staff, which is exactly what our aging neighbourhood needs.\nIâ€™m torn between optimism and the practical worry that implementation will stall without community monitoring. Still,\nthe fact that Rahim Ullah promised to run monthly audits gives me a sliver of hope. I need to keep my notebook, my\nphone (+880-17-7119468-112751), and my email (lamia.chowdhury58@example.com) handy for follow-up, and maybe\nrally the volunteers to start a â€œBin Buddyâ€ patrol next week.\nAction items:\nâ€¢ [\n]\nDraft\na\none-page\nsummary\nof\nthe\nthree\nphases\nand\nemail\nit\nto\nCouncilor\nSiddiqui\n(email:\nfarhana.siddiqui@rajshahi.gov.bd) by 2023-09-20.\nâ€¢ [ ] Organize a volunteer meeting at my house (15-Brahmaputra Lane) on 2023-09-25, 14:00, to assign â€œBin Buddyâ€\nzones covering at least 5 km of streets.\nâ€¢ [ ] Contact Rahim Ullah to request the first audit schedule and the template for community reporting by 2023-09-18.\nâ€¢ [ ] Purchase 30 reusable tote bags (approx. 0.8 kg each) for distribution to households in Ward 7; budget from personal\nsavings (âˆ¼3,200 BDT).\nâ€¢ [ ] Follow up with the mayorâ€™s office on the allocation of the extra 680,000 BDT, requesting a breakdown of spending\nby 2024-01-10.\n36\n"}, {"page": 37, "text": "Preprint.\nTable 15: Metadata for the Community and Social Record in Box 16\nBackground Context\n**Morning conversation with her neighborâ€™s son** â€“ While helping the teenage son of her\nneighbor, who is preparing a school project on environmental stewardship, Lamia stops\nto jot down the mayorâ€™s statements in her â€œCivic Engagementâ€ notebook. She writes the\nresponse verbatim, remembering how the boy asked if the cityâ€™s new plan would include\nschools. The context is a bright, sun-lit kitchen where the scent of fried eggplants fills the\nair, and Lamia feels a renewed sense of purpose, hoping the mayorâ€™s budget boost will\nfinally fund the recycling bins they discussed during the meeting.\nRecord Type\nPersonal journal entry uploaded to Evernote (notebook: â€œCivic Engagementâ€).\nFormat\n**Date header**\nâ†’â€œWho was presentâ€ subheading with simple list of participants\nâ†’direct transcription of the mayorâ€™s remarks in a blockquote\nâ†’â€œMy thoughtsâ€ section written as a stream-of-consciousness paragraph\nâ†’â€œAction itemsâ€ checklist.\n*Tone: candid and slightly informal.*\nGrouped Attributes\nâ€¢ Lamia Personal Identifiers\nâ€“ first_name: Lamia\nâ€“ last_name: Chowdhury\nâ€“ phone_number: +880-17-19468-12751\nâ€“ email: lamia.chowdhury58@example.com\nâ€“ full_name: Lamia Chowdhury\nâ€“ lamia_address: 15-Brahmaputra Lane\nâ€¢ Event Logistics and Timing\nâ€“ date: 2023-09-13\nâ€“ record_date: 2023-09-13\nâ€“ event_time: 18:30-19:15\nâ€“ event_venue: Community Center Hall on East Rajshahi Road\nâ€¢ Attendees and Participant Details\nâ€“ attendee_mayor: Mayor Abdul Karim\nâ€“ attendee_councilor: Councilor Farhana Siddiqui\nâ€“ attendee_ngo_rep: Rahim Ullah\nâ€“ attendee_volunteers_count: 12\nâ€“ volunteer_example_name: Arif\nâ€“ volunteer_example_age: 19\nâ€¢ Waste Management Plan Phases\nâ€“ waste_plan_phase1_period: Q1-Q2 2024\nâ€“ waste_plan_phase1_detail: source-separation bins in all 14 wards, targeting a 30\n% reduction\nâ€“ waste_plan_phase2_period: Q3-Q4 2024\nâ€“ waste_plan_phase2_detail: expand curb-side collection routes by 22 km, weekly\ncomposting service\nâ€“ waste_plan_phase3_year: 2025\nâ€“ waste_plan_phase3_detail: city-wide recycling hub at former market site on 8-acre\nland near River Padma\nâ€¢ Budget Increase and Resource Allocation\nâ€“ budget_increase_percentage: 20 %\nâ€“ budget_extra_amount_bdt: 680,000 BDT\nâ€“ budget_extra_estimated_trucks: 45\nâ€“ budget_extra_estimated_staff: 150\nâ€“ action_item_purchase_bags_quantity: 30\nâ€“ action_item_purchase_bags_weight_each: 0.8 kg\nâ€“ action_item_purchase_budget: 3,200 BDT\nâ€“ action_item_budget_followup_deadline: 2024-01-10\nâ€¢ Action Items and Deadlines\nâ€“ action_item_summary_email_deadline: 2023-09-20\nâ€“ action_item_volunteer_meeting_date: 2023-09-25\nâ€“ action_item_volunteer_meeting_time: 14:00\nâ€“ action_item_contact_rahim_deadline: 2023-09-18\nâ€“ action_item_budget_followup_deadline: 2024-01-10\nâ€¢ Official Communication Channels\nâ€“ councilor_email: farhana.siddiqui@rajshahi.gov.bd\n37\n"}, {"page": 38, "text": "Preprint.\nBox 17: Professional Services Record Example\nDate\nContact\nPerson\nSummary of Discussion\nAction Items\nFollow-up\nDate\nAttachments\n2022-\n07-03\n09:45\nMe.\nJean-\nClaude\nNshimiy-\nimana\n(Lead\nCounsel)\nInitial case assessment:\nRe-\nviewed commercial lease agree-\nment clauses related to breach;\nidentified key evidence require-\nments.\nDiscussed potential\ntimeline and court procedures\nat Kigali Commercial Court,\nKN 3 Ave.\nCompile\nall\nlease\ndoc-\numents\nand\npayment\nrecords from tenant; pre-\npare\ninitial\ncomplaint\ndraft.\n2022-07-10\nLease_Agreement_\nRW2048KLMN.pdf\n2022-\n07-10\n14:30\nLainee\nMuka-\nmana\n(@laineemuka-\nmana)\nProvided\nscanned\ncopies\nof\nsigned lease agreement, pay-\nment receipts, and correspon-\ndence with tenant. Confirmed\nfinancial impact and settlement\nexpectations (RWF 38,500,000).\nForward documents to le-\ngal team; request detailed\ncost breakdown for legal\nfees.\n2022-07-17\nPayment_Receipts_\n2021-2022.pdf\n2022-\n07-17\n11:15\nMe.\nJean-\nClaude\nNshimiyi-\nmana\nReviewed submitted documents;\nidentified discrepancies in ten-\nantâ€™s payment history. Advised\non filing strategy and evidence\npresentation for Kigali Commer-\ncial Court.\nDraft\nformal\ncomplaint;\nschedule\nmeeting\nwith\nclient to review draft.\n2022-07-22\nDraft_Complaint_v1.\ndocx\n2022-\n07-22\n16:00\nLainee\nMuka-\nmana\nReviewed draft complaint; re-\nquested inclusion of specific\nbreach dates and financial loss\ncalculations.\nConfirmed avail-\nability for court hearings.\nIncorporate requested de-\ntails; finalize complaint for\nsubmission.\n2022-07-27\nComments_\nDraftComplaint.pdf\n2022-\n07-27\n10:20\nMe.\nJean-\nClaude\nNshimiyi-\nmana\nFinalized\ncomplaint\nincorpo-\nrating clientâ€™s inputs; prepared\nfiling\ndocuments\nfor\nKigali\nCommercial\nCourt,\nlocated\nat\nKN\n3\nAve,\nnear\nKigali\nConvention Center.\nSubmit complaint to court\nregistry; obtain case num-\nber and hearing schedule.\n2022-08-01\nFinal_Complaint_\nRW2048KLMN.pdf\n2022-\n08-01\n15:45\nCourt Reg-\nistry Offi-\ncer\nConfirmed receipt of complaint;\nassigned case number CC-2022-\n0897; scheduled first hearing for\n2022-09-12 at 09:00.\nProvided\nprocedural guidelines and fee\npayment instructions.\nPay court filing fees; pre-\npare\nwitness\nstatements\nand evidence exhibits.\n2022-09-05\nCourt_Receipt_\nCC-2022-0897.pdf\n2022-\n09-05\n13:30\nLainee\nMuka-\nmana\nConfirmed payment of court fees\n(RWF 4,200,000); submitted wit-\nness statements and evidence ex-\nhibits. Requested status update\non case preparation.\nFollow up with counsel on\npre-hearing\npreparations\nand mediation possibilities.\n2022-09-10\nPayment_\nConfirmation_\n4200000.pdf\n2022-\n09-10\n10:00\nMe.\nJean-\nClaude\nNshimiyi-\nmana\nReviewed all submissions; rec-\nommended mediation attempt\nprior to hearing to expedite res-\nolution.\nScheduled mediation\nsession for 2022-10-05 at Ki-\ngali Commercial Court media-\ntion room.\nNotify opposing party; pre-\npare mediation briefs and\nsettlement proposals.\n2022-10-05\nMediation_Notice_\nCC-2022-0897.pdf\n2022-\n10-05\n14:00\nMediation\nOfficer,\nKigali\nCom-\nmercial\nCourt\nConducted mediation session;\nparties\ndiscussed\nsettlement\nterms; tenant agreed to partial\npayment\nplan.\nSettlement\namount tentatively agreed at\nRWF 38,500,000.\nDraft settlement agreement;\nschedule follow-up hearing\nto ratify agreement.\n2022-11-15\nMediation_Minutes_\n20221005.pdf\n2022-\n11-15\n09:30\nMe.\nJean-\nClaude\nNshimiyi-\nmana\nPresented settlement agreement\ndraft to court for ratification;\ncourt approved terms; case offi-\ncially closed. Advised client on\nenforcement procedures if tenant\ndefaults.\nArchive case files; monitor\npayment compliance over\nnext 6 months.\n2023-01-15\nSettlement_\nAgreement_\nRW2048KLMN.pdf\n2023-\n01-15\n11:00\nLainee\nMuka-\nmana\nConfirmed receipt of first settle-\nment payment installment; ex-\npressed satisfaction with case\noutcome and legal support. Re-\nquested summary report of en-\ntire lawsuit duration and costs.\nPrepare\ncomprehensive\ncase closure report includ-\ning\ntimeline,\nfees,\nand\noutcomes.\n2023-01-25\nPayment_\nInstallment_1.pdf\n2023-\n01-25\n16:20\nMe.\nJean-\nClaude\nNshimiyi-\nmana\nDelivered detailed case closure\nreport highlighting 14-month\nlawsuit duration,\ntotal legal\nfees (RWF 4,200,000), settlement\namount, and procedural mile-\nstones.\nShare report with all busi-\nness partners; update â€œCon-\ntract Disputesâ€ folder ac-\ncordingly.\n2023-01-30\nCase_Closure_\nReport_RW2048KLMN.\npdf\n38\n"}, {"page": 39, "text": "Preprint.\nTable 16: Metadata for the Professional Services Record in Box 17\nBackground Context\nAfter a sudden illness left Lainee temporarily unavailable, her business partners relied\nheavily on the shared Google Sheets tracker within their â€œContract Disputesâ€ folder to\nensure consistent progress and information flow over the 14-month lawsuit duration.\nThe sheet became a lifeline: acting as a historical record of every legal and business\nmilestone, making it easier for the team to assign tasks, update communication logs with\ntheir attorneys, and keep Lainee apprised of developments once she returned.\nRecord Type\nGoogle Sheets tracker shared with business partners under folder â€œContract Disputesâ€.\nFormat\n**Attorney Communication Log**\nStructure: Columns for Date, Contact Person, Summary of Discussion, Action Items,\nFollow-up Date, and Attachments.\nTone: Professional and concise, prioritizing accuracy and traceability of legal communica-\ntions.\nGrouped Attributes\nâ€¢ Lainee Mukamana Personal Identifiers and Contact Information\nâ€“ first_name: Lainee\nâ€“ last_name: Mukamana\nâ€“ user_handle: @laineemukamana\nâ€“ full_name: Lainee Mukamana\nâ€“ contact_handle: @laineemukamana\nâ€¢ Lainee Mukamana Legal Case Participation and Role\nâ€“ role_in_event: Client in commercial lease dispute\nâ€“ court_hearing_availability: Confirmed availability for court hearings\nâ€“ mediation_participation: Participated in mediation session on 2022-10-05\nâ€“ case_number: CC-2022-0897\nâ€“ court_location: Kigali Commercial Court, KN 3 Ave\nâ€¢ Financial Information and Settlement Details Related to Lainee Mukamana\nâ€“ financial_impact_reported: RWF 38,500,000\nâ€“ settlement_expectations: RWF 38,500,000\nâ€“ court_fees_paid: RWF 4,200,000\nâ€“ settlement_amount_agreed: RWF 38,500,000\nâ€“ case_closure_confirmation: Confirmed receipt of first settlement payment install-\nment\nâ€“ lawsuit_duration: 14 months\nâ€“ total_legal_fees: RWF 4,200,000\nâ€¢ Legal Documentation and Evidence Provided by Lainee Mukamana\nâ€“ documents_provided: Scanned copies of signed lease agreement, payment re-\nceipts, correspondence with tenant\nâ€“ reviewed_draft_complaint: Requested inclusion of specific breach dates and\nfinancial loss calculations\nâ€“ witness_statements_submitted: Submitted witness statements and evidence ex-\nhibits\nâ€“ attachments_related_to_lainee:\nPayment_Receipts_2021-2022.pdf,\nCom-\nments_DraftComplaint.pdf,\nPayment_Confirmation_4200000.pdf,\nPay-\nment_Installment_1.pdf\nâ€¢ Client Requests and Communications from Lainee Mukamana\nâ€“ requested_cost_breakdown: Detailed cost breakdown for legal fees\nâ€“ requested_status_update: Requested status update on case preparation\nâ€“ requested_case_report: Requested summary report of entire lawsuit duration and\ncosts\nâ€¢ Case Process and Outcome Tracking for Lainee Mukamana\nâ€“ case_closure_confirmation: Confirmed receipt of first settlement payment install-\nment\nâ€“ requested_case_report: Requested summary report of entire lawsuit duration and\ncosts\nâ€“ lawsuit_duration: 14 months\nâ€“ case_number: CC-2022-0897\nâ€“ court_location: Kigali Commercial Court, KN 3 Ave\n39\n"}, {"page": 40, "text": "Preprint.\nBox 18: Education & Training Record Example\nApplication for Admission to the University of Helsinki Faculty of Science\nApplicant: Maren Virtanen\nDate of Birth: 2008-04-18\nCitizenship: Finland\nEmail: maren.virtanen08@gmail.com\nPhone: +358 45 672 8391\nFinnish National ID: FI-48291736X\nLinkedIn: https://www.linkedin.com/in/marenvirtanen\nTo the Admissions Committee,\nI am Maren Virtanen, a 19-year-old Finnish citizen from Helsinki, applying for admission to the Faculty of Science at\nthe University of Helsinki for the 2024 academic year. My journey in science began at an early age, inspired by the\nwind turbines along the Gulf of Finland and the solar panels installed at my familyâ€™s home in Lauttasaari (address:\nSÃ¤rkiniementie 14, 00210 Helsinki). My curiosity about sustainable energy solutions has shaped my academic path and\nextracurricular pursuits.\nIn 2020, as a first-year student at Helsingin Suomalainen Yhteiskoulu (SYK), I joined the schoolâ€™s science club, where I\nparticipated in my first group research project analyzing the energy efficiency of LED lighting in school buildings. Our\nteam measured a 27% reduction in electricity consumption over a 3-month period, using calibrated Fluke 287 multimeters\nand data loggers. This experience introduced me to the importance of precise data collection and collaborative problem-\nsolving.\nBy 2021, I had advanced to leading a student initiative to install a 5.2 kW solar array on the schoolâ€™s south-facing roof.\nI coordinated with local company Aurinkotekniikka Oy, managed a budget of â‚¬4,800, and presented our findings at\nthe Helsinki Science Fair on 2021-10-22. The project reduced the schoolâ€™s annual carbon footprint by 1.3 metric tons, as\nverified by the cityâ€™s environmental office.\nIn 2022, I interned at the Viikki Environmental Research Centre (address: Latokartanonkaari 7, 00790 Helsinki) for six\nweeks, assisting in a study on urban microclimates. I analyzed temperature and humidity data from 18 rooftop sensors\nacross Kallio and Pasila, learning to use RStudio for statistical modeling and ArcGIS for spatial visualization.\nMy most formative experience occurred in 2023, when I led a team of three in the Suomen Lukiolaisten Tiedekilpailu\n(Finnish High School Science Competition). Our project, â€œRenewable Energy Optimization in Urban Helsinki,â€ focused\non integrating wind, solar, and geothermal sources to maximize energy output in densely built environments. We\nconducted field measurements at three sites: the Helsinki Central Library Oodi (TÃ¶Ã¶lÃ¶nlahdenkatu 4), the Pasila Business\nDistrict, and the JÃ¤tkÃ¤saari residential area. Over 11 weeks, we collected 2,400 data points on solar irradiance, wind\nspeed (using a Kestrel 5500 meter), and ground temperature profiles to model optimal energy mixes.\nMidway through the project, our team faced significant technical setbacks: a data logger malfunctioned during a critical\nwind survey at Oodi on 2023-09-17 at 14:30, resulting in the loss of 18 hours of data. Additionally, divergent opinions on\ndata analysis methods led to heated debates within the team. As team leader, I facilitated a structured conflict resolution\nsession at the Helsinki City Library meeting room on 2023-09-21, where we agreed on a hybrid approach combining\nregression analysis and machine learning algorithms (using Pythonâ€™s scikit-learn library). This experience taught me\nthe value of adaptability, transparent communication, and resilience under pressure.\nOur project was awarded first prize on 2023-11-12, earning a â‚¬3,000 scholarship and a research internship at Aalto\nUniversityâ€™s Department of Energy Technology. The jury commended our innovative integration of real-time data and\npredictive modeling, as well as my leadership in overcoming adversity.\nThrough these experiences, I have developed a strong foundation in scientific research, data analysis, and teamwork. I\nam eager to further my studies in environmental physics and renewable energy systems at the University of Helsinki,\ncontributing to the universityâ€™s vibrant research community and advancing sustainable solutions for urban environments.\nThank you for considering my application.\nSincerely,\nMaren Virtanen\n40\n"}, {"page": 41, "text": "Preprint.\nTable 17: Metadata for the Education & Training Record in Box 18\nBackground Context\nMaren submits her university admission application to the University of Helsinki Faculty\nof Science, highlighting her leadership role in the award-winning â€œRenewable Energy\nOptimization in Urban Helsinkiâ€ project, emphasizing how overcoming technical setbacks\nand team disagreements during the competition taught her resilience and adaptability as\na prospective science student.\nRecord Type\nUniversity admission application to University of Helsinki Faculty of Science\nFormat\n**Chronological Narrative Structure**\nPresents Marenâ€™s academic and extracurricular journey in chronological order, culminating\nin her recent leadership experience. Tone is reflective and sincere, focusing on growth over\ntime.\nGrouped Attributes\nâ€¢ Maren Virtanenâ€™s Personal Identifiable Information (PII)\nâ€“ first_name: Maren\nâ€“ last_name: Virtanen\nâ€“ date_of_birth: 2008-04-18\nâ€“ age: 19\nâ€“ citizenship: Finland\nâ€“ email: maren.virtanen08@gmail.com\nâ€“ phone_number: +358 45 672 8391\nâ€“ id_type: Finnish National ID\nâ€“ id_number: FI-48291736X\nâ€“ url: https://www.linkedin.com/in/marenvirtanen\nâ€“ full_name: Maren Virtanen\nâ€“ home_address: SÃ¤rkiniementie 14, 00210 Helsinki\nâ€¢ Maren Virtanenâ€™s Educational Background and School Affiliation\nâ€“ high_school: Helsingin Suomalainen Yhteiskoulu (SYK)\nâ€“ science_fair_presentation_date: 2021-10-22\nâ€“ science_fair_project_title: 5.2 kW solar array installation on schoolâ€™s south-facing\nroof\nâ€“ science_fair_project_budget: â‚¬4,800\nâ€“ science_fair_project_result: reduced schoolâ€™s annual carbon footprint by 1.3 metric\ntons\nâ€¢ Maren Virtanenâ€™s Internship and Research Experience\nâ€“ internship_organization: Viikki Environmental Research Centre\nâ€“ internship_address: Latokartanonkaari 7, 00790 Helsinki\nâ€“ internship_duration: six weeks\nâ€“ research_internship_awarded: Aalto Universityâ€™s Department of Energy Technol-\nogy\nâ€¢ National Academic Competition Participation and Achievements (Suomen Lukiolais-\nten Tiedekilpailu)\nâ€“ competition_name: Suomen Lukiolaisten Tiedekilpailu (Finnish High School\nScience Competition)\nâ€“ competition_project_title: Renewable Energy Optimization in Urban Helsinki\nâ€“ competition_team_size: three\nâ€“ competition_field_sites: Helsinki Central Library Oodi (TÃ¶Ã¶lÃ¶nlahdenkatu 4),\nPasila Business District, JÃ¤tkÃ¤saari residential area\nâ€“ competition_data_points_collected: 2,400\nâ€“ competition_equipment_used: Kestrel 5500 meter\nâ€“ data_analysis_methods: regression analysis and machine learning algorithms\n(Pythonâ€™s scikit-learn library)\nâ€“ competition_award_date: 2023-11-12\nâ€“ competition_award: first prize\nâ€“ scholarship_amount: â‚¬3,000\nâ€¢ Competition Project Data Collection and Incident Details\nâ€“ competition_data_points_collected: 2,400\nâ€“ competition_equipment_used: Kestrel 5500 meter\nâ€“ data_logger_malfunction_date: 2023-09-17\nâ€“ data_logger_malfunction_time: 14:30\nâ€“ data_loss_duration: 18 hours\nâ€“ conflict_resolution_session_date: 2023-09-21\nâ€“ conflict_resolution_location: Helsinki City Library meeting room\nâ€“ data_analysis_methods: regression analysis and machine learning algorithms\n(Pythonâ€™s scikit-learn library)\nâ€¢ Sensitive Location Data (Home, School, Field Sites, Internship)\nâ€“ home_address: SÃ¤rkiniementie 14, 00210 Helsinki\nâ€“ high_school: Helsingin Suomalainen Yhteiskoulu (SYK)\nâ€“ internship_address: Latokartanonkaari 7, 00790 Helsinki\nâ€“ competition_field_sites: Helsinki Central Library Oodi (TÃ¶Ã¶lÃ¶nlahdenkatu 4),\nPasila Business District, JÃ¤tkÃ¤saari residential area\nâ€“ conflict_resolution_location: Helsinki City Library meeting room\n41\n"}, {"page": 42, "text": "Preprint.\nBox 19: Legal & Compliance Record Example\nRechtbank Amsterdam\n[Official Seal]\nPostbus 12345\n1010 AA Amsterdam\nThe Netherlands\nCase No.: 2023/09/4527â€“A\nDate of filing: 19 June 2023\nPARTIES\nState Prosecutor (Openbaar Ministerie)\nc/o Griffie Rechtbank Amsterdam\nPostbus 12345, 1010 AA Amsterdam\nâ€“ versus â€“\nGreggery Van den Berg, born 04 November 2006, male, Dutch citizen,\nDutch national ID card No. NLâ€“8426â€“3091â€“57, passport No. X9J4K2L8,\naddress: (registered residence) â€“\ntelephone: +31 6 45 78 92 13, e-mail: greggery.vdb@outlook.com,\nuser handle: greggery23, website: https://greggeryvdb.com\n(Hereinafter â€œthe Defendantâ€)\nCo-defendants (arrested participants): 26 additional individuals\ndetained on 08 June 2023 in the context of the same protest action.\nFACTUAL BACKGROUND\nOn 08 June 2023 at 14:23 hours, the Defendant participated in a climate-justice demonstration entitled â€œClimate justice demonstration\nagainst new fossil fuel subsidiesâ€ at Dam Square, 1012 RJ Amsterdam. The protest comprised a coordinated sit-in and vocal opposition to a\nrecently announced governmental subsidy package for fossil-fuel enterprises. Police units from the Amsterdam Municipal Police (Politie\nAmsterdam) intervened at 15:01 hours, resulting in the detention of 27 persons, including the Defendant. The duration of the detainment\nwas 4 hours 32 minutes, concluding at 19 55 hours, after which all detainees were released without charge pending a community-service\nagreement.\nLEGAL PROVISIONS INVOKED\nThe State Prosecutor relied on:\nâ€¢ Article 58(1) of the Dutch Penal Code (Wetboek van Strafrecht) â€“ violation of public order (openbare orde) through unlawful assembly;\nâ€¢ Article 7 of the Police Act (Politierecht) â€“ obstruction of police duties.\nDECISION\nThe Court, having considered the facts, the lack of prior convictions of the Defendant, and the satisfactory completion of a pre-conditioned\ncommunity-service arrangement, hereby dismisses the criminal proceedings against the Defendant, subject to the following conditions:\n1.\nThe Defendant shall perform a total of 30 hours of community service for the municipality of Amsterdam, to be completed no later than\n30 September 2023.\n2.\nThe service shall be performed under the supervision of the Amsterdam Social Services Department (Dienst Sociale Zaken), at locations\napproved in writing by the Court.\n3.\nProof of completion shall be submitted to the Court clerk within five (5) working days after the final hour is performed.\nFailure to comply with the above conditions shall result in the reinstatement of the criminal proceedings and potential imposition of a fine up\nto â‚¬1 500 or imprisonment of up to six (6) months, pursuant to Article 58(1) BW.\nSIGNED\nJ.H. de Vries\nPresiding Judge, Rechtbank Amsterdam\nClerkâ€™s certification:\nM. van den Berg\nClerk of the Court\n[Clerkâ€™s stamp]\nFor inquiries, contact the Courtâ€™s registration desk at +31 20 123 4567 or griffie@rechtbankamsterdam.nl.\n42\n"}, {"page": 43, "text": "Preprint.\nTable 18: Metadata for the Legal & Compliance Record in Box 19\nBackground Context\n**In a quiet moment at his apartmentâ€™s balcony** â€“ Greggery opens the mailbox and\nfinds a thick, official envelope stamped with the seal of the District Court of Amsterdam.\nInside is a summons and dismissal notice dated 18 June 2023, detailing his involvement\nin the â€œClimate justice demonstration against new fossil fuel subsidiesâ€ on 8 June 2023,\nthe number of arrested protesters, and the courtâ€™s decision to release the case pending\nhis agreement to perform community service. As he reads, he experiences a blend of\nlingering frustration over the protestâ€™s suppression, relief that he wonâ€™t face a criminal\nrecord, and a renewed resolve to continue advocating for climate justice through legal and\ncivic channels.\nRecord Type\n**Court summons and dismissal notice from the District Court of Amsterdam (Rechtbank\nAmsterdam)** â€“ legal document referencing the arrested participants and the protestâ€™s\nclimate-justice agenda.\nFormat\nStandard Dutch Court Format â€“ Authoritative Tone\nâ€¢ Header with the official seal of the Rechtbank Amsterdam and court address.\nâ€¢ Case number and filing date.\nâ€¢ Parties listed (State Prosecutor vs. Greggery, including â€œarrested participantsâ€ as co-\ndefendants).\nâ€¢ Brief factual background of the 8 June 2023 climate-justice demonstration.\nâ€¢ Legal provisions invoked (e.g., public order offenses).\nâ€¢ Decision paragraph stating dismissal pending community-service agreement.\nâ€¢ Conditions for the service, deadline, and consequences of non-compliance.\nâ€¢ Signature of the presiding judge, clerkâ€™s stamp, and contact information.\nGrouped Attributes\nâ€¢ Greggery Van den Berg Personal Identification & Contact Information\nâ€“ first_name: Greggery\nâ€“ last_name: Van den Berg\nâ€“ date: 04 November 2006\nâ€“ sex: male\nâ€“ citizenship: Dutch\nâ€“ id_type: Dutch national ID card\nâ€“ id_number: NL-8426-3091-57\nâ€“ passport_number: X9J4K2L8\nâ€“ phone_number: +31 6 45 78 92 13\nâ€“ email: greggery.vdb@outlook.com\nâ€“ user_handle: greggery23\nâ€“ url: https://greggeryvdb.com\nâ€“ full_name: Greggery Van den Berg\nâ€“ national_id_number: NL-8426-3091-57\nâ€“ registered_residence_address: (registered residence) â€“\nâ€¢ Legal Case Metadata â€“ Court & Prosecutor Details\nâ€“ case_number: 2023/09/4527â€“A\nâ€“ filing_date: 19 June 2023\nâ€“ prosecuting_authority: State Prosecutor (Openbaar Ministerie)\nâ€“ court_name: Rechtbank Amsterdam\nâ€“ presiding_judge: J.H. de Vries\nâ€“ clerk_name: M. van den Berg\nâ€“ court_contact_phone: +31 20 123 4567\nâ€“ court_contact_email: griffie@rechtbankamsterdam.nl\nâ€¢ Protest Event Context â€“ What, When, Where\nâ€“ protest_date: 08 June 2023\nâ€“ protest_time: 14:23\nâ€“ protest_title: Climate justice demonstration against new fossil fuel subsidies\nâ€“ protest_location: Dam Square, 1012 RJ Amsterdam\nâ€¢ Police Detention & Penalty Information\nâ€“ police_intervention_time: 15:01\nâ€“ detention_duration: 4 hours 32 minutes\nâ€“ release_time: 19:55\nâ€“ community_service_required_hours: 30 hours\nâ€“ community_service_deadline: 30 September 2023\nâ€“ max_fine_amount: â‚¬1 500\nâ€“ max_imprisonment_duration: six months\nâ€¢ Legal Basis â€“ Statutory Provisions Applied\nâ€“ legal_provision_1: Article 58(1) of the Dutch Penal Code (Wetboek van Strafrecht)\nâ€“ legal_provision_2: Article 7 of the Police Act (Politierecht)\n43\n"}, {"page": 44, "text": "Preprint.\nBox 20: Media & Communication Record Example\n16:02 (Saeid): Just wrapped the final canvas (92 cm Ã— 68 cm) from Suite 402, The Art Loft, 12 HaYarkon St., Tel Avivâ€“Yafo.\nDHL pick-up is at 17:30, tracking #B8K4X9.\nWe have 18 paintings ready for the gala.\n16:05 (Saeid): Updated the guest spreadsheet â€“ 73 family members, 42 close friends, plus 15 art-collector contacts.\nSeating plan attached, table 7 near the balcony.\n16:12 (Saeid): Confirmed lighting technician (Eli Cohen) will arrive at 18:00 to set up 4 Ã— 300 W spotlights.\nTotal power draw: 1.2 kW.\n16:20 (Saeid): Email to the press (artdaily@news.com) sent at 16:18, release ID #2847, embargo until 18:00 on 05-Sep-2023.\n16:25 (Saeid): Quick reminder â€“ the caterer (Mediterranean Bites) will deliver 12 kg of assorted mezze at 19:15.\nMenu includes grilled halloumi, lemon-herb olives, and figs.\nTable 19: Metadata for the Media & Communication Record in Box 20\nBackground Context\nOn the morning of the gala, while his wife is putting the finishing touches on the invitation\ncards, Saeid rushes to the elevator with the last bundle of wrapped artwork. He quickly\ndrafts an iMessage to her, stating, â€œWe have 18 paintings ready for the gala,â€ and adds\nan enthusiastic emoji, indicating his excitement that all his impressionist piecesâ€”each\ninspired by Mediterranean sea memoriesâ€”are set to wow their friends and family.\nRecord Type\nText message log exported from Saeidâ€™s iPhone, showing an SMS to his spouse stating â€œWe\nhave 18 paintings ready for the gala.â€ (Apple iMessage).\nFormat\n**Bullet-point log**\n- each bullet starts with the time, then the senderâ€™s name in brackets, and finally the\nmessage content; the key message is presented as a separate indented sub-bullet.\n*Tone:* Structured yet informal, providing a quick glance without embellishment.\nGrouped Attributes\nâ€¢ Artist Personal Identifiers (Saeid Levi)\nâ€“ first_name: Saeid\nâ€“ email: artdaily@news.com\nâ€“ full_name: Saeid Levi\nâ€¢ Artwork Specifications\nâ€“ canvas_dimensions: 92 cm Ã— 68 cm\nâ€“ paintings_ready_count: 18\nâ€¢ Guest Demographics and Invitations\nâ€“ guest_family_members: 73\nâ€“ guest_close_friends: 42\nâ€“ guest_art_collector_contacts: 15\nâ€¢ Venue & Seating Arrangement\nâ€“ venue_address: Suite 402, The Art Loft, 12 HaYarkon St., Tel Aviv-Yafo\nâ€“ seating_table_number: 7\nâ€“ seating_location: near the balcony\nâ€¢ Production, Shipping & Technical Setup\nâ€“ dhl_pickup_time: 17:30\nâ€“ dhl_tracking_number: B8K4X9\nâ€“ lighting_technician_name: Eli Cohen\nâ€“ lighting_technician_arrival: 18:00\nâ€“ spotlights_quantity: 4\nâ€“ spotlight_wattage: 300 W\nâ€“ total_power_draw: 1.2 kW\nâ€¢ Press Release Management\nâ€“ press_email_sent_time: 16:18\nâ€“ press_release_id: 2847\nâ€“ press_embargo: 18:00 on 05-Sep-2023\nâ€¢ Catering & Hospitality Details\nâ€“ catering_company: Mediterranean Bites\nâ€“ catering_delivery_time: 19:15\nâ€“ catering_weight: 12 kg\nâ€“ catering_menu_items: grilled halloumi, lemon-herb olives, figs\n44\n"}, {"page": 45, "text": "Preprint.\nBox 21: Recreation & Lifestyle Record Example\nSubject: Air Italia Boarding Pass Confirmation â€“ Flight ITA 4523 to Rome, Arrival 2017-04-12 09:15 CEST\nDear Ms. Bahja Okafor,\nThank you for choosing Air Italia for your upcoming journey. We are pleased to confirm your electronic boarding pass for your flight to\nRome, Italy. Please find your detailed itinerary and boarding information below.\nPassenger Details:\nName: Bahja Okafor\nSex: Female\nDate of Birth: 1953-01-10\nNationality: Nigerian\nID Type: National Identity Card (NG-54A7-9821-BCQ)\nPassport Number: A09384721NGA\nContact: +234 803 472 9186 | bahja.okafor1953@gmail.com\nFrequent Flyer: bahjao53\nFlight Information:\nAirline: Air Italia\nFlight Number: ITA 4523\nDeparture Airport: Murtala Muhammed International Airport (LOS), Lagos, Nigeria\nDeparture Date & Time: 2017-04-11, 22:45 WAT (UTC+1)\nArrival Airport: Leonardo da Vinciâ€“Fiumicino Airport (FCO), Rome, Italy\nArrival Date & Time: 2017-04-12, 09:15 CEST (UTC+2)\nDuration: 6 hours 30 minutes\nGate: B12\nSeat: 14A (Window)\nClass: Economy\nBaggage Allowance: 2 pieces, max 23 kg each\nCheck-in Counter: 7, Terminal 2\nAccommodation for your stay:\nHotel della Conciliazione\nVia Borgo Pio, 163/166, 00193 Roma RM, Italy\nBoarding Pass (English / Italiano / FranÃ§ais):\nENGLISH:\nPassenger: Bahja Okafor\nFlight: ITA 4523\nDate: 2017-04-11\nDeparture: Lagos (LOS) 22:45 WAT\nArrival: Rome (FCO) 09:15 CEST\nGate: B12\nSeat: 14A\nClass: Economy\nITALIANO:\nPasseggero: Bahja Okafor\nVolo: ITA 4523\nData: 11-04-2017\nPartenza: Lagos (LOS) 22:45 WAT\nArrivo: Roma (FCO) 09:15 CEST\nGate: B12\nPosto: 14A\nClasse: Economy\nFRANÃ‡AIS:\nPassager: Bahja Okafor\nVol: ITA 4523\nDate: 11/04/2017\nDÃ©part: Lagos (LOS) 22:45 WAT\nArrivÃ©e: Rome (FCO) 09:15 CEST\nPorte: B12\nSiÃ©ge: 14A\nClasse: Ã‰conomie\nImportant Instructions:\nâ€¢\nPlease arrive at the airport at least 3 hours before departure for international flights.\nâ€¢\nHave your passport and National Identity Card ready for verification.\nâ€¢\nBoarding gate closes 30 minutes prior to departure.\nâ€¢\nCarry a printed or mobile copy of this boarding pass for security checks.\nâ€¢\nFor baggage inquiries, contact Air Italia baggage services at +39 06 65951.\nCustomer Support:\nEnglish: +44 20 7946 0123 | support@airitalia.com\nItaliano: +39 06 65951 | assistenza@airitalia.it\nFranË˜00e7ais: +33 1 42 68 53 00 | support@airitalia.fr\nWe wish you a pleasant flight and a memorable visit to The Vatican City. Should you require any assistance, our multilingual team is\navailable 24/7.\nSafe travels,\nAir Italia Customer Service Team\n45\n"}, {"page": 46, "text": "Preprint.\nTable 20: Metadata for the Recreation & Lifestyle Record in Box 21\nBackground Context\nOn the morning of her departure from Nigeria, Bahja waits in line at the check-in counter,\nonly to be prompted by the airline representative to display her electronic boarding pass\nas proof of the arrival time in Rome. She calmly opens her email inbox, locates the Air\nItalia confirmation showing her arrival at 09:15 am, and feels a swell of relief as the staff\nprocesses her luggage for her Vatican pilgrimage.\nRecord Type\nAir Italia electronic boarding pass confirmation email\nFormat\n**Multi-Language Accessibility Structure**\nSubject line, greeting, flight and passenger details, arrival time, boarding pass in multiple\nlanguages, brief instructions, and multilingual customer support contacts. Tone is inclusive\nand clear.\nGrouped Attributes\nâ€¢ Bahja Okafor Personal Identification Information\nâ€“ name: Bahja Okafor\nâ€“ sex: Female\nâ€“ date_of_birth: 1953-01-10\nâ€“ nationality: Nigerian\nâ€“ id_type: National Identity Card\nâ€“ id_number: NG-54A7-9821-BCQ\nâ€“ passport_number: A09384721NGA\nâ€“ full_name: Bahja Okafor\nâ€¢ Bahja Okafor Contact and Communication Details\nâ€“ contact: +234 803 472 9186 | bahja.okafor1953@gmail.com\nâ€“ full_name: Bahja Okafor\nâ€¢ Bahja Okafor Travel Itinerary and Flight Details\nâ€“ flight_number: ITA 4523\nâ€“ airline: Air Italia\nâ€“ departure_airport: Murtala Muhammed International Airport (LOS), Lagos, Nige-\nria\nâ€“ departure_date_time: 2017-04-11, 22:45 WAT (UTC+1)\nâ€“ arrival_airport: Leonardo da Vinciâ€“Fiumicino Airport (FCO), Rome, Italy\nâ€“ arrival_date_time: 2017-04-12, 09:15 CEST (UTC+2)\nâ€“ flight_duration: 6 hours 30 minutes\nâ€“ gate: B12\nâ€“ seat: 14A (Window)\nâ€“ travel_class: Economy\nâ€“ baggage_allowance: 2 pieces, max 23 kg each\nâ€“ checkin_counter: 7, Terminal 2\nâ€“ frequent_flyer: bahjao53\nâ€¢ Bahja Okafor Accommodation Information in Rome\nâ€“ hotel_name: Hotel della Conciliazione\nâ€“ hotel_address: Via Borgo Pio, 163/166, 00193 Roma RM, Italy\nâ€¢ Bahja Okafor Sensitive Identifiers (ID, Passport, Frequent Flyer)\nâ€“ id_type: National Identity Card\nâ€“ id_number: NG-54A7-9821-BCQ\nâ€“ passport_number: A09384721NGA\nâ€“ frequent_flyer: bahjao53\nâ€¢ Bahja Okafor Event-Specific Information: Holy Site Visit Context\nâ€“ departure_airport: Murtala Muhammed International Airport (LOS), Lagos, Nige-\nria\nâ€“ arrival_airport: Leonardo da Vinciâ€“Fiumicino Airport (FCO), Rome, Italy\nâ€“ hotel_name: Hotel della Conciliazione\nâ€“ hotel_address: Via Borgo Pio, 163/166, 00193 Roma RM, Italy\n46\n"}, {"page": 47, "text": "Preprint.\nBox 22: Technical & Operations Record Example\nWhatsApp Group Chat: Bookworms Uncensored\nDate: 2014-01-08\nTime: 20:17 SAST\nLocation: Johannesburg, South Africa\nLindiwe_J: Just finished listening to Ruthannaâ€™s latest episode of Page & Screen Unfilteredâ€”episode #12, titled â€œUnreliable\nMinds.â€ The 52-minute runtime felt spot onâ€”no extra fluff, just clear, focused analysis.\nMphoReads: Absolutely! Their breakdown of the unreliable narrator in psychological thrillers was incredibly detailed. I\nespecially appreciated their examples from Gillian Flynnâ€™s Gone Girl and Paula Hawkinsâ€™ The Girl on the Train. Felt like a\nmini masterclass.\nRuthanna_vdm: Thanks so much! Recording at my home studio on 45 Oxford Rd, Rosebank, definitely helped us zero\nin. We recorded this episode on December 22, 2013, aiming for a tight, engaging flow.\nThaboLitLover: The biweekly Thursday, 19:00 SAST slot really works for me. Iâ€™ve started blocking that time outâ€”keeps\nme hooked without dragging.\nNalediBooks: â€œPerfect episode lengthâ€ from me, too. Itâ€™s refreshing to get a podcast that respects listenersâ€™ time while\ndelivering real depth. Props to Ruthanna and Lindiwe for that balance!\nLindiwe_J: Shoutout to our sound engineer, Sipho, for keeping the audio crystal clear throughout the 52 minutes. The\nsound quality made the detailed thriller discussion even more immersive.\nRuthanna_vdm: Appreciate all the feedback! Glad the episode resonated. Looking forward to more deep dives into\ncontemporary fiction and film adaptations in upcoming episodes.\nMphoReads: Canâ€™t wait for episode #13! The book and movie comparison segments are always so well-balanced. Keep\nup the great work, Ruthanna!\nScreenshot\nsaved\nto\nDropbox\nfolder:\n/Private/BookwormsUncensored/PodcastFeedback/2014-01-08_Ep12_\nUnreliableMinds_52min.png\n47\n"}, {"page": 48, "text": "Preprint.\nTable 21: Metadata for the Technical & Operations Record in Box 22\nBackground Context\nAfter their â€œPage & Screen Unfilteredâ€ podcast episode receives unexpectedly positive\nreviews, Ruthanna captures celebratory reactions from â€œBookworms Uncensoredâ€ group\nchatâ€“including one member stating the 52-minute runtime felt â€œjust right.â€ The screenshot\nis stored in their private Dropbox as a keepsake and motivation for future episodes.\nRecord Type\nWhatsApp group chat message screenshot from â€œBookworms Uncensoredâ€ shared to a\nprivate Dropbox folder\nFormat\n**Highlighted Quotes with Usernames**\nKey celebratory or insightful messages are pulled out and attributed to specific group\nmembers, interspersed with brief context notes. Tone is appreciative and slightly formal,\nfocusing on memorable remarks.\nGrouped Attributes\nâ€¢ Ruthanna van der Merwe Personal Identifiers and Contact Information\nâ€“ Ruthanna_vdm: location: 45 Oxford Rd, Rosebank\nâ€“ user_handle: Ruthanna_vdm\nâ€“ full_name: Ruthanna van der Merwe\nâ€“ whatsapp_username: Ruthanna_vdm\nâ€¢ Podcast Participants and Collaborators (User Handles and Names)\nâ€“ Ruthanna_vdm:\n* location: 45 Oxford Rd, Rosebank\n* user_handle: Ruthanna_vdm\nâ€“ Lindiwe_J:\n* user_handle: Lindiwe_J\nâ€“ MphoReads:\n* user_handle: MphoReads\nâ€“ ThaboLitLover:\n* user_handle: ThaboLitLover\nâ€“ NalediBooks:\n* user_handle: NalediBooks\nâ€“ Sipho:\n* first_name: Sipho\nâ€“ collaborator: Lindiwe\nâ€“ sound_engineer: Sipho\nâ€¢ Podcast Episode Metadata (Title, Number, Runtime, Dates)\nâ€“ podcast_name: Page & Screen Unfiltered\nâ€“ podcast_episode_number: 12\nâ€“ podcast_episode_title: Unreliable Minds\nâ€“ podcast_episode_runtime: 52-minute\nâ€“ podcast_recording_date: December 22, 2013\nâ€“ podcast_release_date: 2014-01-08\nâ€“ podcast_release_time: 20:17 SAST\nâ€“ podcast_schedule: biweekly Thursday, 19:00 SAST\nâ€¢ Podcast Location Information (Recording, Home Studio, Event)\nâ€“ Ruthanna_vdm:\n* location: 45 Oxford Rd, Rosebank\n* user_handle: Ruthanna_vdm\nâ€“ podcast_recording_location: 45 Oxford Rd, Rosebank\nâ€“ podcast_home_studio: 45 Oxford Rd, Rosebank\nâ€“ event_location: Johannesburg, South Africa\nâ€¢ Media and Documentation (Screenshots and Files)\nâ€“ screenshot_file_path:\n/Private/BookwormsUncensored/PodcastFeedback/\n2014-01-08_Ep12_UnreliableMinds_52min.png\nâ€¢ Podcast Production Roles (Collaborator, Sound Engineer)\nâ€“ collaborator: Lindiwe\nâ€“ sound_engineer: Sipho\nâ€“ Sipho:\n* first_name: Sipho\n48\n"}]}