{"doc_id": "arxiv:2511.17818", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.17818.pdf", "meta": {"doc_id": "arxiv:2511.17818", "source": "arxiv", "arxiv_id": "2511.17818", "title": "APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs", "authors": ["Aishwarya Mandyam", "Kalyani Limaye", "Barbara E. Engelhardt", "Emily Alsentzer"], "published": "2025-11-21T22:18:15Z", "updated": "2025-11-21T22:18:15Z", "summary": "Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.17818v1", "url_pdf": "https://arxiv.org/pdf/2511.17818.pdf", "meta_path": "data/raw/arxiv/meta/2511.17818.json", "sha256": "0133aeec7ee75da3d81785d1f36dcaa1305cb66c95a283de1b22bdee8bd80079", "status": "ok", "fetched_at": "2026-02-18T02:26:31.412195+00:00"}, "pages": [{"page": 1, "text": "Proceedings of Machine Learning Research 297, 2025\nMachine Learning for Health (ML4H) 2025\nAPRIL: Annotations for Policy evaluation with Reliable Inference\nfrom LLMs\nAishwarya Mandyam\nam2@stanford.edu\nStanford University\nKalyani Limaye\nlimayk@stanford.edu\nStanford University\nBarbara E. Engelhardt*\nbarbarae@stanford.edu\nStanford University, The Gladstone Institutes\nEmily Alsentzer*\nealsentzer@stanford.edu\nStanford University\nAbstract\nOff-policy evaluation (OPE) estimates the value\nof a contextual bandit policy prior to deploy-\nment.\nAs such, OPE plays a critical role in\nensuring safety in high-stakes domains such\nas healthcare.\nHowever, standard OPE ap-\nproaches are limited by the size and coverage\nof the behavior dataset. While previous work\nhas explored using expert-labeled counterfac-\ntual annotations to enhance dataset coverage,\nobtaining such annotations is expensive, limit-\ning the scalability of prior approaches. We pro-\npose leveraging large language models (LLMs)\nto generate counterfactual annotations for OPE\nin medical domains.\nOur method uses do-\nmain knowledge to guide LLMs in predicting\nhow key clinical features evolve under alternate\ntreatments. These predicted features can then\nbe transformed using known reward functions\nto create counterfactual annotations. We first\nevaluate the ability of several LLMs to pre-\ndict clinical features across two patient sub-\nsets in MIMIC-IV, finding that state-of-the-art\nLLMs achieve comparable performance. Build-\ning on this capacity to predict clinical features,\nwe generate LLM-based counterfactual annota-\ntions and incorporate them into an OPE esti-\nmator. Our empirical results analyze the bene-\nfits of counterfactual annotations under varying\ndegrees of shift between the behavior and tar-\nget policies.\nWe find that in most cases, the\nLLM-based counterfactual annotations signifi-\ncantly improve OPE estimates up to a point.\nWe provide an entropy-based metric to identify\nwhen additional annotations cease to be use-\nful. Our results demonstrate that LLM-based\ncounterfactual annotations offer a scalable ap-\nproach for addressing coverage limitations in\nhealthcare datasets, enabling safer deployment\nof decision-making policies in clinical settings.\nKeywords:\noff-policy evaluation, synthetic\ndatasets, contextual bandits\nData and Code Availability\nThis paper uses\ndata from the broadly available MIMIC-IV dataset.\nCode is available on Github.\nInstitutional Review Board (IRB)\nThis re-\nsearch does not require IRB approval.\n1. Introduction\nOff-policy evaluation (OPE) methods estimate the\nvalue of a new (target) contextual bandit policy us-\ning a behavior dataset of samples collected under a\ndistinct behavior policy (Sutton and Barto, 2018).\nOPE can be particularly useful in high-stakes do-\nmains such as healthcare, where evaluating policies\nby directly deploying them is either impossible or\nunethical. Standard approaches to OPE include im-\nportance sampling (Precup et al., 2000), the direct\nmethod (Beygelzimer and Langford, 2009), and dou-\nbly robust approaches (Dudik et al., 2014).\nHow-\never, the performance of OPE estimators is inher-\nently limited by the coverage of the behavior dataset.\nWhen the target policy takes actions that are under-\nobserved in the behavior dataset, standard OPE\nmethods cannot reliably estimate the value of these\nactions, leading to inaccurate policy value estimates.\nTo address this, recent work proposes augment-\ning the behavior dataset with counterfactual anno-\n© 2025 A. Mandyam, K. Limaye, B.E. Engelhardt* & E. Alsentzer*.\narXiv:2511.17818v1  [cs.LG]  21 Nov 2025\n"}, {"page": 2, "text": "APRIL\ntations (Tang and Wiens, 2023).\nA counterfactual\nannotation is a prediction of the scalar reward re-\nsulting from an action unobserved in the behavior\ndataset. For example, if a patient received 20mEq of\npotassium, a counterfactual annotation would pre-\ndict the reward had the patient instead received\n40mEq. Two strategies have been developed to in-\ncorporate such annotations into OPE: one augments\nan importance sampling–based estimator (Tang and\nWiens, 2023), and the other augments a doubly ro-\nbust estimator (Mandyam et al., 2024). Both demon-\nstrate that incorporating counterfactual annotations\ncan improve OPE estimates, but these approaches\nrely on human experts (e.g., clinicians) to provide\nthe annotations, which is costly and difficult to scale.\nTo address this, we propose a pipeline to source\ncounterfactual annotations for OPE in clinical set-\ntings using large language models (LLMs).\nLLMs\nhave the ability to reason effectively about medical\ndomains, with the capacity to answer medical ques-\ntions (Singhal et al., 2023b), perform differential pa-\ntient diagnoses (Nori et al., 2025), and reason about\nmedical images (Zhou et al., 2025).\nOur approach\nleverages LLMs to predict clinical features of inter-\nest such as downstream laboratory measurements; we\nthen incorporate these predictions into known reward\nfunctions to produce synthetically generated counter-\nfactual annotations.\nWe evaluate our proposed framework on two clini-\ncal tasks: intravenous (IV) potassium and sodium re-\npletion. Both are critical procedures in clinical prac-\ntice, where large errors in administration can lead\nto adverse outcomes (Voldby and Brandstrup, 2016).\nFurthermore, these are routine procedures with well-\nestablished guidelines for treatment and reasonably\npredictable treatment response curves, making them\nespecially tractable settings for applying contextual\nbandit algorithms. We construct corresponding pa-\ntient datasets from the Medical Information Mart for\nIntensive Care IV (MIMIC-IV) database, which con-\ntains electronic health records (EHR) for patients ad-\nmitted to the Beth Israel Deaconess Medical Cen-\nter (Johnson et al., 2024, 2023; Goldberger et al.,\n2000).\nWe first assess the ability of several LLMs\nto predict relevant clinical features, including serum\npotassium and sodium values. Using clinically moti-\nvated reward functions, we then transform these pre-\ndictions into counterfactual annotations. Our results\nshow that LLM-generated counterfactual annotations\nimprove OPE estimates, particularly under large dis-\ntribution shifts between the behavior and target poli-\ncies.\nOur contributions follow:\n• We perform OPE with LLM-generated\ncounterfactual\nannotations\nin\na\nmulti-\ncohort setting using MIMIC-IV. We system-\natically evaluate multiple general-purpose LLMs\nfor their accuracy in predicting downstream clin-\nical features.\n• We\nshow\nthat\nincorporating\nLLM-\ngenerated\nannotations\ncan\nsignificantly\nimprove OPE estimates, reducing RMSE rel-\native to baselines and confirming prior findings\nin real-world data.\n• We demonstrate that additional counter-\nfactual annotations offer diminishing re-\nturns, a phenomenon captured quantitatively\nvia the marginal entropy over the action distri-\nbution.\n2. Preliminaries\n2.1. Problem setting\nWe adopt a contextual bandit setting, as potas-\nsium and sodium repletion are short-horizon deci-\nsions whose outcomes can be observed within a single\ntimestep. A contextual bandit setting is represented\nas (S, A, R, d0), where S is the discrete context space,\nA is the discrete action space, R is the reward distri-\nbution, and d0 is the initial context distribution. The\nreward function R : S × A →[0, 1] assigns a scalar\nreward between 0 and 1. Our goal is to evaluate a\ntarget contextual bandit policy πe by estimating its\nvalue v(πe) = Es∼d0,a∼πe,) [R(s, a)] using a behavior\ndataset.\nThe behavior dataset consists of samples\nD = {si, ai, ri}N\ni=1, where the actions are sampled\nfrom a behavior policy πb.\n2.2. Off-policy evaluation estimators\nMany OPE estimators fall into three broad cate-\ngories: importance sampling (IS), the direct method\n(DM), and doubly robust (DR) estimators.\nIS\nmethods (Precup et al., 2000) re-weigh each sam-\nple in the behavior dataset using an inverse propen-\nsity score (IPS)\nπe(ai|si)\nπb(ai|si).\nThe second class in-\ncludes direct-method (DM) approaches (Beygelzimer\nand Langford, 2009), which learn a reward model\nˆR from the behavior dataset, and use the model\n2\n"}, {"page": 3, "text": "APRIL\nto simulate the returns of samples from the target\npolicy.\nThe final category includes doubly-robust\n(DR) approaches (Dudik et al., 2014; Jiang and Li,\n2016), which combine strategies from IS and DM ap-\nproaches, providing favorable theoretical guarantees\nwhen either the IPS ratio is known or the reward\nmodel is of high quality.\nRecent work has proposed supplementing the be-\nhavior dataset with counterfactual annotations so-\nlicited from an expert.\nTang and Wiens (2023)\nintroduce an IS-based estimator and demonstrate\nthat counterfactual annotations can improve OPE\nestimates when the annotations are of high qual-\nity. Mandyam et al. (2024) extends this to a dou-\nbly robust setting, mitigating the negative impacts\nof noisy or imperfect annotations. Both approaches\nassume that counterfactual annotations are expert-\nlabeled, which limits the scalability of the proposed\napproaches. Other work has proposed using a varia-\ntional auto-encoder to generate synthetic trajectories,\nthus enriching state–action coverage of the behavior\ndataset and tightening variance bounds (Gao et al.,\n2024). Our work builds on these approaches, identi-\nfying a scalable alternative to creating counterfactual\nannotations.\n2.3. Generative models can encode medical\nknowledge\nLLMs have shown impressive general medical rea-\nsoning capabilities.\nModels fine-tuned on web and\nbiomedical corpora now match or surpass physicians\non multiple-choice benchmarks such as MedQA (Jin\net al., 2020).\nDeepMind’s Med-PaLM 2 (Singhal\net al., 2023a) and Gemini models (Saab et al., 2024)\nillustrate that scaling and instruction tuning can\nboost performance across a range of clinical knowl-\nedge tasks. However, these works center on general\nmedical knowledge questions rather than reasoning\nabout individual patient clinical trajectories, which\nis the focus of our work.\nMore granular, patient-specific LLM applications\nare beginning to emerge, including reasoning about\nhow laboratory values evolve over a patient trajec-\ntory.\nBhasuran et al. (2025) explore differential-\ndiagnosis generation from brief clinical vignettes,\nhighlighting the importance of structured patient\nsummaries to improve LLM outputs. He et al. (2024)\nevaluate the ability to generate accurate and safe re-\nsponses to patient lab-result inquiries using prompt\nengineering and detailed quality evaluation metrics.\nThese studies suggest that LLMs can reason about\npatients when provided with curated input and task\nframing (Wei et al., 2022; Chung et al., 2022). Our\nwork leverages prompting strategies that build on\nthose used in these works to guide LLMs in gener-\nating patient-specific counterfactual annotations.\n2.4. Synthetic data for machine learning\nIn our setting, supervision comes from both a real-\nworld dataset and a noisier set of synthetic data. Us-\ning a noisy secondary dataset is a common paradigm\nin supervised learning, and methods to mitigate\nthe covariance shift between the datasets have been\nextensively studied in the robust machine learn-\ning literature.\nEarlier results introduced transfer\nlearning techniques to learn features from secondary\ndatasets while mitigating issues with higher-variance\nsamples (Krizhevsky et al., 2012; Pan and Yang,\n2010; Ben-David et al., 2006; Sugiyama et al., 2007;\nQui˜nonero-Candela et al., 2008). Other methods such\nas prediction-powered inference (Angelopoulos et al.,\n2023) explicitly correct for possible biases that result\nfrom the introduction of synthetic samples.\n3. Methods\n3.1. Dataset\nWe\nconduct\nour\nanalysis\nusing\nthe\nMIMIC-IV\ndataset, partitioned into two subsets of non-ICU pa-\ntients. The first subset includes all patients who re-\nceived IV potassium, and the second includes all pa-\ntients who received IV hypertonic (3%) saline. For\neach patient in the potassium and sodium subsets, we\nrepresent the clinical context as a feature vector com-\nprising 15 variables that characterize the patient’s\nstate four hours prior to treatment (i.e., administra-\ntion of potassium or saline). We focus on a four-hour\nwindow because this corresponds to the highest fre-\nquency of electrolyte administration observed in our\ndataset, with patients receiving electrolytes at most\nonce every four hours. The features used to repre-\nsent the clinical context include laboratory results,\nvital signs, administered medications, and static co-\nvariates such as age and gender. A complete list of\nfeatures is provided in Section A. The action space\ncorresponds to the administered dosage, represented\nin milliequivalents (mEq). For potassium administra-\ntion, the dosage action space is A = {0, 10, 20, 40}.\nFor sodium (i.e., hypertonic saline) administration,\n3\n"}, {"page": 4, "text": "APRIL\nFigure 1: Our work improves OPE estimates\nusing LLM-generated counterfactual annota-\ntions. We first query counterfactual annotations us-\ning domain knowledge guided prediction. We calcu-\nlate the annotations using a known reward function\nR. Finally, we incorporate the counterfactual anno-\ntations and offline behavior dataset to learn an OPE\nestimate ˆv(πe).\ndosages are discretized to accommodate our assump-\ntion of a discrete action space, yielding an action\nspace A = {0, 100, 200, 300, 400, 500}.\nSimilar to prior work (Prasad, 2020), we adopt a\nreward function defined as a function of the clini-\ncal context observed following the administration of\na treatment dosage. Specifically, the scalar reward\ndepends on a single laboratory measurement in the\nnext observed context. For patients who receive IV\npotassium, this is a serum potassium lab, and for\npatients who receive IV hypertonic saline, this is a\nserum sodium lab. The reward function\nR(x) =\n\n\n\n\n\n\n\n\n\nexp\n\u0010\n−1\n2\n\u0000 x−a\n2.5\n\u00012 \u0011\n,\nx < a\n1,\na ≤x ≤b\nexp\n\u0010\n−1\n2\n\u0000 x−b\n2.5\n\u00012 \u0011\n,\nx > b\ntakes as input a laboratory value x, and uses the\nlower bound (a) and upper bound (b) of the reference\nrange to calculate a scalar reward. This reward func-\ntion design reflects the clinical goal of repletion which\nis to bring a patient’s electrolyte level into the normal\nrange and keep it there, while smoothly penalizing\ndeviations outside the range. Visual representations\nof the reward function can be seen in Appendix Fig-\nure 4.\n3.2. Generating Counterfactual Annotations\nusing LLMs\nAs described in Section 3.1, the reward functions for\neach decision-making task are functions of a single\nlab value. Therefore, generating a counterfactual an-\nnotation requires predicting the specified lab value\nunder a counterfactual treatment dosage.\nWe construct prompts that include information\nabout the patient’s clinical state, a paragraph that\ncites the most relevant features for lab value predic-\ntion sourced from UpToDate (Kluwer, n.d.), and a\nquery about the lab value had an alternative treat-\nment dosage been administered (example in Sec-\ntion B). Building on prior work (Hegselmann et al.,\n2025), we organize the patient’s information into cat-\negories such as comorbidities, laboratory results, and\nmedications to create a structured text representation\nof the clinical state. Including features from UpTo-\nDate guides the LLM toward clinically relevant infor-\nmation, as EHRs often contain extraneous data that\nmay not be predictive. To ensure structured outputs,\nwe restrict the LLM’s response to a JSON object con-\n4\n"}, {"page": 5, "text": "APRIL\ntaining two keys: the predicted lab value, and a jus-\ntification for the prediction. This format allows for\nstraightforward extraction of the numerical lab value\nand facilitates verification of the LLM’s reasoning.\nFor potassium administration, dosages are as-\nsumed to be delivered at a rate of 10, mEq/hr, and\nfor sodium administration, at a rate of 30, mEq/hr,\ncorresponding to the most common rates observed in\nMIMIC-IV. The prompt also specifies that the lab\nvalue should be predicted three hours after the IV\ninfusion concludes; this corresponds to the average\nnumber of hours that the lab value post treatment\nadministration was measured.\nOnce the LLM pre-\ndicts the lab value, it is converted into a scalar coun-\nterfactual annotation using the corresponding known\nreward function.\n3.3. Incorporating Counterfactual\nAnnotations into an OPE estimator\nOnce we generate counterfactual annotations, we\nmust incorporate them into an OPE estimator. Prior\nmethods for OPE with counterfactual annotations of-\nten assume that the IPS ratios are fully known. How-\never, in this work, we must infer both πb and πe from\nfinite sample sizes. To mitigate possible biases as a\nresult of unknown IPS ratios, we choose to use a di-\nrect method estimator. The standard direct method\nestimator is\nˆV DM =\nX\ns∈S\nd0(s)\nX\na∈A\nπ(a|s) ˆR(s, a),\nwhere ˆR is a reward function estimate learned from\nthe behavior dataset. When we have access to both a\nbehavior dataset and counterfactual annotations, we\nchoose to use modified version of the standard DM\nestimator suggested by prior work work (Mandyam\net al., 2024),\nˆV DM + =\nX\ns∈S\nd0(s)\nX\na∈A\nπ(a|s) ˆR+(s, a),\nwhere ˆR+ is learned using both the behavior dataset\nand counterfactual annotations. In this work, we ap-\nproximate both ˆR and ˆR+ using linear regression.\n3.4. Evaluation Setup\nA standard metric for assessing the accuracy of\nan OPE estimator is the root mean squared error\n(RMSE), defined as\nRMSE =\np\nE[(ˆv(πe) −v(πe))2],\nwhere ˆv(πe) denotes the value estimated by the OPE\nmethod, and v(πe) is the true value of the target pol-\nicy πe. In practice, v(πe) is rarely available, which\ncomplicates the evaluation of OPE estimators in real-\nworld settings.\nTo address this limitation in the MIMIC-IV\ndataset, we adopt a controlled evaluation strategy.\nWe partition each dataset subset into disjoint behav-\nior and target sub-cohorts, and infer corresponding\npolicies via behavior cloning. Because the target sub-\ncohort contains observed rewards, we approximate\nthe value of the cloned target policy by averaging\nthese rewards. The fidelity of this approximation de-\npends on how well the policies are cloned; to assess\nthis, we evaluate the cloned policies’ accuracy on a\nheld-out validation set and find that they perform\nwell in reproducing the observed treatment decisions\n(e.g., validation accuracy ¿ 90%). This gives us confi-\ndence that the averaged rewards in the target subset\nprovide a reliable reference value against which to\ncompute RMSE for different OPE estimators.\nIt is well known that the performance of OPE esti-\nmators depends considerably on the distribution shift\nbetween the behavior dataset and the samples in-\nduced by the target policy. To systematically study\nthe effect of LLM-generated counterfactual annota-\ntions on an OPE method under varying degrees of\ndistribution shift, we construct three behavior–target\ndataset pairs for each subset of patients from MIMIC-\nIV. The first pair splits by gender, with female pa-\ntients forming the behavior dataset and male patients\nthe target dataset.\nThe second pair splits by co-\nmorbidity status: for potassium repletion, patients\nwithout renal disease form the behavior dataset and\npatients with renal disease the target dataset; for\nsodium repletion, the split is based on cirrhosis. We\nchoose these comorbidities because their presence is\nlikely to influence the patient’s response to drug ad-\nministration.\nThe third pair separates patients by\ndrug dosage, using low-dosage patients as the be-\nhavior dataset and high-dosage patients as the tar-\nget dataset. These partitions are designed to reflect\nclinically meaningful subgroups while also inducing\nprogressively larger divergences between the behav-\nior and target policies.\nThis allows us to evaluate\nwhen counterfactual annotations generated by LLMs\nyield improvements in OPE accuracy.\n5\n"}, {"page": 6, "text": "APRIL\n4. Experiments\nOur empirical analyses seek to answer the following\nquestions: (1) Can LLMs accurately predict down-\nstream patient laboratory values after a treatment is\nadministered? (2) Under what conditions do LLM-\ngenerated counterfactual annotations improve OPE\nestimates? (3) How do OPE estimates vary as the\nnumber of synthetic counterfactual annotations in-\ncreases?\nTo address these questions, we use five LLMs\nspanning a range of parameter counts:\nOpenAI’s\no1 (OpenAI et al., 2024), o3-mini (Zhang et al.), and\ngpt-4o-mini (Hurst et al., 2024), Google’s Gemini\n1.5 (Reid et al., 2024), and Anthropic’s Claude 3.7\nSonnet (Cla). All models are hosted on an internal,\nsandboxed cluster to ensure HIPAA compliance with\nthe MIMIC-IV dataset. All experiments were con-\nducted with a temperature setting of zero whenever\nsupported. For o1 and o3-mini, which do not expose\na temperature parameter, we use the default config-\nuration.\n4.1. LLMs can predict downstream lab\nvalues on real patient populations\nWe first evaluate whether LLMs can accurately pre-\ndict serum potassium and serum sodium laboratory\nvalues.\nIn realistic deployment, the target patient\npopulation may not be directly accessible, so we as-\nsess predictive performance using behavior datasets\nfrom each sub-cohort split. To generate predictions,\nwe prompt the LLM following the procedure in Sec-\ntion 3.2, but instead of asking for counterfactual lab\nvalues, we request the lab value following the dosage\nadministered in MIMIC-IV. Because the correspond-\ning ground-truth lab values are observed in MIMIC-\nIV, we can directly quantify predictive accuracy. We\nevaluate accuracy using a weighted F1 score across\nclinically relevant categories of lab values (e.g., below\nreference range, within reference range, above refer-\nence range). The categories used to calculate the F1\nscore, and further details are reported in Section 3.1.\nWe find that LLMs can predict serum potassium\nand serum sodium lab values with clinically mean-\ningful degrees of accuracy (Table 1, visualized in Ap-\npendix Figure 6). First, we note that serum potas-\nsium lab values are predicted more accurately than\nserum sodium lab values, likely due to the wider dis-\ntribution and higher prevalence of outliers in sodium\nlab measurements.\nWe also find that the perfor-\nmance of a given LLM remains consistent across co-\nhorts within each prediction task, which suggests that\npredictive accuracy does not strongly depend on the\nunderlying patient population.\nFinally, the differ-\nences in predictive accuracy across LLMs are modest,\nsuggesting that multiple models are capable of pro-\nducing reliable predictions of downstream lab values.\nFurthermore, these results demonstrate our proposed\nframework’s ability to produce counterfactual anno-\ntations of reasonable quality. In particular, because\nLLMs can predict downstream lab values within a\ndegree of accuracy that is clinically relevant, the re-\nsulting annotations are likely to be useful for OPE.\n4.2. LLM-produced counterfactual\nannotations can improve OPE estimates\nWe next evaluate the utility of LLM-generated coun-\nterfactual annotations for OPE, following the setup\nin Section 3.4.\nWe report results for the potas-\nsium repletion task in Figure 2, and for the sodium\nrepletion task in Appendix Figure 7.\nOur results\nshow that counterfactual annotations substantially\nimprove OPE estimates in settings with large dis-\ntribution shifts between the actions observed in the\nbehavior and target policies. Across both the potas-\nsium and sodium repletion tasks, the reported RMSE\nreflects the relative difficulty of estimating v(πe) un-\nder each cohort split.\nFor example, in the gender\ncohort split, where behavior and target policies are\nnearly identical, the RMSE is already near zero with-\nout counterfactual annotations, leaving little room for\nimprovement. In contrast, in the dosage cohort split,\nwhere behavior and target policies have little over-\nlap, the baseline RMSE of DM is highest, reflecting\nthe difficulty of the task. Here, the incorporation of\ncounterfactual annotations produces the largest re-\nductions in RMSE, indicating that annotations are\nmost valuable when behavior and target policies di-\nverge strongly. Specifically, in the potassium dosage\ncohort, counterfactual annotations can reduce RMSE\nby 83%, and in the sodium dosage cohort by 49%.\nWe also find that the performance of DM + varies\nwith the choice of LLM used to generate coun-\nterfactual annotations.\nIn the potassium repletion\ntask, annotations from o1 yield the best performance\nas shown by lowest RMSE, whereas in the sodium\nrepletion task, annotations from gpt-4o-mini and\no3-mini yield the best performance. Although the\nbest-performing LLM is not consistent across tasks or\ncohort splits, counterfactual annotations consistently\n6\n"}, {"page": 7, "text": "APRIL\nTask\nCohort\no1\ngpt-4o-mini\no3-mini\nGemini\nClaude 3.7\nPotassium Repletion\nGender\n0.856\n0.809\n0.854\n0.858\n0.866\nComorbidity\n0.871\n0.787\n0.869\n0.872\n0.879\nDosage\n0.878\n0.791\n0.876\n0.879\n0.885\nSodium Repletion\nGender\n0.758\n0.774\n0.738\n0.749\n0.776\nComorbidity\n0.771\n0.801\n0.753\n0.779\n0.796\nDosage\n0.772\n0.809\n0.768\n0.780\n0.804\nTable 1: All LLMs perform comparably across potassium and sodium lab prediction. Predictions\nare evaluated using weighted F1 scores across clinically relevant lab value categories. The best performing\nLLM within each cohort is in bold.\nFigure 2: LLM-generated counterfactual annotations improve OPE estimates in settings with\nhigh divergence between actions observed in behavior and target policies. We report results for the\npotassium repletion task. Our baseline is a direct method estimator (blue) that does not use counterfactual\nannotations. The performance of DM + with annotations from each LLM is reported in the corresponding\ncolors. Error bars represent standard error across 500 bootstrapped datasets sampled with replacement.\nSince RMSE is non-negative, the lower bound of the error bars is truncated at 0 where necessary. Figures\nabove each plot demonstrate the difference in distribution of actions observed in the behavior and target\npolicies.\nreduce RMSE in the most challenging settings (e.g.,\ndosage cohorts), regardless of the LLM used.\nFinally, to assess statistical significance, we com-\npare DM and DM + using a paired t-test, with DM +\nlearned using 500 counterfactual annotations (Sec-\ntion 4.2). In nearly all settings, DM + achieves signifi-\ncantly lower RMSE than DM. The main exception is\nthe gender split in both potassium and sodium tasks,\nwhere annotations from some LLMs do not yield a\nmeaningful performance improvement. This outcome\nis expected, given the substantial overlap between\nbehavior and target policies in the gender cohorts,\nwhich allows DM to perform well even without coun-\nterfactual annotations.\n4.3. Additional counterfactual annotations\nyield marginal improvements in OPE\nestimates\nA key consideration when using synthetic data in\nmachine learning is determining the point at which\nadding further synthetic samples no longer provides\nbenefits. In our setting, a single source of counterfac-\ntual annotations can generate at most N · (|A| −1)\nunique annotations, where |A| is the number of ac-\ntions and N is the number of samples in the behavior\ndataset. When multiple sources are available, each\nsource provides separate predictions for unobserved\nactions, which can either be combined or averaged.\nDirect combination increases the total number of an-\n7\n"}, {"page": 8, "text": "APRIL\nTask\nCohort\no1\ngpt-4o-mini\no3-mini\nGemini\nClaude 3.7\nPotassium Repletion\nGender\n5.4E-31\n9.8E-03\n3.0E-04\n3.7E-01\n2.4E-01\nComorbidity\n1.5E-94\n1.3E-08\n1.1E-07\n7.7E-03\n5.7E-04\nDosage\n1.3E-83\n1.7E-14\n1.9E-20\n7.2E-11\n1.4E-08\nSodium Repletion\nGender\n1.9E-03\n7.0E-04\n1.5E-19\n7.5E-14\n6.6E-08\nComorbidity\n1.0E-07\n2.0E-16\n2.0E-03\n1.0E-04\n2.2E-05\nDosage\n2.8E-37\n3.0E-15\n3.2E-57\n1.44E-44\n7.79E-39\nTable 2: In most cohorts across both tasks, LLM-generated annotations significantly improve\nRMSE. We compare RMSE distributions for DM and DM+ with 500 counterfactual annotations using a\npaired t-test, and report p-values. P-values shown in red indicate results that are not statistically significant\n(p ≥0.05) or cases where RMSE does not improve relative to DM (t < 0).\nFigure 3: Combining annotation sources yields\nlimited returns. (Top) We compare DM to DM +\nwith annotations from the best-performing LLMs for\npotassium repletion in the dosage cohort, with two\naggregation methods: pooling predictions and aver-\naging annotations. Error bars show standard error\nover 500 bootstrapped datasets, truncated at 0. (Bot-\ntom) Marginal entropy over the action space H(A)\nwhen adding counterfactual annotations to the be-\nhavior dataset for the potassium cohort. The dashed\nline marks the maximum possible entropy.\nnotations, whereas averaging maintains the same to-\ntal count. We study both strategies for the potassium\ntask (Figure 3) and sodium task ( Figure 9).\nWe focus on the dosage cohort splits for both tasks,\nwhere counterfactual annotations have the greatest\nimpact in reducing RMSE due to minimal overlap\nbetween the behavior and target policies.\nSpecif-\nically, we examine combinations of the two LLMs\nwhose counterfactual annotations yield the best per-\nformance for DM +: o1 and o3-mini for the potas-\nsium task, and Gemini and o3-mini for the sodium\ntask. We find that, while adding counterfactual an-\nnotations initially reduces OPE error, the improve-\nment quickly plateaus as more annotations are in-\ncluded. Averaging multiple sources does not provide\nadditional gains beyond the best-performing single\nsource. For instance, in the potassium task, averag-\ning annotations from o1 and o3-mini yields OPE per-\nformance worse than using o1 alone, though slightly\nbetter than o3-mini alone. Similarly, combining an-\nnotations without averaging, which nearly doubles\nthe number of annotations, does not improve OPE\nestimates relative to a single source. These results\nindicate that substantially increasing the number of\ncounterfactual annotations provides limited utility.\nTo quantify the effect of additional annotations,\nwe compute the marginal entropy over the action\ndistribution.\nEntropy measures the overall uncer-\ntainty or spread of actions in the dataset.\nFor-\nmally, the marginal entropy over the action distri-\nbution is H(A) = −P\na∈A ˆp(a)ln(ˆp(a)) where ˆp(a)\nis the probability of observing a given action a, es-\ntimated empirically. The maximum entropy occurs\nwhen all actions are equally frequent, in which case\nH(A) = ln(|A|). We observe that, as the number of\nannotations increases, the action coverage approaches\nthe maximum entropy, and further annotations yield\n8\n"}, {"page": 9, "text": "APRIL\nonly marginal gains. In particular, at around 700 an-\nnotations for the potassium task and 500 annotations\nfor the sodium task, OPE improvements have largely\nplateaued, and the marginal action entropy is already\nnear its maximum, indicating that additional coun-\nterfactual annotations provide little further utility.\nThis analysis suggests that marginal entropy over the\naction space is a proxy that may be used to determine\nwhen to stop generating counterfactual annotations.\n5. Discussion\nIn this work, we present a scalable strategy for gen-\nerating counterfactual annotations for OPE in clin-\nical settings. We show that LLMs can reason over\nclinical contexts and predict downstream lab values,\nwhich in turn can be used to construct counterfactual\nannotations. Focusing on the potassium and sodium\nrepletion tasks, we demonstrate that this approach\nleads to substantial improvements in OPE estimates,\nparticularly when there is considerable divergence be-\ntween the behavior and target policies. We recom-\nmend using LLM-generated annotations when there\nare known coverage gaps in the behavior dataset, and\nrelying on an entropy-based metric to decide when\nadditional counterfactual annotations are needed.\nLimitations and Future Work. Our study is\nlimited to reward functions that consider a single\nclinical feature. While our results provide evidence\nthat LLMs can reliably predict these downstream lab\nvalues, future work should evaluate whether similar\ngains can be achieved for predicting more complex\nclinical outcomes.\nAcknowledgments\nAM was funded in part by a Stanford Data Science\nFellowship. BEE was funded in part by grants from\nthe Parker Institute for Cancer Immunology (PICI),\nthe Chan-Zuckerberg Institute (CZI), the Biswas\nFamily Foundation, NIH NHGRI R01 HG012967,\nand NIH NHGRI R01 HG013736. BEE is a CIFAR\nFellow in the Multiscale Human Program. The au-\nthors would like to thank Dr. Chloe Stanwyck for\nsupport with designing and evaluating empirical pro-\ncedures.\nReferences\nClaude 3.7 sonnet system card. URL https://api.\nsemanticscholar.org/CorpusID:276612236.\nAnastasios N. Angelopoulos, Stephen Bates, Clara\nFannjiang, Michael I. Jordan, and Tijana Zrnic.\nPrediction-powered inference, 2023. URL https:\n//arxiv.org/abs/2301.09633.\nShai Ben-David, John Blitzer, Koby Crammer, and\nFernando Pereira. Analysis of representations for\ndomain adaptation. volume 19, pages 137–144, 01\n2006.\nAlina Beygelzimer and John Langford.\nThe offset\ntree for learning with partial labels. Proceedings of\nthe 15th ACM SIGKDD international conference\non Knowledge discovery and data mining, pages\n129–138, 2009.\nBalu Bhasuran, Qiao Jin, Yuzhang Xie, Carl Yang,\nKarim Hanna, Jennifer Costa, Cindy Shavor, Wen-\nshan Han, Zhiyong Lu, and Zhe He. Preliminary\nanalysis of the impact of lab results on large lan-\nguage model generated differential diagnoses. npj\nDigital Medicine, 8(1):166, 2025.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Xin Huang, Andrew\nDai, Xuezhi Wang, Brian Lester, and et al. Scal-\ning instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\nMiroslav Dudik, John Langford, and Lihong Li. Dou-\nbly robust policy evaluation and learning. In Pro-\nceedings of the 31st International Conference on\nMachine Learning, pages 1097–1105, 2014.\nGe Gao, Qitong Gao, Xi Yang, Song Ju, Miroslav Pa-\njic, and Min Chi. On trajectory augmentations for\noff-policy evaluation. In The Twelfth International\nConference on Learning Representations, 2024.\nAry L. Goldberger, Luis A. N. Amaral, Leon Glass,\nJeffrey M. Hausdorff, Plamen Ch. Ivanov, Roger G.\nMark, Joseph E. Mietus, George B. Moody, Chung-\nKang Peng, and H. Eugene Stanley. Physiobank,\nphysiotoolkit, and physionet:\nComponents of a\nnew research resource for complex physiologic sig-\nnals. Circulation, 101(23):e215–e220, 2000. ISSN\n0009-7322. Online.\nZhe He, Balu Bhasuran, Qiao Jin, Shubo Tian, Karim\nHanna, Cindy Shavor, Lisbeth Garcia Arguello,\n9\n"}, {"page": 10, "text": "APRIL\nPatrick Murray, and Zhiyong Lu. Quality of an-\nswers of generative large language models vs peer\npatients for interpreting lab test results for lay pa-\ntients: Evaluation study. ArXiv, pages arXiv–2402,\n2024.\nStefan Hegselmann, Georg von Arnim, Tillmann\nRheude, Noel Kronenberg, David Sontag, Gerhard\nHindricks, Roland Eils, and Benjamin Wild. Large\nlanguage models are powerful electronic health\nrecord encoders, 2025. URL https://arxiv.org/\nabs/2502.17403.\nOpenAI\nAaron\nHurst,\nAdam\nLerer,\nAdam\nP.\nGoucher, Adam Perelman, Aditya Ramesh, Aidan\nClark, AJ Ostrow, Akila Welihinda, Alan Hayes,\nAlec Radford, Aleksander Mkadry, Alex Baker-\nWhitcomb, Alex Beutel, Alex Borzunov, Alex Car-\nney, Alex Chow, Alexander Kirillov, Alex Nichol,\nAlex\nPaino,\nAlex\nRenzin,\nAlexandre\nPassos,\nAlexander Kirillov, Alexi Christakis, Alexis Con-\nneau, Ali Kamali, Allan Jabri, Allison Moyer, Al-\nlison Tam, Amadou Crookes, Amin Tootoochian,\nAmin Tootoonchian, Ananya Kumar, Andrea Val-\nlone, Andrej Karpathy, Andrew Braunstein, An-\ndrew Cann, Andrew Codispoti, Andrew Galu,\nAndrew Kondrich,\nAndrew Tulloch,\nAn drey\nMishchenko, Angela Baek, Angela Jiang, An toine\nPelisse, Antonia Woodford, Anuj Gosalia, Arka\nDhar,\nAshley Pantuliano,\nAvi Nayak,\nAvital\nOliver, Barret Zoph, B. Ghorbani, Ben Leim-\nberger, Ben Rossen, Benjamin Sokolowsky, Ben\nWang, Benjamin Zweig, Beth Hoover, Blake Samic,\nBob McGrew, Bobby Spero, Bogo Giertler, Bowen\nCheng, Brad Lightcap, Brandon Walkin, Bren-\ndan Quinn, Brian Guarraci, Brian Hsu, Bright\nKellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll L. Wainwright, Cary Bassin, Cary Hudson,\nCasey Chu, Chad Nelson, Chak Li, Chan Jun Sh-\nern, Channing Conger, Charlotte Barette, Chelsea\nVoss, Chen Ding, Cheng Lu, Chong Zhang, Chris\nBeaumont, Chris Hallacy, Chris Koch, Christian\nGibson, Christina Kim, Christine Choi, Chris-\ntine McLeavey,\nChris Hesse,\nClaudia Fischer,\nClemens Winter, Coley Czarnecki, Colin Jarvis,\nColin Wei, Constantin Koumouzelis, Dane Sher-\nburn, Daniel Kappler, Daniel Levin, Daniel Levy,\nDavid Carr, David Farhi, David M´ely, David\nRobinson, David Sasaki, Denny Jin, Dev Val-\nladares, Dimitris Tsipras, Doug Li, Phong Duc\nNguyen, Duncan Findlay, Edede Oiwoh, Edmund\nWong, Ehsan Asdar, Elizabeth Proehl, Elizabeth\nYang, Eric Antonow, Eric Kramer, Eric Peterson,\nEric Sigler, Eric Wallace, Eugene Brevdo, Evan\nMays, Farzad Khorasani, Felipe Petroski Such, Fil-\nippo Raso, Francis Zhang, Fred von Lohmann,\nFreddie Sulit, Gabriel Goh, Gene Oden, Geoff\nSalmon, Giulio Starace, Greg Brockman, Hadi\nSalman, Hai-Biao Bao, Haitang Hu, Hannah Wong,\nHaoyu Wang, Heather Schmidt, Heather Whitney,\nHee woo Jun, Hendrik Kirchner, Henrique Pond´e\nde Oliveira Pinto, Hongyu Ren, Huiwen Chang,\nHyung Won Chung, Ian Kivlichan, Ian O’Connell,\nIan Osband, Ian Silber, Ian Sohl, ˙Ibrahim Ci-\nhangir Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya\nSutskever, Ingmar Kanitscheider, Ishaan Gulra-\njani, Jacob Coxon, Jacob Menick, Jakub W. Pa-\nchocki, James Aung, James Betker, James Crooks,\nJames Lennon, Jamie Ryan Kiros, Jan Leike, Jane\nPark, Jason Kwon, Jason Phang, Jason Teplitz,\nJason Wei, Jason Wolfe, Jay Chen, Jeff Harris,\nJenia Varavva, Jessica Gan Lee, Jessica Shieh,\nJi Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi\nYu, Joanne Jang, Joaquin Qui˜nonero Candela,\nJoe Beutler, Joe Landers, Joel Parish, Johannes\nHeidecke, John Schulman, Jonathan Lachman,\nJonathan McKay,\nJonathan Uesato,\nJonathan\nWard, Jong Wook Kim, Joost Huizinga, Jor-\ndan Sitkin, Jos Kraaijeveld, Joshua Gross, Josh\nKaplan, Josh Snyder, Joshua Achiam, Joy Jiao,\nJoyce Lee, Juntang Zhuang, Justyn Harriman,\nKai Fricke, Kai Hayashi, Karan Singhal, Katy\nShi, Kavin Karthik, Kayla Wood, Kendra Rim-\nbach, Kenny Hsu, Kenny Nguyen, Keren Gu-\nLemberg, Kevin Button, Kevin Liu, Kiel Howe,\nKrithika Muthukumar, Kyle Luther, Lama Ah-\nmad, Larry Kai, Lauren Itow, Lauren Workman,\nLeher Pathak, Leo Chen, Li Jing, Lia Guy, Liam\nFedus, Liang Zhou, Lien Mamitsuka, Lilian Weng,\nLindsay McCallum, Lindsey Held, Ouyang Long,\nLouis Feuvrier, Lu Zhang, Lukasz Kondraciuk,\nLukasz Kaiser, Luke Hewitt, Luke Metz, Lyric\nDoshi, Mada Aflak, Maddie Simens, Made laine\nBoyd, Madeleine Thompson, Marat Dukhan, Mark\nChen, Mark Gray, Mark Hudnall, Marvin Zhang,\nMarwan Aljubeh,\nMa teusz Litwin,\nMatthew\nZeng, Max Johnson, Maya Shetty, Mayank Gupta,\nMeghan Shah,\nMehmet Ali Yatbaz,\nMengxue\nYang,\nMengchao Zhong,\nMia Glaese,\nMianna\nChen, Michael Janner, Michael Lampe, Michael\nPetrov,\nMichael Wu,\nMichele Wang,\nMichelle\nFradin, Michelle Pokrass, Miguel Castro, Miguel\nCastro, Mikhail Pavlov, Miles Brundage, Miles\n10\n"}, {"page": 11, "text": "APRIL\nWang, Mina Khan, Mira Murati, Mo Bavarian,\nMolly Lin, Murat Yesildal, Nacho Soto, Natalia\nGimelshein, Na talie Cone, Natalie Staudacher,\nNatalie Summers, Natan LaFontaine, Neil Chowd-\nhury, Nick Ryder, Nick Stathas, Nick Turley, Niko-\nlas A. Tezak, Niko Felix, Nithanth Kudige, Ni-\ntish Shirish Keskar, Noah Deutsch, Noel Bundick,\nNora Puckett, Ofir Nachum, Ola Okelola, Oleg\nBoiko, Oleg Murk, Oliver Jaffe, Olivia Watkins,\nOlivier Godement, Owen Campbell-Moore, Patrick\nChao, Paul McMillan, Pavel Belov, Peng Su, Peter\nBak, Peter Bakkum, Peter Deng, Peter Dolan, Pe-\nter Hoeschele, Peter Welinder, Phil Tillet, Philip\nPronin, Phil Tillet, Prafulla Dhariwal, Qim ing\nYuan, Rachel Dias, Rachel Lim, Rahul Arora, Ra-\njan Troll, Randall Lin, Raphael Gontijo Lopes,\nRaul Puri, Reah Miyara, Reimar H. Leike, Re-\nnaud Gaubert, Reza Zamani, Ricky Wang, Rob\nDonnelly, Rob Honsby, Rocky Smith, Rohan Sa-\nhai, Rohit Ramchandani, Romain Huet, Rory\nCarmichael,\nRowan Zellers,\nRoy Chen,\nRuby\nChen, Ruslan Ramilevich Nigmatullin, Ryan Cheu,\nSaachi Jain, Sam Altman, Sam Schoenholz, Sam\nToizer, Samuel Miserendino, Sandhini Agarwal,\nSara Culver, Scott Ethersmith, Scott Gray, Sean\nGrove, Sean Metzger, Shamez Hermani, Shan-\ntanu Jain, Shengjia Zhao, Sherwin Wu, Shino\nJomoto, Shirong Wu, Shuaiqi Xia, Sonia Phene,\nSpencer Papay, Srinivas Narayanan, Steve Cof-\nfey,\nSteve\nLee,\nStewart\nHall,\nSuchir\nBalaji,\nTal Broda, Tal Stramer, Tao Xu, Tarun Gogi-\nneni, Taya Christianson, Ted Sanders, Tejal Pat-\nwardhan, Thomas Cunninghman, Thomas Degry,\nThomas Dimson, Thomas Raoux, Thomas Shad-\nwell, Tianhao Zheng, Todd Underwood, Todor\nMarkov, Toki Sherbakov, Tom Rubin, Tom Stasi,\nTomer Kaftan,\nTristan Heywood,\nTroy Peter-\nson, Tyce Walters, Tyna Eloundou, Valerie Qi,\nVeit Moeller, Vinnie Monaco, Vishal Kuo, Vlad\nFomenko, Wayne Chang, Weiyi Zheng, Wenda\nZhou, Wesam Manassra, Will Sheu, Wojciech\nZaremba, Yash Patil, Yilei Qian, Yongjik Kim,\nYoulong Cheng, Yu Zhang, Yuchen He, Yuchen\nZhang, Yujia Jin, Yunxing Dai, and Yury Malkov.\nGpt-4o system card.\nArXiv,\nabs/2410.21276,\n2024. URL https://api.semanticscholar.org/\nCorpusID:273662196.\nNan Jiang and Lihong Li. Doubly robust off-policy\nvalue evaluation for reinforcement learning. In Pro-\nceedings of the 33rd International Conference on\nMachine Learning, pages 652–661, 2016.\nDi Jin, Yansong Pan, Wenqiang Ouyang, and Caim-\ning Xiong.\nWhat disease does this patient\nhave? a large-scale open domain question answer-\ning dataset from medical exams.\narXiv preprint\narXiv:2009.13081, 2020.\nAlistair E. W. Johnson, Luca Bulgarelli, Li-wei Shen,\nand et al. MIMIC-IV, a freely accessible electronic\nhealth record dataset.\nScientific Data, 10(1):1,\n2023. doi: 10.1038/s41597-022-01899-x.\nAlistair E. W. Johnson, Luca Bulgarelli, Tom J.\nPollard, Benjamin Gow, Benjamin Moody, Steven\nHorng, Leo A. Celi, and Roger Mark. MIMIC-IV\n(version 3.1). https://physionet.org/content/\nmimiciv/3.1/, 2024.\nPhysioNet. https://doi.\norg/10.13026/kpb9-mt58.\nWolters Kluwer. Uptodate, n.d. URL https://www.\nuptodate.com.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E.\nHinton.\nImagenet classification with deep con-\nvolutional neural networks.\nCommunications of\nthe ACM, 60:84 – 90, 2012. URL https://api.\nsemanticscholar.org/CorpusID:195908774.\nAishwarya Mandyam, Shengpu Tang, Jiayu Yao,\nJenna Wiens, and Barbara E. Engelhardt. Candor:\nCounterfactual annotated doubly robust off-policy\nevaluation, 2024. URL https://arxiv.org/abs/\n2412.08052.\nHarsha Nori, Mayank Daswani, Christopher Kelly,\nScott\nLundberg,\nMarco\nTulio\nRibeiro,\nMarc\nWilson,\nXiaoxuan\nLiu,\nViknesh\nSounderajah,\nJonathan Carlson,\nMatthew P Lungren,\nBay\nGross, Peter Hames, Mustafa Suleyman, Dominic\nKing, and Eric Horvitz. Sequential diagnosis with\nlanguage models, 2025. URL https://arxiv.org/\nabs/2506.22405.\nOpenAI,\n:,\nAaron\nJaech,\nAdam\nKalai,\nAdam\nLerer, Adam Richardson, Ahmed El-Kishky, Aiden\nLow, Alec Helyar, Aleksander Madry, Alex Beu-\ntel, Alex Carney, Alex Iftimie, Alex Karpenko,\nAlex Tachard Passos, Alexander Neitz, Alexan-\nder Prokofiev, Alexander Wei, Allison Tam, Ally\nBennett, Ananya Kumar, Andre Saraiva, Andrea\nVallone, Andrew Duberstein, Andrew Kondrich,\nAndrey Mishchenko, Andy Applebaum, Angela\n11\n"}, {"page": 12, "text": "APRIL\nJiang, Ashvin Nair, Barret Zoph, Behrooz Ghor-\nbani, Ben Rossen, Benjamin Sokolowsky, Boaz\nBarak, Bob McGrew, Borys Minaiev, Botao Hao,\nBowen Baker, Brandon Houghton, Brandon McK-\ninzie, Brydon Eastman, Camillo Lugaresi, Cary\nBassin, Cary Hudson, Chak Ming Li, Charles\nde Bourcy,\nChelsea Voss,\nChen Shen,\nChong\nZhang,\nChris Koch,\nChris Orsinger,\nChristo-\npher Hesse, Claudia Fischer, Clive Chan, Dan\nRoberts, Daniel Kappler, Daniel Levy, Daniel Sel-\nsam, David Dohan, David Farhi, David Mely,\nDavid Robinson, Dimitris Tsipras, Doug Li, Dra-\ngos Oprica, Eben Freeman, Eddie Zhang, Ed-\nmund Wong, Elizabeth Proehl, Enoch Cheung,\nEric Mitchell, Eric Wallace, Erik Ritter, Evan\nMays, Fan Wang, Felipe Petroski Such, Filippo\nRaso, Florencia Leoni, Foivos Tsimpourlas, Fran-\ncis Song, Fred von Lohmann, Freddie Sulit, Ge-\noff Salmon, Giambattista Parascandolo, Gildas\nChabot, Grace Zhao, Greg Brockman, Guillaume\nLeclerc, Hadi Salman, Haiming Bao, Hao Sheng,\nHart Andrin,\nHessam Bagherinezhad,\nHongyu\nRen, Hunter Lightman, Hyung Won Chung, Ian\nKivlichan, Ian O’Connell, Ian Osband, Ignasi Clav-\nera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya\nSutskever, Irina Kofman, Jakub Pachocki, James\nLennon, Jason Wei, Jean Harb, Jerry Twore, Ji-\nacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi\nYu, Joaquin Qui˜nonero Candela, Joe Palermo,\nJoel Parish, Johannes Heidecke, John Hallman,\nJohn Rizzo, Jonathan Gordon, Jonathan Uesato,\nJonathan Ward, Joost Huizinga, Julie Wang, Kai\nChen, Kai Xiao, Karan Singhal, Karina Nguyen,\nKarl Cobbe, Katy Shi, Kayla Wood, Kendra Rim-\nbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu,\nKevin Stone, Kevin Yu, Lama Ahmad, Lauren\nYang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fe-\ndus, Lilian Weng, Linden Li, Lindsay McCallum,\nLindsey Held, Lorenz Kuhn, Lukas Kondraciuk,\nLukasz Kaiser, Luke Metz, Madelaine Boyd, Maja\nTrebacz, Manas Joglekar, Mark Chen, Marko Tin-\ntor, Mason Meyer, Matt Jones, Matt Kaufer,\nMax Schwarzer,\nMeghan Shah,\nMehmet Yat-\nbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan\nYan, Mia Glaese, Mianna Chen, Michael Lampe,\nMichael Malek, Michele Wang, Michelle Fradin,\nMike McClay, Mikhail Pavlov, Miles Wang, Mingx-\nuan Wang, Mira Murati, Mo Bavarian, Mostafa\nRohaninejad, Nat McAleese, Neil Chowdhury, Neil\nChowdhury, Nick Ryder, Nikolas Tezak, Noam\nBrown, Ofir Nachum, Oleg Boiko, Oleg Murk,\nOlivia Watkins, Patrick Chao, Paul Ashbourne,\nPavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul\nArora, Randall Lin, Rapha Gontijo Lopes, Raz\nGaon, Reah Miyara, Reimar Leike, Renny Hwang,\nRhythm Garg, Robin Brown, Roshan James, Rui\nShu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam\nAltman, Sam Toizer, Sam Toyer, Samuel Mis-\nerendino, Sandhini Agarwal, Santiago Hernan-\ndez, Sasha Baker, Scott McKinney, Scottie Yan,\nShengjia Zhao, Shengli Hu, Shibani Santurkar,\nShraman Ray Chaudhuri, Shuyuan Zhang, Siyuan\nFu, Spencer Papay, Steph Lin, Suchir Balaji, Su-\nvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan\nClark, Tao Wang, Taylor Gordon, Ted Sanders,\nTejal Patwardhan, Thibault Sottiaux, Thomas\nDegry, Thomas Dimson, Tianhao Zheng, Timur\nGaripov, Tom Stasi, Trapit Bansal, Trevor Creech,\nTroy Peterson, Tyna Eloundou, Valerie Qi, Vi-\nneet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad\nFomenko, Weiyi Zheng, Wenda Zhou, Wes Mc-\nCabe, Wojciech Zaremba, Yann Dubois, Yinghai\nLu, Yining Chen, Young Cha, Yu Bai, Yuchen He,\nYuchen Zhang, Yunyun Wang, Zheng Shao, and\nZhuohan Li. Openai o1 system card, 2024. URL\nhttps://arxiv.org/abs/2412.16720.\nSinno Jialin Pan and Qiang Yang. A survey on trans-\nfer learning. IEEE Transactions on Knowledge and\nData Engineering, 22(10):1345–1359, 2010.\ndoi:\n10.1109/TKDE.2009.191.\nNiranjani Prasad. Methods for reinforcement learning\nin clinical decision support. PhD thesis, Princeton\nUniversity, 2020.\nDoina Precup, Richard Sutton, and Satinder Singh.\nEligibility traces for off-policy policy evaluation.\nComputer Science Department Faculty Publication\nSeries, 06 2000.\nJoaquin\nQui˜nonero-Candela,\nMasashi\nSugiyama,\nAnton\nSchwaighofer,\nand\nNeil\nD.\nLawrence.\nDataset Shift in Machine Learning.\nThe MIT\nPress,\n12 2008.\nISBN 9780262255103.\ndoi:\n10.7551/mitpress/9780262170055.001.0001.\nURL\nhttps://doi.org/10.7551/mitpress/\n9780262170055.001.0001.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin,\nTimothy P. Lillicrap,\nJean-\nBaptiste Alayrac, Radu Soricut, Angeliki Lazari-\ndou, Orhan Firat, Julian Schrittwieser, Ioannis\nAntonoglou,\nRohan Anil,\nSebastian Borgeaud,\n12\n"}, {"page": 13, "text": "APRIL\nAndrew M. Dai, Katie Millican, Ethan Dyer,\nMia Glaese, Thibault Sottiaux, Ben jamin Lee,\nFabio Viola, Malcolm Reynolds, Yuanzhong Xu,\nJames Molloy, Jilin Chen, Michael Isard, Paul\nBarham, Tom Hennigan, Ross Mcilroy, Melvin\nJohnson,\nJohan Schalkwyk,\nEli Collins,\nEliza\nRutherford, Erica Moreira, Kareem W. Ayoub,\nMegha Goel, Clemens Meyer, Gregory Thornton,\nZhen Yang, Henryk Michalewski, Zaheer Abbas,\nNathan Schucher, Ankesh Anand, Richard Ives,\nJames Keeling, Karel Lenc, Salem Haykal, Sia-\nmak Shakeri, Pranav Shyam, Aakanksha Chowdh-\nery, Roman Ring, Stephen Spencer, Eren Sezener,\nLuke Vilnis, Os car Chang, Nobuyuki Morioka,\nGeorge Tucker,\nCe Zheng,\nOliver Woodman,\nNithya Attaluri, Tom´as Kocisk´y, Evgenii Elty-\nshev, Xi Chen, Timothy Chung, Vittorio Selo,\nSiddhartha Brahma, Petko Georgiev, Ambrose\nSlone, Zhenkai Zhu, James Lottes, Siyuan Qiao,\nBen Caine, Sebastian Riedel, Alex Tomala, Mar-\ntin Chadwick, J Christopher Love, Peter Choy,\nSid Mittal, Neil Houlsby, Yunhao Tang, Matthew\nLamm,\nLibin Bai,\nQiao Zhang,\nLuheng He,\nYong Cheng, Peter Humphreys, Yujia Li, Sergey\nBrin, Albin Cassirer, Ying-Qi Miao, Luk´as Zilka,\nTaylor Tobin, Kelvin Xu, Lev Proleev, Daniel\nSohn, Alberto Magni, Lisa Anne Hendricks, Is-\nabel Gao,\nSantiago Ontan’on,\nOskar Bunyan,\nNathan Byrd, Abhanshu Sharma, Biao Zhang,\nMario Pinto, Rishika Sinha, Harsh Mehta, Dawei\nJia, Sergi Caelles, Albert Webson, Alex Morris,\nBecca Roelofs, Yifan Ding, Robin Strudel, Xue-\nhan Xiong, Marvin Ritter, Mostafa Dehghani,\nRahma Chaabouni, Abhijit Karmarkar, Guangda\nLai,\nFabian Mentzer,\nBibo Xu,\nYaGuang Li,\nYujing\nZhang,\nTom\nLe\nPaine,\nAlex\nGoldin,\nBehnam Neyshabur, Kate Baumli, Anselm Lev-\nskaya, Michael Laskin, Wenhao Jia, Jack W. Rae,\nKefan Xiao, Antoine He, Skye Giordano, Lak-\nshman Yagati, Jean-Baptiste Lespiau, Paul Nat-\nsev, Sanjay Ganapathy, Fangyu Liu, Danilo Mar-\ntins, Nanxin Chen, Yunhan Xu, Megan Barnes,\nRhys May, Arpi Vezer, Junhyuk Oh, Ken Franko,\nSophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil\nMustafa, Sean Sechrist, Emilio Parisotto, Thanu-\nmalayan Sankaranarayana Pillai, Chris Larkin,\nChenjie Gu, Christina Sorokin, Maxim Krikun,\nAlexey Guseynov, Jessica Landon, Romina Datta,\nAlexander Pritzel, Phoebe Thacker, Fan Yang,\nKevin Hui, A.E. Hauth, Chih-Kuan Yeh, David\nBarker, Justin Mao-Jones, Sophia Austin, Hannah\nSheahan, Parker Schuh, James Svensson, Rohan\nJain, Vinay Venkatesh Ramasesh, Anton Briukhov,\nDa-Woon Chung, Tamara von Glehn, Christina\nButterfield, Priya Jhakra, Matt Wiethoff, Justin\nFrye, Jordan Grimstad, Beer Changpinyo, Char-\nline Le Lan, Anna Bortsova, Yonghui Wu, Paul\nVoigtlaender, Tara N. Sainath, Charlotte Smith,\nWill Hawkins, Kris Cao, James Besley, Srivat-\nsan Srinivasan, Mark Omernick, Colin Gaffney,\nGabriela de Castro Surita, Ryan Burnell, Bog-\ndan Damoc, Junwhan Ahn, Andrew Brock, Man-\ntas Pajarskas, Anastasia Petrushkina, Seb Noury,\nLorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi\nAvrahami, Vedant Misra, Raoul de Liedekerke,\nMariko Iinuma, Alex Polozov, Sarah York, George\nvan den Driessche, Paul Michel, Justin Chiu,\nRory Blevins, Zach Gleicher, Adri`a Recasens, Al-\nban Rrustemi, Elena Gribovskaya, Au rko Roy,\nWiktor Gworek, S´ebastien M. R. Arnold, Lisa\nLee, James Lee-Thorp, Marcello Maggioni, En-\nrique Piqueras, Kartikeya Badola, Sharad Vikram,\nLucas Gonzalez, Anirudh Baddepudi, Evan Sen-\nter, Jacob Devlin, James Qin, Michael Azzam,\nMaja Trebacz, Martin Polacek, Kashyap Krish-\nnakumar, Shuo-Yiin Chang, Matthew Tung, Ivo\nPenchev, Rishabh Joshi, Kate Olszewska, Car-\nrie Muir,\nMateo Wirth,\nAle Jakse Hartman,\nJoshua Newlan, Sheleem Kashem, Vijay Bolina,\nElahe Dabir, Joost R. van Amersfoort, Zafarali\nAhmed, James Cobon-Kerr, Aishwarya B Kamath,\nArnar Mar Hrafnkelsson, Le Hou, Ian Mackin-\nnon, Alexandre Frechette, Eric Noland, Xi ance Si,\nEmanuel Taropa, Dong Li, Phil Crone, Anmol Gu-\nlati, S’ebastien Cevey, Jonas Adler, Ada Ma, David\nSilver, Simon Tokumine, Richard Powell, Stephan\nLee, Michael B. Chang, Samer Hassan, Diana\nMincu, Antoine Yang, Nir Levine, Jenny Brennan,\nMingqiu Wang, Sarah Hodkinson, Jeffrey Zhao,\nJosh Lipschultz, Aedan Pope, Michael B. Chang,\nCheng Li, Laurent El Shafey, Michela Paganini,\nSholto Douglas,\nBernd Bohnet,\nFabio Pardo,\nSeth Odoom, Mihaela Ros,ca, Cicero Nogueira\ndos Santos, Kedar Soparkar, Arthur Guez, Tom\nHudson,\nSteven Hansen,\nChulayuth Asawaro-\nengchai, Ravichandra Addanki, Tianhe Yu, Woj-\nciech Stokowiec, Mina Khan, Justin Gilmer, Jae-\nhoon Lee, Carrie Grimes Bostock, Keran Rong,\nJonathan Caton, Pedram Pejman, Filip Pavetic,\nGeoff Brown, Vivek Sharma, Mario Luvci’c, Ra-\njku mar Samuel, Josip Djolonga, Amol Mand-\nhane, Lars Lowe Sjosund, Elena Buchatskaya, El-\n13\n"}, {"page": 14, "text": "APRIL\nspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek\nLim, Ross Hemsley, Jane Labanowski, Nicola De\nCao, David Steiner, Sayed Hadi Hashemi, Ja-\ncob Austin, Anita Gergely, Tim Blyth, Joe Stan-\nton, Kaushik Shivakumar, Aditya Siddhant, An-\nders Andreassen, Carlos L. Araya, Nikhil Sethi,\nRakesh Shivanna, Steven Hand, Ankur Bapna,\nAli Khodaei,\nAntoine Miech,\nGarrett Tanzer,\nAndy Swing, Shantanu Thakoor, Zhufeng Pan,\nZachary Nado, Stephanie Winkler, Dian Yu, Mo-\nhammad Saleh,\nLorenzo Maggiore,\nIain Barr,\nMinh Giang,\nThais Kagohara,\nIvo Danihelka,\nAmit Marathe, Vladimir Feinberg, Mohamed El-\nhawaty,\nNimesh Ghelani,\nDan Horgan,\nHelen\nMiller, Lexi Walker, Richard Tanburn, Mukar-\nram Tariq, Disha Shrivastava, Fei Xia, Chung-\nCheng Chiu, Zoe Ashwood, Khuslen Baatarsukh,\nSina Samangooei, Fred Alcober, Axel Stjerngren,\nPaul Komarek, Katerina Tsihlas, Anudhyan Bo-\nral, Ramona Comanescu, Jeremy Chen, Ruibo\nLiu, Dawn Bloxwich, Charlie Chen, Yanhua Sun,\nFangxi aoyu Feng, Matthew Mauger, Xerxes Doti-\nwalla, Vincent Hellendoorn, Michael Sharman, Ivy\nZheng, Krishna Haridasan, Gabriel Barth-Maron,\nCraig Swanson, Dominika Rogozi’nska, Alek An-\ndreev, Paul Kishan Rubenstein, Ruoxin Sang,\nDan Hurt, Gamaleldin Elsayed, Ren shen Wang,\nDave Lacey, Anastasija Ili’c, Yao Zhao, Woohyun\nHan, Lora Aroyo, Chimezie Iwuanyanwu, Vitaly\nNikolaev, Balaji Lakshminarayanan, Sadegh Jaza-\nyeri, Raphael Lopez Kaufman, Mani Varadara-\njan, Chetan Tekur, Doug Fritz, Misha Khalman,\nDavid Reitter, Kingshuk Dasgupta, Shourya Sar-\ncar, T. Ornduff, Javier Snaider, Fantine Huot,\nJohnson Jia, Rupert Kemp, Nejc Trdin, Anitha\nVijayakumar, Lucy Kim, Christof Angermueller,\nLi Lao, Tianqi Liu, Haibin Zhang, David En-\ngel, Somer Greene, Anais White, Jessica Austin,\nLilly Taylor, Shereen Ashraf, Dangyi Liu, Maria\nGeorgaki, Irene Cai, Yana Kulizhskaya, Sonam\nGoenka, Brennan Saeta, Kiran Vodrahalli, Chris-\ntian Frank, Dario de Cesare, Brona Robenek,\nHarry Richardson, Mah moud Alnahlawi, Christo\npher Yew, Priya Ponnapalli, Marco Tagliasacchi,\nAlex Korchemniy, Yelin Kim, Dinghua Li, Bill Ros-\ngen, Kyle Levin, Jeremy Wiesner, Praseem Ban-\nzal, Praveen Srinivasan, Hongkun Yu, cCauglar\nUnlu, David Reid, Zora Tung, Daniel F. Finchel-\nstein, Ravin Kumar, Andre Elisseeff, Jin Huang,\nMing Zhang,\nRui Zhu,\nRicardo Aguilar,\nMai\nGim’enez, Jiawei Xia, Olivier Dousse, Willi Gierke,\nSoheil Hassas Yeganeh, Damion Yates, Komal\nJalan, Lu Li, Eri Latorre-Chimoto, Duc Dung\nNguyen, Ken Durden, Praveen Kallakuri, Yaxin\nLiu, Matthew Johnson, Tomy Tsai, Alice Tal-\nbert, Jasmine Liu, Alexander Neitz, Chen Elkind,\nMarco Selvi, Mimi Jasarevic, Livio Baldini Soares,\nLivio Baldini Soares, Pidong Wang, Alek Wenjiao\nWang, Xinyu Ye, Krystal Kallarackal, Lucia Loher,\nHoi Lam, Josef Broder, Daniel Niels Holtmann-\nRice, Nina Martin, Bramandia Ramadhana, Daniel\nToyama, Mrinal Shukla, Sujoy Basu, Abhi Mo-\nhan, Nicholas Fernando, Noah Fiedel, Kim Pater-\nson, Hui Li, Ankush Garg, Jane Park, Donghyun\nChoi, Diane Wu, Sankalp Singh, Zhishuai Zhang,\nAmir Globerson, Lily Yu, John Carpenter, F´elix\nde Chaumont Quitry, Carey Radebaugh, Chu-\nCheng Lin, Alex Tudor, Prakash Shroff, Drew Gar-\nmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal,\nAlexey Yakubovich, Nilesh Tripuraneni, James\nManyika, Ha roon Qureshi, Nan Hua, Christel\nNgani, Maria Abi Raad, Hannah Forbes, Anna Bu-\nlanova, Jeff Stanway, Mukund Sundararajan, Vic-\ntor Ungureanu, Colton Bishop, Yunjie Li, Balaji\nVenkatraman, Bo Li, Chloe Thornton, Salvatore\nScellato, Nishesh Gupta, Yicheng Wang, Ian Ten-\nney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal,\nDiana Gage Wright, Ben Bariach, Zhuyun Xiao,\nPeter Hawkins, Sid Dalmia, Cl´ement Farabet, Pe-\ndro Valenzuela, Quan Yuan, Christoper A. Welty,\nAnanth Agarwal, Mianna Chen, Wooyeol Kim,\nBrice Hulse, Nandita Dukkipati, Adam Paszke,\nAndrew Bolt, Elnaz Davoodi, Kiam Choo, Jen-\nnifer Beattie, Jennifer Prendki, Harsha Vashisht,\nRe beca Santamaria-Fernandez, Luis C. Cobo,\nJarek Wilkiewicz, David Madras, Ali Elqursh,\nGrant Uy, Kevin Ramirez, Matt Harvey, Tyler\nLiechty,\nHeiga Zen,\nJeff Seibert,\nClara Huiyi\nHu, A. Ya. Khorlin, Maigo Le, Asaf Aharoni,\nMegan Li, Lily Wang, Sandeep Kumar, Alejandro\nLince, Norman Casagrande, Jay Hoover, Dalia El\nBadawy, David Soergel, Denis Vnukov, Matt Miec-\nnikowski, Jiˇr´ı ˇSimˇsa, Anna Koop, Praveen Kumar,\nThibault Sellam, Daniel Vlasic, Samira Daruki,\nNir Shabat, John Zhang, Guolong Su, Kalpesh\nKrishna, Jiageng Zhang, Jeremiah Liu, Yi Sun,\nEvan Palmer, Alireza Ghaffarkhah, Xi Xiong, Vic-\ntor Cotruta, Michael Fink, Lucas Dixon, Ashwin\nSreevatsa, Lucas Dixon, Alek Dimitriev, Mohsen\nJafari, Remi Crocker, Nicholas Fitzgerald, Aviral\nKumar, Nicholas FitzGerald, Ivan Philips, Freder-\nick Liu, Yannie Liang, Rachel Sterneck, Alena Re-\n14\n"}, {"page": 15, "text": "APRIL\npina, Marcus Wu, Laura Knight, Marin Georgiev,\nHyo Lee, Harry Askham, Abhishek Chakladar, An-\nnie Louis, Carl Crous, Hardie Cate, Dessie Petrova,\nMichael Quinn, Denese Owusu-Afriyie, Achintya\nSinghal, Nan Wei, Solomon Kim, Damien Vin-\ncent, Milad Nasr, Ilia Shumailov, Christopher A.\nChoquette-Choo, Reiko Tojo, Shawn Lu, Diego\nde Las Casas, Yuchung Cheng, Tolga Bolukbasi,\nKather ine Lee, Saaber Fatehi, Rajagopal Anan-\nthanarayanan, Miteyan Patel, Charbel El Kaed,\nJing Li, Jakub Sygnowski, Shreyas Rammohan\nBelle, Zhe Chen, Jaclyn Konzelmann, Siim P˜oder,\nRoopal Garg, Vinod Koverkathu, Adam Brown,\nChris Dyer, Rosanne Liu, Azade Nova, Jun Xu,\nJunwen Bai, Slav Petrov, Demis Hassabis, Ko-\nray Kavukcuoglu, Jeffrey Dean, Oriol Vinyals,\nand Alexandra Chronopoulou.\nGemini 1.5: Un-\nlocking multimodal understanding across millions\nof tokens of context.\nArXiv, abs/2403.05530,\n2024. URL https://api.semanticscholar.org/\nCorpusID:268297180.\nKhaled Saab,\nTao Tu,\nWei-Hung Weng,\nRyu-\ntaro Tanno, David Stutz, Ellery Wulczyn, Fan\nZhang,\nTim Strother,\nChunjong Park,\nElahe\nVedadi, Juanma Zambrano Chaves, Szu-Yeu Hu,\nMike Schaekermann, Aishwarya Kamath, Yong\nCheng, David G. T. Barrett, Cathy Cheung, Basil\nMustafa, Anil Palepu, Daniel McDuff, Le Hou,\nTomer Golany, Luyang Liu, Jean baptiste Alayrac,\nNeil Houlsby,\nNenad Tomasev,\nJan Freyberg,\nCharles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh\nAzizi,\nKimberly Kanada,\nSiWai Man,\nKavita\nKulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He,\nBen Caine, Albert Webson, Natasha Latysheva,\nMelvin Johnson, Philip Mansfield, Jian Lu, Ehud\nRivlin, Jesper Anderson, Bradley Green, Renee\nWong, Jonathan Krause, Jonathon Shlens, Ewa\nDominowska, S. M. Ali Eslami, Katherine Chou,\nClaire Cui, Oriol Vinyals, Koray Kavukcuoglu,\nJames Manyika, Jeff Dean, Demis Hassabis, Yossi\nMatias, Dale Webster, Joelle Barral, Greg Corrado,\nChristopher Semturs, S. Sara Mahdavi, Juraj Got-\ntweis, Alan Karthikesalingam, and Vivek Natara-\njan.\nCapabilities of gemini models in medicine,\n2024. URL https://arxiv.org/abs/2404.18416.\nKaran Singhal, Shekoofeh Azizi, Tien-Ju Tu, and\net al. Towards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617, 2023a.\nKaran Singhal,\nTao Tu,\nJuraj Gottweis,\nRory\nSayres, Ellery Wulczyn, Le Hou, Kevin Clark,\nStephen Pfohl, Heather Cole-Lewis, Darlene Neal,\nMike Schaekermann, Amy Wang, Mohamed Amin,\nSami Lachgar, Philip Mansfield, Sushant Prakash,\nBradley Green, Ewa Dominowska, Blaise Aguera\ny Arcas, Nenad Tomasev, Yun Liu, Renee Wong,\nChristopher Semturs, S. Sara Mahdavi, Joelle Bar-\nral, Dale Webster, Greg S. Corrado, Yossi Ma-\ntias, Shekoofeh Azizi, Alan Karthikesalingam, and\nVivek Natarajan.\nTowards expert-level medi-\ncal question answering with large language mod-\nels, 2023b. URL https://arxiv.org/abs/2305.\n09617.\nMasashi Sugiyama, Matthias Krauledat, and Klaus-\nRobert M¨uller.\nCovariate shift adaptation by\nimportance weighted cross validation.\nJ. Mach.\nLearn. Res., 8:985–1005, dec 2007. ISSN 1532-4435.\nRichard S Sutton and Andrew G Barto. Reinforce-\nment learning: An introduction. MIT press, 2018.\nShengpu Tang and Jenna Wiens.\nCounterfactual-\naugmented importance sampling for semi-offline\npolicy evaluation.\nIn Thirty-seventh Confer-\nence on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?id=\ndsH244r9fA.\nAnders Voldby and Birgitte Brandstrup. Fluid ther-\napy in the perioperative setting-a clinical review.\nJournal of Intensive Care, 4, 12 2016.\ndoi: 10.\n1186/s40560-016-0154-3.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou.\nChain of thought prompting\nelicits reasoning in large language models. arXiv\npreprint arXiv:2201.11903, 2022.\nBrian Zhang, Eric Mitchell, Hongyu Ren, Kevin\nLu, Max Schwarzer, Michelle Pokrass, Shengjia\nZhao, Ted Sanders, Adam Tauman Kalai, Alexan-\ndre Passos, Benjamin Sokolowsky, Elaine Ya Le,\nErik Ritter,\nHao Sheng,\nHanson Wang,\nIlya\nKostrikov, James Lee, Johannes Ferstad, Michael\nLampe, Prashanth Radhakrishnan, Sean Fitzger-\nald, S´ebastien Bubeck, Yann Dubois, Yu Bai,\nAndy Applebaum, Elizabeth Proehl, Evan Mays,\nJoel Parish, Kevin Liu, Leon Maksin, Leyton\nHo, Miles Wang, Michele Wang, Olivia Watkins,\nPatrick Chao, Samuel Miserendino, Tejal Pat-\nwardhan, Antonia Woodford, Beth Hoover, Jake\n15\n"}, {"page": 16, "text": "APRIL\nBrill, Kelly Stirman, Neel Ajjarapu, Nick Tur-\nley, Nikunj Handa, Olivier Godement, Akshay\nNathan, Alyssa Huang, Andy Wang, Ankit Go-\nhel, Ben Eggers, Brian Yu, Bryan Ashley, Chengdu\nHuang, Davin Bogan, Emily Sokolova, Eric Ho-\nracek, Felipe Petroski Such, Jonah Cohen, Joshua\nGross, Justin Becker, Kan Wu, Larry Lv, Lee\nByron, Manoli Liodakis, Max Johnson, Mike Tr-\npcic, Murat Yesildal, Rasmus Rygaard, R. J.\nMarsan, Rohit Ram-chandani, Rohan Kshirsagar,\nSara Conlon,\nTony Xia,\nSiyuan Fu,\nSrinivas\nNarayanan,\nSulman Choudhry,\nTomer Kaftan,\nTrevor Creech, Andrea Vallone, Andrew Duber-\nstein, Enis Sert, Eric Wallace, Grace Zhao, Irina\nKofman, Jieqi Yu, Joaquin Qui˜nonero Candela,\nMade laine Boyd, Mehmet Ali Yatbaz, Mike Mc-\nClay, Mingxuan Wang, Sandhini Agarwal, Saachi\nJain,\nSam Toizer,\nSantiago Hern´andez,\nSteve\nMostovoy, Tao Li, Young Cha, Yunyun Wang,\nLama Ahmad,\nTroy Peterson,\nCarpus Chang,\nKristen Ying, Aidan Clark, Dane Stuckey, Jerry\nTworek, Jakub W. Pachocki, Johannes Heidecke,\nKevin Weil, Liam Fedus, Mark Chen, Sam Alt-\nman, and Wojciech Zaremba. Openai o3-mini sys-\ntem card. URL https://api.semanticscholar.\norg/CorpusID:276119184.\nHong-Yu Zhou, Juli´an Nicol´as Acosta, Subathra\nAdithan, Suvrankar Datta, Eric J. Topol, and\nPranav Rajpurkar. Medversa: A generalist founda-\ntion model for medical image interpretation, 2025.\nURL https://arxiv.org/abs/2405.07988.\n16\n"}, {"page": 17, "text": "APRIL\nFigure 4: Reward functions for both decision-making tasks are a function of the corresponding\nreference range.\nReward is bounded in the range [0, 1], attaining its maximum when the lab value falls\nwithin the corresponding clinical reference range (3.5 −4.5 mEq/L for serum potassium, and 135 −145\nmEq/L for serum sodium). As the lab value deviates from this range, the reward decreases according to a\nGaussian decay, with the lowest rewards assigned to critically low or high values.\nAppendix A. MIMIC-IV dataset\nThe MIMIC-IV dataset consists of patient data for over 65,000 patients admitted to the ICU and over\n200,000 patients admitted to the emergency department. This data is represented as electronic health records\n(EHRs), which capture a variety of information about each patient including static covariates such as age\nand gender, all hospital procedures and events such as lab measurements and administered medications, as\nwell as indications of comorbidities. In this work, we consider non-ICU patients who have been administered\neither IV potassium, or IV hypertonic saline. We have 1622 patients who were administered IV potassium,\nand 1187 patients who were administered saline.\nWe represent each patient context using the following 15 features: age, gender, weight, height, heart rate,\nrespiratory rate, oxygen saturation pulseoxymetry, systolic blood pressure, diastolic blood pressure, serum\ncreatinine lab, administered NaCl 0.9%, administered dextrose 5%, administered propofol, administered\nnorepinephrine, and administered insulin. We choose these features due to their relevance in being able to\npredict downstream serum potassium and serum sodium labs (Kluwer, n.d.). The reward function for both\ntasks is visualized in Figure 4.\nWhen we report the accuracy of the LLM in predicting downstream lab values, we use weighted F1 score.\nThe classes of predictions for potassium (all in mEq/L) are [< 3.2, >= 3.2 and < 5, >= 5 and < 6, >= 6].\nThe classes of predictions for sodium (all in mEq/L) are [< 118, >= 118 and < 135, >= 135 and < 152,\n>= 152 and < 169, >= 169].\nAppendix B. Prompts\nHere we include the format of the prompt used to query downstream lab value predictions. The format is\nconsistent across both the potassium and sodium repletion tasks, and varies only based on individual patient\ndetails. The prompt consists of five components: task information, static covariates, labs and medicines,\ndomain information from UpToDate, and a prediction query. An example prompt is shown in Figure 5.\nAppendix C. Additional Empirical Results\nHere, we include additional empirical results to support our claims in the main text. First, we report figures\nthat demonstrate the quality of downstream lab predictions across LLMs for both potassium and sodium lab\n17\n"}, {"page": 18, "text": "APRIL\nFigure 5: LLMs can be prompted to construct downstream lab value predictions. The prompt\ncontains separate components that first describe the patient’s clinical state four hours prior to receiving\ntreatment, and then contains instructions to perform the lab value prediction. The prompt includes relevant\ninformation from UpToDate, a clinical resource, to help an LLM identify which features in the medical record\nare most predictive.\n18\n"}, {"page": 19, "text": "APRIL\n(a) Potassium lab predictions.\n(b) Sodium lab predictions.\nFigure 6: LLMs can accurately predict sodium and potassium lab values in MIMIC-IV. Predic-\ntions are evaluated using weighted F1 scores across clinically relevant lab value categories. The black line\ndenotes perfect agreement with ground truth, and predictions from a different LLM are reported in different\ncolors.\n19\n"}, {"page": 20, "text": "APRIL\nFigure 7: LLM-generated counterfactual annotations improve OPE estimates for sodium reple-\ntion in settings with high divergence between behavior and target policies. We report results\nusing the DM baseline (blue) which uses no annotations, and DM + with annotations from different LLMs\nin other colors. Error bars represent standard error across 500 bootstrapped datasets, truncated at 0 when\nnecessary.\npredictions (Figure 6). Our results conclude similarly to those reported in Table 1, suggesting that LLMs\ncan predict downstream lab values within clinically relevant degrees of error.\nFurthermore, we investigate whether the age and gender of the patient affects the accuracy of the LLM\nin predicting potassium and sodium levels. We find that the prediction error varies substantially depending\non the model and trends are not consistent given a patient’s age or gender. (Figure 8).\nNow we discuss the utility of LLM-generated counterfactual annotations within the sodium repletion\ntask (Figure 7).\nSimilar to the potassium repletion results, we find that LLM-generated counterfactual\nannotations help most when there is substantial divergence between the actions observed in the behavior\nand target policies. Just as in the potassium task results, the most improvement due to annotations occurs\nin the sodium dosage cohort.\nFinally, we report entropy and further annotations results for the sodium repletion task, suggesting that,\nsimilar to the potassium repletion task, that more annotations may yield only marginal gains (Figure 9).\n20\n"}, {"page": 21, "text": "APRIL\n(a) Potassium Lab Prediction Errors\n(b) Sodium Lab Prediction Errors\nFigure 8: Model prediction error varies depending on the patient’s age and gender. However,\nthe trends are not consistent as observed for both sodium prediction error (Figure 8(b)) and potassium\nprediction error (Figure 8(a)).\n21\n"}, {"page": 22, "text": "APRIL\nFigure 9: Combining annotation sources yields limited returns in the sodium repletion task.\n(Top) We compare DM + with the two best-performing LLMs for sodium repletion (yellow, green) and\ntwo aggregation methods: pooling predictions (orange) and averaging annotations (pink). Error bars show\nstandard error over 500 bootstrapped datasets, truncated at 0. (Bottom) Marginal entropy over the action\nspace H(A) when adding counterfactual annotations to the behavior dataset for the sodium cohort. The\nhorizontal dashed line marks the maximum possible entropy.\n22\n"}]}