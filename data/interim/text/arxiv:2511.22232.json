{"doc_id": "arxiv:2511.22232", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.22232.pdf", "meta": {"doc_id": "arxiv:2511.22232", "source": "arxiv", "arxiv_id": "2511.22232", "title": "From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation", "authors": ["Zhen Chen", "Yihang Fu", "Gabriel Madera", "Mauro Giuffre", "Serina Applebaum", "Hyunjae Kim", "Hua Xu", "Qingyu Chen"], "published": "2025-11-27T08:54:59Z", "updated": "2025-11-27T08:54:59Z", "summary": "Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.22232v1", "url_pdf": "https://arxiv.org/pdf/2511.22232.pdf", "meta_path": "data/raw/arxiv/meta/2511.22232.json", "sha256": "60658238842eabecfd46e3dc3d586f47ef37862e30577cdfdd75f4137c8051ee", "status": "ok", "fetched_at": "2026-02-18T02:25:59.512952+00:00"}, "pages": [{"page": 1, "text": "From Compound Figures to Composite Understanding:\nDeveloping a Multi-Modal LLM from Biomedical Literature with\nMedical Multiple-Image Benchmarking and Validation\nZhen Chen1‡, Yihang Fu1‡, Gabriel Madera1,2, Mauro Giuffre1, Serina Applebaum1, Hyunjae Kim1, Hua Xu1,\nQingyu Chen1∗\n1Department of Biomedical Informatics and Data Science, Yale School of Medicine, Yale University, New\nHaven, CT 06510, USA\n2School of Medicine, University of Puerto Rico, San Juan, PR 00921, USA\n‡Contributed Equally\n*Corresponding author\n1\nAbstract\nMulti-modal large language models (MLLMs) have shown tremendous promise in advancing healthcare. How-\never, most existing models remain confined to single-image understanding, which greatly limits their applica-\nbility in real-world clinical workflows. In practice, medical diagnosis and disease progression assessment\noften require synthesizing information across multiple images from different modalities or time points. The\ndevelopment of medical MLLMs capable of such multi-image understanding has been hindered by the lack of\nlarge-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that\nleverages license-permissive compound images, widely available in biomedical literature, as a rich yet under-\nutilized data source for training medical MLLMs in multi-image analysis. Specifically, we design a five-stage,\ncontext-aware instruction generation paradigm underpinned by a divide-and-conquer strategy that systemati-\ncally transforms compound figures and their accompanying expert text into high-quality training instructions.\nBy decomposing the complex task of multi-image analysis into manageable sub-tasks, this paradigm empow-\ners MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the\ncomplex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over\n237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical\nmulti-image multi-modal large language model. For comprehensive benchmarking, we construct PMC-MI-\nBench for composite understanding, manually validated by medical experts. Extensive experiments show\nthat M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-\nimage, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization\nto real-world clinical settings, achieving superior performance on longitudinal chest X-ray analysis using the\nMIMIC dataset. This work establishes a scalable and efficient paradigm for developing next-generation medi-\ncal MLLMs, capable of composite reasoning across complex multi-image scenarios, bridging the gap between\nbiomedical literature and real-world clinical applications.\n1\narXiv:2511.22232v1  [cs.CV]  27 Nov 2025\n"}, {"page": 2, "text": "2\nIntroduction\nMulti-modal large language models (MLLMs) 1–3 combine natural language processing with multi-modal per-\nception capabilities, and are capable of processing and reasoning across textual and visual data. In the general\ndomain, MLLMs have demonstrated remarkable capability in understanding and integrating information across\nmodalities, paving the way for their adaptation to specialized fields 4. Preliminary results in healthcare applica-\ntions have revealed promising potentials, particularly in processing clinical text, answering medical questions,\nand analyzing visual medical data 5–12. These advancements indicate the prospect of MLLMs to enhance diag-\nnostic processes 13,14, streamline clinical decision-making 15, and support medical education 16. Despite these\nadvances, a critical limitation persists: most existing MLLMs are primarily designed for single-image under-\nstanding, which significantly constrains their applicability in real-world medical scenarios involving complex\nmulti-image, multi-modal data.\nCompared to single-image tasks, multi-image tasks hold greater practical significance in real-world clinical\nworkflows 17–19. For example, longitudinal monitoring requires comparing multiple images collected across\ndifferent time points to track disease progression, while clinical diagnosis often integrates medical images from\ndifferent modalities to provide a comprehensive understanding of a medical case 20,21. For instance, oncologists\nroutinely analyze Magnetic Resonance Imaging (MRI) scans for tumor morphology, Positron Emission Tomog-\nraphy (PET) scans for metabolic activity, and histopathology slides collectively to formulate a comprehensive\ndiagnostic picture 20, while cardiologists and neurologists similarly combine modalities like echocardiography,\nComputed Tomography (CT), and functional MRI to evaluate heart disease and brain disorders 22,23. These\nmultiple-image scenarios, which constitute a substantial portion of clinical workflows, demand the compos-\nite understanding capabilities that synthesize information across multiple medical images. However, existing\nMLLMs 5–12 fail to adequately address these, severely limiting the applicability and adoption. The scarcity of\nmultiple-image MLLMs stems largely from a fundamental data challenge. Medical imaging data is inherently\ndifficult to collect due to privacy and ethical constraints 13,24, and the complexity increases substantially for\nmultiple-image datasets that require curated collections of related images across modalities and time points.\nTo overcome the critical bottleneck of data scarcity, we turn to compound figures from license-permissive\nbiomedical literature, i.e., multi-panel figures that integrate multiple sub-images within a single structured\nlayout, where each panel typically represents a distinct but related aspect of the same medical case. Their\nsignificance lies not just in their public availability, but in their nature as a rich proxy for real-world clinical\nscenarios. As exemplified in Fig. 1, the compound figure exhibits the diverse inter-image complexities, in-\ncluding spatial arrangements highlighting anatomical correspondence, cross-modal combinations integrating\ncomplementary diagnostic information from CT and histopathology images, and temporal sequences showing\ndisease evolution with postoperative examination. These complex relationships demand fundamentally more\nadvanced reasoning capabilities compared to single-image understanding. As such, traditional instruction gen-\neration methods 5,9, primarily designed for single-image scenarios with straightforward image-text pairing,\nfail to capture the multi-dimensional dependencies inherent in compound figures, thus presenting a significant\nmethodological challenge for composite understanding.\nTo address these challenges, we present the first systematic MLLM framework specifically designed for medi-\ncal multiple-image understanding, by leveraging the compound figure data derived from biomedical literature.\nOur primary contribution is a novel five-stage, context-aware instruction generation paradigm underpinned by\na divide-and-conquer strategy. This paradigm decomposes the complex challenge of compound figure under-\nstanding into a sequence of manageable, specialized tasks, ranging from medical knowledge complementation\nto visual perception enhancement, to transform raw compound figures and their associated textual content into\n2\n"}, {"page": 3, "text": "Caption: A 65-year-old female with insulinoma in the neck of the pancreas was treated with laparoscopic \nRFA. (A) Contrast-enhanced CT scan depicted a markedly enhanced lesion in the neck of the pancreas in \nthe early arterial phase (red cycle). (B) Contrast-enhanced CT scan showed an isointense lesion in the \ndelayed phase (red cycle). (C) Pathology confirmed the diagnosis of pancreatic insulinoma (HE×100). \n(D) The postoperative CT scan suggested the tumor was completely ablated.\nInline text: A 65-year-old female complaining for more than 3 years of episodic hypoglycemic symptoms \nincluding recurrent sweating, palpitation and weakness was referred to hospital. No obvious abnormalities \nof her heart, lung, kidney and other major organs were found by physical examination. The results of \nroutine laboratory test of blood, urine and stool routine were normal. The level of blood glucose was 2.3 \nmmol/L, carcinoembryonic antigen (CEA) was 6.5 μg/L, and carbohydrate antigen (CA) 19-9 was 67.5 \nkU/L. The contrast-enhanced CT scan depicted a markedly enhanced 15 mm lesion in the neck of the \npancreas in the early arterial phase and an isointense lesion in the delayed phase (Fig. 2 A & B). A \npreoperative diagnosis of functioning insulinoma of the pancreatic neck was made based on clinical \nsymptoms, laboratory results and the featured CT images…\nCross-modal \nrelationship\nSpatial relationship\nLongitudinal \nrelationship\nFigure 1: Illustration of a compound figure example in PMC literature. This example, derived from\nPMC7029651, features a compound figure composed of multiple sub-images. The example highlights longi-\ntudinal patient records with radiology and histopathology images for a case of insulinoma located in the neck\nof the pancreas. It integrates the accompanying image caption, which describes the visual contents, along\nwith inline text from the manuscript that references the compound figure. To fully understand this medical\ncompound figure, it is essential to comprehend the rich visual content and associated textual information. This\nincludes analyzing the spatial, cross-modal, and longitudinal relationships of the sub-images, particularly con-\ncerning the first CT scan.\nclinically rich and relevant training instructions. Unlike traditional methods 5,6,9 that rely on simple image-text\npairing, this paradigm constructs comprehensive learning scenarios that emulate real-world clinical reason-\ning processes, enabling MLLMs to effectively process and analyze the complex interrelationships inherent in\nmedical compound figures. Then, using this paradigm on a large-scale dataset of over 237,000 compound\nfigures, we develop and train M3LLM, a medical multi-image multi-modal large language model, to under-\nstand and reason over complex visual and textual information in clinical contexts. Furthermore, to facilitate\nrigorous benchmarking for this domain, we curate and release the PMC-MI-Bench, an expert-validated bench-\nmark with comprehensive multi-image understanding tasks. Systematic evaluations demonstrate that M3LLM\nsignificantly outperforms state-of-the-art general-purpose and specialized medical MLLMs in multi-image,\nsingle-image, text-only, and multi-choice scenarios. Notably, the capabilities of M3LLM successfully general-\nize to clinical practice, as shown by its substantial improvements in a longitudinal patient analysis task using\nchest X-ray images from the MIMIC database 18,19, such as disease diagnosis and progression monitoring. To\npromote transparency and further advancements, we release the weights of our M3LLM, the training dataset,\nand our benchmark to the research community.\n3\nResults\nWe conduct extensive benchmarking and validation to assess the performance of our proposed M3LLM against\nstate-of-the-art general-purpose MLLMs (e.g., LLaVA-7B 1, LLaVA-NeXT-7B 25, QWen2.5-VL-7B 2, and\nInternVL3-8B 3) and medical-specific ones (e.g., LLaVA-Med-7B 5, HuatuoGPT-Vision-7B 9, Lingshu-7B 26,\nHealthGPT-14B 12, and MedGemma-27B 27). To ensure a comprehensive and diverse evaluation, our experi-\nments assess performance across several key dimensions. We utilize a wide range of datasets, including our\nnewly curated PMC-MI-Bench, public OmniMedVQA 28 and MMMU-Med 29 benchmarks, and a real-world\nclinical validation task using MIMIC longitudinal X-rays 18,19. The evaluation spans multiple task types, in-\ncluding multi-image VQA, single-image VQA, text-only QA, and multi-choice VQA. We employ a robust suite\nof evaluation metrics, ranging from accuracy for classification tasks to semantic metrics, including BLEU@4,\nROUGE-L, BERTScore, and Semantic Textual Similarity (STS), and LLM-as-a-judge using GPT-4o for open-\n3\n"}, {"page": 4, "text": "PMC Biomedical Literature\n3.1M medical compound figures with paired texts from 6.1M papers\nInline Text \nSummarization\nMedical Knowledge \nComplementation\nSummarize text into \nshort narratives\nAdd domain-specific \nmedical knowledge\nMedical Visual \nPerception Enhancement\nGenerate detailed \nvisual descriptions\nStructural Instruction \nGeneration\nCreate context-question-\nanswer instruction sets\nLeakage-Prevented \nContext Refinement\nPrevent answer leakage \nand ensures quality\nProvide foundational \ncase context\nExplain key medical \nconcepts and relationships\nCapture anatomical and \ndiagnostic features\nSupport multi-image \nunderstanding tasks\nOptimize context for \nclinical reasoning\nManual assessment on randomly sampled instructions\nManual-proofed instructions agreed by two professionals\nPMC-MI Dataset\nPMC-MI-Bench\n237K high-quality training data for M3LLM\n300 manual-proofed data for comprehensive evaluation\nInstruction Quality Control\nContext-aware Instruction Generation Pipeline\nOmniMedVQA\nMIMIC Longitudinal X-ray\nMMMU-Med\nValidate\nOptimize\nMulti-/single-image benchmarks\nM3LLM\nFigure 2: Overview of the study for medical compound figure understanding and clinical validation. The\nframework integrates PMC-derived compound figure data. Through a five-stage, context-aware instruction\ngeneration paradigm, the proposed M3LLM processes medical compound figures and paired texts. The core\narchitecture of M3LLM includes a Vision Transformer (ViT), a connector module for visual-to-text alignment,\nand a large language model (LLM) for clinical reasoning. On this basis, the context-aware instruction tuning\nenables efficient and accurate multi-image comprehension. Extensive evaluation is conducted on the curated\nPMC-MI-Bench, public benchmarks, and MIMIC clinical cases.\nended generation. A holistic visualization of these comparisons in Fig. 3 concisely demonstrates that M3LLM\nachieves superior and well-rounded capabilities across this diverse suite of tasks. In this section, we detail\nthese findings, followed by comprehensive ablation studies and a manual quality assessment of our generated\nPMC-MI dataset for training.\n3.1\nPerformance Comparison on PMC-MI-Bench\nWe conduct comprehensive comparisons with state-of-the-art MLLMs across four instruction types of the cu-\nrated PMC-MI-Bench, including the multi-image VQA, single-image VQA, text-only QA and multi-choice\nVQA. As elaborated in Table 1, 2, 3, and 4, our M3LLM achieves significant improvements across all QA set-\ntings, substantially outperforming existing MLLMs 1–3,5,9,12,25–27. These results demonstrate the effectiveness\nof our five-stage, context-aware instruction generation paradigm in creating clinically relevant training data\nthat enables sophisticated medical reasoning with multi-image, single-image and text-only settings, as shown\n4\n"}, {"page": 5, "text": "MIMIC X-ray \nLongitudinal Progression\nPMC-MI-Bench\nMulti-image VQA\nPMC-MI-Bench\nSingle-image VQA\nPMC-MI-Bench\nMulti-choice VQA\nPMC-MI-Bench\nPure-text QA\nOmniMedVQA\nMMMU-Med\nMIMIC X-ray\nDiagnosis\n78.2\n74.7\n82.5\n80.0\n86.4\n85.3\n90.0\n62.7\n57.3\n85.7\n82.8\n73.9\n67.8\n45.1\n43.7\nQWen2.5-VL-7B\nInternVL3-8B\nHuatuoGPT-Vision-7B\nLingshu-7B\nMedGemma-27B\nM3LLM (Ours)\n84.0\nFigure 3: Comprehensive performance comparison of M3LLM against state-of-the-art MLLMs across\neight tasks. This radar chart visualizes model capabilities across diverse medical tasks, utilizing Accuracy\nfor classification and multi-choice tasks (e.g., PMC-MI-Bench Multi-choice VQA, MIMIC X-ray Diagnosis,\nMIMIC X-ray Longitudinal Progression, OmniMedVQA, and MMMU-Med) and Semantic Textual Similarity\n(STS) for open-ended generation tasks (e.g., PMC-MI-Bench Multi-image VQA, Single-image VQA, and\nText-only QA). The best and second-best performances are marked on each axis, respectively. The expansive\narea covered by M3LLM (orange) visually demonstrates its superior and well-rounded capabilities across this\ndiverse suite of tasks and metrics.\nin Fig. 4.\nIn the multi-image VQA, our M3LLM demonstrates exceptional capability to synthesize information across\nmultiple sub-images for comprehensive medical queries in Table 1, achieving 15.0 BLEU@4, 37.8 ROUGE-L,\n70.1 BERTScore, and 78.2 Semantic Textual Similarity (STS) compared to the second-best performance of\n9.8 BLEU@4 (Lingshu-7B 26), and 31.4 ROGUE-L, 66.9 BERTScore, and 74.7 STS (HuatuoGPT-Vision-7B\n9). The LLM-as-a-judge evaluation in Fig. 5 (a) further confirms superior quality across semantic reasoning\ntasks, with our M3LLM achieving 58.0% win and 17.7% tie compared to MedGemma-27B 27. This substantial\nimprovement directly demonstrates the effectiveness of our context-aware instruction generation paradigm,\nwhich systematically integrates diverse medical findings across multiple imaging perspectives.\nFor the single-image VQA and text-only QA, our M3LLM also achieves superior performance in metrics of\nBLEU@4, ROUGE-L, Semantic Textual Similarity (STS), and the LLM-as-a-judge, compared with state-of-\nthe-art general-purpose and medical MLLMs in Table 2 and 3 and Fig. 5 (b) and (c). It is noteworthy that\nLLaVA-Med-7B 5 shows relatively strong performance on automatic text generation metrics like BLEU@4\nand ROUGE-L. This can be attributed to its pretraining strategy, which is specifically optimized for medical\ncaption generation. While this focus enhances its ability to produce linguistically aligned outputs, it does not\ntranslate as effectively to tasks demanding deeper clinical reasoning. This is highlighted by its significantly\n5\n"}, {"page": 6, "text": "Table 1: Comparison on the PMC-MI-Bench regarding the multi-image VQA.\nMethod\nBLEU@4\nROUGE-L\nBERTScore\nSTS\nLLaVA-7B1\n3.9\n27.1\n58.3\n68.2\nLLaVA-NeXT-7B25\n4.5\n26.5\n59.3\n68.7\nQWen2.5-VL-7B2\n8.5\n29.5\n64.7\n73.9\nInternVL3-8B3\n3.8\n22.5\n55.7\n71.4\nLLaVA-Med-7B5\n5.8\n23.7\n58.8\n63.0\nHuatuoGPT-Vision-7B9\n9.1\n31.4\n66.9\n74.7\nLingshu-7B26\n9.8\n30.4\n66.8\n73.9\nHealthGPT-14B12\n9.3\n30.8\n66.3\n73.7\nMedGemma-27B27\n3.4\n26.7\n62.5\n74.2\nM3LLM-8B (Ours)\n15.0\n37.8\n70.1\n78.2\nTable 2: Comparison on the PMC-MI-Bench regarding the single-image VQA.\nMethod\nBLEU@4\nROUGE-L\nBERTScore\nSTS\nLLaVA-7B1\n2.3\n22.0\n55.3\n70.1\nLLaVA-NeXT-7B25\n2.7\n20.6\n66.0\n67.4\nQWen2.5-VL-7B2\n3.4\n23.5\n55.5\n73.8\nInternVL3-8B3\n6.8\n29.5\n58.7\n78.6\nLLaVA-Med-7B5\n11.6\n34.5\n67.2\n79.4\nHuatuoGPT-Vision-7B9\n9.1\n31.7\n65.0\n79.2\nLingshu-7B26\n10.0\n33.8\n66.3\n80.0\nHealthGPT-14B12\n9.8\n34.0\n67.1\n79.8\nMedGemma-27B27\n2.3\n19.0\n51.9\n73.4\nM3LLM-8B (Ours)\n15.4\n38.4\n65.8\n82.5\nlower accuracy of 46.0% on the multi-choice VQA task (Table 4), where semantic correctness is paramount.\nIn contrast, our M3LLM achieves the highest accuracy of 90.0% in Table 4, outperforming the medical MLLM\nMedGemma-27B 27 with the accuracy of 82.0% and HealthGPT-14B 12 with the accuracy of 88.0%.\nFurthermore, we present the qualitative comparison of our M3LLM and MedGemma-27B 27 in Fig. 9, 10,\n11, 12, 13 and 14 in terms of diverse tasks. These consistent performance advantages fully demonstrate that\nour M3LLM not only has significant advantages in multi-image scenarios, but also can learn effective medical\nknowledge from context-aware instruction tuning in the single-image VQA and text-only QA tasks, as well\nas the multi-choice VQA that existing research focuses on, thereby achieving better prediction answers on\nmultiple tasks and metrics.\n3.2\nPerformance Comparison on Public Medical Benchmarks\nWe further compare our M3LLM with state-of-the-art MLLMs on public single-image medical benchmarks,\nincluding OmniMedVQA 28 and MMMU-Med 29. Extensive evaluation validates that our comprehensive in-\nstruction generation paradigm yields substantial improvements beyond multi-image scenarios, confirming the\npositive transfer effects of systematic medical knowledge integration achieved through our five-stage, context-\naware instruction generation paradigm.\n6\n"}, {"page": 7, "text": "Table 3: Comparison on the PMC-MI-Bench regarding the text-only QA.\nMethod\nBLEU@4\nROUGE-L\nBERTScore\nSTS\nLLaVA-7B1\n9.3\n32.4\n66.1\n81.9\nLLaVA-NeXT-7B25\n9.8\n35.2\n68.3\n83.2\nQWen2.5-VL-7B2\n10.9\n37.7\n70.3\n84.4\nInternVL3-8B3\n9.1\n35.4\n68.3\n83.3\nLLaVA-Med-7B5\n11.5\n36.3\n69.6\n83.5\nHuatuoGPT-Vision-7B9\n11.3\n37.9\n69.1\n83.9\nLingshu-7B26\n11.0\n38.8\n68.3\n85.3\nHealthGPT-14B12\n11.8\n39.0\n69.0\n84.2\nMedGemma-27B27\n7.1\n31.6\n65.3\n82.3\nM3LLM-8B (Ours)\n13.0\n38.5\n73.4\n86.4\nTable 4: Comparison on the PMC-MI-Bench regarding the multi-choice VQA.\nMethod\nAccuracy\nF1\nRecall\nPrecision\nLLaVA-7B1\n66.0\n68.0\n67.6\n68.8\nLLaVA-NeXT-7B25\n70.0\n70.0\n74.4\n72.3\nQWen2.5-VL-7B2\n84.0\n84.0\n86.3\n85.0\nInternVL3-8B3\n78.0\n78.0\n80.9\n79.2\nLLaVA-Med-7B5\n46.0\n47.9\n45.0\n51.1\nHuatuoGPT-Vision-7B9\n74.0\n69.7\n71.9\n75.6\nLingshu-7B26\n90.0\n90.8\n90.8\n91.8\nHealthGPT-14B12\n88.0\n87.5\n89.7\n87.4\nMedGemma-27B27\n82.0\n81.0\n80.5\n83.1\nM3LLM-8B (Ours)\n90.0\n89.9\n91.2\n89.8\nOn the OmniMedVQA benchmark (in Table 5), our M3LLM achieves 85.7% average accuracy across all\nimaging modalities, substantially outperforming both specialized medical MLLMs (e.g., HuatuoGPT-Vision-\n7B 9: 77.9%) and general-purpose models (InternVL3-8B 3: 79.0%). The improvement over the existing\ngeneral-purpose and specialized medical MLLMs demonstrates that the medical knowledge complementation\n(Stage 2) and medical visual perception enhancement (Stage 3) provide richer medical knowledge represen-\ntation compared to conventional single-image focused training approaches. Modality-specific improvements\nare particularly notable in Computed Tomography (CT) (85.1% vs. 78.9% of the best baseline InternVL3-8B\n3) and Magnetic Resonance Imaging (MRI) (89.3% vs. 83.6% of HealthGPT-14B 12), where our systematic\ninstruction generation captures complex visual-clinical relationships essential for radiological diagnosis. X-\nRay analysis shows consistent improvement (88.7% vs. 87.3% of InternVL3-8B 3), while microscopy imaging\ndemonstrates substantial gains (83.6% vs. 82.7% of Lingshu-7B 26), confirming that our multi-image instruc-\ntion generation paradigm enhances understanding across diverse medical imaging modalities. It is noteworthy\nthat the performance of our M3LLM is not the best in all modalities, particularly Ultrasound (US), Fundus\nPhotography (FP), and Dermoscopy (Der). This directly correlates with the modality distribution of training\ndata (Fig. 7), where these modalities are significantly underrepresented (e.g., Ultrasound samples account for\n2.3% and Fundus Photography samples account for 0.4%). The modest performance on these specific tasks\nhighlights the impact of training data diversity and suggests a clear path for future improvement. In general,\nthe exceptional performance of M3LLM in major radiological modalities secures its significant advantage in\noverall average accuracy, confirming the overall effectiveness of our methodology.\n7\n"}, {"page": 8, "text": "Multi-image VQA\nSingle-image VQA\nText-only QA\nBLEU@4\nROUGE-L\nBERTScore\nSTS\nMulti-image VQA\nSingle-image VQA\nText-only QA\nMulti-image VQA\nSingle-image VQA\nText-only QA\nMulti-image VQA\nSingle-image VQA\nText-only QA\n(a)\n(b)\n(c)\n(d)\nLLaVA-7B\nLLaVA-NeXT-7B\nQWen2.5-VL-7B\nInternVL3-8B\nLLaVA-Med-7B\nHuatuoGPT-Vision-7B\nLingshu-7B\nHealthGPT-14B\nMedGemma-27B\nM3LLM-8B (Ours)\nFigure 4: Performance comparison on open-ended text generation tasks within the PMC-MI-Bench. We\ncompare our M3LLM against state-of-the-art general-purpose and specialized medical MLLMs across three\nquestion-answering task types: Multi-image VQA, Single-image VQA, and Text-only QA. Performance is\nevaluated using four standard text generation metrics: (a) BLEU@4, (b) ROUGE-L, (c) BERTScore, and (d)\nSemantic Textual Similarity (STS). The results consistently demonstrate the superior performance of M3LLM\nacross all evaluated tasks and metrics compared to the baseline models.\nFurthermore, MMMU Health & Medicine evaluation in Table 6 confirms consistent superiority across med-\nical specialties, with our M3LLM achieving 62.7% average accuracy compared to the best baseline model\n(InternVL3-8B 3: 57.3%). In particular, the Basic Medical Science (BMS) performance shows particularly\nstrong improvement (63.3% vs. 56.7% of QWen2.5-VL-7B 2), directly reflecting the clinical reasoning capa-\nbilities developed through our comprehensive instruction generation approach. The Clinical Medicine (CM)\nreaches 70.0% versus 66.7% of the baseline QWen2.5-VL-7B 2 and InternVL3-8B 3, demonstrating the en-\nhanced diagnostic reasoning that results from systematic medical knowledge integration. The Public Health\n(PH) (73.3% vs. 63.3% of InternVL3-8B 3) shows consistent improvements, confirming broad medical knowl-\nedge enhancement achieved through our proposed instruction generation paradigm.\n3.3\nAblation Study on Context-aware Instruction Tuning\nWe investigate the performance of our M3LLM to validate the contributions of diverse instructions to substan-\ntial performance gains on the PMC-MI-Bench, OmniMedVQA 28, and MMMU-Med 29 datasets. Specifically,\nwe conduct a detailed ablation study across four types of instructions, including the multi-image VQA, single-\nimage VQA, multi-choice VQA, and text-only QA, by leveraging or removing one of these four instruction\ntypes in the training set. These experiments demonstrate the effects of our comprehensive instruction genera-\ntion approach and identify the relative importance of each instruction category.\nAs illustrated in Table 7, compared to the baseline without instruction tuning (Line 1), we observe that dif-\nferent types of instructions bring significant improvements in Semantic Textual Similarity (STS). On the one\n8\n"}, {"page": 9, "text": "LLaVA-Med-7B\nHuatuoGPT-Vision-7B\nInternVL3-8B\nLingshu-7B\nHealthGPT-14B\nMedGemma-27B\n(a)\n(c)\n(b)\n(d)\nLLaVA-Med-7B\nHuatuoGPT-Vision-7B\nInternVL3-8B\nLingshu-7B\nHealthGPT-14B\nMedGemma-27B\nLLaVA-Med-7B\nHuatuoGPT-Vision-7B\nInternVL3-8B\nLingshu-7B\nHealthGPT-14B\nMedGemma-27B\nLLaVA-Med-7B\nHuatuoGPT-Vision-7B\nInternVL3-8B\nLingshu-7B\nHealthGPT-14B\nMedGemma-27B\nOurs Win\nTie\nOurs Loss \nFigure 5: Comparison of LLM-as-a-judge assessment of our M3LLM against state-of-the-art MLLMs. We\nconduct the assessment using GPT-4o as a judge across multiple tasks on the PMC-MI-Bench, including (a)\nthe overall performance, (b) the multi-image VQA, (c) the single-image VQA, and (d) the text-only QA.\nhand, by adding each type of PMC-MI instructions to the training set, the MLLM can be improved on the\nsame type of samples on the PMC-MI-Bench. In particular, the instructions of multi-image VQA bring a 3.8%\nSTS performance improvement on the same type of the PMC-MI-Bench (Line 2), the instructions of single-\nimage VQA bring a 1.0% STS performance improvement (Line 3), and the instructions for multi-choice VQA\n(Line 4) and text-only QA (Line 5) bring a 2.0% STS performance and 4.0% accuracy improvement, respec-\ntively. On the other hand, these instructions further improve the performance of other tasks, for example, the\nmulti-image VQA instructions can improve the performance of single-image VQA with a STS increase of\n1.9%. This confirms that the generated instructions can provide sufficient medical knowledge to facilitate the\nmodel to better complete various types of downstream tasks. Moreover, by leveraging these diverse types of\ntraining instructions, the tuned models reveal an impressive advantage over the baseline model on the public\nsingle-image benchmarks, with an accuracy increase from 79.0% to 85.7% for OmniMedVQA 28 and from\n59.3% to 62.7% for MMMU-Med 29. These results demonstrate the effectiveness of our designs in the training\ninstruction dataset.\nTo validate the positive transfer effects across different types of training instructions, we further perform the\ninstruction tuning by excluding one type of instruction samples from the training set (Line 6-9 in Table 7).\nBy comparing the M3LLM with all types involved (Line 10), the ablative models confirm that these training\ndata can further promote the performance on different tasks based on other training data, including four tasks\non PMC-MI-Bench, as well as OmniMedVQA 28 and MMMU-Med 29. In particular, single-image VQA\ncan further improve the performance of the model on MMMU-Med by 2.7%. It is worth noting that in the\ntraining instructions, multi-choice VQA has a significant performance gain on the public OmniMedVQA and\nMMMU-Med benchmarks, which shows that the positive transfer of medical knowledge of the same task type\nis effective. Finally, by comprehensively utilizing the four types of instruction samples in the training set,\nour M3LLM achieves the best performance among these downstream tasks, achieving an increase of 6.8%,\n3.9%, 3.1%, and 8.0% in multi-image VQA, single-image VQA, multi-choice VQA, and text-only QA over\nthe baseline (Line 1) on the PMC-MI-Bench, respectively.\n9\n"}, {"page": 10, "text": "Table 5: Comparison of state-of-the-art MLLMs on public OmniMedVQA in terms of different modalities.\nSpecifically, CT denotes Computed Tomography, FP denotes Fundus Photography, MR denotes Magnetic\nResonance Imaging, OCT denotes Optical Coherence Tomography, Der denotes Dermoscopy, Mic denotes\nMicroscopy Images, US denotes Ultrasound.\nMethod\nCT\nFP\nMR\nOCT\nDer\nMic\nX-Ray\nUS\nAvg\nLLaVA-7B1\n30.7\n26.1\n23.9\n24.8\n30.7\n26.4\n23.0\n25.1\n26.0\nLLaVA-NeXT-7B25\n31.7\n27.0\n24.6\n25.7\n30.2\n26.6\n25.8\n25.5\n27.1\nQWen2.5-VL-7B2\n62.5\n70.8\n66.4\n66.2\n68.0\n70.9\n76.8\n35.6\n64.7\nInternVL3-8B3\n78.9\n87.5\n80.4\n77.2\n81.8\n82.2\n87.3\n76.4\n79.0\nLLaVA-Med-7B5\n38.7\n48.3\n39.5\n45.7\n58.8\n49.3\n43.4\n48.1\n46.5\nHuatuoGPT-Vision-7B9\n70.0\n84.2\n72.1\n85.7\n72.5\n75.7\n81.9\n81.2\n77.9\nLingshu-7B26\n77.2\n88.9\n83.3\n89.9\n83.2\n82.7\n86.4\n80.6\n82.8\nHealthGPT-14B12\n70.3\n82.5\n83.6\n88.0\n69.0\n72.7\n83.7\n56.7\n75.8\nMedGemma-27B27\n75.5\n80.8\n66.1\n76.3\n74.0\n66.1\n78.5\n55.8\n70.3\nM3LLM-8B (Ours)\n85.1\n85.5\n89.3\n90.2\n79.8\n83.6\n88.7\n78.2\n85.7\nTable 6: Comparison of state-of-the-art MLLMs on public MMMU-Med in terms of different health and\nmedicine tracks. Specifically, BMS for Basic Medical Science, CM for Clinical Medicine, DLM for Diagnos-\ntics and Laboratory Medicine, P for Pharmacy, and PH for Public Health.\nMethod\nBMS\nCM\nDLM\nP\nPH\nAvg\nLLaVA-7B1\n23.3\n20.0\n26.7\n23.3\n23.3\n23.3\nLLaVA-NeXT-7B25\n20.0\n20.0\n26.7\n33.3\n23.3\n24.7\nQWen2.5-VL-7B2\n56.7\n66.7\n36.7\n56.7\n56.7\n54.7\nInternVL3-8B3\n53.3\n66.7\n43.3\n60.0\n63.3\n57.3\nLLaVA-Med-7B5\n33.3\n40.0\n26.7\n40.0\n53.3\n38.7\nHuatuoGPT-Vision-7B9\n53.3\n70.0\n46.7\n43.3\n40.0\n50.7\nLingshu-7B26\n56.7\n53.3\n60.0\n46.7\n53.3\n54.0\nHealthGPT-14B12\n50.0\n50.0\n43.3\n46.7\n50.0\n48.0\nMedGemma-27B27\n46.7\n53.3\n50.0\n53.3\n43.3\n49.3\nM3LLM-8B (Ours)\n63.3\n70.0\n53.3\n53.3\n73.3\n62.7\nFurthermore, we dive into the multi-image VQA instructions, comprising the VQA regarding the multi-\nsubimage, single-subimage, and the spatial relationship, on the PMC-MI-Bench. Specifically, we implement\nthe ablation study with single-image VQA, multi-choice VQA, and text-only QA of PMC-MI available, by\nadding or removing one type of the multi-image VQA instructions. The details of these multi-image VQA\ninstructions are presented in Stage 4: Context-Question-Answer Instruction Generation of Section 6.2. From\nLine 1 to Line 4 in Table 8, we observe that these three types of multi-image VQA instructions contribute to\nthe performance of multi-image VQA samples in PMC-MI-Bench, with the performance increase of 1.7%,\n0.8%, and 0.2%, respectively. When these instructions are combined, the M3LLM results in the remarkable\nperformance of 78.2%, 82.5%, 86.4% and 90.0% on the multi-image, single-image, text-only, and multi-choice\ntypes of PMC-MI-Bench. These results confirm the impact of the diverse types of instructions on the M3LLM\nto achieve superior performance on multi-image, single-image, multi-choice, and text-only tasks of PMC-MI-\nBench, OmniMedVQA, and MMMU-Med benchmarks.\n10\n"}, {"page": 11, "text": "Table 7: Ablation study of the training data settings on PMC-MI-Bench, OmniMedVQA, and MMMU-Med.\nPerformance is reported using Semantic Textual Similarity (STS) for free-text tasks and Accuracy for the\nmulti-choice task.\nTraining Data Setting\nPMC-MI-Bench\nOmniMedVQA\nMMMU-Med\nMulti-img\nSingle-img\nText-only\nMulti-choice\nMulti-img\nSingle-img\nText-only\nMulti-choice\n1\n71.4\n78.6\n83.3\n82.0\n79.0\n59.3\n2\n✓\n75.2\n80.5\n84.0\n86.0\n81.4\n61.3\n3\n✓\n74.5\n79.6\n84.1\n84.0\n82.4\n60.0\n4\n✓\n73.9\n79.5\n85.3\n84.0\n81.9\n60.7\n5\n✓\n73.4\n80.5\n83.3\n86.0\n80.2\n61.3\n6\n✓\n✓\n✓\n77.1\n80.2\n84.9\n84.0\n84.1\n60.7\n7\n✓\n✓\n✓\n77.3\n81.1\n84.7\n88.0\n83.9\n61.3\n8\n✓\n✓\n✓\n75.8\n80.7\n85.3\n86.0\n85.2\n60.0\n9\n✓\n✓\n✓\n76.4\n81.4\n85.4\n86.0\n84.6\n60.0\n10\n✓\n✓\n✓\n✓\n78.2\n82.5\n86.4\n90.0\n85.7\n62.7\nTable 8: Ablation study of different multi-image training settings on the PMC-MI-Bench. Performance is\nreported using Semantic Textual Similarity (STS) for free-text tasks and Accuracy for the multi-choice task.\nTraining Data Setting of Multi-image VQA\nPMC-MI-Bench\nMulti-subimage\nSingle-subimage\nSpatial relationship\nMulti-img\nSingle-img\nText-only\nMulti-choice\n1\n74.5\n80.2\n85.3\n86.0\n2\n✓\n76.2\n80.8\n85.3\n86.0\n3\n✓\n75.3\n81.4\n85.8\n86.0\n4\n✓\n74.7\n80.1\n85.6\n84.0\n5\n✓\n✓\n77.6\n82.1\n85.7\n88.0\n6\n✓\n✓\n77.1\n80.9\n84.9\n84.0\n7\n✓\n✓\n75.9\n81.7\n85.5\n86.0\n8\n✓\n✓\n✓\n78.2\n82.5\n86.4\n90.0\n3.4\nClinical Validation on Longitudinal Medical Imaging\nTo evaluate the performance of our M3LLM in clinical scenarios requiring longitudinal reasoning, we conduct\nexperiments using the MIMIC chest X-ray dataset, as shown in Fig. 6. The dataset is divided at the patient\nlevel into a training set and a validation set in a 1:1 ratio, ensuring no patient overlap between the two. The\ntraining set is used for fine-tuning all MLLMs, while the validation set is employed for performance evaluation.\nEach data record in the dataset contains two chest X-ray images from different examinations of the same\npatient. For the disease diagnosis task, the first examination image is used as input to predict whether the\npatient has a specific disease. For the progression prediction task, both X-ray images are used to determine the\nprogression of a specific disease, categorizing it as improvement, deterioration, or stability. Furthermore, we\ncalculate the accuracy for cases where both disease diagnosis and progression prediction are correct, offering\na comprehensive effectiveness measure of the MLLMs in longitudinal reasoning.\nThe results, summarized in Fig. 6 (a) and (b), demonstrate that M3LLM outperforms all compared MLLMs\nacross both tasks. Specifically, M3LLM achieves 73.9% accuracy in disease diagnosis, and 45.1% in longitu-\ndinal progression prediction. These results highlight the model’s superior ability in understanding both single-\nimage and longitudinal relationships in medical imaging. Compared to the second-best model, HuatuoGPT-\nVision-7B 9, M3LLM achieves a 6.1% improvement in disease diagnosis accuracy and a 1.4% improvement in\nprogression prediction accuracy. Notably, M3LLM’s superior performance on these two tasks further under-\n11\n"}, {"page": 12, "text": "STS\nLLaVA-NeXT-7B\nQWen2.5-VL-7B\nInternVL3-8B\nHuatuoGPT-Vision-7B\nLingshu-7B\nMedGemma-27B\nM3LLM-8B (Ours)\n(a)\n(b)\n(c)\nAccuracy\nAccuracy\nAccuracy\nDiagnosis\nProgression\nConsolidation\nEdema\nPleural effusion\nPneumonia\nPneumothorax\nFigure 6: Performance comparison on the MIMIC chest X-ray longitudinal benchmark. We compare\nthe performance of our M3LLM against several state-of-the-art MLLMs on clinical validation tasks requiring\nlongitudinal reasoning. (a) The accuracy comparison for the disease diagnosis task based on the first exam-\nination image. (b) The accuracy comparison for the disease progression prediction task (i.e., improvement,\ndeterioration, or stability) using both examination images. (c) Detailed disease diagnosis accuracy comparison\nacross five specific conditions: Consolidation, Edema, Pleural effusion, Pneumonia, and Pneumothorax. The\nresults highlight the superior performance of M3LLM in both diagnosis and progression prediction, as well as\nits strong generalization across various disease types.\nscores its robustness and clinical applicability. To assess the model’s ability to handle different disease types,\nwe analyze its performance across five conditions: consolidation, edema, pleural effusion, pneumonia, and\npneumothorax, as presented in Fig. 6 (c). M3LLM achieves the highest accuracy in three out of five disease\ncategories, including consolidation (80.3%), edema (73.4%), and pneumonia (76.6%), and competitive perfor-\nmance on pneumothorax (79.5% vs. the best 79.7%) and pleural effusion (59.6% vs. the best 61.9%). This\nanalysis confirms the model’s capacity to generalize across various diseases and accurately recognize subtle\nradiological changes associated with each condition. The substantial improvements in disease diagnosis, par-\nticularly for diseases like consolidation and pneumonia, underscore the model’s ability to provide timely and\nprecise predictions, which are critical for effective clinical intervention.\nDespite the challenges inherent in progression prediction, such as the subtlety and complexity of longitudinal\nchanges, M3LLM consistently outperforms the state-of-the-art MLLMs, reflecting the strength of its spatio-\ntemporal reasoning capabilities. The ability to detect nuanced changes in disease progression, whether indica-\ntive of improvement, deterioration, or stability, is vital for longitudinal patient care. The accuracy of MLLMs\non the chest X-ray dataset, which requires simultaneous success in both disease diagnosis and progression pre-\ndiction, further underscores its comprehensive understanding of longitudinal medical imaging. The superior\n12\n"}, {"page": 13, "text": "performance of M3LLM can be attributed to its systematic instruction generation paradigm, which trains the\nmodel to effectively capture progression patterns and spatial-temporal relationships. These results demonstrate\nthe potential of M3LLM to improve clinical decision-making in real-world longitudinal patient care.\n3.5\nTraining Data Scale Analysis\nWe further conduct the systematic evaluation of M3LLM performance across varying training data scales on\nPMC-MI-Bench, as well as public OmniMedVQA 28 and MMMU-Med 29 benchmarks. These results in Table\n9 demonstrate the relationship between instruction data scale and medical multi-image understanding capa-\nbilities, confirming the quality of our instruction generation paradigm and providing insights for optimal de-\nployment strategies. Specifically, the InternVL-3-8B 3 without context-aware instruction tuning serves as the\nbaseline and achieves the accuracy of 79.0% and 59.3% on the OmniMedVQA and MMMU-Med, respectively.\nOn this basis, we increase the ratio of the training set from 0% to 100%. We observe that the M3LLM obtains\nan accuracy increase of 3.1% and 1.4% on OmniMedVQA and MMMU-Med with only 5% of the PMC-MI\ntraining set. Further increasing the number of training samples will continue to improve model performance,\nbut the rate of increase will slow significantly. For example, when the number of training samples reaches\n30%, the performance of M3LLM reaches 83.6% on OmniMedVQA. As we continue to increase the number\nof training samples, the performance increase remains relatively stable until the full training dataset achieves\n85.7% performance. These experimental results show that our instruction data can effectively bring medi-\ncal knowledge to M3LLM. The effect is obvious on a small amount of data, while more data leads to better\nperformance on downstream tasks.\n3.6\nTraining Data Quality Assessment\nWe further conduct the professional medical assessment on the randomly sampled instructions of the PMC-MI\ndataset to confirm high-quality instruction generation across all stages, as well as substantial inter-annotator\nagreement supporting reliable quality assessment. In our implementation, we randomly select 140 training\nsamples from five stages, where five samples are selected from each of the six types in each stage, except\nthat spatial relationship and multi-choice VQA instructions do not need to go through the fifth stage of con-\ntext improvement. Each training sample is evaluated by two medical professionals from the perspectives of\ncorrectness, completeness, and clarity. Each item is scored on a 1, 3, or 5 basis, where 5 means the entire\nsample, including context, question, and answer, is satisfactory, with no error or hallucination, 3 means the\nsample is generally satisfactory, with one or two minor flaws and no significant error, and 1 means the sample\nis unsatisfactory, with significant errors. As illustrated in Table 10, our training samples have generally re-\nceived satisfactory evaluation results, with average scores for correctness, completeness, and clarity exceeding\n4 across different stages. During the data preparation phase, our paradigm performs particularly well in the\nsecond stage of Medical Knowledge Complementation, achieving an average score of 4.8, indicating that it\naccurately provides effective and relevant medical knowledge. In contrast, the third stage of Medical Visual\nPerception Enhancement, proved to be more challenging, with a score of 4.4. This highlights the limitations of\ncurrent medical MLLMs. Notably, the instructions generated in the fourth stage achieved an average score of\n4.6, which was further improved to 4.9 after the fifth stage of Context Refinement. This improvement clearly\ndemonstrates that our pipeline is effective in enhancing the context within the instructions, thereby further\nimproving the overall quality of the instructions.\nInter-annotator agreement analysis. To rigorously quantify the consensus of three medical professionals, we\nconduct the pilot test to assess the instructions for correctness, completeness, and clarity, and calculate the In-\ntraclass Correlation Coefficient (ICC) among their assessments on 78 randomly-sampled cases, and the overall\n13\n"}, {"page": 14, "text": "Table 9: Analysis of training data scale on PMC-MI-Bench, OmniMedVQA and MMMU-Med.\nTraining data ratio\nPMC-MI-Bench\nOmniMedVQA\nMMMU-Med\nOpen-ended tasks\nMulti-choice VQA\n0%\n73.2\n82.0\n79.0\n59.3\n5%\n77.4\n86.0\n82.1\n60.7\n10%\n77.9\n86.0\n82.9\n61.3\n20%\n78.5\n84.0\n83.3\n59.3\n30%\n78.3\n86.0\n83.8\n60.0\n50%\n79.3\n88.0\n84.4\n60.7\n75%\n80.1\n88.0\n84.9\n61.3\n100%\n80.6\n90.0\n85.7\n62.7\nTable 10: Manual assessment of training data quality across five stages in the instruction generation paradigm.\nStage\nCorrectness\nCompleteness\nClarity\nAverage\nStage 1\n4.8±0.2\n4.4±0.4\n4.9±0.1\n4.7±0.2\nStage 2\n4.9±0.1\n4.8±0.3\n5.0±0.0\n4.9±0.1\nStage 3\n4.3±0.1\n4.5±0.3\n4.5±0.0\n4.4±0.1\nStage 4\n4.6±0.1\n4.7±0.2\n4.8±0.2\n4.7±0.1\nStage 5\n4.8±0.1\n4.9±0.1\n5.0±0.1\n4.9±0.1\nICC of 0.816 for all rated aspects indicates excellent reliability. Dimension-specific analyses further confirmed\nthis strong agreement, with an ICC of 0.867 for correctness, 0.751 for completeness, and 0.720 for clarity. This\nrobust statistical consensus is underscored by a high rate of exact agreement (74.8%) and near-perfect agree-\nment within one-score difference (98.3%), confirming a consistent quality assessment across the independent\nmedical professionals. A detailed analysis of the rare disagreements reveals that conflicts primarily involve\nnuanced edge cases in clinical interpretation rather than fundamental accuracy issues. Most disagreements\nconcern completeness assessments where evaluators differed on the optimal level of detail required for a spe-\ncific clinical scenario. Correctness disagreements typically arise in cases involving rare pathological conditions\nor emerging diagnostic criteria, while clarity disagreements focus on the accessibility of technical terminology\nfor different medical specialties.\nPMC-MI dataset characteristics. To ensure comprehensive evaluation across diverse medical scenarios, we\nanalyze the M3LLM with dataset characteristics by randomly sampling 1,000 cases from the PMC-MI dataset.\nEach case is analyzed using GPT-4o to extract key textual information, including image modality and the\nanatomical system involved.\nThe dataset encompasses a wide variety of imaging modalities, as depicted in Fig. 7 (a). The most represented\ncategories are microscopy (24.2%) and histopathology (20.9%), reflecting the critical role of detailed cellular\nand tissue-level imaging in medical diagnostics. Multimodal composite images, which require integration\nacross multiple imaging types, make up 14.2% of the dataset, highlighting the increasing complexity of modern\nmedical imaging scenarios. Advanced radiological modalities, including MRI (10.7%), CT (6.4%), and PET-\nCT (0.9%), ensure sufficient coverage of cross-sectional imaging commonly used in clinical practice. Other\nmodalities, such as ultrasound (2.3%), X-ray (2.3%), and clinical photography (2.2%), provide additional\ndiversity, ensuring the dataset captures a broad spectrum of real-world medical imaging scenarios.\n14\n"}, {"page": 15, "text": "For the anatomical systems, neurological imaging accounts for the largest proportion (23.4%) as shown in\nFig. 7 (b), reflecting the high prevalence of brain and nervous system studies in clinical and research settings.\nMusculoskeletal and cardiovascular systems are also well-represented, contributing 11.0% and 10.3%, respec-\ntively, while gastrointestinal (8.9%) and respiratory (5.3%) systems further ensure diversity. Ophthalmology\n(5.4%), reproductive systems (5.5%), and dermatology (2.3%) are included as specialized areas, ensuring the\nevaluation extends to less common but clinically significant domains.\nThe diversity of the PMC-MI dataset, both in imaging modalities and anatomical systems, ensures that the\nproposed M3LLM is equipped to handle a wide range of real-world medical applications. It enables the model\nto excel in single- and multi-image scenarios, integrate information across varied imaging types, and effec-\ntively reason about complex longitudinal changes. The inclusion of medical multi-modal multiple-image fur-\nther guides the model to synthesize information from diverse sources, a critical requirement for addressing\ncomplex diagnostic challenges. Together, these attributes make the dataset an invaluable resource for driving\nadvancements in medical image understanding and improving the robustness of AI models in clinical practice.\n4\nDiscussion\nThis study introduces M3LLM, a multi-modal LLM tailored to address the unique challenges of medical multi-\nimage understanding. Through its five-stage, context-ware instruction generation paradigm, M3LLM demon-\nstrates superior performance in multi-image understanding, single-image understanding, and longitudinal clin-\nical analysis. By leveraging the training on over 237,000 compound figures, M3LLM bridges the gap between\ncomplex visual biomedical research and real-world clinical applications.\nThe M3LLM represents a significant advancement in handling medical multi-image data, a crucial yet un-\nderexplored aspect of medical AI. It excels by integrating information across multiple sub-images to capture\ncomplex spatial, contextual, and diagnostic relationships, unlike existing models (e.g., LLaVA-Med 5, Med-\nFlamingo 6, HuatuoGPT-Vision 9 and HealthGPT 12) primarily focus on single-image tasks 30–32. This superior\ncapability is evident in its performance on the PMC-MI-Bench for multi-image understanding, where M3LLM\nachieves a STS of 78.2, significantly outperforming HealthGPT (73.7) and LLaVA-Med (63.0) in Table 1.\nThis effectiveness stems directly from our five-stage, context-aware instruction generation paradigm, whose\ncore distinction lies in the explicit modeling and learning of composite reasoning. While advanced MLLMs\nlike InternVL3 3 and QWen2.5-VL 2 possess the architectural capacity for multi-image input, their training\nlacks clinically meaningful fine-tuning specific to medical scenarios and, crucially, does not systematically en-\nforce the synthesis of information across images. They may learn implicit associations when presented with\nmultiple images, but they are not explicitly taught to analyze the spatial, temporal, or cross-modal relationships\nthat define complex medical cases. Our paradigm directly addresses this gap by creating tasks that require the\nmodel to compare sub-images, track changes, or integrate findings from different modalities (e.g., CT and\nhistopathology in Fig. 1). Through this process, we are fundamentally shaping the model’s reasoning capabili-\nties to handle multi-dimensional dependencies explicitly, rather than just fine-tuning for medical content. This\nmethodological advantage is the key driver of M3LLM’s superior performance in complex medical scenarios\nand ensures its alignment with real-world diagnostic workflows.\nComplementing its explicit modeling capabilities, another key strength of M3LLM with multi-image input lies\nin its ability to perform longitudinal analysis, which is critical for tracking disease progression over time. On\nthe basis of the MIMIC chest X-ray longitudinal dataset, M3LLM demonstrated substantial improvements in\npredicting disease progression and integrating temporal relationships across imaging studies. For example,\nM3LLM achieves higher accuracy in identifying both current pathological conditions and future disease trajec-\n15\n"}, {"page": 16, "text": "tories, outperforming baseline models such as HuatuoGPT-Vision 9 and InternVL3 3, which are limited in their\nability to integrate sequential data. This capability reflects the benefits of M3LLM’s context-aware training\ndesign, which specifically incorporates spatial and temporal reasoning tasks. By enabling dynamic analysis of\nlongitudinal imaging data, M3LLM provides a potential solution for chronic disease management, prognosis,\nand follow-ups.\nBeyond these core advantages in multi-image reasoning, M3LLM further demonstrates the strength across\ndiverse datasets, tasks, and input settings. By training on a large-scale dataset derived from the PubMed Cen-\ntral biomedical literature, M3LLM effectively leverages domain-specific knowledge to handle a wide range of\nbenchmarks from various sources, including MIMIC 18,19, OmniMedVQA 28, and MMMU-Med 29, covering\ntasks that span from radiology and pathology to clinical question answering. This showcases how biomedi-\ncal knowledge embedded in PubMed Central data can be utilized to solve problems across distinct domains.\nMoreover, M3LLM also exhibits strong performance across diverse task types, including single-image VQA,\ntext-only QA, and multi-choice VQA. On diverse benchmarks, M3LLM consistently achieved state-of-the-art\nresults, demonstrating its flexibility in adapting to the requirements of different task formats. Unlike existing\nMLLMs that often struggle to generalize beyond single-image VQA, the comprehensive instruction tuning\npipeline of M3LLM allows it to handle diverse settings effectively. These results highlight M3LLM’s ability to\ngeneralize its reasoning capabilities from biomedical literature to a variety of clinical scenarios and task types.\nOn the basis of technical achievements, the clinical implications of M3LLM underscore the potential to trans-\nform real-world healthcare workflows. In practical settings, M3LLM can assist clinicians in synthesizing\ncomplex findings from multi-panel imaging studies, such as integrating MRI, CT, and histopathology images\nto form a unified diagnostic conclusion. This capability reduces the cognitive burden on radiologists and\nsupports faster, more accurate decision-making, particularly in time-sensitive scenarios like emergency care.\nAdditionally, M3LLM’s reliance on routine clinical images and textual data makes it a cost-effective and acces-\nsible solution for low-resource healthcare settings, where access to advanced diagnostic tools is often limited.\nM3LLM is capable of processing free-text health records and dynamic imaging data, which positions it as a\npractical tool for diverse healthcare environments.\nDespite its strengths, this study has limitations that highlight opportunities for further research. First, the per-\nformance of M3LLM relies on the diversity and scale of its training data. In scenarios where training data for\nspecific tasks or rare clinical conditions is limited, the model’s performance may degrade accordingly, e.g.,\nthe underexplored fundus photography and ultrasound imaging as indicated in Table 5. Addressing this limi-\ntation will require curating more diverse datasets, particularly focusing on underrepresented populations, rare\ndiseases, and specialized medical scenarios to ensure robust generalization across all use cases. Second, while\nM3LLM focuses on visual and textual data, integrating additional clinical modalities such as laboratory test\nresults, patient histories, and treatment response data could further enhance its diagnostic capabilities and pro-\nvide a more holistic understanding of patient conditions. Third, while traditional metrics like accuracy, BLEU,\nand ROUGE-L provide useful insights into performance, they may not fully capture the nuances of clinical\nreasoning and decision-making in clinical practice. Developing domain-specific evaluation benchmarks, val-\nidated by medical professionals, will be essential for accurately assessing the model’s utility in real-world\nclinical workflows. By addressing these limitations, future research can further expand the applicability and\nimpact of M3LLM in diverse medical contexts.\nIn conclusion, M3LLM represents a significant step forward in medical AI, offering a robust solution for\nunderstanding and reasoning over medical multi-images. By addressing the challenges of multi-image analysis\nand integrating temporal reasoning, M3LLM sets a new benchmark for multimodal medical AI systems. Its\nscalable, cost-effective, and clinically relevant framework has the potential to transform real-world diagnostic\n16\n"}, {"page": 17, "text": "workflows and improve patient care. Future research should focus on expanding its applications across diverse\nclinical contexts and incorporating additional data modalities to further bridge the gap between biomedical\nresearch and clinical practice.\n5\nComprehensive Datasets and Benchmarks\nCurrent evaluation benchmarks for medical MLLMs predominantly focus on single-image scenarios, which\ninadequately reflect the complexity of real-world clinical practice where physicians integrate multi-modal\nimaging studies for comprehensive diagnosis. To address this critical gap, we introduce the PMC Multiple\nImage (PMC-MI) dataset, a large-scale training dataset for developing robust medical multi-image MLLMs,\nalongside the PMC-MI-Bench, a novel benchmark specifically designed for evaluating medical multi-image\nunderstanding capabilities.\nPMC-MI Dataset. The PMC-MI dataset comprises instruction sets derived from 237,137 medical compound\nfigures harvested from PubMed literature, each paired with rich contextual information. Each compound\nfigure contains an average of 4.97 sub-figures, representing different aspects of the same medical case and\nmirroring the multi-perspective analysis required in clinical practice. Furthermore, the average resolution of\nthe compound figure is 705.1 pixels in width and 599.8 pixels in height. The accompanying textual context\nis substantial, with compound figure captions averaging 102.5 words, associated inline text averaging 188.4\nwords. As demonstrated in Fig. 7, the dataset encompasses diverse medical specialties, including radiology,\nhistopathology, dermatology, and ophthalmology, with imaging modalities ranging from MRI and CT scans\nto microscopic and photographic documentation. Through the sophisticated instruction generation paradigm\n(detailed in Section 6.2), the PMC-MI dataset provides comprehensive training data for developing MLLMs\ncapable of medical multi-image reasoning.\nPMC-MI-Bench. The PMC-MI-Bench serves as our comprehensive evaluation benchmark, featuring 300\ncarefully curated test cases drawn from the diverse pool of processed compound figures. On average, each\nbenchmark case includes 4.81 sub-figures. Furthermore, the average resolution for the compound figure within\nthese benchmark cases is 688.2 pixels in width and 587.4 pixels in height. Each case is accompanied by sub-\nstantial textual context comprising 102.2 words in the main caption, and 191.1 words in associated inline text,\nreflecting the complexity required for rigorous evaluation. The benchmark is structured to assess six distinct\naspects of medical understanding, comprising the three specialized sub-tasks of multi-image VQA (holistic\nmultiple sub-images reasoning, focused single sub-image analysis within a compound figure, and spatial re-\nlationship assessment), alongside standard single-image VQA, text-only QA, and multi-choice VQA. Each\ncategory contains 50 meticulously validated samples, ensuring balanced evaluation across different reasoning\ncapabilities required for clinical practice. Details of the PMC-MI-Bench dataset assessment and proofing are\nprovided in Section 3.6.\nBoth datasets undergo rigorous validation by medical professionals who verify accuracy and clinical relevance.\nPMC-MI-Bench specifically receives independent review from two board-certified physicians who assess the\ndiagnostic appropriateness and medical accuracy of each test case. As such, the PMC-MI dataset enables\ntraining of MLLMs that can handle the multi-image reality of clinical practice, while PMC-MI-Bench provides\nstandardized evaluation beyond simple classification tasks. Together, they capture longitudinal and multi-\nmodal imaging scenarios essential for modern medical AI systems, offering a comprehensive assessment of\nMLLMs for clinical deployment in scenarios requiring integrated analysis of multiple imaging studies.\nOmniMedVQA Benchmark. The OmniMedVQA 28 serves as a broad assessment benchmark for standard\n17\n"}, {"page": 18, "text": "Microscopy: 24.2%\nHistopathology: 20.9%\nMultimodal Composite: 14.2%\nMRI: 10.7%\nCT: 6.4%\nDiagram: 6.0%\nX-Ray: 2.3%\nUltrasound: 2.3%\nClinical Photograph: 2.2%\nFluoroscopy : 1.7%\nOCT: 1.1%\nPET-CT: 0.9%\nOthers: 5.0%\nEndoscopy: 0.9%\nEchocardiography : 0.8%\nFundus photography : 0.4%\nModalities\nNeurologic: 23.4%\nMusculoskeletal: 11.0%\nCardiovascular: 10.3%\nGastrointestinal: 8.9%\nReproductive: 5.5%\nOphthalmologic: 5.4%\nRespiratory: 5.3%\nGenitourinary: 4.3%\nImmune: 3.7%\nOral Maxillofacial: 3.0%\nHematologic/Lymphatic : 2.5%\nDermatologic: 2.3%\nMultisystem: 2.0%\nEndocrine: 1.8%\nIntegumentary: 0.7%\nOtolaryngological: 0.7%\nOthers: 9.2%\nAnatomical \nOrgans\nFigure 7: The distribution of the sampled PMC-MI dataset by modality and anatomical system. (a) The distri-\nbution illustrates the proportion of different imaging modalities, highlighting the diversity across microscopy\n(24.2%), histopathology (20.9%), multimodal composite images (14.2%), and radiological imaging such as\nMRI (10.7%) and CT (6.4%). Other modalities, including ultrasound, X-ray, and clinical photography, ensure\ncomprehensive representation of real-world medical imaging. (b) The distribution shows the distribution of\nanatomical systems covered in the dataset, with the largest proportions attributed to the neurological system\n(23.4%), musculoskeletal system (11.0%), and cardiovascular system (10.3%), alongside significant contribu-\ntions from gastrointestinal, respiratory, and other specialized systems. This distribution ensures comprehensive\nevaluation across diverse medical domains.\nsingle-image medical VQA tasks. Following the evaluation protocol by MedEvalKit 26, we evaluate perfor-\nmance on this benchmark, which comprises 88,996 multi-choice visual questions derived from 82,059 images.\nIt covers eight distinct imaging modalities, including Computed Tomography (CT), Fundus Photography (FP),\nMagnetic Resonance Imaging (MR), Optical Coherence Tomography (OCT), Dermoscopy (Der), Microscopy\n(Mic), X-Ray, and Ultrasound (US). The questions span five types, including Modality Recognition, Anatomy\nIdentification, Disease Diagnosis, Lesion Grading, and Other Biological Attributes. The reported average\nperformance across modalities is weighted based on the number of samples within each modality category 26.\nMMMU-Med Benchmark. The MMMU-Med 29, a specialized subset of the larger MMMU dataset, pro-\nvides a focused evaluation benchmark specifically for assessing single-image understanding capabilities in\nthe medical domain. For our evaluation, we utilize the labeled validation set, which consists of 150 closed-\nended multiple-choice visual questions. These questions are evenly distributed across five distinct biomedical\nsubjects, with 30 questions per subject. The subjects of MMMU-Med cover Basic Medical Science (BMS),\nClinical Medicine (CM), Diagnostics and Laboratory Medicine (DLM), Pharmacy (P), and Public Health (PH).\nMIMIC Longitudinal Chest X-ray Benchmark. For clinical longitudinal validation, we utilize chest X-\nray images sourced from the MIMIC database 18,19, obtained under appropriate CITI approval and fully de-\nidentified following HIPAA guidelines. The benchmark dataset comprises 1,326 pairs of sequential chest\nX-ray examinations from individual patients, each paired with ground-truth labels. It specifically focuses\non assessing disease progression across five common radiological findings: Consolidation, Edema, Pleural\neffusion, Pneumonia, and Pneumothorax. For each finding, the progression between the two examinations\nis categorized into one of three states: Improving, Stable, or Worsening. To ensure a rigorous evaluation\nthat prevents data leakage, the dataset is split into training and test sets at the patient level. This benchmark\nstructure allows us to evaluate the model’s longitudinal reasoning capabilities in a setting that mirrors real-\n18\n"}, {"page": 19, "text": "world diagnostic workflows where clinicians compare serial images to monitor patient status.\n6\nMethods\n6.1\nPreparation and Processing of PMC Multiple-Image Dataset\nWe construct a large-scale training corpus by harvesting the open-access subset of PubMed Central. As of\nJune 18, 2024, the repository contained 6,106,189 papers. From this vast collection, we implement a rigorous\nthree-step filtering pipeline to curate a high-quality dataset specifically for medical multi-image understanding.\nStep 1: Preliminary Filtering. We first filter the papers to retain only those with licenses permitting re-\nsearch use, reducing the pool to 5,099,175 articles. To efficiently identify relevant content, we employ a\nfine-tuned PubmedBERT 33 to classify image-caption pairs based solely on their textual content. This text-\nbased pre-screening allows us to rapidly identify 3.7 million potential medical image-text pairs from the 5.1\nmillion papers. We employ a Vision Transformer (ViT) fine-tuned for compound figure detection to effectively\ndistinguish compound figures from single-panel images and non-medical graphics. As a result, we identify\n3,156,144 medical compound figures, excluding 643,401 non-compound or irrelevant images.\nStep 2: Medical Content Screening. In the second step, we ensure the medical relevance of the images. We\nfurther refine this set to ensure high clinical relevance, and employ a specialized DenseNet-121 34, pretrained\non the ImageCLEF 35, MedICaT 36, and DocFigure 37 datasets, to distinguish genuine medical imagery from\nnon-medical graphics such as charts and diagrams. As a result, this step retains compound figures only if\nmedical sub-images constitute over 90% of their visual content.\nStep 3: Textual Quality Control. In the third step, we apply textual quality controls by establishing mini-\nmum length thresholds to guarantee sufficient context for instruction generation. We require compound-level\ncaptions to exceed 50 words and individual sub-image captions (if available) to contain at least 10 words.\nThe application of these three-step criteria systematically refines the initial harvest, resulting in a final, high-\nquality collection of 237,137 compound figures suitable for the subsequent instruction generation process,\nforming the basis for both the training dataset and the benchmark.\nPMC-MI Dataset Generation. Our instruction generation paradigm comprises five interconnected stages\ndesigned to maximize information extraction from these filtered medical compound figures.\nWe employ\nQWen2.5-32B 38 for automatic summarization of inline texts and medical terminology extraction, the advanced\nmedical MLLM HuatuoGPT-Vision-34B 9 for sub-image analysis, and template-based generation combined\nwith large language model creativity to produce four distinct question types covering comprehensive medical\nimage understanding scenarios. The specific details of this five-stage paradigm are elaborated in Section 6.2.\nPMC-MI-Bench Curation Process and Professional Examination. From the larger pool of processed data,\nwe randomly select diverse samples across medical specialties and imaging modalities for benchmark construc-\ntion. Each potential benchmark sample then undergoes a rigorous preliminary screening for medical relevance,\ncomplexity suitable for benchmarking, and educational value. This process prioritizes cases that exemplify dif-\nferent aspects of compound figure understanding (e.g., spatial, temporal, cross-modal analysis) while ensuring\nbalanced representation across six defined question categories. Finally, these screened candidates undergo\nintensive professional validation distinct from the automated dataset generation process. Two board-certified\n19\n"}, {"page": 20, "text": "An X-ray of the abdomen showed \ngaseous distention of the stomach and \nproximal duodenum, characterizing \nthe typical double-bubble sign (Fig-A) \n…\nMultiple-Choice VQA\nPure-text QA\nSingle-Image VQA\nMultiple-Image VQA\nInline Text \nSummarization\nMedical Knowledge\nComplementation\nMedical Visual \nPerception Enhancement\nStructural Instruction \nGeneration\nLeakage-Prevented \nContext Refinement\nFig-A: X-ray of the abdomen, showing \ngas distention of the stomach and \nduodenum, with little gas seen \ndistally …\nCaption\nInline Text\nCompound  image\nIn a 12-day-old female with \npostprandial vomiting, weight loss …\nFig-A: X-ray of the abdomen, showing \ngas distention of the stomach and \nduodenum, with little gas seen \ndistally …\nCaption\nSummarized Inline Text\nAnnular pancreas is a congenital \nanomaly in which pancreatic tissue \nencircles the duodenum, causing \nextrinsic duodenal obstruction, \nclassically in neonates …\nFig-A: X-ray of the abdomen, showing \ngas distention of the stomach and \nduodenum, with little gas seen \ndistally …\nCaption\nMedical Knowledge Notes\nIn a 12-day-old female with \npostprandial vomiting, weight loss …\nSummarized Inline Text\nFig-A shows an \nabdominal radiograph \nwith prominent gaseous \ndistention of  …\nFig-C demonstrates \nperi-duodenal \npancreatic tissue \nsurrounding .…\nFig-B ultrasound depicts \nechogenic pancreatic \nparenchyma …\nFig-D reveals a \nconstricting ring of \npancreatic tissue \naround the second .…\nMultiple Subimages\nVisual Perception Descriptions\nThe selected sub-images \nFig-B and Fig-C are \nabdominal ultrasounds. \nThey depict a fluid-filled …\nContext\nSelected subimages\nPMC-MI Dataset\nThe selected Fig-B and Fig-C \nare abdominal …\nContext\nBased on Fig-B and Fig-C, \nwhat ultrasound features \nsupport extrinsic……\nQuestion\nThe duodenal lumen is \ndilated proximally and is ……\nAnswser\nRaw Data\nFigure 8: Illustration of the five-stage, context-aware instruction generation paradigm. The paradigm\ninvolves the inline text summarization, medical knowledge complementation, medical visual perception en-\nhancement, structural instruction generation, and leakage-prevented context refinement. As such, the paradigm\nis performed on the collected PMC biomedical literature, and generates the high-quality PMC-MI dataset for\ntraining and manually proofed PMC-MI-Bench for evaluation.\nmedical professionals independently review each candidate, verifying information consistency between con-\nstructed contexts and original source materials, ensuring answer accuracy and completeness, and confirming\ndiagnostic appropriateness. Inter-annotator agreement exceeds 85% across all evaluation criteria, with dis-\nagreements resolved through expert consultation. This multi-step process ensures PMC-MI-Bench maintains\nhigh quality, clinical relevance, and balanced distribution across question categories, medical specialties, and\nimaging complexity.\nQuality Control Measures. Both the training dataset and benchmark employ automated filtering to remove\nquestions with potential answer leakage, factual inconsistencies, or inadequate medical complexity. PMC-MI-\nBench additionally undergoes manual verification of each test case to ensure benchmark reliability and clinical\nrelevance.\n6.2\nContext-Aware Instruction Generation Paradigm\nWe develop a five-stage, context-aware instruction generation paradigm designed to systematically transform\nmedical compound figures and their associated textual descriptions into comprehensive training data. Cen-\n20\n"}, {"page": 21, "text": "tral to our approach is a divide-and-conquer strategy that decomposes the complex challenge of multi-image\ncontext-aware instruction generation into a sequence of manageable, specialized sub-tasks. This instruction\ngeneration paradigm addresses the fundamental challenge of creating instruction data that captures not only\nvisual content but also the complex clinical reasoning, medical knowledge integration, and multi-image rela-\ntionship understanding required for effective medical compound figure analysis.\nStage 1: Inline Text Summarization. To address the challenges posed by the often lengthy and verbose in-\nline text from biomedical literature, we employ the QWen2.5-32B 38 to systematically analyze and process the\ninline text regarding each compound figure. Inline texts in biomedical literature often contain essential clinical\ninsights relevant to compound figure analysis and patient diagnosis, yet their verbosity can hinder the learning\nand understanding processes of MLLMs. To overcome this, our summarization stage goes beyond simple text\nextraction by condensing and reorganizing these inline references into concise and coherent clinical narratives.\nThis process focuses on highlighting key information directly related to compound figures and patient diagno-\nsis, such as pathological findings, diagnostic workflows, and treatment outcomes, while removing extraneous\ndetails. The resulting summarized texts not only preserve medical accuracy but also streamline complex tech-\nnical descriptions into accessible formats, laying a foundational clinical context for the subsequent instruction\ngeneration paradigm. The prompt of this stage is illustrated in Fig. 15, and an example is elaborated in Fig.\n26.\nStage 2: Domain-Specific Medical Knowledge Complementation. Building upon Stage 1, this stage utilizes\nQWen2.5-32B 38 to analyze the compound figure captions and the inline text summaries to systematically\nextract and elaborate on key medical concepts critical to understanding each case. This stage first identifies\nkey medical concepts, such as symptoms, pathologies, diagnostic procedures, and treatment approaches, and\nthen generates comprehensive explanations for each, including their clinical significance, diagnostic criteria,\nimaging characteristics, and relationships to other medical conditions. This process ensures that the instruction\ndata is enriched with sufficient domain-specific medical knowledge, facilitating accurate clinical reasoning\nand diagnosis. We manually check sampled training data, confirming that the medical concepts extracted and\nelaborated by the LLM are both accurate and relevant to the case. By integrating this domain-specific medical\nknowledge into the paradigm, we provide the rich medical context essential for effective medical compound\nfigure analysis. The prompt of this stage is illustrated in Fig. 16, and an example is elaborated in Fig. 27.\nStage 3: Multi-Modal Medical Visual Perception Enhancement. While Stage 1 and Stage 2 provide rich\ntextual information, the context-aware instructions also require accurate visual knowledge to bridge the gap\nbetween text and medical images. To achieve this, it is critical to precisely analyze the content of medical\ncompound figures, which consist of multiple subimages with distinct clinical implications. Given the difficulty\ncurrent MLLMs face in processing compound figures holistically, we adopt a divide-and-conquer strategy,\nwhere each subimage is pre-segmented and analyzed individually using HuatuoGPT-Vision-34B 9. This ap-\nproach captures detailed visual features such as anatomical structures, pathological findings, imaging artifacts,\nand diagnostic characteristics, and provides links between visual findings and the textual context established\nin earlier stages. By synthesizing these subimage descriptions, we create a comprehensive understanding of\nthe compound figure. This stage is essential for enriching our instruction dataset with precise multi-modal\nknowledge, enabling the M3LLM to reason effectively across both textual and visual domains and enhancing\nits diagnostic and clinical reasoning capabilities. The prompt of this stage is illustrated in Fig. 17, and an\nexample is elaborated in Fig. 28.\nStage 4: Context-Question-Answer Instruction Generation. Building on the textual outputs from the pre-\nvious stages, this stage constructs a diverse and clinically relevant instruction dataset designed to enhance the\nM3LLM with multi-modal reasoning. The instructions are categorized into four major types, each addressing\n21\n"}, {"page": 22, "text": "specific challenges in medical image analysis and question answering. Note that the ablation study of these\nfour major types is presented in Table 7. (1) Multi-Image VQA focuses on improving the M3LLM’s ability\nto accurately analyze compound figures. This involves providing at least two input images and posing ques-\ntions that require integration of information across multiple sub-images. Multi-Image VQA is further divided\ninto three distinct subtypes: (a) questions that require synthesizing information from multiple sub-images to\nprovide holistic case assessments, mimicking real-world clinical scenarios; (b) questions focusing on detailed\nunderstanding of a single specific sub-image while maintaining awareness of the broader context of the in-\nput compound figure; and (c) questions distinguishing spatial relationships between two specific sub-images,\nsuch as their relative positioning or alignment. This category is pivotal for enabling the M3LLM to handle the\ncomplexity of medical compound figures and is a key driver for improving its diagnostic accuracy in multi-\nimage settings. We further present the ablation study on these three types of multi-image VQA instruction in\nTable 8. (2) Single-Image VQA ensures the model retains its ability to handle simpler but equally important\ntasks. This focuses on scenarios where only one image is available for analysis, testing the model’s capacity\nfor detailed visual understanding, reasoning, and diagnostic insight from individual medical images. While\nless complex than composite analysis, this category remains essential for many real-world applications. (3)\nText-only QA evaluates the M3LLM’s ability to process medical questions in a text-only context. This ensures\nthat the model’s medical knowledge and reasoning capabilities remain robust even without visual input, allow-\ning it to handle a wide range of clinical scenarios where textual information dominates. These tasks test the\nmodel’s understanding of medical concepts, clinical reasoning, and its ability to connect textual information\nwith broader clinical knowledge. (4) Multi-Choice VQA introduces structured multi-choice questions, which\nare a common format in public benchmarks. These tasks assess the model’s ability to apply medical knowledge\nand diagnostic reasoning in a constrained and highly structured format. This category ensures that the M3LLM\nperforms well on widely-used evaluation standards while maintaining consistency across different question\nformats. By integrating these four instruction types, this stage creates a comprehensive dataset that not only\nstrengthens the M3LLM’s ability to analyze compound figures but also ensures its robustness in single-image\nreasoning, text-based medical question answering, and structured multi-choice formats. The prompt of this\nstage is illustrated in Fig. 18, 19, 20, 21, 22 and 23 for different tasks, and an example is elaborated in Fig. 29.\nStage 5: Leakage-Prevented Context Refinement. The final stage prevents the answer leakage issue in\ncontext-aware instruction generation. When constructing the paired context, question, and answer using LLMs,\nthe process relies on the same source text, which can lead to the context including key information that reveals\nor hints at the correct answer. This issue undermines the challenge posed to the model, reducing the effec-\ntiveness of training and evaluation by allowing the model to rely on cues rather than genuine reasoning. To\nresolve this, we implement a rigorous refinement process using advanced language models to systematically\nreview generated instructions. This involves detecting and removing unintended answer-related information\nin the context through analysis of linguistic patterns, medical terminology, and logical relationships between\ncontext and questions. By ensuring the context remains informative yet neutral, this stage preserves the in-\ntegrity of training data, creating meaningful challenges that reflect authentic clinical reasoning. This not only\nenhances the model’s training efficacy but also ensures its performance is rooted in true understanding and\ninference, rather than exploiting unintended context cues. The prompt of this stage is illustrated in Fig. 24, and\nan example is elaborated in Fig. 30.\nIn summary, the five-stage, context-aware instruction generation paradigm creates a comprehensive corpus\nof training data that systematically develops multiple competencies essential for medical multi-image under-\nstanding. Each generated instruction pair undergoes final validation to ensure clinical accuracy, educational\nappropriateness, and alignment with real-world diagnostic workflows. The resulting training data encompasses\ndiverse medical specialties, imaging modalities, and clinical scenarios, providing comprehensive coverage of\nmedical multi-image understanding requirements.\n22\n"}, {"page": 23, "text": "6.3\nMedical Multi-Image MLLM Architecture\nOur M3LLM adopts a streamlined architecture optimized for medical compound figure understanding. The\nframework comprises three core components, including a Vision Transformer (ViT) 39 for comprehensive med-\nical image feature extraction across multiple sub-images, a connector module consisting of two fully connected\nlayers for visual-to-text alignment, and a LLM 38 for sophisticated clinical reasoning and text generation. This\narchitecture maintains computational efficiency while enabling complex multi-image understanding through\nour innovative instruction generation paradigm. In our implementation, we select the InternVL-3-8B 3 as the\nbase model to fine-tune on the PMC-MI dataset, where the InternViT 3,39 serves as the visual encoder and\nQWen2.5-7B 38 serves as the LLM within the MLLM architecture.\nUnlike previous approaches that focus primarily on architectural modifications, our framework achieves multi-\nimage understanding through sophisticated training data preparation and instruction generation. The ViT pro-\ncesses each sub-image within medical compound figures, generating rich visual representations that capture\nboth individual image characteristics and cross-image relationships. The connector module facilitates seam-\nless integration of these multi-perspective visual features with textual medical knowledge, enabling the LLM\nto perform comprehensive clinical reasoning across multiple imaging modalities.\n6.4\nTraining Methodology and Optimization\nMulti-Stage Training Protocol. Our training methodology implements a carefully designed multi-stage proto-\ncol that progressively develops the model’s capabilities from basic medical knowledge acquisition to sophisti-\ncated multi-image reasoning. The initial training stage focuses on fundamental medical concept understanding\nusing single-image instructions, establishing a solid foundation of medical domain knowledge. Subsequent\nstages introduce increasingly complex multi-image scenarios, enabling the model to develop cross-image rea-\nsoning capabilities while maintaining accuracy in individual image analysis.\nInstruction Diversity and Clinical Relevance Optimization. Throughout the training process, we maintain\na careful balance between instruction complexity, clinical relevance, and educational value. Our methodology\nensures that training data encompasses diverse clinical scenarios, including emergency diagnostics, longitu-\ndinal patient monitoring, multi-modal imaging integration, and specialist consultations. This comprehensive\napproach enables the model to handle the full spectrum of medical multi-image understanding requirements\nencountered in real clinical practice.\nTraining Configuration. To finetune our M3LLM, we use the AdamW optimizer with hyperparameters β1 =\n0.9, β2 = 0.999, and ε = 1 × 10−8. The initial learning rate is set to 5 × 10−5, and we apply a cosine decay\nschedule with a warmup ratio of 0.03 to control the learning rate. Training is conducted for 3 epochs using a\nper-device batch size of 1 and a gradient accumulation step of 1. For regularization, we use a weight decay\nof 0.05 and apply gradient clipping with a maximum norm of 1.0. To enhance the training process, mixed-\nprecision training (bf16) is enabled. The random seed is fixed to 42 to ensure reproducibility. We utilize the\nAdamW implementation from the HuggingFace Transformers framework and use Weights & Biases to track\nthe training and experimental results.\n23\n"}, {"page": 24, "text": "7\nComputing Hardware and Software\nWe use Python (version 3.12) for all experiments and analyses in this study, which can be replicated using\nthe open-source libraries outlined below. All computations are executed on the Yale Misha high-performance\ncomputing platform, utilizing NVIDIA H200 GPUs with mixed precision support to ensure reproducible and\nscalable experimentation. Experiments are based on the PyTorch framework (version 2.7.0) and torchvision\n(version 0.22.0), leveraging the NVIDIA CUDA toolkit (CUDA 12.6, cuDNN 9.5, NCCL 2.26) for GPU\nacceleration, thereby enabling efficient large-scale multimodal model training. For implementation, we utilize\nthe Transformers library (version 4.52.4) and the PEFT library (version 0.17.1) for model configuration and\nparameter-efficient fine-tuning. Concurrently, DeepSpeed (version 0.17.5) and xFormers (version 0.0.30) are\nemployed for distributed optimization and memory-efficient attention computation. Data preprocessing and\nanalysis are conducted using NumPy (version 2.2.6), Pandas (version 2.3.0), scikit-learn (version 1.7.1), and\nSciPy (version 1.15.3). Model evaluation involved Hugging Face Datasets (version 3.6.0), Evaluate (version\n0.4.5), BERTScore (version 0.3.13), and ROUGE (version 1.0.1) libraries for calculating relevant metrics. For\nmultimodal inference, we integrate vLLM (version 0.9.0.1), OpenCLIP (version 2.32.0), and LLaVA (version\n1.7.0.dev0).\n8\nEvaluation Metrics\nEvaluating medical MLLMs presents unique challenges due to the diverse output formats and clinical reasoning\nrequirements. We employ a comprehensive framework for benchmarking and validation that addresses both\nopen-ended text generation and multi-choice question answering scenarios, ensuring robust assessment of\nmodel capabilities across different clinical tasks.\n8.1\nOpen-Ended Text Generation Evaluation\nOpen-ended medical text generation requires precise evaluation beyond simple text matching, as clinical ac-\ncuracy and completeness are paramount for patient safety. To assess different aspects of response quality, we\nadopt a multi-faceted strategy that combines string-based metrics, semantic similarity assessment, and LLM-\nas-a-judge evaluation.\nString-Based Metrics. We employ BLEU 40 and ROUGE 41 to provide a baseline evaluation of linguistic\nsimilarity by quantifying n-gram overlap between model outputs and references. BLEU 40 measures precision,\nwhile ROUGE 41, particularly ROUGE-L, emphasizes recall-oriented similarity. In practice, these metrics are\nlimited in capturing the nuanced meanings of medical language, where small variations in terminology (e.g.,\nmyocardial infarction vs. heart attack) can significantly impact clinical interpretation.\nSemantic Similarity Assessment. To evaluate deeper semantic alignment beyond string matching, we use\ntwo complementary metrics: BERTScore 42 and Semantic Textual Similarity (STS) 43. BERTScore focuses on\ntoken-level semantic overlap. It computes contextual embeddings for each token in the prediction and reference\ntext, performing optimal matching to derive the F1 score. This metric excels at assessing content fidelity and\ncoverage, especially for domain-specific terminology, as it effectively handles paraphrasing and synonyms. In\ncontrast, STS evaluates overall semantic equivalence, typically at the sentence level. It encodes each text into a\nsingle vector representation and calculates cosine similarity, providing a holistic score that reflects whether two\ntexts convey the same meaning, regardless of wording differences. By combining these metrics, we achieve\na comprehensive evaluation: BERTScore provides fine-grained insights into lexical and semantic alignment,\n24\n"}, {"page": 25, "text": "while STS offers a high-level measure of semantic similarity. We use the DeBERTa model 44 for BERTScore\nand MiniLM 45 for STS. This ensures a robust assessment of generated text, capturing nuanced semantic\ndifferences that string-based metrics might overlook.\nLLM-as-a-judge Assessment. To achieve a comprehensive evaluation, we employ the LLM-as-a-judge ap-\nproach 46 to leverage the evaluative LLM capability to provide a scalable assessment. This approach sup-\nplements traditional string-based and semantic metrics by incorporating human-like judgment to evaluate the\nnuanced quality of generated medical text. Specifically, we utilize GPT-4o 47 as the LLM judge to compare the\noutputs of our M3LLM against state-of-the-art MLLMs. For each sample in the evaluation dataset, we provide\nthe LLM judge with a manually-proofed reference answer, alongside the outputs from both M3LLM and the\ncompeting MLLM, with the prompt illustrated in Fig. 25. Following the assessment protocol 48, the LLM\njudge compares the two generated outputs and determines which one more closely aligns with the reference\nanswer, by assigning one of three possible outcomes for each sample: win for M3LLM, lose for the competing\nMLLM, or tie when neither output demonstrates a clear advantage. To quantify the overall performance, we\ncalculate the average scores across the entire dataset by aggregating the win, tie or lose results.\n8.2\nMulti-Choice VQA Evaluation\nFor multi-choice VQA tasks, we evaluate the performance of state-of-the-art MLLMs using accuracy as the\nprimary metric, adhering to the implementation protocols established in Lingshu 26. This evaluation assesses\nthe model’s capability to accurately interpret multi-choice instructions and select the correct response from a set\nof options. We construct standardized inputs where visual features are prefixed to text embeddings containing\nthe specific question and candidate options. To ensure rigorous evaluation, we tailor the system instructions\nto the specific requirements of each benchmark. For our PMC-MI-Bench, we utilize the prompt: You are\na medical expert who is good at solving medical multi-choice tasks. Please answer with the option letter\nonly. The Question is: <Question>. The candidate options are: <Options>. For public benchmarks such\nas OmniMedVQA and MMMU-Med, we adopt the default system prompt from the MedEvalKit codebase 26:\nAnswer with the option’s letter from the given choices directly. The Question is: <Question>. The candidate\noptions are: <Options>. Final predictions are determined by comparing the option letter generated by the\nmodel with the correct option letter from the ground truth. A prediction is considered correct only if the\ngenerated option letter matches the ground truth exactly, and these results are used to calculate accuracy.\n9\nData Availability\nThe PMC-MI dataset, utilized for training M3LLM, and PMC-MI-Bench, designed for comprehensive evalu-\nation, are publicly available for access and download via OneDrive (link). Detailed information on the usage\ncan be found on the Hugging Face repository (https://huggingface.co/datasets/KerwinFu/M3LLM-PMC).\n10\nCode availability\nThe framework used for this study can be found at GitHub (https://github.com/franciszchen/M3LLM). The\nanalysis framework to evaluate all results, generate all plots, and perform all statistical analyses can be found\nat Google Drive (the link). All code uses Python (v3.12), PyTorch (v2.7.0), torchvision (v0.22.0), Transformers\n(v4.52.4), PEFT (v0.17.1), DeepSpeed (v0.17.5), xFormers (v0.0.30), NumPy (v2.2.6), Pandas (v2.3.0), scikit-\nlearn (v1.7.1), SciPy (v1.15.3), Hugging Face Datasets (v3.6.0), Evaluate (v0.4.5), BERTScore (v0.3.13),\nROUGE (v1.0.1), vLLM (v0.9.0.1), OpenCLIP (v2.32.0), and LLaVA (v1.7.0.dev0).\n25\n"}, {"page": 26, "text": "11\nFunding\nThis study is supported by the National Institutes of Health National Library of Medicine under Award Number\nR01LM014604 and R00LM014024.\n12\nCompeting Interests\nThe authors declare no competing interests.\n13\nOnline Content\nAny methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supple-\nmentary information, acknowledgments, peer review information, details of author contributions and compet-\ning interests, and statements of data and code availability are available at the link.\n26\n"}, {"page": 27, "text": "14\nReferences\n1. Liu, H., Li, C., Wu, Q. & Lee, Y. J. Visual instruction tuning. NeurIPS 36, 34892–34916 (2023).\n2. Bai, S. et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025).\n3. Chen, Z. et al. Expanding performance boundaries of open-source multimodal models with model, data,\nand test-time scaling. arXiv preprint arXiv:2412.05271 (2024).\n4. Liu, F. et al. Application of large language models in medicine. Nature Reviews Bioengineering 1–20\n(2025).\n5. Li, C. et al. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. NeurIPS\n36, 28541–28564 (2023).\n6. Moor, M. et al. Med-flamingo: a multimodal medical few-shot learner. In Machine Learning for Health\n(ML4H), 353–367 (PMLR, 2023).\n7. Wu, C. et al. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical\ndata. Nature Communications 16, 7866 (2025).\n8. Zhang, K. et al. A generalist vision–language foundation model for diverse biomedical tasks. Nature\nMedicine 30, 3129–3141 (2024).\n9. Chen, J. et al. Towards injecting medical visual knowledge into multimodal LLMs at scale. In EMNLP,\n7346–7370 (2024).\n10. Xie, Y. et al.\nMedtrinity-25m: A large-scale multimodal dataset with multigranular annotations for\nmedicine. In ICLR (2025).\n11. Zhu, X., Hu, Y., Mo, F., Li, M. & Wu, J. Uni-med: a unified medical generalist foundation model for\nmulti-task learning via connector-moe. In NeurIPS, vol. 37, 81225–81256 (2024).\n12. Lin, T. et al. Healthgpt: A medical large vision-language model for unifying comprehension and genera-\ntion via heterogeneous knowledge adaptation. In ICML (2025).\n13. McKinney, S. M. et al. International evaluation of an ai system for breast cancer screening. Nature 577,\n89–94 (2020).\n14. Ferber, D. et al. In-context learning enables multimodal large language models to classify cancer pathology\nimages. Nature Communications 15, 10104 (2024).\n15. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259–265\n(2023).\n16. Xing, W., Zhu, T., Wang, J. & Liu, B. A survey on mllms in education: application and future directions.\nFuture Internet (2024).\n17. Sabuncu, M. R. et al. Event time analysis of longitudinal neuroimage data. NeuroImage 97, 9–18 (2014).\n18. Johnson, A. E. et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific data 10, 1\n(2023).\n19. Bannur, S. et al. Learning to exploit temporal structure for biomedical vision-language processing. In\nCVPR, 15016–15027 (2023).\n20. Sala, E. et al. Unravelling tumour heterogeneity using next-generation imaging: radiomics, radiogenomics,\nand habitat imaging. Clinical radiology 72, 3–10 (2017).\n21. Rougier, N. P., Droettboom, M. & Bourne, P. E. Ten simple rules for better figures (2014).\n27\n"}, {"page": 28, "text": "22. Takaya, Y. & Ito, H. New horizon of fusion imaging using echocardiography: its progress in the diagnosis\nand treatment of cardiovascular disease. Journal of echocardiography 18, 9–15 (2020).\n23. Tae, W.-S., Ham, B.-J., Pyun, S.-B. & Kim, B.-J. Current clinical applications of structural mri in neuro-\nlogical disorders. Journal of Clinical Neurology 21, 277 (2025).\n24. Langlotz, C. P. et al. A roadmap for foundational research on artificial intelligence in medical imaging:\nfrom the 2018 nih/rsna/acr/the academy workshop. Radiology 291, 781–791 (2019).\n25. Li, F. et al. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv\npreprint arXiv:2407.07895 (2024).\n26. Xu, W. et al. Lingshu: A generalist foundation model for unified multimodal medical understanding and\nreasoning. arXiv preprint arXiv:2506.07044 (2025).\n27. Sellergren, A. et al. Medgemma technical report. arXiv preprint arXiv:2507.05201 (2025).\n28. Hu, Y. et al. Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm. In\nCVPR, 22170–22183 (2024).\n29. Yue, X. et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\nfor expert agi. In CVPR, 9556–9567 (2024).\n30. Lin, W. et al. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In Interna-\ntional Conference on Medical Image Computing and Computer-Assisted Intervention, 525–536 (Springer,\n2023).\n31. Zhang, S. et al. Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million\nscientific image-text pairs. arXiv preprint arXiv:2303.00915 (2023).\n32. Lozano, A. et al. Biomedica: An open biomedical image-caption archive, dataset, and vision-language\nmodels derived from scientific literature. In Proceedings of the Computer Vision and Pattern Recognition\nConference, 19724–19735 (2025).\n33. Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing.\nACM Transactions on Computing for Healthcare (HEALTH) 3, 1–23 (2021).\n34. Huang, G., Liu, Z., Van Der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, 4700–4708 (2017).\n35. Ionescu, B. et al. Overview of the imageclef 2024: Multimedia retrieval in medical applications. In\nInternational Conference of the Cross-Language Evaluation Forum for European Languages, 140–164\n(Springer, 2024).\n36. Subramanian, S. et al. Medicat: A dataset of medical images, captions, and textual references. arXiv\npreprint arXiv:2010.06000 (2020).\n37. Jobin, K., Mondal, A. & Jawahar, C. Docfigure: A dataset for scientific document figure classification.\nIn 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), vol. 1,\n74–79 (IEEE, 2019).\n38. Team, Q. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115 (2025).\n39. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. In\nICLR (2021).\n40. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 311–318 (2002).\n28\n"}, {"page": 29, "text": "41. Banerjee, S. & Lavie, A. Meteor: An automatic metric for mt evaluation with improved correlation with\nhuman judgments. In ACL Workshop, 65–72 (2005).\n42. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y. Bertscore: Evaluating text generation with\nbert. arXiv preprint arXiv:1904.09675 (2019).\n43. Reimers, N. & Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP\n(2019).\n44. He, P., Liu, X., Gao, J. & Chen, W. Deberta: Decoding-enhanced bert with disentangled attention. In\nICLR (2021).\n45. Wang, W. et al. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained\ntransformers. In NeurIPS, vol. 33, 5776–5788 (2020).\n46. Zheng, L. et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, vol. 36, 46595–\n46623 (2023).\n47. Hurst, A. et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024).\n48. Zhang, H. et al. HuatuoGPT, towards taming language model to be a doctor. In Findings of The 2023\nConference on Empirical Methods in Natural Language Processing (2023).\n29\n"}, {"page": 30, "text": "<Fig-1>\n<Fig-2>\nInput\nQuestion: What ultrastructural changes are \nobserved in the microvessels of LDLR \nmice fed a high cholesterol diet (Fig-1 and \nFig-2) compared to LDLR-/- mice fed a \nlow cholesterol diet (Fig-3)?\nIn LDLR mice fed a high cholesterol diet (Fig-1 and Fig-2), the ultrastructural \nchanges include severely degenerated microvessels with disrupted endothelial cell \nintegrity, vacuolation, narrowed lumens, and significant lipid deposits. In contrast, \nLDLR-/- mice fed a low cholesterol diet (Fig-3) exhibit thickened basal laminae, \nexpanded perivascular spaces, and degenerating capillary walls. These differences \nsuggest that high cholesterol diets exacerbate vascular degeneration more severely \nthan low cholesterol diets, even in genetically deficient mice.\nReference Answer\nBased on the provided images, here‘s a comparison of the ultrastructural changes in \nmicrovessels: LDLR mice fed a high cholesterol diet (Fig-1 and Fig-2): Fig-1A: \nShows a cross-section of a vessel with significant lipid deposition (marked by the \nwhite arrow) within the vessel wall, particularly in the medial layer. The intima \nappears thickened. Fig-1B: Shows a longitudinal section, revealing the vessel wall \nstructure and the presence of lipid deposits (marked by the white arrow) within the \nmedial layer. Fig-1C: Shows a cross-section with a prominent lipid-laden \nmacrophage (foam cell, marked by the white arrow) within the intima. The vessel \nwall appears thickened. …… Fig-2B: Shows a cross-section of a vessel with \nextensive lipid deposits (marked by the white arrows) within the medial layer and \npotentially within the intima. The vessel wall appears significantly thickened. LDLR-\n/- mice fed a low cholesterol diet (Fig-3): Fig-3C: Shows a cross-section of a vessel. \nThere are some cellular infiltrates (marked by the white arrows) within the vessel \nwall, but there is no obvious evidence of extensive lipid deposition or foam cell \nformation within the intima or media. The vessel wall appears relatively normal in \nthickness compared to the high-cholesterol diet groups. Comparison: Lipid \nDeposition: LDLR mice on a high cholesterol diet (Fig-1 and Fig-2) exhibit \nprominent lipid deposition within the vessel walls (media and intima), often forming \nfoam cells (lipid-laden macrophages). In contrast, LDLR-/- mice on a low cholesterol \ndiet (Fig-3) show minimal to no evidence of significant lipid deposition within the \nvessel walls. Foam Cell Formation: Foam cells are clearly visible in the intima.\nMedGemma-27B\nIn the LDLR mice fed a high cholesterol diet (Fig-1 and Fig-2), the microvessels \nexhibit significant ultrastructural changes, including the presence of numerous lipid \ndroplets and vacuoles within the endothelial cells and pericytes. This indicates \nincreased lipid accumulation and potential disruption of the vascular barrier function.\nIn contrast, LDLR-/- mice fed a low-cholesterol diet (Fig-3) show abnormal \nmicrovascular architecture characterized by a thickened basal lamina, expanded \nperivascular spaces, and degenerating capillary walls. These features indicate that, \nalthough lipid accumulation is reduced compared to high-cholesterol LDLR mice, \nstructural deterioration of the vessel wall is still present under low-cholesterol \nconditions.\nM!LLM-8B\n<Fig-0>\n<Fig-3>\nFigure 9: Comparative case study of M3LLM-8B and MedGemma-27B on the multi-image VQA (w.r.t.\nmultiple sub-images). The case requires the model to interpret transmission electron micrographs from LDLR\nmice fed a high-cholesterol diet and LDLR–/– mice on a low-cholesterol diet. M3LLM identifies key ultra-\nstructural changes, including disrupted endothelial integrity, lipid accumulation, and narrowed lumens in high-\ncholesterol samples, as well as thickened basal lamina and degenerating capillary walls in low-cholesterol\nsamples. In contrast, MedGemma’s descriptions contain significant errors, such as misclassifying lipid vac-\nuoles and hallucinating structural annotations, demonstrating its limitations in analyzing complex microvascu-\nlar pathologies.\n30\n"}, {"page": 31, "text": "Input\nReference Answer\n<Fig-0>\n<Fig-1>\nQuestion: What specific anatomical \nstructures are affected by the mass \neffect of the saccular dilatation in \n(Fig-1)?\nThe axial contrast-enhanced CT scan (Fig-1) shows that the saccular \ndilatation in the left common carotid artery exerts a mass effect, causing \ncompression and anterior displacement of the trachea. No additional \nabnormalities or significant lymphadenopathy are noted within this \nsectional view.\nMedGemma-27B\nBased on the provided image (Fig-1), the saccular dilatation appears to \nbe located in the region of the internal carotid artery (ICA), specifically \nat the bifurcation or shortly after. The mass effect of this dilatation is \nprimarily affecting the following structures: 1. Optic Chiasm: The \ndilated ICA is compressing the optic chiasm from above and anteriorly. \nThis compression is a classic finding in aneurysms at the ICA \nbifurcation. 2. Optic Nerve The optic nerves are located adjacent to the \noptic chiasm and are also likely compressed by the aneurysm. 3. \nPituitary Gland: The aneurysm is situated superior to the pituitary \ngland, and its mass effect could potentially impinge upon the gland or \nthe pituitary stalk. 4. Sphenoid Sinus The aneurysm is located superior \nto the sphenoid sinus. 5. Cavernous Sinus: The aneurysm is located \nsuperior to the cavernous sinus. The most clinically significant \ncompression is typically that of the optic chiasm and optic nerves, \nwhich can lead to visual field defects.\nM!LLM-8B\nThe saccular dilatation (Fig-1) shows that the saccular dilatation \nproduces mass effect with anterior displacement and mild compression \nof both the trachea and the esophagus. No additional vascular or \nintracranial structures are implicated on this slice.\nFigure 10: Comparative case study of M3LLM-8B and MedGemma-27B on the multi-image VQA (w.r.t.\nsingle sub-image within compound figure). The case requires identifying anatomical structures affected by\na mass effect in a neck CT image. The response of M3LLM, identifying the trachea and esophagus, aligns\nwith the reference answer. MedGemma’s response, however, contains significant factual errors, hallucinating\nanatomical structures not relevant to this image slice and demonstrating a critical risk of error in visual reason-\ning.\n31\n"}, {"page": 32, "text": "Input\n<Fig-0>\n<Fig-1>\n<Fig-2>\nQuestion: What is the spatial relationship \nbetween the axial T2-weighted MRI (Fig-\n1) and the fused PET/CT images (Fig-2) \nin the compound image based on their \nbounding box locations?\nReference Answer\nFig-1 (initial brain MRI) is positioned to the left of Fig-2 (C-spine \nMRI) in the compound image.\nMedGemma-27B\nM!LLM-8B\nThe axial T2-weighted MRI (Fig-1) is located in the upper left corner of \nthe compound image. The fused PET/CT images (Fig-2) are located in \nthe lower left corner of the compound image. Therefore, the axial T2-\nweighted MRI (Fig-1) is superior and slightly to the left of the fused \nPET/CT images (Fig-2).\nThe axial T2-weighted MRI (Fig-1) is positioned to the left of the fused \nPET/CT images (Fig-2) in the compound image. The MRI image is \noriented horizontally, while the PET/CT images are also oriented \nhorizontally but are placed to the right of the MRI image.\nFigure 11: Comparative case study of M3LLM-8B and MedGemma-27B on the multi-image VQA (w.r.t.\nspatial relationship). The case tests the model’s ability to determine the relative positions of sub-images\nin a compound figure. M3LLM accurately describes the left-right spatial relationship between the MRI and\nthe PET/CT. The response from MedGemma, however, contains a fundamental error in spatial localization,\nindicating a deficiency in its ability to comprehend compound figure structures.\n32\n"}, {"page": 33, "text": "Input\n<Fig-0>\nQuestion: How does the distribution \nof CD11c+ dendritic cells correlate \nwith the severity of experimental \nautoimmune encephalomyelitis \n(EAE) in the compound figure?\nThe distribution of CD11c+ dendritic cells correlates with the severity of \nEAE. In mice with more severe EAE, there is increased accumulation of DCs \nin CNS lesions, particularly around blood vessels and demyelinating areas \nmarked by anti-MBP antibodies. This suggests that DCs migrate to CNS \nlesions in response to inflammation, potentially contributing to antigen \npresentation and disease progression.\nReference Answer\nThe compound figure shows the distribution of CD11c+ dendritic cells \n(green) in the spinal cord at different stages of experimental autoimmune \nencephalomyelitis (EAE) using two-photon microscopy. The red channel \nrepresents the EAE score. Fig-0A (EAE Score: 3.0): Shows a relatively high \ndensity of green (CD11c+) cells, particularly concentrated in the area \nindicated by the red arrow in Fig-0B. The EAE score is 3.0, indicating a \nmoderate to severe stage of the disease. Fig-0B (EAE Score: 2.5): The red \narrow points to a region with a high concentration of green (CD11c+) cells. \nThe EAE score is 2.5, which is also a moderate stage. Fig-0C (EAE Score: \n1.5): Shows a lower density of green (CD11c+) cells compared to Figs-0A \nand 0B. The EAE score is 1.5, indicating a milder stage of the disease. \nCorrelation: Based on the images, there appears to be a positive correlation \nbetween the density/concentration of CD11c+ dendritic cells and the severity \nof EAE. Higher EAE scores (3.0 and 2.5) are associated with a higher \ndensity of CD11c+ cells, particularly in specific regions (indicated by the \narrow). A lower EAE score (1.5) is associated with a lower density of \nCD11c+ cells. Therefore, the distribution of CD11c+ dendritic cells seems to \ncorrelate with the severity of EAE, with higher severity being associated with \na greater presence of these cells in the spinal cord.\nMedGemma-27B\nThe distribution of CD11c+ dendritic cells correlates with the severity of \nexperimental autoimmune encephalomyelitis (EAE) as follows: Panel A \n(EAE Score: 3.0): Shows a higher density of CD11c+ dendritic cells, \nindicating a more severe EAE condition. Panel B (EAE Score: 2.5) \nDemonstrates a moderate density of CD11c+ dendritic cells, reflecting a less \nsevere EAE condition. Panel C (EAE Score: 1.5): Exhibits the lowest \ndensity of CD11c+ dendritic cells, corresponding to the mildest EAE \ncondition. This pattern suggests that increased infiltration of CD11c+ \ndendritic cells is associated with more severe EAE.\nM!LLM-8B\nFigure 12: Comparative case study of M3LLM-8B and MedGemma-27B on the single-image VQA. The\ncase requires the model to correlate the intensity of the CD11c+ signal with EAE disease severity across\nthree microscopy images. M3LLM successfully identifies the positive correlation and visual gradient between\nsignal intensity and the disease score. MedGemma, however, misidentifies this group of images as two-photon\nmicroscopy, highlighting M3LLM’s superior comprehensive analysis capabilities for compound figures.\n33\n"}, {"page": 34, "text": "Input\nQuestion: Based on Fig-1, which of the following best describes \nthe significance of the multiple tiny enhancing foci seen \nbilaterally on the MRI brain? \nA. They are likely benign cysts with no clinical significance.\nB. They represent metastatic lesions from a primary lung cancer. \nC. They indicate acute hemorrhagic strokes in the brain. \nD. They are calcified granulomas from previous infections.\n<Fig-0>\n<Fig-1>\nReference Answer\nB\nMedGemma-27B\nA\nM!LLM-8B\nB\nFigure 13: Comparative case study of M3LLM-8B and MedGemma-27B on the multi-choice VQA. The\ncase requires the model to select a diagnosis from multiple options based on brain imaging. M3LLM suc-\ncessfully chooses the correct option B, whereas MedGemma selects the incorrect option A. This comparison\ndemonstrates M3LLM’s superior performance in a structured diagnostic question-answering format.\n34\n"}, {"page": 35, "text": "Input\nQuestion: How do enhanced CT scans \ncontribute to the characterization of liver \nmasses, and what specific imaging \nfeatures suggest malignancy or \nmetastatic disease?\nEnhanced CT scans are essential for characterizing liver masses by providing detailed \ninformation about the size, shape, and enhancement pattern of the lesions. Specific \nimaging features that suggest malignancy or metastatic disease include ill-defined \nmargins, irregular shapes, and dynamic enhancement patterns. For example, a mass that \nenhances heterogeneously on the arterial phase and washes out on the portal venous phase \nis more indicative of malignancy. Additionally, the presence of multiple masses, \nparticularly those involving both the liver parenchyma and the porta hepatis, can suggest \nmetastatic disease or primary malignancies like hepatocellular carcinoma and \ncholangiocarcinoma. These imaging features guide further diagnostic steps, such as biopsy \nor additional imaging studies, to confirm the nature and extent of the lesions.\",\nReference Answer\nMedGemma-27B\nEnhanced CT scans significantly improve the characterization of liver masses by \nproviding information about the vascularity, enhancement patterns, and tissue composition \nof the lesions, which are crucial for differentiating benign from malignant conditions. \nSpecific imaging features suggesting malignancy or metastatic disease on enhanced CT \ninclude: 1. Arterial Phase Enhancement: Rapid and intense enhancement during the \narterial phase, followed by washout (appearing hypodense relative to the surrounding liver \nparenchyma) in the portal venous or delayed phases, is characteristic of hepatocellular \ncarcinoma (HCC) and many metastases. 2. Capsule: A well-defined capsule, particularly a \nthin, enhancing capsule, can be seen in some metastases (e.g., colorectal) or certain \nbenign. lesions, but its presence alone is not definitive for malignancy.\\n…… 8. Specific \nPatterns: Certain patterns like target lesions (central hypodensity with peripheral \nenhancement) can be seen in metastases or focal nodular hyperplasia (FNH), while others \nlike peripheral puddling enhancement are typical for FNH.\\n9. Delayed Phase Washout: \nPersistent hypodensity in the delayed phase relative to the liver parenchyma is a common \nfeature of many malignant lesions, including HCC and metastases.\\n10. Fatty \nChange/Steatosis: While often associated with benign conditions like steatosis or FNH, \nfatty change within a mass can sometimes be seen in specific types of tumors. These \nfeatures are often evaluated in combination with clinical history, laboratory tests, and \nsometimes other imaging modalities (like MRI or ultrasound) to arrive at a definitive \ndiagnosis.\nM!LLM-8B\nEnhanced CT scans contribute to the characterization of liver masses by providing \ndynamic, contrast-phase imaging that highlights vascular patterns and tissue perfusion, \nwhich help differentiate benign from malignant lesions. Malignant or metastatic \nfeatures typically include arterial phase hyperenhancement followed by rapid washout in \nthe portal or delayed phase, irregular or ill-defined margins, heterogeneous enhancement \ndue to necrosis or hemorrhage, capsular retraction, vascular invasion, and the presence of \nmultiple lesions in different lobes. In contrast, benign lesions often display characteristic \npatterns such as uniform enhancement, persistence of contrast, or specific features like the \nperipheral nodular enhancement with centripetal fill-in seen in hemangiomas. Thus, \ncontrast-enhanced CT is essential for both identifying malignancy and guiding further \ndiagnostic or therapeutic strategies.\nFigure 14: Comparative case study of M3LLM-8B and MedGemma-27B on the text-only QA. The\ncase requires the models to answer a technical question about CT features of liver masses without visual\ninput. M3LLM provides an accurate description consistent with the reference answer. The response from\nMedGemma, however, includes multiple errors and imprecise statements, such as conflating the typical fea-\ntures of different lesions. This highlights M3LLM’s superior performance in text-only medical reasoning.\n35\n"}, {"page": 36, "text": "Prompt for Stage 1: Inline Text Summarization \nRole \nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your primary task is to summarize inline \ntext from PubMed biomedical research papers, focusing on key medical observations, analyses, and conclusions related to patient \nconditions and diagnostic findings. \nTask\nInput Description\nThe input consists of inline text, which refers to multiple text excerpts from a research paper where a compound medical image is \nmentioned. These excerpts describe the observations, analyses, or clinical interpretations of the compound image and are often \nscattered across the text. Inline text provides fragmented but relevant details about the image that need to be consolidated into a \ncoherent summary. \nSpecific Task\nSummarize the key points from the inline text, focusing on: \n-\nObservations directly related to patient conditions, medical findings, and diagnostic conclusions. \n-\nInsights that highlight the clinical or research significance of the compound image. \n-\nExcluding irrelevant, repetitive, or vague information. The summary must be clear, concise, and strictly limited to 100 words. \nObjective \n1.\nAccuracy: Extract only medically relevant details from the inline text. Avoid adding unverified or speculative information. \n2.\nClarity: Ensure the summary is concise and easy to understand. Use formal, standardized medical terminology. \n3.\nRelevance: Focus on essential observations, analyses and conclusions related to the compound image. Exclude redundant or \nnon-contributory content. \n4.\nBrevity: Eliminate redundancy or unnecessary complexity. Consolidate information into a logical and compact form. \nInstructions \n1.\nFocus on Key Medical Details: Identify and include only details related to the patient conditions, diagnostic findings, or \nmedical significance of the compound image. \n2.\nEliminate Irrelevant Content: Remove vague, redundant, or non-medical details that do not contribute to understanding the \ncompound image. \n3.\nMaintain Clarity and Precision: Use clear, professional language suitable for academic and clinical audiences. \n4.\nWord Limit: Ensure the summary does not exceed 100 words. \nOutput Format  \nInline Summary: A concise summary of the inline text, focusing on the most important medical observations, analyses, and \nconclusions related to the compound image. \nExample \nInput: \nInline Text: \"The compound image highlights axial and coronal CT views of a pulmonary nodule located in the left upper lobe. \nThe nodule is well-circumscribed, suggesting a benign etiology. Surrounding lung parenchyma appears normal.\", \"Further \nanalysis of the pulmonary nodule indicates no evidence of calcification or ground-glass opacity. These findings reduce the \nlikelihood of malignancy.\", \"Although the nodule's smooth margins suggest benignity, clinical follow-up is recommended to rule \nout potential malignant transformation.\"\nOutput: \nInline Summary: Axial chest CT demonstrates a focal consolidation in the right lung, suggestive of a potential infectious process. \nFigure 15: The prompt design for Stage 1 of the instruction generation paradigm: Inline Text Summarization.\nThis prompt guides the LLM in condensing fragmented and verbose inline text into concise clinical narratives.\nThis stage establishes a standardized textual foundation for the subsequent context-aware instruction generation\nprocess.\n36\n"}, {"page": 37, "text": "Prompt for Stage 2: Medical Knowledge Complementation\nRole \nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your primary task is to generate concise and \naccurate medical knowledge for clinical, academic, and AI training purposes in English. \nTask \nInput Description\nThe input includes: \n1.\nCaption: Describes the visual contents of medical images (e.g., CT, MRI, histology). \n2.\nInline Summary: A concise summary of key medical observations, analyses, and conclusions related to the compound \nimage. \nSpecific Task \n1.\nExtract medically significant keywords from both the input Caption and Inline Summary. \n2.\nGenerate concise medical knowledge focusing on the clinical or diagnostic relevance of the extracted keywords. \nObjective \n1.\nAccuracy: Ensure all extracted keywords and medical knowledge are medically accurate and supported by established \nguidelines or literature. \n2.\nConciseness: Limit the medical knowledge to 5 sentences, focusing on the most clinically relevant details. \n3.\nProfessionalism: Use formal, standardized medical terminology. \n4.\nRelevance: Align the medical knowledge with the extracted keywords and clinical context. \nInstructions \n1.\nExtract Keywords: Identify key medical terms directly related to the Caption and Inline Summary, such as diseases, imaging \nmodalities, anatomical structures, or findings. \n2.\nGenerate Knowledge: Create concise medical knowledge based on the extracted keywords, emphasizing their clinical or \ndiagnostic importance. \nOutput Format \nKeywords: A list of extracted keywords. \nMedical Knowledge: A concise explanation of the keywords, limited to 5 sentences. \nExample \nInput: \n-\nCaption: CT scan of the chest showing a solitary pulmonary nodule in the upper lobe of the right lung. \n-\nInline Summary: Axial and coronal CT scans show a well-circumscribed pulmonary nodule in the right upper lobe, with no \ncalcification or ground-glass opacity, reducing the likelihood of malignancy. Normal surrounding lung parenchyma supports a \nbenign etiology, though clinical follow-up is recommended to monitor for potential malignant transformation. \nOutput:\n-\nKeywords: [\"pulmonary nodule\", \"right upper lobe\", \"benign etiology\", \"CT imaging\"] \n-\nMedical Knowledge: A pulmonary nodule is a small, round lesion within the lung that can represent benign or malignant \nconditions. CT imaging is essential for evaluating nodules, providing detailed information about their size, margins, and \ninternal characteristics. Smooth margins and the absence of calcification or ground-glass opacity suggest a benign etiology. \nHowever, clinical follow-up is recommended to rule out malignancy, as some nodules may transform over time. The location \nin the right upper lobe and normal surrounding lung parenchyma further support the benign diagnosis.\nFigure 16: The prompt design for Stage 2 of the instruction generation paradigm: Medical Knowledge Com-\nplementation. This prompt guides the LLM to extract key medical concepts from captions and summaries, and\nthen generate explanatory background knowledge around them. This stage adds necessary medical depth and\ncontext to the descriptive source texts, forming a basis for complex downstream tasks.\n37\n"}, {"page": 38, "text": "Prompt for Stage 3: Visual Perception Enhancement\nRole \nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate concise and detailed \ndescriptions of medical images for clinical and AI training purposes. \nTask \nInput Description\nThe input includes: \n1.\nImage Captions: Original and rewritten captions describing the medical image. \n2.\nCorresponding Image. \nSpecific Task\nGenerate a professional description of the image, focusing on visible findings, their clinical relevance, and additional insights \nbased on visual perception. \nObjective \n1.\nAccuracy: Ensure all details are medically accurate and based on visible features or provided references. \n2.\nConciseness: Limit the description to 5 sentences, focusing on the most relevant and visually significant findings. \n3.\nProfessionalism: Use formal medical language and maintain scientific rigor. \n4.\nVisual Focus: Provide insights grounded in the image's visual features, avoiding redundancy with the captions or medical \nknowledge. \nInstructions \n1.\nUse Reference: Use the captions and medical knowledge for context but focus on describing what is visually observable in \nthe image. Avoid repeating or paraphrasing the provided references. \n2.\nFocus on Visible Details: Highlight significant findings visible in the image, such as anatomical features, abnormalities, or \npatterns. Provide additional insights where possible. \n3.\nHighlight Clinical Relevance: Emphasize visually derived details that are clinically meaningful and add diagnostic value. \n4.\nAvoid Hallucination: Do not infer or assume details beyond what is visible in the image or explicitly supported by the \nreferences. \nOutput Format \nVisual Perception Description: A concise and professional description of the medical image, limited to 5 sentences, emphasizing \nvisual findings and clinical relevance. \nExample\nInput:\n-\nOriginal Caption: CT scan of the chest showing a solitary pulmonary nodule in the right upper lobe. \n-\nImage Input \nOutput: \nVisual Perception Description: The axial chest CT shows a solitary, well-defined pulmonary nodule in the right upper lobe with \nsmooth margins and no visible calcifications. The nodule measures approximately 1.2 cm and is located adjacent to the pleural \nsurface. No surrounding ground-glass opacity, lymphadenopathy, or pleural effusion is observed. The proximity of the nodule to \nthe pleura may warrant further evaluation for any subtle pleural involvement. These features suggest a likely benign etiology, \nthough clinical correlation is recommended.\nFigure 17: The prompt design for Stage 3 of the instruction generation paradigm: Medical Visual Perception\nEnhancement. This prompt directs the MLLM to generate precise, visually grounded descriptions focusing\non observable anatomical structures and pathological findings. By enforcing strict adherence to each panel\nof the compound figure, this stage effectively bridges the gap between raw visual content and textual medical\nknowledge, ensuring the fidelity of subsequent multi-modal reasoning.\n38\n"}, {"page": 39, "text": "Prompt for Stage 4: Structural Instruction Generation - Multi-image VQA w.r.t. Multiple Sub-images\nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate structured and concise Context-Question-Answer outputs based on the analysis of multiple selected sub-images from a \ncompound medical image. \nTask \nData Description \nThe input includes: \n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms. \n2.\nSelected Sub-Images: \n-\nA list of indices for the selected sub-images (e.g., Fig-1, Fig-2). \n3.\nSpecific Details for Each Selected Sub-Image: \n-       Index: The numeric identifier of the sub-image. \n-       Caption: A description of the visual content of the sub-image. \n-\nVisual Perception Description: Observations of the sub-image’s visual features, including key findings and structures. \n-\nInline Summary: A concise summary of medical observations, analyses, and conclusions related to the sub-image. \nSpecific Task \nGenerate a structured output consisting of: \n1.\nContext: A concise medical background introducing the selected sub-images, based on their Captions, Visual Perception Descriptions, and Inline Summaries. \n2.\nQuestion: A clear and specific question that requires an integrative analysis of the selected sub-images, encouraging detailed observation and clinical reasoning. \n3.\nAnswer: A precise and accurate response addressing the question, focusing only on the selected sub-images. The outputs must focus only on the selected sub-images and avoid interference from other sub-images or the \ncompound image as a whole. \nObjective \n1.\nAccuracy: Ensure all outputs are accurate and based strictly on the provided references for the selected sub-images. \n2.\nClarity: Ensure the Question and Answer are logically connected and specific to the selected sub-images. \n3.\nRelevance: Focus on the selected sub-images, avoiding interference from other sub-images or the compound image. \nInstructions \n1.\nUse References: Base outputs on the provided information for the selected sub-images, ensuring the Context, Question, and Answer are supported by their details. \n2.\nFocus on Selected Sub-Images: Ensure all outputs focus only on the selected sub-images, avoiding references to unselected sub-images or the compound image as a whole. \n3.\nIntegrate Sub-Image Information: Synthesize information from the selected sub-images to create a cohesive Context, Question, and Answer that encourages integrative thinking. \nOutput Format \n-\nContext: A concise and relevant medical background introducing the selected sub-images. \n-\nQuestion: A specific and clear question that requires an integrative analysis of the selected sub-images. \n-\nAnswer: A precise and accurate response addressing the question, focusing only on the selected sub-images. \nExample \nInput: \nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, and surrounding lung parenchyma. Imaging \nfrom multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \nSelected Sub-Images: \n-\nIndices: Fig-1, Fig-2 \n-\nSpecific Sub-Image Details:  \n-\nIndex: Fig-1 \n-\nCaption: An axial CT image of the chest demonstrating a solitary pulmonary nodule in the left upper lobe. \n-\nVisual Perception Description: The CT image shows a well-defined, round pulmonary nodule located in the left upper lobe. The surrounding lung parenchyma appears normal, with no signs of pleural effusion or \nlymphadenopathy. \n-\nInline Summary: The axial view highlights a well-defined pulmonary nodule with smooth margins, suggesting a benign etiology. \n-\nIndex: Fig-2 \n-\nCaption: A coronal CT image showing the same pulmonary nodule in the left upper lobe from a different plane. \n-\nVisual Perception Description: The coronal CT image demonstrates the nodule's position relative to the surrounding lung structures, confirming its well-defined borders and normal adjacent lung parenchyma. \n-\nInline Summary: The coronal view provides complementary information about the nodule's location and its relationship to nearby lung structures, supporting the interpretation of a benign lesion. \nOutput:\n-\nContext: The selected sub-images (Fig-1 and Fig-2) highlight axial and coronal CT views of a solitary pulmonary nodule located in the left upper lobe. Axial and coronal imaging provide complementary perspectives for \nevaluating the nodule’s size, margins, and its relationship to adjacent lung structures. Together, these views confirm the nodule’s well-defined appearance and normal surrounding lung parenchyma, which are key features in \nassessing its clinical significance. \n-\nQuestion: How do the axial and coronal CT views (Fig-1 and Fig-2) contribute to the evaluation of the pulmonary nodule in the left upper lobe? \n-\nAnswer: Axial and coronal CT views provide complementary information for assessing the pulmonary nodule. The axial view characterizes the nodule’s shape, size, and margins, while the coronal view offers insights into its \nlocation and relationship to surrounding lung structures. Together, these perspectives confirm the nodule’s well-defined borders and the absence of abnormalities in the adjacent lung parenchyma, supporting the interpretation of \na benign lesion. \nFigure 18: The prompt design for Stage 4 of the instruction generation paradigm: Structural Instruction Gen-\neration for multi-image VQA (w.r.t. multiple sub-images). This prompt demonstrates how information from\nmultiple sub-images is automatically converted into a structured training sample comprising a context, ques-\ntion, and answer. As a core part of Stage 4, it is designed to create complex instructions that train the model to\nperform integrative reasoning across different views.\n39\n"}, {"page": 40, "text": "Prompt for Stage 4: Structural Instruction Generation - Multi-image VQA w.r.t. Single Sub-image in Compound Figure\nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate structured and concise Context-Question-Answer outputs based on the analysis of multiple \nselected sub-images from a compound medical image. \nTask \nData Description \nThe input includes: \n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms. \n2.\nSelected Sub-Image: \n-\nThe index for the selected sub-image (e.g., Fig-1). \n3.\nSpecific Details for Selected Sub-Image: \n-       Index: The numeric identifier of the sub-image. \n-       Caption: A description of the visual content of the sub-image. \n-\nVisual Perception Description: Observations of the sub-image’s visual features, including key findings and structures. \n-\nInline Summary: A concise summary of medical observations, analyses, and conclusions related to the sub-image. \nSpecific Task \nGenerate a structured output consisting of: \n1.\nContext: A concise medical background introducing the selected sub-images, based on their Captions, Visual Perception Descriptions, and Inline Summaries. \n2.\nQuestion: A clear and specific question that requires an integrative analysis of the selected sub-images, encouraging detailed observation and clinical reasoning. \n3.\nAnswer: A precise and accurate response addressing the question, focusing only on the selected sub-images. The outputs must focus only on the selected sub-image.\nObjective \n1.\nAccuracy: Ensure all outputs are accurate and based strictly on the provided references for the selected sub-image. \n2.\nClarity: Ensure the Question and Answer are logically connected and specific to the selected sub-image. \n3.\nRelevance: Focus on the selected sub-image.\nInstructions \n1.\nUse References: - Base outputs on the provided information for the selected sub-images, ensuring the Context, Question, and Answer are supported by their details. \n2.\nFocus on Selected Sub-Image: - Ensure all outputs focus only on the selected sub-images, avoiding references to unselected sub-image or the compound image as a whole. \n3.\nIntegrate Sub-Image Information: - Synthesize information from the selected sub-image to create a cohesive Context, Question, and Answer that encourages integrative thinking. \nOutput Format \n-\nContext: A concise and relevant medical background introducing the selected sub-image. \n-\nQuestion: A specific and clear question that requires an integrative analysis of the selected sub-images. \n-\nAnswer: A precise and accurate response addressing the question, focusing only on the selected sub-image. \nExample \nInput: \nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, and surrounding lung \nparenchyma. Imaging from multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \nSelected Sub-Image: \n-\nIndex: Fig-1 \n-\nSpecific Sub-Image Details:  \n-\nIndex: Fig-1 \n-\nCaption: An axial CT image of the chest demonstrating a solitary pulmonary nodule in the left upper lobe. \n-\nVisual Perception Description: The CT image shows a well-defined, round pulmonary nodule located in the left upper lobe. The surrounding lung parenchyma appears normal, with no signs of pleural \neffusion or lymphadenopathy. \n-\nInline Summary: The axial view highlights a well-defined pulmonary nodule with smooth margins, suggesting a benign etiology. \nOutput:\n-\nContext: This axial CT image (Fig-1) shows a solitary pulmonary nodule in the left upper lobe. Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. The axial view \nis essential for characterizing the nodule's size, shape, and margins, as well as evaluating surrounding lung parenchyma.\n-\nQuestion: What are the key visible imaging features of the pulmonary nodule in (Fig-1)?\n-\nAnswer: The axial CT image (Fig-1) demonstrates a well-defined, round pulmonary nodule located in the left upper lobe. The surrounding lung parenchyma appears normal, with no calcification, pleural \neffusion, or lymphadenopathy, suggesting a benign diagnosis.\nFigure 19: The prompt design for Stage 4 of the instruction generation paradigm: Structural Instruction Gen-\neration for multi-image VQA (w.r.t. single sub-image within compound figure). This prompt is designed to\nconvert detailed information from a single sub-image into a training sample. This task ensures that while the\nmodel learns complex cross-image reasoning, it does not lose its foundational ability to conduct in-depth anal-\nysis of individual images.\n40\n"}, {"page": 41, "text": "Prompt for Stage 4: Structural Instruction Generation - Multi-image VQA w.r.t. Spatial Relationship\nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate structured and concise Question-Answer outputs based on the relative positions of two selected \nsub-images from a compound medical image. You already pre-computed the relative position of two figures, please rephrase this positional information into a professional and medically accurate QA pair.\nTask\nData Description\nThe input includes:\n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms. \n2.\nSelected Sub-Images: \n-\nA list of indices for the selected sub-images (e.g., Fig-1, Fig-2). \n3.\nSpecific Details for Each Selected Sub-Image: \n-       Index: The numeric identifier of the sub-image. \n-       Caption: A description of the visual content of the sub-image. \n-\nVisual Perception Description: Observations of the sub-image’s visual features, including key findings and structures. \n-\nBounding Box Information: The coordinates of the central point of the sub-image bounding box in the compound image, provided as (x,y). \n-\nPrecomputed relative position: The precomputed relative position of two selected figures.\nSpecific Task\nGenerate a structured output consisting of: \n1.\nQuestion: A clear and specific question about the spatial relationship (literal physical placement) between the two selected sub-images.\n2.\nAnswer: A precise and accurate response describing the relative positions of the two selected sub-images. Please use the given center points of the given two subfigures. The outputs must focus only on \nthe spatial relationship between the two selected sub-images.\nObjective \n1.\nAccuracy: Ensure all outputs are accurate and based strictly on the provided references for the two selected sub-images. \n2.\nClarity: Write all outputs in a clear, concise, and professional manner. \n3.\nRelevance: Focus on the spatial relationship between the two selected sub-images. \nInstructions\n1.\nUse References: - Base outputs on the provided information for the two selected sub-images, ensuring the Question and Answer are supported by their details.\n2.\nFocus on Selected Sub-Images: - Ensure all outputs focus only on the two selected sub-images. \n3.\nAnswer Separation: - please rephrase this positional information into a professional and medically accurate QA pair. No need to mention the reason, directly state the position.\nOutput Format\n-\nQuestion: A specific and clear question about the spatial relationship between the two selected sub-images. \n-\nAnswer: A precise and accurate response describing the relative positions of the two selected sub-images.\nExample\nInput:\nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, and surrounding lung \nparenchyma. Imaging from multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \nSelected Sub-Images: \n-\nIndices: Fig-1, Fig-2 \n-\nSpecific Sub-Image Details:  \n-\nIndex: Fig-1 \n-\nVisual Perception Description: The CT image shows a well-defined, round pulmonary nodule located in the left upper lobe. The surrounding lung parenchyma appears normal, with no signs of pleural \neffusion or lymphadenopathy. \n-\nBounding Box Information: (100, 150) \n-\nIndex: Fig-2 \n-\nCaption: A coronal CT image showing the same pulmonary nodule in the left upper lobe from a different plane. \n-\nVisual Perception Description: The coronal CT image demonstrates the nodule's position relative to the surrounding lung structures, confirming its well-defined borders and normal adjacent lung \nparenchyma. \n-\nBounding Box Information: (250, 100)\nOutput: \n-\nQuestion: What is the spatial relationship between Fig-1 and Fig-2 of the pulmonary nodule in the compound image based on their bounding box locations? \n-\nAnswer: Fig-1 is positioned to the left and below Fig-2 in the compound image. \nFigure 20: The prompt design for Stage 4 of the instruction generation paradigm: Structural Instruction Gener-\nation for multi-image VQA (w.r.t. the spatial relationship). This prompt demonstrates how raw positional data\nof sub-images (from bounding box information) is automatically converted into a structured question-answer\npair. As a part of Stage 4 of the instruction generation paradigm, it is designed to create training samples that\nspecifically target the spatial reasoning capabilities.\n41\n"}, {"page": 42, "text": "Prompt for Stage 4: Structural Instruction Generation - Single-image VQA\nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate structured and concise Context-Question-Answer outputs based on the analysis of compound \nmedical images for clinical education and AI training.\nTask\nData Description\nThe input includes:\n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms. \nSpecific Task\nGenerate a structured output consisting of: \n1.\nContext: A concise medical background introducing the compound image, based on its Caption, Inline Summary, and Medical Knowledge. \n2.\nQuestion: A clear and specific question that encourages detailed clinical analysis of the compound image. \n3.\nAnswer: A precise and accurate response, focusing solely on the compound image as a whole. The outputs should focus on the compound image as a whole, avoiding sub-image-specific details unless \nthey are critical.\nObjective \n1.\nAccuracy: Ensure all outputs are accurate and based strictly on the provided references for the selected sub-images. \n2.\nClarity: Ensure the Question and Answer are logically connected and specific to the compound image. \n3.\nRelevance: Focus on the compound image, avoiding interference from other sub-images or the compound image. \nInstructions\n1.\nUse References: Base outputs on the provided Compound Image Information, ensuring they address the compound image as a whole. \n2.\nFocus on the Compound Image: Ensure all outputs provide insights into the compound image’s overall findings, clinical importance, or diagnostic significance. Avoid focusing on specific sub-images or \ntheir details unless explicitly relevant to the compound image. \n3.\nAnswer Separation: Ensure the Answer provides unique information not repeated in the Context or Question. \nOutput Format\n-\nContext: Concise medical background for the compound image.\n-\nQuestion: A specific clinical question about the compound image. \n-\nAnswer: A precise response based on the provided textual information.\nExample\nInput:\nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, and surrounding lung \nparenchyma. Imaging from multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \nOutput: \n-\nContext: This compound figure includes multiple CT views of a pulmonary nodule in the left upper lobe. Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their \nevaluation typically involves analyzing size, shape, margins, and the condition of surrounding lung parenchyma. Imaging from multiple planes, such as axial and coronal views, provides detailed \ninformation about the nodule’s characteristics and its relationship to surrounding structures. \n-\nQuestion: What is the clinical significance of using multiple CT views to evaluate pulmonary nodules in Fig-0?\n-\nAnswer: Multiple CT views, such as axial and coronal planes, provide complementary perspectives for assessing pulmonary nodules. They help determine the nodule’s size, shape, boundaries, and its \nrelationship to nearby tissues. This approach aids in distinguishing between benign and malignant nodules and guides further diagnostic or treatment decisions.\nFigure 21: The prompt design for Stage 4 of the instruction generation paradigm: Structural Instruction Gen-\neration for single-image VQA. This prompt is designed to convert comprehensive information about an entire\ncompound figure into a structured training sample with Context, Question and Answer. This task specifically\ntrains the model to develop a global and holistic understanding of a multi-panel figure, rather than analyzing\nits individual components.\n42\n"}, {"page": 43, "text": "Prompt for Stage 4: Structural Instruction Generation - Text-only QA\nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate structured and concise Context-Question-Answer outputs based on the analysis of multiple \nselected sub-images from a compound medical image. \nNote: The model will not be shown the image. All outputs must be based solely on the provided textual information (caption, summary, and medical knowledge).\nTask \nData Description \nThe input includes: \n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms. \nSpecific Task \nGenerate a structured output consisting of: \n1.\nContext: Summarize the Caption and Inline Summary into a concise medical background. The Context should focus on the key findings and clinical significance described in the input text and exclude \nany Medical Knowledge details.\n2.\nQuestion: Create a self-contained and insightful question based on Medical Knowledge. The question should relate to the clinical concepts or keywords described in the context, but it must expand the \ndiscussion beyond the context by leveraging general medical knowledge.\n3.\nAnswer: Provide a precise and detailed response to the question, expanding on the medical knowledge or principles relevant to the case.\nObjective \n1.\nAccuracy: Ensure all outputs are medically accurate and based strictly on the provided references. Avoid speculative or unrelated details.\n2.\nClarity: Write in a clear, concise, and professional manner.\n3.\nRelevance: Focus on meaningful and clinically significant concepts. \n4.\nDistinct Context, Question, and Answer: The Context should summarize the Caption and Inline Summary, while the Question and Answer should focus on expanding the discussion using Medical \nKnowledge. The Answer must introduce new content that is not already present in the Context. \nInstructions \n1.\nSummarize for Context: Use the Caption and Inline Summary to create a concise, text-based summary. Avoid including any general medical knowledge or broader clinical concepts in this section. \n2.\nExpand with the Question: Identify keywords or concepts in the context and use Medical Knowledge to construct an insightful question. Ensure the question is self-contained and does not depend on \nthe context to be understood. \n3.\nAnswer Separately: Provide a detailed and accurate answer to the question, focusing on relevant medical knowledge. Avoid duplicating any information already present in the context. \n4.\nExamples of Good and Bad Practices: \n-\nGood: What are the common risk factors for pulmonary nodule malignancy, and how are they assessed clinically? \n-\nBad: What are the key observations in the compound figure? \n-\nBad: What does the image suggest about the lesion’s malignancy? \nReason: These imply the model has visual access, which it does not.\nOutput Format \n-\nContext: A concise summary of the Caption and Inline Summary.\n-\nQuestion: A specific and clear question that requires an integrative analysis of the selected sub-images. \n-\nAnswer: A precise and detailed response to the question, introducing information not present in the context.\nExample \nInput: \nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule.\n-\nInline Summary: Axial and coronal CT scans show a well-circumscribed pulmonary nodule in the left upper lobe, with no calcification or ground-glass opacity, reducing the likelihood of malignancy. \nNormal surrounding lung parenchyma supports a benign etiology, though clinical follow-up is recommended to monitor for potential malignant transformation. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, and surrounding lung \nparenchyma. Imaging from multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \nOutput: \n-\nContext: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. CT imaging can help assess the likelihood of malignancy by analyzing features such as size, margins, \nand opacity. A well-circumscribed nodule with no calcification or ground-glass opacity, as described in this case, is less likely to be malignant.\n-\nQuestion: What are the risk factors for pulmonary nodule malignancy, and what imaging features are most commonly associated with malignant nodules? \n-\nAnswer: The axial CT image (Fig-1) demonstrates a well-defined, round pulmonary nodule located in the left upper lobe. The surrounding lung parenchyma appears normal, with no calcification, pleural \neffusion, or lymphadenopathy, suggesting a benign diagnosis.\nFigure 22: The prompt design for Stage 4 of the instruction generation paradigm: Structural Instruction Gener-\nation for text-only QA. This prompt is designed to create a training sample based solely on textual information,\nas the model does not receive any image input for this task. This task specifically targets the linguistic and\nknowledge-based reasoning skills, ensuring it can perform accurate medical Q&A even without visual aids.\n43\n"}, {"page": 44, "text": "Prompt for Stage 4: Structural Instruction Generation - Multiple-choice VQA\nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to generate structured and concise Four-Choice Question-Answer outputs based on the analysis of medical \nimages for clinical education and AI training.\nTask \nData Description \nThe input includes: \n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms. \nSpecific Task \nGenerate a structured output consisting of: \n1.\nQuestion: A clear and specific four-choice question about the compound image or selected sub-image(s) that encourages detailed analysis and clinical reasoning.\n2.\nOptions: Four plausible answer options, with one correct option and three incorrect distractors. The distractors should be relevant but clearly incorrect based on the input data. \nObjective \n1.\nAccuracy: Ensure all outputs are medically accurate and based strictly on the provided references for the compound image or sub-image(s). \n2.\nClarity: Write all outputs in a clear, concise, and professional manner. \n3.\nRelevance: Focus on clinically significant findings or diagnostic insights from the input data. \n4.\nOption Plausibility: Ensure the distractors are relevant to the context and plausible enough to challenge the reader, but clearly incorrect upon analysis. The correct answer will have an equal 25% \nprobability of distribution across A, B, C, or D in the Options list.\nInstructions \n1.\nUse References: Base outputs on the provided information for the compound image or sub-image(s), ensuring the Question, and Options are supported by their details. \n2.\nFocus on Clinical Value: Ensure the question and options are clinically relevant and encourage critical thinking about the image data. \n3.\nCreate Plausible Distractors: Distractors should be related to the same clinical or diagnostic context but be clearly incorrect upon closer analysis. \nOutput Format \n-\nQuestion: A specific and clear four-choice question that encourages detailed analysis and clinical reasoning. \n-\nOptions: Four plausible answer options, with one correct answer and three incorrect distractors. \n-\nCorrect Answer: Indicate the correct option (e.g., A, B, C, or D). \nExample \nInput: \nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule.\n-\nInline Summary: Axial and coronal CT scans show a well-circumscribed pulmonary nodule in the left upper lobe, with no calcification or ground-glass opacity, reducing the likelihood of malignancy. \nNormal surrounding lung parenchyma supports a benign etiology, though clinical follow-up is recommended to monitor for potential malignant transformation. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, and surrounding lung \nparenchyma. Imaging from multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \nOutput: \n-\nQuestion: Based on Fig-1, which of the following features most strongly supports a benign diagnosis for the pulmonary nodule? \n-\nOptions: A. Presence of ground-glass opacity.\\n B. Smooth, well-defined borders.\\n C. Enlarged mediastinal lymph nodes.\\n D. Irregular shape with spiculated margins. \n-\nCorrect Answer: B\nFigure 23: The prompt design for Stage 4 of the instruction generation paradigm: Structural Instruction Gen-\neration for multi-choice VQA. This prompt is designed to convert information about a compound figure into a\nstructured multi-choice question. It specifically emphasizes the need to generate three plausible distractors to\nensure the quality and challenge of the questions, thereby creating data suitable for both training and standard-\nized evaluation.\n44\n"}, {"page": 45, "text": "Prompt for Stage 5: Leakage-Prevented Context Refinement \nRole\nYou are a biomedical AI assistant specializing in medical imaging and clinical notes. Your task is to revise and consolidate information to generate a single improved context for a \ncompound image. The improved context should exclude any information that directly appears in the Answer of a specific sub-image while providing relevant medical background and \ninsights about the compound image as a whole.\nTask \nData Description \nThe input includes: \n1.\nCompound Image Information: \n-\nIndex: The unique numeric identifier of the compound image (e.g., Fig-0). \n-\nCaption: A description of the compound figure as a whole, summarizing its contents and visible findings. \n-\nInline Summary: Key medical observations and conclusions related to the compound image. \n-\nMedical Knowledge: Relevant diagnostic or clinical information about the compound image, general findings or mechanisms.  \n-\nVisual Perception Description Highlights visible findings in the compound image. \n2.\nSpecific Sub-Image Information: Index, Context, Question, and Answer for a single selected sub-images.\nSpecific Task \nGenerate an improved comprehensive context for the compound image by using: \n-\nThe compound image's Inline Summary, Medical Knowledge, and Visual Perception Description. \n-\nThe information of the selected sub-image. \nRequirement: Exclude any content that directly appears in the Answer of the specific sub-image.\nObjective \n1.\nAccuracy: Ensure the improved context is medically accurate and avoids including any information from the Answer of the sub-image. \n2.\nClarity: Keep the consolidated context concise, clear, and professionally written. \n3.\nRelevance: Focus on overall findings, mechanisms, or clinical significance of the compound image without introducing unrelated or speculative details. \n4.\nAnswer Separation: Ensure the improved context does not leak any information that directly appears in the Answer of the sub-image.\nInstructions \n1.\nUse References: Base the improved context on the provided sub-image contexts, but avoid including any information that overlaps directly with the Answer of the sub-image. \nConsolidate relevant medical background or broader insights related to the compound image. \n2.\nRemove Answer Information: Exclude any content from the sub-image context that overlaps with the sub-image Answer. Provide general information about the compound image \nwithout revealing specific details from the Answer. \n3.\nFocus on Compound Image: Ensure the improved context provides information that is relevant to the compound image as a whole. \n4.\nEnsure Clarity and Conciseness: Write the improved context in a clear, concise, and professional manner, adhering to medical accuracy and educational value.\nOutput Format \n-\nImproved Context: A consolidated and comprehensive context for the compound image, excluding information that overlaps with the Answer of the sub-image.\nExample \nInput \nCompound Image: \n-\nIndex: 0 \n-\nCaption: A compound figure of chest CT imaging showing multiple views of a pulmonary nodule.\n-\nInline Summary: Axial and coronal CT scans show a well-circumscribed pulmonary nodule in the left upper lobe, with no calcification or ground-glass opacity, reducing the \nlikelihood of malignancy. Normal surrounding lung parenchyma supports a benign etiology, though clinical follow-up is recommended to monitor for potential malignant \ntransformation. \n-\nMedical Knowledge: Pulmonary nodules are small, round growths in lung tissue that may be benign or malignant. Their evaluation typically involves analyzing size, shape, margins, \nand surrounding lung parenchyma. Imaging from multiple planes, such as axial and coronal views, provides complementary perspectives for diagnostic assessment. \n-\nVisual Perception Description: The compound figure contains multiple sub-images showing different CT views of a solitary pulmonary nodule in the left upper lobe. The nodule is \nwell-defined and round, with normal surrounding lung parenchyma. \nSpecific Sub-Image: \n-\nIndex: 1 \n-\nContext: This axial CT image depicts a solitary pulmonary nodule in the left upper lobe. Pulmonary nodules are often evaluated for their size, shape, and margins to determine \nwhether they are benign or malignant. \n-\nQuestion: What are the key visible features of the pulmonary nodule in (Fig-1)? \n-\nAnswer: The image shows a well-defined, round pulmonary nodule located in the left upper lobe. The surrounding lung parenchyma appears normal, with no signs of pleural effusion \nor lymphadenopathy. \nOutput \n-\nImproved Context: This compound figure includes multiple CT views of a pulmonary nodule located in the left upper lobe. Pulmonary nodules are small, round growths in lung tissue \nthat may be benign or malignant. Their evaluation typically involves analyzing their size, shape, borders, and the condition of surrounding lung parenchyma. Different imaging planes, \nsuch as axial and coronal views, provide complementary perspectives on the nodule's characteristics and its relationship to surrounding structures, aiding in diagnostic assessment.\nFigure 24: The prompt design for Stage 5 of the instruction generation paradigm: Leakage-Prevented Context\nRefinement. This prompt guides the AI model to revise the generated context by removing specific details\nthat overlap with the answer for a particular sub-image. This ensures the context provides general background\nwithout inadvertently revealing the solution, thereby enhancing the quality and challenge of the training in-\nstructions.\n45\n"}, {"page": 46, "text": "Prompt for LLM-as-a-judge \nRole\nYou are an impartial medical QA evaluator.\nTask\nGiven a Question, a Reference Answer (ground truth), and two model predictions (A and \nB), determine which prediction is more aligned with the reference.\nYour evaluation should prioritize the following aspects:\n1. Correctness:\n•\nWhether the prediction is factually correct and consistent with the reference answer.\n•\nWhether it avoids false or misleading medical statements.\n2. Completeness:\n•\nWhether the prediction covers the essential clinical points required to match the reference.\n3. Clarity:\n•\nWhether the prediction is understandable and unambiguous.\nIf A and B are equally correct and complete, return \"Tie\".\nDo not judge writing style, verbosity, or elegance beyond clarity.\nFocus strictly on factual and clinical alignment with the reference answer.\nOutput\nReturn ONLY valid JSON using exactly this format:\n{\n  \"winner\": \"A\" | \"B\" | \"Tie\",\n  \"rationale\": \"Short explanation (1–3 sentences) identifying correctness differences.\"\n}\nFigure 25: The LLM-as-a-judge evaluation prompt for pairwise medical QA comparison. The figure illustrates\nthe structured prompt used to evaluate two model predictions against a reference answer in medical visual\nquestion answering. The prompt instructs the judge model to prioritize factual correctness, with completeness\nand clarity as secondary criteria, and to output a strict JSON object specifying the winner and a brief rationale.\nThis standardized judging prompt enables consistent and reproducible comparison.\n46\n"}, {"page": 47, "text": "Upon our evaluation, her abdomen was distended but nontender. \nHer case was discussed in a multidisciplinary conference among \nseveral surgeons, radiologists, and gastroenterologists, and it was \nagreed that her duodenum was non-functional and required \nresection. She was offered immediate surgery, but the family \nchose to postpone surgery for several weeks given that she was in \nthe middle of her school semester. However, before reaching the \ndate of elective surgery, she emergently presented with severe \nabdominal pain, nausea, vomiting, and failure to pass gas or stool. \nLaboratory studies demonstrated electrolyte abnormalities with \nleukocytosis (14.6/L), elevated amylase (1053 U/L), and elevated \nlipase (>3500 U/L). All other labs were unremarkable, including \ntotal bilirubin, triglyceride, and liver enzymes. She was \nsubsequently found on computed tomography (CT) to have a large \nduodenal volvulus with pneumatosis intestinalis of the massively \ndilated duodenum concerning for ischemia (Figure 2). There was \nsignificant mesenteric swirling with a portion of the pancreas \nbeing pulled into the volvulus, and both superior mesenteric artery \n(SMA) and the distal stomach were being pulled to the right of the \nmidline.\nThe patient presented with a distended but nontender abdomen. A \nmultidisciplinary team consensus determined that the non-\nfunctional duodenum required resection. Prior to scheduled \nsurgery, she developed severe abdominal pain, nausea, vomiting, \nand ileus. Lab results showed leukocytosis (14.6/L), elevated \namylase (1053 U/L), and lipase (>3500 U/L). CT imaging \nrevealed a large duodenal volvulus with pneumatosis intestinalis, \nindicating ischemia. Significant mesenteric swirling involved the \npancreas and SMA, with displacement of the distal stomach to the \nright of the midline.\nInline Text\nSummarized Inline Text\nQWen2.5-32B\nFigure 26: Example of the Stage 1: Inline Text Summarization process. This figure illustrates the transfor-\nmation of a raw, complex block of inline text (left) into a refined and more structured summary (right). This\ninitial stage of the instruction generation paradigm is designed to standardize the textual input for subsequent\nautomated processing.\nLITT (Laser Interstitial Thermal Therapy) trajectory planning \ninvolves precise navigation through specific brain structures. The \ntrajectory depicted incorporates critical areas like the lateral \ngeniculate nucleus (LGN) and brainstem, ensuring safety and \nefficacy. The intrahippocampal and extrahippocampal portions \ndelineate the trajectory's angulation, which is crucial for avoiding \ndamage to adjacent structures such as the amygdalohippocampal \ncomplex, lateral ventricles, parahippocampal gyrus, entorhinal \ncortex, and optic tract. This detailed 3D modeling aids in \npreoperative planning to minimize complications and optimize \ntherapeutic outcomes.\nCaption\nMedical Knowledge Note\nQWen2.5-32B\nTemplate‐based A, 3D model and B, coronal MRI of an automated \nleft sided LITT trajectory (light green) at the level of the LGN \n(dark green) showing its incorporation within the brainstem \nsegmentation (blue). Entire LITT trajectory overlaid onto 2D \ncoronal slice with intrahippocampal (opaque green) and \nextrahippocampal (transparent green) portions delineating the \nlateral to medial and superior to inferior angulation of the \ntrajectory. Position of trajectory at level of LGN (green). \nAmygdalohippocampal complex (yellow), lateral ventricles \n(pink), parahippocampal gyrus (orange), entorhinal cortex (teal), \nand optic tract (white) also shown\nVisual field deficits (VFDs) are the most common complications \nfollowing laser interstitial thermal therapy (LITT) for mesial \ntemporal lobe epilepsy (MTLE), including contralateral superior \nquadrantanopia and hemianopia. Superior quadrantanopia results \nfrom ablation cavities extending into the optic radiation posterior \nto the hippocampus, while homonymous hemianopia can result \nfrom heat transfer to the lateral geniculate nucleus (LGN) during \nposterior hippocampal ablation. EpiNav-generated trajectories \navoid excessive heat transfer to the sagittal striatum, reducing the \nrisk of superior quadrantanopia. The ambient cistern and choroidal \nfissure provide natural barriers, and EpiNav maximizes distance \nfrom the brainstem and LGN to prevent thermal injury.\nSummarized Inline Text\nFigure 27: Example of the Stage 2: Medical Knowledge Complementation process. This figure illustrates\nhow an explanatory knowledge text (right) is automatically distilled and generated from a descriptive caption\nand summary (left). This process enriches the dataset with critical medical background information to support\ndeeper reasoning tasks.\n47\n"}, {"page": 48, "text": "The image depicts a color-coded representation of the pial surface \nmapped by an advanced algorithm. Each triangle on this mesh is \nassigned a unique region ID, which corresponds to different \ncerebral arteries such as ACA (Anterior Cerebral Artery) and PCA \n(Posterior Cerebral Artery) among others. The colors range from \nblue, yellow, red, green, purple, and pink, each representing \ndistinct regions or segments within the brain's vascular system. \nThis detailed mapping aids in understanding the complex network \nof blood supply across various parts of the brain.\nSub-figure Caption\nVisual Perception Descriptions\nHuatuoGPT-Vision-34B\n(a) The pial surface mapped by the coupling algorithm. The major \nregion ID for each triangle of the pial surface mesh is shown. Note \nthat this result is enforced by the algorithm.\nSub-figure\nFigure 28: Example of the Stage 3: Medical Visual Perception Enhancement process. This figure illustrates\nhow a multi-modal model is used to transform a sub-image and its basic textual descriptions (left) into a richer\nand more detailed visual analysis text (right). This process provides a deeper semantic interpretation of the\nvisual features within the image.\nSelected Figure 1\nContext\nThe selected sub-images (Fig-1 and Fig-2) illustrate a template-\nbased 3D model and a coronal MRI of an automated left-sided \nLITT trajectory (light green) targeting the level of the lateral \ngeniculate nucleus (LGN, dark green). Both images show the \nLITT trajectory incorporated within the brainstem segmentation \n(blue). These images provide complementary perspectives on the \nprecise targeting and positioning of the LITT trajectory, ensuring \nminimal damage to surrounding neural structures such as the optic \nchiasm and adjacent pathways. \nHow do the 3D model and coronal MRI (Fig-1 and Fig-2) \ncontribute to the precise targeting and positioning of the LITT \ntrajectory at the level of the LGN?\nQuestion\nAnswer\nThe 3D model (Fig-1) and the coronal MRI (Fig-2) provide \ndetailed and complementary views of the LITT trajectory's precise \ntargeting and positioning at the level of the LGN. The 3D model \noffers a comprehensive spatial understanding of the trajectory's \nintegration within the brainstem segmentation, while the coronal \nMRI confirms the trajectory's alignment and avoids critical \nstructures such as the optic chiasm and adjacent neural pathways. \nTogether, these images ensure a high degree of accuracy and \nsafety in the LITT procedure.\nCaption\nTemplate‐based A, 3D model and B, coronal MRI of an automated \nleft sided LITT trajectory (light green) at the level the LGN (dark \ngreen) showing its incorpoofration within the brainstem \nsegmentation (blue). Entire LITT trajectory overlaid onto 2D \ncoronal slice with intrahippocampal (opaque green) and \nextrahippocampal (transparent green) portions delineating the \nlateral to medial and superior to inferior angulation of the \ntrajectory. Position of trajectory at level of LGN (green). \nAmygdalohippocampal complex (yellow), lateral ventricles \n(pink), parahippocampal gyrus (orange), entorhinal cortex (teal), \nand optic tract (white) also shown …\nVisual field deficits (VFDs) are the most common complications \nfollowing laser interstitial thermal therapy (LITT) for mesial \ntemporal lobe epilepsy (MTLE), including contralateral superior \nquadrantanopia and hemianopia. Superior quadrantanopia results \nfrom ablation cavities extending into the optic radiation posterior \nto the hippocampus, while homonymous hemianopia can result \nfrom heat transfer to the lateral geniculate nucleus (LGN) during \nposterior hippocampal ablation. EpiNav-generated trajectories \navoid excessive heat transfer to the sagittal striatum, reducing the \nrisk of superior quadrantanopia. The ambient cistern and choroidal \nfissure provide natural barriers, and EpiNav maximizes distance \nfrom the brainstem and LGN to prevent thermal injury.\nSummarized Inline Text\nMedical Knowledge Note\nLITT (Laser Interstitial Thermal Therapy) trajectory planning \ninvolves precise navigation through specific brain structures. The \ntrajectory depicted incorporates critical areas like the lateral \ngeniculate nucleus (LGN) and brainstem, ensuring safety and \nefficacy. The intrahippocampal and extrahippocampal portions \ndelineate the trajectory's angulation, which is crucial for avoiding \ndamage to adjacent structures such as the amygdalohippocampal \ncomplex, lateral ventricles, parahippocampal gyrus, entorhinal \ncortex, and optic tract. This detailed 3D modeling aids in \npreoperative planning to minimize complications and optimize \ntherapeutic outcomes. \nHow do the 3D model and coronal MRI (Fig-1 and Fig-2) \ncontribute to the precise targeting and positioning of the LITT \ntrajectory at the level of the LGN?\nVisual Perception Description (Fig-1)\nVisual Perception Description (Fig-2)\nThe 3D model (Fig-1) and the coronal MRI (Fig-2) provide \ndetailed and complementary views of the LITT trajectory's precise \ntargeting and positioning at the level of the LGN. The 3D model \noffers a comprehensive spatial understanding of the trajectory's \nintegration within the brainstem segmentation, while the coronal \nMRI confirms the trajectory's alignment and avoids critical \nstructures such as the optic chiasm and adjacent neural pathways. \nTogether, these images ensure a high degree of accuracy and \nsafety in the LITT procedure.\nSelected Figure 2\nQWen2.5-32B\nFigure 29: Example of the Stage 4: Structural Instruction Generation process. This figure demonstrates how\nall the information generated in the preceding three stages (left) is integrated to produce a final, structured\nContext-Question-Answer instruction (right) for model training. This is the key step where processed data is\nconverted into a final training sample.\n48\n"}, {"page": 49, "text": "The two sub-images (Fig-1 and Fig-2) present a template-based \n3D model and a coronal MRI from a case involving an automated \nleft-sided LITT approach near the lateral geniculate nucleus \n(LGN). Anatomical annotations identify the LGN and a brainstem \nsegmentation, along with a visualized path related to the \nprocedure. The figures focus on depicting relevant neuroanatomy \nand the planned route in relation to labeled structures. \nContext\nRefined Context\nQWen2.5-32B\nThe selected sub-images (Fig-1 and Fig-2) illustrate a template-\nbased 3D model and a coronal MRI of an automated left-sided \nLITT trajectory (light green) targeting the level of the lateral \ngeniculate nucleus (LGN, dark green). Both images show the \nLITT trajectory incorporated within the brainstem segmentation \n(blue). These images provide complementary perspectives on the \nprecise targeting and positioning of the LITT trajectory, ensuring \nminimal damage to surrounding neural structures such as the optic \nchiasm and adjacent pathways.\nHow do the 3D model and coronal MRI (Fig-1 and Fig-2) \ncontribute to the precise targeting and positioning of the LITT \ntrajectory at the level of the LGN?\nQuestion\nAnswer\nThe 3D model (Fig-1) and the coronal MRI (Fig-2) provide \ndetailed and complementary views of the LITT trajectory's precise \ntargeting and positioning at the level of the LGN. The 3D model \noffers a comprehensive spatial understanding of the trajectory's \nintegration within the brainstem segmentation, while the coronal \nMRI confirms the trajectory's alignment and avoids critical \nstructures such as the optic chiasm and adjacent neural pathways. \nTogether, these images ensure a high degree of accuracy and \nsafety in the LITT procedure.\nFigure 30: Example of the Stage 5: Leakage-Prevented Context Refinement process. This figure demonstrates\nthe final quality control step. The model reviews the auto-generated context (left) and removes specific details\nthat might leak the answer, producing a more neutral and challenging refined context (right). This process is\ndesigned to prevent the model from learning to exploit ”cheating” cues in the context, thereby improving the\nquality of the training.\n49\n"}]}