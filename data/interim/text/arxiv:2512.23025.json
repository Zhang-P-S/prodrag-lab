{"doc_id": "arxiv:2512.23025", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.23025.pdf", "meta": {"doc_id": "arxiv:2512.23025", "source": "arxiv", "arxiv_id": "2512.23025", "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models", "authors": ["Wenxuan Xu", "Arvind Pillai", "Subigya Nepal", "Amanda C Collins", "Daniel M Mackin", "Michael V Heinz", "Tess Z Griffin", "Nicholas C Jacobson", "Andrew Campbell"], "published": "2025-12-28T18:00:57Z", "updated": "2025-12-28T18:00:57Z", "summary": "Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.23025v1", "url_pdf": "https://arxiv.org/pdf/2512.23025.pdf", "meta_path": "data/raw/arxiv/meta/2512.23025.json", "sha256": "b6d882381692a751b1f3853fecd181276a3dde8e866ffc00cdb72e3973acb08e", "status": "ok", "fetched_at": "2026-02-18T02:23:40.832130+00:00"}, "pages": [{"page": 1, "text": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning\nMultimodal Sensing with Language Models\nWenxuan Xu*1, Arvind Pillai*1, Subigya Nepal2, Amanda C Collins3, 4,\nDaniel M Mackin1, Michael V Heinz1, Tess Z Griffin1, Nicholas C Jacobson1,\nAndrew Campbell 1,\n1Dartmouth College, 2University of Virginia,\n3Massachusetts General Hospital, 4Harvard Medical School,\n*Equal contribution,\nCorrespondence: {wenxuan.xu.gr, arvind.pillai.gr}@dartmouth.edu\nAbstract\nMultimodal health sensing offers rich behav-\nioral signals for assessing mental health, yet\ntranslating these numerical time-series mea-\nsurements into natural language remains chal-\nlenging. Current LLMs cannot natively ingest\nlong-duration sensor streams, and paired sen-\nsor–text datasets are scarce. To address these\nchallenges, we introduce LENS, a framework\nthat aligns multimodal sensing data with lan-\nguage models to generate clinically grounded\nmental-health narratives. LENS first constructs\na large-scale dataset by transforming Ecologi-\ncal Momentary Assessment (EMA) responses\nrelated to depression and anxiety symptoms\ninto natural-language descriptions, yielding\nover 100,000 sensor–text QA pairs from 258\nparticipants. To enable native time-series in-\ntegration, we train a patch-level encoder that\nprojects raw sensor signals directly into an\nLLM’s representation space. Our results show\nthat LENS outperforms strong baselines on\nstandard NLP metrics and task-specific mea-\nsures of symptom-severity accuracy. A user\nstudy with 13 mental-health professionals fur-\nther indicates that LENS-produced narratives\nare comprehensive and clinically meaningful.\nUltimately, our approach advances LLMs as\ninterfaces for health sensing, providing a scal-\nable path toward models that can reason over\nraw behavioral signals and support downstream\nclinical decision-making.\n1\nIntroduction\nMental health conditions on the spectrum of anx-\niety and depression affect an estimated 18% and\n9.5% of adults in the United States each year (Johns\nHopkins Medicine, 2023).\nTraditional screen-\ning methods typically rely on structured clinical\ninterviews and validated self-report instruments\nsuch as the Patient Health Questionnaire (PHQ-\n9) (Kroenke et al., 2001) and the Generalized Anx-\niety Disorder scale (GAD-2) (Spitzer et al., 2006).\nHowever, these assessments are limited by their\nLENS\nQuestion\nAnswer\nFigure 1: Illustration of the LENS idea. Mobile and\nwearable sensing signals, combined with a question, are\npassed to LENS, which produces a natural-language\ndescription. Clinicians can then view an interpretable\nsnapshot of the user’s mental state instead of raw sensor\nstreams.\nhigh burden on clinicians, dependence on retro-\nspective self-reports, and reduced ecological valid-\nity because they are administered in controlled set-\ntings that do not capture an individual’s real-world\ncontext (Abd-Alrazaq et al., 2023).\nTo address these challenges, recent work has\nused mobile and wearable technologies to collect\npassive sensing data (for example, phone usage,\nspeech features, and heart rate) and to adminis-\nter ecological momentary assessments (EMA) (Xu\net al., 2023; Gomes et al., 2023; Nepal et al., 2024).\nA growing body of evidence shows that behavioral\nand physiological signals such as activity levels,\nsleep patterns, mobility, voice characteristics, and\nsmartphone interactions can indicate the severity\nof depression and anxiety symptoms (Wang et al.,\n2014; Saeb et al., 2015; Sheikh et al., 2021; Jacob-\nson et al., 2021). Together, these findings highlight\npassive sensing as a promising complementary ap-\nproach for mental health monitoring.\nIn parallel, recent work has demonstrated the po-\ntential of large language models (LLMs) for mental\nhealth assessment. Studies show that prompt engi-\nneering and fine-tuning can enable depression de-\ntection, symptom severity inference, physiological\nindicator prediction, and even generation of psycho-\nlogical rationales (Yang et al., 2024; Moon et al.,\n1\narXiv:2512.23025v1  [cs.CL]  28 Dec 2025\n"}, {"page": 2, "text": "2025; Kim et al., 2024). Despite these promis-\ning capabilities, LLMs struggle with long-duration\ntime series due to limitations in context length and\ntokenizer design, which prevent them from directly\ningesting raw numerical sequences (Spathis and\nKawsar, 2024). In contrast, multimodal vision\nand language models benefit from mature frame-\nworks that support native visual understanding (Li\net al., 2022; Dosovitskiy et al., 2021; Radford et al.,\n2021). However, comparable methods for native\ntime-series integration remain scarce (Tan et al.,\n2025). As a result, the few studies that combine\npassive health sensing with LLMs rarely operate\ndirectly on raw sensor streams (Kim et al., 2024;\nJustin et al., 2024; Englhardt et al., 2024). Overall,\nprogress in integrating behavioral sensing data with\nlanguage is limited by the lack of large datasets\nthat pair raw sensor streams with text and by the\nlimited capabilities of existing methods that align\ntime-series signals with language models.\nToward the goal of aligning sensing with lan-\nguage models for mental health, our contributions\nare threefold. (1) LENS data synthesis pipeline:\nWe introduce a pipeline that generates semantic\nmental health descriptions of multimodal sensing\ndata from EMA responses (Section 3.1), produc-\ning a dataset of more than 100,000 sensor–text\npairs and directly addressing the shortage of such\nresources. (2) LENS training: We propose a train-\ning strategy based on a patch-level time-series en-\ncoder that projects sensor signals into the language\nmodel’s representation space (Section 3.2). By\ninterleaving time-series and text embeddings and\nusing a two-stage curriculum, we show that LENS\ncan generate clinically grounded narratives. (3)\nComprehensive evaluation: We evaluate our ap-\nproach on a clinical dataset of 258 participants\ncomprising 50,957 unique samples (Section 4.2).\nBeyond custom LLM metrics, we conduct a user\nstudy with 13 mental health experts who manually\nassessed 117 narratives.\n2\nRelated Work\nRecent work explores mobile and wearable data\nfor mental health prediction, primarily focusing\non symptom classification (Kim et al., 2024; En-\nglhardt et al., 2024) or improving reasoning via\nmultimodal encoders (Justin et al., 2024). These\nsystems prioritize prediction over natural language\ngeneration, often producing text only as secondary\nreasoning traces. Furthermore, methods that se-\nrialize numerical data into tokens face significant\nscalability and tokenization constraints (Pillai et al.,\n2025; Spathis and Kawsar, 2024; Yoon et al., 2024).\nIn contrast, LENS anchors narrative generation in\nraw sensor measurements and clinically validated\nPHQ/GAD items to produce meaningful symptom\ndescriptions.\nLLMs are increasingly used to synthesize paired\ndatasets for time-series tasks by generating artifi-\ncial sequences or surrogate signals for QA pairs\nand explanations (Xie et al., 2025; Li et al., 2025b;\nYan et al., 2023; Imran et al., 2024). While scal-\nable, these pipelines rely on synthetic inputs that\nfail to capture the complexity of real physiological\nand behavioral signals. LENS instead pairs real-\nworld sensor streams with clinical assessments, us-\ning LLMs only to refine linguistic fluency via rig-\norous quality checks.\nWhile vision-language alignment has progressed\nrapidly (Liu et al., 2023), time-series and language\nintegration remains limited. Existing text-based\nserialization is constrained by numerical encoding\nand context length (Xue and Salim, 2023; Gruver\net al., 2023), while vision-based methods convert\nseries into images, introducing plot-engineering\nbiases and indirect representations (Yoon et al.,\n2024; Zhang et al., 2023). Alignment-based ap-\nproaches project encoder representations into LLM\nhidden states (Ming et al., 2023; Xie et al., 2025).\nHowever, by optimizing for synthetic QA, they\nrarely generate natural language aligned with real-\nworld clinical constructs. Our work differs from\nthese approaches by producing symptom-oriented\nnarratives tied to psychometric instruments and\ngrounded directly in raw signals\n3\nMethods\nLENS consists of two components: a scalable\ndataset-construction pipeline that transforms EMA\nresponses into high-quality sensor–text pairs (Sec-\ntion 3.1), and a sensor–text alignment method that\nenables native integration of raw time-series sig-\nnals into an LLM through a patch-based encoder\n(Section 3.2).\n3.1\nLENS: Dataset Construction\n3.1.1\nStudy\nOur longitudinal study investigates intra-day fluc-\ntuations in mental health symptoms among individ-\nuals diagnosed with major depressive disorder. In\nthis 90-day study, we recruited participants aged 18\n2\n"}, {"page": 3, "text": "years and older, residing in the United States. Each\nparticipant wore a Garmin vivoactive 3 device and\ninstalled our Android application. This setup en-\nabled the collection of passive sensing data, which\nare behavioral and physiological signals automat-\nically recorded from smartphones and wearables,\nand ecological momentary assessments (EMAs), in\nwhich participants actively reported depression and\nanxiety symptoms experienced over the past four\nhours. The study data will be publicly available\nthrough the funding body after an embargo period\nto verify privacy of personal identifiable informa-\ntion.\nThe EMA consists of 13 items adapted from the\nPHQ-9 and GAD-4 (Table 2; Appendix §B). It is\nadministered three times per day in the morning,\nafternoon, and evening, customized to each par-\nticipant’s waking time. Each item is rated on a\ncontinuous 0 (“Not at all”) to 100 (“Constantly”)\nscale. In addition to the EMAs, we collect mobile\nand wearable time-series signals, including GPS\ntraces, step counts, accelerometer-derived zero-\ncrossing rate (ZCR) and energy, conversation time,\nphone lock and unlock events, heart rate, sleep\nestimates, and stress levels. These signals have\nbeen shown to correlate with depressive and anxi-\nety symptoms (Choudhary et al., 2022). To align\nself-reported EMAs with corresponding sensing\ndata, we use each EMA’s completion time to re-\ntrieve the preceding four hours of sensor data. This\nprocedure results in a temporally aligned, multi-\nmodal dataset suitable for narrative synthesis and\nmodeling. Ultimately, we utilize 50,957 EMAs\nfrom 258 participants.\n3.1.2\nSensor Data Processing\nThe sensing data includes two types of signals:\n(1) continuous streams, which contain time-series\ndata such as steps, heart rate, accelerometer-derived\nzero-crossing rate (ZCR) and energy, phone lock\nand unlock state, stress level, and GPS traces, and\n(2) aggregated streams, which contain daily or\nwindow-level values such as sleep duration from\nthe previous night and total conversation time. Af-\nter preprocessing, all signals are standardized to\nfixed sampling rates to ensure consistent temporal\nalignment across modalities (see Appendix §C for\nspecific details).\n3.1.3\nLENS Narrative Synthesis\nTo address the lack of sensor-text datasets describ-\ning mental health symptoms, LENS constructs\nIn the past 4 hours, I have\nhad little interest or\npleasure in doing things.\nThe user {frequency} had\nlittle interest or pleasure\nin doing things.\nThe user not at all had\nlittle interest or pleasure\nin doing things.\nThe user reported having\nvery little interest or\nenjoyment in activities\nthey usually find\npleasurable, though they\ndidn’t feel consistently\ndown, depressed ...\nThe user has not\nexperienced any interest\nor enjoyment in\nactivities they usually\nfind pleasurable.\nLLM Language\nEnhancement\nInput\nTemplate\nTemplate Matching\nRaw Narrative\nScore injection \n(0-25: not at all)\nEnhanced\nItem-level\nNarrative\nConcatenate raw\nitem-level narrative\nRaw Item-level\nNarrative\nEnhanced\nSummary-level\nNarrative\nFigure 2:\nOverview of the narrative synthesis\npipeline. EMA questions and responses are mapped\nto templates populated with corresponding frequency\nphrases. Subsequently, an LLM refines the text at both\nthe item and summary levels (concatenated narratives)\nto enhance fluency and lexical diversity.\nhigh-quality QA datasets from self-reported EMA\nresponses to serve as ground truth for model train-\ning and evaluation. Because each numeric EMA\nresponse corresponds to a symptom intensity cate-\ngory aligned with DSM-5 semantics, these values\ncan be converted into natural language descriptions\nof participants’ experiences. We produce two QA\ndatasets: (1) Item-level QA dataset: Each sam-\nple represents a single EMA item and supports\nquestion-specific supervision, and (2) Summary-\nlevel QA dataset: Each sample reflects all 14 EMA\nquestions and targets full-summary generation.\nResponses →Answers (A). EMA responses are\nconverted into narrative labels as shown in Figure 2.\nFor each question, we first transform the EMA item\ninto a symptom-focused template sentence. We\nthen insert the frequency phrase associated with its\nnumeric range (0–25: not at all, 26–50: sometimes,\n51–75: often, 76–100: constantly) to obtain the raw\nitem-level narrative. Question 14 is treated as bi-\n3\n"}, {"page": 4, "text": "Original EMA\nLLM\nEnhancement\nEnhanced Summary-\nlevel Narrative \nMulti-agent LLM-\nas-a-judge\nQuality?\nFinal Narrative\nLabels\nQA Datasets\nPASS\nFAIL\nSummary-level \nNarrative Template\nItem-level \nNarrative Template\nEnhanced Item-level\nNarrative \nFigure 3: LENS dataset construction pipeline. EMA responses are first converted into item-level and summary\ntemplate narratives, which are then rewritten into fluent, enhanced narratives using Qwen2.5-14B. A multi-agent\nLLM-as-a-judge system conducts automatic quality control, routing failed cases back for regeneration until they\nsatisfy all criteria. The final accepted narratives are then paired with paraphrased question variants to construct the\nQA datasets.\nnary, and overall severity is categorized as mild,\nmoderate, or severe. Summary-level narratives\nare formed by concatenating item-level narratives\nbefore applying LLM refinement. To reduce the\nmechanical tone of the templates, we use prompt-\nbased rewriting with the locally deployed Qwen2.5-\n14B model. The system prompt enforces factual\naccuracy, consistent terminology, and stigma-free\nlanguage, while the user prompt supplies the rule-\nbased text and requests a fluent rewrite(See prompt\ntemplate in Appendix§G). This process produces\n101,914 item-level narratives and 50,957 summary-\nlevel narratives that serve as ground truth for train-\ning LENS.\nQuestions (Q). The ground-truth questions con-\nsist of item-level prompts for each EMA question\nand a summary-level prompt that covers all items\nalong with the overall severity statement. To in-\ncrease linguistic diversity, we use GPT-4o to gen-\nerate paraphrased variants of every question. For\neach original prompt, we created ten semantically\nequivalent but lexically distinct phrasings. During\nQA dataset construction, one paraphrased variant\nis randomly sampled for each instance.\nQuality Control. To ensure narrative reliabil-\nity, we implement an automatic quality-control\npipeline using a multi-model LLM-judge system\n(Figure 3). Each judge model follows an LLM-as-\na-Judge prompting template (Li et al., 2025a) with\nrule-augmented instructions that embed evalua-\ntion principles, baseline references, and dimension-\nspecific rubrics directly in the prompt. Judges com-\npare the template-based and enhanced narratives in\na pairwise format and output five dimension scores\n(1–5), confidence values, and a short rationale, pro-\nviding structured and transparent assessments. Be-\ncause individual LLM judges can be biased, we use\nthree independent models (Mistral-7B (Jiang et al.,\n2023), Llama-3.1-8B (Grattafiori et al., 2024), and\nQwen2.5-7B (Qwen et al., 2025)).\nDimension\nscores are averaged and rounded, and their sum\nforms a total quality score; confidence values are\naveraged similarly. A narrative is accepted only if\nits average total score exceeds 20 (out of 25) and\nits mean confidence exceeds 0.8. Otherwise, it is\nreturned for regeneration and re-evaluation (FAIL\nin Figure 3). This iterative refine-and-judge loop,\nakin to Refine-n-Judge (Cayir et al., 2025), filters\nout low-quality outputs and ensures that finalized\nnarratives faithfully reflect EMA responses while\nimproving fluency and diversity without introduc-\ning distortions.\n3.2\nLENS: Training & Architecture\n3.2.1\nTime-series Encoder\nWe encode each time–series stream indepen-\ndently using a lightweight patch-based module\nthat converts raw scalar values into language-\nmodel–compatible embeddings. Given a univariate\nsequence S = {st}T\nt=1, we first apply a reversible\nvalue normalization to stabilize scale while keep-\ning absolute magnitudes recoverable. Let µ and\nσ denote the per-stream mean and standard devia-\ntion, then ˜st = st−µ\nσ , and auxiliary statistics such\nas (µ, σ, mmin, mmax) are inserted into the textual\nprompt as metadata, allowing the model to reason\nabout original numerical ranges inside the language\nspace (Langer et al., 2025; Xie et al., 2025). The\nnormalized sequence is divided into N = ⌈T/k⌉\nnon-overlapping patches of width k, where k = 8\nfor all streams in our experiments. Each timestep is\naugmented with a learnable positional embedding\n4\n"}, {"page": 5, "text": "qt ∈Rdp with dp = 16. For each patch, scalar\nvalues and positional codes are concatenated to\nform\nui = {˜st ⊕qt}ik\nt=(i−1)k+1 ∈Rk(1+dp).\nEach patch representation is projected into the\nhidden space of the pretrained language model via a\nmultilayer perceptron fθ consisting of 5 layers with\nhidden width 5120, matching the LLM embedding\ndimension d. The encoder outputs one embedding\nper patch,\nzi = fθ(ui) ∈Rd,\nZ = {z1, . . . , zN}.\nThis design captures localized temporal patterns at\nthe patch level while preserving absolute numerical\nsemantics through reversible normalization.\n3.2.2\nArchitecture\nGiven an instruction text X = {x1, . . . , xM} and\nK time–series streams {S(k)}K\nk=1, we inject the\nnormalization metadata described above into the\ntext. After this step, the prompt ˜X contains special\nplaceholder tokens <ts> and </ts> to mark the\nposition of each stream. The text ˜X is then tok-\nenized and passed through the pretrained LLM em-\nbedding layer femb\nϕ\n, producing a sequence of text\nembeddings Etext = {e1, . . . , eL} ∈RL×d. In\nparallel, each time–series stream S(k) is processed\nby the encoder, yielding patch embeddings Z(k) =\n{z(k)\n1 , . . . , z(k)\nNk} with z(k)\ni\n∈Rd. The multimodal\nembedding sequence is formed by concatenating\nthe text embeddings and the patch embeddings at\nthe positions referenced by the placeholders, i.e.,\nH = concat\n\u0000Etext, Z(1), . . . , Z(K)\u0001\n∈RLmm×d,\nwhich interleaves natural–language and time–series\nrepresentations in a single ordered context. This\nunified sequence is then fed into the subsequent\ntransformer blocks of the pretrained LLM, enabling\nmultimodal reasoning over both textual instructions\nand temporal patterns.\n3.2.3\nTraining\nStage 1: Encoder Alignment. The first stage aims\nto establish a stable alignment between temporal\nfeatures and their textual descriptions. We use the\nalignment dataset from ChatTS (Xie et al., 2025),\nwhich takes a QA form and probes time–series un-\nderstanding across different signal types and tem-\nporal behaviors, enabling the encoder to capture\ntrends, correlations, and local pattern variations\nrather than raw numeric fluctuations. However,\nQ: Summarize\nthe user’s\ncurrent\nmental well-\nbeing?\nTime Series\nEncoder\nLLM Text\nEmbedder\nLLM\nLLM Text\nEmbedder\nHeart rate\n(beats per\nminute, bpm)\nis of length\n1440 <ts>\n</ts>\nAnswer\n... The user has reported significant\nfatigue and low energy levels ...\nFigure 4: LENS Architecture. The model accepts\nmultimodal inputs consisting of description text (e.g.,\n\"Heart rate...\"), instruction text (e.g., \"Summarize the\nuser’s current mental well-being?\") and raw time-series\nsensor streams (e.g., heart rate). The text is processed by\na frozen LLM text embedder (f emb\nϕ\n), while time-series\ndata is encoded by a trainable patch-based encoder (fθ).\nThe resulting embeddings are concatenated into a uni-\nfied sequence (H) and processed by the LLM backbone\nto generate a natural language response (Y ).\nour final objective is to generate clinically coher-\nent symptom narratives rather than attribute-only\ndescriptions. To prevent the encoder from over-\nspecializing on low-level attribute queries, we in-\nterleave a small portion of narrative and general\nQA samples into the alignment corpus (8:1:1 with\nalignment data), giving the model early exposure\nto natural question forms and narrative structure\nand better preparing it for downstream symptom-\nfocused generation.\nStage 2: Supervised Fine-Tuning on Symp-\ntom Narratives. After encoder alignment, we train\nthe model to perform symptom-centered question\nanswering and narrative generation. We fine-tune\n5\n"}, {"page": 6, "text": "LENS on the two EMA-derived QA datasets de-\nscribed in Section 3.1.3, which respectively pro-\nvide item-level supervision for single-symptom\ninterpretation and summary-level supervision for\nmulti-symptom synthesis. To prevent the model\nfrom overfitting to a single response style and to\nenhance its ability to follow structured outputs,\nwe additionally include an instruction-following\n(IF) dataset constructed from predefined templates.\nThis dataset exposes the model to consistent an-\nswer formats, encouraging stable response organi-\nzation under different prompts. Because real-world\ntime-series queries vary widely in temporal reso-\nlution, we further interleave an alignment-random\nsubset in which sequence lengths are uniformly\nsampled between 64 and 1024, enabling the model\nto generalize across heterogeneous sampling rates\nand time spans. During supervised fine-tuning,\nthe four datasets are mixed using a 0.3:0.3:0.2:0.2\nratio (item-level QA : summary-level QA : IF :\nalignment-random), which balances symptom nar-\nrative grounding, structural instruction patterns,\nand robustness to sequence-length variability.\nGiven an instruction text X = {x1, . . . , xM}\nand K time–series streams {S(k)}K\nk=1, the model\nis trained in a standard supervised autoregressive\nmanner. For a target response Y = {y1, . . . , yU},\nthe objective is\nL = −\nU\nX\nt=1\nlog pϕ\n\u0000yt | X, {S(k)}K\nk=1, y1:t−1\n\u0001\n.\nwhere ϕ denotes all trainable parameters. We adopt\nfull-parameter fine-tuning, updating both the pre-\ntrained LLM backbone and the time-series encoder.\nSee Appendix §A for hardware and batching de-\ntails.\n4\nExperiments\n4.1\nEvaluation Settings\nBaselines. To contextualize LENS’s performance,\nwe compare it with baselines that follow common\nstrategies for modeling time-series signals with\nLLMs. Prior work often encodes time-series val-\nues as raw text (Kim et al., 2024; Gruver et al.,\n2023), so we adopt the same principles and utilize\nthe Qwen2.5-14B (Yang et al., 2025) with few-shot\nprompting as the backbone; we refer to this as TS-\nText. Another promising approach converts time-\nseries data into visual representations to support\ndownstream reasoning (Yoon et al., 2024; Liu et al.,\n2025). Following this approach, we transform each\nsignal into a plot and generate narratives using\nthe Qwen-2.5VL-32B model in a few-shot setting.\nThis baseline is denoted as TS-Image. Inference\nfor all baseline models utilizes default configura-\ntions to ensure a controlled comparison, and we\nutilize participant-level splits (approx. 70:15:15) to\nprevent information leakage, ensuring that all data\nfrom a given individual appears in only one split\n(Appendix §A).\nMetrics.\nWe evaluate model performance\nusing three categories of metrics. (1) Linguistic\nMetrics: Generated narratives are assessed with\nROUGE-1/2/L (Lin, 2004), BLEU-4 (Papineni\net al., 2002), METEOR (Banerjee and Lavie,\n2005), and BERTScore (Zhang et al., 2020), which\nmeasure lexical overlap, structural consistency,\nand semantic similarity with reference narratives.\n(2) Symptom-grounded Evaluation: To assess\nclinical alignment,\nwe employ a structured\nLLM-as-a-judge protocol using gpt-4.1-mini\n(See prompt template in Appendix §G. The judge\nis queried with temperature set to 0 to ensure\ndeterministic and reproducible evaluations). For\neach of the PHQ-related symptom categories,\nthe judge first outputs a JSON record containing\nref_presence,ref_severity\npred_presence,\nref_severity, and pred_severity, indicating\nwhether the model omits or hallucinates symptom\ndimensions and whether predicted severity is\nfaithful to the ordinal reference. (3) Item-level QA\nEvaluation: For single-question outputs, evalua-\ntion reduces to a record specifying ref_severity\nand\npred_severity;\nsymptom\npresence\nis\nimplicit in the question itself, allowing severity\ncorrectness to be measured without aggregating\nacross narrative spans.\nDetailed definitions of\ncoverage, presence-aware severity alignment, and\nthe weighting procedure used for ordinal scoring\nare provided in Appendix §D.\nUser study.\nTo assess the characteristics of\nLENS-generated narratives, we recruited 13 men-\ntal health experts to evaluate LENS (14B; zero-\nshot) along with the two baselines: TS-Image (32B,\nfew-shot) and TS-Text (14B, few-shot), across four\nrating dimensions. Each example is a narrative\npaired with a table of 12 ground-truth symptoms\nand assessed as follows: (1) Comprehensiveness:\n“Does the narrative mention the symptom?”; (2)\nAccuracy: “Does the narrative accurately describe\nthe symptom severity (including synonyms)?”; (3)\nClinical Utility: “Does the narrative provide clini-\n6\n"}, {"page": 7, "text": "Method\nLinguistic Metrics\nLLM-as-a-Judge Metrics\nROUGE-L\nBERTScore\nCoverage\nPresence Alignment\nSummary-level Evaluation\nLENS\n0.409\n0.775\n0.801\n0.601\nTS-Text\n0.151\n0.630\n0.614\n0.372\nTS-Image\n0.373\n0.764\n0.740\n0.579\nItem-level Evaluation\nLENS\n0.603\n0.832\n–\n0.732\nTS-Text\n0.142\n0.604\n–\n0.665\nTS-Image\n0.136\n0.602\n–\n0.687\nTable 1: Results on Summary and Item level genera-\ntion. Additional metrics are provided in Appendix §E.\ncally useful information that informs care?”; and\n(4) Language Cohesion: “Is the narrative coher-\nent, easy to understand, and focused on depression\nand anxiety symptoms?” Comprehensiveness and\naccuracy were collected as yes/no responses and\ngrouped into five categories. For comprehensive-\nness: 0–2 = Very Poor, 3–5 = Poor, 6–7 = Adequate,\n8–11 = Good, 12 = Excellent. For accuracy: 0–2 =\nMajor Discrepancy, 3–5 = Substantial Discrepancy,\n6–7 = Partial Agreement, 8–11 = High Agreement,\n12 = Complete Agreement. Clinical utility and lan-\nguage cohesion were rated on a 1–5 Likert scale.\nIn total, we evaluated 117 narratives, with each\nexpert rating 9 narratives (3 samples from each\nmodel). Additional information regarding the user\nstudy and expert background is provided in Ap-\npendix §F. To compare the models across the four\ndomains, we first computed per-rater mean scores\nfor each model by averaging over the three eval-\nuated samples, and then performed paired t-tests\nacross raters with Bonferroni correction. Effect\nsizes are reported using Cohen’s dz.\n4.2\nResults\nLENS consistently outperforms baselines in gen-\nerating comprehensive clinical narratives. In\nthe summary-level evaluation (Table 1), LENS\nachieves the highest scores across all linguistic\nmetrics, recording a ROUGE-L of 0.409 and a\nBERTScore of 0.775. This represents a 9.6% and\n1.4% improvement over the strongest baseline, TS-\nImage, which achieved 0.373 and 0.764, respec-\ntively. The significantly lower performance of TS-\nText (ROUGE-L=0.151) highlights the difficulty\nof generating cohesive summaries from time-series\ntext descriptions alone, whereas LENS effectively\nintegrates multi-modal signals to produce struc-\nturally and semantically robust narratives.\nLENS demonstrates superior grounding in\nclinical symptoms, minimizing hallucinations.\nTS-Text 14B\n(few-shot)\nTS-Image 32B \n(few-shot)\nLENS 14B \n(zero-shot)\n4\n11\n10\n10\n21\n30\n4\n18\n9\nComprehensiveness\nExcellent:\nAll required symptoms \ncovered [12]\nGood:\nAll major symptoms \ncovered [8 11]\nAdequate:\nSome symptoms \nmissing [6 7]\nPoor:\nSeveral key symptoms \nmissing [3 5]\nVery Poor:\nMany critical symptoms \nmissing [0 2]\nFigure 5: User Study: Comprehensiveness. Expert\nratings compare how many symptoms each model’s\nnarrative successfully covers.\nFrom a clinical perspective, LENS achieves the\nhighest alignment with ground-truth diagnoses. In\nsummary-level generation, it attains a Symptom\nCoverage score of 0.801 and a Presence Align-\nment score of 0.601. This indicates that LENS is\nmore capable of capturing the full spectrum of pa-\ntient symptoms (K = 14 categories) without omit-\nting critical indicators or fabricating non-existent\nones, outperforming TS-Image (Coverage=0.740)\nand greatly surpassing TS-Text (Presence Align-\nment=0.372).\nLENS exhibits strong performance in fine-\ngrained, item-level query answering. Compared\nto summary-level narrative generation, the perfor-\nmance gap in the item-level evaluation is more\npronounced, indicating that the task requires more\nprecise retrieval (Table 1). For example, LENS\nachieves a ROUGE-L score of 0.603, which is more\nthan four times higher than both TS-Text (0.142)\nand TS-Image (0.136). This trend holds for se-\nmantic and clinical metrics as well, with LENS\nsecuring the highest Presence Alignment of 0.732\n(10.1% improvement over TS-Text and 6.6% over\nTS-Image) and a BERTScore of 0.832 (over 37%\nimprovement against both baselines). These results\nsuggest that while baselines struggle to isolate spe-\ncific symptom intensities in a QA format, LENS\nmaintains high fidelity to the reference severity lev-\nels.\nMental health experts rate LENS narratives\nsignificantly higher than TS-Text. As shown in\nFigures 5 to 7, LENS consistently outperforms TS-\nText across all four evaluation dimensions. Specifi-\ncally, LENS narratives receive significantly higher\nexpert ratings for comprehensiveness (4.23 vs.\n2.97; T = 7.02; p < 0.01; dz = 1.12), accuracy\n(2.61 vs. 1.53; T = 6.49; p < 0.01; dz = 1.04),\nclinical utility (3.25 vs. 1.84; T = 7.16; p < 0.01;\n7\n"}, {"page": 8, "text": "TS-Text 14B\n(few-shot)\nTS-Image 32B \n(few-shot)\nLENS 14B \n(zero-shot)\n23\n6\n5\n12\n15\n16\n3\n4\n7\n1\n13\n11\n1\nAccuracy\nComplete Agreement:\nAll symptom severities \nare correct [12]\nHigh Agreement:\nAll symptom severities \nare correct [8 11]\nPartial Agreement:\nSome symptom severities \nare correct [6 7]\nSubstantial Discrepancy:\nMany key symptom severities \nare incorrect [3 5]\nMajor Discrepancy:\nAlmost all symptom severities \nare incorrect [0 2]\nFigure 6: User Study: Accuracy. Expert ratings assess\nhow well each model’s narrative captures the severity\nof symptoms reported in the ground-truth EMA.\ndz = 1.15), and language cohesion (3.82 vs. 2.23;\nT = 7.34; p < 0.01; dz = 1.17). These results\ndemonstrate that experts find narratives derived\nfrom native time-series integration to be not only\nmore accurate but also more practically useful for\ncare than those generated from text descriptions\nalone.\nExpert evaluation demonstrates performance\nparity with larger visual baselines. Comparing\nLENS to TS-Image (Figures 5 to 7), we observe no\nstatistically significant differences between the two\nmodels across any of the four evaluation dimen-\nsions. Expert ratings are comparable for compre-\nhensiveness (4.23 vs. 4.46; T = −2.47; p > 0.01;\ndz = −0.39), accuracy (2.61 vs. 2.69; T = −0.42;\np > 0.01; dz = −0.06), clinical utility (3.82\nvs.\n3.71; T = 1.52; p > 0.01; dz = 0.24),\nand language cohesion (3.25 vs. 2.94; T = 0.56;\np > 0.01; dz = 0.09). These results indicate that\nLENS matches the performance of the 2.2× larger\nTS-Image while offering a significantly streamlined\ninference process. By replacing complex visual\nplot engineering with native time-series encoding,\nLENS achieves these results in a zero-shot setting.\nInterestingly, experts identified LENS as having\nbetter real-world applicability, evidenced by its\nmarginally higher ratings for clinical utility and\nlanguage cohesion.\nAblation and scalability analysis. We analyze\nLENS’ architecture through ablation and scaling\nstudies detailed in Appendix §E. An evaluation\ncomparing LENS to a fine-tuned text-only base-\nline (TS-Text-FT) reveals that explicit time-series\nencoding is essential for capturing temporal struc-\ntures; LENS significantly outperforms the text-\nonly variant, particularly in item-level QA where\nthe ROUGE-L gap is markedly wider (0.6030 vs.\n0.1717) (Table 4). Furthermore, LENS demon-\nTS-Text 14B\n(few-shot)\nTS-Image 32B \n(few-shot)\nLENS 14B \n(zero-shot)\n17\n5\n3\n15\n10\n9\n3\n8\n5\n4\n14\n19\n2\n3\nClinical Utility\nTS-Text 14B\n(few-shot)\nTS-Image 32B \n(few-shot)\nLENS 14B \n(zero-shot)\n12\n2\n1\n13\n4\n4\n7\n3\n4\n7\n24\n22\n6\n8\nLanguage Cohesion\nStrongly Agree\nAgree\nNeutral\nDisagree\nStrongly Disagree\nFigure 7: User Study: Clinical Utility & Language\nCohesion. Comparing expert ratings indicating the use-\nfulness of the narratives and the cohesiveness of the\nlanguage.\nstrates superior computational efficiency, requir-\ning approximately 930 tokens per sample, a re-\nduction of roughly 94% compared to verbose text\nserialization and 4× relative to vision-based mod-\nels (Figure 8). We also observe that while basic\ninstruction-following can be established with 10%\nof the training data, the full dataset is necessary\nto maximize clinical fidelity in complex narratives,\nwith Presence Alignment improving monotonically\nas data size increases (Table 5). Finally, experi-\nments with a lighter LENS-7B variant show that it\nmaintains linguistic performance (ROUGE-L 0.409\nvs. 0.410) while trailing slightly in complex clini-\ncal alignment (0.559 vs. 0.601) (Table 6).\n5\nConclusion\nIn this work, we introduced LENS, a framework for\ngenerating clinical narratives by aligning raw sen-\nsor streams with LLMs. Through a scalable data-\nsynthesis pipeline and a patch-based time-series\nencoder, LENS addresses two key challenges: the\nscarcity of paired sensor–text data and the diffi-\nculty of processing long-horizon numerical signals\nwithin LLMs. Using a dataset of 50,957 samples,\nour evaluation showed that LENS produced linguis-\ntically fluent and clinically accurate descriptions\nof depression and anxiety symptoms. LENS also\nmatched the performance of 2.2× larger vision-\nbased models while avoiding the overhead of plot\ngeneration, and mental-health experts rated its nar-\nratives as more applicable and clinically useful than\ntext-based baselines. Future work should examine\nLENS’s transferability to broader demographics\nand additional psychological instruments. Overall,\nLENS represents a fundamental step toward eco-\nlogically valid mental-health monitoring by trans-\nforming raw sensor data into interpretable clinical\ninsights.\n8\n"}, {"page": 9, "text": "Limitations\nSome limitations of this work should be noted.\nFirst, although we demonstrate symptom-level gen-\neralization for depression and anxiety within a clin-\nical population, we do not evaluate whether the ap-\nproach transfers to other demographic groups or ad-\nditional self-report psychological instruments. Ex-\namining generalization across broader populations\nand diverse measurement scales remains an impor-\ntant direction for future work and would strengthen\nthe case for large-scale generation of paired sen-\nsor data and corresponding narrative descriptions.\nSecond, the current framework primarily focuses\non short temporal windows, analyzing behavior\nover the preceding four hours. While this design\nsupports in-the-moment assessment and responsive-\nness, it may fail to capture longer-term behavioral\npatterns and symptom trajectories that unfold over\nweeks or months. Incorporating multi-scale tempo-\nral modeling could improve sensitivity to chronic\nor slowly evolving mental health states.\nThird, although the model integrates multiple\nsensor streams, it may still lack sufficient environ-\nmental or situational context to fully disambiguate\ncertain physiological or behavioral signals. For ex-\nample, elevated heart rate or reduced mobility may\nreflect physical activity, illness, or stress, depend-\ning on context that is not always observable from\npassive sensing alone. Future work could mitigate\nthis limitation by incorporating richer contextual\nsignals or structured user input. Finally, the data\nsynthesis pipeline relies on an iterative Refine-n-\nJudge loop involving multiple LLMs. While this\napproach improves output quality and consistency,\nit introduces additional computational overhead.\nAs a result, scaling the limitations pipeline to real-\ntime deployment or very large datasets may be\nchallenging under constrained computational bud-\ngets. Exploring more efficient training or distilla-\ntion strategies could help address this limitation.\nEthical Considerations\nStudy Participation. The study procedures re-\nceived approval from an institutional review board,\nand all participants provided both written and ver-\nbal informed consent before data collection began.\nOf the 300 participants recruited via online adver-\ntisements, data from 258 were utilized for this anal-\nysis. Study eligibility was determined through a\nStructured Clinical Interview for DSM-5 (SCID)\nconducted by trained clinicians, with inclusion lim-\nited to individuals diagnosed with Major Depres-\nsive Disorder (MDD) who did not have comorbid\nbipolar disorder, active suicidality at intake, or psy-\nchosis. The participant sample was predominantly\nWhite (79%) and female (84%), with a mean age of\n40 years. Representation of racial minority groups,\nincluding 12% Hispanic or Latino participants, was\ncomparable to the broader U.S. population with\nMDD. Most participants (93%) had some college\neducation, and 61% were employed, with house-\nhold incomes broadly aligned with national dis-\ntributions. Participants were compensated $1 for\neach completed EMA, plus a $50 bonus for achiev-\ning a 90% completion rate. Safety was ensured\nthrough ongoing clinical oversight and automated\nsafeguards within the mobile application that noti-\nfied clinicians in the event of active suicidality.\nSystems & Software. All modeling was per-\nformed within a secure, closed computing envi-\nronment using locally deployed large language\nmodels. Raw participant data were never trans-\nmitted to external APIs, cloud services, or third-\nparty platforms; only AI-generated outputs were\npermitted outside the secure environment. Access\nto sensitive data was restricted to authorized re-\nsearchers who had completed institutional review\nboard–approved human subjects training. The mod-\neling environment employed role-based access con-\ntrols, audit logging, and continuous monitoring to\ntrack data usage and prevent unauthorized access.\nCollectively, these safeguards reduced the risk of\nre-identification of personally identifiable informa-\ntion and ensured adherence to IRB-approved data\nprotection and privacy protocols.\nAdverse Usage. LLMs for interpreting time-\nseries data in mental health contexts offer substan-\ntial potential benefits. However, we caution against\nseveral forms of misuse to protect participant safety,\nprivacy, and clinical integrity. Although LLM nar-\nratives provide valuable insights, they must aug-\nment clinical workflows rather than serve as stan-\ndalone diagnostic tools. Over-reliance on auto-\nmated summaries may obscure critical nuances\nwithin the raw sensor data, while the utilization of\nsensitive signals like GPS traces necessitates strict\ngovernance and access controls to prevent unautho-\nrized surveillance or secondary data use. Further-\nmore, as current LLMs cannot entirely eliminate\nthe risk of hallucinations, all generated outputs\nmust be reviewed by qualified experts to ensure\nclinical judgment remains the final authority for\ninterpretation and intervention.\n9\n"}, {"page": 10, "text": "References\nAlaa Abd-Alrazaq, Rawan AlSaad, Farag Shuweihdi,\nArfan Ahmed, Sarah Aziz, and Javaid Sheikh. 2023.\nSystematic review and meta-analysis of performance\nof wearable artificial intelligence in detecting and pre-\ndicting depression. NPJ Digital Medicine, 6(1):84.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nDerin Cayir, Renjie Tao, Rashi Rungta, Kai Sun, Sean\nChen, Haidar Khan, Minseok Kim, Julia Reinspach,\nand Yue Liu. 2025.\nRefine-n-judge:\nCurating\nhigh-quality preference chains for llm-fine-tuning.\nPreprint, arXiv:2508.01543.\nSoumya Choudhary, Nikita Thomas, Janine Ellenberger,\nGirish Srinivasan, and Roy Cohen. 2022.\nA ma-\nchine learning approach for detecting digital behav-\nioral patterns of depression using nonintrusive smart-\nphone data (complementary path to patient health\nquestionnaire-9 assessment): Prospective observa-\ntional study. JMIR Formative Research, 6(5):e37736.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021.\nAn image\nis worth 16x16 words:\nTransformers for image\nrecognition at scale. Preprint, arXiv:2010.11929.\nZachary Englhardt, Chengqian Ma, Margaret E. Morris,\nXuhai \"Orson\" Xu, Chun-Cheng Chang, Lianhui Qin,\nDaniel McDuff, Xin Liu, Shwetak Patel, and Vikram\nIyer. 2024. From Classification to Clinical Insights:\nTowards Analyzing and Reasoning About Mobile and\nBehavioral Health Data With Large Language Mod-\nels. Proceedings of the ACM on Interactive, Mobile,\nWearable and Ubiquitous Technologies, 8(2):1–25.\nArXiv:2311.13063 [cs].\nNuno Gomes, Matilde Pato, Andre Ribeiro Lourenco,\nand Nuno Datia. 2023. A survey on wearable sensors\nfor mental health monitoring. Sensors, 23(3):1330.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nand 1 others. 2024. The llama 3 herd of models.\nPreprint, arXiv:2407.21783.\nNate Gruver, Marc Finzi, Shikai Qiu, and Andrew G\nWilson. 2023. Large language models are zero-shot\ntime series forecasters. Advances in Neural Informa-\ntion Processing Systems, 36:19622–19635.\nSheikh Asif Imran, Mohammad Nur Hossain Khan,\nSubrata Biswas, and Bashima Islam. 2024. LLaSA:\nA Multimodal LLM for Human Activity Analysis\nThrough Wearable and Smartphone Sensors. arXiv\npreprint. ArXiv:2406.14498 [cs] version: 2.\nNicholas C Jacobson, Damien Lekkas, Raphael Huang,\nand Natalie Thomas. 2021. Deep learning paired\nwith wearable passive sensing data predicts deteri-\noration in anxiety disorder symptoms across 17–18\nyears. Journal of affective disorders, 282:104–111.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, and 1 others. 2023.\nMistral 7b.\nPreprint,\narXiv:2310.06825.\nJohns Hopkins Medicine. 2023. Mental Health Dis-\norder Statistics. https://www.hopkinsmedicine.\norg/health/wellness-and-prevention/\nmental-health-disorder-statistics.\nAc-\ncessed: November 14, 2025.\nCosentino Justin, Belyaeva Anastasiya, Liu Xin, Fur-\nlotte Nicholas, A., Yang Zhun, Lee Chace, Schenck\nErik, Patel Yojan, Cui Jian, Schneider Logan, Dou-\nglas, Bryant Robby, Gomes Ryan, G., Jiang Allen,\nLee Roy, Liu Yun, Perez Javier, Rogers Jameson,\nK., Speed Cathy, Tailor Shyam, and 15 others. 2024.\nTowards a personal health large language model.\nYubin Kim, Xuhai Xu, Daniel McDuff, Cynthia\nBreazeal, and Hae Won Park. 2024.\nHealth-llm:\nLarge language models for health prediction via wear-\nable sensor data.\nKurt Kroenke, Robert L Spitzer, and Janet BW Williams.\n2001. The phq-9: validity of a brief depression sever-\nity measure. Journal of general internal medicine,\n16(9):606–613.\nPatrick Langer,\nThomas Kaar,\nMax Rosenblattl,\nMaxwell A. Xu, Winnie Chow, Martin Maritsch,\nAradhana Verma, Brian Han, Daniel Seung Kim,\nHenry Chubb, Scott Ceresnak, Aydin Zahedivash,\nAlexander Tarlochan Singh Sandhu, Fatima Ro-\ndriguez, Daniel McDuff, Elgar Fleisch, Oliver\nAalami, Filipe Barata, and Paul Schmiedmayer. 2025.\nOpentslm: Time-series language models for reason-\ning over multivariate medical text- and time-series\ndata. Preprint, arXiv:2510.02410.\nDawei Li, Bohan Jiang, Liangjie Huang, Alimohammad\nBeigi, Chengshuai Zhao, Zhen Tan, Amrita Bhat-\ntacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu,\nKai Shu, Lu Cheng, and Huan Liu. 2025a. From Gen-\neration to Judgment: Opportunities and Challenges of\nLLM-as-a-judge. arXiv preprint. ArXiv:2411.16594\n[cs].\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. Preprint, arXiv:2201.12086.\nZechen Li, Shohreh Deldari, Linyao Chen, Hao Xue,\nand Flora D. Salim. 2025b. SensorLLM: Human-\nIntuitive Alignment of Multivariate Sensor Data with\nLLMs for Activity Recognition.\narXiv preprint.\nArXiv:2410.10624 [cs].\n10\n"}, {"page": 11, "text": "Chin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023.\nVisual instruction tuning.\nPreprint,\narXiv:2304.08485.\nHaoxin Liu, Chenghao Liu, and B. Aditya Prakash.\n2025. A picture is worth a thousand numbers: En-\nabling LLMs reason about time series via visualiza-\ntion. In Proceedings of the 2025 Conference of the\nNations of the Americas Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers), pages 7486–\n7518, Albuquerque, New Mexico. Association for\nComputational Linguistics.\nJin Ming, Wang Shiyu, Ma Lintao, Chu Zhixuan, Zhang\nJames, Y., Shi Xiaoming, Chen Pin-Yu, Liang Yux-\nuan, Li Yuan-Fang, Pan Shirui, and Wen Qingsong.\n2023. Time-LLM: Time series forecasting by repro-\ngramming large language models.\nSehwan Moon, Aram Lee, Jeong Eun Kim, Hee-Ju\nKang, Il-Seon Shin, Sung-Wan Kim, Jae-Min Kim,\nMin Jhon, and Ju-Wan Kim. 2025. Depressllm: In-\nterpretable domain-adapted language model for de-\npression detection from real-world narratives.\nSubigya Nepal, Wenjun Liu, Arvind Pillai, Weichen\nWang, Vlado Vojdanovski, Jeremy F Huckins, Court-\nney Rogers, Meghan L Meyer, and Andrew T Camp-\nbell. 2024. Capturing the college experience: A four-\nyear mobile sensing study of mental health, resilience\nand behavior of college students during the pandemic.\nProceedings of the ACM on interactive, mobile, wear-\nable and ubiquitous technologies, 8(1):1–37.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nArvind Pillai,\nDimitris Spathis,\nSubigya Nepal,\nAmanda C. Collins, Daniel M. Mackin, Michael V.\nHeinz, Tess Z. Griffin, Nicholas C. Jacobson, and\nAndrew Campbell. 2025.\nBeyond Prompting:\nTime2Lang – Bridging Time-Series Foundation Mod-\nels and Large Language Models for Health Sensing.\narXiv preprint. ArXiv:2502.07608 [cs].\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan\nLin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, and 25 oth-\ners. 2025.\nQwen2.5 technical report.\nPreprint,\narXiv:2412.15115.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. Preprint, arXiv:2103.00020.\nSohrab Saeb, Mi Zhang, Christopher J Karr, Stephen M\nSchueller, Marya E Corden, Konrad P Kording,\nDavid C Mohr, and 1 others. 2015. Mobile phone\nsensor correlates of depressive symptom severity in\ndaily-life behavior: an exploratory study. Journal of\nmedical Internet research, 17(7):e4273.\nMahsa Sheikh, Meha Qassem, and Panicos A Kyriacou.\n2021. Wearable, environmental, and smartphone-\nbased passive sensing for mental health monitoring.\nFrontiers in digital health, 3:662811.\nDimitris Spathis and Fahim Kawsar. 2024. The first step\nis the hardest: Pitfalls of representing and tokenizing\ntemporal data for large language models. Journal\nof the American Medical Informatics Association,\n31(9):2151–2158.\nRobert L Spitzer, Kurt Kroenke, Janet BW Williams,\nand Bernd Löwe. 2006. A brief measure for assessing\ngeneralized anxiety disorder: the gad-7. Archives of\ninternal medicine, 166(10):1092–1097.\nMingtian Tan, Mike A. Merrill, Vinayak Gupta, Tim\nAlthoff, and Thomas Hartvigsen. 2025. Are language\nmodels actually useful for time series forecasting? In\nProceedings of the 38th International Conference on\nNeural Information Processing Systems, NIPS ’24,\nRed Hook, NY, USA. Curran Associates Inc.\nRui Wang, Fanglin Chen, Zhenyu Chen, Tianxing Li,\nGabriella Harari, Stefanie Tignor, Xia Zhou, Dror\nBen-Zeev, and Andrew T Campbell. 2014.\nStu-\ndentlife: assessing mental health, academic perfor-\nmance and behavioral trends of college students us-\ning smartphones. In Proceedings of the 2014 ACM\ninternational joint conference on pervasive and ubiq-\nuitous computing, pages 3–14.\nZhe Xie, Zeyan Li, Xiao He, Longlong Xu, Xidao Wen,\nTieying Zhang, Jianjun Chen, Rui Shi, and Dan Pei.\n2025. ChatTS: Aligning Time Series with LLMs\nvia Synthetic Data for Enhanced Understanding and\nReasoning. arXiv preprint. ArXiv:2412.03104 [cs].\nXuhai Xu, Xin Liu, Han Zhang, Weichen Wang, Subi-\ngya Nepal, and 1 others. 2023.\nGlobem: Cross-\ndataset generalization of longitudinal human behav-\nior modeling. Proc. ACM Interact. Mob. Wearable\nUbiquitous Technol., 6(4).\nHao Xue and Flora D Salim. 2023. Promptcast: A\nnew prompt-based learning paradigm for time series\nforecasting. IEEE Transactions on Knowledge and\nData Engineering, 36(11):6851–6864.\nKun Yan, Lei Ji, Zeyu Wang, Yuntao Wang, Nan Duan,\nand Shuai Ma. 2023.\nVoila-a: Aligning vision-\nlanguage models with user’s gaze attention. Preprint,\narXiv:2401.09454.\n11\n"}, {"page": 12, "text": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayi-\nheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, and 41 others.\n2025. Qwen3 technical report.\nKailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie,\nJimin Huang, and Sophia Ananiadou. 2024. Mental-\nlama: Interpretable mental health analysis on social\nmedia with large language models. In Proceedings\nof the ACM Web Conference 2024, WWW ’24, page\n4489–4500, New York, NY, USA. Association for\nComputing Machinery.\nHyungjun Yoon, Biniyam Aschalew Tolera, Taesik\nGong, Kimin Lee, and Sung-Ju Lee. 2024. By my\neyes: Grounding multimodal large language models\nwith sensor data via visual prompting. arXiv preprint\narXiv:2407.10385.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020.\nBertscore:\nEvaluating text generation with bert.\nPreprint,\narXiv:1904.09675.\nYunkai Zhang, Yawen Zhang, Ming Zheng, Kezhen\nChen, Chongyang Gao, Ruian Ge, Siyuan Teng,\nAmine Jelloul, Jinmeng Rao, Xiaoyuan Guo, Chiang-\nWei Fang, Zeyu Zheng, and Jie Yang. 2023. Insight\nminer: A large-scale multimodal model for insight\nmining from time series.\nA\nImplementation Details\nTraining Specifications. Training is conducted on\n4×H200 GPUs under a DeepSpeed distributed envi-\nronment with bf16 precision. We use the AdamW\noptimizer with a cosine learning-rate schedule. The\nper-device batch size is set to 4, and gradients are\naccumulated for 32 steps, resulting in an effective\nbatch size of 128 sequences per update. The model\nis fine-tuned with a base learning rate of 1 × 10−5.\nBaseline Inference Configurations.\nFor all\nbaseline models, inference parameters follow the\ndefault configurations provided in the official\nQwen2.5 Hugging Face repositories, with tempera-\nture set to 0.7, top-p to 0.8, and top-k to 20. Few-\nshot exemplars are selected from the training split\nand formatted according to the prompt templates\nin Appendix §G.\nDataset Splits. All experiments use participant-\nlevel splits to prevent information leakage across\nEMA windows from the same individual. The\n258 participants are partitioned into disjoint train-\ning (180; 69.77%), validation (38; 14.73%), and\ntest (40; 15.50%) sets, following an approximate\n70:15:15 ratio. All windows from a given partici-\npant appear in only one split, ensuring evaluation\non unseen individuals.\nB\nEMA Questions\nThe Ecological Momentary Assessment (EMA)\nused in this study consists of 13 primary items\ndesigned to monitor intra-day variations in men-\ntal health (Table 2).\nThese items were specifi-\ncally adapted from the Patient Health Question-\nnaire (PHQ) (Kroenke et al., 2001) and the Gen-\neralized Anxiety Disorder (GAD) (Spitzer et al.,\n2006) scale to assess symptoms over the preced-\ning four-hour window. As shown in Table 2, the\nquestionnaire covers 14 distinct categories, includ-\ning core depression indicators like anhedonia and\ndepressed mood, alongside physical and cognitive\nmarkers such as somatic discomfort, fatigue, and\nconcentration. It also includes a negative event\nquestion to understand context about mental health\nsymptoms.\nC\nSensor Data Preprocessing\nThe sensor streams are preprocessed as follows.\nFor steps and heart rate, we applied empiri-\ncal thresholds to remove outliers.\nStress and\naccelerometer-derived features were already pro-\ncessed by the data collection software and required\nno additional filtering. GPS data was reduced to\nlatitude and longitude pairs. Phone lock and unlock\nstate was encoded as a binary sequence, where 0 in-\ndicates locked and 1 indicates unlocked, and down-\nsampled from a 1-second to a 60-second resolution\nto avoid excessively long sequences. Conversation\nevents were logged only when speech was detected,\nand for each EMA-aligned window we summed\nevent durations to obtain total conversational time\nin seconds. Sleep duration was recorded once per\nday as the previous night’s total hours slept.\nAfter preprocessing the sensor streams, we stan-\ndardized all signals to fixed sampling rates, includ-\ning heart rate every 10 seconds, GPS every 10 min-\nutes, steps every 60 seconds, stress every 60 sec-\nonds, ZCR every 30 seconds, and phone lock and\nunlock every 60 seconds. Sleep duration and con-\nversation length were stored as scalar values for\neach window. This unified representation ensured\nconsistent temporal alignment across all sensing\nmodalities.\nD\nMetric Definitions\nD.1\nCoverage\nPresence detection is computed over all symptom\ncategories. Let aj, ˆaj ∈{0, 1} denote reference\n12\n"}, {"page": 13, "text": "Question Categories\nQuestions\n1. Anhedonia (Interest/Pleasure)\nIn the past 4 hours, how much has the user shown little interest or pleasure in activities?\n2. Depressed Mood\nIn the past 4 hours, how much has the user appeared down, depressed, or hopeless?\n3. Sleep Disturbance\nLast night, how much trouble did the user have with sleep?\n4. Fatigue / Energy\nIn the past 4 hours, how tired or low in energy has the user been?\n5. Appetite Change\nIn the past 4 hours, how much has the user shown a poor appetite or overeating?\n6. Self-worth / Guilt\nIn the past 4 hours, how much has the user felt bad about themselves?\n7. Concentration\nIn the past 4 hours, how much trouble has the user had concentrating?\n8. Psychomotor Change\nIn the past 4 hours, how much has the user been moving or speaking more slowly than usual?\n9. Suicidal Ideation\nIn the past 4 hours, how often has the user had thoughts of harming themselves or wishing\nto be dead?\n10. Somatic Discomfort\nIn the past 4 hours, how much has the user experienced headache, abdominal discomfort, or\nbody aches?\n11. Inverted Question\nAn inverted question randomized from Q1, Q4, or Q7.\n12. Anxiety Arousal\nIn the past 4 hours, how much has the user felt nervous, anxious, or on edge?\n13. Uncontrollable Worry\nIn the past 4 hours, how much has the user been unable to stop or control worrying?\n14. Negative Event\nIn the past 4 hours, did the user experience a negative event? If yes: How negative was the\nevent?\nOverall Summary\nPlease summarize the user’s overall mental and physical state in the past 4 hours, integrating\nmood, energy, sleep, appetite, concentration, and physical symptoms.\nTable 2: EMA Questions. Categories of depression- and anxiety-related symptoms and their corresponding\nquestions or statements.\nand predicted presence for category j. Then:\nPcov =\nTP\nTP + FP ,\nRcov =\nTP\nTP + FN ,\nF1cov = 2PcovRcov\nPcov + Rcov\n,\nwhere TP, FP, and FN represent correct men-\ntions, hallucinations, and omissions.\nD.2\nPresence-aware Severity Alignment\nSeverity is scored only when either reference or\nprediction marks a symptom as present:\nJ = {j | aj = 1 or ˆaj = 1}.\nFor each j ∈J , a weight wj is assigned based on\nordinal deviation:\nwj =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0,\naj ̸= ˆaj,\n1,\n|sj −ˆsj| = 0,\n0.75,\n|sj −ˆsj| = 1,\n0.25,\n|sj −ˆsj| = 2,\n0,\n|sj −ˆsj| ≥3,\nand the final alignment score is\nSalign =\n1\n|J |\nX\nj∈J\nwj.\nThis captures both hallucination penalties (aj ̸=\nˆaj) and ordinal severity mismatch.\nE\nAdditional Results\nImpact of Time Series Modality. To disentangle\nthe effect of supervised fine-tuning from the contri-\nbution of native time-series modeling, we conduct\nan ablation using a text-only baseline (TS-Text-\nFT). In this setting, we remove the time-series en-\ncoder from LENS and fine-tune the same Qwen2.5-\n14B backbone on the identical sensor–text training\npairs. Instead of being encoded by a patch-based\ntime-series encoder, sensor streams are directly se-\nrialized as textual inputs. To control for sequence\ntruncation effects, the text-only model is trained\nwith a cutoff length of 20,000 tokens, covering\nmore than 99.95% of training samples.\nAs reported in Table 4, LENS consistently\noutperforms TS-Text-FT across all evaluation di-\nmensions. For summary-level generation, LENS\nachieves higher clinical alignment (0.601 vs. 0.583)\nand stronger linguistic quality (METEOR score of\n0.467 vs. 0.408). The gap widens substantially in\nthe item-level QA setting, where TS-Text-FT ex-\nhibits difficulty reasoning over long, unstructured\nnumerical sequences, resulting in markedly lower\nROUGE-L scores (0.172 vs. 0.603). These results\nindicate that supervised fine-tuning alone is insuf-\nficient to capture the temporal structure of sensor\ndata, and that explicit time-series encoding plays a\ncentral role in LENS’s performance.\nComputational Efficiency and Token Consump-\ntion.\nIn addition to accuracy, we analyze the\ncomputational efficiency of different modeling\n13\n"}, {"page": 14, "text": "Model Specification\nLinguistic Metrics\nLLM-as-a-Judge Metrics\nMethod\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU-4\nMETEOR\nBERTScore\nCoverage\nAlignment\nSummary-level Evaluation\nLENS\nQwen2.5-14B-Instruct\n0.593\n0.265\n0.410\n0.220\n0.467\n0.776\n0.753\n0.601\nTS-Text\nQwen2.5-14B-Instruct\n0.294\n0.057\n0.151\n0.032\n0.218\n0.631\n0.614\n0.372\nTS-Image\nQwen2.5-VL-32B\n0.556\n0.212\n0.373\n0.166\n0.452\n0.765\n0.740\n0.579\nItem-level Evaluation\nLENS\nQwen2.5-14B-Instruct\n0.624\n0.460\n0.603\n0.383\n0.611\n0.832\n-\n0.732\nTS-Text\nQwen2.5-14B-Instruct\n0.176\n0.047\n0.142\n0.017\n0.218\n0.604\n-\n0.665\nTS-Image\nQwen2.5-VL-32B\n0.162\n0.044\n0.136\n0.016\n0.211\n0.602\n-\n0.687\nTable 3: Evaluation on summary-level and item-level QA datasets across all linguistic and clinical alignment\nmetrics. LENS demonstrates consistent superiority in both narrative generation and specific symptom retrieval.\nModel Specification\nLinguistic Metrics\nLLM-as-a-Judge Metrics\nMethod\nModality\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU-4\nMETEOR\nBERTScore\nCoverage\nAlignment\nSummary-level Evaluation\nLENS\nMultimodal\n0.593\n0.265\n0.410\n0.220\n0.467\n0.776\n0.753\n0.601\nTS-Text-FT\nText-Only\n0.578\n0.275\n0.390\n0.207\n0.408\n0.772\n0.789\n0.583\nItem-level Evaluation\nLENS\nMultimodal\n0.624\n0.460\n0.603\n0.383\n0.611\n0.832\n-\n0.732\nTS-Text-FT\nText-Only\n0.192\n0.078\n0.172\n0.032\n0.281\n0.598\n-\n-\nTable 4: Ablation Study: with or without time series encoder. Comparison between LENS and TS-Text-FT\n(fine-tuned on text-serialized time series).\nparadigms by comparing their token consumption.\nSpecifically, we measure the mean prefill token\ncount per sample and the total token consumption\nover the Narrative (n = 8,192) and Item-level QA\n(n = 16,384) datasets for three approaches: text-\nonly (Qwen2.5), vision-based (Qwen2.5-VL-32B),\nand LENS.\nFor LENS, effective prefill tokens are computed\nas the sum of textual tokens and patch-based em-\nbeddings (patch size k = 8) that replace raw time-\nseries placeholders. For the vision-language base-\nline, token counts follow the model’s native pro-\ncessor, which accounts for both textual inputs and\nimage-derived tokens. For the text-only baseline,\nthe tokenizer needs to handle all the sensor streams\nas text, resulting in substantially longer input se-\nquences.\nFigure 8 summarizes the results. Across both\ntasks, LENS exhibits the lowest token consumption\nby a large margin. Compared to the text-only base-\nline, which averages over 15,800 tokens per sample\ndue to verbose serialization of high-frequency sig-\nnals, LENS requires approximately 930 tokens per\nsample, corresponding to a reduction of about 94%.\nRelative to the vision-language model, which repre-\nsents signals as plots, LENS remains roughly four\ntimes more token-efficient (933 vs. 3,679 tokens).\nThis reduction in context length provides a prac-\ntical explanation for the degraded QA performance\nobserved in text-only fine-tuning, where long se-\nrialized inputs are prone to attention dilution in\nextended contexts. By encoding raw signals into\ncompact patch-level representations, LENS sub-\nstantially reduces sequence length while preserving\ntask-relevant temporal information, enabling more\nefficient and scalable inference for long-duration\nhealth monitoring scenarios.\nImpact of Data Scale. To investigate the influence\nof training data volume, we trained LENS using\nvarying proportions (10%, 50%, and 100%) of the\ngenerated dataset, keeping the base model archi-\ntecture consistent. As shown in Table 5, the im-\npact of data scale varies across different evaluation\ndimensions. For summary-level narrative genera-\ntion, Presence Alignment improves strictly mono-\ntonically with data size, rising from 0.545 (10%\ndata) to 0.601 (100% data), indicating that larger\ndatasets are essential for minimizing hallucinations\nin complex clinical summaries. However, Symp-\ntom Coverage is not strictly monotonic; the model\n14\n"}, {"page": 15, "text": "Summary-level\nItem-level\n0\n2500\n5000\n7500\n10000\n12500\n15000\nMean Prefill Tokens per Prompt\n(a) Token Consumption per Prompt\nSummary-level\nItem-level\n0\n50\n100\n150\n200\n250\n300\nTotal Tokens (Millions)\n129.8M\n259.5M\n30.1M\n58.8M\n7.6M\n15.1M\n(b) Total Dataset Consumption (M)\nTS-Text\nTS-Image\nLENS\nFigure 8:\nComputational Efficiency Analysis.\nComparison of token consumption across three\nmodalities: Text (Qwen-2.5), Vision-Language (TS-\nImage/Qwen2.5-VL), and Time-Series (LENS). (a)\nMean prefill tokens per prompt. (b) Total dataset to-\nken consumption (in Millions) for Narrative and QA\ndatasets.\ntrained on 50% data achieved the highest cover-\nage (0.823), slightly outperforming the full model\n(0.801) and the 10% model (0.783). In the Item-\nlevel QA task, the full 100% model achieves the\nbest overall performance in both linguistic metrics\n(ROUGE-L 0.603) and clinical alignment (0.732).\nNotably, the model trained on only 10% data ex-\nhibits surprisingly strong performance in QA tasks,\nachieving a higher alignment score (0.729) than the\n50% model (0.706). This suggests that while basic\ninstruction-following for short-form QA can be es-\ntablished with smaller data scales, the full dataset\nis necessary to maximize structural coherence and\nclinical fidelity in longer narratives.\nSmaller LENS Variants Remain Competitive.\nTo address computational feasibility for deploy-\nment in resource-constrained environments, we\nevaluate LENS-7B—a lighter variant of our frame-\nwork—alongside models trained on reduced data\nproportions. As shown in our comparison, LENS-\n7B achieves slightly lower performance on com-\nplex clinical alignment than LENS-14B (0.559\nvs. 0.601), reflecting the trade-off between model\nsize and reasoning depth. Nevertheless, LENS-\n7B matches the 14B model’s mean performance\non linguistic metrics (e.g., ROUGE-L 0.409 vs.\n0.410) and closely trails it in Item-level QA tasks\n(Presence Alignment 0.727 vs. 0.732). Similarly,\nmodels trained on limited data demonstrate surpris-\ning robustness in short-form generation. These re-\nsults suggest that LENS-7B or low-data fine-tuning\nprovides a strong balance between efficiency and\nperformance, making it a cost-effective choice for\nreal-world, resource-limited scenarios such as edge-\ndevice deployment.\nF\nUser Study\nSurvey Design. The survey was designed with\ninput from both a clinical psychologist and a ther-\napist. Each rating example includes a narrative\npresented alongside a ground-truth symptom table,\nfollowed by questions assessing accuracy and com-\nprehensiveness. Note that the user study excludes\nthe inverted question (Q11 in Table 2), which is\na semantic reversal of an existing item and Sleep\nDisturbance question which is administered only in\nmorning EMAs to ensure non-redundant and tem-\nporally consistent evaluation. Because our expert\nreviewers noted that mentally tracking symptoms\nacross the narrative was difficult, we incorporated\ntwo design changes: (1) presenting the narrative\nand symptom table simultaneously on the screen,\nand (2) adding Yes/No checkboxes to help raters\ntrack individual symptoms when answering these\nquestions. For the clinical utility and language\ncohesion items, we display the narrative again to\nremind raters of the content. After receiving posi-\ntive design feedback from our expert reviewers, we\nadministered it to the 13 mental health experts.\nPre-survey Questions. Before beginning the rat-\ning process, raters were asked to answer four back-\nground questions:\n1. What is your professional role or background?\n(Options: Psychiatrist, Clinical Psychologist,\nTherapist, Other)\n2. What is your highest degree held? (Options:\nHigh school/Diploma, Bachelor’s, Master’s,\nDoctorate/MD)\n3. How familiar are you with the Patient\nHealth Questionnaire-9 (PHQ-9) for depres-\nsion screening?\n4. How familiar are you with the Generalized\nAnxiety Disorder Questionnaire (GAD-7) for\nanxiety screening?\nFor Questions 3 and 4, the response options were:\nNot familiar at all, Slightly familiar, Moderately\nfamiliar, and Very familiar. The experts’ pre-survey\nresponses are summarized in Table 7.\nSurvey Flow. After completing the pre-survey\nquestions, each expert rater is introduced to the\ntask through a detailed explanation of the overall\ngoal, problem setting, and rating procedure. They\nare then shown an annotated example that includes\nsample answers and reasoning to illustrate how the\n15\n"}, {"page": 16, "text": "Model Specification\nLinguistic Metrics\nLLM-as-a-Judge Metrics\nMethod\nData Size\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU-4\nMETEOR\nBERTScore\nCoverage\nAlignment\nSummary-level Evaluation\nLENS\n100% Data\n0.593\n0.265\n0.410\n0.220\n0.467\n0.776\n0.801\n0.601\nLENS\n50% Data\n0.586\n0.255\n0.399\n0.211\n0.463\n0.773\n0.823\n0.585\nLENS\n10% Data\n0.583\n0.260\n0.400\n0.213\n0.475\n0.772\n0.783\n0.545\nItem-level Evaluation\nLENS\n100% Data\n0.624\n0.460\n0.603\n0.383\n0.611\n0.832\n-\n0.732\nLENS\n50% Data\n0.611\n0.446\n0.590\n0.369\n0.603\n0.823\n-\n0.706\nLENS\n10% Data\n0.612\n0.451\n0.594\n0.371\n0.602\n0.827\n-\n0.729\nTable 5: Data Efficiency Analysis. Performance comparison of LENS trained on varying proportions of the dataset\n(100% vs. 50% vs. 10%). Results are reported for both Summary-level (narrative generation) and Item-level (QA)\ntasks.\nModel Specification\nLinguistic Metrics\nLLM-as-a-Judge Metrics\nMethod\nModel Size\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU-4\nMETEOR\nBERTScore\nCoverage\nAlignment\nSummary-level Evaluation\nLENS\n14B\n0.593\n0.265\n0.410\n0.220\n0.467\n0.776\n0.753\n0.601\nLENS\n7B\n0.592\n0.265\n0.409\n0.220\n0.468\n0.775\n0.826\n0.559\nItem-level Evaluation\nLENS\n14B\n0.624\n0.460\n0.603\n0.383\n0.611\n0.832\n-\n0.732\nLENS\n7B\n0.622\n0.457\n0.600\n0.380\n0.608\n0.831\n-\n0.727\nTable 6: Model Size Analysis. Performance comparison between LENS-14B and LENS-7B across summary-level\nand item-level tasks.\n#\nBackground\nEducation\nPHQ Familiarity\nGAD Familiarity\n1\nTherapist\nMaster’s\nVery Familiar\nVery Familiar\n2\nPsychologist\nDoctorate\nVery Familiar\nVery Familiar\n3\nTherapist\nBachelor’s\nSlightly Familiar\nModerately Familiar\n4\nTherapist\nMaster’s\nVery Familiar\nVery Familiar\n5\nHealth Coach\nDoctorate\nSlightly Familiar\nModerately Familiar\n6\nPsychologist\nBachelor’s\nModerately Familiar\nModerately Familiar\n7\nResearcher\nDoctorate\nVery Familiar\nModerately Familiar\n8\nPsychologist\nBachelor’s\nModerately Familiar\nModerately Familiar\n9\nPsychologist\nMaster’s\nVery Familiar\nVery Familiar\n10\nPsychologist\nMaster’s\nSlightly Familiar\nSlightly Familiar\n11\nTherapist\nMaster’s\nVery Familiar\nVery Familiar\n12\nTherapist\nMaster’s\nModerately Familiar\nVery Familiar\n13\nPsychologist\nBachelor’s\nModerately Familiar\nModerately Familiar\nTable 7: Background information of mental health\nexperts.\nevaluation should be performed. In every survey,\nthe first example presented is a dummy item in-\ntended solely to help raters become familiar with\nthe interface and question format. This is followed\nby the nine narratives that constitute the actual rat-\ning set.\n16\n"}, {"page": 17, "text": "G\nPrompt Templates and Selected\nExamples\nPrompt Templates.\nTables 8-15 summarize the\nprompt templates used throughout our experiments\nfor narrative generation, question answering, and\nevaluation.\nTables 8 and 9 define the prompts\nfor rewriting rule-based EMA-derived symptom\ndescriptions into fluent narrative labels and for\nLLM-based quality assessment of generated nar-\nratives. Tables 10 and 11 specify structured eval-\nuation prompts for extracting symptom presence\nand severity, as well as for assessing ordinal sever-\nity alignment in single-item QA. Tables 12 and 13\npresent the prompts used for vision–language base-\nlines, where multivariate sensor streams are pro-\nvided as multi-panel time-series visualizations to-\ngether with contextual features. Finally, Tables 14\nand 15 describe the text-based baseline prompts,\nin which the same sensor data are serialized as nu-\nmerical sequences using placeholder tokens. We\nadopt few-shot prompting for non-fine-tuned base-\nlines to mitigate systematic mismatches in output\nformat observed under zero-shot settings; as shown\nin Table 16, zero-shot baselines often fail to follow\nthe required PHQ-9–style narrative structure and\ninstead produce generic, data-centric descriptions.\nBy applying few-shot exemplars to the baseline\nmodels, we raise their performance floor and iso-\nlate differences arising from input representation\nand architectural design rather than prompt or for-\nmat misalignment.\nSelected Qualitative Examples.\nUsing the\nprompt templates described above, we present se-\nlected qualitative examples illustrating narrative\nQA and single-question QA across different model\nvariants (See Figure9. Each example shows the\nprompt context, the form of sensor input provided\nto the model, and the resulting output. For VLM-\nbased baselines, time-series inputs are rendered as\nvisual plots, whereas LENS directly processes the\nsame raw sensor streams via a patch-based time-\nseries encoder. Text-based baselines receive identi-\ncal information serialized as numerical arrays.\nSystem Prompt\nYou are both a mental health and language specialist ex-\nperienced with clinical records concerning mental health\nconditions. Rewrite rule-based psychological assessment\ntemplates into fluent, engaging narrative passages, strictly\npreserving every factual detail and original severity de-\nscription. These improved narratives serve as ground-truth\nlabels for training AI models to predict mental health states\nusing physiological sensor data (such as smartwatch read-\nings).\nParaphrasing Guidelines\n1. Factual Accuracy and Preservation. Retain every origi-\nnal severity level exactly as presented. Do not add any in-\nterpretations, clinical reasoning, or extra context. Preserve\nall frequency and intensity information without omission\nor alteration.\n2. Natural, Readable Language. Remove mechanical or\nrepetitive phrasing. Employ varied sentence structures and\nnatural transitions between symptoms. Ensure that the\nnarrative reads as a natural human description.\n3. Consistency in Terminology and Tone. Use identical\nlanguage for identical severity levels across all narratives.\nMaintain a uniform style and tone throughout all para-\nphrased outputs.\n4. Accessibility and Clarity. Write in straightforward,\naccessible language suitable for general audiences. Avoid\ntechnical or clinical terms whenever possible. Use person-\nfirst, stigma-free wording. Keep sentences clear, concise,\nand complete.\nUser Prompt Template\nYour task: Transform the below rule-based assessment\ninto a well-structured, fluent narrative that fully preserves\nall factual content and improves readability.\nOriginal Assessment: {rule_based_template}\nEnhanced Narrative:\nTable 8: Prompt template for narrative rewriting.\nThis prompt is used to rewrite rule-based EMA-derived\nsymptom descriptions into fluent narrative labels.\n17\n"}, {"page": 18, "text": "Prompt:\nFor this study, seven streams are available: Heart rate (beats per minute, bpm, 1440 samples)\n<ts></ts>; Pseudoactigraphy (Garmin watch accelerometer, product of ZCR count and energy,\n480 samples) <ts></ts>; Steps per minute (no desc, 240 samples) <ts></ts>; Stress level\n(Garmin watch estimate related to HRV, 240 samples) <ts></ts>; GPS longitude (coordinates,\n24 samples) <ts></ts>; GPS latitude (coordinates, 24 samples) <ts></ts>; Phone unlock status\n(binary events per minute, 240 samples) <ts></ts>.sleep duration is 8.150 hours conversation\nlength is 2232 seconds\nQuestions: Provide an integrated overview of the user’s status in the past 4 hours, linking\nemotional, cognitive, and physical responses.\nSensor Time Series Data:\n[86.0,85.0,86.0,84.0,85.0,81.0,84.0,85.0,84.0,85.0,90.0,83.0,84.0,80.0,84.0,88.0,85.0,87.0,86.\n0,91.0,89.0,86.0,85.0,86.0,85.0,86.0,88.0,90.0,87.0,88.0,90.0,95.0,86.0,87.0,85.0,92.0,86.0,88\n.0,89.0,88.0,94.0,86.0,87.0,87.0,86.0,90.0,88.0,84.0,89.0,89.0,86.0,89.0,87.0,89.0,87.0,88.0,8\n8.0,88.0,87.0,89.0,87.0,90.0,91.0,91.0,92.0,89.0,87.0,87.0,90.0,93.0,87.0,87.0,93.0,88.0,90.0,\n90.0,84.0,86.0,87.0,89.0,84.0,92.0,86.0,92.0,91.0,89.0,88.0,84.0,87.0,87.0,91.0,90.0,89.0,90.\n... [truncated]\nThe user has been experiencing a persistent\nlack of interest or enjoyment in activities they\nonce \nfound \nmeaningful. \nThey've \nfelt\nconsistently down, hopeless, or depressed, and\noften describe feeling drained of energy or\nconstantly tired. Despite this, their appetite\nhasn't \nbeen \nsignificantly \naffected‚Äîthey\nhaven't experienced a loss of appetite or\novereating. However, they've been struggling\nwith self-criticism and a persistent sense of\nworthlessness. Concentration has also been a\nchallenge, making it difficult to focus on tasks.\nThey haven't shown signs of slowed movement\nor speech, nor have they been unusually fidgety.\nThere have been no thoughts of self-harm or\nsuicide. They've also been dealing with physical\ndiscomfort, \nincluding \nheadaches, \nstomach\naches, and body aches. Their sleep was only\nminimally disrupted the previous night. While\nthey've not experienced a complete loss of\ninterest in activities, they've still felt a\nconsistent sense of nervousness, anxiety, or\nbeing on edge. They haven't struggled with\nuncontrollable \nworrying. \nOverall, \ntheir\nsymptoms \nalign \nwith \nmoderately \nsevere\ndepression.\nOver the past four hours, the individual has\nexhibited a generally low level of physical\nactivity, with minimal steps taken and periods\nof prolonged inactivity interspersed with brief\nbursts of movement. The heart rate shows some\nvariability, \nparticularly \nduring \nperiods \nof\nincreased movement, but remains within a\nnormal range. The stress levels fluctuated, with\noccasional \nspikes \nindicating \nmoments \nof\nheightened tension or anxiety. There were no\nsignificant changes in sleep patterns, but the\nrecent sleep duration of 8.15 hours suggests\nadequate rest. The phone was unlocked\nfrequently, which may indicate periods of\nengagement with digital devices, possibly to\nmanage \nstress \nor \nseek \ndistraction. \nThe\nindividual's GPS location indicates minimal\ntravel, suggesting they have remained in a\nconsistent location, likely their home or a\nfamiliar setting. Overall, the individual appears\nto be experiencing mild symptoms of anxiety\nand fatigue, with no signs of severe depression\nor hopelessness. The recent exposure to\nnegative events is not directly evident, but the\nfluctuating stress levels and low activity levels\nsuggest a state of mild anxiety.\nThe user has been experiencing a range of\nsymptoms that suggest a moderate level of\ndepression. They often find themselves with\nlittle interest or enjoyment in activities they\nused to enjoy, and at times feel persistently\ndown, sad, or without hope. A constant sense of\nfatigue or low energy has also been reported,\nmaking daily tasks feel more difficult. Their\nappetite has been affected too‚Äîsometimes\nthey eat very little, while other times they\novereat. They frequently struggle with self-\ncriticism and feel bad about themselves.\nConcentration has become challenging, and\nthey often find it hard to focus on what they're\ndoing. Their movements and speech have\nslowed, or they may feel restless and fidgety.\nFortunately, they have not had any thoughts of\nself-harm or feelings that life isn‚Äôt worth\nliving. \nThey \nalso \ncommonly \nexperience\nphysical \ndiscomfort \nsuch \nas \nheadaches,\nstomachaches, or body aches. Sleep was only\nminimally \naffected \nlast \nnight, \nand \nthey\noccasionally report having a lot of energy. They\nalso sometimes feel nervous, anxious, or on\nedge, and have trouble controlling their\nworries. Overall, these symptoms align with a\nmoderate level of depression.\nThe user has been experiencing a persistent\nlack of interest or enjoyment in activities they\nonce found pleasurable. They often feel down,\ndepressed, or hopeless, and frequently report\nfeelings of fatigue or low energy. Their appetite\nhas also been affected, with occasional loss of\nappetite or overeating. They struggle with self-\ncriticism and often feel bad about themselves.\nConcentration has been difficult at times, and\nthey sometimes move or speak more slowly\nthan usual, or feel restless and fidgety. They\nhave not had any thoughts of self-harm or\nfeelings that life would be better if they were\nnot here. They also experience physical\ndiscomfort, such as headaches, stomach aches,\nor body aches. Overall, their symptoms align\nwith a moderate level of depression.\nLENS \nTS-Text\nTS-Image\nTS-Text-FT\nPrompt:\nThe monitoring system provides seven continuous data streams. Heart rate, measured as\nbeats per minute, bpm, has a length of 1440 and is represented as <ts><ts/>.\nPseudoactigraphy, measured as Garmin watch accelerometer, product of ZCR count and\nenergy, has a length of 480 and is represented as <ts><ts/>. Steps per minute has a length of\n240 and is represented as <ts><ts/>. Stress level, measured as Garmin watch estimate related\nto HRV, has a length of 240 and is represented as <ts><ts/>. GPS longitude, measured as\ncoordinates, has a length of 24 and is represented as <ts><ts/>. GPS latitude, measured as\ncoordinates, has a length of 24 and is represented as <ts><ts/>. Phone unlock status,\nmeasured as binary events per minute, has a length of 240 and is represented as <ts>\n<ts/>.sleep duration is 7.770 hours\\nconversation length is 6665 seconds\nQuestions:Over the last 4 hours, how noticeable was the user’s anxiety?\nSensor Time Series Data:\n[81.0, 84.0, 88.0, 91.0, 89.0, 89.0, 93.0, 92.0, 92.0, 95.0, 94.0, 94.0, 92.0, 93.0, 89.0, 89.0,\n85.0, 85.0, 84.0, 87.0, 85.0, 82.0, 82.0, 80.0, 81.0, 86.0, 81.0, 81.0, 83.0, 88.0, 89.0, 90.0,\n86.0, 86.0, 85.0, 84.0, 83.0, 83.0, 80.0, 87.0, 88.0, 86.0, 84.0, 81.0, 82.0, 82.0, 83.0, 81.0,\n78.0, 78.0, 77.0, 77.0, 78.0, 78.0, 78.0, 81.0, 83.0, 87.0, 89.0, 85.0, 74.0, 67.0, 64.0, 68.0,\n68.0, 68.0, 72.0, 74.0, 76.0, 78.0, 86.0, 85.0, 87.0, 90.0, 94.0, 98.0, 103.0, 93.0, ... [truncated]\nThe user has appeared on edge occasionally.\nThis is inferred from the fluctuating heart rate,\nstress level, and pseudoactigraphy patterns,\nwhich suggest periods of increased arousal and\nactivity. The relatively low sleep duration and\nextended conversation length also indicate\npotential stress or engagement in demanding\nactivities.\nBased on the provided time-series data, the user\nhas not shown significant variation in their GPS\nlocation, indicating they have remained in a\nrelatively stationary position over the past 4\nhours. Therefore, it does not appear that the\nuser has been on the edge or moving to\ndifferent locations frequently.\nThe user occasionally experienced feelings of\nnervousness or anxiety, often feeling slightly on\nedge from time to time\nThe user has not experienced any feelings of\nbeing on edge or overly nervous at all.\nLENS \nTS-Text\nTS-Image\nTS-Text-FT\nFigure 9: Qualitative examples for narrative QA (top) and single-question QA (bottom). Each example shows\nthe full prompt context, the form of sensor input provided to the model, and the generated output. LENS directly\nconsumes raw multivariate time-series via a patch-based time-series encoder. VLM-based baselines receive the\nsame signals rendered as multi-panel plots, while text-based baselines process serialized numerical sequences. All\nmodels are prompted with identical task instructions, enabling a controlled comparison of qualitative behavior\nacross input representations.\n18\n"}, {"page": 19, "text": "System Prompt\nAs a highly meticulous and objective clinical quality reviewer, your primary responsibility is to evaluate the quality and\nsafety of an AI-generated mental health narrative. You must ground your judgment strictly in the provided source data.\nYou will score the AI-generated Narrative on five specific dimensions using a 1–5 Likert scale and then provide a concise,\nstructured critique.\nTemplate-Based Narrative (Baseline for Comparison/ground): This is the rule-based description generated directly\nfrom the PHQ-9 scores using a template. Use this as a baseline for factual alignment and coverage assessment.\nAI-generated Narrative (To Be Evaluated): This is the AI-rewritten version of the narrative that you must score and\ncritique. You must respond with only a valid JSON object, with no additional text before or after.\nUser Prompt Template\nTemplate-Based Narrative (Baseline for Comparison/ground):\n{original_template}\nAI-generated Narrative (To Be Evaluated):\n{enriched_narrative}\nPlease evaluate the AI-generated Narrative based on the following five dimensions. For each dimension, provide a score\nfrom 1 (Very Poor) to 5 (Excellent).\nFactual Alignment:\nDoes the narrative accurately reflect the presence or absence of symptoms reported in the Template-Based Narrative?\nDoes it contradict any facts from the source data?\nScoring guide: 1 indicates significant factual contradictions. 3 indicates general alignment with minor inaccuracies. 5\nindicates perfect alignment with no factual errors.\nSymptom Coverage:\nDoes the narrative mention or allude to all relevant symptoms that were reported with a non-zero score?\nScoring guide: 1 indicates multiple significant symptoms are missed. 3 indicates most severe symptoms are covered with\nsome omissions. 5 indicates comprehensive coverage of all reported symptoms.\nSeverity Fidelity:\nDoes the language and tone accurately reflect the severity levels from the Template-Based Narrative (for example, not at\nall, several days, more than half the days, nearly every day)?\nScoring guide: 1 indicates gross misrepresentation of severity. 3 indicates approximate severity with limited precision. 5\nindicates precise and appropriate severity representation.\nFluency and Naturalness:\nIs the narrative coherent, well-written, and natural-sounding? Does it avoid robotic or repetitive phrasing without\nsounding artificial?\nScoring guide: 1 indicates awkward or highly artificial text. 3 indicates generally fluent but slightly unnatural phrasing. 5\nindicates natural, engaging, and human-like language.\nHallucination Risk:\nDoes the narrative introduce any new symptoms, details, or assumptions not supported by the Template-Based Narrative?\nScoring guide: 1 indicates significant and potentially harmful fabrications. 3 indicates minor unsupported but clinically\nneutral additions. 5 indicates strict adherence to the provided source data.\nConfidence Scoring Guide:\nFor each dimension, provide a confidence score from 0.0 to 1.0 indicating certainty of the evaluation. 1.0 indicates\ncomplete certainty. 0.8–0.9 indicates high confidence. 0.6–0.7 indicates moderate confidence. 0.4–0.5 indicates low\nconfidence. 0.1–0.3 indicates very low confidence. 0.0 indicates no confidence.\nReturn your evaluation result in the following JSON format:\n{\"scores\": [...], \"confidence\": [...], \"critique\": {...}}\nTable 9: Prompt template for LLM-based narrative quality evaluation. This prompt implements an LLM-as-a-\nJudge framework to assess the quality and safety of rewritten narratives. Judge models compare template-based and\nenhanced narratives and assign scores across five dimensions, along with confidence estimates and a brief rationale.\n19\n"}, {"page": 20, "text": "System Prompt\nYou are a clinical evaluation model. Your task is to extract\nsymptom information from two texts: a ground-truth refer-\nence summary and a model-generated prediction summary.\nDo not interpret or rewrite either text. Do not generate\nexplanations or narrative. You must only evaluate whether\nsymptoms are present and how severe they are.\nYou must evaluate the following 14 symptom categories:\n1. Anhedonia (loss of interest or pleasure)\n2. DepressedMood\n3. SleepDisturbance\n4. FatigueEnergy\n5. AppetiteChange\n6. SelfWorthGuilt\n7. Concentration\n8. PsychomotorChange\n9. SuicidalIdeation\n10. SomaticDiscomfort\n11. AnxietyArousal\n12. UncontrollableWorry\n13. NegativeEvent\n14. OverallSeverity\nSeverity scale is ordinal and must be inferred from the\noverall semantic strength of the description. If a symptom\nis not present in a text, you must set both presence and\nseverity to 0.\nUser Prompt Template\nReference Summary:\n{reference}\nPrediction Summary:\n{prediction}\nStructured Response Schema\nSymptomEvaluation object with 14 symptom fields. Each\nfield contains:\n• ref_presence: {0, 1} — whether symptom is present\nin reference\n• pred_presence:\n{0, 1} — whether symptom is\npresent in prediction\n• ref_severity: {0, 1, 2, 3} — severity level in refer-\nence\n• pred_severity: {0, 1, 2, 3} — severity level in pre-\ndiction\nSeverity Scale: 0 = Not mentioned/None, 1 = Mild, 2 =\nModerate, 3 = Severe\nTable 10: Prompt template for structured symptom\nevaluation on narratives. This prompt uses a struc-\ntured output schema to extract symptom presence and\nordinal severity from both reference and prediction nar-\nratives.\nSystem Prompt\nYou are a clinical evaluation model. Your task is to assess\nthe severity of a symptom or behavior described in two\ntexts: a ground-truth reference and a model-generated\nprediction.\nFor each text, output a severity score from 0 to 3:\n• 0: No symptom / absent / not at all\n• 1: Mild / occasionally / somewhat\n• 2: Moderate / often / frequently\n• 3: Severe / almost always / very frequently\nBase your judgment on the semantic intensity and fre-\nquency descriptors in each text. Do not add explanations\nor any additional fields.\nUser Prompt Template\nQuestion: {question}\nReference: {reference}\nPrediction: {prediction}\nStructured Response Schema\nSeverityPair object with two fields:\n• ref_severity: {0, 1, 2, 3} — severity score for ref-\nerence text\n• pred_severity: {0, 1, 2, 3} — severity score for pre-\ndiction text\nTable 11: Prompt template for QA severity evalua-\ntion. This prompt uses a structured output schema to\nextract ordinal severity scores from both reference and\nprediction answers for single-item QA pairs.\n20\n"}, {"page": 21, "text": "VLM Narrative Prompt (Image Input)\nYou are a clinical psychologist interpreting behavioral and\nphysiological data visualized in the image below. Each\nchart represents one data stream collected continuously\nduring the past four hours:\n1. Heart rate (bpm): indicator of arousal, stress, and\nautonomic balance.\n2. Pseudoactigraphy: derived from wrist accelerom-\neter signals, representing movement intensity and\nrest–activity rhythm.\n3. Steps per minute: reflects overall mobility and en-\ngagement in physical activity.\n4. Stress level: Garmin HRV-based estimation of phys-\niological stress.\n5. GPS coordinates (longitude and latitude): capture\nspatial mobility and time spent in different environ-\nments.\n6. Phone unlock status: number of unlock events per\nminute, representing cognitive or social engagement.\nAdditional contextual features:\n{contextual_features}\nYour task:\nAnalyze the figure carefully and provide a clinical sum-\nmary of the user’s recent psychological and behavioral\nstate, as if you were writing a short report based on PHQ-\n9–related observations.\nIn your reasoning, consider the following aspects:\n• Interest or pleasure in activities\n• Mood (sadness, hopelessness)\n• Energy and fatigue\n• Sleep quality\n• Appetite or eating changes\n• Self-evaluation (guilt, self-worth)\n• Concentration or attention\n• Psychomotor changes (slowed or restless behavior)\n• Anxiety or worry\nImportant Instructions:\n• Write a single, concise clinical narrative (150–200\nwords maximum)\n• Do NOT generate multiple versions, drafts, or revi-\nsions\n• Avoid mentioning charts, axes, or numerical values\n• Write naturally, as if summarizing for another clini-\ncian\n• Stop immediately after completing the summary\nTable 12: Prompt template for VLM-based narrative\ngeneration. The model receives a multi-panel time-\nseries visualization and contextual features (sleep dura-\ntion, conversation length) to generate a clinical mental\nhealth summary.\nVLM QA Prompt (Image Input)\nYou are a clinical psychologist interpreting behavioral and\nphysiological data shown in the accompanying image. The\nvisualization contains seven streams collected across the\npast four hours:\n1. Heart rate (bpm): indicator of arousal, stress, and\nautonomic balance.\n2. Pseudoactigraphy: derived from wrist accelerom-\neter signals, representing movement intensity and\nrest–activity rhythm.\n3. Steps per minute: reflects overall mobility and en-\ngagement in physical activity.\n4. Stress level: Garmin HRV-based estimation of phys-\niological stress.\n5. GPS coordinates (longitude and latitude): capture\nspatial mobility and time spent in different environ-\nments.\n6. Phone unlock status: number of unlock events per\nminute, representing cognitive or social engagement.\nAdditional contextual features:\n{contextual_features}\nQuestion:\n{question}\nYour task:\nAnalyze the figure and answer the question directly. Base\nyour reasoning only on observable behavioral and physi-\nological patterns plus the contextual features. Produce a\nconcise, clinically grounded answer (2–3 sentences) with\nno bullet points.\nImportant Instructions:\n• Provide one clear answer, no alternative scenarios\n• Avoid referencing charts, axes, or specific numeric\nvalues\n• Do not speculate beyond the available evidence\n• Stop immediately after giving the answer\nTable 13: Prompt template for VLM-based question\nanswering. The model receives a time-series visual-\nization, contextual features, and a clinical question to\ngenerate a focused answer.\n21\n"}, {"page": 22, "text": "Text Baseline Narrative Prompt\nYou are a clinical reasoning assistant that interprets physi-\nological and behavioral time-series data to infer a user’s\npsychological and physical wellbeing.\nYou will receive seven time-series streams recorded over\nthe last 4 hours, each represented in text form, along with\ntwo summary variables (sleep duration and conversation\nlength).\nTime-series Inputs:\n1. Heart rate (1 reading per minute, length 1440)\n<ts></ts>\n2. Pseudoactigraphy (accelerometer-based movement\nintensity\n×\nzero-crossing\nrate,\nlength\n480)\n<ts></ts>\n3. Steps per minute (length 240) <ts></ts>\n4. Stress level (length 240) <ts></ts>\n5. GPS longitude (length 24) <ts></ts>\n6. GPS latitude (length 24) <ts></ts>\n7. Phone unlock status (binary 0/1 per minute, length\n240) <ts></ts>\n{sleep_conversation}\nTask:\nUsing only the provided textual data, produce a short clin-\nical summary (about one concise paragraph) describing\nthe user’s psychological and physical state over the last 4\nhours.\nYour description should resemble a human-written mental-\nhealth assessment and cover these symptom dimensions:\n• Interest or pleasure in activities\n• Depressed or hopeless mood\n• Sleep quality or restfulness\n• Energy or fatigue\n• Appetite or eating pattern\n• Self-esteem or self-criticism\n• Concentration or focus\n• Psychomotor activity (slowed or restless)\n• Thoughts of self-harm or hopelessness\n• Physical discomfort (e.g., headache, stomach, or\nbody aches)\n• Nervousness or anxiety\n• Uncontrollable worry\n• Exposure to recent negative events\nOutput Format:\nReturn only the narrative summary paragraph. Do not\ninclude bullet points, lists, or section headers. End with a\nbrief statement summarizing the likely mood severity (e.g.,\nmild, moderate, or severe depression/anxiety).\nTable 14: Prompt template for text-based narrative\ngeneration. The <ts></ts> placeholders are replaced\nwith raw numerical time-series arrays at inference time.\nText Baseline QA Prompt\nYou are a clinical reasoning assistant that interprets physi-\nological and behavioral time-series data to answer clinical\nwellbeing questions about the user.\nYou will receive seven time-series streams recorded over\nthe last 4 hours, each represented in text form, along with\ntwo summary variables (sleep duration and conversation\nlength).\nTime-series Inputs:\n1. Heart rate (1 reading per minute, length 1440)\n<ts></ts>\n2. Pseudoactigraphy (accelerometer-based movement\nintensity\n×\nzero-crossing\nrate,\nlength\n480)\n<ts></ts>\n3. Steps per minute (length 240) <ts></ts>\n4. Stress level (length 240) <ts></ts>\n5. GPS longitude (length 24) <ts></ts>\n6. GPS latitude (length 24) <ts></ts>\n7. Phone unlock status (binary 0/1 per minute, length\n240) <ts></ts>\n{sleep_conversation}\nQuestion:\n{question}\nAnswer Requirements:\n• Provide a concise, clinically grounded answer in one\nor two sentences.\n• Refer only to the information implied by the time-\nseries data; do not add external facts.\n• If the data is insufficient, explicitly say so.\nAnswer:\nTable 15: Prompt template for text-based question\nanswering. The <ts></ts> placeholders are replaced\nwith raw numerical time-series arrays at inference time.\nZero-shot Baseline Output Example\nThe user demonstrates a pattern of moderate physical ac-\ntivity, with periods of increased movement and heart rate,\nsuggesting engagement in various tasks or activities. Sleep\nduration of 10.02 hours indicates good rest, potentially sup-\nporting mood stability. The stress level shows fluctuations,\nwith peaks that may correlate with increased activity or\ncognitive demands, as evidenced by frequent phone unlock-\ning. The GPS data reveals limited spatial mobility, possibly\nreflecting a routine or sedentary lifestyle. The conversation\nlength of 2303 seconds suggests active social engagement,\nwhich can be beneficial for mood. Overall, the user appears\nto have a balanced routine with adequate rest and social\ninteraction, though the fluctuating stress levels warrant fur-\nther investigation to assess potential underlying anxiety\nor worry. The data does not strongly indicate significant\nmood disturbances or psychomotor changes, but continued\nmonitoring is advised to capture any emerging patterns\nrelated to depression or anxiety.\nTable 16: Example output from a zero-shot baseline\nmodel. Without few-shot guidance, the model focuses\non descriptive interpretations of sensor data and be-\nhavioral signals, rather than producing a PHQ-9–style\nnarrative summary aligned with EMA-derived symptom\ncategories and severity framing.\n22\n"}]}