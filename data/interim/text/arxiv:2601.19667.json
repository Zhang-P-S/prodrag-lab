{"doc_id": "arxiv:2601.19667", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.19667.pdf", "meta": {"doc_id": "arxiv:2601.19667", "source": "arxiv", "arxiv_id": "2601.19667", "title": "SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking", "authors": ["Adam Remaki", "Christel Gérardin", "Eulàlia Farré-Maduell", "Martin Krallinger", "Xavier Tannier"], "published": "2026-01-27T14:47:17Z", "updated": "2026-01-27T14:47:17Z", "summary": "We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.19667v1", "url_pdf": "https://arxiv.org/pdf/2601.19667.pdf", "meta_path": "data/raw/arxiv/meta/2601.19667.json", "sha256": "7a97dae787289fdfbc77964b6e0a95a4dd3ce7b6b79fe95cb8ceb8810b864c30", "status": "ok", "fetched_at": "2026-02-18T02:20:21.331575+00:00"}, "pages": [{"page": 1, "text": "SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity\nLinking\nAdam Remaki1 , Christel G´erardin1 , Eul`alia Farr´e-Maduell2 , Martin Krallinger2 and Xavier\nTannier1\n1 Sorbonne Universit´e, Inserm, Universit´e Sorbonne Paris Nord, Limics, 75006 Paris, France\n2 Barcelona Supercomputing Center, Barcelona, Spain\nadam.remaki@etu.sorbonne-universite.fr, christel.gerardin@aphp.fr,\n{eulalia.farre,martin.krallinger}@bsc.es, xavier.tannier@sorbonne-universite.fr\nAbstract\nWe present SynCABEL (Synthetic Contextualized\nAugmentation for Biomedical Entity Linking), a\nframework that addresses a central bottleneck in\nsupervised biomedical entity linking (BEL): the\nscarcity of expert-annotated training data. SynCA-\nBEL leverages large language models to generate\ncontext-rich synthetic training examples for all can-\ndidate concepts in a target knowledge base, pro-\nviding broad supervision without manual annota-\ntion. We demonstrate that SynCABEL, when com-\nbined with decoder-only models and guided infer-\nence establish new state-of-the-art results across\nthree widely used multilingual benchmarks: Med-\nMentions for English, QUAERO for French, and\nSPACCC for Spanish. Evaluating data efficiency,\nwe show that SynCABEL reaches the performance\nof full human supervision using up to 60% less\nannotated data, substantially reducing reliance on\nlabor-intensive and costly expert labeling. Finally,\nacknowledging that standard evaluation based on\nexact code matching often underestimates clini-\ncally valid predictions due to ontology redundancy,\nwe introduce an LLM-as-a-judge protocol.\nThis\nanalysis reveals that SynCABEL significantly im-\nproves the rate of clinically valid predictions. Our\nsynthetic datasets, models, and code are released to\nsupport reproducibility and future research:\n• HuggingFace Datasets & Models\n• GitHub Repository\n1\nIntroduction\nBiomedical Entity Linking (BEL) is the task of mapping\nspans of text in biomedical documents to concepts in knowl-\nedge bases (KBs) such as the UMLS or SNOMED CT.\nEarly BEL approaches were largely context-free, relying\non lexical matching or self-supervised models that exploited\nthe KB alone, thus avoiding the need for context-dependent\nannotated training data. However, such methods struggle with\npervasive lexical ambiguity: terms like “discharge” (release\nvs. secretion) or abbreviations common in clinical texts, such\nas “AS” (Aortic Stenosis vs. Ankylosing Spondylitis), may\nrefer to distinct concepts [Newman-Griffis et al., 2021; Xu et\nal., 2007].\nTo overcome these limitations, context-aware models\nemerged, leveraging surrounding text to resolve ambiguity.\nWhile superior when sufficiently trained, they introduce a\nmajor constraint: the need for large-scale, high-quality an-\nnotated datasets, which are scarce in the biomedical domain.\nCreating such resources is labor-intensive, requiring clinical\nexperts to match mentions to ontology concepts. This scarcity\nseverely limits generalization and creates a bottleneck for su-\npervised BEL, as illustrated in Figure 1 where an ambiguous\nmention requiring context is absent from the training set.\nOutputs\nHuman-annotated Data\n(e.g. GenBioEL)\nInputs\n1. [TCA] was used to precipitate proteins.\nTrichloroacetic acid\nCode: C0040900\nDef: A strong acid used\nas a protein precipitant\n2. [TCA] was prescribed for depression.\nTricyclic Antidepressant\nCode: C3536819\nDef: Medication used as\ntreatment for depression\nPresent in \nhuman annot.\nAbsent from\nhuman annot.\n1. [TCA]  is\n2. [TCA]  is\nC0040613\nC0003289\nAugmented Data\n(SynCABEL)\nKnowledge Base\n(e.g. SapBERT)\nOutputs\n1. [TCA]  is\n2. [TCA]  is\nC0040900\nOutputs\n1. [TCA]  is\n2. [TCA]  is\nC3536819\nC3536819\nC3536819\nContext\nModel is\ntrained on...\nCoverage\nContext\nCoverage\nContext\nCoverage\nFigure 1:\nIllustration of biomedical entity linking annotation\nscarcity: a context-free model fails without context, a supervised\ncontext-aware model trained on human annotations fails on unseen\nconcepts, while a SynCABEL-augmented model leverages synthetic\ndata to recover the correct concepts.\nTo address this annotation scarcity issue, we introduce\nSynCABEL (Synthetic Contextualized Augmentation for\nBiomedical Entity Linking), a framework designed to en-\nhance context-aware BEL by leveraging large language mod-\nels (LLMs) to generate context-rich training instances for all\ncandidate concepts in a KB. Our contributions are as follows:\n• We release the first large-scale multilingual synthetic\ndataset for BEL in English, French, and Spanish.\n• We\nshow\nthat\nSynCABEL\nmatches\nfull\nhuman-\nannotated training performance with substantially fewer\nhuman annotations.\n• We demonstrate that combining recent decoder-only\nmodels with SynCABEL and guided inference achieves\nstate-of-the-art performance on multiple BEL bench-\nmarks.\narXiv:2601.19667v1  [cs.CL]  27 Jan 2026\n"}, {"page": 2, "text": "Additionally, we developed an LLM-as-a-judge evaluation\nprotocol for BEL that goes beyond exact code matching by\nassessing the semantic relationship between predicted and\ngold concepts, including equivalence, broader, narrower, and\nunrelated relations.\n2\nRelated Works\n2.1\nBEL Systems\nResearch in BEL has evolved through several distinct\nparadigms, each addressing the problem with different\nstrengths and limitations [Chen et al., 2025b].\nRule-based Systems.\nEarly systems like cTAKES [Savova\net al., 2010], and SciSpacy [Neumann et al., 2019] relied\non heuristic string matching. While widely adopted, their\nreliance on static dictionaries limits generalization to prede-\nfined lexical variants, causing failures when a mention refers\nto an existing concept using a surface form not listed among\nits synonyms in the ontology.\nContext-free\nBi-encoder\nModels.\nTransformer-\nbased\nmodels\nlike\nSapBERT\n[Liu\net\nal.,\n2021a;\nLiu et al., 2021b] and CODER [Yuan et al., 2022b]\nuse self-supervised contrastive learning on synonyms. While\nefficient for embedding names in a shared space without\nannotated data, they ignore surrounding context, limiting\ntheir ability to resolve ambiguity.\nPrompting LLM.\nLLMs have been used via prompting\nto simplify mentions before linking [Borchert et al., 2024;\nVollmers et al., 2025], as re-rankers [Ye and Mitchell, 2025],\nor guided via constrained decoding without training [Lin et\nal., 2025]. However, prompting-based methods often yield\ninferior results compared to specialized EL models.\nContextualized Bi-encoder Models.\nThese systems en-\ncode mentions with context and candidates in a shared space.\nUnlike context-free models, they explicitly learn to capture\ncontextual information. ArboEL [Agarwal et al., 2022], for\nexample, adapts BLINK [Wu et al., 2020] and introduces\na graphical arborescence objective to model cross-document\ncoreference through directed spanning trees, achieving state-\nof-the-art results on MedMentions [Mohan and Li, 2019].\nGenerative Models.\nThese models treat entity linking as\nconditional text generation, directly producing concept iden-\ntifiers. GENRE [Cao et al., 2021] introduced constrained de-\ncoding to ensure valid outputs and strictly improve memory\nefficiency by avoiding dense indexes. Currently, biomedical\nimplementations like GenBioEL [Yuan et al., 2022a] are built\non pretrained encoder-decoder architectures.\nThe general-\ndomain InsGenEL [Xiao et al., 2023] has extended this ap-\nproach by fine-tuning a decoder-only model, though it is op-\ntimized for a joint mention detection and linking objective\nand not for entity linking only.\n2.2\nBEL Data Augmentation\nDespite architectural differences, both retrieval and genera-\ntive methods depend on annotated datasets which are cur-\nrently covering only a fraction of the KB and lack variability.\nThis scarcity creates a bottleneck for developing robust, gen-\neralizable BEL systems, motivating alternative data creation\nstrategies to improve scalability and coverage while reducing\nannotation costs.\nWeak and Distant Supervision.\nApproaches using exact\nstring matching in large corpora (e.g., PubMed, Wikipedia)\nto create training data [Zhang et al., 2022; Vashishth et al.,\n2021; Wang et al., 2023] are scalable but noisy due to syn-\nonym ambiguity and non-clinical contexts.\nTemplate-Based Synthetic Data.\nTo better capture entity\nsemantics, Yuan et al. [2022a] used UMLS definitions as tem-\nplates, filled with synonyms. This provides clean, concept-\nfocused data but lacks the lexical and contextual diversity of\nnatural language.\nLLM-generated Synthetic Data.\nXin et al. [2024] and\nChen et al. [2025a] have demonstrated the effectiveness of\nLLMs for generating additional training examples, but re-\nstrict synthesis to concepts already observed during training.\nWhile this increases contextual diversity and improves gener-\nalization across known entities, it does not address annotation\nscarcity for entirely unseen concepts. Relatedly, Josifoski et\nal. [2023] leverage synthetic data to mitigate data scarcity,\nbut their approach targets a different task setting, namely re-\nlational triple extraction.\n2.3\nPositioning of SynCABEL\nSynCABEL advances BEL on two fronts. First, it introduces\na data augmentation strategy that generates examples for the\nentire space of candidate concepts in the KB. It overcomes\nthe limitations of prior work that either used templates for full\ncoverage (low quality) or LLMs only for training-set concepts\n(limited coverage). Second, it is the first approach to fine-\ntune a decoder-only model specifically for BEL, allowing us\nto leverage recent improvements in large foundation models.\n3\nMethodology\n3.1\nProblem statement\nLet E represent the set of candidate concepts from a given\nKB. Each concept e ∈E is associated with a semantic\ngroup ge (e.g., Disorders) and multiple synonyms Se =\n{s1\ne, . . . , sne\ne }. Given a mention m with left context cl, right\ncontext cr and semantic group ge, our objective is to predict\nthe correct concept em that the mention m refers to.\n3.2\nGenerative BEL Training Objective\nWe adopt the autoregressive formulation from Cao et\nal. [2021]. The input sequence x is defined as:\nx = cl [ m ] { ge } cr [ SEP ][ m ]is\nwhere cl, cr denote context, m the target mention, and ge its\nsemantic group. The token [SEP] and cue phrase “m is”\nprompt the model to generate output sequence y defined as:\ny = em\nwhere em is the concept corresponding to mention m. We\nmaximize the conditional likelihood:\npθ(y|x) =\nNy\nY\ni=1\npθ(yi|y<i, x)\n"}, {"page": 3, "text": "where Ny is the number of tokens in the output, and yi is the\ni-th token.\n3.3\nAdaptive Concept Representation for Training\nA key challenge during training is representing the concept\nem in natural language for the target output. We employ an\nadaptive method that selects the most appropriate synonym\nof a concept based on the input mention. As shown in Fig-\nure 2, the method proceeds in two steps. First, we preprocess\neach concept’s list of synonyms by removing those that are\nambiguous with other concepts in the same semantic group.\nFor example, the term “discharge” is removed from the Dis-\norder group because it can refer to multiple clinical symp-\ntoms. Then, for each remaining unambiguous synonym se,\nwe compute the cosine similarity between the embeddings of\nthe mention m and se. The concept representation em is de-\nfined as the synonym with the highest similarity score.\nem = arg max\nse∈Se\nvm · vse\n∥vm∥∥vse∥\nwhere vm and vse are the embedding vectors of the mention\nm and synonym se, respectively. Importantly, the embed-\ndings can be derived from any model, in our experiments, we\ncompare different embedding models to assess their impact\non linking performance.\nCode: C0012621\nSynonyms\nBody Fluid\nDischarge\nFluid Discharge\nDischarge\nOutput: Fluid Discharge.\nAmbiguous\nCosine similarity score computation\nBody Fluid\nDischarge\nFluid\nDischarge\nSynonyms\nEmbedding\ndischarge\nText\nEmbedding\nMention\nText\nMax\n0.71\n0.78\nInput: Signs of [discharge]{Disorder} on exam.<SEP>[discharge] is\nFigure 2: Example of the disambiguation process used to select a\nnatural representation em = “Fluid Discharge” for a concept e =\n“C0012621” and a mention m = “discharge”.\n3.4\nSynthetic Data Generation\nTo improve concept coverage while maintaining contextual\nrealism, we generate synthetic training data using a struc-\ntured prompt-based approach.\nFor a given concept e, our\nmethod generates contextual sentences that capture diverse,\nnatural ways of expressing e. As illustrated in Figure 3, this\nis achieved by constructing a structured prompt composed\nof three key components: (i) the task description that ex-\nplicitly structures the generation process into two sequential\ntasks: first, selecting and generating mentions of the concept.\nSecond, generating contextualized sentences for each gener-\nated mention, aligned with the style of the provided human-\nannotated dataset examples. (ii) Random contextual exam-\nples from a human-annotated training set (iii) Title, semantic\ngroup, semantic type, definitions and synonyms of the target\nconcept e from the KB, and this structured prompt is pro-\nvided to an LLM, which produces the final contextualized\nsentences. This structured prompt is formulated in the same\nlanguage as the target dataset.\n3.5\nTraining Data Composition\nThe synthetic examples generated by SynCABEL are com-\nbined with human-annotated training data to fine-tune the en-\ntity linking model (Figure 3). To preserve the characteristics\nof clinical reports while benefiting from the increased cov-\nerage of synthetic data, we jointly train on both sources and\nupsample human-annotated examples such that they consti-\ntute half of the training instances.\n3.6\nGuided Inference\nGreedy decoding often yields valid entities absent from the\nKB. To address this, we adopt the guided inference mecha-\nnism from Cao et al. [2021], illustrated in Figure 3. First,\nthe KB vocabulary is filtered by the target entity’s semantic\ngroup (e.g., Disorders) and structured as a trie, where each\npath maps a synonym to a unique concept. Then, the model\ndynamically restricts token selection at each step to valid\nbranches. For instance, after generating the prefix “A”, the trie\npermits only valid continuations like “the” (for Atherosclero-\nsis) or “ortic” (for Aortic).\n3.7\nLLM-as-a-judge Evaluation\nStandard entity linking evaluation relies on exact code match-\ning and therefore may fail to capture the true clinical rela-\ntionship between predicted and gold concepts. In practice, a\nprediction can be clinically correct despite a code mismatch\ndue to ontology redundancy (e.g., multiple codes for the same\nconcept) or annotation noise, where the prediction better re-\nflects the mention than the human label. To capture these nu-\nances, we introduce a four-class scheme (Figure 4): (i) Cor-\nrect, clinically appropriate even without exact code match;\n(ii) Broad, relevant but overly general; (iii) Narrow, relevant\nbut overly specific; and (iv) No relation, clinically unrelated.\nLabels are assigned automatically by an LLM using precise\nclass definitions and five clinician-annotated examples.\n4\nExperiments\n4.1\nDatasets\nHuman-Annotated Datasets\nWe rely on three human-annotated corpora for our experi-\nments:\n• MedMentions-ST21pv (MM-ST21pv) [Mohan and Li,\n2019], a curated subset of the MedMentions corpus, con-\nsisting of 4,392 English PubMed abstracts. MedMen-\ntions is currently the largest publicly available human-\nannotated dataset for biomedical entity linking.\n• QUAERO\nFrench\nMedical\nCorpus\n(QUAERO)\n[N´ev´eol et al., 2014], the largest French dataset for\nbiomedical entity linking, composed of two subsets:\nQUAERO-MEDLINE, derived from MEDLINE titles,\nand QUAERO-EMEA, extracted from documents pub-\nlished by the European Medicines Agency.\n"}, {"page": 4, "text": "Training Data Composition\nGuided Inference\nSynthetic Data Generation\nTemplate prompt\n# Task Description\n# Context Examples\n# User Input\nKnowledge base\nHuman-annotated\ntraining dataset\nLLM\ngeneration\nRandom\nsampling\nPatient diagnosed with [AS]{Disorder}.<SEP>[AS] is\nDisorder\nTrie\nFine-tuned\ngenerative\nmodel\nA\nAn\nor\nky\ntic\nlos ing s pond y litis\nthe\nhyper ost osis\nros\nclerosis\nVal\nve\nSte nos is\ns clerosis\n}\n}\n}\n}\n}\nInput :\nOutput :\nTrie constrained beam search\nAugmented\ndataset\nSynthetic\ndataset\nOversampled\nhuman\ndataset\nAugmented\ndataset\nFigure 3: Overview of SynCABEL. LLM: large language model\nInput: The patient has [lesions below the macula in the left eye]\nGold standard concept\nLLM\nAS-A-JUDGE\nTitle: Lesion of retina\nPredicted concept\n   Correct\n    Broad\n   Narrow\n No relation\nCode: C1402302\nTitle: Lesion of eye\nCode: C0743711\nSystem prompt\n# Label Definitions\n# Annotated Examples\nFigure 4: LLM-as-a-judge evaluation of predicted vs. gold concepts\nusing four clinical-relation classes: Correct, Broad, Narrow, and No\nrelation.\n• Spanish Clinical Case Corpus (SPACCC) [Miranda-\nEscalada et al., 2022; Lima-L´opez et al., 2023a; Lima-\nL´opez et al., 2023b], a manually annotated collection of\nclinical case reports derived from open-access Spanish\nmedical publications. The corpus contains 1,000 clinical\ncases, with disease, procedure, and symptom mentions\nannotated using the SNOMED CT ontology.\nMM-ST21pv and QUAERO were selected because they\nare the largest human-annotated biomedical entity linking\ndatasets available in their respective languages and because\nthey cover nearly all UMLS semantic groups. SPACCC was\nchosen as the largest Spanish BEL dataset and, more impor-\ntantly, because it consists of clinical reports, making it partic-\nularly valuable for assessing the potential applicability of our\napproach in clinical settings. Table 1 summarizes the main\ncharacteristics of these datasets.\nSynthetic Datasets\nWe\nconstructed\nthree\nsynthetic\ndatasets\n(SynthMM,\nSynthQUAERO,\nand\nSynthSPACCC)\nto\ncomple-\nment\nthe\nhuman-annotated\ncorpora.\nWe\nemployed\nLlama-3-70B [Grattafiori et al., 2024] to generate three\ntraining examples per concept, using prompts containing\nfive random annotated examples from the target training set.\nWhile our method is applicable to all candidate concepts,\nfor SynthMM and SynthQUAERO we restricted generation\nto the subset of UMLS concepts with available definitions\n(covering 6.5% and 4.5% of the KB, respectively) to maintain\ncomputational feasibility.\nMM-ST21PV\nQUAERO\nSPACCC\n# Docs\n4,392\n2,422\n1,000\n# Mentions\n203,282\n16,185\n27,799\n# Concepts\n25,419\n5,045\n9,254\n# Sem. Groups\n14\n10\n3\nLanguage\nEnglish\nFrench\nSpanish\nClinical cases\n✗\n✗\n✓\nTable 1: Summary statistics of the human-annotated datasets. MM-\nST21pv: MedMentions-ST21pv; SPACCC: Spanish Clinical Case\nCorpus\nDataset\nSynthMM SynthQUAERO SynthSPACCC\n# Mentions\n460,878\n396,914\n1,813,463\n# Concepts\n153,374\n132,089\n276,649\nTable 2: Summary of synthetic datasets.\nTable 2 summarizes the key statistics of these generated\ndatasets. The full datasets, along with examples and all asso-\nciated prompts are available on HuggingFace.\n4.2\nImplementation details\nHardware and software\nAll experiments were conducted on a single NVIDIA H100\nGPU (80GB). The codebase is implemented in Python using\nPyTorch and the Hugging Face Transformers library. For re-\nproducibility, full implementation details, hyperparameters,\nand code are provided on GitHub.\nKnowledge bases\nWe used three KBs aligned with each dataset’s annotation\nguidelines.\nFor MM-ST21pv, we used UMLS 2017AA\nrestricted to the ST21-pv scheme (21 semantic types and\nsynonyms from 18 source ontologies). For QUAERO, we\nused UMLS 2014AA filtered to 10 semantic groups.\nFor\nSPACCC, we relied on gazetteers derived from relevant\nbranches of Spanish SNOMED-CT (July 31, 2021 release);\nsince SPACCC comprises three subsets (diseases, symptoms,\nprocedures), these categories served as semantic groups.\n"}, {"page": 5, "text": "Model\nMM-\nST21PV\n(english)\nQUAERO-\nMEDLINE\n(french)\nQUAERO-\nEMEA\n(french)\nSPACCC\n(spanish)\nAverage\nRULE-BASED (UNSUPERVISED)\nSciSpacy [Neumann et al., 2019]\n53.8\n40.5\n37.1\n13.2\n36.2\nCONTEXT-FREE BI-ENCODER (SELF-SUPERVISED)\nSapBERT [Liu et al., 2021a]\n51.1\n50.6\n49.8\n33.9\n46.4\nCODER-all [Yuan et al., 2022b]\n56.6\n58.7\n58.1\n43.7\n54.3\nSapBERT-all [Liu et al., 2021b]\n64.6\n74.7\n67.9\n47.9\n63.8\nCONTEXTUALIZED BI-ENCODER (SUPERVISED)\nArboEL [Agarwal et al., 2022]\n74.5\n70.9\n62.8\n49.0\n64.2\nGENERATIVE ENCODER-DECODER (SUPERVISED)\nmBART-large [Tang et al., 2020]\n65.5\n61.5\n58.6\n57.7\n60.8\n+ Guided inference [Cao et al., 2021]\n70.0\n72.8\n71.1\n61.8\n68.9\n+ SynCABEL (Our method)\n71.5\n77.1\n75.3\n64.0\n72.0\nGENERATIVE DECODER-ONLY (SUPERVISED)\nLlama-3-8B [Grattafiori et al., 2024]\n69.0\n66.4\n65.5\n59.9\n65.2\n+ Guided inference [Cao et al., 2021]\n74.4\n77.5\n72.9\n64.2\n72.3\n+ SynCABEL (Our method)\n75.4\n79.7\n79.0\n67.0\n75.3\nTable 3: Entity linking performance (Recall@1) on biomedical benchmarks. The highlighted row shows our contribution, where the base\nmodel is trained on an augmented dataset. The best results are shown in bold, the second-best results are underlined, and the “Average”\ncolumn reports the mean score across the four benchmarks.\n4.3\nResults\nBenchmark Performance\nWe reproduced several state-of-the-art BEL systems from\ndifferent paradigms:\nthe rule-based SciSpacy [Neumann\net al., 2019]; the context-free bi-encoders SapBERT [Liu\net al., 2021a], SapBERT-all [Liu et al., 2021b], and\nCODER-all [Yuan et al., 2022b]; the contextualized bi-\nencoder ArboEL [Agarwal et al., 2022]; the generative\nencoder-decoder mBART-large [Tang et al., 2020] and\ndecoder-only Llama-3-8B [Grattafiori et al., 2024]. For gen-\nerative models (mBART-large and Llama-3-8B), we eval-\nuated three configurations:\n(i) supervised fine-tuning on\nhuman-annotated data; (ii) adding guided inference adapted\nfrom GENRE [Cao et al., 2021]; and (iii) our proposed Syn-\nCABEL approach, which augments the training data with\nsynthetic examples while retaining guided inference. For fair\ncomparison, all baselines were constrained to the same KBs,\nand for each mention, only concepts from the matching se-\nmantic group were considered as candidates. In practice, this\nmeant that for a given input, every baseline was provided with\nthe same candidate set. This constraint could not be applied\nto SciSpacy, as its rule-based approach uses the entire KB and\ncannot be modified.\nTable\n3\npresents\nentity\nlinking\nperformance\n(Re-\ncall@1) across four biomedical benchmarks: MM-ST21pv,\nQUAERO-MEDLINE, QUAERO-EMEA, and SPACCC.\nModels are grouped by architecture paradigm.\nOur Syn-\nCABEL framework with Llama-3-8B achieves the highest\nscores on all four benchmarks: 75.4 on MM-ST21pv, 79.7 on\nQUAERO-MEDLINE, 79.0 on QUAERO-EMEA, and 67.0\non SPACCC. The mBART-large variant of SynCABEL also\nshows consistent improvements over its baseline across all\ndatasets.\nAdaptive Concept Representation for Training\nWe evaluate our adaptive concept representation strategy\nagainst a static baseline using concept preferred titles. We\nconsider two adaptive representations: character-level 3-gram\nTF-IDF [Neumann et al., 2019], following its effective use\nin GenBioEL [Yuan et al., 2022a] and CODER-all embed-\ndings [Yuan et al., 2022b]. Experiments are conducted on\nUMLS-based datasets (MM-ST21pv, QUAERO-MEDLINE\nand QUAERO-EMEA) using mBART-large, with training\nlimited to the original data (no augmentation) and guided de-\ncoding at inference.\nMM-\nST21PV\nQUAERO-\nMEDLINE\nQUAERO-\nEMEA\nTitle (Static)\n61.7\n53.7\n50.6\nCODER-all\n68.4\n72.5\n66.0\nTF-IDF\n70.0\n72.8\n71.1\nTable 4: Recall@1 scores for different concept representation meth-\nods. mBART-large was supervised only on the human-annotated\ndataset (i.e., without data augmentation), and inference was per-\nformed using the guided inference method. The highlighted row\nshows the chosen representation method.\n"}, {"page": 6, "text": "As shown in Table 4, the TF-IDF-based representation con-\nsistently achieves the highest Recall@1 across all datasets,\noutperforming both the static baseline and CODER-all. We\ntherefore adopt TF-IDF-based adaptive representations for all\nother experiments.\nTraining Data Composition\nWe evaluate three training data composition strategies when\ncombining human-annotated and synthetic data:\n(i) Syn-\nthetic Pretrain, which uses a two-stage training procedure,\nfirst training on synthetic data and then fine-tuning on human\ndata, without resampling; (ii) Combined, which performs a\nsingle-stage training on the union of human and synthetic\ndatasets as-is, without resampling; (iii) Interleaved, which\nperforms a single-stage training on merged data while over-\nsampling human samples so that each training step alternates\nbetween a human and a synthetic instance. These experi-\nments use Llama-3-8B with guided inference.\nMM-\nST21pv\nQUAERO-\nMEDLINE\nQUAERO-\nEMEA\nSPACCC\nSPT\n74.1\n78.0\n77.1\n64.0\nCOMB\n75.4\n79.4\n77.0\n64.3\nINT\n75.4\n79.7\n79.0\n67.0\nTable 5: Recall@1 scores for different training data composition\nstrategies. The highlighted row shows the chosen stategy.\nRecall@1 results in Table 5 show that interleaved over-\nsampling consistently outperforms other strategies across all\ndatasets, highlighting the importance of preserving a strong\nhuman signal while leveraging synthetic diversity. We there-\nfore adopt this strategy in other experiments.\nLLM-as-a-Judge Evaluation\nTo complement exact code matching, we employed GPT-5.21\nas an automated judge to assess clinical validity on SPACCC,\nthe only dataset containing full clinical reports. We evaluated\nall cases where the prediction of our best-performing model\n(Llama-3-8B) differed from the gold standard.2\nA clinician labeled 150 mismatches across Disorders,\nSymptoms, and Procedures. GPT-5.2 achieved 66.2% agree-\nment, with 79% precision for the ‘Correct’ label. Crucially,\nerrors are conservative: only 12.4% of judge predictions over-\nstated the human label (e.g., Narrow for No relation). This\ntendency to under-predict relevance implies the judge acts as\na lower-bound estimator of validity.\nTable 6 compares standard exact matching against the\nLLM-as-a-judge evaluation metric. While SynCABEL yields\na 2.8% gain in exact matching, it achieves a larger 3.7% im-\nprovement in clinically valid predictions (rising from 68.9%\nto 72.6%).\nGiven the judge’s proven conservative bias, it\nshows SynCABEL helps the model capture correct semantics\neven when it misses the exact target code.\n1Developed by OpenAI and accessed via their API.\n2We also explored the KB-based architecture to explicitly detect\nbroader/narrower relations, but it did not yield reliable results, likely\ndue to the complexity and brittleness of these KBs.\nLabel\nw/o SynCABEL\nw SynCABEL\nExact Matching\n64.2 (62.6-65.8)\n67.0 (65.5-68.6)\nCorrect\n68.9 (67.4-70.4)\n72.6 (71.2-74.1)\nBroad\n13.0 (12.1-13.8)\n11.0 (10.3-11.8)\nNarrow\n4.3 (3.8-4.7)\n5.9 (5.3-6.5)\nNo relation\n13.8 (11.1-16.7)\n10.5 (7.6-13.2)\nTable 6: Llama-3-8B performance on SPACCC with and without\nSynCABEL augmentation. Results are reported as percentages with\n95% confidence intervals estimated via document-level empirical\nbootstrap, for exact code matching and each LLM-as-a-judge label.\n5\nDiscussion\n5.1\nAnnotation Scarcity\nTo assess synthetic data impact, we stratify performance by\nwhether concepts appeared in training. As shown in Table 7,\nSynCABEL consistently improves unseen concepts perfor-\nmance across benchmarks while preserving seen results, with\nnotable gains on SPACCC and QUAERO-EMEA (+9.4 and\n+9.9 points). However, unseen performance remains below\nseen (e.g., 30.2 vs 83.0 on SPACCC), showing that SynCA-\nBEL reduces but does not fully close the annotation scarcity\ngap. For UMLS-based datasets (QUAERO, MM), generation\nwas restricted to concepts with definitions, leaving some “un-\nseen” test concepts absent from augmented data, a filter that\nlimits gains compared to SPACCC, suggesting that extending\ngeneration to all concepts could further bridge this gap.\nSubset\nw/o SynCABEL\nw/ SynCABEL\nMEDMENTIONS-ST21PV\nOverall\n74.4 (73.6–75.4)\n75.4 (74.5–76.3)\nSeen Concepts\n81.8 (81.0–82.6)\n82.2 (81.4–83.0)\nUnseen Concepts\n46.0 (43.9–48.2)\n49.0 (46.7–51.3)\nQUAERO-MEDLINE\nOverall\n77.5 (75.9–79.1)\n79.7 (78.0–81.2)\nSeen Concepts\n90.5 (89.1–91.8)\n89.7 (88.3–91.0)\nUnseen Concepts\n58.7 (55.8–61.6)\n65.1 (62.1–68.1)\nQUAERO-EMEA\nOverall\n72.9 (68.3.77.3)\n79.0 (75.7–82.2)\nSeen Concepts\n89.0 (86.1–91.6)\n92.0 (89.6–94.3)\nUnseen Concepts\n52.5 (46.1–59.3)\n62.4 (56.4–68.7)\nSPACCC\nOverall\n64.2 (62.6–65.8)\n67.0 (65.5–68.6)\nSeen Concepts\n83.0 (81.8–84.2)\n83.0 (81.8–84.2)\nUnseen Concepts\n20.8 (19.2–22.6)\n30.2 (28.1–32.4)\nTable 7: Exact-match Recall@1 performance on three biomedical\nbenchmarks comparing two variants of Llama-3-8B: one trained\nonly on the corresponding human-annotated dataset with guided in-\nference, and one further enhanced with SynCABEL. Results are\nreported overall and for subsets defined by seen/unseen concepts.\n95% confidence intervals were estimated via the empirical bootstrap\nmethod at the document level.\n"}, {"page": 7, "text": "5.2\nReducing Annotation Effort\nWe evaluate data efficiency by varying the fraction of human-\nannotated training data (0–100%), with subsets created by\nrandomly sampling documents, and comparing SynCABEL\naugmentation to the human-annotated only baseline.\n0 %\n(Synth.)\n20 %\n40 %\n60 %\n80 % 100 %\n0\n20\n40\n60\n80\nRecall@1\nMedMentions\n0 %\n(Synth.)\n20 %\n40 %\n60 %\n80 % 100 %\nQUAERO-MEDLINE\n0 %\n(Synth.)\n20 %\n40 %\n60 %\n80 % 100 %\nHuman-annotated Data (%)\n0\n20\n40\n60\n80\nRecall@1\nQUAERO-EMEA\n0 %\n(Synth.)\n20 %\n40 %\n60 %\n80 % 100 %\nHuman-annotated Data (%)\nSPACCC\nw/ SynCABEL\nw/o SynCABEL\nBest w/o SynCABEL\n Best w/o SynCABEL\nFigure 5: Data Efficiency Analysis. Exact-match Recall@1 perfor-\nmance as a function of human training data availability. The dashed\nline marks the performance ceiling of the standard baseline trained\non human-annotated data. Stars (⋆) indicate where the SynCABEL-\naugmented model (Blue) matches or surpasses this ceiling. 0% im-\nplies training on synthetic data only. Error bars denote 95% boot-\nstrap CIs.\nFigure 5 demonstrates that SynCABEL substantially reduces\nannotation needs: it reaches full-data performance using 60%\nof human annotations on MM-ST21pv and SPACCC, and\n40% on QUAERO. The gains are largest in low-resource\nregimes and diminish as more human data becomes available,\nthis trend is clear on SPACCC and MedMention, though it is\nless observable on QUAERO due to the larger confidence in-\ntervals inherent to its smaller size. Finally, models trained\nsolely on synthetic data significantly underperform fully su-\npervised baselines, confirming that human annotations re-\nmain essential for optimal results on human-annotated data.\n5.3\nReal-World Applicability\nBEL plays a central role in enabling the secondary use of\nelectronic health records. By transforming unstructured free-\ntext into structured concepts, it supports medical research,\nclinical decision, and downstream tasks such as automated\nclinical coding, cohort identification, and patient recruitment\nfor clinical trials [Remaki et al., 2025; French and Mclnnes,\n2023]. To assess the viability of our models for such real-\nworld deployment, we evaluate two dimensions: the utility\nof confidence scores for ensuring high-precision predictions,\nand the computational efficiency required for scale.\nPrediction Confidence\nFor each prediction, our model out-\nputs a confidence score, computed as the sum of token-level\nlog-probabilities of the generated sequence. This score en-\nables confidence-based filtering to trade precision for recall\ndepending on application needs. On SPACCC, we empiri-\ncally set a confidence threshold of 0.9 and evaluated perfor-\nmance on predictions exceeding this threshold.\nIt yields a precision of 80.8% (79.6–81.9), recall of 61.9%\n(60.2–63.5), and F1-score of 70.1% (68.5–71.6). These re-\nsults illustrate how applying a strict confidence threshold\ncan substantially boost precision for high-stakes applications\nwhile retaining reasonable recall.\nInference Efficiency and Memory Footprint\nWe evalu-\nate real-world deployability based on throughput and mem-\nory usage (Table 8).\nWhile SapBERT achieves the high-\nest speed, it requires a massive 20.1 GB candidate index.\nArboEL uses a smaller BERT encoder, reducing model size\n(1.2 GB) and candidate memory (7.1 GB), but throughput is\nlower (38.9 mentions/s) due to the cross-encoder re-ranking\nstep required at inference. In contrast, generative models rely\non a compact candidate trie (5.4 GB), independent of model\nsize. The mBART configuration processes 51.0 mentions/s\nwith a small model footprint (2.3 GB), whereas Llama-3-8B\nrequires substantially more GPU memory (28.6 GB) and\nachieves lower throughput (19.1 mentions/s).\nModel (GB)\nCand. (GB)\nSpeed (/s)\nSapBERT\n2.1\n20.1\n575.5\nArboEL\n1.2\n7.1\n38.9\nmBART\n2.3\n5.4\n51.0\nLlama-3-8B\n28.6\n5.4\n19.1\nTable 8: Inference speed and memory footprint. Model: Model pa-\nrameters size; Cand.: Candidate memory size.\n6\nConclusion and Future Work\nWe introduced SynCABEL, a framework designed to address\nannotation scarcity in BEL by generating synthetic, contextu-\nalized training examples for all candidate concepts in a target\nKB. Our approach achieves state-of-the-art performance on\nmultilingual benchmarks, yielding significant recall improve-\nments on unseen concepts and enhancing clinical validity as\nverified by our novel LLM-as-a-judge protocol. Although our\nresults confirm that human annotations remain the most ef-\nfective source of supervision, SynCABEL demonstrates that\ncompetitive performance can be achieved with substantially\nless of this costly data, maximizing the value of expert effort.\nFuture directions include extending the generation context\nbeyond single sentences, extending to other languages and\nrefining training with negative sampling like ANGEL [Kim et\nal., 2025]. We also aim to improve synthetic data quality by\nrefining generation prompts and validating clinical coherence\nwith intrinsic assessments by medical experts.\n"}, {"page": 8, "text": "References\n[Agarwal et al., 2022] Dhruv\nAgarwal,\nRico\nAngell,\nNicholas Monath, and Andrew McCallum. Entity Linking\nvia Explicit Mention-Mention Coreference Modeling.\nIn Marine Carpuat, Marie-Catherine de Marneffe, and\nIvan Vladimir Meza Ruiz, editors, Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage\nTechnologies,\npages\n4644–4658,\nSeattle,\nUnited States, July 2022. Association for Computational\nLinguistics.\n[Borchert et al., 2024] Florian Borchert, Ignacio Llorca, and\nMatthieu-P. Schapranow.\nImproving biomedical entity\nlinking for complex entity mentions with LLM-based text\nsimplification. Database, 2024, February 2024. Publisher:\nOxford Academic.\n[Cao et al., 2021] Nicola De Cao, Gautier Izacard, Sebastian\nRiedel, and Fabio Petroni. Autoregressive Entity Retrieval,\nMarch 2021. arXiv:2010.00904 [cs].\n[Chen et al., 2025a] Haihua Chen, Ruochi Li, Ana Cleve-\nland, and Junhua Ding. Enhancing data quality in med-\nical concept normalization through large language mod-\nels. Journal of Biomedical Informatics, 165:104812, May\n2025.\n[Chen et al., 2025b] Haihua Chen, Yuhan Zhou, Ruochi Li,\nAryan Murthy Illa, Ana Cleveland, and Junhua Ding. A\nComprehensive Survey on Medical Concept Normaliza-\ntion: Datasets, Techniques, Applications, and Future Di-\nrections, November 2025.\n[French and Mclnnes, 2023] Evan French and Bridget T.\nMclnnes.\nAn overview of Biomedical Entity Linking\nthroughout the years. Journal of biomedical informatics,\n137:104252, January 2023.\n[Grattafiori et al., 2024] Aaron\nGrattafiori,\nAbhimanyu\nDubey,\nAbhinav Jauhri,\nAbhinav Pandey,\nAbhishek\nKadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur,\nAlan Schelten, Alex Vaughan, and Yang. The Llama 3\nHerd of Models, November 2024. arXiv:2407.21783 [cs].\n[Josifoski et al., 2023] Martin\nJosifoski,\nMarija\nSakota,\nMaxime Peyrard, and Robert West. Exploiting Asymme-\ntry for Synthetic Training Data Generation: SynthIE and\nthe Case of Information Extraction. In Houda Bouamor,\nJuan Pino, and Kalika Bali, editors, Proceedings of the\n2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 1555–1574, Singapore, Decem-\nber 2023. Association for Computational Linguistics.\n[Kim et al., 2025] Chanhwi Kim, Hyunjae Kim, Sihyeon\nPark, Jiwoo Lee, Mujeen Sung, and Jaewoo Kang. Learn-\ning from Negative Samples in Biomedical Generative En-\ntity Linking. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar, editors, Findings\nof the Association for Computational Linguistics: ACL\n2025, pages 10714–10730, Vienna, Austria, July 2025.\nAssociation for Computational Linguistics.\n[Lima-L´opez et al., 2023a] Salvador Lima-L´opez,\nEulalia\nFarr´e-Maduell, Luis Gasco Sanchez, Jan Rodr´ıguez-Miret,\nand Martin Krallinger.\nOverview of SympTEMIST at\nBioCreative VIII: corpus, guidelines and evaluation of sys-\ntems for the detection and normalization of symptoms,\nsigns and findings from text. In Proceedings of the BioCre-\native VIII Challenge and Workshop: Curation and Evalu-\nation in the Era of Generative Models, November 2023.\n[Lima-L´opez et al., 2023b] Salvador Lima-L´opez, Eul`alia\nFarr´e-Maduell, Luis Gasco, Anastasios Nentidis, Anas-\ntasia Krithara, Georgios Katsimpras, Georgios Paliouras,\nand Martin Krallinger. Overview of MedProcNER task on\nmedical procedure detection and entity linking at BioASQ\n2023. Working Notes of CLEF, 2023.\n[Lin et al., 2025] Zhenxi Lin,\nZiheng Zhang,\nJian Wu,\nYefeng Zheng, and Xian Wu. Guiding Large Language\nModels for Biomedical Entity Linking via Restrictive and\nContrastive Decoding.\nIn Christos Christodoulopoulos,\nTanmoy Chakraborty, Carolyn Rose, and Violet Peng, edi-\ntors, Findings of the Association for Computational Lin-\nguistics:\nEMNLP 2025, pages 23745–23759, Suzhou,\nChina, November 2025. Association for Computational\nLinguistics.\n[Liu et al., 2021a] Fangyu Liu, Ehsan Shareghi, Zaiqiao\nMeng, Marco Basaldella, and Nigel Collier.\nSelf-\nAlignment Pretraining for Biomedical Entity Representa-\ntions. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 4228–\n4238, Online, 2021. Association for Computational Lin-\nguistics.\n[Liu et al., 2021b] Fangyu Liu, Ivan Vuli´c, Anna Korhonen,\nand Nigel Collier. Learning Domain-Specialised Repre-\nsentations for Cross-Lingual Biomedical Entity Linking.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Nav-\nigli, editors, Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers), pages 565–574, Online,\nAugust 2021. Association for Computational Linguistics.\n[Miranda-Escalada et al., 2022] Antonio Miranda-Escalada,\nLuis Gasco, Salvador Lima-L´opez, Eul`alia Farr´e-Maduell,\nDarryl Estrada, Anastasios Nentidis, Anastasia Krithara,\nGeorgios Katsimpras, Georgios Paliouras, and Martin\nKrallinger. Overview of DisTEMIST at BioASQ: Auto-\nmatic detection and normalization of diseases from clini-\ncal texts: results, methods, evaluation and multilingual re-\nsources. In Working Notes of Conference and Labs of the\nEvaluation (CLEF) Forum. CEUR Workshop Proceedings,\n2022.\n[Mohan and Li, 2019] Sunil Mohan and Donghui Li. Med-\nMentions: A Large Biomedical Corpus Annotated with\nUMLS Concepts, February 2019. arXiv:1902.09476 [cs].\n[Neumann et al., 2019] Mark\nNeumann,\nDaniel\nKing,\nIz Beltagy, and Waleed Ammar.\nScispaCy: Fast and\nRobust Models for Biomedical Natural Language Pro-\ncessing.\nIn Dina Demner-Fushman, Kevin Bretonnel\nCohen, Sophia Ananiadou, and Junichi Tsujii, editors,\nProceedings of the 18th BioNLP Workshop and Shared\n"}, {"page": 9, "text": "Task, pages 319–327, Florence, Italy, August 2019.\nAssociation for Computational Linguistics.\n[Newman-Griffis et al., 2021] Denis Newman-Griffis, Guy\nDivita, Bart Desmet, Ayah Zirikly, Carolyn P. Ros´e, and\nEric Fosler-Lussier. Ambiguity in medical concept nor-\nmalization: An analysis of types and coverage in elec-\ntronic health record datasets.\nJournal of the American\nMedical Informatics Association: JAMIA, 28(3):516–532,\nMarch 2021.\n[N´ev´eol et al., 2014] Aur´elie N´ev´eol, Cyril Grouin, Jeremy\nLeixa, Sophie Rosset, and Pierre Zweigenbaum.\nThe\nQUAERO French Medical Corpus: A Ressource for Med-\nical Entity Recognition and Normalization. In Proc of Bio-\nTextMining Work, pages 24–30, 2014.\n[Remaki et al., 2025] Adam Remaki, Jacques Ung, Pierre\nPages, Perceval Wajsburt, Elise Liu, Guillaume Faure,\nThomas Petit-Jean, Xavier Tannier, and Christel G´erardin.\nImproving\nPhenotyping\nof\nPatients\nWith\nImmune-\nMediated Inflammatory Diseases Through Automated\nProcessing of Discharge Summaries: Multicenter Cohort\nStudy.\nJMIR Medical Informatics, 13(1):e68704, April\n2025.\nCompany: JMIR Medical Informatics Distribu-\ntor: JMIR Medical Informatics Institution: JMIR Medical\nInformatics Label: JMIR Medical Informatics Publisher:\nJMIR Publications Inc., Toronto, Canada.\n[Savova et al., 2010] Guergana K Savova, James J Masanz,\nPhilip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C\nKipper-Schuler, and Christopher G Chute.\nMayo clin-\nical Text Analysis and Knowledge Extraction System\n(cTAKES): architecture, component evaluation and appli-\ncations. Journal of the American Medical Informatics As-\nsociation, 17(5):507–513, September 2010.\n[Tang et al., 2020] Yuqing Tang, Chau Tran, Xian Li, Peng-\nJen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu,\nand Angela Fan. Multilingual Translation with Extensi-\nble Multilingual Pretraining and Finetuning, August 2020.\narXiv:2008.00401 [cs].\n[Vashishth et al., 2021] Shikhar Vashishth, Denis Newman-\nGriffis, Rishabh Joshi, Ritam Dutt, and Carolyn P. Ros´e.\nImproving broad-coverage medical entity linking with se-\nmantic type prediction and large-scale datasets. Journal of\nBiomedical Informatics, 121:103880, September 2021.\n[Vollmers et al., 2025] Daniel Vollmers, Hamada Zahera,\nDiego Moussallem, and Axel-Cyrille Ngonga Ngomo.\nContextual Augmentation for Entity Linking using Large\nLanguage Models. In Owen Rambow, Leo Wanner, Mar-\nianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio,\nand Steven Schockaert, editors, Proceedings of the 31st\nInternational Conference on Computational Linguistics,\npages 8535–8545, Abu Dhabi, UAE, January 2025. As-\nsociation for Computational Linguistics.\n[Wang et al., 2023] Yi Wang, Corina Dima, and Steffen\nStaab.\n[novel] wikimed-DE: Constructing a silver-\nstandard dataset for german biomedical entity linking us-\ning wikipedia and wikidata. In The 4th Wikidata Work-\nshop, 2023.\n[Wu et al., 2020] Ledell Wu, Fabio Petroni, Martin Josi-\nfoski, Sebastian Riedel, and Luke Zettlemoyer. Scalable\nZero-shot Entity Linking with Dense Entity Retrieval. In\nBonnie Webber, Trevor Cohn, Yulan He, and Yang Liu,\neditors, Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP),\npages 6397–6407, Online, November 2020. Association\nfor Computational Linguistics.\n[Xiao et al., 2023] Zilin Xiao, Ming Gong, Jie Wu, Xingyao\nZhang, Linjun Shou, and Daxin Jiang.\nInstructed Lan-\nguage Models with Retrievers Are Powerful Entity Link-\ners. In Houda Bouamor, Juan Pino, and Kalika Bali, ed-\nitors, Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 2267–\n2282, Singapore, December 2023. Association for Com-\nputational Linguistics.\n[Xin et al., 2024] Amy Xin, Yunjia Qi, Zijun Yao, Fangwei\nZhu, Kaisheng Zeng, Xu Bin, Lei Hou, and Juanzi Li. LL-\nMAEL: Large Language Models are Good Context Aug-\nmenters for Entity Linking, July 2024. arXiv:2407.04020\n[cs].\n[Xu et al., 2007] Hua Xu, Peter D Stetson, and Carol Fried-\nman. A study of abbreviations in clinical notes. In AMIA\nannual symposium proceedings, volume 2007, page 821,\n2007.\n[Ye and Mitchell, 2025] Christophe\nYe\nand\nCassie\nS.\nMitchell. LLM as Entity Disambiguator for Biomedical\nEntity-Linking.\nIn Wanxiang Che, Joyce Nabende,\nEkaterina Shutova,\nand Mohammad Taher Pilehvar,\neditors, Proceedings of the 63rd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2:\nShort Papers), pages 301–312, Vienna, Austria, July\n2025. Association for Computational Linguistics.\n[Yuan et al., 2022a] Hongyi Yuan, Zheng Yuan, and Sheng\nYu. Generative Biomedical Entity Linking via Knowledge\nBase-Guided Pre-training and Synonyms-Aware Fine-\ntuning.\nIn Marine Carpuat, Marie-Catherine de Marn-\neffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, pages 4038–4048, Seattle, United\nStates, July 2022. Association for Computational Linguis-\ntics.\n[Yuan et al., 2022b] Zheng Yuan, Zhengyun Zhao, Haixia\nSun, Jiao Li, Fei Wang, and Sheng Yu.\nCODER:\nKnowledge-infused cross-lingual medical term embed-\nding for term normalization. Journal of Biomedical In-\nformatics, 126:103983, February 2022.\n[Zhang et al., 2022] Sheng Zhang,\nHao Cheng, Shikhar\nVashishth, Cliff Wong, Jinfeng Xiao, Xiaodong Liu,\nTristan Naumann, Jianfeng Gao, and Hoifung Poon.\nKnowledge-Rich Self-Supervision for Biomedical Entity\nLinking. In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 868–880, Abu Dhabi,\nUnited Arab Emirates, 2022. Association for Computa-\ntional Linguistics.\n"}]}