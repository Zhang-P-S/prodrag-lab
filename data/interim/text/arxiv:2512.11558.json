{"doc_id": "arxiv:2512.11558", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.11558.pdf", "meta": {"doc_id": "arxiv:2512.11558", "source": "arxiv", "arxiv_id": "2512.11558", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "authors": ["Zhenyang Cai", "Jiaming Zhang", "Junjie Zhao", "Ziyi Zeng", "Yanchao Li", "Jingyi Liang", "Junying Chen", "Yunjin Yang", "Jiajun You", "Shuzhi Deng", "Tongfei Wang", "Wanting Chen", "Chunxiu Hao", "Ruiqi Xie", "Zhenwei Wen", "Xiangyi Feng", "Zou Ting", "Jin Zou Lin", "Jianquan Li", "Guangjun Yu", "Liangyi Chen", "Junwen Wang", "Shan Jiang", "Benyou Wang"], "published": "2025-12-12T13:42:57Z", "updated": "2025-12-12T13:42:57Z", "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.11558v1", "url_pdf": "https://arxiv.org/pdf/2512.11558.pdf", "meta_path": "data/raw/arxiv/meta/2512.11558.json", "sha256": "2b868a7330d22ae2457cb9af98386af9051ea33aa110d7a89688c1bd3143a752", "status": "ok", "fetched_at": "2026-02-18T02:24:20.226401+00:00"}, "pages": [{"page": 1, "text": "DentalGPT: Incentivizing Multimodal Complex\nReasoning in Dentistry\nZhenyang Cai2†, Jiaming Zhang1†, Junjie Zhao5,6†, Ziyi Zeng2, Yanchao Li2, Jingyi Liang2\nJunying Chen2, Yunjin Yang2, Jiajun You2,4, Shuzhi Deng1, Tongfei Wang1\nWanting Chen1, Chunxiu Hao1, Ruiqi Xie1, Zhenwei Wen5, Xiangyi Feng4\nZou Ting1, Jin Zou Lin1, Jianquan Li4, Guangjun Yu2,7\nLiangyi Chen3∗, Junwen Wang5∗, Shan Jiang1∗, Benyou Wang2,7,8,9∗\n1 Shenzhen Stomatology Hospital (Pingshan) of Southern Medical University\n2 The Chinese University of Hong Kong, Shenzhen 3 State Key Laboratory of Membrane Biology,\nBeijing Key Laboratory of Cardiometabolic Molecular Medicine, Institute of Molecular Medicine,\nNational Biomedical Imaging Center, School of Future Technology, Peking University\n4 Freedom AI 5 Division of Applied Oral Sciences & Community Dental Care\nFaculty of Dentistry, The University of Hong Kong\n6 Beijing Institute of Collaborative Innovation 7 National Health Data Institute, Shenzhen\n8 Shenzhen Loop Area Institute 9 Shenzhen Institute of Big Data\nAbstract\nReliable interpretation of multimodal data in dentistry is essential for automated\noral healthcare, yet current multimodal large language models (MLLMs) struggle\nto capture fine-grained dental visual details and lack sufficient reasoning ability\nfor precise diagnosis. To address these limitations, we present DentalGPT, a\nspecialized dental MLLM developed through high-quality domain knowledge in-\njection and reinforcement learning. Specifically, the largest annotated multimodal\ndataset for dentistry to date was constructed by aggregating over 120k dental im-\nages paired with detailed descriptions that highlight diagnostically relevant visual\nfeatures, making it the multimodal dataset with the most extensive collection of\ndental images to date. Training on this dataset significantly enhances the MLLM’s\nvisual understanding of dental conditions, while the subsequent reinforcement\nlearning stage further strengthens its capability for multimodal complex reasoning.\nComprehensive evaluations on intraoral and panoramic benchmarks, along with\ndental subsets of medical VQA benchmarks, show that DentalGPT achieves supe-\nrior performance in disease classification and dental VQA tasks, outperforming\nmany state-of-the-art MLLMs despite having only 7B parameters. These results\ndemonstrate that high-quality dental data combined with staged adaptation provides\nan effective pathway for building capable and domain-specialized dental MLLMs.\n1\nIntroduction\nDental healthcare is an essential area of public health, yet the workload of dental professionals\ncontinues to increase each year [1, 2, 3, 4]. To support both clinicians and patients, multimodal large\nlanguage models (MLLMs) [5, 6, 7, 8] capable of interactive communication through dialogue have\nrecently attracted significant attention, offering new possibilities for intelligent dental care. However,\ndespite their promising performance in general medical applications [9, 10, 11, 12, 13, 14, 15],\ncurrent MLLMs still face notable limitations when dealing with more specialized medical imaging\nproblems, such as images in dentistry.\n† Equal contribution. ∗Corresponding authors.\narXiv:2512.11558v1  [cs.CV]  12 Dec 2025\n"}, {"page": 2, "text": "After extensive observation, we found that although current MLLMs can sometimes identify relevant\nfeatures in dental images, they often fail to extract the necessary information and reason over it to\nproduce correct answers. This exposes two challenges when applying MLLMs to dentistry. First,\ncurrent MLLMs lack sufficient visual understanding of dental images. This limitation prevents\nthem from effectively using their knowledge to perform reliable diagnostic reasoning. Second,\nalthough recent MLLMs exhibit strong complex reasoning capabilities that have led to substantial\ngains in complex image understanding tasks [14, 16, 17, 18, 19], our quantitative results show that\nsuch reasoning provides only marginal improvements in dentistry. This suggests that there remains\nsignificant room for advancing complex reasoning specifically tailored to dental diagnosis.\nTo address these challenges, we injected dental knowledge into the MLLM and strengthened its\nability for complex reasoning. Specifically, we collected a large number of dental images with\ntextual descriptions or labels from online sources and combined them with professionally annotated\nimages from dental hospitals. Together, these resources form a dataset, which includes over 120k\ndental images with detailed descriptions and additional QA pairs for specific downstream tasks. The\ndescriptions highlight diagnostically relevant cues in the image, helping the model connect visual\nfeatures with its textual understanding of the conditions, while the QA data further enhances its\ndownstream task performance. After the enhancement of multimodal understanding, a reinforcement\nlearning (RL) stage with the Group Relative Policy Optimization (GRPO) [20] algorithm was applied,\nguiding the model to use the added knowledge to explore more explanatory solutions for dental\nquestions. This process resulted in DentalGPT, the first specialized dental MLLM equipped with\ncomplex reasoning capabilities.\nThen, a comprehensive evaluation was conducted to assess DentalGPT’s ability in dental image\nanalysis. First, MMOral Bench and a dentistry-focused subset of medical VQA benchmarks were\ncurated to evaluate the model’s generalization ability. Second, professionally annotated intraoral\nand panoramic images were used to assess its competence in identifying specific dental diseases.\nThe results show that DentalGPT, despite having only 7 billion parameters, surpasses existing\nMLLMs in dental image understanding and question answering, highlighting its efficiency and\ndomain specialization.\nBeyond overall performance, the impact of the two training stages was examined through in-depth\nanalysis, revealing several key findings. The training data exhibited higher knowledge density\nand greater professional quality compared with GPT-5–distilled data, indicating their advantage for\ndomain learning. Based on this stronger data foundation, the Multimodal Understanding Enhancement\nstage was shown to substantially enrich the model’s dental knowledge and improve its performance\nacross diverse image analysis tasks. These gains were further amplified during reinforcement\nlearning, resulting in higher disease classification accuracy and more professional identification of\ndiagnostically relevant visual cues. Together, these results demonstrate that high-quality dental data,\ncombined with staged training, plays a critical role in shaping DentalGPT’s specialized capabilities.\nIn conclusion, our contributions are: 1) Introduce DentalGPT, a specialized dental MLLM equipped\nwith advanced dental image analysis and complex reasoning capabilities to interpret fine visual details\nand associate them with related conditions. 2) Curate a large-scale VQA dataset containing the largest\ncollection of dental images to date with detailed descriptions of diagnostically relevant visual features.\n3) Demonstrate that DentalGPT achieves strong performance on multiple disease classification and\nVQA tasks in dentistry, surpassing many larger MLLMs.\n2\nOn Incentivizing Multimodal Complex Reasoning in Dentistry\n2.1\nMotivations for Multimodal Complex Reasoning in Dentistry\nDentistry is a key medical field that relies on clinicians analyzing patients’ imaging data and commu-\nnicating with them for diagnosis, yet even leading MLLMs with strong general multimodal abilities\nstill fall far behind professional dentists in multimodal diagnosis.\nCase Study\nTo investigate this phenomenon, we conducted a case study on a dental multimodal\ntask using one leading commercial MLLM as well as both the Instruct and Thinking mode of a\nstate-of-the-art open-source MLLM (Figure 1). After analyzing the model outputs, we found that\nalthough they could identify the relevant visual features to be counted, they still produced incorrect\nanswers. Notably, by examining the reasoning trajectory of Qwen3-VL-235B-A22B-Thinking, we\n2\n"}, {"page": 3, "text": "observed that it repeatedly reflected on and revised its own intermediate conclusions. Through this\ntype of complex reasoning pattern, the model gradually approached the correct answer step by step.\nAlthough it still failed to produce the correct final prediction, this behavior highlights the potential of\napplying complex reasoning strategies to achieve more accurate multimodal diagnosis in dentistry.\nInput Question:\nHow many visible teeth in the image\nappear to have dental fillings based on\ntheir radiopaque characteristics?\nGround Truth:\n10\nGPT-5\nFrom the panoramic X-ray, the\nradiopaque (bright white) areas on the\nchewing surfaces of several molars\nindicate dental fillings...\nBy visually examining the image, there\nappear to be: Approximate number of\nteeth with fillings: 6 teeth. ...\nQwen3-VL-235B-A22B-Instruct\nBased on the provided panoramic dental radiograph, we can identify dental fillings by their radiopaque (bright\nwhite) appearance within the tooth structure — typically in the occlusal, proximal, or cervical areas — contrasting\nwith the less dense enamel and dentin. Let’s analyze the image systematically: Upper Arch ... Lower Arch ...\nTotal: 8 (upper)+8 (lower)=16 teeth with visible radiopaque fillings ...\nQwen3-VL-235B-A22B-Thinking\n<think> Okay, let's see. ... First, I need to remember that in dental X-rays, fillings (amalgam or other materials)\nappear radiopaque, meaning they show up as white or bright areas. So I need to count the teeth with those ...\n... There are 4 teeth with visible radiopaque fillings ... Wait, but maybe more. Let me check the image again. In the\nlower jaw, looking at the molars: ... There are 6 teeth with visible radiopaque fillings ...\nBut the question says \"appear to have dental fillings based on radiopaque characteristics\". So if it's visible, even if\nsmall. Let me count again: ... Total 8. Yes, I think that's it! </think>\n<answer> ... Total count: 8 teeth ...</answer>\nFigure 1: Examples of top-tier general-purpose MLLMs analyzing a dental image task. Red indicates\nincorrect analysis, green indicates correct analysis, and orange highlights reflective turns in the\ncomplex reasoning process.\nQuantitative Experiment\nFurthermore, to quantitatively assess model performance on dental tasks\nand examine whether complex reasoning provides additional benefits, We evaluated leading MLLMs 2\nwith and without the complex reasoning mode using the existing MMOral-OPG-Bench [21]. The\nresults in Figure 2 show that both models achieve higher scores in their complex reasoning mode,\nfurther demonstrating the potential of complex reasoning in dentistry.\nGPT-5\nGemini-2.5-Pro\nClaude-Sonnet-4.5\nQwen3-VL-235B-A22B\n0\n10\n20\n30\n40\n50\nAccuracy (%) on MMOral-OPG-Bench\n47.7\n46.8\n45.7\n45.1\n50.3\n47.0\n40.6\n40.3\nw/ complex reasoning\nw/o complex reasoning\nFigure 2: Accuracy (%) of MLLMs with and without the complex reasoning mode on the MMOral-\nOPG-Bench.\n2Complex reasoning mode: GPT5-2025-08-07, Gemini-2.5-Pro-Thinking, Claude-Sonnet-4-5-20250929-\nThinking, Qwen3-VL-235B-A22B-Thinking. Without complex reasoning mode: GPT5-chat-2025-08-07,\nGemini-2.5-Pro-NoThinking, Claude-Sonnet-4-5-20250929, Qwen3-VL-235B-A22B-Instruct.\n3\n"}, {"page": 4, "text": "2.2\nChallenges for Multimodal Complex Reasoning in Dentistry\nPrior works [14, 16, 17, 18, 19] have shown that complex reasoning can significantly improve MLLM\nperformance in mathematics, medicine and other complex image understanding tasks. However, the\nresults in Section 2.1 indicate that the benefits of complex reasoning for dentistry-related multimodal\ntasks remain limited. Thus, after a closer analysis of the quantitative results, we conclude the\nfollowing challenges:\nChallenge I: Limited Multimodal Understanding in Dentistry\nMLLMs in Section 2.1 perform\npoorly on MMOral-OPG-Bench (None of them exceed 60% accuracy). The foundation of complex\nvisual reasoning is a model’s ability to accurately interpret dental images, and a straightforward way\nto enhance this ability is to inject visual domain knowledge through large-scale data [5]. However,\nexisting dental vision–language datasets remain very limited; for instance, only about 0.3 percent\nof PubMedVision [13] images involve teeth. To address this gap and support greater multimodal\nunderstanding in dentistry, we introduce a multimodal dataset with 120k dental images.\nChallenge II: Limited Complex Reasoning in Dentistry\nMLLMs employing complex reasoning\nrefine and verify their intermediate conclusions, allowing them to gradually approach the correct\nanswer. As shown in Section 2.1, current MLLMs achieve only limited gains on dentistry-related\ntasks with the complex reasoning. However, prior studies [14, 18, 19] about medical MLLMs have\nshown that task-specific reinforcement learning can strengthen these reasoning abilities and improve\ndownstream performance. This motivates us to explore domain-specific reinforcement learning to\nfurther enhance complex reasoning and improve performance in dentistry.\n3\nDentalGPT: From Multimodal Understanding to Complex Reasoning\nTo overcome the limited multimodal understanding and basic reasoning patterns of existing MLLMs\nin dental imaging, DentalGPT is proposed as a dentistry-specific MLLM built through a 2-stage\ntraining process (Figure 3). Stage I focuses on multimodal understanding enhancement to strengthen\nthe MLLM’s understanding of dental images, while Stage II leverages this enhanced ability through\nreinforcement learning to improve complex reasoning.\nStep I: Enhancement of Multimodal Understanding\nDental\nImage\nLabel\nCaptioner\nImage Captioning Data\nThe scan covers a bilateral region\nincluding the jaws, maxillary sinuses,\nnasal cavity, and temporomandibular\njoints, \nwith \ngenerally \nsymmetric\nappearance. Although mild blurring is\nseen centrally on this typical panoramic\nview, the alveolar bone details remain\nsufficient for evaluation.\nDataset\nInstruction Tuning Data\nQ: What observable signs in the image\nsuggest that the tooth has caries?\nA ：A distinct dark carious lesion can\nbe seen on the tooth, characterized by\npronounced dark pigmentation...\nComplex Reasoning Data\nQ: Please analyze what diseases are\npresent in the image.\nA：<think>Based on the image...</think>\n<answer>  Mild gingival recession is\nnoticeable around ... </answer>\nImage Captioning\nGeneral Domain\nComplex Reasoning\nInstruction Tuning\nData\nDistribution\nStep II: Reinforcement Learning for Complex Reasoning\nLLM\nThinking Path 1\nThinking Path 2\nThinking Path 3\nThinking Path G\nGRPO Enhancement\nRule-based\nReward\n \nAdvantage\nCalculation\nDataset\nfor RL\nMCQ\nQuestions\nAdvanced Reasoning\n<think>\nFrom the input image, I can't find recession ...\nHowever, after examining other parts of the\nimage, I make new observations ...\nTherefore, the final answer should be ...\n</think>\n<answer>  Mild gingival recession is noticeable\naround ... </answer>\nFigure 3: The 2-stage process of building DentalGPT. Multimodal Understanding Enhancement stage\nuses a large dataset to align the model’s medical knowledge with its multimodal understanding and\nprepare it for downstream tasks; Reinforcement Learning then strengthens complex reasoning ability.\n4\n"}, {"page": 5, "text": "3.1\nStage I: Enhancement of Multimodal Understanding\nTo address the limited multimodal understanding of existing MLLMs in dentistry, this stage enriches\nthe model with high-quality dental knowledge using large-scale, professionally curated image–text\ndata. GPT-5 was used to generate detailed descriptions, forming a specialized multimodal dataset\nthat strengthens fine-grained visual understanding and basic downstream task performance.\nData Composition\nAt this stage, the collected images and their corresponding labels are organized\ninto several types of training data. First, Image Captioning data is used to train the model to clearly\nand comprehensively describe dental images, helping align textual representations with real-world\ndental concepts. Second, Instruction Tuning data consists of a large number of question–answer\nsamples, enabling the model to understand user intent and respond appropriately. Third, Complex\nReasoning data includes multi-step and reflective thinking examples, serving as a foundation for\nsubsequent reinforcement learning to enhance multimodal complex reasoning capabilities. Finally,\ngeneral-domain data [5, 22, 23] is incorporated to maintain the model’s ability to understand both\nimages and text beyond dental scenarios, preventing overfitting to dental-specific tasks.\nTraining Settings\nThe model was then trained on this dataset for two epochs with a batch size of\n256 and a learning rate of 2 × 10−5. All parameters were fully updated during training, with the first\n5% of steps allocated for learning-rate warmup.\n3.2\nStage II: Reinforcement Learning for Complex Reasoning\nAfter gaining new knowledge, the MLLM must learn to apply it for improved complex reasoning\nin multimodal diagnosis. Recent works such as DeepSeek-R1 [24] and GPT-o1 [25] show that\nreinforcement learning can encourage long chain-of-thought generation and enhance reasoning\nquality. Following this paradigm, we adopt GRPO to optimize the reasoning process of DentalGPT.\nData Composition\nTo achieve this goal, we selected a set of dental images that were not used\nduring the Stage I to construct a new dataset. Based on the original labels and their label sets, we\ngenerated multiple-choice questions with correct answers, enabling rule-based correctness checking,\nwhich is crucial for reward computation in GRPO.\nTraining Strategy\nDuring this stage, we employ the GRPO algorithm to enhance the model’s\nreasoning ability on dental multiple-choice tasks. GRPO evaluates relative advantages within sampled\ngroups of responses, enabling efficient optimization without requiring a value network.\n• Sampling Action Groups. For each input state s = (I, Q), where I is the dental image and\nQ the question, a fixed prompt is appended to require the model to generate reasoning inside\n<think> tags and produce the final answer within <answer> tags. GRPO then samples a\ngroup of candidate responses:\nai ∼πθ(a | I, Q),\ni = 1, . . . , G.\nThis promotes diverse reasoning paths and prevents premature convergence.\n• Reward Design. Each sampled answer ai is evaluated with a composite reward:\nR(ai) = 0.1 Rformat(ai) + 0.9 Racc(ai).\n– Format Reward: Ensures adherence to the required template:\nRformat(ai) =\n\u001a1,\nif formatted correctly,\n0,\notherwise.\n– Accuracy Reward: For multiple-choice questions:\nRacc(ai) =\n\u001a1,\nif the prediction is correct,\n0,\notherwise.\n• Policy Update. Rewards within the group are normalized to produce relative advantages:\nAi = R(ai) −mean(R(a1), . . . , R(aG))\nstd(R(a1), . . . , R(aG))\n.\nGRPO then updates the policy to reinforce high-advantage actions while constraining\ndeviations from the reference policy via KL regularization.\n5\n"}, {"page": 6, "text": "Training Settings\nDuring GRPO optimization, the model was optimized using grouped rollouts\nwith a sampling size of 10 responses per prompt. Training was conducted with a rollout batch\nsize of 256 and a learning rate of 1 × 10−6. The optimization ran for 5 epochs, and the maximum\nresponse length was capped at 8192 tokens to accommodate long CoT reasoning. This configuration\nensured stable exploration within each action group while maintaining sufficient capacity for detailed\nreasoning outputs.\n4\nData Engineering\nThis section elaborates on the data foundation supporting DentalGPT, including the sources of the\ntraining data and the quality control processes used to ensure high-quality dental images; additionally,\nwe analyze the overall data quality to ensure that its completeness, knowledge depth, and safety\nremain at a leading level.\n4.1\nData Collection\nBefore improving an MLLM’s understanding and reasoning on dental images, it is necessary to\ncollect a sufficiently large set of training samples (examples are shown in Figure 4), which provides\nthe foundation for the model to effectively use its knowledge during image interpretation [5].\nPMC-Dental-Caption-47k\nImage\nThis is a picture showing a cavity.\nCaptions from Paper\nOpensource-Dental-\nDetection-31k\nImage\nDental Cavity: [10, 25, 32, 45]\nGingival Recession: [1, 25, 17, 24]\nDiseases and their locations\nOpensource-Dental-\nClassification-49k\nImage\nA) Cavity; B) Hyperplasia;\nC) Gingivitis; D) Abrasion\nExisting diseases\nExpert-Annotated\nDataset\nImage\nTooth decay; periodontal disease...\nDoctor-labeled diseases\nNewly Annotated\nFigure 4: Annotation examples from different dental image collections.\n4.1.1\nExisting Annotated Data\nTo efficiently obtain a sufficiently large number of dentistry-specific multimodal datasets, we first\nsourced datasets from a variety of open-source platforms. By leveraging certified, high-quality\ndental image datasets and existing literature, we aim to enrich the model’s understanding of relevant\nradiographic data.\nPMC-Dental-Caption-47k\nPubMed Central (PMC) is a publicly accessible biomedical repository\nthat hosts a vast collection of peer-reviewed medical publications. It is considered a reliable and\nwidely used data source in previous research. From PMC, we filtered a large number of dental\nimages and retained the associated captions and labels provided within the original papers. This rich\ntextual context is expected to provide valuable information for enhancing visual understanding and\nfacilitating the integration of dental domain knowledge into our model.\nOpensource-Dental-Classification-49k\nTo further leverage image datasets that have previously\nbeen used to train classification models, we collected a wide range of dental-related classification\ndatasets and consolidated them into a larger corpus of dental images with corresponding labels.\nSpecifically, each image is associated with one or more disease labels; for multi-class or multi-label\ndatasets, we also retain the negative labels so that all available, clinically validated annotations can be\nfully exploited. This unified resource supports MLLMs in better aligning common dental disease\ncategories with key feature identification.\n6\n"}, {"page": 7, "text": "Opensource-Dental-Detection-31k\nWe also collected a number of datasets previously utilized\nfor dental lesion localization tasks, in which each image is annotated with one or more lesion\ninstances together with their spatial coordinates. Although our model is not explicitly trained to\npredict bounding boxes, such annotations provide MLLMs with implicit spatial cues and lesion\ncounts, thereby supporting the model’s capability to understand spatial relationships and quantify\ndental abnormalities.\n4.1.2\nNewly Annotated Data\nBuilding upon the open-source datasets described above, we observed that although they contain a\nconsiderable number of annotations, the diagnostic focus is predominantly centered on a few common\ndental conditions. From a clinical perspective, however, there exist additional critical abnormalities\nand visual manifestations that warrant greater attention yet are underrepresented in existing data\nsources. To address this gap, we expanded the diagnostic label set and further curated a new subset\nof dental images, which were annotated by certified dental experts with an emphasis on clinically\nsignificant conditions and indicative visual features.\nExpert-Annotated Dataset\nWe collected some dental images from internet sources, hospital\nimaging archives, and publicly available datasets. After removing duplicates and low-resolution\nimages, we curated a candidate dataset that was subsequently annotated by professional dental\nclinicians. To ensure annotation quality, a strict cross-validation mechanism was applied with\ndifferent levels of control over data variation. For the training set, annotations from dentists with a\ncross-validation agreement rate below 85% were filtered out, while the remaining labels were retained\nfor constructing caption-style training data. For the test set, a more rigorous protocol was adopted:\neach sample was annotated by at least two dentists, and only those with consistent diagnostic results\nwere preserved. This dataset equips the model with specialized clinical knowledge and a broader\nability to recognize dental diseases and clinically relevant signs.\n4.2\nData Curation\nTo enrich the model’s ability in image understanding, instruction following, and task-specific dental\nreasoning, we further curated and refined all collected data using GPT-5. This curation step enables\nthe model to learn not only visual concepts but also how to interpret clinical instructions and produce\nstructured diagnostic outputs. The overall process is outlined as follows:\nImage Captioning\nThis component focuses on enhancing the model’s ability to capture diagnos-\ntically relevant visual details in dental images. GPT-5 was instructed to describe all observable\nfeatures that may aid diagnosis while referencing the original descriptions and labels, and to avoid\nany diagnostic assumptions. These observation-based captions were then paired with predefined\nquestions and answers to form a caption-based VQA subset, improving the model’s interpretation of\ndental images and reducing visual information gaps.\nInstruction Tuning\nTo enhance performance on different downstream tasks, GPT-5 was prompted\nto generate questions based on the collected images and their descriptions, simulating real diagnostic\nscenarios. Additionally, several vision models were used to annotate a portion of the datasets, and only\nhigh-confidence annotations were retained. GPT-5 then refined these annotations and transformed\nthem into structured question–answer pairs, further improving the model’s ability to handle dental\nquestion answering.\nComplex Reasoning\nTo support the subsequent reinforcement learning stage, complex chain-of-\nthought data were constructed by GPT-5 following the HuatuoGPT-o1 [26] methodology. Fixed\nreasoning templates were added to activate the model’s basic multi-step reasoning ability, enabling it\nto analyze before answering.\nAfter these steps were completed, GPT-5-mini was used to perform a secondary verification of all\ndata. Entries that diverged from the original image descriptions or labels were removed, further\nensuring data accuracy and producing the training dataset.\n7\n"}, {"page": 8, "text": "4.3\nQuality Assessment\nAs described in the methodology, the training dataset of DentalGPT was generated by GPT-5 while\nreferencing existing image labels or descriptions to minimize hallucinations and ensure professional\ndomain knowledge injection. To validate the effectiveness of this approach, five evaluation dimensions\nwere defined, and a randomly sampled set of 3,000 entries was assessed and compared against data\nobtained through direct GPT-5 distillation. To ensure fairness, all comparisons were evaluated using\nGemini-2.5-Pro as the judge.\nEvaluation Setup\nSpecifically, the dataset was evaluated across the following five dimen-\nsions: 1) Description Completeness : Whether all observable visual details in the image are thor-\noughly described, with particular attention to features that may contribute to dental diagnosis. 2)\nTerminology Consistency : Whether professional dental terminology is used correctly and consis-\ntently throughout the description. 3) Content Safety : Whether the content adheres to medical ethics\nand safety standards, avoiding sensitive, discriminatory, misleading, or inappropriate statements. 4)\nText–Image Consistency : Whether the textual description is well written and accurately aligned\nwith the corresponding image content. 5) Knowledge Depth : Whether the description demonstrates\nan appropriate level of dental knowledge.\nGemini-2.5-Pro was asked to score each dimension on a scale of 1 to 5, and the final dataset quality\nwas reported using the average score across all evaluated samples.\nGPT-5-Distilled Dataset\nTraining Dataset of DentalGPT\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nScore\n4.35\n4.78\n4.07\n4.88\n5.0\n5.0\n4.13\n4.96\n3.97\n4.76\nFigure 5: Gemini-2.5-Pro’s multi-dimensional evaluation of GPT-5–distilled data and the train-\ning dataset of DentalGPT (scores range from 0 to 5). For each dataset, the color blocks from\nleft to right represent Description Completeness ,\nTerminology Consistency ,\nContent Safety ,\nText–Image Consistency , and Knowledge Depth .\nResults Analysis\nResults are shown in Figure 5, our training dataset demonstrates clear advantages\nover the directly distilled version across multiple evaluation metrics. It can be observed that the data\ngenerated with label references shows the most significant improvements in terminology consistency\nand knowledge depth, and also achieves notable gains in the completeness of visual detail descriptions.\nInterestingly, Gemini-2.5-Pro assigns a perfect score for safety to both datasets, indicating that\nGPT-5–generated data perform exceptionally well in medical safety, avoiding harmful diagnostic\nsuggestions and providing timely guidance to reduce potential risks.\nIn conclusion, the results indicate that our dataset, by leveraging annotations from public datasets\nand descriptions from academic literature, provides more comprehensive and more professional\nknowledge injection for the model. Such a foundation ensures substantial improvement in the\nperformance of DentalGPT.\n5\nBenchmark Design and Construction\nTo comprehensively evaluate the model, existing benchmarks containing dental images were first\nused to assess its performance. Additionally, a large set of dental images was collected and annotated\n8\n"}, {"page": 9, "text": "with disease labels by professional dentists, ensuring clinical validity and allowing further assessment\naligned with expert consensus.\n5.1\nExisting Benchmarks\nWe also conducted comprehensive evaluations on several open-source medical VQA benchmarks to\nassess the model’s performance in more scenarios.\n(1) MMOral-OPG-Bench MMOral [21] contains multiple panoramic dental images with high-\nquality expert annotations and spans five clinically grounded dimensions, offering a thorough as-\nsessment of an MLLM’s panoramic X-ray understanding. We use its open-ended test split to more\ndirectly and intuitively evaluate model performance.\n(2) DentalBench-Mixed was further constructed by strictly filtering tooth-related images from\nexisting medical VQA benchmarks. Specifically, the data were sourced from PMC-VQA [27], Omn-\niMedVQA [28], and MedXpertQA-MM [29], which are widely used for evaluating the capabilities\nof medical MLLMs. The dental-relevant portions of these benchmarks were extracted to form the\nDentalBench-Mixed dataset, enabling targeted assessment of models on dental image understanding.\n5.2\nExpert-annotated Benchmarks\nAnnotation Workflow\nTo maximize data diversity and ensure clinically reliable model outputs, we\ncollected dental images from multiple sources and invited professional dentists to perform expert\nannotations. Based on advice from dentists, we defined a set of commonly observed dental diseases\nand clinically relevant signs that are either diagnostically important themselves or serve as auxiliary\nevidence in clinical reasoning. These labels were then used to assess whether the model could\ncorrectly identify key visual cues in dental images.\nDrop\nPeriodontal Disease\nLabel n\nGingival Recession\nRetain\nLabel 1\nFigure 6: Cross-validation workflow used for benchmark labeling.\nTo ensure annotation reliability, a rigorous cross-validation workflow (shown in Figure 6) was\nimplemented. Each image was independently annotated by at least two dentists, who were asked to\nselect all clinically relevant labels present in the image. Dentists were also allowed to mark an image\nas “uncertain” when image quality or lighting conditions made visual assessment unreliable, thereby\nreducing the risk of forced or ambiguous judgments. After annotation, labels with a low agreement\nrate (below 85%) were removed, as they indicate visual patterns that cannot be consistently identified\nfrom the images alone. Furthermore, any labels with disagreement between annotators were filtered\nout, and only those with full consensus were retained to guarantee high diagnostic reliability.\nAfter the expert annotation process, only high-quality and clinically reliable labels were retained to\nform the final annotated dataset. This curated dataset serves as the foundation for constructing our\nexpert-annotated benchmarks and enables a rigorous evaluation of the model’s multimodal diagnostic\ncapability.\nBenchmark Composition\nBased on the annotations workflow described above, we construct three\nbenchmarks to comprehensively evaluate the multimodal diagnosis performance of DentalGPT. These\n9\n"}, {"page": 10, "text": "subsets cover both clinical and in-the-wild imaging conditions and span key dental diseases and\nclinically relevant signs that dentists frequently observe in real practice. Each benchmark targets\ndifferent imaging modalities or usage scenarios, enabling a thorough assessment of the model’s\nreliability, generalization ability, and robustness across diverse dental settings.\nIntraoral-Classification-I\nAnalyze this image to assess\nwhether the condition 'Abnormal\ngingival coloration' is present.\nA. True\nB. False\nIntraoral-Classification-II\nAnalyze this image to assess\nwhether the condition 'Tooth\nLoss' is present.\nA. True\nB. False\nPanorama-Classification\nKindly evaluate if the condition\n'Impacted tooth' is present in\nthis image.\nA. True\nB. False\nExpert-Annotated\nDataset\nCross-Validation\nFigure 7: Examples of Expert-annotated Benchmarks\n(1) Intraoral-Classification-I: This benchmark is sourced from the existing dental image classifi-\ncation dataset AlphaDent [30], which contains intraoral photographs collected by multiple clinical\ndentists under standardized lighting and imaging conditions. The dataset provides high-quality\nclinical views and includes expert annotations for ten conditions: tooth discoloration, abnormal\ngingival coloration, gingival recession, dental caries, tooth pigmentation, tooth defect or loss, tooth\nloss, dental calculus, abnormal tooth morphology, and abnormal gingival morphology.\n(2) Intraoral-Classification-II: This benchmark is composed of intraoral images collected from\ninternet searches. Most images were taken directly by patients, resulting in diverse lighting conditions\nand shooting angles. After removing duplicates with the training set, professional dentists annotated\nseven conditions: tooth pigmentation, abnormal gingival coloration, dental calculus, tooth loss,\ndental caries, abnormal gingival morphology, and gingival recession. This dataset reflects the\nmodel’s generalization ability to non-clinical, in-the-wild photographs.\n(3) Panorama-Classification: This benchmark consists of real clinical panoramic radiographs\n(X-ray images) collected from hospitals. Unlike intraoral photos captured by conventional cameras,\npanoramic images reveal structural and pathological features invisible in standard photographs.\nProfessional dentists annotated six categories: periodontal disease, root canal treatment, tooth defect\nor loss, jawbone lesion, periapical lesion, and impacted tooth.\nEach benchmark supports multi-label classification across both clinical and in-the-wild scenarios,\nenabling comprehensive evaluation of multimodal diagnostic ability. To avoid biased evaluation,\nwe further applied strict data balancing strategies: the label distributions and the ratios of positive\nand negative samples for each category were aligned across subsets. This ensures that accuracy\nreliably reflect model performance rather than being influenced by label frequency or overrepresented\ndiseases.\n6\nExperiment\n6.1\nExperimental Setup\nTraining Settings\nDentalGPT is developed on top of the Qwen2.5-VL-7B-Instruct. During both\nstages of our training pipeline, all parameters of the model are fully updated to ensure comprehensive\nadaptation to domain knowledge of dentistry and complex reasoning. The complete hyperparameter\nsettings and implementation details for each stage are provided in the respective subsections of\nSection 3. All training experiments are performed using a cluster equipped with 8 NVIDIA H200\nGPUs.\n10\n"}, {"page": 11, "text": "Model\nMMOral\nOPG-Bench\nDentalBench\nMixed\nIntraoral\nClassification-I\nIntraoral\nClassification-II\nPanorama\nClassification\nAvg.\nOpen-source MLLMs\nDeepseek-VL2 [31]\n39.1\n22.6\n51.1\n59.4\n55.1\n45.5\nMistral-Large-2512 [32]\n41.9\n48.2\n50.7\n58.0\n44.2\n48.6\nPhi-4-Multimodal-Instruct [33]\n38.5\n44.4\n52.2\n63.3\n61.5\n52.0\nErnie-4.5-VL-424B-A47B∗[34]\n45.0\n51.4\n58.1\n65.1\n44.9\n52.9\nQwen3-VL-235B-A22B-Instruct [16]\n40.3\n51.6\n50.7\n58.0\n55.8\n51.3\nGemma-3-27B-it [35]\n42.2\n43.0\n51.5\n61.4\n59.6\n51.5\nGLM-4.5v∗[17]\n45.7\n51.4\n54.8\n64.7\n54.5\n54.2\nQwen3-VL-235B-A22B-Thinking∗[16]\n40.6\n51.6\n56.7\n65.7\n60.3\n55.0\nLLaMA-4-Maverick [36]\n51.4\n53.9\n61.1\n67.1\n59.0\n58.5\nProprietary MLLMs\nClaude-Sonnet-4.5 [37]\n47.0\n50.4\n51.9\n59.4\n50.0\n51.7\nClaude-Sonnet-4.5-Thinking∗[37]\n50.3\n53.9\n55.2\n66.7\n55.8\n56.4\nGrok-4.1-Fast\n47.1\n52.2\n57.0\n65.2\n62.2\n56.7\nGemini-2.5-Pro-Thinking∗[38]\n45.7\n57.4\n57.0\n65.2\n64.1\n57.9\nGPT-4.1 [39]\n47.2\n51.7\n60.4\n70.5\n61.5\n58.3\nGPT-5∗[40]\n47.7\n54.3\n59.3\n71.0\n63.5\n59.2\nDentalGPT and Its Backbone\nQwen2.5-VL-7B-Instruct [6]\n27.0\n46.1\n48.8\n61.8\n50.0\n46.7\nDentalGPT∗\n60.0\n54.4\n64.1\n72.9\n84.0\n67.1\nTable 1: Accuracy (%) of MLLMs on Dental-related VQA Benchmarks. Bold indicates the best\nscore; underlines marks the second-best. ∗indicates that the model has activated complex reasoning.\nEvaluation Settings\nWe conduct evaluations on the curated Dentistry-Specific Benchmark (Sec-\ntion 5), which assesses models across dental disease classification, lesion recognition, and common\ndental consultation scenarios. All models are required to provide responses to the given tasks. For\nmodels marked with an asterisk (∗), complex reasoning mode is enabled, while other models are\ninstructed to directly output the correct option without additional reasoning steps.\n6.2\nResults and Analysis\nAs shown in Table 1, DentalGPT delivers clear and consistent performance gains over both compa-\nrable and substantially larger MLLMs across all expert-annotated datasets. It exhibits substantial\nimprovements over its backbone, Qwen2.5-VL-7B-Instruct, underscoring the effectiveness of the\nproposed data construction pipeline and domain-aligned training strategy. Despite its compact 7B\nparameter scale, DentalGPT also surpasses many general-purpose models with over 100B parameters\nacross nearly all benchmarks, demonstrating that domain-specialized modeling can achieve expert-\nlevel capability at a fraction of the computational cost and offering a promising path toward efficient,\nfield-specific MLLMs.\nBeyond expert-annotated datasets, DentalGPT maintains strong performance on the high-quality\nMMOral-OPG-Bench and the DentalBench-Mixed subset derived from general medical VQA bench-\nmarks, outperforming most competing models and showing robust generalization across diverse\ndental tasks. Together, these results establish DentalGPT as a leading multimodal foundation model\nfor dental image understanding.\n7\nIn-depth Analysis\nAfter evaluating the overall diagnostic performance, we further conducted fine-grained analyses to\nexamine how each training stage enhances different capabilities of DentalGPT, providing a clearer\nunderstanding of the respective roles of alignment and reinforcement learning in shaping its final\ndiagnostic behavior.\n7.1\nEffect of Multimodal Understanding Enhancement (on Stage I)\nEnhancement of multimodal understanding enables the MLLM to map its understanding of dental\nimages into the textual semantic space, allowing it to effectively leverage existing knowledge for\naccurate interpretation of dentistry-specific multimodal tasks. To further assess how the strength of\n11\n"}, {"page": 12, "text": "domain-specific alignment influences both visual comprehension and complex multimodal reasoning,\nwe conducted an ablation study by varying the amount of Stage-I alignment data used during training.\nExperimental Setup\nSpecifically, we conducted three controlled experiments by incorporating 0%,\n30%, and 100% of the Stage-I dataset to assess its impact. The effectiveness of different alignment\nlevels was evaluated by analyzing reward improvements during the subsequent RL stage for complex\nreasoning. To ensure fairness, duplicate images between the RL data and alignment data were strictly\nremoved, and all complex reasoning samples were excluded from the Stage-I dataset. Consistent with\nthe DentalGPT training setup, Qwen2.5-VL-7B was used as the backbone. Each model underwent\n30 RL training steps, and accuracy-based reward changes on the validation set were monitored to\nassess multimodal reasoning performance.\n0\n5\n10\n15\n20\n25\n30\nStep\n20\n30\n40\n50\n60\n70\nValidation Accuracy (%)\n100% Stage I Dataset\n30% Stage I Dataset\n0% Stage I Dataset\nFigure 8: Accuracy reward (%) of MLLM during RL training under different scales of the Stage I\ndataset.\nResults Analysis\nAs shown in Figure 8, the control group trained with 0% Stage-I dataset, which\nreceives no dental-domain knowledge, exhibits only marginal reward gains during the subsequent RL\nstage. In contrast, increasing the proportion of domain-specific training data consistently raises the\nperformance ceiling throughout RL training. These results demonstrate that the Stage-I Training pro-\nvides essential knowledge and visual grounding, thereby improving downstream reasoning capability\nin dentistry and enabling deeper image interpretation. In summary, incorporating the enhancement of\nmultimodal understanding in dentistry effectively elevates the upper bound of performance attainable\nthrough multimodal complex reasoning.\n7.2\nEffect of RL (on Stage II)\nThis section investigates how the reinforcement learning (RL) stage further shapes the model’s\ncapabilities. After applying RL training on 10k multiple-choice dental questions, the performance of\nthe model was reevaluated across the same set of benchmarks. As shown in Table 2, reinforcement\nlearning brings consistent improvements across all tasks, demonstrating that it further enhances\nthe model’s ability to execute downstream dental tasks. These gains confirm that RL effectively\nstrengthens both the accuracy and reliability of the model’s reasoning in dental image understanding.\n7.3\nCase Study\nWe further analyze the outputs produced by DentalGPT at different training stages, as well as the\noriginal backbone model. As illustrated in Figure 9, the backbone model struggles with this query:\nalthough it describes relevant visual characteristics in the image, it fails to correctly identify any\nteeth with fillings. After the enhancement of multimodal understanding, the model without complex\nreasoning is able to perform basic image analysis and detect most teeth, missing only one less\nnoticeable case. This demonstrates the importance of alignment, which enables the model to learn\nhow to interpret dental images and forms the foundation for further improving complex reasoning.\n12\n"}, {"page": 13, "text": "Benchmarks\nQwen2.5-VL\nBackbone\nQwen2.5-VL\n+ Stage I\nw/o Stage II\nDentalGPT\nw/ Stage I\n& Stage II\nMMOral-OPG-Bench\n27.0\n56.8\n60.0\nDentalBench-Mixed\n46.1\n51.7\n54.4\nIntraoral-Classification-I\n48.8\n61.5\n64.1\nIntraoral-Classification-II\n61.8\n67.6\n72.9\nPanorama-Classification\n50.0\n78.4\n84.0\nTotal\n46.7\n63.2\n67.1\nTable 2: Accuracy (%) comparison between Qwen2.5-VL, Qwen2.5-VL with Stage I training, and\nDentalGPT (with both Stage I and Stage II training) across dentistry-specific benchmarks.\nFinally, after all the training stages, DentalGPT with complex reasoning enabled conducts deeper\nmultimodal analysis. Although intermediate thoughts may include incorrect counts, the iterative\nchecking and reflection eventually lead to the correct answer. This again highlights the necessity of\nreinforcement learning for enhancing accurate multimodal reasoning in dentistry.\nInput Question:\nHow many visible teeth in the image\nappear to have dental fillings based on\ntheir radiopaque characteristics?\nGround Truth:\n10\nQwen2.5-VL-7B-Instruct\n(Backbone of DentalGPT)\nDetermine how many ..., we need to look\nfor areas of increased density\n(radiopacity) within the tooth\nstructure that do not correspond with\nnatural anatomy such as enamel or\ndentin. ...\nFinal Answer : $\\boxed{0}$\nDentalGPT w/o Complex Reasoning\nDental fillings are typically made from materials such as amalgam or composite resin, ...\nUpper Jaw (Maxilla): Careful examination reveals three teeth with clear radiopaque spots indicative of dental\nfillings. These are distributed across both the left and right sides of the upper jaw.\nLower Jaw (Mandible): The lower jaw shows six teeth with similar radiopaque features, suggesting the presence of\nfillings. These are primarily located in the molar and premolar regions, where restorations are most common due to\nhigher risk of decay. ...\nIn summary, the panoramic X-ray demonstrates 9 teeth with visible dental fillings, ...\nDentalGPT w/ Complex Reasoning\n<think> Examining the panoramic X-ray image for signs of radiopaque materials that might represent dental\nfillings. Fillings typically appear as bright white areas within the crown of the teeth.\n \nStarting from the front teeth (incisors) and moving towards the molars, ...\nAfter identifying radiopaque regions, I noted that a few molars and premolars in both the upper and lower jaw\ndisplayed these features. The exact number of teeth with fillings is confirmed to be nine ...\nHowever, after reviewing the image and reasoning, I observed discrepancies in the count. The conclusion is false,\nand further refinement is necessary. ... I systematically inspected each visible tooth in the panoramic image,\nfocusing on molars and premolars where fillings are more common. ...\nStarting from the right upper quadrant to the left upper quadrant, then proceeding from the left lower quadrant\nto the right lower quadrant, I recounted the teeth exhibiting radiopaque (bright white) regions indicative of\nfillings on the X-ray image. I identified ten teeth with these characteristics. ... </think>\n<answer> ... In summary, the panoramic X-ray demonstrates 10 teeth with visible dental fillings, easily identified\nby their radiopaque appearance.</answer>\nFigure 9: Examples of DentalGPT and its backbones analyzing a multimodal task in dentistry. Red\nindicates incorrect analysis, green indicates correct analysis, and orange highlights reflective turns in\nthe complex reasoning process.\n13\n"}, {"page": 14, "text": "8\nRelated Work\nMedical fields involve a large amount of imaging data, making them one of the most practical\napplication areas for MLLMs. General medical MLLMs [13, 41, 42, 43, 44] have demonstrated\nthe ability to perform basic medical question answering and conduct preliminary visual analysis of\nmedical images. Their aim is to generalize across diverse clinical scenarios and contribute to domain\nadaptation through large-scale multimodal medical data [27, 45, 46, 47].\nTo better support complex imaging scenarios, subsequent research has focused on adapting MLLMs\nto specific medical modalities. Some models incorporate richer modality support beyond 2D images,\nextending to 3D medical scans [11, 48, 49] and biomedical signal analysis [50, 51, 52, 53]. Addi-\ntionally, for tasks requiring high-resolution understanding such as reading detailed case images or\nzoom-level reasoning, several models [54, 55] introduce dedicated processing pipelines or tailored\ntraining strategies to address fine-grained clinical perception.\nIn dentistry, a specialty-centered medical domain, there has also been notable progress in multimodal\ndiagnosis. DentVLM [56] aggregates large-scale hospital report data to build powerful image–text\nunderstanding models tailored for dental applications. OralGPT [21, 57] further advances this\ndirection by evaluating multimodal benchmarks in dentistry and supporting various dental imaging\nmodalities. These works demonstrate the growing recognition of dentistry as a valuable and distinct\nmultimodal AI research scenario.\n9\nConclusion\nThis work introduces DentalGPT, a specialized MLLM designed to address the challenges of\nmultimodal diagnosis in dentistry. By constructing the largest annotated dental image dataset to\ndate and integrating high-quality domain knowledge through a staged enhancement of multimodal\nunderstanding and reinforcement learning pipeline, the model gains the ability to capture fine-grained\nvisual cues and perform more reliable disease-related reasoning. Extensive evaluations on intraoral,\npanoramic, and dental VQA benchmarks demonstrate that DentalGPT achieves strong performance\ndespite its compact 7B parameter size, surpassing many state-of-the-art general-purpose MLLMs.\nThese findings highlight the critical role of domain-specific data and training strategies in advancing\ndental AI, and underscore the potential of DentalGPT as a foundational model for future research and\napplications in automated dental imaging and intelligent oral healthcare.\nAcknowledgment\nThis work was supported by Shenzhen Medical Research Fund (B2503005), Major Frontier Explo-\nration Program (Grant No. C10120250085) from the Shenzhen Medical Academy of Research and\nTranslation (SMART), the Shenzhen Science and Technology Program (JCYJ20220818103001002),\nNSFC grant 72495131, Shenzhen Doctoral Startup Funding (RCBS20221008093330065),\nTianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC)\n(12326608), Shenzhen Science and Technology Program (Shenzhen Key Laboratory Grant No.\nZDSYS20230626091302006), the 1+1+1 CUHK-CUHK(SZ)-GDSTC Joint Collaboration Fund,\nGuangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence\n(2023B1212010001), and Shenzhen Stability Science Program 2023.\nReferences\n[1] Centers for Disease Control and Prevention. 2024 oral health surveillance report: Selected\nfindings, 2024. Accessed: 2025-04-28.\n[2] Xiaochen Jiang, Zhiguo Ding, Yanlei Su, Fei Wang, Weifeng Wang, Ziyang Wang, Xueling Qiu,\nChenxi Sun, Fan Sun, Lu Tang, et al. Dentists’ views on the role orientation of dental hygienists\nin china: A qualitative content analysis. BMC Oral Health, 24(1):1563, 2024.\n[3] Peixin Zheng, Xiaoting Qiu, Lingxiao Zhang, Peizhang Liu, Zeyi Peng, and Zhijian Huang.\nComparative analysis of oral disorder burden in china and globally from 1990 to 2021 based on\ngbd data. Scientific Reports, 15(1):10061, 2025.\n14\n"}, {"page": 15, "text": "[4] Richard J Lilford, Benjamin Daniels, Barbara McPake, Zulfiqar A Bhutta, Robert Mash, Frances\nGriffiths, Akinyinka Omigbodun, Elzo Pereira Pinto, Radhika Jain, Gershim Asiki, et al. Supply-\nside and demand-side factors affecting allopathic primary care service delivery in low-income\nand middle-income country cities. The Lancet Global Health, 13(5):e942–e953, 2025.\n[5] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\nin neural information processing systems, 36:34892–34916, 2023.\n[6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923,\n2025.\n[7] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\nfor generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 24185–24198, 2024.\n[9] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo: An open-\nsource framework for training large autoregressive vision-language models. arXiv preprint\narXiv:2308.01390, 2023.\n[10] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023.\n[11] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist\nfoundation model for radiology. arXiv preprint arXiv:2308.02463, 2023.\n[12] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-\nllama: toward building open-source language models for medicine. Journal of the American\nMedical Informatics Association, page ocae045, 2024.\n[13] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen,\nXidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, and Benyou\nWang. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at\nscale, 2024.\n[14] Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin\nYe, Pengcheng Chen, Ming Hu, et al. Gmai-vl-r1: Harnessing reinforcement learning for\nmultimodal medical reasoning. arXiv preprint arXiv:2504.01886, 2025.\n[15] Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Cheng-\nhao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. Lingshu: A generalist foun-\ndation model for unified multimodal medical understanding and reasoning. arXiv preprint\narXiv:2506.07044, 2025.\n[16] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao\nDeng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report. arXiv preprint\narXiv:2511.21631, 2025.\n[17] V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang,\nJiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng,\nZehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi,\nChangyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali\nChen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong,\nLeyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong,\nShiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang,\nTianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang,\n15\n"}, {"page": 16, "text": "Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi\nWang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang\nYue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou,\nZhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao\nDong, and Jie Tang. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning\nwith scalable reinforcement learning, 2025.\n[18] Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. Med-rlvr:\nEmerging medical reasoning from a 3b base model via reinforcement learning. arXiv preprint\narXiv:2502.19655, 2025.\n[19] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen,\nCheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability\nof vision-language models (vlms) via reinforcement learning. In International Conference\non Medical Image Computing and Computer-Assisted Intervention, pages 337–347. Springer,\n2025.\n[20] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li,\nY. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models, 2024.\n[21] Jing Hao, Yuxuan Fan, Yanpeng Sun, Kaixin Guo, Lizhuo Lin, Jinrong Yang, Qi Yong H Ai,\nLun M Wong, Hao Tang, and Kuo Feng Hung. Towards better dental ai: A multimodal bench-\nmark and instruction dataset for panoramic x-ray analysis. arXiv preprint arXiv:2509.09254,\n2025.\n[22] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang,\nSongcen Xu, Changrui Chen, Chunsheng Wu, Huajie Tan, Chunyuan Li, Jing Yang, Jie Yu,\nXiyao Wang, Bin Qin, Yumeng Wang, Zizhen Yan, Ziyong Feng, Ziwei Liu, Bo Li, and Jiankang\nDeng. Llava-onevision-1.5: Fully open framework for democratized multimodal training. In\narXiv, 2025.\n[23] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and\nDahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European\nConference on Computer Vision, pages 370–387. Springer, 2024.\n[24] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning, 2025.\n[25] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv\npreprint arXiv:2412.16720, 2024.\n[26] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye\nHou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv\npreprint arXiv:2412.18925, 2024.\n[27] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint\narXiv:2305.10415, 2023.\n[28] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Om-\nnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm. arXiv\npreprint arXiv:2402.09181, 2024.\n[29] Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning\nDing, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and\nunderstanding. arXiv preprint arXiv:2501.18362, 2025.\n[30] Evgeniy I. Sosnin, Yuriy L. Vasilev, Roman A. Solovyev, Aleksandr L. Stempkovskiy, Dmitry V.\nTelpukhov, Artem A. Vasilev, Aleksandr A. Amerikanov, and Aleksandr Y. Romanov. Alphadent:\nA dataset for automated tooth pathology detection, 2025.\n16\n"}, {"page": 17, "text": "[31] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao,\nYiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-\nlanguage models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302,\n2024.\n[32] Mistral AI.\nMistral large instruct 2407.\nhttps://huggingface.co/mistralai/\nMistral-Large-Instruct-2407, 2024. Accessed: 2025-12-06.\n[33] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach,\nJianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini\ntechnical report: Compact yet powerful multimodal language models via mixture-of-loras.\narXiv preprint arXiv:2503.01743, 2025.\n[34] Baidu-ERNIE-Team.\nErnie 4.5 technical report.\nhttps://ernie.baidu.com/blog/\npublication/ERNIE_Technical_Report.pdf, 2025.\n[35] Gemma Team. Gemma 3. arXiv preprint arXiv:2503.19786, 2025.\n[36] Meta AI. Llama 4 maverick. https://www.llama.com/models/llama-4/, 2025. Accessed:\n2025-12-06.\n[37] Anthropic.\nClaude opus 4-5 system card.\nhttps://assets.anthropic.com/m/\n64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf, 2025. Accessed: 2025-11-30.\n[38] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\ncapabilities. arXiv preprint arXiv:2507.06261, 2025.\n[39] OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, 2024. Accessed: 2024-11-30.\n[40] OpenAI.\nGpt-5 system card,\n2025.\nAvailable at https://cdn.openai.com/\ngpt-5-system-card.pdf.\n[41] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He,\nHao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, and\nBeng Chin Ooi. Healthgpt: A medical large vision-language model for unifying comprehension\nand generation via heterogeneous knowledge adaptation, 2025.\n[42] LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu\nWang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen,\nChaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, and Yu Rong. Lingshu: A\ngeneralist foundation model for unified multimodal medical understanding and reasoning, 2025.\n[43] Chieh-Ju Chao, Imon Banerjee, Reza Arsanjani, Chadi Ayoub, Andrew Tseng, Garvan C. Kane,\nJae K Oh, Li Fei-Fei, Ehsan Adeli, and Curtis Langlotz. Echogpt: A large language model for\nechocardiography report summarization. medRxiv, 2024.\n[44] Yuechun Yu, Han Ying, Haoan Jin, Wenjian Jiang, Dong Xian, Binghao Wang, Zhou Yang,\nand Mengyue Wu. Medkgeval: A knowledge graph-based multi-turn evaluation framework for\nopen-ended patient interactions with clinical llms, 2025.\n[45] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-\nlabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th\ninternational symposium on biomedical imaging (ISBI), pages 1650–1654. IEEE, 2021.\n[46] Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei\nXing, James Zou, Cihang Xie, et al. Medtrinity-25m: A large-scale multimodal dataset with\nmultigranular annotations for medicine. arXiv preprint arXiv:2408.02900, 2024.\n[47] Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie\nSong, Yize Chen, Zixu Zhang, and Benyou Wang. Exploring compositional generalization\nof multimodal llms for medical imaging. In Proceedings of the 63rd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 13057–13079, 2025.\n17\n"}, {"page": 18, "text": "[48] Jing Wu, Yuli Wang, Zhusi Zhong, Weihua Liao, Natalia Trayanova, Zhicheng Jiao, and\nHarrison X Bai. Vision-language foundation model for 3d medical imaging. npj Artificial\nIntelligence, 1(1):17, 2025.\n[49] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Hui Hui, Yanfeng Wang, and Weidi Xie. Towards\ngeneralist foundation model for radiology by leveraging web-scale 2d&3d medical data. Nature\nCommunications, 16(1):7866, 2025.\n[50] Yubao Zhao, Tian Zhang, Xu Wang, Puyu Han, Tong Chen, Linlin Huang, Youzhu Jin, and\nJiaju Kang. Ecg-chat: A large ecg-language model for cardiac disease diagnosis. arXiv preprint\narXiv:2408.08849, 2024.\n[51] Ruoqi Liu, Yuelin Bai, Xiang Yue, and Ping Zhang. Teach multimodal llms to comprehend\nelectrocardiographic images. arXiv preprint arXiv:2410.19008, 2024.\n[52] Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao,\nXiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, et al. Shizhengpt: Towards multimodal llms for\ntraditional chinese medicine. arXiv preprint arXiv:2508.14706, 2025.\n[53] Ziyi Zeng, Zhenyang Cai, Yixi Cai, Xidong Wang, Junying Chen, Rongsheng Wang, Yipeng\nLiu, Siqi Cai, Benyou Wang, Zhiguo Zhang, et al. Wavemind: Towards a conversational eeg\nfoundation model aligned to textual and visual modalities. arXiv preprint arXiv:2510.00032,\n2025.\n[54] Jingyun Chen, Linghan Cai, Zhikang Wang, Yi Huang, Songhan Jiang, Shenjin Huang,\nHongpeng Wang, and Yongbing Zhang. Pathagent: Toward interpretable analysis of whole-\nslide pathology images via large language model-based agentic reasoning. arXiv preprint\narXiv:2511.17052, 2025.\n[55] Sheng Wang, Ruiming Wu, Charles Herndon, Yihang Liu, Shunsuke Koga, Jeanne Shen, and\nZhi Huang. Pathology-cot: Learning visual chain-of-thought agent from expert whole slide\nimage diagnosis behavior. arXiv preprint arXiv:2510.04587, 2025.\n[56] Zijie Meng, Jin Hao, Xiwei Dai, Yang Feng, Jiaxiang Liu, Bin Feng, Huikai Wu, Xiaotang Gai,\nHengchuan Zhu, Tianxiang Hu, et al. Dentvlm: A multimodal vision-language model for com-\nprehensive dental diagnosis and enhanced clinical practice. arXiv preprint arXiv:2509.23344,\n2025.\n[57] Jing Hao, Yuci Liang, Lizhuo Lin, Yuxuan Fan, Wenkai Zhou, Kaixin Guo, Zanting Ye,\nYanpeng Sun, Xinyu Zhang, Yanqi Yang, et al. Oralgpt-omni: A versatile dental multimodal\nlarge language model. arXiv preprint arXiv:2511.22055, 2025.\n18\n"}]}