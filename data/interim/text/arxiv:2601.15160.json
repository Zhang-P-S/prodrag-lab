{"doc_id": "arxiv:2601.15160", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.15160.pdf", "meta": {"doc_id": "arxiv:2601.15160", "source": "arxiv", "arxiv_id": "2601.15160", "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning", "authors": ["Yuval Kansal", "Niraj K. Jha"], "published": "2026-01-21T16:38:59Z", "updated": "2026-01-21T16:38:59Z", "summary": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.15160v1", "url_pdf": "https://arxiv.org/pdf/2601.15160.pdf", "meta_path": "data/raw/arxiv/meta/2601.15160.json", "sha256": "06340c73b8f0fb061cdba23208eef725db0c611696b4f1f7c4af27ec59f07b5c", "status": "ok", "fetched_at": "2026-02-18T02:20:52.485680+00:00"}, "pages": [{"page": 1, "text": "Knowledge Graphs are Implicit Reward Models: Path-Derived\nSignals Enable Compositional Reasoning\nYuval Kansal\nPrinceton University\nyuvalkansal@princeton.edu\nNiraj K. Jha\nPrinceton University\njha@princeton.edu\nABSTRACT\nLarge language models have achieved near-expert performance in structured reasoning domains like\nmathematics and programming, yet their ability to perform compositional multi-hop reasoning in\nspecialized scientific fields remains limited. We propose a bottom-up learning paradigm in which\nmodels are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks.\nTo this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and\nreinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving\nnovel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded\nsupervision that encourages models to compose intermediate axioms rather than optimize only final\nanswers during RL. We validate this approach in the medical domain, training a 14B model on\nshort-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop\nqueries (4-5 hops). Our experiments show that path-derived rewards act as a “compositional bridge,”\nenabling our model to significantly outperform much larger models and frontier systems like GPT-5.2\nand Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness\nof our approach to adversarial perturbations against option-shuffling stress tests. This work suggests\nthat grounding the reasoning process in structured knowledge is a scalable and efficient path toward\nintelligent reasoning.\n1\nIntroduction\nRecent advances in language models have revealed that reasoning capabilities can be significantly enhanced through a\ncombination of high-quality pretraining, supervised fine-tuning (SFT), carefully tuned reinforcement learning (RL)-\nbased post-training, and strategic use of additional test-time compute (OpenAI, 2025; Google DeepMind, 2025; Yang\net al., 2025; Muennighoff et al., 2025). The resulting systems achieve near-expert performance in well-structured\ndomains, such as mathematics and programming, where high-quality data have been curated, reasoning steps are\nclear, ground truth is unambiguous, and intermediate verification is tractable (Lightman et al., 2023; Anthropic, 2025).\nHowever, true human-level intelligence in specialized fields requires more than just general pattern matching or\nlong-form generation; it requires compositional reasoning: the ability to reliably combine axiomatic facts for complex\nmulti-hop problem solving Kamp & Partee (1995); Fodor (1975). While current large language models (LLMs) excel\nwhen reasoning steps are clear and carefully curated expert data are available, compositional reasoning in high-stakes\nscientific domains, where reasoning paths are multi-faceted, remains elusive (Yin et al., 2025; Kim et al., 2025).\nTo bridge this gap, we argue in favor of a bottom-up learning paradigm: grounding models in axiomatic facts and\nthen composing these fundamentals into sophisticated domain knowledge. Knowledge graphs (KGs) provide a natural\nand promising scaffold for this grounding; they encode entities and relations in a structured, interpretable fashion\nthat can represent the building blocks of domain knowledge at scale. Recent work Dedhia et al. (2025); Wang et al.\n(2024) has shown how high-quality data can be curated from such graphs and used to fine-tune models to obtain better\nreasoning traces. However, good static data is just the first step towards mastering the process of composition. Beyond\nhigh-quality data, robust reward design is a key lever for shaping models that can compose axiomatic facts from a\ndomain to arrive at a logical conclusion.\nPreprint. Under review.\narXiv:2601.15160v1  [cs.AI]  21 Jan 2026\n"}, {"page": 2, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nQuestion (3-hop) (Category: Neoplasms) (Difficulty: 2):\nA 58-year-old male presents with a 6-month history of intermittent flushing, diarrhea, and new-onset wheezing. Serum 5-\nHIAA is markedly elevated. Over the past month, he has also developed significant edema and ascites. Laboratory\ninvestigations reveal low serum albumin, IgG, IgA, and IgM levels. His C3 and C4 complement levels are normal. A CT scan\nreveals a mass in the ileum with liver metastases.\nWhich of the following mechanisms is MOST likely contributing to this patient's hypogammaglobulinemia?\nOptions:\nA. Direct bone marrow suppression by tumor metastases.\nB. Loss of immunoglobulins due to a protein-losing enteropathy secondary to chronic diarrhea and complement\nhyperactivation.\nC. Decreased immunoglobulin production secondary to tryptophan depletion by the tumor, impairing B-cell function.\nD.  Immune complex formation with vasoactive substances released by the tumor, leading to clearance of\nimmunoglobulins.\nCorrect Answer: B\nFigure 1: Compositional Reasoning: A sample 3-hop query that requires systematic traversal of axiomatic triples to\nmake a grounded, multi-step clinical deduction.\nExisting post-training methods, e.g., reinforcement learning from human feedback Ouyang et al. (2022) and direct\npreference optimization Rafailov et al. (2023), optimize models to match human preference with final outputs, not\nthe process that produced them. Proxy reward signals, such as reward length and alignment with expert-written\nanswers, while useful, fail to account for the composition intricacies needed to answer a complex multi-hop query. In\npractice, reward models often conflate superficial correlates (fluency, deference) with quality, thus leading to reward\nover-optimization and brittle answers (Shrivastava et al., 2025). In safety-critical domains, the result is a mismatch\nbetween human-liked style and ground-truth validity (Damani et al., 2025; Weng, 2024; Rafailov et al., 2023). Whereas\nprocess supervision (rewarding intermediate steps) has shown promise in mathematics and logic Zhang et al. (2025);\nCui et al. (2025); Wang et al. (2025); Lightman et al. (2023), curating and scaling expert-annotated data for other\ndomains are notoriously difficult to achieve and nontrivial. This raises a key question: How can we build systems and\nreward signals at scale that promote grounded compositional reasoning in multi-hop tasks without relying on\nexpensive human-in-the-loop annotations?\nKGs offer an implicit solution to this scaling problem. In a KG, domain-specific concepts and their relationships\nare represented as axiomatic triples (head, relation, tail). Our core insight is that by comparing the reasoning and\nassertions of a model during post-training against relevant triples and the chain of axiomatic facts required to solve the\nproblem, we can turn the match (or mismatch) into a high-quality reward signal. Instead of an answer that “looks good,”\nthis lets us reward the model to the degree its response is supported by verifiable domain knowledge and implicitly\nreward it for correctly composing facts to produce a solution. This is readily scalable without requiring external expert\nsupervision and further enables us to move away from top-down distillation and ground the model’s reasoning in the\nfield’s fundamental building blocks.\nIn this article, we realize this idea through a Base Model →SFT [Low-Rank Adaptation (LoRA)]→RL [Group\nRelative Policy Optimization (GRPO)] post-training pipeline that uses a grounded KG to derive a novel reward\nsignal to enable compositional reasoning (Hu et al., 2022; Guo et al., 2025; Yasunaga et al., 2021). Whereas the\napproach can be generally applied, we study it in the medical domain, a field that serves as a rigorous stress test for\ncompositional reasoning. Medical knowledge inherently requires multi-hop reasoning; a single clinical diagnosis may\nrequire navigating from a patient’s demographics and medical history to symptoms, from those symptoms to a disease,\nand finally to a drug (a sample multi-hop query is shown in Fig. 1). By training a Qwen3 14B model Yang et al. (2025)\n2\n"}, {"page": 3, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\non simple 1-, 2-, and 3-hop reasoning paths derived from a KG, we probe whether it can learn the underlying “logic of\ncomposition” to solve unseen, complex medical queries, ranging from 2- to 5-hop, in the ICD-Bench test suite (Dedhia\net al., 2025). Our results indicate that this grounded SFT+RL approach leads to large accuracy improvements on the\nmost difficult questions, and remains robust under stress tests, such as option shuffling and ICD-10 category breakdowns\n(Organization, 1992). We find that while SFT provides the necessary knowledge base, RL acts as the “compositional\nbridge.” We demonstrate that insights learned on an 8B model transfer effectively to a 14B model, outperforming larger\nreasoning and frontier models.\nOur core contributions can be summarized as follows:\n• A Grounded, Scalable Reinforcement Learning with Verifiable Rewards (RLVR) Pipeline: We introduce\na scalable SFT+RL post-training framework designed to enable compositional reasoning in models using KGs\nas a verifiable ground truth.\n• KG-Path Inspired Reward: We conduct a thorough investigation to design a novel reward signal derived\nfrom the KG that encourages compositional reasoning, correctness, and enables process supervision at scale.\n• Compositional Generalization: We demonstrate how training on 1-to-3-hop paths enables a model to\ngeneralize to difficult and longer 4-, 5-hop questions, significantly outperforming base models and larger\nmodels.\n• Robustness & Real-World Validation: We stratify our model’s performance by different difficulty levels, on\nreal-world medical categories (ICD-10), and its resilience against adversarial option shuffling.\n2\nRelated Work\n2.1\nRole of SFT and RL in Reasoning\nRecent studies have intensely debated the distinct contributions of SFT and RL to model performance (Jin et al., 2025;\nKang et al., 2025; Matsutani et al., 2025). The authors of Chu et al. (2025) argue that “SFT memorizes, RL generalizes,”\nclaiming that while SFT stabilizes outputs, it struggles with out-of-distribution scenarios that RL can navigate. The\nauthors of Rajani et al. (2025) characterize GRPO as a “scalpel” that amplifies existing capabilities and SFT as a\n“hammer” to overwrite prior knowledge. Our findings align with these dynamics. We use SFT to instill atomic domain\nknowledge in the model and RL to amplify compositional logic required to connect such knowledge.\nContrary to the findings in Yue et al. (2025), our results on unseen 4-, 5-hop queries demonstrate that when rewards are\ngrounded in relevant axiomatic primitives, RL can elicit novel compositional abilities beyond the baseline. This echoes\nthe findings in Yuan et al. (2025) that demonstrate that RL can teach models to compose old skills into new ones; we\nvalidate this in a high-stakes real-world domain rather than a synthetic one.\n2.2\nRL on KGs\nTraditional applications of RL on KGs, e.g., Das et al. (2017) and Xiong et al. (2017), primarily focus on traversing\ngraph structures to complete missing triples (link prediction) or find missing entities. The authors of Lin et al. (2018)\nfurther refine this approach with reward shaping to improve multi-hop reasoning, but still largely confine themselves to\nthe task of graph completion instead of open-ended question answering in a real-world setting. In more recent works, the\nauthors of Wang et al. (2024) propose “Learning to Plan,” where KGs guide the retrieval process for retrieval-augmented\ngeneration systems, and those of Yan et al. (2025) introduce RL from KG feedback to replace human feedback with KG\nsignals. While promising, these approaches often limit the role of the KG to retrieval planning or simple search tools\nfor alignment.\nOur work differs fundamentally by centrally positioning KGs as a dense process verifier for real-world multi-hop\nreasoning. The authors of Khatwani et al. (2025) use LLMs as a reward model for KG reasoning, but found the approach\nto be brittle, with poor transfer to downstream diagnostic tasks. We attribute this to the lack of a compositional training\ncurriculum; by combining the bottom-up data curation of Dedhia et al. (2025) with treating the KG as a reward model to\nderive path-aligned signals, we overcome this limitation. Furthermore, unlike Gunjal et al. (2025) that uses unstructured\nLLM-created rubrics as rewards, or rule-based Logic-RL Xie et al. (2025), we derive our signal directly from grounded\naxiomatic paths of the KG.\n3\n"}, {"page": 4, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n3\nPreliminaries\n3.1\nNotation and RL for Language Models\nWe treat an LLM as a stochastic policy πθ that maps a query q [multiple-choice question (MCQ) task] to a distribution\nover possible completions y. Each completion y comprises a reasoning trace r (chain-of-thought) Wei et al. (2022) and\na final answer ˆa (spanning A-D). Each training task q is associated with a ground-truth answer a∗and a ground-truth\nKG path P = (hi, ri, ti)L\ni=1 (see Section 4.1).\nA composite scalar reward function R(y), derived from the KG, is used to score a generated completion y (see Section\n4.4). The RL objective is to maximize the expected reward under the prompt distribution:\nJ(θ) = Eq∼DEy∼πθ(·|q)[R(y)]\nWhereas the response y is produced token-by-token, we treat the entire completion as a single trajectory for reward\nassignment, following common practice in LLM post-training.\nPolicy updates are performed using GRPO, a popular proximal policy optimization-like optimizer Schulman et al.\n(2017) that drops the critic and estimates advantages at the group level using normalization (Guo et al., 2025). See\nAppendix E for details of our hyperparameter configuration for the SFT and RL stages.\n3.2\nSFT Followed by RL\nOur training recipe follows the widely adopted Base Model →SFT →RL framework for improving LLMs. First, an\nSFT stage initializes the policy (base LLM) to produce high-quality, KG-grounded reasoning traces. A subsequent\nRL stage refines the policy directly to optimize the reward signal and enable compositional reasoning. Formally, SFT\nminimizes the negative log-likelihood on a supervised dataset consisting of question-answer (QA) tasks paired with\nreference reasoning traces and answers.\nIn our experiments, the SFT stage provides broad KG coverage, whereas the RL stage is deliberately small: a design\nchoice motivated by observed instability in an RL-from-scratch approach and because targeted RL with good reward\ncan enable compositional abilities when built atop SFT initialization (Section 4.2, Appendix G have more details).\n3.3\nMedical KG: Unified Medical Language System (UMLS)\nWe instantiate our framework on a standard biomedical KG based on UMLS Bodenreider (2004), which encodes\ncanonical medical ontology in a structured graph format. Each fact is represented as a triple (head, relation, tail)\nand multi-hop paths P = (hi, ri, ti)L\ni=1 serve as axiomatic compositional primitives used to generate QA tasks and\npath-alignment reward signals, and evaluate correctness. Full QA task generation and reward-design choices/strategies\nare described in the following section.\n4\nMethodology\nThis section describes our data construction and training pipeline. We emphasize the sequential decisions we make:\ndataset choices, SFT warm start, RL budget, reward design, and experiments that guide these choices. Our training\npipeline is designed to transition a base model from broad competence to deep, compositional medical-domain reasoning.\nFig. 2 presents an overview of our pipeline.\n4.1\nData Construction and Axiomatic Grounding\nGiven our testbed that involves compositional reasoning and to ensure our model learns true depth rather than mere\npattern matching, we adopt the data-generation and curation pipeline from Dedhia et al. (2025), which enables scalable\ngeneration of multi-hop reasoning questions grounded in a medical KG. The structured representation of medical\nconcepts, such as diseases, drugs, signs, and mechanisms, in a KG as (head, relation, tail) triples enables question\ngeneration directly from verifiable and grounded KG paths. Questions are generated in natural language in an MCQ\nformat using a backend LLM by traversing n-length paths within the KG, where n represents the number of “hops”\nrequired to link a starting node to a final node. This enables precise control over the compositional complexity of\neach query. Furthermore, each question is paired with a rich reasoning trace and a ground truth path: a sequence of\n(head, relation, tail) triples that constitutes a verifiable logical chain. The pipeline helps stratify questions by hop\nlength, difficulty, and ICD-10 category, and enforces strict separation between the training and test sets at the path and\nentity levels to avoid leakage.\n4\n"}, {"page": 5, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nTraining Phase\nShort Hops (1-3)\nInference Phase\nLong Hops (4-5)\nSFT+RL Training Phase\n1-Hop\n2-Hop\n3-Hop\n4-Hop \n5-Hop\nImproved generalization on\ndifficult, unseen 4/5 hop tasks \n(High-quality reasoning\ntraces, Compositional\nreasoning)\nBase Model\nSFT (LoRA)\nRL (GRPO)\nKG-Path Inspired +\ncorrectness reward\nsignal\nFigure 2: SFT+RL pipeline overview: Schematic of the pipeline from SFT to KG-grounded RL. While SFT enables\ndomain-specific grounding, the path-derived reward signal during RL provides the process supervision necessary for\ncompositional reasoning.\nWe generate a training set of 24,660 QA tasks designed to ensure maximum node coverage across the KG (Yasunaga\net al., 2021). For evaluation, we use ICD-Bench, a non-overlapping test set of 3,675 questions (Dedhia et al., 2025).\nImportantly, the training set consists of 1-3 hop paths, whereas ICD-Bench includes 2-5 hop path tasks across 15\nICD-10 categories to test zero-shot compositional generalization at different difficulty levels [1 (very easy) - 5 (very\nhard)]. More analysis of overlaps between the training and test sets is presented in Appendix D.\nOur training pipeline consists of three stages: Base Model →SFT (LoRA) →RL (GRPO). This design reflects our\ncentral hypothesis that compositional reasoning emerges most reliably when models are first grounded in rich reasoning\ntraces via supervised learning and then tuned using scalable process-aligned rewards derived from the KG paths. We\nemphasize that all training data and rewards are derived from the same KG to ensure consistency between training and\nevaluation.\n4.2\nRL Alone is Insufficient\nA core finding of our work is that the Zero-RL approach, applying GRPO directly to the base LLM, is insufficient for\ndeep domain expertise at our model scale. The model requires an understanding of the domain axioms before it can\nlearn to compose. Starting from the base model (Qwen3 8B), we apply GRPO directly using subsets of training data:\n5k, 10k, and all 24.66k examples. Across these settings, we find that while RL improves performance relative to the\nbase model, it does not consistently outperform SFT-only training on the base model. Interestingly, the budget of 5k\nexamples yields the strongest results among all other settings, suggesting how large-scale vanilla RL without proper\ngrounding is insufficient for compositional behavior, motivating the use of SFT for an initial warm start.\nBased on these observations, we use 5k examples in the RL stage and the remaining 19.66k in the SFT stage. The base\nmodel is fine-tuned using LoRA on 19.66k examples, followed by GRPO on the remaining 5k examples as per the\nSFT+RL pipeline.\nSee Appendix A, B for detailed ablations with the Zero-RL and SFT+RL pipelines on the Qwen3 8B model, respectively.\nSee Appendix H for our GRPO training prompt.\n5\n"}, {"page": 6, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n4.3\nReward Design Exploration for Compositional Reasoning\nA central goal and a novel contribution of this work is to design scalable reward signals during training that enable\ncompositional reasoning beyond surface-level understanding. Towards this end, we conduct extensive ablation studies\nusing a combination of four distinct reward signals to determine which best fosters verifiable composition:\n• Binary Correctness (Rbin): A simple outcome-based signal that rewards the final answer.\n• Similarity (Rsim): A distillation-based reward that measures the Jaccard similarity between the model output\nand an expert reasoning trace (generated by Gemini 2.5 Pro during data curation).\n• Thinking Quality (Rthink): A reward designed to score the thinking quality and length of the generation.\n• Path Alignment (Rpath): A novel reward that scores the model based on the coverage of the ground-truth\nKG triples in the model response.\nWe conduct a systematic exploration by evaluating a combination of these rewards, always including Rbin (+1 for\ncorrectness, 0 otherwise) as a minimal signal. Empirically, we discover that Rthink is often unstable and leads to reward\nhacking, generating inefficacious chains. Rsim also proved sub-optimal, suggesting distillation rewards over-optimize\naesthetic mimicry rather than true logical composition.\nOur findings highlight the power of simplicity: The combination of path alignment and binary correctness provides the\nstrongest signal for composition. Whereas Rbin optimizes correctness, Rpath rewards the model for identifying and\napplying the axiomatic facts (triples) required to compose the correct solution. To further strengthen the outcome signal,\nwe replace the simple Rbin with negative sampling reinforcement Zhu et al. (2025), which penalizes incorrect generations\nby upweighting the negative reward, thereby encouraging the model to explore alternative/correct trajectories.\n4.4\nKG-Grounded Reward Formulation\nTo provide a robust reward signal to enable composition, we develop a composite reward that balances outcome\ncorrectness with path-level alignment grounded in the KG. Let the model generate a response y for a question q with\nthe reasoning trace r and a final answer ˆa. For each QA task, there is a ground truth answer a∗and a ground-truth KG\npath P = (hi, ri, ti)L\ni=1, where L is the path length. The total reward is then a combination of the two rewards:\nRtotal(y) = Rbin(ˆa, a∗) + Rpath(r, P)\nBinary Correctness Reward: This provides a minimal but necessary supervision signal on the final answer.\nRbin(ˆa, a∗) =\n\u001aα,\nif ˆa = a∗\n−β,\notherwise\nwhere α, β > 0 and β > α. This asymmetric design ensures stable learning by reinforcing exploration of correct\nalternate paths. We use α = 0.1, β = 1 in accordance with investigations by (Zhu et al., 2025).\nPath Alignment Reward (KG-grounded): The primary technical innovation is Rpath, which provides automated and\nscalable process supervision by evaluating whether the reasoning trace of the model aligns with the ground-truth KG\npath P during RL post-training. We first tokenize and normalize the reasoning trace r to extract a set of textual tokens,\nT(r). We derive a corresponding set of path tokens, T(P), from the ground-truth path P, representing the entities in P.\nThe core signal is path coverage:\ncoverage(r, P) = | T(r) ∩T(P) |\n| T(P) |\nWe include a minimum-hit constraint that requires alignment with at least two distinct path entities to discourage trivial\nmatches and promote logical composition. We apply an additional repetition penalty to reduce reward hacking and\navoid linguistic collapse. The final reward is defined as:\nRpath(r, P) = min(γ1 · coverage(r, P)\n+ γ2 · I(|T(r) ∩T(P)| ≥2), Rmax),\nscaled by a repetition penalty factor, ϕrep, and clipped to a fixed maximum. We use γ1 = 1.2, γ2 = 0.3, Rmax = 1.5.\nThe total reward, Rtotal, is process-level, grounded, and compositional. It is readily scalable and easily verifiable as a\nresult of the grounding in the KG. Furthermore, unlike similarity-based distillation or AI-based rubric rewards Gunjal\net al. (2025), alignment is scored against true domain structure rather than stylistic mimicry. We present a formal\nformulation of Rsim and Rthink in Appendix C.\n6\n"}, {"page": 7, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n4.5\nScaling and Benchmarking\nWe evaluate our pipeline on the Qwen3 8B model before scaling the findings to the 14B variant without modification.\nThe 14B model trained on our pipeline not only generalizes to 2- and 3-hop tasks, but also to 4-and 5-hop tasks with\nremarkable efficacy, surpassing much larger frontier models. In addition to overall accuracy, we stratify performance by\nhop length, difficulty level, ICD-10 category, and robustness under the option-shuffling stress test.\n5\nResults\nSetup: We initially evaluate three systems: Base Qwen3 14B model, model trained using LoRA on the full training set\n(24,660 QA tasks), and our proposed SFT+RL pipeline (SFT on 19,660 tasks followed by GRPO on the remaining\n5k tasks) based on leveraging the KG-derived reward. We do not report results for models trained using the Zero-RL\napproach, described in Section 4.2, since all of them performed worse than SFT on the full dataset (see Appendix A for\ndetails). All evaluations use the held-out test ICD-Bench test set (3,675 tasks) to validate whether path-derived signals\ntruly enable compositional reasoning. Finally, we evaluate our SFT+RL model against larger frontier and reasoning\nmodels. See Appendix F for sample model responses of our final Qwen3 14B SFT+RL model.\n5.1\nScaling Composition: From Short-Hop Training to Long-Hop Reasoning\nThe primary claim of this work is that KGs function as implicit reward models, and grounding the model reasoning in\npath-derived signals enables it to learn the underlying logic of composition rather than regurgitating information seen\nduring training.\n2\n3\n4\n5\nNumber of Reasoning Hops\n60\n65\n70\n75\n80\n85\n90\n95\nAccuracy (%)\n+11.1%\nGeneralization\n(unseen complexity)\nBase Model\nSFT Only\nSFT+RL\nFigure 3: Accuracy by Hop Length: Our SFT+RL model not only outperforms baselines on 2-3 hop tasks but exhibits\na positive performance gradient on unseen 4-, 5-hop reasoning tasks, validating the “compositional bridge” enabled by\npath-aligned rewards.\nPath-derived Signals Enable Compositional Reasoning: Whereas the model was exposed to 1-, 2-, and 3-hop paths\nduring the SFT+RL training phase, it remained totally naive to tasks involving 4-, 5-hop reasoning. As shown in Fig. 3,\nthe SFT+RL model demonstrates substantially stronger generalization to longer paths, achieving a notable gain of\n7.5% on unseen 4-hop and 11.1% on unseen 5-hop questions relative to the SFT-only approach. This improvement\nis not attributable to exposure to longer chains, since the training and evaluation distributions are identical across\nall models. Instead, it reflects the effect of path-derived signals introduced during RL. By rewarding assertions that\nalign the model directly with the ground-truth KG path, the model learns the logic of composition. Importantly, the\ngeneralization gap between the SFT-only and SFT+RL approaches widens as hop-length increases. This is a hallmark of\n7\n"}, {"page": 8, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\ngenuine compositional learning. The KG-derived reward signal (Rpath) enables the model to decompose long-horizon\nreasoning into verifiable steps and compose reasoning beyond the complexity observed during training.\n5.2\nRobustness to Tasks Involving Reasoning Depth\nA significant challenge in medical reasoning is maintaining integrity as the complexity of tasks increases. Aggregate\nperformance metrics often obscure model failure on long-tail high-difficulty scenarios, where deep reasoning is\nparamount. We assign difficulty ratings from 1 (very easy) to 5 (very hard) Dedhia et al. (2025) to each ICD-Bench\nquestion and assess model performance as a function of question difficulty.\n1\n(Very Easy)\n2\n(Easy)\n3\n(Medium)\n4\n(Hard)\n5\n(Very Hard)\nDifficulty Level\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nBase Model\nSFT Only\nSFT+RL\nFigure 4: Accuracy by Difficulty Level: Whereas the Base Model’s reasoning collapses as task complexity increases,\nthe SFT+RL pipeline exhibits robustness, maintaining a consistent lead over the SFT-only baseline across all levels.\nDominance in High-Complexity Tasks: The results shown in Fig. 4 demonstrate that the SFT+RL pipeline, grounded\nin path-derived signals, provides the most significant gains as task complexity increases. On Level-5 tasks, the base\nmodel accuracy collapses to 19.94%, indicating worse-than-random-guess accuracy in complex clinical scenarios.\nWhereas the SFT-only approach improves this to 48.93%, our SFT+RL model achieves 56.75%, nearly tripling the base\nmodel performance.\nConsistency across all Difficulty Levels: On Level-1 tasks, our model reaches a near-ceiling accuracy (93.49%)\nand the performance gap remains robust across difficulty levels, with the SFT+RL model consistently maintaining a\n7-10% lead over the SFT-only model. This demonstrates that by using the KG as an implicit reward model, we raise\nthe performance floor for the most complex queries. On Level-5 tasks, where even large frontier models struggle,\nmaintaining over 56% accuracy is a testament to how path-aligned rewards do more than just stylistic inference; they\nenable the model to reliably compose multi-step chains.\n5.3\nICD-10 Category Analysis: KG-grounded Gains Are Broadly Distributed\nTo connect our empirical evaluation to real-world, clinically meaningful structures, we follow the categorization\nintroduced in Dedhia et al. (2025) that groups ICD-Bench questions into 15 ICD-10 categories in the data curation step.\nThis classification lets us evaluate the breadth of our approach, enabling us to measure whether improvements of the\nSFT+RL model are concentrated in a few medical sub-domains or distributed across practical specialties.\nThe analysis, visualized in Fig. 5, reveals that our SFT+RL pipeline consistently achieves the highest accuracy and\nmaintains a substantial lead over the SFT-only baseline across all medical categories. Some of the most significant\nimprovements occur in high-stakes areas such as “Blood and Immune System Diseases” and “Circulatory System\nDiseases,” where diagnostic reasoning often requires complex, multi-hop evidence composition. This is consistent with\nour design choice to derive the reward from KG paths; the path-alignment reward can assign a meaningful intermediate\nsignal and better shape compositional behavior.\n8\n"}, {"page": 9, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n0\n5\n10\n15\n20\n25\nImprovement over Base Model (%)\nMental Health\nNervous\nDigestive\nEndocrine\nSkin\nRespiratory\nInfectious\nBlood/Immune\nMusculoskeletal\nEye\nPharmacology\nCirculatory\nNeoplasms\nCongenital\nEar\nSFT Only\nSFT+RL (Ours)\nFigure 5: Accuracy by ICD-10 Category: Path-aligned rewards consistently improve performance across all 15\nmedical sub-domains.\n5.4\nRobustness to Format Perturbation\nA common failure mode for LLMs is reliance on superficial cues, e.g., the order of options and answer in a multiple-\nchoice list, rather than the true logical content and chain of reasoning. To evaluate the robustness of our models against\nsuch positional bias, we subject them to Stress-Test 3 (option shuffling) Gu et al. (2025). In this test, the order of\nincorrect distractor options is randomized while keeping the correct answer choice constant.\nTable 1: Analysis of Option Format Perturbation.\nMETHOD\nSTANDARD\nSHUFFLED\nDELTA (∆)\nSFT-ONLY\n75.95%\n74.91%\n−1.04%\nSFT+RL (OURS)\n83.62%\n82.45%\n−1.17%\nThe results in Table 1 demonstrate a remarkable degree of robustness of our approach compared to frontier models\nanalyzed in the literature. Whereas leading systems, such as GPT-5 and Gemini-2.5 Pro, have been shown to suffer\nperformance drops of 4-6% under similar perturbations (in text-only medical contexts) Gu et al. (2025), our models\nmaintain nearly stable performance with a negligible drop of ∼1%.\nImportance of High-Quality Data: This resilience highlights a fundamental axiom of machine learning: use of\nhigh-quality, grounded data is paramount. Our data curation and training pipeline incentivizes the model to identify\nthe correct answer based on a verifiable reasoning path rather than shortcut patterns seen during unstructured training.\nThe fact that even the SFT-only model trained on high-quality traces exhibits such stability suggests that grounding the\nmodel in structured domain axiomatic knowledge is as critical as subsequent RL optimization. By grounding the model\n9\n"}, {"page": 10, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nand ensuring it learns true logical composition, we move closer to systems capable of genuine domain competence\nrather than “illusion of readiness.”\n5.5\nAlgorithmic Efficiency vs. Scale: Surpassing Frontier Models\nOne of our central claims is that careful reward design and bottom-up data curation can enable compositional reasoning\nthat outperforms top-down brute-force scaling. To validate this, we compare our 14B SFT+RL model against two\ndistinct classes of benchmarks: (1) large frontier models (GPT-5.2, Gemini 3 Pro) that represent the ceiling of generalist\nzero-shot reasoning, and (2) QwQ-Med-3 (32B), a domain-expert model distilled for medical reasoning.\n2\n3\n4\n5\nNumber of Reasoning Hops\n65\n70\n75\n80\n85\n90\n95\nAccuracy (%)\nSimpler\nComplex\n+7%\n+7%\n+10%\n+19%\n14B params vs larger frontier models\nGPT-5.2\nGemini 3 Pro\nSFT+RL (14B, Ours)\nFigure 6: Accuracy Comparisons against Frontier Models by Hop Level: Whereas the accuracy of generalist giants\ndecays on longer chains, our 14B SFT+RL model achieves its highest accuracy on unseen 5-hop queries, validating the\nKG as a superior supervisor for complex composition.\nSuperiority over Frontier Models: We evaluate the zero-shot performance of the model against leading generalist\nmodels in Fig. 6. Our results demonstrate that grounding a smaller model in a domain’s axioms enables it to surpass\nlarger, generally trained giants at complex reasoning tasks. We also note a striking trend: Whereas GPT-5.2 and Gemini\n3 Pro maintain respectable performance on shorter hops, their accuracy stagnates or declines as the hop count increases.\nIn contrast, our model exhibits a positive compositional gradient and achieves its highest accuracy (89.33%) on the\n5-hop queries. This supports our claim that KG-grounded path-derived signals teach the model how to compose axioms,\nnot just standard pattern matching.\nSurpassing Expert-distilled Scale (SFT-Only): Finally, we compare our model with the QwQ-Med-3 model presented\nin Dedhia et al. (2025), which was trained on a similar data distribution. For a fair comparison, we use the majority-\nvoting (n = 16) metric, matching the aggregation metric used in their study (see Table 2).\nWhereas the QwQ-Med-3 model holds a slight advantage on lower-difficulty tasks, which typically rely on factual\nrecall, our model successfully bridges the recall-reasoning gap. By using the KG as an implicit reward model, we\nenable a smaller architecture to “out-reason” a much larger model. This confirms our premise that while scale is a\npowerful tool for breadth of knowledge, path-aligned rewards are the true bridge to deep compositional reasoning.\n10\n"}, {"page": 11, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nTable 2: Performance by difficulty level (using majority voting).\nDIFF.\nQWQ-MED-3 32B\nOURS-14B\n∆\n1\n96.75%\n94.23%\n−2.52%\n2\n83.79%\n85.63%\n+1.84%\n3\n79.33%\n80.33%\n+1.00%\n4\n70.56%\n71.50%\n+0.94%\n5\n49.69%\n59.05%\n+9.36%\n6\nDiscussion\nThe ideas and results presented in this work highlight a fundamental shift in how we approach the development of\nexpert-level reasoning. By using a KG as an implicit reward model, we demonstrate how bottom-up primitives in a\ndomain can serve as highly scalable, verifiable process supervisors. Unlike contemporary human-in-the-loop process\nsupervision, which is prohibitively expensive and infeasible to scale to millions of reasoning chains across domains, our\npath-derived reward signal provides an automated, scalable axiomatic grounding mechanism.\nFurthermore, our findings reinforce the basic axiom of machine learning: Good data are paramount! We show that\nwhen models are trained with proper axiomatic grounding, they develop a robust skill for compositional reasoning\nbeyond simple factual recall. This is most evident by our model’s ability to surpass much larger generalist giants. While\nbrute-force scaling continues to dominate the search for general intelligence, our work suggests a more efficient path\ntowards building superintelligent systems: building small, specialized models that master composition within their\nrespective domains.\nFinally, our reward design mechanism is inherently scalable and domain-agnostic. Any scientific or technical field\nthat can be represented as a structured KG (from organic chemistry to case law) is a candidate for this pipeline. As\ndomain KGs continue to expand in coverage and fidelity, they offer a practical route to building systems that reason\nfrom first principles rather than surface correlations or simple pattern matching. We posit this work as an early step\ntoward scalable, verifiable domain-specific superintelligence, and we encourage future research to explore richer graph\nstructures, broader domains, and tighter integration between symbolic knowledge and neural architectures to build\nbetter reasoning systems.\n7\nConclusion\nWe introduced a simple, general idea: Treat KGs as implicit reward models and use path-derived rewards in a scalable\nfashion to teach models how to compose domain primitives into longer reasoning chains. We combined SFT with\na compact RL stage and a KG path-aligned reward (plus correctness) to synthesize models that can generalize from\n1-3-hop training to unseen 4-5-hop problems, improve baseline accuracy on the hardest tasks, and remain robust under\nformat perturbations. Our recipe yields consistent gains over an SFT-only approach and can outperform much larger\nmodels, validating that good grounded data and reward design, not just scale, are central to compositional reasoning.\nAlthough we have successfully validated our claims within the medical domain, we acknowledge that this is only a\nstarting point. We encourage the community to further investigate how structured KGs can be leveraged as reward\nsupervisors to build the next generation of superintelligent systems.\nAcknowledgments\nThe experiments reported in this paper were performed on the computational resources managed and supported by\nPrinceton Research Computing, Princeton AI Lab, and the Princeton Language and Intelligence Initiative at Princeton\nUniversity.\n11\n"}, {"page": 12, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nReferences\nAnthropic. Claude Opus 4.5 System Card Technical Report, 2025. URL https://assets.anthropic.com/m/\n64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf.\nBodenreider, O. The Unified Medical Language System (UMLS): Integrating Biomedical Terminology. Nucleic acids\nresearch, 32:D267–D270, 2004.\nChu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. SFT Memorizes, RL\nGeneralizes: A Comparative Study of Foundation Model Post-Training. arXiv preprint arXiv:2501.17161, 2025.\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang,\nK., Lv, X., Wang, S., Yao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process\nReinforcement Through Implicit Rewards. arXiv preprint arXiv:2502.01456, 2025.\nDamani, M., Puri, I., Slocum, S., Shenfeld, I., Choshen, L., Kim, Y., and Andreas, J. Beyond Binary Rewards: Training\nLMs to Reason About Their Uncertainty. arXiv preprint arXiv:2507.16806, 2025.\nDas, R., Dhuliawala, S., Zaheer, M., Vilnis, L., Durugkar, I., Krishnamurthy, A., Smola, A., and McCallum, A. Go for a\nWalk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases Using Reinforcement Learning. arXiv\npreprint arXiv:1711.05851, 2017.\nDedhia, B., Kansal, Y., and Jha, N. K. Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is\nWhat We Need. arXiv preprint arXiv:2507.13966, 7 2025.\nFodor, J. A. The Language of Thought, volume 5. Harvard University Press, 1975.\nGoogle DeepMind. Gemini 3 Pro Model Card, November 2025. URL https://storage.googleapis.com/\ndeepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf. Technical Report.\nGu, Y., Fu, J., Liu, X., Valanarasu, J. M. J., Codella, N. C., Tan, R., Liu, Q., Jin, Y., Zhang, S., Wang, J., et al. The\nIllusion of Readiness in Health AI. arXiv preprint arXiv:2509.18234, 2025.\nGunjal, A., Wang, A., Lau, E., Nath, V., Liu, B., and Hendryx, S. Rubrics as Rewards: Reinforcement Learning Beyond\nVerifiable Domains. arXiv preprint arXiv:2507.17746, 2025.\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-R1:\nIncentivizing Reasoning Capability in LLMs Via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. LoRA: Low-Rank Adaptation\nof Large Language Models. In Proceedings of International Conference on Learning Representations, 1(2):3, 2022.\nJin, H., Luan, S., Lyu, S., Rabusseau, G., Rabbany, R., Precup, D., and Hamdaqa, M. RL Fine-Tuning Heals OOD\nForgetting In SFT. arXiv preprint arXiv:2509.12235, 2025.\nKamp, H. and Partee, B. Prototype Theory and Compositionality. Cognition, 57(2):129–191, 1995.\nKang, F., Kuchnik, M., Padthe, K., Vlastelica, M., Jia, R., Wu, C.-J., and Ardalani, N. Quagmires in SFT-RL\nPost-Training: When High SFT Scores Mislead and What to Use Instead. arXiv preprint arXiv:2510.01624, 2025.\nKhatwani, S., Cheng, H., Afshar, M., Dligach, D., and Gao, Y. Brittleness and Promise: Knowledge Graph Based\nReward Modeling for Diagnostic Reasoning. arXiv preprint arXiv:2509.18316, 2025.\nKim, Y., Jeong, H., Chen, S., Li, S. S., Lu, M., Alhamoud, K., Mun, J., Grau, C., Jung, M., Gameiro, R., Fan, L., Park,\nE., Lin, T., Yoon, J., Yoon, W., Sap, M., Tsvetkov, Y., Liang, P., Xu, X., Liu, X., McDuff, D., Lee, H., Park, H. W.,\nTulebaev, S., and Breazeal, C. Medical Hallucination in Foundation Models and Their Impact on Healthcare. arXiv\npreprint arXiv:2503.05777, 2025.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe,\nK. Let’s Verify Step by Step. arXiv preprint arXiv:2305.20050, 2023.\nLin, X. V., Socher, R., and Xiong, C. Multi-Hop Knowledge Graph Reasoning With Reward Shaping. arXiv preprint\narXiv:1808.10568, 2018.\nMatsutani, K., Takashiro, S., Minegishi, G., Kojima, T., Iwasawa, Y., and Matsuo, Y. RL Squeezes, SFT Expands: A\nComparative Study of Reasoning LLMs. arXiv preprint arXiv:2509.21128, 2025.\nMuennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and\nHashimoto, T. B. s1: Simple Test-Time Scaling. In Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, pp. 20286–20332, 2025.\nOpenAI. Introducing GPT-5.2. https://openai.com/index/introducing-gpt-5-2/, 2025. Accessed: 2026.\nOrganization, W. H. International Statistical Classification of Diseases and Related Health Problems 10th Revision\n(ICD-10). World Health Organization, 1992.\n12\n"}, {"page": 13, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training Language Models to Follow Instructions with Human Feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct Preference Optimization: Your\nLanguage Model is Secretly a Reward Model. Advances in Neural Information Processing Systems, 36, 2023.\nRajani, N., Gema, A. P., Goldfarb-Tarrant, S., and Titov, I. Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities,\nSFT Replaces Them. arXiv preprint, 2025. Preprint.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal Policy Optimization Algorithms. arXiv\npreprint arXiv:1707.06347, 2017.\nShrivastava, V., Awadallah, A., Balachandran, V., Garg, S., Behl, H., and Papailiopoulos, D. Sample More to Think\nLess: Group Filtered Policy Optimization for Concise Reasoning. arXiv preprint arXiv:2508.09726, 2025.\nWang, G., Li, J., Sun, Y., Chen, X., Liu, C., Wu, Y., Lu, M., Song, S., and Abbasi-Yadkori, Y. Hierarchical Reasoning\nModel. arXiv preprint arXiv:2506.21734, 2025.\nWang, J., Chen, M., Hu, B., Yang, D., Liu, Z., Shen, Y., Wei, P., Zhang, Z., Gu, J., Zhou, J., Pan, J. Z., Zhang, W., and\nChen, H. Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs. In Findings\nof the Association for Computational Linguistics: EMNLP 2024, pp. 7813–7835. Association for Computational\nLinguistics, 2024.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. Advances in Neural Information Processing Systems, 35:24824–24837,\n2022.\nWeng,\nL.\nReward Hacking in Reinforcement Learning.\nhttps://lilianweng.github.io/posts/\n2024-11-28-reward-hacking/, 2024. Lil’Log blog post.\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-RL: Unleashing\nLLM Reasoning with Rule-Based Reinforcement Learning. arXiv preprint arXiv:2502.14768, 2025.\nXiong, W., Hoang, T., and Wang, W. Y. DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning.\narXiv preprint arXiv:1707.06690, 2017.\nYan, L., Tang, C., Guan, Y., Wang, H., Wang, S., Liu, H., Yang, Y., and Jiang, J. RLKGF: Reinforcement Learning\nfrom Knowledge Graph Feedback without Human Annotations. In Findings of the Association for Computational\nLinguistics: ACL 2025, pp. 6619–6633, 2025.\nYang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 Technical\nReport. arXiv preprint arXiv:2505.09388, 2025.\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. QA-GNN: Reasoning with Language Models and\nKnowledge Graphs for Question Answering. arXiv preprint arXiv:2104.06378, 2021.\nYin, M., Qu, Y., Yang, L., Cong, L., and Wang, M. Toward Scientific Reasoning in LLMs: Training from Expert\nDiscussions via Reinforcement Learning. arXiv preprint arXiv:2505.19501, 2025.\nYuan, L., Chen, W., Zhang, Y., Cui, G., Wang, H., You, Z., Ding, N., Liu, Z., Sun, M., and Peng, H. From f(x) and\ng(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones. arXiv preprint arXiv:2509.25123, 2025.\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does Reinforcement Learning Really Incentivize\nReasoning Capacity in LLMs Beyond the Base Model? arXiv preprint arXiv:2504.13837, 2025.\nZhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The Lessons of Developing\nProcess Reward Models in Mathematical Reasoning. arXiv preprint arXiv:2501.07301, 2025.\nZhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The Surprising Effectiveness of Negative Reinforcement\nin LLM Reasoning. arXiv preprint arXiv:2506.01347, 2025.\n13\n"}, {"page": 14, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nA\nZero-RL and Reward Ablation Studies on Qwen3 8B\nIn this section, we provide the empirical details of our experiments with the “Zero-RL” approach (applying RL directly\nto the base model without a prior SFT phase). These ablations were instrumental in determining our final training scale,\npipeline design, and reward configuration.\nA.1\nZero-RL Performance and Scaling\nWe evaluate the performance of the Qwen3 8B base model using the GRPO algorithm across three data scales: 5k,\n10k, and 24.66k examples. For each scale, we test four reward configurations: (1) path alignment (path overlap), (2)\nJaccard similarity (distillation), (3) binary correctness, and (4) all rewards combined (see Table 3). All experiments are\nperformed on the Qwen3 8B model using 8 H100 NVIDIA GPUs.\nNote that we always include the binary correctness reward as a minimal signal.\nTable 3: Ablation study on reward components across training scales on Qwen3 8B.\nTRAINING SIZE\nREWARD\nACCURACY\n∼TRAINING TIME (HRS)\n0\nBASELINE\n64.98%\n–\n5K\nPATH ALIGN.\n67.51%\n12\nJACCARD SIM.\n68.03%\n12\nBINARY\n69.36%\n12\n10K\nPATH ALIGN.\n64.24%\n24\nJACCARD SIM.\n65.74%\n24\nBINARY\n67.56%\n24\nALL REWARDS\n68.44%\n24\n24.66K\nPATH ALIGN.\n68.41%\n65\nJACCARD SIM.\n65.77%\n65\nBINARY\n70.18%\n65\nALL REWARDS\n68.44%\n65\nKey Observations:\n• SFT Requirement: Whereas Zero-RL provides a marginal improvement over the base model, no configuration\nreaches the performance levels of the SFT-only baseline (70.86%) or our final SFT+RL model (∼82%). This\nconfirms that the model requires a grounded factual foundation from SFT before it can effectively leverage\nRL for compositional reasoning. In the final SFT+RL pipeline, path alignment, along with binary correctness\n(with upweighted negative reward), provides the best and most reliable signal.\n• Scale vs. Efficiency: Performance gains from 5k to 24.66k examples are minimal (e.g., a ∼0.8% gain for\nbinary correctness reward) while significantly increasing training time from 12 to 65 hours. This justifies our\ndecision to set the RL stage to use a high-quality 5k subset of our final pipeline.\n• Reward Stability: The binary correctness reward remains the most stable signal in the Zero-SFT setting.\nThe distillation-based rewards are prone to reward hacking and stylistic mimicry without providing superior\nreasoning abilities.\nA.2\nGRPO on LoRA Parameters Only\nWe also explore a variant of the pipeline where we perform SFT followed by GRPO, with updates limited only to the\nLoRA modules. This configuration yields an accuracy of 66.75% on the 8B model. The significantly lower performance\ncompared to our full SFT+RL results suggests that compositional reasoning requires more extensive/aggressive weight\nupdates during RL than those afforded by the restricted LoRA-only approach.\n14\n"}, {"page": 15, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nB\nSFT+RL Ablation Studies on Qwen3 8B\nThis study investigates the synergy between the binary reward and path alignment reward, and provides the final missing\nlink by ablating the specific reward components used in the RL stage. Whereas Appendix A focuses on the Zero-RL\nsettings, the following experiments demonstrate the impact of reward designs on the model that has already undergone\nSFT. We conduct these ablations on the Qwen3 8B model to efficiently identify the optimal configuration for the larger\n14B SFT+RL pipeline.\nWe specifically evaluate the addition of negative reinforcement Zhu et al. (2025) by penalizing the model more for\nincorrect final answers than correct reasoning paths. As shown in Table 4, the combination of path-derived signals and\nnegative binary reward provides the most robust reasoning improvements, significantly outperforming configurations\nthat rely on binary signals alone or a combination of all rewards.\nTable 4: Ablation study on reward components with the SFT+RL training pipeline (8B Model). All RL runs are\nconducted for 5k steps following a 19.66k SFT baseline.\nTRAINING SETUP\nREWARD CONFIGURATION\nACCURACY (%)\nSFT BASELINE\n-\n70.86%\n19.66K SFT + 5K RL\nPATH ALIGNMENT ONLY\n79.29%\nPATH ALIGNMENT + BINARY\n68.03%\nBINARY ONLY (NORMAL)\n79.46%\nNEGATIVE BINARY ONLY\n79.54%\nALL REWARDS\n55.21%\nPATH ALIGN. + NEGATIVE BINARY\n82.20%\nKey Insights:\n• The Synergy of Negative Reinforcement: Consistent with the findings of Zhu et al. (2025), we observe that\n“Normal” binary rewards (positive reinforcement only) can be unstable when combined with path-aligned\nrewards. Transitioning to negative binary rewards while rewarding valid intermediate paths yields our highest\naccuracy of 82.20%.\n• The “All Rewards” Failure: Attempting to combine all reward signals (path alignment, Jaccard similarity,\nnormal binary, and thinking quality) leads to a significant performance collapse (55.21%). This suggests that\nover-optimizing the reward signal can lead to conflicting gradients or reward hacking, where the model fails to\noptimize for the primary reasoning task.\n• Effect of the Path Alignment Signal: Even without outcome-based binary rewards, path alignment only\nprovides a substantial ∼9% boost over the SFT baseline. This confirms that the KG itself acts as a sufficient\nimplicit reward model to guide the model towards better compositional reasoning.\n15\n"}, {"page": 16, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nC\nAdditional Reward Function Formulations\nIn addition to the path-alignment reward, Rpath, and the binary correctness reward, Rbin, we investigate two alternative\nreward functions for RL experiments: semantic answer similarity (distillation-based reward) (Rsim) and thinking quality\n(Rthink). Whereas these functions provided some learning signals, they are ultimately less effective than path-derived\nrewards in enabling deep compositional reasoning.\nC.1\nSemantic Answer Similarity Reward (Rsim)\nThe semantic answer similarity (or distillation) reward, Rsim, is designed to distill the reasoning patterns of the SFT\nmodel by rewarding semantic overlap between the model-generated reasoning and the ground truth answer reasoning\ntrace.\nThe reward is calculated using the Jaccard similarity of normalized token sets:\nRsim = |Tmodel ∩Ttarget|\n|Tmodel ∪Ttarget| × ϕrep,\nwhere Tmodel is the set of unique tokens from the thinking trace of the model, Ttarget is the token set of the ground\ntruth reasoning text (distilled from a larger LLM), and ϕrep is a repetition penalty factor used to discourage repetitive\ngeneration. This function encourages the model to mention the correct clinical entities and mechanisms associated with\nthe target answer, though it does not explicitly verify the logical validity of the connections between those entities.\nC.2\nThinking Quality Reward (Rthink)\nThe thinking quality reward, Rthink, is designed to encourage the model to produce well-organized, stepwise reasoning.\nIt evaluates the response based on its adherence to logical formatting cues. It is calculated as a weighted sum of three\nstructural components:\n• Structure (50%): A binary indicator of whether the reasoning trace exceeds a minimum length threshold (20\ncharacters).\n• Step-wise Keywords (30%): A normalized count of logical-thinking indicator words such as “first,” “then,”\n“therefore,” and “finally.”\n• Enumeration (20%): A score based on the presence of numbered lists (e.g., “1.,” “2.”), which often indicate\na systematic breakdown of a multi-hop clinical diagnostic problem.\nIn addition, we employ a penalty, ϕrep, to discourage repetitive output. Whereas Rthink shows initial promise in\nimproving the readability of the model outputs, it often leads to reward hacking in contrast to the rigorous logical\ntraversal induced by path-derived reward signals.\n16\n"}, {"page": 17, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nD\nTrain-Test Split Overlap Analysis\nA critical concern in any reasoning-based benchmark is the potential for data leakage or memorization. As per the\ndesign of our data curation pipeline, which builds upon the methodology established in Dedhia et al. (2025), each\nquestion created by traversing a KG path is unique. Per design, arriving at a correct answer requires explicit reasoning\ninvolving all intermediate edges and nodes in the path, rather than simple pattern matching.\nWe intentionally allow partial overlap of individual nodes or single triples between the training and test sets to ensure\nthe benchmark exercises broad coverage of medical concepts. To more concretely address concerns regarding the\ninfluence of partial knowledge overlap, we performed a rigorous analysis of our model performance on the test set\ncategorized by the longest consecutive matching subsequence of triples found in any training question. This match can\nstart at any position in either the training or test path. The distribution and corresponding accuracy metrics are detailed\nin Figs. 7, 8, respectively.\n0\n1\n2\n3\nMax Triple Overlap with Any Training Question\n0\n500\n1000\n1500\n2000\n2500\nNumber of Test Questions\n478\n(13.0%)\n2719\n(74.0%)\n441\n(12.0%)\n37\n(1.0%)\nFigure 7: Number of KG Triples shared with the Train-\ning Set\n0\n1\n2\n3\nMax Triple Overlap with Any Training Question\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n82.0%\n(n=478)\n83.8%\n(n=2719)\n84.6%\n(n=441)\n78.4%\n(n=37)\nOverall Accuracy: 83.6%\nFigure 8: Accuracy by Number of Overlapping KG\nTriples\nOnly 1.0% of test questions have a full 3-triple chain match. Since our training set is restricted to a maximum of 3-hop\nreasoning tasks, this ensures that the vast majority of our benchmark consists of structurally unique reasoning tasks.\nFurthermore, there is no clear trend in accuracy as overlap increases. This suggests that performance is not significantly\npredicted by the presence of familiar triple sequences. Interestingly, the 3-overlap category exhibits slightly lower\naccuracy (78.4%) than the 0-overlap category (82.0%). This further demonstrates that our model internalizes the logic\nof composition rather than memorizing specific paths.\nWhereas the majority of questions share at least one triple with the training set, note that the ICD-Bench test set contains\nonly 2-5-hop questions. This requires the model to perform novel composition even when individual building blocks\nhave been previously seen in different contexts.\n17\n"}, {"page": 18, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nE\nTraining Hyperparameters and GRPO Configuration\nIn this section, we detail the specific hyperparameter configurations used for both the SFT (Table 5) and RL (GRPO)\n(Table 6) stages of our pipeline.\nE.1\nHardware Compute\nAll experiments are conducted using high-performance GPU clusters. The 8B model runs are executed on a node\nequipped with 8× NVIDIA H100 GPUs. For the 14B parameter model, we use 8× NVIDIA H200 GPUs to accommo-\ndate for increased memory requirements and ensure efficient throughput in the RL phase. DeepSpeed is employed as\nthe inference engine to optimize model sharding, memory management, and logging during GRPO.\nE.2\nSFT Stage\nFor the SFT stage, we use the LoRA method Hu et al. (2022). The hyperparameters are kept constant across both model\nscales.\nTable 5: Hyperparameters for SFT.\nHYPERPARAMETER\nVALUE\nLORA RANK (r)\n16\nLORA ALPHA (α)\n16\nLORA DROPOUT\n0.05\nLEARNING RATE\n2 × 10−4\nE.3\nRL (GRPO) Stage\nGiven the high cost of model generation in full-parameter GRPO, we optimize per-device efficiency while maintaining\na stable learning signal by using a constant learning rate with a warmup period. The choice of num generations = 2\nis made to balance memory requirements with completion length. We found that low temperature is essential for\nmaintaining logical consistency and diversity.\nTable 6: Hyperparameters for GRPO training.\nHYPERPARAMETER\nVALUE\nNUM GENERATIONS (G)\n2\nBATCH SIZE (PER DEVICE)\n1\nLEARNING RATE\n8 × 10−6\nLR SCHEDULE\nCONSTANT W/ WARMUP\nTEMPERATURE (T )\n0.6\nTOP-P\n0.9\nOPTIMIZER\nADAMW\nPRECISION\nBF16\nMAX COMPLETION LENGTH\n1792\nREPETITION PENALTY\n1.15\n18\n"}, {"page": 19, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nF\nSample Model Responses\nNext, we present representative samples of the output of our 14B SFT+RL model on complex, multi-hop medical\nqueries from the ICD-Bench test set. These examples illustrate how our model effectively composes several 1-hop facts\nto solve a 3-hop and a 5-hop problem, respectively.\nDetailed Compositional Reasoning — Category: Neoplasms — Difficulty: 1 — Num Hops: 3\nQuestion: A 6-month-old infant presents with a rapidly growing mass in their left flank. Imaging reveals a large renal mass\nsuspicious for atypical teratoid rhabdoid tumor (ATRT). Family history is significant for a maternal grandfather with multiple\ncutaneous neurilemmomas diagnosed in his 60s. Genetic testing confirms a germline pathogenic variant in SMARCB1 in\nthe infant. Which of the following secondary findings would warrant the MOST urgent investigation in this patient?\nOptions:\nA. Mild macrocephaly with normal development.\nB. Small, subcutaneous lipomas noted on physical exam.\nC. Bilateral sensorineural hearing loss identified on newborn screening.\nD. Widening of the mediastinum on chest X-ray following a persistent cough.\nStep-by-Step Reasoning (< think >)\nOkay, let’s break this down. We have a 6-month-old baby with a likely ATRT, a pretty aggressive brain tumor in\nthis context, and the genetic testing confirms a germline SMARCB1 mutation. This mutation is associated with\nRhabdoid Tumor Predisposition Syndrome (RTPS), which significantly increases the risk of certain cancers, includ-\ning the ATRT seen here.\nNow we need to look at the secondary findings and figure out which one needs the most urgent attention. Let’s go\nthrough the options:\n• A. Mild macrocephaly with normal development: While macrocephaly can be seen in some genetic syndromes,\nincluding RTPS, mild macrocephaly isn’t typically an immediate life-threatening sign, especially when development\nis normal. It’s something to note but might not be the most urgent thing to investigate right now compared to other\npossibilities.\n• B. Small, subcutaneous lipomas: Like macrocephaly, lipomas can occur in RTPS, but small lipomas are generally\nbenign and don’t usually require urgent evaluation unless they are growing rapidly or causing symptoms. They don’t\npoint to an immediate, critical problem.\n• C. Bilateral sensorineural hearing loss identified on newborn screening: Hearing loss is a known feature of some\nSMARCB1 related conditions. However, it’s usually detected early (like via newborn screening) and while important\nfor the child’s development, it’s not typically an immediate medical emergency requiring the same level of urgency\nas a potential airway or cardiac issue.\n• D. Widening of the mediastinum on chest X-ray following a persistent cough: This is the big one.\nA\nwidening mediastinum, especially in the context of a persistent cough, is highly suspicious for a thymoma.\nThymomas are tumors located in the anterior mediastinum. While not always malignant, they can be associated with\nmyasthenia gravis, hypogammaglobulinemia, and sometimes other paraneoplastic syndromes. Crucially, large or\nmalignant thymomas can compress airways or the superior vena cava, leading to respiratory distress or superior vena\ncava syndrome, which is a medical emergency requiring urgent evaluation and potential intervention.\nConsidering the infant already has a severe malignancy (ATRT) and a confirmed SMARCB1 mutation, the possibility\nof a second, potentially rapidly progressive and life-threatening condition like a thymoma, which can cause immediate\nsymptoms (breathing difficulty, facial/arm swelling, neck vein distension), makes the mediastinal widening the most\ncritical finding demanding urgent investigation. The respiratory compromise from a thymoma is a time-sensitive issue.\nTherefore, the widening mediastinum is the most pressing concern that needs investigation urgently.\nGround Truth Knowledge Graph Path:\nRTPS Type 1\nmay be allelic with\n−−−−−−−−−−→Schwannomatosis Type 1\nis a risk factor of\n−−−−−−−−−→Neurilemmoma\nmay cause\n−−−−−−→Mediastinal Enlargement\nFinal Answer: D\n✓Correct\n19\n"}, {"page": 20, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nDetailed Compositional Reasoning — Category: Neoplasms — Difficulty: 5 — Num Hops: 5\nQuestion: A 17-year-old female presents with worsening pain and swelling in her left distal femur. Imaging reveals a\nlytic lesion with periosteal reaction highly suggestive of Ewing’s sarcoma. She is started on a multi-agent chemotherapy\nregimen including Etoposide. Several weeks into treatment, she develops significant mucositis and general malaise, leading\nto difficulty maintaining adequate oral nutrition. Her oncologist is considering adjunctive therapies to improve her nutritional\nstatus and muscle mass while carefully considering potential risks and interactions. Which of the following factors should\nbe given MOST consideration when making this decision?\nOptions:\nA. The patient’s reported history of childhood asthma and potential for steroid-induced bronchospasm.\nB. The documented presence of a BRCA1 mutation in a distant relative and the risk of increased malignancy with certain\ngrowth factors.\nC. The potential for teratogenic effects of some anabolic steroids, given the need for effective contraception.\nD. The need to avoid medications that could exacerbate chemotherapy-induced neuropathy, potentially hindering physical\nrehabilitation.\nStep-by-Step Reasoning (< think >):\nOkay, let’s break this down. We have a 17-year-old patient with Ewing’s sarcoma, getting chemotherapy that includes\nEtoposide. She’s having side effects like mucositis and general malaise, making it hard to eat. The team is thinking\nabout adding something to help her nutrition and build muscle mass, likely an anabolic steroid or similar agent. We need\nto figure out which factor is most important to consider when choosing this adjunctive therapy.\n1. Understand the Core Problem: The patient needs help with nutrition and muscle mass due to chemo side effects.\nPotential solutions might involve anabolic steroids or similar compounds. We need to pick the most critical factor\ninfluencing this decision.\n2. Analyze Option A (Childhood Asthma): The patient has childhood asthma. Anabolic steroids, like Oxymetholone,\ncan be used to treat certain types of pulmonary issues, including sometimes asthma. So, there’s a potential therapeutic\nbenefit here. However, the primary concern with asthma and steroids is usually bronchospasm, which is a risk with\ncorticosteroids (like Prednisone), not typically the main issue directly with anabolic steroids, although some anabolic\nsteroids can have corticosteroid effects or cause fluid retention. While important for overall care, it’s probably not the\nmost critical factor specifically for choosing an adjunctive therapy for nutrition/muscle mass itself, especially compared\nto direct risks of the therapy.\n3. Analyze Option B (BRCA1 Mutation): The BRCA1 mutation is relevant for cancer predisposition (increases risk\nof breast, ovarian cancer). While some growth factors (like Peginesatide) have been associated with risks in BRCA1\nmutation carriers, this is a very specific scenario and doesn’t directly relate to the common adjunctive agents used for\nnutritional support or muscle mass building, which are often anabolic steroids. It’s highly unlikely to be the most critical\nfactor for this patient’s specific situation.\n4. Analyze Option D (Neuropathy): The patient is on Etoposide, which can cause neurotoxicity (like peripheral\nneuropathy). Adding another drug that could potentially worsen this (maybe some agents used for nutrition/muscle mass\nhave neurotoxic potential, or the patient’s overall condition makes them more sensitive) is definitely a consideration for\nquality of life and rehabilitation. It’s a valid concern, but is it the most critical? Probably not, especially when compared\nto other potential issues.\n5. Analyze Option C (Teratogenic Effects): The patient is 17 years old. While not yet of childbearing age, it’s crucial\nto consider future fertility. Anabolic steroids, such as Oxandrolone, are known to be teratogenic, meaning they can\ncause birth defects. This is a serious risk if the patient were to become pregnant, either during treatment or in the near\nfuture after stopping treatment. Given her age and the potential long-term implications of treatment side effects, the\nrisk of teratogenesis stands out as a major, long-term safety concern that must be addressed. If an anabolic steroid is\nconsidered, effective contraception is absolutely essential, and the teratogenic potential is a primary reason why it needs\ncareful consideration and management. This risk is significant and directly related to the choice of therapy, making it\narguably the most critical factor to consider.\n6. Synthesize and Conclude: Comparing the options, the teratogenic potential (Option C) is the most critical factor.\nWhy? Because the patient is young (17), and the potential for future pregnancy exists. Anabolic steroids, which might\nbe considered for her muscle mass and nutrition, are teratogenic. This is a serious long-term health risk that needs\ncareful planning and discussion. While neuropathy (D) is a concern, and asthma (A) has some relevance, the potential for\nteratogenesis is a major safety issue directly impacting the choice and management of therapy for a young female patient.\nTherefore, the factor that should be given the MOST consideration is the potential for teratogenic effects.\n20\n"}, {"page": 21, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nGround Truth Knowledge Graph Path (5 Hops):\nEwing’s sarcoma\nmay treat\n−−−−−→Etoposide\nmay treat\n−−−−−→Brain neoplasms\nmay treat\n−−−−−→Hydroxycarbamide\nmay contraindicate\n−−−−−−−−−−→Pregnancy\nmay contraindicate\n−−−−−−−−−−→Oxandrolone\nFinal Answer: C\n✓Correct\n21\n"}, {"page": 22, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nG\nSFT+RL Pipeline Algorithm\nTo provide a clear, formal overview of our training process, we present the SFT+RL Pipeline in Algorithm 1. This\nalgorithm details the transition from high-quality grounded SFT to path-aligned RL using the GRPO framework.\nAlgorithm 1 Grounded Compositional Reasoning Pipeline (SFT+RL)\nInput: Base model θbase, Knowledge graph G, Training set D = {(Qi, Ai, Pi, Ri)}, where Pi is a KG reasoning\npath and Ri is the reasoning trace.\nOutput: Compositional reasoning model θRL.\n{Stage 1: Supervised Fine-Tuning (SFT)}\nInitialize θSF T ←θbase\nfor each (Q, A, P, R) ∈DSF T do\nFine-tune θSF T to minimize LSF T = −log P(A, R|Q; θSF T )\n{Model learns atomic facts and reasoning structure}\nend for\n{Stage 2: Path-Aligned Reinforcement Learning (RL)}\nInitialize θRL ←θSF T\nfor each question Q ∈DRL do\nGenerate G independent outputs {O1, O2, . . . , OG} from θRL\nfor each output Og do\nExtract reasoning triples ˆTg from ⟨think⟩block\nCompute binary reward Rbin based on final answer correctness\nCompute path reward Rpath by verifying triples in ˆTg against P\nR(Og) = α · Rbin + β · Rpath {Weighted reward signal}\nend for\nCompute Advantage and update θRL using GRPO objective\nend for\n22\n"}, {"page": 23, "text": "KGs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\nH\nModel Prompt\nWe employ a structured prompting strategy to enforce the separation of internal reasoning and final answer selection.\nThis structure is critical for the path alignment reward, Rpath, as it enables our reward function to isolate the think\nblock for axiomatic triple extraction. The exact prompt is shown in Fig. 9.\nSystem Prompt: A conversation between user and assistant. The user asks a single-choice Multiple Choice Question,\nand the assistant solves it using step-by-step reasoning. Please answer the multiple-choice question by selecting only\none from option A, option B, option C, or option D. The assistant first thinks through the problem systematically, then\nprovides the explanation and final answer. Use <think>...</think> tags for internal reasoning, then provide the\nanswer in the format — Final Answer:\n.\nTask Instructions: Please provide complete and accurate answers with clear reasoning. The answer must only be a\nsingle letter from A, B, C, D.\nFormat Enforcement:\n<think> [Reasoning Path] </think>\nFinal Answer:\n[Letter]\nFigure 9: Prompt template for GRPO training.\n23\n"}]}