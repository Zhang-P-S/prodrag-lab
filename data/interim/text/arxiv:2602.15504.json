{"doc_id": "arxiv:2602.15504", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.15504.pdf", "meta": {"doc_id": "arxiv:2602.15504", "source": "arxiv", "arxiv_id": "2602.15504", "title": "Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit", "authors": ["Aswathy Velutharambath", "Amelie Wührl"], "published": "2026-02-17T11:21:40Z", "updated": "2026-02-17T11:21:40Z", "summary": "Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.15504v1", "url_pdf": "https://arxiv.org/pdf/2602.15504.pdf", "meta_path": "data/raw/arxiv/meta/2602.15504.json", "sha256": "d93c356ca6e2f748a95f3ecb0cfdfbec48519468600f646a963356ae457dc50c", "status": "ok", "fetched_at": "2026-02-18T02:19:12.755600+00:00"}, "pages": [{"page": 1, "text": "Towards Expectation Detection in Language: A Case Study on\nTreatment Expectations in Reddit\nAswathy Velutharambath, Amelie Wührl\nUniversity of Stuttgart, Germany, IT University of Copenhagen, Denmark\naswathy.velutharambath@ims.uni-stuttgart.de, amwy@itu.dk\nAbstract\nPatients’ expectations towards their treatment have a substantial effect on the treatments’ success. While primarily\nstudied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment\nexpectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine\nwhat type of expectations users discuss online and how they express them. Presumably this is because expectations\nhave not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation\nDetection, arguing that expectations are relevant for many applications, including opinion mining and product design.\nSubsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract.\nWe contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a\nlarge language model (LLM) to silver-label the data and validate its quality manually (label accuracy ≈78%). Based\non this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We\nfind that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses\ncompared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative\noutcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect\nKeywords: corpus creation, expectation, LLM annotation, social media health mining\n1.\nIntroduction\nExpressions of expectations are ubiquitous in nat-\nural language. They hold valuable information to\nmonitor public opinion toward policy changes (see\nTable 1, Ex. 1), detect student needs (Ex. 2), or in-\nform product design (Ex. 3). In the medical domain\nthey are particularly relevant: Expectations toward\na treatment are powerful psychosomatic mecha-\nnisms known to affect treatment success (Bingel\net al., 2011; Rief et al., 2016; Aulenkamp et al.,\n2023). Pain medication, for example, can be twice\nas effective if the patient expects the treatment\nto work, while expecting it not to work can can-\ncel its effect (Bingel et al., 2011). So far, studying\nthese effects relies on small-scale patient surveys.\nWhile indispensable, their scale is limited, which\nleads to the following constraints: they might fail to\ndetect expectations that patients do not consider\n‘report-worthy’, an issue that Duh et al. (2016) show\nfor reports of adverse drug effects. Additionally,\nsurveys are known to be biased, leaving knowl-\nedge gaps for underrepresented groups (Weber\net al., 2021). Both hinder doctors from addressing\nconcerns caused by negative expectations and to\nachieve effective treatment.\nSocial media, where patients frequently discuss\nmedical experiences, enables us to study expecta-\ntions and their effects in ‘real-world’ environments.\nHere, patients may share skepticism towards a\ntreatment, even if they are uncomfortable or find it\nunnecessary to disclose their concerns with medi-\ncal authorities (see Ex. 4 and 5). Similarly, marginal-\nized groups who have experienced medical discrim-\nid\nTexts expressing expectation\n1\nAll this talk about the new bus lanes. At\nleast I’ll have a new excuse for being late to\nwork from now on.\n2\nI’m pretty optimistic that the new exam for-\nmat might finally test how well we think, not\nhow long we can sit still\n3\nThis phone should last at least three years\n4\nHow is this supposed to have an effect with\nall the weed I’ve been smoking?\n5\nIt’s a stupid reason, but everyone says how\nthe meds can make you feel bloated.\n6\nI bet no one tested this for darker skin tones.\nMakes me scared to even fill the prescription\nTable 1: Example texts expressing expectation\nacross general and medical domain.\nination may distrust doctors’ recommendations (see\nEx. 6) and therefore discuss treatment expectations\nand concerns mostly online. Identifying such on-\nline discussions can help practitioners anticipate a\nwider range of concerns that may foster negative\nexpectations.\nHowever, we are lacking a crucial prerequisite\nto extract this knowledge: research on detecting\nexpectations from text. While tasks like hope and\nregret detection (Chakravarthi, 2020; Balouchzahi\net al., 2023a,b, 2024), or complaint detection (Jin\nand Aletras, 2020) are related to expectations, the\nlinguistic characteristics and domain-specific prop-\nerties of expectations are not studied. We assume\narXiv:2602.15504v1  [cs.CL]  17 Feb 2026\n"}, {"page": 2, "text": "this is because identifying descriptions of expecta-\ntions is difficult. As illustrated by the examples in\nTable 1, expectations are often implicit, nuanced,\nor require understanding contextual cues. Current\nLLMs may be capable of extracting them. To the\nbest of our knowledge, however, they have not been\nevaluated for this task. This is likely because we\nstill lack a comprehensive definition of relevant ex-\npectation types and resources for evaluation.\nTherefore, we make the following contributions:\n(1) We introduce Expectation Detection as a\nnovel NLP task. The goal is to identify linguistic and\nstylistic patterns associated with expression of ex-\npectation in which a writer conveys a belief, anticipa-\ntion, or prediction about a possible future outcome.\nUnlike sentiment or stance analysis, which evaluate\nattitudes about the present, past or future, expec-\ntation detection specifically targets future-oriented\nmental states: utterances in which a person articu-\nlates what they think, hope, or fear may happen.\n(2) Considering the crucial impact expectations\nhave on treatment success, we start exploring this\ntask with a case study on patient-authored health\ntexts. To this end, we contribute the first resource\nto study treatment expectations in social media text.\nThe corpus, RedHOTExpect, consists of ≈4.5K\nmedical Reddit posts expressing expectation, of\nwhich around 2.5K include token-level annotations\nfor treatment, expectations, and outcome descrip-\ntions (TEO triplets). Hypothesizing that LLMs have\na rudimentary notion of ‘expectation’, we use an\nLLM to filter and silver-label expectations and out-\ncomes. We manually validate the LLM annota-\ntions in a representative subset of the data (250\ninstances), and find them to be accurate in 77.5%\nof cases. For the same subset, we additionally char-\nacterize expectation and outcome mentions with\nrespect to their type and the basis of expectation.\n(3) Using this data, we study how patients dis-\ncuss treatment expectations on social media by an-\nalyzing which linguistic and stylistic patterns charac-\nterize these utterances (§5). In a targeted manual\nanalysis (§6, we explore what patients expect and\nwhy. We find that expectation-related discourse\nis characterized by a more optimistic and action-\noriented tone, whereas non-expectation posts more\noften convey caution, frustration, or negative affect.\nOverall, we observe that optimism and proactive\nframing are most pronounced in posts about physi-\ncal or treatment-related illnesses, whereas mental-\nhealth contexts convey expectations in a more in-\ntrospective and socially detached tone. Notably,\nexpectations in our dataset mostly discuss benefits\nrather than negative outcomes.\n2.\nRelated Work\nPatients frequently use online platforms to discuss\nmedical topics (Chen et al., 2021). Therefore, a\nsubfield of medical NLP specializes on biomedical\ncontent from social media and other online sources.\nCentral tasks are pharmacovigilance, i.e., identi-\nfying descriptions of adverse drug reactions (Nik-\nfarjam et al., 2015; Cocos et al., 2017; Magge\net al., 2021; Karimi et al., 2015), monitoring public\nhealth (Paul and Dredze, 2012; Choudhury et al.,\n2013; Sarker et al., 2016; Stefanidis et al., 2017),\nextracting personal health experiences (Yin et al.,\n2015; Klein et al., 2017; Karisani and Agichtein,\n2018; Wadhwa et al., 2023; Falk and Lapesa, 2024),\nanalyzing mental health discourse (Garg, 2023),\nand fact-checking (Hossain et al., 2020; Mattern\net al., 2021; Saakyan et al., 2021; Kim et al., 2023,\ni.a.). Health mining datasets are commonly col-\nlected from patient fora (Karimi et al., 2015; Vladika\net al., 2024) and various social media platforms.\nUntil the company limited its API access, Twitter\n(now X) served as a popular data source (Sarker\net al., 2016; Wührl and Klinger, 2022; Sundriyal\net al., 2022, i.a.). Reddit is another frequently used\nsource, particularly because posts are not length-\nrestricted\n(Scepanovic et al., 2020; Basaldella\net al., 2020; Wadhwa et al., 2023, i.a.). Recently,\nsynthetic data also has become increasingly popu-\nlar (Wührl et al., 2024; Yamagishi and Nakamura,\n2024; Ghanadian et al., 2024, i.a.).\nWhile\nconnected\nto\nconcepts\nsuch\nas\nhope (Balouchzahi et al., 2023b), stance (Hardalov\net al., 2022), or emotion (Plaza-del Arco et al.,\n2024), which have been studied in NLP, expec-\ntation detection is an unexplored task.\nSome\nwork addresses adjacent tasks: Jin and Aletras\n(2020) study complaint detection, which involves\nidentifying when someone voices a mismatch\nbetween their expectation and reality. Similarly,\nwork on conversational intent (Sakurai and Miyao,\n2024) may capture the speakers’ expectation\nof how the conversation could evolve.\nThe\nmost closely related is work on hope and regret\ndetection (Chakravarthi, 2020; Balouchzahi et al.,\n2023a,b, 2024). While an expression of hope con-\nveys an expectation, it is limited to a positive stance\ntowards the event.\nRegret always expresses a\nnegative attitude towards a past event. Bringula\net al. (2022) extract expectations in student essays\nusing word clustering and sentiment counts, but\ndo not model expectation expressions or their\nlinguistic grounding.\nSome work brings together concepts relevant\nto expectations with health tasks. With respect to\nemotion, (Khanpour and Caragea, 2018) detect\npatients’ emotional states from online posts. (Tur-\ncan et al., 2021) detect psychological stress us-\n"}, {"page": 3, "text": "ing emotion-informed models, while others explore\nunderstanding emotional states towards medical\nevents like pandemics (Ng et al., 2020; Sosea et al.,\n2022, i.a.). The work most related to our case study\non extracting treatment expectations is Balouchzahi\net al. (2024) who model expressions of hope and\nregret with respect to drug use, and Ettlin et al.\n(2025) who explore patient expectations in clinical\nself-reports using topic modeling and linguistic style\nanalysis\nTo the best of our knowledge, we are the first\nto formalize the task of expectation detection in\nNLP. So far, no work on extracting treatment ex-\npectations from social media exist. This motivates\ncreating a resource that allows us to study how pa-\ntients discuss their expectations towards a medical\ntreatment.\n3.\nExpectation Detection\nWe formalize the task of Expectation Detection by\ndefining what constitutes an expectation and what\nlinguistic properties can be attributed to it.\nDefining Expectation Event. We define an Expec-\ntation Event (EE) as any statement or implication\nabout a possible or likely future state, whether ben-\neficial, harmful, or neutral, that is explicitly linked\nto an action, intervention, or situation. In linguis-\ntic terms, EEs capture how people verbalize their\nbelief, hope, or fear about a future outcome (See\nTable 1). To establish what constitutes an expecta-\ntion and how it differs from related constructs such\nas sentiment or stance, we attribute the following\nproperties to each Expectation Event:\n• Expectation Type: the valence or direction\nof the anticipated outcome, such as benefit,\nharm, no effect, worsening or mixed, or other.\n• Expectation Basis: the source or justification\nunderlying the belief, e.g., Personal (based\non one’s own past experience), Social (based\non peers or community experiences), Author-\nity (based on statements from doctors or ex-\nperts), Information/Media (based on studies,\nadvertisements, or online information), Cul-\ntural (stemming from social or cultural beliefs),\nSelf-efficacy (based on belief in one’s ability\nto follow the treatment).\n• Certainty: the degree of confidence or tenta-\ntiveness expressed by the author (e.g., “I hope\nit helps” vs. “It will definitely help”).\n• Temporal Orientation: whether the expec-\ntation is prospective (before the outcome is\nknown) or retrospective (recalled after an out-\ncome has occurred).\nFigure 1 shows examples of expectations anno-\ntated with these dimensions.\nTask Definition. This study examines expectations\nexpressed in medical discourse. We specifically\nExample 1: “My doctor prescribed me X. I really hope\nit will stop my headaches this time, but I’m a bit afraid\nit could make me dizzy.”\nExpectation Events:\nEE1 “I really hope it will stop my headaches”\nExpectation Type: Benefit\nExpectation Basis: Authority (doctor’s prescrip-\ntion)\nCertainty: Moderate (“hope”)\nTemporal Orientation: Prospective\nEE2 “I’m a bit afraid it could make me dizzy.”\nExpectation Type: Harm\nExpectation Basis: None\nCertainty: Low (“could”)\nTemporal Orientation: Prospective\nExample 2: “All this talk about the new bus lanes. At\nleast I’ll have a new excuse for being late to work from\nnow on”\nExpectation Events:\nEE1 bus lanes will lead to delays in traffic\nExpectation Type: Worsening\nExpectation Basis: Information/Media\nCertainty: High\nTemporal Orientation: Prospective\nFigure 1: Annotated examples illustrating Expecta-\ntion Events (EEs) in posts.\ntarget outcome-related expectations, statements\nthat express anticipated effects of a treatment or\nintervention that could plausibly lead to a health-\nrelated outcome.\nFor example, “I think this new medication will\nfinally help me sleep” and “I’m worried the drug\nmight make me nauseous” both describe future\noutcomes linked to a treatment and thus constitute\nExpectation Events. By contrast, “I expect to see\nmy doctor next week” expresses an expectation but\nnot one about a treatment outcome and therefore\nfalls outside our scope.\nFormally, given a text span T, the Expectation\nDetection task in this study involves two subtasks:\n1. Expectation\nIdentification:\ndetermine\nwhether T contains one or more Expectation\nEvents; and\n2. Treatment-Expectation-Outcome annota-\ntion: if so, identify their key components: (a)\nthe treatment or intervention about which the\nexpectation is expressed), (b) the expectation\n(what is expected to occur) and (c) the actual\noutcome (what actually occurred).\n4.\nExpectation Corpus\n4.1.\nCorpus Creation\nTo systematically study how patients express ex-\npectations about medical treatments on social\n"}, {"page": 4, "text": "LLM\nLLM\nRedHOT\nFilter\nExperiences\nDetect\nExpectations\nAnnotate\nTEO Triplet\nRedHot-\nExpectation\ntreatment: lower dose of Kaftrio (trikfta), \nexpectation: things get a little bit better as my body\ngets used to this lower dose, \nobserved_outcome: started to feel really unwell\nExample TEO annotation\nFigure 2: Automated pipeline for creating annotated RedHOT-Expectation data.\nmedia, we require data that captures this con-\ncept. To facilitate this, we construct RedHOTEx-\npect, a corpus of Reddit posts containing expres-\nsions of expectation, further annotated with Treat-\nment–Expectation–Outcome (TEO) triplets. The\nfollowing section outlines the steps involved in cre-\nating the corpus. The overall annotation process is\nillustrated in Figure 2.\nExperience filtering. As a starting point, we build\non the existing RedHOT corpus (Wadhwa et al.,\n2023), which contains ≈22k Reddit posts related\nto health conditions. We selected this dataset be-\ncause it already focuses on patient-authored health\ndiscussions and provides high-level discourse an-\nnotations such as questions, experiences, and\nclaims. This structure allows us to efficiently iden-\ntify posts in which users describe their own medical\nexperiences, which are the contexts where treat-\nment expectations are most likely to appear. We\ntherefore subset all posts labeled as experience,\nresulting in a collection of ≈12k posts. This filtered\nsubset forms the foundation for identifying explicit\nmentions of expectations in the following step.\nExpectation labeling. Identifying expressions of\nexpectation in Reddit posts is challenging because\nthe texts are often long, narrative, and loosely struc-\ntured. Expectations are frequently implicit, context-\ndependent, and not always expressed within a con-\ntinuous span of text. For instance, a post may\nintroduce a medication or treatment at the begin-\nning and describe the anticipated outcome only at\na later stage. Given this complexity, we employ a\nlarge language model to leverage its broad seman-\ntic understanding and contextual reasoning capabil-\nities for identifying posts that express expectations.\nSpecifically, we use the open-weight model gpt-\noss-20b1 to classify each post as either contains\nexpectation or not. The model is prompted with a\ndetailed definition of outcome-related expectations\nand few-shot examples illustrating positive and neg-\native cases. It is based on the expectation definition\n1https://huggingface.co/openai/\ngpt-oss-20b\nClass / Subset\nCount\nMean\nStd\nMedian\nMax\nNo expectation\n7,364\n136.6\n124.7\n102\n1,771\nExpectation (all)\n4,433\n182.9\n170.8\n138\n2,944\nhas TEO\n2,529\n212.6\n198.7\n158\n2,944\nno TEO\n1,904\n143.6\n113.0\n113\n1,042\nTable 2:\nToken length statistics by class and\nwithin the subset of expectation posts, distin-\nguishing those with and without extracted treat-\nment–expectation–outcome (TEO) triplets.\nand examples discussed in §32. We conduct sev-\neral iterations of prompt refinement and manual\nverification to optimize the precision expectation\ndetection.\nTreatment-Expectation-Outcome triplet. In ad-\ndition to examining how expectations are ex-\npressed linguistically, a further motivation for\nbuilding the RedHOTExpect corpus is to en-\nable analyses of how treatment expectations re-\nlate to reported outcomes. To support this goal,\nwe extend the annotation to a structured Treat-\nment–Expectation–Outcome (TEO) triplet, where\nthe Treatment refers to the intervention or medica-\ntion mentioned, the Expectation to the anticipated\noutcome, and the Outcome to the described or im-\nplied result. We again use the open-weight model\ngpt-oss-20b, providing detailed prompts to au-\ntomatically generate these triplets.\nGiven the narrative style of Reddit posts, we in-\nstruct the model to extract one or more TEO triplets\nper post, typically one per treatment mention. Be-\ncause Reddit threads often include exchanges be-\ntween community members, an author may report\nthe treatment outcome only later in the discussion.\nTo account for this, we retrieve all comments posted\nby the same author and append them to the prompt,\nenabling the model to detect outcomes described\nin follow-up messages. This procedure allows us to\ncapture both prospective expectations (before the\noutcome is known) and retrospective expectations\n(beliefs recalled after an outcome has occurred).\n2See Appendix A.1for the full prompt.\n"}, {"page": 5, "text": "4.2.\nHuman Validation\nTo\nevaluate\nhow\nreliably\nthe\nLLM\nextracts\ntreatment–expectation–outcome (TEO) triplets we\nannotate a subset of instances manually. We select\na stratified sample of 245 posts3 from the TEO an-\nnotated subset of the RedHOTExpect. To ensure\ndiversity while maintaining relevance, we restrict\nthe sample to condition–treatment pairs that occur\nat least three times in the corpus, guaranteeing suf-\nficient contextual coverage across domains. The\nsampling procedure includes at least one instance\nof each eligible pair, with the remaining posts se-\nlected at random and balanced across health con-\nditions.\nOut of the 245 manually inspected posts, the\nautomatic pipeline had extracted 502 Treatment–\nExpectation–Outcome (TEO) triplets. Manual vali-\ndation confirms that 389 of these are correct, dis-\ntributed across 200 posts, while 45 posts contained\nno valid triplet. This corresponds to a labeling ac-\ncuracy of 77.5%, indicating that the automatic ex-\ntraction process achieves a reasonably high level\nof precision while allowing for multiple expectations\nto be captured within a single post.\n4.3.\nCorpus Details\nThe initial subset of posts drawn from the Red-\nHOTdataset contained 11,797 Reddit posts cover-\ning 23 health conditions. Each post was automati-\ncally classified by a large language model as either\ncontaining or not containing an expectation-related\nexpression, following the procedure described in\n§4.1. Overall, 4,433 posts (37.6%) were labeled as\ncontaining treatment expectations, suggesting that\nexpectation-related discourse is a frequent phe-\nnomenon in patient-authored online discussions.\nThe remaining 7,364 posts (62.4%) describe treat-\nment experiences, advice, or symptom trajectories\nwithout explicitly expressing anticipation or belief\nabout a future treatment outcome. In the TEO an-\nnotation step, out of the 4,433 posts labeled as con-\ntains expectation, 2,529 posts (57.0%) contained\nat least one complete triplet linking a treatment, an\nexpectation, and an outcome.\nFrom the manual validation step (§4.2) we ob-\nserve an identification accuracy of approximately\n78%, implying an expected error rate of around\n22%. For the subsequent analyses, we should\nkeep this in mind and interpret findings in light of\nthe estimated labeling noise.\n3Originally sampled 250, five exclude because expec-\ntations were labeled \"None\" or \"null\".\n5.\nHow do patients discuss treatment\nexpectations on social media?\nOur goal is to identify how language use differs\nbetween posts that express treatment expectations\nand other health-related narratives. We focus on\nlinguistic and stylistic markers that may capture\nhow people formulate predictions, intentions, or\noutcome-oriented thinking in health discourse.\n5.1.\nExperimental setup\nTo examine linguistic markers of expectation in\nhealth-related discourse, we perform a lexical fea-\nture analysis using the LIWC-22 dictionary (Boyd\net al., 2022). It provides frequency-based indica-\ntors of psychological and linguistic processes and\nis widely used in NLP and social media text anal-\nysis. In addition, we include sentiment analysis\nusing the VADER lexicon (Hutto and Gilbert, 2014),\nthe Gunning–Fog readability metric, and a lexical\ndiversity measure4. From the LIWC lexicon, we se-\nlect categories that we hypothesize to be relevant\nto expressions of expectation in health-related com-\nmunication. As shown in Table 3, these categories\nare grouped into seven interpretable dimensions:\nUncertainty (hedges and cognitive stance terms),\nTime focus (past, present, and future orientation),\nEmotion (positive and negative affect), Social refer-\nences (self- and other-related pronouns), Motiva-\ntion (goal- and need-related words), Health (illness\nand wellness vocabulary), and Style (readability\nand lexical diversity).\nTo test for linguistic differences between ex-\npectation and non-expectation posts, we use the\nMann–Whitney U test, a non-parametric alterna-\ntive to the two-sample t-test that does not assume\nnormality. We report effect sizes using Cliff’s δ,\nindicating the direction and magnitude of group dif-\nferences, and adjust all p-values for multiple com-\nparisons using the Benjamini–Hochberg FDR pro-\ncedure (Benjamini and Hochberg, 1995). This pro-\ncedure is conducted both at the group level (ag-\ngregated dimensions) and at the individual-feature\nlevel to identify which specific LIWC categories\ndrive the observed effects.\n5.2.\nResults\nAs shown in Table 2, within this dataset, posts la-\nbeled as containing expectations exhibit higher to-\nken counts (median = 138) than those without ex-\npectations (median = 102). The Mann–Whitney\ntest confirms that this difference is statistically sig-\nnificant. While this finding does not imply a general\n4For extracting these features we use\nhttps://bitbucket.org/aswathyve/\nlinguistic_style_features.\n"}, {"page": 6, "text": "Drives\nfocusfuture\nachieve\ntone_pos\nWPS\ndiscrep\nfocuspast\ngunning_fog\nwant\nsubstances\nemo_pos\nwellness\naffiliation\nreward\nneed\nemo_anx\nAffect\ncertitude\nfulfill\nwe\nlack\npower\nemo_sad\nfriend\nfamily\nrisk\nlexical_diversity\nemo_anger\nmental\nself_reference\ni\nyou\nhealth\nppron\nfocuspresent\ncogproc\ntentat\nillness\nemo_neg\ninsight\ntone_neg\nSocial\n0.4\n0.2\n0.0\n0.2\n0.4\nMean Difference (Expectation  Non-Expectation)\n+0.45\n+0.43\n+0.39\n+0.38\n+0.35\nMean Feature Differences\nFigure 3: Mean feature difference in Expectation vs. Non-Expectations posts, with non-significant\ndifferences in grey.\nDimension\nFeatures\nUncertainty\ntentat, certitude, cogproc, discrep, in-\nsight\nTime focus\nfocuspast, focuspresent, focusfuture\nEmotion\nAffect,\ntone_pos,\ntone_neg,\nemo_pos,\nemo_neg,\nemo_anx,\nemo_anger, emo_sad\nSocial ref.\nself_reference, ppron, i, we, you, So-\ncial, family, friend\nMotivation\nDrives, achieve, power, affiliation,\nneed, want, lack, fulfill, reward, risk\nHealth\nhealth, illness, wellness, mental, sub-\nstances\nStyle\nWPS, lexical_diversity, gunning_fog\nTable 3: LIWC feature groups used in the analysis.\nrelationship, it suggests that expectation-related\ncontent may be accompanied by more elaborate\ndiscourse in this sample.\nAs shown in Table 4, expectation-related posts\ndiffer systematically from other health narratives\nacross several linguistic dimensions. The strongest\neffect is observed for Motivation (∆= +0.11), in-\ndicating greater use of goal-oriented and desire-\nexpressing language. Time focus also shows a\nlarge overall difference (∆= +0.19), suggesting\nthat expectation-related posts contain more tempo-\nral references in general, reflecting stronger tempo-\nral framing of events. When examining individual\nfeatures within this category, the largest contribu-\ntion comes from focusfuture (∆= +0.43), showing\nthat people tend to talk more about future outcomes\nwhen expressing expectations. Style features also\ndiffer reliably (∆= +0.18), with posts expressing\nDimension\n∆(Exp–Non)\nCliff’s δ\nFDR-p\nMotivation\n+0.1084\n0.1780\n<10−41\nTime focus\n+0.1898\n0.0912\n<10−11\nStyle\n+0.1800\n0.0867\n<10−9\nSocial refs.\n-0.0985\n-0.0389\n0.0045\nHealth\n-0.0465\n0.0273\n0.0482\nEmotion\n-0.0138\n0.0229\n0.0895\nUncertainty\n-0.0689\n-0.0146\n0.2581\nTable\n4:\nGroup-level\nLIWC\ndifferences\n(Mann–Whitney with Benjamini–Hochberg FDR).\n∆is mean(Exp) −mean(Non); positive values\nindicate higher means in expectation posts.\nexpectation containing longer and more syntacti-\ncally complex sentences. Smaller yet significant\neffects appear for Social references (∆= −0.10)\nand Health vocabulary (∆= −0.05), indicating\nfewer personal pronouns and health-specific terms\nin expectation posts. Differences for Emotion and\nUncertainty were not statistically significant.\nAt the individual-feature level (Figure 3), expec-\ntation posts show increased use of words related\nto Drives, achieve, and tone_pos, highlighting a\nmotivational and positive framing. Conversely, cat-\negories such as risk, emo_anger, emo_neg, and\ntone_neg tend to have higher mean scores in non-\nexpectation posts. Although not all of these dif-\nferences reach statistical significance, the overall\npattern suggests that expectation-related discourse\nis characterized by a more optimistic and action-\noriented tone, whereas non-expectation posts more\noften convey caution, frustration, or negative affect.\nTo examine if the observed patterns hold across\ndifferent medical conditions, we group the condi-\n"}, {"page": 7, "text": "Time focus\nMotivation\nStyle\nEmotion\nSocial references\nHealth\nUncertainty\nAutoimmune / I...\nEndocrine / Me...\nGastrointestin...\nMental health\nNeurological\nPhysical / oth...\nRespiratory\n0.29*** 0.14*** 0.32*** 0.05\n0.08\n-0.09\n0.05\n0.21** 0.09*** 0.71*** 0.17*** -0.03\n-0.05\n0.21*\n0.16· 0.19*** -0.14** -0.08\n0.02\n-0.12\n0.07\n0.09\n0.07**\n0.41·\n-0.06\n-0.20* 0.18*** -0.25*\n0.10·\n0.06** -0.01\n0.08*\n-0.15·\n0.03\n-0.15\n0.18* 0.18*** 0.16\n-0.13\n-0.01 -0.28** -0.01\n0.38** 0.10*** 0.23·\n0.03\n-0.18\n-0.13\n-0.07\nMean differences across condition types\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nMean diff (Expectation  Non-expectation)\nFigure 4: Mean differences by condition types.\ntions into seven categories: Mental health, Autoim-\nmune/Inflammatory, Endocrine/Metabolic, Neuro-\nlogical, Gastrointestinal, Respiratory, and Physi-\ncal/Other. As shown in Figure 4) across medical\ncondition types, expectation-related posts consis-\ntently show a more future-oriented and motivational\ntone than non-expectation narratives. The effect\nis strongest in endocrine/metabolic and respiratory\nconditions, where posts were also longer and stylis-\ntically more complex, suggesting deliberate and\ngoal-directed framing. In contrast, mental health\ndiscussions exhibited weaker differences and fewer\nsocial references, indicating that expectations in\nthis context tend to be more introspective and self-\nfocused. Overall, these patterns highlight that while\nthe linguistic signature of expectation is broadly sta-\nble across domains, its expression varies with the\nnature of the health condition and the type of expe-\nrience patients describe.\n6.\nWhat patients expect and what\nactually happens?\nWhile the linguistic analysis in §5 characterizes how\nexpectations are expressed at the surface level (lex-\nical and stylistic features), this section focuses on\nwhat patients expect and why. Therefore, we con-\nduct a targeted manual analysis of a representative\nsubset of the corpus.\nAs discussed in §4.2, by manually annotating the\n245 posts for validating the quality of annotations,\nwe identified 389 Treatment–Expectation–Outcome\ntriplets . In this section, we annotate each of these\ntriplets with the Expectation type (Benefit, Harm,\nWorsening, No effect, Mixed, or Other), the Ex-\npectation basis (Personal, Social, Authority,\nInformation/Media, Cultural, Self-efficacy, or\nOther), and the Outcome basis (Benefit, Harm,\nWorsening, No effect, Mixed, or Other). To-\ngether, these annotations provide a preliminary\nunderstanding of how treatment expectations are\n157\n57\n45\n111\n2\n3\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n4\n0\n2\n0\nBenefit\nHarm\nMixed\nNo Effect\nUncertain\nBenefit\nHarm\nMixed\nNo Effect\nUncertain\n0\n20\n40\n60\n80\n100\n120\n140\nConfusion Matrix\nOutcome Type\nExpectation Type\nFigure 5: Confusion matrix representing alignment\nof expectation and outcomes types.\nformed, what factors motivate them, and how they\nrelate to the outcomes that patients report.\nExpectation Types. Within the manually verified\ntriplets, expectations are overwhelmingly benefit-\noriented: 372 out of 389 (95.6%) expectations are\nlabeled as Benefit, compared to Harm (10; 2.6%)\nand Uncertain (7; 1.8%); no instances were an-\nnotated as Mixed or No Effect at the expectation\nlevel in this sample. In line with the observation\nmade in §5.2, we see that in this limited sample,\npeople predominantly articulate benefit-oriented\nexpectations.\nExpectation–Outcome alignment.\nFigure 5 shows the alignment between the treat-\nment expectations and the actual outcomes re-\nported by the patients.\nAs shown in Figure 5,\nmost Benefit expectations are followed by Ben-\nefit outcomes (157/372; 42.2%), yet nearly one\nthird correspond to No Effect (111/372; 29.8%)\nand a smaller share to Harm (57/372; 15.3%) or\nMixed (45/372; 12.1%). The few Harm expecta-\ntions show some alignment with harmful outcomes\n(6/10; 60.0%), and Uncertain expectations tend to\naccompany neutral or negative outcomes (4/7 and\n2/7, respectively). Since majority of the sampled in-\nstances contain expectation if Benefit, it is hard to\ndraw a strong conclusion. A larger-scale analyses\nis necessary to test whether expectations system-\natically align with outcomes in naturally occurring\npatient discourse.\nLinguistic certainty and expectation strength.\nThe strength or confidence in their expectations\n(\"I think\" vs. \"I am absolutely certain\") may also\ninfluence the corresponding treatment outcomes.\nTo approximate this dimension, we derive a Cer-\ntainty score from two LIWC features that capture\nepistemic stance: certitude and tentat.\nCertainty score = zcertitude −ztentat\n"}, {"page": 8, "text": "ankylosing...\ngout\nIBD\nrheumatoid...\nlupus\nthyroidcan...\nHypothyroi...\nDiabetes\nGERD\nibs\nGastropare...\nADHD\nDysthymia\nPsychosis\nbulimia\nnarcolepsy\nMultipleSc...\nEpilepsy\nCFS\nHyperhidro...\ncostochond...\nPOTS\nSinusitis\nCysticFibr...\nTopic (Medical Condition)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of Expectation Basis\n12\n35\n3\n33\n3\n11\n2\n10\n4\n1\n8\n1\n25\n5\n10\n4\n12\n1\n29\n1\n1\n24\n3\n1\n16\n2\n5\n9\n2\n1\n11\n3\n6\n1\n1\n1\n1\n31\n4\n1\n5\n7\n1\n1\n2\n1\n19\n1\n1\n16\n1\n7\n22\n3\n50\n2\n2\n1\n8\nDistribution of Expectation Basis Across Topics\nExpectation Basis\nAuthority\nCultural\nInfo/Media\nPersonal\nSocial\nCondition Type\nAutoimmune / Inflammatory\nEndocrine / Metabolic\nGastrointestinal\nMental health\nNeurological\nPhysical / other\nRespiratory\nFigure 6: Distribution of expectation basis across medical conditions grouped by condition types.\nwhere, certitude contributes positively, reflect-\ning confident or assertive phrasing, while tentat\ncontributes negatively, capturing hedging or uncer-\ntainty. Higher Certainty score indicates more con-\nfident or decisive language, whereas lower value\nreflects tentative or reflective expression. We then\ncompare this score between aligned(Expectation\ntype = Outcome type) and misaligned(Expectation\ntype ̸= Outcome type) posts using a Mann–Whitney\nU test to assess distributional differences.\nWe observe a mean index close to zero for both\naligned (M = −0.027) and misaligned (M = 0.020)\ninstances, and the difference was not statistically\nsignificant (Mann–Whitney p = 0.86; Cliff’s δ =\n−0.011). These results suggest that linguistic cer-\ntainty, as captured by LIWC features is not predic-\ntive of expectation-outcome alignment.\nOverall, this exploratory finding highlights that\ncertainty of expectations may not be easily captured\nby lexicon-based measures. Future work should\ntherefore include explicit certainty annotations or\ncontext-sensitive modeling (e.g., modality or con-\nfidence detection) to better capture how strongly\npatients believe in their stated expectations.\nExpectation basis across health topics. Figure 6\nillustrates how the rationale behind patients’ expec-\ntations varies across different medical conditions.\nAcross all topics, expectations are predominantly\ngrounded in personal experience, reflecting that\nmost users draw on their own prior treatment his-\ntory or symptom when anticipating outcomes. Ref-\nerences to authority (e.g., medical professionals\nor prescriptions) constitute the second most com-\nmon basis and appear more prominently in certain\nconditions such as Sinusitis, GERD, and Ankylos-\ning Spondylitis, though without a consistent pattern\nacross condition types. Mentions of social or in-\nfo/media sources occur infrequently, while cultural\nreferences are rare in the current subset. Overall,\nthis pattern suggests that patients’ expectations in\nthe current subset are shaped largely by individ-\nual experience and clinical authority rather than by\nbroader social, informational, or cultural influences.\n7.\nConclusion\nIn this paper, we introduce the novel task of expec-\ntation detection which aims to capture how beliefs\nand anticipations about the future are expressed\nin language. To understand and formalize the no-\ntion of expectation, we conduct a case study on\nhealth communication, where expectations about\ntreatment outcomes are known to have an influ-\nence on the effectiveness of the treatment. To\nenable this task, we create RedHOTExpect, a cor-\npus of patient-authored Reddit posts expressing\nexpectation and a subset of the dataset annotated\nwith Treatment-Expectation-Outcome(TEO) triplets.\nOur analyses show that posts with expectations are\nlonger, more future-oriented and motivational than\nother narratives. Manual verification of TEO triplets\nshowed that expectations in the subset are mostly\ndiscussing benefits rather than negative outcomes.\nOverall, this work takes a first step towards mod-\neling expectation as a measurable linguistic phe-\nnomenon. With the qualitative analysis on the treat-\nment expectations, basis and outcomes, we high-\nlight the opportunities and challenges in computa-\ntional research on expectation effects using large\nscale social media data. In future work, we aim to\nexamine how expectations vary in their temporal\norientation and level of certainty.\n"}, {"page": 9, "text": "Ethical Considerations\nThe dataset used in this study builds on the publicly\navailable RedHOTcorpus (Wadhwa et al., 2023),\nwhich was constructed in compliance with Reddit’s\nterms of use and includes an opt-out procedure\nfor users. We retrieved the data from the Reddit\npost identifiers provided in the original dataset and\nexcluded instances that had been deleted by users.\nNo personally identifying information was collected\nor shared, and we did not attempt to identify or\ncontact any users. In line with the ethical princi-\nples established by the original authors, we do not\nredistribute raw post texts. Instead, we release\nthe annotations, along with scripts needed to re-\nproduce the dataset by retrieving the posts directly\nfrom Reddit5.\nModeling expectations in health-related dis-\ncourse has potential benefits for understanding\npatient perspectives but also entails ethical respon-\nsibility. We therefore emphasize that all analyses\nare conducted at an aggregate level and are not\nintended to make inferences about individual users.\nLimitations\nThe present study offers an initial step toward mod-\neling expectations in language but has several lim-\nitations. The corpus is derived entirely from Red-\ndit, representing a specific subset of patients (Duh\net al., 2016) and a limited range of health-related\nsubreddits; thus, the findings may not generalize\nto other populations, platforms, or clinical contexts.\nThe results should therefore be interpreted with the\nunderstanding that this is a case study, and further\ninvestigation is needed to draw more robust conclu-\nsions about the linguistic properties of expectation.\nFurther, the Treatment–Expectation–Outcome\nannotations were generated automatically using\nan LLM, and although manual validation indicated\ngood accuracy, the data inevitably contain noise\nand inconsistencies. Moreover, LLM-based anno-\ntations are sensitive to prompt design and model\nbehavior, which may introduce systematic biases\nthat are difficult to control.\nAcknowledgements\nThis work is part of the SoftwareCampus project\nPlacebo, which is funded by German Federal Min-\nistry of Research, Technology and Space (BMFTR)\nunder the grant 01IS23072.\n5The dataset and script for downloading are available\nat https://www.ims.uni-stuttgart.de/data/\nRedHOTExpect\nReferences\nJan Lukas Aulenkamp, Anita Icenhour, and Susan\nElsenbruch. 2023. Nocebo effects in visceral\npain: concept and design of the experimental\nrandomized-controlled pain study ’novis’. Fron-\ntiers in psychiatry, 14:1270189.\nFazlourrahman Balouchzahi, Sabur Butt, Abeed\nSarker, Al-Garadi MA, Grigori Sidorov, and\nAlexander Gelbukh. 2024. Analysis of expres-\nsions of hope and regret associated with nonmed-\nical prescription drug use in x chatter. Available\nat SSRN 4930517.\nFazlourrahman Balouchzahi, Sabur Butt, Grigori\nSidorov, and Alexander Gelbukh. 2023a. Red-\ndit: Regret detection and domain identification\nfrom text.\nExpert Systems with Applications,\n225:120099.\nFazlourrahman Balouchzahi, Grigori Sidorov, and\nAlexander Gelbukh. 2023b. Polyhope: Two-level\nhope speech detection from tweets. Expert Sys-\ntems with Applications, 225:120078.\nMarco Basaldella, Fangyu Liu, Ehsan Shareghi,\nand Nigel Collier. 2020. COMETA: A corpus for\nmedical entity linking in the social media.\nIn\nProceedings of the 2020 Conference on Empir-\nical Methods in Natural Language Processing\n(EMNLP), pages 3122–3137, Online. Associa-\ntion for Computational Linguistics.\nYoav Benjamini and Yosef Hochberg. 1995. Con-\ntrolling the false discovery rate: A practical and\npowerful approach to multiple testing. Journal of\nthe Royal Statistical Society. Series B (Method-\nological), 57(1):289–300.\nUlrike\nBingel,\nVishvarani\nWanigasekera,\nKatja Wiech, Rhiannon Ni Mhuircheartaigh,\nMinyeong C Lee, Markus Ploner, and Irene\nTracey. 2011. The effect of treatment expectation\non drug efficacy: imaging the analgesic benefit\nof the opioid remifentanil. Science translational\nmedicine, 3(70):70ra14.\nRyan L. Boyd, Ashwin Ashokkumar, Sajjad Seraj,\nand James W. Pennebaker. 2022. The devel-\nopment and psychometric properties of liwc-22.\nTechnical report, University of Texas at Austin,\nAustin, TX.\nRex Bringula, Saida Ulfa, John Paul P. Miranda,\nand Francis Arlando L. Atienza. 2022. Text min-\ning analysis on students’ expectations and anx-\nieties towards data analytics course.\nCogent\nEngineering, 9(1):2127469.\n"}, {"page": 10, "text": "Bharathi Raja Chakravarthi. 2020. HopeEDI: A\nmultilingual hope speech detection dataset for\nequality, diversity, and inclusion. In Proceedings\nof the Third Workshop on Computational Model-\ning of People’s Opinions, Personality, and Emo-\ntion’s in Social Media, pages 41–53, Barcelona,\nSpain (Online). Association for Computational\nLinguistics.\nJunhan Chen, Yuan Wang, et al. 2021.\nSocial\nmedia use for health purposes: systematic re-\nview.\nJournal of medical Internet research,\n23(5):e17917.\nMunmun De Choudhury, Scott Counts, and Eric\nHorvitz. 2013. Social media as a measurement\ntool of depression in populations. In In Proceed-\nings of the 5th ACM International Conference\non Web Science (Paris, France, May 2-May 4,\n2013). WebSci 2013.\nAnne Cocos, Alexander G Fiks, and Aaron J\nMasino. 2017. Deep learning for pharmacovigi-\nlance: recurrent neural network architectures for\nlabeling adverse drug reactions in twitter posts.\nJournal of the American Medical Informatics As-\nsociation, 24(4):813–821.\nMei Sheng Duh, Pierre Cremieux, Marc Van Au-\ndenrode, Francis Vekeman, Paul Karner, Haimin\nZhang, and Paul Greenberg. 2016.\nCan so-\ncial media data lead to earlier detection of drug-\nrelated adverse events? Pharmacoepidemiology\nand drug safety, 25(12):1425–1433.\nDominik A. Ettlin, Miriam Wolf, Nikola Biller-\nAndorno, and Gerhard Schneider. 2025. In pa-\ntients’ words: natural language processing of\nreports from patients experiencing orofacial pain\nand dysfunction. The Journal of Headache and\nPain, 26(1):172.\nNeele Falk and Gabriella Lapesa. 2024. Stories\nand personal experiences in the COVID-19 dis-\ncourse. In Proceedings of the 2024 Joint Interna-\ntional Conference on Computational Linguistics,\nLanguage Resources and Evaluation (LREC-\nCOLING 2024), pages 15320–15340, Torino,\nItalia. ELRA and ICCL.\nMuskan Garg. 2023. Mental health analysis in so-\ncial media posts: A survey. Archives of Computa-\ntional Methods in Engineering, 30(3):1819–1842.\nHamideh Ghanadian, Isar Nejadgholi, and Hus-\nsein Al Osman. 2024. Socially aware synthetic\ndata generation for suicidal ideation detection\nusing large language models.\nIEEE Access,\n12:14350–14363.\nMomchil Hardalov, Arnav Arora, Preslav Nakov,\nand Isabelle Augenstein. 2022.\nA survey on\nstance detection for mis- and disinformation iden-\ntification.\nIn Findings of the Association for\nComputational Linguistics: NAACL 2022, pages\n1259–1277, Seattle, United States. Association\nfor Computational Linguistics.\nTamanna Hossain, Robert L. Logan IV, Arjuna\nUgarte, Yoshitomo Matsubara, Sean Young, and\nSameer Singh. 2020.\nCOVIDLies: Detecting\nCOVID-19 misinformation on social media. In\nProceedings of the 1st Workshop on NLP for\nCOVID-19 (Part 2) at EMNLP 2020, Online. As-\nsociation for Computational Linguistics.\nC. Hutto and Eric Gilbert. 2014. Vader: A parsimo-\nnious rule-based model for sentiment analysis\nof social media text. Proceedings of the Inter-\nnational AAAI Conference on Web and Social\nMedia, 8(1):216–225.\nMali Jin and Nikolaos Aletras. 2020. Complaint\nidentification in social media with transformer net-\nworks. In Proceedings of the 28th International\nConference on Computational Linguistics, pages\n1765–1771, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nSarvnaz\nKarimi,\nAlejandro\nMetke-Jimenez,\nMadonna Kemp, and Chen Wang. 2015. Cadec:\nA corpus of adverse drug event annotations.\nJournal of Biomedical Informatics, 55:73–81.\nPayam Karisani and Eugene Agichtein. 2018. Did\nyou really just have a heart attack? Towards\nrobust detection of personal health mentions in\nsocial media. In Proceedings of the 2018 World\nWide Web Conference, page 137–146, Republic\nand Canton of Geneva, CHE.\nHamed Khanpour and Cornelia Caragea. 2018.\nFine-grained emotion detection in health-related\nonline posts. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 1160–1166, Brussels, Bel-\ngium. Association for Computational Linguistics.\nJongin Kim, Byeo Rhee Bak, Aditya Agrawal, Ji-\naxi Wu, Veronika Wirtz, Traci Hong, and Derry\nWijaya. 2023. COVID-19 vaccine misinforma-\ntion in middle income countries. In Proceedings\nof the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 3903–\n3915, Singapore. Association for Computational\nLinguistics.\nAri Klein, Abeed Sarker, Masoud Rouhizadeh,\nKaren O’Connor, and Graciela Gonzalez. 2017.\nDetecting personal medication intake in Twitter:\nAn annotated corpus and baseline classification\nsystem. In BioNLP 2017, pages 136–142, Van-\ncouver, Canada,. Association for Computational\nLinguistics.\n"}, {"page": 11, "text": "Arjun Magge, Elena Tutubalina, Zulfat Miftahut-\ndinov, Ilseyar Alimova, Anne Dirkson, Suzan\nVerberne, Davy Weissenbacher, and Graciela\nGonzalez-Hernandez. 2021. DeepADEMiner: a\ndeep learning pharmacovigilance pipeline for ex-\ntraction and normalization of adverse drug event\nmentions on Twitter. Journal of the American\nMedical Informatics Association, 28(10):2184–\n2192.\nJustus Mattern, Yu Qiao, Elma Kerz, Daniel Wiech-\nmann, and Markus Strohmaier. 2021. FANG-\nCOVID: A new large-scale benchmark dataset\nfor fake news detection in German. In Proceed-\nings of the Fourth Workshop on Fact Extraction\nand VERification (FEVER), pages 78–91, Do-\nminican Republic. Association for Computational\nLinguistics.\nHui Xian Lynnette Ng, Roy Ka-Wei Lee, and Md Ra-\nbiul Awal. 2020. I miss you babe: Analyzing emo-\ntion dynamics during COVID-19 pandemic. In\nProceedings of the Fourth Workshop on Natural\nLanguage Processing and Computational Social\nScience, pages 41–49, Online. Association for\nComputational Linguistics.\nAzadeh Nikfarjam, Abeed Sarker, Karen O’Connor,\nRachel Ginn, and Graciela Gonzalez. 2015. Phar-\nmacovigilance from social media: mining ad-\nverse drug reaction mentions using sequence\nlabeling with word embedding cluster features.\nJournal of the American Medical Informatics As-\nsociation, 22(3):671–681.\nMichael J. Paul and Mark Dredze. 2012. A model for\nmining public health topics from Twitter. Health,\n11(1).\nFlor Miriam Plaza-del Arco, Alba A. Cercas Curry,\nAmanda Cercas Curry, and Dirk Hovy. 2024.\nEmotion analysis in NLP: Trends, gaps and\nroadmap for future directions. In Proceedings\nof the 2024 Joint International Conference on\nComputational Linguistics, Language Resources\nand Evaluation (LREC-COLING 2024), pages\n5696–5710, Torino, Italia. ELRA and ICCL.\nWinfried Rief, Arthur J Barsky, Ulrike Bingel, Bet-\ntina K Doering, Ralph Schwarting, Markus Wöhr,\nand Ulrike Schweiger. 2016.\nRethinking psy-\nchopharmacotherapy: The role of treatment con-\ntext and brain plasticity in antidepressant and\nantipsychotic interventions. Neuroscience and\nbiobehavioral reviews, 60:51–64.\nArkadiy\nSaakyan,\nTuhin\nChakrabarty,\nand\nSmaranda Muresan. 2021. COVID-fact: Fact\nextraction and verification of real-world claims\non COVID-19 pandemic.\nIn Proceedings of\nthe 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th\nInternational\nJoint\nConference\non\nNatural\nLanguage Processing (Volume 1: Long Papers),\npages 2116–2129,\nOnline. Association for\nComputational Linguistics.\nHiromasa Sakurai and Yusuke Miyao. 2024. Eval-\nuating intention detection capability of large lan-\nguage models in persuasive dialogues. In Pro-\nceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume\n1: Long Papers), pages 1635–1657, Bangkok,\nThailand. Association for Computational Linguis-\ntics.\nAbeed Sarker, Karen O’Connor, Rachel Ginn,\nMatthew Scotch, Karen Smith, Dan Malone, and\nGraciela Gonzalez. 2016. Social media mining\nfor toxicovigilance: automatic monitoring of pre-\nscription medication abuse from Twitter. Drug\nsafety, 39(3):231–240.\nSanja Scepanovic, Enrique Martin-Lopez, Daniele\nQuercia, and Khan Baykaner. 2020. Extracting\nmedical entities from social media. In Proceed-\nings of the ACM Conference on Health, Inference,\nand Learning, CHIL ’20, pages 170–181. Asso-\nciation for Computing Machinery. Event-place:\nToronto, Ontario, Canada.\nTiberiu Sosea, Chau Pham, Alexander Tekle,\nCornelia Caragea, and Junyi Jessy Li. 2022.\nEmotion analysis and detection during COVID-\n19. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages\n6938–6947, Marseille, France. European Lan-\nguage Resources Association.\nAnthony Stefanidis, Emily Vraga, Georgios Lam-\nprianidis, Jacek Radzikowski, Paul L Delamater,\nKathryn H Jacobsen, Dieter Pfoser, Arie Croitoru,\nand Andrew Crooks. 2017. Zika in twitter: Tempo-\nral variations of locations, actors, and concepts.\nJMIR Public Health Surveill, 3(2):e22.\nMegha Sundriyal,\nAtharva Kulkarni,\nVaibhav\nPulastya,\nMd. Shad Akhtar,\nand Tanmoy\nChakraborty. 2022.\nEmpowering the fact-\ncheckers!\nautomatic identification of claim\nspans on Twitter. In Proceedings of the 2022\nConference on Empirical Methods in Natural\nLanguage Processing, pages 7701–7715, Abu\nDhabi, United Arab Emirates. Association for\nComputational Linguistics.\nElsbeth Turcan, Smaranda Muresan, and Kathleen\nMcKeown. 2021. Emotion-infused models for\nexplainable psychological stress detection. In\nProceedings of the 2021 Conference of the North\n"}, {"page": 12, "text": "American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, pages 2895–2909, Online. Association for\nComputational Linguistics.\nJuraj Vladika,\nPhillip Schneider,\nand Florian\nMatthes. 2024. HealthFC: Verifying health claims\nwith evidence-based medical fact-checking. In\nProceedings of the 2024 Joint International Con-\nference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING\n2024), pages 8095–8107, Torino, Italia. ELRA\nand ICCL.\nSomin Wadhwa, Vivek Khetan, Silvio Amir, and\nByron Wallace. 2023.\nRedHOT: A corpus of\nannotated medical questions, experiences, and\nclaims on social media. In Findings of the Associ-\nation for Computational Linguistics: EACL 2023,\npages 809–827, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nAnn M. Weber, Ribhav Gupta, Safa Abdalla, Beni-\namino Cislaghi, Valerie Meausoone, and Gary L.\nDarmstadt. 2021. Gender-related data missing-\nness, imbalance and bias in global health sur-\nveys. BMJ global health, 6(11).\nAmelie\nWührl,\nLynn\nGreschner,\nYarik\nMen-\nchaca Resendiz, and Roman Klinger. 2024.\nIMS_medicALY at #SMM4H 2024: Detecting im-\npacts of outdoor spaces on social anxiety with\ndata augmented ensembling. In Proceedings of\nThe 9th Social Media Mining for Health Research\nand Applications (SMM4H 2024) Workshop and\nShared Tasks, pages 83–87, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAmelie Wührl and Roman Klinger. 2022. Recover-\ning patient journeys: A corpus of biomedical enti-\nties and relations on Twitter (BEAR). In Proceed-\nings of the Thirteenth Language Resources and\nEvaluation Conference, pages 4439–4450, Mar-\nseille, France. European Language Resources\nAssociation.\nYosuke Yamagishi and Yuta Nakamura. 2024.\nUTRad-NLP at #SMM4H 2024:\nWhy LLM-\ngenerated texts fail to improve text classification\nmodels. In Proceedings of the 9th Social Me-\ndia Mining for Health Research and Applications\n(SMM4H 2024) Workshop and Shared Tasks,\npages 42–47, Bangkok, Thailand. Association\nfor Computational Linguistics.\nZhijun Yin, Daniel Fabbri, S Trent Rosenbloom, and\nBradley Malin. 2015. A scalable framework to de-\ntect personal health mentions on Twitter. Journal\nof Medical Internet Research, 17(6):e138.\n"}, {"page": 13, "text": "A.\nAppendix\nA.1.\nPrompt Design\nIn this section, we provide the prompts used for our annotation pipeline. We employ two types of\nprompts: (1) a binary classification prompt for detecting the presence of Expectation Events (EEs) , and\n(2) a structured extraction prompt for identifying Treatment–Expectation–Outcome (TEO) triplets. Both\nprompts are designed to guide the model toward consistent and task-specific outputs, with clearly defined\ninstructions, constraints, and output formats. The expectation detection prompt targets the identification\nof outcome-related expectations in text. The TEC prompt focuses on extracting fine-grained relational\ninformation between treatments, expectations, and outcomes. The full prompts are provided below for\nreproducibility.\nA.1.1.\nExpectation Detection Prompt\nSystem:\nYou are an expert annotator. Your task is to determine whether the text\nexpresses an \"Expectation Event (EE).\"\n,→\nDefinition of Expectation Event (EE):\nAn EE is a statement or implication about a possible or likely outcome (benefit,\nharm, no change, or worsening of condition) tied to a treatment, medication,\nor care action.\n,→\n,→\nExpectations may be prospective (future-oriented, concerning possible or likely\noutcomes such as benefit, harm, no change, or worsening) or retrospective\n(recalling a past belief about what was expected, in relation to the outcome\nthat actually occurred).\n,→\n,→\n,→\nImportant:\n- The expectation must concern the **treatment outcome**, not a possible\ndiagnosis, appointment, or unrelated event.\n,→\n- Outcome-related expectations include:\n- Benefit: expected symptom relief, functional improvement, disease control.\n- Adverse Effects: expected side effects or emotional impact.\n- No Effect: skepticism about benefit.\n- Worsening: anticipated deterioration or progression.\n- Expectations can be:\n- Prospective (future-oriented) or retrospective (recalling a past belief).\n- Explicit (directly stated) or implicit (implied through goals/conditions).\n- Positive, negative, neutral, or mixed.\nHere are some examples:\nExpectation about treatment outcome counts:\n- ``I expect this new medication will reduce my pain within two weeks.\"\n- ``If the surgery works, I can get back to work.\"\n- ``I was worried the drug would make me nauseous.\" (retrospective)\nNon-outcome expectation does NOT count:\n- ``I expect to see my doctor again next month for a check-up.\"\n- ``I expect they will finally diagnose me with lupus.\"\nYour output should strictly be in JSON format:\n- {{\"expectation\": true}}\n- {{\"expectation\": false}}\nRespond ONLY with the JSON object, nothing else. Answer with very minimal\nreasoning.\n,→\nAvoid step-by-step explanations unless strictly necessary.\nUser:\nThe given text is a Reddit post where a person shares their experience related\nto a medical condition or treatment.\n,→\nDetermine whether the text expresses an Expectation Event (EE).\n"}, {"page": 14, "text": "Strictly respond ONLY with a JSON of format: {{\"expectation\": true}} or\n{{\"expectation\": false}}.\n,→\nText: {post}\nA.1.2.\nTEC Prompt Design\nSystem:\nYou are an expert annotator. Extract all (Treatment, Expectation, Outcome)\ntriplets (TEC) from a Reddit post and, if present, the author's own\nfollow-up comments.\n,→\n,→\nDefinitions:\n- Treatment: any medication, surgery, therapy, care action, or dosage/regimen\nchange.\n,→\n- Expectation: the author's belief/anticipation about what the treatment will do\n(benefit, harm, no effect), explicit or implicit. Return exact spans if\nexplicitly stated; summarize if only implied.\n,→\n,→\n- Outcome (observed_outcome): the realized/actual result reported after the\ntreatment (improvement, worsening, side effect, or no change).\n,→\nRules:\n1. UNIT OF ANNOTATION = Triplet\n- Each triplet must connect exactly ONE treatment to ONE expectation to ONE\noutcome (observed_outcome).\n,→\n- Never mix expectations/outcomes from different treatments.\n- Return exact spans if explicitly specified, summarize if implicitly\nspecified, else return null.\n,→\n2. EXPECTATION REQUIREMENT\n- Must be about treatment outcomes (benefit, harm, or no effect) that the\npatient is undergoing or had undergone.\n,→\n- Strictly exclude diagnosis expectations, appointments, or logistics.\n- Both explicit and implicit expectations count.\n- Explicit: ``Antibiotics will clear this up.\"\n- Implicit: ``Antibiotics didn't help\" (implies expectation they should\nhave helped).\n,→\n- If no expectation is expressed for a treatment, no triplet is created.\n3. OBSERVED OUTCOME\n- observed_outcome = what actually happened after the treatment (symptom\nchange or side effect).\n,→\n- If only an expectation is given and no result is mentioned, set\nobserved_outcome = null.\n,→\n- If both an expectation and a statement about what happened are present, use\nthat statement as observed_outcome.\n,→\n- If the author later provides the result in a follow-up comment, update the\ntriplet with that observed_outcome.\n,→\n- IMPORTANT: Receiving or taking the treatment itself (e.g., ``got the flu\nshot\", ``took the pill\") is NOT a treatment related outcome.\n,→\n4. TREATMENT REQUIREMENT\n- Must be a concrete intervention (drug, surgery, therapy, self-care,\ndosage/regimen change).\n,→\n- Vague ``it/this\" counts only if clearly referring to a treatment.\n- If expectation/outcome exists without a treatment, set treatment = null.\n5. NULL HANDLING (STRICT)\n- If one component is missing, set it explicitly to null.\n- Example: ``Let's see if it works\" -> treatment = null, expectation = ``see\nif it works\", observed_outcome = null.\n,→\n6. COMMENTS\n"}, {"page": 15, "text": "- Look into the author's own follow-up comments for additional\nexpectations/outcomes.\n,→\n- Ignore generic conversational replies (e.g., ``This is miserable\", ``Did\nyou get surgery?\").\n,→\n- If later comments update the outcome for the same treatment/regimen, merge\ninto the original triplet (do not duplicate).\n,→\n- If a comment introduces a new treatment/regimen, create a separate triplet.\n7. MULTIPLE TRIPLETS\n- If multiple treatments/expectations/outcomes appear, output all of them\nseparately.\n,→\n- Do not merge across different treatments.\n8. IF NOTHING FOUND\n- If no valid triplets are detected, return:\n{ \"triplets\": [] }\nOutput format:\n{ \"triplets\": [ { \"treatment\": \"...\", \"expectation\": \"...\", \"observed_outcome\":\n\"...\" } ] }\n,→\nExamples:\nText: \"Doctor gave me antibiotics, I hope they clear my infection. Using a nasal\nspray too but it doesn't help my congestion.\"\n,→\n-> { \"triplets\": [\n{ \"treatment\": \"antibiotics\", \"expectation\": \"hope they clear my\ninfection\", \"observed_outcome\": null },\n,→\n{ \"treatment\": \"nasal spray\", \"expectation\": \"should help with congestion\",\n\"observed_outcome\": \"no improvement\" }\n,→\n] }\nText: \"Started melatonin, hoping I'll sleep better.\"\nComment: \"Update: I'm sleeping through the night now.\"\n-> { \"triplets\": [\n{ \"treatment\": \"melatonin\", \"expectation\": \"hoping I'll sleep better\",\n\"observed_outcome\": \"sleeping through the night\" }\n,→\n] }\nText: \"Zoloft 25mg didn't do much. Upped to 50mg; mood is finally better.\"\n-> { \"triplets\": [\n{ \"treatment\": \"Zoloft 25mg\", \"expectation\": \"should help with mood\",\n\"observed_outcome\": \"no improvement\" },\n,→\n{ \"treatment\": \"Zoloft 50mg\", \"expectation\": \"should help with mood\",\n\"observed_outcome\": \"mood improvement\" }\n,→\n] }\nText: \"I thought it wouldn't work, but it did. Scheduled to see ENT next week.\"\n-> { \"triplets\": [\n{ \"treatment\": null, \"expectation\": \"thought it wouldn't work\",\n\"observed_outcome\": \"improvement\" }\n,→\n] }\nUse null for any missing field.\nRespond ONLY with the JSON object, nothing else.\nAvoid repeating the post. Keep reasoning to very low. Strictly avoid step by\nstep reasoning.\n,→\nUser:\nExtract all (Treatment, Expectation, Outcome) triplets from the Reddit post and\nauthor comments.\n,→\nIf any component is missing, set it explicitly to null.\nRespond ONLY with the JSON object, following this format exactly:\n"}, {"page": 16, "text": "{{ \"triplets\": [ {{ \"treatment\": \"...\", \"expectation\": \"...\",\n\"observed_outcome\": \"...\" }} ] }}\n,→\nAvoid repeating the post. Keep reasoning to very low. Strictly avoid step by\nstep reasoning.\n,→\nPOST: {post}\nCOMMENTS: {author_comments}\n"}]}