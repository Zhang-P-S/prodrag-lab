{"doc_id": "arxiv:2601.05353", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.05353.pdf", "meta": {"doc_id": "arxiv:2601.05353", "source": "arxiv", "arxiv_id": "2601.05353", "title": "GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting", "authors": ["Shovito Barua Soumma", "Hassan Ghasemzadeh"], "published": "2026-01-08T20:07:59Z", "updated": "2026-01-08T20:07:59Z", "summary": "Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.05353v1", "url_pdf": "https://arxiv.org/pdf/2601.05353.pdf", "meta_path": "data/raw/arxiv/meta/2601.05353.json", "sha256": "2263496d8e1319ecbf4bd136365d45ee27b9406a5488602f7a62feb7583440c3", "status": "ok", "fetched_at": "2026-02-18T02:22:22.759229+00:00"}, "pages": [{"page": 1, "text": "IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\n1\nGlyRAG: Context-Aware Retrieval-Augmented\nFramework for Blood Glucose Forecasting\nShovito Barua Soumma1,2, Student Member, IEEE, Hassan Ghasemzadeh1 Senior Member, IEEE\nAbstract‚Äî Accurate forecasting of blood glucose from\ncontinuous glucose monitoring (CGM) is essential for pre-\nventing dysglycemic events, thus enabling proactive dia-\nbetes management. However, current forecasting models\ntreat blood glucose readings captured using CGMs as a\npurely numerical sequence, either ignoring contextual in-\nformation or relying on additional sensors/modalities (e.g.,\ndietary intake, physical activity) that are difficult to col-\nlect and deploy at scale. Recently, large language models\n(LLMs) have shown promise for time-series forecasting\ntasks, yet their role as agentic context extractors in dia-\nbetes care remains largely unexplored. To address these\nlimitations, we propose GlyRAG, a context-aware, retrieval-\naugmented forecasting framework that derives semantic\nunderstanding of blood glucose dynamics directly from\nCGM traces without requiring additional sensor modalities.\nGlyRAG employs an LLM as a contextualization agent to\ngenerate clinically meaningful textual summaries. These\nsummaries are embedded by a language model and fused\nwith patch-based glucose representations in a multimodal\ntransformer architecture with a cross-modal translation\nloss that aligns textual and physiological embeddings. A\nretrieval module then identifies similar historical episodes\nin the learned embedding space and uses cross-attention\nto integrate these case-based analogues prior to making\na forecasting inference. Extensive evaluations on two real-\nworld type 1 diabetes cohorts (i.e., OhioT1DM and AZT1D)\nshow that GlyRAG consistently outperforms state-of-the-\nart methods, achieving up to 39% lower RMSE (Root Mean\nSquare Error) across various prediction horizons and a\nfurther 1.7% reduction in RMSE over the BGL-only baseline\n(i.e., same architecture without contextual input). Further\nclinical evaluation shows that GlyRAG places 85% of the\npredictions in safe zones and achieves an average of 51%\nimprovement in predicting dysglycemic events across both\ncohorts. These results indicate that LLM-based contextu-\nalization and retrieval over CGM traces can enhance the\naccuracy and clinical reliability of long-horizon glucose\nforecasting without the need for additional sensors, thus\nsupporting future agentic decision-support tools for dia-\nbetes management.\nIndex Terms‚Äî Wearables, continuous glucose monitor,\ndiabetes, forecasting, multimodal data, LLM, Transformer,\nRAG\n1College of Health Solutions, Arizona State University, Phoenix, AZ\n85004, USA. Emails: {shovito, hghasemz}@asu.edu.\n2School of Computing and Augmented Intelligence, Arizona State\nUniversity, Tempe, AZ 85281, USA.\nThis work was supported in part by the National Science Foundation\n(NSF) under grant IIS-2402650. The content is solely the responsibility\nof the authors and does not necessarily represent the official views of\nthe NSF and NIH.\nI. INTRODUCTION\nD\nIABETES mellitus is a chronic metabolic disorder af-\nfecting hundreds of millions of adults worldwide and\nits prevalence continues to rise, driven largely by aging\npopulations, sedentary lifestyles, and unhealthy diets. Type\n2 diabetes (T2D) accounts for the majority of cases and is\nclosely linked to obesity and physical inactivity [1], whereas\ntype 1 diabetes (T1D) is an autoimmune disease in which\ndestruction of pancreatic Œ≤-cells leads to lifelong dependence\non exogenous insulin [2], [3]. Globally, over 537 million\nadults live with diabetes, while an estimated 240 million\nremain undiagnosed for 4‚Äì7 years before detection [4]. Poor\nglycemic control‚Äîwith glucose frequently outside the target\nrange of roughly 70‚Äì180 mg/dL‚Äîsubstantially increases the\nrisk of micro and macrovascular complications, including\ncardiovascular disease, kidney failure, retinopathy, and neu-\nropathy, underscoring the need for tight day-to-day glucose\nmanagement in patients with diabetes [5], [6].\nAccurate forecasting of future blood glucose levels is\nessential for maintaining safe and stable glycemic control\nin diabetes management. Short-term predictions (e.g., 15‚Äì30\nminutes prediction horizon) enable timely interventions such\nas insulin dose adjustments, carbohydrate supplementation,\nand activity modifications to mitigate impending dysglycemic\nepisodes. However, extending prediction horizons (e.g., ‚â•60\nminutes) provides additional lead time to adjust therapy or\nbehavior, with the potential to reduce acute events and the\ncognitive burden of constant manual decision making [7].\nErrors at extreme glucose ranges carry the greatest clinical\nrisk, whereas modest deviations within the euglycemic band\nare less critical [8].\nContinuous glucose monitoring (CGM) has emerged as a\nkey enabling technology for such forecasting. Modern CGM\nsystems provide dense, near-real-time glucose measurements\nand are increasingly used not only by individuals with T1D but\nalso by those with T2D and even by people with prediabetes\nor at-risk individuals seeking to understand and improve their\nmetabolic health [9], [10]. Recent regulatory changes and\nover-the-counter availability have made CGM more accessible\nto the general public, creating an opportunity to leverage these\ndata streams for personalized feedback and lifestyle guidance.\nIn this setting, robust blood glucose forecasting models can\nserve as decision-support tools to maintain time-in-range,\nprevent extreme excursions, and promote healthier day-to-day\nbehaviors across the spectrum from T1D to T2D and predia-\narXiv:2601.05353v1  [cs.LG]  8 Jan 2026\n"}, {"page": 2, "text": "2\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nLLM Agent\n(e.g. GPT 4, TimesFM)\nTime \nSeries\n!ùíö\nPredict\nDNN\n(e.g. LSTM, Transformer)\nTime \nSeries\n!ùíö\nPredict\n(a) Forecast Time series \n(Existing approaches)\nTime \nSeries\n!ùíö\nPredict\nMulti-modal\nEncoder\nLLM Agent\n(e.g. GPT 4)\nText  Summary\nContextualization\nRAG\nAttention &\nFCN\n(b) Contextualize, Augment & Forecast \nTime series (GlyRAG)\nTraining\nFrozen\nFig. 1.\nApproaches of time-series forecasting using LLM: (a) Existing\nmethods either use LLM directly on CGM time series or depend on other\nsensors. (b) GlyRAG first uses an LLM agent to contextualize CGM win-\ndows, then fuses text and signal embeddings with retrieval-augmented\nattention for forecasting.\nbetes [11].\nPrior approaches: Early work on glucose prediction com-\npared classical time-series models (e.g., autoregressive and\nstate-space methods) with data-driven machine learning, show-\ning that learned models generally outperform linear base-\nlines, especially at longer horizons [12]. Recurrent neural\nnetworks‚Äîmost notably LSTMs and their variants then be-\ncame the dominant paradigm for sequential CGM forecast-\ning, often augmented with ancillary inputs such as insulin\nand carbohydrate records [13]. More recently, transformer-\nbased architectures and multitask learning frameworks have\ndemonstrated strong performance by leveraging self-attention\nmechanisms and multimodal covariates to extend predictive\nhorizons and improve temporal representation [14], [15].\nKey limitations: Despite recent progress, most exist-\ning models treat CGM primarily as a numerical sequence\nand either ignore contextual information or encode it in\na narrow, hand-engineered manner. Behavioral and clinical\ndrivers‚Äîsuch as meals, activity, stress, heart rate, illness, and\ntherapy changes‚Äîare only partially observed, and collect-\ning rich multimodal data from multiple sensors at scale is\nchallenging, making context-heavy models difficult to train\nand deploy in routine care [15]‚Äì[17]. Recent work in other\ntime-series domains shows that foundation models and mul-\ntimodal agents can enhance accuracy and interpretability via\ncontextual summaries and semantic reasoning, but their ap-\nplication to CGM forecasting remains limited by difficulties\nin extracting clinically meaningful context and designing ef-\nficient retrieval mechanisms [18], [19]. These gaps motivate\nframeworks that derive context understanding and semantic\nreasoning directly from CGM data, enabling models that\nrely solely on glucose traces to predict future values while\ncapturing the underlying physiological state transitions that\ndrive them. In current practice, these models act mainly as pas-\nsive predictors rather than agentic systems that continuously\ninterpret streaming CGM data, reason over similar past cases,\nand proactively support individualized intervention planning.\nNovel contributions: To address the above-mentioned gaps,\nwe propose GlyRAG, a context-aware, retrieval-augmented\nagentic forecasting framework designed specifically for ac-\ncurate blood glucose forecasting. In contrast to prior work\nthat either ignores context or depends on additional sensors,\nGlyRAG derives semantic context directly from glucose traces\nand uses it to guide long-horizon forecasting as shown in\nFig. 1. Our main contributions are:\n‚Ä¢ Autonomous Context Extraction: We use an LLM as\na contextualization agent to generate morphology-aware,\nclinically meaningful textual summaries from blood-\nglucose windows and project them back into the forecast-\ning pipeline, enabling context-aware prediction without\nrequiring extra modalities (e.g., wearables or additional\nclinical sensors).\n‚Ä¢ Retrieval-augmented (RAG) forecasting: We introduce\na retrieval module that searches a library of historical\nblood-glucose embeddings for similar patterns and fuses\nthem via cross-attention, operationalizing case-based rea-\nsoning.\n‚Ä¢ Clinical Validation: We further perform clinical evalu-\nation (e.g., clarke error grid, time-in-range etc.) on two\nlarge-scale datasets, showing that GlyRAG delivers clini-\ncally reliable long-horizon forecasts for decision support.\nAcross two T1D cohorts and multiple prediction horizons,\nextensive experiments and ablation studies show that GlyRAG\nachieves superior long-horizon accuracy and clinically relevant\nperformance compared with strong baselines, while providing\nmore interpretable, morphology-aware predictions suitable for\npatient-centric decision support.\nII. RELATED WORK\nA. Deep Learning & Multimodal Approaches\nBlood glucose forecasting has progressed from classical au-\ntoregressive models to deep neural architectures. Early LSTM-\nbased approaches dominated sequential CGM modeling by\ncapturing temporal dependencies [17], while hybrid CNN-\nLSTM methods combined local pattern extraction with re-\ncurrent processing [20], [21]. Several works have explicitly\ninjected clinical knowledge into glucose prediction pipelines.\nDe Bois et al. incorporate regulatory criteria into training via\na clinically weighted loss (gcMSE) and a progressive opti-\nmization scheme, trading raw accuracy for improved clinical\nacceptability of predictions [22]. Prendin et al. use explainable\nAI (SHAP) to reveal when similarly accurate LSTM models\ndiffer in physiological plausibility, arguing that interpretability\nis essential for safe decision support [23], while Marigliano\net al. demonstrate that CGM systems with predictive hypo-\nglycemia alarms can materially improve time-below-range,\nunderscoring the need for forecasting models that are both\naccurate and clinically aligned [7].\nRecognizing that glucose dynamics depend on multiple fac-\ntors, several studies have incorporated additional modalities.\nGluNet integrates CGM with insulin delivery and carbohydrate\nintake, while deep multitask LSTM frameworks jointly model\nglucose trajectories with meals and exercise streams to im-\nprove forecasting and hypoglycemia detection [16], [24]. More\nrecent multimodal systems‚Äîsuch as Hwang et al.‚Äôs DA-CMTL\nframework [14], time-aware cross-attention model [15], and\nGlucoNet [25]‚Äîfuse CGM with physiological and behavioral\nsignals (e.g., heart rate, accelerometry, electrodermal activity)\nand hybrid transformer architectures, achieving strong accu-\nracy but at the cost of dense multi-sensor logging, increased\n"}, {"page": 3, "text": "SOUMMA et al.: GLYRAG\n3\nMULTIMODAL ENCODER\nRAG MODULE\nPREDICTION LAYER\nGlyRAG\nData Collection\nForecasting Model\nData Preparation\nOutput\nMulti-Horizon Glucose \nPrediction \n(5 | 30 |60 min)\nOverlap \nRatio[90%]\nProgression\nWindowing\nPast 3Hr BGL Trend\nSummary\nThis metric has shown\nan overall upward trend,\nwith gradual increases\ninterrupted\nby\nbrief\nplateaus\nand\nsmall\ndips..\nBERT\nLLM Agent\n(e.g. GPT 4)\nInput\nBehavioural Feedback\nFig. 2.\nOverview of the proposed GlyRAG pipeline: Three-hour CGM windows are extracted and summarized by an LLM agent into short\nmorphology-aware text, which is embedded and fused with glucose patches in a multimodal encoder. A retrieval-augmented module then attends\nto similar historical patterns to generate multi-horizon forecasts that can be used for behavioral feedback and decision support.\nuser burden, and greater system complexity, which limit de-\nployment in routine care.\nB. LLM for Time-Series Forecasting (TSF)\nThe success of large language models in natural language\nprocessing has inspired their application to TSF. Models\nsuch as TimeGPT [26], Time-LLM [19], and TimesFM [18]\nshow that pretrained transformers can be adapted to diverse\nforecasting tasks via reprogramming or few-shot prompting,\nand achieve competitive performance on standard benchmarks.\nHowever, these approaches typically operate on raw time-\nseries values or learned numeric embeddings that differ sub-\nstantially from the textual distributions on which LLMs are\ntrained, limiting the extent to which they can exploit rich\nsemantic priors.\nRecent work has tried to bridge this gap by ‚Äútextualizing‚Äù\ntime series and adding simple metadata as prompts in a\nzero-shot setting, but the resulting context is often shallow\nand hand-crafted [27], [28].\nBuilding on this idea, TimeCAP introduces dual LLM\nagents and a multimodal encoder that contextualize, augment,\nand then predict discrete events from time series, showing\nsizable gains for classification tasks in weather, finance, and\naggregated healthcare signals [29]. Yet, these frameworks still\ntreat LLMs primarily as predictors and have not been adapted\nto patient-level blood-glucose forecasting, where long, contin-\nuous trajectories must remain clinically plausible over 30‚Äì60\nminutes and beyond. Moreover, these foundation models often\nstruggle to generalize to specialized domains like healthcare.\nIn our experiments, general-purpose models such as TimesFM\nalso show limited generalizability to CGM data ( Table II),\nunderperforming domain-specific baselines for long-horizon\nBGL prediction.\nUnlike TimesFM‚Äôs task-agnostic forecasting and Time-\nCAP‚Äôs event-classification focus, GlyRAG uses the LLM as\nan agentic contextualization module that guides multi-horizon\nprediction from CGM alone‚Äîaddressing the underexplored\nneed for context-aware BGL forecasting in healthcare. To\nthe best of our knowledge, this is the first work to de-\nploy an LLM-based contextual and retrieval framework for\nlong-horizon CGM forecasting.\nIII. MATERIALS AND METHODS\nA. Problem Formulation\nLet x denote blood glucose readings captured using a\ncontinuous glucose monitor (CGM):\nx = (x1, x2, . . . , xL),\nxt ‚ààR.\nwhere xt denotes the glucose value at time step t, and L is\nthe length of the input signal segment (i.e., time window). The\nforecasting task aims to predict future glucose values over a\nhorizon H, producing estimates\nÀÜy = (ÀÜyL+1, ÀÜyL+2, . . . , ÀÜyL+H)\nto\napproximate\nthe\nground\ntruth\nsequence\ny\n=\n(yL+1, yL+2, . . . , yL+H).\nWe\nformulate\nthis\ntask\nas\na\nsupervised sequence-to-sequence regression problem, where\nthe forecasting loss is measured by the Huber Loss with\nŒ¥ = 1.0:\nLforecast = 1\nH\nH\nX\nh=1\n‚ÑìŒ¥\n\u0000ÀÜyL+h ‚àíyL+h\n\u0001\n,\n(1)\nwhere the Huber function is given by:\n‚ÑìŒ¥(e) =\n(\n1\n2e2\nif |e| ‚â§Œ¥\nŒ¥\n\u0000|e| ‚àí1\n2Œ¥\n\u0001\nif |e| > Œ¥\n(2)\nand e = ÀÜyL+h ‚àíyL+h represents the prediction error at the\nforecast step h. We choose the Huber loss because it provides\nrobustness to outliers commonly present in CGM data due to\nsensor noise and physiological anomalies [30].\nUnlike conventional time-series forecasting that relies solely\non numerical sequences, blood glucose dynamics exhibit com-\nplex patterns influenced by physiological context‚Äîincluding\nglycemic variability, directional trends, and rate-of-change\ncharacteristics‚Äîthat are not directly captured in raw CGM\nvalues. We formulate this as a multimodal learning problem\nwhere contextual understanding augments numerical pattern\nrecognition and our framework incorporates contextual signals\nderived from large language models (LLMs) to augment the\ntime series and improve predictive robustness.\n"}, {"page": 4, "text": "4\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nPredictor (LSTM & FCN)\nText Summary \n(ùë∫ùíô)\nLLM Agent\nùë®ùíÑ(ùíô)\nLM (e.g., BERT)\nùìõùíïùíìùíÇùíèùíî\nInput\n(ùíô)\nùì©\nSearch (Eq. 5)\nTraining Set (ùìì)\nIn Context \nExample(ùìù)\nRAG\nMulti Head Self-Attention\nCross-Attention\nMulti-Head \nAttention1\nùë≤ùüè\nùë∏ùüè\nùëΩùüè\nMulti-Head \nAttention1\nùë≤ùüè\nùë∏ùüè\nùëΩùüè\n+\nOutput\n(-ùíö)\nFig. 3.\nOverall GlyRAG architecture: (a) An LLM agent gener-\nates a morphology-aware text summary from the input CGM window,\nwhich is encoded by a language model and fused with patch-based\nglucose embeddings in a multi-head self-attention encoder to pro-\nduce a joint context‚ÄìCGM representation (z). (b) The fused query\nembedding searches a retrieval index for K similar historical episodes;\ncross-attention branches combine the query with each neighbor, and\nthe aggregated retrieval-aware representation is fed to a predictor to\nforecast.\nB. Framework Overview\nAs shown in Fig. 3, GlyRAG consists of three main com-\nponents: (1) an LLM-based context generator that produces\ntextual descriptions of glucose dynamics such as rising trends,\nsharp drops, or oscillations, (2) a multimodal transformer\n(MMT) encoder that fuses context and time-series representa-\ntions through cross-attention with alignment constraints, and\n(3) a retrieval-augmented forecasting module that leverages\nsimilar historical patterns during inference. An overview of\nour framework‚Äîfrom CGM windowing and LLM-based con-\ntextualization to retrieval-augmented multimodal forecasting\nand behavioral feedback‚Äîis shown in Fig. 2, and the internal\narchitecture of the multimodal encoder and retrieval adapter is\ndetailed in Fig. 3.\nC. LLM-Based Context Generation\nWe employ a large language model MŒ∏ (e.g., GPT-4) as a\ncontextualization agent AC to generate textual summaries of\nglucose morphology. Given an input CGM sequence x, the\nagent produces a context summary sx:\nsx = AC(x) = MŒ∏(pC(x))\nwhere pC(x) is a prompt function designed to elicit clinically\nrelevant contextual information. The prompt instructs the LLM\nto analyze:\n1) Glycemic trends: Overall trajectory (rising, falling, sta-\nble)\n2) Rate of change: Gradients indicating rapid vs. gradual\ntransitions\n3) Clinical indicators: Time-in-range, Bolus characteristics,\nCarb intake\nOur prompt template is structured as follows:\nSystem Role: You are a medical assistant specializing in diabetes management\nand glucose monitoring. Your job is to analyze time-series glucose data along\nwith carbohydrate intake and insulin delivery information to predict future\ntrends and assist in managing the patient‚Äôs blood sugar levels.\nUser Task: Your task is to analyze glucose level readings recorded at 5-minute\nintervals over the last 3 hours, along with associated carbohydrate intake and\ninsulin administration data.\nData Summary in last 30 minutes: <Carbohydrate Intake>, <Total Insulin\nBolus>, <Food Bolus>, <Correction Bolus>, <Other Bolus>, <Current\nBGL>, <Time In Range>, <Trend>\nHistorical CGM values:\nx1|x2|x3|...|x36\nOutput Requirements: Based on this comprehensive data including glucose\nreadings, carbohydrate intake, and insulin delivery patterns, write a concise\nmedical summary that analyzes the patient‚Äôs blood sugar trends and forecasts\nthe likely trend for the next 60 minutes (twelve readings). Your report should\nbe limited to five sentences, describing: Past glucose trends, The impact\nof recent carbohydrate intake and insulin administration, Predicted future\ndirection (e.g., likely to rise, fall, or stabilize), Potential health impacts (e.g.,\nrisk of hypoglycemia or hyperglycemia), Brief consideration of insulin-on-\nboard and carbohydrate effects.\nUse qualitative descriptions rather than exact numerical values in the sum-\nmary.\nThe generated text summaries sx provide auxiliary signals\nthat complement raw numerical data, capturing qualitative\npatterns that may be overlooked by purely data-driven models.\nD. Multimodal Transformer (MMT) Encoder\nThe multimodal transformer encoder integrates contextual\ninformation derived from textual summaries with glucose\ntime-series representations. It operates in three stages: context\nembedding, time-series patch embedding, and cross-modal\nfusion.\n1) Context Embedding: The textual summary sx is pro-\ncessed through a pre-trained language model (LM) to generate\na contextual representation. We employ BERT [31] as the\ncontext encoder:\nztext = LM(sx) ‚ààRd‚Ä≤\nwhere we extract the [CLS] token embedding from the final\nlayer. This representation is then projected into the multimodal\nembedding space:\nÀÜzcontext = ztextWtext ‚ààRd\nwhere Wtext ‚ààRd‚Ä≤√ód is a learnable linear projection with d =\n512.\n2) Time-Series Patch Embedding: To capture local temporal\npatterns, the CGM input sequence x ‚ààRL is segmented into\nnon-overlapping patches, following the design principles of\nPatchTST [32]. Specifically, the sequence is divided into N\npatches of length Lp with stride Ls:\nx = [x(1), x(2), . . . , x(N)] ‚ààRN√óLp,\nN =\n\u0018L ‚àíLp\nLs\n\u0019\n+1\nFor our experiments, we use Lp = 4, corresponding to 60\nminutes at 5-minute sampling, with Ls = Lp . Each patch is\nthen projected into the latent space:\nz(i)\nbgl = x(i)Wbgl,\nWbgl ‚ààRLp√ód\n"}, {"page": 5, "text": "SOUMMA et al.: GLYRAG\n5\nCGM readings initially decline, reflecting effective glucose \nutilization or insulin action, but later rise steadily to a \npronounced peak. This upward phase indicates a strong post-\nmeal or rebound response, followed by a gradual reduction \ntoward the end of the sequence. The overall pattern suggests a \nshift from stable control to hyperglycemic elevation and then \nearly recovery. Given this trend, the short-term outlook points \nto a continued mild decrease as glucose regulation \nmechanisms take effect. Continuous monitoring is advised to \nconfirm sustained improvement and prevent a return to \nelevated levels.\nCGM readings over the past three hours exhibit a general upward \ntrend, with values consistently increasing from the initial \nmeasurements and peaking towards the end of the observed \nperiod. This trend suggests that the patient's blood sugar levels are \ncurrently elevated, indicating a risk of hyperglycemia if the trend \ncontinues unchecked. Given the recent peak, it is likely that the \nglucose levels will stabilize or experience a slight decline in the \nnext hour, provided no significant dietary or activity changes occur. \nHowever, if current behaviors persist, there may still be a \ncontinued risk of crossing into hyperglycemic territory. Close \nmonitoring is advised to prevent potential complications \nassociated with elevated blood sugar levels.\nThe CMG data over the past three hours shows a clear trend of \ndecreasing blood sugar levels, starting from elevated values and \ngradually declining to lower levels. This decline suggests that the \npatient may have been experiencing a postprandial drop in \nglucose, potentially indicating effective insulin action or \ncarbohydrate metabolism. Given the current downward trajectory, \nit is likely that the glucose levels will continue to decrease over \nthe next 60 minutes. If this trend persists, there is a risk of \nhypoglycemia, which could lead to symptoms such as dizziness or \nweakness. Close monitoring and appropriate interventions may be \nnecessary to prevent the patient from reaching dangerously low \nblood sugar \nTime (minutes)\nTime (minutes)\nTime (minutes)\nGlucose \n(mg/values)\n(a)\n(b)\n(c)\nFig. 4.\nQualitative effect of contextual summaries on glucose forecasting (three examples). Panels (a‚Äìc) show a 3-hour CGM window (blue),\n12-step/60-min forecasts from GlyRAG (red) and a baseline model (orange), and the ground truth future trajectory (green). Shaded bands indicate\nclinical ranges (low, target 70‚Äì180 mg/dL, high). The callout under each panel is the LLM-generated context summarizing morphology (e.g., post-\nmeal rise, rebound, early recovery or sustained decline). GlyRAG leverages this context to anticipate turning points and slope changes, closely\ntracking the subsequent decrease or stabilization, whereas the baseline tends to overshoot or miss reversals. These examples illustrate how\ncontextual reasoning improves longer-horizon, physiologically coherent forecasts.\nPH = 5 minutes\nPH = 30 minutes\nPH = 60 minutes\n(a)\n(b)\n(c)\nFlagged Risk\nFlagged Risk\nFlagged Risk\nFig. 5.\nGlyRAG glucose forecasts across prediction horizons (PH = 5, 30, 60 minutes). GlyRAG predictions (red) closely follow actual CGM traces\n(green) within shaded clinical zones, leveraging contextual morphology to anticipate peaks and nadirs. Dashed oval marks risk markers (hypo/hyper)\nwhere GlyRAG preserves turning-point fidelity even as the horizon lengthens.\nThe N patch embeddings are aggregated into a single\nglucose representation:\nÀÜzbgl = 1\nN\nN\nX\ni=1\nÀÜz i\nbgl ‚ààRd.\n3) Multi-Modal Fusion via Self-Attention: Finally, the contex-\ntual embedding ÀÜzcontext and the glucose embedding ÀÜzbgl are\nfused using multi-head self-attention (MHSA). MHSA allows\nthe model to weight contributions dynamically based on the\njoint signal, preserving modality identity while exchanging\ninformation. This enables the fused representation to cap-\nture long-range trends (e.g., slow drifts, postprandial arcs)\nand short-term variations (e.g., rapid drops), while reducing\ndirectional bias and stabilizing training. We first construct a\ncombined sequence representation:\nZ =\n\u0002\nÀÜzbgl ; ÀÜzcontext\n\u0003\n‚ààR2√ód.\nFor each attention head h ‚àà{1, . . . , H}, the query, key, and\nvalue matrices are computed as\nQh = ZWQh,\nKh = ZWKh,\nVh = ZWVh.\n(3)\nwith WQh, WKh, WVh ‚ààRd√ó d\nH . The attention operation is\nthen defined as\nah = softmax\n \nQhK‚ä§\nh\np\nd/H\n!\nVh\n(4)\nOutputs from all H heads are concatenated and projected:\nZfused =\n\u0002\na1 ; a2 ; . . . ; aH\u0003\nW O ‚ààR2√ód.\nwhere W O ‚ààRd√ód and H=4. The fused output is then pooled\n(e.g., mean over the two tokens) to obtain the multimodal\nrepresentation Zfused\n‚àà\nRd This fused embedding Zfused\ncaptures bidirectional interactions, enabling ÀÜzbgl to attend to\nÀÜzcontext, and vice versa. To further enforce consistency between\nthe two modalities, we introduce a cross-translational loss\nthat explicitly aligns contextual and physiological embeddings\n(described in Section III-D.4).\n4) Cross-Translational Loss: To ensure that embeddings\nfrom both modalities capture complementary information\nwhile maintaining semantic alignment, we introduce a cross-\ntranslational loss. This loss encourages representations from\n"}, {"page": 6, "text": "6\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\none modality to be linearly projected into the space of the\nother, reducing modality mismatch.\nFormally, let Ebgl and Ectx denote the glucose and context\nembeddings, respectively. For each modality k ‚àà{bgl, ctx},\nand the corresponding paired modality t Ã∏= k, we define:\nLtrans =\nX\nk‚àà{bgl,ctx}\nX\nt‚àà{bgl,ctx}\ntÃ∏=k\n\r\r Projk‚Üít(Ek) ‚àíEt\n\r\r2\n2\n(5)\nwhere Projk‚Üít is a learnable linear projection from modality\nk to modality t. This formulation ensures that the contextual\nand glucose signals remain mutually informative, thereby\nstabilizing multimodal fusion and enhancing forecasting gen-\neralization. It also mitigates modality collapse, preventing the\nmodel from disregarding one source of information.\nE. Retrieval-Augmented (RAG) Forecasting\n1) Neighbor Retrieval: Once the multimodal encoder is pre-\ntrained, we construct an embedding database from the training\nset. For each training sample (xj, yj), the fused representation\nis computed as\nzj = Encoder(xj, sxj) ‚ààRd,\nwhere sxj denotes the LLM-generated context corresponding\nto the input sequence xj. The collection of all embeddings\npaired with their outcomes forms the retrieval database\nD = {(zj, yj) : j = 1, . . . , |Train|}.\nDuring inference, a test input window xtest is passed through\nthe MMT to produce an embedding Ztest. Using cosine simi-\nlarity, the system retrieves the top-K most similar embeddings\nfrom the training set:\nN(Ztest) = {(Zjk, yjk) : jk ‚ààarg top-K\nj‚ààD\nZtest ¬∑ Zj\n‚à•Ztest‚à•‚à•Zj‚à•}\n(6)\n2) Cross-attention adapter (query‚Äìneighbor fusion): We then\nfused Ztest with each neighbor via cross-attention branches. For\nbranch i ‚àà1 ¬∑ ¬∑ ¬∑ K with mH heads:\nH(i) = Concat\n\u0010\nAttn\n\u0000ZtestW (i)\nQh, zjiW (i)\nKh, zjiW (i)\nVh\n\u0001\u0011mH\nh=1 W (i)\nH ,\nwhere the attention operation is defined as\nAttn(Q, K, V ) = Softmax\n\u0012QK‚ä§\n‚àö\nd\n\u0013\nV,\nwith learnable projection matrices W (i)\nQh, W (i)\nKh, W (i)\nVh ‚ààRd√ód\nand W (i)\nH ‚ààR(mHd)√ód.\nThe branch outputs are aggregated (e.g., mean or learned\nweights) to produce a retrieval-aware representation:\nzrag = Aggregate\n\u0000H(1), . . . , H(K)\u0001\n‚ààRd.\nThe forecast is produced by an MLP head over the query and\nretrieval features (transformer frozen; only the adapter/MLP is\nfine-tuned):\nÀÜy = fMLP([ Ztest ; zrag ])\nIn our experiments we set K = 3. The retrieval mechanism\nenables the model to adapt predictions based on similar\nhistorical patterns, effectively implementing a form of case-\nbased reasoning that leverages the learned embedding space.\nAlgorithm 1 GlyRAG for BGL forecasting (Inference). xtest\n1:L ‚àà\nRL: input window and H: forecast horizon. AC(¬∑): contextualization\nagent; ELM: text encoder; Wctx: projection to d; Ebgl: CGM encoder\nto d; Fœâ: two-token fusion (MHSA) with params œâ; Aœà: retrieval\nadapter (neighbor cross-attention + mixing) with params œà; fœï :\nR2d ‚ÜíRH: forecast head with params œï.\nD = {(zj, yj)}n\nj=1: retrieval index storing fused representations and\ntargets from training; K: number of neighbors;\nInput: xtest\n1:L; retrieval index D; configuration (H, K)\nOutput: Forecast ÀÜy ‚ààRH\n1: Begin\n2: sx ‚ÜêMŒ∏(p(xtest\n1:L))\n3: Àúzctx ‚ÜêELM(sx)Wctx\n{morphology ‚Üícontext vector}\n4: ¬Øzbgl ‚ÜêEbgl(xtest\n1:L)\n{Encode CGM to d-dim}\n5: z ‚ÜêFœâ([¬Øzbgl; Àúzctx])\n{Fuse CGM + context}\n6: N(z) ‚Üêarg‚ä§K\nz‚ä§zj\n‚à•z‚à•2‚à•zj‚à•2 from D\n{K nearest neighbors}\n7: zrag ‚ÜêAœà(z, {zj}j‚ààN (z))\n{Cross-attn + mixing;}\n8: ÀÜy ‚Üêfœï([z; zrag])\n{Forecast H future points}\n9: ÀÜy ‚ÜêDenorm(ÀÜy)\n{Denormalization}\n10: return ÀÜy\n11: End\nIV. EXPERIMENTAL SETUP\nA. Datasets\nWe evaluated the proposed framework using two large-scale,\nreal-world T1D datasets that capture diverse physiological and\nbehavioral patterns under both conventional and automated\ninsulin delivery settings.\nWe chose the OhioT1DM [33] to demonstrate the results\nof our proposed methods. OhioT1DM was collected over\nan eight-week long clinical study of 12 deidentified T1\ndiabetes patients. Participants wore Medtronic 530G/630G\ninsulin pumps and Medtronic Enlite CGM that transmits blood\nglucose level every 5 minutes. Their physiological data (accel-\neration, skin response etc.) was recorded on either Basis Peak\nfitness band or Empatica Embrace while their self-reported\nCHO intake, work, exercise intensity and sleep quality were\nrecorded on smartphones. Of these eight weeks, roughly 44\nand 12 days were allocated for train and test respectively.\nMissing values were imputed with interpolation or extrapo-\nlation. This dataset provides a well-studied benchmark with\ndiverse day-to-day patterns (postprandial excursions, exercise\neffects, and insulin-related dynamics).\nTo assess generalization to contemporary automated insulin\ndelivery (AID) use, we also analyze AZT1D [34], a new\nclinic-sourced dataset collected at Mayo Clinic (Scottsdale,\nAZ) between Dec 2023‚ÄìApr 2024. The cohort includes 25\nadults (13 female, 12 male; age 27‚Äì80, mean 59), each\ncontributing on average 26 days of real-world data. AZT1D\ncontains Dexcom G6 Pro CGM (5-min sampling), Tandem\nt:slim X2 pump logs (including granular bolus details: total\ndose, bolus type, correction components), carbohydrate intake,\nand device mode (regular/sleep/exercise). In total, the release\ncomprises 320,488 CGM readings spanning\n26,707 hours.\nCompared with OhioT1DM, AZT1D offers richer therapy\ncontext under AID, enabling studies on morphology-aware\nforecasting, decision support, and patient-twin personalization.\n"}, {"page": 7, "text": "SOUMMA et al.: GLYRAG\n7\nWindow Size [3Hr]\nOverlap Ratio[90%]\nProgression\nTrain 80%\nTest  20%\nBlood Glucose\nGlucose Readings \nover last 3 hours\nExtract Context \nBGL History[3Hr]\nForecast\n[5/30/60 min]\nGround Truth\nBGL History\nFinal Input\nWINDOWING\nEach Sample\nFeature Extraction\nInsulin\nand\nCarb\nintakes\nsummary\nduring\nlast\n30\nminutes\nContext Summary\nGlyRAG\nFig. 6.\nPreprocessing and context-extraction workflow: CGM\nstreams are segmented into overlapping 3-h windows with 5/30/60-min\ntargets; an LLM summarizes each window into a context token that is\nconcatenated with the 3-h glucose history to form GlyRAG‚Äôs input.\nB. Preprocessing\nThe dataset is divided into training and testing datasets\nfor each participant. The models are trained on the training\ndataset. Prediction metrics are computed using the test dataset\nfor each participant. The model‚Äôs input consists of a three-hour\n(180 minutes) sliding window of historical data, providing\nsufficient information for making good predictions for the next\nprediction horizon (PH). Three different PHs (5, 30 and 60\nminutes) are considered for comparison.\nEach patient‚Äôs raw CGM sequence is first segmented into\nfixed-length windows of length L, corresponding to the input\nhorizon for forecasting. For our main experiments, we use\nL = 36, which represents a three-hours history of glucose\nvalues. Each input window is paired with prediction targets at\n5-, 30-, and 60-minute horizons. To standardize across patients,\nglucose values are normalized to zero mean and unit variance\nwithin each individual‚Äôs data before training and evaluation.\nC. Training Objective\nThe overall training objective is a weighted combination of\nthe forecasting loss and the cross-translational loss:\nL = Lforecast + ŒªLtrans\nwhere the hyperparameter Œª balances predictive accuracy\nwith modality alignment. This formulation ensures that the\nmodel not only forecasts accurately but also learns meaning-\nful relationships between glucose trajectories and contextual\nsummaries.\nD. Evaluation Metrics\nTo comprehensively assess forecasting performance, we\nemploy four complementary evaluation metrics: root mean\nsquare error (RMSE), mean absolute error (MAE), and Clarke\nError Grid Analysis (CEGA) and Pearson correlation coeffi-\ncient (r). These metrics jointly capture numerical accuracy,\nclinical relevance, and correlation strength between predicted\nand actual glucose values.\n1) Root Mean Square Error (RMSE): The RMSE measures\nthe average magnitude of prediction errors with a higher\npenalty on large deviations, providing insight into worst-case\npredictive performance:\nRMSE =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n\u0000yi ‚àíÀÜyi\n\u00012\nwhere n is the number of test samples, yi denotes the ground-\ntruth CGM value, and ÀÜyi is the predicted value.\n2) Mean Absolute Error (MAE): The MAE captures the\naverage absolute deviation between predicted and observed\nglucose values, offering a straightforward interpretation of\nprediction accuracy:\nMAE = 1\nn\nn\nX\ni=1\n\f\fyi ‚àíÀÜyi\n\f\f\n3) Clarke Error Grid (CEGA): To evaluate the clinical rele-\nvance of predictions, we use Clarke Error Grid Analysis [41],\nwhich classifies predicted glucose values into five zones (A‚ÄìE)\nbased on their proximity to reference values. Zone A repre-\nsents clinically accurate predictions, Zone B represents benign\nerrors, while Zones C‚ÄìE indicate increasingly dangerous mis-\nclassifications that could adversely affect treatment decisions.\nCEGA is widely used in diabetes research to assess the safety\nof predictive algorithms.\n4) Pearson Correlation Coefficient (r): The Pearson correla-\ntion quantifies the linear relationship between predicted and\nactual glucose values:\nr =\nPn\ni=1(yi ‚àí¬Øy) (ÀÜyi ‚àí¬ØÀÜy)\npPn\ni=1(yi ‚àí¬Øy)2\nqPn\ni=1(ÀÜyi ‚àí¬ØÀÜy)2\nwhere ¬Øy and ¬ØÀÜy are the sample means of the actual and predicted\nglucose values, respectively.\nE. Architecture Configuration\nAt first, CGM windows are patch-embedded (patch length\n= 6, stride = 3) with sinusoidal positional encoding and\nprocessed by a Transformer encoder (dmodel = 512, nlayers = 3,\nnheads = 4, dff = 2048, dropout = 0.05). A pre-trained\nlanguage model from the BERT family provides a 768-\ndimensional text vector, which is projected to dmodel and\nappended as a context token. Encoder outputs are reshaped\nper variable and summarized by an LSTM forecast head (2\nlayers, hidden size = 256) that emits multi-horizon predic-\ntions. During pretraining, we use a bidirectional cross-modal\ntranslator (3-layer MLPs, translation hidden size = 512) with\na weighted translation loss (Œ± = 0.1), combined with huber\nforecasting loss.\nWe then freeze the backbone and index encoder embeddings\nfor retrieval using cosine similarity (k = 3). A compact cross-\nattention module is applied over retrieved analogs (separate\nmulti-head attention per neighbor with residual connections\nand feed-forward layers), and its output is passed through a\nsmall LSTM. This retrieval hidden state is concatenated with\nthe pooled embedding from the LSTM forecast head and fed\ninto a final MLP prediction head.\nV. RESULTS\nWe evaluated the proposed GlyRAG framework against rep-\nresentative state-of-the-art blood glucose forecasting models\non two benchmark datasets under multiple prediction horizons.\nAll methods were re-implemented or evaluated following their\noriginal experimental settings, and performance was assessed\nusing standard forecasting error metrics as well as clinically\nmotivated measures reported in subsequent tables.\n"}, {"page": 8, "text": "8\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nTABLE I\nCOMPARISON OF GLYRAG WITH STATE-OF-THE-ART FORECASTING BLOOD GLUCOSE MODELS. THE BEST VALUES ARE COLORED IN RED. BGL:\nBLOOD GLUCOSE, I: INSULIN, C: CARB, BASELINE: BGL-ONLY FORECASTER WITHOUT CONTEXTUAL OR RETRIEVAL COMPONENTS.\nPrediction Horizon (PH)\n5 min\nNext Sample\n30 min\n60 min\nOhio Dataset\nStudy\nModalities\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\n2018\nLSTM [35]\nBGL\n-\n-\n18.86\n-\n31.40\n-\nGluNet [16]\nBGL, I, C\n-\n-\n19.28\n-\n31.82\n-\nMTL-LSTM [17]\nBGL, I, C\n-\n-\n15.73\n10.43\n30.01\n21.36\nTimesFM [18]\nBGL\n2.91\n1.89\n10.95\n6.54\n20.72\n12.78\nBaseline\nBGL\n2.02\n1.16\n10.63\n6.29\n19.80\n12.45\nOurs\nBGL, Context\n1.89\n1.13\n10.48\n6.13\n19.57\n12.14\n2020\nCNN-RNN [20]\nBGL, I, C\n-\n-\n17.54\n11.22\n33.67\n23.25\nDeep Residual [36]\nBGL, FG, BI, ToD, C\n-\n-\n18.22\n12.83\n31.66\n23.60\nKnowledge Distillation [37]\nBGL, I, C\n-\n-\n19.21\n13.08\n31.77\n23.09\nMTL-LSTM [17]\nBGL, I, C\n16.39\n10.86\n31.78\n22.77\nTimesFM [18]\nBGL\n3.52\n2.02\n11.97\n6.85\n22.70\n13.33\nBaseline\nBGL\n2.04\n1.22\n10.77\n6.32\n21.19\n12.86\nOurs\nBGL, Context\n2.05\n1.22\n10.73\n6.24\n20.87\n12.51\n2018, 2020\nRecurrent Self-Attention [38]\nBGL, I, C\n-\n-\n17.82\n-\n28.54\n-\nCNN-RNN [24]\nBGL, I, C, E\n-\n-\n18.8\n13.2\n31.8\n23.4\nMTL-LSTM [17]\nBGL, I, C\n-\n-\n16.06\n10.64\n30.89\n22.07\nGlySim [39]\nBGL, I, C\n11.3\n8.2\n17.5\n12.3\n24.2\n16.5\nGLIMMER [40]\nBGL\n-\n-\n-\n-\n23.97\n15.83\nTimesFM [18]\nBGL\n3.22\n1.96\n11.46\n6.69\n21.71\n13.05\nBaseline\nBGL\n2.03\n1.19\n10.70\n6.31\n20.49\n12.65\nOurs\nBGL, Context\n1.97\n1.83\n10.61\n6.19\n20.22\n12.33\n559\n563\n570\n588\n575\n591\n540\n552\n544\n567\n584\n596\nGlyRAG\nTimeFM\nGlySIM\nMTL+LSTM\n2018\n2020\nPID\n30 minutes\n60 minutes\n5 minutes\nFig. 7.\nPatient-wise RMSE Comparison Across Prediction Horizons for\nGlyRAG and Baselines.\nA. Compare with State-of-the-Art (SOTA)\nTable I summarizes the quantitative comparison in terms\nof RMSE and MAE at 5-, 30-, and 60-minute prediction hori-\nzons. The compared methods employ diverse input modalities,\nincluding blood glucose (BGL), insulin (I), and carbohydrate\nintake (C); however, none explicitly incorporate CGM contex-\ntual information. In contrast, GlyRAG integrates CGM-derived\ncontext while using the same core glucose signal.\nAcross all evaluated settings and horizons, GlyRAG con-\nsistently achieves lower or competitive error compared to\nprior approaches and the baseline e.g., at the longest horizon\n(PH=60min) GlyRAG reduces RMSE by 1.16% on the 2018\ncohort and 1.51% on the 2020 cohort (relative to the baseline),\ndemonstrating improved forecasting accuracy without reliance\non additional physiological inputs. Error values are reported\nas averages to ensure a fair comparison across studies and\ndatasets. These results indicate that incorporating CGM con-\ntext enhances predictive performance and provides a strong\nfoundation for the subsequent clinical reliability analysis.\nB. Clinical Evaluation\nAccurate clinical evaluation of glucose forecasting models\nrequires metrics that capture both numerical precision and\nmedical relevance. Sensitivity analysis measures the model‚Äôs\nability to detect critical dysglycemic events‚Äîhypoglycemia\nand hyperglycemia‚Äîwhere early intervention is vital for pa-\ntient safety. The Clarke Error Grid (CEG) and Continuous\nGlucose‚ÄìError Grid Analysis (CG-EGA) are widely used\nclinical frameworks that assess how prediction errors may\ntranslate into treatment risks. While CEG evaluates the clin-\nical safety of predicted glucose values across zones A‚ÄìE,\nCG-EGA jointly considers point and rate-of-change errors\nto examine the physiological coherence of temporal trends.\nComplementary to these, Time-in-Range (TIR) quantifies how\nwell predicted glucose levels align with the clinically optimal\nrange (typically 70‚Äì180 mg/dL), serving as an aggregate\nmeasure of overall glycemic stability. Together, these metrics\nprovide a comprehensive evaluation of forecasting reliability,\nsafety, and clinical utility in diabetes management.\n1) Clarke Error Grid and TIR Analysis: Table II summarizes\nclinical performance at PH = 60 min using clarke error grid\n(CEG), event sensitivities, pearson correlation (r), and time-\nin-range (TIR) deviation for the Ohio and AZT1D cohorts.\n"}, {"page": 9, "text": "SOUMMA et al.: GLYRAG\n9\nTABLE II\nCLINICAL EVALUATION AT PH = 60 MINUTES. HYPER-/HYPOGLYCEMIA\nSENSITIVITY AND CEG DISTRIBUTIONS (ZONES A‚ÄìE). GLYRAG\nCONCENTRATES PREDICTIONS IN ZONES A‚ÄìB WITH COMPETITIVE\nSENSITIVITY AND SHOWS SMALLER TIR DEVIATION THAN THE\nBASELINE, INDICATING CLINICALLY RELIABLE FORECASTING.\nOhio Dataset, PH = 60 minutes\nMethod\nSensitivity\nClarke Error Grid Regions (%) P-Coeff\nHyper Hypo\nA‚Üë\nB‚Üì\nC‚Üì\nD‚Üì\nE‚Üì\n(r)\nGLIMMER [40] 86\n42\n85.4\n13.26 0.13 1.10\n0.02\n0.938\nCNN-LSTM\n78\n16\n74.31 23.12 0.11 2.43\n0.03\n0.902\nTimesFM [18]\n97\n81\n75.95 22.36 0.29 1.33\n0.07\n0.918\nGlyRAG\n97\n92\n85.53 13.59 0.15 0.83\n0.025\n0.942\nAZT1D Dataset, PH = 60 minutes\nMethod\nSensitivity\nClarke Error Grid Regions (%) P-Coeff\nHyper Hypo\nA‚Üë\nB‚Üì\nC‚Üì\nD‚Üì\nE‚Üì\n(r)\nGLIMMER [40]\n73\n13\n83.89 14.94 0.02 1.12\n0.02\n0.831\nCNN-LSTM\n48\n2\n73.27 24.47 0.03 2.21\n0.02\n0.814\nTimesFM [18]\n91.7\n60.4\n69.94 28.43 0.26 1.27\n0.09\n0.821\nGlyRAG\n92\n44.2\n84.9\n13.6\n0.04 0.38\n0.02\n0.836\nTime In Range (TIR) difference between observed and predicted\nMethod\nOhio\nAZT1D\nGlyRAG\n0.91¬±0.85\n0.70¬±0.60\nBaseline\n1.27¬±0.96\n0.97¬±0.78\nGlyRAG concentrates most predictions in clinically acceptable\nCEG Zones A‚ÄìB and maintains low proportions in the clin-\nically dangerous Zones D‚ÄìE, indicating a reduced likelihood\nof forecasts that could lead to inappropriate treatment actions.\nGlyRAG also shows balanced sensitivity to hypo- and hy-\nperglycemic events‚Äîespecially hypoglycemia, where timely\ndetection is critical‚Äîwhile preserving specificity and temporal\nagreement (higher r) with observed traces. For example, in\nthe Ohio cohort, GlyRAG achieves 14% improvement in\nhypoglycemia sensitivity compared with the best prior model,\nTimesFM. The comparatively lower hypoglycemia sensitivity\nobserved in AZT1D is attributable to the limited number of\nhypoglycemic events in that cohort, a known challenge for\nevent-based evaluation that reduces statistical power rather\nthan indicating systematic model failure. We also compared\nGlyRAG to TimesFM-a foundation time-series model that uses\nglucose trajectories alone and found that explicitly modeling\nCGM-derived context yields superior clinical reliability.\nThe lower mean absolute TIR deviation (<1) versus the\nbaseline further confirms that GlyRAG better preserves over-\nall glycemic exposure over the forecast horizon. Together,\nthese results indicate GlyRAG delivers clinically reliable long-\nhorizon forecasts suitable for downstream decision support.\nOverall, GlyRAG shows lower mean absolute TIR deviation\nthan the baseline and concentrates predictions in safe CEG\nZones A‚ÄìB, indicating better preservation of overall glycemic\nexposure and fewer clinically risky forecasts. These results\nunderscore that context augmentation materially improves\nlong-horizon CGM forecasting for decision support.\n2) Continuous Glucose‚ÄìError Grid Analysis (CG-EGA): Ta-\nble III reports the CG-EGA at 60-minute PH, stratified by\nhypoglycemic, euglycemic, and hyperglycemic ranges for both\n(a)\nHorizon = 5min\nHorizon = 30min\nHorizon = 60min\nHorizon = 5min\nHorizon = 30min\nHorizon = 60min\nPID: 570\n(b)\nPID: 552\nFig. 8. Clarke Error Grid analysis for Patients 552 (a) and 570 (b) across\n1-, 6-, and 12-hour prediction horizons. Most predictions fall within Zone\nA, indicating high clinical accuracy, with minor dispersion into Zone\nB at longer horizons, showing slightly reduced but reliable forecasting\nperformance.\nFig. 9.\nPatient-level CG-EGA (point- and rate-error grids) for an Ohio\nparticipant (PID 512).\ndatasets. GlyRAG achieves high Accurate Prediction (AP)\nrates across glucose ranges, particularly in the euglycemic\nregion, while maintaining low Erroneous Prediction (EP) per-\ncentages; on average, the overall EP decreases from 1.9% to\n1.1% on Ohio and from 9.2% to 7.9% on AZT1D, indicating\nfewer clinically unsafe predictions. This pattern indicates that\nthe model preserves clinically acceptable trend and point\naccuracy without increasing the risk of misleading predictions\nduring physiologically stable periods. As illustrated in Fig. 9,\nthe patient-level CG-EGA for a representative Ohio subject at\nPH = 60 min shows that most forecasts (green AP points) fall\nin Zones A/B of both the point and rate-error grids, with only\na few BE/EP points, indicating clinically acceptable accuracy\nin both glucose values and rates of change.\nImportantly, GlyRAG maintains competitive AP and con-\ntrolled EP rates in hypoglycemic and hyperglycemic ranges,\nwhich are most relevant for safety-critical decision mak-\ning.\nAlthough\nvariability\nincreases\nin\nextreme\nglucose\nranges‚Äîconsistent with prior CGM forecasting studies‚Äîthe\nEP values remain limited, suggesting that contextual modeling\nhelps mitigate clinically unsafe trend errors. These CG-EGA\nresults complement the Clarke Error Grid (CEGA) findings\n"}, {"page": 10, "text": "10\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\nTABLE III\nCG-EGA RESULTS AT PH = 60 MIN FOR THE OHIO AND AZT1D COHORTS, STRATIFIED BY HYPOGLYCEMIC, EUGLYCEMIC, AND HYPERGLYCEMIC\nRANGES. METRICS REPORT ACCURATE PREDICTION (AP), BENIGN ERROR (BE), AND ERRONEOUS PREDICTION (EP) RATES; HIGHER AP AND\nLOWER EP INDICATE IMPROVED CLINICAL SAFETY. RED: BEST VALUES IN EACH CATEGORY.\nDataset\nHypo (‚â§70mg/dL)\nEu (70-180 mg/dL)\nHyper (‚â•180 mg/dL)\nAverage\nAP‚Üë\nBE\nEP‚Üì\nAP‚Üë\nBE\nEP‚Üì\nAP‚Üë\nBE\nEP‚Üì\nAP‚Üë\nBE\nEP‚Üì\nOhio\n92.5¬±5.3\n5.1¬±4.2\n2.3¬±2.1\n94.6¬±4.1\n5.0¬±3.4\n0.42¬±0.7\n92.2¬±6.9\n7.3¬±5.8\n0.7¬±1.0\n93.0¬±5.4\n5.8¬±4.5\n1.1¬±1.3\n(88.4¬±18) (7.06¬±9.2)\n(4.5¬±9.3)\n(94.4¬±3.9) (5.2¬±3.3) (0.39¬±0.6) (92.0¬±6.9)\n(7.2¬±5.8)\n(0.7¬±1.2) (91.6¬±9.6)\n(6.5¬±6.1)\n(1.9¬±3.7)\nAZT1D\n65.4¬±22.9 12.3¬±10.4\n19.0¬±18.7\n82.4¬±9.7\n16.1¬±8.6\n1.5¬±0.9\n71.6¬±18.0\n25.1¬±15.6\n3.4¬±2.9\n73.1¬±16.9 17.8¬±11.5\n7.9¬±7.5\n(65.7¬±23.2) (11.7¬±9.9) (22.53¬±20.7) (81.6¬±9.6) (16.9¬±8.8) (1.55¬±1)\n(70.7¬±18.1) (25.9¬±16.1) (3.4¬±2.6) (72.7¬±17) (18.7¬±11.8) (9.16¬±8.1)\nin Table II, where predictions are concentrated in Zones A‚ÄìB\nwith strong event sensitivity and reduced TIR deviation.\nC. Ablation Study\nTABLE IV\nABLATION STUDY TO SEE THE EFFECT OF DIFFERENT COMPONENTS IN\nGLYRAG\nOhio Dataset\nRAG\nContext\nBGL\nRMSE\nMAE\nCA\nCTL\n5\n30\n60\n5\n30\n60\n‚úî\n‚úî\n‚úî\n‚úî\n1.97\n10.61\n20.22\n1.43\n6.19\n12.33\n‚úî\n‚úó\n‚úî\n2.03\n10.90\n20.17\n1.2\n6.22\n12.29\n‚úó\n‚úî\n‚úî\n2.07\n10.84\n20.41\n1.26\n6.43\n12.56\n‚úó\n‚úî\n‚úî\n‚úî\n1.98\n10.95\n20.83\n1.81\n6.25\n12.42\n‚úó\n‚úó\n‚úó\n‚úî\n2.04\n11.71\n21.59\n1.18\n6.31\n12.65\nAZT1D Dataset\nRAG\nContext\nBGL\nRMSE\nMAE\nCA\nCTL\n5\n30\n60\n5\n30\n60\n‚úî\n‚úî\n‚úî\n‚úî\n4.17\n13.52\n22.32\n2.98\n9.48\n15.24\n‚úî\n‚úó\n‚úî\n4.18\n13.45\n22.46\n3.05\n9.11\n15.31\n‚úó\n‚úî\n‚úî\n4.60\n14.12\n23.01\n3.29\n9.69\n16.37\n‚úó\n‚úî\n‚úî\n‚úî\n4.19\n13.70\n23.07\n3.05\n9.53\n15.46\n‚úó\n‚úó\n‚úó\n‚úî\n4.22\n13.48\n23.52\n3.04\n9.17\n15.49\nTable IV evaluates the contribution of retrieval (RAG)\nand\ncontextual\ncomponents‚Äîcross-attention\n(CA)\nand\ncross-translation\nloss\n(CTL)‚Äîon\ntop\nof\na\nfixed\nBGL\nforecaster for both datasets. The full GlyRAG configuration\n(RAG + CA + CTL) consistently yields the best or near-best\nRMSE/MAE, reducing 60-min RMSE from 21.59 to 20.22 on\nOhio and from 23.52 to 22.32 on AZT1D (5% improvement\nover the baseline). Any setting that activates RAG and/or\ncontext improves upon the BGL-only model, but CA+CTL\nwithout RAG offers (4th row on\nTable IV) only modest\nchanges, indicating that cross-attention over retrieved analogs\nis necessary to fully exploit the context-aligned representation.\nWhen CTL is used without CA, performance degrades further\nrelative\nto\nthe\nfull\nmodel,\nsuggesting\nthat\nembedding\nalignment alone is insufficient in the absence of an explicit\ncontext token.\nFig.\n10\nshows\nthe\nsensitivity\nof\nGlyRAG\nto\nthe\ntranslation-loss weight Œ±, plotting mean RMSE across four\npatients at 5, 30 and 60-min horizons as Œ± varies from 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\nAlpha ( )\n4\n8\n12\n16\n20\nRMSE\nPrediction Horizon\n5 min\n30 min\n60 min\nFig. 10. Sensitivity of GlyRAG to Œ±. Mean RMSE across 4 patients at\n5-, 30-, and 60-minute horizons as the weighting parameter Œ± varies\nfrom 0.1 to 0.5. Performance remains largely insensitive to Œ±, with\nonly modest, horizon-dependent fluctuations‚Äîindicating robustness of\nGlyRAG to this hyperparameter.\nto 0.5. RMSE curves remain largely flat, with only small\nhorizon-dependent variations (the 30- and 60-min horizons\nachieving slightly lower error around Œ± ‚âà0.3), indicating\nthat GlyRAG is not overly sensitive to the exact choice of\nŒ±. This suggests that the cross-modal translation acts as a\nstable regularizer: moderate weighting is enough to benefit\nfrom the alignment objective without destabilizing forecasting\nperformance.\nVI. DISCUSSION AND FUTURE WORK\nThis study demonstrates that injecting context into the CGM\nforecasting loop‚Äîvia LLM-based morphological summaries\nand retrieval of case-based analogues‚Äîyields forecasts that\nare more clinically coherent for day-to-day decision support.\nUnlike prior uses of context for event prediction, our design\ntargets continuous trajectories, which matter for dosing, meal\ntiming, and exercise planning. In practice, the combination of\nnatural-language contextualization (to expose behavior-driven\nregimes) and retrieval (to ground the model in similar histori-\ncal episodes) helps the forecaster attend to situations clinicians\nand users care about, rather than relying solely on numeric\nhistory. Consistent with this, GlyRAG improves 60-min RMSE\nover the BGL-only baseline and reduces clinically unsafe\npredictions, as reflected by lower EP in CG-EGA (Table III),\nhigher hypoglycemia sensitivity on Ohio, and smaller TIR\ndeviation on both datasets (Table II).\nThe research conducted in this work shows that LLMs can\nserve as a promising mechanism in digital health forecasting\n"}, {"page": 11, "text": "SOUMMA et al.: GLYRAG\n11\ntasks. Even off-the-shelf, they provide interpretable descriptors\n(e.g., post-prandial rise, rebound) that can be surfaced to\nusers and edited in natural language‚Äîopening the door to\ninteractive ‚Äúwhat-if‚Äù exploration before acting. That said, clin-\nical forecasting remains difficult: regimes are non-stationary,\nexogenous drivers (meals, activity, illness) are incompletely\nobserved, and safety demands reliable behavior in edge states.\nThese realities underscore the need for stronger uncertainty\nawareness, calibration, and safeguards when context is missing\nor conflicting.\nOur ablation study in Table IV further suggests that context\nand retrieval are complementary: the best performance arises\nwhen cross-attention, cross-translation loss (CTL), and RAG\nare used together, whereas removing RAG or CTL erodes the\ngains, and performance remains largely stable across a range\nof Œ± values‚Äîindicating that the translation term behaves as a\nrobust regularizer rather than a fragile tuning knob.\nOur work has several limitations that we plan to address\nin future studies. Patient cohorts are modest in the number of\npatients and focused primarily on type 1 diabetes; broader\nvalidation on type 2 diabetes and pre-diabetes populations\nis an important contribution to demonstrate the potential\ngeneralizability of GlyRAG. Context signals can be noisy\nand incomplete, and the language module is not fine-tuned,\nwhich may limit domain faithfulness. Finally, our evaluation is\nretrospective. The true clinical impact can be demonstrated by\nconducting a prospective study where real-time performance\nand integration with decision policies are incorporated.\nFuture work will: (i) conduct prospective trials with active\ninterventions (e.g., carb advice, temporary basal adjustments,\nexercise prompts) and with additional sensors/modalities; (ii)\nscale to larger, multi-site cohorts and diverse diabetes types;\n(iii) fine-tune and safety-align the LLM for diabetes discourse;\n(iv) enrich retrieval with privacy-preserving, context-paired\nlibraries; and (v) strengthen uncertainty quantification to sup-\nport risk-aware recommendations. These steps move from\naccurate predictions toward actionable, safe decision support\nin everyday diabetes care.\nVII. CONCLUSIONS\nThis paper introduced GlyRAG, a context-aware, retrieval-\naugmented framework that demonstrates how large language\nmodels can serve as agentic contextualization agents for\nblood glucose forecasting. By autonomously extracting se-\nmantic understanding directly from CGM traces and inte-\ngrating case-based reasoning through retrieval mechanisms,\nGlyRAG achieves superior predictive accuracy (up to 39%\nRMSE reduction over prior methods, 1.7% over baseline) and\nenhanced clinical safety (85% predictions in safe zones, 51%\nimproved dysglycemic event detection). Comprehensive eval-\nuation across two real-world Type 1 diabetes cohorts confirms\nthat LLM-driven contextualization enables more accurate, in-\nterpretable, and clinically reliable long-horizon forecasting.\nThese results establish a foundation for agentic AI systems\nin diabetes care that can autonomously reason over physio-\nlogical signals, adapt to individual patterns, and provide trans-\nparent decision support‚Äîadvancing toward proactive, human-\ncentric glucose management tools that reduce patient burden\nwhile maintaining clinical safety.\nREFERENCES\n[1] World\nHealth\nOrganization,\n‚ÄúDiabetes,‚Äù\nhttps://www.who.int/\nnews-room/fact-sheets/detail/diabetes, 2020, accessed: 2025-01-25.\n[2] D. Control, C. T. of Diabetes Interventions, and C. D. S. R. Group,\n‚ÄúIntensive diabetes treatment and cardiovascular disease in patients with\ntype 1 diabetes,‚Äù New England Journal of Medicine, vol. 353, no. 25,\npp. 2643‚Äì2653, 2005.\n[3] M. Karvonen, M. Viik-Kajander, E. Moltchanova, I. Libman, R. LaPorte,\nand J. Tuomilehto, ‚ÄúIncidence of childhood type 1 diabetes worldwide.\ndiabetes mondiale (diamond) project group.‚Äù Diabetes care, vol. 23,\nno. 10, pp. 1516‚Äì1526, 2000.\n[4] International Diabetes Federation, ‚ÄúIdf diabetes atlas,‚Äù https://www.\ndiabetesatlas.org, 2021, accessed: 2025-01-25.\n[5] B. Fullerton, K. Jeitler, M. Seitz, K. Horvath, A. Berghold, and\nA. Siebenhofer, ‚ÄúIntensive glucose control versus conventional glucose\ncontrol for type 1 diabetes mellitus,‚Äù Cochrane Database of Systematic\nReviews, no. 2, 2014.\n[6] R. G. McCoy, C. Ngufor, H. K. Van Houten, B. Caffo, and N. D. Shah,\n‚ÄúTrajectories of glycemic change in a national cohort of adults with\npreviously controlled type 2 diabetes,‚Äù Medical care, vol. 55, no. 11,\npp. 956‚Äì964, 2017.\n[7] M. Marigliano, C. Piona, V. Mancioppi, E. Morotti, A. Morandi, and\nC. Maffeis, ‚ÄúGlucose sensor with predictive alarm for hypoglycaemia:\nImproved glycaemic control in adolescents with type 1 diabetes,‚Äù\nDiabetes, Obesity and Metabolism, vol. 26, no. 4, pp. 1314‚Äì1320, 2024.\n[8] A. Z. Woldaregay, E. ÀöArsand, S. Walderhaug, D. Albers, L. Mamykina,\nT. Botsis, and G. Hartvigsen, ‚ÄúData-driven modeling and prediction\nof blood glucose dynamics: Machine learning applications in type 1\ndiabetes,‚Äù Artificial intelligence in medicine, vol. 98, pp. 109‚Äì134, 2019.\n[9] S. Y. Kwon and J. S. Moon, ‚ÄúAdvances in continuous glucose moni-\ntoring: clinical applications,‚Äù Endocrinology and Metabolism, vol. 40,\nno. 2, pp. 161‚Äì173, 2025.\n[10] G. Cappon, M. Vettoretti, G. Sparacino, and A. Facchinetti, ‚ÄúContinuous\nglucose monitoring sensors for diabetes management: a review of\ntechnologies and applications,‚Äù Diabetes & metabolism journal, vol. 43,\nno. 4, p. 383, 2019.\n[11] A. Arefeen, S. Khamesian, M. A. Grando, B. Thompson, and\nH. Ghasemzadeh, ‚ÄúGlytwin: Digital twin for glucose control in\ntype\n1\ndiabetes\nthrough\noptimal\nbehavioral\nmodifications\nusing\npatient-centric\ncounterfactuals,‚Äù\n2025.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2504.09846\n[12] J. Xie and Q. Wang, ‚ÄúBenchmarking machine learning algorithms on\nblood glucose prediction for type i diabetes in comparison with classical\ntime-series models,‚Äù IEEE Transactions on Biomedical Engineering,\nvol. 67, no. 11, pp. 3101‚Äì3124, 2020.\n[13] O. Mujahid, I. Contreras, and J. Vehi, ‚ÄúMachine learning techniques\nfor hypoglycemia prediction: Trends and challenges,‚Äù Sensors, vol. 21,\nno. 2, p. 546, 2021.\n[14] M. Hwang, V. P. Rachim, J. Yoo, Y. Lee, and S.-M. Park, ‚ÄúGeneralized\nmulti task learning framework for glucose forecasting and hypoglycemia\ndetection using simulation to reality,‚Äù npj Digital Medicine, vol. 8, no. 1,\np. 612, 2025.\n[15] A. Machiraju, E. Farahmand, S. B. Soumma, A. Arefeen, C. Johnston,\nand H. Ghasemzadeh, ‚ÄúTime-aware cross-attention for multi-modal\nsensor-based blood glucose forecasting,‚Äù in IEEE-EMBS International\nConference on Body Sensor Networks 2025, 2025. [Online]. Available:\nhttps://openreview.net/forum?id=BYmtjRxfAg\n[16] K. Li, C. Liu, T. Zhu, P. Herrero, and P. Georgiou, ‚ÄúGlunet: A deep\nlearning framework for accurate glucose forecasting,‚Äù IEEE journal of\nbiomedical and health informatics, vol. 24, no. 2, pp. 414‚Äì423, 2019.\n[17] M. M. H. Shuvo and S. K. Islam, ‚ÄúDeep multitask learning by stacked\nlong short-term memory for predicting personalized blood glucose\nconcentration,‚Äù IEEE Journal of Biomedical and Health Informatics,\nvol. 27, no. 3, pp. 1612‚Äì1623, 2023.\n[18] A. Das, W. Kong, R. Sen, and Y. Zhou, ‚ÄúA decoder-only foundation\nmodel for time-series forecasting,‚Äù in Forty-first International Confer-\nence on Machine Learning, 2024.\n[19] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen,\nY. Liang, Y.-F. Li, S. Pan, and Q. Wen, ‚ÄúTime-LLM: Time series\nforecasting by reprogramming large language models,‚Äù in The Twelfth\nInternational Conference on Learning Representations, 2024. [Online].\nAvailable: https://openreview.net/forum?id=Unb5CVPtae\n"}, {"page": 12, "text": "12\nIEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS\n[20] J. Freiburghaus, A. Rizzotti, and F. Albertetti, ‚ÄúA deep learning approach\nfor blood glucose prediction of type 1 diabetes,‚Äù in Proceedings of the\nProceedings of the 5th International Workshop on Knowledge Discovery\nin Healthcare Data co-located with 24th European Conference on\nArtificial Intelligence (ECAI 2020), 29-30 August 2020, Santiago de\nCompostela, Spain, vol. 2675.\n29-30 August 2020, 2020.\n[21] M. Jaloli and M. Cescon, ‚ÄúLong-term prediction of blood glucose levels\nin type 1 diabetes using a cnn-lstm-based deep neural network,‚Äù Journal\nof diabetes science and technology, vol. 17, no. 6, pp. 1590‚Äì1601, 2023.\n[22] M. De Bois, M. A. El-Yacoubi, and M. Ammi, ‚ÄúIntegration of clin-\nical criteria into the training of deep models: Application to glucose\nprediction for diabetic people,‚Äù Smart Health, vol. 21, p. 100193, 2021.\n[23] F. Prendin, J. Pavan, G. Cappon, S. Del Favero, G. Sparacino, and\nA. Facchinetti, ‚ÄúThe importance of interpreting machine learning models\nfor blood glucose prediction in diabetes: an analysis using shap,‚Äù\nScientific reports, vol. 13, no. 1, p. 16865, 2023.\n[24] J. Daniels, P. Herrero, and P. Georgiou, ‚ÄúA multitask learning approach\nto personalized blood glucose prediction,‚Äù IEEE Journal of Biomedical\nand Health Informatics, vol. 26, no. 1, pp. 436‚Äì445, 2021.\n[25] E. Farahmand, S. B. Soumma, N. T. Chatrudi, and H. Ghasemzadeh,\n‚ÄúHybrid attention model using feature decomposition and knowledge\ndistillation\nfor\nglucose\nforecasting,‚Äù\n2025.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2411.10703\n[26] A. Garza and M. Mergenthaler-Canseco, ‚ÄúTimegpt-1,‚Äù 2023.\n[27] H. Xue and F. D. Salim, ‚ÄúPromptcast: A new prompt-based learning\nparadigm for time series forecasting,‚Äù IEEE Transactions on Knowledge\nand Data Engineering, vol. 36, no. 11, pp. 6851‚Äì6864, 2023.\n[28] X. Liu, D. McDuff, G. Kovacs, I. Galatzer-Levy, J. Sunshine, J. Zhan,\nM.-Z. Poh, S. Liao, P. Di Achille, and S. Patel, ‚ÄúLarge language models\nare few-shot health learners,‚Äù arXiv preprint arXiv:2305.15525, 2023.\n[29] G. Lee, W. Yu, K. Shin, W. Cheng, and H. Chen, ‚ÄúTimecap: Learning\nto contextualize, augment, and predict time series events with large\nlanguage model agents,‚Äù in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 39, no. 17, 2025, pp. 18 082‚Äì18 090.\n[30] K. Gokcesu and H. Gokcesu, ‚ÄúGeneralized huber loss for robust\nlearning and its efficient minimization for a robust statistics,‚Äù 2021.\n[Online]. Available: https://arxiv.org/abs/2108.12627\n[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\nof deep bidirectional transformers for language understanding,‚Äù in North\nAmerican Chapter of the Association for Computational Linguistics,\n2019.\n[Online].\nAvailable:\nhttps://api.semanticscholar.org/CorpusID:\n52967399\n[32] Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam, ‚ÄúA time\nseries is worth 64 words: Long-term forecasting with transformers,‚Äù in\nInternational Conference on Learning Representations, 2023.\n[33] C. Marling and R. Bunescu, ‚ÄúThe ohiot1dm dataset for blood glucose\nlevel prediction: Update 2020,‚Äù in CEUR workshop proceedings, vol.\n2675, 2020, p. 71.\n[34] S. Khamesian, A. Arefeen, B. M. Thompson, A. Grando, and\nH.\nGhasemzadeh,\n‚ÄúAzt1d:\nA\nreal-world\ndataset\nfor\ntype\n1\ndiabetes,‚Äù 2025. [Online]. Available: https://data.mendeley.com/datasets/\ngk9m674wcx/1\n[35] J. Martinsson, A. Schliep, B. Eliasson, and O. Mogren, ‚ÄúBlood glucose\nprediction with variance estimation using recurrent neural networks,‚Äù\nJournal of Healthcare Informatics Research, vol. 4, pp. 1‚Äì18, 2020.\n[36] H. Rubin-Falcone, I. Fox, and J. Wiens, ‚ÄúDeep residual time-series\nforecasting: Application to blood glucose prediction.‚Äù KDH@ ECAI,\nvol. 20, pp. 105‚Äì109, 2020.\n[37] H. Hameed and S. Kleinberg, ‚ÄúInvestigating potentials and pitfalls of\nknowledge distillation across datasets for blood glucose forecasting,‚Äù in\nProceedings of the 5th Annual Workshop on Knowledge Discovery in\nHealthcare Data, 2020.\n[38] R. Cui, C. Hettiarachchi, C. J. Nolan, E. Daskalaki, and H. Suominen,\n‚ÄúPersonalised short-term glucose prediction via recurrent self-attention\nnetwork,‚Äù in 2021 IEEE 34th International Symposium on Computer-\nBased Medical Systems (CBMS).\nIEEE, 2021, pp. 154‚Äì159.\n[39] A. Arefeen and H. Ghasemzadeh, ‚ÄúGlysim: Modeling and simulating\nglycemic response for behavioral lifestyle interventions,‚Äù in 2023 IEEE\nEMBS International Conference on Biomedical and Health Informatics\n(BHI).\nIEEE, 2023, pp. 1‚Äì5.\n[40] S. Khamesian, A. Arefeen, M. A. Grando, B. M. Thompson, and\nH. Ghasemzadeh, ‚ÄúType 1 diabetes management using glimmer:\nGlucose level indicator model with modified error rate,‚Äù 2025. [Online].\nAvailable: https://arxiv.org/abs/2502.14183\n[41] W. L. Clarke, D. Cox, L. A. Gonder-Frederick, W. Carter, and S. L.\nPohl, ‚ÄúEvaluating clinical accuracy of systems for self-monitoring of\nblood glucose,‚Äù Diabetes Care, vol. 10, no. 5, pp. 622‚Äì628, 09 1987.\n[Online]. Available: https://doi.org/10.2337/diacare.10.5.622\nShovito Barua Soumma, MS (Student Mem-\nber, IEEE) received his BSc degree in Computer\nScience & Engineering from the Bangladesh\nUniversity of Engineering & Technology (BUET),\nDhaka, Bangladesh, in 2022 and the MSc de-\ngree from Arizona State University in 2025. Cur-\nrently, he is working toward a PhD degree in\nBiomedical Informatics & Data Science at Ari-\nzona State University. He is interested in digital\nhealth, embedded systems, machine learning,\nand explainable AI. The focus of his research\nis on the design and development of power-efficient AI models for\nwearable monitoring systems with various applications in healthcare.\nHassan Ghasemzadeh, PhD (Senior Member,\nIEEE), received the BSc degree from the Sharif\nUniversity of Technology, Tehran, Iran, in 1998,\nthe MSc degree from the University of Tehran,\nTehran, Iran, in 2001, and the PhD degree from\nthe University of Texas at Dallas, Richardson,\nTX, in 2010, all in computer engineering. He\nwas on the faculty of Azad University from 2003-\n2006 where he served as founding chair of the\nComputer Science and Engineering Department\nat the Damavand branch, Tehran, Iran. He spent\nthe academic year 2010- 2011 as a postdoctoral fellow at the West\nWireless Health Institute, La Jolla, CA. He was a research manager\nat the UCLA Wireless Health Institute in 2011-2013. Currently he is\nan associate professor of biomedical informatics, the director of un-\ndergraduate biomedical informatics program, and a graduate faculty of\ncomputer science, computer engineering, and biomedical engineering\nat Arizona State University (ASU). Prior to joining ASU, he was an\nassistant/associate professor of computer science at Washington State\nUniversity (WSU 2014-2021). The focus of his research is on algorithm\ndesign and system level optimization of embedded and pervasive sys-\ntems with applications in healthcare and wellness.\n"}]}