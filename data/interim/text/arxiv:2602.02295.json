{"doc_id": "arxiv:2602.02295", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.02295.pdf", "meta": {"doc_id": "arxiv:2602.02295", "source": "arxiv", "arxiv_id": "2602.02295", "title": "EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models", "authors": ["Shaima Ahmad Freja", "Ferhat Ozgur Catak", "Betul Yurdem", "Chunming Rong"], "published": "2026-02-02T16:32:40Z", "updated": "2026-02-02T16:32:40Z", "summary": "Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.02295v1", "url_pdf": "https://arxiv.org/pdf/2602.02295.pdf", "meta_path": "data/raw/arxiv/meta/2602.02295.json", "sha256": "979fa0c94a2608fe4194b7aca50f8acdf170ae5ec3ecbecc3548676c60b30fab", "status": "ok", "fetched_at": "2026-02-18T02:20:03.009797+00:00"}, "pages": [{"page": 1, "text": "1\nEvalQReason: A Framework for Step-Level\nReasoning Evaluation in Large Language Models\nShaima Ahmad Freja, Ferhat Ozgur Catak, Senior Member, IEEE, Betul Yurdem, and Chunming Rong, Senior\nMember, IEEE\nAbstract—Large Language Models (LLMs) are increasingly\ndeployed in critical applications requiring reliable reasoning, yet\ntheir internal reasoning processes remain difficult to evaluate\nsystematically. Existing methods focus on final-answer correct-\nness, providing limited insight into how reasoning unfolds across\nintermediate steps.\nWe present EvalQReason, a framework that quantifies LLM\nreasoning quality through step-level probability distribution\nanalysis without requiring human annotation. The framework\nintroduces two complementary algorithms: Consecutive Step\nDivergence (CSD), which measures local coherence between adja-\ncent reasoning steps, and Step-to-Final Convergence (SFC), which\nassesses global alignment with final answers. Each algorithm\nemploys five statistical metrics to capture reasoning dynamics.\nExperiments across mathematical and medical datasets with\nopen-source 7B-parameter models demonstrate that CSD-based\nfeatures achieve strong predictive performance for correctness\nclassification, with classical machine learning models reaching\nF1=0.78 and ROC-AUC=0.82, and sequential neural models\nsubstantially improving performance (F1=0.88, ROC-AUC=0.97).\nCSD consistently outperforms SFC, and sequential architectures\noutperform classical machine learning approaches.\nCritically, reasoning dynamics prove domain-specific: mathe-\nmatical reasoning exhibits clear divergence-based discrimination\npatterns between correct and incorrect solutions, while medical\nreasoning shows minimal discriminative signals, revealing fun-\ndamental differences in how LLMs process different reasoning\ntypes. EvalQReason enables scalable, process-aware evaluation\nof reasoning reliability, establishing probability-based divergence\nanalysis as a principled approach for trustworthy AI deployment.\nIndex Terms—Large Language Models; Reasoning Evaluation;\nProbability Distribution Analysis; Step-Level Reasoning; Trust-\nworthy AI\nI. INTRODUCTION\nLarge Language Models (LLMs) are increasingly deployed\nin critical applications such as medical diagnostics [1], [2],\nadvanced mathematics [3], [4], and scientific research. Despite\ntheir impressive performance, the internal reasoning processes\nof these models remain difficult to interpret and systematically\nevaluate. While comprehensive surveys have documented var-\nious dimensions of LLM evaluation [5], existing reasoning\nevaluation approaches focus primarily on final-answer accu-\nracy, offering limited insight into how reasoning unfolds across\nintermediate steps or whether logical coherence is preserved\nShaima Ahmad Freja, Ferhat Ozgur Catak, and Chunming Rong are\nwith the Department of Electrical Engineering and Computer Science,\nUniversity of Stavanger, 4021 Stavanger, Norway (e-mail: {shaima.a.freja,\nf.ozgur.catak,chunming.rong}@uis.no).\nBetul Yurdem is with the Department of Electrical and Electronics En-\ngineering, Izmir Bakircay University, 35665 Izmir, Turkiye (e-mail: be-\ntul.yurdem@bakircay.edu.tr).\nthroughout the inference process [6]. This lack of transparency\nposes significant challenges for trustworthiness and reliability,\nparticularly in high-stakes domains where understanding how\na model arrives at a conclusion is as important as whether\nthe conclusion is correct. This gap in evaluation fundamen-\ntally limits progress toward trustworthy AI. When models\nproduce incorrect solutions, current methods cannot localize\nwhere or why the reasoning process failed [7]. Moreover,\nexisting frameworks lack a unified mechanism for capturing\nthe dynamic and sequential nature of LLM reasoning using\ncontinuous, probability-based signals. Consequently, reasoning\nbehavior cannot be systematically analyzed across domains,\ndifficulty levels, and correctness outcomes, hindering princi-\npled assessment of model reliability in critical applications [6],\n[8], [9].\nWe introduce EvalQReason, a three-stage framework for\nquantitative, probability-based evaluation of LLM reasoning\nthat does not require human annotation or external eval-\nuators. EvalQReason operates by (1) extracting step-level\nlogits from model-generated reasoning chains, (2) quantifying\nreasoning dynamics using two divergence-based algorithms,\nand (3) performing predictive modeling to assess reasoning\nreliability. Unlike approaches that depend on labeled reasoning\ntraces [7], [10] or semantic evaluation frameworks based\non LLM judges [11], [12], EvalQReason derives continuous\nstatistical signals directly from the model’s own probability\ndistributions.\nThere are two complementary algorithms at the core of\nthe framework. Consecutive Step Divergence (CSD) quan-\ntifies local reasoning coherence by measuring distributional\nshifts between adjacent reasoning steps, while Step-to-Final\nConvergence (SFC) captures global alignment by comparing\nintermediate steps to the final-answer distribution. Using five\nstatistical metrics, Kullback–Leibler (KL) Divergence [13],\nJensen–Shannon (JS) Divergence [14], Hellinger Distance\n[15], Cosine Similarity [16], and Entropy Difference [17],\nEvalQReason characterizes both local and global properties\nof reasoning dynamics. Experimental results across math-\nematical and medical domains reveal that these dynamics\nare fundamentally domain-specific: divergence-based patterns\nstrongly discriminate correctness in mathematical reasoning,\nwhile exhibiting minimal discriminative signal in medical\ndiagnostic tasks.\nThis work makes two primary contributions:\n1) We propose EvalQReason, a three-stage framework that\nquantifies LLM reasoning dynamics from step-level\narXiv:2602.02295v1  [cs.LG]  2 Feb 2026\n"}, {"page": 2, "text": "2\nprobability distributions without requiring human anno-\ntation.\n2) We introduce two divergence-based algorithms, Consec-\nutive Step Divergence (CSD) and Step-to-Final Conver-\ngence (SFC), that capture local and global reasoning\nbehavior across domains, difficulty levels, and reasoning\nlengths.\nThe remainder of this article is organized as follows. Sec-\ntion II reviews related work on reasoning evaluation and inter-\npretability. Section III presents the EvalQReason framework,\nincluding datasets, models, and the CSD/SFC algorithms.\nSection IV reports experimental results and analysis. Section V\ndiscusses limitations and future directions, and Section VI\nconcludes the paper.\nII. RELATED WORK\nThe trustworthiness of LLM reasoning is critical as models\nare deployed in high-stakes domains, motivating evaluation\nbeyond final answer accuracy [8]. Traditional benchmarks\nthat rely solely on outcome-oriented metrics risk masking\nsystemic deficiencies in the reasoning process itself, such as\nlogical fallacies or error propagation [6], [18], [19]. This has\nmotivated a shift toward process-aware assessment methods.\nA. Existing Evaluation Paradigms\nEarly approaches trained outcome reward models to verify\nfinal answers [20], which Lightman et al. extended to process\nsupervision by training reward models on 800K+ human anno-\ntations to score individual steps [10]. Math-Shepherd [21] and\nProcessBench [7] further developed process reward models for\nstep-level evaluation, but these supervised approaches require\nsubstantial human labor and struggle with domain transfer\nbeyond training data (GSM8K, MATH).\nTo avoid human annotation, recent frameworks employ\nLLMs as evaluators [22]. ROSCOE [23] provides multi-\ndimensional evaluation but relies on reference-based compar-\nisons that limit accommodation of diverse reasoning paths.\nReasonEval [24] assesses validity and redundancy using\ntrained LLM judges, while CaSE [12] evaluates relevance and\ncoherence in a reference-free manner. AutoRace [11] uses\nGPT-4 to evaluate reasoning chains based on automatically\ngenerated rubrics. While interpretable, these approaches de-\npend on external LLMs (often proprietary models) and provide\ncategorical rather than continuous quantification of reasoning\ndynamics.\nFoundational work on Chain-of-Thought prompting [25]\nand self-consistency methods [26] demonstrated how to elicit\nand improve reasoning chains, though these focus on genera-\ntion rather than evaluation.\nB. Domain-Specific Reasoning Challenges\nLLM reasoning capabilities vary fundamentally across do-\nmains. Mathematical reasoning requires multi-step logical\ndeduction [3], while statistical and causal reasoning present\ndistinct challenges [27]. Medical reasoning is particularly\nproblematic: Savage et al. [2] demonstrated the importance\nof interpretable diagnostic reasoning, while Wu et al. [28]\nshowed that chain-of-thought reasoning systematically fails\nin clinical contexts (86.3% of models show degradation),\nattributed to the complexity and fragmentation of clinical\ndocumentation. These domain-specific differences motivate\nevaluation frameworks that systematically characterize how\nreasoning dynamics vary across problem types.\nC. Positioning EvalQReason\nWhile existing approaches provide valuable evaluation\nframeworks, they face key limitations. Supervised methods [7],\n[10], [21] require extensive annotation and struggle with do-\nmain transfer. LLM-judge approaches [11], [12], [24] depend\non external models and provide categorical assessments.\nEvalQReason addresses these limitations through fully au-\ntomated, probability-based divergence analysis. By quantify-\ning step-level reasoning dynamics through probability dis-\ntributions—signals that recent work has shown reflect gen-\nuine model confidence [29] and enable effective adapta-\ntion [30]—our framework requires no human supervision or\nexternal evaluators. As demonstrated in Section IV-B3, this\napproach outperforms existing supervised and LLM-judge\nmethods while eliminating annotation costs entirely.\nIII. METHODOLOGY\nThe methodology begins with the use of mathematical and\nmedical datasets combined with domain-specialized models\n(Table I) to establish the experimental foundation. As illus-\ntrated in Figure 1, the framework consists of three main\nstages: (i) generating reasoning steps and extracting token-\nlevel probabilities, (ii) evaluating reasoning coherence and\nconvergence through the Consecutive Step Divergence (CSD)\nand Step-to-Final Convergence (SFC) algorithms, and (iii)\nperforming pattern analysis and predictive modeling to assess\nreasoning reliability.\nA. Datasets and Models\nTo analyze reasoning reliability across domains, we employ\nthree challenging benchmarks: AIME, Math-500, and MedQA,\ncovering mathematical question answering (QA) and medical\nmultiple-choice questions (MCQ). Mathematical reasoning is a\ncritical testbed for LLM evaluation [3], as it requires precise\nmulti-step deduction and error-sensitive computation. These\nproperties make math datasets particularly suitable for process-\nlevel analysis of reasoning dynamics across intermediate steps.\nMedical reasoning provides a complementary setting, empha-\nsizing domain knowledge and diagnostic inference, enabling\ncross-domain comparison of reasoning coherence under fun-\ndamentally different constraints.\nThe selected datasets are designed to span diverse reason-\ning patterns, difficulty levels, and domain-specific knowledge\nrequirements. Dataset statistics and difficulty distributions are\nsummarized in Table I.\n1) AIME: A collection of 935 high-difficulty mathematical\nproblems from the American Invitational Mathematics Exam-\nination (1983–2024). These problems are designed to assess\n"}, {"page": 3, "text": "3\nTABLE I\nOVERVIEW OF DATASETS AND LLMS USED FOR REASONING\nGENERATION.\nDataset\nDomain\nSize\nDifficulty\nLLMs Used\nMath-AIME\nMath QA\n240\n3 Levels\nQwen2.5-7B,\nMathStral-7B\nMath-500\nMath QA\n500\n5 Levels\nQwen2.5-7B,\nMathStral-7B\nMed-QA\nMed MCQ\n1,243\n2 Levels\nQwen2.5-7B,\nQwen-Medicine-7B\nadvanced mathematical reasoning through creative, multi-step\nproblem solving. We use a cleaned version derived from\nthe publicly available Kaggle release [31], after removing\nduplicates and incomplete entries.\nDifficulty Reclassification. To support reasoning-level anal-\nysis, the original 15-problem ordering in each exam was\naggregated into three difficulty levels: Easy (Problems 1–5),\nMedium (6–10), and Hard (11–15). This grouping reflects\nthe inherent progression of problem difficulty and enables\ncontrolled comparison across reasoning depths. To mitigate\nclass imbalance, we apply stratified sub-sampling, yielding\na balanced subset of 240 problems (80 per difficulty level).\nDetails are provided in Section III-C1.\n2) Math-500: A curated subset of 500 competition-level\nproblems from the MATH benchmark [3]. The original MATH\ndataset was introduced to evaluate multi-step mathematical\nreasoning in large language models and contains over 12,000\nproblems spanning five difficulty levels. This subset preserves\nthe same structure (Levels 1–5) and exhibits a natural skew\ntoward greater difficulty, making it well-suited for fine-grained\nanalysis of reasoning stability under increasing problem com-\nplexity.\n3) MedQA: A large-scale medical multiple-choice question\nanswering benchmark constructed from professional medi-\ncal board examinations [32]. It evaluates models’ ability to\nperform clinical reasoning and apply domain knowledge to\ndiagnostic decision-making. We use the English subset to\nensure consistency with the mathematical datasets, while pro-\nviding a contrasting domain for analyzing differences between\nstructured problem solving and knowledge-intensive inference.\nPreprocessing and Difficulty Grouping. Difficulty levels\nwere derived from the meta_info field, which categorizes\nquestions into Type 1 (single knowledge point) and Type 2\n(multi-step clinical scenario). These categories were mapped to\ntwo difficulty levels: Level 1 (Basic) and Level 2 (Advanced),\nenabling analysis of reasoning behavior across increasing\nclinical complexity.\nB. Models\nWe employ three open-source 7B-parameter LLMs to gen-\nerate step-by-step reasoning chains across mathematical and\nmedical domains. The use of open-source models is method-\nologically essential for EvalQReason, as the framework re-\nquires direct access to token-level logits at inference time to\nextract step-level probability distributions for divergence anal-\nysis. Such access is unavailable in closed-source or API-based\nmodels (e.g., GPT-4, Gemini), which expose only final text\noutputs and therefore cannot support process-level reasoning\nanalysis based on internal distributional dynamics.\n• Mathstral 7B [33]: A Mistral 7B variant fine-tuned on\nsynthetic mathematical data, exhibiting strong multi-step\nlogical reasoning suitable for AIME and Math-500.\n• Qwen2.5 7B-Instruct [34]: The primary model used\nacross all datasets, selected for its strong instruction-\nfollowing capability, enhanced mathematical reasoning,\nand extended context length (up to 128K tokens).\n• Qwen-Medicine 7B [35]: A medical-domain extension of\nQwen2.5 7B fine-tuned on 340K medical dialogues, opti-\nmized for clinical terminology and diagnostic reasoning.\nThese models enable controlled comparison of reasoning\ndynamics across mathematical and medical problem-solving\ndomains.\nC. EvalQReason Framework Architecture\nEvalQReason is a three-stage framework for analyzing and\nquantifying reasoning behavior in LLMs. As illustrated in\nFigure 1, the framework first generates step-by-step reasoning\nchains and extracts token-level logits (Stage 1). These logits\nare transformed into step-level probability distributions, which\nare then quantified using statistical divergence metrics to\ncapture local coherence and global convergence across rea-\nsoning steps (Stage 2). Finally, EvalQReason performs pattern\nanalysis and predictive modeling to assess reasoning reliability\nand characterize domain-specific dynamics (Stage 3). This\nmodular design enables systematic analysis of how reasoning\nevolves across steps and how evaluation behavior varies across\ndomains and difficulty levels.\n1) Stage 1: Reasoning Generation and Logit Extraction:\nStage 1 establishes the foundation for quantitative reasoning\nanalysis by generating structured reasoning chains and extract-\ning step-level logit distributions from LLMs across diverse\ndomains and difficulty levels. The output is a curated dataset\ncontaining reasoning chains with their corresponding raw logit\nmatrices, forming the input for divergence-based analysis in\nStage 2.\nWe employ domain-specialized model pairs: (i) MathStral-\n7B and Qwen2.5-7B for mathematical reasoning (AIME,\nMath-500), and (ii) Qwen2.5-7B and Qwen-Medicine-7B for\nmedical reasoning (MedQA). Domain-specific prompts (Sup-\nplementary Material, Fig. S1) encourage structured step-by-\nstep reasoning with explicit formatting requirements. For\nmathematical problems, models generate clear logical steps\nculminating in a boxed final answer. For medical problems,\nmodels work through clinical cases by analyzing symptoms,\neliminating incorrect options with justification, and selecting\nthe correct answer. Reasoning chains are generated using\ngreedy decoding (temperature=0) to ensure deterministic out-\nputs, with logit collection enabled at each generation step.\nThe pipeline consists of four sequential steps:\n1) Reasoning Chain Generation. For each problem, we\nemploy greedy decoding (do_sample=False) with adap-\ntive token limits of 512, 1024, and 2048 tokens and a repetition\n"}, {"page": 4, "text": "4\nStage 3 - Pattern Analysis & Reliability Evaluation\nStage 2 - Reasoning Dynamics Quantification\nStage1 - Reasoning Generation\nMulti-Domain Datasets\nMathimatical\nMath-500\nMath AIME \nMathStral-7B\nQwen2.5-7B\nMedical \nMedical-QA\n Qwen-Medicine-7B\nQwen2.5-7B\nCSD \n(Consecutive Step Divergence)\n------------------------------------------\nSFC \n(Step-to-Final Convergence)\n------------------------------------------\n5 Statistical Metrics\nKL Divergence\nJS Divergence\nCosine Similarity\nHellinger Distance\nEntropy\nSequential Models\nNeural Network (NN)\nGRU\n LSTM\nClassical ML\nLR\nSVM\nXGBoost\n✓ Correctness prediction\n✓ Cross-domain reasoning insights\nStep i\nStep i+1\nStep i\nStep final\nGen-Reasoning      Parsing      Extracting Logits \n12 Quantified Datasets\n(6 CSD + 6 SFC)\nPattern Recognition and Interpretation \n-----------------------------------------------------------------------------\n6 Datasets with Step-Level Logits\nFig. 1. EvalQReason framework architecture.\npenalty of 1.2. Generation proceeds with logit extraction\nenabled through return_dict_in_generate=True and\noutput_scores=True. The generated text is validated\nto confirm the presence of a final answer in the expected\nformat (e.g., \\boxed{} for mathematics). If validation fails,\ngeneration retries with the next higher token limit.\n2) Parsing and Step Alignment. The generated reasoning\ntext is parsed into individual reasoning steps based on explicit\nstep markers (e.g., “Step 1:”, “Step 2:”). Each step’s tokens\nare aligned with their corresponding model-generated logits,\nestablishing a direct mapping between textual reasoning and\nthe model’s internal logit outputs.\n3) Logit Extraction and Storage. For each reasoning\nstep i containing Ti tokens, we extract the raw logit matrix\nℓi ∈RTi×|V |, where each row ℓi,t ∈R|V | contains the\nlogit vector for token t at step i, and |V | is the vocabulary\nsize. These raw logit matrices are stored without further\nprocessing, preserving the complete distributional information\nfor subsequent analysis.\n4) Dataset Curation and Storage. All reasoning steps\nand their corresponding logit matrices are stored in a pickle\nformat, preserving problem metadata (difficulty level, domain,\ncorrectness label) alongside reasoning dynamics. For AIME,\nwe applied stratified sub-sampling after generating chains for\nall 935 problems using MathStral-7B. To address severe class\nimbalance (25.8% correct), we selected a balanced subset of\n240 problems (80 per difficulty level), enabling fair compari-\nson across difficulty strata. For Math-500 and MedQA, natural\ncorrectness distributions were preserved without sub-sampling.\nThis stage produces curated datasets containing parsed\nreasoning chains with step-level logits, forming the foundation\nfor the quantitative divergence analysis conducted in Stage 2.\nDataset statistics and model accuracy are reported in Table II.\n2) Stage 2: Reasoning Dynamics Quantification: Stage 2\nquantifies reasoning dynamics from the logit representations\nextracted in Stage 1 using divergence-based analysis. We in-\ntroduce two complementary algorithms: Consecutive Step Di-\nvergence (CSD), which measures local coherence between ad-\njacent reasoning steps, and Step-to-Final Convergence (SFC),\nwhich assesses global alignment of intermediate steps toward\nthe final answer.\nThis formulation is inspired by interpretability methods that\nanalyze prediction evolution within LLMs. The logit lens [36]\nand tuned lens [37] project intermediate transformer layers\ninto vocabulary space to study convergence across layers.\nEvalQReason adapts this principle from an architectural setting\n(layer-wise evolution) to a behavioral setting, quantifying how\nprobability distributions evolve across multi-step reasoning\nchains to capture both local coherence and global convergence.\n(i) Step-Level Probability Distribution Construction\nGiven raw logit matrices from Stage 1, we construct step-\nlevel probability distributions through token-level normaliza-\ntion and averaging. For each reasoning step i with logit matrix\nℓi ∈RTi×|V |, we apply softmax to each token’s logit vector:\npi,t = softmax(ℓi,t) ∈∆|V |−1,\nt = 1, . . . , Ti\n(1)\nThe step-level distribution is the arithmetic mean across all\ntokens:\n¯pi = 1\nTi\nTi\nX\nt=1\npi,t ∈∆|V |−1\n(2)\nThis yields a mixture distribution representing typical next-\ntoken preferences within the step, ensuring length invariance\nand preserving full vocabulary information. For numerical\nstability, we apply additive smoothing ˜pi = ¯pi + ε where\nε\n=\n10−7. All divergences are computed over the full\nvocabulary using natural logarithms (units: nats).\n(ii) Divergence-Based Algorithms\nUsing the step-level distributions {˜p1, ˜p2, . . . , ˜pn}, we quan-\ntify reasoning dynamics through two complementary algo-\nrithms.\nConsecutive Step Divergence (CSD). CSD measures local\nreasoning coherence by quantifying distributional shifts be-\ntween adjacent steps. For a reasoning chain with n steps, we\ncompute the divergence between each consecutive pair:\nCSDi = D(˜pi∥˜pi+1),\ni = 1, 2, . . . , n −1\n(3)\nwhere D(·, ·) represents a divergence metric. Low CSD values\nindicate smooth reasoning progression; high values suggest\nreasoning drift or contradictions.\n"}, {"page": 5, "text": "5\nStep-to-Final Convergence (SFC). SFC evaluates global\nalignment by measuring how each intermediate step relates\nto the final-answer distribution:\nSFCi = D(˜pi∥˜pfinal),\ni = 1, 2, . . . , n −1\n(4)\nwhere ˜pfinal is the distribution at the final reasoning step.\nDecreasing SFC values indicate progressive convergence; fluc-\ntuating values suggest tangential reasoning.\n(iii) Statistical Metrics\nWe employ five complementary statistical metrics to quan-\ntify distributional relationships in both CSD and SFC:\nKullback-Leibler (KL) Divergence, Jensen-Shannon (JS) Di-\nvergence, Hellinger Distance, Cosine Similarity, and Entropy\nDifference. These metrics capture distinct aspects of reasoning\ndynamics—divergence measures (KL, JS, Hellinger) quan-\ntify distributional shifts, Cosine Similarity measures direc-\ntional consistency, and Entropy Difference tracks uncertainty\nchanges. All metrics are computed over the full vocabulary\nusing natural logarithms (units: nats). Complete mathematical\ndefinitions are provided in Supplementary Material Section S2.\nFor CSD, we compute the signed entropy change between\nconsecutive steps:\n∆Hi = H(Pi+1) −H(Pi)\n(5)\nPositive values indicate increasing uncertainty (model becom-\ning less confident), while negative values indicate decreasing\nuncertainty (model becoming more confident).\nFor SFC, we compute the absolute entropy deviation from\nthe final step:\n∆Hi = |H(Pi) −H(Pfinal)|\n(6)\nThis measures how each step’s uncertainty differs from the\nfinal answer’s uncertainty, with lower values indicating con-\nvergence in confidence levels.\nThe resulting outputs form two structured datasets per\ndataset-model pair: one for CSD and one for SFC. Each\nCSD dataset contains all five statistical metrics computed\nbetween consecutive steps, while each SFC dataset contains\nthe same five metrics computed relative to the final step.\nAcross six dataset-model pairs (AIME-MathStral, AIME-\nQwen2.5, Math500-MathStral, Math500-Qwen2.5, MedQA-\nQwen2.5, MedQA-QwenMedicine), this produces 12 quanti-\nfied datasets (6 CSD + 6 SFC), each enriched with metadata\nincluding model name, question identifier, difficulty level, and\ncorrectness label. These datasets serve as the analytical foun-\ndation for Stage 3, enabling detailed investigation of reasoning\ntrajectories, domain-specific behavior, and predictive modeling\nof reliability.\n3) Stage 3: Pattern Analysis and Reliability Evaluation:\nWe perform two complementary analyses on the quantified\noutputs from Stage 2: (i) pattern analysis to identify systematic\ndifferences in reasoning dynamics, and (ii) predictive modeling\nto evaluate whether these patterns can reliably predict answer\ncorrectness.\na) Pattern Recognition and Interpretation.: To examine\nhow reasoning behavior varies across domains, difficulty lev-\nels, and correctness outcomes, we analyze statistical metrics\n(KL Divergence, JS Divergence, Cosine Similarity, Hellinger\nDistance, and Entropy Difference) stratified by correctness and\ndifficulty. This analysis characterizes how reasoning coherence\nevolves, fluctuates, or degrades across reasoning steps, provid-\ning insight into model reliability and stability.\nWe study patterns along five dimensions: (i) metric compari-\nson to identify the most discriminative measures, (ii) difficulty-\nlevel analysis to assess the impact of problem complexity, (iii)\nstep-length analysis to evaluate the effect of reasoning chain\nlength, (iv) algorithmic comparison (CSD vs. SFC) to contrast\nlocal and global dynamics, and (v) cross-domain analysis to\nreveal domain-specific reasoning characteristics.\nb) Predictive Modeling of Reasoning Reliability:\nTo\nassess whether quantified reasoning dynamics can predict\nanswer correctness, we employ two complementary modeling\napproaches.\n(i) Classical Machine Learning Models. We train three\nclassical models—Logistic Regression (LR), Support Vector\nMachine (SVM), and XGBoost (XGB)—on engineered tem-\nporal features. For each dataset–LLM–algorithm combination,\nwe extract approximately 20 features per reasoning chain that\ncapture three aspects of reasoning dynamics: (i) aggregate\nstatistics (e.g., mean and final values), (ii) temporal progres-\nsion (slopes reflecting stability trends), and (iii) volatility\nindicators (maximum changes between consecutive steps).\nFor divergence-based metrics (KL, JS, and Hellinger), we\ncompute the mean, temporal slope, maximum jump, and final\nvalue. For Cosine Similarity, we extract the mean, slope, and\nfinal value. For Entropy Difference, we derive six features\ncapturing uncertainty dynamics, including cumulative entropy\nchange and overall entropy trends. Together, these features\nmap variable-length reasoning sequences to fixed-dimensional\nrepresentations while preserving both statistical and temporal\ncharacteristics.\n(ii) Sequential Modeling of Reasoning Dynamics. To\ncapture temporal dependencies in reasoning sequences, we em-\nploy Neural Network (NN), Gated Recurrent Unit (GRU) [38],\nand Long Short-Term Memory (LSTM) [39] models. Unlike\nclassical models based on engineered features, these sequential\nmodels operate directly on step-wise metric sequences. Each\nreasoning chain is represented as a temporal sequence S =\n[s1, s2, . . . , sn], where si ∈R5 denotes the five-dimensional\nmetric vector at step i. Variable-length sequences are handled\nvia dynamic padding within each batch.\nAll sequential models share a common base architecture\nwith hidden dimensions of 64 and 128, ReLU activation,\nbatch size 32, and training for up to 100 epochs with early\nstopping (patience = 50). Preserving temporal order enables\nthese models to capture reasoning dynamics such as mid-step\ninstability, progressive degradation, and late-stage convergence\nthat aggregated features cannot capture.\nModels are evaluated using stratified train–test splits based\non the correctness label. Hyperparameters for classical mod-\nels are optimized via grid search, randomized search, and\nBayesian optimization, while sequential models explore learn-\ning rate, L2 regularization, and dropout. The best configuration\nfor each dataset–model–algorithm pair is selected using F1-\nScore. Performance is reported using F1-Score (primary),\n"}, {"page": 6, "text": "6\nTABLE II\nCURATED REASONING DATASET STATISTICS FOR STAGE 1, INCLUDING\nCORRECTNESS AND ACCURACY OF EACH MODEL.\nDataset\nSize LLMs\nCorrect In-correct Accuracy (%)\nMath-AIME\n240 MathStral-7B\nQwen2.5-7B\n62\n33\n178\n201\n25.8\n14.1\nMath-500\n500 MathStral-7B\nQwen2.5-7B\n207\n239\n270\n256\n43.4\n48.3\nMed-QA\n1,243 Qwen2.5-7B\nQwen-Med-7B\n592\n536\n651\n721\n47.6\n42.6\nROC-AUC, Accuracy, and Balanced Accuracy for both CSD\nand SFC. Complete hyperparameters are in Supplementary\nMaterial Tables S1-S4.\nIV. RESULTS AND DISCUSSION\nThis section presents experimental findings from applying\nthe EvalQReason framework to evaluate reasoning dynamics\nacross mathematical and medical domains. We first analyze\nsystematic patterns in reasoning behavior (Section IV-A),\nrevealing how local coherence and global convergence dis-\ntinguish correct from incorrect reasoning. We then evaluate\npredictive performance (Section IV-B), demonstrating that\ndivergence-based features enable reliable correctness predic-\ntion. Table II summarizes the six curated reasoning datasets\ngenerated in Stage 1, showing overall correctness distribution\nand baseline accuracies. Mathematical reasoning proves more\nchallenging than medical reasoning, with AIME presenting the\ngreatest difficulty (14.1–25.8% accuracy) compared to Math-\n500 (43.4–48.3%) and MedQA (42.6–47.6%), reflecting its\ncompetition-level problem complexity.\nA. Pattern Recognition and Interpretation\nWe analyze systematic patterns within the quantified CSD\nand SFC datasets to examine how coherence and convergence\nbehaviors vary across domains, difficulty levels, and correct-\nness outcomes.\n1) CSD Analysis Across Metrics: To identify the most dis-\ncriminative measure of local reasoning coherence, we compare\nfive statistical metrics within the CSD framework. Figure 2\ncompares these metrics for the AIME dataset using Qwen2.5-\n7B.\nAmong the five metrics, KL Divergence demonstrates the\nclearest and most consistent separation between correct and\nincorrect reasoning trajectories, maintaining stability across\nreasoning steps and exhibiting the narrowest confidence inter-\nvals. JS Divergence and Hellinger Distance reveal similar but\nweaker discrimination, particularly in mid-step regions. Cosine\nSimilarity exhibits the expected inverse pattern: correct reason-\ning maintains higher similarity values (indicating consistent\ndistributional alignment), while incorrect reasoning displays\nlower similarity with greater volatility. Entropy Difference\ndisplays higher variability and lacks consistent directionality\nacross correctness classes.\nThis comparison validates KL Divergence as the primary\nmetric for subsequent analyses based on its superior sensitivity\nand interpretability.\n2) CSD Analysis Across Difficulty Levels and Correctness:\nWe analyze how consecutive-step coherence varies across\nproblem difficulty and correctness outcomes using the AIME\ndataset with Qwen2.5-7B. As shown in Figure 3, correct\nreasoning consistently maintains lower and more stable diver-\ngence values compared to incorrect reasoning across all three\ndifficulty levels. At Level 1 (Easy), correct reasoning exhibits\na KL divergence of approximately 4-6 units while incorrect\nreasoning ranges from 6-10 units. As difficulty increases to\nLevels 2 and 3, both correct and incorrect trajectories show\nelevated divergence values and substantially wider confidence\nintervals, indicating that reasoning stability degrades with task\ncomplexity. The persistent separation between correct and\nincorrect reasoning across all difficulty levels demonstrates\nthat CSD effectively captures reasoning quality differences\neven as problem complexity increases.\n3) Reasoning Step-Length Analysis: To examine the effect\nof reasoning-chain length on performance and divergence\npatterns, we group problems into Short, Medium, and Long\ncategories using dataset-adaptive thresholds that reflect each\ndataset’s natural step-count distribution (Table III). This nor-\nmalization is necessary because mathematical problems typi-\ncally require more reasoning steps than medical problems.\nTable III reveals strong domain-dependent behavior. In\nmathematical reasoning, accuracy consistently declines as\nchain length increases. For Math-500 with Qwen2.5-7B, ac-\ncuracy drops from 59.81% (Short) to 31.11% (Long), a 48%\nrelative decrease, while AIME with MathStral-7B shows an\neven steeper decline from 34.69% to 13.64% (61%). This\nmonotonic degradation is consistent across both mathematical\ndatasets and models. In contrast, medical reasoning on MedQA\nexhibits qualitatively different behavior. Qwen-7B improves\naccuracy on longer chains (51.13% →58.21%), while Qwen-\nMed-7B remains relatively stable (39.17% →44.38%), indi-\ncating robustness to extended reasoning.\nFigure 4 explains the mathematical degradation through\nConsecutive Step Divergence (CSD) dynamics. Short and\nmedium chains exhibit lower and more stable divergence for\ncorrect reasoning, whereas long chains show highly volatile\ntrajectories with widened confidence bands, indicating loss\nof local coherence. This instability directly accounts for the\nobserved accuracy decline as reasoning length increases.\nOverall, these results demonstrate that extended reasoning\namplifies instability in mathematical problem-solving, while\nmedical diagnostic reasoning follows distinct dynamics in\nwhich longer chains do not necessarily harm—and may even\nimprove—performance.\n4) SFC vs. CSD Across Difficulty Levels: To compare\nhow local coherence (CSD) and global convergence (SFC)\ncapture reasoning quality, Figure 5 presents both algorithms\non Math-500 with MathStral-7B across five difficulty levels.\nCSD (Fig. 5a) shows varying separation between correct (blue)\nand incorrect (orange) reasoning, with incorrect trajectories\ndisplaying greater volatility, particularly at higher difficulty\nlevels. In contrast, SFC (Fig. 5b) displays consistently mono-\n"}, {"page": 7, "text": "7\nStatistical Metric\n3\n6\n9\n12\nStep Transition\n3\n4\n5\n6\n7\n8\n(a) KL Divergence\n3\n6\n9\n12\nStep Transition\n0.40\n0.48\n0.56\n0.64\n0.72\n(b) JS Divergence\n3\n6\n9\n12\nStep Transition\n0.60\n0.66\n0.72\n0.78\n0.84\n(c) Hellinger Distance\nStatistical Metric\n3\n6\n9\n12\nStep Transition\n0.15\n0.30\n0.45\n0.60\n(d) Cosine Similarity\n3\n6\n9\n12\nStep Transition\n−3.2\n−2.4\n−1.6\n−0.8\n0.0\n0.8\n(e) Entropy-Diff\nCorrect\nIncorrect\nFig. 2. Comparison of five statistical metrics for CSD computation on AIME (Qwen2.5-7B). (a) KL Divergence, (b) JS Divergence, (c) Hellinger Distance,\n(d) Cosine Similarity, and (e) Entropy Difference. Shaded regions: ±1 SD.\n3\n6\n9\n12\nStep Transition\n2.5\n5.0\n7.5\n10.0\n12.5\nKL Divergence\nLevel 1 (Correct: 20, Incorrect: 59)\n3\n6\n9\n12\nStep Transition\n0\n2\n4\n6\n8\nLevel 2 (Correct: 5, Incorrect: 74)\n3\n6\n9\n12\nStep Transition\n3.0\n4.5\n6.0\n7.5\nLevel 3 (Correct: 8, Incorrect: 68)\nCorrect\nIncorrect\nFig. 3.\nCSD analysis across difficulty levels for the AIME dataset using Qwen2.5-7B with KL Divergence. Panels correspond to Level 1 (Easy), Level 2\n(Medium), and Level 3 (Hard).\ntonic decrease for both correctness groups, reflecting gradual\nconvergence toward the final answer regardless of correctness.\nBoth correct and incorrect reasoning follow nearly parallel\ntrajectories with overlapping confidence intervals. This algo-\nrithmic comparison reveals that CSD exposes local structural\ninconsistencies that distinguish correct from incorrect reason-\ning, while SFC captures global convergence strength that oc-\ncurs similarly for both. This fundamental difference has direct\nimplications for predictive modeling (Section IV-B), where the\nability to detect local coherence breakdowns becomes critical\nfor correctness discrimination.\n5) Cross-Domain Comparison: To assess whether CSD\npatterns generalize across domains and models, Figure 6\npresents three dataset–model pairs: AIME with MathStral-\n7B, Math-500 with MathStral-7B, and MedQA with Qwen-\nMedicine-7B, complementing earlier analyses (Figs. 3, 5).\nMathematical datasets show progressive volatility increase\nas difficulty rises. Math-500 (Fig. 6b) demonstrates this across\nfive levels: Level 1 exhibits stable trajectories while Levels 4–\n5 display substantial fluctuations. Despite increasing volatil-\nity and varying separation across individual difficulty levels,\nincorrect reasoning generally displays greater instability with\nwider confidence bands.\nMathematical reasoning (AIME, Math-500) exhibits varying\nbut persistent patterns distinguishing correct from incorrect\ntrajectories, with separation strength depending on difficulty\nlevel and problem structure. Medical reasoning (MedQA,\nFig. 6c) displays parallel, nearly identical trajectories with\nminimal separation, indicating fundamentally different local\ncoherence dynamics. This cross-domain analysis establishes\nthat mathematical and medical reasoning follow fundamentally\ndifferent dynamics. Mathematical reasoning shows difficulty-\n"}, {"page": 8, "text": "8\n1\n2\n3\n4\n5\nStep Transition Index\n3.0\n4.5\n6.0\n7.5\n9.0\nKL Divergence\nShort Steps \n(Correct: 128, Incorrect: 86)\n2\n4\n6\n8\nStep Transition Index\n3.0\n4.5\n6.0\n7.5\n9.0\nMedium Steps \n(Correct: 97, Incorrect: 139)\n4\n8\n12\n16\n20\nStep Transition Index\n3.0\n4.5\n6.0\n7.5\n9.0\nLong Steps \n(Correct: 14, Incorrect: 31)\nCorrect\nIncorrect\nFig. 4. Consecutive Step Divergence (CSD) patterns across step-length categories for the Math-500 dataset using Qwen2.5-7B with KL Divergence. Step-length\ncategories are defined by total reasoning steps: Short (≤6 steps), Medium (7–10 steps), Long (≥11 steps).\n1\n2\n3\n4\n5\n6\nStep Transition\n3.0\n4.5\n6.0\n7.5\nKL Divergence\nLevel 1 (Correct: 35, Incorrect: 7)\n2\n4\n6\n8\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 2 (Correct: 61, Incorrect: 23)\n3\n6\n9\n12\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 3 (Correct: 53, Incorrect: 48)\n2\n4\n6\n8\n10\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 4 (Correct: 36, Incorrect: 86)\n3\n6\n9\n12\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 5 (Correct: 22, Incorrect: 106)\n(a) CSD for Math-500 Mathstral (KL Divergence) across difficulty levels.\n1\n2\n3\n4\n5\n6\nStep Transition\n3.0\n4.5\n6.0\n7.5\n9.0\nKL Divergence\nLevel 1 (Correct: 29, Incorrect: 13)\n2\n4\n6\n8\nStep Transition\n5\n6\n7\n8\n9\nLevel 2 (Correct: 43, Incorrect: 41)\n3\n6\n9\n12\nStep Transition\n0\n2\n4\n6\n8\nLevel 3 (Correct: 41, Incorrect: 60)\n2\n4\n6\n8\n10\nStep Transition\n4.5\n6.0\n7.5\n9.0\nLevel 4 (Correct: 27, Incorrect: 95)\n3\n6\n9\n12\nStep Transition\n4.5\n6.0\n7.5\n9.0\nLevel 5 (Correct: 15, Incorrect: 113)\n(b) SFC for Math-500 Mathstral (KL Divergence) across difficulty levels.\nCorrect\nIncorrect\nFig. 5.\nComparison of Consecutive Step Divergence (CSD) and Step-to-Final Convergence (SFC) using KL Divergence across five difficulty levels in the\nMath-500 dataset with Mathstral-7B. (a) CSD measures local coherence with varying correctness separation. (b) SFC measures global convergence toward\nthe final answer with minimal correctness separation.\nTABLE III\nACCURACY (%) ACROSS DATASETS, MODELS, AND REASONING\nSTEP-LENGTH GROUPS. CELL SHADING INDICATES RELATIVE ACCURACY\nWITHIN THE TABLE (DARKER = HIGHER).\nDataset\nModel\nThresholds\nShort\nMedium\nLong\nMath-AIME\nMathStral-7B\n7–11\n34.69\n20.83\n13.64\nQwen2.5-7B\n6–10\n19.57\n13.77\n10.00\nMath-500\nMathStral-7B\n4–7\n49.15\n40.85\n16.67\nQwen2.5-7B\n6–10\n59.81\n41.10\n31.11\nMed-QA\nQwen-7B\n3–5\n51.13\n45.56\n58.21\nQwen-Med-7B\n3–5\n39.17\n45.99\n44.38\nAccuracy scale (approx.):\n≤10%\n15–20%\n30–40%\n40–50%\n≥55%\ndependent patterns where local coherence provides a signal for\ncorrectness discrimination. Medical reasoning exhibits patterns\nwhere local step-level coherence does not distinguish correct\nfrom incorrect conclusions, indicating that diagnostic reason-\ning relies on mechanisms less reflected in step-level proba-\nbility distributions. This domain-specific finding aligns with\nrecent work demonstrating that chain-of-thought reasoning\nsystematically fails in clinical contexts (86.3% of models show\ndegradation), attributed to the complexity and fragmentation\nof clinical documentation [28]. Our divergence-based analysis\nprovides quantitative evidence for this distinction, suggesting\nthat diagnostic reasoning may require alternative evaluation\nframeworks tailored to clinical inference dynamics.\nB. Predictive Modeling of Reasoning Reliability\nFollowing pattern recognition analysis, which revealed sys-\ntematic differences in CSD and SFC trajectories between\ncorrect and incorrect reasoning, we now evaluate whether these\nquantified features can predict answer correctness. We employ\ntwo complementary approaches: classical machine learning\nmodels (LR, SVM, XGBoost) trained on aggregated statistical\nfeatures, and sequential neural models (NN, GRU, LSTM) that\nprocess stepwise divergence sequences. Performance is evalu-\nated using F1-Score (primary metric), ROC-AUC, Accuracy,\nand Balanced Accuracy for both CSD and SFC algorithms to\nenable direct algorithmic comparison.\n"}, {"page": 9, "text": "9\n3\n6\n9\n12\nStep Transition\n1\n2\n3\n4\n5\n6\nKL Divergence\nLevel 1 (Correct: 39, Incorrect: 41)\n0\n4\n8\n12\n16\n20\nStep Transition\n2\n4\n6\n8\n10\nLevel 2 (Correct: 16, Incorrect: 64)\n3\n6\n9\n12\nStep Transition\n0.0\n1.5\n3.0\n4.5\n6.0\nLevel 3 (Correct: 7, Incorrect: 73)\n(a) AIME – MathStral-7B (KL Divergence) across difficulty levels.\n1\n2\n3\n4\n5\n6\nStep Transition\n3.0\n4.5\n6.0\n7.5\nKL Divergence\nLevel 1 (Correct: 35, Incorrect: 7)\n2\n4\n6\n8\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 2 (Correct: 61, Incorrect: 23)\n3\n6\n9\n12\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 3 (Correct: 53, Incorrect: 48)\n2\n4\n6\n8\n10\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 4 (Correct: 36, Incorrect: 86)\n3\n6\n9\n12\nStep Transition\n1.5\n3.0\n4.5\n6.0\n7.5\nLevel 5 (Correct: 22, Incorrect: 106)\n(b) Math-500 – MathStral-7B (KL Divergence) across difficulty levels.\n2\n4\n6\n8\nStep Transition\n4.0\n4.8\n5.6\n6.4\n7.2\n8.0\nKL Divergence\nLevel 1 (Correct: 281, Incorrect: 390)\n2\n4\n6\n8\n10\nStep Transition\n4.5\n6.0\n7.5\n9.0\nLevel 2 (Correct: 255, Incorrect: 331)\n(c) MedQA – Qwen-Medicine-7B (KL Divergence) across difficulty levels.\nFig. 6.\nCross-domain CSD analysis using KL Divergence across difficulty levels. (a) AIME with MathStral-7B, (b) Math-500 with MathStral-7B, and\n(c) MedQA with Qwen-Medicine-7B.\n1) Classical Machine Learning Models: Table IV presents\nthe best-performing classifier per dataset-LLM-algorithm com-\nbination, with CSD and SFC results shown side-by-side.\nFigure 7 visualizes CSD performance across metrics. CSD\nconsistently outperforms SFC, with the largest margin reach-\ning +17 F1 points on AIME-Qwen2.5. SFC achieves com-\npetitive performance only on balanced mathematical settings\n(Math-500), indicating that local coherence provides stronger\ncorrectness discrimination than global convergence, particu-\nlarly on challenging or imbalanced datasets. The best perfor-\nmance on mathematical reasoning reaches F1=0.78 and ROC-\nAUC=0.82, while medical reasoning shows minimal signal.\nDetailed best SFC performance and the hyperparameters are\nprovided in Supplementary Material Tables S2.\n2) Sequential Neural Models:\nWhile classical machine\nlearning models establish strong baselines, their reliance on\naggregated statistics prevents them from capturing temporal\nreasoning dynamics. We now investigate sequential neural\narchitectures (NN, GRU, LSTM) that directly model stepwise\ndivergence evolution. Table V presents sequential architecture\nperformance across datasets. Figure 8 visualizes CSD results\nacross metrics.\nSequential models outperform classical models (Supple-\nmentary Material Table S3), with average F1 improvements\nof +13.5 points. Math-500 improves from F1=0.77 to 0.88\n(+11 points), while AIME shows dramatic gains from F1=0.57\nto 0.86 (+29 points). Larger improvements on challenging\ndatasets indicate that temporal modeling becomes especially\nvaluable when aggregated features provide a limited signal.\nCSD vs SFC Performance: Sequential architectures narrow\nthe algorithmic gap observed in classical ML. CSD still\noutperforms SFC in most configurations, but margins are\nsmaller. In some cases, SFC becomes superior: on AIME-\nMathStral, SFC (F1 = 0.86, AUC = 0.94) surpasses CSD\n(F1 = 0.77, AUC = 0.88) across all metrics, suggesting that\ntemporal modeling enhances SFC’s ability to leverage global\nconvergence patterns. Even advanced sequential architectures\nshow limited discrimination on medical reasoning (F1=0.63-\n0.70), indicating domain-specific limitations rather than model\ncapacity constraints. Detailed hyperparameters are in Supple-\nmentary Material Tables S3.\n3) Comparison with Existing Methods: Table VI compares\nEvalQReason with automated reasoning evaluation meth-\nods on mathematical datasets. Embedding-based approaches\n(ROSCOE [23]) achieve F1=48.2-51.6 [24], while supervised\nmethods reach F1=62.6-79.6 [7], [21], [24]. EvalQReason\nachieves F1=88.0 without training labels, outperforming the\nprevious best method (ReasonEval, F1=79.6) by 8.4 points.\nThis demonstrates that probability-based divergence analysis\nprovides stronger correctness discrimination than supervised\n"}, {"page": 10, "text": "10\nTABLE IV\nPERFORMANCE OF LR, SVM, AND XGB ML MODELS ACROSS DATASET-LLM PAIRS USING CSD AND SFC FEATURES.\nDataset\nLLM Model\nML model\nCSD\nSFC\nF1\nAUC\nAcc\nBalAcc\nF1\nAUC\nAcc\nBalAcc\nMath-500\nMathStral-7B\nLR\n0.76\n0.82\n0.79\n0.79\n0.67\n0.74\n0.73\n0.72\nSVM\n0.77\n0.81\n0.79\n0.79\n0.72\n0.75\n0.72\n0.74\nXGB\n0.74\n0.78\n0.77\n0.77\n0.68\n0.78\n0.71\n0.71\nQwen2.5-7B\nLR\n0.71\n0.77\n0.72\n0.72\n0.72\n0.78\n0.72\n0.72\nSVM\n0.75\n0.76\n0.74\n0.74\n0.75\n0.77\n0.75\n0.75\nXGB\n0.75\n0.78\n0.74\n0.74\n0.78\n0.76\n0.77\n0.78\nAIME\nMathStral-7B\nLR\n0.57\n0.75\n0.83\n0.70\n0.50\n0.58\n0.58\n0.67\nSVM\n0.50\n0.40\n0.75\n0.67\n0.50\n0.45\n0.67\n0.67\nXGB\n0.53\n0.60\n0.75\n0.69\n0.38\n0.55\n0.72\n0.59\nQwen2.5-7B\nLR\n0.33\n0.56\n0.89\n0.60\n0.36\n0.61\n0.70\n0.65\nSVM\n0.57\n0.82\n0.92\n0.70\n0.32\n0.64\n0.72\n0.60\nXGB\n0.50\n0.67\n0.92\n0.67\n0.40\n0.69\n0.83\n0.65\nMedQA\nQwen2.5-7B\nLR\n0.59\n0.59\n0.59\n0.59\n0.55\n0.58\n0.58\n0.57\nSVM\n0.58\n0.60\n0.60\n0.60\n0.56\n0.62\n0.60\n0.60\nXGB\n0.58\n0.58\n0.61\n0.61\n0.62\n0.58\n0.56\n0.57\nQwen-Med-7B\nLR\n0.59\n0.60\n0.60\n0.61\n0.58\n0.67\n0.62\n0.62\nSVM\n0.58\n0.63\n0.65\n0.64\n0.59\n0.65\n0.62\n0.62\nXGB\n0.54\n0.60\n0.59\n0.59\n0.56\n0.56\n0.62\n0.61\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.57\nLR\n0.77\nSVM\n0.57\nSVM\n0.75\nXGB\n0.59\nLR\n0.59\nLR\n(a) F1-Score\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.75\nLR\n0.81\nSVM\n0.82\nSVM\n0.78\nXGB\n0.59\nLR\n0.60\nLR\n(b) ROC-AUC\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.83\nLR\n0.79\nSVM\n0.92\nSVM\n0.74\nXGB\n0.59\nLR\n0.60\nLR\n(c) Accuracy\nMathStral\nQwen2.5\nQwen medicine\nFig. 7. Best classical ML model performance using CSD features across datasets and LLM models showing (a) F1-Score, (b) ROC-AUC, and (c) accuracy.\nEach bar represents the optimal classifier (LR, SVM, or XGB) for that configuration.\napproaches while eliminating annotation costs.\nV. LIMITATIONS AND FUTURE WORK\nEvalQReason provides a process-level framework for ana-\nlyzing reasoning behavior in LLMs, with several directions\nfor future work. Our experiments focus on open-source 7B-\nparameter models to enable direct access to token-level logits,\nextending to larger models would clarify whether reasoning\nstability improves with scale.\nAn important future direction involves applying EvalQRea-\nson to adversarial and safety-critical scenarios to investigate\nwhether unsafe inputs induce distinctive instability patterns,\ninforming process-level detection mechanisms and reasoning-\naware guardrails.\nFinally, our cross-domain analysis reveals that divergence-\nbased evaluation provides strong signals for mathematical\nreasoning but limited discrimination for medical diagnos-\ntic tasks. This suggests that clinical reasoning may require\ncomplementary evaluation approaches tailored to diagnostic\ninference dynamics, motivating research on domain-adaptive\nframeworks beyond step-level probability analysis.\nVI. CONCLUSION\nThis study introduced EvalQReason, a framework for quan-\ntifying LLM reasoning behavior through step-level divergence\nanalysis using CSD and SFC algorithms. Experiments across\nmathematical and medical domains reveal that correct reason-\ning exhibits stable, low-divergence trajectories, while incorrect\nreasoning shows volatility that intensifies with task complexity.\nCSD consistently outperforms SFC for correctness prediction,\nachieving up to 17 F1-point improvements in classical ML.\nSequential models that capture temporal reasoning patterns de-\n"}, {"page": 11, "text": "11\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.77\nLSTM\n0.88\nNN\n0.86\nNN\n0.81\nLSTM\n0.70\nLSTM 0.63\nLSTM\n(a) F1-Score\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.88\nLSTM\n0.90\nNN\n0.97\nNN\n0.82\nLSTM\n0.68\nLSTM 0.62\nLSTM\n(b) ROC-AUC\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.88\nLSTM\n0.88\nNN\n0.96\nNN\n0.80\nLSTM\n0.67\nLSTM\n0.51\nLSTM\n(c) Accuracy\nMathStral\nQwen2.5\nQwen medicine\nFig. 8. Best sequential model performance using CSD features across datasets and LLM models, showing (a) F1 Score, (b) ROC-AUC, (c) Accuracy. Each\nbar represents the optimal architecture (NN, GRU, or LSTM) for that configuration.\nTABLE V\nPERFORMANCE OF NN, GRU, AND LSTM ARCHITECTURES ACROSS DATASET-LLM PAIRS USING CSD AND SFC FEATURES.\nDataset\nLLM Model\nCSD\nSFC\nModel\nF1\nAUC\nAcc\nBalAcc\nModel\nF1\nAUC\nAcc\nBalAcc\nMath-500\nMathStral-7B\nNN\n0.88\n0.90\n0.88\n0.89\nNN\n0.78\n0.83\n0.78\n0.80\nGRU\n0.77\n0.86\n0.75\n0.77\nLSTM\n0.75\n0.80\n0.72\n0.75\nLSTM\n0.77\n0.84\n0.75\n0.77\nGRU\n0.74\n0.85\n0.71\n0.74\nQwen2.5-7B\nLSTM\n0.81\n0.82\n0.80\n0.81\nNN\n0.82\n0.87\n0.82\n0.82\nGRU\n0.80\n0.82\n0.76\n0.76\nLSTM\n0.80\n0.83\n0.80\n0.80\nNN\n0.80\n0.88\n0.76\n0.77\nGRU\n0.79\n0.76\n0.76\n0.77\nAIME\nMathStral-7B\nLSTM\n0.77\n0.88\n0.88\n0.86\nLSTM\n0.86\n0.94\n0.92\n0.94\nGRU\n0.77\n0.80\n0.88\n0.86\nGRU\n0.80\n0.94\n0.88\n0.92\nNN\n0.77\n0.82\n0.88\n0.86\nNN\n0.71\n0.94\n0.79\n0.86\nQwen2.5-7B\nNN\n0.86\n0.97\n0.96\n0.98\nNN\n0.80\n0.79\n0.96\n0.83\nGRU\n0.80\n1.00\n0.96\n0.83\nGRU\n0.73\n0.90\n0.92\n0.87\nLSTM\n0.75\n0.90\n0.92\n0.95\nLSTM\n0.73\n0.91\n0.92\n0.87\nMedQA\nQwen2.5-7B\nLSTM\n0.70\n0.68\n0.67\n0.68\nNN\n0.67\n0.55\n0.57\n0.58\nGRU\n0.69\n0.67\n0.63\n0.64\nGRU\n0.67\n0.64\n0.53\n0.55\nNN\n0.69\n0.62\n0.61\n0.62\nLSTM\n0.67\n0.66\n0.53\n0.55\nQwen-Med-7B\nLSTM\n0.63\n0.62\n0.51\n0.57\nGRU\n0.64\n0.66\n0.60\n0.63\nGRU\n0.63\n0.63\n0.55\n0.59\nNN\n0.62\n0.59\n0.52\n0.58\nNN\n0.63\n0.63\n0.51\n0.56\nLSTM\n0.62\n0.62\n0.50\n0.56\nTABLE VI\nPERFORMANCE COMPARISON WITH REASONING EVALUATION METHODS\nON MATHEMATICAL REASONING TASKS, RESULTS REPORTED ON\nMATH-500 AND MR-MATH DATASETS\nMethod\nF1\nAUC\nEmbedding-Based\nROSCOE-SA [24]\n48.2\n57.5\nROSCOE-SS [24]\n51.6\n49.6\nSupervised (Require Training)\nMath-Shepherd [24]\n70.1\n77.3\nProcessBench (PRM-Qwen2.5-Math-7B-PRM800K) [7]\n62.6\n–\nProcessBench (Critic-QwQ-32B-Preview) [7]\n78.7\n–\nReasonEval [24]\n79.6\n90.8\nUnsupervised (No Training)\nEvalQReason-CSD (Ours)\n88.0\n90.0\nliver further gains (+6 to +29 F1 points), reaching F1=0.88 and\nROC-AUC=0.97 on mathematical tasks. Critically, reasoning\ndynamics prove domain-specific: mathematical reasoning ex-\nhibits clear divergence-based patterns, while medical reasoning\nshows minimal discriminative signal, indicating fundamentally\ndifferent inference mechanisms.\nEvalQReason enables scalable and interpretable evaluation\nof LLM reliability by shifting assessment from final outcomes\nto process-level reasoning dynamics, addressing a critical gap\nin trustworthy AI assessment. This divergence-based perspec-\ntive lays the groundwork for extending process-level reasoning\nanalysis to adversarial robustness and safety monitoring, where\nreasoning-pattern analysis may support early detection of\nharmful or manipulative inputs—an essential capability for\ndeploying LLMs in high-stakes applications.\nAPPENDIX\nCONTENTS\n• Section S1: Prompts for Reasoning Chain Generation\nComplete prompts used for mathematical and medical\n"}, {"page": 12, "text": "12\nreasoning (Figure S1)\n• Section S2: Statistical Metrics for Divergence Analysis\nMathematical definitions of KL divergence, JS diver-\ngence, Hellinger distance, cosine similarity, and entropy\n• Section S3: Additional Performance Visualizations\nPerformance comparisons between classical ML and\nsequential models, SFC performance plots, and cross-\ndomain CSD analysis (Figures S2–SX)\n• Section S4: Hyperparameter Configurations\nComplete hyperparameters for classical ML and sequen-\ntial models (Tables S1–S4)\nA. S1. Prompts for Reasoning Chain Generation\nFig. 1 presents the complete prompts used to elicit struc-\ntured reasoning chains from LLMs across mathematical and\nmedical domains. Specifically, the mathematical prompt in-\nstructs models to act as expert mathematicians, solving prob-\nlems through step-by-step reasoning, while the medical prompt\nframes the model as a medical board examination tutor, guid-\ning students through USMLE-style multiple-choice questions\nusing structured clinical reasoning.\nB. S2. Statistical Metrics for Divergence Analysis\nEvalQReason employs five complementary statistical met-\nrics to quantify distributional relationships in CSD and SFC\nalgorithms. These metrics capture distinct aspects of reasoning\ndynamics through divergence measures, similarity measures,\nand uncertainty quantification.\n1) S2.1 Divergence Measures: Kullback-Leibler (KL) Di-\nvergence. Quantifies information loss when Q approximates\nP, providing a directed measure of distributional difference:\nKL(P∥Q) =\nX\nx\nP(x) log P(x)\nQ(x)\nKL divergence is non-symmetric and unbounded, making\nit suitable for measuring directed distributional shifts between\nconsecutive reasoning steps.\nJensen-Shannon\n(JS)\nDivergence. A symmetric and\nbounded divergence measure:\nJSD(P∥Q) = 1\n2KL(P∥M) + 1\n2KL(Q∥M)\nMath Prompt\nYou are an expert mathematician. Solve the following\nmath problem by thinking clearly and step by step. \nUse the format:\nStep 1: [Reasoning]  \nStep 2: [Reasoning]  …\nFinal Answer:  \\\\boxed {your final numeric answer}\nGuidelines:\n- Make each step distinct, logical, and concise.\n- Do not repeat reasoning across steps.\n- Provide only one final answer at the end, using the\nboxed format.\nExample:\nStep 1: Let x = 5 and y = 3. Compute their product.  \nStep 2: Multiply x and y to get 5 × 3 = 15.  \nFinal Answer: 15\nNow solve the problem below.\nMedical Prompt\nYou are a medical board exam tutor. Your job is to\nhelp students answer USMLE-style multiple-choice\nquestions by thinking through clinical reasoning in a\nstructured way.\nInstructions:\n- Work through the case step by step, analyzing\nsymptoms and lab findings.\n- Eliminate incorrect options with justification.\n- Highlight any guideline-based treatments or\ncontraindications.\nRespond using this format:\nStep 1: [Highlight key information and interpret it]  \nStep 2: [Eliminate one or more incorrect answers]  ...  \nFinal Answer: \\\\boxed{<Letter>. <Correct Answer\nText>}\nLet’s solve the following question:\nFig. 9. Math and medical prompts are used to elicit structured step-by-step\nreasoning from LLMs.\nwhere M = 1\n2(P + Q). JSD is symmetric and bounded in [0,\n1], making it suitable for comparing reasoning steps without\ndirectional bias.\nHellinger Distance. A bounded metric sensitive to shifts in\nprobability mass:\nH(P, Q) =\n1\n√\n2\n\r\r\r\n√\nP −\np\nQ\n\r\r\r\nHellinger distance is particularly sensitive to transitions\nin high-probability regions, highlighting major distributional\nshifts.\n2) S2.2 Similarity Measure: Cosine Similarity. Measures\nangular alignment between probability distributions:\ncos(P, Q) =\nP · Q\n∥P∥∥Q∥\nValues range from -1 (opposite directions) to +1 (identical\ndirections). Higher values indicate that consecutive reasoning\nsteps maintain consistent distributional patterns.\n3) S2.3 Uncertainty Measure: Entropy Difference. Mea-\nsures changes in uncertainty between reasoning steps. The\nentropy of distribution P is:\nH(P) = −\nX\nx\nP(x) log P(x)\nFor CSD, we compute signed entropy change: ∆Hi =\nH(Pi+1) −H(Pi). For SFC, we compute absolute deviation:\n∆Hi = |H(Pi) −H(Pfinal)|.\n4) S2.4 Implementation Details: All divergences are com-\nputed over the full vocabulary without truncation, using natural\nlogarithms (units: nats). For numerical stability, we apply\nadditive smoothing: ˜pi = ¯pi + ϵ where ϵ = 10−7.\nC. S3. Additional Performance Visualizations\nThis section provides supplementary visualizations support-\ning the analysis in the main text, including comparative perfor-\nmance analysis and step-to-final convergence (SFC) patterns.\n1) S3.1. Classical ML vs. Sequential Model Comparison:\nTable S presents a comprehensive comparison of predictive\nperformance between classical machine learning and sequen-\ntial neural network models across all datasets and algo-\nrithms. Sequential models consistently outperform classical\napproaches, demonstrating the value of temporal modeling in\ncapturing reasoning dynamics. Performance gains are partic-\nularly pronounced on mathematical datasets, where sequen-\ntial models improve F1-scores by 6–29 points over classical\nmethods, with the largest improvements observed on AIME.\nThis superior performance reflects sequential models’ ability\nto capture temporal patterns such as mid-step instability and\nprogressive degradation that static feature aggregation cannot\ndetect.\n2) S3.2. SFC Performance Analysis: To complement the\nCSD performance visualizations in the main text (Figures 2\nand 3), this subsection provides corresponding performance\nplots for step-to-final convergence (SFC) features.\nFigures S2 present SFC-based performance to complement\nthe CSD visualizations in the main text. SFC achieves com-\npetitive performance on Math-500 but underperforms CSD on\nAIME.\n"}, {"page": 13, "text": "13\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.50\nLR\n0.72\nSVM\n0.40\nXGB\n0.78\nXGB\n0.62\nXGB\n0.59\nSVM\n(a) F1-Score\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.58\nLR\n0.75\nSVM\n0.69\nXGB\n0.76\nXGB\n0.58\nXGB\n0.65\nSVM\n(b) ROC-AUC\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.58\nLR\n0.72\nSVM\n0.83\nXGB\n0.77\nXGB\n0.56\nXGB\n0.62\nSVM\n(c) Accuracy\nMathStral\nQwen2.5\nQwen medicine\nFig. 10. Best Classical ML model performance using SFC features across datasets and LLM models showing (a) F1-Score, (b) ROC-AUC, and (c) Accuracy.\nEach bar represents the optimal classifier (LR, SVM, or XGB) for that configuration.\nTABLE VII\nCLASSICAL ML VS SEQUENTIAL MODELS: PERFORMANCE GAINS USING\nCSD FEATURES\nDataset\nLLM\nClassical\nSequential\nGain\nF1 (Model)\nF1 (Model)\n(pts)\nMath-500\nMathStral\n0.77 (SVM)\n0.88 (NN)\n+11\nMath-500\nQwen\n0.75 (SVM)\n0.81 (LSTM)\n+6\nAIME\nMathStral\n0.57 (LR)\n0.77 (LSTM)\n+20\nAIME\nQwen\n0.57 (SVM)\n0.86 (NN)\n+29\nMedQA\nQwen\n0.59 (LR)\n0.70 (LSTM)\n+11\nMedQA\nQwenMed\n0.59 (LR)\n0.63 (LSTM)\n+4\nAverage Improvement:\n+13.5\nFigures S3 presents the best-performing sequential model\nresults using SFC features. Sequential architectures substan-\ntially narrow the CSD-SFC performance gap observed in\nclassical models, with SFC achieving competitive or superior\nresults on several configurations.\nD. S4. Hyperparameter Configurations\nThis section provides detailed hyperparameter configura-\ntions for all predictive models to ensure full reproducibility\nof results reported in the main paper.\n1) S4.1. Classical Machine Learning Models: Table S2\nreports hyperparameters for the best-performing classical ML\nclassifiers using both CSD and SFC features. All models\nwere optimized through grid search, randomized search, and\nBayesian optimization, with final configurations selected based\non F1-Score on validation sets.\n2) S4.2. Sequential Neural Network Models: Table S3\nreports hyperparameters for the best-performing sequential\nmodels (NN, GRU, LSTM) using both CSD and SFC features.\nSequential models were optimized by exploring learning rate,\nL2 regularization, and dropout configurations. All models use\nhidden dimensions of 64 and 128, ReLU activation, batch\nsize 32, and early stopping with patience of 50 epochs. Final\nconfigurations were selected based on F1-Score on validation\nsets.\nREFERENCES\n[1] K. Singhal et al., “Large language models encode clinical knowledge,”\nNature, vol. 620, no. 7972, pp. 172–180, 2023. [Online]. Available:\nhttps://www.nature.com/articles/s41586-023-06291-2\n[2] T.\nSavage\net\nal.,\n“Diagnostic\nreasoning\nprompts\nreveal\nthe\npotential for large language model interpretability in medicine,”\nNPJ Digital Medicine, vol. 7, no. 1, p. 20, 2024. [Online]. Available:\nhttps://www.nature.com/articles/s41746-024-01010-1\n[3] D. Hendrycks et al., “Measuring mathematical problem solving with the\nmath dataset,” arXiv preprint arXiv:2103.03874, 2021.\n[4] P.\nLu\net\nal.,\n“A\nSurvey\nof\nDeep\nLearning\nfor\nMathematical\nReasoning,” in Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\nA. Rogers, J. Boyd-Graber, and N. Okazaki, Eds.\nAssociation\nfor Computational Linguistics, 2023, pp. 14 605–14 631. [Online].\nAvailable: https://aclanthology.org/2023.acl-long.817/\n[5] Y. Chang et al., “A Survey on Evaluation of Large Language\nModels,” ACM transactions on intelligent systems and technology,\nvol.\n15,\nno.\n3,\npp.\n39:1–39:45,\n2024.\n[Online].\nAvailable:\nhttps://doi.org/10.1145/3641289\n[6] Z.\nLi\net\nal.,\n“Think-bench:\nEvaluating\nthinking\nefficiency\nand\nchain-of-thought quality of large reasoning models,” arXiv preprint\narXiv:2505.22113, 2025.\n[7] C. Zheng et al., “Processbench: Identifying process errors in mathe-\nmatical reasoning,” in Proceedings of the 63rd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\n2025, pp. 1009–1024.\n[8] Y. Wang et al., “A comprehensive survey on trustworthiness in reasoning\nwith large language models,” arXiv preprint arXiv:2509.03871, 2025.\n[9] Y. Huang et al. TrustLLM: Trustworthiness in Large Language Models.\n[Online]. Available: http://arxiv.org/abs/2401.05561\n[10] H. Lightman et al., “Let’s verify step by step,” in The Twelfth Interna-\ntional Conference on Learning Representations, 2023.\n[11] S. Hao et al. LLM Reasoners: New Evaluation, Library, and Analysis\nof Step-by-Step Reasoning with Large Language Models. [Online].\nAvailable: http://arxiv.org/abs/2404.05221\n[12] H. Do et al. What Defines Good Reasoning in LLMs? Dissecting\nReasoning Steps with Multi-Aspect Evaluation. [Online]. Available:\nhttp://arxiv.org/abs/2510.20603\n[13] S. Kullback and R. A. Leibler, “On information and sufficiency,” The\nannals of mathematical statistics, vol. 22, no. 1, pp. 79–86, 1951.\n[14] J. Lin, “Divergence measures based on the shannon entropy,” IEEE\nTransactions on Information Theory, vol. 37, no. 1, pp. 145–151, 1991.\n[15] E. Hellinger, “Neue begr¨undung der theorie quadratischer formen von\nunendlichvielen ver¨anderlichen,” Journal f¨ur die reine und angewandte\nMathematik, vol. 136, pp. 210–271, 1909.\n"}, {"page": 14, "text": "14\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.86\nLSTM\n0.78\nNN\n0.80\nNN\n0.82\nNN\n0.67\nNN\n0.64\nGRU\n(a) F1-Score\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.94\nLSTM\n0.83\nNN\n0.79\nNN\n0.87\nNN\n0.55\nNN\n0.66\nGRU\n(b) ROC-AUC\nAIME\nMath-500\nMedQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.92\nLSTM\n0.78\nNN\n0.96\nNN\n0.82\nNN\n0.57\nNN\n0.60\nGRU\n(c) Accuracy\nMathStral\nQwen2.5\nQwen medicine\nFig. 11. Best Sequential model performance using SFC features across datasets and LLM models, showing (a) F1-Score, (b) ROC-AUC, and (c) Accuracy.\nEach bar represents the optimal architecture (NN, GRU, or LSTM) for that configuration.\nTABLE VIII\nBEST-PERFORMING MACHINE LEARNING CLASSIFIERS AND HYPERPARAMETERS FOR CSD AND SFC ACROSS DATASETS.\nDataset\nLLM\nModel\nCSD Hyperparameters\nSFC Hyperparameters\nMath-500\nMathStral-7B\nSVM\nkernel=linear, C=0.08\nkernel=linear, C=0.08\nMath-500\nQwen2.5-7B\nXGB\nlr= 0.07, estimators= 200\nlr= 0.07, estimators= 200\nMath-AIME\nMathStral-7B\nLR\nC=10, solver=lbfgs\nC=10, solver=lbfgs\nMath-AIME\nQwen2.5-7B\nSVM\nkernel=poly, C=3, γ=0.07\nkernel=poly, C=3, γ=0.07\nMed-QA\nQwen-7B\nLR\nC=7\nC=7\nMed-QA\nQwen-Med-7B\nLR\nC=1e−4, solver=lbfgs\nC=1e−4, solver=lbfgs\nTABLE IX\nBEST-PERFORMING SEQUENTIAL MODELS AND TUNED HYPERPARAMETERS FOR CSD AND SFC ACROSS DATASETS.\nDataset\nLLM\nModel\nCSD\nSFC\nLR\nl2\nDropout\nLR\nl2\nDropout\nMath-500\nMathStral-7B\nNN\n5e–4\n0.1\n0.3\n1e–4\n0.001\n0.2\nMath-500\nQwen2.5-7B\nLSTM\n5e–3\n0.001\n0.2\n5e–3\n0.1\n0.1\nMath-AIME\nMathStral-7B\nLSTM\n5e–3\n0.001\n0.2\n1e–4\n0.1\n0.01\nMath-AIME\nQwen2.5-7B\nNN\n1e–4\n0.1\n0.2\n1e–3\n0.1\n0.3\nMed-QA\nQwen-7B\nLSTM\n1e–3\n1e–5\n0.2\n1e–3\n0.01\n0.1\nMed-QA\nQwen-Med-7B\nLSTM\n5e–4\n0.1\n0.3\n5e–4\n0.001\n0.3\n[16] G. G. Chowdhury, Introduction to modern information retrieval.\nFacet\npublishing, 2010.\n[17] T. M. Cover, Elements of information theory.\nJohn Wiley & Sons,\n1999.\n[18] J. Lee and J. Hockenmaier, “Evaluating step-by-step reasoning traces:\nA survey,” arXiv preprint arXiv:2502.12289, 2025.\n[19] P. Mondorf and B. Plank, “Beyond accuracy: evaluating the rea-\nsoning behavior of large language models–a survey,” arXiv preprint\narXiv:2404.01869, 2024.\n[20] K. Cobbe et al., “Training verifiers to solve math word problems,” arXiv\npreprint arXiv:2110.14168, 2021.\n[21] P. Wang et al., “Math-shepherd: Verify and reinforce llms step-by-\nstep without human annotations,” in Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), 2024, pp. 9426–9439.\n[22] J. Gu et al., “A survey on llm-as-a-judge,” The Innovation, 2024.\n[23] O. Golovneva et al., “Roscoe: A suite of metrics for scoring step-by-step\nreasoning,” arXiv preprint arXiv:2212.07919, 2022.\n[24] S. Xia et al., “Evaluating mathematical reasoning beyond accuracy,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 39,\nno. 26, 2025, pp. 27 723–27 730.\n[25] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[26] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdh-\nery, and D. Zhou, “Self-consistency improves chain of thought reasoning\nin language models,” arXiv preprint arXiv:2203.11171, 2022.\n[27] X. Liu et al. Are LLMs Capable of Data-based Statistical and\nCausal Reasoning? Benchmarking Advanced Quantitative Reasoning\nwith Data. [Online]. Available: http://arxiv.org/abs/2402.17644\n[28] J. Wu et al. Why Chain of Thought Fails in Clinical Text Understanding.\n[Online]. Available: http://arxiv.org/abs/2509.21933\n[29] S. Kadavath et al., “Language models (mostly) know what they know,”\narXiv preprint arXiv:2207.05221, 2022.\n[30] G. Hiranandani et al. Logits are All We Need to Adapt Closed Models.\n[Online]. Available: http://arxiv.org/abs/2502.06806\n[31] README.md\n·\ngneubig/aime-1983-2024\nat\nmain.\n[On-\nline].\nAvailable:\nhttps://huggingface.co/datasets/gneubig/aime-1983-\n2024/blob/main/README.md\n[32] D. Jin et al., “What Disease Does This Patient Have? A Large-Scale\nOpen Domain Question Answering Dataset from Medical Exams,”\n"}, {"page": 15, "text": "15\nApplied Sciences, vol. 11, no. 14, p. 6421, 2021. [Online]. Available:\nhttps://www.mdpi.com/2076-3417/11/14/6421\n[33] MathStral\n—\nMistral\nAI.\n[Online].\nAvailable:\nhttps://mistral.ai/news/mathstral\n[34] A.\nYang\net\nal.\nQwen2\nTechnical\nReport.\n[Online].\nAvailable:\nhttp://arxiv.org/abs/2407.10671\n[35] WangCa/Qwen2.5-7B-Medicine · Hugging Face. [Online]. Available:\nhttps://huggingface.co/WangCa/Qwen2.5-7B-Medicine\n[36] nostalgebraist.\n(2020)\nInterpreting\nGPT:\nThe\nlogit\nlens.\nAccessed:\n2025-11-28.\n[Online].\nAvailable:\nhttps://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-\ngpt-the-logit-lens\n[37] N. Belrose et al. Eliciting Latent Predictions from Transformers with\nthe Tuned Lens. [Online]. Available: http://arxiv.org/abs/2303.08112\n[38] K.\nCho\net\nal.,\n“Learning\nPhrase\nRepresentations\nusing\nRNN\nEncoder–Decoder for Statistical Machine Translation,” in Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), A. Moschitti, B. Pang, and W. Daelemans,\nEds.\nAssociation for Computational Linguistics, 2014, pp. 1724–1734.\n[Online]. Available: https://aclanthology.org/D14-1179/\n[39] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997. [Online]. Available:\nhttps://doi.org/10.1162/neco.1997.9.8.1735\nShaima Ahmad Freja received the M.Sc. degree in data science from the\nUniversity of Stavanger, Norway. She is currently pursuing the Ph.D. degree\nin trustworthy artificial intelligence with the University of Stavanger. Her\nresearch interests include the security, privacy, and reliability of generative\nAI systems.\nFerhat Ozgur Catak received the Ph.D. degree in informatics from Istanbul\nUniversity, Istanbul, Turkey, in 2014. He is currently an Associate Profes-\nsor with the Department of Electrical Engineering and Computer Science,\nUniversity of Stavanger, Stavanger, Norway. He has previously held research\npositions at the Norwegian University of Science and Technology (NTNU)\nand Simula Research Laboratory, Norway. His research interests include\ntrustworthy and robust artificial intelligence, secure and explainable machine\nlearning, radio frequency signal processing, spectrum sensing, and next-\ngeneration (6G) wireless communication systems. He has authored numerous\npeer-reviewed journal and conference papers and a research monograph on\ntrustworthy AI. Dr. Catak is a Senior Member of the IEEE and currently\nserves as the Chair of the IEEE Norway Communications Society Chapter.\nBetul Yurdem received the B.Sc. and M.Sc. degrees in Electrical and Elec-\ntronics Engineering from Dokuz Eylul University, Turkiye. She is currently\na Research Assistant and Ph.D. candidate at Izmir Bakircay University.\nHer research interests include trustworthy AI, vision-language models, and\nbiomedical signal processing.\nChunming Rong is a Professor and the head of the Data-centered and Secure\nComputing (DSComputing) at the University of Stavanger (UiS) and Chief\nScientist (adjunct) at NORCE Norwegian Research Centre. His research work\nfocuses on cloud computing, data analytics, cyber security, and blockchain. He\nserved as a co-chair of IEEE Blockchain in 2018, as the chair of IEEE Cloud\nComputing from 2017 to 2020, and has extensive experience in managing\nlarge-scale R&D projects, both in Norway and the EU. He is a Senior Member\nof the IEEE.\n"}]}