{"doc_id": "arxiv:2512.00194", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.00194.pdf", "meta": {"doc_id": "arxiv:2512.00194", "source": "arxiv", "arxiv_id": "2512.00194", "title": "AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI", "authors": ["Zag ElSayed", "Grace Westerkamp", "Gavin Gammoh", "Yanchen Liu", "Peyton Siekierski", "Craig Erickson", "Ernest Pedapati"], "published": "2025-11-28T20:19:34Z", "updated": "2025-11-28T20:19:34Z", "summary": "We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.00194v1", "url_pdf": "https://arxiv.org/pdf/2512.00194.pdf", "meta_path": "data/raw/arxiv/meta/2512.00194.json", "sha256": "913558416daf3ac6d1869008254ae68102d2f4f02d7f00a669e2510168a81f6a", "status": "ok", "fetched_at": "2026-02-18T02:25:51.965380+00:00"}, "pages": [{"page": 1, "text": "AutocleanEEG - ICVision: Automated ICA Artifact\nClassification Using Vision-Language AI\nZag ElSayed, Senior Member IEEE\nSchool of Information Technology\nUniversity of Cincinnati\nOhio, USA\nGrace Westerkamp, Gavin Gammoh, Yanchen Liu, Peyton Siekierski, Craig Erickson, Ernest Pedapati\nDivision of Child and Adolescent Psychiatry\nCincinnati Children’s Hospital Medical Center (CCHMC)\nOhio, USA\nAbstract—We introduce EEG Autoclean Vision-Language AI\n(ICVision)—a first-of-its-kind system that emulates expert-level\nEEG ICA component classification through AI-agent vision and\nnatural language reasoning. Unlike conventional classifiers such\nas ICLabel, which rely on handcrafted features, ICVision directly\ninterprets ICA dashboard visualizations topography, time series,\npower spectra, and ERP plots, using a multimodal large language\nmodel (GPT-4 Vision). This allows the AI to ”see and explain”\nEEG components the way trained neurologists do, making it\nthe first scientific implementation of AI-agent visual cognition in\nneurophysiology. ICVision classifies each component into one of\nsix canonical categories (brain, eye, heart, muscle, channel noise,\nand other noise), returning both a confidence score and a human-\nlike explanation. Evaluated on 3,168 ICA components from 124\nEEG datasets, ICVision achieved κ = 0.677 agreement with ex-\npert consensus, surpassing MNE-ICLabel, while also preserving\nclinically relevant brain signals in ambiguous cases. Over 97% of\nits outputs were rated as interpretable and actionable by expert\nreviewers. As a core module of the open-source EEG Autoclean\nplatform, ICVision signals a paradigm shift in scientific AI, where\nmodels don’t just classify, but see, reason, and communicate. It\nopens the door to globally scalable, explainable, and reproducible\nEEG workflows, marking the emergence of AI agents capable of\nexpert-level visual decision-making in brain science and beyond.\nIndex Terms—EEG preprocessing, Autoclean, ICA compo-\nnent, Vision-language AI, Explainable artificial intelligence, XAI,\nartifact rejection, AI-agent, visual cognition, Brain-computer\ninterface, BCI, Neurological signal interpretation.\nI. INTRODUCTION\nElectroencephalography (EEG) remains a cornerstone of\nboth clinical diagnostics and experimental neuroscience, of-\nfering a direct, millisecond-scale view into neural activity. Yet\nEEG’s greatest strength, its exquisite sensitivity, also reveals\nits greatest challenge: it captures everything, including what\nwe wish it wouldn’t. Muscle activity, eye movements, heart-\nbeats, and electromagnetic interference routinely contaminate\nrecordings, making the distinction between brain signals and\nartifacts both essential and challenging.\nOne of the most effective approaches to solve this problem\nis Independent Component Analysis (ICA), a blind source\nseparation technique that decomposes EEG signals into in-\ndependent components (ICs). ICA allows researchers and\nclinicians to isolate and remove artifact components while\npreserving brain activity. But ICA offers no labels. Deter-\nmining accurately whether a component reflects brain activity\nor an artifact has long required manual visual inspection of\nICA diagnostic plots, including scalp topographies, time series\ntraces, power spectra, and ERP image segments. This expert-\ndriven process, although effective, is poorly scalable and time-\nconsuming.\nTo address this challenge, semi-automated classification\ntools, such as ICLabel [1] [2], have emerged. ICLabel utilizes\na supervised neural network trained on large, labeled EEG\ndatasets to classify ICA components into a predefined set of\ncategories (e.g., brain, muscle, eye, heart, and line noise).\nWhile ICLabel has improved reproducibility and reduced\nmanual effort, it operates strictly on predefined numerical\nfeatures extracted from components, lacking contextual aware-\nness and offering no descriptive insight into its decisions. In\npractice, this black-box nature has limited its clinical adoption,\nparticularly in environments that demand interpretability and\ntraceability for regulatory compliance and stakeholder trust.\nA. The Trifecta of Challenges\nEEG labs face a convergence of three core issues that hinder\nreproducibility and scale. Scalability, remains a bottleneck as\ndatasets grow into tens of thousands of components, making\nmanual ICA review impractical. Consistency, is challenged by\nthe subjectivity of interpretation, even experienced analysts\noften disagree or shift their criteria over time. Continuity,\nsuffers when expert reviewers leave, taking with them implicit\nclassification logic that was never documented. While tools\nlike ICLabel [3] aim to address scalability, often sacrifice\naccuracy and transparency, limiting trust and clinical adoption.\nB. ICVision Vision-Language AI\nTo overcome these limitations, we developed ICVision, a\nvision-language AI system that performs expert-level ICA\ncomponent classification by analyzing the same visual plots\nused by human reviewers. It is the first model to pair image-\nbased classification with confidence scores and natural lan-\nguage reasoning, enabling interpretable, scalable decision-\narXiv:2512.00194v1  [cs.CV]  28 Nov 2025\n"}, {"page": 2, "text": "making. ICVision serves as a core module in the open-source\nEEG Autoclean platform, designed to automate EEG prepro-\ncessing and analysis at scale (https://github.com/cincibrainlab/\nautocleaneeg\\textunderscorepipeline).\nFig. 1.\nThe System Dashboard ICvision component image labeled parts\ncovering: 1) Topography (Spatial), 2) Time Series (Temporal), 3) Power\nSpectrum (Frequency), and 4) ERP Image (Trial-level)\nTo enhance scalability and accessibility, we chose to use\nOpenAI’s GPT 4.1 multimodal (”Vision”) large language\nmodel (LLM) [4]rather than a fine-tuned model. The model\ndemonstrates strong baseline performance and still allows for\nfuture fine-tuning to optimize performance for specialized\ntasks or cohorts as needed. The model utilizes a panel figure\nwith plots representing IC characteristics to classify each com-\nponent into a standard IC category classification, and provides\na descriptive explanation for its classification decision. As\nshown in Fig.1, each component is represented by a single\ncomposite image containing:\n• Topographic map (top-left): spatial distribution across the\nscalp.\n• Time series plot (top-right): temporal structure.\n• Continuous data segments or ERP-image (bottom-left):\ntrial-wise pattern regularity.\n• Power spectral density plot (bottom-right): frequency\ncontent and dominant bands.\nThese are precisely the diagnostic cues used by expert\nhuman reviewers. By submitting the full image to a vision-\nlanguage transformer, ICVision processes the component\nholistically interpreting spatial, temporal, and spectral signa-\ntures simultaneously.\nC. Technical Approach\nThe classification process begins with Extended Infomax\nICA decomposition of a multichannel EEG timeseries (imple-\nmented in EEGLAB or MNE Python), followed by automated\nvisualization of each component in the standard multi-panel\nformat. These visualizations are passed as inputs to the Ope-\nnAI GPT-4.1 Vision API [3], which is instructed via custom\nprompt engineering to: Analyze the visual input, Assign one\nof seven class labels, Provide a brief yet rigorous explanation,\nReturn a confidence score for each prediction. The model\nis stateless and deterministic under fixed prompts, ensuring\nreproducibility. It supports batch processing of hundreds of\ncomponents with high throughput.\nFig. 2. A logical comparison of Feature-Based vs Vision-Based ICA Classi-\nfication block diagram\nD. Vision-Language Reasoning\nThe core innovation of this system lies in its explainability.\nInstead of treating the model’s label as the endpoint, we treat\nit as a hypothesis and the accompanying natural language\nexplanation as the reasoning trail. For example, a component\nwith minor vertical eye movement might be labeled as “brain”\nrather than “eye” if the power spectrum and ERP-image show\ndominant oscillatory structure consistent with alpha rhythms.\nICVision may justify this by stating:\n“This component’s spatial map suggests frontal origin, but\nthe consistent rhythmic activity in the 8–12 Hz range and\nclean trial-wise alignment are more consistent with neural\noscillatory processes than artifact.”\nThis human-readable insight can be used by clinicians to verify\ndecisions, by researchers to audit preprocessing, or by trainees\nto learn EEG interpretation. A logical comparison between\ncurrent methods and the proposed vision method is shown in\nFig.2.\nE. Verification and Impact\nWe validated ICVision on a dataset of 124 ICA dashboard\nimages from real EEG recordings across varied paradigms.\nExperienced reviewers curated component labels, and ICVi-\nsion achieved 95% agreement with expert consensus, out-\nperforming ICLabel in ambiguous cases and maintaining\nreproducibility across sessions. The model was trained and\nevaluated on a curated dataset of 124 ICA dashboard images,\neach labeled by an expert technician. The EEG data were\npreprocessed using EEGLAB [5] [6] and MNE-Python [7] [8],\nensuring compatibility with standard neuroscience workflows.\nIn one illustrative case, ICLabel rejected a component with\nmixed eye-brain features, while ICVision retained it—leading\nto improved alpha-band definition in the cleaned data. In\naddition, heart rate components are notoriously challenging\nfor automated classifiers to identify, but trivial for human\nexperts and ICVision, which clearly identify the repetitive QRS\ncomplex. This shows not only higher classification fidelity but\n"}, {"page": 3, "text": "also tangible impact on downstream signal quality. Beyond\naccuracy, the actual value of ICVision lies in preserving\ninstitutional EEG knowledge, enabling transparent decision\npipelines, and scaling human-like diagnostic reasoning to\nmassive datasets.\nF. From Engineering to IT and Clinical\nEEG Autoclean Vision-Language AI redefines what au-\ntomation can be. It is not just fast, it is intelligent, inter-\npretable, and trustworthy. By capturing how experts see and\nreason, ICVision bridges the gap between human expertise\nand machine scalability. This system offers a sustainable path\nforward for EEG reproducibility, a new model of clinical-\nhuman-AI collaboration, and a foundation for continuous\nlearning and knowledge retention in EEG science. As we\nmove toward a future of multi-site, multimodal, AI-assisted\nneurotechnologies, solutions like ICVision will be vital, not\njust for cleaning the data but for preserving the meaning within\nthe content.\nII. PROPOSED METHOD\nThe core objective of this work is to replicate and scale\nhuman-level EEG ICA component interpretation through\nvision-language modeling. Our proposed system, Autoclea-\nnEEG ICVision Vision-Language AI (ICVision), treats each\nICA component as a visual object and applies modern\ntransformer-based multimodal reasoning to classify it, explain\nthe decision, and support downstream artifact rejection. Unlike\nexisting ICA classifiers that operate solely on engineered\nfeatures [1], our method views the diagnostic plots directly and\nresponds as a trained expert would: visually and linguistically.\nThe holistic proposed system block diagram is shown in Fig.3.\nFig. 3. The Proposed System Block Diagram\nA. Data Acquisition and ICA Decomposition\nRaw EEG data used in this study were processed using\nindustry-standard pipelines. Datasets included clinical and\ncognitive recordings acquired with 128-channel systems using\nstandard 10-20 montages. Preprocessing included: Bandpass\nfiltering (1–80 Hz), Line noise removal using adaptive notch\nfilters, Channel rejection and interpolation via MNE-Python\nand EEGLAB. Independent Component Analysis (ICA) was\nperformed using ICA (EEGLAB) or FastICA (MNE), de-\npending on platform preference. Each EEG file yielded be-\ntween 20–40 ICA components per subject. Across 124 ICA\ndashboards, a total of 3,000+ components were extracted and\nvisualized for classification.\nB. Dashboard Generation\nFor each component, we generated a 4-panel composite\nimage that mimics the neurologist’s diagnostic dashboard, the\nTopographic Map which is a spatial projection of the IC\nweights across the scalp (µV), the Time Series showing the\ntemporal activation of the component over a 2.5 s segment, the\nPower Spectral Density (PSD) in dB-scaled power across 1–80\nHz, and the ERP Image itself that illustrates the component\namplitude across continuous data epochs, arranged vertically.\nEach dashboard was rendered at 512×512 resolution using\na standardized color scheme, grid spacing, and axis scaling.\nThese visualizations were saved as .png files and named ac-\ncording to the subject and component indices. This multimodal\nvisualization format was inspired by clinical EEG workflows\nand designed to match the cognitive strategy employed by\nhuman experts.\nC. Vision-Language Model Pipeline\nUnlike conventional feature-based classifiers, our pipeline\ndirectly analyzes the visual diagnostic space using a multi-\nmodal model. This enables the AI to interpret ICA dashboards\nsimilarly to human experts, by integrating spatial, spectral, and\ntemporal patterns, while maintaining structured, explainable\noutput critical for clinical and research interpretability.\n1) Model Selection: We used the OpenAI GPT-4 Vision\nAPI [4] as the core of our classification engine. This multi-\nmodal transformer model accepts image-text input pairs and\nreturns structured text output. It includes: Vision Encoder (VE)\nthat extracts image embeddings and spatial attention patterns,\nand a Language Decoder (LD) that generates descriptive and\nreasoned output text. The choice of GPT-4 Vision was driven\nby its state-of-the-art visual question-answering capabilities,\nreasoning capacity across spatial, temporal, and frequency\ndomains, as well as prompt alignment with scientific and\nclinical narratives.\n2) Prompt Engineering and Input Formatting: Each API\ncall included the component dashboard .png as the input\nimage, and A task-specific prompt, e.g.: “You are a neurologist\nevaluating this EEG ICA component. Determine whether the\ncomponent reflects brain activity, or one of the following\nartifacts: eye, muscle, heart, line noise, channel noise, or other\nartifact. Provide a classification, confidence score (0–1), and\na brief explanation for your decision.”\nThis contextual framing was key to guiding the model’s\nattention across the diagnostic panels. For the purpose of\nsimple illustration, an example of the description prompt as\ngiven to the AI agent is shown in Fig.4.\n"}, {"page": 4, "text": "Fig. 4.\nIllustration of the configuration example given to the AI thinking\nmachine.\nD. Classification Workflow and Output Schema\nThe combination of class prediction and descriptive reason-\ning enables transparent, auditable, and human-aligned classi-\nfication. This structured output can be seamlessly integrated\ninto preprocessing pipelines and supports quality assurance,\ndocumentation, and training across clinical and research set-\ntings. An example output is shown in Fig.5. The outputs are\nsaved into four catalogs as shown in Table.I. For each ICA\ncomponent, the system returns:\n• Class Label: One of seven predefined categories: brain,\neye, muscle, heart, line-noise, channel-noise, or other-\nartifact.\n• Confidence score: Scalar between 0 and 1, extracted from\nthe model response or approximated using soft prompts.\n• Descriptive Reasoning: Natural language paragraph (30-\n70 words) explaining the decision logic.\nFig. 5.\nExample of the JSON output enables transparent, auditable, and\nhuman-aligned classification.\nE. Auto-Exclusion and Post-Processing\nThe integration of confidence-based filtering ensures that\nthe system remains both autonomous and cautious, striking a\nbalance between automation and human-level judgment. High-\nconfidence artifact classifications are automatically excluded,\nwhile low-confidence or potentially neural components are\npreserved or flagged for expert review. This conservative\nfiltering strategy allows for safe deployment in both clinical\nand research contexts, where over-rejection or under-cleaning\ncan distort downstream cognitive, behavioral, or diagnostic\ninterpretations. A threshold-based filtering mechanism was im-\nplemented via components with confidence ≤0.80 for artifact\nlabels, which were auto-marked for rejection, as well as Brain-\nlabeled components, which were retained unless confidence\n> 0.40, in which case they were flagged for manual review.\nRejected components were zeroed from the ICA solution and\nprojected back into the cleaned EEG dataset. A consistency\nlog ensured reproducibility across re-processing sessions.\nTABLE I\nTHE OUTPUT CATALOG PARAMETERS LIST\nID\nParameter\nResource\n1\nResults.csv\nFull classification top level summary\n2\nCleanedRawSet.fif\nEEG data with rejected comp. removed\n3\nReportAllComp.pdf\nVisual of dashboard/decision mapping\n4\nSummary.txt\nCounts, costs, & rejection statistics\nF. Implementation Details\nICVision was engineered for scalable deployment and clin-\nical robustness. It supports both EEGLAB and MNE pipelines\nand enables real-time, batched classification via asynchronous\nAPI calls, scaling from individual analyses to hospital-grade\nworkflows. Its modular architecture allows for future inte-\ngration of improved vision-language models (e.g., domain-\ntuned biomedical backends), ensuring long-term adaptability\nand relevance, as summarized in Table.II.\nTABLE II\nAI AGENT IMPLEMENTATION SPECS\nID\nParameter\nResource\n1\nPlfrm.Compatibility\nFully compatible with MNE-Python 1.4+,\nEEGLAB 2023+, and supports both .set\nand .fif formats\n2\nBatch Size\n10 components per request\n3\nParallelization\nUp to 4 concurrent API calls (via Python\nasyncio).\n4\nAverage Cost\n∼$0.002 per component using gpt-4.1-\nmini; ∼50¢, for 128 components\n5\nLanguage Runtime\nPython 3.10, MATLAB R2023a for EEG\nI/O compatibility\nThese contributions establish ICVision as the first system\nto merge clinical reasoning with scalable AI for ICA clas-\nsification. By reframing the task as visual-linguistic rather\nthan numeric, it introduces a new paradigm for explainable,\nconsistent, and reproducible EEG cleaning across datasets,\nusers, and institutions.\nIII. RESULTS AND COMPARISON\nWe evaluated the performance of EEG Autoclean Vision-\nLanguage AI (ICVision) on a curated EEG dataset, bench-\nmarking it against both expert human annotations and the\nwidely used MNE-ICLabel classifier [9] [10]. Results were\nanalyzed using classification metrics, agreement rates, inter-\npretability scoring, and the system’s downstream impact on\nsignal quality.\nA. Component Distribution and Labeling\nInitially, the input data were labeled into six classes: 1)\nBrain, neural components with oscillatory, spatial structure.\n2) Eye, blink/saccade-related frontal artifacts. 3) Heart, cardiac\npulse wave components (∼1 Hz) .4) Muscle, high-frequency\nEMG contamination. 5) Channel-noise Hardware-specific, flat-\nlined, or spiky channels. 6) Other Noise, unclassified or\nambiguous non-brain signals the label distribution. The label\nclass distribution and the Confusion Matrix for the first 20\n"}, {"page": 5, "text": "components are shown in Fig.6, with an estimated time of\n30 seconds per component, reaching 100 hours for manual\nlabeling of the data.\nFig. 6. The label Distribution comparison and the Confusion matrix for the\nfirst 20 components\nAcross the entire dataset, ICVision assigned component\nlabels in six classes: brain, eye, heart, muscle, channel noise,\nand other noise. The most frequently detected class was muscle\n(35.5%), followed by brain (30.6%), eye (16.9%), channel\nnoise (16.1%), and heart (0.8%). Notably, “other noise” was\nnot directly labeled in this implementation, likely due to\nmerging with adjacent artifact classes through visual context\ninterpretation. This class distribution reflects the real-world\nnature of EEG data, where physiological artifacts like EMG\nand eye movements often dominate ICA results.\nB. Agreement with Expert Labels and Classifier Benchmark-\ning\nQuantitatively, ICVision achieved a Cohen’s κ score of\n0.677 compared to human expert labels, outperforming MNE-\nICLabel, which scored κ = 0.661 on the same dataset. While\nthe improvement is incremental, it is statistically and clinically\nmeaningful given the complexity of ICA classification tasks.\nFurthermore, exact agreement with expert labels was observed\nin 59% of components. Additional stratification of agreement\npatterns showed that:\n• 67.6% of components had unanimous agreement across\nICVision, ICLabel, and human labels.\n• 13.5% showed mixed agreement between the two meth-\nods.\n• 18.9% were labeled similarly by ICVision and ICLabel\nbut diverged from human labels.\nThus, these results suggest that ICVision is not only accurate\nbut also robust to methodological variance and better aligned\nwith expert interpretation, as illustrated in Fig.7.\nTo better understand how the methods aligned with expert\nopinion, we analyzed agreement patterns across the three\nlabel sources: ICVision, ICLabel, and Human. Results showed\nthat 67.6% of components were unanimously classified across\nall methods, reflecting strong consensus on clearly defined\ncomponents. Another 13.5% showed mixed agreement (two\nmethods matching), while the remaining 18.9% were labeled\nsimilarly by ICVision and ICLabel but disagreed with human\nreviewers. This indicates that while automated methods often\nFig. 7. Algorithm performance vs Human Ground Truth and the methods vs.\nHuman Ground Truth, All Components\nalign, their occasional divergence from human interpretation\nunderscores the value of explainable AI.\nMoreover, a roughly 11,000 images (10,967 to be exact)\nwere processed from previously collected experimental and\nclinical datasets. Each ICA component (totaling 3,168 com-\nponents) was independently labeled by an experienced EEG\nanalyst with over 7 years of ICA interpretation experience.\nDiscrepancies were resolved through consensus review. The\nresults comparison between the MNE-ICLabel and the pro-\nposed ICVision, both in comparison to human analysis, is\nshown in Fig.8.\nFig. 8.\nComparison between the common MNE-IClabels and ICVisionvs\nHuman Expert analysis\nC. Qualitative Reasoning and Interoperability\nA distinguishing feature of ICVision is its natural language\noutput, which explains classification decisions in clinician-\nstyle language. For instance: “This component shows lateral\ntemporal topography with prominent high-frequency bursts\n(20–40 Hz), irregular waveform structure, and dispersed ERP\nactivity, consistent with EMG contamination.” Expert review-\ners rated these explanations as excellent (85%) or acceptable\n(12%) in 97% of reviewed cases, confirming that the model’s\noutputs are interpretable, justifiable, and ready for integration\ninto clinical or teaching settings. This explainability, absent\nfrom most EEG classifiers, positions ICVision as an auditable\nAI agent, capable of bridging automation and human review.\nD. Signal Quality Preservation\nWe assessed how classification decisions influenced down-\nstream EEG quality. In one case, ICVision retained a frontal\n"}, {"page": 6, "text": "component labeled brain that ICLabel rejected as eye, resulting\nin the preservation of a clear alpha peak at 8.2Hz, a signal lost\nin the ICLabel-cleaned data. This illustrates a key advantage of\nvision-language reasoning: its ability to preserve meaningful\nneural content by interpreting component complexity beyond\nstatistical features. In cognitive and clinical contexts, such\ndifferences can have a direct impact on research outcomes.\nICVision’s deterministic behavior further ensures traceability\nand clinical-grade reproducibility, critical for multi-site studies\nand regulatory deployment.\nIV. DISCUSSION\nThis work introduces EEG Autoclean Vision-Language AI\n(ICVision) as a novel system that classifies ICA components\nby interpreting full EEG dashboards and generating human-\nreadable explanations. Unlike feature-based models, ICVision\nleverages visual-linguistic reasoning, achieving higher align-\nment with expert labels and providing transparency critical\nfor clinical workflows, training, and auditability. More than\na classifier, ICVision enhances signal quality by retaining\nborderline components with dominant neural features, thereby\npreserving spectral content (e.g., alpha peaks) that is often\nlost with conventional methods. This has a direct impact on\napplications such as BCI, biomarker detection, and clinical\nEEG analysis. Engineered for flexibility, ICVision supports\nPython, MATLAB, .set/.fif formats, and scalable batch in-\nference. Its modular architecture allows integration of future\nvision-language models or EEG-specialized tuning, supporting\nbroader use across pediatric, clinical, and real-time domains.\nUltimately, ICVision establishes a new class of explainable\nAI agents in neuroscience, systems that not only classify, but\nalso reason and communicate with traceability and clinical\nrelevance.\nV. CONCLUSION\nWe presented EEG Autoclean Vision-Language AI (ICVi-\nsion), a novel system that performs ICA component classifi-\ncation using full EEG dashboard visualizations and generates\nhuman-like reasoning for each decision. Unlike traditional\nfeature-based classifiers, ICVision integrates multimodal vi-\nsual interpretation with natural language output, enabling both\nhigh classification accuracy and expert-aligned transparency.\nValidation results demonstrated that ICVision outperforms\nMNE-ICLabel in agreement with expert labels, while also\npreserving critical neural signals in cases that are borderline.\nIts descriptive explanations bridge the gap between automation\nand clinical trust, making it well-suited for applications in neu-\nrodiagnostics, cognitive neuroscience, and BCI preprocessing.\nMost importantly, ICVision addresses a foundational challenge\nin EEG science: preserving interpretive expertise in a scalable,\nexplainable, and auditable form.\nAs vision-language models continue to evolve, systems\nlike ICVision will become essential in enabling transparent,\nintelligent, and reproducible neurotechnology pipelines, where\nAI not only classifies but also explains, collaborates, and\npreserves the human logic behind EEG interpretation.\nREFERENCES\n[1] L. Pion-Tonachini, K. Kreutz-Delgado, and S. Makeig, “ICLabel: An\nautomated electroencephalographic independent component classifier,\ndataset, and website,” Neuroimage, vol. 198, pp. 181–197, May 2019.\n[2] G. Soghoyan, A. Ledovsky, M. Nekrashevich, O. Martynova, I. Po-\nlikanova, G. Portnova, A. Rebreikina, O. Sysoeva, and M. Sharaev,\n“A toolbox and crowdsourcing platform for automatic labeling of\nindependent components in electroencephalography,” Frontiers in neu-\nroinformatics, vol. 15, p. 720229, 2021.\n[3] M. G. Asogbon, Y. Huai, O. W. Samuel, Z. Jing, Y. Ma, J. Liu, Y. Jiang,\nY. Fu, G. Li, and Y. Li, “Analysis of artifactual components rejection\nthreshold towards enhanced characterization of neural activity in post-\nstroke survivor,” in 2023 45th Annual International Conference of the\nIEEE Engineering in Medicine & Biology Society (EMBC), pp. 1–5,\nIEEE, 2023.\n[4] K. Carolan, L. Fennelly, and A. F. Smeaton, “A review of multi-modal\nlarge language and vision models,” arXiv preprint arXiv:2404.01322,\n2024.\n[5] A. Delorme and S. Makeig, “EEGLAB: an open source toolbox for\nanalysis of single-trial EEG dynamics including independent component\nanalysis,” J Neurosci Methods, vol. 134, pp. 9–21, Mar. 2004.\n[6] M. Fayaz, “The bibliometric analysis of eeglab software in the web\nof science indexed articles,” Neuroscience Informatics, vol. 4, no. 1,\np. 100154, 2024.\n[7] A. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier,\nC.\nBrodbeck,\nR.\nGoj,\nM.\nJas,\nT.\nBrooks,\nL.\nParkkonen,\nand\nM. H¨am¨al¨ainen, “MEG and EEG data analysis with MNE-Python,” Front\nNeurosci, vol. 7, p. 267, Dec. 2013.\n[8] A. P. Rockhill, E. Larson, B. Stedelin, A. Mantovani, A. M. Raslan,\nA. Gramfort, and N. C. Swann, “Intracranial electrode location and\nanalysis in mne-python,” Journal of open source software, vol. 7, no. 70,\np. 3897, 2022.\n[9] A. Li, J. Feitelberg, A. P. Saini, R. H¨ochenberger, and M. Scheltienne,\n“Mne-icalabel: Automatically annotating ica components with iclabel in\npython,” Journal of Open Source Software, vol. 7, no. 76, p. 4484, 2022.\n[10] L.-M. Zapata-Saldarriaga, A.-D. Vargas-Serna, J. Gil-Guti´errez, Y.-J.\nMantilla-Ramos, and J.-F. Ochoa-G´omez, “Evaluation of strategies based\non wavelet-ica and iclabel for artifact correction in eeg recordings,”\nRevista cient´ıfica, no. 46, pp. 61–76, 2023.\n"}]}