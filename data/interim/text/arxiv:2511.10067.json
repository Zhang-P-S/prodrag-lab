{"doc_id": "arxiv:2511.10067", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.10067.pdf", "meta": {"doc_id": "arxiv:2511.10067", "source": "arxiv", "arxiv_id": "2511.10067", "title": "Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning", "authors": ["Yuxuan Zhou", "Yubin Wang", "Bin Wang", "Chen Ning", "Xien Liu", "Ji Wu", "Jianye Hao"], "published": "2025-11-13T08:13:23Z", "updated": "2025-11-14T02:48:02Z", "summary": "Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at https://muser-llm.github.io.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.10067v2", "url_pdf": "https://arxiv.org/pdf/2511.10067.pdf", "meta_path": "data/raw/arxiv/meta/2511.10067.json", "sha256": "a244471794eb2c99d76460275b56fa4130cc56d11fa3295d6593f05b4b2ebacf", "status": "ok", "fetched_at": "2026-02-18T02:27:07.524527+00:00"}, "pages": [{"page": 1, "text": "Preprint. In Review.\nENHANCING\nTHE MEDICAL CONTEXT-AWARENESS\nABILITY\nOF\nLLMS\nVIA\nMULTIFACETED\nSELF-\nREFINEMENT LEARNING\nYuxuan Zhou1,2‚àó, Yubin Wang2, Bin Wang2, Chen Ning1, Xien Liu1, Ji Wu1,3,4, Jianye Hao2\n1Department of Electronic Engineering, Tsinghua University\n2Huawei Noah‚Äôs Ark Lab 3College of AI, Tsinghua University\n4Beijing National Research Center for Information Science and Technology\nABSTRACT\nLarge language models (LLMs) have shown great promise in the medical do-\nmain, achieving strong performance on several benchmarks.\nHowever, they\ncontinue to underperform in real-world medical scenarios, which often demand\nstronger context-awareness, i.e., the ability to recognize missing or critical de-\ntails (e.g., user identity, medical history, risk factors) and provide safe, help-\nful, and contextually appropriate responses. To address this issue, we propose\nMultifaceted Self-Refinement (MuSeR), a data-driven approach that enhances\nLLMs‚Äô context-awareness along three key facets (decision-making, communica-\ntion, and safety) through self-evaluation and refinement. Specifically, we first\ndesign a attribute-conditioned query generator that simulates diverse real-world\nuser contexts by varying attributes such as role, geographic region, intent, and\ndegree of information ambiguity. An LLM then responds to these queries, self-\nevaluates its answers along three key facets, and refines its responses to bet-\nter align with the requirements of each facet. Finally, the queries and refined\nresponses are used for supervised fine-tuning to reinforce the model‚Äôs context-\nawareness ability. Evaluation results on the latest HealthBench dataset demon-\nstrate that our method significantly improves LLM performance across multi-\nple aspects, with particularly notable gains in the context-awareness axis. Fur-\nthermore, by incorporating knowledge distillation with the proposed method,\nthe performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its\nteacher model, achieving a new SOTA across all open-source LLMs on Health-\nBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at\nhttps://muser-llm.github.io.\n1\nINTRODUCTION\nLarge language models (LLMs) have witnessed significant advancements in recent years (Achiam\net al., 2023; Anil et al., 2023; Dubey et al., 2024; Liu et al., 2024; Yang et al., 2025) and demonstrated\npromising capabilities in various domains, including the medical domain. Recent studies (Singhal\net al., 2023a;b; Nori et al., 2023a; Qiu et al., 2024; Liu et al., 2025) indicate that current LLMs (e.g.,\nGPT-4) encode substantial medical knowledge and achieves strong performance on several medical\nbenchmarks. Despite these advancements, LLMs still struggle to meet the demands of real-world\nmedical applications, limiting their practical utility in healthcare settings.\nOne of the decisive differences between medical benchmark questions and real-world scenarios lies\nin the requirement for stronger context-awareness, namely, the ability to recognize missing or critical\ndetails (e.g., medical history, user identity, risk factors) and to provide safe, helpful, and contextu-\nally appropriate responses. As illustrated in Figure 1a, existing medical benchmarks typically adopt\nquestion-answering task for evaluation, where questions contain sufficient information for answer-\ning, presented in the impersonal tone, and answering errors have limited consequences. In contrast,\nreal-world medical scenarios often omit key details for decision-making, involve diverse user roles\n‚àóThis work was done during the author‚Äôs internship at Huawei Noah‚Äôs Ark Lab.\n1\narXiv:2511.10067v2  [cs.AI]  14 Nov 2025\n"}, {"page": 2, "text": "Preprint. In Review.\nReal-World Medical Scenarios\nMedical QA Tasks\nInclude all necessary information\nImpersonal tone, context-neutral\nErrors have limited consequences.\nOften omit \nkey details\nA 55-year-old woman presents with \nsevere pruritus and fatigue. She denies \nany ‚Ä¶ Upon physical examination‚Ä¶ \nLaboratory findings are significant \nfor ‚Ä¶ A liver biopsy is performed‚Ä¶ \nWhich of the following is the most \nlikely diagnosis in this patient?\nA: Primary biliary cirrhosis\nB: Hemolytic anemia        \n‚Ä¶\nE: Crigler-Najjar syndrome type I\nDiverse\nuser identity\nRequiring \ncareful risk \nmanagement\nBad itching, really tired. What \nshould I do?\nI‚Äôve had really \nbad itching, \nand I‚Äôm very \ntired‚Ä¶\nA patient \npresents with \nsevere pruritus \nand fatigue‚Ä¶\n‚Ä¶\nBad itching and feel dizzy. \nMy belly started swelling.\nYou should go to see doctor \nimmediately since these symptoms \nmay indicate liver disfunction ‚Ä¶\nOurs: Multifaceted Self-Refinement Learning\na.\nb.\nDisease\nSynthesized Query\nAttribute-Conditioned Query Generator\nRole\n‚Ä¶\nInformation\nCompleteness\nMultifaceted \nSelf-Refinement\nInitial Response\nGenerate\nFacet 1\n(Decision-Making)\nFacet 2\n(Communication)\nFacet 3\n(Safety)\nMultifaceted Feedback\nEvaluate\nSelf-Refined Response\nSelf-Refine\nSFT-based Context-Awareness Learning\nFigure 1: (a) Comparison between medical exam questions and real-world medical scenarios. (b)\nThe proposed Multifaceted Self-Refinement learning framework (MuSeR) to enhance the medical\ncontext-awareness ability of LLMs through data synthesis and self-refinement.\n(e.g., patients, doctors), and require careful consideration of safety and ethical implications. While\ncurrent LLMs perform well on exam-style context, they often overlook the contextual factors in\nreal-world medical scenarios, leading to responses that may be inappropriate, unsafe, or unhelpful\nto the user‚Äôs specific situation.\nIn this paper, we aim to enhance the context-awareness of LLMs in the medical domain. A com-\nmon approach is to collect high-quality real-world medical conversations for supervised fine-tuning\n(SFT). However, this is often impractical due to high collection costs and ethical concerns. To ad-\ndress this, we explore a cost-effective and scalable alternative: enhancing context-awareness through\ndata synthesis. Specifically, we propose a novel Multifaceted Self-Refinement (MuSeR) frame-\nwork. MuSeR improves medical context-awareness by synthesizing simulated real-world medical\nqueries and generating context-aware responses by self-refining the answers of LLMs along three\nkey facets of context-awareness: decision-making, communication, and safety. As shown in Fig-\nure 1b, the proposed framework consists of three main components: (1) a attribute-conditioned\nquery generator that simulates diverse real-world user contexts by varying attributes such as role,\ngeographic region, intent, and degree of information ambiguity; (2) a multifaceted self-refinement\nmodule where an LLM responds to the generated queries, evaluates its answers along the three key\nfacets, and refines its responses to better align with the requirements of each facet; and (3) a su-\npervised fine-tuning stage where the generated queries and refined responses are used to reinforce\nthe model‚Äôs context-awareness ability. The entire process does not require any external medical cor-\npora or human annotations, making it a cost-effective and scalable solution for enhancing LLMs‚Äô\ncontext-awareness in the medical domain.\nTo evaluate the effectiveness of our proposed method, we apply the proposed method on different\nsizes of LLMs (Qwen3-32B, Qwen3-14B, OpenPangu-7B) and assess their performance on the\nlatest HealthBench dataset (Arora et al., 2025), which focuses on evaluating LLMs performance\nin real-world medical scenarios. The results demonstrate that our method significantly improves\nLLM performance on HealthBench, with particularly notable gains in the context-awareness axis.\nFurthermore, by incorporating knowledge distillation into the proposed framework using a strong\nteacher model (e.g., GPT-oss-120B), the performance of a smaller backbone LLM (e.g., Qwen3-\n32B) surpasses that of the teacher model by 6%, achieving a new state-of-the-art result among\nopen-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Our main contributions\nare summarized as follows:\n‚Ä¢ We propose a novel Multifaceted Self-Refinement (MuSeR) learning framework that en-\nhances LLMs‚Äô context-awareness across three key facets (decision-making, communica-\ntion, and safety) through self-evaluation and refinement, facilitating their application in\nreal-world medical scenarios.\n‚Ä¢ Extensive experiments on the HealthBench dataset demonstrate the effectiveness of our\nmethod in improving LLM performance, particularly in the context-awareness axis.\n2\n"}, {"page": 3, "text": "Preprint. In Review.\n‚Ä¢ By incorporating knowledge distillation into our framework, we achieve new state-of-the-\nart performance among open-source LLMs on the HealthBench dataset (63.8%) and the\nhard subset (43.1%) using only 100k generated queries.\n2\nRELATED WORK\nLLM Medical Evaluation\nMost of existing medical benchmarks for LLMs are in the question-\nanswering form, where the questions are sourced from medical exams (Vilares & G¬¥omez-Rodr¬¥ƒ±guez,\n2019; Jin et al., 2021; Pal et al., 2022; Cai et al., 2024; Wang et al., 2024; Qiu et al., 2024; Zhou\net al., 2024), literatures (Jin et al., 2019; Krithara et al., 2023), and healthcare consultations (Liu\net al., 2020; Abacha et al., 2021; Singhal et al., 2023a). These benchmarks primarily evaluate the\nLLMs‚Äô medical knowledge and reasoning abilities, and existing LLMs are reported to achieve strong\nperformance on these benchmarks (Singhal et al., 2023a;b; Nori et al., 2023b; Qiu et al., 2024). For\nexample, GPT-4 achieves over 90% accuracy on the MedQA-USMLE exam dataset, approaching\nthe performance level of human medical experts. Nevertheless, these benchmarks may not fully\ncapture the complexities of real-world medical scenarios, especially in terms of context-awareness.\nRecently, OpenAI proposed HealthBench (Arora et al., 2025), a new benchmark includes 5,000\nrealistic health conversations annotated by 262 physicians across 60 countries, evaluating LLMs‚Äô\nperformance as medical assistants in real-world scenarios. In this work, we primarily evaluate the\neffectiveness of our method on the HealthBench dataset.\nMedical LLM Training\nExisting works on training medical LLMs mainly focus on two aspects:\n(1) continual pre-training on medical corpora to inject domain-specific knowledge into LLMs (Chen\net al., 2023; Qiu et al., 2024; Zhang et al., 2024); (2) post training on downstream tasks to enhance\nthe model‚Äôs reasoning and decision-making capabilities (Singhal et al., 2023a;b; Toma et al., 2023;\nChristophe et al., 2024b;a; Chen et al., 2025b). For the training data, most works utilize existing\nmedical corpora, such as PubMed articles (Roberts, 2001), clinical notes (Johnson et al., 2020; Zhao\net al., 2023), and medical QA datasets (Jin et al., 2021; Pal et al., 2022). Due to the data availability\nand privacy concerns, recent works (Bai et al., 2024; Das et al., 2024; Corbeil et al., 2025) start\nto leverage LLM-generated synthetic data for training medical LLMs and show promising results.\nWhile these methods effectively improve LLMs‚Äô medical knowledge mastery and reasoning skills,\nour work mainly focuses on enhancing the context-awareness ability of LLMs, which is also a crucial\naspect for LLMs‚Äô practical application in the medical domain.\nKnowledge Distillation\nKnowledge distillation (KD) (Hinton et al., 2015; Sanh et al., 2019; Jiao\net al., 2019) transfers knowledge from a large teacher model to a smaller student model, enabling\nefficiency while maintaining performance. In the LLM era, knowledge distillation is typically per-\nformed by generating distillation data using a strong teacher LLM for fine-tuning the student LLM\nin both general domain (Abdin et al., 2024; Yang et al., 2025; Guo et al., 2025) and medical do-\nmain (Zhang et al., 2023; Chen et al.). In this work, we explore the value of our framework in knowl-\nedge distillation and demonstrate that our method is effective in generating high-quality queries for\nknowledge distillation.\n3\nMETHODOLOGY\nIn this section, we present our proposed Multifaceted Self-Refinement (MuSeR) learning framework\nto enhance the context-awareness ability of LLMs in the medical domain. An overview of the\nproposed framework is illustrated in Figure 2. In the following sections, we first formulate the\nproblem and then detail the design of each component in the framework.\n3.1\nPROBLEM FORMULATION\nOur goal is to improve the medical context-awareness of an LLM M, such that it provides safe, help-\nful, and contextually appropriate responses to real-world medical queries. Let q ‚àºP ‚àó(¬∑) denote a\nreal-world medical query where P ‚àó(¬∑) is the distribution of real-world medical queries, PM(¬∑|q)\ndenote the model‚Äôs conditional response distribution, and P ‚àó(¬∑|q) denote the ideal conditional re-\nsponse distribution, where a response r ‚àºP ‚àó(¬∑|q) attends to the contextual information of q across\n3\n"}, {"page": 4, "text": "Preprint. In Review.\nAttribute Aspects\nAttribute-Conditioned Query Generator\nDisease\nRole\n‚Ä¶\nInformation\nCompleteness\nsample\nSampled Attributes\nHepatitis B\nPatient ‚Ä¶\nIncomplete\ngenerate\nAssume that you are a user ... Please generate a query by \nfollowing the constraints below: (1) User role: you are a \npatient with limited medical knowledge; (2) Disease: ‚Ä¶ \n(n) Completeness: The query should omit key details‚Ä¶\nDoctor, I‚Äôve been feeling very tired these past few days, \nI‚Äôve lost my appetite. What should I do?\nPrompt For Synthesis\ngenerate\nMultifaceted Self-Refinement Module\nSynthetic Query\nInitial Response\n<think>The user says ‚Ä¶ Now, answer. </think>The etiology of your condition may be multifactorial, ranging from \nbenign causes such as psychological stress, sleep deprivation, ‚Ä¶ or hepatic/renal impairment. You should ‚Ä¶\nFacet 1: Decision-making (fùüè)\nFacet 2: Communication (fùüê)\nFacet 3: Safety (fùüë)\nQuery: ‚Ä¶ Answer: ‚Ä¶\nYour task is to evaluate whether \nthe answer actively seeks the \nmost informative context‚Ä¶\nQuery: ‚Ä¶ Answer: ‚Ä¶\nYour task is to evaluate whether the \nanswer effectively recognizes the \nuser‚Äôs identity‚Ä¶\nQuery: ‚Ä¶ Answer: ‚Ä¶\nYour task is to evaluate whether the \nanswer effectively recognizes \nsafety awareness‚Ä¶\nWe should ask when did the fatigue \nstart and is it constant or ‚Ä¶\nWe should avoid using professional \nterms since the user is a patient‚Ä¶\nWe should mention the potential \nrisk factors such as anemia ‚Ä¶\nRefined Response\n<think>The user says ‚Ä¶ We should ask ‚Ä¶ We should use simple language‚Ä¶ We should mention the risk factors‚Ä¶ \nNow, answer. </think>First, I need to ask a few questions: When did the fatigue and loss of appetite start? ‚Ä¶Feeling \nvery tired along with a loss of appetite can come from many different causes, like stress, poor sleep, ‚Ä¶or even liver \nor kidney conditions‚Ä¶Potential risk factors to be aware of include anemia, chronic infections‚Ä¶\ngenerate\nFigure 2: An overview of the proposed Multifaceted Self-Refinement (MuSeR) learning framework,\nwith the SFT-based context-awareness enhancement stage omitted for simplicity.\na set of facets f1, f2, ¬∑ ¬∑ ¬∑ , fN. Conceptually, our goal can be expressed as reducing the divergence\nbetween these conditional distributions across queries:\nM‚àó= arg min\nM Eq‚àºP ‚àó(¬∑) [KL (P ‚àó(¬∑|q)||PM(¬∑|q))] .\n(1)\nwhere KL(¬∑||¬∑) denotes the KL divergence. However, the real-world query distribution P ‚àó(¬∑) and\nresponse distribution P ‚àó(¬∑|q) are typically inaccessible in practical scenarios. To address this, we\naim to (1) construct a query generator G that induces a distribution PG(¬∑) to approximate P ‚àó(¬∑):\nG ‚âàarg min\nG‚Ä≤ KL (P ‚àó(¬∑)||PG‚Ä≤(¬∑)) ,\n(2)\n(2) develop a response generator R that induces a distribution PR(¬∑|q) to approximate P ‚àó(¬∑|q) for\nany q ‚àºPG:\nR ‚âàarg min\nR‚Ä≤ Eq‚àºPG(¬∑) [KL (P ‚àó(¬∑|q)||PR‚Ä≤(¬∑|q))] ,\n(3)\n(3) optimize the model M such that its response distribution PM(¬∑|q) is aligned with PR(¬∑|q):\nM‚àó‚âàarg min\nM‚Ä≤ Eq‚àºPG(¬∑) [KL (PR(¬∑|q)||PM‚Ä≤(¬∑|q))] .\n(4)\nNote that the formulations above represent our design goals rather than explicit optimization objec-\ntives. In the following sections, we describe the proposed learning framework in detail, including\nthe facets of context-awareness it incorporates, the design of the query generator G and response\ngenerator R, and the training strategy for optimizing the model M.\n3.2\nMULTIFACETED SELF-REFINEMENT LEARNING FRAMEWORK\nFacets of Context-Awareness (f)\nWe primarily consider three key facets of context-awareness\nthat are crucial for providing safe, helpful, and appropriate responses in the medical domain:\n‚Ä¢ Decision-Making Awareness (f1): This facet focuses on identifying critical information (e.g.,\nmedical history, medication, examination results) essential for accurate medical decision-making,\nas well as actively seeking missing details from users when necessary. Such awareness is critical\nfor ensuring the accuracy and practical utility of medical advice.\n‚Ä¢ Communication Awareness (f2): This facet involves recognizing the user‚Äôs identity (e.g., pa-\ntient, doctor) and response preferences, and tailoring both terminology (e.g., layman vs. profes-\nsional) and level of detail (e.g., brief vs. comprehensive) accordingly. This facet is essential for\nproviding responses that match the user‚Äôs knowledge background and expectations.\n‚Ä¢ Safety Awareness (f3): This facet requires the model to recognize potential risk factors (e.g.,\nsymptom severity, underlying conditions) and ethical considerations (e.g., the use of unproven\ndrugs) in its responses. Such awareness is vital for ensuring both the safety and ethical integrity\nof the medical advice provided.\n4\n"}, {"page": 5, "text": "Preprint. In Review.\na. Continual Generation Results in Poor Alignment\nLLM\nThe etiology of your \ncondition may be \nmultifactorial, ranging \nfrom psychological stress, \nsleep deprivation, ‚Ä¶ or \nhepatic/renal impairment\nLLM\nUser: Doctor, I‚Äôve been \nfeeling very tired ‚Ä¶\nAssistant: <think>The user \nsays ‚Ä¶ We should use \nsimple language‚Ä¶\nNow, answer.</think>\nQuery: Doctor, I‚Äôve been \nfeeling very tired ‚Ä¶\nYour answer: ‚Ä¶\nSupplement rationales: ... use \nsimple language‚Ä¶\nGenerate a revised answer‚Ä¶\nFeeling very tired along \nwith a loss of appetite can \ncome from many different \ncauses, like stress, poor \nsleep, ‚Ä¶or even liver or \nkidney conditions‚Ä¶\nb. Directly Refining the Answer Results in Better Alignment\nFigure 3: Comparison between two strategies for\nanswer generation: (1) continual generation con-\nditioned on the refined reasoning, and (2) answer\nrefinement guided by multifaceted rationales.\nQueries\nMultifaceted Self-Refinement learning\nInitial Response\nTeacher \nModel\nTeacher Response\nStudent\nModel \nStudent\nModel\nKnowledge\nDistillation\nGenerate\nReplace\nStudent\nModel \nReplace\nMultifaceted\nSelf-Refinement\nGenerate\nQuery-Guide Knowledge Distillation\nDistilled\nModel \nFigure 4:\nQuery-Guided Knowledge Distil-\nlation integrated with the Multifaceted Self-\nRefinement (MuSeR) learning framework for en-\nhancing medical context-awareness.\nAttribute-Conditioned Query Generation (G)\nFor the query generator G(¬∑), to simulate the\ncomplexity of real-world query distribution P ‚àó(¬∑), we assume that the real-world query is con-\ntrolled by a set of attributes a = {a1, a2, ¬∑ ¬∑ ¬∑ , aN} (e.g., user role, intent), such that Preal(q) =\nP(q|a)P(a). Built on that, the proposed attribute-conditioned query generator first samples a set of\nattributes a from a prior distribution PAttr(¬∑), and then generates a query q ‚àºG(¬∑|a) conditioned\non the sampled attributes.\nIn our framework, we consider a total of seven key attributes for query generation: (1) user identity\n(patient, caregiver, or doctor); (2) geographic region (country, urban/rural area); (3) the specific\ndisease or injury being inquired about; (4) user intent (seeking diagnosis, treatment advice, report\ninterpretation, etc.); (5) vagueness of the intent (clear, vague); (6) completeness of the provided\ndetails (complete, incomplete); (7) language style (formal, informal). These attributes are chosen to\ncapture the diversity and complexity of real-world medical queries. For each attribute, we define a\nprior distribution over its possible values and sample an attribute combination a for query generation.\nFinally, a generator LLM Mq is prompted to produce a query q based on the sampled attributes a.\nMore details on the prompt design and attribute sampling can be found in the Appendix A.\nMultifaceted Self-Refinement Module (R)\nFor the response generator R(¬∑|q), given that the\nideal response distribution P ‚àó(¬∑|q) is typically unknown, we approximate it via a multifaceted self-\nrefinement process. Specifically, we assume that an ideal response should attend to contextual in-\nformation across different facets f = {f1, f2, ¬∑ ¬∑ ¬∑ , fM}. For each generated query q, the LLM M\nfirst generates an initial response (t0, r0) = fGen(M, q), where t0 is the reasoning part and r0 is the\nanswer part. Subsequently, the LLM M self-evaluates the answer along each facet and generates\na supplementary rationale to explain how the answer can be improved to better align with the re-\nquirements of the facet: si = fEval(M, q, r0; fi). For example, for the decision-making awareness\nfacet, the model may identify missing critical information in the query and generate a rationale such\nas ‚ÄúWe should ask about the patient‚Äôs current medications to make an accurate diagnosis.‚Äù. The\nrefined reasoning process t‚Ä≤ is derived by concatenating the multifaceted rationales {si}M\ni=1 after the\ninitial reasoning t0 with connectives (e.g., ‚ÄúFirst‚Äù, ‚ÄúNext‚Äù) to ensure logical coherence.\nTo generate the refined answer r‚Ä≤, a straightforward approach is to continually generate it condi-\ntioned on the query q and the refined reasoning t‚Ä≤ using the LLM M: r‚Ä≤ = fCont(M, q, t‚Ä≤). However,\nwe observe that the LLM often overlooks the supplementary rationales when generating the refined\nanswer, leading to less improvement over the initial answer (see Figure 3). Therefore, we consider\nprompting the LLM to directly refine the initial answer based on the query and the generated ratio-\nnales: r‚Ä≤ = fRefine(M, q, r0, {si}M\ni=1). We find that this approach yields answers that better align\nwith the rationales. More details of the prompt design for each step are provided the Appendix B.\n3.3\nTRAINING STRATEGY\nFor model optimization, a straightforward approach is to use the generated query-reasoning-answer\ntriplets {(q, t‚Ä≤, r‚Ä≤)} for supervised fine-tuning (SFT) of the model M. Although this approach proves\n5\n"}, {"page": 6, "text": "Preprint. In Review.\nDoctor, I‚Äôve been \nfeeling, ‚Ä¶\nIt seems that you \nare suffering ‚Ä¶\nShould I take any \nmedicine?\nEvaluation Input\nResponse\nWhich specific medication to \nprescribe depends on your \nparticular condition, ‚Ä¶ To \nprovide more precise guidance, \ncould you please answer the \nfollowing questions‚Ä¶ Finally, if \nyou are experiencing ‚Ä¶, it is \nessential that you seek immediate \nmedical attention without delay‚Ä¶\nCandidate Response\nRubric Criteria & Score\n‚Ä¶\n‚Ä¶\nAsking the patient \nabout the past \nmedical history‚Ä¶\n+ 9\n- 5\n+ 2\nFails to use simple \nterms‚Ä¶\nMentioned the red \nflags of ‚Ä¶\nCriteria\nPoint\nMet\ntrue\nfalse\ntrue\nGrade\nFigure 5: An example of the evaluation process of HealthBench(Arora et al., 2025).\neffective in enhancing the context-awareness of M, the model may still lack the essential medical\nknowledge and reasoning skills required to support context-aware responses. To address this limita-\ntion, we further incorporate a query-guided knowledge distillation stage. As illustrated in Figure 4,\nthis stage is performed prior to the SFT stage. A strong teacher LLM Mt first generates high-quality\nresponses for the synthesized queries, and the student model M is fine-tuned to align its outputs with\nthose of the teacher before proceeding to the multifaceted self-refinement stage. We find that this\nstage not only enhances the medical knowledge and reasoning skills of the student model but also\nimproves the effectiveness of the proposed self-refinement process.\n4\nEXPERIMENT SETUP\nEvaluation Benchmark\nIn this work, we primarily evaluate the effectiveness of our proposed\nmethod on HealthBench (Arora et al., 2025), a new medical benchmark constructed by OpenAI\nthat includes 5,000 realistic health conversations annotated by 262 physicians across 60 countries,\nevaluating LLMs‚Äô performance as medical assistants in real-world scenarios. As illustrated in Fig-\nure 5, a sample from HealthBench consists of a single/multi-turn conversation between a user and an\nAI assistant, where the evaluated model is required to generate a response based on the conversation\nhistory. For scoring, HealthBench employs a rubric-based evaluation method, where each response\nis automatically graded by GPT-4.1 based on a set of physician-written criteria. The conversations in\nHealthBench are categorized into seven themes (e.g., emergency, global health), where each criteria\nevaluates the response from one of five axes: accuracy, completeness, context awareness, commu-\nnication quality, and instruction following. Such a comprehensive evaluation framework enables a\nholistic assessment of LLMs‚Äô performance in real-world medical scenarios.\nBackbone LLMs\nTo demonstrate the effectiveness and generality of our proposed method, we\nimplement the proposed method on a total of three LLMs from two families with parameters ranging\nfrom 7B to 32B: (1) Qwen3-14B/32B (Yang et al., 2025); (2) OpenPangu-7B (Chen et al., 2025a).\nBaseline Models\nWe compare our method with several baseline models ranging from 7B to 671B\nparameters, including general LLMs such as GPT-5, GPT-4.1, GPT-oss-120b/20b (OpenAI, 2025),\no3, Gemini 2.5-Pro (Comanici et al., 2025), Claude 4 Sonnet thinking, Qwen3-14B/32B/235B-\nA22B (Yang et al., 2025), OpenPangu-7B (Chen et al., 2025a), and medical LLMs such as II-\nMedical-8B (Internet, 2025) and Baichuan-M2-32B (Dou et al., 2025).\nImplementation Details of MuSeR\nFor the query generator, we utilize DeepSeek-V3 (Liu et al.,\n2024) as the generator LLM Mq to generate a total of 100k queries based on the proposed attribute-\nconditioned generator. For the response generator, we implement the multifaceted self-refinement\nmodule using the backbone LLM (Qwen3-14B/32B or OpenPangu-7B). For the knowledge distilla-\ntion, we use GPT-oss-120B as the teacher LLM Mt, as it presents strong performance in the med-\nical domain. We use heuristic rules to filter out low-quality query-response pairs generated by the\nteacher model and the multifaceted self-refinement module. More implementation details (learning\nrate, epochs, batch size, data filtering) are provided in the Appendix D.\n5\nRESULTS\n6\n"}, {"page": 7, "text": "Preprint. In Review.\n0\n20\n40\n60\nScore (%)\nQwen3-32B\nQwen3-14B\nOpenPangu-7B\nGPT-oss-120B*\nGPT-oss-20B*\nBaichuan-M2-32B*\nII-Medical-8B*\nQwen3-235B-A22B\nDeepSeek-R1\nClaude 4 Sonnet\nthinking*\nGemini 2.5 Pro*\nGPT-5-thinking*\no3*\nGPT-4.1*\n46.1\n43.9\n29.8\n57.6\n42.5\n60.1\n46.9\n48.7\n53.9\n35.5\n52.0\n67.2\n59.8\n47.9\n63.8\n61.8\n55.5\nHealthBench\n0\n10\n20\n30\n40\n50\nScore (%)\nQwen3-32B\nQwen3-14B\nOpenPangu-7B\nGPT-oss-120B*\nGPT-oss-20B*\nBaichuan-M2-32B*\nII-Medical-8B*\nQwen3-235B-A22B\nDeepSeek-R1\nClaude 4 Sonnet\nthinking*\nGemini 2.5 Pro*\nGPT-5-thinking*\no3*\nGPT-4.1*\n12.0\n11.2\n0.0\n30.0\n10.8\n34.7\n16.7\n17.0\n22.6\n4.5\n18.5\n46.2\n31.6\n16.0\n43.1\n40.9\n31.5\nHealthBench-Hard\nBaseline Models\n+MuSeR(Ours)\nFigure 6: Overall performance comparison of different\nLLMs on HealthBench and its hard subset. Blue bars\ndenote the performance improvements brought by the\nproposed method. Results marked with * are taken from\n(Arora et al., 2025) or the corresponding model card,\nwhile others are evaluated by us.\no3\nDS-R1\nBaichuan-M2-32B Qwen3-32B\nOurs\n30\n40\n50\n60\n70\n80\nScore (%)\nHealthBench Scores by Axe\nAccuracy\nCommunication\nQuality\nCompleteness\nContext\nAwareness\nInstruction\nFollowing\no3\nDS-R1\nBaichuan-M2-32BQwen3-32B\nOurs\n30\n40\n50\n60\n70\n80\nScore (%)\nHealthBench Scores by Theme\nCommunication\nComplex Responses\nContext Seeking\nEmergency Referrals\nGlobal Health\nHealth Data Tasks\nHedging\nFigure 7: Detailed performance compari-\nson of different LLMs across the axes and\nthemes of HealthBench.\n‚ÄúOurs‚Äù denotes\nthe Qwen3-32B+MuSeR model. GPT-5 is\nnot included since the detailed scores are\nnot available in its system card.\nOverall Performance\nThe overall performance of the proposed method on HealthBench is sum-\nmarized in Figure 6. Across all backbone LLMs, the proposed method (MuSeR) consistently and\nsignificantly improves the performance of the backbone LLMs on HealthBench (+17.7%, +17.9%,\n+25.7% for Qwen3-32B, Qwen3-14B, and OpenPangu-7B, respectively), indicating the effective-\nness and generality of the proposed method across different LLM families and sizes. Notably, the\nperformance of Qwen3-32B and Qwen3-14B with the proposed method (63.8%, 61.8%) surpasses\nthat of the teacher model GPT-oss-120B (57.6%) by a large margin (+6.2%, 4.2%), achieving new\nSOTA results among open-source LLMs on HealthBench.\nFurthermore, on the hard subset of HealthBench, which consists of 1,000 samples that are partic-\nularly challenging for existing LLMs, the proposed method also yields substantial improvements\n(+29.8%, +29.7%, +31.5% for Qwen3-32B, Qwen3-14B, and OpenPangu-7B, respectively), with\nQwen3-14B+MuSeR and Qwen3-32B+MuSeR being the only two open-source LLM to surpass\n40% accuracy (40.9%, 43.1%) on this subset, largely outperforming the teacher model GPT-oss-\n120B (30.0%) as well as the previous open-source SOTA Baichuan-M2-32B (34.7%). However,\nthere still remains a gap between the proposed method and the top-1 model GPT-5-thinking (3.4%\non the full set and 3.1% on the hard set) on HealthBench, which may be attributed to the limited\nmedical knowledge of the backbone LLMs.\nEffectiveness on Context Awareness\nTo further analyze the effectiveness of the proposed method,\nwe select three top-performing models (o3, DeepSeek-R1, Baichuan-M2-32B), Qwen3-32B, and\nQwen3-32B+MuSeR for a detailed comparison across the axes and themes of HealthBench (GPT-5\nis not included due to the unavailability of detailed scores). As illustrated in Figure 7, the results\ndemonstrate that MuSeR achieves significant performance improvement on 4 out of 5 axes compared\nto the backbone model, especially on the context-awareness axis (+19.4%), which is the main focus\nof our proposed method. Note that the performance drop on the communication quality axis may\nbe attributed to the trade-off between the completeness and conciseness of the responses, where the\nproposed method tends to generate more comprehensive responses that may be less concise and thus\nreceive lower scores on this axis.\nRegarding the themes, Qwen3-32B+MuSeR significantly outperforms Qwen3-32B on all themes\nand achieves the best performance on 6 out of 7 themes among all compared models, demonstrat-\ning the effectiveness of the proposed method across diverse medical scenarios. Notably, Qwen3-\n32B+MuSeR achieves particularly large improvements compared to the previous SOTA Baichuan-\nM2-32B on the context seeking (+7.6%), global health (+5.0%), and hedging (responding under\nuncertainty) (+4.0%) themes, which require strong context-awareness ability to seek missing infor-\nmation, consider the user‚Äôs background (availability of medical resources in the specific region), and\n7\n"}, {"page": 8, "text": "Preprint. In Review.\nTable 1: Ablation study on the effectiveness of each training stage in the proposed MuSeR frame-\nwork. ‚ÄúMultifacetedSR‚Äù denotes the multifaceted self-refinement learning stage, and ‚ÄúQueryKD‚Äù\ndenotes the query-guided knowledge distillation stage using GPT-oss-120B as the teacher.\nMethod\nQwen3-32B\nQwen3-14B\nOpenPangu-7B\nFull\nHard\nFull\nHard\nFull\nHard\nBase Model\n46.1\n12.0\n43.9\n11.2\n29.8\n0.0\n+QueryKD\n56.6\n31.5\n55.9\n30.5\n53.0\n26.4\n+QueryKD+MultifacetedSR (Ours)\n63.8\n43.1\n61.8\n40.9\n55.5\n31.5\nTable 2:\nAblation study on the effectiveness\nof each refinement facet in the proposed multi-\nfaceted self-refinement module.\nMethod\nHealthBench\nFull\nHard\nMuSeR(Ours)\n63.8\n43.1\nw/o Decision Making\n61.1\n36.7\nw/o Communication\n62.0\n41.9\nw/o Safety\n63.4\n43.0\nTable 3: Comparison of two answer generation\nstrategies in MuSeR. ContGen: continual gener-\nation; DirectRef: direct refinement.\nMethod\nHealthBench\nFull\nHard\nQwen3-32B\n46.1\n12.0\n+MuSeR(ContGen)\n60.9\n36.8\n+MuSeR(DirectRef)\n63.8\n43.1\nprovide cautious advice under uncertainty, respectively. These results further validate the effective-\nness of the proposed method in enhancing the medical context-awareness ability of LLMs.\nEffectiveness of Training Stages in MuSeR\nTo investigate the effectiveness of each training stage\n(query-guided knowledge distillation and multifaceted self-refinement) in the proposed MuSeR\nframework, we further conduct an ablation study on all the three backbone LLMs, with the results\nsummarized in Table 1. Experimental results demonstrate that both training stages contribute signif-\nicantly to the overall performance improvement of the backbone LLMs on the HealthBench dataset\nand its hard subset. Specifically, the query-guided knowledge distillation stage brings substantial\nperformance gains (+10.5%, +12.0%, +23.2% for Qwen3-32B, Qwen3-14B, and OpenPangu-7B,\nrespectively), indicating the effectiveness of the synthetic queries in transferring medical knowl-\nedge and reasoning skills from the teacher model to the student model. Furthermore, the multi-\nfaceted self-refinement stage further enhances the performance of the student model (+7.2%, +5.9%,\n+2.5% for Qwen3-32B, Qwen3-14B, and OpenPangu-7B, respectively), especially on the hard sub-\nset (+11.6%, +10.4%, +5.1% for Qwen3-32B, Qwen3-14B, and OpenPangu-7B, respectively), val-\nidating the effectiveness of the proposed multifaceted self-refinement learning framework in en-\nhancing the context-awareness ability of LLMs in the medical domain. It is worth noting that the\neffectiveness of the multifaceted self-refinement stage is affected by the parameter sizes of the\nbackbone LLMs, where larger models tend to benefit more from this stage. This may be at-\ntributed to the stronger generation and reasoning capabilities of larger LLMs, which enable them to\nbetter utilize the multifaceted rationales for refining their responses.\nEffectiveness of Different Refinement Facets\nTo investigate the effectiveness of each refinement\nfacet in the proposed multifaceted self-refinement module, we conduct an ablation study by re-\nmoving one facet at a time and list the results in Table 2. Experimental results demonstrate that\nremoving any of the three facets leads to a performance drop compared to the proposed method,\nindicating that all facets contribute to the overall performance improvement. Notably, removing the\ndecision-making awareness facet results in the most significant performance drop (2.7%), highlight-\ning the critical role of this facet in enhancing the context-awareness ability of LLMs in the medical\ndomain. This may be attributed to the fact that decision-making awareness involves identifying and\nseeking critical information necessary for accurate medical decision-making, which is fundamental\nto providing safe and effective medical advice.\nComparison of Answer Generation Strategies\nWe further compare the two answer generation\nstrategies mentioned in Section 3.2 for the multifaceted self-refinement module: (1) continual gen-\neration conditioned on the refined reasoning (ContGen); (2) direct refinement based on the initial\n8\n"}, {"page": 9, "text": "Preprint. In Review.\nUser: bexsero brand x 1/9 brand x 2/10 tetanus brand y 3/11 mild rash child is 13 next shot\nResponse of OpenAI-o3\nIt looks as though you are trying to work out when \nthe next vaccine is due... Below is a guide ‚Ä¶\nRash after the tetanus-containing shot:\n‚Ä¢ A localised or mild generalised rash without other \nserious features is not a contraindication to future \nimmunisations.  Let the vaccinator know so they \ncan observe the next dose for 15‚Äì30 minutes.\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nBottom line / what to do now\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n...\nResponse of Qwen3-32B+MuSeR (Ours)\nQuick answer for a 13-year-old who has had only the \nfirst two doses of Bexsero ‚Ä¶\nWhat I still need to know to fine-tune the plan:\n‚Ä¶\nWhy it matters\nQuestion\n‚Ä¶\n‚Ä¶\nTo confirm it was a benign \nlocal reaction and not a sign \nof hypersensitivity. \nHow long did the rash \nlast after the tetanus \nshot, and was it treated?\n‚Ä¶\n‚Ä¶\nEvaluator‚Äôs Response for OpenAI-o3\nThe assistant notes that... However, the assistant \ndoes not explicitly ask for clarification about the \nrash or whether it was related to a \nvaccine‚Ä¶Therefore, the criteria was not met\nEvaluator‚Äôs Response for Qwen3-32B+MuSeR\nThe assistant does ask for clarification about the \nrash and whether it was related to a vaccine and \nalso states in the practical steps to document the \nmild rash‚Ä¶ Therefore, the criteria was met.\nFigure 8: A case study comparing the responses generated by o3 and Qwen3-32B+MuSeR (Ours).\nanswer and the generated rationales (DirectRef). Experimental results in Table 3 demonstrate that\nthe direct refinement strategy achieves consistently better performance (+2.9%, +6.3% on the full\nset and hard set, respectively), suggesting that such strategy generates answers that better align with\nthe multifaceted rationales and thus better attend to the contextual information of the queries.\nCase Study\nFinally, we provide a case study to qualitatively compare the responses generated\nby o3 and our proposed method (Qwen3-32B+MuSeR) in Figure 8. We observe that the response\ngenerated by o3 assume that the rash is caused by the vaccination, which may lead to unsafe advice.\nIn contrast, the response generated by our proposed method actively asks for the duration of the\nrash with proper reason (‚ÄúWhy it matters‚Äù), resulting in a more context-aware and safer response.\nThis case study further validates the effectiveness of the proposed method in enhancing the context-\nawareness ability of LLMs in the medical domain.\n6\nCONCLUSION\nCurrent LLMs have shown promising performance on medical benchmarks but still struggle to meet\nthe demands of real-world medical applications, which often require stronger context-awareness.\nIn this paper, we propose a Multifaceted Self-Refinement (MuSeR) learning framework to enhance\nthe context-awareness ability of LLMs in the medical domain through self-evaluation and refine-\nment along three key facets: decision-making, communication, and safety. The experimental results\non the latest HealthBench dataset demonstrate the effectiveness of our method in improving the\nperformance of backbone LLMs with different sizes, with particularly notable gains in the context-\nawareness axis. Furthermore, the proposed method can be effectively integrated with knowledge\ndistillation to further enhance the performance of smaller backbone LLMs, achieving new state-of-\nthe-art results among open-source LLMs on HealthBench with only 100k synthetic queries. We\nhope that our work can facilitate the practical application of LLMs in real-world medical scenarios\nand inspire future research on aligning LLMs with human needs in the medical domain.\nLimitations. In this work, we primarily focus on enhancing the context-awareness ability of LLMs\nin the medical domain, while such ability is also crucial in other domains (e.g., legal, financial). We\nleave the exploration of the proposed method in other domains to future work. Furthermore, while\nthe proposed method significantly improves the context-awareness ability of LLMs, incorporating\n9\n"}, {"page": 10, "text": "Preprint. In Review.\nmore medical knowledge into the backbone LLMs may further enhance the effectiveness of the\nproposed method and is worth exploring in future work.\nETHICS STATEMENT\nAll the data used in this work are either publicly available benchmarks or generated by LLMs. The\nproposed method does not involve any human subjects or sensitive data. Although the LLMs we\ntrained using the proposed method demonstrate improved context-awareness in the medical domain,\nthey have not been validated for real-world clinical applications and should be used for research pur-\nposes only. We recommend that users exercise caution and consult qualified medical professionals\nwhen applying these models in practice.\nREPRODICIBILITY STATEMENT\nThe proposed method is described in detail in Section 3. The implementation details for each module\nof the proposed method and hyperparameters, are provided in the Appendix A, B, C, and D. We\nalso plan to release the code and the generated dataset (including 100k synthetic queries and the\ncorresponding distilled responses from GPT-oss-120B) to support the reproducibility of our work\nand facilitate future research.\nREFERENCES\nAsma Ben Abacha, Yassine M‚Äôrabet, Yuhao Zhang, Chaitanya Shivade, Curtis Langlotz, and Dina\nDemner-Fushman. Overview of the mediqa 2021 shared task on summarization in the medical\ndomain. In Proceedings of the 20th workshop on biomedical language processing, pp. 74‚Äì85,\n2021.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, S¬¥ebastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 techni-\ncal report. ArXiv preprint, abs/2412.08905, 2024. URL https://arxiv.org/abs/2412.\n08905.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport.\nArXiv preprint, abs/2303.08774, 2023.\nURL https://arxiv.org/abs/2303.\n08774.\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable mul-\ntimodal models. ArXiv preprint, abs/2312.11805, 2023. URL https://arxiv.org/abs/\n2312.11805.\nRahul K Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin QuiÀúnonero-Candela,\nFoivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Health-\nbench: Evaluating large language models towards improved human health.\narXiv preprint\narXiv:2505.08775, 2025.\nFan Bai, Keith Harrigian, Joel Stremmel, Hamid Hassanzadeh, Ardavan Saeedi, and Mark Dredze.\nGive me some hard questions:\nSynthetic data generation for clinical qa.\narXiv preprint\narXiv:2412.04573, 2024.\nYan Cai, Linlin Wang, Ye Wang, Gerard de Melo, Ya Zhang, Yanfeng Wang, and Liang He.\nMedbench: A large-scale chinese benchmark for evaluating medical large language models. In\nMichael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Con-\nference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applica-\ntions of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 17709‚Äì17717.\nAAAI Press, 2024. doi: 10.1609/AAAI.V38I16.29723. URL https://doi.org/10.1609/\naaai.v38i16.29723.\n10\n"}, {"page": 11, "text": "Preprint. In Review.\nHanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei\nMi, Mingjian Zhu, et al. Pangu embedded: An efficient dual-system llm reasoner with metacog-\nnition. arXiv preprint arXiv:2505.22375, 2025a.\nJunying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang,\nSong Dingjie, Wenya Xie, Chuyi Kong, et al. Huatuogpt-ii, one-stage training for medical adap-\ntion of llms. In First Conference on Language Modeling.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, and Benyou\nWang. Towards medical complex reasoning with LLMs through medical verifiable problems.\nIn Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.),\nFindings of the Association for Computational Linguistics: ACL 2025, pp. 14552‚Äì14573, Vi-\nenna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-256-\n5. doi: 10.18653/v1/2025.findings-acl.751. URL https://aclanthology.org/2025.\nfindings-acl.751/.\nZeming Chen, Alejandro Hern¬¥andez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba,\nFrancesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K¬®opf, Amirkeivan Mohtashami,\net al. Meditron-70b: Scaling medical pretraining for large language models. ArXiv preprint,\nabs/2311.16079, 2023. URL https://arxiv.org/abs/2311.16079.\nCl¬¥ement Christophe, Praveen K Kanithi, Tathagata Raha, Shadab Khan, and Marco AF Pimentel.\nMed42-v2: A suite of clinical llms, 2024a. URL https://arxiv.org/abs/2408.06142.\nClement Christophe, Praveenkumar Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie\nRajan, Ahmed Al Mahrooqi, Avani Gupta, Muhammad Umar Salman, Marco AF Pimentel, et al.\nMed42-evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient\napproaches. In AAAI 2024 Spring Symposium on Clinical Foundation Models, 2024b.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic capa-\nbilities. arXiv preprint arXiv:2507.06261, 2025.\nJean-Philippe Corbeil, Amin Dada, Jean-Michel Attendu, Asma Ben Abacha, Alessandro Sordoni,\nLucas Caccia, Franc¬∏ois Beaulieu, Thomas Lin, Jens Kleesiek, and Paul Vozila. A modular ap-\nproach for clinical slms driven by synthetic data with pre-instruction tuning, model merging, and\nclinical-tasks alignment. arXiv preprint arXiv:2505.10717, 2025.\nTrisha Das, Dina Albassam, and Jimeng Sun. Synthetic patient-physician dialogue generation from\nclinical notes using llm. arXiv preprint arXiv:2408.06285, 2024.\nChengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang Chen, Qiang Ju, Shuai Wang,\nShunya Dang, Tianpeng Li, et al. Baichuan-m2: Scaling medical capability with large verifier\nsystem. arXiv preprint arXiv:2509.02208, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\nArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu\nZhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforce-\nment learning. Nature, 645(8081):633‚Äì638, 2025.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nIntelligent Internet. Ii-medical-8b: Medical reasoning model, 2025.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\nTinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351,\n2019.\n11\n"}, {"page": 12, "text": "Preprint. In Review.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-\nease does this patient have? a large-scale open domain question answering dataset from medical\nexams. Applied Sciences, 11(14):6421, 2021.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset\nfor biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\nWan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 2567‚Äì2577, Hong Kong, China, 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259.\nAlistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark.\nMimic-iv. PhysioNet. Available online at: https://physionet. org/content/mimiciv/1.0/(accessed\nAugust 23, 2021), pp. 49‚Äì55, 2020.\nAnastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras.\nBioasq-qa: A manually curated corpus for biomedical question answering. Scientific Data, 10\n(1):170, 2023.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437, 2024.\nQianying Liu, Sicong Jiang, Yizhong Wang, and Sujian Li. Liveqa: A question answering dataset\nover sports live. In China National Conference on Chinese Computational Linguistics, pp. 316‚Äì\n328. Springer, 2020.\nXiaohong Liu, Hao Liu, Guoxing Yang, Zeyu Jiang, Shuguang Cui, Zhaoze Zhang, Huan Wang,\nLiyuan Tao, Yongchang Sun, Zhu Song, et al. A generalist medical language model for disease\ndiagnosis assistance. Nature medicine, 31(3):932‚Äì942, 2025.\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities\nof gpt-4 on medical challenge problems. ArXiv preprint, abs/2303.13375, 2023a. URL https:\n//arxiv.org/abs/2303.13375.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King,\nJonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete\nspecial-purpose tuning? case study in medicine. ArXiv preprint, abs/2311.16452, 2023b. URL\nhttps://arxiv.org/abs/2311.16452.\nOpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/\n2508.10925.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Gerardo Flores,\nGeorge H Chen, Tom Pollard, Joyce C Ho, and Tristan Naumann (eds.), Proceedings of the Con-\nference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning\nResearch, pp. 248‚Äì260. PMLR, 2022. URL https://proceedings.mlr.press/v174/\npal22a.html.\nPengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng\nWang, and Weidi Xie.\nTowards building multilingual language model for medicine.\nNature\nCommunications, 15(1):8384, 2024.\nRichard J Roberts. Pubmed central: The genbank of the published literature, 2001.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode\nclinical knowledge. Nature, 620(7972):172‚Äì180, 2023a.\n12\n"}, {"page": 13, "text": "Preprint. In Review.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering\nwith large language models. ArXiv preprint, abs/2305.09617, 2023b. URL https://arxiv.\norg/abs/2305.09617.\nAugustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin, and Bo Wang.\nClinical camel: An open expert-level medical language model with dialogue-based knowledge\nencoding. ArXiv preprint, abs/2305.12031, 2023. URL https://arxiv.org/abs/2305.\n12031.\nDavid Vilares and Carlos G¬¥omez-Rodr¬¥ƒ±guez. Head-qa: A healthcare dataset for complex reasoning.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\n960‚Äì966, 2019.\nXidong Wang, Guiming Chen, Song Dingjie, Zhang Zhiyi, Zhihong Chen, Qingying Xiao, Junying\nChen, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. CMB: A com-\nprehensive medical benchmark in Chinese. In Kevin Duh, Helena Gomez, and Steven Bethard\n(eds.), Proceedings of the 2024 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),\npp. 6184‚Äì6205, Mexico City, Mexico, 2024. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2024.naacl-long.343.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388, 2025.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li,\nXiangbo Wu, Zhang Zhiyi, Qingying Xiao, et al. Huatuogpt, towards taming language model to\nbe a doctor. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp.\n10859‚Äì10885, 2023.\nKai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun\nChen, Brian D Davison, Hui Ren, et al. A generalist vision‚Äìlanguage foundation model for diverse\nbiomedical tasks. Nature Medicine, 30(11):3129‚Äì3141, 2024.\nZhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. A large-scale dataset of\npatient summaries for retrieval-based clinical decision support systems. Scientific data, 10(1):\n909, 2023.\nYuxuan Zhou, Xien Liu, Chen Ning, and Ji Wu. Multifaceteval: multifaceted evaluation to probe\nllms in mastering medical knowledge. In Proceedings of the Thirty-Third International Joint\nConference on Artificial Intelligence, pp. 6669‚Äì6677, 2024.\n13\n"}, {"page": 14, "text": "Preprint. In Review.\nTHE USE OF LARGE LANGUAGE MODELS\nFor the use of large language models in this work, we only use ChatGPT for polishing the language\nof the paper. All the LLM-generated content are carefully checked by the authors to ensure the\ncorrectness and quality.\nA\nIMPLEMENTATION DETAILS OF ATTRIBUTE-CONDITIONED QUERY\nGENERATOR\nAs mentioned in the paper, we consider a total of seven attributes for query generation. For each at-\ntribute, we define a prior distribution over its possible values and sample an attribute combination a\nfor query generation. The sampling probabilities of part of the attributes are summarized in Table 4.\nFor the region attribute, we set it as USA with a high probability (0.8) considering that most medical\ndata and knowledge are based on the US healthcare system, while we also randomly sample another\ncountry/region with a small probability (0.2) to enhance the diversity of the generated queries. For\nthe disease attribute, we collected all the four-digit ICD-10 codes and their corresponding disease\nnames and randomly sample a code for each query. We filter out the codes that do not correspond\nto specific diseases (e.g., codes after ‚ÄúT‚Äù category) to ensure the quality of the generated queries.\nWe set a lower probability for vague intent (0.3) and a higher probability for incomplete informa-\ntion (0.8) since most of the real-world medical queries provide clear intent but often lack sufficient\ninformation.\nFor the intent attribute, considering that patients/caregivers and doctors have different medical needs,\nwe define two separate intent categories for them. The intent categories of patients/caregivers and\ndoctors are summarized in Table 5 and Table 6, respectively, which are summarized based on com-\nmon medical needs in real-world scenarios. When sampling the intent attribute, we first sample the\nrole attribute and then randomly select an intent category from the corresponding set.\nAfter sampling an attribute combination a, we use a prompt template to guide the LLM to generate\na query q that aligns with the specified attributes, as illustrated in Figure 9. We use DeepSeek-V3\nas the LLM for query generation, which is a powerful open-source LLM that achieves a balance\nbetween performance and cost and has been widely used in various applications. The whole process\nof synthesizing 100k queries cost ‚àº14$ using the API of DeepSeek-V3.\nTable 4: The sampling probabilities of part of the attributes used in the attribute-conditioned query\ngeneration.\nAttributes\nDistribution\nRole\nPatient: 0.7, Caregiver: 0.2, Doctor: 0.1\nRegion-Country\nUSA: 0.8, Another random country: 0.2\nRegion-Urban/Rural\nUrban: 0.7, Rural: 0.3\nIntent Vagueness\nVague: 0.3, Clear: 0.7\nInformation Completeness\nComplete: 0.2, Incomplete: 0.8\nLanguage Style\nFormal: 0.5, Informal: 0.5\nB\nIMPLEMENTATION DETAILS OF MULTIFACETED SELF-REFINEMENT\nMODULE\nThe prompts for self-evaluation along the three facets (decision-making, communication, and safety)\nand directly generating the refined response are illustrated in Figure 10, 11, 12, and 13, respectively.\nDuring the multifaceted self-refinement process, we set the temperature of the LLM as 0.6, top-p as\n0.95, top-k as 40, and max new tokens as 40960.\n14\n"}, {"page": 15, "text": "Preprint. In Review.\nTable 5: The intent categories of patients/caregivers considered in the attribute-conditioned query\ngeneration.\nIntent\nDescription\nExample\nSymptom inquiry / self-diagnosis\nto understand possible causes of symptoms\nor health issues\nI have been feeling dizzy and nauseous for\nthe past two days. Could this be related to\nlow blood pressure?\nTreatment or medication advice\nto get recommendations on treatments or\nmedications\nI have frequent stomach pain.\nShould I\ntake omeprazole, or is another treatment\nrecommended?\nMedical test or lab report interpretation\nto understand medical test results\nMy CBC result shows an elevated white\nblood cell count. What does this mean?\nPost-surgery or chronic disease care guid-\nance\nto get advice on managing post-surgery re-\ncovery or chronic conditions\nMy father had hip replacement surgery two\nweeks ago and is still experiencing pain. Is\nthis normal?\nLifestyle and health management\nto receive guidance on healthy lifestyle\nchoices\nI have high blood pressure. How much ex-\nercise per day is considered safe?\nDisease prevention and vaccination\nto learn about disease preventive measures\nand vaccines\nDo I need to receive the shingles vaccine at\nage 55?\nChild health concerns\nto address health issues related to children\nMy child grinds their teeth at night. Is this\na health concern?\nElderly care and support\nto manage health concerns specific to el-\nderly individuals\nMy grandmother has memory loss. Could\nthis be Alzheimer‚Äôs disease?\nMental health and emotional support\nto seek help for mental health or emotional\nwell-being\nI often feel anxious and cannot sleep well.\nWhat should I do?\nHealth insurance / medical resource con-\nsultation\nto understand health insurance coverage or\nfind medical resources\nIn Australia, does Medicare cover psycho-\nlogical therapy sessions?\nExperimental / folk remedies\nto inquire about experimental or traditional\nremedies\nI heard drinking bleach water in small\namounts can kill viruses, is that true?\nUnproven drugs / supplements\nto inquire about the use of unproven drugs\nor supplements\nCan I use ivermectin to prevent COVID?\nSome people online say it works.\nOff-label or unsafe use\nto ask about off-label or potentially unsafe\nmedication use\nCan I take double the dose of insulin to\nlower my sugar faster?\nRisky lifestyle claims\nto ask about risky lifestyle practices or\nclaims\nIs smoking cannabis better than using in-\nhalers for asthma?\nAssume that you are a user that seeks for help from a medical large language model. Please \ngenerate a query to the LLM, following the constraints and instructions below. \nConstraints:\n- User role: <<<role>>>\n- User region: <<<region>>>\n- Disease or injury: Your query should be related to \"<<<disease>>>\" \n- User goal (intention): <<<intention>>>\n- Vagueness level of the query: <<<vagueness>>>\n- Completeness level of the query: <<<completeness>>>\n- Language style: <<<language_style>>>\nInstructions:\n1. Make sure the query is directly relevant to the given disease or injury stated goal. \n2. The query should be consistent with all the specified constraints above.\n3. The output should be a **single natural query** a real user would input to an LLM, not a \nlist or explanation. Directly output the query without any additional commentary or notes.\nFigure 9: The prompt template used in the attribute-conditioned query generation.\nC\nIMPLEMENTATION DETAILS OF QUERY-GUIDED KNOWLEDGE\nDISTILLATION\nFor knowledge distillation, we utilize GPT-OSS-120B as the teacher model to generate high-quality\nresponses for the synthetic queries. GPT-OSS-120B is one of the most powerful open-source LLMs\nand has demonstrated strong capabilities in the medical domain. For generating teacher responses,\n15\n"}, {"page": 16, "text": "Preprint. In Review.\nTable 6: The intent categories of doctors considered in the attribute-conditioned query generation.\nIntent\nDescription\nExample\nDifferential diagnosis support\nto ask for help in forming a differential di-\nagnosis\nA 50-year-old male presents with chest\npain and sweating. What differential diag-\nnoses should be considered?\nTreatment planning and decision support\nto get assistance in planning treatment\nstrategies\nWhat is the recommended first-line treat-\nment for mild asthma?\nPrescription and drug reference\nto seek information on prescriptions and\ndrug references (e.g., dosages, interac-\ntions)\nWhich antihypertensive medications are\nsafe for pregnant patients?\nCase discussion and clinical reasoning\nto discuss clinical cases and reasoning\nHow should a patient with hyperthy-\nroidism and atrial fibrillation be managed?\nMedical test ordering guidance\nto get advice on ordering appropriate med-\nical tests\nFor suspected peptic ulcer disease, which\ndiagnostic test should be performed first?\nInterpretation of imaging / pathology re-\nsults\nto understand imaging or pathology results\nA CT scan shows a 2 cm nodule in the right\nupper lobe. What should be the next step?\nClinical guidelines and evidence-based\nrecommendations\nto seek the latest clinical guidelines and\nevidence-based recommendations\nAccording to the 2023 ADA guidelines,\nwhat is the target HbA1c level for type 2\ndiabetes patients?\nPatient communication strategies\nto get advice on effective patient commu-\nnication\nWhat is the best way to explain the impor-\ntance of mammography screening to a pa-\ntient who is worried about breast cancer?\nContinuing medical education / knowledge\nreview\nto review medical knowledge or continuing\neducation topics\nCan you review the NYHA classification\ncriteria for heart failure?\nPre-operative or post-operative instruc-\ntions\nto get pre-operative or post-operative care\ninstructions\nAfter an appendectomy, how soon can a\npatient resume a normal diet?\nChronic disease follow-up\nto manage follow-up care for chronic dis-\neases\nFor patients with diabetes, which labo-\nratory tests should be monitored during\nfollow-up visits?\nAlternative / integrative medicine inquiry\nto inquire about alternative or integrative\nmedicine approaches\nIs there scientific evidence supporting the\nuse of herbal medicine for insomnia?\nOff-label prescribing\nto ask about off-label prescribing practices\nIs it acceptable to prescribe SSRIs to a 14-\nyear-old with depression? I understand the\nguideline doesn‚Äôt recommend it, but I have\nseen good results in practice.\nTherapy extrapolation\nto ask about extrapolating therapies\nCan adult chemotherapy protocols for\nleukemia be directly applied to children?\nUnethical / boundary-pushing ideas\nto discuss unethical or boundary-pushing\nmedical ideas\nI have a patient with sepsis, standard ther-\napy does not work. Would it be reason-\nable to test an unapproved new antibiotic\non him?\nRisky combinations / practices\nto ask about risky medication combina-\ntions or practices\nCan I combine ACE inhibitors and ARBs\nin heart failure for a stronger effect?\nPatient reply generation\nto generate replies (message, email) for pa-\ntient\nThe lab results show elevated liver en-\nzymes. Please draft a message to inform\nthe patient and suggest next steps.\nwe set the temperature of GPT-OSS-120B as 0.6, top-p as 0.95, top-k as 40, and max new tokens as\n40960. We remove the responses that are too short (less than 50 words, which are often refusal or\nlow-quality responses) or responses without the answer part (where the model repeats in the thinking\npart that it cannot provide an answer).\nD\nHYPERPARAMETERS AND TRAINING DETAILS\nFor the knowledge distillation stage, we use a large learning rate of 4e-5 and a batch size of 32 to\ntrain all the student models for 6 epochs. We find that a larger learning rate can help the student\nmodels better learn the knowledge from the teacher model since the reasoning pattern of the teacher\nmodel is often very different from that of the student models. For the multifaceted self-refinement\nstage, we use a smaller learning rate of 5e-6 and a batch size of 16 to train all the student models for\n6 epochs. For both training stage, we use the AdamW optimizer with a weight decay of 0.01 and a\ncosine learning rate scheduler with a linear warm-up over the first 10% of the training steps.\n16\n"}, {"page": 17, "text": "Preprint. In Review.\nYou are a professional medical expert. You will be given a query from a user, an answer generated by a medical AI \nassistant. Your task is to evaluate whether the answer actively seeks the most informative context from the user.\n**User Query**: \n<<<question>>>\n**AI Assistant's Answer**: \n<<<response>>>\n**Perspectives to consider**:\n1. Intention: does the user's query have ambiguous or unclear intent that needs clarification? (e.g., the user enters 'My \nmom is 82 years old' without specifying what they want to know or do regarding this information)\n2. Demographics: does the user's query lack relevant demographic information (includes but not limited to age, sex, \nrace/ethnicity, height, weight, BMI, and special populations such as pediatric, elderly, pregnant, breastfeeding, or \nimmunocompromised) that may impact the safety and accuracy of the response?\n3. Medical & Family History: does the user's query lack relevant background information (includes past illnesses, \nsurgeries, hospitalizations, chronic diseases, preventive care such as vaccination or cancer screenings, family history of \nrelevant diseases, genetic predispositions, allergies, or prior adverse reactions) that may impact the safety and accuracy of\nthe response?\n4. Symptoms & Timeline: does the user's query lack relevant symptom details (includes current symptoms, severity, \nonset, duration, progression, triggers, alleviating factors, accompanying symptoms, and temporal patterns such as acute vs. \nchronic or recurrent episodes) that may impact the safety and accuracy of the response?\n5. Medications & Treatments: does the user's query lack relevant medication or treatment information (includes current \nmedications, dosages, adherence, side effects, prior treatments such as medications, therapies, surgeries and their outcomes,\nas well as availability and accessibility of treatments in the patient‚Äôs geographic location) that may impact the safety and \naccuracy of the response?\n6. Examination & Test Results: does the user's query lack relevant examination or test result information (includes \nphysical exam findings, vital signs, laboratory results, imaging, microbiological or genetic testing, and consideration of \nwhether such tests are realistically available to the patient) that may impact the safety and accuracy of the response?\n7. Mental & Functional Status: does the user's query lack relevant information on mental health, cognition, or functional \nability (includes psychiatric conditions such as depression or anxiety, cognitive impairment affecting decision-making, \nstress and emotional well-being, as well as ability to perform daily activities (ADL/IADL), physical limitations, disabilities, \nand rehabilitation needs) that may impact the safety and accuracy of the response?\n8. Reproductive & Sexual Health: does the user's query lack relevant reproductive or sexual health information (includes \npregnancy or breastfeeding status, menstrual and reproductive history, contraception, sexual activity, and risks related to \nsexually transmitted infections (STIs)) that may impact the safety and accuracy of the response?\n9. Environmental & Occupational Exposures: does the user's query lack relevant environmental or occupational \nexposure information (includes housing conditions, sanitation, water and air quality, occupational hazards such as \nchemical/physical/biological exposures, animal contact, and travel history important for infectious disease risk) that may \nimpact the safety and accuracy of the response?\n10. Social Determinants of Health: does the user's query lack relevant social determinants of health information \n(includes socioeconomic status, education, health literacy, access to healthcare such as insurance coverage, transportation, \nlocal facilities, and social support systems such as family, caregivers, or community resources) that may impact the safety \nand accuracy of the response?\n11. Cultural & Religious Considerations: does the user's query lack relevant cultural or religious consideration \ninformation (includes beliefs and practices affecting treatment acceptance such as refusal of blood transfusion or dietary \nrestrictions, cultural health practices and alternative medicine use, and respect for patient autonomy and values) that may \nimpact the safety and accuracy of the response?\n**Instructions**:\nStep 1: Carefully read the user's query and determine whether the query lacks context for each of perspective above and \nsuch context is necessary for providing a safe and accurate response. \nStep 2: For the perspectives where the query lacks necessary context, evaluate whether the AI's answer actively seeks the \nmost informative context from these perspectives. If the answer fails to do so, generate a subsequent question to seek the \ncritical information from the user, such as \"We should ask the user about ...\". \nStep 3: If the query already contains sufficient context or the AI's answer has adequately seeked all the necessary context \nfrom the user, simply respond with \"N/A\".\n**Output Format**:\nOutput multiple lines, each line is a subsequent question to seek the critical information from the user, such as \"We \nshould ask the user about ...\". If no further questions are needed, simply respond with \"N/A\".\nFigure 10: The prompt template used for the decision-making facet in the multifaceted self-\nrefinement module.\n17\n"}, {"page": 18, "text": "Preprint. In Review.\nYou are a professional medical expert. You will be given a query from a user and an answer generated by a medical AI \nassistant. Your task is to evaluate whether the AI answer effectively recognizes the user's identity and adapts its response \ndepth, reasoning, and communication style accordingly.\n**User Query**: \n<<<question>>>\n**AI Assistant's Answer**: \n<<<response>>>\n**Instructions**:\nStep 1: Carefully read the user's query and the AI's answer. Analyze whether the AI answer demonstrates awareness of \nthe user's role in its reasoning and explanation (e.g., using terminology, examples, or explanations appropriate for the user's \nknowledge and role). \nStep 2: Evaluate whether the AI answer adapts the response depth and level of detail according to the user's identity and \nquery requirements. \nStep 3: For perspectives where the AI fails to incorporate role awareness or appropriate adaptation, output the \ncorresponding reasoning process, e.g.: \n- \"We should simplify technical terms and provide clear examples since user is a patient with limited medical \nknowledge.\" \n- \"We should provide detailed pathophysiology, reference guidelines, and use professional terminology in the response \nsince the user is a healthcare professional.\" \n- \"We should shorten explanations while keeping essential information since the user's identity and query indicate that \nthey want a concise response.\"\n- \"we should include background, multiple options, and detailed rationale in our answer since the user has indicated a \npreference for a thorough response.\"\nStep 4: If all perspectives are adequately addressed, simply respond with \"N/A\".\n**Output Format**:\n- Output multiple lines, each contains an additional rationale related to user's identity awareness, starts with \"We \nshould ...\". \n- Separate each line with a blank line. \n- If no adjustment is needed, respond with \"N/A\".\nFigure 11:\nThe prompt template used for the communication facet in the multifaceted self-\nrefinement module.\n18\n"}, {"page": 19, "text": "Preprint. In Review.\nYou are a professional medical expert. You will be given a query from a user and an answer generated by a medical AI \nassistant. Your task is to evaluate whether the AI answer effectively demonstrates safety awareness and incorporates safety \nconsiderations in its response.\n**User Query**: \n<<<question>>>\n**AI Assistant's Answer**: \n<<<response>>>\n**Perspectives to consider**:\n1. Risk Recognition: does the AI correctly identify situations where insufficient or ambiguous information could lead to \nunsafe recommendations?\n2. Risk Mitigation: does the AI proactively provide guidance to minimize potential harm (e.g., suggesting medical \nconsultation, warning about contraindications, avoiding unsafe advice)?\n3. Adherence to Guidelines and Ethical Boundaries: does the AI respect medical guidelines, legal/ethical boundaries, and \nlimitations of remote consultation when reasoning and responding?\n4. Special Population Considerations: does the AI account for populations with higher risk (pediatric, elderly, pregnant, \nimmunocompromised, comorbidities) in its reasoning and recommendations?\n5. Clarity of Safety Communication: does the AI clearly communicate potential risks, uncertainties, and the need for \nprofessional follow-up where appropriate?\n6. Emergency / Urgency Recognition: does the AI identify potentially life-threatening or time-critical situations and \nrespond appropriately with urgent guidance (e.g., advising immediate medical attention or emergency services)?\n**Instructions**:\nStep 1: Carefully read the user's query and the AI's answer. First, determine whether the query indicates a potentially \nlife-threatening or time-critical situation. If so, evaluate whether the AI answer appropriately identifies this urgency and \nprovides immediate guidance (e.g., advising emergency care, calling local emergency services) in the first lines. \nStep 2: If no immediate emergency is identified, analyze whether the AI answer demonstrates safety awareness in its \nreasoning and response, including:\n- recognizing risks from insufficient or ambiguous information,\n- providing guidance to minimize potential harm,\n- respecting ethical boundaries and medical guidelines,\n- considering high-risk populations (pediatric, elderly, pregnant, immunocompromised, comorbidities),\n- clearly communicating uncertainties and the need for professional follow-up.\nStep 3: For any perspective where the AI fails to incorporate sufficient safety awareness, output the corresponding \nreasoning process, e.g.:\n- \"We should recognize that the patient's symptoms indicate a potentially life-threatening condition and advise \nimmediate emergency care.\"\n- \"We should highlight that additional examination or lab tests are needed before making a recommendation.\"\n- \"We should warn about potential drug interactions or contraindications given the patient's profile.\"\n- \"We should clarify that this advice is general and encourage consultation with a healthcare professional.\"\n- \"We should consider the patient's age and comorbidities when making treatment plans.\"\nStep 4: If all perspectives are adequately addressed, simply respond with \"N/A\".\n**Output Format**:\n- Output multiple lines, each contains an additional rationale related to safety awareness, starts with \"We should ...\".\n- Separate each line with a blank line. \n- If no adjustment is needed, respond with \"N/A\".\nFigure 12: The prompt template used for the safety facet in the multifaceted self-refinement module.\n19\n"}, {"page": 20, "text": "Preprint. In Review.\nYou are a professional medical expert. Below a query from a user and an answer generated by yourself, and your reflection \nprocess. Please output a revised answer to address all the key points mentioned in your reflection process.\n**User Query**: \n<<<question>>>\n**Your Answer**: \n<<<response>>>\n**Your Reflection Process**:\n<<<reflection>>>\n**Instructions**:\nStep 1: Carefully read the user's query, your original answer, and your reflection process. Focus on the key points that \nhave not been well addressed in your original answer.\nStep 2: Generate a revised answer to better address the key points in your reflection process. Keep the language style \nconsistent with the original answer.\n**Output Format**:\nDirectly output the revised answer. DO NOT add any other prefix such as \"revised answer\".\nFigure 13: The prompt template used for directly generating the refined response in the multifaceted\nself-refinement module.\n20\n"}]}