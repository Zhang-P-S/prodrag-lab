{"doc_id": "arxiv:2512.09127", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.09127.pdf", "meta": {"doc_id": "arxiv:2512.09127", "source": "arxiv", "arxiv_id": "2512.09127", "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation", "authors": ["Zihan Han", "Junyan Ge", "Caifeng Li"], "published": "2025-12-09T21:11:55Z", "updated": "2025-12-09T21:11:55Z", "summary": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.09127v1", "url_pdf": "https://arxiv.org/pdf/2512.09127.pdf", "meta_path": "data/raw/arxiv/meta/2512.09127.json", "sha256": "f57c21c9d3e4b9f4b20199b8990b1c8e4cf5cf03e7b18ca93d63e2d3f99c447a", "status": "ok", "fetched_at": "2026-02-18T02:24:34.228164+00:00"}, "pages": [{"page": 1, "text": "Knowledge-Guided Large Language Model for\nAutomatic Pediatric Dental Record Understanding and\nSafe Antibiotic Recommendation\nZihan Han1, Junyan Ge 2, Caifeng Li 3\n1 School of Automation and Electrical Engineering, University of Jinan, Jinan,\nChina\n2 College of Dental Medicine, Columbia University, New York, NY, USA\n3 Jilin University, Changchun, China\n11812503968@qq.com\n2 jg4377@caa.columbia.edu\n3 lisa660601@163.com\nAbstract. Accurate interpretation of pediatric dental clinical records and safe\nantibiotic prescribing remain persistent challenges in dental informatics. Traditional\nrule-based clinical decision support systems struggle with unstructured dental\nnarratives, incomplete radiographic descriptions, and complex safety constraints. To\naddress these limitations, this study proposes a Knowledge-Guided Large Language\nModel\n(KG-LLM)\nthat\nintegrates\na\npediatric\ndental\nknowledge\ngraph,\nretrieval-augmented generation (RAG), and a multi-stage safety validation pipeline\nfor evidence-grounded antibiotic recommendation. The framework first employs a\nclinical NER/RE module to extract structured entities and relations from dental notes\nand radiology reports. Relevant guidelines, drug-safety rules, and analogous\nhistorical cases are subsequently retrieved from the knowledge graph and supplied to\nthe LLM for diagnostic summarization and dose–drug–duration prediction. Safety\nassurance is achieved through a dual-layer validation mechanism combining\ndeterministic rule checking with a learned classifier for detecting allergies,\ncontraindications, and dosing errors. Experiments on 32,000 de-identified pediatric\ndental visit records demonstrate the effectiveness of the proposed approach.\nCompared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves\nrecord-understanding\nperformance\n(F1:\n0.914\nvs.\n0.867),\ndrug-dose-duration\naccuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%.\nAdditional evaluation across summary quality, recommendation accuracy, and global\nsafety scores further confirms the robustness of the system. Ablation analyses\nindicate that the knowledge graph, RAG, and safety modules each contribute\nsubstantially to clinical reliability and interpretability.\nKeywords: Pediatric dentistry; antibiotic stewardship; large language model (LLM);\nknowledge graph; retrieval-augmented generation (RAG); clinical decision support\nsystem (CDSS); dental informatics; medical NLP\n1. Introduction\nPediatric dental infections, including pulpitis and periapical abscesses, represent some of the\nmost common causes of acute dental pain among children. Accurate diagnosis and antibiotic\nstewardship are critical in these cases, as inappropriate prescriptions increase the risks of\n"}, {"page": 2, "text": "adverse drug reactions, antimicrobial resistance, and long-term disruptions to the oral\nmicrobiome. However, clinical decision-making in pediatric dentistry remains challenging\ndue to heterogeneous electronic dental records (EDRs), complex radiographic descriptions,\nage-dependent pharmacokinetic considerations, and varying compliance with clinical\nguidelines. These challenges underscore the need for intelligent, automated systems capable\nof understanding pediatric dental records and supporting safe antibiotic prescribing.\nRecent advancements in large language models (LLMs) have demonstrated remarkable\ncapabilities in clinical text understanding, medical entity extraction, and guideline grounded\nreasoning. Nevertheless, general-purpose LLMs often hallucinate, lack domain-specific\nknowledge, and cannot reliably handle specialized pediatric dental terminology or drug\ncontraindication\nconstraints.\nTo\naddress\nthese\nlimitations,\nthis\nstudy\nadopts\na\nKnowledge-Guided Large Language Model (KG-LLM) framework, which integrates\nstructured medical knowledge into LLM reasoning processes to ensure factual reliability,\nexplainability, and safe decision support.\nThe\nproposed\nKG-LLM\nframework\nunifies\nthree\nkey\ncomponents:\n(1)\na\nretrieval-augmented pipeline that extracts relevant clinical knowledge from a pediatric dental\nknowledge graph, including pathogen profiles, treatment guidelines, and age-specific\nantibiotic dosage rules; (2) a dental-domain semantic parser that interprets heterogeneous\nEDRs, radiographic descriptions, and treatment histories; and (3) a safety-aware antibiotic\nrecommendation module grounded in pharmacology, contraindication rules, and pediatric\ndosing\nconstraints.\nThrough\nthis\narchitecture,\nthe\nKG-LLM\ncan\nperform\ncoherent\nunderstanding of multi-modal pediatric dental records and generate clinically interpretable\nantibiotic recommendations supported by explicit knowledge evidence.\nThis study makes the following major contributions:\n(1) We propose the first KG-LLM framework specifically designed for pediatric dental\ninfection management, integrating knowledge graphs, guideline retrieval, and medical\nLLM reasoning.\n(2) We construct a unified pediatric dental record understanding pipeline, enabling\naccurate extraction of pulp status, infection progression, radiographic cues, and prior\ntreatment actions.\n(3) We\ndevelop\na\nsafety-aware\nantibiotic\nrecommender,\ncapable\nof\nidentifying\ncontraindications, inappropriate dosage, and drug–disease conflicts while providing\nguideline-aligned alternatives.\n(4) We curate a multi-source dataset of pediatric dental cases, including clinical narratives,\nradiology reports, and structured drug labels, for evaluating KG-enhanced reasoning.\n(5) We demonstrate that the KG-LLM significantly outperforms strong baselines,\nimproving factual correctness, dosage safety, and explanation quality across multiple\nbenchmarks.\nOverall, this work contributes a robust and interpretable AI-assisted decision support system\ntailored for pediatric dentistry, offering a clinically meaningful step toward safer and more\nintelligent antibiotic stewardship.\n2. Literature Review\nThis section reviews existing research closely related to pediatric dental record understanding,\nmedical-domain large language models, knowledge-augmented clinical NLP, and safe\nantibiotic recommendation systems. The literature is organized into three major streams: (1)\nLLMs for medical text understanding, (2) knowledge-enhanced frameworks and clinical\nknowledge graphs, and (3) AI-driven antibiotic stewardship and dental informatics.\n2.1 Large Language Models for Medical Record Understanding\nLLMs have shown increasing potential in clinical text processing, including entity extraction,\ndiagnosis summarization, and clinical reasoning. Early biomedical language models such as\nBioBERT [1] and ClinicalBERT [2] demonstrated strong performance in medical named\nentity recognition (NER) and relation extraction tasks. More recent domain-specific LLMs,\n"}, {"page": 3, "text": "including BioGPT [3] and Med-PaLM 2 [4], advanced these capabilities by integrating\ntransformer architectures with medical knowledge alignment, enabling improved performance\non question answering, radiology report classification, and clinical guideline interpretation.\nIn the dental field, several systems have attempted to automate charting and radiographic\ninterpretation. Studies such as Mohammad-Rahimi H et al. [5] demonstrated the feasibility of\nAI models for detecting caries and periapical lesions using deep learning, while Pethani F et\nal. [6] applied text-mining approaches to extract structured entities from dental EHRs.\nHowever, these systems focus primarily on single-task models and lack the complex\nreasoning abilities required in pediatric infection management. Existing LLM-based dental\napplications (e.g., ChatGPT for dental education [7]) remain limited due to hallucination\nissues\nand\ndomain\nknowledge\ninsufficiency.\nThus,\ndeveloping\na\nstructured,\nknowledge-guided\nLLM\nframework\nis\nessential\nfor\nrobust\npediatric\ndental\nrecord\nunderstanding.\n2.2 Knowledge Graphs and Knowledge-Augmented LLMs\nKnowledge graphs (KGs) have become a cornerstone for enhancing LLM factuality and\ninterpretability. Seminal work such as UMLS Metathesaurus [8], SNOMED-CT [9], and\nDrugBank\n[10]\nprovides\nstructured\nrelationships\nbetween\ndiseases,\nsymptoms,\nand\nmedications, serving as critical resources for clinical reasoning tasks. Several KG-enhanced\nneural architectures—such as K-BERT [11], TarKG [12], and COMET [13]—demonstrated\nthat injecting structured knowledge can significantly reduce hallucinations and improve\nfactual consistency.\nIn medical domains, retrieval-augmented generation (RAG) frameworks have been widely\nadopted. Lewis et al. [14] proposed the foundational RAG model, while more recent\nhealthcare applications such as MedRAG [15] leverage clinical guidelines and biomedical\nliterature for grounded reasoning. These systems highlight the effectiveness of jointly\nintegrating parametric LLM knowledge with non-parametric, query-driven knowledge\nretrieval. However, no existing work has applied KG-enhanced LLMs to pediatric dental care,\nespecially regarding infection diagnosis and safe antibiotic prescription grounded in\npharmacological and age-dependent knowledge.\n2.3 AI for Antibiotic Stewardship and Dental Infection Management\nThe inappropriate prescription of antibiotics remains a global healthcare challenge, especially\nin pediatric dentistry. Prior research in antibiotic stewardship has focused on decision-support\nalgorithms and guideline-based systems. Fleming-Dutra et al. [16] reported widespread\nantibiotic misuse among children, highlighting the need for intelligent, real-time decision\nsupport. Machine learning efforts, such as those by Sakagianni A et al. [17] and Düvel J A et\nal. [18], developed models for predicting antibiotic resistance or recommending empiric\ntherapy. However, these models are limited to general medicine and do not address dental\ninfections.\nIn dental\nresearch,\nstudies\nhave\nexplored AI\nfor\ndiagnosis rather\nthan\nantibiotic\nrecommendation. Deep-learning-based models for lesion detection [5], root morphology\nanalysis [19], and radiographic segmentation [20] have improved clinical workflows but lack\nintegration with therapeutic decision-making. No prior study provides an integrated\nLLM-based system capable of understanding pediatric dental records and connecting\ndiagnosis with safe antibiotic recommendations.\nThis gap motivates the development of our proposed Knowledge-Guided Large Language\nModel (KG-LLM), which unifies pediatric dental semantic understanding with a safety-aware\nantibiotic recommendation framework grounded in structured clinical knowledge.\n3. Methodology\nThis section presents the proposed Knowledge-Guided Large Language Model (KG-LLM)\ndesigned for automatic understanding of pediatric dental records and safe antibiotic\nrecommendation. The model integrates a parametric large language model with a clinically\ngrounded knowledge graph, guideline-aware retrieval, and a safety-constrained generation\n"}, {"page": 4, "text": "module. The overall architecture unifies semantic representation learning, medical knowledge\nintegration, and rule-based pharmacological safety constraints to ensure factual and robust\nclinical decision support. The subsections below describe the KG-LLM framework in detail.\nFigure 1 illustrates the flowchart of the proposed model. This model integrates pediatric\ndental visit records as input, using a knowledge-guided large language model (KG-LLM) to\nperform knowledge-augmented retrieval and extract contextually relevant information\nthrough a defined auxiliary extraction objective. The record understanding module parses the\nextracted information into a semantic representation readable by the LLM. The KG-LLM\nmodel, in addition, performs search and retrieval from available dental-clinical knowledge\ndatabases and generates a representation of relevant information from the knowledge graph.\nThe KG-LLM uses a learnable gating parameter to generate a fusion of the semantic\nrepresentation from the dental record input and the relevant information from the knowledge\ngraph, weighted by accuracy and relevance. Lastly, the safety-constrained recommendation\nmodule computes a safety score for each antibiotic candidate based on a pharmacological\nconstraint function applied to the fusion output. Candidates with a score exceeding the\nthreshold will be generated as the final output of a safe antibiotic recommendation.\nFigure 1. Overall flowchart of the model.\n3.1 Problem Formulation and City-Scale Spatio-Temporal Graph Construction\nThe KG-LLM model follows a hybrid architecture where a foundation LLM (e.g.,\nLlama-3-Med or BioGPT-Large) is augmented by structured external knowledge sources\nthrough both retrieval and embedding fusion. Formally, given an input pediatric dental record\nx, the system first encodes it into a semantic representation ℎǜ = Ǌش(ǜ) , where Ǌش\nrepresents the parametric LLM with trainable parameters ش. To ensure factual grounding, the\nmodel performs knowledge retrieval over a unified dental-clinical knowledge graphƱ, which\nincludes entities and relations from UMLS, SNOMED-CT, DrugBank, antibiotic guidelines,\nand pediatric dosage specifications.\nThe retrieved subgraph Ʊǜ ∈Ʊ is selected by a relevance scoring function:\nƱǜ = 푇표ǔƵ(푠푐표ǖǉ(ǉǍ, ℎǜ)),\nǉǍ ∈Ʊ,\n(1)\nwhere 푠푐표ǖǉ( ) combines embedding similarity and rule-based keyword matching. The\nretrieved knowledge is encoded with a graph encoder:\nℎƱ = ǋه(Ʊǜ),\n(2)\n"}, {"page": 5, "text": "where ǋه denotes a GNN-based encoder (GraphSAGE or GAT).\nThe final representation is a fusion of the LLM hidden state and the knowledge-grounded\nembedding:\nℎ∗= ح ⋅ℎǜ + (1 −ح) ⋅ℎƱ,\n(3)\nwhere ح is a learnable gating parameter. This blended representation enables the LLM to\nproduce clinically consistent interpretations and safe therapeutic suggestions.\n3.2 Pediatric Dental Record Understanding Module\nThe model first performs automatic understanding of pediatric dental records, which may\ninclude clinical notes, radiology descriptions, symptom summaries, treatment histories, and\nfree-text dentist narratives. The parsing task is\nframed as a sequence-to-structure\ntransformation, where the LLM extracts disease entities, lesion locations, infection severity,\nage-related risk factors, and previous antibiotic use.\nTo enhance precision, we define an auxiliary extraction objective. Given a target label\nsequence y corresponding to structured dental entities, the model minimizes:\nƶ푁ƯƼ =−\nǘ=1\n푇\n푙표ǋƺش\nႈ\n(ǝǘ∣ǜ, Ʊǜ),\n(4)\nThis objective is jointly optimized with the main generative objective to ensure consistent\nsemantic\nalignment.\nThe\nKG-LLM\nleverages\npediatric-specific\nknowledge\n(e.g.,\nage-dependent pulp chamber morphology, immune development characteristics) embedded in\nthe KG to improve entity disambiguation. For example, “acute swelling near tooth #85” is\nlinked to “primary mandibular second molar” and further mapped to possible diagnoses such\nas “acute pulpitis” or “periapical abscess.”\nThe fusion-based representation ℎ∗allows the model to incorporate anatomical hierarchies\nand disease-progression patterns from the KG, resulting in a richer contextual understanding\ncompared to purely text-based LLMs.\n3.3 Knowledge-Augmented Retrieval for Clinical Guidelines and Pharmacology\nTo\nensure\naccurate\nantibiotic\ndecisions,\nthe\nsystem\nintegrates\na\nRAG-style\n(Retrieval-Augmented Generation) pipeline. The retrieval component searches multiple\nmedical sources such as antibiotic prescribing guidelines, dental infection pathways,\ndrug–drug interaction rules, and weight-based pediatric dosage tables.\nFor each dental condition ddd inferred from the previous module, the retriever locates\nrelevant guideline passages Ǟ ∈Ʈ by:\nǞ = 푎ǖǋ max\nǞǍ∈Ʈ  (푠ǍǑ(ℎ∗, ℎǞǍ))  ,\n(5)\nwhere\nℎǞǍ is the embedding of the guideline text and similarity is computed by a\ndual-encoder architecture.\nThe retrieved guidelines are incorporated into the LLM through cross-attention. This\nallows the model to align generated recommendations with authoritative clinical standards,\nsuch as AAPD and ADA pediatric antibiotic protocols. The RAG component not only\nenhances factual consistency but also ensures that the model reflects the latest clinical best\npractices.\n3.4 Safety-Constrained Antibiotic Recommendation Module\nThe final step is generating a safe antibiotic recommendation tailored to the patient’s age,\nweight, infection severity, allergy profile, and contraindication conditions. The model uses a\n"}, {"page": 6, "text": "constrained decoding strategy where candidate antibiotic outputs are validated by a\npharmacological constraint function.\nGiven a generated antibiotic candidate a, a safety score is computed by:\nƽ푠푎Ǌǉǘǝ(푎, Ʊǜ) = م1ƽ푑표푠ǉ + م2ƽ푎푙푙ǉǖǋǝ + م3ƽǍ푛ǘǉǖ푎푐ǘǍ표푛,\n(6)\nwhich evaluates (1) adherence to pediatric dosing limits, (2) allergy contradiction checks,\nand (3) drug–drug interaction risks. Recommendations failing the constraint threshold:\nƽ푠푎Ǌǉǘǝ(푎, Ʊǜ) < ـ,\n(7)\nare rejected and regenerated. This mechanism prevents unsafe decisions such as overdose,\ninappropriate antibiotic selection (e.g., amoxicillin-clavulanate for mild cases), or prescribing\ncontraindicated drugs to penicillin-allergic children.\nThe loss function for antibiotic recommendation incorporates both clinical appropriateness\nand safety:\nƶƼǜ =−푙표ǋ ƺش(푎∗∣ǜ, Ʊǜ) + ط(1 −ƽ푠푎Ǌǉǘǝ),\n(8)\n3.5 Training Strategy and Optimization\nThe KG-LLM is trained in two stages. The first stage is supervised instruction tuning using\nannotated dental records, guideline-derived question-answer pairs, and synthetic pediatric\ninfection scenarios generated through self-instruct pipelines. The second stage applies\nReinforcement Learning from Human Feedback (RLHF) to correct hallucinations and\nenhance clinical reliability.\nThe RL objective follows a standard advantage-weighted framework:\nƶƼƶƲư =−Ư푎∼ؼش[퐴푑ǚ(푎)],\n(9)\nwhere the reward model incorporates correctness, safety, and guideline adherence.\nThrough multi-stage optimization, the KG-LLM develops strong semantic understanding,\nclinically aligned reasoning ability, and robust medication safety judgment.\n4. Experiment\n4.1 Dataset Preparation\nThe dataset used in this study is composed of multi-source pediatric dental clinical records\ncollected from partner dental hospitals, pediatric oral health clinics, and publicly available\nde-identified medical text repositories. All data were fully anonymized following HIPAA and\nGDPR guidelines, ensuring the removal of personal identifiers, timestamps, and institutional\ntags before preprocessing. The dataset reflects real-world pediatric dental workflows,\ncovering diagnostic descriptions, radiographic interpretations, antibiotic\nprescriptions,\ntreatment plans, and follow-up notes. This heterogeneous data environment provides a rich\nfoundation for developing a Knowledge-Guided Large Language Model (KG-LLM) capable\nof both semantic understanding and safe antibiotic recommendation.\nEach clinical record includes several major components essential for downstream modeling.\nThe chief complaint section typically contains short natural-language descriptions provided\nby parents or dentists, such as “intermittent spontaneous tooth pain” or “facial swelling for\ntwo days.” The clinical examination notes include structured and unstructured text describing\ncaries progression, pulp vitality testing, sinus tract presence, periodontal findings, and\nocclusion conditions. A significant subset also contains radiographic reports, written either\nmanually by clinicians or generated via speech-to-text transcripts, describing periapical\nradiolucency, widening of periodontal ligament space, or furcation involvement. These\n"}, {"page": 7, "text": "textual records contain valuable features for symptom extraction, diagnosis classification, and\nseverity estimation.\nThe dataset additionally incorporates prescription records, which form the basis of the\nantibiotic safety recommendation task. For each pediatric patient, clinicians documented the\nprescribed antibiotic (e.g., amoxicillin, clindamycin), dosage, frequency, duration, and any\nmodifications during follow-up visits. To support safety constraints, the dataset also links\neach case with clinical guideline references, such as American Academy of Pediatric\nDentistry\n(AAPD)\nrecommendations,\nantimicrobial\nstewardship\nguidelines,\nand\ncontraindication tables for children with comorbidities such as asthma, cardiac defects, or\ndrug\nallergies.\nThese\nknowledge\nelements\nwere\nfurther\naligned\nwith\na\ncurated\ndental-pharmacological knowledge graph, which encodes drug–condition interactions, dosage\nrules, contraindications, and age-specific restrictions.\nIn terms of scale, the dataset contains approximately 32,000 pediatric dental visit records,\nwith about 18,500 including complete radiographic transcripts and 7,200 containing\nhigh-quality prescription histories. On average, each visit record contains around 180–260\nwords of free-text clinical descriptors, along with structured entries such as ICD-10\ndiagnostic codes, antibiotic class labels, allergy status, and age-group identifiers. The final\nprocessed dataset includes a vocabulary of nearly 38,000 unique medical terms, enhanced\nthrough domain-specific tokenization and linked to 1,200 nodes and 5,600 edges in the\nconstructed dental-pharmacological knowledge graph.\nTogether, these diverse data sources allow the KG-LLM to learn fine-grained clinical\nsemantics while grounding its generation process in verified antibiotic safety knowledge. The\ndataset’s richness in longitudinal prescriptions, diagnostic variability, and pediatric-specific\nconstraints ensures that the model develops robust capabilities for both record understanding\nand clinically safe antibiotic recommendation.\n4.2 Experimental Setup\nAll experiments were conducted using the pediatric dental clinical dataset described in\nSection 4.1, following a strict data anonymization and preprocessing pipeline to ensure\ncompliance with medical data regulations. The dataset was randomly divided into training\n(70%), validation (15%), and testing (15%) sets at the patient level to avoid information\nleakage across visits. Textual records, including clinical notes, radiographic descriptions, and\nprescription\nentries,\nwere\nnormalized\nthrough domain-specific\ntokenization,\nmedical\nabbreviation expansion, and negation detection. Knowledge graph entities were aligned with\ntext spans using a hybrid string-matching and embedding-based linker, enabling integration\ninto the KG-LLM framework. The model was implemented using PyTorch and HuggingFace\nTransformers, trained on four NVIDIA A100 GPUs for 100 epochs with a batch size of 16\nand a learning rate of 2e-5. Retrieval components were built using FAISS-based dense vector\nsearch, and knowledge-grounding modules were jointly optimized with the language model\nthrough the multi-task objective. Baseline models included BioGPT, ClinicalBERT,\nMed-PaLM 2, and a Llama-2 Clinical fine-tuning variant without knowledge integration.\n4.3 Evaluation Metrics\nTo comprehensively assess the system’s ability to understand pediatric dental records and\nrecommend safe antibiotics, we employed metrics that evaluate both semantic understanding\nand clinical decision reliability. For the record-understanding task, accuracy, F1-score, and\nBLEU were used to measure the quality of extracted clinical concepts and the coherence of\ngenerated diagnostic summaries. For the antibiotic recommendation component, Top-1 and\nTop-3\nrecommendation\naccuracy\nwere\nused\nto\ncapture\npredictive\nprecision,\nwhile\nsafety-oriented metrics such as Contraindication Violation Rate (CVR), Dosage Error Rate\n(DER), and Guideline Compliance Score (GCS) evaluated the model’s ability to avoid unsafe\nprescriptions and adhere to pediatric infectious disease guidelines. Additionally, the\nExplainability Alignment Score (EAS) was introduced to quantify how well the model’s\nreasoning traces aligned with expert-annotated explanations. All metrics were computed on\nthe held-out test set and statistically validated through bootstrapped confidence intervals.\n"}, {"page": 8, "text": "4.4 Results\nTable 1 summarizes the outcome variables of the different models evaluated in this\nexperiment. Compared to existing models (ClinicalBERT, BioGPT, Med-PaLM-2, Llama-2\nClinical),\nthe\nproposed\nKG-LLM\ndemonstrates\nimproved\npediatric\ndental\nrecord\nunderstanding (F1: 0.914 vs. 0.867), higher drug–dose–duration recommendation accuracy\n(Top-1: 0.782 vs. 0.716), and a lower unsafe antibiotic suggestion rate (CVR: 0.042 vs.\n0.084). Overall, the proposed model achieves a 50% reduction in potentially harmful outputs.\nThe system also attains superior performance in summary quality (BLEU), Top-3 accuracy,\nand the global clinical safety index, collectively confirming its ability to generate accurate\nand guideline-adherent recommendations.\nTable 1. Main Results of Pediatric Dental Record Understanding and Antibiotic\nRecommendation\nModel\nRecord\nUnderstanding\nF1\nSummary\nBLEU\nTop-1\nAccuracy\nTop-3\nAccuracy\nCVR\nDER\nGCS\nClinicalBERT\n0.812\n23.5\n0.641\n0.796\n0.124\n0.087\n0.743\nBioGPT\n0.846\n28.1\n0.672\n0.821\n0.109\n0.071\n0.781\nMed-PaLM 2\n0.874\n32.4\n0.708\n0.854\n0.091\n0.054\n0.824\nLlama-2\nClinical (no KG)\n0.867\n31.9\n0.716\n0.857\n0.084\n0.048\n0.836\nProposed\nKG-LLM\n(ours)\n0.914\n38.7\n0.782\n0.904\n0.042\n0.019\n0.906\nThe ablation study demonstrates that each component contributes meaningfully to system\nperformance. Removing the knowledge graph leads to a sharp drop in safety indicators, with\nCVR rising to 0.083. Eliminating the retrieval mechanism reduces semantic richness in the\ngenerated summaries, lowering BLEU to 34.8. The causal safety controller proves especially\ncritical for antibiotic correctness; without it, the model’s dosage and contraindication error\nrates nearly double. The full KG-LLM achieves the best performance across all metrics,\nconfirming that knowledge grounding, retrieval augmentation, and causal safety constraints\njointly enhance clinical reliability (As shown in Table 2).\nTable 2. Ablation Study of KG-LLM Components.\nModel Variant\nRecord ư1\nBLEU\nTop-1\nAccuracy\nCVR\nDER\nGCS\nKG-LLM w/o\nKG (text only)\n0.871\n32.1\n0.731\n0.083\n0.047\n0.842\nKG-LLM w/o\nRetrieval (no\nRAG)\n0.884\n34.8\n0.748\n0.067\n0.036\n0.861\nKG-LLM w/o\nCausal Safety\nModule\n0.901\n36.9\n0.764\n0.058\n0.029\n0.873\nFull KG-LLM\n(ours)\n0.914\n38.7\n0.782\n0.042\n0.019\n0.906\n"}, {"page": 9, "text": "Figure 2. Corresponding training curve.\nThe training loss curve (As shown in Figure 2) illustrates the optimization dynamics of the\nproposed Knowledge-Guided Large Language Model (KG-LLM) designed for pediatric\ndental record understanding and safe antibiotic recommendation. At the beginning of training\n(epoch 1), the loss exceeds 10, reflecting the model’s initial difficulty in jointly learning\nstructured clinical semantics, extracting pathological features, and aligning them with\nantibiotic safety constraints derived from the integrated dental–pharmacology knowledge\ngraph. As training progresses, the loss consistently decreases but exhibits natural fluctuations,\nwhich is expected in complex multimodal, knowledge-integrated LLM training where textual,\ngraph-based, and retrieval-augmented signals interact. By epoch 30, the loss drops below 4,\nindicating that the model is beginning to correctly interpret pediatric endodontic terminology,\ninfection severity markers, and medication-related contraindications.\nBetween epochs 50 and 80, the curve shows moderate oscillations between 2.0–1.6, driven\nby the KG-LLM’s adaptation to fine-grained constraints such as weight-based dosing rules\nand age-restricted antibiotic usage. After approximately epoch 90, the loss steadily converges\naround 1.3, demonstrating stable learning of the cross-modal reasoning tasks required for\ngenerating clinically safe and guideline-consistent antibiotic recommendations. The final\nconvergence suggests that the KG-LLM effectively integrates knowledge-graph guidance and\ndental domain semantics, achieving reliable performance on this specialized medical NLP\ntask.\n4.5 Discussion\nThe experimental findings strongly demonstrate the value of integrating structured medical\nknowledge with large language models for pediatric dental applications. Unlike purely\ntext-based\nLLMs,\nthe\nKG-LLM\nframework\nbenefits\nfrom\nexplicit\ngrounding\nin\npharmacological guidelines and dental diagnostic ontologies, enabling the model not only to\nextract\nclinical\ninformation\nmore\naccurately\nbut\nalso\nto\navoid\nunsafe\nantibiotic\nrecommendations. The significant reduction in contraindication and dosage errors shows that\nknowledge-guided reasoning is essential when dealing with vulnerable pediatric populations.\nThe\nimprovements\nobserved\nin\nthe\nablation\nanalysis\nfurther\nconfirm\nthat\neach\ncomponent—knowledge\ngraph\nintegration,\nretrieval augmentation,\nand\ncausal\nsafety\nmodeling—plays a complementary role. These results highlight that large language models,\nwhen properly constrained by domain knowledge, can move beyond pattern recognition and\ntoward clinically trustworthy decision support. Future work may expand the model to\nmultimodal radiographic inputs or integrate real-time clinician feedback loops to further\nenhance reliability and adoption in pediatric dentistry.\n"}, {"page": 10, "text": "6. Conclusions\nThis study presents a Knowledge-Guided Large Language Model (KG-LLM) framework\ndesigned to automatically interpret pediatric dental clinical records and generate safety-aware\nantibiotic recommendations. Pediatric dental informatics faces persistent challenges arising\nfrom unstructured narrative notes, incomplete radiographic descriptions, and strict drug-safety\nconstraints associated with children’s rapidly changing physiology. Traditional clinical\ndecision support systems often fail to integrate heterogeneous data sources or to ensure\nguideline-compliant dosing, prompting the need for a more intelligent and interpretable\nsolution. By integrating a domain-specific knowledge graph, retrieval-augmented generation,\nand a dual-layer safety verification mechanism, the proposed KG-LLM advances the\ncapabilities of dental AI systems in both diagnostic understanding and therapeutic safety.\nThrough comprehensive experiments on 32,000 de-identified pediatric dental visit records,\nthe KG-LLM demonstrates strong and consistent performance gains over a domain-adapted\nLlama-2 clinical baseline. The model achieves notable improvements in pediatric dental\nrecord understanding (F1: 0.914 vs. 0.867), drug–dose–duration recommendation accuracy\n(Top-1: 0.782 vs. 0.716), and unsafe antibiotic suggestion rate (CVR: 0.042 vs. 0.084),\nrepresenting a 50% reduction in potentially harmful outputs. Furthermore, the system attains\nsuperior scores in summary quality (BLEU), Top-3 accuracy, and a global clinical safety\nindex, collectively validating its ability to generate both accurate and guideline-adherent\nrecommendations.\nThe\nablation\nstudies\nfurther\nconfirm\nthe\nimportance\nof\neach\ncomponent—knowledge graph guidance, RAG-based evidence retrieval, and causal safety\nchecking—highlighting their synergistic contribution toward safer antibiotic decision-making\nin pediatric dentistry.\nThe findings of this research carry significant clinical and practical implications. For\npediatric dentists and dental clinics, the KG-LLM provides a scalable tool capable of\ninterpreting complex multimodal clinical records while reducing the risk of inappropriate or\nunsafe antimicrobial prescriptions. For healthcare institutions, it offers a transparent and\nevidence-grounded framework that can complement existing CDSS infrastructures and\nsupport antimicrobial stewardship initiatives. More broadly, this study demonstrates the\nfeasibility of combining structured domain knowledge with large language models to build\nclinically interpretable, safety-oriented AI systems for high-risk medical scenarios.\nDespite the important findings, this study has some limitations, such as [the variable\nreliability of external data sources and the differing levels of evidence quality across these\nsources. The model also has limited ability to recognize when essential patient information is\nmissing and appropriately request clarification from the provider. In addition, it must\ncontinually adapt to the ongoing updates in medical research to ensure its knowledge remains\ncurrent.] Future research could further explore [strategies to better assess and weight\nheterogeneous evidence sources and develop mechanisms to ensure timely, reliable\nknowledge updates while strengthening the model’s ability to detect information gaps and\nprompt provider input.]\nIn conclusion, this study [evaluated the antibiotic recommendations produced by the\nproposed KG-LLM against multiple baseline models using de-identified pediatric EDRs. The\nresults\ndemonstrated\nthat\nthe\nKG-LLM\nachieved\nsuperior\nperformance\nin\nrecord\nunderstanding, recommendation accuracy, and safety compliance. The ablation study\nconfirmed the contribution of each model component, and the training loss curve showed\nstable learning behavior necessary for generating safe, guideline-adherent antibiotic\nrecommendations. Overall, this proposed model offers new insights into AI-assisted record\ninterpretation and clinical decision-making, supporting improved clinical outcomes in the\nmanagement of acute pediatric dental infections.]\nReferences\n[1] Lee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedical language representation\nmodel for biomedical text mining[J]. Bioinformatics, 2020, 36(4): 1234-1240.\n"}, {"page": 11, "text": "[2] Alsentzer\nE,\nMurphy\nJ,\nBoag\nW,\net\nal.\nPublicly\navailable\nclinical\nBERT\nembeddings[C]//Proceedings of the 2nd clinical natural language processing workshop. 2019:\n72-78.\n[3] Luo R, Sun L, **a Y, et al. BioGPT: generative pre-trained transformer for biomedical\ntext generation and mining[J]. Briefings in bioinformatics, 2022, 23(6): bbac409.\n[4] Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge[J].\nNature, 2023, 620(7972): 172-180.\n[5] Mohammad-Rahimi H, Motamedian S R, Rohban M H, et al. Deep learning for caries\ndetection: a systematic review[J]. Journal of Dentistry, 2022, 122: 104115.\n[6] Pethani F, Dunn A G. Natural language processing for clinical notes in dentistry: a\nsystematic review[J]. Journal of Biomedical Informatics, 2023, 138: 104282.\n[7] Alhaidry H M, Fatani B, Alrayes J O, et al. ChatGPT in dentistry: a comprehensive\nreview[J]. Cureus, 2023, 15(4).\n[8] Bodenreider O. The unified medical language system (UMLS): integrating biomedical\nterminology[J]. Nucleic acids research, 2004, 32(suppl_1): D267-D270.\n[9] Stearns M Q, Price C, Spackman K A, et al. SNOMED clinical terms: overview of the\ndevelopment process and project status[C]//Proceedings of the AMIA Symposium. 2001: 662.\n[10] Wishart D S, Knox C, Guo A C, et al. DrugBank: a knowledgebase for drugs, drug\nactions and drug targets[J]. Nucleic acids research, 2008, 36(suppl_1): D901-D906.\n[11] Liu W, Zhou P, Zhao Z, et al. K-bert: Enabling language representation with knowledge\ngraph[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(03):\n2901-2908.\n[12] Zhou C, Cai C P, Huang X T, et al. TarKG: a comprehensive biomedical knowledge\ngraph for target discovery[J]. Bioinformatics, 2024, 40(10): btae598.\n[13] Bosselut A, Rashkin H, Sap M, et al. COMET: Commonsense transformers for\nautomatic knowledge graph construction[J]. arxiv preprint arxiv:1906.05317, 2019.\n[14] Lewis\nP,\nPerez\nE,\nPiktus\nA,\net\nal.\nRetrieval-augmented\ngeneration\nfor\nknowledge-intensive nlp tasks[J]. Advances in neural information processing systems, 2020,\n33: 9459-9474.\n[15] **ong G, ** Q, Lu Z, et al. Benchmarking retrieval-augmented generation for\nmedicine[C]//Findings of the Association for Computational Linguistics ACL 2024. 2024:\n6233-6251.\n[16] Fleming-Dutra K E, Hersh A L, Shapiro D J, et al. Prevalence of inappropriate\nantibiotic prescriptions among US ambulatory care visits, 2010-2011[J]. Jama, 2016, 315(17):\n1864-1873.\n[17] Sakagianni A, Koufopoulou C, Feretzakis G, et al. Using machine learning to predict\nantimicrobial resistance―a literature review[J]. Antibiotics, 2023, 12(3): 452.\n[18] Düvel J A, Lampe D, Kirchner M, et al. An AI-Based Clinical Decision Support System\nfor Antibiotic Therapy in Sepsis (KINBIOTICS): Use Case Analysis[J]. JMIR Human Factors,\n2025, 12(1): e66699.\n[19] Karobari M I, Parveen A, Mirza M B, et al. Root and root canal morphology\nclassification systems[J]. International Journal of Dentistry, 2021, 2021(1): 6682189.\n"}, {"page": 12, "text": "[20] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical\nimage\nsegmentation[C]//International\nConference\non\nMedical\nimage\ncomputing\nand\ncomputer-assisted intervention. Cham: Springer international publishing, 2015: 234-241.\n"}]}