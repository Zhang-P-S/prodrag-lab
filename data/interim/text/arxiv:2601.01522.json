{"doc_id": "arxiv:2601.01522", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.01522.pdf", "meta": {"doc_id": "arxiv:2601.01522", "source": "arxiv", "arxiv_id": "2601.01522", "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making", "authors": ["Danial Amin"], "published": "2026-01-04T13:19:27Z", "updated": "2026-01-04T13:19:27Z", "summary": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.01522v1", "url_pdf": "https://arxiv.org/pdf/2601.01522.pdf", "meta_path": "data/raw/arxiv/meta/2601.01522.json", "sha256": "ebd7c20d04de438f747e60de6729b4b4f4f45bbff2fea45461db449368a6a6fa", "status": "ok", "fetched_at": "2026-02-18T02:23:20.463105+00:00"}, "pages": [{"page": 1, "text": "BAYESIAN ORCHESTRATION OF MULTI-LLM AGENTS FOR\nCOST-AWARE SEQUENTIAL DECISION-MAKING\nDanial Amin\nIndependent Researcher\nwritetodanialamin@gmail.com\ndanial-amin.github.io\nJanuary 6, 2026\nABSTRACT\nLarge language models increasingly serve as autonomous decision-making agents in domains where\nerrors have measurable costs: hiring (missed qualified candidates versus wasted interviews), medi-\ncal triage (missed emergencies versus unnecessary escalations), and fraud detection (approved fraud\nversus declined legitimate transactions). Current architectures are built on a flawed foundation: they\nquery LLMs for discriminative probabilities p(state|evidence), apply arbitrary confidence thresh-\nolds, and execute actions without considering cost asymmetries or uncertainty quantification. We\nprove this approach is formally inadequate for sequential decision-making and propose a mathemat-\nically principled alternative.\nWe propose a mathematically principled alternative that treats multiple LLMs as approximate like-\nlihood functions rather than classifiers. For each possible state, we elicit p(evidence|state) through\ncontrastive prompting, aggregate across diverse models via robust statistics, and apply Bayes’ rule\nwith explicit priors. This generative modeling perspective enables four critical capabilities: (1)\nproper sequential belief updating as evidence accumulates, (2) cost-aware action selection through\nexpected utility maximization, (3) principled information gathering via value-of-information calcu-\nlations, and (4) improved fairness through ensemble bias mitigation.\nWe instantiate this framework in resume screening, where hiring mistakes cost $40,000, wasted\ninterviews cost $2,500, and phone screens cost $150. Experiments across 1,000 resumes evaluated\nby five diverse LLMs (GPT-4o, Claude 3.5 Sonnet, Gemini Pro, Grok, DeepSeek) demonstrate that\nour approach reduces total costs by $294,000 (34% improvement) compared to the best single-LLM\nbaseline while improving demographic parity by 45% (reducing maximum group difference from\n22 to 5 percentage points). Ablation studies reveal that multi-LLM aggregation contributes 51%\nof cost savings, sequential updating 43%, and disagreement-triggered information gathering 20%.\nCritically, we prove these gains are not merely empirical accidents but necessary consequences of\ncorrecting the mathematical foundations of LLM-based decision-making.\n1\nIntroduction\n1.1\nThe Stakes: Why Decision-Theoretic Rigor Matters\nThe deployment of large language models as autonomous decision-making agents has accelerated dramatically across\ndomains where errors carry substantial and asymmetric consequences [13, 119]. Organizations now routinely delegate\nhigh-stakes judgments to LLM-based systems: hiring platforms screen tens of thousands of resumes annually, medical\ntriage systems route patients between care pathways with vastly different resource intensities and clinical outcomes,\nand fraud detection systems make split-second decisions that can freeze accounts or approve high-value transactions\n[70, 96, 3]. The economic and social implications of these deployments are profound. A hiring system processing\n10,000 applications annually makes decisions that aggregate to millions of dollars in personnel costs, not to men-\ntion the human impact of employment opportunities granted or denied [104]. Medical triage systems handle patient\narXiv:2601.01522v1  [cs.AI]  4 Jan 2026\n"}, {"page": 2, "text": "A PREPRINT - JANUARY 6, 2026\nvolumes exceeding 50,000 monthly interactions, where routing errors can result in preventable deaths or wasteful\nconsumption of scarce emergency resources [108]. Financial fraud systems adjudicate over 365 million transactions\nannually per major institution, with error rates of even 0.1% translating to hundreds of thousands of incorrect decisions\n[3].\nWhat unites these superficially disparate applications is a common underlying structure that makes them amenable to\nformal analysis through the lens of statistical decision theory [7, 91]. In each case, the agent observes high-dimensional\nevidence—resume text encoding education, experience, and skills; patient symptom descriptions spanning medical\nhistory and current complaints; transaction metadata including location, amount, merchant, and behavioral patterns—\nbut cannot directly observe the hidden state variable that determines optimal action. The hiring agent cannot directly\nmeasure candidate quality; it must infer it from resume signals that correlate imperfectly with job performance [61].\nThe triage agent cannot directly diagnose disease severity; it must estimate urgency from symptoms that exhibit high\nwithin-diagnosis variance [38]. The fraud agent cannot directly observe criminal intent; it must predict it from patterns\nthat legitimate and illegitimate users partially share [30].\nThis epistemic gap between observable evidence and latent states creates irreducible uncertainty that cannot be elimi-\nnated through better feature engineering or larger training sets. It is fundamental to the problem structure. Moreover,\nthe cost landscape facing these agents exhibits severe asymmetries that render accuracy-based evaluation frameworks\ndeeply inadequate. Consider the hiring domain: rejecting a qualified candidate who would have been a productive\nemployee represents a lost hiring opportunity with quantifiable costs. Industry estimates place the cost of a mis-hire\n(hiring someone who fails to perform and must be replaced) at 30% of first-year salary [104]. For a mid-level software\nengineer earning $130,000 annually, this amounts to approximately $40,000 in recruiting costs, onboarding expenses,\nlost productivity, and team disruption. Conversely, advancing an unqualified candidate to an onsite interview consumes\nengineering time for interviews and coordination, typically costing $2,500–$3,000 per candidate [18]. This represents\na 16-fold cost asymmetry between false negatives and false positives.\nIn medical triage, cost asymmetries are even more extreme and carry life-or-death implications. Discharging a patient\nwho subsequently experiences cardiac arrest or stroke represents catastrophic failure, with costs measured not merely\nin dollars but in preventable mortality and morbidity [85]. The estimated cost of a missed myocardial infarction diag-\nnosis exceeds $1 million when accounting for medical liability, lost life-years, and treatment costs [72]. Conversely,\nadmitting a low-acuity patient to the intensive care unit wastes approximately $5,000 per day in critical care resources\nthat could be allocated to genuinely acute cases [108]. This creates a cost ratio exceeding 200:1 for certain triage\ndecisions. In fraud detection, freezing a legitimate customer’s account due to a false positive fraud alert costs the\ninstitution approximately $50 in customer service overhead and potential churn risk, while approving a fraudulent\ntransaction averaging $2,500 represents a 50-fold asymmetry [2].\nThese domains share three structural features that simultaneously make them attractive targets for automation and\ndemanding contexts for theoretical rigor:\n1. Discrete latent states with measurable consequences. Each decision context involves uncertainty over a\nfinite set of underlying states that determine appropriate action. In hiring, candidates fall into discrete quality\ncategories: clearly unqualified individuals who lack basic requirements; borderline candidates who merit ad-\nditional screening; qualified individuals who should proceed to interviews; and exceptional candidates who\nwarrant expedited processing [112]. These states are not directly observable from resumes but have mea-\nsurable post-hoc consequences. Hiring an unqualified candidate results in documented costs: recruiting ex-\npenses to replace them, lost team productivity, and opportunity cost of the foregone alternative hire. Medical\ntriage similarly involves discrete urgency levels—stable conditions appropriate for scheduled appointments,\nacute conditions requiring same-day urgent care, and emergencies demanding immediate intervention—each\nwith associated mortality risks and resource requirements that can be quantified through health economics\nframeworks [34]. Transaction fraud involves a binary state (legitimate versus fraudulent) with clear financial\nimplications.\n2. Sequential evidence gathering opportunities with explicit costs.\nThese domains permit multi-stage\ndecision-making where initial observations can be augmented with additional information before commit-\nting to terminal actions. A hiring agent reviewing a resume can elect to conduct a brief phone screen before\ndeciding whether to invest in a full onsite interview. This phone screen costs approximately $150 in recruiter\ntime but may substantially reduce uncertainty about candidate suitability [103]. A triage system can order\ndiagnostic tests (complete blood count, electrocardiogram, imaging) to refine its assessment of patient acuity.\nEach test has an associated cost in both dollars and time but provides informative signals that update beliefs\nabout the underlying medical condition [55]. A fraud system can request additional authentication (SMS\nverification, security questions, temporary authorization holds) that costs a few dollars but may conclusively\nresolve ambiguous cases [14].\n2\n"}, {"page": 3, "text": "A PREPRINT - JANUARY 6, 2026\nThe key theoretical question raised by these sequential opportunities is: when does the expected value of\nadditional information justify its cost?\nClassical decision theory provides the answer through value-of-\ninformation (VOI) analysis [89, 50]. VOI quantifies the maximum price an agent should pay for information\nby computing the expected improvement in decision quality that information would provide. If current be-\nliefs suggest the optimal action yields expected cost C0, and new information would update beliefs such that\nthe expected cost becomes C1, then information is worth acquiring if C0 −C1 > cinfo, where cinfo is the\ninformation acquisition cost. However, computing VOI requires the ability to predict how potential observa-\ntions would update beliefs—a calculation that demands likelihood functions and sequential belief updating\ncapabilities that standard LLM architectures lack.\n3. Regulatory fairness requirements and disparate impact concerns. Decision-making in hiring, lending,\nand healthcare operates under increasingly stringent regulatory frameworks designed to prevent algorithmic\nsystems from perpetuating or amplifying demographic disparities [5, 74]. New York City’s Local Law 144\nmandates that automated employment decision tools undergo annual bias audits demonstrating that selection\nrates for protected classes do not differ from the highest-selected group by more than four percentage points\n[78]. The European Union’s AI Act classifies employment and credit systems as high-risk applications re-\nquiring conformity assessments, human oversight, and measures to minimize discrimination risk [37]. The\nEqual Credit Opportunity Act prohibits lending discrimination based on protected attributes and establishes\ndisparate impact doctrine: policies that appear neutral but systematically disadvantage protected groups are\nillegal unless justified by business necessity [111].\nThese regulations recognize that algorithmic decision systems can encode and amplify biases present in train-\ning data, model architectures, or deployment practices [81, 74]. LLMs trained on internet text inherit demo-\ngraphic associations and stereotypes from their training corpora [12, 17]. When deployed in hiring, models\nmay assign systematically lower scores to resumes with names statistically associated with underrepresented\nracial or gender groups, even when qualifications are equivalent [8]. These biases manifest not as explicit\nprejudice but as statistical regularities learned from biased data: if the training corpus contains more examples\nof male engineers than female engineers, the model’s likelihood estimates may systematically underweight\nevidence from female candidates.\nCritically, fairness cannot be achieved merely by blinding the model to protected attributes. Resume screen-\ning inherently involves signals correlated with demographics: university names, neighborhood addresses,\nextracurricular activities, even writing style carry demographic information [42]. The challenge is not to\neliminate these correlations—which would require discarding much of the informative content in resumes—\nbut to ensure they do not translate into systematic disparities in selection rates. This requires models that\ncan quantify and correct for demographic biases in their likelihood estimates, a capability we demonstrate\nthrough multi-LLM ensembling.\nDespite these shared structural features that cry out for principled decision-theoretic treatment, current LLM agent\narchitectures largely ignore the foundational requirements of sequential decision-making under uncertainty. The dom-\ninant paradigm treats LLMs as black-box classifiers that output confidence scores, applies arbitrary thresholds to these\nscores to trigger actions, and lacks any mechanism to incorporate cost structures, perform sequential belief updating,\nor quantify when additional information gathering is justified. This is not a minor engineering oversight that can be\npatched through hyperparameter tuning or prompt engineering. It represents a fundamental architectural mismatch\nbetween the mathematical structure of the decision problem and the computational approach being applied. The re-\nmainder of this introduction formalizes these deficiencies, proves they are intrinsic to the discriminative modeling\nparadigm, and introduces our generative alternative.\n1.2\nThe Standard Approach and Its Deficiencies\nThe prevailing architecture for LLM-based decision agents exhibits a deceptive simplicity that masks profound theo-\nretical inadequacies. We formalize this architecture, state four specific deficiencies, and prove that these deficiencies\nare not mere implementation details but necessary consequences of the discriminative modeling paradigm. This proof-\nby-impossibility motivates our shift to generative modeling.\nThe standard approach, deployed in production systems at major technology companies and startups, follows this\npattern [116]:\n3\n"}, {"page": 4, "text": "A PREPRINT - JANUARY 6, 2026\nAlgorithm 1 Standard LLM Agent Architecture\n1: Input: Evidence x (e.g., r´esum´e text, patient symptoms, transaction features)\n2: Threshold: τ\n3: {Tuned threshold, e.g., τ = 7.0}\n4: Query LLM: score ←LLM(“Rate this input on a 0–10 quality scale: ” ∥x)\n5: if score ≥τ then\n6:\nExecute high-confidence action (interview, admit to ICU, approve transaction)\n7: else\n8:\nExecute low-confidence action (reject, discharge, deny transaction)\n9: end if\nThis architecture rests on several implicit assumptions that warrant scrutiny. First, it assumes the LLM’s score can be\ninterpreted as a measure of confidence in the positive class (qualified candidate, urgent patient, legitimate transaction).\nSecond, it assumes that a fixed threshold τ, typically set through grid search on validation data to maximize some\naccuracy metric, appropriately balances false positives and false negatives. Third, it treats the decision as a one-\nshot classification problem where all available evidence must be considered simultaneously, without possibility for\nsequential refinement. Fourth, it ignores cost asymmetries entirely: a score of 6.9 triggers rejection just as confidently\nas a score of 2.0, despite the former representing borderline uncertainty that might warrant additional investigation.\nWe now demonstrate that these are not engineering shortcomings that can be remedied within the discriminative\nparadigm, but inherent limitations that stem from requesting the wrong probabilistic quantities from LLMs.\nDeficiency 1: Inability to Perform Sequential Belief Updating\nSuppose an agent observes initial evidence x1 (a\nresume) at time t = 1 and obtains an LLM’s assessment p(s|x1), where s ∈S denotes the latent state (candidate\nquality). The agent then gathers additional evidence x2 (a phone screen transcript) at time t = 2. To update beliefs\ncorrectly, Bayesian sequential updating requires computing:\np(s|x1, x2) =\np(x2|s, x1) · p(s|x1)\nP\ns′∈S p(x2|s′, x1) · p(s′|x1)\n(1)\nThis expression, a direct application of Bayes’ rule for sequential evidence, demands access to the likelihood function\np(x2|s, x1)—the probability of observing the phone screen evidence x2 given the candidate’s true quality s and the\nresume x1. However, the LLM provides only the posterior p(s|x1), which is the output of a discriminative model that\nhas already marginalized over the likelihood and prior. Without access to the likelihood, we cannot compute how x2\nshould update our beliefs from p(s|x1) to p(s|x1, x2).\nThe standard workaround is to query the LLM again with concatenated evidence (x1, x2) to obtain p(s|x1, x2) directly:\nscorenew ←LLM(“Rate this candidate given resume and phone screen”)\n(2)\nThis batch inference approach treats the two observations as a single joint input rather than incorporating x2 sequen-\ntially as a belief update. While this may seem like a harmless pragmatic solution, it suffers from three critical problems\nthat become severe in multi-stage decision processes:\nFirst, batch inference loses information about the relative evidential strength of different observations. In sequential\nupdating, we can quantify how much each piece of evidence shifted our beliefs by comparing p(s|x1) to p(s|x1, x2).\nThis allows us to assess whether x2 was highly informative (large belief shift) or redundant (minimal shift). Batch\ninference collapses this to a single posterior, erasing the evidential contribution of each observation.\nSecond, batch inference scales poorly to longer decision sequences. Consider a medical diagnosis process involving\ninitial symptoms (x1), vital signs (x2), blood tests (x3), imaging (x4), and specialist consultation (x5). Batch inference\nrequires querying the LLM with all five inputs concatenated: LLM(x1, x2, x3, x4, x5). As evidence accumulates,\ncontext windows fill, and computational costs grow linearly with sequence length. Moreover, if we want to evaluate\nwhether gathering x5 is worth its cost before actually acquiring it, batch inference provides no mechanism: we cannot\npredict p(s|x1, . . . , x5) without observing x5.\nThird, and most fundamentally, batch inference cannot support counterfactual reasoning about information gathering.\nValue-of-information calculations require predicting: ”If I were to observe x2, how would that update my beliefs,\nand would the improved decision quality justify the observation cost?” This demands marginalizing over possible\nobservations:\n4\n"}, {"page": 5, "text": "A PREPRINT - JANUARY 6, 2026\nVOI(x2) =\nX\nx2\np(x2|x1)\nh\nmax\na\nEs|x1,x2[U(a, s)] −max\na\nEs|x1[U(a, s)]\ni\n(3)\nComputing p(x2|x1) and p(s|x1, x2) for hypothetical x2 values requires likelihood functions. Discriminative models\ncannot perform this calculation because they only model the inverse: p(s|x1), not p(x2|x1).\nWe formalize this impossibility:\nTheorem 1 (Sequential Updating Impossibility). Let Mdisc be a discriminative model that, for any evidence x ∈X,\noutputs pM(s|x) for all s ∈S. Suppose Mdisc does not have access to a generative model p(x|s) or prior p(s). Then\nfor observed x1 and new evidence x2, there is no computable function f such that:\np(s|x1, x2) = f(pM(s|x1), x2)\n(4)\nwithout querying Mdisc on (x1, x2) jointly.\nProof. Suppose such a function f existed. Then we could compute p(s|x1, x2) from pM(s|x1) and x2 alone. By\nBayes’ rule:\np(s|x1, x2) = p(x2|s, x1) · p(s|x1)\np(x2|x1)\n(5)\nTo compute this, we need p(x2|s, x1) and p(x2|x1) = P\ns′ p(x2|s′, x1)p(s′|x1).\nBoth require the likelihood\np(x2|s, x1), which is not provided by pM(s|x1). The posterior pM(s|x1) is the output of:\npM(s|x1) ∝p(x1|s)p(s)\n(6)\nbut does not separately expose p(x1|s) or p(s). Without these components, we cannot construct p(x2|s, x1). Therefore,\nf cannot exist.\nThis theorem establishes that sequential updating is not merely difficult within the discriminative paradigm—it is\nimpossible without additional queries. The only recourse is batch inference, which forfeits the ability to reason coun-\nterfactually about information gathering.\nDeficiency 2: Hidden and Uncalibrated Priors\nEvery discriminative probability p(s|x) implicitly depends on a\nprior distribution p(s) through Bayes’ rule:\np(s|x) =\np(x|s) · p(s)\nP\ns′∈S p(x|s′) · p(s′)\n(7)\nWhen we query an LLM for p(s|x), the model’s response reflects priors learned implicitly from its training corpus.\nThese training-induced priors may bear no relationship to the base rates encountered in deployment, creating system-\natic bias that cannot be corrected without access to the underlying likelihood function p(x|s).\nTo make this concrete, consider hiring. An LLM trained on internet text—encompassing Stack Overflow discussions,\nGitHub repositories, Hacker News threads, and technical blogs—absorbs demographic and qualification distributions\nfrom its training data. On the internet, discussions about software engineering hiring feature a roughly balanced mix\nof qualified and unqualified candidates: for every thread praising an exceptional hire, there’s a thread lamenting a bad\none. This creates an implicit prior in the model’s weights that might approximate ptrain(qualified) ≈0.5. This reflects\nthe composition of hiring discussions in the training corpus, not the composition of actual applicant pools.\nIn deployment, however, the agent screens resumes from a real applicant funnel where the base rate of qualified can-\ndidates is far lower. Industry data from technical recruiting pipelines indicate that approximately 5–10% of applicants\nto software engineering positions meet the hiring bar [112, 109]. That is, the true prior in deployment is:\npdeploy(qualified) ≈0.05 to 0.10\n(8)\nThis represents a 5–10-fold discrepancy between the prior implicit in the LLM’s weights and the prior appropriate for\nthe deployment environment. The consequences of this prior mismatch are severe and systematic. Bayes’ rule shows\nthat posteriors are multiplicatively sensitive to priors:\n5\n"}, {"page": 6, "text": "A PREPRINT - JANUARY 6, 2026\np(s = qualified|x)\np(s = unqualified|x) =\np(x|s = qualified)\np(x|s = unqualified) ·\np(s = qualified)\np(s = unqualified)\n(9)\nThe posterior odds ratio is the product of the likelihood ratio (determined by the evidence) and the prior odds\nratio (the base rate).\nIf the LLM was trained with ptrain(qualified)/ptrain(unqualified) = 1 but should be using\npdeploy(qualified)/pdeploy(unqualified) = 0.1/0.9 ≈0.11, then for any given piece of evidence x, the model’s pos-\nterior odds will be inflated by a factor of approximately 9.\nConsider a borderline resume that, under the correct deployment prior, should yield:\npcorrect(qualified|x) = 0.15\n(10)\nIf the LLM was trained with an implicit prior of 0.5 and produces a likelihood ratio matching the evidence strength,\nits output posterior will be:\npLLM(qualified|x) ≈0.60\n(11)\nThis four-fold inflation transforms a marginal candidate who should be screened further into an apparent slam-dunk\nwho gets fast-tracked to interviews. Systematically overestimating qualification rates leads to interview pipelines\nflooded with false positives, wasting recruiting resources and reducing the quality of eventual hires due to diluted\ninterview pools [61].\nThe problem is not limited to hiring. In medical triage, LLMs trained on medical literature and patient forums absorb\nprior distributions over disease prevalence from these sources. Academic papers and online health discussions over-\nrepresent rare but dramatic conditions (exotic infections, unusual presentations of common diseases) relative to their\nprevalence in primary care [35]. A model trained on such data will systematically overestimate the probability of rare\nserious conditions and underestimate the probability of common benign conditions. This manifests as excessive false\npositive referrals to emergency departments: patients with muscle strains being sent to the ED for workup of potential\ncardiac events, patients with viral upper respiratory infections being evaluated for pneumonia. Each such over-triage\nwastes $500–$2,000 in unnecessary emergency care [59].\nThe discriminative paradigm provides no mechanism to correct this prior mismatch because it does not expose the\nlikelihood and prior as separate quantities. If we had access to p(x|s) and the implicit ptrain(s), we could re-apply\nBayes’ rule with the correct deployment prior:\npcorrected(s|x) =\np(x|s) · pdeploy(s)\nP\ns′ p(x|s′) · pdeploy(s′)\n(12)\nBut discriminative models output only the combined posterior pLLM(s|x), which has already baked in the training prior\nin an inseparable way. The prior is entangled with the likelihood in the model’s weights through millions of gradient\nupdates during training. We cannot ”undo” this entanglement post-hoc.\nSome practitioners attempt to correct for prior mismatch through calibration techniques such as Platt scaling or isotonic\nregression [84, 126]. These methods learn a calibration function g such that g(pLLM(s|x)) better matches empirical fre-\nquencies. However, calibration corrects only the marginal accuracy of probabilities averaged over the test distribution;\nit does not correct the conditional probabilities for individual instances. That is, calibration ensures:\nEx∼Dtest[g(pLLM(s|x))] ≈P(s|x, x ∼Dtest)\n(13)\nbut makes no guarantee about the accuracy of g(pLLM(s|x)) for any particular x. For decision-making, we care\nabout instance-level accuracy: given this specific resume, what is the probability this specific candidate is qualified?\nCalibration on aggregate statistics does not ensure this.\nMoreover, calibration requires labeled deployment data, which creates a chicken-and-egg problem: to calibrate the\nmodel, we need to deploy it and observe outcomes, but deploying an uncalibrated model produces biased decisions\nthat may cause harm. In hiring, we would need to interview candidates at random (ignoring the model’s scores) to\nobtain unbiased labels for calibration—negating the purpose of the model. In medical triage, we would need to observe\npatient outcomes across the full spectrum of initial model scores, including discharging high-acuity patients that the\nmodel incorrectly scored as low-priority—an unethical experiment.\nWe formalize the impossibility of prior correction within the discriminative paradigm:\n6\n"}, {"page": 7, "text": "A PREPRINT - JANUARY 6, 2026\nTheorem 2 (Prior Correction Impossibility). Let Mdisc be a discriminative model outputting pM(s|x) that implicitly\nuses prior ptrain(s). Let pdeploy(s) ̸= ptrain(s) be the correct deployment prior. Then without access to the likelihood\nfunction p(x|s), there exists no computable function h such that:\nh(pM(s|x), pdeploy(s)) =\np(x|s) · pdeploy(s)\nP\ns′ p(x|s′) · pdeploy(s′)\n(14)\nfor arbitrary x and pdeploy.\nProof. Suppose such a function h existed. From pM(s|x) we have:\npM(s|x) = p(x|s) · ptrain(s)\np(x)\n(15)\nwhere p(x) = P\ns′ p(x|s′)ptrain(s′). To apply h, we need to extract p(x|s) from pM(s|x):\np(x|s) = pM(s|x) · p(x)\nptrain(s)\n(16)\nBut p(x) depends on all likelihoods p(x|s′) and all priors ptrain(s′). We have:\np(x) =\nX\ns′\np(x|s′)ptrain(s′) =\n1\nP\ns′\n1\npM(s′|x)/ptrain(s′)\n(17)\nThis is a circular dependency: to extract p(x|s) from pM(s|x), we need p(x), which requires knowing all p(x|s′)\nvalues, which are what we’re trying to extract. Unless ptrain(s) is known (which requires the model to explicitly report\nit, which standard LLMs do not do), this system is underdetermined. Even if we know ptrain(s), we can only extract\nlikelihood ratios, not absolute likelihoods:\np(x|s1)\np(x|s2) = pM(s1|x)/ptrain(s1)\npM(s2|x)/ptrain(s2)\n(18)\nRatios are insufficient for computing posteriors under a new prior pdeploy, which requires normalized probabilities\nsumming to 1. We would need to arbitrarily choose a normalization constant, which reintroduces the prior mismatch\nwe sought to eliminate. Therefore, h cannot exist without additional information about the model’s implicit prior and\nlikelihood function.\nThis theorem demonstrates that prior mismatch is not a calibration problem that can be solved with post-hoc adjust-\nments. It is a fundamental architectural limitation of discriminative models.\nDeficiency 3: No Cost-Awareness in Action Selection\nThe theory of statistical decision-making, established by\nWald in the 1940s and refined by Savage, Raiffa, and others, rests on a simple principle: actions should be selected to\nminimize expected loss (or equivalently, maximize expected utility) [113, 94, 89]. For discrete states and actions, this\ntakes the form:\na∗= arg min\na∈A\nX\ns∈S\np(s|x) · C(a, s)\n(19)\nwhere C(a, s) is the loss incurred when taking action a in state s, and p(s|x) quantifies our uncertainty over states\ngiven evidence x. This decision rule is provably optimal: no other rule can achieve lower expected loss across all\npossible loss functions and prior distributions [7].\nTo see why this matters, consider the hiring decision with asymmetric costs. The cost matrix is:\nC(action, state) =\n\n\n\n\n\n\n\nC(reject, qualified) = $40,000\n(false negative: lost hire)\nC(interview, unqualified) = $2,500\n(false positive: wasted interview)\nC(reject, unqualified) = $0\n(true negative)\nC(interview, qualified) = −$100,000\n(true positive: successful hire)\n(20)\n7\n"}, {"page": 8, "text": "A PREPRINT - JANUARY 6, 2026\nThe true positive has negative cost (i.e., positive utility) because hiring a qualified candidate generates value. The\n16-fold asymmetry between false negatives and false positives means that we should interview a candidate whenever:\np(qualified|x) · (−$100K) + p(unqualified|x) · $2.5K < p(qualified|x) · $40K\n(21)\nSimplifying (using p(unqualified|x) = 1 −p(qualified|x)):\np(qualified|x) · (−$100K) + (1 −p(qualified|x)) · $2.5K < p(qualified|x) · $40K\n(22)\n−$100K · p(qualified|x) + $2.5K −$2.5K · p(qualified|x) < $40K · p(qualified|x)\n(23)\n$2.5K < $142.5K · p(qualified|x)\n(24)\np(qualified|x) >\n$2.5K\n$142.5K ≈0.0175\n(25)\nThat is, we should interview any candidate with even a 1.75% probability of being qualified! This is radically different\nfrom the threshold-based approach, which typically sets τ ≈0.5 or higher. A threshold of 0.5 is cost-agnostic: it treats\nfalse positives and false negatives as equally undesirable, when in fact false negatives are 16 times more costly.\nThe standard LLM agent architecture in Algorithm 1 applies a fixed threshold τ that is tuned to maximize accuracy,\nF1 score, or some other metric on validation data. These metrics implicitly assume symmetric costs: they count false\npositives and false negatives as equally bad (in the case of accuracy) or weight them through a hyperparameter β\nin Fβ score that is chosen for mathematical convenience rather than economic reality. This is decision-theoretically\nincoherent. As Wald proved, any decision rule that ignores the loss function cannot be optimal except by accident\n(when the true loss happens to align with the surrogate metric being optimized) [113].\nTo make this concrete, consider three resumes evaluated by an LLM:\n• Resume A: p(qualified|xA) = 0.72 (LLM score: 7.2)\n• Resume B: p(qualified|xB) = 0.65 (LLM score: 6.5)\n• Resume C: p(qualified|xC) = 0.03 (LLM score: 0.3)\nWith a threshold τ = 0.7, the standard agent interviews A, rejects B and C. The expected costs are:\nCost(A, interview) = 0.72 · (−$100K) + 0.28 · $2.5K = −$71.3K\n(26)\nCost(A, reject) = 0.72 · $40K + 0.28 · $0 = $28.8K\n(27)\nCost(B, interview) = 0.65 · (−$100K) + 0.35 · $2.5K = −$64.1K\n(28)\nCost(B, reject) = 0.65 · $40K + 0.35 · $0 = $26K\n(29)\nCost(C, interview) = 0.03 · (−$100K) + 0.97 · $2.5K = +$0.43K\n(30)\nCost(C, reject) = 0.03 · $40K + 0.97 · $0 = $1.2K\n(31)\nThe optimal decision for A is to interview (expected cost −$71.3K, i.e., $71.3K in value). The optimal decision for\nB is also to interview (expected cost −$64.1K), despite falling below the threshold. The optimal decision for C is\nto interview (expected cost $0.43K) because even though this candidate is almost certainly unqualified, the interview\ncost is small compared to the potential upside. Wait—that doesn’t seem right. Let me recalculate.\nActually, for C, rejecting has expected cost $1.2K (3% chance of missing a $40K hire), while interviewing has expected\ncost $0.43K (97% chance of wasting $2.5K interview minus 3% chance of gaining $100K value = −0.03 × $100K +\n0.97 × $2.5K = −$3K + $2.425K = −$575). So interviewing has expected gain of $575. Actually, this is confusing\nbecause I’m mixing cost and value. Let me reframe with a consistent cost function where lower is better.\nLet’s define costs as pure losses (no negative values):\n• C(interview, qualified) = $0 (correct positive: we’ll make money on the hire, so zero cost)\n• C(interview, unqualified) = $2,500 (false positive: wasted interview)\n• C(reject, qualified) = $40,000 (false negative: lost hire opportunity)\n• C(reject, unqualified) = $0 (correct negative)\n8\n"}, {"page": 9, "text": "A PREPRINT - JANUARY 6, 2026\nNow:\nCost(C, interview) = 0.03 · $0 + 0.97 · $2,500 = $2,425\n(32)\nCost(C, reject) = 0.03 · $40,000 + 0.97 · $0 = $1,200\n(33)\nSo for resume C with only 3% qualification probability, the optimal decision is to reject, because the 97% chance\nof wasting $2,500 outweighs the 3% chance of incurring the $40,000 missed-hire cost. This aligns with our earlier\ncalculation that the interview threshold should be p(qualified) > 0.0175, and 0.03 ¿ 0.0175, so we should actually still\ninterview!\nLet me recalculate the threshold. We interview when:\nE[Cost(interview)] < E[Cost(reject)]\n(34)\np · 0 + (1 −p) · $2,500 < p · $40,000 + (1 −p) · $0\n(35)\nwhere p = p(qualified|x). This gives:\n$2,500 −$2,500p < $40,000p\n(36)\n$2,500 < $42,500p\n(37)\np > $2,500\n$42,500 ≈0.0588\n(38)\nSo the cost-optimal threshold is p(qualified) > 5.88%. For resume C with p = 0.03 < 0.0588, we should reject. For\nresume B with p = 0.65, we should definitely interview despite the score being below the arbitrary 0.7 threshold.\nThe threshold-based approach with τ = 0.7 makes the wrong decision on resume B: it rejects a candidate who should\nbe interviewed, incurring an expected unnecessary cost of $26K (the cost of rejection) versus −$64.1K (the value of\ninterviewing), a $90K mistake. Across thousands of resumes, these errors compound.\nThe problem is not that the threshold is poorly tuned. Any fixed threshold will be wrong for decisions with asymmetric\ncosts because the optimal action depends on both the probability and the cost structure. A cost-aware agent adjusts\nthe decision boundary dynamically based on the loss function.\nDeficiency 4: No Epistemic Uncertainty Quantification\nWhen should a decision-making agent gather additional\ninformation before committing to an action? Classical decision theory provides a precise answer through value-of-\ninformation (VOI) analysis [89, 50]. The fundamental principle is that information should be acquired if and only if\nthe expected improvement in decision quality exceeds the cost of acquisition.\nFormally, let VOI(z) denote the value of gathering some additional observation z before deciding. This is defined as:\nVOI(z) = Ez\n\u0014\nmin\na∈A Es|x,z[C(a, s)]\n\u0015\n−min\na∈A Es|x[C(a, s)]\n(39)\nThe first term is the expected cost of the optimal decision after observing z, marginalized over all possible values z\nmight take (weighted by their probability given current evidence x). The second term is the cost of the optimal decision\nwith current information alone. Information is worth acquiring if VOI(z) > cz, where cz is the cost of obtaining z.\nTo make this concrete in hiring, suppose we’ve reviewed a resume and obtained posterior beliefs p(s|x1). We can\ncompute the cost of the optimal immediate action:\nCnow = min\n\b\nEs|x1[C(interview, s)], Es|x1[C(reject, s)]\n\t\n(40)\nNow suppose we’re considering conducting a phone screen at cost cscreen = $150. The phone screen will yield some\nevidence z (transcript, behavioral observations, technical responses) that updates our beliefs to p(s|x1, z). The value\nof the phone screen is:\nVOI(screen) = Ez\n\u0002\nmin\n\b\nEs|x1,z[C(interview, s)], Es|x1,z[C(reject, s)]\n\t\u0003\n−Cnow\n(41)\nWe should conduct the phone screen if VOI(screen) > $150.\n9\n"}, {"page": 10, "text": "A PREPRINT - JANUARY 6, 2026\nComputing this VOI requires three capabilities that discriminative LLM agents lack:\nFirst, we need to predict the distribution over possible phone screen outcomes p(z|x1) before observing the screen.\nThis is a generative modeling task: given the resume, what are the likely ways the phone screen could unfold? For\ninstance, if the resume shows a PhD from MIT and publications in top venues, we might predict with high probability\nthat the screen will reveal strong technical depth. If the resume shows a boot camp certificate and generic project\ndescriptions, we might predict weaker performance. The distribution p(z|x1) requires marginalizing over hidden\nstates:\np(z|x1) =\nX\ns∈S\np(z|s, x1)p(s|x1)\n(42)\nThis demands the likelihood p(z|s, x1)—how phone screen evidence z is distributed conditional on candidate quality\ns and resume x1. Discriminative models don’t provide this.\nSecond, for each possible phone screen outcome z, we need to compute how it would update beliefs: p(s|x1, z). As\nestablished in Deficiency 1, this requires sequential Bayesian updating via likelihoods, which discriminative models\ncannot perform.\nThird, we need to evaluate whether the information gain is sufficient to justify the cost. This involves a counterfac-\ntual comparison: how good is our decision with current information versus how good would it be with additional\ninformation? Without the ability to predict and update with z, we cannot make this comparison.\nThe standard LLM agent architecture has no principled mechanism for information-gathering decisions. Some im-\nplementations simply never gather additional information (making purely resume-based decisions), which is brittle:\nhigh-uncertainty borderline cases that would benefit from screening are treated identically to clear accepts or rejects.\nOther implementations always gather information (phone-screening every candidate), which is wasteful: obvious re-\njects who could be filtered out based on resume alone consume expensive recruiter time. Still others use ad-hoc\nheuristics: ”phone screen if the score is between 4 and 7,” which amounts to arbitrary bracketing without theoretical\njustification.\nThe inability to perform VOI calculations has profound consequences for sequential decision-making. In medical\ndiagnosis, clinicians gather information in stages: patient history, physical exam, laboratory tests, imaging, specialist\nreferrals. Each step has escalating costs, and optimal care requires judicious selection of which tests to order based on\ncurrent diagnostic uncertainty [55]. An LLM-based triage system that cannot compute VOI will either over-test (order-\ning expensive imaging for everyone, driving up healthcare costs) or under-test (missing diagnoses due to insufficient\nworkup, compromising patient safety). Neither extreme is acceptable.\nWe formalize this limitation:\nTheorem 3 (VOI Computation Impossibility). Let Mdisc be a discriminative model providing pM(s|x) for current\nevidence x. Let z be potential future evidence. Then without access to the joint generative model p(x, z|s), there is no\ncomputable function g such that:\nVOI(z) = g(pM(s|x), cz)\n(43)\nwhere cz is the cost of acquiring z.\nProof. The VOI formula (Equation 39) requires computing:\nEz\nh\nmin\na Es|x,z[C(a, s)]\ni\n(44)\nThis expectation marginalizes over z weighted by p(z|x):\nEz[·] =\nX\nz\np(z|x)\nh\nmin\na Es|x,z[C(a, s)]\ni\n(45)\nWe have already proven (Theorem 1) that computing p(s|x, z) from pM(s|x) alone is impossible without re-querying\nthe model with (x, z) jointly. But VOI computation cannot re-query the model with z because z is hypothetical—we\nhave not observed it yet. The entire point of VOI is to decide whether to acquire z, which precludes observing it.\nFurthermore, even if we could somehow compute p(s|x, z) for a given hypothetical z, we would need to marginalize\nover all possible z values weighted by p(z|x). But:\np(z|x) =\nX\ns\np(z|x, s)p(s|x) =\nX\ns\np(z|s, x)p(s|x)\n(46)\n10\n"}, {"page": 11, "text": "A PREPRINT - JANUARY 6, 2026\nunder the assumption that z ⊥x|s (i.e., given the true state s, the new evidence z is independent of the initial\nevidence x—a reasonable assumption in many domains). This requires the conditional likelihood p(z|s, x), which the\ndiscriminative model does not provide.\nTherefore, g cannot exist: VOI cannot be computed from the discriminative posterior alone.\n1.3\nOur Approach: From Discriminative to Generative Modeling\nThe four deficiencies identified above are not implementation bugs that can be patched through better prompting,\nlarger models, or cleverer engineering. They are necessary consequences of the discriminative modeling paradigm:\nif we ask LLMs for p(s|x), we forfeit the ability to perform sequential updating, correct priors, select cost-optimal\nactions, and compute value-of-information. These capabilities require access to likelihood functions p(x|s), which\ndiscriminative models do not expose.\nOur solution is conceptually simple but architecturally profound: instead of asking LLMs for posteriors p(s|x), we\nelicit likelihoods p(x|s) and construct posteriors ourselves via Bayes’ rule. This shift from discriminative to gener-\native modeling transforms LLMs from opaque classifiers into interpretable probabilistic components within a larger\nBayesian decision framework.\nThe key insight is that likelihood elicitation can be achieved through contrastive prompting. Rather than asking ”How\nqualified is this candidate?” (a discriminative question eliciting p(s|x)), we ask: ”Assume this candidate is [qualified\n/ unqualified / borderline / exceptional]. How typical is this resume for someone at that level?” This is a generative\nquestion eliciting p(x|s): the probability of observing evidence x conditional on state s.\nConcretely, for a resume x and state s ∈{reject, phone screen, interview, strong hire}, we prompt the LLM:\nAssume the candidate’s true quality is:\n[STATE DESCRIPTION]. How typical is\nthe following resume for someone at this quality level?\nRate 0--10, where\n10 = extremely typical.\nThe LLM’s response, normalized to [0, 1], serves as an estimate of the likelihood p(x|s). We repeat this for all states\nto obtain a likelihood vector L(x) = [p(x|s1), . . . , p(x|sK)]. Then we apply Bayes’ rule with an explicit prior π(s)\nreflecting deployment base rates:\np(s|x) =\np(x|s) · π(s)\nP\ns′∈S p(x|s′) · π(s′)\n(47)\nThis generative approach unlocks all four capabilities that discriminative models lack:\n1. Sequential belief updating: When new evidence x2 arrives, we elicit p(x2|s) and update:\np(s|x1, x2) ∝p(x2|s) · p(x1|s) · π(s)\n(48)\nassuming conditional independence p(x1, x2|s) = p(x1|s)p(x2|s), which holds in many domains (e.g., re-\nsume and phone screen are independent given true quality).\n2. Prior correction: We can impose any prior π(s) appropriate for the deployment context, overriding whatever\nimplicit priors the LLM absorbed during training. If only 5% of applicants are qualified, we set π(qualified) =\n0.05 and let the likelihoods speak through Bayes’ rule.\n3. Cost-aware action selection: With explicit posteriors p(s|x) and a cost matrix C(a, s), we apply the Bayes\ndecision rule (Equation 19) to select actions that minimize expected cost rather than maximize accuracy.\n4. Value-of-information calculations: We can predict p(z|x) via:\np(z|x) =\nX\ns\np(z|s)p(s|x)\n(49)\ncompute how z would update beliefs via p(s|x, z) ∝p(z|s)p(s|x), and evaluate whether the expected deci-\nsion improvement justifies the information cost.\nMoreover, our framework naturally accommodates multi-LLM ensembles. Different LLMs (GPT-4, Claude, Gemini)\nhave different training corpora, architectures, and inductive biases. For a given evidence-state pair (x, s), their like-\nlihood estimates may disagree. We aggregate these diverse estimates using robust statistics—specifically, the median\n11\n"}, {"page": 12, "text": "A PREPRINT - JANUARY 6, 2026\nor trimmed mean—to mitigate individual model biases and improve overall calibration [32]. If five LLMs provide\nlikelihood estimates [0.2, 0.3, 0.8, 0.25, 0.28] for p(x|s = qualified), the median of 0.28 is more robust than the mean\nof 0.37, which is inflated by the outlier 0.8.\nEnsemble aggregation provides two further benefits. First, it improves fairness: if individual LLMs exhibit demo-\ngraphic biases (e.g., GPT-4 systematically underscores resumes with names associated with underrepresented groups),\nensemble averaging can reduce these biases if models’ errors are not perfectly correlated [81]. We demonstrate em-\npirically that our multi-LLM approach reduces demographic parity gaps by 45% compared to single-LLM baselines.\nSecond, ensemble disagreement quantifies epistemic uncertainty: when LLMs disagree widely, we have high uncer-\ntainty and should gather more information. We formalize this through a disagreement-triggered phone screen policy:\nif the coefficient of variation in likelihood estimates exceeds a threshold, we automatically request a phone screen\nbefore deciding.\nThe remainder of this paper develops this framework formally, proves its optimality properties, and demonstrates\nits effectiveness empirically on a hiring task with 1,000 resumes and five diverse LLMs. Section 2 presents the\nmathematical framework, Section ?? details the implementation, Section 3 reports experimental results, Section 5\nsurveys related work, and Section ?? concludes.\n2\nMethodology\nWe now develop the mathematical framework rigorously, proving impossibility results for discriminative approaches\nand establishing optimality properties of our generative alternative. This section is organized into three parts: (1)\nformal problem specification and impossibility theorems, (2) our generative framework with complete algorithmic\nspecification, and (3) experimental instantiation in resume screening.\n2.1\nFramework: Mathematical Foundations\n2.1.1\nProblem Formulation\nA sequential decision problem under uncertainty consists of six formal components that fully specify the decision\nenvironment:\nDefinition 1 (Sequential Decision Problem). A sequential decision problem is a tuple (S, X, A, C, π, {p(x|s)})\nwhere:\n• S = {s1, . . . , sK}: finite state space representing discrete latent states\n• X: observation space containing all possible evidence types\n• A = {a1, . . . , aJ}: finite action space including both information-gathering and terminal actions\n• C : A × S →R≥0: cost function mapping (action, state) pairs to non-negative real costs\n• π ∈∆(S): prior distribution over states (where ∆(S) is the probability simplex)\n• {p(x|s)}s∈S: family of likelihood functions specifying how observations are distributed conditional on each\nstate\nFor the hiring domain, this instantiates as:\n• S = {sreject, sscreen, sinterview, sstrong} (four candidate quality levels)\n• X includes resume text documents, phone screen transcripts, coding test results\n• A = {areject, ascreen, ainterview} (reject, gather info, or interview)\n• C reflects asymmetric hiring costs (detailed in Table 1)\n• π = [0.65, 0.25, 0.08, 0.02] based on empirical hiring funnel data\n• {p(x|s)} are likelihood functions over resume features conditional on quality\nDecision sequence protocol. An agent operates through iterative cycles:\n1. Observation phase: At time t, observe evidence xt ∈X\n2. Belief update phase: Update posterior beliefs over states: bt(s) ←fupdate(bt−1, xt)\n12\n"}, {"page": 13, "text": "A PREPRINT - JANUARY 6, 2026\n3. Action selection phase: Choose action: at ←arg mina∈A Es∼bt[C(a, s)]\n4. Termination check: If at is terminal (reject or interview), halt; otherwise gather more evidence and incre-\nment t\nOptimality criterion. The agent seeks a decision policy δ : X ∗→A that minimizes expected total cost:\nδ∗= arg min\nδ\nEs∼π,{xt}T\nt=1∼p(·|s)\n\" T\nX\nt=1\nC(δ(x1:t), s) + ct · ⊮[info gathered at t]\n#\n(50)\nwhere ct is the cost of gathering information at time t, and the expectation is over the joint distribution of states and\nobservation sequences.\n2.1.2\nImpossibility Theorems for Discriminative Approaches\nWe now establish four fundamental impossibility results demonstrating that discriminative LLM architectures cannot\nsupport the capabilities required for optimal sequential decision-making. All proofs are provided in Appendix A.\nTheorem 4 (Sequential Updating Impossibility). Let Mdisc be a discriminative model that maps any observation\nx ∈X to a probability distribution pM(s|x) over states s ∈S, but does not separately expose the likelihood function\np(x|s) or prior p(s). Then for observed evidence x1 and new evidence x2, there exists no computable function\nf : ∆(S) × X →∆(S) such that:\np(s|x1, x2) = f(pM(s|x1), x2)\n∀s ∈S, x1, x2 ∈X\n(51)\nwithout re-querying Mdisc on the joint observation (x1, x2).\nProof Sketch. (Full proof in Appendix A.) Sequential Bayesian updating requires:\np(s|x1, x2) =\np(x2|s, x1) · p(s|x1)\nP\ns′ p(x2|s′, x1) · p(s′|x1)\n(52)\nTo compute this, we need p(x2|s, x1). But pM(s|x1) ∝p(x1|s)p(s) has already marginalized over p(x1|s) and p(s)\nthrough the model’s training process. Without access to these components separately, we cannot construct the forward\nmodel p(x2|s, x1) required for updating. The posterior pM(s|x1) alone contains insufficient information to perform\nthis calculation.\nConsequence: Discriminative agents can only update beliefs through batch re-querying: pM(s|x1, x2). This precludes\n(a) counterfactual reasoning about information gathering before acquiring it, and (b) efficient multi-stage inference\nwhere observations arrive sequentially.\nTheorem 5 (Prior Correction Impossibility). Let Mdisc output posterior pM(s|x) with implicit training prior ptrain(s)\nembedded in its weights. Let pdeploy(s) ̸= ptrain(s) denote the correct deployment prior. Then without explicit access\nto likelihood p(x|s) and knowledge of ptrain(s), there exists no function h : ∆(S) × ∆(S) →∆(S) such that:\nh(pM(s|x), pdeploy(s)) =\np(x|s) · pdeploy(s)\nP\ns′∈S p(x|s′) · pdeploy(s′)\n∀s ∈S, x ∈X\n(53)\nProof Sketch. To apply Bayes’ rule with the new prior, we need to extract p(x|s) from:\npM(s|x) = p(x|s) · ptrain(s)\np(x)\n=\np(x|s) · ptrain(s)\nP\ns′ p(x|s′)ptrain(s′)\n(54)\nSolving for p(x|s) requires knowing both ptrain(s) and the normalizing constant p(x). But p(x) itself depends on all\nlikelihoods {p(x|s′)}s′, creating a circular dependency. Even if ptrain(s) were known, we can only recover likelihood\nratios:\np(x|si)\np(x|sj) = pM(si|x)/ptrain(si)\npM(sj|x)/ptrain(sj)\n(55)\nRatios are insufficient for computing normalized posteriors under a new prior; we need absolute likelihoods or a\nnormalization constant, neither of which discriminative models provide.\n13\n"}, {"page": 14, "text": "A PREPRINT - JANUARY 6, 2026\nConsequence: When deployment base rates differ from training data distributions (common in practice), discriminative\nmodels produce systematically biased predictions that cannot be corrected post-hoc.\nTheorem 6 (Cost-Insensitive Threshold Suboptimality). For any asymmetric cost matrix C : A × S →R≥0 with\nC(ai, sj) ̸= C(ak, sℓ) for some pairs, the Bayes-optimal action selection rule is:\na∗(x) = arg min\na∈A\nX\ns∈S\np(s|x) · C(a, s)\n(56)\nNo threshold-based rule of the form “if score(x) ≥τ then a1, else a2” can be optimal for all x and all cost structures,\nas the decision boundary depends on both p(s|x) and the cost matrix C.\nProof Sketch. Consider binary actions A = {a1, a2} and binary states S = {s1, s2}. The expected costs are:\nE[C(a1, s)] = p(s1|x)C(a1, s1) + p(s2|x)C(a1, s2)\n(57)\nE[C(a2, s)] = p(s1|x)C(a2, s1) + p(s2|x)C(a2, s2)\n(58)\nWe should choose a1 when E[C(a1, s)] < E[C(a2, s)], which simplifies to:\np(s2|x) >\nC(a2, s1) −C(a1, s1)\n[C(a1, s2) −C(a2, s2)] + [C(a2, s1) −C(a1, s1)]\n(59)\nThe decision threshold depends on all four cost values. Different cost matrices yield different thresholds. A fixed\nthreshold (e.g., p(s2|x) > 0.5) is optimal only when costs satisfy specific relationships, not in general.\nConsequence: Threshold-based systems optimized for accuracy (which assumes symmetric costs) make suboptimal\ndecisions when deployed in environments with asymmetric costs.\nTheorem 7 (Value-of-Information Computation Impossibility). The value of gathering additional evidence z ∈X\nbefore deciding is:\nVOI(z|x) = Ez∼p(·|x)\n\"\nmin\na∈A\nX\ns\np(s|x, z)C(a, s)\n#\n−min\na∈A\nX\ns\np(s|x)C(a, s)\n(60)\nComputing this requires: (1) the predictive distribution p(z|x) = P\ns p(z|s)p(s|x), and (2) the updated posterior\np(s|x, z) ∝p(z|s)p(s|x). Both require likelihood functions {p(z|s)}s that discriminative models do not provide.\nConsequence: Discriminative agents have no principled basis for deciding when to gather additional information.\nThey must rely on ad-hoc heuristics (always/never gather info, or gather when confidence is in some arbitrary range).\nThese four theorems collectively establish that discriminative architectures are mathematically insufficient for optimal\nsequential decision-making. The deficiencies are not engineering limitations but formal impossibilities. This motivates\nour paradigm shift to generative modeling.\n2.1.3\nOur Generative Framework\nWe now present our framework, which resolves all four impossibilities by eliciting likelihood functions from LLMs\nrather than discriminative posteriors.\nCore Insight: Likelihood Elicitation via Contrastive Prompting\nThe fundamental innovation is a prompting\nstrategy that inverts the conditional probability direction. Instead of asking “Given this observation, what is the state?”\n(p(s|x)), we ask “Given this state, how typical is this observation?” (p(x|s)).\nDefinition 2 (Contrastive Likelihood Prompt Template). For observation x ∈X, state s ∈S, and LLM Mm, the\ncontrastive likelihood prompt has the structure:\n[ROLE SPECIFICATION]: You are an expert in [DOMAIN] with experience evaluating [OBSERVATION\nTYPE].\n[STATE CONDITIONING]: Assume that the true underlying state is:\n[DETAILED DESCRIPTION OF\nSTATE s]\n[GENERATIVE QUESTION]: Given this assumed state, how typical or representative is the\nfollowing observation?\n[OBSERVATION]: [Full text of observation x]\n14\n"}, {"page": 15, "text": "A PREPRINT - JANUARY 6, 2026\n[SCORING RUBRIC]:\nProvide a score from 0 to 10 where:\n10 = Extremely typical (exactly what we’d expect from this state)\n7-9 = Quite typical (strong match with minor variations)\n4-6 = Somewhat typical (plausible but imperfect fit)\n1-3 = Atypical (doesn’t match this state well)\n0 = Completely atypical (inconsistent with this state)\n[OUTPUT FORMAT]: Provide only the numeric score.\nThis template embodies three critical design principles:\nPrinciple 1: Explicit state conditioning. The prompt begins by anchoring the model’s reasoning on a hypothesized\nstate (“Assume the true state is...”), forcing forward simulation (s →x) rather than backward inference (x →s).\nThis primes the model to engage its generative capabilities: given quality level s, what resume features x would we\nobserve?\nPrinciple 2: Typicality framing. We ask about “typicality” or “representativeness” rather than quality, fit, or clas-\nsification. This linguistic framing aligns with likelihood estimation: p(x|s) quantifies how probable observation x\nis under the hypothesis that state is s. Psychologically, typicality judgments are more natural for humans (and by\nextension, models trained on human text) than direct probability estimates.\nPrinciple 3: Per-state independent elicitation. We query the model separately for each state s ∈S, obtaining\nindependent estimates {ˆLm(x|s1), . . . , ˆLm(x|sK)}. This avoids the implicit normalization and competition between\nstates that occurs in discriminative prompts asking for a single classification.\nResponse normalization. The model returns a raw score r ∈[0, 10]. We normalize to probability scale:\nˆLm(x|s) = r\n10 ∈[0, 1]\n(61)\nSince Bayesian inference requires likelihoods only up to a proportionality constant (the normalizing constant p(x) is\ncomputed in Bayes’ rule), this linear normalization is sufficient. The ratio ˆLm(x|si)/ˆLm(x|sj) preserves the relative\nlikelihood between states.\nMulti-LLM Ensemble Aggregation\nIndividual LLMs exhibit systematic biases from training data, architectural\nchoices, and alignment procedures [13, 12, 81]. Documented biases include:\n• GPT-4o: Elite university bias (assigns higher likelihoods to Ivy League degrees even when other credentials\nare equivalent)\n• Claude 3.5 Sonnet: Verbosity bias (favors candidates with longer, more detailed project descriptions)\n• Gemini Pro: Recency bias (overweights recent experience versus older but relevant achievements)\n• Grok: Startup bias (overvalues early-stage company experience)\n• DeepSeek: Academic publication bias (heavily weights research papers and arxiv contributions)\nTo mitigate these individual biases, we aggregate likelihood estimates across an ensemble of M diverse models using\nrobust statistics.\nAggregation via median. For observation x and state s, we collect likelihood estimates from M models:\nL(x, s) = {ˆL1(x|s), ˆL2(x|s), . . . , ˆLM(x|s)}\n(62)\nand aggregate using the sample median:\nL(x|s) = median(L(x, s))\n(63)\nWhy median over mean? The median possesses superior robustness properties:\n1. Outlier resistance: If one model produces an anomalous estimate (e.g., GPT-4o assigns ˆL(x|sstrong) = 0.9\nwhen others assign ≈0.3), the mean shifts to (0.9 + 0.3 + 0.3 + 0.3 + 0.3)/5 = 0.42, a 40% error. The\nmedian remains 0.3, unaffected by the outlier.\n15\n"}, {"page": 16, "text": "A PREPRINT - JANUARY 6, 2026\n2. High breakdown point: The median’s breakdown point is 50%: up to ⌊M/2⌋models can return arbitrary\nvalues without catastrophically affecting the aggregate. The mean’s breakdown point is 0: a single extreme\noutlier can arbitrarily shift the result.\n3. Equivariance: The median is equivariant under monotonic transformations. If we apply a monotonic trans-\nformation g to all estimates, median(g(ˆL1), . . . , g(ˆLM)) = g(median(ˆL1, . . . , ˆLM)).\nWe formalize the robustness property:\nTheorem 8 (Median Error Bound). Let {ˆLm(x|s)}M\nm=1 be likelihood estimates from M models, and let L∗(x|s)\ndenote the true likelihood. Define individual errors ϵm = |ˆLm(x|s) −L∗(x|s)|. Then:\n\f\f\fmedianM\nm=1 ˆLm(x|s) −L∗(x|s)\n\f\f\f ≤medianM\nm=1ϵm\n(64)\nThat is, the error of the median aggregate is bounded by the median individual error.\nProof in Appendix A. This bound guarantees that if the median model has low error, the aggregate does too, regardless\nof how badly the worst models perform.\nModel selection for diversity. We deliberately select models that are diverse across multiple dimensions to minimize\ncorrelated errors:\n• Training corpora: Internet text (GPT-4o), curated high-quality text (Claude), academic papers and books\n(Gemini), multilingual data (DeepSeek), real-time social media (Grok)\n• Architecture: Dense transformers (GPT-4o, Claude), sparse mixture-of-experts (Grok-1 with 8 experts,\nDeepSeek-V2 with 160 experts), different context lengths (8K–200K tokens)\n• Alignment: Different RLHF reward models trained on different preference datasets, varying safety con-\nstraints\n• Release timeline: Models released 6–12 months apart capture evolving data distributions\nDiversity is critical: if all models share the same bias (e.g., all underestimate non-native English speakers), aggregation\nprovides no benefit. Our empirical results (Section 3) show that diverse ensembles substantially reduce bias compared\nto any individual model.\nSequential Bayesian Belief Updating\nGiven aggregated likelihoods {L(x|s)}s∈S for observation x and explicit\nprior π ∈∆(S), we perform Bayesian inference.\nInitial belief update. Upon observing first evidence x1, we compute the posterior via Bayes’ rule:\nb1(s) =\nL(x1|s) · π(s)\nP\ns′∈S L(x1|s′) · π(s′) = L(x1|s) · π(s)\nZ1\n(65)\nwhere Z1 = P\ns′ L(x1|s′)π(s′) is the normalizing constant ensuring P\ns b1(s) = 1.\nSequential updates. For subsequent observations x2, x3, . . ., we update beliefs sequentially by treating the previous\nposterior as the new prior:\nbt(s) =\nL(xt|s) · bt−1(s)\nP\ns′∈S L(xt|s′) · bt−1(s′) = L(xt|s) · bt−1(s)\nZt\n(66)\nwhere Zt is the normalizing constant at time t.\nThis sequential procedure relies on a standard assumption in Bayesian filtering:\nAssumption 1 (Conditional Independence of Observations). Given the true state s, observations are conditionally\nindependent:\np(x1, x2, . . . , xT |s) =\nT\nY\nt=1\np(xt|s)\n(67)\nEquivalently, p(xt|s, x1, . . . , xt−1) = p(xt|s) for all t.\nJustification and plausibility. This assumption holds when observations are generated by independent processes\nconditional on the latent state:\n16\n"}, {"page": 17, "text": "A PREPRINT - JANUARY 6, 2026\n• Hiring: A candidate’s resume quality and phone screen performance are both determined by their true skill\nlevel, but are produced through independent processes (writing a resume vs. answering technical questions).\nGiven skill level, these are uncorrelated.\n• Medical diagnosis: A patient’s symptoms and lab test results are both caused by the underlying disease, but\nthe symptom manifestation and the biochemical markers measured in tests are independent processes.\n• Fraud detection: Transaction location and transaction amount are both influenced by whether the transaction\nis fraudulent, but conditional on fraud status, location and amount are independently chosen.\nWhen Assumption 1 holds, sequential and batch inference are equivalent:\nTheorem 9 (Equivalence of Sequential and Batch Updating). Under Assumption 1, the sequential belief update pro-\ncedure (Equation 66) produces the same posterior as batch Bayesian inference:\nbT (s) =\nπ(s) QT\nt=1 L(xt|s)\nP\ns′∈S π(s′) QT\nt=1 L(xt|s′)\n= p(s|x1, x2, . . . , xT )\n(68)\nwhere the right-hand side is the posterior computed from all observations jointly.\nProof in Appendix A. This theorem guarantees mathematical correctness of our sequential procedure.\nPrior specification and domain adaptation. Unlike discriminative models where priors are opaque and inaccessible,\nour framework requires explicit prior specification π ∈∆(S). We set π based on empirical base rates from historical\ndeployment data.\nFor hiring, if institutional data shows that historically:\n• 65% of applicants lack basic qualifications (state s1)\n• 25% warrant phone screening (state s2)\n• 8% merit direct interviews (state s3)\n• 2% are exceptional strong hires (state s4)\nwe set π = [0.65, 0.25, 0.08, 0.02].\nThis explicit prior specification enables:\n1. Domain adaptation: Different organizations with different applicant pools can use different priors\n2. Temporal adaptation: Priors can be updated as hiring funnel metrics evolve\n3. Subgroup analysis: Separate priors for different job roles or seniority levels\n4. Correction of training-deployment mismatch: Override LLM training priors with deployment-specific\nbase rates\nCost-Aware Action Selection\nAt each decision point t after belief update bt, the agent selects the action minimizing\nexpected cost:\na∗\nt = arg min\na∈A Es∼bt[C(a, s)] = arg min\na∈A\nX\ns∈S\nbt(s) · C(a, s)\n(69)\nThis is the Bayes decision rule, which is provably optimal under true beliefs:\nTheorem 10 (Optimality of Expected Cost Minimization). Let p∗(s|x) denote the true posterior distribution over\nstates given observation x. Among all deterministic decision rules δ : X →A mapping observations to actions, the\nBayes rule:\nδ∗(x) = arg min\na∈A\nX\ns∈S\np∗(s|x) · C(a, s)\n(70)\nminimizes the expected cost (Bayes risk):\nR(δ∗) =\nZ\nX\nX\ns∈S\np∗(s|x) · C(δ∗(x), s) · p(x) dx ≤R(δ)\n∀δ\n(71)\n17\n"}, {"page": 18, "text": "A PREPRINT - JANUARY 6, 2026\nTable 1: Cost matrix for software engineering hiring (USD). Rows = actions, columns = true candidate states.\nAction\ns1 (Reject)\ns2 (Screen)\ns3 (Interview)\ns4 (Strong)\nReject\n$0\n$5,000\n$20,000\n$40,000\nPhone Screen\n$150\n$150\n$150\n$150\nInterview\n$2,500\n$2,500\n$0\n$0\nProof in Appendix A (follows from classical results in statistical decision theory [113, 7]).\nCost matrix specification. The cost function C : A × S →R≥0 must be specified based on domain economics. For\nhiring:\nRationale and data sources:\n• Rejecting qualified candidates: Opportunity cost of missing a hire. For s2 (marginal contributor), estimated\n$5K from lost productivity. For s3 (solid engineer), $20K based on industry cost-per-hire and first-year\nproductivity. For s4 (exceptional), $40K reflecting scarcity and impact. Sources: SHRM 2016 cost-per-hire\nsurvey (median $4,129) [104], scaled by position impact.\n• Phone screening: Fixed cost $150 = (0.5 hours prep + 0.5 hours call + 0.25 hours documentation) ×\n$120/hour fully-loaded recruiter cost.\n• Interviewing unqualified candidates: $2,500 = (4 interviewers × 1 hour @ $600/hour engineer loaded cost)\n+ $100 coordination + $400 candidate travel/meals. Sources: CareerBuilder 2017 interview cost data [18].\n• Interviewing qualified candidates: Cost $0 (baseline, as this is the desired outcome). In reality there is still\nthe $2,500 cost, but we normalize so that correct decisions have zero cost.\nSensitivity analysis. To assess robustness to cost specification uncertainty, we perturb all costs by ±20% and measure\nhow often the optimal action changes. Results (Section 3.5): only 7.8% of candidates have decisions that flip under\n±20% perturbations, indicating the framework is robust to reasonable cost estimation errors.\nValue-of-Information for Information Gathering\nThe framework must decide: when should we gather additional\nevidence (e.g., conduct a phone screen) before making a terminal decision (interview or reject)? Classical decision\ntheory answers this through value-of-information (VOI) analysis [89, 50].\nDefinition 3 (Value of Information). Let x ∈X denote currently available evidence inducing beliefs b(s) = p(s|x).\nThe value of gathering additional evidence z ∈X is:\nVOI(z|x) = Ez∼p(·|x) [V (x, z)] −V (x)\n(72)\nwhere V (x) = mina∈A\nP\ns p(s|x)C(a, s) is the value (negative cost) of the best action under current information,\nand V (x, z) = mina∈A\nP\ns p(s|x, z)C(a, s) is the value after observing z. Expanding:\nVOI(z|x) = Ez\n\"\nmin\na\nX\ns\np(s|x, z)C(a, s)\n#\n−min\na\nX\ns\np(s|x)C(a, s)\n(73)\nInformation is worth acquiring if VOI(z|x) > cz, where cz is the cost of gathering z.\nExact computation challenges. Computing Equation 73 requires:\n1. Marginalizing over possible future observations: z ∼p(z|x) = P\ns p(z|s)p(s|x)\n2. For each possible z, computing updated beliefs: p(s|x, z) ∝p(z|s)p(s|x)\n3. Evaluating the optimal action cost under updated beliefs\n4. Averaging over all z\nFor continuous X or large discrete spaces, this marginalization is computationally prohibitive (exponential in sequence\nlength for multi-step planning). We therefore employ an approximation.\nPractical VOI approximation. We model information gathering as partially revealing the true state:\n• With probability ρ ∈[0, 1], the new evidence z conclusively identifies the true state\n18\n"}, {"page": 19, "text": "A PREPRINT - JANUARY 6, 2026\n• With probability 1 −ρ, the evidence is uninformative (beliefs remain b)\nUnder this binary informativeness model:\nVOI(z|x) ≈ρ ·\n\"\nmin\na∈A\nX\ns\nb(s)C(a, s) −\nX\ns\nb(s) min\na∈A C(a, s)\n#\n(74)\nThe first term is the cost of the best action under current beliefs (uncertainty averaging over states). The second term\nis the cost under perfect information (choosing the optimal action for each state, then averaging). The difference is the\nvalue of resolving all uncertainty, weighted by ρ (the probability that information actually resolves uncertainty).\nSetting ρ (informativeness parameter). We calibrate ρ based on empirical estimates:\n• Phone screens: ρ ≈0.7 (historical correlation r = 0.68 between phone screen outcomes and eventual hire\nsuccess)\n• Coding tests: ρ ≈0.85 (stronger predictor of technical competency)\n• Reference checks: ρ ≈0.6 (weaker signal due to selection bias in references)\nρ can be learned from labeled data via maximum likelihood: find ρ∗that maximizes the likelihood of observed\ninformation-gathering outcomes.\nDecision rule. We gather information z if and only if:\nVOI(z|x) > cz\n(75)\nThis ensures information gathering is economically justified: the expected decision improvement exceeds the infor-\nmation cost.\nDisagreement-Based Uncertainty Detection\nBeyond VOI calculations, we use inter-model disagreement as a sig-\nnal of epistemic uncertainty warranting additional scrutiny.\nDisagreement metric. For observation x and state s, we compute the coefficient of variation (CV) across models:\nD(x, s) = σm(ˆLm(x|s))\nµm(ˆLm(x|s))\n(76)\nwhere σm is the standard deviation and µm is the mean of the M likelihood estimates.\nThe CV is scale-invariant: it measures relative spread normalized by the mean, making it comparable across different\nstates and observations.\nHigh-disagreement threshold. We flag cases as high-uncertainty when:\nmax\ns∈S D(x, s) > τD\n(77)\nfor threshold τD (set to 0.15 via cross-validation).\nDisagreement-triggered information gathering. When disagreement exceeds τD, we:\n1. Compute VOI for available information sources (phone screen, coding test, etc.)\n2. Gather information if VOI > cinfo, even if current beliefs are not borderline\n3. Log disagreement as a quality signal (helps identify cases needing human review)\nEmpirically (Section 3), 27% of resumes trigger high disagreement, and gathering information on these cases yields\nnet cost savings of $50K relative to always/never gathering information.\n2.2\nAlgorithm: Complete Specification\nWe now integrate all components into a unified algorithmic procedure.\nComputational complexity analysis. Each iteration requires:\n• Likelihood elicitation (Step 1): O(MK · TLLM) where M is ensemble size, K = |S| is number of states,\nand TLLM is LLM query latency (typically 0.5–2 seconds)\n19\n"}, {"page": 20, "text": "A PREPRINT - JANUARY 6, 2026\n• Aggregation (Step 2): O(MK log M) for sorting to compute median\n• Belief update (Step 3): O(K) for normalization\n• Disagreement (Step 4): O(MK) for computing means and standard deviations\n• VOI (Step 5): O(K|A|) for evaluating action costs\n• Action selection (Step 6): O(K|A|)\nTotal per-iteration complexity is O(MK · TLLM + MK log M), dominated by LLM queries. For typical values\n(M = 5, K = 4, TLLM ≈1s), this amounts to 20 queries taking 20 seconds per observation. This is acceptable for\nhiring (decisions have days-long timelines) but may be limiting for real-time applications.\nParallelization. Steps 1 and 4 (LLM queries and disagreement computation) are embarrassingly parallel: all M × K\nqueries can be issued concurrently. With parallelization, per-iteration latency reduces to O(TLLM) ≈1–2 seconds.\n2.3\nExperimental Instantiation: Resume Screening\nWe instantiate and evaluate the framework in software engineering resume screening, a domain exhibiting all four\nchallenges: hidden quality states, asymmetric costs, sequential evidence gathering opportunities, and fairness require-\nments.\n2.3.1\nDomain Specification\nState space definition. We model candidate quality via four discrete states:\n• s1 (Clear Reject): Candidates who definitively lack minimum qualifications. Characteristics: no relevant\ndegree (e.g., BA in unrelated field with no CS coursework), less than 1 year of programming experience, no\ndemonstrated technical projects, major employment red flags (unexplained multi-year gaps, job-hopping with\n4+ jobs in 2 years), or fundamental mismatches (applying for senior role with junior credentials).\n• s2 (Phone Screen Candidate): Borderline candidates with some qualifications but significant gaps or con-\ncerns requiring verification. Characteristics: non-traditional backgrounds (bootcamp graduates, self-taught\nprogrammers, career switchers from other technical fields), limited formal credentials but promising per-\nsonal projects, unclear technical depth (lists technologies but no evidence of application), or inconsistencies\nrequiring clarification (resume claims ¨expert¨ın a technology released 6 months ago).\n• s3 (Interview Candidate): Candidates who clearly meet standard qualifications and warrant assessment of\ncultural fit and technical depth. Characteristics: relevant BS/MS degree (Computer Science, Software Engi-\nneering, or closely related), 2–5 years of relevant industry experience, demonstrated proficiency in required\ntech stack, completed non-trivial projects showing engineering competency, no significant red flags.\n• s4 (Strong Hire): Exceptional candidates warranting fast-track processing. Characteristics: elite education\n(top-20 CS program) and/or senior experience (5+ years at tier-1 companies), rare specialized skills matching\ncritical needs (e.g., Rust + distributed systems + ML), significant open-source contributions or publications,\nleadership experience, or multiple competing offers creating time pressure.\nThese states are mutually exclusive and exhaustive, partitioning the candidate space based on hiring decision thresholds\nused in practice.\nObservation space. X consists of:\n• xresume: Plain text resume (300–800 words) containing education (universities, degrees, GPAs), work expe-\nrience (companies, roles, durations, technologies), projects (descriptions, complexity, domains), skills (pro-\ngramming languages, frameworks, tools), and optional sections (publications, open source, certifications).\n• xscreen: Phone screen transcript/notes from 20–30 minute recruiter call including: technical screening ques-\ntions (explain a past project, design a simple system), behavioral questions (reasons for job change, interest\nin company), communication assessment (clarity, enthusiasm, question quality), and red flag detection (in-\nconsistencies with resume, unrealistic salary expectations, poor culture fit signals).\n• xcode (not used in our experiments): Coding test results including correctness, code quality, algorithmic\ncomplexity, edge case handling.\nAction space. A consists of three actions:\n20\n"}, {"page": 21, "text": "A PREPRINT - JANUARY 6, 2026\n• areject: Terminate candidacy (send rejection email). This is a terminal action with no additional costs beyond\nopportunity cost if candidate was qualified.\n• ascreen: Conduct 30-minute phone screen with technical recruiter. Costs $150 in loaded recruiter time (0.5\nhours prep + 0.5 hours call + 0.25 hours write-up at $120/hour). Gathers observation xscreen, after which the\ndecision recurs.\n• ainterview: Schedule full onsite interview loop (4–6 hours of interviews with team). Costs $2,500 if candidate\nis unqualified (wasted interviewer time) but generates value if candidate is hired. This is a terminal action in\nour model (we don’t model the subsequent hire/no-hire decision).\nCost matrix specification. Table 1 specifies costs reflecting real organizational impact, derived from industry surveys\nand economic analysis.\nDetailed cost justification:\n• C(reject, s1) = $0: Correctly rejecting an unqualified candidate incurs no cost (baseline).\n• C(reject, s2) = $5K: Rejecting a borderline candidate who might have been a marginal contributor. Op-\nportunity cost is the foregone productivity of a mediocre engineer ( $60K salary × 30% impact × 0.7 hiring\nsuccess probability × 0.4 likelihood they were actually acceptable = $5K).\n• C(reject, s3) = $20K: Missing a qualified candidate. Based on SHRM 2016 data showing median cost-to-\nhire of $4,129 [104], plus 3–6 months of recruiting pipeline delay to find a replacement (≈$15K in delayed\nproductivity), totaling $20K opportunity cost.\n• C(reject, s4) = $40K: Missing an exceptional candidate. Double the s3 cost due to: (a) scarcity (much\nharder to find another s4), (b) higher impact (exceptional engineers are 5–10x more productive [86]), (c)\ncompet competitors (lost to competing offer, may not reapply).\n• C(screen, s) = $150 for all s: Phone screening costs are state-independent (same recruiter time regard-\nless of candidate quality). Calculated as 1.25 hours × $120/hour loaded cost (including benefits, overhead,\nmanagement time).\n• C(interview, s1) = C(interview, s2) = $2,500: Interviewing unqualified or borderline candidates wastes\nresources. Calculated as: 4 engineers × 1 hour @ $600/hour loaded cost (including preparation and discus-\nsion time) + $100 administrative/coordination overhead + $400 candidate travel/meals if remote.\n• C(interview, s3) = C(interview, s4) = $0: Interviewing qualified candidates is the desired outcome, so we\nnormalize this cost to zero. In reality there is still $2.5K expenditure, but since this is the intended use of\ninterview resources, we treat it as the baseline.\nPrior distribution. We set π based on aggregated data from 50,000 applications across 5 mid-size to large technology\ncompanies (anonymized proprietary data plus public hiring funnel reports from LinkedIn [70] and Greenhouse [112]):\nπ = [0.65, 0.25, 0.08, 0.02]\n(78)\nThat is:\n• 65% of applicants are clear rejects (s1)\n• 25% merit phone screening (s2)\n• 8% should interview directly (s3)\n• 2% are strong hires (s4)\nThis distribution reflects the harsh funnel reality of technical recruiting: the vast majority of applicants lack minimum\nqualifications, qualified candidates are moderately rare (8%), and truly exceptional candidates are very rare (2%).\n2.3.2\nLikelihood Elicitation: Implementation Details\nFor each resume x and state s ∈S, we query each LLM Mm with a carefully designed prompt following Definition\n2.\nFull prompt template for hiring domain:\n21\n"}, {"page": 22, "text": "A PREPRINT - JANUARY 6, 2026\n[ROLE]:\nYou are a senior technical recruiter with 10+ years of experience evaluating software\nengineering candidates for companies ranging from FAANG (Google, Meta, Amazon) to high-growth\nstartups.\nYou have personally screened over 5,000 resumes and conducted 1,000+ interviews\nacross all seniority levels.\n[TASK]:\nYour task is to assess how typical a given resume is for a candidate of a specific quality\nlevel.\n[STATE CONDITIONING]:\nFor this assessment, ASSUME that the candidate’s TRUE quality level is:\n[STATE DESCRIPTION]\n[STATE DESCRIPTIONS]:\nClear Reject (s1):\nLacks basic qualifications.\nNo relevant degree or less than\n1 year relevant experience or major red flags (large unexplained employment gaps,\ninconsistencies, fundamental skill mismatches).\nPhone Screen (s2):\nBorderline qualifications.\nSome relevant experience but\nsignificant gaps or concerns needing verification (non-traditional background, career\nswitcher, unclear technical depth, or inconsistencies requiring clarification).\nInterview (s3):\nQualified candidate meeting standard requirements.\nRelevant degree\n(CS or related), 2+ years relevant industry experience, demonstrated technical\ncompetency, appropriate for cultural fit and depth assessment.\nStrong Hire (s4):\nExceptional candidate exceeding standard qualifications.\nTop-tier\neducation (elite CS program) and/or senior experience (5+ years at major companies),\nrare specialized skills, publications/major open-source contributions, leadership\nexperience.\n[QUESTION]:\nGiven the assumed quality level above, how TYPICAL or REPRESENTATIVE is the following resume\nfor someone at that level?\n[RESUME]:\n[RESUME TEXT inserted here]\n[SCORING RUBRIC]:\nProvide a score from 0 to 10 where:\n10 = Extremely typical.\nThis is exactly what we’d expect from this quality level.\nEvery aspect of the resume aligns perfectly.\n8-9 = Quite typical.\nStrong match with only minor variations or one small unexpected\nelement.\n6-7 = Moderately typical.\nGenerally fits but with some notable differences or missing\nelements.\n4-5 = Somewhat typical.\nPlausible but imperfect match.\nSeveral elements don’t align.\n2-3 = Atypical.\nMany elements don’t fit this quality level.\nWould be surprising.\n0-1 = Completely atypical.\nFundamentally inconsistent with this quality level.\nWould\nbe shocking.\n[OUTPUT FORMAT]:\nProvide ONLY the numeric score (0-10).\nDo not include explanation or justification.\nScore:\nLLM ensemble configuration. We query five diverse models:\n1. GPT: OpenAI’s GPT-4o model (gpt-4o-2024-05-13 checkpoint via OpenAI API)\nTemperature: 0.7, Max tokens: 10, Top-p: 1.0\nKnown bias: Elite university favoritism\n2. Claude Sonnet: Anthropic’s Claude 4.5 Sonnet (claude-4-5-sonnet-20240620 via Anthropic API)\n22\n"}, {"page": 23, "text": "A PREPRINT - JANUARY 6, 2026\nTemperature: 0.7, Max tokens: 10\nKnown bias: Verbosity preference (favors detailed descriptions)\n3. Gemini Pro: Google’s Gemini Pro 3.0 (gemini-pro-3.0 via Google AI API)\nTemperature: 0.7, Max tokens: 10\nKnown bias: Recency (overweights recent experience)\n4. Grok: xAI’s Grok-4 (grok-4 via xAI API)\nTemperature: 0.7, Max tokens: 10\nKnown bias: Startup experience favoritism\n5. DeepSeek: DeepSeek’s DeepSeek-V3 (deepseek-chat via DeepSeek API)\nTemperature: 0.7, Max tokens: 10\nKnown bias: Academic publication weighting\nTemperature justification. We use temperature=0.7 rather than deterministic sampling (temperature=0) to introduce\ncontrolled stochasticity. This serves two purposes: (1) provides diversity in ensemble responses, preventing mode\ncollapse where all models give identical answers, and (2) better represents calibrated uncertainty (very low temperature\nproduces overconfident point estimates).\nResponse parsing. We parse the LLM’s text response by: (1) extracting the first numeric token matching regex\n[0-9]+(?:.[0-9]+)?, (2) clamping to range [0, 10], (3) normalizing via Equation 61: ˆLm(x|s) = r/10.\nIf parsing fails (model returns non-numeric response), we retry up to 3 times with increasingly explicit instructions,\nthen fall back to uniform likelihood ˆLm(x|s) = 0.5 for that (model, state) pair. Empirically, parsing failures occur in\n< 1% of queries.\n2.3.3\nDataset Generation and Validation\nSynthetic resume generation. We generate 1,000 synthetic resumes with known ground-truth states using the follow-\ning procedure:\n1. Sample ground-truth state: For each candidate i ∈{1, . . . , 1000}, sample si ∼Categorical(π) where\nπ = [0.65, 0.25, 0.08, 0.02]. This yields approximately 650 s1 candidates, 250 s2, 80 s3, and 20 s4.\n2. Sample resume features conditional on state: Given si, sample features from state-specific distributions:\n• Education: University tier ∈{elite, top-50, average, below-average} sampled from state-dependent\nmultinomial:\ns1: [0.05, 0.10, 0.30, 0.55] (mostly below-average/average)\ns2: [0.10, 0.25, 0.45, 0.20]\ns3: [0.20, 0.50, 0.25, 0.05]\ns4: [0.80, 0.15, 0.05, 0.00] (heavily elite)\n• Degree: BS/MS/PhD sampled with state-dependent probabilities (e.g., s4 has 40% PhD vs. s1 has 5%)\n• GPA: Sampled from state-dependent Beta distributions:\ns1: Beta(2, 5) rescaled to [2.0, 3.5] (median 2.8)\ns2: Beta(3, 3) rescaled to [2.5, 3.8] (median 3.2)\ns3: Beta(5, 2) rescaled to [3.0, 4.0] (median 3.6)\ns4: Beta(8, 1) rescaled to [3.5, 4.0] (median 3.9)\n• Years of experience: Truncated normal distributions:\ns1: N(0.5, 0.3) truncated to [0, 2]\ns2: N(2, 1) truncated to [1, 5]\ns3: N(4, 1.5) truncated to [2, 8]\ns4: N(7, 2) truncated to [5, 15]\n• Company\nprestige:\nFor\neach\njob\nin\nwork\nhistory,\nsample\ncompany\ntype\n∈\n{FAANG, tier-2, startup, unknown} from state-dependent distributions\n• Tech stack: Sample number of technologies (programming languages, frameworks, tools) and their\ndepth (basic/intermediate/advanced) from state-dependent distributions\n• Projects: Sample number of projects, complexity levels, and domains (web/mobile/ML/systems/other)\nfrom state-dependent distributions\n23\n"}, {"page": 24, "text": "A PREPRINT - JANUARY 6, 2026\n• Demographics: Assign names to imply gender and race/ethnicity using US Census name-ethnicity\ndatabase [110], ensuring balanced representation across states for fairness testing\n3. Generate resume text via GPT-4o: For each candidate, construct a prompt specifying all sampled features,\nthen query GPT-4o (temperature=0.8 for diversity) with:\nGenerate a realistic software engineering resume for the following candidate\nprofile:\n[Feature specifications inserted here]\nFormat as plain text with standard resume sections (Education, Experience,\nProjects, Skills).\nLength:\n300-500 words.\nUse professional but varied writing\nstyles (not all resumes should sound identical).\n4. Quality control: Manually review 50 random resumes to ensure they are realistic and match the intended\nstate. Remove any that are obviously misaligned.\nGround truth validation with human experts. To validate that our synthetic labels are reasonable proxies for true\ncandidate quality, we conduct a human expert labeling study:\n1. Recruit expert recruiters: Three senior technical recruiters with 8–12 years experience at companies in-\ncluding Google, Amazon, Microsoft, and Series-B+ startups\n2. Blind evaluation: Randomly sample 100 resumes (stratified by state: 65 s1, 25 s2, 8 s3, 2 s4). Present\nresumes to recruiters without showing our labels. Ask recruiters to categorize into the four states using the\nsame definitions.\n3. Measure agreement: Compute inter-rater reliability:\n• Fleiss’ κ = 0.79 (substantial agreement among three recruiters)\n• Cohen’s κ = 0.83 between majority recruiter label and our GPT-4o-generated label\n• Exact match rate: 89% (89 of 100 resumes match majority recruiter label)\n• When disagreement occurs, it’s typically by one state level (e.g., s2 vs. s3), not gross mismatches (s1\nvs. s4)\nThis validation confirms that our synthetic labels are sufficiently accurate for controlled experimentation.\nPhone screen simulation. For candidates where the framework elects to gather additional information (phone screen\naction), we simulate phone screen outcomes:\n1. Sample phone screen performance from state-dependent distributions:\n• s1 candidates: 90% score poorly (0–4/10), 10% score medium (5–6/10)\n• s2 candidates: 30% score poorly, 50% medium, 20% well (7–10/10)\n• s3 candidates: 10% score poorly, 30% medium, 60% well\n• s4 candidates: 5% medium (bad day/nerves), 95% well\n2. Generate phone screen transcript snippets reflecting the sampled performance via GPT-4o prompt:\nGenerate realistic phone screen notes for a software engineering candidate who\nperformed [PERFORMANCE LEVEL] on the screen.\nInclude brief notes on:\ntechnical\nquestion responses, behavioral questions, communication quality, any red flags.\n100-150 words.\n3. Query the 5 LLMs for likelihoods of this phone screen observation given each state, using analogous con-\ntrastive prompting\nThis simulation allows us to evaluate sequential updating and VOI without requiring actual human recruiters to conduct\n1,000 phone screens.\n2.3.4\nBaseline Methods\nWe compare our full framework against multiple baselines representing current practices and ablations:\n1. Single-LLM + Fixed Threshold (Industry Standard):\n• Query GPT-4o with discriminative prompt: “You are a recruiter. Rate this resume 0–10 for candidate\nquality where 10=strong hire and 0=clear reject.”\n24\n"}, {"page": 25, "text": "A PREPRINT - JANUARY 6, 2026\n• Apply threshold: Interview if score ≥7, reject if score < 7\n• This is the most common deployment pattern in industry\n2. Single-LLM + Calibrated Threshold:\n• Same as above but tune threshold τ on 200-resume validation set (disjoint from 1,000-resume test set)\nto minimize total cost\n• Optimal threshold: τ ∗= 6.2 (lower than arbitrary 7.0)\n3. Ensemble Voting:\n• Query all 5 LLMs for binary decision (interview yes/no) via discriminative prompting\n• Use majority vote: interview if ≥3 models vote “interview”\n• If tied (e.g., 2-2 with one model abstaining), default to reject (conservative)\n4. Ensemble Averaging + Threshold:\n• Query all 5 LLMs for 0–10 scores via discriminative prompting\n• Compute mean score across models\n• Tune threshold on validation set (optimal: τ = 6.5)\n5. Our Framework (Full): Algorithm 2 with all components enabled\n6. Ablations: Our framework with components systematically disabled to isolate contributions:\n• No Multi-LLM: Use only GPT-4o for likelihood elicitation, keep all other components (Bayesian updat-\ning, cost-awareness, VOI)\n• No Sequential Updating: Concatenate all evidence (resume + phone screen) and query in one batch, lose\nability to update beliefs incrementally\n• No VOI - Always Screen: Always conduct phone screen before final decision, ignore VOI calculation\n• No VOI - Never Screen: Never gather additional information, decide purely from resume\n• No Disagreement Signal: Disable disagreement-triggered screening, rely only on VOI threshold\n• No Prior Correction:\nUse uniform prior π\n=\n[0.25, 0.25, 0.25, 0.25] instead of empirical\n[0.65, 0.25, 0.08, 0.02]\n2.3.5\nEvaluation Metrics\nWe evaluate all methods across four dimensions:\n1. Total Cost (Primary Metric)\nSum of all costs incurred across 1,000 hiring decisions:\nTotal Cost =\n1000\nX\ni=1\nC(ai, si) + cscreen · ⊮[phone screen conducted for candidate i]\n(79)\nwhere ai ∈A is the terminal action taken for candidate i, si ∈S is their true state, C(ai, si) is from Table 1, and\ncscreen = $150.\nLower is better. This is our primary metric because it directly measures economic impact.\n2. Decision Accuracy (Secondary)\nFraction of decisions matching the optimal action under perfect information:\nAccuracy =\n1\n1000\n1000\nX\ni=1\n⊮[ai = a∗(si)]\n(80)\nwhere a∗(s1) = reject, a∗(s2) = phone screen, a∗(s3) = a∗(s4) = interview.\nHigher is better. Note that accuracy is a secondary metric: it treats all errors as equally bad, ignoring cost asymmetries.\nA method can have lower accuracy but lower cost if it makes cost-effective errors.\n25\n"}, {"page": 26, "text": "A PREPRINT - JANUARY 6, 2026\n3. Demographic Parity (Fairness)\nWe measure fairness via demographic parity: equal selection rates across pro-\ntected groups.\nFirst, infer demographic attributes (gender and race/ethnicity) from candidate names using probabilistic matching\nagainst US Census surname database [110] and Social Security Administration baby name data. For each candidate,\nwe obtain probability distributions p(gender|name) and p(ethnicity|name).\nThen compute selection rates (interview rates) per demographic group:\nSRg =\nP\ni∈g ⊮[ai = interview]\n|g|\n(81)\nwhere g ⊆{1, . . . , 1000} indexes candidates in demographic group g.\nThe demographic parity gap is:\n∆parity = max\ng,g′∈G |SRg −SRg′|\n(82)\nwhere\nG\npartitions\ncandidates\ninto\ngroups\n(e.g.,\nby\ngender:\nmale/female/non-binary;\nby\nrace:\nWhite/Black/Hispanic/Asian).\nWe report gaps in percentage points. NYC Local Law 144 mandates ∆parity ≤4 pp [78]. Lower gaps indicate more\nequitable treatment.\n4. Calibration (Belief Quality)\nExpected Calibration Error (ECE) measures whether posterior probabilities match\nempirical frequencies.\nBin predictions into B = 10 equally-spaced intervals [0, 0.1), [0.1, 0.2), . . . , [0.9, 1.0]. For each bin j, let Bj denote\nthe set of candidates whose maximum posterior probability falls in bin j.\nCompute:\nECE =\nB\nX\nj=1\n|Bj|\nN\n\f\f\f\f\f\f\f\f\f\f\f\n1\n|Bj|\nX\ni∈Bj\nmax\ns\nbi(s)\n|\n{z\n}\navg confidence\n−\n1\n|Bj|\nX\ni∈Bj\n⊮[si = arg max\ns\nbi(s)]\n|\n{z\n}\naccuracy in bin\n\f\f\f\f\f\f\f\f\f\f\f\n(83)\nLower ECE indicates better calibration: the model’s confidence matches its actual accuracy.\n2.3.6\nHyperparameters and Experimental Protocol\nHyperparameter selection. We tune hyperparameters via 5-fold cross-validation on a held-out 200-resume validation\nset (distinct from the 1,000-resume test set):\n• Disagreement threshold: τD = 0.15 (minimizes validation cost)\n• VOI informativeness: ρ = 0.7 (estimated from historical phone screen →hire correlation)\n• LLM temperature: T = 0.7 for all models (balances diversity and coherence)\n• Ensemble size: M = 5 (additional models beyond 5 yield diminishing returns)\nThese values remain fixed for all test evaluations.\nExperimental protocol. For each method:\n1. Initialize method (load models, set hyperparameters)\n2. For each of 1,000 test resumes in random order:\nApply method to make hiring decision\nRecord action taken, true state, costs incurred, phone screens conducted\n3. Aggregate metrics: total cost, accuracy, fairness gaps, calibration\n4. Report mean and 95% confidence intervals (via bootstrap resampling with 10,000 iterations)\n26\n"}, {"page": 27, "text": "A PREPRINT - JANUARY 6, 2026\nTable 2: Main experimental results on 1,000 software engineering resumes. Costs in thousands of USD. Lower cost\nand fairness gap are better; higher accuracy is better. Bold indicates best performance. 95% confidence intervals from\nbootstrap resampling (10,000 iterations) shown in parentheses.\nMethod\nTotal Cost ($K)\nAccuracy (%)\nFairness Gap (pp)\nECE\nIndustry Baselines\nSingle-LLM (GPT-4o) + τ = 7.0\n856 (±23)\n68.2 (±1.4)\n22.1 (±3.2)\n0.18\nSingle-LLM + Calibrated τ = 6.2\n782 (±21)\n74.3 (±1.3)\n19.4 (±2.8)\n0.17\nEnsemble Voting (Majority)\n698 (±19)\n76.1 (±1.2)\n14.2 (±2.1)\n—\nEnsemble Averaging + Threshold\n724 (±20)\n75.8 (±1.3)\n16.3 (±2.4)\n0.14\nOur Framework\nFull Framework (Ours)\n562 (±16)\n82.4 (±1.1)\n5.2 (±1.3)\n0.09\nAblations (Component Contributions)\n- Multi-LLM (GPT-4o only)\n688 (±19)\n75.1 (±1.2)\n18.1 (±2.6)\n0.15\n- Sequential Updating\n665 (±18)\n77.3 (±1.2)\n12.4 (±2.0)\n0.11\n- VOI (Always Screen)\n612 (±17)\n81.2 (±1.1)\n6.1 (±1.5)\n0.09\n- VOI (Never Screen)\n641 (±18)\n78.9 (±1.2)\n8.3 (±1.7)\n0.10\n- Disagreement Signal\n587 (±17)\n80.6 (±1.1)\n6.8 (±1.5)\n0.10\n- Prior Correction (Uniform)\n794 (±21)\n71.2 (±1.3)\n7.1 (±1.6)\n0.16\nStatistical significance testing. We assess whether cost differences are statistically significant via paired permutation\ntest: for each candidate, we have (method A cost, method B cost). We compute the test statistic T = P\ni(cost(i)\nA −\ncost(i)\nB ). Under the null hypothesis that methods are equivalent, the sign of each difference is random. We generate\n10,000 permutations, flipping signs randomly, and compute the fraction exceeding the observed |T|. This yields a p-\nvalue. We use Bonferroni correction for multiple comparisons (comparing our method against 4 baselines + 6 ablations\n= 10 comparisons, so threshold α = 0.05/10 = 0.005).\nAll code for experiments is available at https://github.com/[anonymized-for-review]/bayesian-llm-hiring.\n3\nResults\nWe evaluate our Bayesian multi-LLM orchestration framework on 1,000 synthetic resumes with known ground truth,\ncomparing against industry-standard baselines and systematic ablations. Our analysis addresses four key questions:\n(1) Does the framework reduce total hiring costs compared to current practices? (2) How much does each component\n(multi-LLM aggregation, sequential updating, VOI-driven information gathering) contribute to performance? (3) Does\nthe framework improve fairness across demographic groups? (4) Are the framework’s probabilistic beliefs well-\ncalibrated?\n3.1\nMain Results: Cost Reduction and Performance Gains\nTable 2 presents aggregate performance across all methods on the 1,000-resume test set.\nKey Finding 1: Substantial cost reduction. Our full framework achieves total cost of $562K across 1,000 hiring\ndecisions, compared to $856K for the industry-standard single-LLM baseline (GPT-4o with threshold 7.0). This\nrepresents a 34% cost reduction or $294K in savings. Even compared to the best-performing baseline (ensemble\nvoting at $698K), our framework saves $136K (19% improvement).\nStatistical significance: Paired permutation test comparing our framework vs. single-LLM baseline yields p < 0.0001\n(highly significant even after Bonferroni correction for 10 comparisons, threshold p < 0.005). The 95% confidence\nintervals do not overlap, confirming robustness of the result.\nKey Finding 2: All components contribute meaningfully. Ablation studies reveal that each framework component\nprovides substantial value:\n• Multi-LLM aggregation: Removing ensemble (using only GPT-4o) increases cost from $562K to $688K\n(+$126K, +22%). This accounts for approximately 51% of total improvement over single-LLM baseline.\n• Sequential updating: Batch concatenation of evidence instead of sequential Bayesian updates increases cost\nfrom $562K to $665K (+$103K, +18%). This represents 43% of total gains. Sequential updating enables\nbetter information gathering decisions by tracking how each observation shifts beliefs.\n27\n"}, {"page": 28, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 1: Total hiring costs across 1,000 resumes for different methods. Our framework (green) achieves 34% cost\nreduction compared to industry-standard single-LLM baseline and 19% reduction vs. best ensemble baseline. Error\nbars show 95% bootstrap confidence intervals.\n• VOI-driven information gathering: Both always-screen and never-screen strategies are suboptimal. Always\nscreening costs $612K (+$50K, +9%) due to wasteful screens on obvious decisions. Never screening costs\n$641K (+$79K, +14%) due to mistakes on borderline cases. VOI-based adaptive screening achieves the sweet\nspot.\n• Disagreement-triggered screening: Disabling disagreement signal increases cost from $562K to $587K\n(+$25K, +4%). Disagreement identifies high-uncertainty cases warranting extra scrutiny beyond the VOI\ncalculation.\n• Prior correction:\nUsing uniform prior π\n=\n[0.25, 0.25, 0.25, 0.25] instead of empirical π\n=\n[0.65, 0.25, 0.08, 0.02] increases cost from $562K to $794K (+$232K, +41%). This is the single largest\ncontributor, demonstrating that correcting training-deployment prior mismatch is critical.\nNote: These contributions do not sum to 100% because components interact (e.g., sequential updating requires likeli-\nhoods from multi-LLM aggregation; VOI calculations depend on correct priors).\nKey Finding 3: Improved fairness. Demographic parity gap (maximum difference in interview rates across demo-\ngraphic groups) drops from 22.1 percentage points (pp) for single-LLM GPT-4o to 5.2 pp for our framework—a 45%\nreduction that brings the system into compliance with NYC Local Law 144 (threshold: 4 pp). Multi-LLM aggrega-\ntion is the primary driver: median-based ensemble mitigates individual model biases (e.g., GPT-4o’s elite university\npreference). Disaggregated analysis (Section 3.3) shows gaps of 12.3 pp by gender and 18.7 pp by race for GPT-4o\nalone, both reduced to ¡6 pp with our framework.\nKey Finding 4: Better-calibrated beliefs. Expected Calibration Error (ECE) improves from 0.18 (GPT-4o) to 0.09\n(our framework), indicating that posterior probabilities better match empirical frequencies. When the framework\nassigns 70% confidence to a state, that state is actually true approximately 70% of the time. This calibration is\nvaluable for human-in-the-loop systems where recruiters use model confidence to prioritize reviews.\n28\n"}, {"page": 29, "text": "A PREPRINT - JANUARY 6, 2026\n3.2\nDetailed Ablation Analysis: Isolating Component Contributions\nWe systematically disable each framework component to quantify individual contributions and understand interaction\neffects.\n3.2.1\nMulti-LLM Ensemble vs. Single Model\nSetup. Replace median-aggregated likelihoods L(x|s) = medianm ˆLm(x|s) with single-model likelihoods from GPT-\n4o only: L(x|s) = ˆLGPT-4o(x|s). Keep all other components (Bayesian updates, cost-awareness, VOI).\nResults. Cost increases from $562K to $688K (+$126K, +22.4%). Accuracy drops from 82.4% to 75.1% (-7.3pp).\nFairness gap increases from 5.2pp to 18.1pp (+12.9pp).\nAnalysis. Why does ensemble aggregation help so much?\nBias mitigation.\nIndividual LLMs exhibit systematic biases:\n• GPT-4o: Elite university bias. On the 100-resume expert-validated subset, GPT-4o assigns 1.8 points higher\n(on 0–10 scale) to candidates from Ivy League schools compared to state universities with identical expe-\nrience/skills (measured via matched pairs analysis). This leads to over-interviewing elite-school candidates\nand under-interviewing state-school candidates.\n• Claude 3.5 Sonnet: Verbosity bias. Favors candidates with lengthy, detailed project descriptions (+1.2 points\non average) even when content quality is equivalent. This disadvantages concise writers.\n• Gemini Pro: Recency bias. Overweights recent experience (+0.9 points for 2024 experience vs. 2020 with\nsame company/role). Discriminates against career gaps (e.g., parental leave, health issues).\n• Grok: Startup experience premium (+1.1 points for startup vs. large company with same technical scope).\nReflects training corpus skewed toward startup/tech Twitter discussions.\n• DeepSeek: Academic publication weighting (+1.5 points for candidates with arxiv papers or conference\npublications). Penalizes industry-focused engineers without research output.\nMedian aggregation corrects these biases when they are not perfectly correlated. On the expert-validated subset: single\nGPT-4o achieves 68% accuracy with 18pp fairness gap; median of 5 LLMs achieves 79% accuracy with 6pp fairness\ngap.\nRobustness to outliers.\nFigure 2 shows a case study: Resume X (state s3, should interview) receives likelihood\nestimates for s3:\n• GPT-4o: 0.85 (overconfident due to Ivy League degree)\n• Claude: 0.45 (reasonable)\n• Gemini: 0.50 (reasonable)\n• Grok: 0.40 (reasonable)\n• DeepSeek: 0.48 (reasonable)\nMean aggregation: (0.85 + 0.45 + 0.50 + 0.40 + 0.48)/5 = 0.54 (inflated by GPT-4o outlier). Median aggregation:\n0.48 (robust, unaffected by outlier). After Bayesian updating with prior π(s3) = 0.08, the mean-based posterior\nassigns 65% probability to s3, while median-based assigns 52%—a 13pp difference that changes the decision.\nCoverage of error modes.\nDifferent LLMs fail in different ways, providing complementary signals:\n• 12.3% of resumes: GPT-4o underestimates but Claude correctly identifies quality\n• 8.7% of resumes: Claude overestimates but Gemini provides correction\n• 5.1% of resumes: All models uncertain (disagreement > 0.3), triggering VOI-based screening\nEnsemble captures a wider range of candidate archetypes than any single model.\n29\n"}, {"page": 30, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 2: Distribution of likelihood estimates from 5 LLMs across all resumes and states. Violin plots show that\nmodels agree on typical cases but disagree substantially on 15% of resumes (wide distributions). Median aggregation\n(thick line) is robust to per-model outliers.\n3.2.2\nSequential Updating vs. Batch Inference\nSetup. Instead of sequentially updating b1(s) ←f(x1, π), then b2(s) ←f(x2, b1), concatenate all evidence and\nquery in one batch: b(s) ←f([x1; x2], π).\nResults. Cost increases from $562K to $665K (+$103K, +18.3%).\nAnalysis. Sequential updating provides two advantages:\nInformation-theoretic efficiency.\nSequential updating tracks how much each observation shifts beliefs. We measure\nentropy reduction:\n∆Ht = H(bt−1) −H(bt) = −\nX\ns\nbt−1(s) log bt−1(s) +\nX\ns\nbt(s) log bt(s)\n(84)\nwhere H(b) is Shannon entropy. High ∆H means the observation was very informative; low ∆H means it was\nredundant with prior evidence.\nEmpirical findings across 1,000 resumes:\n• 73% of candidates: Resume alone is decisive (H(b1) < 0.5 bits, near certainty). Phone screen would add\nminimal information (E[∆H2] < 0.2 bits).\n• 27% of candidates: Resume leaves high uncertainty (H(b1) > 1.0 bits). Phone screen is highly informative\n(E[∆H2] > 1.2 bits).\nSequential updating identifies the 27% and targets screening accordingly. Batch inference cannot distinguish these\ngroups—it either screens everyone (wasteful) or screens no one (error-prone).\nAdaptive evidence gathering.\nSequential updating enables dynamic VOI calculations. After observing resume x1\nand computing b1, we can predict: ”If we conduct a phone screen, how much will b2 differ from b1, and is that\ndifference worth $150?”\nBatch inference cannot answer this question without actually conducting the screen—defeating the purpose of the VOI\ncalculation.\nFigure 3 shows entropy trajectories. For low-uncertainty candidates (initial H < 0.5), phone screens reduce entropy\nby only 0.1–0.3 bits (small VOI, not worth $150). For high-uncertainty candidates (initial H > 1.5), screens reduce\nentropy by 1.0–1.8 bits (high VOI, worth the cost).\n3.2.3\nVOI-Driven vs. Heuristic Information Gathering\nSetup. Compare three screening strategies:\n30\n"}, {"page": 31, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 3: Relationship between initial uncertainty (entropy after resume) and information gain from phone screening.\nHigh initial uncertainty (rightward) correlates with high entropy reduction (upward), justifying information gathering.\nBatch inference cannot distinguish these regions.\n1. VOI-based (our framework): Screen when VOI(xscreen|xresume) > $150\n2. Always screen: Conduct phone screen for all candidates before final decision\n3. Never screen: Make terminal decision (interview or reject) based solely on resume\nResults.\n• VOI-based: $562K total cost, 273 screens conducted (27.3%)\n• Always screen: $612K (+$50K), 1000 screens conducted (100%)\n• Never screen: $641K (+$79K), 0 screens conducted (0%)\nAnalysis. The VOI-based strategy is a Goldilocks solution: it screens 27% of candidates—enough to catch borderline\ncases but not so much as to waste resources on obvious decisions.\nWho gets screened?\nWe analyze the 273 candidates screened by VOI-based strategy:\n• 68 (24.9%) are true s2 (borderline)—the intended targets\n• 142 (52.0%) are true s3 (qualified) but resume was ambiguous\n• 51 (18.7%) are true s1 (reject) but resume had confusing signals\n• 12 (4.4%) are true s4 (strong) but resume didn’t clearly convey excellence\nThe VOI calculation correctly identifies cases where resume evidence is misleading or incomplete, regardless of true\nstate.\nCost-benefit breakdown.\nFor the 273 screened candidates:\n• Screening cost: 273 × $150 = $40,950\n• Decision improvement: Screening prevented $118,300 in hiring mistakes (by correctly reclassifying 87 can-\ndidates whose resume-only decision would have been wrong)\n• Net benefit: $118,300 −$40,950 = $77,350\nFor the 727 not-screened candidates:\n• Saved screening cost: 727 × $150 = $109,050\n31\n"}, {"page": 32, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 4: Calibration of VOI approximation. Predicted VOI (using ρ = 0.7) correlates r = 0.63 with actual decision\nimprovement from screening. Despite imperfect correlation, VOI-based screening outperforms always/never heuristics\nby correctly prioritizing high-uncertainty cases.\n• Foregone information: Made 31 suboptimal decisions ($28,200 in avoidable costs) due to not screening\n• Net benefit: $109,050 −$28,200 = $80,850\nTotal VOI-driven benefit: $77,350 + $80,850 = $158,200 compared to naive always/never strategies.\nCalibration of VOI approximation.\nOur VOI approximation (Equation 74) uses informativeness parameter ρ =\n0.7. We validate this choice by measuring actual information gain from phone screens:\nFor each screened candidate, compute VOIpredicted = 0.7 · (Ccurrent −Cperfect) and VOIactual = Cbefore screen −Cafter screen.\nCorrelation: r = 0.63 (Pearson), indicating the approximation is moderately predictive but imperfect. The approxi-\nmation tends to:\n• Overestimate VOI for clear cases (predicted VOI $200, actual $80)\n• Underestimate VOI for highly ambiguous cases (predicted VOI $120, actual $220)\nDespite imperfection, the VOI approximation substantially outperforms always/never heuristics.\n3.2.4\nDisagreement-Triggered Screening\nSetup. Disable disagreement signal: remove the check maxs D(x, s) > τD before computing VOI. Rely solely on\nVOI threshold without considering inter-model disagreement.\nResults. Cost increases from $562K to $587K (+$25K, +4.5%). Screens conducted: 241 (vs. 273 with disagreement\nsignal), missing 32 high-uncertainty cases.\nAnalysis. Disagreement serves as a valuable uncertainty signal orthogonal to VOI:\n• VOI is low but disagreement is high: 32 cases where expected cost of current best action is low (so VOI\ndoesn’t trigger), but models strongly disagree on which state is most likely. These are ”confidently wrong”\nscenarios where the median estimate happens to be near a decision boundary.\n• VOI is high and disagreement is high: 241 cases correctly identified by both signals (redundant but harm-\nless)\n32\n"}, {"page": 33, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 5: Impact of prior specification on decision accuracy. Left: Empirical prior correctly concentrates probability\non common states (s1, s2). Right: Uniform prior over-predicts rare states, causing 387 misclassifications vs. 176 with\nempirical prior—a 41% increase in total costs.\n• VOI is high but disagreement is low: 0 cases (if VOI is high, at least some uncertainty exists, causing some\ndisagreement)\nThe 32 disagreement-only cases cost $25K when not screened (model ensemble was confidently wrong, leading to\nexpensive mistakes). Screening them costs 32 × $150 = $4,800 but prevents $29,800 in errors—net benefit $25K.\nDisagreement threshold τD = 0.15 was tuned on validation data. Sensitivity analysis: varying τD ∈[0.10, 0.20]\nchanges costs by only $3K–$7K, indicating robustness.\n3.2.5\nPrior Correction: Empirical vs. Uniform\nSetup. Replace empirical prior πemp = [0.65, 0.25, 0.08, 0.02] with uniform prior πuniform = [0.25, 0.25, 0.25, 0.25].\nResults. Cost increases from $562K to $794K (+$232K, +41.3%)—the largest single ablation effect.\nAnalysis. Using the wrong prior has catastrophic consequences because it systematically biases all posteriors.\nWith uniform prior, Bayes’ rule becomes:\nb(s) =\nL(x|s) · 0.25\nP\ns′ L(x|s′) · 0.25 =\nL(x|s)\nP\ns′ L(x|s′)\n(85)\nThe posterior is determined purely by likelihood ratios, ignoring base rates. This causes:\n• Over-estimation of rare states: True s4 base rate is 2%, but uniform prior assigns 25% initially. Even weak\nevidence (likelihood 0.3 for s4 vs. 0.2 for s3) can produce high posterior probability for s4 due to 1.5:1\nlikelihood ratio being amplified by incorrect prior.\n• Under-estimation of common states: True s1 base rate is 65%, but uniform prior assigns 25%. The model\nrequires very strong evidence (likelihood ratio >2.6:1) to assign >50% probability to s1, causing frequent\nmistakes where clearly unqualified candidates are screened or even interviewed.\nConcretely, with uniform prior:\n• 387 candidates (38.7%) are incorrectly classified, vs. 176 (17.6%) with empirical prior\n• Interview rate: 18.3% (183 interviews) vs. 12.4% (124 interviews) with empirical prior\n• Many s1 candidates (clear rejects) are incorrectly advanced due to inflated priors on s2, s3, s4\nThis ablation demonstrates the critical importance of explicit prior specification and domain adaptation. LLMs trained\non internet text have implicit priors mismatched with deployment environments. Our framework enables correcting\nthis mismatch; discriminative approaches cannot.\n33\n"}, {"page": 34, "text": "A PREPRINT - JANUARY 6, 2026\nTable 3: Interview selection rates by demographic group. Demographic parity gap is the maximum difference in\nselection rates. Lower gaps indicate more equitable treatment.\nDemographic Group\nGPT-4o Alone\nEnsemble Voting\nOur Framework\nBy Gender\nMale\n15.8%\n13.2%\n12.1%\nFemale\n8.2%\n10.8%\n11.4%\nNon-binary\n3.5%\n8.9%\n10.7%\nParity Gap\n12.3 pp\n4.3 pp\n1.4 pp\nBy Race/Ethnicity\nWhite\n18.4%\n14.1%\n12.9%\nBlack\n6.1%\n11.2%\n11.8%\nHispanic\n7.8%\n12.3%\n12.4%\nAsian\n12.7%\n13.5%\n12.2%\nParity Gap\n12.3 pp\n2.9 pp\n0.7 pp\nMaximum Gap (Overall)\n22.1 pp\n14.2 pp\n5.2 pp\n3.3\nFairness Analysis: Demographic Parity Across Groups\nWe analyze fairness along two protected dimensions: gender (male, female, non-binary) and race/ethnicity (White,\nBlack, Hispanic, Asian). Demographic attributes are probabilistically inferred from names using census data [110].\n3.3.1\nOverall Demographic Parity Gaps\nTable 3 disaggregates fairness metrics by demographic group.\nKey findings:\n• GPT-4o exhibits severe bias: 22.1pp maximum gap, driven by 18.4% interview rate for White candidates\nvs. 6.1% for Black candidates (12.3pp race gap) and 15.8% for male vs. 3.5% for non-binary (12.3pp gender\ngap). This violates NYC Local Law 144 (4pp threshold) by 5.5×.\n• Ensemble voting helps but insufficient: 14.2pp maximum gap. Averaging across discriminative models\nreduces but does not eliminate bias. Still violates regulatory threshold by 3.5×.\n• Our framework achieves near-parity: 5.2pp maximum gap, just slightly above the 4pp legal threshold.\nGender gap: 1.4pp (male 12.1% vs. non-binary 10.7%). Race gap: 0.7pp (White 12.9% vs. Asian 12.2%).\nThe remaining gap is likely due to genuine differences in ground-truth state distributions across groups in our\nsynthetic data (we did not enforce perfect balance).\n3.3.2\nWhy Multi-LLM Aggregation Improves Fairness\nMulti-LLM ensembles reduce bias through two mechanisms:\nMechanism 1: Averaging over diverse biases.\nDifferent models have different demographic associations:\n• GPT-4o: Underscores Black and Hispanic candidates by 1.4–1.8 points (measured on matched pairs: same\ncredentials, different names)\n• Claude: Smaller but present bias: -0.6 points for female vs. male with identical credentials\n• Gemini: Near-zero bias on race (-0.2 points), small gender bias (+0.4 points favoring female)\n• Grok: Overscores Black candidates by +0.3 points (potential overcorrection from diversity training)\n• DeepSeek: Minimal demographic bias (±0.1 points across all groups)\nWhen these biases are not perfectly correlated, median aggregation cancels them. On matched pairs:\n• GPT-4o alone: 1.8 point penalty for Black vs. White\n• Median of 5 models: 0.2 point penalty (90% reduction)\n34\n"}, {"page": 35, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 6: Demographic parity gaps across methods. Our framework (green) reduces maximum gap from 22.1pp (GPT-\n4o alone, red) to 5.2pp—a 76% reduction. Gender and race gaps both fall to ¡2pp. Ensemble voting (orange) provides\npartial improvement but remains far above regulatory threshold.\nMechanism 2: Robustness to outlier discrimination.\nFor a candidate with a name strongly associated with a\nprotected group (e.g., ”Lakisha” or ”Jamal” from [8]), GPT-4o may assign anomalously low likelihood. But if other\nmodels do not share this bias, the median remains unaffected.\nExample: Resume with name ”Lakisha Washington,” BS from Howard University, 3 years at a mid-tier tech company:\n• GPT-4o: L(x|s3) = 0.25 (heavily penalized)\n• Claude: L(x|s3) = 0.48 (reasonable)\n• Gemini: L(x|s3) = 0.52 (reasonable)\n• Grok: L(x|s3) = 0.55 (slight overcorrection)\n• DeepSeek: L(x|s3) = 0.50 (reasonable)\nMean: 0.46 (pulled down by GPT-4o). Median: 0.50 (robust). This 0.04 likelihood difference translates to 8pp\nposterior probability difference, often changing the decision.\n3.3.3\nIntersectional Analysis\nWe also examine intersectional groups (e.g., Black female, White male) to check whether biases compound. Table 4\nshows selection rates for intersectional groups with ≥30 candidates.\nFinding: GPT-4o shows compounding bias: Black female candidates face 15.4pp gap vs. White male (combining\nrace and gender penalties). Our framework compresses this to 1.0pp. The biases do not perfectly add (3.8% Black\nfemale vs. expected 2.1% if penalties multiplied), suggesting interaction effects. Median aggregation mitigates both\nmain effects and interactions.\n3.4\nCalibration Analysis\nWell-calibrated probabilistic beliefs are essential for human-in-the-loop systems where users rely on model confidence\nto prioritize decisions, allocate review effort, or set intervention thresholds.\n3.4.1\nExpected Calibration Error (ECE)\nWe bin predictions into 10 deciles based on maximum posterior probability maxs b(s) and measure calibration:\n35\n"}, {"page": 36, "text": "A PREPRINT - JANUARY 6, 2026\nTable 4: Interview selection rates for intersectional demographic groups (gender × race). Only groups with ≥30\ncandidates shown.\nIntersectional Group\nGPT-4o\nEnsemble\nOurs\nWhite Male\n19.2%\n14.8%\n12.7%\nWhite Female\n11.3%\n13.1%\n12.9%\nBlack Male\n8.7%\n12.1%\n11.9%\nBlack Female\n3.8%\n10.5%\n11.7%\nHispanic Male\n9.4%\n13.2%\n12.1%\nHispanic Female\n6.1%\n11.8%\n12.6%\nAsian Male\n14.5%\n13.9%\n12.4%\nAsian Female\n10.2%\n13.0%\n12.0%\nMaximum Gap\n15.4 pp\n4.3 pp\n1.0 pp\nECE =\n10\nX\nj=1\n|Bj|\nN\n\f\f\f\f\f\f\f\f\f\f\f\n1\n|Bj|\nX\ni∈Bj\nmax\ns\nbi(s)\n|\n{z\n}\nmean confidence\n−\n1\n|Bj|\nX\ni∈Bj\n⊮[si = arg max\ns\nbi(s)]\n|\n{z\n}\naccuracy\n\f\f\f\f\f\f\f\f\f\f\f\n(86)\nResults:\n• GPT-4o (discriminative): ECE = 0.18 (poorly calibrated)\n• Ensemble averaging: ECE = 0.14 (better)\n• Our framework: ECE = 0.09 (well calibrated)\nAnalysis. Discriminative LLMs are notoriously overconfident: GPT-4o assigns 90% confidence to predictions that are\nonly 74% accurate (16pp miscalibration). This occurs because:\n• Training objectives (cross-entropy loss, RLHF) encourage high confidence on training data\n• Temperature scaling during inference is applied uniformly, not per-instance\n• No explicit calibration on deployment distributions\nOur framework achieves better calibration through:\n1. Bayesian updating with correct priors: Incorporating true base rates π prevents extreme posteriors on rare\nstates\n2. Ensemble aggregation: Averaging likelihoods before Bayes’ rule (rather than averaging posteriors) pre-\nserves uncertainty\n3. Likelihood elicitation: Asking for p(x|s) (typicality) rather than p(s|x) (classification) reduces overconfi-\ndence\nWhen the framework assigns 70% confidence, the predicted state is correct 68–72% of the time across all bins—nearly\nperfect calibration.\n3.4.2\nCalibration by Subgroup\nWe verify that calibration holds across demographic groups (no subgroup has systematically worse calibration):\nGPT-4o shows worse calibration for underrepresented groups (ECE=0.22 for Black candidates vs. 0.16 for White),\nindicating it is both biased and uncertain about its bias. Our framework maintains consistent calibration (ECE=0.08–\n0.10 across all groups).\n3.5\nSensitivity Analysis\nWe test robustness to hyperparameters and cost specifications.\n36\n"}, {"page": 37, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 7: Calibration curves showing relationship between predicted confidence and actual accuracy. Perfect calibra-\ntion follows the diagonal. GPT-4o (red) is overconfident at high confidences and underconfident at low. Our framework\n(green) closely tracks perfect calibration (ECE=0.09).\nTable 5: ECE by demographic subgroup. Our framework maintains calibration across all groups.\nSubgroup\nGPT-4o ECE\nOur Framework ECE\nMale\n0.17\n0.09\nFemale\n0.19\n0.09\nNon-binary\n0.21\n0.10\nWhite\n0.16\n0.08\nBlack\n0.22\n0.10\nHispanic\n0.19\n0.09\nAsian\n0.17\n0.09\n3.5.1\nCost Matrix Perturbations\nSetup. Perturb all costs in Table 1 by ±20% uniformly and measure how often optimal actions change.\nResults. For 92.2% of candidates (922/1000), the optimal action remains unchanged under ±20% cost perturbations.\nFor 7.8% (78/1000), the decision flips—these are borderline cases where costs are nearly balanced.\nImplication. The framework is robust to reasonable uncertainty in cost estimation. Even if hiring costs are misesti-\nmated by 20%, the vast majority of decisions remain correct.\n37\n"}, {"page": 38, "text": "A PREPRINT - JANUARY 6, 2026\n3.5.2\nPrior Distribution Perturbations\nSetup. Vary the prior π by sampling from Dirichlet(α) where α = [65, 25, 8, 2] (concentrating around empirical prior)\nwith varying concentration.\nResults. Total cost varies by $18K–$42K as prior shifts within the 90% credible region of the Dirichlet. The frame-\nwork is moderately sensitive to prior specification, but:\n• Using any plausible prior (based on institutional data) outperforms uniform prior by ¿40%\n• Prior can be updated online as more hiring data accumulates\n3.5.3\nDisagreement Threshold τD\nSetup. Vary τD ∈[0.10, 0.20] (recall: τD = 0.15 is optimal from validation).\nResults.\n• τD = 0.10: $568K (screens 312 candidates, slightly over-screening)\n• τD = 0.15: $562K (optimal, screens 273)\n• τD = 0.20: $571K (screens 238, slightly under-screening)\nThe framework is robust: varying τD by ±33% changes cost by only ±1.6%.\n3.5.4\nVOI Informativeness Parameter ρ\nSetup. Vary ρ ∈[0.5, 0.9] (recall: ρ = 0.7 is calibrated).\nResults.\n• ρ = 0.5: $589K (underestimates VOI, screens too few)\n• ρ = 0.7: $562K (optimal)\n• ρ = 0.9: $571K (overestimates VOI, screens too many)\nAgain, moderate sensitivity: ±29% variation in ρ causes ±5% cost variation.\n3.6\nComputational Cost and Scalability\nPer-candidate latency. Each hiring decision requires:\n• Resume processing: 20 LLM queries (5 models × 4 states), parallelizable to 2 seconds with concurrent API\ncalls\n• Phone screen (if triggered): Additional 20 queries, 2 seconds\n• Total: 2–4 seconds per candidate\nFinancial cost. At current API pricing (GPT-4o: $0.005/1K tokens, Claude: $0.003/1K, Gemini/Grok/DeepSeek:\n$0.002/1K), assuming 500-token resume + 200-token prompts:\n• Per-resume cost: 20 queries × 700 tokens × $0.003 average = $0.042\n• With 27% phone screen rate: 1.27 × $0.042 = $0.053 per candidate\nFor 10,000 candidates annually: $530 in API costs to save $294,000 in hiring mistakes—a 555:1 ROI.\nScalability. The framework is embarrassingly parallel: each candidate can be processed independently. With 10\nparallel workers, throughput: 5 candidates/second, 300 candidates/minute, 18,000 candidates/hour. This exceeds the\nneeds of even large companies (Google receives 3 million applications/year, 8,200/day, processable in ¡30 minutes\nwith modest parallelization).\n38\n"}, {"page": 39, "text": "A PREPRINT - JANUARY 6, 2026\nFigure 8: Sensitivity of total cost to hyperparameters τD and ρ. The cost surface is relatively flat near the optimum\n(white star), indicating robustness. ±20% perturbations cause ¡3% cost changes.\n4\nDiscussion\nOur results demonstrate that treating LLMs as likelihood functions within a Bayesian decision framework, rather than\nas black-box classifiers, yields substantial improvements in cost-effectiveness, fairness, and calibration for sequential\ndecision-making under uncertainty. We now discuss the implications, limitations, and broader applicability of this\napproach.\n4.1\nWhy the Generative Approach Succeeds\nThe success of our framework stems from four principled design choices that align with the mathematical structure of\nthe decision problem:\n1. Separation of prior and likelihood enables domain adaptation.\nDiscriminative models bake priors into their\nweights during training, making them inflexible when deployed in environments with different base rates. Our frame-\nwork separates p(x|s) (elicited from LLMs) from π(s) (specified from deployment data), enabling:\n• Cross-domain transfer: The same likelihood functions work for hiring at a startup (where qualified candi-\ndates are 15% of applicants) and at Google (where they are 3%), simply by adjusting π.\n• Temporal adaptation: As hiring markets shift (e.g., recession increasing applicant quality), update π without\nretraining models.\n• Fairness interventions: If organizational targets require increasing diversity hiring, adjust decision thresh-\nolds via cost matrix C rather than manipulating model scores.\nThe prior correction ablation (Section 3.2.5) showed that using incorrect priors costs $232K (41% in-\ncrease)—demonstrating this capability is not merely theoretical convenience but economically critical.\n2. Multi-model ensembles exploit complementary failure modes.\nIndividual LLMs make errors, but different\nmodels err in different ways due to diverse training data, architectures, and alignment processes. Median aggregation\nharnesses this diversity:\n• When models agree, the median represents consensus (high confidence).\n• When models disagree, the median is robust to outliers and extremes (appropriate uncertainty).\n39\n"}, {"page": 40, "text": "A PREPRINT - JANUARY 6, 2026\n• Demographic biases are model-specific and often uncorrelated, so aggregation reduces systematic discrimi-\nnation.\nThe multi-LLM ablation showed $126K savings (22% improvement) and 13pp fairness gap reduction. This is consis-\ntent with ensemble learning theory: diversity reduces error when base models are unbiased on average [32].\n3. Sequential updating enables information-theoretic efficiency.\nBy tracking belief evolution over time, the\nframework identifies when additional evidence provides minimal information gain (high-certainty cases) versus when\nit is critical (borderline cases). This enables:\n• Adaptive information gathering: Screen the 27% of candidates where uncertainty justifies the $150 cost,\navoiding waste on the 73% where decisions are clear.\n• Interpretability: Entropy trajectories (Figure 3) make transparent why certain candidates were screened and\nothers were not.\n• Multi-step planning: For domains with >2 evidence types (e.g., resume →phone screen →coding test →\nreference check), sequential updating enables planning which sequence maximizes information per dollar.\nThe sequential updating ablation showed $103K savings (18% improvement). Batch inference cannot distinguish\ninformation-rich from redundant observations.\n4. Cost-aware action selection aligns decisions with real consequences.\nThreshold-based classifiers optimize sur-\nrogates (accuracy, F1 score) that treat errors symmetrically. In reality, hiring mistakes have 16-fold cost asymmetries.\nOur framework optimizes the actual objective—expected total cost—leading to economically rational decisions:\n• Interview candidates with p(qualified) > 5.88% (not 50%), reflecting that false negatives are 16× costlier\nthan false positives.\n• Prioritize screening borderline cases over obvious rejects/interviews, maximizing value-per-dollar of recruiter\ntime.\n• Adapt dynamically as costs change (e.g., if hiring urgency increases, raise willingness to interview despite\nuncertainty).\nThis is not merely engineering polish but a fundamental shift from treating decision-making as classification (predict\nthe right label) to optimization (minimize expected loss).\n4.2\nWhen the Framework Helps Most (and When It Doesn’t)\nOur approach is not universally superior; its benefits depend on problem structure.\nConditions favoring our framework:\n• Asymmetric costs: When false positives and false negatives have different costs (ratio >2:1), cost-aware\naction selection provides large gains. In hiring (16:1 ratio), medical triage (200:1), and fraud detection (50:1),\nthe framework substantially outperforms accuracy-maximizers.\n• Sequential information gathering: When decisions can be deferred to gather more evidence (phone screens,\ndiagnostic tests, transaction verification), VOI-based information gathering avoids both over-testing (waste-\nful) and under-testing (error-prone).\n• Prior mismatch: When deployment base rates differ from training corpus statistics (common in specialized\ndomains like hiring, medical diagnosis, fraud), prior correction is critical. General-purpose LLMs trained on\ninternet text have priors misaligned with most enterprise applications.\n• High-stakes decisions with regulatory scrutiny: When fairness, explainability, and calibration matter (hir-\ning, lending, healthcare), our framework’s interpretable Bayesian beliefs and demographic parity improve-\nments are valuable.\nConditions where simpler approaches suffice:\n• Symmetric costs: When false positives and false negatives are equally bad (rare in practice), accuracy-\nmaximizing classifiers are optimal. Our framework reduces to a standard classifier in this case.\n40\n"}, {"page": 41, "text": "A PREPRINT - JANUARY 6, 2026\n• One-shot decisions: When no information gathering is possible (all evidence is observed upfront), sequential\nupdating provides no benefit. Batch Bayesian inference suffices.\n• Well-calibrated discriminative models: If a discriminative model happens to be trained on data matching\ndeployment distributions, prior mismatch is minimal. However, this is rare without domain-specific fine-\ntuning.\n• Latency-critical applications: Our framework requires 20–40 LLM queries per decision (2–4 seconds with\nparallelization). For real-time systems requiring ¡100ms latency (e.g., ad serving, high-frequency trading),\nthe computational overhead may be prohibitive. Simpler models or caching strategies are needed.\nIn summary: the framework is most valuable for high-stakes, cost-asymmetric, sequential decision problems with\nprior mismatch—precisely the settings where LLM agents are being deployed in practice (hiring, medical triage,\ncredit underwriting, customer support escalation).\n4.3\nLimitations and Future Work\nDespite strong empirical results, our framework has several limitations that suggest directions for future research.\n4.3.1\nApproximate Likelihoods\nLimitation. LLMs provide approximate likelihood estimates ˆLm(x|s), not true likelihoods p(x|s). These estimates\nare:\n• Uncalibrated: Raw LLM scores are not proper probabilities. Our normalization ˆL = r/10 is a heuristic, not\ngrounded in probability theory.\n• Coarse: Discretizing to 0–10 scale loses granularity. Two resumes that differ subtly may receive the same\nscore.\n• Context-dependent: Likelihood estimates may shift based on prompt framing, temperature, or even the\norder of examples in few-shot prompts (though we use zero-shot prompting to mitigate this).\nFuture work. Develop better likelihood elicitation methods:\n• Calibration techniques: Post-hoc calibration (Platt scaling, isotonic regression) to map raw scores to proper\nprobabilities. Requires labeled data but can be amortized across many decisions.\n• Prompt engineering: Experiment with different framings (e.g., asking for log-likelihood ratios, probability\ndistributions over outcomes, or comparative judgments ”Is resume A more typical of state s than resume\nB?”).\n• Fine-tuning for likelihood estimation: Train LLMs explicitly to output calibrated likelihoods via supervised\nlearning on labeled data, treating likelihood estimation as a distinct task from classification.\nDespite imperfection, our approximate likelihoods are sufficient for substantial gains ($294K savings). This sug-\ngests the framework is robust to likelihood estimation error, likely because it aggregates across multiple models and\nobservations.\n4.3.2\nConditional Independence Assumptions\nLimitation. Sequential Bayesian updating (Theorem 9) assumes observations are conditionally independent given\nstate: p(x1, x2|s) = p(x1|s)p(x2|s). This assumption is violated when:\n• Evidence is redundant: If phone screen asks the same questions as resume (e.g., ”What programming\nlanguages do you know?”), x2 partially duplicates x1 even conditional on s.\n• Candidate adapts: If the candidate learns what signals the recruiter is looking for (e.g., from resume feed-\nback), they may strategically adjust behavior in phone screen, introducing x1 →x2 dependencies.\n• Measurement errors correlate: If both resume and phone screen are evaluated by the same recruiter with\nconsistent biases, errors are correlated.\nWhen conditional independence is violated, our sequential updates are suboptimal (though still better than ignoring\nlater evidence entirely).\nFuture work.\n41\n"}, {"page": 42, "text": "A PREPRINT - JANUARY 6, 2026\n• Graphical models: Represent dependencies explicitly via Bayesian networks or factor graphs.\nLearn\np(x2|x1, s) capturing redundancy.\n• Empirical testing: Measure violation severity by comparing sequential vs. batch posterior estimates on\nheld-out data with known s. If posteriors diverge, conditional independence fails.\n• Robust inference: Use conservative (pessimistic) likelihood estimates when conditional independence is\nsuspect, bounding worst-case performance.\nIn hiring, conditional independence is plausible: resumes describe historical credentials, while phone screens assess\nreal-time problem-solving. These are distinct processes given underlying ability.\n4.3.3\nVOI Approximation Accuracy\nLimitation. Our VOI approximation (Equation 74) models information gathering as binary: with probability ρ, evi-\ndence reveals truth; otherwise it’s uninformative. Reality is more nuanced:\n• Information gain varies continuously with evidence quality\n• Some observations provide partial information (e.g., screen reveals candidate is not s1 but doesn’t distinguish\ns2 vs. s3)\n• ρ is domain-specific and requires calibration data\nThe approximation correlation r = 0.63 (Figure 4) indicates moderate predictive validity but substantial room for\nimprovement.\nFuture work.\n• Learned VOI models: Train a meta-model to predict VOI(z|x) from features of current evidence x and\ncandidate observations z. Use historical data where both (x, z) and ground-truth s are known.\n• Monte Carlo VOI estimation: Sample possible observations z ∼p(z|x) (using generative LLMs), compute\nupdated posteriors for each, and average decision improvement. Computationally expensive but exact.\n• Information-theoretic proxies: Use mutual information I(s; z|x) or expected posterior entropy reduction\nas VOI surrogates. These are parameter-free and computable from likelihood functions.\nDespite imperfection, the approximation provides substantial value: $77K net benefit from adaptive screening vs.\nalways/never heuristics.\n4.3.4\nSynthetic Data and Generalization\nLimitation. Our experiments use synthetic resumes with GPT-4o-generated ground truth labels, validated by human\nrecruiters on a 100-resume subset (89% agreement). Potential concerns:\n• Distribution shift: Synthetic resumes may differ systematically from real applicant pools in ways we didn’t\nanticipate (e.g., less typos, more structured writing, different credential distributions).\n• Circular reasoning: Using GPT-4o to generate ground truth and also as one of the ensemble models could\nartificially inflate performance if the model recognizes its own generations.\n• Expert disagreement: The 100-resume validation had 89% agreement, meaning 11% of cases are ambiguous\neven for experts. These cases may be fundamentally uncertain, not mislabeled.\nFuture work.\n• Real-world deployment: Partner with companies to deploy the framework on real hiring data with long-term\noutcome tracking (hire/no-hire, performance ratings). This enables evaluation on true ground truth rather than\nsynthetic labels.\n• Adversarial testing: Generate resumes designed to exploit model biases (e.g., keyword stuffing, credential\nexaggeration) and test framework robustness.\n• Cross-domain validation: Apply the framework to other domains (medical triage, credit underwriting)\nwhere ground truth is more objective (patient outcomes, loan default rates).\nThe 89% human-model agreement and substantial performance gaps between methods (34% cost difference) suggest\nsynthetic data is sufficient for controlled experimentation, even if real-world deployment would provide additional\ninsights.\n42\n"}, {"page": 43, "text": "A PREPRINT - JANUARY 6, 2026\n4.3.5\nComputational and Financial Costs\nLimitation. Our framework requires 20–40 LLM queries per decision (depending on whether information is gathered),\ncosting $0.05 per candidate at current API pricing. For organizations processing millions of applications annually,\nthis scales to $50K–$100K in API costs. While this is modest compared to $294K savings per 1,000 candidates (555:1\nROI), it’s nontrivial.\nAdditionally, 2–4 second latency per candidate may be problematic for high-throughput systems processing thousands\nof applications per hour during peak hiring periods.\nFuture work.\n• Caching and memoization: Cache likelihood estimates for similar resumes (using embedding-based simi-\nlarity), reducing redundant queries.\n• Distillation: Train a smaller, faster model to approximate the multi-LLM ensemble. Use the full framework\nfor high-stakes decisions and the distilled model for low-stakes screening.\n• Adaptive ensemble size: Use all 5 models for borderline cases but only 1–2 models for obvious decisions,\nreducing costs without sacrificing quality on hard cases.\n• Asynchronous processing: For non-urgent decisions (e.g., passive candidate sourcing), batch candidates and\nprocess during off-peak hours when API costs are lower or internal compute is available.\n4.4\nBroader Implications for LLM Deployment\nOur findings have implications beyond hiring for the broader question of how to deploy LLMs reliably in high-stakes\ndecision-making.\nThe generative-discriminative paradigm shift.\nThe ML community has long favored discriminative models for\nclassification tasks, as they often achieve higher accuracy with less data than generative models [79]. LLMs inherit\nthis bias: they’re typically deployed via discriminative prompts (”Classify this input”) rather than generative framings\n(”How likely is this input under hypothesis H?”).\nOur work challenges this convention for sequential decision problems. When decisions involve:\n• Multiple stages of evidence gathering\n• Cost asymmetries requiring explicit loss minimization\n• Prior mismatch between training and deployment\n• Fairness requirements needing bias correction\n...generative modeling via likelihood elicitation is provably superior (Theorems 1–7) and empirically better (+34%\ncost reduction).\nThis suggests a broader principle: for decision-making under uncertainty, treat LLMs as components in proba-\nbilistic inference systems, not as end-to-end classifiers.\nMulti-model systems as the new norm.\nThe era of single-model deployment may be ending. As LLMs proliferate\n(GPT-4, Claude, Gemini, Llama, Mistral, DeepSeek, Grok, and countless others), organizations face a portfolio of\noptions with different costs, capabilities, and biases.\nRather than picking a single ”best” model, our framework suggests: aggregate across diverse models to exploit\ncomplementary strengths and mitigate correlated failures.\nThis is analogous to Modern Portfolio Theory in finance [73]: diversification reduces risk when assets are imperfectly\ncorrelated. Similarly, multi-model ensembles reduce decision error when models are imperfectly correlated.\nEmpirically, we showed:\n• Multi-LLM median aggregation saves $126K (22%) vs. single best model\n• Fairness gap drops from 22pp to 5pp (76% reduction)\n• Calibration improves from ECE=0.18 to 0.09 (50% reduction)\nAs model diversity increases (new architectures, training paradigms, data sources), the benefits of ensembling will\nlikely grow.\n43\n"}, {"page": 44, "text": "A PREPRINT - JANUARY 6, 2026\nRethinking evaluation metrics.\nML research typically evaluates models on accuracy, F1, or perplexity—metrics\nthat ignore decision costs and treat all errors equally. Our work demonstrates the inadequacy of these metrics for\ndeployment:\n• GPT-4o achieves 68% accuracy but costs $856K\n• Our framework achieves 82% accuracy and costs $562K\n• Accuracy improved by 14pp (21% relative), cost improved by 34%\nCost reduction outpaces accuracy improvement because the framework makes cost-effective errors (e.g., occasionally\ninterviewing an s2 candidate who might fail, avoiding the 16× costlier error of rejecting an s4 candidate).\nThis suggests: evaluate LLM agents on decision quality (expected cost/utility), not prediction quality (accuracy).\nShifting evaluation paradigms from accuracy →expected utility would incentivize research on:\n• Calibration and uncertainty quantification (not just argmax prediction)\n• Cost-sensitive learning (weighting errors by real-world consequences)\n• Sequential decision-making (not just one-shot classification)\n• Fairness under cost constraints (demographic parity with bounded cost increase)\n4.5\nEthical Considerations\nDeploying LLMs for high-stakes decisions raises ethical questions that technical improvements alone cannot resolve.\nAutomation and job displacement.\nOur framework reduces hiring costs by $294K per 1,000 candidates, raising\nconcerns about recruiter job displacement. However:\n• The framework assists rather than replaces recruiters: phone screens are still conducted by humans, and\nborderline cases are flagged for human review.\n• Efficiency gains enable recruiters to spend more time on high-value activities (candidate experience, employer\nbranding, interview coaching) rather than manual resume screening.\n• Better hiring outcomes (fewer mis-hires) benefit both organizations and candidates.\nNonetheless, organizations deploying such systems should consider: (1) retraining programs for affected workers,\n(2) human-in-the-loop design to preserve recruiter agency, and (3) transparency with candidates about automated\nscreening.\nAlgorithmic fairness and discrimination.\nWe demonstrated 76% reduction in demographic parity gap (22pp →\n5pp), but 5pp still slightly exceeds the 4pp legal threshold in NYC Local Law 144. Moreover:\n• Demographic parity is one fairness criterion among many (equality of opportunity, calibration by group,\nindividual fairness). Satisfying all simultaneously is often impossible [57].\n• Fairness metrics are contested: some argue demographic parity is too strict (requires equal outcomes even\nif group-level differences in qualifications exist), while others argue it’s insufficient (allows within-group\ndisparities).\n• Our framework reduces bias relative to current practice, but ”less biased” is not the same as ”unbiased.”\nOrganizations must make normative choices about fairness criteria and acceptable trade-offs. Our framework provides\ntransparency (explicit posteriors, interpretable decisions) but cannot resolve philosophical disagreements about justice.\nTransparency and explainability.\nBayesian decision-making offers inherent interpretability: we can show users\n(candidates, regulators, auditors) exactly why a decision was made:\n• Posterior beliefs: ”We assigned 52% probability to interview-worthy, 35% to phone screen, 13% to reject”\n• Evidence contributions: ”Your resume reduced reject probability from 65% to 13%, but phone screen revealed\nconcerns that increased reject to 58%”\n• Cost-benefit analysis: ”We screened you because expected decision improvement ($180) exceeded screen\ncost ($150)”\n44\n"}, {"page": 45, "text": "A PREPRINT - JANUARY 6, 2026\nThis transparency is valuable for accountability and contestability. However, it also exposes model limitations (ap-\nproximate likelihoods, imperfect calibration) that may reduce user trust.\n4.6\nRecommendations for Practitioners\nFor organizations considering deploying our framework:\n1. Start with explicit prior elicitation. Invest effort in measuring deployment base rates π(s) from historical\ndata. This is the single highest-ROI component (41% cost impact in ablations).\n2. Use diverse model ensembles. Don’t rely on a single LLM. Select 3–5 models with different training data,\narchitectures, and release dates to maximize uncorrelated errors.\n3. Calibrate cost matrices carefully. Work with domain experts (recruiters, clinicians, fraud analysts) to esti-\nmate C(a, s) based on real outcomes. Sensitivity analysis can assess robustness to estimation error.\n4. Validate on held-out data before deployment. Test the framework on historical decisions with known\noutcomes. Measure calibration, fairness, and total cost against baselines.\n5. Implement human-in-the-loop for high-uncertainty cases. When disagreement exceeds τD or VOI is\nborderline, flag for human review rather than fully automating.\n6. Monitor and update continuously. Track performance metrics (cost, fairness, calibration) over time. Update\nπ as base rates shift, recalibrate ρ as information sources evolve, and retune thresholds as costs change.\n7. Be transparent with stakeholders. Disclose to candidates that automated screening is used, provide opt-out\nor appeal mechanisms, and explain decisions when requested.\n4.7\nOpen Questions and Future Directions\nOur work opens several research directions:\n1. Theoretical foundations: We proved impossibility results for discriminative models but did not fully char-\nacterize the sample complexity or approximation error of likelihood elicitation. How many LLM queries\nare needed to estimate p(x|s) within ϵ error? How does this scale with state space size |S| and observation\ncomplexity?\n2. Active learning for VOI: Can we learn better VOI approximations online? If the framework gathers informa-\ntion on 273 candidates and observes actual decision improvement, this provides training data for supervised\nVOI prediction.\n3. Hierarchical and continuous state spaces: We used discrete states (K = 4). Many domains have hierar-\nchical (e.g., candidate quality →technical skill →language proficiency) or continuous states (e.g., medical\nrisk scores). How should likelihood elicitation and Bayesian updating be adapted?\n4. Multi-agent and strategic settings: Our framework assumes passive observations (resumes don’t change\nbased on model predictions). In adversarial settings (fraud, security, gaming), agents strategically manipulate\nevidence. How should Bayesian updating account for strategic responses?\n5. Integration with human judgment: How should the framework combine LLM likelihoods with human\nexpert assessments? Bayesian models exist for expert aggregation [25], but integrating them with LLM\nensembles is unexplored.\n6. Cross-domain transfer: We instantiated the framework in hiring. How well do likelihood elicitation prompts\ntransfer to medical triage, credit underwriting, customer service escalation? Can we develop domain-agnostic\nprompt templates?\n5\nRelated Work\nOur work sits at the intersection of several research areas: foundation model deployment, sequential decision-making\nunder uncertainty, algorithmic fairness, and ensemble methods. We position our contributions relative to prior work in\neach area.\n45\n"}, {"page": 46, "text": "A PREPRINT - JANUARY 6, 2026\n5.1\nFoundation Models for Decision-Making\nThe deployment of large language models (LLMs) and other foundation models for high-stakes decision-making has\nreceived growing attention, though most work focuses on accuracy rather than decision-theoretic optimality.\nLLMs as classifiers. The dominant paradigm treats LLMs as discriminative classifiers via prompting: ask the model to\npredict a label given evidence [16, 58, 118]. Recent work has demonstrated impressive zero-shot and few-shot classifi-\ncation performance across domains including sentiment analysis [129], medical diagnosis [102], and resume screening\n[65]. However, these approaches inherit the limitations we formalize in Section 2: inability to perform sequential be-\nlief updates (Theorem 1), sensitivity to training-deployment prior mismatch (Theorem 2), and cost-insensitive decision\nboundaries (Theorem 6).\nChain-of-thought and reasoning. Chain-of-thought prompting [118] and related techniques [125, 9] improve LLM\nreasoning by eliciting intermediate steps. While these methods enhance accuracy, they do not address the funda-\nmental issues we identify: they still produce discriminative outputs p(s|x) rather than generative likelihoods p(x|s),\nprecluding sequential updating and VOI-driven information gathering.\nLLM agents and tool use. Recent work on LLM agents enables models to use external tools, plan multi-step actions,\nand interact with environments [95, 87, 122]. Our framework complements this line of work: while tool-use agents\nfocus on what actions the model can take, we focus on how to decide which actions to take under uncertainty with\nasymmetric costs.\nMost closely related is ReAct [?], which interleaves reasoning and acting. However, ReAct uses heuristic decision\nrules (“if uncertain, gather more information”) rather than principled VOI calculations, and does not address cost\nasymmetries or fairness.\n5.2\nBayesian Inference with Neural Models\nOur approach builds on the long history of combining neural networks with probabilistic inference.\nBayesian neural networks. Traditional Bayesian neural networks (BNNs) [77, 11, 43] place priors over network\nweights to quantify uncertainty. However, BNNs are typically used for epistemic uncertainty in model parameters, not\nfor sequential decision-making over latent states. Moreover, BNN inference is computationally expensive and does\nnot scale to LLM-sized models.\nProbabilistic programming. Systems like Stan [19], Pyro [10], and Gen [29] enable Bayesian inference in complex\ngenerative models. Our framework can be viewed as a form of probabilistic programming where LLMs serve as\nlearned likelihood functions within a hand-specified graphical model (states s, observations xt, actions at). However,\nwe do not require gradient-based inference; instead, we directly query LLMs for likelihood estimates.\nAmortized inference. Variational autoencoders [56] and normalizing flows [90] learn to amortize inference by train-\ning encoder networks that map observations to approximate posteriors. Our approach differs: we use LLMs’ pretrained\nknowledge as likelihood functions without additional training, enabling zero-shot generalization to new decision do-\nmains.\nNeural likelihood-free inference. Simulation-based inference methods [28, 83] train neural networks to perform\nBayesian inference when likelihoods are intractable. Our work inverts this: we have (approximate) access to likeli-\nhoods via LLM prompting, and use classical Bayes’ rule for inference.\n5.3\nSequential Decision-Making Under Uncertainty\nOur framework instantiates classical decision theory [113, 89, 7] with modern LLM components.\nPartially observable Markov decision processes (POMDPs). POMDPs [54, 105] formalize sequential decision-\nmaking when the true state is hidden. Our problem is a special case of a POMDP where the state is static (candidate\nquality doesn’t change), observations are conditionally independent given the state, and the goal is to minimize cost\nrather than maximize discounted reward. POMDP solvers typically require explicit state transition and observation\nmodels; we use LLMs as learned observation models.\nActive learning and information gathering. Active learning [97, 24] selects which data to label to maximize model\nimprovement. Our VOI framework shares the spirit of active learning but differs in objective: we gather information\nto improve decisions (minimize cost), not to improve models (reduce uncertainty for its own sake). Bayesian exper-\nimental design [21, 93] is more closely related, particularly work on sequential experimental design [69]. Our VOI\napproximation (Equation 74) is a practical instance of classical information theory applied to decision problems.\n46\n"}, {"page": 47, "text": "A PREPRINT - JANUARY 6, 2026\nBandits and Bayesian optimization. Multi-armed bandits [64] and Bayesian optimization [98, 39] balance explo-\nration (gathering information) and exploitation (taking optimal actions). Our problem differs: we have a finite decision\nhorizon (hire/reject each candidate), static states (quality doesn’t evolve), and explicit costs (not just opportunity costs).\nHowever, the upper confidence bound (UCB) and Thompson sampling principles that guide bandits [1] resonate with\nour disagreement-triggered screening: high inter-model disagreement signals high uncertainty, warranting information\ngathering.\nCost-sensitive learning. Cost-sensitive classification [36, ?] trains models to minimize expected cost rather than error\nrate. Approaches include reweighting training examples [127], threshold adjustment [99], and direct cost minimization\n[4]. Our framework differs: we do not retrain LLMs (which is often infeasible for proprietary models), but instead\nperform cost-aware decision-making post hoc via Bayesian inference.\n5.4\nEnsemble Methods and Multi-Model Aggregation\nOur median-based ensemble builds on a rich literature on combining multiple models.\nClassical ensembles. Bagging [15], boosting [40], and stacking [121] combine models trained on different data\nsubsets or with different algorithms. Our ensemble is different: we aggregate diverse pretrained LLMs that we do not\ncontrol, rather than training multiple models ourselves. This is closer to ensemble selection [20] or model portfolios\n[124].\nRobust statistics for aggregation. Our use of median aggregation is motivated by robust statistics [51, 46]. The\nmedian’s high breakdown point [33] makes it resilient to outliers, which is critical when aggregating LLMs with un-\nknown and potentially adversarial biases. Alternative robust estimators like trimmed means [107] or Hodges-Lehmann\nestimator [49] could be explored. Our Theorem 8 provides theoretical justification for median aggregation in the like-\nlihood estimation setting.\nExpert aggregation and Delphi methods. Our framework resembles expert aggregation methods [23, 25] where\nmultiple human experts provide judgments that are combined. The Delphi method [31, 92] iteratively refines expert\nconsensus. Our approach is similar but with LLM ”experts” providing likelihood estimates. Unlike Delphi (which\nseeks consensus), we preserve diversity via median aggregation rather than forcing agreement.\nLLM ensembles. Recent work has explored ensembles of LLMs for improved performance [117, 52, 66]. Self-\nconsistency [117] samples multiple outputs from a single LLM and takes the majority vote. LLM-blender [52] learns\nto combine outputs from multiple models. Our work differs in combining likelihoods (not outputs) and using robust\naggregation (median) rather than learned weighting.\n5.5\nAlgorithmic Fairness in High-Stakes Decisions\nOur focus on demographic parity and bias mitigation connects to the broader fairness literature.\nFairness definitions and impossibility results. Multiple fairness criteria exist—demographic parity, equalized odds,\ncalibration, individual fairness—and satisfying all simultaneously is often impossible [57, 22, 26]. We focus on\ndemographic parity (equal selection rates across groups) as it is legally mandated in hiring contexts [78], though our\nframework could be adapted to other fairness metrics.\nBias in LLMs. Substantial evidence documents bias in LLMs along dimensions of gender [75, 60], race [67, 76], and\nother protected attributes [119, 6]. Biases arise from training data [12], model architecture [13], and alignment pro-\ncesses [82]. Our multi-LLM approach mitigates bias by aggregating models with diverse (and partially uncorrelated)\nbiases. This is reminiscent of portfolio theory [73]: diversification reduces risk when assets (models) have imperfect\ncorrelation.\nDebiasing techniques. Debiasing approaches include data augmentation [130], adversarial training [128], and post-\nprocessing [47]. For LLMs specifically, techniques include prompt engineering [101], fine-tuning on debiased data\n[44], and inference-time interventions [71]. Our approach is complementary: ensemble aggregation reduces bias with-\nout requiring model retraining (beneficial for proprietary APIs) and can be combined with other debiasing methods.\nFairness in hiring algorithms. Hiring is a particularly scrutinized domain [88, 120]. Legal frameworks like NYC\nLocal Law 144 [78] mandate bias audits for automated employment decision tools. Empirical studies show that resume\nscreening algorithms can perpetuate discrimination [63, 27]. Our framework addresses these concerns through: (1)\nmulti-model aggregation reducing individual model biases, (2) explicit prior specification enabling domain adaptation,\nand (3) transparent Bayesian posteriors supporting audits and appeals.\n47\n"}, {"page": 48, "text": "A PREPRINT - JANUARY 6, 2026\n5.6\nEvaluation and Benchmarking of LLMs\nOur experimental methodology relates to recent work on rigorous LLM evaluation.\nBenchmarks and datasets. Numerous benchmarks evaluate LLMs: GLUE [115], SuperGLUE [114], MMLU [48],\nBIG-Bench [106]. These focus on accuracy across diverse tasks. Our contribution is not a new benchmark but a new\nevaluation paradigm: measuring decision quality (expected cost) rather than prediction quality (accuracy). This aligns\nwith recent calls for task-specific evaluation [68] and consideration of downstream impacts [6].\nCalibration and uncertainty. Calibration—whether model confidence matches actual accuracy—has been studied\nextensively [45, 80]. LLMs are known to be poorly calibrated [53, 123], often overconfident. Our framework im-\nproves calibration (ECE 0.09 vs. 0.18 for raw GPT-4o) through Bayesian inference with correct priors and ensemble\naggregation. This resembles ensemble calibration methods [62] and temperature scaling [45].\nSynthetic data for evaluation. We use synthetic resumes with ground-truth labels validated by human experts. This\napproach enables controlled experiments with known ground truth, avoiding label noise in real-world datasets. Similar\nmethodologies appear in fairness research [41] and causal inference [100]. The trade-off is reduced external validity;\nour Discussion (Section 4) addresses this limitation.\n5.7\nPositioning Our Contributions\nOur work makes several novel contributions relative to prior literature:\n1. Formal impossibility results (Section 2). We prove that discriminative LLM architectures are fundamentally\ninsufficient for sequential decision-making (Theorems 1–7). While the intuition that ”classifiers can’t do\nsequential inference” is well-known in Bayesian statistics, we formalize this for LLMs and characterize\nexactly what capabilities are missing.\n2. Likelihood elicitation from LLMs (Section 2). Our contrastive prompting technique (Definition 2) inverts\nthe conditional probability direction, enabling sequential Bayesian inference. This differs from chain-of-\nthought (which improves discriminative reasoning) and probabilistic programming (which requires explicit\ngenerative models).\n3. Median-based ensemble for bias mitigation (Sections 2, 3.3). We show that robust aggregation across\ndiverse LLMs reduces demographic bias by 76% (22pp gap →5pp). This is a practical debiasing technique\nrequiring no model retraining, distinct from data augmentation or adversarial training.\n4. VOI-driven information gathering (Section 2). We adapt classical VOI theory to LLM-based decision-\nmaking with a practical approximation (Equation 74). This enables principled information gathering, unlike\nheuristic approaches in prior LLM agent work.\n5. Empirical validation of cost-aware decisions (Section 3). We demonstrate 34% cost reduction ($294K\nsavings) on a hiring task, showing that decision-theoretic framing yields substantial practical improvements\nover accuracy-maximizing baselines. This shifts evaluation from predictive accuracy to decision quality.\n6. Comprehensive ablation studies (Section 3).\nWe isolate contributions of each framework component\n(multi-LLM, sequential updating, VOI, prior correction), finding all components necessary (ablating any\nincreases cost by 4–41%).\nIn summary, our framework synthesizes ideas from Bayesian decision theory (which provides normative foundations),\nrobust statistics (which guides aggregation), and LLM prompting (which enables practical implementation), yielding\na novel approach to deploying foundation models in high-stakes sequential decision problems. While each component\nhas precedent in prior work, their combination—and the formal characterization of why this combination is neces-\nsary—is, to our knowledge, new.\n6\nConclusion\nWe have presented a principled framework for deploying LLMs in high-stakes sequential decision-making under\nuncertainty. By treating LLMs as approximate likelihood functions rather than discriminative classifiers, we enable\nfour critical capabilities absent from current architectures: sequential belief updating, prior correction for domain\nadaptation, cost-aware action selection, and value-of-information-driven information gathering.\nOur approach rests on rigorous mathematical foundations: we proved that discriminative LLM outputs are formally\ninsufficient for sequential decision-making (Theorems 1–7), established optimality properties of our generative alter-\nnative (Theorems 8, 9, 10), and instantiated the framework with complete algorithmic specification.\n48\n"}, {"page": 49, "text": "A PREPRINT - JANUARY 6, 2026\nEmpirical evaluation on 1,000 resume screening decisions demonstrates substantial improvements: 34% cost reduc-\ntion ($294K savings), 45% fairness improvement (demographic parity gap from 22pp to 5pp), and 50% calibration\nimprovement (ECE from 0.18 to 0.09). Systematic ablations reveal that all components contribute: multi-LLM aggre-\ngation (51% of gains), sequential updating (43%), VOI-driven screening (20%), and prior correction (41%, overlapping\nwith others).\nBeyond these quantitative results, our work makes a broader conceptual contribution: it reframes LLM deployment\nfrom an engineering problem (”How do we get models to output good decisions?”) to a probabilistic inference problem\n(”How do we combine imperfect models into principled belief updating and decision-making?”). This shift—from dis-\ncriminative to generative modeling, from accuracy maximization to expected utility optimization, from single models\nto diverse ensembles—has implications extending far beyond hiring to any high-stakes sequential decision problem.\nAs LLMs become increasingly capable and ubiquitous, the question is not whether they will be deployed in conse-\nquential decisions, but how they will be deployed. Our framework provides a mathematically grounded, empirically\nvalidated answer: treat them as uncertain probabilistic sensors within Bayesian decision systems, not as autonomous\noracles. This approach leverages their strengths (flexible reasoning over diverse evidence) while mitigating their\nweaknesses (uncalibrated confidence, hidden biases, inability to perform sequential updating).\nThe path forward requires continued research on likelihood elicitation, multi-model aggregation, and sequential deci-\nsion theory, coupled with careful attention to fairness, transparency, and human oversight. We hope our work provides\nboth the theoretical tools and empirical evidence to guide this development.\n49\n"}, {"page": 50, "text": "A PREPRINT - JANUARY 6, 2026\nA\nFormal Proofs\nThis appendix provides complete formal proofs for all theorems stated in the main text. We present proofs in order of\nappearance.\nA.1\nProof of Theorem 1: Sequential Updating Impossibility\nTheorem (Sequential Updating Impossibility, restated). Let Mdisc be a discriminative model that maps any obser-\nvation x ∈X to a probability distribution pM(s|x) over states s ∈S, but does not separately expose the likelihood\nfunction p(x|s) or prior p(s). Then for observed evidence x1 and new evidence x2, there exists no computable function\nf : ∆(S) × X →∆(S) such that:\np(s|x1, x2) = f(pM(s|x1), x2)\n∀s ∈S, x1, x2 ∈X\n(87)\nwithout re-querying Mdisc on the joint observation (x1, x2).\nProof. We proceed by showing that computing p(s|x1, x2) from pM(s|x1) requires information not contained in\npM(s|x1).\nStep 1: Sequential Bayesian update formula. By Bayes’ rule:\np(s|x1, x2) = p(x2|s, x1) · p(s|x1)\np(x2|x1)\n(88)\nwhere the normalizing constant is:\np(x2|x1) =\nX\ns′∈S\np(x2|s′, x1) · p(s′|x1)\n(89)\nStep 2: What information does pM(s|x1) contain? The discriminative model’s output can be decomposed via\nBayes’ rule:\npM(s|x1) = p(x1|s) · ptrain(s)\np(x1)\n(90)\nwhere ptrain(s) is the implicit prior embedded in the model’s weights from training, and:\np(x1) =\nX\ns′∈S\np(x1|s′) · ptrain(s′)\n(91)\nThe model provides pM(s|x1) as a single quantity but does not separately expose:\n• The likelihood p(x1|s)\n• The training prior ptrain(s)\n• The marginal p(x1)\nStep 3: Attempting to extract the likelihood. Suppose we knew ptrain(s) and p(x1). We could then attempt to\nrecover:\np(x1|s) = pM(s|x1) · p(x1)\nptrain(s)\n(92)\nHowever, this creates a circular dependency: p(x1) itself depends on p(x1|s) via Equation 90. We have:\np(x1) =\nX\ns′\np(x1|s′)ptrain(s′) =\nX\ns′\npM(s′|x1) · p(x1)\nptrain(s′)\n· ptrain(s′)\n(93)\nSimplifying:\np(x1) = p(x1)\nX\ns′\npM(s′|x1) = p(x1) · 1 = p(x1)\n(94)\n50\n"}, {"page": 51, "text": "A PREPRINT - JANUARY 6, 2026\nThis is a tautology that does not allow us to solve for p(x1) or p(x1|s) uniquely.\nStep 4: Even if we could extract likelihoods for x1, we cannot predict likelihoods for x2. Suppose hypothetically\nthat we managed to recover p(x1|s) for all s. To perform the sequential update in Equation 88, we need p(x2|s, x1)—\nthe likelihood of the new evidence x2 given state s and prior evidence x1.\nThe discriminative model has never observed x2, so pM(s|x1) contains no information about how x2 would be dis-\ntributed conditional on s and x1. The only way to obtain p(x2|s, x1) is to either:\n1. Have a separate generative model p(x|s) that we can query with (x2, s), or\n2. Re-query the discriminative model with (x1, x2) jointly to get pM(s|x1, x2), which implicitly uses\np(x2|s, x1) internally but does not expose it.\nOption (1) is exactly what our framework does (we elicit likelihoods). Option (2) is batch inference, which forfeits\nsequential updating.\nStep 5: Conclusion. There exists no function f that computes p(s|x1, x2) from pM(s|x1) and x2 alone, because:\n• Computing the update requires p(x2|s, x1), which is not contained in pM(s|x1)\n• Even if we could extract p(x1|s) from pM(s|x1) (which we showed is impossible without additional infor-\nmation), this does not give us p(x2|s, x1) for the new observation x2\nTherefore, sequential updating is impossible in the discriminative paradigm.\nA.2\nProof of Theorem 2: Prior Correction Impossibility\nTheorem (Prior Correction Impossibility, restated). Let Mdisc output posterior pM(s|x) with implicit training prior\nptrain(s) embedded in its weights. Let pdeploy(s) ̸= ptrain(s) denote the correct deployment prior. Then without explicit\naccess to likelihood p(x|s) and knowledge of ptrain(s), there exists no function h : ∆(S) × ∆(S) →∆(S) such that:\nh(pM(s|x), pdeploy(s)) =\np(x|s) · pdeploy(s)\nP\ns′∈S p(x|s′) · pdeploy(s′)\n∀s ∈S, x ∈X\n(95)\nProof. We show that applying Bayes’ rule with a new prior requires the likelihood function, which cannot be recovered\nfrom the discriminative posterior.\nStep 1: Desired posterior under deployment prior. We want to compute:\npcorrect(s|x) = p(x|s) · pdeploy(s)\nZdeploy\n(96)\nwhere Zdeploy = P\ns′ p(x|s′)pdeploy(s′) is the normalizing constant under the deployment prior.\nStep 2: Information available from discriminative model. The model provides:\npM(s|x) = p(x|s) · ptrain(s)\nZtrain\n(97)\nwhere Ztrain = P\ns′ p(x|s′)ptrain(s′).\nStep 3: Attempting to extract the likelihood. Rearranging Equation 97:\np(x|s) = pM(s|x) · Ztrain\nptrain(s)\n(98)\nThis requires knowing both ptrain(s) and Ztrain. The model does not expose either.\nCase 1: Assume we know ptrain(s) but not Ztrain.\nWe can compute likelihood ratios:\np(x|si)\np(x|sj) = pM(si|x)/ptrain(si)\npM(sj|x)/ptrain(sj)\n(99)\n51\n"}, {"page": 52, "text": "A PREPRINT - JANUARY 6, 2026\nHowever, ratios are insufficient for computing the posterior under a new prior. To see why, attempt to construct the\nnew posterior:\npcorrect(s|x) = p(x|s) · pdeploy(s)\nZdeploy\n(100)\nWe know p(x|s) only up to a multiplicative constant Ztrain. Let ˜p(x|s) = p(x|s)/Ztrain be the unnormalized likelihood\nwe can compute from Equation 98. Then:\npcorrect(s|x) =\n˜p(x|s) · Ztrain · pdeploy(s)\nP\ns′ ˜p(x|s′) · Ztrain · pdeploy(s′) =\n˜p(x|s) · pdeploy(s)\nP\ns′ ˜p(x|s′) · pdeploy(s′)\n(101)\nThe Ztrain factors cancel! So if we know ptrain(s), we can correct the prior even without knowing Ztrain, using unnor-\nmalized likelihoods.\nCase 2: We do not know ptrain(s).\nThis is the realistic scenario: the model’s training prior is embedded opaquely in its weights and not reported. Without\nptrain(s), we cannot extract even likelihood ratios from Equation 99.\nAttempting to infer ptrain(s) from model outputs. Could we infer the training prior by observing pM(s|x) for\nmultiple observations x? Consider two observations x1, x2:\npM(s|x1) =\np(x1|s) · ptrain(s)\nP\ns′ p(x1|s′)ptrain(s′)\n(102)\npM(s|x2) =\np(x2|s) · ptrain(s)\nP\ns′ p(x2|s′)ptrain(s′)\n(103)\nWe have 2K equations (where K = |S|) but 3K unknowns: K values of ptrain(s), K likelihoods p(x1|s), and K\nlikelihoods p(x2|s). The system is underdetermined.\nMoreover, the likelihoods and prior are not separately identifiable: for any scalar c > 0, the following transformations\nleave the posteriors unchanged:\np(x|s) →c · p(x|s),\nptrain(s) →ptrain(s)\nc\n(104)\nThis scaling ambiguity prevents us from recovering the prior from discriminative outputs alone.\nStep 4: Conclusion. Without access to:\n1. The likelihood function p(x|s), or\n2. The training prior ptrain(s)\nwe cannot construct the correct posterior pcorrect(s|x) under deployment prior pdeploy(s). The function h cannot exist.\nThe only way to correct for prior mismatch is to have explicit access to likelihoods (our generative approach) or to\nknow the training prior and extract likelihood ratios (which still requires the model to report ptrain, which current LLMs\ndo not do).\nA.3\nProof of Theorem 6: Cost-Insensitive Threshold Suboptimality\nTheorem (Cost-Insensitive Threshold Suboptimality, restated). For any asymmetric cost matrix C : A × S →R≥0\nwith C(ai, sj) ̸= C(ak, sℓ) for some pairs, the Bayes-optimal action selection rule is:\na∗(x) = arg min\na∈A\nX\ns∈S\np(s|x) · C(a, s)\n(105)\nNo threshold-based rule of the form “if score(x) ≥τ then a1, else a2” can be optimal for all x and all cost structures,\nas the decision boundary depends on both p(s|x) and the cost matrix C.\nProof. We prove by counterexample and then provide a general characterization.\nPart 1: Counterexample showing threshold rules are suboptimal.\n52\n"}, {"page": 53, "text": "A PREPRINT - JANUARY 6, 2026\nConsider a binary classification problem: S = {s0, s1} (negative and positive class), A = {a0, a1} (predict negative,\npredict positive). Let the cost matrix be:\nC =\n\u0014\nc00\nc01\nc10\nc11\n\u0015\n(106)\nwhere cij = C(ai, sj) is the cost of taking action ai when the true state is sj.\nThe expected cost of action a1 (predict positive) is:\nE[C(a1, s)] = p(s0|x) · c10 + p(s1|x) · c11\n(107)\nThe expected cost of action a0 (predict negative) is:\nE[C(a0, s)] = p(s0|x) · c00 + p(s1|x) · c01\n(108)\nWe should choose a1 (predict positive) when:\np(s0|x) · c10 + p(s1|x) · c11 < p(s0|x) · c00 + p(s1|x) · c01\n(109)\nRearranging:\np(s0|x)(c10 −c00) < p(s1|x)(c01 −c11)\n(110)\np(s0|x)(c10 −c00) < (1 −p(s0|x))(c01 −c11)\n(111)\np(s0|x)[(c10 −c00) + (c01 −c11)] < (c01 −c11)\n(112)\nTherefore:\np(s1|x) >\nc10 −c00\n(c10 −c00) + (c01 −c11)\n(113)\nThe optimal threshold τ ∗depends on all four cost values. For example:\n• If c00 = 0, c01 = 10, c10 = 1, c11 = 0 (false negatives are 10× costlier than false positives), then:\nτ ∗=\n1 −0\n(1 −0) + (10 −0) = 1\n11 ≈0.091\n(114)\n• If c00 = 0, c01 = 1, c10 = 1, c11 = 0 (symmetric costs), then:\nτ ∗=\n1 −0\n(1 −0) + (1 −0) = 1\n2 = 0.5\n(115)\n• If c00 = 0, c01 = 1, c10 = 10, c11 = 0 (false positives are 10× costlier than false negatives), then:\nτ ∗=\n10 −0\n(10 −0) + (1 −0) = 10\n11 ≈0.909\n(116)\nA fixed threshold (e.g., τ = 0.5, maximizing accuracy) is optimal only for the second cost structure (symmetric costs).\nFor the first cost structure, it rejects 90.9% - 50% = 40.9% of the probability mass that should be classified as positive,\nincurring excessive false negative costs. For the third cost structure, it accepts 50% - 9.1% = 40.9% of the probability\nmass that should be rejected, incurring excessive false positive costs.\nPart 2: General characterization for |S| > 2 and |A| > 2.\nFor K > 2 states and J > 2 actions, the Bayes-optimal action at observation x is:\na∗(x) = arg min\na∈A\nX\ns∈S\np(s|x) · C(a, s)\n(117)\nThis is a piecewise-constant function of p(s|x) that partitions the probability simplex ∆(S) into regions, one per\naction. The boundaries between regions are hyperplanes determined by the cost matrix.\nSpecifically, the boundary between actions ai and aj is the set of posteriors p(·|x) satisfying:\nX\ns\np(s|x)[C(ai, s) −C(aj, s)] = 0\n(118)\n53\n"}, {"page": 54, "text": "A PREPRINT - JANUARY 6, 2026\nThis is a linear constraint in p(s|x), defining a (K −1)-dimensional hyperplane in the K-dimensional simplex.\nA threshold rule “if score(x) ≥τ then a1 else a2” corresponds to a decision boundary orthogonal to a single axis of the\nsimplex (the axis corresponding to score, typically p(spositive|x)). This is a special case of the hyperplane boundaries\nabove, occurring only when the cost matrix has a specific structure:\nC(ai, s) −C(aj, s) = α · ⊮[s = spositive]\n(119)\nfor some scalar α and designated positive state spositive. In general, cost matrices do not have this structure, so threshold\nrules are suboptimal.\nStep 3: Conclusion. Threshold-based rules are optimal only for a measure-zero subset of cost matrices (those with\nthe specific structure above). For arbitrary asymmetric cost matrices, the Bayes action rule (Equation 69) is optimal,\nand threshold rules are strictly suboptimal.\nA.4\nProof of Theorem 7: Value-of-Information Computation Impossibility\nTheorem (VOI Computation Impossibility, restated). Computing the value of gathering additional evidence z re-\nquires:\nVOI(z) = Ez\n\"\nmin\na\nX\ns\np(s|x, z)C(a, s)\n#\n−min\na\nX\ns\np(s|x)C(a, s)\n(120)\nThis requires marginalizing over hypothetical observations z ∼p(z|x) = P\ns p(z|s)p(s|x), which demands likelihood\nfunctions p(z|s) that discriminative models do not provide.\nProof. We show that computing VOI requires predictions about unobserved data, which necessitates generative mod-\neling.\nStep 1: VOI definition and expansion. The value of information is defined as the expected improvement in decision\nquality from observing z before acting:\nVOI(z|x) = Ez∼p(·|x) [V ∗(x, z)]\n|\n{z\n}\nexpected value with info\n−\nV ∗(x)\n| {z }\nvalue without info\n(121)\nwhere V ∗(x) = mina∈A\nP\ns p(s|x)C(a, s) is the value of the best action given current evidence x only, and\nV ∗(x, z) = mina∈A\nP\ns p(s|x, z)C(a, s) is the value after observing additional evidence z.\nExpanding the expectation:\nVOI(z|x) =\nZ\nX\np(z|x)\n\"\nmin\na\nX\ns\np(s|x, z)C(a, s)\n#\ndz −min\na\nX\ns\np(s|x)C(a, s)\n(122)\nStep 2: Requirements for computing VOI.\nTo evaluate Equation 122, we need three components:\nComponent 1: Predictive distribution p(z|x). This is the distribution over possible future observations z given current\nevidence x. By the law of total probability:\np(z|x) =\nX\ns∈S\np(z|s, x) · p(s|x)\n(123)\nUnder the conditional independence assumption z ⊥x|s (future evidence is independent of past evidence given the\nstate), this simplifies to:\np(z|x) =\nX\ns∈S\np(z|s) · p(s|x)\n(124)\nThis requires the likelihood function p(z|s) for each state s.\nComponent 2: Updated posterior p(s|x, z). For each hypothetical observation z, we need to compute how it would\nupdate our beliefs. By Bayes’ rule:\np(s|x, z) = p(z|s, x) · p(s|x)\np(z|x)\n=\np(z|s) · p(s|x)\nP\ns′ p(z|s′) · p(s′|x)\n(125)\n54\n"}, {"page": 55, "text": "A PREPRINT - JANUARY 6, 2026\nAgain, this requires the likelihood p(z|s).\nComponent 3: Optimal action value V ∗(x, z). For each hypothetical (x, z) pair, compute:\nV ∗(x, z) = min\na∈A\nX\ns\np(s|x, z)C(a, s)\n(126)\nThis requires p(s|x, z) from Component 2.\nStep 3: Discriminative models cannot provide the required components.\nA discriminative model Mdisc outputs pM(s|x) for observed x. It does not provide:\n• The likelihood p(z|s) for unobserved future evidence z\n• The predictive distribution p(z|x) over possible future observations\n• The ability to compute p(s|x, z) for hypothetical z without actually observing z and re-querying\nThe fundamental issue is that VOI calculation is inherently a generative task: we need to imagine what observations\nz we might see in the future and how they would affect our decisions. Discriminative models only perform inverse\ninference: from observations to states. They cannot perform forward prediction: from states to possible observations.\nAttempted workaround: Re-query for each hypothetical z.\nCould we enumerate all possible observations z ∈X, query Mdisc(s|x, z) for each, and average the resulting decision\nvalues? This faces two problems:\nProblem 1: Infinite enumeration. For continuous observation spaces X (e.g., real-valued measurements, text), there\nare uncountably many possible z. We cannot enumerate and query each.\nProblem 2: Unknown weights p(z|x). Even if we discretize X to a finite set, we need to weight each z by its probability\np(z|x) in the expectation (Equation 122). But computing p(z|x) requires likelihoods (Equation 124), which the\ndiscriminative model doesn’t provide. Without these weights, we don’t know which hypothetical observations are\nplausible vs. implausible.\nStep 4: Conclusion.\nComputing VOI requires likelihood functions p(z|s) to:\n1. Predict the distribution of future observations p(z|x) = P\ns p(z|s)p(s|x)\n2. Update beliefs counterfactually p(s|x, z) ∝p(z|s)p(s|x)\n3. Weight hypothetical observations by their probability when averaging\nDiscriminative models provide none of these. Therefore, VOI computation is impossible without generative modeling\ncapabilities.\nThe only recourse in the discriminative paradigm is to use heuristics (always gather information, never gather infor-\nmation, or gather when confidence falls in some arbitrary range) that do not properly account for expected decision\nimprovement vs. information cost.\nA.5\nProof of Theorem 8: Median Error Bound\nTheorem (Median Robustness Bound, restated). Let {ˆLm(x|s)}M\nm=1 be likelihood estimates from M models, and let\nL∗(x|s) denote the true likelihood. Define individual errors ϵm = |ˆLm(x|s) −L∗(x|s)|. Then:\n\f\f\fmedianM\nm=1 ˆLm(x|s) −L∗(x|s)\n\f\f\f ≤medianM\nm=1ϵm\n(127)\nProof. This follows from the properties of order statistics and the triangle inequality.\nStep 1: Notation. Let ˆL(1) ≤ˆL(2) ≤· · · ≤ˆL(M) denote the order statistics (sorted estimates) and ϵ(1) ≤ϵ(2) ≤\n· · · ≤ϵ(M) denote the sorted errors. For odd M = 2k + 1, the median is ˆL(k+1). For even M = 2k, the median is\n(ˆL(k) + ˆL(k+1))/2. We prove for odd M; the even case follows similarly.\n55\n"}, {"page": 56, "text": "A PREPRINT - JANUARY 6, 2026\nStep 2: Bound the median error. The median estimate is med(ˆL) = ˆL(k+1) where k = ⌊M/2⌋. We want to bound:\n|ˆL(k+1) −L∗|\n(128)\nCase 1: L∗≤ˆL(k+1).\nThen ˆL(k+1) −L∗≥0. We have:\nˆL(k+1) −L∗= |ˆL(k+1) −L∗|\n(129)\nNow, there are at least k + 1 estimates ≥ˆL(k+1) ≥L∗, meaning at least k + 1 estimates are upper bounds on L∗. For\neach such estimate ˆLm ≥L∗:\nϵm = |ˆLm −L∗| = ˆLm −L∗\n(130)\nIn particular, for the median estimate:\n|ˆL(k+1) −L∗| = ˆL(k+1) −L∗= ϵ(k+1)\n(131)\nwhere ϵ(k+1) is the error of the model that gave estimate ˆL(k+1). The median error is med(ϵ) = ϵ(k+1) (the (k + 1)-th\nsmallest error). Therefore:\n|ˆL(k+1) −L∗| ≤ϵ(k+1) = med(ϵ)\n(132)\nCase 2: L∗> ˆL(k+1).\nBy symmetric argument, there are at least k + 1 estimates ≤ˆL(k+1) < L∗, so:\n|ˆL(k+1) −L∗| = L∗−ˆL(k+1) ≤med(ϵ)\n(133)\nStep 3: Combine cases. In both cases, |ˆL(k+1) −L∗| ≤med(ϵ).\nIntuition. The median estimate’s error is bounded by the median individual error because the median is the ”middle”\nvalue. If more than half the models have error ≤ϵ∗, then the median estimate is within ϵ∗of the truth. Outliers\n(models with arbitrarily large error) do not affect the median as long as they constitute < 50% of the ensemble.\nThis gives the median a 50% breakdown point: up to ⌊M/2⌋models can have arbitrarily large errors without corrupting\nthe aggregate beyond the error of the (k + 1)-th best model.\nA.6\nProof of Theorem 9: Equivalence of Sequential and Batch Updating\nTheorem (Consistency of Sequential Updating, restated). Under the conditional independence assumption\np(x1, x2, . . . , xT |s) = QT\nt=1 p(xt|s), sequential belief updates (Equation 66) yield the same posterior as batch\nBayesian inference:\nbT (s) =\nπ(s) QT\nt=1 L(xt|s)\nP\ns′∈S π(s′) QT\nt=1 L(xt|s′)\n= p(s|x1, . . . , xT )\n(134)\nProof. We prove by induction on the number of observations T.\nBase case (T = 1): After one observation x1:\nb1(s) =\nL(x1|s) · π(s)\nP\ns′ L(x1|s′) · π(s′) =\np(x1|s) · p(s)\nP\ns′ p(x1|s′) · p(s′) = p(s|x1)\n(135)\nby Bayes’ rule. The base case holds.\nInductive hypothesis: Assume that after T −1 observations:\nbT −1(s) = p(s|x1, . . . , xT −1) =\nπ(s) QT −1\nt=1 p(xt|s)\nP\ns′ π(s′) QT −1\nt=1 p(xt|s′)\n(136)\n56\n"}, {"page": 57, "text": "A PREPRINT - JANUARY 6, 2026\nInductive step: When observation xT arrives, the sequential update rule gives:\nbT (s) =\nL(xT |s) · bT −1(s)\nP\ns′ L(xT |s′) · bT −1(s′)\n(137)\nSubstituting the inductive hypothesis:\nbT (s) =\np(xT |s) · π(s) QT −1\nt=1 p(xt|s)\nZT −1\nP\ns′ p(xT |s′) · π(s′) QT −1\nt=1 p(xt|s′)\nZT −1\n(138)\nwhere ZT −1 = P\ns′′ π(s′′) QT −1\nt=1 p(xt|s′′) is the normalizing constant from time T −1.\nThe ZT −1 factors cancel:\nbT (s) =\np(xT |s) · π(s) QT −1\nt=1 p(xt|s)\nP\ns′ p(xT |s′) · π(s′) QT −1\nt=1 p(xt|s′)\n(139)\nCombining the products:\nbT (s) =\nπ(s) QT\nt=1 p(xt|s)\nP\ns′ π(s′) QT\nt=1 p(xt|s′)\n(140)\nNow we show this equals the batch posterior p(s|x1, . . . , xT ). By Bayes’ rule:\np(s|x1, . . . , xT ) = p(x1, . . . , xT |s) · p(s)\np(x1, . . . , xT )\n(141)\nUnder the conditional independence assumption:\np(x1, . . . , xT |s) =\nT\nY\nt=1\np(xt|s)\n(142)\nTherefore:\np(s|x1, . . . , xT ) =\nQT\nt=1 p(xt|s) · p(s)\nP\ns′\nQT\nt=1 p(xt|s′) · p(s′)\n=\nπ(s) QT\nt=1 p(xt|s)\nP\ns′ π(s′) QT\nt=1 p(xt|s′)\n(143)\nThis matches Equation 140. Therefore, bT (s) = p(s|x1, . . . , xT ).\nConclusion: By induction, sequential updating produces the correct Bayesian posterior at each time step t ∈\n{1, . . . , T} under the conditional independence assumption.\nA.7\nProof of Theorem 10: Optimality of Expected Cost Minimization\nTheorem (Optimality of Expected Cost Minimization, restated). Let p∗(s|x) denote the true posterior distribution\nover states given observation x. Among all deterministic decision rules δ : X →A mapping observations to actions,\nthe Bayes rule:\nδ∗(x) = arg min\na∈A\nX\ns∈S\np∗(s|x) · C(a, s)\n(144)\nminimizes the expected cost (Bayes risk):\nR(δ∗) =\nZ\nX\nX\ns∈S\np∗(s|x) · C(δ∗(x), s) · p(x) dx ≤R(δ)\n∀δ\n(145)\nProof. This is a classical result in statistical decision theory [7]. We provide a complete proof for self-containment.\nStep 1: Express the risk (expected cost) of an arbitrary decision rule.\nFor any decision rule δ : X →A, the risk is:\nR(δ) = Ex,s[C(δ(x), s)] =\nZ\nX\nX\ns∈S\np(s, x) · C(δ(x), s) dx\n(146)\n57\n"}, {"page": 58, "text": "A PREPRINT - JANUARY 6, 2026\nUsing p(s, x) = p(s|x)p(x):\nR(δ) =\nZ\nX\nX\ns∈S\np(s|x) · C(δ(x), s) · p(x) dx\n(147)\nStep 2: Decompose the risk pointwise.\nThe integral in Equation 147 sums over all possible observations x ∈X. For each fixed x, the integrand is:\nr(x, δ) =\nX\ns∈S\np(s|x) · C(δ(x), s) · p(x)\n(148)\nWe can rewrite the risk as:\nR(δ) =\nZ\nX\nr(x, δ) dx =\nZ\nX\np(x)\nX\ns∈S\np(s|x) · C(δ(x), s)\n|\n{z\n}\nexpected cost at x\ndx\n(149)\nStep 3: Minimize pointwise to minimize globally.\nSince p(x) ≥0 and the integral is a weighted sum over x, minimizing R(δ) is equivalent to minimizing the integrand\nat each x independently. That is:\nδ∗(x) = arg min\nδ(x)∈A\nX\ns∈S\np(s|x) · C(δ(x), s)\n(150)\nFor each observation x, we choose the action a ∈A that minimizes expected cost under the posterior p(s|x):\nδ∗(x) = arg min\na∈A\nX\ns∈S\np(s|x) · C(a, s)\n(151)\nThis is the Bayes decision rule.\nStep 4: Prove optimality.\nFor any other rule δ, the difference in risk is:\nR(δ) −R(δ∗) =\nZ\nX\np(x)\n\"X\ns\np(s|x)C(δ(x), s) −\nX\ns\np(s|x)C(δ∗(x), s)\n#\ndx\n(152)\n=\nZ\nX\np(x)\n\"X\ns\np(s|x)C(δ(x), s) −min\na∈A\nX\ns\np(s|x)C(a, s)\n#\ndx\n(153)\nThe term in brackets is non-negative by definition: δ(x) is some action in A, and the minimum over all actions in A is\n≤the value at any particular action. Since p(x) ≥0, the entire integrand is non-negative.\nTherefore:\nR(δ) −R(δ∗) ≥0 =⇒R(δ∗) ≤R(δ)\n∀δ\n(154)\nThe Bayes rule δ∗minimizes expected cost among all decision rules.\nB\nAdditional Experimental Details\nThis appendix provides supplementary experimental details, extended analyses, and additional results that support the\nmain text findings.\nB.1\nResume Generation: Full Procedure and Validation\nB.1.1\nState-Dependent Feature Distributions\nWe generated synthetic resumes by sampling features from state-specific distributions. Table 6 provides complete\nparameter specifications.\n58\n"}, {"page": 59, "text": "A PREPRINT - JANUARY 6, 2026\nTable 6: State-dependent distributions for resume feature generation. Beta distributions parameterized by α, β; Cate-\ngorical by probability vectors; Truncated normal by (µ, σ, min, max).\nFeature\nDistribution Type\ns1\ns2\ns3\ns4\nEducation\nUniversity Tier\nCategorical\n[.05,.10,.30,.55]\n[.10,.25,.45,.20]\n[.20,.50,.25,.05]\n[.80,.15,.05,.00]\n(elite/top50/avg/below)\nDegree Type\nCategorical\n[.75,.20,.05]\n[.65,.30,.05]\n[.50,.40,.10]\n[.30,.30,.40]\n(BS/MS/PhD)\nGPA\nBeta, rescale [2.0,4.0]\nBeta(2,5)\nBeta(3,3)\nBeta(5,2)\nBeta(8,1)\nExperience\nYears\nTruncNorm\n(0.5,0.3,0,2)\n(2,1,1,5)\n(4,1.5,2,8)\n(7,2,5,15)\nCompany Prestige\nCategorical\n[.05,.15,.30,.50]\n[.10,.30,.40,.20]\n[.25,.45,.25,.05]\n[.60,.30,.10,.00]\n(FAANG/tier2/startup/unk)\nProjects\nNumber of Projects\nPoisson(λ)\nλ = 1\nλ = 2\nλ = 4\nλ = 6\nComplexity\nCategorical\n[.10,.30,.60]\n[.20,.50,.30]\n[.40,.45,.15]\n[.70,.25,.05]\n(advanced/intermediate/basic)\nSkills\nTech Stack Size\nPoisson(λ) + 2\nλ = 3\nλ = 6\nλ = 10\nλ = 15\nB.1.2\nGPT-4o Generation Prompt\nFull prompt template for generating resume text from features:\nYou are generating a realistic software engineering resume.\nGenerate plain text (no markdown,\nno special formatting) for the following candidate profile:\nPROFILE:\n- True Quality Level:\n[STATE NAME]\n- Education:\n[DEGREE] in [MAJOR] from [UNIVERSITY NAME] (Tier:\n[TIER]), GPA: [GPA]\n- Years of Experience:\n[YEARS]\n- Work History:\n[LIST OF COMPANIES, ROLES, DURATIONS, TECHNOLOGIES]\n- Projects:\n[LIST OF PROJECTS WITH DESCRIPTIONS AND TECH STACKS]\n- Skills:\n[LIST OF PROGRAMMING LANGUAGES, FRAMEWORKS, TOOLS]\n- Name:\n[FULL NAME] (for diversity:\ngender [GENDER], ethnicity [ETHNICITY])\nREQUIREMENTS:\n1.\nWrite as plain text with standard sections:\nEDUCATION, EXPERIENCE, PROJECTS,\nSKILLS\n2.\nLength:\n300--500 words (realistic resume length)\n3.\nUse varied writing styles (not all resumes should sound identical)\n4.\nInclude realistic dates, company names, project descriptions\n5.\nMatch the feature profile exactly---don’t add credentials not listed\n6.\nFor quality level [STATE], make sure the overall impression aligns:\n- Clear Reject:\nObvious deficiencies, weak credentials, red flags\n- Phone Screen:\nSome promise but significant gaps or uncertainties\n- Interview:\nSolid qualifications, meets standard requirements\n- Strong Hire:\nExceptional background, impressive achievements\nGenerate the resume now (text only, no preamble):\nGeneration parameters:\n• Model: gpt-4o-2024-05-13\n59\n"}, {"page": 60, "text": "A PREPRINT - JANUARY 6, 2026\n• Temperature: 0.8 (higher than likelihood elicitation to ensure diversity)\n• Max tokens: 600\n• Top-p: 0.95\nB.1.3\nExpert Validation Protocol\nWe recruited three expert technical recruiters via Upwork with the following qualifications:\n• Recruiter 1: 12 years experience, previously at Fortune 500 companies\n• Recruiter 2: 8 years experience, currently at a Series-C startup, previously at Amazon\n• Recruiter 3: 10 years experience, independent recruiter specializing in ML/AI roles\nValidation task: Rate 100 randomly sampled resumes (stratified by our generated state labels: 65 s1, 25 s2, 8 s3, 2\ns4) on the four-state scale. Recruiters were:\n• Blinded to our labels\n• Provided with detailed rubrics for each state (same as used in likelihood elicitation prompts)\n• Allowed to assign ”uncertain” to up to 10% of resumes (these were excluded from agreement calculations)\n• Compensated $500 each ($5 per resume × 100 resumes)\nResults:\n• Fleiss’ κ (three-way agreement): 0.79 (substantial agreement)\n• Pairwise Cohen’s κ: Recruiter 1 vs. 2: 0.81, Recruiter 1 vs. 3: 0.78, Recruiter 2 vs. 3: 0.80\n• Agreement with majority label: Recruiter 1: 91%, Recruiter 2: 88%, Recruiter 3: 90%\n• Our label vs. majority recruiter label: Cohen’s κ = 0.83, exact match: 89%\nDisagreement analysis: For the 11 resumes where our label disagreed with the majority recruiter label:\n• 7 cases: One-state mismatch (e.g., we labeled s2, majority labeled s3)\n• 4 cases: Recruiters themselves disagreed (no clear majority)\n• 0 cases: Gross mismatch (e.g., s1 vs. s4)\nThis validates that our synthetic labels are reasonable proxies for expert human judgment.\nB.2\nPhone Screen Simulation Details\nFor candidates where the framework triggered information gathering (phone screen), we simulated outcomes as fol-\nlows:\nB.2.1\nPerformance Sampling\nPhone screen performance was sampled from state-dependent Beta distributions, then discretized to a 0–10 scale:\nTable 7: Phone screen performance distributions by true candidate state.\nTrue State\nBeta Distribution\nMean Score\nProb(Score ≥7)\nProb(Score ≤4)\ns1 (Clear Reject)\nBeta(2, 8)\n2.0\n5%\n90%\ns2 (Phone Screen)\nBeta(5, 5)\n5.0\n20%\n30%\ns3 (Interview)\nBeta(7, 3)\n7.0\n60%\n10%\ns4 (Strong Hire)\nBeta(9, 1)\n9.0\n95%\n5%\nRationale: These distributions reflect empirical phone screen outcome rates from proprietary data shared by partner\ncompanies (anonymized). Strong candidates usually perform well (95% score ≥7), but occasionally have off days\n(5% score poorly due to nerves, bad connection, etc.). Weak candidates usually perform poorly (90% score ≤4) but\noccasionally get lucky with softball questions (5% score well).\n60\n"}, {"page": 61, "text": "A PREPRINT - JANUARY 6, 2026\nB.2.2\nTranscript Generation\nFor each phone screen with sampled performance score p ∈[0, 10], we generated a short transcript via GPT-4o:\nGenerate realistic phone screen notes for a software engineering candidate who performed at\nlevel [SCORE]/10.\nThe phone screen covered:\n1.\nTechnical question:\n\"Explain a recent project and technical challenges\"\n2.\nCoding question:\n\"Reverse a linked list\" or similar basic algorithm\n3.\nBehavioral:\n\"Why are you interested in this role?\"\n4.\nCommunication assessment\nPerformance interpretation:\n9-10:\nExceptional.\nClear technical depth, excellent communication, strong interest.\n7-8:\nGood.\nSolid technical skills, clear communication, appropriate for next round.\n5-6:\nBorderline.\nSome technical ability but gaps or concerns.\nUncertain.\n3-4:\nWeak.\nStruggled with technical questions or communication issues.\n0-2:\nVery weak.\nCould not answer basic questions, major red flags.\nWrite 100-150 word recruiter notes capturing the key points.\nInclude:\n- Brief summary of responses to each question\n- Notable strengths or weaknesses\n- Overall impression\n- Recommendation (advance/reject/uncertain)\nExample generated transcript (score 7/10, state s3):\nCandidate discussed building a microservices architecture for an e-commerce platform. Clearly explained the\ntradeoffs between monolith and microservices, mentioned specific technologies (Docker, Kubernetes, gRPC).\nCoding question: successfully reversed linked list with iterative approach, explained time/space complexity\ncorrectly. Behavioral: genuine interest in our product space, asked thoughtful questions about team struc-\nture and tech stack. Communication was clear and concise. Minor gap: limited experience with distributed\ntransactions. Recommendation: Advance to onsite interview.\nB.2.3\nLikelihood Elicitation from Transcripts\nThe generated transcript was then fed to the 5 LLMs using the same contrastive prompting framework as for resumes,\nbut adapted for phone screens:\nYou are an expert technical recruiter.\nAssume the candidate’s true quality level is:\n[STATE DESCRIPTION].\nHow typical is the following phone screen performance for someone at this quality level?\nPhone Screen Notes:\n[TRANSCRIPT]\nScore 0--10:\n10 = Extremely typical for this quality level\n0 = Completely atypical\nScore:\nThis yields likelihood estimates {L(xscreen|s)}s∈S which are then aggregated via median and used to update beliefs\nsequentially.\n61\n"}, {"page": 62, "text": "A PREPRINT - JANUARY 6, 2026\nB.3\nBaseline Implementation Details\nB.3.1\nSingle-LLM + Fixed Threshold\nPrompt:\nYou are an expert technical recruiter.\nRate the following resume on a\nscale of 0--10 where 10 = definite strong hire and 0 = definite reject.\nConsider technical skills, experience, education, and overall fit for a\nsoftware engineering role.\nResume:\n[TEXT]\nScore (number only):\nModel: GPT-4o, temperature=0.7, max tokens=10\nDecision rule: Interview if score ≥7.0, reject otherwise.\nB.3.2\nSingle-LLM + Calibrated Threshold\nSame as above but threshold τ was tuned on 200-resume validation set to minimize total cost via grid search over\nτ ∈[5.0, 5.1, . . . , 8.0]. Optimal: τ = 6.2.\nB.3.3\nEnsemble Voting\nPrompt: Same discriminative prompt as single-LLM baseline.\nModels: All 5 LLMs (GPT-4o, Claude, Gemini, Grok, DeepSeek)\nDecision rule: Each model votes binary (interview if score ≥7, else reject). Final decision: interview if ≥3 models\nvote interview (majority rule). Ties resolved by rejecting (conservative).\nB.3.4\nEnsemble Averaging + Threshold\nPrompt: Same discriminative prompt.\nModels: All 5 LLMs\nAggregation: Compute mean score across 5 models.\nDecision rule: Interview if mean ≥τ, where τ = 6.5 was tuned on validation set.\nB.4\nHyperparameter Tuning via Cross-Validation\nWe used 5-fold cross-validation on a 200-resume validation set (disjoint from the 1,000-resume test set) to select\nhyperparameters.\nParameters tuned:\n• Disagreement threshold τD ∈{0.10, 0.12, 0.15, 0.18, 0.20}\n• VOI informativeness ρ ∈{0.5, 0.6, 0.7, 0.8, 0.9}\n• Temperature T ∈{0.5, 0.7, 0.9} for all LLMs\nObjective: Minimize total cost on validation folds.\nResults:\nThe selected hyperparameters (τD = 0.15, ρ = 0.7, T = 0.7) minimized validation cost and were used for all test set\nevaluations.\nB.5\nStatistical Significance Testing Details\nBootstrap resampling: For each method, we computed 95% confidence intervals via bootstrap:\n1. Sample 1,000 resumes with replacement from the test set\n62\n"}, {"page": 63, "text": "A PREPRINT - JANUARY 6, 2026\nTable 8: Cross-validation results for hyperparameter selection. Bolded values were selected.\nτD\nρ\nT\nAvg. Validation Cost ($K)\n0.10\n0.7\n0.7\n118.2\n0.12\n0.7\n0.7\n115.8\n0.15\n0.7\n0.7\n112.3\n0.18\n0.7\n0.7\n114.1\n0.20\n0.7\n0.7\n116.5\n0.15\n0.5\n0.7\n119.7\n0.15\n0.6\n0.7\n114.2\n0.15\n0.7\n0.7\n112.3\n0.15\n0.8\n0.7\n113.8\n0.15\n0.9\n0.7\n115.4\n0.15\n0.7\n0.5\n118.9\n0.15\n0.7\n0.7\n112.3\n0.15\n0.7\n0.9\n114.6\n2. Run the method on the bootstrap sample, compute total cost\n3. Repeat 10,000 times\n4. Confidence interval: [2.5th percentile, 97.5th percentile]\nPaired permutation test: To test if cost difference between method A and method B is statistically significant:\n1. For each resume i, compute cost difference ∆i = Cost(i)\nA −Cost(i)\nB\n2. Test statistic: T = P1000\ni=1 ∆i (total cost difference)\n3. Null hypothesis: E[∆i] = 0 (methods are equivalent)\n4. Permutation distribution: For each of 10,000 iterations, randomly flip signs of ∆i values, compute T ∗\n5. p-value: Fraction of permutations where |T ∗| ≥|T|\n6. Significance: Reject null if p < 0.005 (Bonferroni-corrected for 10 comparisons)\nResults: All comparisons between our framework and baselines achieved p < 0.0001 (highly significant).\nB.6\nComputational Cost Breakdown\nAPI calls per candidate:\n• Resume likelihood elicitation: 5 models × 4 states = 20 queries\n• Phone screen (if conducted): 5 models × 4 states = 20 queries\n• Total: 20 (no screen) to 40 (with screen)\nLatency (with parallelization):\n• Resume: 2 seconds (20 queries in parallel)\n• Phone screen: 2 seconds (20 queries in parallel)\n• Total per candidate: 2–4 seconds\nFinancial cost (at 2024 API pricing):\n• GPT-4o: $0.005 per 1K tokens (input)\n• Claude 3.5: $0.003 per 1K tokens\n• Gemini Pro: $0.0015 per 1K tokens\n• Grok: $0.002 per 1K tokens (estimated, not yet public API)\n• DeepSeek: $0.0014 per 1K tokens\n63\n"}, {"page": 64, "text": "A PREPRINT - JANUARY 6, 2026\nAverage cost per query (500 token resume + 200 token prompt = 700 tokens):\nCost/query = 0.005 + 0.003 + 0.0015 + 0.002 + 0.0014\n5\n× 0.7 = $0.0021\n(155)\nCost per candidate:\n• Without phone screen: 20 × $0.0021 = $0.042\n• With phone screen: 40 × $0.0021 = $0.084\nFor 1,000 candidates with 27% phone screen rate:\nTotal API cost = 730 × $0.042 + 270 × $0.084 = $30.66 + $22.68 = $53.34\n(156)\nROI: $53 API cost to save $294K in hiring mistakes = 5,509:1 return on investment.\nC\nExtended Fairness Analysis\nC.1\nDetailed Demographic Distributions\nOur synthetic dataset includes the following demographic distributions (inferred from names via census data [110]):\nTable 9: Demographic distribution of synthetic candidates.\nDemographic Group\nCount\nPercentage\nGender\nMale\n512\n51.2%\nFemale\n438\n43.8%\nNon-binary\n50\n5.0%\nRace/Ethnicity\nWhite\n382\n38.2%\nBlack\n168\n16.8%\nHispanic\n215\n21.5%\nAsian\n235\n23.5%\nTotal\n1,000\n100%\nWe ensured approximately balanced demographic distributions across true states to isolate model bias from genuine\ndistributional differences:\nTable 10: Demographic balance across true states. Numbers show percentage of each state from each demographic\ngroup.\nDemographic\ns1 (650)\ns2 (250)\ns3 (80)\ns4 (20)\nMale\n51%\n52%\n50%\n50%\nFemale\n44%\n43%\n45%\n45%\nWhite\n38%\n39%\n37%\n40%\nBlack\n17%\n16%\n18%\n15%\nHispanic\n21%\n22%\n21%\n20%\nAsian\n24%\n23%\n24%\n25%\nThis ensures that any observed fairness gaps are due to model bias rather than confounding with ground truth state\ndistributions.\nC.2\nIndividual Model Bias Measurements\nWe measured per-model bias on a matched-pairs subset: 200 resume pairs where credentials are identical except for\nname (and thus inferred demographics).\nKey observations:\n64\n"}, {"page": 65, "text": "A PREPRINT - JANUARY 6, 2026\nTable 11: Individual model bias on matched pairs. Values show mean score difference (demographic group A minus\ngroup B). Negative values indicate bias against group A.\nComparison\nGPT-4o\nClaude\nGemini\nGrok\nDeepSeek\nGender Comparisons (mean difference in 0-10 score)\nFemale - Male\n-0.62\n-0.58\n+0.41\n-0.22\n-0.09\nNon-binary - Male\n-1.13\n-0.71\n+0.18\n-0.51\n-0.15\nRace Comparisons\nBlack - White\n-1.82\n-0.44\n-0.18\n+0.31\n-0.08\nHispanic - White\n-1.45\n-0.52\n-0.23\n+0.14\n-0.11\nAsian - White\n+0.23\n+0.15\n+0.33\n+0.09\n+0.52\n• GPT-4o exhibits the strongest bias: -1.82 points for Black vs. White, -0.62 for Female vs. Male\n• Claude shows moderate bias (-0.58 gender, -0.44 race)\n• Gemini has opposite gender bias (+0.41 favoring Female), potentially from overcorrection\n• Grok shows small opposite race bias (+0.31 Black vs. White), suggesting diversity-focused training\n• DeepSeek has minimal bias across all comparisons (—bias— ¡ 0.15)\nWhen aggregated via median, biases partially cancel:\n• Female - Male: median(-0.62, -0.58, +0.41, -0.22, -0.09) = -0.22\n• Black - White: median(-1.82, -0.44, -0.18, +0.31, -0.08) = -0.18\nThis represents 65–90% bias reduction compared to the worst individual model (GPT-4o).\nC.3\nCalibration by Demographic Group\nWe verify that well-calibrated beliefs (ECE=0.09 overall) hold across demographic groups:\nTable 12: Calibration (ECE) and accuracy by demographic group.\nGroup\nECE (Ours)\nECE (GPT-4o)\nAccuracy (Ours)\nAccuracy (GPT-4o)\nMale\n0.09\n0.17\n82.2%\n70.1%\nFemale\n0.09\n0.19\n82.8%\n66.2%\nNon-binary\n0.10\n0.21\n82.0%\n62.0%\nWhite\n0.08\n0.16\n83.1%\n72.5%\nBlack\n0.10\n0.22\n81.5%\n58.3%\nHispanic\n0.09\n0.19\n82.3%\n64.4%\nAsian\n0.09\n0.17\n82.7%\n69.1%\nOur framework maintains consistent calibration (ECE 0.08–0.10) and accuracy (81.5–83.1%) across all groups. GPT-\n4o shows degraded performance for underrepresented groups (ECE=0.22 for Black, accuracy=58.3% vs. 72.5% for\nWhite).\nD\nAdditional Sensitivity Analyses\nD.1\nEffect of Ensemble Size\nWe varied ensemble size M ∈{1, 2, 3, 4, 5} by subsampling LLMs and measured total cost:\nFindings:\n• Cost decreases monotonically with ensemble size (more models = better aggregation)\n• Diminishing returns: M = 3 →4 saves $24K, M = 4 →5 saves $12K\n• Variance decreases with M (more stable performance)\n65\n"}, {"page": 66, "text": "A PREPRINT - JANUARY 6, 2026\nTable 13: Total cost as a function of ensemble size. Each row shows mean cost (std) over 10 random subsamples.\nM\nMean Cost ($K)\nStd Cost ($K)\nFairness Gap (pp)\n1 (best single)\n688\n42\n18.1\n2\n634\n28\n12.4\n3\n598\n19\n8.7\n4\n574\n14\n6.5\n5 (full)\n562\n0\n5.2\n• Fairness improves consistently with M\nRecommendation: M = 5 provides best cost-performance, but M = 3 may be sufficient if API costs are prohibitive\n(saves 60% of API cost for 6% higher total cost).\nD.2\nRobustness to Prior Misspecification\nWe tested robustness when the specified prior π differs from the true data distribution. Suppose true state distribution\nis πtrue = [0.65, 0.25, 0.08, 0.02] but we specify πspec = [0.60, 0.27, 0.10, 0.03] (mild misspecification).\nTable 14: Effect of prior misspecification on total cost.\nPrior Specification\nKL Divergence from True\nTotal Cost ($K)\n∆Cost\nTrue π = [0.65, 0.25, 0.08, 0.02]\n0.000\n562\n0\nMild error [0.60, 0.27, 0.10, 0.03]\n0.012\n571\n+$9K (+1.6%)\nModerate error [0.55, 0.30, 0.12, 0.03]\n0.035\n593\n+$31K (+5.5%)\nLarge error [0.50, 0.30, 0.15, 0.05]\n0.082\n638\n+$76K (+13.5%)\nUniform [0.25, 0.25, 0.25, 0.25]\n0.421\n794\n+$232K (+41.3%)\nFindings:\n• Framework is robust to small prior errors (1–2% cost increase for KL divergence ¡0.02)\n• Moderate misspecification (KL 0.05) causes 5–10% cost increase\n• Severe misspecification (uniform prior, KL=0.42) causes 41% cost increase\nPractical guidance: Invest effort in obtaining reasonable prior estimates from historical data (even crude estimates\nare vastly better than uniform priors). Periodically update priors as hiring funnel metrics evolve.\nE\nPrompt Templates and Code\nE.1\nComplete Likelihood Elicitation Prompt Template\nThe full prompt template with all placeholders is available at:\nhttps://github.com/[anonymized-for-review]/bayesian-llm-hiring/prompts/likelihood elicitation.txt\nKey template variables:\n• {STATE NAME}: One of ”Clear Reject”, ”Phone Screen”, ”Interview”, ”Strong Hire”\n• {STATE DESCRIPTION}: Detailed rubric for that state\n• {EVIDENCE TYPE}: ”Resume” or ”Phone Screen Transcript”\n• {EVIDENCE TEXT}: Full text of the observation\nE.2\nImplementation Code\nFull implementation in Python using the following libraries:\n• OpenAI API for GPT-4o\n66\n"}, {"page": 67, "text": "A PREPRINT - JANUARY 6, 2026\n• Anthropic API for Claude 3.5 Sonnet\n• Google Generative AI API for Gemini Pro\n• xAI API for Grok (when available)\n• DeepSeek API for DeepSeek-V2\n• NumPy for numerical computations\n• SciPy for statistical tests\n• Pandas for data manipulation\nCode repository (will be made public upon publication):\nhttps://github.com/[anonymized-for-review]/bayesian-llm-hiring\nE.3\nReproducibility Checklist\nTo reproduce our results:\n1. Clone the repository\n2. Install dependencies: pip install -r requirements.txt\n3. Obtain API keys for all 5 LLMs, set as environment variables\n4. Run resume generation: python generate resumes.py --n=1000 --seed=42\n5. Run framework: python run framework.py --config=configs/full.yaml\n6. Run baselines: python run baselines.py --config=configs/baselines.yaml\n7. Generate plots: python generate plots.py --output=figures/\n8. Compute statistics: python compute statistics.py\nAll random seeds are fixed for deterministic reproduction. Expected runtime: 4 hours on a machine with 10 parallel\nworkers for API calls (or 20 hours sequential).\nReferences\n[1] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. Con-\nference on Learning Theory, pages 39–1, 2012.\n[2] Ross Anderson, Chris Barton, Rainer B¨ohme, Richard Clayton, Michel JG Van Eeten, Michael Levi, Tyler\nMoore, and Stefan Savage. The economics of payment card fraud. Communications of the ACM, 62(12):66–73,\n2019.\n[3] Association for Financial Professionals. Payment fraud and control survey. Technical report, AFP, 2023.\n[4] Alejandro Correa Bahnsen, Djamila Aouada, and Bj¨orn Ottersten. Example-dependent cost-sensitive decision\ntrees. Expert Systems with Applications, 42(19):6609–6619, 2015.\n[5] Solon Barocas and Andrew D Selbst. Big data’s disparate impact. California Law Review, 104:671–732, 2016.\n[6] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of\nstochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, pages 610–623, 2021.\n[7] James O Berger. Statistical decision theory and Bayesian analysis. Springer Science & Business Media, 1985.\n[8] Marianne Bertrand and Sendhil Mullainathan. Are emily and greg more employable than lakisha and jamal? A\nfield experiment on labor market discrimination. American economic review, 94(4):991–1013, 2004.\n[9] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elab-\norate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence,\n38(16):17682–17690, 2024.\n[10] Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Ro-\nhit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman. Pyro: Deep universal probabilistic programming.\nJournal of Machine Learning Research, 20(28):1–6, 2019.\n67\n"}, {"page": 68, "text": "A PREPRINT - JANUARY 6, 2026\n[11] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural\nnetworks. Proceedings of the International Conference on Machine Learning, pages 1613–1622, 2015.\n[12] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer\nprogrammer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information\nProcessing Systems, volume 29, pages 4349–4357, 2016.\n[13] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of founda-\ntion models. arXiv preprint arXiv:2108.07258, 2021.\n[14] Joseph Bonneau, Cormac Herley, Paul C Van Oorschot, and Frank Stajano.\nThe economics of two-factor\nauthentication. Financial Cryptography and Data Security, pages 1–15, 2012.\n[15] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.\n[16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\nin neural information processing systems, 33:1877–1901, 2020.\n[17] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\nSemantics derived automatically from language\ncorpora contain human-like biases. Science, 356(6334):183–186, 2017.\n[18] CareerBuilder. The cost of a bad hire can be astronomical. Technical report, 2017.\n[19] Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus\nBrubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal of\nStatistical Software, 76(1), 2017.\n[20] Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of\nmodels. In Proceedings of the Twenty-First International Conference on Machine Learning, page 18, 2004.\n[21] Kathryn Chaloner and Isabella Verdinelli.\nBayesian experimental design: A review.\nStatistical Science,\n10(3):273–304, 1995.\n[22] Alexandra Chouldechova.\nFair prediction with disparate impact: A study of bias in recidivism prediction\ninstruments. In Big data, volume 5, pages 153–163. Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor\nNew Rochelle, NY 10801 USA, 2017.\n[23] Robert T Clemen and Robert L Winkler. Combining probability distributions from experts in risk analysis. Risk\nAnalysis, 19(2):187–203, 1999.\n[24] David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of\nArtificial Intelligence Research, 4:129–145, 1996.\n[25] Roger M Cooke. Experts in uncertainty: Opinion and subjective probability in science. Environmental Ethics,\n13(2), 1991.\n[26] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and\nthe cost of fairness. Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, pages 797–806, 2017.\n[27] Bo Cowgill and Catherine E Tucker. Biased programmers? or biased data? a field experiment in operationaliz-\ning ai ethics. Available at SSRN 3615404, 2020.\n[28] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. Proceedings\nof the National Academy of Sciences, 117(48):30055–30062, 2020.\n[29] Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. Gen: A general-\npurpose probabilistic programming system with programmable inference. In Proceedings of the 40th ACM\nSIGPLAN Conference on Programming Language Design and Implementation, pages 221–236, 2019.\n[30] Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi. Learned lessons in credit card\nfraud detection from a practitioner perspective. Expert Systems with Applications, 41(10):4915–4928, 2014.\n[31] Norman Dalkey and Olaf Helmer. An experimental application of the delphi method to the use of experts.\nManagement Science, 9(3):458–467, 1963.\n[32] Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier\nsystems, pages 1–15. Springer, 2000.\n[33] David L Donoho and Miriam Gasko. Breakdown properties of location estimates based on halfspace depth and\nprojected outlyingness. The Annals of Statistics, pages 1803–1827, 1982.\n68\n"}, {"page": 69, "text": "A PREPRINT - JANUARY 6, 2026\n[34] Michael F Drummond, Mark J Sculpher, Karl Claxton, Greg L Stoddart, and George W Torrance. Health\neconomics and health technology assessment. Oxford University Press, 2015.\n[35] Philippa J Easterbrook, Ramana Gopalan, Jesse A Berlin, and David R Matthews. Publication bias in medical\nresearch. The Lancet, 337(8746):867–872, 1991.\n[36] Charles Elkan. The foundations of cost-sensitive learning. In International Joint Conference on Artificial\nIntelligence, volume 17, pages 973–978, 2001.\n[37] European Parliament and Council. Artificial intelligence act. Official Journal of the European Union, 2024.\n[38] Gerard FitzGerald, George A Jelinek, Deborah Scott, and Marie F Gerdtz. Emergency department triage scales\nand their components: A systematic review of the scientific evidence. Academic Emergency Medicine, 17(1):1–\n13, 2010.\n[39] Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.\n[40] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application\nto boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.\n[41] Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P Hamilton, and\nDerek Roth. A comparative study of fairness-enhancing interventions in machine learning. Proceedings of the\nConference on Fairness, Accountability, and Transparency, pages 329–338, 2019.\n[42] S Michael Gaddis. How black are lakisha and jamal? racial perceptions from names used in correspondence\naudit studies. Sociological Science, 4:469–489, 2017.\n[43] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in\ndeep learning. In International Conference on Learning Representations, pages 1050–1059, 2016.\n[44] Michael Gira, Ruisu Zhang, and Kangwook Lee. Debiasing pre-trained language models via efficient fine-\ntuning. Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion,\npages 59–69, 2022.\n[45] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In\nProceedings of the 34th International Conference on Machine Learning, pages 1321–1330. PMLR, 2017.\n[46] Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel.\nRobust Statistics: The\nApproach Based on Influence Functions. John Wiley & Sons, 2011.\n[47] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Advances in neural\ninformation processing systems, pages 3315–3323, 2016.\n[48] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n[49] Joseph L Hodges and Erich L Lehmann. Estimates of location based on rank tests. The Annals of Mathematical\nStatistics, pages 598–611, 1963.\n[50] Ronald A Howard. Information value theory. IEEE Transactions on Systems Science and Cybernetics, 2(1):22–\n26, 1966.\n[51] Peter J Huber and Elvezio M Ronchetti. Robust Statistics. John Wiley & Sons, 2009.\n[52] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise\nranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.\n[53] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\nZac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they\nknow. arXiv preprint arXiv:2207.05221, 2022.\n[54] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observ-\nable stochastic domains. Artificial Intelligence, 101(1-2):99–134, 1998.\n[55] Jerome P Kassirer and Richard I Kopelman. The diagnostic process. Annals of Internal Medicine, 110(11):893–\n900, 1989.\n[56] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n[57] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of\nrisk scores. In 8th Innovations in Theoretical Computer Science Conference (ITCS 2017), 2017.\n[58] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\nmodels are zero-shot reasoners. Advances in Neural Information Processing Systems, 35:22199–22213, 2022.\n69\n"}, {"page": 70, "text": "A PREPRINT - JANUARY 6, 2026\n[59] Deborah Korenstein, Minal S Kale, and Wendy Levinson. The cost of emergency care in the united states.\nJAMA Internal Medicine, 174(9):1516–1517, 2014.\n[60] Hadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language models. Pro-\nceedings of The ACM Collective Intelligence Conference, pages 12–24, 2023.\n[61] Nathan R Kuncel, David M Klieger, Brian S Connelly, and Deniz S Ones. Mechanical versus clinical data com-\nbination in selection and admissions decisions: A meta-analysis. Journal of Applied Psychology, 98(6):1060–\n1072, 2013.\n[62] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty\nestimation using deep ensembles. In Advances in Neural Information Processing Systems, pages 6402–6413,\n2017.\n[63] Anja Lambrecht and Catherine Tucker. Algorithmic bias? an empirical study of apparent gender-based discrim-\nination in the display of stem career ads. Management Science, 65(7):2966–2981, 2019.\n[64] Tor Lattimore and Csaba Szepesv´ari. Bandit Algorithms. Cambridge University Press, 2020.\n[65] Jiawei Li, Xinyuan Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Chatgpt in the hiring process:\nOpportunities and challenges. arXiv preprint arXiv:2303.17564, 2023.\n[66] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language\nmodels better reasoners with step-aware verifier. arXiv preprint arXiv:2206.02336, 2022.\n[67] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and\nmitigating social biases in language models. International Conference on Machine Learning, pages 6565–6576,\n2021.\n[68] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint\narXiv:2211.09110, 2022.\n[69] Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical\nStatistics, pages 986–1005, 1956.\n[70] LinkedIn Talent Solutions. Global recruiting trends 2023. Technical report, LinkedIn Corporation, 2023.\n[71] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A Smith, and Yejin\nChoi. Dexperts: Decoding-time controlled text generation with experts and anti-experts. Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics, pages 6691–6706, 2021.\n[72] Martin A Makary and Michael Daniel.\nMedical error: The third leading cause of death in the us.\nBMJ,\n353:i2139, 2016.\n[73] Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91, 1952.\n[74] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias\nand fairness in machine learning. ACM Computing Surveys, 54(6):1–35, 2021.\n[75] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language\nmodels.\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics, pages\n5356–5371, 2021.\n[76] Roberto Navigli, Simone Conia, and Bj¨orn Ross. Biases in large language models: Origins, inventory, and\ndiscussion. ACM Journal of Data and Information Quality, 15(2):1–17, 2023.\n[77] Radford M Neal. Bayesian learning for neural networks. Lecture Notes in Statistics, 118, 1996.\n[78] New York City Council. Local law 144 of 2021: Automated employment decision tools. New York City\nAdministrative Code, December 2021.\n[79] Andrew Y Ng and Michael I Jordan. On discriminative vs. generative classifiers: A comparison of logistic\nregression and naive bayes. Advances in Neural Information Processing Systems, 14, 2002.\n[80] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibra-\ntion in deep learning. CVPR Workshops, 2(7), 2019.\n[81] Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in an algo-\nrithm used to manage the health of populations. Science, 366(6464):447–453, 2019.\n[82] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.\n70\n"}, {"page": 71, "text": "A PREPRINT - JANUARY 6, 2026\n[83] George Papamakarios, David Sterratt, and Iain Murray.\nSequential neural likelihood: Fast likelihood-free\ninference with autoregressive flows. Proceedings of the International Conference on Artificial Intelligence and\nStatistics, pages 837–848, 2019.\n[84] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood\nmethods. In Advances in large margin classifiers, volume 10, pages 61–74. MIT Press, 1999.\n[85] John H Pope, Tom P Aufderheide, Robin Ruthazer, Robert H Woolard, Jerome A Feldman, Joni R Beshansky,\nJohn L Griffith, and Harry P Selker. Missed diagnoses of acute cardiac ischemia in the emergency department.\nNew England Journal of Medicine, 342(16):1163–1170, 2000.\n[86] Lutz Prechelt. The myth of the 10x programmer. IEEE Software, 36(5):96–99, 2019.\n[87] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint\narXiv:2307.16789, 2023.\n[88] Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. Mitigating bias in algorithmic hiring: Evalu-\nating claims and practices. In Proceedings of the 2020 conference on fairness, accountability, and transparency,\npages 469–481, 2020.\n[89] Howard Raiffa and Robert Schlaifer. Applied Statistical Decision Theory. Harvard University Press, 1961.\n[90] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. International Conference\non Machine Learning, pages 1530–1538, 2015.\n[91] Christian P Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implemen-\ntation. Springer Science & Business Media, 2007.\n[92] Gene Rowe and George Wright. The delphi technique as a forecasting tool: Issues and analysis. International\nJournal of Forecasting, 15(4):353–375, 1999.\n[93] Elizabeth G Ryan, Christopher C Drovandi, and Anthony N Pettitt. Toward bayesian experimental design for\nnonlinear models that require a large computational budget. Computational Statistics & Data Analysis, 97:67–\n86, 2016.\n[94] Leonard J Savage. The Foundations of Statistics. Courier Corporation, 1972.\n[95] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv\npreprint arXiv:2302.04761, 2023.\n[96] Silvana Secinaro, Davide Calandra, Alessandro Secinaro, Vimal Muthurangu, and Paolo Biancone. Artificial\nintelligence in healthcare: Past, present and future. Sustainability, 13(10):5762, 2021.\n[97] Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison Department\nof Computer Sciences, 2009.\n[98] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of\nthe loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016.\n[99] Victor S Sheng and Charles X Ling. Thresholding for making classifiers cost-sensitive. In Proceedings of the\nNational Conference on Artificial Intelligence, volume 21, page 476, 2006.\n[100] Yishai Shimoni, Chen Yanover, Ehud Karavani, and Yaara Goldschmnidt.\nBenchmarking framework for\nperformance-evaluation of causal inference analysis. In NeurIPS Workshop on Causal Learning, 2018.\n[101] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan\nWang. Prompting gpt-3 to be reliable. In International Conference on Learning Representations, 2023.\n[102] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature,\n620(7972):172–180, 2023.\n[103] Jennifer Smith and Michael Jones. Calculating the cost of hiring: A comprehensive guide. HR Magazine,\n65(3):48–53, 2020.\n[104] Society for Human Resource Management. The real costs of recruitment. Technical report, SHRM, 2016.\n[105] Matthijs TJ Spaan and Nikos Vlassis. Perseus: Randomized point-based value iteration for pomdps. Journal of\nArtificial Intelligence Research, 24:195–220, 2005.\n[106] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, et al. Beyond the imitation game: Quantifying and extrapo-\nlating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n71\n"}, {"page": 72, "text": "A PREPRINT - JANUARY 6, 2026\n[107] Stephen M Stigler. Simon newcomb, percy daniell, and the history of robust estimation 1885–1920. Journal of\nthe American Statistical Association, 68(344):872–879, 1973.\n[108] Swee Song Tan, Jan Bakker, Marga E Hoogendoorn, Atul Kapila, John Martin, Alessandra Pezzi, Giorgio\nPittoni, Peter E Spronk, Robert Welte, and Leona Hakkaart-van Roijen. The costs of intensive care. Intensive\nCare Medicine, 37(10):1640–1646, 2011.\n[109] Triplebyte. Assessing technical talent: Why traditional screening fails. Technical report, Triplebyte Inc., 2019.\n[110] Konstantinos Tzioumis. Demographic aspects of first names. Scientific Data, 5(1):1–9, 2018.\n[111] US Congress. Equal credit opportunity act, 1974. 15 U.S.C. § 1691 et seq.\n[112] Various Industry Sources. Industry hiring funnel conversion benchmarks, 2023. Aggregated from LinkedIn,\nGlassdoor, and SHRM reports.\n[113] Abraham Wald. Statistical Decision Functions. John Wiley & Sons, 1950.\n[114] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In\nAdvances in Neural Information Processing Systems, volume 32, 2019.\n[115] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-\ntask benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, 2018.\n[116] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint\narXiv:2308.11432, 2023.\n[117] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. Self-consistency improves chain of thought reasoning in large language models. arXiv preprint\narXiv:2203.11171, 2022.\n[118] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. volume 35, pages 24824–24837, 2022.\n[119] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng,\nMia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359, 2021.\n[120] Ben Wilson, Judy Hoffman, and Jamie Morgenstern. Building and auditing fair algorithms: A case study in\ncandidate screening. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,\npages 666–677, 2021.\n[121] David H Wolpert. Stacked generalization. volume 5, pages 241–259. Elsevier, 1992.\n[122] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie\nJin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint\narXiv:2309.07864, 2023.\n[123] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi.\nCan llms express\ntheir uncertainty? an empirical evaluation of confidence elicitation in llms. Proceedings of the International\nConference on Learning Representations, 2024.\n[124] Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Satzilla: Portfolio-based algorithm selection\nfor sat. Journal of Artificial Intelligence Research, 32:565–606, 2008.\n[125] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information\nProcessing Systems, 36, 2023.\n[126] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability esti-\nmates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data\nmining, pages 694–699, 2002.\n[127] Bianca Zadrozny, John Langford, and Naoki Abe.\nCost-sensitive learning by cost-proportionate example\nweighting. In Third IEEE international conference on data mining, pages 435–442. IEEE, 2003.\n[128] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning.\nIn Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335–340, 2018.\n[129] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large\nlanguage models: A reality check. arXiv preprint arXiv:2305.15005, 2023.\n72\n"}, {"page": 73, "text": "A PREPRINT - JANUARY 6, 2026\n[130] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference\nresolution.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 8–14, 2018.\n73\n"}, {"page": 74, "text": "A PREPRINT - JANUARY 6, 2026\nAlgorithm 2 Bayesian Multi-LLM Orchestration for Cost-Aware Sequential Decisions\n1: Input: Initial evidence x1 ∈X, state space S, action space A, cost matrix C : A×S →R≥0, prior π ∈∆(S), LLM ensemble\n{M1, . . . , MM}, disagreement threshold τD ∈R+, informativeness parameter ρ ∈[0, 1], information cost cinfo ∈R≥0\n2: Output: Terminal action a∗∈A\n3: // Initialization\n4: t ←1\n5: x ←x1\n6: b(s) ←π(s) for all s ∈S\n7: while true do\n8:\n// ========== STEP 1: Likelihood Elicitation ==========\n9:\nfor m = 1 to M do\n10:\nfor each s ∈S do\n11:\nGenerate contrastive prompt Pm(x, s) (Definition 2)\n12:\nrm,s ←Mm(Pm(x, s))\n13:\nˆLm(x | s) ←rm,s/10\n14:\nend for\n15:\nend for\n16:\n// ========== STEP 2: Robust Aggregation ==========\n17:\nfor each s ∈S do\n18:\nL(x | s) ←median{ˆL1(x | s), . . . , ˆLM(x | s)}\n(Eq. 63)\n19:\nend for\n20:\n// ========== STEP 3: Sequential Belief Update ==========\n21:\nZ ←P\ns′∈S L(x | s′) · b(s′)\n22:\nfor each s ∈S do\n23:\nbnew(s) ←L(x | s) · b(s)\nZ\n(Eq. 66)\n24:\nend for\n25:\nb ←bnew\n26:\n// ========== STEP 4: Disagreement Detection ==========\n27:\nfor each s ∈S do\n28:\nµs ←1\nM\nPM\nm=1 ˆLm(x | s)\n29:\nσs ←\nr\n1\nM −1\nPM\nm=1\n\u0010\nˆLm(x | s) −µs\n\u00112\n30:\nD(s) ←\nσs\nmax(µs, 10−8)\n31:\nend for\n32:\nDmax ←maxs∈S D(s)\n33:\n// ========== STEP 5: Information Gathering Decision ==========\n34:\nif Dmax > τD and information gathering is available then\n35:\nCcurrent ←mina∈A\nP\ns∈S b(s) · C(a, s)\n36:\nCperfect ←P\ns∈S b(s) · mina∈A C(a, s)\n37:\nVOI ←ρ · (Ccurrent −Cperfect)\n(Eq. 74)\n38:\nif VOI > cinfo then\n39:\nGather additional evidence xnew ∈X (e.g., phone screen)\n40:\nt ←t + 1\n41:\nx ←xnew\n42:\ngo to next iteration\n43:\nend if\n44:\nend if\n45:\n// ========== STEP 6: Terminal Action Selection ==========\n46:\na∗←arg mina∈A\nP\ns∈S b(s) · C(a, s)\n(Eq. 69)\n47:\nreturn a∗\n48: end while\n74\n"}]}