{"doc_id": "arxiv:2601.09696", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.09696.pdf", "meta": {"doc_id": "arxiv:2601.09696", "source": "arxiv", "arxiv_id": "2601.09696", "title": "Empathy Applicability Modeling for General Health Queries", "authors": ["Shan Randhawa", "Agha Ali Raza", "Kentaro Toyama", "Julie Hui", "Mustafa Naseem"], "published": "2026-01-14T18:47:02Z", "updated": "2026-01-14T18:47:02Z", "summary": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.09696v1", "url_pdf": "https://arxiv.org/pdf/2601.09696.pdf", "meta_path": "data/raw/arxiv/meta/2601.09696.json", "sha256": "cfcd4e69f00fa4fc57f40f9ad5a53901686cb8c84651fd09ec7164d0b481d85f", "status": "ok", "fetched_at": "2026-02-18T02:21:32.765429+00:00"}, "pages": [{"page": 1, "text": "Empathy Applicability Modeling for General Health Queries\nShan Randhawa1\nAgha Ali Raza2\nKentaro Toyama1\nJulie Hui1\nMustafa Naseem1\n1University of Michigan\n2Lahore University of Management Sciences\n{shanmr,toyama,juliehui,mnaseem}@umich.edu\nagha.ali.raza@lums.edu.pk\nAbstract\nLLMs are increasingly being integrated into\nclinical workflows, yet they often lack clini-\ncal empathy, an essential aspect of effective\ndoctor–patient communication. Existing NLP\nframeworks focus on reactively labeling em-\npathy in doctors’ responses but offer limited\nsupport for anticipatory modeling of empathy\nneeds, especially in general health queries. We\nintroduce the Empathy Applicability Frame-\nwork (EAF), a theory-driven approach that clas-\nsifies patient queries in terms of the applica-\nbility of emotional reactions and interpreta-\ntions, based on clinical, contextual, and lin-\nguistic cues. We release a benchmark of real\npatient queries, dual-annotated by Humans and\nGPT-4o. In the subset with human consensus,\nwe also observe substantial human–GPT align-\nment. To validate EAF, we train classifiers on\nhuman-labeled and GPT-only annotations to\npredict empathy applicability, achieving strong\nperformance and outperforming the heuristic\nand zero-shot LLM baselines. Error analysis\nhighlights persistent challenges: implicit dis-\ntress, clinical-severity ambiguity, and contex-\ntual hardship, underscoring the need for multi-\nannotator modeling, clinician-in-the-loop cali-\nbration, and culturally diverse annotation. EAF\nprovides a framework for identifying empathy\nneeds before response generation, establishes a\nbenchmark for anticipatory empathy modeling,\nand enables supporting empathetic communi-\ncation in asynchronous healthcare.\n1\nIntroduction\nClinical empathy integrates cognitive (understand-\ning), emotional (resonating), and action-oriented\n(expressing) components (Guidi and Traversa,\n2021). It is indispensable for clinical care, deep-\nening therapeutic relationships and improving out-\ncomes such as patient satisfaction, care effective-\nness, reduced distress, and hospital length of stay\n(Guidi and Traversa, 2021; Olson, 1995; Hoff-\nstädt et al., 2020); yet clinicians miss 70-90% of\nempathic opportunities during patient interactions\n(Morse et al., 2008; Hsu et al., 2012).\nLarge Language Models (LLMs) are increas-\ningly integrated into healthcare workflows and pa-\ntient interactions, with major EHR vendors such\nas EPIC adopting them for clinical messaging and\nnearly half of physicians reporting patients con-\nsult ChatGPT before visits (Antoniak et al., 2024;\nSermo Team, 2025). While these trends highlight\nrapid adoption of LLMs in healthcare, they also\nraise concerns of lacking empathy crucial for asyn-\nchronous physician-patient interactions (Koranteng\net al., 2023). However, effective empathy requires\ndiscernment, not just fluency. This highlights a\ncritical, antecedent challenge: How can we sys-\ntematically model the applicability of empathy, al-\nlowing systems to recognize the specific clinical\nand linguistic cues that necessitate an emotional\nresponse?\nModeling empathy in text is inherently difficult\nwithout non-verbal cues, and NLP research has his-\ntorically over-weighted emotional aspects while\noverlooking cognitive empathy (Lahnala et al.,\n2022), even though both are vital in clinical care.\nTo redress this imbalance, EPITOME (Sharma\net al., 2020) captures the multidimensionality of\nempathy through emotional reactions, interpreta-\ntions, and explorations, offering an empathetic lens\non warmth, understanding, and curiosity in mental\nhealth support. Online Empathy (Chai et al., 2019)\nalso addresses multidimensionality, classifying re-\nsponses as Informational and Emotional. However,\nboth EPITOME and Online Empathy assess empa-\nthy post hoc, labeling support-giver responses after\nthey appear and thus offering no guidance while\na clinician is composing a response to the patient\nquery.\nLahnala et al. attempt to solve this particular\nproblem with with an Appraisal Framework that an-\nnotates empathic opportunities and clinician elicita-\ntion and response as functions of (affect | judgment\n1\narXiv:2601.09696v1  [cs.CL]  14 Jan 2026\n"}, {"page": 2, "text": "| appreciation) in breaking-bad-news oncology dia-\nlogues (Lahnala et al., 2024). This discourse analy-\nsis lens excels at teaching stance shifts over multi-\nturn synchronous conversations, yet is not suited to\nsingle-turn, asynchronous general health queries:\nit classifies stance, not what the patient needs (cog-\nnitive clarification vs emotional warmth). Thus, it\nremains need-blind, providing little actionable help\nfor one-off replies.\nTo address this gap, we propose the Empathy\nApplicability Framework (EAF), a theoretically\ngrounded method to proactively identify when and\nwhat type of clinical empathy should be expressed\nin response to patient queries. EAF operational-\nizes empathy along two key dimensions: affec-\ntive (emotional reactions) and cognitive (interpre-\ntations) – labeling each as Applicable or Not Ap-\nplicable based on clinical, contextual, and linguis-\ntic cues within patient queries. Unlike prior work\nthat evaluates empathy in the response itself (Chai\net al., 2019; Sharma et al., 2020), EAF analyzes\nthe patient’s query before any response, enabling\nanticipatory reasoning. Evidence for the value of\nanticipation comes from Sibyl (Wang et al., 2025),\nwhich shows that anticipating user’s emotional and\ncontextual trajectory improves empathetic response\ngeneration. EAF can also mitigate limitations of\nreactive frameworks that overrely on surface lex-\nical cues. For instance, EPITOME misclassified\ngeneric phrases as empathetic in nonsensical con-\ntexts, with false-positive rates exceeding 95% (Lee\net al., 2023). By shifting empathy assessment from\npost-hoc response scoring to pre-response query\nanalysis, EAF is methodologically significant: it\nenables anticipatory inference of emotional and\ninterpretive support needs, helping providers and\nLLMs detect and act on empathic opportunities in\ngeneral health queries.\nWe make three primary contributions: (i) Frame-\nwork Design:\nwe introduce and theoretically\nground the EAF in clinical empathy literature,\nclearly differentiating our anticipatory model from\nprior post-hoc approaches; (ii) Annotated and\nanalyzed Benchmark: a novel dataset of 1,300\npatient queries annotated by humans and GPT-4o\n(included in the supplementary materials), demon-\nstrating EAF’s reliability and interpretability; and\n(iii) Operationalization Challenges: we identify\nand systematically analyze specific contexts where\nanticipatory empathy annotations diverge, high-\nlighting opportunities for future research in multi-\nannotator modeling, clinician-in-the-loop systems,\nand culturally sensitive annotation strategies.\n2\nEmpathy Applicability Framework and\nTheoretical Grounding\nThe EAF identifies empathetic needs proactively\nby assessing patient queries along two dimensions\nadapted from EPITOME (Sharma et al., 2020) and\ninformed by Chai et al.’s distinction between emo-\ntional and informational support (Chai et al., 2019):\nEmotional Reactions and Interpretations. Table 1\nsummarizes the EAF, detailing applicable and non-\napplicable cues for each dimension.\nTo develop EAF, we performed inductive the-\nmatic coding on 300 randomly selected patient\nqueries from the HealthcareMagic and iCliniq\ndatasets (Li et al., 2023), identified themes, formed\nsubcategories (cues), and finally, we iteratively re-\nfined to comprehensively and distinctly capture\nempathy applicability.\nAdditionally, we ground EAF cues in Patient-\nCentred Care (PCC), a widely used framework for\nclinician–patient communication, focusing on the\nPCC functions of responding to emotions and man-\naging uncertainty to ensure clinically grounded ex-\npressions of empathy (Epstein and Street Jr, 2007;\nMcCormack et al., 2011).\n3\nMethods\nTo determine whether EAF is reliably interpretable\nacross a range of clinical queries and to identify any\nsystematic challenges, we curated a diverse dataset\nof health-related queries and annotated them us-\ning the EAF, employing both human annotators\nand an LLM. To assess whether these annotations\nexhibit learnable patterns, indicating the internal\nconsistency of EAF, we trained classifiers on the\nEAF-labeled data. The following subsections detail\nthe annotation and modeling procedures.\n3.1\nData Source\nWe sampled 9,500 patient queries from two pub-\nlicly available datasets (HealthCareMagic and\niCliniq) released by Li et al. (Li et al., 2023). We\nsampled 4,750 queries each from HealthCareMagic\n(≈100k dialogues) and iCliniq (≈10k), to maxi-\nmize linguistic and contextual diversity and avoid\noverfitting to a single source. As these datasets\nare publicly available and anonymized, our IRB\ndetermined that this study was exempt from human\nsubjects review. The datasets do not carry an ex-\nplicit license; therefore, we use them exclusively\n2\n"}, {"page": 3, "text": "Dimensions\nApplicable cues\nNot Applicable cues\nEmotional\nReactions\nExpressions of\nwarmth, compassion,\nconcern, or similar\nfeelings conveyed by\na doctor in response\nto a patient’s query.\n• Severe Negative Emotion\n• Inferred Negative State\n• Seriousness of Symptoms\n• Concern for Relations\nRationale: Signals reflect distinct pathways of\nemotional distress, guiding when emotional reac-\ntions are warranted.\n• Routine Health Management\n• Purely Factual Medical Queries\n• Neutral Symptom Descriptions\n• Hypothetical Queries\nRationale: Signals no emotional content; omit re-\nactions to maintain factual medical focus.\nInterpretations\nCommunication of an\nunderstanding of the\npatient’s feelings\n(expressed or implied)\nand/or experiences\n(including contextual\nfactors) inferred from\nthe patient’s query.\n• Expression of Feeling\n• Experiences or Context Affecting Emotional\nState\n• Symptoms with an Emotional Impact\n• Distressing Uncertainty About Health\nRationale: Signals lived burden, context, or uncer-\ntainty requiring interpretive acknowledgment.\n• Emotional-Reactions N/A cues +:\nwith ab-\nsence of distressing contextual or experiential\ndetails.\nRationale: Signals absence of both emotional and\ncontextual cues, preventing over-empathizing and\nmaintaining focus on informational needs.\nTable 1: Empathy Applicability Framework (EAF). Each dimension lists cues for when an empathic dimension\nis Applicable or Not Applicable; Brief rationales explaining what each cue set captures follow the cues. Detailed\ndescription of the EAF and its cues with examples is provided in the Appendix A. Also, see Appendix Table 4 for\nconcrete query scenarios illustrating cues usage and EAF operationalization.\nfor non-commercial research, in line with the au-\nthors’ stated intention and public availability, and\nwill release our de-identified EAF benchmark un-\nder the same non-commercial terms. To balance\nrigor and cost, 1,500 of the queries were earmarked\nfor dual annotation by humans and GPT-4o to sup-\nport reliability and error analyses, while the re-\nmaining 8,000 were annotated only by GPT-4o for\npredictive validity testing.\n3.2\nAnnotation Task\nThe annotation task required using EAF to label\npatient queries as applicable or not applicable (see\nTable 1) on two dimensions of empathy: Emotional\nReactions (EA) and Interpretations (IA). Human an-\nnotators were instructed to identify at least one best-\nfitting subcategory per dimension to justify their\nlabels (they mostly selected a single best-fitting\nsubcategory). The GPT annotations listed all rele-\nvant subcategories supporting labeling decisions.\n3.2.1\nAnnotator Recruitment, Training and\nCalibration\nDue to empathy annotation subjectivity, we priori-\ntized consistency by avoiding crowdsourcing and\ninstead recruited and trained two annotators from\nPakistan with high English proficiency: HA1, a fe-\nmale with an MS in Linguistics, and HA2, a male\nwith a BS in Computer Science. We recruited two\nannotators via departmental channels for about a\none-month engagement. Informed consent to use\nthe annotated dataset to train large language models\nwas collected from the annotators prior to the start\nof the annotation process. Annotators were com-\npensated US$360 (equivalent to a local monthly\nresearch salary). Annotators underwent three-stage\ntraining on 200 queries (50 + 50 + 100) from a sub-\nset of 1,500, with convergence meetings after each\nstage to clarify misunderstandings and align label-\ning. Training queries were excluded from later ex-\nperiments. Annotators then independently labeled\nthe remaining 1,300 queries following procedures\nin Section 3.2. Annotation instructions are detailed\nin Appendix B.\nWe intentionally employed lay annotators to\ncapture the patient’s perspective. Prior research\nshows that empathy is ‘in the eye of the beholder’\n(Bernardo et al., 2018), and given that the empathy\nlevels in the clinician’s response will be perceived\nby the patient, lay annotators whose judgments\nreflect the patient/recipient experience are better-\nsuited for this task. Additionally, prior studies show\nthat clinicians often overlook empathic opportuni-\nties in favor of diagnostic focus (Hsu et al., 2012)\nand that patients’ ratings of clinicians’ empathy of-\nten diverge from clinicians’ assessments (Bernardo\net al., 2018; Hermans et al., 2018).\n3\n"}, {"page": 4, "text": "3.2.2\nGPT Annotations\nTo scale the data set and enable comparison with\nhuman annotations, we used GPT-4o via the Ope-\nnAI API, prompted to act as an expert annotator\nusing contrastive prompting (Gao and Das, 2024).\nThe model was given definitions of EA and IA,\nsubcategory descriptions with examples, and la-\nbels indicating whether each subcategory was Ap-\nplicable or Not Applicable. Then it returned the\nmatching subcategories, with the format inherently\nindicating the applicability class (annotation scripts\nincluded in the supplementary software). Complete\nprompt specifications, including with and without\nour framework are included in Appendix F.\nFor the 1,300 human-annotated queries, GPT-4o\ngenerated five annotation passes per query, with\nfinal labels determined by majority vote1. For the\nremaining 8,000 queries, a single-pass annotation\nwas used due to cost constraints. This yielded two\nsubsets: 1,300 queries labeled by both humans\nand GPT (with majority-voted GPT labels) and\n8,000 labeled solely by GPT (single-pass annota-\ntion). Note: Throughout the remainder of this text,\nall references to GPT refer specifically to GPT-4o.\n3.3\nModeling Task and Approach\nWe frame empathy applicability prediction as two\nindependent binary classification tasks. Given a\npatient query Pi, the objective is to predict, for each\nempathy dimension d ∈{EA, IA}, whether that\ndimension is Applicable (1) or Not Applicable (0),\ndenoted Aid. For each dimension, we fine-tune a\ndistinct RoBERTa-based classifier (Liu et al., 2019).\nFull architectural details, including the attention\nmechanism, the pooling operation, and the model\ndiagram, are provided in the appendix E.\n4\nEvaluation Setup and Experiments\nThis section details the evaluation setup and model\ntraining configurations used in our experiments.\n4.1\nAnnotator Agreement\nWe assessed human annotation reliability using raw\nagreement and Cohen’s Kappa across the 1,300 in-\ndependently labeled queries. For GPT-generated\nannotations, we compared majority-voted GPT la-\nbels with a subset of human-annotated queries:\n1Majority voting ensured consistency across passes. More\nthan 94% of queries received the same label on the first pass\nand as the majority vote for both empathy dimensions, indicat-\ning minimal divergence. Hence, we report evaluation metrics\nonly with the majority-voted labels.\nqueries where both human annotators reached an\nagreement. This allows us to evaluate GPT per-\nformance without confounding disagreement over\nerror or subjectivity.\n4.2\nConceptual Alignment\nTo examine whether humans and GPT rely on simi-\nlar rationales, we performed an UpSet plot analysis\n(Figure 1). This analysis was limited to queries\nwhere humans and GPT agreed on the overall ap-\nplicability label, allowing us to assess alignment\nin subcategory reasoning rather than outcome. A\nmatch is coded as Full if GPT includes both sub-\ncategories selected by the two human annotators,\nPartial if GPT’s subcategories overlap with only\none human’s subcategory label and No match if\nGPT matches neither human subcategory.\n4.3\nDivergence Bar and Qualitative Analysis\nGiven the subjective nature of empathy, we ana-\nlyze mismatches as directional divergences rather\nthan strict errors. To characterize disagreement,\nwe use three-way divergence bars (Figure 2) that\ndecompose label mismatches within each subcat-\negory into Annotator Spread (one human labeled\nApplicable, the other Not), LLM-Adds (GPT la-\nbeled Applicable, humans Not), and LLM-Omits\n(GPT labeled Not, humans Applicable). Further-\nmore, we performed qualitative analysis on a subset\nof queries where GPT labeled differently, and iden-\ntified thematic patterns that highlight the different\nlabeling.\n4.4\nModel Evaluation\nWe evaluated the performance of the classifiers\ntrained to predict empathy applicability (Applica-\nble vs. Not Applicable). Reported metrics include\naccuracy, weighted F1 score, and macro-averaged\nF1 score across both dimensions (EA and IA). To\ncontextualize classifier performance, we compared\nresults against four baselines: Random Guessing\n(assigns labels at random), Always Applicable, Al-\nways Not Applicable, and o1-Zero-Shot (based on\nOpenAI’s reasoning model, without invoking em-\npathy applicability framework). For the o1 base-\nline, we provide only the definition of the target\ndimension (EA or IA) and prompt it to classify each\npatient query as ‘Applicable’ or ‘Not Applicable’,\npreserving the zero-shot setting without framework\ncues. These baselines help determine whether our\ntrained models learn meaningful patterns beyond\nsimple heuristics or zero-shot LLM reasoning. In\n4\n"}, {"page": 5, "text": "Dimension\nHuman–Human\nHuman–GPT\nκ (agree / disagree)\nκ (agree/ disagree)\nEA\n0.521 (981 / 315)\n0.617 (668 / 152)\nIA\n0.404 (898 / 398)\n0.652 (678 / 142)\nTable 2:\nCohen’s κ with agreement counts:\nhu-\nman–human agreement on the full set and human–GPT\nalignment on the human-consensus subset.\naddition, following best practices for sanity check-\ning and overfitting control, we include classical text\nclassifiers trained and evaluated on the Human Set\n(Section 4.5): Logistic Regression (LR) and Linear\nSVM with TF–IDF features(1–3 grams). These\nlinear models provide a transparent reference for\nwhat is learnable from local lexical features.\n4.5\nModel Training and Training Sets\nEach classifier for the EA and IA tasks is based\non RoBERTa-base (≈125 M parameters) and was\ntrained on two distinct datasets (data and scripts\nincluded in the supplementary material): Human\nSet: Contains only queries where both human anno-\ntators reach consensus on a label for a given dimen-\nsion, serving as a high-fidelity benchmark aligned\nwith human judgment. Autonomous Set: Consists\nof GPT-labeled data from the 8,000-query pool,\nwith no human supervision. This tests whether\nmodels trained solely on GPT output can approxi-\nmate human consensus.\nFor the Human Set, we split the data into sub-\nsets of training (75%), validation (5%), and test\n(20%). For the Autonomous Set, training was done\nentirely on GPT-labeled data, but testing used the\nsame human-consensus test set as the Human Set\nto enable consistent evaluation relative to human\nagreement. Training used a single NVIDIA A40\nGPU per run. A Human-Set run finished in ≈15\nmin GPU time, while an Autonomous-Set run took\n≈40 min; thus the total compute budget per dimen-\nsion is <1 GPU-hour. All models were trained for\n10 epochs using a learning rate of 2 × 10−5 and a\nbatch size of 8. To ensure comparability, all models\nshared the same architecture and hyperparameters.\n5\nResults\nIn this section, we present our findings related to\nthe reliability of the EAF and the challenges in\noperationalizing it. Additional dataset characteri-\nzation beyond agreement and modeling results is\nprovided in Appendix G.\n5.1\nReliability of the EAF\nWe evaluated reliability along three axes of Con-\nsistency, Predictive validity and Conceptual align-\nment.\nConsistency.\nWe first assess agreement on Ap-\nplicable/Not Applicable labeling between human\nannotators across 1,300 queries, and between GPT-\n4o and the human consensus on a subset of 8202\nqueries. As shown in Table 2, human annotators\nachieved moderate agreement on both empathy di-\nmensions, with an overall Cohen’s κ of 0.46. This\nfalls within the typical range for empathy annota-\ntion tasks; for example, Sibyl (Wang et al., 2025) re-\nported scores between 0.4 and 0.6. Notably, agree-\nments outnumbered disagreements by a factor of\ntwo to three, suggesting that the EAF supports rela-\ntively consistent human labeling despite the inher-\nent subjectivity of empathy.\nGPT aligned well with the human consensus\ndataset, queries where both humans agreed, achiev-\ning three-way agreement. For both EA and IA, Co-\nhen’s κ exceeded 0.6 and raw agreement was about\n80% (Table 2). These results reflect agreement\non human-aligned cases, demonstrating EAF’s ef-\nfectiveness in guiding GPT to anticipate empathy\napplicability in clearer contexts, excluding more\nambiguous or complex queries (see section 5.2).\nPredictive Validity.\nWe next evaluated whether\nEAF annotations are machine-learnable. As shown\nin Table 3, classifiers trained on human consen-\nsus data achieved high performance: LR attains\n0.84 Macro-F1 for EA and 0.80 for IA (SVM:\n0.83/0.77), establishing a strong classical reference.\nOur transformer (RoBERTa-base) exceeds these\nvalues significantly (for EA: (0.92 vs. 0.84); and\nfor IA (0.87 vs. 0.80)). Models trained on GPT-\nonly annotations (the Autonomous set) also per-\nformed well, achieving around 0.85 for EA and\n0.77 for IA on the same held-out human-consensus\ntest set, reflecting expected loss from noisier labels\nor differences from human labeling. Our models\nalso significantly outperformed the trivial baselines\n(random guessing, always applicable, always not\napplicable, and o1 Zero-Shot), which yielded sub-\nstantially lower scores. McNemar’s test (McNemar,\n1947) confirmed statistical significance of the trans-\nformers over the trivial baselines (max p < 10−4)\n2For the full set, Appendix Table 5 shows comparable GPT\nagreement with each human annotator on affective EA, but\nsubstantially more variable agreement on cognitive IA.\n5\n"}, {"page": 6, "text": "Table 3: Classification results across training sets and baselines (single run on the human-consensus test set). Bold\nindicates best performance. Classical baselines (TF–IDF+LR/SVM) are trained on the Human Set only. Our\ntransformer significantly outperforms all the baselines.\nTraining Set / Model\nEA\nIA\nAcc Macro-F1 Wtd-F1 Acc Macro-F1 Wtd-F1\nRandom\n0.47\n0.47\n0.47\n0.44\n0.43\n0.44\nAlways Applicable\n0.52\n0.34\n0.36\n0.53\n0.35\n0.37\nAlways Not Applicable\n0.48\n0.32\n0.31\n0.47\n0.32\n0.30\no1 Zero-Shot\n0.55\n0.40\n0.41\n0.62\n0.53\n0.54\nHuman-supervised models (train and tested on human-consensus set)\nLogistic Regression\n0.84\n0.84\n0.84\n0.80\n0.80\n0.80\nLinear SVM\n0.83\n0.83\n0.83\n0.77\n0.77\n0.77\nTransformer (RoBERTa-base) 0.92\n0.92\n0.92\n0.87\n0.87\n0.87\nAutonomous-supervised model (train on GPT labels, test on human-consensus test set)\nTransformer (RoBERTa-base)\n0.85\n0.85\n0.85\n0.78\n0.77\n0.77\n(a) Interpretation Applicability (IA) subcategory matches\n(b) Emotional Applicability (EA) subcategory matches\nFigure 1: UpSet plots comparing GPT and human rationales for (a) IA and (b) EA subcategories. For each query,\neach human annotator selects one best-fit subcategory for their rationale; thus the human set is either a single-\ndot combination (both humans chose the same subcategory) or a two-dot combination (humans chose different\nsubcategories). Horizontal bars show how often each subcategory appears in the human annotation set across all\nqueries. Each vertical bar shows the frequency of a unique human-combination and is split by GPT agreement: Full\n(GPT’s subcategory set covers the entire human set), Partial (GPT matches only one of the two human subcategories),\nand No match (GPT matches neither human subcategory).\nand over the classical baselines (max p ≤0.02).\nTaken together, strong linear performance indicates\nconsistent linguistic realizations of the constructs,\nwhile the transformer’s margin suggests benefits\nfrom broader context rather than overfitting. Over-\nall, these results show that EAF-labeled data en-\ncode structured and learnable patterns.\nConceptual Alignment.\nWe further examined\nwhether humans and GPT rely on similar reason-\ning when assigning EAF labels. UpSet plot analy-\nsis (Figure 1) shows strong conceptual alignment.\nIn many cases, both human annotators indepen-\ndently selected the same subcategory and GPT\nmatched it, especially for both applicability and\nnon-applicability cues such as Severe Emotion or\nFactual Queries. These matches indicate that the\nEAF defines meaningful categories that are consis-\ntently identifiable by both humans and LLMs.\nWhen annotators selected different subcate-\ngories for the same label, GPT often matched both.\nFor example, in queries involving both Expression\nof Feeling and Distressing Uncertainty, GPT cited\nboth reasons, suggesting that GPT can reconcile di-\nverse human rationales and underscores the frame-\nwork’s breadth in conceptualizing clinical empa-\nthy. No-match cases are rare, and GPT typically\noverlaps with at least one human subcategory. Ap-\npendix G.4 quantifies and corroborates these trends,\nshowing match rates (match vs. miss, conditioned\non humans using that subcategory) above 80% for\nmost subcategories.\nCollectively, these results establish that EAF sup-\nports consistent human judgments, yields learn-\nable patterns, and promotes interpretable reasoning\n6\n"}, {"page": 7, "text": "across both humans and LLMs, making it well\nsuited for anticipatory empathy modeling in clini-\ncal settings.\n5.2\nSystematic Challenges in Operationalizing\nAnticipatory Empathy\nDivergence bar analysis (Section 4) revealed that\ninter-human agreement is significantly lower for\ninterpretations (IA) than for Emotional Reactions\n(EA) (Table 2), and that despite moderate overall\nhuman-GPT agreement (Table 2), there is diver-\ngence at the subcategory level. Subsequent qual-\nitative analysis revealed three key challenges in\napplying the EAF, with implications for any clini-\ncal empathy framework in NLP.\n5.2.1\nChallenge 1: Subjectivity in Identifying\nImplied Distress\nThe categories Inferred Negative State (EA) and\nDistressing Uncertainty (IA) show substantial di-\nvergence in inter-human and human-GPT annota-\ntions (Figure 2).\nA qualitative review of 50 randomly selected\ncases3 (25 each for Distressing Uncertainty and In-\nferred Negative State)4 by the first author acting as\nadjudicator revealed that in more than 50% of the\nqueries, one could reasonably infer implied emo-\ntional distress or determine that the query is driven\nby factual intent. For instance, the female anno-\ntator labeled a pain-and-menstrual-cycle query as\nDistressing Uncertainty, while the male annotator\ntreated it as a factual diagnostic request, illustrating\nthe subjectivity of distress inference.\n5.2.2\nChallenge 2: Clinical-Severity\nAmbiguity\nIn the category Serious Symptoms (EA), GPT la-\nbeled 100 queries as requiring emotional reactions\nwhen humans did not (Figure 2). Qualitative anal-\nysis of 25 randomly selected cases3 where only\nGPT had labeled empathy applicability revealed\nthree patterns: (1) In 40% of the cases, GPT appro-\npriately flagged empathy needed for patients with\nchronic or life-threatening conditions (e.g., post-\nliver transplant complications) that human annota-\ntors with no medical background had overlooked\n(2) borderline cases with reasonable disagreement\n3Detailed patient queries, mis-aligned labels, and qualita-\ntive interpretations are included in the supplementary material\nas the misalignment_analysis.csv file\n4a sample size consistent with prior clinical-NLP error\nanalyses; (Hu et al., 2024)\n(16%), such as prolonged low-grade fever after kid-\nney stones, and (3) GPT overgeneralization of vivid\nbut non-serious pain symptoms (44%) that did not\nmeet the EAF criteria of chronic or life-threatening\nseverity (for example, lip numbness after dental\nproblems).\n5.2.3\nChallenge 3: Contextual Hardship\nGPT frequently over-applied Symptoms Emotional\nImpact (SEI) and Context Sharing (CS) tags com-\npared to humans (Figure 2). An analysis of 25 ran-\ndomly selected3 mismatched labels in SEI category,\nand all 20 mismatches in CS revealed that while\nGPT sometimes correctly identified complex dis-\ntress signals humans missed (20-25% of the cases),\nit more often (75-80% of the cases) equated physi-\ncal discomfort with emotional distress – potentially\nreflecting Western-centric training biases (Johnson\net al., 2022; Cao et al., 2023).\nThese challenges, rooted in subjective inference,\nclinical ambiguity, and cultural variation, highlight\nthe complexity of implementing clinical empathy.\nAddressing them requires moving beyond single-\nannotator consensus toward frameworks that em-\nbrace interpretive pluralism, clinical expertise, and\ncultural sensitivity.\n6\nDiscussion and Conclusion\nAsynchronous patient communication requires an-\nticipatory mechanisms that can signal empathic\nneeds before a response is written. EAF addresses\nthis gap by assigning applicability labels to patient\nqueries, indicating whether empathy is warranted\nand which dimension (emotional versus interpre-\ntive) should be expressed. This framing comple-\nments recent advances in empathetic response gen-\neration, which enrich responses by reasoning about\nemotional causes, rhetorical framing, and likely fu-\nture trajectories. For example, Lee et al. integrate\nfigurative language and empathy-cause signals to\ngenerate responses that are both emotionally en-\ngaging and contextually grounded. Complementar-\nily, Chen et al. use cause-aware chain-of-thought\nprompting: they extract the emotional cause from\nthe dialogue and insert generated commonsense\nrelations (xEffect, xReact, xIntent, xNeed, xWant)\nconditioned on the extracted cause into the prompt,\nwhich significantly improves empathy in human\nevaluations (Chen et al., 2024). Likewise, Sibyl\n(Wang et al., 2025) helps models anticipate the\nseeker’s likely next move and improves empa-\nthetic generation. However, these systems are de-\n7\n"}, {"page": 8, "text": "Figure 2: Three-way divergence for every subcategory. Orange = Annotator Spread in Humans (One Applicable,\nother not); Blue = LLM-Adds Empathy Dimension (GPT Applicable, Humans Not); Green = LLM-Omits Empathy\nDimension (GPT Not, Humans Applicable).\nsigned primarily for open-domain emotional sup-\nport and often assume the user’s emotional state\nis explicit or readily inferable. In contrast, gen-\neral health queries frequently contain ambiguous,\nsubtle, or medically grounded cues about whether\nemotional reactions or interpretive support are ap-\npropriate. Our anticipatory assessment framework\nidentifies the empathetic needs embedded in the\npatient’s query prior to generation, and can be inte-\ngrated with cause-aware, figurative-language, and\ncommonsense-augmented methods to guide empa-\nthetic response generation in clinical and general-\nhealth settings.\nHowever, EAF faces challenges from subjec-\ntive inference, particularly when cues are implicit\n(e.g., Inferred Negative Emotional State). As ap-\npraisal theory suggests, divergent interpretations\nof distress often reflect genuine ambiguity rather\nthan noise (Wondra and Ellsworth, 2015). Further-\nmore, these appraisals are shaped by cultural pri-\nors of emotion. Eichbaum et al. (2023) warn that\nWestern-centric empathy models can misfire cross-\nculturally. Indeed, GPT-4o—trained on predomi-\nnantly Western data (Johnson et al., 2022)—often\nlabeled minor inconveniences as empathy-worthy\nwhere our South Asian annotators did not. This\nhighlights a cultural bias inherent to LLMs that\nencode American norms (Cao et al., 2023).\nThe NLP community increasingly embraces\nthis variability through multi-annotator models\nand annotator-aware representations that yield cali-\nbrated uncertainty estimates and capture interpre-\ntive styles (Davani et al., 2022; Mokhberian et al.,\n2023). Gordon et al.’s (Gordon et al., 2021) jury\nlearning further shows how selecting annotator sub-\nsets aligned with demographic perspectives can pre-\nserve pluralism (Gordon et al., 2021). In clinical\nempathy contexts, retaining subjective variability\ncan help anticipate diverse patient needs, includ-\ning domain-specific perspectives (e.g., oncologists\nwho prioritize emotional support as central to care)\n(Dekker et al., 2020). Building on this, we advo-\ncate extending disagreement-aware and annotator-\naware frameworks toward diversity-aware model-\ning that explicitly accounts for culturally patterned\ndifferences in what counts as an empathy need,\nrather than collapsing them into a single consensus\nlabel.\nThis work makes three contributions to clinical\nempathy in NLP. First, we introduce the Empathy\nApplicability Framework (EAF), shifting from re-\nactive to anticipatory modeling. Second, we estab-\nlish a benchmark of 1,300 patient queries demon-\nstrating reliable EAF labels. Third, our analysis\nidentifies challenges, namely subjective inference,\nclinical-severity ambiguity, and contextual hard-\nship, as opportunities to embrace interpretive plu-\nralism via multi-annotator frameworks. By com-\nbining a practical framework with empirical oper-\nationalization, this work advances empathy mod-\neling that respects interpretive complexity while\nremaining computationally tractable.\n8\n"}, {"page": 9, "text": "7\nLimitations\nOur study faces three key constraints, the first\ntwo mirroring limitations reported by Ali et al.\n(2025). First, we relied on only two human annota-\ntors, neither of whom had clinical training, which\nlimited the range of perspectives represented; ex-\npanding the size, clinical expertise, and cultural\ndiversity of the annotator pool would better cap-\nture the variability of empathy judgments. Sec-\nond, all automatic annotations were produced with\nGPT-4o—selected for its widespread availability\nthrough ChatGPT—but this exclusive focus on the\nGPT series limits the generalization of our find-\nings to other model architectures (e.g., Gemini,\nClaude, GPT reasoning models, or open-source\nalternatives). Third, human annotators selected\na single most-salient subcategory per dimension,\nwhile GPT-4o returned multiple subcategories; this\nprocedural mismatch hinders direct comparison of\ndisagreement patterns, and aligning the guidelines\nwould allow for more rigorous evaluation. Future\nwork should therefore involve a more diverse set of\nhuman annotators, evaluate multiple LLM families\ntrained under different specifications, and standard-\nize annotation procedures between humans and\nmodels to obtain broader insights for improving\nempathy modeling in NLP for clinical contexts.\n8\nEthical considerations\nWe developed the EAF to augment not replace clini-\ncian empathy judgments. Deploying EAF therefore\nrequires close attention to several intertwined ethi-\ncal risks that must be mitigated through thoughtful\ndesign and implementation.\nA primary concern is the moral and social im-\npact of artificial empathy. Because LLMs lack au-\nthentic emotional experience, we must ask whether\nthe ‘applicable emotional reactions’ they gener-\nate can truly convey warmth or connection. If\nusers perceive these reactions as hollow or manip-\nulative, an uncanny valley effect could ensue, in\nwhich attempted comfort backfires by appearing\ninauthentic. Determining whether, when, and how\nautomated empathy should be implemented, and\naddressing potential deception or user discomfort,\nrequires a systematic study of user perceptions of\nauthenticity versus artificiality.\nA second mirror image danger arises from the\nsame gap between simulated language and gen-\nuine feeling. As Empathic AI Can’t Get Under\nthe Skin discussed, LLMs lack the biological and\npsychological underpinnings that ground human\nempathy, yet their empathic language can evoke\nreal emotional responses (Nature Machine Intel-\nligence, 2024). Kirk et al. warn that users may\nform perceived emotional bonds with such systems,\nrisking unhealthy attachment or disclosure of sen-\nsitive information (Nature Machine Intelligence,\n2024). Thus, rejection born of perceived inauthen-\nticity and devotion born of mistaken authenticity\nare twin failure modes rooted in the same ontologi-\ncal limitation.\nFor these reasons, we insist that the EAF be\nused strictly within a human-in-the-loop pipeline.\nClinicians must retain final authority over how and\nwhen empathy is expressed, supported by trans-\nparent rationales and safeguards that guard against\nboth deceptive alienation and false intimacy, thus\nprotecting patients from the dual harms of artificial\nempathy.\nReferences\nIqra Ali, Jesse Atuhurra, Hidetaka Kamigaito, and Taro\nWatanabe. 2025. Hlu: Human vs llm generated text\ndetection dataset for urdu at multiple granularities.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, pages 3495–3510.\nMaria Antoniak, Aakanksha Naik, Carla S Alvarado,\nLucy Lu Wang, and Irene Y Chen. 2024. Nlp for\nmaternal healthcare: Perspectives and guiding princi-\nples in the age of llms. In Proceedings of the 2024\nACM Conference on Fairness, Accountability, and\nTransparency, pages 1446–1463.\nMonica Oliveira Bernardo, Dario Cecílio-Fernandes,\nPatrício Costa, Thelma A Quince, Manuel João\nCosta, and Marco Antonio Carvalho-Filho. 2018.\nPhysicians’ self-assessed empathy levels do not\ncorrelate with patients’ assessments.\nPloS one,\n13(5):e0198488.\nYong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min\nChen, and Daniel Hershcovich. 2023.\nAssessing\ncross-cultural alignment between chatgpt and hu-\nman societies: An empirical study. arXiv preprint\narXiv:2303.17466.\nYibo Chai, Fengyang Wu, Rui Sun, Zhongliang Zhang,\nJie Bao, Runxin Ma, Qizhou Peng, Danqin Wu, Yex-\ning Wan, and Keyu Li. 2019. Predicting future allevi-\nation of mental illness in social media: an empathy-\nbased social network perspective. In 2019 IEEE Intl\nConf on Parallel & Distributed Processing with Appli-\ncations, Big Data & Cloud Computing, Sustainable\nComputing & Communications, Social Computing\n& Networking (ISPA/BDCloud/SocialCom/Sustain-\nCom), pages 1564–1571. IEEE.\n9\n"}, {"page": 10, "text": "Xinhao Chen, Chong Yang, Man Lan, Li Cai, Yang\nChen, Tu Hu, Xinlin Zhuang, and Aimin Zhou.\n2024.\nCause-aware empathetic response genera-\ntion via chain-of-thought fine-tuning. arXiv preprint\narXiv:2408.11599.\nAida Mostafazadeh Davani, Mark Díaz, and Vinodku-\nmar Prabhakaran. 2022. Dealing with disagreements:\nLooking beyond the majority vote in subjective an-\nnotations. Transactions of the Association for Com-\nputational Linguistics, 10:92–110.\nJoost Dekker,\nJeanet Karchoud,\nAnnemarie MJ\nBraamse, Hilde Buiting, Inge RHM Konings, Myra E\nvan Linde, Claudia SEW Schuurhuizen, Mirjam AG\nSprangers, Aartjan TF Beekman, and Henk MW Ver-\nheul. 2020. Clinical management of emotions in\npatients with cancer: introducing the approach “emo-\ntional support and case finding”. Translational be-\nhavioral medicine, 10(6):1399–1405.\nQuentin Eichbaum, Charles-Antoine Barbeau-Meunier,\nMary White, Revathi Ravi, Elizabeth Grant, Helen\nRiess, and Alan Bleakley. 2023. Empathy across\ncultures–one size does not fit all: from the ego-logical\nto the eco-logical of relational empathy. Advances in\nHealth Sciences Education, 28(2):643–657.\nRonald M Epstein and Richard L Street Jr. 2007. Patient-\ncentered communication in cancer care: promoting\nhealing and reducing suffering.\nXiang Gao and Kamalika Das. 2024. Customizing lan-\nguage model responses with contrastive in-context\nlearning. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 38, pages 18039–\n18046.\nMitchell L Gordon, Kaitlyn Zhou, Kayur Patel, Tat-\nsunori Hashimoto, and Michael S Bernstein. 2021.\nThe disagreement deconvolution: Bringing machine\nlearning performance metrics in line with reality. In\nProceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, pages 1–14.\nClarissa Guidi and Chiara Traversa. 2021. Empathy\nin patient care: from ‘clinical empathy’to ‘empathic\nconcern’. Medicine, Health Care and Philosophy,\n24:573–585.\nLianne Hermans, Tim Olde Hartman, and Patrick W\nDielissen. 2018. Differences between gp perception\nof delivered empathy and patient-perceived empa-\nthy: a cross-sectional study in primary care. British\nJournal of General Practice.\nHinke Hoffstädt, Jacqueline Stouthard, Maartje C Mei-\njers, Janine Westendorp, Inge Henselmans, Peter\nSpreeuwenberg, Paul de Jong, Sandra van Dulmen,\nand Liesbeth M van Vliet. 2020. Patients’ and clin-\nicians’ perceptions of clinician-expressed empathy\nin advanced cancer consultations and associations\nwith patient outcomes. Palliative Medicine Reports,\n1(1):76–83.\nIan Hsu, Somnath Saha, Phillip Todd Korthuis, Victo-\nria Sharp, Jonathon Cohn, Richard D Moore, and\nMary Catherine Beach. 2012. Providing support to\npatients in emotional encounters: a new perspective\non missed empathic opportunities. Patient education\nand counseling, 88(3):436–442.\nYan Hu, Qingyu Chen, Jingcheng Du, Xueqing Peng,\nVipina Kuttichi Keloth, Xu Zuo, Yujia Zhou, Zehan\nLi, Xiaoqian Jiang, Zhiyong Lu, et al. 2024. Im-\nproving large language models for clinical named\nentity recognition via prompt engineering. Journal\nof the American Medical Informatics Association,\n31(9):1812–1820.\nRebecca L Johnson, Giada Pistilli, Natalia Menédez-\nGonzález, Leslye Denisse Dias Duran, Enrico Panai,\nJulija Kalpokiene, and Donald Jay Bertulfo. 2022.\nThe ghost in the machine has an american ac-\ncent:\nvalue conflict in gpt-3.\narXiv preprint\narXiv:2203.07785.\nErica Koranteng, Arya Rao, Efren Flores, Michael Lev,\nAdam Landman, Keith Dreyer, and Marc Succi. 2023.\nEmpathy and equity: Key considerations for large\nlanguage model adoption in health care. JMIR Medi-\ncal Education, 9:e51199.\nAllison Lahnala, Charles Welch, David Jurgens, and\nLucie Flek. 2022. A critical reflection and forward\nperspective on empathy and natural language process-\ning. arXiv preprint arXiv:2210.16604.\nAllison Claire Lahnala, Béla Neuendorf, Alexander\nThomin, Charles Welch, Tina Stibane, and Lucie Flek.\n2024. Appraisal framework for clinical empathy: A\nnovel application to breaking bad news conversa-\ntions. In Proceedings of the 2024 Joint International\nConference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024),\npages 1393–1407.\nAndrew Lee, Jonathan K Kummerfeld, Larry An, and\nRada Mihalcea. 2023. Empathy identification sys-\ntems are not accurately accounting for context. In\nProceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 1686–1695.\nGyeongeun Lee, Zhu Wang, Sathya N Ravi, and Natalie\nParde. 2025. From heart to words: Generating em-\npathetic responses via integrated figurative language\nand semantic context signals. In Findings of the As-\nsociation for Computational Linguistics: ACL 2025,\npages 4490–4502.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve\nJiang, and You Zhang. 2023. Chatdoctor: A medical\nchat model fine-tuned on a large language model\nmeta-ai (llama) using medical domain knowledge.\nCureus, 15(6).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n10\n"}, {"page": 11, "text": "Lauren A McCormack, Katherine Treiman, Douglas Ru-\npert, Pamela Williams-Piehota, Eric Nadler, Neeraj K\nArora, William Lawrence, and Richard L Street Jr.\n2011. Measuring patient-centered communication in\ncancer care: a literature review and the development\nof a systematic approach. Social science & medicine,\n72(7):1085–1095.\nQuinn McNemar. 1947. Note on the sampling error\nof the difference between correlated proportions or\npercentages. Psychometrika, 12(2):153–157.\nN Mokhberian, MG Marmarelis, FR Hopp, V Basile,\nF Morstatter, and K Lerman. 2023.\nCapturing\nperspectives of crowdsourced annotators in subjec-\ntive learning tasks. arxiv preprint. arXiv preprint\narXiv:2311.09743.\nDiane S Morse, Elizabeth A Edwardsen, and Howard S\nGordon. 2008. Missed opportunities for interval em-\npathy in lung cancer communication. Archives of\ninternal medicine, 168(17):1853–1858.\nNature Machine Intelligence. 2024. Empathic ai can’t\nget under the skin. Nature Machine Intelligence,\n6:495.\nJoanne K Olson. 1995. Relationships between nurse-\nexpressed empathy, patient-perceived empathy and\npatient distress.\nImage: The Journal of Nursing\nScholarship, 27(4):317–322.\nTorkel Richert, Björn Johnson, and Bengt Svensson.\n2018. Being a parent to an adult child with drug\nproblems: Negative impacts on life situation, health,\nand emotions. Journal of Family Issues, 39(8):2311–\n2335.\nSermo Team. 2025. Can physicians and patients trust\nAI doctor apps like ChatGPT? https://www.sermo.\ncom/resources/ai-doctor-app/. Blog post; ac-\ncessed 22 July 2025.\nAshish Sharma, Adam S Miner, David C Atkins, and\nTim Althoff. 2020. A computational approach to un-\nderstanding empathy expressed in text-based mental\nhealth support. arXiv preprint arXiv:2009.08441.\nLanrui Wang, Jiangnan Li, Chenxu Yang, Zheng Lin,\nHongyin Tang, Huan Liu, Yanan Cao, Jingang Wang,\nand Weiping Wang. 2025. Sibyl: Empowering empa-\nthetic dialogue generation in large language models\nvia sensible and visionary commonsense inference.\nIn Proceedings of the 31st International Conference\non Computational Linguistics, pages 123–140.\nJoshua D Wondra and Phoebe C Ellsworth. 2015.\nAn appraisal theory of empathy and other vicari-\nous emotional experiences. Psychological review,\n122(3):411.\nA\nEmpathy Applicability Framework\nDetail\nA.1\nEmotional Reactions in General Health\nQueries\nA.1.1\nDefinition\nEmotional Reactions refer to expressions of\nwarmth, compassion, concern, or similar feelings\nconveyed by a doctor in response to a patient’s\nquery. These reactions aim to provide emotional\nsupport and reassurance to the patient.\nA.1.2\nEmotional Reactions Not Applicable\n(N/A)\nEmotional reactions are not necessary or expected\nin the doctor’s response when the patient’s query is\nfactual, neutral, or a simple advice request, without\nexpressing emotional distress. Below are detailed\ncategories reflecting when emotional reactions are\nnot applicable:\n1. Purely Factual Medical Queries Descrip-\ntion: The patient requests specific medical informa-\ntion, including explanations of medical concepts,\nwithout emotional distress or underlying distress-\ning uncertainty.\nExamples:\n• \"What is the use of Tylenol?\"\n• \"Is it possible to outgrow a seafood allergy?\"\n2.\nGeneral Health Management Without\nEmotional Involvement Description: The patient\nseeks guidance on health management, follows up\non prior advice, or requests basic guidance on mi-\nnor health issues, without expressing emotional\ndistress or underlying distressing uncertainty. Here\nthe guidance is on what the patient should do.\nExamples:\n• \"I’m managing diabetes with insulin. How often\nshould I check my blood sugar levels?\"\n• \"I have swelling in my ankle after a long walk.\nShould I be concerned?\"\n• \"I had an X-ray for a fracture; should it be\nstrapped or cast right away?\"\n3. Diagnosis Requests with Neutral Symptom\nDescriptions Description: The patient describes\nsymptoms neutrally without expressing emotional\ndistress or underlying distressing uncertainty. Here\nthe request is about asking what the doctor thinks\nthe issue is.\nExamples:\n• \"I have intermittent knee pain from working out.\nHow would I know if I tore cartilage?\"\n11\n"}, {"page": 12, "text": "• \"Hello. I am having pain in my jaw area, immedi-\nately in front of my left ear. The pain is random.\nMy feeling is it is somehow related to sinus but\nthat’s just a gut feeling.\"\n4.\nHypothetical Medical Queries Without\nEmotional Concern Description: The patient in-\nquires about hypothetical situations without emo-\ntional involvement.\nExamples:\n• \"If someone has XYZ symptoms, what might be\nthe cause?\"\n• \"What would happen if a person skipped their\nmedication?\"\nA.1.3\nEmotional Reactions Applicable\nDefinition: Emotional reactions are necessary or\nexpected in the doctor’s response when:\n• The patient expresses emotions like fear, worry,\nfrustration, or distress.\n• The patient implies emotional distress over symp-\ntoms affecting their well-being.\n• The patient’s tone suggests a need for reassurance\nor emotional support.\n• The patient is expressing concern for a close re-\nlation (e.g., a child, spouse).\nBelow are detailed categories reflecting when\nemotional reactions are applicable:\n1. Seriousness of Symptoms Definition: The\npatient describes symptoms that suggest a life-\nthreatening or chronic health condition signifi-\ncantly impacting long-term health or quality of\nlife. This includes diseases like cancer, heart dis-\nease, mental health issues, or chronic conditions\nleading to disability. The symptoms suggest a life-\nthreatening or serious health condition that could\nsignificantly impact long-term health or quality of\nlife.\nExamples:\n• \"My father has been having severe chest pains\nand shortness of breath. Could it be a heart at-\ntack?\"\n• \"I’ve been experiencing numbness and weakness\nin my limbs for months. Could this be multiple\nsclerosis?\"\n• \"I’m 78 and have been told I have a floating her-\nnia after bowel cancer surgery. Can it be cured?\"\n2. Severe Negative Emotion Expressed Defini-\ntion: The patient explicitly states intense emotions\nsuch as fear, frustration, or anger regarding their\nhealth.\nExamples:\n• \"I feel depressed and anxious like never before. I\ncannot sleep at night.\"\n• \"I am scared and plan on taking my son to the\ndoctor. Should I be overly worried?\"\n• \"I’m terrified about my recent diagnosis of can-\ncer.\"\n3.\nUnderlying Negative Emotional State In-\nferred\nDefinition: The patient implies emotional\ndistress that isn’t explicitly stated but can be in-\nferred from their tone or descriptions, such as sub-\ntle signs of emotional worry, frustration, or distress\nabout delays or uncertainties. Focus on emotional\nworry, not the medical concern.\nExamples:\n• \"I am starting to get a little alarmed by this spot-\nting after ovulation. Is this cause for concern?\"\n(Worry inferred)\n• \"I have been trying to conceive, and the report\ndoes not look right to me. I just want to take a\nsecond opinion.\" (Anxiety inferred)\n• \"I need to be a bit more at ease after what I read\nabout diabetic enteropathy. I was a bit scared if\nit might be fatal.\" (Fear inferred)\n4. Concern Severity for Close Relations Defi-\nnition: The patient is asking on behalf of someone\nwith whom they share a close, protective relation-\nship, implying heightened emotional concern.\nExamples:\n• \"Hello, I am the mother of a five-year-old. He\nhas a small lump that hasn’t gone away. Should I\ntake him to a dermatologist?\"\n• \"My son recently started daycare and has gotten\nsick. His fever was 102.9. Should I take him to\nthe hospital?\"\nA.2\nInterpretations in General Health\nQueries\nA.2.1\nDefinition\nInterpretations refer to the communication of an\nunderstanding of the patient’s feelings (expressed\nor implied) and/or experiences (contextual factors)\ninferred from the patient’s query. It’s about recog-\nnizing and articulating what the patient is feeling\nand why, based on their situation, concerns, and\nhistory.\nA.2.2\nInterpretations Applicable\nInterpretations are necessary when the patient’s\nquery requires the doctor to communicate an un-\n12\n"}, {"page": 13, "text": "derstanding of the patient’s feelings (expressed or\nimplied) and/or experiences (contextual factors).\nThis involves acknowledging emotions, underly-\ning concerns, or contextual elements that influence\nthe patient’s emotional state. Below are detailed\ncategories reflecting when interpretations are appli-\ncable:\n1. Expression of Feelings (Explicit or Implicit)\nDescription:\nThe patient expresses emotions directly or im-\nplies them through language or tone. This includes\nfeelings such as fear, anxiety, frustration, sadness,\nor hopelessness.\nExamples:\n• Explicit Expression:\n– \"I’m really scared about these chest pains.\"\n– \"I’m frustrated because my symptoms aren’t\nimproving.\"\n– \"I have been in severe pain. It hurts so bad\ngetting out of bed.\"\n• Implicit Expression:\n– \"I guess I have to accept this is how things will\nbe now.\"\n– \"Nothing seems to be helping.\"\n– \"I don’t know what to do anymore.\"\n2. Sharing Experiences or Contextual Factors\nAffecting Emotional State and Well-being\nDescription:\nThe patient shares personal experiences, contex-\ntual factors, or circumstances that influence their\nhealth and emotional state. These include social,\nenvironmental, or personal situations beyond medi-\ncal concerns that affect their emotional state.\nExamples:\n• \"With my father’s illness and financial stress, I’m\nfeeling overwhelmed.\"\n• \"I’ve been under a lot of pressure at work, and\nnow I’m having trouble sleeping.\"\n• \"Ever since the accident, I can’t stop thinking\nabout what happened.\"\n• \"I recently moved to a different state, haven’t\nfound a general practitioner, and haven’t paid my\nhigh deductible for the year.\"\n3.\nExpressions of Distressing Uncertainty\nAbout Health or Treatment\nDescription:\nUncertainties, confusion, or mistrust about their\nhealth status, treatment, or future are leading to\nemotional distress. This includes questions about\nprognosis, treatment effectiveness, or doubt about\npotential outcomes that indicate or imply underly-\ning emotional distress. The focus should not be\non uncertainty alone but specifically on uncertainty\nthat reflects or suggests emotional distress in the\npatient.\nExamples:\n• \"I’m not sure if this treatment is really working\nfor me.\"\n• \"Do you think I should get a second opinion?\"\n• \"Will chemo be fatal?\"\n• \"Should my wife also get examined?\"\n• \"Is this something that sounds like I should con-\nsider doing?\"\n• \"I am wondering if I should see a doctor.\"\n4. Symptoms Significantly Affecting Emo-\ntional Well-being or Daily Life\nDescription:\nThe patient describes symptoms that signifi-\ncantly impact their emotional well-being or daily\nfunctioning, and they express or imply emotional\ndistress because of these symptoms. The key is\nthe emotional impact of the symptoms, not just the\nsymptoms themselves.\nExamples:\n• \"My symptoms have been affecting my job for\nmonths.\"\n• \"I’m so tired all the time that I can’t take care of\nmy kids properly.\"\n• \"These migraines are making it impossible to\nenjoy my hobbies.\"\n• \"The pain is getting worse every day, and it’s\nreally wearing me down.\"\nA.2.3\nInterpretations Not Applicable\nInterpretations are not necessary when the patient’s\nquery does not require the doctor to communicate\nan understanding of the patient’s feelings or experi-\nences. This occurs when:\n• The query is straightforward, factual, or routine.\n• There are no expressed or implied feelings need-\ning acknowledgment.\n• There are no contextual factors (experiences) or\nunderlying uncertainty concerns leading to emo-\ntional distress that require understanding.\nBelow are detailed categories reflecting when\ninterpretations are not applicable:\n13\n"}, {"page": 14, "text": "1. Straightforward Medical Queries Lacking\nEmotion, Distressing Uncertainty, and Context\nDescription: The patient requests specific med-\nical information or explanations of medical con-\ncepts without expressing emotional distress, under-\nlying distressful uncertainty, or providing context\n(social, environmental, or personal situations) im-\nplying an emotional state. These queries are strictly\ninformational and lack emotional or experiential\nelements requiring interpretation.\nExamples:\n• \"What is the use of Tylenol?\"\n• \"Hello doctor, I would like to get an opinion re-\ngarding the attached chest radiograph. I wish\nto know if there are any abnormalities like scar-\nring.\"\n2.\nGeneral Health Management Requests\nWithout Emotion, Context, and Distressing Un-\ncertainty\nDescription: The patient seeks guidance on\nhealth management, follows up on prior advice,\nor requests basic guidance on minor health issues\nwithout expressing emotional distress, underlying\ndistressful uncertainty, or providing contextual fac-\ntors (social, environmental, or personal situations)\nthat imply an emotional state. Here the guidance is\non what the patient should do.\nExamples:\n• \"I’m managing diabetes with insulin. How often\nshould I check my blood sugar levels?\"\n• \"I have intermittent knee pain from working out.\nHow would I know if I tore cartilage?\"\n• \"I had an X-ray for a fracture; should it be\nstrapped or cast right away?\"\n3. Diagnosis Requests with Neutral Symptom\nDescriptions Lacking Distressing Uncertainty\nand Context\nDescription:\nThe patient describes symptoms neutrally with-\nout expressing emotional distress or underlying\ndistressful uncertainty. They provide necessary\ndetails without implying feelings or contextual fac-\ntors (social, environmental, or personal situations)\nthat need acknowledgment. These descriptions are\nstraightforward and lack emotional or experiential\ncontent requiring interpretation. Here the request\nis about asking what the doctor thinks the issue is.\nExamples:\n• \"I have swelling in my ankle after a long walk.\nShould I be concerned?\"\n• \"Hello doctor, I am suffering from pain in my\nmouth. It feels like sensitivity pain. I cannot say\nit is pain exactly; it is irritating a lot. No pain in\nteeth. It feels like itching in my gums (middle of\nthe teeth). Please tell me what I can do.\"\n4. Hypothetical Medical Queries With No\nEmotions, Context, and Distressing Uncertainty\nDescription:\nThe patient inquires about hypothetical situa-\ntions or general medical information without ex-\npressing or implying personal feelings or contex-\ntual factors (social, environmental, or personal situ-\nations) that need acknowledgment.\nThese queries are theoretical and lack emotional\nor experiential aspects requiring interpretation.\nExamples:\n• \"If someone has XYZ symptoms, what might be\nthe cause?\"\n• \"What would happen if a person skipped their\nmedication?\"\nB\nAnnotation Instructions for Human\nAnnotators\nAnnotators received an Excel workbook containing\nthe patient queries and a fixed header with the in-\nstructions shown in Figure 3. For each pat_query,\nthey assigned Emotional Reactions and Interpreta-\ntions labels (Applicable / Not Applicable) and\nselected the justifying sub-category, as defined in\nAppendix A. The header also links to a Google Doc,\nreproduced verbatim in Appendix A, that provides\nthe full framework details for reference during an-\nnotation.\nFor accessibility and clarity, we restate the in-\nstructions here in text form.\nB.1\nInstructions Given to Annotators\nInstructions:\n1. Read the Document: Access and thoroughly\nreview the following document contaning the\nFramework Details: defined in Appendix A.\nFocus on understanding the details outlined\nbelow.\n2. Understand Emotional Reactions:\n• Emotional Reactions Definition: Learn\nwhat emotional reactions are and their role\nin doctor-patient communication.\n• Understand when emotional reactions are\napplicable or not applicable by reviewing:\n14\n"}, {"page": 15, "text": "Sub-definitions, Subcategories and Exam-\nples that illustrate their use in the relevant\nscenarios.\n3. Classify Emotional Reactions: For each pa-\ntient query, follow these steps:\n• Determine Emotional Reactions Appli-\ncability or Not Applicability:\nDecide\nwhether emotional reactions are applicable\nor not applicable in response to the patient\nquery.\n• Select a Subcategory:\n– If applicable, choose the subcategory that\nbest explains why emotional reactions are\nneeded in response to the patient query.\n– If not applicable, select the subcategory\nthat justifies why emotional reactions are\nnot necessary in response to the patient\nquery.\n4. Understand Interpretations:\n• Interpretations Definition: Learn what in-\nterpretations are and their role in doctor-\npatient communication.\n• Understand when interpretations are appli-\ncable or not applicable by reviewing: Sub-\ndefinitions, Subcategories and Examples\nthat illustrate their use in the relevant sce-\nnarios.\n5. Classify Interpretations: For each patient\nquery, follow these steps:\n• Determine Interpretations Applicability\nor Not Applicability: Decide whether inter-\npretations are applicable or not applicable\nin response to the patient query.\n• Select a Subcategory:\n– If applicable, choose the subcategory\nthat best explains why interpretations are\nneeded in response to the patient query.\n– If not applicable, select the subcategory\nthat justifies why interpretations are not\nnecessary in response to the patient query.\nB.2\nAdditional Verbal Clarifications\nDuring training sessions, annotators received the\nfollowing clarifications:\n• If they could not understand whether the symp-\ntoms or medical issue is severe, they were al-\nlowed to briefly search online (e.g., Google) to\ncheck whether the condition is typically serious.\n• If they were unsure whether the query was emo-\ntionally significant for the patient, they were en-\ncouraged to go with what they felt and believe\ntheir own judgment.\n• For each dimension (EA and IA), annotators were\ninstructed to:\n1. Read the patient query.\n2. While annotating a dimension, first read\nthe Applicable definition. If they believe\nit fits, go through the Applicable subcate-\ngories one by one and tag at least the one\nthey think fits best.\n3. If the Applicable definition does not feel\nlike it fits, they should still briefly review\nthe Applicable subcategories to verify this.\n4. Then move to the Not Applicable definition\nand repeat the same process with the Not\nApplicable subcategories.\nB.3\nBoundary Cases: Subjectivity and Lack\nof Medical Expertise\nEmpathy applicability judgments are inherently\nsubjective, and some patient queries lie at the\nboundary between emotional and informational in-\ntent. Such disagreements often reflect legitimate\ninterpretive variability rather than simple annota-\ntion error. These challenges are explored in detail\nin Section 5.2 (Systematic Challenges in Opera-\ntionalizing Anticipatory Empathy); here, we briefly\nrevisit representative cases to make these boundary\nconditions more transparent.\nTo\nmake\nthese\ncases\ntranspar-\nent,\nwe\nprovide\na\nsupplementary\nfile\n(misalignment_analysis.csv)\nlisting\nde-\ntailed patient queries, mis-aligned labels, and\nqualitative interpretations. The queries highlighted\nas exhibiting Divergent Interpretation correspond\nto Reasonable Disagreement around both symptom\nseverity and whether emotional content is present.\nC\nIllustrative Scenarios for EAF\nOperationalization\nSee Table 4 for illustrative scenarios demonstrating\nthe operationalization of the EAF.\nD\nAppendix: Human-GPT Agreement\nAnalysis\nTable 5 presents pairwise agreement between GPT\nand each human annotator. “Agreed” and “Dis-\nagreed” columns denote the number of queries\n15\n"}, {"page": 16, "text": "Figure 3: Screenshot of the annotation spreadsheet provided to annotators. The header shows the instructions and\nlinks to the framework document.\nwhere both annotators assigned the same or dif-\nferent labels of Applicable or Not Applicable, re-\nspectively.\nE\nModel Architecture Details\nEach empathy dimension—Emotional Reactions\n(EA) and Interpretations (IA)—is modeled inde-\npendently. We fine-tune a pretrained RoBERTa-\nbased model (Liu et al., 2019) separately for each\ndimension, while maintaining the same overall ar-\nchitecture. “Independently” means each classifier\nlearns to predict the applicability of one dimension\nwithout sharing parameters or optimization across\ntasks. For fine-tuning, we incorporate an attention\nmechanism based on a feed-forward network. The\nmodel architecture is illustrated in Figure 4.\nThe model follows an attention-based pooling\napproach built on top of a pretrained RoBERTa en-\ncoder. The encoder converts patient queries into\ncontextualized token embeddings, capturing the\nmeaning of each word based on its surrounding con-\ntext. When a sentence is processed by RoBERTa, it\ngenerates a hidden representation for each token, re-\nflecting its contextual meaning. Unlike traditional\nmethods that rely solely on the [CLS] token or an\naverage of all embeddings, this model applies a\nlearned attention mechanism to identify the most\nrelevant tokens for classification.\nSpecifically, the model uses a feed-forward neu-\nral network to compute attention scores for each\ntoken. A linear transformation first maps each to-\nken embedding to a scalar score, which then passes\nthrough a Tanh activation to constrain values be-\ntween [−1, 1] and avoid extremes. Since not all to-\nkens contribute equally to classification, the model\nconverts these raw scores into attention weights\nusing a softmax function across the sequence. This\nnormalization ensures that important words receive\nhigher weights, while less relevant words are as-\nsigned lower importance.\nAfter computing attention weights, the model\nperforms a weighted sum of token embeddings.\nTokens with higher attention scores contribute\nmore significantly to the final pooled representa-\ntion, highlighting the most relevant parts of the\nquery. This pooled vector is then passed through\na classification-linear layer, which outputs logits\nrepresenting the likelihood of belonging to either\nthe \"Not Applicable\" or \"Applicable\" class. Dur-\ning training, the model optimizes both the attention\nmechanism and the classification layer via cross-\nentropy loss, thereby improving accuracy in empa-\nthy classification.\nTraining separate models for EA and IA avoids\ncrosstalk between tasks.\nEach classifier learns\n16\n"}, {"page": 17, "text": "Empathy Dimen-\nsion\nScenario Type\nScenario\nApplicability\nExplanation\nEmotional Reaction Explicit Need\n\"Hello doctor, I am having con-\nstant eye floaters, low back and\nhip pain, and also my rib cage\nhurts. I feel depressed and anx-\nious like never before. I cannot\nsleep at night. An MRI of my\nbrain shows a tiny flare, but radi-\nologists say it’s nothing to worry\nabout. What should I do?\"\nApplicable\nThe patient explicitly expresses intense\nnegative emotions—feeling depressed\nand anxious—and states an inability to\nsleep. An emotional reaction from the\ndoctor is necessary to provide support\nand reassurance.\nEmotional Reaction Implicit Need\n\"Hello doctor, my son has been\nexperiencing frequent headaches\nover the past week.\nWe’ve\ntried over-the-counter medica-\ntions, but there’s no improvement.\nWhat should we do?\"\nApplicable\nEmotional reactions are applicable here\nbecause, as Richert et al. (Richert et al.,\n2018) find, parents of children with\nhealth (drug) issues often experience\nsignificant distress and negative mental\nhealth effects. The mother may be ex-\nperiencing worry and anxiety about her\nchild’s well-being, even if she doesn’t\nexplicitly express it.\nEmotional Reaction Not Needed\n\"Hello doctor, I was suffering\nfrom an infection in my tonsil for\nthe past four days. I went to an\nENT specialist who prescribed\nantibiotics. Now my tonsil pain\nhas subsided, but I still feel some-\nthing stuck on the left side of my\nthroat where the pain was. I have\nno problem swallowing. Kindly\nadvise me on what to do next.\"\nNot Applicable\nThe patient provides a neutral descrip-\ntion of symptoms without expressing\nemotional concern or distress. The pri-\nmary need is factual medical advice. An\nemotional reaction from the doctor is\nnot necessary in this case.\nInterpretation\nExplicit Need\n\"Hello doctor, I am feeling ex-\ntremely anxious about my upcom-\ning surgery. I can’t stop worry-\ning about the possible complica-\ntions.\"\nApplicable\nThe patient explicitly expresses feelings\nof anxiety and worry. The doctor should\ncommunicate an understanding of these\nfeelings, acknowledging the patient’s\nemotional state and providing appropri-\nate support.\nInterpretation\nImplicit Need\n\"Hello doctor, I’ve been taking\nthe medication as prescribed, but\nI’m not seeing any improvement.\nIs there something I’m doing\nwrong?\"\nApplicable\nThe patient implies feelings of frustra-\ntion and possibly self-blame. The doc-\ntor should interpret and acknowledge\nthese underlying feelings, demonstrat-\ning understanding and support.\nInterpretation\nNot Needed\n\"I was playing with my sister s\nboyfriends brother and I swung\nto hit him like I said we were play-\ning around and I my wrist hit his\nelbow really hard when it hap-\npened my hand got really numb\nand my vein was hurting really\nbad and it s 6 hours later and my\nvein still hurts what should I do\"\nNot Applicable\nThe query is a straightforward request\nfor diagnosis with neutral symptom de-\nscriptions. It does not express emotions\nor distressing contextual factors that re-\nquire acknowledgment. The doctor’s\nresponse should focus solely on provid-\ning a factual diagnosis.\nTable 4: Empathy Dimensions, Scenarios, Applicability, and Explanations\ndimension-specific patterns from the data, resulting\nin a simple and modular approach that enables fo-\ncused analysis of empathy applicability in patient\nqueries.\nF\nPrompt Design for LLM Annotations\nFor more detail on the prompt design used for LLM\n(GPT-4o, o1) based annotations, we provide here\nthe exact prompts used in our experiments.\nWe used two styles of prompts:\n• With-framework (contrastive) prompts: the\nLLM received the full Empathy Applicability\nFramework for a given dimension (Emotional\nReactions or Interpretations), including both Ap-\nplicable and Not Applicable subcategories with\nexamples for each. This creates a contrastive in-\ncontext signal: the model must decide between\nmultiple subcategories across both classes.\n• Without-framework prompts: the LLM only\nreceived a short task definition (dimension defini-\n17\n"}, {"page": 18, "text": "Table 5: Cohen’s κ agreement scores and confusion matrix counts between GPT-4o and each human annotator for\nEmotional Reactions (EA) and Interpretations (IA)\nAnnotator 1 Annotator 2 Kappa EA Kappa IA Agreed EA Disagreed EA Agreed IA Disagreed IA\nHA2\nGPT\n0.4402\n0.5306\n917\n379\n988\n308\nHA1\nGPT\n0.4096\n0.3612\n940\n356\n890\n406\nFigure 4: Empathy Dimension Applicability Model Architecture\ntion + binary Applicability decision), without any\nsubcategories or examples. This approximates a\ngeneric zero-shot setup without our framework.\nF.1\nEmotional Reactions: With-Framework\nContrastive Prompt\nThe core schema passed to the LLM for Emotional\nReactions with the full framework was:\nANNOTATION_SCHEMA = {\n\"instruction\": \"Annotate emotional reactions in general\nhealth queries based on the following schema. For each\nquery, return the matching subcategories. Think\nlogically and ensure to revisit your annotation for\neach query\",\n\"definitions\": {\n\"Emotional Reactions\": {\n\"description\": \"Expressions of warmth, compassion,\nconcern, or similar feelings conveyed by a\ndoctor in response to a patient’s query.\",\n\"categories\": [\n{\n\"name\": \"Purely Factual Medical Queries\",\n\"description\": \"The patient requests specific\nmedical information, including\nexplanations of medical concepts,\nwithout emotional distress or\nunderlying distressing uncertainty.\",\n\"examples\": [\"What is the use of Tylenol?\", \"\nIs it possible to outgrow a seafood\nallergy?\"],\n\"class\" : \"Emotional Reactions Not Applicable\n\"\n},\n{\n\"name\": \"General Health Management Without\nEmotional Involvement\",\n\"description\": \"The patient seeks guidance on\nhealth management, follows up on prior\nadvice, or requests basic guidance on\nminor health issues, without expressing\nemotional distress or underlying\ndistressing uncertainty. Here the\nguidance is on what the patient should\ndo.\",\n\"examples\": [\"I’m managing diabetes with\ninsulin. How often should I check my\nblood sugar levels?\", \"I have swelling\nin my ankle after a long walk. Should I\nbe concerned?\"],\n\"class\" : \"Emotional Reactions Not Applicable\n\"\n},\n{\n\"name\": \"Diagnosis Requests with Neutral\nSymptom Descriptions\",\n\"description\": \"The patient describes\nsymptoms neutrally without expressing\nemotional distress or underlying\ndistressing uncertainty. Here the\nrequest is about asking what the doctor\nthinks the issue is.\",\n\"examples\": [\"I have intermittent knee pain\nfrom working out. How would I know if I\ntore cartilage?\", \"Hello. I am having\npain in my jaw area, immediately in\nfront of my left ear.\"],\n\"class\" : \"Emotional Reactions Not Applicable\n\"\n},\n{\n\"name\": \"Hypothetical Medical Queries Without\nEmotional Concern\",\n\"description\": \"The patient inquires about\nhypothetical situations without\nemotional involvement.\",\n\"examples\": [\"If someone has XYZ symptoms,\nwhat might be the cause?\", \"What would\nhappen if a person skipped their\nmedication?\"],\n\"class\" : \"Emotional Reactions Not Applicable\n\"\n},\n{\n\"name\": \"Seriousness of Symptoms\",\n\"description\": \"The patient describes\nsymptoms that suggest a life-\nthreatening or chronic health condition\nsignificantly impacting long-term\nhealth or quality of life. This\nincludes diseases like cancer, heart\ndisease, mental health issues, or\nchronic conditions leading to\ndisability.\",\n\"examples\": [\"My father has been having\nsevere chest pains and shortness of\nbreath. Could it be a heart attack?\", \"\nI’ve been experiencing numbness and\nweakness in my limbs for months.\"],\n\"class\" : \"Emotional Reactions Applicable\"\n},\n18\n"}, {"page": 19, "text": "{\n\"name\": \"Severe Negative Emotion Expressed\",\n\"description\": \"The patient explicitly states\nintense emotions such as fear,\nfrustration, or anger regarding their\nhealth.\",\n\"examples\": [\"I feel depressed and anxious\nlike never before. I cannot sleep at\nnight.\", \"I’m terrified about my recent\ndiagnosis of cancer.\"],\n\"class\" : \"Emotional Reactions Applicable\"\n},\n{\n\"name\": \"Underlying Negative Emotional State\nInferred\",\n\"description\": \"The patient implies emotional\ndistress that isn’t explicitly stated\nbut can be inferred from their tone or\ndescriptions, such as subtle signs of\nemotional worry, frustration, or\ndistress about delays or underlying\ndistressing uncertainties. Focus on\nemotional worry, not the medical\nconcern.\",\n\"examples\": [\"I am starting to get a little\nalarmed by this spotting after\novulation. Is this cause for concern?\",\n\"I need to be a bit more at ease after\nwhat I read about diabetic enteropathy\n.\"],\n\"class\" : \"Emotional Reactions Applicable\"\n},\n{\n\"name\": \"Concern Severity for Close Relations\n\",\n\"description\": \"The patient is asking on\nbehalf of someone with whom they share\na close, protective relationship,\nimplying heightened emotional concern.\",\n\"examples\": [\"Hello, I am the mother of a\nfive-year-old. He has a small lump that\nhasn’t gone away.\", \"My son recently\nstarted daycare and has gotten sick.\nHis fever was 102.9. Should I take him\nto the hospital?\"],\n\"class\" : \"Emotional Reactions Applicable\"\n}\n]\n}\n},\n\"output_format\": \"json\",\n\"example_query\": {\n\"query\": \"I’m scared and plan on taking my son to the\ndoctor. Should I be overly worried?\",\n\"annotations\": [\n{\n\"subcategories\": [\n{\"name\": \"Severe Negative Emotion Expressed\"},\n{\"name\": \"Concern Severity for Close Relations\"}\n],\n\"class\": \"Emotional Reactions Applicable\",\n\"reason\": \"The patient explicitly expresses fear\nregarding their son’s health and shows\nheightened emotional concern for a close\nrelation.\"\n}\n]\n}\n}\nF.2\nInterpretations: With-Framework\nContrastive Prompt\nThe corresponding schema for Interpretations (IA)\nwith the full framework was:\nANNOTATION_SCHEMA = {\n\"instruction\": \"Annotate interpretations in general health\nqueries based on the following schema. For each query,\nreturn the matching subcategories. Think logically\nand ensure to revisit your annotation for each query\",\n\"definitions\": {\n\"Interpretations\": {\n\"description\": \"Interpretations refer to the\ncommunication of an understanding of the\npatient’s feelings (explicit or implied) and/or\nexperiences (contextual factors) inferred\nfrom their query. It’s about recognizing and\narticulating what the patient is feeling and\nwhy, based on their situation, concerns, and\nhistory.\",\n\"categories\": [\n{\n\"name\": \"Expression of Feelings (Explicit or\nImplicit)\",\n\"description\": \"The patient expresses\nemotions directly or implies them\nthrough language or tone. This includes\nfeelings such as fear, anxiety,\nfrustration, sadness, or hopelessness.\",\n\"examples\": [\n\"I’m really scared about these chest\npains.\",\n\"I’m frustrated because my symptoms aren’\nt improving.\",\n\"I guess I have to accept this is how\nthings will be now.\",\n\"Nothing seems to be helping.\",\n\"I don’t know what to do anymore.\"\n],\n\"class\": \"Interpretations Applicable\"\n},\n{\n\"name\": \"Sharing of Experiences or Contextual\nFactors Affecting Emotional State and\nWell being\",\n\"description\": \"The patient shares personal\nexperiences, contextual factors, or\ncircumstances that influence their\nhealth and emotional state. These\ninclude social, environmental, or\npersonal situations, beyond medical\nconcerns, that affect their emotional\nstate.\",\n\"examples\": [\n\"With my father’s illness and financial\nstress, I’m feeling overwhelmed.\",\n\"I’ve been under a lot of pressure at\nwork, and now I’m having trouble\nsleeping.\",\n\"Ever since the accident, I can’t stop\nthinking about what happened.\",\n\"I recently moved to a different state,\nhaven’t found a general practitioner\n, and haven’t paid my high\ndeductible for the year.\"\n],\n\"class\": \"Interpretations Applicable\"\n},\n{\n\"name\": \"Expressions of Distressing\nUncertainty About Health or Treatment\",\n\"description\": \"Uncertainties, confusion, or\nmistrust expressed by a patient about\ntheir health status, treatment, or\nfuture that lead to significant\nemotional distress. This includes\nstatements involving questions about\nprognosis, treatment effectiveness, or\ndoubt about potential outcomes,\nspecifically when accompanied by\nexplicit or implicit signs of emotional\ndistress.\",\n\"examples\": [\n\"I’m not sure if this treatment is really\nworking for me, and it’s making me\nanxious.\",\n\"Is this something that sounds like I\nshould consider doing? I’m so\nconfused about what’s right.\",\n\"I feel lost. Should my wife also get\nexamined?\",\n\"Do you think there’s any hope for me\nafter trying this?\"\n],\n\"class\": \"Interpretations Applicable\"\n},\n{\n19\n"}, {"page": 20, "text": "\"name\": \"Symptoms Significantly Affecting\nEmotional Well-being or Daily Life\",\n\"description\": \"The patient describes\nsymptoms that significantly impact\ntheir emotional well-being or daily\nfunctioning, and they express or imply\nemotional distress because of these\nsymptoms. The key is the emotional\nimpact of the symptoms, not just the\nsymptoms themselves.\",\n\"examples\": [\n\"These migraines are making it impossible\nto enjoy my hobbies.\",\n\"I’m so tired all the time that I can’t\ntake care of my kids properly.\",\n\"My symptoms have been affecting my job\nfor months.\",\n\"The pain is getting worse every day, and\nit’s really wearing me down.\"\n],\n\"class\": \"Interpretations Applicable\"\n},\n{\n\"name\": \"Straightforward Medical Queries\nLacking Emotion, Distressing\nUncertainty, and Context\",\n\"description\": \"The patient requests specific\nmedical information or explanations of\nmedical concepts without expressing\nemotional distress, underlying\ndistressful uncertainty or providing\ncontext (social, environmental, or\npersonal situations) implying an\nemotional state. These queries are\nstrictly informational and lack\nemotional or experiential elements\nrequiring interpretation.\",\n\"examples\": [\n\"What is the use of Tylenol?\",\n\"Hello doctor, I would like to get an\nopinion regarding the attached chest\nradiograph. I wish to know if there\nare any abnormalities like scarring\n.\"\n],\n\"class\": \"Interpretations Not Applicable\"\n},\n{\n\"name\": \"General health management requests\nWithout Emotion, Context, and\nDistressing Uncertainty\",\n\"description\": \"The patient seeks guidance on\nhealth management, follows up on prior\nadvice, or requests basic guidance on\nminor health issues without expressing\nemotional distress, underlying\ndistressful uncertainty, or providing\ncontextual factors that imply an\nemotional state. Additionally, they may\ninclude personal medical context, such\nas test results, medications taken,\nand previous medical consultations.\nHere the guidance is on what the\npatient should do.\",\n\"examples\": [\n\"I have intermittent knee pain from\nworking out. How would I know if I\ntore cartilage?\",\n\"I had an X-ray for a fracture; should it\nbe strapped or cast right away?\",\n\"Hi, my husband is 39, and his SGPT and\nSGOT levels in a recent test were\n101 and 98 respectively. His\ntriglycerides are 280, which is high\n. His height is 168 cm and weight is\n79 kg. What does a rise in these\nvalues indicate? What precautions\nshould he take?\"\n],\n\"class\": \"Interpretations Not Applicable\"\n},\n{\n\"name\": \"Diagnosis Requests with Neutral\nSymptom Descriptions Lacking\nDistressing Uncertainty and Context\",\n\"description\": \"The patient describes\nsymptoms neutrally without expressing\nemotional distress or underlying\ndistressful uncertainty. They provide\nnecessary details without implying\nfeelings or contextual factors. These\ndescriptions are straightforward and\nlack emotional or experiential content\nrequiring interpretation. Here the\nrequest is about asking what the doctor\nthinks the issue is.\",\n\"examples\": [\n\"I have swelling in my ankle after a long\nwalk. Should I be concerned?\",\n\"Hello doctor, I am suffering from pain\nin my mouth. It feels like\nsensitivity pain. I cannot say it is\npain exactly; it is irritating a\nlot. No pain in teeth. It feels like\nitching in my gums (middle of the\nteeth). Please tell me what I can do\n.\"\n],\n\"class\": \"Interpretations Not Applicable\"\n},\n{\n\"name\": \"Hypothetical Medical Queries with no\nEmotions, Context, and Distressing\nUncertainty\",\n\"description\": \"The patient inquires about\nhypothetical situations or general\nmedical information without expressing\nor implying personal feelings or\ncontextual factors that need\nacknowledgment. These queries are\ntheoretical and lack emotional or\nexperiential aspects requiring\ninterpretation.\",\n\"examples\": [\n\"If someone has XYZ symptoms, what might\nbe the cause?\",\n\"What would happen if a person skipped\ntheir medication?\"\n],\n\"class\": \"Interpretations Not Applicable\"\n}\n]\n}\n},\n\"output_format\": \"json\",\n\"example_query\": {\n\"query\": \"I’m not sure if this treatment is really\nworking for me.\",\n\"annotations\": [\n{\n\"subcategories\": [\n{\n\"name\": \"Expressions of Distressing\nUncertainty About Health or\nTreatment\"\n}\n],\n\"class\": \"Interpretations Applicable\",\n\"reason\": \"The patient explicitly expresses doubt\nabout the effectiveness of the treatment,\nwhich requires interpretation.\"\n}\n]\n}\n}\nF.3\nPrompts Without the Framework\n(Definition-Only)\nFor the without-framework condition, the LLM re-\nceived only a short task description and the names\nof the Applicability labels. No subcategories or\nexamples were provided.\nF.3.1\nEmotional Reactions (without\nframework).\nANNOTATION_SCHEMA = {\n20\n"}, {"page": 21, "text": "\"instruction\": \"Read the patient query and decide whether\nemotional reactions are necessary in the response. \"\n\"Emotional reactions refer to the expressions\nof warmth, compassion, concern, or\nsimilar feelings \"\n\"conveyed by a doctor in response to a patient\n’s query. \"\n\"If emotional reactions are necessary, mark it\nas ’Emotional Reactions Applicable’. \"\n\"If not, mark it as ’Emotional Reactions Not\nApplicable’. Think carefully and be\nconsistent.\",\n\"output_format\": \"json\"\n}\nF.3.2\nInterpretations (without framework).\nANNOTATION_SCHEMA = {\n\"instruction\": \"Read the patient query and decide whether\ninterpretations are necessary in the response. \"\n\"Interpretations refer to the communication of\nan understanding of the patient’s\nfeelings \"\n\"(explicit or implied) and/or experiences (\ncontextual factors) inferred from their\nquery. \"\n\"If interpretations are necessary, mark it as\n’Interpretations Applicable’. \"\n\"If not, mark it as ’Interpretations Not\nApplicable’. Think carefully and be\nconsistent.\",\n\"output_format\": \"json\"\n}\nTogether, these listings document the exact\nprompts used in both the with-framework (con-\ntrastive) and without-framework settings for LLM\nannotations.\nG\nDataset Analyses\nTo characterize the released benchmark beyond\nagreement and modeling results, Figure 5 summa-\nrizes label base rates (H1/H2/GPT), EA–IA co-\napplicability patterns, and the heavy-tailed distri-\nbution of query lengths, providing a high-level\nview of dataset variability. In the following sec-\ntions, we present additional analyses of annotation\nconsistency, framework coherence, and stability\nacross the labeling process. Specifically, we ana-\nlyze: (i) subcategory usage and co-occurrence pat-\nterns across human annotators and GPT rationales;\n(ii) length effects on applicability with confidence\nintervals; (iii) run-order drift checks to assess sta-\nbility over the labeling sequence; and (iv) match\nvs. miss portions of subcategory rationales when\nhumans and GPT agree on the overall applicability\nlabel, to assess alignment in rationales.\nG.1\nSubcategory Prevalence and EA×IA\nCo-occurrence (Humans vs. GPT)\nTable 6 and Table 7 report the prevalence of sub-\ncategories along with the Applicable or Not Ap-\nplicable classification of each subcategory under\nthe EAF. This makes it possible to assess the in-\ntuitive coherence of the framework: subcategories\nthat encode similar affective cues or uncertainty\n(Applicable) should align with each other in all di-\nmensions, while informational or routine requests\n(Not Applicable) should be clustered separately.\nBecause each query receives exactly one subcat-\negory from each human annotator, human counts\nsum to N = 1296 per dimension. In contrast, GPT\nmay assign multiple subcategories per query; thus\nGPT totals exceed N (EA: 2100; IA: 2572), corre-\nsponding to an average of 1.62 EA subcategories\nand 1.98 IA subcategories per query.\nThe prevalence distributions show both stable\nstructure and meaningful variability in how cues\nare operationalized. For EA, H1 frequently uses\nthe Not Applicable subcategory Factual Medical\nQuery (31.8%), while H2 rarely uses it (2.9%) and\ninstead assigns both Applicable and Not Applicable\ncategories such as Concern for Relations (20.6%)\nand General Health Management (19.4%). A sim-\nilar pattern appears in IA: H1 frequently assigns\nthe Not Applicable subcategory Straightforward\nMedical Query (33.3%), whereas H2 more often\nassigns the Applicable subcategory Distressing Un-\ncertainty (40.8%). GPT assigns Applicable EA\ncategories at higher rates, especially Inferred Nega-\ntive Emotion (59.4%) and Symptom Seriousness\n(27.5%), reflecting broader recall for empathic-\nneed cues.\nAcross dimensions, GPT again as-\nsigns multiple IA cues per query and frequently\nmarks Applicable interpretive cues (Distressing\nUncertainty, Feelings Expression, Impact on Daily\nLife, Sharing Affecting Context), consistent with its\nbroader recall of applicability signals.\nFigure 6 further supports the framework’s intu-\nitive coherence when interpreted through the Appli-\ncable vs. Not Applicable split of subcategories (Ta-\nbles 6–7). Not Applicable request-types tend to pair\nwith Not Applicable interpretations (Not×Not),\nwhile affective/uncertainty cues (Applicable) more\noften co-occur with Applicable interpretive signals\n(App×App). Concretely, in the human heatmaps,\nEA Factual Medical Query aligns strongly with IA\nStraightforward Medical Query (H1: 30%), and\nNeutral Diagnosis Request co-occurs with its IA\ncounterpart (H1: 19%; H2: 9%), both canonical\nNot×Not pairings. In contrast, Applicable EA cues\nalign with Applicable IA cues: Severe Negative\nEmotion co-occurs with Feelings Expression (H1:\n15%), and Concern for Relations frequently pairs\nwith Distressing Uncertainty (H2: 16%), reflecting\nclinically intuitive links between expressed (or in-\nferred) affect and corresponding interpretive needs.\n21\n"}, {"page": 22, "text": "Figure 5: Dataset overview panel: base rates, EA–IA coupling, and query length distributions. Panels (a–b)\nreport the binary label base rates for Emotional Reactions (EA) and Interpretations (IA) from Human Annotator\n1 (HA1), Human Annotator 2 (HA2), and GPT. Bars are shown as stacked proportions of Applicable vs. Not\nApplicable. Panels (c–d) show EA×IA co-occurrence as 2×2 heatmaps for (c) Human consensus (only items where\nHA1 and HA2 agree on both EA and IA) and (d) Majority consensus (majority vote over HA1, HA2, and GPT), with\neach cell annotated by the percentage of items in that consensus subset; Panels (e–f) summarize query length: (e)\ntoken-count histogram (simple word tokenization) and (f) character-count histogram; vertical reference lines mark\nthe mean (solid red) and median (dashed black). Together, the figure summarizes label prevalence, the empirical\ncoupling between EA and IA decisions, and the distribution of textual input lengths in patient queries.\nGPT exhibits broader App×App cross-pairings,\nmost prominently EA Inferred Negative Emotion\nco-occurring with multiple Applicable IA cues in-\ncluding Distressing Uncertainty (38%), Feelings\nExpression (27%), Impact on Daily Life (24%),\nand Sharing Affecting Context (20%), which is ex-\npected given GPT’s multi-label rationale annota-\ntions. Overall, these structured pairings indicate\nthat cross-dimensional co-occurrence is not arbi-\ntrary: it aligns with the EAF’s applicability seman-\ntics while also highlighting how multi-cue ratio-\nnales (GPT) differ from single-label human assign-\nments.\nAdditionally,\nthese\nprevalence\nand\nco-\noccurrence results provide actionable signals\nfor refining the EAF and improving annotation\npractice. First, the strong Not×Not and App×App\nstructure suggests the framework’s applicability\nsplit is broadly coherent, but the sharp anno-\ntator skews in how “routine/informational” vs.\n“affective/uncertainty” cues are operationalized\n(e.g., heavier use of Factual Medical Query and\nStraightforward Medical Query versus greater\nuse of Concern for Relations and Distressing\nUncertainty) highlight subcategories that may\nbe very broad or boundary-sensitive.\nThese\nare prime candidates for guideline refinement\n(clearer decision rules, additional contrastive\nexamples, or merging/splitting categories), while\nconsistently rare categories can be reconsidered\nfor consolidation if they contribute limited\ndiscriminative value. Second, the co-occurrence\nmatrices can be leveraged to diagnose systematic\nannotation patterns and potential drift:\nstable,\nclinically intuitive pairings (e.g., Severe Negative\nEmotion\nwith\nFeelings\nExpression)\nindicate\nconsistent interpretation,\nwhereas unexpected\nor diffuse pairings can reveal where annotators\nmay be diverging due to inconsistent application\nrather than subjectivity, and targeted review\ncould mitigate those differences. In this sense,\nco-occurrence structure is not only a validation\nof the EAF semantics, but also a practical tool\nfor targeted adjudication, annotator training, and\nfuture modeling choices.\nG.2\nLength Effects on Applicability (with\nConfidence Intervals)\nTo assess whether query length is associated with\napplicability judgments, we stratify items into\ntoken-length deciles and plot EA/IA applicability\nrates for each label source (HA1, HA2, GPT, Hu-\n22\n"}, {"page": 23, "text": "EA subcategory\nShortname\nClass\nHA1\nHA2\nGPT\nUnderlying\nNegative\nEmotional\nState Inferred\ninferred\nnegative\nemotion\nApp\n121 (9.3)\n197 (15.2)\n770 (59.4)\nConcern Severity For Close Rela-\ntions\nconcern\nfor\nrela-\ntions\nApp\n246 (19.0)\n267 (20.6)\n277 (21.4)\nSevere Negative Emotion Expressed\nsevere\nnegative\nemotion\nApp\n209 (16.1)\n132 (10.2)\n177 (13.7)\nSeriousness Of Symptoms\nsymptom\nserious-\nness\nApp\n9 (0.7)\n150 (11.6)\n357 (27.5)\nDiagnosis Requests With Neutral\nSymptom Descriptions\nneutral diagnosis re-\nquest\nNot\n262 (20.2)\n165 (12.7)\n226 (17.4)\nPurely Factual Medical Queries\nfactual\nmedical\nquery\nNot\n412 (31.8)\n38 (2.9)\n108 (8.3)\nGeneral Health Management With-\nout Emotional Involvement\ngeneral health man-\nagement\nNot\n37 (2.9)\n252 (19.4)\n156 (12.0)\nHypothetical Medical Queries With-\nout Emotional Concern\nhypothetical medi-\ncal query\nNot\n0 (0.0)\n95 (7.3)\n29 (2.2)\nTable 6: EA subcategory prevalence with applicability class (App vs. Not). Values are count (percent of N = 1296).\nHumans assign one subcategory per query (counts sum to N). GPT may assign multiple subcategories per query;\nthus GPT counts can exceed N.\n(a) HA1: EA×IA co-occurrence.\n(b) HA2: EA×IA co-occurrence.\n(c) GPT: EA×IA co-occurrence (multi-\nlabel IA/EA allowed).\nFigure 6: EA×IA subcategory co-occurrence (percent of queries, N = 1296). Humans provide a single EA and\nIA subcategory per query, yielding sharper pairings; GPT may assign multiple subcategories per query, producing\nbroader co-occurrence patterns.\nmanCons, MajorityCons). The x-axis reports the\nmean token length per decile (µ), and we anno-\ntate the number of items per decile (n). Error bars\ndenote confidence intervals for each applicability\nestimate.\nAcross both dimensions, applicability increases\nwith length: shorter queries (D1–D2) receive sub-\nstantially lower applicability rates, while longer\nqueries (D9–D10) show consistently higher appli-\ncability across all sources. Although absolute rates\ndiffer by source (e.g., GPT tends to assign applica-\nbility more frequently than humans), the upward\ntrend is shared, suggesting that length (and the addi-\ntional context typically present in longer queries) is\nsystematically associated with applicability rather\nthan reflecting annotator idiosyncrasies alone.\nG.3\nRun-order Drift Checks\nFinally, we evaluate whether labeling behavior\ndrifts over the annotation sequence (e.g., fatigue\nor order effects). We plot the rolling mean of pair-\nwise differences in applicability rates (window size\n= 32) across run index for each pair (H1–H2, H1–\nGPT, H2–GPT), and apply ADWIN change detec-\ntion (δ = 0.002) to flag potential change points.\nAcross N = 1296 items, ADWIN identifies only\na small number of localized change points in each\nrolling-difference stream. For EA, ADWIN flags\n3 points for H1–H2 (at indices 767, 1023, 1279),\n2 for H1–GPT (223, 1151), and 3 for H2–GPT\n(191, 479, 959). For IA, ADWIN flags 3 points for\nH1–H2 (415, 639, 1279), 3 for H1–GPT (191, 863,\n1151), and 4 for H2–GPT (223, 415, 607, 1087).\nBecause these indices correspond to positions in\n23\n"}, {"page": 24, "text": "IA subcategory\nShortname\nClass\nHA1\nHA2\nGPT\nExpressions Of Distressing Uncer-\ntainty About Health Or Treatment\ndistressing\nuncer-\ntainty\nApp\n222 (17.1)\n529 (40.8)\n556 (42.9)\nExpression Of Feelings (Explicit Or\nImplicit)\nfeelings expression\nApp\n274 (21.1)\n176 (13.6)\n423 (32.6)\nSymptoms\nSignificantly\nAffecting\nEmotional Well-Being Or Daily Life\nimpact on daily life\nApp\n44 (3.4)\n36 (2.8)\n382 (29.5)\nSharing Experiences Or Contextual\nFactors Affecting Emotional State And\nWell Being\nsharing\naffecting\ncontext\nApp\n15 (1.2)\n48 (3.7)\n308 (23.8)\nStraightforward Medical Queries Lack-\ning Emotion, Distressing Uncertainty,\nAnd Context\nstraightforward\nmedical query\nNot\n431 (33.3)\n41 (3.2)\n285 (22.0)\nDiagnosis Requests With Neutral\nSymptom Descriptions Lacking Dis-\ntressing Uncertainty And Context\nneutral diagnosis re-\nquest\nNot\n269 (20.8)\n118 (9.1)\n339 (26.2)\nGeneral Health Management Requests\nWithout Emotion, Context, And Dis-\ntressing Uncertainty\ngeneral health man-\nagement\nNot\n41 (3.2)\n264 (20.4)\n242 (18.7)\nHypothetical Medical Queries With\nNo Emotions, Context, And Distress-\ning Uncertainty\nhypothetical medi-\ncal query\nNot\n0 (0.0)\n84 (6.5)\n37 (2.9)\nTable 7: IA subcategory prevalence with applicability class (App vs. Not). Values are count (percent of N = 1296).\nHumans assign one subcategory per query (counts sum to N). GPT may assign multiple subcategories per query;\nthus GPT counts can exceed N.\nthe rolling stream, each detection reflects a local\nshift over the surrounding ∼32-item neighborhood\nrather than a single item. Overall, the sparse de-\ntections and the absence of sustained shifts in the\nrolling trajectories suggest broadly stable annota-\ntion behavior over the run, with only occasional\nlocal fluctuations in pairwise disagreement.\nG.4\nMatch vs. Miss by subcategory\nIn this section, we quantify subcategory-level ratio-\nnale overlap in cases where GPT agrees with the hu-\nman consensus label (Applicable / Not Applicable)\nfor the given dimension. For each query, each hu-\nman provides one best-fit subcategory; the human\nrationale set is therefore either a singleton (both\nhumans chose the same subcategory) or a size-two\nset (humans chose different subcategories). We\nthen compare this human set to GPT’s provided\nsubcategories as rationales for labeling the query\nas Applicable or not. For each human-selected sub-\ncategory occurrence, we count a Match if GPT\nincludes that same subcategory, and a Miss other-\nwise. Figure 9 reports, for each subcategory, the %\nMatch vs. % Miss, along with the total number of\nsuch occurrences (N).\nAcross both EA and IA, match rates are gener-\nally high, indicating that when GPT agrees with\nhumans on the binary applicability label, it often\nalso identifies the same underlying rationale cues.\nHowever, several subcategories exhibit higher miss\nrates, suggesting systematic differences in how\nGPT justifies an agreed-upon label.\nFor EA (Figure 9a), GPT shows near-complete\nalignment for Concern for Relations (N = 204)\nand Inferred Negative Emotion (N = 102), and\nvery strong alignment for Neutral Diagnosis Re-\nquest (N = 92) and Symptom Seriousness (N =\n70). In contrast, Hypothetical Medical Query ex-\nhibits the largest miss portion (N = 39), and both\nSevere Negative Emotion (N = 189) and Factual\nMedical Query (N = 147) show notable misses,\nindicating that GPT sometimes agrees on EA appli-\ncability while grounding its justification in different\ncues than the humans.\nFor IA (Figure 9b), alignment is strongest for\nDistressing Uncertainty (N = 262) and Feelings\nExpression (N = 233), and is also high for Neu-\ntral Diagnosis Request (N = 133). Impact on\nDaily Life shows a perfect match in this subset\n(N = 49), though this category is comparatively\nsmaller. The most challenging IA category is again\nHypothetical Medical Query (N = 58), which has\nthe largest miss fraction; Straightforward Medical\nQuery (N = 183) also shows a higher miss rate\nthan the more affective/uncertainty categories.\nTogether, these patterns indicate substantial ra-\n24\n"}, {"page": 25, "text": "tionale overlap when GPT agrees with the human\nconsensus applicability label, but alignment varies\nby subcategory. Misses occur not only for hypothet-\nical and some informational categories, but also for\ncertain affective categories (e.g., Severe Negative\nEmotion). This suggests that mismatches reflect\nless a single cue-type split and more the prevalence\nof multi-cue queries, where multiple plausible ratio-\nnales can support the same applicability judgment.\n25\n"}, {"page": 26, "text": "(a) EA: Applicability vs. token-length decile (µ tokens shown per decile; error bars are confidence intervals).\n(b) IA: Applicability vs. token-length decile (µ tokens shown per decile; error bars are confidence intervals).\nFigure 7: Applicability rates by query-length decile. Bars indicate the number of queries per decile (n). Error bars\ndenote confidence intervals.\n26\n"}, {"page": 27, "text": "(a) EA: rolling mean pairwise difference + ADWIN drift points (win=32, δ = 0.002).\n(b) IA: rolling mean pairwise difference + ADWIN drift points (win=32, δ = 0.002).\nFigure 8: Run-order drift diagnostics using rolling mean differences in applicability rates and ADWIN change\ndetection. Vertical dashed lines indicate detected change points.\nFigure 9: Rationale Match vs. Miss by subcategory Stacked bars show, for each subcategory, the percentage\nof Match (green) vs. Miss (red) between GPT and the human-selected subcategory rationale, computed only on\nqueries where GPT agrees with the human consensus applicability label. The N label denotes the number of times\nthe subcategory appears in the human rationale set within this agreement subset. Panels show (a) EA and (b) IA.\n27\n"}]}