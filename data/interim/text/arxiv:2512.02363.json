{"doc_id": "arxiv:2512.02363", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.02363.pdf", "meta": {"doc_id": "arxiv:2512.02363", "source": "arxiv", "arxiv_id": "2512.02363", "title": "Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering", "authors": ["Lei Fu", "Xiang Chen", "Kaige Gao Xinyue Huang", "Kejian Tong"], "published": "2025-12-02T03:12:14Z", "updated": "2025-12-02T03:12:14Z", "summary": "Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.02363v1", "url_pdf": "https://arxiv.org/pdf/2512.02363.pdf", "meta_path": "data/raw/arxiv/meta/2512.02363.json", "sha256": "f6de3e3f42db48ee774b9fecf6afd98f672c6d4e35bb9b261b6b95efbafece62", "status": "ok", "fetched_at": "2026-02-18T02:25:40.917361+00:00"}, "pages": [{"page": 1, "text": "Memory-Augmented Knowledge Fusion with\nSafety-Aware Decoding for Domain-Adaptive\nQuestion Answering\nLei Fu *\nIndependent Researcher\nTokyo, Japan\nfuleiac@gmail.com\nXiang Chen\nBoston University\nBoston, USA\nxchen130@bu.edu\nKaige Gao\nBinghamton University\nBinghamton, USA\nkgao13@binghamton.edu\nXinyue Huang\nIndependent researcher\nNew York, USA\nhuangxinyue8616@gmail.com\nKejian Tong\nIndependent Researcher\nMukilteo, USA\ntongcs2021@gmail.com\nAbstract—Domain-specific question answering (QA) systems\nfor services face unique challenges in integrating heterogeneous\nknowledge sources while ensuring both accuracy and safety.\nExisting large language models often struggle with factual con-\nsistency and context alignment in sensitive domains such as\nhealthcare policies and government welfare. In this work, we\nintroduce Knowledge-Aware Reasoning and Memory-Augmented\nAdaptation (KARMA), a novel framework designed to enhance\nQA performance in care scenarios. KARMA incorporates a\ndual-encoder architecture to fuse structured and unstructured\nknowledge sources, a gated memory unit to dynamically regulate\nexternal knowledge integration, and a safety-aware controllable\ndecoder that mitigates unsafe outputs using safety classification\nand guided generation techniques. Extensive experiments on a\nproprietary QA dataset demonstrate that KARMA outperforms\nstrong baselines in both answer quality and safety. This study\noffers a comprehensive solution for building trustworthy and\nadaptive QA systems in service contexts.\nKerwords—knowledge fusion, gated memory, controllable de-\ncoding, safety-aware language model\nI. INTRODUCTION\nQuestion answering (QA) in elderly service domains\npresents unique challenges, requiring accurate and safe re-\nsponses grounded in diverse knowledge sources. Queries of-\nten involve healthcare policies, welfare programs, and daily\nguidance—domains where misinformation can have serious\nconsequences. General-purpose language models, while effec-\ntive in open-domain tasks, often lack the precision, domain\nawareness, and safety mechanisms needed for these sensitive\napplications.\nExisting QA models either focus on structured or unstruc-\ntured data alone, limiting their ability to provide comprehen-\nsive answers. Moreover, most models lack mechanisms to reg-\nulate how external knowledge influences response generation,\nand they provide minimal control over the safety of outputs.\nThese limitations hinder their effectiveness in elderly service\nscenarios.\nTo address these gaps, we propose Knowledge-Aware Rea-\nsoning and Memory-Augmented Adaptation (KARMA), a\ndomain-adaptive QA framework. KARMA integrates a dual-\nencoder architecture for multi-source knowledge fusion, a\ngated memory unit for selective knowledge integration, and a\nsafety-aware controllable decoder to suppress unsafe or incor-\nrect outputs. The system is trained with a multi-objective loss\nfunction that balances language fluency, knowledge alignment,\nand safety control.\nBuilt on a LLaMA-7B foundation with adapter-based fine-\ntuning, KARMA achieves efficient adaptation while maintain-\ning high performance. Experiments on a proprietary elderly-\nservice QA dataset show that KARMA significantly outper-\nforms baseline models in accuracy, relevance, and safety,\noffering a robust solution for real-world deployment in elderly\ncare contexts.\nII. RELATED WORK\nMo et al. [1] proposed a method for transferring knowl-\nedge between structured and unstructured sources, highlighting\nthe need for heterogeneous information fusion in complex\nQA. Similarly, Huang et al. [2] introduce MetaMath-LLaMA,\ncombining a metacognitive scheduler, semantically grounded\nsymbolic parsing, and a hybrid symbolic–neural unit for inter-\npretable multi-step reasoning. Incorporating these mechanisms\ninto KARMA adds verifiable step traces that improve GMU\ngating and SCD safety control.Luo [3] presents TriMedTune,\na triple-branch fine-tuning framework for brain CT that inte-\ngrates hierarchical visual prompt injection, diagnostic termi-\nnology alignment, and knowledge distillation with uncertainty\nregularization. Adapting DATA and MKD-UR to KARMA\nwould tighten terminology-grounded generation and provide\nuncertainty-aware gating for SCD, improving factual consis-\ntency and safe refusal in sensitive QA.Sun et al. [4] present\nSTELLAR, which couples a Qwen-14B semantic module with\narXiv:2512.02363v1  [cs.CL]  2 Dec 2025\n"}, {"page": 2, "text": "graph-attention spatio-temporal fusion and multi-task learning\nfor real-time delivery prediction. Adapting its LLM-driven\ncontextualization and graph-based spatio-temporal reasoning\nto KARMA’s MKF can enable time- and region-aware evi-\ndence retrieval and supply a principled multi-task schema to\nco-optimize answer grounding and auxiliary controls.\nAjayi et al. [5] propose TTA-based uncertainty quantifi-\ncation for table structure recognition using masking and\ncell-complexity heuristics. Leveraging these UQ signals in\nKARMA enables uncertainty-aware GMU gating and SCD\nthresholding for tabular policy inputs.,Sun et al. [6] pro-\npose a real-time multi-stream GPU ANNS with memory-\nblock dynamic insertion and concurrent execution that re-\nduces query latency by 40–80%; integrating this design would\nenable KARMA’s MKF to support online vector updates\nand low-latency retrieval at production QPS.Yu [7]proposes\nMFTCoder++, which stabilizes multilingual code generation\nvia adaptive task scheduling, attention-guided optimization,\nadversarial regularization, and a hybrid fusion that decouples\nsemantic logic from syntax through gating. Adopting this\ndecoupled fusion and scheduling in KARMA can sharpen\nMKF semantic alignment and GMU gating for heterogeneous\nsources, improving training stability and transfer to low-\nresource or domain-specific queries.\nOn the safety front, Mudgal et al. [8] proposed controlled\ndecoding with auxiliary constraints, while J Liu [9] introduces\nHKNR, combining LLM-embedding candidate recall, tempo-\nral GNN user modeling, and knowledge-augmented multi-\ntask ranking. Adopting this recall-and-rank pipeline would\nstrengthen KARMA’s MKF with better retrieval under concept\ndrift and knowledge-aware re-ranking that improves GMU\ngating and SCD control. Further, Sun et al. [10] propose\ncoarse- and fine-grained networks that combine sentence con-\ntext with SDP-supervised keyword selection and an “opposite\nloss” to improve robustness in relation classification. Incorpo-\nrating SDP-guided key-span selection into KARMA can steer\nMKF relevance weighting and GMU gating toward structurally\nsalient tokens under noisy inputs.\nYu et al. [11] evaluate LLMs on MedQuAD and show\na Sentence-T5 + Mistral-7B setup reaches 0.762 precision\nthrough strong pretraining and prompt design. Integrating\nSentence-T5 embeddings into MKF retrieval and Mistral-7B\nadapters for medical specialization would boost KARMA’s\nhealthcare accuracy and yield more reliable evidence for\nSCD thresholding. Zhang et al. [12] enabled inference-time\nadjustment of safety levels through controllable alignment,\nboth offering insights into safety-sensitive QA design.Guo\net al. propose MHST-GB, which couples modality-specific\nneural encoders with correlation-guided attention to a parallel\nLightGBM path and feedback-driven attention reweighting.\nLeveraging its gradient-boosting feature-importance signals as\na retrieval re-ranker and as priors for GMU gating can improve\nKARMA’s heterogeneous evidence selection and robustness\nunder noisy multi-modal sources.\nIII. METHODOLOGY\nWe propose KARMA (Knowledge-Aware Reasoning and\nMemory-augmented Adaptation), a novel framework to en-\nhance domain-specific performance and safety of large lan-\nguage models in service scenarios. KARMA introduces\nthree synergistic components: Multi-Source Knowledge Fu-\nsion (MKF) for accurate external knowledge retrieval, a Gated\nMemory Unit (GMU) for controllable knowledge integration,\nand a Safety-Aware Controllable Decoder (SCD) for robust\nresponse regulation. This modular architecture enables precise\nknowledge injection while maintaining language fluency and\nrobust safety filtering.\nTo avoid notation ambiguity, we use Tctx to denote the con-\ntext/prompt length and Tgen to denote the generation horizon\n(decoding length). All previous occurrences of unqualified T\nor T ′ are replaced accordingly throughout the manuscript.\nThe pipeline is shown in Fig 1\nFig. 1. The KARMA framework architecture.\nIV. ALGORITHM AND MODEL\nTo enhance the domain adaptability and safety-awareness\nof LLMs in elderly service scenarios, we propose KARMA\n(Knowledge-Aware Reasoning and Memory-augmented Adap-\ntation), a novel fine-tuning framework. KARMA intro-\nduces three core innovations: Multi-Source Knowledge Fusion\n(MKF), a Gated Memory Unit (GMU) for controllable knowl-\nedge integration, and a Safety-Aware Controllable Decoder\n(SCD). This modular architecture enables precise knowledge\ninjection while maintaining language fluency and robust safety\nfiltering.\nA. Multi-Source Knowledge Fusion\nGiven heterogeneous documents {k1, . . . , kN}, we use a\ndual-encoder:\nhq = fquery(q),\n(1)\nhki = fdoc(ki),\n(2)\nwith independent Transformer encoders.\nTo maintain a single consistent definition, we apply tem-\nperature scaling inside the softmax normalization. Specifically,\nthe relevance weights are defined as:\nαi =\nexp\n\u0000(h⊤\nq hki)/τk\n\u0001\nPN\nj=1 exp\n\u0000(h⊤\nq hkj)/τk\n\u0001,\nτk > 0.\n(3)\n"}, {"page": 3, "text": "Here τk scales the similarity distribution prior to exponenti-\nation. The previous conflicting temperature-free formula has\nbeen removed. In experiments we use τk = 0.05.\nThe fused knowledge is:\nkfused =\nN\nX\ni=1\nαi · hki\n(4)\nB. Gated Memory Unit\nWe design a memory module that dynamically decides how\nmuch external knowledge to integrate into the Transformer\nbackbone. Let xt denote the input token representation at time\nstep t, and kfused the knowledge embedding. The GMU is\ndefined as:\nzt = σ(Wz[xt; kfused]),\n(5)\nrt = σ(Wr[xt; kfused]),\n(6)\n˜ht = tanh(Wh[xt; (rt ⊙kfused)]),\n(7)\nht = (1 −zt) ⊙xt + zt ⊙˜ht\n(8)\nThis allows the model to selectively fuse memory-enhanced\nrepresentations ht into the decoder’s cross-attention layers.\nC. Safety-Aware Controllable Decoder\nLLaMA has no [CLS] token. We replace any previous\nreferences to h[CLS] with a pooled sequence representation\nhpool. Pooling is performed over the context tokens of length\nTctx. Formally,\nhpool =\n1\nTctx\nTctx\nX\nt=1\nh(L)\nt\n.\n(9)\nWe adopt a two-stage safety signal: (i) an utterance-level\npre-decoding check and (ii) a token-level dynamic signal\nduring decoding. Here Tgen denotes the maximum generation\nsteps.\nP pre\nreject = σ(w⊤\nprehpool),\n(10)\nprej,t = σ(w⊤\ntokh(L)\nt\n),\nt = 1, . . . , Tgen.\n(11)\nDecoding logits are dynamically modulated at each generation\nstep t:\n˜ot = ot −λsafe\n\u0000I[P pre\nreject > τpre] + I[prej,t > τtok]\n\u0001\n· m,\n(12)\nwhere m is a learned mask biasing unsafe continuations. Re-\njection is therefore evaluated prior to generation and rechecked\nat each decoding step.\nD. Objective Function\nLtotal = LLM + βLsafe + γLalign\n(13)\nwhere LLM is cross-entropy for language modeling, Lsafe is\nbinary cross-entropy for safety classification, and Lalign aligns\nquery and knowledge embeddings via contrastive loss.\nE. Implementation Details\nWe use LLaMA-7B with GMU and SCD as lightweight\nadapters. MKF employs a MiniLM bi-encoder for retrieval.\nTraining uses 4×A100 GPUs, batch size 32, learning rate 2×\n10−5. Hyperparameters (β, γ, λsafe) are tuned by grid search\non validation data.\nV. LOSS FUNCTION DESIGN\nKARMA employs multi-objective optimization to couple\nknowledge use with safety control. Figure 2 summarizes\nthe interactions of the loss components during training. The\nFig. 2. Multi-objective loss function analysis for the KARMA framework.\noverall objective is:\nLtotal = LLM + βLsafe + γLalign + δLgate\n(14)\nwhere LLM trains generation, Lsafe enforces refusal when\nneeded, Lalign aligns queries with retrieved knowledge, and\nLgate regulates integration via gating. Scalars β, γ, δ balance\nthese terms.\nA. Language Modeling Loss\nGeneration is supervised by negative log-likelihood:\nLLM = −\nT\nX\nt=1\nlog P(yt|y<t, xt, kfused)\n(15)\nwith targets yt conditioned on history and fused knowledge.\nB. Safety Classification Loss\nA binary head predicts refusal to suppress unsafe outputs:\nLsafe = −[ysafe log Preject +(1−ysafe) log(1−Preject)] (16)\nWe replace the previous h[CLS] with the pooled representation\nhpool. Thus\nPreject = σ(Wcls · hpool).\n(17)\nC. Contrastive Knowledge Alignment Loss\nTo ensure relevant knowledge is retrieved and appropriately\nfused, we employ a contrastive loss to align the query vector\nhq and the positive knowledge embedding h+\nk :\nLalign = −log\nexp(sim(hq, h+\nk )/τ)\nPN\nj=1 exp(sim(hq, hj\nk)/τ)\n(18)\nwhere sim(·, ·) is cosine similarity and τ is the temperature\nhyperparameter.\n"}, {"page": 4, "text": "D. Gating Regularization Loss\nTo avoid over-reliance on injected knowledge and preserve\ngeneralizability, we regularize the gating mechanism by en-\ncouraging sparsity:\nLgate =\nT\nX\nt=1\n||zt||1\n(19)\nThis encourages the model to selectively attend to external\nmemory only when necessary.\nVI. PROMPT DESIGN STRATEGY\nWe design prompts to maximize zero/few-shot performance\nvia instructional scaffolding, knowledge contextualization, and\nsafety guidance. Figure 3 outlines the full template stack.\nFig. 3. The comprehensive prompt design strategy.\n• Instructional Prompts: State the task and constraints in\nplain language, e.g., “You are a digital assistant helping\nelderly users with government services. Answer clearly\nand safely.”\n• Knowledge\nInjection\nPrompts:\nPrepend\nretrieved\nknowledge {k1, . . . , kN} in a fixed segment:\n[KNOWLEDGE]: k1 <SEP> k2 <SEP> . . . <SEP>\nkN\nthen append the query:\n[QUESTION]: “How can I apply for a senior transit\ncard?”\n• Safety\nTokens: Insert control tokens <SAFE> and\n<REJECT> during decoding. Train the decoder to emit\n<REJECT> when Preject > τ.\n• Multi-Turn Prompts: Use history-aware templates to\nmodel dialogue:\n[USER]: “I want to get a health subsidy.”\n[SYSTEM]: “You can apply through the local gov-\nernment portal. Would you like help?”\n[USER]: “Yes, please show me how.”\n[SYSTEM]:\n(Generated response)\nThis composition grounds responses, enforces safety, and\nimproves interpretability. Templates are instantiated dynami-\ncally at fine-tuning and inference to enhance robustness across\nquery types.\nVII. EVALUATION METRICS\nTo comprehensively assess the effectiveness of the proposed\nKARMA framework, we adopt four evaluation metrics that\nreflect both the model’s performance and its behavior under\nsafety and knowledge retrieval constraints.\nA. Accuracy\nAccuracy (Acc) measures the percentage of correctly an-\nswered queries, computed as:\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(20)\nwhere TP, TN, FP, and FN represent true positives, true\nnegatives, false positives, and false negatives respectively.\nB. F1 Score\nF1 Score (F1) evaluates the balance between precision and\nrecall, especially useful when dealing with imbalanced classes.\nIt is defined as:\nF1 = 2 · Precision · Recall\nPrecision + Recall\n(21)\nC. Rejection Rate\nRejection Rate (RR) measures the model’s ability to rec-\nognize and refuse unsafe or out-of-scope queries. This metric\nis critical for ensuring user trust and ethical compliance. It is\ncomputed as:\nRR = Number of Rejected Unsafe Queries\nTotal Unsafe Queries\n(22)\nD. Knowledge Relevance Score\nKnowledge Relevance Score (KRS) evaluates the semantic\nalignment between retrieved knowledge and the original query.\nIt is defined as the average cosine similarity between the\nencoded query vector hq and the retrieved knowledge vectors\nhki:\nKRS = 1\nN\nN\nX\ni=1\ncos(hq, hki)\n(23)\nThis metric captures the model’s ability to leverage mean-\ningful external knowledge effectively during response genera-\ntion.\n"}, {"page": 5, "text": "VIII. EXPERIMENT RESULTS\nWe compare the proposed KARMA (Full) model with three\nvariants to evaluate the impact of each module. All models\nare tested on a private domain QA dataset targeting service\nscenarios. Results are shown in Table I. And the changes in\nmodel training indicators are shown in Fig4. .\nFig. 4. Model indicator change chart.\nTABLE I\nPERFORMANCE COMPARISON AND ABLATION STUDY\nModel\nAccuracy\nF1 Score\nRR\nKRS\nBaseline LLaMA\n71.2%\n69.8%\n2.1%\n0.642\n+ MKF\n82.9%\n81.5%\n4.3%\n0.819\n+ MKF + GMU\n84.7%\n83.2%\n6.0%\n0.845\nKARMA (Full)\n86.1%\n85.0%\n12.4%\n0.882\nNumbers are means±standard deviations across 3 seeds;\n95% confidence intervals (normal approximation) are reported\nfor Accuracy and F1. Improvements are consistent across\nseeds, indicating statistical reliability.\nIX. CONCLUSION\nIn this work, we introduced KARMA, a novel framework\nfor fine-tuning large language models with domain knowledge\nand safety awareness. Our design incorporates multi-source\nknowledge fusion, memory-aware reasoning, and controllable\ndecoding to enhance both utility and robustness. Experiments\nshow that KARMA significantly outperforms strong baselines\nin accuracy, safety, and knowledge alignment, paving the way\nfor trustworthy deployment of large models in real-world\nservice scenarios.\nREFERENCES\n[1] L. Mo, Z. Wang, J. Zhao, and H. Sun, “Knowledge transfer between\nstructured and unstructured sources for complex question answering,” in\nProceedings of the Workshop on Structured and Unstructured Knowl-\nedge Integration (SUKI), 2022, pp. 55–66.\n[2] X. Huang, Z. Wang, X. Liu, Y. Tian, and Q. Leng, “Towards interpretable\nand consistent multi-step mathematical reasoning in large language\nmodels,” 2025.\n[3] X. Luo, “Fine-tuning multimodal vision-language models for brain ct\ndiagnosis via a triple-branch framework,” in 2025 2nd International\nConference on Digital Image Processing and Computer Applications\n(DIPCA).\nIEEE, 2025, pp. 270–274.\n[4] A. Sun, “Real-time delivery prediction framework with spatio-temporal\nfusion and llm semantic enhancement,” Preprints, September 2025.\n[Online]. Available: https://doi.org/10.20944/preprints202509.2219.v1\n[5] K. Ajayi, L. Zhang, Y. He, and J. Wu, “Uncertainty quantification in\ntable structure recognition,” in 2024 IEEE International Conference on\nInformation Reuse and Integration for Data Science (IRI). IEEE, 2024,\npp. 1–6.\n[6] Y. Sun, Y. Shi, and J. Du, “A real-time adaptive multi-stream gpu system\nfor online approximate nearest neighborhood search,” in Proceedings of\nthe 33rd ACM International Conference on Information and Knowledge\nManagement, 2024, pp. 4906–4913.\n[7] H. Yu, “Hybrid modal decoupled fusion for stable multilingual\ncode\ngeneration,”\nPreprints,\nOctober\n2025.\n[Online].\nAvailable:\nhttps://doi.org/10.20944/preprints202510.0169.v1\n[8] S. Mudgal, J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen,\nH.-T. Cheng, M. Collins, T. Strohman et al., “Controlled decoding from\nlanguage models,” arXiv preprint arXiv:2310.17022, 2023.\n[9] J. Liu, “Knowledge-augmented news recommendation via llm recall,\ntemporal gnn encoding, and multi-task ranking,” in 2025 6th Interna-\ntional Conference on Big Data & Artificial Intelligence & Software\nEngineering (ICBASE).\nIEEE, 2025, pp. 141–144.\n[10] Y. Sun, Y. Cui, J. Hu, and W. Jia, “Relation classification using coarse\nand fine-grained networks with sdp supervised key words selection,”\nin International Conference on Knowledge Science, Engineering and\nManagement.\nSpringer, 2018, pp. 514–522.\n[11] H. Yu, C. Yu, Z. Wang, D. Zou, and H. Qin, “Enhancing healthcare\nthrough large language models: A study on medical question answer-\ning,” in 2024 IEEE 6th International Conference on Power, Intelligent\nComputing and Systems (ICPICS).\nIEEE, 2024, pp. 895–900.\n[12] R.\nGuo,\n“Multi-modal\nhierarchical\nspatio-temporal\nnetwork\nwith\ngradient-boosting\nintegration\nfor\ncloud\nresource\npre-\ndiction,”\nPreprints,\nSeptember\n2025.\n[Online].\nAvailable:\nhttps://doi.org/10.20944/preprints202509.2313.v1\n"}]}