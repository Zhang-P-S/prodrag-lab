{"doc_id": "arxiv:2511.15355", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.15355.pdf", "meta": {"doc_id": "arxiv:2511.15355", "source": "arxiv", "arxiv_id": "2511.15355", "title": "HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning", "authors": ["Alexis Correa-Guillén", "Carlos Gómez-Rodríguez", "David Vilares"], "published": "2025-11-19T11:31:32Z", "updated": "2025-11-19T11:31:32Z", "summary": "We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.15355v1", "url_pdf": "https://arxiv.org/pdf/2511.15355.pdf", "meta_path": "data/raw/arxiv/meta/2511.15355.json", "sha256": "2d216b7b1e489115b957b981e8a75f47366ccae251a6b1b466f02ab5bef9dba1", "status": "ok", "fetched_at": "2026-02-18T02:26:36.539493+00:00"}, "pages": [{"page": 1, "text": "HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning\nAlexis Correa-Guillén, Carlos Gómez-Rodríguez, David Vilares\nUniversidade da Coruña, CITIC\nDepartamento de Ciencias de la Computación y Tecnologías de la Información\nCampus de Elviña s/n 15071, A Coruña, Spain\n{alexis.cguillen@udc.es, carlos.gomez, david.vilares}@udc.es\nAbstract\nWe introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice\nreasoning dataset originally released by Vilares and Gómez-Rodríguez (2019).\nThe update responds to the\ngrowing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning.\nWe extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark\nseveral open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional\nmultilingual versions to support future work. Results indicate that performance is mainly driven by model scale\nand intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results\nestablish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.\nKeywords: Multi-choice question answering, LLMs, Healthcare\n1.\nIntroduction\nHEAD-QA (v1) (Vilares and Gómez-Rodríguez,\n2019) is a Spanish/English multiple-choice health-\ncare dataset designed to evaluate model reasoning\nabilities. It comprises 6,765 questions from official\nexams issued between 2013 and 2017. It was con-\nceived as a step toward more demanding bench-\nmarks, following the rise of early reading compre-\nhension datasets such as SQuAD (Rajpurkar et al.,\n2016), SNLI (Bowman et al., 2015), and the AI2\nReasoning Challenge (Clark et al., 2018), among\nothers, as well as the neural architectures devel-\noped for them (Kumar et al., 2016; Chen et al.,\n2017). Notably, experimental results revealed that\nthese architectures lacked the capacity to reason\neffectively about diagnostic knowledge and failed\nto capture definitions and domain-specific concepts\nessential for accurate inference, often performing\nworse than simple information retrieval baselines.\nMore specifically, HEAD-QA consists of multiple-\nchoice questions modeled after Spain’s competi-\ntive specialization exams (Ministerio de Sanidad\nde España, 2023), which are used to evaluate and\nrank graduates in fields such as medicine (MIR),\nnursing (EIR), biology (BIR), chemistry (QIR), psy-\nchology (PIR), and pharmacy (FIR). These highly\ndemanding exams require months or even years\nof preparation, as their results determine both the\nspecialization and the training location where can-\ndidates complete the final 3–5 years of residency\nbefore becoming fully qualified professionals. The\ndataset has since gained notable adoption, hav-\ning been used to evaluate influential architectures\nand models such as RMKV (Peng et al., 2023),\nFalcon (Penedo et al., 2023) and OLMo (Groen-\neveld et al., 2024), to investigate data reliability in\nboth open-source and proprietary systems (Elazar\net al., 2023), and to develop and assess special-\nized solutions in the medical domain (Zhang et al.,\n2023; Wang et al., 2024). It has also served as a\nprecursor to similar medical QA datasets in other\nlanguages, including Chinese (Li et al., 2021) and\nFrench (Labrak et al., 2022), extending its influence\non healthcare question answering research.\nIn the current context, the landscape of ques-\ntion answering and reasoning has changed pro-\nfoundly with the rise of large language models\n(LLMs) (OpenAI, 2023; Jiang et al., 2024a; Dubey\net al., 2024; Liu et al., 2024a; Gemma, 2025;\nYang et al., 2025). These models have advanced\nsubstantially in reasoning, knowledge integration,\nand domain adaptation through instruction tuning\nand retrieval-augmented generation (RAG). This\nshift has redefined what constitutes a challenging\nbenchmark—spanning domains such as coding\n(Zheng et al., 2025), Ph.D.-level knowledge (Phan\net al., 2025), machine translation (Andrews et al.,\n2025) and multimodal reasoning (Padlewski et al.,\n2024)—and has led to an explosion of datasets\n(Rogers et al., 2023; Liu et al., 2024b).\nContribution\nWe present HEAD-QA v2, an ex-\npanded and updated version designed to better re-\nflect the era of large-scale reasoning models. The\nnew release addresses the limited size and tem-\nporal coverage of its predecessor by incorporat-\ning 12,751 multiple-choice questions from Spanish\nprofessional medical qualification exams—more\nthan doubling the dataset and extending its time\nspan. We expect this expansion to enable future\nresearch on model generalization, knowledge reten-\narXiv:2511.15355v1  [cs.CL]  19 Nov 2025\n"}, {"page": 2, "text": "tion, and temporal effects to a greater extent than its\npredecessor. We further establish new baselines\nthrough a systematic evaluation of open-source\nLLMs, exploring multiple inference strategies, in-\ncluding prompting, retrieval-augmented generation,\nand a probability-based approach. Together, we ex-\npect that these contributions offer a practical bench-\nmark for studying how LLMs adapt to domain evolu-\ntion, balance accuracy with efficiency, and perform\ncomplex reasoning in specialized contexts. The\ndataset is available at https://huggingface.\nco/datasets/alesi12/head_qa_v2.\n2.\nDataset Construction\nThis section outlines the construction of HEAD-QA\nv2, which, like its predecessor, is based on offi-\ncial, publicly available exams from the Ministerio\nde Sanidad de España. Each exam includes: (i)\na two-column PDF containing the text, (ii) a CSV\nfile listing the correct answers, and (iii) when appli-\ncable, a folder with referenced images indexed nu-\nmerically (e.g., 1, 2, 3, 4, ...), enabling text–image\nalignment.1\n2.1.\nPreprocessing\nThe preprocessing pipeline involves converting,\ncleaning, and standardizing the exam data.\n1. PDF to text conversion.\nExams were con-\nverted from PDF to plain text using pdfto-\ntext, preserving the two-column layout.\n2. Image mapping. Images were automatically\nlinked, as related questions begin with “Ques-\ntion linked to image no. X,” where X is the\nimage identifier.\n3. Question filtering. Questions without an official\nanswer from the Spanish Ministry of Health\nwere removed, as they correspond to disputed\nor withdrawn items.\n4. Manual corrections. Minor edits to fix errors\nand standardize content. Chemical formulas\nwere converted to SMILES notation (see Fig-\nure 1) using Mathpix , facilitating processing by\ntext-based ML models (Schwaller et al., 2017;\nChithrananda et al., 2020). The few affected\nquestions were processed manually.\nFigure 1: Example of chemical formula converted\nto SMILES notation for text processing.\n1Questions containing images are relatively rare, and\nvisual processing is therefore excluded from this work.\n5. Storage. Files are stored in Parquet format\n(The Apache Software Foundation, 2024) for\nefficient compression and fast download.\n2.2.\nFormat\nEach question includes eight fields (Figure 2). A\nunique identifier requires both name (exam name)\nand qid (question ID).\n• qid (int): Question number within the exam.\n• qtext (str): Question text.\n• ra (int): Correct answer identifier.\n• answers (list): Answer options, each with:\n– aid (int): Option ID.\n– atext (str): Option text.\n• image (Image): Associated image in PIL for-\nmat (Clark and Contributors, 2023), or null if\nnone.\n• year (int): Exam year.\n• category (str): Discipline (e.g., Medicine,\nNursing).\n• name\n(str):\nExam\nidentifier\ncombin-\ning\nyear,\ndiscipline,\nand\nversion\n(e.g.,\nCuaderno_2013_0_B).\n{’qid ’:\n1,\n’qtext ’:\n’Excitatory\npostsynaptic\npotentials :’,\n’ra ’:\n3,\n’answers ’:\n[{’aid ’:\n1,\n’atext ’:\n’Are\nall -\nor - none\nresponses .’},\n{’aid ’:\n2,\n’atext ’:\n’Are\nhyperpolarizing .’},\n{’aid ’:\n3,\n’atext ’:\n’Can\nbe\nsummed .’},\n{’aid ’:\n4,\n’atext ’:\n’Propagate\nover\nlong\ndistances .’},\n{’aid ’:\n5,\n’atext ’:\n’Exhibit\na\nrefractory\nperiod . ’}] ,\n’image ’:\nNone ,\n’year ’:\n2013 ,\n’category ’:\n’biology ’,\n’name ’:\n’Cuaderno_2013_1_B ’}\nFigure 2: A HEAD-QA v2 question in JSON format.\n2.3.\nDataset statistics\nThe dataset contains a total of 12,751 questions\ndistributed across six disciplines and ten years (see\nTable 1). Among them, 334 questions include im-\nages. Of these, 36 correspond to the four most\nrecent nursing exams (2019–2022), while the rest\nbelong to the medical exams—with over 30 image-\nbased questions per test until 2018, and around 25\nper test in subsequent years.\n"}, {"page": 3, "text": "’13\n’14\n’15\n’16\n’17\n’18\n’19\n’20\n’21\n’22\nTotal\nBIR\n227\n225\n226\n228\n226\n221\n177\n177\n203\n209 2 119\nQIR\n228\n228\n228\n231\n227\n229\n179\n179\n205\n206 2 140\nMIR\n227\n228\n231\n232\n231\n230\n181\n183\n207\n206 2 156\nEIR\n181\n203\n230\n223\n232\n228\n181\n180\n206\n205 2 069\nFIR\n229\n228\n225\n228\n229\n228\n180\n182\n207\n210 2 146\nPIR\n226\n227\n226\n230\n225\n228\n180\n173\n202\n204 2 121\nTotal 1 318 1 339 1 366 1 372 1 370 1 364 1 078 1 074 1 230 1 240 12 751\nTable 1: Number of questions per discipline/year.\nEach question has one and only one correct an-\nswer.\nIn the 2013 and 2014 exams, questions\ninclude five possible answers (2,657 items, rep-\nresenting 21% of the total), while the remaining\nexams feature four options per question. The cor-\nrect answer is approximately uniformly distributed\nacross the available options, although it is slightly\nless likely to appear in the first and last positions.\nThis is not specific to this dataset but rather a well-\ndocumented bias in test design, as examiners tend\nto avoid placing the correct answer at the extremes\n(Attali and Bar-Hillel, 2003). This minor imbalance\nis not directly relevant to the purposes of this work,\nas no model is trained or conditioned on answer po-\nsitions. Yet, recent studies have showed that LLMs\nexhibit positional biases in multiple-choice tasks,\nslightly favoring middle options (Pezeshkpour and\nHruschka, 2023; Zheng et al., 2024).\nIn terms of question length (Figure 3), it remains\nstable over time, with the trend observed in HEAD-\nQA v1 persisting in recent years. Differences are\nmore evident across disciplines (Figure 4): ques-\ntions in biology, chemistry, pharmacology, and psy-\nchology tend to be shorter, while those in medicine\nand nursing are generally longer and detailed, often\ninvolving diagnostic reasoning that requires precise,\ncontext-rich information.\n2.4.\nMachine Translation and Variants\nTo assess the impact of language variation, we con-\nsider the original Spanish dataset and its machine-\ntranslated English version, based on the approach\nof Vilares and Gómez-Rodríguez, who addressed\nthe same objective in HEAD-QA v1 using Google’s\nseq2seq model.\nFor v2, we follow the recent\ntrend of using LLMs for translation, leveraging their\nstrong contextual reasoning and ability to process\nlonger inputs while maintaining high translation\nquality across domains (Vilar et al., 2023; Zhu et al.,\n2024). In particular, we adopt LLaMA-3.1-8B and\nits instruction-tuned variant.\nTranslation prompt.\nWe explored three prompt-\ning configurations: (i) zero-shot, providing a min-\nimal translation instruction; (ii) one-shot, adding\na single manually translated example mirroring\nthe HEAD-QA format; and (iii) an instruction-\ntuned setup where the system message defines\nFigure 3: Question length distribution by year.\nFigure 4: Question length distribution by discipline.\nthe model as an “expert translator,” sets the\nSpanish→English direction, and enforces two rules:\n(a) preserve the multiple-choice format, and (b) out-\nput only the translation. The user message is the\nSpanish question and options verbatim.\nFormat integrity.\nTo maintain structural parity\nwith the source, we apply light post-processing:\nearly stopping when the model emits the last option\nor begins a new prompt keyword (e.g., SPANISH);\nremoval of trailing, non-requested text; normaliza-\ntion of option identifiers (replacing variants like “A)”,\n“a.”, etc., with 1., 2., ...); and validation checks\nto ensure that the output is a proper translation\nrather than an attempted answer or commentary\n(i.e., non-empty question and options, and consis-\ntent number of options). We check output valid-\nity through automatic checks for (i) empty text in\nthe question or options, (ii) mismatched number\nof options, and (iii) incorrect numbering (e.g., re-\npeated or misordered identifiers). For English, the\ninstruction-tuned configuration produced the fewest\nerrors (28), followed by the zero-shot (44) and one-\nshot (186) setups. This pattern shows that these\nmodels adhere to structured translation prompts,\nobtaining stable, well-aligned outputs.\n"}, {"page": 4, "text": "Selection of final translations.\nEach question\nhas three translated versions per target language,\ncorresponding to different prompting configurations.\nThe final dataset is compiled by selecting the most\nreliable translation according to the following rules:\n(i) if only one version is valid, it is kept; (ii) if several\nare valid, the one from the configuration with fewer\nerrors is chosen; and (iii) if all are valid, the two\nmost similar are compared, selecting the one from\nthe lower-error setup. Questions without a valid\ntranslation are discarded. A manual evaluation of\na random sample of questions was conducted to\nverify the reliability and consistency of this selection\nprocedure.\nOther language variants.\nUsing the same trans-\nlation and selection pipeline, we additionally gener-\nated Italian, Galician, and Russian versions of the\ndataset. Automatic format checks confirmed good\nstructural consistency across these languages.\nWhile model evaluation was not conducted on these\nversions—since, unlike for English, no human vali-\ndation could be performed in the final selection step\ndue to resource constraints—they will be released\nalongside the main dataset to serve as a foundation\nfor future research on cross-lingual and multilingual\nevaluation within the HEAD-QA framework.\nQualitative evaluation of translations.\nStill, to\nautomatically assess the quality of the full trans-\nlated datasets (English, Italian, Russian, and Gali-\ncian) and enable comparison with future versions,\nwe apply a back-translation (BT) approach. Each\ntarget-language version is translated back into\nSpanish and compared with the original text, ob-\ntaining round-trip translation (RTT) scores as a\nreference-free quality proxy (Zhuo et al., 2023). We\ncompute both surface-level (BLEU) and semantic\n(BERTScore) similarity metrics. Results show that\nGalician and Italian achieve the highest BLEU (0.66\nand 0.57) and BERTScore-F1 (0.80 and 0.77), fol-\nlowed by English (0.41 / 0.69) and Russian (0.33\n/ 0.65). These values are strong overall and con-\nsistent with linguistic distance, languages closer to\nSpanish yield higher lexical and semantic similarity,\nconfirming that the translation pipeline maintains\nrobust and semantically reliable outputs across all\nlanguages.\n3.\nBaselines and Inference Strategies\nNext, we present the models and inference strate-\ngies adopted, following standard practices.\n3.1.\nModels\nWe evaluate four open-access, instruction-tuned\nLLMs:\nLlama 3.1 (8B, 70B)\n(Dubey et al., 2024)\nDecoder-only models with 8B and 70B param-\neters, trained on multilingual data and officially\nsupporting several languages beyond English, in-\ncluding Spanish.\nBoth are optimized for long-\ncontext processing and use grouped-query atten-\ntion (GQA) (Ainslie et al., 2023) to improve in-\nference efficiency over standard multi-head atten-\ntion (Vaswani et al., 2017).\nMistral v0.3 (7B)\n(Jiang et al., 2023) A 7B\ndecoder-only model that combines grouped-query\nand sliding-window attention for efficient process-\ning of sequences.\nMixtral v0.1 (8×7B)\n(Jiang et al., 2024b) Archi-\ntecturally similar to Mistral 7B, Mixtral introduces a\nMixture-of-Experts (MoE) design, activating two of\neight experts per token to enhance efficiency by\nlimiting active computation at each step.\nThe two model families, Llama 3.1 and Mistral,\nwere selected for their broad adoption and good\nperformance across diverse NLP tasks. Choos-\ning one smaller and one larger model from each\nfamily enables a controlled examination of scaling\neffects in HEAD-QA v2, clarifying how model capac-\nity influences biomedical reasoning and multiple-\nchoice performance. While an exhaustive com-\nparison across all available LLMs is beyond this\nstudy’s scope, these models span both dense and\nmixture-of-experts architectures, offering a repre-\nsentative and methodologically sound basis for\nanalysis. Since the primary objective of this work is\nthe dataset itself, model evaluation serves mainly to\ncharacterize its difficulty and illustrate how different\narchitectures respond to its challenges.2\n3.2.\nAnswer Selection Strategies\nEach model answers multiple-choice questions us-\ning a consistent input–output scheme.\nModel Input.\nBy default, each question is format-\nted as a single text sequence that includes the ques-\ntion stem and its possible answers, each preceded\nby a numerical index, as illustrated in Figure 5.\nModel Output.\nFor all inference strategies, the\nmodel is queried to produce a short JSON struc-\nture indicating the index of the selected answer.\n2All experiments were conducted under consistent\nhardware conditions using NVIDIA A100 GPUs (40 GB)\nwith 16-bit precision. Smaller models (Mistral-7B and\nLlama-3.1-8B) were run on single-GPU nodes, whereas\nlarger ones (Llama-3.1-70B and Mixtral-8x7B) required\nfour GPUs, distributing the computational load evenly\nacross devices.\n"}, {"page": 5, "text": "Excitatory\npostsynaptic\npotentials :\n1.\nAre\nall -or - none .\n2.\nAre\nhyperpolarizing .\n3.\nCan\nbe\nsummed .\n4.\nPropagate\nover\nlong\ndistances .\n5.\nHave\na\nrefractory\nperiod .\nFigure 5: HEAD-QA v2 question encoded as a\nsingle input sequence.\nFor example, if the chosen option is the third, the\nexpected output is {answer:\n3}. Enforcing a\nfixed output format simplifies both extraction and\npost-processing of predictions, regardless of minor\nvariations in spacing, casing, or punctuation.\n3.2.1.\nPrompting Strategies\nZero-shot prompting\nFigure 6 shows the prompt\nused in the zero-shot setting. It defines the ex-\npected output format and provides minimal condi-\ntioning, instructing the model to act as an expert in\nscientific and healthcare domains.\n<| begin_of_text |><| start_header_id |>system\n<| end_header_id |>\n<| eot_id |><| start_header_id |>user <|\nend_header_id |>\nYou\nare\nan\nexpert\nin\nspecialized\nscientific\nand\nhealth\ndisciplines .\nRespond\nto\nthe\nfollowing\nmultiple - choice\nquestion :\nProvide\nthe\nanswer\nin\nthe\nfollowing\nJSON\nformat :\n{ Answer :\n[ number ]}\nFor\nexample ,\nif\nthe\nanswer\nis\n1,\nwrite :\n{\nAnswer :\n1} <| eot_id |><| start_header_id |>\nuser <| end_header_id |>\n<PLACEHOLDER\nFOR\nTHE\nQUESTION\nAND\nOPTIONS\n><| eot_id |><| start_header_id |> assistant\n<| end_header_id |>\nFigure 6: Zero-shot prompt.\nThe example, for\nLlama-3.1, shows the use of headers and special\ntokens that delimit user–assistant interactions and\nmetadata as specified by the model architecture.\nIn-context learning\nLLMs often perform better\nwhen given examples within the prompt, as these\nhelp condition their responses. In this work, Fig-\nure 7 shows the few-shot prompt for Spanish ques-\ntions, which includes three fixed examples from\ndiverse disciplines. These examples, adapted from\nthe United States Medical Licensing Examination\n(USMLE) questions, were selected to match the\nnature of HEAD-QA.3 While a detailed analysis\nis beyond the scope of this study, prior work has\nshown that the choice and quality of in-context ex-\namples can strongly influence performance (Bon-\nisoli et al., 2025). This phenomenon has also been\n3Parallel Spanish and English versions were created\nto ensure linguistic and domain consistency.\ninterpreted as a form of implicit learning during infer-\nence (Dherin et al., 2025), suggesting that models\nmay adapt dynamically—an ability that would be\nparticularly relevant for sensitive (and very person-\nalized) domains such as healthcare.\n<| begin_of_text |><| start_header_id |>system <| end_header_id |>\nYou\nare\nan\nexpert\nin\nspecialized\nscientific\nand\nhealth\ndisciplines .\nRespond\nto\nthe\nfollowing\nmultiple - choice\nquestion\nby\nindicating\nonly\nthe\nnumber\nof\nthe\ncorrect\noption .\nNo\nexplanations\nare\nneeded .<| eot_id |><|\nstart_header_id |>user <| end_header_id |>\nWhich\nneurotransmitter\nis\nprimarily\ninvolved\nin\nmood\nregulation ?\n1.\nDopamine\n2.\nSerotonin\n3.\nGABA\n4.\nAcetylcholine <| eot_id |><| start_header_id |> assistant <|\nend_header_id |>\n{ Answer :\n2} <| eot_id |><| start_header_id |>user <| end_header_id\n|>\nWhich\nof\nthe\nfollowing\nis\nan\nexample\nof\na\nneutralization\nreaction\nin\nchemistry ?\n1.\nCH4\n+\n2O2\n->\nCO2\n+\n2H2O\n2.\nNa\n+\nCl2\n->\n2 NaCl\n3.\n2H2\n+\nO2\n->\n2H2O\n4.\nHCl\n+\nNaOH\n->\nNaCl\n+\nH2O <| eot_id |><| start_header_id |>\nassistant <| end_header_id |>\n{ Answer :\n4} <| eot_id |><| start_header_id |>user <| end_header_id\n|>\n...\n<PLACEHOLDER\nFOR\nTHE\nQUESTION\nAND\nOPTIONS ><| eot_id |><|\nstart_header_id |> assistant <| end_header_id |>\nFigure 7: Example of a few-shot prompt with sam-\nples. Case shown for the Llama-3.1-8B model.\nChain-of-Thought prompting\nIn this setting, the\nmodel is instructed to produce brief reasoning steps\nbefore providing an answer. As shown in Figure 8,\nthe prompt asks the model to evaluate each op-\ntion before selecting the most plausible one. This\ndesign encourages reasoning while keeping gener-\nations concise and inference efficient.\n<| begin_of_text |><| start_header_id |>system <| end_header_id |>\nYou\nare\nan\nexpert\nin\nscientific\nand\nhealth\ndisciplines .\nCarefully\nanalyze\nthe\nfollowing\nmultiple - choice\nquestion\nand\nprovide\nthe\ncorrect\nanswer .\nThere\nis\none\nand\nonly\none\ncorrect\nanswer .\nThink\nthrough\neach\noption\nbriefly\nbefore\nresponding\nin\nthe\nJSON\nformat :\n{ Answer :\n[ number ]}. <| eot_id |><| start_header_id |>user <|\nend_header_id |>\n...\nFigure 8: Example of a CoT prompt with brief rea-\nsoning before the final answer, using the Llama-\n3.1-8B model.\n3.2.2.\nRetrieval-Augmented Generation\nFollowing an approach shown to improve biomedi-\ncal question answering (Xiong et al., 2024), in this\nwork we also aim to mitigate potential hallucinations\nby retrieving relevant passages from an external,\nreliable corpus and appending them to the model’s\nprompt to better guide its responses.\n"}, {"page": 6, "text": "Our RAG implementation consists of three com-\nponents: (i) an LLM, (ii) a biomedical corpus, and\n(iii) a retrieval system. For (i), we use the mod-\nels introduced in §3.1. For (ii), we use the cor-\npus proposed by Jin et al. (2021), which con-\ntains 18 medical textbooks commonly used for\nUSMLE preparation.4 For (iii), we use MedCPT (Jin\net al., 2023), a dual-encoder model based on\nBERT (Devlin et al., 2019). It includes two spe-\ncialized encoders—ncbi/MedCPT-Article-Encoder\nand ncbi/MedCPT-Query-Encoder—that map cor-\npus fragments and queries (here, HEAD-QA v2\nquestions) into 768-dimensional vectors.5,6\nSince the corpus is in English, retrieval was per-\nformed using English-translated versions of the\nquestions, and the retrieved passages were reused\nfor both the English and Spanish versions of the\nbenchmark. Each question was paired with the\ntwo most relevant fragments, balancing contextual\ncoverage with efficiency during LLM inference (to-\ngether with the zero-shot prompt).\nAssessing corpus alignment\nTo evaluate the\nsuitability of this corpus for our benchmark, Fig-\nures 9, 10, and 11 show a two-dimensional\nUMAP (McInnes et al., 2020) projection of all 126k\ncorpus fragments and the 12k HEAD-QA v2 ques-\ntions. Distinct clusters correspond to individual\ntextbooks, with minimal overlap. Importantly, most\nHEAD-QA v2 questions project into high-density\ncorpus regions, indicating strong topical alignment.\nFor example, psychology questions cluster around\nPsychiatry_DSM-5 and Neurology_Adams, while\nbiology and pharmacology items align with related\nsources. These observations suggest that the cor-\npus and retrieval setup may supply relevant con-\ntextual evidence, motivating their inclusion as a\nbaseline for our benchmark.\n3.2.3.\nSelection via log-probabilities\nUnlike the previous methods, which require au-\ntoregressive text generation, this approach directly\n4This dataset, publicly available on the Hugging\nFace Hub (https://huggingface.co/datasets/\nMedRAG/textbooks),\nconsists\nof\napproximately\n126,000 short text fragments,\neach under 1,000\ncharacters.\n5Semantic similarity is computed via dot product.\nThese models were trained on 255 million PubMed\nquery–article pairs, making them highly effective for\nbiomedical retrieval.\n6For similarity search, we use FAISS (Facebook AI\nSimilarity Search) (Douze et al., 2024), leveraging its na-\ntive integration with the Hugging Face datasets library\nfor low-memory data handling. We employ a flat index\ntype, which performs exhaustive comparison across all\nvectors with 32-bit precision, ensuring maximal retrieval\naccuracy.\ncompares the probabilities that a language model\nassigns to each candidate answer sequence.\nFormally, let C\n=\n(c1, c2, . . . , cn) represent\nthe token sequence of a question and Ai\n=\n(a1, a2, . . . , am) the sequence corresponding to the\ni-th answer option. For each token aj, the model\ncomputes a conditional probability qj = P(Xn+j |\nX1 = c1, . . . , Xn = cn, Xn+1 = a1, . . . , Xn+j−1 =\naj−1).\nThe overall likelihood of an answer se-\nquence is then defined as the geometric mean of\nits token probabilities, P(Ai) =\n\u0000 Qm\nj=1 qj(aj)\n\u00011/m.\nThe model selects as correct the option that max-\nimizes this probability, i.e., = arg maxi P(Ai). Be-\ncause multiplying many small probabilities can lead\nto numerical instability, all computations are per-\nformed in 32-bit precision. In addition, probabili-\nties are evaluated in log-space to improve stabil-\nity and efficiency, using the equivalent formulation\nlog P(Ai) = 1\nm\nPm\nj=1 log qj(aj).\n4.\nExperimental setup\nPerformance is evaluated using three metrics:(1)\naccuracy, the proportion of correct answers; (2)\nthe normalized exam score, based on the official\nSpanish medical exam scheme (three wrong an-\nswers cancel one correct) and normalized by total\nitems; and (3) the unanswered ratio, the fraction of\nquestions with no valid response.\n4.1.\n‘Prompting Strategy’ Evaluation\nTable 2 reports performance metrics for all prompt-\ning configurations (zero-shot, few-shot, and CoT).\nPrompt\nModel\nEnglish (en)\nSpanish (es)\nAcc\nScore\nPna\nAcc\nScore\nPna\nZero-shot\nMixtral-8x7B\n70.59 66.97 2.03 66.01 60.43\n4.94\nMistral-7B\n59.55 52.61 4.82 52.79 43.56\n3.25\nLlama-3.1-8B\n70.43 67.86 0.39 61.93 56.61\n0.38\nLlama-3.1-70B 83.15 84.16 0.43 83.27 84.14\n0.40\nFew-shot\nMixtral-8x7B\n69.78 66.23 4.64 66.85 62.05\n3.83\nMistral-7B\n60.63 54.59 4.42 54.06 45.90\n3.02\nLlama-3.1-8B\n70.49 68.24 0.36 62.58 57.44\n0.24\nLlama-3.1-70B 82.90 84.14 0.51 83.24 84.41\n0.38\nCoT\nMixtral-8x7B\n67.08 62.53 6.71 64.27 58.66\n7.22\nMistral-7B\n56.19 47.89 8.54 48.07 36.55 10.55\nLlama-3.1-8B\n69.13 66.50 5.55 61.05 55.30\n6.11\nLlama-3.1-70B 82.54 84.20 2.11 82.10 83.21\n4.07\nTable 2: Performance metrics (accuracy, exam\nscore, and proportion of unanswered questions) for\nall prompting configurations (zero-shot, few-shot,\nand CoT) in English and Spanish. Best values per\ncolumn are highlighted in bold.\nOverall, performance is consistently higher in\nEnglish than in Spanish across all configurations,\nexcept for Llama-3.1-70B, where results are equiv-\nalent.\nThis confirms that models handle En-\nglish—either natively or through translation—more\neffectively.\nThe gap is particularly pronounced\n"}, {"page": 7, "text": "Figure 9: Kernel density estima-\ntion of corpus fragments by text-\nbook source.\nFigure 10: Global kernel density\nestimation of the corpus (with-\nout separating by textbook).\nFigure 11: Scatter plot of HEAD-\nQA v2 questions by discipline\noverlaid on the corpus density\nmap.\nFigure 12: Performance evolution over time for\neach model on the English subset under the prompt-\ning setup. Colors indicate model families: Mistral-\n7B (green), Mixtral-8x7B (blue), Llama-3.1-70B (or-\nange), and Llama-3.1-8B (pink). Markers denote\nprompting strategies: squares for zero-shot, trian-\ngles for few-shot, and diamonds for CoT.\nin smaller models, suggesting that limited capac-\nity (at least to represent specialized healthcare\nknowledge) amplifies cross-lingual variability. In\ncontrast, larger models show stronger generaliza-\ntion, narrowing the difference between languages.\nModel scale has a clear impact: accuracy and exam\nscores increase steadily with model size, while the\nproportion of unanswered questions decreases.\nRegarding prompting strategies, zero-shot and\nfew-shot approaches achieve comparable results,\nsuggesting that providing a single example offers\nlimited additional benefit given the models’ instruc-\ntion tuning. Exploring the impact of example selec-\ntion could be an interesting direction for future work.\nIn contrast—perhaps unexpectedly—CoT prompt-\ning consistently reduces accuracy and increases\nnon-response rates except for the Llama-3.1-70B,\nindicating that explicit reasoning steps may actually\nreduce performance in this healthcare domain.\nFigure 12 shows that English performance re-\nmains stable across exam years, with larger mod-\nels outperforming smaller ones. English results are\nslightly higher than Spanish (not shown for space),\nand simpler prompting strategies get the most reli-\nable outcomes.\n4.2.\n‘RAG Strategy’ Evaluation\nPrompt Model\nEnglish (en)\nSpanish (es)\nAcc\nScore\nPna\nAcc\nScore\nPna\nRAG\nMixtral-8x7B\n69.80 66.25 4.67 66.90 62.07 3.91\nMistral-7B\n56.63 49.11 2.14 49.61 39.70 2.30\nLlama-3.1-8B\n66.45 62.83 0.69 59.13 52.86 0.57\nLlama-3.1-70B 82.45 83.13 0.32 82.52 83.03 0.22\nTable 3: Performance metrics (accuracy, exam\nscore, and proportion of unanswered questions)\nfor all RAG-based configurations in English and\nSpanish.\nTable 3 presents the performance metrics for the\nmodels that used RAG to condition their prompt.\nOverall, results show that incorporating retrieved\ncontext through RAG does not lead to consistent im-\nprovements over standard prompting. Performance\nremains slightly higher in English than in Spanish.\nLarger models benefit the most from RAG, maintain-\ning competitive accuracy and lower non-response\nrates, while smaller models tend to degrade when\nexposed to noisy or weakly relevant evidence.\nCompared to the zero-shot baseline, RAG gets\nslightly lower scores in both languages. This sug-\ngests that the retrieved passages are not always\neffectively integrated into the generation process,\nthat the model can often rely on its internal knowl-\nedge instead, or that the retrieved information is not\nsufficiently relevant. The weak correlation between\nretrieval relevance (see §3.2.2) and answer correct-\nness (r = 0.07) further supports this interpretation:\nmodel performance appears to depend primarily on\ninternal knowledge rather than external evidence.\nYearly trends remain stable across models and lan-\nguages, closely mirroring those observed in the\nprompting experiments, as shown in Figure 13.\n4.3.\n‘Log-probability’ Evaluation\nTable 4 reports accuracy and normalized exam\nscores for this setup. By design, the unanswered\n"}, {"page": 8, "text": "Figure 13: Performance evolution over time for\neach model in English under the RAG setup. Col-\nors indicate model families: Mixtral-8x7B (green),\nMistral-7B (orange), Llama-3.1-8B (blue), and\nLlama-3.1-70B (pink).\nratio is 0%, as models are required to select one\noption per question. Despite this, scores drop no-\ntably compared to prompting-based approaches,\nindicating that direct likelihood evaluation is less\neffective for multiple-choice reasoning.\nPerfor-\nmance remains consistently higher in English than\nin Spanish, with the gap being more pronounced\nin smaller models. Larger models mitigate this dif-\nference, maintaining more stable accuracy across\nlanguages. The observed performance gap can\nalso be attributed to the independent evaluation of\neach option: without jointly considering all alterna-\ntives, models lose the elimination-based reasoning\nthat benefits prompting approaches.\nAs shown in Figure 14, yearly trends remain\nstable, with only minor fluctuations after 2018.\nAlthough this method minimizes resource us-\nage—since no text generation is involved—the effi-\nciency gain does not compensate for the accuracy\nloss, limiting its practical value for HEAD-QA v2.\nStrategy\nModel\nEnglish (en)\nSpanish (es)\nAcc\nScore\nAcc\nScore\nProb-based\nMixtral-8x7B\n52.84\n44.06\n47.87\n37.60\nMistral-7B\n47.86\n37.49\n39.42\n27.69\nLlama-3.1-8B\n45.25\n34.31\n37.82\n24.80\nLlama-3.1-70B\n54.15\n46.04\n51.42\n42.12\nTable 4: Performance metrics (accuracy and exam\nscore) for the probability-based selection strategy\n(Section 3.2.3) in English and Spanish.\n5.\nDiscussion\nThe results reveal consistent trends across model\nfamilies, highlighting how architectural scale, lan-\nguage, and methodological design shape perfor-\nmance in HEAD-QA v2. Model size emerges as\nthe most decisive factor: Llama-3.1-70B consis-\nFigure 14: Performance evolution over time for\neach model in English under the probability-based\nselection setup. Colors indicate model families:\nLlama-3.1-8B (green), Llama-3.1-70B (orange),\nMistral-7B (blue), and Mixtral-8x7B (pink).\ntently achieves the highest accuracy and normal-\nized exam scores, while smallest models performs\nlowest across all metrics. These results align with\nbroader findings in LLM evaluation, where scaling\nenhances both factual recall and reasoning stability.\nLanguage effects are present but moderate, with\nsmaller models showing slightly reduced perfor-\nmance in Spanish. This may stem from differences\nin tokenization efficiency, knowlegde integration,\nand from weaker multilingual representations in\nsmaller architectures, which maybe be less robust\nto lexical and syntactic variability across languages.\nMethodologically, neither more elaborate prompt-\ning (few-shot or CoT) nor retrieval-augmented gen-\neration produces consistent improvements.\nIn\nsome cases, these strategies even reduce perfor-\nmance, suggesting that additional contextual input\ncan introduce noise or divert the model from lever-\naging its internal knowledge.\nConsidering their\nhigher computational and developmental costs,\nsuch methods offer limited benefit in this setting.\nFinally, the probability-based answer selection\nstrategy performs notably worse than generation-\nbased approaches. Since each option is scored\nindependently, the model cannot perform the com-\nparative reasoning and contextual alignment typical\nof human multiple-choice problem-solving, result-\ning in systematic accuracy drops.\n6.\nConclusion\nThis work introduced HEAD-QA v2, a new large-\nscale, multilingual benchmark designed to eval-\nuate complex reasoning in the biomedical do-\nmain. Through extensive experiments across mul-\ntiple modern large language models and inference\nstrategies, we established empirical baselines and\n"}, {"page": 9, "text": "analyzed the factors that most influence perfor-\nmance. Our findings indicate that, for highly special-\nized biomedical question answering, the intrinsic\nknowledge and reasoning capacity of the language\nmodel play a far greater role than the sophistication\nof the inference strategy. Techniques such as RAG\nand CoT prompting, while successful in other do-\nmains, did not obtain consistent gains in this setting\nand introduced additional computational and im-\nplementation overhead. Overall, improvements on\nHEAD-QA v2 seem more closely tied to scaling and\nrefining the underlying models than to increasing\ninference complexity, though alternative strategies\nmay still offer potential for future exploration.\nLimitations\nThis study did not include evaluations with fron-\ntier proprietary LLMs such as GPT-4, Claude, or\nGemini, primarily due to funding resources to ac-\ncess APIs. Consequently, the results reflect trends\namong open-access models up to 70B parameters.\nAdditionally, while the English translations were\nautomatically generated and reviewed for termino-\nlogical consistency, large-scale human validation\nwas not feasible. Minor translation inconsistencies\ncould therefore influence model performance, es-\npecially in domain-specific terminology.\nAnother limitation concerns the scope of the\nbenchmark itself. HEAD-QA v2 focuses on multiple-\nchoice biomedical questions, which represent only\na subset of complex reasoning skills.\nEthical Considerations\nHEAD-QA v2 is based on publicly available exami-\nnation questions designed for healthcare education,\ncontaining no personal or patient data. Neverthe-\nless, the dataset and experiments involve content\nrelated to medical knowledge, and outputs from\nlarge language models should not be interpreted\nas clinical advice.\nAll experiments were conducted with open-\naccess models and publicly available data, ensur-\ning reproducibility and compliance with data use\nterms. We acknowledge that automatic translation\nand model-generated text may propagate biases or\ninaccuracies, and encourage caution when using\nthe dataset or models in real-world or educational\nhealthcare contexts.\nAcknowledgments\nWe\nacknowledge\ngrants\nGAP\n(PID2022-\n139308OA-I00)\nfunded\nby\nMICI-\nU/AEI/10.13039/501100011033/\nand\nERDF,\nEU; LATCHING (PID2023-147129OB-C21) funded\nby MICIU/AEI/10.13039/501100011033 and ERDF,\nEU; and TSI-100925-2023-1 funded by Ministry\nfor Digital Transformation and Civil Service and\n“NextGenerationEU” PRTR; as well as funding by\nXunta de Galicia (ED431C 2024/02), and CITIC,\nas a center accredited for excellence within the\nGalician University System and a member of the\nCIGUS Network, receives subsidies from the\nDepartment of Education, Science, Universities,\nand Vocational Training of the Xunta de Galicia.\nAdditionally, it is co-financed by the EU through the\nFEDER Galicia 2021-27 operational program (Ref.\nED431G 2023/01).\n7.\nBibliographical References\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong,\nYury Zemlyanskiy, Federico Lebrón, and Sumit\nSanghai. 2023. Gqa: Training generalized multi-\nquery transformer models from multi-head check-\npoints.\nPierre Andrews, Mikel Artetxe, Mariano Coria\nMeglioli, Marta R Costa-jussà, Joe Chuang,\nDavid Dale, Cynthia Gao, Jean Maillard, Alex\nMourachko, Christophe Ropers, et al. 2025. Bou-\nquet: dataset, benchmark and open initiative for\nuniversal quality evaluation in translation. arXiv\npreprint arXiv:2502.04314.\nYigal Attali and Maya Bar-Hillel. 2003.\nGuess\nwhere:\nThe position of correct answers in\nmultiple-choice test items as a psychometric\nvariable. Journal of Educational Measurement,\n40(2):109–128.\nGiovanni Bonisoli, David Vilares, Federica Rollo,\nand Laura Po. 2025. Document-level event ex-\ntraction from italian crime news using minimal\ndata. Knowledge-Based Systems, 317:113386.\nSamuel R. Bowman, Gabor Angeli, Christopher\nPotts, and Christopher D. Manning. 2015. A large\nannotated corpus for learning natural language\ninference. In Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 632–642, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and An-\ntoine Bordes. 2017. Reading Wikipedia to an-\nswer open-domain questions. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long\nPapers), pages 1870–1879, Vancouver, Canada.\nAssociation for Computational Linguistics.\nSeyone Chithrananda, Gabriel Grand, and Bharath\nRamsundar. 2020. Chemberta: Large-scale self-\n"}, {"page": 10, "text": "supervised pretraining for molecular property pre-\ndiction.\nAlex Clark and Contributors. 2023.\nPillow (pil\nfork). https://python-pillow.org/. Ver-\nsion 10.0.0, Accessed: October 13, 2025.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar\nKhot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis,\nMinnesota. Association for Computational Lin-\nguistics.\nBenoit Dherin, Michael Munn, Hanna Mazzawi,\nMichael Wunder, and Javier Gonzalvo. 2025.\nLearning without training: The implicit dynam-\nics of in-context learning.\nMatthijs\nDouze,\nAlexandr\nGuzhva,\nChengqi\nDeng, Jeff Johnson, Gergely Szilvasy, Pierre-\nEmmanuel Mazaré, Maria Lomeli, Lucas Hos-\nseini, and Hervé Jégou. 2024. The faiss library.\nAbhimanyu Dubey,\nAbhinav Jauhri,\nAbhinav\nPandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten,\nAmy Yang, Angela Fan, et al. 2024. The llama\n3 herd of models. arXiv e-prints, pages arXiv–\n2407.\nYanai Elazar, Akshita Bhagia, Ian Magnusson, Ab-\nhilasha Ravichander, Dustin Schwenk, Alane\nSuhr, Pete Walsh, Dirk Groeneveld, Luca Sol-\ndaini, Sameer Singh, et al. 2023. What’s in my\nbig data? arXiv preprint arXiv:2310.20707.\nGemma. 2025. Gemma 3 technical report. arXiv\npreprint arXiv:2503.19786.\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita\nBhagia, Rodney Kinney, Oyvind Tafjord, Ananya\nJha, Hamish Ivison, Ian Magnusson, Yizhong\nWang, Shane Arora, David Atkinson, Russell Au-\nthur, Khyathi Chandu, Arman Cohan, Jennifer\nDumas, Yanai Elazar, Yuling Gu, Jack Hessel,\nTushar Khot, William Merrill, Jacob Morrison,\nNiklas Muennighoff, Aakanksha Naik, Crystal\nNam, Matthew Peters, Valentina Pyatkin, Abhi-\nlasha Ravichander, Dustin Schwenk, Saurabh\nShah, William Smith, Emma Strubell, Nishant\nSubramani, Mitchell Wortsman, Pradeep Dasigi,\nNathan Lambert, Kyle Richardson, Luke Zettle-\nmoyer, Jesse Dodge, Kyle Lo, Luca Soldaini,\nNoah Smith, and Hannaneh Hajishirzi. 2024.\nOLMo: Accelerating the science of language\nmodels. In Proceedings of the 62nd Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 15789–\n15809, Bangkok, Thailand. Association for Com-\nputational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur\nMensch, Chris Bamford, Devendra Singh Chap-\nlot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile\nSaulnier, Lélio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and\nWilliam El Sayed. 2023. Mistral 7b.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand,\net al. 2024a. Mixtral of experts. arXiv preprint\narXiv:2401.04088.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lam-\nple, Lélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subrama-\nnian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Théophile Gervet, Thibaut Lavril, Thomas\nWang, Timothée Lacroix, and William El Sayed.\n2024b. Mixtral of experts.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung\nWeng, Hanyi Fang, and Peter Szolovits. 2021.\nWhat disease does this patient have?\na\nlarge-scale open domain question answering\ndataset from medical exams. Applied Sciences,\n11(14):6421.\nQiao Jin, Won Kim, Qingyu Chen, Donald C\nComeau, Lana Yeganova, W John Wilbur, and\nZhiyong Lu. 2023.\nMedcpt: Contrastive pre-\ntrained transformers with large-scale pubmed\nsearch logs for zero-shot biomedical information\nretrieval. Bioinformatics, 39(11).\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit\nIyyer, James Bradbury, Ishaan Gulrajani, Vic-\ntor Zhong, Romain Paulus, and Richard Socher.\n2016. Ask me anything: Dynamic memory net-\nworks for natural language processing. In Pro-\nceedings of The 33rd International Conference\non Machine Learning, volume 48 of Proceed-\nings of Machine Learning Research, pages 1378–\n1387, New York, New York, USA. PMLR.\n"}, {"page": 11, "text": "Yanis Labrak, Adrien Bazoge, Richard Dufour, Beat-\nrice Daille, Pierre-Antoine Gourraud, Emmanuel\nMorin, and Mickael Rouvier. 2022. FrenchMedM-\nCQA: A French multiple-choice question answer-\ning dataset for medical domain. In Proceedings\nof the 13th International Workshop on Health Text\nMining and Information Analysis (LOUHI), pages\n41–46, Abu Dhabi, United Arab Emirates (Hy-\nbrid). Association for Computational Linguistics.\nJing Li, Shangping Zhong, and Kaizhi Chen. 2021.\nMLEC-QA: A Chinese Multi-Choice Biomedical\nQuestion Answering Dataset. In Proceedings\nof the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8862–\n8874, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang,\nBochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan,\net al. 2024a. Deepseek-v3 technical report. arXiv\npreprint arXiv:2412.19437.\nYang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and\nLianwen Jin. 2024b. Datasets for large language\nmodels: A comprehensive survey. arXiv preprint\narXiv:2402.18041.\nLeland McInnes, John Healy, and James Melville.\n2020. Umap: Uniform manifold approximation\nand projection for dimension reduction.\nMinisterio\nde\nSanidad\nde\nEspaña.\n2023.\nCuadernos\nde\nexamen\n-\nformación\nsan-\nitaria\nespecializada.\nhttps://fse.\nmscbs.gob.es/fseweb/view/public/\ndatosanteriores/cuadernosExamen/\nbusquedaConvocatoria.xhtml.\nAccedido\nel 19 de octubre de 2023.\nOpenAI. 2023. Chatgpt (3.5 version). https://\nchat.openai.com/. Large language model.\nPiotr Padlewski, Max Bain, Matthew Henderson,\nZhongkai Zhu, Nishant Relan, Hai Pham, Dono-\nvan Ong, Kaloyan Aleksiev, Aitor Ormazabal,\nSamuel Phua, et al. 2024.\nVibe-eval:\nA\nhard evaluation suite for measuring progress\nof multimodal language models. arXiv preprint\narXiv:2405.02287.\nGuilherme Penedo,\nQuentin Malartic,\nDaniel\nHesslow, Ruxandra Cojocaru, Hamza Alobei-\ndli, Alessandro Cappelli, Baptiste Pannier, Ebte-\nsam Almazrouei, and Julien Launay. 2023. The\nrefinedweb dataset for falcon llm: Outperform-\ning curated corpora with web data only. In Ad-\nvances in Neural Information Processing Sys-\ntems, volume 36, pages 79155–79172. Curran\nAssociates, Inc.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon\nAlbalak, Samuel Arcadinho, Stella Biderman,\nHuanqi Cao, Xin Cheng, Michael Chung, Leon\nDerczynski, Xingjian Du, Matteo Grella, Kran-\nthi Gv, Xuzheng He, Haowen Hou, Przemyslaw\nKazienko, Jan Kocon, Jiaming Kong, Bartłomiej\nKoptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit\nMantri, Ferdinand Mom, Atsushi Saito, Guangyu\nSong, Xiangru Tang, Johan Wind, Stanisław\nWoźniak, Zhenyuan Zhang, Qinghua Zhou, Jian\nZhu, and Rui-Jie Zhu. 2023. RWKV: Reinvent-\ning RNNs for the transformer era. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 14048–14077, Singapore.\nAssociation for Computational Linguistics.\nPouya Pezeshkpour and Estevam Hruschka. 2023.\nLarge language models sensitivity to the order\nof options in multiple-choice questions.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\nJosephina Hu, Hugh Zhang, Chen Bo Calvin\nZhang, Mohamed Shaaban, John Ling, Sean\nShi, et al. 2025. Humanity’s last exam. arXiv\npreprint arXiv:2501.14249.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopy-\nrev, and Percy Liang. 2016. SQuAD: 100,000+\nquestions for machine comprehension of text. In\nProceedings of the 2016 Conference on Empir-\nical Methods in Natural Language Processing,\npages 2383–2392, Austin, Texas. Association\nfor Computational Linguistics.\nAnna Rogers, Matt Gardner, and Isabelle Augen-\nstein. 2023. Qa dataset explosion: A taxonomy\nof nlp resources for question answering and read-\ning comprehension. ACM Computing Surveys,\n55(10):1–45.\nPhilippe Schwaller, Theophile Gaudin, David Lanyi,\nCostas Bekas, and Teodoro Laino. 2017. \"found\nin translation\":\nPredicting outcomes of com-\nplex organic chemistry reactions using neural\nsequence-to-sequence models.\nThe Apache Software Foundation. 2024. Apache\nparquet documentation. Accedido el 19 de oc-\ntubre de 2024.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. Atten-\ntion is all you need.\nDavid Vilar, Markus Freitag, Colin Cherry, Ji-\naming Luo, Viresh Ratnakar, and George Foster.\n2023. Prompting PaLM for translation: Assess-\ning strategies and performance. In Proceedings\nof the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long\n"}, {"page": 12, "text": "Papers), pages 15406–15427, Toronto, Canada.\nAssociation for Computational Linguistics.\nDavid Vilares and Carlos Gómez-Rodríguez. 2019.\nHEAD-QA: A healthcare dataset for complex rea-\nsoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 960–966, Florence, Italy. Association\nfor Computational Linguistics.\nXidong Wang, Nuo Chen, Junyin Chen, Yidong\nWang, Guorui Zhen, Chunxian Zhang, Xiangbo\nWu, Yan Hu, Anningzhe Gao, Xiang Wan, et al.\n2024. Apollo: A lightweight multilingual medi-\ncal llm towards democratizing medical ai to 6b\npeople. arXiv preprint arXiv:2403.03640.\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and\nAidong Zhang. 2024. Benchmarking retrieval-\naugmented generation for medicine.\nAn Yang, Anfeng Li, Baosong Yang, Beichen\nZhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al.\n2025. Qwen3 technical report. arXiv preprint\narXiv:2505.09388.\nXinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang\nChen, Zekun Li, and Linda Ruth Petzold. 2023.\nAlpacare: Instruction-tuned large language mod-\nels for medical application.\narXiv preprint\narXiv:2310.14558.\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou,\nand Minlie Huang. 2024. Large language models\nare not robust multiple choice selectors.\nZihan Zheng, Zerui Cheng, Zeyu Shen, Shang\nZhou, Kaiyuan Liu, Hansen He, Dongruixuan\nLi, Stanley Wei, Hangyi Hao, Jianzhu Yao, et al.\n2025. Livecodebench pro: How do olympiad\nmedalists judge llms in competitive program-\nming? arXiv preprint arXiv:2506.11928.\nWenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing\nXu, Shujian Huang, Lingpeng Kong, Jiajun Chen,\nand Lei Li. 2024. Multilingual machine translation\nwith large language models: Empirical results\nand analysis. In Findings of the Association for\nComputational Linguistics: NAACL 2024, pages\n2765–2781, Mexico City, Mexico. Association for\nComputational Linguistics.\nTerry Yue Zhuo, Qiongkai Xu, Xuanli He, and Trevor\nCohn. 2023. Rethinking round-trip translation for\nmachine translation evaluation.\n"}]}