{"doc_id": "arxiv:2511.02894", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.02894.pdf", "meta": {"doc_id": "arxiv:2511.02894", "source": "arxiv", "arxiv_id": "2511.02894", "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models", "authors": ["W. K. M Mithsara", "Ning Yang", "Ahmed Imteaj", "Hussein Zangoti", "Abdur R. Shahid"], "published": "2025-11-04T15:59:10Z", "updated": "2025-11-21T15:30:31Z", "summary": "The widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart homes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and user experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks that compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often require extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This work proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates \\textit{role play} prompting, whereby the LLM assumes the role of expert to contextualize and evaluate sensor anomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on curation of extensive datasets and enable robust, adaptable defense mechanisms in real-time. We perform an extensive evaluation of the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the practicality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.02894v3", "url_pdf": "https://arxiv.org/pdf/2511.02894.pdf", "meta_path": "data/raw/arxiv/meta/2511.02894.json", "sha256": "3ef03e35085dbef12038b8eb7d2d7efdf5e22e07f07e8fec26ded99ee8c230f9", "status": "ok", "fetched_at": "2026-02-18T02:28:27.434196+00:00"}, "pages": [{"page": 1, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT\nSystems using Large Language Models\nW.K.M MITHSARA, Southern Illinois University, USA\nNING YANG, Southern Illinois University, USA\nAHMED IMTEAJ, Florida Atlantic University, USA\nHUSSEIN ZANGOTI, Jazan University, Saudi Arabia\nABDUR R. SHAHID, Southern Illinois University, USA\nThe widespread integration of wearable sensing devices in Internet of Things (IoT) ecosystems, particularly in healthcare, smart\nhomes, and industrial applications, has required robust human activity recognition (HAR) techniques to improve functionality and\nuser experience. Although machine learning models have advanced HAR, they are increasingly susceptible to data poisoning attacks\nthat compromise the data integrity and reliability of these systems. Conventional approaches to defending against such attacks often\nrequire extensive task-specific training with large, labeled datasets, which limits adaptability in dynamic IoT environments. This\nwork proposes a novel framework that uses large language models (LLMs) to perform poisoning detection and sanitization in HAR\nsystems, utilizing zero-shot, one-shot, and few-shot learning paradigms. Our approach incorporates role-play prompting, whereby\nthe LLM assumes the role of expert to contextualize and evaluate sensor anomalies and think step-by-step reasoning, guiding the\nLLM to infer poisoning indicators in the raw sensor data and plausible clean alternatives. These strategies minimize reliance on\ncuration of extensive datasets and enable robust, adaptable defense mechanisms in real time. We perform an extensive evaluation\nof the framework, quantifying detection accuracy, sanitization quality, latency, and communication cost, thus demonstrating the\npracticality and effectiveness of LLMs in improving the security and reliability of wearable IoT systems.\nCCS Concepts: ‚Ä¢ Security and privacy ‚ÜíIntrusion/anomaly detection and malware mitigation; ‚Ä¢ Information systems ‚Üí\nLanguage models.\nAdditional Key Words and Phrases: Large Language Models, Wearable IoT, Poisoning Attacks, Prompt Engineering\nACM Reference Format:\nW.K.M Mithsara, Ning Yang, Ahmed Imteaj, Hussein Zangoti, and Abdur R. Shahid. 2018. Adaptive and Robust Data Poisoning Detection\nand Sanitization in Wearable IoT Systems using Large Language Models. In Proceedings of Make sure to enter the correct conference title\nfrom your rights confirmation emai (Conference acronym ‚ÄôXX). ACM, New York, NY, USA, 30 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nWearable technology for human activity recognition (HAR) has become an integral component of Internet of Things\n(IoT) ecosystems, enabling seamless interaction between devices in applications such as fitness tracking [25], smart\nAuthors‚Äô Contact Information: W.K.M Mithsara, malithi.mithsara@siu.edu, Southern Illinois University, Carbondale, Illinois, USA; Ning Yang, nyang@siu.\nedu, Southern Illinois University, Carbondale, Illinois, USA; Ahmed Imteaj, aimteaj@fau.edu, Florida Atlantic University, Boca Raton, Florida, USA;\nHussein Zangoti, Jazan University, Jizan, Saudi Arabia, hmzangoti@jazanu.edu.sa; Abdur R. Shahid, shahid@cs.siu.edu, Southern Illinois University,\nCarbondale, Illinois, USA.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2511.02894v3  [cs.LG]  21 Nov 2025\n"}, {"page": 2, "text": "2\nMithsara et al.\nhomes [65], and healthcare monitoring [22, 23]. These systems rely on interconnected sensors to recognize daily\nactivities (e.g., walking, sitting, standing, or jogging) and provide real-time insights. To enhance accuracy for complex\ntasks, modern HAR systems often deploy multiple sensors placed strategically across the body [57]. Their integration in\nresource-constrained wearable IoT devices introduces significant challenges, including ensuring data privacy, achieving\nlow-latency processing, and reducing reliance on network connectivity. Furthermore, the interconnected nature of IoT\necosystems raises critical concerns about data integrity and reliability [1, 68]. One prominent threat is data poisoning\nattacks, in which attackers manipulate sensor data or alter data set labels before data are integrated into IoT systems\n[41, 50]. It is significant to detect data poisoning attacks; after detecting them, we need to sanitize the data because they\ncompromise the performance of HAR models and the reliability and trustworthiness of the broader IoT infrastructure.\nIn detection, we identify a data poisoning attack in the data by recognizing label flipping or feature changes, while in\nsanitization, we correct the attack to restore data integrity [8].\nTraditional methods for detecting data poisoning attacks often rely on curated datasets, making it difficult to stay\nupdated in dynamic environments. Additionally, adapting to changes in data distribution is challenging, resulting in\ndegraded model performance over time and a lack of flexibility in IoT data. For instance, some researchers focus on\nmodel-level detection, specifically backdoor detection using a support vector machine, which explores the intuition that\ndata poisoning attacks alter the model‚Äôs decision boundary by injecting mislabeled data points [67]. Others, however,\nconcentrate on data-level detection using the K-means algorithm cite wang2024poisoning. Battista et al. [4] introduced\nthe first-ever poisoning attack, a data poisoning technique targeting support vector machines (SVMs), which utilizes\ngradient ascent to modify training data and maximize SVM classification errors.\nAdditionally, some researchers propose local data training instead of centralized model training, utilizing the concept\nof federated learning [67]. Federated learning aims to enhance privacy and minimize risks associated with central data\nstorage by distributing the training process across multiple clients. However, despite its advantages, federated learning\nstill depends on extensive data training to defend against data poisoning attacks. Furthermore, this approach may\nnot be effective if malicious clients introduce adversarial local updates, potentially compromising and diverting the\naggregated global model, this is also called a model poisoning attack [15, 29, 55, 56, 64].\nRecognizing these challenges, this paper contributes to the IoT domain by comprehensively exploring the strengths\nand limitations of leveraging large language models (LLMs) for securing Wearable IoT systems. Specifically, our research\nfocuses on detecting and sanitizing data poisoning attacks using large language models (LLMs) that utilize zero-shot,\none-shot, and few-shot learning approaches. These methods eliminate the need for additional training on large datasets,\noffering a more efficient and adaptable solution for identifying and mitigating data poisoning attacks. In dynamic IoT\nenvironments, LLMs have a significant advantage through their prompt engineering [24] techniques as they enable them\nto identify new attack methods without the need for retraining or large labeled datasets, thereby saving computational\npower and time. LLMs are uniquely positioned to detect anomalies, such as those generated by data poisoning attacks,\ndue to their ability to process structured and unstructured data and contextual understanding of the relationships\nbetween data elements. Furthermore, in rapidly changing contexts, the flexibility of LLMs to adapt to new attacks offers\na more reliable solution than traditional methods.\nOur previous works explored the application of Large Language Models (LLMs) for detecting and sanitizing data\npoisoning attacks in wearable IoT systems within the context of Human Activity Recognition (HAR) [38‚Äì40]. By the\nzero-shot integrating learning capabilities of LLMs, we developed a prompt-based framework capable of classifying\nsensor data, such as accelerometer and gyroscope readings, while simultaneously identifying and correcting tampered\nactivity labels. This approach addressed significant limitations of traditional methods, such as the need for extensive\nManuscript submitted to ACM\n"}, {"page": 3, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models3\nlabeled datasets and frequent retraining, providing a scalable and adaptive solution for real-time poisoning detection\nand data sanitization in dynamic environments. Furthermore, we conducted a comprehensive evaluation using models\nsuch as ChatGPT1 and Gemini2, demonstrating the robustness, effectiveness, and practicality of LLMs in maintaining\nthe integrity and security of HAR systems in wearable IoT. However, these works have several limitations, including its\nzero-shot learning setup, which exhibits a higher level of poisoning sample detection and a lower sanitization capability.\nAgainst this backdrop, this paper advances the use of LLMs in securing HAR by introducing a framework for detecting\nand correcting data poisoning attacks in wearable IoT systems. Building on prior work that employed zero-shot learning,\nwe extend this foundation by incorporating one-shot and few-shot prompting techniques, enhancing the adaptability and\nrobustness of LLMs in dynamic and adversarial sensing environments. Unlike existing research that typically focuses on\nanomaly detection or interpreting wearable sensor data using LLM, our work presents a holistic analysis of the end-to-\nend pipeline, evaluating poisoning detection accuracy, data sanitization quality, communication cost, response latency,\nand privacy leakage. Along the way, we introduce a suite of security- and deployment-focused metrics and conduct\nrigorous benchmarking against traditional and LLM-based baselines. This system-oriented perspective, currently\nmissing in the literature, provides a foundation for the next generation of trustworthy, secure, and privacy-aware\nLLM-IoT integration in HAR systems. The contributions of this paper are highlighted as follows.\n‚Ä¢ A Security-Oriented LLM Framework for Wearable IoT. We propose a novel framework that uses LLMs to\ndetect and sanitize poisoned sensor data in wearable IoT systems, shifting the focus from anomaly detection to\ndata restoration in adversarial environments and hence, enhancing the reliability and resilience of IoT-connected\ndevices.\n‚Ä¢ Role-Playing and Step-by-Step Reasoning for Poison Detection and Sanitization. Our framework incorpo-\nrates advanced prompt engineering strategies, including role play‚Äîwhere the LLM assumes the role of a domain\nexpert‚Äîand step-by-step (chain-of-thought) reasoning. These techniques enable the LLM to contextualize sensor\nanomalies and systematically infer both the likelihood of poisoning and plausible clean alternatives for corrupted\ndata.\n‚Ä¢ Integration of One-Shot and Few-Shot Learning for Poisoning Defense. Extending beyond zero-shot\nparadigms, we incorporate one-shot and few-shot prompting techniques using GPT-3.5, GPT-4, and Gemini. This\nenhances the system‚Äôs adaptability and robustness with minimal reliance on large, labeled datasets.\n‚Ä¢ Extensive Evaluation Across Practical Metrics. We provide theoretical, as well as extensive experiments\nassessing detection accuracy, data sanitization quality, latency, and communication overhead, providing a\ncomplete view of the system‚Äôs performance and deployability.\n‚Ä¢ Benchmarking Against Traditional Methods. The study benchmarks LLM-based methods against traditional\nsupervised learning approaches, providing insights into their relative strengths, limitations, and feasibility for\nsecuring IoT systems.\nOrganization of The paper: In the remainder of this paper, related work is presented in Section 2. Section 3 presents\nthe details of the threat model. The proposed framework is described in Section 4. Section 6 discusses the evaluation\nand results. Finally, Section 7 concludes the paper.\n1https://openai.com/index/chatgpt/\n2https://gemini.google.com/\nManuscript submitted to ACM\n"}, {"page": 4, "text": "4\nMithsara et al.\n2\nRelated Work\nIn this section, we review data poisoning attacks and survey traditional defense methods for IoT sensing systems,\nemphasizing their critical limitations in dynamic and adversarial settings. We then introduce recent advances in large\nlanguage models (LLMs) and prompt engineering, highlighting state-of-the-art LLM applications for IoT sensor data.\nFinally, we clarify how our proposed research advances the intersection of IoT and LLMs by addressing key gaps in\nrobustness and adaptability under adversarial environment.\n2.1\nData Poisoning Attack and Traditional Defense Mechanisms\nThese attacks involve manipulating the training dataset by an adversary to degrade the ability of a machine learning\nmodel in predicting the outcomes accurately. For instance, an adversary can add malicious data samples, enabling\nmachine learning models to train on malicious data to affect the system‚Äôs accuracy. In addition to adding malicious\nsamples, attackers can modify data labels in specific contexts, such as human activity recognition (HAR) using sensor\ndata. For example, Perdisci et al. [45] demonstrated one of the first experiments of data poisoning in detecting computer\nworms in cybersecurity by targeting worm signatures. There is more research done on data poisoning attacks on\nfederated learning for instance, Gupta et al. [18] introduce a unique poisoning attack in the context of federated learning.\nAn attacker inverts the loss function of a model by creating malicious gradients at every SGD step to create a poisoned\noutcome. This was evaluated using MNIST, Fashion-MNIST, and CIFAR-10 datasets. Expanding on these approaches,\nSun et al. [54] introduced a federated learning-based poisoning attack that calculates gradients to poison data more\neffectively. Li et al.[31] proposes a deep reinforcement learning-based data poisoning attack in crowdsensing systems\nusing a Partially Observable Markov Decision Process. A label-flipping poisoning attack was developed by Shahid\net al. [52] on wearable sensor data in HAR and was tested on a multi-layer decision tree, perceptron, random forest,\nand XGBoost. They also introduce another type of poisoning attack based on context-aware spatiotemporal poisoning\nattacks in HAR that utilize specific patterns and conditions to alter labels effectively [50]. The authors further extend\nthe work for federated learning-based wearable AI [51].\nIn response to these advanced poisoning techniques, several defense mechanisms have been introduced, such as data\naggregation or setting the weights of parameters to help reduce the impact of the poisoned data [36] data augmentation\nor adding regularization to decision boundaries to prevent misclassification of data [49], and sanitization or cleaning\nthe data before training [9]. Zheng et al. [70] introduce a bi-level optimization-based data poisoning attack and\ncountermeasure-based defense mechanisms using Local Differential Privacy (LDP) for crowdsensing systems. Attackers\nachieve this by manipulating certain compromised workers to upload malicious sensory data. Consequently, defenders\ncan identify the most damaging data by calculating corruption probabilities and formulating optimal countermeasures\nto detect the poisoning attack. Additionally, Feng et al. [15] propose another defending method in a poisoning attack in\nprivate federated learning, and they apply the removable marks to the gradient. Here, Fang et al. [14] propose defense\nmethods such as maximizing the influence of estimation (MIE) and median of a weighted average (MWA) for enhanced\nresilience. Moreover, to counter label-flipping attacks, Andrea et al. [44] present a label sanitization approach using\nk-nearest neighbors (k-NN) to ensure consistent labels within neighborhoods.\nLimitations of Existing Research Practices: Existing defenses against poisoning attacks in wearable IoT systems\nhave notable limitations, which can be summarized as follows.\nManuscript submitted to ACM\n"}, {"page": 5, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models5\n‚Ä¢ Dependency on Curated Datasets: Traditional methods often require meticulously curated datasets such as\nUCI HAR[2], WISDM [30], PAMAP2[47], which are labor-intensive to develop and maintain. Furthermore, these\ndatasets are difficult to keep updated in dynamic, real-world environments where data evolves rapidly.\n‚Ä¢ Challenges with Changing Data Distributions: Struggle to adapt to shifts in data characteristics, leading to\nmodel drift, where model performance degrades over time. There is also need for frequent retraining to maintain\naccuracy, which is time-consuming and resource-intensive.\n‚Ä¢ Lack of Flexibility Across Diverse IoT Data Sources: Inability to generalize effectively across heterogeneous\ndata streams generated by different wearable devices. Hence, methods that perform well in one scenario may fail\nin another due to variations in data modalities and device contexts.\nIt is important to note that, while these limitations are discussed in the context of poisoning attacks in wearable sensing\nsystems, they are fundamental challenges in dynamic sensing environments more broadly. These shortcomings have\nmotivated recent interest in exploring Large Language Models (LLMs) as a means to enhance adaptability and robustness\nin wearable sensing applications.\n2.2\nLarge Language Models for Wearable IoT\nLarge language models are increasingly being applied to various tasks using sensor data in fields such as human activity\nrecognition[24] and health prediction[26]. Central to this progress is the practice of prompt engineering, which is the\ndesign of carefully crafted inputs that specify the desired tone, format, and scope, essential in zero-shot, one-shot, and\nfew-shot learning to enable models to perform tasks without extensive fine-tuning[48]. In zero-shot learning, prompts\nguide the model to generalize using only a task description, while one-shot and few-shot prompts further refine task\nunderstanding with a limited number of examples[27, 35].\nSimilar to many other IoT and related domains [6, 13, 28, 33, 46, 60], Several recent studies exemplify these advances\nin wearable sensing interpretations. Ji et al.[24] investigated whether Large Language Models (LLMs) can perform zero-\nshot human activity recognition (HAR) using raw sensor data (such as sleeping, walking, bicycling, sit-stand, moving\ndownstairs or upstairs) without prior training and using unseen data. Their proposed model, HARGPT, takes raw IMU\n(Inertial Measurement Unit) data as input and feeds it to LLMs with a Chain-of-thought (CoT) prompt. The HARGPT\nwas evaluated using two datasets, Capture24 [10], containing human motion activities, and HHAR [53], containing\ndownstairs and upstairs movement data with different levels of inter-class similarities. The findings demonstrate that\nHARGPT or LLMs can effectively interpret raw IMU data and exceed traditional classification models, such as machine\nlearning and state-of-the-art deep learning models, with high accuracy without fine-tuning. Moreover, their findings\nsuggest that LLMs are robust in handling unseen data samples, which can be challenging in conventional classification\nmodels, such as machine learning and deep learning. Similar to the HARGPT paper, which examines LLMs in zero-shot\nhuman activity recognition or sensor data, the work in [16] examines the broader potential of LLMs in wearable sensor\napplications, such as human activity recognition and health monitoring. Another LLM that can interpret raw sensor data\nis called ADL-LLM [12], which uses LLMs to recognize Activities of Daily Living (ADLs) using sensor data from smart\nhomes. The ADL-LLM can effectively recognize human activities even with few or no labeled training data. Li et al.\n[32] present SENSORLLM, a novel framework that enables Large Language Models (LLMs) to perform Human Activity\nRecognition (HAR) from multivariate wearable sensor data using a two-stage alignment and classification approach.\nHere aligns sensor data is aligned with natural language using a specially created dataset (SENSORQA) of question-\nanswer (QA) pairs. Yang et al. [63] introduce LLMTrack that leverages LLMs for zero-shot trajectory tracing using\nManuscript submitted to ACM\n"}, {"page": 6, "text": "6\nMithsara et al.\nreal-world raw sensor data or Inertial Measurement Unit (IMU). The authors combine role-playing and step-by-step\nreasoning with a minimalist prompt approach to effectively predict robot movement trajectories in indoor and outdoor\nenvironments. The experiment shows that LLMTrack can surpass traditional machine learning and state-of-the-art deep\nlearning models in accuracy and achieve over 80% F1-score in their experiment. This approach also raised concerns\nrelated to the fact that LLMs can be used maliciously for tracking purposes without fine-tuning or in zero-shot mode.\nOuyang et al. [43] present LLMSense, a system that utilizes LLMs for high-level reasoning of spatiotemporal raw sensor\ndata. The system analyzes various activities from sensor data and applies high-level reasoning to recognize complex\nevents such as a person suffering from a medical condition. LLMSense achieves over 80% accuracy and demonstrates its\neffectiveness in edge-cloud deployment. Civitarese et al. [12] introduce ADL-LLM, a Large Language Model designed\nto reason Activities of Daily Living (ADL) using sensor-based Human Activity Recognition (HAR) sensor data. The\nsystem operates in both zero-shot and few-shot settings, converting raw sensor inputs into textual representations for\nhigh-level reasoning. ADL-LLM was evaluated on real-world datasets, such as MARBLE and the UCI ADL datasets\n[3, 42]. In zero-shot testing, the model achieved a weighted average F1-score of 0.94 on MARBLE and 0.80 on UCI\nADL datasets. Under few-shot settings, it reached up to 0.96 and 0.80 on MARBLE and UCI ADL, respectively. Hu et\nal. [20] present LightLLM, a light-based sensing pre-trained LLM that specializes in light-based sensing tasks such as\nindoor localization (such as office or apartment), outdoor solar-power forecasting, and indoor solar-energy estimation.\nThe pre-trained LightLLM model can deliver more satisfactory results than state-of-the-art baselines and GPT-4. For\ninstance, LightLLM can improve localization accuracy by up to 4.4 times and enhance indoor solar estimation by 3.4\ntimes on unseen data. Xu et al. [61] propose AutoLife, a life journaling with LLMs and mobile devices. By utilizing\nlow-cost mobile device data (from accelerometer, gyroscope, barometer, GPS, WiFi), AutoLife analyzes these sensor\ndata and generates an automatic life journaling series of sentences, based on the user‚Äôs daily tasks, such as visiting a\nspecific place or sleeping. Next, AutoLife detects users‚Äô contexts and fuses them to enhance precision and generate\nfine-grained contexts. The experiment to evaluate this approach does not utilize a real-world or public dataset as\nthey claim there is no existing public dataset that covers life journaling, and it was conducted on four people across\n58 experiments. AutoLife was evaluated using the following metrics: hallucination, precision, recall, and F1 score,\nachieving up to 0.0, 0.650, 0.782, and 0.704, respectively. Zhang et al. [66] present Wi-Chat, a LLM that analyzes raw\nWi-Fi signals to determine four human activity types: walking, falling, breathing, and no-event activity. Wi-Chat can\nanalyze Channel State Information (CSI) data without traditional signal processing techniques. Wi-Chat is evaluated\nusing a self-collected Wi-Fi CSI dataset from conventional Wi-Fi devices using two million Wi-Fi CSI packets. The\nmodel was also compared with two other baseline models (Conventional Wi-Fi-based Systems and Machine Learning\nModels with Raw Signals). The experiment shows that under zero-shot settings, Wi-Chat can achieve an accuracy of\nup to 0.90 and an F1-score of 0.90 when processing Wi-Fi signals using GPT-4o-mini with Chain-of-Thought. Fiori et\nal. [17] assess how large language models (LLMs) can enrich Activities of Daily Living (ADLs) recognition in smart\nhomes by introducing LLMe2e that reasons sensor-based data. LLMe2e is a zero-shot pipeline that processes raw sensor\ndata and asks an LLM to output both the predicted Activity of Daily Living (ADL) and a plain-language rationale in a\nway that can avoid the cost of labeled data collection. The experiment is conducted on two publicly available datasets,\nthe MARBLE and the UCI ADL datasets [3, 42]. The authors selected two aspects to evaluate the model: the LLMe2e\nrecognition rate and explanation quality. The F1 weighted score for LLMe2e can reach up to 0.96 compared to another\nbaseline model with 0.94. He et al. [19] turn off-the-shelf earphones, with a stereo microphone pair and a tiny Inertial\nMeasurement Unit (IMU), into a zero-shot, fine-grained daily activity logger in a system called EmbodiedSense.\nManuscript submitted to ACM\n"}, {"page": 7, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models7\nCommon Themes in the intersection of LLM and Wearable IoT: Across the reviewed LLM-based HAR research,\nwe observe several common themes towards the goal of using LLMs to understand human behavior in the physical\nworld through wearable sensing.\n‚Ä¢ Modality Bridging: A foundational challenge addressed in nearly all papers is the transformation of raw,\nmulti-channel sensor data into representations that are compatible with LLMs.\n‚Ä¢ Zero/Few-Shot Inference and Cross-Dataset Generalization: Many proposed systems leverage the general-\nization power of pretrained LLMs to perform activity recognition without extensive retraining. This enables\nzero-shot or few-shot HAR, where models can infer activities from new domains or unseen classes using\nlanguage-based reasoning, while generalizing across distributional variations in datasets.\n‚Ä¢ Human-Like Reasoning: Several papers frame LLMs as expert agents capable of interpreting sensor patterns\nin a step-by-step, interpretable manner. This mimics the reasoning process of domain experts and enhances the\nsystem‚Äôs capacity to understand complex behavior trends.\n‚Ä¢ Interpretability: By producing textual outputs, either in the form of trend summaries, explanations, or reasoning\nchains, these models provide higher transparency and explainability compared to black-box deep learning models.\n2.3\nDifferences from the State of the Art\nWhile our work incorporates all of the aforementioned themes in the intersection of LLM and Wearable IoT, it diverges\nfrom current research trends by focusing on the security dimension of wearable sensing-based HAR systems using LLMs.\nUnlike traditional approaches that primarily detect anomalies, our work goes further by leveraging LLMs to actively\nrestore corrupted sensor data. This is a significant shift, as in conventional anomaly detection pipelines, anomalous data\nis typically discarded. In contrast, our approach aims to correct corrupted data, especially valuable in attack scenarios\nwhere only a small portion of the data stream is compromised. This perspective not only improves data utility but also\ndemonstrates the adaptive and corrective potential of LLMs in dynamic and adversarial sensing environments.\nFurthermore, unlike traditional systems built around extensive dataset curation and periodic retraining, our LLM-\ndriven framework operates with minimal dependence on large, task-specific labeled datasets. Instead, it harnesses the\nzero-shot, one-shot, and few-shot generalization capabilities of LLMs to identify, explain, and repair poisoning attacks\nin real time. This greatly reduces reliance on costly, time-consuming, and sometimes infeasible annotation efforts. As\na result, the proposed framework enables rapid deployment and continuous adaptation in environments where data\ndistributions may shift or labeled data is scarce. Ultimately, this has broader implications for enhancing the resilience\nand trustworthiness of LLM-integrated HAR systems in real-world deployments.\n3\nThreat Model\nFig. 1 illustrates the threat model, which involves poisoning the original data to manipulate the training process. This\nmodel considers two key aspects: inter-class similarities and inter-class differences. In Human Activity Recognition\n(HAR), certain activities exhibit inter-class similarities, such as ‚Äòstanding‚Äô and ‚Äòsitting‚Äô or ‚Äòwalking‚Äô and ‚Äòjogging,‚Äô where\nmotion patterns are closely related. In contrast, activities like ‚Äòwalking‚Äô and ‚Äòsitting‚Äô display inter-class differences.\nBuilding on these similarities and differences between classes, we define a threat model that assumes the presence of an\nadversary aiming to compromise the integrity of HAR systems.\nAttacker‚Äôs Goals. The adversary seeks to undermine the reliability of HAR systems by inducing misclassification\nthrough label poisoning, where such degradation can have severe consequences. The adversary‚Äôs goals can be articulated\nManuscript submitted to ACM\n"}, {"page": 8, "text": "8\nMithsara et al.\nWearable Sensors\nData gather\nRaw Data\nPre-Process\nDataset\nModel Training\nOutput\nAdversary\nPoisoned Sample \nConstruction\nPoisoning\nModel Training\nPoisoned Dataset\nOutput\nInput\nInput\nModel\nTraining\nInference\nPoisoned Model\nA\nB\nFig. 1. Overview of the Threat Model: An adversary introduces manipulated data to poison the model, compromising its perfor-\nmance and accuracy. (A): Normal flow of model training, (B): Model training after an attack with poisoned data.\nalong two key dimensions. First, in terms of degradation of model performance, the attacker aims to reduce the overall\naccuracy and reliability of the HAR model. This degradation can occur either through broad performance deterioration\nacross multiple activity classes, reducing the system‚Äôs general reliability, or through focused attacks targeting specific\nactivities, such as consistently misclassifying sitting as standing. Second, the goal of stealth and evasion is paramount, as\nthe adversary seeks to ensure that the poisoning attack remains difficult to detect during both data preprocessing and\npost-deployment monitoring. By exploiting inter-class similarities where activity boundaries are inherently ambiguous,\nthe adversary can introduce manipulations that blend seamlessly with legitimate data, evading traditional anomaly\ndetection mechanisms.\nAttack Strategies. To achieve the outlined goals, the adversary employs data poisoning attacks that manipulate the\ntraining data of Human Activity Recognition (HAR) systems. We categorize the adversary‚Äôs attack strategies into two\nprimary approaches. (Strategy 1: Targeted Label Flipping Attacks) The adversary selectively flips labels between pairs\nof activities that exhibit high feature similarity, such as sitting vs. standing or walking vs. jogging. These activities\noften have overlapping motion patterns in the feature space, making the mislabeled data difficult to distinguish from\nlegitimate samples. This focused performance degradation of the system is difficult to detect using standard anomaly\ndetection techniques due to the semantic similarities among the closely related activities. The similarity can be measured\nin various ways including in the form cosine similarity, Euclidean distance, or manually defined based on domain\nknowledge. We adapted the domain knowledge-based similarity in the experiment. (Strategy 2: Random Label Flipping\nAttack) The adversary randomly selects data points from the training dataset and flips their labels to arbitrary classes,\nwithout considering semantic or feature-space similarities between activities. For instance, sitting could be mislabeled\nas jogging, or walking could be flipped to downstairs, despite the lack of any logical correlation between these activities.\nThe primary objective of this attack is to maximize overall performance degradation by introducing significant label\nnoise, thereby disrupting the model‚Äôs ability to learn consistent patterns. While this attack is generally less stealthy\ncompared to targeted label flipping, making it more susceptible to detection, it can still be highly effective, especially\ntrue in environments where anomaly detection mechanisms are outdated, poorly tuned, or trained on data distributions\nthat differ from the current dataset, reducing their ability to identify the attack. The success of such an attack poses\nbroader implications for HAR systems, as it highlights vulnerabilities that persist even without sophisticated targeting\nstrategies.\nManuscript submitted to ACM\n"}, {"page": 9, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models9\nTraditional Defense\nProposed Defense\nDataset\n3. Poisoned/\nnot poisoned?\nSanitization\n1. Large Language Models (LLMs)\n2. Preprocessed\nPoisoned Data /\nNon-poisoned Data\nInput\n4. Cleaned Data\n5. Model Training\nOutput\nInput\nDataset\n5. Poisoned/\nnot poisoned?\nSanitization\n3. Poison Detection/Sanitizer Model\n4. Preprocessed\nPoisoned Data /\nNon-poisoned Data\nInput\n6. Cleaned Data\n7. Model Training\n1. Trusted Partial \nDataset\nOutput\nInput\n2. Model Training\nFig. 2. LLM-Driven Secure Wearable IoT: Traditional Defenses suffer from dependency on the curation of trusted datasets, limiting\ntheir adaptability and scalability in dynamic environments. We aim to address these limitations with our proposed Large Language\nModel (LLM)-based defense.\n4\nThe Proposed LLM-based Defense Framework\nMotivated by the limitations of existing defenses, this section presents the design and methodology of the proposed\nLLM-driven framework for detecting and mitigating data poisoning attacks in wearable IoT systems (Fig.2). The design\nis guided by several fundamental objectives: (i) independence from the curation of extensive trusted datasets, (ii)\nadaptability of evolving data distributions, and (iii) scalability across diverse environments. It is further grounded in\nrecent advances in LLM-based reasoning, anomaly detection [62], and prompt-based generalization.\nOur design draws inspiration from common themes observed in emerging LLM-based HAR research. First, to\naddress the challenge of modalitiy bridging, our framework converts sensor data into structured prompt templates,\npreserving spatiotemporal features (e.g., gravity, attitude, acceleration) while embedding them in domain-relevant\nnatural language contexts. Second, the framework exploits the generalization capacity of LLMs under zero-shot and\nfew-shot settings[7, 69], which allows it to reason about poisoned activity labels, even in domains or datasets it was not\nexplicitly trained on, without retraining or labeled supervision. Third, we incorporate human-like reasoning through\nstructured prompting strategies such as role-play [10] and chain-of-thought thoughts [58] to allow the LLM to perform\nmulti-step plausibility analysis when assessing label validity. Finally, our approaches provide interpretability through\nLLMs‚Äô capability to produce natural language justification for their predictions and label sanitization, which enhances\nthe transparency and allows for human-in-the-loop decision making when poisoned data is flagged[21].\nBuilding on these principles, the framework operates as a modular pipeline that integrates LLM-based detection with\nprompt-driven sanitization to ensure the integrity of the sensor data used for subsequent downstream tasks, such as\nrecognition model training, as shown in Fig.3. The process begins with the collection of raw sensor data from wearable\ndevices, such as accelerometer and gyroscope readings, which is then preprocessed for analysis by the LLM-based\ndefense system. Given the streaming nature of sensor data associated with physical activities, the data is processed in\nWindows (e.g., 100 continuous samples). Once preprocessed, the data is transmitted to a remote LLM in the form of a\nprompt. The prompt includes specific instructions along with a question formulated to identify potential poisoning in\nactivity labels, utilizing zero-shot, one-shot, and few-shot prompting techniques. Fig. 4 provides examples of one-shot\nprompt for the datasets used in the experiment. In the experiment, for zero-shot learning, the model is provided only\nManuscript submitted to ACM\n"}, {"page": 10, "text": "10\nMithsara et al.\nSelect Relevant Features \nEmbed Features & \nLabels into Template\nConstruct Prompt\nPrompt Construction\nAnalyze LLM Response \nExtract Sanitized Label \nParse & Interpret Response\nInsert Into \nDatabase\nStorage for \nDownstream Use\nDownstream \nTask (e.g., HAR \nModel Training)\nCollect Raw Sensor Data\nAssociate Reported \nActivity Label\nSegment Into Windows\nData Ingestion\nForm Sanitized Sample\nData Sanitization\nReplace with LLM-\nSuggested Label\nIs Poisoned?\nNo\nRetain \nOriginal\nYes\nWearables\nAdversary\nOnly used in one-\n/few-shot model\nPool of \nExamples\nSubmit Prompt To LLM\nReceive LLM Response \nLLM Query via Interface\nLLM: Role Playing + \nStep by Step \nReasoning\nFig. 3. Overview of the proposed LLM-based framework for detecting and sanitizing data poisoning in wearable IoT sensor data. The\nframework enables zero-/one-/few-shot poisoning detection and sanitization with interpretable reasoning and minimal supervision\nfor generating trustworthy datasets and improving the robustness of downstream activity recognition models.\n#Instruction: You are an expert in crowd sensing-based \nsmartphone-based  human activity analysis. The reported activity \nname can be flipped to carry out a poisoning attack.\n#Question:  This data contains the readings of two motion \nsensors commonly found in smartphones. Reading was \nrecorded while users executed activities scripted in no specific \norder carrying smartwatches and smartphones. Sensors: \nSensors: Two embedded sensors, i.e., Accelerometer and \nGyroscope, sampled at the highest frequency the respective \ndevice allows. \nThe person‚Äôs action belongs to one of the  following \ncategories:[WALKING, \nJOGGING, \nUPSTAIRS, \nSITTING, \nSTANDING, DOWNSTARIS] \nAttitude: roll {},pitch {},yaw {} Gravity: x {},y {},z {} Rotation: x {},y \n{},z {} Acceleration :x {}, y {},z {}\nFollowings are the Sitting data \n‚Ä¶...............\nThe following data is reported as activity is JOGGING. However, it \ncould be a poisoning attack by flipping the label. Could you please \ntell me whether the activity label was poisoned? If so, what is the \ncorrect activity? \n‚Ä¶...............\n####Response: Answer\nattitude.roll\nattitude.p\nitch\nattitude.y\naw\ngravity.x\ngravity.y\ngravity.z\nrotationR\nate.x\nrotationR\nate.y\nrotationR\nate.z\nuserAccel\neration.x\nuserAccel\neration.y\nuserAccel\neration.z Label\n0.351138\n-0.4163 -0.09802 0.314588 0.404381 -0.85878 -0.00366 0.009577 -0.00023 9.40E-05 0.000252 -0.00695Sitting\nattitude.ro\nll\nattitude.pi\ntch\nattitude.y\naw\ngravity.x\ngravity.y\ngravity.z\nrotationRa\nte.x\nrotationRa\nte.y\nrotationRa\nte.z\nuserAccel\neration.x\nuserAccel\neration.y\nuserAccel\neration.z\n0.351138\n-0.4163\n-0.09802 0.314588 0.404381\n-0.85878\n-0.00366 0.009577\n-0.00023\n9.40E-05 0.000252\n-0.00695\n#Instruction: You are an expert in crowd sensing-based \nsmartphone-based  human activity analysis. The reported activity \nname can be flipped to carry out a poisoning attack.\n#Question:  This data contains the readings of two motion \nsensors commonly found in smartphones. Reading was \nrecorded while users executed activities scripted in no specific \norder carrying smartwatches and smartphones. Sensors: \nSensors: Two embedded sensors, i.e., Accelerometer and \nGyroscope, sampled at the highest frequency the respective \ndevice allows. \nThe person‚Äôs action belongs to one of the following categories: \n[STAIRDOWN, STAIRUP, SITTING, STANDING, WALKING, \nBIKING]. \nAccelerometer: X {}, Y {}, Z {} Gyroscope: X {}, Y {}, Z {} \nFollowings are the biking data \n‚Ä¶...............\nThe following data is reported as activity is JOGGING. However, it \ncould be a poisoning attack by flipping the label. Could you please \ntell me whether the activity label was poisoned? If so, what is the \ncorrect activity? \n‚Ä¶...............\n####Response: Answer\nAcc_x\nAcc_y\nAcc_z\nGyro_X\nGyro_Y\nGyro_Z\n-5.17981 1.132004 9.469543 0.438232 -0.21811 0.191956\nAcc_x\nAcc_y\nAcc_z\nGyro_X\nGyro_Y\nGyro_Z\n-5.17981 1.132004 9.469543 0.438232 -0.21811 0.191956\n#Instruction: You are an expert in crowd sensing-based \nsmartphone-based  human activity analysis. The reported activity \nname can be flipped to carry out a poisoning attack.\n#Question:  This data contains the readings of two motion \nsensors commonly found in smartphones. Reading was \nrecorded while users executed activities scripted in no specific \norder carrying smartwatches and smartphones. Sensors: \nSensors: Two embedded sensors, i.e., Accelerometer and \nGyroscope, sampled at the highest frequency the respective \ndevice allows. \nThe person‚Äôs action belongs to one of the following categories: \n[STAIRDOWN, STAIRUP, SITTING, STANDING, WALKING, \nBIKING]. \nAccelerometer: X {}, Y {}, Z {} \nFollowings are the Sitting data \n‚Ä¶...............\nThe following data is reported as activity is JOGGING. However, it \ncould be a poisoning attack by flipping the label. Could you please \ntell me whether the activity label was poisoned? If so, what is the \ncorrect activity? \n‚Ä¶...............\n####Response: Answer\nX_Acc\nY_Acc\nZ_Acc\nLabel\n2.6\n9.66 1.035147Sitting\nX_Acc\nY_Acc\nZ_Acc\n2.6\n9.66 1.035147\nFig. 4. One-shot prompt templates for ChatGPT-3.5-turbo, ChatGPT-4.0, and Gemini on the MotionSense (left), HHAR (middle), and\nWISDM (right) datasets.\nwith a task description and the raw sensor data to classify, without any labeled examples. For one-shot learning, we pass\none example of sensor data corresponding to a specific activity, followed by new sensor data for the model to identify\nthe activity. For few-shot learning, we provide a few labeled examples for each activity class before presenting new,\nunlabeled sensor data for classification. These prompts are designed to capture each dataset‚Äôs unique features and apply\nthem to real activity data with flipped labels. The prompt is then analyzed by an LLM (e.g., GPT-3.5 turbo or GPT-4,\nGemini). The LLM‚Äôs strength in this context stems from its zero-shot and few-shot learning abilities, which enable\nit to detect poisoning in previously unseen data distributions without relying on a curated, trusted dataset. During\nanalysis, the LLM evaluates whether the data has been poisoned. If poisoning is detected, the LLM initiates a sanitization\nManuscript submitted to ACM\n"}, {"page": 11, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n11\nprocess, flagging and correcting the poisoned data. This approach is advantageous as it allows for the retention and\nsanitization of poisoned data, making it usable in model training rather than discarding it entirely. Following detection\nand sanitization by the LLMs, the framework compiles a cleaned dataset composed of sanitized and validated data. This\ndata is then fed into the model training pipeline, ensuring that the recognition model is trained on high-integrity data,\nthereby enhancing the robustness and reliability of the wearable IoT system.\nAlgorithm 1 describes the detection and sanitization of data poisoning in wearable sensor data. It begins with a\nsequence of human activity sensor data defined as D, a pool of labeled examples E for one-shot and few-shot prompting,\na prompt template P, and a remote LLM L to which prompts are submitted for inference.\nThe process begins with data ingestion and preprocessing, where D is segmented into fixed-length windows Dùëñùëñ= 1ùëö\n(e.g., 100 time steps per window). For each window Dùëñ, the algorithm selects a set of labeled examples from the pool E,\naccording to the chosen prompting strategy (zero-shot, one-shot, or few-shot). These examples, together with relevant\nsensor features from Dùëñ, are embedded into the prompt template P to generate a structured prompt Pùëñ.\nThe constructed prompt Pùëñis then submitted to the LLM L via the web interface. The LLM processes the prompt\nand returns a natural language response Rùëñ, indicating whether the reported label for Dùëñis likely poisoned. If poisoning\nis detected, the response includes a sanitized or corrected label ÀÜùë¶ùëñ, and the corrected sample (Dùëñ, ÀÜùë¶ùëñ) is appended to\nthe sanitized dataset Dsan. If no poisoning is indicated, the original sample (Dùëñ,ùë¶ùëñ) is retained in Dsan. This process\niterates over all windows, resulting in a sanitized dataset suitable for robust model training.\nAlgorithm 1: LLM-Based Detection and Sanitization of Data Poisoning in Wearable Sensor Data\nInput: D: Raw stream of human activity sensor data\nE: Pool of labeled sensor data examples for one-shot/few-shot prompting\nP: Prompt template for poisoning detection and label sanitization\nL: Remote Large Language Model accessed via web interface\nOutput: Dsan: Sanitized dataset for robust model training\nStep 1: Data Ingestion and Preprocessing\nSegment D into fixed-length windows {Dùëñ}ùëö\nùëñ=1;\nStep 2: Prompt-Based Poisoning Detection and Sanitization\nInitialize Dsan ‚Üê‚àÖ;\nforeach window Dùëñin {Dùëñ}ùëö\nùëñ=1 do\n(a) Example Selection for Prompting\nSelect zero, one, or multiple labeled examples from E based on the prompting mode (zero-/one-/few-shot);\n(b) Prompt Construction\nConstruct structured prompt Pùëñby embedding selected examples and features from Dùëñinto the template P;\nSubmit Pùëñto L and receive response Rùëñ;\nif Rùëñindicates poisoning then\nExtract sanitized label ÀÜùë¶ùëñor corrected data from Rùëñ;\nAppend (Dùëñ, ÀÜùë¶ùëñ) to Dsan;\nelse\nAppend original pair (Dùëñ, ùë¶ùëñ) to Dsan;\nend\nend\nreturn Dsan\n5\nFramework Analysis and Comparison\nWe conduct a thorough analysis of the framework‚Äôs performance across key areas critical to wearable IoT applications.\nThis assessment evaluates poisoning detection accuracy, sanitization quality, latency, communication cost, and privacy\nManuscript submitted to ACM\n"}, {"page": 12, "text": "12\nMithsara et al.\nleakage, providing quantitative insights into the framework‚Äôs efficiency and effectiveness. Finally, we provide a detailed\ncomparison of the proposed framework with other approaches along the key areas\nSensor Data\nPoison Detection\nAfter Detecting?\nAcc_X\nAcc_Y\nAcc_Z\n...\nActual\nPredicted\nLabel Sanitization\n...\n...\n...\n...\nP\nP\nTP ‚úì\n...\n...\n...\n...\nP\nN\nFN\n...\n...\n...\n...\nN\nP\nFP ‚úì\n...\n...\n...\n...\nN\nN\nTN\nTable 1. Illustration of sensor data poisoning detection (Poison as P, if not N) and label sanitization. The table presents actual and\npredicted poisoning labels, followed by the corresponding sanitization action after detection. A checkmark (‚úì) indicates that it goes\nthrough label sanitization after detecting poisoned data.\n5.1\nTheoretical Evaluation of The Framework\n5.1.1\nPoisoning Detection Accuracy: Poisoning detection accuracy serves as a crucial metric for evaluating the effective-\nness of our framework in accurately identifying instances of poisoned data within the continuous stream of wearable\nsensor inputs. This metric quantifies the system‚Äôs ability to differentiate between correctly labeled and manipulated\nactivity labels, thereby assessing the robustness of our detection mechanism. Mathematically, poisoning detection\naccuracy is defined as the proportion of poisoned data instances that are correctly identified by the framework relative\nto the total number of data instances in the dataset. Let ùëêùëÅ\nùëñ=1 represent the number of poisoned instances that the\nframework successfully detects. The total number of poisoned samples in the dataset is given by √çùëÅ\nùëñ=1(ùë•ùëñ,ùë¶‚Ä≤\nùëñ), where ùë•ùëñ\ndenotes the sensor data for the ùëñ-th instance, and ùë¶‚Ä≤\nùëñrepresents the poisoned (flipped) activity label. The total number of\nsamples both poisoned and unaltered is represented as √çùëÅ\nùëñ=1\n\u0000(ùë•ùëñ,ùë¶ùëñ) ‚à™(ùë•ùëñ,ùë¶‚Ä≤\nùëñ)\u0001, where ùë¶ùëñcorresponds to the correct\nactivity label for the ùëñ-th sample. Using these definitions, the poisoning detection accuracy can be formulated as:\nDetection Accuracy =\nùëêùëÅ\nùëñ=1 ‚äÜ\n\u0010√çùëÅ\nùëñ=1(ùë•ùëñ,ùë¶‚Ä≤\nùëñ)\n\u0011\n√çùëÅ\nùëñ=1\n\u0000(ùë•ùëñ,ùë¶ùëñ) ‚à™(ùë•ùëñ,ùë¶‚Ä≤\nùëñ)\u0001\n(1)\nIn other words, we can define the poisoning detection accuracy as follows:\nDetection Accuracy =\nùëáùëÉ+ùëáùëÅ\nùëáùëÉ+ùëáùëÅ+ ùêπùëÉ+ ùêπùëÅ\n(2)\nWhere, True Positives (TP) represents correctly identified poisoned instances, False Negative (FN) represents correctly\nidentified clean (unaltered) instances, False Positives (FP) represents clean data wrongly classified as poisoned, and\nTrue Negative (TN) represents poisoned data that the detection model missed.\n5.1.2\nSanitization Quality: Sanitization quality measures the effectiveness of the proposed framework in accurately\ncorrecting poisoned labels within a dataset. This metric is critical in evaluating how well the model distinguishes\nbetween clean and manipulated data, ensuring the integrity of the dataset used for training and inference. Unlike\ntraditional approaches that focus solely on poisoning detection, our framework extends this by assessing the Large\nLanguage Model‚Äôs (LLM) ability not only to detect poisoned data but also to correct or sanitize them effectively.\nReferring to Table 1 and Fig. 3 (right), we observe that the sanitization module processes only data that have been\ndetected as poisoned. This means that only True Positives (TP) and False Positives (FP) reach the sanitization module,\nmaking it crucial to carefully quantify the sanitization quality based on different cases.\nManuscript submitted to ACM\n"}, {"page": 13, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n13\nCase 1: True Positives (TP) ‚Äì Correctly Detected Poisoned Data. In this scenario, the framework has successfully\nidentified poisoned data. The sanitization quality in this case depends on how many of these TP instances are correctly\nsanitized. Let ùê∂ùëÜùëáùëÉrepresent the number of correctly sanitized TP instances, and TP be the total number of true\npositives. We define the Sanitization Quality for TP instances as follows.\nùëÜùëÑùëáùëÉ= ùëêùë†ùëáùëÉ\nùëáùëÉ\n(3)\nHere, ùëÜùëÑùëáùëÉrepresents the proportion of correctly sanitized poisoned samples. Clearly, a higher value of ùëÜùëÑùëáùëÉindicates\nthat the model is effectively removing adversarial effects while retaining legitimate patterns.\nCase 2: False Positives (FP) ‚Äì Incorrectly Detected Clean Data. In this case, the data was not poisoned but was\nmistakenly classified as poisoned. Here, the ideal scenario would be for the LLM to recognize the mistake and leave the\nsample unaltered. Therefore, the sanitization quality for FP instances is defined by how many of these false positives\nare left unchanged. Let ùê∂ùêπùêπùëÉdenote the number of FP instances that the LLM did not alter (i.e., left as-is). We define the\nSanitization Quality for FP instances as follows.\nùëÜùëÑùêπùëÉ= ùëêùë†ùêπùëÉ\nùêπùëÉ\n(4)\nHere, ùëÜùëÑùêπùëÉrepresents the proportion of false positive samples that were left unchanged, preserving the original clean\ndata. A higher value of ùëÜùëÑùêπùëÉindicates that the model is successfully avoiding unnecessary modifications to clean data.\nFrom these two metrics, we can express the overall sanitization quality as the ratio of total number of correctly\nprocessed samples and all samples reaching the sanitization module (TP+FP) as follows.\nùëÜùëÑ= ùëêùë†ùëáùëÉ+ ùëêùë†ùêπùëÉ\nùëáùëÉ+ ùêπùëÉ\n(5)\n5.1.3\nPortion of Data Remaining Poisoned: To quantify the fact that the data is still poisoned in the data set after\nsanitization, that is, poisoning the data set, we must consider three cases.\nCase 1: False Negatives (FN). These are poisoned data instances that were never detected as poisoned and therefore\nremain poisoned.\nCase 2: Incorrectly Sanitized True Positives (TP). The fraction of TP instances that were not properly sanitized\n1 ‚àíùê∂ùëÜùëáùëÉ\nùëáùëÉ, from which we get its contribution of the remaining poisoned data as (1 ‚àíùê∂ùëÜùëáùëÉ\nùëáùëÉ) √óùëáùëÉ.\nCase 3: Altered False Positives (FP). False positives (FP) are clean data mistakenly classified as poisoned. Ideally,\nthese should be left unchanged by the sanitization module. However, if any FP instances are altered during sanitization,\nthey effectively become poisoned. The fraction of FP instances that were modified is 1 ‚àíùê∂ùëÜùêπùëÉ\nùêπùëÉ, from which we get its\ncontribution of the remaining poisoned data as (1 ‚àíùê∂ùëÜùêπùëÉ\nùêπùëÉ) √ó ùêπùëÉ.\nFrom all these three cases, we get the total number of poisoned instances remaining in the dataset as follows.\nPoisoned Remaining = ùêπùëÅ+ [(1 ‚àíùê∂ùëÜùëáùëÉ\nùëáùëÉ) √óùëáùëÉ] + [(1 ‚àíùê∂ùëÜùêπùëÉ\nùêπùëÉ) √ó ùêπùëÉ]\n(6)\nWith (ùêπùëÅ+ùëáùëÉ) as the total poisoned data, we can express the probability of a poisoned sample from the original\ndataset remains poisoned in the final dataset, after applying poison detection and sanitization as follows.\nùëÉ(sample remains poisoned) =\nùêπùëÅ+ [(1 ‚àíùê∂ùëÜùëáùëÉ\nùëáùëÉ) √óùëáùëÉ] + [(1 ‚àíùê∂ùëÜùêπùëÉ\nùêπùëÉ) √ó ùêπùëÉ]\nùêπùëÅ+ùëáùëÉ\n(7)\nA lower probability indicates a more effective defense mechanism, where most poisoned data has been successfully\ndetected and sanitized. Conversely, a higher probability suggests that a significant amount of poisoned data persists,\nhighlighting weaknesses in either detection or sanitization effectiveness.\nManuscript submitted to ACM\n"}, {"page": 14, "text": "14\nMithsara et al.\n5.1.4\nCommunication Cost: In the proposed framework, the communication cost primarily depends on the amount\nof data transferred between the data ingestion module and the LLM through the interface, as depicted in Fig. 3. Each\ninteraction involves sending a prompt with sensor data and receiving a response on data poisoning and potential\nsanitization, quantified by the number of characters in both prompt and response.\nLet ùëÅbe the total number of data samples in the sensor data stream, ùëäthe fixed window size (number of samples\nper window), ùëö= ùëÅ\nùëäthe total number of windows, ùëêùëùthe number of characters in each prompt generated from a data\nwindow, and ùëêùëüthe number of characters in the response from the LLM. The communication cost of sending each prompt\nto the LLM can be approximated by ùëÇ(ùëêùëù). Given that we have ùëöwindows, the total cost of transmitting prompts to the\nLLM is ùëÇ(ùëö.ùëêùëù) = ùëÇ( ùëÅ\nùëä.ùëêùëù). After processing each prompt, the LLM returns a response of ùëêùëücharacters, which provides\nan assessment of poisoning and, if applicable, sanitized version for the given data window. The communication cost of\nreceving each response is therefore ùëÇ(ùëêùëü). With ùëöwindows, the total cost of receiving response is ùëÇ(ùëö.ùëêùëü) = ùëÇ( ùëõ\nùëä.ùëêùëü).\nFinally, the total communication cost, which includes both the prompt transmission and response retrieval, can be\nrepresented as follows.\nùëÇ(ùëö.ùëêùëù+ ùëö.ùëêùëü) = ùëÇ( ùëÅ\nùëä(ùëêùëù+ ùëêùëü))\n(8)\n5.1.5\nResponse Time: Like communication cost, latency arises from delays during LLM interactions via web interface.\nTotal latency is influenced by data segmentation, prompt transmission, LLM processing for poisoning detection, and\nresponse retrieval. Based on our communication cost analysis, transmission and response retrieval latency are ùëÇ(ùëö.ùëêùëù)\nand ùëÇ(ùëö.ùëêùëü), respectively. With ùë°ùëùas the LLM‚Äôs processing time per window, the total latency can be expressed as\nfollows.\nùëÇ(ùëö.(1 + ùëêùëù+ ùë°ùëù+ ùëêùëü)) = ùëÇ( ùëÅ\nùëä.(1 + ùëêùëù+ ùë°ùëù+ ùëêùëü))\n(9)\n5.1.6\nPrivacy: In our approach, privacy risks arise primarily from the communication with the Large Language Model\n(LLM) through the web interface, where sensitive data is transmitted for processing. With each interaction with the\nLLM, there is a probability ùëùthat data could be intercepted. Therefore, the probability of data exposure increases with\nthe number of interactions is as follows.\nùëÉùëôùëíùëéùëò= 1 ‚àí(1 ‚àíùëù)ùëö\n(10)\nThus, privacy risks grow with the number of interactions ùëö, indicating a trade-off between communication frequency\nand data privacy in the proposed approach.\n5.1.7\nInterpretability: A key advantage of the proposed framework is its inherent interpretability, derived from the\nnatural language reasoning capabilities of the LLM. Unlike conventional machine learning models that operate as black\nboxes, the LLM generates human-readable explanations when evaluating sensor-label consistency. For each instance,\nthe model produces a justification for its decision, explicitly stating whether the reported label aligns with the observed\nsensor data and explaining the rationale. For example, when analyzing a potentially poisoned activity label, the LLMs\nin our experiment follow an step-by-step analysis which includes (1) summarizing the expected sensor data patterns for\neach activity based on established domain knowledge; (2) inspecting and describing the actual sensor data values for\nthe queried instance; (3) comparing the empirical sensor readings to the canonical patterns of all possible activities;\nand (4) providing a transparent, natural language explanation that makes its reasoning and conclusion explicit. These\nstructured explanations not only support transparency and auditability of poisoning detection outcomes but also enable\nhuman analysts to verify the model‚Äôs behavior.\nManuscript submitted to ACM\n"}, {"page": 15, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n15\n5.2\nComparative Analysis\nWe compare our proposed API-based LLM approach with two traditional machine learning models: on-premise models\nand API-based cloud models. The main difference between on-premises and API-based Traditional Models is the\ndeployment location of the model. The comparison is structured across several key criteria that highlight the strengths\nand limitations of each approach, as shown in the Table. 2.\nOur proposed API-based LLM framework eliminates the dependency on curated, trusted datasets for poisoning\ndetection and sanitization, making it advantageous in environments with limited labeled data. Due to LLMs ‚Äô inherent\ngeneralization capabilities, the framework also demonstrates high adaptability, handling data shifts without frequent\nretraining. In contrast, traditional on-premise and cloud-based models generally exhibit low adaptability, requiring\nperiodic retraining to adjust to changing data distributions. Additionally, our approach offers flexibility across diverse\nIoT ecosystems, with a scalable design that accommodates various IoT contexts and device types. However, the frequent\nAPI calls required for poisoning detection and sanitization result in higher communication costs compared to on-premise\nmodels, which incur minimal communication costs by processing data locally. Regarding latency, our framework and\ncloud-based models experience moderate to high latency, as they depend on network speed and API response time.\nIn contrast, on-premise models offer the lowest latency, making them ideal for time-sensitive applications. Finally, in\nterms of privacy, our framework presents greater risks than on-premise models due to the data sent to third-party LLM\nAPIs, where inherent privacy risks are higher.\nIn terms of interpretability, our LLM-based framework provides a distinct advantage by generating natural language\njustifications for its poisoning assessments on the sensor data, thereby offering direct insight into the reasoning behind\neach decision. This stands in contrast to traditional models, which typically function as black boxes and require\nunreliable post hoc explanation tools to interpret their outputs.\nTable 2. Comparison of our proposed LLM-based approach with different approaches.\nApproach\nRequires\nTrusted\nDataset\nAdaptability to\nEvolving Data\nDistributions\nFlexibility in Diverse\nIoT Ecosystems\nCommunication\nCost\nResponse\nTime Delay\nPrivacy\nInterpretability\nOur Proposed\nLLM-based\nApproach\nNo\nHigh; adaptable\nwithout retraining\n(zero-/one-/few-shot\nlearning)\nHigh; scalable across\nvarious contexts and\ndevice types\nHigh (due to frequent\ncalls to the LLM)\nModerate to\nHigh\nLow\nHigh;\ngenerates\nhuman-readable\njustifications\nOn-Premise\nTraditional\nModels\n[9, 49, 50, 52]\nYes\nLow; requires\nretraining to adapt to\ndata shifts\nLow; requires\nretraining\nLow\nLow\nHigh\nLow; black-box mod-\nels with limited ex-\nplainability\nAPI-Based\nTraditional\nModels\n[9, 49, 50, 52]\nYes\nLow; requires\nretraining to adapt to\ndata shifts\nLow; requires\nretraining\nHigh (due to frequent\nAPI calls)\nModerate to\nHigh\nLow\nLow; limited to post\nhoc\ninterpretation\ntools\n*The main difference between On-Premise and API-based Traditional Models is the deployment location of the model.\n5.2.1\nTakeaway. While our proposed remote LLM-based framework demonstrates substantial advantages in adaptability,\nscalability, interpretability, and reduced dependence on curated datasets, these benefits are accompanied by trade-offs.\nReliance on remote proprietary LLMs, such as GPT or Gemini, introduces communication overhead, increased response\nlatency, and privacy risks associated with transmitting sensitive sensor data to third-party servers.\nTo address these challenges, several research directions merit further exploration. These include developing tech-\nniques for compressing or summarizing sensor data before transmission, which can reduce communication overhead;\nManuscript submitted to ACM\n"}, {"page": 16, "text": "16\nMithsara et al.\ninvestigating the on-device or edge deployment of lightweight LLMs, including open-source and small language mod-\nels [37], or leveraging hybrid architectures that balance local inference with cloud-based reasoning to minimize latency\nand communication costs; and integrating privacy-enhancing technologies, such as local differential privacy [11], into\nsensor data processing to safeguard user information. Furthermore, it is important to systematically explore the trade-\noffs between model complexity and resource consumption, particularly for resource-constrained edge deployments in\na federated learning setting. Advancing these directions will require rigorous evaluation of robustness, adaptability,\nand performance of LLMs in diverse and dynamic IoT environments, ensuring reliable and efficient human activity\nrecognition under real-world conditions.\n6\nExperiment and Evaluation\n6.1\nExperimental Setup\n6.1.1\nDatasets: In this study, we employed three datasets: the MotionSense Dataset [34], the Heterogeneity Activity\nRecognition (HHAR) dataset [5], and Wireless Sensor Data Mining (WISDM)[59]. The MotionSense dataset, collected\nvia an iPhone 6s placed in participants‚Äô front pockets, includes six activities: walking, jogging, stairs-up, stairs-down,\nsitting, and standing. It captures measurements of orientation in three-dimensional space (roll, pitch, yaw), gravity,\nrotation, and acceleration across the x, y, and z axes. The HHAR dataset, gathered from four smartwatches and eight\nsmartphones, records six activities‚Äîbiking, sitting, standing, walking, stairsup, and stairsdown‚Äîcapturing accelerometer\nand gyroscope data along the x, y, and z axes. The WISDM data was collected from mobile and wearable devices. The\ndataset includes labeled activities such as walking, jogging, sitting, standing, going upstairs, and going downstairs,\ncaptured from 36 participants. Each record contains information such as the user ID, activity label, timestamp, and\naccelerometer readings along the x, y, and z axes.\n6.2\nPoisoning Attack Strategy\n6.2.1\nPoisoning Attack Strategy for Large Language Models (LLMs): We randomly select continuous segments of 100\nsensor data samples from the MotionSense and Heterogeneity Human Activity Recognition (HHAR) and 500 samples\nfrom the WISDM datasets to compare the dataset diversity. Each of these segments undergoes label-flipping poisoning\nattacks for various activities to simulate poisoning attacks, following the approach of Sijie et al. [24] for human activity\ndetection via zero-shot learning. Only poisoned samples (true positives) are provided to the models throughout the\nexperiments. Hence, our experiment does not cover the cases of false positives and true negatives. We adopted the\nformulas for poison detection accuracy and sanitization quality, as expressed in equations 2 and 6, accordingly. In\nzero-shot learning, no poisoned example was provided. For the one-shot setting, a single example of each activity\nestablishes a reference before introducing poisoned samples. In contrast, the few-shot setting includes a few examples\nper activity category to help the model recognize patterns before detecting poisoned data. Our analysis evaluates\ninter-class similarities and differences for label poisoning detection and sanitization across various activities, employing\na one-shot prompt template (Fig. 4) in models such as ChatGPT-3.5, ChatGPT-4, and Gemini.\n6.2.2\nPoisoning Attack Strategy for Traditional Methods: Prior studies on poisoning attacks primarily address label\nsanitization using traditional models like k-Nearest Neighbors (k-NN) [52], focusing more on sanitization than attack\ndetection. This work extends the analysis by comparing conventional methods with Large Language Models (LLMs) for\nefficiency in sanitization, measured by time and communication costs (i.e., training sample requirements). We use k-NN,\nLong Short-Term Memory (LSTM) networks, and TinyBert as baselines, each trained on 40,915 samples per activity\nManuscript submitted to ACM\n"}, {"page": 17, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n17\nfrom the MotionSense, HHAR, and WISDM datasets. Testing involves 100-sample segments with poisoned labels from\nthe MotionSense and HHAR datasets, and 500 samples from the WISDM dataset, to evaluate classification accuracy\nunder attack.\n6.3\nEvaluation Metrics\nTo evaluate the efficacy of ChatGPT-3.5, ChatGPT-4, and Gemini, we employ various metrics, including detection\naccuracy, sanitization quality, response time or latency, and communication costs. Since our focus is on assessing\nthe ability of LLMs to detect and sanitize poisoned data, our experiments are conducted exclusively on poisoned\nsamples. Accordingly, we adapt the definitions of poison detection accuracy and sanitization quality to align with\nour experimental setup. The metrics used throughout the evaluation are as follows. (Poison Detection Accuracy)\nThis metric is defined as the ratio of correctly identified poisoned samples to the total number of poisoned samples.\n(Sanitization Quality) Given that we evaluate only poisoned data, sanitization quality is measured as the ratio\nof correctly sanitized samples to the total number of correctly detected poisoned samples. (Communication Cost)\nCommunication cost is assessed by counting the number of characters in the responses generated by each LLM under\nzero-shot, one-shot, and few-shot scenarios. (Response Time) The response time is measured using the Python ‚Äòtime‚Äô\nlibrary, which marks the start at the beginning of processing and the end at the conclusion, followed by calculating the\nelapsed time in seconds. We corroborate these measurements by manually timing several samples with a stopwatch. We\nmaintain a consistent networking environment and conduct tests at various times, including mornings around 8:00 AM,\nafternoons around noon, and evenings around 6:00 PM. The results show that the response times are approximately\nthe same across different time intervals for all LLMs, with times recorded after evaluating various samples. (Recall)\nA metric that measures a model‚Äôs ability to correctly predict positive instances, in this case, poisoned data, out of all\nactual positive instances. We used this metric only for the traditional models. Due to our experimental setup, in the\ncase of LLM, it is similar to poisoning detection accuracy.\nZero-shot\nOne-shot\nFew-shot\nMethod\n0.9\n0.92\n0.94\n0.96\n0.98\n1.0\nAccuracy\nChatGPT-3.5\nChatGPT-4\nGemini\na. MotionSense Dataset\nZero-shot\nOne-shot\nFew-shot\nMethod\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\nChatGPT-3.5\nChatGPT-4\nGemini\nb. HHAR Dataset\nZero-shot\nOne-shot\nFew-shot\nMethod\n0.970\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\nAccuracy\nChatGPT-3.5\nChatGPT-4\nGemini\nc. WISDM Dataset\nFig. 5. Poison Detection Accuracy Comparison of Zero-shot, One-shot, and Few-shot Methods for ChatGPT-3.5, ChatGPT-4, and\nGemini on MotionSense, HHAR, and WISDM Datasets.\n6.4\nExperiment with LLMs\nWe evaluate ChatGPT-3.5, ChatGPT-4, and Gemini in their capability of detecting poisoning samples and sanitizing\nthem in zero-shot, one-shot, and few-shot learning settings. Fig. 5, 6, 7, and 8 present the accuracy, sanitization quality,\nresponse time, and communication cost among the LLMs.\nManuscript submitted to ACM\n"}, {"page": 18, "text": "18\nMithsara et al.\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nSanitization Quality\nZero-shot\nOne-shot\nFew-shot\nMethod\nChatGPT-3.5\nChatGPT-4\nGemini\na. MotionSense Dataset\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nSanitization Quality\nZero-shot\nOne-shot\nFew-shot\nMethod\nChatGPT-3.5\nChatGPT-4\nGemini\nb. HHAR Dataset\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\nSanitization Quality\nZero-shot\nOne-shot\nFew-shot\nMethod\nChatGPT-3.5\nChatGPT-4\nGemini\nc. WISDM Dataset\nFig. 6. Sanitization Quality Comparison of Zero-shot, One-shot, and Few-shot Methods for ChatGPT-3.5, ChatGPT-4, and Gemini on\nMotionSense, HHAR, and WISDM Datasets.\nZero-shot\nOne-shot\nFew-shot\nMethod\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\nAvg. Resp. Time (s)\nChatGPT-3.5\nChatGPT-4\nGemini\na. MotionSense Dataset\nZero-shot\nOne-shot\nFew-shot\nMethod\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\nAvg. Resp. Time (s)\nChatGPT-3.5\nChatGPT-4\nGemini\nb. HHAR Dataset\nZero-shot\nOne-shot\nFew-shot\nMethod\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nAvg. Resp. Time (s)\nChatGPT-3.5\nChatGPT-4\nGemini\nc. WISDM Dataset\nFig. 7. Average Response Time Comparison of Zero-shot, One-shot, and Few-shot Methods for ChatGPT-3.5, ChatGPT-4, and Gemini\nacross MotionSense, HHAR, and WISDM Datasets.\nZero-shot\nOne-shot\nFew-shot\nMethod\nChatGPT-3.5\nChatGPT-4\nGemini\n3321\n3050\n2563\n3121\n2879\n2622\n3011\n2089\n2003\n2200\n2400\n2600\n2800\n3000\n3200\nCommunication Cost\na. MotionSense Dataset\nZero-shot\nOne-shot\nFew-shot\nMethod\nChatGPT-3.5\nChatGPT-4\nGemini\n2564\n2879\n2622\n3086\n3063\n1754\n2838\n2818\n960\n1000\n1250\n1500\n1750\n2000\n2250\n2500\n2750\n3000\nCommunication Cost\nb. HHAR Dataset\nZero-shot\nOne-shot\nFew-shot\nMethod\nChatGPT-3.5\nChatGPT-4\nGemini\n369\n990\n2633\n413\n4082\n4083\n1845\n707\n615\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nCommunication Cost\nc. WISDM Dataset\nFig. 8. Communication Cost Comparison of Zero-shot, One-shot, and Few-shot Methods for ChatGPT-3.5, ChatGPT-4, and Gemini\non MotionSense, HHAR, and WISDM Datasets.\n6.4.1\nZero-shot setting: We provided 100 consecutive, randomly selected poisoned samples from the MotionSense\nand HHAR datasets for each activity in the zero-shot setting. Additionally, we conducted an experiment using 500\ndata points from the WISDM dataset. The final results of this experiment are presented in Fig. 5, 6, 7, and 8. This\nexperiment builds upon our previous works [38‚Äì40], where we focused exclusively on zero-shot learning based on\ninter-class similarities and differences for all datasets. Tables 3, 4, 5, 6, 7, 8 present the results on zero-shot learning. In\nthis study, we compared the accuracy and sanitization quality across each LLM to evaluate how effectively they identify\ndata poisoning attacks using zero-shot learning. As indicated in the given tables, ChatGPT-3.5 produced incorrect\nManuscript submitted to ACM\n"}, {"page": 19, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n19\nresults most of the time. So we expanded our study to include one-shot and few-shot learning settings to assess further\nthe performance of Large Language Models (LLMs), which helps to evaluate improvements in the LLMs‚Äô abilities to\ndetect and sanitize data poisoning attacks, enabling us to compare how LLMs perform when given examples rather\nthan relying solely on zero-shot learning. As shown in Fig. 5 and 6, ChatGPT-4 demonstrates higher accuracy and\nSanitization Quality than both ChatGPT-3.5 and Gemini across the two datasets when considering overall performance\nin the zero-shot setting. ChatGPT-3.5 incurs a lower communication cost for the HHAR dataset and WISDM dataset as\nin Fig.8b, and 8a, although it shows a higher cost for the MotionSense dataset(Fig. 8a). ChatGPT-4 displays a consistent\ncommunication cost across MotionSense and HHAR datasets, but a lower communication cost for the WISDM dataset,\nwhile Gemini shows approximately similar costs. The response times follow the same pattern as the communication\ncosts, as shown in Fig. 7a and Fig. 7b, depending on the volume of data processed in each response. However, for the\nWISDM dataset it shows a lower response time across all models. All experiments were conducted in the same lab\nenvironment, and the response times reported here represent average and approximate values.\n6.4.2\nOne-shot setting: In one-shot learning, a single sample is provided to familiarize the model with the activity before\nasking it to detect a poisoning attack. As in Fig. 5 and Fig. 6, we observe that ChatGPT-3.5 achieves an accuracy of 1 and\na Sanitization Quality of 0.97, demonstrating its effectiveness in detecting poisoning attacks within the MotionSense\ndataset. However, it sometimes fails to identify a poisoning attack, particularly when the actual label is ‚ÄòStanding‚Äô and\nthe poisoned label is ‚ÄòSitting‚Äô, as shown in Table 9. In the HHAR dataset, ChatGPT-3.5 struggles to identify the ‚ÄòStairsup‚Äô\nactivity when the poisoned label is ‚ÄòStanding‚Äô, suggesting a correct label of ‚ÄòStairsup or Stairsdown‚Äô. Additionally, when\nthe actual label is ‚ÄòSitting‚Äô and the poisoned label is ‚ÄòStairsup‚Äô, it suggests ‚ÄòSitting or Standing‚Äô as the correct label, as\nin Table 12. But in the WISDM dataset, ChatGPT-3.5 can correctly classify all the labels. In contrast, both ChatGPT-4\nand Gemini achieve perfect accuracy and Sanitization Quality scores of 1.0 across both datasets, as in Fig. 6. When\ncomparing communication costs and response times across all LLMs as in Fig. 7 and Fig. 8, Chat-GPT-3.5 shows a higher\ncommunication cost than the other models for the MotionSense dataset but a lower communication cost for the HHAR\ndataset and WISDM. ChatGPT-4 exhibits a higher communication cost for all datasets, while Gemini shows a lower\ncommunication cost for WISDM and a higher cost for the HHAR dataset, as indicated in Fig. 8. It is worth noting that\nwe did not provide any specific instructions to the LLMs regarding the control of response length or the number of\nwords generated for each prompt. We plan to explore this aspect in future work.\nRegarding response times, Gemini demonstrates lower latency, while GPT-3.5 exhibits the highest response time\non the MotionSense dataset. However, for the WISDM dataset, the response times for GPT-3.5 and GPT-4 are quite\nsimilar. Overall, response time is influenced by the amount of text produced, which is closely related to the number of\ncharacters generated in each response. In the one-shot setting, both ChatGPT-4 and Gemini successfully detect and\nsanitize the poisoned labels across all datasets, effectively handling inter-class similarities and differences.\n6.4.3\nFew-shot setting: In few-shot learning, we send three sensor examples corresponding to each activity, followed\nby sensor data from a poisoned activity to detect and sanitize the instance. Table 17 conclusively shows that the\ndetection and sanitization of poisoning attacks in both the MotionSense and HHAR and WISDM datasets achieve a\nscore of 1.0. This indicates that all LLMs (ChatGPT-3.5, ChatGPT-4, and Gemini) are capable of accurately identifying\ndata poisoning attacks across activities, considering inter-class similarities and differences, related to the outcomes in\none-shot learning. Moreover, few-shot learning exhibits lower communication costs and response times compared to\nzero-shot and one-shot learning for the MotionSense and HHAR datasets, as detailed in Fig. 7 and Fig. 8. However, for\nManuscript submitted to ACM\n"}, {"page": 20, "text": "20\nMithsara et al.\nTable 3. Comparison of Detection and Label Sanitization Among ChatGPT 3.5/4 and Gemini for Inter-Class Similarities in MotionSense\nDataset in Zero-shot learning\nActual Label\nPoisoned Label\nGPT 3.5\nGPT 4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nSitting\nYes\nStanding\nYes\nStanding\nYes\nStanding\nSitting\nStanding\nYes\nSitting/Lying Down\nYes\nSitting\nYes\nDownstairs/Upstairs\nUpstairs\nDownstairs\nNo\nN/A\nYes\nWalking\nYes\nJogging/Walking\nDownstairs\nUpstairs\nYes\nWalking\nYes\nDownstairs\nYes\nDownstairs\nUpstairs\nJogging\nYes\nWalking\nYes\nUpstairs\nNo\nN/A\nDownstairs\nJogging\nYes\nWalking\nYes\nDownstairs\nYes\nDownstairs/Upstairs\nJogging\nUpstairs\nYes\nDownstairs\nYes\nJogging\nYes\nWalking/Standing\nJogging\nDownstairs\nYes\nWalking/Jogging\nYes\nJogging\nYes\nWalking/Standing\nJogging\nWalking\nYes\nDownstairs\nYes\nJogging\nYes\nStanding\nWalking\nJogging\nYes\nWalking\nYes\nWalking\nYes\nWalking/Standing\nWalking\nUpstairs\nNo\nN/A\nYes\nWalking\nYes\nWalking/Standing\nUpstairs\nWalking\nYes\nJogging\nYes\nUpstairs\nYes\nJogging\nWalking\nDownstairs\nNo\nN/A\nYes\nWalking\nYes\nWalking/Standing\nDownstairs\nWalking\nYes\nJogging\nYes\nDownstairs\nYes\nDownstairs/Upstairs\nTable 4. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for Inter-Class Difference in the MotionSense\nDataset in Zero-shot learning\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nWalking\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nJogging\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nUpstairs\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nDownstairs\nYes\nStanding\nYes\nStanding\nYes\nStanding\nWalking\nStanding\nYes\nDownstairs\nYes\nWalking\nNo\nN/A\nJogging\nStanding\nYes\nWalking/Jogging\nYes\nJogging\nNo\nN/A\nUpstairs\nStanding\nYes\nJogging\nYes\nUpstairs\nYes\nJogging/Walking\nDownstairs\nStanding\nYes\nWalking\nYes\nDownstairs\nYes\nDownstairs/Upstairs\nSitting\nWalking\nYes\nSitting/Standing\nYes\nSitting\nYes\nDownstairs/Upstairs\nSitting\nJogging\nYes\nStanding\nYes\nSitting\nYes\nSitting\nSitting\nUpstairs\nYes\nSitting/Standing\nYes\nSitting\nYes\nWalking or Jogging\nSitting\nDownstairs\nYes\nSitting/Standing\nYes\nSitting\nYes\nJogging/Walking\nWalking\nSitting\nYes\nUpstairs\nYes\nWalking\nYes\nWalking/Standing\nJogging\nSitting\nYes\nWalking/Jogging\nYes\nJogging\nYes\nWalking/Standing\nUpstairs\nSitting\nYes\nJogging\nYes\nUpstairs\nYes\nJogging\nDownstairs\nSitting\nYes\nWalking\nYes\nDownstairs\nYes\nDownstairs/Upstairs\nthe WISDM dataset, it shows higher communication costs for ChatGPT-3.5 and ChatGPT-4, as illustrated in Fig. 8c.\nAdditionally, the response time is higher for Gemini on the WISDM dataset, as shown in Fig. 7c.\n6.5\nExperiment with Traditional Methods\nWe compare our proposed approach with KNN and LSTM, and TinyBert models on the same datasets (MotionSense\nand HHAR, and WISDM). This analysis includes 40,915 samples per activity, totaling 245,490 samples from all datasets.\nFor the KNN model, we employ Scikit-learn‚Äôs GridSearchCV, testing odd values from 1 to 29 for the number of nearest\nneighbors to select the optimal parameter, with a 5-fold cross-validation (cv=5) for tuning. In the LSTM model, we\nimplement one LSTM layer and one hidden layer using PyTorch, training the model over 100 epochs. The TinyBert\nmodel has 4 transformer layers and 312-dimensional hidden layers, and 12 attention heads; the total model is 14.5 million.\nIn this evaluation, we compute accuracy and Sanitization Quality for each activity, enabling a comparative analysis of\nManuscript submitted to ACM\n"}, {"page": 21, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n21\nTable 5. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for Inter-Class Similarities in HHAR Dataset\nin Zero-shot learning\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nSitting\nNo\nN/A\nNo\nN/A\nYes\nStairsdown\nSitting\nStanding\nNo\nN/A\nYes\nSitting\nNo\nN/A\nStairsup\nStairsdown\nYes\nSitting\nYes\nStairsup\nYes\nSitting/Standing\nStairsdown\nStairsup\nYes\nWalking\nYes\nStairsdown\nYes\nStairsdown\nStairsup\nBiking\nYes\nStanding\nYes\nStanding\nYes\nSitting/Standing\nStairsdown\nBiking\nYes\nWalking\nYes\nStairsdown\nYes\nStairsdown/Upstairs\nBiking\nStairsup\nYes\nStanding\nYes\nBiking\nYes\nStairsdown\nBiking\nStairsdown\nYes\nStanding\nYes\nBiking\nYes\nStanding\nBiking\nWalking\nYes\nStairsdown\nYes\nBiking\nYes\nStanding\nWalking\nBiking\nNo\nN/A\nYes\nWalking\nYes\nStanding\nWalking\nStairsup\nYes\nStanding\nYes\nWalking\nNo\nN/A\nStairsup\nWalking\nNo\nN/A\nYes\nStairsup\nYes\nStairsup\nWalking\nStairsdown\nYes\nSitting or Standing\nYes\nWalking or Stairsup\nYes\nStanding\nStairsdown\nWalking\nYes\nStairsdown\nYes\nStairsdown\nYes\nStairsdown\nTable 6. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for Inter-Class Difference in the HHAR\nDataset in Zero-shot learning\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nWalking\nYes\nStanding\nYes\nStanding\nYes\nStairsdown\nStanding\nBiking\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nStairsup\nYes\nStanding\nYes\nStanding\nYes\nStairsdown\nStanding\nStairsdown\nYes\nSitting\nYes\nStanding\nYes\nStanding\nWalking\nStanding\nYes\nStairsup/Stairsdown\nYes\nWalking\nYes\nStairsup\nBiking\nStanding\nNo\nN/A\nYes\nBiking\nYes\nStairsdown\nStairsup\nStanding\nYes\nStairsup\nYes\nStairsup\nYes\nStairsup\nStairsdown\nStanding\nYes\nStairsup\nYes\nStairsdown\nYes\nStairsdown\nSitting\nWalking\nYes\nStanding\nYes\nSitting\nYes\nStanding\nSitting\nBiking\nYes\nSitting/Standing\nYes\nSitting\nYes\nSitting\nSitting\nStairsup\nYes\nSitting/Standing\nYes\nSitting\nYes\nStanding\nSitting\nStairsdown\nYes\nSitting\nYes\nSitting\nYes\nStanding\nWalking\nSitting\nYes\nWalking\nYes\nWalking\nYes\nStanding\nBiking\nSitting\nYes\nWalking\nYes\nBiking\nYes\nStanding\nStairsup\nSitting\nYes\nStanding\nYes\nStairsup\nYes\nSitting/Standing\nStairsdown\nSitting\nYes\nWalking\nYes\nStairsdown\nYes\nStanding\nsanitization accuracy across KNN, LSTM, TinyBert, and LLMs. Accuracy and Sanitization Quality are calculated based\non both true and poisoned labels.\nThe KNN models take approximately 11,010 seconds to train on the MotionSense dataset and 1,866 seconds on the\nHHAR dataset. The testing dataset is the same as LLMs 100 data, and the time is around 0.01 or 0.02 seconds for all\nmodels. The LSTM model requires about 1,713 seconds to train on the MotionSense dataset and 1,990 seconds on the\nHHAR dataset. These times are calculated approximately. The TinyBERT model takes around 438 seconds for training\nand 0.04 seconds for testing on the HHAR dataset, approximately 500 seconds for training and 0.02 seconds for testing\non the MotionSense dataset, and 300 seconds for training and 0.02 seconds for testing on the WISDM dataset.\nAs shown in Table 16 and Fig. 9, 10 and 11, the KNN model consistently achieves high accuracy in identifying\nall activities across both the MotionSense and HHAR datasets, with only a slight decrease (0.99 probability) for the\n‚ÄòWalking‚Äô activity in the WISDM dataset. In comparison, the LSTM model demonstrates strong performance for specific\nactivities, such as ‚ÄòStanding‚Äô, ‚ÄòSitting‚Äô, and ‚ÄòUpstairs‚Äô in the MotionSense dataset, but is less effective in sanitizing\nlabels for ‚ÄòDownstairs‚Äô, ‚ÄòWalking‚Äô, and ‚ÄòJogging‚Äô. This trend is also observed in the HHAR dataset, where the LSTM\naccurately classifies ‚ÄòStanding‚Äô, ‚ÄòSitting‚Äô, and ‚ÄòBiking‚Äô, but has difficulty with ‚ÄòStairsdown‚Äô, ‚ÄòStairsup‚Äô, and ‚ÄòWalking‚Äô.\nManuscript submitted to ACM\n"}, {"page": 22, "text": "22\nMithsara et al.\nTable 7. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for Inter-Class Similarities in WISDM Dataset\nin Zero-shot learning\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nSitting\nYes\nWalking\nYes\nStanding\nYes\nJogging\nSitting\nStanding\nYes\nWalking\nYes\nSitting or Standing\nYes\nSitting\nWalking\nJogging\nYes\nSitting\nYes\nWalking or Jogging\nYes\nUpstairs\nWalking\nDownstairs\nYes\nUpstairs\nYes\nWalking\nYes\nUpstairs\nWalking\nUpstairs\nYes\nDownstairs\nYes\nWalking\nYes\nDownstairs\nJogging\nWalking\nYes\nStanding\nYes\nJogging\nYes\nSitting or Standing\nJogging\nDownstairs\nYes\nUpstairs\nYes\nJogging\nNo\nN/A\nJogging\nUpstairs\nYes\nDownstairs\nYes\nJogging\nYes\nJogging\nDownstairs\nUpstairs\nYes\nDownstairs\nYes\nDownstairs\nYes\nJogging\nDownstairs\nJogging\nYes\nStanding\nYes\nDownstairs\nYes\nWalking\nDownstairs\nWalking\nYes\nSitting\nYes\nDownstairs\nYes\nJogging\nUpstairs\nDownstairs\nYes\nUpstairs\nYes\nUpstairs or Downstairs\nYes\nUpstairs\nUpstairs\nJogging\nYes\nStanding\nYes\nUpstairs\nYes\nWalking\nUpstairs\nWalking\nYes\nStanding\nYes\nUpstairs\nYes\nJogging\nTable 8. Comparison of Detection and Label Sanitization in ChatGPT-3.5, ChatGPT-4, and Gemini for Inter-Class Difference in\nWISDM Dataset in Zero-shot Learning\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nDownstairs\nYes\nUpstairs\nYes\nStanding\nYes\nWalking\nStanding\nUpstairs\nYes\nDownstairs\nYes\nStanding\nYes\nJogging\nStanding\nWalking\nNo\nN/A\nYes\nStanding\nYes\nUpstairs\nStanding\nJogging\nYes\nWalking\nYes\nStanding\nYes\nWalking\nSitting\nDownstairs\nYes\nUpstairs\nYes\nSitting\nYes\nStanding\nSitting\nUpstairs\nYes\nDownstairs\nYes\nSitting\nYes\nSitting\nSitting\nWalking\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nJogging\nYes\nSitting\nYes\nSitting\nYes\nStanding\nWalking\nStanding\nYes\nWalking\nYes\nJogging\nYes\nJogging\nWalking\nSitting\nYes\nJogging\nYes\nWalking\nYes\nJogging\nJogging\nStanding\nYes\nSitting\nYes\nJogging\nYes\nJogging\nJogging\nSitting\nYes\nStanding\nYes\nJogging\nYes\nSitting or Standing\nDownstairs\nStanding\nYes\nSitting\nYes\nDownstairs\nYes\nUpstairs\nDownstairs\nSitting\nYes\nJogging\nYes\nDownstairs\nYes\nJogging\nUpstairs\nStanding\nYes\nDownstairs\nYes\nUpstairs\nYes\nJogging\nUpstairs\nSitting\nYes\nJogging\nYes\nUpstairs\nYes\nJogging\nTable 9. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for Inter-Class Similarities in motionsense\nDataset in One-shot learning\nActual Label\nPoisoned Label\nChatGPT 3.5\nChatGPT 4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nSitting\nNo\nN/A\nYes\nStanding\nYes\nStanding\nSitting\nStanding\nYes\nSitting\nYes\nSitting\nYes\nSitting\nUpstairs\nDownstairs\nYes\nUpstairs\nYes\nWalking\nYes\nWalking\nDownstairs\nUpstairs\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nUpstairs\nJogging\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nDownstairs\nJogging\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nJogging\nUpstairs\nYes\nJogging\nYes\nJogging\nYes\nJogging\nJogging\nDownstairs\nYes\nJogging\nYes\nJogging\nYes\nJogging\nJogging\nWalking\nYes\nDownstairs\nYes\nJogging\nYes\nJogging\nWalking\nJogging\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nUpstairs\nYes\nWalking\nYes\nWalking\nYes\nWalking\nUpstairs\nWalking\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nWalking\nDownstairs\nYes\nWalking\nYes\nWalking\nYes\nWalking\nDownstairs\nWalking\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nFor the WISDM dataset, the LSTM correctly identifies ‚ÄòStanding‚Äô, ‚ÄòSitting‚Äô, ‚ÄòWalking‚Äô, and ‚ÄòJogging‚Äô, yet struggles with\n‚ÄòUpstairs‚Äô and ‚ÄòDownstairs‚Äô. TinyBERT is achieves reliable detection for ‚ÄòStanding‚Äô and ‚ÄòSitting‚Äô in the HHAR dataset\nManuscript submitted to ACM\n"}, {"page": 23, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n23\nTable 10. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for inter-class difference in motionsense\ndataset One-shot learning\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nWalking\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nJogging\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nUpstairs\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nDownstairs\nYes\nStanding\nYes\nStanding\nYes\nStanding\nWalking\nStanding\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nSitting\nYes\nWalking\nYes\nWalking\nYes\nWalking\nJogging\nStanding\nYes\nJogging\nYes\nJogging\nYes\nJogging\nJogging\nSitting\nYes\nJogging\nYes\nJogging\nYes\nJogging\nUpstairs\nStanding\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nUpstairs\nSitting\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nDownstairs\nSitting\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nDownstairs\nStanding\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nSitting\nWalking\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nJogging\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nUpstairs\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nDownstairs\nYes\nSitting\nYes\nSitting\nYes\nSitting\nTable 11. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for inter-class similarities in the HHAR\ndataset in one-shot learning.\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nSitting\nYes\nStanding\nYes\nStanding\nYes\nStanding\nSitting\nStanding\nYes\nSitting\nYes\nSitting\nYes\nSitting\nStairsup\nStairsdown\nYes\nStairsup\nYes\nStairsup\nYes\nStairsup\nStairsup\nBiking\nYes\nStairsup\nYes\nStairsup\nYes\nStairsup\nStairsup\nWalking\nYes\nStairsup\nYes\nStairsup\nYes\nStairsup\nStairsdown\nStairsup\nYes\nStairsdown\nYes\nStairsdown\nYes\nStairsdown\nStairsdown\nWalking\nYes\nStairsdown\nYes\nStairsdown\nYes\nStairsdown\nStairsdown\nBiking\nYes\nStairsdown\nYes\nStairsdown\nYes\nStairsdown\nBiking\nStairsup\nYes\nBiking\nYes\nBiking\nYes\nBiking\nBiking\nStairsdown\nYes\nBiking\nYes\nBiking\nYes\nBiking\nBiking\nWalking\nYes\nBiking\nYes\nBiking\nYes\nBiking\nWalking\nBiking\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nStairsup\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nStairsdown\nYes\nWalking\nYes\nWalking\nYes\nWalking\nTable 12. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for inter-class difference in the HHAR\ndataset in one-shot learning.\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nWalking\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nBiking\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nStairsup\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nStairsdown\nYes\nStanding\nYes\nStanding\nYes\nStanding\nWalking\nStanding\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nSitting\nYes\nWalking\nYes\nWalking\nYes\nWalking\nBiking\nStanding\nYes\nBiking\nYes\nBiking\nYes\nBiking\nBiking\nSitting\nYes\nBiking\nYes\nBiking\nYes\nBiking\nStairsup\nStanding\nYes\nStairsup/ stairsdown\nYes\nStairsup\nYes\nStairsup\nStairsup\nSitting\nYes\nStairsup\nYes\nStairsup\nYes\nStairsup\nStairsdown\nStanding\nYes\nStairsdown\nYes\nStairsdown\nYes\nStairsdown\nStairsdown\nSitting\nYes\nStairsdown\nYes\nStairsdown\nYes\nStairsdown\nSitting\nWalking\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nBiking\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nStairsup\nYes\nSitting/Standing\nYes\nSitting\nYes\nSitting\nSitting\nStairsdown\nYes\nSitting\nYes\nSitting\nYes\nSitting\nbut faces challenges in classifying activities such as ‚ÄòUpstairs‚Äô and ‚ÄòWalking‚Äô. Overall, the KNN model demonstrates\nsuperior performance compared to both the LSTM and TinyBERT models across all evaluated datasets.\nManuscript submitted to ACM\n"}, {"page": 24, "text": "24\nMithsara et al.\nTable 13. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for inter-class similarities in the WISDM\ndataset using one-shot learning.\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nSitting\nYes\nStanding\nYes\nStanding\nYes\nStanding\nSitting\nStanding\nYes\nSitting\nYes\nSitting\nYes\nSitting\nWalking\nJogging\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nDownstairs\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nUpstairs\nYes\nWalking\nYes\nWalking\nYes\nWalking\nJogging\nWalking\nYes\nJogging\nYes\nJogging\nYes\nJogging\nJogging\nDownstairs\nYes\nJogging\nYes\nJogging\nYes\nJogging\nJogging\nUpstairs\nYes\nJogging\nYes\nJogging\nYes\nJogging\nDownstairs\nUpstairs\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nDownstairs\nJogging\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nDownstairs\nWalking\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nUpstairs\nDownstairs\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nUpstairs\nJogging\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nUpstairs\nWalking\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nTable 14. Comparison of Detection and Label Sanitization in ChatGPT-3.5/4 and Gemini for inter-class difference in the WISDM\ndataset using One-shot learning.\nActual Label\nPoisoned Label\nChatGPT-3.5\nChatGPT-4\nGemini\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nDetection\nLabel Sanitization\nStanding\nDownstairs\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nUpstairs\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nWalking\nYes\nStanding\nYes\nStanding\nYes\nStanding\nStanding\nJogging\nYes\nStanding\nYes\nStanding\nYes\nStanding\nSitting\nDownstairs\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nUpstairs\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nWalking\nYes\nSitting\nYes\nSitting\nYes\nSitting\nSitting\nJogging\nYes\nSitting\nYes\nSitting\nYes\nSitting\nWalking\nStanding\nYes\nWalking\nYes\nWalking\nYes\nWalking\nWalking\nSitting\nYes\nWalking\nYes\nWalking\nYes\nWalking\nJogging\nStanding\nYes\nJogging\nYes\nJogging\nYes\nJogging\nJogging\nSitting\nYes\nJogging\nYes\nJogging\nYes\nJogging\nDownstairs\nStanding\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nDownstairs\nSitting\nYes\nDownstairs\nYes\nDownstairs\nYes\nDownstairs\nUpstairs\nStanding\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nUpstairs\nSitting\nYes\nUpstairs\nYes\nUpstairs\nYes\nUpstairs\nTable 15. Comparison of Poisoning Detection Accuracy (Acc.) and Sanitization Quality (SQ) of LLMs for MotionSense, HHAR, and\nWISDM Datasets in Zero-shot, One-shot, and Few-shot Learning.\nModel\nMotionSense\nHHAR\nWISDM\nZero-shot\nOne-shot\nFew-shot\nZero-shot\nOne-shot\nFew-shot\nZero-shot\nOne-shot\nFew-shot\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nAcc.\nSQ\nGPT 3.5\n0.90\n0.20\n0.97\n1.00\n1.00\n1.00\n0.83\n0.28\n1.00\n0.93\n1.00\n1.00\n0.97\n0.13\n1.00\n1.00\n1.00\n1.00\nGPT 4\n1.00\n0.97\n1.00\n1.00\n1.00\n1.00\n0.97\n0.90\n1.00\n1.00\n1.00\n1.00\n1.00\n0.83\n1.00\n1.00\n1.00\n1.00\nGemini\n0.90\n0.26\n1.00\n1.00\n1.00\n1.00\n0.93\n0.32\n1.00\n1.00\n1.00\n1.00\n0.97\n0.16\n1.00\n1.00\n1.00\n1.00\n6.6\nComparison of LLMs with Traditional Methods\nAs shown in Fig. 5, Fig. 6, and Table 15, ChatGPT-4 demonstrates high accuracy and sanitization quality in few-shot\nlearning, establishing it as the most effective model for detecting and sanitizing data poisoning attacks when compared\nto ChatGPT-3.5 and Gemini. We further compare ChatGPT-4 with traditional methods based on sanitization quality,\ncommunication cost, and response time, as presented in Table 17.\nWhen comparing LLMs and traditional methods in terms of sanitization quality, which, in our experiments, is\nmeasured by recall for traditional models, the results indicate that KNN achieves a perfect sanitization quality of 1 on\nthe MotionSense dataset and the HHAR dataset. In contrast, the sanitization quality of the LSTM model is lower than\nManuscript submitted to ACM\n"}, {"page": 25, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n25\nTable 16. Comparison of Poisoning Detection Accuracy (Acc.) and Recall (Rec.) for KNN, LSTM, and TinyBERT Models across\nMotionSense, HHAR, and WISDM datasets in One-shot Learning\nModel\nMetric\nMotionSense\nHHAR\nWISDM\nStand\nSit\nUp\nDown\nJog\nWalk\nStand\nSit\nUp\nDown\nBike\nWalk\nStand\nSit\nUp\nDown\nJog\nWalk\nKNN\nAcc.\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\nKNN\nRec.\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.50\n1.00\n0.33\n1.00\n0.47\n0.50\nLSTM\nAcc.\n1.00\n1.00\n0.93\n0.98\n0.93\n0.90\n1.00\n1.00\n0.73\n0.51\n1.00\n0.27\n1.00\n1.00\n0.90\n0.60\n1.00\n1.00\nLSTM\nRec.\n1.00\n1.00\n0.00\n0.98\n0.98\n0.98\n1.00\n1.00\n0.24\n0.13\n1.00\n0.09\n1.00\n1.00\n0.45\n0.30\n1.00\n1.00\nTinyBERT\nAcc.\n0.99\n0.69\n0.15\n0.97\n0.62\n0.24\n1.00\n1.00\n0.76\n0.53\n0.95\n0.32\n0.89\n0.99\n0.15\n0.70\n0.70\n0.58\nTinyBERT\nRec.\n0.50\n1.00\n0.04\n0.24\n0.15\n0.06\n1.00\n1.00\n0.16\n0.13\n1.00\n0.06\n0.89\n0.99\n0.15\n0.70\n0.70\n0.58\nTable 17. Performance Comparison of LLM-Based and Traditional Methods on MotionSense, HHAR, and WISDM Datasets.\nModel\nMotionSense\nHHAR\nWISDM\nSanit.Q.\nComm.\nCost\nResp.\nTime\nSanit.Q.\nComm.\nCost\nResp.\nTime\nSanit.Q.\nComm.\nCost\nResp. Time\nGPT-4-Zero\n0.97\n3121\n25\n0.83\n3086\n25\n0.83\n3100\n24\nGPT-4-One\n1.00\n2879\n25\n1.00\n3063\n23\n1.00\n2800\n22\nGPT-4-Few\n1.00\n2622\n21\n1.00\n1754\n13\n1.00\n1700\n12\nKNN\n1.00\n0.00\n0.01\n1.00\n0.00\n0.01\n0.63\n0.00\n0.01\nLSTM\n0.83\n0.00\n0.01\n0.74\n0.00\n0.01\n0.80\n0.00\n0.01\nTinyBERT\n0.59\n0.00\n0.01\n0.63\n0.00\n0.01\n0.62\n0.00\n0.01\nthe KNN model. TinyBERT also achieves lower accuracy and sanitization quality across all datasets. Among the LLMs,\nChatGPT-4 consistently exhibits the highest sanitization quality across all datasets.\nTraditional methods have zero communication costs, as they do not process text data and operate locally. Additionally,\ntheir response times are lower, since only the model testing response time is considered. In contrast, ChatGPT-4\nincurs higher communication costs and response times across all datasets. Notably, few-shot learning achieves lower\ncommunication costs and response times on the MotionSense and HHAR datasets, even though it requires sample data\nbecause it generates shorter responses compared to zero-shot and one-shot learning. However, for the WISDM dataset,\nChatGPT-4 exhibits higher communication costs for both one-shot and few-shot scenarios.\nOverall, the results clearly show that ChatGPT-4 outperforms traditional methods across all datasets. Further-\nmore, LLMs such as ChatGPT-4 provide flexible interfaces for natural language queries and explanations, supporting\ntransparent decision-making and ease of integration in real-world applications.\n7\nConclusion\nTraditional AI-based solutions for dealing with poisoning attacks in Internet of Things (IoT) systems, including wearable,\nsensor-based Human Activity Recognition (HAR), are limited by their reliance on curated, labeled datasets and lack\nadaptability to evolving data environments. In this paper, we present our research on harnessing the adaptability of\nLLMs to address these two critical limitations. Our proposed framework harnesses the capabilities of zero-shot, one-shot,\nand few-shot learning, enabling effective poisoning detection and sanitization without extensive retraining or additional\ndata requirements. Through comprehensive evaluation, we demonstrate the framework‚Äôs adaptability, accuracy, and\nefficiency in poison detection and sanitizing poisoned data. We critically analyze the system‚Äôs performance in key areas,\nincluding communication efficiency, latency, and privacy in design. Our findings underscore the potential of LLMs as\npowerful, scalable solutions for securing wearable IoT systems in dynamic, data-driven environments.\nImportantly, while our focus is on HAR, the diversity and complexity encompassed by the three datasets utilized in the\nexperiment, MotionSense, HHAR, and WISDM, support the potential generalizability of our framework to broader IoT\ndomains. These datasets collectively introduce considerable diversity in user demographics, device types, and sensing\nManuscript submitted to ACM\n"}, {"page": 26, "text": "26\nMithsara et al.\nStanding\nSitting\nJogging\nWalking\nDownstairs\nUpstairs\nFig. 9. Confusion matrices of Human Activities using KNN (top row) and LSTM (Middle row), and TinyBert (Bottom row) on the\nMotionsense dataset. The activity labels are visually distinguished by colored boxes.\nStanding\nSitting\nBiking\nWalking\nStairsdown\nStairsup\nFig. 10. Confusion matrices of Human Activities using KNN (top row) and LSTM (Middle row), and TinyBert (Bottom row) on the\nHHAR dataset. The activity labels are visually distinguished by colored boxes.\nenvironments. The MotionSense dataset includes inertial sensor data (accelerometer, gyroscope, gravity, and attitude)\nsampled at 50 Hz from 24 participants performing six activities in a controlled setting. Crucially, the participants vary\nwidely in weight (48‚Äì102 kg), height (161‚Äì190 cm), age (18‚Äì46 years), and gender (13 male, 11 female), introducing\nintra-class variability that reflects user heterogeneity common in real-world IoT deployments. The HHAR dataset\nManuscript submitted to ACM\n"}, {"page": 27, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n27\nStanding\nSitting\nJogging\nWalking\nDownstairs\nUpstairs\nFig. 11. Confusion matrices of Human Activities using KNN (top row) and LSTM (Middle row), and TinyBert (Bottom row) on the\nWISDM dataset. The activity labels are visually distinguished by colored boxes.\nintroduces substantial device diversity, with data collected from 4 smartwatches (2 LG watches, 2 Samsung Galaxy\nGears) and 8 smartphones (2 Samsung Galaxy S3 mini, 2 Samsung Galaxy S3, 2 LG Nexus 4, 2 Samsung Galaxy S+),\nacross 9 users performing six daily activities. These devices differ in sampling rates (typically 50‚Äì200Hz), orientations,\nand internal sensor calibrations, mimicking deployment challenges in heterogeneous IoT ecosystems. The WISDM\ndataset, gathered from 36 users, contains accelerometer data sampled at 20 Hz during semi-naturalistic usage of Android\nsmartphones, with activities such as walking, jogging, and stair navigation. Together, the datasets span a wide range of\nenvironmental conditions (lab-controlled, semi-natural, and real-world), device types, and user demographics. This\ndiversity positions our framework as a robust solution for addressing key challenges in real-world IoT deployments,\nsuch as sensor and user variability, device heterogeneity, and environmental complexity.\nFuture work will aim to refine the framework‚Äôs performance in real-time applications, enhance privacy measures,\nand expand its application across diverse IoT ecosystems to solidify LLMs as a core component in the defense against\ndata poisoning in wearable IoT. We will explore the trade-off between the performance of the LLM in dealing with\ndifferent attacks on IoT systems and critical system performance metrics. Our proposed research makes several key\ncontributions to the broader landscape of IoT. Specifically, it explores the potential of utilizing Large Language Models\n(LLMs) in the domain of IoT security, highlighting their role in enhancing the reliability and resilience of IoT-connected\ndevices. Our work demonstrates the adaptability of LLMs in dynamic IoT environments while reducing the dependence\non large, labeled datasets, making IoT security more efficient and scalable.\nReferences\n[1] Muhammad Adil, Muhammad Khurram Khan, Neeraj Kumar, Muhammad Attique, Ahmed Farouk, Mohsen Guizani, and Zhanpeng Jin. 2024.\nHealthcare Internet of Things: Security threats, challenges, and future research directions. IEEE Internet of Things Journal 11, 11 (2024), 19046‚Äì19069.\n[2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge L. Reyes-Ortiz. 2013. A Public Domain Dataset for Human Activity Recognition\nUsing Smartphones. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\nManuscript submitted to ACM\n"}, {"page": 28, "text": "28\nMithsara et al.\n[3] Luca Arrotta, Claudio Bettini, and Gabriele Civitarese. 2022. The MARBLE Dataset: Multi-inhabitant Activities of Daily Living Combining Wearable\nand Environmental Sensors Data. In Mobile and Ubiquitous Systems: Computing, Networking and Services, Takahiro Hara and Hirozumi Yamaguchi\n(Eds.). Springer International Publishing, Cham, 451‚Äì468.\n[4] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning attacks against support vector machines. In Proceedings of the 29th International\nCoference on International Conference on Machine Learning (Edinburgh, Scotland) (ICML‚Äô12). Omnipress, Madison, WI, USA, 1467‚Äì1474.\n[5] Blunck, Bhattacharya Henrik, Sourav, Kjrgaard Prentow, Thor, Mikkel, Dey, and Anind. 2015. Heterogeneity Activity Recognition. UCI Machine\nLearning Repository. DOI: https://doi.org/10.24432/C5689X.\n[6] Gordon Owusu Boateng, Hani Sami, Ahmed Alagha, Hanae Elmekki, Ahmad Hammoud, Rabeb Mizouni, Azzam Mourad, Hadi Otrok, Jamal\nBentahar, Sami Muhaidat, et al. 2025. A survey on large language models for communication, network, and service management: Application\ninsights, challenges, and future directions. IEEE Communications Surveys & Tutorials (2025).\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877‚Äì1901.\n[8] Tarek Chaalan, Shaoning Pang, Joarder Kamruzzaman, Iqbal Gondal, and Xuyun Zhang. 2024. The Path to Defence: A Roadmap to Characterising\nData Poisoning Attacks on Victim Models. ACM Comput. Surv. 56, 7, Article 175 (April 2024), 39 pages. https://doi.org/10.1145/3627536\n[9] Patrick PK Chan, Zhimin He, Xian Hu, Eric CC Tsang, Daniel S Yeung, and Wing WY Ng. 2021. Causative label flip attack detection with data\ncomplexity measures. International Journal of Machine Learning and Cybernetics 12 (2021), 103‚Äì116.\n[10] Shing Chan, Yuan Hang, Catherine Tong, Aidan Acquah, Abram Schonfeldt, Jonathan Gershuny, and Aiden Doherty. 2024. CAPTURE-24: A\nlarge dataset of wrist-worn activity tracker data collected in the wild for human activity recognition. Scientific Data 11, 1 (16 Oct 2024), 1135.\nhttps://doi.org/10.1038/s41597-024-03960-3\n[11] Woo-Seok Choi, Matthew Tomei, Jose Rodrigo Sanchez Vicarte, Pavan Kumar Hanumolu, and Rakesh Kumar. 2018. Guaranteeing local differential\nprivacy on ultra-low-power systems. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE, IEEE, Los\nAlamitos, CA, USA, 561‚Äì574.\n[12] Gabriele Civitarese, Michele Fiori, Priyankar Choudhary, and Claudio Bettini. 2025. Large Language Models Are Zero-Shot Recognizers for Activities\nof Daily Living. ACM Trans. Intell. Syst. Technol. 16, 4, Article 78 (June 2025), 32 pages. https://doi.org/10.1145/3725856\n[13] Hongwei Cui, Yuyang Du, Qun Yang, Yulin Shao, and Soung Chang Liew. 2025. LLMind: Orchestrating AI and IoT with LLM for Complex Task\nExecution. IEEE Communications Magazine 63, 4 (2025), 214‚Äì220. https://doi.org/10.1109/MCOM.002.2400106\n[14] Jiaxin Fan, Qi Yan, Mohan Li, Guanqun Qu, and Yang Xiao. 2022. A Survey on Data Poisoning Attacks and Defenses. In 2022 7th IEEE International\nConference on Data Science in Cyberspace (DSC). IEEE, Los Alamitos, CA, USA, 48‚Äì55. https://doi.org/10.1109/DSC55868.2022.00014\n[15] Xia Feng, Wenhao Cheng, Chunjie Cao, Liangmin Wang, and Victor S Sheng. 2024. DPFLA: Defending Private Federated Learning Against Poisoning\nAttacks. IEEE Transactions on Services Computing 17, 4 (2024), 1480‚Äì1491.\n[16] Emilio Ferrara. 2024. Large Language Models for Wearable Sensor-Based Human Activity Recognition, Health Monitoring, and Behavioral Modeling:\nA Survey of Early Trends, Datasets, and Challenges. Sensors 24, 15 (2024), 5045.\n[17] Michele Fiori, Gabriele Civitarese, Priyankar Choudhary, and Claudio Bettini. 2025. Leveraging Large Language Models for Explainable Activity\nRecognition in Smart Homes: A Critical Evaluation. ACM Trans. Internet Things (Sept. 2025). https://doi.org/10.1145/3766900 Just Accepted.\n[18] Prajjwal Gupta, Krishna Yadav, Brij B. Gupta, Mamoun Alazab, and Thippa Reddy Gadekallu. 2023. A Novel Data Poisoning Attack in Federated\nLearning based on Inverted Loss Function. Computers & Security 130 (2023), 103270. https://doi.org/10.1016/j.cose.2023.103270\n[19] Lixing He, Bufang Yang, Di Duan, Zhenyu Yan, and Guoliang Xing. 2025. EmbodiedSense: Understanding Embodied Activities with Earphones.\narXiv preprint arXiv:2504.02624 abs/2504.02624 (2025).\n[20] Jiawei Hu, Hong Jia, Mahbub Hassan, Lina Yao, Brano Kusy, and Wen Hu. 2025. Lightllm: A versatile large language model for predictive light\nsensing. In Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems. Association for Computing Machinery, New York, NY,\nUSA, 158‚Äì171.\n[21] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, et al. 2024. A\nsurvey of safety and trustworthiness of large language models through the lens of verification and validation. Artificial Intelligence Review 57, 7\n(2024), 175.\n[22] Rajan Jha, Pratik Mishra, and Santosh Kumar. 2024. Advancements in optical fiber-based wearable sensors for smart health monitoring. Biosensors\nand Bioelectronics 254 (2024), 116232.\n[23] Geonwoo Ji, Jiyoung Woo, Geon Lee, Constantino Msigwa, Denis Bernard, and Jaeseok Yun. 2024. AIoT-Based Smart Healthcare in Everyday\nLives: Data Collection and Standardization From Smartphones and Smartwatches. IEEE Internet of Things Journal 11, 16 (2024), 27597‚Äì27619.\nhttps://doi.org/10.1109/JIOT.2024.3400509\n[24] Sijie Ji, Xinzhe Zheng, and Chenshu Wu. 2024. HARGPT: Are LLMs Zero-Shot Human Activity Recognizers? arXiv preprint arXiv:2403.02727 (2024),\n38‚Äì43.\n[25] Kanitthika Kaewkannate and Soochan Kim. 2016. A comparison of wearable fitness devices. BMC public health 16 (2016), 1‚Äì16.\n[26] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. Health-LLM: Large Language Models for Health Prediction via\nWearable Sensor Data. In Proceedings of the fifth Conference on Health, Inference, and Learning (Proceedings of Machine Learning Research, Vol. 248),\nTom Pollard, Edward Choi, Pankhuri Singhal, Michael Hughes, Elena Sizikova, Bobak Mortazavi, Irene Chen, Fei Wang, Tasmie Sarker, Matthew\nMcDermott, and Marzyeh Ghassemi (Eds.). PMLR, 522‚Äì539. https://proceedings.mlr.press/v248/kim24b.html\nManuscript submitted to ACM\n"}, {"page": 29, "text": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models\n29\n[27] James R. Kirk, Robert E. Wray, Peter Lindes, and John E. Laird. 2024. Improving knowledge extraction from LLMs for task learning through agent\nanalysis. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of\nArtificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI‚Äô24/IAAI‚Äô24/EAAI‚Äô24). AAAI Press, Article\n2051, 9 pages. https://doi.org/10.1609/aaai.v38i16.29799\n[28] Ibrahim Kok, Orhan Demirci, and Suat Ozdemir. 2024. When IoT Meet LLMs: Applications and Challenges . In 2024 IEEE International Conference on\nBig Data (BigData). IEEE Computer Society, Los Alamitos, CA, USA, 7075‚Äì7084. https://doi.org/10.1109/BigData62323.2024.10825187\n[29] ≈Åukasz Korycki and Bartosz Krawczyk. 2023. Adversarial concept drift detection under poisoning attacks for robust data stream mining. Machine\nLearning 112, 10 (2023), 4013‚Äì4048.\n[30] Jennifer R Kwapisz, Gary M Weiss, and Samuel A Moore. 2011. Activity recognition using cell phone accelerometers. ACM SigKDD Explorations\nNewsletter 12, 2 (2011), 74‚Äì82.\n[31] Mohan Li, Yanbin Sun, Hui Lu, Sabita Maharjan, and Zhihong Tian. 2019. Deep reinforcement learning for partially observable data poisoning\nattack in crowdsensing systems. IEEE Internet of Things Journal 7, 7 (2019), 6266‚Äì6278.\n[32] Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, and Flora D Salim. 2024. Sensorllm: Aligning large language models with motion sensors for\nhuman activity recognition. arXiv preprint arXiv:2410.10624 (2024).\n[33] Kaiwei Liu, Bufang Yang, Lilin Xu, Yunqi Guo, Guoliang Xing, Xian Shuai, Xiaozhe Ren, Xin Jiang, and Zhenyu Yan. 2025. TaskSense: A Translation-like\nApproach for Tasking Heterogeneous Sensor Systems with LLMs. Association for Computing Machinery, New York, NY, USA, 213‚Äì225.\nhttps:\n//doi.org/10.1145/3715014.3722070\n[34] Mohammad Malekzadeh, Richard G. Clegg, Andrea Cavallaro, and Hamed Haddadi. 2018. Protecting Sensory Data against Sensitive Inferences. In\nProceedings of the 1st Workshop on Privacy by Design in Distributed Systems (Porto, Portugal) (W-P2DS‚Äô18). Association for Computing Machinery,\nNew York, NY, USA, Article 2, 6 pages. https://doi.org/10.1145/3195258.3195260\n[35] Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. 2024. Prompt Engineering in Large Language Models. In\nData Intelligence and Cognitive Informatics, I. Jeena Jacob, Selwyn Piramuthu, and Przemyslaw Falkowski-Gilski (Eds.). Springer Nature Singapore,\nSingapore, 387‚Äì402.\n[36] Chenglin Miao, Qi Li, Lu Su, Mengdi Huai, Wenjun Jiang, and Jing Gao. 2018. Attack under Disguise: An Intelligent Data Poisoning Attack\nMechanism in Crowdsourcing. In Proceedings of the 2018 World Wide Web Conference (Lyon, France) (WWW ‚Äô18). International World Wide Web\nConferences Steering Committee, Republic and Canton of Geneva, CHE, 13‚Äì22. https://doi.org/10.1145/3178876.3186032\n[37] W.K.M Mithsara, Abdur R. Shahid, and Ning Yang. 2024. Intelligent Fall Detection and Emergency Response for Smart Homes Using Language\nModels. In 2024 International Conference on Machine Learning and Applications (ICMLA). 230‚Äì235. https://doi.org/10.1109/ICMLA61862.2024.00037\n[38] W.K.M Mithsara, Abdur R. Shahid, and Ning Yang. 2024. Poster: Detection of Poisoning Attacks in Human Activity Recognition using Large\nLanguage Models. In Proceedings of the IEEE-EMBS International Conference on Body Sensor Networks (IEEE BSN). IEEE.\n[39] W.K.M Mithsara, Abdur R. Shahid, and Ning Yang. 2024. Poster: Leveraging Large Language Models for Zero-Shot Detection and Mitigation of\nData Poisoning in Wearable AI Systems. In Proceedings of the NeurIPS 2024 Workshop on GenAI for Health: Potential, Trust and Policy Compliance.\nVancouver, BC, Canada. Poster presentation.\n[40] W.K.M Mithsara, Abdur R. Shahid, and Ning Yang. 2024. Zero-Shot Detection and Sanitization of Data Poisoning Attacks in Wearable AI Using Large\nLanguage Models. In 2024 International Conference on Machine Learning and Applications (ICMLA). 1510‚Äì1515. https://doi.org/10.1109/ICMLA61862.\n2024.00233\n[41] Ehsan Nowroozi, Imran Haider, Rahim Taheri, and Mauro Conti. 2025. Federated Learning Under Attack: Exposing Vulnerabilities Through Data\nPoisoning Attacks in Computer Networks. IEEE Transactions on Network and Service Management 22, 1 (2025), 822‚Äì831. https://doi.org/10.1109/\nTNSM.2025.3525554\n[42] Fco Javier Ord√≥nez, Paula De Toledo, and Araceli Sanchis. 2013. Activity recognition using hybrid generative/discriminative models on home\nenvironments using binary sensors. Sensors 13, 5 (2013), 5460‚Äì5477.\n[43] Xiaomin Ouyang and Mani Srivastava. 2024. LLMSense: Harnessing LLMs for high-level reasoning over spatiotemporal sensor traces. In 2024 IEEE\n3rd Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML). IEEE, 9‚Äì14.\n[44] Andrea Paudice, Luis Mu√±oz-Gonz√°lez, and Emil C Lupu. 2019. Label sanitization against label flipping poisoning attacks. In ECML PKDD 2018\nWorkshops: Nemesis 2018, UrbReas 2018, SoGood 2018, IWAISe 2018, and Green Data Mining 2018, Dublin, Ireland, September 10-14, 2018, Proceedings 18.\nSpringer, 5‚Äì15.\n[45] Roberto Perdisci, David Dagon, Wenke Lee, Prahlad Fogla, and Monirul Sharif. 2006. Misleading worm signature generators using deliberate noise\ninjection. In 2006 IEEE Symposium on Security and Privacy (S&P‚Äô06). IEEE, 15‚Äìpp.\n[46] Benedetta Picano, Dinh Thai Hoang, and Diep N. Nguyen. 2025. A Matching Game for LLM Layer Deployment in Heterogeneous Edge Networks.\nIEEE Open Journal of the Communications Society 6 (2025), 3795‚Äì3805. https://doi.org/10.1109/OJCOMS.2025.3561605\n[47] Attila Reiss and Didier Stricker. 2012. Introducing a new benchmarked dataset for activity monitoring. Proc. of the 16th IEEE International\nSymposium on Wearable Computers. https://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring PAMAP2 Physical Activity\nMonitoring Dataset.\n[48] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering\nin large language models: Techniques and applications. arXiv preprint arXiv:2402.07927 (2024).\nManuscript submitted to ACM\n"}, {"page": 30, "text": "30\nMithsara et al.\n[49] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. 2021. Just how toxic is data poisoning? a unified benchmark\nfor backdoor and data poisoning attacks. In International Conference on Machine Learning. PMLR, 9389‚Äì9398.\n[50] Abdur R. Shahid, Syed Mhamudul Hasan, Ahmed Imteaj, and Shahriar Badsha. 2024. Context-Aware Spatiotemporal Poisoning Attacks on\nWearable-Based Activity Recognition. In IEEE INFOCOM 2024 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS). 1‚Äì2.\nhttps://doi.org/10.1109/INFOCOMWKSHPS61880.2024.10620768\n[51] Abdur R Shahid, Ahmed Imteaj, Shahriar Badsha, and Md Zarif Hossain. 2023. Assessing wearable human activity recognition systems against\ndata poisoning attacks in differentially-private federated learning. In 2023 IEEE International Conference on Smart Computing (SMARTCOMP). IEEE,\n355‚Äì360.\n[52] Abdur R Shahid, Ahmed Imteaj, Peter Y Wu, Diane A Igoche, and Tauhidul Alam. 2022. Label flipping data poisoning attack against wearable\nhuman activity recognition system. In 2022 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 908‚Äì914.\n[53] Allan Stisen, Blunck, Henrik, Bhattacharya, Sourav, Prentow, Thor Siiger, Mikkel Baun Kj√¶rgaard, Dey, Anind, Tobias Sonne, and Mads M√∏ller\nJensen. 2015. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In Proceedings of the 13th\nACM conference on embedded networked sensor systems. 127‚Äì140.\n[54] Gan Sun, Yang Cong, Jiahua Dong, Qiang Wang, Lingjuan Lyu, and Ji Liu. 2021. Data poisoning attacks on federated machine learning. IEEE Internet\nof Things Journal 9, 13 (2021), 11365‚Äì11375.\n[55] Shihua Sun, Shridatt Sugrim, Angelos Stavrou, and Haining Wang. 2024. Partner in Crime: Boosting Targeted Poisoning Attacks against Federated\nLearning. arXiv preprint arXiv:2407.09958 (2024).\n[56] Wei Sun, Bo Gao, Ke Xiong, Yuwei Wang, Pingyi Fan, and Khaled Ben Letaief. 2024. A GAN-Based Data Poisoning Attack Against Federated\nLearning Systems and Its Countermeasure. arXiv preprint arXiv:2405.11440 (2024).\n[57] Shan Ullah, Mehdi Pirahandeh, and Deok-Hwan Kim. 2024. Self-attention deep ConvLSTM with sparse-learned channel dependencies for wearable\nsensor-based human activity recognition. Neurocomputing 571 (2024), 127157.\n[58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 24824‚Äì24837.\n[59] Gary M Weiss. 2019. Wisdm smartphone and smartwatch activity and biometrics dataset. UCI Machine Learning Repository: WISDM Smartphone and\nSmartwatch Activity and Biometrics Dataset Data Set 7, 133190-133202 (2019), 5.\n[60] Bin Xiao, Burak Kantarci, Jiawen Kang, Dusit Niyato, and Mohsen Guizani. 2024. Efficient prompting for llm-based generative internet of things.\nIEEE Internet of Things Journal (2024).\n[61] Huatao Xu, Panrong Tong, Mo Li, and Mani Srivastava. 2024. AutoLife: Automatic Life Journaling with Smartphones and LLMs. arXiv preprint\narXiv:2412.15714 (2024).\n[62] Ruiyao Xu and Kaize Ding. 2025. Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey. In Findings of the Association\nfor Computational Linguistics: NAACL 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque,\nNew Mexico, 5992‚Äì6012. https://aclanthology.org/2025.findings-naacl.333/\n[63] Huanqi Yang, Sijie Ji, Rucheng Wu, and Weitao Xu. 2024. Are you being tracked? discover the power of zero-shot trajectory tracing with llms!. In\n2024 IEEE Coupling of Sensing & Computing in AIoT Systems (CSCAIoT). IEEE, 13‚Äì18.\n[64] Abbas Yazdinejad, Ali Dehghantanha, Hadis Karimipour, Gautam Srivastava, and Reza M. Parizi. 2024. A Robust Privacy-Preserving Federated\nLearning Model Against Model Poisoning Attacks. IEEE Transactions on Information Forensics and Security 19 (2024), 6693‚Äì6708. https://doi.org/10.\n1109/TIFS.2024.3420126\n[65] Andrea Zanella, Nicola Bui, Angelo Castellani, Lorenzo Vangelista, and Michele Zorzi. 2014. Internet of Things for Smart Cities. IEEE Internet of\nThings Journal 1, 1 (Feb 2014), 22‚Äì32. https://doi.org/10.1109/JIOT.2014.2306328\n[66] Haopeng Zhang, Yili Ren, Haohan Yuan, Jingzhe Zhang, and Yitong Shen. 2025. Wi-Chat: Large Language Model Powered Wi-Fi Sensing. arXiv\npreprint arXiv:2502.12421 (2025).\n[67] Jiale Zhang, Bing Chen, Xiang Cheng, Huynh Thi Thanh Binh, and Shui Yu. 2021. PoisonGAN: Generative Poisoning Attacks Against Federated\nLearning in Edge Computing Systems. IEEE Internet of Things Journal 8, 5 (2021), 3310‚Äì3322. https://doi.org/10.1109/JIOT.2020.3023126\n[68] Yao Zhao, Youyang Qu, Yong Xiang, Md Palash Uddin, Dezhong Peng, and Longxiang Gao. 2024. A Comprehensive Survey on Edge Data Integrity\nVerification: Fundamentals and Future Trends. ACM Comput. Surv. 57, 1, Article 8 (Oct. 2024), 34 pages. https://doi.org/10.1145/3680277\n[69] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models.\nIn International conference on machine learning. PMLR, 12697‚Äì12706.\n[70] Zhirun Zheng, Zhetao Li, Cheng Huang, Saiqin Long, Mushu Li, and Xuemin Shen. 2024. Data poisoning attacks and defenses to LDP-based\nprivacy-preserving crowdsensing. IEEE Transactions on Dependable and Secure Computing (2024).\nManuscript submitted to ACM\n"}]}