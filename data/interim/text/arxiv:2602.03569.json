{"doc_id": "arxiv:2602.03569", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.03569.pdf", "meta": {"doc_id": "arxiv:2602.03569", "source": "arxiv", "arxiv_id": "2602.03569", "title": "EHRWorld: A Patient-Centric Medical World Model for Long-Horizon Clinical Trajectories", "authors": ["Linjie Mu", "Zhongzhen Huang", "Yannian Gu", "Shengqian Qin", "Shaoting Zhang", "Xiaofan Zhang"], "published": "2026-02-03T14:12:24Z", "updated": "2026-02-03T14:12:24Z", "summary": "World models offer a principled framework for simulating future states under interventions, but realizing such models in complex, high-stakes domains like medicine remains challenging. Recent large language models (LLMs) have achieved strong performance on static medical reasoning tasks, raising the question of whether they can function as dynamic medical world models capable of simulating disease progression and treatment outcomes over time. In this work, we show that LLMs only incorporating medical knowledge struggle to maintain consistent patient states under sequential interventions, leading to error accumulation in long-horizon clinical simulation. To address this limitation, we introduce EHRWorld, a patient-centric medical world model trained under a causal sequential paradigm, together with EHRWorld-110K, a large-scale longitudinal clinical dataset derived from real-world electronic health records. Extensive evaluations demonstrate that EHRWorld significantly outperforms naive LLM-based baselines, achieving more stable long-horizon simulation, improved modeling of clinically sensitive events, and favorable reasoning efficiency, highlighting the necessity of training on causally grounded, temporally evolving clinical data for reliable and robust medical world modeling.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.03569v1", "url_pdf": "https://arxiv.org/pdf/2602.03569.pdf", "meta_path": "data/raw/arxiv/meta/2602.03569.json", "sha256": "8162129fd2082eefdbef298adbf0e7037b35ba1b5142e2fdc0a85d00046054c6", "status": "ok", "fetched_at": "2026-02-18T02:19:53.365575+00:00"}, "pages": [{"page": 1, "text": "EHRWorld: A Patient-Centric Medical World Model for\nLong-Horizon Clinical Trajectories\nLinjie Mu1, Zhongzhen Huang1, Yannian Gu1, Shengqian Qin1,\nShaoting Zhang1,*, Xiaofan Zhang1,2,*\n1Shanghai Jiao Tong University, 2Shanghai Innovation Institute\nAbstract\nWorld models offer a principled framework for\nsimulating future states under interventions, but\nrealizing such models in complex, high-stakes\ndomains like medicine remains challenging.\nRecent large language models (LLMs) have\nachieved strong performance on static med-\nical reasoning tasks, raising the question of\nwhether they can function as dynamic medi-\ncal world models capable of simulating dis-\nease progression and treatment outcomes over\ntime. In this work, we show that LLMs only\nincorporating medical knowledge struggle to\nmaintain consistent patient states under sequen-\ntial interventions, leading to error accumula-\ntion in long-horizon clinical simulation. To ad-\ndress this limitation, we introduce EHRWorld,\na patient-centric medical world model trained\nunder a causal sequential paradigm, together\nwith EHRWorld-110K, a large-scale longitu-\ndinal clinical dataset derived from real-world\nelectronic health records. Extensive evaluations\ndemonstrate that EHRWorld significantly out-\nperforms naive LLM-based baselines, achiev-\ning more stable long-horizon simulation, im-\nproved modeling of clinically sensitive events,\nand favorable reasoning efficiency, highlighting\nthe necessity of training on causally grounded,\ntemporally evolving clinical data for reliable\nand robust medical world modeling.\n1\nIntroduction\nThe concept of world models has emerged as a\npivotal paradigm in the pursuit of general artificial\nintelligence (Ha and Schmidhuber, 2018; LeCun,\n2022). By constructing a comprehensive internal\nrepresentation of the environment, a world model\nenables simulating future states conditioned on pre-\nvious actions (Hafner et al., 2019). This capabil-\nity supports planning in imagination prior to real-\nworld execution (Bengio et al., 2019). Such an\nability to reason about how current interactions\n* Corresponding authors.\nWorld Model\nNaive LLM-based Simulator\nClinical Events\n43s\n200K/μL\nPPT (sec)\nPlatelet\nFactor VIII\nVWF Antigen \nDesmopressin\n150%\nSimulation fails after \nthe intervention event \noccurs!\nScope \nHit \nScope \nHit \nScope \nMiss \nTimeline\nA 20-year-old female presents with chronic menorrhagia and a history of easy \nbruising. Family history is negative for bleeding disorders. Vital signs are stable. \nLaboratory tests reveals normal platelets (200,000/µL) and PT (12s), but an \nisolated prolongation of the PTT at 43s.  Von Willebrand Disease is the  diagnosis.\n43s\n200K/μL\nScope \nHit \nScope \nHit \n60%\nRobust\nSimulation\nUnderlying State \nUpdate\n75%\nRobust\nSimulation\nPatient Static Information\nComparison of Relative Error in \nChloride (mEq/L)\nFigure 1: Overview of patient simulation challenges and\nperformance evaluation. The upper and middle panels il-\nlustrate a clinical scenario where a standard LLM-based\nsimulator fails to infer implicit physiological states or\ncorrectly update patient status following medical inter-\nventions. In contrast, the proposed EHRWorld model\nmaintains logical consistency and robustness. The lower\npanel presents the trajectory of relative error for Chlo-\nride levels across eight simulation rounds. We compare\nour approach against GPT-5.2, demonstrating that our\nmodel significantly constrains the rate of error propa-\ngation, resulting in a widening performance gap that\nhighlights robustness in long-horizon simulations.\nshape future dynamics is fundamental to effec-\ntive autonomous decision-making in complex and\nevolving environments, yet remains challenging to\nrealize in real-world, high-stakes domains.\nAs a multifaceted endeavor, medicine provides\na particularly compelling setting for world mod-\nels. Clinical practice involves highly complex sys-\ntems in which heterogeneous signals, ranging from\nphysiological measurements and imaging to lab-\noratory tests and clinical narratives, interact over\ntime. Clinical decisions, such as medication choice,\ndosage, and timing, can alter a patient’s future phys-\niological trajectory, often in ways that are uncer-\n1\narXiv:2602.03569v1  [cs.AI]  3 Feb 2026\n"}, {"page": 2, "text": "tain and patient-specific. Consequently, clinicians\nmust routinely perform counterfactual reasoning,\nimplicitly asking how outcomes might differ under\nalternative treatment plans. Accurately modeling\nlong-horizon patient state transitions under vari-\nous treatment strategies is therefore essential for\neffective and personalized care. A medical world\nmodel could help clinicians “see” health as a con-\ntinuous evolving process, while also providing a\nprincipled foundation for AI systems that estimate\nand leverage the causal effects of clinical actions.\nRecent advances in large language models\n(LLMs) have significantly reshaped the landscape\nof medical AI. These models have demonstrated\nstrong performance across a range of medical tasks,\nincluding clinical report generation (Wang et al.,\n2023; Jin et al., 2024), diagnostic reasoning (Dou\net al., 2025; Chen et al., 2024), and medical ques-\ntion answering (Li et al., 2023; Mu et al., 2025).\nMotivated by these successes, a natural question\narises: can LLMs, trained on vast corpora of medi-\ncal textbooks, literature and clinical case reports,\nserve as world models capable of simulating the\ntemporal evolution of clinical indicators and treat-\nment outcomes?\nWe examine whether LLMs incorporating medi-\ncal knowledge can function as medical world mod-\nels by analyzing their behavior in sequential clin-\nical simulations. As illustrated in Figure 1, while\nthese models can accurately replicate clinical ob-\nservations at individual time points, they struggle\nwhen the simulation moves beyond static infor-\nmation. Meanwhile, they struggle to internally\nmaintain consistent patient states when interven-\ntion events occur. These phenomena result in the\naccumulation of errors across multi-step interac-\ntions, reflecting the absence of explicit mechanisms\nfor tracking underlying physiological states.\nTo address these limitations, we establish a ro-\nbust data foundation by curating a large-scale clin-\nical dataset, EHRWorld-110K, derived from real-\nworld Electronic Health Records (EHRs) (Johnson\net al., 2023). The construction pipeline consists\nof three main stages: (1) extracting episode-level\nstatic patient profiles from unstructured clinical\nnotes; (2) organizing temporally ordered event se-\nquences from time-stamped event logs; and (3) inte-\ngrating patient profiles with event sequences at the\nhospitalization-episode level, followed by rigorous\nquality filtering. In total, EHRWorld-110K com-\nprises approximately 110 thousand diverse hospi-\ntalization episodes and 17.5 million highly clinical\nevents, covering the full trajectory from admission\nto discharge. This dataset provides a principled\nfoundation for learning patient state evolution and\nintervention-conditioned transitions in longitudinal\nreal-world clinical care.\nBuilding upon this foundation, we introduce a\ngenerative training paradigm that models clinical\ntrajectories as a continuous sequential process, fa-\ncilitating the learning of intervention-driven phys-\niological transitions. We then train a family of\nmodels, EHRWorld, at different parameter scales.\nEHRWorlds function as evolving patient simula-\ntors, dynamically learning and updating physiolog-\nical states based on interaction history and thera-\npeutic inputs. Through extensive evaluations, we\ndemonstrate that EHRWorld significantly outper-\nforms naive LLM-based baselines, exhibiting a\nmarked reduction in error accumulation in long-\nhorizon simulations, enhanced stability during clin-\nically sensitive events, and improved reasoning ef-\nficiency compared to other models. These findings\nunderscore the importance of training on causally\ngrounded, temporally evolving clinical trajectory\ndata to ensure reliable modeling.\nIn summary, our main contributions are:\n• We introduce EHRWorld-110K, a large-scale\nlongitudinal dataset that captures complete\nhigh-fidelity patient clinical care trajectories\nfrom admission to discharge, enabling the\nstudy of temporally evolving and underlying\nintervention-conditioned clinical dynamics.\n• We propose EHRWorld, a unified family of\npatient-centric medical world models trained\nunder a causal sequential paradigm, which\nsimulate dynamic disease progression by\nmaintaining and updating physiological states\nin response to clinical interventions.\n• We present a comprehensive evaluation. The\nresults demonstrate that EHRWorld signifi-\ncantly outperforms naive LLM-based base-\nlines in long-horizon clinical simulation, with\nreduced error accumulation, improved stabil-\nity on clinically sensitive events, and favorable\nreasoning efficiency.\n2\nRelated Work\n2.1\nNaive LLM-based Simulation\nRecent medical AI evaluation has increasingly\nshifted from static knowledge assessment toward\n2\n"}, {"page": 3, "text": "interactive patient simulation that better reflects\nreal-world clinical practice. Several frameworks\nemploy LLMs as virtual patients to enable multi-\nturn clinical interaction. Representative systems\nsuch as AgentClinic (Schmidgall et al., 2024) and\nAutoMedic (Oh et al., 2025) use prompt-based role-\nplaying to simulate patients with predefined clinical\nprofiles, facilitating the evaluation of history-taking\nand diagnostic reasoning. Extensions such as CP-\nEnv (Zhu et al., 2025) and MAQuE (Gong et al.,\n2025) further adopt dialogue-driven settings, where\nmedical agents iteratively query patient agents to re-\nsolve underlying and inherent clinical uncertainty.\nDespite their interactive interfaces, these ap-\nproaches rely on fundamentally static simulation\nmechanisms. Patient states are typically grounded\nin fixed case descriptions, resulting in immutable\nrepresentations without temporal or physiological\ndynamics. As a consequence, such simulators are\nprone to hallucination when queried beyond ex-\nplicitly provided information, and they do not sup-\nport causal state transitions under clinical interven-\ntions. These limitations prevent existing simulators\nfrom modeling disease progression or treatment ef-\nfects over time, highlighting the need for dynamic,\ncausally grounded medical world models.\n2.2\nEHR World Models\nWorld models are fundamentally characterized by\ntheir capacity to internally represent a complex en-\nvironment and simulate future states conditioned\non agent actions (Ha and Schmidhuber, 2018). In\nthe context of EHRs, prior research has progressed\nfrom passive patient representation toward limited\nforms of generative modeling; however, these ap-\nproaches have largely fallen short of providing truly\ninteractive, intervention-aware simulation.\nEarly efforts in EHR modeling primarily fo-\ncused on discriminative prediction. Models such as\nBEHRT (Li et al., 2020) and Med-BERT (Rasmy\net al., 2021) leverage transformer (Vaswani et al.,\n2017) architectures to encode longitudinal patient\nhistories for downstream risk estimation, includ-\ning mortality and readmission. While effective for\nrisk stratification, these approaches operate as pas-\nsive observers: they predict outcomes from fixed\nsequences without modeling how patient states\nevolve in response to clinical interventions.\nSubsequent work explored generative modeling\nto synthesize patient trajectories. Rule-based simu-\nlators such as Synthea (Chen et al., 2019) and data-\ndriven approaches like PatientSim (Kyung et al.,\n2025) generate synthetic EHR sequences for data\naugmentation and benchmarking. However, these\nmethods typically produce static trajectories and do\nnot support dynamic state transitions conditioned\non therapeutic actions, limiting their use for com-\nplex treatment planning or counterfactual reason-\ning. In contrast, EHRWorld models patient state\nevolution as a fully causal and dynamic process,\nenabling continuous simulation of physiological\nresponses under sequential clinical interventions.\n3\nData Construction Pipeline\nTo facilitate longitudinal and intervention-aware\nclinical trajectory simulation, we curate a large-\nscale dataset derived from real-world EHR records.\nIn this section, we will detail the formalization\nof the EHRWorld-110K pipeline, as schematically\nillustrated in the upper panel of Figure 2.\n3.1\nData Sources and Preprocessing\nOur pipeline starts from raw records in the MIMIC-\nIV database (Johnson et al., 2023), including un-\nstructured clinical notes and structured event logs.\nThese two data streams are processed in parallel\nand subsequently integrated at the episode level.\nParsing Unstructured Clinical Notes.\nUnstruc-\ntured discharge summaries are processed using\nLLMs, such as Qwen3-235B-A22B-Instruct (Yang\net al., 2025), to extract patient-level static infor-\nmation. This step yields structured demographic\nattributes (e.g., age and gender) as well as a hier-\narchical diagnostic set, including primary and sec-\nondary diagnoses. These elements provide a stable\nclinical context for each hospitalization episode.\nProcessing Raw Event Sequences.\nIn parallel,\nraw time-stamped clinical events are extracted and\norganized into event sequences spanning the entire\nhospital stay. Each event is categorized based on its\nclinical role and whether it produces an observable\nvalue. Specifically, we distinguish between:\n• Inquiry Events, which correspond to passive\nobservations of the patient state, such as labora-\ntory tests and physical examinations, and yield\nexplicit measurement values;\n• Intervention Events, which correspond to active\nclinical actions aimed at altering the patient’s con-\ndition, such as medication administrations and\nprocedures, that aim to modify the patient state\nwithout directly generating observable outcomes.\n3\n"}, {"page": 4, "text": "MIMIC-IV Database\n(Raw EHR Records)\nUnstructured Notes\nRaw Event Sequence\nIntervention Events\n(          , Active)\nInquiry Events\n(          , Passive)\nCategorization\n& Filtering\nLLM Parsering\nStatic\nDemographics\nHierarchical \nDiagnostic Set\nRigorous\nFiltering\n(a) Data Construction Pipeline\n(b) The EHRWorld Model\nEHRWorld-110K\nPatient State\nClinical Actions (    )\nEHRWorld Model\n( Set-Based Conditional \nGenerator      )\nDual-Mode Prediction Mechanism\nIntervention \n(       )\nInquiry \n(       )\nPredicted \nValues (      )\nLatent \nOutcome (   )\nOutcome Set \n(     )\nNext Step \nUpdate History: \nDeterministic State Transition\n(Update Loop)\nFinal Loss\n(       )\nFigure 2: Overview of the EHRWorld framework. (a) Data construction pipeline. Raw EHR records from MIMIC-IV\nare processed into the EHRWorld-110K dataset by integrating static patient context with longitudinal clinical events.\n(b) The EHRWorld model. At each step, the model conditions on the current patient state and a set of clinical\nactions, generates outcomes via a dual-mode mechanism for inquiries and interventions, and updates the interaction\nhistory for sequential trajectory simulation.\nOnly events that were explicitly executed are\nretained, while incomplete, duplicated, or adminis-\ntratively recorded entries are removed.\n3.2\nData Filtering and Partition\nFor each hospitalization, the parsed static informa-\ntion, including patient demographics and hierarchi-\ncal diagnosis, and event sequence are seamlessly\ncombined into a single episode, with events strictly\nordered chronologically to preserve the temporal\nstructure of the clinical trajectory. We then ap-\nply stringent filtering criteria to ensure data quality\nand consistency across the constructed episodes,\nexcluding admissions with insufficient clinical ac-\ntivity or incomplete records. The resulting dataset,\nEHRWorld-110K, consists of 110, 513 hospitaliza-\ntion episodes and approximately 17.5 million clini-\ncal events, offering high-quality longitudinal clini-\ncal trajectories for downstream simulation tasks. A\ndetailed discussion is provided in Appendix A.\nDataset Partition. To support reliable evaluation\nwhile preserving the diversity of real-world clini-\ncal data, we partition the constructed dataset using\nstratified sampling based on primary diagnostic cat-\negories. This procedure yields a held-out test set\nof 579 hospitalization episodes, comprising 84,010\ninquiry events and 25,798 intervention events, and\nincludes 1,043 unique primary and secondary di-\nagnostic conditions, ensuring a comprehensive rep-\nresentation of various clinical scenarios. This bal-\nanced distribution ensures rigorous testing across\ndiverse clinical conditions. The remaining episodes\nare used for model training.\n4\nThe EHRWorld Model\nWe formulate patient simulation as a sequential de-\ncision process in which clinical interactions evolve\nover discrete simulation steps indexed by t. Each\nstep corresponds to a physiological timestamp\nτt, and the simulator models how patient states\nevolve in response to sets of concurrent clinical\nactions. As illustrated in the lower panel of Fig-\nure 2, EHRWorld is designed as a conditional world\nmodel that explicitly tracks patient states and up-\ndates them under sequential inquiries and interven-\ntions.\n4.1\nPatient State Representation\nLet S denote the patient state space. At step t, the\npatient state St ∈S is defined as\nSt = ⟨τt, d, Y, Ht⟩,\n(1)\nwhere τt denotes the current physiological times-\ntamp, d represents static demographic attributes,\nand Y denotes the diagnostic profile of the current\nhospitalization episode, extracted from discharge\nsummaries and used as episode-level clinical con-\ntext. To provide causal grounding, Y includes both\n4\n"}, {"page": 5, "text": "primary and secondary diagnoses together with\ntheir clinical rationales:\nY = {⟨ypri, rpri⟩} ∪{⟨y(k)\nsec , r(k)\nsec ⟩}K\nk=1.\n(2)\nThe interaction history Ht = {E1, . . . , Et−1} ag-\ngregates all past clinical events prior to τt and is\ninitialized as empty, where each Ek contains all\nactions and outcomes occurring at timestamp τk.\n4.2\nSet-Based Conditional Generation\nReal-world clinical practice often involves issuing\nmultiple medical orders simultaneously. To reflect\nthis complexity, EHRWorld models clinical interac-\ntion as a set-based conditional generation problem.\nAt step t, a clinician issues a set of orders, which we\nformally model as actions At = {a(1)\nt , . . . , a(N)\nt\n}.\nThe model is required to generate a correspond-\ning set of outcomes Vt = {v(1)\nt , . . . , v(N)\nt\n}, rep-\nresenting the effects of these clinical actions on\nthe patient’s state. These outcomes are necessary\nto capture the dynamic and concurrent nature of\nclinical decision-making, where multiple actions\ncan influence patient states in parallel.\nEHRWorld parameterizes a conditional distribu-\ntion Pθ over outcome sets given the current state\nand action set:\nPθ(Vt | St, At) =\nN\nY\nj=1\nPθ(v(j)\nt\n| St, At),\n(3)\nallowing the model to generate coherent outcomes\nfor concurrent clinical actions while conditioning\non a shared latent patient state.\n4.3\nDual-Mode Prediction Mechanism\nClinical actions differ in whether they produce im-\nmediate observable outcomes. We therefore adopt\na dual-mode prediction mechanism for inquiry and\nintervention that conditions the model behavior on\nthe semantic type of each action.\nInquiry Mode.\nFor inquiry actions a(j)\nt\n∈Ainq,\nsuch as laboratory tests or physical examinations,\nthe model predicts explicit observable values. The\ntraining objective for these actions is the negative\nlog-likelihood of the ground-truth observations:\nLinq = −\nX\nj:a(j)\nt\n∈Ainq\nlog Pθ(v(j)\ngt | St, At).\n(4)\nIntervention Mode.\nFor intervention actions\na(j)\nt\n∈Aint (e.g., medications or procedures), the\nmodel is not required to generate an immediate ob-\nservable value. We formalize this by assigning an\nempty outcome Pθ(v(j)\nt\n= ∅) = 1. Instead of yield-\ning direct feedback, these actions are recorded into\nthe patient’s history as state-altering intervention\nevents. Their primary function is to update the sim-\nulation context, thereby influencing the trajectory\nof future clinical predictions.\n4.4\nDeterministic State Transition\nAfter generating the outcome set Vt, we construct\nthe event set at step t as Et = {(a(j)\nt , v(j)\nt )}N\nj=1.\nConsequently, the interaction history is then deter-\nministically updated:\nHt+1 = Ht ∪{Et},\nSt+1 = ⟨τt+1, d, Y, Ht+1⟩.\n(5)\nThis update loop ensures that all subsequent predic-\ntions are conditioned on the complete sequence of\nprior inquiries and interventions, enabling consis-\ntent long-horizon simulation of disease progression\nunder clinical decision-making. Appendix B pro-\nvides a description of the simulation procedure.\n5\nExperiments\n5.1\nExperimental Setup\nImplementation Details. We train three medical\nworld model variants, EHRWorld-4B, EHRWorld-\n8B, and EHRWorld-14B, by fine-tuning the Qwen3-\n4B-Instruct, Qwen3-8B (Yang et al., 2025), and\nQwen2.5-14B-Instruct (Yang et al., 2024) founda-\ntion models on the EHRWorld-110K dataset. All\nmodels are trained for three epochs. Training is per-\nformed on a cluster of 8× NVIDIA H100 GPUs us-\ning DeepSpeed ZeRO-2 optimization (Rasley et al.,\n2020). We adopt AdamW (Loshchilov and Hutter,\n2019) with a learning rate of 1 × 10−6, a cosine de-\ncay schedule, and a warm-up ratio of 0.05. During\ntraining, models are optimized to predict outcomes\nconditioned on sets of clinical events, with causal\nmasking applied to preserve the temporal order.\nBaselines. We compare our models against a di-\nverse set of strong LLMs baselines to assess per-\nformance across different model scales and train-\ning paradigms. The baselines are grouped into\nthree categories based on availability and domain\nspecialization: (1) Closed-Source Models, includ-\ning the proprietary frontier systems GPT-5.2 (Ope-\n5\n"}, {"page": 6, "text": "Table 1: Overall performance of world model under full trajectory prediction. We evaluate numerical precision\nand label accuracy. Avg Score is calculated as the mean of S@25, Stat F1, and Label F1. Err denotes SMAPE. ↑\nindicates higher is better, ↓indicates lower is better. The best results are bolded and the second best are underlined.\nModel\nSize\nNumerical Precision\nLabel Accuracy\nAvg\nS@10 ↑\nS@15 ↑\nS@25 ↑\nErr ↓\nStat F1 ↑\nPrecision ↑\nRecall ↑\nF1 ↑\nScore ↑\nClosed-Source Models\nGPT-5.2\n−\n0.435\n0.540\n0.674\n0.348\n0.627\n0.635\n0.490\n0.553\n0.618\nGemini-3.0-Pro-Preview\n−\n0.445\n0.543\n0.681\n0.299\n0.565\n0.481\n0.473\n0.477\n0.574\nOpen-Source General-Purpose Models\nQwen3-4B-Instruct\n4B\n0.365\n0.471\n0.607\n0.448\n0.473\n0.450\n0.258\n0.328\n0.469\nQwen3-8B\n8B\n0.359\n0.463\n0.600\n0.437\n0.466\n0.448\n0.373\n0.407\n0.491\nQwen2.5-14B-Instruct\n14B\n0.393\n0.507\n0.659\n0.339\n0.423\n0.358\n0.237\n0.285\n0.456\nQwen3-30B-A3B-Instruct\n30B\n0.403\n0.508\n0.645\n0.338\n0.419\n0.324\n0.415\n0.364\n0.476\nLlama-3.3-70B-Instruct\n70B\n0.381\n0.484\n0.625\n0.364\n0.377\n0.581\n0.439\n0.500\n0.501\nQwen3-Next-80B-A3B-Instruct\n80B\n0.398\n0.501\n0.630\n0.381\n0.538\n0.416\n0.471\n0.442\n0.537\nGPT-OSS-120B\n120B\n0.402\n0.502\n0.634\n0.372\n0.483\n0.432\n0.346\n0.384\n0.500\nMiniMax-M2.1\n229B\n0.410\n0.513\n0.650\n0.338\n0.502\n0.475\n0.486\n0.480\n0.544\nQwen3-235B-A22B-Instruct\n235B\n0.409\n0.514\n0.647\n0.339\n0.469\n0.483\n0.343\n0.402\n0.506\nGLM-4.7\n358B\n0.442\n0.525\n0.663\n0.322\n0.476\n0.381\n0.254\n0.305\n0.481\nDeepSeek-V3.2\n671B\n0.416\n0.518\n0.649\n0.340\n0.554\n0.509\n0.402\n0.449\n0.551\nOpen-Source Medical Models\nMedGemma-4B-IT\n4B\n0.336\n0.430\n0.554\n0.561\n0.438\n0.412\n0.142\n0.211\n0.401\nMedGemma-27B-IT\n27B\n0.364\n0.460\n0.588\n0.496\n0.436\n0.259\n0.205\n0.229\n0.418\nBaichuan-M2-32B\n32B\n0.372\n0.472\n0.601\n0.418\n0.499\n0.456\n0.386\n0.418\n0.506\nOurs ( EHRWorld)\nEHRWorld-4B\n4B\n0.460\n0.566\n0.703\n0.274\n0.649\n0.939\n0.886\n0.912\n0.755\nEHRWorld-8B\n8B\n0.468\n0.577\n0.714\n0.269\n0.658\n0.936\n0.891\n0.913\n0.762\nEHRWorld-14B\n14B\n0.475\n0.582\n0.716\n0.262\n0.667\n0.925\n0.901\n0.913\n0.765\nnAI, 2025) and Gemini-3.0-Pro-Preview (Deep-\nMind et al., 2023); (2) Open-Source General-\nPurpose Models, spanning a wide range of param-\neter scales, including Qwen3-4B-Instruct, Qwen3-\n8B, Qwen2.5-14B-Instruct, and Qwen3-30B-A3B-\nInstruct as well as larger models such as Llama-\n3.3-70B-Instruct (Dubey et al., 2024), Qwen3-\nNext-80B-A3B-Instruct, GPT-OSS-120B (OpenAI,\n2025), MiniMax-M2.1 (MiniMax, 2025), Qwen3-\n235B-A22B-Instruct, GLM-4.7 (Team et al., 2025),\nand DeepSeek-V3.2 (DeepSeek-AI, 2025); and (3)\nOpen-Source Medical Models, which are typi-\ncally trained or adapted on static medical corpora\nsuch as textbooks, clinical guidelines, and case re-\nports. This category includes MedGemma-4B-IT,\nMedGemma-27B-IT (Sellergren et al., 2025), and\nBaichuan-M2-32B (Dou et al., 2025). All baseline\nmodels are evaluated under their default inference\nconfigurations without task-specific fine-tuning.\nEvaluation Metrics.\nWe evaluate the fidelity\nof simulated physiological states using a multi-\ndimensional metric suite, with formal definitions of\nall metrics provided in Appendix C. Our evaluation\nconsiders two complementary aspects: (1) Numer-\nical Precision for continuous variables (e.g., vital\nsigns and laboratory results). Numerical precision\nis assessed using regression-based metrics, includ-\ning Success Rate (S@k) and SMAPE. Beyond raw\nnumerical error, we further evaluate clinical inter-\npretability by mapping predicted values to normal\nor abnormal ranges defined by clinical reference\nstandards, and report Clinical Status F1 to mea-\nsure whether pathological deviations are correctly\ncaptured. (2) Label Accuracy for discrete out-\nputs (e.g., microbiology results), where label cor-\nrectness is evaluated using standard classification\nmetrics, including precision, recall, and F1 score.\n5.2\nMain Results: Full Trajectory Prediction\nA robust medical world model must demonstrate\nthe ability to simulate the continuous evolution of\npatient health over extended horizons, maintaining\ncoherence under sequential interventions. To test\nthis capability, we evaluate full trajectory predic-\ntion, where the model must autoregressively gener-\nate the entire hospitalization trajectory from admis-\nsion to discharge, conditioned on sequential clinical\nevents and interventions. Table 1 presents a com-\n6\n"}, {"page": 7, "text": "Table 2: Stability Analysis: Next-Step Prediction vs. Full Trajectory Prediction. We assess model robustness\nacross three dimensions: Numerical Precision (Numer. S@25), Clinical Interpretation (Stat F1), and Label Accuracy\n(Label F1). Ret (%) denotes the Retention Rate ( Full\nNext ×100), quantifying the model’s resistance to error accumulation\nduring long-horizon generation. The best results are bolded and the second best are underlined.\nModel\nNumer. S@25\nStat F1\nLabel F1\nOverall\nNext\nFull\nRet (%) ↑\nNext\nFull\nRet (%) ↑\nNext\nFull\nRet (%) ↑\nNext\nFull\nRet (%) ↑\nBaichuan-M2-32B\n0.752\n0.601\n79.9\n0.692\n0.499\n72.1\n0.527\n0.418\n79.3\n0.657\n0.506\n73.4\nQwen3-Next-80B-A3B-Instruct 0.759\n0.630\n83.0\n0.715\n0.538\n75.2\n0.558\n0.442\n79.2\n0.677\n0.537\n76.3\nQwen3-235B-A22B-Instruct\n0.769\n0.647\n84.1\n0.707\n0.469\n66.3\n0.557\n0.402\n72.2\n0.678\n0.506\n71.9\nGLM-4.7\n0.775\n0.663\n85.5\n0.670\n0.476\n71.0\n0.426\n0.305\n71.6\n0.624\n0.481\n77.1\nDeepSeek-V3.2\n0.770\n0.649\n82.3\n0.722\n0.554\n76.7\n0.608\n0.449\n73.8\n0.700\n0.551\n77.6\nGPT-5.2\n0.789\n0.674\n85.4\n0.741\n0.627\n84.6\n0.620\n0.553\n86.2\n0.717\n0.618\n86.2\nEHRWorld-14B (Ours)\n0.806 0.716\n88.8\n0.784 0.667\n85.1\n0.928 0.913\n98.4\n0.839 0.765\n92.6\n50\n55\n60\n65\n70\n60.1%\n54.7%\n-5.4\n62.5%\n57.1%\n-5.4\n63.0%\n57.2%\n-5.8\n63.4%\n57.8%\n-5.6\n64.7%\n60.1%\n-4.6\n64.9%\n60.3%\n-4.6\n65.0%\n61.6%\n-3.4\n66.3%\n62.2%\n-4.1\n67.4%\n63.7%\n-3.7\n68.1%\n64.7%\n-3.4\n71.6%\n69.7%\n-1.9\nS@25 Accuracy Drop\nBaichuan-M2\nLlama3.3-70B\nQwen3-Next-80B-A3B\nGPT-OSS-120B\nQwen3-235B-A22B\nDeepSeek-V3\nMiniMax-M2.1\nGLM-4.7\nGPT-5.2\nGemini-3-Pro\nEHRWorld-14B\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.418\n0.480\n+0.062\n0.364\n0.429\n+0.065\n0.381\n0.443\n+0.062\n0.372\n0.424\n+0.052\n0.339\n0.384\n+0.045\n0.344\n0.398\n+0.054\n0.338\n0.388\n+0.050\n0.322\n0.371\n+0.049\n0.348\n0.397\n+0.049\n0.299\n0.341\n+0.042\n0.262\n0.292\n+0.030\nSMAPE Error Increase\nGlobal (Average)\nDyn (High Sensitivity)\nFigure 3: Performance stability analysis across global\nand dynamic clinical metrics. EHRWorld-14B demon-\nstrates superior robustness, exhibiting the smallest per-\nformance degradation in S@25 accuracy and SMAPE\nerror compared to general foundation models.\nprehensive comparison between our EHRWorld\nfamily and a diverse set of baselines.\nAs aforementioned in Section 1, naive LLM-\nbased simulators exhibit pronounced error drift\nin long-horizon clinical trajectory, leading to de-\ngraded downstream state consistency. This phe-\nnomenon persists even for strong closed-source\nbaselines: GPT-5.2 and Gemini-3.0-Pro-Preview\nachieve Avg Scores of 0.618 and 0.574, respec-\ntively. In contrast, EHRWorld attains an Avg Score\nof 0.755–0.765. These results demonstrate that\nEHRWorld yields substantially more stable long-\nhorizon simulation with reduced error drift.\nThe performance gap further indicates that pa-\nrameter scaling alone does not reliably improve\nlong-horizon trajectory simulation for general-\npurpose LLMs. For instance, scaling from Qwen3-\n30B-A3B-Instruct to the massive Qwen3-235B-\nA22B-Instruct yields negligible gains in numerical\nprecision, with S@25 increasing marginally from\n0.645 to 0.647. In stark contrast, even our smallest\nmodel, EHRWorld-4B, achieves a superior S@25\nof 0.703, outperforming much larger baselines in-\ncluding the closed-source Gemini-3.0-Pro-Preview\n(0.681). This disparity underscores that causal se-\nquential training matters more than parameter\nscaling for modeling patient dynamics, as special-\nized training objectives better capture underlying\ntemporal dependencies than simple scale.\nFurthermore, we observe that medical LLM is\ninsufficient for long-horizon trajectory predic-\ntion. While models specialized via medical instruc-\ntion tuning excel at tasks like medical question-\nanswering and report generation, they still struggle\nwith sustained clinical rollouts. As shown in Ta-\nble 1, medical baselines such as MedGemma and\nBaichuan-M2 achieve lower Avg Scores (0.401–\n0.506), lagging behind general-purpose models\nand falling significantly below EHRWorld (0.755–\n0.765). These results suggest that training on static\nknowledge does not naturally translate into reliable\nsequential simulation, reinforcing the need for lon-\ngitudinal, intervention-conditioned supervision for\ndynamic modeling.\n5.3\nNext-Step vs. Full Trajectory Prediction\nWe evaluate model performance in two distinct\nsettings: next-step prediction, where the model\npredicts the state at t+1 conditioned on ground-\ntruth history, and full-trajectory simulation, where\npredictions are generated autoregressively. This\nisolates the model’s intrinsic predictive capability\nfrom the effects of compounding rollout errors. To\nquantify robustness against error accumulation, we\nintroduce the Retention Rate, defined as Ret(%) =\nFull\nNext × 100, where a higher percentage indicates\n7\n"}, {"page": 8, "text": "Clinical Event History\nPatient Basic Information\nStatic Demographics: 69-year-old male with allergy to lisinopril admitted for left lower extremity claudication.\nPrimary Diagnosis: Left Femoral Artery Stenosis (Dx: LLE claudication. NIAS: L CFA 7x PSV step-up, chronic SFA occlusion.)\n<Time: 10-12 (Unknown)>\nExtirpation of Matter from Left Femoral Artery, \nOpen Approach; Supplement Left Femoral Artery \nwith Nonautologous Tissue Substitute, Open \nApproach\n<Time: 10-12 07:25:00>\nAcetaminophen | 1000 mg\n<Time: 10-12 14:12:00>\nHeparin | 5000 UNIT\n<Time: 10-12 17:11:00>\nAcetaminophen | 650 mg; Clopidogrel | 75 mg\n<Time: 10-12 19:46:00>\nHematocrit: 34.0 % \nHemoglobin: 10.8 g/dL \nMCH: 28.6 pg\nMCHC: 31.8 g/dL \nMCV: 90.0 fL\nPlatelet Count: 192.0 K/uL\nRDW: 14.7 %\nRDW-SD: 48.2 fL\nRed Blood Cells: 3.77 m/uL\nWhite Blood Cells: 10.7 K/uL\n<Time: 10-12 19:47:00>\nCilostazol | 100 mg; Ranitidine | 300 mg\n<Time: 10-12 22:15:00>\nHeparin | 5000 UNIT\nMetoprolol Tartrate | 25 mg\nRanolazine ER | 1000 mg\n<Time: 10-12 23:05:00>\nVancomycin | 1000 mg\n<Time: 10-13 03:09:00>\nAcetaminophen | 650 mg\nOxyCODONE | 5-10 mg\nModel Prediction at <Time 10-13 04:33:00>\nGround Truth\nCalcium, Total: 8.0 mg/dL\nCreatinine: 0.9 mg/dL\nHemolysis Index: 2.0\nHematocrit: 31.2\nHemoglobin: 10.3 g/dL\nIcterus Index: 1.0\nMagnesium: 1.7 mg/dL\neAG: 114.0 mg/dL\nEHRWorld-14B (ours)\n→ 8.3 mg/dL  (Δ=+3.8%)\n→ 0.8 mg/dL  (Δ=-11.1%)\n→ 2.0                    (Δ=0)\n→ 29.9           (Δ=-11.1%)\n→ 9.7 g/dL     (Δ=-4.2%)\n→ 1.0                    (Δ=0)\n→ 1.8 mg/dL  (Δ=+5.9%)\n→ 117mg/dL  (Δ=+2.6%)\nGPT-5.2\n→ 8.6 mg/dL  (Δ=+7.5%)\n→ 1.0 mg/dL  (Δ=+11.1%)\n→ 1.0              (Δ=-50%)\n→ 34.5         (Δ=+10.6%)\n→ 11.4 g/dL  (Δ=+10.7%)\n→ 0.0            (Δ=-100%)\n→ 1.9 mg/dL (Δ=+11.7%)\n→ 120 mg/dL (Δ=+5.3%)\nDeepSeek-V3.2\n→ 9.2 mg/dL   (Δ=+15%) \n→ 1.2 mg/dL   (Δ=+33%)\n→ 0.0            (Δ=-100%)\n→ 36.5         (Δ=+17.0%)\n→ 12.6 g/dL (Δ=+22.3%)\n→ 0.0            (Δ=-100%)\n→ 1.9 mg/dL (Δ=+11.7%)\n→ 150 mg/dL (Δ=+32%)\n→ 9.5 mg/dL (Δ=-18.8%)\n→ 1.2 mg/dL   (Δ=+33%)\n→ 0.0            (Δ=-100%)\n→ 42.0         (Δ=+34.6%)\n→ 14.0 g/dL (Δ=+35.9%)\n→ 0.0            (Δ=-100%)\n→ 2.2 mg/dL  (Δ=+29%)\n→ 125 mg/dL (Δ=+9.6%)\nBaichuan-M2-32B\nFigure 4: Qualitative case study comparison. We visualize the ground truth versus model predictions for a\n69-year-old patient. Colors denote relative error (∆) severity: precise (∆≤10%), acceptable (10% < ∆≤20%),\nand deviation (∆> 20%). EHRWorld-14B shows superior stability, keeping most metrics within low-error margin.\ngreater resilience and stability over long horizons.\nThe results in Table 2 reveal a critical distinction\nbetween local forecasting and long-term stability.\nWhile the performance gap between baselines and\nEHRWorld is narrow in next-step prediction (e.g.,\nNumer. S@25 ranges tightly from 0.752 to 0.806),\nit widens significantly in the full-trajectory setting.\nEHRWorld-14B achieves a superior overall reten-\ntion rate of 92.6%, surpassing the best baseline,\nGPT-5.2 (86.2%). This indicates that while many\ngeneral-purpose LLMs can effectively predict im-\nmediate outcomes when teacher-forced, they fail\nto maintain coherent patient states over time. In\ncontrast, EHRWorld demonstrates exceptional sta-\nbility, effectively mitigating the error accumulation\nthat degrades performance in standard models.\n5.4\nHigh-Sensitivity Stability Analysis\nClinical trajectories often contain abrupt physiolog-\nical shifts (e.g., acute deterioration or rapid treat-\nment response), where small inconsistencies can be\namplified into clinically implausible rollouts. To\nstress-test robustness under such transitions, we\nconstruct a high-sensitivity subset in which the rela-\ntive change between two consecutive steps exceeds\n50%. We then compare model performance on the\nglobal test distribution against this high-sensitivity\nsubset, reporting the degradation in S@25 (accu-\nracy drop) and SMAPE (error increase).\nAs shown in Figure 3, EHRWorld-14B shows the\nmost stable behavior under high-sensitivity shifts,\nwith S@25 decreasing from 71.6% to 69.7% (a\n−1.9 point drop) and SMAPE increasing only by\n+0.030 (0.262 →0.292).\nIn comparison, gen-\neral foundation models exhibit larger accuracy\ndrops and more pronounced SMAPE inflation on\nthe same subset, suggesting that their predictions\nare more susceptible to drift when the patient\nstate changes abruptly. Overall, EHRWorld-14B’s\nsmaller gap between global and high-sensitivity\nperformance indicates stronger consistency in\ntracking rapidly evolving clinical states.\n5.5\nCase Study\nFigure 4 shows the trajectory prediction for a pa-\ntient with left femoral artery stenosis. We cate-\ngorize prediction accuracy into three levels: pre-\ncise (≤10%), acceptable (10–20%), and deviation\n(>20%). EHRWorld-14B performs well, with most\npredictions falling in the precise range. For more\ndynamic metrics, such as Hematocrit, it maintains\naccuracy within the acceptable range, outperform-\ning other models. This demonstrates EHRWorld’s\nability to maintain state consistency over time, un-\nlike models that tend to deviate.\n6\nConclusion\nIn this study, we evaluated the ability of LLMs\nto function as medical world models for long-\nhorizon clinical simulations. We found that mod-\nels incorporating medical knowledge struggle with\n8\n"}, {"page": 9, "text": "state inconsistency and error drift under sequen-\ntial interventions. To address this, we introduced\nEHRWorld and the EHRWorld-110K dataset, en-\nabling causally-aligned modeling of clinical trajec-\ntories. Extensive experiments show that EHRWorld\noutperforms both proprietary and open-source base-\nlines in long-horizon clinical trajectory prediction.\nLimitations\nDespite its strong performance, EHRWorld has sev-\neral limitations. First, although the proposed train-\ning paradigm substantially reduces long-horizon\nerror accumulation, the model still relies on autore-\ngressive generation, which can propagate residual\ninaccuracies over extended rollouts. Second, our\nevaluation primarily focuses on trajectory-level fi-\ndelity, stability, and consistency under sequential\ninterventions, and does not directly assess down-\nstream clinical utility such as treatment optimiza-\ntion, policy learning, or real-world decision sup-\nport.\nFuture work could extend the evaluation\nto decision-centric settings, including counterfac-\ntual treatment comparison and clinician-in-the-loop\nstudies, to better understand the practical implica-\ntions of medical world models.\nEthical Considerations\nThis work involves modeling sensitive clinical pro-\ncesses and therefore raises important ethical con-\nsiderations. All data used to construct EHRWorld-\n110K are de-identified and processed in compli-\nance with established privacy and data protection\nstandards. However, models trained on observa-\ntional clinical data may reflect existing practice\npatterns and implicit biases, which could influence\nsimulated outcomes if interpreted without caution.\nWe emphasize that EHRWorld is designed as a\nresearch-oriented medical world model for simula-\ntion and analysis, rather than a system intended for\nautonomous or real-time clinical decision-making.\nAny application beyond research settings would\nrequire careful validation, transparency, and human\noversight.\nReferences\nYoshua Bengio and 1 others. 2019. From system 1\ndeep learning to system 2 deep learning. In Neural\nInformation Processing Systems.\nJunqiao Chen, David Chun, Milesh Patel, Epson Chi-\nang, and Jesse James. 2019. The validity of synthetic\nclinical data: a validation study of a leading syn-\nthetic data generator (synthea) using clinical quality\nmeasures. BMC medical informatics and decision\nmaking, 19(1):44.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang,\nWanlong Liu, Rongsheng Wang, Jianye Hou, and\nBenyou Wang. 2024. Huatuogpt-o1, towards med-\nical complex reasoning with llms. arXiv preprint\narXiv:2412.18925.\nGoogle DeepMind and 1 others. 2023. Gemini: A fam-\nily of highly capable multimodal models. https:\n//arxiv.org/abs/2312.11805.\nPreprint,\narXiv:2312.11805.\nDeepSeek-AI. 2025. Deepseek-v3.2: Pushing the fron-\ntier of open large language models.\nChengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan\nJia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya\nDang, Tianpeng Li, and 1 others. 2025. Baichuan-\nm2: Scaling medical capability with large verifier\nsystem. arXiv preprint arXiv:2509.02208.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783.\nLinlu Gong, Ante Wang, Yunghwei Lai, Weizhi Ma,\nand Yang Liu. 2025. The dialogue that heals: A\ncomprehensive evaluation of doctor agents’ inquiry\ncapability. arXiv preprint arXiv:2509.24958.\nDavid Ha and Jürgen Schmidhuber. 2018. World mod-\nels. arXiv preprint arXiv:1803.10122, 2(3).\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mo-\nhammad Norouzi. 2019. Dream to control: Learn-\ning behaviors by latent imagination. arXiv preprint\narXiv:1912.01603.\nHaibo Jin, Haoxuan Che, Yi Lin, and Hao Chen. 2024.\nPromptmrg: Diagnosis-driven prompts for medical\nreport generation. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 38, pages\n2607–2615.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nDaeun Kyung, Hyunseung Chung, Seongsu Bae, Jiho\nKim, Jae Ho Sohn, Taerim Kim, Soo Kyung Kim,\nand Edward Choi. 2025.\nPatientsim: A persona-\ndriven simulator for realistic doctor-patient interac-\ntions. arXiv preprint arXiv:2505.17818.\nYann LeCun. 2022. A path towards autonomous ma-\nchine intelligence version 0.9. 2, 2022-06-27. Open\nReview, 62(1):1–62.\n9\n"}, {"page": 10, "text": "Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-\nmed: Training a large language-and-vision assistant\nfor biomedicine in one day. Advances in Neural In-\nformation Processing Systems, 36:28541–28564.\nYikuan Li, Shishir Rao, José Roberto Ayala Solares,\nAbdelaali Hassaine, Rema Ramakrishnan, Dexter\nCanoy, Yajie Zhu, Kazem Rahimi, and Gholamreza\nSalimi-Khorshidi. 2020. Behrt: transformer for elec-\ntronic health records. Scientific reports, 10(1):7155.\nIlya Loshchilov and Frank Hutter. 2019.\nDe-\ncoupled weight decay regularization.\nPreprint,\narXiv:1711.05101.\nMiniMax. 2025. Minimax-m1: Scaling test-time com-\npute efficiently with lightning attention. Preprint,\narXiv:2506.13585.\nLinjie Mu, Yannian Gu, Zhongzhen Huang, Yakun Zhu,\nShaoting Zhang, and Xiaofan Zhang. 2025. Medceg:\nReinforcing verifiable medical reasoning with critical\nevidence graph. arXiv preprint arXiv:2512.13510.\nGyutaek Oh, Sangjoon Park, and Byung-Hoon Kim.\n2025. Automedic: An automated evaluation frame-\nwork for clinical conversational agents with medical\ndataset grounding. Preprint, arXiv:2512.10195.\nOpenAI. 2025. Gpt-5 system card. Technical report,\nOpenAI. Accessed: 2026-01-02.\nOpenAI. 2025. gpt-oss-120b & gpt-oss-20b model card.\nPreprint, arXiv:2508.10925.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD international conference on knowl-\nedge discovery & data mining, pages 3505–3506.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and\nDegui Zhi. 2021. Med-bert: pretrained contextual-\nized embeddings on large-scale structured electronic\nhealth records for disease prediction. NPJ digital\nmedicine, 4(1):86.\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo\nReis, Jeffrey Jopling, and Michael Moor. 2024.\nAgentclinic: a multimodal agent benchmark to eval-\nuate ai in simulated clinical environments. arXiv\npreprint arXiv:2405.07960.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nand 1 others. 2025. Medgemma technical report.\narXiv preprint arXiv:2507.05201.\nGLM Team, Aohan Zeng, Xin Lv, Qinkai Zheng,\nZhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang\nWang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong\nWang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin\nCao, Xiaohan Zhang, Xuancheng Huang, Yao Wei,\nand 152 others. 2025. Glm-4.5: Agentic, reason-\ning, and coding (arc) foundation models. Preprint,\narXiv:2508.06471.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nZhanyu Wang, Lingqiao Liu, Lei Wang, and Luping\nZhou. 2023. R2gengpt: Radiology report generation\nwith frozen llms. Meta-Radiology, 1(3):100033.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayi-\nheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, and 41 oth-\ners. 2025. Qwen3 technical report. arXiv preprint\narXiv:2505.09388.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jian-\nhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,\nJingren Zhou, Junyang Lin, Kai Dang, and 22 oth-\ners. 2024. Qwen2.5 technical report. arXiv preprint\narXiv:2412.15115.\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue\nSun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos\nKozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark\nBarrett, and Ying Sheng. 2024. Sglang: Efficient\nexecution of structured language model programs.\nPreprint, arXiv:2312.07104.\nYakun Zhu, Zhongzhen Huang, Qianhan Feng, Linjie\nMu, Yannian Gu, Shaoting Zhang, Qi Dou, and Xiao-\nfan Zhang. 2025. Cp-env: Evaluating large language\nmodels on clinical pathways in a controllable hospital\nenvironment. Preprint, arXiv:2512.10206.\n10\n"}, {"page": 11, "text": "A\nDetails on Data Construction\nIn this section, we provide a comprehensive break-\ndown of the data construction process for the\nEHRWorld-110K dataset, focusing on the extrac-\ntion of static patient context and the statistical char-\nacteristics of the processed clinical trajectories.\nA.1\nStatic Information Extraction\nTo obtain a structured representation of patient de-\nmographics and clinical context from unstructured\ntextual records, we employed the Qwen3-235B-\nA22B-Instruct (Yang et al., 2025). The extraction\nprocess targeted raw discharge summaries from\nMIMIC-IV to identify key static attributes, includ-\ning age, gender, allergy history, and a hierarchical\nset of diagnoses (primary vs. secondary).\nWe designed a specific system prompt to guide\nthe LLM in parsing these narratives into a standard-\nized JSON schema. The prompt strictly constrains\nthe model to extract only diagnoses relevant to the\ncurrent hospital admission, filtering out historical\nconditions unless they were actively treated. The\nfull prompt template used in our pipeline is pre-\nsented below:\nStatic Information Extraction\n# Role\nYou are an expert Clinical Data Analyst and Medical\nTerminology Specialist. Your task is to extract spe-\ncific data points from a patient’s Discharge Record\nand structure them into a precise JSON format.\n# Instructions\nAnalyze the provided text and extract the information\ninto the fields defined below.\n• 1. Basic Information: Extract Age, Gender, Al-\nlergy History, and Chief Complaint.\n• 2. History Information: Summarize PMH, Fam-\nily History, and Social History.\n• 3. Diagnosis Results:\n– Extract ONLY diagnoses addressed during this\nstay.\n– Split into Primary Diagnosis and Secondary\nDiagnoses (sorted chronologically).\n# Output Format (JSON Template)\n{\n\"Basic Information\": \"String...\",\n\"History Information\": \"String...\",\n\"Diagnosis Results\": {\n\"Primary Diagnosis\": { \"Content\": \"...\",\n\"Reason\": \"...\" },\n\"Secondary Diagnoses\": [\n{ \"Content\": \"...\", \"Reason\": \"...\" }\n]\n}\n}\nA.2\nData Filtering and Partitioning\nTo ensure the robustness of the benchmark and\nmitigate the noise inherent in raw electronic health\nrecords, we implemented a rigorous two-stage data\nprocessing pipeline consisting of statistical filtering\nand patient-centric partitioning.\nStatistical Filtering and Outlier Removal.\nClin-\nical event sequences often exhibit long-tail distribu-\ntions that introduce computational inefficiency and\nmodeling instability. To address this, we first com-\nputed the population-level statistics for key metrics,\nincluding Length of Stay (LOS) and the frequency\nof distinct event types (laboratory tests, microbiol-\nogy, and medication administrations). We applied\na truncation strategy where any admission episode\ncontaining a metric exceeding the 90th percentile\nwas excluded. Furthermore, we restricted the total\nevent count per episode to the 10th–90th percentile\nrange, removing sequences that were either unin-\nformatively short or excessively long. To guarantee\nthat all samples contained meaningful therapeutic\nsignals, we strictly filtered out episodes with zero\nmedication administration (EMAR) records.\nPatient-Centric Splitting Strategy.\nTo rigor-\nously evaluate the model’s ability to generalize to\nunseen subjects, we adopted a patient-centric par-\ntitioning strategy rather than a random admission-\nlevel split. We grouped all data by unique patient\nidentifiers (subject_id) and assigned patients\nexclusively to either the training or the test set. This\nensures that all hospitalization episodes for a given\nindividual reside in the same partition, thereby pre-\nventing data leakage where the model might other-\nwise exploit patient-specific idiosyncrasies or his-\ntorical correlations across different admissions.\nA.3\nData Statistics and Distribution\nThe EHRWorld-110K dataset exhibits diverse dis-\ntributions across key demographic and clinical met-\nrics, as shown in Figure 6. These distributions are\ncritical for characterizing the variability and clini-\ncal fidelity of the data.\nPatient Age.\nThe dataset spans a broad age spec-\ntrum, with a predominant concentration of patients\nbetween 40 and 80 years old. A pronounced peak\nis observed in the 50–60 age bracket, reflecting a\nsignificant representation of middle-aged and el-\nderly patients, which is typical for hospital-based\ncohorts.\n11\n"}, {"page": 12, "text": "20\n40\n60\n80\n100\nAge (years)\n0\n1000\n2000\n3000\n4000\nMax: 103.0\nAvg: 62.0\nMed: 64.0\nAge Distribution\n0\n2\n4\n6\n8\n10\nLOS (days)\n102\n103\n104\nMax: 11.1\nAvg: 3.2\nMed: 2.9\nLength of Stay\n100\n200\n300\n400\n500\nCount\n102\n103\nMax: 519.0\nAvg: 175.1\nMed: 154.0\nTotal Events\n0\n50\n100\n150\n200\n250\n300\n350\n400\nCount\n102\n103\nMax: 392.0\nAvg: 122.9\nMed: 107.0\nLab Events\n0\n2\n4\n6\n8\n10\nCount\n103\n104\nMax: 10.0\nAvg: 1.8\nMed: 1.0\nMicrobiology\n0\n5000\n10000\n15000\n20000\nEvents/Day\n100\n101\n102\n103\n104\n105\nMax: 23800.0\nAvg: 65.5\nMed: 58.1\nEvent Intensity\n0\n2\n4\n6\n8\nCount\n100\n101\n102\n103\n104\nMax: 9.0\nAvg: 0.6\nMed: 0.0\nProcedures\n0\n20\n40\n60\n80\n100\n120\nCount\n103\nMax: 132.0\nAvg: 46.3\nMed: 39.0\nMedication (EMAR)\n0\n2\n4\n6\n8\n10\n12\n14\nCount\n100\n101\n102\n103\n104\nMax: 13.0\nAvg: 2.2\nMed: 2.0\nOMR\nFigure 5: Distributions of demographics and clinical event statistics for the processed EHRWorld-110K dataset.\nThe histograms illustrate patient age, length of stay (LOS), and event counts across diverse modalities (e.g., Lab\nEvents, Medications, Procedures). Additionally, we plot Event Intensity (total events divided by LOS) to represent\nthe density of clinical activities. Inset boxes report descriptive statistics (maximum, mean, and median) for each\nmetric. Note that the y-axes for LOS and event counts use a logarithmic scale to visualize the long-tail distributions.\nLength of Stay (LOS).\nThe LOS distribution\nfollows a significant long-tail pattern. While the\nmajority of hospitalizations are relatively brief (1–\n3 days), a subset of patients requires extended care,\nwith some episodes exceeding 100 days.\nThis\nskewed distribution is visualized using a logarith-\nmic scale to effectively capture the contrast be-\ntween routine short-term admissions and protracted\nhospitalization episodes.\nClinical Event Counts.\nEvent frequencies vary\nsignificantly by modality; laboratory tests and\nmedication administrations are the most volumi-\nnous, whereas procedural events are comparatively\nsparse. Similar to LOS, these counts exhibit a\nheavy-tailed distribution: while most episodes in-\nvolve moderate clinical activity, a distinct fraction\nof patients undergoes extensive interventions, ac-\ncumulating high volumes of diagnostic and thera-\npeutic records.\nEvent Intensity.\nEvent Intensity, defined as the\ntotal number of clinical events normalized by the\nLOS, serves as a proxy for the density of medi-\ncal care. This metric reveals that episodes char-\nacterized by high event intensity are often associ-\nated with longer hospitalizations, indicating that\npatients requiring prolonged treatment also demand\nmore frequent monitoring and intervention.\nThe descriptive statistics (mean, median, and\nmaximum) presented in the inset boxes of Figure 6\nfurther quantify this variability. For instance, the\ndivergence between the mean LOS (≈5 days) and\nthe maximum LOS (> 100 days) underscores the\nwide range of patient care trajectories. Collectively,\nthese statistics highlight the heterogeneity of the\nEHRWorld-110K dataset, providing a rich and var-\nied foundation for evaluating models in clinical\ndata analysis.\n12\n"}, {"page": 13, "text": "B\nEHRWorld Simulation Algorithm\nThis appendix provides a procedural description of\nthe simulation process to complement the formula-\ntion in Section 4. The algorithm summarizes how\npatient states are updated over discrete simulation\nsteps given sets of clinical actions, following the\ndual-mode prediction mechanism for inquiry and\nintervention events. The procedure is intended to\nclarify the execution flow of the world model and\ndoes not introduce additional modeling assump-\ntions beyond those described in the main text.\nAlgorithm 1: Sequential Simulation\nInput: Initial patient context (d, Y), initial\ntimestamp τ1, trained model Pθ\nOutput: Simulated clinical trajectory H\nInitialize H1 ←∅;\nInitialize state S1 ←⟨τ1, d, Y, H1⟩;\nfor t = 1, 2, . . . do\nReceive a set of clinical actions\nAt = {a(1)\nt , . . . , a(N)\nt\n};\nforeach a(j)\nt\n∈At do\nif a(j)\nt\n∈Ainq then\nSample observable outcome\nv(j)\nt\n∼Pθ(v | St, At);\nelse if a(j)\nt\n∈Aint then\nSet v(j)\nt\n←∅;\nConstruct event set\nEt ←{(a(j)\nt , v(j)\nt )}N\nj=1;\nUpdate history Ht+1 ←Ht ∪{Et};\nUpdate state\nSt+1 ←⟨τt+1, d, Y, Ht+1⟩;\nreturn H\nC\nDetailed Evaluation Metrics\nThis appendix presents the formal mathematical\ndefinitions and methodologies employed to eval-\nuate the fidelity of the EHRWorld World Model.\nOur evaluation framework is bifurcated into numer-\nical precision for continuous physiological vari-\nables and semantic accuracy for discrete clinical\nevents.\nC.1\nNumerical Metrics\nTo assess the generation of continuous values (e.g.,\nvital signs, laboratory results), we utilize two com-\nplementary metrics that evaluate point-wise accu-\nracy and robustness to numerical instability. Let\nD = {(yi, ˆyi)}N\ni=1 denote a set of N pairs consist-\ning of ground truth values yi and model predictions\nˆyi.\n1. Success Rate at Threshold X (S@X).\nThis\nmetric quantifies the proportion of generated values\nfalling within a clinically acceptable relative error\nmargin X (e.g., 10%, 25%). It serves as a direct\nproxy for clinical utility. We first compute the\nrelative error Ei for each sample:\nEi =\n\f\f\f\f\nyi −ˆyi\nyi\n\f\f\f\f\n(6)\nThe Success Rate is subsequently defined as:\nS@X = 1\nN\nN\nX\ni=1\nI\n\u0012\nEi ≤X\n100\n\u0013\n(7)\nwhere I(·) represents the indicator function. We\nreport S@10 (Strict), S@15 (Intermediate), and\nS@25 (Standard). Higher values denote superior\naccuracy.\n2. Symmetric Mean Absolute Percentage Er-\nror (SMAPE).\nStandard MAPE can be unstable\nwhen ground truth values approach zero and asym-\nmetrically penalizes over-predictions. To mitigate\nthis, we employ SMAPE (denoted as Err in experi-\nmental results), which ensures symmetric penalties\nand numerical stability:\nSMAPE = 100%\nN\nN\nX\ni=1\n2 · |yi −ˆyi|\n|yi| + |ˆyi| + ϵ\n(8)\nwhere ϵ = 10−10 is a smoothing term included to\nprevent division by zero. Lower values indicate\nhigher numerical precision.\nC.2\nHigh-Sensitivity Analysis\nClinical trajectories frequently involve rapid phys-\niological shifts rather than steady states. Global\naggregate metrics may obscure model performance\nduring these critical episodes.\nTo evaluate the\nmodel’s responsiveness to acute changes, we iden-\ntify a subset of high-sensitivity events, denoted as\nHsens.\nAn event at time t is included in this subset if the\nground truth value exhibits a substantial deviation\nrelative to the preceding time step t −1:\nHsens =\n\u001a\nt |\n\f\f\f\f\nyt −yt−1\nyt−1\n\f\f\f\f > 0.5\n\u001b\n(9)\nWe strictly report S@25 and SMAPE for this subset\nto quantify the model’s capability in simulating\ndynamic clinical transitions (∆> 50%).\n13\n"}, {"page": 14, "text": "EHRWorld-14B\nGPT-OSS-120B\nQwen3-235B-A22B\nMedGemma-27B\nMiniMax-M2.1\nQwen3-Next-80B-A3B\n0\n1\n2\n3\n4\n5\nAvg Time per Case (s)\n1.29s\n1.60s\n1.53s\n2.60s\n4.52s\n3.57s\nAvg Time/Case (s)\nThroughput (tok/s)\n0\n5k\n10k\n15k\n20k\n25k\nThroughput (tok/s)\n23,318\n14,571\n13,539\n7,988\n6,081\n5,808\nFigure 6: Comparison of inference time and token\nthroughput across different models. EHRWorld-14B\ndemonstrates superior computational efficiency, achiev-\ning the fastest generation speed while maintaining the\nhighest volume compared to all baselines.\nC.3\nLabel Event Metrics\nFor discrete clinical outputs, such as microbiology\nculture results or qualitative nursing assessments,\nthe output space is categorical. We evaluate the\nsemantic correctness of the generated descriptions\nusing standard classification metrics:\nPrecision =\nTP\nTP + FP ,\nRecall =\nTP\nTP + FN\n(10)\nF1 = 2 · Precision · Recall\nPrecision + Recall\n(11)\nwhere TP, FP, and FN denote True Positives,\nFalse Positives, and False Negatives, respectively.\nD\nAdditional Analysis\nD.1\nInference Efficiency\nTo assess deployment practicality for large-scale\nclinical simulation, we benchmark inference effi-\nciency on a standardized cluster equipped with 8×\nNVIDIA H100 GPUs. We restrict our compari-\nson to competitive models that are viable for de-\nployment on this specific hardware configuration.\nAll models are served using the SGLang frame-\nwork (Zheng et al., 2024) with a request concur-\nrency set to 64. We report two key metrics: the\naverage wall-clock time required to generate a full\nclinical case (Latency) and the aggregate system\nthroughput (Tokens/s).\nAs illustrated in Figure 6, EHRWorld-14B es-\ntablishes a clear efficiency advantage across both\nmetrics: Latency reduction. Our model achieves\nthe lowest average generation time of 1.29 seconds\nper case. This represents a significant speedup\ncompared to domain-specific baselines such as\nMedGemma-27B-IT, which requires 2.60 seconds\nper case—nearly double the inference time. Fur-\nthermore, EHRWorld-14B maintains lower latency\nthan massive general-purpose models, outperform-\ning GPT-OSS-120B (1.60s) and the MoE-based\nQwen3-235B-A22B-Instruct (1.53s), despite their\nhighly optimized architectures. Throughput supe-\nriority. In terms of generation volume, EHRWorld-\n14B reaches a peak throughput of 23,318 token-\ns/second.\nThis is substantially higher than the\nrunner-up, GPT-OSS-120B (14,571 tokens/s), and\noffers an approximate 3× to 4× improvement\nover the heavier dense models like MiniMax-\nM2.1 (6,081 tokens/s) and Qwen3-Next-80B-A3B-\nInstruct (5,808 tokens/s).\nThese results confirm that EHRWorld-14B occu-\npies a “sweet spot” in the trade-off between model\nscale and computational cost. By delivering high-\nfidelity clinical simulations with sub-1.5s latency\nand maximal throughput, it enables cost-effective,\nreal-time applications that are computationally pro-\nhibitive for larger baseline models.\nD.2\nHyperparameter Sensitivity\nWe examine the impact of the peak learning rate\nand training epochs on EHRWorld-14B perfor-\nmance. The model shows sensitivity to these hy-\nperparameters, as summarized below:\n• Learning Rate: 1 × 10−6 achieves optimal\nperformance. Higher rates cause instability,\nwhile lower rates result in slow convergence.\n• Epochs: Performance plateaus at 3 epochs,\nwith further training yielding minimal gains\nand increasing computational cost.\nTable 3: Ablation study on EHRWorld-14B. Sensitivity\nto Learning Rate (LR) and Epochs is shown, with the\nfinal settings highlighted in color .\nParameter\nValue\nS@25\nLabel F1\nAvg Score\nLearning Rate (Epochs fixed at 3)\n1 × 10−7\n0.652\n0.885\n0.720\n5 × 10−7\n0.698\n0.902\n0.751\n1 × 10−6\n0.716\n0.913\n0.765\nLR\n5 × 10−6\n0.645\n0.860\n0.702\nTraining Epochs (LR fixed at 1 × 10−6)\n1\n0.640\n0.856\n0.705\n2\n0.692\n0.898\n0.748\n3\n0.716\n0.913\n0.765\nEpochs\n5\n0.712\n0.911\n0.761\n14\n"}]}