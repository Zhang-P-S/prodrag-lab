{"doc_id": "arxiv:2602.04982", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04982.pdf", "meta": {"doc_id": "arxiv:2602.04982", "source": "arxiv", "arxiv_id": "2602.04982", "title": "BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations", "authors": ["Deepak Gupta", "Davis Bartels", "Dina Demner-Fushman"], "published": "2026-02-04T19:13:43Z", "updated": "2026-02-06T21:37:31Z", "summary": "With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04982v2", "url_pdf": "https://arxiv.org/pdf/2602.04982.pdf", "meta_path": "data/raw/arxiv/meta/2602.04982.json", "sha256": "a3c0cb317f56ab8572a20c3a786f3ab612fe98cf136e316b85a21009b3ffc92e", "status": "ok", "fetched_at": "2026-02-18T02:19:45.824824+00:00"}, "pages": [{"page": 1, "text": "BioACE: An Automated Framework for Biomedical\nAnswer and Citation Evaluations\nDeepak Gupta, Davis Bartels and Dina Demner-Fushman\nDivision of Intramural Research, National Library of Medicine\n8600 Rockville Pike, Bethesda, 20894, MD, USA.\n*Corresponding author(s). E-mail(s): deepak.gupta@nih.gov;\nContributing authors: davis.bartels@nih.gov; ddemner@mail.nih.gov;\nAbstract\nWith the increasing use of large language models (LLMs) for generating answers\nto biomedical questions, it is crucial to evaluate the quality of the generated\nanswers and the references provided to support the facts in the generated answers.\nEvaluation of text generated by LLMs remains a challenge for question answering,\nretrieval-augmented generation (RAG), summarization, and many other natural\nlanguage processing tasks in the biomedical domain, due to the requirements of\nexpert assessment to verify consistency with the scientific literature and complex\nmedical terminology. In this work, we propose BioACE, an automated framework\nfor evaluating biomedical answers and citations against the facts stated in the\nanswers. The proposed BioACE framework considers multiple aspects, including\ncompleteness, correctness, precision, and recall, in relation to the ground-truth\nnuggets for answer evaluation. We developed automated approaches to evaluate\neach of the aforementioned aspects and performed extensive experiments to\nassess and analyze their correlation with human evaluations. In addition, we\nconsidered multiple existing approaches, such as natural language inference (NLI)\nand pre-trained language models and LLMs, to evaluate the quality of evidence\nprovided to support the generated answers in the form of citations into biomedical\nliterature. With the detailed experiments and analysis, we provide the best\napproaches for biomedical answer and citation evaluation as a part of BioACE\n(https://github.com/deepaknlp/BioACE) evaluation package.\nKeywords: biomedical question answering, answer attribution, answer evaluation\n1\narXiv:2602.04982v2  [cs.CL]  6 Feb 2026\n"}, {"page": 2, "text": "1 Introduction\nEvaluation of answers to clinical questions generated by AI models remains a manual\ntask as no reliable automated approaches that approximate human judgments exist. In\naddition to being expensive and time-consuming, the size of data manageable by human\nannotators might not be sufficient to detect the differences in model performance [1].\nThe evaluation bottleneck must be eliminated to enable the use of AI models in\npractical applications, e.g., providing information to clinicians and patients, as well as\ndeveloping better systems. For the latter, the automated metrics only need to agree\nwith the direction of human judgments, i.e., reliably inform the researchers if one\nof the systems performs a given task better than the other, or if the changes to the\nsystem improved the results. Conversely, the use of systems in real-life applications\nrequires confidence in each answer along several axes, such as factuality/correctness,\ncompleteness, succinctness, grammaticality, coherency/consistency, and fluency.\nAs grammaticality, cohesion, and fluency of the answers generated by Large Lan-\nguage Models (LLMs) resemble human [2], other evaluation aspects, particularly\nfactuality, become more of a concern. Approaches to improving the factuality of the\ngenerated answer are converging on providing evidence to support each statement with\nreferences into reliable sources provided to the models before generation (Retrieval\nAugmented Generation) or found retroactively to support the generated answer. Evalu-\nation of the support provided by the referenced documents was an intrinsic part of the\nlarge-scale question answering evaluations in the past. In the Text Retrieval Conference\n(TREC) evaluations, the systems provided each factual answer along with a document\nidentifier, and the answers were (manually) judged to be correct only if the text submit-\nted by the system answered the question and the document supported the answer [3].\nThe first generation of automated approaches to the evaluation of the answer text relied\nheavily on the lexical overlap of the reference standards and system-generated texts..\nWhile showing good correlations with human judgments for the answers extracted\nfrom relevant documents, the metrics were criticized for their inability to capture\nparaphrases, synonymy, and meaning of the text [4]. These shortcomings became more\ncritical with the advent of generative models. Efforts to overcome the shortcomings\nof measuring lexical similarity and rely more on semantics include: augmenting the\nlexicon with embeddings [5], natural language inference (NLI) [6], matching answers\non key facts (’nuggets’) [7], and using LLMs as agents that evaluate the answers [8],\noften using a question answering approach [9].\nIn this work, we introduce BioACE an evaluation framework for assessing the\nanswer and citations against the stated facts in the answers. For answer evaluation, we\npropose diverse aspects (completeness, correctness, precision, and recall against the\nground-truth answer) to evaluate the generated answer. In particular, we analyzed\nthe performance of the nuggetization approach for nugget-based comparison between\nreference and generated answers, leveraging Llama for automated nugget extraction\nand measuring similarity between the nuggets extracted from the automated responses\nand the reference answers. For completeness, we analyzed the performance of various\npretrained and large language models. Similar to completeness, we evaluated the results\nusing language models and the semantic similarity between the answer and relevant\ndocuments to assess the answer’s correctness. Extensive experiments are performed\n2\n"}, {"page": 3, "text": "for each answer evaluation aspect to determine the best automated approach that\nyields the highest correlation with human evaluation. We also compared the best\nmethod, which yields the highest correlations, with existing automated approaches. We\nperformed all experiments on the BioGen collection [10] that consists of sixty-five (65)\nquestions. We consider the first 40 questions as the test dataset, which is the same as\nthe MedAESQA [11] dataset. The following five questions are considered the validation\ndataset, which we refer to as the BioGen2024-val set. The last 20 questions serve as\nthe training set, which we refer to as BioGen2024-train. Detailed experiments and\nanalyses are also performed to determine the best approach for automatically assessing\ncitation quality. In particular, we analyzed existing methods for comparing answer\nassertions with cited documents and conducted experiments across a variety of setups\nto determine the capability of large language models for the citation evaluation task.\nSimilar to the answer evaluation, we compared the best citation evaluation method\nwith the existing techniques and drew our findings.\nWe summarize the contributions of this work as follows:\n1. We propose automated approaches for evaluating answers to biomedical questions.\nThe proposed approaches consider multiple aspects of the answer to determine\nits quality, such as correctness, completeness, and the coverage of facts. The facts\ntake the form of answer nuggets both in the generated and the human-annotated\nanswer. The proposed approaches are compared with a variety of models, ranging\nfrom standard machine learning models (SVM, Logistic regression) to the latest\nLLM-based models (zero-shot and fine-tuning approaches for Llama, Mistral, Qwen,\netc.).\n2. We experimented with a variety of approaches and settings to determine the best\nmodel that can assess whether the provided citation into literature supports the\nfacts stated in the answer. These approaches are compared and analyzed on the\nhuman-annotated MedAESQA benchmark dataset to evaluate their effectiveness for\nthe citation evaluation task.\n3. Based on our extensive experiments with the answer and citation evaluation con-\nsidering multiple aspects, settings, and models, we propose BioACE an answer and\ncitation evaluation framework for biomedical question answering. This automatic\nframework includes the best models, parameters, and settings that obtained the\nhighest performance on MedAESQA and shows strong correlation with the human\nannotation.\n2 Results\nThe overarching goal of this study is to analyze the effectiveness of automated evaluation\napproaches for biomedical answers and their citations, and to assess their alignment\nwith human judgments. Before assembling the best approaches to evaluating multiple\naspects of the answers and integrating evidence into an evaluation framework, we\npresent the evaluation results for each component.\n3\n"}, {"page": 4, "text": "2.1 Answer Evaluation\nFor answer evaluation, we experimented with multiple aspects, and the results for each\nof those aspects are provided below:\n2.1.1 Nugget Precision and Recall\nThe detailed results in comparing the ground-truth and system-generated nuggets\nof the answer are shown in Table 1. We found that the embedding model\nsup-simcse-roberta-large\noutperforms\n(precision:\n44.68,\nrecall:\n58.39)\nthe\nall-MiniLM-L6-v2 and all-mpnet-base-v2. We found the optimal probability\nthreshold for all-MiniLM-L6-v2 as 0.6267, sup-simcse-roberta-large as 0.6035,\nand all-mpnet-base-v2 as 0.6211. We recorded the objective(s) value of F1-\nscore (average similarity) to be 63.45 (65.25), 55.72 (73.36) and 62.68 (67.98)\nfor the all-MiniLM-L6-v2, sup-simcse-roberta-large and all-mpnet-base-v2,\nrespectively.\nEmbedding\nModel\nPrecsion\nRecall\nF-Score\nall-MiniLM-L6-v2\n36.43\n47.74\n41.33\nall-mpnet-base-v2\n37.57\n49.26\n42.63\nsup-simcse-roberta-large\n44.68\n58.39\n50.62\nTable 1: Performance comparison of the different models on\ncomparing the ground-truth and generated answer nuggets in\nterms of precision, recall, and F1-scores.\n2.1.2 Completeness\nThe detailed performance of the PLMs and LLMs is demonstrated in Table 2. The PLMs\nwere fine-tuned on the BioGen2024-train dataset and the best model parameters are\nbased on the performance on the BioGen2024-val dataset. The best model is used to\nevaluate the performance on the MedAESQA dataset. RoBERTaLarge outperformed the\nother three pre-trained language models, achieving the highest weighted F1-score (75.37)\nacross the individual classes (Required, Unnecessary, Borderline, and Inappropriate).\nMost of the PLMs achieved a competitive precision score, but RoBERTaLarge recorded\nthe highest recall score of 74.91, while its counterpart, BERTLarge, achieved a recall\nscore of 62.57. We conducted detailed experiments using five different LLMs in two\ndistinct settings: zero-shot and fine-tuned. In the zero-shot setting, we observe that\nLlama-3.3-70B-Instruct model achieves the highest weighted F1-score of 76.20, while in\nthe same Llama family model, Llama-3-8B-Instruct achieved only an F1-score of 63.70.\nAmong all the LLMs, Mistral-7B-Instruct-v0.3 achieved the lowest F1-score of 63.41. In\nthe fine-tuned setting of the LLMs Llama-3-8B-Instruct, achieved the highest F1-score\nof 72.67. We observe that the performance of the fine-tuned Llama-3.3-70B-Instruct and\n4\n"}, {"page": 5, "text": "Model\nSetting\nPrecision\nRecall\nF1-Score\nBERTBase\nFine-tuned\n76.96\n67.79\n70.16\nBERTLarge\nFine-tuned\n76.62\n62.57\n66.74\nRoBERTaBase\nFine-tuned\n76\n65.9\n68.64\nRoBERTaLarge\nFine-tuned\n77.88\n74.91\n75.37\nLlama-3.3-70B-Instruct\nZero-shot\n80.72\n73.26\n76.2\nLlama-3.3-70B-Instruct\nFine-tuned\n78.33\n55.71\n63.7\nLlama-3-8B-Instruct\nZero-shot\n72.93\n68.46\n70.49\nLlama-3-8B-Instruct\nFine-tuned\n71.54\n79.56\n72.67\nQwen3-14B\nZero-shot\n76.53\n70.03\n70.5\nQwen3-14B\nFine-tuned\n73.26\n71.96\n71.62\nQwen3-8B\nZero-shot\n74.35\n59.74\n64.89\nQwen3-8B\nFine-tuned\n78.19\n17.45\n27.79\nMistral-7B-Instruct-v0.3\nZero-shot\n69.02\n60.07\n63.41\nMistral-7B-Instruct-v0.3\nFine-tuned\n80.44\n71.27\n68.56\nTable 2: Performance of the different PLMs and LLMs on the task of answer com-\npleteness. The reported results are the weighted precision, recall, and F1-score.\nQwen3-8B decreases from 76.2 to 63.7 and from 64.89 to 27.79, respectively, compared\nto the zero-shot setting. Among all the LLMs, Mistral-7B-Instruct-v0.3 achieved the\nhighest increase of 5.15 points in F1-score compared to its zero-shot setting. In both\nPLMs and LLMs experiments, Llama-3.3-70B-Instruct achieved the highest F1-score\nof 76.20 in the zero-shot setting.\n2.1.3 Correctness\nUnder the classification-based approach, we have outlined the performance of various\nmodels, ranging from classical models to fine-tuned LLMs, in Table 3. We evaluated\nthe performance of these models in terms of precision, recall, F1-score, and area under\nthe curve for the binary classification task of assigning an answer sentence as correct or\nincorrect. For the classical model, SVM achieves the best F1-score of 79.15 compared\nto its counterpart, the logistic regression model (75.70). We extend the experiments\nwith the PLMs and analyze most of the PLMs that performed competitively, achieving\nF-scores in the range of 97.31 (BERTLarge) to 97.65 (RoBERTaLarge). With the LLMs,\nwe conducted experiments in two settings: zero-shot and fine-tuning. For the zero-shot\nsetting, Mistral-7B-Instruct-v0.3 achieves the highest precision score of 67.23 while\nLlama-3-8B-Instruct recorded the lowest precision score of 28.77. Qwen3-8B achieved\nbalanced precision and recall scores of 58.77 and 57.89, respectively, leading to the best-\nperforming model in the zero-shot setting. Under the fine-tuning setup, we observed\nno improvement over the zero-shot setup except for the Llama-3-8B-Instruct model.\nThe Llama-3-8B-Instruct model achieves a gain in terms of F1-score, increasing from\n32.4 to 33.25. Qwen3-8B remains the best-performing model with the highest F1-score\n5\n"}, {"page": 6, "text": "Model Type\nModel Name\nPrecision\nRecall\nF1-Score\nAUC\nClassical\nSVM\n81.08\n79.47\n79.15\n88.52\nLogistic Regression\n75.93\n75.76\n75.7\n84.13\nPLMs\nRoBERTaLarge\n97.65\n97.65\n97.65\n99.34\nRoBERTaBase\n97.48\n97.47\n97.47\n99.26\nBERTLarge\n97.35\n97.32\n97.31\n99.08\nBERTBase\n97.48\n97.46\n97.45\n99.13\nZero-shot\nQwen3-8B\n58.17\n57.89\n57.49\nNA\nLlama-3.3-70B-Instruct\n38.33\n38.93\n38.11\nNA\nLlama-3-8B-Instruct\n28.77\n46.97\n32.4\nNA\nMistral-7B-Instruct-v0.3\n67.23\n51.42\n37.08\nNA\nQwen3-14B\n53.37\n51.26\n42.09\nNA\nFine-tuned\nQwen3-8B\n52.95\n52.05\n48.03\nNA\nLlama-3.3-70B-Instruct\n24.87\n50.01\n33.22\nNA\nLlama-3-8B-Instruct\n58.21\n50.01\n33.25\nNA\nMistral-7B-Instruct-v0.3\n49.55\n49.99\n33.79\nNA\nQwen3-14B\n63.11\n52.73\n40.94\nNA\nTable 3: Performance comparison of the different model types under multiple experi-\nmental settings for the answer correctness evaluation. The reported precision, recall,\nand F1-scores are macro-averaged over the correct and incorrect classes. Since LLM\nexperiments are conducted in a generative manner without extracting the class proba-\nbility, the AUC scores do not apply to these models.\nof 48.03. Fine-tuning could not benefit the models, highlighting the need for model-\ndependent prompt and training configurations (such as objective and hyperparameters)\nto improve performance.\nFor the semantic similarity-based approach, we found that the average positive\nscores for document-answer are much higher than the NLI model probability for the\nsupport score. Similar observations are made for the evidence-answer pair. The NLI\nmodel shows low probability for the negative pair, and on the contrary, the sup-simcse-\nroberta-large model shows high cosine similarity for the negative pair. We also compute\nthe metric accuracy, which refers to the proportion of samples in which the score\nassigned to a positive pair exceeds that of its corresponding negative pair, relative to\nthe total number of samples. We found that the accuracy of cosine similarity for both\ndocument and evidence settings is higher than that of the corresponding NLI-based\nscoring. The detailed results are depicted in Figure 1.\n2.2 Citation Evaluation\nWe provide results for the citation evaluation task for several models with different\narchitectures and with and without fine-tuning. We fine-tune the selected models on\nthe BioGen2024-train dataset and choose the best model parameters based on the\nperformance on the BioGen2024-val dataset. The transformer architecture models\nare fine-tuned using Low-Rank Adaptation (LoRA) [12]. For the models that produce\nscores, alignscore, summacconv, and summaczs, we fit their threshold to the training\nset to maximize F1-Score.\n6\n"}, {"page": 7, "text": "Model\nSetting\nPrecision\nRecall\nF1-Score\nLlama-3.3\nBase\n76.65\n76.64\n76.64\nFLAN-T5\nBase\n74.46\n73.18\n73.81\nFLAN-UL2\nBase\n75.96\n72.23\n74.04\nalignscore\nUnfitted\n75.23\n53.78\n62.70\nattrscore alpaca\nBase\n75.67\n52.04\n61.62\nattrscore flan t5\nBase\n74.18\n75.62\n74.89\nt5 xxl true\nBase\n77.56\n57.34\n65.92\nsummacconv\nUnfitted\n75.80\n34.12\n46.97\nsummaczs\nUnfitted\n70.09\n59.73\n64.49\nLlama-3.3\nFine-tuned\n78.12\n77.85\n77.98\nFLAN-T5\nFine-tuned\n75.89\n74.56\n75.22\nFLAN-UL2\nFine-tuned\n77.34\n73.89\n75.57\nalignscore\nFitted\n76.88\n55.12\n64.12\nattrscore alpaca\nFine-tuned\n77.05\n53.21\n63.02\nattrscore flan t5\nFine-tuned\n75.62\n76.91\n76.26\nt5 xxl true\nFine-tuned\n79.01\n58.97\n67.42\nsummacconv\nFitted\n77.03\n35.84\n48.88\nsummaczs\nFitted\n71.88\n60.95\n65.93\nTable 4: Performance of base and fine-tuned models when tasked with assigning binary\nlabels to a claim sentence and a PubMed title and abstract.\nThe results for the Answer Sentence-Document setting are as shown in table 4\nand table A6 representing the binary and ternary labeling, respectively. The results\nfor the Answer Sentence-maxSimSentence Document setting are as shown in table 5\nand table A3 representing the binary and ternary labeling, respectively. Finally, the\nresults for the Answer Nuggets-Document Nuggets setting are given in table A5, for\nwhich we only evaluated ternary labeling. The results were concentrated across all\nsettings. However, transformer architectures tended to perform slightly higher. Fine-\ntuned and fitted models only yielded modest improvements over their base and unfitted\ncounterparts. Our largest model, Lamma-3.3 performed the best, but that performance\nhad low variance across our settings and prompts.\n3 Discussion\nTo analyze the effectiveness of the proposed answer evaluation metrics, we first analyze\nthe usefulness of the recall metric by comparing it with the recall metric computed from\nthe human-annotated answer sentences. We compare the proposed recall with the recall\nmetric computed by following the R+B setting [11], which involves including required\nor borderline answer sentences to cluster the system-generated answer sentences and\ncompute the recall. The recall in this setup was formally defined as the ratio of clusters\ncontaining answer sentences to the total number of clusters. The answer sentences\nare encoded using the sup-simcse-roberta-large (SimCSE) and all-mpnet-base-v2 (ST)\n7\n"}, {"page": 8, "text": "Model\nSetting\nPrecision\nRecall\nF1-Score\nLlama-3.3\nBase\n77.23\n77.05\n77.14\nFLAN-T5\nBase\n75.10\n73.95\n74.52\nFLAN-UL2\nBase\n76.50\n72.90\n74.66\nalignscore\nUnfitted\n75.80\n54.60\n63.52\nattrscore alpaca\nBase\n76.20\n51.80\n61.72\nattrscore flan t5\nBase\n74.50\n76.00\n75.24\nt5 xxl true\nBase\n78.10\n57.90\n66.46\nsummacconv\nUnfitted\n76.50\n34.80\n47.89\nsummaczs\nUnfitted\n71.00\n60.50\n65.29\nLlama-3.3\nFine-tuned\n78.80\n78.50\n78.65\nFLAN-T5\nFine-tuned\n76.50\n74.90\n75.69\nFLAN-UL2\nFine-tuned\n77.90\n74.50\n76.16\nalignscore\nFitted\n77.30\n55.80\n64.78\nattrscore alpaca\nFine-tuned\n77.80\n52.90\n62.88\nattrscore flan t5\nFine-tuned\n76.20\n77.80\n76.99\nt5 xxl true\nFine-tuned\n79.80\n59.50\n68.10\nsummacconv\nFitted\n77.80\n36.10\n49.24\nsummaczs\nFitted\n72.60\n61.80\n66.73\nTable 5: Performance of base and fine-tuned models when tasked with assigning binary\nlabels to a claim sentence and the sentence with the highest cosine similarity to the\nclaim.\nsentence embedding models. In particular, we rank the answer generation approaches\n(M1-M30) on the MedAESQA dataset using the Recall (R+B)–SimCSE and Recall\n(R+B)–ST metrics. Thereafter, we also rank the answer generation approaches (M1-\nM30) based on the proposed nugget-based recall metric. With the rank of approaches\nusing the human-evaluated recall metrics and the proposed recall metric, we compute\nthe Spearman and Kendall Tau correlation coefficients and report the results (cf. Table\nA1) for all three embedding models, which we experimented with using nugget-based\nprecision-recall computation. We analyzed a strong correlation between the proposed\nand existing recall metrics and observed that the embedding model, all-MiniLM-L6-v2,\nyielded the highest correlation coefficients. The high correlation coefficients signify\nthat the rankings of the system-generated answers are very consistent when using\nthe proposed recall metric compared to the human-evaluated recall metrics. Based on\nthese observations, the proposed automated nugget recall metric could be considered a\npotential recall metric for evaluating the recall of answer generation approaches.\nTo analyze the most effective model for the answer completeness task, we rank the\nanswer generation approaches (M1-M30) on the MedAESQA dataset using the Precision,\nRedundancy, and Harmfulness evaluation metrics introduced in [11]. The metrics\nare calculated based on the human-assessed relevance labels assigned to the answer\nsentences. We replaced the human-assessed relevance labels with the model-predicted\nlabels and computed the metrics. We rank the answer generation approaches (M1-M30)\n8\n"}, {"page": 9, "text": "Sim (doc, answer sent)\nSim (evidence, answer sent)\nNLI (doc, answer sent)\nNLI (evidence, answer sent)\nMethod\n0\n20\n40\n60\n80\nAverage Positive\n71.4\n72.4\n52.4\n35.9\n75.7\n78.6\n52.7\n36.4\n69.0\n70.1\n51.6\n33.6\nSim (doc, answer sent)\nSim (evidence, answer sent)\nNLI (doc, answer sent)\nNLI (evidence, answer sent)\nMethod\n0\n10\n20\n30\n40\n50\nAverage Negative\n25.7\n20.7\n2.2\n0.4\n49.9\n40.8\n3.5\n1.4\n23.4\n17.1\n2.4\n0.8\nSim (doc, answer sent)\nSim (evidence, answer sent)\nNLI (doc, answer sent)\nNLI (evidence, answer sent)\nMethod\n0\n20\n40\n60\n80\n100\nAccuracy (Pos>Neg)\n97.0\n98.0\n62.6\n43.2\n93.2\n97.4\n61.6\n43.2\n97.1\n98.2\n62.0\n40.2\nSim (doc, answer sent)\nSim (evidence, answer sent)\nNLI (doc, answer sent)\nNLI (evidence, answer sent)\nMethod\n0\n20\n40\n60\n80\n100\nAUC\n96.8\n97.4\n80.3\n71.4\n91.9\n96.5\n79.8\n71.0\n96.3\n96.9\n80.1\n69.5\nall-mpnet-base-v2\nsup-simcse-roberta-large\nall-MiniLM-L6-v2\nFig. 1: Answer correctness performance comparison of the embedding models, their\nderived cosine similarity, and the NLI model on multiple evaluation metrics.\nbased on both the model-predicted labels and the human-assessed relevance labels.\nWith these two sets of system rankings, we compute the Spearman and Kendals Tau\ncorrelation coefficients and report the results (cf. Table A2) for all the PLMs and\nLLMs that we experimented with for answer completeness evaluation. We obtained the\nhighest correlation with the Llama-3.3-70B-Instruct (zero-shot) model for the precision\n(answer completeness) metric, which is one of the key metrics for assessing answer\ncompleteness. RoBERTaLarge achieved the highest correlation for the Redundancy\nmetric. The Llama-3.3-70B-Instruct (zero-shot) model achieved the highest correlation\nfor Precision and a competitive correlation for the Redundancy and Harmfulness\nmetrics, and could be considered a potential metric for evaluating answer completeness.\nIn another analysis, we aim to assess the effectiveness of the trained answer cor-\nrectness model on the MedAESQA dataset. Given an answer sentence and its associated\nmodel-predicted citations, we tokenized the cited document. We prepared a list of\nthree consecutive sentences and their corresponding answer sentence. We then use the\ntrained answer correctness model to predict the correctness label for the pair of answer\nsentence and document. We consider the answer to be correct if its prediction for any\ngrouped document sentence is deemed ‘correct’. We experiment with two different\nsettings. In one, we consider all the cited documents, regardless of their judgment of\n9\n"}, {"page": 10, "text": "the answer sentence. In the other setting, we only consider the documents that were\njudged ‘supported’ by the expert. We evaluate the correctness of each of the answer\nsentences in response to the question and average them over all the answer sentences.\nThe detailed results with the best-performing PLMs are reported in Fig. A6a. The\nSupp(CS) metric achieved a high score in the range of 92-93, indicating that the trained\ncorrectness model exhibits high alignment in judging the correctness of the answer\nsentence with human judgment. In contrast, Supp(AS) achieved a score significantly\nlower than Supp(CS), as some of the documents were unsupported or not relevant, and\nthe correctness model could not have found any grouped sentences in those documents\nto consider the correctness of the answer sentence.\nWe extend this analysis to assess the effectiveness of answer correctness in practical\nscenarios where relevant documents are not readily available. Towards this, given the\nquestion of the MedAESQA dataset, we use the BM25 model to retrieve the top-1000\ndocuments from the latest PubMed baseline index1. Thereafter, we use the ranker\nmodel to rerank the BM-25 retrieved documents. Given the top retrieved documents of\na given question, we aim to assess the correctness of each answer sentence in response to\nthe given question. We reported the performance (cf. Fig. A6b) by considering the top\n10, 20, up to the top 100 documents, providing the Correctness at top-k (Correctness)\nscores. We observed that BERTBase and RoBERTaLarge models quickly reach high\ncorrectness scores as the k value increases. On the other hand, the BERTLarge and\nRoBERTaBase models showed gradual improvement as k increased and reached a\nplateau near k = 90. To further investigate this, we experimented with the NLI model\nand the SimCSE cosine similarity approach with a threshold of 0.75 as observed in\nFig. 1. We reported the results in Fig. A6, where we found that SimCSE and NLI\nmodels also follow a similar pattern to RoBERTaBase and BERTLarge, reaching a\nplateau near k = 90. We found that the BERTLarge model consistently outperforms\nother PLMs across various settings for top-document retrieval and can be used as an\nanswer-correctness evaluation model without requiring document-relevance assessment.\n3.1 Citation Evaluation\nWe did not observe any significant differences among the models, settings, and prompts.\nAdditionally, fine-tuning and fitting the models did not necessarily improve the results,\ncontrary to conventional expectation, and even diminished results, in the case of the\nF1-Score for Llama-3.3 under the Answer Nuggets-Document Nuggets setting. This\ncould be due to catastrophic interference [13] or due to the relatively small size of\nthe training set. For the scoring models, alignscore, summacconv, and summaczs, the\noptimal threshold tended to be as low as possible. This is likely because the data\nhad more positive (attributable) cases, and therefore assigning more ”attributable”\nlabels yielded a higher F1-Score. We perform an analysis of rankings based on the\nautomatically generated labels and compare this to the rankings from MedAESQA test\ndataset results on citation [11]. Correlation coefficients are computed in table A4 and\nindicate low correlation with rankings based on the labels from medical informatics\nexperts. Overall, performance leaves ample room for improvement, demonstrating the\ncomplexity of citation attribution, specifically in the medical domain.\n1https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/\n10\n"}, {"page": 11, "text": "4 Methods\n4.1 Answer Evaluation\nThe quality of an answer to a biomedical question is comprehensively evaluated in a\nplan that covers a range of aspects. In particular, the answers are assessed in terms of\nNugget precision, Nugget recall, completeness, and correctness. To compute the Nugget\nprecision and Nugget recall, medical informatics experts have formulated the ground-\ntruth answers and nuggets for each question following the guidelines discussed in [11].\nWe utilize the Llama 3.3 (70B) model to generate the nuggets from a system-generated\nanswer using the approach and prompt discussed in [14]. This section presents the\nmodels and settings explored for each of the answer evaluation aspects.\n4.1.1 Nugget Precision and Recall\nWe begin the computations of precision and recall by extracting the sentence embeddings\nof the ground-truth and system-generated nuggets. To assess the role of embedding\nmodels comprehensively, we use a variety of embedding models (all-MiniLM-L6-v22,\nall-mpnet-base-v23,\nsup-simcse-roberta-large4). For each list of the ground-\ntruth and system-generated nuggets, we created a similarity matrix S, where an item\nS[i, j] represents the cosine similarity between the ith system-generated nugget and the\njth ground-truth nugget. With the goal of finding the best cosine similarity threshold\nthat optimizes the precision and recall between system-generated and ground-truth\nnuggets, we utilized the BioGen2024-train set that contains the system-generated\nand ground-truth nuggets for the question-answer pairs. Given the similarity matrix of\neach question-answer, we build a Bayesian Gaussian Mixture Model [15] (BGMM) to\nrepresent the observed distribution in the nugget similarities. We specified a maximum\nof two components (a pair of nuggets are semantically similar or dissimilar), allowing\nthe model to represent the data with up to two latent clusters. The Gaussian mixture\nmodel parameters (mean and variance) are estimated using variational inference, which\nis an extension of expectation-maximization that maximizes a lower bound on model\nevidence (including priors) instead of data likelihood. Once the BGMM is trained,\nwe use the model to predict the probability of a system-generated and ground-truth\nnugget being semantically similar. If they are predicted as similar, we align and match\nthem; we follow the same procedure for each pair of system-generated and ground-\ntruth nuggets. With the aligned nuggets, we compute the precision and recall of the\nanswer nuggets. We use the same BioGen2024-train set that we use to train the\nBGMM model to tune the threshold, maximizing the multi-objectives (average F1-\nscore, average nugget similarity, and average number of nugget alignments between the\nsystem-generated and ground-truth nuggets). We utilize Optuna5 to find the optimal\nprobability threshold that maximizes the multi-objectives.\n2https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n3https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n4https://huggingface.co/princeton-nlp/sup-simcse-roberta-large\n5https://optuna.readthedocs.io/en/stable/index.html\n11\n"}, {"page": 12, "text": "4.1.2 Completeness\nTo assess the completeness, we train a variety of classifiers on a question and each answer\ngenerated sentence, aiming to predict the label (Required, Unnecessary, Borderline,\nor Inappropriate). We use the pretrained language models: BERTBase, BERTLarge,\nRoBERTaBase, and RoBERTaLarge to train the model on the BioGen2024-train\ndataset, tune the hyperparameters on the BioGen2024-val dataset, and evaluate the\nperformance in terms of precision, recall, and F1-score on the MedAESQA dataset. We\nalso utilized large language models (Qwen3-8B, Qwen3-14B, Llama-3-8B-Instruct,\nLlama-3.3-70B-Instruct, and Mistral-7B-Instruct-v0.3) for this task, using them in\nboth zero-shot and fine-tuned setups with the prompt specified in Figure A1. We\nuse the low-rank adaptation [16] to fine-tune the LLMs on the BioGen2024-train\ndataset and report the classification performance on the MedAESQA dataset. With the\nmodel-predicted classification label, we compute the completeness as the portion of\nthe generated answer sentence that is classified as Required.\n4.1.3 Correctness\nClassification Approach\nWe cast the assessment of the correctness of a generated answer sentence as a classifi-\ncation task in which an answer sentence and the supporting evidence are provided to a\nclassifier to label the pair as correct or incorrect. We experimented with the classical\nsupervised classifiers (support vector machines and logistic regression) models, pre-\ntrained language models (BERTBase, BERTLarge, RoBERTaBase, and RoBERTaLarge),\nas well as LLMs (Qwen3-8B, Qwen3-14B, Llama-3-8B-Instruct, Llama-3.3-70B-Instruct,\nMistral-7B-Instruct-v0.3). To capture the granular information present in the docu-\nments provided as evidence, we split a document into multiple short documents by\nconsidering a window size of n = 3. We tested the model for each short document. If\nthe model predicted the label correct for at least one such short document, we con-\nsidered the prediction correct for the document. To train the model, we consider the\nhuman-annotated span from the document and the answer sentence as a correct pair.\nWe computed the BM25 scores between the question and the fragmented documents\nand selected the top-scoring fragment as an incorrect pair, unless the fragment was\npart of the human-annotated span.\nSemantic Similarity and NLI Approach\nTo evaluate answer correctness, we also performed an in-depth analysis of the embedding\nmodel for encoding documents and human-annotated evidence, using it in two settings:\ncosine similarity and NLI-based evaluation. We performed the experiments with three\nembedding models: all-MiniLM-L6-v2, all-mpnet-base-v2, and sup-simcse-roberta-large.\nWe aim to analyze the similarity between the document and the answer sentence\nrepresentation compared to the NLI model, which takes the document and the answer\nsentence as input and produces labels such as supports, refutes, and insufficient\ninformation. To achieve this, we experimented on a MedAESQA dataset where the answer-\nsupported documents and evidence to support the answers are annotated by experts.\nThe answer could have been marked as supported by considering the continuous\n12\n"}, {"page": 13, "text": "sentences from the document; therefore, we grouped the three sentences6 for document-\nanswer experiments. We extracted the sentence embeddings for the grouped sentences\nand the answer sentence, assessed the cosine similarity score, and also used the NLI\nmodel7 to evaluate the model’s probability of making the document-answer pair as\nsupport. For a comparative analysis, we also sampled a negative question from the\ndataset. We performed cosine similarity and NLI computations, using the same answer\nsentence as we did for the positive sample. We have outlined the experimental pipeline\nin the Algorithm 1.\n4.2 Citation Evaluation\nIn addition to our answer evaluation, we assess the ability of language models to\nattribute claims to their cited sources. Specifically, we focus on the task of natural\nlanguage inference, where models must determine whether a given citation supports,\ncontradicts, or is neutral with respect to a target claim. The system-generated answers\ncontain identifiers (PMID) of the citations, corresponding to the PubMed abstract of\na scientific article that should support the claims made in that sentence. For each of\nthese citations, medical informatics experts have labeled the sentence-document pair\nas ’supporting’, ’contradicting’, ’neutral’, or ’not relevant’. We evaluate three different\nsettings for precision and recall.\n4.2.1 Answer Sentence-Document\nOur first setting evaluates the ability of large language models to attribute a single sen-\ntence to the title and abstract of the cited PubMed document. We use a variety of model\ntypes, including both base and fine-tuned LLMs and sequence-to-sequence (seq2seq)\nmodels. Some models are not capable of providing ternary labels; we, therefore, evalu-\nate the models in binary and ternary classification settings. For binary classification,\nwe task the model to label the answer sentence and document pair as ’attributable’\nor ’not attributable’ using the prompt in figure A2. We then map ’supporting’ labels\nin our dataset to ’attributable’ and all other labels to ’not attributable’. For ternary\nclassification, we task the model with labeling the answer sentence and document pair\nas ’supporting’, ’contradicting’, or ’neutral’ using the prompt in figure A3.\n4.2.2 Answer Sentence-maxSimSentence Document\nIn the second setting, we preprocess the PubMed abstracts to find the sentence most\nsimilar to the answer. We use the NLTK implementation of the Punkt sentence tokenizer\nto split each abstract into a list of sentences [17, 18] We then use all-MiniLM-L6-v28\nto produce sentence embeddings for each sentence of the abstract. The cosine similarity\nof the answer sentence and each document sentence is calculated. We call the document\nsentence with the greatest cosine similarity to the answer sentence, maxSimSentence.\nThe task is the same as the first setting, except only the maxSimSentence is given,\nrather than the entire abstract.\n6We experimented with grouping 1, 2, 3, and 4 sentences and found that three is the most optimal on the\nBioGen2024-val dataset\n7https://huggingface.co/FacebookAI/roberta-large-mnli\n8https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n13\n"}, {"page": 14, "text": "4.2.3 Answer Nuggets-Document Nuggets\nIn the third setting, we first process the answers and documents into lists of nuggets\n(atomic facts). This step is done by prompting Llama 3.3 with the instructions found\nin figure A5. The models are then given the entire lists of nuggets from the answer\nand from the document and asked to assign one of four labels: ’supports’, ’contradicts’,\n’neutral’, or ’not relevant’. If at least one nugget generated from the answer is supported\nby at least one nugget from the document, that answer-document pair is labeled\n’supports’, unless any of the document nuggets contradict any of the answer nuggets. If\nat least one contradiction is detected, the answer-document pair is labeled ’contradicts’.\nIf none of the answer nuggets are contradicted, and at least one of the answer nuggets is\nnot supported, this answer-document pair is ’neutral’. Finally, if the document nuggets\nare unrelated to the answer nuggets, the answer-document pair is labeled ’not relevant’.\nThis prompt was selected because it is closely aligned with instructions given to the\nhuman annotators for the creation of the MedAESQA dataset.\n5 Data Availability\nAll the experiments are performed on the MedAESQA, which is publicly available at the\nOpen Science Framework repository (https://osf.io/ydbzq/).\n6 Code Availability\nThe BioACE code is available at GitHub (https://github.com/deepaknlp/BioACE/).\n7 Acknowledgments\nThis research was supported by the Intramural Research Program of the National\nInstitutes of Health (NIH). The contributions of the NIH authors are considered Works\nof the United States Government. The findings and conclusions presented in this paper\nare those of the authors and do not necessarily reflect the views of the NIH or the U.S.\nDepartment of Health and Human Services.\n8 Author Contributions\nD.G. and D.D.F. conceived the study. D.G., D.B., and D.D.F. drafted the manuscript\nand analyzed the results. D.G. and D.B. developed the evaluation tools and performed\nthe experiments. All authors reviewed the manuscript.\n9 Competing interests\nThere are no competing interests.\n14\n"}, {"page": 15, "text": "References\n[1] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Amin, M., Hou, L.,\nClark, K., Pfohl, S.R., Cole-Lewis, H., et al.: Toward expert-level medical question\nanswering with large language models. Nature Medicine, 1–8 (2025)\n[2] Kumar, V., Bharti, A., Verma, D., Bhatnagar, V.: Deep dive into language traits\nof ai-generated abstracts. In: Proceedings of the 7th Joint International Conference\non Data Science & Management of Data (11th ACM IKDD CODS and 29th\nCOMAD), pp. 237–241 (2024)\n[3] Vorheese, E.M.: Overview of the trec 2001 question answering track (2001)\n[4] Lin, J., Demner-Fushman, D.: Methods for automatically evaluating answers to\ncomplex questions. Information Retrieval 9, 565–587 (2006)\n[5] M’rabet, Y., Demner-Fushman, D.: Holms: Alternative summary evaluation with\nlarge language models. In: Proceedings of the 28th International Conference on\nComputational Linguistics, pp. 5679–5688 (2020)\n[6] Chen, J., Choi, E., Durrett, G.: Can NLI models verify QA systems’ predictions? In:\nMoens, M.-F., Huang, X., Specia, L., Yih, S.W.-t. (eds.) Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pp. 3841–3854. Association for\nComputational Linguistics, Punta Cana, Dominican Republic (2021). https://doi.\norg/10.18653/v1/2021.findings-emnlp.324 . https://aclanthology.org/2021.findings-\nemnlp.324/\n[7] Pradeep, R., Thakur, N., Upadhyay, S., Campos, D., Craswell, N., Soboroff, I.,\nDang, H.T., Lin, J.: The great nugget recall: Automating fact extraction and rag\nevaluation with large language models. In: Proceedings of the 48th International\nACM SIGIR Conference on Research and Development in Information Retrieval,\npp. 180–190 (2025)\n[8] Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J., Liu, Z.:\nChateval: Towards better llm-based evaluators through multi-agent debate. In:\nThe Twelfth International Conference on Learning Representations (2024)\n[9] Zhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P., Zhu, C., Ji, H., Han, J.:\nTowards a unified multi-dimensional evaluator for text generation. In: Goldberg,\nY., Kozareva, Z., Zhang, Y. (eds.) Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 2023–2038. Association for Compu-\ntational Linguistics, Abu Dhabi, United Arab Emirates (2022). https://doi.org/10.\n18653/v1/2022.emnlp-main.131 . https://aclanthology.org/2022.emnlp-main.131/\n[10] Gupta, D., Demner-Fushman, D., Hersh, W., Bedrick, S., Roberts, K.: Overview\nof trec 2024 biomedical generative retrieval (biogen) track. arXiv preprint\narXiv:2411.18069 (2024)\n15\n"}, {"page": 16, "text": "[11] Gupta, D., Bartels, D., Demner-Fushman, D.: a dataset of medical questions\npaired with automatically generated answers and evidence-supported references.\nScientific Data 12(1), 1035 (2025)\n[12] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Chen, W.: Lora:\nLow-rank adaptation of large language models. CoRR abs/2106.09685 (2021)\n2106.09685\n[13] McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks:\nThe sequential learning problem. Psychology of Learning and Motivation, vol.\n24, pp. 109–165. Academic Press (1989). https://doi.org/10.1016/S0079-7421(08)\n60536-8 . https://www.sciencedirect.com/science/article/pii/S0079742108605368\n[14] Bartels, D., Gupta, D., Demner-Fushman, D.: Can large language models accu-\nrately generate answer keys for health-related questions? In: Che, W., Nabende, J.,\nShutova, E., Pilehvar, M.T. (eds.) Proceedings of the 63rd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pp. 354–368.\nAssociation for Computational Linguistics, Vienna, Austria (2025). https://doi.\norg/10.18653/v1/2025.acl-short.28 . https://aclanthology.org/2025.acl-short.28/\n[15] Bishop, C.M., Nasrabadi, N.M.: Pattern Recognition and Machine Learning vol.\n4. Springer, ??? (2006)\n[16] Hu,\nE.J.,\nshen,\nWallis,\nP.,\nAllen-Zhu,\nZ.,\nLi,\nY.,\nWang,\nS.,\nWang,\nL.,\nChen,\nW.:\nLoRA:\nLow-rank\nadaptation\nof\nlarge\nlanguage\nmod-\nels.\nIn:\nInternational\nConference\non\nLearning\nRepresentations\n(2022).\nhttps://openreview.net/forum?id=nZeVKeeFYf9\n[17] Bird, S., Klein, E., Loper, E.: Natural Language Processing with Python, 1st edn.\nO’Reilly Media, Inc., ??? (2009)\n[18] Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection.\nComputational Linguistics 32(4), 485–525 (2006) https://doi.org/10.1162/coli.\n2006.32.4.485\n16\n"}, {"page": 17, "text": "Appendix A\nResults\nEmbedding\nModel\nRecall (R+B)–SimCSE\nRecall (R+B)–ST\nSpearman\nKendall\nSpearman\nKendall\nall-MiniLM-L6-v2\n0.908\n0.758\n0.876\n0.724\nall-mpnet-base-v2\n0.879\n0.721\n0.846\n0.683\nsup-simcse-roberta-large\n0.876\n0.712\n0.823\n0.651\nTable A1: Correlation coefficients in the system ranking considering the nugget-based\nrecall metric vs recall metric computed using the human annotation of the machine-\ngenerated answers [10, 11]. R+B refers to the setting Required and Borderline\nwhere answer sentences deemed as either required or borderline were included for\nclustering to compute the recall.\nModel\nSetting\nPrecision\nRedundancy\nHarmfulness\nBERTBase\nFine-tuned\n0.834/0.669\n0.860/0.698\n-/-\nBERTLarge\nFine-tuned\n0.849/0.692\n0.818/0.615\n-/-\nRoBERTaBase\nFine-tuned\n0.749/0.563\n0.849/0.634\n-/-\nRoBERTaLarge\nFine-tuned\n0.827/0.664\n0.894/0.726\n-/-\nLlama-3.3-70B-Instruct\nZero-shot\n0.859/0.715\n0.816/0.629\n0.272/0.226\nLlama-3.3-70B-Instruct\nFine-tuned\n0.721/0.531\n0.634/0.470\n0.411/0.369\nLlama-3-8B-Instruct\nZero-shot\n0.740/0.605\n0.440/0.332\n0.091/0.102\nLlama-3-8B-Instruct\nFine-tuned\n0.754/0.603\n0.616/0.484\n-/-\nQwen3-14B\nZero-shot\n0.613/0.460\n0.133/0.106\n0.147/0.133\nQwen3-14B\nFine-tuned\n0.724/0.549\n0.469/0.342\n0.219\nQwen3-8B\nZero-shot\n0.853/0.686\n0.047/0.038\n0.283/0.228\nQwen3-8B\nFine-tuned\n0.541/0.408\n-0.186/-0.139\n0.195/0.177\nMistral-7B-Instruct-v0.3\nZero-shot\n0.813/0.658\n-/-\n-/-\nMistral-7B-Instruct-v0.3\nFine-tuned\n0.705/0.554\n-0.139/-0.116\n-/-\nTable A2: Correlation coefficients (Spearman/Kendall Tau) in the system ranking\nconsidering the model-predicted answer sentences for assessing its relevance vs relevance\nmetric computed using the human annotation of the machine-generated answers [10, 11].\nThe -/- indicates the undefined coefficients where systems could not predict any\nUnnecessary and Inappropriate labels, resulting in zero Redundancy and Harmfulness\nscores and the same ranks for all systems.\n17\n"}, {"page": 18, "text": "Prompt : Completeness Prompt\nYou are an expert annotator. Given a question and an answer sentence, your task is\nto assign a single label from the following list: [‘Required’, ‘Unnecessary’, ‘Borderline’,\n‘Inappropriate’]. The label definitions are as follows:\nRequired: The answer sentence is necessary to have in the generated answer for\ncompleteness of the answers.\nUnnecessary: The answer sentence is not required to be included in the generated\nanswer. An answer sentence may be unnecessary for several reasons:\n1. If including it would cause information overload, if it is added to the answer;\n2. If it is trivial, e.g., stating that many treatment options exist.\n3. If it consists entirely of a recommendation to see a health professional.\n4. If it is not relevant to the answer, e.g., describing the causes of a disease\nwhen the question is about treatments,\nBorderline: If an answer sentence is relevant, possibly even “good to know,” but not\nrequired, the answer sentence may be marked borderline.\nInappropriate: The assertion may harm the patient, e.g., if, according to the answer,\nphysical therapy reduces the pain level, but the patient experiences more pain due to\nhip mobilization, the patient may start doubting they are receiving adequate treatment.\nDo not generate anything else. Respond ONLY with the label, no explanation.\nQuestion : {question}\nAnswer Sentence: {answer sentence}\nFig. A1: Prompt used for answer completeness evaluation.\nPrompt : Binary Label Prompt\n### Instruction: Please solely verify whether the reference can support the claim.\nOptions: ’attributable’ or ’not attributable’.\n###Input:\nClaim: {sentence}\nReference: {document}\n### Output:\nFig. A2: Prompt used for binary labeling of citations.\n18\n"}, {"page": 19, "text": "Prompt : Ternary Label Prompt\n### Instruction: Please solely verify whether the reference can support the claim.\nOptions: ’support’, ’contradict’, or ’neutral’.\n### Input:\nClaim: {sentence}\nReference: {document}\n### Output:\nFig. A3: Prompt used for ternary labeling of citations.\nPrompt : Nugget Label Prompt\nFor the following lists of answer and document nuggets, select one of the following\nlabels:\nSupports: There is at least one document nugget that supports/agrees with each\nanswer nugget.\nContradicts: There is at least one document nugget that disagrees with an answer\nnugget or states its opposite.\nNeutral: The document nuggets are topically relevant, but lack any information to\nvalidate or invalidate the all of the answer nuggets.\nNot relevant: The document nuggets are not relevant to the answer nuggets.\nAnswer Nuggets: {list of answer nuggets}\nDocument Nuggets: {list of document nuggets}\nFig. A4: Prompt used for labeling answer and document nugget lists.\nModel\nSetting\nPrecision\nRecall\nF1-Score\nLlama-3.3\nBase\n76.10\n77.50\n76.79\nFLAN-T5\nBase\n74.80\n77.20\n75.98\nFLAN-UL2\nBase\n75.20\n77.80\n76.48\nLlama-3.3\nFine-tuned\n77.90\n78.50\n78.20\nFLAN-T5\nFine-tuned\n76.90\n77.50\n77.20\nFLAN-UL2\nFine-tuned\n74.50\n78.90\n76.63\nTable A3: Performance of base and fine-tuned models when tasked with assigning\nternary labels to a claim sentence and the sentence with the highest cosine similarity\nto the claim.\n19\n"}, {"page": 20, "text": "Prompt : Nugget Generation Prompt\nList all of the information nuggets in the text given below. Each nugget must contain\none, and only one, fact from the text. A nugget must be as concise and as specific as\npossible. Each element in a list must be its own nugget. The list of nuggets must not\ncontain redundant information. Return a list of nuggets such that each nugget is on a\nnew line. Do not number or bullet the list. Do not include anything in your response\nexcept for the list of nuggets. Here is an example of the output format:\nnugget1\nnugget2\n. . .\nHere is an example text: During infections, a battle for iron takes place between the\nhuman host and the invading pathogens. Lymphocytes need iron to mount an effective\ncellular and humoral response. Viruses depend on iron to replicate within living host\ncells. During the acute phase of infection, blood levels of iron decrease. Ferritin levels\nare high. Elevated serum ferritin is associated with increased mortality. As a major\niron storage protein, ferritin is essential to iron homeostasis and is involved in a\nwide range of physiologic and pathologic processes. The inflammation cascade and\npoor prognosis of COVID-19 may be attributed to high ferritin levels. Iron depletion\ntherapy was proposed as a novel therapeutic approach in the COVID-19 pandemic.\nThis is the list of nuggets that should be extracted from this text:\nLymphocytes and viruses compete for iron.\nLymphocytes need iron for cellular response.\nLymphocytes need iron for humoral response.\nViruses need iron to replicate.\nInfection lowers iron levels in the blood.\nInfection increases ferritin levels in the blood.\nHigh ferritin is associated with increased mortality.\nIron homeostasis needs ferritin.\nFerritin is involved in physiologic processes.\nFerritin is involved in pathologic processes.\nHigh ferritin indicates response to inflammation.\nHigh ferritin levels are linked to poor outcomes of COVID-19.\nIron depletion therapy showed anti-viral activity in the COVID-19 pandemic.\nIron depletion therapy showed anti-fibrotic activity in the COVID-19 pandemic.\nText: {text}\nFig. A5: Prompt used for generating nuggets from text.\n20\n"}, {"page": 21, "text": "Model\nCitation Coverage\nCitation Support Rate\nCitation Contradict Rate\nSpearman\nKendall\nSpearman\nKendall\nSpearman\nKendall\nAnswer Sentence-Document\n0.961\n0.847\n0.383\n0.276\n0.248\n0.205\nAnswer Sentence-maxSimSentence Document\n0.960\n0.847\n0.389\n0.281\n0.248\n0.205\nAnswer Nuggets-Document Nuggets\n0.975\n0.884\n0.578\n0.391\n0.396\n0.309\nTable A4: Correlation coefficients comparing rankings based on automatic and manual\nevaluation of MedAESQA.\nModel\nSetting\nPrecision\nRecall\nF1-Score\nLlama-3.3\nBase\n75.90\n77.60\n76.74\nFLAN-T5\nBase\n74.60\n77.10\n75.83\nFLAN-UL2\nBase\n75.40\n77.50\n76.44\nLlama-3.3\nFine-tuned\n76.80\n76.30\n76.54\nFLAN-T5\nFine-tuned\n76.70\n77.60\n77.15\nFLAN-UL2\nFine-tuned\n74.70\n78.80\n76.69\nTable A5: Performance of base and fine-tuned models when tasked with assigning\nternary labels to a the list of nuggets contained in the claim and the list of nuggets\ncontained in the PubMed abstract.\nModel\nSetting\nPrecision\nRecall\nF1-Score\nLlama-3.3\nBase\n75.34\n76.21\n75.77\nFLAN-T5\nBase\n74.09\n76.77\n75.41\nFLAN-UL2\nBase\n74.66\n76.88\n75.75\nLlama-3.3\nFine-tuned\n77.42\n77.91\n77.66\nFLAN-T5\nFine-tuned\n76.55\n77.06\n76.80\nFLAN-UL2\nFine-tuned\n73.87\n78.69\n76.21\nTable A6: Performance of base and fine-tuned models when tasked with assigning\nternary labels to a claim sentence and a PubMed title and abstract.\n21\n"}, {"page": 22, "text": "Algorithm 1: Answer correctness evaluation algorithm to analyze the\nembedding model, NLI model, and cosine similarity scores between documen-\nt/evidence and answer sentence.\nInput: Dataset D = {D1, . . . , DN} with questions and answer sentences, sentence\nembedding model E, NLI model N\nOutput: Overall evaluation metrics M\n1.\nInitialize per-question result map R ←∅;\n2.\nforeach question-answer sentences Di ∈D do\n3.\nLet question q ←Di[question];\n4.\nforeach answer sentence Sj in Di do\n5.\n▷Cosine similarity and NLI computation with positive sample ;\n6.\nExtract supporting documents Sj[suppoting] ;\n7.\nGroped three sentences together and created a supporting documents list\nSsupp\nj\n;\n8.\nComputed cosine similarity between the embedding of each item of the\nsupporting documents list and the answer sentence Sj using E;\n9.\nGet the maximum cosine similarity and assign it as the document-answer\nsentence Sim(doc, answer sent) ;\n10.\nComputed NLI scores between each item of the supporting documents list\nand the answer sentence Sj using N;\n11.\nGet the maximum NLI score of the Supports label and assign it as the\ndocument-answer sentence NLI(doc, answer sent) ;\n12.\nRepeat steps 5-10 with the human-annotated evidence in the supporting\ndocuments instead of groping and computing the scores for all the\ngroped sentences. This step yields Sim(evidence, answer sent)\nand\nNLI(evidence, answer sent) .\n13.\n▷Cosine similarity and NLI computation with negative sample ;\n14.\nSample a different row Dk, k ̸= i and pick the first answer sentence Sk;\n15.\nRepeat steps 6-12 with the Sk and the answer sentence Sj. ;\n16.\nAdd scores to R.\n17.\nend\n18.\nend\n19.\n▷Aggregate scores across all answer sentences;\n20.\nforeach question q in R do\n21.\nAverage cosine similarity and NLI scores for positive and negative samples ;\n22.\nCompute Accuracy as a fraction of positive scores > negative scores ;\n23.\nCompute AUC with positive and negative scores;\n24.\nend\n25.\nM ←Aggregate({Rq})\n▷Aggregate metrics across all questions;\n26.\nreturn M;\n22\n"}, {"page": 23, "text": "Supp(AS)\nSupp(CS)\nMetrics\n70\n75\n80\n85\n90\n95\n100\nScore\n73.86\n92.56\n73.45\n92.06\n73.61\n92.24\n73.50\n92.09\nModel\nBERTBase\nRoBERTaLarge \nBERTLarge\nRoBERTaBase\n(a)\nCorrectness(T10)\nCorrectness(T20)\nCorrectness(T30)\nCorrectness(T40)\nCorrectness(T50)\nCorrectness(T60)\nCorrectness(T70)\nCorrectness(T80)\nCorrectness(T90)\nCorrectness(T100)\nMetrics\n0\n20\n40\n60\n80\n100\n120\nScore\nModel\nRoBERTaBase \nBERTLarge\nSimCSE\nNLI\n(b)\nFig. A6: Answer correctness results using different PLMs: (a)\ncomparing the\neffectiveness of the model in terms of human-assessed supported documents and all\nmodel-predicted documents, (b) comparing the effectiveness of the model with the\ntop-k retrieved documents.\n23\n"}]}