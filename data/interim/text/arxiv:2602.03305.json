{"doc_id": "arxiv:2602.03305", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.03305.pdf", "meta": {"doc_id": "arxiv:2602.03305", "source": "arxiv", "arxiv_id": "2602.03305", "title": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions", "authors": ["Qianyi Xu", "Gousia Habib", "Feng Wu", "Yanrui Du", "Zhihui Chen", "Swapnil Mishra", "Dilruk Perera", "Mengling Feng"], "published": "2026-02-03T09:30:32Z", "updated": "2026-02-04T13:20:50Z", "summary": "Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.03305v2", "url_pdf": "https://arxiv.org/pdf/2602.03305.pdf", "meta_path": "data/raw/arxiv/meta/2602.03305.json", "sha256": "d2e16bfcba9b600d2b70f88abc566ab59188ec7998e00409d7f519366b86520a", "status": "ok", "fetched_at": "2026-02-18T02:19:54.327069+00:00"}, "pages": [{"page": 1, "text": "MEDR: REWARD ENGINEERING FOR CLINICAL OFFLINE\nREINFORCEMENT LEARNING VIA TRI-DRIVE POTENTIAL\nFUNCTIONS\nQianyi Xu1, Gousia Habib2, Feng Wu1, Yanrui Du3, Zhihui Chen1, Swapnil Mishra1, Dilruk Perera1, and\nMengling Feng1\n1National University of Singapore, Singapore\nxuqianyi, zhihui.chen@u.nus.edu\nfengwu, swapnil.mishra, dilruk, mornin@nus.edu.sg\n2Finish Centre of Artificial Intelligence, University of Helsinki, Finland\ngousia.habib@helsinki.fi\n3Harbin Institute of Technology, China\nyrdu.hit@gmail.com\nABSTRACT\nReinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes\n(DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge\nof defining signals that safely and effectively guide policy learning in complex, sparse offline\nenvironments. Existing approaches often rely on manual heuristics that fail to generalize across\ndiverse pathologies. To address this, we propose an automated pipeline leveraging Large Language\nModels (LLMs) for offline reward design and verification. We formulate the reward function using\npotential functions consisted of three core components: survival, confidence, and competence. We\nfurther introduce quantitative metrics to rigorously evaluate and select the optimal reward structure\nprior to deployment. By integrating LLM-driven domain knowledge, our framework automates the\ndesign of reward functions for specific diseases while significantly enhancing the performance of the\nresulting policies.\n1\nIntroduction\nCritical care medicine is fundamentally defined by high-stakes, sequential decision-making, RL is a natural fit for\noptimizing DTRs where an agent must make sequential decisions that adapts to a patient’s changing state over time.\nReward serves as the core of RL that not only guides the agent learning but also defines the optimization problem [1].\nSmall changes in reward function weights can cause profound policy changes[2] and bad reward signals affect beyond\n“win-or-lose\" in gaming and can lead to “life-or-death\" in healthcare. Errors in sensitive fields such as autonomous\ndriving have already been shown to be problematic [3]. However, reward engineering in clinical RL is less studied and\nbenchmarked. This underscores an urgent need to design better rewards that align with clinical goals [4].\nCurrent rewards in RL for DTRs largely depend on manual and static design. Drawing an analogy to recent advances\nin LLM, we categorize existing clinical reward mechanisms into three distinct paradigms: sparse Outcome Reward\nModel (ORM), which assigns scalar rewards based on terminal patient outcomes like survival [5]. The second is\ndense Process Reward Model (PRM), which depends on dense feedback based on intermediate physiological state\nchanges [6]. To mitigate the sparsity of terminal outcomes, the standard approach augments them with intermediate\nfeedback to form a composite reward (ROP RM = RORM + RP RM) [7, 8]. In this paradigm, clinicians manually select\ndisease-relevant features and encode them using simple heuristic logic. However, the terminal reward is often weighted\ndisproportionately high, overshadowing the dense shaping signals and the selection of specific parameters and weights\nremains largely ad-hoc, lacking systematic justification [8, 6].\narXiv:2602.03305v2  [cs.LG]  4 Feb 2026\n"}, {"page": 2, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nDespite efforts in designing various reward signals to accelerate policy learning, there are currently no guidelines on\ndesigning effective rewards for clinical RL due to several major challenges. First, outcomes in clinical settings are sparse\nand delayed. While mortality is the ultimate ground truth outcome, it occurs only at the end of ICU stay which leads\nto credit assignment problem. Second, the target in a treatment process is often implicit and complicated. It extends\nbeyond binary survival to encompass the quality of recovery. There is a profound gap between the high-dimensional\ncomplexity of patient physiology and the scalar reward value required by RL algorithms. This gap leads to misaligned\nobjectives, such as the “Pyrrhic Victory”[9] where an agent optimizes for survival via aggressive interventions or\nreward hacking. Third, rewards are not verifiable. Unlike robotics or games, clinical RL is strictly offline, precluding\nonline exploration to validate reward signals. Consequently, current rewards rely on brittle, manual heuristics that lack\ninterpretability and fail to generalize across different diseases or datasets.\nRecent advancements have identified LLMs as a promising solution to bridge the gap between high-level human intent\nand low-level reinforcement signals. While early work utilized LLMs as direct evaluators of agent behavior[10], more\nrecent methods focus on using LLMs to generate executable reward function codes[11, 12]. We leverage this generative\ncapability to construct a reward function that captures the nuance of DTRs without the prohibitively high cost of manual\ntrial-and-error engineering. Our pipeline leverages LLMs to bridge the semantic gap between medical knowledge and\nmathematical representation, utilizing a “Tri-Drive” mechanism comprising Survival, Confidence, and Competence to\nguide both reward generation and offline verification. Our specific contributions are:\n• We propose an automated pipeline for clinical RL that utilizes LLMs to perform interpretable feature selection\nand formulate a novel reward structure based on potential functions.\n• We design the reward functions via a Tri-Drive mechanism and derive corresponding offline quantitative\nmetrics to act as a proxy for ground truth and verify reward functions.\n• We empirically validate our framework on three distinct, high-stakes clinical tasks and demonstrate that\npolicies trained with our automated rewards consistently outperform those using standard rewards, achieving\n77.3%, 66.7%, and 60.3% higher WIS scores over clinician baselines on three tasks, proving the framework’s\ngeneralizability across diverse diseases.\nFigure 1: Framework pipeline of medR.\n2\nBackground\nWe model the underlying clinical dynamics as a Partially Observable Markov Decision Process (POMDP), defined\nby the tuple (S∗, A, T , R, Ω, O, γ). Here, S∗denotes the unobservable, true physiological state of the patient, and A\nrepresents the space of clinical interventions. The agent can not access S∗directly; instead, it receives intermittent\nobservations ot ∈Ωlike vital signs and lab results O(o|s∗, a). To ensure computational tractability for offline policy\nlearning, we approximate this POMDP as a Markov Decision Process (MDP) defined by (S, A, P, R, γ) during policy\ntraining. We explicitly construct the proxy state space S in our reward functions to capture temporal irregularities\ninherent in clinical data: a state is defined as st = (ot, ∆t), where ∆t represents the time elapsed since the last\nobservation. While standard MDP approximations often discard the uncertainty arising from partial observability, we\nexplicitly mitigate this gap through our reward engineering framework.\n2\n"}, {"page": 3, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n2.1\nIntrinsic Motivation\nIn this work, our reward function design follows the evolutionary perspective proposed in [13]. They distinguish between\nreward signals and fitness functions where reward signals are internal scalar values that drive learning within an agent\nand fitness is the external evaluation that measures an agent’s success. For bounded agents in an offline environment,\nthe optimal reward function is not the fitness function[14]. The design of reward functions should optimize intrinsic\nmotivations (IM), unlike previous literature about IM that focuses on incentivizing exploration which is dangerous in\nour offline setting, in our paper, we form it as a threefold drive: the drive to restore homeostasis(survival), the drive to\nlower uncertainty(confidence), and the drive to minimize cost(competence). In Hull’s Drive Reduction Theory[15],\norganisms are motivated to maintain physiological equilibrium, we treat homeostasis as a proxy of survival to optimize\nin the reward function. Unlike curiosity-driven frameworks that encourage exploration under uncertainty[16, 17], in\norder to design a reward function specific to our offline environment[13], we conversely suppress uncertainty due to the\nsensitivity in critical care[18]. (author?) [19] furthers the learning process of effective interaction with the environment,\nwhich is transferrable as effective treatment under clinical setting.\n2.2\nPotential-Based Reward\nReward shaping was used in previous literature on clinical RL to address the challenge of sparse feedback[20]. To\nensure that this augmentation does not alter the optimal policy π∗, (author?) [21] formalized Potential-Based Reward\nShaping (PBRS). They proved that if the shaping term is defined as the difference of a potential function Φ(s) over\nstates:F(s, a, s′) = γΦ(s′) −Φ(s), then the optimal policy under the shaped reward R′ = R + F is identical to that\nunder R. Later extensions generalized this framework to state-action potentials [22] and dynamic, time-dependent\npotentials Φ(s, t) [23]. Most recently, (author?) [24] applied this theory to intrinsic motivation (PBIM), utilizing\npotentials to mathematically neutralize the policy-altering bias of exploration bonuses such as curiosity. However, the\nstrict policy invariance mandated by PBRS relies on the assumption that the base reward signal which in our case is\nthe mortality already defines the ideal behavior. Here we keep the form of potential functions as indicators for patient\nhealth transients and transform the problem into potential-based reward engineering.\n3\nMethodology\n3.1\nInterpretable Feature Selection\nWe leverage the semantic reasoning capabilities of LLMs to select critical features for constructing the reward functions.\nFigure1 shows the framework of our method. We first compute a statistical summary for all candidate features in\nthe dataset D. Let f be a candidate feature and y be the patient outcome mortality. We generate a metadata tuple\nMf = (µf, σf, ηf, ρoutcome\nf\n, ρaction\nf\n), where µ is the mean and σ is the standard deviation of the non-missing values.\nWe explicitly analyze the missingness rate η because Electronic Health Records(EHRs) are inherently sparse, simply\nusing imputed data for reward computation is misleading and can introduce bias. ρoutcome, ρaction are the correlation\nbetween feature x and patient outcome y or action a, for each mortality and action dimension, we compute the Pearson\ncorrelation coefficient with every feature f in the state space.\nWe create a structured prompt that feeds this statistical metadata M into an LLM. The LLM acts as a clinical data\nscientist to select K critical state features that affect the specific disease treatment. Crucially, for every selected feature,\nthe model must generate a rationale explaining its selection. To mitigate the hallucination and instability inherent\nin LLM generation, we employ a stochastic ensemble selection strategy. The final feature set F∗is determined via\nmajority voting: F∗= {f ∈Fall | 1\nN\nPN\ni=1 I(f ∈Si) ≥τ} where Si is the set of features selected in iteration i, and\nκ is the consensus threshold. We then double check with clinicians on the features chosen by LLM and the rationales\nbehind them. In this way we effectively simulate the knowledge of multiple clinicians to vote on the most robust\ncandidates and validate with real clinicians to guarantee feature reliability.\n3.2\nPotential Based Reward Function Generation\nWe design a health potential that reflects the quality of physiological states. Past reward functions often define specific\nrules that directly scores the action, for example, Reward +1 if 70 < mbp < 80, which will likely cause reward hacking\nand encouraging actions that can directly increase blood pressure. Instead, we form it as the gradient of potential to\ncapture the patient state change. To bridge the gap between sparse outcome labels and dense clinical supervision, the\nreward function R is composed of three distinct drives—Survival, Confidence, and Competence leveraging the intrinsic\nmotivation theory [13]. We define the reward function as follows:\nR(s, a, t, s′, a′, t′) = [γΦ(st+1, t + 1) −Φ(st, t)] −λC(at)\n(1)\n3\n"}, {"page": 4, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nΦ(st, t) = δ(t) ·\nX\nf∈F\nωf · Sf(vt,f) · Uf(∆tt,f)\n(2)\nwhere γ is the discount factor, Φ(·) is the health potential, δ(t) is the strategic decay, Sk(vt,f) is the survival score with\nconfidence weight Uf(∆tt,f) and C(·) is the competence cost.\nWe use physiological stabilization as the proxy for survival. By embedding the metadata of critical features as part of\nthe prompt, we personalize the reward design to handle critical patients with comorbidities. The LLM is prompted to\ndesign a potential function that rewards states closer to homeostasis. In the offline clinical setting, data irregularity\nposes a significant risk: a normal heart rate recorded 12 hours ago is not a reliable indicator of current stability. Since\nwe are unable to refresh the data by ordering new lab tests, we utilize the elapsed time ∆t as indicators of freshness\nof the data st = (ot, ∆tt). The LLM generates confidence weights for each critical feature that inversely scales the\nreward based on ∆t,f. This mechanism penalizes stale states, effectively discounting the survival reward when data\nis uncertain to reduce epistemic uncertainty. We also add a strategic decay to prevent unnecessarily longer ICU stay.\nTo prevent the agent from achieving survival via excessive or harmful interventions, we incorporate a competence\ncomponent focused on treatment efficiency. Mathematically, this serves as a regularization term: if two treatments result\nin the same physiological transition st →st+1, the potential function assigns a higher value to the less invasive action,\npenalizing over-treatment or flailing behavior. By decoupling the accumulating action cost C(a) from the telescoping\npotential Φ(s, t)1, we effectively formulate the RL objective as the Lagrangian of a Constrained Markov Decision\nProcess (CMDP). The agent maximizes the Lagrangian L = Physiology −λ · Toxicity, ensuring that high-stability\nstates are only pursued if the cost of the intervention path does not exceed the value of the stabilization2. Instead of\nmanual heuristic tuning, the LLM acts as the logic designer. We feed the definitions of these three drives along with\nthe feature metadata into the LLM, which autonomously synthesizes the logic into executable Python code for the\nreward function. To mitigate the risk of hallucination and capture diverse clinical perspectives, we generate N distinct\ncandidate functions in parallel, creating a diverse pool of candidates for the subsequent verification phase.\n3.3\nTri-Drive Reward Selection\nSince online verification is ethically prohibitive in clinical settings, we cannot evaluate reward candidates by deploying\nthem on patients. Instead, we propose a set of offline fitness metrics to rigorously assess the quality of each generated\nreward function R. We formulate reward design as a Tri-Drive Selection Problem aiming to balance three conflicting\nobjectives based on trajectory-level labeled ground truth: Survival, Confidence, and Competence Fitness.Survival\nFitness (Jsurv) validates whether the induced reward signal correlates with actual patient outcomes. We define a\nground-truth trajectory score G(τ) that consists of both mortality and Sequential Organ Failure Assessment(SOFA)\nscore[25] increase compared to baseline(time when admitted to ICU). The fitness is the correlation ρ between the\ncumulative reward and G(τ):\nG(τ) = 1(Survival) + 1\nT\nT −1\nX\nt=0\n1(|SOFAt −SOFAbaseline| < ϵ)\n(3)\nJsurv(R) = ρ (Rτ, G(τ))\n(4)\nConfidence Fitness (Jconf) penalizes reward functions that assign high credit to decisions made under high epistemic\nuncertainty. We define an uncertainty proxy U(τ) as the average time-gap ∆t of all the features f in the critical feature\nset F. The fitness is the negative correlation between the cumulative reward and trajectory uncertainty:\nU(τ) =\n1\nT · F\nT −1\nX\nt=0\nF\nX\nf=1\n∆tt,f.\n(5)\nJconf(R) = −ρ (Rτ, U(τ))\n(6)\nCompetence Fitness (Jcomp) ensures that the reward function prioritizes efficient strategies that achieve or maintain\nhigh physiological stability with minimal necessary intervention. We define feature type as T = {N, Dl, Dh} where\nN denotes Normal-Range features, and Dl, Dh denote Directional features where minimization or maximization is\nphysiologically optimal, respectively. We define the unified homeostasis score H(x) ∈[0, 1] for a feature of type\nT ∈T as:\nh(f) =\n\n\n\n\n\n1f∈I + 1f /∈I · σ\n\u0010\nk\nh\n0.5 −d(f,I)\nIQR\ni\u0011\nif T = N,\nσ (k [0.5 −f])\nif T = Dl,\nσ (−k [0.5 −f])\nif T = Dh,\n(7)\nH(st) =\n1\n|F|\nX\nf∈F\nh(t,f).\n(8)\n4\n"}, {"page": 5, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nWe then define the Efficiency Et as the homeostasis increase penalized by the magnitude of the dose taken:\nEt = H(st+1) −H(st) −α · ¯at.\n(9)\nJcomp = ρ (Rτ, Eτ) ,\n(10)\nwhere σ(z) = (1 + e−z)−1 is the standard sigmoid function, d(f, I) is the distance from feature f to the interval\nI, IQR is the interquartile range width used for normalization. α and k are cost parameters, ¯at is the magnitude of\nthe intervention. We employ the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) [26] to rank the generated\ncandidates based on these three objectives. Instead of selecting a single weighted average, we identify the Pareto\nFrontier where no objective can be improved without degrading another. From this frontier, the solution closest to the\nutopia point representing the best trade-off.\n4\nExperiments\n4.1\nTasks\nWe evaluate our framework on three distinct critical care tasks, covering different datasets, demographics, and action\nspace types.\nSepsis Treatment We utilize the Medical Information Mart for Intensive Care (MIMIC)-IV v3.1[27] database to\nlearn optimal Intravenous (IV) fluid and Vasopressor administration strategies for sepsis patients. We select adult\npatients (age ≥18) satisfying the Sepsis-3 criteria [28], and exclude patients with missing mortality, demographics, and\ninterventions, extreme missingness (>60% missing data) or short hospital stays (<24 hours). We define a septic shock\nstarting from 24 hours before diagnosis and ending at 48 hours post-diagnosis. After filtration, 7,501 patients with 46\nfeatures are retained in the dataset. The action space is discrete consisting of two primary interventions: IV fluids and\nVasopressors. We discretize IV fluid volume and vasopressor dosage into 5 bins each, resulting in a combined action\nspace of 5 × 5 = 25 discrete actions.\nMechanical Ventilation We use the multi-center eICU Collaborative Research Database (eICU-CRD)[29] to optimize\nventilator settings for patients with respiratory failure. The cohort includes adult patients who underwent invasive\nmechanical ventilation (MV) for at least 24 hours. We select patients aged ≥16 who received MV for at least 24 hours.\nWe also exclude patients with missing mortality outcomes, demographics, interventions, and those ventilated for >14\ndays. The final cohort contains 2168 patients with 41 features. The action space is discrete, controlling three key\nventilator parameters: Positive End-Expiratory Pressure (PEEP), Fraction of Inspired Oxygen (FiO2), and Tidal Volume.\nWe define a discrete action set based on clinically relevant adjustments for each parameter, resulting in a composite\ndiscrete action space of 18 actions (PEEP: 2 levels, FiO2: 3 levels, Tidal Volume: 3 levels).\nRenal Replacement Therapy We apply our method to the AmsterdamUMCdb [30] to optimize continuous renal\nreplacement therapy (RRT) dosing. We select adult patients (age ≥18) who received CRRT during the ICU stay where\nthe stay is no longer than 60 days. There are 857 patients with 18 features in the cohort. Unlike the previous tasks, the\naction space here is continuous. The agent controls the effluent flow rate clipped to the range [0, 60] ml/kg/h, a critical\nparameter determining the intensity of dialysis.\nFor all tasks, clinical time-series data (vitals, labs, medications) are aggregated into 1-hour time steps. Missing values\nare imputed using forward-filling for time-varying features. For CRRT effluent rate, we imputed values based on RRT\nsessions, defining a new session whenever the gap between two dialysis records exceeded 8 hours. All continuous\nstate features are normalized to zero mean and unit variance to ensure training stability. For policy training, we use\ncommon safe offline RL methods, Batch-Constrained Q-Learning (BCQ)[31] for discrete action space tasks (Sepsis and\nVentilation), and Implicit Q-Learning (IQL)[32] for continuous action space task (RRT).\n4.2\nBaselines\nTo rigorously evaluate the effectiveness of our automated reward design framework, we compare it against two categories\nof baselines: manually engineered heuristic rewards and direct LLM approaches.\n4.2.1\nHeuristic Reward Design\nWe implement three standard reward structures commonly used in clinical RL literature. These serve as the \"human-\nexpert\" baselines, representing varying degrees of clinical foresight:\nOutcome Reward Model (ORM): A purely outcome-oriented function that assigns a non-zero reward only at the\nterminal step, +100 for survival (discharge) and −100 for mortality. The value of ±100 is used in both Sepsis[5]\nand Ventilation[33] tasks in previous literature. Since there is no past work that utilized outcome reward for RRT\ntask, we adopt the same setting of ±100 for RRT. This baseline tests the agent’s ability to solve the credit assignment\n5\n"}, {"page": 6, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nproblem without intermediate guidance. We explicitly utilized ICU-discharge mortality rather than 90-day mortality as\nour terminal state definition in the baselines. We argue that an RL agent can only be held accountable for outcomes\nwithin its scope of control. An agent penalized for a death occurring 60 days post-discharge may erroneously learn that\nstabilizing and discharging a patient is suboptimal, leading to conservative strategies.\nProcess Reward Model (PRM): A process-oriented function that provides immediate feedback based on step-wise\nimprovements in physiological stability. It incentivizes short-term stabilization without long-term survival awareness.\nWe use the reward function fully based on SOFA score[6] for Sepsis. For Ventilation, though we note that there is one\nprocess reward based on ventilator free days[34], the reward calculated on our cohort is not dense enough for efficient\npolicy training, we therefore adopt the intermediate reward from the OPRM[35]. This further indicates that a fixed\nreward function may not be transferrable between different cohorts even with the same dataset. For RRT, we use the\nreward function based on Estimated Glomerular Filtration Rate (eGFR)[36].\nOutcome Process Reward Model(OPRM): A weighted combination of the sparse and dense signals. This represents\nthe state-of-the-art manual design, aiming to balance immediate physiological regularization with the ultimate goal of\npatient survival. We select from current SOTA methods [37]for Sepsis and [35]for Ventilation. Due to the lack of work\non using OPRM for RRT, we choose to combine the PRM(eGFR) with ORM(±100) as OPRM.\n4.2.2\nLLM-based Design\nWe also compare against methods that leverage LLMs for reward generation without our proposed iterative refinement\nprocess. Since there are limited methods that leverage LLM to generate rewards for offline RL, we design two variants:\nLLM-as-Reward (LLMR): In this setting, the LLM is used as a direct proxy for the reward function or a scorer of the\nclinicians’ actions. We embed the whole patient trajectory (s, a, s′) with final outcome mortality as part of the prompt\nalong with clinical context, based on the mortality of the patient, we let the LLM do credit assignment of +15 for\nsurvival or −15 for mortality along the trajectory. The LLM will choose steps that it thinks is the most crucial for the\ndecision-making and the rest of the steps will be assigned 0. After the reward generation we will make sure the sum of\nreward is +15 or −15. This tests the LLM’s innate ability to evaluate clinical actions without explicit function coding.\nCode Generation (CodeGen): The LLM is prompted to write a Python reward function in a single pass, given only the\ntask description and list of available features without any further instructions on the reward components. This baseline\nevaluates the code-generation capability of the LLM absent the Tri-Drive and potential functions utilized in our method.\n4.3\nEvaluation Metrics\nWe employ a comprehensive set of metrics to evaluate both the intrinsic quality of the generated reward functions and\nthe extrinsic performance of the resulting RL policies.\nAction Agreement among Survivors: We calculate the percentage of time steps among the whole trajectory where the\nRL agent’s chosen action matches the clinician’s action, specifically within the cohort of patients who survived. High\nagreement on survivors suggests the agent learns safe, effective behaviors from successful clinical examples.\nOff-Policy Evaluation(OPE) To quantify the policy’s theoretical performance, we employ Weighted Importance\nSampling (WIS), a standard unbiased estimator for off-policy value estimation. To facilitate direct comparison, we use\nthe same rewards generated by DeepSeek-R1 that was not involved in training nor as a baseline for WIS calculation on\nall the policies. We also plot WIS training curve to make sure that the return is higher as we train the policy.\nMortality vs. Cumulative Reward: To prove that our reward is highly correlated with ground truth mortality, we plot\nthe observed patient mortality rate against cumulative reward along patient trajectories assigned by the clinician’s policy.\nA monotonic decreasing trend indicates that the learned value function correctly identifies patient risk. We compare\nwith PRM which is the only baseline that also does not contain mortality as part of rewards.\nTable 1: Agreement rates among survivors compared to clinician actions across tasks.\nMethod\nSepsis\nVentilation\nRRT\nJoint\nIV Fluids\nVasopressors\nJoint\nPEEP\nFiO2\nTidal Volume\nDose\nOn/Off\nORM\n25.37 ± 0.78\n30.01 ± 0.69\n77.98 ± 1.01\n19.79 ± 3.13\n74.80 ± 3.13\n47.87 ± 2.26\n48.23 ± 3.41\n50.77 ± 15.96\n67.54 ± 16.14\nPRM\n27.54 ± 1.03\n33.17 ± 0.96\n78.47 ± 0.95\n21.24 ± 2.37\n74.49 ± 3.26\n49.34 ± 2.60\n50.70 ± 3.51\n54.64 ± 10.51\n74.45 ± 3.03\nOPRM\n27.51 ± 1.39\n33.04 ± 1.27\n78.44 ± 1.08\n21.25 ± 2.58\n73.24 ± 3.63\n49.30 ± 2.22\n50.02 ± 2.88\n56.46 ± 11.66\n74.83 ± 2.46\nLLMR+GPT-OSS-20B\n24.78 ± 0.48\n30.46 ± 0.48\n77.62 ± 1.05\n21.86 ± 5.85\n75.33 ± 5.59\n52.39 ± 5.86\n46.08 ± 7.39\n50.63 ± 15.14\n72.95 ± 5.04\nCodeGen+GPT-5\n27.63 ± 0.87\n33.21 ± 0.61\n78.46 ± 1.00\n21.09 ± 2.21\n76.04 ± 3.34\n49.30 ± 2.06\n49.18 ± 2.90\n54.15 ± 11.38\n74.43 ± 3.00\nCodeGen+Qwen3-Max\n27.55 ± 0.47\n33.41 ± 0.28\n78.56 ± 0.99\n21.31 ± 2.85\n76.48 ± 3.52\n49.78 ± 2.59\n50.72 ± 4.25\n53.82 ± 11.77\n70.46 ± 9.84\nmedR+GPT-OSS-20B\n27.45 ± 0.93\n33.67 ± 0.59\n78.46 ± 1.07\n24.10 ± 4.40\n77.06 ± 4.29\n57.13 ± 3.30\n50.83 ± 3.65\n53.82 ± 11.77\n70.46 ± 9.84\nmedR+GPT-4\n27.70 ± 0.41\n33.75 ± 0.22\n78.47 ± 0.95\n24.20 ± 4.23\n76.67 ± 3.95\n56.92 ± 3.79\n50.41 ± 3.59\n66.44 ± 8.42\n76.92 ± 4.30\n6\n"}, {"page": 7, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n4.4\nRQ1: Should we use mortality as fitness or reward?\nA prevalent assumption in clinical RL is that agents trained solely on sparse mortality outcomes will asymptotically\nconverge to optimal behavior via automatic temporal credit assignment. However, this premise rarely holds true in the\noffline, high-noise, stochastic regime of critical care, and can lead to “superstitious learning.” In ICU a robust patient\nmay survive despite suboptimal actions, while a high-severity patient may die despite optimal care. As shown in Table1\nand Table8, the policy trained under ORM performs significantly worse than the other baseline and our method on the\nagreement among survivors which reveals that the former struggles to distinguish between policy quality and patient\nstochasticity. Simply maximizing survival may move the patient away from wellbeing. In Table 2, the ORM reward\nalso results in the lowest return in both Sepsis and Ventilation tasks. Furthermore, utilizing outcome that happens later\ncan introduce hindsight bias, the training signal should only depend on information available at decision time. We avoid\nthis by aligning the reward with the clinician’s reality, optimizing immediate physiology as a proxy of survival. We\nseparate mortality from reward and instead use the hindsight ground truth as fitness to evaluate whether the reward\nfunction induces high-quality policies. This confirms that explicitly rewarding the process of stabilization provides a\nmore reliable learning signal than the noisy outcome.\nFigure 2: The relationship between cumulative process reward and patient mortality probability for different tasks. A\nvalid process should demonstrate clear downward trend with mortality, baselines that include mortality as part of the\nreward design are excluded for fair comparison.\n4.5\nRQ2: Can we use naive shaping of outcome reward model?\nTo facilitate policy training previous literature actively utilizes clinically-guided dense rewards that include both terminal\nand intermediate rewards to form an OPRM[38, 8], which can be approximated as a naive reward shaping in a broader\nsense. As shown in Figure2, the standalone process reward is not well correlated with mortality, which is dangerous as a\nsecondary outcome to guide policy learning. In Figure4, naive shaping candidates often dominate on the Jsurv axis but\ncollapse on the Jcomp axis. This indicates a reward hacking phenomenon where the agent learns to accumulate stability\nbonuses by maintaining the patient in a safe but non-improving state, rather than efficiently steering them toward\ndischarge. This is proved by Table2 where it achieves low WIS in both Ventilation and RRT tasks. In contrast, medR\n7\n"}, {"page": 8, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nmaintains high performance across all three objectives. Our method avoids this failure mode through the time-dependent\ndecay factor δ(t) explicitly penalizes loitering. As Φ(s, t) decays towards zero over time, the agent is mathematically\nforced to achieve stabilization targets early in the episode to maximize the return. By utilizing the potential difference\nγΦt+1 −Φt, our method mathematically ensures that rewards are only generated by state improvement rather than\nstate maintenance, effectively preventing the infinite horizon hacking observed in the baseline.\nTable 2: Comparison of WIS scores with bootstrapped confidence intervals (95%CI).\nMethod\nSepsis\nVentilation\nRRT\nClinician (Baseline)\n−54.50\n(−55.49, −53.51)\n−56.94\n(−57.29, −56.59)\n−87.61\n(−92.51, −82.83)\nORM\n−45.34\n(−46.53, −44.02)\n−31.06\n(−32.90, −29.20)\n−34.95\n(−51.31, −17.35)\nPRM\n−13.12\n(−14.03, −12.24)\n−24.97\n(−26.47, −23.59)\n−38.97\n(−51.82, −26.13)\nOPRM\n−17.73\n(−18.93, −16.63)\n−24.30\n(−25.59, −23.20)\n−35.57\n(−51.75, −19.90)\nLLMR+GPT-OSS-20B\n−17.86\n(−18.88, −17.11)\n−20.72\n(−22.89, −18.82)\n−38.98\n(−52.63, −25.34)\nCodeGen+GPT-5\n−28.68\n(−31.08, −25.75)\n−21.97\n(−24.73, −19.34)\n−39.90\n(−53.42, −26.37)\nCodeGen+Qwen3-Max\n−15.17\n(−15.82, −14.52)\n−24.06\n(−26.98, −22.04)\n−41.15\n(−53.77, −27.98)\nmedR+GPT-OSS-20B\n−12.44\n(−13.23, −11.64)\n−18.42\n(−20.13, −16.71)\n−39.49\n(−52.37, −26.61)\nmedR+GPT-4\n−12.37\n(−13.12, −11.63)\n−18.96\n(−20.77, −17.13)\n−34.73\n(−52.70, −16.53)\n4.6\nRQ3: Do we need to include action as part of reward?\nPrevious reward function design rarely includes action as part of reward signals. Without action costs, clinical agents\nare prone to masking physiological deterioration with high-intensity interventions. While this improves the immediate\nstate representation, it often induces long-term toxicity. In Figure5,6,7, policies trained by heuristic reward tend to give\nhigher dosages compared to medR. By strictly penalizing the intervention magnitude, our full Tri-Drive objective forces\nthe agent to solve a Lagrangian relaxation of the clinical problem: maximize stability subject to minimum necessary\nharm. Our policy achieves the highest Jcomp in all three tasks (Figure4) while achieving the highest WIS(Figure2).\n4.7\nRQ4: Do LLMs qualify as reward functions?\nLLMs are rapidly developing and demonstrating excellent reasoning capabilities. Previous research has explored\nthe possibility of directly using LLMs as planners for DTRs[39] and concludes that they exhibit notable limitations.\nTherefore, a natural question is whether they can instead directly serve as the reward signal instead and output a scalar\nquality score. Our experiments indicate that LLMs are ill-suited for this direct role in the clinical offline RL setting.\nWhile an LLM can accurately identify immediate physiological abnormalities, it fails to perform temporal credit\nassignment. The standard LLMR baseline consistently demonstrates lower agreement with clinical policies compared to\nmedR across all tasks, particularly in complex interventions like RRT dosing where it trails by over 15 percentage points\n(50.63% vs. 66.44%). Furthermore, LLMR exhibits notably higher variance, suggesting that generic LLM-generated\nrewards lack the stability and precision of our potential-based approach. While they naturally demonstrate comparably\nhigher Jsurv since we force the sum of reward to be a direct indicator of mortality, they systematically collapse on\nJconf and Jcomp, suggesting it generates survival-at-all-costs policies that ignore clinical efficiency and data uncertainty.\nBy prompting the LLM with the full trajectory and outcome, the hindsight bias appears again and the model will tend to\nhallucinate positive or negative rewards along the whole trajectory. This confirms that while LLMs possess domain\nknowledge, they function as semantic pattern matchers rather than value function approximators.\n8\n"}, {"page": 9, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nTable 3: Ablation study. We evaluate the contribution of feature selection, the potential term, and Tri-Drive logic and\nprove medR demonstrates the most consistent performance across tasks.\nAblation Variant\nJsurv\nJconf\nJcomp\nSepsis\nw/o Feature Selection\n0.61\n0.47\n0.55\nw/o Potential\n0.44\n0.50\n0.65\nw/o Tri-Drive Logic\n0.56\n0.54\n0.56\nmedR\n0.68\n0.57\n0.69\nVentilation\nw/o Feature Selection\n0.65\n0.77\n0.55\nw/o Potential\n0.41\n0.24\n0.49\nw/o Tri-Drive Logic\n0.60\n0.74\n0.59\nmedR\n0.69\n0.82\n0.61\nRRT\nw/o Feature Selection\n0.50\n0.46\n0.42\nw/o Potential\n0.54\n0.32\n0.48\nw/o Tri-Drive Logic\n0.52\n0.70\n0.58\nmedR\n0.60\n0.66\n0.60\n4.8\nRQ5: Can LLMs directly generate reward code?\nIf LLMs cannot directly act as the reward model, can they write the codes for a high-quality reward function directly? We\nevaluated the capability of multiple LLMs to generate reward functions from open-ended clinical prompts. The results\nwere consistently suboptimal. Table 1 reveals a critical limitation in direct code generation approaches (CodeGen) where\nthey lack the structural robustness required for complex interventions like RRT. They also yield lower WIS(Figure2\nwhere our potential-based medR framework outperforms the strongest CodeGen baseline. These results underscore\nthat raw LLM reasoning capability alone is insufficient; without the control-theoretic scaffolding provided by medR,\neven superior models struggle to synthesize reward functions that capture the nuance of rare clinical events. Moreover,\nCodeGen variants exhibit significant volatility; CodeGen+GPT-5(CG(G)) achieves a high Jsurv of 0.73 in Sepsis but\ndrops to 0.50 in Competence, indicating the agent maximizes the outcome signal via inefficient or unsafe intervention\nloads. In contrast, medR maintains a robust fitness profile across all three dimensions, validating that our potential\nfunction effectively balances survival objectives with necessary safety and efficiency constraints.\nFigure 3: WIS training plot.\n4.9\nAblation Study\nTo rigorously evaluate the individual contributions of our framework’s components, we conducted an ablation study by\nsystematically removing (1) the LLM-driven Feature Selection, (2) the Potential-based reward formulation, and (3) the\nTri-Drive structured logic. Table 3 summarizes the impact of these components on policy fitness (Jsurv, Jconf, Jcomp)\nacross all three clinical domains. The most critical component of our framework is the potential difference formulation\n(γΦt+1 −Φt). Removing this term results in a catastrophic collapse in performance, particularly in the Mechanical\nVentilation domain, where Jconf drops precipitously from 0.82 to 0.24. This confirms that without the telescoping sum\n9\n"}, {"page": 10, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nproperty, the reward signal fails to distinguish between state maintenance and state improvement, leading to policies that\nare highly uncertain and poorly correlated with patient survival. The ablation of automated feature selection highlights\nthe necessity of filtering the high-dimensional clinical state space. This effect is most pronounced in the RRT task,\nwhere removing feature selection degrades survival fitness (Jsurv) from 0.60 to 0.40. In complex domains like renal\nreplacement, irrelevant features introduce significant noise; our LLM-driven selection effectively isolates the causal\nvariables required for stable learning. Finally, we evaluate the structural scaffolding of the Survival, Confidence, and\nCompetence drives. While this variant occasionally achieves high scores in isolated metrics, for example, achieving the\nhighest Jconf of 0.70 in RRT, it fails to maintain the Pareto balance required for effective care. For instance, in RRT, the\nincrease in confidence comes at the cost of survival fitness. The full medR framework forces a necessary trade-off,\noptimizing for policies that are simultaneously safe, effective, and efficient.\n5\nConclusion\nIn this work, we introduced medR, a novel framework for automating reward engineering in offline clinical RL. By\nleveraging LLMs to define Tri-Drive potential functions, medR successfully addressed policy optimization across\nthree distinct clinical tasks. The substantial performance gains observed in both traditional OPE metrics and our\nproposed Tri-Drive fitness scores validate the efficacy of utilizing LLMs for offline reward shaping. Given the clinical\nnecessity for reward functions that are personalized to specific diseases and patient cohorts, medR establishes a scalable\nfoundation for applying reinforcement learning to broader medical challenges and more complex intervention spaces.\n6\nImpact Statement\nThis work addresses one of the most significant barriers to the deployment of RL in healthcare: the difficulty of defining\nmathematically precise reward functions that align with complex clinical objectives. By automating this process through\nmedR, we lower the technical barrier for clinicians and researchers to develop custom treatment policies for diverse\npatient cohorts and rare pathologies, potentially democratizing access to precision medicine tools. Methodologically,\nour Tri-Drive framework contributes to AI safety by explicitly embedding uncertainty quantification (Confidence) and\ndosage constraints (Competence) into the reward structure, mitigating the risks of aggressive policy behavior common\nin offline RL. However, we acknowledge that LLM-generated rewards may inherit biases present in training data or\nprompt phrasing. While our results demonstrate high correlation with clinical outcomes, rigorous prospective validation\nand human-in-the-loop oversight remain essential prerequisites before any clinical deployment.\nReferences\n[1] Yiwen Zhu, Jinyi Liu, Pengjie Gu, Yifu Yuan, Zhenxing Ge, Wenya Wei, Zhou Fang, Yujing Hu, and Bo An.\nImproving reward models with proximal policy exploration for preference-based reinforcement learning. In The\nThirty-ninth Annual Conference on Neural Information Processing Systems.\n[2] Luca F Roggeveen, Ali el Hassouni, Harm-Jan de Grooth, Armand RJ Girbes, Mark Hoogendoorn, Paul WG\nElbers, and Dutch ICU Data Sharing Against COVID-19 Collaborators. Reinforcement learning for intensive care\nmedicine: actionable clinical insights from novel approaches to reward shaping and off-policy model evaluation.\nIntensive Care Medicine Experimental, 12(1):32, 2024.\n[3] W Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis) design for\nautonomous driving. Artificial Intelligence, 316:103829, 2023.\n[4] Sumana Basu, Adriana Romero-Soriano, and Doina Precup. Reward the reward designer: Making reinforcement\nlearning useful for clinical decision making. In Women in Machine Learning Workshop@ NeurIPS 2025, 2025.\n[5] M. Komorowski, L. A. Celi, O. Badawi, A. C. Gordon, and A. A. Faisal. The artificial intelligence clinician learns\noptimal treatment strategies for sepsis in intensive care. Nature medicine, 24(11):1716–1720, 2018.\n[6] Tianyi Zhang, Yimeng Qu, Deyong Wang, Ming Zhong, Yunzhang Cheng, and Mingwei Zhang. Optimizing\nsepsis treatment strategies via a reinforcement learning model. Biomedical Engineering Letters, 14(2):279–289,\n2024.\n[7] A. Raghu, M. Komorowski, and S. Singh. Model-based reinforcement learning for sepsis treatment. arXiv preprint\narXiv:1811.09602, 2018.\n[8] Zhilin Lu, Jingming Liu, Ruihong Luo, and Chunping Li. Reinforcement learning with balanced clinical reward\nfor sepsis treatment. In International Conference on Artificial Intelligence in Medicine, pages 161–171. Springer,\n2024.\n10\n"}, {"page": 11, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n[9] Erin T Livingston, Md Huzzatul Mursalin, and Michelle C Callegan. A pyrrhic victory: the pmn response to\nocular bacterial infections. Microorganisms, 7(11):537, 2019.\n[10] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. arXiv\npreprint arXiv:2303.00001, 2023.\n[11] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv\npreprint arXiv:2310.12931, 2023.\n[12] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu.\nText2reward: Reward shaping with language models for reinforcement learning. arXiv preprint arXiv:2309.11489,\n2023.\n[13] Satinder Singh, Richard L Lewis, and Andrew G Barto. Where do rewards come from. In Proceedings of the\nannual conference of the cognitive science society, pages 2601–2606. Cognitive Science Society, 2009.\n[14] Satinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated reinforcement\nlearning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70–82,\n2010.\n[15] Clark L Hull. Principles of behavior, 1945.\n[16] Daniel E Berlyne. Conflict, arousal, and curiosity. 1960.\n[17] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers.\nIn Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages\n222–227, 1991.\n[18] Benjamin Djulbegovic. Uncertainty and equipoise: at interplay between epistemology, decision making and ethics.\nThe American journal of the medical sciences, 342(4):282–289, 2011.\n[19] Robert W White. Motivation reconsidered: the concept of competence. Psychological review, 66(5):297, 1959.\n[20] Floris den Hengst, Martijn Otten, Paul Elbers, Frank van Harmelen, Vincent Francois-Lavet, and Mark Hoogen-\ndoorn. Guideline-informed reinforcement learning for mechanical ventilation in critical care. Artificial Intelligence\nin Medicine, 147:102742, 2024.\n[21] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and\napplication to reward shaping. In Icml, volume 99, pages 278–287. Citeseer, 1999.\n[22] Eric Wiewiora, Garrison W Cottrell, and Charles Elkan. Principled methods for advising reinforcement learning\nagents. In Proceedings of the 20th international conference on machine learning (ICML-03), pages 792–799,\n2003.\n[23] Sam Michael Devlin and Daniel Kudenko. Dynamic potential-based reward shaping. In 11th International\nConference on Autonomous Agents and Multiagent Systems (AAMAS 2012), pages 433–440. IFAAMAS, 2012.\n[24] Grant C Forbes, Nitish Gupta, Leonardo Villalobos-Arias, Colin M Potts, Arnav Jhala, and David L Roberts.\nPotential-based reward shaping for intrinsic motivation. arXiv preprint arXiv:2402.07411, 2024.\n[25] Simon Lambden, Pierre Francois Laterre, Mitchell M Levy, and Bruno Francois. The sofa score—development,\nutility and challenges of accurate assessment in clinical trials. Critical Care, 23(1):374, 2019.\n[26] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic\nalgorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2):182–197, 2002.\n[27] Alistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark. Mimic-iv.\nPhysioNet. Available online at: https://physionet. org/content/mimiciv/1.0/(accessed August 23, 2021), pages\n49–55, 2020.\n[28] Mervyn Singer, Clifford S Deutschman, Christopher Warren Seymour, Manu Shankar-Hari, Djillali Annane,\nMichael Bauer, Rinaldo Bellomo, Gordon R Bernard, Jean-Daniel Chiche, Craig M Coopersmith, et al. The third\ninternational consensus definitions for sepsis and septic shock (sepsis-3). Jama, 315(8):801–810, 2016.\n[29] Tom J Pollard, Alistair EW Johnson, Jesse D Raffa, Leo A Celi, Roger G Mark, and Omar Badawi. The eicu\ncollaborative research database, a freely available multi-center database for critical care research. Scientific data,\n5(1):1–13, 2018.\n[30] Patrick J Thoral, Jan M Peppink, Ronald H Driessen, Eric JG Sijbrands, Erwin JO Kompanje, Lewis Kaplan,\nHeatherlee Bailey, Jozef Kesecioglu, Maurizio Cecconi, Matthew Churpek, et al. Sharing icu patient data\nresponsibly under the society of critical care medicine/european society of intensive care medicine joint data\nscience collaboration: the amsterdam university medical centers database (amsterdamumcdb) example. Critical\ncare medicine, 49(6):e563–e577, 2021.\n11\n"}, {"page": 12, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n[31] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In\nInternational conference on machine learning, pages 2052–2062. PMLR, 2019.\n[32] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv\npreprint arXiv:2110.06169, 2021.\n[33] Arne Peine, Ahmed Hallawa, Johannes Bickenbach, Guido Dartmann, Lejla Begic Fazlic, Anke Schmeink, Gerd\nAscheid, Christoph Thiemermann, Andreas Schuppert, Ryan Kindle, et al. Development and validation of a\nreinforcement learning algorithm to dynamically optimize mechanical ventilation in critical care. NPJ digital\nmedicine, 4(1):32, 2021.\n[34] Muhammad Hamza Yousuf, Jason Li, Sahar Vahdati, Raphael Theilen, Jakob Wittenstein, and Jens Lehmann.\nIntellilung: Advancing safe mechanical ventilation using offline rl with hybrid actions and clinically aligned\nrewards. arXiv preprint arXiv:2506.14375, 2025.\n[35] Siqi Liu, Qianyi Xu, Zhuoyang Xu, Zhuo Liu, Xingzhi Sun, Guotong Xie, Mengling Feng, and Kay Choong\nSee. Reinforcement learning to optimize ventilator settings for patients on invasive mechanical ventilation:\nretrospective study. Journal of Medical Internet Research, 26:e44494, 2024.\n[36] Haoyuan Zhang, Minqi Xiong, Tongyue Shi, Wentie Liu, Haowei Xu, Huiying Zhao, and Guilan Kong. Rein-\nforcement learning-based decision-making for renal replacement therapy in icu-acquired aki patients. In Artificial\nIntelligence and Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare, 2024.\n[37] Dilruk Perera, Siqi Liu, Kay Choong See, and Mengling Feng. Smart imitator: Learning from imperfect clinical\ndecisions. Journal of the American Medical Informatics Association, 33(1):49–66, 2026.\n[38] A. Raghu, M. Komorowski, L. A. Celi, P. Szolovits, and M. Ghassemi. Continuous state-space models for optimal\nsepsis treatment: a deep reinforcement learning approach. In Machine Learning for Healthcare Conference, pages\n147–163. PMLR, 2017.\n[39] Zhiyao Luo and Tingting Zhu. Are large language models dynamic treatment planners? an in silico study from a\nprior knowledge injection angle. arXiv preprint arXiv:2508.04755, 2025.\n[40] Y. Huang, R. Cao, and A. Rahmani. Reinforcement learning for sepsis treatment: A continuous action space\nsolution. In Machine Learning for Healthcare Conference, pages 631–647. PMLR, 2022.\n[41] X. Peng, Y. Ding, D. Wihl, O. Gottesman, M. Komorowski, L. W. H. Lehman, and F. Doshi-Velez. Improving sepsis\ntreatment strategies by combining deep and kernel-based reinforcement learning. In AMIA Annual Symposium\nProceedings, volume 2018, page 887, 2018.\n[42] J. Futoma, S. Hariharan, K. Heller, M. Sendak, N. Brajer, M. Clement, and C. O’Brien. An improved multi-output\ngaussian process rnn with real-time validation for early sepsis detection. In Machine Learning for Healthcare\nConference, pages 243–254. PMLR, 2017.\n[43] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large language\nmodels in medicine. Nature medicine, 29(8):1930–1940, 2023.\n[44] J. Haltaufderheide and R. Ranisch. The ethics of chatgpt in medicine and healthcare: a systematic review on large\nlanguage models (llms). NPJ digital medicine, 7(1):183, 2024.\n[45] S. Shool, S. Adimi, R. Saboori Amleshi, E. Bitaraf, R. Golpira, and M. Tara. A systematic review of large language\nmodel (llm) evaluations in clinical medicine. BMC Medical Informatics and Decision Making, 25(1):117, 2025.\n[46] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin, and V. Natarajan. Toward expert-level medical\nquestion answering with large language models. Nature Medicine, pages 1–8, 2025.\n[47] R. Yang, F. Ye, J. Li, S. Yuan, Y. Zhang, Z. Tu, and D. Yang. The lighthouse of language: Enhancing llm agents\nvia critique-guided improvement. arXiv preprint arXiv:2503.16024, 2025.\n[48] E. Asgari, N. Montaña-Brown, M. Dubois, S. Khalil, J. Balloch, J. A. Yeung, and D. Pimenta. A framework\nto assess clinical safety and hallucination rates of llms for medical text summarisation. npj Digital Medicine,\n8(1):1–15, 2025.\n[49] S. Bhambri, A. Bhattacharjee, D. Kalwar, L. Guan, H. Liu, and S. Kambhampati. Extracting heuristics from large\nlanguage models for reward shaping in reinforcement learning. arXiv preprint arXiv:2405.15194, 2024.\n[50] X. Wu. From reward shaping to q-shaping: Achieving unbiased learning with llm-guided knowledge. arXiv\npreprint arXiv:2410.01458, 2024.\n[51] S. Zhang, S. Zheng, S. Ke, Z. Liu, W. Jin, J. Yuan, and Z. Wang. How can llm guide rl? a value-based approach.\narXiv preprint arXiv:2402.16181, 2024.\n12\n"}, {"page": 13, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n[52] J. Gao, S. Xu, W. Ye, W. Liu, C. He, W. Fu, and Y. Wu. On designing effective rl reward at training time for llm\nreasoning. arXiv preprint arXiv:2410.15115, 2024.\n[53] M. Pternea, P. Singh, A. Chakraborty, Y. Oruganti, M. Milletari, S. Bapat, and K. Jiang. The rl/llm taxonomy tree:\nReviewing synergies between reinforcement learning and large language models. Journal of Artificial Intelligence\nResearch, 80:1525–1573, 2024.\n[54] H. Sun. Reinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf,\nprompting, and beyond. arXiv preprint arXiv:2310.06147, 2023.\n[55] M. S. Nazir and C. Banerjee. Zero-shot llms in human-in-the-loop rl: Replacing human feedback for reward\nshaping. arXiv preprint arXiv:2503.22723, 2025.\n[56] J. Fu, X. Zhao, C. Yao, H. Wang, Q. Han, and Y. Xiao. Reward shaping to mitigate reward hacking in rlhf. arXiv\npreprint arXiv:2502.18770, 2025.\n[57] Y. Deng, X. Qiu, J. Chen, and X. Tan. Reward guidance for reinforcement learning tasks based on large language\nmodels: The lmgt framework. Knowledge-Based Systems, page 113689, 2025.\n[58] H. Wang, C. T. Leong, J. Wang, J. Wang, and W. Li. Spa-rl: Reinforcing llm agents via stepwise progress\nattribution. arXiv preprint arXiv:2505.20732, 2025.\n[59] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. High-dimensional continuous control using\ngeneralized advantage estimation. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML), pages 871–879, 2015.\n[60] A. Kwiatkowski, V. Kalogeiton, J. Pettré, and M.-P. Cani. Ugae: A novel approach to non-exponential discounting.\narXiv, 2023.\n[61] J. Wang, E. Blaser, H. Daneshmand, and S. Zhang. Transformers can learn temporal difference methods for\nin-context reinforcement learning. In Proceedings of ICLR 2025, 2025.\n[62] Feng Wu, Guoshuai Zhao, Yuerong Zhou, Xueming Qian, Baedorf-Kassis Elias, and Li-wei H Lehman. Forecasting\ntreatment outcomes over time using alternating deep sequential models. IEEE Transactions on Biomedical\nEngineering, 71(4):1237–1246, 2023.\n[63] Hong Xiong, Feng Wu, Leon Deng, Megan Su, Zach Shahn, and Li-wei H Lehman. G-transformer: Counterfactual\noutcome prediction under dynamic and time-varying treatment regimes. Proceedings of machine learning research,\n252:https–proceedings, 2024.\n[64] Leon Deng, Hong Xiong, Feng Wu, Sanyam Kapoor, Soumya Ghosh, Zach Shahn, and Li-wei H Lehman. Uncer-\ntainty quantification for conditional treatment effect estimation under dynamic treatment regimes. Proceedings of\nmachine learning research, 259:248, 2024.\n[65] E. Pignatelli, J. Ferret, T. Rockäschel, E. Grefenstette, D. Paglieri, S. Coward, and L. Toni. Assessing the zero-shot\ncapabilities of llms for action evaluation in rl. In arXiv preprint arXiv:2409.12798, 2024.\n[66] Z. Jiang, B. Zhang, A. Wei, and Z. Xu. Qllm: Do we really need a mixing network for credit assignment in\nmulti-agent reinforcement learning? arXiv preprint arXiv:2504.12961, 2025.\n[67] Y. Qu, Y. Jiang, B. Wang, Y. Mao, C. Wang, C. Liu, and X. Ji. Latent reward: Llm-empowered credit assignment\nin episodic reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39,\npages 20095–20103, 2025.\n13\n"}, {"page": 14, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nA\nPrompts\nListing 1: Prompt for Feature Selection\nYou are an expert\nClinical\nData\nScientist\nand\nIntensivist\nspecializing in Offline\nReinforcement\nLearning.\nTASK: Select the top 7 ** Critical\nState\nFeatures ** based on the\nstatistical\nanalysis\nprovided\nbelow.\nSELECTION\nCRITERIA:\n- Select\nfeatures\nthat are strong\nindicators of patient\ncondition\nthat are highly\ncorrelated to the\ndisease\nand patient\noutcome.\n- Exclude\nfeatures\nthat are direct\nproxies of interventions to prevent\nreward\nhacking (high\ncorrelation\nwith\nactions).\n- EXCLUDE\nall\ndemographic\nand\nbaseline\nfeatures: age , gender , elixhauser\nvanwalraven , weight , readmission , step id. These are static\npatient\ncharacteristics , not\ndynamic\nstate\nfeatures. Focus\nONLY on dynamic\nphysiological\nand\nclinical\nstate\nfeatures\nthat\nchange\nover time.\n- Prefer\nfeatures\nwith low\nmissingness\nand strong\npredictive\npower for\noutcomes.\nOUTPUT\nFORMAT: IMPORTANT: Output\nONLY\nvalid\nJSON. Do not\ninclude\nany preamble ,\nthinking\nprocess , explanations , or text\nbefore or after the JSON.\nReturn a JSON\nobject\nwith keys: critical\nstate\nfeatures , rank the\nfeatures by\ntheir\nimportance\nfor reward\nmodeling. For each feature , provide a 1-sentence\nrationale\nexplaining\nits\nselection\nbased on the\nprovided\nstatistics.\nFormat\nyour\nresponse as structured\nJSON with the\nfollowing\nschema:\n{\" critical_state_features \": [{\n\"feature_name \": \"...\" ,\n\"rationale \": \"...\"\n}]}\nYour\nresponse\nmust\nstart\nwith \" and end with \". Do not\ninclude\nany other\ntext.\nDATASET\nSUMMARY:\nTotal\nrecords:\nTotal\npatients:\nAverage\nrecords\nper\npatient:\nMortality\nRates:\nFEATURE\nSTATISTICS:\nFeature:\nCount:\nMean:\nRange:\nMedian:\nIQR:\nCORRELATIONS\nWITH\nOUTCOMES:\nOutcome: mortality\n- feature\\_name: r= (p=, n=)\nACTION -FEATURE\nCORRELATIONS (to identify\naction -dependent\nfeatures):\nAction: action1\n- feature\\_name: r= (p=, n=)\nListing 2: Prompt for Reward Generation\nYou are an expert in clinical\ndata\nscience\nspecializing in Offline\nReinforcement\nLearning. Your task is to ** DESIGN ** a reward\nfunction\nbased on a Potential\nFunction ($\\Phi$) for an RL agent to learn the\noptimal\npolicy for sepsis\ntreatment ([’ vaso_5quantile ’, ’iv_fluid_5quantile ’]).\nReward is difference -based\nwith\ndiscount\nfactor: R(s, a, t, s’, a’, t’) = \\gamma \\\nPhi(s’, t’) - \\Phi(s, t) - \\lambda C(a)\n\\Phi(s, a, t) is the\npotential\nfunction C(a) is the\ncompetence cost , s=(o, \\Delta\nt) is the state , observations (o) are the\ncritical\nfeatures\nthat we selected\nimputed\nwith\nforward fill , and \\Delta t is the time gap from\ncurrent\ntime step\nto the last step\nwhere\nthere is a real\nrecord. a is the action\ndictionary\nwith\nkeys [’vaso_5quantile ’, ’iv_fluid_5quantile ’] for\ntreatment. t is the\nabsolute\ntime\nstamp\nindicated\nusing\ntime step (integers\nstarting\nfrom 0).\nYou must ** invent ** the\nmathematical\nlogic for three\ncomponents\nbased on the data\nbelow.\n## PART 1: MATHEMATICAL\nDESIGN (You must\nthink\nfirst)\n14\n"}, {"page": 15, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nThe\npotential\nfunction\nconsists of three\ncomponents , survival , confidence , and\ncompetence. For each\ncomponent , explicitly\nstate the\nformula\nyou will use.\n1. ** Survival :** Choose a function\nthat\nscores\nphysiology\n(0 -1).\n- *Design\nConstraint :* Use both\nclinical\nknowledge\nand\nfeature\nstatistics to\ndefine the\nhealthy\nrange. Higher\npotential\nwhen the\nphysiological\nstates\nmove\ntowards\nhealthier\nrange. \"Goldilocks\" features\nneed Bell\ncurves. Directional\nfeatures\nneed\ndecay\ncurves.\n2. ** Confidence :** Choose a decay\nfunction\nfor $\\Delta t$.\n- *Design\nConstraint :* Trust\nmust drop as time gap\nincreases.\n3. ** Competence :** Choose a penalty\nfunction\nfor\nactions.\n- *Design\nConstraint :* Higher\ndose = Lower\npotential.\nAlso\ndesign a time\ndecay\nfuntion\nfor\nstrategy\nannealing , which\ndecays the\npotential\ntoward\nzero as the\nepisode\nprogresses. This is different\nfrom the\nconfidence\ncomponent.\n### PART 2: PYTHON\nIMPLEMENTATION\nRULES (CRITICAL)\n1. **NO DUMMY\nFUNCTIONS .** Every\nfunction\nyou call must be defined in this\nscript.\nDo not write ‘_compute_survival ()‘ unless you write the code for it\nimmediately\nabove.\n2. **NO EXTERNAL\nCONFIG .** All\ndictionaries (Targets , Sigmas , Taus) must be\ndefined as variables\ninside the script.\n3. ** BALANCED\nCOPONENTS .** All\ncomponent\nscores\nmust be balanced\nand normalized ,\nno single\ncomponent\noverpowers\nothers.\n### INPUT\nFORMAT:\n- state ={’feature1 ’: (value1 , delta1),\n’feature2 ’: (value2 , delta2),\n...\n}\n- Values for the\nfeatures\nare\nnormalized\nwith min max values to the range of 0 to\n1\n- t and \\Delta t are\nintegers in the unit of hours\n- action ={’action1 ’: value1 ,\n’action2 ’: value2 ,\n...\n}\n- Values for the\nactions\nare in levels\nrepresented\nwith\nintegers (e.g. 0, 1, 2, 3\nfrom low to high)\n### OUTPUT\nFORMAT\nProvide a single\nPython\ncode\nblock\nthat\ndefines\nthe\npotential\nfunction\nand the\nreward\nfunction.\n** CODE\nTEMPLATE (You MUST\nfollow\nthis\nstructure):**\n‘‘‘python\nimport\nnumpy as np\nimport\nmath\n# --- 1. PARAMETER\nDEFINITIONS (HARD -CODED\nFROM DATA) ---\n# Design\nyour\ntargets\nhere\nbased on the JSON\nsummary\nSURVIVAL_CONFIG = {\n# ... FILL ALL\nFEATURES\n...\n}\nCONFIDENCE_TAU = {\n# ... FILL ALL\nFEATURES\n...\n}\n# --- 2. MATH\nHELPER\nFUNCTIONS (NO PLACEHOLDERS) ---\ndef\ntime_decay(t):\n# WRITE THE TIME\nDECAY\nFUNCTION\nFOR\nSTRATEGIC\nANNEALING\nreturn ...\ndef\ncompute_survival_score (val , params):\n# WRITE THE ACTUAL\nMATH HERE\nreturn ...\ndef\ncompute_confidence_weight (delta_t , tau):\n# WRITE THE ACTUAL\nEXPONENTIAL\nDECAY\nMATH\nreturn ...\ndef\ncompute_competence_cost (action):\n# WRITE THE DOSE\nPENALTY\nMATH\nreturn ...\n# --- 3. MAIN\nPOTENTIAL\nFUNCTION\n---\n15\n"}, {"page": 16, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\ndef\npotential_function (state , action , t):\n# Iterate\nthrough\nfeatures , calculate\nscores , return\nweighted\nsum\n# base_potential = ...\ndecay_factor = time_decay(t)\nreturn\nbase_potential * decay_factor\n# --- 4. REWARD\nFUNCTION\n---\ndef\nreward_function (s, a, t, s_next , a_next , t_next , gamma =0.99):\nreturn\ngamma * potential_function (s_next , a_next , t_next) - potential_function\n(s, a, t) - compute_competence_cost (a)\n###\nDATASET\nSUMMARY:\n{\n\" total_records \":\n\" total_patients \":\n\" mortality_rate \":\n\" features_analyzed \":\n}\n###\nCRITICAL\nSTATE\nFEATURES:\n1. ** feature1 **\n- Missing:\n- Mean:\n- Std:\n- Range:\n- Median:\n- Q25: Q75:\n- Correlation\nwith\nmortality:\n**Use these\nstatistics\nalong\nwith\nclinical\nknowledge to define\ntarget\nrange **\n** Target\nshould\nreflect\nhomeostasis: values in normal/healthy\nphysiological\nrange\n**\n** CRITICAL\nREQUIREMENTS\nFOR\nEXECUTABLE\nCODE :**\n- Include\nALL\nimports\nneeded: ‘import\nnumpy as np ‘, ‘import\npandas as pd ‘, ‘import\nmath ‘ (if needed)\n- Define ALL helper\nfunctions\nyou use (e.g., distance\nfunctions , penalty\nfunctions\n, cost\nfunctions)\n- Define ALL\nconstants/variables\nused (e.g., ‘FEATURE_WEIGHTS ‘, ‘FEATURE_TARGETS ‘,\n‘TAU ‘, ‘ACTION_COST_SCALE ‘)\n- Code must be directly\nrunnable\nwithout\nany\nundefined\nvariables or functions\n- The reward\nfunction\nMUST call\npotential_function\nand\ncompute\nthe\ndifference\n- ALL imports , helper\nfunctions , and\nconstants\nMUST be defined in your code\n- Code must be directly\nexecutable - no undefined\nvariables or functions\nB\nReward Examples\nB.1\nSample of Reward Function for Sepsis Management\n1\nimport\nnumpy as np\n2\nimport\nmath\n3\n4\n# --- 1. PARAMETER\nDEFINITIONS (HARD -CODED\nFROM DATA) ---\n5\n6\n# Normalization\nassumptions (min -max from raw data):\n7\n# sofa_24hours: 0-23\n8\n# baseexcess:\n-25 to 0\n9\n# lactate: 0.3 to 29\n10\n# urineoutput:\n-3000 to 4400\n11\n# mbp: 20 to 200\n12\n# heartrate: 23 to 212\n13\n14\n# Survival\nconfig\ncombines\nbest from\nchampions:\n15\n# - Use bell\ncurves for \"Goldilocks\" features\nwith\ntargets\nnear\nclinical\nmedians/\nnormals\n16\n# - Use\ndirectional\ndecay for\ndirectional\nfeatures (sofa_24hours , urineoutput)\n17\n# - Use\ndecay_low or decay_high\nstyle\nfrom\nChampion 3 and 4 for\ndirectional\nfeatures\n16\n"}, {"page": 17, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n18\nSURVIVAL_CONFIG = {\n19\n’sofa_24hours ’: {\n20\n’type ’: ’directional_decay ’,\n21\n’direction ’: ’low’,\n# lower\nsofa\nbetter\n22\n’min’: 0.0,\n23\n’max’: 1.0,\n24\n# decay\nsteepness\nchosen so score\n~0.1 at val=1 (max sofa)\n25\n’k’: 2.3\n26\n},\n27\n’baseexcess ’: {\n28\n’type ’: ’bell ’,\n29\n’target ’: normalize_raw (-2.0,\n-25.0, 0.0) ,\n# ~0.92\n30\n’sigma ’: 0.1\n# moderate\nspread\nfrom\nChampion 1 and 4\n31\n},\n32\n’lactate ’: {\n33\n’type ’: ’decay_lower ’,\n# penalize\nvalues\nabove\ntarget\n34\n’target ’: normalize_raw (1.6, 0.3, 29.0) ,\n# ~0.045\n35\n’sigma ’: 0.05\n# for decay\nrate\ncalculation\n36\n},\n37\n’urineoutput ’: {\n38\n’type ’: ’directional_decay ’,\n39\n’direction ’: ’high ’,\n# higher\nurine\noutput\nbetter\n40\n’threshold ’: normalize_raw (40.0 ,\n-3000.0 , 4400.0) ,\n# ~0.414\n41\n’k’: 5.0\n# steep\ndecay\nbelow\nthreshold\n42\n},\n43\n’mbp’: {\n44\n’type ’: ’bell ’,\n45\n’target ’: normalize_raw (75.0 , 20.0 , 200.0) ,\n# ~0.31\n46\n’sigma ’: 0.1\n47\n},\n48\n’heartrate ’: {\n49\n’type ’: ’bell ’,\n50\n’target ’: normalize_raw (85.0 , 23.0 , 212.0) ,\n# ~0.33\n51\n’sigma ’: 0.1\n52\n},\n53\n}\n54\n55\n# Confidence\ndecay\ntaus (hours) from\nChampion 2 (best\nconfidence\nanchor)\n56\n# Uniform tau = 6 hours for all\nfeatures to reflect\nfaster\nconfidence\ndecay\n57\nCONFIDENCE_TAU = {\n58\n’sofa_24hours ’: 6.0,\n59\n’baseexcess ’: 6.0,\n60\n’lactate ’: 6.0,\n61\n’urineoutput ’: 6.0,\n62\n’mbp’: 6.0,\n63\n’heartrate ’: 6.0,\n64\n}\n65\n66\n# Action\npenalty\nparameters\nfrom\nChampion 3 (best\ncompetence\nanchor)\n67\nMAX_DOSE_LEVEL = 4\n68\nACTION_COST_SCALE = 0.25\n# penalty\nscale per action\ncomponent\n69\n70\n# Weights for\ncomponents\nbalanced as in Knee\nPoint\nChampion (equal\nweighting)\n71\nCOMPONENT_WEIGHTS = {\n72\n’survival ’: 1/3,\n73\n’confidence ’: 1/3,\n74\n’competence ’: 1/3,\n75\n}\n76\n77\n# --- 2. MATH\nHELPER\nFUNCTIONS (NO PLACEHOLDERS ) ---\n78\n79\ndef\ntime_decay(t):\n80\n\"\"\"\n81\n␣␣␣␣Strategic␣annealing␣time␣decay␣function.\n82\n␣␣␣␣Exponential␣decay␣with␣half -life␣48␣hours.\n17\n"}, {"page": 18, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n83\n␣␣␣␣\"\"\"\n84\nhalf_life = 48.0\n85\ndecay = 0.5 ** (t / half_life)\n86\nreturn\ndecay\n87\n88\ndef\ncompute_survival_score (val , params):\n89\n\"\"\"\n90\n␣␣␣␣Compute␣survival␣score␣for␣a␣single␣normalized␣feature␣value␣in␣[0 ,1].\n91\n␣␣␣␣Types:\n92\n␣␣␣␣-␣’bell ’:␣Gaussian␣bell␣curve␣centered␣at␣target␣with␣sigma.\n93\n␣␣␣␣-␣’decay_lower ’:␣exponential␣decay␣if␣val␣>␣target.\n94\n␣␣␣␣-␣’directional_decay ’:␣exponential␣decay␣from␣threshold␣or␣zero␣depending␣on␣\ndirection.\n95\n␣␣␣␣Returns␣score␣in␣[0 ,1].\n96\n␣␣␣␣\"\"\"\n97\nif val is None:\n98\n# Missing\nvalue: neutral\nsurvival\nscore 0.5\n99\nreturn 0.5\n100\n101\nftype = params[’type ’]\n102\n103\nif ftype == ’bell ’:\n104\ntarget = params[’target ’]\n105\nsigma = params[’sigma ’]\n106\nif sigma\n<= 0:\n107\nreturn 0.0\n108\ndiff = val - target\n109\nscore = math.exp (-0.5 * (diff / sigma) ** 2)\n110\nreturn\nscore\n111\n112\nelif\nftype == ’decay_lower ’:\n113\n# Penalize\nvalues\nabove\ntarget\nexponentially\n114\ntarget = params[’target ’]\n115\nsigma = params[’sigma ’]\n116\nif val\n<= target:\n117\nreturn 1.0\n118\nelse:\n119\n# decay\nrate so score\n~0.5 at val=target+sigma\n120\ndecay_rate = math.log (2) / sigma\n121\nscore = math.exp(-decay_rate * (val - target))\n122\nreturn\nscore\n123\n124\nelif\nftype == ’directional_decay ’:\n125\ndirection = params.get(’direction ’, None)\n126\nif direction == ’low’:\n127\n# best at 0, decay as val\nincreases\n128\nk = params.get(’k’, 2.3)\n129\nscore = math.exp(-k * val)\n130\nreturn\nscore\n131\nelif\ndirection == ’high ’:\n132\nthreshold = params.get(’threshold ’, 0.5)\n133\nk = params.get(’k’, 5.0)\n134\nif val\n>= threshold:\n135\nreturn 1.0\n136\nelse:\n137\ndiff = (threshold - val) / threshold if threshold > 0 else 1.0\n138\nscore = math.exp(-k * diff)\n139\nreturn\nscore\n140\nelse:\n141\n# Unknown\ndirection , neutral\n142\nreturn 0.5\n143\nelse:\n144\n# Unknown type , neutral\n145\nreturn 0.5\n146\n18\n"}, {"page": 19, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n147\ndef\ncompute_confidence_weight (delta_t , tau):\n148\n\"\"\"\n149\n␣␣␣␣Exponential␣decay␣of␣confidence␣with␣delta_t␣(hours).\n150\n␣␣␣␣Returns␣weight␣in␣(0 ,1].\n151\n␣␣␣␣\"\"\"\n152\nif delta_t is None or delta_t < 0:\n153\ndelta_t = 0\n154\nreturn\nmath.exp(-delta_t / tau)\n155\n156\ndef\ncompute_competence_cost (action):\n157\n\"\"\"\n158\n␣␣␣␣Compute␣competence␣penalty␣from␣actions.\n159\n␣␣␣␣Penalize␣higher␣doses␣linearly␣scaled.\n160\n␣␣␣␣Sum␣penalties␣for␣vaso␣and␣iv␣fluid ,␣capped␣at␣1.\n161\n␣␣␣␣Returns␣penalty␣in␣[0 ,1].\n162\n␣␣␣␣\"\"\"\n163\nvaso_level = action.get(’vaso_5quantile ’, 0)\n164\niv_level = action.get(’iv_fluid_5quantile ’, 0)\n165\nvaso_norm = vaso_level / MAX_DOSE_LEVEL\n166\niv_norm = iv_level / MAX_DOSE_LEVEL\n167\ntotal_penalty = vaso_norm * ACTION_COST_SCALE + iv_norm * ACTION_COST_SCALE\n168\ntotal_penalty = min(total_penalty , 1.0)\n169\nreturn\ntotal_penalty\n170\n171\n# --- 3. MAIN\nPOTENTIAL\nFUNCTION\n---\n172\n173\ndef\npotential_function (state , t):\n174\n\"\"\"\n175\n␣␣␣␣Compute␣potential␣function␣Phi(s,a,t)␣as␣balanced␣weighted␣sum␣of␣survival ,\n176\n␣␣␣␣confidence ,␣and␣competence␣components␣with␣strategic␣time␣decay.\n177\n␣␣␣␣state:␣dict␣of␣feature:␣(value_normalized ,␣delta_t)\n178\n␣␣␣␣action:␣dict␣with␣keys␣[’ vaso_5quantile ’,␣’iv_fluid_5quantile ’]\n179\n␣␣␣␣t:␣absolute␣time␣step␣(int)\n180\n␣␣␣␣\"\"\"\n181\nsurvival_scores = []\n182\nconfidence_weights = []\n183\n184\nfor feat in SURVIVAL_CONFIG.keys ():\n185\nval , delta_t = state.get(feat , (None , None))\n186\nsurv_score = compute_survival_score (val , SURVIVAL_CONFIG [feat ])\n187\ntau = CONFIDENCE_TAU.get(feat , 6.0)\n188\nconf_weight = compute_confidence_weight (delta_t , tau) if delta_t is not\nNone else 0.0\n189\n190\nsurvival_scores .append(surv_score * conf_weight)\n191\nconfidence_weights .append(conf_weight)\n192\n193\n# Aggregate\nsurvival\ncomponent: weighted\naverage\nsurvival\nweighted by\nconfidence\n194\nsum_confidence = sum( confidence_weights )\n195\nif sum_confidence > 0:\n196\nsurvival_component = sum( survival_scores ) / sum_confidence\n197\nelse:\n198\nsurvival_component = 0.5\n# neutral if no confidence\n199\n200\n# Confidence\ncomponent: average\nconfidence\nweight\nnormalized\n[0 ,1]\n201\nif len( confidence_weights ) > 0:\n202\nconfidence_component = sum( confidence_weights ) / len( confidence_weights )\n203\nelse:\n204\nconfidence_component = 0.0\n205\n206\n# Combine\ncomponents\nequally\nweighted (balanced)\n207\nbase_potential = (\n208\nCOMPONENT_WEIGHTS [’survival ’] * survival_component +\n209\nCOMPONENT_WEIGHTS [’confidence ’] * confidence_component +\n19\n"}, {"page": 20, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n210\nCOMPONENT_WEIGHTS [’competence ’] * competence_component\n211\n)\n212\n213\n# Clamp\nbase\npotential to [0,1]\n214\nbase_potential = max (0.0, min (1.0 ,\nbase_potential ))\n215\n216\n# Apply\nstrategic\ntime\ndecay\n217\ndecay_factor = time_decay(t)\n218\n219\nreturn\nbase_potential * decay_factor\n220\n221\n# --- 4. REWARD\nFUNCTION\n---\n222\n223\ndef\nreward_function (s, t, s_next , t_next , a, gamma =0.99):\n224\n\"\"\"\n225\n␣␣␣␣Reward␣is␣difference -based␣with␣discount␣factor:\n226\n␣␣␣␣R(s,t,s’,t ’)␣=␣gamma␣*␣Phi(s’,t ’)␣-␣Phi(s,t)\n227\n␣␣␣␣\"\"\"\n228\nreturn\ngamma * potential_function (s_next , t_next) - potential_function (s, t) -\ncompute_competence_cost (a)\nB.2\nSample of Reward Function for Ventilation Setting\n1\nimport\nnumpy as np\n2\nimport\nmath\n3\n4\n# --- 1. PARAMETER\nDEFINITIONS (HARD -CODED\nFROM DATA) ---\n5\n6\n# Feature\nnames: [’mbp ’, ’sofa_24hours ’, ’lactate ’, ’RASS ’, ’pH ’, ’spo2 ’, ’paco2 ’]\n7\n# All\nfeatures\nnormalized 0-1, so targets\nand sigmas are also\nnormalized\naccordingly.\n8\n9\n# We define\ntargets\nand sigmas for bell\ncurves (\" Goldilocks\" features)\n10\n# and\ndirectional\ndecay\nparameters\nfor\ndirectional\nfeatures.\n11\n12\n# Normal\nphysiological\nranges and\nclinical\nknowledge:\n13\n# mbp (mean\nblood\npressure): normal ~ 65 -105 mmHg\n14\n# sofa_24hours: lower is better (0 best , 24 worst)\n15\n# lactate: normal < 2 mmol/L (lower\nbetter)\n16\n# RASS: target\naround\n-2 (sedation\nlevel)\n17\n# pH: normal\n7.35 -7.45 (center\n~7.4)\n18\n# spo2: higher better , normal > 95%\n19\n# paco2: normal 35-45 mmHg (lower\nbetter)\n20\n21\n# Since\nfeatures\nare\nnormalized 0-1, we must map\nclinical\ntargets to normalized\nscale.\n22\n# We assume min -max\nnormalization\nper\nfeature as per\ndataset\nrange.\n23\n24\n# Raw ranges\nfrom\ndataset (for\nnormalization ):\n25\nRANGES = {\n26\n’mbp’: (1.0, 250.0) ,\n27\n’sofa_24hours ’: (0.0, 23.0) ,\n28\n’lactate ’: (0.2, 30.0) ,\n29\n’RASS ’: (-5.0, 4.0) ,\n30\n’pH’: (6.59 , 7.78) ,\n31\n’spo2 ’: (0.0, 100.0) ,\n32\n’paco2 ’: (16.9 , 157.8) ,\n33\n}\n34\n35\n# Targets and sigmas for bell\ncurve\nfeatures (normalized)\n36\n# We choose\nsigma to cover\nroughly\nthe\ninterquartile\nrange\nnormalized.\n37\n38\n# mbp: target ~ 80 mmHg (normal\n65 -105) , normalized:\n39\nmbp_target = normalize (80, *RANGES[’mbp’])\n20\n"}, {"page": 21, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n40\nmbp_sigma = (normalize (105, *RANGES[’mbp’]) - normalize (65, *RANGES[’mbp’])) / 2\n# half IQR approx\n41\n42\n# sofa_24hours: lower better , target\nnear 0, directional\ndecay\n43\n# lactate: lower better , directional\ndecay\n44\n# RASS: target\n-2, normalized:\n45\nrass_target = normalize (-2, *RANGES[’RASS ’])\n46\nrass_sigma = 0.05\n# tight\naround\n-2\n47\n48\n# pH: target 7.4, normalized:\n49\nph_target = normalize (7.4, *RANGES[’pH’])\n50\nph_sigma = (normalize (7.45 , *RANGES[’pH’]) - normalize (7.35 , *RANGES[’pH’])) / 2\n51\n52\n# spo2: higher better , directional\ndecay\n53\n# paco2: lower better , directional\ndecay\n54\n55\n# Define\nsurvival\nconfig\nwith type and\nparameters:\n56\n# ’bell ’ for\nGoldilocks (bell\ncurve)\n57\n# ’decay_low ’ for\ndirectional\ndecay\nwhere\nlower is better\n58\n# ’decay_high ’ for\ndirectional\ndecay\nwhere\nhigher is better\n59\n60\nSURVIVAL_CONFIG = {\n61\n’mbp’: {\n62\n’type ’: ’bell ’,\n63\n’target ’: mbp_target ,\n64\n’sigma ’: mbp_sigma ,\n65\n’weight ’: 1.0,\n66\n},\n67\n’sofa_24hours ’: {\n68\n’type ’: ’decay_low ’,\n# lower\nbetter\n69\n’tau’: 0.3,\n70\n’weight ’: 1.0,\n71\n},\n72\n’lactate ’: {\n73\n’type ’: ’decay_low ’,\n# lower\nbetter\n74\n’tau’: 0.2,\n75\n’weight ’: 1.0,\n76\n},\n77\n’RASS ’: {\n78\n’type ’: ’bell ’,\n79\n’target ’: rass_target ,\n80\n’sigma ’: rass_sigma ,\n81\n’weight ’: 0.8,\n82\n},\n83\n’pH’: {\n84\n’type ’: ’bell ’,\n85\n’target ’: ph_target ,\n86\n’sigma ’: ph_sigma ,\n87\n’weight ’: 1.0,\n88\n},\n89\n’spo2 ’: {\n90\n’type ’: ’decay_high ’,\n# higher\nbetter\n91\n’tau’: 0.3,\n92\n’weight ’: 1.0,\n93\n},\n94\n’paco2 ’: {\n95\n’type ’: ’decay_low ’,\n# lower\nbetter\n96\n’tau’: 0.25,\n97\n’weight ’: 1.0,\n98\n},\n99\n}\n100\n101\n# Confidence\ndecay\ntaus for\ndelta_t (hours)\n102\n# Trust\ndecays\nexponentially\nwith time gap since\nlast real\nrecord\n103\nCONFIDENCE_TAU = {\n21\n"}, {"page": 22, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n104\n’mbp’: 12,\n105\n’sofa_24hours ’: 24,\n106\n’lactate ’: 8,\n107\n’RASS ’: 6,\n108\n’pH’: 12,\n109\n’spo2 ’: 6,\n110\n’paco2 ’: 8,\n111\n}\n112\n113\n# Action\ncost\nscale: penalty\nper action\nlevel (higher\nlevel = higher\npenalty)\n114\n# Actions: [’PEEP_level ’, ’FiO2_level ’, ’Tidal_level ’]\n115\n# Assume max levels 0-3 (4 levels), scale\npenalty\nlinearly 0 to 1\n116\nACTION_MAX_LEVEL = {\n117\n’PEEP_level ’: 3,\n118\n’FiO2_level ’: 3,\n119\n’Tidal_level ’: 3,\n120\n}\n121\nACTION_COST_WEIGHT = 0.5\n# weight of competence\npenalty in total\npotential\n122\n123\n# --- 2. MATH\nHELPER\nFUNCTIONS (NO PLACEHOLDERS ) ---\n124\n125\ndef\ntime_decay(t):\n126\n\"\"\"\n127\n␣␣␣␣Strategic␣annealing␣decay:␣potential␣decays␣toward␣zero␣as␣episode␣progresses.\n128\n␣␣␣␣Use␣exponential␣decay␣with␣half -life␣of␣48␣hours␣(2␣days).\n129\n␣␣␣␣\"\"\"\n130\nhalf_life = 48.0\n131\ndecay = 0.5 ** (t / half_life)\n132\nreturn\ndecay\n133\n134\ndef\ncompute_survival_score (val , params):\n135\n\"\"\"\n136\n␣␣␣␣Compute␣survival␣score␣for␣one␣feature␣normalized␣val␣in␣[0 ,1].\n137\n␣␣␣␣params:␣dict␣with␣keys␣depending␣on␣type:\n138\n␣␣␣␣␣␣-␣’type ’:␣’bell ’,␣’decay_low ’,␣’decay_high ’\n139\n␣␣␣␣␣␣-␣For␣bell:␣’target ’,␣’sigma ’\n140\n␣␣␣␣␣␣-␣For␣decay:␣’tau’\n141\n␣␣␣␣Returns␣score␣in␣[0 ,1].\n142\n␣␣␣␣\"\"\"\n143\nif params[’type ’] == ’bell ’:\n144\n# Gaussian\nbell\ncurve\ncentered at target\nwith\nsigma\n145\ndiff = val - params[’target ’]\n146\nscore = math.exp (-0.5 * (diff / params[’sigma ’]) **2)\n147\nreturn\nscore\n148\nelif\nparams[’type ’] == ’decay_low ’:\n149\n# Lower\nvalues better , exponential\ndecay\nfrom 0\n150\n# score = exp(-val / tau)\n151\nscore = math.exp(-val / params[’tau’])\n152\nreturn\nscore\n153\nelif\nparams[’type ’] == ’decay_high ’:\n154\n# Higher\nvalues better , exponential\ndecay\nfrom 1\n155\n# score = exp(-(1 - val) / tau)\n156\nscore = math.exp(-(1 - val) / params[’tau’])\n157\nreturn\nscore\n158\nelse:\n159\n# Unknown type , return\nneutral\n0.5\n160\nreturn 0.5\n161\n162\ndef\ncompute_confidence_weight (delta_t , tau):\n163\n\"\"\"\n164\n␣␣␣␣Exponential␣decay␣of␣confidence␣with␣delta_t␣(hours).\n165\n␣␣␣␣\"\"\"\n166\nreturn\nmath.exp(-delta_t / tau)\n167\n168\ndef\ncompute_competence_cost (action):\n22\n"}, {"page": 23, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n169\n\"\"\"\n170\n␣␣␣␣Compute␣penalty␣cost␣for␣actions.\n171\n␣␣␣␣Higher␣dose␣(level)␣=>␣higher␣penalty.\n172\n␣␣␣␣Normalize␣each␣action␣level␣by␣max␣level ,␣sum␣and␣scale.\n173\n␣␣␣␣Return␣cost␣in␣[0 ,1].\n174\n␣␣␣␣\"\"\"\n175\ntotal_cost = 0.0\n176\nn_actions = len(action)\n177\nfor k, v in action.items ():\n178\nmax_level = ACTION_MAX_LEVEL .get(k, 3)\n179\nnorm_level = v / max_level if max_level > 0 else 0\n180\ntotal_cost += norm_level\n181\navg_cost = total_cost / n_actions if n_actions > 0 else 0\n182\n# Scale by weight\n183\nreturn\navg_cost * ACTION_COST_WEIGHT\n184\n185\n# --- 3. MAIN\nPOTENTIAL\nFUNCTION\n---\n186\n187\ndef\npotential_function (state , t):\n188\n\"\"\"\n189\n␣␣␣␣Compute␣potential␣function␣Phi(s,t)␣=␣survival␣*␣confidence\n190\n␣␣␣␣with␣strategic␣time␣decay.\n191\n␣␣␣␣state:␣dict␣of␣feature␣->␣(val ,␣delta_t)\n192\n␣␣␣␣action:␣dict␣of␣action_name␣->␣level\n193\n␣␣␣␣t:␣absolute␣time␣step␣(int)\n194\n␣␣␣␣\"\"\"\n195\nsurvival_scores = []\n196\nconfidence_weights = []\n197\nweights = []\n198\nfor feat , (val , delta_t) in state.items ():\n199\nif feat not in SURVIVAL_CONFIG or feat not in CONFIDENCE_TAU :\n200\ncontinue\n201\nparams = SURVIVAL_CONFIG [feat]\n202\ntau_conf = CONFIDENCE_TAU [feat]\n203\nsurvival = compute_survival_score (val , params)\n204\nconfidence = compute_confidence_weight (delta_t , tau_conf)\n205\nw = params[’weight ’]\n206\nsurvival_scores .append(survival * confidence * w)\n207\nweights.append(w)\n208\n# Normalize\nsurvival\ncomponent by sum\nweights to balance\n209\nif weights:\n210\nsurvival_component = sum( survival_scores ) / sum(weights)\n211\nelse:\n212\nsurvival_component = 0.0\n213\n214\nbase_potential = survival_component\n215\n# Clip\npotential to [0,1] to avoid\nnegative or\n>1 values\n216\nbase_potential = max (0.0, min (1.0 ,\nbase_potential ))\n217\n218\ndecay_factor = time_decay(t)\n219\n220\nreturn\nbase_potential * decay_factor\n221\n222\n# --- 4. REWARD\nFUNCTION\n---\n223\n224\ndef\nreward_function (s, t, s_next , t_next , a, gamma =0.99):\n225\n\"\"\"\n226\n␣␣␣␣Reward␣=␣gamma␣*␣Phi(s’,t ’)␣-␣Phi(s,t)\n227\n␣␣␣␣\"\"\"\n228\nreturn\ngamma * potential_function (s_next , t_next) - potential_function (s, t) -\ncompute_competence_cost (a)\nB.3\nSample of Reward Function for Renal Replacement Therapy\n23\n"}, {"page": 24, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n1\nimport\nnumpy as np\n2\nimport\nmath\n3\n4\n# --- 1. PARAMETER\nDEFINITIONS (HARD -CODED\nFROM DATA) ---\n5\n6\n# Feature\ntargets\nand\nhealthy\nranges (normalized 0-1)\n7\n# We define \"Goldilocks\" features\nwith bell\ncurves\ncentered at median (healthy\ntarget)\n8\n# Directional\nfeatures\nwith\nmonotonic\ndecay\nfrom\nhealthy\nboundary\n9\n10\n# From data\nsummary\nand\nclinical\nknowledge:\n11\n# pH normal\nrange\n~7.35 -7.45 (normalized\napprox\n0.45 -0.55)\n12\n# potassium\nnormal\n~3.5 -5.0 mEq/L (normalized\napprox\n0.25 -0.5)\n13\n# mbp (mean\nblood\npressure) normal\n~65 -105 mmHg (normalized\napprox\n0.3 -0.6)\n14\n# creatinine\nnormal low (kidney\nfunction) ~0.6 -1.2 mg/dL (normalized\napprox\n0.1 -0.2)\n15\n# urineoutput\nnormal > 0.5 ml/kg/hr (normalized\napprox\n>0.04) but here\nmedian\n80/2000\nnormalized\n~0.04\n16\n# sofa_24hours\nlower is better (0 is best), so directional\ndecay\nfrom 0 upwards\n17\n# heartrate\nnormal\n~60 -100 bpm (normalized\napprox\n0.3 -0.5)\n18\n19\n# We use\nnormalized\nvalues 0-1 as input , so we set\ntargets\naccordingly:\n20\n# For bell\ncurve\nfeatures: center=median\nnormalized , sigma\nchosen to cover IQR\nroughly\n21\n# For\ndirectional\nfeatures: exponential\ndecay\nfrom\nhealthy\nboundary\n22\n23\n# Weights to balance\ncomponents (sum to 1)\n24\nSURVIVAL_WEIGHTS = {\n25\n’pH’: 1.0,\n26\n’potassium ’: 1.0,\n27\n’mbp’: 1.0,\n28\n’creatinine ’: 1.0,\n29\n’urineoutput ’: 1.0,\n30\n’sofa_24hours ’: 1.0,\n31\n’heartrate ’: 1.0,\n32\n}\n33\n# Normalize\nweights so sum = 1\n34\ntotal_w = sum( SURVIVAL_WEIGHTS .values ())\n35\nfor k in SURVIVAL_WEIGHTS :\n36\nSURVIVAL_WEIGHTS [k] /= total_w\n37\n38\n# Targets and sigmas for bell\ncurve (Goldilocks) features\n39\n# Using\nmedian and IQR from data summary , normalized 0-1 scale\nassumed\ninput\n40\n# We approximate\nsigma as (Q75 -Q25)/1.35 (approx std dev for normal\ndist)\n41\nSURVIVAL_CONFIG = {\n42\n# bell\ncurve\nfeatures: pH , potassium , mbp , creatinine , heartrate\n43\n’pH’: {\n44\n’type ’: ’bell ’,\n45\n’target ’: 0.52,\n# approx\nnormalized\nmedian\n7.39 in range\n6.72 -7.94\n46\n’sigma ’: 0.03,\n# approx\n(0.55 -0.45) /1.35 ~0.07 , but\ntighter\nfor pH\n47\n},\n48\n’potassium ’: {\n49\n’type ’: ’bell ’,\n50\n’target ’: 0.42,\n# median 4.2 in range\n2.5 -9.03\nnormalized\n~0.42\n51\n’sigma ’: 0.07,\n# (0.46 -0.39) /1.35 ~0.05 ,\nslightly\nrelaxed\n52\n},\n53\n’mbp’: {\n54\n’type ’: ’bell ’,\n55\n’target ’: 0.40,\n# median\n79.67 in range\n20.67 -198\nnormalized\n~0.4\n56\n’sigma ’: 0.07,\n# (0.45 -0.36) /1.35\n~0.07\n57\n},\n58\n’creatinine ’: {\n59\n’type ’: ’bell ’,\n60\n’target ’: 0.21,\n# median\n0.211 in range\n0.024 -1.97\nnormalized\n~0.21\n61\n’sigma ’: 0.05,\n# (0.28 -0.16) /1.35 ~0.09 ,\ntighter\npenalty\nfor kidney\n24\n"}, {"page": 25, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n62\n},\n63\n’heartrate ’: {\n64\n’type ’: ’bell ’,\n65\n’target ’: 0.43,\n# median\n91.13 in range 0 -207\nnormalized\n~0.44\n66\n’sigma ’: 0.1,\n# (0.5 -0.38) /1.35\n~0.09\n67\n},\n68\n# directional\ndecay\nfeatures:\n69\n# urineoutput: higher is better , so decay if below\nthreshold\n70\n’urineoutput ’: {\n71\n’type ’: ’directional ’,\n72\n’threshold ’: 0.04,\n# normalized\nmedian\n~80/1975\n~0.04\n73\n’direction ’: ’above ’,\n# potential\ndecays if below\nthreshold\n74\n’tau’: 0.1,\n75\n},\n76\n# sofa_24hours: lower is better , decay if above\nthreshold\n77\n’sofa_24hours ’: {\n78\n’type ’: ’directional ’,\n79\n’threshold ’: 0.05,\n# normalized\nmedian\n0/14 ~0, so threshold\nsmall\n80\n’direction ’: ’below ’,\n# potential\ndecays if above\nthreshold\n81\n’tau’: 10.0,\n82\n},\n83\n}\n84\n85\n# Confidence\ndecay\ntaus for\ndelta_t (hours)\n86\n# Trust\ndecays\nexponentially\nwith time gap since\nlast real\nrecord\n87\n# Use feature -specific\ntaus\nreflecting\nclinical\nurgency\n88\nCONFIDENCE_TAU = {\n89\n’pH’: 12,\n# pH changes fast , trust\ndecays\nquickly\n90\n’potassium ’: 24,\n# potassium\nchanges\nslower\n91\n’mbp’: 6,\n# blood\npressure\nchanges\nfast\n92\n’creatinine ’: 48,\n# kidney\nfunction\nslower\n93\n’urineoutput ’: 24,\n94\n’sofa_24hours ’: 72,\n95\n’heartrate ’: 6,\n96\n}\n97\n98\n# Competence\npenalty\nscale for action\ndose\nlevels\n99\n# action ’action ’ levels: 0 (no dose) to 3 (high dose)\n100\n# Penalty\nincreases\nwith dose\nlevel\n101\nACTION_COST_SCALE = 0.1\n# max\npenalty\n0.3 for dose =3\n102\n103\n# --- 2. MATH\nHELPER\nFUNCTIONS (NO PLACEHOLDERS ) ---\n104\n105\ndef\ntime_decay(t):\n106\n\"\"\"\n107\n␣␣␣␣Strategic␣annealing␣decay:␣potential␣decays␣toward␣zero␣as␣episode␣progresses.\n108\n␣␣␣␣Use␣exponential␣decay␣with␣half -life␣of␣100␣time␣steps.\n109\n␣␣␣␣\"\"\"\n110\nhalf_life = 100\n111\ndecay = 0.5 ** (t / half_life)\n112\nreturn\ndecay\n113\n114\ndef\ncompute_survival_score (val , params):\n115\n\"\"\"\n116\n␣␣␣␣Compute␣survival␣score␣for␣one␣feature␣value␣normalized␣0-1.\n117\n␣␣␣␣For␣bell␣curve:␣Gaussian␣centered␣at␣target␣with␣sigma.\n118\n␣␣␣␣For␣directional:␣exponential␣decay␣from␣threshold.\n119\n␣␣␣␣Returns␣score␣in␣[0 ,1].\n120\n␣␣␣␣\"\"\"\n121\nif params[’type ’] == ’bell ’:\n122\ntarget = params[’target ’]\n123\nsigma = params[’sigma ’]\n124\n# Gaussian\nbell\ncurve\n125\ndiff = val - target\n126\nscore = math.exp (-0.5 * (diff / sigma) ** 2)\n25\n"}, {"page": 26, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n127\nreturn\nscore\n128\nelif\nparams[’type ’] == ’directional ’:\n129\nthreshold = params[’threshold ’]\n130\ntau = params[’tau’]\n131\ndirection = params[’direction ’]\n132\nif direction == ’above ’:\n133\n# score = 1 if val\n>= threshold , else\ndecays\nexponentially\nbelow\nthreshold\n134\nif val\n>= threshold:\n135\nreturn 1.0\n136\nelse:\n137\nreturn\nmath.exp(-( threshold - val) / tau)\n138\nelif\ndirection == ’below ’:\n139\n# score = 1 if val\n<= threshold , else\ndecays\nexponentially\nabove\nthreshold\n140\nif val\n<= threshold:\n141\nreturn 1.0\n142\nelse:\n143\nreturn\nmath.exp(-(val - threshold) / tau)\n144\nelse:\n145\n# Unknown\ndirection , return 0\n146\nreturn 0.0\n147\nelse:\n148\nreturn 0.0\n149\n150\ndef\ncompute_confidence_weight (delta_t , tau):\n151\n\"\"\"\n152\n␣␣␣␣Exponential␣decay␣of␣confidence␣with␣delta_t␣(hours).\n153\n␣␣␣␣\"\"\"\n154\nreturn\nmath.exp(-delta_t / tau)\n155\n156\ndef\ncompute_competence_cost (action):\n157\n\"\"\"\n158\n␣␣␣␣Penalize␣higher␣dose␣actions.\n159\n␣␣␣␣action␣is␣dict␣with␣key␣’action ’␣and␣integer␣dose␣level␣0-3.\n160\n␣␣␣␣Returns␣penalty␣in␣[0,␣1].\n161\n␣␣␣␣\"\"\"\n162\ndose = action.get(’action ’, 0)\n163\n# Clamp\ndose to 0-3\n164\ndose = max(0, min(3, dose))\n165\npenalty = dose * ACTION_COST_SCALE\n166\n# Normalize\npenalty to max 1 if dose =10 (not\nexpected\nhere)\n167\nreturn min(penalty , 1.0)\n168\n169\n# --- 3. MAIN\nPOTENTIAL\nFUNCTION\n---\n170\n171\ndef\npotential_function (state , t):\n172\n\"\"\"\n173\n␣␣␣␣Compute␣potential␣function␣Phi(s,t)␣as␣weighted␣sum␣of␣survival␣*␣confidence␣-\n␣competence␣cost ,\n174\n␣␣␣␣then␣apply␣strategic␣time␣decay.\n175\n␣␣␣␣\"\"\"\n176\nsurvival_sum = 0.0\n177\nconfidence_sum = 0.0\n178\ntotal_weight = 0.0\n179\n180\nfor feat , weight in SURVIVAL_WEIGHTS .items ():\n181\nif feat not in state:\n182\n# Missing\nfeature , skip\n183\ncontinue\n184\nval , delta_t = state[feat]\n185\nparams = SURVIVAL_CONFIG .get(feat)\n186\nif params is None:\n187\ncontinue\n188\nsurvival_score = compute_survival_score (val , params)\n26\n"}, {"page": 27, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\n189\ntau = CONFIDENCE_TAU.get(feat , 24)\n# default\ntau =24 if missing\n190\nconfidence_weight = compute_confidence_weight (delta_t , tau)\n191\n# Combine\nsurvival\nand\nconfidence\nmultiplicatively\n192\ncomponent_score = survival_score * confidence_weight\n193\nsurvival_sum += component_score * weight\n194\ntotal_weight += weight\n195\n196\n# Normalize\nsurvival\ncomponent (should be close to 1)\n197\nif total_weight > 0:\n198\nsurvival_component = survival_sum / total_weight\n199\nelse:\n200\nsurvival_component = 0.0\n201\n202\n# Apply\nstrategic\ntime\ndecay\n203\ndecay_factor = time_decay(t)\n204\n205\npotential = base_potential * decay_factor\n206\n207\n# Clamp\npotential to [0,1]\n208\npotential = max (0.0, min (1.0, potential))\n209\n210\nreturn\npotential\n211\n212\n# --- 4. REWARD\nFUNCTION\n---\n213\n214\ndef\nreward_function (s, t, s_next , t_next , a, gamma =0.99):\n215\n\"\"\"\n216\n␣␣␣␣Reward␣is␣difference -based␣with␣discount␣factor:\n217\n␣␣␣␣R␣=␣gamma␣*␣Phi(s’,t ’)␣-␣Phi(s,t)\n218\n␣␣␣␣\"\"\"\n219\nphi_next = potential_function (s_next , t_next)\n220\nphi_curr = potential_function (s, t)\n221\nreward = gamma * phi_next - phi_curr - compute_competence_cost (a)\n222\nreturn\nreward\nC\nMathematical form of Drives in Reward Function\nC.1\nSurvival Score S\nThree canonical forms for normalized feature values:\n• Bell Curve (Goldilocks features):Snormal(x) = exp\n\u0010\n−1\n2\n\u0000 x−µ\nσ\n\u00012\u0011\n• Directional Decay: Sdirectional(x) =\n\n\n\n\n\nexp\n\u0010\n−x\nτ\n\u0011\n(lower-better)\nexp\n\u0012\n−1 −x\nτ\n\u0013\n(higher-better)\n• Asymmetric Decay-Lower (thresholded): Sasymetric(x) =\n\n\n\n1\nx ≤µ\nexp\n\u0012\n−ln 2\nσ (x −µ)\n\u0013\nx > µ\nC.2\nConfidence Weight U\nU(∆t; τ) = exp\n\u0000−∆t\nτ\n\u0001\nC.3\nCompetence Cost C\nC(a) = wc\nPN\ni=1\nai\namax,i\n27\n"}, {"page": 28, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nD\nTheoretical Proof\nD.1\nBreaking Policy Invariance\nProposition 1 (Cost Regularization Breaks Invariance and Enforces Efficiency). Let the reward function be defined as\nR(st, at, st+1) = F(st, st+1) −λC(at), where F(st, st+1) = γΦ(st+1) −Φ(st) is a potential-based shaping function\nand C(at) > 0 is a strictly positive action cost.\nWhile the shaping term F forms a telescoping sum that preserves policy invariance (Ng et al., 1999), the inclusion of\nthe non-telescoping cost term −λC(at) renders the cumulative return path-dependent. This modification compels the\noptimal policy to satisfy two objectives: maximizing the time-discounted physiological recovery γT Φ(sT ) (preventing\nprolonged ICU stays) while simultaneously minimizing the cumulative intervention cost.\nProof. Consider a trajectory τ = (s0, a0, s1, . . . , sT ). The discounted cumulative return G(τ) is:\nG(τ) =\nT −1\nX\nt=0\nγt [(γΦ(st+1) −Φ(st)) −λC(at)] .\n(11)\nWe separate the potential-based shaping term from the cost term. The shaping term telescopes as follows:\nT −1\nX\nt=0\nγt(γΦ(st+1) −Φ(st)) =\nT −1\nX\nt=0\n(γt+1Φ(st+1) −γtΦ(st))\n(12)\n= γT Φ(sT ) −Φ(s0).\n(13)\nSubstituting this back into G(τ), we obtain the decomposed objective:\nG(τ) =\n\u0000γT Φ(sT ) −Φ(s0)\n\u0001\n|\n{z\n}\nTelescoped Potential (Time-Dependent)\n−\nλ\nT −1\nX\nt=0\nγtC(at)\n|\n{z\n}\nCumulative Cost (Path-Dependent)\n.\n(14)\nHere, the first term depends only on the boundary states and the trajectory duration T. The factor γT provides a strategic\nincentive to reduce T (avoiding \"holding\" behavior), but remains invariant to the path taken to reach sT at time T.\nHowever, the second term, −λ P γtC(at), strictly does not telescope. It introduces a dependency on the specific\nsequence of actions a0:T −1. Consequently, among policies that achieve the same recovery state Φ(sT ) in the same\ntime T, the objective function breaks invariance to strictly prefer the policy with the minimum cumulative intervention\ncost.\nD.2\nEquivalence to Lagrangian Relaxation of CMDP\nProposition 2 (Equivalence to Lagrangian Relaxation of CMDP). Maximizing the expected cumulative return of\nthe regularized reward function R(s, a, s′) = (γΦ(s′) −Φ(s)) −λC(a) is equivalent to maximizing the Lagrangian\nrelaxation of a Constrained Markov Decision Process (CMDP), where the objective is to maximize physiological gain\nJΦ subject to a constraint on the cumulative intervention cost JC ≤β.\nProof. We define the primal CMDP optimization problem as:\nMaximize: JΦ(π) = Eπ\n\" ∞\nX\nt=0\nγt (γΦ(st+1) −Φ(st))\n#\n(15)\nSubject to: JC(π) = Eπ\n\" ∞\nX\nt=0\nγtC(at)\n#\n≤β.\n(16)\nThe Lagrangian function L(π, λ) with Lagrange multiplier λ ≥0 is constructed as:\nL(π, λ) = JΦ(π) −λ(JC(π) −β).\n(17)\n28\n"}, {"page": 29, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nExpanding the expectations and rearranging terms by the linearity of the summation:\nL(π, λ) = Eπ\n\" ∞\nX\nt=0\nγt (γΦ(st+1) −Φ(st)) −λ\n∞\nX\nt=0\nγtC(at)\n#\n+ λβ\n(18)\n= Eπ\n\n\n∞\nX\nt=0\nγt\n\n\nγΦ(st+1) −Φ(st) −λC(at)\n|\n{z\n}\nR(st,at,st+1)\n\n\n\n\n+ const.\n(19)\nThus, finding the optimal policy π∗that maximizes the Lagrangian L(π, λ) for a fixed λ corresponds exactly to\nmaximizing the value function defined by the composite reward R. The parameter λ serves as the dual variable\ngoverning the trade-off between physiological stabilization and intervention cost.\nE\nHyperparameters and Experimental Details\nWe separate the data into 7:1:1:1 split on patient level as training set for policy, training set for reward, test set for policy\nand test set for reward. All the experiments are run with 5 different seeds.\nE.1\nComputation Resources\nAll experiments were conducted on a high-performance computing cluster node equipped with 8 × NVIDIA H200\nNVL GPUs.\nE.2\nSelection of LLM\nFor the CodeGen baselines, we intentionally selected the most powerful state-of-the-art proprietary models available.\nSince the baseline approach involves a simple, single-pass inference to generate a policy, the computational cost is\nmanageable even with expensive APIs. This ensures our method is compared against the strongest possible \"zero-shot\"\nclinical agents, establishing a rigorous upper bound for performance without regarding training constraints. In contrast,\nour medR framework requires generating dense reward signals across vast state-action spaces, making the use of\ncommercial APIs prohibitively expensive and computationally intractable for large-scale training. Consequently, we\nemploy smaller, open-source models that can be deployed locally. A key finding of our work is that medR, even when\npowered by these smaller, accessible models, consistently outperforms baselines using significantly larger, proprietary\nLLMs. This validates that our improvements stem from the theoretical rigor of our potential-based Lagrangian\nframework rather than raw model scale.\nE.3\nReward Function Hyperparameters\nThe discount factor γ was set to 0.99 for all environments. For feature selection we use κ = 0.6 for the threshold and\nthe number of features selected k = 7. We generate N = 20 functions as reward candidates for selection. For Jsurv\nepsilon is 2. For Jcomp we use k = 10 for H, α = 0.1 for E.\nE.4\nRL Policy Hyperparameters\nSepsis Treatment\nThe Sepsis task utilizes the MIMIC-III dataset with a state dimension of 46 and a discrete action\nspace of 25. The policy network is constructed with 2 hidden layers of 32 units each. Optimization was performed with\na batch size of 512 and a learning rate of 1 × 10−3 over 10 epochs.\nMechanical Ventilation\nFor the Ventilation task, data is sourced from the eICU database. The environment consists\nof a 41-dimensional state space and a discrete action space of size 18. The network architecture uses 3 hidden layers\nwith 32 units per layer. Training used a batch size of 512, a learning rate of 5 × 10−4, and ran for 10 epochs.\nRenal Replacement Therapy\nThe RRT task is based on the AmsterdamUMCdb dataset, featuring a compact state\ndimension of 18 and a binary action space (size 1). The network is deeper but narrower, using 3 hidden layers with 16\nunits each. Due to the smaller dataset size, a smaller batch size of 256 was used, with a learning rate of 1 × 10−3 for 10\nepochs.\n29\n"}, {"page": 30, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nE.5\nFeature Selection\nFor each task, a subset of critical features was identified to inform the LLM-generated reward potential. These features\nare listed below:\n• Sepsis: SOFA score, Base Excess, Lactate, Urine Output, Mean Blood Pressure, Heart Rate.\n• Ventilation: Mean Blood Pressure, SOFA score, Lactate, RASS, pH, SpO2, PaCO2.\n• RRT: pH, Potassium, Mean Blood Pressure, Creatinine, Urine Output, SOFA score, Heart Rate.\nF\nRelated Work\nF.1\nReward Design in Healthcare RL:\nTraditional RL methodologies in healthcare typically define reward functions using clinical indicators, such as changes\nin SOFA scores, expert-derived heuristics, or terminal outcomes, including 90-day mortality. A foundational example\nis the AI Clinician by [5], which employed sparse, outcome-based rewards reflecting 90-day survival. A subsequent\nanalysis[40] introduced safety constraints by limiting actions to those frequently taken by clinicians, further restricting\nexploration. [38] addressed the limitations of sparse signals by incorporating intermediate clinical metrics such as\nSOFA, lactate, and increasing feedback frequency through their heuristic-based approach, which risks encoding clinical\nbias. [7] and [41] advanced this direction by using model-based shaping, where predicted mortality log-odds serve as\nreward signals. While this enhances continuity, it introduces dependence on opaque, learned predictors. [42] employed\nhigh-fidelity simulators to define reward functions, though such strategies are resource-intensive and may lack clinical\ntransparency. Despite these advances, most reward functions rely on proxy measures that lack semantic clarity and deep\nclinical grounding. Reward design thus remains an open challenge, necessitating signals that are temporally dense,\ninterpretable, and ethically aligned.\nF.2\nLLMs in Clinical Settings\nLLMs have transformed natural language tasks in clinical medicine [43, 44, 45], aiding in medical question answering\n[46], policy critique [47], and patient summaries[48]. However, their application in structured reward modelling for\nhealthcare RL remains largely unexplored, presenting a promising opportunity. Recent studies have started exploring\nhow LLMs can enhance RL in healthcare by improving sample efficiency and reward design.\nFor example,[49] uses LLMs for abstract plans in potential-based reward engineering, boosting sample efficiency in toy\nenvironments, but their method relies on handcrafted abstractions and external verifiers.[50] proposes Q-engineering,\nwhich utilises LLM heuristics to shape Q-values, thereby improving efficiency but relying on task-specific heuristics\nand lacking interpretability. [51] shows how LLMs refine intrinsic rewards through code synthesis in rule-based\nenvironments, but their method is constrained by fixed domain rules and static strategies.\nThese methods advance LLM-guided reward design; however, they remain limited by the use of symbolic models,\nheuristics, and predefined knowledge, which reduces their applicability in complex clinical settings. These limitations\nare highlighted by [52], who explore reward models but face reward hacking issues.[53] proposes an RL/LLM\ntaxonomy but doesn’t address real-world clinical challenges, while [54] reviews RLHF but struggles with sparse\nfeedback in healthcare. In contrast, our approach leverages LLMs for temporal credit assignment in clinical settings,\ntransforming sparse outcomes into clinically interpretable rewards from raw patient data, without relying on external\nverifiers or predefined rules, enabling scalable, adaptive learning for dynamic healthcare environments.\nRecent studies have made significant strides in LLM-based reward engineering for RL. [55] use zero-shot LLMs to\nreplace human feedback, improving sample efficiency.[56] introduces PAR, addressing reward hacking in RLHF. [57]\nproposes the LMGT framework to balance exploration and exploitation, while [58] develop SPA-RL for stepwise\nreward attribution. [51] proposes a simple framework using LLMs for intrinsic reward generation.\nDespite these advancements, our approach is unique. Unlike methods that rely on predefined models or biased human\nfeedback, we utilise raw clinical data to generate clinically interpretable rewards, thereby eliminating the need for\nexternal verifiers or predefined rules and offering a scalable, adaptive solution for real-time healthcare applications.\n30\n"}, {"page": 31, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nF.3\nCredit Assignment\nTemporal credit assignment remains a major challenge in RL, for complex, long-horizon tasks. Traditional methods,\nsuch as TD-lambda ([54]) and GAE ([59]), focus on temporal differences but lack semantic reasoning, which limits their\nperformance. Recent advances, such as Universal GAE by [60], improve generalisation. Meanwhile, transformer-based\nTD learning by [61], which integrates attention-based models, addresses some limitations but still falls short in complex,\nreal-world tasks. In addition, some works [62, 63, 64] attempt to analyze the timing of action provision from a causal\nperspective.\nLLMs have emerged as powerful solutions, bringing new levels of contextual understanding and reasoning to RL,\neffectively tackling long-standing challenges in temporal credit assignments. For example, [65] introduces CALM,\nwhere LLMs act as critics to decompose tasks into subgoals and assign rewards in zero-shot settings; [66] presents\nQLLM, replacing complex mixing networks in multi-agent RL with LLM-generated, interpretable credit functions;\nand [67] propose LaRe, using LLMs to derive multi-dimensional latent rewards that capture nuanced contributions in\nepisodic tasks. Collectively, these works establish LLMs as scalable, interpretable, and powerful tools for automating\ncredit assignments in both single- and multi-agent environments.\nOur approach, medR, uniquely advances this line by leveraging real clinical data to integrate Large Language Models\nwith potential-based reward shaping. Unlike CALM’s reliance on toy subgoals [65], QLLM’s multi-agent focus [65], or\nLaRe’s [67] latent outcome alignment, medR transforms sparse terminal outcomes into dense, mathematically grounded\nlearning signals. By prompting LLMs to estimate the physiological stability of patient states (Φ(s)), we construct a\nregularized reward structure that explicitly balances clinical improvement with intervention cost. This formulation\nsolves a Lagrangian-constrained optimization problem, bridging the gap between high-level medical reasoning and\nprecise temporal credit assignment in critical care.\nG\nAnalysis of Tri-Drive Fitness\nTo assess the reward functions before training, we utilize the three proposed fitness objectives: Survival (Jsurv),\nConfidence (Jconf), and Competence (Jcomp). We acknowledge that baseline rewards explicitly designed with sparse\noutcome signals will naturally exhibit high Jsurv scores by definition, as they directly encode the survival target.\nConsequently, a superior reward function should not merely maximize Jsurv but demonstrate a robust balance across all\nthree Pareto dimensions, particularly in Competence (Jcomp) and Confidence (Jconf). We use J in both reward selection\nand evaluation since it is an off-policy evaluation functional of a fixed dataset and a fixed clinician policy, not a training\nobjective. Reward selection and policy training are decoupled in our setting. We treat reward design as a model selection\nproblem, and J as a task-level performance criterion that is external to the learning dynamics.\nFigure 4: Decomposition of Tri-Drive fitness across tasks. The stacked bars represent the contribution of three distinct\nevaluation metrics: Survival (Jsurv), Confidence (Jconf), and Competence (Jcomp).\n31\n"}, {"page": 32, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nH\nFeatures and Actions\nH.1\nSepsis\nThe sepsis cohort utilizes 46 observation features, comprising demographics, vital signs, and a comprehensive set\nof laboratory values, as detailed in Table 4. For the action space, we consider two primary interventions: IV fluids\nand Vasopressors. The dosage for each intervention is discretized into four quantile bins plus a no-treatment option,\nresulting in 5 discrete levels per drug. The joint action space is the Cartesian product of these levels, yielding 5×5 = 25\ndiscrete treatment combinations.\nTable 4: List of features for the Sepsis task.\nCategory\nFeature Name\nDemographics (6)\nAge, Gender, Weight, Readmission, Elixhauser Score, Mechanical Ventilation Status\nVital Signs (8)\nHeart Rate, Respiratory Rate, SpO2, Temperature, SBP, DBP, MBP, Shock Index\nLab Values (30)\nLactate, PaO2, PaCO2, pH, Base Excess, CO2, Hemoglobin, PaO2/FiO2, WBC,\nPlatelet, BUN, Creatinine, PTT, PT, INR, AST, ALT, Bilirubin, Magnesium, Ionized\nCalcium, Calcium, Urine Output, Potassium, Sodium, Chloride, Glucose, Albumin,\nBicarbonate, FiO2\nScores (2)\nSOFA (24hr), SIRS, GCS\nAbbreviations - SBP/DBP/MBP: Systolic/Diastolic/Mean Blood Pressure; GCS: Glasgow Coma Scale; BUN: Blood Urea Nitrogen; WBC: White Blood Cell; PTT: Partial\nThromboplastin Time; PT: Prothrombin Time; INR: International Normalized Ratio; AST: Aspartate Aminotransferase; ALT: Alanine Aminotransferase.\nH.2\nMechanical Ventilation (MV)\nFor the ventilation task, we utilize 41 features selected for training, listed in Table 5. The action space consists of 18\ndiscrete combinations derived from three ventilator settings: Positive End-Expiratory Pressure (PEEP), Fraction of\nInspired Oxygen (FiO2), and Tidal Volume adjusted for ideal body weight. These are categorized into Low, Medium,\nand High levels as summarized in Table 6.\nTable 5: List of features for the Mechanical Ventilation task.\nCategory\nFeature Name\nDemographics (4)\nAge, Gender, Weight, Elixhauser Score\nVital Signs (7)\nHeart Rate, Respiratory Rate, SpO2, Temperature, SBP, DBP, MBP\nLab Values (26)\nLactate, Bicarbonate, PaCO2, pH, Base Excess, Chloride, Potassium, Sodium, Glu-\ncose, Hemoglobin, Ionized Calcium, Calcium, Magnesium, Albumin, BUN, Creati-\nnine, WBC, Platelet, PTT, PT, INR, ETCO2, Urine Output, Rate Std, Analgesic/Seda-\ntive Admin, Neuromuscular Blocker Admin\nScores (4)\nGCS, SOFA (24hr), SIRS (24hr), RASS\nTable 6: Categorization of action levels for PEEP, FiO2 and Tidal Volume in the MV task.\nIntervention\nCategory\nThreshold\nPEEP (cmH2O)\nLow\n≤5\nHigh\n> 5\nFiO2 (%)\nLow\n< 35\nMedium\n35–50\nHigh\n≥50\nTidal Volume (ml/kg IBW)\nLow\n< 6.5\nMedium\n6.5–8\nHigh\n≥8\n32\n"}, {"page": 33, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nH.3\nRenal Replacement Therapy (RRT)\nThe RRT task utilizes a specific subset of 18 features focusing on renal function and hemodynamic stability, as shown\nin Table 7. Unlike the multi-dimensional discrete actions in Sepsis and Ventilation, the RRT action space is defined as a\nsingle dimension representing the aggregate dosage of renal replacement therapy.\nTable 7: List of features for the RRT task.\nCategory\nFeature Name\nDemographics (3)\nAge, Gender, Weight\nVital Signs (5)\nHeart Rate, SBP, MBP, DBP, SpO2\nLab Values (8)\nUrea, Creatinine, pH, Bicarbonate, Potassium, Sodium, PaO2/FiO2, SOFA (24hr)\nUrine Output (2)\nTotal Urine Output (6hr), Urine Output Change Rate (6hr)\nI\nBehavior Policy\nTo approximate the clinician’s decision-making process and facilitate Off-Policy Evaluation (OPE), we developed a\nbehavior policy parameterized by a Multi-Layer Perceptron (MLP). This policy was trained via supervised learning\nto imitate observed clinical actions. Across all tasks, models were trained for 100 epochs, with architectures and\nhyperparameters optimized for each specific domain. For Sepsis, we employed a 3-layer network with 32 hidden units\n(batch size 256, learning rate 5 × 10−4). The Ventilation task utilized a compact 2-layer architecture with 8 hidden\nunits (batch size 128, learning rate 1 × 10−3), while the RRT model consisted of 3 layers with 16 hidden units (batch\nsize 128, learning rate 5 × 10−4).\nJ\nPseudocodes\nAlgorithm 1 Reward Engineering via Tri-Drive Potential Functions\nInput: Clinical Dataset D, LLM M, Candidate Features F\nOutput: Pareto-Optimal Set of Reward Functions R∗\n// Phase 1: Interpretable Feature Selection\nCalculate statistical metadata Mf for all f ∈F (missingness, correlation ρ)\nFcrit ←EnsembleSelect(M(Mf, Promptfeat))\n// Phase 2: Candidate Reward Function Generation\nInitialize Candidate Set C ←∅\nfor i = 1 to N do\nri ←ExtractCode(M(Promptgen, Fcrit))\nAdd ri to C\nend for\n// Phase 3: Multi-Objective Selection\nFor all r ∈C, compute Offline Fitness Vector:\nJ(Φ) = [Jsurv, Jconf, Jcomp]\nFinalize: R∗←ParetoFront(C)\nK\nComparison of Action Distributions between RL and Clinicians\n33\n"}, {"page": 34, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nTable 8: Agreement rates between AI policy and Clinician actions across three clinical tasks. Values represent Mean ±\nStandard Deviation across seeds. The **Joint** column represents the exact match rate across all action dimensions\ncombined. Bold indicates the highest agreement in each category.\nMethod\nSepsis Agreement (%)\nVentilation Agreement (%)\nRRT (%)\nJoint\nIV\nVaso\nJoint\nPEEP\nFiO2\nTidal\nDose\nORM\n23.99 ± 0.40\n29.06 ± 0.23\n75.57 ± 0.89\n21.28 ± 2.20\n72.97 ± 0.36\n51.66 ± 5.10\n47.28 ± 0.40\n54.39 ± 9.23\nPRM\n25.82 ± 0.55\n31.62 ± 0.71\n76.34 ± 0.65\n22.17 ± 2.72\n75.29 ± 1.64\n55.29 ± 5.48\n46.06 ± 0.50\n52.54 ± 11.33\nOPRM\n25.50 ± 0.54\n31.13 ± 0.60\n76.34 ± 0.81\n20.81 ± 2.50\n73.01 ± 0.68\n54.63 ± 2.65\n42.88 ± 1.79\n60.43 ± 3.12\nCodeGen+LLM\n25.72 ± 0.38\n31.80 ± 0.40\n76.29 ± 0.73\n17.43 ± 4.15\n68.40 ± 0.32\n50.00 ± 7.55\n38.18 ± 3.78\n60.14 ± 2.47\nCodeGen+GPT-4\n25.79 ± 0.45\n31.58 ± 0.21\n76.43 ± 0.77\n22.39 ± 1.01\n75.50 ± 3.98\n52.42 ± 2.93\n47.30 ± 3.25\n53.30 ± 10.30\nCodeGen+Qwen\n25.89 ± 0.52\n31.85 ± 0.51\n76.45 ± 0.70\n20.84 ± 4.16\n75.77 ± 1.48\n54.49 ± 4.41\n44.44 ± 2.98\n54.11 ± 9.49\nmedR+GPT-OSS-20B\n25.93 ± 0.52\n32.21 ± 0.47\n76.46 ± 0.79\n21.82 ± 0.48\n71.72 ± 0.05\n51.82 ± 3.10\n48.03 ± 0.55\n54.35 ± 9.18\nmedR+GPT-4\n26.02 ± 0.33\n32.22 ± 0.35\n76.46 ± 0.74\n22.59 ± 0.47\n71.53 ± 0.41\n51.50 ± 3.64\n48.20 ± 0.78\n65.99 ± 2.46\nFigure 5: Sepsis Policy Agreement. Confusion matrices comparing AI vs. Clinician actions for IV Fluids (left) and\nVasopressors (right) across different reward functions.\n34\n"}, {"page": 35, "text": "medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions\nFigure 6: Ventilation Policy Agreement. Comparison of discrete action choices for PEEP, FiO2, and Tidal Volume.\nDarker diagonals indicate higher concordance with clinical standards.\nFigure 7: RRT Dosing Agreement. Analysis of continuous dosing converted to discrete clinical bins (Low, Mid, High).\nThe proposed method shows stronger alignment in the ’Mid’ (Standard) dosing range.\n35\n"}]}