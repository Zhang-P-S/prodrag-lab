{"doc_id": "arxiv:2511.03328", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.03328.pdf", "meta": {"doc_id": "arxiv:2511.03328", "source": "arxiv", "arxiv_id": "2511.03328", "title": "Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks", "authors": ["Jindong Hong", "Tianjie Chen", "Lingjie Luo", "Chuanyang Zheng", "Ting Xu", "Haibao Yu", "Jianing Qiu", "Qianzhong Chen", "Suning Huang", "Yan Xu", "Yong Gui", "Yijun He", "Jiankai Sun"], "published": "2025-11-05T09:47:15Z", "updated": "2025-11-05T09:47:15Z", "summary": "A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of \"reasoning MLLMs\" that offer explicit control over their internal thinking processes (normally referred as the \"thinking mode\") alongside the standard \"non-thinking mode\". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these \"dual-state\" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active \"thinking mode\" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.03328v1", "url_pdf": "https://arxiv.org/pdf/2511.03328.pdf", "meta_path": "data/raw/arxiv/meta/2511.03328.json", "sha256": "52be95fa7f996d70caf2e70cc997e491441c74f78e199675ad7678dbab578401", "status": "ok", "fetched_at": "2026-02-18T02:28:25.251222+00:00"}, "pages": [{"page": 1, "text": "Benchmarking the Thinking Mode of Multimodal Large\nLanguage Models in Clinical Tasks\nJindong Hong1,2\nTianjie Chen1\nLingjie Luo1\nChuanyang Zheng3\nTing Xu3\nHaibao Yu4\nJianing Qiu5\nQianzhong Chen6\nSuning Huang6\nYan Xu7\nYong Gui1\nYijun He1\nJiankai Sun1,†\n1Bytedance\n2Peking University\n3The Chinese University of Hong Kong\n4The University of Hong Kong\n5Mohamed bin Zayed University of Artificial Intelligence\n6Stanford University\n7University of Michigan\n†Corresponding Author\nAbstract\nA recent advancement in Multimodal Large Language Models (MLLMs) research is the emer-\ngence of \"reasoning MLLMs\" that offer explicit control over their internal thinking processes\n(normally referred as the \"thinking mode\") alongside the standard \"non-thinking mode\". This\ncapability allows these models to engage in a step-by-step process of internal deliberation before\ngenerating a final response. With the rapid transition to and adoption of these \"dual-state\"\nMLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs\nimpact model performance and reliability in clinical tasks. This paper evaluates the active\n\"thinking mode\" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for med-\nical applications. We assessed their performance on four visual medical tasks using VQA-RAD\nand ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking\nmode remains marginal compared to the standard non-thinking mode for the majority of the\ntasks. Their performance on complex medical tasks such as open-ended VQA and medical\nimage interpretation remains suboptimal, highlighting the need for domain-specific medical\ndata and more advanced methods for medical knowledge integration.\n1. Introduction\nThinking is widely acknowledged as a cornerstone of human intelligence, enabling structured\nproblem-solving, logical inference, and complex decision making across diverse tasks. In the\ncontext of large language models (LLMs) (Thirunavukarasu et al., 2023; Zheng et al., 2025b,a;\nRen et al., 2025; Xu et al., 2025; Ye et al., 2024; Zheng et al., 2024) and multimodal large language\nmodels (MLLMs) (Qiu et al., 2024b), this refers to the model’s ability to explicitly articulate\nits internal processing steps, verify its logic, and adjust its thought process before arriving\nat a final answer (Sun et al., 2023). Recent MLLMs, such as Gemini 2.5 (Kavukcuoglu, 2025),\nSeed1.5-VL (Guo et al., 2025), provide more flexibility of this ability by direct control of the\n†Corresponding author: Jiankai Sun (jiankai.jksun@bytedance.com)\narXiv:2511.03328v1  [cs.CL]  5 Nov 2025\n"}, {"page": 2, "text": "thinking mode (Zhang et al., 2025b). The user can choose to open or close the thinking mode\nbased on its demand. In addition, Gemini-2.5-flash can manually set the number of tokens used\nfor thinking. Although longer sequences of thinking processes often lead to improved accuracy,\nthey also result in an increase of inference latency and substantial computational overhead, a\nphenomenon often referred to as the \"overthinking problem\" (Sui et al., 2025). Experiments\non MATH500 dataset show that thinking only shows its supremacy on challenging problems\n(Zhang et al., 2025a), while non-thinking mode responds both quickly and accurately on simple\nones. Therefore, it is necessary to investigate the appropriate usage scenarios of the thinking\nmode, particularly in the medical field where task difficulty varies significantly, ranging from\nsimple tasks like discharge summary (Patel and Lam, 2023) to complex scenarios such as AI\nmedical consultant (Fan et al., 2024).\nAlthough MLLMs with thinking mode evolve rapidly, the effectiveness of their thinking\nability in medical tasks has not been fully explored. Firstly, most medical benchmarks focus\non the comparison of different models, while the performance difference of the same model\nwith and without the thinking mode remains unexplored. Secondly, only a limited number of\nbenchmarks have compared the differences within the same model; however, these comparisons\nare restricted to single tasks such as question answering (Nori et al., 2024) and clinical document\nclassification (Mustafa et al., 2025), the influence of task complexity on the advantages of thinking\nmode has not been examined.\nTo address the above challenges, we aim to answer two questions: To what extent does\nthe thinking mode enhance the performance of MLLMs in solving visual-language medical\ntasks, especially for those complex ones? And under which specific medical tasks does thinking\nmode yield the most substantial performance advantages compared with non-thinking mode?\nSpecifically, we devise four medical tasks (close-ended visual question answering (VQA), open-\nended VQA, concept detection, and caption prediction) and conduct tests on two prominent\nMLLMs with thinking and non-thinking mode control (Seed1.5-VL and Gemini-2.5-Flash).\nAfter extensive experiments, our findings can be summarized as: (1) activating the thinking\nmode generally improves performance, but the improvement gain is below expectation and\ntheir advantages only become noticeable when the complexity of medical tasks increases. (2)\nthe consistency of the model output is lower in the thinking mode than in the non-thinking\nmode. (3) overall performance of leading MLLMs on highly complex medical tasks remains\nsuboptimal, highlighting the need for integrating domain-specific medical data and more\nadvanced multimodal learning methods.\n2. Related Works\n2.1. MLLMs in the Medical Field\nMLLMs are rapidly catalyzing transformative changes across various facets of healthcare by\neffectively integrating and processing diverse medical data types, including textual patient\nrecords, medical images, audio recordings of consultations, and even video data from surgical\nprocedures (Qiu et al., 2023; Hong et al., 2025; Qiu et al., 2025). In particular, MLLMs are\nbeing developed to generate accurate and comprehensive medical reports from various imaging\nmodalities, such as X-ray (Thawkar et al., 2023), CT (Zhang et al., 2025c), MRI (Lei et al., 2024)\nand 3D scan images (Li et al., 2025). By pre-training or fine-tuning on the medical data, MLLMs\ncan be used as chatbots to answer visual medical questions or make diagnosis from multimodal\ndata. Foundation models like HuatuoGPT-Vision (Chen et al., 2024) and LLaVA-Med (Li et al.,\n2023) can perform VQA tasks on various medical images. Fine-tuned MLLMs such as SkinGPT-\n2\n"}, {"page": 3, "text": "4 (Zhou et al., 2023), can make diagnosis based on the skin image uploaded by the user in\nan interactive way. Due to its strong feature extraction ability, MLLMs can also be trained\nto complete traditional supervised tasks. For example, by pre-training on text and image\nmodalities, Med-MLLM (Liu et al., 2023) can be used for COVID-19 diagnosis and prognosis\ntasks. MLLMs are being proposed as powerful tools to assist or even augment expert work in\ndigital surgery (Lam and Qiu, 2024). Models like SurgicalGPT (Seenivasan et al., 2023), trained\non surgical video data, can answer real-time questions in operating room settings, generate\ndetailed surgical reports, and provide crucial decision support for subsequent procedural steps.\nMLLMs has also been used to assist dietary assessment ranging from food recognition, volume\nestimation, to nutrition quantification and recommendation (Lo et al., 2024)\n2.2. Reasoning MLLMs and Thinking Mode Control\nRecent reasoning MLLMs, such as OpenAI-o3 (OpenAI, 2025), Gemini2.5 and Seed1.5-VL,\nall provide the powerful thinking ability to generate chain of thought before responding. To\nacquire this ability, these models normally need large-scale reinforcement learning and carefully-\ndesigned training recipe. While longer sequences of thinking process often lead to improved\naccuracy, they also result in an increase in inference latency and substantial computational over-\nhead, a phenomenon often referred to as the \"overthinking problem\". Ma et al. (Ma et al., 2025)\ndemonstrates that when controlling the number of tokens or latency, Nothinking (deliberately\nskipping the thinking process of reasoning models by prompt) is more effective compared with\nthinking. Zhang et al. (Zhang et al., 2025a) conducted experiments on the MATH500 dataset\nshowing that thinking only shows its supremacy on challenging problems, while non-thinking\nmode responses both quickly and accurately on simple ones. Some multimodal foundation\nmodels, such as Gemini 2.5, Qwen3 (Yang et al., 2025) and Seed1.5-VL, provide the controlla-\nbility of thinking mode, which means users can dynamically control if using thinking mode.\nFurthermore, Gemini 2.5 and Qwen3 provide more freedom on how much token can be used\nfor thinking (also known as \"thinking budget\"). This gives more space for users to dynamically\nchoose the appropriate mode to balance speed, cost and performance.\n3. Experiments\n3.1. Dataset Preparation\nIn our study, we utilize two publicly available datasets to evaluate the performance of dual-state\nMLLMs: VQA-RAD for visual question answering and ROCOv2 for image captioning and\nconcept detection.\n3.1.1. VQA-RAD\nThe VQA-RAD (Lau et al., 2018) (Visual Question Answering in Radiology) dataset is a clinically\nfocused collection of medical images paired with question-answer sets, designed to assess\nmodels’ abilities to interpret radiological images in response to clinically relevant questions. It\ncomprises 315 radiology images, including modalities such as Computed Tomography (CT),\nMagnetic Resonance Imaging (MRI), and X-ray, evenly distributed across three anatomical\nregions: head, chest, and abdomen. These images are associated with 3,515 question-answer\n(QA) pairs, averaging approximately 10 QA pairs per image. The questions are categorized\ninto 11 distinct types, reflecting clinically relevant tasks: abnormality, attribute, modality, organ\nsystem, color, counting, object/condition presence, size, plane, positional reasoning, and a\n3\n"}, {"page": 4, "text": "general “other” category. This categorization provides insight into the types of queries clinicians\ncommonly pose.\nIn this work, we leverage the entire VQA-RAD dataset to evaluate MLLM performance on\ntwo distinct visual question answering tasks:\n• Closed-ended VQA: This task involves questions with a limited set of possible answers,\nspecifically “yes/no” questions in VQA-RAD. The model’s objective is to select the correct\nanswer based on the visual information in the corresponding medical image.\n• Open-ended VQA: This task requires the model to generate concise free-text answers to\nquestions about a given medical image, demanding deeper image understanding and\nnatural language generation capabilities, as answers are not restricted to predefined\noptions.\nFor closed-ended VQA, we primarily report accuracy, defined as the proportion of correctly\npredicted answers relative to the total number of questions. To ensure stable results, we conduct\nthree runs for each closed-form VQA task and report consistency as a reference metric, measuring\nthe proportion of identical outputs across the three runs. For open-ended VQA, where answers\nare free-form text, we report traditional NLP metrics such as BLEU and ROUGE-L. Given the\nvariability in model outputs, semantic correctness is prioritized. We employ an LLM judge to\nevaluate whether the model’s output matches the ground-truth answer, returning a True or False\njudgment for each question. Consequently, the score of accuracy is also reported for open-form\nVQA.\n3.1.2. ROCOv2\nThe ROCOv2 (Rückert et al., 2024) (Radiology Objects in Context version 2) dataset is a compre-\nhensive multimodal dataset for medical image analysis, representing an enhanced and expanded\nversion of the original ROCO dataset. It contains 79,789 radiological images across various\nmodalities, including CT scans, X-rays, ultrasounds, and MRIs, paired with associated medical\nconcepts (primarily UMLS CUIs) and their original textual captions. In this study, we use the\nROCOv2 dataset to assess MLLM performance in generating medical image captions. Due to\nrate limitations, we evaluate the first 2,000 images from the test set, which maintain a similar\ndistribution to the full test set. We designed two tasks to reflect varying levels of complexity:\n• Concept Detection: This task requires the model to predict relevant medical concepts,\nsuch as UMLS CUIs, imaging modalities, or anatomical regions, directly from the visual\ninformation in the radiological images. It focuses on identifying key concepts without\nrequiring semantic connections between them.\n• Caption Prediction: This task demands that the model fully interprets the image and cap-\ntures the semantic relationships among multiple medical concepts to generate a coherent\ncaption.\nFor concept detection, we employ another LLM to count the number of CUIs recalled in the\nmodel’s output. We report two metrics: 1) Simple Accuracy: The proportion of model outputs\nwith the correct number of recalled CUIs; 2) Total Concept Recall: The ratio of CUIs recalled by\nthe model to the total CUIs in the ground truth.\nFor caption prediction, we report traditional NLP metrics such as ROUGE-L and BLEU.\nAdditionally, we use an LLM judge to assess caption quality, rating model outputs on a scale\n4\n"}, {"page": 5, "text": "from 0 to 3 (0 indicates no semantic correlation with the reference caption, and 3 indicates\nhigh semantic correlation or equivalent meaning). The reliability of the LLM judge is validated\nthrough human evaluation by three medical imaging professionals, with 70% of the LLM judge’s\nscores aligning with human assessments.\nTable 1 | Overview of VQA-RAD and ROCOv2 datasets\nDataset\nSize\nImage Modality\nTasks\nEvaluation Metric\nVQA-RAD\n• 1,192 close-\nform VQA\n• 1,053 open-\nform VQA\n• 104\nhead\naxial\nsingle-slice CTs or\nMRIs\n• 107 chest X-rays\n• 104 abdominal ax-\nial CTs\n• Close-ended\nVQA:\nyes/no\nquestions\n• Open-ended\nVQA: free-text\nanswers\n• Close-ended: Ac-\ncuracy\n• Open-ended:\nLLM judge accu-\nracy\nROCOv2\n2,000 images\n• 746 CTs\n• 515 X-rays\n• 326 ultrasounds\n• 306 MRIs\n• 82 angiograms\n• 25 other modali-\nties (mainly PET\nand PET-CT)\n• Concept detec-\ntion\n• Caption predic-\ntion\n• Concept detection:\nAccuracy,\ntotal\nconcept recall\n• Caption\npredic-\ntion: LLM judge\nscore\n3.2. Experimental Setup\n3.2.1. MLLM Setup\nWe conducted our experiments using two state-of-the-art MLLMs with direct control over\nthinking capabilities: Seed1.5-VL and Gemini-2.5-Flash. Other models with thinking ability\ncontrol were excluded from this study. Specifically, the Qwen3 series does not currently support\nimage inputs, and Gemini-2.5-Pro lacks the option to disable its thinking ability. At the time of\nour experiments, the latest versions used were Doubao-1.5-Thinking-Vision-Pro-0428 for Seed1.5-\nVL and Gemini-2.5-Flash-Preview-04-17 for Gemini-2.5-Flash. We deploy our experiments via\nAPI provided by Volcano Engine and Azure. To optimize reasoning performance, we imposed\nno restrictions on the number of reasoning tokens used in thinking mode.\n3.2.2. System Prompt\nOur system prompt setting for each task is explicit and provide few example outputs to control\noutput style.\n• Close-ended VQA\n5\n"}, {"page": 6, "text": "You are a medical doctor and expert in medical imaging.\nPlease read the uploaded image and answer the question.\nNote:\n1. We expect the answer to only use \"yes\" or \"no\", no other words\n2. Respond only in English.\nQuestion:\n• Open-ended VQA\nYou are a medical doctor and expert in medical imaging.\nPlease read the uploaded image and answer the question.\nNote:\n1. We expect the response to be short and concise\n2. Respone only in English.\n3. Be careful about the potential flips on the image, especially left-right flips.\nExample 1:\nQuestion: what type of imaging is this\nAnswer: CT\nExample 2:\nQuestion: where are the brain lesions located\nAnswer: left hemisphere\nExample 3:\nQuestion: what is the primary abnormality in this image\nAnswer: ring enhancing lesion in the right occipital lobe\nQuestion:\n• Caption Prediction & Concept Detection\n6\n"}, {"page": 7, "text": "You are a medical doctor and expert in medical imaging.\nPlease read the uploaded image and generate the corresponding caption.\nNote:\n1. We expect the responsed to be short and concice.\n2. Your answer should be precise and free of incomplete or incorrect biomedical or\nclinical information.\nExample Captions:\n1. Anteroposterior pelvic radiograph of a 30-year-old female diagnosed with Ehlers-\nDanlos Syndrome demonstrating fusion of pubic symphysis and both sacroiliac\njoints (anterior plating, bone grafting and sacroiliac screw insertion)\n2. CT scan image for lung cancer.\n3. A giant retroperitoneal tumor.\n4. Early axial T2-weighted MRI.\n5. Neck and head computed tomography image showing left odontogenic infection.\n3.3. Main Results\nFigure 1a shows the performance comparison of different models across visual medical tasks.\nFigure 1b shows the relative increase in thinking mode performance compared to non-thinking\nmode. In summary:\nModel performance decreases progressively as task complexity increases. Both models\nshow a significant decreasing trend in model performance as task complexity increases, with\nscores decreasing progressively from Close-ended VQA to Open-ended VQA, Concept Detection,\nand Caption Prediction.\nThe advantage of thinking over non-thinking only becomes noticeable when the task\nis more complex For two VQA tasks, thinking mode shows limited advantages, even a 1.28%\nrelative decline for Gemini-2.5-Flash on open-ended VQA. For two image caption tasks, thinking\nmode shows slightly clear advantages.\nPerformance of both models on complex medical tasks remains suboptimal. Except for\nthe Close-ended VQA, the average score of other tasks is close or lower than 50%, suggesting a\npotential need for domain-specific training or fine-tuning to enhance accuracy. Although the\nthinking ability improves model performance, the incremental gains are insufficient to meet\nusability standards for highly complex tasks.\n3.4. Fine-grained Analysis of Thinking Mode\n3.4.1. Does VQA performance vary among different question types?\nAs Table 2 shows, for the VQA-RAD dataset, all questions fall into 11 categories defined by\nclinicians, reflecting the corresponding tasks naturally occurring about radiology images.\n7\n"}, {"page": 8, "text": "Seed1.5-VL-T\nSeed1.5-VL\nGemini-2.5-Flash-T\nGemini-2.5-Flash\n0\n10\n20\n30\n40\n50\n60\n70\nScore\n76.0\n75.9\n72.9\n72.1\n51.6\n51.5\n46.1\n46.7\n47.4\n46.3\n47.6\n46.6\n37.8\n36.5\n37.1\n34.7\nClose-ended VQA\nOpen-ended VQA\nConcept Detection\nCaption Prediction\n(a) Model performance across different tasks (T: thinking mode).\nClose-ended VQA\nOpen-ended VQA\nConcept Detection\nCaption Prediction\n-2%\n0%\n2%\n4%\n6%\n8%\nRelative Performance Gain (%)\n0.13%\n1.11%\n0.19%\n-1.28%\n2.38%\n2.15%\n3.56%\n6.92%\nSeed1.5-VL\nGemini-2.5-Flash\n(b) Thinking mode performance gain.\nFigure 1 | (a) Model performance across tasks; (b) Thinking mode relative performance gain.\nFor close-ended VQA, the main question type are object/condition presence, accounting\nfor 52.18% of questions as shown in Table 3. For all 11 question types, the thinking mode of\nSeed1.5-VL leads 5 times, and that of Gemini-2.5-Flash leads 8 times as shown in Figure 2a .\nNotably, all model achieve high accuracy on \"plane\" and \"modality\", which are also the question\ntypes thinking mode exceed non-thinking mode.\nFor open-ended VQA, the main question types are position, object/condition presence,\n8\n"}, {"page": 9, "text": "accounting for 28.77% and 18.14% of questions as shown in Table 4. For all 11 question types,\nboth the thinking mode of Seed1.5-VL and Gemini-2.5-Flash leads 4 times as shown in Figure\n2b. Notably, except \"organ\", thinking mode shows no consistent supremacy on most categories.\npres\nsize\nabnormality\nmodality\nplane\nother\nattrib\ncolor\npos\norgan\ncount\n0\n20\n40\n60\n80\n100\nAccuracy\n77\n66\n76\n88\n93\n72\n76\n67\n84\n80\n50\n77\n64\n75\n86\n90\n72\n76\n67\n86\n82\n60\n74\n68\n70\n75\n92\n72\n67\n53\n75\n86\n57\n74\n65\n72\n74\n90\n70\n68\n46\n79\n80\n43\nSeed1.5-VL-T\nSeed1.5-VL\nGemini-2.5-Flash-T\nGemini-2.5-Flash\n(a) Close-ended VQA: Model performance across question types\npos\npres\nother\nmodality\nabnormality\nplane\nattrib\norgan\ncolor\nsize\ncount\n0\n20\n40\n60\n80\n100\nScore\n43\n46\n46\n83\n27\n87\n35\n74\n56\n42\n50\n45\n50\n38\n80\n32\n88\n36\n71\n65\n21\n71\n36\n40\n38\n87\n21\n84\n36\n81\n70\n5\n43\n35\n41\n39\n89\n22\n81\n44\n79\n61\n5\n50\nSeed1.5-VL-T\nSeed1.5-VL\nGemini-2.5-Flash-T\nGemini-2.5-Flash\n(b) Open-ended VQA: Model performance across question types\nFigure 2 | (a) Close-ended VQA; (b) Open-ended VQA performance across question types.\n3.4.2. Does the model performance on ROCOv2 vary across different image modalities?\nTable 5 shows the distribution of modalities for our ROCOv2 evaluation dataset. CT, X-Ray,\nultrasound and MRI are 4 main modalities, accounting for 94.65% of images. The remaining are\nrare modalities like angiogram and pet-ct.\nThe models demonstrated varying performance across imaging modalities, with the highest\n9\n"}, {"page": 10, "text": "average scores achieved for X-ray, followed by ultrasound, while MRI and CT showed compa-\nrable performance (X-ray > ultrasound > MRI ≈CT) as shown in Figure 3a. Relatively lower\nperformance was observed for angiography and other rare modalities (e.g., PET-CT), likely\nreflecting the limited representation of these imaging modalties in the training dataset, which\nmay have resulted in insufficient learning of their distinctive features.\nThe performance gain of the thinking mode over non-thinking mode also seems to be\nmodality-dependent. As shown in Figure 3b, for the caption prediction task, the thinking\nmode shows advantages over the non-thinking mode in only CT, X-ray, Ultrasound, and MRI\nmodalities.\nct\nx-ray\nultrasound\nmri\nangiogram\nothers\n0\n10\n20\n30\n40\n50\nTotal Concept Recall\n42.8\n56.4\n47.9\n41.3\n38.3\n34.9\n41.9\n54.0\n47.4\n41.5\n38.7\n36.5\n44.4\n53.8\n46.0\n44.7\n42.4\n41.3\n44.1\n51.0\n47.6\n43.5\n42.0\n44.4\nSeed1.5-VL-T\nSeed1.5-VL\nGemini-2.5-Flash-T\nGemini-2.5-Flash\n(a) Concept Detection Across Modalities\nct\nx-ray\nultrasound\nmri\nangiogram\nothers\n0\n5\n10\n15\n20\n25\n30\n35\n40\nLLM Judge Score\n36.8\n40.0\n38.5\n37.5\n31.3\n37.5\n35.5\n38.3\n37.1\n36.1\n34.6\n33.3\n36.1\n40.0\n36.1\n36.6\n36.6\n29.2\n34.2\n37.1\n33.7\n34.1\n30.9\n31.9\nSeed1.5-VL-T\nSeed1.5-VL\nGemini-2.5-Flash-T\nGemini-2.5-Flash\n(b) Caption Prediction Across Modalities\nFigure 3 | (a) Concept detection; (b) Caption prediction across modalities.\n10\n"}, {"page": 11, "text": "pres\nsize\nabnormality\nmodality\nplane\nother\nattrib\ncolor\npos\norgan\ncount\n0\n20\n40\n60\n80\n100\nConsistency (%)\n92\n86\n91\n100\n91\n90\n90\n90\n100\n94\n100\n93\n89\n87\n97\n96\n98\n95\n84\n95\n100\n100\n78\n78\n77\n76\n98\n71\n73\n71\n79\n94\n80\n88\n80\n88\n90\n91\n90\n83\n87\n100\n94\n80\nSeed1.5-VL-T\nSeed1.5-VL\nGemini-2.5-Flash-T\nGemini-2.5-Flash\nFigure 4 | Consistency across Problem Categories\n3.4.3. How thinking mode affects model output consistency?\nBeyond performance metrics, model output consistency is another important metric, which\nmeasures if LLMs can produce the same or similar outputs when supplied with inputs that\nare semantically equivalent. In the medical domain, high consistency ensures the reliability of\nmodel and minimize the risk of incorrect diagnoses or treatment recommendations. We measure\nthe consistency by checking whether MLLMs output the same results on 3 rounds of close-ended\nVQA tasks. To ensure fair comparison, the default model output temperature is set same as\n0.8 for each model. Our findings, summarized in Table 6, indicate that for both Gemini-2.5-\nFlash and Seed1.5-VL, thinking mode resulted in lower model output consistency compared\nto the non-thinking mode. This suggests that the thinking processes may introduce more\nrandomness. Specifically, Gemini-2.5-Flash exhibited a more substantial decrease in consistency,\nwith an 8.8% drop when the thinking mode was activated. In contrast, Seed1.5-VL demonstrated\ngreater robustness in this regard, showing only a 0.84% drop in consistency. Figure 4 shows the\nconsistency across different problem categories, Gemini-2.5-Flash shows decrease of consistency\nin almost each category except \"plane\", whereas Seed1.5-VL demonstrates random advantages\nof thinking and non-thinking modes over each other.\n3.4.4. How does thinking tokens change as task complexity varies?\nConsidering the average thinking tokens for each task, we can see a clear trend that model tends\nto think more when task complexity increases. From Close-ended VQA to Open-ended VQA,\nboth Seed1.5-VL-T and Gemini-2.5-Flash-T make more effort on thinking. While in caption\ngeneration task, Seed1.5-VL and Gemini-2.5-flash show totally different trends, Seed1.5-VL-T\ntreated it as a simple task and spent few tokens on thinking, while Gemini-2.5-Flash-T continues\nto expand thinking effort as shown in Table 7. We further conducted an experiment by modifying\nthe prompt templates and forced Seed1.5-VL-T to \"think carefully before you generate caption,\ndo not ignore details of images\". The average thinking tokens remain 141.6, which may reflect\n11\n"}, {"page": 12, "text": "the difference between models’ innate understanding of task complexity.\n3.4.5. Model Speed VS. Performance\nBesides performance improvement, speed is another factor that affects usability. We define the\nlatency of LLM output as: from when a prompt is originally entered by the user to when they\nreceive the completed output from the model, measured by second. Table 8 shows the latency\nand performance score for all tasks. For open-ended VQA, the thinking mode has a long delay\nand brings very little improvement or even a decline in performance.\n4. Discussion\nThis work has several limitations. The first is limited dataset coverage. The evaluation is\nbased solely on radiology image datasets, which restricts the generalization of our findings.\nWhile radiology is a significant domain in medical imaging, it does not encompass the full\nspectrum of medical image modalities (e.g., pathology slides, dermatology images). As such,\nour conclusions may not be fully applicable to other medical imaging fields. The second is\npotential data leakage. Due to the absence of transparent training data documentation for\nSeed1.5-VL and Gemini-2.5-Flash, it is difficult to ascertain whether the evaluation datasets\nwere part of the models’ pretraining data. This lack of clarity introduces the risk of data leakage,\nwhich could inadvertently inflate the performance results of some models. The third is reliance\non LLM-based evaluation. While we validated the reliability of LLM-based judge against a\nsubset of expert-annotated ground truths, full-scale human evaluation remains limited. Human\nexpert judgment is still essential, especially for nuanced medical tasks, and future work should\nincorporate broader clinical assessments to corroborate automated evaluation metrics. The\nlimited accuracy across multiple tasks of the tested \"dual-state\" MLLMs may stem from their\ninternal capabilities, e.g., lacking proper medical knowledge which can be strengthened through\nfine-tuning or through agentic AI approaches that augment models with external knowledge Qiu\net al. (2024a). More importantly, this work suggests that medical reasoning may not be a fix for\nall tasks. It is critical to understand the complexity of the task before implementing MLLMs as\nthe solution.\n5. Conclusion\nIn this paper, we evaluated the active \"thinking mode\" capabilities of two leading dual-state\nMLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. Our experiments show\nthat while thinking mode is a critical factor in enhancing MLLM performance, its advantages\nover non-thinking mode on various medical tasks are below the expectation. A significant gap\npersists: current general MLLMs, even with activated thinking modes, continue to perform\ninadequately in highly complex visual medical scenarios such as medical image interpretation.\nThis indicates that while thinking mode offers an improvement, it is not a complete solution for\nachieving clinical usability with current general MLLMs.\n6. Supplementary Material\n6.1. Details of Data Preprocessing\nVQA-RAD\nFor VQA-RAD dataset, we use the whole dataset for evaluation. The original\ndataset has a total of 2248 questions. After dropping 3 duplicate questions, there remain 2245\n12\n"}, {"page": 13, "text": "questions and 314 corresponding images. For question with answer \"yes\" and \"no\", we define it\nas close-form vqa. For remaining ones with no fixed answer, we define it as open-form vqa.\nFor very few questions that have multiple categories, we classify each of them into the most\nappropriate category for later analysis\n• ’pos, pres’: which side is more clearly visualized –> ’pos’\n• ’pos, abnormality’: ’which lobes demonstrate pathology’ –> ’pos’\n• ’size, color’: what is the size and density of the lesion –> ’size’\n• ’pres, pos’: what organ is enlarged –> ’pres’\n• ’abnormality, pos’: what pathology is seen in this image what side –> ’abnormality’\nROCOv2\nFor ROCOv2, the original images are in png format, which may exceed the maximum\nfile size of model input, we convert them into jpeg to meet the requirement\n6.2. Details of Experiment Setup\n6.2.1. LLM Setup\nFor both Seed1.5-VL and Gemini-2.5-flash, we can explicitly control their thinking ability by\nchanging certain parameters. In API of Gemini-2.5-flash, The \"thinkngBudget\" parameter can\ncontrol the behavior of thinking, setting the thinking budget to 0 will disable thinking. In API\nof Seed1.5-VL, the \"thinking.type\" parameter can control if thinking ability is enabled, when\nsetting to \"disabled\", the model will disable thinking ability. Since all tasks require relative few\ntokens for output, to balance thinking ability and speed, we set maximum output tokens to be\n4000 for all tasks, which is adequate for most of scenarios. In very few times, thinking model\nmay fail with this restriction, then we re-run the model until it can generate full output.\nWe limit the maximum output token to be 4000 in all scenarios.\n6.2.2. LLM Judge Setup\n13\n"}, {"page": 14, "text": "1. Prompt for open-form question judge.\n14\n"}, {"page": 15, "text": "Prompt for open-form question judge\nYou are an expert in Natural Language Understanding, specializing in comparing the\nsemantic meaning of texts, particularly in question-answering contexts.\nYour task is to determine if the ‘Model Output‘ has substantially the same se-\nmantic meaning as the ‘Answer‘, given the ‘Question‘.\n**Evaluation Criteria:**\nConsider the following when making your judgment:\n1. **Core Information:** Does the ‘Model Output‘ convey the same essential information\nas the ‘Answer‘ in relation to the ‘Question‘?\n2. **Accuracy:** Is the ‘Model Output‘ factually correct with respect to the ‘Answer‘?\n3. **Completeness:**\n* If ‘Model Output‘ is a generalization of ‘Answer‘ (e.g., ‘Answer‘: \"chest x-ray\", ‘Model\nOutput‘: \"X-ray\"), it can be considered to have the same meaning if the generalization\nstill correctly and adequately answers the ‘Question‘.\n* If ‘Model Output‘ is more specific but still accurately represents the ‘Answer‘’s core\nconcept (e.g., ‘Answer‘: \"cystic lesions\", ‘Model Output‘: \"multiple cysts\"), it can have the\nsame meaning.\n* However, if ‘Model Output‘ omits critical details from the ‘Answer‘ that are necessary\nfor a complete and accurate response to the ‘Question‘, it does not have the same\nmeaning.\n4. **Synonyms and Paraphrasing:** Equivalent terms or rephrased statements that retain\nthe original meaning are considered the same.\n5. **Contradiction/Different Concepts:** If the ‘Model Output‘ states something different\nor contradictory to the ‘Answer‘, it does not have the same meaning.\n**Output Format:**\nReturn your evaluation as a boolean (‘True‘ if they have the same semantic meaning,\n‘False‘ otherwise) and a brief ‘Reason‘.\nFormat your response exactly as follows:\nEvaluation: [True/False]\nReason: [Your brief explanation]\n**Examples:**\n**Example 1:**\n‘Question:‘ what type of image is this\n‘Answer:‘ chest x-ray\n‘Model Output:‘ X-ray\nEvaluation: True\nReason: \"X-ray\" is a correct and acceptable generalization of \"chest x-ray\" in the context\nof identifying the image type.\n15\n"}, {"page": 16, "text": "Prompt for open-form question judge (Cont’)\n**Example 2:**\n‘Question:‘ which organ has the abnormality\n‘Answer:‘ pancreas\n‘Model Output:‘ stomach\nEvaluation: False\nReason: \"Stomach\" is a different organ than \"pancreas,\" making the model output\nfactually incorrect in relation to the answer.\n**Example 3:**\n‘Question:‘ what is the brightness in the abdominal aorta\n‘Answer:‘ atherosclerotic calcification\n‘Model Output:‘ Calcification\nEvaluation: True\nReason: \"Calcification\" captures the core meaning of \"atherosclerotic calcification\" in\nresponse to the question about brightness and is an acceptable generalization.\n**Example 4:**\n‘Question:‘ describe the lesions in the right kidney\n‘Answer:‘ cystic lesions\n‘Model Output:‘ multiple cysts\nEvaluation: True\nReason: \"Multiple cysts\" is a semantically equivalent description to \"cystic lesions\" in this\nmedical context; both refer to the same type of pathological finding.\n—\n**Now, please evaluate the following:**\n‘Question:‘ question\n‘Answer:‘ answer\n‘Model Output:‘ model_output\nEvaluation:\nReason:\n2. Prompt for concept recall task judge\n16\n"}, {"page": 17, "text": "Prompt for Concept Matching Task\nYou are an expert medical text analyst. Your task is to determine how many concepts\nfrom a provided list are substantially represented in a given output string.\nInstructions:\n1. You will be given an output_string and a concepts_list.\n2. For each concept in the concepts_list, carefully evaluate if its core meaning is\npresent in the output_string.\n3. When evaluating, consider the following:\n• Direct Match: The concept is explicitly stated.\n• Synonyms & Abbreviations: Common medical synonyms or abbreviations (e.g., “CT”\nfor “Computed Tomography”, “MI” for “myocardial infarction”) should be considered a\nmatch.\n• Phrasing Variations: The order of words or slight variations in phrasing should still be\nconsidered a match if the core components of the concept are present (e.g., “aneurysm of\nthe ascending aorta” for “aneurysm, ascending aorta”).\n• Substantial Representation: The key elements of the concept must be present. A\nfleeting mention or an unrelated context does not count.\n4. Finally, provide the total count of matched concepts as a single integer.\nExample:\noutput_string: “ct chest axial view showing a huge ascending aortic aneurysm (*).”\nconcepts_list: [’x-ray computed tomography’, ’aneurysm, ascending aorta’, ’pul-\nmonary embolism’]\nReasoning Process:\n1. x-ray computed tomography: MATCH - “ct” in the output string is a common\nabbreviation for “computed tomography”.\n2. aneurysm, ascending aorta: MATCH - “ascending aortic aneurysm” in the\noutput string directly refers to this concept.\n3. pulmonary embolism: NO MATCH - There is no mention of pulmonary embolism\nor related terms in the output string.\nExample Output Format:\n2\n—\nNow, perform the task for the following inputs:\noutput_string: {output}\nconcepts_list: {concepts}\nProvide the final count and brief reasoning process\nCount:\nReason:\n3. Prompt for Caption Prediction judge\n17\n"}, {"page": 18, "text": "Prompt for Caption Prediction judge\nYou are an expert in medical image caption analysis and semantic similarity. Your task\nis to score how closely the semantic meaning of a ‘Model Output‘ (a description of a\nmedical image) matches a ‘Caption‘ (a reference description), by also considering a\n‘Reference Concept List‘ extracted from the ‘Caption‘.\n**Inputs:**\n1. ‘Caption‘: The reference string describing the medical image.\n2. ‘Model Output‘: The model-generated string describing the medical image.\n3. ‘Reference Concept List‘: A list of key medical concepts explicitly mentioned or\nstrongly implied in the ‘Caption‘.\n**Scoring Rubric (0 to 3):**\nYour score should reflect how well the ‘Model Output‘ captures the essence of\nthe ‘Caption‘, with particular attention to whether the concepts in the ‘Reference Concept\nList‘ are appropriately represented in the ‘Model Output‘.\n* **Score 3 (High Semantic Correlation / Same Meaning):**\n* The ‘Model Output‘ conveys essentially the same core medical information, findings,\nand context as the ‘Caption‘.\n* Most, if not all, concepts from the ‘Reference Concept List‘ are accurately and clearly\nrepresented in the ‘Model Output‘ (or their direct synonyms/equivalents).\n* Minor differences in wording or specificity that do not alter the core meaning or the\nrepresentation of reference concepts are acceptable.\n* **Score 2 (Moderate Semantic Correlation):**\n* The ‘Model Output‘ shares significant commonalities with the ‘Caption‘ and accurately\nrepresents a good portion (e.g., more than half) of the concepts from the ‘Reference\nConcept List‘.\n* Some reference concepts might be missing, generalized, or slightly misidentified in the\n‘Model Output‘, but the overall gist and several key concepts align.\n* Alternatively, the ‘Model Output‘ might correctly represent the imaging modality and\ngeneral anatomy but miss some key pathological concepts from the list or vice-versa.\n* **Score 1 (Low Semantic Correlation):**\n* The ‘Model Output‘ has only a superficial or very general overlap with the ‘Caption‘.\n* Only a few (or none, but with some general contextual similarity like modality) concepts\nfrom the ‘Reference Concept List‘ are identifiable in the ‘Model Output‘, or they are\nrepresented inaccurately.\n* The core medical message, guided by the reference concepts, is largely different.\n* **Score 0 (No Semantic Correlation):**\n* The ‘Model Output‘ describes entirely different findings, anatomical regions, or imaging\nmodalities in a way that makes it irrelevant or contradictory to the ‘Caption‘.\n* Essentially none of the concepts from the ‘Reference Concept List‘ are found or\naccurately represented in the ‘Model Output‘.\n18\n"}, {"page": 19, "text": "Prompt for Caption Prediction judge (Cont’)\n**Evaluation Considerations (guided by the Reference Concept List):**\n* Does the ‘Model Output‘ reflect the correct imaging modality mentioned/implied by\nthe concepts?\n* Does it identify the correct anatomical structures/regions from the concepts?\n* Does it describe the key findings/pathologies represented in the concepts?\n* Are qualifiers (e.g., location, number, severity) from the concepts maintained if critical?\n**Output Format:**\nReturn your score as an integer (0, 1, 2, or 3) and a brief ‘Reason‘ explaining your\njudgment based on the rubric, the ‘Caption‘, the ‘Model Output‘, and how well the\n‘Reference Concept List‘ was reflected.\nFormat your response exactly as follows:\nScore: [0-3]\nReason: [Your brief explanation]\n**Examples:**\n**Example 1:**\n‘Caption:‘ anterior-posterior tibia-fibula radiographs of initial open tibia/fibula shaft\nfractures.\n‘Model Output:‘ ap x-ray of the left ankle showing a distal fibular fracture without\nsignificant displacement.\n‘Reference Concept List:‘ [’plain x-ray’, ’lower extremity’, ’postero-anterior’]\nScore: 1\nReason: ‘Model Output‘ matches ’plain x-ray’ (ap x-ray) and ’lower extremity’. However,\n‘Caption‘ refers to \"tibia/fibula shaft fractures,\" while output specifies \"distal fibular\nfracture\" in the \"left ankle,\" a more specific and potentially different location than implied\nby \"shaft.\" Key concept of \"tibia fracture\" is missing. Limited alignment with core concepts.\n**Example 2:**\n‘Caption:‘ ct chest before starting chemotherapy showed bilateral parenchymal metastatic\nnodules(red arrow)\n‘Model Output:‘ axial cect thorax image showing a well-defined solitary pulmonary\nnodule in the left lower lobe (indicated by red arrow).\n‘Reference Concept List:‘ [’x-ray computed tomography’, ’structure of parenchyma of\nlung’, ’metastatic to’, ’nodule’]\nScore: 2\nReason: ‘Model Output‘ matches ’x-ray computed tomography’ (cect thorax), ’structure of\nparenchyma of lung’ (pulmonary), and ’nodule’. However, ‘Caption‘ specifies \"bilateral\nparenchymal metastatic nodules,\" while output describes a \"solitary pulmonary nodule\nin the left lower lobe.\" The concepts ’metastatic to’ and the plurality/bilaterality are\nmissing or contradicted, which are key. Good overlap on modality and general pathology,\nbut key characteristics differ.\n19\n"}, {"page": 20, "text": "Prompt for Caption Prediction judge (Cont’)\n**Example 3:**\n‘Caption:‘ a ct scan of the chest.\nthe scan shows a small sub-pleural nodule-like\nconsolidation (white arrow).\n‘Model Output:‘ axial chest ct showing a peripheral, wedge-shaped area of consolidation\nin the left lower lobe, suggestive of pulmonary infarction (arrow).\n‘Reference Concept List:‘ [’x-ray computed tomography’, ’nodule’]\nScore: 1\nReason: ‘Model Output‘ matches ’x-ray computed tomography’ (chest ct). While both\nmention \"consolidation,\" the ‘Caption‘’s key finding is a \"nodule-like consolidation.\"\nThe ‘Model Output‘ describes a \"wedge-shaped area of consolidation suggestive of\npulmonary infarction,\" which is a different morphology and implication than a \"nodule.\"\nThe ’nodule’ concept is poorly represented.\n**Example 4:**\n‘Caption:‘ chest radiograph during initial presentation demonstrating complete opacifica-\ntion of the right hemithorax with mediastinal shift to the opposite side.\n‘Model Output:‘ chest x-ray shows complete opacification of the left hemithorax with\nmediastinal shift to the left, consistent with left lung collapse.\n‘Reference Concept List:‘ [’plain x-ray’, ’chest’, ’postero-anterior’, ’right thorax structure’]\nScore: 0\nReason:\nWhile ‘Model Output‘ matches ’plain x-ray’ (chest x-ray) and ’chest’, a\ncritical concept ’right thorax structure’ (opacification of *right* hemithorax) is directly\ncontradicted by the output mentioning the *left* hemithorax. This fundamental difference\nin laterality makes the core medical information entirely different despite shared modality.\n—\n**Now, please score the following:**\n‘Caption:‘ caption\n‘Model Output:‘ model_output\n‘Reference Concept List:‘ reference_concept_list\nScore:\nReason:\n20\n"}, {"page": 21, "text": "References\nChen, J., Gui, C., Ouyang, R., Gao, A., Chen, S., Chen, G. H., Wang, X., Zhang, R., Cai, Z., Ji, K.,\net al. (2024). Huatuogpt-vision, towards injecting medical visual knowledge into multimodal\nllms at scale. arXiv preprint arXiv:2406.19280.\nFan, Z., Tang, J., Chen, W., Wang, S., Wei, Z., Xi, J., Huang, F., and Zhou, J. (2024). Ai hospital:\nInteractive evaluation and collaboration of llms as intern doctors for clinical diagnosis. arXiv\ne-prints, pages arXiv–2402.\nGuo, D., Wu, F., Zhu, F., Leng, F., Shi, G., Chen, H., Fan, H., Wang, J., Jiang, J., Wang, J., et al.\n(2025). Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062.\nHong, J., Zhang, W., Qiao, S., Chen, J., Qiu, J., Zheng, C., Xu, Q., Ji, Y., Wen, Q., Sun, W.,\net al. (2025). Diagnosing shoulder disorders using multimodal large language models and\nconsumer-grade cameras. arXiv preprint arXiv:2510.09230.\nKavukcuoglu, K. (2025). Gemini 2.5: Our most intelligent ai model. Google Blog. Accessed:\n2025-06-19.\nLam, K. and Qiu, J. (2024). Foundation models: the future of surgical artificial intelligence?\nBritish Journal of Surgery, 111(4):znae090.\nLau, J. J., Gayen, S., Ben Abacha, A., and Demner-Fushman, D. (2018). A dataset of clinically\ngenerated visual questions and answers about radiology images. Scientific data, 5(1):1–10.\nLei, J., Zhang, X., Wu, C., Dai, L., Zhang, Y., Zhang, Y., Wang, Y., Xie, W., and Li, Y. (2024).\nAutorg-brain: Grounded report generation for brain mri. arXiv preprint arXiv:2407.16684.\nLi, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H., and Gao, J.\n(2023). Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\nAdvances in Neural Information Processing Systems, 36:28541–28564.\nLi, C.-Y., Chang, K.-J., Yang, C.-F., Wu, H.-Y., Chen, W., Bansal, H., Chen, L., Yang, Y.-P., Chen,\nY.-C., Chen, S.-P., et al. (2025). Towards a holistic framework for multimodal llm in 3d brain ct\nradiology report generation. Nature Communications, 16(1):2258.\nLiu, F., Zhu, T., Wu, X., Yang, B., You, C., Wang, C., Lu, L., Liu, Z., Zheng, Y., Sun, X., et al. (2023).\nA medical multimodal large language model for future pandemics. NPJ Digital Medicine,\n6(1):226.\nLo, F. P.-W., Qiu, J., Wang, Z., Chen, J., Xiao, B., Yuan, W., Giannarou, S., Frost, G., and Lo, B.\n(2024). Dietary assessment with multimodal chatgpt: a systematic analysis. IEEE Journal of\nBiomedical and Health Informatics.\nMa, W., He, J., Snell, C., Griggs, T., Min, S., and Zaharia, M. (2025). Reasoning models can be\neffective without thinking. arXiv preprint arXiv:2504.09858.\nMustafa, A., Naseem, U., and Azghadi, M. R. (2025). Can reasoning llms enhance clinical\ndocument classification? arXiv preprint arXiv:2504.08040.\nNori, H., Usuyama, N., King, N., McKinney, S. M., Fernandes, X., Zhang, S., and Horvitz, E.\n(2024). From medprompt to o1: Exploration of run-time strategies for medical challenge\nproblems and beyond. arXiv preprint arXiv:2411.03590.\n21\n"}, {"page": 22, "text": "OpenAI (2025). Introducing openai o3 and o4-mini. OpenAI Blog. Accessed: 2025-06-19.\nPatel, S. B. and Lam, K. (2023). Chatgpt: the future of discharge summaries? The Lancet Digital\nHealth, 5(3):e107–e108.\nQiu, J., Lam, K., Li, G., Acharya, A., Wong, T. Y., Darzi, A., Yuan, W., and Topol, E. J. (2024a).\nLlm-based agentic systems in medicine and healthcare. Nature Machine Intelligence, 6(12):1418–\n1420.\nQiu, J., Li, L., Sun, J., Peng, J., Shi, P., Zhang, R., Dong, Y., Lam, K., Lo, F. P.-W., Xiao, B., et al.\n(2023). Large ai models in health informatics: Applications, challenges, and the future. IEEE\nJournal of Biomedical and Health Informatics, 27(12):6074–6087.\nQiu, J., Li, L., Sun, J., Wei, H., Xu, Z., Lam, K., and Yuan, W. (2025). Emerging cyber attack risks\nof medical ai agents. arXiv preprint arXiv:2504.03759.\nQiu, J., Yuan, W., and Lam, K. (2024b). The application of multimodal large language models in\nmedicine. The Lancet Regional Health–Western Pacific, 45.\nRen, L., Chen, C., Xu, H., Kim, Y. J., Atkinson, A., Zhan, Z., Sun, J., Peng, B., Liu, L., Wang, S.,\net al. (2025). Decoder-hybrid-decoder architecture for efficient reasoning with long generation.\narXiv preprint arXiv:2507.06607.\nRückert, J., Bloch, L., Brüngel, R., Idrissi-Yaghir, A., Schäfer, H., Schmidt, C. S., Koitka, S., Pelka,\nO., Abacha, A. B., G. Seco de Herrera, A., et al. (2024). Rocov2: Radiology objects in context\nversion 2, an updated multimodal image dataset. Scientific Data, 11(1):688.\nSeenivasan, L., Islam, M., Kannan, G., and Ren, H. (2023). Surgicalgpt: end-to-end language-\nvision gpt for visual question answering in surgery. In International conference on medical image\ncomputing and computer-assisted intervention, pages 281–290. Springer.\nSui, Y., Chuang, Y.-N., Wang, G., Zhang, J., Zhang, T., Yuan, J., Liu, H., Wen, A., Zhong, S., Chen,\nH., et al. (2025). Stop overthinking: A survey on efficient reasoning for large language models.\narXiv preprint arXiv:2503.16419.\nSun, J., Zheng, C., Xie, E., Liu, Z., Chu, R., Qiu, J., Xu, J., Ding, M., Li, H., Geng, M., et al. (2023).\nA survey of reasoning with foundation models: Concepts, methodologies, and outlook. ACM\nComputing Surveys.\nThawkar, O., Shaker, A., Mullappilly, S. S., Cholakkal, H., Anwer, R. M., Khan, S., Laaksonen,\nJ., and Khan, F. S. (2023). Xraygpt: Chest radiographs summarization using medical vision-\nlanguage models. arXiv preprint arXiv:2306.07971.\nThirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., and Ting, D. S. W.\n(2023). Large language models in medicine. Nature medicine, 29(8):1930–1940.\nXu, T., Huang, Z., Sun, J., Cheng, S., and Lam, W. (2025). Seqpo-simt: Sequential policy\noptimization for simultaneous machine translation. arXiv preprint arXiv:2505.20622.\nYang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al.\n(2025). Qwen3 technical report. arXiv preprint arXiv:2505.09388.\nYe, Z., Chen, J., Light, J., Wang, Y., Sun, J., Schwager, M., Torr, P., Li, G., Chen, Y., Yang, K., et al.\n(2024). Reasoning in reasoning: A hierarchical framework for better and faster neural theorem\nproving. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS’24.\n22\n"}, {"page": 23, "text": "Zhang, J., Lin, N., Hou, L., Feng, L., and Li, J. (2025a). Adaptthink: Reasoning models can learn\nwhen to think. arXiv preprint arXiv:2505.13417.\nZhang, W., Qiao, S., Luo, L., Li, Y., Zheng, C., Xu, Q., Li, M., Gui, Y., He, Y., Qiu, J., et al. (2025b).\nSynapseroute: An auto-route switching framework on dual-state large language model. arXiv\npreprint arXiv:2507.02822.\nZhang, X., Shi, Y., Ji, J., Zheng, C., and Qu, L. (2025c). Mepnet: Medical entity-balanced\nprompting network for brain ct report generation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 39, pages 25940–25948.\nZheng, C., Gao, Y., Shi, H., Xiong, J., Sun, J., Li, J., Huang, M., Ren, X., Ng, M., Jiang, X., et al.\n(2024). Dape v2: Process attention score as feature map for length extrapolation. arXiv preprint\narXiv:2410.04798.\nZheng, C., Sun, J., Gao, Y., Wang, Y., Wang, P., Xiong, J., Ren, L., Cheng, H., Kulkarni, J., Shen, Y.,\net al. (2025a). Sas: Simulated attention score. arXiv preprint arXiv:2507.07694.\nZheng, C., Sun, J., Gao, Y., Xie, E., Wang, Y., Wang, P., Xu, T., Chang, M., Ren, L., Li, J., et al.\n(2025b). Understanding the mixture-of-experts with nadaraya-watson kernel. arXiv preprint\narXiv:2509.25913.\nZhou, J., He, X., Sun, L., Xu, J., Chen, X., Chu, Y., Zhou, L., Liao, X., Zhang, B., and Gao, X. (2023).\nSkingpt-4: an interactive dermatology diagnostic system with visual large language model.\narXiv preprint arXiv:2304.10691.\n23\n"}, {"page": 24, "text": "Table 2 | Descriptive information of VQA-RAD dataset\nQuestion\nType\nAbbreviation Description\nExample\nQues-\ntion\nCount\nProportion\nobject/condition\npresence\npres\nif\ncertain\nob-\nject/condition\npresent\nis there evidence\nof\nan\naortic\naneurysm\n813\n36.21%\npositional rea-\nsoning\npos\nposition or location\nof an object or organ\nwhere is the ab-\nnormality\n322\n14.34%\nabnormality\nabnormality\nif abnormality shows\nin the image\nis the lung normal\n204\n9.09%\nother\nother\nother questions out\nof listed categories\nhow is the patient\noriented\n196\n8.73%\nmodality\nmodality\nmodality of image\nis this a MRI of\nthe chest\n185\n8.24%\nsize\nsize\nsize of an object\nis the spleen nor-\nmal size\n175\n7.80%\nplane\nplane\norientation of image\nslicing through the\nbody\nis this an anterior\nor posterior im-\nage\n120\n5.35%\nattribute other\nattrib\nother types of de-\nscription questions\nare the hilar soft\ntissue\ndensities\nsymmetric\n93\n4.14%\norgan system\norgan\ncategory of a system\nwhat organ sys-\ntem is pictured\n59\n2.63%\ncolor\ncolor\nsignal intensity\ndoes the csf have\nhigh signal inten-\nsity\n54\n2.41%\ncounting\ncount\nquantity of objects\nhow many lesions\nare in the spleen\n24\n1.07%\n24\n"}, {"page": 25, "text": "Table 3 | Close-ended VQA Question Type Distribution\nType\nCount\nProportion\npres\n622\n52.18%\nsize\n156\n13.09%\nabnormality\n119\n9.98%\nmodality\n72\n6.04%\nplane\n53\n4.45%\nother\n52\n4.36%\nattrib\n41\n3.44%\ncolor\n31\n2.60%\npos\n19\n1.59%\norgan\n17\n1.43%\ncount\n10\n0.84%\nTable 4 | Open-ended VQA Question Type Distribution\nType\nCount\nProportion\npos\n303\n28.77%\npres\n191\n18.14%\nother\n144\n13.68%\nmodality\n113\n10.73%\nabnormality\n85\n8.07%\nplane\n67\n6.36%\nattrib\n52\n4.94%\norgan\n42\n3.99%\ncolor\n23\n2.18%\nsize\n19\n1.80%\ncount\n14\n1.33%\nTable 5 | Distribution of modalities for ROCOv2\nModality\nCount\nProportion\nCT\n746\n37.30%\nX-ray\n515\n25.75%\nUltrasound\n326\n16.30%\nMRI\n306\n15.30%\nangiogram\n82\n4.10%\nothers\n25\n1.25%\nTable 6 | Overall Consistency on Close-ended VQA\nModel\nConsistency\nSeed1.5-VL-T\n91.69%\nSeed1.5-VL\n92.53%\nGemini-2.5-Flash-T\n78.27%\nGemini-2.5-Flash\n87.08%\n25\n"}, {"page": 26, "text": "Table 7 | Average thinking tokens across tasks\nModel\nClose-\nended\nVQA\nOpen-ended VQA\nConcept Detection\nSeed1.5-VL-T\n75.2\n273.1\n105.5\nGemini-2.5-Flash-T\n381.4\n756.2\n1167.9\nTable 8 | Comparison of model latency and performance\nClose-ended VQA\nOpen-ended VQA\nConcept Recall\nCaption Prediction\nModel\nLatency Score\nLatency Score\nLatency Score\nLatency Score\nSeed1.5-VL-T\n2.59\n76.03\n10.48\n51.57\n4.09\n47.42\n4.09\n37.78\nSeed1.5-VL\n0.94\n75.87\n0.80\n51.47\n0.95\n46.32\n0.95\n36.50\nGemini-2.5-Flash-T\n4.06\n72.93\n6.62\n46.15\n9.60\n47.57\n9.60\n37.12\nGemini-2.5-Flash\n1.56\n72.12\n1.55\n46.72\n1.75\n46.63\n1.75\n34.68\n26\n"}]}