{"doc_id": "arxiv:2511.11922", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.11922.pdf", "meta": {"doc_id": "arxiv:2511.11922", "source": "arxiv", "arxiv_id": "2511.11922", "title": "Additive Large Language Models for Semi-Structured Text", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "published": "2025-11-14T23:06:16Z", "updated": "2025-11-14T23:06:16Z", "summary": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.11922v1", "url_pdf": "https://arxiv.org/pdf/2511.11922.pdf", "meta_path": "data/raw/arxiv/meta/2511.11922.json", "sha256": "2775077fc9383b12509fce805abbceca1c9862ec13fdec6e7e3d4d23c1146652", "status": "ok", "fetched_at": "2026-02-18T02:27:03.904028+00:00"}, "pages": [{"page": 1, "text": "Additive Large Language Models for Semi-Structured Text\nKarthikeyan K\nDuke University\nkarthikeyan.k@duke.edu\nRaghuveer Thirukovalluru\nDuke University\nraghuveer.thirukovalluru@duke.edu\nDavid Carlson\nDuke University\ndavid.carlson@duke.edu\nAbstract\nLarge Language Models have advanced clinical\ntext classification, but their opaque predictions\nremain a critical barrier to practical adoption in\nresearch and clinical settings where investiga-\ntors and physicians need to understand which\nparts of a patient’s record drive risk signals. To\naddress this challenge, we introduce CALM,\nshort for Classification with Additive Large\nLanguage Models, an interpretable framework\nfor semi-structured text where inputs are com-\nposed of semantically meaningful components,\nsuch as sections of an admission note or ques-\ntion–answer fields from an intake form. CALM\npredicts outcomes as the additive sum of each\ncomponent’s contribution, making these contri-\nbutions part of the forward computation itself\nand enabling faithful explanations at both the\npatient and population level. The additive struc-\nture also enables clear visualizations, such as\ncomponent-level risk curves similar to those\nused in generalized additive models, making\nthe learned relationships easier to inspect and\ncommunicate. Although CALM expects semi-\nstructured inputs, many clinical documents al-\nready have this form, and similar structure can\noften be automatically extracted from free-text\nnotes. CALM achieves performance compa-\nrable to conventional LLM classifiers while\nimproving trust, supporting quality-assurance\nchecks, and revealing clinically meaningful pat-\nterns during model development and auditing.\n1\nIntroduction\nThe availability of large-scale patient data has ac-\ncelerated the development of machine learning in\nhealthcare, enabling models that detect patterns in\ncomplex clinical data for tasks such as risk predic-\ntion (e.g., ICU mortality) (Che et al., 2017; Mc-\nNamara et al., 2016), diagnostic support (e.g., dis-\ntinguishing cancer types) (Wu and Hicks, 2021),\nand prognosis estimation (e.g., hospital readmis-\nsion)(Kourou et al., 2015; Zhang et al., 2023; Diller\net al., 2019). These research applications demon-\nstrate the potential of machine learning to improve\nclinical decision-making and outcomes, yet transla-\ntion from research success to routine clinical adop-\ntion remains limited. Clinicians often rely instead\non simple, rule-based scores such as the Framing-\nham Risk Score for cardiovascular disease (Mah-\nmood et al., 2014) or APACHE (Larvin and Mcma-\nhon, 1989) and SOFA (Ferreira et al., 2001; Lamb-\nden et al., 2019) for ICU mortality risk. Although\nless accurate than modern deep learning models,\nthese tools remain popular because the contributing\nfactors—such as heart rate, blood pressure, or lab\nvalues—are explicit and interpretable. Models that\noutput only a “high-risk” label offer little value if\nusers cannot see which factors shaped the predic-\ntion or by how much, underscoring the need for\ninterpretability to make machine-learning outputs\nactionable and trustworthy in clinical settings.\nMost inherently interpretable machine-learning\napproaches have been developed for structured, tab-\nular data, where each patient is represented by\na fixed-length vector of numeric or categorical\nvariables (e.g., vitals, lab results, demographics).\nThese tabular models include sparse linear scoring\nsystems, rule-based models, generalized additive\nmodels, and constrained trees or ensembles (Us-\ntun et al., 2013; Ustun and Rudin, 2017; Angelino\net al., 2017; Lakkaraju et al., 2016; Lou et al., 2013;\nCaruana et al., 2015; Nori et al., 2019; Freund and\nSchapire, 1997; Speybroeck, 2012). These meth-\nods work well on standardized features, but tabu-\nlar models may miss important nuances of clini-\ncal care and vary across institutions, which limits\nperformance and transferability. In contrast, text-\nbased classifiers operate directly on unstructured or\nsemi-structured clinical narratives such as admis-\nsion notes, progress notes, or discharge summaries.\nClinical text can capture richer observations about a\npatient’s status (e.g., evolving symptoms, clinician\nimpressions, or social context often missing from\narXiv:2511.11922v1  [cs.CL]  14 Nov 2025\n"}, {"page": 2, "text": "structured fields). Recent advances in LLMs posi-\ntion text as a promising foundation for inherently\ninterpretable methods that can generate meaningful\ninsights and support real-world decision-making.\nUnlike tabular models, text-based classifiers in\nhealthcare are typically opaque, and interpretability\nhas often been sought through post-hoc explana-\ntions of black-box models. Common approaches in-\nclude local surrogate models (Ribeiro et al., 2016),\nsaliency- or attribution-based techniques (Lund-\nberg and Lee, 2017; Sundararajan et al., 2017;\nZeiler and Fergus, 2013; Fong and Vedaldi, 2017),\nattention-based rationales (Bahdanau et al., 2014;\nJain and Wallace, 2019), and influence-function\nmethods that trace predictions to training examples\n(Koh and Liang, 2017; Pruthi et al., 2020; Wen,\n2024). However, these explanations often fail to\ncapture true model reasoning: saliency maps can be\ninsensitive to parameter or data changes (Adebayo\net al., 2018), attention weights may not align with\nfeature importance (Jain and Wallace, 2019), and\nremoving tokens flagged as important frequently\nleaves predictions unchanged (Hooker et al., 2018).\nSuch limitations undermine trust in clinical appli-\ncations and highlight that opaque models with ap-\nproximate explanations are not a substitute for in-\nherently interpretable models(Rudin, 2018). These\nchallenges motivate the development of text-native,\ninherently interpretable methods that can provide\nfaithful insights into model behavior.\nDespite substantial progress on interpretable\nmodels for tabular data, text-based prediction still\nrelies largely on post-hoc explanations of black-\nbox models, which remain unreliable for high-\nstakes settings. To address this gap, we propose\nCALM (Classification with Additive Large Lan-\nguage Models), an inherently interpretable frame-\nwork for predictive modeling with text. CALM is\ndesigned for semi-structured inputs, where each\ndocument—such as a patient note—can be decom-\nposed into meaningful components like sections\nof clinical documents, question–answer fields, or\nsurvey responses. Recent tools such as Clinstruc-\ntor (K et al., 2025) can automatically transform\nfree-text notes into this semi-structured form. By\nenforcing an additive decomposition across compo-\nnents, CALM produces faithful component-level\ncontributions that make predictions transparent and\nreadily visualized.\nWe evaluate CALM on three real-world clini-\ncal outcome tasks. Across all tasks, CALM per-\nforms competitively with black-box LLM classi-\nfiers, achieving comparable AUC-PR while offer-\ning inherent interpretability at both the patient- and\npopulation-level. Variants that capture pairwise in-\nteractions (CALM2) or distill knowledge from fine-\ntuned teachers further close the already small per-\nformance gap. These results demonstrate that addi-\ntive LLM-based models can provide transparency\nwithout sacrificing performance, offering a practi-\ncal framework for building trustworthy machine-\nlearning systems in healthcare.\n2\nBackground and Related Works\nNeural Additive Models (NAMs)\n(Agarwal\net al., 2021; Hastie, 2017)are inherently inter-\npretable architectures that learn a distinct neural\nnetwork for each individual input feature. The fi-\nnal prediction is computed as the bias term plus\nthe sum (or mean) of the logits from these feature-\nspecific subnetworks, allowing users to directly\nobserve each feature’s contribution to the outcome.\nSeveral variants of NAMs such as NA2M, GAMI-\nNet have also been proposed that incorporate pair-\nwise interactions (Yang et al., 2021; Chang et al.,\n2021; Xu et al., 2023; Peroni et al., 2022; Enouen\nand Liu, 2022; Chang et al., 2021). Unlike NAMs\nand its variants, where the input to each neural\nmodel is a scalar feature, CALM operates over tex-\ntual inputs—each feature corresponds to a piece\nof text such as a section from a patient’s notes or\na question–answer pair and uses shared LLMs as\nfeature-specific subnetworks.\nClinStructor\n(K et al., 2025)is a three-stage\npipeline that uses Large Language Models (LLMs)\nto convert unstructured clinical notes into struc-\ntured question-answer pairs for predictive model-\ning. First, it identifies the most relevant clinical\nquestions based on the patient notes (Feature Iden-\ntification). Next, it extracts answers to these ques-\ntions from each patient’s record (Feature Extrac-\ntion). Finally, it fine-tunes a language model on the\nconcatenated question-answer-pairs. We follow the\nthe same process as ClinStructor’s question-answer\ncreation and use it as one of the dataset for our\nexperiments.\nPost-hoc Explanation Methods:\nCommon ap-\nproaches include feature-attribution techniques\nsuch as SHAP (Lundberg and Lee, 2017; Mosca\net al., 2022) and Integrated Gradients (Sundarara-\njan et al., 2017), local surrogate models like\nLIME (Ribeiro et al., 2016), attention visualiza-\n"}, {"page": 3, "text": "tion methods (Vig, 2019; Yeh et al., 2023), and\nsimilarity-based approaches such as K-Nearest\nNeighbors (Papernot and McDaniel, 2018) and In-\nfluence Functions (Koh and Liang, 2017; Pruthi\net al., 2020), which relate predictions to influential\nor similar examples from the training data. How-\never, such post-hoc explanation may not be faithful\nor reliable explanations and may not accurately\nreflect a model’s true reasoning process (Rudin,\n2018; Basu et al., 2020; Karthikeyan and Søgaard,\n2021; Slack et al., 2020; Dasgupta et al., 2022;\nBarr et al., 2023; Miró-Nicolau et al., 2024; Bhalla\net al., 2023). Unlike post-hoc explanation methods,\nCALM is an inherently interpretable architectures,\nthat provides both local and global interpretability.\n3\nCALM: Classification with Additive\nLarge Language Models\nWe propose CALM, a general and interpretable\nframework for text classification. The input to\nCALM, denoted as x = x1, x2, . . . , xM, consists\nof a collection of M textual components, where\neach component xi represents a semantically mean-\ningful text segment—such as a section of a clin-\nical note, a question–answer pair, or a response\nfrom a survey form. While CALM requires semi-\nstructured text as input, it can also be applied to\nunstructured text. In such cases, we can first trans-\nform the text into a semi-structured form suitable\nfor CALM by following the segmentation proce-\ndures introduced in ClinStructor (K et al., 2025).\nBefore introducing the CALM architecture, we\nfirst outline the standard fine-tuned LLM classifier,\nwhich serves as our baseline.\n3.1\nBaseline: Standard LLM Fine-tuning\nIn the standard text classification approach, all\ninput components are concatenated into a single\nsequence ˜x = xix2..xM. This sequence is then\npassed through a Large Language Model (LLM)\nbackbone FT (·; wT ), followed by a classification\nhead Flast(·; Wlast). The logits z ∈RC and proba-\nbilities pc for C classes are computed as\nz = flast\n\u0000fT (˜x; wT ), wlast\n\u0001\n,\npc =\nexp(zc)\nPC\nj=1 exp(zj),\nc = 1, . . . , C.\nThe model is trained by minimizing a classification\nloss L(ytrue, p), such as cross-entropy. While such\nfine-tuning typically achieves strong predictive per-\nformance, its critical limitation is a complete lack\nof interpretability.\n3.2\nCALM Architecture\nThe CALM architecture modifies the standard clas-\nsifier by enforcing an additive structure on the final\nprediction, ensuring component-level interpretabil-\nity. Each input component xi is first independently\nencoded by the shared LLM backbone FT (·; wT ).\nThe resulting representation hi is then mapped to\nclass logits ℓi ∈RC by a component-specific clas-\nsification head F i\nlast(·; W i\nlast):\nhi = fT (xi; wT );\nℓi = fi\nlast(hi; wi\nlast).\nThe overall logit vector z ∈RC is the average of\nthe component contributions, with a learned bias b:\nz =\n1\nM\nPM\ni=1 ℓi + b.\nPredicted class probabilities pc are then calculated\nusing the softmax function over z. The model is\ntrained using the same loss L(ytrue, p) as the stan-\ndard fine-tuned LLM classifier. This additive for-\nmulation inherently exposes the influence of every\ncomponent directly at the logit level, as ℓi depends\nonly on xi and naturally represents its contribution\nto the final prediction.\nInterpretability\nThe additive structure provides\nintrinsic patient-level interpretability by outputting\nthe contribution ℓi for each component xi. These\ncontributions quantify the magnitude and direction\nof influence each component has on the final pre-\ndiction, allowing the decision to be directly traced\nto specific input evidence. Furthermore, aggregat-\ning ℓi across a dataset provides model-level inter-\npretability, quantifying the global importance of\ndifferent input components across the population.\nA detailed analysis and discussion of CALM inter-\npretability is provided in Section 5.\nTime Complexity\nLet Li := |xi|, Lmax :=\nmaxi Li, and Ltot\n:= PM\ni=1 Li.\nA standard\nfine-tuned classifier processes the concatenated se-\nquence in O(L2\ntot) time. In contrast, CALM pro-\ncesses each component independently, resulting\nin a theoretical total cost of PM\ni=1 O(L2\ni ). Since\nPM\ni=1 L2\ni < L2\ntot for M > 1, CALM offers signifi-\ncant efficiency improvements. For M equal-length\ncomponents, CALM achieves an M-fold speedup\n(ML2 vs. M2L2). Thus, CALM is not only in-\nterpretable but can also be significantly more effi-\ncient than direct fine-tuning, provided component\nlengths are not highly imbalanced. In practice,\ncomponents are often batched by padding to Lmax,\n"}, {"page": 4, "text": "yielding a practical cost of O(ML2\nmax), which can\noccasionally be larger than O(L2\ntot). To mitigate\nthis, we discuss an alternative packed implementa-\ntion in the Appendix that is mathematically equiva-\nlent and has an overall cost of at most O(L2\ntot).\n3.3\nCALM2 Modeling Pairwise Interactions\nWhile the CALM architecture enforces indepen-\ndent component contributions to the final predic-\ntion, considering components jointly can reveal\na stronger signal than when viewed in isolation.\nFor instance, certain lab values may appear nor-\nmal on their own, yet become clinically significant\nwhen interpreted alongside a patient’s age from an-\nother segment. To effectively capture such interac-\ntions, we propose CALM2, an extension of CALM\nthat incorporates explicit pairwise interaction terms.\nPairwise interactions can be incorporated in two\nprincipal ways.\n(i) Text-level interactions.\nThis approach pro-\ncesses the M individual components along with\nall\n\u0000M\n2\n\u0001\npairwise concatenations [xi; xj].\nEach\ncombined sequence is passed through the LLM\nbackbone f(·; wT ) and a pair-specific classification\nhead gij(·; wij). However, the overall computation\ncost is prohibitively high, as this requires O(M2)\nforward passes over longer sequences, making the\napproach impractical for moderate or large M.\n(ii) Embedding-level interactions.\nA more ef-\nficient alternative computes the embeddings of\neach component hi = f(xi; wT ) and then de-\nfines interaction terms ℓij in the hidden space us-\ning a lightweight interaction function gij, where\nℓij = gij(hi, hj; wij). The final logits are\nz = 1−β\nM\nPM\ni=1 ℓi +\nβ\n(M\n2 )\nP\n1≤i<j≤M ℓij.\nHere, β is an inductive weight, representing the\nprior belief on how much to rely on the interaction\nlogits. This approach reuses the CALM computa-\ntions and adds pairwise terms through lightweight\noperations, maintaining near CALM complexity\nwhile providing greater expressivity.\nChoice of gij.\nWe instantiate gij using a low-rank\nbilinear interaction model. Each embedding hi is\nfirst projected into an R-dimensional latent space\n(˜hi = Uihi, ˆhj = Ujhj), and their interaction is\ncaptured by an elementwise (Hadamard) product,\nfollowed by a pair-specific classifier head:\nℓij = wij\nout(˜hi ⊙ˆhj),\nwij\nout ∈RC×R.\nBy choosing a small rank R (e.g., 8 or 16), the\nadditional parameters and computation introduced\nby the quadratic number of pairs remain limited,\nwith an overall complexity of O(M2R). Note that\nCALM2 remains highly interpretable, as each in-\nteraction term ℓij depends only on (xi, xj). This\nallows us to visualize ℓij as a function of (xi, xj)\nusing heatmaps or 3D plots, analogous to the pair-\nwise analysis in GA2Ms.\n3.4\nCALM-Distill: Enhancing CALM with\nKnowledge Distillation\nThe CALM architecture achieves efficient and in-\nherently interpretable classification via its additive\nstructure. To further enhance its predictive capabili-\nties, we introduce CALM-Distill, an extension that\ntransfers knowledge from a strong, fully fine-tuned\nblack-box teacher LLM while preserving CALM’s\ninterpretable architecture.\nThe teacher model produces logits z(T), and the\nCALM student produces additive logits z(S) =\n1\nM\nPM\ni=1 ℓi + b. CALM-Distill training minimizes\na combined objective function L that is a convex\ncombination of the ground-truth cross-entropy loss\n(LCE) and the knowledge distillation loss (LKD).\nThe distillation loss LKD is the T 2-scaled\nKullback-Leibler (KL) divergence between the stu-\ndent and teacher distributions, q(S) and q(T), which\nare calculated from the logits and temperature T as\nfollows:\nq(T) = softmax\n\u0010\nz(T )\nT\n\u0011\n,\nq(S) = softmax\n\u0010\nz(S)\nT\n\u0011\nLKD = T 2 KL\n\u0010\nq(T) ∥q(S)\u0011\nL = (1 −α) LCE + α LKD,\nwhere the hyperparameter α ∈[0, 1] balances\nground-truth supervision against the teacher’s guid-\nance. This process effectively transfers the dark\nknowledge – the relative probabilities and uncer-\ntainties encoded in the teacher’s soft targets – to the\nstudent. CALM-Distill leverages teacher’s knowl-\nedge without modifying its architecture, thereby\nretaining full interpretability.\n4\nExperiments and Results\n4.1\nDatasets\nWe evaluate CALM and its extensions on three\nreal-world datasets derived from Intensive Care\nUnit (ICU) patient notes: (1) MIMIC Admission\nNotes, (2) ClinStructor, and (3) the Long Clinical\nDocument (LCD) benchmark. All three datasets\n"}, {"page": 5, "text": "Dataset\nModel\nFinetune\nCALM\nAUC-PR\nF1\nAUC-ROC\nAUC-PR\nF1\nAUC-ROC\nClinStructor\nQwen3-0.6B-Base\n0.43\n0.449\n0.839\n0.418\n0.441\n0.825\nQwen3-1.7B-Base\n0.458\n0.464\n0.855\n0.427\n0.439\n0.826\nQwen3-8B-Base\n0.467\n0.474\n0.857\n0.451\n0.466\n0.842\nPhi-3.5-mini-instruct\n0.454\n0.477\n0.857\n0.450\n0.455\n0.847\nMediPhi\n0.465\n0.473\n0.854\n0.442\n0.465\n0.841\ngemma-3-4b-pt\n0.469\n0.47\n0.856\n0.428\n0.441\n0.832\nmedgemma-4b-pt\n0.448\n0.467\n0.852\n0.440\n0.447\n0.842\nMIMIC-III\nQwen3-0.6B-Base\n0.457\n0.471\n0.85\n0.448\n0.457\n0.841\nQwen3-1.7B-Base\n0.494\n0.481\n0.859\n0.470\n0.469\n0.849\nQwen3-8B-Base\n0.532\n0.506\n0.874\n0.496\n0.5\n0.862\nPhi-3.5-mini-instruct\n0.522\n0.506\n0.87\n0.482\n0.477\n0.854\nMediPhi\n0.508\n0.486\n0.866\n0.455\n0.475\n0.851\ngemma-3-4b-pt\n0.51\n0.49\n0.868\n0.481\n0.487\n0.862\nmedgemma-4b-pt\n0.514\n0.491\n0.866\n0.479\n0.478\n0.859\nLCD\nQwen3-0.6B-Base\n0.251\n0.315\n0.872\n0.262\n0.292\n0.851\nQwen3-1.7B-Base\n0.258\n0.332\n0.877\n0.268\n0.312\n0.867\ngemma-3-4b-pt\n0.267\n0.33\n0.865\n0.276\n0.336\n0.867\nmedgemma-4b-pt\n0.249\n0.311\n0.858\n0.251\n0.307\n0.863\nTable 1: Comparison of CALM and Black-box LLM Finetuning\nexhibit significant class imbalance. To address this,\nwe subsample the training data by retaining all pos-\nitive examples and randomly sampling an equal\nnumber of negative examples. The original class\ndistributions are strictly maintained for the valida-\ntion and test sets.\nMIMIC Admission Notes\n(van Aken et al.,\n2021) This dataset is constructed from MIMIC-III\ndischarge summaries by retaining only the tex-\ntual sections typically recorded upon a patient’s\nICU arrival (e.g., Chief Complaint, Medical His-\ntory, and Physical Exam). We preprocess and seg-\nment these into eight distinct sections, where each\nsection serves as an input component, xi, for the\nCALM framework. The objective is to predict in-\nhospital mortality: whether a patient dies during the\nICU stay or is discharged alive. After subsampling\nthe training data, the final dataset comprises 7,027\ntraining examples (3,506 deceased, 3,521 alive),\n4,882 validation examples (514 deceased, 4,368\nalive), and 9,773 test examples (1,020 deceased,\n8,753 alive).\nClinStructor\n(K et al., 2025) is a structured rep-\nresentation of clinical notes derived from MIMIC\nAdmission Notes. To create ClinStructor data, we\nfollow the procedures in (K et al., 2025); how-\never, instead of Llama 3.3 and Qwen 2.5, we use\nGemini-flash-2.51. The task setup and the number\nof examples are identical to the MIMIC Admission\nNotes dataset. Unlike MIMIC, ClinStructor com-\nprises of 50 fine-grained question–answer pairs,\neach corresponding to one CALM component; see\nthe appendix for the full list.\nLong Clinical Document (LCD) Benchmark\n(Yoon et al., 2025) consists of long discharge notes\nfrom the MIMIC-IV database for patients admitted\nto later discharged from ICU. Similar to MIMIC,\nwe pre-process and split each note into 22 sections.\nThe task is to predict 30-day out-of-hospital mor-\ntality. The final dataset includes 2,534 training\nexamples (1,267 deceased and 1,267 alive), 7,505\nvalidation examples (302 deceased and 7,203 alive),\nand 7,568 test examples (261 deceased and 7,307\nalive).\n4.2\nExperiment Setup\nModels:\nWe experimented with seven open-\nsource LLMs—Qwen 3 (0.6B, 1.7B, 8B) (Yang\net al., 2025), Phi-3.5-mini, MediPhi (Corbeil et al.,\n2025), Gemma-3 (4B) (Team et al., 2025), and\n1We use Gemini through Vertex AI, which complies with\nthe MIMIC data use agreement.\n"}, {"page": 6, "text": "Model\nCALM2 (AUC-PR)\nFinetune\nCALM\nR=8, β=0.1\nR=8,β=0.5\nR=16, β=0.1\nR=16, β=0.5\nQwen3-0.6B-Base\n0.447\n0.445\n0.437\n0.464\n0.457\n0.448\nQwen3-1.7B-Base\n0.476\n0.486\n0.475\n0.464\n0.494\n0.470\nQwen3-8B-Base\n0.504\n0.482\n0.485\n0.503\n0.532\n0.496\nPhi-3.5-mini-instruct\n0.480\n0.462\n0.473\n0.476\n0.522\n0.482\ngemma-3-4b-pt\n0.487\n0.500\n0.487\n0.492\n0.510\n0.481\nTable 2: Comparison of CALM2 and CALM: We report AUC-PR on the MIMIC admission notes dataset. CALM2\noutperforms CALM and further narrows the gap to black-box fine-tuning.\nModel\nCALM-Distill (AUC-PR)\nFinetune/Teacher\nCALM\nα = 0.2\nα = 0.4\nα = 0.6\nQwen3-0.6B-Base\n0.454\n0.445\n0.45\n0.457\n0.448\nQwen3-1.7B-Base\n0.479\n0.486\n0.477\n0.494\n0.470\nQwen3-8B-Base\n0.512\n0.517\n0.517\n0.532\n0.496\nPhi-3.5-mini-instruct\n0.486\n0.495\n0.503\n0.522\n0.482\ngemma-3-4b-pt\n0.505\n0.504\n0.524\n0.510\n0.481\nTable 3: Comparison of CALM-Distill and CALM: We report AUC-PR on the MIMIC admission notes dataset.\nCALM-Distill consistently outperforms CALM and further narrows the gap to black-box finetuning.\nMedGemma-3 (4B) (Sellergren et al., 2025)—span-\nning diverse parameter sizes, model families, and\ndomains, including both general-purpose and med-\nical LLMs.\nTraining Hyperparameters:\nWe ensure fair\ncomparison across all experiments – regular fine-\ntuning, CALM, and its variants – by using LoRA\nfine-tuning for 5 epochs with batch size 1, gradi-\nent accumulation 16, LoRA dropout 0.05, and the\nAdamW optimizer. We tune learning rate (1e-4, 2e-\n4, 5e-4), LoRA rank (8, 16), and scaling factor (16,\n32) across eight settings, applied consistently in\nall experiments (see Appendix). The best model is\nselected using validation data, and its performance\nis reported on the test set.\nMetrics.\nFor all experiments, we report AUC-\nPR, F1, and AUC-ROC. AUC-PR is used for early\nstopping and for model selection.\n4.3\nResults\n4.3.1\nBlack-box LLM Vs. CALM Finetuning:\nAcross three datasets and seven language models,\nwe compare black-box LLM finetuning with our\nproposed CALM approach. From Table 1, CALM\nperforms competitively with finetuning baseline.\nperformance drops by 0.02 and 0.03 AUC-PR on\nClinStructor and MIMIC dataset respectively. On\nthe LCD data, CALM performs on par with fine-\ntuning. Similar to NAMs, minor decrease in per-\nformance is an expected trade-off for interpretabil-\nity. Nonetheless, the trade-off is worthwhile, as\nCALM’s interpretability makes the model far more\nactionable than black-box predictions.\nWe evaluated the LCD dataset on only four mod-\nels, as its long clinical notes often exceed the token\nlimits of Phi-3.5-mini-instruct and MediPhi. The\nlarge context length also makes finetuning with\nQwen 8B and even Gemma-3 or MedGemma 4B\nchallenging. To address this, we used an alternate\nimplementation described in the appendix, which\nis mathematically identical to CALM.\nTakeaway: CALM performs competitively with\nblack-box finetuning but, with its interpretability,\nCALM predictions that are more actionable and\ntrustworthy.\n4.3.2\nCALM2erformance Improvements\nThe reduced performance of CALM arises from\nits strict additive constraint, which disallows inter-\nactions between components. To assess whether\npairwise interactions enhance predictive power, we\npropose CALM2 and compare it with CALM and\nblack-box finetuning using five representative mod-\nels on the MIMIC admission dataset.\nAs described in §3.3, CALM2 introduces two\n"}, {"page": 7, "text": "(a) Influential features of ClinStructor\n(b) Influential features of MIMIC Admission Notes\nFigure 1: Influential features: The influence score is defined as the absolute risk score averaged across the\npopulation. The figure shows the top influential features and their scores from Clinstructor and MIMIC, respectively.\n(a) Age\n(b) Mental Status\n(c) Respiratory Support\n(d) Admission Reason\nFigure 2: Feature values and corresponding risk scores: We select the top 20 most frequent feature values and,\namong them, choose 5 values—spanning the range of logit scores—to visualize feature logit function ℓi(xi).\nadditional hyperparameters: the inductive weight\nof interaction logits β and the rank R of the low-\nrank bilinear projection matrices. We test R ∈\n8, 16 and β ∈0.1, 0.5. Table 2 shows that CALM2\noutperforms CALM and further narrows the gap\nwith black-box finetuning.\nTakeaway: CALM2 improves performance while\nstill maintaining full interpretability.\n4.3.3\nImprovements with CALM-Distill\nAs described in § 3.4, we enhance CALM by\ndistilling dark knowledge from a teacher model.\nFor fair comparison, CALM, CALM-Distill, and\nthe teacher (black-box finetuned) model share the\nsame LLM backbone and training data. CALM-\nDistill introduces two additional hyperparame-\nters—temperature and the trade-off parameter α\n(weight of the KL loss). We fix the temperature at\n2 and test α ∈0.2, 0.4, 0.6. Table 3 shows that\nCALM-Distill consistently outperforms CALM\nand further reduces the gap with black-box finetun-\ning. Since only the optimization objective changes,\nCALM-Distill remains fully interpretable.\nTakeaway: CALM-Distill consistently improves\nover CALM while preserving full interpretability.\n5\nGlobal and Local Interpretability\nCALM provides inherent interpretability at both\nglobal (model-level) and local (patient-level) scales.\nIn this section, we illustrate both.\n5.1\nGlobal Interpretability\nFeature Importance Scores:\nFeature impor-\ntance scores quantify, across the population, how\neach input component contributes to the model’s\ndecision. For each patient n and input component\ni, CALM produces logit scores [ℓ(0)\ni,n, ℓ(1)\ni,n]. The\ndifference ℓ(1)\ni,n −ℓ(0)\ni,n represents the feature’s contri-\nbution toward label 1; a large positive value (≫0)\nindicates strong support for label 1, while a large\nnegative value (≪0) indicates strong support for\nlabel 0. The magnitude |ℓ(1)\ni,n −ℓ(0)\ni,n| reflects the\nstrength of influence, and its sign indicates direc-\ntion. The overall influence of a feature i is com-\nputed as the average magnitude of this difference\nacross all patients:\n"}, {"page": 8, "text": "(a) Patient 1: Risk scores\n(b) Patient 24: Risk scores\nFigure 3: Patient-level interpretability: We plot the top 5 and bottom 5 risk score features and their corresponding\nscores for two patients from ClinStructor—one who died and one who was discharged alive.\nInfluence(i) = 1\nN\nPN\nn=1 |ℓ(1)\ni,n −ℓ(0)\ni,n|\nFigure 1 reports the influence values for the top\n10 features (question–answers) in ClinStructor and\nall 8 sections in the MIMIC admission dataset using\nthe Qwen3-8B CALM model. Features such as\nAdmission Reason and Age are most influential for\nClinStructor, while Present Illness and Allergies\ndominate in MIMIC3.\nVisualization at Individual Feature Levels:\nA\nkey advantage of additive models such as NAMs\nand CALMs is the ability to directly visualize,\nfor each feature i, the learned function ℓi(xi) =\nF i\nlast(FT (Xi)) across input values. This enables\nan intuitive understanding of how variations in a\nsingle feature affect the model’s output. Figure 2 il-\nlustrates this for four features from the ClinStructor\nQwen3-8B model: Age, Mental Status, Respiratory\nSupport, and Admission Reason. For each feature,\nwe identify the 20 most frequent values, compute\ntheir risk scores ℓ(1)\ni,n −ℓ(0)\ni,n, and plot feature values\nagainst risk scores at the 0th, 25th, 50th, 75th, and\n100th percentiles.\nThese plots reveal clear patterns: risk increases\nwith age; patients who are Unresponsive have\nhigher risk than those Awake and alert; and admis-\nsions for Abdominal Pain correlates more higher\nrisk than for Hematemesis or Seizures. Interest-\ningly, they also reveal model’s weaknesses, such as\ndiffering risk estimates for semantically equivalent\ninputs like Room air and Room air (RA), indicating\nsensitivity to minor textual variations.\n5.2\nLocal Interpretability\nLocal interpretability explains how a feature con-\ntribute to a model’s prediction on a specific input.\nPatient-Level Interpretability.\nFor each patient,\nCALM provides feature-wise risk scores ℓ(1)\ni,n −ℓ(0)\ni,n.\nFigure 3 illustrates this for two patients from the\nClinStructor Qwen3-8B model—one who died and\none discharged alive. For each patient, we plot the\ntop five and bottom five contributing features based\non risk scores. For Patient 1, the model assigns\nhigher risk due to chronic conditions and GI bleed-\ning history, and lower risk due to substance use\nand heart rate. For Patient 24, higher risk arises\nfrom malignancy history and immunosuppressants,\nwhile substance use and neurologic deficits con-\ntribute to lower risk.\n6\nConclusion\nIn this work, we introduce CALM, an inherently in-\nterpretable framework for classification with semi-\nstructured text. By decomposing the overall pre-\ndiction into a sum of individual contributions,\nCALM provides faithful, component-level inter-\npretation while maintaining performance compa-\nrable to black-box classifiers.\nOur extensions,\nCALM2 and CALM-Distill, further narrow this\nperformance gap while preserving interpretabil-\nity. Through evaluations using seven LLMs across\nthree datasets, we demonstrate that CALM and its\nvariants achieve competitive performance and en-\nable transparent, model- and patient-level insights.\nCollectively, these results highlight CALM as a\ntrustworthy solution for high-stakes domains.\n"}, {"page": 9, "text": "7\nLimitations\nCALM enables us to interpret the model and un-\nderstand how its predictions are made. However,\nit does not provide any mechanism to modify or\ncorrect the model’s behavior. It is important to note\nthat the interpretations produced by CALM—such\nas feature importance—are specific to how the cur-\nrent model uses each feature, not to the inherent\nvalue or causal influence of the features themselves.\nFor instance, if a particular feature strongly con-\ntributes to a high-risk prediction, this does not im-\nply that the feature causes the high risk; rather,\nit indicates a strong correlation. As an example,\nif a certain medication is associated with higher\npredicted mortality risk, this does not mean the\nmedication increases mortality, it may simply be\nprescribed for severe conditions that carry higher\nrisk. In fact, the medication could even be protec-\ntive.\nEthical considerations: Although our model is\nnot designed for any specific application, it may be\ndeployed in sensitive domains such as healthcare\nand related areas. We strongly recommend that any\nimplementation of our method undergo thorough\nquality assurance and robustness evaluations prior\nto deployment in such settings.\nLicense: We use the PhysioNet datasets, which\nrequire credentialed access. All data and model\nusage in this work comply with their respective\nlicenses.\nReplicability: All source code and the complete\nset of hyperparameters used in our experiments will\nbe released publicly to facilitate transparency and\nreproducibility.\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian J.\nGoodfellow, Moritz Hardt, and Been Kim. 2018. San-\nity checks for saliency maps. In Neural Information\nProcessing Systems.\nRishabh Agarwal, Levi Melnick, Nicholas Frosst,\nXuezhou Zhang, Ben Lengerich, Rich Caruana, and\nGeoffrey E Hinton. 2021. Neural additive models:\nInterpretable machine learning with neural nets. Ad-\nvances in neural information processing systems,\n34:4699–4711.\nElaine Angelino, Nicholas Larus-Stone, Daniel Alabi,\nMargo Seltzer, and Cynthia Rudin. 2017. Learn-\ning certifiably optimal rule lists. In Proceedings of\nthe 23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, KDD ’17,\npage 35–44, New York, NY, USA. Association for\nComputing Machinery.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473.\nBrian Barr, Noah Fatsi, Leif Hancox-Li, Peter Richter,\nDaniel Proano, and Caleb Mok. 2023. The disagree-\nment problem in faithfulness metrics. arXiv preprint\narXiv:2311.07763.\nSamyadeep Basu, Philip Pope, and Soheil Feizi. 2020.\nInfluence functions in deep learning are fragile. arXiv\npreprint arXiv:2006.14651.\nUsha Bhalla, Suraj Srinivas, and Himabindu Lakkaraju.\n2023. Discriminative feature attributions: Bridging\npost hoc explainability and inherent interpretability.\nAdvances in Neural Information Processing Systems,\n36:44105–44122.\nRich Caruana, Yin Lou, Johannes Gehrke, Paul Koch,\nM. Sturm, and Noémie Elhadad. 2015. Intelligible\nmodels for healthcare: Predicting pneumonia risk\nand hospital 30-day readmission. Proceedings of\nthe 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining.\nChun-Hao Chang, Rich Caruana, and Anna Golden-\nberg. 2021. Node-gam: Neural generalized additive\nmodel for interpretable deep learning. arXiv preprint\narXiv:2106.01613.\nZhengping Che, Sanjay Purushotham, Robinder Khe-\nmani, and Yan Liu. 2017. Interpretable deep models\nfor icu outcome prediction. In AMIA annual sympo-\nsium proceedings, volume 2016, page 371.\nJean-Philippe Corbeil, Amin Dada, Jean-Michel At-\ntendu, Asma Ben Abacha, Alessandro Sordoni, Lu-\ncas Caccia, François Beaulieu, Thomas Lin, Jens\nKleesiek, and Paul Vozila. 2025.\nA modular ap-\nproach for clinical slms driven by synthetic data with\npre-instruction tuning, model merging, and clinical-\ntasks alignment. arXiv preprint arXiv:2505.10717.\nSanjoy Dasgupta, Nave Frost, and Michal Moshkovitz.\n2022. Framework for evaluating faithfulness of local\nexplanations. In International Conference on Ma-\nchine Learning, pages 4794–4815. PMLR.\nGerhard-Paul Diller, Aleksander Kempny, Sonya V\nBabu-Narayan, Marthe Henrichs, Margarita Brida,\nAnselm Uebing, Astrid E Lammers, Helmut Baum-\ngartner, Wei Li, Stephen J Wort, and 1 others. 2019.\nMachine learning algorithms estimating prognosis\nand guiding therapy in adult congenital heart disease:\ndata from a single tertiary centre including 10 019 pa-\ntients. European heart journal, 40(13):1069–1077.\nJames Enouen and Yan Liu. 2022. Sparse interaction ad-\nditive networks via feature interaction detection and\nsparse selection. Advances in Neural Information\nProcessing Systems, 35:13908–13920.\n"}, {"page": 10, "text": "Flavio Lopes Ferreira, Daliana Peres Bota, Annette\nBross, Christian Mélot, and Jean-Louis Vincent.\n2001.\nSerial evaluation of the sofa score to\npredict outcome in critically ill patients.\nJama,\n286(14):1754–1758.\nRuth C. Fong and Andrea Vedaldi. 2017. Interpretable\nexplanations of black boxes by meaningful perturba-\ntion. 2017 IEEE International Conference on Com-\nputer Vision (ICCV), pages 3449–3457.\nYoav Freund and Robert E. Schapire. 1997. A decision-\ntheoretic generalization of on-line learning and an\napplication to boosting. In European Conference on\nComputational Learning Theory.\nTrevor J Hastie. 2017. Generalized additive models.\nStatistical models in S, pages 249–307.\nSara Hooker, D. Erhan, Pieter-Jan Kindermans, and\nBeen Kim. 2018. A benchmark for interpretability\nmethods in deep neural networks. In Neural Informa-\ntion Processing Systems.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. In North American Chapter of the\nAssociation for Computational Linguistics.\nKarthikeyan K, Raghuveer Thirukovalluru, and David\nCarlson. 2025. ClinStructor: AI-Powered Structur-\ning of Unstructured Clinical Texts.\nKarthik Karthikeyan and Anders Søgaard. 2021. Revis-\niting methods for finding influential examples. CoRR,\nabs/2111.04683.\nPang Wei Koh and Percy Liang. 2017. Understanding\nblack-box predictions via influence functions. In\nInternational conference on machine learning, pages\n1885–1894. PMLR.\nKonstantina Kourou, Themis P Exarchos, Konstanti-\nnos P Exarchos, Michalis V Karamouzis, and Dim-\nitrios I Fotiadis. 2015. Machine learning applications\nin cancer prognosis and prediction. Computational\nand structural biotechnology journal, 13:8–17.\nHimabindu Lakkaraju, Stephen H. Bach, and Jure\nLeskovec. 2016. Interpretable decision sets: A joint\nframework for description and prediction. In Pro-\nceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, KDD ’16, page 1675–1684, New York, NY, USA.\nAssociation for Computing Machinery.\nSimon Lambden, Pierre Francois Laterre, Mitchell M\nLevy, and Bruno Francois. 2019.\nThe sofa\nscore—development, utility and challenges of ac-\ncurate assessment in clinical trials. Critical Care,\n23(1):374.\nMichael Larvin and MichaelJ Mcmahon. 1989. Apache-\nii score for assessment and monitoring of acute pan-\ncreatitis. The Lancet, 334(8656):201–205.\nYin Lou, Rich Caruana, Johannes Gehrke, and Giles\nHooker. 2013.\nAccurate intelligible models with\npairwise interactions. Proceedings of the 19th ACM\nSIGKDD international conference on Knowledge dis-\ncovery and data mining.\nScott M. Lundberg and Su-In Lee. 2017. A unified\napproach to interpreting model predictions. In Pro-\nceedings of the 31st International Conference on Neu-\nral Information Processing Systems, NIPS’17, page\n4768–4777, Red Hook, NY, USA. Curran Associates\nInc.\nSyed S Mahmood, Daniel Levy, Ramachandran S Vasan,\nand Thomas J Wang. 2014. The framingham heart\nstudy and the epidemiology of cardiovascular disease:\na historical perspective. The lancet, 383(9921):999–\n1008.\nRobert L McNamara, Kevin F Kennedy, David J Co-\nhen, Deborah B Diercks, Mauro Moscucci, Stephen\nRamee, Tracy Y Wang, Traci Connolly, and John A\nSpertus. 2016. Predicting in-hospital mortality in\npatients with acute myocardial infarction. Journal of\nthe American College of Cardiology, 68(6):626–635.\nMiquel Miró-Nicolau, Antoni Jaume-i Capó, and\nGabriel Moyà-Alcover. 2024. Assessing fidelity in\nxai post-hoc techniques: A comparative study with\nground truth explanations datasets. Artificial Intelli-\ngence, 335:104179.\nEdoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel\nGallagher, and Georg Groh. 2022. Shap-based expla-\nnation methods: a review for nlp interpretability. In\nProceedings of the 29th international conference on\ncomputational linguistics, pages 4593–4603.\nHarsha Nori, Samuel Jenkins, Paul Koch, and Rich\nCaruana. 2019.\nInterpretml:\nA unified frame-\nwork for machine learning interpretability. ArXiv,\nabs/1909.09223.\nNicolas Papernot and Patrick McDaniel. 2018. Deep\nk-nearest neighbors:\nTowards confident, inter-\npretable and robust deep learning. arXiv preprint\narXiv:1803.04765.\nMatthew Peroni, Marharyta Kurban, Sun Young Yang,\nYoung Sun Kim, Hae Yeon Kang, and Ji Hyun\nSong. 2022. Extending the neural additive model\nfor survival analysis with ehr data. arXiv preprint\narXiv:2211.07814.\nGarima Pruthi, Frederick Liu, Mukund Sundarara-\njan, and Satyen Kale. 2020.\nEstimating training\ndata influence by tracking gradient descent. ArXiv,\nabs/2002.08484.\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin.\n2016. “why should I trust you?”: Explaining the pre-\ndictions of any classifier. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Demon-\nstrations, pages 97–101, San Diego, California. As-\nsociation for Computational Linguistics.\n"}, {"page": 11, "text": "Cynthia Rudin. 2018. Stop explaining black box ma-\nchine learning models for high stakes decisions and\nuse interpretable models instead. Nature Machine\nIntelligence, 1:206 – 215.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nand 1 others. 2025. Medgemma technical report.\narXiv preprint arXiv:2507.05201.\nDylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh,\nand Himabindu Lakkaraju. 2020. Fooling lime and\nshap: Adversarial attacks on post hoc explanation\nmethods. In Proceedings of the AAAI/ACM Confer-\nence on AI, Ethics, and Society, pages 180–186.\nNiko Speybroeck. 2012. Classification and regression\ntrees. International Journal of Public Health, 57:243–\n246.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 3319–3328.\nJMLR.org.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, Alexandre Ramé, Morgane\nRivière, and 1 others. 2025. Gemma 3 technical\nreport. arXiv preprint arXiv:2503.19786.\nBerk Ustun and Cynthia Rudin. 2017. Optimized risk\nscores. In Proceedings of the 23rd ACM SIGKDD In-\nternational Conference on Knowledge Discovery and\nData Mining, KDD ’17, page 1125–1134, New York,\nNY, USA. Association for Computing Machinery.\nBerk Ustun, Stefano Tracà, and Cynthia Rudin. 2013.\nSupersparse linear integer models for interpretable\nclassification. arXiv: Machine Learning.\nBetty van Aken, Jens-Michalis Papaioannou, Manuel\nMayrdorfer, Klemens Budde, Felix Gers, and Alexan-\nder Loeser. 2021. Clinical outcome prediction from\nadmission notes using self-supervised knowledge in-\ntegration. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 881–893,\nOnline. Association for Computational Linguistics.\nJesse Vig. 2019.\nA multiscale visualization of at-\ntention in the transformer model.\narXiv preprint\narXiv:1906.05714.\nXiming Wen. 2024.\nLanguage model meets pro-\ntotypes: Towards interpretable text classification\nmodels through prototypical networks.\nPreprint,\narXiv:2412.03761.\nJiande Wu and Chindo Hicks. 2021. Breast cancer type\nclassification using machine learning. Journal of\npersonalized medicine, 11(2):61.\nShiyun Xu, Zhiqi Bu, Pratik Chaudhari, and Ian J\nBarnett. 2023. Sparse neural additive model: In-\nterpretable deep learning with feature selection via\ngroup sparsity. In Joint European Conference on\nMachine Learning and Knowledge Discovery in\nDatabases, pages 343–359. Springer.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nZebin Yang, Aijun Zhang, and Agus Sudjianto. 2021.\nGami-net: An explainable neural network based on\ngeneralized additive models with structured interac-\ntions. Pattern Recognition, 120:108192.\nCatherine Yeh, Yida Chen, Aoyu Wu, Cynthia Chen,\nFernanda Viégas, and Martin Wattenberg. 2023. At-\ntentionviz: A global view of transformer attention.\nIEEE Transactions on Visualization and Computer\nGraphics, 30(1):262–272.\nWonJin Yoon, Shan Chen, Yanjun Gao, Zhanzhan Zhao,\nDmitriy Dligach, Danielle S Bitterman, Majid Afshar,\nand Timothy Miller. 2025. Lcd benchmark: long clin-\nical document benchmark on mortality prediction for\nlanguage models. Journal of the American Medical\nInformatics Association, 32(2):285–295.\nMatthew D Zeiler and Rob Fergus. 2013. Visualizing\nand understanding convolutional networks. Preprint,\narXiv:1311.2901.\nBo Zhang, Huiping Shi, and Hongtao Wang. 2023. Ma-\nchine learning and ai in cancer prognosis, prediction,\nand treatment selection: a critical approach. Journal\nof multidisciplinary healthcare, pages 1779–1791.\n"}, {"page": 12, "text": "A\nAlternate Implementation of CALM\n(Packed Input)\nA.1\nPacked Concatenation with Per-Feature\nIsolation\nLet X = {X1, . . . , XM} with tokenizations Xi =\n[xi1, . . . , xi|Xi|]. Define the feature segment for the\ni-th input as\nseg(i) = Xi.\nThe packed input stream is the concatenation of all\nsegments:\nInput = concat\n\u0000seg(i)\n\u0001M\ni=1,\nwhere concat(·) denotes true sequence concatena-\ntion.\nBlock-Diagonal Attention Mask.\nLet Si be the\nset of token indices belonging to segment seg(i)\nwithin the packed stream. The attention mask en-\nforces isolation between features:\nAttnMask(u, v) =\n(\n1,\nif u, v ∈Si for some i,\n0,\notherwise.\nThis ensures that tokens from different features\ncannot attend to each other.\nPer-Feature Positional Re-Indexing.\nLet π(t)\ndenote the positional index at packed position t.\nFor any u ∈Si,\nπ(u) = rankSi(u) −1,\nso [START]i has position 0 and subsequent tokens\nare numbered sequentially within each segment,\nindependent of their absolute position in the packed\nstream.\nFeature Representations and Additive Logits.\nAfter the backbone forward pass:\nhi = hidden([EOS]i),\nℓi = F i\nlast(hi; W i\nlast) ∈RC.\nCALM combines the feature logits additively:\nz = 1\nM\nM\nX\ni=1\nℓi + b,\npc =\nexp(zc)\nPC\nj=1 exp(zj)\n.\nEquivalence to Direct CALM.\nSince attention is\nrestricted within each Si and positional indices are\nreset per segment, the backbone computes the same\nfunction for each Xi as in independent forward\npasses. Thus, reading hi at [EOS]i matches the\ndirect implementation exactly.\nA.2\nTime-Complexity Comparison\nLet Li = |Xi|, Lmax = maxi Li, and Ltot =\nPM\ni=1 Li. We consider the quadratic self-attention\ncost, omitting constants and lower-order terms.\n(1) Ideal CALM (Independent, No Padding).\nCost =\nM\nX\ni=1\nL2\ni .\nThis represents the theoretical lower bound, where\neach feature is processed independently without\npadding.\n(2) Practical CALM (Batched with Padding).\nCost = M · L2\nmax.\nHere, all features in the batch are padded to the\nsame maximum length Lmax, leading to compu-\ntational waste when feature lengths differ signifi-\ncantly.\n(3) Packed Implementation.\nIn the packed ap-\nproach, all features are concatenated into a sin-\ngle sequence of total length Ltot. With a standard\ndense attention kernel, the cost is\nCost =\n M\nX\ni=1\nLi\n!2\n.\nThis approach has two main properties:\n• It is always at least as large as the ideal cost\nP\ni L2\ni , since cross-feature interactions are\nmasked but still computed by the dense ker-\nnel.\n• It can be smaller than the padded batch cost\nML2\nmax when feature lengths vary widely, as\nno computation is wasted on padding.\nIf a block-sparse attention kernel is used that ex-\nploits the block-diagonal mask, the packed cost\nreduces exactly to the ideal P\ni L2\ni .\nA.3\nCompatibility with CALM2\nThe packed implementation produces identical fea-\nture representations hi and is fully compatible with\nCALM2.\nB\nComparison of CALM2 Vs. CALM:\nAll metrics\n"}, {"page": 13, "text": "Model\nRank, β\nCALM2\nFinetune\nAUC-PR\nF1\nAUC-ROC\nAUC-PR\nF1\nAUC-ROC\nQwen3-0.6B-Base\nRank = 8, β = 0.1\n0.447\n0.46\n0.838\n0.457\n0.471\n0.85\nRank = 8, β = 0.5\n0.445\n0.453\n0.841\nRank = 16, β = 0.1\n0.437\n0.462\n0.843\nRank = 16, β = 0.5\n0.464\n0.467\n0.845\nQwen3-1.7B-Base\nRank = 8, β = 0.1\n0.476\n0.48\n0.852\n0.494\n0.481\n0.859\nRank = 8, β = 0.5\n0.486\n0.484\n0.852\nRank = 16, β = 0.1\n0.475\n0.482\n0.854\nRank = 16, β = 0.5\n0.464\n0.461\n0.853\nQwen3-8B-Base\nRank = 8, β = 0.1\n0.504\n0.49\n0.864\n0.532\n0.506\n0.874\nRank = 8, β = 0.5\n0.482\n0.49\n0.856\nRank = 16, β = 0.1\n0.485\n0.494\n0.857\nRank = 16, β = 0.5\n0.503\n0.498\n0.861\nPhi-3.5-mini-instruct\nRank = 8, β = 0.1\n0.480\n0.487\n0.86\n0.522\n0.506\n0.87\nRank = 8, β = 0.5\n0.462\n0.474\n0.85\nRank = 16, β = 0.1\n0.473\n0.484\n0.86\nRank = 16, β = 0.5\n0.476\n0.484\n0.857\ngemma-3-4b-pt\nRank = 8, β = 0.1\n0.487\n0.487\n0.859\n0.51\n0.49\n0.868\nRank = 8, β = 0.5\n0.5\n0.494\n0.863\nRank = 16, β = 0.1\n0.487\n0.491\n0.856\nRank = 16, β = 0.5\n0.492\n0.491\n0.857\nTable 4: Comparison of CALM2 Vs. CALM.\nC\nComparison of CALM-Distill Vs.\nCALM: All metrics\nD\nInitialization of CALM model from\nFully Finetuned Model\nE\nClinStructor Questions ANd Influence\nScores\nF\nHyperparameter Settings\nconfigs = {\n\"C1\": {\"lr\": 1e-4, \"rank\": 8, \"alpha\": 16, \"dropout\": 0.05},\n\"C2\": {\"lr\": 1e-4, \"rank\": 8, \"alpha\": 32, \"dropout\": 0.05},\n\"C3\": {\"lr\": 1e-4, \"rank\": 16, \"alpha\": 16, \"dropout\": 0.05},\n\"C4\": {\"lr\": 1e-4, \"rank\": 16, \"alpha\": 32, \"dropout\": 0.05},\n\"C5\": {\"lr\": 2e-4, \"rank\": 8, \"alpha\": 16, \"dropout\": 0.05},\n\"C6\": {\"lr\": 2e-4, \"rank\": 16, \"alpha\": 32, \"dropout\": 0.05},\n\"C7\": {\"lr\": 5e-4, \"rank\": 8, \"alpha\": 32, \"dropout\": 0.05},\n\"C8\": {\"lr\": 5e-4, \"rank\": 16, \"alpha\": 16, \"dropout\": 0.05},\n}\n"}, {"page": 14, "text": "Table 5: Comparison of CALM-Distill Vs. CALM.\nModel\nTemperature α\nCALM-Distill\nFinetune\nAUC-PR\nF1\nAUC-ROC\nAUC-PR\nF1\nAUC-ROC\nQwen3-0.6B-Base\n0.2\n0.454\n0.466\n0.847\n0.457\n0.471\n0.85\n0.4\n0.445\n0.463\n0.848\n0.6\n0.45\n0.469\n0.854\nQwen3-1.7B-Base\n0.2\n0.479\n0.472\n0.857\n0.494\n0.481\n0.859\n0.4\n0.486\n0.488\n0.862\n0.6\n0.477\n0.482\n0.853\nQwen3-8B-Base\n0.2\n0.512\n0.508\n0.868\n0.532\n0.506\n0.874\n0.4\n0.517\n0.5\n0.87\n0.6\n0.517\n0.506\n0.867\nPhi-3.5-mini-instruct\n0.2\n0.486\n0.488\n0.858\n0.522\n0.506\n0.87\n0.4\n0.495\n0.488\n0.865\n0.6\n0.503\n0.505\n0.868\ngemma-3-4b-pt\n0.2\n0.505\n0.494\n0.864\n0.51\n0.49\n0.868\n0.4\n0.504\n0.487\n0.872\n0.6\n0.524\n0.497\n0.876\nModel\nCALM−FTInit\nAUC-PR\nF1\nAUC-ROC\nInitialize Transformer Weights\nQwen3-0.6B-Base\n0.299\n0.329\n0.753\nQwen3-1.7B-Base\n0.39\n0.401\n0.808\nQwen3-8B-Base\n0.408\n0.406\n0.815\nInitialize Transformer and Classifier Weights\nQwen3-0.6B-Base\n0.273\n0.348\n0.74\nQwen3-1.7B-Base\n0.376\n0.379\n0.787\nQwen3-8B-Base\n0.37\n0.392\n0.797\nTable 6: Comparison of Initialization from Fully Model: Initializing from Finetuned models makes it worse\n"}, {"page": 15, "text": "ID\nQuestion\nID\nQuestion\n1\nwhat is the patient’s age?\n2\nwhat chronic medical conditions does the patient have?\n3\nwhat is the patient’s blood pressure on admission?\n4\nwhat is the patient’s heart rate on admission?\n5\nwhat is the patient’s reported history of tobacco, alcohol,\nor illicit drug use?\n6\nwhat is the patient’s respiratory rate on admission?\n7\nwhat is the primary reason for the patient’s current ad-\nmission?\n8\nwhat medications is the patient currently taking on ad-\nmission?\n9\nwhat is the patient’s temperature on admission?\n10\nwhat is the patient’s left ventricular ejection fraction?\n11\nwhat is the patient’s oxygen saturation on admission?\n12\nwhat is the patient’s current mental status or level of\nconsciousness?\n13\nwhat type of respiratory support is the patient currently\nreceiving?\n14\nwhat is the patient’s gender?\n15\nwhat is the patient’s baseline creatinine level?\n16\nwhat chronic cardiovascular conditions does the patient\nhave?\n17\nwhat is the patient’s history of malignancy, including\ntype and treatment status?\n18\nwhat is the patient’s living situation and social support\nstructure?\n19\nwhat antiplatelet or anticoagulant medications is the pa-\ntient currently prescribed?\n20\nwhat is the patient’s white blood cell count on admis-\nsion?\n21\nwhat is the patient’s current oxygen saturation and the\namount of supplemental oxygen required?\n22\nwhat are the patient’s known drug allergies?\n23\nwhat is the patient’s history of hypertension?\n24\nwhat was the primary reason for the patient’s intubation?\n25\nwhat is the patient’s history of diabetes mellitus, includ-\ning type?\n26\nfrom what type of facility was the patient transferred?\n27\nwhat are the key findings from the abdominal physical\nexamination?\n28\nwhat is the patient’s history of myocardial infarction?\n29\nwhat is the reported severity of mitral regurgitation?\n30\nwhat chronic respiratory conditions does the patient\nhave?\n31\nwhat is the patient’s body mass index (bmi)?\n32\nwhat significant family medical history is reported?\n33\nwhat is the patient’s inr on admission?\n34\nwhat specific neurological deficits are identified during\nthe physical examination?\n35\nhow is the patient’s general appearance described in the\nphysical exam?\n36\nwhat is the patient’s current renal replacement therapy\nstatus?\n37\nwhat is the patient’s employment status?\n38\nwhat immunosuppressive medications is the patient cur-\nrently taking?\n39\nwhat is the extent and location of any edema noted on\nphysical examination?\n40\nwhat is the patient’s lactate level?\n41\nwhat are the key findings from the patient’s respiratory\nphysical examination?\n42\nwhat physical exam findings indicate the patient’s hydra-\ntion status?\n43\nwhat was the mechanism of injury?\n44\nwhat was the patient’s initial arterial blood gas ph?\n45\nwhat is the patient’s history of gastrointestinal bleeding?\n46\nwhat is the patient’s history and current status regarding\ncongestive heart failure?\n47\nwhat is the patient’s hematocrit level on admission?\n48\nwhat is the patient’s primary cardiac diagnosis?\n49\nwhat is the patient’s troponin level?\n50\nwhat is the patient’s history of coronary artery disease?\nTable 7: 50 Questions used in the ClinSructor Dataset\n"}, {"page": 16, "text": "Figure 4: ClinStructor Influence Scores of ALl 50 features\n"}, {"page": 17, "text": "(a) Feature 1\n(b) Feature 2\n(c) Feature 3\n(d) Feature 4\n(e) Feature 5\n(f) Feature 6\n(g) Feature 7\n(h) Feature 8\n(i) Feature 9\n(j) Feature 10\n(k) Feature 11\n(l) Feature 12\n(m) Feature 13\n(n) Feature 14\n(o) Feature 15\n(p) Feature 16\n(q) Feature 17\n(r) Feature 18\n(s) Feature 19\n(t) Feature 20\nFigure 5: Top 20 feature-level risk trends. Each panel shows the learned logit function ℓi(xi) for one of the top 20\nmost influential features, illustrating how model risk varies with feature value. The plots are ordered by overall\nimportance.\n"}]}