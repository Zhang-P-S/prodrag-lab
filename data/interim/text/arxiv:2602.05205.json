{"doc_id": "arxiv:2602.05205", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.05205.pdf", "meta": {"doc_id": "arxiv:2602.05205", "source": "arxiv", "arxiv_id": "2602.05205", "title": "Aligning Large Language Model Behavior with Human Citation Preferences", "authors": ["Kenichiro Ando", "Tatsuya Harada"], "published": "2026-02-05T02:02:43Z", "updated": "2026-02-05T02:02:43Z", "summary": "Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\\%$ relative to humans) and sentences containing personal names (by $-20.1\\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.05205v1", "url_pdf": "https://arxiv.org/pdf/2602.05205.pdf", "meta_path": "data/raw/arxiv/meta/2602.05205.json", "sha256": "5b29c8242e24ec94b05c222fa17f0250143b784aad49025e7092406b80f9e7e7", "status": "ok", "fetched_at": "2026-02-18T02:19:45.123080+00:00"}, "pages": [{"page": 1, "text": "ALIGNING LARGE LANGUAGE MODEL BEHAVIOR\nWITH HUMAN CITATION PREFERENCES\nKenichiro Ando1,2, Tatsuya Harada2,1\n1RIKEN AIP 2The University of Tokyo\nkenichiro.ando@riken.jp\nABSTRACT\nMost services built on powerful large-scale language models (LLMs) add cita-\ntions to their output to enhance credibility. Recent research has paid increasing\nattention to the question of what reference documents to link to outputs. How-\never, how LLMs recognize cite-worthiness and how this process should be con-\ntrolled remains underexplored. In this study, we focus on what kinds of content\nLLMs currently tend to cite and how well that behavior aligns with human prefer-\nences. We construct a dataset to characterize the relationship between human cita-\ntion preferences and LLM behavior. Web-derived texts are categorized into eight\ncitation-motivation types, and pairwise citation preferences are exhaustively eval-\nuated across all type combinations to capture fine-grained contrasts. Our results\nshow that humans most frequently seek citations for medical text, and stronger\nmodels display a similar tendency. We also find that current models are as much\nas 27% more likely than humans to add citations to text that is explicitly marked\nas needing citations on sources such as Wikipedia, and this overemphasis reduces\nalignment accuracy. Conversely, models systematically underselect numeric sen-\ntences (by −22.6% relative to humans) and sentences containing personal names\n(by −20.1%), categories for which humans typically demand citations. Further-\nmore, experiments with Direct Preference Optimization demonstrate that model\nbehavior can be calibrated to better match human citation preferences. We expect\nthis study to provide a foundation for more fine-grained investigations into LLM\ncitation preferences.\n1\nINTRODUCTION\nLarge language models (LLMs) possess extensive knowledge about the world and have the poten-\ntial to fundamentally transform human society. In practice, powerful generative models such as\nthe GPT series and Gemini are already deployed across a variety of downstream platforms, mak-\ning the assurance of factuality and verifiability an important issue that affects many services. One\napproach to improving the verifiability of LLM outputs is to add citations. This is, through some\nworkflow, external documents are attached to support the content generated by the LLM in the form\nof references. Most current high-performing closed models employ this functionality, and systems\nsuch as the GPT series(OpenAI, 2025), Gemini(Comanici et al., 2025), Claude(Anthropic, 2025),\nQwen(Yang et al., 2025), and Perplexity1 present citations to users.\nThe relationship between LLMs and citations has also drawn attention in recent research. In par-\nticular, the question of which documents should be linked to a given piece of text has been actively\nstudied alongside RAG and AI-agent technologies, yielding many results. This line of work aligns\nwell with the common LLM pipeline in which the system receives a user query, performs web\nsearch, and generates an answer based on information retrieved from the web (Google).\nBy contrast, the question of what information ought to receive a citation—that is, the importance\nof citations conditioned on the text content—has not been sufficiently investigated. Such research\nwould enable citation behavior that aligns with user preferences and, crucially, would make it pos-\nsible to control citation frequency. Too few citations undermine users’ ability to verify claims and\n1https://www.perplexity.ai\n1\narXiv:2602.05205v1  [cs.CL]  5 Feb 2026\n"}, {"page": 2, "text": "erode trust (Ding et al., 2025), whereas too many citations can reduce satisfaction, harm efficiency,\nand lower decision accuracy (De Jong, 2010; Eppler & Mengis, 2004), thereby degrading the user\nexperience. In other words, a balance between verifiability and user experience is essential; rather\nthan exhaustively presenting every source for a model’s output, citations should focus on the most\nimportant information.\nIn this work, we address the open questions of whether LLMs align with users’ citation preferences\nand whether models can be trained to do so. In today’s high-performing LLM services, key deci-\nsions—such as which information to web-search and where in the output to attach citations—are\noften controlled by the LLM itself (Google; Anthropic). Thus, if we can teach LLMs users’ citation\npreferences, we can adapt the citation behavior of AI-agent systems to match user needs.\nTo this end, we first conducted human annotation to investigate users’ citation preferences. The\ndata consisted of 6,000 Wikipedia sentences with quality labels, carefully annotated by Wikipedia\neditors. We grouped the quality labels into eight categories and, for each pair of categories, an-\nnotated—both with human preferences and with LLM preferences—which side should receive a\ncitation (pairwise comparison). Our analysis showed that stronger LLMs exhibit higher alignment\nwith human preferences. We also found that for sentences labeled “Citation needed” on Wikipedia,\nLLMs selected them at rates up to 19.5% higher for open models and up to 27.4% higher for closed\nmodels than humans did, indicating a strong influence from training data. Conversely, models sys-\ntematically underselect sentences containing numbers (by up to −22.6% relative to humans) and\nsentences containing personal names (by up to −20.1%), precisely the categories for which humans\ntypically demand citations. Next, we trained LLMs using Direct Preference Optimization (DPO)\non the human-annotated data to align them with human preferences. As a result, we achieved an\nimprovement of 11.8% and demonstrated that the influence of training data such as Wikipedia can\nbe mitigated.\nOur contributions are as follows.\n• We present, to our knowledge, the first study that focuses on the need for citations within\ntext and examines citation preferences of both humans and LLMs.\n• We construct a dataset of 6,000 sentences with human citation-preference labels across\neight content-based categories.\n• Through dataset analysis, we show that LLMs are strongly influenced by their training data,\nwhich in part leads to divergences from human preferences.\n• We demonstrate that DPO can attenuate the impact of training data and bring model behav-\nior closer to human citation preferences.\n2\nRELATED WORKS\nRecent work has explored many ways of combining LLMs with citations (L´ala et al., 2023; Gao\net al., 2023). Examples include datasets for self-querying RAG methods in which the model decides\nat generation time whether external search is needed and, if so, retrieves and integrates paragraph-\nlevel evidence (Asai et al., 2024); tasks that require explicit source attribution in ambiguous QA\n(Shaier et al., 2024); and benchmarks that evaluate sentence-level citation in long-context QA\n(Zhang et al., 2024a).\nThere is also a line of work that evaluates the validity of citations (Li et al., 2024)—including studies\nthat assess citation validity in the legal domain (Zhang et al., 2024b), datasets that test whether\ncitations in generative search substantiate answers (Liu et al., 2023), and investigations of citation\nappropriateness in the medical domain (Wu et al., 2025). However, these efforts primarily focus on\nattaching an appropriate citation to a given sentence.\nCloser to our work, there are a few studies that ask which content should receive citations. These\ninclude analyses of reasons for adding citations on Wikipedia (Redi et al., 2019) and careful ex-\naminations of citation intent in scientific papers (Wright & Augenstein, 2021; Saxena et al., 2024;\nCohan et al., 2019). Such studies remain relatively scarce, and most target academic writing. Our\ninterest lies in whether LLMs exhibit the same citation preferences as humans—and whether they\ncan be aligned to do so—rather than in an exhaustive characterization of human citation preferences.\n2\n"}, {"page": 3, "text": "Table 1: Categories and brief descriptions of sentence labels for citation preference data, reorga-\nnized from Wikipedia inline templates.\nCategory\nBrief description\nMissing Information\nWho?\nContains claims that do not identify individuals.\nWhen?\nTime period is so vague or ambiguous.\nWhich?\nReferences to organizations or other things are vague.\nWhere?\nContains no specific place at which an event took place.\nSic\nSic\nTextual error in the statement is copied exactly from the source.\nDoubt\nDubious\nSourced statement, but that seems dubious or unlikely.\nDisputed\nStatement whose truth or factual is in dispute by editors.\nVague\nVague\nContains vague words or statement.\nWeasel words\nContains weasel words.\nAmbiguous\nContains ambiguous phrases.\nPOV\nNeutrality disputed\nStatement seemed to be biased.\nUnbalanced opinion?\nStatement may express a non-neutral point of view.\nMedical Content\nMedical citation needed\nUnsourced medical/health claim requiring citation.\nJargon\nJargon\nOverly jargonistic and too technical statement.\nExpand acronym\nAcronym/initialism should be expanded.\nUnclear\nClarification needed\nHard to understand; needs clarification.\nIncomprehensible\nContains incomprehensible text.\n3\nTASK DEFINITION\nLet X be the set of sentences and let K = {1, . . . , 8} be the set of content categories. Each sentence\nx ∈X is assigned a category via a mapping g : X →K.\nA comparison item is a pair\np ∈{ (xa, xb) | g(xa) ̸= g(xb) }.\n(1)\nThat is, the two sentences come from distinct categories in K. We balance the construction of pairs\nacross unordered category pairs\n{ka, kb} ⊆K,\nka ̸= kb,\n(2)\nso that each of the\n\u00008\n2\n\u0001\n= 28 category combinations is comparably represented.\n4\nDATA CREATION\n4.1\nDATA SOURCE\nWe require a collection of sentences annotated with quality labels as the data for our study. We\nadopt Wikipedia’s Inline templates2—sentence-level tags that editors apply when some aspect of\nthe content is problematic. These templates come in many varieties and are actively used across\narticles. Because inline templates tend to be used by relatively experienced editors, they serve as\nreasonably reliable signals.\nIn this study, we extract target sentences from the WikiSQE dataset curated from Wikipedia (Ando\net al., 2024). This large-scale collection contains 3.4M sentences labeled with 153 categories. Al-\n2https://en.wikipedia.org/wiki/Category:Inline_templates\n3\n"}, {"page": 4, "text": "Table 2: Human preference matrix (%). Each cell is the share of judgments choosing the row\ncategory over the column category.\nCategory\nInfo\nSic\nDoubt\nVague\nPOV\nMed\nJarg\nUncl\nInfo\n–\n50.5\n58.4\n43.1\n45.4\n38.5\n48.9\n42.0\nSic\n49.5\n–\n49.5\n52.8\n51.2\n40.4\n43.3\n42.9\nDoubt\n41.6\n50.5\n–\n48.9\n56.0\n38.5\n53.5\n51.7\nVague\n56.9\n47.2\n51.1\n–\n42.2\n24.1\n43.8\n41.2\nPOV\n54.6\n48.8\n44.0\n57.8\n–\n42.2\n52.5\n41.6\nMed\n61.5\n59.6\n61.5\n75.9\n57.8\n–\n57.3\n66.3\nJarg\n51.1\n56.7\n46.5\n56.2\n47.5\n42.7\n–\n53.5\nUncl\n58.0\n57.1\n48.3\n58.8\n58.4\n33.7\n46.5\n–\nthough minimal noise filtering has already been applied, we further improve quality by manually\nidentifying and removing broken sentences during annotation.\n4.2\nCATEGORY GROUPING\nWe extract labels related to human citation preferences and reorganize them into eight categories.\nTo this end, the authors reviewed all labels, selected 19 of them, and regrouped them into the fol-\nlowing eight categories: Missing Information, Sic, Doubt, Vague, POV, Medical Content, Jargon,\nand Unclear. Details of the label–category mapping are shown in Table 1. Missing Information\nindicates that required details are absent; Sic flags typographical errors; Doubt marks statements of\nquestionable veracity; Vague indicates imprecise or ambiguous wording; POV flags non-neutral or\none-sided claims; Medical Content marks health/medical statements; Jargon flags highly technical\nor jargon-heavy wording; and Unclear indicates text that is difficult to understand.\n4.3\nANNOTATION\nWe annotated citation preferences using the collected dataset. We sampled 6,000 sentences to con-\nstruct the annotation set: 750 sentences were drawn uniformly from each of the eight categories and\npaired to form 3,000 comparison items (sentence pairs). Pairs were balanced across all\n\u00008\n2\n\u0001\n= 28\ncategory combinations; that is, we created approximately 107 sentence pairs per category pair. Dur-\ning annotation, non-sentential items were flagged, two items per batch were duplicated for quality\ncontrol, and we retained only submissions that were fully consistent on the duplicated items.\nIn total, 402 participants annotated the 3,000 pairs. Each pair is assigned to exactly one annotator\nand evaluated only once. However, if a pair fails the quality check, it is released and reassigned to\na different worker. We identified 404 pairs containing at least one non-sentence and removed them,\nyielding a final dataset of 2,596 pairs. Table 2 reports the human selection rates between categories.\nMedical content sentences won broadly across categories, most notably against Vague (75.9%) and\nUnclear (66.3%), indicating a strong user preference to secure verifiability for medically conse-\nquential content. Unclear and Jargon were often favored or competitive, suggesting that citations\nare expected to serve as an “anchor of meaning” for hard-to-understand or highly technical sen-\ntences. Moreover, Vague exceeded Missing Information at 56.9% and Unclear exceeded Missing\nInformation at 58.0%, implying a tendency to prioritize readability and clarity with citations before\nfilling in missing details.\n5\nCITATION PREFERENCE OF LLMS\n5.1\nSETUP\nWe evaluate LLMs using the dataset we constructed to study citation preferences. To cover a broad\nrange, we include both open- and closed-source models as well as small and large models. Specif-\nically, we consider 11 models: Mistral Small, Mistral Large, Llama 1B, Llama 3B, Llama 70B,\nDeepSeek Chat, GPT-5, Claude Sonnet 4, CommandR+, Gemini 2.5 Flash, and Qwen Max. For\nfurther details, see Appendix B. We attempted to collect outputs for all prompts; however, some\n4\n"}, {"page": 5, "text": "Table 3: Agreement rates between models and humans by category (%). The agreement rate is the\nprobability that a model’s chosen sentence matches the human choice.\nModel\nInfo\nSic\nDoubt\nVague\nPOV\nMed\nJarg\nUncl\nAvg\nLlama 1B\n50.2\n48.7\n49.2\n50.4\n49.1\n50.9\n52.6\n48.7\n50.0\nLlama 3B\n56.0\n54.5\n54.2\n59.6\n59.1\n59.5\n53.6\n53.5\n56.3\nLlama 70B\n61.9\n61.2\n61.0\n64.4\n58.7\n65.6\n59.8\n60.3\n61.6\nMistral small\n56.2\n57.8\n54.6\n55.8\n57.8\n60.0\n55.2\n60.8\n57.3\nMistral large\n55.8\n61.2\n61.7\n58.1\n58.3\n61.4\n59.8\n60.7\n59.6\nGPT-5\n60.7\n59.8\n61.5\n61.7\n59.2\n63.6\n60.1\n63.0\n61.2\nGemini\n56.8\n58.9\n58.3\n59.2\n59.5\n65.4\n57.6\n61.3\n59.6\nClaude\n59.8\n61.0\n62.0\n62.7\n59.6\n64.7\n61.3\n61.3\n61.5\nDeepseek\n64.0\n61.1\n61.3\n61.6\n61.8\n67.1\n61.9\n62.6\n62.7\nQwen\n62.2\n63.9\n63.7\n62.2\n58.9\n66.2\n59.0\n63.2\n62.4\nCommandR+\n55.9\n54.4\n58.9\n56.8\n55.4\n59.1\n55.6\n56.1\n56.5\nmodels refused to answer certain items due to safety or political restrictions, resulting in up to three\nmissing samples per model. Moreover, open and closed LLMs may differ in how they handle cita-\ntions. For open LLMs, everything hinges on the training data and training strategy, whereas closed\nLLMs may, in addition to the LLM itself, employ various tools or agent-like procedures. However,\nin our setting all closed LLMs are accessed via APIs that do not invoke any external components\nbeyond the LLM itself. Therefore, this concern does not apply here.\n5.2\nANALYSIS OF MODEL PREFERENCE\nTable 3 presents the models’ citation preferences. DeepSeek (62.7%) attains the highest overall per-\nformance, followed by Qwen (62.4%), Llama-70B (61.6%), Claude (61.5%), and GPT-5 (61.2%),\nrevealing a clear parameter–scale effect. This pattern is especially pronounced within the Llama\nfamily; in particular, Llama 1B sits at the random baseline, implying that at this scale it effec-\ntively fails to produce meaningful citation preferences. Moreover, small LLMs are known to exhibit\npronounced option–position biases in multiple-choice settings (Li & Gao, 2025), which likely con-\ntributes to this behavior.\nAcross categories, MEDICAL shows relatively high agreement with humans, suggesting that LLMs\nare comparatively adept at seeking citations for medical information. An interesting open question\nis at which stage of training this capability is acquired.\nNevertheless, aggregate scores plateau around 60%, indicating that current models only weakly\npredict human preferences and that the capability remains insufficient. These results suggest that\ninferring cite-worthiness implicitly from pretraining text alone is highly nontrivial—even for large,\nwell-trained models—and remains a challenging task.\n5.3\nRELATIONSHIP BETWEEN INFORMATION AND CITATION PREFERENCES\nWe investigate what kinds of information drive the citation preferences of models versus humans\nfrom three perspectives.\nCitation needed\nFirst, we examine sentences labeled “Citation needed.” On Wikipedia, this is the\nlabel editors attach to mark that a citation is required, and it is among the most frequently applied\ninline templates. Because this label is common on Wikipedia, it is likely to appear frequently in\nLLM training data as well. If models are not explicitly debiased with respect to citation behavior,\nwe hypothesize that sentences bearing this label will be judged as requiring citations with high\nprobability. (Note that “Medical citation needed” is a subtype of “Citation needed,” and we include\nit in our analysis.)\nThe results are shown in Table 4a. With the exception of Llama 1B, all models select “Citation\nneeded” substantially more often than humans. In particular, Llama 70B and DeepSeek exceed the\nhuman rate by more than +25%. This suggests a strong imprint of training-data biases that hinders\nalignment with human preferences. By contrast, models with similarly large parameter counts such\n5\n"}, {"page": 6, "text": "Table 4: Selection rates for citation-worthiness across evaluators and sentence types. “Rate” is\nthe probability that the evaluator selected that sentence type; “vs Human” indicates the percentage\nchange relative to the human selection rate. Bold numbers indicate the most pronounced values.\n(a) Citation needed\nEvaluator\nRate (%)\nvs Human\nHuman (Reference)\n58.7\n–\nLlama 1B\n52.9\n-9.9%\nLlama 3B\n66.1\n+12.6%\nLlama 70B\n74.8\n+27.4%\nMistral small\n64.5\n+9.9%\nMistral large\n61.2\n+4.3%\nGPT-5\n68.6\n+16.9%\nGemini\n70.0\n+19.3%\nClaude\n70.1\n+19.5%\nDeepseek\n73.5\n+25.3%\nQwen\n68.6\n+16.9%\nCommandR+\n60.4\n+2.9%\n(b) Numeric sentences\nEvaluator\nRate (%)\nvs Human\nHuman (Reference)\n61.9\n–\nLlama 1B\n49.7\n-19.8%\nLlama 3B\n55.3\n-10.8%\nLlama 70B\n57.7\n-6.9%\nMistral small\n48.0\n-22.6%\nMistral large\n56.7\n-8.4%\nGPT-5\n61.6\n-0.6%\nGemini\n55.8\n-9.9%\nClaude\n61.0\n-1.5%\nDeepseek\n57.3\n-7.5%\nQwen\n59.6\n-3.9%\nCommandR+\n57.6\n-7.0%\n(c) Sentences with person names\nEvaluator\nRate (%)\nvs Human\nHuman (Reference)\n51.7\n–\nLlama 1B\n49.7\n-3.9%\nLlama 3B\n45.2\n-12.7%\nLlama 70B\n41.3\n-20.1%\nMistral small\n43.9\n-15.1%\nMistral large\n45.4\n-12.1%\nGPT-5\n48.5\n-6.2%\nGemini\n42.5\n-17.8%\nClaude\n51.2\n-1.0%\nDeepseek\n42.9\n-17.0%\nQwen\n47.4\n-8.3%\nCommandR+\n50.2\n-3.0%\nas Mistral Large and CommandR+ stay within +5%, which hints that some corrective design in data\nor training strategy may be at play. Given that Llama 1B is near the random baseline throughout our\nstudy, we do not discuss it further.\nNumeric sentences\nSecond, we examine sentences containing numeric expressions. Such sen-\ntences include dates and quantitative expressions, which demand high precision and leave little\nroom for error. When precision is required, users are expected to rely on external sources to ver-\nify factuality. Indeed, prior work reports that users deem inline citations necessary for sentences\ncontaining statistics (Redi et al., 2019). Accordingly, we hypothesize that quantitative expressions\nincrease users’ demand for citations. We detect numeric sentences via simple pattern matching.\nThe results are shown in Table 4b. All models exhibit lower selection rates than humans. Notably,\nMistral Small is 22.6% below the human rate. This indicates that models do not yet adequately cap-\nture users’ citation preferences in this setting and that little specialized training for citation behavior\nhas been conducted. Meanwhile, models such as GPT-5 and Claude show near-human alignment\n(within −1.5%), again suggesting that some targeted design choices during training may contribute.\nPerson names\nThird, we examine sentences containing personal names. When people are men-\ntioned, particular care is required regarding factuality. Wikipedia explicitly flags this sensitivity 3,\n3https://en.wikipedia.org/wiki/Wikipedia:Biographies_of_living_persons\n6\n"}, {"page": 7, "text": "Table 5: Results after applying DPO on the annotation data. Percent change relative to the non-fine-\ntuned baseline (%). We observe an average increase in performance.\nModel\nInfo\nSic\nDoubt\nVague\nPOV\nMed\nJarg\nUncl\nAvg\nLlama 1B\n3.2\n8.6\n19.3\n14.7\n6.9\n15.9\n12.5\n14.0\n11.8\nLlama 3B\n7.0\n9.4\n20.5\n6.9\n3.4\n1.7\n10.5\n10.7\n9.1\nLlama 70B\n-5.2\n-2.6\n-3.1\n-6.1\n2.0\n-5.3\n4.5\n2.0\n-1.6\nMistral small\n2.8\n-6.1\n0.9\n2.5\n7.1\n-3.2\n1.4\n-6.2\n-0.2\nMistral large\n12.4\n8.2\n7.9\n16.2\n2.7\n9.9\n11.9\n9.1\n9.7\nand biographies are tightly governed in practice. Because this caution is also widely recognized\nsocially, we hypothesize that users similarly demand citations in such contexts.\nThe results are shown in Table 4c. As with “Numeric sentences,” all models undershoot human\nselection rates. In particular, Llama 70B is 20.1% below the human rate. As before, this pro-\nvides evidence that models insufficiently capture users’ citation preferences. That said, Claude and\nCommandR+ align comparatively well with human behavior, consistent with the possibility of ad-\nditional targeted design. A notable pattern is that most models fall below 50%, i.e., they often judge\nthat a citation is not required in these cases.\n6\nALIGNING LLMS WITH HUMAN CITATION PREFERENCES\nSince the previous sections established that current LLMs are not aligned with users’ citation pref-\nerences, we attempt to align them by training the models. If agreement with humans improves, this\nwould constitute evidence that the models’ citation-related behavior has been aligned. We consider\nfive target models: Llama 1B, Llama 3B, Llama 70B, Mistral Small, and Mistral Large. For training\nand testing, we split the dataset constructed in Section 4 into training and test sets of approximately\nequal size. The split is performed so that category pairs are balanced; when an odd count occurs, the\nextra items are assigned to the training set. In addition, we randomly sample 100 instances from the\ntraining portion as a validation set. As a result, the train/validation/test sets contain 1,206, 100, and\n1,288 pairs, respectively. Further training details are provided in Appendix C.\n6.1\nDPO\nAs a first approach, we perform DPO. DPO is a commonly used method for preference optimization\nand appears to be the most suitable approach for our objective. We use LoRA (Hu et al., 2022), which\nenables parameter-efficient training of LLMs with minimal loss in accuracy. For large models, we\nenable DeepSpeed ZeRO-Offload (Ren et al., 2021) to offload optimizer states and gradients to CPU\nmemory.\nThe results are shown in Table 5. Overall, performance increased. The gain is particularly pro-\nnounced for Llama 1B, whose agreement improves by 11.8% relative to its no-DPO counterpart.\nLlama 3B and Mistral Large also improve by roughly 9%, indicating that DPO is effective. By\ncontrast, Mistral Small drops slightly, and Llama 70B declines by 1.6%. On average, we observe a\n5.76% improvement, supporting DPO as a viable strategy for teaching models citation preferences.\n7\nCONCLUSION\nWe studied how LLMs decide where to add citations and how closely these decisions align with hu-\nman preferences. Using 5,192 Wikipedia sentences reorganized into eight content-based categories\nand annotated via pairwise comparisons, we found that current models align with human citation\npreferences only weakly (in the low 60s on average), with systematic divergences by sentence type.\nIn particular, models substantially overselect sentences explicitly marked “Citation needed,” while\nunderselecting numeric and person-name sentences, both of which humans tend to treat as citation-\nworthy. Medical content emerges as the most consistently prioritized category across humans and\nmodels.\n7\n"}, {"page": 8, "text": "We then examined whether alignment can be improved through training. DPO yielded consistent\ngains, with a mean improvement of 5.76% and especially large benefits for smaller models. These\nresults indicate that preference-based training provides an effective mechanism for calibrating cite-\nworthiness judgments. Furthermore, our experiments demonstrate that LLM citation preferences\ncan be controlled via DPO.\nImplications.\nFirst, citation behavior should be explicitly trained and evaluated rather than as-\nsumed to emerge from pretraining. Second, deployment should consider category-aware routing\n(e.g., boosting numeric and person-name triggers) and frequency control to balance verifiability\nagainst information overload. Third, because many modern systems let the LLM implicitly gov-\nern search and attachment decisions, improving cite-worthiness alignment at the model level can\ndirectly benefit agentic pipelines.\nLimitations and future work.\nOur analysis is bounded by Wikipedia-derived labels and English-\nonly annotations; extending to other domains, languages, and high-risk classes (e.g., legal and scien-\ntific claims) is important. In addition, using larger-scale annotated datasets will enable a more rigor-\nous examination of causality. Finally, integrating cite-worthiness prediction with retrieval/reranking\nand evaluating end-to-end user outcomes (task success, trust, cognitive load) remain open directions.\nTo facilitate reproducibility, we plan to release the data and code upon acceptance of the paper.\n8\nETHICS STATEMENT\nThe dataset we constructed includes opinions and descriptions originating from Wikipedia that may\ncarry bias. Some items may contain offensive or discriminatory language. However, our objective\nin this study is to learn citation preferences including such real-world biases. We plan to release the\ndataset, with clear notices at distribution time that it may contain biased content.\nWe monitored annotators’ working time and ensured that their pay always exceeded £6 per hour.\nWe also paid annotators in full even if they failed the quality-control checks.\nWe used GPT-5 and Claude Code as AI tools; they assisted with searching and organizing prior\nwork, checking text, and supporting code creation.\n9\nREPRODUCIBILITY STATEMENT\nTo facilitate reproducibility, we plan to release the data and code upon acceptance of the paper.\nREFERENCES\nKenichiro Ando, Satoshi Sekine, and Mamoru Komachi. Wikisqe: A large-scale dataset for sentence\nquality estimation in wikipedia. Proceedings of the AAAI Conference on Artificial Intelligence,\n2024.\nAnthropic.\nCitations.\nURL\nhttps://docs.claude.com/en/docs/\nbuild-with-claude/citations(accessed25September2025).\nAnthropic. System card: Claude Opus 4 & Claude Sonnet 4. May 2025.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection. 2024.\nArman Cohan, Waleed Ammar, Madeleine Van Zuylen, and Field Cady. Structural scaffolds for\ncitation intent classification in scientific publications. arXiv preprint arXiv:1904.01608, 2019.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin\nGaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-\nJiang Jiang, Krishna Haridasan, and 3291 authors. Gemini 2.5: Pushing the frontier with advanced\nreasoning, multimodality, long context, and next generation agentic capabilities. arXiv, 2025.\n8\n"}, {"page": 9, "text": "Ton De Jong. Cognitive load theory, educational research, and instructional design: Some food for\nthought. Instructional science, 2010.\nYifan Ding, Matthew Facciani, Ellen Joyce, Amrit Poudel, Sanmitra Bhattacharya, Balaji Veera-\nmani, Sal Aguinaga, and Tim Weninger. Citations and trust in llm generated responses. Proceed-\nings of the AAAI Conference on Artificial Intelligence, 2025.\nMartin J Eppler and Jeanne Mengis. The concept of information overload: A review of literature\nfrom organization science, accounting, marketing, mis, and related disciplines. The information\nsociety, 2004.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to gen-\nerate text with citations. Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 2023.\nGoogle. Grounding with Google Search. URL https://ai.google.dev/gemini-api/\ndocs/google-search(accessed25September2025).\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022.\nJakub L´ala, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and\nAndrew D White. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv\npreprint arXiv:2312.07559, 2023.\nRuizhe Li and Yanjun Gao. Anchored answers: Unravelling positional bias in GPT-2’s multiple-\nchoice questions, 2025.\nYifei Li, Xiang Yue, Zeyi Liao, and Huan Sun. Attributionbench: How hard is automatic attribution\nevaluation?, 2024.\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848, 2023.\nOpenAI. GPT-5 system card. August 2025.\nMiriam Redi, Besnik Fetahu, Jonathan Morgan, and Dario Taraborelli. Citation needed: A taxonomy\nand algorithmic assessment of wikipedia’s verifiability. The world wide web conference, 2019.\nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Min-\njia Zhang, Dong Li, and Yuxiong He.\n{Zero-offload}: Democratizing {billion-scale} model\ntraining. 2021 USENIX Annual Technical Conference (USENIX ATC 21), 2021.\nYash Saxena, Deepa Tilwani, Ali Mohammadi, Edward Raff, Amit Sheth, Srinivasan Parthasarathy,\nand Manas Gaur. Attribution in scientific literature: New benchmark and methods. arXiv preprint\narXiv:2405.02228, 2024.\nSagi Shaier, Ari Kobren, and Philip Ogren. Adaptive question answering: Enhancing language\nmodel proficiency for addressing knowledge conflicts with source citations.\narXiv preprint\narXiv:2410.04241, 2024.\nDustin Wright and Isabelle Augenstein. Citeworth: Cite-worthiness detection for improved scientific\ndocument understanding. arXiv preprint arXiv:2105.10912, 2021.\nKevin Wu, Eric Wu, Kevin Wei, Angela Zhang, Allison Casasola, Teresa Nguyen, Sith Riantawan,\nPatricia Shi, Daniel Ho, and James Zou. An automated framework for assessing how well llms\ncite relevant medical references. Nature Communications, 2025.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\n9\n"}, {"page": 10, "text": "Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report. arXiv, 2025.\nJiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou,\nYuxiao Dong, Ling Feng, et al. Longcite: Enabling llms to generate fine-grained citations in\nlong-context qa. arXiv preprint arXiv:2409.02897, 2024a.\nKepu Zhang, Weijie Yu, Sunhao Dai, and Jun Xu. Citalaw: Enhancing llm with citations in legal\ndomain. arXiv preprint arXiv:2412.14556, 2024b.\n10\n"}, {"page": 11, "text": "A\nADDITIONAL DETAILS ON DATASET CREATION\nIn WikiSQE, the same sentence may appear under multiple labels. For pairwise construction we\navoid pairing identical sentences with themselves; however, if duplicates occur across different cat-\negories, this is not problematic for our task, whereas duplicates within the same category group\nare filtered out. When assembling the data, we sample within each category so that counts are as\nbalanced as possible across labels; if a label is underrepresented, we compensate by sampling from\nlabels with larger pools. Although malformed sentences were discarded during annotation, the num-\nber of pairs per category after filtering is reported in Table 6. Finally, although LLMs are known\nto exhibit option–position bias (Li & Gao, 2025), in our setup the correct option is not fixed to a\nparticular position, so this bias is not a concern.\nTable 6: Head-to-head pair counts by citation category after filtering.\nGroup\nInfo\nSic\nDoubt\nVague\nPOV\nMed\nJarg\nUncl\nTotal\nInfo\n–\n111\n120\n127\n117\n124\n114\n107\n820\nSic\n111\n–\n115\n109\n107\n117\n112\n114\n785\nDoubt\n120\n115\n–\n114\n117\n107\n120\n105\n798\nVague\n127\n109\n114\n–\n96\n113\n112\n115\n786\nPOV\n117\n107\n117\n96\n–\n134\n116\n120\n807\nMed\n124\n117\n107\n113\n134\n–\n111\n115\n821\nJarg\n114\n112\n120\n112\n116\n111\n–\n107\n792\nUncl\n107\n114\n105\n115\n120\n115\n107\n–\n783\nB\nMODELS\nThe models used for training are as follows.\nFor GPT-5 we use the model snap-\nshot as of September 1,\n2025;\nfor Claude we use claude-sonnet-4-20250514;\nfor Gemini we use gemini-2.5-flash as of September 1, 2025;\nfor Qwen we use\nqwen-max-2025-01-25;\nfor DeepSeek we use deepseek-chat as of September 1,\n2025; for CommandR+ we use c4ai-command-r-plus-08-2024; for Llama 70B we use\nllama3-3-70b-instruct; for Llama 3B we use llama3-2-3b-instruct; for Llama 1B\nwe use llama3-2-1b-instruct; for Mistral Small we use mistral-small-2402; and for\nMistral Large we use mistral-large-2402.\nC\nTRAINING DETAILS\nThe hyperparameters used during training are listed in Table 7. Training is terminated once the\nbest epoch on the validation data is reached, after which we proceed to inference on the test set.\nTo obtain high-quality responses, the decoding temperature is fixed at 1.0 for all models. During\ntraining, prompts are constructed following the official Mistral and Llama instruction templates.\n11\n"}, {"page": 12, "text": "Table 7: Training configurations, optimization settings, and compute resources used in our experi-\nments.\n(a) LoRA (Low-Rank Adaptation) Configuration\nParameter\nValue\nLoRA Rank (r)\n16\nLoRA Alpha\n32\nLoRA Dropout\n0.05\nTarget Modules\nq proj, v proj, k proj, o proj\ngate proj, up proj, down proj\nBias Training\nnone\nTask Type\nCAUSAL LM\n(b) DeepSpeed ZeRO-3 Configuration (CPU Offload)\nParameter\nValue\nZeRO Stage\n3\nOverlap Communication\nTrue\nContiguous Gradients\nTrue\nOffload Parameters\ncpu (pin memory=True)\nOffload Optimizer\ncpu (pin memory=True)\nReduce Bucket Size\n5e7\nStage3 Prefetch Bucket Size\n5e7\nStage3 Param Persistence Thresh.\n1e6\nGather 16-bit on Save\nTrue\nGradient Clipping\n1.0\nBF16 Training\nEnabled\nSteps per Print\n0\nWall-clock Breakdown\nFalse\n(c) DPO Training Hyperparameters\nParameter\nValue\nPer-device Batch Size\n1\nGradient Accumulation\n8\nNum Train Epochs\n20\nLearning Rate\n2e-5\nLR Scheduler\ncosine\nWarmup Ratio\n0.05\nMax Prompt Length\n770\nMax Sequence Length\n1026\nGradient Clipping\n1.0\nMixed Precision\nBF16\nGradient Checkpointing\nTrue\nBeta (DPO)\n0.1\nLabel Smoothing\n0.0\n(d) Compute Infrastructure\nResource\nConfiguration\nGPU Type\nA100-80GB\nNumber of GPUs\n8\nGPU Memory per Device\n80GB\nMixed Precision\nBF16\nDistributed Training\nDeepSpeed ZeRO-3\n12\n"}]}