{"doc_id": "arxiv:2601.00216", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.00216.pdf", "meta": {"doc_id": "arxiv:2601.00216", "source": "arxiv", "arxiv_id": "2601.00216", "title": "From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark", "authors": ["Jinning Zhang", "Jie Song", "Wenhui Tu", "Zecheng Li", "Jingxuan Li", "Jin Li", "Xuan Liu", "Taole Sha", "Zichen Wei", "Yan Li"], "published": "2026-01-01T05:20:54Z", "updated": "2026-01-01T05:20:54Z", "summary": "In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.00216v1", "url_pdf": "https://arxiv.org/pdf/2601.00216.pdf", "meta_path": "data/raw/arxiv/meta/2601.00216.json", "sha256": "18dc5fc0842f6d06562e88d4026c91062625080b80c99a1d2617a0ac29267650", "status": "ok", "fetched_at": "2026-02-18T02:23:31.720567+00:00"}, "pages": [{"page": 1, "text": "From Evidence-Based Medicine to Knowledge\nGraph: Retrieval-Augmented Generation for\nSports Rehabilitation and a Domain Benchmark\nJinning Zhang1,2†, Jie Song1,2†, Wenhui Tu‡1,2, Zecheng Li‡1,2,\nJingxuan Li3, Jin Li4, Xuan Liu5, Taole Sha7, Zichen Wei6,\nYan Li1,2*\n1Beijing Key Laboratory of Sports Performance and Skill Assessment,\nBeijing Sport University, Beijing, 100084, China.\n2Department of Exercise Physiology, School of Sport Science, Beijing\nSport University, Beijing, 100084, China.\n3School of Sport Medicine and Physical Therapy, Beijing Sport\nUniversity, Beijing, 100084, China.\n4Rehabilitation Center, Beijing Rehabilitation Hospital, Beijing,\n100144, China.\n5Optum Care Washington, Everett, Washington, 98201, USA.\n6School of Management, Beijing Sport University, Beijing, 100084,\nChina.\n7Department of Statistics and Actuarial Science, The University of\nHong Kong, Hong Kong, China.\n*Corresponding author(s). E-mail(s): bsuliyan@bsu.edu.cn;\n†These authors contributed equally to this work.\nAbstract\nIn medicine, large language models (LLMs) commonly leverage retrieval-\naugmented generation (RAG) to ground outputs in up-to-date external evidence\nand reduce hallucinations. However, current RAG approaches focus primarily on\nperformance improvements while overlooking evidence-based medicine (EBM)\nprinciples. This study addresses two key gaps: (1) the lack of PICO alignment\nbetween queries and retrieved evidence, and (2) the absence of evidence hierarchy\nconsiderations during reranking. We present a reusable adaptation strategy for\nintegrating EBM into graph-based RAG, integrating the PICO framework into\n1\narXiv:2601.00216v1  [cs.CL]  1 Jan 2026\n"}, {"page": 2, "text": "knowledge graph construction and retrieval, and proposing a Bayesian-inspired\nreranking algorithm to calibrate ranking scores by evidence grade without intro-\nducing predefined weights. We validated this framework in sports rehabilitation,\na literature-rich domain currently lacking RAG systems and benchmarks. We\nreleased a knowledge graph (357,844 nodes and 371,226 edges) and a reusable\nbenchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819\nanswer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy.\nIn a 5-point Likert evaluation, five expert clinicians rated the system 4.66–\n4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment.\nThese findings demonstrate that the proposed EBM adaptation strategy sub-\nstantially improves retrieval and answer quality and is readily transferable to\nother clinical domains; while the released resources help address the scarcity of\nRAG datasets in sports rehabilitation.\nKeywords: Large language models, Evidence-based medicine, Knowledge graph,\nRetrieval-augmented generation, Sports rehabilitation\n1 Introduction\nConsider the following question posed to a large language model (LLM): “My child\nhas congenital heart disease and has just undergone surgery. How should we conduct\npostoperative exercise rehabilitation?” As of 2024, the evidence base for rehabilita-\ntion in children with congenital heart disease remains limited, consisting primarily of\nobservational studies[1, 2], with authoritative guidelines only recently emerging[3]. In\nsuch cases, LLMs often provide outdated or suboptimal answers due to their training\ndata cutoff[4].\nIn medicine, clinical evidence evolves rapidly. For example, the latest American\nCollege of Sports Medicine (ACSM) edition adopts the metabolic chronotropic reserve\n(MCR) to assess whether the heart rate response during exercise is appropriate[5],\nreplacing the traditional 220 minus age formula, which recent studies have shown can\ncarry significant errors[6, 7]. The ACSM has also updated exercise prescriptions for\nspecial populations (e.g., transgender individuals, older adults, and children), rehabili-\ntation strategies for chronic diseases, and predictive equations[5]. Retrieval-Augmented\nGeneration (RAG) is designed to incorporate such time-sensitive information, dynam-\nically extending LLM knowledge to prevent obsolescence and reduce hallucinations[8].\nIn brief, RAG comprises three stages: the Retriever searches user-curated corpora in\na vector database; the Augmenter filters, reranks, and injects results into the model’s\ncontext; and the Generator synthesizes retrieved information to produce answers with\ncited sources[8]. In medicine, the need for traceable evidence and timely information\nmakes RAG a widely adopted approach for enhancing LLM reliability[9, 10].\nWith the rapid advancement of artificial intelligence (AI), RAG methods have\nbeen evolving rapidly. The seminal RAG framework by Lewis et al. employed a vector\n‡ These authors contributed equally as second authors.\n2\n"}, {"page": 3, "text": "retriever to recall the top-K text chunks from a knowledge base, which were concate-\nnated with the user query and fed into an LLM for answer generation[8]. In 2021, Luan\net al. proposed a hybrid retrieval strategy within RAG that significantly improved\nthe reliability of downstream tasks[11]. Specifically, this strategy fuses sparse retrieval\n(e.g., keyword-based methods) with dense retrieval (e.g., semantic embeddings) at the\nrecall stage, thereby enhancing retrieval performance. Subsequently, domain-specific\nRAG systems for medicine began to emerge. In 2023, Shi et al. introduced MKRAG,\nthe first to incorporate a structured medical knowledge base, and conducted system-\natic evaluations that established a baseline for subsequent medical RAG research[12].\nThe following year, RAG research began to introduce domain-specific methodological\ninnovations for medicine. For example, the RAG2 framework by Sohn et al. leverages\nintermediate reasoning steps generated by the LLM during answer generation to iter-\natively retrieve supporting evidence, thereby reducing hallucinations and enabling a\ntraceable evidence chain[13]. Additionally, MRD-RAG by Sun et al. combines disease\ninformation trees with pseudo-medical-history indexing to develop the first multi-turn\ndiagnostic RAG system, which substantially improves diagnostic accuracy and serves\nas an early prototype of Agentic RAG in medicine[14].\nReturning to the original user question, the patient is a child with congenital\nheart disease—a special population. Recently evolved RAG frameworks typically focus\non performance improvements without explicitly accounting for query-specific target\npopulations[15–18], potentially generating population-mismatched answers (especially\nwhen limited corpus evidence leads to insufficient recall[19]). Crucially, most evaluation\nmetrics cannot identify population mismatches. For example, semantic similarity uses\ncosine similarity to quantify differences between generated and reference answers at a\nmacro level, making general phrasing far more influential than specific details; answer\nfaithfulness measures only factual alignment between the answer and retrieved context,\nignoring relevance to the query—retrieving passages about adults with congenital\nheart disease would not affect this metric[20]. Another key issue is that even if RAG\nincorporates the latest authoritative guidelines for the rehabilitation of children with\ncongenital heart disease, these sources would still be weighted the same as earlier\nscattered observational evidence during retrieval[21], which violates the concept of a\nhierarchy of evidence[22]. The two issues above can be summarized as follows: Current\nmedical RAG approaches neglect the evidence-based medicine (EBM) framework[23].\nIn particular, the Population–Intervention–Comparator–Outcome (PICO) framework\nand the hierarchy of evidence remain largely overlooked.\nRegarding the combination of EBM and RAG, especially the integration of the\nPICO framework, graph-based retrieval-augmented generation (GraphRAG) shows\npromising potential. This paradigm leverages knowledge graphs constructed from\nentity-relation-attribute triples to provide a structured hierarchy for the corpus[24].\nOriginally, it’s designed to enhance the multihop reasoning capabilities of LLMs for\ncomplex questions, which are particularly important in medical contexts[25]. However,\nthis highly structured arrangement can also implicitly guide LLMs to retrieve from\nnodes aligned with the query, thereby potentially improving PICO alignment between\nquestions and answers. Based on this, we propose the key question of this study:\n3\n"}, {"page": 4, "text": "• How can the EBM framework be adapted to RAG pipelines, particularly\nGraphRAG?\nSince the pioneering work of Edge et al.[24], GraphRAG has evolved along two main\ntrajectories: graph retrieval and graph construction[26]. The former focuses on improv-\ning retrieval efficiency and relevance, while the latter aims to enrich graph topology\nand enable multi-granularity retrieval. Recently, Youtu-GraphRAG, proposed by Dong\net al.[26], unifies graph retrieval and graph construction through schema-bounded\nagentic extraction. This strategy reduces extraction noise during graph construction\nwhile enabling rapid domain adaptation through schema replacement. Essentially, it\nshifts the implicit retrieval constraints of traditional GraphRAG to a new paradigm\nof explicitly constrained retrieval by predefining the schema. In the medical domain,\nwhen we directly define the schema as PICO-related entity types, PICO mismatch\nbetween queries and retrieved chunks may be substantially reduced (Fig. 1). Accord-\ningly, this study aims to address EBM adaptation based on the Youtu-GraphRAG\nframework. In particular, although the field of sports rehabilitation is resource-rich,\nno domain-specific RAG system or reusable benchmark currently exists. We therefore\nfocus on this domain, curating a corpus, constructing a benchmark, and validating\nour framework.\nIn summary, the main contributions of this study are as follows:\n• We curate high-quality corpora covering 21 common conditions in sports rehabil-\nitation along with domain-relevant general guidelines, and construct a knowledge\ngraph comprising 357,844 nodes and 371,226 relationships.\n• We propose a Bayesian-inspired reranking algorithm that calibrates the rank-\ning scores of a cross-encoder by incorporating both semantic and evidence-grade\ninformation of candidate chunks without introducing predefined weights, enabling\nexplicit consideration of evidence hierarchy during reranking.\n• We present Sports Rehabilitation RAG (SR-RAG), which integrates the EBM\nframework into GraphRAG pipeline through a generalizable adaptation strategy,\nwith sports rehabilitation as the validation domain.\n• We develop a reusable benchmark for sports rehabilitation comprising 1,637 QA\npairs and conduct systematic evaluation of SR-RAG through automated metrics\nand sampled expert clinician review, both confirming system reliability.\n2 Results\n2.1 Knowledge Graph Construction\nFollowing corpus preprocessing and evidence-grade annotation, we constructed a\nknowledge graph for sports rehabilitation based on the Youtu-GraphRAG frame-\nwork. The schema for nodes, relations, and attributes was replaced with PICO-related\nterms to explicitly encode evidence-based query structure during graph construc-\ntion(Supplementary Table S1) . The final knowledge graph consisted of 21 types of\nsports rehabilitation disease corpora, together with general cross-disease guidelines,\ncomprising a total of 357,844 nodes and 371,226 edges. Of all nodes, 44,033 (12.3%)\nwere core medical entities directly aligned with the PICO framework. Among these,\n4\n"}, {"page": 5, "text": "Fig. 1 Evolution from traditional RAG to graph-based RAG and the proposed EBM-adapted\nGraphRAG framework integrating evidence hierarchy and the PICO framework.\nIntervention nodes were most prevalent (13,866; 31.5%), followed by Condition\n(9,979; 22.7%), Outcome (8,609; 19.5%), and Population (6,838; 15.5%). Arm, Device,\nand Comparator accounted for 2,134 (4.9%), 1,569 (3.6%), and 1,038 (2.4%), respec-\ntively. The graph data are available at https://huggingface.co/datasets/Ning311/\nsr-rag-knowledge-graph.\n2.2 Bayesian Evidence Tier Reranking\nTo simultaneously reflect semantic relevance and the evidence hierarchy of EBM dur-\ning reranking, while avoiding the limited interpretability and transferability caused\nby manually predefined weights, we proposed Bayesian Evidence Tier Reranking\n(BETR). This algorithm utilized a Bayesian maximum a posteriori (MAP) estimation\nparadigm to learn bias scores among evidence grades, which were then used to cali-\nbrate the reranking scores of candidate chunks. Specifically, the calibrator constructed\npositive–negative pairs (d+, d−) from candidate chunks and modeled the probability\np(d+ ≻d−| θ) that the positive example ranks above the negative in pairwise logis-\ntic form. The influence of evidence grades was captured through a set of learnable\n5\n"}, {"page": 6, "text": "Fig. 2 Training and validation objective curves for the ordered-grade pairwise calibrator.\ngrade bias scores ut. Model parameters were learned by maximizing the log-posterior,\nwhich includes both the pairwise log-likelihood and a shrinkage prior. For detailed\nalgorithmic steps, see the Methods section.\nWe partitioned the benchmark into a training set (n = 1310) and a validation\nset (n = 327), used for parameter fitting and hyperparameter selection, respectively.\nAfter fixing the candidate generation and negative sampling strategies, we constructed\n26,755 pairwise samples (train=21,546; val=5,209). As shown in Fig. 2, both the\ntraining and validation objectives (negative log loss) decreased monotonically with\nepochs and converged at approximately 60–80 epochs. The gap between training and\nvalidation curves remained small, with no evident overfitting.\nThe training results were listed in Table 1. The grade biases ut learned by BETR\nwere strictly monotonically decreasing, consistent with EBM principles. The spacing\nδ between adjacent grades was approximately constant (≈0.129), indicating simi-\nlar penalty magnitudes across grades under the current data distribution, with no\nextreme biases. Finally, the semantic score scale parameter a = 1.0348 indicated min-\nimal rescaling of the reranker score, and the final online ranking score was still mainly\ndriven by semantic relevance, with grade bias affecting only candidates of compara-\nble semantic similarity, which aligned with our aim. Additional training details and\nhyperparameters were provided in Supplementary Table S3.\n2.3 Case Study\nSR-RAG adapts Youtu-GraphRAG to accommodate medical domain tasks. Specif-\nically, modifications focus on three key components: (1) PICO-guided Hypothetical\nDocument Embeddings (HyDE): generates hypothetical documents with PICO soft\n6\n"}, {"page": 7, "text": "Table 1 Learned grade bias parameters of BETR.\nGrade\nut\nodds ratio\n∆from prev.\nA\n0.0000\n1.0000\n–\nB\n-0.1287\n0.8792\n0.1287\nC\n-0.2575\n0.7729\n0.1288\nD\n-0.3863\n0.6797\n0.1288\nE\n-0.5151\n0.5974\n0.1288\nconstraints and fuses them with dense and graph retrieval; (2) Evidence-grade-aware\nretrieval: leverages the grade biases ut learned by BETR to weight candidate chunks\nby evidence grade, given that all literature types are manually annotated during cor-\npus collection. Guideline evidence is retrieved separately from other evidence, forming\na dual-track retrieval strategy; (3) Two-stage reranking: employs ColBERT for coarse\nranking followed by a cross-encoder for fine ranking, improving robust matching of\nmedical synonyms, abbreviations, and numeric contexts within a controlled computa-\ntional budget. In this section, we selected a representative question as a case study\nand used actual SR-RAG logs to demonstrate the full pipeline. The LLM used was\nDeepSeek-V3. Fig. 3 provides an overview of the case study workflow.\n7\n"}, {"page": 8, "text": "Fig. 3 Case-study workflow of SR-RAG, illustrating PICO-guided HyDE, graph retrieval, dual-track\nevidence recall, reranking with BETR, and structured output.\n8\n"}, {"page": 9, "text": "Query Case. We selected the following question: “In cardiopulmonary rehabilitation\nprograms, under what circumstances should clinicians choose high-intensity interval\ntraining (HIIT) over moderate-intensity continuous training (MICT) for prepubertal\npatients with congenital heart disease?” This question exhibits a clear PICO structure,\nrequires cross-evidence integration and conditional reasoning, making it well suited as\na case study.\nQuery Decomposer. This module follows the schema-enhanced decomposition\nparadigm of Youtu-GraphRAG: under schema constraints, complex queries are decom-\nposed into three subproblem types—node-level retrieval, triple-level matching, and\ncommunity-level verification—each aligned with the corresponding level of the hierar-\nchical knowledge tree constructed during graph building[26]. In this case, we replaced\nthe schema with PICO-related phrases; the resulting subproblems and retrieval paths\nwere shown in Fig. 3.\nPICO-guided HyDE. HyDE leverages LLMs to generate hypothetical documents\nrelevant to a query as retrieval intermediaries, bridging the lexical gap between queries\nand evidence texts for retrieval augmentation[27]. In this study, we incorporated\nPICO soft constraints into HyDE prompts: the model preferentially reuses extractable\nP/I/C/O/T anchors from the query, tolerates missing fields, but is prohibited from\nhallucinating values for missing fields, which reduces hallucination and retrieval drift\nrisks. In this case, the LLM generated three hypothetical documents (Fig. 3), con-\nverting the query into declarative hypothetical answers. This approach bridges the\nsemantic gap between the query and ground truth evidence, thereby increasing their\ncosine similarity. Two key design principles apply: (1) many queries do not fully cover\nall five P/I/C/O/T dimensions, so generation should not be enforced for missing fields;\n(2) generating specific hypothetical values risks irrelevant value matching and should\nbe avoided.\nEBM-Adaptive Retrieval Strategy. SR-RAG’s adaptation to the evidence hierar-\nchy is reflected in two mechanisms: dual-track retrieval and BETR-calibrated ranking\nafter candidate merging. During data preprocessing, the corpus was mapped to evi-\ndence grades A–E: A = guidelines, B = systematic reviews and meta-analyses, C =\nrandomized controlled trials (RCTs), D = cohort studies, and E = other studies (case\nreports, narrative reviews, observational studies). Considering that Grade A accounts\nfor a small proportion of the overall corpus and is easily diluted by Grades B–E, we\nran separate candidate generation processes on the Grade A corpus and the Grades\nB–E corpus with their own recall quotas at the retrieval stage, and then merged the\ncandidate sets. BETR calibration (see Methods section) was then applied to the rank-\ning scores for the merged candidates, ensuring that while semantic relevance remains\ndominant, higher grades were prioritized when the relevance is comparable.\nThree-channel Retrieval Fusion. SR-RAG combines the ranking results from three\nretrieval channels using Reciprocal Rank Fusion (RRF)[28]. For any candidate window\nd, the RRF score is defined as:\nRRF(d) =\nX\nc∈{Dense,Graph,HyDE}\n1\nk + rankc(d),\n(1)\n9\n"}, {"page": 10, "text": "where rankc(d) is the rank of window d in channel c , and k is a smoothing constant.\nAfter fusion, the pipeline follows Youtu-GraphRAG’s parallel retrieval strategy (Entity\nMatching, Triple Matching, Community Filtering) to generate candidate windows.\nIterative Reasoning and Reflection (IRCoT). This module follows the Agentic\nRetriever paradigm of Youtu-GraphRAG, enabling schema-guided iterative reasoning\nand reflection. In this case, IRCoT performed three iterations. An example iteration\nwas shown in Fig. 3.\nTwo-Stage Cascaded Reranking Strategy. We adopted a two-stage reranking\napproach cascading ColBERT (mxbai-edge-colbert-v0) with a cross-encoder (BGE-\nreranker-v2-m3): ColBERT applies the MaxSim mechanism to coarse-rank fused\ncandidate windows and filters to top-K[29]; the cross-encoder then performs fine-\ngrained ranking within top-K to capture global semantic and logical relationships.\nBETR calibration was then applied to the cross-encoder logit scores to produce the\nfinal ranking score:\nr(q, d) = ˆa s(q, d) + ˆuGrade(d),\n(2)\nwhere s(q, d) denotes the semantic score and ˆuGrade(d) the learned grade bias (see\nMethods).\nQuota-Based Final Selection. For final selection, we adopted a soft quota strategy:\nif Grade A candidates have sufficiently high scores, at least one Grade A window\nis retained; otherwise, selection defaults to the global top-K without enforcement.\nAdditionally, when the candidate pool is smaller than the quota target, all available\nitems are returned to avoid padding with low-quality evidence. In this case, two key\nwindows entered the top-2, replacing candidates with potential population mismatch\nrisks (Fig. 3).\nStructured Output. The first paragraph concisely summarizes the core answer; the\nsecond elaborates on guidelines and supporting evidence; the third compares the query\nand response, highlighting limitations (Fig. 3).\n2.4 Results of automated evaluation and ablation studies\nWe conducted an end-to-end evaluation of SR-RAG using 1,637 benchmark queries,\nreporting four metrics: nugget coverage, answer faithfulness, semantic similarity, and\nPICOT match accuracy. We ran the same retrieval pipeline across Baichuan-M2, GPT-\n4o, and DeepSeek-V3, and performed ablation experiments on the best-performing\nmodel, DeepSeek-V3. All results were shown in Table 2. The benchmark is publicly\navailable at https://huggingface.co/datasets/Ning311/sr-rag-benchmark.\nModel performance differences. Among the three models, DeepSeek-V3 performed\nbest overall, followed by GPT-4o (which achieved the highest answer faithfulness),\nwith Baichuan-M2 ranking lowest. This may reflect architectural improvements in\nDeepSeek-V3. Although Baichuan-M2 is a medically specialized model, its relatively\nsmall parameter count (32B, compared to 671B for DeepSeek-V3) limits its capacity\nfor the complex multi-step reasoning required by SR-RAG.\nAblation studies. We performed three ablations on DeepSeek-V3: (1) w/o HyDE:\ndisables the PICO-guided HyDE channel, retaining only graph and dense retrieval;\n(2) w/o ColBERT: removes ColBERT coarse ranking, using only the cross-encoder\nfor reranking; (3) w/o PICO-extended schema: replaces the graph construction and\n10\n"}, {"page": 11, "text": "retrieval schema with generic medical terms unrelated to PICO, to assess the contri-\nbution of the PICO-extended schema to query–answer alignment. The PICO-extended\nschema and ablation schemas are detailed in Supplementary Tables S1 and S2,\nrespectively.\nNugget coverage. Nuggets denote the atomic factual units of the ground truth\n(GT). Because nugget coverage directly reflects answer comprehensiveness and cor-\nrectness, recent RAG studies have increasingly adopted this metric[30, 31]. Specifically,\nwe adopted the LLM-as-judge paradigm[32]: for each GT nugget, the judge returned\n“covered” or “not covered” based on prompt-constrained criteria, and we computed\nthe nugget coverage per query. Results showed that DeepSeek-V3 achieved the highest\nscore on this metric (0.830). Removing the PICO-extended schema caused a signif-\nicant drop (0.830 to 0.774), indicating that structured PICO-based retrieval aids in\nrecalling and articulating key points aligned with GT nuggets; removing HyDE or\nColBERT also led to varying declines.\nAnswer faithfulness. This metric measures how faithfully the generated answer\nadheres to the retrieved context. GPT-4o achieved the highest score on this metric.\nAblation showed that removing the PICO-extended schema slightly increased faithful-\nness (0.819 to 0.834), possibly because a narrower retrieval scope yields more focused\ncontext, encouraging the model to stay within well-supported evidence. However,\nthis ablation also caused PICOT alignment to drop significantly (Table 2). Addition-\nally, removing ColBERT had the greatest negative impact on faithfulness (0.819 to\n0.752), suggesting that the absence of token-level coarse ranking significantly increased\ncandidate noise.\nSemantic similarity. This metric measures the cosine similarity between generated\nanswers and GT in embedding space, primarily reflecting surface-level textual simi-\nlarity. DeepSeek-V3 achieved the highest score (0.882). Notably, this metric showed a\nslight increase under the w/o PICO-extended schema setting (0.886), indicating that\npure semantic similarity is insufficient to detect critical population or intervention mis-\nmatches in medical QA and must be complemented by domain-specific metrics such\nas PICOT match accuracy.\nPICOT match accuracy. As mentioned earlier, mainstream RAG evaluation meth-\nods currently lack specific metrics for medical QA, making it difficult to detect PICO\nmismatches. To address this, we developed an evaluation paradigm analogous to\nnugget coverage. During benchmark construction, we extracted PICOT fields from GT\nas gold standards and extracted corresponding fields from system outputs for item-\nby-item matching (synonymous expressions were permitted; missing GT fields were\nexcluded). We then computed the overall field match rate. This metric directly quanti-\nfies critical mismatches between query and answer, such as population or intervention\ndiscrepancies. Results showed that DeepSeek-V3 achieved the best performance on this\nmetric (0.788). In ablation, replacing the PICO-extended schema with generic medical\nentity types significantly lowered match accuracy (0.788 to 0.701), demonstrating the\neffectiveness of PICO as a schema within the GraphRAG paradigm. Disabling HyDE\nalso led to a drop (0.788 to 0.723), confirming that PICO-guided HyDE is critical for\nPICO matching.\n11\n"}, {"page": 12, "text": "Table 2 Automated evaluation and ablation results on the SR-RAG benchmark.\nModel\nPipeline\nNugget coverage\nFaithfulness\nSemantic similarity\nPICO match\nGPT-4o[33]\nSR-RAG\n0.825\n0.842\n0.862\n0.762\nBaichuan-M2[34]\nSR-RAG\n0.740\n0.785\n0.806\n0.755\nDeepSeek-V3[35]\nSR-RAG\n0.830\n0.819\n0.882\n0.788\nDeepSeek-V3\nw/o HyDE\n0.819\n0.801\n0.879\n0.723\nDeepSeek-V3\nw/o ColBERT\n0.798\n0.752\n0.884\n0.740\nDeepSeek-V3\nw/o PICO-extended schema\n0.774\n0.834\n0.886\n0.701\n2.5 Results of doctor evaluation\nFig. 4 Expert clinician evaluation results, including rating distributions and inter-rater disagreement\nacross dimensions.\nWe randomly sampled 20 questions and invited five sports rehabilitation experts to\nconduct manual evaluation. Each expert reviewed the same set of questions and mate-\nrials, with each review taking approximately one hour. Assessment used a 1–5 Likert\nscale with independent scoring across five dimensions: medical factual accuracy (fac-\ntuality), answer faithfulness (faithfulness), answer relevance (relevance), safety, and\nPICOT alignment (PICO match) (Fig. 4). For detailed evaluation procedures and\nscoring criteria, see the Methods section.\n12\n"}, {"page": 13, "text": "SR-RAG received high scores across all five dimensions, with most ratings in the\n4–5 range (Fig. 4(a)). Aggregated mean (± SD) scores were: medical factual accuracy\n4.71±0.50, answer faithfulness 4.84±0.37, answer relevance 4.81±0.44, safety 4.72±\n0.57, and PICOT alignment 4.66 ± 0.76. Notably, answer faithfulness and answer\nrelevance had the highest means with the lowest variability, indicating that the system\ngenerated responses closely aligned with queries. PICOT alignment had the largest\nSD (0.76), suggesting greater susceptibility to variation in question format.\nFig. 4(b–c) presents inter-rater consistency analysis. Mean inter-rater SD for the\nfive dimensions was: medical factual accuracy 0.46, answer faithfulness 0.28, answer\nrelevance 0.33, safety 0.42, and PICOT alignment 0.63. Reviewer disagreement was\ngreatest for PICOT alignment, driven primarily by a few outlier questions (e.g., Q3:\nSD = 1.73; Q17: SD = 1.34), as shown in Fig. 4(c). Overall, manual evaluation\naligned with automated assessment results, demonstrating SR-RAG’s reliability in\nsports rehabilitation clinical QA.\n3 Discussion\nIn this study, we present SR-RAG, an EBM-adapted GraphRAG framework for sports\nrehabilitation. SR-RAG builds on Youtu-GraphRAG and adapts it to medical tasks\nthrough PICO-guided HyDE, dual-track evidence retrieval, and BETR-calibrated\ntwo-stage reranking. SR-RAG focuses on sports rehabilitation. We adopted Youtu-\nGraphRAG’s community compression algorithm to construct the first knowledge\ngraph for sports rehabilitation and generate 1,637 reusable QA pairs for evaluation.\nSR-RAG’s reliability was validated through both automated evaluation and expert\nreview.\nDuring graph construction, we did not perform ontology normalization; conse-\nquently, the same clinical concept expressed differently across studies appears as\nmultiple nodes, yielding a long-tailed distribution with many nodes and edges. We\nretained the original graph structure for three reasons. First, standard ontologies\nin sports rehabilitation are diverse with rich terminological variation, making auto-\nmatic normalization challenging and risking erroneous merging of studies that are\nsemantically similar yet differ in disease subtype, intervention parameters, or follow-\nup duration[36]. Second, Youtu-GraphRAG already clusters and consolidates graph\nstructure via dual-perception community compression; additional concept merging\nmay introduce noise. Third, downstream RAG tasks already handle synonymous\nexpressions implicitly via two-stage reranking with satisfactory retrieval and QA per-\nformance, so normalization offers limited additional benefit. Accordingly, we retained\nthe original graph structure and plan to explore finer-grained ontology alignment in\nfuture work.\nWe address the core question of this paper: how to adapt the EBM framework to\nRAG systems, via generalizable components that can be reused across clinical domains.\n• Evidence hierarchy principles.\n– At the data collection stage, we distinguish literature types and manually label\neach corpus entry with its evidence grade. This practice is readily transferable\n13\n"}, {"page": 14, "text": "to future medical RAG systems. Given the cost of manual labeling, a rea-\nsonable fallback is to use LLMs to read abstracts and assign document types\nautomatically.\n– At the reranking stage, we propose BETR, a Bayesian-inspired algorithm that\nminimizes manual intervention while improving weight interpretability. Because\nBETR only requires reranker logits and ordered evidence grades, the same formu-\nlation can be reused across medical domains without redesigning ad-hoc weighting\nrules.\n– SR-RAG stratifies recall by evidence grade: Grade A and Grades B–E evidence\nare recalled separately to ensure guideline coverage. The two candidate sets are\nthen merged, BETR calibrates the ranking scores, and a soft quota strategy avoids\nforcing semantically irrelevant evidence into the final selection. This dual-track\nretrieval strategy is domain-agnostic and can be applied to any corpus with tiered\nevidence sources. This mirrors clinical retrieval practice: guidelines and consensus\nare prioritized, with research evidence supplementing details and applicability\nboundaries.\n• PICO principles. Beyond replacing the schema with PICO-related terms, we\nenhance the HyDE method. Traditional HyDE narrows the semantic gap between\nqueries and corpus by generating hypothetical documents via LLMs, thereby boost-\ning similarity with relevant passages[27]. Building on this, we modify prompts to\nadd soft PICOT constraints for medical adaptation, with effectiveness demonstrated\nexperimentally.\nNotably, recent RAG studies in medicine have begun to emphasize integration with\nEBM. PICOs-RAG, proposed by Sun et al.[37], improves PICO alignment in QA\nby expanding and standardizing user questions and extracting PICO elements for\nretrieval. While this approach is similar to ours, the key distinction is the rewriting\ntarget—they reformulate the query, whereas HyDE generates hypothetical answers.\nRewriting the question enables explicit PICO standardization and effective match-\ning via sparse keyword retrieval. However, a semantic gap typically exists between\nuser queries and retrieved documents[38]. Queries are often short or vague, creat-\ning an asymmetric match against concrete, comprehensive documents and weakening\nretrieval effectiveness. Hypothetical documents can bridge this semantic gap and\nimprove retrieval matches, but also pose a risk of hallucination[39]. In this study,\nwe extracted PICOT elements via prompts without forcing completion, avoiding\nfabrication of missing fields or specific values to mitigate hallucination risk.\nSimilar to PICOs-RAG, Quicker emphasizes the PICO principle and incorporates\nit throughout the pipeline[40]. In question decomposition, the LLM structures clinical\nproblems into PICO elements. In document retrieval, PICO components are expanded\ninto search terms and combined into Boolean queries. In study screening, PICO ele-\nments serve as inclusion and exclusion criteria to guide full-text assessment. Quicker\nalso implements the GRADE system for EBM principles[22], systematically evaluating\nevidence quality across five dimensions: risk of bias, inconsistency, indirectness, impre-\ncision, and publication bias. This multidimensional paradigm goes beyond SR-RAG’s\ncurrent approach, which relies on document type as the primary dimension. However,\n14\n"}, {"page": 15, "text": "Quicker and BETR are methodologically complementary: GRADE provides quantita-\ntive within-study evidence quality assessment, whereas evidence hierarchy focuses on\ncross-study comparison.\nSome RAG systems have begun to focus on the issue of evidence hierarchy. Med-\nR2 decomposes the EBM workflow into stages: clinical question construction, evidence\nretrieval and evaluation, and evidence application[41]. In the evidence evaluation stage,\nit introduces a dedicated evidence reranking module. This module maps evidence\nhierarchy to confidence levels, which are then incorporated as factors in the rerank-\ning score. However, this approach is based on a manually defined linear mapping\nfh(x) = 9 −e + 1 that converts hierarchy into scores, which presents the following\nproblems: (1) Excessive manual intervention: the nine-level division and equal-interval\nassumption lack justification; (2) Poor interpretability of coupled multiplication after\nmapping: once the hierarchical score is multiplied by document type and usefulness\nscores, it becomes difficult to disentangle the independent contribution of each fac-\ntor; (3) Lack of cross-system reusability: fixed rules cannot adapt to the evidence\ndistribution in different domains. For example, RCTs are scarce for certain rare\ndiseases, and rule-based weighting cannot simultaneously accommodate semantic rel-\nevance. META-RAG further refines the evidence screening framework by drawing on\nmeta-analysis principles to rerank evidence across three dimensions: reliability, het-\nerogeneity, and extrapolation[21]. Notably, its heterogeneity analysis introduces the\nDerSimonian-Laird random-effects model to explicitly exclude evidence inconsistent\nwith the mainstream conclusion, and its extrapolation analysis employs PIO matching\nto assess evidence applicability to the current patient, providing a more systematic\nframework than the single linear mapping of Med-R2. However, the final ranking score\nin META-RAG still relies on a manually defined formula Sj = r2\nj · Tj, where the\nweight formulation lacks theoretical justification and the fixed rules cannot adapt to\nevidence distributions in different domains. The BETR algorithm proposed in this\nstudy addresses these limitations by learning grade bias parameters through Bayesian\nmaximum a posteriori estimation, avoiding manual weight specification while ensuring\ncross-domain transferability.\nRegarding the guideline-priority principle, GARMLE-G by Li et al. uses only clin-\nical practice guidelines as the external knowledge source[42]. Unlike traditional RAG,\nit returns original guideline segments as answers rather than allowing free-form gener-\nation. This is conceptually aligned with SR-RAG’s dual-track retrieval strategy; both\nprioritize authoritative clinical guidance over primary research.\nFinally, most public datasets in sports rehabilitation consist of multimodal data\nfor motion analysis or injury prediction, oriented toward biomechanics or computer\nvision[43–45], with very limited resources for LLM research. This study leveraged\ncurated corpora to create 1,637 QA pairs for evaluation through automated genera-\ntion pipelines combined with manual review. Additionally, we constructed a full-scale\nknowledge graph dataset from the entire corpus using Youtu-GraphRAG’s graph\nconstruction pipeline. This partially addressed the current scarcity of LLM-related\ndatasets in sports rehabilitation. However, we did not include rare diseases or condi-\ntions lacking authoritative guidelines. Future work could supplement benchmarks for\nadditional conditions to enrich domain coverage.\n15\n"}, {"page": 16, "text": "Our study has the following limitations. First, the scale of expert-reviewed ques-\ntions is limited, potentially introducing evaluation bias. We mitigate this by (1)\ninviting multiple experts with standardized one-hour review sessions and (2) imple-\nmenting fully automated evaluation across the entire dataset. Second, the absence of\nontology normalization in the knowledge graph introduces noise and structural redun-\ndancy. Finally, our system focuses on macro-level comparisons across study types\nwithout assessing within-study evidence quality. In contrast, Quicker implements the\nGRADE system to evaluate five quality dimensions at the individual study level[40],\nand META-RAG introduces heterogeneity analysis to filter inconsistent findings and\nextrapolation analysis to assess patient-specific applicability[21]. Integrating such\nfine-grained quality assessment into BETR-calibrated reranking represents a promis-\ning direction. More broadly, future work should pursue tighter integration of EBM\nprinciples throughout the RAG pipeline.\n4 Methods\n4.1 Corpus Collection and Data Preprocessing\nCorpus collection was performed by four graduate students from the Department of\nSports Science. We first defined inclusion criteria based on common conditions in\nclinical sports rehabilitation, excluding conditions with scarce literature or lacking\nauthoritative guideline support. Next, we retrieved open-access and institution-\nsubscribed literature on each disease from major databases, such as PubMed and\nEmbase, and supplemented materials from authoritative sports rehabilitation orga-\nnization websites. Referring to established evidence grading frameworks[46–48], we\ncategorized the literature types as follows: guidelines and expert consensus, systematic\nreviews and meta-analyses, RCTs, cohort studies, and other research (case reports,\nnarrative reviews, observational studies, and other corpus). Through targeted search\nstrategies and manual abstract review, we ensured accurate evidence-grade assignment\nfor each document. Ultimately, we compiled a domain corpus covering 21 conditions,\nmapped to evidence grades A–E.\nDuring data preprocessing, we used Docling to convert PDFs to Markdown and\nextract hierarchical structure (titles, sections, tables)[49]. We then applied regular\nexpressions to remove header and footer noise (e.g., copyright statements, references)\nand standardized numeral and unit formatting to reduce cross-document inconsisten-\ncies. Finally, we recorded each document’s evidence grade in metadata for subsequent\nevidence-grade-aware retrieval and BETR calibration.\nIn the chunking phase, we adopted an LLM-aware hybrid chunking strategy. First,\nMarkdownHeaderTextSplitter divided documents into sections by heading level[50].\nEach section was further split into numbered atomic blocks preserving paragraph\nstructure. Under prompt constraints, the LLM performed semantic grouping of atomic\nblocks and returned group IDs. The program concatenated each group into final evi-\ndence windows. This strategy aimed to enhance the semantic integrity and clinical\ninformation density of the windows while preserving paragraph boundaries, aligning\nwith recent LLM-aware chunking methods such as AutoChunker, MetaChunking, and\nLumberChunker[51–53].\n16\n"}, {"page": 17, "text": "Algorithm 1 Bayesian Evidence Tier Reranking\nRequire: Disjoint query splits Qtrain, Qval with gold windows W⋆(q); candidate\ngenerator Cand(·); reranker fθ returning logit s(q, d); ordered evidence grades\nA ≻B ≻C ≻D ≻E; grade function Grade(d) ∈{A, B, C, D, E}; negatives per\npositive K; shrinkage scale τ (selected via grid search on Qval and fixed for all\nexperiments); scale prior σa for a.\nEnsure: Calibrator parameters (α, δB, δC, δD, δE) and online ranking score r(q, d). x\nStep 1: Build pairwise records (train split)\n1: {Pq}q∈Qtrain ←∅\n2: for q ∈Qtrain do\n3:\nCq ←Cand(q);\nC+\nq ←Cq ∩W⋆(q);\nC−\nq ←Cq \\ W⋆(q)\n4:\nIf C+\nq = ∅or C−\nq = ∅, set Pq ←∅.\n5:\nElse form Pq ⊆C+\nq × C−\nq by sampling K negatives per d+ ∈C+\nq .\n6:\nFor each (d+, d−) ∈Pq, compute ∆s = s(q, d+)−s(q, d−) and t± = Grade(d±).\n7: end for\nStep 2: Ordered grade effects and MAP fit\nDefine uA = 0, uB = −δB, uC = −(δB + δC), uD = −(δB + δC + δD), uE =\n−(δB + δC + δD + δE), with δB, δC, δD, δE ≥0.\nReparameterize a = exp(α).\nFit (α, δB, δC, δD, δE) by maximizing the query-normalized MAP objective:\nmax\nα,δB,δC,δD,δE≥0\n1\n|Qtrain|\nX\nq∈Qtrain\n1\nmax(1, |Pq|)\nX\n(d+,d−)∈Pq\nlog σ(a ∆s + ut+ −ut−)\n−\n1\n2τ 2\n\u0000δ2\nB + δ2\nC + δ2\nD + δ2\nE\n\u0001\n−\n1\n2σ2a\nα2.\nwhere σ(z) = (1 + e−z)−1.\nStep 3: Online ranking\n8: For a new query q and each candidate window d ∈Cand(q), set a = exp(α) and\nt = Grade(d).\n9: Compute r(q, d) = a s(q, d) + ut and rank by r(q, d).\n4.2 BETR Algorithm Workflow\nThe evidence hierarchy principle in EBM was integrated into the reranking pipeline\nto optimize ranking order. In existing literature, evidence-hierarchy-based rerank-\ning paradigms mostly rely on subjective preset scores, yielding heuristic weighting\nschemes[41]. Such schemes introduce excessive manually defined rules that reduce\ninterpretability and preclude cross-system reuse. To address this, we proposed BETR.\nThis algorithm introduced evidence hierarchy as an ordered structure into the ranking\ncalibrator, leveraging a data-driven paradigm to improve weight interpretability and\ncross-system reusability. The complete workflow is presented in Algorithm 1.\n17\n"}, {"page": 18, "text": "Task Definition and Objective. Given a clinical question q and a candidate evidence\nwindow d, the reranker outputs an uncalibrated relevance score s(q, d). For the candi-\ndate set Cq, we aimed to combine the relevance score s(q, d) with evidence hierarchy\nGrade(d) ∈{A ≻B ≻C ≻D ≻E} to yield a unified ranking score r(q, d) satisfying:\n(1) when semantic relevance differences are large, s(q, d) dominates the ranking; (2)\nwhen candidates have comparable s(q, d), higher evidence grades receive higher final\nscores. This study adopted a unified five-grade evidence hierarchy: A = guidelines and\nexpert consensus, B = systematic reviews and meta-analyses, C = RCTs, D = cohort\nstudies, E = other research (case reports, narrative reviews, observational studies, and\nother corpus).\nTraining Labels. For each question q, candidate windows included in the reference\nevidence chain serve as positive examples: C+\nq = Cq ∩W⋆(q); the remainder serve as\nnegatives: C−\nq = Cq \\W⋆(q). This design avoids per-item manual annotation and aligns\nthe ranking objective with evidence window selection.\nPairwise\nRanking\nObjective.\nBETR\nadopts\na\npairwise\nlearning-to-rank\napproach[54], learning preferences through pairwise comparisons. Specifically, for the\ncandidate pool of question q, we construct positive–negative pairs (d+, d−) with\nd+ ∈Cq ∩W⋆(q) and d−∈Cq \\ W⋆(q), and model preference probability via the\nBradley–Terry model[55]:\nP(d+ ≻d−| q) = σ\n\u0000a∆s + ut+ −ut−\u0001\n,\n(3)\nwhere ∆s = s(q, d+)−s(q, d−) is the semantic relevance difference; t+ = Grade(d+)\nand t−= Grade(d−) denote evidence grades; ut is the grade bias; a > 0 is the\nscale parameter; and σ(·) is the sigmoid function. This formulation jointly considers\ntwo signals: (1) semantic relevance difference ∆s, and (2) evidence grade difference\nut+ −ut−. When semantic scores are comparable, windows with higher evidence grades\nreceive additional positive bias.\nOrdered hierarchical parameterization. To explicitly encode a pyramid-shaped evi-\ndence hierarchy and prevent grade inversion under noisy labels, we fix the grade\nordering as A ≻B ≻C ≻D ≻E and adopt monotonically constrained incremental\nparameterization for grade effects:\nuA = 0,\nuB = −δB,\nuC = −(δB + δC),\nuD = −(δB + δC + δD),\nuE = −(δB + δC + δD + δE).\n(4)\nwhere δB, δC, δD, δE ≥0. This form naturally guarantees uA ≥uB ≥uC ≥uD ≥\nuE, yielding an evidence-grade pyramid consistent with EBM.\nWe then cast BETR parameter estimation in a Bayesian framework, jointly learn-\ning the scale parameter a and grade increments δ = (δB, δC, δD, δE) via MAP\nestimation.\n18\n"}, {"page": 19, "text": "Prior distribution. We impose zero-centered priors on the parameters, encoding\nthe default assumption of no grade bias.\nα ∼N(0, σ2\na),\nδi ∼N +(0, τ 2), i ∈{B, C, D, E}\n(5)\nwhere N + denotes a Gaussian truncated to nonnegative values, ensuring grade\nmonotonicity. This prior has maximum density at δ = 0, implying that ranking defaults\nto being driven by semantic relevance s(q, d). Meanwhile, the prior for α is centered\nat 0, corresponding to the default a = exp(α) ≈1, meaning that semantic scores\nand grade biases are summed on the same scale. This prior assumption aligns with\nstandard reranking, which orders results solely by relevance scores.\nLikelihood function. Given parameters (α, δ), the likelihood of the observed\npairwise preference data (d+ ≻d−) is\nP(D | α, δ) =\nY\nq∈Qtrain\nY\n(d+,d−)∈Pq\nσ\n\u0000a ∆s + ut+ −ut−\u0001\n,\n(6)\nwhere a = exp(α) and ∆s = s(q, d+) −s(q, d−).\nPosterior and MAP estimations. According to Bayes’ theorem[56], the posterior\ndistribution is proportional to the product of the likelihood and priors:\nP(α, δ | D) ∝P(D | α, δ) · P(α) · P(δ).\n(7)\nTaking the logarithm and normalizing, the MAP objective is equivalent to\nmax\nα, δB,δC,δD,δE≥0\n1\n|Qtrain|\nX\nq∈Qtrain\n1\nmax(1, |Pq|)\nX\n(d+,d−)∈Pq\nlog σ\n\u0000a ∆s + ut+ −ut−\u0001\n−\n1\n2τ 2\n\u0000δ2\nB + δ2\nC + δ2\nD + δ2\nE\n\u0001\n−\n1\n2σ2a\nα2.\n(8)\nThe first term is the query-normalized pairwise log-likelihood, where we average\nwithin each query to avoid overweighting queries with more sampled pairs. The remain-\ning terms are quadratic shrinkage penalties corresponding to the Gaussian priors on\nα and δ, respectively.\nThis framework offers several advantages: (1) the prior provides interpretable\ndefault behavior, treating grade biases as minimum necessary adjustments; (2) the\nposterior is likelihood-dominated when data are ample and shrinks toward the prior\nwhen data are scarce, achieving adaptive regularization; (3) the hyperparameter τ is\nselected via grid search on a validation set and fixed across all experiments, offer-\ning high interpretability: it quantifies prior confidence in the assumed grade effects,\nthereby minimizing manual intervention.\nTraining details. We split benchmark queries into non-overlapping sets: Qtrain for\nfitting calibrator parameters, and Qval for hyperparameter selection and early stopping\nvia grid search. Splitting was query-grouped, ensuring that candidate windows for the\nsame query did not span sets, thereby preventing information leakage. During BETR\n19\n"}, {"page": 20, "text": "training, candidate sets Cq were constructed using the same candidate generation\nprocess Cand(·) as at inference, reducing distribution shift.\nFinal reranking score. At inference time, for any candidate window d ∈Cq, the\nfinal BETR ranking score is\nr(q, d) = ˆa s(q, d) + ˆuGrade(d).\n(9)\nThat is, the scale-calibrated semantic relevance score is combined with the\ncorresponding evidence-grade bias.\n4.3 PICO-extended Schema and Knowledge Graph\nConstruction\nDuring knowledge graph construction, we instantiated the Youtu-GraphRAG seed\nschema as a PICO-extended schema and followed its graph construction workflow to\nbuild the knowledge graph over the full corpus.\nSchema Definition and Constrained Extraction. Following Youtu-GraphRAG, the\nschema is defined as\nS ≜⟨Se, Sr, Sattr⟩,\n(10)\nwhere Se, Sr, and Sattr represent the sets of entity, relation, and attribute types,\nrespectively. Based on this schema, the LLM extraction agent performs constrained\ntriple extraction for each document (denoted by x):\nT (x) =\nn\n(h, r, t), (e, rattr, eattr)\n\f\f\f {f(h), f(t), f(e)} ∈Se, {r, rattr} ∈Sr, eattr ∈Sattr\no\n.\n(11)\nHere, (h, r, t) denotes an entity–relation–entity triple, and (e, rattr, eattr) an entity–\nattribute pair; f(·) maps text spans to types, constraining the extraction space and\nsuppressing noise.\nInstantiation of the PICO-extended schema. To explicitly incorporate the PICO\nframework into extraction constraints, we instantiate S as a collection of PICO-related\ntypes: (1) entity types SPICO\ne\n: Population, Condition, Intervention, Comparator,\nOutcome, Timepoint, plus domain-specific extensions frequently occurring in sports\nrehabilitation literature (e.g., Arm, Device); (2) relation types SPICO\nr\n: directed\nrelations linking studies to PICO entities (e.g., has population, has condition,\nuses intervention, compares to, reports outcome); (3) attribute types SPICO\nattr\n: key\ndimensions refining PICO elements (e.g., age bin, followup weeks, measure name,\nprotocol params). A complete list is provided in Supplementary Table S1.\nSchema self-expansion. Youtu-GraphRAG allows agents to propose schema exten-\nsions based on document content:\n∆S = ⟨∆Se, ∆Sr, ∆Sattr⟩= I[fLLM(x, S) ⊙S] ≥µ,\n(12)\nwhere µ is the confidence threshold for accepting new schema elements, and ∆S\ncontains candidate extensions for entities, relations, and attributes, respectively. For\nour task, this mechanism enables graph and index updates across disease subtypes.\n20\n"}, {"page": 21, "text": "Dually-perceived community compression and knowledge tree indexing. To reduce\ndensity and noise in the raw triple graph and shorten retrieval context, we adopted\nYoutu-GraphRAG’s dually-perceived community detection. The affinity between node\nei and community Cm is defined as\nϕ(ei, Cm) = Sr(ei, Cm)\n|\n{z\n}\nrelational\n⊕λ Ss(ei, Cm)\n|\n{z\n}\nsemantic\n,\n(13)\nwhere Sr measures Jaccard similarity over relation-type sets Ψ(·), Ss measures\nsubgraph semantic similarity, and λ is a trade-off coefficient. Iteratively, the algorithm\nperforms pairwise community merging based on the following threshold criterion:\nE\nh\nϕ\n\u0010\nei, C(t)\na\n\u0011i\n−E\nh\nϕ\n\u0010\nei, C(t)\nb\n\u0011i\n< ϵ.\n(14)\nUltimately, the community structure forms a four-layer knowledge tree K =\nS4\nℓ=1 Lℓ: L4 = community, L3 = keyword, L2 = entity–relation triple, L1 = attribute.\nThis knowledge tree serves as a structured index for subsequent graph retrieval and\ncommunity filtering.\n4.4 SR-RAG Pipeline\nAs previously mentioned, SR-RAG introduced three key improvements to Youtu-\nGraphRAG: (1) a PICO-guided HyDE channel fused with graph retrieval via RRF;\n(2) two-stage reranking using ColBERT and a cross-encoder; (3) evidence-grade-aware\nretrieval that recalls guideline evidence separately to prevent dilution, followed by\nBETR-based reranking and a soft quota strategy to exclude semantically irrelevant\ncandidates. For specific hyperparameter settings, see Supplementary Table S3.\nPICO-guided HyDE retrieval channel. The original HyDE method addresses poor\nzero-shot dense retrieval performance without relevance annotations[27]. LLMs gen-\nerate several hypothetical documents related to a user query, which are subsequently\nused for dense retrieval in the corpus. This paradigm tolerates factual errors in hypo-\nthetical documents: because retrieval relies on dense embeddings rather than lexical\nmatching, semantic similarity is preserved even when generated content contains\ninaccuracies[27]. Subsequent work identified another advantage of HyDE: bridging the\nsemantic gap between queries and documents[38]. In RAG, document-form inputs gen-\nerally outperform raw question-form queries; thus, HyDE effectively improves retrieval\nquality and downstream performance. Based on this finding, we incorporated PICO\nsoft constraints into the HyDE prompt: available P/I/C/O/T keywords were extracted\nfrom the query as anchors (missing fields are permitted). Hypothetical documents\nmust reuse these anchors and are prohibited from fabricating missing fields. This\ndesign ensures HyDE serves purely as a retrieval intermediary for semantic alignment,\nwithout deviating from the original query’s PICO elements. The specific prompts are\ndetailed in Supplementary Listing S1.\nTwo-stage reranking. We first used ColBERT as the coarse ranking model. Specifi-\ncally, we used mxbai-edge-colbert-v0, which was recently proposed by Clavi´e et al.[57].\nColBERT uses the MaxSim mechanism for scoring:\n21\n"}, {"page": 22, "text": "scol(q, d) =\n|q|\nX\ni=1\nmax\nj≤|d| cos\n\u0000eq\ni , ed\nj\n\u0001\n(15)\nHere, eq\ni and ed\nj denote the i-th and j-th token embeddings of the query and\nwindow, respectively. The MaxSim mechanism computes, for each query token, its\nmaximum similarity with any window token and sums across tokens[29], improv-\ning sensitivity to medical abbreviations, scale names, and synonyms while increasing\nmatching robustness[58].\nAfter coarse ranking, we used BGE-reranker-v2-m3 as a cross-encoder to per-\nform fine-grained ranking on the top-K high-scoring candidates[59]. The cross-encoder\njointly encodes the query and each candidate window via concatenation, enabling full\ninteraction through multiple self-attention layers and outputting an overall relevance\nlogit s(q, d). The computational cost of the cross-encoder increases linearly with the\nnumber of candidates. By adopting a two-stage pipeline, the system reduces rerank-\ning time and computational cost while balancing precise term matching and semantic\nunderstanding of long texts.\nEvidence-grade-aware retrieval and scoring. To ensure retrieval and ranking align\nwith EBM principles, we explicitly introduced evidence-grade information at both\nthe candidate generation and scoring stages. First, the corpus was partitioned by\nannotated evidence grade into Grade A and Grades B–E, forming two candidate pools.\nCandidate generation and truncation were performed separately on each pool. The\ncandidate sets were then merged, and initial reranking yielded cross-encoder logit\nscores. BETR calibration was then applied for final global ranking:\nr(q, d) = ˆa s(q, d) + ˆuGrade(d).\n(16)\nThis design enables the system to prioritize higher-grade evidence when candidate\nrelevance is comparable.\n4.5 Benchmark Construction\nTo facilitate automated evaluation of SR-RAG, we created 1,637 QA pairs using a\ncombination of LLM generation and human review. The benchmark composition by\nsub-condition and guideline set is summarized in Supplementary Table S4. Given the\nunique logic of SR-RAG’s QA process and to ensure both reliable evaluation and\ncross-system reusability, we adopted the following pipeline.\nFirst, using the corpus with completed evidence-grade annotation and chunking,\nwe performed stratified sampling by evidence grade to obtain candidate evidence\nwindows, filtering those with insufficient information density or lacking substantive\nclinical conclusions. For each candidate window, an LLM generated a clinical question\nstrictly corresponding to the window’s core conclusion while simultaneously extract-\ning PICOT elements. During generation, questions had to be directly supported by\nthe window; inclusion of information not present in the window was prohibited[60].\nAdditionally, the LLM annotated evidence certainty (sufficient or uncertain) for each\nquestion, enabling stratified evaluation and error analysis.\n22\n"}, {"page": 23, "text": "Next, we performed retrieval accessibility checks for each generated question, simi-\nlar to the round-trip consistency filtering in ARES[60]. Using a system-agnostic hybrid\nretrieval baseline (combining sparse and dense methods), we retrieved the top-K candi-\ndate windows. If the seed window was recalled within top-K, the question was deemed\n“accessible” and entered the main split; otherwise, it entered the challenge split for\nmanual review.\nFor reference evidence construction, we selected gold windows for each accessible\nquestion from a larger candidate pool. This entailed reranking candidates by relevance,\nthen having the LLM grade the evidence relationship (strongly supportive/support-\nive/weakly related/unrelated). Strongly supportive and supportive windows formed\nthe gold evidence set. Based on gold windows, we generated hierarchical reference\nanswers: an exact answer summarizing the core conclusion, and an ideal answer orga-\nnized by evidence grades (A–E), with Grade A prioritized and remaining evidence\npresented in descending pyramid order. All statements were strictly aligned with gold\nwindows, with source identifiers for traceability. The LLM then decomposed the exact\nanswer into atomic facts (nuggets) for automated assessment[30].\nFinally, four sports science graduate students manually reviewed all QA pairs,\nwith particular attention to samples marked “evidence uncertain” and those in the\nchallenge split. Approximately 2,000 questions were generated initially; after human\nscreening, 1,637 were retained as the official evaluation set. We extracted questions,\nexact answers, and nuggets and compiled them into a public dataset for reusability.\nBecause some gold windows originate from institutionally subscribed materials, the\npublic version excluded evidence window text.\n4.6 Evaluation Process\nFig. 5 Automated and human evaluation pipelines of SR-RAG.\n23\n"}, {"page": 24, "text": "SR-RAG evaluation comprised two dimensions: automated and manual (Fig. 5). For\nautomated evaluation, we used all 1,637 QA pairs to conduct multi-metric assessments\nacross generative models and ablation experiments. Metrics included: (1) Nugget cov-\nerage: extent to which the answer covers GT core factual units; (2) Answer faithfulness:\nwhether answer statements are supported by retrieved evidence; (3) Semantic sim-\nilarity: semantic consistency between answer and GT; (4) PICOT match accuracy:\nalignment of the answer with P/I/C/O/T elements.\nNugget coverage and PICOT match accuracy were implemented via LLM-as-\njudge[32]: for each nugget, the LLM determined whether it was covered by the system\nanswer; for each extracted P/I/C/O/T field, the LLM evaluated whether it matched\nthe reference (paraphrasing permitted; missing GT fields default to matched). Cover-\nage and match rates were then aggregated. Answer faithfulness and semantic similarity\nwere assessed via the RAGAS framework[20]. RAGAS first extracted claims from the\nanswer, then determined for each claim whether it was supported by the actually\nretrieved context, yielding a faithfulness score (also LLM-as-judge based). Semantic\nsimilarity was computed as cosine similarity between text embeddings of the system\nanswer and GT.\nFor manual evaluation, we invited five sports rehabilitation experts to review a\nrandom sample of 20 questions, each using identical review materials. Review materi-\nals include task instructions, scoring criteria, evaluation dimensions, and the question\nset. For each question, materials provide the question and GT, system-generated\nanswer, retrieved evidence fragments, and a scoring sheet. A five-point Likert scale\nwas used, with each question scored independently across five dimensions: (1) Medi-\ncal factual accuracy: whether conclusions, values, and prescriptions align with current\nevidence, guidelines, and consensus; (2) Answer faithfulness: whether key statements\nare substantiated by retrieved fragments; (3) Answer relevance: whether the response\naddresses core concerns and covers key constraints; (4) Safety: whether the answer\ncontains potentially harmful or misleading content; (5) PICOT alignment: whether\nthe answer is structured around the question’s P/I/C/O/T elements without popula-\ntion or intervention mismatch. We aggregated scores from all five experts across five\ndimensions for the 20 questions and computed means and standard deviations per\nquestion and dimension for analysis.\n4.7 Implementation details\nAll experiments were implemented in Python 3.11. Graph construction followed\nYoutu-GraphRAG’s schema-bounded extraction and community compression using\nNetworkX, with graph artifacts stored as JSON. Dense retrieval was implemented\nwith sentence-transformers and FAISS; candidate lists from dense, graph, and HyDE\nretrieval were fused via RRF. Two-stage reranking used ColBERT(mxbai-edge-\ncolbert-v0) for coarse ranking and a cross-encoder (bge-reranker-v2-m3) for final logits.\nBETR training used PyTorch 2.6.0 with the Adam optimizer (learning rate: 0.05,\nepochs: 80, K = 20 negatives per positive). Hyperparameters (τ) were selected via grid\nsearch on a query-disjoint split (train: n=1,310; validation: n=327). Unless otherwise\nspecified, LLM decoding used temperature = 0.\n24\n"}, {"page": 25, "text": "Data availability\nThe SR-RAG benchmark dataset is publicly available at https://huggingface.co/\ndatasets/Ning311/sr-rag-benchmark. The SR-RAG knowledge graph is publicly avail-\nable at https://huggingface.co/datasets/Ning311/sr-rag-knowledge-graph. Because\nsome gold windows originate from institutionally subscribed materials, the public\nrelease does not include the full text of evidence windows.\nCode availability\nThe SR-RAG code will be publicly released after publication.\nAcknowledgements\nWe thank the expert clinicians for participating in the human evaluation and provid-\ning valuable feedback. This work was supported by the National Key Research and\nDevelopment Program of China (Grant No. 2022YFC3600201), the National Natu-\nral Science Foundation of China (Grant No. 32000838), and the Chinese Universities\nScientific Fund (Grant No. 2024JNPD002).\nAuthor contributions\nJ.Z., J.S., and Y.L. contributed to the conception and design of the study. T.S. and\nJ.Z. designed the BETR algorithm. W.T., Z.L., J.Z., J.S., and Z.W. contributed to\ndata collection and benchmark construction. J.X.L., J.L., and X.L. provided medical\nexpertise and were responsible for the recruitment and execution of the expert clinician\nevaluation. Y.L. supervised the overall project, reviewed the experimental design and\nresults, and acquired funding. J.Z. and Y.L. drafted and revised the manuscript and\napproved the final version.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nThis preprint includes supplementary material.\nCorrespondence and requests for materials should be addressed to the correspond-\ning author.\n25\n"}, {"page": 26, "text": "Supplementary Materials\nS1. Schema Specifications\nTable S1 lists the PICO-extended schema used for graph construction and retrieval.\nTable S2 lists the PICO-neutral schema used in the w/o PICO-extended schema\nablation.\nTable S1 PICO-extended schema used in graph construction and retrieval.\nNodes\nRelations\nAttributes\nPopulation, Condition,\nIntervention, Comparator,\nOutcome, Timepoint, Arm,\nDevice, Recommendation,\nAdverseEvent,\nContraindication\nhas population, has condition,\nuses intervention, compares to,\nreports outcome, has timepoint,\ntargets arm, uses device,\nrecommends for,\nrecommends against,\nhas adverse event,\ncontraindicated for\nage bin, age range, sex,\nbaseline severity,\ntime since injury or surgery,\ninclusion criteria,\nexclusion criteria, dose,\nfrequency, session duration,\nintensity, progression rule,\nsetting, supervision,\nfollowup weeks, timepoint value,\ntimepoint unit, measure name,\noutcome domain, primary outcome,\nadverse event, contraindication,\nrecommendation strength,\nevidence certainty,\napplicability notes,\nstudy design, sample size,\nprotocol params\nTable S2 PICO-neutral schema used in the w/o PICO-extended schema ablation.\nNodes\nRelations\nAttributes\nClinicalConcept,\nEvidenceStatement, Study,\nGuideline, Recommendation,\nProtocol, OutcomeMeasure,\nDevice, AdverseEvent,\nContraindication, Topic\nmentions, associated with,\nbelongs to topic, reported in,\nsupported by, has protocol,\nuses device, reports measure,\nrecommends for,\nrecommends against,\nhas adverse event,\ncontraindicated for\nage bin, age range, sex,\nbaseline severity,\ntime since injury or surgery,\ninclusion criteria,\nexclusion criteria, dose,\nfrequency, session duration,\nintensity, progression rule,\nsetting, supervision,\nfollowup weeks, timepoint value,\ntimepoint unit, measure name,\noutcome domain, primary outcome,\nadverse event, contraindication,\nrecommendation strength,\nevidence certainty,\napplicability notes,\nstudy design, sample size,\nprotocol params\n26\n"}, {"page": 27, "text": "S2. Prompt Templates\nThis section provides representative prompt excerpts used in SR-RAG. For brevity,\nwe include only the core instructions and output constraints.\nPICO-guided HyDE.\nListing S1 PICO-guided HyDE prompt excerpt.\nYou are a sports rehabilitation clinician.\nWrite a concise, evidence-style passage that answers the question, explicitly following PICOT:\n- P: population characteristics and key constraints\n- I: intervention details (dose/frequency/intensity/progression)\n- C: comparator if applicable\n- O: outcomes and relevant measures\n- T: time horizon / follow-up\nConstraints:\n- Do NOT cite sources.\n- Use neutral clinical language.\n- 5-8 sentences, concise, declarative, paper-like.\nQuestion:\n{question}\nOutput ONLY the hypothetical evidence window.\nIn practice, we invoke this template multiple times per query to obtain diversified\nhypothetical windows.\nEvidence window selection.\nListing S2 Evidence window selection prompt excerpt.\nYou are an evidence extraction assistant. Given a question and a medical passage,\nidentify up to MAX contiguous snippets from the passage that directly help answer the question.\nEach snippet must be copied verbatim and kept short.\nReturn strictly JSON:\n{\n\"windows\": [\n{\n\"snippet\": \"...\",\n\"reason\": \"...\",\n\"contains_population\": true/false,\n\"contains_intervention\": true/false,\n\"contains_comparator\": true/false,\n\"contains_outcome\": true/false,\n\"contains_time\": true/false,\n\"outcome_and_time_same_window\": true/false\n}\n]\n}\nQuestion: {question}\nPassage: <<<{passage}>>>\nNugget coverage.\nListing S3 Nugget coverage prompt excerpt.\nYou are a clinical QA evaluator.\nGiven the question, a system answer, and ONE ground-truth nugget,\njudge whether the nugget is explicitly covered by the system answer.\n27\n"}, {"page": 28, "text": "Rules:\n- Only judge coverage by the system answer text itself.\n- Treat synonymous expressions as covered.\n- If the nugget is contradicted or missing, mark it as not covered.\n- Output RFC8259-compliant JSON only.\nReturn JSON:\n{\n\"covered\": true/false,\n\"reason\": \"brief justification\"\n}\nQuestion: {question}\nSystem answer: {answer}\nGT nugget: {nugget}\nPICOT match accuracy.\nListing S4 PICOT match prompt excerpt.\nYou are a medical information extraction and matching assistant.\nGiven gold PICOT fields and the system answer, extract PICOT from the system answer and compare\nfield-by-field.\nRules:\n- Do not invent missing fields. If a field is not stated, output null.\n- Consider medically equivalent synonyms as a match.\n- Exclude GT fields that are explicitly marked as null/missing from scoring.\n- Output RFC8259-compliant JSON only.\nReturn JSON:\n{\n\"system_picot\": {\"P\": \"...|null\", \"I\": \"...|null\", \"C\": \"...|null\", \"O\": \"...|null\", \"T\":\n\"...|null\"},\n\"match\": {\"P\": 0/1, \"I\": 0/1, \"C\": 0/1, \"O\": 0/1, \"T\": 0/1},\n\"reason\": {\"P\": \"...\", \"I\": \"...\", \"C\": \"...\", \"O\": \"...\", \"T\": \"...\"}\n}\nQuestion: {question}\nGold PICOT: {gold_picot}\nSystem answer: {answer}\nS3. Key Hyperparameters and BETR Training Details\nTable S3 summarizes selected hyperparameters for SR-RAG and BETR. Evi-\ndence grades were defined as: A = guidelines/expert consensus; B = systematic\nreviews/meta-analyses; C = randomized controlled trials; D = cohort studies; E =\nother studies.\nS4. Benchmark Composition by Sub-condition\nTable S4 summarizes the SR-RAG benchmark composition (n = 1,637 queries). The\nbenchmark covers 21 common sports rehabilitation sub-conditions, along with two\ncross-condition guideline sets. Specifically, the guideline sets include a general guideline\nset and a special-population subset.\n28\n"}, {"page": 29, "text": "Table S3 Selected hyperparameters and training settings.\nItem\nSetting\nSR-RAG retrieval and reranking\nFinal evidence budget\nTop-K contexts = 12; soft quota: retain ≥1 Grade\nA if its score is sufficiently high\nRecall capacities\ndense top-K = 300; graph top-K = 120; HyDE pas-\nsages retrieved via dense top-K = 300; RRF k = 60\nHyDE\n3 hypothetical passages per query; temperature =\n0.3 for lexical diversification\nWindow selection\nwindow size = 320 tokens; max windows/chunk =\n3; overlap = 64 tokens\nTwo-stage reranking\nColBERT →cross-encoder\nBETR\nData split\nquery-disjoint train/validation: 1310/327 queries;\n21,546/5,209 pairs\nPair construction\n20 negatives per positive\nOptimization\nAdam, learning rate 0.05, epochs 80, seed 42\nShrinkage strength\nτ selected via grid search on validation; selected τ =\n1.0 and fixed for all experiments\nScale prior\nσa = 5.0; reparameterized a = exp{α}\nLearned parameters\na = 1.0348;\nuA = 0, uB = −0.1287, uC = −0.2575, uD =\n−0.3863, uE = −0.5151\nTable S4 SR-RAG benchmark composition by sub-condition and guideline set.\nCode\nSub-condition / Guideline set\nQueries (n)\nGuideline sets\nGL-GEN\nGeneral clinical exercise guideline set\n92\nGL-SP\nSpecial-population guideline subset\n11\nSub-conditions\nACL\nAnterior cruciate ligament injury\n46\nAT\nAchilles tendinopathy\n36\nBSI\nBone stress injury\n38\nFS\nAdhesive capsulitis of the shoulder\n79\nGPA\nGroin pain in athletes\n79\nGTPS\nGreater trochanteric pain syndrome\n96\nHSI\nHamstring strain injury\n70\nIS\nIsthmic spondylolisthesis\n105\nLAS\nLateral ankle sprain\n86\nLBP\nLow back pain\n50\nITBS\nIliotibial band syndrome\n98\nLET\nLateral elbow tendinopathy\n70\nMACL\nKnee meniscal and articular cartilage lesions\n78\nMTSS\nMedial tibial stress syndrome\n55\nNAHJP\nNonarthritic hip joint pain\n73\nNP\nNeck pain\n63\nPHP\nPlantar heel pain\n76\nPFP\nPatellofemoral pain\n87\nPT\nPatellar tendinopathy\n77\nRCRSP\nRotator cuff related shoulder pain\n102\nFTASD\nFirst-time anterior shoulder dislocation\n70\n29\n"}, {"page": 30, "text": "References\n[1] Barbazi, N., Shin, J. Y., Hiremath, G. & Lauff, C. A. Exploring health educational\ninterventions for children with congenital heart disease: Scoping review. JMIR\nPediatrics and Parenting 8, e64814 (2025).\nURL https://pediatrics.jmir.org/\n2025/1/e64814.\n[2] Ubeda\nTikkanen,\nA.\net\nal.\nCore\ncomponents\nof\na\nrehabilitation\nprogram\nin\npediatric\ncardiac\ndisease.\nFrontiers\nin\nPediatrics\n11\n(2023). URL https://www.frontiersin.org/journals/pediatrics/articles/10.3389/\nfped.2023.1104794/full. Publisher: Frontiers.\n[3] Interamerican\nSociety\nof\nCardiology\n(SIAC).\n2024\nSIAC\nguide-\nlines\non\ncardiorespiratory\nrehabilitation\nin\npediatric\npatients\nwith\ncongenital\nheart\ndisease.\nRevista\nEspa˜nola\nde\nCardiolog´ıa\n(English\nEdition)\n77,\n680–689\n(2024).\nURL\nhttps://www.revespcardiol.org/\nen-2024-siac-guidelines-on-cardiorespirator-articulo-S1885585724001543.\nPublisher: Elsevier.\n[4] Hager, P. et al. Evaluation and mitigation of the limitations of large language\nmodels in clinical decision-making.\nNature Medicine 30, 2613–2622 (2024).\nURL https://www.nature.com/articles/s41591-024-03097-1.\nPublisher: Nature\nPublishing Group.\n[5] American College of Sports Medicine. ACSM’s Guidelines for Exercise Testing\nand Prescription 12th edn (Wolters Kluwer, 2025). URL https://shop.lww.com/\nACSM-s-Guidelines-for-Exercise-Testing-and-Prescription/p/9781975219215.\n[6] Almaadawy, O. et al. Target Heart Rate Formulas for Exercise Stress Testing:\nWhat Is the Evidence?\nJournal of Clinical Medicine 13 (2024).\nURL https:\n//www.mdpi.com/2077-0383/13/18/5562.\nCompany: Multidisciplinary Digital\nPublishing Institute Distributor: Multidisciplinary Digital Publishing Institute\nInstitution: Multidisciplinary Digital Publishing Institute Label: Multidisci-\nplinary Digital Publishing Institute Publisher: publisher.\n[7] Lauer, M. S. et al. Impaired Chronotropic Response to Exercise Stress Testing\nas a Predictor of Mortality. JAMA 281, 524–529 (1999). URL https://doi.org/\n10.1001/jama.281.6.524.\n[8] Lewis, P. et al.\nRetrieval-Augmented Generation for Knowledge-Intensive\nNLP\nTasks.\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n33,\n9459–9474\n(2020).\nURL\nhttps://proceedings.neurips.cc/paper/2020/hash/\n6b493230205f780e1bc26945df7481e5-Abstract.html.\n[9] Yang, R. et al. Retrieval-augmented generation for generative artificial intelli-\ngence in health care. npj Health Systems 2, 2 (2025). URL https://www.nature.\ncom/articles/s44401-024-00004-1.\n30\n"}, {"page": 31, "text": "[10] B´echard, P. & Ayala, O. M.\nReducing hallucination in structured outputs\nvia Retrieval-Augmented Generation.\nProceedings of the 2024 Conference of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 6: Industry Track) 228–238 (2024). URL\nhttp://arxiv.org/abs/2404.08189.\n[11] Luan, Y., Eisenstein, J., Toutanova, K. & Collins, M. Sparse, Dense, and Atten-\ntional Representations for Text Retrieval. Transactions of the Association for\nComputational Linguistics 9, 329–345 (2021).\nURL https://aclanthology.org/\n2021.tacl-1.20/.\n[12] Shi, Y. et al. MKRAG: Medical Knowledge Retrieval Augmented Generation\nfor Medical Question Answering. AMIA Annual Symposium Proceedings (2024).\nURL https://arxiv.org/abs/2309.16035.\n[13] Sohn, J., Jeong, M., Sung, M. & Kang, J. RAG2: Rationale-Guided Retrieval\nAugmented Generation for Medical Question Answering. Proceedings of the 2025\nConference of the North American Chapter of the Association for Computational\nLinguistics 635–650 (2025). URL https://aclanthology.org/2025.naacl-long.635.\n[14] Sun, W., Chen, J. et al.\nMRD-RAG: Enhancing Medical Diagnosis with\nMulti-Round Retrieval-Augmented Generation. arXiv preprint arXiv:2504.07724\n(2025). URL https://arxiv.org/abs/2504.07724.\n[15] Gupta, S., Ranjan, R. & Singh, S. N.\nA comprehensive survey of retrieval-\naugmented generation (RAG): Evolution, current landscape and future directions.\narXiv preprint arXiv:2410.12837 (2024). URL https://arxiv.org/abs/2410.12837.\n[16] Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking retrieval-augmented gen-\neration for medicine. Findings of the Association for Computational Linguistics:\nACL 2024 6233–6251 (2024). URL https://aclanthology.org/2024.findings-acl.\n372/.\n[17] Asai, A., Wu, Z., Wang, Y., Sil, A. & Hajishirzi, H.\nSelf-RAG: Learning to\nretrieve, generate, and critique through self-reflection. Proceedings of the Twelfth\nInternational Conference on Learning Representations (ICLR) (2024).\nURL\nhttps://arxiv.org/abs/2310.11511.\n[18] Yan, S.-Q., Gu, J.-C., Zhu, Y. & Ling, Z.-H.\nCorrective retrieval augmented\ngeneration. arXiv preprint arXiv:2401.15884 (2024).\nURL https://arxiv.org/\nabs/2401.15884.\n[19] Huang, L. et al. A survey on hallucination in large language models: Principles,\ntaxonomy, challenges, and open questions. ACM Transactions on Information\nSystems 43, 1–55 (2024). URL https://dl.acm.org/doi/10.1145/3703155.\n31\n"}, {"page": 32, "text": "[20] Es, S., James, J., Espinosa Anke, L. & Schockaert, S. RAGAs: Automated Evalu-\nation of Retrieval Augmented Generation. Proceedings of the 18th Conference of\nthe European Chapter of the Association for Computational Linguistics: System\nDemonstrations 150–158 (2024). URL https://aclanthology.org/2024.eacl-demo.\n16/.\n[21] Sun, M., Zhao, S., Chen, J., Wang, H. & Qin, B.\nMETA-RAG: Meta-\nanalysis-inspired evidence-re-ranking method for retrieval-augmented generation\nin evidence-based medicine.\narXiv preprint arXiv:2510.24003 (2025).\nURL\nhttps://arxiv.org/abs/2510.24003.\n[22] Guyatt, G. H. et al. GRADE: an emerging consensus on rating quality of evidence\nand strength of recommendations. BMJ 336, 924–926 (2008). URL https://\nwww.bmj.com/content/336/7650/924.\n[23] Sackett, D. L., Rosenberg, W. M. C., Gray, J. A. M., Haynes, R. B. & Richardson,\nW. S. Evidence based medicine: what it is and what it isn’t. BMJ 312, 71–72\n(1996). URL https://www.bmj.com/content/312/7023/71.\n[24] Edge, D. et al. From local to global: A graph RAG approach to query-focused\nsummarization. arXiv preprint arXiv:2404.16130 (2024).\nURL https://arxiv.\norg/abs/2404.16130.\n[25] Cabello, L., Suster, S. & Hershcovich, D. MEG: Medical knowledge-augmented\nlarge language models for question answering. arXiv preprint arXiv:2411.03883\n(2024). URL https://arxiv.org/abs/2411.03883.\n[26] Dong, Y. et al. Youtu-GraphRAG: Vertically unified agents for graph retrieval-\naugmented complex reasoning. arXiv preprint arXiv:2508.19855 (2025). URL\nhttps://arxiv.org/abs/2508.19855.\n[27] Gao, L., Ma, X., Lin, J. & Callan, J. Precise zero-shot dense retrieval without\nrelevance labels. Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) 1762–1777 (2023).\n[28] Cormack, G. V., Clarke, C. L. A. & B¨uttcher, S. Reciprocal rank fusion out-\nperforms condorcet and individual rank learning methods.\nProceedings of the\n32nd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval 758–759 (2009).\n[29] Khattab, O. & Zaharia, M. ColBERT: Efficient and effective passage search via\ncontextualized late interaction over BERT. Proceedings of the 43rd International\nACM SIGIR Conference on Research and Development in Information Retrieval\n39–48 (2020).\n[30] Min, S. et al.\nFActScore: Fine-grained atomic evaluation of factual precision\nin long form text generation. Proceedings of the 2023 Conference on Empirical\n32\n"}, {"page": 33, "text": "Methods in Natural Language Processing 12076–12100 (2023).\n[31] Lin, J., Pradeep, R., Sethi, S. & Wang, S. Initial nugget evaluation results for\nthe TREC 2024 RAG track with the AutoNuggetizer framework. arXiv preprint\narXiv:2411.09607 (2024). URL https://arxiv.org/abs/2411.09607.\n[32] Zheng, L. et al. Judging LLM-as-a-judge with MT-Bench and chatbot arena.\nAdvances in Neural Information Processing Systems 36 (2023). URL https://\narxiv.org/abs/2306.05685.\n[33] OpenAI. GPT-4o system card. arXiv preprint arXiv:2410.21276 (2024). URL\nhttps://arxiv.org/abs/2410.21276.\n[34] Baichuan AI. Baichuan-M2: Scaling medical capability with large verifier system.\narXiv preprint arXiv:2509.02208 (2025). URL https://arxiv.org/abs/2509.02208.\n[35] DeepSeek-AI. DeepSeek-V3 technical report. arXiv preprint arXiv:2412.19437\n(2024). URL https://arxiv.org/abs/2412.19437.\n[36] Newman-Griffis, D. et al. Ambiguity in medical concept normalization: An analy-\nsis of clinical concept overlap in health records. Journal of the American Medical\nInformatics Association 28, 516–524 (2021).\n[37] Sun, M., Zhao, S., Wang, H. & Qin, B. PICOs-RAG: PICO-supported query\nrewriting for retrieval-augmented generation in evidence-based medicine. arXiv\npreprint arXiv:2510.23998 (2025). URL https://arxiv.org/abs/2510.23998.\n[38] Wang, L., Yang, N. & Wei, F. Query2doc: Query expansion with large language\nmodels. Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing 9414–9423 (2023).\n[39] Zhu, W., Zhao, L., Yu, C., Huang, S. & Liu, Z. HyQE: Ranking contexts with\nhypothetical query embeddings. Findings of the Association for Computational\nLinguistics: EMNLP 2024 13024–13036 (2024).\n[40] Wang, X. et al. Large language models driving evidence-based clinical decision\nmaking. npj Digital Medicine (2025).\n[41] Wang, S. et al. Med-R2: Crafting trustworthy LLM physicians via retrieval and\nreasoning of evidence-based medicine. arXiv preprint arXiv:2501.11885 (2025).\nURL https://arxiv.org/abs/2501.11885.\n[42] Li, W. et al. Refine medical diagnosis using generation augmented retrieval and\nclinical practice guidelines. arXiv preprint arXiv:2506.21615 (2025). URL https:\n//arxiv.org/abs/2506.21615.\n[43] Xia, H. et al. Language and multimodal models in sports: A survey of datasets\nand applications. arXiv preprint arXiv:2406.12252 (2024). URL https://arxiv.\n33\n"}, {"page": 34, "text": "org/abs/2406.12252.\n[44] Musat, C.-L. et al.\nDiagnostic applications of AI in sports: A comprehensive\nreview of injury risk prediction methods. Diagnostics 14, 2516 (2024).\n[45] Reis, F. J. J., Alaiti, R. K., Vallio, C. S. & Hespanhol, L. Artificial intelligence and\nmachine learning approaches in sports: Concepts, applications, challenges, and\nfuture perspectives. Brazilian Journal of Physical Therapy 28, 101083 (2024).\n[46] OCEBM Levels of Evidence Working Group.\nThe Oxford Levels of Evi-\ndence 2 (2011). URL https://www.cebm.ox.ac.uk/resources/levels-of-evidence/\nocebm-levels-of-evidence. Oxford Centre for Evidence-Based Medicine.\n[47] National Health and Medical Research Council.\nNHMRC additional levels of\nevidence and grades for recommendations for developers of guidelines. Tech. Rep.,\nAustralian Government (2009). URL https://www.nhmrc.gov.au.\n[48] Joanna Briggs Institute. JBI levels of evidence (2014). URL https://jbi.global/\nsites/default/files/2019-05/JBI-Levels-of-evidence 2014 0.pdf.\n[49] Auer, C. et al. Docling technical report. arXiv preprint arXiv:2408.09869 (2024).\nURL https://arxiv.org/abs/2408.09869.\n[50] Chase, H. LangChain (2022). URL https://github.com/langchain-ai/langchain.\n[51] Jain, A., Aggarwal, P. & Saladi, A. AutoChunker: Structured text chunking and\nits evaluation. Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 6: Industry Track) 983–995 (2025).\n[52] Zhao, J. et al. Meta-chunking: Learning text segmentation and semantic com-\npletion via logical perception. arXiv preprint arXiv:2410.12788 (2024). URL\nhttps://arxiv.org/abs/2410.12788.\n[53] Duarte, A. V. et al. LumberChunker: Long-form narrative document segmenta-\ntion. Findings of the Association for Computational Linguistics: EMNLP 2024\n(2024). URL https://arxiv.org/abs/2406.17526.\n[54] Burges, C. et al. Learning to rank using gradient descent. Proceedings of the\n22nd International Conference on Machine Learning 89–96 (2005).\n[55] Bradley, R. A. & Terry, M. E. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika 39, 324–345 (1952).\n[56] Gelman, A. et al. Bayesian Data Analysis 3rd edn (Chapman and Hall/CRC,\n2013).\n[57] Takehi, R., Clavi´e, B., Lee, S. & Shakir, A. Fantastic (small) retrievers and how to\ntrain them: mxbai-edge-colbert-v0 tech report. arXiv preprint arXiv:2510.14880\n34\n"}, {"page": 35, "text": "(2025). URL https://arxiv.org/abs/2510.14880.\n[58] Mart´ınez Rivera, E. & Menolascina, F.\nModernBERT + ColBERT: Enhanc-\ning biomedical RAG through an advanced re-ranking retriever. arXiv preprint\narXiv:2510.04757 (2025). URL https://arxiv.org/abs/2510.04757.\n[59] Chen, J. et al.\nBGE M3-embedding: Multi-lingual, multi-functionality, multi-\ngranularity text embeddings through self-knowledge distillation. arXiv preprint\narXiv:2402.03216 (2024). URL https://arxiv.org/abs/2402.03216.\n[60] Saad-Falcon, J., Khattab, O., Potts, C. & Zaharia, M. ARES: An automated\nevaluation framework for retrieval-augmented generation systems. Proceedings\nof the 2024 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies 338–354 (2024).\n35\n"}]}