{"doc_id": "arxiv:2602.07540", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.07540.pdf", "meta": {"doc_id": "arxiv:2602.07540", "source": "arxiv", "arxiv_id": "2602.07540", "title": "LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing", "authors": ["Huimin Yan", "Liang Bai", "Xian Yang", "Long Chen"], "published": "2026-02-07T13:29:15Z", "updated": "2026-02-07T13:29:15Z", "summary": "Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.07540v1", "url_pdf": "https://arxiv.org/pdf/2602.07540.pdf", "meta_path": "data/raw/arxiv/meta/2602.07540.json", "sha256": "663f786b44dc6b2fb92b9ec807b5ee1844d61b4dfb83c8b5bb93ae88e4272499", "status": "ok", "fetched_at": "2026-02-18T02:19:30.397514+00:00"}, "pages": [{"page": 1, "text": "1\nLLM-Guided Diagnostic Evidence Alignment for\nMedical Vision–Language Pretraining under\nLimited Pairing\nHuimin Yan, Liang Bai, Xian Yang, Long Chen\nAbstract—Most existing CLIP-style medical vision–language\npretraining methods rely on global or local alignment with\nsubstantial paired data. However, global alignment is easily\ndominated by non-diagnostic information, while local alignment\nfails to integrate key diagnostic evidence. As a result, learning\nreliable diagnostic representations becomes difficult, which limits\ntheir applicability in medical scenarios with limited paired data.\nTo address this issue, we propose an LLM-Guided Diagnostic\nEvidence Alignment method (LGDEA), which shifts the pre-\ntraining objective toward evidence-level alignment that is more\nconsistent with the medical diagnostic process. Specifically, we\nleverage LLMs to extract key diagnostic evidence from radiol-\nogy reports and construct a shared diagnostic evidence space,\nenabling evidence-aware cross-modal alignment and allowing\nLGDEA to effectively exploit abundant unpaired medical images\nand reports, thereby substantially alleviating the reliance on\npaired data. Extensive experimental results demonstrate that\nour method achieves consistent and significant improvements on\nphrase grounding, image–text retrieval, and zero-shot classifica-\ntion, and even rivals pretraining methods that rely on substantial\npaired data.\nIndex Terms—Medical Vision-Language Pretraining, Multi-\nmodal Learning, Diagnosic Evidence Alignment\nI. INTRODUCTION\nM\nEDICAL vision–language pretraining (VLP) aims to\nlearn transferable multimodal representations by mod-\neling the correspondences between medical images and clin-\nical reports [1], which plays an important role in enabling\nclinically meaningful diagnostic decision-making, and is there-\nfore widely applied to downstream tasks such as disease\nclassification and image–report retrieval [2], [3]. Early medical\nVLP methods typically adapt the CLIP framework to the\nmedical domain [4], employing global contrastive objectives\nto align entire medical images with their corresponding full\nclinical reports on substantial paired data, thereby learning\ntransferable multimodal representations [5], [6]. This paradigm\nimplicitly assumes that global alignment is sufficient to capture\ndiagnostic semantics.\nHowever, radiology reports often describe multiple local-\nized pathological findings, while diagnostically critical visual\nHuimin Yan and Liang Bai are with Institute of Intelligent Information Pro-\ncessing, Shanxi University, Taiyuan, 030006, China. (Corresponding author:\nLiang Bai). E-mail: yanhm0925@163.com, bailiang@sxu.edu.cn\nXian Yang is with Alliance Manchester Business School, The University of\nManchester, Manchester, M13 9PL, UK. E-mail: xian.yang@manchester.ac.uk\nLong Chen is with Department of Computer Science and Engineering, The\nHong Kong University of Science and Technology, Hong Kong, China. E-\nmail: longchen@ust.hk\nReport\nLesions\nAt both lung bases, but more \nextensive on the right than left, \nthere are patchy opacities, fairly \nstreaky in nature but extensive.\nImage\nVisual Evidence \nVisual Patch \nAt\nboth\nbases\nbut\nmore\n...\nWords\n...\nReport Evidence\nboth lung bases\npatchy opacities\nLocal Alignment\nDiagnostic Evidence Alignment\nGlobal Alignment\nFig. 1.\nMotivation of LGDEA. Global and local alignment may overlook\ndiagnostic evidence, whereas LGDEA aligns images and reports in a shared\ndiagnostic evidence space.\nlesions are typically subtle and spatially localized. To mitigate\nthis issue, some methods introduce local alignment by match-\ning report words or phrases with image regions. Such local\nalignment methods partially alleviate the limitation of global\nalignment in modeling localized diagnostic cues, enabling\nthe model to attend to image regions that are relevant to\npathological descriptions [7], [8], [9].\nSo far, state-of-the-art global and local alignment methods\nshare two common characteristics: 1) They heavily rely on\nsubstantial and high-quality paired data to provide sufficient\ncross-modal supervisory signals. 2) They essentially follow\na feature-level alignment paradigm, which is well suited for\nscenarios such as natural image–text pairs where semantic\ncorrespondences are relatively explicit. However, in the med-\nical domain, high-quality paired data are inherently limited,\nand diagnostic semantics are often implicit in the integrated\nreasoning over a small number of diagnostic evidence, which\ncannot be adequately captured by feature similarity alone\n[10], [11]. Consequently, as illustrated in Figure 1, under lim-\nited paired supervision in medical settings, global alignment\nmethods tend to overlook fine-grained diagnostic details [12],\nallowing non-diagnostic information to dominate the learned\nembeddings, whereas local alignment methods overemphasize\nlocal correspondences, shifting the learning objective toward\nlow-level visual details and making it difficult to acquire\nhigh-level diagnostic evidence that underpin clinical decision-\nmaking [13].\nBased on these observations, we argue that effective medical\nVLP under limited pairing should follow the real diagnostic\narXiv:2602.07540v1  [cs.CV]  7 Feb 2026\n"}, {"page": 2, "text": "2\nprocess of clinicians, shifting from feature-level alignment\nto diagnostic evidence alignment. However, performing di-\nagnostic evidence alignment under limited paired data poses\ntwo key challenges: (1) How to construct a reliable diag-\nnostic evidence space. (2) How to effectively learn diagnostic\nevidence alignment when paired data are scarce, but addi-\ntional unimodal data are available without explicit cross-modal\ncorrespondences. To address these challenges, we propose\nLGDEA, an LLM-Guided Diagnostic Evidence Alignment\npretraining framework that leverages medical knowledge to\ncompensate for limited paired data. Specifically, we first\nutilize large language models (LLMs) to extract key diag-\nnostic evidence from radiology reports and organize them\ninto a structured cross-modal diagnostic evidence space. Then,\nreport-derived evidence from limited paired data are used\nto guide lesion-level visual representation learning, aligning\nvisual evidence within the diagnostic evidence space. Finally,\nbased on evidence-centric representations, we infer cross-\nmodal relations for unimodal data by propagating sparse paired\nsupervision over image–image and text–text evidence graphs,\nthereby guiding evidence-aware cross-modal alignment over\nboth limited paired data and abundant unpaired images and\nreports. Our main contributions are summarized as follows:\n• We leverage LLMs to construct a cross-modal diagnostic\nevidence space, establishing a diagnostic evidence foun-\ndation for medical multimodal pretraining.\n• We propose an evidence-aware cross-modal alignment\nmethod that effectively leverages abundant unpaired med-\nical images and reports, thereby substantially reducing the\nreliance on paired data.\n• Extensive experiments show that our method consistently\nimproves performance on phrase grounding, image–text\nretrieval, and zero-shot classification, even outperforming\nmethods trained with more paired data.\nII. RELATED WORK\nA. Global Alignment in Medical VLP\nInspired by the CLIP-style contrastive learning framework\n[14], [15], numerous studies aligns medical images and their\ncorresponding reports by projecting them into a shared embed-\nding space. PubMedCLIP [16] adapts CLIP to medical data to\nimprove medical visual question answering, while MedCLIP\n[17] performs alignment at the disease level using similarity\nmatrices. CARZero [18] further enhances alignment accuracy\nby introducing cross-attention between image and report fea-\ntures to capture more complex semantic relationships. Despite\nthese advances, existing medical vision–language pre-training\nmethods largely rely on global representation alignment, over-\nlooking the localized and multi-lesion nature of radiological\ndata [19]. Under limited paired supervision, such strategies\noften damage diagnostic signals, allowing non-diagnostic in-\nformation to dominate the embedding space and weakening\nclinically meaningful semantics.\nB. Local Alignment in Medical VLP\nTo address the limitations of global contrastive learning in\ncapturing localized pathology, prior work has explored fine-\ngrained vision–language alignment strategies [20], [21], [22].\nGLoRIA [23] and MGCA [24] incorporate local or multi-\ngranularity contrastive objectives to align words or tokens\nwith attention-weighted image regions, while MedKLIP [7]\nand MedAligner [9] leverage structured reports or explicit\nword–region alignment to enhance local semantic matching.\nAFLoc [13] further introduces multi-level semantic contrastive\nlearning for unlabeled pathological localization. However,\nthese approaches mainly focus on region-level correspon-\ndences [25], whereas medical diagnosis relies on identifying\nand integrating key diagnostic evidence at the sample level.\nUnder limited paired supervision, neither global alignment nor\nisolated local matching is sufficient, motivating a shift toward\ndiagnostic evidence–level alignment.\nIII. METHOD\nA. Problem Settings\nWe consider a realistic medical vision–language pretraining\nsetting where only a small subset of images and radiology\nreports are paired, while the majority are unpaired. Let Dp =\n{(Ip, Rp)}Np\np=1 denote paired samples, and DI\nu = {Iu}N I\nu\nu=1\nand DR\nu = {Ru}NR\nu\nu=1 denote unpaired images and reports. We\nextract representations using an image encoder fimage [26] and\na text encoder ftext\n[27]: fimage : I →(Ig, Il),\nftext : R →\n(Rg, Rl), where Ig and Rg denote global embeddings, and Il\nand Rl denote local patch- and token-level embeddings.\nExisting CLIP-style pretraining methods rely on global\ncontrastive alignment over substantial paired data, which is\nformulated as:\nLR←I\ng\n= −1\nB\nB\nX\np=1\nlog\nexp(Ig\np · Rg\np/τ1)\nPB\nk=1 exp(Ig\np · Rg\nk/τ1)\n,\n(1)\nwhere B is the batch size and τ1 is a temperature parameter.\nThe dot operator · refers to the inner product of vectors.\nSimilarly, the loss for aligning text to images, LI←R\ng\n, is defined\nsymmetrically to LR←I\ng\n. The total global alignment loss is the\nsum of these two components Lg = LR←I\ng\n+ LI←R\ng\n.\nUnder limited paired data, global alignment often fails to\ncapture sparse and localized diagnostic information, while\nlocal alignment tends to overemphasize fragmented de-\ntails. Therefore, effective medical vision–language pretraining\nshould follow the actual clinical diagnostic process, shifting\nfrom feature-level alignment to diagnostic evidence alignment.\nHowever, evidence-level alignment under scarce paired super-\nvision is non-trivial, as it requires building a reliable diagnostic\nevidence space and learning effective cross-modal alignment\nby leveraging abundant unpaired unimodal data as shown in\nFigure 2.\nB. LLM-guided Diagnostic Evidence Space\nWe leverage LLMs to construct a structured cross-modal\ndiagnostic evidence space from data with limited pairing and\nabundant unpaired samples, providing a diagnostic evidence\nfoundation for medical multimodal pretraining. First, we ex-\ntract diagnostic evidence from reports and organize them into\na modality-shared diagnostic evidence space:\nE(R) = {en}NR\nn=1 = LLM(R),\n(2)\n"}, {"page": 3, "text": "3\n(a) LLM-Guided Diagnostic Evidence Space \nstreaky in nature\nboth lung bases\npatchy opacities\n...\nText\nEncoder\n�����\n(b) Evidence-Guided Cross-modal Alignment \n…\nPaired Reports\n...\nProjection\nHead\nDiagnostic evidence\nSpace\n}\n{\nk\n\n...\nLLM \nUnpaired  Reports\nExtracted evidence\nAt both lung bases, \nbut more extensive \non the right than \nleft, there are \npatchy opacities, \nfairly streaky in \nnature but extensive.\n...\n...\nUnpaired Images\nPaired Images\n������\nImage\nEncoder\n...\nProjection\nHead\nProjection\nHead\n...\nReport-Report Graph\n...\nboth lung bases\npatchy opacities\n...\nstreaky in nature\nDiagnostic evidence\nAt both lung bases, \nbut more extensive \non the right than \nleft, there are \npatchy opacities, \nfairly streaky in \nnature but extensive.\nReport\nLLM \nImage-Image Graph\n...\n...\nProjection\nHead\nProjection\nHead\nProjection\nHead\n1\nR\nH\nImage\nVisual \nevidence\nDiagnostic evidence\n1I\nH\n����−�����\n1\n\n2\n\n3\n\n4\n\nK\n\n \nLearnable Queries\nReport \nnodes\nImage \nnodes\nReport \nevidence\nPaired \nSeed Edges Y\nP\nFig. 2.\nOverview of the proposed LGDEA framework. (a) LLMs extract diagnostic evidence from radiology reports, and both report evidence and lesion-level\nvisual cues are projected into a shared diagnostic evidence space. (b) Under limited pairing, paired evidence links are used as seed edges to align report\nand image nodes, while report–report and image–image graphs propagate evidence-aware relations to leverage abundant unpaired reports and images for\nvision–language pretraining.\nwhere each evidence phrase en corresponds to a concise and\nclinically meaningful abnormal finding. Given an evidence\nphrase en, we encode it into a continuous embedding using\na text encoder zn = ftext(en). Second, we introduce a set of\nlearnable diagnostic prototypes:\nC = {µk}K\nk=1,\nµk ∈Rd.\n(3)\nEach prototype represents a latent diagnostic concept that may\nbe shared across different reports and images. We compute a\nsoft assignment over the diagnostic prototypes:\np(k | zn) =\nexp(z⊤\nn µk/τt)\nPK\nj=1 exp(z⊤\nn µj/τt)\n,\n(4)\nwhere τt is a temperature parameter. We jointly learn the\ntext encoder and diagnostic prototypes by encouraging each\nevidence embedding to be reconstructed from the prototype\nspace:\nLrec =\nX\nen∈E(R)\n\r\r\r\r\rzn −\nK\nX\nk=1\np(k | zn)µk\n\r\r\r\r\r\n2\n2\n+\nK\nX\nk=1\n∥µk∥2\n2.\n(5)\nThrough this mechanism, unpaired reports can effectively par-\nticipate in representation learning by providing weak semantic\nsupervision at the evidence level. This diagnostic evidence\nspace provides a foundation for cross-modal learning of visual\nevidence.\nMedical images often contain multiple localized patholog-\nical regions whose visual manifestations are subtle, spatially\nsparse, and difficult to annotate. Under limited pairing, global\nalignment is insufficient to guide models to focus on such\ndiagnostically critical regions.\nWe therefore ground visual learning in the shared diagnostic\nevidence space established from text, enabling evidence-level\nalignment between images and reports. First, we introduce\na set of L learnable lesion queries {qℓ}L\nℓ=1 that attend to\npatch-level visual features. Given a medical image I, a vision\nencoder produces patch embeddings Il ∈RP ×dv. Each lesion\nquery interacts with the patch features through a query-based\nattention mechanism: vℓ= Attn(qℓ, Il, Il),\nℓ= 1, . . . , L,\nyielding a set of lesion-level embeddings vℓ∈Rdv. These\nembeddings represent latent visual evidence corresponding to\npotential pathological regions.\nTo enable evidence-level alignment with text, each lesion\nembedding is projected into the shared diagnostic prototype\nspace learned from reports:\nQI(ℓ, k) =\nexp(ϕ(vℓ)⊤µk/τp)\nPK\nj=1 exp(ϕ(vℓ)⊤µj/τp)\n,\n(6)\nwhere µk denotes the k-th diagnostic prototype. The resulting\ndistribution QI(ℓ, ·) characterizes the diagnostic evidence as-\nsociated with each lesion. Lesion-level prototype distributions\nare aggregated to obtain an image-level diagnostic distribution:\n¯QI = 1\nL\nL\nX\nℓ=1\nQI(ℓ, ·).\n(7)\nFor images paired with radiology reports, report-derived\ndiagnostic evidence provide explicit semantic guidance. Given\na report Rp paired with an image Ip, we aggregate prototype\nassignments of all extracted report evidence to form a report-\ninduced diagnostic distribution:\n¯QR(k) =\n1\n|E(Rp)|\nX\nen∈E(Rp)\np(k | zn).\n(8)\nWe then align visual evidence with textual evidence by\nminimizing the KL divergence:\nLevid\np\n= KL\n\u0000 ¯QR ∥¯QI\n\u0001\n,\n(9)\n"}, {"page": 4, "text": "4\nwhere the report-induced distribution ¯QR serves as a teacher\nsignal. This evidence-level distillation enables paired images\nto learn lesion-level visual evidence consistent with diagnostic\nsemantics, without requiring explicit lesion annotations.\nFor unpaired images, report-derived supervision is unavail-\nable. However, unpaired images still exhibit meaningful visual\nstructure and similarity patterns, which can be exploited to\nstabilize evidence learning. Specifically, we collect all le-\nsion instances from both paired and unpaired images within\na mini-batch as {vi, Qi}NL\ni=1, where NL\n= B × L. We\nmeasure visual similarity using cosine similarity between\nlesion embeddings and define the k-nearest neighbors as\nNk(i) = TopKj̸=i\n\u0000cos(vi, vj)\n\u0001\n. To encourage evidence-level\nconsistency, we align the diagnostic prototype distributions\nof visually similar lesions using a similarity-weighted KL\ndivergence:\nLevid\nu\n=\n1\nNL\nNL\nX\ni=1\nX\nj∈Nk(i)\nwij KL(Qi ∥stopgrad(Qj)) , (10)\nwhere wij is computed by a softmax over cosine similari-\nties. This consistency constraint allows unpaired images to\nactively participate in training by absorbing diagnostic seman-\ntics from nearby, evidence-aligned lesions, thereby anchoring\ntheir lesion-level representations within the shared diagnostic\nevidence space.\nC. Evidence-guided Cross-modal Alignment\nTo enable medical vision–language pretraining under lim-\nited paired data, we propose an evidence-guided weakly-\nsupervised alignment strategy. The key challenge is that\nwhen explicit paired relations are scarce or even absent,\ndirect image–report alignment becomes unreliable. Instead, we\naim to learn higher-order cross-modal relations that reflect\nevidence-level semantic relatedness beyond direct pairing, and\nuse them as weak supervision for training.\nGiven a mini-batch of B images and B reports, we define\nan evidence-aware cross-modal alignment loss as\nLevi-align = LR←I\nevi\n+ LI←R\nevi\n,\n(11)\nwhere the image-to-report direction is\nLR←I\nevi\n= −1\nB\nB\nX\ni=1\nB\nX\nj=1\nPij log\nexp\n\u0000H⊤\nIiHRj/τ2\n\u0001\nPB\nk=1 exp\n\u0000H⊤\nIiHRk/τ2\n\u0001,\n(12)\nand LI←R\nevi\nis defined symmetrically. Here, τ2 is the tempera-\nture hyperparameter, and Pij ∈[0, 1] denotes the higher-order\nevidence relation between image Ii and report Rj. Unlike hard\npaired labels, Pij provides soft supervision by measuring how\nlikely two samples share similar diagnostic evidence.\nFor each report R, we extract a set of diagnostic evidence\nE(R) using an LLM, and obtain its report-level diagnostic\nrepresentation by aggregating evidence embeddings:\nHR =\n1\n|E(R)|\nX\nen∈E(R)\nzn ∈Rd.\n(13)\nPaired data manifold\n(\n1)\n( )\nt\nt\nI\nT\nS\nS\n\n\n\nP\nP\nY\nHigher-order \nrelations\nP\nPaired data\nLack of paired \nrelations\nUnpaired Images\nUnpaired Reports\nY\nIS\nT\nS\nUnpaired data manifold\n❌\nUnified manifold\nFig. 3.\nEvidence-guided higher-order alignment under scarce paired super-\nvision. Sparse paired links Y are grounded in a shared diagnostic evidence\nspace and propagated over intra-modal evidence graphs to infer higher-order\nrelations P.\nSimilarly, for an image I with L lesion queries, we aggre-\ngate lesion-level visual evidence projected into the diagnostic\nevidence embedding space:\nHI = 1\nL\nL\nX\nℓ=1\nϕ(vℓ) ∈Rd,\n(14)\nwhere ϕ(·) maps lesion embeddings vℓinto the diagnostic\nevidence embedding space.\nLet Y ∈{0, 1}NI×NR denote the sparse binary relation\nmatrix induced by paired image–report samples, where Yij =\n1 indicates that Ii and Rj form a paired instance. When paired\nsupervision is available, we can directly set P = Y, leading\nto evidence-aware contrastive learning with hard positives.\nHowever, under scarce paired supervision, Y is extremely\nsparse and cannot provide sufficient alignment signals. To\naddress this limitation, we infer higher-order relations by prop-\nagating the sparse seed relations over intra-modal evidence\ngraphs, which effectively transfers supervision from paired\ndata to unpaired unimodal data.\nWe first construct a report–report evidence graph based on\nreport-level representations:\nAT T (i, j) = sim(HRi, HRj),\n(15)\nand row-normalize it to obtain the text-side propagation matrix\nST . Likewise, we construct an image–image evidence graph:\nAII(i, j) = sim(HIi, HIj),\n(16)\nyielding the image-side propagation matrix SI after normal-\nization.\nStarting from P(0) = Y, we perform label propagation to\ninfer higher-order cross-modal relations:\nP(t+1) = SIP(t)ST + Y,\n(17)\nwhere the residual term Y preserves original paired relations\nand stabilizes propagation. After two propagation steps, we\nobtain higher-order relations P and apply row-wise normal-\nization. The resulting P enables us to treat evidence-similar\n"}, {"page": 5, "text": "5\nimage–report pairs as weak positives, thereby supporting\nevidence-aware cross-modal alignment even when explicit\npairing is limited as shown in Figure 3.\nThe overall training objective integrates diagnostic evidence\nmodeling and cross-modal alignment:\nL = Lrec + Levid\np\n+ Levid\nu\n+ Levi-align,\n(18)\nwhere Lrec learns a structured diagnostic evidence space from\nreports, Levid\np\naligns the limited paired samples within this\ndiagnostic evidence space, Levid\nu\nenforces evidence consistency\nfor unpaired images, and Levi-align performs evidence-guided\ncross-modal alignment. All algorithms are described as fol-\nlows.\nAlgorithm 1: LGDEA: LLM-Guided Diagnostic Evi-\ndence Alignment\nRequire: Paired data Dp, unpaired images DI\nu, unpaired reports\nDR\nu .\nRequire: Encoders fimage, ftext, prototypes {µk}K\nk=1,\nlesion queries {qℓ}L\nℓ=1.\nEnsure: Trained model parameters.\n1: while not converged do\n2:\nSample a mini-batch of paired and\nunpaired images/reports.\n3:\n(1) Build diagnostic evidence space.\n4:\nExtract evidence E(R) = LLM(R) and encode {zn}.\n5:\nUpdate evidence prototypes by minimizing Lrec.\n6:\n(2) Learn lesion-level visual evidence.\n7:\nObtain lesion embeddings {vℓ} using lesion queries and\nattention.\n8:\nProject lesions into prototype space to get ¯QI.\n9:\nFor paired samples, align with report evidence via Levid\np .\n10:\nFor unpaired images, enforce lesion consistency via Levid\nu .\n11:\n(3) Infer higher-order cross-modal relations.\n12:\nCompute image/report evidence representations\n{HI}, {HR}.\n13:\nConstruct evidence graphs SI, ST and propagate\nseed links: P(t+1) = SIP(t)ST + Y.\n14:\n(4) Evidence-guided weakly-supervised alignment.\n15:\nOptimize Levi-align using P-weighted contrastive loss.\n16:\nUpdate parameters by minimizing\nL = Lrec + Levid\np\n+ Levid\nu\n+ Levi-align.\n17: end while\nIV. EXPERIMENT\nA. Experimental Settings\nWe evaluate LGDEA under both single-domain and cross-\ndomain medical vision–language pretraining settings. Below\nwe briefly describe the training data, downstream evaluation\nbenchmarks, and baselines.\na) Training Data: We conduct pretraining primarily on\nthe MIMIC-CXR dataset [28], which contains substantial\nchest X-ray images and corresponding radiology reports. To\nsimulate realistic scenarios with limited pairing, we randomly\nsample different proportions of paired image–report data,\nwhile treating the remaining images and reports as unpaired\nunimodal data. For cross-domain pretraining, we additionally\nincorporate images from CheXpert [29] as unpaired visual\ndata.\nMIMIC-CXR MIMIC-CXR contains 377,110 chest X-\nray images and 227,835 radiology reports from 65,379 pa-\ntients [28]. We use the Findings and Impression sections as\ntextual input. Following MedCLIP [17], the dataset is split into\na pretraining set and an evaluation subset (MIMIC-5×200). To\nsimulate limited pairing, we randomly sample 5% and 10% of\npaired image–report data, while treating the remaining images\nand reports as unpaired unimodal data. The dataset is publicly\navailable at https://physionet.org/content/mimic-cxr-jpg/2.1.0/.\nCheXpert CheXpert contains 224,316 chest radiographs\nfrom 65,240 patients [29]. In cross-domain experiments,\nCheXpert images are used as additional unpaired visual data,\ncombined with paired and unpaired text data from MIMIC-\nCXR. The dataset is available at https://www.kaggle.com/\ndatasets/ashery/chexpert.\nb) Downstream Tasks: We evaluate learned representa-\ntions on a diverse set of downstream medical vision–language\ntasks, including phrase grounding, image–text retrieval, and\nzero-shot classification. Specifically, we use MS-CXR for\nphrase grounding, MIMIC-5×200 for retrieval and zero-shot\nclassification, and RSNA Pneumonia, COVID, and NIH Chest\nX-rays for disease classification.\nMS-CXR. MS-CXR is a phrase grounding benchmark\nwith radiologist-annotated bounding boxes for eight disease\ncategories [20].\nMIMIC-5×200.\nA\nbalanced\nsubset\nconstructed\nfrom\nMIMIC-CXR for zero-shot classification and image–text\nretrieval, covering five disease categories following GLo-\nRIA [23].\nRSNA Pneumonia. A binary chest X-ray dataset annotated\nfor pneumonia detection with bounding boxes [30]. We follow\nMedCLIP [17] to construct class-balanced training and testing\nsplits.\nCOVID and NIH Chest X-rays. We evaluate on the\nCOVID-19 dataset [31] and the NIH Chest X-rays dataset [32]\nfor disease classification using official splits.\nc) Baselines:\nWe compare LGDEA with representa-\ntive state-of-the-art medical VLP methods, including Med-\nCLIP\n[17],\nMGCA\n[24],\nMedKLIP\n[7],\nCLEFT\n[33],\nMAVL [34], PRIOR [35], CARZero [18], MedAligner [9],\nand AFLoc [13]. All baselines are trained under consistent\ndata settings for fair comparison.\nd) Implementation Details: LGDEA is trained with a\nmulti-phase optimization schedule. The initial training phase\nuses a batch size of 128 and a learning rate of 5 × 10−5 for\n2 epochs, followed by subsequent phases trained for 5 epochs\neach with a batch size of 64 and a learning rate of 1×10−4. All\nexperiments use AdamW with a weight decay of 1×10−6 and\nare conducted on two NVIDIA A100 GPUs (40GB). Images\nare resized to 256×256 and randomly cropped to 224×224\nduring training.\nB. Experimental Results\nWe evaluate the proposed LGDEA framework under both\nsingle-domain and cross-domain medical vision–language pre-\ntraining settings with limited paired image–report. In the\nsingle-domain setting, all data are drawn from MIMIC-CXR,\n"}, {"page": 6, "text": "6\nTABLE I\nTHE USAGE OF THE DATASET EMPLOYED BY THE MODEL IN PRE-TRAINING AND DOWNSTREAM MEDICAL TASKS.\nDataset\nSize/Images\nAnnotations\nPre-training\nPhrase Grounding\nImage-Text Retrieval\nZero-shot Classification\nMIMIC-CXR\n377,110\nPaired images and reports\n✓\nCheXpert\n224,316\nImages\n✓\nMS-CXR\n206\nBounding boxes, radiology phrase\n✓\nMIMIC-5 × 200\n1,000\nRadiology text, 5 diseases\n✓\n✓\nCOVID\n5,162\nCOVID-19/normal labels\n✓\nRSNA Pneumonia\n12,024\nPneumonia/normal labels\n✓\nNIH Chest X-rays\n112,120\n14 disease/normal labels\n✓\nTABLE II\nTHE PERFORMANCE OF THE PHRASE GROUNDING TASK IS EVALUATED ON THE MS-CXR DATASET USING CONTRAST-TO-NOISE RATIO (CNR) SCORES\nACROSS EIGHT DISEASE CATEGORIES.\nGroup\nModel\nAtelectasis\nCardiomegaly\nConsolidation\nLung Opacity\nEdema\nPneumonia\nPneumothorax\nPleural\nEffusion\nPaired\nBaseline\nMedCLIP\n0.5554\n0.5253\n0.5767\n0.5599\n0.4433\n0.6175\n0.4686\n0.5076\nGLoRIA\n0.6088\n0.5253\n0.6194\n0.5376\n0.5062\n0.7308\n0.5896\n0.8991\nBioViL\n0.5376\n0.5014\n0.5852\n0.5266\n0.4727\n0.4602\n0.4882\n0.5841\nMGCA\n0.6338\n0.5680\n0.6543\n0.5586\n0.5247\n0.8324\n0.5240\n0.8207\nMedKLIP\n0.5570\n0.5282\n0.5921\n0.5926\n0.4905\n0.6781\n0.5858\n0.6398\nPRIOR\n0.6452\n0.5601\n0.5876\n0.6624\n0.5548\n0.6693\n0.6564\n0.8221\nCLEFT\n0.6871\n0.5687\n0.6492\n0.5699\n0.4989\n0.7259\n0.5976\n0.7974\nMAVL\n0.5811\n0.5417\n0.5486\n0.6381\n0.5115\n0.7056\n0.5952\n0.7601\nCARZero\n0.6757\n0.5214\n0.6889\n0.5572\n0.5209\n0.8438\n0.6326\n0.9672\nAFLoc\n0.7941\n0.5928\n0.7688\n0.6894\n0.6271\n0.7484\n0.6810\n0.9866\nMedAligner\n0.8312\n0.6058\n0.8144\n0.7163\n0.7335\n0.8489\n0.7249\n1.0207\nSingle\nDomain\nLGDEA5%pair\n0.8000\n0.9127\n0.8071\n0.9032\n0.8081\n0.7849\n0.7902\n0.9742\nLGDEA10%pair\n0.8449\n0.9525\n0.8357\n0.9564\n0.8172\n0.9353\n0.8526\n1.0928\nCross\nDomain\nLGDEA5%pair\n0.8011\n0.9111\n0.7975\n0.8789\n0.7868\n0.8235\n0.7067\n0.6667\nLGDEA10%pair\n0.8306\n0.9416\n0.8126\n0.9003\n0.7891\n0.9214\n0.7952\n0.7519\nwhere only a small proportion (5% or 10%) of image–report\npairs is retained and the remaining images and reports are\nincluded without explicit cross-modal pairing. In the cross-\ndomain setting, paired samples and reports are from MIMIC-\nCXR, while images from CheXpert are incorporated as an\nadditional visual source. Unimodal reports are derived from\nthe remaining MIMIC-CXR reports. Across different pairing\nratios, LGDEA is evaluated on multiple downstream tasks,\nincluding phrase grounding, image–text retrieval and zero-shot\nclassification, assessing fine-grained diagnostic localization,\ncross-modal semantic alignment, and generalization to unseen\ndiseases and domains.\nPhrase Grounding. Phrase grounding aims to accurately\nassociate textual phrases with their corresponding regions in\nmedical images, thereby enabling fine-grained localization of\ndiagnostic cues and enhancing model interpretability.Table II\nsummarizes the phrase grounding results on the MS-CXR\ndataset, evaluated using the contrast-to-noise ratio (CNR)\n[36]. With only 10% of the paired image–text data, LGDEA\nachieves the highest CNR scores across all eight disease\ncategories, substantially outperforming methods that leverage\nfully paired image–text relations and focus on local alignment\nmodeling, such as AFLoc and MedAligner. Even when trained\non cross-domain image–text data, LGDEA achieves higher\nCNR scores across all eight disease categories using only 10%\nof the paired image–text samples. In addition, visualization\nresults based on Grad-CAM [37] heatmaps (Figure 4) further\ndemonstrate that LGDEA can accurately localize disease-\nrelated phrases to the corresponding pathological regions.\nMore visualizations are in the Figure 5. More importantly,\nwe re-evaluate baseline methods that depend on fully paired\nsupervision under the limited pairing ratios (Table III), which\nfurther demonstrates the superiority of LGDEA in scenarios\nwith scarce paired data.\nImage-Text Retrieval. To evaluate the matching perfor-\nmance between medical images and textual descriptions, we\nconducted experiments on the MIMIC-5 × 200 dataset. Specif-\nically, for each query image, we computed the cosine similarity\nbetween its [CLS] token representation and the representations\nof candidate sentences for retrieval. Retrieval performance\nwas assessed using the Precision@K metric. The results are\nsummarized in Table IV. Although PRIOR and CARZero rely\non 100% paired image–text data and benefit from their global\nimage–text alignment to achieve competitive performance,\nLGDEA, using only 10% of the paired image–text relations,\nsurpasses them on Precision@1, Precision@2, Precision@5,\nand Precision@10. Moreover, with only 10% of the paired im-\nage–text relations, LGDEA outperforms the best-performing\n"}, {"page": 7, "text": "7\nTABLE III\nTHE PERFORMANCE OF BASELINES TRAINED WITH LIMITED PAIRED IMAGE–TEXT DATA ON THE PHRASE GROUNDING TASK ON THE MS-CXR DATASET,\nEVALUATED USING THE CONTRAST-TO-NOISE RATIO (CNR) SCORE ACROSS EIGHT DISEASE CATEGORIES.\nGroup\nModel\nAtelectasis\nCardiomegaly\nConsolidation\nLung Opacity\nEdema\nPneumonia\nPneumothorax\nPleural\nEffusion\nSingle\nDomain\nGLoRIA50%pair\n0.4486\n0.3050\n0.4359\n0.4479\n0.2069\n0.4941\n0.3017\n0.3533\nMGCA50%pair\n0.4620\n0.2966\n0.4363\n0.4897\n0.2352\n0.6071\n0.3569\n0.4338\nMedAligner50%pair\n0.6346\n0.4149\n0.5344\n0.5431\n0.5889\n0.6836\n0.6021\n0.7298\nLGDEA5%pair\n0.8000\n0.9127\n0.8071\n0.9032\n0.8081\n0.7849\n0.7902\n0.9742\nLGDEA10%pair\n0.8449\n0.9525\n0.8357\n0.9564\n0.8172\n0.9353\n0.8526\n1.0928\nCross\nDomain\nLGDEA5%pair\n0.8011\n0.9111\n0.7975\n0.8789\n0.7868\n0.8235\n0.7067\n0.6667\nLGDEA10%pair\n0.8306\n0.9416\n0.8126\n0.9003\n0.7891\n0.9214\n0.7952\n0.7519\nLGDEA \nImage\nPhrases\nGT\nsmall right \napical \npneumothorax\nenlarged \ncardiac \nsilhouette\nFig. 4. Attention maps for two disease categories are visualized on the MS-\nCXR dataset, comparing LGDEA with GT. Red bounding boxes indicate the\nground truth regions relevant to phrase grounding. Highlighted pixels corre-\nspond to higher activation weights, reflecting stronger associations between\nspecific diagnostic terms and image regions.\nTABLE IV\nTHE IMAGE–TEXT RETRIEVAL TASK IS COMPARED WITH\nSTATE-OF-THE-ART METHODS ON THE MIMIC-5×200 DATASET, AND\nPRECISION (%) SCORES ARE REPORTED.\nGroup\nModel\nPrec@1\nPrec@2\nPrec@5\nPrec@10\nPaired\nBaseline\nMedCLIP\n44.40\n43.45\n44.50\n45.58\nGLoRIA\n42.38\n45.65\n47.74\n41.98\nBioViL\n48.81\n47.88\n50.05\n32.88\nMGCA\n50.06\n49.77\n49.05\n47.53\nMedKLIP\n50.00\n50.00\n49.42\n50.00\nPRIOR\n53.19\n51.72\n51.89\n41.69\nCLEFT\n52.75\n50.31\n48.75\n49.81\nMAVL\n50.00\n50.62\n51.06\n49.97\nCARZero\n50.00\n50.00\n51.65\n47.38\nAFLoc\n54.37\n52.78\n49.79\n43.20\nMedAligner\n55.88\n54.08\n54.05\n50.56\nSingle\nDomain\nLGDEA5%pair\n53.25\n51.03\n50.66\n49.98\nLGDEA10%pair\n56.31\n53.85\n53.71\n50.65\nCross\nDomain\nLGDEA5%pair\n50.88\n50.68\n48.57\n48.33\nLGDEA10%pair\n53.58\n51.23\n50.06\n49.14\nMedAligner in terms of Precision@1 and Precision@10, while\nachieving comparable precision on Precision@2 and Preci-\nsion@5. To better illustrate LGDEA semantic understanding\ncapabilities, we present a case study of image-text retrieval\nexamples in Figure 6.\nTABLE V\nTHE ACCURACY (%) OF THE ZERO-SHOT CLASSIFICATION IS EVALUATED\nON THE MIMIC-5 × 200, COVID, RSNA PNEUMONIA, AND NIH CHEST\nX-RAYS DATASETS.\nGroup\nModel\nMIMIC-\n5 × 200\nCOVID\nRSNA\nNIH Chest\nX-rays\nPaired\nBaseline\nMedCLIP\n52.50\n77.70\n79.90\n58.84\nGLoRIA\n72.22\n87.77\n78.55\n59.47\nBioViL\n73.38\n80.10\n78.46\n61.33\nMGCA\n74.87\n86.87\n79.20\n68.40\nMedKLIP\n51.94\n83.26\n74.65\n79.40\nPRIOR\n76.83\n86.27\n74.73\n80.84\nCLEFT\n75.47\n84.18\n78.25\n78.95\nMAVL\n74.60\n83.06\n78.14\n82.77\nCARZero\n76.06\n86.80\n78.55\n83.16\nAFLoc\n76.20\n87.80\n73.52\n83.87\nMedAligner\n77.89\n88.20\n80.05\n84.11\nSingle\nDomain\nLGDEA5%pair\n79.44\n87.53\n78.48\n85.88\nLGDEA10%pair\n80.00\n90.47\n79.26\n87.06\nCross\nDomain\nLGDEA5%pair\n74.10\n88.77\n77.03\n77.06\nLGDEA10%pair\n76.78\n90.13\n78.92\n78.56\nZero-Shot Classification. To assess the adaptability and\ngeneralization capability of the learned image encoder, we\nfurther evaluated its performance on zero-shot classification\ntasks. On the MIMIC-5 × 200 dataset, we perform classifi-\ncation evaluation by computing the similarity between the\nlabels predicted by the image encoder and the ground-truth\nlabels. For the COVID, RSNA Pneumonia, and NIH ChestX-\nray datasets, we attach a classification head to the image\nencoder and train it using the cross-entropy loss, enabling the\nmodel to make predictions solely based on image features.\nTable V summarizes the classification results on MIMIC-\n5 × 200, COVID, RSNA Pneumonia, and NIH ChestX-ray.\nWhen trained with only 10% paired image–text data, LGDEA\noutperforms all baselines across most datasets, demonstrating\nstrong cross-dataset generalization. While it performs slightly\nworse than MedAligner and MedCLIP on RSNA, it surpasses\nthem on the remaining datasets.\nC. Ablation Study\nThe contributions of different components. We conduct\nablation studies to assess the contribution of each core com-\nponent in LGDEA. All variants are evaluated on the same\n"}, {"page": 8, "text": "8\nAtelectasis\nCardiomegaly\nConsolidation\nLung Opacity\nEdema\nPneumonia\nPneumothorax\nPleural Effusion\nMedCLIP\nMGCA\nMedKLIP\nPRIOR\nCLEFT\nMAVL\nCARZero\nMed-\nAligner\nGT\nModel\nAFLoc\nLGDEA\n(Ours)\nFig. 5. Attention maps for eight disease categories are visualized on the MS-CXR dataset, comparing LGDEA with nine baseline methods. Red bounding\nboxes indicate the ground truth regions relevant to phrase grounding. Highlighted pixels correspond to higher activation weights, reflecting stronger associations\nbetween specific diagnostic terms and image regions.\n"}, {"page": 9, "text": "9\nTABLE VI\nTHE PERFORMANCE OF THE PHRASE GROUNDING TASK IS COMPARED WITH THE ALL ABLATED VARIANTS, USING CONTRAST-TO-NOISE RATIO (CNR)\nSCORES ACROSS EIGHT DISEASE CATEGORIES.\nModel\nAtelectasis\nCardiomegaly\nConsolidation\nLung Opacity\nEdema\nPneumonia\nPneumothorax\nPleural Effusion\nFull LGDEA\n0.8449\n0.9525\n0.8357\n0.9564\n0.8172\n0.9353\n0.8526\n1.0928\nw/o Lrec\n0.6516\n0.7170\n0.6172\n0.7604\n0.7236\n0.8142\n0.6407\n0.8125\nw/o Levid\np\n0.7214\n0.8121\n0.7210\n0.8946\n0.7571\n0.8856\n0.7179\n0.9072\nw/o Levid\nu\n0.7064\n0.7977\n0.6972\n0.8828\n0.7724\n0.8153\n0.7348\n0.8524\nw/o Levi-align\n0.7355\n0.8302\n0.8049\n0.9150\n0.7751\n0.8973\n0.6238\n0.9196\nPRIOR\nInput:\nGround Truth:\nCARZero\nRight\nWrong\nLabel: \nPleural Effusion\nthe moderate multiloculated left \npleural effusion persists\n1.severe left lower \nlobe consolidation \nmay have worsened\n2.the lungs are clear \nwithout focal \nconsolidation effusion , \nor overt edema\nConsolidation\nConsolidation\n1.there could be a \nvery small region of \nconsolidation in the \nlingula\n2.the cardiac silhouette \nis incompletely \nassessed but maybe \nmildly enlarged\nConsolidation\nCardiomegaly\nMedAligner\n1.the moderate \nmultiloculated left \npleural effusion \npersists\n2.lingula is still \nsubstantially airless \nand there is a large \nleft pleural effusion ...\nLGDEA\n(ours)\n1.increased size of \nleft pleural effusion \nwhich is now \nmoderate with a ...\n2.the moderate \nmultiloculated left \npleural effusion \npersists\nFig. 6.\nQualitative results of image-to-text retrieval. We show the top two\nretrieved texts for LGDEA, with comparisons to three baselines. We note the\ncategories below wrongly retrieved samples.\ndownstream benchmarks for fair comparison. Specifically, we\nconsider: (1) w/o Lrec, removing diagnostic evidence prototype\nlearning and using global text embeddings instead; (2) w/o\nLevid\np\n, removing paired evidence distillation; (3) w/o Levid\nu ,\nremoving unpaired evidence consistency; and (4) w/o Levi-align,\ndisabling evidence relation inference and using only paired\nsamples as positives. As shown in Table VI and VII, removing\nany component degrades performance, with w/o Lrec causing\nthe largest drop, highlighting the importance of diagnostic\nevidence prototype learning under limited paired supervision.\nThe effects of different LLMs. In our main experiments,\ndiagnostic evidence is extracted using Spark-Desk. To evaluate\nthe robustness of LGDEA to the choice of evidence extractor,\nwe replace Spark with two open-source LLMs, Qwen-7B\n[38] and LLaMA-8B [39], while keeping all other compo-\nnents unchanged. As shown in Table\nIX and Table\nVIII,\nLGDEA achieves stable performance across different LLM\nsettings on both phrase grounding and image classification\ntasks. Although the extracted diagnostic evidence varies in\nlinguistic style across LLMs, the overall performance remains\nlargely consistent, indicating that LGDEA does not rely on a\nspecific LLM but instead benefits primarily from the semantic\neffectiveness of the diagnostic evidence.\nTABLE VII\nTHE IMAGE–TEXT RETRIEVAL TASK IS COMPARED WITH THE ALL\nABLATED VARIANTS ON THE MIMIC-5 × 200 DATASET, AND PRECISION\n(%) SCORES ARE REPORTED.\nModel\nPrec@1\nPrec@2\nPrec@5\nPrec@10\nFull LGDEA\n56.31\n53.85\n53.71\n50.65\nw/o Lrec\n51.56\n50.42\n48.12\n45.29\nw/o Levid\np\n54.06\n52.12\n49.89\n46.37\nw/o Levid\nu\n53.12\n51.34\n50.24\n47.68\nw/o Levi-align\n53.25\n51.68\n50.47\n48.61\nD. Hyper-parameter Analysis\nWe analyze the sensitivity of LGDEA to two key hyper-\nparameters: the number of diagnostic prototypes K, which\ncontrols the semantic capacity of the shared evidence space,\nand the number of lesion queries L, which determines the\ngranularity of lesion-level visual modeling.\nNumber of diagnostic prototypes K. We vary K ∈\n{32, 64, 96, 128} while fixing L = 64. As shown in Figure 7\nand Table X, a small K limits semantic expressiveness by\nforcing heterogeneous evidence to share prototypes, degrading\nphrase grounding and zero-shot classification performance. In-\ncreasing K improves fine-grained evidence–lesion alignment,\nwhile overly large values lead to performance saturation and\nslight degradation. Overall, K = 64 provides the best trade-\noff between expressiveness and stability and is used in all\nexperiments.\nNumber of lesion queries L. We fix K = 64 and vary\nL ∈{32, 48, 64, 80}. As shown in Figure 7 and Table XI,\nincreasing L enhances coverage of multiple pathological re-\ngions and improves grounding accuracy. However, excessively\nlarge L introduces redundant and overlapping queries, leading\nto slightly degraded performance. We therefore adopt L = 64\nas a balanced setting across all downstream tasks.\nV. CONCLUSION\nIn this paper, we show that CLIP-style global and local\nalignment becomes unreliable for medical vision–language\npretraining under scarce paired data. To address this issue,\n"}, {"page": 10, "text": "10\nTABLE VIII\nTHE PERFORMANCE OF THE PHRASE GROUNDING TASK IS COMPARED WITH LLAMA AND QWEN ON THE MS-CXR DATASET, USING\nCONTRAST-TO-NOISE RATIO (CNR) SCORES ACROSS EIGHT DISEASE CATEGORIES.\nModel\nAtelectasis\nCardiomegaly\nConsolidation\nLung Opacity\nEdema\nPneumonia\nPneumothorax\nPleural Effusion\nLLaMA\n0.8000\n0.9443\n0.8343\n0.9235\n0.8115\n0.9271\n0.6894\n0.9328\nQwen\n0.8182\n0.9469\n0.8314\n0.9160\n0.8020\n0.8805\n0.8521\n0.8003\nOurs\n0.8449\n0.9525\n0.8357\n0.9564\n0.8172\n0.9353\n0.8526\n1.0928\nTABLE IX\nTHE ACCURACY (%) OF THE ZERO-SHOT CLASSIFICATION IS EVALUATED\nON THE MIMIC-5 × 200, COVID, RSNA PNEUMONIA, AND NIH CHEST\nX-RAYS DATASETS AND COMPARED WITH LLAMA AND QWEN.\nModel\nMIMIC-\n5 × 200\nCOVID\nRSNA\nNIH Chest\nX-rays\nLLaMA\n79.72\n90.10\n79.60\n85.80\nQwen\n79.92\n90.39\n79.15\n86.14\nOurs\n80.00\n90.47\n79.26\n87.06\n     (a)\n     (b)\nFig. 7.\n(a) Effect of the number of diagnostic prototypes K and (b) effect of\nthe number of lesion queries L, evaluated using average CNR on the MS-CXR\nphrase grounding task and ACC on the MIMIC-5×200 zero-shot classification\ntask.\nTABLE X\nIMAGE CLASSIFICATION ACCURACY (%) ON THE COVID, RSNA, AND\nNIH CHEST X-RAYS DATASETS WITH VARYING NUMBERS OF DIAGNOSTIC\nPROTOTYPES K, EVALUATING THE EFFECT OF SEMANTIC CAPACITY IN\nTHE SHARED EVIDENCE SPACE.\nNumber of diagnostic\nprototypes K\nCOVID\nRSNA\nNIH Chest\nX-rays\n32\n87.14\n87.14\n85.91\n64\n90.47\n79.26\n87.06\n96\n86.80\n79.37\n86.88\n128\n84.72\n78.21\n86.59\nwe propose LGDEA, an LLM-guided framework that shifts\npretraining from feature-level alignment to diagnostic evidence\nTABLE XI\nIMAGE CLASSIFICATION ACCURACY (%) ON THE COVID, RSNA, AND\nNIH CHEST X-RAYS DATASETS WITH DIFFERENT NUMBERS OF LESION\nQUERIES L, ANALYZING THE IMPACT OF LESION-LEVEL VISUAL\nMODELING GRANULARITY.\nNumber of lesion queries L\nCOVID\nRSNA\nNIH Chest\nX-rays\n32\n85.10\n77.71\n85.06\n48\n86.73\n78.95\n85.53\n64\n90.47\n79.26\n87.06\n80\n85.57\n79.16\n86.72\nalignment. LGDEA extracts key evidence from radiology\nreports and guides lesion-level visual evidence learning to\nalign images with clinically meaningful semantics. Exten-\nsive experiments on phrase grounding, image–text retrieval,\nand zero-shot classification demonstrate consistent gains, and\nLGDEA can rival methods pretrained with substantially more\npaired data.\nREFERENCES\n[1] K. Zhu, P. Xia, Y. Li, H. Zhu, S. Wang, and H. Yao, “Mmedpo: Align-\ning medical vision-language models with clinical-aware multimodal\npreference optimization,” in Forty-second International Conference on\nMachine Learning, 2025.\n[2] Y. Zou and Z. Yin, “Mvcm: Enhancing multi-view and cross-modality\nalignment for medical visual question answering and medical image-\ntext retrieval,” in Proceedings of the Computer Vision and Pattern\nRecognition Conference, 2025, pp. 180–190.\n[3] Z. Lu, H. Li, N. A. Parikh, J. R. Dillman, and L. He, “Radclip: En-\nhancing radiologic image analysis through contrastive language-image\npretraining,” IEEE Transactions on Neural Networks and Learning\nSystems, vol. 36, no. 10, pp. 17 613–17 622, 2025.\n[4] Z. Zhang, Y. Yu, Y. Chen, X. Yang, and S. Y. Yeo, “Medunifier:\nUnifying vision-and-language pre-training on medical data with vision\ngeneration task using discrete visual representations,” in Proceedings\nof the Computer Vision and Pattern Recognition Conference, 2025, pp.\n29 744–29 755.\n[5] S. Gijsen and K. Ritter, “Eeg-language pretraining for highly label-\nefficient clinical phenotyping,” in Forty-second International Conference\non Machine Learning, 2025.\n[6] M. Li, M. Meng, M. Fulham, D. D. Feng, L. Bi, and J. Kim, “Enhancing\nmedical vision-language contrastive learning via inter-matching relation\nmodelling,” IEEE Transactions on Medical Imaging, vol. 44, no. 6, pp.\n2463–2476, 2025.\n[7] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, “Medklip: Medical\nknowledge enhanced language-image pre-training for x-ray diagnosis,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 21 372–21 383.\n[8] Z. Li, L. T. Yang, B. Ren, X. Nie, Z. Gao, C. Tan, and S. Z.\nLi, “Mlip: Enhancing medical visual representation with divergence\nencoder and knowledge-guided contrastive learning,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 11 704–11 714.\n[9] H. Yan, X. Yang, L. Bai, and J. Liang, “Local alignment for medical\nvision-language pre-training,” IEEE Transactions on Image Processing,\nvol. 34, pp. 7321–7334, 2025.\n"}, {"page": 11, "text": "11\n[10] Y. Xie, J. Zhang, Y. Xia, and Q. Wu, “Unimiss+: Universal medical\nself-supervised learning from cross-dimensional unpaired data,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 46,\nno. 12, pp. 10 021–10 035, 2024.\n[11] Z. Zhang, H. Zhang, T. Zeng, G. Yang, Z. Shi, and Z. Gao, “Bridging\nmulti-level gaps: Bidirectional reciprocal cycle framework for text-\nguided label-efficient segmentation in echocardiography,” Medical Image\nAnalysis, vol. 102, p. 103536, 2025.\n[12] J. Yang, B. Su, X. Zhao, and J. Wen, “Unlocking the power of spatial and\ntemporal information in medical multimodal pre-training,” in Forty-first\nInternational Conference on Machine Learning, 2024.\n[13] H. Yang, H.-Y. Zhou, J. Liu, W. Huang, C. Li, Z. Li, Y. Gao, Q. Liu,\nY. Liang, Q. Yang et al., “A multimodal vision–language model for\ngeneralizable annotation-free pathology localization,” Nature Biomedical\nEngineering, pp. 1–15, 2026.\n[14] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\n“Learning transferable visual models from natural language supervi-\nsion,” in Proceedings of the International Conference on Machine\nLearning, vol. 139, 2021, pp. 8748–8763.\n[15] Q. Chen and Y. Hong, “Medblip: Bootstrapping language-image pre-\ntraining from 3d medical images and texts,” in Proceedings of the Asian\nConference on Computer Vision, 2024, pp. 2404–2420.\n[16] S. Eslami, C. Meinel, and G. De Melo, “Pubmedclip: How much\ndoes clip benefit visual question answering in the medical domain?” in\nFindings of the Association for Computational Linguistics: EACL 2023,\n2023, pp. 1181–1193.\n[17] Z. Wang, Z. Wu, D. Agarwal, and J. Sun, “Medclip: Contrastive\nlearning from unpaired medical images and text,” in Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing,\n2022, pp. 3876–3887.\n[18] H. Lai, Q. Yao, Z. Jiang, R. Wang, Z. He, X. Tao, and S. K. Zhou,\n“Carzero: Cross-attention alignment for radiology zero-shot classifica-\ntion,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2024, pp. 11 137–11 146.\n[19] C. Lian, H.-Y. Zhou, D. Liang, J. Qin, and L. Wang, “Efficient medical\nvision-language alignment through adapting masked vision models,”\nIEEE Transactions on Medical Imaging, vol. 44, no. 11, pp. 4499–4510,\n2025.\n[20] B. Boecking, N. Usuyama, S. Bannur, D. C. Castro, A. Schwaighofer,\nS. L. Hyland, M. Wetscherek, T. Naumann, A. V. Nori, J. Alvarez-Valle,\nH. Poon, and O. Oktay, “Making the most of text semantics to improve\nbiomedical vision-language processing,” in Proceedings of the European\nconference on computer vision, vol. 13696, 2022, pp. 1–21.\n[21] X. Liang, X. Li, F. Li, J. Jiang, Q. Dong, W. Wang, K. Wang, S. Dong,\nG. Luo, and S. Li, “Medfilip: Medical fine-grained language-image pre-\ntraining,” IEEE Journal of Biomedical and Health Informatics, vol. 29,\nno. 5, pp. 3587–3597, 2025.\n[22] T. Yu, Z. Tong, J. Yu, and K. Zhang, “Fine-grained adaptive visual\nprompt for generative medical visual question answering,” in Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 9,\n2025, pp. 9662–9670.\n[23] S.-C. Huang, L. Shen, M. P. Lungren, and S. Yeung, “Gloria: A\nmultimodal global-local representation learning framework for label-\nefficient medical image recognition,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2021, pp. 3942–3951.\n[24] F. Wang, Y. Zhou, S. Wang, V. Vardhanabhuti, and L. Yu, “Multi-\ngranularity cross-modal alignment for generalized medical visual repre-\nsentation learning,” Advances in Neural Information Processing Systems,\nvol. 35, pp. 33 536–33 549, 2022.\n[25] Z. Chen, Y. Bie, H. Jin, and H. Chen, “Large language model with\nregion-guided referring and grounding for ct report generation,” IEEE\nTransactions on Medical Imaging, vol. 44, no. 8, 2025.\n[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in 9th International Conference\non Learning Representations, 2021.\n[27] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Naumann,\nand M. McDermott, “Publicly available clinical bert embeddings,” in\nProceedings of the 2nd clinical natural language processing workshop,\n2019, pp. 72–78.\n[28] A. E. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P.\nLungren, C.-y. Deng, R. G. Mark, and S. Horng, “Mimic-cxr, a de-\nidentified publicly available database of chest radiographs with free-text\nreports,” Scientific data, vol. 6, no. 1, p. 317, 2019.\n[29] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Mark-\nlund, B. Haghgoo, R. Ball, K. Shpanskaya et al., “Chexpert: A large\nchest radiograph dataset with uncertainty labels and expert comparison,”\nin Proceedings of the AAAI conference on artificial intelligence, vol. 33,\nno. 01, 2019, pp. 590–597.\n[30] G. Shih, C. C. Wu, S. S. Halabi, M. D. Kohli, L. M. Prevedello, T. S.\nCook, A. Sharma, J. K. Amorosa, V. Arteaga, M. Galperin-Aizenberg\net al., “Augmenting the national institutes of health chest radiograph\ndataset with expert annotations of possible pneumonia,” Radiology:\nArtificial Intelligence, vol. 1, no. 1, p. e180041, 2019.\n[31] T. Rahman, A. Khandakar, Y. Qiblawey, A. Tahir, S. Kiranyaz, S. B. A.\nKashem, M. T. Islam, S. Al Maadeed, S. M. Zughaier, M. S. Khan et al.,\n“Exploring the effect of image enhancement techniques on covid-19\ndetection using chest x-ray images,” Computers in biology and medicine,\nvol. 132, p. 104319, 2021.\n[32] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,\n“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on\nweakly-supervised classification and localization of common thorax\ndiseases,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 2097–2106.\n[33] Y. Du, B. Chang, and N. C. Dvornek, “CLEFT: language-image con-\ntrastive learning with efficient large language model and prompt fine-\ntuning,” in Proceedings of the International Conference Medical Image\nComputing and Computer Assisted Intervention, vol. 15012, 2024, pp.\n465–475.\n[34] V. M. H. Phan, Y. Xie, Y. Qi, L. Liu, L. Liu, B. Zhang, Z. Liao,\nQ. Wu, M. To, and J. W. Verjans, “Decomposing disease descriptions\nfor enhanced pathology detection: A multi-aspect vision-language pre-\ntraining framework,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 11 492–11 501.\n[35] P. Cheng, L. Lin, J. Lyu, Y. Huang, W. Luo, and X. Tang, “Prior: Pro-\ntotype representation joint learning from medical images and reports,”\nin Proceedings of the IEEE/CVF international conference on computer\nvision, 2023, pp. 21 361–21 371.\n[36] R. E. Hendrick, “Signal, noise, signal-to-noise, and contrast-to-noise\nratios,” in Breast MRI: Fundamentals and Technical Aspects, R. E.\nHendrick, Ed.\nNew York, NY, USA: Springer, 2008, pp. 55–69.\n[37] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-cam: Visual explanations from deep networks via\ngradient-based localization,” in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 618–626.\n[38] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge,\nY. Han, F. Huang et al., “Qwen technical report,” arXiv preprint\narXiv:2309.16609, 2023.\n[39] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,\nA. Letman, A. Mathur, A. Schelten, A. Vaughan et al., “The llama 3\nherd of models,” arXiv preprint arXiv:2407.21783, 2024.\n"}]}