{"doc_id": "arxiv:2601.03792", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03792.pdf", "meta": {"doc_id": "arxiv:2601.03792", "source": "arxiv", "arxiv_id": "2601.03792", "title": "VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation", "authors": ["Huynh Trung Kiet", "Dao Sy Duy Minh", "Nguyen Dinh Ha Duong", "Le Hoang Minh Huy", "Long Nguyen", "Dien Dinh"], "published": "2026-01-07T10:49:56Z", "updated": "2026-01-07T10:49:56Z", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03792v1", "url_pdf": "https://arxiv.org/pdf/2601.03792.pdf", "meta_path": "data/raw/arxiv/meta/2601.03792.json", "sha256": "cb001b7e363ec929e2101452109701e481e8629e98f1fb49212f04a92ea4a4b5", "status": "ok", "fetched_at": "2026-02-18T02:22:28.912517+00:00"}, "pages": [{"page": 1, "text": "VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for\nVietnamese Traditional Medicine Evaluation\nHuynh Trung Kiet1,2,†, Dao Sy Duy Minh1,2,†, Nguyen Dinh Ha Duong1,2,†,\nLe Hoang Minh Huy1,2,†, Long Nguyen1,2,∗, Dien Dinh1,2\n1Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam\n2Vietnam National University, Ho Chi Minh City, Vietnam\n†Equal contribution. ∗Corresponding author.\n{23122039,23122041,23122002,23122033}@student.hcmus.edu.vn\n{nhblong,ddien}@fit.hcmus.edu.vn\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated remarkable proficiency in general med-\nical domains. However, their performance sig-\nnificantly degrades in specialized, culturally\nspecific domains such as Vietnamese Tradi-\ntional Medicine (VTM), primarily due to the\nscarcity of high-quality, structured benchmarks.\nIn this paper, we introduce VietMed-MCQ,\na novel multiple-choice question dataset gen-\nerated via a Retrieval-Augmented Generation\n(RAG) (Lewis et al., 2020) pipeline with an au-\ntomated consistency check mechanism. Unlike\nprevious synthetic datasets, our framework in-\ncorporates a dual-model validation approach to\nensure reasoning consistency through indepen-\ndent answer verification, though the substring-\nbased evidence checking has known limita-\ntions (detailed in Section 6). The complete\ndataset of 3,190 questions spans three diffi-\nculty levels (Easy: 22.0%, Medium: 51.8%,\nHard: 26.3%) and underwent validation by\none medical expert and four students, achiev-\ning 94.2% approval with substantial inter-rater\nagreement (Fleiss’ κ = 0.82). We benchmark\nseven open-source models on VietMed-MCQ.\nSurprisingly, results show that general-purpose\nmodels like Qwen2.5 outperform Vietnamese-\ncentric models, yet all struggle with complex\ndiagnostic reasoning. Our code and dataset\nare publicly available to foster research in low-\nresource medical domains.\n1\nIntroduction\nSynthetic data generation has emerged as a criti-\ncal technique for addressing data scarcity in spe-\ncialized domains, yet maintaining factual fidelity\nremains a fundamental challenge. While LLM-\ngenerated datasets have proliferated across NLP\ntasks, medical applications demand rigorous qual-\nity control due to the risk of hallucinated content\n(Ji et al., 2023). This challenge intensifies in low-\nresource medical domains like Vietnamese Tradi-\ntional Medicine (VTM), where knowledge exists\nprimarily in unstructured textbooks rather than cu-\nrated databases (Johnson et al., 2023).\nWe address three core NLP challenges through\nthis work:\n(1) Hallucination mitigation in\nsynthetic data generation via automated dual-\nmodel consensus verification that filters 7.78%\nof generated samples without human labeling;\n(2) Cross-lingual knowledge transfer by inves-\ntigating whether conceptual overlap (shared Sino-\nVietnamese medical terminology) enables models\ntrained on Traditional Chinese Medicine to general-\nize to Vietnamese contexts; (3) Quality-controlled\nbenchmark creation through a teacher-student dis-\ntillation pipeline (Hinton et al., 2015) that produces\n3,190 clinically validated questions from unstruc-\ntured corpora, achieving 94.2% expert approval\n(Fleiss’ κ = 0.82) (Landis and Koch, 1977).\nOur methodology combines retrieval-augmented\ngeneration (Lewis et al., 2020) with automated con-\nsistency filtering: a 70B teacher model (Llama-3.1-\n70B) generates context-referenced questions, while\na 32B student model (Qwen2.5-32B) indepen-\ndently validates reasoning consistency. This dual-\nverification achieves 92.22% retention (3,190 from\n3,459 candidates), demonstrating that multi-model\nconsensus can serve as a scalable quality gate for\nsynthetic data pipelines, though the substring-based\nevidence verification has known limitations (Sec-\ntion 6). To address positional answer bias inher-\nent in LLM generation (50.3% Option B), we em-\nploy randomized option shuffling during evalua-\ntion to ensure positional invariance. Evaluation of\nseven representative models reveals a counterintu-\nitive finding: models with Chinese language priors\nsubstantially outperform Vietnamese-specialized\nvariants (+7.21% accuracy), suggesting that cross-\nlingual conceptual transfer from related medical\ntraditions dominates language-specific fine-tuning\nfor culturally-rooted knowledge domains. These\nfindings have broader implications for multilin-\ngual NLP in specialized domains where linguistic\narXiv:2601.03792v1  [cs.CL]  7 Jan 2026\n"}, {"page": 2, "text": "boundaries do not align with conceptual bound-\naries.\n2\nRelated Work\nMedical QA Benchmarks.\nEarly medical bench-\nmarks focused on Western medicine, such as\nMedQA (Jin et al., 2021) and PubMedQA (Jin\net al., 2019). Recent work has expanded to multilin-\ngual and regional contexts, including AfriMedQA\n(Tonja et al., 2024) for African languages and\nMMedBench (Zhang et al., 2024a) for multilingual\nevaluation. However, traditional medicine systems\nremain underrepresented.\nMCQ Format Considerations.\nRecent studies\n(Chen et al., 2024) have shown that MCQ formats\ncan overestimate model capabilities compared to\nfree-response evaluation, as models may exploit\nsurface patterns or positional biases. We address\nthis through explicit de-biasing (option shuffling)\nand plan to release a free-response variant in future\nwork.\nTraditional Chinese Medicine (TCM) Re-\nsources.\nTCM-specific benchmarks like Bian-\nCang (Guo et al., 2023) and TCM-Ladder (Li et al.,\n2024) demonstrate the value of domain-specialized\nevaluation. Given the conceptual overlap between\nTCM and VTM, we investigate whether TCM-prior\nmodels transfer effectively to Vietnamese contexts.\nSynthetic Data Generation.\nOur teacher-student\npipeline builds on knowledge distillation (Hinton\net al., 2015) and retrieval-augmented generation\n(Lewis et al., 2020). Similar consistency filter-\ning approaches have been explored in MedRGAG\n(Zhang et al., 2024b) for reducing hallucinations in\nbiomedical text generation.\n3\nMethodology\nOur proposed framework, VietMed-Gen, oper-\nates on a generator-validator pipeline designed to\nsynthesize high-fidelity multiple-choice questions\n(MCQs) from unstructured medical texts.1 As illus-\ntrated in Figure 1, the process comprises three dis-\ntinct phases: (1) Context Extraction, (2) Teacher-\nGuided Generation, and (3) Student-Based Consis-\ntency Filtering.\n1We use \"Teacher-Student\" terminology to denote a\nproducer-verifier architecture where the Student independently\nvalidates outputs, rather than knowledge distillation in the tra-\nditional sense (Hinton et al., 2015).\n3.1\nData Acquisition and Chunking\nWe curated a corpus of authoritative Vietnamese\nTraditional Medicine (VTM) textbooks and clin-\nical guidelines. Documents were segmented into\ncoherent text chunks containing specific medical\nknowledge (e.g., herbal properties, diagnostic pro-\ntocols). Let D be the document corpus; we decom-\npose it into a set of contexts C = {c1, c2, ..., cn},\nwhere each ci represents a localized knowledge\nunit. Chunking parameters are detailed in Ap-\npendix D.\n3.2\nTeacher-Guided Generation (MT )\nWe designate a large-scale instruction-tuned model,\nspecifically Llama-3.1-70B-Instruct-AWQ, as the\nTeacher (MT ). For each context ci, we construct a\nprompt Pgen that instructs the Teacher to generate\na tuple T = (q, O, a, r, e), where:\n• q: The medical question stem.\n• O: The set of four options {A, B, C, D}.\n• a: The correct answer key (a ∈O).\n• r: The reasoning chain explaining why a is\ncorrect.\n• e: The specific evidence span quoted directly\nfrom ci.\nThe generation process is formalized as:\n(q, O, a, r, e) ∼MT (Pgen|ci; θ)\n(1)\nwhere θ denotes sampling hyperparameters opti-\nmized for factual accuracy (see Appendix D for full\nconfiguration). We enforce structured JSON output\nthrough schema-constrained generation.\n3.3\nStudent-Based Consistency Filtering\n(MS)\nA major challenge in synthetic data generation is\n\"hallucination,\" where the generated question is\nunanswerable or the answer key is incorrect. To\nmitigate this, we introduce a Consistency Filter us-\ning a Student model (MS), specifically Qwen2.5-\n32B-Instruct-AWQ. The Student operates as an\nindependent verifier with conservative sampling to\nenforce stricter reasoning validation.\nThe validation process involves two rigorous\nchecks:\n"}, {"page": 3, "text": "Figure 1: The VietMed-MCQ Data Synthesis Framework. We employ a Teacher model (MT ) to generate\ncandidate questions from medical contexts, followed by a Student model (MS) that validates the answers blindly.\nOnly samples achieving teacher-student consensus are retained.\n1. Answer Consistency (Reasoning Check).\nWe\nfeed the generated question q, options O, and the\noriginal context ci to the Student model, masking\nthe Teacher’s answer a. The Student predicts its\nown answer ˆa:\nˆa = MS(q, O|ci)\n(2)\nA question is deemed reasoning-consistent if and\nonly if ˆa = a. This simulates a \"double-blind\"\nreview process.\n2. Evidence Grounding (Fact Check).\nWe ver-\nify the faithfulness of the generated evidence e.\nWe calculate the string overlap between e and the\nsource context ci.\nValid(e) = I(e ⊆ci)\n(3)\nWhere I is the indicator function. Limitation: This\nsubstring-containment check is computationally ef-\nficient but has significant limitations-it cannot ver-\nify semantic entailment, reasoning correctness, or\nevidence sufficiency. It only ensures that the cited\ntext exists in the source, not that it adequately sup-\nports the answer. Future work will explore semantic\nentailment verification using Natural Language In-\nference models (Bowman et al., 2015) or retrieval-\naugmented fact-checking to strengthen grounding\n(see Section 6).\nOnly samples that satisfy both conditions are\nadded to the final VietMed-MCQ dataset.\n4\nThe VietMed-MCQ Dataset\n4.1\nDataset Statistics\nAfter the automated consistency filtering pipeline,\nwe obtained 3,190 high-quality questions. Table 1\ndetails the descriptive statistics of the corpus.\nMetric\nValue\nPipeline Configuration\nTeacher Model\nLlama-3.1-70B\nStudent Model\nQwen2.5-32B\nChunk Size\n2000\nChunk Overlap\n200\nDataset Statistics\nInitial Candidates\n3,459\nFiltered Out\n269 (7.78%)\nFinal Questions\n3,190\nMissing Values\n0\nHuman Validation\nEvaluators\n5 total\nQuestions Validated\n500\nAccept Rate\n94.2%\nMinor Revision\n4.1%\nRejected/Flagged\n1.7%\nFleiss’ κ\n0.82\nText Lengths (Avg.)\nQuestion Stem\n59.7\nEvidence Context\n86.4\nExplanation\n97.0\nStudent Reasoning\n421.3\nTable 1: Descriptive statistics of VietMed-MCQ. No-\ntably, the Student Reasoning length significantly exceeds\nthe generated explanation, indicating rich inferential\ntraces during the validation phase.\n"}, {"page": 4, "text": "4.2\nHuman Expert Validation\nTo assess clinical correctness and educational value\npost-hoc, we conducted rigorous human validation\nusing a stratified random sample of 500 questions\n(15.7% of the 3,190-question dataset) evaluated by\none medical expert and four students. This valida-\ntion serves as a quality indicator for the automati-\ncally filtered dataset, not as an additional filtering\nstep. The sample was stratified by difficulty level\nand answer key to ensure representativeness. Each\nevaluator independently assessed questions across\nfour dimensions:\n1. Factual Correctness: Whether the correct\nanswer is medically accurate according to\ncanonical VTM texts.\n2. Distractor Quality: Whether incorrect op-\ntions are plausible but clearly distinguishable.\n3. Clinical Relevance: Whether the question\ntests practical medical knowledge.\n4. Language Quality: Whether the question is\ngrammatically correct and unambiguous.\nEvaluators rated each question as Accept, Mi-\nnor Revision, or Reject. Questions receiving at\nleast three Accept votes (majority consensus) were\nretained in the final dataset.\nImportant: The\n500 validation questions were sampled from the\nalready-filtered 3,190-question dataset for quality\nassessment only-no questions were removed based\non validation results, as the sample serves as a\npost-hoc quality indicator rather than a filtering\nstep. Results show that 94.2% (471/500) of sam-\npled questions were accepted without modification,\n4.1% (21 questions) required only minor terminol-\nogy standardization issues that do not affect cor-\nrectness, and only 1.7% (8 questions) were flagged\nfor potential substantive issues. Inter-rater reliabil-\nity measured by Fleiss’ Kappa was 0.82, indicating\nsubstantial agreement (Landis and Koch, 1977).\nCommon issues identified included: ambiguous\ndistractor phrasing (38%), outdated terminology\n(29%), and overly technical language (21%), as il-\nlustrated in Figure 2. The high approval rate across\nthe stratified sample provides strong evidence of\noverall dataset quality. The complete validation\nprotocol is documented in Appendix A.\nThis validation confirms that our automated\nteacher-student pipeline produces high-quality\nquestions suitable for educational assessment and\nmodel evaluation, with the majority passing expert\nscrutiny without human intervention.\nComparison with Medical Benchmarks.\nTa-\nble 2 compares our validation metrics with other\nmedical QA benchmarks. VietMed-MCQ achieves\nthe highest expert approval rate among synthetic\ndatasets and approaches the quality of human-\nauthored benchmarks, while maintaining substan-\ntially higher inter-rater agreement than most com-\nparable works.\nBenchmark\nType\nApproval\nAgreement\nMedQA\nHuman\n98.5%\nN/A\nPubMedQA\nHuman\n96.3%\nN/A\nAfriMedQA\nSynthetic\n85.2%\nκ = 0.71\nEMSQA\nSynthetic\n89.4%\nN/A\nFreeMedQA\nSynthetic\n88.9%\nκ = 0.74\nMMedBench\nMixed\n91.7%\nN/A\nVietMed-MCQ\nSynthetic\n94.2%\nκ = 0.82\nTable 2: Comparison of expert validation metrics across\nmedical QA benchmarks. VietMed-MCQ achieves the\nhighest approval rate among synthetic datasets with sub-\nstantial inter-rater agreement.\nFigure 2: Distribution of issues identified in human\nvalidation. Among the 29 flagged questions (5.8% of\nthe 500 validation sample), the most common issues\nwere ambiguous distractor phrasing (38%), outdated\nmedical terminology (29%), and overly technical lan-\nguage requiring clarification (21%). Remaining 12%\nincluded factual errors or multiple correct answers.\n4.3\nQuality Metrics\nReasoning Depth.\nA distinguishing feature of\nour framework is the capture of validation traces.\nAs shown in Table 1, the average Student Rea-\nsoning length is 421.3 characters, approximately\n4.3× longer than the Teacher’s initial explanations\n(97.0 characters). This suggests that the validation\nmodel generates detailed justifications during vali-\ndation, though we cannot definitively claim explicit\n"}, {"page": 5, "text": "Chain-of-Thought reasoning without analysis of the\ninternal reasoning process (Wei et al., 2022).\nDifficulty Distribution.\nWe categorize questions\ninto three difficulty levels: Easy (22.0%, 701\nquestions), Medium (51.8%, 1,651 questions), and\nHard (26.3%, 838 questions). The difficulty labels\nwere assigned during dataset generation based on\nquestion complexity features (question length, evi-\ndence length, medical terminology density), then\nrebalanced to achieve the reported distribution.2\nThis distribution enables comprehensive evalua-\ntion across difficulty levels, from basic medical\nknowledge recall to complex diagnostic reasoning.\nPositional Bias Analysis.\nWe analyzed the dis-\ntribution of answer keys (A, B, C, D) to assess\npotential biases. As illustrated in Figure 3, we\nobserve a positional bias where Option B is the\ncorrect answer in 50.3% of cases, followed by A\n(22.4%), C (20.1%), and D (7.2%). This bias is in-\nherent in LLM generation patterns and represents a\ncommon phenomenon in synthetic datasets (Zheng\net al., 2023).\nFigure 3: Distribution of answer keys. The prevalence\nof Option B (50.3%) highlights a generation bias com-\nmon in LLMs. We mitigate this through randomized\noption shuffling during evaluation to ensure positional\ninvariance.\nTo address this in our benchmarks (Section ??),\nwe apply randomized option shuffling during in-\nference: for each sample, we permute options\n{A, B, C, D} uniformly and update the ground-\ntruth label accordingly. This ensures that models\ncannot exploit positional patterns. We validate de-\nbiasing effectiveness by comparing against naive\nbaselines: Random Guess (expected 25% accuracy)\nand Original Position B (theoretically 50.3% if ex-\nploited). All reported results use shuffled data to\nensure complete positional invariance.\n2The final difficulty distribution reflects manual balancing\nrather than automatic classification. While we provide the\ncomplexity features used, exact thresholds are not reproducible\nas they were adjusted to achieve target proportions.\n5\nExperiments\n5.1\nExperimental Setup\nTo assess the capability of current LLMs on Viet-\nnamese Traditional Medicine, we benchmarked\nseven representative models:\n• Vietnamese-Optimized (7B): VinaLlama-\n7B-Chat (Tran et al., 2023), Vistral-7B-Chat\n(Nguyen and Nguyen, 2024).\n• General Purpose (7B-8B): Llama-3-8B-\nInstruct\n(AI@Meta,\n2024),\nMistral-7B-\nInstruct-v0.3 (Jiang et al., 2023), Qwen2.5-\n7B-Instruct (Qwen Team, 2024).\n• Large-Scale (30B+): Qwen2.5-32B-Instruct\n(Qwen Team, 2024), Llama-3.1-70B-AWQ\n(AI@Meta, 2024).\nImplementation Details.\nAll models were eval-\nuated in a zero-shot setting to test intrinsic knowl-\nedge without task-specific fine-tuning. We em-\nployed a consistent chat template with a system\nprompt enforcing strict output formatting. Infer-\nence was performed using 4-bit quantization (NF4)\nvia bitsandbytes (Dettmers et al., 2023) with batch\nsize = 256 and padding_side=\"left\" to simulate\nresource-constrained deployment scenarios. Gen-\neration parameters: temperature = 0.01 (greedy\ndecoding), max_new_tokens = 256, ensuring re-\nproducibility while allowing sufficient response\nlength.\n5.2\nResults and Analysis\nTable 3 summarizes the zero-shot performance\nacross all models. We report 95% confidence in-\ntervals computed via bootstrap resampling (10,000\niterations). Statistical significance is assessed us-\ning McNemar’s test (p < 0.01) for pairwise model\ncomparisons.\nThe \"Language vs. Knowledge\" Paradox.\nA\nstriking finding is that Qwen2.5-7B, a model with\nstrong Chinese priors, significantly outperforms\nVietnamese-centric models, achieving an accuracy\nof 62.01% versus 54.80% (Vistral) and 55.17%\n(VinaLlama). This +7.21% gap over the best Viet-\nnamese model demonstrates that conceptual knowl-\nedge dominates language-specific fine-tuning in\nspecialized domains.\nWe hypothesize that this is due to the conceptual\noverlap between Vietnamese Traditional Medicine\n(VTM) and Traditional Chinese Medicine (TCM)\n"}, {"page": 6, "text": "Model\nBackbone\nAccuracy (%)\nMacro F1\nValidity (%)\nVietnamese-Optimized Models\nVinaLlama-7B-Chat\nLlama-2\n55.17 ± 0.9\n0.5316 ± 0.011\n100.0\nVistral-7B-Chat\nMistral\n54.80 ± 0.9\n0.5105 ± 0.011\n100.0\nGeneral Purpose Models (7B-8B)\nLlama-3-8B-Instruct\nLlama-3\n58.87 ± 0.8\n0.5548 ± 0.010\n100.0\nMistral-7B-Instruct-v0.3\nMistral\n45.39 ± 2.7\n0.3060 ± 0.029\n9.5\nQwen2.5-7B-Instruct\nQwen2.5\n62.01∗∗∗± 0.9\n0.5958 ± 0.010\n100.0\nLarge-Scale Models\nQwen2.5-32B-Instruct\nQwen2.5\n64.58 ± 0.8\n0.5982 ± 0.009\n100.0\nLlama-3.1-70B-AWQ\nLlama-3.1\n63.95 ± 0.8\n0.4737 ± 0.008\n100.0\nRandom Baseline\n–\n25.0 ± 0.9\n0.2500 ± 0.009\n–\nTable 3: Zero-shot performance on VietMed-MCQ with 95% confidence intervals (N=3,190). ∗∗∗indicates statistical\nsignificance (p < 0.001) compared to all other 7B-8B models via McNemar’s test. Mistral-7B shows low validity\n(9.5%) due to frequent formatting errors.\n(Pham and Nguyen, 2020; Nguyen and Le, 2018).\nSince VTM terminology (e.g., Am Duong for Yin-\nYang, Ngu Hanh for Five Elements) shares Sino-\nVietnamese roots with Chinese concepts present in\nQwen’s pre-training corpus, the model effectively\nperforms \"Cross-Lingual Knowledge Transfer\" (Hu\net al., 2020; Conneau et al., 2020), compensating\nfor its lower Vietnamese fluency.\nScale Matters:\nLarge Model Performance.\nLarge-scale models demonstrate substantial im-\nprovements.\nQwen2.5-32B achieves the high-\nest accuracy (64.58%), representing a +2.57%\nimprovement over its 7B counterpart. Notably,\nLlama-3.1-70B-the Teacher model used in dataset\ngeneration-achieves 63.95% accuracy, validating\nthat the benchmark poses genuine challenges even\nfor models that contributed to its creation. How-\never, its relatively low Macro F1 (0.4737) suggests\nprediction bias toward majority classes.\nAccuracy-F1 Discrepancy.\nWhile Qwen2.5-7B\nleads in accuracy, Llama-3-8B achieves compet-\nitive Macro F1 (0.5548), only 0.041 lower than\nQwen2.5-7B’s 0.5958. This suggests Llama-3-8B\nmaintains better class balance in predictions, mak-\ning it more robust for minority answer options de-\nspite slightly lower overall accuracy.\nValidity\nIssues\nwith\nMistral.\nMistral-7B-\nInstruct-v0.3 shows critical limitations,\nwith\nonly 9.5% validity (304/3,190 valid outputs)\nin zero-shot evaluation.\nThe model frequently\nfails to follow the strict output format required\nfor MCQ tasks, producing verbose explanations\ninstead of single-letter answers. This highlights\na crucial consideration for benchmark design:\ninstruction-following capability is a prerequisite\nfor evaluation in constrained formats.\n5.3\nFew-Shot Learning Analysis\nTo assess in-context learning capabilities, we eval-\nuate all models in a 3-shot setting with exemplars\nrandomly sampled from the training set. Table 4\npresents comprehensive results.\nMixed Few-Shot Effects.\nThe impact of few-\nshot examples varies dramatically across mod-\nels. Vistral-7B shows positive transfer (+2.19%\naccuracy), suggesting effective in-context learn-\ning. In contrast, Llama-3-8B exhibits significant\ndegradation (-8.12% accuracy), potentially due to\nconfusion from Vietnamese exemplars conflict-\ning with its primarily English instruction-tuning.\nVinaLlama-7B also degrades (-2.94%), indicating\nlimited in-context learning capacity.\nNotably, Qwen2.5-7B maintains stable perfor-\nmance (+0.47%), suggesting its strong zero-shot\ncapabilities already capture relevant patterns. Large\nmodels show minimal changes: Qwen2.5-32B\n(+0.90%) and Llama-3.1-70B (-0.41%) remain\nnear their zero-shot baselines, indicating that scale\nreduces dependency on in-context exemplars.\nFormat Compliance Improvement.\nMistral-\n7B’s validity improves substantially from 9.5%\n(zero-shot) to 55.3% (3-shot), demonstrating that\nexamples help models understand output con-\nstraints.\nHowever, performance remains weak\n(43.31% accuracy), suggesting that format com-\npliance alone is insufficient without underlying\nmedical knowledge.\n"}, {"page": 7, "text": "Model\nBackbone\nAccuracy (%)\nMacro F1\nValidity (%)\nVietnamese-Optimized Models\nVinaLlama-7B-Chat\nLlama-2\n52.23 ± 0.9\n0.5031 ± 0.011\n100.0\nVistral-7B-Chat\nMistral\n56.99 ± 0.9\n0.5603 ± 0.010\n100.0\nGeneral Purpose Models (7B-8B)\nLlama-3-8B-Instruct\nLlama-3\n50.75 ± 0.9\n0.4821 ± 0.009\n100.0\nMistral-7B-Instruct-v0.3\nMistral\n43.31 ± 1.1\n0.3759 ± 0.011\n55.3\nQwen2.5-7B-Instruct\nQwen2.5\n62.48 ± 0.9\n0.5993 ± 0.009\n100.0\nLarge-Scale Models\nQwen2.5-32B-Instruct\nQwen2.5\n65.48 ± 0.8\n0.6115 ± 0.009\n100.0\nLlama-3.1-70B-AWQ\nLlama-3.1\n63.54 ± 0.8\n0.4701 ± 0.036\n100.0\nTable 4: 3-shot performance on VietMed-MCQ with 95% confidence intervals (N=3,190). Mistral validity improved\nto 55.3% with examples.\nFigure 4: Comparative performance of evaluated models\nacross zero-shot and 3-shot settings. The significant gap\nbetween Qwen2.5 and others highlights the impact of\ndomain-specific pre-training knowledge over general\nlanguage adaptation.\n5.4\nError Analysis\nQualitative analysis of model predictions reveals\nsystematic error patterns.\nVietnamese models\n(VinaLlama, Vistral) struggled significantly with\ndistinguishing between specific herbal functions,\noften selecting answers based on common lan-\nguage patterns rather than medical logic. In con-\ntrast, Qwen2.5 models demonstrated better dis-\ncrimination of TCM-related terminology due to\ncross-lingual knowledge transfer. This emphasizes\nthat for specialized domains, instruction tuning on\ngeneral data (as done in Vistral/VinaLlama) is in-\nsufficient without domain-specific knowledge in-\njection. Detailed per-class performance breakdown\nis available in the supplementary materials.\n6\nConclusion\nIn this work, we introduced VietMed-MCQ, the\nfirst comprehensive benchmark dedicated to Viet-\nnamese Traditional Medicine (VTM). By synergiz-\ning retrieval-augmented generation with a teacher-\nstudent consistency mechanism and rigorous expert\nvalidation (94.2% approval rate, substantial inter-\nrater agreement), we successfully transformed un-\nstructured medical texts into a clinically verified,\nhigh-fidelity dataset of 3,190 questions. Our ex-\ntensive benchmarking reveals that while general-\npurpose models like Qwen2.5 exhibit promising\ncross-cultural transfer capabilities due to shared\nSino-Vietnamese medical concepts, they still lack\nthe nuanced reasoning required for accurate diag-\nnosis and prescription. We hope VietMed-MCQ\nwill serve as a catalyst for future research in low-\nresource medical NLP and culturally-aware LLM\nevaluation.\nLimitations\nEvidence Grounding.\nOur evidence grounding\nmechanism relies on substring containment (e ⊆\nci), which has significant limitations: (1) it cannot\nverify semantic entailment-the evidence may exist\nin context but not logically support the answer; (2)\nit cannot assess reasoning correctness-the inferen-\ntial chain from evidence to answer is not validated;\n(3) it cannot check evidence sufficiency-partial or\nout-of-context quotes may pass verification. This\nrepresents a fundamental limitation of the current\npipeline. While the dual-model consensus (92.22%\nagreement) and human validation (94.2% approval)\nsuggest the approach produces reasonable quality,\nthe evidence verification step should be considered\na necessary but insufficient quality gate. Future\nwork should explore NLI-based entailment verifi-\ncation (Bowman et al., 2015) or automated fact-\nchecking to strengthen factual fidelity.\nDataset Characteristics.\nThe dataset exhibits\npositional answer bias (50.3% Option B), a com-\nmon pattern in LLM-generated datasets (Zheng\net al., 2023). We employ inference-time option\nshuffling to mitigate positional effects during eval-\n"}, {"page": 8, "text": "uation. Future work could explore bias mitigation\nduring generation (e.g., explicit answer position\nconstraints in prompts) to produce more naturally\nbalanced distributions.\nSource Material Scope.\nOur source corpus pri-\nmarily consists of foundational VTM textbooks. In-\ncorporating clinical case studies, differential diag-\nnosis scenarios, and contemporary treatment proto-\ncols would further increase the proportion of Hard\nquestions and enhance benchmark comprehensive-\nness for real-world applicability.\nMCQ Format Limitations.\nFollowing Chen\net al. (2024), MCQ benchmarks may overestimate\nreasoning capabilities. Future iterations will in-\nclude free-response variants to better assess clinical\nreasoning depth.\nFuture Work\nMultimodal Extensions.\nThe dataset is purely\ntextual, omitting diagnostic cues such as pulse\npatterns, tongue images, and patient examination\nrecords common in VTM practice. Multimodal\nquestion generation would enable more realistic\nclinical evaluation.\nThe proportion of Hard\nquestions (26.3\nAdvanced Quality Assurance.\nLongitudinal\nvalidation with student performance data would\nprovide ecological validity for educational applica-\ntions. NLI-based entailment verification (Bowman\net al., 2015) and retrieval-augmented fact-checking\ncould further strengthen factual fidelity beyond our\ncurrent substring containment approach.\nExpanded Evaluation Scope.\nWe focus on zero-\nshot evaluation with 7B-8B models.\nFuture\nwork should explore few-shot prompting, chain-\nof-thought reasoning (Wei et al., 2022), retrieval-\naugmented inference, and larger proprietary models\n(GPT-4, Claude) to fully characterize benchmark\ndifficulty. Ablation studies isolating the student fil-\nter’s impact on item quality would also strengthen\nmethodological claims. Free-response variants and\ncase-based diagnostic tasks would better assess\nclinical reasoning depth beyond MCQ format limi-\ntations.\nAcknowledgments\nWe thank the medical expert and four students\nwho dedicated their time to validating this dataset.\nWe also acknowledge computational resources pro-\nvided by [Institution Name] for running the teacher-\nstudent pipeline.\nA\nHuman Validation Protocol\nA.1\nEvaluator Qualifications\nWe recruited one medical expert and four students\nto ensure diverse perspectives in evaluation cov-\nerage. The medical expert holds a valid medi-\ncal license and specializes in Vietnamese Tradi-\ntional Medicine, with expertise in herbal pharma-\ncology, diagnostic methods, and clinical practice.\nThe four student evaluators are currently pursu-\ning advanced studies in medicine and traditional\nmedicine, providing complementary perspectives\non question clarity, difficulty appropriateness, and\neducational value. This combination of expert clin-\nical judgment and student-level assessment ensures\nthe dataset is suitable for both evaluation and edu-\ncational purposes.\nA.2\nAnnotation Guidelines\nEach evaluator independently assessed questions\nusing a structured rubric across four quality di-\nmensions. Factual Correctness (critical) verified\nwhether the designated answer aligns with canon-\nical VTM texts and clinical practice, rating each\nquestion as Correct, Partially Correct, or Incor-\nrect. Distractor Quality (important) evaluated\nwhether incorrect options are plausible yet distin-\nguishable, represent common misconceptions, and\navoid obvious clues such as grammatical incon-\nsistencies, with ratings of Excellent, Adequate, or\nPoor. Clinical Relevance (important) assessed\nwhether questions test practical knowledge appli-\ncable to VTM practice at appropriate difficulty\nlevels (Highly Relevant, Moderately Relevant, or\nNot Relevant). Finally, Language Quality (sec-\nondary) checked grammatical correctness, termi-\nnology consistency, and stem clarity (Excellent,\nMinor Issues, or Major Issues).\nBased on these dimensions, evaluators assigned\none of three overall judgments: Accept (all criteria\nmet, no changes needed), Minor Revision (accept-\nable quality but requires small edits), or Reject\n(fails critical criteria). Questions receiving at least\ntwo Accept votes were retained; those with mixed\nreviews underwent adjudication.\n"}, {"page": 9, "text": "A.3\nAnnotation Process\nValidation proceeded in three phases. During the\ntraining phase, all five evaluators independently\nannotated 50 pilot questions, then met to discuss\ndisagreements and refine criteria interpretation.\nInitial inter-rater agreement (κ = 0.71) improved to\nκ = 0.79 after calibration. In the main annotation\nphase, each evaluator independently assessed the\nremaining 450 questions from the stratified sample\nover four weeks, with weekly check-ins to address\nprocedural questions. The annotation interface ran-\ndomized question order and masked AI-generated\nexplanations to prevent anchoring bias. For ad-\njudication, questions with split decisions under-\nwent synchronous discussion where evaluators re-\nviewed evidence spans, consulted reference texts\nwhen needed, and reached consensus via major-\nity vote (at least 3 of 5 agreeing). Final inter-rater\nreliability was Fleiss’ κ = 0.82.\nB\nExample Questions\nB.1\nEasy Difficulty Examples\nQ1 (Easy): Khi trẻmới tập ăn dặm, nên pha bột\nnhư thếnào?\n(When a child is just starting solid foods, how\nshould powder be mixed?)\nA. Pha bột quá đặc (Mix powder too thick)\nB. Pha bột loãng vừa phải\n✓(Mix powder mod-\nerately thin)\nC. Pha bột quá loãng (Mix powder too thin)\nD. Không pha bột (Don’t mix powder)\nEvidence: “Không nên pha bột quá đặc khi trẻ\nmới tập ăn dặm.”\nExplanation: Tests basic knowledge of traditional\nVietnamese medicine regarding proper feeding\npractices for young children.\nQ2 (Easy): Cách chữa đái dắt bằng lá cây nào?\n(Which plant leaves are used to treat urinary tract\ninfection?)\nA. Lá chè xanh\n✓(Green tea leaves)\nB. Lá duối (Duối leaves)\nC. Lá cà gai leo (Cà gai leo leaves)\nD. Lá mò trắng (White mò leaves)\nEvidence: “Lấy một nấm lá chè xanh rửa sạch,\nvẩy ráo nước sau đó mang giã nhỏ, rồi đổnước\nđun sôi vào.”\nExplanation:\nTests knowledge of traditional\nherbal remedies for common ailments.\nB.2\nMedium Difficulty Examples\nQ3 (Medium): Cách điều trịchứng co giật do sốt\nởtrẻlà gì?\n(What is the treatment method for fever-induced\nseizures in children?)\nA. Tắm bằng nước có nhiệt độcao (Bath with high\ntemperature water)\nB. Tắm bằng nước có nhiệt độthấp hơn 2 độso\nvới thân nhiệt của trẻ\n✓(Bath with water 2 de-\ngrees lower than child’s body temperature)\nC. Cho trẻuống thuốc kháng sinh (Give antibi-\notics)\nD. Cho trẻăn nhiều đường (Feed child more\nsugar)\nEvidence: “Tắm cho trẻbằng nước có nhiệt độ\nthấp hơn 2 độso với thân nhiệt của trẻ(có thểtắm\nnhiều lần như vậy).”\nExplanation: Tests understanding of fever man-\nagement techniques in traditional Vietnamese pe-\ndiatric medicine.\nQ4 (Medium): Cách chữa bệnh lang ben là gì?\n(What is the treatment method for Tinea versi-\ncolor?)\nA. Đắp kem Ciconten Plus lên vùng da bịbệnh\n(Apply Ciconten Plus cream)\nB. Trộn củriềng với rượu trắng nồng độcao và\nđắp lên chỗbịbệnh\n✓(Mix galangal root with\nhigh-proof alcohol and apply)\nC. Ăn sống hoặc giã nhỏtôi pha với nước đun\nsôi đểnguội uống (Eat raw or crush garlic with\nboiled water)\nD. Uống 49 hạt đậu đen xanh lòng mỗi buổi sáng\n(Drink 49 black beans each morning)\nEvidence: “Hãy lấy củriềng rửa sạch, giã nhỏ\nvà trộn với rượu trắng nồng độcao (loại đểngâm\nthuốc) sao cho sền sệt, rồi đắp lên chỗbịbệnh.”\nExplanation: Tests knowledge of traditional treat-\nments for skin conditions using natural remedies.\nB.3\nHard Difficulty Examples\nQ5 (Hard): Cách điều trịbệnh bìu dái quá lớn ở\ntrẻem dưới 1 tuổi là gì?\n(What is the treatment for excessively large scro-\ntum in children under 1 year old?)\nA. Đắp bột khô (Apply dry powder)\nB. Ngâm bìu dái vào nước ấm và đốt lửa ngải\n✓\n(Soak in warm water and burn mugwort)\nC. Trộn Phục long can với lòng trắng trứng (Mix\nPhuc long can with egg white)\nD. Giã nhừ1 con giun đất lớn trộn với đường cát\ntrắng (Crush earthworm with white sugar)\nEvidence: “Trong vòng 1 tuổi, vào giờTý, ngày\nĐoan Dương (mồng 5 tháng 5) lấy thau đựng\nnước nóng đặt giữa nhà. Cho trẻngồi ngâm bìu\ndái vào nước ấm rồi bểtrẻđặt ngồi lên ngưỡng\ncửa, nước ởbìu dái sẽin ngắn (vết) trên ngưỡng\ncửa. Đốt lửa ngải 3 lần trên vết nước ấy, lúc đốt\nhơi bìu dái trên khói ngải, bìu dái sẽliền rút nhỏ\nlại ngay.”\nExplanation: Tests knowledge of complex tradi-\ntional remedies with specific timing and procedu-\nral steps.\nQ6 (Hard): Công dụng của bài thuốc trịcảm\nmạo, sưng phù, đau họng, chóng mặt, sốt, miệng\nlưỡi khô rát là gì?\n(What is the effect of the remedy for cold, swelling,\nsore throat, dizziness, fever, and dry mouth?)\nA. Thanh nhiệt, sơ phong, dưỡng âm (Clear heat,\ndisperse wind, nourish yin)\nB. Giải độc, tiêu thũng (Detoxify, reduce swelling)\nC. Thanh lợi yết hầu, tiêu thũng, sinh nước bọt\ngiải khát\n✓(Clear throat, reduce swelling, gen-\nerate saliva and quench thirst)\nD. Trịbệnh cước chân mùa rét (Treat cold feet in\nwinter)\nEvidence: “Có tác dụng thanh lợi yết hầu, tiêu\nthũng, sinh nước bọt giải khát.”\nExplanation: Tests understanding of complex\n"}, {"page": 10, "text": "therapeutic effects combining multiple medicinal\nproperties.\nC\nError Analysis Details\nAmong 29 flagged questions (5.8% of the 500 val-\nidation sample), we identified four primary issue\ncategories through systematic review. Ambigu-\nous distractor phrasing (11 questions, 38%) in-\nvolved non-standard terminology, regional vari-\nants, or overly similar options-for example, using\nboth synonymous terms for hypertension as sep-\narate choices. Outdated terminology (8 ques-\ntions, 29%) reflected archaic terms from older text-\nbooks or inconsistent mixing of Sino-Vietnamese\nand pure Vietnamese terms, such as alternating\nbetween equivalent expressions for wind-cold syn-\ndrome. Overly technical language (6 questions,\n21%) required specialized knowledge beyond typ-\nical practitioner training, including rare herb sub-\nspecies or obscure acupuncture point combinations.\nOther issues (4 questions, 12%) comprised factual\nerrors, multiple defensible answers, and grammati-\ncal problems. These 29 flagged questions included\n21 that underwent minor terminology standardiza-\ntion (counted in the 4.1% Minor Revision category)\nand 8 that were flagged for potential issues (1.7%\nRejected/Flagged).\nD\nImplementation Details\nD.1\nText Chunking Configuration\nWe utilized the RecursiveCharacterTextSplit-\nter (Chase, 2022) with the following parameters:\nchunk size = 2000 characters, overlap = 200 charac-\nters. This configuration balances context complete-\nness with token limit constraints while ensuring\nsmooth transitions between adjacent segments.\nD.2\nTeacher Model Sampling\nLlama-3.1-70B-Instruct-AWQ generation param-\neters:\n• Temperature: 0.6 (balances creativity with\nfactual accuracy)\n• Max tokens: 1024\n• Stop sequences: [“‘] (terminates at JSON clos-\ning delimiter)\n• Top-p: 0.9\nD.3\nStudent Model Sampling\nQwen2.5-32B-Instruct-AWQ validation parame-\nters:\n• Temperature: 0.2 (conservative sampling for\nstricter validation)\n• Max tokens: 768\n• Top-p: 0.85\nD.4\nJSON Output Schema\nWe enforce structured generation using the follow-\ning schema with one-shot exemplar prompting:\n{\n\"question\": str,\n\"options\": {\"A\": str, \"B\": str,\n\"C\": str, \"D\": str},\n\"answer\": str, // Must be A/B/C/D\n\"explanation\": str,\n\"evidence\": str // Quoted from context\n}\nReferences\nAI@Meta. 2024. Llama 3 model card. Meta AI Re-\nsearch.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nHarrison Chase. 2022. LangChain: Building appli-\ncations with LLMs through composability. https:\n//github.com/langchain-ai/langchain.\nGitHub\nRepository.\nYifan Chen, Ananya Pal, Hamid Palangi, and 1 others.\n2024. FreeMedQA: Benchmarking applied medical\nknowledge with free-response evaluations.\narXiv\npreprint arXiv:2405.09384.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. QLoRA: Efficient fine-\ntuning of quantized LLMs. In Advances in Neural\nInformation Processing Systems, volume 36, pages\n10088–10115.\n"}, {"page": 11, "text": "Yanqiao Guo, Xiaoqing Chen, and 1 others. 2023. Bian-\nCang: A traditional chinese medicine large language\nmodel. arXiv preprint arXiv:2310.15864.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalization. In Proceedings of the 37th International\nConference on Machine Learning, pages 4411–4421.\nPMLR.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hallu-\ncination in natural language generation. ACM Com-\nputing Surveys, 55(12):1–38.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, and 1 others.\n2023. Mistral 7B. arXiv preprint arXiv:2310.06825.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hsuan Weng,\nHanyi Fang, and Peter Szolovits. 2021. What dis-\nease does this patient have? a large-scale dataset\nwith symptom-to-diagnosis and treatment rationales.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n3404–3417, Online. Association for Computational\nLinguistics.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. PubMedQA: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567–2577,\nHong Kong, China. Association for Computational\nLinguistics.\nAlistair Johnson, Tom J. Pollard, and Roger G. Mark.\n2023. Biomedical text mining and natural language\nprocessing in low-resource languages. Journal of\nBiomedical Informatics, 138:104280.\nJ. Richard Landis and Gary G. Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nBiometrics, 33(1):159–174.\nPatrick Lewis,\nEthan Perez,\nAleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler,\nMike Lewis,\nWen-tau Yih,\nTim Rockt¨aschel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive NLP tasks. In Proceedings of\nthe 34th International Conference on Neural Informa-\ntion Processing Systems, pages 9459–9474. Curran\nAssociates Inc.\nZiyang Li, Mengzhou Wang, and 1 others. 2024. TCM-\nLadder:\nA benchmark for evaluating large lan-\nguage models on traditional chinese medicine. arXiv\npreprint arXiv:2402.08159.\nChien Van Nguyen and Liem Tan Nguyen. 2024. Vis-\ntral: Vietnamese Mistral 7B. https://huggingface.\nco/Viet-Mistral/Vistral-7B-Chat. Hugging Face\nModel Hub.\nVan Thanh Nguyen and Minh Hoang Le. 2018. Tradi-\ntional medicine knowledge representation and rea-\nsoning: A survey. In Proceedings of the 10th Asian\nConference on Intelligent Information and Database\nSystems, pages 365–375. Springer.\nQuoc Long Pham and Thi Hong Nguyen. 2020. Viet-\nnamese traditional medicine: Historical development\nand contemporary practice. Journal of Ethnophar-\nmacology, 252:112589.\nQwen Team. 2024. Qwen2.5 technical report. arXiv\npreprint arXiv:2409.12191.\nAtnafu Lambebo Tonja, Vijeta Mullachery, and 1 others.\n2024. AfriMed-QA: A pan-african multi-specialty\nmedical question answering benchmark. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing. Association for Com-\nputational Linguistics.\nQuan Tran, Long Phan, Tuan Vo, and Hieu Nguyen.\n2023. VinaLLaMA: LLaMA-based Vietnamese large\nlanguage models. arXiv preprint arXiv:2312.11011.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. Ad-\nvances in Neural Information Processing Systems,\n35:24824–24837.\nQian Zhang, Junnan Chen, and 1 others. 2024a. MMed-\nBench: A multilingual medical benchmark for large\nlanguage models. arXiv preprint arXiv:2402.09856.\nYucheng Zhang, Zhe Liu, and 1 others. 2024b. MedR-\nGAG: Retrieval-augmented generation with adap-\ntive selection for medical question answering. arXiv\npreprint arXiv:2403.12807.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nLLM-as-a-judge with MT-Bench and Chatbot Arena.\nIn Advances in Neural Information Processing Sys-\ntems, volume 36, pages 46595–46623.\n"}]}