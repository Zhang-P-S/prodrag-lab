{"doc_id": "arxiv:2512.22738", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.22738.pdf", "meta": {"doc_id": "arxiv:2512.22738", "source": "arxiv", "arxiv_id": "2512.22738", "title": "Harnessing Large Language Models for Biomedical Named Entity Recognition", "authors": ["Jian Chen", "Leilei Su", "Cong Sun"], "published": "2025-12-28T01:34:23Z", "updated": "2025-12-28T01:34:23Z", "summary": "Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.22738v1", "url_pdf": "https://arxiv.org/pdf/2512.22738.pdf", "meta_path": "data/raw/arxiv/meta/2512.22738.json", "sha256": "9cf3de41b66273c1cdeefdf87dd63961fa69b2cc2a17ca3c6158efeaebc52c41", "status": "ok", "fetched_at": "2026-02-18T02:23:45.012578+00:00"}, "pages": [{"page": 1, "text": "Harnessing Large Language Models for Biomedical Named Entity\nRecognition\nJian Chena, Leilei Sub and Cong Sunc,âˆ—\naDepartment of Data Science and Big Data Technology, Hainan University, Haikou 570228, China\nbDepartment of Mathematics, Hainan University, Haikou 570228, China\ncDepartment of Population Health Sciences, Weill Cornell Medicine, New York 10022, USA\nA R T I C L E I N F O\nKeywords:\nInstruction Tuning\nData Filtering\nLarge Language Models\nBiomedical Named Entity Recognition\nA B S T R A C T\nBackground and Objective:\nBiomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics,\ncrucial for downstream applications like drug discovery and clinical trial matching. However, adapting\ngeneral-domain Large Language Models (LLMs) to this task is often hampered by their lack of\ndomain-specific knowledge and the performance degradation caused by low-quality training data. To\naddress these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for\nfine-tuning LLMs that prioritizes data quality over quantity.\nMethods and Results:\nBioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel\nHybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak\nmodel to distill a compact, high-impact training dataset.\nConclusions:\nThrough extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA)\nperformance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the\ncurated positive data, not only surpasses the fully-trained baseline but also outperforms powerful\ndomain-specialized models like BioMedBERT.\n1. INTRODUCTION\nLarge Language Models (LLMs), such as GPT-4 [1], have\nsparked a paradigm shift in Natural Language Processing\n(NLP), demonstrating exceptional performance across a wide\nspectrum of tasks. Pre-trained on vast text corpora, LLMs\npossess powerful generalization capabilities, enabling them\nto tackle complex problems through zero-shot and few-shot\nprompting [2]. This has accelerated their adoption in diverse\nfields, including education, law, and healthcare.\nIn the biomedical domain, specialized LLMs like Med-\nPaLM2 [3], PMC-Llama [4], and Chat-Doctor [5] have\nshown promise in conversational and question-answering\ntasks. However, a significant performance gap remains when\napplying these models to fundamental information extrac-\ntion tasks, particularly Biomedical Named Entity Recogni-\ntion (BioNER). General-domain LLMs often lack the deep,\ndomain-specific knowledge required to interpret complex\nbiomedical texts accurately. Furthermore, studies have shown\nthat generative LLMs tend to yield low precision and recall\non NER tasks, failing to meet the high-accuracy demands\nof biomedical research [6, 7]. As BioNER is a cornerstone\nfor downstream applications such as drug discovery, gene\nfunction analysis, and clinical trial matching, bridging this\nperformance gap is of critical importance.\nTo address these challenges, we formulate BioNER as\nan instruction-driven, structured data generation task. As\nâˆ—Corresponding author\nEmail address: csun.nlp@gmail.com (C. Sun)\nORCID(s):\nillustrated in Figure 1, the model is provided with a piece\nof biomedical text and a specific instruction and is trained\nto generate a standardized, machine-readable JSON list of\nthe identified entities. This approach not only unifies the\nextraction paradigm across different entity types but also\ncapitalizes on the powerful instruction-following and text gen-\neration capabilities of modern LLMs. And we propose a novel\nframework to efficiently adapt general-domain LLMs for high-\nperformance BioNER. Rather than relying on costly domain-\nspecific pre-training, we focus on unlocking the potential\nof existing models through instruction tuning. We select the\nQwen3 family of models as our foundation [8] and, to this end,\ncurate and unify four benchmark BioNER datasets [9] into an\ninstruction-following format. The core of our framework is a\nnovel data curation strategy we term \"Hybrid Superfiltering,\"\n[10] which leverages a computationally inexpensive \"weak\"\nmodel to intelligently identify and select the most informative\nand difficult training samples for fine-tuning a more powerful\n\"strong\" model. Our main contributions are as follows:\nâ€¢ We introduce Hybrid Superfiltering, a weak-to-strong\ndata filtering strategy tailored for BioNER instruc-\ntion tuning. By separating positive and negative sam-\nples and using a homologous weak model to score\nInstruction-Following Difficulty (IFD), this method\ncurates a high-quality training subset that significantly\nboosts learning efficiency and model performance.\nâ€¢ We reformulate BioNER as an end-to-end text-to-\nstructured-data generation task. By fine-tuning the\nSu et al.: Preprint submitted to Elsevier\nPage 1 of 9\narXiv:2512.22738v1  [cs.CL]  28 Dec 2025\n"}, {"page": 2, "text": "Figure 1: Templates for instruction-following data and test data.\nLLM to directly output entities in a JSON format,\nwe bypass the complexities of traditional sequence\nlabeling and provide a clean, effective paradigm for\nsolving information extraction tasks with generative\nmodels.\nâ€¢ Through extensive experiments, we demonstrate that\nour model, BioSelectTune trained on approximately\n50% of the highest-quality curated data, achieves state-\nof-the-art results on multiple in-domain and out-of-\ndomain BioNER datasets. Our method surpasses not\nonly powerful generalist models like GPT-4 but also\ndomain-specialized models like BioMedBERT, vali-\ndating its effectiveness and generalization capabilities.\n2. Related Work\nLLMs for NER: Recent studies have explored the\napplication of LLMs in NER by leveraging their generative\nnature. A notable example is GPT-NER, which adapts LLMs\nby converting sequence labeling into a generation task.\nThis transformation uses specialized prompts and tokens,\nenhancing the performance of NER in various domains\n[11]. llmNER is a Python library for zero-shot and few-shot\nNER with LLMs, featuring an intuitive interface for prompt\ndesign, model querying, and output parsing, validated on\nmultiple NER tasks [12]. UniversalNER [13] is a lightweight\nyet powerful model distilled from ChatGPT using mission-\nfocused instruction tuning. It outperforms general instruction-\ntuned models like Alpaca [14], Vicuna [15], and InstructUIE\n[16] across domains, despite using far fewer parameters. In\nthe biomedical domain, research has focused on enhancing\nin-context learning and fine-tuning strategies to improve LLM\nperformance in BioNER tasks [17, 18].\nInstruction Tuning: Instruction-tuning enables LLMs to\nfollow natural language instructions for various downstream\ntasks. Given the high training costs of LLMs, this low-cost\nand efficient tuning method has attracted significant attention\nfrom researchers. For example, Tk-INSTRUCT, trained on a\nlarge-scale benchmark of diverse NLP tasks using instruction\ntuning, demonstrates strong generalization to unseen tasks,\nproviding valuable insights for developing more general-\npurpose models [19]. Ouyang et al. collected high-quality\ninstruction data using crowd-sourcing and fine-tuning GPT-3\nto create InstructGPT, significantly improving its ability to\nunderstand user intent and follow instructions [20]. Recent\ndevelopments [14, 15, 21] have also led to smaller models that\nexhibit task-following capabilities after being fine-tuned on\ninstruction data generated by LLMs like ChatGPT or GPT-4.\nBut the prevailing paradigms have largely overlooked a\ncrucial question: can we achieve state-of-the-art performance\nnot through more extensive pre-training or larger datasets,\nbut through a more intelligent, data-centric approach to\ninstruction tuning? This question is the primary motivation\nfor our work. We hypothesize that the key to unlocking the full\npotential of general-purpose LLMs for specialized tasks like\nBioNER lies not in data quantity, but in its curated quality.\nWe bridge this gap by proposing a novel framework that\nprioritizes the selection of a compact, high-impact training\nsubset, aiming to match or even exceed the performance of\ndomain-specialized models in a more efficient manner.\n3. Methodology\nOur proposed methodology advances the state-of-the-\nart in BioNER by synergizing an efficient data curation\nstrategy with a structured generation paradigm. We introduce\na novel Hybrid Superfiltering approach that leverages a weak\nlanguage model to curate a highly informative, balanced\nSu et al.: Preprint submitted to Elsevier\nPage 2 of 9\n"}, {"page": 3, "text": "Figure 2: Overall architecture of our method.\ndataset for instruction-tuning a powerful, larger model. The\nentire task is formulated as a unified structured data gen-\neration problem, where the model learns to output entities\nin a consistent JSON format. Our framework consists of\nthree main stages: (1) formulating BioNER as a structured\ngeneration task, (2) curating the training data via our Hybrid\nSuperfiltering strategy, and (3) fine-tuning the large language\nmodel on the curated data, as illustrated in Figure 2.\n3.1. BioNER as a Structured Generation Task\nTraditional BioNER is often treated as a sequence label-\ning task. More recent approaches have reformulated it as a\ntext-to-text generation task, using in-text markers to identify\nentities [22]. We depart from these paradigms and formulate\nBioNER as a text-to-structured-data generation task.\nGiven a raw dataset îˆ°raw = {(ğ‘‡ğ‘–, ğ‘Œğ‘–)}ğ‘\nğ‘–=1, where ğ‘‡ğ‘–is a\nbiomedical text and ğ‘Œğ‘–is the set of ground-truth entities, we\ntransform each instance into an instruction-following format.\nEach training example is a tuple (ğ‘¥ğ‘–, ğ‘¦ğ‘–), where:\nâ€¢ Input (ğ‘¥ğ‘–): The input is a concatenation of a task-\nspecific instruction ğ¼ğ‘–and the source text ğ‘‡ğ‘–, denoted\nas ğ‘¥ğ‘–= ğ¼ğ‘–âŠ•ğ‘‡ğ‘–. For example, ğ¼ğ‘–could be â€œExtract the\nchemical entities from the following text.â€.\nâ€¢ Output (ğ‘¦ğ‘–): The output ğ‘¦ğ‘–is a structured representa-\ntion of the entity set ğ‘Œğ‘–, serialized into a JSON array\nstring. Each entity ğ‘’ğ‘—âˆˆğ‘Œğ‘–is represented as a key-value\nobject with its type and name. Formally, ğ‘Œğ‘–= {ğ‘’ğ‘—}ğ‘˜ğ‘–\nğ‘—=1,\nwhere ğ‘’ğ‘—= (ğ‘ğ‘—, ğ‘›ğ‘—) consists of a category ğ‘ğ‘—and a\nname ğ‘›ğ‘—. The target output is its string representation:\nğ‘¦ğ‘–= JSON\n({(\"entity\", ğ‘ğ‘—), (\"name\", ğ‘›ğ‘—)}ğ‘˜ğ‘–\nğ‘—=1\n)\n(1)\nCrucially, if a text ğ‘‡ğ‘–contains no entities (ğ‘Œğ‘–= âˆ…, thus\nğ‘˜ğ‘–= 0), the target output is a string representing an\nempty array: ğ‘¦ğ‘–= â€œ[]â€.\nThe overall learning objective is to train a model with\nparameters ğœƒto maximize the conditional probability of\ngenerating the correct structured output string ğ‘¦ğ‘–given the\ninput ğ‘¥ğ‘–, i.e., to learn the distribution ğ‘ƒğœƒ(ğ‘¦|ğ‘¥).\n3.2. Hybrid Superfiltering\nA key challenge in instruction tuning is the presence\nof low-quality or redundant data. To address this, we adapt\nthe â€œSuperfilteringâ€ [10] framework, a data curation strategy\nbased on the weak-to-strong principle. Our novel approach,\ntermed Hybrid Superfiltering, is specifically tailored for NER\ntasks by uniquely handling positive and negative samples.\n3.2.1. Homologous Weak-to-Strong Filtering via IFD\nScore\nThe core premise of Superfiltering is that a computa-\ntionally inexpensive, weaker model (îˆ¹weak) can serve as an\neffective proxy to evaluate data quality for a stronger model\n(îˆ¹strong) [10]. To maximize the fidelity of this proxy, we\nintroduce a homologous filtering setup. Specifically, we select\na weak model from the same family as our strong target model.\nIn our experiments, we use Qwen3-0.6B as îˆ¹weak to curate\ndata for Qwen3-8B (îˆ¹strong). The rationale is that models\nfrom the same family often share architectural designs,\ntokenizers, and pre-training data distributions. This inherent\nalignment is hypothesized to yield a stronger correlation in\ntheir perception of instruction difficulty, making the weak\nmodel a more reliable and accurate proxy for the strong one.\nWe leverage the Instruction-Following Difficulty (IFD)\nscore as the primary metric for data quality. The IFD score\nquantifies how much an instruction ğ‘¥ğ‘–aids the model in\ngenerating the corresponding response ğ‘¦ğ‘–. It is defined as\nthe ratio of the modelâ€™s perplexity in generating ğ‘¦ğ‘–with and\nwithout the instructional context ğ‘¥ğ‘–[10]. First, we define the\nperplexity (PPL) [23] of a sequence ğ‘†= (ğ‘ 1, â€¦ , ğ‘ ğ¿) given a\nmodel îˆ¹as:\nPPL(ğ‘†; îˆ¹) = exp\n(\nâˆ’1\nğ¿\nğ¿\nâˆ‘\nğ‘—=1\nlog ğ‘ƒîˆ¹(ğ‘ ğ‘—|ğ‘ <ğ‘—)\n)\n(2)\nThe IFD score for a data pair (ğ‘¥ğ‘–, ğ‘¦ğ‘–) is then calculated\nusing our weak homologous model îˆ¹weak:\nIFD(ğ‘¥ğ‘–, ğ‘¦ğ‘–) = PPL(ğ‘¦ğ‘–|ğ‘¥ğ‘–; îˆ¹weak)\nPPL(ğ‘¦ğ‘–; îˆ¹weak)\n(3)\nwhere PPL(ğ‘¦ğ‘–|ğ‘¥ğ‘–; îˆ¹weak) is the perplexity of generating\nthe output conditioned on the input, and PPL(ğ‘¦ğ‘–; îˆ¹weak) is\nthe unconditional perplexity. A higher IFD score suggests\nthat the instruction provides less direct guidance, indicating\na more complex or informative sample.\nSu et al.: Preprint submitted to Elsevier\nPage 3 of 9\n"}, {"page": 4, "text": "3.2.2. Hybrid Composition of Positive and Negative\nSamples\nA direct application of high-IFD filtering to our dataset\nrevealed a critical insight: this method exclusively selects\ninstances containing entities (positive samples) while discard-\ning all instances without entities (negative samples, where\nğ‘¦ğ‘–= â€œ[]â€). This is because generating an empty array is a\nsimple task, resulting in a low IFD score.\nHowever, negative samples are indispensable for robust\nNER performance, as they teach the model to avoid gener-\nating false positives. To resolve this, we devised a Hybrid\nData Composition Strategy. Let the full instruction-formatted\ndataset be îˆ°. We first partition it into positive and negative\nsets:\nîˆ°pos = {(ğ‘¥ğ‘–, ğ‘¦ğ‘–) âˆˆîˆ°âˆ£ğ‘¦ğ‘–â‰ â€œ[]â€}\n(4)\nîˆ°neg = {(ğ‘¥ğ‘–, ğ‘¦ğ‘–) âˆˆîˆ°âˆ£ğ‘¦ğ‘–= â€œ[]â€}\n(5)\nWe then apply IFD-based filtering only to the positive\nset îˆ°pos. Following the methodology in [10], we first discard\nsamples with an IFD score greater than or equal to 1. From\nthe remaining samples, we select the top-ğœŒpercentile based\non the highest IFD scores to form a high-quality positive\nsubset îˆ°â€²\npos. The final training dataset îˆ°train is constructed\nby combining this elite positive subset with the entire set of\nnegative samples:\nîˆ°train = îˆ°â€²\npos âˆªîˆ°neg\n(6)\nThis hybrid approach ensures that the model is trained on\nthe most informative entity-bearing examples while retaining\ncomprehensive knowledge of non-entity contexts, striking\nan optimal balance between learning efficiency and model\nrobustness.\n3.3. Instruction Fine-Tuning of the Generative\nModel\nThe final stage involves fine-tuning the strong language\nmodel îˆ¹strong on the curated dataset îˆ°train. The model\nis trained to minimize the negative log-likelihood of the\ntarget JSON strings. The optimization objective is to find\nthe optimal parameters ğœƒâˆ—that minimize the following loss\nfunction îˆ¸(ğœƒ):\nîˆ¸(ğœƒ) = âˆ’\nâˆ‘\n(ğ‘¥ğ‘–,ğ‘¦ğ‘–)âˆˆîˆ°train\n|ğ‘¦ğ‘–|\nâˆ‘\nğ‘—=1\nlog ğ‘ƒğœƒ(ğ‘¦ğ‘–,ğ‘—|ğ‘¦ğ‘–,<ğ‘—, ğ‘¥ğ‘–)\n(7)\nwhere |ğ‘¦ğ‘–| is the tokenized length of the target JSON\nstring ğ‘¦ğ‘–, and ğ‘¦ğ‘–,ğ‘—is the ğ‘—-th token. This standard auto-\nregressive training objective effectively teaches the model to\ngenerate well-formed JSON structures corresponding to the\nentities present in the input text, or an empty array otherwise.\nAlgorithm 1 Hybrid Superfiltering and Fine-Tuning Frame-\nwork\nRequire: Full dataset îˆ°, weak model îˆ¹weak (Qwen3-0.6B),\nstrong model îˆ¹strong (Qwen3-8B), selection ratio ğœŒ\n1: Partition îˆ°into îˆ°pos and îˆ°neg\n2: Initialize an empty list îˆ¿IFD\n3: for all (ğ‘¥ğ‘–, ğ‘¦ğ‘–) âˆˆîˆ°pos do\n4:\nCalculate IFD(ğ‘¥ğ‘–, ğ‘¦ğ‘–) using Equation (3) with îˆ¹weak\n5:\nAppend (IFD(ğ‘¥ğ‘–, ğ‘¦ğ‘–), (ğ‘¥ğ‘–, ğ‘¦ğ‘–)) to îˆ¿IFD\n6: end for\n7: Sort îˆ¿IFD in descending order based on IFD scores\n8: Let ğ‘˜= âŒŠğœŒâ‹…|îˆ°pos|âŒ‹\n9: Extract the top ğ‘˜samples from the sorted list to form\nîˆ°â€²\npos\n10: Construct the final training set: îˆ°train â†îˆ°â€²\npos âˆªîˆ°neg\n11: Fine-tune îˆ¹strong on îˆ°train by minimizing the loss in\nEquation (7)\n12: return Fine-tuned model îˆ¹strong\n4. Experiments\n4.1. Experimental Datasets\nTo rigorously evaluate our proposed methodology, we\ncurated a comprehensive suite of benchmark datasets, par-\ntitioned into in-domain and out-of-domain categories. This\nselection facilitates a thorough assessment of both the modelâ€™s\nprimary task proficiency and its generalization capabilities\non unseen data.\n4.1.1. In-domain Datasets\nThe core of our fine-tuning and evaluation process\nutilized four widely recognized biomedical NER corpora.\nThese datasets represent canonical entity types within the\nbiomedical domain and serve to benchmark our modelâ€™s\nperformance on the primary tasks it was trained for. The\nselected datasets are:\nâ€¢ NCBI-Disease: This corpus is a standard benchmark\nfor disease name recognition, consisting of manually\nannotated PubMed abstracts [24].\nâ€¢ BC5CDR (Chemical & Disease): The BioCreative V\nChemical Disease Relation corpus contains PubMed\nabstracts annotated for both chemical and disease enti-\nties [25]. We treat the chemical and disease annotations\nas two distinct tasks for evaluation purposes (BC5CDR-\nChemical and BC5CDR-Disease).\nâ€¢ BC2GM: The BioCreative II Gene Mention corpus is\nfocused on the recognition of gene and gene product\nmentions in MEDLINE sentences, providing a bench-\nmark for another critical entity type [26].\nThe inclusion of these datasets ensures a multifaceted evalua-\ntion of the modelâ€™s ability to identify fundamental biomedical\nentities.\nSu et al.: Preprint submitted to Elsevier\nPage 4 of 9\n"}, {"page": 5, "text": "Table 1\nModel performance on in-domain biomedical datasets. Bolded values indicate the best performance. A model with the suffix â€˜-SFTâ€™\ndenotes a fine-tuned version. The best F1-scores in each column are highlighted in bold.\nModels\nF1-Score with Strict Match (%)\nNCBI-Disease\nBC5CDR-Chemical\nBC5CDR-Disease\nBC2GM\nGPT-4\n67.40\n83.70\n67.80\n57.20\nTaiyi\n73.10\n80.20\n69.10\n/\nInstructUIE-11B\n86.21\n/\n/\n85.16\nUniNER-7B\n86.96\n88.82\n80.09\n82.42\nBioNER-Llama2-7B\n88.00\n92.80\n66.90\n83.40\nBioMedBERT\n87.82\n93.33\n85.62\n84.52\nQwen3-8B-SFT\n86.78\n91.58\n86.11\n81.01\nBioSelectTune-8B (50%)\n88.29\n91.93\n85.71\n81.44\n4.1.2. Out-of-domain Datasets\nTo assess the modelâ€™s generalization performance and ro-\nbustness, we conducted evaluations on two datasets that were\nnot included in any stage of the fine-tuning process. These out-\nof-domain corpora allow us to measure the modelâ€™s ability to\nadapt its learned knowledge to novel data distributions and\nannotation guidelines. The selected datasets are:\nâ€¢ NLM-Chem: This dataset, from the BioCreative VII\nchallenge, consists of full-text articles annotated for\nchemical entities, presenting a more complex challenge\nthan abstract-only corpora [27].\nâ€¢ NLM-Gene: This corpus provides annotations for gene\nmentions across multiple species within PubMed ab-\nstracts, testing the modelâ€™s ability to handle ambiguous\nand diverse gene names [28].\nPerformance on these datasets provides critical insights into\nthe real-world applicability and generalization capacity of\nour fine-tuned model.\n4.2. Experimental Settings\nWe compared our method with the following baselines:\nâ€¢ GPT-4 [1, 29], a large language model developed by\nOpenAI. As a baseline for various information extrac-\ntion tasks, GPT-4 demonstrates strong performance by\nleveraging its extensive pre-training on diverse textual\ndata, enabling it to generate contextually relevant and\ncoherent outputs even for complex queries.\nâ€¢ InstructUIE [16], a universal end-to-end information\nextraction framework that leverages natural language\ninstructions to guide large models in performing IE\ntasks.\nâ€¢ UniversalNER [13], which proposed a more tar-\ngeted distillation method and performed task-centered\nprompt fine-tuning, improving NER performance in\nmost domains.\nâ€¢ Taiyi [18], a bilingual (Chinese and English) large\nmodel fine-tuning on a substantial amount of biomedi-\ncal data.\nâ€¢ BioNER-Llama2 [30], a large language model built\non Llama2-7B and designed for biomedical Named\nEntity Recognition. It utilizes a novel instruction-\ntuning paradigm to transform the NER task into a\ngenerative one, achieving performance comparable to\nspecialized, fine-tuned biomedical models.\nâ€¢ BioMedBERT [31], a model pre-trained on a massive\ncorpus of biomedical literature and clinical records,\nand one of the largest pre-trained models in the\nbiomedical domain.\nAll experiments were conducted based on the Qwen3-8B\n[8] model. We employed the Low-Rank Adaptation (LoRA)\ntechnique for parameter-efficient fine-tuning [32]. The model\nwas trained for 3.0 epochs with a maximum sequence length\nof 128 tokens. We utilized a cosine learning rate scheduler\nwith a peak learning rate of 1.0 Ã— 10âˆ’4 and accelerated\ntraining with BF16 mixed-precision. For evaluation, model\nperformance was measured by the micro-F1.\n4.3. Main Results\n4.3.1. Performance on In-Domain Datasets\nWe present the primary results of our method on four\nstandard in-domain BioNER datasets in Table 1. We de-\nnote our proposed method, which combines Hybrid Su-\nperfiltering with structured JSON generation, as BioSelect-\nTune. The specific instance trained on the 50% curated\npositive data subset using the Qwen3-8B base model is\nreferred to as BioSelectTune-8B (50%). The results are\nbenchmarked against several strong baselines, including\ngeneral-purpose LLMs (GPT-4), other instruction-tuned mod-\nels (BioNER-Llama2-7B), and domain-specific pre-trained\nmodels (BioMedBERT).\nThe experimental results, presented in Table 1, pow-\nerfully validate our BioSelectTune framework. Our model\nnot only demonstrates highly competitive performance but\nalso achieves new state-of-the-art results, underscoring the\nsuperiority of a curated data approach.\nA pivotal comparison is against the fully fine-tuned\nQwen3-8B-SFT baseline. Our BioSelectTune-8B (50%)\nmodel, despite using only half of the positive training data,\noutperforms the full-data baseline on three of the four datasets.\nSu et al.: Preprint submitted to Elsevier\nPage 5 of 9\n"}, {"page": 6, "text": "Table 2\nPerformance comparison on the NLM-Gene and NLM-Chem datasets (Strict Match). The best F1-scores in each column are\nhighlighted in bold.\nModel\nNLM (Gene)\nNLM (Chem)\nPrecision\nRecall\nMicro-F1\nPrecision\nRecall\nMicro-F1\nBioMedBERT\n83.00\n79.50\n81.20\n86.20\n65.20\n74.20\nBioNER-LLaMA2\n88.10\n77.00\n82.20\n85.30\n59.80\n70.30\nBioSelectTune-8B (50%)\n85.79\n80.64\n83.14\n85.24\n64.97\n73.73\nThe most significant improvement is on the NCBI-Disease\ncorpus, where our method achieves a remarkable F1-score\nof 88.29%, surpassing the SFT model by a full 1.5 points.\nThis result provides compelling evidence that our Hybrid\nSuperfiltering strategy distills a higher-quality training signal,\nproving that data quality, not mere quantity, is the key driver\nof performance.\nFurthermore, when benchmarked against the highly\nspecialized BioMedBERT, our method establishes a new\nstate-of-the-art for LLM-based approaches on this task.\nBioSelectTune-8B (50%) surpasses the domain-expert model\non both the NCBI-Disease and BC5CDR-Disease datasets.\nThis is a critical finding, demonstrating that a general-purpose\nfoundation model, when trained on a meticulously curated\ndataset via our method, can transcend the performance of\nmodels that have undergone extensive and costly pre-training\non specialized biomedical corpora.\nIt is also instructive to analyze the datasets where BioMed-\nBERT retains a performance advantage, namely BC5CDR-\nChemical and BC2GM. We hypothesize that this stems\nfrom the fundamental differences between our data-centric\nfine-tuning and domain-specific pre-training. Entities such\nas chemical compounds and gene mentions often consist\nof a vast lexicon of specific proper nouns that rely less\non contextual semantics and more on lexical recognition.\nThe exhaustive pre-training of BioMedBERT on biomedical\nliterature likely endows it with a stronger \"memory\" for\nthis extensive and specific vocabulary. In contrast, our\nHybrid Superfiltering method excels at teaching the model\nto recognize generalizable patterns from complex and am-\nbiguous contexts, making it particularly effective for more\ndescriptively rich entities like diseases. Nevertheless, the\nfact that our model achieves highly competitive performance\nacross all categoriesâ€”without the need for costly domain pre-\ntrainingâ€”powerfully highlights the efficiency and strategic\nvalue of our approach.\n4.3.2. Generalization on Out-of-Domain Datasets\nA critical measure of a modelâ€™s robustness is its ability\nto perform well on datasets it has not been trained on.\nTo this end, we evaluated our BioSelectTune-8B model\non two completely unseen, out-of-domain datasets: NLM-\nGene and NLM-Chem. We benchmarked its performance\nagainst the strong, domain-specific BioMedBERT and an-\nother instruction-tuned model, BioNER-LLAMA2.\nThe results, presented in Table 2, demonstrate that\nBioSelectTune-8B exhibits excellent generalization capa-\nbilities. On the NLM-Gene dataset, our model achieves a\nnew state-of-the-art F1-score of 83.14%, surpassing both\nthe BioMedBERT baseline and the BioNER-LLAMA2. This\nindicates that our model learned a more robust and effective\nrepresentation for gene entity recognition.\nFurthermore, on the NLM-Chem dataset, BioSelectTune-\n8B remains highly competitive with the domain-expert\nmodel, achieving an F1-score of 73.73% and substantially\noutperforming BioNER-LLAMA2. This strong performance\non unseen data suggests that our Hybrid Superfiltering\nstrategy does not lead to overfitting on the in-domain corpora.\nInstead, it guides the model to learn generalizable features\nof biomedical entities, underscoring the real-world potential\nand robustness of our approach.\n4.4. Ablation Studies\nEffect of Data Selection Ratio To determine the optimal\nquantity of curated data for training, we evaluated the\nperformance of our BioSelectTune-8B model using different\nratios of the top-ranked positive samples identified by our\nfiltering strategy. The results are presented in the heatmap in\nFigure 3.\nA key finding from this analysis is that the relationship\nbetween the volume of curated data and model performance\nis non-monotonic. The model achieves its peak performance\non three of the four datasetsâ€”NCBI-Disease, BC5CDR-\nChemical, and BC2GMâ€”when trained on 50% of the curated\npositive samples. Notably, on NCBI-Disease, the 50% data\nsubset yields an F1-score of 88.29%, significantly outperform-\ning both smaller subsets (82.06% at 25%) and larger ones,\nincluding the full-data baseline (86.78% at 100%).\nThis trend strongly suggests that beyond an optimal point,\nadding more data that the IFD metric ranked as lower-quality\nintroduces noise that can degrade, rather than improve, final\nmodel performance. While the pattern varies slightly across\ndatasets, the overwhelming evidence points to the 50% mark\nrepresenting the most effective balance between signal and\nnoise. This result powerfully reinforces the central tenet of our\nBioSelectTune framework: meticulous data curation is more\ncritical than raw data volume for achieving state-of-the-art\nperformance.\nImpact of the Filtering Model. To validate our choice of\na homologous filter, we compared it against using a domain-\nspecific model, BioGPT[33], for data curation. We replaced\nSu et al.: Preprint submitted to Elsevier\nPage 6 of 9\n"}, {"page": 7, "text": "Figure 3: Heatmap of F1-scores showing the impact of varying\nthe curated data ratio. The results indicate a clear optimal\nperformance point around the 50% mark.\nour standard weak filter (Qwen3-0.6B) with BioGPT and\nused the resulting dataset to fine-tune the Qwen3-8B model,\na variant we refer to as BioSelectTune (BioGPT).\nThe results, presented in Figure 4, are decisive. Our\nprimary method, using the in-family Qwen3-0.6B model,\nnow significantly outperforms the variant filtered by BioGPT\non all four datasets. The performance gap is substantial\nacross the board, reaching a remarkable 11.1 F1 points on\nNCBI-Disease and 9.8 points on BC5CDR-Disease. Even\non the BC2GM dataset, where performance was previously\nsimilar, our homologous filtering approach now holds a clear\nadvantage of nearly 5 F1 points.\nThis provides overwhelming evidence for our hypothesis\nthat the architectural and distributional alignment between\nthe filter and target models is far more critical for accurately\nassessing data difficulty than the filterâ€™s specialized domain\nknowledge. The consistent superiority across all tested\ncorpora validates our choice of a homologous model as the\noptimal strategy for the BioSelectTune framework.\nEffectiveness on Smaller-Scale Models. To assess the\nscalability of our data curation strategy, we applied it to a\nsmaller Qwen3-4B model. The results, presented in Figure 5,\ncompellingly demonstrate that our BioSelectTune framework\nis highly effective even at a smaller scale. Our method,\nBioSelectTune-4B (50%), which uses only half the curated\npositive data, outperforms the fully fine-tuned Qwen3-4B-\nSFT baseline on the two largest datasets, NCBI-Disease\n(84.95% vs. 84.20%) and BC5CDR-Chemical (92.07% vs.\n91.71%). While the full-data baseline retains a marginal\nadvantage on the other two corpora, our findings strongly\nvalidate that BioSelectTune can produce smaller models that\nare not only more efficient to train but can also achieve a\nsuperior level of performance.\nFigure 4: Performance comparison between using a homologous\nweak model (our standard BioSelectTune-8B at 50%) and a\ndomain-specific weak model (BioSelectTune with BioGPT)\nfor data filtering. The homologous filter demonstrates clear\nsuperiority across all datasets.\n5. Conclusion\nIn conclusion, we introduced BioSelectTune, a data-\ncentric fine-tuning framework that demonstrates the primacy\nof data quality over quantity in adapting LLMs for BioNER.\nBy combining a structured JSON generation paradigm with\nour novel Hybrid Superfiltering strategy, our method sets a\nnew state-of-the-art on multiple benchmarks. Notably, our\nmodel, trained on only half of the curated positive data, not\nonly surpasses the fully-trained baseline but also outperforms\ndomain-specialized models such as BioMedBERT. The\nframeworkâ€™s effectiveness, scalability to smaller models,\nand robust generalization to out-of-domain data provide\ncompelling evidence that our weak-to-strong data curation\napproach offers an efficient and powerful pathway for un-\nlocking LLMs in specialized scientific domains. Future work\nwill extend this paradigm to more complex biomedical tasks,\nincluding relation extraction and document classification.\nCRediT authorship contribution statement\nJian Chen: Investigation, Methodology, Software, Writ-\ning - original draft. Leilei Su: Writing - original draft, Writing\n- review & editing. Cong Sun: Conceptualization, Writing -\nreview & editing, Supervision.\nDeclaration of competing interest\nThe authors declare that they have no known competing\nfinancial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nAcknowledgment\nWe thank the anonymous reviewers for their constructive\ncomments.\nSu et al.: Preprint submitted to Elsevier\nPage 7 of 9\n"}, {"page": 8, "text": "Figure 5: Performance comparison of the fully fine-tuned Qwen3-4B-SFT (100% data) against our BioSelectTune method applied\nto both the 4B and 8B models (50% curated positive data). Our strategy enables the smaller 4B model to outperform the full-data\nbaseline on key datasets.\nData availability\nData will be made available on request.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,\nSam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877â€“1901,\n2020.\n[3] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei,\nHyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-\nLewis, Stephen Pfohl, et al. Large language models encode clinical\nknowledge. Nature, 620(7972):172â€“180, 2023.\n[4] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie,\nand Yanfeng Wang. Pmc-llama: toward building open-source language\nmodels for medicine. Journal of the American Medical Informatics\nAssociation, page ocae045, 2024.\n[5] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You.\nChatdoctor: A medical chat model fine-tuned on llama model using\nmedical domain knowledge. arXiv preprint arXiv:2303.14070, 2023.\n[6] Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng,\nJie Zhou, Tao Gui, Qi Zhang, and Xuanjing Huang. How robust is gpt-\n3.5 to predecessors? a comprehensive study on language understanding\ntasks. arXiv preprint arXiv:2303.00293, 2023.\n[7] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun\nLiu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. A\ncomprehensive capability analysis of gpt-3 and gpt-3.5 series models.\narXiv preprint arXiv:2303.10420, 2023.\n[8] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al.\nQwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\n[9] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama,\nXiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon.\nDomain-specific language model pretraining for biomedical natural\nlanguage processing. ACM Transactions on Computing for Healthcare\n(HEALTH), 3(1):1â€“23, 2021.\n[10] Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong\nWang, Ning Cheng, and Tianyi Zhou.\nSuperfiltering: Weak-to-\nstrong data filtering for fast instruction-tuning.\narXiv preprint\narXiv:2402.00530, 2024.\n[11] Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tian-\nwei Zhang, Jiwei Li, and Guoyin Wang. Gpt-ner: Named entity recog-\nnition via large language models. arXiv preprint arXiv:2304.10428,\n2023.\n[12] FabiÃ¡n Villena, Luis Miranda, and Claudio Aracena. llmner:(zero|\nfew)-shot named entity recognition, exploiting the power of large\nlanguage models. arXiv preprint arXiv:2406.04528, 2024.\n[13] Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung\nPoon. Universalner: Targeted distillation from large language models\nfor open named entity recognition. arXiv preprint arXiv:2308.03279,\n2023.\n[14] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen\nLi, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.\nStanford alpaca: An instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca, 2023.\n[15] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao\nZhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E.\nGonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nSu et al.: Preprint submitted to Elsevier\nPage 8 of 9\n"}, {"page": 9, "text": "[16] Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen\nZhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, et al. Instructuie:\nmulti-task instruction tuning for unified information extraction. arXiv\npreprint arXiv:2304.08085, 2023.\n[17] Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami,\nFazlolah Mohaghegh, Mozhdeh Rouhsedaghat, and Kai-Wei Chang.\nLlms in biomedicine: A study on clinical named entity recognition.\narXiv preprint arXiv:2404.07376, 2024.\n[18] Ling Luo, Jinzhong Ning, Yingwen Zhao, Zhijun Wang, Zeyuan\nDing, Peng Chen, Weiru Fu, Qinyu Han, Guangtao Xu, Yunzhi Qiu,\net al. Taiyi: a bilingual fine-tuned large language model for diverse\nbiomedical tasks.\nJournal of the American Medical Informatics\nAssociation, page ocae037, 2024.\n[19] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh\nKordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Sel-\nvan Dhanasekaran, Atharva Naik, David Stap, et al.\nSuper-\nnaturalinstructions: Generalization via declarative instructions on\n1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022.\n[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wain-\nwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina\nSlama, Alex Ray, et al. Training language models to follow instructions\nwith human feedback. Advances in neural information processing\nsystems, 35:27730â€“27744, 2022.\n[21] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng\nGao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277,\n2023.\n[22] Jian Chen, Leilei Su, Yihong Li, Mingquan Lin, Yifan Peng, and\nCong Sun. A multimodal approach for few-shot biomedical named\nentity recognition in low-resource languages. Journal of Biomedical\nInformatics, 161:104754, 2025.\n[23] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker.\nPerplexityâ€”a measure of the difficulty of speech recognition tasks.\nThe Journal of the Acoustical Society of America, 62(S1):S63â€“S63,\n1977.\n[24] Rezarta Islamaj DoÄŸan, Robert Leaman, and Zhiyong Lu.\nNcbi\ndisease corpus: a resource for disease name recognition and concept\nnormalization. Journal of biomedical informatics, 47:1â€“10, 2014.\n[25] Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-\nHsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly,\nThomas C Wiegers, and Zhiyong Lu. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction. Database, 2016,\n2016.\n[26] Larry Smith, Lorraine K Tanabe, Rie Johnson nee Ando, Cheng-Ju\nKuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger,\nChristoph M Friedrich, Kuzman Ganchev, et al.\nOverview of\nbiocreative ii gene mention recognition. Genome biology, 9:1â€“19,\n2008.\n[27] Rezarta Islamaj, Robert Leaman, David Cissel, Cathleen Coss, Joseph\nDenicola, Carol Fisher, Rob Guzman, Preeti Gokal Kochar, Nicholas\nMiliaras, Zoe Punske, et al.\nNlm-chem-bc7: manually annotated\nfull-text resources for chemical entity annotation and indexing in\nbiomedical articles. Database, 2022:baac102, 2022.\n[28] Rezarta Islamaj, Chih-Hsuan Wei, David Cissel, Nicholas Miliaras,\nOlga Printseva, Oleg Rodionov, Keiko Sekiya, Janice Ward, and\nZhiyong Lu.\nNlm-gene, a richly annotated gold standard dataset\nfor gene entities that addresses ambiguity and multi-species gene\nrecognition. Journal of biomedical informatics, 118:103779, 2021.\n[29] Leilei Su, Jian Chen, Yifan Peng, and Cong Sun. Demonstration-\nbased learning for few-shot biomedical named entity recognition under\nmachine reading comprehension. Journal of Biomedical Informatics,\n159:104739, 2024.\n[30] Vipina K Keloth, Yan Hu, Qianqian Xie, Xueqing Peng, Yan Wang,\nAndrew Zheng, Melih Selek, Kalpana Raja, Chih Hsuan Wei, Qiao\nJin, et al. Advancing entity recognition in biomedicine via instruction\ntuning of large language models. Bioinformatics, 40(4):btae163, 2024.\n[31] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama,\nXiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon.\nDomain-specific language model pretraining for biomedical natural\nlanguage processing. ACM Transactions on Computing for Healthcare\n(HEALTH), 3(1):1â€“23, 2021.\n[32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank\nadaptation of large language models. ICLR, 1(2):3, 2022.\n[33] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung\nPoon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings in bioinformatics,\n23(6):bbac409, 2022.\nSu et al.: Preprint submitted to Elsevier\nPage 9 of 9\n"}]}