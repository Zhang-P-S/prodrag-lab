{"doc_id": "arxiv:2602.04074", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04074.pdf", "meta": {"doc_id": "arxiv:2602.04074", "source": "arxiv", "arxiv_id": "2602.04074", "title": "Stroke Lesions as a Rosetta Stone for Language Model Interpretability", "authors": ["Julius Fridriksson", "Roger D. Newman-Norlund", "Saeed Ahmadi", "Regan Willis", "Nadra Salman", "Kalil Warren", "Xiang Guan", "Yong Yang", "Srihari Nelakuditi", "Rutvik Desai", "Leonardo Bonilha", "Jeff Charney", "Chris Rorden"], "published": "2026-02-03T23:22:37Z", "updated": "2026-02-03T23:22:37Z", "summary": "Large language models (LLMs) have achieved remarkable capabilities, yet methods to verify which model components are truly necessary for language function remain limited. Current interpretability approaches rely on internal metrics and lack external validation. Here we present the Brain-LLM Unified Model (BLUM), a framework that leverages lesion-symptom mapping, the gold standard for establishing causal brain-behavior relationships for over a century, as an external reference structure for evaluating LLM perturbation effects. Using data from individuals with chronic post-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict brain damage location from behavioral error profiles, applied systematic perturbations to transformer layers, administered identical clinical assessments to perturbed LLMs and human patients, and projected LLM error profiles into human lesion space. LLM error profiles were sufficiently similar to human error profiles that predicted lesions corresponded to actual lesions in error-matched humans above chance in 67% of picture naming conditions (p < 10^{-23}) and 68.3% of sentence completion conditions (p < 10^{-61}), with semantic-dominant errors mapping onto ventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream patterns. These findings open a new methodological avenue for LLM interpretability in which clinical neuroscience provides external validation, establishing human lesion-symptom mapping as a reference framework for evaluating artificial language systems and motivating direct investigation of whether behavioral alignment reflects shared computational principles.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04074v1", "url_pdf": "https://arxiv.org/pdf/2602.04074.pdf", "meta_path": "data/raw/arxiv/meta/2602.04074.json", "sha256": "d0dc336793c2dd53efa93bcee3e5277afba379a3fdddd262b3339a952ef7bdba", "status": "ok", "fetched_at": "2026-02-18T02:19:50.931531+00:00"}, "pages": [{"page": 1, "text": "BLUM: Brain-LLM Unified Model \nStroke Lesions as a Rosetta Stone for Language Model \nInterpretability \nJulius Fridriksson1,2*, Roger D. Newman-Norlund1,2, Saeed Ahmadi1, Regan Willis3, Nadra Salman4, \nKalil Warren4, Xiang Guan3, Yong Yang3, Srihari Nelakuditi3, Rutvik Desai5, Leonardo Bonilha6, Jeff \nCharney2,7, Chris Rorden5 \n1 Department of Communication Sciences and Disorders, University of South Carolina \n2 ALLT.AI, LLC, Columbia, SC \n3 Department of Computer Science and Engineering, University of South Carolina \n4 Linguistics Program, University of South Carolina \n5 Department of Psychology, University of South Carolina \n6 Department of Neurology, USC School of Medicine \n7 MKHSTRY, LLC, Cleveland, OH \n*Corresponding author: fridriks@mailbox.sc.edu \nAbstract \nLarge language models (LLMs) have achieved remarkable capabilities, yet methods \nto verify which model components are truly necessary for language function remain \nlimited. Current interpretability approaches rely on internal metrics and lack external \nvalidation. Here we present the Brain-LLM Unified Model (BLUM), a framework that \nleverages lesion-symptom mapping‚Äîthe gold standard for establishing causal \nbrain-behavior relationships for over a century‚Äîas an external reference structure \nfor evaluating LLM perturbation effects. Using data from individuals with chronic \npost-stroke aphasia (N = 410), we trained symptom-to-lesion models that predict \nbrain damage location from behavioral error profiles, applied systematic \nperturbations to transformer layers, administered identical clinical assessments to \nperturbed LLMs and human patients, and projected LLM error profiles into human \nlesion space. LLM error profiles were sufficiently similar to human error profiles that \npredicted lesions corresponded to actual lesions in error-matched humans above \nchance in 67% of picture naming conditions (p < 10 -23) and 68.3% of sentence \ncompletion conditions (p < 10 -61), with semantic-dominant errors mapping onto \nventral-stream lesion patterns and phonemic-dominant errors onto dorsal-stream \npatterns. These findings open a new methodological avenue for LLM interpretability \nin which clinical neuroscience provides external validation, establishing human \nlesion-symptom mapping as a reference framework for evaluating artificial language \nsystems and motivating direct investigation of whether behavioral alignment reflects \nshared computational principles. \nKeywords: large language models (LLMs), causal interpretability, lesion‚Äìsymptom mapping, aphasia, \ntransformer perturbation, brain‚Äìmodel alignment, neurobiological ground truth, model compression \nPage 1 \n"}, {"page": 2, "text": "BLUM: Brain-LLM Unified Model \n1. Introduction \n \n1.1 The Validation Gap in Large Language Model Interpretability \nLarge language models (LLMs) have achieved remarkable capabilities across diverse \nlinguistic and cognitive tasks, yet a fundamental challenge persists: methods to verify which \nmodel components are truly necessary for language function lack external grounding. \nPerturbation-based approaches‚Äîincluding pruning, layer ablation, and causal tracing‚Äîcan \nidentify components whose removal degrades performance on specific benchmarks (Men et \nal. 2025; Gromov et al. 2025; Meng et al. 2022; Hase et al. 2023). However, these \nbenchmarks were designed primarily to assess reasoning, knowledge retrieval, and general \ncognitive performance rather than to isolate foundational language processing. They reflect \nconsidered judgments about what constitutes useful AI capability, but they are not grounded \nin principled accounts of human language or cognition. Without such grounding, \ninterpretability research can identify components necessary for benchmark performance, but \ncannot establish whether those components implement anything fundamental about language \nitself. \nThis limitation reflects a deeper absence of ground truth. Probing classifiers may reveal that \nsyntactic information is recoverable from specific activations, but this does not establish \nwhether such information is organized as it is in systems that actually process language \n(Belinkov 2022; Hewitt and Manning 2019; Conneau et al. 2018; Belinkov and Glass 2019; \nRavichander et al. 2021). Magnitude-based pruning may remove parameters without \ndegrading benchmark performance, but benchmarks capture only what their designers chose \nto measure (Mohanty et al. 2024). Sparse autoencoders may discover features that support \naccurate reconstruction, but whether these features carve language at its joints‚Äîor merely at \njoints convenient for reconstruction‚Äîcannot be determined through internal metrics (Shu et \nal. 2025; Templeton et al. 2024; Cunningham et al. 2023). The limitations of these metrics are \nincreasingly evident: models can maintain performance on benchmarks such as MMLU even \nwhen 40% of their layers are removed, while next-token prediction on held-out data degrades \nproportionally to pruning depth (Gromov et al. 2025; Men et al. 2025). If a model can lose \nnearly half its computational depth without degrading on a \"language understanding\" \nbenchmark, either the pruned layers contributed nothing to language understanding, or the \nbenchmark does not measure it. Without external grounding, there is no principled way to \nPage 2 \n"}, {"page": 3, "text": "BLUM: Brain-LLM Unified Model \nadjudicate. The proliferation of benchmarks does not resolve this problem; it multiplies \nevaluation metrics without establishing contact with the phenomenon being modeled. \nLLMs are, fundamentally, models of human language. Language is not an abstract formal \nsystem but a biological capacity that emerges from human neural organization. The only \nsystems that genuinely produce and comprehend language‚Äîas opposed to generating \nstatistically plausible approximations‚Äîare human brains. This makes human language \nprocessing not merely one possible reference point among many, but the appropriate ground \ntruth against which language models can be evaluated. \nMoreover, the human language system did not evolve to optimize benchmark performance or \ntoken prediction accuracy. Evolution solves for survival, not for any particular cognitive \ncapacity in isolation (Buzs√°ki 2019). Language probably emerged in service of survival, \nshaped by pressures fundamentally different from those governing LLM training‚Äîpressures \ninvolving real-time communication, social coordination, and adaptive behavior in uncertain \nenvironments. The brain's organization of language reflects these evolutionary constraints: \nrobust, efficient under severe metabolic limitations, and integrated with perception, action, \nand social cognition. This makes convergence between artificial and biological language \nsystems a meaningful empirical question rather than an expected outcome. Where LLM \nperturbation produces breakdown patterns that correspond to human lesion-symptom \nrelationships, we gain evidence that the artificial system has captured aspects of how \nlanguage is organized in biological systems. Where correspondence fails, we learn that the \nmodel‚Äîwhatever computations it performs‚Äîis solving a different problem than the one \nevolution solved. \nIf human language processing provides the appropriate ground truth for evaluating LLMs, \nthen the study of language breakdown in humans becomes directly relevant to understanding \nartificial language systems. Aphasia‚Äîthe impairment of language following brain \ndamage‚Äîhas yielded over a century of causal evidence about how the human language \nsystem is organized (Broca 1861; Wernicke 1874; Geschwind 1970). Unlike correlational \nneuroimaging methods, lesion studies establish which neural substrates are necessary for \nspecific language functions: when damage to a region consistently produces a specific deficit, \nthat region is causally implicated in the corresponding computation. Decades of research in \nindividuals with post-stroke aphasia have mapped the functional architecture of human \nlanguage with remarkable precision, identifying dissociable neural substrates for \nPage 3 \n"}, {"page": 4, "text": "BLUM: Brain-LLM Unified Model \nphonological, semantic, syntactic, and fluency processes (Fridriksson et al., 2018; Mirman et \nal., 2015; Bates et al., 2003; Dronkers et al., 2004). Modern multivariate approaches account \nfor more than 60% of the variance in chronic aphasia severity from lesion location alone \n(Pustina et al., 2017; DeMarco and Turkeltaub, 2018). This methodology provides exactly \nwhat LLM interpretability lacks‚Äîan externally grounded, causally validated map of how a \nreal language system is organized. \nComparing how LLMs fail under perturbation to how humans fail following brain damage is \ntherefore not an arbitrary exercise in cross-system comparison. It is the principled application \nof the only available ground truth for language processing to the evaluation of systems \ndesigned to model language. Crucially, this approach respects our ignorance: we do not need \na complete theory of human cognition to learn from empirical patterns of language \nbreakdown. By using data accumulated over 150 years of clinical observation, we can let the \nevidence reveal which aspects of LLM organization correspond to biological necessity and \nwhich do not, without presupposing that we understand either system completely. If \nperturbing an LLM component produces errors that correspond to those produced by damage \nto specific brain regions, we gain evidence that the component implements computations \nrelated to those performed by the neural substrate. If perturbation produces errors that bear no \nresemblance to any human deficit pattern, we conclude that the component‚Äîwhatever \nfunction it serves‚Äîdoes not implement language processing as it occurs in biological \nsystems. \n1.2 Lesion‚ÄìSymptom Mapping as External Ground Truth \nA central strength of lesion-based approaches is their ability to identify neural regions that are \nindispensable for successful task performance. Whereas popular activation-based measures \nindicate which regions are engaged during a task, lesions reveal which computations are truly \nnecessary. Functional neuroimaging and brain-model alignment approaches largely rely on \ncorrelational associations, which cannot distinguish whether a region or representation is \nnecessary for a given computation. In contrast, focal brain lesions offer direct evidence of \ncausal necessity by revealing which computations fail when specific neural substrates are \ndestroyed. Lesion‚Äìsymptom mapping has served as the gold standard for establishing causal \nbrain-behavior relationships for well over a century (Broca 1861; Wernicke 1874; Geschwind \n1970). Stroke provides natural experiments revealing which computations cannot be \nadequately compensated by remaining tissue and which can be lost with minimal \nPage 4 \n"}, {"page": 5, "text": "BLUM: Brain-LLM Unified Model \nconsequence. Decades of research in individuals with post-stroke aphasia have established \nthat the human language system comprises distributed but functionally specialized regions \nwhose contributions can be causally identified (Bates et al. 2003; Dronkers et al. 2004; \nFridriksson et al. 2018). Modern multivariate approaches predict over 60% of variance in \nchronic aphasia severity from lesion location alone (Pustina et al. 2017; DeMarco and \nTurkeltaub 2018), and large-scale studies consistently identify dissociable neural substrates \nfor phonological, semantic, syntactic, and fluency processes (Fridriksson et al. 2018; Mirman \net al. 2015).  This body of work provides precisely what LLM interpretability currently lacks: \nan external criterion for distinguishing essential from dispensable computations. \n1.3 The Brain‚ÄìLLM Unified Model Framework \nBuilding on this motivation, we introduce the Brain‚ÄìLLM Unified Model (BLUM), a \nframework that uses human lesion‚Äìsymptom relationships as an external reference structure \nfor interpreting large language models (LLMs) (Figure 1). BLUM does not assume that \nbiological and artificial systems implement language through identical mechanisms, nor does \nit necessarily treat the human brain as a template that LLMs are expected to replicate. \nInstead, it leverages a well-established causal map of human language organization to ask a \nmore constrained and falsifiable question: whether the error patterns produced by perturbed \nLLMs can be meaningfully situated within a neurobiologically grounded space defined by \nhuman functional necessity. In humans, decades of lesion‚Äìsymptom mapping demonstrate \nthat patterns of spoken errors carry information about which neural substrates are causally \nindispensable for language performance. BLUM takes this causal structure as given on the \nhuman side and uses it as an external standard against which LLM behavior can be evaluated. \nThe core premise of BLUM is that behavioral error profiles can serve as a shared \nrepresentational interface between humans and language models, even when the underlying \nsubstrates and learning histories differ radically. Symptom-to-lesion models are trained \nexclusively on human data, where irreversible neural damage licenses causal inference and \nestablishes a mapping from error structure to neuroanatomical necessity. Error profiles \ngenerated by perturbed LLMs are then projected into this human-defined lesion space. If \nLLM error patterns were arbitrary, idiosyncratic, or organized along dimensions unrelated to \nhuman language function, this projection would either fail or yield anatomically nonspecific \npredictions. Systematic alignment, by contrast, would indicate that the dimensions governing \nbreakdown in LLM performance under perturbation overlap with dimensions of breakdown \nPage 5 \n"}, {"page": 6, "text": "BLUM: Brain-LLM Unified Model \nthat are causally critical in the human brain. Importantly, such alignment does not imply \nmechanistic equivalence, biological homology, or independent rediscovery of \nneuroanatomical organization by the model; rather, it demonstrates that LLM behavior can be \nembedded within a space defined by human causal constraints. \nThe BLUM pipeline proceeds in three stages. First, symptom-to-lesion models are trained on \nhuman aphasia data to predict lesion location from behavioral error profiles alone, \nestablishing that error structure contains sufficient information to reconstruct \nneuroanatomical damage. Second, systematic perturbations are applied to LLM components, \nand the same clinically validated language assessments used with human patients are \nadministered to the perturbed models, with outputs classified using an identical error \ntaxonomy. Third, LLM error profiles are projected into human lesion space using the \nhuman-trained models, and the resulting predicted lesions are evaluated by comparison to \nactual lesions observed in humans with similar error profiles. If lesion patterns inferred from \nLLM error profiles correspond to lesion patterns observed in behaviorally matched humans \nmore strongly than expected by chance, this provides evidence that LLM error behavior \nreflects functionally meaningful structure aligned with human language organization. In this \nway, BLUM establishes human clinical data as an external, causally grounded reference point \nfor evaluating which perturbations and components of LLMs are functionally consequential, \ncomplementing internal-model interpretability methods with an independent standard rooted \nin human neuroscience. \n1.4 Present Study \nThe present study introduces BLUM as an initial experimental test of a broader framework \nfor using human lesion‚Äìsymptom relationships as external ground truth for LLM \ninterpretability. Using data from individuals with chronic post-stroke aphasia and a \n13-billion-parameter transformer-based language model, we apply BLUM to two \nwell-characterized, complementary language tasks: picture naming (Philadelphia Naming \nTest: PNT) and sentence completion (Western Aphasia Battery‚ÄìRevised: WAB-R). We \nselected picture naming because anomia is the hallmark impairment in aphasia, present across \nall subtypes and severity levels (Goodglass and Kaplan 1983; Laine and Martin 2023). We \nselected sentence completion because it mirrors the next-word prediction task, which is \nfundamental to LLM architecture. By administering identical assessments to both human \npatients and perturbed LLMs and classifying responses using a common error taxonomy, we \nPage 6 \n"}, {"page": 7, "text": "BLUM: Brain-LLM Unified Model \nevaluated whether LLM-derived predicted lesions correspond to actual lesions in \nerror-matched humans, and whether these predictions respect the anatomical organization \nestablished by decades of human lesion‚Äìsymptom mapping. Importantly, the present analyses \nare intentionally limited in scope. The goal here is not to exhaustively model human \nlanguage, but to establish proof of concept: to test whether even a restricted set of clinically \ngrounded behavioral dimensions is sufficient to reveal nontrivial, anatomically meaningful \ncorrespondence between perturbed LLMs and the aphasic human brain. This is a necessary \nfirst step towards extending BLUM to richer linguistic features and more sophisticated \narchitectures in future work.  \n \nFigure 1. The Brain-LLM Unified Model (BLUM) framework. The BLUM approach establishes \nsystematic mappings between lesion‚Äìdeficit relationships in the human brain and perturbation‚Äìdeficit \nrelationships in large language models (LLMs). Left: Both humans with aphasia (top) and LLMs \n(bottom) perform a picture-naming task. Different error types emerge from each system, including \nsemantic errors ('Goat'), mixed errors ('Ship'), and phonological errors ('Sheeb'). Top right: In humans, \ndistinct error types are associated with damage to different brain regions. Bottom right: Different error \ntypes in LLMs are associated with perturbations to different transformer layers. Center: The BLUM \nalgorithm maps LLM error profiles into human brain space using symptom-to-lesion models, enabling \nvalidation against actual human lesions. \nPage 7 \n"}, {"page": 8, "text": "BLUM: Brain-LLM Unified Model \n2. Methods \n2.1 Overview \nThe BLUM pipeline proceeds in three stages (Figure 2). First, we establish \nsymptom-to-lesion models in humans that predict lesion location solely from spoken error \nprofiles, validating that behavioral patterns contain sufficient neuroanatomical information to \npredict lesion patterns. Second, in the LLM, we apply systematic perturbations to transformer \nlayers, varying target layer, percent disruption, and noise level, and classify the resulting \noutput errors using the same taxonomy applied to human behavior to characterize error \ndistributions across perturbation conditions. Third, we map LLM error profiles into human \nbrain space by passing them through our human data-based symptom-to-lesion models and \nvalidate the resulting predicted lesions against actual lesions in humans with similar error \nprofiles. \nThis design tests whether LLM error patterns contain information that maps onto human \nneuroanatomy in predictable ways. Again, correspondence between LLM-derived predictions \nand actual human lesions would validate BLUM as a framework for using clinical data as \nexternal ground truth for LLM interpretability. \n \nFigure 2. BLUM pipeline. First, we applied our patented symptom-to-lesion pipeline to build \npredictive models that map aphasia error patterns to lesion locations in human patients. Next, we input \nerror profiles generated by large language models (LLMs) into these human-trained models to \ngenerate corresponding virtual brain lesions. We then identified the five human patients whose \naphasia error profiles most closely matched each LLM error profile and computed the average of their \nlesion maps. To assess alignment, we calculated the Pearson correlation between this averaged human \nlesion and the LLM-predicted lesion and compared it, using permutation testing (2,000 permutations), \nPage 8 \n"}, {"page": 9, "text": "BLUM: Brain-LLM Unified Model \nto correlations obtained between the LLM-predicted lesion and averaged lesions from five randomly \nselected human patients. This analysis demonstrates that LLM error patterns encode neurobiologically \nmeaningful information. \n2.2 Human Foundation Data \nData were drawn from an archival database maintained by the Aphasia Lab at the University \nof South Carolina. Although the original database contains data from 410 patients with \nchronic post-stroke aphasia, the final sample used in the analyses presented here was 214 \n(PNT) and 184 (WAB-R). This reduction was caused by not all participants having individual \nspoken errors transcribed on the aphasia tests or insufficient lesion data. Despite the reduced \nsample size, the final sample represents one of the largest lesion-symptom mapping studies to \ndate. All participants had left-hemisphere-only stroke confirmed by neuroimaging, presence \nof aphasia as determined by clinical evaluation, and premorbid proficiency in English.  \nParticipants completed the Western Aphasia Battery‚ÄìRevised (WAB-R) and the Philadelphia \nNaming Test (PNT). We developed a unified error classification system to categorize \nresponses as either correct or as one of six error types: Correct, Semantic, Phonemic \n(combining Formal and Nonword errors), Mixed, Neologism, No Response, and Unrelated. \nThe automated classification system achieved >97% agreement with consensus human \nannotations (see Supplementary Methods S1 for the full decision tree and inter-rater \nreliability analysis). \nStructural MRI data were acquired for all participants. Lesion masks were manually \ndemarcated on T2-weighted images and normalized to Montreal Neurological Institute space \nusing enantiomorphic normalization (see Supplementary Methods S2 for neuroimaging \ndetails). Normalized lesion maps were intersected with the Johns Hopkins University (JHU) \natlas, yielding lesion load values (proportion of voxels damaged) for 189 cortical and white \nmatter regions. Lesion coverage across participants is shown in Supplementary Figure S1. \n2.3 Symptom-to-Lesion Models \nWe trained models to predict lesion location solely from error profiles, laying the foundation \nfor mapping LLM errors into brain space. For each participant, we paired their error profile \n(proportions of each error type across WAB-R and PNT responses) with their lesion map \n(proportion of damage for each of 24 a priori language-related cortical regions). These areas \nconstituted a subset of the 189 regions of interest (ROI) in the Johns Hopkins University \n(JHU) neuroanatomical atlas (Faria et al. 2012) and were based on their empirically \nPage 9 \n"}, {"page": 10, "text": "BLUM: Brain-LLM Unified Model \ndemonstrated relationship to language production and comprehension (Fridriksson et al. \n2018; Papagno 2011; Janelle et al. 2022; Nogueira et al. 2025). Details of the lesion-symptom \nmapping statistical framework are provided in Supplementary Methods S3. \nWe employed a per-ROI regression framework: for each of the 24 a priori left-hemisphere \nROIs, we trained a linear regression model predicting lesion load from error proportions. \nModel performance was evaluated using leave-one-out cross-validation, computing R¬≤ and \nPearson correlation between predicted and actual lesion loads for held-out participants. A \nfinal model, trained on all participant data, was saved for future stages of the pipeline. \nThis approach tested whether error profiles contain sufficient information to predict lesion \nlocation. Successful prediction validates the symptom-to-lesion direction and establishes \ntrained models that can be applied to any error profile, including those generated by \nperturbed LLMs. \n2.4 LLM Selection and Lesioning \nFor all LLM analyses, we used a Large Language and Vision Assistant \n(LLaVA-1.6-Vicuna-13B) model. This model has 13 billion parameters and is a \nstate-of-the-art vision‚Äìlanguage model (Liu et al. 2023). Our choice to use this model was \ndictated by methodological constraints of the BLUM framework. Picture naming (PNT) \nrequires direct visual input and single-word responses, which precludes text-only language \nmodels without introducing external vision pipelines that would confound interpretation; \nLLaVA enables end-to-end image-to-word processing within a unified architecture. All \nperturbations and analyses (for both PNT and WAB-R-related processing) utilized the Vicuna \nlanguage model component, ensuring comparability across tasks. Vicuna-13B provides \nsufficient scale to exhibit graded, human-like error profiles under perturbation while \nremaining fully accessible for systematic, layer-wise causal intervention, unlike larger \nproprietary models (Chiang et al. 2023). Our aim was not to compare architectures but to test \nwhether a contemporary, perturbable transformer exhibits error structures that can be \nmeaningfully projected into human lesion space. \nWe evaluated the model on WAB-R sentence-completion and responsive-speech items, the \nsame items administered to human participants. Outputs were generated with deterministic \ndecoding (do_sample=False) and fixed random seeds to ensure reproducibility. LLM outputs \nwere classified using the same error taxonomy applied to human responses. \nPage 10 \n"}, {"page": 11, "text": "BLUM: Brain-LLM Unified Model \nPerturbations were applied to individual transformer layers using multiplicative Gaussian \nnoise. For each weight w in a targeted layer, we sampled a noise term Œµ ~ ùí©(0, œÉ¬≤), where Œµ \ndenotes a zero-mean random perturbation and œÉ controls its standard deviation (i.e., \nperturbation severity), and updated the weight as w‚Ä≤ = w √ó (1 + Œµ). Each perturbation \ncondition was defined by: layer index (1‚Äì40), modification percentage (proportion of weights \nperturbed 10‚Äì19%, in 10% increments), and noise standard deviation (œÉ=0.1‚Äì, in 0.1-unit \nincrements). For each given condition of layer, modification percentage, and noise, the subset \nof nodes selected for perturbation was randomly chosen from the 5,120 hidden units in the \ntargeted layer. This parameterization allows systematic exploration of lesion location (layer), \nextent (modification percentage), and severity (noise level). Additional implementation \ndetails and parameter sweeps are described in Supplementary Methods S4. \n2.5 Mapping LLM Errors to Brain Space \nFor each LLM perturbation condition, we first reduced the dataset by removing degenerate \nerror profiles (total error mass < 0.02, effectively error-free; the No Response category \nexceeding 0.95, indicating model collapse to nonresponse; or entropy <0.4 bits, indicating \nerror mass concentrated in a single category), resulting in ~1,987 nondegenerate PNT LLM \nconditions and 2,228 nondegenerate WAB-R LLM conditions. We then extracted the error \nprofile (proportions of each error type) from each LLM condition and passed it to our human \ndata-based symptom-to-lesion model to generate predicted lesion load for each of the 24 \nlanguage-related ROIs, constituting a predicted brain lesion for that LLM condition.  \n2.6 Validation Against Human Lesions \nTo validate LLM-derived predicted lesions, we compared them to lesion profiles in humans \nwith similar aphasia error profiles. Specifically, for each LLM perturbation condition, we \nidentified the top k = 5 humans whose error profiles were most similar to the LLM‚Äôs error \nprofile, measured using Euclidean distance on raw error proportions. The choice of k = 5 \nreflects a bias‚Äìvariance tradeoff: smaller k yields unstable lesion estimates driven by \nidiosyncratic stroke anatomy, whereas larger k reduces anatomical specificity by averaging \nacross heterogeneous aphasia phenotypes. This value is consistent with prior lesion-based \nvalidation approaches that average across small, behaviorally matched patient sets (Bates et \nal. 2003; Pustina et al. 2018; DeMarco and Turkeltaub 2018; Fridriksson et al. 2018). We \nthen computed the mean lesion profile across these matched human participants and \nquantified its similarity to the LLM-predicted lesion profile using Pearson correlation. \nPage 11 \n"}, {"page": 12, "text": "BLUM: Brain-LLM Unified Model \nTo assess whether this correspondence exceeded what would be expected by chance, we \nconstructed a random baseline for each condition by repeatedly sampling sets of five human \nparticipants at random (2,000 permutations per condition), computing the mean lesion profile \nfor each random set, and measuring its correlation with the LLM-predicted lesion profile. \nThis yielded a null distribution of correspondence values against which the observed \nerror-matched lesion profile correspondence was compared. \nStatistical significance was evaluated at both the condition level and the population level. At \nthe condition level, permutation-based p-values were computed as the proportion of \nrandom-set correlations exceeding the observed correlation. At the population level, we \ntested whether correspondence between LLM-predicted lesions and error-matched human \nlesions was consistently greater than correspondence with randomly selected human lesions \nusing binomial tests on the excess of significant conditions, and Wilcoxon signed-rank tests.‚Äã\n‚Äã\n3. Results‚Äã\n3.1 Human-Based Symptom-to-Lesion Models Predict Lesion Location \nError-based symptom-to-lesion models predicted lesion damage to a subset of \nlanguage-related JHU brain regions with moderate-to-high accuracy in held-out humans \nacross both the PNT and WAB-R tasks (Figure 3). We derived these results using \nleave-one-out cross-validation, in which each regional model was trained to predict lesion \nload for a held-out participant using models fit on all remaining participants (PNT: N-1 = \n213, WAB-R: N-1 = 183). Figure 3 shows the average predictive accuracy (R¬≤) for each ROI. \nFor the PNT-based models, prediction performance was the strongest in the left superior \nlongitudinal fasciculus (SLF_L; R¬≤ = 0.26), posterior middle temporal gyrus (PSMG_L; R¬≤ = \n0.22), left superior temporal gyrus (STG_L; R¬≤ = 0.22), and left posterior superior temporal \ngyrus (PSTG_L; R¬≤ = 0.21). Additional regions showed moderate predictive accuracy, \nincluding the left angular gyrus (AG_L), left posterior insula (PIns_L), and left middle \noccipital gyrus (MOG_L). \nFor the WAB-R-based models, prediction accuracy was likewise highest in the SLF_L (R¬≤ = \n0.21), PSMG_L (R¬≤ = 0.15), and PSTG_L (R¬≤ = 0.14), with additional contributions from the \nleft supramarginal gyrus (SMG_L) and left middle temporal gyrus (MTG_L). \nPage 12 \n"}, {"page": 13, "text": "BLUM: Brain-LLM Unified Model \nCritically, three regions, the SLF_L, PSTG_L, and PSMG_L, ranked among the most reliably \npredicted regions in both the PNT and WAB-R analyses. This convergence across tasks \nindicates that error profiles derived from independent behavioral batteries encode lesion \ninformation for core dorsal‚Äìtemporal language pathways. Overall error type distributions \nacross the PNT and WAB-R are shown in Supplementary Figure S2, and rank-order \ncorrespondence of lesion‚Äìsymptom associations across PNT and WAB-R error categories is \nreported in Supplementary Table S1. Prediction accuracy was generally higher for larger \nregions with greater lesion prevalence, consistent with the increased statistical power \nafforded by adequate lesion coverage. \nThese results validate that error profiles contain sufficient information to predict lesion \nlocation. The trained models can therefore be meaningfully applied to error profiles, \nincluding those generated by LLMs, establishing the foundation for BLUM cross-system \nmapping. Comprehensive lesion‚Äìsymptom mapping results showing the neural correlates of \neach error type, along with additional validation analyses including complementary \nmultivariate analyses, are provided in Supplementary Results S2. \nFigure 3. Symptom-to-lesion model \nperformance for PNT-based and \nWAB-R-based models. Bar plots \nshow R¬≤ values for each of the 24 a \npriori JHU atlas ROIs known to be \ninvolved in language processing. \nRegions are color-coded by \nanatomical class: dorsal stream \n(blue), ventral stream (green), and \nwhite matter tracts (white). Error \nprofiles alone significantly predict \nlesion location with moderate-to-high \naccuracy, validating the foundation \nfor mapping LLM errors into brain \nspace. Top: PNT-based \nsymptom-to-lesion models. Bottom: \nWAB-R-based symptom-to-lesion \nmodels. \n \n3.2 LLM Perturbations Produce Graded Error Profiles \nLLM error distributions varied systematically with perturbation severity and transformer \nlayer (Figure 4). At low perturbation levels (10‚Äì20% modification, low noise), the model \nmaintained near-ceiling performance with predominantly correct responses. As perturbation \nPage 13 \n"}, {"page": 14, "text": "BLUM: Brain-LLM Unified Model \nseverity increased, correct responses declined systematically and were replaced by a \ncharacteristic progression of error types. \nAt moderate perturbation levels (30‚Äì50% modification), semantic and unrelated errors \nemerged as the dominant error types, suggesting that intermediate disruption preferentially \naffects lexical‚Äìsemantic retrieval. At high perturbation levels (60‚Äì90% modification), \nno-response errors became increasingly prevalent, paralleling patterns observed in severe \naphasia where retrieval failures dominate. \nPerturbation effects also varied across transformer layers. Semantic errors exhibited a \ndistinctive pattern, peaking when middle layers (approximately 8‚Äì20) were perturbed, \nconsistent with evidence that these layers encode abstract semantic representations. Early \nlayers (1‚Äì5) showed greater robustness to perturbation, suggesting redundancy or reduced \ncriticality for task performance. Importantly, LLM error profiles fell within the distribution of \nhuman error profiles (Supplementary Figure S5), confirming that the two systems can be \nmeaningfully compared. The full perturbation parameter space and its effect on error \ndistributions are shown in Supplementary Figure S6. \n \n \n \n \n \n \n \n \n \n \nPage 14 \n"}, {"page": 15, "text": "BLUM: Brain-LLM Unified Model \n \nFigure 4. LLM error distributions across perturbation conditions. Top: Distribution of error types \nas a function of modification percentage (proportion of weights perturbed). Bottom: Distribution of \nerror types as a function of the perturbed transformer layer (1‚Äì40). These graded profiles parallel \npatterns observed in human aphasia. For example, semantic errors peak at moderate perturbation \nlevels and in middle layers, whereas no-response errors dominate at severe perturbation levels. \nPage 15 \n"}, {"page": 16, "text": "BLUM: Brain-LLM Unified Model \n3.3 LLM-Derived Predicted Lesions Correspond to Matched Human Lesions \nValidation of LLM-predicted lesion profiles against human data. \nWe evaluated whether lesion profiles predicted from LLM error behavior corresponded more \nclosely to lesion profiles observed in humans with similar spoken error patterns than expected \nby chance. For each LLM perturbation condition (PNT: N = 1,987; WAB-R: N = 2,228), we \nidentified the five human participants whose error profiles were most similar to the LLM‚Äôs \nerror profile (measured by Euclidean distance on raw error proportions) and computed the \nmean lesion profile across these five matched participants. We then compared the similarity \nbetween this error-matched human lesion profile and the LLM-predicted lesion profile to a \nrandom baseline obtained by repeatedly sampling sets of five humans uniformly at random \nfrom the full participant pool, irrespective of error similarity, and averaging their lesion \nprofiles. \nNaming errors. \nFor the PNT, correspondence between LLM-predicted lesion profiles and lesion profiles of \nerror-matched humans exceeded chance expectations across conditions. Across all \nperturbations, matched correspondence exceeded the random baseline in 1,333 of 1,987 \nconditions (67.1%), a proportion significantly higher than expected under the null hypothesis \nof random matching (binomial test, one-sided p = 5.23√ó10‚àí24). \nAt the distributional level, a Wilcoxon signed-rank test confirmed that the median \nimprovement in lesion-profile similarity relative to the random baseline exceeded zero \n(median Œîr = 0.073; one-sided p = 6.93√ó10‚àí26). Although the mean improvement in similarity \nwas modest (Œîr = 0.013), the consistency of positive differences across conditions indicates a \nsystematic alignment between LLM error behavior and human lesion patterns for the PNT \n(Figure 5). \nSentence completion errors. \nFor the WAB-R, correspondence between LLM-predicted lesion profiles and error-matched \nhuman lesions was significantly stronger. Matched correspondence exceeded the random \nbaseline in 1,522 of 2,228 conditions (68.3%), again significantly above chance (binomial \ntest, one-sided p = 1.25√ó10‚àí68). \nPage 16 \n"}, {"page": 17, "text": "BLUM: Brain-LLM Unified Model \nThe magnitude of this effect exceeded that observed for the PNT. A Wilcoxon signed-rank \ntest revealed a robust positive shift in lesion-profile similarity relative to the random baseline \n(median Œîr = 0.081; one-sided p = 2.62√ó10‚àí92), with a mean improvement of Œîr = 0.071. \nThese results indicate that LLM error patterns on the WAB-R map more strongly and more \nconsistently onto anatomically grounded lesion patterns observed in humans (Figure 5). \nComparison across tasks. \nTogether, these findings demonstrate that LLM error behavior contains anatomically \nmeaningful structure that aligns with human lesion‚Äìsymptom relationships across tasks, with \nsubstantially stronger and more consistent alignment for the WAB-R than for the PNT. This \ndifference implies that the WAB-R elicits error patterns that more directly reflect underlying \nneuroanatomical constraints, whereas PNT performance may involve greater variability or \nredundancy across anatomical substrates. \nFigure 5. Model‚Äìhuman \nlesion similarity relative to \na matched random \nbaseline. For each LLM \ncondition, we compared the \nmodel‚Äôs predicted lesion \nprofile to human lesion data \nafter identifying the k = 5 \nmost similar human \nparticipants in error-profile \nspace (Euclidean distance on \nraw error proportions, \nwithout normalization). \nViolin plots show the \ndistribution across conditions \nof the Pearson correlation \nbetween the LLM-predicted \nlesion profile and the mean \nlesion profile of the \nerror-matched humans (Matched), alongside a matched random baseline (Random) computed by \nrepeatedly sampling random sets of five humans and averaging their lesion profiles. Central lines \nindicate the median and interquartile range. Results are shown separately for the PNT and WAB-R \ndatasets, demonstrating that similarity between model predictions and empirically matched human \nlesions consistently exceeds chance expectations. The broader dispersion for error-matched humans \nreflects condition-specific variability in diagnostic specificity, whereas correlations for randomly \nchosen humans form a narrow null distribution due to exchangeability and averaging. \nPage 17 \n"}, {"page": 18, "text": "BLUM: Brain-LLM Unified Model \n \nFigure 6. Example LLM perturbation conditions illustrating low, medium, and high \ncorrespondence with human lesion profiles. For each example condition, panels show (left to right): \n(1) the LLM error profile (proportions of each error type), (2) the lesion profile predicted from the \nLLM‚Äôs errors using a symptom-to-lesion model trained on human data, (3) the average error profile of \nthe top-k human participants whose error profiles most closely matched the LLM‚Äôs error profile, and \n(4) the mean lesion profile of those same matched human participants. Lesion profiles are shown \nacross 24 anatomically defined ROIs, labeled by JHU atlas region names and color-coded by \nanatomical class (dorsal stream = blue, ventral stream = green, white matter tracts = white). Absolute \nsimilarity between predicted and matched human lesion profiles is quantified using the Pearson \ncorrelation coefficient (C). The top two rows show example WAB-R conditions with strong vs. weak \nabsolute similarity; the bottom row shows the same comparison for the PNT. Filenames include both \nabsolute similarity (C) and specificity relative to chance (ŒîC). Row 1: WAB-R, high similarity \n(L8.0_P80.0_N1.9; C = 0.97). Row 2: WAB-R, low similarity (L28.0_P80.0_N1.3; C = 0.21). Row 3: \nPNT, high similarity (L25.0_P60.0_N1.6_C0.97; C = 0.97). Row 4: PNT, low similarity \n(L33.0_P50.0_N1.6_C0.08; C = 0.08). \n3.4 Anatomical Consistency with Dual-Stream Organization \nWe tested whether LLM-derived predicted lesions respected the dual-stream organization \nestablished by human lesion‚àísymptom mapping. We classified LLM conditions with high \nPage 18 \n"}, {"page": 19, "text": "BLUM: Brain-LLM Unified Model \nsemantic error proportions as 'semantic-dominant' and conditions with high phonemic error \nproportions as 'phonemic-dominant'. \nFor each LLM condition, we computed a ventral‚Äìdorsal stream index defined as the mean \npredicted lesion load across ventral ROIs minus the mean predicted lesion load across dorsal \nROIs. We ranked conditions by a semantic‚àíphonemic error score. As a sensitivity analysis, \nwe compared the top 200 semantic-dominant and top 200 phonemic-dominant LLM \nconditions using a permutation test (50,000 permutations) on the difference in mean stream \nindex; we additionally report Mann‚ÄìWhitney U and Welch t-tests. \nRestricting the analysis to extreme-condition comparisons (top 200 semantic-dominant \nvs. top 200 phonemic-dominant), both the PNT and WAB-R exhibited significantly \ngreater ventral bias for semantic-dominant conditions (PNT mean difference = 0.027, \npermutation p = 2√ó10‚Åª‚Åµ, Cohen‚Äôs d ‚âà 1.37; WAB-R mean difference = 0.056, \npermutation p = 2√ó10‚Åª‚Åµ, Cohen‚Äôs d ‚âà 3.38).  \nThis anatomical dissociation replicates patterns established by decades of human \nlesion‚Äìsymptom mapping, suggesting that LLM error patterns reflect processing distinctions \nthat parallel the functional organization of human language (Indefrey and Levelt 2004).  \nFigure 7. Evidence for dual-stream (dorsal‚àíventral) organization for phonemic vs. \nsemantic extreme conditions in LLMs. Violin plots show the distribution of the \nventral‚Äìdorsal stream index for the top 200 semantic-dominant and top 200 \nphonemic-dominant LLM conditions. Lower values indicate relatively greater predicted \ndamage in dorsal-stream brain regions, whereas higher values indicate relatively greater \npredicted damage in ventral-stream regions. Left: PNT (mean difference = 0.027; permutation \np = 2√ó10‚Åª‚Åµ). Right: WAB-R (mean difference = 0.056; permutation p = 2√ó10‚Åª‚Åµ). Dots indicate \nmedians; jittered points show individual conditions. These comparisons indicate that \nLLM-predicted lesion profiles exhibit ventral-stream bias for strongly semantic conditions and \nPage 19 \n"}, {"page": 20, "text": "BLUM: Brain-LLM Unified Model \ndorsal-stream bias for strongly phonemic conditions, with a larger effect size observed for \nWAB-R. \n3.5 Divergences Between Biological and Artificial Systems \nSeveral divergences emerged between biological and artificial systems. Phonemic errors \noccurred substantially less frequently in LLM perturbations than in human aphasia, \nsuggesting that transformer architectures may implement more robust phonological encoding \nor that their failure modes preferentially affect semantic rather than phonological processing. \nOne likely source of this divergence is a difference in representational units and processing \nstages. Human language production involves explicit phonological encoding and articulatory \nsequencing, supported by dorsal-stream mechanisms, such that damage frequently yields \nphonemic paraphasias and neologisms. In contrast, transformer-based LLMs operate over \ndiscrete lexical or sublexical tokens that are optimized for statistical prediction rather than \nphonological assembly. As a result, perturbations in LLMs are more likely to manifest as \nsemantic or lexical substitution errors than as phonological assembly errors, which have no \ndirect computational analogue in current token-based architectures. No-response errors \nemerged at extreme perturbation levels, but their layer-wise gradients differed from those \npredicted by human lesion patterns. The divergent layer-wise gradients observed for No \nResponse errors warrant special consideration. In human aphasia, no-response behavior often \nreflects breakdowns in lexical retrieval or phonological encoding that occur despite preserved \nengagement with the task, and is typically associated with damage to specific dorsal or \nventral stream components depending on the underlying deficit. In contrast, in LLMs, \nno-response errors frequently emerge at extreme perturbation levels and may reflect global \ndisruption of generation dynamics, such as collapse of token probability distributions or \nfailure to initiate decoding, rather than selective impairment of a linguistic subroutine. The \nfact that superficially similar error categories arise from different internal failure modes \nunderscores an important strength of the BLUM framework: it does not presuppose \nequivalence between biological and artificial systems, but instead enables identification of \nwhere behavioral similarity masks mechanistic divergence. \nThese divergences highlight key differences between transformer architectures and biological \nlanguage systems. Importantly, such differences are themselves informative, as they identify \naspects of language processing where artificial and biological systems may rely on \nfundamentally different computational strategies. \n4. Discussion \nPage 20 \n"}, {"page": 21, "text": "BLUM: Brain-LLM Unified Model \n4.1 Summary of Principal Findings \nThe present study establishes a proof of concept for BLUM as a potential framework for \nusing human clinical data as external ground truth for LLM interpretability. Three principal \nfindings support this framework. First, symptom-to-lesion models trained on spoken error \nprofiles from individuals with chronic post-stroke aphasia predicted lesion location with \nmoderate-to-high accuracy, establishing that behavioral error patterns encode sufficient \nneuroanatomical information to reconstruct cortical damage location (Bates et al. 2003; \nPustina et al. 2017; DeMarco and Turkeltaub 2018). Second, systematic perturbations to \ntransformer layers produced graded error profiles that fell within the distribution of human \nerror profiles, confirming that the two systems can be meaningfully compared using a \ncommon error classification taxonomy (Patterson et al. 1994; Plaut 1996). Third, lesion \nprofiles predicted from LLM error patterns corresponded to actual lesions in humans with \nsimilar error profiles above chance in 67% of picture-naming conditions and 68.3% of \nsentence-completion conditions; semantic-dominant LLM errors predicted ventral-stream \nlesions, and phonemic-dominant errors predicted dorsal-stream lesions (Hickok and Poeppel \n2007; Fridriksson et al. 2018; Mirman et al. 2015). Together, these findings demonstrate that \nhuman lesion‚Äìsymptom mapping can serve as external ground truth for understanding LLM \norganization. \n4.2 Human Lesion Data as External Ground Truth for LLM Interpretability \nBLUM addresses a fundamental challenge in LLM interpretability: the absence of external \nvalidation criteria. Current approaches evaluate interpretability claims using metrics internal \nto the model or benchmark performance, without an independent criterion for assessing \nvalidity (Belinkov 2022; Belinkov and Glass 2019; Hewitt and Manning 2019; Conneau et al. \n2018). Probing classifiers may reveal that linguistic information is recoverable from specific \nactivations, but such recoverability does not establish that the information causally \ncontributes to model outputs (Ravichander et al. 2021). Sparse autoencoders may identify \ninterpretable features, but without external validation it remains unclear whether these \nfeatures represent the correct decomposition (Cunningham et al. 2023; Bricken et al. 2023; \nTempleton et al. 2024).  \nBLUM leverages what is arguably the most rigorous causal framework available for \nunderstanding language function: more than a century of human lesion‚Äìsymptom mapping \n(Broca 1861; Wernicke 1874; Geschwind 1970). The accumulated knowledge from clinical \nPage 21 \n"}, {"page": 22, "text": "BLUM: Brain-LLM Unified Model \nneuroscience provides detailed maps of which brain regions are necessary for specific \nlanguage functions (Dronkers et al. 2004; Fridriksson et al. 2018; Mirman et al. 2015), \noffering precisely the kind of external reference that LLM interpretability currently lacks. The \ncorrespondence we observe between LLM-predicted lesions and actual human lesions \ndemonstrates that this form of external validation is achievable. This alignment is not \nguaranteed by the method; rather, it constitutes an empirical finding that validates the use of \nhuman clinical data as an external reference for LLM interpretability \n4.3 Task Differences in Cross-System Alignment \nThe substantially stronger correspondences observed for sentence completion compared to \npicture naming likely reflect differences in task constraints and the mechanistic sources of \nerrors. In humans, sentence completion inherently narrows the space of plausible responses \nfor each item, reducing variability in error patterns and facilitating more consistent mapping \nbetween LLM perturbation effects and human lesion profiles. In contrast, picture naming can \nyield errors at multiple processing stages (Levelt et al. 1999; Dell et al. 1997). For example, \nfailure to produce the correct word could reflect impaired visual recognition, degraded \nsemantic access, failed lexical retrieval, or disrupted phonological encoding, each of which \nmay recruit distinct neural substrates (Indefrey and Levelt 2004; Hickok and Poeppel 2007). \nThis mechanistic complexity introduces variability that may not map cleanly onto LLM \nperturbation effects, suggesting that task selection should be carefully considered when \nextending BLUM to new domains. \n4.4 Behavioral Consistency with Dual-Stream Organization \nThe observed dissociation between semantic-dominant LLM conditions, which predicted \nventral-stream lesions, and phonemic-dominant conditions aligns with the dual-stream \norganization of human language processing (Hickok and Poeppel 2007; Rauschecker and \nScott 2009; Saur et al. 2008). However, this finding should be interpreted with appropriate \ncaution. The symptom-to-lesion models were trained on human patients whose error patterns \nalready reflect dual-stream organization (Fridriksson et al. 2016), raising the possibility that \nthe anatomical dissociation in LLM-predicted lesions is inherited from the training data rather \nthan reflecting intrinsic organizational properties of the LLM itself. Nevertheless, the \ncorrespondence demonstrates that LLM error patterns contain sufficient structure to be \nmeaningfully projected into neuroanatomically organized space. Future work can strengthen \nthese findings by integrating BLUM with direct analysis of representational geometry within \nPage 22 \n"}, {"page": 23, "text": "BLUM: Brain-LLM Unified Model \ntransformer layers (Geiger et al. 2021; Wu et al. 2023), enabling tests of whether behavioral \nalignment with human lesion patterns corresponds to intrinsic organizational properties \nwithin LLM architectures. \n4.5 Divergences Between Biological and Artificial Systems \nThe relative scarcity of phonemic errors in perturbed LLMs compared to human aphasia \nlikely reflects fundamental differences in representational units. In humans, phonemes or \nsyllables constitute the basic units of language processing, and the dorsal stream supports the \nassembly of these units into articulable sequences (Hickok and Poeppel 2007; Hickok 2012). \nLLM tokenizers, by contrast, represent language primarily as whole words or subword units \nsuch as morphemes and word stems (Sennrich et al. 2016; Kudo and Richardson 2018). \nWhen LLM layers are perturbed, failures are therefore more likely to manifest as semantic or \nlexical errors rather than as phonological errors involving incorrect assembly of sound \nsequences. This pattern contrasts with human aphasia, in which phonemic paraphasias and \nneologisms are common consequences of dorsal-stream damage (Buchsbaum et al. 2011; \nFridriksson et al. 2018). Rather than representing a limitation of the BLUM framework, this \ndivergence is scientifically informative, as it highlights an aspect of language processing \nwhere biological and artificial systems employ fundamentally different representational \nstrategies, delineating the boundary conditions of cross-system comparison (Chen et al. 2024; \nFaries and Raja 2022). \n4.6 Implications and Future Directions \nBLUM provides a principled external criterion for evaluating interpretability claims. If \nactivation analyses suggest that a particular set of middle transformer layers encodes \nsemantic information (Meng et al. 2022; Zhang and Nanda 2023), BLUM can test this claim \nby examining whether targeted perturbation of these layers produces error profiles enriched \nfor semantic errors and whether the resulting predicted lesions concentrate in ventral-stream \nregions associated with semantic processing in humans (Binder et al. 2009; Mirman et al. \n2015). If perturbation instead produces diffuse error patterns that fail to map onto coherent \nlesion predictions, the original interpretability claim warrants skepticism. Conversely, if \nperturbation yields anatomically specific predictions that align with established human \nlesion‚Äìsymptom relationships, BLUM provides external validation that complements internal \nactivation analyses (Geiger et al. 2021; Wu et al. 2023). Most critically, BLUM as presented \nPage 23 \n"}, {"page": 24, "text": "BLUM: Brain-LLM Unified Model \nvalidates behavioral alignment but leaves open whether this reflects convergent internal \norganization. A direct test would reverse the mapping: taking human patient error profiles \nand identifying which LLM perturbation conditions produce maximally similar errors. If \npatients with semantic deficits consistently map onto middle-layer perturbations while \npatients with phonological deficits map onto distinct layers, this would constitute evidence \nthat LLM organization recapitulates functional distinctions that are neuroanatomically \ngrounded in humans, transforming BLUM from a behavioral validation framework into a \nbidirectional mapping between biological and artificial systems. \nBLUM also offers a principled approach to model compression grounded in functional \nnecessity rather than parameter magnitude (Han et al. 2015; Ma et al. 2024). Current \ncompression methods rely on magnitude-based or gradient-based heuristics and lack \nprincipled criteria for distinguishing truly dispensable parameters from those supporting \ncapabilities not captured by existing benchmarks (Sun et al. 2023; Kim et al. 2024). If \nperturbation of early transformer layers produces minimal deficits across a range of severity \nlevels, with predicted lesion profiles that fail to correspond to human lesions, these layers \nmay represent candidates for pruning or aggressive quantization (Men et al. 2025; Gromov et \nal. 2025). In contrast, if perturbation of middle layers produces severe deficits even at low \nseverity and yields predicted lesions that strongly correspond to human data, these layers \nshould be preserved. By systematically characterizing perturbation effects across layers and \nmapping the resulting error profiles into brain space, BLUM enables construction of a \nfunctional importance map that distinguishes critical hubs from redundant components (Chen \net al. 2024; Song et al. 2024). Future work should develop compression algorithms that \nleverage these BLUM-identified dispensable components, systematically removing \nparameters whose perturbation produces minimal functional deficits while preserving circuits \nwhose perturbation yields severe and specific impairments. \nIn clinical neuroscience, BLUM opens the possibility of creating patient-specific digital twins \nfor conditions such as aphasia and dementia (Broderick et al. 2022; Laubenbacher et al. \n2021). By identifying LLM perturbation conditions that replicate individual patient error \nprofiles, researchers can generate computational surrogates that simulate patient-specific \ndeficits. Archival databases contain rich clinical data beyond error profiles‚Äîlesion maps, \ntreatment histories, comorbidities, and longitudinal assessments‚Äîthat could serve as \nconditioning variables for multivariate patient-to-model matching. For progressive conditions \nPage 24 \n"}, {"page": 25, "text": "BLUM: Brain-LLM Unified Model \nlike dementia, systematic increases in perturbation severity may simulate disease trajectory, \ngenerating testable predictions about functional decline. Digital twins would enable \nexperiments impractical in human participants: simulating thousands of virtual patients to \nidentify treatment moderators, testing intervention parameters in silico, and generating \nsynthetic datasets for machine learning applications (Plaut 1996; Welbourne and Lambon \nRalph 2007). Validating such twins against held-out patient outcomes would establish clinical \nutility and potentially transform how intervention research is conducted. \nFinally, extending BLUM to multiple LLM architectures is necessary to test generalization \nand identify architecture-specific organizational principles. Applying BLUM to GPT, \nLLaMA, Claude, and related model families would reveal whether correspondence with \nhuman lesion patterns represents a general property of transformer-based language models or \nvaries across architectures (Tuckute et al. 2024; Oota et al. 2023). The aphasia database used \nhere also contains extensive behavioral data beyond the tasks examined in this study, \nincluding assessments of auditory comprehension (Kertesz and Raven 2007), syntax (Caplan \net al. 2007), reading (Coltheart et al. 2001), and writing (Rapcsak and Beeson 2004), as well \nas measures of non-verbal cognitive abilities (Helm-Estabrooks 2001; Murray 2012). These \ndata enable testing whether BLUM generalizes beyond spoken language production, \npotentially revealing whether the framework captures broader aspects of cognition. \n4.7 Limitations \nSeveral limitations warrant consideration across the three stages of the BLUM pipeline. At \nthe foundation stage, symptom-to-lesion models were trained on a single database of \nEnglish-speaking participants from one research center; generalization to other languages, \netiologies, and populations requires further testing (Swinburn et al. 2004; Ivanova et al. \n2021). At the translation stage, we evaluated a single model architecture, \nLLaVA-1.6-Vicuna-13B (Liu et al. 2023), and extension to other architectures remains an \nempirical question. The assumption that error profiles are comparable across biological and \nartificial systems despite fundamental architectural differences is strong (Tuckute et al. 2024; \nAntonello et al. 2024), and tokenization strategies employed by LLMs may limit certain error \ntypes (Sennrich et al. 2016). At the validation stage, we employed a top-k matching approach \nwith k = 5 and Pearson correlation as the similarity metric; alternative matching strategies or \nsimilarity measures might yield different results (Kriegeskorte et al. 2008). In addition, \nvalidation was limited to two tasks, capturing only a subset of language functions. Despite \nPage 25 \n"}, {"page": 26, "text": "BLUM: Brain-LLM Unified Model \nthese limitations, the consistent above-chance correspondence observed across nearly 2,000 \nperturbation conditions, combined with anatomically appropriate dual-stream dissociations, \nestablishes proof of concept for the BLUM framework.‚ÄÉ \n5. Conclusions \nThis study demonstrates that human lesion‚Äìsymptom mapping‚Äîthe gold standard for \nestablishing causal brain-behavior relationships for over a century‚Äîcan serve as an external \nreference structure for evaluating LLM perturbation effects. By applying identical clinical \nassessments to stroke patients and perturbed LLMs and classifying responses using a \ncommon error taxonomy, we showed that LLM error profiles can be meaningfully projected \ninto human neuroanatomical space. \nThree findings support this framework: symptom-to-lesion models predicted lesion location \nfrom error profiles alone; LLM perturbations produced error distributions comparable to \nhuman aphasia; and LLM-predicted lesions corresponded to actual lesions in error-matched \nhumans, with semantic errors mapping onto ventral-stream lesion patterns and phonemic \nerrors onto dorsal-stream patterns consistent with the organization learned from human \npatients. These findings validate behavioral alignment between perturbed LLMs and human \naphasia, while leaving open whether such alignment reflects convergent internal \norganization‚Äîa question that motivates direct investigation through reverse mapping of \nhuman error profiles onto LLM perturbation space. For AI development, BLUM provides a \nprincipled approach to model compression based on functional necessity and offers external \nvalidation for interpretability claims. For clinical neuroscience, the framework opens the \npossibility of creating patient-specific digital twins that incorporate rich clinical data to \nsimulate individual deficits, model disease progression, and predict treatment responses \nwithout requiring participation in costly trials.\n \nPage 26 \n"}, {"page": 27, "text": "BLUM: Brain-LLM Unified Model \nDeclarations \nCompeting Interests \nJ.F. and J.C. have ownership interests in NXTLLM, LLC, and ALLT.AI, which hold patents \nrelated to the Brain-LLM Unified Model (BLUM) technology described in this manuscript. \nThe symptom-to-lesion mapping approach is protected under U.S. Patent US-10916348-B2. \nJ.F. and R.N. inventors on a provisional U.S. patent application related to methods described \nin this work (U.S. Provisional Application No. 63/974,622, filed February 3, 2026). Other \nauthors declare no competing interests.  \nFunding \nThis research was supported by a SmartState endowment established with private funding \narranged by the state of South Carolina for J.F. The funder had no role in study design, data \ncollection, and analysis, decision to publish, or preparation of the manuscript. \nData Availability \nBehavioral and demographic data will be made available upon reasonable request to the \ncorresponding author. Lesion data cannot be shared publicly due to patient privacy \nconstraints, but are available to qualified researchers through data use agreements. \n \nPage 27 \n"}, {"page": 28, "text": "BLUM: Brain-LLM Unified Model \nSupplementary Materials \nSupplementary Methods \nS1. Error Taxonomy and Classification \nWe developed a unified error classification system to categorize responses from both the \nWestern Aphasia Battery‚ÄìRevised (WAB-R) and the Philadelphia Naming Test (PNT) using a \ncommon taxonomy, enabling direct comparison of error distributions across tasks. The \nclassification scheme was adapted from standard naming-error taxonomies used in aphasia \nresearch (Schwartz et al. 2006; Dell et al. 1997) and implemented as a rule-based decision \ntree. \n \nSupplementary Methods Figure 1. Decision-tree for classification of naming responses. This \nflowchart illustrates the step-by-step procedure used to classify naming responses based on response \npresence, lexical status, phonetic similarity, orthographic overlap, and semantic relatedness. \nResponses were first categorized based on whether the participant produced a scorable verbal \nresponse. Failures to respond within the allotted time or explicit statements of inability (e.g., \n\"I don't know\") were classified as No Response errors. All other responses were categorized \nas Produced Word and subjected to further classification. \nFor produced responses, the decision tree first determined whether the output was a real \nEnglish word. Real-word responses were then evaluated for correctness: responses matching \nPage 28 \n"}, {"page": 29, "text": "BLUM: Brain-LLM Unified Model \nthe target or an accepted synonym were classified as Correct. Incorrect real-word responses \nwere assessed for semantic relatedness to the target using two normative databases: the \nFlorida Associative Norms (Nelson et al. 2004) and a taxonomic similarity dataset. \nResponses with a documented semantic relationship to the target (associative, categorical, or \nfunctional) were classified as Semantic errors (e.g., \"goat\" for sheep). Real-word responses \nthat were both semantically related and phonologically similar to the target (‚â•50% phoneme \noverlap) were classified as Mixed errors (e.g., \"rat\" for cat). Real-word responses with no \nidentifiable semantic relationship to the target were classified as Unrelated errors if \nphonologically dissimilar, or as Formal errors if they shared ‚â•50% phonological similarity \nwith the target (e.g., \"sheep\" for sheet). \nNonword responses were classified based on their phonological and orthographic similarity \nto the target. Nonword errors exhibited ‚â•50% phoneme or letter overlap with the target (e.g., \n\"sheeb\" for sheep), representing phonological distortions that retained substantial similarity \nto the intended word. Neologisms exhibited <50% overlap with the target, representing \nsevere phonological disruptions that obscured the relationship to the intended response. \nFollowing standard practice in the aphasia literature, Formal and Nonword errors were \nconsolidated into a single Phonemic error category for analyses requiring comparison with \nprior lesion-symptom mapping studies. \nAll responses were independently coded by two trained raters: a speech-language pathologist \nwith clinical experience in aphasia assessment and a linguist specializing in lexical-semantic \nprocessing. Each rater independently categorized every response according to the decision \ntree. Inter-rater discrepancies were resolved through discussion, and consensus labels were \nused for all subsequent analyses. \nTo facilitate reliable classification of large datasets and ensure reproducibility, we developed \nan automated scoring system implementing identical decision-tree criteria. Phonological \nsimilarity was computed algorithmically using phoneme-level Levenshtein distance, with \nphonemic transcriptions derived from the Carnegie Mellon University Pronouncing \nDictionary. Semantic relatedness was determined by querying the Florida Associative Norms \nand taxonomic similarity databases. The automated system achieved >97% agreement with \nconsensus human annotations. Disagreements occurred most frequently for responses near \nclassification boundaries, particularly between Semantic and Unrelated categories (27 \ndiscrepancies in 908 error responses). \nPage 29 \n"}, {"page": 30, "text": "BLUM: Brain-LLM Unified Model \nS2. Neuroimaging Acquisition and Lesion Delineation \nStructural magnetic resonance imaging (MRI) data were acquired for all participants using \nT1-weighted and T2-weighted sequences. Lesion masks were generated following protocols \nestablished in prior large-scale lesion‚Äìsymptom mapping studies of aphasia (Fridriksson et \nal., 2018). Lesions were manually demarcated on T2-weighted images by a trained \nneurologist, as T2 sequences provide superior contrast for identifying chronic stroke damage. \nManual delineation remains the gold standard for lesion identification, ensuring accurate \ncapture of infarct boundaries that automated methods may miss or misclassify. \nLesion masks were normalized to Montreal Neurological Institute stereotactic space using \nenantiomorphic normalization (Nachev et al. 2008), a procedure that addresses distortion \nartifacts that can arise when normalizing brains with large lesions. This approach replaces \ndamaged tissue with mirrored tissue from the intact contralateral hemisphere before spatial \nnormalization, yielding more accurate alignment with the template brain. The normalization \npipeline was implemented using our open-source nii_preprocess toolbox. \nNormalized lesion maps were intersected with the Johns Hopkins University (JHU) atlas. \nThis widely used parcellation scheme provides anatomically defined regions of interest \n(ROIs) spanning cortical gray matter and white matter tracts. The JHU atlas includes 189 \nlabeled structures, enabling systematic quantification of lesion burden across the brain. \nThe present analysis focused on 24 left-hemisphere ROIs selected based on prior work \nimplicating these regions in language processing (Fridriksson et al., 2018). These included \ncortical regions associated with the dorsal stream (inferior frontal gyrus pars opercularis and \ntriangularis, precentral gyrus, postcentral gyrus, supramarginal gyrus, superior temporal \ngyrus) and ventral stream (middle temporal gyrus, inferior temporal gyrus, fusiform gyrus, \nangular gyrus), as well as the insula and adjacent structures. We expanded the anatomical \nspace beyond prior work to include three white-matter tracts strongly implicated in language \nfunction: the superior longitudinal fasciculus, uncinate fasciculus, and inferior \nfronto-occipital fasciculus. \nS3. Lesion‚ÄìSymptom Mapping Statistical Approach \nLesion‚Äìsymptom mapping analyses were conducted to identify which brain regions are \nassociated with each error type. We employed two complementary statistical approaches: \nmass-univariate regression and multivariate stepwise regression. \nPage 30 \n"}, {"page": 31, "text": "BLUM: Brain-LLM Unified Model \nMass-univariate regression.  \nFor each behavioral measure (error counts per category, converted to z-scores), we fit \nseparate ordinary least squares regression models predicting behavioral performance as a \nfunction of lesion load in each ROI. This approach tests the association between damage to \neach region and each behavioral outcome while remaining agnostic to relationships among \nregions. Statistical significance was assessed using permutation testing, a nonparametric \napproach that controls for the spatial autocorrelation and non-normal distributions common in \nlesion data. For each behavioral measure, 400 permutations were conducted, randomly \nshuffling the mapping between lesion data and behavioral scores to generate an empirical null \ndistribution. We employed one-tailed tests in the negative direction, consistent with the \nhypothesis that greater lesion burden predicts poorer performance. Resulting p-values were \ncorrected for multiple comparisons across ROIs using the Benjamini‚ÄìHochberg false \ndiscovery rate (FDR) procedure. \nMultivariate stepwise regression.  \nTo identify the subset of regions jointly predicting each behavioral measure while accounting \nfor shared variance among correlated predictors, we also employed forward‚Äìbackward \nstepwise regression. This procedure iteratively adds and removes predictors based on \npredefined probability thresholds (p_enter = 0.05; p_remove = 0.10). A region enters the \nmodel only when it significantly reduces residual variance; a region is removed if its \ncontribution becomes nonsignificant after other predictors are incorporated. This multivariate \napproach complements mass-univariate lesion‚Äìsymptom mapping by addressing collinearity \namong neighboring regions that share vascular supply and are often damaged together. \nS4. Simulated Large Language Model ‚ÄúLesions‚Äù Through Systematic \nPerturbation \nIn an effort to better understand similarities between human brains and large language models \n(LLMs), we attempt to disrupt a fully trained LLM in a way that mirrors lesions seen in an \naphasic brain. We apply noise to an LLM in a systematic way and measure the impact of \nthese artificial lesions by characterizing incorrect responses according to error categories \ncreated by speech-language pathologists. \nWe use the 13-billion-parameter Vicuna model, which is a transformer-based model (Chiang \net al. 2023). Like the generative pre-trained transformer (GPT) models, it is autoregressive, \nPage 31 \n"}, {"page": 32, "text": "BLUM: Brain-LLM Unified Model \nmeaning the model makes a prediction of the next token given the previous token (Radford et \nal. 2018). We use this model to simulate a speech-language pathologist administering the \nWAB-R test to an artificial patient. We use a prompt that is based on the original WAB-R \nprompt but includes extra suggestions such as not responding with more than one word. We \nthen give the sentence or question to the model and record the first word that the model \nresponds with, allowing for typical model behaviors such as repeating the prompt. \nSupplementary Methods Figure 2 below shows the method used to collect responses from the \nmodel. \n \nSupplementary Methods Figure 2. Vicuna model prompting procedure. Example illustrating \nhow the model is prompted with a question from the WAB-R dataset and how the first word is \nselected after parsing out model artifacts. \nThe Vicuna model is a fine-tuned LLaMA model (Touvron et al. 2023). The LLaMA \narchitecture used in the Vicuna model is shown in Supplementary Methods Figure 3 below. It \nhas 40 multihead attention layers, each with 40 attention heads and a hidden size of 5,120, \nmeaning each head dimension is size 128. We apply the artificial lesion to the weights in \nthese 40 multihead attention layers. Noise may be applied to any of the weights in the \nLLaMA decoder layers. \nPage 32 \n"}, {"page": 33, "text": "BLUM: Brain-LLM Unified Model \n \nSupplementary Methods Figure 3. LLaMA model architecture. The model consists of 40 LLaMA \ndecoder layers, each of which includes an LLaMA attention module, which is a multihead attention \nlayer with 40 attention heads. \n \nTo add the artificial lesion, we use three parameters to control the amount and location of \nnoise added. We control the level of noise added, the percentage of nodes in a layer that are \nmodified, and the layer to which this noise is applied. We apply the percentage of nodes \nmodified from 10‚Äì90% in 10% increments. For each modification percentage, we apply a \nnoise standard deviation in the range of 0‚Äì1.9 with a step size of 0.1. Finally, these changes \nare applied to each of the 40 target layers separately. This process creates 7,200 different \nmodels with artificial lesions of different sizes in different locations. \nResponses from the model on the WAB-R dataset were classified into to seven categories, \nincluding ‚ÄúCorrect‚Äù and the error types: ‚ÄúUnrelated,‚Äù ‚ÄúSemantic,‚Äù ‚ÄúFormal,‚Äù ‚ÄúMixed,‚Äù \n‚ÄúNeologism,‚Äù and ‚ÄúNo Response.‚Äù These classifications match the response classifications of \nactual patients on the WAB-R dataset, allowing direct comparison. \nOnce classified, the artificial patients (LLM output) and actual patients can be matched based \non their responses over the 10 items in the WAB-R dataset. For example, the model instance \nwith no artificial lesioning and the healthy patient are a match, with both ‚Äúpatients‚Äù scoring \n‚Äúcorrect‚Äù on every item. Out of 410 aphasic patients, 78 M patients and 10 SPARC patients \nhad a response profile that exactly matched that of an artificial patient. Note that the quantity \nof responses of a certain type is considered, but the response type is not matched per \nPage 33 \n"}, {"page": 34, "text": "BLUM: Brain-LLM Unified Model \nindividual item. Supplementary Methods Figure 4 below shows selected matching artificial \nand actual patients. \n \nSupplementary Methods Figure 4. Selected response profiles of matching artificial and \nhuman patients. Across the WAB-R dataset, artificial and human patients produce responses \nclassified into the same response categories.  \n \n \nPage 34 \n"}, {"page": 35, "text": "BLUM: Brain-LLM Unified Model \nSupplementary Tables \nSupplementary Table S1. Cross-task correspondence in lesion‚Äìsymptom patterns. \nFor each error category, 24 left-hemisphere ROIs were rank-ordered by the strength of their \nlesion‚Äìbehavior association, and these rankings were correlated across tasks (PNT vs. \nWAB-R). \n \nCorrect, Formal, and Semantic errors showed strong correspondence (œÅ > 0.65, p < 0.001); No \nResponse (NR) showed moderate correspondence (œÅ = 0.46, p < 0.05); Mixed, Neologism, and \nUnrelated errors showed weak correspondence (œÅ < 0.37, p > 0.05). \n \nPage 35 \n"}, {"page": 36, "text": "BLUM: Brain-LLM Unified Model \nSupplementary Results \nS1. Extended Symptom-to-Lesion Model Validation \nWe compared linear regression (LR) and support vector regression (SVR) models for \nsymptom-to-lesion prediction. Both approaches yielded comparable prediction accuracy, with \nlinear regression performing slightly better for most regions. This suggests that the \nrelationship between behavioral profiles and lesion location is predominantly linear and does \nnot require more complex nonlinear modeling, consistent with the relatively direct mapping \nbetween localized damage and specific behavioral impairments established by decades of \nlesion‚Äìsymptom mapping research. \nThe spatial patterns recovered by symptom-to-lesion models converged with established \nrelationships between aphasia phenotypes and neuroanatomy. WAB-R profiles dominated by \nphonological production deficits, repetition impairments, or reduced fluency elicited \npredicted lesion patterns concentrated in dorsal-stream regions, including premotor cortex, \npars opercularis, and supramarginal gyrus. Profiles characterized by impaired comprehension, \nreduced information content, and difficulty with semantic retrieval produced predictions \nconcentrated in ventral-stream regions, including the posterior and middle temporal gyri. \nS2. Full Lesion-Symptom Mapping Results \nSupplementary Figure S3 presents the ROI-level lesion‚Äìsymptom associations across all error \ntypes for both the PNT and WAB-R, while Supplementary Figure S4 displays the \ncorresponding brain renderings. \nConsistent with the dual-stream model of language processing (Hickok & Poeppel, 2007; \nFridriksson et al., 2016), lesion‚Äìsymptom associations revealed a clear anatomical \ndissociation between error types. Error categories associated with disrupted phonological \nencoding (Formal errors and neologisms) showed robust associations with dorsal-stream \nstructures, including the inferior frontal gyrus (pars opercularis and pars triangularis), \nprecentral and postcentral gyri, supramarginal gyrus, and the superior longitudinal fasciculus. \nThe involvement of the insula and pars opercularis for Formal errors is consistent with lesion \npatterns that characterize conduction aphasia, in which phonological sequencing and \narticulatory buffering may be compromised. \nError types reflecting degraded lexical‚Äìsemantic access (Semantic and Mixed errors) engaged \na well-defined ventral-stream network, including the middle and inferior temporal gyri, \nPage 36 \n"}, {"page": 37, "text": "BLUM: Brain-LLM Unified Model \nfusiform cortex, angular gyrus, and the uncinate and inferior fronto-occipital fasciculi. \nNotably, PNT Semantic and WAB-R Semantic errors demonstrated nearly identical \nventral-stream involvement, indicating that shared cortical substrates across tasks support \nsemantic retrieval processes despite differences in task demands. \nUnrelated responses showed more heterogeneous anatomical signatures spanning both dorsal \nand ventral territories, likely reflecting simultaneous degradation of semantic constraints and \nphonological well-formedness. Correct responses showed the inverse pattern: better \nperformance was associated with preserved tissue in regions linked to error production. \nThe multivariate analyses revealed that considerably fewer regions emerged as independent \npredictors compared to the univariate analyses. The regions most frequently emerging as \nindependent predictors across error types were posterior superior temporal gyrus, precentral \ngyrus, and posterior insula. These regions could be considered especially important hubs in \nthe cortical speech and language network. \n \n \nPage 37 \n"}, {"page": 38, "text": "BLUM: Brain-LLM Unified Model \nSupplementary Figures \n \nSupplementary Figure S1. Lesion overlay map. Voxelwise lesion overlap map for 301 participants \nwith neuroimaging-based lesion data, overlaid on the SPM152 standard template. Lesion maps were \nbinary at the individual level (lesion = 1, non-lesion = 0), and color-bar values in the group map \nrepresent the count of participants exhibiting a lesion at that location. Results are displayed as mosaic \ncoronal slices. Only voxels in which at least 10% of individuals had a lesion are shown.  \n \n \n \n \n \n \n \n \n \nPage 38 \n"}, {"page": 39, "text": "BLUM: Brain-LLM Unified Model \n     \nSupplementary Figure S2. Error type distributions. Radar plots show the percentage of responses \nclassified into each error category for automated scoring (left) and manual scoring by clinicians \n(right), with PNT scores shown in blue and WAB-R scores shown in yellow. No Response errors \ndominate both tasks. The PNT shows higher proportions of Phonemic and Neologism errors, whereas \nthe WAB-R shows increased Unrelated responses. \n \nSupplementary Figure S3. ROI-level lesion‚Äìsymptom associations. Categorical ROI-level \nlesion‚Äìsymptom associations across PNT and WAB-R error types. Each cell reflects the statistical \nassociation between lesion load in a given JHU ROI and performance on a specific error type. \nAssociations were obtained using mass-univariate linear regression with FDR correction \n(Benjamini‚ÄìHochberg). Color intensity encodes the strength of association. Behaviors are ordered to \nplace matched PNT/WAB-R error types adjacent to each other. Regions are colored by dorsal stream \n(blue), ventral stream (green), and white matter tracts (black).‚Äã\nPage 39 \n"}, {"page": 40, "text": "BLUM: Brain-LLM Unified Model \n‚Äã\n \n \nSupplementary Figure S4. Brain renderings of lesion‚Äìsymptom maps. Lesion correlates of \nsentence completion and picture naming response types. Each brain rendering shows the continuous \nlesion‚Äìsymptom association map for a specific error type. Statistical maps were transformed using \n-log‚ÇÅ‚ÇÄ(p) and rendered onto a standard cortical mesh. Colors range from red to yellow, with yellow \nindicating stronger lesion‚Äìsymptom associations. The key result is that the ‚Äòpatterns of associations‚Äô, \nrather than the strength of the associations, are similar between specific error types measured using \neither the PNT or the WAB-R; colorbars are therefore omitted. Note the consistent spatial \ncorrespondence between WAB-R and PNT for correct naming, as well as for semantic and formal \nerrors. \n \n \n \nSupplementary Figure S5. LLM error profiles vs. human error distributions. Radar plots show \nthe proportional distribution of naming error types produced by human participants and the large \nlanguage model (LLM) across two tasks: the Philadelphia Naming Test (PNT; left) and the Western \nAphasia Battery‚ÄìRevised (WAB-R; right). Error categories include phonemic, semantic, unrelated, \nmixed, non-response (NR), and neologism errors. Human and LLM profiles are overlaid to facilitate \ndirect comparison, demonstrating broad alignment in relative error patterns across tasks and \nsupporting the comparability of LLM-generated errors to human aphasic naming behavior. \n \nPage 40 \n"}, {"page": 41, "text": "BLUM: Brain-LLM Unified Model \nSupplementary Figure S6. Full LLM perturbation parameter space. Distribution of error types as \na function of perturbation noise standard deviation, averaged across all transformer layers. \n \n \nPage 41 \n"}, {"page": 42, "text": "BLUM: Brain-LLM Unified Model \nReferences \nAntonello, Richard, Chandan Singh, Shailee Jain, et al. 2024. ‚ÄúGenerative Causal Testing to Bridge \nData-Driven Models and Scientific Theories in Language Neuroscience.‚Äù arXiv. \nhttps://arxiv.org/abs/2410.00812. \nBates, Elizabeth, Stephen M. Wilson, Ayse Pinar Saygin, et al. 2003. ‚ÄúVoxel-Based Lesion-Symptom \nMapping.‚Äù Nature Neuroscience 6 (5): 448‚Äì450. \nBelinkov, Yonatan. 2022. ‚ÄúProbing Classifiers: Promises, Shortcomings, and Advances.‚Äù \nComputational Linguistics 48 (1): 207‚Äì219. \nBelinkov, Yonatan, and James Glass. 2019. ‚ÄúAnalysis Methods in Neural Language Processing: A \nSurvey.‚Äù Transactions of the Association for Computational Linguistics 7 (April): 49‚Äì72. \nBricken, T., A. Templeton, J. Batson, et al. 2023. Towards Monosemanticity: Decomposing Language \nModels with Dictionary Learning. \nBroca, P. 1861. ‚ÄúRemarques Sur Le Si√®ge de La Facult√© Du Langage Articul√©, Suivies D‚Äôune \nObservation D‚Äôaph√©mie.‚Äù Bulletin de La Soci√©t√© Anatomique de Paris 6: 330‚Äì357. \nBuchsbaum, Bradley R., Juliana Baldo, Kayoko Okada, et al. 2011. ‚ÄúConduction Aphasia, \nSensory-Motor Integration, and Phonological Short-Term Memory - an Aggregate Analysis of \nLesion and fMRI Data.‚Äù Brain and Language 119 (3): 119‚Äì128. \nBuzs√°ki, Gy√∂rgy. 2019. The Brain from inside out. Oxford University Press. \nhttps://doi.org/10.1093/oso/9780190905385.001.0001. \nChen, Z., Y. Yang, B. Qi, et al. 2024. ‚ÄúCompressing Large Language Models by Streamlining the \nUnimportant Layer.‚Äù arXiv Preprint arXi:2403.19135. \nChiang, Wei-Lin, Zhuohan Li, Zi Lin, et al. 2023. ‚ÄúVicuna: An Open-Source Chatbot Impressing \nGPT-4 with 90%* ChatGPT Quality.‚Äù Preprint, March. \nhttps://lmsys.org/blog/2023-03-30-vicuna/. \nConneau, Alexis, German Kruszewski, Guillaume Lample, Lo√Øc Barrault, and Marco Baroni. 2018. \n‚ÄúWhat You Can Cram into a Single $&!#* Vector: Probing Sentence Embeddings for Linguistic \nProperties.‚Äù Paper presented Proceedings of the 56th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers), Melbourne, Australia. Proceedings of the \n56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) \n(Stroudsburg, PA, USA). https://doi.org/10.18653/v1/p18-1198. \nCunningham, Hoagy, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. 2023. ‚ÄúSparse \nAutoencoders Find Highly Interpretable Features in Language Models.‚Äù arXiv, ahead of print, \nOctober. https://doi.org/10.48550/arXiv.2309.08600. \nDell, Gary S., Myrna F. Schwartz, Nadine Martin, Eleanor M. Saffran, and Deborah A. Gagnon. 1997. \n‚ÄúLexical Access in Aphasic and Nonaphasic Speakers.‚Äù Psychological Review 104 (4): 801‚Äì838. \nDeMarco, Andrew T., and Peter E. Turkeltaub. 2018. ‚ÄúA Multivariate Lesion Symptom Mapping \nToolbox and Examination of Lesion-Volume Biases and Correction Methods in Lesion-Symptom \nMapping.‚Äù Human Brain Mapping 39 (11): 4169‚Äì4182. \nDronkers, Nina F., David P. Wilkins, Robert D. Van Valin Jr, Brenda B. Redfern, and Jeri J. Jaeger. \n2004. ‚ÄúLesion Analysis of the Brain Areas Involved in Language Comprehension.‚Äù Cognition 92 \n(1-2): 145‚Äì177. \nPage 42 \n"}, {"page": 43, "text": "BLUM: Brain-LLM Unified Model \nFaria, Andreia V., Suresh E. Joel, Yajing Zhang, et al. 2012. ‚ÄúAtlas-Based Analysis of Resting-State \nFunctional Connectivity: Evaluation for Reproducibility and Multi-Modal Anatomy-Function \nCorrelation Studies.‚Äù NeuroImage 61 (3): 613‚Äì621. \nFaries, F. W., and K. Raja. 2022. ‚ÄúHow Neural Networks Learn: A Computational Analysis.‚Äù \nCognitive Science 46 (3). \nFridriksson, Julius, Dirk-Bart den Ouden, Argye E. Hillis, et al. 2018. ‚ÄúAnatomy of Aphasia \nRevisited.‚Äù Brain : A Journal of Neurology 141 (3): 848‚Äì862. \nFridriksson, Julius, Grigori Yourganov, Leonardo Bonilha, Alexandra Basilakos, Dirk-Bart Den \nOuden, and Christopher Rorden. 2016. ‚ÄúRevealing the Dual Streams of Speech Processing.‚Äù \nProceedings of the National Academy of Sciences of the United States of America 113 (52): \n15108‚Äì15113. \nGeschwind, N. 1970. ‚ÄúThe Organization of Language and the Brain.‚Äù Science (New York, N.Y.) 170 \n(3961): 940‚Äì944. \nGoodglass, Harold, and Edith Kaplan. 1983. Assessment of Aphasia and Related Disorders. 2nd ed. \nLea & Febiger. \nHewitt, John, and Christopher D. Manning. 2019. ‚ÄúA Structural Probe for Finding Syntax in Word \nRepresentations.‚Äù Paper presented Proceedings of the 2019 Conference of the North, \nMinneapolis, Minnesota. Proceedings of the 2019 Conference of the North (Stroudsburg, PA, \nUSA). https://doi.org/10.18653/v1/n19-1419. \nHickok, Gregory. 2012. ‚ÄúComputational Neuroanatomy of Speech Production.‚Äù Nature Reviews. \nNeuroscience 13 (2): 135‚Äì145. \nHickok, Gregory, and David Poeppel. 2007. ‚ÄúThe Cortical Organization of Speech Processing.‚Äù \nNature Reviews. Neuroscience 8 (5): 393‚Äì402. \nIndefrey, P., and W. J. M. Levelt. 2004. ‚ÄúThe Spatial and Temporal Signatures of Word Production \nComponents.‚Äù Cognition 92 (1-2): 101‚Äì144. \nIvanova, Maria V., Timothy J. Herron, Nina F. Dronkers, and Juliana V. Baldo. 2021. ‚ÄúAn Empirical \nComparison of Univariate versus Multivariate Methods for the Analysis of Brain-Behavior \nMapping.‚Äù Human Brain Mapping 42 (4): 1070‚Äì1101. \nJanelle, Felix, Christian Iorio-Morin, Sabrina D‚Äôamour, and David Fortin. 2022. ‚ÄúSuperior \nLongitudinal Fasciculus: A Review of the Anatomical Descriptions with Functional Correlates.‚Äù \nFrontiers in Neurology 13 (April): 794618. \nKriegeskorte, Nikolaus, Marieke Mur, and Peter Bandettini. 2008. ‚ÄúRepresentational Similarity \nAnalysis - Connecting the Branches of Systems Neuroscience.‚Äù Frontiers in Systems \nNeuroscience 2 (November): 4. \nKudo, Taku, and John Richardson. 2018. ‚ÄúSentencePiece: A Simple and Language Independent \nSubword Tokenizer and Detokenizer for Neural Text Processing.‚Äù Paper presented Proceedings \nof the 2018 Conference on Empirical Methods in Natural Language Processing: System \nDemonstrations, Brussels, Belgium. Proceedings of the 2018 Conference on Empirical Methods \nin Natural Language Processing: System Demonstrations (Stroudsburg, PA, USA). \nhttps://doi.org/10.18653/v1/d18-2012. \nLaine, Matti, and Nadine Martin. 2023. Anomia. 2nd ed. Routledge. \nLevelt, Willem J. M., Ardi Roelofs, and Antje S. Meyer. 1999. ‚ÄúA Theory of Lexical Access in \nPage 43 \n"}, {"page": 44, "text": "BLUM: Brain-LLM Unified Model \nSpeech Production.‚Äù Behavioral and Brain Sciences 22 (1): 1‚Äì38. \nLiu, Haotian, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. ‚ÄúVisual Instruction Tuning.‚Äù In \nAdvances in Neural Information Processing Systems, edited by A. Oh, T. Naumann, A. \nGloberson, K. Saenko, M. Hardt, and S. Levine, vol. 36, 36. Curran Associates, Inc. \nMirman, Daniel, Qi Chen, Yongsheng Zhang, et al. 2015. ‚ÄúNeural Organization of Spoken Language \nRevealed by Lesion-Symptom Mapping.‚Äù Nature Communications 6 (April): 6762. \nNachev, Parashkev, Elizabeth Coulthard, H. Rolf J√§ger, Christopher Kennard, and Masud Husain. \n2008. ‚ÄúEnantiomorphic Normalization of Focally Lesioned Brains.‚Äù NeuroImage 39 (3): \n1215‚Äì1226. \nNelson, Douglas L., Cathy L. McEvoy, and Thomas A. Schreiber. 2004. ‚ÄúThe University of South \nFlorida Free Association, Rhyme, and Word Fragment Norms.‚Äù Behavior Research Methods, \nInstruments, & Computers: A Journal of the Psychonomic Society, Inc 36 (3): 402‚Äì407. \nNogueira, Pedro Aleixo, Julia Franco Neiva, Ma√≠ra Piani Couto, et al. 2025. ‚ÄúFrom Classic Models to \nNew Pathways: Unraveling the Anatomy and Function of the Inferior Fronto-Occipital \nFasciculus in Language Processing.‚Äù Frontiers in Psychology 16 (April): 1561482. \nPapagno, Costanza. 2011. ‚ÄúNaming and the Role of the Uncinate Fasciculus in Language Function.‚Äù \nCurrent Neurology and Neuroscience Reports 11 (6): 553‚Äì559. \nPatterson, Kay, Mark S. Seidenberg, and James L. McClelland. 1994. ‚ÄúConnections and \nDisconnections: A Connectionist Account of Surface Dyslexia.‚Äù In Neural Modeling of Brain \nand Cognitive Disorders. World Scientific. \nPlaut, D. C. 1996. ‚ÄúRelearning after Damage in Connectionist Networks: Toward a Theory of \nRehabilitation.‚Äù Brain and Language 52 (1): 25‚Äì82. \nPustina, D., H. B. Coslett, P. E. Turkeltaub, N. Tustison, M. F. Schwartz, and B. & Avants. 2017. \n‚ÄúEnhanced Estimations of Post-Stroke Aphasia Severity Using Stacked Multimodal Predictions.‚Äù \nHuman Brain Mapping 38 (11): 5603‚Äì5615. \nPustina, Dorian, Brian Avants, Olufunsho K. Faseyitan, John D. Medaglia, and H. Branch Coslett. \n2018. ‚ÄúImproved Accuracy of Lesion to Symptom Mapping with Multivariate Sparse Canonical \nCorrelations.‚Äù Neuropsychologia 115 (July): 154‚Äì166. \nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. ‚ÄúImproving Language \nUnderstanding by Generative Pre-Training.‚Äù \nhttps://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.p\ndf. \nRauschecker, Josef P., and Sophie K. Scott. 2009. ‚ÄúMaps and Streams in the Auditory Cortex: \nNonhuman Primates Illuminate Human Speech Processing.‚Äù Nature Neuroscience 12 (6): \n718‚Äì724. \nRavichander, Abhilasha, Yonatan Belinkov, and Eduard Hovy. 2021. ‚ÄúProbing the Probing Paradigm: \nDoes Probing Accuracy Entail Task Relevance?‚Äù Paper presented Proceedings of the 16th \nConference of the European Chapter of the Association for Computational Linguistics: Main \nVolume, Online. Proceedings of the 16th Conference of the European Chapter of the Association \nfor Computational Linguistics: Main Volume (Stroudsburg, PA, USA). \nhttps://doi.org/10.18653/v1/2021.eacl-main.295. \nSaur, Dorothee, Bj√∂rn W. Kreher, Susanne Schnell, et al. 2008. ‚ÄúVentral and Dorsal Pathways for \nPage 44 \n"}, {"page": 45, "text": "BLUM: Brain-LLM Unified Model \nLanguage.‚Äù Proceedings of the National Academy of Sciences of the United States of America \n105 (46): 18035‚Äì18040. \nSchwartz, M., G. Dell, N. Martin, S. Gahl, and P. Sobel. 2006. ‚ÄúA Case-Series Test of the Interactive \nTwo-Step Model of Lexical Access: Evidence from Picture Naming‚òÜ.‚Äù Journal of Memory and \nLanguage 54 (2): 228‚Äì264. \nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare \nWords with Subword Units. \nSwinburn, Kate, Gillian Porter, and David Howard. 2004. Comprehensive Aphasia Test. Taylor & \nFrancis Group. \nTempleton, A., T. Conerly, J. Marcus, et al. 2024. Scaling Monosemanticity: Extracting Interpretable \nFeatures from Claude 3 Sonnet. Anthropic. \nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, et al. 2023. ‚ÄúLLaMA: Open and Efficient Foundation \nLanguage Models.‚Äù In arXiv [cs.CL]. February 27. arXiv. http://arxiv.org/abs/2302.13971. \nTuckute, Greta, Nancy Kanwisher, and Evelina Fedorenko. 2024. ‚ÄúLanguage in Brains, Minds, and \nMachines.‚Äù Annual Review of Neuroscience 47 (1): 277‚Äì301. \nWernicke, C. 1874. Der Aphasische Symptomencomplex: Eine Psychologische Studie Auf \nAnatomischer Basis. Cohn & Weigert. \n \nPage 45 \n"}]}