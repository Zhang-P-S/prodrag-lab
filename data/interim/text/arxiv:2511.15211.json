{"doc_id": "arxiv:2511.15211", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.15211.pdf", "meta": {"doc_id": "arxiv:2511.15211", "source": "arxiv", "arxiv_id": "2511.15211", "title": "OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition", "authors": ["Xinli Tao", "Xin Dong", "Xuezhong Zhou"], "published": "2025-11-19T08:02:55Z", "updated": "2025-11-20T04:13:45Z", "summary": "With the rapid expansion of unstructured clinical texts in electronic health records (EHRs), clinical named entity recognition (NER) has become a crucial technique for extracting medical information. However, traditional supervised models such as CRF and BioClinicalBERT suffer from high annotation costs. Although zero-shot NER based on large language models (LLMs) reduces the dependency on labeled data, challenges remain in aligning example selection with task granularity and effectively integrating prompt design with self-improvement frameworks. To address these limitations, we propose OEMA, a novel zero-shot clinical NER framework based on multi-agent collaboration. OEMA consists of three core components: (1) a self-annotator that autonomously generates candidate examples; (2) a discriminator that leverages SNOMED CT to filter token-level examples by clinical relevance; and (3) a predictor that incorporates entity-type descriptions to enhance inference accuracy. Experimental results on two benchmark datasets, MTSamples and VAERS, demonstrate that OEMA achieves state-of-the-art performance under exact-match evaluation. Moreover, under related-match criteria, OEMA performs comparably to the supervised BioClinicalBERT model while significantly outperforming the traditional CRF method. OEMA improves zero-shot clinical NER, achieving near-supervised performance under related-match criteria. Future work will focus on continual learning and open-domain adaptation to expand its applicability in clinical NLP.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.15211v2", "url_pdf": "https://arxiv.org/pdf/2511.15211.pdf", "meta_path": "data/raw/arxiv/meta/2511.15211.json", "sha256": "59aeaf7a81d7ae599cb7a69392c5b84887de3a9c5b54db21333a1d782d4ed00f", "status": "ok", "fetched_at": "2026-02-18T02:26:37.045182+00:00"}, "pages": [{"page": 1, "text": "Journal Title Here, 2022, pp. 1–11\ndoi: DOI HERE\nAdvance Access Publication Date: Day Month Year\nPaper\nPAPER\nOEMA: Ontology-Enhanced Multi-Agent\nCollaboration Framework for Zero-Shot Clinical\nNamed Entity Recognition\nXinli Tao, B.S.,1 Xin Dong, M.S.1,∗and Xuezhong Zhou, Ph.D.1,∗\n1Department of Artificial Intelligence, Beijing Key Lab of Traffic Data Analysis and Mining, School of Computer Science & Technology,\nBeijing Jiaotong University, Beijing, 100044, China\n∗Corresponding authors. xzzhou@bjtu.edu.cn; x dong@bjtu.edu.cn.\nAbstract\nObjective: With the rapid expansion of unstructured clinical texts in electronic health records (EHRs), clinical named\nentity recognition (NER) has become a crucial technique for extracting medical information. However, traditional\nsupervised models such as CRF and BioClinicalBERT suffer from high annotation costs. Although zero-shot NER based\non large language models (LLMs) reduces the dependency on labeled data, challenges remain in aligning example selection\nwith task granularity and effectively integrating prompt design with self-improvement frameworks.\nMaterials and Methods: To address these limitations, we propose OEMA, a novel zero-shot clinical NER framework\nbased on multi-agent collaboration. OEMA consists of three core components: (1) a self-annotator that autonomously\ngenerates candidate examples; (2) a discriminator that leverages SNOMED CT to filter token-level examples by clinical\nrelevance; and (3) a predictor that incorporates entity-type descriptions to enhance inference accuracy.\nResults: Experimental results on two benchmark datasets, MTSamples and VAERS, demonstrate that OEMA achieves\nstate-of-the-art performance under exact-match evaluation. Moreover, under related-match criteria, OEMA performs\ncomparably to the supervised BioClinicalBERT model while significantly outperforming the traditional CRF method.\nDiscussion: OEMA combines ontology-guided reasoning with multi-agent collaboration to address two key challenges\nin zero-shot clinical NER: granularity mismatch and prompt–self-improvement integration. Its ontology-based filtering\nreduces noise and enhances semantic alignment, showing strong performance across different LLMs.\nConclusion: OEMA improves zero-shot clinical NER, achieving near-supervised performance under related-match\ncriteria. Future work will focus on continual learning and open-domain adaptation to expand its applicability in clinical\nNLP.\nKey words: Medical ontology; Clinical natural language processing; Zero-shot learning; Named entity recognition; Multi-\nagent systems\nIntroduction\nElectronic health records (EHRs) contain vast amounts of\nunstructured clinical information, including clinical notes, and\nthis type of information is of great value to clinical experts\n[11], for which many studies have been devoted to the problem\nof clinical information extraction [12]. A key aspect in the\nextraction of clinical information is named entity recognition\n(NER), which is focused on identifying specific concepts such\nas medical problems, treatments, and examinations [14].\nEarly clinical natural language processing systems typically\nrelied on predefined lexical resources and syntactic/semantic\nrules derived from manual analysis of large amounts of text\n[22]. In the last decade, machine learning-based approaches\n(e.g., CRF [10]) have become increasingly popular [9]. In recent\nyears, migration learning-based models (e.g., BioClinicalBERT\n[1])\nhave\nbeen\napplied\nto\nthe\ntask\nof\nclinical\nnamed\nentity recognition, showing improved performance with fewer\nannotated samples [2],\nbut also requiring time-consuming\ndevelopment of annotated corpora by clinical experts [6].\nRecently, with the advancement of large language models\n(LLMs), the task of NER has also ushered in new research\ndirections and application possibilities [15, 17, 3].\nOwing\nto their vast search space and large-scale pre-training data,\nLLMs show great potential for zero-shot NER, as evidenced\nby recent empirical studies [24]. Existing work has proceeded\nalong two main avenues: first, prompt-engineering methods\nthat construct more effective zero-shot prediction prompts to\nenhance LLMs’ generalization to out-of-distribution (OOD)\nknowledge [27, 7, 23]; and second, self-improvement frameworks\nthat use the LLM’s own generated self-consistency scores [21]\n© The Author 2022. Published by Oxford University Press. All rights reserved. For permissions, please e-mail:\njournals.permissions@oup.com\n1\narXiv:2511.15211v2  [cs.CL]  20 Nov 2025\n"}, {"page": 2, "text": "2\nTao et al.\nFig. 1. Challenge Analysis Diagram. In zero-shot learning, OEMA tackles two key challenges: (1) the mismatch between example selection and task\ngranularity, and (2) the lack of effective integration between prompt design and the self-improvement framework.\nto automatically label unlabeled data,\nthereby building a\nself-annotated corpus and improving performance at inference\ntime through self-annotated few-shot in-context learning (ICL)\n[25, 19]. As shown in Fig. 1, despite these advances, current\nzero-shot NER methods still face two challenging problems:\n•\nChallenge\n1:\nThe\nmismatch\nbetween\nexample\nselection\nand\ntask\ngranularity.\nIn self-improvement\nframeworks,\nself-annotated examples must be added to\nprompts to guide the LLM. However, example selection is\nbased on shallow, sentence-level strategies (e.g., random\nsampling\nor\nk-nearest\nneighbors\n[25]),\nwhereas\nNER\nis a token-level task—relying on sentence-level semantic\nsimilarity is therefore inappropriate [20].\n•\nChallenge\n2:\nThe\nlack\nof\npractical\nintegration\nbetween\nprompt\ndesign\nand\nself-improvement\nframeworks.\nAlthough existing studies have suggested\nthat self-improvement frameworks are “prompt-agnostic”\nand,\nin theory,\ncan incorporate any advanced prompt\ndesign methods to boost performance [25], this has not\nbeen empirically validated, causing advanced prompts to\nfall short of realizing their potential within self-annotation\nframeworks [26].\nTo address the above challenges, we propose OEMA, a\nzero-shot clinical NER framework illustrated in Fig. 2, which\ncomprises\nthree\ncollaborative\nagents:\na\nself-annotator\nfor\nunlabeled data, a discriminator for selecting examples, and a\npredictor for final NER. First, the self-annotator uses LLMs\nto label an unlabeled corpus,\nthereby constructing a self-\nannotated corpus [25]. Next, the discriminator retrieves K\ncandidate examples via cosine-similarity–based search, then\napplies a dedicated ICL method to extract top-level SNOMED\nCT concepts and their source text spans from both the\ntest sentence and each example. It assigns each example a\nusefulness score with respect to the target input and selects\nthe top k examples (K\n>\nk) as the final example set.\nFinally, the predictor performs inference on the target test\nsentence using a prompt that combines entity-type description\nwith the self-annotated few-shot examples (built from that\nexample set). Experimental results on two benchmark datasets\ndemonstrate that OEMA achieves state-of-the-art performance,\nwhile ablation experiment, hyperparameter tuning, and case\nstudy jointly validate its effectiveness and interpretability.\nIn summary, the main contributions of this study can be\ndistilled into three aspects:\n1.\nTo address Challenge 1, we propose a medical-ontology-\ndriven,\ntoken-level\nexample\nselection\nstrategy.\nThis\napproach refines self-annotated example filtering from the\ntraditional sentence level to a finer-grained token level, and\nemploys a dynamic scoring mechanism to select examples\nthat are highly relevant to the target test sentence, thereby\nenhancing the guidance provided to the LLM. This strategy\neffectively narrows the gap between example selection and\ntask granularity,\nenabling more fine-grained and task-\naligned example selection, particularly suitable for clinical\nNER tasks.\n2.\nTo address Challenge 2, we design the OEMA framework\nunder a multi-agent collaborative mechanism, integrating\nadvanced\nprompt\ndesign\nwith\nthe\nself-improvement\nframework.\nThe\nformer\nintroduces\ntype-level\nsemantic\npriors through entity-type descriptions, effectively compensating\nfor\nthe\ngeneralization\nlimitations\nof\nthe\nlatter’s\nself-\nannotated\nfew-shot\nlearning.\nThis\nresults\nin\na\ndual\nprompting strategy—“type priors + structured examples”—which\neffectively alleviates the performance bottleneck between\nprompt design and the self-improvement framework. To the\nbest of our knowledge, this is the first work to explicitly\nemphasize the synergy between type priors and structured\nexamples within a multi-agent collaborative framework.\n3.\nBenchmark experiments demonstrate that OEMA outperforms\nmainstream approaches in zero-shot settings.\nAblation\nstudies validate the synergistic effect of combining entity-\ntype prompts with self-annotated few-shot prompts, while\n"}, {"page": 3, "text": "Short Article Title\n3\ncase analyses further highlight the critical role of high-\nquality self-annotated examples in driving performance\ngains. Additionally, the results provide concrete evidence of\nthe fine-grained collaboration achieved by the multi-agent\ndesign in orchestrating prompt design and self-improvement\nwithin the OEMA framework.\nRelated work\nClinical named entity recognition. Early approaches to\nclinical NER mainly relied on manually crafted linguistic rules\nand curated dictionaries, built through labor-intensive text\nanalysis by domain experts [22]. Over time, research in clinical\nNER has shifted toward data-driven, machine learning–based\ntechniques,\nwhich\nhave\ndemonstrated\ngreater\nadaptability\nand scalability [9]. Well-known clinical information extraction\nframeworks\nsuch\nas\ncTAKES\nand\nCLAMP\nnow\nemploy\nhybrid architectures that combine rule-based modules with\nmachine learning components [16]. More recently, transformer-\nbased\nLLMs\nhave\nrevolutionized\nclinical\nnatural\nlanguage\nprocessing\n(NLP).\nModels\nsuch\nas\nBidirectional\nEncoder\nRepresentations from Transformers (BERT) have become the\nfoundation for capturing contextual meaning in unstructured\nclinical text [4]. Building upon BERT, several domain-specific\nadaptations—such as BioBERT and PubMedBERT, trained\non biomedical research literature, and ClinicalBERT, trained\non\nthe\nMIMIC-III\nclinical\ndataset—have\nbeen\nintroduced\n[13,\n6,\n8].\nThrough\ntransfer\nlearning,\nthese\nspecialized\nmodels can be fine-tuned for clinical NER tasks, achieving\nstate-of-the-art\nperformance\neven\nwith\nrelatively\nlimited\nannotated data.Despite these advances, a persistent challenge\nremains—the creation of large, high-quality annotated corpora,\nwhich demands significant time and expert involvement [13, 6,\n8].\nZero-shot\nnamed\nentity\nrecognition.\nWith\nthe\nemergence of LLMs such as GPT, zero-shot NER has become\na promising alternative to supervised approaches. Owing to\ntheir large-scale pretraining and broad world knowledge, LLMs\ncan perform entity recognition tasks through prompt-based\ninstruction without explicit fine-tuning [24]. Research in this\narea mainly follows two directions: the first focuses on prompt\nengineering, which aims to design effective task descriptions\nand input templates to guide LLMs toward accurate entity\nextraction.\nFor example,\nIILLM [7] reformulates the NER\ntask as an HTML-style code generation problem, incorporating\nannotation guidelines and error analysis instructions to improve\nstructure awareness. Similarly, Chatie [23] treats information\nextraction\nas\nan\ninteractive\ndialogue\nprocess,\nleveraging\nconversational cues to enhance reasoning and flexibility. Other\nstudies such as Zhao et al. [27] explore optimizing in-context\nexamples to align model behavior with human annotation\nconventions. The second research direction emphasizes self-\nimprovement\nor\nself-annotation\nmechanisms,\nwhere\nLLMs\niteratively\ngenerate\npseudo-labeled\nexamples\nto\nbootstrap\ntheir own in-context learning. Xie et al. [25] introduced the\nSelf-Improving LLM (SILLM) framework,\nwhich uses self-\nconsistency voting [21] and example selection to improve NER\nwithout external labels, while Wang et al. [19] extended this\nidea through Reversener, employing self-generated examples for\nrobust zero-shot entity detection.\nCompared with traditional clinical NER, zero-shot named\nentity recognition offers a significant advantage by eliminating\nthe\nneed\nfor\ncostly\nmanual\nannotation,\nenabling\nrapid\nadaptation\nto\nnew\nentity\ntypes\nand\ndatasets.\nDespite\nthese advances, existing zero-shot NER approaches still face\ntwo\ncritical\nchallenges.\nFirst,\nthere\nremains\na\npersistent\nmismatch between example selection granularity and task level\n[20].\nMost self-improvement frameworks rely on sentence-\nlevel\nsimilarity\nretrieval,\nlacking\nfine-grained\nmodeling\nof\ntoken-level semantic relevance. In clinical NER, such coarse\nselection often introduces noisy examples,\nimpairing entity\nboundary precision and semantic consistency. Second, there\nis limited integration between prompt engineering and self-\nimprovement\nframeworks\n[26].\nExisting\nstudies\ntypically\ntreat these components independently—prompt engineering\nfocuses\non\ndesigning\ntask-specific\nprompts,\nwhile\nself-\nimprovement emphasizes optimization through self-generated\ndata.\nWithout\nexplicit\nsynergy,\nthese\nmethods\nstruggle\nto achieve both generalization and semantic constraint in\nunsupervised settings. To address these issues, a collaborative\nsystem integrating token-level example selection with type-\naware prompting under a tri-agent architecture consisting of\na self-annotator, discriminator, and predictor is proposed.\nMethods\nProblem definition\nWe now formally define the zero-shot NER task. Given an input\nsentence x = (w1, w2, · · · , wn), where wi represents the i-th\ntoken in the sentence, the goal of zero-shot NER is to identify\nall semantically meaningful named entities in the sentence x\nwithout using any manually annotated training examples, and\nto structure the identified entities in the following output form:\ny = {(e, t) | e ⊆x, t ∈T },\n(1)\nHere, e denotes an entity span, i.e., a contiguous segment of\ntext in sentence x; t is the type of the entity, which belongs to a\npredefined set of entity types T . The output structure y consists\nof multiple pairs (e, t), representing all identified entities and\ntheir corresponding types.\nOEMA framework\nAs shown in Fig. 2, OEMA comprises three core agents: the\nself-annotator, which constructs a self-annotated corpus from\nunlabeled data;\nthe discriminator,\nwhich scores and ranks\nexamples using the top-level SNOMED CT ontology; and the\npredictor,\nwhich combines self-annotated few-shot prompts\nwith entity-type descriptions to produce the final NER output.\nSelf-annotator for unlabelled data\nIn the zero-shot NER setting,\nwe only have access to an\nunlabeled corpus.\nTherefore,\nwe introduce a self-annotator\ninspired by the self-improvement strategy [25], which guides the\nreasoning process of LLMs. Specifically, the self-annotator uses\nzero-shot prompt to annotate the unlabeled corpus, thereby\nconstructing a self-annotated corpus. For each unlabeled sample\nxi, we use zero-shot prompt to generate predictions based on\nthe LLMs. This process is defined by (2) as follows,\nyi = arg max\ny\nPs(y | Ts, xi)\n(2)\nwhere Ts is the prompt template used for self-annotation. For\nan example of Ts, refer to Table. A1 in the Supplementary\nMaterial. Ps denotes the output probability from the self-\nannotator. The prediction result yi is structured as shown in\n"}, {"page": 4, "text": "4\nTao et al.\nFig. 2. Framework of OEMA: (1) Self-Annotator creates a corpus from unlabeled data; (2) Discriminator scores token-level examples via top-level\nSNOMED CT ontologies; (3) Predictor fuses entity-type descriptors with selected examples to yield the NER output. Agents are grouped by function\n(dashed boxes) and connected by arrows to show the execution order.\n(3), which consists of a set of entity-type pairs. Here, l denotes\nthe number of predicted entities.\nyi =\nn\n(ei\nj, ti\nj)\nol\nj=1\n(3)\nTo improve the reliability of annotations, we employ self-\nconsistency [21] and adopt a two-stage majority voting strategy\n[24]. We sample multiple responses from LLMs. In the first\nstage, if a candidate mention appears in more than half of all\nresponses, it is considered an entity; otherwise, it is discarded.\nIn the second stage, for each mention retained from the first\nstage, we determine its entity type based on the majority\nopinion among the retained mentions, and this is taken as the\nfinal predicted entity type.\nThe self-annotator provides the self-improvement framework\nwith essential,\nhigh-quality foundational data,\nsignificantly\nreducing dependence on manual annotation and its associated\ncosts.\nDiscriminator for selecting examples\nThrough the aforementioned self-annotator agent, we obtain a\nself-annotated corpus, and to further identify high-quality self-\nannotated examples, we design a discriminator for selecting\nexamples. When the target sentence xq arrives, we adopt a\ndiversified nearest-neighbor approach.\nFirst,\nwe retrieve K\nrelevant examples Sd = {(xi, yi)}K\ni=1 from the self-annotated\ncorpus based on cosine similarity, and then select the top\nk examples with the highest usefulness scores.\nTo enable\nthe model to select examples based on token-level similarity\ngrounded in medical ontologies, we design a specialized ICL for\nclinical ontology extraction. Let the self-annotated example set\nSd and the target sentence xq form the input set Iu ∈Sd∪{xq}.\nFor each sample xi ∈Iu, we use few-shot example prompt to\ngenerate results. The formula for this procedure is as follows,\noi = arg max\no\nPe\n\u0000y | Te, xi\n\u0001\n(4)\nwhere Te denotes the ICL prompt template for clinical ontology\nextraction.\nFor an example of Te,\nrefer to Table. A2 in\nthe\nSupplementary\nMaterial.\nPe(·)\nrepresents\nthe\noutput\nprobability of the clinical ontology extraction.\nWhen examples retrieved by a shallow similarity–based\nstrategy may be highly irrelevant to the target sentence and\nseverely mislead the LLMs’ predictions [25], we address this\nissue by assigning a helpfulness score to each example to\nautomatically assess its contribution to the prediction of the\ntarget test sentence. Specifically, given the example set Sd\nfor the target sentence xq and the generated clinical ontology\nset Od = {(xi, oi)}K+1\ni=1 , the corresponding helpfulness scores\n{hi}K\ni=1 are predicted by the following equation,\nhi = arg max\nh\nPd\n\u0000y | Td, Sd, Od, xi, xq\u0001\n(5)\nwhere Td denotes the ICL prompt template for clinical ontology\nextraction. For an example of Td, refer to Table. A3 in the\nSupplementary Material. Pd(·) represents the probability used\nfor scoring each example.\nFinally, we sort the helpfulness scores {hi}K\ni=1, and select\nthe top k examples with the highest scores as the final example\nset So = {(xi, yi)}k\ni=1, thus completing the entire diversified\nnearest neighbor selection process.\nThis token-level, ontology-enhanced selection mechanism\ntackles challenge 1 by retrieving fine-grained,\nsemantically\nrelevant examples,\noffering the LLM precise guidance and\navoiding misalignment from coarse, sentence-level matching.\nPredictor for final NER\nBuilding\non\nthe\nprevious\ntwo\nagents\n(self-annotator\nand\ndiscriminator),\nthe\nkey\nis\nnow\nto\nfully\nexploit\nthe\ndual\n"}, {"page": 5, "text": "Short Article Title\n5\nprompting strategy of “type prior and structured examples”.\nTherefore,\nwe\nintroduce\na\npredictor\nfor\nfinal\nNER.\nThe\npredictor performs few-shot ICL by combining the k examples\nSo with the target sentence xq. The final prediction is given as\nfollows,\nyq = arg max\ny\nPo\n\u0000y | To, So, xq\u0001\n(6)\nwhere To denotes the ICL prompt template that incorporates\nentity-type\ndescriptions.\nFor\nan\nexample\nof\nTo,\nrefer\nto\nTable. A4 in the Supplementary Material. Po(·) represents the\npredictor’s output probability.\nThis fusion of entity-type descriptions and self-annotated\nexamples addresses Challenge 2 via a multi-agent workflow: the\nself-annotator generates candidate knowledge from unlabeled\ndata, the discriminator filters high-quality examples, and the\npredictor performs final NER. This design allows the LLM to\nintegrate type priors with structured examples, achieving state-\nof-the-art performance in clinical NER.\nEXPERIMENTS AND RESULTS\nTo evaluate the effectiveness of the proposed OEMA, our goal\nis to verify it by answering the following research questions.\nRQ1: Does OEMA’s exact-match performance on zero-shot\nclinical named entity recognition tasks outperform the state-of-\nthe-art methods? (See Section 4.4.1)\nRQ2: How does OEMA’s performance on zero-shot clinical\nnamed entity recognition tasks compare to that of traditional\nsupervised learning? (See Section 4.4.1)\nRQ3: What are the specific contributions of OEMA’s self-\nannotated few-shot prompting strategy versus its entity-type\ndescription prompting strategy? (See Section 4.4.2)\nRQ4: How are the two key hyperparameters, K and k,\ntuned in OEMA’s diversified nearest-neighbor method? (See\nSection 4.4.3)\nRQ5:\nWhat is the detailed process by which OEMA\nimproves the evaluation metrics? (See Section 4.4.4)\nDataset\nThis study utilizes two clinical NER datasets: (i) MTSamples[18],\nwhich contains 163 synthetic discharge summaries annotated\naccording to the 2010 i2b2 guidelines,\nused for extracting\nmedical problems, treatments, and tests; (ii) VAERS[5], which\nincludes 91 publicly available VAERS safety reports, used for\nidentifying neurological disorder events.\nThe datasets were split into training, validation, and test\nsets. The self-annotated samples for the OEMA framework\nwere drawn from the first 500 instances of the training set;\nthe validation set was aligned with those used for the CRF\nand BioClinicalBERT models; and the test set was used for\nperformance evaluation and comparison. Table. 1 presents the\nentity statistics for each dataset.\nBaseline\nTo compare model performance,\nwe adopted the following\nbaseline methods: (i) Vanilla [24] employs a straightforward and\ncommonly used prompting strategy that directly asks the large\nlanguage model to extract entity labels from the input text. (ii)\nIILLM [7] transforms the zero-shot NER task into an HTML\ncode generation task using a carefully designed prompting\nstrategy that includes task descriptions, annotation guidelines,\nTable 1. Datasets and Entities Distribution.\nDatasets\nEntities\nTrain\nValid\nTest\nTotal\nMTSamples[18]\nMedical problem\n538\n203\n199\n940\nTreatment\n149\n43\n35\n227\nTest\n120\n39\n50\n209\nVAERS[5]\nInvestigation\n148\n29\n59\n236\nNervous adverse event\n406\n83\n162\n651\nOther adverse event\n301\n62\n167\n530\nProcedure\n338\n57\n126\n521\nAbbreviation:\nVAERS—vaccine adverse event reporting\nsystem.\nand error analysis instructions. (iii) SILLM [25] applies a self-\nimprovement framework that leverages a self-annotated corpus\nto stimulate the LLM’s self-learning capabilities in zero-shot\nNER.\nEvaluation criteria and experimental setup\nModel performance was evaluated using the 2010 i2b2 challenge\nevaluation script [18], computing precision (P), recall (R), and\nF1 score (F1) under both exact-match (exact boundary and\nentity type agreement) and relaxed-match (same entity type\nwith text overlap). Following Xie et al. [25], the self-consistency\nscore was computed with temperature = 0.7 over 5 sampled\noutputs, and diversified K-nearest neighbors (K = 12) were\nused to generate k = 3 self-annotated examples.\nFor a fair comparison, all baselines were re-implemented\nusing gpt-3.5-turbo-0125 (to address version discrepancies in\nthe original papers), and both OEMA and all baselines were\nalso implemented on gemini-2.5-flash-preview-04-17 to evaluate\ninference capabilities. Text embeddings were generated using\ntext-embedding-ada-002. In the experiments, the self-annotator\nand predictor varied based on the experimental setup (gpt-3.5\nor gemini), while the discriminator was consistently fixed to\ngpt-3.5-turbo-0125 to balance performance and cost.\nResults\nMain Results\nWe first focus on RQ1,\nwhich compares the performance\nof the OEMA model against existing baselines on clinical\ntext information extraction. Table. 2 shows that under the\nexact-match F1 metric, OEMA significantly outperforms all\nbaseline models on both datasets, demonstrating its superior\nclinical entity recognition capability. When using gpt-3.5 as\nthe backbone LLM, OEMA achieves an exact-match F1 score\nthat is 6.2% higher than the best baseline on the MTSamples\ndataset and 10.1% higher on the VAERS dataset. With gemini-\n2.5-flash as the backbone LLM, OEMA likewise maintains its\nlead, improving F1 scores by 1.4% on MTSamples and 3.3%\non VAERS compared to the top baseline. These figures clearly\ndemonstrate the effectiveness of the OEMA framework.\nFurther investigation revealed that the choice of LLM\nbackbone has a significant impact on model performance.\nExperimental results show that baseline models using gemini-\n2.5-flash generally outperform those using gpt-3.5, reflecting\ndifferences\nin\nLLM\ncapabilities\non\nmedical-domain\ntasks.\nNotably, when gemini-2.5-flash is combined with the OEMA\nframework, its performance potential is further unlocked: the\nexact-match F1 score surpasses that of the traditional CRF\nmethod by 4%–6%. Perhaps most interestingly, although the\ngemini backbone yields stronger baseline performance,\nthe\nperformance gains introduced by OEMA are larger when\n"}, {"page": 6, "text": "6\nTao et al.\nTable 2. MT Samples and VAERS Exact-Match Results. Numbers in bold are the highest results for the corresponding dataset, while\nnumbers underlined represent the second-best results. Significant improvements against the best-performing baseline for each dataset are\nmarked with ∗(t-test, p < 0.05).\nMT Samples\nVAERS\nBackbones\nModels\nP\nR\nF1\nP\nR\nF1\ngpt-3.5-turbo\nVanilla\n41.5\n33.6\n37.1\n36.4\n29.6\n32.7\nSILLM\n48.0\n43.1\n45.4\n39.1\n36.7\n37.9\nIILLM\n46.7\n40.1\n43.2\n44.7\n23.0\n30.3\nOEMA (ours)\n49.4 ∗\n54.1 ∗\n51.6 ∗\n46.4 ∗\n49.6 ∗\n48.0 ∗\ngemini-2.5-flash\nVanilla\n54.3\n62.2\n58.0\n48.0\n49.2\n48.6\nSILLM\n57.7\n63.3\n60.4\n48.1\n51.6\n49.8\nIILLM\n60.1\n66.9\n63.3\n49.5\n58.4\n53.6\nOEMA (ours)\n61.5 ∗\n68.2 ∗\n64.7 ∗\n53.3 ∗\n61.0 ∗\n56.9 ∗\nsupervised learning\nCRF\n51.1\n68.1\n58.4\n47.3\n59.1\n52.5\nBioClinicalBERT\n78.5\n78.5\n78.5\n69.8\n64.0\n66.8\ngpt-3.5 is used,\nsuggesting that OEMA may offer greater\noptimization for relatively weaker LLM backbones.\nTable 3. MT Samples and VAERS Relaxed-Match Results.\nMT Samples\nVAERS\nBackbones\nModels\nP\nR\nF1\nP\nR\nF1\ngpt-3.5-turbo\nOEMA (ours)\n78.4\n85.9\n82.0\n65.3\n69.8\n67.5\ngemini-2.5-flash\n83.1\n92.2\n87.4\n72.3\n82.8\n77.2\nsupervised learning\nCRF\n66.2\n88.7\n75.8\n60.9\n76.4\n67.8\nBioClinicalBERT\n91.5\n88.7\n90.1\n84.6\n76.1\n80.2\nWe\nnow\nturn\nto\nRQ2,\nexamining\nthe\nperformance\nof\nOEMA on zero-shot clinical NER tasks compared to traditional\nsupervised learning.\nBioClinicalBERT, a model specifically\nfine-tuned on biomedical-domain text with clearly annotated\nentity boundaries, contrasts with LLMs, which are pretrained\non\nmuch\nbroader,\nmore\ndiverse\ngeneral\ncorpora.\nThis\nfundamental difference in training strategy can lead to varying\nabilities in boundary detection when handling clinical NER\ntasks—particularly in highly specialized clinical texts with\ncomplex medical terminology[7]. As shown in Table. 2, under\nthe exact-match evaluation, OEMA with gemini-2.5-flash as\nits LLM backbone trails BioClinicalBERT by 10%–14% in F1\nscore, prompting us to further investigate performance under\nthe relaxed-match scenario.\nThe relaxed-match experimental results in Table. 3 reveal\neven\nmore\nvaluable\ninsights.\nUnder\nthe\nrelaxed-match\nevaluation criterion, the OEMA framework built on gpt-3.5\nalready demonstrates performance on par with the traditional\nCRF method. It is noteworthy that, although BioClinicalBERT\nstill achieves the best results across all datasets, this advantage\nrests on supervised training with a large volume of manually\nannotated data. Specifically, BioClinicalBERT’s F1 score is\nonly about 3% higher than that of the gemini-2.5-flash–based\nOEMA framework,\nwhile the latter achieves a substantial\n8%–10% improvement over the CRF model.\nThese figures\nindicate that, in a fully zero-shot setting and without any\ndomain-specific annotations, the OEMA framework can match\nthe performance of the supervised BioClinicalBERT model\nand significantly outperform the traditional machine learning\nmethods like CRF.\nAblation Experiment\nTo\nbetter\nunderstand\nthe\nindividual\ncontributions\nof\nthe\nentity-type description prompt and the self-annotated few-shot\nprompt in the OEMA framework, we address RQ3. Using GPT-\n3.5 as the LLM backbone, we conduct controlled experiments\non two medical datasets (MTSamples and VAERS) and report\ndetailed results in Tables 4. From the results, we can observe\nthat both the entity-type descriptions and the self-annotated\nfew-shot prompts play important roles in OEMA.\n•\nNo Entity type description. Under this setting, OEMA’s\nperformance on both datasets dropped noticeably, yet it\nstill demonstrated strong entity recognition capabilities.\nSpecifically, on the MTSamples dataset, the strict-match F1\nscore remained 5.81% higher than the SILLM baseline; on\nthe VAERS dataset, the exact-match F1 score held a 3.64%\nadvantage. However, under the relaxed-match evaluation,\nperformance on VAERS declined by 0.45% compared to\nSILLM. This phenomenon reveals an interesting finding:\nalthough the self-annotated few-shot prompting strategy\neffectively\npreserves\nprecise\nentity\nboundary\ndetection\n(exact-match), in the absence of semantic guidance from\nentity-type descriptions it impairs the model’s ability to\ngeneralize and recognize entity variants (relaxed-match).\n•\nNo\nExamples.\nThis\nconfiguration\nled\nto\na\nmore\npronounced\nperformance\ndecline:\non\nthe\nMTSamples\ndataset, the exact-match F1 score fell by 5.2% and the\nrelaxed-match score by 5.9%; on the VAERS dataset, exact-\nmatch dropped by 4.1%, and relaxed-match plummeted\nby\n8.6%.\nThese\nresults\ncompellingly\ndemonstrate\nthe\nfoundational role of the self-annotated few-shot prompting\nstrategy\nin\nsupporting\noverall\nmodel\nperformance.\nIn\nparticular, without concrete example guidance, the model\nstruggles\nto\naccurately\ncapture\nthe\nspecific\nexpression\npatterns and contextual features of medical entities, leading\nto a broad limitation in recognition ability. This finding also\nconfirms the constraints of non-reasoning language models\nwhen deprived of exemplar references.\nHyperparameter Tuning\nWe now turn to RQ4, focusing on the tuning process and\nunderlying mechanisms of two key hyperparameters in the\nOEMA framework’s diversified nearest-neighbor method: K\n(the candidate-set size) and k (the number of final examples).\n"}, {"page": 7, "text": "Short Article Title\n7\nTable 4. MTSamples and VAERS Ablation Test Results, Numbers in bold are the highest results for the corresponding dataset, while\nnumbers underlined represent the second-best results.\nMTSamples\nVAERS\nModels\nExact match\nRelaxed match\nExact match\nRelaxed match\nBackbones\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\ngpt-3.5-turbo\nSILM\n48.0\n43.1\n45.4\n82.7\n74.2\n78.2\n39.1\n36.7\n37.9\n61.7\n57.8\n59.7\nOEMA (ours)\n49.4\n54.1\n51.6\n78.4\n85.9\n82.0\n46.4\n49.6\n48.0\n65.3\n69.8\n67.5\n– No Entity type description\n50.2\n52.3\n51.2\n78.6\n82.0\n80.3\n41.7\n41.4\n41.5\n59.5\n59.0\n59.3\n– No Examples\n46.9\n45.9\n46.4\n78.0\n76.3\n77.1\n44.7\n43.1\n43.9\n60.0\n57.8\n58.9\nFig. 3. F1 scores (%) of Diversified KNN under different k (left, with\nK=12) and K (right, with k=3) settings on MTSamples and VAERS.\nAs\nshown\nin\nFig.\n3,\nunder\nthe\nconfiguration\nusing\ngpt-\n3.5 as the LLM backbone, we explored the performance of\ndifferent parameter combinations under both strict and lenient\nmatching evaluation criteria. The experiments revealed that the\nOEMA framework’s performance exhibits a clear “bell-shaped”\ncharacteristic\nwith\nrespect\nto\nthese\ntwo\nhyperparameters,\nindicating a distinct optimal parameter range. When fixing the\ncandidate-set size at K = 12 and comparing k ∈{1, 2, 3, 4, 5},\nk = 3 achieves the best balance under both matching modes;\nconversely, when fixing the final example count at k = 3 and\nvarying K ∈{6, 8, 10, 12, 15}, K = 12 demonstrates the most\nstable performance advantage. This tuning process established\nthe optimal configuration for the aforementioned experiments:\na diversified nearest-neighbor candidate-set size of K = 12 and\na final self-labeled example count of k = 3.\nA deeper analysis of these two hyperparameters’ mechanisms\nshows that k directly determines the number of examples\navailable for the model at inference,\nrequiring a delicate\nbalance.\nA\ntoo-small\nk\n(e.g.,\nk\n=\n1)\nseverely\nlimits\ngeneralization, as the few examples cannot cover the diversity\nof clinical entity expressions; conversely, a too-large k (e.g.,\nk = 5) introduces excessive noise from inevitably imperfect\nself-annotations, degrading prediction quality. Equally crucial\nis the choice of K, which governs the search space for candidate\nexamples and also demands careful trade-offs. If K is too small\n(e.g., K = 6), the retrieval space becomes constrained, possibly\nmissing truly high-quality references; in the extreme case when\nK = k, the diversification mechanism collapses into a standard\nk-nearest-neighbor algorithm, losing its diversity guarantees. If\nK is too large (e.g., K = 15), although the candidate pool\nexpands, the discriminator faces increased selection pressure,\nraising computational overhead and potentially lowering the\nfinal set’s quality.\nCase Study\nWe now turn to RQ5 to delve into the specific mechanisms\nand implementation processes by which the OEMA framework\nenhances performance.\nWe selected two representative test\ntarget sentences and, based on the gpt-3.5 model architecture,\nconducted\na\nfine-grained\ncomparison\nof\nOEMA\nagainst\nthe\nbaseline\nmethod\nSILLM.\nFig.\n4\nclearly\nillustrates\nhow OEMA improves prediction quality through example-\nguided prompting: by leveraging its innovative self-annotation\nmechanism\nto\ngenerate\nhigh-quality\nexamples,\nOEMA\nsuccessfully corrects recognition errors that occurred in SILLM.\nA\ncloser\ninspection\nshows\nthat\nOEMA’s\nperformance\nimprovements stem primarily from its novel, ontology-enhanced\nstrategy for selecting token-level examples. Unlike traditional\nsentence-level filtering,\nthis fine-grained approach elevates\nmatching precision from the sentence level down to individual\ntokens,\nensuring\nthat\nthe\nselected\nexamples\nare\nhighly\nrelevant—both in entity type and contextual features—to the\ncurrent recognition task.\nSuch precise matching markedly\nenhances\nthe\nguiding\neffect\nof\nexamples\non\nthe\nLLM’s\npredictions, allowing the model to learn correct recognition\npatterns from the most pertinent reference cases. By contrast,\ntraditional\nsentence-level\nexample-selection\nmethods\nsuffer\nclear\nlimitations:\nwhole-sentence\nsimilarity\noften\nfails\nto\ncapture\nthe\nspecific\nmatching\nneeds\nof\nentity\nrecognition\ntasks, and imprecise filtering may introduce irrelevant or even\nincorrect self-annotated samples, whose noise can disrupt the\nmodel’s predictive judgments.\nDiscussion\nThis\nstudy\npresents\nOEMA,\nan\nontology-enhanced\nmulti-\nagent\nframework\ndesigned\nto\nadvance\nzero-shot\nclinical\nnamed entity recognition (NER). By integrating ontology-\nguided reasoning with collaborative agent interactions, OEMA\naddresses\ntwo\npersistent\nchallenges\nin\nzero-shot\nclinical\nNER: the mismatch between example granularity and task\nobjectives,\nand\nthe\nlimited\nintegration\nbetween\nprompt\ndesign\nand\nself-improvement\nmechanisms.\nThe\nmodular\narchitecture—consisting of a self-annotator, discriminator, and\npredictor.\nEmpirical results demonstrate that OEMA outperforms\nexisting zero-shot baselines and achieves performance comparable\nto supervised models under relaxed evaluation criteria. The\nontology-enhanced\ndiscriminator\neffectively\nfilters\npseudo-\nexamples at the token level, reducing noise in self-annotated\ndata and improving semantic alignment with clinical concepts.\nCompared with prior prompt-based approaches that rely on\nhandcrafted examples or fixed templates,\nOEMA provides\nan automated and flexible mechanism for adaptive corpus\ngeneration and prompt optimization. These features highlight\n"}, {"page": 8, "text": "8\nTao et al.\nFig. 4. Two specific case analyses. Text marked in bold green indicates entities corrected by OEMA, text marked in italic red indicates incorrect\nentities, and text marked in underlined blue indicates entities from high-quality self-annotation examples that may help with error correction.\nits potential to reduce reliance on manual annotations while\nmaintaining clinical relevance and interpretability. Importantly,\nthe framework exhibits robustness across multiple backbone\nlarge language models (LLMs), including GPT-3.5 and Gemini\n2.5, demonstrating that its design can generalize well across\ndifferent model architectures and domains.\nNevertheless, several limitations should be acknowledged.\nThe\ncurrent\nevaluation\nis\nbased\non\ntwo\nrelatively\nsmall\ndatasets, which may not capture the full diversity of clinical\nlanguage. The choice of smaller datasets was primarily due to\ncost and resource constraints associated with large-scale data\nannotation. Further validation on larger, more heterogeneous\ncorpora is necessary to confirm robustness and generalizability.\nMoreover, the dependence on SNOMED CT constrains the\nframework’s applicability to domains lacking comprehensive\nontologies.\nBeyond these aspects, several promising research directions\nemerge for OEMA. One significant avenue involves integrating\ncontinual learning mechanisms that enable agents to iteratively\nrefine\ntheir\nreasoning\nand\ndecision-making\nthrough\nuser\nfeedback,\nadaptive\nsupervision,\nor\nevolving\nclinical\ndata\nstreams. Such mechanisms would transform OEMA from a\nstatic,\nzero-shot framework into a dynamic,\nself-improving\necosystem capable of maintaining performance and relevance\nin complex, real-world clinical environments. Future research\nshould also focus on broadening OEMA’s applicability to open-\ndomain clinical NER tasks, where diverse terminologies and\ncontext-dependent meanings challenge current ontology-based\nmethods.\nIncorporating\nhybrid\nsymbolic–neural\nreasoning\ncould help balance interpretability with flexibility, mitigating\noverreliance on predefined ontologies while allowing the system\nto generalize more effectively.\nMoreover,\nextending OEMA\nbeyond NER to downstream tasks—such as relation extraction,\nentity linking,\nand clinical event detection—could further\nestablish it as a unified foundation for ontology-enhanced\nclinical\nNLP,\ncapable\nof\nsupporting\nricher,\ncontext-aware\nunderstanding across the entire clinical text analysis pipeline.\nIn summary,\nOEMA demonstrates that ontology-guided\nmulti-agent collaboration can enhance zero-shot clinical NER.\nWhile preliminary results are promising, further studies on\nscalability,\nreal-world\ndeployment,\nand\nhuman-in-the-loop\nevaluation are warranted to realize its full potential in clinical\nnatural language processing.\nConclusion\nTo address the mismatch between example selection and task\ngranularity, as well as the lack of practical integration between\nprompt design and self-improvement frameworks — both in\nthe context of zero-shot clinical named entity recognition\n—\nthis\nstudy\nproposes\nan\nontology-enhanced\nmulti-agent\nframework (OEMA). Experimental results show that under\nstrict matching criteria,\nthe OEMA approach significantly\noutperforms previous zero-shot learning methods in terms\nof precision, recall, and F1 score. Moreover, under relaxed\nmatching criteria, OEMA achieves performance close to that\nof\nthe\nsupervised\nlearning\nmodel\nBioClinicalBERT,\nwhile\nsubstantially surpassing traditional machine learning methods\nsuch as CRF. In future work, we plan to further extend OEMA\nto support open-domain clinical NER tasks.\nAuthor contributions\nX.Z. conceived and designed the research. X.T. performed the\nexperiments, and analyzed the data. X.T., and X.D. drafted\nthe manuscript. X.T., X.D., and X.Z. were involved in the data\ncuration and analysis. X.Z., and X.D. revised the manuscript.\nAll authors read and approved the final manuscript.\nData availability\nOur code and datasets are available at: https://github.com/\nXinliTao/OEMA\nFunding\nThis work is partially supported by the National Natural\nScience Foundation of China (Nos.\nU23B2062,\n82374302,\n82274352),\nthe\nNational\nKey\nResearch\nand\nDevelopment\nProgram (No.\n2023YFC3502604),\nand the Natural Science\nFoundation of Beijing (No. L232033).\n"}, {"page": 9, "text": "Short Article Title\n9\nConflicts of interest\nNone declared.\nReferences\n1.\nEmily\nAlsentzer,\nJohn\nMurphy,\nWilliam\nBoag,\nWei-\nHung Weng, Di Jindi, Tristan Naumann, and Matthew\nMcDermott.\nPublicly available clinical bert embeddings.\nIn Proceedings of the 2nd Clinical Natural Language\nProcessing Workshop, pages 72–78, 2019.\n2.\nNooshin Bayat,\nNuria Garc´ıa-Santa,\nKendrick Cetina,\nand\nAnder\nMart´ınez.\nSurvey\nand\nexperiments\non\nbiomedical pre-trained language models for named entity\nrecognition.\nIn 2023 IEEE International Conference on\nBioinformatics and Biomedicine (BIBM), pages 4282–\n4288. IEEE, 2023.\n3.\nAakanksha Chowdhery,\nSharan Narang,\nJacob Devlin,\nMaarten Bosma,\nGaurav Mishra,\nAdam Roberts,\nPaul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann,\net al.\nPalm:\nScaling language modeling\nwith pathways.\nJournal of Machine Learning Research,\n24(240):1–113, 2023.\n4.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training\nof\ndeep\nbidirectional\ntransformers for language understanding.\nIn Proceedings\nof the 2019 conference of the North American chapter\nof the association for computational linguistics: human\nlanguage technologies, volume 1 (long and short papers),\npages 4171–4186, 2019.\n5.\nJingcheng\nDu,\nYang\nXiang,\nMadhuri\nSankaranarayanapillai,\nMeng\nZhang,\nJingqi\nWang,\nYuqi Si, Huy Anh Pham, Hua Xu, Yong Chen, and Cui\nTao. Extracting postmarketing adverse events from safety\nreports in the vaccine adverse event reporting system\n(vaers) using deep learning.\nJournal of the American\nMedical Informatics Association, 28(7):1393–1400, 2021.\n6.\nYu\nGu,\nRobert\nTinn,\nHao\nCheng,\nMichael\nLucas,\nNaoto\nUsuyama,\nXiaodong\nLiu,\nTristan\nNaumann,\nJianfeng\nGao,\nand\nHoifung\nPoon.\nDomain-specific\nlanguage model pretraining for biomedical natural language\nprocessing.\nACM\nTransactions\non\nComputing\nfor\nHealthcare (HEALTH), 3(1):1–23, 2021.\n7.\nYan Hu,\nQingyu Chen,\nJingcheng Du,\nXueqing Peng,\nVipina Kuttichi Keloth, Xu Zuo, Yujia Zhou, Zehan Li,\nXiaoqian Jiang,\nZhiyong Lu,\net al.\nImproving large\nlanguage models for clinical named entity recognition via\nprompt engineering.\nJournal of the American Medical\nInformatics Association, 31(9):1812–1820, 2024.\n8.\nKexin Huang,\nJaan Altosaar,\nand Rajesh Ranganath.\nClinicalbert: Modeling clinical notes and predicting hospital\nreadmission. arXiv preprint arXiv:1904.05342, 2019.\n9.\nZhiheng Huang,\nWei Xu,\nand Kai Yu.\nBidirectional\nlstm-crf models for sequence tagging.\narXiv preprint\narXiv:1508.01991, 2015.\n10. Min Jiang, Yukun Chen, Mei Liu, S Trent Rosenbloom,\nSubramani Mani, Joshua C Denny, and Hua Xu. A study\nof machine-learning-based approaches to extract clinical\nentities and their assertions from discharge summaries.\nJournal of the American Medical Informatics Association,\n18(5):601–606, 2011.\n11. Navya Martin Kollapally,\nMahshad Koohi H Dehkordi,\nYehoshua Perl,\nJames Geller,\nFadi P Deek,\nHao Liu,\nVipina K Keloth, Gai Elhanan, Andrew J Einstein, and\nShuxin Zhou. Using clinical entity recognition for curating\nan interface terminology to aid fast skimming of ehrs. In\n2024 IEEE International Conference on Bioinformatics\nand Biomedicine (BIBM), pages 6427–6434. IEEE, 2024.\n12. Mohamed Yassine Landolsi,\nLobna Hlaoua,\nand Lotfi\nBen Romdhane.\nInformation extraction from electronic\nmedical documents: state of the art and future research\ndirections.\nKnowledge\nand\nInformation\nSystems,\n65(2):463–516, 2023.\n13. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert:\na pre-trained biomedical language representation model for\nbiomedical text mining. Bioinformatics, 36(4):1234–1240,\n2020.\n14. Prakash M Nadkarni, Lucila Ohno-Machado, and Wendy W\nChapman.\nNatural language processing: an introduction.\nJournal of the American Medical Informatics Association,\n18(5):544–551, 2011.\n15. Konstantinos\nI\nRoumeliotis\nand\nNikolaos\nD\nTselikas.\nChatgpt and open-ai models: A preliminary review. Future\nInternet, 15(6):192, 2023.\n16. Guergana K Savova, James J Masanz, Philip V Ogren,\nJiaping Zheng, Sunghwan Sohn, Karin C Kipper-Schuler,\nand Christopher G Chute.\nMayo clinical text analysis\nand knowledge extraction system (ctakes):\narchitecture,\ncomponent evaluation and applications.\nJournal of the\nAmerican Medical Informatics Association,\n17(5):507–\n513, 2010.\n17. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi,\nYasmine Babaei,\nNikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n18. ¨Ozlem Uzuner, Brett R South, Shuying Shen, and Scott L\nDuVall. 2010 i2b2/va challenge on concepts, assertions, and\nrelations in clinical text. Journal of the American Medical\nInformatics Association, 18(5):552–556, 2011.\n19. Anbang Wang, Difei Mei, Zhichao Zhang, Xiuxiu Bai, Ran\nYao, Zewen Fang, Min Hu, Zhirui Cao, Haitao Sun, Yifeng\nGuo, et al.\nReversener: A self-generated example-driven\nframework for zero-shot named entity recognition with large\nlanguage models. arXiv preprint arXiv:2411.00533, 2024.\n20. Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei\nWu, Tianwei Zhang, Jiwei Li, Guoyin Wang, and Chen\nGuo. Gpt-ner: Named entity recognition via large language\nmodels. In Findings of the Association for Computational\nLinguistics: NAACL 2025, pages 4257–4275, 2025.\n21. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain of thought reasoning\nin language models.\narXiv preprint arXiv:2203.11171,\n2022.\n22. Yanshan Wang,\nLiwei Wang,\nMajid Rastegar-Mojarad,\nSungrim Moon, Feichen Shen, Naveed Afzal, Sijia Liu,\nYuqun Zeng,\nSaeed Mehrabi,\nSunghwan Sohn,\net al.\nClinical information extraction applications: a literature\nreview. Journal of biomedical informatics, 77:34–49, 2018.\n23. Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin\nZhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen,\nMeishan Zhang,\net al.\nChatie:\nZero-shot information\nextraction via chatting with chatgpt.\narXiv preprint\narXiv:2302.10205, 2023.\n24. Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu,\nand Hongwei Wang.\nEmpirical study of zero-shot ner\n"}, {"page": 10, "text": "10\nTao et al.\nwith chatgpt. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pages\n7935–7956, 2023.\n25. Tingyu\nXie,\nQi\nLi,\nYan\nZhang,\nZuozhu\nLiu,\nand\nHongwei Wang. Self-improving for zero-shot named entity\nrecognition with large language models. In Proceedings of\nthe 2024 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies (Volume 2: Short Papers), pages\n583–593, 2024.\n26. Zikang Zhang, Wangjie You, Tianci Wu, Xinrui Wang,\nJuntao Li,\nand Min Zhang.\nA survey of generative\ninformation\nextraction.\nIn\nProceedings\nof\nthe\n31st\nInternational Conference on Computational Linguistics,\npages 4840–4870, 2025.\n27. Jin Zhao, Qian Guo, Jiaqing Liang, Zhixu Li, and Yanghua\nXiao.\nEffective in-context learning for named entity\nrecognition.\nIn 2024 IEEE International Conference on\nBioinformatics and Biomedicine (BIBM), pages 1376–\n1382. IEEE, 2024.\n"}, {"page": 11, "text": "Short Article Title\n11\nSupplementary Material\nTable A1. An example of the zero-shot in-context learning (ICL) prompt used in the self-annotation stage of the OEMA framework. The\nprompt guides the LLM to identify named entities in raw clinical text based on a predefined entity type and output them in a JSON format.\nThe resulting self-annotated corpus is then filtered via ontology-enhanced token-level example selection to obtain high-quality few-shot\nexemplars for the final NER inference.\nPrompt designed for self-annotation\nYou are an expert in medical named entity recognition. You’re very good at extracting information. Given entity label set:\n[‘Medical problem’, ‘Treatment’, ‘Test’]\nPlease recognize the named entities in the given text. Based on the given entity label set, provide answer in the following\nJSON format: [{‘Entity Name’: ‘Entity Label’}]. If there is no entity in the text, return the following empty list: []. Only\nreturn answer, not explanations.\nText: “The patient presented to our emergency room for worsening abdominal pain as well as swelling of the right lower\nleg.”\nAnswer:\nTable A2. An example of the ICL prompt used for extracting clinical ontologies based on the SNOMED CT top-level hierarchy. The prompt\ninstructs the LLM to identify medical concepts appearing in the input text according to the 18 top-level SNOMED CT categories, and to\noutput results in a JSON format. These ontology-level annotations are later used by the discriminator to evaluate token-level similarity and\nassist in selecting the most relevant examples for downstream NER.\nPrompt designed for extracting the clinical ontologies\nPlease refer to the 18 top-level categories defined in the Concept Hierarchy of SNOMED CT, and explicitly extract the\nclinical medical ontologies mentioned in the text in the order of their appearance.\nProvide answer in the format: {“(top-level category, ontology)”: “original text fragment”, ...}. Only a dictionary string\nshould be returned, without any Markdown formatting, code blocks, or additional content.\nText: “She started off with a little pimple on the buttock.”\nAnswer: {“(Clinical finding, Pustule)”: “pimple”, “(Body structure, Buttock)”: “buttock”}\n. . . . . .\n(Selected examples with medicine ontology)\n. . . . . .\nText: The patient presented to our emergency room for worsening abdominal pain as well as swelling of the right lower leg.\nAnswer:\n"}, {"page": 12, "text": "12\nTao et al.\nTable A3. An example of the ICL prompt used in the example-scoring stage of the OEMA framework. The prompt guides the LLM to\nassign helpfulness scores to candidate self-annotated examples by evaluating the relevance of their clinical ontology–text fragment pairs to\nthose in the target sentence. This scoring process enables the discriminator to select the most suitable few-shot exemplars for constructing\nthe final inference prompt.\nPrompt designed for scoring examples\n### Example Scoring for Entity Recognition Tasks\nGiven entity label set: [‘Medical problem’, ‘Treatment’, ‘Test’] and target sentence: {\n‘sentence’: ‘She started off with a little pimple on the buttock.’,\n‘ontology’: ‘{“(Clinical finding, Pustule)”: “pimple”, “(Body structure, Buttock)”: “buttock”}’\n}\n### Scoring Guidelines\nBased on the target sentence has learned SNOMED CT medical ontology and may involve entity type, please predict the\nhelpfulness scores and give reasons of each sentence, which indicates the degree to which providing the current sentence can\naid in extracting named entities from the target sentence. The score ranges from 1 to 5, with 1 being the least helpful and\n5 being the most helpful.\nProvide answer in the following JSON format: [{“idx”: “sentence identifier”, “score”: “be strict and reflect the differences\nin scores, not all 1 or all 5”, “reason”: “combined with the characteristics of the target sentence”}, ...]\nMake sure that the output is a complete string, do not use newline characters, Markdown format, ‘‘‘json, or any additional\ninstructions, and only return formatted string results.\n. . . . . .\n(Selected examples with medicine ontology)\n. . . . . .\nTable A4. An example of the ICL prompt used in the final prediction stage of the OEMA framework. The prompt integrates entity-type\ndescriptions with the selected high-quality self-annotated examples to provide structured, type-aware guidance for the LLM. Based on this\ncombined prompting, the LLM performs the final clinical named entity recognition on the target sentence and outputs the results in a JSON\nformat.\nPrompt designed for the final prediction\nYou are an expert in medical named entity recognition. You’re very good at extracting information.\n. . . . . .\n(Entity type description)\n. . . . . .\nGiven entity label set: [‘Medical problem’, ‘Treatment’, ‘Test’]\nPlease recognize the named entities in the given text. Based on the given entity label set, provide answer in the following\nJSON format: [{‘Entity Name’: ‘Entity Label’}]. If there is no entity in the text, return the following empty list: []. Only\nreturn answer, not explanations.\nText: “She would usually have pustular type of lesion that would eventually break and would be quite painful.”\nAnswer: [{“pustular type of lesion”: “Medical problem”}]\n. . . . . .\n(Selected examples with self-annotated label)\n. . . . . .\nText: “She started off with a little pimple on the buttock.”\nAnswer:\n"}]}