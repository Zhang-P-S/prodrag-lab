{"doc_id": "arxiv:2511.20507", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.20507.pdf", "meta": {"doc_id": "arxiv:2511.20507", "source": "arxiv", "arxiv_id": "2511.20507", "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models", "authors": ["Nathan Roll", "Jill Kries", "Flora Jin", "Catherine Wang", "Ann Marie Finley", "Meghan Sumner", "Cory Shain", "Laura Gwilliams"], "published": "2025-11-25T17:16:38Z", "updated": "2025-11-25T17:16:38Z", "summary": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.20507v1", "url_pdf": "https://arxiv.org/pdf/2511.20507.pdf", "meta_path": "data/raw/arxiv/meta/2511.20507.json", "sha256": "b7a101ff0afce84b03a5e881654ccd70d4ecb7261129a63152c29f6c99e84ae9", "status": "ok", "fetched_at": "2026-02-18T02:26:16.839837+00:00"}, "pages": [{"page": 1, "text": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for\nAphasia-Like Deficits in Language Models\nNathan Roll1, Jill Kries1, Flora Jin2, Catherine Wang5,6, Ann Marie Finley3,\nMeghan Sumner1, Cory Shain1, Laura Gwilliams1\n1Stanford University, 2University of California, San Francisco, 3Temple University,\n5San Diego State University, 6University of California, San Diego\nAbstract\nLarge language models (LLMs) have emerged\nas a candidate ‘model organism’ for human\nlanguage, offering an unprecedented opportu-\nnity to study the computational basis of lin-\nguistic disorders like aphasia. However, tra-\nditional clinical assessments are ill-suited for\nLLMs, as they presuppose human-like prag-\nmatic pressures and probe cognitive processes\nnot inherent to artificial architectures. We in-\ntroduce the Text Aphasia Battery (TAB), a\ntext-only benchmark adapted from the Quick\nAphasia Battery (QAB) to assess aphasic-like\ndeficits in LLMs. The TAB comprises four\nsubtests: Connected Text, Word Comprehen-\nsion, Sentence Comprehension, and Repetition.\nThis paper details the TAB’s design, subtests,\nand scoring criteria. To facilitate large-scale\nuse, we validate an automated evaluation pro-\ntocol using Gemini 2.5 Flash, which achieves\nreliability comparable to expert human raters\n(prevalence-weighted Cohen’s κ\n=\n0.255\nfor model–consensus agreement vs. 0.286 for\nhuman–human agreement). We release TAB\nas a clinically-grounded, scalable framework\nfor analyzing language deficits in artificial sys-\ntems.\n1\nIntroduction\nThe clinical framework for assessing aphasia, a\nlanguage impairment resulting from brain damage,\nhas long relied on batteries such as the Western\nAphasia Battery–Revised (WAB-R) (Kertesz, 2022;\nClark et al., 2020), the Boston Diagnostic Aphasia\nExamination (BDAE), the Comprehensive Aphasia\nTest (CAT) (Bruce and Edmundson, 2010; Springer\nand Mantey, 2010), and the Quick Aphasia Battery\n(QAB) (Wilson et al., 2018). Recently, researchers\nhave begun exploring connections between compu-\ntational language models and aphasia, for instance\nusing language model surprisals in tandem with\nclinical features to predict aphasia subtypes (Cong\net al., 2024). However, directly evaluating language\nmodels using traditional aphasia batteries faces a\nfundamental challenge: these batteries use multi-\nmodal examination to probe deficits arising from\nlesions to specific brain structures. Their ques-\ntions and scoring criteria make implicit assump-\ntions about patterns of behavioral deficits that have\nno analog in text-only computational models.\nTo address this challenge, we introduce the Text\nAphasia Battery (TAB). Developed with speech-\nlanguage pathologists, the TAB is a benchmark that\nadapts core components of the clinically-validated\nQuick Aphasia Battery (QAB) (Wilson et al., 2018)\nfor text-constrained environments. We posit that\nwhile aphasia is a multimodal disorder, a signif-\nicant component of its behavioral signature is\nidentifiable and recognizable in transcribed text\nalone. Leveraging the availability of large-scale\ntranscribed datasets like AphasiaBank, we can iso-\nlate these text-based linguistic patterns to study\nlanguage breakdown at scale. The TAB reframes\naphasia assessment from a neuropsychological tool\nto a behavioral benchmark, identifying patterns of\nlanguage degradation in LLMs that are analogous,\nbut not homologous, to human syndromes.\nThis paper’s primary contribution is to introduce\nand describe the TAB. We detail its design princi-\nples, four subtests, and clinical grounding, offering\nit as a resource for standardized, text-only evalua-\ntion of LLMs. To facilitate large-scale adoption, we\nalso present and validate an automated evaluation\nprotocol that achieves high human-expert rater reli-\nability, ensuring the benchmark is both robust and\nscalable. We release the TAB to provide the field\nwith a new tool for assessing linguistic breakdown\nin artificial systems.\n2\nBackground: Traditional Aphasia\nBatteries\nAphasia batteries are standardized assessments that\nevaluate language abilities in individuals with apha-\n1\narXiv:2511.20507v1  [cs.CL]  25 Nov 2025\n"}, {"page": 2, "text": "sia, defined as receptive and/or expressive language\ndeficits due to brain damage resulting from stroke,\ntraumatic brain injury, brain tumor, or neurode-\ngenerative processes. These tests are essential in\nclinical practice, enabling speech-language pathol-\nogists (SLPs) to identify the presence, type, and\nseverity of aphasia, inform treatment, and monitor\nrecovery.\n2.1\nMajor Aphasia Assessment Tools\nBoston\nDiagnostic\nAphasia\nExamination\n(BDAE)\nThe BDAE is a comprehensive battery\ndesigned to diagnose aphasia and related disorders.\nIt evaluates a broad range of language skills,\nincluding conversation, narrative speech, auditory\ncomprehension,\noral\nexpression,\nrepetition,\nreading, and writing, allowing for the classification\nof aphasia syndromes based on symptom patterns.\nWestern Aphasia Battery–Revised (WAB-R)\nOne of the most widely used aphasia assessments,\nthe WAB-R evaluates language and related cog-\nnitive functions through 8 subtests comprising 32\nshort tasks (Kertesz, 2022). It assesses spontaneous\nspeech, auditory comprehension, repetition, nam-\ning, reading, and writing, along with nonlinguistic\nskills like apraxia and calculation. The test pro-\nduces an Aphasia Quotient (AQ) to quantify sever-\nity and classify aphasia types such as Broca’s or\nWernicke’s (Clark et al., 2020).\nComprehensive Aphasia Test (CAT)\nThe CAT\nevaluates the recognition, comprehension, and pro-\nduction of spoken and written language through\nthree components: cognitive screening, language\nassessment, and a disability questionnaire, with the\nlatter assessing functional communication and psy-\nchosocial factors (Bruce and Edmundson, 2010;\nSpringer and Mantey, 2010). The CAT is a reli-\nable and validated tool for assessing underlying\nlanguage impairments in adults with aphasia (Halai\net al., 2022).\nQuick Aphasia Battery (QAB)\nThe QAB is a\nrapid, multidimensional assessment that provides\na reliable language evaluation in 15 to 20 minutes\n(Wilson et al., 2018). It evaluates major language\ndomains through eight subtests and uses a graded\nscoring system sensitive to changes in language\nfunction over time.\n2.2\nCommon Assessment Domains\nMajor aphasia batteries share several core evalu-\nation targets that are central to language function\n(Salter et al., 2006; Wilson et al., 2018). These\ninclude auditory comprehension of words and sen-\ntences, repetition of words, phrases, and sentences,\nnaming based on auditory or written description,\nnaming a visually presented object, reading com-\nprehension, and writing production. Many batteries\nalso assess adjunct domains such as praxis (motor\nprogramming), arithmetic, and visuoconstruction\nto distinguish aphasia from other cognitive disor-\nders (Kertesz, 2022).\n3\nLimitations of Traditional Batteries for\nLLM Evaluation\nAphasia batteries have proven invaluable for diag-\nnosing and managing aphasia in clinical settings.\nHowever, their design and underlying assumptions\nmake them fundamentally ill-suited for evaluating\nLLMs. These limitations arise from multimodal\ndependencies, functional communication, and the\ninclusion of psychosocial measures.\n3.1\nNon-Linguistic and Contextual\nDependencies\nClinical assessments require integration across sen-\nsory modalities (visual, auditory, gestural) and rely\non human-specific psychosocial context. For exam-\nple, picture naming tasks and verbal repetition with\nprosodic cues are common in traditional batteries\n(Wilson et al., 2018). Furthermore, these assess-\nments often incorporate metacognitive measures\nprobing self-awareness and motivation (Bruce and\nEdmundson, 2010; Springer and Mantey, 2010).\nMany text-only LLMs, however, lack auditory\nperception, sensory grounding, and the pragmatic\nintent or self-awareness required for these tasks.\nThey cannot perceive images, produce gestures, or\nexhibit speech-motor impairments (e.g., apraxia).\nTherefore, it is impossible to apply these assess-\nments directly. This mismatch highlights the need\nfor benchmarks that isolate linguistic competence\nfrom these non-linguistic dependencies.\n3.2\nSubjectivity and Granularity of\nMeasurement\nTraditional batteries often rely on subjective human\nrating, which may not capture the full granularity\nof linguistic impairment or the complete range of\ndisease presentation. Clinical ratings can be vari-\n2\n"}, {"page": 3, "text": "able and may miss subtle distributional patterns in\nspeech. Shifting to text-only evaluation enables the\nobjective, scalable analysis of specific linguistic di-\nmensions—such as lexical retrieval, syntactic com-\nplexity, and discourse coherence—that are robustly\nidentifiable in transcripts. This allows for a more\ngranular characterization of linguistic breakdown\nthan is typically possible with composite clinical\nscores.\n3.3\nFunctional Communication vs. Structural\nLanguage\nAphasia tests measure deficits in functional com-\nmunication, the ability to use language flexibly for\ncommunicative purpose and meaning (Wilson et al.,\n2018). This includes pragmatic skills, contextual\nunderstanding, and the ability to convey meaning\neffectively. In contrast, to the extent that LLMs\nhave social goals and intents, these are at best quite\ndifferent from those of humans, and therefore the\nnotion of functional communication does not nec-\nessarily apply to LLMs in the ways assumed by\nexisting aphasia batteries. Nonetheless, beyond\nfunctional communication, aphasias also plausibly\nimplicate aspects of structural language compe-\ntence that appear to be present (and impairable) in\nLLMs (Mahowald et al., 2024), including gram-\nmatical accuracy, textual coherence, and fluency.\nTo use LLMs to study such dimensions in aphasia,\nLLM-appropriate assessment strategies are needed.\n4\nMaterials: The Text Aphasia Battery\n(TAB)\nTo address the limitations of traditional aphasia bat-\nteries while preserving their clinical insights, we de-\nveloped the TAB, a text-only benchmark that adapts\ncore QAB components for modality-constrained\nevaluation. The TAB focuses on linguistic compe-\ntence rather than neuropsychological deficit map-\nping, which makes it suitable for large-scale LLM\nassessment.\nThis shift to text is justified because the core lin-\nguistic dimensions affected in aphasia—lexical re-\ntrieval (anomia), syntactic structure (agrammatism,\nparagrammatism), discourse coherence (empty\nspeech), and repetition fidelity—are directly iden-\ntifiable in transcribed text. By isolating these fea-\ntures, the TAB provides a principled lens for char-\nacterizing language breakdown in computational\nsystems without requiring multimodal or psychoso-\ncial constructs. We developed the TAB in close\ncollaboration with speech-language pathologists\nand aphasiologists to ensure that its adapted tasks\nmaintain clinical relevance while being suitable for\nautomated computational evaluation.\n4.1\nDesign Principles\nThe TAB adheres to three core design principles.\nThe first is modality constraint: all inputs and\noutputs are text-based, eliminating dependencies\non auditory perception, visual naming, or motor\nproduction. The second is computational inter-\npretability: scoring focuses on observable linguis-\ntic patterns (e.g., morphosyntactic errors, semantic\nparaphasias) that can be systematically identified\nthrough automated analysis. The third principle\nis clinical grounding: subtests and evaluation cri-\nteria are derived from established clinical aphasia\nresearch. This includes the QAB framework (Wil-\nson et al., 2018) and the APROCSA (Auditory-\nPerceptual Rating of Connected Speech in Apha-\nsia) system (Casilio et al., 2019), with additional\ninsights from clinical discourse analysis methods\n(MacWhinney et al., 2011; Forbes et al., 2012).\nWe validated the adaptation process with clinical\nexperts.\n4.2\nTAB Subtests\nThe TAB consists of four subtests that evaluate\ncomplementary aspects of linguistic function (see\nTable 2 in the Appendix for a full overview). The\nfour subtests are:\n• Connected Text, which evaluates fluency,\ngrammaticality, and discourse coherence us-\ning 5 open-ended prompts. Responses are\nscored for 19 linguistic features based on the\nAPROCSA framework (Casilio et al., 2019).\n• Word\nComprehension,\nwhich\nassesses\nlexical-semantic processing with 5 forced-\nchoice questions, requiring selection from six\ncompeting alternatives.\n• Sentence Comprehension, which tests syn-\ntactic processing through 5 Yes/No questions\ninvolving passive voice, negation, and condi-\ntional reasoning.\n• Repetition, which measures morphosyntactic\nintegrity and attentional stability by requiring\nexact reproduction of 5 items of increasing\nlength and complexity.\n3\n"}, {"page": 4, "text": "4.2.1\nConnected Text\nObjective: Evaluate fluency, grammaticality, and\ndiscourse coherence.\nInstructions: The LLM system is prompted to\nrespond to five open-ended prompts in 3 to 5 full\nsentences. To elicit naturalistic responses and avoid\nmeta-commentary (e.g., “As an AI, I don’t have per-\nsonal experiences”), we employ system prompts\nthat establish a conversational context without ex-\nplicitly requesting the model to assume a persona.\nThe prompts are: “Tell me about the best trip you\never took,” “Describe a happy childhood memory,”\n“Tell me about your first job,” “What do you like\nabout where you live?,” and “Describe the steps to\nmake a simple meal.”\nEvaluation: Responses are analyzed for 19\naphasic features adapted from the APROCSA\n(Auditory-Perceptual Rating of Connected Speech\nin Aphasia) framework (Casilio et al., 2019). These\nfeatures include anomia, paraphasias (semantic\nand phonemic), agrammatism (omission of bound\nmorphemes or function words), paragrammatism,\nempty speech, perseverations, neologisms, and\noverall communication impairment. Each feature\nis scored as present (1) or absent (0).\n4.2.2\nWord Comprehension\nObjective: Evaluate lexical-semantic processing\nand selection among competing alternatives.\nInstructions: The system responds verbatim to\nfive forced-choice items, with six options provided\nfor each. The foils were selected to be phono-\nlogically and semantically varied. The items are:\n“Which one is an animal with a mane? (lion, drum,\nviolin, giraffe, boot, boat),” “Which object is typ-\nically used to make music? (violin, giraffe, lion,\ndoor, boot, boat),” “Which item is usually worn on\nthe feet? (boot, boat, lion, drum, violin, giraffe),”\n“Which object is used for cutting? (knife, kite, lion,\ndrum, violin, giraffe),” and “Which one is a large\nmammal with a long neck? (giraffe, horse, lion,\ndrum, violin, boot).”\nEvaluation: Binary correct/incorrect scoring.\nErrors may indicate deficits in semantic processing\nor susceptibility to phonological similarity effects.\nThe six-option format increases task difficulty and\nreduces chance performance.\n4.2.3\nSentence Comprehension\nObjective: Evaluate syntactic processing, compre-\nhension of passive structures, and logical reason-\ning.\nInstructions: The system responds verbatim\n(Yes/No) to five items: “Are babies watched by\nbabysitters?” (Expected: Yes), “Do you cut the\ngrass with an axe?” (Expected: No), “If you’re\nabout to leave, have you left yet?” (Expected: No),\n“Are witnesses questioned by police?” (Expected:\nYes), and “If I was at the park when you arrived,\ndid I get there first?” (Expected: Yes).\nEvaluation: Binary correct/incorrect scoring.\nErrors may indicate difficulties with passive voice,\nnegation, or temporal/conditional reasoning.\n4.2.4\nRepetition\nObjective: Evaluate morphosyntactic integrity and\nexact reproduction.\nInstructions: The system repeats five items ex-\nactly: “house,” “breakfast,” “catastrophe,” “The\nsun rises in the East,” and “The ambitious journal-\nist discovered where we’d be going.”\nEvaluation:\nExact match required.\nErrors\n(substitutions, deletions, insertions) indicate mor-\nphosyntactic processing deficits or instability in\nrepresentation. For an LLM, this task is not a probe\nof the auditory-verbal loop. It is a test of attentional\nstability and the ability to resist semantic drift or\nelaboration. Failure on this verbatim copy task\ncan reveal subtle deficits in sequence-to-sequence\ntransduction. The model might paraphrase, over-\ngeneralize, or otherwise deviate from the source\ntext. This provides a window into its capacity for\nprecise information transfer.\n5\nMethod: Automated Evaluation\nA key design goal of the TAB is scalability through\nautomated evaluation. While Subtests 2–4 can be\nscored algorithmically (exact string matching or\nbinary classification), Subtest 1 (Connected Text)\nrequires nuanced linguistic analysis. We employ in-\ncontext learning with Gemini 2.5 Flash to identify\naphasic features systematically. This automated\nscoring protocol enables large-scale analysis and\nachieves inter-rater reliability comparable to expert\nclinical raters when properly weighted by feature\nprevalence (see Section 6 for validation details).\n5.1\nAPROCSA-Based Feature Set\nConnected Text responses are evaluated for 19\nfeatures adapted from the APROCSA (Auditory-\nPerceptual Rating of Connected Speech in Apha-\nsia) system (Casilio et al., 2019).\nAPROCSA\nis an auditory-perceptual rating system for con-\nnected speech in aphasia that assesses 27 features\n4\n"}, {"page": 5, "text": "of connected speech on a five-point scale. For\nthe TAB, we selected 19 features applicable to\ntext-only evaluation and adapted them to binary\nscoring. The lexical features are anomia, seman-\ntic paraphasias, phonemic paraphasias, and neolo-\ngisms. Fluency and productivity features are empty\nspeech and short and simplified utterances. Mor-\nphosyntactic features include omission of bound\nmorphemes, omission of function words, and para-\ngrammatism. Disfluency features consist of aban-\ndoned utterances, false starts, retracing, and con-\nduite d’approche. Perseverative features are per-\nseverations, stereotypies, and automatisms. Co-\nherence features include jargon, meaning unclear,\nand off-topic utterances. A final feature is overall\ncommunication impairment.\n5.2\nIn-Context Prompting Protocol\nWe provide the evaluating LLM with a definition\nand an example for each of the 19 features, adapted\nfrom the APROCSA framework (Casilio et al.,\n2019). We also provide two annotated example\ntranscripts with ground-truth feature labels. Finally,\nwe provide instructions to output a JSON object\nwith each feature as a key and binary (0/1) values.\nThis few-shot prompting approach enables con-\nsistent feature identification across large datasets\nwithout requiring manual annotation. The prompt\ntemplate is included in the TAB’s repository and\ncan be adapted for different LLM architectures.\n6\nValidation of Automated Evaluation\nTo validate our automated protocol, we conducted\nan inter-rater reliability (IRR) study comparing our\nGemini 2.5 Flash-based system against a ground\ntruth derived from expert human annotators.\n6.1\nData Collection and Annotation\nThe validation dataset for this study comprises 561\nEnglish text samples from two sources.\nHuman Data\nWe sampled 306 transcribed re-\nsponses from individuals with aphasia, sourced\nfrom the AphasiaBank database (MacWhinney\net al., 2011). These samples cover a range of apha-\nsia types and severities, providing a clinically rele-\nvant baseline.\nAI Data\nWe generated 255 text samples by ad-\nministering the TAB’s protocol to a variety of \"le-\nsioned\" large language models, including GPT-2\n(Radford et al., 2019), Pythia (Biderman et al.,\n2023), and Llama (Touvron et al., 2023). Lesions\nwere simulated using techniques such as param-\neter zeroing, swapping, or global scaling applied\nto specific components (e.g., attention heads, feed-\nforward networks) and layers, with varying severity\nfrom 10% to 40%. Lesioning techniques were dis-\ntributed across models and components to ensure\ndiverse aphasic-like patterns, with specific propor-\ntions varying by model architecture.\nExpert Annotation\nA pool of five expert speech-\nlanguage pathologists (SLPs) collectively anno-\ntated the dataset of 561 samples through a custom\nweb interface. For each sample, SLPs rated the\npresence (1) or absence (0) of the 19 APROCSA-\nbased features. These manual ratings, stored in a\nFirestore database, form our ground truth consen-\nsus.\n6.2\nInter-Rater Reliability\nFrom a dataset of 561 text samples, 82 were anno-\ntated by multiple (2–3) speech-language patholo-\ngists from a pool of five experts. We established\nground truth via majority vote on these items and\nused Cohen’s Kappa (κ) to measure agreement.\nA significant methodological issue emerged\nfrom this analysis: the choice of how to aggregate\nagreement scores across the 19 evaluated aphasic\nfeatures. An unweighted macro-average, which\ntreats all features equally, is problematic when fea-\ntures have vastly different clinical prevalence rates.\nIn our validation set, some features never occurred\n(zero positive instances), while others like Perse-\nverations appeared in 34% of samples. For features\nwith zero prevalence, human-human agreement is\nundefined (κ cannot be calculated when there is\nno variation), while the automated system trivially\nachieves perfect agreement by also never detect-\ning these features. This creates an artifact: the\nmodel appears to have perfect agreement on zero-\nprevalence features (contributing 1.0 to its average),\nwhile human raters contribute 0.0 or undefined val-\nues, artificially inflating the unweighted model av-\nerage while deflating the human average.\nTo provide a more clinically meaningful re-\nsult, we report the prevalence-weighted Cohen’s\nKappa, where each feature’s score is weighted\nby its positive instance count in the validation set.\nPrevalence-weighting is standard in meta-analytic\ncontexts (Cohen, 1968) because it ensures that esti-\nmates reflect the real-world distribution of phenom-\nena. In our case, this is particularly appropriate\n5\n"}, {"page": 6, "text": "because clinically important features (those that\nactually occur in aphasic language) should carry\nmore weight than features absent in our sample.\nThis approach aligns with clinical practice, where\ndiagnostic decisions are based on observable symp-\ntoms rather than the absence of rare phenomena.\nAs shown in Table 1, this weighted analysis\nreveals that humans and the automated system\nachieve comparable performance. The weighted\nhuman–human agreement (κ = 0.286) is close to\nthe model–consensus agreement (κ = 0.255), with\nboth falling into a commonly described “fair” range.\nWe acknowledge that these agreement scores are\nrelatively low, which likely reflects the inherent dif-\nficulty of the annotation task—identifying subtle\nlinguistic features in text without prosodic or gestu-\nral cues. Future improvements might include pro-\nviding annotators with more surrounding linguistic\ncontext, developing more explicit annotation guide-\nlines with additional examples, or incorporating a\ntraining phase to calibrate annotators. Despite these\nchallenges, the comparable performance between\nhuman and automated annotation suggests that au-\ntomated evaluation can serve as a reliable screening\ntool when paired with expert review, with comple-\nmentary strengths supporting a hybrid workflow\nwhere automated screening is paired with expert\nreview of high-frequency clinical features.\nAggregation Method\nHuman κ\nModel κ\nUnweighted average\n0.061\n0.794\nWeighted (by prevalence)\n0.286\n0.255\nExcluding zero-prevalence\n0.215\n0.381\nTable 1: Inter-rater reliability (Cohen’s κ) for human\nvs. automated ratings on 82 multiply-annotated samples.\nThe prevalence-weighted average, our primary metric,\nshows comparable performance. The unweighted aver-\nage is misleadingly high for the model due to agreement\non many features with zero positive instances in the\nvalidation set.\n7\nDiscussion\n7.1\nTAB as a Behavioral Benchmark\nUnlike traditional aphasia batteries designed for\nneuropsychological diagnosis, the TAB functions\nas a behavioral benchmark. It reveals patterns of\nbehavioral breakdown in text-based systems. When\nLLMs exhibit TAB-identified deficits (e.g., agram-\nmatism, semantic paraphasias), these patterns do\nnot imply neurological dysfunction. Rather, they\nreflect limitations in the model’s training data, ar-\nchitectural properties, or induced representational\ndamage (in the case of ablations or lesioning in-\nterventions). The release of the TAB provides the\nresearch community with a standardized tool to\nexplore these phenomena systematically.\nThis behavioral focus enables a novel form of\ninvestigation: by systematically characterizing how\nlanguage breaks down in LLMs, we can gener-\nate hypotheses about the computational underpin-\nnings of human aphasic deficits. Conversely, vali-\ndating that LLMs exhibit recognizable, clinically-\ngrounded patterns of language breakdown is a nec-\nessary step toward using them as tractable models\nfor studying human aphasia at scale.\n7.2\nBridging Clinical and Computational\nLinguistics\nThe TAB establishes a principled connection be-\ntween clinical aphasiology and computational lin-\nguistics. By grounding evaluation in established\nclinical frameworks like the QAB (Wilson et al.,\n2018) and APROCSA (Casilio et al., 2019), it en-\nables researchers to systematically compare LLM\nperformance against human aphasic profiles. It\nalso helps identify specific linguistic competen-\ncies (e.g., morphosyntactic processing, semantic\ncoherence) that may be compromised under cer-\ntain conditions. Finally, it allows researchers to\ndevelop targeted interventions, such as fine-tuning\nstrategies or architectural modifications, informed\nby clinical insights into language breakdown.\nConversely, this framework positions LLMs as\npowerful testbeds for theories of language process-\ning relevant to aphasiology. By systematically \"le-\nsioning\" models (e.g., through architectural abla-\ntion, parameter pruning, or adversarial fine-tuning)\nand assessing them with the TAB, researchers can\nconduct \"virtual lesion studies\" that are impossible\nin humans. This approach offers a novel method-\nology for testing hypotheses about the functional\narchitecture of language, such as the relationship\nbetween syntactic and semantic processing, and\nfor modeling patterns of language breakdown that\ncould, in turn, inform clinical neuroscience.\n7.3\nApplications\nThe TAB is suitable for diverse applications, includ-\ning model evaluation for systematic assessment\nof LLM linguistic competencies, ablation studies\nfor evaluating the effects of architectural changes\non linguistic integrity, adversarial robustness test-\n6\n"}, {"page": 7, "text": "ing to probe model stability, and interpretability\nresearch for mapping internal representations to\nobservable linguistic patterns.\n8\nConclusion\nWe have presented the Text Aphasia Battery (TAB),\na benchmark for evaluating aphasic-like linguis-\ntic deficits in modality-constrained environments.\nBy adapting clinical aphasia assessment principles\nfor text-only evaluation, the TAB addresses the\nfundamental limitations of traditional batteries for\nLLMs while preserving their clinical grounding.\nIts four subtests, detailed in this paper, provide\nsystematic coverage of key linguistic domains and\ncan be used with a validated, scalable automated\nevaluation protocol. TAB represents a conceptual\nshift from neuropsychological deficit mapping to\ncomputational representational analysis. By releas-\ning it as a resource, we aim to equip researchers\nwith a standardized tool to investigate linguistic\nbreakdown in artificial systems, fostering new con-\nnections between clinical aphasiology and compu-\ntational linguistics.\nLimitations\nWhile the TAB addresses key limitations of tradi-\ntional aphasia batteries for LLM evaluation, several\nconstraints remain.\nClinical Validity\nThe TAB is not validated for\nthe clinical diagnosis of human aphasia and should\nnot be used as a replacement for established clinical\ninstruments like the WAB-R, CAT, or QAB.\nModality Constraints\nBy design, the TAB omits\nimportant dimensions of human language process-\ning, including auditory perception, prosody, motor\nspeech production, and gesture, to enable the eval-\nuation of text-only systems.\nAutomated Evaluation Reliability\nOur vali-\ndation establishes that our automated protocol\nachieves reliability comparable to expert human\nannotators when weighted by feature prevalence.\nThe prevalence-weighted Cohen’s Kappa of 0.255\n(model) vs. 0.286 (human) suggests practical par-\nity. An important insight from our work is that\nunweighted averaging can produce misleading re-\nsults when features have highly variable prevalence\nrates; we recommend prevalence-weighting for fu-\nture studies.\nCoverage of Aphasic Syndromes\nThe TAB\nevaluates linguistic features associated with var-\nious aphasic syndromes but does not classify sys-\ntems into traditional syndrome categories (e.g.,\nBroca’s, Wernicke’s), as such classification may\nnot be meaningful for artificial systems lacking\nneuroanatomical substrates.\nLimited Item Set\nEach subtest contains five\nitems, prioritizing rapid assessment over compre-\nhensive coverage. Expanded versions may be nec-\nessary for fine-grained evaluation.\nInstruction-Following Requirement\nThe TAB\ntasks require instruction-following capabilities,\nmaking them most suitable for instruction-tuned\nmodels. Base models without instruction-tuning\nmay require adaptation of the protocol or may per-\nform poorly regardless of their underlying linguis-\ntic competence.\nLanguage and Cultural Specificity\nThe current\nimplementation of the TAB is English-only and re-\nflects cultural assumptions from the original QAB.\nAdaptation to other languages will require careful\nlinguistic and cultural consideration.\nJudge Dependence and Robustness\nAutomated\nevaluation currently relies on a single judge model\nand prompt. Future work will evaluate robustness\nacross multiple judge models and prompts and\ninclude human adjudication of disagreements to\nbound judge-specific bias.\nPsychometrics and Item Design\nFuture work\nwill increase subtest difficulty to mitigate ceiling ef-\nfects (e.g., harder foils, broader syntactic phenom-\nena, balanced Yes/No) and perform item-response\nanalysis to estimate item difficulty and discrimina-\ntion. We will also analyze repetition errors under a\nspecified normalization policy.\nEthics Statement\nThe TAB is released as an open research tool under\nthe MIT License. We emphasize that the TAB is\ndesigned exclusively for computational evaluation\nand research. It should not be used for clinical diag-\nnosis of human aphasia, medical decision-making,\nor assessment of individuals without appropriate\nclinical expertise and validation. Researchers using\nthe TAB should clearly communicate its limitations\nand intended use cases to prevent misapplication.\n7\n"}, {"page": 8, "text": "Acknowledgments\nThe TAB was developed in close collaboration with\nspeech-language pathologists and aphasiologists,\nwhose clinical expertise was essential to this work.\nThe expert annotators who conducted all reliability\nassessments are listed as co-authors on this paper.\nWe thank Maria Ivanova and Alexis Pracar (both at\nUniversity of California, Berkeley) for their contri-\nbutions to annotations and TAB development. We\nthank the developers of the Quick Aphasia Battery\nand the APROCSA rating scale for establishing the\nclinical foundation upon which the TAB is built.\nWe also acknowledge the broader speech-language\npathology and aphasiology communities for their\ndecades of rigorous research that informs this work.\nData Availability\nThe AphasiaBank data used in this study is\navailable to researchers through the Aphasi-\naBank database (https://aphasia.talkbank.\norg/). The lesioned model outputs and expert an-\nnotations generated for this study will be made\navailable upon request to support reproducibility\nand further research.\nReferences\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Edward O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, and 1 others. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling. In International Conference on\nMachine Learning, pages 2397–2430. PMLR.\nCaroline Bruce and Anna Edmundson. 2010. Letting\nthe cat out of the bag: A review of the comprehensive\naphasia test. Aphasiology, 24(1):29–38.\nMarianne Casilio, Katy Rising, Pélagie M. Beeson, Kate\nBunton, and Stephen M. Wilson. 2019. Auditory-\nperceptual rating of connected speech in aphasia.\nAmerican Journal of Speech-Language Pathology,\n28(2):550–568.\nHeather M. Clark, Rene L. Utianski, Joseph R. Duffy,\nEdythe A. Strand, Hugo Botha, Keith A. Josephs,\nand Jennifer L. Whitwell. 2020.\nWestern Apha-\nsia Battery–Revised profiles in primary progressive\naphasia and primary progressive apraxia of speech.\nAmerican Journal of Speech-Language Pathology,\n29(1):498–510.\nJacob Cohen. 1968. Weighted kappa: Nominal scale\nagreement provision for scaled disagreement or par-\ntial credit. Psychological bulletin, 70(4):213.\nYan Cong, Arianna N LaCroix, and Jiyeon Lee. 2024.\nClinical efficacy of pre-trained large language mod-\nels through the lens of aphasia. Scientific reports,\n14(1):15573.\nMargaret Forbes, Davida Fromm, and Brian MacWhin-\nney. 2012. Aphasiabank: A resource for clinicians.\nSeminars in Speech and Language, 33(3):217–222.\nAjay Halai, Blanca De Dios Perez, Justyna Stefaniak,\nand Matthew A. Lambon Ralph. 2022. Efficient and\neffective assessment of deficits and their neural bases\nin stroke aphasia. Cortex, 155:333–349.\nAndrew Kertesz. 2022. The Western Aphasia Battery:\nA systematic review of research and clinical applica-\ntions. Aphasiology, 36(3):267–281.\nBrian MacWhinney, Davida Fromm, Margaret Forbes,\nand Audrey Holland. 2011. Aphasiabank: Methods\nfor studying discourse. Aphasiology, 25(11):1286–\n1307.\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank,\nNancy Kanwisher, Joshua B. Tenenbaum, and\nEvelina Fedorenko. 2024.\nDissociating language\nand thought in large language models. Trends in\nCognitive Sciences, 28(6):517–540.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nKatherine Salter, Jeffrey Jutai, Norine Foley, Carolyn\nHellings, and Robert Teasell. 2006. Identification of\naphasia post stroke: A review of screening assess-\nment tools. Brain Injury, 20(6):559–568.\nLuise Springer and Sabine Mantey. 2010. The com-\nprehensive aphasia test: A review.\nAphasiology,\n24(1):11–28.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, and 1 others. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nStephen M. Wilson, Dana K. Eriksson, Sarah M. Sch-\nneck, and Jillian M. Lucanie. 2018. A quick apha-\nsia battery for efficient, reliable, and multidimen-\nsional assessment of language function. PLoS ONE,\n13(2):e0192773.\nA\nTAB Subtest Overview\nB\nComplete TAB Protocol\nThis appendix provides the complete Text Aphasia\nBattery protocol. TAB is a modified subset of the\nQuick Aphasia Battery (QAB) (Wilson et al., 2018)\nintended to assess aphasic symptoms in text-only\nenvironments, such as with large language models.\n8\n"}, {"page": 9, "text": "Subtest\nObjective\nItems\nEvaluation Features\nConnected Text\nFluency, grammaticality,\ndiscourse coherence\n5\nLexical (4): Anomia, semantic paraphasias, phone-\nmic paraphasias, neologisms\nFluency/Productivity (2): Empty speech, short &\nsimplified utterances\nMorphosyntactic (3):\nOmission of bound mor-\nphemes, omission of function words, paragramma-\ntism\nDisfluency (4): Abandoned utterances, false starts,\nretracing, conduite d’approche\nPerseverative (2): Perseverations, stereotypies & au-\ntomatisms\nCoherence (3): Jargon, meaning unclear, off-topic\nOverall (1): Communication impairment\nWord Comprehension\nLexical-semantic process-\ning\n5\nSemantic processing, lexical selection among six\ncompeting alternatives\nSentence Comprehen-\nsion\nSyntactic processing\n5\nPassive voice, negation, temporal/conditional rea-\nsoning\nRepetition\nMorphosyntactic\nin-\ntegrity\n5\nExact reproduction, phonemic/morphemic preserva-\ntion, attentional stability\nTable 2: Overview of TAB subtests with complete evaluation taxonomy. Connected Text evaluates 19 APROCSA-\nbased features (counts in parentheses) organized into seven categories. Other subtests use binary correct/incorrect or\nexact match scoring.\nTAB is not a diagnostic tool and is not a replace-\nment for the QAB for the clinical assessment of\nhuman aphasia.\nB.1\nSubtest 1: Connected Text\nObjective: Evaluates fluency, grammaticality, and\ncoherence.\nInstructions: “Respond to the following prompt\nin 3–5 full sentences:”\n1. “Tell me about the best trip you ever took.”\n2. “Describe a happy childhood memory.”\n3. “Tell me about your first job.”\n4. “What do you like about where you live?”\n5. “Describe the steps to make a simple meal.”\nScoring: For each response, identify the pres-\nence (1) or absence (0) of APROCSA-based apha-\nsic features (see Section B.5).\nB.2\nSubtest 2: Word Comprehension\nObjective: Evaluates lexical-semantic processing\nand selection among competing meanings.\nInstructions: Elicit responses verbatim. Pro-\nvide the options alongside each question. Require\na one-token answer corresponding exactly to one\nof the listed options; do not include explanations.\n1. “Which one is an animal with a mane? (lion,\ndrum, violin, giraffe, boot, boat)”\n2. “Which object is typically used to make mu-\nsic? (violin, giraffe, lion, door, boot, boat)”\n3. “Which item is usually worn on the feet?\n(boot, boat, lion, drum, violin, giraffe)”\n4. “Which object is used for cutting? (knife, kite,\nlion, drum, violin, giraffe)”\n5. “Which one is a large mammal with a long\nneck?\n(giraffe, horse, lion, drum, violin,\nboot)”\nScoring:\nResponses should be binary (cor-\nrect/incorrect), with incorrect selections suggesting\nsemantic processing deficits.\nB.3\nSubtest 3: Sentence Comprehension\nObjective: Evaluates syntactic processing, passive\nstructures, and logical comprehension.\nInstructions: Elicit responses verbatim. Re-\nquire a single-token “Yes” or “No” answer without\nadditional text.\n1. “Are babies watched by babysitters?” (Ex-\npected: Yes)\n2. “Do you cut the grass with an axe?” (Expected:\nNo)\n9\n"}, {"page": 10, "text": "3. “If you’re about to leave, have you left yet?”\n(Expected: No)\n4. “Are witnesses questioned by police?” (Ex-\npected: Yes)\n5. “If I was at the park when you arrived, did I\nget there first?” (Expected: Yes)\nScoring: Delayed or incorrect responses may\nsuggest syntactic processing difficulties, impaired\nnegation handling, or confusion with passive con-\nstructions.\nB.4\nSubtest 4: Repetition\nObjective: Evaluates lexical access, phonological\nencoding, and morphosyntax.\nInstructions: Elicit responses verbatim. Inform\nthe system that punctuation and casing matter.\n1. “Please repeat exactly: house.”\n2. “Please repeat exactly: breakfast.”\n3. “Please repeat exactly: catastrophe.”\n4. “Please repeat exactly: The sun rises in the\nEast.”\n5. “Please repeat exactly: The ambitious journal-\nist discovered where we’d be going.”\nScoring: Check for exact reproduction under\nstrict normalization: case-sensitive, punctuation-\nsensitive, and whitespace-insensitive (trim lead-\ning/trailing spaces only).\nErrors may include\nphonemic substitutions, deletions, or distortions\n(e.g., “catastrophe” →“catastroph” or “catastroph-\nically”).\nB.5\nAPROCSA Feature Set for Connected\nText Evaluation\nBinary Scoring: Pass/Fail.\nConnected Text Scoring: For each response,\nidentify the presence or absence of the features\nbelow, adapted from the APROCSA (Auditory-\nPerceptual Rating of Connected Speech in Apha-\nsia) framework (Casilio et al., 2019), using 1 or 0\nrespectively. APROCSA assesses 27 features of\nconnected speech on a five-point scale; TAB uses\n19 features applicable to text-only evaluation with\nbinary scoring.\nEvaluated Features:\n• Anomia\n• Abandoned utterances\n• Empty speech\n• Semantic paraphasias\n• Phonemic\nparaphasias\n(evaluated\nat\nto-\nken/orthographic level for text-only settings)\n• Neologisms\n• Jargon\n• Perseverations\n• Stereotypies and automatisms\n• Short and simplified utterances\n• Omission of bound morphemes\n• Omission of function words\n• Paragrammatism\n• Retracing\n• False starts\n• Conduite d’approche\n• Meaning unclear\n• Off-topic\n• Overall communication impairment\nOmitted Features (not applicable to text-only\nevaluation):\n• Pauses between utterances\n• Pauses within utterances\n• Halting and effortful\n• Reduced speech rate\n• Expressive aphasia\n• Apraxia of speech\n• Dysarthria\n• Target unclear\n10\n"}, {"page": 11, "text": "B.6\nOperational Clarifications for Text-Only\nEvaluation\nPhonemic paraphasias (text-only):\nMark as\npresent when orthographic outputs plausibly reflect\nsound-level errors (substitution, insertion, deletion,\nor transposition) relative to an intended target (e.g.,\n“coffah” for “coffee”). Exclude typographical er-\nrors that do not reflect plausible phonology unless\nthey pattern with other phonemic errors.\nOverall communication impairment: Rate in-\ndependently based on the overall intelligibility and\ncommunicative adequacy of the passage; do not\nderive mechanically from other feature flags.\nOne-token outputs for Subtests 2–3: Enforce\nsingle-token answers (an item from the provided\nlist; “Yes”/“No”) with no additional text to avoid\nconfounds from explanation length or hedging.\nRepetition normalization policy:\nEvaluate\nexact reproduction using case- and punctuation-\nsensitive matching while trimming leading/trailing\nwhitespace; report error type as insertion, deletion,\nor substitution.\nC\nFeature-Level Inter-Rater Reliability\nResults\nD\nAutomated Evaluation Prompt for\nConnected Text\nThis section provides the complete prompt for auto-\nmated identification of aphasic features in Con-\nnected Text responses.\nFeature definitions are\nadapted from the APROCSA (Auditory-Perceptual\nRating of Connected Speech in Aphasia) frame-\nwork (Casilio et al., 2019).\nD.1\nSystem Prompt\nYou are an automated system for analyzing tran-\nscripts of speech for aphasic features. Your task is\nto read a transcript passage and determine whether\neach of the following features is present (1) or not\npresent (0). Your output must be a JSON file with\neach feature as a key and its value as either 0 or 1.\nD.2\nFeature Definitions and Examples\nAnomia:\nOverall impression of word-finding dif-\nficulties.\nExample: “I can’t find the word for that.”\nAbandoned utterances:\nUtterances left incom-\nplete before the speaker moves on.\nExample: “I was going to say that the How are\nyou doing today?”\nEmpty speech:\nSpeech that conveys little or no\nmeaning, using nonspecific words.\nExample: “You know, stuff and things.”\nSemantic paraphasias:\nSubstitution of one con-\ntent word for another (related or unrelated).\nExample: “I used a fork to eat my soup.”\nPhonemic paraphasias:\nErrors in sound produc-\ntion (substitution, insertion, deletion, or transposi-\ntion).\nExample: “I need coffah in the morning.”\nNeologisms:\nInvented words that are not real.\nExample: “I need to buy a blorf from the store.”\nJargon:\nFluent but largely meaningless speech\ncomposed of unintelligible strings.\nExample: “Flimby gorp snizz and blah.”\nPerseverations:\nRepeating a word or phrase in-\nappropriately.\nExample: “The ball, the ball, the ball”\nStereotypies and automatisms:\nOverlearned, re-\npeated words or phrases produced with ease.\nExample: “Dammit, dammit, dammit.”\nShort and simplified utterances:\nUtterances\nthat are unusually brief or lack expected complex-\nity.\nExample: “I go store. I buy carrot.”\nOmission of bound morphemes:\nLeaving out\ninflectional or derivational morphemes.\nExample: “I go to store”.\nOmission of function words:\nMissing small\nwords such as articles or prepositions.\nExample: “I going store.”\nParagrammatism:\nInappropriate juxtaposition\nof words or misuse of grammar.\nExample: “It’s so much wonderful, makes it hard\nto speech.”\nFalse starts:\nAbandoned beginnings of words.\nExample: “I want to have a ca cat”\nRetracing:\nRedundant repetition or revision of a\nsequence of words.\nExample: “The kite is the kite is flying.”\nConduite d’approche:\nSuccessive approxima-\ntions toward a target word, with corrections.\nExample: “I want a pa pen, I mean, pencil.”\n11\n"}, {"page": 12, "text": "Meaning unclear:\nThe overall message is vague\nor incomprehensible.\nExample: “That thing was just. . . not right.”\nOff-topic:\nUtterances that do not relate to the\ngiven context.\nExample: “I like ice cream.” (when discussing a\ndifferent topic)\nD.3\nEvaluation Examples\nExample 1:\nTranscript Passage:\n“I was trying to tell you about my day but I just\nI mean I wanted to say something about the store I\ngo store I wanted a pen I mean pencil The ball the\nball the ball kept bouncing and I just stopped you\nknow I keep saying dammit dammit dammit all the\ntime”\nAssociated JSON Output:\n{\n\"Anomia\": 1,\n\"Abandoned utterances\": 1,\n\"Empty speech\": 0,\n\"Semantic paraphasias\": 0,\n\"Phonemic paraphasias\": 0,\n\"Neologisms\": 0,\n\"Jargon\": 0,\n\"Perseverations\": 1,\n\"Stereotypies and automatisms\": 1,\n\"Short and simplified utterances\": 1,\n\"Omission of bound morphemes\": 1,\n\"Omission of function words\": 1,\n\"Paragrammatism\": 0,\n\"False starts\": 0,\n\"Retracing\": 0,\n\"Conduite d'approche\": 1,\n\"Meaning unclear\": 0,\n\"Off-topic\": 0,\n\"Overall communication impairment\": 1\n}\nExample 2:\nTranscript Passage:\n“I want to go to the store to buy a blorf You know\nI keep trying to say it but I say I want to go to the\nst store I want a pa pen I mean pencil I dont know\nwhat im trying to say It all seems not right”\nAssociated JSON Output:\n{\n\"Anomia\": 1,\n\"Abandoned utterances\": 0,\n\"Empty speech\": 0,\n\"Semantic paraphasias\": 0,\n\"Phonemic paraphasias\": 0,\n\"Neologisms\": 1,\n\"Jargon\": 0,\n\"Perseverations\": 0,\n\"Stereotypies and automatisms\": 0,\n\"Short and simplified utterances\": 0,\n\"Omission of bound morphemes\": 0,\n\"Omission of function words\": 0,\n\"Paragrammatism\": 0,\n\"False starts\": 1,\n\"Retracing\": 0,\n\"Conduite d'approche\": 1,\n\"Meaning unclear\": 1,\n\"Off-topic\": 0,\n\"Overall communication impairment\": 1\n}\nD.4\nInstructions Recap\n1. Read the given transcript passage.\n2. For each of the features listed above, de-\ncide whether the feature is present (1) or not\npresent (0) in the transcript.\n3. Output your result as a JSON file with exactly\nthe keys provided (each key must appear) and\nwith values of either 0 or 1.\n4. Your analysis should strictly follow the defini-\ntions and examples provided. Do not include\nany additional keys or extraneous information\nin your JSON output.\nYour output must include every one of the above\nfeatures as a key in the JSON. For each key, assign\n1 if the feature is present in the transcript, or 0 if it\nis not.\n12\n"}]}