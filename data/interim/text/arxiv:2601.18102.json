{"doc_id": "arxiv:2601.18102", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.18102.pdf", "meta": {"doc_id": "arxiv:2601.18102", "source": "arxiv", "arxiv_id": "2601.18102", "title": "CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations", "authors": ["Stephanie Fong", "Zimu Wang", "Guilherme C. Oliveira", "Xiangyu Zhao", "Yiwen Jiang", "Jiahe Liu", "Beau-Luke Colton", "Scott Woods", "Martha E. Shenton", "Barnaby Nelson", "Zongyuan Ge", "Dominic Dwyer"], "published": "2026-01-26T03:25:06Z", "updated": "2026-01-26T03:25:06Z", "summary": "The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.18102v1", "url_pdf": "https://arxiv.org/pdf/2601.18102.pdf", "meta_path": "data/raw/arxiv/meta/2601.18102.json", "sha256": "69cb92af9a2c05f24f6b2fccdbed5521931e48714ccc1d6e486a69c3255d6bb8", "status": "ok", "fetched_at": "2026-02-18T02:20:34.031106+00:00"}, "pages": [{"page": 1, "text": "CHiRPE: A Step Towards Real-World Clinical NLP with\nClinician-Oriented Model Explanations\nStephanie Fong1,2, Zimu Wang2,3, Guilherme C. Oliveira2, Xiangyu Zhao2, Yiwen Jiang2,\nJiahe Liu2, Beau-Luke Colton1, Scott Woods4, Martha E. Shenton5, Barnaby Nelson1,\nZongyuan Ge2, Dominic Dwyer1,2,*\n1Orygen and The University of Melbourne\n2AIM for Health Lab, Monash University\n3University of Liverpool\n4Yale School of Medicine, Yale University\n5Brigham and Women’s Hospital, Harvard Medical School\nstephanie.fong@unimelb.edu.au, dominic.dwyer@orygen.org.au\nAbstract\nThe medical adoption of NLP tools requires\ninterpretability by end users, yet traditional\nexplainable AI (XAI) methods are misaligned\nwith clinical reasoning and lack clinician input.\nWe introduce CHiRPE (Clinical High-Risk Pre-\ndiction with Explainability), an NLP pipeline\nthat takes transcribed semi-structured clinical\ninterviews to: (i) predict psychosis risk; and\n(ii) generate novel SHAP explanation formats\nco-developed with clinicians. Trained on 944\nsemi-structured interview transcripts across 24\ninternational clinics of the AMP-SCZ study, the\nCHiRPE pipeline integrates symptom-domain\nmapping, LLM summarisation, and BERT clas-\nsification. CHiRPE achieved over 90% accu-\nracy across three BERT variants and outper-\nformed baseline models. Explanation formats\nwere evaluated by 28 clinical experts who indi-\ncated a strong preference for our novel concept-\nguided explanations, especially hybrid graph-\nand-text summary formats. CHiRPE demon-\nstrates that clinically-guided model develop-\nment produces both accurate and interpretable\nresults. Our next step is focused on real-world\ntesting across our 24 international sites.\n1\nIntroduction\nMental illness is a major contributor to the global\nhealth burden (McGorry et al., 2025), with psy-\nchotic disorders posing particular concern due to\ntheir relatively high mortality risk (Walker et al.,\n2015). Psychosis is often preceded by a prodromal\nClinical High-Risk for Psychosis (CHR-P) phase\n(Wang et al., 2022), during which early detection\ncan enable interventions that may delay or prevent\nprogression, improving long-term outcomes.\nThe PSYCHS (Positive SYmptoms and Diagnos-\ntic Criteria for the CAARMS-Harmonized SIPS) is\n*Corresponding author.\na clinician-administered, semi-structured interview\n(Woods et al., 2024) for detecting CHR-P. However,\nits routine use is constrained by substantial time\nand administrative effort (Oliver et al., 2022). To\naddress these limitations, recent advances in NLP\noffer a viable path forward. Automated analysis\nof interview transcripts can support scalable and\nconsistent screening of disorder markers, thereby\nalleviating the reliance on labour-intensive manual\nassessment (Molina et al., 2025; Na et al., 2025).\nHowever, their “black-box” nature significantly im-\npedes adoption in clinical contexts where trans-\nparency and interpretability are essential (Topol,\n2019; Cinà et al., 2022).\nPost-hoc explainable AI (XAI) techniques, such\nas SHapley Additive exPlanations (SHAP; Lund-\nberg and Lee, 2017), have been proposed to bridge\nthis gap. In NLP, SHAP assigns each input feature\n(e.g. word or token) a value representing its aver-\nage influence on the model’s prediction (Shapley\net al., 1953). While these attributions capture the\nmodel’s logic, they fundamentally misalign with\nreal-world clinician reasoning (Lawrence et al.,\n2024). In parallel, large language models (LLMs)\nremain opaque (Lawrence et al., 2024) and sensi-\ntive to subtle prompts or context changes (Peng\net al., 2023; Zhou et al., 2023; Chatterjee et al.,\n2024). Despite the importance of interpretability\nfor clinical trust (Di Martino and Delmastro, 2023;\nTalebi et al., 2024), clinician involvement in XAI\ndesign remains limited (Ghassemi et al., 2021).\nMotivated by this gap, we introduce CHiRPE\n(Clinical High-Risk Prediction with Explainabil-\nity), a human-centred NLP framework that pro-\ncesses transcribed PSYCHS interviews to classify\nindividuals as CHR-P and generates five SHAP ex-\nplanation formats, three of which are co-developed\nwith clinicians (see Figure 1). Trained on 943 tran-\narXiv:2601.18102v1  [cs.CL]  26 Jan 2026\n"}, {"page": 2, "text": "Have you ever had the feeling that something\nodd is going on or that something is wrong?\nYes, its ju-just like, gut feelings or hunches, it\nhappens a lot, but nothing too intense ... \nHave you felt that you have gifts or special\npowers to do things that other people can’t do?\nNot really. I might be good at some things\ncompared to others, but I do not feel like ...\nHave you ever felt your eyes are playing tricks \non you?\nYeah, like shadows or shapes move when they\nshould not. I usually tell myself it is just ...\nUnusual Thoughts and Experiences\nGrandiosity\nVisual Perceptual Abnormalities\nUnusual Thoughts and Experiences\nGrandiosity\nVisual Perceptual Abnormalities\nInput: PSYCHS Transcripts \nSymptom Segmentation + \nLLM Summarisation\nBERT-based Classification\nSHAP Explanation Formats \nWord-level plots\nToken-level heatmaps\nSymptom-level bar plots\nSentence-level summary\nNarrative summary\nOutput: Label\nMajority Voting\nStandard SHAP outputs\nClinically-aligned SHAP outputs\nThe interviewee reported recurrent, mild\nsensations that something was wrong ...\nThe interviewee denied special powers and\nviewed their abilities as normal ...\nThe interviewee described brief visual\nmisperceptions in their peripheral vision...\nClinical High Risk\nHealthy Control\nFigure 1: CHiRPE pipeline. Raw PSYCHS transcripts are segmented by symptom domain and summarised, then\npassed to a BERT-based classifier. The system outputs a CHR-P or Healthy label alongside SHAP explanations.\nscripts across 24 international sites, the pipeline\nintegrates symptom domain mapping, LLM sum-\nmarisation, BERT-based classification, and SHAP-\nbased explanations. Our key contributions are as\nfollows: (i) an integrated pipeline CHR prediction\nthat aligns model inputs with clinically meaning-\nful constructs while preserving high classification\naccuracy; (ii) co-designed SHAP explanation for-\nmats grounded in how clinicians reason about psy-\nchosis risk; (iii) empirical clinician expert evalua-\ntion of these co-designed SHAP formats, showing\nthat they outperform standard explanations in inter-\npretability, clinical reasoning, and alignment.\n2\nRelated Work\nA systematic review of nine studies using shallow\nclassifiers with handcrafted features or static em-\nbeddings reported CHR-P diagnostic accuracies\nbetween 56-95% (Molina et al., 2025), but most\nrelied on small, single-site samples and internal val-\nidation, raising concerns about overfitting and gen-\neralisability. Recent AMP-SCZ work using Llama\n3 “normality” features and Naïve Bayes achieved\nmoderate accuracy (AUC ≈0.68) on a smaller sub-\nset of the same PSYCHS interview data (Bilgrami\net al., 2025), yet lacked interpretability.\nExisting results demonstrate that the shift to\ndeep learning enables richer linguistic modelling\nbut introduces opacity, necessitating interpretabil-\nity methods. While SHAP formats have shown\npromise in broader medical applications such as\ncerebral infarction (Nohara et al., 2022) and tho-\n0.010\n0.011\n0.012\n0.013\n0.014\nhead\nfeeling\ncontinued\nauditory\nmates\nprocessing\nsensation\nSHAP value\nFigure 2: SHAP word-level bar plot showing the top\ncontributing words for a CHR-P classification decision.\nracic surgery (Hur et al., 2025), mental health NLP\nstudies remain limited (Baki et al., 2022; Nie and\nWu, 2025). Standard SHAP word-level bar plots\n(Figure 2) and token-level heatmaps (Figure 4) ei-\nther present decontextualised words in isolation or\nspan entire transcripts, which are both misaligned\nwith the types of concept-driven summaries used\nin clinical reasoning to make decisions. To our\nknowledge, no mental health NLP studies have\nexamined adapted SHAP formats that align with\nclinical reasoning, and it remains a critical gap that\nis preventing the clinical use of NLP tools.\n3\nCHiRPE\nThe CHiRPE pipeline is presented in Figure 1. It\nconsists of the following: a) concept mapping to\ngenerate transcript segments; b) summarisation of\nconcept segments; c) CHR detection classifier train-\ning; d) application of clinically-informed SHAP\napproaches. In this section, we introduce each step\n"}, {"page": 3, "text": "in detail.\n3.1\nSymptom Domain Segmentation\nThe PSYCHS interview follows a fixed order of\nstandardised questions organised into 15 attenuated\npositive symptom domains (see Appendix A). Inter-\nviewer utterances were mapped to domains using\nfuzzy string matching1 with a threshold of 80%\n(justified in Appendix B).\n3.2\nSummarisation of Interview Segments\nEach interview segment was rephrased into third-\nperson using a two-step process with Mistral-7B-\nInstruct-v0.32 to better match BERT pretraining\ndata. The process involved an initial clinician style\nrewrite followed by refinement for completeness\nand coherence (see Appendix C for prompts and\nAppendix F for prompt sensitivity analysis).\n3.3\nClassification Models\nWe fine-tuned BERT (Devlin et al., 2019), Clinical-\nBERT (Alsentzer et al., 2019), and MentalBERT\n(Ji et al., 2021), domain-specific models for clinical\nand mental health language, to classify each sum-\nmary symptom segment as CHR-P or control. Seg-\nments exceeding 512 tokens were split into fixed-\nlength chunks, and transcript-level labels were de-\ntermined by majority voting across segments.\n3.4\nExplanation Generation with SHAP\nAs baselines, the standard SHAP visualisations\nof word-level bar plots for top impact words and\ncolour-coded heatmaps were generated, as shown\nin Figures 2 and 4. In consultation with clinicians,\nwe introduced new presentation formats, including\nsentence-level summaries, narrative explanations,\nand symptom-level plots.\nSentence-level Summary.\nFor individuals classi-\nfied as CHR, the sentence with the highest average\nnet SHAP contribution toward a CHR prediction\nwas extracted as a Sentence-level Summary. For\nexample,\nP4 Ideas of Guilt\nThey admit to constantly dwelling on past prob-\nlems, which significantly impact their daily\nlife.\n1https://pypi.org/project/fuzzywuzzy/\n2https://huggingface.co/mistralai/\nMistral-7B-Instruct-v0.3\nNarrative Summary.\nA concise Narrative Sum-\nmary was generated using Qwen3-4B (Yang et al.,\n2025) for its strong contextual generation ability.\nEach summary included ±1 sentence of context\nand a representative interviewee quote. The prompt\nis shown in Appendix E. An example summary is\nas follows:\nP4 Ideas of Guilt\nThe interviewee reports persistent and intru-\nsive thoughts centered on moral concerns and\nothers’ perspectives, particularly during emo-\ntional distress. They describe frequent rumina-\ntion on past issues, which interferes with daily\nfunctioning, and recurrent episodes of intense,\nunexplained guilt occurring at least weekly.\n“Even if it’s not actively thinking about the prob-\nlem, like, ‘Oh, I wish I’d done something dif-\nferent,’ it affects little things in life that make\nyou think about it.”\nSymptom-Level Plots.\nSHAP values were aggre-\ngated into mean net scores per symptom domain\nand visualised as horizontal bar charts (Figure 3),\nshowing each domain’s influence on CHR-P or con-\ntrol prediction.\n0.000\n-0.002\n-0.004\n0.000\n0.002\n0.004\n0.006\nDisorganised Communication\nSuspiciousness\nIdeas of Guilt\nUnusual Thoughts and Experiences\nUsual Somatic Ideas\nMean net SHAP per token\n0.000\n-0.002\n-0.004\n0.000\n0.002\n0.004\n0.006\nDisorganised Communication\nSuspiciousness\nIdeas of Guilt\nUnusual Thoughts and Experiences\nUsual Somatic Ideas\nMean net SHAP per token\nFigure 3: Symptom-level plots (red: CHR-P, blue:\nhealthy control).\n4\nData Analysis Methods\n4.1\nPerformance Metrics\nAccuracy, precision, recall, F1, and AUC were eval-\nuated at the transcript level on the test set.\n4.2\nClinical Expert Feedback\nClinical experts completed a mixed-method ques-\ntionnaire on SHAP explanation preferences, in-\ncluding: a) word-level bar plots; b) colour-coded\nheatmaps; c) symptom-level bar plots; d) sentence-\nlevel summary; and e) narrative summaries (full\nquestionnaire in Appendix H). Descriptive and\n"}, {"page": 4, "text": "Model\nBaseline\nSummary Only\nSegmentation Only\nProposed (Summ + Seg)\nAcc\nF1\nPrec\nRec\nAcc\nF1\nPrec\nRec\nAcc\nF1\nPrec\nRec\nAcc\nF1\nPrec\nRec\nBERT\n83.91 89.78 83.11 97.62 81.03 88.09 80.79 96.83 91.81 95.10 93.15 97.14 91.23 94.50 96.99 92.14\nClinicalBERT 81.03 88.00 81.21 96.03 82.76 88.97 82.88 96.03 87.51 92.56 91.97 93.21 90.64 94.20 95.59 92.86\nMentalBERT\n83.91 89.85 82.67 98.41 84.48 90.04 84.14 96.83 91.23 94.77 92.52 97.14 91.23 94.85 91.39 98.57\nTable 1: Performance metrics of BERT-based models across ablation settings. Best results for each metric within\neach model are highlighted in bold.\ninferential statistics were used to compare inter-\npretability ratings across formats.\n5\nData and Experimental Setup\n5.1\nDataset\nParticipants were drawn from the Accelerating\nMedicines Partnership Schizophrenia3 (AMP-SCZ)\nstudy, focusing on CHR-P identification and transi-\ntion to psychosis (Wannan et al., 2024). The dataset\ncomprised 943 English PSYCHS transcripts from\n581 unique participants aged 12-30 (M = 20.9, SD\n= 4.1), 63.3% of whom were female. Labels were\nassigned by trained research assistants, with 83.6%\nCHR-P and 16.4% Healthy Controls across 24 in-\nternational sites of the AMP-SCZ study.\nThe dataset was partitioned into 64% for train-\ning, 16% for development (via nested 5-fold cross-\nvalidation on an 80% subset), and 20% for held-\nout testing. Splits were stratified by CHR status\nand grouped by participant ID to prevent data leak-\nage. A fixed random seed ensured consistent splits\nacross model variants.\n5.2\nPre-processing\nAudio recordings were human transcribed4, with all\ntimestamps and personally identifiable information\nremoved. A few-shot XLM-RoBERTa classifier\n(Conneau et al., 2020) distinguished interviewer\nand interviewee turns.\n5.3\nModels and Hyperparameters\nBERT-based models were trained on a single A100\nGPU within a high-performance computing clus-\nter. All classification models were fine-tuned using\nweighted cross-entropy loss to address class im-\nbalance. Hyperparameters, including learning rate,\nbatch size, weight decay, and number of epochs,\nwere optimised via inner-loop grid search (details\nin Appendix D).\n3https://www.ampscz.org/\n4https://www.transcribeme.com/\n6\nResults\n6.1\nModel Classification Performance\nAll three transformer-based models performed\nstrongly on the held-out test set (Table 1; optimal\nhyperparameters in Appendix D), outperforming\nbaselines without segmentation and summarisation.\nAUCs were 0.95 for BERT and 0.97 for both Clin-\nicalBERT and MentalBERT, exceeding baseline\nAUCs of 0.94, 0.90, and 0.95, respectively.\n6.2\nAblation Analysis\n6.2.1\nSegmentation and Summarisation\nAblation results show that symptom domain seg-\nmentation is the main driver of performance gains,\nwhile summarisation provides smaller but consis-\ntent improvements (see Table 1). In particular, seg-\nmentation reduces noise from long, heterogeneous\ntranscripts and focuses the classifier on clinically\nmeaningful information. By contrast, summarisa-\ntion maintains comparable accuracy despite sub-\nstantially reducing input length, improving mod-\nelling efficiency.\nIn the CHiRPE pipeline, segmentation and sum-\nmarisation together achieve the best overall trade-\noff between F1, precision, and efficiency. The two\ncomponents appear complementary: segmentation\nimposes clinically meaningful structure, while sum-\nmarisation standardises and condenses content for\nSHAP attribution and downstream narrative gen-\neration, yielding both improved predictive perfor-\nmance and a more scalable explanation workflow.\n6.2.2\nPrecision–Recall Trade off\nAs shown in Table 2, while the proposed setting\n(summarisation + segmentation) improves F1 and\nprecision, it yields a modest drop (-4%) in recall\nand is accompanied by a substantial gain (+41%)\nin specificity. From a modelling perspective, this\npattern suggests reduced bias from class imbalance\n(CHR = 83.6%, HC = 16.4%) and less overfitting\nto CHR cases. Clinically, given that CHR reflects\nrisk rather than diagnosis, and that false positives\n"}, {"page": 5, "text": "carry nontrivial clinical costs (De Pablo et al., 2021;\nDi Lisi et al., 2022), this shift indicates a more con-\nservative and better calibrated decision boundary.\nRecall (CHR)\nSpecificity (HC)\nBaseline\n0.9603\n0.3958\nProposed (Summ + Seg)\n0.9286\n0.8065\nTable 2: Recall–specificity trade off for ClinicalBERT\nunder ablation settings.\n6.3\nClinical Expert Evaluation of Explanation\nFormats\nTwenty-eight clinical experts, including psychia-\ntrists, clinical psychologists, and trained research\nassistants, completed a 10-minute questionnaire\nevaluating the interpretability of SHAP outputs on\na 5-point Likert scale (Table 3). Responses were\nanonymous, and participants gave informed con-\nsent for data usage (Appendix H).\nExplanation Format\nInterpretability\nBaseline\nWord-level plots\n2.18\nToken-level heatmaps\n2.50\nProposed\nSymptom bar plots\n3.71\nSentence-level summary\n4.25\nSingle narrative\n4.08\nGraph + single narrative\n4.32\nMultiple narratives\n4.21\nTable 3: Mean interpretability ratings for baseline and\nproposed explanation formats (n = 28).\n6.3.1\nQuantitative Clinical Expert Feedback\nAll novel SHAP formats outperformed the tradi-\ntional SHAP word-level bar plots and heatmaps.\nThe combination of symptom-level plots with\nnarrative summaries received the highest inter-\npretability ratings. A repeated-measures ANOVA\nconfirmed significant differences across formats,\nF(6, 28) = 26.485, p < .001. Full ANOVA re-\nsults are presented in Appendix I.\nAdditional ratings on Clinical Reasoning and\nClinical Intuition Alignment showed the same pat-\ntern of results, with proposed formats consistently\noutperforming baseline visualisations. Full quanti-\ntative results are reported in Appendix I.\n6.3.2\nQualitative Clinical Expert Feedback\nQualitative coding of 20 clinical expert comments\nidentified eight themes, most commonly favouring\nhybrid graph-text formats and concise presenta-\ntions. Representative quotes are provided in Ap-\npendix I.\nTheme\nn\n%\nGraph + text summaries preferred\n6\n30\nConcise formats (dot points, fragments)\n4\n20\nStructured symptom tables needed\n3\n15\nClear definition of experiences\n2\n10\nClarify model inputs and outputs\n2\n10\nOverview across multiple symptoms\n1\n5\nAdd risk severity scale for decisions\n1\n5\nData access and governance issues\n1\n5\nTable 4: Themes from qualitative feedback.\n7\nConclusion and Future Work\nIn conclusion, CHiRPE outperformed baseline\nmodels in classification and surpassed existing\nSHAP approaches in interpretability. The prefer-\nence for hybrid graph–text summaries and concise\ntext formats highlighted CHiRPE’s practical con-\ntributions to future explainable clinical AI designs.\nCHiRPE addresses a key gap in mental health NLP\nin light of evolving AI regulations related to inter-\npretability (e.g., the EU AI Act).\nWhile recent AMP-SCZ work using Llama 3 fea-\ntures achieved moderate accuracy (AUC ≈0.68)\ncompared to CHiRPE’s higher performance (AUC\n> 0.95) (Bilgrami et al., 2025), differing methods\nand limited reporting in evaluation metrics preclude\ndirect comparison. Prior work has also shown per-\nformance gains from domain-specific BERT mod-\nels in mental health tasks (Ji et al., 2021; Turchin\net al., 2023), but CHiRPE achieved comparable\naccuracy without task-specific pretraining. If repli-\ncated, this observation potentially suggests a need\nto shift focus from continual fine-tuning to aligning\nmodels with clinical reasoning, particularly in areas\nwhere diagnoses rely on subjective narratives rather\nthan objective biomarkers (Oliver et al., 2022).\nFuture work will focus on refining CHiRPE’s\ntext-based summaries and symptom-level visualisa-\ntions and piloting an interface prototype (Figure 5\nin Appendix J) across our network of 24 interna-\ntional sites. We will also incorporate patient per-\nspectives to enhance concept mappings and extend\nthe pipeline to additional mental health conditions,\nsuch as depression, mania, and anorexia.\nLimitations\nThis study was limited to English transcripts col-\nlected from the 24 international sites. Future work\n"}, {"page": 6, "text": "could extend concept mapping and model evalu-\nation to multilingual data for broader applicabil-\nity. Additional recruitment of participants in future\nstudies could also strengthen the robustness and\ngeneralisability of findings. It is also worth explor-\ning alternative models that are not BERT-based for\nthis task.\nEthical Considerations\nThe transcript data used in this study were ob-\ntained from the Accelerating Medicines Partner-\nship Schizophrenia (AMP-SCZ) consortium under\na data use agreement. All data were de-identified\nand collected with informed consent for research\npurposes. Recruitment and consent procedures\nwere IRB-approved (Accelerating Medicines Part-\nnership, 2023). Participants will be reimbursed\nfor their time and expenses associated with com-\npleting the research assessments. The amount of\nreimbursement varies by data collection site and\nthe extent of study completion.\nWe used the pre-trained BERT model released\nby Google under the Apache License 2.05 (Devlin\net al., 2019), ClinicalBERT under the MIT license6\n(Alsentzer et al., 2019), and MentalBERT under\nthe Creative Commons Attribution Non Commer-\ncial 4.0 license7 (Ji et al., 2021). All models were\nfine-tuned on AMP-SCZ data solely for research\npurposes, in accordance with their respective li-\ncenses.\nThe development of explainable AI tools for sen-\nsitive mental health settings raises important eth-\nical and privacy challenges, particularly around\ninformed consent. In clinical contexts where am-\nbient data capture and AI-driven interpretation\nare involved, ethical safeguards cannot be as-\nsumed—they must be explicitly designed, tested,\nand adapted to the needs of each case.\nAs\npart of our design process, we are co-developing\nCHiRPE’s consent procedures with both patients\nand clinicians to ensure transparency, agency, and\nappropriateness. This form of human-centred eth-\nical design is expected to further enhance model\nperformance, explanatory capacity, and ultimately\nthe trust required for CHiRPE to contribute to the\nlong-term sustainability of AI technologies in men-\ntal health care.\n5https://www.apache.org/licenses/LICENSE-2.0\n6https://tlo.mit.edu/understand-ip/\nexploring-mit-open-source-license-comprehensive-guide\n7https://creativecommons.org/licenses/by-nc/4.\n0/deed.en\nData and Code Availability\nThe AMP-SCZ dataset used in this study is\navailable to researchers through the National\nData\nArchive\nof\nthe\nNational\nInstitute\nof\nHealth in the USA: https://www.ampscz.org/\nscientists/data/.\nAll code used for data\npreprocessing, model training, and explanation\ngeneration is available at https://github.com/\nstephaniesyfong/CHiRPE.\nUse of AI\nChatGPT and GitHub Copilot were used to assist\nwith code debugging and language editing. All\noutputs were manually reviewed and verified by\nthe authors.\nAcknowledgements\nThe\nAccelerating\nMedicines\nPartnership\nSchizophrenia (AMP-SCZ) is a public-private\npartnership managed by the Foundation for the Na-\ntional Institutes of Health. The AMP-SCZ research\nprogram is supported by contributions from the\nAMP-SCZ public and private partners, including\nNIMH (U24MH124629, U01MH124631, and\nU01MH124639) and Wellcome (220664/Z/20/Z\nand 220664/A/20/Z). The views expressed in\nthis article are personal views of the authors\nand may not be understood or quoted as being\nmade on behalf of or reflecting the position of\nthe Department of Health and Human Services,\nincluding the National Institutes of Health and the\nUS Food and Drug Administration, the United\nStates Government, or the European Medicines\nAgency or one of its committees or working parties.\nThe research was supported by the University of\nMelbourne’s Research Computing Services and\nthe Petascale Campus Initiative. The authors report\nno conflict of interest. Dwyer was supported by\nan NHMRC Emerging Leadership II Investigator\nGrant (#2034943).\nReferences\nAccelerating Medicines Partnership. 2023.\nAc-\ncelerating\nmedicines\npartnership®\nschizophre-\nnia\nobservational\nstudy\nprotocol.\nhttps:\n//cdn.clinicaltrials.gov/large-docs/\n03/NCT05905003/Prot_000.pdf.\nAccessed May\n18, 2025.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\n"}, {"page": 7, "text": "McDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nPınar Baki, Heysem Kaya, Elvan Çiftçi, Hüseyin Güleç,\nand Albert Ali Salah. 2022. A multimodal approach\nfor mania level prediction in bipolar disorder. IEEE\nTransactions on Affective Computing, 13(4):2119–\n2131.\nZarina Bilgrami, Cheryl Corcoran, Guillermo Cecchi,\nand Phillip Wolff. 2025. Leveraging ai and language\nanalysis to predict psychosis risk in clinical high-risk\nindividuals. Biological Psychiatry, 97(9):S68–S69.\nAnwoy Chatterjee, HSVNS Kowndinya Renduchintala,\nSumit Bhatia, and Tanmoy Chakraborty. 2024. Posix:\nA prompt sensitivity index for large language models.\narXiv preprint arXiv:2410.02185.\nGiovanni Cinà, Tabea Röber, Rob Goedhart, and Ilker\nBirbil. 2022. Why we do need explainable ai for\nhealthcare. arXiv preprint arXiv:2206.15363.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th annual meeting of the associa-\ntion for computational linguistics, pages 8440–8451.\nGonzalo Salazar De Pablo, Joaquim Radua, Joana\nPereira, Ilaria Bonoldi, Vincenzo Arienti, Filippo Be-\nsana, Livia Soardo, Anna Cabras, Lydia Fortea, Ana\nCatalan, and 1 others. 2021. Probability of transition\nto psychosis in individuals at clinical high risk: an\nupdated meta-analysis. JAMA psychiatry, 78(9):970–\n978.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 conference of the\nNorth American chapter of the association for com-\nputational linguistics: human language technologies,\nvolume 1 (long and short papers), pages 4171–4186.\nAlessandro Di Lisi, Simona Pupo, Marco Menchetti,\nand Lorenzo Pelizza. 2022. Antipsychotic treatment\nin people at clinical high risk for psychosis: a narra-\ntive review of suggestions for clinical practice. Jour-\nnal of Clinical Psychopharmacology, pages 10–1097.\nFlavio Di Martino and Franca Delmastro. 2023. Ex-\nplainable ai for clinical and remote health applica-\ntions: a survey on tabular and time series data. Artifi-\ncial Intelligence Review, 56(6):5261–5315.\nMarzyeh Ghassemi, Luke Oakden-Rayner, and An-\ndrew L Beam. 2021.\nThe false hope of current\napproaches to explainable artificial intelligence in\nhealth care. The Lancet Digital Health, 3(11):e745–\ne750.\nSujeong Hur, Yura Lee, Joongheum Park, Yeong Jeong\nJeon, Jong Ho Cho, Duck Cho, Dobin Lim, Wonil\nHwang, Won Chul Cha, and Junsang Yoo. 2025.\nComparison of shap and clinician friendly explana-\ntions reveals effects on clinical decision behaviour.\nnpj Digital Medicine, 8(1):578.\nShaoxiong Ji, Tianlin Zhang, Luna Ansari, Jie Fu,\nPrayag Tiwari, and Erik Cambria. 2021. Mentalbert:\nPublicly available pretrained language models for\nmental healthcare. arXiv preprint arXiv:2110.15621.\nHannah R Lawrence, Renee A Schneider, Susan B\nRubin, Maja J Matari´c, Daniel J McDuff, and\nMegan Jones Bell. 2024. The opportunities and risks\nof large language models in mental health. JMIR\nMental Health, 11(1):e59479.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. Advances\nin neural information processing systems, 30.\nPatrick McGorry, Hasini Gunasiri, Cristina Mei, Simon\nRice, and Caroline X Gao. 2025. The youth mental\nhealth crisis: analysis and solutions. Frontiers in\nPsychiatry, 15:1517533.\nJosé Tomás García Molina, Pablo A Gaspar, and Alicia\nFigueroa-Barra. 2025. Automatic speech recognition\nin psychiatric interviews: a rocket to diagnostic sup-\nport in psychosis. Revista Colombiana de Psiquiatría\n(English ed.), 54(4):624–631.\nHongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei\nYu, Lilin Wang, Wei Wang, John Torous, and Ling\nChen. 2025. A survey of large language models in\npsychotherapy: Current landscape and future direc-\ntions. In Findings of the Association for Computa-\ntional Linguistics: ACL 2025, pages 7362–7376.\nHui Nie and Xiaoyan Wu. 2025.\nA dual-channel\nprediction-interpretation framework with pre-trained\nlanguage models and shap explainability. Journal of\nComputer and Communications, 13(3):116–137.\nYasunobu Nohara, Koutarou Matsumoto, Hidehisa Soe-\njima, and Naoki Nakashima. 2022. Explanation of\nmachine learning models using shapley additive ex-\nplanation and application for real data in hospital.\nComputer Methods and Programs in Biomedicine,\n214:106584.\nDominic Oliver, Maite Arribas, Joaquim Radua, Gon-\nzalo Salazar de Pablo, Andrea De Micheli, Giulia\nSpada, Martina Maria Mensi, Magdalena Kotlicka-\nAntczak, Renato Borgatti, Marco Solmi, and 1 others.\n2022.\nPrognostic accuracy and clinical utility of\npsychometric instruments for individuals at clinical\nhigh-risk of psychosis: a systematic review and meta-\nanalysis. Molecular Psychiatry, 27(9):3670–3678.\nHao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yun-\njia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu,\nLei Hou, and 1 others. 2023. When does in-context\nlearning fall short and why? a study on specification-\nheavy tasks. arXiv preprint arXiv:2311.08993.\n"}, {"page": 8, "text": "Lloyd S Shapley and 1 others. 1953. A value for n-\nperson games.\nSalmonn Talebi, Elizabeth Tong, Anna Li, Ghiam\nYamin, Greg Zaharchuk, and Mohammad RK\nMofrad. 2024. Exploring the performance and ex-\nplainability of fine-tuned bert models for neuroradiol-\nogy protocol assignment. BMC Medical Informatics\nand Decision Making, 24(1):40.\nEric J Topol. 2019. High-performance medicine: the\nconvergence of human and artificial intelligence. Na-\nture medicine, 25(1):44–56.\nAlexander Turchin, Stanislav Masharsky, and Marinka\nZitnik. 2023. Comparison of bert implementations\nfor natural language processing of narrative medi-\ncal documents. Informatics in Medicine Unlocked,\n36:101139.\nElizabeth Reisinger Walker, Robin E McGee, and Ben-\njamin G Druss. 2015. Mortality in mental disorders\nand global disease burden implications: a system-\natic review and meta-analysis.\nJAMA psychiatry,\n72(4):334–341.\nPeng Wang, Chuan-Dong Yan, Xiao-Jie Dong, Lei\nGeng, Chao Xu, Yun Nie, and Sheng Zhang. 2022.\nIdentification and predictive analysis for participants\nat ultra-high risk of psychosis: A comparison of three\npsychometric diagnostic interviews. World journal\nof clinical cases, 10(8):2420.\nCassandra MJ Wannan, Barnaby Nelson, Jean Adding-\nton, Kelly Allott, Alan Anticevic, Celso Arango,\nJustin T Baker, Carrie E Bearden, Tashrif Billah,\nSylvain Bouix, and 1 others. 2024. Accelerating\nmedicines partnership® schizophrenia (amp® scz):\nrationale and study design of the largest global\nprospective cohort study of clinical high risk for psy-\nchosis. Schizophrenia bulletin, 50(3):496–512.\nScott W Woods, Sophie Parker, Melissa J Kerr, Bar-\nbara C Walsh, S Andrea Wijtenburg, Nicholas\nPrunier, Angela R Nunez, Kate Buccilli, Catalina\nMourgues-Codern, Kali Brummitt, and 1 others.\n2024. Development of the psychs: Positive symp-\ntoms and diagnostic criteria for the caarms harmo-\nnized with the sips. Early Intervention in Psychiatry,\n18(4):255–272.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023.\nContext-faithful prompt-\ning for large language models.\narXiv preprint\narXiv:2303.11315.\n"}, {"page": 9, "text": "A\nPSYCHS template questions and symptom-domain matching\nSample list of P1–P3 standard template questions\nP1 Unusual Thoughts and Experiences\n• Have you ever had the feeling that something odd is going on or that something is wrong?\n• Have you ever been confused at times whether something you have experienced is real or imaginary?\n• Have you ever daydreamed a lot or found yourself preoccupied with stories, fantasies, or ideas?\n• Has your experience of time ever seemed to have changed? Has it become unnaturally faster or unnaturally slower?\n• Have you ever seemed to live through events exactly as you have experienced them before?\n• Have familiar people or surroundings ever seemed strange?\n• Have you felt that you or others or the world have changed in some way?\n• Have you ever felt that you might not actually exist? Or that the world might not exist?\n• Have you ever felt you can predict the future?\n• Have you felt that things that were happening around you had a special meaning just for you?\n• Have you ever felt the radio or TV or other electronic devices are communicating directly with you?\n• Do you know what it means to be superstitious? Have you been superstitious?\n• Have you ever felt that some person or force outside yourself has been controlling or interfering with your thoughts,\nfeelings, actions or urges?\n• Have you ever felt that ideas or thoughts that are not your own have been put into your head? Or that your own thoughts\nhave been taken out of your head?\n• Have your thoughts ever been broadcast so that other people know what you are thinking? Or ever said out loud so that\nother people can hear them?\n• Have you ever thought that people might be able to read your mind? Or that you could read other people’s minds?\nP2 Suspiciousness\n• Have you ever felt like people have been talking about you, laughing at you or thinking about you in a negative way?\n• Have you ever found yourself feeling mistrustful or suspicious of other people?\n• Have you ever felt that you have to pay close attention to what’s going on around you in order to feel safe?\n• Have you ever felt like you are being singled out or watched?\n• Has anybody been giving you a hard time or trying to hurt you? Do you have a sense of who that might be? Do you feel\nthey have hostile or negative intentions?\nP3 Unusual Somatic Ideas\n• Have you ever worried that something might be wrong with your body, your health, or a part of your body? Have you\nthought that it seems different to others in some way?\n• Have you worried about your body shape?\n• Have you ever worried that something odd is going on with your body that you can’t explain?\nDomain\nTemplate Question\nMatched Utterance (segment)\nP1\nHave you experienced odd or unusual\nbeliefs that other people find strange?\nInterviewer: I’m wondering if you’ve ever held beliefs that others might\nsee as a bit unusual.\nInterviewee: [silence]\nInterviewer: For instance, do you ever feel like everyday events—say, a\nnews headline—carry a secret message meant just for you?\nInterviewee: Sometimes I feel like casual conversations on TV are\nspeaking directly to me.\nP2\nHave you ever heard or seen things that\nother people couldn’t perceive?\nInterviewer: Do you ever notice sounds or sights that others around you\ndon’t seem to pick up on?\nInterviewee: [silence]\nInterviewer: Like hearing soft voices in an empty room, or glimpsing\nshadows that no one else sees?\nInterviewee: Yes, sometimes I catch voices calling my name in an empty\nroom.\nTable 5: Symptom–question mapping example for PSYCHS domains P1 and P2 with interviewer segments.\n"}, {"page": 10, "text": "B\nSegmentation threshold with Fuzzy\nMatching\nA random subset of fifty transcripts was manually\nsegmented into the 15 PSYCHS symptom domains\nby two human expert raters. The resulting gold-\nstandard segments were compared with automati-\ncally generated ones using fuzzy matching at simi-\nlarity thresholds of 70%, 80%, and 90%. The 80%\nthreshold achieved best performance (see Table\nbelow) and was therefore used in our subsequent\nanalyses.\nThreshold\nPrecision\nRecall\nF1\n70%\n0.628\n0.512\n0.341\n80%\n0.869\n0.881\n0.817\n90%\n0.869\n0.876\n0.817\nTable 6: Averaged Macro Performance at different\nthresholds.\nC\nPrompt for Rephrasing and\nSummarisation of Transcripts\nFirst Pass (Initial Draft Prompt)\nYou are an expert clinical interviewer. Sum-\nmarise the following interview segment in a\nsingle third person paragraph, covering what\nwas asked and the detailed response.\nInterview segment: <segment>\nDraft summary:\nSecond Pass (Refinement Draft Prompt)\nHere is a transcript segment and an initial draft\nsummary. Improve the summary by adding\nany important information from the segment\nthat was missed. Keep third person narration,\none coherent paragraph, and no bullet points.\nInterview segment: <segment>\nDraft summary: <draft>\nImproved summary:\nD\nHyperparameter tables\nWe performed a grid search over the following\nranges: learning rate 1e-5, 2e-5, batch size 8, 16,\nnumber of epochs 2, 3, and weight decay 0.0, 0.01\nfor all models.\nModel\nLearning Rate\nBatch Size\nEpochs\nWeight Decay\nBERT\n2e-5\n8\n3\n0.01\nClinicalBERT\n2e-5\n8\n3\n0.01\nMentalBERT\n2e-5\n8\n3\n0.01\nTable 7: Best hyperparameters found via grid search.\nE\nPrompt for Generating Narrative\nSummaries\nDescription based on Excerpt Summary:\nYou are an expert clinical interviewer. Rewrite\nthe excerpt into ONE clinician-friendly para-\ngraph (max 3 sentences) describing *symp-\ntom*.\nExcerpt: <segment>\nDescription based on Excerpt Summary:\nProvide ONLY the interviewee quote (en-\nclosed in double quotation marks) that clearly\nillustrates *symptom* and supports \"anchor\".\nOutput the quote and nothing else.\nTranscript: <segment>\nQuote:\nF\nPrompt Sensitivity for Transcript\nSummarisation\nPrompt sensitivity was evaluated with 3 different\nprompts, varying on role framing (experienced clin-\nician, clinical assessor, clinical note writer), in-\nstruction specificity (ask simply to “summarise”\nversus explicitly specify including both questions\nand responses or improving clarity), and emphasis\n(conciseness, completeness, accuracy and struc-\nture). Summaries remained semantically consistent\nacross prompts, with BERTScore F1 = 0.7558 and\nSentenceBERT cosine = 0.7758.\nG\nSHAP-based stability\nWe also evaluated the factual consistency and se-\nmantic quality of Qwen3-4B narrative summaries\nderived from symptom domains with highest SHAP\nattributions.\nFactual consistency was assessed\nusing an NLI-based faithfulness check, showing\n6.76% contradicted statements. Semantic similar-\nity remained high, with SentenceBERT cosine =\n0.7100 and BERTScore F1 = 0.8883 (precision =\n0.8754, recall = 0.9019).\n"}, {"page": 11, "text": "H\nQuestionnaire for Explanation Feedback\nBackground Information and Data Collection Purpose\nAbout Chirpe\nIn Orygen, our team produces Artificial Intelligence (AI) devices that enhance human connection and identify consumers\nwho need specialised care. We have developed an AI called \"Chirpe\" (Clinical High Risk Prediction with Explainability).\nChirpe is able to identify young people at clinical high-risk for psychosis (CHR) using recorded speech of the PSYCHS\nquestionnaire at accuracies above 90%. It’s very exciting because it means that we may not need to rate the PSYCHs in the\nfuture.\nWhy your feedback matters\nChirpe sometimes struggles with explaining why the person is CHR and that’s where we need your help. We have designed\ndifferent ways that Chirpe can explain their decisions and we’d like you to let us know what style of communication you\nprefer. We’d be really grateful for any other feedback too as you are the experts.\nContribute to research\nIf you want to be involved in this paper as a clinical advisor, please let us know. We will also use your responses to design\nbetter ways that CHiRPE can talk with you.\nDescription of task\nComprehensibility of Output Formats\nWe are interested in how we can explain Chirpe’s predictions to clinicians and have created 7 different options to choose\nfrom. Think about whether you understand why Chirpe made the decision: Is it comprehensible?\nRating of Explanability Outputs\nClinical experts were asked to evaluate the interpretability of the following seven explanation formats. For each, they rated\ninterpretability on a 5-point Likert scale (1 = Not Interpretable; 5 = Very Interpretable).\n1. Word-level SHAP plots\nLonger bars in red indicate stronger influence towards CHR prediction (see Figure 2)\n2. Highlighting text in the transcript or summary\nWord highlights show influence (darker = stronger): red for CHR, blue for Control (see Figure 4)\n3. Symptom-level Plots (P-items)\nRed bars = CHR; Green bars = Controls. (See Figure 3 in Main Text)\n4. Selected sentences from the most influential symptom categories\nThese sentences contain words that are most contributing to the CHR prediction. (See Sentence-level Summary in\nMain Text)\n5. Single clinical narrative (See Narrative Summary in Main Text)\n6. Symptom-level Plots + Single clinical narrative (See Narrative Summary in Main Text)\n7. Multiple narrative summaries across symptom categories\nQualitative Feedback In addition to the above ratings, clinicians responded to the following open-ended questions:\n• Would you suggest any changes to how the graphs are presented?\n• Would you suggest any changes to how text information is presented?\n• Do you have any other comments on how CHiRPE’s explanations can be improved?\n"}, {"page": 12, "text": "I\nClinician Ratings of Explanation\nFormats\nI.1\nANOVA Results\nA one-way repeated-measures ANOVA revealed\na significant effect of explanation format on per-\nceived comprehensibility, F(6, 28) = 26.485, p <\n.001. Holm-adjusted pairwise t-tests showed that\nOptions 4, 5, 6, and 7 were all rated significantly\nhigher than Options 1 and 2 (p < .001). No signifi-\ncant differences were found between the top-rated\nformats (Options 4–7).\nI.2\nAdditional Evaluation with Clinical\nReasoning and Alignment\nTwo additional questions regarding clinical reason-\ning and alignment of SHAP explanation formats\nwere evaluated using a 7-point Likert Scale.\n1. Clinical Reasoning: “To what extent does this\nexplanation format align with how you would\nunderstand symptom information?”\n2. Clinical Intuition Alignment: “Does the ex-\nplanation format present information in a way\nthat is clinically intuitive?”\nExplanation Format\nReasoning\nAlignment\nWord-level plots\n2.0\n1.8\nToken-level Heatmaps\n2.4\n2.4\nSymptom bar plots\n4.2\n4.4\nSentence-level summary\n5.4\n5.6\nSingle narrative\n5.8\n5.6\nGraph + Single narrative\n6.0\n6.2\nMultiple narratives\n6.0\n5.6\nTable 8: Mean and SD of Comprehensibility (n = 5)\nAcross these new evaluations, the proposed for-\nmats from this paper remained the highest rated,\nconsistent with our earlier findings.\nI.3\nClinical Expert Quotes from Qualitative\nFeedback\n• Graph plus text summaries preferred\nClinical experts consistently valued mixed\nformats. One comment noted that “a graph\ncombined with select sentences would be\nmost useful” and another stated “I think a mix-\nture of option six and seven would be perfect.”\n• Concise formats\nSeveral\nclinical\nexperts\npreferred\nbrief\nexplanations, e.g. “Can be more concise. No\nneed to be complete sentences” and “Instead\nof a paragraph of text, I’d like dot points.”\n• Preference for structured symptoms\nAn expert emphasised alignment with clinical\nstructure: “I prefer a summary and a table\nof all Ps with symptoms described through\ntenacity, distress, interference and frequency.”\n• Clear definitions of experiences\nTwo comments highlighted the need to\ndefine the target experience: “More specific\ndescription of the experience itself” and\n“still providing a global picture of several\nsymptoms rather than only a single narrative.”\n• Clarify model inputs and outputs\nRequest of clarification of the model’s input,\n“I’m unclear if the system was given a full\ntranscript or only the case summary.”\n• Severity or risk indication\nSuggestion of adding a risk scale: “Could\na scale of risk severity be included as a\nconclusion in the future.”\n• Data access and governance\nOne comment raised governance concerns:\n“Is the model in-house only. Who has access\nto these data.”\n"}, {"page": 13, "text": "J\nAdditional Figures and Tables\nFigure 4: Inline token level heatmaps with SHAP\nFigure 5: Website Interface.\n"}]}