{"doc_id": "arxiv:2511.03441", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.03441.pdf", "meta": {"doc_id": "arxiv:2511.03441", "source": "arxiv", "arxiv_id": "2511.03441", "title": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field", "authors": ["Doria Bonzi", "Alexandre Guiggi", "Frédéric Béchet", "Carlos Ramisch", "Benoit Favre"], "published": "2025-11-05T13:02:06Z", "updated": "2025-11-06T11:06:10Z", "summary": "Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.03441v2", "url_pdf": "https://arxiv.org/pdf/2511.03441.pdf", "meta_path": "data/raw/arxiv/meta/2511.03441.json", "sha256": "25c0d6b1110db2be4358f3e3afc83adf2e3fd7b9fd506a8d7194a91b1654d498", "status": "ok", "fetched_at": "2026-02-18T02:28:24.787637+00:00"}, "pages": [{"page": 1, "text": "CareMedEval dataset: Evaluating Critical Appraisal and Reasoning\nin the Biomedical Field\nDoria Bonzi1, Alexandre Guiggi2, Frédéric Béchet3, Carlos Ramisch3, Benoit Favre3,4\n1University of Lorraine, LORIA, France\n2University Grenoble-Alpes, France\n3Aix-Marseille University, LIS, France\n4CNRS, Grenoble INP, LIG, France\ndoria.bonzi@loria.fr, alexandre.guiggi@gmail.com, {frederic.bechet, carlos.ramisch, benoit.favre}@lis-lab.fr\nAbstract\nCritical appraisal of scientific literature is an essential skill in the biomedical field. While large language models\n(LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in\nspecialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical\ncritical appraisal and reasoning tasks.\nDerived from authentic exams taken by French medical students, the\ndataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly\nevaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and\nbiomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial\nmodels fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably\nimproves the results. Yet, models remain challenged especially on questions about study limitations and statistical\nanalysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations\nand paving the way for future development of automated support for critical appraisal.\nKeywords: critical appraisal, reasoning, evaluation, medical, domain specific dataset, LLM\n1.\nIntroduction\nMedical professionals must engage in continuous\nlearning to stay up to date with evolving medical\nknowledge. Even though they can gather knowl-\nedge from trusted sources such as Cochrane, they\nalso engage with latest research published in sci-\nentific papers often available before peer review in\ndedicated archives.\nCritically appraising scientific publications is a\ncomplex cognitive task, even for trained physicians.\nAs highlighted by previous studies, proper inter-\npretation of biomedical literature requires not only\nfamiliarity with medical content, but also aware-\nness of methodology and statistics (du Prel et al.,\n2009). Moreover, studies have shown that medi-\ncal research can suffer from major methodological\nflaws, raising concerns about the reliability and over-\nall trustworthiness of scientific evidence (Ioannidis,\n2005; Begley and Ellis, 2012). These issues often\nrelate to study design quality and various forms\nof bias (Chalmers and Glasziou, 2009; Dickersin,\n1987). These challenges show why teaching and\nassessing critical appraisal skills remains a major\nand ongoing issue in the biomedical field.\nNatural language processing (NLP), and in par-\nticular large language models (LLMs), represents\na promising technological solution for supporting\nthis lifelong learning process. Recent development\nof LLMs has put focus on their \"reasoning capabili-\nties\", opening new possibilities for supporting med-\nical professionals in the critical reading, analysis,\nand synthesis of scientific literature. It is important\nto evaluate the reliability of such technologies.\nSeveral evaluation benchmarks already exist in\nthe biomedical domain, but these resources do not\nexplicitly target the evaluation of research method-\nology or a system’s ability to identify limitations and\nbiases in a study. As a result, they are not well-\nsuited for measuring the specific skills involved in\ncritical appraisal.\nDespite the growing interest in applying LLMs\nto these tasks, there remain significant challenges\nrelated to hallucinations, bias, and keeping scien-\ntific accuracy intact (Wang et al., 2024b; Yun et al.,\n2023; Meng et al., 2024). Recent work has ex-\nplored long-context processing in LLMs (Nelson\net al., 2024; Li et al., 2023) and their application\nto the medical field (Bazoge et al., 2024), includ-\ning retrieval-augmented generation (RAG) systems,\nwhere external knowledge sources are integrated\ninto the model’s reasoning process (Liu et al., 2025).\nIn these studies, RAG methods make biomedical\nQA more accurate and robust, yet they mainly sup-\nport information access and synthesis. The task of\ncritically assessing study design and validity is not\nwell represented in benchmarks, especially when\ngrounded in a given study described in a scientific\narticle.\nIn this work, we introduce (1) a dataset for evalu-\nating critical appraisal of medical studies described\nin scientific articles1, derived from multiple choice\nquestion-answers (MCQA) medical education ex-\nams in French.\nWith this dataset release, we\npresent (2) a comprehensive evaluation of diverse\nstate-of-the-art language models on this challeng-\narXiv:2511.03441v2  [cs.CL]  6 Nov 2025\n"}, {"page": 2, "text": "ing task, providing baseline performance results\nand insights into model capabilities and limitations.\nUnlike existing biomedical question answering\ndatasets which are typically not grounded in re-\nsearch articles, our resource is directly contextu-\nalized with authentic scientific publications, aim-\ning to evaluate the information gathering and rea-\nsoning capabilities of models. We argue that this\ndataset is complementary both to factual and RAG-\noriented medical benchmarks, by addressing the\nunderexplored dimension of critical appraisal in\nthe medical field. The dataset developed in this\nwork could also support future technologies beyond\nLLMs, providing a foundation for the development\nof tools aimed at enhancing medical reasoning and\nevidence-based decision-making.\n2.\nRelated Work\nSystematic reviewing is a time-consuming process\nthat has recently attracted interest for automation\n(Nikiforovskaya et al., 2020, Han et al., 2024). How-\never, it has been observed that automation still\nfaces limitations, with human reviewers outper-\nforming current automated methods on such tasks\n(Yuan et al., 2022). The main goal of our study is to\nreflect the challenge of critical appraisal of medical\narticles, providing a benchmark for evaluating how\nwell models can support both retrieval and critical\nreasoning in biomedical contexts.\nTo our knowledge, no existing dataset specifically\ntargets the task of critical appraisal and analysis of\nscientific articles. However, similar resources for\nthe general evaluation of biomedical NLP models\nare available: for instance, PubMedQA (Jin et al.,\n2019) is an English-language dataset where ques-\ntions are derived from the abstracts of biomedical\nresearch articles; MedQA (Jin et al., 2020) offers an\nopen-domain QA benchmark in English, simplified\nChinese, and traditional Chinese, based on medi-\ncal textbooks; SciDQA (Singh et al., 2024) provides\na collection of questions grounded in full-length sci-\nentific articles with figures and images. However,\nnone of these datasets explicitly focus on the critical\nevaluation of research methodology. They primarily\nassess factual comprehension or domain knowl-\nedge, rather than the ability to review a scientific\narticle in terms of study design, methodology or lim-\nitations. In contrast, our dataset is specifically de-\nsigned to capture and evaluate these skills through\nmultiple-choice questions grounded in medical lit-\nerature.\nWhile the French biomedical NLP landscape still\nlacks a dataset that aims to explore critical appraisal\nof scientific articles, other resources exist for adja-\ncent tasks: MedFrenchmark (Quercia et al., 2024)\n1Dataset and code available at https://github.\ncom/bonzid/CareMedEval.\nand FrenchMedMCQA (Labrak et al., 2023) focus\non MCQA in the medical domain, though without\ncontextual grounding in articles; CAS (Grabar et al.,\n2018) supports information extraction tasks; and\nQUAERO (Névéol et al., 2014) provides annota-\ntions for named entity recognition. More broadly,\nDrBenchmark (Labrak et al., 2024) consolidates\n20 different biomedical tasks in French, offering a\ncomprehensive evaluation suite for LLMs.\nAlongside, recent efforts have proposed RAG\nframeworks tailored for tasks in the biomedical field.\nFor example, Li et al. (2024b) provide a system-\natic evaluation of RAG-based approaches across\nvarious biomedical applications. Studies (He et al.,\n2025) have shown that RAG systems significantly\noutperform standard LLMs on tasks such as infor-\nmation extraction and question answering. Biome-\ndRAG (Li et al., 2024a) simplifies the integration\nof retrieved knowledge by incorporating relevant\npassages into LLM inputs, while BioRAG (Wang\net al., 2024a) combines a scientific corpora with\ndomain-specific embedding and hierarchical knowl-\nedge structures to improve biological question rea-\nsoning. While our approach does not employ RAG,\nit complements this line of research by addressing\nthe critical appraisal of scientific articles, an aspect\nthat remains underexplored in existing resources.\nThese resources reflect growing interest in eval-\nuating LLMs in the biomedical domain, but they do\nnot yet address the evaluation of critical reading\nskills, grounded in scientific literature, which our\ndataset aims to address.\n3.\nDataset overview\nCareMedEval (Critical Appraisal and REasoning\nMedical Evaluation) is a French dataset focused\non evaluating critical appraisal skills in the medical\nfield for scientific articles, as practiced in French\nmedical education.\nThis dataset is composed of 534 questions taken\nfrom Lecture Critique d’Articles exams (LCA, Critical\nappraisal of research articles), taken by sixth-year\nmedical students in France. During the LCA ex-\nams, which last three hours, students are asked\nto answer a series of multiple-choice questions\ngrounded in critical reading of a given scientific\narticle. Students are required to critically analyze\nand interpret scientific biomedical articles, most\nof which are clinical studies published in peer-\nreviewed journals. These articles include obser-\nvational studies like case-control or interventional\nstudies such as randomized clinical trials, and cover\na broad range of medical specialties like epidemi-\nology, biostatistics or public health. Students are\nexpected to demonstrate critical reasoning skills\nby classifying study types and methodological ap-\nproaches, understanding the implications for clini-\n"}, {"page": 3, "text": "Sample question with context and answer choices taken from the dataset\nArticle, available in PDF or plain text:\nYou are a physician capable of rigorously interpret-\ning data from medical studies. Based on the given\narticle, answer the following multiple-choice ques-\ntion. Provide only the letter(s) corresponding to\nthe correct answer(s) among: A, B, C, D, E. Your\nanswer must strictly follow the exact format: one\nor more letters, separated by commas (e.g., B, E).\nQuestion:\nWhat is the main limitation of this study?\nAnswers:\na) The 2:1 randomization\nb) The choice of the primary endpoint\nc) The number of subjects included\nd) The amount of antibodies contained in the\nplasma\ne) The fact that the study was conducted in Ar-\ngentina\nFigure 1: Example from the dataset showing an excerpt from a scientific article, the given instruction\nprompt with a corresponding question, and answer choices.\ncal practice, recognizing potential biases or study\nlimitations, and evaluating statistical evidence sup-\nporting the study conclusions. A sample of our\ndataset showing an excerpt from a scientific article,\nthe given instruction prompt with a corresponding\nquestion, and answer choices is shown in Figure 1.\n3.1.\nDataset collection\nSource data\nThis dataset is built from two main\nsources:\n• Epreuves Classantes Nationales (ECN)\nwebsite2, where we can find the official na-\ntional LCA exams,\n• Collège National des Enseignants de\nThérapeutique (CNET) website3, which pub-\nlishes mock LCA exams reviewed and ap-\nproved by an educational committee to ensure\nalignment with real exams.\nThe articles used in these exams are publicly\navailable genuine scientific papers. We used these\ntwo websites as they provide official and mock ex-\nams, freely accessible and suitable for corpus con-\nstruction. The ECN website offers real past exams,\nadding authenticity and relevance to our dataset.\n2https://www.cng.sante.fr/candidats/\ninternats/concours-medicaux/etudiants/\nepreuves-classantes-nationales-ecn\n3https://therap.fr/lca/\nAs for the CNET website, it features training ex-\nams with professional corrections and commentary,\nwhich is particularly valuable for understanding the\nreasoning behind correct answers. To our knowl-\nedge, there are no other free and easily accessible\nonline sources offering similar LCA exam content.\nLanguages\nThe scientific articles are in English.\nThe questions, answers, and justifications are in\nFrench. We also provide a preliminary English\ntranslation of the French parts of the dataset, gen-\nerated using Gemini 2.5 Flash.\nAnnotations and labels\nEach question in the\nCareMedEval dataset was manually annotated with\none or more labels, reflecting the cognitive and an-\nalytical skills required to answer it. Question label-\ning was conducted by one expert annotator with\na background in general practice, using reference\ntextbooks and their medical expertise. The labels\nare defined in Table 1. Since questions often target\nmultiple dimensions of skills, they can be assigned\nmore than one label. These labels can help deter-\nmine whether certain categories of questions are\nmore challenging than others for models, depend-\ning on the types of skills they require.\nFor data sourced from the ECN website, we man-\nually corrected the exam questions with the help of\na general practitioner, as the official answer keys\nwere not publicly available for these past exams.\nFor data from CNET website, we also collected\nthe correct answers and justifications provided by\n"}, {"page": 4, "text": "Label\nDescription\nSkills required\nSupport\ndesign\nIdentification of study design\nInformation retrieval\n105\nstatistics\nUnderstanding and interpretation of statistics\nGeneral knowledge, Information retrieval\n239\nmethodology\nKnowledge of scientific methodology\nGeneral conceptual understanding\n219\nlimitations\nCritical review of biases and limitations\nContextual reasoning\n132\napplicability\nClinical relevance and applicability\nContextual reasoning\n115\nTable 1: Labels by type of reasoning involved in the critical appraisal of biomedical articles. Each question\nin the dataset was annotated with one or more labels reflecting the cognitive and analytical skills required\nto answer it.\nmedical professionnals for a subset of questions.\nThese justifications explain why some answers are\ncorrect or false, offering valuable insight into clinical\nreasoning and critical appraisal.\n3.2.\nDataset structure\nCareMedEval is composed of 534 questions in\nJSON format and 37 articles available in PDF.\nStatistics of this dataset are shown in Table 2. On\naverage, each question contains 15.6 tokens and\nhas 2.60 correct answers. Most questions have\nmultiple correct answers: about 29% have two or\nthree correct options, 20% have four, 19% only\nhave one, and a small portion (around 3%) have\nfive correct answers.\nStatistic\nValue\nQuestions (total)\n534\nWith justifications\n204\nVocabulary size\n1,273 words\nQuestion length (avg.)\n15.6 tokens\nCorrect answers (avg.)\n2.6\nArticles (total)\n37\nQuestions per article (avg.)\n14.4\nmin / max\n8 / 16\nArticle length (avg.)\n5,675 tokens\nmin / max\n2,747 / 8,332\nArticle length (PDF, avg.)\n10 pages\nAbstract length (avg.)\n1,019 tokens\nmin / max\n276 / 1,832\nTable 2: CareMedEval dataset statistics.\nEach question in the dataset is represented as a\nJSON object with the fields described in Table 3.\nEach question in the dataset is linked to a scien-\ntific article through the id_article field, which\nserves as a reference key to the corresponding\narticle file. There are on average 14.4 questions\nper article. The minimum is 8 and the maximum\nis 16.\nArticles are available in plain text (.txt)\nformat for easier processing, and the original\nPDF versions are also included. The plain text\nfiles were generated using the PyMuPDF library4,\nwhich extracts raw textual content from each\npage, without figures. We manually reviewed and\ncorrected the formatting issues in the resulting files\nto ensure readability and to preserve the original\nstructure of the articles as much as possible.\nUsing the Byte-Pair Encoding tokenizer from the\ntiktoken5 library, we found that scientific articles\nin our dataset contain between 2,747 and 8,332\ntokens, with an average length of 5,675 tokens per\narticle. For the abstracts alone, also included in\nour dataset and matched with their corresponding\narticle IDs, the number of tokens ranges from 276\nto 1,832, with an average length of 1,019 tokens.\nFor the PDF versions, articles are on average\napproximately 10 pages long, with around 5,400\nwords and 36,000 characters. On average, each ar-\nticle includes about 3.3 figures, for an estimated to-\ntal of 123 figures across the entire dataset (approx-\nimations computed using the PyMuPDF library).\nThe publication years range from 2007 to 2023. In\naddition of the plain text and PDF versions, each\nquestion includes a direct article_link field\npointing to the online HTML version of the article.\n4.\nBenchmark\nTo systematically assess the critical appraisal abili-\nties of LLMs, we introduce a dedicated benchmark\nbuilt upon our dataset. It provides a structured eval-\nuation framework that integrates multiple metrics,\ncontrolled scenarios, and a representative set of\nmodels to enable comprehensive and reproducible\nanalysis.\n4.1.\nMetrics\nOur benchmark relies on four different metrics cho-\nsen to reflect the multiple-choice question-answer\nnature of the dataset:\n• Exact Match Ratio (EMR) measures the pro-\nportion of questions for which the predicted set\nof answers exactly matches the gold standard.\n• F1-score is the harmonic mean of precision\nand recall, computed between predicted and\ngold answer sets.\n• Hamming score evaluates the proportion of\ncorrectly predicted labels (answer options)\nover the total number of possible labels, aver-\naged over all questions.\n4https://github.com/pymupdf/PyMuPDF\n5https://github.com/openai/tiktoken\n"}, {"page": 5, "text": "Field\nDescription\nid\nA unique identifier for the question\nid_article\nInternal article ID\nsource_exam\nURL of the exam or online resource\ndate_exam\nDate of the exam\narticle_link\nURL of the article\narticle_date\nArticle publication date\nquestion\nQuestion as presented in the exam\nanswers\nDictionary mapping each option label (A to E) to its full answer text\ncorrect_answers\nList of correct answers labels\nessential_answers\nList of essential answers for LCA grading (23)\nunacceptable_answers\nList of inadmissible answers for LCA grading (19)\nlabels\nLabels describing skills or knowledge required (see Table 1)\njustification\nExpert-written explanation of correct and incorrect answers (204)\nnb_correct_answers\nNumber of correct answer options for the question\nTable 3: Field descriptions for the CareMedEval dataset. Each entry corresponds to a multiple-choice\nquestion associated with a biomedical research article. Some fields are specific to certain subsets and\nare designed to support fine-grained evaluation, reasoning analysis, and expert-based interpretation.\nSubset size is specified in parenthesis.\n• LCA score is a custom metric inspired by the\ngrading system used in the original LCA exam\nfrom which our dataset is derived. It reflects\nexam-style grading: for each question, a per-\nfect match yields 1 point, one mismatch 0.5,\ntwo mismatches 0.25, and more than two mis-\nmatches or no response 0. In addiction, there\nare two other constraints to the LCA grading\nsystem: if a required (essential) answer is\nmissing, the score is automatically 0, regard-\nless of other matches. If an unacceptable an-\nswer (one that should never be selected) is\nincluded, the score is 0. The final LCA score\nis averaged over all questions.\nIn France, the LCA exam is part of the \"Epreuves\nde Connaissances\" (\"Knowledge Exams\"), which\naccount for 60% of the final score in the national\nmedical competition. Results for the LCA exam and\nfor these knowledge exams in general are not pub-\nlicly available. However, we know that students\nmust achieve a minimum score of 14/20 (70%)\nto advance to the next stage of the competition6,\nwhich can be used as a reference point for assess-\ning models’ success on the task.\nThese metrics allow us to distinguish between full\ncorrectness (EMR), partial correctness (F1, Ham-\nming), and real-world grading fairness (LCA score).\n4.2.\nEvaluation scenarios and prompts\nWe designed multiple evaluation scenarios in which\nthe model was given an instruction-style prompt in\nFrench specifying the expected output format, a\nquestion, and the answer choices. Depending on\nthe setting, the full article was provided, or only the\nabstract was included, or no article content was\ngiven. This pipeline is shown in Figure 2.\n6https://www.cng.sante.fr/\nepreuves-dematerialisees-nationales-edn\nWe used a role-oriented instruction prompt, fram-\ning the model as a medical professional. While\nthe impact of such role framing remains debated\n(Zheng et al., 2024), we kept it consistent across\nevaluation scenarios.\n4.3.\nModels\nTo enable a comprehensive evaluation on this\ndataset, we selected models of varying sizes (from\n8B to 120B parameters), architectures, domain spe-\ncialization (general-purpose vs. biomedical-tuned),\nreasoning token generation capabilities, allowing\nus to assess the effect of the benchmark on differ-\nent dimensions: Qwen3-8B/32B (Yang et al., 2025),\nII-Medical-8B (Intelligent Internet, 2025), Gemma3-\n27B-text-IT (Google, 2025), MedGemma-27B-text-\nIT (Google, 2025). We also evaluated GPT-4.1\n(OpenAI et al., 2024), GPT-4o-mini (OpenAI et al.,\n2024) and GPT-OSS-20B/120B (OpenAI, 2025) to\nassess how frontier and derived models perform\ncompared to smaller or domain-specific alterna-\ntives.\n5.\nResults and analysis\nWe conducted a series of experiments on the\nCareMedEval dataset to evaluate the critical ap-\npraisal skills of a diverse pool of LLMs. While this\ndataset can support multiple experimental setups,\nwe focus here on a MCQA task based solely on the\ntextual content of the articles.\nAll experiments were run on an cluster equipped\nwith NVIDIA L40-48GB and A100-80GB GPUs.\nInference was performed using the vLLM (Kwon\net al., 2023) and Ollama7 engines.\nIn this section, we showcase some of the find-\nings, while the complete set of results is available\n7https://ollama.com/\n"}, {"page": 6, "text": "Figure 2: Overview of the model evaluation pipeline of the CareMedEval benchmark. The input consists of\na zero-shot instruction prompt containing a question and possible answer choices, along with article (plain\ntext only in our experiment setting). The model generates predicted answers, which are then evaluated\nusing a set of quantitative metrics to assess performance.\nin the accompanying material. Overall, GPT-4.1\ndemonstrates the highest performance across all\nevaluation scenarios, with Qwen3-32B consistently\nranking as the second best. Notably, only four mod-\nels surpass an EMR of 0.25 and none surpasses\na LCA score of 0.70, which would be the minimal\nmark for the exam, highlighting the difficulty of the\ntask. For comparison, GPT-4.1 reaches an EMR\nof 0.79 on the FrenchMedMCQA dataset (Labrak\net al., 2023). The difficulty of French medical MCQs\nfrom licensing examinations was highlighted in a\nprevious study (Alfertshofer et al., 2023), where the\n\"multiple correct answers\" format appeared to be\na factor contributing to models’ poor performance\non the task. This observation is consistent with our\nanalysis of the subset presented in Influence of arti-\ncle access on performance: models tend to perform\nbetter when the question explicitly requires a single\ncorrect answer. Interestingly, models specialized\nfor the biomedical domain do not consistently out-\nperform generalist models, exhibiting comparable\nperformance at best.\n5.1.\nComparison of models performance\n5.1.1.\nGeneralist vs. specialized\nThe goal is to assess whether biomedical-\npretrained or fine-tuned models offer a concrete\nadvantage on medical tasks compared to the mod-\nels they have been specialized from, under our\nhypothesis that specialized models are expected\nto perform better on a domain-specific critical rea-\nsoning task. Results presented in Table 4 show\ncomparable EMR scores between some special-\nized and generalist models (e.g., Medgemma vs.\nGemma3). In several cases, generalist models\neven outperform their specialized counterparts,\nsuch as Qwen3-8B vs. II-Medical-8B. To evalu-\nate whether EMR differences between generalist\nand specialized models were significant, we ap-\nplied McNemar’s test, which compares paired out-\ncomes by counting where one model is correct and\nthe other is not. Despite noticeable differences\nin raw performance, the test shows that, in most\ncases, these gaps are not statistically significant\n(p ≥0.05). Significant differences were only ob-\nserved for Qwen3-8B and II-Medical-8B. Conse-\nquently, our benchmark does not provide sufficient\nevidence to confirm our hypothesis. However, in\nline with prior works (Labrak et al., 2024; Dorfner\net al., 2024), our results are consistent with the\nobservation that generalist models can perform\ncompetitively against domain-specialized models\nin medical tasks.\n5.1.2.\nInfluence of article access on\nperformance\nTo assess the importance of context for critical ap-\npraisal, we evaluated models under three different\nsettings as shown in Figure 3. In addition to the\nquestion and answer choices, models were pro-\nvided with either the full article, only the abstract,\nor no contextual information. The vast majority of\nquestions in our dataset require access to the arti-\ncle to be answered correctly. While a few questions\ncan be answered without reading the article, rely-\ning mostly on general medical knowledge, we still\nexpect a performance drop when the article is not\nprovided.\nPerformance varied significantly depending on\nthe amount of context available. When given ac-\ncess to the full article, models achieved their high-\nest scores; for example, GPT-4.1 reached an EMR\nof 0.49, and Qwen3-32B 0.36. While this confirms\nthat access to the entire article improves question\ncomprehension and accuracy, the overall perfor-\nmance remains moderate, reflecting the inherent\ndifficulty of the task. Moreoever, it is possible that\nsome articles may have been included in the mod-\nels’ pre-training data. In such cases, the models\ncould retrieve or reproduce information from mem-\nory rather than truly understanding the context,\nwhich might artificially inflate performance scores.\nWith only the abstract, model performance\n"}, {"page": 7, "text": "Model\nEMR\nF1\nHamming\nLCA\nRandom (1-3 options)\n0.03\n0.39\n0.29\n0.16\nOnly first-option\n0.00\n0.20\n0.11\n0.00\nMost 2 frequent answers\n0.03\n0.45\n0.33\n0.18\nMost 3 frequent answers\n0.03\n0.55\n0.42\n0.20\nGPT4.1\n0.49\n0.84\n0.78\n0.68\nGPT4o-mini\n0.25\n0.75\n0.65\n0.50\nQwen2.5-3B-Instruct\n0.10\n0.59\n0.46\n0.31\nQwen2.5-3B-GRPO-medical-reasoning\n0.11\n0.57\n0.45\n0.30\nQwen3-8B\n0.19\n0.68\n0.57\n0.42\nII-Medical-8B\n0.13\n0.51\n0.43\n0.30\nQwen3-32B\n0.37\n0.78\n0.70\n0.58\nGemma3-27B-text-IT\n0.27\n0.75\n0.65\n0.51\nMedgemma-27B-text-IT\n0.28\n0.73\n0.63\n0.50\nTable 4: Results of long-context models on the biomedical MCQA task of our dataset: zero-shot with\nfull-text article provided. Bold values indicate the best results per metric, and underlined values the\nsecond-best. Baselines include random or frequency-based answer selection strategies.\nFigure 3: Exact Match Ratio comparison across\ndifferent evaluation scenarios, illustrating model\nperformance when provided with the full article,\nonly the abstract, or no context (only the question\nand answer options with the instruction prompt).\ndropped slightly but remained better than in the\nno-context condition. GPT-4o-mini showed slightly\nlower results with just the abstract compared to the\nfull article, yet its performance remained competi-\ntive, suggesting that the abstract alone contains a\nsubstantial portion of the relevant information. With-\nout any contextual input, all models experienced a\nnotable decrease in performance, with EMR scores\ndropping by 5 to 15 points.\nThese findings highlight the importance of full-\ntext access to maximize model performance on\nthe task. While models like GPT-4.1 and Qwen3-\n32B demonstrate relative robustness even with lim-\nited or no context, by possibly mastering questions\nwhich do not require context, smaller models gener-\nally struggle to compensate for missing information.\nWe annotated a subset of 16 questions to indi-\ncate whether they require context to be answered\ncorrectly (field requires_context, can be true\nor false).\nThese annotations were based on\ngeneral trends observed in model performance:\nFigure 4: Heatmap of Exact Match Ratio by model\nand label for the MCQA task, illustrating perfor-\nmance differences across reasoning categories in\nthe critical appraisal of scientific articles. Labels\ncorrespond to distinct cognitive skills required to\nanswer the questions as described in Table 1.\nwhether models could answer questions correctly\nwith or without access to the article.\nThanks to this subset, we can see some regu-\nlarities and performance patterns with context or\nno context provided: some questions, particularly\nthose directly related to specific aspects of the study\n(like \"This is a study of:\") consistently require the\narticle for all models. Other questions show little\ndifference in performance between the with- and\nwithout-context settings, suggesting that context is\nnot necessary for these. Conversely, some ques-\ntions (like \"What was the main reason the data and\nsafety monitoring board re-evaluated the sample\nsize during the trial?\") were answered more accu-\nrately without the article, suggesting the presence\nof a bias for this type of question rather than a\nbenefit from using contextual information.\n"}, {"page": 8, "text": "Model\nEMR\nF1\nHamming\nLCA\nQwen3-8B (w/o→with)\n0.19→0.35\n0.68→0.75\n0.57→0.66\n0.42→0.55\nQwen3-32B (w/o→with)\n0.37→0.45\n0.78→0.81\n0.70→0.73\n0.58→0.64\nGPT-OSS-20b (low→high)\n0.36→0.49\n0.77→0.81\n0.60→0.75\n0.57→0.66\nGPT-OSS-120b (low→high)\n0.46→0.54\n0.81→0.85\n0.74→0.79\n0.65→0.71\nGPT4.1 (w/o→with)\n0.49→0.53\n0.84→0.85\n0.78→0.79\n0.68→0.71\nTable 5: Comparison of model performance according to \"reasoning\" tokens generation level on the\nCareMedEval benchmark.\n5.1.3.\nEvaluation details by labels\nEach question in our dataset was annotated with\none or more labels highlighting the cognitive and\nanalytical skills required to answer it. These labels\nallow us to analyze which types of questions are\nmore or less challenging for the models. These\nresults are presented in Figure 4.\nModels appear to struggle the most with ques-\ntions labeled limitations, which involve reviewing\nthe biases or limitations of the study. This label typ-\nically requires fine-grained contextual understand-\ning and often goes beyond what is explicitly stated\nin the text. The low scores suggest that models are\nsubject to difficulty with implicit critical reasoning.\nThe statistics label, which requires understand-\ning and interpreting statistical results, also shows\nlower performance compared to other categories.\nThis can be partly explained by limitations in quan-\ntitative reasoning, but also by the fact that articles\nare provided in plain text format, excluding figures\nwhere statistical information is often presented.\nQuestions labeled design and methodology are\nthose on which the models perform best. The top-\nperforming models reach strong scores in these\ncategories which may reflect the models’ ability to\nrecognize study structure and generalize research\nconcepts as typically presented in medical articles.\n5.1.4.\nImpact of reasoning tokens generation\non performance\nWe evaluated the impact of explicit reasoning to-\nkens generation on performance by comparing\nstandard predictions (without reasoning) to tests\nwhere models were prompted or trained to produce\na reasoning sequence before answering (with rea-\nsoning). For models that do not expose a mode\nwithout reasoning, we compare low and high rea-\nsoning effort presets. The results are available in\nTable 5. For each experiment we generated a sin-\ngle reasoning trace based on default parameters;\non average, 879 tokens were generated ranging\nfrom 36 to 20,019 tokens across all models.\nFor GPT-4.1, reasoning is explicitly requested\nin the prompt, whereas for the Qwen3 models, we\nextract the reasoning part naturally generated by\nthe model (content between <think> tags). We\nalso evaluate GPT-OSS models (OpenAI, 2025),\naccording to low and high reasoning profiles. Even\nthough we do not show results here, the medium\nreasoning profile yields results between those of\nthe low and high profiles, generally closer to the\nlater.\nIncorporating intermediate reasoning steps im-\nproves performance over no/low reasoning across\nall metrics, sometimes substantially, suggesting\nthat generating explicit reasoning tokens helps mod-\nels produce more accurate answers. This result\nsuggests that CareMedEval indeed requires some\nform reasoning and can evaluate efforts from LLM\nmakers to address this aspect. We leave the man-\nual evaluation of reasoning quality to future work\nas it is non trivial to match the reasoning trace out-\nput by models with the human-written justifications\navailable in the dataset.\n6.\nConclusion\nIn this work, we introduced an original dataset de-\nsigned for evaluation of critical appraisal of scien-\ntific articles in the medical domain, combining data\ncollection and expert annotation. We assess the\nsuitability of the dataset by computing the perfor-\nmance of a range of models.\nOverall, our experiments show that larger models\nlike GPT-4.1 and Qwen3-32B tend to perform bet-\nter on the task and domain-specialized biomedical\nmodels do not reliably outperform generalist mod-\nels, often showing similar levels of performance.\nHowever, using our LCA score based on real-life\nmedical exams, we find that none of the tested mod-\nels without reasoning achieve the passing score\nthat human candidates typically reach.\nProviding the full-text article as context consid-\nerably improves model performance compared to\nusing only abstracts or no context at all. This under-\nlines the necessity of access to complete scientific\ninformation for accurate question answering and\ncritical reasoning. Moreover, allowing models to\ngenerate reasoning tokens improves performance,\nhighlighting that reasoning is essential for produc-\ning more reliable and contextually grounded an-\nswers in a critical appraisal task.\nIn future work, we plan on extending the bench-\n"}, {"page": 9, "text": "mark to vision LLMs that can leverage the content\nof figures which is sometimes referenced in ques-\ntions or necessary to produce correct answers. We\nwould also like to create an evaluation framework\nof the reasoning traces produced by models, com-\npared to the justifications provided by experts.\n7.\nLimitations & Ethics statement\nLimitations\nOur study has several limitations that\nshould be acknowledged.\nFirst, our evaluation only used the textual content\nof the articles, without including figures, tables, or\nother materials found in the original PDFs. These\nparts often have important information that helps\nunderstand a study better, like how it was designed\nor its results.\nSecond, we evaluated a limited number of mod-\nels, with a focus on general-purpose LLMs rather\nthan domain-specific ones. Their performances\nmay not fully reflect the potential of specialized\nbiomedical models. In addition, our experiments\nused a fixed prompt structure without exploring\nprompt engineering variations or model-specific\nadaptations. Exploring alternative prompting strate-\ngies, including dynamic or adaptive prompts, could\nlead to improved performance.\nThird, the manual annotation of justifications was\nperformed by a small number of annotators, which\nmay introduce variability and bias in the dataset.\nExpanding the number and diversity of annotators\nwould help increase the reliability of the ground-\ntruth justifications.\nFourth, the scientific articles included in our\ndataset are available on the internet and may have\nbeen part of the training data of the evaluated LLMs.\nThis raises the possibility that some answers could\nbenefit from memorization rather than genuine rea-\nsoning, potentially inflating performance metrics.\nFinally, the dataset itself is relatively small (534\nquestions) and focused on a very specific educa-\ntional context (French medical LCA exams). Future\nwork could benefit from expanding the dataset in\nsize and scope to cover a broader range of biomed-\nical topics and question formats.\nEthics statement\nThe goal of this work is to eval-\nuate natural language processing technologies in\norder to better understand their capabilities and\nlimitations, in particular within the context of the\nEU AI Act regulatory framework. We stress the\nimportance of building useful tools for humans and\nthe human society, that do not decrease expertise\nor agency, both of which are critical in the medical\ndomain.\n8.\nAcknowledgements\nThis work was financially supported by ANR\nMALADES (ANR-23-IAS1-0005).\nExperiments\nwere conducted using HPC resources provided by\nthe Laboratoire d’Informatique et Systèmes (LIS) in\nMarseille. We thank Guillaume Lomet, Elie Antoine\nand Irina Illina for helpful feedback on an earlier\nversion of this work.\n9.\nBibliographical References\nMichael Alfertshofer, Cosima C. Hoch, Paul F.\nFunk, Katharina Hollmann, Barbara Wollenberg,\nSamuel Knoedler, and Leonard Knoedler. 2023.\nSailing the seven seas: A multinational compar-\nison of ChatGPT’s performance on medical li-\ncensing examinations. Annals of Biomedical En-\ngineering, 52(6):1542–1545.\nAdrien Bazoge, Emmanuel Morin, Beatrice Daille,\nand Pierre-Antoine Gourraud. 2024. Adaptation\nof biomedical and clinical pretrained models to\nfrench long documents: A comparative study.\nC. Glenn Begley and Lee M. Ellis. 2012. Raise\nstandards for preclinical cancer research. Nature,\n483(7391):531–533.\nIain Chalmers and Paul Glasziou. 2009. Avoidable\nwaste in the production and reporting of research\nevidence. The Lancet, 374(9683):86–89.\nKay Dickersin. 1987. Publication bias and clinical\ntrials. Controlled Clinical Trials, 8(4):343–353.\nFelix J. Dorfner, Amin Dada, Felix Busch, Mar-\ncus R. Makowski, Tianyu Han, Daniel Truhn,\nJens Kleesiek, Madhumita Sushil, Jacqueline\nLammert, Lisa C. Adams, and Keno K. Bressem.\n2024. Biomedical large languages models seem\nnot to be superior to generalist models on unseen\nmedical data.\nJean-Baptist du Prel, Bernd Röhrig, and Maria Blet-\ntner. 2009. Critical appraisal of scientific arti-\ncles: part 1 of a series on evaluation of scientific\npublications. Deutsches Ärzteblatt International,\n106(7):100–105.\nGoogle.\n2025.\nMedgemma-27b-text-it:\nInstruction-tuned medical llm based on gemma 3\n(27b). https://huggingface.co/google/\nmedgemma-27b-text-it.\nNatalia Grabar, Vincent Claveau, and Clément\nDalloux. 2018. CAS: French Corpus with Clin-\nical Cases.\nIn Ninth International Workshop\non Health Text Mining and Information Analysis\n"}, {"page": 10, "text": "(LOUHI) Proceedings of the Workshop, Ninth In-\nternational Workshop on Health Text Mining and\nInformation Analysis (LOUHI) Proceedings of the\nWorkshop, pages 1–7, Bruxelles, France.\nBinglan Han, Teo Susnjak, and Anuradha Math-\nrani. 2024. Automating systematic literature re-\nviews with retrieval-augmented generation: A\ncomprehensive overview.\nApplied Sciences,\n14(19):9103.\nJiawei He, Boya Zhang, Hossein Rouhizadeh,\nYingjian Chen, Rui Yang, Jin Lu, Xudong Chen,\nNan Liu, Irene Li, and Douglas Teodoro. 2025.\nRetrieval-augmented generation in biomedicine:\nA survey of technologies, datasets, and clinical\napplications.\nIntelligent\nInternet.\n2025.\nIi-medical-8b.\nhttps://huggingface.co/\nIntelligent-Internet/II-Medical-8B.\nJohn P. A. Ioannidis. 2005. Why most published\nresearch findings are false.\nPLoS Medicine,\n2(8):e124.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung\nWeng, Hanyi Fang, and Peter Szolovits. 2020.\nWhat disease does this patient have? a large-\nscale open domain question answering dataset\nfrom medical exams.\nQiao\nJin,\nBhuwan\nDhingra,\nZhengping\nLiu,\nWilliam W. Cohen, and Xinghua Lu. 2019. Pub-\nmedqa: A dataset for biomedical research ques-\ntion answering.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Ef-\nficient memory management for large language\nmodel serving with pagedattention.\nYanis Labrak, Adrien Bazoge, Richard Dufour,\nMickael Rouvier, Emmanuel Morin, Béatrice\nDaille,\nand Pierre-Antoine Gourraud. 2023.\nFrenchmedmcqa: A french multiple-choice ques-\ntion answering dataset for medical domain.\nYanis Labrak, Adrien Bazoge, Oumaima El Khettari,\nMickael Rouvier, Pacome Constant dit Beaufils,\nNatalia Grabar, Beatrice Daille, Solen Quiniou,\nEmmanuel Morin, Pierre-Antoine Gourraud, and\nRichard Dufour. 2024. Drbenchmark: A large\nlanguage understanding evaluation benchmark\nfor french biomedical domain.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng,\nLianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang. 2023. How long can\ncontext length of open-source llms truly promise?\nIn Workshop on Instruction Tuning and Instruc-\ntion Following at NeurIPS 2023. OpenReview\npreprint.\nMingchen Li, Halil Kilicoglu, Hua Xu, and Rui Zhang.\n2024a. Biomedrag: A retrieval augmented large\nlanguage model for biomedicine.\nMingchen Li, Zaifu Zhan, Han Yang, Yongkang\nXiao, Jiatan Huang, and Rui Zhang. 2024b.\nBenchmarking retrieval-augmented large lan-\nguage models in biomedical nlp: Application,\nrobustness, and self-awareness.\nSiru Liu, Allison B McCoy, and Adam Wright. 2025.\nImproving large language model applications in\nbiomedicine with retrieval-augmented genera-\ntion: a systematic review, meta-analysis, and clin-\nical development guidelines. Journal of the Amer-\nican Medical Informatics Association, 32(4):605–\n615.\nXiangbin Meng, Xiangyu Yan, Kuo Zhang, Da Liu,\nXiaojuan Cui, Yaodong Yang, Muhan Zhang,\nChunxia Cao, Jingjia Wang, Xuliang Wang,\nJun Gao, Yuan-Geng-Shuo Wang, Jia-ming Ji,\nZifeng Qiu, Muzi Li, Cheng Qian, Tianze Guo,\nShuangquan Ma, Zeying Wang, Zexuan Guo,\nYoulan Lei, Chunli Shao, Wenyao Wang, Haojun\nFan, and Yi-Da Tang. 2024. The application of\nlarge language models in medicine: A scoping\nreview. iScience, 27(5):109713.\nElliot Nelson, Georgios Kollias, Payel Das, Subhajit\nChaudhury, and Soham Dan. 2024. Needle in\nthe haystack for memory based large language\nmodels.\nAnna Nikiforovskaya,\nNikolai Kapralov,\nAnna\nVlasova, Oleg Shpynov, and Aleksei Shpilman.\n2020. Automatic generation of reviews of sci-\nentific papers. In 2020 19th IEEE International\nConference on Machine Learning and Applica-\ntions (ICMLA), pages 314–319.\nAurélie Névéol, Cyril Grouin, Jeremy Leixa, Sophie\nRosset, and Pierre Zweigenbaum. 2014. The\nQUAERO French medical corpus: A ressource\nfor medical entity recognition and normalization.\nIn Proc of BioTextMining Work, pages 24–30.\nOpenAI. 2025. gpt-oss-120b & gpt-oss-20b model\ncard.\nOpenAI, Josh Achiam, Steven Adler, Sandhini\nAgarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Al-\ntenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie\nBalcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake\nBerdine, Gabriel Bernadett-Shapiro, Christopher\n"}, {"page": 11, "text": "Berner, Lenny Bogdonoff, Oleg Boiko, Made-\nlaine Boyd, Anna-Luisa Brakman, Greg Brock-\nman, Tim Brooks, Miles Brundage, Kevin Button,\nTrevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael,\nBrooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen,\nMark Chen, Ben Chess, Chester Cho, Casey\nChu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux,\nThomas Degry, Noah Deutsch, Damien Dev-\nille, Arka Dhar, David Dohan, Steve Dowling,\nSheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna\nEloundou, David Farhi, Liam Fedus, Niko Fe-\nlix, Simón Posada Fishman, Juston Forte, Is-\nabella Fulford, Leo Gao, Elie Georges, Chris-\ntian Gibson, Vik Goel, Tarun Gogineni, Gabriel\nGoh, Rapha Gontijo-Lopes, Jonathan Gordon,\nMorgan Grafstein, Scott Gray, Ryan Greene,\nJoshua Gross, Shixiang Shane Gu, Yufei Guo,\nChris Hallacy, Jesse Han, Jeff Harris, Yuchen He,\nMike Heaton, Johannes Heidecke, Chris Hesse,\nAlan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu,\nXin Hu, Joost Huizinga, Shantanu Jain, Shawn\nJain, Joanne Jang, Angela Jiang, Roger Jiang,\nHaozhun Jin, Denny Jin, Shino Jomoto, Bil-\nlie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz\nKaiser, Ali Kamali, Ingmar Kanitscheider, Ni-\ntish Shirish Keskar, Tabarak Khan, Logan Kil-\npatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt\nKnight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic,\nGretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung,\nDaniel Levy, Chak Ming Li, Rachel Lim, Molly\nLin, Stephanie Lin, Mateusz Litwin, Theresa\nLopez, Ryan Lowe, Patricia Lue, Anna Makanju,\nKim Malfacini, Sam Manning, Todor Markov,\nYaniv Markovski, Bianca Martin, Katie Mayer, An-\ndrew Mayne, Bob McGrew, Scott Mayer McKin-\nney, Christine McLeavey, Paul McMillan, Jake\nMcNeil, David Medina, Aalok Mehta, Jacob\nMenick, Luke Metz, Andrey Mishchenko, Pamela\nMishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk,\nDavid Mély, Ashvin Nair, Reiichiro Nakano, Ra-\njeev Nayak, Arvind Neelakantan, Richard Ngo,\nHyeonwoo Noh, Long Ouyang, Cullen O’Keefe,\nJakub Pachocki, Alex Paino, Joe Palermo, Ash-\nley Pantuliano, Giambattista Parascandolo, Joel\nParish, Emy Parparita, Alex Passos, Mikhail\nPavlov, Andrew Peng, Adam Perelman, Filipe\nde Avila Belbute Peres, Michael Petrov, Hen-\nrique Ponde de Oliveira Pinto, Michael, Poko-\nrny, Michelle Pokrass, Vitchyr H. Pong, Tolly\nPowell, Alethea Power, Boris Power, Elizabeth\nProehl, Raul Puri, Alec Radford, Jack Rae,\nAditya Ramesh, Cameron Raymond, Francis\nReal, Kendra Rimbach, Carl Ross, Bob Rot-\nsted, Henri Roussez, Nick Ryder, Mario Saltarelli,\nTed Sanders, Shibani Santurkar, Girish Sas-\ntry, Heather Schmidt, David Schnurr, John\nSchulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav\nShyam, Szymon Sidor, Eric Sigler, Maddie\nSimens, Jordan Sitkin, Katarina Slama, Ian\nSohl, Benjamin Sokolowsky, Yang Song, Na-\ntalie Staudacher, Felipe Petroski Such, Na-\ntalie Summers, Ilya Sutskever, Jie Tang, Niko-\nlas Tezak, Madeleine B. Thompson, Phil Tillet,\nAmin Tootoonchian, Elizabeth Tseng, Preston\nTuggle, Nick Turley, Jerry Tworek, Juan Fe-\nlipe Cerón Uribe, Andrea Vallone, Arun Vi-\njayvergiya, Chelsea Voss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang,\nJonathan Ward, Jason Wei, CJ Weinmann, Ak-\nila Welihinda, Peter Welinder, Jiayi Weng, Lil-\nian Weng, Matt Wiethoff, Dave Willner, Clemens\nWinter, Samuel Wolrich, Hannah Wong, Lauren\nWorkman, Sherwin Wu, Jeff Wu, Michael Wu,\nKai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming\nYuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao\nZheng, Juntang Zhuang, William Zhuk, and Bar-\nret Zoph. 2024. Gpt-4 technical report.\nAmandine Quercia, Jamil Zaghir, Christian Lovis,\nand Christophe Gaudet-Blavignac. 2024. Med-\nfrenchmark, a small set for benchmarking gen-\nerative llms in medical french. Studies in Health\nTechnology and Informatics, 316:601–605.\nShruti Singh, Nandan Sarkar, and Arman Cohan.\n2024. Scidqa: A deep reading comprehension\ndataset over scientific papers.\nChengrui Wang, Qingqing Long, Meng Xiao,\nXunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi\nWang, and Yuanchun Zhou. 2024a. Biorag: A\nrag-llm framework for biological question reason-\ning.\nJinqiang Wang, Huansheng Ning, Yi Peng, Qikai\nWei, Daniel Tesfai, Wenwei Mao, Tao Zhu, and\nRunhe Huang. 2024b. A survey on large lan-\nguage models from general purpose to medical\napplications: Datasets, methodologies, and eval-\nuations.\nAn Yang, Anfeng Li, Baosong Yang, Beichen\nZhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, Chu-\njie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang,\nFeng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong\nTang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jing Zhou, Jingren\n"}, {"page": 12, "text": "Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin\nYang, Le Yu, Lianghao Deng, Mei Li, Mingfeng\nXue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu,\nRui Men, Ruize Gao, Shixuan Liu, Shuang Luo,\nTianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng\nRen, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu\nCui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. 2025. Qwen3 technical report.\nWeizhe Yuan, Pengfei Liu, and Graham Neu-\nbig. 2022. Can we automate scientific review-\ning? Journal of Artificial Intelligence Research,\n73:12863–12894.\nHye Sun Yun, Iain J. Marshall, Thomas A. Trikali-\nnos, and Byron C. Wallace. 2023. Appraising\nthe potential uses and harms of llms for medical\nsystematic reviews.\nMingqian Zheng,\nJiaxin Pei,\nLajanugen Lo-\ngeswaran, Moontae Lee, and David Jurgens.\n2024. When \"a helpful assistant\" is not really\nhelpful: Personas in system prompts do not im-\nprove performances of large language models.\n"}]}