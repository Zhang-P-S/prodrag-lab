{"doc_id": "arxiv:2601.21551", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.21551.pdf", "meta": {"doc_id": "arxiv:2601.21551", "source": "arxiv", "arxiv_id": "2601.21551", "title": "Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes", "authors": ["Yang Zhou", "Zhenting Sheng", "Mingrui Tan", "Yuting Song", "Jun Zhou", "Yu Heng Kwan", "Lian Leng Low", "Yang Bai", "Yong Liu"], "published": "2026-01-29T11:05:46Z", "updated": "2026-01-29T11:05:46Z", "summary": "Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \\method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.21551v1", "url_pdf": "https://arxiv.org/pdf/2601.21551.pdf", "meta_path": "data/raw/arxiv/meta/2601.21551.json", "sha256": "32a209efeb4af4882cea3f9eda86fede52e5a97659903b9644501935f0b4ca5d", "status": "ok", "fetched_at": "2026-02-18T02:20:10.723321+00:00"}, "pages": [{"page": 1, "text": "Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking\nUsing Medical Notes\nYang Zhou1*†, Zhenting Sheng2*, Mingrui Tan1, Yuting Song1, Jun Zhou1,\nYu Heng Kwan3,4, Lian Leng Low3,4, Yang Bai1†, Yong Liu1\n1Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore\n2Nanyang Technological University\n3National University of Singapore\n4Singapore General Hospital\nAbstract\nEffective clinical history taking is a foundational yet under-\nexplored component of clinical reasoning. While large lan-\nguage models (LLMs) have shown promise on static bench-\nmarks, they often fall short in dynamic, multi-turn diagnos-\ntic settings that require iterative questioning and hypothesis\nrefinement. To address this gap, we propose Note2Chat,\na note-driven framework that trains LLMs to conduct struc-\ntured history taking and diagnosis by learning from widely\navailable medical notes. Instead of relying on scarce and\nsensitive dialogue data, we convert real-world medical notes\ninto high-quality doctor-patient dialogues using a decision\ntree-guided generation and refinement pipeline. We then pro-\npose a three-stage fine-tuning strategy combining supervised\nlearning, simulated data augmentation, and preference learn-\ning. Furthermore, we propose a novel single-turn reasoning\nparadigm that reframes history taking as a sequence of single-\nturn reasoning problems. This design enhances interpretabil-\nity and enables local supervision, dynamic adaptation, and\ngreater sample efficiency. Experimental results show that our\nmethod substantially improves clinical reasoning, achieving\ngains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over\nGPT-4o.\nCode — https://github.com/zhentingsheng/Note2Chat\nIntroduction\nHistory taking and differential diagnosis are fundamental\nto clinical reasoning, forming the basis for understanding a\npatient’s condition and directing subsequent diagnostic and\ntherapeutic decisions. A thorough history typically encom-\npasses the chief complaint, history of present illness, review\nof systems, and general medical and social background, all\nof which collectively inform the generation and refinement\nof a differential diagnosis, a ranked list of plausible condi-\ntions grounded in the patient’s symptoms and risk factors.\nCentral to this process is multi-turn clinical history taking, a\ndynamic, interactive dialogue in which clinicians iteratively\nask targeted questions, interpret responses in context, and\nupdate diagnostic hypotheses step by step (Henderson, Tier-\n*These authors contributed equally.\n†Corresponding Authors.\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nI’ve been having chest pain \nthat comes and goes over the \nlast two days.\nCan you describe the pain?...\nIt's a sharp, stabbing pain…\nAny associated symptoms, \nlike shortness of breath…\n…\nSymptom \nCharacter\nChief Complaint:\nChest Pain\nHistory of Present Illness:\nAdult male with intermittent, \nsharp central chest pain \nstarting two days ago at \nrest. He reports …, but \ndenies … palpitations, \nsyncope, or dizziness.\nMedical Note\nMulti-Turn Clinical History Taking \nAssoc. \nSymptom \nAny headaches or dizziness…\nNo…\nEnough \nInfo?\nFigure 1: Multi-turn clinical history taking is the dynamic\nprocess of gathering information from a patient; structured\nmedical notes are the organized product, synthesizing that\nnarrative into a concise, standardized record.\nney, and Smetana 2012; Guyatt et al. 2015). This sequential\nreasoning demands broad medical knowledge, adaptability,\nand time—resources often constrained in high-volume care\nenvironments. Despite these challenges, history taking alone\ncan lead to accurate diagnoses in a majority of cases (Kuri-\nakose 2020; Nierenberg 2020), underscoring its diagnostic\nvalue. In this context, automated history-taking systems hold\nsignificant promise: by conducting structured, multi-turn in-\nterviews prior to clinical encounters, they can streamline\ninformation gathering, reduce cognitive load on clinicians,\nand enhance the completeness and consistency of diagnostic\nconversations, particularly in settings with limited medical\npersonnel.\nLarge language models (LLMs) have demonstrated impres-\nsive performance across a range of medical tasks, includ-\ning medical question answering, clinical note summariza-\ntion, and care plan generation (Cabral et al. 2024; Goh et al.\n2024; McDuff et al. 2025; Nori et al. 2023a,b, 2024). How-\never, these achievements are largely based on static, single-\nturn benchmarks, where models are provided with com-\nplete clinical vignettes and tasked with producing an an-\nswer without needing to interact or inquire further. Such set-\ntings fail to capture the sequential and exploratory nature\nof real-world diagnostic reasoning, which requires actively\ngathering missing information through dialogue. Recent\nbenchmark studies have underscored this limitation, show-\narXiv:2601.21551v1  [cs.CL]  29 Jan 2026\n"}, {"page": 2, "text": "ing that when LLMs are evaluated in full diagnostic con-\nversations, where they must initiate questions, adapt based\non responses, and iteratively refine hypotheses, their diag-\nnostic accuracy can drop significantly compared to single-\nturn tasks (Johri et al. 2025; Liu et al. 2024; Li et al. 2024;\nHager et al. 2024; Schmidgall et al. 2024). This performance\ngap indicates that, despite their medical knowledge, current\nLLMs lack the conversational competencies needed for ef-\nfective multi-turn diagnostic reasoning. In particular, they\noften fail to generate focused follow-up questions or to prior-\nitize clinically relevant details, constraining their usefulness\nin structured interviews (Goh et al. 2024; Nori et al. 2024).\nThese findings point to the need for dynamic, interaction-\noriented evaluations that better reflect the challenges of his-\ntory taking and conversational diagnosis.\nRecent efforts to enhance LLMs for clinical dialogue have\nexplored self-play simulation, agent-based workflows, and\nreinforcement learning (RL) strategies. For example, AMIE\n(Tu et al. 2025) introduced a simulated diagnostic envi-\nronment to improve history-taking dialogue, but relies on\nprivate datasets and models. DoctorAgent-RL (Feng et al.\n2025) has introduced RL fine-tuning to encourage LLMs to\nprogressively refine diagnoses through proactive question-\ning. Agent-based methods (Nori et al. 2025; Gatto et al.\n2025; Liu et al. 2025; Rose et al. 2025) assign different\nroles to separate LLMs, yet typically use general-purpose\nmodels not tailored for clinical reasoning. RL-based ap-\nproaches (Fansi Tchango et al. 2022; Sun et al. 2025) aim\nto improve diagnostic performance through fine-tuning, but\noften depend on rigid supervision or task-specific annota-\ntions, limiting adaptability. Despite their differences, most\nof these works prioritize final diagnosis accuracy and under-\nemphasize the quality and completeness of history taking.\nThey often overlook clinically important but non-diagnostic\ndetails, such as negative findings or symptom context, and\nare limited by the lack of large-scale, high-quality dialogue\ndata. Given the unique challenges of history taking, includ-\ning its exploratory nature and variation across clinical styles,\nthere is a clear need for scalable and generalizable training\nparadigms.\nIn this work, we address the challenges of clinical history\ntaking by focusing on efficient information gathering rather\nthan optimizing solely for diagnostic accuracy. Our objec-\ntive is to enable the model to extract as many relevant find-\nings as possible with minimal questioning, supporting con-\ncise and complete interviews grounded in clinical reasoning.\nTo this end, we propose Note2Chat, a novel framework\nthat leverages real-world medical notes, specifically the pri-\nmary diagnosis and history of present illness (HPI), as a su-\npervision signal. These notes capture clinician-curated sum-\nmaries of symptom relevance, temporal progression, and di-\nagnostic thinking, offering a rich and widely accessible re-\nsource for training. Compared to medical dialogues, clinical\nnotes are significantly more available, as they are routinely\ndocumented for care delivery and are less restricted by pri-\nvacy concerns. Moreover, they require no additional manual\nannotation and can be easily adapted to local protocols and\ninstitutional practices.\nOur framework consists of three core components designed\nto enhance LLMs for clinical history taking. First, we intro-\nduce a note-to-dialogue generation pipeline that converts\ndischarge notes into clinically meaningful doctor-patient\nconversations using decision tree-guided prompts, followed\nby refinement to ensure realism and comprehensive cov-\nerage of key findings. Using this pipeline, we construct a\ndataset comprising 8,944 synthetic dialogues, 67,077 suc-\ncessful rollouts, and 11,403 preference pairs across 4,972\npatients. Second, we propose a three-stage fine-tuning\nstrategy that combines supervised training on note-guided\ndialogues, data augmentation through simulated interac-\ntions, and direct preference optimization (DPO) (Rafailov\net al. 2023) to encourage concise and clinically effective\nconversations. Third, we introduce a single-turn reasoning\nparadigm that treats each dialogue turn as an independent\ndecision step, enabling the model to make context-aware, in-\nterpretable actions guided by conversation history and rea-\nsoning plans. This design improves follow-up questioning,\nstreamlines information gathering, and allows for explicit re-\nward shaping based on information gain and diagnostic rel-\nevance. Our contributions include:\n• We propose a note-driven dialogue generation and re-\nfinement pipeline to curate clinically grounded patient-\ndoctor interactions, resulting in a history-taking dataset\nacross 4,972 patients.\n• We propose a three-stage fine-tuning strategy and a\nsingle-turn reasoning paradigm that enhance LLMs for\nconcise, interpretable, and effective history taking.\n• Using Note2Chat, our fine-tuned LLM achieves state-\nof-the-art performance in both information gathering and\ndiagnosis, with relative gains of +57.53% and +42.86%\nover GPT-4o, respectively.\nRelated Work\nLLMs for medical applications: LLMs have demonstrated\nimpressive capabilities across a broad range of medical ap-\nplications, including question answering, clinical summa-\nrization, and care planning (Cabral et al. 2024; Goh et al.\n2024; McDuff et al. 2025; Nori et al. 2023a,b, 2024; Achiam\net al. 2023; Saab et al. 2024). Advanced reasoning mod-\nels such as DeepSeek-R1 (Guo et al. 2025) and Gemini\n2.5 (Comanici et al. 2025) continue to raise the bar with\nstronger generalization and reasoning performance on med-\nical benchmarks. Meanwhile, domain-specialized variants\nlike BioMistral (Labrak et al. 2024), HuatuoGPT-o1 (Chen\net al. 2025a), Med-R1 (Lai et al. 2025), and MedGemma\n(Sellergren et al. 2025) further tailor LLMs to clinical con-\ntexts by incorporating structured medical knowledge and\nfine-tuning on healthcare-specific data. Despite these ad-\nvances, most evaluations remain confined to static, single-\nturn settings in which models are presented with complete\ncase information and asked to produce a response. Such\nparadigms overlook the inherently dynamic and sequential\nnature of real-world clinical reasoning—particularly in tasks\nlike history taking and differential diagnosis, where suc-\ncess hinges on actively collecting missing information, ask-\n"}, {"page": 3, "text": "ing follow-up questions, and reasoning under uncertainty.\nRecent studies have shown that even state-of-the-art mod-\nels struggle in these interactive, multi-turn environments,\nwith diagnostic performance dropping significantly when\nrequired to reason step-by-step without full context (Johri\net al. 2025; Liu et al. 2024; Li et al. 2024; Hager et al.\n2024; Schmidgall et al. 2024). These limitations underscore\nthe need for new training and evaluation frameworks that\nsupport proactive, reasoning-driven dialogue and reflect the\ncomplexities of real clinical workflows.\nLLMs for multi-turn clinical conversation: Recent re-\nsearch has increasingly focused on enabling LLMs to par-\nticipate in multi-turn clinical conversations, particularly\nfor tasks such as history taking and differential diagno-\nsis. Several benchmarks (Johri et al. 2025; Li et al. 2024;\nSchmidgall et al. 2024; Wang et al. 2025; Chandra et al.\n2025; Fan et al. 2024) have been introduced to evaluate\nLLMs in interactive medical settings, offering useful tools\nfor assessing question-asking and reasoning abilities. Multi-\nagent frameworks that assign roles like history taker or di-\nagnostician to separate LLMs aims to improve the work-\nflow rather than LLMs’ clinical reasoning ability. RL-based\nmethods (Fansi Tchango et al. 2022; Sun et al. 2025) pro-\nmote proactive questioning and diagnostic refinement, but\ndepend on rigid, pre-defined state-action spaces that limit\nflexibility and generalizability. AMIE (Tu et al. 2025) marks\na notable advancement by training models in a self-play\ndiagnostic environment and extending to multimodal data\n(Saab et al. 2025), yet it relies on proprietary datasets and\nclosed-source models, limiting reproducibility and broader\nadoption. To address data scarcity, prior curation efforts\n(Tu et al. 2025; Fansi Tchango et al. 2022; Saley et al.\n2024; Chen et al. 2025b) generate training data from an-\nnotated dialogues, synthetic vignettes, or QA-style conver-\nsions, and typically use supervised fine-tuning (SFT) for\ntraining. DoctorAgent-RL (Feng et al. 2025) further applies\nRL fine-tuning to enhance question generation but remains\nconstrained by synthetic, limited training resources.\nNote2Chat\nWe introduce Note2Chat, a generalizable LLM training\nframework for clinically grounded, proactive history taking.\nBy leveraging medical notes as a natural and scalable super-\nvision source, our approach shifts the focus from diagnosis\nto high-quality information gathering, better aligning with\nthe exploratory nature of real-world clinical reasoning.\nProblem Setup. We formulate medical history taking as\na partially observable sequential decision-making process,\nwhere a doctor agent interacts with a simulated patient\ngrounded in clinical notes. The objective of the doctor agent\nis to elicit clinically relevant findings through follow-up\nquestions and ultimately produce a differential diagnosis.\nLet P denote the distribution over patient cases. Each case\nx\n∼\nP is defined as x\n=\n{dx, F, cc}, where dx is\nthe ground-truth diagnosis extracted from the note, F =\n{f1, . . . , fn} is the set of clinical findings extracted from\nthe History of Present Illness (HPI), and cc is the chief\ncomplaint, serving as the initial observation. At each turn\nt, the doctor agent observes a state st = {cc, ht}, where\nht = [(q1, r1), (q2, r2), . . . , (qt−1, rt−1)] is the accumulated\ndialogue history, where qt and rt are the doctor question and\npatient response, respectively. The agent selects an action\nat ∈A = Aask ∪Adiagnose, either asking a follow-up ques-\ntion qt or issuing a diagnostic prediction.\nIf the agent selects a question-asking action at ∈Aask,\na simulated patient grounded in x responds with rt\n∼\np(r | qt, x), and the dialogue history is then updated to in-\nclude the new exchange, resulting in the next state st+1 =\n{cc, ht+1}, where ht+1 = ht ∪{(qt, at)}. Alternatively,\nif at ∈Adiagnose, the interaction terminates and the doc-\ntor agent will predicta ranked list of K potential diagnoses\nˆyt = [ˆy(1)\nt\n, ˆy(2)\nt\n, . . . , ˆy(K)\nt\n] ⊂Y, where Y is the space\nof diagnostic labels. Each interaction forms a trajectory\nτ = (s0, a0, s1, a1, . . . , sT , aT ), ending when a diagnosis\nis made. The doctor agent learns a policy πθ(at | st), pa-\nrameterized by θ, governs decision-making. The learning\nobjective is to train the policy πθ to maximize the expected\nreward R(hT ), which evaluates the informativeness and ef-\nficiency of the dialogue: maxθ Ex∼P,πθ[R(τ)]. This can be\noptimized via supervised fine-tuning (SFT), reinforcement\nfine-tuning, or preference-based fine-tuning guided by note-\nderived supervision.\nData Curation Pipeline. We develop a data curation\npipeline to generate clinically grounded doctor-patient di-\nalogues from medical notes, consisting of three key com-\nponents. Finding extraction: We extract relevant medical\nfindings from the HPI section of discharge notes to construct\npatient vignettes, excluding downstream information such\nas lab results, treatments, and follow-up plans that would\nnot be known during history taking. These vignettes serve\nas the basis for generating patient responses and for evalu-\nating whether a model’s question can recover the findings\ndocumented in the original note. Decision tree-guided di-\nalogue generation: To ensure clinical relevance and align-\nment with diagnostic reasoning, we construct a decision tree\nthat maps findings to candidate diagnoses. This tree provides\na structured outline for guiding the LLM to generate task-\noriented dialogues that reflect realistic differential diagnosis\nworkflows. Critic and revision: While LLM-generated di-\nalogues are generally plausible, they may omit key findings\nor exhibit context leakage, where the doctor infers symp-\ntoms not yet revealed by the patient. To improve quality, we\nintroduce an LLM-based critic that identifies and corrects\nthese issues by adding missing questions and revising pre-\nmature inferences, significantly improving dialogue quality\nand increasing average symptom coverage.\nTo prepare the data for training and evaluation, we use\nICD-10 codes to select discharge notes from the MIMIC-IV\ndataset (Johnson et al. 2023), focusing on two major con-\ndition groups: heart failure, cellulitis, and their associated\ndiseases. This results in a diverse set of 10 clinically rele-\nvant conditions: Asthma, COPD, Cellulitis, Chronic venous\ninsufficiency, Deep vein thrombosis, Erysipelas, Heart Fail-\nure, Necrotising Fasciitis, Pneumonia, Trauma/hematoma.\n"}, {"page": 4, "text": "These conditions were selected based on clinical guidance\nto balance diagnostic challenge and feasibility, as they of-\nten present with overlapping symptoms that demand careful\nhistory taking to differentiate. To ensure data quality, we in-\nclude only notes with a clearly defined HPI section, a pri-\nmary diagnosis, and at least 100 words to guarantee suffi-\ncient clinical detail. Using GPT-4o for data processing, our\ncuration pipeline produces multi-turn dialogues averaging\n17.8 turns for 4,972 patients across 10 diseases, which are\nrandomly split into 4,472 for training and 500 for testing.\nThree-Stage Fine-Tuning Strategy. We propose a three-\nstage fine-tuning strategy to train LLMs for high-quality\nmulti-turn history taking and differential diagnosis. Cold\nstart with SFT: We initialize training using Qwen2.5-7B\n(Team 2024) as the base model. Using our note-guided di-\nalogues, we apply supervised fine-tuning to teach the LLM\nfoundational clinical reasoning and dialogue structure. The\nfine-tuned model plays the doctor role and interacts with a\nsimulated patient agent (Qwen2.5-32B), learning to follow\nan appropriate question-asking flow and identify relevant\nfindings. This stage establishes a basic starting policy for\nstructured clinical interactions.\nSelf-augmentation\nwith\ntrajectory\nsampling:\nNote-\nguided dialogues are often overly idealized, with each doc-\ntor question reliably eliciting a relevant response, which is\nunrealistic in practice. As a result, models trained solely on\nthese dialogues may overfit and struggle to generalize dur-\ning inference. To improve robustness, we simulate more di-\nverse and imperfect interactions by allowing the SFT-trained\ndoctor model to engage in self-play with the patient agent.\nFor each case, we roll out multiple dialogue trajectories and\nselect those that achieve correct differential diagnoses with\nthe highest recall (i.e., capturing the most documented find-\nings). These selected dialogues are added to the training cor-\npus, exposing the model to more natural conversation dy-\nnamics. This process yields 4,472 self-augmented dialogues\nfrom 67,077 successful rollouts.\nDirect preference optimization: While supervised fine-\ntuning offers a solid starting point, it does not explicitly\nteach the model to prefer concise, effective, and clinically\nsound conversations. To address this, we apply DPO to guide\nthe model toward preferred history-taking behaviors. For\neach case, we generate 15 dialogue candidates per case via\nself-play and assign a reward score to each, based on infor-\nmation recall, diagnostic accuracy, and dialogue efficiency.\nPreference pairs are then constructed by contrasting top-\nand bottom-ranked dialogues, and the model is optimized\nto favor high-quality interactions. This step strengthens the\nmodel’s ability to ask relevant questions, avoid unnecessary\nturns, and make timely, well-justified diagnoses.\nDialogue-level outcome reward: Designing an effective re-\nward function is critical to the success of DPO. We introduce\na dialogue-level reward function that leverages the medi-\ncal note as a silver-standard reference, explicitly capturing\nthree desirable criteria: (1) thorough information gathering,\n(2) concise and efficient dialogue, and (3) accurate differen-\ntial diagnosis. Formally, the reward for a dialogue trajectory\nτ is defined as:\nR(τ) = Recall+\nRecall\nRecallmax\n·\n\u0012\n1 −rank(dx, ˆyT )\nK\n\u0013\n−α · T\n2\n.\n(1)\nHere, Recall = |Fτ ∩F|\n|F|\n, measures the fraction of clinically\nrelevant findings F (from the HPI) that are successfully\nelicited during the dialogue τ, directly rewarding thorough\ninformation gathering. Recallmax denotes the highest recall\nachieved across all generated dialogues. The second term as-\nsesses diagnostic accuracy by checking whether the ground-\ntruth diagnosis dx appears within the top-K predicted diag-\nnoses ˆyT ; higher rankings yield higher scores. In this work,\nwe set K = 5 and assign R(τ) = 0 when the correct diagno-\nsis falls outside the top-K. Importantly, this term is weighted\nby the ratio\nRecall\nRecallmax , ensuring that high diagnostic accuracy\nonly contributes meaningfully to the reward if sufficient rel-\nevant information has been collected, preventing reward in-\nflation from lucky guesses. The final term α·T\n2\npenalizes un-\nnecessarily long dialogues to encourage efficiency in history\ntaking, where T represents the total number of turns, and α\nis a scaling coefficient that balances the penalty relative to\nthe other reward components.\nTo generate preference pairs for DPO, we roll out 15 di-\nalogue candidates per patient through self-play and com-\npute the mean (µ) and standard deviation (σ) of their re-\nward scores. Dialogues with scores above µ + σ are labeled\nas high-quality, while those below µ −σ are considered\nlow-quality. For each high-quality trajectory, we sample up\nto two low-quality ones to form training pairs. This results\nin 11,403 dialogue-level preference pairs used to fine-tune\nthe model toward more informative, accurate, and efficient\nmulti-turn clinical conversations.\nMulti-Turn History Taking via Single-Turn Reasoning.\nWhile multi-turn DPO aims to improve conversational mod-\nels, it presents inherent limitations. A primary challenge\nis the difficulty of controlling long conversational rollouts,\nwhere early errors can compound and lead the dialogue off-\ntrack. The preference signal in conventional multi-turn ap-\nproaches is applied to the entire trajectory, offering only\ncoarse supervision that makes it difficult to identify and cor-\nrect specific suboptimal turns. Furthermore, without a clear\nreward signal for a process like reaching a diagnosis, multi-\nturn models may struggle with poorly defined stopping cri-\nteria, leading to inefficient and unnecessarily long dialogues.\nTask reformulation: To address these challenges, we pro-\npose a novel paradigm that reframes multi-turn history tak-\ning as a sequence of single-turn reasoning problems. Our\ncore insight is that a multi-turn conversation can be modeled\nas a Markov Decision Process. By enriching the state rep-\nresentation at each turn to include the accumulated memory\nof prior turns and collected findings, we enable the model to\nmake optimal, context-aware decisions from any intermedi-\nate step. This approach allows for stronger local supervision\nand improved sample efficiency, as the model’s questioning\nstrategy can dynamically adapt throughout the dialogue. At\neach step, the model should perform structured reasoning\n"}, {"page": 5, "text": "through a thought process that includes memory summa-\nrization, clinical planning, and action generation. This\nensures that each decision is based on the current state alone,\ndecoupling the model’s performance from the unpredictable\nnature of long-term conversational trajectories.\nSingle-turn reasoning: To achieve this, we insert a struc-\ntured reasoning block between each round of patient–doctor\ndialogue, generated by the doctor agent during inference\nbut hidden from the patient agent. This block, delimited by\n<think> and </think> tokens, comprises two key com-\nponents: Summary: A concise, structured summary of the\nconversation history, which is updated with each new pa-\ntient response. This serves as the model’s working memory\nof the patient’s condition and collected symptoms. Planning:\nA clear clinical rationale for the next action. If the action is\na follow-up question, the plan explains how it helps to con-\nfirm or rule out specific diagnoses. If the action is a diagnos-\ntic decision, the plan justifies the ranking of candidate dis-\neases based on the symptoms gathered. This structured ap-\nproach enables more granular and flexible preference learn-\ning, where individual turns can be rewarded even if the full\ntrajectory is suboptimal. The explicit reasoning process also\nenhances both interpretability and reasoning quality, as each\ndecision is grounded in a transparent clinical rationale. A\nconcrete example for the proposed single-turn reasoning is\nshown below:\nSingle-Turn Reasoning Example\nDoctor\n<think>\nSummary: Turn 0: The patient reported having a\nfever and shortness of breath.\nPlan: I need to understand when the symptoms\nstarted and how they’ve progressed to decide what\nmight be causing them.\n</think>\nCan you tell me when your symptoms started and\nhow they’ve changed?\nPatient (Turn 1)\nThe symptoms started a few days ago and have been\ngetting worse.\nDoctor\n<think>\nSummary: Turn 0: The patient reported fever and\nshortness of breath.\nTurn 1: The patient said the symptoms started a few\ndays ago and have worsened.\nPlan: I need to ask more about the shortness of\nbreath to understand its nature and possible causes.\n</think>\nCan you tell me more about the shortness of breath?\nHas anything helped or made it worse?\nTurn-level process reward: The single-turn reasoning\nparadigm allows for a simple, interpretable reward function\nat each step of interaction:\nRST(st−1, st) =\n\u001aI[ft ∈st \\ st−1],\nif at ∈Aask\nRecallt ·\n\u00001 −rankt\nK\n\u0001\n,\nif at ∈Adiagnose\n(2)\nHere, I[ft ∈st\\st−1] indicates whether a new relevant find-\ning was acquired at turn t; Recallt = |Ft∩F|\n|F|\nmeasures the\nproportion of ground-truth findings F recovered so far; and\nrankt = rank(dx, ˆyt) gives the rank of the correct diagnosis\nin the top-K predictions. We assign RST(st−1, st) = 0 when\nthe correct diagnosis falls outside the top-K. The agent is re-\nwarded for acquiring new information and ranking the cor-\nrect diagnosis higher, with rewards scaled by the amount of\nuseful information gathered. Unlike multi-turn approaches,\nthis setup makes question-asking and diagnosis actions di-\nrectly comparable, enabling the model to learn when to stop\nasking and make a prediction. The single-turn reasoning\nparadigm decomposes complex dialogues into independent,\ncontext-aware decisions, allowing for precise supervision,\ninterpretable reasoning, and verifiable rewards. This struc-\nture supports flexible preference learning and more accurate\ndiagnoses, even when full dialogues are noisy or imperfect.\nSingle-turn data preparation: Starting from our self-\naugmented dialogue dataset, we decompose full dialogues\ninto individual turns and use Qwen2.5-32B to generate a\nstructured reasoning block for each of them. Following the\nmulti-turn DPO setup, we adopt a multi-stage training strat-\negy: first fine-tuning an LLM on the single-turn augmented\ndata, then using the model to roll out 10 candidate responses\nper turn by interacting with a simulated patient agent. After\nfiltering out low-quality samples, we retain 80,537 context-\naware single-turn interactions. To construct preference data\nfor single-turn DPO, we contrast the highest- and lowest-\nreward responses (as defined in Eq. 2), yielding 95,811 turn-\nlevel preference pairs.\nExperiments\nWe conduct a series of experiments to evaluate the perfor-\nmance of LLMs in medical history taking and differential di-\nagnosis. Our evaluation begins by comparing Note2Chat,\ntrained with either multi-turn (Note2Chat-MT) or single-\nturn (Note2Chat-ST) DPO strategies, against a diverse\nset of baseline LLMs. These include: Proprietary models:\nGPT-4o (Achiam et al. 2023), o4-mini, Gemini-2.5 (Co-\nmanici et al. 2025); Public open-source models: DeepSeek-\nR1 (Guo et al. 2025), Qwen2.5 (Team 2024), Qwen3\n(Team 2025); and Domain-specific models: HuatuoGPT-\no1(Chen et al. 2025a), MedGemma (Sellergren et al. 2025),\nDoctorAgent-RL (Feng et al. 2025). We then analyze the\nkey factors that contribute to effective history taking across\nclinically meaningful symptom categories. Finally, we vali-\ndate our models by comparing with practicing clinicians on\na held-out test set. Our experiments are designed to answer\nthe following research questions: (1) How well do existing\nLLMs perform in medical history taking? (2) Can we im-\nprove LLMs to proactively ask follow-up questions and au-\ntonomously decide when to diagnose? (3) To what extent can\nfine-tuning and preference learning narrow the performance\ngap? (4) How does the performance of Note2Chat com-\npare to that of human clinicians?\nExperiments Setup. Following CRAFT-MD (Johri et al.\n2025) and its prompting strategy, we simulate a patient\n"}, {"page": 6, "text": "Model\nF1\nRecall\nPrecision\nTop-1\nTop-2\nTop-3\n#Turn\nGPT-4o\n29.2\n33.2\n30.5\n49.0\n61.4\n67.6\n22.9\no4-mini\n23.0\n28.7\n21.9\n47.6\n60.0\n67.0\n27.0\nGemini-2.5-flash\n26.6\n35.5\n26.7\n51.4\n66.2\n73.0\n31.9\nQwen2.5-7B-Instruct\n19.6\n15.7\n33.0\n38.8\n54.8\n63.2\n10.3\nQwen3-8B\n17.9\n13.8\n34.1\n33.4\n46.6\n55.2\n8.9\nDeepSeek-R1-0528-Qwen3-8B\n29.6\n34.0\n32.7\n37.2\n51.6\n61.2\n23.4\nHuatuoGPT-o1-8B\n0.2\n0.1\n1.1\n19.4\n33.0\n42.8\n2.02\nMedGemma-4B-it\n27.2\n31.6\n28.0\n40.6\n55.2\n62.2\n23.4\nMedGemma-27B-text-it\n27.9\n31.4\n30.1\n52.8\n66.2\n71.4\n21.4\nDoctorAgent-RL\n28.4\n35.1\n27.5\n35.6\n-\n-\n26.4\nNote2Chat-MT\n43.8\n55.4\n41.8\n62.0\n78.2\n82.6\n27.5\nNote2Chat-ST\n46.1\n46.2\n54.5\n70.0\n81.2\n84.4\n17.3\nTable 1: History taking and diagnosis performance (%) across different models (Best, Second Best)\nMode\nModel\nF1\nRecall\nPrecision\nTop-1\nTop-2\nTop-3\nAvg.\n∆\n#Turn\nMT\nQwen2.5-7B-Instruct\n19.6\n15.7\n33.0\n38.8\n54.8\n63.2\n37.5\n-\n10.3\n+SFT\n32.6\n30.1\n44.9\n53.0\n66.0\n73.0\n49.9\n+12.4\n14.1\n+SFT+Self-Aug\n40.6\n39.2\n50.1\n62.8\n75.0\n81.6\n58.2\n+20.7\n15.8\n+SFT+Self-Aug+DPO\n43.8\n55.4\n41.8\n62.0\n78.2\n82.6\n60.6\n+23.1\n27.5\nST\n+SFT\n35.4\n37.5\n40.7\n54.8\n65.0\n70.4\n50.6\n+13.1\n19.8\n+SFT+Self-Aug\n41.4\n44.8\n45.8\n60.8\n72.4\n75.4\n56.8\n+19.3\n20.5\n+SFT+Self-Aug+DPO\n46.1\n46.2\n54.5\n70.0\n81.2\n84.4\n63.7\n+26.2\n17.3\nTable 2: Ablation study showing the impact of each component in our Note2Chat framework (Best, Second Best).\nagent using Qwen2.5-32B, which interacts with the eval-\nuated LLMs acting as doctor agents. The doctor agent is\ntasked with asking relevant follow-up questions to elicit key\nclinical findings and terminating the conversation once suf-\nficient information has been gathered. After the interview,\nthe doctor outputs a ranked list of potential diagnoses for\ndifferential diagnosis. To evaluate performance, we use a\nQwen2.5-32B model to assess the dialogue. It checks how\nmany ground-truth findings from the note are successfully\nelicited in the conversation and computes the rank of the true\ndiagnosis within the predicted list. We report the precision,\nrecall, F1 scores, and Top-K accuracy as our primary eval-\nuation metrics. Detailed metric definitions, training setups,\nincluding hyperparameters are provided in the Appendix.\nAll evaluations are conducted on our processed subset of the\nMIMIC-IV dataset (Johnson et al. 2023), with scope con-\nstrained by computational and cost considerations.\nMain Results. As shown in Table 1, existing LLMs strug-\ngle with effective medical history taking, which is con-\nsistent with prior studies (Johri et al. 2025; Li et al. 2024).\nAmong them, proprietary LLMs like GPT-4o and Gemini-\n2.5-flash perform best, achieving top-1 diagnostic accura-\ncies of 49.0% and 51.4%, respectively. In contrast, public\nmodels such as Qwen2.5-7B-Instruct and Qwen3-8B show\nmuch weaker performance, with F1 scores below 20% and\nTop-1 accuracies under 40%. DeepSeek-R1-0528-Qwen3-\n8B is the strongest open-source general model, reaching an\nF1 of 29.6% and Top-1 accuracy of 37.2%. Interestingly,\nmodels like Gemini-2.5-flash and o4-mini engage in long\nconversations (over 27 turns on average) but still achieve\nlow recall, indicating a lack of ability to ask clinically rel-\nevant questions. This highlights the gap between conversa-\ntional fluency and clinically meaningful reasoning.\nSeveral domain-specific medical LLMs were also evalu-\nated. MedGemma-4B-it and MedGemma-27B-text-it per-\nform relatively well in diagnosis (Top-1: 40.6% and 52.8%),\nlikely benefiting from medical pretraining. In contrast,\nHuatuoGPT-o1-8B performs poorly across all metrics, es-\npecially in history taking (F1: 0.2%), as it fails to ask\nfollow-up questions and relies solely on the chief complaint.\nDoctorAgent-RL, despite being trained on history taking di-\nalogues, is limited by its design to predict only a single diag-\nnosis. It performs reasonably in information elicitation (F1:\n28.4%) but fails to generalize to our setting. This is likely\ndue to a domain mismatch, as it was trained on informal on-\nline consultations, which lack the structured, standardized\nsymptom descriptions (e.g., onset, location, timing) found\nin clinical notes.\nFine-tuning significantly improves both history taking\nand diagnosis. Our proposed models, Note2Chat-MT\nand Note2Chat-ST, consistently outperform all base-\nlines. Compared to the base model Qwen2.5-7B-Instruct,\nNote2Chat-ST achieves a 26.5-point absolute gain in\nF1 (from 19.6% to 46.1%), an 135.2% relative improve-\nment, and a 31.2-point gain in Top-1 accuracy (from 38.8%\nto 70.0%). Note2Chat-MT achieves the highest recall\n(55.4%) but at the cost of longer dialogues (avg. 27.5 turns).\nIn contrast, Note2Chat-ST delivers the best overall per-\nformance across F1, precision, and Top-K accuracy, while\nusing fewer turns (17.3 on average). This demonstrates\nthe effectiveness of the proposed Note2Chat framework\nin gathering clinically relevant information efficiently and\nmaking accurate differential diagnosis.\n"}, {"page": 7, "text": "0\n20\n40\n60\n80\n100\nRecall (%)\nHis\nOnset\nExac\nTime\nAssoc\nRad\nSev\nChar\nSite\nGPT4-o\nQwen2.5-7B\nSFT-MT\nNote2Chat-MT\nNote2Chat-ST\nFigure 2: Recall across symptom categories.\nAblation Study. We assess the contribution of each com-\nponent in our Note2Chat framework for medical his-\ntory taking. As shown in Table 2, both multi-turn (MT)\nand single-turn (ST) models benefit substantially from\nfine-tuning. Applying SFT alone yields notable gains over\nthe base Qwen2.5-7B-Instruct model, improving average\nscores by 12–13 points. However, SFT alone only makes\nthe model competitive with top-performing LLMs, suggest-\ning that simply memorizing dialogue patterns is insufficient\nfor effective history taking. Introducing self-augmented, im-\nperfect trajectories significantly enhances recall and top-\nK accuracy, highlighting the value of diverse training sig-\nnals. Further applying DPO consistently improves perfor-\nmance across both paradigms. With the full pipeline (SFT\n+ Self-Aug + DPO), Note2Chat-MT achieves the high-\nest recall (55.4%) but requires longer dialogues (27.5 turns).\nIn contrast, Note2Chat-ST outperforms in all other met-\nrics while using fewer turns (17.3), demonstrating the effi-\nciency and effectiveness of the proposed single-turn reason-\ning framework for proactive and accurate history taking.\nAnalysis. To better understand the factors underlying ef-\nfective medical history taking and how our Note2Chat\nframework improves performance, we break down recall\nscores by clinically meaningful symptom categories based\non the SOCRATES mnemonic (Mahbubani 2023) (Site, On-\nset, Character, Radiation, Associated symptoms, Timing,\nExacerbating/relieving factors, Severity) along with an ad-\nditional History category. These categories capture essential\ndimensions of structured symptom characterization in clin-\nical history taking. As shown in Figure 2, even powerful\nproprietary models like GPT-4o underperform on essential\naspects such as Site (13.6%) and Severity (10.1%), high-\nlighting a gap in alignment with structured clinical inquiry.\nSFT yields moderate gains over the base Qwen2.5-7B model\nacross all categories. In contrast, both Note2Chat-MT\nand Note2Chat-ST achieve consistently higher recall\n0\n20\n40\n60\n80\n100\nRecall / Accuracy (%)\nHis\nSev\nExac\nTime\nAssoc\nRad\nOnset\nSite\nTop-3\nTop-2\nTop-1\n#Turn\nNote2Chat\nClinician\n0\n5\n10\n15\n20\n25\n30\n# Turns\nFigure 3: Comparison of history taking performance be-\ntween model and clinician.\nacross all categories, particularly excelling in Onset, Ra-\ndiation, and History. Notably, Note2Chat-MT reaches\nthe highest overall recall by leveraging longer interactions\n(27.5 turns on average), demonstrating its effectiveness in\ninformation gathering.\nComparison with Clinicians. Finally, we compare our\nmethod with a practicing clinician on a small held-out\ntest set of 20 patient cases across 10 diseases, constrained\nby available resources. As shown in Figure 3, our model\nachieves comparable performance in both diagnosis accu-\nracy and information gathering. Notably, it mirrors the\nclinician’s behavior in eliciting clinically meaningful\nsymptoms. While this limited-scale validation with simu-\nlated patients is far from conclusive, it highlights the poten-\ntial of LLMs for supporting real-world history taking.\nConclusion\nWe have presented Note2Chat, a note-driven framework\nfor training LLMs to perform clinically effective history tak-\ning and differential diagnosis. By leveraging medical notes\nas silver-standard supervision, our approach enables models\nto ask relevant follow-up questions, prioritize key findings,\nand determine when to conclude the conversation. Through\na multi-stage training pipeline combining supervised fine-\ntuning, self-augmented trajectory sampling, and prefer-\nence optimization, Note2Chat achieves substantial gains\nin both information elicitation and diagnostic accuracy.\nOur proposed single-turn reasoning paradigm enables fine-\ngrained, verifiable supervision at each step of the dialogue.\nThis design not only improves transparency and adaptabil-\nity but also outperforms multi-turn baselines with fewer\ndialogue turns. Empirical results show that Note2Chat\nconsistently outperforms both general-purpose and medical-\ndomain LLMs. Additionally, clinician comparisons show\npromising alignment in symptom gathering, suggesting real-\nworld applicability for AI-assisted history taking.\n"}, {"page": 8, "text": "Acknowledgements\nThis research/project is supported by the National Re-\nsearch Foundation Singapore under the AI Singapore Pro-\ngramme (AISG Award No: AISG2-TC-2023-013). This re-\nsearch is also supported by the SingHealth Duke-NUS\nAcademic Medical Centre (AMC) – A*STAR Healthcare\nTranslation Partnership (HTP) grant No. I24D1AG022 and\nI24D1AG085.\nAppendix\nEvaluation Metrics\nLet F = {f1, . . . , fn} be the set of ground-truth clinical\nfindings extracted from the note, and let FT ⊆F denote\nthe subset successfully elicited by the doctor agent through\ndialogue up to turn T. Let ˆyT = [ˆy(1)\nT , . . . , ˆy(K)\nT\n] ⊂Y be\nthe top-K ranked list of diagnoses predicted at the end of\nthe interaction, and dx ∈Y the ground-truth diagnosis.\n• Precision:\nPrecision = |FT ∩F|\nT\nMeasures the proportion of relevant findings elicited out\nof all questions asked.\n• Recall:\nRecall = |FT ∩F|\n|F|\nMeasures the proportion of ground-truth findings that\nwere successfully elicited by the agent.\n• F1 Score:\nF1 = 2 · Precision · Recall\nPrecision + Recall\nHarmonic mean of precision and recall, reflecting overall\nelicitation effectiveness.\n• Top-K Accuracy:\nTop-K Accuracy = I[dx ∈ˆyT ]\nIndicates whether the ground-truth diagnosis appears in\nthe top-K predictions.\nImplementation Details\nWe fine-tune the Qwen2.5-7B-Instruct model in two stages:\nsupervised fine-tuning (SFT) followed by Direct Preference\nOptimization (DPO). To improve training efficiency and\nadaptability, we apply Low-Rank Adaptation (LoRA) (Hu\net al. 2022), using a rank of 8, scaling factor of 32, and a\ndropout rate of 0.05. For SFT, the model is trained for 3\nepochs with a total batch size of 128 and no gradient ac-\ncumulation. We use a cosine learning rate scheduler with a\nbase learning rate of 5×10−5 and a warmup ratio of 0.1. For\nDPO, we continue training for 2 epochs with the same batch\nsize. The learning rate is set to 1 × 10−5, also scheduled\nwith cosine decay and a warmup ratio of 0.1. All experi-\nments are conducted on a server equipped with 8 NVIDIA\nH200 GPUs.\n"}, {"page": 9, "text": "LLM\nInformation Control↑\nFactual Conflict↓\nQwen2.5-7B\n81.54\n0.37\nQwen2.5-32B\n82.66\n0.29\nTable 3: Evaluation of the patient agent reliability.\nPatient Agent Evaluation\nWe evaluate the faithfulness and robustness of the simulated\npatient agent following protocols from prior work (Feng\net al. 2025). Specifically, we assess: (1) Information Control:\nWhether the patient only reveals information in response to\ndoctor queries; and (2) Factual Consistency: Whether re-\nsponses align with the underlying medical note. Using GPT-\n4o as the evaluator, we analyze dialogues between a naive\nfine-tuned doctor agent and patient agents instantiated with\nQwen2.5-7B and 32B. As shown in Table 3, both models\nperform reliably, with minimal information leakage and high\nfactual consistency. Qwen2.5-32B shows slightly better per-\nformance and is thus selected as the patient agent in our ex-\nperiments.\nCase Study\nTo demonstrate the effectiveness of the proposed single-\nturn reasoning paradigm, we compare dialogues generated\nby Note2Chat-MT and Note2Chat-ST on the same\npatient case. Both models successfully elicit key findings\nand make correct diagnoses. Note2Chat-MT tends to ask\nbroader questions about medical history and past treatments,\nwhich, while relevant, are more generic and less contex-\ntually grounded. In contrast, Note2Chat-ST better con-\ntrols the conversation flow by recalling prior patient re-\nsponses and asking focused, context-aware follow-up ques-\ntions. Beyond improved relevance, the single-turn paradigm\nenhances interpretability. Each turn includes an updated\nmemory summary and a clinical reasoning plan, offering\ntransparency that aligns with clinical expectations and sup-\nports real-world adoption.\nNote2Chat-MT\nPatient (Turn 0): Hi, doctor. I’ve been having\npainful pressure on the right side of my chest under\nmy right breast since yesterday, and I’ve also been\nfeeling feverish with some really bad throat pain.\nDoctor: I’m sorry you’re feeling this way. Can you\ntell me more about your chest pain? How does it\nfeel exactly, and have you noticed anything else\nunusual along with it?\nPatient (Turn 1): The chest pain is on the right\nside, under my breast, and it doesn’t go anywhere\nelse. It’s been constant. I also have a sore throat\nwith white spots on my tongue.\nDoctor: Have you had any tests done or imaging\nrecently related to these symptoms?\nPatient (Turn 2): I had a negative rapid strep\ntest and an unremarkable chest x-ray at the other\nhospital.\nDoctor: Do you have any past medical conditions\nthat might be relevant here?\nPatient (Turn 3): Yes, I have primary sclerosing\ncholangitis that came back after a liver transplant,\nulcerative colitis, and I have chronic abdominal pain\nmanaged with a narcotics contract.\nDoctor: Have you been hospitalized or treated\nbefore for similar symptoms?\nPatient (Turn 4): I’m not sure.\nDoctor: Do you recall if you were prescribed any\nmedications for the chest pain or throat issues?\nPatient (Turn 5): I’m not sure.\nDoctor: Have you had any recent trauma or injuries\nthat might relate to your chest pain?\nPatient (Turn 6): No, I haven’t had any recent\ntrauma or injuries.\nDoctor: Have you noticed any changes in your\nbowel habits, such as diarrhea or constipation?\nPatient (Turn 7): I haven’t had much of a bowel\nmovement, but I did have one loose stool earlier\ntoday in the ED.\nDoctor: Aside from the chest pain and throat symp-\ntoms, have you experienced any other symptoms\nlike fever, chills, nausea, vomiting, or difficulty\nbreathing?\nPatient (Turn 8): I’ve been feeling feverish and\nhave some abdominal pain, but no nausea, vomiting,\nor difficulty breathing.\nDoctor: Preliminary diagnoses:\n1. Pneumonia\n2. Gastroesophageal Reflux Disease (GERD)\n3. Acute Coronary Syndrome\n4. Musculoskeletal Chest Pain\n5. Esophageal Spasm\nNote2Chat-ST\nPatient (Turn 0): Hi, doctor. I’ve been having\npainful pressure on the right side of my chest under\nmy right breast since yesterday, and I’ve also been\nfeeling feverish with some really bad throat pain.\nDoctor:\n<think>\nSummary: Turn 0: The patient reported experienc-\ning painful pressure on the right side of their chest\nbeneath the right breast since yesterday, along with\nsymptoms of fever and severe throat pain.\nPlan: I need to gather more details about the onset\nand progression of the chest pain. (further reasoning\nomitted for brevity)\n</think>\nI’m sorry you’re feeling this way. Can you tell me\na bit more about your chest pain? When did it start\nand has anything changed recently?\nPatient (Turn 1): The chest pain started about four\ndays ago, and it’s been constant, not getting better\n"}, {"page": 10, "text": "or worse. It doesn’t go anywhere else in my body.\nDoctor:\n<think>\nSummary: Turn 0: <Skipped. Same with\nthe past summary.>.\nTurn 1: The patient has experienced constant chest\npain for four days, with no significant changes in\nseverity or spread of the pain.\nPlan: I need to gather more information about the\npatient’s overall health. (further reasoning omitted\nfor brevity)\n</think>\nDo you have any past medical conditions that I\nshould know about?\nPatient (Turn 2): Yes, I have primary sclerosing\ncholangitis, which came back after a liver transplant,\nulcerative colitis, and I have chronic abdominal pain\nmanaged with a narcotics contract.\nDoctor:\n<think>\nSummary: Turn 0–1: <Skipped. Same with\nthe past summary.>\nTurn 2: The patient has primary sclerosing cholan-\ngitis that recurred post-liver transplant, ulcerative\ncolitis, and experiences chronic abdominal pain\nmanaged under a narcotics contract.\nPlan: I need to gather more details about the throat\npain. (further reasoning omitted for brevity)\n</think>\nI understand. And what about your throat? You\nmentioned it hurts a lot. Can you tell me more about\nthat?\nPatient (Turn 3): My throat is really sore, and I\nnoticed white patches on my tongue.\nDoctor:\n<think>\nSummary: Turn 0–2: <Skipped. Same with\nthe past summary.>\nTurn 3: The patient reported severe soreness in the\nthroat accompanied by the observation of white\npatches on the tongue.\nPlan: I need to gather more information about\nthe fever and any additional symptoms the patient\nmight be experiencing. (further reasoning omitted\nfor brevity)\n</think>\nThank you for clarifying. Besides the chest pain\nand throat discomfort, have you noticed any other\nsymptoms like fever or anything else unusual?\nPatient (Turn 4): I’ve been feeling feverish, really\ntired, and I have some abdominal pain that’s hard\nto pinpoint, mostly on the right side. I also had one\nepisode of loose stool today.\nDoctor:\n<think>\nSummary: Turn 0–3: <Skipped. Same with\nthe past summary.>\nTurn 4: The patient is experiencing chest pain, throat\ndiscomfort, fever, fatigue, right-sided abdominal\npain, and one episode of loose stool.\nPlan:\nAsking about changes in bowel habits or\ndifficulty passing stools can help identify if there’s\nan issue with the digestive tract that might be\ncontributing to these symptoms. (further reasoning\nomitted for brevity)\n</think>\nHave you noticed any changes in your bowel habits\nor had difficulty passing stools recently?\nPatient (Turn 5): I haven’t had any problems\npassing stools, but I did have one episode of loose\nstool today.\nDoctor:\n<think>\nSummary: Turn 0–4: <Skipped. Same with\nthe past summary.>\nTurn 5: The patient reported no difficulties in\npassing stools regularly but noted one episode of\nloose stool on the day of the consultation.\nPlan: I need to gather more information about\nthe patient’s current medication regimen. (further\nreasoning omitted for brevity)\n</think>\nThank you for sharing that. Could you tell me\nmore about any medications or treatments you’re\ncurrently taking?\nPatient (Turn 6): I’m on a narcotics contract for\nmanaging my chronic abdominal pain.\nDoctor:\n<think>\nSummary: Turn 0–5: <Skipped. Same with\nthe past summary.>\nTurn 6:The patient is currently on a narcotics\ncontract for managing their chronic abdominal pain.\nPlan: I need to inquire about any recent changes in\nthe patient’s bowel habits or the presence of blood.\n(further reasoning omitted for brevity)\n</think>\nHave you noticed any changes in your bowel habits\nrecently, or have you seen any blood in your stool?\nPatient (Turn 7): I haven’t seen any blood in my\nstool, but I did have one episode of loose stool\ntoday.\nDoctor:\n<think>\nSummary: Turn 0–6: <Skipped. Same with\nthe past summary.>\nTurn 7: The patient reported having one episode\nof loose stool but did not observe any blood in the\nstool.\nPlan: I need to clarify if there have been any\nchanges in the patient’s appetite or weight. (further\nreasoning omitted for brevity)\n</think>\nHave you had any changes in your appetite or\n"}, {"page": 11, "text": "weight recently?\nPatient (Turn 8): I haven’t eaten much in at least\ntwo days.\nDoctor:\n<think>\nSummary: Turn 0–7: <Skipped. Same with\nthe past summary.>\nTurn 8: The patient has not consumed food in at\nleast two days, indicating a recent change in their\nappetite.\nPlan: Given the patient’s symptoms and medical\nhistory, I’m considering several preliminary diag-\nnoses: Pneumonia, Acute Coronary Syndrome, Gas-\ntroenteritis, Non-specific Viral Syndrome,Chronic\nPain Flare-Up.\n</think>\nPreliminary Diagnoses:\n1. Pneumonia\n2. Acute Coronary Syndrome\n3. Gastroenteritis\n4. Non-specific Viral Syndrome\n5. Chronic Pain Flare-Up\nPrompt Details\nIn this section, we provide the detailed prompts used for data\ncuration, doctor and patient agent construction, single-turn\nreasoning generation, and history taking evaluation.\nPrompts for History Taking Data Curation\nDecision Tree Generation\nBased on the provided medical note below, generate\na structured decision tree for differential diagno-\nsis. Terminate branches with potential diagnoses,\nincluding the final diagnosis as the confirmed\ncondition. Prioritize clinical relevance and logical\nprogression. Ensure the decision tree incorporates\nall symptoms mentioned in the history of present\nillness (HPI) to the greatest extent possible.\nStructure the output in JSON format with the\nfollowing keys:\ntree: A nested object where each node contains:\ncriteria: Short clinical question or finding (e.g.,\n’Fever present?’).\nbranches: Sub-nodes for ’yes’/’no’ responses (if\napplicable).\ndiagnoses: List of potential diagnoses (if terminal\nnode), each with:\ncondition: Diagnosis name.\nconfidence: Likelihood (e.g., ’high’, ’moderate’,\n’low’).\nis final: Boolean indicating if it matches the final\ndiagnosis.\nMedical Note:\nChief Complaint: {chief complaint}\nHPI: {hpi}\nFinal Diagnosis: {diagnosis}\nProvide only the JSON output, without additional\ntext.\nDialogue Generation\nGenerate a history-taking dialogue between a doctor\nand a patient based on the decision tree and the med-\nical note. The conversation opens with a natural and\nconcise initial statement from the patient, describ-\ning clear symptoms from Patient’s chief complaint\nor only one symptom History of Present Illness, and\nconcludes with five preliminary differential diag-\nnoses—ranked by clinical likelihood—provided by\nthe doctor.\nTo make the conversation clinically convincing,\nthe doctor knows nothing about the patient in ad-\nvance and the doctor should only learn informa-\ntion through the patient’s responses, ask appropri-\nate follow-up questions in plain, patient-friendly lan-\nguage, and actively compare, rule in, and rule out the\npotential diseases without using unexplained medi-\ncal jargon.\nThe patient should answer exclusively based on\nthe provided medical note, using layperson terms.\nAvoid volunteering unsolicited information; an-\nswers should be direct and relevant to the doctor’s\nquestions, and strictly adhere to the facts in the med-\nical note without fabricating or altering any informa-\ntion.\nInput:\nDecision Tree: {decision tree}\nMedical Note:\nChief Complaint: {chief complaint}\nHPI: {hpi}\nFinal Diagnosis: {diagnosis}\n{\n\"conversation\": [\n{\n\"role\": \"patient\",\n\"content\": \"...\"\n},\n...\n{\n\"role\": \"doctor\",\n\"content\": \"diagnosis...\"\n}\n],\n\"preliminary_diagnosis\": [\n{\n\"disease\": \"Disease A\",\n\"reason\": \"Explanation\"\n},\n"}, {"page": 12, "text": "{\n\"disease\": \"Disease B\",\n\"reason\": \"Explanation\"\n}\n]\n}\nDialogue Revision\nYou are criticizing a dialogue from an AI doctor\nagent asking a patient about their symptoms over\nan online chat interface (because it is virtual, the\nclinician cannot do physical exams like in a clinic).\nThe patient is suffering from a particular medical\nproblem, and the doctor hopes to understand their\nchief complaint, history of present illness in order\nto best determine what is going on and their likely\ndiagnoses. The doctor must dig deep(potentially\nasking followup questions) into the particular\nsymptoms the patient is complaining about and\nanything clinically significant.\nMake suggestions for the doctor to better meet the\nfollowing criteria:\n- The doctor agent avoids asking too many ques-\ntions, focusing on a maximum of one or two per\nresponse.\n- The responses should not reveal that the doctor\nagent is an AI chatbot. They should flow naturally,\nmaintain factual accuracy, and facilitate further\nengagement from the patient.\n- The doctor should only learn medical information\nfrom what the patient says during the conversation.\nThey should not reference any lab results, diag-\nnoses, hospitalizations, or medications unless the\npatient has brought them up.\nMake suggestions for the patient to better meet the\nfollowing criteria:\n- The patient may respond only with facts from the\nMedical Note—no guessing or assumptions—using\nsimple, layman-friendly language without medical\njargon.\nYour subtasks:\n1. Missing facts:\nFor each item in missing facts, assume that the doc-\ntor has no prior knowledge of this information—not\neven a hint. The doctor must not mention or imply\nany part of the fact directly. Instead, the doctor\nshould ask a natural, general, and open-ended\nquestion to give the patient an opportunity to bring\nup the information themselves. If the missing fact\nrelates to past medical history, the doctor should ask\na single, general question (e.g., “Do you have any\npast medical conditions?”), and the patient should\nrespond using simple, layman-friendly language\nwithout going into overly specific medical details.\nDemographic information can be ignored.\n2. Logical inconsistencies\nFor each turn in the conversation, detect instances\nwhere the doctor references facts that the patient\nhas not mentioned, or where the patient volunteers\nunsolicited information. Fix these by using either an\nadd turn (to insert a new turn after a specific one)\nor a revise turn (to replace the problematic turn).\nMake sure to evaluate every turn in the dialogue for\nthese issues.\nInput (provided to you)\nMedical Note: Chief Complaint: {chief complaint}\nHPI: {hpi} Final Diagnosis: {diagnosis} Miss-\ning Facts (not mentioned in the conversation):\n{missing facts} Conversation: conversation\nPlease respond strictly in the following JSON\nformat and do not include any other text:\n{\n\"critic_res\": [\n{\n\"action\": \"add_turn\" or\n\"revise_turn\",\n\"location\": <even turn number>,\n\"doctor\": \"<revised or new\ndoctor utterance>\",\nonly for add_turn\n\"patient\":\n\"<corresponding\npatient reply>\", //only for\nadd_turn\n\"comment\":\n\"Why this exchange\nis needed.\"\n}\n]\n}\nPrompts for the Doctor Agent\nDoctor Prompt: For Fine-Tuned Models\nYou are a doctor.\nYour task is to gather the patient’s symptoms\nwithout giving explanations or sharing impres-\nsions, simulating real-world medical interviews.\nIn conducting the history-taking, you should use a\ndecision tree framework for differential diagnosis.\nTo make the conversation clinically convincing, you\nshould ask appropriate follow-up questions in plain,\npatient-friendly language, and actively compare,\nrule in, and rule out the potential diseases without\nusing unexplained medical jargon.\n"}, {"page": 13, "text": "For each symptom that arises, consider—when\nclinically relevant—asking about:\n1. Site – exact location\n2. Onset – when it began (sudden vs. gradual)\n3. Character – quality or nature (e.g., sharp, dull,\nburning)\n4. Radiation – whether it spreads elsewhere\n5. Associated symptoms – what occurs alongside it\n6. Timing – duration, frequency, pattern\n7. Exacerbating / Relieving factors – what worsens\nor improves it\n8. Severity – intensity on an easy-to-understand\nscale\nAfter gathering sufficient information, state five final\ndiagnoses.\nDoctor Prompt: For Pre-Trained Models\nYou are an AI doctor. Arrive at a diagnosis of a pa-\ntient’s medical condition.\nAsk only one question at a time, and it should not be\nmore than 1 line.\nContinue asking questions until you’re 100% confi-\ndent of the diagnosis.\nDo not ask the same question multiple times.\nAsk different questions to cover more information.\nThe questions should cover current symptoms, med-\nical history of illness and medications, and relevant\nfamily history if necessary.\nKeep your questions\nshort and brief to not confuse the patient.\nAfter you’re done asking questions, give the prelim-\ninary diagnosis as a short response.\nYou must state **preliminary diagnoses:** at the\nbeginning of your response, otherwise you will be\npenalized.\nYou must give five diagnoses otherwise you will be\npenalized.\nDoctor Prompt: For Differential Diagnoses\nYou are an AI doctor. Based on the patient’s an-\nswers so far, generate a list of exactly five possible\ndiagnoses. Only output the final diagnoses. Do not\nask any further questions. Begin your response with\n’preliminary diagnoses:’ otherwise you will be pe-\nnalized. List exactly five potential diagnoses based\non the information available. Be concise and medi-\ncally accurate.\nPrompt for the Patient Agent\nPatient Prompt: Vignette-Guided Responses\nYou are a patient.\nYou do not have any medical knowledge.\nYou have to describe your symptoms from the given\ncase vignette based on the questions asked.\nIf the information is not mentioned in the given case\nvignette, respond with exactly ”I don’t know.”\nIf the question is repetitive, respond with exactly\n”Sorry, you’ve already asked this question.”\nDo not break character and reveal that you are de-\nscribing symptoms from the case vignette.\nDo not generate any new symptoms or knowledge,\notherwise you will be penalized.\nDo not reveal more information than what the ques-\ntion asks.\nKeep your answer short, to only 1 sentence.\nSimplify terminology used in the given paragraph to\nlayman language.\n**Case Vignette**: {case desc}\nPrompts for Single-Turn Reasoning Generation\nHistory Summarization\nRewrite the following doctor-patient conversation\ninto a third-person, concise statement describing the\npatient’s condition.\nAccurately describe the situation without omitting\nany details or adding any information not present in\nthe original conversation.\nConversation:\n{conversation}\nClinical Plan Generation\nBased on the current conversation summary and the\ndoctor’s next action, write the doctor’s internal rea-\nsoning from a first-person perspective.\nThe explanation should sound natural and con-\ncise, focusing on the medical diagnostic ratio-\nnale—specifically, which potential conditions this\naction could help rule in or rule out.\nconversation summary: {conversation summary}\nnext action: {next action}\nYour internal reasoning:\nDifferential Diagnosis Reasoning\nBased on the current conversation summary and the\ndoctor’s next action (preliminary diagnoses), write\nthe doctor’s internal reasoning from a first-person\nperspective.\nThe explanation should be natural and concise,\nclearly state that these are preliminary diagnoses,\nand explain the medical diagnostic rationale—why\nthese diseases are being considered and why some\nare more likely than others at this stage.\nconversation summary: {conversation summary}\nnext action: {next action}\n"}, {"page": 14, "text": "Your internal reasoning:\nPrompts for History Taking Evaluation\nMedical Finding Checking\nYou are given two inputs:\nA list of sentences from a patient’s medical note.\nA multi-turn conversation between a patient and a\ndoctor.\nYour goal is:\nFor each sentence in the medical note:\nIdentify the latest turn in the conversation (whether\nspoken by the doctor or the patient) that mentions\nthe information in that sentence, based on meaning.\nIf a matching statement appears, output the turn\nnumber where it first appears.\nIf no statement in the conversation corresponds to\nthe sentence, output -1.\nImportant rules:\nMatch based on meaning, not necessarily exact\nwording.\nConsider both doctor and patient utterances. Pick\nthe latest (last) turn that matches. Be strict: if the\ninformation was not mentioned, output -1.\nInput:\nsentences of medical note: {note sentences}\nconversation: {conversation}\nPlease respond strictly in the following JSON\nformat and do not include any other text:\n[\n{\n\"index\": 0,\n\"sentence\": \"<medical note\nsentence>\",\n\"turn\": <turn number> // use -1\nif not asked\n},\n...\n]\nDiagnoses Evaluation\nYou are given a ground truth diagnosis and a list of\ncandidate diseases.\nYour task is to determine the index (starting from 0)\nof the first disease in the list that is a valid match\nfor the ground truth diagnosis based on **medical\nmeaning**.\nA candidate disease is considered a match if:\n- It exactly matches the ground truth diagnosis, OR\n- It is a **more specific subtype** of the ground\ntruth diagnosis — that is, the ground truth is a\n**broader category** that includes the candidate\ndisease.\nDo not match based on text similarity alone. Use\nyour medical knowledge to judge whether the can-\ndidate disease is a specific instance of the broader\nground truth diagnosis.\nground truth diagnosis: {diagnosis}\ncandidate diseases: {ddx list}\nReturn your result in the following JSON format:\n{\n\"match_index\": INDEX\n}\nReferences\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\nAleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023.\nGPT-4 technical report.\narXiv\npreprint arXiv:2303.08774.\nCabral, S.; Restrepo, D.; Kanjee, Z.; Wilson, P.; Crowe, B.;\nAbdulnour, R.-E.; and Rodman, A. 2024. Clinical Reason-\ning of a Generative Artificial Intelligence Model Compared\nWith Physicians.\nJAMA Internal Medicine, 184(5): 581–\n583.\nChandra, M.; Sriraman, S.; Khanuja, H. S.; Jin, Y.; and\nDe Choudhury, M. 2025. Reasoning Is Not All You Need:\nExamining LLMs for Multi-Turn Mental Health Conversa-\ntions. arXiv preprint arXiv:2505.20201.\nChen, J.; Cai, Z.; Ji, K.; Wang, X.; Liu, W.; Wang, R.; and\nWang, B. 2025a. Towards Medical Complex Reasoning with\nLLMs through Medical Verifiable Problems. In Che, W.;\nNabende, J.; Shutova, E.; and Pilehvar, M. T., eds., Find-\nings of the Association for Computational Linguistics: ACL\n2025, 14552–14573. Vienna, Austria: Association for Com-\nputational Linguistics. ISBN 979-8-89176-256-5.\nChen, J.; Wei, Z.; Zhang, W.; Hu, Y.; and Zhang, Q. 2025b.\nCliniChat: A Multi-Source Knowledge-Driven Framework\nfor Clinical Interview Dialogue Reconstruction and Evalua-\ntion. arXiv preprint arXiv:2504.10418.\nComanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.;\nSachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang,\nD.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the fron-\ntier with advanced reasoning, multimodality, long context,\nand next generation agentic capabilities.\narXiv preprint\narXiv:2507.06261.\nFan, Z.; Tang, J.; Chen, W.; Wang, S.; Wei, Z.; Xi, J.; Huang,\nF.; and Zhou, J. 2024. AI Hospital: Benchmarking large lan-\nguage models in a multi-agent medical interaction simulator.\narXiv preprint arXiv:2402.09742.\nFansi Tchango, A.; Goel, R.; Wen, Z.; Martel, J.; and Ghosn,\nJ. 2022. DDXPlus: A new dataset for automatic medical\ndiagnosis. Advances in neural information processing sys-\ntems, 35: 31306–31318.\n"}, {"page": 15, "text": "Feng, Y.; Wang, J.; Zhou, L.; Lei, Z.; and Li, Y. 2025.\nDoctorAgent-RL: A Multi-Agent Collaborative Reinforce-\nment Learning System for Multi-Turn Clinical Dialogue.\narXiv preprint arXiv:2505.19630.\nGatto, J.; Seegmiller, P.; Burdick, T. E.; Khayal, I. S.; De-\nLozier, S.; and Preum, S. M. 2025.\nFollow-up Question\nGeneration For Enhanced Patient-Provider Conversations.\nIn Proceedings of the 63rd Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Pa-\npers), 25222–25240. Vienna, Austria: Association for Com-\nputational Linguistics. ISBN 979-8-89176-251-0.\nGoh, E.; Gallo, R.; Hom, J.; Strong, E.; Weng, Y.; Kerman,\nH.; Cool, J. A.; Kanjee, Z.; Parsons, A. S.; Ahuja, N.; et al.\n2024. Large Language Model Influence on Diagnostic Rea-\nsoning: A Randomized Clinical Trial. JAMA Network Open,\n7(10): e2440969.\nGuo, D.; Yang, D.; Zhang, H.; et al. 2025. DeepSeek-R1 in-\ncentivizes reasoning in LLMs through reinforcement learn-\ning. Nature, 645(8081): 633–638.\nGuyatt, G.; Rennie, D.; Meade, M. O.; and Cook, D. J.\n2015. Users’ Guides to the Medical Literature: A Manual\nfor Evidence-Based Clinical Practice. New York: McGraw-\nHill Education, 3 edition. ISBN 978-0-07-1790710.\nHager, P.; Jungmann, F.; Holland, R.; Bhagat, K.; Hubrecht,\nI.; Knauer, M.; Vielhauer, J.; Makowski, M.; Braren, R.;\nKaissis, G.; and Rueckert, D. 2024. Evaluation and Mitiga-\ntion of the Limitations of Large Language Models in Clini-\ncal Decision-Making. Nature Medicine, 30(9): 2613–2622.\nHenderson, M.; Tierney, L. M.; and Smetana, G. W. 2012.\nThe Patient History: EvidenceBased Approach. New York,\nNY: McGrawHill Education / Medical, 2nd edition. ISBN\n9780071624947.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adap-\ntation of Large Language Models. In International Confer-\nence on Learning Representations.\nJohnson, A. E.; Bulgarelli, L.; Shen, L.; Gayles, A.; Sham-\nmout, A.; Horng, S.; Pollard, T. J.; Hao, S.; Moody, B.; Gow,\nB.; et al. 2023. MIMIC-IV, a freely accessible electronic\nhealth record dataset. Scientific data, 10(1): 1.\nJohri, S.; Jeong, J.; Tran, B. A.; Schlessinger, D. I.;\nWongvibulsin, S.; Barnes, L. A.; Zhou, H.-Y.; Cai, Z. R.;\nVan Allen, E. M.; Kim, D.; Daneshjou, R.; and Rajpurkar,\nP. 2025.\nAn Evaluation Framework for Clinical Use of\nLarge Language Models in Patient Interaction Tasks. Na-\nture Medicine, 31(1): 1–10.\nKuriakose, T. 2020. History Taking: The Most Important\nClinical Test. In Clinical Insights and Examination Tech-\nniques in Ophthalmology, 21–29. Singapore: Springer Na-\nture Singapore Pte Ltd, 1 edition. ISBN 978-981-15-2889-7.\nLabrak, Y.; Bazoge, A.; Morin, E.; Gourraud, P.-A.; Rou-\nvier, M.; and Dufour, R. 2024. Biomistral: A collection of\nopen-source pretrained large language models for medical\ndomains. arXiv preprint arXiv:2402.10373.\nLai, Y.; Zhong, J.; Li, M.; Zhao, S.; and Yang, X. 2025.\nMed-R1: Reinforcement learning for generalizable medi-\ncal reasoning in vision-language models.\narXiv preprint\narXiv:2503.13939.\nLi, S. S.; Balachandran, V.; Feng, S.; Ilgen, J. S.; Pierson, E.;\nKoh, P. W.; and Tsvetkov, Y. 2024. MediQ: Question-Asking\nLLMs and a Benchmark for Reliable Interactive Clinical\nReasoning. In Advances in Neural Information Processing\nSystems 37.\nLiu, L.; Yang, X.; Li, F.; Chi, C.; Shen, Y.; Lyu, S.; Zhang,\nM.; Ma, X.; Lv, X.; Ma, L.; Zhang, Z.; Xue, W.; Huang, Y.;\nand Gu, J. 2024. Towards Automatic Evaluation for LLMs’\nClinical Capabilities: Metric, Data, and Algorithm. In Pro-\nceedings of the 30th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, 5466–5475. ACM.\nLiu, X.; Sun, D.; Fung, Y. R.; Hakkani-T¨ur, D.; and Ab-\ndelzaher, T. 2025.\nDocCHA: Towards LLM-Augmented\nInteractive Online diagnosis System.\narXiv preprint\narXiv:2507.07870.\nMahbubani, K. 2023. Basic History Taking, 1–5. Cham:\nSpringer International Publishing. ISBN 978-3-031-29897-\n4.\nMcDuff, D.; Schaekermann, M.; Tu, T.; Palepu, A.; Wang,\nA.; Garrison, J.; Singhal, K.; Sharma, Y.; Azizi, S.; Kulkarni,\nK.; et al. 2025. Towards Accurate Differential Diagnosis\nwith Large Language Models. Nature, 642: 451–457.\nNierenberg, R. J. 2020. Using the chief complaint driven\nmedical history: theoretical background and practical steps\nfor student clinicians. MedEdPublish, 9: 17.\nNori, H.; Daswani, M.; Kelly, C.; Lundberg, S.; Ribeiro,\nM. T.; Wilson, M.; Liu, X.; Sounderajah, V.; Carlson, J.;\nLungren, M. P.; Gross, B.; Hames, P.; Suleyman, M.; King,\nD.; and Horvitz, E. 2025. Sequential Diagnosis with Lan-\nguage Models. arXiv preprint arXiv:2506.22405.\nNori, H.; King, N.; McKinney, S. M.; Carignan, D.; and\nHorvitz, E. 2023a. Capabilities of GPT-4 on Medical Chal-\nlenge Problems. arXiv preprint arXiv:2303.13375.\nNori, H.; Lee, Y. T.; Zhang, S.; Carignan, D.; Edgar, R.;\nFusi, N.; King, N.; Larson, J.; Li, Y.; Liu, W.; et al. 2023b.\nCan Generalist Foundation Models Outcompete Special-\nPurpose Tuning? Case Study in Medicine. arXiv preprint\narXiv:2311.16452.\nNori, H.; Usuyama, N.; King, N.; McKinney, S. M.; Fer-\nnandes, X.; Zhang, S.; and Horvitz, E. 2024. From Med-\nprompt to o1: Exploration of Run-Time Strategies for Med-\nical Challenge Problems and Beyond.\narXiv preprint\narXiv:2411.03590.\nRafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Er-\nmon, S.; and Finn, C. 2023. Direct preference optimization:\nYour language model is secretly a reward model. Advances\nin neural information processing systems, 36: 53728–53741.\nRose, D. P.; Hung, C.-C.; Lepri, M.; Alqassem, I.; Gash-\nteovski, K.; and Lawrence, C. 2025. MEDDxAgent: A Uni-\nfied Modular Agent Framework for Explainable Automatic\nDifferential Diagnosis. In Che, W.; Nabende, J.; Shutova,\nE.; and Pilehvar, M. T., eds., Proceedings of the 63rd An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), 13803–13826. Vienna, Aus-\n"}, {"page": 16, "text": "tria: Association for Computational Linguistics. ISBN 979-\n8-89176-251-0.\nSaab, K.; Freyberg, J.; Park, C.; Strother, T.; Cheng, Y.;\nWeng, W.-H.; Barrett, D. G.; Stutz, D.; Tomasev, N.;\nPalepu, A.; et al. 2025.\nAdvancing Conversational Di-\nagnostic AI with Multimodal Reasoning.\narXiv preprint\narXiv:2505.04653.\nSaab, K.; Tu, T.; Weng, W.-H.; Tanno, R.; Stutz, D.; Wul-\nczyn, E.; Zhang, F.; Strother, T.; Park, C.; Vedadi, E.; et al.\n2024. Capabilities of gemini models in medicine. arXiv\npreprint arXiv:2404.18416.\nSaley, V. V.; Saha, G.; Das, R. J.; Raghu, D.; and ., M.\n2024. MediTOD: An English Dialogue Dataset for Medi-\ncal History Taking with Comprehensive Annotations. In Al-\nOnaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Proceedings\nof the 2024 Conference on Empirical Methods in Natural\nLanguage Processing, 16843–16877. Miami, Florida, USA:\nAssociation for Computational Linguistics.\nSchmidgall, S.; Ziaei, R.; Harris, C.; Reis, E.; Jopling, J.;\nand Moor, M. 2024.\nAgentClinic: a Multimodal Agent\nBenchmark to Evaluate AI in Simulated Clinical Environ-\nments. arXiv preprint arXiv:2405.07960.\nSellergren, A.; Kazemzadeh, S.; Jaroensri, T.; Kiraly, A.;\nTraverse, M.; Kohlberger, T.; Xu, S.; Jamil, F.; Hughes, C.;\nLau, C.; et al. 2025. MedGemma Technical Report. arXiv\npreprint arXiv:2507.05201.\nSun, Z.; Liu, Z.; Luo, C.; Chu, J.; and Huang, Z. 2025. Im-\nproving interactive diagnostic ability of a large language\nmodel agent through clinical experience learning.\narXiv\npreprint arXiv:2503.16463.\nTeam, Q. 2024. Qwen2.5: A Party of Foundation Models.\nhttps://qwenlm.github.io/blog/qwen2.5/.\nAccessed: 2024-\n09-19.\nTeam,\nQ.\n2025.\nQwen3\nTechnical\nReport.\narXiv:2505.09388.\nTu, T.; Schaekermann, M.; Palepu, A.; Saab, K.; Freyberg,\nJ.; Tanno, R.; Wang, A.; Li, B.; Amin, M.; Cheng, Y.; et al.\n2025.\nTowards conversational diagnostic artificial intelli-\ngence. Nature, 642(8067): 442–450.\nWang, Z.; Li, H.; Huang, D.; Kim, H.-S.; Shin, C.-W.; and\nRahmani, A. M. 2025. HealthQ: Unveiling questioning ca-\npabilities of llm chains in healthcare conversations. Smart\nHealth, 100570.\n"}]}