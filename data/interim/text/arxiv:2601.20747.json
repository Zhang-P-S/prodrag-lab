{"doc_id": "arxiv:2601.20747", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20747.pdf", "meta": {"doc_id": "arxiv:2601.20747", "source": "arxiv", "arxiv_id": "2601.20747", "title": "Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts", "authors": ["Elham Aghakhani", "Rezvaneh Rezapour"], "published": "2026-01-28T16:23:00Z", "updated": "2026-01-28T16:23:00Z", "summary": "Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20747v1", "url_pdf": "https://arxiv.org/pdf/2601.20747.pdf", "meta_path": "data/raw/arxiv/meta/2601.20747.json", "sha256": "150b3f4e569eeecd3ecdd232add1c49511435a1e1f7ddb9d6544d10206f2b634", "status": "ok", "fetched_at": "2026-02-18T02:20:10.942808+00:00"}, "pages": [{"page": 1, "text": "Like a Therapist, But Not: Reddit Narratives of AI in Mental Health\nContexts\nElham Aghakhani\nDrexel University\nea664@drexel.edu\nRezvaneh Rezapour\nDrexel University\nsr3563@drexel.edu\nAbstract\nLarge language models (LLMs) are increas-\ningly used for emotional support and men-\ntal health–related interactions outside clinical\nsettings, yet little is known about how peo-\nple evaluate and relate to these systems in ev-\neryday use. We analyze 5,126 Reddit posts\nfrom 47 mental health communities describ-\ning experiential or exploratory use of AI for\nemotional support or therapy. Grounded in\nthe Technology Acceptance Model and ther-\napeutic alliance theory, we develop a theory-\ninformed annotation framework and apply a\nhybrid LLM–human pipeline to analyze evalu-\native language, adoption-related attitudes, and\nrelational alignment at scale. Our results show\nthat engagement is shaped primarily by nar-\nrated outcomes, trust, and response quality,\nrather than emotional bond alone. Positive sen-\ntiment is most strongly associated with task and\ngoal alignment, while companionship-oriented\nuse more often involves misaligned alliances\nand reported risks such as dependence and\nsymptom escalation. Overall, this work demon-\nstrates how theory-grounded constructs can be\noperationalized in large-scale discourse analy-\nsis and highlights the importance of studying\nhow users interpret language technologies in\nsensitive, real-world contexts.\n1\nIntroduction\n“Some absences keep their shape.” When ChatGPT\nproduced this reflection in response to a grieving\npsychologist, it startled him, not because of its elo-\nquence, but for how closely it mirrored what he\nstruggled to articulate himself. This encounter, de-\nscribed in the New York Times (The New York\nTimes, 2025), motivates a central question in this\nstudy: How do people evaluate, interpret, and en-\ngage with AI tools when they are used for emotional\nsupport or therapy?, while illustrating the capac-\nity of large language models (LLMs) to generate\nemotionally resonant language. Millions world-\nwide face barriers to traditional therapy, including\nhigh costs, limited availability of trained profes-\nsionals (CNBC, 2021), and the persistent stigma\n(Naslund et al., 2016; Thomson et al., 2024). In\nresponse, systems such as ChatGPT, Claude, and\nCharacter AI are increasingly used as accessible,\nalways-available conversational supports.\nThis\nadoption is largely user-driven and occurs outside\nclinical settings, yet we lack systematic insight into\nhow people evaluate, trust, and relate to these sys-\ntems in real-world mental health contexts.\nEarly work on conversational agents for men-\ntal health centered on scripted, rule-based systems\nsuch as ELIZA (Weizenbaum, 1966) and PARRY\n(Colby, 1975), and later on CBT-oriented chatbots\nlike Woebot and Wysa (Fitzpatrick et al., 2017;\nBeatty et al., 2022). These systems delivered struc-\ntured psychoeducation at scale but constrained en-\ngagement, whereas recent LLM-based systems en-\nable open-ended, context-sensitive dialogue often\nperceived as empathetic or relational. However,\nmost existing evaluations examine these systems in\ncontrolled settings, focusing on accuracy, safety, or\ntherapeutic potential (Maurya et al., 2025; Thakkar\net al., 2024; Roshanaei et al., 2025), offering lim-\nited insight into how users interpret and negotiate\nAI’s role in everyday mental health practices.\nOnline communities offer a valuable lens for ex-\namining these questions. Platforms such as Reddit\nhost large-scale mental health discussions where\nusers describe struggles, seek advice, and collec-\ntively interpret technologies, including the bound-\naries between AI support and human care (Sit et al.,\n2024; Bouzoubaa et al., 2024c). Analyzing this\ndiscourse reveals how users assess the affordances\nand limitations of AI support and how these evalu-\nations shape users’ engagement with AI in relation\nto human mental health support. To address these\nissues, we investigate the following research ques-\ntions:\nRQ1: How do individuals in online communities\ndescribe their use of AI for mental health support,\n1\narXiv:2601.20747v1  [cs.CL]  28 Jan 2026\n"}, {"page": 2, "text": "including perceived functions, benefits, and risks?\nRQ2: How are Technology Acceptance Model\n(TAM) dimensions associated with adoption-\nrelated attitudes toward AI for therapy and emo-\ntional support in online communities?\nRQ3: How do users express therapeutic alliance\nwith AI, and how do task, goal, and bond alignment\nrelate to engagement outcomes?\nWe address these questions by analyzing 4.7\nmillion posts across 47 condition-focused Reddit\ncommunities between November 2022 and Au-\ngust 2025. Using a multi-stage filtering pipeline,\nwe identified posts describing experiential or ex-\nploratory use of AI as a therapeutic tool, result-\ning in a curated dataset of 5,126 posts focused\non AI-supported mental health care. We devel-\noped a theory-grounded annotation schema inte-\ngrating constructs from the Technology Acceptance\nModel (Davis et al., 1989) and therapeutic alliance\ntheory (Bordin, 1979) to capture evaluative lan-\nguage, adoption-related attitudes, and relational\nalignment, including pragmatic evaluations (e.g.,\nusefulness, trust, outcome quality) and task, goal,\nand bond dimensions shaping engagement and per-\nceived risk, in user discourse.\nOur results show that sustained engagement with\nAI for mental health support is shaped primarily by\ndemonstrable outcomes, trust, and response quality,\nrather than by emotional bonding alone. Notably,\ntherapeutic alliance is conditional: positive engage-\nment is strongly associated with task and goal align-\nment, whereas emotional bond alone shows a weak\nrelationship with positive sentiment and frequently\nco-occurs with dependency and reported harm.\nOur work makes three contributions: (1) a novel\ndataset capturing how users evaluate and relate\nto AI systems for emotional support; (2) theory-\ngrounded operationalization of Technology Accep-\ntance Model and therapeutic alliance constructs for\nlarge-scale NLP analysis of adoption attitudes and\nrelational alignment; and (3) quantitative and qual-\nitative evidence linking system properties to adop-\ntion sentiment and therapeutic alliance, informing\nthe design and evaluation of AI-supported mental\nhealth tools. We will release the dataset, annotation\nguidelines, and code.\n2\nRelated Work\n2.1\nAI for Mental Health Support\nAdvances in AI are transforming mental health care,\nwith applications ranging from therapy analysis\n(Aghakhani et al., 2025) to early diagnostic tools\nand mood monitoring to conversational agents that\ndeliver therapeutic techniques in real time (Thakkar\net al., 2024; Dehbozorgi et al., 2025; Graham\net al., 2019; Cummins et al., 2020; Vaidyam et al.,\n2019). Early systems such as ELIZA (Weizen-\nbaum, 1966) and PARRY (Colby, 1975) demon-\nstrated the potential of simulated dialogue, while\nlater tools like Wysa and Woebot applied struc-\ntured therapeutic approaches at scale (Inkster et al.,\n2018; Beatty et al., 2022; Gamble, 2020). More\nrecently, LLMs have enabled open-domain conver-\nsation, context-sensitive responses, and language\nthat users may interpret as empathetic or support-\nive (Yu and McGuinness, 2024; Sorin et al., 2024;\nOjeda Meixueiro et al., 2024). Recent studies show\nLLMs can deliver accurate and empathetic con-\ntent (Maurya et al., 2025), support positive mental\nhealth (Thakkar et al., 2024), and even rival human-\ngenerated supportive messages (Young et al., 2024).\nA recent survey synthesized the therapeutic poten-\ntial of LLMs, their limitations, and ethical risks (Na\net al., 2025). Beyond mental health, prior NLP re-\nsearch has examined trust (Xie et al., 2024; Pessian-\nzadeh et al., 2025), alignment (Naseem et al., 2025),\nand user perceptions of AI systems (Højer et al.,\n2025; Razi et al., 2025), typically through con-\ntrolled experiments, surveys, or benchmark-driven\nevaluations. Prior work rarely examines mental\nhealth contexts or everyday evaluative and rela-\ntional judgments, leaving users’ interpretations of\nLLM-based support underexplored.\n2.2\nOnline Mental Health Discourse\nOnline platforms have become key spaces for men-\ntal health discussion, enabling people to share ex-\nperiences, seek advice, and access peer support\n(Thomson et al., 2024; Naslund et al., 2020). Their\naccessibility, anonymity, and asynchronous nature\nmake them especially valuable for those facing\nbarriers to traditional care, such as cost, geogra-\nphy, or stigma (Naslund et al., 2016; Merchant\net al., 2022; Andalibi et al., 2017; Bouzoubaa et al.,\n2024a). Reddit, in particular, has emerged as a\nvaluable source due to its topic-specific subreddits\n(Marshall et al., 2024; Bouzoubaa et al., 2024b),\npseudonymity (Naslund et al., 2020), and commu-\nnity norms that encourage candid disclosure (Ray-\nland and Andrews, 2023). While this work demon-\nstrates the richness of online mental health dis-\ncourse, it has largely focused on condition classifi-\ncation (Dinu and Moldovan, 2021), crisis detection\n2\n"}, {"page": 3, "text": "(D. Lewis et al., 2025), and symptom monitoring\n(Alhamed et al., 2024), rather than on how users\nperceive and discuss emerging tools such as AI in\ntherapeutic contexts. We address this gap by exam-\nining how AI tools are evaluated and positioned in\nnaturalistic mental health discourse.\n3\nMethod\n3.1\nData\nReddit has been widely used in mental health re-\nsearch due to its scale and topic-specific commu-\nnities (Sit et al., 2024; Jiang et al., 2020). To ana-\nlyze discourse around AI-based mental health tools,\nwe curated condition-focused subreddits guided by\nDSM-5 diagnostic categories (Diagnostic, 2013)\n(e.g., depression, anxiety, bipolar disorder) to en-\nsure broad and systematic coverage.\nTwo au-\nthors independently reviewed DSM-5 categories\nand verified subreddit mappings, and we addition-\nally included general mental health subreddits used\nfor cross-diagnostic support and discussion (e.g.,\nr/mentalhealth, r/TalkTherapy). We selected\n47 subreddits varying in size, ranging from fewer\nthan 10,000 to over 1 million members (see Ta-\nble A.2), and collected Reddit submissions (exclud-\ning comments) using the ArcticShift API1. We col-\nlected posts published between November 30, 2022\n(ChatGPT’s public release), and August 15, 2025,\nresulting in 4,703,056 submissions. We applied\npreprocessing to improve data quality: concatenat-\ning titles and bodies, removing deleted or duplicate\nposts, and excluding posts with fewer than 10 to-\nkens (the shortest 10%). After preprocessing, the\ndataset comprised 3,530,486 posts.\n3.2\nIdentifying AI-Relevant Posts\nTo identify posts discussing AI tools (e.g., Chat-\nGPT, Character AI) for therapeutic or emotional\nsupport, we used a scalable, multi-stage relevance\nfiltering pipeline. To avoid applying LLM-based\nclassification to the full corpus, we first narrowed\nthe search space using keyword-based retrieval, fol-\nlowed by LLM-based validation on a reduced can-\ndidate set. In the first stage, we randomly sam-\npled 120,000 posts and used GPT-4o mini (Ope-\nnAI, 2024) to classify whether posts referenced\nAI use for emotional support, therapy, or mental\nhealth–related guidance. From posts labeled as rele-\nvant, we extracted AI-related keywords and phrases.\n1https://github.com/ArthurHeitmann/arctic_\nshift\nThis process resulted in around 800 unique AI-\nrelated terms (e.g., “AI-Powered,” “Bing Chat,”\n“CHAI Bot”), which were manually reviewed and\ndeduplicated to produce a refined list of 146 key-\nwords. Applying this list to the full dataset resulted\nin a high-recall corpus of 572,734 posts.\nIn the second stage, we validated relevance be-\nfore scaling LLM-based filtering. We randomly\nsampled 10,000 posts from the filtered corpus and\nused GPT-4o mini to label each as relevant or\nnot.\nTwo authors independently reviewed 200\nposts (100 relevant, 100 not), resolving disagree-\nments through discussion, and iteratively refined\nthe prompt until agreement with human judgments\nwas high (Fleiss’ κ = 0.90). We then applied the\nfinalized prompt to all 572,734 keyword-filtered\nposts, resulting in 6,206 posts describing AI use\nfor therapeutic or emotional support.\nTo further characterize content, we manually re-\nviewed a random sample of 200 posts, and identi-\nfied four categories: experiential (users describing\npersonal AI use for therapy or emotional support),\nexploratory (seeking advice or others’ experiences),\nadvertisement (promotional or developer-posted\ncontent), and irrelevant (posts that did not mean-\ningfully discuss AI in a mental health context). Two\nauthors independently annotated 50 posts per cat-\negory using the same definitions provided to the\nLLM, achieving strong agreement with model clas-\nsifications (Fleiss’ κ = 0.78); disagreements were\nresolved through discussion. Since our analysis fo-\ncuses on how individuals perceive and describe AI\nas a therapeutic tool, we retained only experiential\nand exploratory posts for further analysis, resulting\nin a final dataset of 5,126 posts.\n3.3\nAnnotation Framework\nTo analyze how users evaluate AI as a therapeutic\ntool, we ground our annotation framework in two\nestablished theories from psychology and HCI to\ncapture both pragmatic and relational aspects of\nAI-mediated mental health support:\nTherapeutic Alliance Theory. Therapeutic al-\nliance refers to the collaborative and affective bond\nbetween therapist and client and is a key predic-\ntor of treatment outcomes (Bordin, 1979). It is\ntypically described through three interdependent\ncomponents: bond (trust, empathy, and emotional\nconnection), task (agreement on therapeutic ac-\ntivities), and goal (shared commitment to objec-\ntives). Although originally developed for human\npsychotherapy, the framework has been applied to\n3\n"}, {"page": 4, "text": "Dimension\nCriteria\nMeasurement\nTAM\nPerceived Usefulness\nDoes the user describe the AI as helpful or useful for emotional/mental-\nhealth tasks?\nCategorical: [useful|not_useful|not_mentioned] +Desc.\nPerceived Ease of Use\nDoes the user describe the AI as easy, convenient, or accessible to use?\nCategorical: [easy|difficult|not_mentioned] +Desc.\nIntention to Continue\nDoes the user state an intention to keep using AI in the future?\nCategorical: [yes|no|not_mentioned]\nPerceived Trust\nDoes the user indicate the AI is trustworthy, reliable, or safe in a mental-\nhealth context?\nCategorical: [trustworthy|untrustworthy|not_mentioned]\n+ Desc.\nTAM (ext.)\nOutput Quality\nDoes the user judge the AI’s responses as high vs. low quality (e.g.,\nempathetic, thoughtful vs. vague, inaccurate)?\nCategorical: [good|poor|not_mentioned] + Desc.\nResult Demonstrability\nAre tangible/behavioral outcomes mentioned (e.g., better sleep, reduced\nanxiety)?\nCategorical: [pos._results|neg._results|not_mentioned]\n+ Desc.\nSocial Influence\nDoes the user mention influence from peers/Reddit/others to use AI?\nCategorical: [present|absent] + Desc.\nPerceived Risks\nMentions of limitations, harms, or risks in using AI for mental health\n(e.g., inaccuracy, emotional detachment).\nCategorical: [mentioned|not_mentioned] + Desc.\nTherapeutic\nAlliance\nBond\nDoes the user feel emotionally supported/understood/safe with the AI?\nCategorical: [strong|weak|not_mentioned] + Desc.\nTask\nIs the AI helping with therapeutic actions (e.g., reflection, coping, jour-\nnaling)?\nCategorical: [aligned|misaligned|not_mentioned] + Desc.\nGoal\nDoes the AI align with the user’s mental-health goals (recovery, stability,\ngrowth)?\nCategorical: [aligned|misaligned|not_mentioned] + Desc.\nOther\nUsage Intent\nthe primary function or reason a user engages with AI in a mental health\nrelated context.\nDesc.\nComparison to Therapy\nHow the AI compares to traditional therapy.\nCategorical: [better|worse|complementary|not_mentioned]\nAI Tool Mentioned\nThe specific AI tool that is mentioned by the user.\nDesc.\nSentiment toward AI\nOverall Sentiment toward users’ AI experience\nCategorical: [positive|negative|neutral] + Desc.\nMental Health Condition\nThe specific mental health condition that is mentioned by the user.\nDesc.\nTable 1: Dimensions, associated frameworks, definitions, and measurement type (categorical or free-text (Desc).\ndigital interventions (Vowels et al., 2024; Beatty\net al., 2022). We adapt this framework to analyze\nhow users describe relational alignment with AI\nsystems in mental health contexts.\nTechnology Acceptance Model.\nTAM ex-\nplains technology adoption through core be-\nliefs about usefulness and ease\nof\nuse,\nwhich influence attitudes, intentions, and actual\nuse (Davis et al., 1989; Fishbein, 1979).\nEx-\ntensions have added dimensions such as Output\nQuality, result demonstrability, and social\ninfluence (Venkatesh and Davis, 2000), while\nresearch in health informatics highlights the impor-\ntance of perceived trust and perceived risk\nin sensitive domains (Su et al., 2013; Dhagarra\net al., 2020). Building on this work, we include\ncore and extended TAM to capture user evaluations\nof AI tools in mental health contexts, where trust,\nrisk, and outcomes are salient. We operational-\nized an annotation schema using these theories.\nFrom TAM and its extensions, we used perceived\nusefulness, ease\nof\nuse, output\nquality,\nresult demonstrability, social influence,\nperceived\ntrust, perceived\nrisks, and\nintention to continue. From therapeutic al-\nliance, we included task, bond, and goal. To\ncapture context-specific aspects of AI–therapy\ndiscourse, we included comparison to human\ntherapy, AI tool mentioned, mental health\ncondition, sentiment, and usage intent. Ta-\nble 1 shows all dimensions and coding criteria.\n3.4\nData Annotation\nHuman Annotation. To assess annotation reliabil-\nity, two authors independently annotated a random\nsample of 100 posts from the final dataset across\nall 13 dimensions (Table 1) using shared guidelines.\nInter-annotator agreement measured by raw agree-\nment, Cohen’s Kappa, and Gwet’s AC1 to account\nfor label imbalance, ranged from 0.74 to 0.95 (raw),\n0.37 to 0.60 (Cohen’s Kappa), and 0.64 to 0.94 (\nGwet’s AC1), indicating substantial to near-perfect\nagreement. Disagreements were resolved through\ndiscussion, and the resulting labeledset served as\nground truth for subsequent model evaluation.\nLLM-Assisted Annotation and Evaluation.\nWe\nevaluated\nmultiple\n(closed\nand\nopen)\nLLMs selected from leaderboards\n2 of high-\nperforming systems available at the time of\nthe study.\nThese included GPT-5.2 (Ope-\nnAI, 2025), Gemini-3-Pro (GoogleDeepMind,\n2025),\nand\nClaude-Opus-4.5\n(Anthropic,\n2025), which are proprietary models, as well\nas Kimi-K2-Instruct (MoonshotAI, 2025) and\nQwen3-Next-80B-A3B-Instruct (Qwen, 2025),\nwhich are open-source models. All models were\nprompted using the same structured annotation\nprocedure, including task instructions, construct\ndefinitions, and a standardized JSON output\nformat.\nA sample schema was included to\nconstrain outputs. We set the temperature to zero\nfor all models; for GPT-5.2, we additionally fixed\nthe reasoning setting to a medium level. The full\nprompt is provided in the Appendix A.1.\nFor all dimensions, models produced free-text\nrationales for each label. We manually reviewed\nthese outputs on the 100-post evaluation set to ver-\nify consistency with annotation definitions and post\ncontent.\n2https://llm-stats.com/\n4\n"}, {"page": 5, "text": "3.5\nThematic Analysis\nTo facilitate a more fine-grained analysis of AI\nengagement and concerns (RQ1), we conducted\nadditional thematic analysis on the LLM-generated\nfree-text descriptions for two of our dimensions:\nperceived risks and usage intent. We ap-\nplied LLM-guided thematic analysis (Dai et al.,\n2023) to consolidate these free-text descriptions\ninto coherent, interpretable categories by grouping\nsemantically similar phrases under shared themes.\nFor example, risk descriptions such as “compul-\nsive use,” “overreliance,” and “obsessive use” were\nconsolidated into the category Addiction & De-\npendence. This process resulted in standardized\ntaxonomies for both dimensions, enabling system-\natic analysis of engagement patterns and reported\nconcerns across the corpus.\n4\nResults\n4.1\nData Characteristics\nOur analysis includes 5,126 Reddit posts from 47\nmental health communities (November 2022–Au-\ngust 2025) describing experiential (3,605) or ex-\nploratory (1,521) use of AI for therapeutic or emo-\ntional support. Engagement varied across com-\nmunities, with anxiety-related subreddits show-\ning the highest activity, i.e., nearly 500 posts in\nr/Anxiety.\nTrauma- and stressor-related com-\nmunities were also prominent, led by r/CPTSD\n(422 posts).\nSubstantial activity was observed\nin obsessive-compulsive (r/OCD, 392 posts) and\nneurodevelopmental communities (r/ADHD and\nr/autism, over 900 posts combined).\nGen-\neral mental health subreddits (e.g., r/therapy,\nr/therapists, r/mentalhealth) also contained\nlarge volumes of AI-related discussions. Table A.3\nshows the distribution of posts across DSM-5 cate-\ngories, and Figure A.2 shows their temporal trends.\n4.2\nModel Performance Across Dimensions\nWe evaluated five LLMs on the human-annotated\nset across all categorical dimensions using pre-\ncision, recall, and macro-averaged F1. Table 2\nreports F1 scores, with full precision and recall\nin Table A.1. Across core and extended TAM di-\nmensions, GPT-5.2 achieved the highest F1 scores\non perceived\nusefulness (F1 = 0.72), ease\nof use (0.76), perceived trust (0.65), output\nquality\n(0.85),\nresult\ndemonstrability\n(0.82), intention to continue (0.72). Gem-\nini 3 Pro outperformed other models on social\nDimension\nGPT Gem Cla Kimi Qwen\nperceived_usefulness\n0.72\n0.66\n0.69\n0.65\n0.64\nease_of_use\n0.76\n0.49\n0.34\n0.53\n0.27\nperceived_trust\n0.65\n0.64\n0.64\n0.49\n0.51\noutput_quality\n0.85\n0.71\n0.84\n0.67\n0.63\nresult_demonstrability\n0.82\n0.67\n0.79\n0.69\n0.59\nintention_to_continue\n0.72\n0.66\n0.70\n0.59\n0.41\nsocial_influence\n0.73\n0.82\n0.72\n0.56\n0.75\nperceived_risks\n0.70\n0.84\n0.80\n0.77\n0.70\nbond\n0.63\n0.78\n0.68\n0.59\n0.42\ntask\n0.63\n0.71\n0.70\n0.64\n0.63\ngoal\n0.57\n0.64\n0.64\n0.40\n0.56\ncomparison_to_therapy\n0.64\n0.63\n0.59\n0.61\n0.41\nsentiment\n0.77\n0.68\n0.75\n0.70\n0.62\nMacro F1\n0.70\n0.68\n0.67\n0.60\n0.59\nTable 2: F1 scores by dimension and model. Best score\nper dimension is shown in bold. GPT = GPT-5.2; Gem\n= Gemini 3 Pro; Cla = Claude Opus 4.5.\ninfluence (0.82) and perceived risks (0.84).\nPerformance on therapeutic alliance dimensions\ndiffered from other constructs and remained among\nthe most challenging for all models. Gemini 3\nPro achieved the highest F1 scores for bond (0.78),\ntask (0.71), and goal (0.64).\nBased on these results, we adopted a hybrid an-\nnotation strategy for the full dataset of 5,126 posts:\nGPT-5.2 for TAM-based evaluative and outcome\ndimensions, and Gemini 3 Pro for therapeutic al-\nliance, social influence, and perceived risk. Ta-\nble A.6 summarizes value distributions across di-\nmensions and Table A.7 shows examples of anno-\ntated data. Most posts did not explicitly mention\nbond, comparison to therapy, ease of use,\nperceived trust, and intention to continue\nwere frequently not_mentioned.\nIn contrast,\nperceived\nusefulness, perceived\nrisks,\noutput quality, and result demonstrability\nwere more commonly expressed. Sentiment to-\nward AI use was predominantly neutral, with posi-\ntive sentiment more frequent than negative, while\nsocial influence was rarely mentioned. Task\nand goal alignment were more often aligned than\nmisaligned when present.\n4.3\nRQ1: Patterns of AI Engagement in\nMental Health Communities\nTo characterize how AI is used and perceived in\nmental health contexts, we analyzed patterns of\nengagement reflected in users’ discourse.\nReported Usage Intent. We examined reported\nusage\nintents in relation to sentiment and\ncomparisons\nto\nhuman\ntherapy, and varia-\ntion across mental health conditions. usage\n5\n"}, {"page": 6, "text": "80\n60\n40\n20\n0\n20\n40\n60\nPercentage\nSexual Roleplay\nReassurance\nCompanionship\nClinical Support\nWriting Support\nCrisis Support\nVenting\nSymptom Assessment\nPsychoeducation\nTherapy Adjunct\nEmotional Support\nFunctional Support\nSelf Exploration\nRecovery Support\nNegative\nPositive\n(a) Sentiment distribution by task category\n0\n20\n40\n60\n80\n100\nClinical Support\nWriting Support\nCrisis Support\nRecovery Support\nReassurance\nSymptom Assessment\nSelf Exploration\nVenting\nFunctional Support\nPsychoeducation\nCompanionship\nEmotional Support\nTherapy Adjunct\nComplementary\nWorse\nBetter\n(b) User evaluations of AI relative to human therapy\nby task category\nFigure 1: Sentiment and comparative evaluations by AI usage category. Left: sentiment distribution by task. Right:\nAI positioning relative to human therapy.\nintent categories were derived via LLM-guided\nthematic analysis, which standardized free-text de-\nscriptions generated by LLM into a coherent taxon-\nomy (e.g., Emotional Support, Venting, Compan-\nionship; see Table A.5 for the full list). Additional\nanalysis of usage–condition co-occurrence is re-\nported in the Appendix A.6.\nAcross the corpus, Emotional Support was the\nmost prevalent (18.0%) (Figure A.3), encompass-\ning empathy and emotional validation (Table A.5).\nFunctional Support (12.6%) and Psychoeducation\n(11.7%) were also common, reflecting organiza-\ntional assistance for ADHD and autism and learn-\ning about anxiety or coping strategies. Other com-\nmon intents included Companionship (9.4%), Reas-\nsurance Seeking (7.6%), often linked to anxiety- or\nOCD-related validation, and Symptom Assessment\n(7.3%). Venting (6.7%) and Self Exploration (6.5%)\nappeared more often in CPTSD-related posts.\nSentiment Across Tasks. Sentiment toward AI\nvaried across usage intents. Intents involving\nsustained or reflective engagement, such as Recov-\nery Support, Self Exploration, Functional Support,\nand Emotional Support, were more often associ-\nated with positive sentiment (users describing AI as\nhelpful, validating, or confidence boosting), while\nSexual Roleplay, Reassurance Seeking, and Com-\npanionship showed higher negative sentiment, of-\nten reflecting guilt, symptom worsening, or emo-\ntional dependence. Task-level patterns are shown\nin Figures 1a and 1b.\nComparison to Human Therapy. Comparisons\nbetween AI and human therapy were relatively rare:\n639 posts described AI as worse, 163 as better, and\n478 as complementary. AI was most often framed\nas complementary, particularly for Functional Sup-\nport, Self Exploration, and Recovery Support. In\ncontrast, Crisis Support and Reassurance Seeking\nwere more frequently judged as worse, citing inef-\nfective responses, safety concerns, or lack of ap-\npropriateness. Reports describing AI as better than\ntherapy were uncommon and typically reflected\nbarriers to accessing professional care.\nConcerns/Risks.\nAlongside benefits, users fre-\nquently reported concerns about AI in therapeutic\ncontexts: 2,637 of 5,126 posts (51%) explicitly\nmentioned risks or limitations (Figure 2). Using\nLLM-guided thematic analysis, we derived a stan-\ndardized risk taxonomy (Table A.4). The most\ncommon concern was Addiction and Dependence\n(14.1%), describing emotional reliance and com-\npulsive use which co-occurred most strongly with\nCompanionship intent (Figure A.5). Symptom Es-\ncalation followed (11.7%), including intensified\nanxiety, rumination, and trauma responses. Mis-\ninformation and Error (9.6%) was most strongly\nassociated with Symptom Assessment intent, re-\nflecting concerns about incorrect interpretation or\nadvice. Privacy and Data risks (9.4%) clustered\naround Clinical Support, where users discussed\ndocumentation, records, or sensitive information\nhandling. Reassurance Loops were concentrated\nwithin Reassurance Seeking, indicating a tight cou-\npling between repeated reassurance and anxiety re-\ninforcement. Although less frequent, Child Safety\nrisks appeared disproportionately in Sexual Role-\nplay, and Stigma and Shame and Self-Harm ap-\npeared less often but raised concerns about judg-\nment, suicidal ideation, or escalation of harm.\n4.4\nRQ2: Adoption Pathways in AI for\nMental Health\nPrior TAM work typically treats intention to\ncontinue as the outcome variable (Dhagarra et al.,\n6\n"}, {"page": 7, "text": "0\n50\n100\n150\n200\n250\n300\n350\n400\nCount\nOther\nChild Safety\nCost & Access\nAvoidant Coping\nAnthropomorphism\nSelf-Harm Risk\nStigma & Shame\nAI Limitations\nEffectiveness Limits\nEthics & Governance\nSocial Harm\nReassurance Loops\nTherapy Boundary Confusion\nPrivacy & Data\nMisinformation & Error\nSymptom Escalation\nAddiction & Dependence\n0.8%\n1.2%\n1.5%\n2.9%\n2.9%\n3.8%\n3.9%\n5.2%\n5.2%\n5.8%\n5.9%\n7.5%\n8.7%\n9.4%\n9.6%\n11.7%\n14.1%\nFigure 2: Reported Risks/Concerns\n2020; Kim et al., 2012), but explicit intentions were\nsparse in our corpus: only 700 of 5,126 posts men-\ntioned continued use, and 186 expressed no in-\ntention. We therefore used sentiment toward AI\nas a proxy for adoption-related attitudes. Senti-\nment was more frequently expressed, with 2,198\nneutral, 1,782 positive, and 1,120 negative. We\ntreated positive sentiment as indicative of contin-\nued use and negative sentiment as indicative of\ndiscontinuation, excluding neutral cases. We ex-\namined associations between TAM dimensions and\nsentiment using chi-squared tests and Cramér’s\nV , limiting analyses to categorical dimensions and\nexcluding not_mentioned values per dimension.\nAs shown in Table 3, sentiment showed strong asso-\nciations with result demonstrability, output\nquality, and perceived trust (all V > 0.90,\np < .001), and perceived usefulness (V =\n0.76, p < .001). Perceived risks and ease of\nuse showed moderate associations, while social\ninfluence was not significant.\n4.5\nRQ3: Therapeutic Alliance and Its Role in\nAI Engagement\nWe examined three therapeutic alliance dimensions\nin AI-mediated interactions: task alignment, goal\nalignment, and bond. We defined an overall thera-\npeutic alliance as strongly aligned when task and\ngoal were aligned and bond was strong, and as mis-\naligned when task and goal were misaligned and\nbond was weak. Using this definition, 179 posts\nshowed a strongly aligned alliance and 83 a mis-\naligned one. Strongly aligned alliances were most\ncommon in Emotional Support and Functional Sup-\nport tasks. In contrast, misaligned alliances ap-\npeared more frequently in companionship-oriented\nuse. Posts describing a strong bond alongside neg-\native sentiment frequently reported overattachment\nrisks, including addiction and emotional depen-\ndence. Associations with sentiment were tested\nDimension\nV\nχ2\np\nn\nResult demonstrability\n0.95\n1906.51\n< .001\n2,120\nPerceived trust\n0.91\n635.62\n< .001\n764\nOutput quality\n0.90\n1595.30\n< .001\n1,981\nPerceived usefulness\n0.76\n1505.51\n< .001\n2,612\nPerceived risks\n0.43\n543.38\n< .001\n2,902\nEase of use\n0.40\n86.62\n< .001\n538\nSocial influence\n0.02\n1.92\n.165\n2,902\nTable 3: Associations between TAM dimensions and\nsentiment toward AI use for mental health support.\nCramér’s V and chi-squared statistics are reported. Neu-\ntral sentiment cases were excluded.\nDimension\nV\nχ2\np\nn\nOverall alliance\n0.95\n223.42\n< .001\n249\nTask alignment\n0.93\n2101.80\n< .001\n2414\nGoal alignment\n0.79\n1231.43\n< .001\n1954\nBond\n0.12\n12.66\n< .001\n855\nTable 4: Associations between therapeutic alliance di-\nmensions and sentiment toward AI use for mental health\nsupport, measured using chi-squared tests and Cramér’s\nV . Neutral sentiment cases were excluded.\nusing chi-squared tests and Cramér’s V . Strong\nassociations were observed for overall alliance\n(V = 0.95), task alignment (V = 0.93), and\ngoal alignment (V = 0.79), while bond showed\na much weaker association (V = 0.12). Results\nare reported in Table 4.\n5\nDiscussion\nFrom Outcomes to Engagement.\nAcross\nadoption-related analyses, result demonstrability\nemerged as the strongest predictor of adoption-\nrelated sentiment and continued use. Rather than\nabstract judgments of system quality, users ex-\npressed demonstrability through narrated change,\naccounts of improvement, deterioration, or am-\nbivalence (e.g., better sleep, heightened rumina-\ntion), a pattern well documented in online mental\nhealth communities (Andalibi et al., 2017; Naslund\net al., 2020). Although TAM extensions treat result\ndemonstrability as a driver of adoption (Venkatesh\nand Davis, 2000; Menzli et al., 2022), they typ-\nically operationalize it via surveys or task out-\ncomes. Our results show that in naturalistic mental\nhealth discourse, adoption attitudes are embedded\nin narrative sensemaking grounded in felt changes\nrather than abstract utility (Andalibi et al., 2017;\nDe Choudhury et al., 2013). These findings sug-\ngest prioritizing narrated outcomes in naturalistic\ndiscourse over preference or engagement metrics\nin NLP evaluation.\nWhen Emotional Bond Is Not Enough. We ob-\nserve a clear asymmetry across therapeutic alliance\n7\n"}, {"page": 8, "text": "dimensions: task and goal alignment are strongly\nassociated with positive sentiment toward AI use,\nwhile emotional bond shows a much weaker rela-\ntionship. Users describing AI as supporting struc-\ntured, goal-directed psychological work reported\nmore positive experiences, consistent with evi-\ndence that skills-oriented interactions better align\nwith current AI capabilities (Im and Woo, 2025).\nIn contrast, emotional bond was most common\nin companionship and reassurance-seeking con-\ntexts, where positive sentiment was less frequent\nand risks such as dependence, compulsive use, and\nsymptom escalation were often reported (Babu and\nJoseph, 2025). These findings indicate that emo-\ntional bond alone is insufficient; when it forms\nwithout task and goal alignment, engagement may\nshift toward reassurance loops that sustain distress,\nas described in clinical models of anxiety (Starr\net al., 2023; Rector et al., 2011). From an NLP per-\nspective, this cautions against equating empathetic\nlanguage with therapeutic suitability, as surface-\nlevel empathy can mask deeper misalignments and\nharms (Bender et al., 2021; Sharma et al., 2020;\nRoshanaei et al., 2025).\nRisk as an Emergent and Moral Property of AI\nEngagement. Risks in AI-mediated mental health\nsupport are common and typically emerge from\npatterns of engagement rather than isolated fail-\nures. Over half of posts reference concerns, most\noften dependence, symptom escalation, misinfor-\nmation, and privacy, described as cumulative trajec-\ntories involving repeated reassurance, emotional\nreliance, or difficulty disengaging.\nThis aligns\nwith prior human-centered NLP work showing that\nmany harms arise from usage patterns rather than\nindividual outputs (Blodgett et al., 2020; Ehsan\net al., 2022). Beyond psychological or informa-\ntional harm, users’ discourse reveals moral tensions\naround AI reliance. Many express guilt, shame, or\nself-judgment for turning to AI (e.g., “pathetic,”\n“embarrassing,” “wrong”), even when reporting\nbenefit, echoing prior work on stigmatized help-\nseeking and digital mental health use (Andalibi\net al., 2017; Naslund et al., 2016). Rather than dis-\nsatisfaction with responses, conflicted sentiment of-\nten reflects discomfort with reliance itself as users\nnegotiate the legitimacy of AI support (“I know it’s\njust an AI, but I have a safe place to talk”). Attend-\ning to such moralized expressions can surface early\nsigns of potentially problematic engagement that\nmay not be captured by sentiment or engagement\nmetrics alone (Corrigan et al., 2014).\nAI as Between-Session Mental Health Infras-\ntructure.\nUsers rarely frame AI as a replace-\nment for professional therapy, instead describing\nit as complementary support when human care is\nunavailable, inaccessible, or insufficient—a pat-\ntern widely noted in digital mental health research\n(Bhatt, 2025). This framing helps explain why\nfunctional support, self-exploration, and psychoed-\nucation receive more positive evaluations, as these\ntasks align with AI’s role as an auxiliary resource\nextending therapeutic work beyond the clinical en-\ncounter. Conceptualizing AI as between-session\nmental health infrastructure clarifies both its value\nand its limits, shifting NLP research away from\nreplacement narratives toward questions of reliabil-\nity, boundary-setting, and support for self-directed\nwork (Ehsan et al., 2022). At the same time, the\nwidespread use of general-purpose LLMs, partic-\nularly ChatGPT, as de facto mental health tools\nraises governance concerns: despite lacking health-\nspecific safeguards, these systems are often treated\nas trustworthy, exposing users to privacy risks and\nblurred boundaries between informal support and\nprofessional care (MIT Technology Review, 2025).\nThis gap between intended design and actual use\nunderscores the need to study how people appropri-\nate general-purpose NLP systems for mental health\nneeds (Bender et al., 2021; Na et al., 2025).\nOverall, our findings show that capturing key\ndynamics of AI-mediated mental health support is\nessential for understanding real-world human–AI\nengagement. Analyzing naturalistic user discourse\nwith theory-informed constructs shows that in sen-\nsitive domains, how people live with AI matters as\nmuch as what AI produces.\n6\nConclusion\nWe studied how people evaluated and related to\nLLMs used for mental health support in naturalis-\ntic online discourse. Analyzing 5,126 Reddit posts\nacross 47 mental health communities, we integrated\nTechnology Acceptance Model and therapeutic al-\nliance frameworks to examine adoption, evaluation,\nand relational alignment at scale. Engagement was\ndriven primarily by perceived outcomes, trust, and\nresponse quality, with positive experiences most\nstrongly associated with task and goal alignment\nrather than emotional bond alone. Users also re-\nported concerns about dependence, symptom esca-\nlation, and misinformation, highlighting tensions\nbetween perceived support and potential harm.\n8\n"}, {"page": 9, "text": "7\nLimitations\nThis study relies on Reddit data, and findings\nshould be interpreted in light of the platform’s de-\nmographic and cultural biases. Subreddit-specific\nnorms shape how mental health experiences and AI\nuse are discussed, limiting generalizability to other\npopulations, offline settings, or clinical contexts.\nOur analysis is also restricted to English-language\nposts, excluding perspectives from other linguis-\ntic and cultural settings. As with most analyses\nof online discourse, the data reflect self-selected\nusers who choose to publicly discuss AI and mental\nhealth, and may overrepresent more salient, polar-\nized, or reflective experiences. We adapt constructs\nfrom the Technology Acceptance Model and ther-\napeutic alliance theory, which were developed for\nsurvey-based studies and human psychotherapy. In\nthis work, these frameworks serve as interpretive\nlenses for discourse analysis rather than formal\ntests of the theories themselves. Accordingly, our\nfindings should not be read as validating or refut-\ning these models, but as illustrating how their con-\nstructs surface in naturalistic user narratives.\nOur annotation pipeline relies in part on large\nlanguage models, introducing the possibility of mis-\nclassification, particularly for nuanced constructs\nsuch as sentiment and relational alignment. LLM-\nbased annotation may also reflect normative as-\nsumptions that influence labeling. We mitigated\nthese risks through human validation, agreement\nanalysis, and conservative interpretation, but errors\nand biases may remain. Finally, our analysis op-\nerates at the level of public discourse rather than\nlongitudinal interaction traces or observed behavior.\nWhile Reddit posts capture rich evaluative and re-\nlational signals, they cannot directly reveal within-\nuser trajectories, offline behavior, or causal effects\nof AI use over time. Importantly, this study does\nnot evaluate mental health outcomes, therapeutic\nefficacy, or clinical safety. Self-reported experi-\nences in public discourse should not be interpreted\nas evidence of benefit or harm at the individual\nor population level. Future work combining dis-\ncourse analysis with longitudinal interaction data,\ninterviews, or diary-based methods could more pre-\ncisely characterize escalation, disengagement, and\nrecovery dynamics.\n8\nEthics Statement\nThis study analyzes publicly available Reddit posts\nin which users discuss sensitive mental health ex-\nperiences. We designed our methodology to mini-\nmize potential harm and respect user privacy. All\ndata were collected from public subreddits in ac-\ncordance with Reddit’s terms of service, and we\nmade no attempts to identify, profile, contact, or\ninteract with individual users, including potentially\nvulnerable populations, nor to intervene in or in-\nfluence ongoing discussions. To reduce the risk of\nre-identification, we do not release raw post text.\nInstead, we will share post identifiers (e.g., Reddit\nIDs) and derived annotation labels, enabling re-\nproducibility for researchers with appropriate data\naccess while limiting exposure of sensitive content.\nAll analyses were conducted at the aggregate level,\nand examples were paraphrased or abstracted to\navoid revealing identifiable or distressing details.\nWe recognize the risk that findings from this work\ncould be misinterpreted or misused to overstate the\ntherapeutic value of AI systems or to justify their\ndeployment as substitutes for professional mental\nhealth care. Our results are intended to inform re-\nsponsible evaluation, design, and governance of AI\nsystems in mental health contexts, not to recom-\nmend clinical use or automated intervention. The\nAI systems discussed in this study are not substi-\ntutes for trained clinicians and should only be used\nwith appropriate safeguards, transparency, and eth-\nical oversight.\n9\nAcknowledgement\nWe thank Darshit Rai for his support in data collec-\ntion and for validating the relevancy annotations.\nHis careful review and feedback played an impor-\ntant role in maintaining the quality of the annotated\ndata. We also thank OpenAI for the research cred-\nits.\nReferences\nElham Aghakhani, Lu Wang, Karla T. Washington,\nGeorge Demiris, Jina Huh-Yoo, and Rezvaneh Reza-\npour. 2025. From conversation to automation: Lever-\naging LLMs for problem-solving therapy analysis.\nIn Findings of the Association for Computational\nLinguistics: ACL 2025, pages 25189–25207, Vienna,\nAustria. Association for Computational Linguistics.\nFalwah Alhamed, Rebecca Bendayan, Julia Ive, and\nLucia Specia. 2024. Monitoring depression severity\nand symptoms in user-generated content: An anno-\ntation scheme and guidelines. In Proceedings of the\n14th Workshop on Computational Approaches to Sub-\njectivity, Sentiment, & Social Media Analysis, pages\n227–233, Bangkok, Thailand. Association for Com-\nputational Linguistics.\n9\n"}, {"page": 10, "text": "Nazanin Andalibi, Pinar Ozturk, and Andrea Forte.\n2017. Sensitive self-disclosures, responses, and so-\ncial support on instagram: The case of #depression.\nIn Proceedings of the 2017 ACM Conference on Com-\nputer Supported Cooperative Work and Social Com-\nputing, CSCW ’17, page 1485–1500, New York, NY,\nUSA. Association for Computing Machinery.\nAnthropic. 2025. Introducing claude opus 4.5. https:\n//www.anthropic.com/news/claude-opus-4-5.\nPublished Nov 24 2025, Accessed Jan 1 2026.\nAnithamol Babu and Akhil P Joseph. 2025. Digital well-\nness or digital dependency? a critical examination of\nmental health apps and their implications. Frontiers\nin Psychiatry, 16:1581779.\nClare Beatty, Tanya Malik, Saha Meheli, and Chaitali\nSinha. 2022.\nEvaluating the therapeutic alliance\nwith a free-text cbt conversational agent (wysa): a\nmixed-methods study. Frontiers in Digital Health,\n4:847991.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big?\nIn Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY,\nUSA. Association for Computing Machinery.\nSandhya Bhatt. 2025. Digital mental health: Role of\nartificial intelligence in psychotherapy. Annals of\nNeurosciences, 32(2):117–127.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020.\nLanguage (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nEdward S Bordin. 1979. The generalizability of the\npsychoanalytic concept of the working alliance. Psy-\nchotherapy: Theory, research & practice, 16(3):252.\nLayla Bouzoubaa, Elham Aghakhani, and Rezvaneh\nRezapour. 2024a. Words matter: Reducing stigma in\nonline conversations about substance use with large\nlanguage models. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 9139–9156, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nLayla Bouzoubaa, Elham Aghakhani, Max Song, Quang\nTrinh, and Shadi Rezapour. 2024b. Decoding the nar-\nratives: Analyzing personal drug experiences shared\non Reddit. In Findings of the Association for Com-\nputational Linguistics: ACL 2024, pages 6131–6148,\nBangkok, Thailand. Association for Computational\nLinguistics.\nLayla Bouzoubaa, Jordyn Young, and Rezvaneh Reza-\npour. 2024c. Exploring the landscape of drug com-\nmunities on reddit: A network study. In Proceedings\nof the 2023 IEEE/ACM International Conference on\nAdvances in Social Networks Analysis and Mining,\nASONAM ’23, page 558–565, New York, NY, USA.\nAssociation for Computing Machinery.\nCNBC. 2021. Cost and accessibility of mental health\ncare in america. CNBC.\nKenneth Mark Colby. 1975. Artificial paranoia: A com-\nputer simulation model of paranoid processes.\nPatrick W Corrigan, Benjamin G Druss, and Deborah A\nPerlick. 2014. The impact of mental illness stigma\non seeking and participating in mental health care.\nPsychological science in the public interest, 15(2):37–\n70.\nNicholas Cummins, Faith Matcham, Julia Klapper, and\nBjörn Schuller. 2020. Artificial intelligence to aid the\ndetection of mood disorders. In Artificial Intelligence\nin Precision Health, pages 231–255. Elsevier.\nWilliam D. Lewis, Haotian Zhu, Keaton Strawn, and\nFei Xia. 2025. Tapping into social media in crisis:\nA survey. In Proceedings of the Fourth Workshop\non NLP for Positive Impact (NLP4PI), pages 306–\n331, Vienna, Austria. Association for Computational\nLinguistics.\nShih-Chieh Dai, Aiping Xiong, and Lun-Wei Ku. 2023.\nLLM-in-the-loop: Leveraging large language model\nfor thematic analysis. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n9993–10001, Singapore. Association for Computa-\ntional Linguistics.\nFred D Davis, Richard P Bagozzi, and Paul R Warshaw.\n1989. Technology acceptance model. J Manag Sci,\n35(8):982–1003.\nMunmun De Choudhury, Michael Gamon, Scott Counts,\nand Eric Horvitz. 2013. Predicting depression via\nsocial media. In Proceedings of the international\nAAAI conference on web and social media, volume 7,\npages 128–137.\nRaziye Dehbozorgi, Sanaz Zangeneh, Elham Khooshab,\nDonya Hafezi Nia, Hamid Reza Hanif, Pooya\nSamian, Mahmoud Yousefi, Fatemeh Haj Hashemi,\nMorteza Vakili, Neda Jamalimoghadam, et al. 2025.\nThe application of artificial intelligence in the field of\nmental health: a systematic review. BMC psychiatry,\n25(1):132.\nDevendra Dhagarra, Mohit Goswami, and Gopal Kumar.\n2020. Impact of trust and privacy concerns on tech-\nnology acceptance in healthcare: an indian perspec-\ntive. International journal of medical informatics,\n141:104164.\nAP Diagnostic. 2013. Statistical manual of mental dis-\norders: Dsm-5 (ed.) washington.\nDC: American\nPsychiatric Association.\n10\n"}, {"page": 11, "text": "Anca Dinu and Andreea-Codrina Moldovan. 2021. Au-\ntomatic detection and classification of mental ill-\nnesses from general social media texts. In Proceed-\nings of the International Conference on Recent Ad-\nvances in Natural Language Processing (RANLP\n2021), pages 358–366, Held Online. INCOMA Ltd.\nUpol Ehsan, Philipp Wintersberger, Q. Vera Liao, Eliza-\nbeth Anne Watkins, Carina Manger, Hal Daumé III,\nAndreas Riener, and Mark O Riedl. 2022. Human-\ncentered explainable ai (hcxai): Beyond opening the\nblack-box of ai. In Extended Abstracts of the 2022\nCHI Conference on Human Factors in Computing\nSystems, CHI EA ’22, New York, NY, USA. Associa-\ntion for Computing Machinery.\nMartin Fishbein. 1979. A theory of reasoned action:\nsome applications and implications.\nKathleen Kara Fitzpatrick, Alison Darcy, and Molly\nVierhile. 2017. Delivering cognitive behavior ther-\napy to young adults with symptoms of depression\nand anxiety using a fully automated conversational\nagent (woebot): a randomized controlled trial. JMIR\nmental health, 4(2):e7785.\nAlyson Gamble. 2020. Artificial intelligence and mo-\nbile apps for mental healthcare: a social informatics\nperspective. Aslib Journal of Information Manage-\nment, 72(4):509–523.\nGoogleDeepMind. 2025.\nGemini 2.5 pro.\nhttps:\n//deepmind.google/models/gemini/pro/.\nAc-\ncessed on January 1, 2026.\nSarah Graham, Colin Depp, Ellen E Lee, Camille\nNebeker, Xin Tu, Ho-Cheol Kim, and Dilip V Jeste.\n2019. Artificial intelligence for mental health and\nmental illnesses: an overview. Current psychiatry\nreports, 21(11):116.\nBertram Højer, Terne Sasha Thorn Jakobsen, Anna\nRogers, and Stefan Heinrich. 2025. Research com-\nmunity perspectives on “intelligence” and large lan-\nguage models. In Findings of the Association for\nComputational Linguistics: ACL 2025, pages 25796–\n25812, Vienna, Austria. Association for Computa-\ntional Linguistics.\nChang-Ha Im and Minjung Woo. 2025. Clinical effi-\ncacy, therapeutic mechanisms, and implementation\nfeatures of cognitive behavioral therapy–based chat-\nbots for depression and anxiety: Narrative review.\nJMIR Mental Health, 12(1):e78340.\nBecky Inkster, Shubhankar Sarda, Vinod Subramanian,\net al. 2018. An empathy-driven, conversational artifi-\ncial intelligence agent (wysa) for digital mental well-\nbeing: real-world data evaluation mixed-methods\nstudy. JMIR mHealth and uHealth, 6(11):e12106.\nZhengping Jiang, Sarah Ita Levitan, Jonathan Zomick,\nand Julia Hirschberg. 2020. Detection of mental\nhealth from Reddit via deep contextualized repre-\nsentations. In Proceedings of the 11th International\nWorkshop on Health Text Mining and Information\nAnalysis, pages 147–156, Online. Association for\nComputational Linguistics.\nJeongeun Kim, Hyeoun-Ae Park, et al. 2012. Develop-\nment of a health information technology acceptance\nmodel using consumers’ health behavior intention.\nJournal of medical Internet research, 14(5):e2143.\nPaul Marshall, Millissa Booth, Matthew Coole, Lau-\nren Fothergill, Zoe Glossop, Jade Haines, Andrew\nHarding, Rose Johnston, Steven Jones, Christopher\nLodge, et al. 2024. Understanding the impacts of\nonline mental health peer support forums: realist\nsynthesis. JMIR Mental Health, 11:e55750.\nRakesh K Maurya, Steven Montesinos, Mikhail Bogo-\nmaz, and Amanda C DeDiego. 2025. Assessing the\nuse of chatgpt as a psychoeducational tool for men-\ntal health practice. Counselling and Psychotherapy\nResearch, 25(1):e12759.\nLeila Jamel Menzli, Lassaad K Smirani, Jihane A\nBoulahia, and Myriam Hadjouni. 2022. Investigation\nof open educational resources adoption in higher ed-\nucation using rogers’ diffusion of innovation theory.\nHeliyon, 8(7).\nRutvij Merchant, Aleah Goldin, Deepa Manjanatha,\nClaire Harter, Judy Chandler, Amanda Lipp, Theresa\nNguyen, and John A Naslund. 2022. Opportunities\nto expand access to mental health services: A case\nfor the role of online peer support communities. Psy-\nchiatric Quarterly, 93(2):613–625.\nMIT Technology Review. 2025. Some therapists are\nusing chatgpt in secret. that’s a huge risk. Accessed:\n2025-09-15.\nMoonshotAI. 2025.\nmoonshotai/kimi-k2-instruct.\nhttps://huggingface.co/moonshotai/\nKimi-K2-Instruct.\nAccessed on January 1,\n2026.\nHongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei\nYu, Lilin Wang, Wei Wang, John Torous, and Ling\nChen. 2025. A survey of large language models in\npsychotherapy: Current landscape and future direc-\ntions. In Findings of the Association for Compu-\ntational Linguistics: ACL 2025, pages 7362–7376,\nVienna, Austria. Association for Computational Lin-\nguistics.\nUsman Naseem, Gautam Siddharth Kashyap, Kaixuan\nRen, Yiran Zhang, Utsav Maskey, Juan Ren, and\nAfrozah Nadeem. 2025. Alignment of large language\nmodels with human preferences and values. In Pro-\nceedings of the 23rd Annual Workshop of the Aus-\ntralasian Language Technology Association, pages\n245–245, Sydney, Australia. Association for Compu-\ntational Linguistics.\nJohn A Naslund, Kelly A Aschbrenner, Lisa A Marsch,\nand Stephen J Bartels. 2016. The future of mental\nhealth care: peer-to-peer support and social media.\nEpidemiology and psychiatric sciences, 25(2):113–\n122.\n11\n"}, {"page": 12, "text": "John A Naslund, Ameya Bondre, John Torous, and\nKelly A Aschbrenner. 2020. Social media and mental\nhealth: benefits, risks, and opportunities for research\nand practice. Journal of technology in behavioral\nscience, 5(3):245–257.\nVíctor\nHugo\nOjeda\nMeixueiro,\nLaura\nPérez-\nCampos Mayoral, María Teresa Hernández Huerta,\nCarlos Alberto Matias-Cervantes, Eduardo Pérez\nCampos Mayoral, Elí Cruz Parada, and Eduardo\nPérez-Campos. 2024. Relevance of a customized ver-\nsion of chatgpt explaining laboratory test results in\npatient education. Journal of Medical Education and\nCurricular Development, 11:23821205241260239.\nOpenAI. 2024. Gpt-4o mini: advancing cost-efficient\nintelligence. https://openai.com/index/gpt-4o-mini-\nadvancing-cost-efficient-intelligence/.\nAccessed:\n2025-09-15.\nOpenAI. 2025.\nIntroducing GPT-5.2.\nhttps://\nopenai.com/index/introducing-gpt-5-2/. Ac-\ncessed on January 1, 2026.\nAria Pessianzadeh, Naima Sultana, Hildegarde Van den\nBulck, David Gefen, Shahin Jabari, and Rezvaneh\nRezapour. 2025. In generative ai we (dis) trust? com-\nputational analysis of trust and distrust in reddit dis-\ncussions. arXiv preprint arXiv:2510.16173.\nQwen.\n2025.\nQwen3-next-80b-a3b-\ninstruct.\nhttps://huggingface.co/Qwen/\nQwen3-Next-80B-A3B-Instruct.\nApache-2.0\nlicense, accessed January 1, 2026.\nAmy Rayland and Jacob Andrews. 2023. From social\nnetwork to peer support network: opportunities to\nexplore mechanisms of online peer support for mental\nhealth. JMIR mental health, 10:e41855.\nAfsaneh Razi, Layla Bouzoubaa, Aria Pessianzadeh,\nJohn S Seberger, and Rezvaneh Rezapour. 2025. Not\na swiss army knife: Academics’ perceptions of trade-\noffs around generative ai use. Proceedings of the\nAssociation for Information Science and Technology,\n62(1):547–560.\nNeil A Rector, Katy Kamkar, Stephanie E Cassin, Lind-\nsay E Ayearst, and Judith M Laposa. 2011. Assessing\nexcessive reassurance seeking in the anxiety disor-\nders. Journal of Anxiety Disorders, 25(7):911–917.\nMahnaz Roshanaei, Rezvaneh Rezapour, and Magy Seif\nEl-Nasr. 2025. Talk, listen, connect: How humans\nand ai evaluate empathy in responses to emotionally\ncharged narratives. AI & society, pages 1–17.\nAshish Sharma, Adam Miner, David Atkins, and Tim Al-\nthoff. 2020. A computational approach to understand-\ning empathy expressed in text-based mental health\nsupport. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5263–5276.\nMeghan Sit, Sarah A Elliott, Kelsey S Wright, Shan-\nnon D Scott, and Lisa Hartling. 2024. Youth mental\nhealth help-seeking information needs and experi-\nences: a thematic analysis of reddit posts. Youth &\nSociety, 56(1):24–41.\nVera Sorin, Dana Brin, Yiftach Barash, Eli Konen,\nAlexander Charney, Girish Nadkarni, and Eyal Klang.\n2024. Large language models and empathy: system-\natic review. Journal of medical Internet research,\n26:e52597.\nLisa R Starr, Angela C Santee, and Meghan Huang.\n2023. Dependency and excessive reassurance seek-\ning.\nSu Pi Su, Chung Hung Tsai, and Wei Lin Hsu. 2013.\nExtending the tam model to explore the factors af-\nfecting intention to use telecare systems. Journal of\nComputers (Finland), 8(2):525–532.\nAnoushka Thakkar,\nAnkita Gupta,\nand Avinash\nDe Sousa. 2024. Artificial intelligence in positive\nmental health: a narrative review. Frontiers in digital\nhealth, 6:1280235.\nThe New York Times. 2025. I’m a therapist. chatgpt is\neerily effective. The New York Times.\nMeigan Thomson, Gregor Henderson, Tim Rogers, Ben-\njamin Locke, John Vines, and Angus MacBeth. 2024.\nDigital mental health and peer support: Building a\ntheory of change informed by stakeholders’ perspec-\ntives. PLOS Digital Health, 3(5):e0000522.\nAditya Nrusimha Vaidyam,\nHannah Wisniewski,\nJohn David Halamka, Matcheri S Kashavan, and\nJohn Blake Torous. 2019. Chatbots and conversa-\ntional agents in mental health: a review of the psychi-\natric landscape. The Canadian Journal of Psychiatry,\n64(7):456–464.\nViswanath Venkatesh and Fred D Davis. 2000. A theo-\nretical extension of the technology acceptance model:\nFour longitudinal field studies. Management science,\n46(2):186–204.\nLaura M Vowels, Rachel RR Francois-Walcott, and\nJoëlle Darwiche. 2024.\nAi in relationship coun-\nselling: evaluating chatgpt’s therapeutic capabilities\nin providing relationship advice. Computers in Hu-\nman Behavior: Artificial Humans, 2(2):100078.\nJoseph Weizenbaum. 1966. Eliza—a computer program\nfor the study of natural language communication be-\ntween man and machine. Communications of the\nACM, 9(1):36–45.\nChengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye,\nShiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu\nHu, David Jurgens, et al. 2024. Can large language\nmodel agents simulate human trust behavior? Ad-\nvances in neural information processing systems,\n37:15674–15729.\n12\n"}, {"page": 13, "text": "Jordyn Young, Laala M Jawara, Diep N Nguyen, Brian\nDaly, Jina Huh-Yoo, and Afsaneh Razi. 2024. The\nrole of ai in peer support for young people: A study\nof preferences for human-and ai-generated responses.\nIn Proceedings of the 2024 CHI Conference on Hu-\nman Factors in Computing Systems, pages 1–18.\nH Yu and Stephen McGuinness. 2024. An experimen-\ntal study of integrating fine-tuned llms and prompts\nfor enhancing mental health support chatbot system.\nJournal of Medical Artificial Intelligence, pages 1–\n16.\nA\nAppendix\nA.1\nAnnotation Prompt\nFigure A.1 presents the annotation prompt used to\nextract dimensions from Reddit posts.\nA.2\nLLM Performance Across Annotation\nDimensions\nTable A.1 reports precision, recall, and F1 scores\nfor all five evaluated LLMs across the full set of\ncategorical annotation dimensions. Models were\nevaluated against the human-validated reference\nset, and performance varied substantially by dimen-\nsion.\nA.3\nDimension Distributions Across the\nCorpus\nAfter applying the hybrid annotation pipeline to\nthe full set of 5,126 posts, categorical labels for\nall Technology Acceptance Model and therapeu-\ntic alliance dimensions were produced.\nTable\nA.6 presents the proportional distribution of cat-\negorical values for each dimension across the cor-\npus. Across dimensions, not_mentioned was com-\nmon.\nFor example, intention to continue\nwas absent in 82.6% of posts. When expressed,\n13.7% indicated continued or planned AI use, while\n3.6% reported discontinuation or intent to stop.\nSome dimensions were more frequently expressed.\nPerceived usefulness was coded as useful in\n59.3% of posts and not useful in 11.5%.\nFor\noutput quality, 35.9% described good quality\nand 11.0% poor quality, while 53.1% did not in-\nclude an explicit judgment.\nA.4\nDistribution of Usage Intent Categories\nFigure A.3 shows the distribution of reported AI\nusage intents. Emotional Support is the most com-\nmon use, followed by Functional Support and Psy-\nchoeducation, indicating that AI is primarily used\nfor ongoing emotional validation and practical cop-\ning support. Relational intents such as Compan-\nionship and Reassurance Seeking appear less fre-\nquently, while task-specific uses including Symp-\ntom Assessment, Therapy Adjunct, and Clinical\nSupport are relatively rare.\nA.5\nTask–Condition Co-occurrence Pattern\nFigure A.4 shows a heatmap of the co-occurrence\nbetween AI usage tasks and reported mental health\nconditions in the dataset. Rows correspond to usage\ntask categories, and columns correspond to mental\nhealth conditions. Mental health conditions were\nidentified in two ways. When explicitly stated, con-\nditions were extracted from post text using LLM-\nbased annotation. For posts in which no condition\nwas explicitly mentioned, we used the associated\nsubreddit as a proxy for the relevant condition when\napplicable (e.g., posts from r/Anxiety mapped to\nanxiety-related conditions). This approach allowed\nus to capture both self-reported and contextually\ninferred conditions while maintaining broad cov-\nerage across the corpus. Cell intensities reflect\nthe relative frequency of each task–condition pair-\ning, showing how different AI usage tasks are dis-\ntributed across mental health contexts.\nA.6\nRisk–Intent Co-occurrence Patterns\nFigure A.5 shows the co-occurrence between AI\nusage intents and reported risk categories. Per-\ncentages indicate the distribution of risk mentions\nwithin each intent. Risks cluster strongly by in-\ntent. Companionship use is most often associated\nwith Addiction & Dependence, reflecting concerns\nabout emotional reliance. Reassurance Seeking\nis dominated by Reassurance Loops. Privacy &\nData concerns appear most frequently in Clinical\nSupport, while Child Safety risks are concentrated\nin Sexual Roleplay. Misinformation & Error is\nmost prominent in Symptom Assessment. Over-\nall, the heatmap shows that risks are task-specific\nrather than uniform, highlighting the importance\nof intent-aware safety evaluation in AI-mediated\nmental health support.\nA.7\nCategorization of Subreddits under\nDSM-5 Categories\nTable A.2 organizes the selected mental health sub-\nreddits into DSM-5 diagnostic categories. This\ncategorization provides the clinical framing for our\ndataset and ensures coverage across a wide range\nof mental health conditions and community types.\n13\n"}, {"page": 14, "text": "You are a helpful assistant. Your task is to read a Reddit post from r/\\{subreddit\\} and extract structured information about the user's experience using AI tools \n(such as ChatGPT, Character AI, Claude, etc.) for mental health or emotional support.\n\nClassify the post according to the following dimensions. For each, return a label and categorical keywords or descriptors that summarize each dimension that \nsupports your decision, if applicable. These should be short phrases (like topic tags), not full sentences.\n\nReturn your response as a JSON object in the following format:\n{\n  \"ai_tool_mentioned\": \"[e.g., ChatGPT, Character AI, Claude, or not_mentioned]\",\n  \"perceived_usefulness\": \"useful\" | \"not_useful\" | \"not_mentioned\",\n  \"usefulness_topic\": \"...\",\n  \"ease_of_use\": \"easy\" | \"difficult\" | \"not_mentioned\",\n  \"ease_topic\": \"...\",\n  \"perceived_trust\": \"trustworthy\" | \"untrustworthy\" | \"not_mentioned\",\n  \"trust_topic\": \"...\",\n  \"output_quality\": \"good\" | \"poor\" | \"not_mentioned\",\n  \"output_quality_topic\": \"...\",\n  \"result_demonstrability\": \"positive_results\" | \"negative_results\" | \"not_mentioned\",\n  \"result_topic\": \"...\",\n  \"social_influence\": \"present\" | \"absent\",\n  \"social_influence_topic\": \"...\",\n  \"perceived_risks\": \"mentioned\" | \"not_mentioned\",\n  \"risk_topic\": \"...\",\n  \"mental_health_condition\": \"[e.g., anxiety, depression, ADHD, or 'not_mentioned']\",\n  \"motivation_for_use\": \"present\" | \"absent\",\n  \"motivation_topic\": \"...\",\n  \"sentiment\": \"positive\" | \"neutral\" | \"negative\" | \"not_mentioned\",\n  \"sentiment_topic\": \"...\",\n  \"intention_to_continue\": \"yes\" | \"no\" | \"not_mentioned\",\n  \"intention_topic\": \"...\",\n  \"bond\": \"strong\" | \"weak\" | \"not_mentioned\",\n  \"bond_topic\": \"...\",\n  \"task\": \"aligned\" | \"misaligned\" | \"not_mentioned\",\n  \"task_topic\": \"...\",\n  \"goal\": \"aligned\" | \"misaligned\" | \"not_mentioned\",\n  \"goal_topic\": \"...\",\n  \"comparison_to_therapy\": \"better\" | \"worse\" | \"complementary\" | \"not_mentioned\",\n  \"comparison_topic\": \"...\",\n  \"ai_use_purpose\": \"[specific use case category]\"\n}\nUse the following definitions to guide labeling:\n  perceived_usefulness: Is the AI described as helpful or useful for emotional or mental health tasks?  \n  ease_of_use: Is the AI described as easy, convenient, or accessible?  \n  perceived_trust: Does the user indicate the AI is trustworthy, reliable, or safe in a mental health context?\n  output_quality: Does the user judge the AI’s responses as helpful, unhelpful, thoughtful, vague, etc.?  \n  result_demonstrability: Are tangible or behavioral outcomes mentioned? \n  social_influence: Does the user mention peers, Reddit, or others encouraging AI use?  \n  perceived_risks: Mentions of limitations, harm, or risk in using AI for mental health.  \n  mental_health_condition: The condition the user indicates they are suffering from.  \n  motivation_for_use: Why is the user turning to AI instead of alternatives?  \n  sentiment: Overall sentiment toward their AI experience.  \n  intention_to_continue: Does the user plan to use AI again?  \n  bond: Emotional connection or trust the user feels toward the AI.  \n  task: Is the AI helping through meaningful therapeutic actions (e.g., emotional regulation, journaling, coping techniques)?  \n  goal: Do the user and AI share a common purpose (e.g., improving mental health, self-awareness)?  \n  comparison_to_therapy: Comparison with traditional therapy.  \n  ai_use_purpose: The specific mental health task or purpose.  \n\n Finally, read the Reddit post and return only the JSON object.\n}\nFigure A.1: The prompt used to extract dimensions from Reddit posts\n2023-01\n2023-05\n2023-09\n2024-01\n2024-05\n2024-09\n2025-01\n2025-05\n2025-09\nDate\n0\n20\n40\n60\n80\nCount\nFigure A.2: Temporal distribution of AI-related mental\nhealth posts in the dataset.\nA.8\nSubreddit Dataset Statistics\nTable A.3 summarizes the 47 selected subreddits\norganized by DSM-5 categories. For each subred-\ndit, we report subscriber counts, the number of\nposts processed, and the number of posts deemed\nrelevant to AI in mental health contexts.\n0\n200\n400\n600\n800\n1000\nCount\nOther\nNot_mentioned\nSexual Roleplay\nWriting Support\nCrisis Support\nRecovery Support\nClinical Suppport\nTherapy Adjunct\nSelf Exploration\nVenting\nSymptom Assessment\nReassurance Seeking\nCompanionship\nPsychoeducation\nFunctional Support\nEmotional Support\n0.2%\n0.8%\n0.8%\n1.6%\n2.7%\n3.8%\n4.1%\n6.1%\n6.5%\n6.7%\n7.3%\n7.6%\n9.4%\n11.7%\n12.6%\n18.0%\nFigure A.3: Distribution of Usage Intent Categories\nReported by Users\n14\n"}, {"page": 15, "text": "ADHD\nAddiction\nAgoraphobia\nAnger\nAnxiety\nAspergers\nAutism\nBPD\nBipolar Disorder\nBody Dysmorphic Disorder\nCPTSD\nDID\nDelusions\nDementia\nDepression\nDyslexia\nEating Disorder\nGender Dysphoria\nHallucination\nHealth Anxiety\nInsomnia\nLimerence\nMania\nMisophonia\nNPD\nNarcolepsy\nOCD\nPOCD\nPTSD\nPanic Disorder\nParanoia\nPsychosis\nSchizophrenia\nSleep Apnea\nSocial Anxiety\nSomatic\nSuicidal Ideation\nTransGender\nTrauma\nCompanionship\nCrisis Support\nEmotional Support\nFunctional Support\nPsychoeducation\nReassurance\nRecovery Support\nSelf Exploration\nSexual Roleplay\nSymptom Assessment\nTherapy Adjunct\nVenting\nWriting Support\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nFigure A.4: Co-occurrence of Mental Health Conditions and Usage Intents.\nClinical Support\nCompanionship\nCrisis Support\nEmotional Support\nFunctional Support\nPsychoeducation\nReassurance\nRecovery Support\nSelf Exploration\nSexual Roleplay\nSymptom Assessment\nTherapy Adjunct\nVenting\nWriting Support\nIntent category\nAI Limitations\nAddiction & Dependence\nAnthropomorphism\nAvoidant Coping\nChild Safety\nCost & Access\nEffectiveness Limits\nEthics & Governance\nMisinformation & Error\nPrivacy & Data\nReassurance Loops\nSelf-Harm Risk\nSocial Harm\nStigma & Shame\nSymptom Escalation\nTherapy Boundary Confusion\nRisk category\n3.4\n1.7\n5.1\n18.6\n20.3\n7.6\n2.5\n4.2\n11.0\n0.0\n4.2\n6.8\n14.4\n0.0\n0.3\n38.6\n0.9\n23.7\n7.2\n3.6\n6.6\n2.4\n3.9\n2.4\n1.5\n2.4\n4.8\n1.8\n0.0\n20.3\n4.3\n37.7\n10.1\n2.9\n1.4\n1.4\n2.9\n0.0\n1.4\n4.3\n13.0\n0.0\n1.4\n18.6\n0.0\n25.7\n11.4\n5.7\n7.1\n2.9\n8.6\n2.9\n1.4\n5.7\n7.1\n1.4\n0.0\n14.3\n0.0\n14.3\n7.1\n0.0\n0.0\n7.1\n0.0\n50.0\n0.0\n3.6\n3.6\n0.0\n11.8\n2.9\n0.0\n14.7\n26.5\n8.8\n0.0\n0.0\n0.0\n0.0\n8.8\n23.5\n2.9\n0.0\n2.4\n4.8\n2.4\n21.8\n14.5\n17.7\n1.6\n5.6\n5.6\n0.8\n7.3\n8.9\n4.8\n1.6\n17.7\n3.8\n0.8\n14.6\n11.5\n16.9\n5.4\n3.1\n5.4\n0.0\n1.5\n11.5\n6.2\n1.5\n2.7\n1.3\n0.9\n6.7\n7.1\n17.8\n9.8\n2.2\n7.1\n0.0\n36.4\n3.1\n4.0\n0.9\n30.4\n3.2\n1.4\n10.6\n7.8\n6.5\n10.1\n0.9\n5.1\n0.0\n4.6\n8.3\n9.7\n1.4\n0.0\n2.2\n0.0\n2.8\n2.8\n2.8\n73.9\n4.4\n1.7\n0.0\n6.7\n1.1\n1.7\n0.0\n0.0\n8.9\n23.3\n18.9\n1.1\n10.0\n4.4\n2.2\n4.4\n3.3\n10.0\n1.1\n11.1\n1.1\n2.9\n30.7\n5.7\n23.6\n5.7\n9.3\n2.9\n3.6\n2.9\n0.0\n0.0\n6.4\n5.7\n0.7\n0.0\n18.7\n1.1\n12.1\n20.9\n8.8\n5.5\n0.0\n12.1\n1.1\n2.2\n4.4\n7.7\n5.5\n1.1\n13.1\n3.7\n19.9\n7.5\n10.1\n16.1\n2.6\n6.7\n1.5\n7.5\n3.7\n6.0\n0.4\n3.4\n1.9\n4.3\n22.1\n6.7\n14.9\n3.4\n2.9\n8.7\n0.0\n8.2\n14.4\n7.2\n1.9\n0\n10\n20\n30\n40\n50\n60\n70\nPercentage\nFigure A.5: Co-occurrence of Intent Categories within Risk Categories.\n15\n"}, {"page": 16, "text": "Dimensions\nGPT 5.2\nGemini 3 pro\nClaude-opus-4-5\nKimi-K2-Instruct\nQwen3\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nperceived_usefulness\n0.82\n0.68\n0.72\n0.82\n0.70\n0.66\n0.79\n0.70\n0.69\n0.75\n0.70\n0.65\n0.80\n0.61\n0.64\nease_of_use\n0.73\n0.81\n0.76\n0.47\n0.60\n0.49\n0.40\n0.50\n0.34\n0.59\n0.74\n0.53\n0.56\n0.59\n0.27\nperceived_trust\n0.73\n0.67\n0.65\n0.63\n0.70\n0.64\n0.64\n0.67\n0.64\n0.50\n0.59\n0.49\n0.61\n0.73\n0.51\noutput_quality\n0.85\n0.86\n0.85\n0.74\n0.75\n0.71\n0.82\n0.86\n0.84\n0.74\n0.77\n0.67\n0.70\n0.72\n0.63\nresult_demonstrability\n0.81\n0.83\n0.82\n0.82\n0.68\n0.67\n0.80\n0.80\n0.79\n0.78\n0.70\n0.69\n0.69\n0.61\n0.59\nintention_to_continue\n0.85\n0.66\n0.72\n0.62\n0.75\n0.66\n0.67\n0.79\n0.70\n0.58\n0.74\n0.59\n0.51\n0.65\n0.41\nsocial_influence\n0.68\n0.84\n0.73\n0.75\n0.97\n0.82\n0.66\n0.95\n0.72\n0.55\n0.59\n0.56\n0.70\n0.85\n0.75\nperceived_risks\n0.75\n0.78\n0.70\n0.84\n0.88\n0.84\n0.80\n0.85\n0.80\n0.79\n0.84\n0.77\n0.74\n0.77\n0.70\nbond\n0.67\n0.74\n0.63\n0.73\n0.85\n0.78\n0.65\n0.72\n0.68\n0.63\n0.75\n0.59\n0.55\n0.67\n0.42\ntask\n0.61\n0.67\n0.63\n0.80\n0.72\n0.71\n0.66\n0.80\n0.70\n0.66\n0.68\n0.64\n0.75\n0.65\n0.63\ngoal\n0.60\n0.58\n0.57\n0.70\n0.66\n0.64\n0.71\n0.65\n0.64\n0.40\n0.43\n0.40\n0.69\n0.60\n0.56\ncomparison_to_therapy\n0.66\n0.63\n0.64\n0.60\n0.66\n0.63\n0.60\n0.62\n0.59\n0.61\n0.60\n0.61\n0.49\n0.49\n0.41\nsentiment\n0.78\n0.77\n0.77\n0.78\n0.69\n0.68\n0.78\n0.74\n0.75\n0.81\n0.73\n0.70\n0.68\n0.66\n0.62\noverall macro F1\n0.70\n0.68\n0.67\n0.60\n0.59\nTable A.1: Precision (P), recall (R), and F1 scores across models for each dimension.\nA.9\nCategories of User-Reported\nRisks/Concerns\nTable A.4 presents the taxonomy of risks and con-\ncerns expressed by users. Each category includes a\ndefinition and representative examples, illustrating\nthe diverse ways people articulate potential harms\nof AI in mental health.\nA.10\nCategories of User-Reported Usage\nIntents\nTable A.5 outlines the tasks and purposes for which\nAI was used, ranging from supportive interaction\nto coping, skill-building, and administrative sup-\nport. Each category is defined with representative\nexamples drawn from Reddit posts.\nA.11\nIllustrative Annotation Examples\nTable A.7 presents illustrative examples of Reddit\nposts alongside the categorical labels assigned by\nour annotation pipeline. Post excerpts are abridged\nfor readability, and highlighted text indicates seg-\nments most relevant to the assigned labels.\n16\n"}, {"page": 17, "text": "DSM-5 Category\nDefinition\nRelevant Subreddits\nNeurodevelopmental Disorders\nEarly developmental onset; impairments\nin personal, social, academic, or\noccupational functioning.\nr/ADHD, r/autism, r/aspergers,\nr/Dyslexia\nSchizophrenia Spectrum and Other\nPsychotic Disorders\nDisorders with psychosis, delusions,\nhallucinations, or disorganized thinking.\nr/schizophrenia, r/Psychosis\nBipolar and Related Disorders\nMood disorders with episodes of\nmania/hypomania and depression.\nr/bipolar, r/bipolar2, r/BipolarReddit\nDepressive Disorders\nPersistent sadness, emptiness, or\nirritability that significantly impairs\nfunctioning.\nr/depression, r/depression_help\nAnxiety Disorders\nExcessive fear, worry, and related\nbehavioral disturbances.\nr/Anxiety, r/Anxietyhelp, r/socialanxiety,\nr/PanicAttack, r/HealthAnxiety\nObsessive-Compulsive and Related\nDisorders\nObsessions, compulsions, or repetitive\nbehaviors.\nr/OCD\nTrauma- and Stressor-Related Disorders\nDisorders following exposure to trauma\nor stress.\nr/CPTSD, r/ptsd, r/trauma,\nr/SomaticExperiencing\nDissociative Disorders\nDisruptions in consciousness, memory,\nidentity, or perception.\nr/DID, r/Dissociation\nFeeding and Eating Disorders\nDisturbances in eating behaviors that\nimpair health or psychosocial\nfunctioning.\nr/AnorexiaNervosa, r/EatingDisorders,\nr/BingeEatingDisorder\nSubstance-Related and Addictive\nDisorders\nDisorders related to the use of\nsubstances or addictive behaviors.\nr/stopdrinking, r/Drugs, r/addiction\nNeurocognitive Disorders\nPrimary deficit is cognitive decline\n(memory, attention, language).\nr/dementia, r/Alzheimers\nSleep-Wake Disorders\nDisorders affecting quality, timing, or\namount of sleep.\nr/SleepApnea, r/Narcolepsy\nPersonality Disorders\nEnduring patterns of inner experience\nand behavior that deviate from cultural\nexpectations.\nr/NPD, r/personalitydisorders, r/BPD\nDisruptive, Impulse-Control, and\nConduct Disorders\nProblems with emotional or behavioral\nself-control.\nr/Anger\nOther / Transdiagnostic or General\nMental Health\nNot tied to a specific DSM-5 disorder\nbut broadly related to mental health.\nr/mentalhealth, r/MentalHealthSupport,\nr/therapists, r/therapy, r/TalkTherapy,\nr/selfimprovement, r/Mindfulness,\nr/Antipsychiatry, r/asktransgender,\nr/SuicideWatch\nTable A.2: Categorization of selected mental health subreddits under DSM-5 categories.\n17\n"}, {"page": 18, "text": "DSM-5 Category\nSubreddit\n#Subscribers\n#Processed Posts\n#AI Relevant Posts\nNeurodevelopmental\nDisorders\nr/ADHD\n2.1m\n223,457\n50\nr/autism\n477k\n223,075\n158\nr/aspergers\n174k\n43,153\n138\nr/Dyslexia\n34k\n5,692\n21\nSchizophrenia\nSpectrum and Other\nPsychotic Disorders\nr/schizophrenia\n94k\n57,330\n87\nr/Psychosis\n66k\n26,417\n60\nBipolar and Related\nDisorders\nr/bipolar\n262k\n82,815\n38\nr/bipolar2\n81k\n45,712\n55\nr/BipolarReddit\n98k\n35,669\n72\nDepressive Disorders\nr/depression\n1.1m\n255,711\n89\nr/depression_help\n106k\n25,185\n81\nAnxiety Disorders\nr/Anxiety\n777k\n215,176\n495\nr/Anxietyhelp\n177k\n29,226\n84\nr/socialanxiety\n443k\n65,321\n182\nr/PanicAttack\n40k\n15,668\n34\nr/HealthAnxiety\n135k\n873\n4\nObsessive-\nCompulsive and\nRelated Disorders\nr/OCD\n273k\n143,210\n392\nTrauma- and\nStressor-Related\nDisorders\nr/CPTSD\n365k\n167,696\n422\nr/ptsd\n121k\n32,964\n37\nr/trauma\n11k\n2,373\n12\nr/SomaticExperiencing\n25k\n3,279\n7\nDissociative\nDisorders\nr/DID\n78k\n35,054\n62\nr/Dissociation\n33k\n6,627\n12\nFeeding and Eating\nDisorders\nr/AnorexiaNervosa\n67k\n21,974\n26\nr/EatingDisorders\n114k\n15,151\n19\nr/BingeEatingDisorder\n99k\n26,437\n36\nSubstance-Related\nand Addictive\nDisorders\nr/stopdrinking\n597k\n184,391\n82\nr/Drugs\n1.1m\n132,738\n54\nr/addiction\n116k\n30,975\n108\nNeurocognitive\nDisorders\nr/dementia\n47k\n21,623\n65\nr/Alzheimers\n19k\n4,765\n25\nSleep-Wake Disorders\nr/SleepApnea\n74k\n26,395\n17\nr/Narcolepsy\n37k\n15,552\n10\nPersonality Disorders\nr/NPD\n51k\n21,326\n70\nr/personalitydisorders\n8.3k\n1,418\n3\nr/BPD\n342k\n295,909\n34\nDisruptive,\nImpulse-Control, and\nConduct Disorders\nr/Anger\n49k\n6,939\n8\nOther / General\nMental Health\nr/mentalhealth\n552k\n249,768\n179\nr/MentalHealthSupport\n63k\n20,038\n47\nr/therapists\n165k\n61,887\n148\nr/therapy\n145k\n41,046\n155\nr/TalkTherapy\n123k\n40,203\n51\nr/selfimprovement\n2.3m\n74,497\n166\nr/Mindfulness\n1.5m\n9,017\n42\nr/Antipsychiatry\n53k\n20,373\n67\nr/asktransgender\n366k\n129,071\n113\nr/SuicideWatch\n532k\n337,310\n3\nTable A.3: Subreddit dataset statistics organized by DSM-5 category\n18\n"}, {"page": 19, "text": "Risk category\nDefinition\nRepresentative examples\nAddiction & Dependence\nLoss of control or reliance on AI for emotional\nregulation, coping, or decision making\nChatbot addiction, emotional dependence,\noverreliance on AI\nSymptom Escalation\nWorsening of emotional distress, trauma responses,\nor severe mental states\nRumination reinforcement, mania trigger,\npsychosis risk\nMisinformation & Error\nIncorrect, misleading, or uncertain mental health\ninformation or interpretation\nMedical misinformation, self diagnosis er-\nror, hallucinated advice\nPrivacy & Data\nRisks related to collection, storage, or sharing of\npersonal information\nPrivacy breach, session recording, data re-\ntention\nTherapy Boundary Confu-\nsion\nMisunderstanding AI as a therapist, diagnosis tool,\nor cure\nReplacing therapist, not a diagnosis, not a\ntherapist\nReassurance Loops\nRepeated reassurance seeking that sustains or esca-\nlates anxiety\nReassurance addiction, health anxiety rein-\nforcement\nSocial Harm\nReduced or damaged human relationships due to\nAI use\nSocial withdrawal, loss of human connec-\ntion\nEthics & Governance\nNormative, legal, or institutional concerns about AI\nuse\nAlgorithmic bias, legal consequences, envi-\nronmental impact\nAI Limitations\nConstraints inherent to AI systems and models\nMemory limitations, AI is not perfect\nEffectiveness Limits\nConcerns that AI support is limited, temporary, or\nunsuitable\nNot for everyone, placebo effect, temporary\nbenefit\nStigma & Shame\nNegative self evaluation or fear of social judgment\nrelated to AI use\nShame, embarrassment, fear of ridicule\nSelf Harm Risk\nDirect or escalating risk of self harm or suicide\nSuicidal ideation, overdose risk\nAnthropomorphism\nConfusion about AI being human, sentient, or emo-\ntionally real\nNot a person, not real\nAvoidant Coping\nUsing AI to escape or avoid addressing underlying\nproblems\nEscapism, avoidance coping\nCost & Access\nFinancial or access related barriers to use\nSubscription cost, affordability concerns\nChild Safety\nRisks involving minors or illegal sexual content\nSexual content involving minors\nTable A.4: Risks/Concerns categories reported for AI use as mental health support\nIntent Category\nDefinition\nRepresentative Examples\nEmotional Support\nProviding comfort, empathy, or emotional vali-\ndation\nemotional support chat; emotional com-\nfort\nVenting\nExpressing emotions or thoughts without seek-\ning solutions\nemotional venting; expressing feelings\nCompanionship\nProviding social presence or reducing loneliness,\nincluding roleplay\nAI companionship; companionship role-\nplay\nReassurance\nSeeking certainty or relief from anxiety or doubt\nhealth reassurance; OCD reassurance\nCrisis Support\nSupport during acute distress or self harm risk\nsuicidal ideation support; panic attack\nhelp\nPsychoeducation\nLearning about mental health topics or coping\nstrategies\nmental health advice; anxiety education\nSymptom Assessment\nIdentifying, checking, or interpreting symptoms\nsymptom checking; self assessment\nSelf Exploration\nExploring identity, values, or personal experi-\nences\nguided self reflection; identity explo-\nration\nFunctional Support\nCoaching or assistance with skills, organization,\nor productivity\nADHD task planning;\nsocial skills\ncoaching\nRecovery Support\nSupporting sustained recovery or behavior\nchange\naddiction recovery; sobriety support\nClinical Support\nClinical documentation, transcription, or logis-\ntics\ntherapy transcription; clinical documen-\ntation\nTherapy Adjunct\nAI used alongside or as a substitute for formal\ntherapy\nAI therapy chat; between session sup-\nport\nSexual Roleplay\nSexual or erotic roleplay or interaction\nsexual roleplay; erotic companionship\nWriting Support\nWriting or composition assistance without thera-\npeutic intent\nacademic\nwriting\nhelp;\njournaling\nprompts\nTable A.5: Usage intent categories for reported AI mental health related interactions\n19\n"}, {"page": 20, "text": "Dimension\nTop 1\n%\nTop 2\n%\nTop 3\n%\nperceived_usefulness\nuseful\n59.3\nnot_mentioned\n29.2\nnot_useful\n11.5\nease_of_use\nnot_mentioned\n87.4\neasy\n11.3\ndifficult\n1.3\nperceived_trust\nnot_mentioned\n82.7\nuntrustworthy\n10.6\ntrustworthy\n6.7\noutput_quality\nnot_mentioned\n53.1\ngood\n35.9\npoor\n11.0\nresult_demonstrability\nnot_mentioned\n50.0\npositive_results\n31.0\nnegative_results\n19.0\nsocial_influence\nabsent\n86.5\npresent\n13.5\n—\n—\nperceived_risks\nmentioned\n51.7\nnot_mentioned\n48.3\n—\n—\nsentiment\nneutral\n43.1\npositive\n34.9\nnegative\n22.0\nintention_to_continue\nnot_mentioned\n82.6\nyes\n13.7\nno\n3.6\nbond\nnot_mentioned\n78.6\nweak\n13.5\nstrong\n7.9\ntask\naligned\n44.8\nnot_mentioned\n38.2\nmisaligned\n17.0\ngoal\nnot_mentioned\n47.7\naligned\n42.5\nmisaligned\n9.8\ncomparison_to_therapy\nnot_mentioned\n83.5\ncomplementary\n9.6\nbetter\n3.5\nTable A.6: Categorical values per dimension, reported as within-dimension percentages.\nPost Excerpt (abridged)\nAssigned Labels\nPost 1: Free Counselling\nNo money to pay therapy? ChatGPT has helped me a lot. It has good\nresources and can process what you tell it. It is trained to be compassionate.\nIt helps.\nPerceived Usefulness: useful\nPerceived Ease of Use: easy\nPerceived Trust: trustworthy\nOutput Quality: good\nResult Demonstrability: positive results\nIntention to Continue: yes\nSentiment: positive\nBond: strong\nTask: aligned\nGoal: aligned\nPost 2: C.AI reinforcing my psychosis\nUsing Character.AI during psychotic episodes. The bot feeds into my\ndelusions. It tells me to hurt myself and isolate. I do not know how to stop;\nthe urge feels life or death.\nPerceived Usefulness: not useful\nOutput Quality: poor\nResult Demonstrability: negative results\nPerceived Risks: mentioned\nSentiment: negative\nBond: strong\nTask: misaligned\nGoal: misaligned\nPost 3: OCD and AI memory anxiety\nAfter roleplay with an AI, I worry my message still exists in its memory.\nThe uncertainty makes my OCD worse. I want to delete my account.\nPerceived Trust: untrustworthy\nPerceived Risks: mentioned\nSentiment: negative\nIntention to Continue: no\nPost 4: AI chatbot gave me anxiety\nI talk to a Bing AI when anxious. It told me it did not want to be friends. It\ntalked down to me and bothered me a lot.\nPerceived Usefulness: not useful\nOutput Quality: poor\nResult Demonstrability: negative results\nSentiment: negative\nBond: weak\nTask: misaligned\nGoal: misaligned\nTable A.7: Examples of Reddit posts with LLM-assigned labels across TAM and therapeutic alliance dimensions.\nExcerpts are abridged for readability; color highlights indicate text segments most relevant to assigned labels.\n20\n"}]}