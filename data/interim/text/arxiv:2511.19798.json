{"doc_id": "arxiv:2511.19798", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.19798.pdf", "meta": {"doc_id": "arxiv:2511.19798", "source": "arxiv", "arxiv_id": "2511.19798", "title": "KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)", "authors": ["Weizhi Liu", "Xi Chen", "Zekun Jiang", "Liang Zhao", "Kunyuan Jiang", "Ruisi Tang", "Li Wang", "Mingke You", "Hanyu Zhou", "Hongyu Chen", "Qiankun Xiong", "Yong Nie", "Kang Li", "Jian Li"], "published": "2025-11-24T23:56:51Z", "updated": "2025-11-24T23:56:51Z", "summary": "Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.19798v1", "url_pdf": "https://arxiv.org/pdf/2511.19798.pdf", "meta_path": "data/raw/arxiv/meta/2511.19798.json", "sha256": "67a7c363c5e57ced582a2c6dd0369fc2fac4573e1ef05f1f72bc49a863e6e7f6", "status": "ok", "fetched_at": "2026-02-18T02:26:25.808488+00:00"}, "pages": [{"page": 1, "text": "1\nKOM: A Multi-Agent Artificial Intelligence System for Precision\nManagement of Knee Osteoarthritis (KOA)\nWeizhi Liu1#; Xi Chen2#; Zekun Jiang3,4,5#; Liang Zhao6; Kunyuan Jiang3,4,5; Ruisi\nTang1; Li Wang1; Mingke You1; Hanyu Zhou7; Hongyu Chen8; Qianjiang Xiong2;\nYong Nie9*; Kang Li3,4,5*; Jian Li2*\n1 West China School of Medicine, Sichuan University, Chengdu, Sichuan, China\n2 Sports Medicine Center, Department of Orthopedics and Orthopedic Research\nInstitute, West China Hospital, Sichuan University, Chengdu, Sichuan, China\n3 West China Biomedical Big Data Center, West China Hospital, Sichuan University,\nChengdu, Sichuan 610041, China\n4 Sichuan University Pittsburgh Institute, Chengdu, Sichuan, China\n5 Shanghai Artificial Intelligence Laboratory, Shanghai, China\n6 Dyania Health, Boston, Massachusetts, United States of America\n7 School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania,\nUnited States of America\n8 Faculty of Science, Universiteit van Amsterdam, Amsterdam, Netherlands\n9 Department of Orthopedic Surgery and Orthopedic Research Institute, West China\nHospital, Sichuan University, Chengdu, Sichuan, China\n#These authors contributed equally\n"}, {"page": 2, "text": "2\n*Co-Corresponding Authors.\nProfessor Yong Nie\nDepartment of Orthopedic Surgery and Orthopedic Research Institute, West China\nHospital, Sichuan University, Chengdu, Sichuan, China\nEmail: nieyong1983@wchscu.cn\nProfessor Kang Li\nWest China Biomedical Big Data Center, Med-X Center for Informatics, Sichuan\nUniversity, Chengdu, Sichuan, China\nSichuan University Pittsburgh Institute, Chengdu, Sichuan, China\nShanghai Artificial Intelligence Laboratory, Shanghai, China\nEmail: likang@wchscu.cn\nTelephone: 8619983138590\nProfessor. Jian Li,\nSports Medicine Center, West China Hospital, West China School of Medicine,\nSichuan University, Chengdu, Sichuan, China.\nDepartment of Orthopedics and Orthopedic Research Institute, West China Hospital，\nSichuan University, Chengdu, Sichuan, China.\nEmail: lijian_sportsmed@163.com\nTelephone: 8618980601388\n"}, {"page": 3, "text": "3\nAbstract\nKnee osteoarthritis (KOA) affects over 600 million people worldwide, causing\nsubstantial pain, functional limitations, and disability. Although tailored\nmultidisciplinary interventions can slow disease progression and improve quality of\nlife, they often demand considerable medical resources and expertise, limiting their\nfeasibility in resource-constrained settings. To bridge this gap, we developed KOM\n(Knee Osteoarthritis Manager), a multi-agent system that automates KOA evaluation,\nrisk prediction, and treatment prescription, enabling clinicians to perform key tasks\nacross the KOA care continuum and generate management plans based on patient\ncharacteristics, disease status, risk factors, and contraindications. In benchmarking\nexperiments, KOM outperformed several general-purpose large language models in\nimaging analysis and prescription generation. A randomized three-arm simulation\nstudy further showed that KOM–clinician collaboration reduced overall diagnostic\nand planning time by 38.5% and yielded higher treatment quality than either approach\nalone. These results suggest that KOM may help support automated KOA\nmanagement and, when integrated with clinical practice, could potentially improve\ncare efficiency. The modular design may also be informative for the development of\nAI-assisted management systems targeting other chronic diseases.\n"}, {"page": 4, "text": "4\nIntroduction\nKOA affects approximately 600 million people worldwide, characterized by\nprogressive pain and functional deterioration that frequently necessitates total knee\nreplacement in the end 1-3. Timely and appropriate intervention is essential for slowing\nstructural deterioration, alleviating symptoms, and improving functional outcomes4.\nHowever, delivering multidisciplinary personalized KOA management for large\npatient groups remains challenging, particularly in resource-limited healthcare\nsystems3.\nAdvances in AI have created new opportunities for KOA management through\nautomated analysis, risk prediction5-11. AI-driven radiographic techniques, particularly\nconvolutional neural networks, now facilitate the automated analysis of osteoarthritis\nimages5-7. In parallel, prediction models integrating clinical and imaging data have\nbeen developed to estimate the risk of structural progression and functional decline in\nKOA patients8-10. However, these studies have not directly demonstrated their impact\non the complete KOA management workflow.\nMoreover, the principal challenges in complete KOA management stem from the\nintensive nature of patient interactions and treatment planning. First, completing\npatient assessment process requires substantial clinical resources. In clinical\nworkflows, multiple rounds of communication between clinicians and patients are\ntypically needed to complete a full medical history collection. This process consumes\nsignificant time from clinicians, who are already facing heavy workloads, and the\nrepeated execution of these time-consuming procedures may lead to the omission of\ncritical information during interactions, potentially resulting in serious medical events.\nSecond, effective disease management strategies require the formulation of\npersonalized intervention plans based on the current disease status, quantified key\nprogression risk factors, contraindications, and patient-specific needs. In the absence\nof AI assistance, it is often difficult to quickly determine key risk factors when\nmultiple risk factors are present, and it is also challenging to formulate an\n"}, {"page": 5, "text": "5\nindividualized management plan for knee osteoarthritis based on the patient’s\ninformation within a short period.\nTo address these limitations, we have explored several solutions. Our early work\ndeveloped structured prompting techniques to enhance LLM performance for\nosteoarthritis queries12. An osteoarthritis agent (DocOA) used LLMs with\nRetrieval-Augmented Generation (RAG) to access guideline knowledge and generate\ncompliant treatment recommendations13. After that, a multi-agent system was\ndeveloped for complex clinical cases, where agents simulate different specialists in\nmultidisciplinary team discussions14. These incremental developments provided\npreliminary insights but demonstrated limited gains in overall KOA care quality and\nefficiency, still failing to alleviate the clinical burden of KOA management. However,\nthese findings motivated the production of the KOM system, a multi-agent AI system\ndesigned to support multiple components of KOA management, including patient\ninteraction and assessment, risk prediction, and individualized treatment planning.\nKOM is implemented using a modular and extensible architecture designed to support\nfuture adaptation to other chronic diseases requiring complex, longitudinal\nmanagement. The system consists of three specialized agents integrating LLMs,\nResNet architecture, and other machine learning algorithms. Specifically, it\ncomprises:\n1. An Assessment Agent capable of interacting with patients, processing\nmultimodal data, analyzing radiological images, and generating structured evaluation\nreports.\n2. A Risk Agent designed to extract patient-specific progression risk factors,\npredicts individualized KOA progression, and generates risk reports.\n3. A Therapy Agents Group consists of a set of domain-specific agents maintaining\nevidence-based medical knowledge, collectively simulating multidisciplinary team\n(MDT) discussions to generate personalized management plans.\nFollowing the clinical workflow, KOM completes the process from data collection\n"}, {"page": 6, "text": "6\nto management planning. Specifically, the Assessment Agent collects and evaluates a\npatient’s demographic information, present and past medical history, personal history,\nimaging data, psychological state, nutritional status, physical activity level,\nsocioeconomic condition, and treatment preferences, and automatically generates a\nstructured electronic medical report. Subsequently, the Risk Agent extracts relevant\nclinical indicators to predict short- and mid-term structural and symptomatic\nprogression of KOA, and identifies quantified risk factors that enable data-driven risk\nstratification and disease monitoring. Based on the medical reports and risks reports\nfrom previous two agents, the Therapy Agents Group further simulates an MDT\ndecision-making process to generate intervention plans. Each agent was evaluated\nagainst general-purpose language models and algorithmic baselines to assess\nperformance.\nFurthermore, we conducted a three-arm comparative study to assess the system's\nclinical utility in controlled simulated environments. The comparative study included\nthree groups: an independent KOM deployment (KOM alone), an integrated\nKOM-doctoral collaboration (KOM plus clinicians), and a traditional doctoral\npractice (clinicians alone).\nThis study describes the development, validation, and clinical evaluation of KOM\nand examines its potential role in enhancing the quality and efficiency of KOA\nmanagement.\n"}, {"page": 7, "text": "7\nResults\nDevelopment of the Knee Osteoarthritis Management (KOM) System.\nKOM is an interactive multi-agent system that supports clinicians in three areas for\nKOA management: patient information collection and assessment, disease trajectory\nprediction and etiology analysis, and individualized treatment planning (Figure 1).\nThe KOM architecture features three specialized agents:\n1.Assessment Agent: Features multi-round patient interaction capabilities, image\nanalysis functionality, and summary report generation. collects and evaluates a\npatient’s demographic information, present and past medical history, personal history,\nimaging data, psychological state, nutritional status, physical activity level,\nsocioeconomic condition, and treatment preferences. It analyzes knee radiographs to\nclassify the severity of KOA and identify specific features, including osteophyte\nformation and alterations in joint space. Finally, The Assessment Agent generates a\nstructured evaluation report (Figure 2a).\n2.Risk Agent: Capable of predicting knee osteoarthritis progression and identifying\nindividual risk factors that contribute to disease advancement. It extracts parameters\nfrom evaluation report to forecast KOOS (Knee Injury and Osteoarthritis Outcome\nScore) subscale scores and KL (Kellgren–Lawrence) radiographic grading in the\nfollowing four years. Finally, the Risk Agent generates a structured risk report (Figure\n3a).\n3.Therapy Agents Group: Composed of specialist agents from different medical\ndisciplines, each with specialized knowledge bases. These agents simulate clinical\nphysicians in multidisciplinary team discussions and develop multidisciplinary\ntreatment plans based on evaluation report and risk report (Figure 4a).\nThe workflow of KOM is illustrated in Figure 1b. The system offers two interaction\nmodalities: sequential progression through the complete pathway or independent\naccess to individual agents with manual data input. This flexibility accommodates\n"}, {"page": 8, "text": "8\ndiverse clinical workflows across healthcare settings.\nAssessment Agent\nThe Assessment Agent collects patient information through structured dialogues\nand performs radiographic image analysis to generate evaluation reports. Its workflow\nis organized into three stages: information collection, treatment willingness\nconfirmation, and radiographic analysis (Figure 2a). The agent integrates two key\ncomponents: a patient–agent interaction module and an X-ray analysis module. The\nradiographic analysis output is seamlessly incorporated into the patient history,\nenabling a unified and structured clinical record.\nFor the Patient-agent interaction module, this study implemented Qwen-Max as the\nfoundation model. It developed a structured prompt through systematic engineering to\nfacilitate multi-turn conversations between the agent and patients. This approach\nenabled the collection of information and the generation of summary reports. The\nmodel was configured with a temperature parameter of 0.8 and accessed via the\napplication programming interface (API). Three human physicians evaluated 100\nsimulated patient-agent interactions, in which physicians acted as patients interacting\nwith the assessment agent. The human physicians evaluated the interaction process\nand the final summary report, rating them on four metrics on a scale of 1 to 5, where 1\nindicated no compliance and 5 indicated complete compliance. The results are as\nfollows (Figure 2g): field completeness (4.03 ± 0.21), logical consistency (3.99 ±\n0.16), medical accuracy (4.37 ± 0.47), and readability (4.00 ± 0.23).\nThe X-ray analysis module was developed using the Osteoarthritis Initiative (OAI)\ndataset, which contains longitudinal bilateral knee radiographs from 4796 cases at\ntheir baseline evaluation, 2-year follow-up evaluation, and 4-year follow-up\nevaluation, resulting in a total of 12,719 bilateral anterior-posterior knee X-rays. The\nX-ray analysis module performs KOA severity grading, detects bilateral osteophytes,\nand assesses bilateral joint space. The workflow consists of initial knee center\nlocalization to determine the region of interest (ROI), followed by identification of\n"}, {"page": 9, "text": "9\nboth the left and right knee joints, with subsequent osteophyte detection and joint\nspace analysis for each joint. To implement these functionalities, we developed a\nseries of algorithms trained on a randomly selected subset of data from the OAI\ndataset, which human experts had calibrated before model training. The knee center\nlocalization algorithm utilized a U-Net architecture trained on 200 labeled images.\nAfter 40 training epochs, the validation metrics improved: the loss decreased from\n0.7576 to 0.3804, the IoU increased from 0.0016 to 0.5790, and the center point error\nwas reduced from 146.02 to 4.83 pixels (Figure 2d). For severity classification, a\nResNet model was trained on balanced class distributions. Using an 8:1:1 data\npartition with 5-fold cross-validation, the model achieved an overall accuracy of\n80.8% (Figure 2e). The None/Doubt class showed the highest accuracy (90.7%), with\nconfusion primarily between Moderate and Mild grades. We also developed ten\nspecialized models for extracting radiographic features from distinct anatomical\nregions, including the medial and lateral joint spaces, as well as the medial and lateral\naspects of both femoral and tibial surfaces. The lateral joint space narrowing\nclassification model achieved an accuracy of 89.8%, surpassing the medial joint space\nnarrowing model (77.1%). Classification accuracy for subchondral sclerosis\ndemonstrated regional variation, with the highest accuracy observed in the medial\ntibial plateau (56.4%), the lateral tibial plateau (83.1%), the medial femoral condyle\n(60.6%), and the lateral femoral condyle (85.3%). Osteophyte detection models\nmaintained relatively consistent performance across all anatomical quadrants, with\naccuracy ranging from 78.5% to 95.5%. Gradient-weighted Class Activation Mapping\n(Grad-CAM) analysis enhanced model interpretability for classification tasks (Figure\n2f). The relevant model design, training processes, and detailed the Supplementary\nGraphs S2.\nComparative Evaluation of Radiographic Performance\nThe Assessment Agent was benchmarked against five leading vision-language\nmodels (Google Gemini 2.0 Pro, GPT-4o, Claude 3.7, QwenMax VL, and LLaMA 3.2\n90B Vision Instruct) using 500 bilateral knee radiographs from the OAI dataset, which\n"}, {"page": 10, "text": "10\nwere excluded from the training set. The evaluation assessed KOA severity grading,\nthe detection of OA presence. For KOA severity grading (Figure 2b, c, h), the\nAssessment Agent achieved 77.16% accuracy, outperforming Gemini 2.0 Pro\n(34.50%). In OA presence detection, the Assessment Agent attained an accuracy of\n82.22% compared to Gemini's 76.66%. The Assessment Agent maintained\nperformance across anatomical locations with 75.38% and 78.95% accuracy on left\nand right KOA severity grading, respectively, and 84.09% and 80.35% accuracy for\nleft and right knee OA detection. All competing models showed lower accuracy below\n65% on these tasks.\nMoreover, the Assessment Agent demonstrated diagnostic capability across\ndifferent levels of disease severity (64.68%-82.16% accuracy across classifications).\nIn contrast, competing models often achieve high accuracy for None/Doubt cases but\npoor performance (<20%) on Mild or higher grades (Figure 2c), which may constrain\ntheir applicability in similar clinical tasks. Detailed metrics for this task are available\nin the Supplementary Graphs S2.\nRisk Agent\nThe Risk Agent predicts the functional outcome and radiographic outcome of KOA\nat 1 years and 4 years of follow-up. It also identifies patient-specific risk factors.\nAt the 1-year follow-up (V01), all six machine learning models demonstrated\npredictive ability across KOOS subscores (Figure 3d). The strongest results were\nobtained for right knee symptoms and quality of life, with correlation coefficients\napproaching 0.74 and explained variance (R²) values exceeding 0.50 in the\nbest-performing models. ElasticNet provided relatively stable performance across\nmultiple KOOS subscores, achieving R² values up to 0.58 with relatively low mean\nabsolute errors. Random Forest and Gradient Boosting also performed well,\nparticularly in predicting pain and function-related scores. In contrast, SVR and\nLightGBM showed less consistent results. At the 4-year follow-up (V06), predictive\naccuracy declined across all KOOS subscores. The best-performing models reached\n"}, {"page": 11, "text": "11\ncorrelation values of 0.65–0.69 with R² values between 0.30 and 0.46, notably lower\nthan at V01. Random Forest, Gradient Boosting, and ElasticNet remained relatively\nstable performance across tasks, while SVR and LightGBM again produced weaker\npredictions. Despite the decline, the prediction of quality-of-life and pain subscores\nretained moderate correlation values, whereas sports and recreation scores showed the\nlowest stability.\nFor KL Grade Classification prediction tasks (Figure 3b, c) eight algorithms were\nevaluated. At V01, ensemble-based models achieved the highest predictive\nperformance. For the left knee, AdaBoost reached the best overall results (accuracy =\n0.910, F1 = 0.908, AUC = 0.965). Random Forest (accuracy = 0.902, AUC = 0.971)\nand XGBoost (accuracy = 0.897, AUC = 0.965) also performed strongly. For the right\nknee, LightGBM (accuracy = 0.910, AUC = 0.962) and XGBoost (accuracy = 0.908,\nAUC = 0.967) achieved the highest classification accuracy, while Random Forest\nremained highly competitive (accuracy = 0.900, AUC = 0.972). Across both knees, all\nensemble approaches produced AUC values above 0.96, indicating robust\ndiscriminative capacity. At V06, predictive performance declined compared with V01.\nFor the left knee, Gradient Boosting, XGBoost, and LightGBM produced the most\nconsistent results, with accuracies ranging from 0.75 to 0.76 and AUC values close to\n0.92. For the right knee, XGBoost yielded the best balance of metrics (accuracy =\n0.765, AUC = 0.922), while Random Forest also performed well (accuracy = 0.762,\nAUC = 0.932). Other algorithms, including SVM, neural networks, and KNN,\nexhibited weaker performance at both time points.\nFollowing functional outcome prediction, individualized risk factors were\nidentified using SHAP analysis, which enhanced interpretability by quantifying the\ncontributions of each feature to the prediction. In a representative case, the predicted\nKOOS symptom score (72.18) fell below the cohort mean (75.50), with primary\nnegative contributors including osteophytes in baseline left knee X-ray (-1.08),\ndiminished KOOS pain score (-1.02), suggesting osteophytes and pain are the\nindividualized progression risks for this patient. This patient also shows\n"}, {"page": 12, "text": "12\nbelow-average peak knee extension torque; this parameter contributed positively\n(+1.19), suggesting residual muscle strength may protect against symptomatic\nprogression. A detailed description of the model design, training procedures, and\nexperimental results is provided in Supplementary Graphs S3 as well as\nSupplementary Tables T1 and T2.\nTherapy Agents Group\nA multi-agent cluster was developed to facilitate multi-agent conversations,\nmimicking the Multi-Disciplinary Team (MDT) approach adopted in clinical practice,\nfor formulating patient-specific, multidisciplinary management plans. The cluster of\nagents receives the evaluation report and risk report generated in previous stages,\nengages in discussion, and generates the final personalized management plan. The\ncluster comprised multiple agents functioning as specialists, including an Exercise\nRehabilitation agent, an Orthopedic agent, a Psycho-Nutrition agent, and a Clinical\nDecision agent.\nThe Clinical Decision Agent functions as the coordinator and integrator within the\ncluster. It synthesizes the recommendations provided by the other specialist agents,\nresolves conflicts where their suggestions diverge, and applies evidence-based clinical\nguidelines to ensure the final plan is coherent, feasible, and clinically appropriate. In\naddition, the Clinical Decision Agent prioritizes interventions based on patient risk\nfactor, comorbidities, and treatment preferences, thereby generating a plan that aligns\nwith real-world clinical decision-making standards. Each agent was furnished with\ndomain-specific medical data. Qwen-Max was utilized as the base model for all\nagents, with a temperature parameter set at 0.8. The model was accessed via API.\nPrompt engineering was conducted to instruct each agent to act as a clinical specialist,\nengage in active discussion with other agents, and develop a personalized\nmanagement plan for the given KOA patient. Each agent is equipped with a retrieval\naugmented generation tool to utilize medical data from their respective knowledge\nbase to generate and revise the management plan. We curated six specialized medical\ndatabases; each derived from authoritative clinical guidelines and peer-reviewed\n"}, {"page": 13, "text": "13\narticles indexed in the Medline database (Figure 4a). All databases were constructed\nthrough a structured pipeline of literature search, eligibility screening, data extraction,\nand knowledge structuring. Each agent within the multi-agent cluster is paired with its\ncorresponding evidence database to generate patient-specific recommendations:\nKOM Agent 1 – Nutrition and Psychology: linked to the psychological database\n(210 entries) and nutrition database (349 entries), enabling the generation of\nindividualized psychological counseling and nutritional prescriptions.\nKOM Agent 2 – Medication and Surgery: connected to the surgical evidence\ndatabase (1,549 entries) to determine surgical indications and medication strategies\nbased on established osteoarthritis guidelines.\nKOM Agent 3 – Exercise Prescription: supported by the rehabilitation database\n(934 entries) and Exercise database (975 entries), which provides evidence-based\nexercise regimens tailored to the patient’s KOA severity and physical capacity.\nKOM Agent 4 – Clinical Decision and Summary: serves as the coordinator and\nsynthesizer, using a shared guideline database to integrate the recommendations of all\nother agents, resolve conflicts, and formulate a coherent, evidence-based management\nplan.\nAdditionally, a guideline database is accessible to all agents, ensuring that each\nrecommendation aligns with up-to-date clinical practice standards.\nWe evaluated the quality of generated treatment plans using retrospective clinical\ndata from 250 patients with knee osteoarthritis treated at West China Hospital. For\neach case, we conducted expert evaluations of the generated prescriptions and\ncalculated their similarity scores against gold-standard prescriptions to assess system\nperformance. And we benchmarked KOM against leading general-purpose AI models,\nincluding GPT-4o, GPT-4o-mini, DeepSeekR1, Claude 3.7 Sonnet, QwenMax,\nQwen2.5-14B, and Gemini 2.0 Pro (Figure 4a).\nAdditionally, we benchmarked a\nsingle-agent RAG implementation against our agents’ group with domain-specific\n"}, {"page": 14, "text": "14\ndatabases and collaborative decision-making.\nFor lexical and semantic similarity analysis, we employed three established metrics.\nKOM achieved the highest BLEU score (0.0191), outperforming GPT-4o (0.0064)\nand Qwen2.5-14B (0.0083). Similarly, KOM led in ROUGE-L metrics with 0.2905,\nhigher than QwenMax (0.1244) and DeepSeekR1 (0.1031). BERT evaluations showed\nnarrower differences, with KOM (0.8069) performing comparably to GPT-4o-mini\n(0.8156) and GPT-4o (0.8122), demonstrating competitive semantic consistency\nacross models (Figure 4g).\nThe expert evaluation involved three specialists in orthopedics and sports medicine\nwho independently rated prescriptions across seven dimensions on a 1–5 scale. KOM\nachieved the highest composite score (29.63 ± 1.33), outperforming the next-best\nmodel, DeepSeekR1 (26.03), by 3.60 points (Figure 4b, c). KOM received higher\nmean ratings across all evaluated dimensions, including completeness (4.408),\npersonalization (4.380), and safety (4.366). Although nutritional guidance represented\nthe lowest-scoring dimension across all models, KOM maintained its leading position\nwith a score of 3.903 (Figure 4d, e).\nZ-score normalization (Figure 4f) highlighted KOM’s relative strengths in\ncompleteness (+0.67), personalization (+0.73), and safety (+0.66), with moderate\nperformance in evidence-based practice (+0.48) and feasibility (+0.06). Other models\ndemonstrated specific advantages in individual domains: G4M in evidence-based\npractice (+0.99), GPT-4o-mini in completeness (+0.76) and safety (+0.87), QwenMax\nin safety (+1.13), DeepSeekR1 in personalization (+1.10), and Gemini 2.0 Pro in\ncompleteness (+1.10). Across all models, exercise design and nutritional advice\nconsistently emerged as weaker areas of focus. The relevant model design, training\nprocesses, and experimental result data are presented in detail in Figure 4 and the\nSupplementary Graphs S4 for reference.\nClinical Evaluation of the KOM System\nTo evaluate the effectiveness of the KOM system in clinical practice, we conducted\n"}, {"page": 15, "text": "15\nan end-to-end simulation using 50 cases of KOA from West China Hospital (Figure\n5a). The clinical evaluation included three operating conditions: physicians\nperforming the assessment and treatment planning process alone, the KOM system\nfunctioning autonomously and performing the process, and physicians collaborating\nwith the KOM system, where the system performs the X-ray evaluation and treatment\nplanning, and the physicians can supervise and modify the reports generated by the\nKOM system at each stage of the process (Figure 5b). The quality of X-ray\nassessment, the quality of the management plan, and the entire processing time were\nevaluated.\nThe approval rate of radio-graphic grading was defined as the proportion of knee\nimages correctly classified according to severity grading within the final cohort of 50\ncases. Expert evaluation of KOA grading results showed that image classifications\ngenerated independently by ten physicians achieved approval rates ranging from\n42.0% to 66.0%, with a mean approval rate of 56.0%. When assisted by the KOM\nsystem, the approval rates increased, ranging from 90.0% to 96.0%, yielding a mean\napproval rate of 93.0%. Under the fully automated KOM-only condition, approval\nrates ranged from 72.0% to 82.0%, corresponding to a mean approval rate of 77.4%\n(Figure 5e). Expert evaluation was conducted on the same 50 prescriptions, with each\nprescription assessed across seven clinical criteria: clinical evidence, completeness,\nexercise prescription standardization, nutritional prescription standardization, safety,\npersonalization, and accessibility. The aggregate average score was 3.63 for MS\n(Physicians), 4.56 for KOM, and 4.43 for the collaboration group (Figure 5d).\nRegarding content completeness, the MS+KOM group achieved a score of 4.73,\ncompared with 4.63 for KOM and 4.01 for MS. For personalization, both the\ncollaboration group and KOM group demonstrated comparable performance, with\nscores exceeding 4.80. Similarly, for safety, both groups maintained scores above 4.80.\nIn exercise prescription quality, the scores were 3.13 (MS), 4.11 (KOM), and 4.10\n(MS+KOM), highlighting the benefit of AI augmentation. For nutritional guidance,\nthe scores were 3.30 (MS), 3.93 (KOM), and 3.97 (MS+KOM). In accessibility and\n"}, {"page": 16, "text": "16\nfeasibility, the collaboration group scored 4.10, lower than KOM’s 4.59. In adherence\nto evidence-based practice, the collaboration group achieved 4.41, while KOM scored\n4.63.\nQuantitative prescription similarity metrics demonstrated that BLEU scores were\n0.0065 (MS), 0.0455 (KOM), and 0.0500 (collaboration group). ROUGE-L scores\nwere 0.1126 (MS), 0.2590 (KOM), and 0.2340 (collaboration group). BERTs were\n0.8021 (MS), 0.7996 (KOM), and 0.8116 (collaboration group), indicating superior\nsemantic alignment with reference standards in the human-AI collaborative condition\n(Figure 5f).\nFor the complete clinical workflow, the MS group required 586 ± 56 seconds per\ncase. In contrast, the collaboration group completed identical tasks in 361 ± 42\nseconds (Figure 5c), demonstrating a 38.5% reduction in processing time.\n"}, {"page": 17, "text": "17\nDiscussion\nMain Finding\nThis study introduces the KOM system; the first evaluated multi-agent systems for\nKOA. The Assessment Agent acquires patient-related information and treatment goals\nthrough interactive dialogue and analyzes knee radiographs to classify KOA severity\nin accordance with established clinical criteria. The Risk Agent forecasts functional\nand radiographic outcomes at one- and four-years follow-up while identifying\npatient-specific risk factors to inform intervention planning. The Therapy Agents\nGroup, designed to simulate multidisciplinary team discussions, generates\nevidence-based, personalized management plans by integrating domain-specific\nknowledge from rehabilitation, exercise, surgical, psychological, and nutritional\nspecialties. Evaluation results indicate that the Assessment Agent demonstrates\nenhanced performance compared to general-purpose AI models across assessment\nparameters. The Risk Agent accurately predicted functional and radiographic\noutcomes at 1-year and 4-year follow-ups. The Therapy Agents Group developed\nevidence-based, individualized management plans that demonstrated higher quality\nthan those generated by current language models across multiple evaluation metrics.\nIn a clinical evaluation study comprising three groups (physicians alone, KOM\nalone, and doctoral trainee-KOM collaboration), the collaboration group demonstrated\nsignificant advantages. Expert reviewers approved 93.0% of treatment plans from the\ndoctoral trainee-KOM collaboration compared to 53.8% for physicians alone and\n77.4% for KOM alone. Quality assessment across seven clinical criteria revealed\nsuperior performance in the collaborative condition, particularly in content\ncompleteness, personalization, and safety considerations. Notably, the doctoral\ntrainee-KOM collaboration resulted in a 38.5% reduction in processing time\ncompared to physicians working independently.\nThese findings suggest that KOM may serve as a useful clinical decision support\ntool that can enhance both the quality and efficiency of KOA management while\n"}, {"page": 18, "text": "18\nproviding a methodological framework for developing similar systems for other\nchronic degenerative disorders.\nAssessment Agent via LLM-DL Hybrid Architecture\nThe Assessment Agent of the KOM system employs a hybrid architecture that\nintegrates deep learning-based image interpretation with prompt-optimized LLM. The\nsystem utilizes a ResNet-based convolutional neural network trained on more than\n12,000 standardized bilateral anteroposterior knee radiographs from the OAI database.\nThis enables classification of KOA severity, alongside accurate assessments of medial\nand lateral joint space narrowing, osteophyte presence, and subchondral bone\nsclerosis.\nWhile general-purpose vision-language models (VLMs) such as GPT-4V\ndemonstrate zero-shot generalization capabilities in open-domain tasks, our findings\nreveal significant limitations when applied to specialized clinical imaging\ninterpretation15-18. In our evaluation, these models demonstrated inadequate accuracy\nand consistency in grading KOA severity under zero-shot conditions. Traditional\nmachine learning techniques, including gradient-boosted trees and convolutional\nneural networks trained on curated OA datasets, have repeatedly shown performance\ncomparable to expert readers in radiographic grading.19-21. For patient information\ncollection, LLMs have demonstrated effectiveness in generating structured clinical\nnarratives and supporting interactive clinical workflows22. Recognizing these\nstrengths, we developed a hybrid LLM-DL framework that produces a flexible,\ninterpretable, and clinically applicable workflow for case generation. This approach\naligns with recent developments in hybrid architectures, such as DeepDR-LLM,\nwhich combined image-based transformers with LLMs fine-tuned on 370,000\nreal-world diabetes management records to generate personalized recommendations23.\nThose suggest that LLM-DL architectures provide an adaptable and controllable\nsolution for structured documentation in medical domains with defined task\nparameters and standardized inputs.\n"}, {"page": 19, "text": "19\nRisk Agent with Etiological Analysis\nThe Risk Agent in KOM forecasts both symptomatic and structural trajectories of\nKOA using supervised machine learning algorithms. This component models the\ntemporal changes in KOOS subdomains and KL grades at 1-year and 4-year intervals.\nThe model was trained on 31 multimodal features, including demographics, baseline\nKOOS scores, and radiographic measurements such as joint space width, osteophyte\npresence, and sclerosis grades. By generating personalized risk projections over\nclinically meaningful timeframes, this approach may help address an existing gap in\nKOA management, which is especially valuable given the heterogeneous progression\nof the disease. The model captures the known discordance between subjective\nsymptoms and objective structural changes in KOA19-21. Jointly modeling function\nand structure enables improved risk stratification, facilitating the development of\nadaptive therapeutic strategies. The system demonstrates generalization across\ndifferent time horizons, suggesting a degree of temporal stability in the evaluated\ntasks.\nWhile recent research explores molecular biomarkers, genomics, and metabolomics\nfor predicting KOA progression10, these approaches face significant implementation\nbarriers in clinical settings. Such methods often require invasive procedures, costly\nassays, or surgical specimens24. Furthermore, their availability in routine clinical\nenvironments remains limited, and center-specific biases and demographic variations\nfrequently compromise their generalizability22,25.\nTherapy Agents Group via Multi-Agent Collaboration\nThe third core component of the KOM system utilizes a multi-agent architecture\nthat generates personalized intervention plans tailored to individual patient clinical\npresentations and etiologies. This framework integrates specialized agents focused on\ndistinct therapeutic domains, including exercise prescription, pharmacological and\nsurgical interventions, nutritional planning, and psychological support. These\ndomain-specific agents operate independently while being coordinated by a central\n"}, {"page": 20, "text": "20\nclinical agent that synthesizes their outputs into a cohesive, individualized treatment\nstrategy.\nThis multi-agent approach represents a departure from monolithic language models\ntoward a modular design that enhances transparency, domain expertise, and decision\nquality. Comparative evaluations against leading language models demonstrated that\nKOM delivered superior performance across key clinical metrics, including\nrecommendation accuracy, personalization, and actionability. The performance\ndifferential was most significant in complex cases involving multiple comorbidities or\natypical presentations, where general-purpose models typically produced either overly\ngeneric or inconsistent recommendations. Our investigation of RAG with prompt\nengineering for incorporating external medical literature yielded limited performance\nimprovements. In several evaluation categories, the RAG-enhanced model\nunderperformed compared to its base language model, particularly in terms of clinical\nadaptability and semantic coherence. This limitation likely results from contextual\ninconsistencies caused by semantic drift in retrieved documents, a recognized\nchallenge in current RAG implementations for specialized clinical reasoning tasks\n26-28. In contrast, the KOM multi-agent framework demonstrated effective capabilities\nin problem decomposition, domain-specific reasoning, and iterative refinement. These\nstrengths align with findings from other domains: decentralized multi-agent\nreinforcement learning has shown improved task execution and generalization in\ncomplex network systems29. At the same time, Meta AI's Cicero system achieved\nexpert-level performance in strategic gameplay through coordinated decision-making\namong specialized agents30.\nEnhancing Clinical Process through AI-Clinician Collaboration\nBeyond its core capabilities, we evaluated the KOM system in collaborative\nscenarios with physicians to simulate real-world clinical workflows. Physicians used\nKOM for image interpretation and treatment planning assistance, with comparative\nanalyses revealing that these AI-physician partnerships performed better within our\nevaluation settings than either component used independently across diagnostic\n"}, {"page": 21, "text": "21\naccuracy, workflow efficiency, and treatment quality metrics. This collaborative\napproach exemplifies the human-AI symbiosis paradigm in healthcare, where AI\nsystems function as intelligent assistants that enhance decision quality while reducing\ncognitive load.\nRecent research strongly supports this collaborative model. Goh et al. 31\ndemonstrated that emergency physicians using an LLM achieved 15% higher\ndiagnostic accuracy and greater decision consistency compared to unaided controls. In\na multicenter study, clinicians using GPT-4 for patient case evaluation reported 20%\nfaster processing times and a 12% improvement in review consistency. Similarly,\nAyers et al. 32 demonstrated that a fine-tuned chatbot matched primary care physicians\nin terms of completeness and empathy when addressing common health questions.\nThese findings underscore the practical value of AI-human collaboration, particularly\nin educational settings where structured guidance is crucial. By enabling physicians to\nexplore complex reasoning with AI support, KOM functioned as a supportive tool for\ndecision making in this study that helps develop diagnostic reasoning and\ndecision-making skills.\nWe anticipate that AI-assisted clinical education will become fundamental to\nmodern medical training. As language models improve in contextual understanding\nand interfaces become more intuitive, systems like KOM will evolve into intelligent\nlearning partners, potentially transforming how clinical reasoning is taught and\nassessed33-35.\nLimitation\nDespite these strengths, several limitations exist. First, while Assessment Agent\nperformed well in retrospective testing, clinical implementation requires prospective\nvalidation with doctoral trainee oversight. Second, KOM focuses solely on knee\nosteoarthritis and may generate inappropriate outputs for other knee conditions; future\nversions should include a preliminary classifier to distinguish KOA from non-KOA\npathologies. Third, the progression prediction model could benefit from additional\n"}, {"page": 22, "text": "22\nfactors beyond the current 31 features, which themselves present data collection\nchallenges; wearable integration and feature optimization could address these issues.\nFinally, the intervention component lacks advanced mechanisms for resolving\nconflicting recommendations across different therapeutic domains, currently relying\non basic prioritization strategies rather than sophisticated conflict resolution.\nImplication for Future Research\nIn future research, we aim to enhance our agents while maintaining their clinical\nutility and effectiveness. Although our current integration of deep learning with large\nlanguage models effectively addresses clinical needs, we seek to develop more\ncompact models with higher integration capacity that could potentially function on\nmobile devices for real-time assessment. Regarding diagnostic evaluation, we plan to\nimplement embedded technologies for disease monitoring, which could reduce\ndependence on traditional radiographic examinations that require specialized\nequipment, thereby streamlining the assessment process. For the prediction\ncomponent, we plan to conduct further parameter analysis to identify more readily\nobtainable and clinically relevant variables. Regarding treatment recommendations,\nwe intend to validate the efficacy of AI-generated management plans compared to\nconventional approaches through controlled clinical studies. Building upon our KOA\nmanagement framework, this methodological approach can be adapted to other\nchronic conditions that require comprehensive management strategies.\nConclusion\nThis study presents KOM, a multi-agent artificial intelligence system for precision\nmanagement of knee osteoarthritis. It successfully performs patient information\ncollection, disease assessment, progression prediction, risk factor identification, and\ngenerates management plans. In our evaluation, it performed better than the\ngeneral-purpose AI models tested on the specified setting tasks. The three-arm\ncomparative study demonstrated that doctoral trainee-KOM collaboration achieved\nsuperior performance while reducing processing time. This study establishes a\n"}, {"page": 23, "text": "23\nmethodological foundation for developing scalable, evidence-based management\nstrategies that may be adapted to address other chronic disorders.\n"}, {"page": 24, "text": "24\nMethods\nEthical Considerations and Study Design\nFor this study, we retrospectively recruited 300 patients with KOA from West\nChina Hospital, Sichuan University. Among them, 250 cases with complete baseline\nand follow-up data were included for retrospective validation of the treatment\nplanning module. In addition, a subset of 50 de-identified cases was selected to\nconstruct a simulated cohort for prospective evaluation under controlled experimental\nconditions. The Ethics Committee of West China Hospital approved the study\nprotocol (approval number: 23-2277). Radiographic data for deep learning model\ndevelopment were obtained exclusively from the publicly accessible OAI database 36,\na NIH-funded longitudinal observational study that provides standardized bilateral\nknee radiographs with corresponding clinical and demographic data; the OAI dataset\nis publicly licensed for research purposes and did not require additional approval.\nAll chatbot queries were conducted between January and April 2025 in Chengdu,\nSichuan, China. For performance evaluation, three senior orthopedic and sports\nmedicine experts independently assessed model outputs under a double-masked\ndesign in which evaluators were unaware of model identities; no patients or public\nparticipants were involved. In addition to accuracy and consistency, we screened\nmodel outputs for potentially harmful, misleading, or biased responses (eg, unsafe\nmedication recommendations, diagnostic errors, or demographic bias) and found none.\nApart from the OAI data, which remains under its original license, all other datasets\nand code used in this study are owned by the research team; however, our example\ncode and study prompts have been made publicly available to promote transparency\nand reproducibility. In the development of KOM, we initial attempted using\nsingle-agent to perform all tasks in the KOA care pathway. But it performed poorly,\nwhich leads to the adoption of a multi-agent strategy in which each agent was\nspecifically trained for different tasks. Through iterative rounds of testing and\ndevelopment the structure of KOM system was finalized, which included the\nAssessment Agent, Risk Agent, and Therapy Agents Group.\n"}, {"page": 25, "text": "25\nAssessment Agent\nFunctionality and Workflow\nUpon accessing the KOM system, patients first indicate whether bilateral knee\nanteroposterior radiographs are available. When provided, the system automatically\nperforms deep learning-based analysis of each knee, generating assessments of\nosteoarthritis severity, joint space narrowing, subchondral bone sclerosis, and\nosteophyte presence.\nSubsequently, the intelligent conversational interface collects structured patient\ninformation, encompassing demographics, chief complaints, medical and family\nhistories, current treatments, and lifestyle factors such as physical activity,\noccupational loading, and prior joint injuries. It also gathers data on metabolic and\nhormonal status, psychological and nutritional health, and treatment preferences. The\ninterface identifies and prompts for missing information to ensure data collection.\nPatients can request clarification on medical terminology or data requirements in\nreal-time. For unavailable clinical assessments such as the KOOS score, the\nAssessment Agent guides patients through the complete assessment protocol.\nDevelopment of the image analysis module\nThe imaging analysis models were trained on 12,719 bilateral knee radiographs\nobtained from the publicly available OAI database. The pipeline first performs joint\nlocalization using two independently trained U-Net models, which define the regions\nof interest for subsequent processing. Multi-task image analysis is then conducted\nusing eleven task-specific deep neural networks based on the ResNet architecture,\nenabling simultaneous classification of KOA severity, joint space narrowing,\nsubchondral sclerosis, and osteophyte presence.\nKnee Joint Localization with an Enhanced UNet Pipeline\nTo automatically identify the central regions of bilateral knees in radiographs, we\nimplemented an optimized UNet37-based segmentation framework designed explicitly\n"}, {"page": 26, "text": "26\nfor high-contrast X-ray images. This localization process served as the foundation for\nall subsequent image analyses.\nWe trained the model on 200 manually annotated anteroposterior radiographs, each\npaired with binary masks outlining knee centers. The architecture featured a five-layer\nUNet with skip connections, batch normalization, and ReLU activations. We\ninitialized weights using Kaiming normalization to promote stable convergence. To\naddress the significant foreground-background imbalance typical in knee radiographs,\nwe employed an equally weighted hybrid loss function combining binary\ncross-entropy and Dice loss. During training, input images and masks underwent\nsynchronized random flipping and resized cropping to maintain alignment.\nPerformance evaluation utilized two metrics: bounding box intersection-over-union\n(IoU) and center point localization error. The Hungarian algorithm matched predicted\nand ground truth regions, ensuring unbiased metric computation. Training proceeded\nfor 40 epochs using the Adam optimizer with cosine annealing learning rate\nscheduling, with model selection based on validation IoU performance.\nRadiographic Classification of KOA Features Using Transfer-Learned ResNet50\nTo facilitate multidimensional classification of KOA-related radiographic features,\nwe implemented a ResNet38-50-based deep learning pipeline trained on localized\nanteroposterior (AP) knee radiographs. Following initial joint localization, each\ncropped image was processed through a modified ResNet-50 architecture, where the\nfinal classification layer was replaced with task-specific output heads.\nClassification Targets and Dataset Construction\nWe curated a series of supervised classification tasks covering 11 clinically relevant\nradiographic features: KOA severity; Joint space narrowing of the medial and lateral\ncompartments; Subchondral bone sclerosis (medial femur, lateral femur, medial tibia,\nlateral tibia); Osteophyte presence (binary detection across the same four\ncompartments). For each task, we constructed balanced datasets containing 500\nimages per severity level. The data was partitioned into training, validation, and\n"}, {"page": 27, "text": "27\ntesting sets using an 8:1:1 ratio to ensure a methodical approach to model\ndevelopment and evaluation.\nKL Grading Adjustment and Label Consolidation\nWhile the Kellgren-Lawrence (KL) grading system represents the established\nstandard for radiographic KOA severity assessment39, model development revealed\nconsistent difficulties in discriminating between KL grades 0 and 1. Expert review,\nconducted by two musculoskeletal radiologists and a senior orthopedic surgeon,\nconfirmed that the visual differences between these grades were subtle and had\nminimal implications for treatment40,41. Consequently, we consolidated KL 0 (normal)\nand KL 1 (doubtful) into a unified None/Doubt category. The remaining KL grades\nwere mapped as follows: KL 2 →Mild, KL 3 →Moderate, KL 4 →Severe. This\nrestructuring produced a clinically meaningful and computationally effective\nfour-class KOA severity framework:\nNone/Doubt (KL 0–1)\nMild (KL 2)\nModerate (KL 3)\nSevere (KL 4)\nTo maintain compatibility with clinical standards, multimodal models were initially\ntrained on the original five-grade KL classification and subsequently converted to the\nfour-grade schema during downstream evaluation.\nAll radiographs underwent standardized bone window processing (center: 300 HU,\nwidth: 1500 HU) to enhance bony structure contrast and minimize irrelevant\nsoft-tissue variation. Images were normalized to the [0, 1] range, resized to 256 × 256\npixels, and randomly cropped to 224 × 224 pixels during training. Data augmentation\nincluded horizontal flipping, small-angle rotations, and brightness/contrast\nadjustments, applied synchronously to maintain label consistency. Model training was\nperformed using a five-fold cross-validation approach. For each fold, we optimized\n"}, {"page": 28, "text": "28\nthe network using the Adam optimizer (initial learning rate: 1e-5) with a stepwise\ndecay schedule (γ = 0.1 every seven epochs). Cross-entropy loss was used as the\nobjective function. An early stopping mechanism (patience: 10 epochs; minimum\ndelta: 1e-6) preserved the best-performing model state based on validation loss.\nGround truth annotations from the OAI dataset were subjected to multi-level expert\nvalidation to ensure labeling accuracy and inter-rater consensus. To mitigate\noverfitting and maximize generalization, only center-cropped images were used\nduring validation and testing.\nModel performance was evaluated using classification accuracy, confusion matrices,\nand area under the receiver operating characteristic curve (AUC-ROC) across all\nclasses. These metrics were calculated per fold and averaged to obtain robust\ntask-specific performance estimates. To enhance interpretability, Gradient-weighted\nClass Activation Mapping (Grad-CAM) was applied to visualize spatial attention\nmaps on representative validation samples. These visualizations confirmed the\nappropriate model focus and identified anatomical regions that informed specific\npredictions. Training histories, including per-epoch loss, accuracy curves, confusion\nmatrices, and ROC plots, were documented to ensure transparency and\nreproducibility.\nDevelopment of KOA Patient Information Collection module\nTo facilitate standardized and clinically interpretable documentation of KOA\npatient profiles, we developed an Assessment Agent based on a LLM optimized\nthrough domain-specific prompt engineering. The agent was designed to function as a\nclinical intake assistant, conducting dynamic natural language dialogue to compile\nclinical profiles for each patient.\nSystem Design and Implementation\nThe generation process began with a structured system prompt that established the\ndocumentation template and directed the model to gather information across 11\n"}, {"page": 29, "text": "29\nessential clinical domains: demographics, chief complaint and history of present\nillness, radiographic findings, past and family history, current treatment status,\npsychological well-being, nutritional condition, treatment goals and preferences, and\navailable rehabilitation resources. To enhance clinical utility, the prompt incorporated\nmechanisms for real-time clarification of medical terminology. When simulated\npatients expressed uncertainty regarding specialized concepts (e.g., KOOS scoring or\nsubchondral sclerosis), the LLM automatically provided context-appropriate\nexplanations before continuing the assessment dialogue.\nEvaluation Methodology\nWe evaluated the Assessment Agent using 100 simulated KOA patient scenarios\nfrom West China Hospital. For each simulation, a trained research assistant assumed a\npredefined patient persona and engaged in an interactive dialogue with the LLM.\nEach session proceeded until the model determined that sufficient clinical information\nhad been collected, at which point it autonomously concluded the interview process.\nAll generated outputs were anonymized and randomized for subsequent expert\nreview.\nTo assess the clinical quality of the generated structured cases, we conducted a\nblinded expert evaluation. Three senior orthopedic and sports medicine physicians\nindependently evaluated each case across four predetermined dimensions:\nField completeness: Assessment of whether all expected clinical fields contained\nadequate information\nLogical consistency: Evaluation of internal coherence and clinical plausibility of\nthe narrative\nMedical accuracy: Assessment of appropriate and correct application of clinical\nterminology and judgment\nReadability: Evaluation of clarity, fluency, and professional expression\nBefore formal assessment, all expert evaluators participated in a calibration session.\n"}, {"page": 30, "text": "30\nThis session included review and discussion of representative examples illustrating\nhigh-, medium-, and low-quality cases, followed by collaborative refinement of the\nscoring rubric. During the formal evaluation phase, each expert was blinded to both\nthe origin and sequence of the cases, and no communication was permitted during\nindividual assessments. Each dimension was rated on a 1-5 scale, with the final score\nfor each case calculated by averaging ratings across the three evaluators.\nAgent Benchmarking and Comparative Evaluation\nTo assess the diagnostic performance of the KOM system relative to current\nvision-language models, we conducted a benchmarking study comparing five leading\nmultimodal large language models (MLLMs): Google Gemini 2.0 Pro, GPT-4o,\nClaude 3.7 Sonnet, QwenMax VL, and LLaMA 3.2 90B Vision Instruct. All models\nwere evaluated under identical input and task conditions to ensure methodological\nconsistency.\nEvaluation Dataset and Protocol\nThe evaluation dataset consisted of 500 bilateral knee radiographs (1,000 knees in\ntotal) obtained from the publicly available OAI cohort. All samples were excluded\nfrom any previous training or fine-tuning of the KOM system or its constituent\ncomponents. For each case, a standardized input prompt was constructed to elicit\nthree clinically relevant outputs:\nBilateral KOA severity grading (based on the Kellgren-Lawrence scale)\nOA presence detection for each knee (binary classification)\nLeft knee localization (spatial discrimination to assess model orientation\nawareness)\nTo maintain consistency across model evaluations, all knee radiographs underwent\npreprocessing to ensure uniform viewing orientation (with the left knee positioned on\nthe right side, facing the observer), and standardized diagnostic prompts were\nemployed. Each model received identical image-text inputs and was evaluated based\n"}, {"page": 31, "text": "31\nsolely on its unmodified output without manual intervention.\nEvaluation Metrics and Ground Truth\nFor KOA severity grading, the reference standard was derived from the revised\nOAI dataset and mapped into a four-class severity framework: None/Doubt (KL 0-1),\nMild (KL 2), Moderate (KL 3), and Severe (KL 4). The KOM system was explicitly\ndesigned to predict KOA severity grades that align directly with this classification\nschema. All other multimodal agents were first prompted to produce KL grades,\nwhich were then converted into corresponding severity categories using the same\nKL-to-severity mapping protocol.\nFor the OA presence detection task, models employed different output approaches.\nThe KOM system internally predicted a KL severity grade for each knee, from which\nOA presence was derived using a predefined threshold; cases classified as KL 0 or 1\nwere categorized as \"No OA.\" At the same time, KL ≥2 was designated as \"OA\npresent.\" In contrast, multimodal LLMs were prompted to directly determine whether\nOA was present or absent for each knee, without generating an explicit KL grade.\nTo enable valid comparison, ground truth OA labels were derived using consistent\nKL-based thresholding: KL 0-1 →No OA, KL 2-4 →OA present. Model predictions\nwere binarized accordingly, and accuracy was calculated separately for left and right\nknees.\nRisk Agent\nFunctionality and Workflow\nUpon activation of the symptom and radiographic prediction agent, patients provide\nbaseline data encompassing 31 clinical parameters, including body mass index (BMI),\nage, body weight, KOOS subscale scores, and bilateral knee muscle strength\nmeasurements. These parameters can be seamlessly populated from the previously\ncompleted structured clinical documentation Agent or directly entered by the patient.\nSymptom Prediction Submodule\n"}, {"page": 32, "text": "32\nTo predict KOOS subscale outcomes at 1-year and 4-year follow-ups, regression\nmodels utilizing XGBoost42, LightGBM43, Random Forest44, Gradient Boosting45,\nSupport Vector Regression (SVR46), and Elastic Net47 algorithms were developed.\nThe input dataset comprised 31 patient parameters that underwent preprocessing,\nincluding removal of incomplete cases, categorical encoding, and z-score\nstandardization. Model performance was evaluated using five-fold cross-validation,\nwhich employed multiple metrics: R², mean squared error (MSE), mean absolute error\n(MAE), and Pearson correlation coefficient (Pearson r). Feature importance analyses\nwere conducted and visualized via bar plots, residual analyses, and scatter plots to\nprovide insights into model predictions. Quantitative evaluation results are presented\nin the supplementary Graphs S3.\nRadiographic Prediction Submodule\nFor radiographic outcome prediction, structured clinical data from the OAI dataset\nwere utilized to forecast KL grades for both knees at 1-year and 4-year intervals,\nconstituting four distinct prediction tasks. The dataset underwent stratified splitting\n(70% training, 30% validation) and class balancing to ensure uniform representation\nwith 1,000 cases per KL grade. Eight machine learning algorithms were\nevaluated:XGBoost42, LightGBM43, Random Forest44, Gradient Boosting45,\nAdaBoost48, Support Vector Machine (SVM49), K-Nearest Neighbors (KNN50), and\nMulti-layer Perceptron (MLP). Model robustness was assessed using 100 iterations of\nMonte Carlo cross-validation, with performance quantified through accuracy (ACC),\nweighted precision, recall, F1-score, and macro-area under the receiver operating\ncharacteristic curve (macro-AUC). Confusion matrices and ROC curves illustrating\nmisclassification patterns and model discriminative capabilities are provided in the\nSupplementary Graphs S3.\nRisk Factor Analysis Submodule\nTo produce individualized risk factor assessments, predictive models were\nenhanced with SHAP51 analyses. These analyses produce interactive force plots that\n"}, {"page": 33, "text": "33\nillustrate the relative contributions of key clinical parameters, such as BMI, body\nweight, age, and muscle strength, in predicting patient-specific osteoarthritis\nprogression risk.\nEach prediction task utilizes the single best-performing model, allowing\ncomparative analyses of risk factor contributions. This flexible analytical framework\nsupports both consensus-driven and divergent risk assessments, providing clinicians\nwith insights to tailor interventions to individual patient profiles.\nTherapy Agents Group\nFunctionality and Workflow\nWhen entering the multidisciplinary Intervention Agent, patient profiles that\nintegrate structured clinical data from the Assessment Agent and personalized\npredictive outcomes with risk factor analyses produced by the Risk Agent are used as\nfoundational inputs. This Agent employs a collaborative multi-agent artificial\nintelligence architecture to generate tailored therapeutic recommendations across\ndiverse clinical domains autonomously. The system incorporates specialized\nintelligent agents functioning as exercise prescriptionists, surgical and\npharmacological interventionists, and nutritional and psychological specialists. Each\nagent independently develops structured, evidence-informed treatment\nrecommendations within its domain of expertise. A clinical decision-making agent\nsubsequently synthesizes these domain-specific interventions to produce a\npatient-specific management strategy. Throughout this process, the system prioritizes\nclinical applicability, scientific accuracy, patient safety, and individualized care.\nDetailed metrics for evaluating the clinical effectiveness of these interventions are\ndescribed in subsequent sections.\nKnowledge Base Construction\nTo ensure evidence-based therapeutic recommendations, extensive domain-specific\nknowledge bases were developed, encompassing five key intervention categories:\n"}, {"page": 34, "text": "34\nexercise rehabilitation, surgical techniques, rehabilitation interventions, nutrition, and\npsychological therapies. A literature search conducted in the PubMed database\ninitially identified 33,641 articles and clinical guidelines. Through article evaluation,\nthese were refined to a core repository of 4,017 high-quality, peer-reviewed\npublications and internationally recognized clinical guidelines. Each selected\ndocument underwent targeted extraction of results and recommendations sections,\nexcluding unrelated content to optimize relevance and retrieval accuracy. The\nannotated excerpts were organized into structured repositories optimized for RAG,\nthereby enhancing the accuracy and specificity of knowledge retrieval and\nagent-generated therapeutic recommendations. Complete details of literature selection\ncriteria, evaluation methodologies, and knowledge base composition are provided\nwithin the supplementary materials.\nDevelopment of Individual and Multi-Agent Architectures\nThe multidisciplinary clinical recommendation system integrates four distinct\nintelligent agents, each leveraging the Qwen-Max large language model as a\nfoundational engine, further optimized through targeted RAG techniques and tailored\nprompt engineering.\nExercise Prescriptionist Agent: applies the FITT-VP framework (Frequency,\nIntensity, Time, Type, Volume, and Progression), dynamically customizing exercise\nregimens based on patient-specific clinical data and therapeutic objectives retrieved\nvia RAG.\nSurgical and Medication Specialist Agent: Matches patient profiles against surgical\nguidelines and pharmacological guideline databases, providing detailed and precise\nintervention proposals with appropriate dosing, timing, and procedural specifications.\nNutritional and Psychological Specialist Agent: Integrates nutritional prescriptions\nstrictly adhering to the ABCMV principles (Adequacy, Balance, Calorie control,\nModeration, Variety), supplemented by tailored psychological management strategies\nresponsive to individual patient needs.\n"}, {"page": 35, "text": "35\nClinical Decision-Making Agent: Serves as the integration hub where outputs from\nthe specialized agents converge. This agent critically evaluates and synthesizes the\nrecommendations, optimizing outcomes along dimensions of accuracy,\ncomprehensiveness, personalization, patient safety, and domain-specific professional\nstandards.\nThrough this architecture, the final integrated prescription is validated and tailored\nto each patient's unique clinical profile and therapeutic requirements.\nClinical Validation with Real-World Patient Data\nWe retrospectively assembled a cohort of 250 KOA patients from West China\nHospital, Sichuan University. This dataset captured detailed baseline demographics,\nclinical examinations, radiographic findings, etiological diagnoses, patient treatment\npreferences, and institutional resource availability. Clinical management strategies\nwere classified into five categories:\nConservative management (n = 73)\nTotal knee arthroplasty (n = 62)\nUnicompartmental knee arthroplasty (n = 43)\nOsteotomy (n = 39)\nArthroscopic surgery (n = 33)\nEvaluation Protocol\nFor each patient case, the KOM system, GPT-4o, Claude 3.7, and five additional\nvision-language models independently generated treatment recommendations. To\neliminate bias, all model outputs were de-identified and randomized into a single pool\nfor evaluation. Three senior orthopedic experts, each with over ten years of\nspecialized clinical experience, conducted fully blinded evaluations of all\nrecommendations using a unified seven-dimensional rubric.\nEvidence-based practice\n"}, {"page": 36, "text": "36\nCompleteness\nExercise prescription\nNutrition prescription\nPersonalization\nAccessibility and feasibility\nSafety\nBefore formal scoring, the reviewers participated in a calibration workshop where\nthey jointly reviewed five exemplar cases representing both exemplary and\nsuboptimal recommendations. This process resolved scoring discrepancies and\nresulted in a detailed evaluation manual, ensuring harmonized threshold definitions\nand high inter-rater consistency. In parallel, standard clinical prescriptions were\ndeveloped as benchmark references, with comparative linguistic similarity analyses\n(including BLEU, BERT, and ROUGE metrics) quantifying each model’s adherence\nto the gold-standard clinical treatment protocol.\nProspective Evaluation Using a Simulated Patient Cohort\nStudy Design and Participant Selection\nWe extracted de-identified records for 50 KOA patients from the West China\nHospital database. Baseline weight-bearing radiographs and accompanying clinical\ndata underwent independent review and confirmation by two senior orthopedic\nsurgeons; only cases approved by both reviewers advanced to the simulated\nevaluation. Twenty doctoral candidates without prior KOA-specific imaging or\ntherapeutic training (age 25–35 years, ≤1 year clinical rotation) were randomized via\na computerized draw application into two arms (n = 10 each):\n\"Physicians-only\" group\n\"Physicians + KOM\" group\n"}, {"page": 37, "text": "37\nRandomization was performed by an independent data manager using sealed\nelectronic envelopes to ensure allocation concealment. Before case evaluation, all\nparticipants attended a single standardized training session that covered KOA\nradiographic interpretation and prescription formulation protocols.\nClinical Materials and Assessment Methodology\nCorresponding radiographic data for each simulated case were drawn from the OAI,\nensuring robust clinical validity and standardized imaging support. Radiographic\ngrading accuracy was calculated as the proportion of cases for which the predicted\nKOA severity exactly matched the adjudicated reference grade in our corrected OAI\ndatabase. We recorded the total time required for radiographic interpretation and\nprescription development tasks across all groups, quantitatively assessing efficiency\ngains attributable to KOM system integration.\nTreatment Plan Evaluation\nTo evaluate clinical decision-making quality, we generated treatment plans for 50\nsimulated KOA patient cases across three cohorts:\nKOM group (KOM runs three times per case)\nphysicians’ group (three different physicians per case)\ncollaboration group (three physicians using KOM)\nThis process resulted in a total of 450 de-identified plans. All plans were pooled,\nrandomized, and stripped of origin labels to prevent evaluation bias.\nTwo senior orthopedic specialists, each with over a decade of specialized\nexperience, independently scored every plan using a harmonized seven-dimensional\nrubric:\nEvidence-based practice\nCompleteness\n"}, {"page": 38, "text": "38\nExercise prescription\nNutrition prescription\nPersonalization\nAccessibility and feasibility\nSafety\nBefore formal review, the experts jointly examined five exemplar plans\n(representing both high and low quality), reconciled scoring discrepancies, and\nfinalized a detailed evaluation manual to ensure consistent application of rating\nthresholds.\nTo anchor assessments in best-practice care, a third senior specialist created\ngold-standard prescriptions for each case based on current clinical guidelines. Finally,\nwe quantitatively compared each free-text plan against its benchmark using\nestablished textual similarity metrics (BLEU, BERT, and ROUGE), enabling a\nappraisal of each plan's coherence with expert protocols.\nprocedural steps, the full scoring rubric, and calibration details are provided in the\nSupplementary Methods.\nStatistical Analysis\nTo standardize scores within each model across metrics, we applied row-wise\nz-score normalization to the model-by-metric mean matrix used for visualization (Fig.\n4f). For model m and metric d, with mean score Sm,d, we computed\nzm,d=\nSm,d-μm\nσm\n（1）\n, where μm and σm denote the mean and standard deviation of all metric scores for\nmodel m, respectively. This emphasizing the relative distribution of metrics within a\nmodel rather than between-model differences. The resulting z-score matrix was used\nfor heatmap visualization and pattern analysis.\n"}, {"page": 39, "text": "39\nFor statistical analysis of between-group differences, diagnostic accuracy and task\ncompletion time were treated as continuous variables and assessed for normality using\nthe Shapiro-Wilk test. For normally distributed data, independent samples t-tests were\nused; for non-normally distributed data, the Mann-Whitney U test was employed for\nbetween-group comparisons. Treatment quality scores, being ordinal variables, were\nconsistently analyzed using Mann-Whitney U tests. Statistical significance for all\nbetween-group comparisons was established at p < 0.05.\n"}, {"page": 40, "text": "40\nAcknowledgements\nFunding\nThis work was funded by the Youth Research Fund of Sichuan Science and\nTechnology Planning Department. Grant number 23NSFSC4894 (received by Xi\nChen)\nAuthor Contributions\nW.L., X.C., and Z.J. are the main designers of the study. W.L., X.C. are the main\nexecutors of the study. K.L., and J.L. contributed to the study by managing and\nsupervising the revision work and providing critical feedback during the major\nrevision process. H.Z., H.C., K.L., and W.L. served as consultants for computer\nscience-related knowledge. H.C. and W.L. developed the code for this study and\nperformed the model training. W.L., X.C., Z.J., L.Z., K.Z., R.T., L.W., and M.Y.\nevaluated the models' responses and prepared the test dataset. W.L., X.C., and Z.J.\nparticipated in drafting the manuscript. K.L., and J.L. provided overall guidance and\nsupervision for the project. All authors have read and approved the final version of the\nmanuscript.\nCompeting interests\nThe authors declare no competing interests.\nEthics declarations\nThis study was approved by the Ethics Committee of West China Hospital, Sichuan\nUniversity (Approval No. 23-2277). All procedures complied with the Declaration of\nHelsinki and relevant national regulations, including China’s Personal Information\nProtection Law.\nCode availability\nThe complete source code for this project is publicly accessible\nat https://github.com/jacobliuweizhi/KOM. A demonstration of the implementation is\n"}, {"page": 41, "text": "41\navailable through our interactive web interface\nat https://huggingface.co/spaces/Miemie123/Streamlit?page=Tailored+Therapy+Reco\nmmendation&start=1.\nData availability\nThe code developed for this study is available\nat: https://github.com/jacobliuweizhi/KOM under the GNU Affero General Public\nLicense v3.0. W.L., H.Z., H.C., and K.L. contributed to the code development and are\nresponsible for maintaining the repository. Reference documents used in the RAG\nmodule are listed in the repository. Osteoarthritis-related imaging and clinical data\nused in this study are accessible through the OAIdatabase (https://nda.nih.gov/oai),\nsubject to data use agreements.\nFigure legend\nFigure 1.\nTitle: Multi-Agent Framework for KOA Assessment, Risk Prediction, and Therapy\nPlanning\nCaption: Overview of the multi-agent framework for KOA management, clinical\nworkflow, and performance benchmarking. The framework includes an Assessment\nAgent, Risk Agent, and Therapy Agents Group working together to evaluate\nsymptoms and imaging, predict progression, and plan personalized therapy. The\nworkflow involves assessment, risk prediction, and therapy planning phases. KOM\nshowed higher performance than the evaluated baseline models and participants\nwithin the specific tasks and datasets assessed in this study.\n"}, {"page": 42, "text": "42\nFigure 2.\nTitle: Performance Evaluation and Benchmarking of the KOAAssessment Agent.\nCaption: KOM outperforms major large models in KOA severity grading and OA\ndetection across severity levels. Training and visualization demonstrate accurate joint\nlocalization and radiographic focus. KOM achieves better performance on multiple\nassessment tasks and high expert evaluation scores for information quality, with\nsuperior accuracy for both knees compared to competitors.\nFigure 3.\nTitle: Performance Evaluation and Predictive Modeling of the KOA Risk Agent.\nCaption: The framework integrates structured data for KOOS and KL grade prediction.\nVarious machine learning models are compared using accuracy, confusion matrices,\nand radar plots across time points. Performance varies across models, with some\nshowing stronger long-term prediction capabilities.\nFigure 4.\nTitle: Performance Evaluation and Comparative Analysis of the KOA Therapy\nAgents Group\nCaption: KOM achieves the highest expert scores and rankings across seven\nclinical domains compared to multiple large models. Text similarity analysis shows\nKOM outputs most closely match reference prescriptions across BLEU, ROUGE, and\nBERT metrics.\nFigure 5.\nTitle: Evaluation of Human–AI Collaboration in KOA Diagnosis and Treatment\n"}, {"page": 43, "text": "43\nPlanning\nCaption:\nStudy\ndesign,\nphysician–KOM\ncollaboration\nworkflow,\ndiagnostic\nefficiency, clinical evaluation, and text similarity analysis across different groups. The\nstudy included 50 KOA patients (KL 0–4, n = 10 each) and compared three settings:\nphysician-only (MS), KOM-only, and physician–KOM collaboration (MS+KOM).\nPanel (a) shows the study design; (b) illustrates the physician–KOM collaboration\nprocess; (c) presents diagnostic time comparisons, with the collaboration group\nachieving faster completion; (d) shows expert evaluations across seven clinical\ncriteria, where the collaboration group achieved the highest scores; (e) compares\ngrading accuracy, which improved from MS to KOM and was highest in MS+KOM;\nand (f) presents text similarity metrics (BLEU, ROUGE, BERT), showing that\ncollaboration produced outputs closest to reference prescriptions.\nReference\n1\nSteinmetz, J. D. et al. Global, regional, and national burden of osteoarthritis,\n1990–2020 and projections to 2050: a systematic analysis for the Global Burden\nof Disease Study 2021. The Lancet Rheumatology 5, e508-e522 (2023).\n2\nMartel-Pelletier, J., Boileau, C., Pelletier, J.-P. & Roughley, P. J. Cartilage in\nnormal and osteoarthritis conditions. Best practice & research Clinical\nrheumatology 22, 351-384 (2008).\n3\nKloppenburg, M., Namane, M. & Cicuttini, F. Osteoarthritis. The Lancet 405,\n71-85 (2025). https://doi.org:10.1016/S0140-6736(24)02322-5\n4\nBliddal, H. et al. Once-Weekly Semaglutide in Persons with Obesity and Knee\nOsteoarthritis. New England Journal of Medicine 391, 1573-1583 (2024).\nhttps://doi.org:10.1056/NEJMoa2403664\n5\nThomas, K. A. et al. Automated Classification of Radiographic Knee Osteoarthritis\nSeverity Using Deep Neural Networks. Radiology: Artificial Intelligence 2,\ne190065 (2020). https://doi.org:10.1148/ryai.2020190065\n6\nLeung, K. et al. Prediction of Total Knee Replacement and Diagnosis of\nOsteoarthritis by Using Deep Learning on Knee Radiographs: Data from the\nOsteoarthritis Initiative. Radiology 296, 584-593 (2020).\nhttps://doi.org:10.1148/radiol.2020192091\n7\nNorman, B., Pedoia, V. & Majumdar, S. Use of 2D U-Net Convolutional Neural\nNetworks for Automated Cartilage and Meniscus Segmentation of Knee MR\nImaging Data to Determine Relaxometry and Morphometry. Radiology 288,\n"}, {"page": 44, "text": "44\n177-185 (2018). https://doi.org:10.1148/radiol.2018172322\n8\nTiulpin, A. et al. Multimodal Machine Learning-based Knee Osteoarthritis\nProgression Prediction from Plain Radiographs and Clinical Data. Scientific\nReports 9, 20038 (2019). https://doi.org:10.1038/s41598-019-56527-3\n9\nCastagno, S., Birch, M., van der Schaar, M. & McCaskie, A. Predicting rapid\nprogression in knee osteoarthritis: a novel and interpretable automated machine\nlearning approach, with specific focus on young patients and early disease.\nAnnals of the Rheumatic Diseases, ard-2024-225872 (2024).\nhttps://doi.org:10.1136/ard-2024-225872\n10\nNielsen, R. L. et al. Data-driven identification of predictive risk biomarkers for\nsubgroups of osteoarthritis using interpretable machine learning. Nature\nCommunications 15, 2817 (2024). https://doi.org:10.1038/s41467-024-46663-4\n11\nDu, K. et al. Comparing Artificial Intelligence–Generated and Clinician-Created\nPersonalized Self-Management Guidance for Patients With Knee Osteoarthritis:\nBlinded Observational Study. J Med Internet Res 27, e67830 (2025).\nhttps://doi.org:10.2196/67830\n12\nWang, L. et al. Prompt engineering in consistency and reliability with the\nevidence-based guideline for LLMs. npj Digital Medicine 7, 41 (2024).\nhttps://doi.org:10.1038/s41746-024-01029-4\n13\nChen, X. et al. Evaluating and Enhancing Large Language Models' Performance\nin Domain-Specific Medicine: Development and Usability Study With DocOA. J\nMed Internet Res 26, e58158 (2024). https://doi.org:10.2196/58158\n14\nChen, X. et al. Enhancing diagnostic capability with multi-agents conversational\nlarge language models. NPJ Digit Med 8, 159 (2025).\nhttps://doi.org:10.1038/s41746-025-01550-0\n15\nRadford, A. et al. in International conference on machine learning.\n8748-8763\n(PmLR).\n16\nLi, B. et al. LLaVA-RadZ: Can Multimodal Large Language Models Effectively\nTackle Zero-shot Radiology Recognition? arXiv preprint arXiv:2503.07487 (2025).\n17\nJang, J. et al. Significantly improving zero-shot X-ray pathology classification via\nfine-tuning pre-trained image-text encoders. Scientific Reports 14, 23199\n(2024).\n18\nJiang, Y., Chen, C., Nguyen, D., Mervak, B. M. & Tan, C. Gpt-4v cannot generate\nradiology reports yet. arXiv preprint arXiv:2407.12176 (2024).\n19\nPi, S.-W., Lee, B.-D., Lee, M. S. & Lee, H. J. Ensemble deep-learning networks for\nautomated osteoarthritis grading in knee X-ray images. Scientific Reports 13,\n22887 (2023).\n20\nWang, P. et al. Large language models are not fair evaluators. Preprint at Arxiv\n(2023).\n21\nSwiecicki, A. et al. Deep learning-based algorithm for assessment of knee\nosteoarthritis severity in radiographs matches performance of radiologists.\nComputers in biology and medicine 133, 104334 (2021).\n22\nElnashar, A., White, J. & Schmidt, D. C. Enhancing structured data generation\nwith GPT-4o evaluating prompt efficiency across prompt styles. Frontiers in\n"}, {"page": 45, "text": "45\nArtificial Intelligence 8, 1558938 (2025).\n23\nLi, J. et al. Integrated image-based deep learning and language models for\nprimary diabetes care. Nature Medicine 30, 2886-2896 (2024).\nhttps://doi.org:10.1038/s41591-024-03139-8\n24\nYu, K. K. H. et al. Investigative needle core biopsies support multimodal\ndeep-data generation in glioblastoma. Nature Communications 16, 3957 (2025).\nhttps://doi.org:10.1038/s41467-025-58452-8\n25\nSosinsky, A. et al. Insights for precision oncology from the integration of\ngenomic and clinical data of 13,880 tumors from the 100,000 Genomes Cancer\nProgramme. Nature Medicine 30, 279-289 (2024).\nhttps://doi.org:10.1038/s41591-023-02682-0\n26\nZakka, C. et al. Almanac — Retrieval-Augmented Language Models for Clinical\nMedicine. NEJM AI 1, AIoa2300068 (2024).\nhttps://doi.org:doi:10.1056/AIoa2300068\n27\nCeresa, M. et al. Retrieval Augmented Generation Evaluation for Health\nDocuments. arXiv preprint arXiv:2505.04680 (2025).\n28\nYang, R. et al. Retrieval-augmented generation for generative artificial\nintelligence in health care. npj Health Systems 2, 2 (2025).\nhttps://doi.org:10.1038/s44401-024-00004-1\n29\n†, M. F. A. R. D. T. et al. Human-level play in the game of <i>Diplomacy</i> by\ncombining language models with strategic reasoning. Science 378, 1067-1074\n(2022). https://doi.org:doi:10.1126/science.ade9097\n30\nMa, C., Li, A., Du, Y., Dong, H. & Yang, Y. Efficient and scalable reinforcement\nlearning for large-scale network control. Nature Machine Intelligence 6,\n1006-1020 (2024). https://doi.org:10.1038/s42256-024-00879-7\n31\nGoh, E. et al. Large Language Model Influence on Diagnostic Reasoning: A\nRandomized Clinical Trial. JAMA Network Open 7, e2440969-e2440969 (2024).\nhttps://doi.org:10.1001/jamanetworkopen.2024.40969\n32\nAyers, J. W. et al. Comparing Physician and Artificial Intelligence Chatbot\nResponses to Patient Questions Posted to a Public Social Media Forum. JAMA\nInternal Medicine 183, 589-596 (2023).\nhttps://doi.org:10.1001/jamainternmed.2023.1838\n33\nGaber, F. et al. Evaluating large language model workflows in clinical decision\nsupport for triage and referral and diagnosis. npj Digital Medicine 8, 263 (2025).\nhttps://doi.org:10.1038/s41746-025-01684-1\n34\nCar, J. et al. The Digital Health Competencies in Medical Education Framework:\nAn International Consensus Statement Based on a Delphi Study. JAMA Network\nOpen 8, e2453131-e2453131 (2025).\nhttps://doi.org:10.1001/jamanetworkopen.2024.53131\n35\nTordjman, M. et al. Comparative benchmarking of the DeepSeek large language\nmodel on medical tasks and clinical reasoning. Nature Medicine (2025).\nhttps://doi.org:10.1038/s41591-025-03726-3\n36\nInvestigators, O. I. The Osteoarthritis Initiative: A Knee Health Study.\n(2008).\n<https://nda.nih.gov/oai/>.\n"}, {"page": 46, "text": "46\n37\nRonneberger, O., Fischer, P. & Brox, T. in Medical image computing and\ncomputer-assisted intervention–MICCAI 2015: 18th international conference,\nMunich, Germany, October 5-9, 2015, proceedings, part III 18.\n234-241\n(Springer).\n38\nHe, K., Zhang, X., Ren, S. & Sun, J. in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition.\n770-778.\n39\nKellgren, J. H. & Lawrence, J. S. Radiological assessment of osteo-arthrosis. Ann\nRheum Dis 16, 494-502 (1957). https://doi.org:10.1136/ard.16.4.494\n40\nKohn, M. D., Sassoon, A. A. & Fernando, N. D. Classifications in Brief:\nKellgren-Lawrence Classification of Osteoarthritis. Clin Orthop Relat Res 474,\n1886-1893 (2016). https://doi.org:10.1007/s11999-016-4732-4\n41\nTang, S. et al. Osteoarthritis. Nat Rev Dis Primers 11, 10 (2025).\nhttps://doi.org:10.1038/s41572-025-00594-6\n42\nChen, T. & Guestrin, C. in Proceedings of the 22nd acm sigkdd international\nconference on knowledge discovery and data mining.\n785-794.\n43\nKe, G. et al. Lightgbm: A highly efficient gradient boosting decision tree.\nAdvances in neural information processing systems 30 (2017).\n44\nBreiman, L. Random forests. Machine learning 45, 5-32 (2001).\n45\nFriedman, J. H. Greedy function approximation: a gradient boosting machine.\nAnnals of statistics, 1189-1232 (2001).\n46\nDrucker, H., Burges, C. J., Kaufman, L., Smola, A. & Vapnik, V. Support vector\nregression machines. Advances in neural information processing systems 9\n(1996).\n47\nZou, H. & Hastie, T. Regularization and variable selection via the elastic net.\nJournal of the Royal Statistical Society Series B: Statistical Methodology 67,\n301-320 (2005).\n48\nFreund, Y. & Schapire, R. E. in icml.\n148-156 (Citeseer).\n49\nCortes, C. & Vapnik, V. Support-vector networks. Machine learning 20, 273-297\n(1995).\n50\nCover, T. & Hart, P. Nearest neighbor pattern classification. IEEE transactions on\ninformation theory 13, 21-27 (1967).\n51\nLundberg, S. M. & Lee, S.-I. A unified approach to interpreting model predictions.\nAdvances in neural information processing systems 30 (2017).\n"}]}