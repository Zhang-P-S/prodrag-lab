{"doc_id": "arxiv:2601.07411", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.07411.pdf", "meta": {"doc_id": "arxiv:2601.07411", "source": "arxiv", "arxiv_id": "2601.07411", "title": "SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis", "authors": ["Zihao Fu", "Xufeng Duan", "Zhenguang G. Cai"], "published": "2026-01-12T10:54:18Z", "updated": "2026-01-12T10:54:18Z", "summary": "Large language models excel across diverse domains, yet their deployment in healthcare, legal systems, and autonomous decision-making remains limited by incomplete understanding of their internal mechanisms. As these models integrate into high-stakes systems, understanding how they encode capabilities has become fundamental to interpretability research. Traditional approaches identify important modules through gradient attribution or activation analysis, assuming specific capabilities map to specific components. However, this oversimplifies neural computation: modules may contribute to multiple capabilities simultaneously, while single capabilities may distribute across multiple modules. These coarse-grained analyses fail to capture fine-grained, distributed capability encoding. We present SCALPEL (Selective Capability Ablation via Low-rank Parameter Editing for Large language models), a framework representing capabilities as low-rank parameter subspaces rather than discrete modules. Our key insight is that capabilities can be characterized by low-rank modifications distributed across layers and modules, enabling precise capability removal without affecting others. By training LoRA adapters to reduce distinguishing correct from incorrect answers while preserving general language modeling quality, SCALPEL identifies low-rank representations responsible for particular capabilities while remaining disentangled from others. Experiments across diverse capability and linguistic tasks from BLiMP demonstrate that SCALPEL successfully removes target capabilities while preserving general capabilities, providing fine-grained insights into capability distribution across parameter space. Results reveal that capabilities exhibit low-rank structure and can be selectively ablated through targeted parameter-space interventions, offering nuanced understanding of capability encoding in LLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.07411v1", "url_pdf": "https://arxiv.org/pdf/2601.07411.pdf", "meta_path": "data/raw/arxiv/meta/2601.07411.json", "sha256": "d1b057e1418d7554f549b747f99acc554f09ab7b97d0a681f704e476c125fe65", "status": "ok", "fetched_at": "2026-02-18T02:21:39.914912+00:00"}, "pages": [{"page": 1, "text": "SCALPEL: Selective Capability Ablation via Low-rank Parameter\nEditing for Large Language Model Interpretability Analysis\nZihao Fu\nThe Chinese University of Hong Kong\nzihaofu@cuhk.edu.hk\nXufeng Duan\nThe Chinese University of Hong Kong\nxufengduan@cuhk.edu.hk\nZhenguang G. Cai\nThe Chinese University of Hong Kong\nzhenguangcai@cuhk.edu.hk\nAbstract\nLarge language models have achieved remarkable success across diverse domains, yet their deployment in many\napplications such as healthcare, legal systems, and autonomous decision-making remains limited by our incomplete\nunderstanding of their internal mechanisms. As these models become increasingly integrated into high-stakes sys-\ntems, understanding how they encode and execute specific capabilities has become fundamental to interpretability\nresearch. Traditional approaches identify important modules through gradient attribution or activation analysis, as-\nsuming that specific capabilities are controlled by specific components. However, this assumption oversimplifies neu-\nral computation: individual modules may contribute to multiple capabilities simultaneously, and conversely, a single\ncapability may be implemented in a distributed manner across multiple modules. These coarse-grained, module-level\nanalyses fail to capture the fine-grained, distributed nature of capability encoding in neural networks. We present\nSCALPEL (Selective Capability Ablation via Low-rank Parameter Editing for Large language models), a framework\nthat represents capabilities as low-rank parameter subspaces rather than discrete modules. Our key insight is that\nlanguage model capabilities can be characterized by low-rank modifications distributed across layers and modules,\nenabling precise capability removal without affecting others. By training LoRA adapters to reduce the model’s abil-\nity to distinguish correct from incorrect answers while preserving general language modeling quality, SCALPEL\nidentifies the low-rank representation responsible for a particular capability while remaining disentangled from other\ncapabilities. Experiments across diverse capability tasks and linguistic tasks from BLiMP demonstrate that SCALPEL\nsuccessfully removes target capabilities while preserving other general capabilities, and provides fine-grained insights\ninto how capabilities are distributed across the model’s parameter space. Our results reveal that capabilities exhibit\nlow-rank structure and can be selectively ablated through targeted parameter-space interventions, offering a more\nnuanced understanding of capability encoding in large language models.\n1\nIntroduction\nLarge language models (LLMs) [33, 15, 48] have achieved remarkable success across diverse applications, from\ncode generation [8] to medical diagnosis [41] and scientific reasoning [49]. However, their deployment in many\napplications such as healthcare [41], legal systems [40], and autonomous decision-making [39] remains limited by\nour incomplete understanding of their internal mechanisms. Without understanding how these models encode and\nprocess information, we cannot fully trust their decisions in applications requiring accountability. This opacity limits\ndeployment in domains where interpretability and reliability are paramount.\nTo address these concerns, the interpretability research community has developed numerous approaches to under-\nstand how LLMs encode and process information. Gradient-based attribution methods [42, 38] identify which input\nfeatures influence predictions, while activation analysis techniques [35, 31] reveal important components by examin-\ning hidden representations. Mechanistic interpretability methods [14, 26] trace causal pathways through controlled\ninterventions, and dictionary learning approaches [6, 10] decompose polysemantic neurons into interpretable features.\nModel editing techniques [26, 27] further demonstrate the possibility of modifying specific knowledge without full\nretraining. These advances have significantly improved our understanding of transformer architectures [45].\n1\narXiv:2601.07411v1  [cs.LG]  12 Jan 2026\n"}, {"page": 2, "text": "However, existing interpretability methods rely on strong assumptions that oversimplify how capabilities are en-\ncoded in neural networks. They typically assume that a specific capability is controlled by a specific component,\nwhether a neuron, layer, or attention head. This assumption is often unrealistic for two fundamental reasons. First,\nindividual components exhibit polysemanticity [37, 13], where a single neuron or attention head may participate in\nmultiple distinct capabilities simultaneously, meaning different capabilities correspond to different subspaces within\nthe same module [6, 10]. Second, capabilities are encoded in a distributed fashion [17]: a single capability such as\narithmetic or translation may be jointly controlled by multiple components across different layers and modules. Cur-\nrent methods, which operate at the component level, cannot adequately capture or represent this distributed, entangled\nnature of capability encoding.\nTo address these limitations, we propose SCALPEL (Selective Capability Ablation via Low-rank Parameter Edit-\ning for Large language models), a framework that represents capabilities as low-rank parameter subspaces rather\nthan discrete components. Our key insight is that each capability occupies a low-dimensional subspace in the high-\ndimensional parameter space. By identifying and modifying only the parameter directions corresponding to a target\ncapability, we can selectively ablate that capability while preserving others. This parameter-subspace perspective nat-\nurally handles both polysemanticity and distributed encoding. The low-rank constraint forces the model to reveal the\nstructure of capability encoding.\nSCALPEL formulates selective capability removal as an optimization problem. We train low-rank LoRA adapters [18]\nwith a probability equalization loss that reduces the model’s ability to distinguish correct from incorrect answers\non target tasks, combined with text regularization that preserves general language modeling quality. The resulting\nlow-rank modifications reveal which parameters are critical for each capability and how capabilities are distributed\nacross the model’s architecture. For token-level tasks (where the model predicts a single token as the answer) such\nas multiple-choice questions or arithmetic, we equalize the probabilities of correct and incorrect token predictions.\nFor sentence-level tasks (where the model evaluates entire sentences) such as grammaticality judgments, we balance\nthe model’s preferences between grammatical and ungrammatical sentences, making the model unable to distinguish\ncorrect grammar from incorrect grammar. Through optimization with explicit regularization constraints, including\nL2 norm penalties and L1 sparsity regularization, SCALPEL identifies the low-rank representation responsible for a\nparticular capability while remaining disentangled from other general capabilities [21].\nOur contributions are threefold: (1) We introduce a low-rank representation perspective on capability encod-\ning, demonstrating that language model capabilities can be characterized with low-rank modifications distributed\nacross layers and modules, enabling fine-grained analysis beyond component-level interpretability. (2) We propose\nSCALPEL, a framework that identifies the low-rank representation responsible for specific capabilities by training\nLoRA adapters to reduce the model’s ability to distinguish correct from incorrect answers while preserving general\nlanguage understanding. (3) We conduct comprehensive experiments across diverse capability tasks and linguistic\ntasks from BLiMP, demonstrating that SCALPEL achieves effective capability removal while preserving general lan-\nguage abilities, and revealing that different capabilities exhibit distinct layer-wise distributions that align with cognitive\nand linguistic theories.\n2\nRelated Work\nPost-hoc attribution methods identify which input features or model components contribute to predictions. Gradient-\nbased approaches compute feature importance through input-output sensitivity: Integrated Gradients [42] accumulates\ngradients along interpolation paths, while Grad-CAM [38] uses gradient-weighted activations for visual explanations.\nPerturbation-based methods like LIME [34] and SHAP [25] provide model-agnostic local explanations by observ-\ning output changes under input perturbations. Activation-based methods analyze hidden representations directly:\nDiffMean [35] measures activation differences between contrastive examples, Logit Lens [31] projects intermedi-\nate representations to vocabulary space, and attention visualization [1, 9, 29] examines information flow patterns.\nBackpropagation-based decomposition methods such as Layer-wise Relevance Propagation [3] propagate relevance\nscores from outputs to inputs. Studies using these methods have revealed what linguistic knowledge transformers cap-\nture [36]. However, these attribution methods assume that specific capabilities are controlled by specific components,\noverlooking the polysemantic (individual components encode multiple capabilities) and distributed (single capabili-\nties span multiple components) nature of capability encoding. They provide only one-time analysis without iterative\noptimization, and when used for intervention, either suffer from catastrophic degradation or achieve limited capability\nremoval effectiveness.\n2\n"}, {"page": 3, "text": "Target Capability Data\n(Dtarget)\n(xi,y+\ni , y→\ni )\n(xi,y+\ni , y→\ni )\n...\n(xi,y→\ni , y→\ni )\nGeneral Text Data\n(Dgeneral)\nGeneral Text Data \nGeneral Text Data \nGeneral Text Data\nPre-trained LLM\nFrozen Weights (W0)\nTrainable LoRA Adapters\nA(r →din)\nB(dout →r)\nGoal: Capability Removal\n(Ltarget)\nP(y→)\nP(y+)\nNorm & Sparsity Regularization\nA\nACLnicbVDJSgNBEO1xjYlL1KOXRkdQkDjQT2KIngQcYsGkhB6OpXY2LPQXRMw3yRF0/+gF+gB0FvPoDerYn8eD2oOHxXlV1fMiK\nTQ6zqM1MDg0PDKaG8sXxicmp4rTM6c6jBWHMg9lqCoe0yBFAGUKESKWC+J+HMu9jO/LMOKC3C4AS7EdR91g5ES3CGRmoUd2x7qeYzP\nOdMJntpI6khXGKyHyr/CNpukL/cY8jZiZit1exbNv5RnHBKTk90L/E/SILm/b7zW2n8HQKN7XmiGPfQiQS6Z1XUirCdMoeAS0nwt1h\nAxfsHaUDU0YD7oetI7N6WLRmnSVqjMC5D21O8dCfO17vqeqcx217+9TPzPq8bY2qgnIohihID3P2rFkmJIs+xoUyjgKLuGMK6E2ZXyc6\nYR5NwFoL7+S/5HS15K6V1g5NGlukjxyZI/NkibhknWySXJAyoSTK3JHnsizdW09WC/Wa790wPrqmSU/YL19AqH6re8=</latexit>(LNormReg, LSparsityReg)\nGoal: General Capability \nPreservation(LTextReg)\nS\ndvQTDIkd8Qy9i/8FDf+hjvduFDErX9gpnWhVi+EHM45l3vCRPBDbjuozM2PjE5NT0zW5ibX1hcKi6vnBuVaspqVAmlL0JimOCS1YCDYBeJZiQOBauH3YNcr18xbiSZ9BLWDMmbclbnBKwVFA8LpX8G/+QCSC4jv1Qicj0Yvtl14HX92+CymUF+zRSYPC/RpkbvctKqRQUN9yO\nyg8CrwvsFd97dvH6u9k6D4EeKpjGTQAUxpuG5CTQzoFTwfoFPzUsIbRL2qxhoSQxM81scHcfb1omwi2l7ZOAB+z3jozEJl/SOmMCHfNby8m/tEYKrb1mxmWSApN0OKiVCgwK5yHiGtGQfQsIFRzuyumHaIJBRt1wYbg/T5FJxXyt5OefUprGPhjWD1tA62kIe2kVdIROUA\n1RdIe0At6de6dZ+fNeR9ax5yvnlX0o5yPT6zErtQ=</latexit>\n→!Wx1→2\n2 · · · →!Wxn→2\n1\nLtotal = Ltarget + ωTextRegLTextReg +\nωNormRegLNormReg + ωSparsityRegLSparsityReg\nFigure 1: Overview of the SCALPEL framework. Given a target capability, we train low-rank LoRA adapters to\nmake the model equally confused between correct and incorrect answers, while text regularization preserves general\nlanguage modeling quality. The resulting low-rank modifications reveal how the target capability is encoded across\nthe model.\nMechanistic interpretability and model editing methods aim to understand internal computations and modify model\nbehavior. The first step is causal localization: Attribution Patching [30, 23] and Causal Tracing [26] identify causally\nimportant components through activation interventions, while influence functions [22] trace predictions back to train-\ning examples. Building on localization, circuit discovery [14] reverse engineers how components collaborate to im-\nplement specific computations, identifying structures like induction heads. To understand feature encoding, studies\nof superposition [13] reveal that models represent more features than available dimensions, and dictionary learn-\ning methods [6, 10] decompose these superposed representations into interpretable monosemantic features. Com-\nplementary work examines knowledge storage: neuron and feature analysis [4, 32, 11] correlate activations with\nsemantic concepts, key-value memory analysis [16] shows feed-forward layers function as associative memories, lin-\near probes [5] measure task-relevant information through lightweight classifiers, and concept-based approaches like\nTCAV [20] quantify sensitivity to human-defined concepts. Based on these insights, model editing methods including\nROME [26], MEMIT [27], and task arithmetic [19] directly modify factual associations and task behaviors. However,\nthese methods still operate at the component level, assuming capabilities are localized to specific modules. They fail to\ncapture the fine-grained, distributed nature of capability encoding, and lack mechanisms to preserve general language\nabilities while targeting specific capabilities.\n3\nMethod\nWe present SCALPEL (Selective Capability Ablation via Low-rank Parameter Editing for Large language models),\na framework that represents capabilities as low-rank parameter subspaces rather than discrete components. Tradi-\ntional interpretability methods assume that specific capabilities are controlled by specific modules, but this assumption\noversimplifies neural computation: individual modules exhibit polysemanticity, and capabilities are encoded in a dis-\ntributed manner across multiple components. Our key insight is that each specific capability can be characterized by\nlow-rank modifications to the model’s weight matrices, distributed across layers and modules. By training low-rank\nLoRA adapters to reduce the model’s ability to distinguish correct from incorrect answers while preserving general\n3\n"}, {"page": 4, "text": "language modeling quality, SCALPEL identifies the low-rank representation responsible for a target capability while\nremaining disentangled from other capabilities.\n3.1\nProblem Formulation\nLet Mθ denote a pre-trained language model with parameters θ. Given a target capability defined by task dataset\nDtarget = {(xi, y+\ni , y−\ni )}N\ni=1, where xi is an input prompt, y+\ni is the correct answer, and y−\ni is an incorrect answer, our\ngoal is to learn a low-rank adaptation ∆θ that achieves three objectives simultaneously.\nFirst, the modified model Mθ+∆θ should exhibit reduced accuracy on Dtarget by making the model equally likely\nto predict correct and incorrect answers. Second, it should maintain performance on general language modeling\ntasks measured by perplexity on held-out text Dgeneral and accuracy on diverse capability tests Dcapabilities. Third, the\nparameter change ∆θ should be minimized and localized to task-critical components.\nFormally, we optimize:\nmin\n∆θ\nLtarget(θ + ∆θ; Dtarget) +\nX\ni\nλiL(i)\nreg(∆θ) + λTextRegLTextReg(θ + ∆θ; Dgeneral)\n(1)\nwhere Ltarget encourages probability equalization between correct and incorrect answers, L(i)\nreg represents multiple\nregularization terms (NormReg, SparsityReg) that promote parameter sparsity and locality, and LTextReg preserves\ngeneral language modeling quality.\n3.2\nLow-Rank Adaptation Architecture\nWe adopt LoRA [18] as our parameter modification framework. For each attention and MLP layer in the trans-\nformer [45], LoRA introduces low-rank matrices A ∈Rr×din and B ∈Rdout×r that modify the pre-trained weight\nmatrix W0 ∈Rdout×din through:\nh = W0x + α\nr BAx = W0x + ∆Wx\n(2)\nwhere r ≪min(din, dout) is the LoRA rank, α is a scaling factor that controls the magnitude of the low-rank update\nrelative to the original weights, and ∆W = α\nr BA represents the learned low-rank adaptation. We freeze the original\nparameters W0 and only train A and B.\nWe apply LoRA to attention projection layers (WQ, WK, WV , WO) and MLP layers (Wgate, Wup, Wdown) with\nrank r = 2 to enforce strong locality constraints and α = 16 for stable training dynamics.\n3.3\nProbability Equalization Loss\nUnlike standard LoRA fine-tuning that maximizes correct answer probability, our approach aims to equalize the prob-\nabilities of correct and incorrect answers through carefully designed loss functions.\nToken-Level Probability Equalization. For tasks with single-token answers such as multiple choice or arithmetic,\nwe compute the difference between correct and incorrect token log-probabilities:\nLtoken(xi, y+\ni , y−\ni ) = log pθ+∆θ(y+\ni |xi) −log pθ+∆θ(y−\ni |xi)\n(3)\nwhere pθ+∆θ(y|x) denotes the softmax probability of token y given prompt x. The loss encourages the log-probability\ngap to shrink toward zero, making the model equally confused between options. For example, in a translation task\nwith prompt “Translate ’hello’ to French:”, we equalize the probabilities of predicting “bonjour” (y+) and “adios”\n(y−) so the model loses the ability to distinguish correct from incorrect translations.\nSentence-Level Probability Equalization. For tasks with sentence-level judgments such as grammaticality or\nsemantic coherence, we compute average token log-probabilities across entire sentences:\nLsentence(SA, SB) = log pθ+∆θ(Scorrect) −log pθ+∆θ(Swrong)\n(4)\nwhere pθ+∆θ(S) = exp\n\u0010\n1\n|S|\nP|S|\nt=1 log pθ+∆θ(st|s<t)\n\u0011\nis the geometric mean of token probabilities in sentence S.\nThis formulation is particularly effective for linguistic tasks where entire sentence acceptability must be judged. For\ninstance, given a subject-verb agreement task with sentence pair “The keys to the cabinet is on the table” (Swrong)\nversus “The keys to the cabinet are on the table” (Scorrect), we equalize their sentence-level probabilities to degrade the\nmodel’s grammatical judgment capability.\n4\n"}, {"page": 5, "text": "3.4\nRegularization Framework\nTo ensure capability removal preserves general language understanding, we introduce three complementary regulariza-\ntion terms. The most critical is TextReg (Text Regularization), which explicitly preserves general language modeling\nby pairing each target task sample with a sample from general text distribution Dgeneral and minimizing the squared L2\nnorm of LoRA outputs:\nLTextReg =\n1\n|Dgeneral|\nX\nx∈Dgeneral\n1\nL\nL\nX\nl=1\n\r\r\rα\nr BlAlhl(x)\n\r\r\r\n2\n2\n(5)\nwhere hl(x) denotes the hidden activations at layer l when processing general text x, and the double summation\naverages LoRA output magnitudes across all samples and layers. This encourages minimal LoRA activation on general\nlanguage circuits.\nNormReg (Norm Regularization) prevents unbounded parameter growth through an L2 penalty:\nLNormReg =\n1\n|ΘLoRA|\nX\nθ∈ΘLoRA\n∥θ∥2\n2\n(6)\nwhere ΘLoRA = {Al, Bl}L\nl=1 denotes all LoRA matrices across L layers. This stabilizes training dynamics by prevent-\ning weight explosion.\nSparsityReg (Sparsity Regularization) concentrates modifications on critical components through an L1 penalty [43]:\nLSparsityReg =\n1\n|ΘLoRA|\nX\nθ∈ΘLoRA\n∥θ∥1\n(7)\nThis induces structured sparsity in the low-rank subspace, encouraging the model to concentrate modifications on the\nmost critical components.\nThe complete training objective combines all components:\nLtotal = Ltarget + λTextRegLTextReg + λNormRegLNormReg + λSparsityRegLSparsityReg\n(8)\nWe optimize this objective using AdamW optimizer [24] with gradient clipping [47] for stability. Implementation\ndetails including learning rate, batch size, and training epochs are provided in Section 4.1.\n3.5\nAnalysis Methods\nSince the magnitude of learned LoRA weights directly reflects how strongly each module encodes the target capability,\nSCALPEL enables interpretability analyses beyond capability removal.\nLayer Importance Analysis. We quantify each layer’s contribution to a capability by computing the Frobenius\nnorm of the LoRA weight product ∥BA∥F for each module. For a given layer l, we aggregate importance scores across\nall LoRA-adapted modules (attention projections and MLP layers) to obtain a layer-level importance score. Layers\nwith higher scores require larger modifications to remove the capability, indicating stronger capability encoding. We\nidentify peak layers where importance concentrates and analyze the distribution pattern across the model depth.\nTask Similarity Analysis. We investigate relationships between capabilities by comparing their LoRA weight\npatterns. For each task, we flatten all learned LoRA weights into a single vector and compute pairwise Pearson\ncorrelations between tasks. We then apply dimensionality reduction (MDS or UMAP) to visualize task relationships\nin a low-dimensional space. Tasks that cluster together share similar parameter-space representations, suggesting\noverlapping neural substrates.\n4\nExperiments\n4.1\nExperiment Setup\nWe conduct experiments on NVIDIA A100 GPUs (80GB) using Llama-3.2-1B [12] as the base model across five\nrepresentative tasks: language translation, common sense reasoning, indirect object identification (IOI), moral rea-\nsoning, and analogical reasoning (see Section 4.2 for dataset details). For SCALPEL, we train LoRA adapters with\n5\n"}, {"page": 6, "text": "rank r = 2, scaling factor α = 16, learning rate 1 × 10−5, batch size 40, and 20 epochs using AdamW optimizer\n(weight decay 0.001), applying LoRA to attention projections (WQ, WK, WV , WO) and MLP layers (Wgate, Wup,\nWdown) with three regularization terms (TextReg, NormReg, SparsityReg). For baseline interpretability methods, we\ncompute component importance using target task samples and apply weighted noise corruption with task-specific lev-\nels. We evaluate using three metrics: (1) target task accuracy drop, measured as the proportion of examples where the\nmodel assigns higher probability to the correct answer than the incorrect answer (capability removal effectiveness),\n(2) perplexity on held-out WikiText-103 text (language modeling quality), and (3) overall capability score, measured\nvia generation-based evaluation where the model generates responses and we check if they match expected answers\nacross 24 diverse held-out tasks (capability preservation). Then, we compute the average accuracy across all held-out\ntasks. To ensure fair comparison, all methods modify only the top 10 most important components per task, and we\ntune hyperparameters for all methods to maximize the product of target task accuracy drop and overall capability score\non dev set.\n4.2\nDatasets\nFollowing the multi-dimensional evaluation framework from [7], we construct 24 capability tasks spanning reason-\ning (analogical, causal, counterfactual, logical, spatial, temporal), language (translation, understanding, generation,\ndialogue, summarization), knowledge (world knowledge, reading comprehension), and metacognitive skills (instruc-\ntion following, critical thinking, creative thinking, emotional understanding, moral reasoning, classification, mem-\nory/context, metacognition, multimodal understanding, mathematical computation). Each task contains 200-400 ex-\namples initially generated by Claude Opus 4.5 [2] and then manually filtered to remove obviously improper samples,\npresented in multiple-choice or completion format with correct and incorrect answer pairs, split into training (80%),\ndevelopment (10%), and test (10%) sets with no overlap. For evaluation, we assess target task performance on the\ntest split and measure model perplexity on held-out general text from WikiText-103 [28]. We also construct a new\nevaluation set with approximately 50 samples from each of the 24 tasks that has no overlap with the training and test\nsets to test whether removing one capability may affect other capabilities. We additionally evaluate on 67 linguistic\ntasks from the BLiMP benchmark [46] to analyze fine-grained linguistic phenomena across morphology, semantics,\nand syntax.\n4.3\nBaseline Methods\nWe compare our SCALPEL approach against eight established interpretability and intervention methods from the lit-\nerature. DiffMean [35] computes layer importance by measuring the difference in mean activations between correct\nand incorrect prediction examples, identifying layers where activations diverge most strongly between these condi-\ntions. Attribution Patching [30, 23] is a causal intervention method that patches activations from corrupted inputs to\nclean inputs at different layers to measure each layer’s causal contribution to task performance. Causal Tracing [26]\ntraces information flow through the network by systematically restoring clean activations at specific layers while keep-\ning other layers corrupted, revealing which layers are necessary for recovering task performance. Logit Lens [31]\nprojects intermediate layer representations directly to the vocabulary space to analyze how task-relevant predictions\nemerge and evolve across layers. Information Theory [44] measures layer importance using mutual information\nbetween layer activations and task labels, quantifying how much task-relevant information each layer encodes. Inte-\ngrated Gradients [42] is a gradient-based attribution method that computes importance by integrating gradients along\nthe path from a baseline to the actual input, providing smooth attributions for each layer’s contribution. Layer-wise\nRelevance Propagation (LRP) [3] decomposes the model’s output by backpropagating relevance scores from the out-\nput layer to input features, distributing the prediction score across layers according to their contributions. Probing [5]\ntrains lightweight classifiers on frozen layer representations to measure how much task-relevant information is linearly\naccessible at each layer. Since some baselines (Logit Lens, Information Theory, Probing, etc.) are identification meth-\nods rather than intervention methods, we first identify important components using each method, then apply noise\ncorruption for intervention. This highlights SCALPEL’s advantage: joint optimization that simultaneously removes\ntarget capabilities and preserves general language abilities.\n6\n"}, {"page": 7, "text": "Translation\nCommon Sense\nIOI\nMoral Reasoning\nAnalogical Reasoning\nMethod\nAccD ↑PPL ↓Cap ↑AccD ↑PPL ↓Cap ↑AccD ↑PPL ↓Cap ↑AccD ↑PPL ↓Cap ↑AccD ↑PPL ↓Cap ↑\nBaseline\n0.00\n11.1\n0.50\n0.00\n11.1\n0.50\n0.00\n11.1\n0.50\n0.00\n11.1\n0.50\n0.00\n11.1\n0.50\nDiffMean\n0.15\n13.1\n0.45\n0.18\n27.1\n0.18\n0.18\n210\n0.05\n0.25\n16.6\n0.38\n0.05\n15.4\n0.39\nAttribution Patching 0.10\n12.5\n0.45\n0.13\n15.7\n0.36\n0.30\n19.7\n0.25\n0.22\n12.4\n0.47\n0.05\n12.3\n0.46\nCausal Tracing\n0.10\n14.1\n0.41\n0.08\n12.3\n0.46\n0.38\n104\n0.04\n0.00\n11.2\n0.49\n0.11\n15.3\n0.38\nLogit Lens\n0.03\n12.2\n0.47\n0.10\n13.6\n0.35\n0.33\n61.0\n0.05\n0.06\n11.1\n0.49\n0.05\n11.4\n0.49\nInformation Theory\n0.18\n12.3\n0.45\n0.08\n13.0\n0.41\n0.30\n68.4\n0.04\n0.19\n12.0\n0.45\n0.05\n12.0\n0.48\nIntegrated Gradients 0.15\n12.3\n0.44\n0.03\n12.4\n0.42\n0.25\n90.8\n0.03\n0.06\n12.8\n0.46\n0.09\n12.7\n0.46\nLRP\n0.15\n13.0\n0.43\n0.15\n13.5\n0.39\n0.25\n42.0\n0.09\n0.22\n12.5\n0.43\n0.05\n11.8\n0.46\nProbing\n0.08\n12.4\n0.44\n0.15\n15.4\n0.35\n0.30\n83.5\n0.03\n0.11\n12.3\n0.43\n0.01\n12.6\n0.42\nSCALPEL\n0.20\n11.2\n0.49\n0.21\n11.2\n0.47\n0.43\n11.2\n0.49\n0.28\n11.1\n0.50\n0.20\n11.1\n0.48\nTable 1: Comparative evaluation across five tasks with each method modifying the top 10 most important components.\nAccD: Accuracy Drop, PPL: Perplexity, Cap: Overall Capability. SCALPEL achieves the best overall balance between\ncapability removal effectiveness and general capability preservation.\n0\n2\n4\n6\n8\n10\nNumber of Corrupted Components\n0.35\n0.40\n0.45\n0.50\n0.55\nAccuracy (Target Task)\nAccuracy\n0\n2\n4\n6\n8\n10\nNumber of Corrupted Components\n11.0\n11.5\n12.0\n12.5\n13.0\n13.5\n14.0\nPerplexity\nPerplexity\n0\n2\n4\n6\n8\n10\nNumber of Corrupted Components\n0.42\n0.44\n0.46\n0.48\n0.50\nOverall Capability\nOverall Capability\nBaseline\nDiffMean\nAttribution Patching\nCausal Tracing\nLogit Lens\nInformation Theory\nIntegrated Gradients\nLRP\nProbing\nSCALPEL\nFigure 2: Multi-dimensional comparison of interpretability methods on the language translation task. The visualization\nshows the relationship between target accuracy degradation, model perplexity, and overall capability preservation\nacross all baseline methods. SCALPEL (highlighted) achieves the optimal balance, positioned in the region of low\nperplexity and high capability retention while achieving the most effective capability removal.\n4.4\nMain Results\nTable 1 presents the comparative evaluation results across five representative tasks.\n(1) SCALPEL consistently\nachieves the highest accuracy drops across all tasks while maintaining near-baseline perplexity and strong overall\ncapability scores, particularly on IOI where baseline methods suffer catastrophic perplexity degradation. This demon-\nstrates that SCALPEL enables targeted removal without disrupting general language circuits. (2) DiffMean and Causal\nTracing show catastrophic perplexity degradation on IOI (orders of magnitude above baseline) while achieving only\nmodest accuracy drops elsewhere. This reveals that activation-based importance identification does not guarantee safe\nintervention. (3) While gradient-based methods like Integrated Gradients and LRP achieve moderate accuracy drops\non individual tasks, they consistently fail to maintain low perplexity, indicating that one-time attribution methods lack\nthe iterative optimization needed for balanced capability removal.\nIn order to visualize the trade-off between capability removal effectiveness and general capability preservation,\nwe plot accuracy degradation, perplexity, and overall capability against corrupted components for all methods. The\nresults are shown in Figure 2. SCALPEL achieves much lower perplexity than other baseline methods while main-\ntaining effective capability removal. Most baseline methods suffer from a fundamental trade-off between removal\neffectiveness and capability preservation, whereas SCALPEL’s gradient-based LoRA optimization with TextReg suc-\ncessfully navigates this trade-off by selectively modifying task-relevant parameters while leaving general language\ncircuits intact.\n7\n"}, {"page": 8, "text": "0\n2\n4\n6\n8\n10\nNumber of Corrupted Components\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nAccuracy (Target Task)\nAccuracy\nBaseline\nFull (all regularizations)\nw/o TextReg\nw/o NormReg\nw/o SparsityReg\n0\n2\n4\n6\n8\n10\nNumber of Corrupted Components\n0.480\n0.485\n0.490\n0.495\n0.500\nOverall Capability\nOverall Capability\nFigure 3: Ablation study comparing SCALPEL configurations on\nlanguage translation task. Left: Accuracy degradation shows tar-\ngeted capability removal effectiveness. Right: Overall capability\npreservation demonstrates the impact of each regularization compo-\nnent on maintaining general language abilities. The full SCALPEL\nmethod (with all regularizations) achieves the optimal balance.\nModel\n∆Acc\n∆PPL\n∆Cap\nDeepSeek-R1\n-0.28\n0.15\n0.01\nGLM-4\n-0.15\n0.25\n0.00\nGemma3\n-0.13\n0.18\n0.00\nLlama-3.2\n-0.36\n0.17\n-0.03\nMinistral\n-0.38\n0.00\n0.00\nQwen3\n-0.10\n0.11\n0.00\nTable 2:\nCross-architecture validation re-\nsults showing delta metrics (SCALPEL -\nBase) across six language models on com-\nmon sense reasoning task. ∆Acc: accuracy\nchange, ∆PPL: perplexity change, ∆Cap:\noverall capability change.\n4.5\nAblation Study on Regularization Components\nTo understand the individual contributions of SCALPEL’s regularization components, we conduct an ablation study on\nthe language translation task by systematically removing TextReg (preserves general language modeling), NormReg\n(constrains LoRA weight magnitude), and SparsityReg (encourages sparse low-rank adaptations). Figure 3 reveals\nthat removing any component slightly reduces capability removal effectiveness (higher accuracy) while degrading\noverall capability preservation. Specifically: (1) Removing TextReg yields higher accuracy (less effective removal)\nbecause the gradient signal focuses solely on capability removal without balancing preservation, leading to suboptimal\nconvergence, and reduces overall capability as the model loses guidance to preserve general language modeling. (2)\nRemoving NormReg yields higher accuracy because unbounded weight magnitudes lead to unstable updates that fail\nto consistently target the capability, and reduces overall capability as excessive modifications interfere with non-target\nabilities. (3) Removing SparsityReg yields higher accuracy because dense adaptations dilute the removal signal across\nmany parameters rather than concentrating on task-critical components, and reduces overall capability as widespread\nmodifications affect unrelated circuits. These results demonstrate that each regularization component contributes to\nboth effective capability removal and preservation of general abilities.\n4.6\nCross-Architecture Generalization\nTo demonstrate that SCALPEL generalizes beyond a single model architecture, we evaluate its effectiveness across\nsix diverse language models with varying sizes and architectural designs: Llama-3.2-1B, Qwen3-4B, Gemma3-2B,\nMinistral-8B, DeepSeek-R1-8B, and GLM-4-9B. We apply SCALPEL to remove common sense reasoning capa-\nbility from each model while preserving general language abilities. Table 2 presents the delta metrics comparing\nSCALPEL-modified models against their base counterparts. (1) All models exhibit negative accuracy changes with\nvarying magnitudes across architectures. This demonstrates SCALPEL’s consistent effectiveness regardless of model\nscale or design. (2) Perplexity changes remain minimal across all models. This confirms that capability removal\ndoes not compromise general language generation. (3) Overall capability changes show near-zero deviations across\nall models. This validates that SCALPEL’s regularization framework transfers to diverse transformer architectures\nwithout architecture-specific tuning.\n4.7\nLoRA Rank Ablation Study\nTo investigate how LoRA rank affects the effectiveness and specificity of capability removal, we conduct a compre-\nhensive rank ablation study across five diverse tasks: language translation, common sense reasoning, indirect object\nidentification (IOI), moral reasoning, and analogical reasoning. We evaluate four different LoRA ranks (1, 2, 4, and\n8) to understand the trade-off between the capacity of low-rank adaptations and the precision of targeted capability\nremoval.\nTable 3 reveals three key findings. (1) Rank 1 achieves the strongest removal on some tasks but shows inconsistent\neffectiveness across different capabilities, as a single rank may find a sufficient subspace for some capabilities while\n8\n"}, {"page": 9, "text": "Rank\nTranslation\nCommon Sense\nIOI\nMoral\nAnalogical\n∆Acc ∆PPL ∆Cap ∆Acc ∆PPL ∆Cap ∆Acc ∆PPL ∆Cap ∆Acc ∆PPL ∆Cap ∆Acc ∆PPL ∆Cap\n1\n-0.20\n0.00\n-0.03 -0.03\n0.22\n-0.02 -0.28\n0.10\n0.00 -0.44\n0.03\n0.00 -0.34\n0.05\n-0.01\n2\n-0.20\n0.02\n-0.02 -0.21\n0.08\n-0.01 -0.43\n-0.05\n-0.01 -0.28\n0.00\n-0.01 -0.20\n-0.07\n0.00\n4\n-0.13\n0.08\n-0.02 -0.03\n0.15\n-0.01 -0.25\n0.02\n0.00 -0.31\n0.01\n0.00 -0.27\n0.06\n-0.01\n8\n-0.15\n-0.07\n-0.01 -0.03\n0.05\n-0.01 -0.25\n0.01\n0.01 -0.28\n0.03\n0.00 -0.20\n-0.04\n-0.02\nTable 3: LoRA rank ablation study showing delta metrics (SCALPEL - Base) across five diverse tasks. Negative ∆Acc\nvalues indicate successful capability reduction. Positive ∆PPL values indicate perplexity increase. ∆Cap values near\nzero demonstrate preservation of general language abilities. Rank 2 demonstrates the optimal balance with consistent\ncapability removal and minimal perplexity degradation across all tasks, while Rank 8 achieves the unique property of\nimproving language quality (negative ∆PPL) despite capability removal.\n4\n5\n6\n7\n8\n9\n10\n11\nAverage Top 50% Layer Index\nInstruction Following\nCreative Thinking\nClassification Categorization\nAnalogical Reasoning\nWorld Knowledge\nMoral Reasoning\nEmotional Understanding\nLanguage Translation\nCommon Sense Reasoning\nSpatial Reasoning\nLogical Reasoning\nCausal Reasoning\nSummarization\nTemporal Reasoning\nCounterfactual Reasoning\nDialogue\nMetacognition\nMemory Context\nLanguage Generation\nReading Comprehension\nCritical Thinking\nMultimodal Understanding\nLanguage Understanding\nMathematical Computation\nCapability Tasks\nTop 50% Layers Average Distribution by Capability Task\nKnowledge & Processing\nLanguage & Communication\nSpecialized Domains\nReasoning & Analysis\nCognitive & Perceptual\n4\n5\n6\n7\n8\n9\n10\n11\nAverage Top 50% Layer Index\nPrinciple A Reconstruction\nLeft Branch Island Simple Question\nDeterminer Noun Agreement 1\nAdjunct Island\nOnly Npi Licensor Present\nOnly Npi Scope\nPrinciple A C Command\nDeterminer Noun Agreement With Adj 2\nCoordinate Structure Constraint Complex Left Branch\nRegular Plural Subject Verb Agreement 2\nComplex Np Island\nDistractor Agreement Relational Noun\nDeterminer Noun Agreement With Adjective 1\nDeterminer Noun Agreement Irregular 1\nAnimate Subject Trans\nCoordinate Structure Constraint Object Extraction\nPrinciple A Domain 2\nSuperlative Quantifiers 2\nLeft Branch Island Echo Question\nRegular Plural Subject Verb Agreement 1\nPrinciple A Domain 3\nIrregular Plural Subject Verb Agreement 1\nDeterminer Noun Agreement With Adj Irregular 1\nDeterminer Noun Agreement Irregular 2\nExistential There Object Raising\nIrregular Past Participle Verbs\nDeterminer Noun Agreement 2\nIrregular Plural Subject Verb Agreement 2\nDeterminer Noun Agreement With Adj Irregular 2\nAnaphor Number Agreement\nExistential There Quantifiers 1\nEllipsis N Bar 2\nEllipsis N Bar 1\nInchoative\nPrinciple A Case 2\nAnaphor Gender Agreement\nDistractor Agreement Relative Clause\nMatrix Question Npi Licensor Present\nTransitive\nAnimate Subject Passive\nCausative\nPassive 2\nWh Vs That No Gap\nIntransitive\nSentential Negation Npi Scope\nTough Vs Raising 2\nPrinciple A Case 1\nIrregular Past Participle Adjectives\nWh Questions Object Gap\nPassive 1\nDrop Argument\nSentential Negation Npi Licensor Present\nNpi Present 1\nExpletive It Object Raising\nWh Vs That With Gap\nWh Vs That With Gap Long Distance\nWh Vs That No Gap Long Distance\nTough Vs Raising 1\nExistential There Quantifiers 2\nWh Island\nPrinciple A Domain 1\nSentential Subject Island\nWh Questions Subject Gap Long Distance\nNpi Present 2\nExistential There Subject Raising\nSuperlative Quantifiers 1\nWh Questions Subject Gap\nBLiMP Tasks\nTop 50% Layers Average Distribution by BLiMP Task\nMorphology\nSemantics\nSyntax\nSyntax-Semantics\nFigure 4: Peak layer analysis for capability tasks (left) and BLiMP tasks (right). Capability tasks show a progression\nfrom basic language tasks in early layers to complex reasoning in middle layers and creative tasks in late layers.\nBLiMP tasks reveal morphological processing in early layers, syntactic processing in later layers, and semantic tasks\ndistributed throughout.\nbeing insufficient for others. (2) Rank 2 provides the most stable performance with effective capability removal across\nall tasks, suggesting that a two-dimensional subspace is sufficient to disable most capabilities (though not necessarily\nthe unique or minimal causal representation). (3) Higher ranks generally show reduced removal effectiveness, support-\ning our hypothesis that capabilities occupy low-dimensional subspaces and can be effectively captured with minimal\nrank.\n4.8\nLayer-wise Capability Analysis\nTo understand how different capabilities are distributed across transformer layers, we analyze peak layer distributions\nacross 24 capability tasks and 67 BLiMP linguistic tasks. Figure 4 (left) reveals three key patterns for capability tasks:\n(1) Basic language tasks peak in early-to-middle layers, reflecting reliance on fundamental linguistic processing. (2)\nComplex reasoning tasks concentrate in middle-to-late layers, suggesting higher-order cognitive functions require\ndeeper semantic representations. (3) Creative and generative tasks show the latest peaks, indicating dependence on the\nmost sophisticated abstractions in deepest layers.\nFigure 4 (right) presents complementary patterns for BLiMP linguistic tasks: (1) Morphological tasks peak in\nthe earliest layers, indicating that surface-level morphological features are processed at the initial stages of the trans-\nformer hierarchy. (2) Syntactic tasks concentrate in later layers, suggesting that structural grammatical relationships\nrequire deeper representations built upon morphological features. (3) Semantic and syntax-semantics interface tasks\nexhibit distributed peaks across all layers, indicating that abstract meaning composition is processed throughout the\nentire transformer hierarchy. These layer-wise distributions align with cognitive and linguistic theories of hierarchical\nlanguage processing, validating SCALPEL’s ability to reveal fine-grained capability organization within transformer\narchitectures.\n9\n"}, {"page": 10, "text": "1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nAnalogical Reasoning\nCausal Reasoning\nClassification Categorization\nCommon Sense Reasoning\nCounterfactual Reasoning\nCreative Thinking\nCritical Thinking\nDialogue\nEmotional Understanding\nInstruction Following\nLanguage Generation\nLanguage Translation\nLanguage Understanding\nLogical Reasoning\nMathematical Computation\nMemory Context\nMetacognition\nMoral Reasoning\nMultimodal Understanding\nReading Comprehension\nSpatial Reasoning\nSummarization\nTemporal Reasoning\nWorld Knowledge\nMDS Pearson\nTask Categories\nLanguage & Communication\nReasoning & Analysis\nKnowledge & Processing\nCognitive & Perceptual\nSpecialized Domains\n0.4\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nSentential Negation Npi Scope\nOnly Npi Scope\nTough Vs Raising 2\nTough Vs Raising 1\nExpletive It Object Raising\nExistential There Subject Raising\nExistential There Object Raising\nPrinciple A Reconstruction\nPrinciple A Domain 3\nPrinciple A Domain 2\nPrinciple A Domain 1\nPrinciple A Case 2\nPrinciple A Case 1\nPrinciple A C Command\nAnimate Subject Trans\nAnimate Subject Passive\nWh Island\nSentential Subject Island\nLeft Branch Island Simple Question\nLeft Branch Island Echo Question\nCoordinate Structure Constraint Object Extraction\nCoordinate Structure Constraint Complex Left Branch\nComplex Np Island\nAdjunct Island\nWh Vs That With Gap\nWh Vs That With Gap Long Distance\nWh Vs That No Gap\nWh Vs That No Gap Long Distance\nWh Questions Subject Gap\nWh Questions Subject Gap Long Distance\nWh Questions Object Gap\nEllipsis N Bar 2\nEllipsis N Bar 1\nTransitive\nPassive 2\nPassive 1\nIntransitive\nInchoative\nDrop Argument\nCausative\nSuperlative Quantifiers 2\nSuperlative Quantifiers 1\nExistential There Quantifiers 2\nExistential There Quantifiers 1\nSentential Negation Npi Licensor Present\nOnly Npi Licensor Present\nNpi Present 2\nNpi Present 1\nMatrix Question Npi Licensor Present\nRegular Plural Subject Verb Agreement 2\nRegular Plural Subject Verb Agreement 1\nIrregular Plural Subject Verb Agreement 2\nIrregular Plural Subject Verb Agreement 1\nDistractor Agreement Relative Clause\nDistractor Agreement Relational Noun\nIrregular Past Participle Verbs\nIrregular Past Participle Adjectives\nDeterminer Noun Agreement With Adjective 1\nDeterminer Noun Agreement With Adj Irregular 2\nDeterminer Noun Agreement With Adj Irregular 1\nDeterminer Noun Agreement With Adj 2\nDeterminer Noun Agreement Irregular 2\nDeterminer Noun Agreement Irregular 1\nDeterminer Noun Agreement 2\nDeterminer Noun Agreement 1\nAnaphor Number Agreement\nAnaphor Gender Agreement\nMDS Pearson\nTask Categories\nSyntax-Semantics\nSyntax\nSemantics\nMorphology\nFigure 5: Dimensionality reduction visualization of task similarity in LoRA weight space. Left: Capability tasks\nshowing clustering patterns among reasoning, knowledge, and linguistic domains. Right: BLiMP linguistic tasks\n(67 fine-grained linguistic phenomena) revealing structural relationships among syntax, semantics, morphology, and\nsyntax-semantics interfaces.\n4.9\nTask Similarity Analysis\nWe investigate whether SCALPEL training reveals meaningful task relationships by analyzing LoRA weight similarity\npatterns. If capabilities are indeed represented as low-rank subspaces distributed across the model, we would expect\nrelated capabilities to occupy similar regions in parameter space. Figure 5 applies Multidimensional Scaling (MDS)\nanalysis using Pearson correlation, revealing two key findings. (1) Tasks within the same cognitive category exhibit\nstrong clustering behavior, with Language & Communication capabilities forming coherent clusters distinct from\nReasoning & Analysis functions. This demonstrates that SCALPEL captures cognitively meaningful relationships,\nwith different capabilities corresponding to different subspaces as predicted by our framework. (2) Fine-grained\nlinguistic analysis across 67 BLiMP tasks shows that tasks within the same grammatical category cluster together in\nparameter space. This reveals that SCALPEL captures hierarchical relationships within specific domains, providing\nevidence that the low-rank representation perspective successfully disentangles capability encoding.\n5\nConclusion\nWe presented SCALPEL, a framework for selective capability ablation in large language models through low-rank\nparameter editing. Unlike traditional interpretability methods that assume capabilities are controlled by specific com-\nponents, SCALPEL represents capabilities as low-rank parameter subspaces distributed across layers and modules,\nnaturally handling both polysemanticity and distributed encoding. By training LoRA adapters to reduce the model’s\nability to distinguish correct from incorrect answers while preserving general language modeling quality, SCALPEL\nidentifies the low-rank representation responsible for specific capabilities while remaining disentangled from other\ncapabilities.\nOur experiments across diverse capability tasks and linguistic tasks from BLiMP validate the three contributions\noutlined in the introduction: (1) Low-rank modifications are sufficient for effective capability ablation across the stud-\nied tasks and multiple model architectures; (2) SCALPEL achieves targeted capability removal with significantly less\ncollateral damage compared to existing methods, maintaining near-baseline perplexity while reducing target task ac-\ncuracy; (3) The learned LoRA weight patterns reveal that different capabilities exhibit distinct layer-wise distributions\nthat align with cognitive and linguistic theories, with morphological processing in early layers, syntactic processing\nin middle layers, and complex reasoning in deeper layers. These findings offer a more nuanced understanding of\ncapability encoding in large language models and open new directions for interpretability research.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics, pages 4190–4197. Association for Computational\n10\n"}, {"page": 11, "text": "Linguistics, 2020.\n[2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. Technical report, Anthropic, 2024.\n[3] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Woj-\nciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.\nPLoS ONE, 10(7):e0130140, 2015.\n[4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. In Computer Vision and Pattern Recognition (CVPR), 2017.\n[5] Yonatan Belinkov.\nProbing classifiers: Promises, shortcomings, and advances.\nComputational Linguistics,\n48(1):207–219, 2022.\n[6] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem\nAnil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with\ndictionary learning. Transformer Circuits Thread, 2023.\n[7] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang\nWang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on\nevaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2024.\n[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,\nHeidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea\nPower, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,\nSam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code.\narXiv preprint arXiv:2107.03374, 2021.\n[9] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an anal-\nysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 276–286. Association for Computational Linguistics, 2019.\n[10] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models. In Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2024.\n[11] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained\ntransformers.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8493–8502. Association for Computational Linguistics, 2022.\n[12] Abhimanyu Dubey, Aaron Grattafiori, Abhinav Jauhri, et al.\nThe llama 3 herd of models.\narXiv preprint\narXiv:2407.21783, 2024.\n[13] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac\nHatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario\nAmodei, Martin Wattenberg, and Chris Olah. Toy models of superposition. arXiv preprint arXiv:2209.10652,\n2022.\n[14] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny\nHernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,\nJared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. 2021.\n11\n"}, {"page": 12, "text": "[15] Google Gemini Team.\nGemini:\nA family of highly capable multimodal models.\narXiv preprint\narXiv:2312.11805, 2023.\n[16] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npages 5484–5495. Association for Computational Linguistics, 2021.\n[17] Geoffrey E. Hinton, James L. McClelland, and David E. Rumelhart. Distributed representations. In David E.\nRumelhart and James L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstruc-\nture of Cognition, Volume 1: Foundations, pages 77–109. MIT Press, Cambridge, MA, 1986.\n[18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[19] Gabriel Ilharco, Marco Túlio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali\nFarhadi. Editing models with task arithmetic. In The Eleventh International Conference on Learning Repre-\nsentations (ICLR). OpenReview.net, 2023.\n[20] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres.\nInterpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In Pro-\nceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine\nLearning Research, pages 2668–2677. PMLR, 2018.\n[21] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath,\nDharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of\nthe National Academy of Sciences, 114(13):3521–3526, 2017.\n[22] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Proceedings of\nthe 34th International Conference on Machine Learning, volume 70 of PMLR, pages 1885–1894, 2017.\n[23] János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. Atp*: An efficient and scalable method for local-\nizing llm behaviour to components. arXiv preprint arXiv:2403.00745, 2024.\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations (ICLR), 2019.\n[25] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural\nInformation Processing Systems 30, pages 4765–4774. Curran Associates, Inc., 2017.\n[26] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in\ngpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.\n[27] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a\ntransformer. arXiv preprint arXiv:2210.07229, 2023.\n[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n[29] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Advances in Neural\nInformation Processing Systems, volume 32, 2019.\n[30] Neel Nanda. Attribution patching: Activation patching at industrial scale. https://www.neelnanda.io/\nmechanistic-interpretability/attribution-patching, 2023. Blog post.\n[31] nostalgebraist.\ninterpreting\ngpt:\nthe\nlogit\nlens.\nhttps://www.lesswrong.com/posts/\nAcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens, 2020. LessWrong blog post.\n[32] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11), 2017.\n[33] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n12\n"}, {"page": 13, "text": "[34] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should I trust you?\": Explaining the predictions\nof any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, pages 1135–1144, 2016.\n[35] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner. Steering llama\n2 via contrastive activation addition. In Proceedings of the 62nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages 15504–15522, Bangkok, Thailand, 2024. Association for\nComputational Linguistics.\n[36] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842–866, 2020.\n[37] Adam Scherlis, Kshitij Sachan, Adam S. Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and capacity\nin neural networks. arXiv preprint arXiv:2210.01892, 2022.\n[38] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv\nBatra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV), pages 618–626, 2017.\n[39] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka,\nWei Zhan, and Mingyu Ding. Languagempc: Large language models as decision makers for autonomous driving.\narXiv preprint arXiv:2310.03026, 2023.\n[40] Ruihao Shui, Yixin Cao, Xiang Wang, and Tat-Seng Chua. A comprehensive evaluation of large language models\non legal judgment prediction. In Findings of the Association for Computational Linguistics: EMNLP 2023, 2023.\n[41] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly,\nNathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S.\nCorrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral,\nChristopher Semturs, Alan Karthikesalingam, and Vivek Natarajan.\nLarge language models encode clinical\nknowledge. arXiv preprint arXiv:2212.13138, 2022.\n[42] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the\n34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,\npages 3319–3328. PMLR, 2017.\n[43] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society:\nSeries B (Methodological), 58(1):267–288, 1996.\n[44] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. arXiv preprint\narXiv:1503.02406, 2015.\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, vol-\nume 30, 2017.\n[46] Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for\nComputational Linguistics, 8:377–392, 2020.\n[47] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A\ntheoretical justification for adaptivity. arXiv preprint arXiv:1905.11881, 2020.\n[48] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\nJunjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\nmodels. arXiv preprint arXiv:2303.18223, 2023.\n13\n"}, {"page": 14, "text": "[49] Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T.N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan.\nLarge language models for scientific synthesis, inference and explanation. arXiv preprint arXiv:2310.07984,\n2023.\n14\n"}, {"page": 15, "text": "A\nComprehensive Capability Decomposition Results\nWe evaluate our SCALPEL training approach across 24 diverse language tasks using the Llama-3.2-1B-Instruct model\nwith TextReg regularization to assess both the effectiveness of capability removal and the preservation of general lan-\nguage abilities. We measure target task performance degradation, overall model perplexity changes, and broad capa-\nbility retention to evaluate the specificity and safety of our approach. The experimental results demonstrate three key\nfindings: (1) Our method successfully reduces performance across almost all target domains, with only one task (Spa-\ntial Reasoning) showing a slight accuracy increase. (2) While overall model perplexity increases following capability\nremoval, most tasks show moderate increases, though certain generation-heavy tasks exhibit larger perplexity changes.\n(3) Although overall capability scores show some decline, the reduction is minimal compared to the substantial degra-\ndation observed in target tasks, demonstrating our model’s strong specificity in removing targeted capabilities while\npreserving the majority of other linguistic and reasoning abilities. (4) Some results suggest possible over-removal for\nglobal generative tasks (e.g., Dialogue, Language Generation). This is expected because generation-heavy capabilities\nrely on broad, shared circuitry and are inherently harder to isolate than localized capabilities.\nTask\nAccuracy\nPerplexity\nOverall Capability\nBase\nOurs\nBase\nOurs\nBase\nOurs\nAnalogical Reasoning\n79.5%\n75.0% (-4.5%) 11.12\n11.41 (+0.30) 0.497\n0.507 (+0.010)\nCausal Reasoning\n80.0%\n70.0% (-10.0%) 11.12\n11.49 (+0.37) 0.497\n0.489 (-0.009)\nClassification & Categorization 76.9%\n74.4% (-2.6%) 11.12\n11.30 (+0.18) 0.497\n0.434 (-0.063)\nCommon Sense Reasoning\n94.9%\n56.4% (-38.5%) 11.12\n12.42 (+1.30) 0.497\n0.379 (-0.118)\nCounterfactual Reasoning\n26.0%\n16.0% (-10.0%) 11.12\n11.22 (+0.10) 0.497\n0.510 (+0.013)\nCreative Thinking\n30.0%\n27.5% (-2.5%) 11.12\n12.01 (+0.89) 0.497\n0.466 (-0.031)\nCritical Thinking\n46.0%\n26.0% (-20.0%) 11.12\n12.43 (+1.31) 0.497\n0.511 (+0.014)\nDialogue\n82.5%\n0.0% (-82.5%) 11.12\n13.56 (+2.44) 0.497\n0.413 (-0.084)\nEmotional Understanding\n61.3%\n48.4% (-12.9%) 11.12\n11.20 (+0.08) 0.497\n0.480 (-0.017)\nInstruction Following\n66.7%\n54.5% (-12.1%) 11.12\n11.46 (+0.34) 0.497\n0.528 (+0.031)\nLanguage Generation\n100.0%\n77.8% (-22.2%) 11.12\n50.60 (+39.48) 0.497\n0.415 (-0.083)\nLanguage Translation\n82.5%\n57.5% (-25.0%) 11.12\n11.05 (-0.07) 0.497\n0.513 (+0.016)\nLanguage Understanding\n100.0%\n96.7% (-3.3%) 11.12\n13.55 (+2.43) 0.497\n0.477 (-0.020)\nLogical Reasoning\n93.9%\n51.5% (-42.4%) 11.12\n11.29 (+0.17) 0.497\n0.460 (-0.037)\nMathematical Computation\n100.0%\n0.0% (-100.0%) 11.12\n11.11 (-0.01) 0.497\n0.516 (+0.019)\nMemory & Context\n75.0%\n41.7% (-33.3%) 11.12\n18.01 (+6.89) 0.497\n0.422 (-0.075)\nMetacognition\n19.4%\n2.8% (-16.7%) 11.12\n11.72 (+0.60) 0.497\n0.517 (+0.020)\nMoral Reasoning\n94.4%\n19.4% (-75.0%) 11.12\n11.21 (+0.09) 0.497\n0.480 (-0.017)\nMultimodal Understanding\n12.2%\n0.0% (-12.2%) 11.12\n12.83 (+1.71) 0.497\n0.454 (-0.043)\nReading Comprehension\n62.1%\n24.1% (-37.9%) 11.12\n17.18 (+6.06) 0.497\n0.426 (-0.071)\nSpatial Reasoning\n72.5%\n75.0% (+2.5%) 11.12\n11.04 (-0.08) 0.497\n0.519 (+0.021)\nSummarization\n23.5%\n0.0% (-23.5%) 11.12\n11.74 (+0.62) 0.497\n0.483 (-0.014)\nTemporal Reasoning\n65.0%\n37.5% (-27.5%) 11.12\n11.36 (+0.24) 0.497\n0.486 (-0.011)\nWorld Knowledge\n76.3%\n65.8% (-10.5%) 11.12\n12.36 (+1.24) 0.497\n0.454 (-0.043)\nOverall\n67.5%\n41.6% (-25.9%) 11.12\n13.90 (+2.78) 0.497\n0.472 (-0.025)\nTable 4: Comprehensive evaluation results showing the impact of SCALPEL training across 24 capability domains.\nThe table compares baseline performance (Base) with our results (Ours), where the delta change is shown in paren-\ntheses. Green values indicate improvements while red values show degradation. The results demonstrate selective\ncapability removal with varying degrees of impact across different domains.\nB\nDataset Examples\nWe provide representative examples from our three dataset categories. All examples were initially generated by Claude and manu-\nally filtered to remove obviously improper samples.\nTable 5 shows capability tasks using token-level format with prompt-correct-wrong triplets. These tasks test specific cogni-\ntive abilities where the model must predict a single correct token. Table 6 presents linguistic tasks using A/B format comparing\ngrammatically correct vs. incorrect sentences, drawn from BLiMP to evaluate fine-grained grammatical knowledge. Table 7 illus-\n15\n"}, {"page": 16, "text": "Prompt\nCorrect\nWrong\nCommon Sense Reasoning\nWhat do you wear on your feet? Answer:\nshoes\ngloves\nWhat do bees make? Answer:\nhoney\nmilk\nWhere do fish live? They live in\nwater\nair\nLanguage Translation\nTranslate ’cat’ to French. The answer is\nchat\nchien\nTranslate ’water’ to French. The word is\neau\nfeu\nWhat is ’hello’ in French? Answer:\nbonjour\nbonsoir\nIndirect Object Identification (IOI)\nWhen Alice and Bob went to the store, Alice gave a book to\nBob\nAlice\nAfter Eve and Frank arrived, Eve passed the letter to\nFrank\nEve\nWhen Grace and Henry met at the cafe, Grace sent the package to\nHenry\nGrace\nTable 5: Examples of capability task datasets with prompt-correct-wrong format.\nCorrect Sentence (A)\nWrong Sentence (B)\nMorphology - Subject-Verb Agreement\nA niece of most senators hasn’t descended most slopes.\nA niece of most senators haven’t descended most slopes.\nThe sketch of those trucks hasn’t hurt Alan.\nThe sketch of those trucks haven’t hurt Alan.\nA newspaper article about the Borgias has disagreed with\nMarcus.\nA newspaper article about the Borgias have disagreed with\nMarcus.\nTable 6: Examples of linguistic task datasets with A/B comparison format.\ntrates general evaluation tasks from our held-out set, which test diverse capabilities to ensure capability removal does not cause\ncatastrophic forgetting.\nC\nCase Study\nTable 8 presents representative examples across multiple tasks showing how SCALPEL reduces the model’s ability to distinguish\ncorrect from incorrect answers. For each example, we show the probability difference (pcorrect −pwrong) before and after applying\nSCALPEL. The results reveal two key findings: (1) SCALPEL consistently reduces the probability difference to near zero across\nall tasks, demonstrating effective capability removal. (2) In several IOI cases, SCALPEL actually flips the prediction to prefer the\nwrong answer (negative values), indicating that the model has lost the ability to perform indirect object identification.\n16\n"}, {"page": 17, "text": "Prompt\nCorrect\nWrong\nAnalogical Reasoning\nCat is to kitten as dog is to\npuppy\ncat\nHot is to cold as up is to\ndown\nleft\nBird is to fly as fish is to\nswim\nwalk\nMoral Reasoning\nIs it right to help others? Answer:\nyes\nno\nIs it wrong to steal? Answer:\nyes\nno\nShould you keep promises? Answer:\nyes\nno\nLogical Reasoning\nIf all birds can fly and a robin is a bird, can a robin fly? Answer:\nyes\nno\nComplete the pattern: 2, 4, 6, 8, __. Answer:\n10\n9\nIf it’s raining, the ground is wet. It’s raining. Is the ground wet? Answer:\nyes\nno\nTable 7: Examples of general evaluation tasks (held-out test set).\nTask\nPrompt (correct/wrong)\nBase\nSCALPEL\nIOI\nRoderick handed a book to... (Scar-\nlett/Roderick)\n0.703\n-0.019\nIOI\nDiego gave the gift to...\n(Es-\ntelle/Diego)\n0.312\n-0.224\nTranslation\nTranslate\n‘sun’\nto\nGerman\n(Sonne/Mond)\n0.923\n0.000\nTranslation\nTranslate ‘red’ to Spanish (rojo/azul)\n0.775\n0.000\nAnalogical\nEye is to see as ear is to (hear/speak)\n0.566\n0.000\nAnalogical\nHelmet is to head as armor is to\n(body/protection)\n0.376\n0.001\nMoral\nIs filthiness good? (no/yes)\n0.024\n0.001\nMoral\nShould you stand up for justice?\n(yes/no)\n0.014\n0.000\nTable 8: Case study across multiple tasks. Values show probability difference pcorrect −pwrong. SCALPEL substantially\nreduces the probability difference in all cases. Negative values (bold) indicate the model prefers the wrong answer\nafter capability removal.\n17\n"}]}