{"doc_id": "arxiv:2511.19858", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.19858.pdf", "meta": {"doc_id": "arxiv:2511.19858", "source": "arxiv", "arxiv_id": "2511.19858", "title": "A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction", "authors": ["Farzad Ahmed", "Joniel Augustine Jerome", "Meliha Yetisgen", "Özlem Uzuner"], "published": "2025-11-25T02:40:49Z", "updated": "2025-11-26T09:29:37Z", "summary": "Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.   Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.   Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.   Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.19858v2", "url_pdf": "https://arxiv.org/pdf/2511.19858.pdf", "meta_path": "data/raw/arxiv/meta/2511.19858.json", "sha256": "bd82deb36680b835aad39657cf605ba73c97f299e401a3edfb1d0e862cb2d9e9", "status": "ok", "fetched_at": "2026-02-18T02:26:24.547484+00:00"}, "pages": [{"page": 1, "text": "A Systematic Analysis of Large Language Models with RAG-enabled\nDynamic Prompting for Medical Error Detection and Correction\nFarzad Ahmed1, Joniel Augustine Jerome1, Meliha Yetisgen2, Özlem Uzuner1\n1George Mason University, 2University of Washington\nAbstract\nObjective: Clinical documentation is prone to factual, diagnostic, and management errors that\ncan compromise patient safety. Large language models (LLMs) offer potential for automatic error\ndetection and correction, but their behavior under different prompting strategies remains poorly\nunderstood. This study systematically analyzes the strengths and weaknesses of zero-shot prompt-\ning, static prompting with random exemplars (SPR), and retrieval-augmented generation (RAG)-\nenabled dynamic prompting (RDP), to address medical error detection and correction through three\nsubtasks: error flag detection, error sentence detection, and error correction.\nMethods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs—including\nGPT, Claude, Gemini and OpenAI’s o-series variants—across the three subtasks. This diverse set\nof models, spanning compact and frontier-scale architectures, allowed us to assess how model size,\narchitecture, and training strategies influence performance in medical error detection and correction.\nPerformance on error flag and sentence detection was measured with Accuracy and Recall, False-\nPositive Rate (FPR) measured performance on error flag detection, and Aggregate Score (AggScore)\nof ROUGE-1, BLEURT, and BERTScore measured performance on error correction. We analyzed\nLLM outputs, examining representative cases, failure patterns, and LLM–clinician differences to\nexplain quantitative trends.\nResults: Zero-shot prompting exhibited low recall in error flag detection and error sentence\ndetection, often missing atypical or abbreviation-heavy errors. SPR improved recall in both detec-\ntion tasks but increased FPR. Across all nine LLMs, these trends remained consistent—RDP that\nretrieves semantically relevant exemplars during inference, reduced FPR by about 15% in error flag\ndetection, increased recall by 5-10% in error sentence detection, and improved the contextual and\nsemantic accuracy of error correction.\nConclusion: Our systematic analysis of nine LLMs with three prompting strategies highlights\nlimitations of zero-shot and SPR for medical error detection and correction. RDP outperforms zero-\nshot and SPR across all LLMs, grounding predictions in relevant exemplars for effective medical\nerror detection and correction.\nKeywords:\nmedical error detection, clinical NLP, prompting strategies, Retrieval-Augmented\nGeneration, large language models\n1. Introduction\nClinical documentation is a cornerstone of patient care but can contain medical errors that can\ncompromise safety, delay treatment, and propagate across systems. These errors can arise from\narXiv:2511.19858v2  [cs.CL]  26 Nov 2025\n"}, {"page": 2, "text": "diagnostic inaccuracies, transcription inconsistencies, or management decisions, and they remain a\nleading contributor to preventable adverse events in healthcare [1–3]. Manual review of clinical notes\nis resource-intensive and infeasible at scale, motivating the development of automated methods for\nreliable error detection and correction.\nNatural Language Processing (NLP) has played a central role in structuring and analyzing\nunstructured clinical text, improving efficiency in clinical reasoning, information retrieval, and\npatient outcome prediction [2–4]. The emergence of large language models (LLMs) has acceler-\nated progress in biomedical NLP, combining contextual understanding with reasoning capabilities.\nDomain-specific models such as BioBERT [5] and PubMedGPT [6], alongside general-purpose ar-\nchitectures like PaLM [7] and Med-PaLM [8], have demonstrated strong performance on benchmark\nbiomedical tasks. However, their ability to identify and correct medical errors remains underex-\nplored.\nThe MEDIQA-CORR shared task [9] and the MEDEC dataset [10] offer a standardized bench-\nmark for evaluating automatic medical error detection and correction.\nMEDEC defines three\nsubtasks: (1) Error flag detection—determining whether a clinical note contains an error; (2)\nError sentence detection—identifying the sentence containing erroneous information; and (3) Er-\nror correction—producing a corrected version of that sentence.\nThe dataset includes five error\ntypes—diagnosis, causal organism, management, treatment, and pharmacotherapy.\nWhile biomedical LLMs have achieved promising results [8, 11, 12], most prior systems rely\non static prompting [10, 13], i.e., prompts with randomly selected exemplars that are applied\nto all inputs, or classification-only architectures, limiting generalization across institutions and\ndocumentation styles. Furthermore, these systems tend to show low recall for atypical errors, high\nfalse positive rates (FPR) due to exemplar mismatch, and difficulties handling abbreviations and\nshorthand [11–13].\nRetrieval-Augmented Generation (RAG) [14] dynamically retrieves semantically relevant clinical\nexemplars tailored to each input during inference, allowing for dynamic prompting that can mitigate\nerrors due to exemplar mismatch. The retrieval is dynamic because the exemplars in the prompt\nare selected for each input sample specifically and change from input sample to input sample. We\nrefer to the resulting approach as RAG-enabled dynamic prompting (RDP).\nContributions. In this paper:\n• We introduce RDP, enabling exemplar selection based on semantic similarity to each input\nsample rather than random sampling.\n• We conduct the first systematic analysis of zero-shot, static prompting with random exemplars\n(SPR), and RDP for medical error detection and correction using the MEDEC benchmark.\n• We evaluate nine instruction-tuned LLMs—including GPT-4o, GPT-4o-mini, GPT-4.1, GPT-\n4.1-mini, GPT-5, o1-mini, o4-mini, Claude 3.5 Sonnet, and Gemini 2.0 Flash—across all three\nMEDIQA-CORR subtasks using three prompting strategies.\n• We systematically analyze how model size, architecture, and prompting strategies impact\nperformance in medical error detection and correction across nine LLMs.\n• We complement quantitative evaluation with a qualitative analysis of representative model\noutputs, identifying error patterns, failure modes, and clinician–model discrepancies to explain\nobserved trends.\n2\n"}, {"page": 3, "text": "Together, these analyses provide a comprehensive view of how RDP mitigates the weaknesses\nof SPR, enhancing recall, reducing FPR, and improving LLM-based error detection and correction.\nSpecifically, we find that RDP (1) improves recall in error sentence detection by retrieving semanti-\ncally aligned exemplars that better match each clinical input; (2) reduces FPR in error flag detection\nby exposing models to correct, in-domain examples that discourage unnecessary corrections; and\n(3) enhances handling of abbreviations and shorthand by retrieving cases with similar linguistic\npatterns, enabling more accurate interpretation of clinical shorthand and numeric expressions.\nStatement of Significance\nProblem\nClinical documentation can contain medical errors that, if undetected, can compromise\npatient safety. Manual review is resource-intensive, highlighting the need for automatic\nsystems that can flag errors, identify the erroneous sentences, and generate corrected\nversions.\nWhat is\nAlready\nKnown\nLarge language models (LLMs) have shown promise in biomedical NLP but have had lim-\nited success in error flag detection, error sentence detection, and error correction: they\ntend to achieve low recall for atypical errors, over-generate unnecessary corrections for\nalready correct sentences, and face difficulties handling abbreviations and shorthand no-\ntations. Most prior work evaluates classification or detection of medical errors in isolation,\nwith limited exploration of RDP for error detection and correction.\nWhat\nThis\nPaper\nAdds\nThis paper systematically evaluates SPR and RDP strategies for medical error flag detec-\ntion, error sentence detection, and error correction. It demonstrates that RDP improves\n(1) recall of true error sentence detection, (2) reduces FPR in error flag detection, (3)\nenhances interpretation of abbreviations and shorthand, and (4) improves contextual and\nsemantic accuracy of error correction. RDP grounds LLM predictions in clinically relevant\nexamples, achieving state-of-the-art performance across all three subtasks.\nWho\nWould\nBenefit\nAutomatic error detection and correction can support clinicians, clinical researchers, in-\nformatics practitioners, as well as patients. Our approach can lead the way in integrating\ndynamic exemplars in prompting strategies, to improve the relevance of exemplars to the\ninput and ultimately improve performance.\n2. Related Work\nPrior work on medical error detection and correction can be grouped into: benchmarks and\ndatasets, shared task systems, and computational strategies.\nBenchmarks and Datasets\nMEDEC corpus [10] is designed to evaluate LLMs on medical error detection and correc-\ntion. MEDEC provides thousands of annotated clinical texts covering multiple error types, in-\ncluding diagnosis errors, causal organism, management, treatment, and pharmacotherapy errors.\nWhile MEDEC is the primary dataset for this study (see Section 3.1), several other resources\nhave contributed to the development of evaluation standards for medically inaccurate informa-\ntion. For example, the Spanish Real-Word Error Corpus [15] and the MEDIC dataset of\nmedication directions [16] address medical error correction in multilingual and pharmacy settings.\nMisinformation-oriented datasets such as COVID-Lies [17], ReCOVery [18], and CoAID [19]\nfocus on health misinformation in social media, while fact-checking benchmarks like SciFact [20],\n3\n"}, {"page": 4, "text": "HealthVer [21], PubHealth [22], and BEAR-FACT [23] provide evidence-based claim verifica-\ntion. Recent efforts also target model hallucination in medical reasoning using evaluation suites\nsuch as Med-HALT [24]. Together, these datasets reflect a growing ecosystem of benchmarks that\nsupport research on medical error detection, misinformation mitigation, and fact checking.\nShared Task Systems\nThe MEDIQA-CORR 2024 shared task [25] divided medical error detection and correction\ninto three subtasks: error flag detection, error sentence detection, and error correction. The official\nMEDEC benchmark evaluation [10] showed that instruction-tuned LLMs such as Claude 3.5\nand GPT-4o reached an accuracy of 0.70 in error flag detection and 0.66 in error sentence detection,\nwhile medical doctors continue to outperform models on error correction, with the best medical\ndoctor result of 0.7742 compared to the best model result of 0.7043. Interpretable strategies to\nthe task included PromptMind, which combined chain-of-thought prompting with an ensemble\nof LLMs [13], and HSE NLP which integrated biomedical entity recognition with MeSH-based\ngraph reasoning [26]. PromptMind reached an accuracy of 0.6216 in error flag detection, 0.6086\nin error sentence detection, and an AggScore of 0.7866 in error correction (ROUGE-1 = 0.8070,\nBLEURT = 0.7470). HSE NLP obtained 0.5222 and 0.5200 in error flag and sentence detection,\nrespectively, with an AggScore in error correction of 0.7806 (ROUGE-1 = 0.7795, BLEURT =\n0.7564). Nonetheless, current results show that accurate error detection and correction remains an\nopen challenge.\nComputational Strategies\nRecent work has examined how computational paradigms influence clinical reasoning perfor-\nmance. Cai et al. [27] contrasted train-time computation models, which concentrate reasoning\nability in extensive pre-training and require relatively little computation at inference (e.g., GPT-4),\nwith test-time computation models, which allocate substantial computation during inference\nthrough extended reasoning traces or search (e.g., GPT o1, DeepSeek R1).\nTrain-time models\nshowed strength in error detection, while test-time models excelled at correction. A hybrid logis-\ntic regression ensemble achieved the best balance, illustrating complementary advantages across\napproaches.\nPersistent Gaps\nTaken together, benchmarks, shared task systems, and computational strategies highlighted two\nmajor gaps. First, existing studies emphasize quantitative metrics but rarely examine how LLMs\nfail across different error types. As a result, recurring weaknesses remain underexplored—such as\nlow recall in error flag and sentence detection, elevated FPR that triggers unnecessary corrections of\nvalid sentences, and limited handling of clinical abbreviations and shorthand. Second, while shared\ntask systems have experimented informally with retrieval, the potential of RAG to dynamically\nadapt prompting has not been systematically evaluated. In short, prior work has not yet established\nhow prompting strategies influence model behavior or how retrieval-based methods might mitigate\nspecific error patterns.\n4\n"}, {"page": 5, "text": "Our Contribution\nWe address these problems by performing the first systematic analysis of prompting strategies—\nzero-shot, SPR, and RDP—for medical error flag detection, error sentence detection and error\ncorrection.\nBy retrieving semantically aligned exemplars for each input, RDP improves recall\nin error sentence detection, reduces false-positive corrections of accurate sentences, and enhances\nreasoning over clinical shorthand. Our results show that dynamic prompting reliably bridges the\ngap between high-level error detection and fine-grained clinical correction.\n3. Materials and Methods\n3.1. Dataset\nWe use the MEDEC corpus [10], released as part of the MEDIQA-CORR 2024 shared\ntask [9]. MEDEC consists of two subsets: the MS collection, derived from MedQA-style board\nexamination scenarios [28], and the UW collection, derived from de-identified clinical notes.\nTogether, they provide 3,848 clinical texts, each either correct or containing a single medically\nplausible error manually injected by clinically trained annotators to preserve contextual coherence.\nThe errors span five types—diagnosis, causal organism, management, treatment, and pharma-\ncotherapy. As an example from the MEDEC dataset, the sentence “Patient is diagnosed with aortic\nstenosis after physical examination reveals a double apical impulse.” contains an error and is cor-\nrected as “Patient is diagnosed with hypertrophic cardiomyopathy after physical examination\nreveals a double apical impulse.”\nTable 1 presents dataset splits, as well as prevalence of errors in each of the splits. MEDEC\ndataset is publicly available at: https://github.com/abachaa/MEDEC.\nCollection\nTraining\nValidation\nTest\nTotal\nMS\n2,189\n574\n597\n3,360\nUW\n–\n160\n328\n488\nMEDEC\n2,189\n734\n925\n3,848\n# texts without errors\n970 (44.3%)\n335 (45.6%)\n450 (48.7%)\n1,755 (45.6%)\n# texts with errors\n1,219 (55.7%)\n399 (54.4%)\n475 (51.3%)\n2,093 (54.4%)\nTable 1: MEDEC dataset [10].\nAcross the MEDEC dataset, management errors constitute roughly 45–50% of all annotated\ncases, followed by diagnosis errors at around 30–35%.\nTreatment and pharmacotherapy\nerrors together account for approximately 15–20%, while causal organism errors are the least\nfrequent (below 5%). This distribution remains consistent across the MS and UW subsets and their\nrespective training, validation, and test splits.\n3.2. Task Formulation\nMEDIQA-CORR shared task contained three subtasks:\n• Subtask A: error flag detection. Predict whether a clinical note contains an error.\n• Subtask B: error sentence detection. Identify the sentence containing the error.\n• Subtask C: error correction. Generate a corrected version of the erroneous sentence.\n5\n"}, {"page": 6, "text": "3.3. Language Models\nWe evaluated nine recent LLMs, covering both compact variants such as o1-mini, o4-mini,\nGPT-4o-mini, GPT-4.1-mini, and Claude 3.5 Sonnet, optimized for efficiency and frontier-scale\nsystems such as GPT-4o, GPT-4.1, GPT-5, and Gemini 2.0 Flash, optimized for reasoning. This\ndiversity allows us to evaluate how model size, architecture, and training strategies impact perfor-\nmance in medical error detection and correction across different types of LLMs.\nGPT-4o and GPT-4o-mini. GPT-4o (OpenAI, 2024) is a multimodal LLM providing GPT-4–level\nintelligence with improved efficiency and latency [29]. Its smaller sibling, GPT-4o-mini (∼8B pa-\nrameters), is optimized for instruction-following and lightweight reasoning while retaining strong\nalignment capabilities [30].\nGPT-4.1 and GPT-4.1-mini. GPT-4.1 (OpenAI, late 2024) introduces refinements in reasoning\nrobustness and factual grounding, improving multi-turn consistency and reliability [31]. We addi-\ntionally evaluate GPT-4.1-mini, a compact version intended to balance efficiency with reasoning\nperformance, particularly suitable for controlled few-shot settings [32].\nGPT-5. GPT-5 (OpenAI, 2025) represents the latest generation of frontier-scale models, trained\nfor improved factual accuracy, consistency, and domain adaptability. GPT-5 has shown superior\nperformance across open-domain reasoning tasks and serves as a strong benchmark [33].\no1-mini and o4-mini. The o1-mini and o4-mini families (OpenAI, 2024) are smaller-scale opti-\nmized reasoning models, reported at ∼100B and ∼8B parameters, respectively. These models were\ndesigned for fast inference while maintaining high reasoning quality, and represent OpenAI’s more\ncompact but capable architectures [34], [35].\nClaude 3.5 Sonnet. Claude 3.5 Sonnet (Anthropic, October 2024) is a ∼175B parameter model\ntrained with Constitutional AI for alignment and factual reliability. Claude models are designed to\nreduce hallucination rates and improve transparency [36].\nGemini 2.0 Flash. Gemini 2.0 Flash (Google DeepMind, 2024) is the most recent release in the\nGemini series. It is optimized for high efficiency and fast reasoning, with strong multilingual and\ndomain-adaptation capabilities. While precise parameter counts remain undisclosed, Gemini 2.0\nFlash represents Google’s current state-of-the-art foundation model family [37].\nIn addition to the above LLMs, we experimented with ChatGPT (gpt-4) [38], Phi-3 [39], BioGPT\n[40], and open-source models from the LLaMA family [41–43], but omit those experiments from\nthis paper because they underperformed substantially relative to the instruction-tuned models listed\nabove. Together, our selected nine models span diverse architectures, parameter scales, and training\nstrategies, enabling a comprehensive evaluation of how model design impacts performance in error\nflag detection, error sentence detection, and error correction.\n3.4. Approaches\nWe implemented an LLM-based framework capable of performing all subtasks in an end-to-end\nmanner.\nThree prompting strategies were compared: zero-shot, static prompting with random\nexemplars (SPR), RAG-enabled dynamic prompting (RDP).\n6\n"}, {"page": 7, "text": "Zero-Shot Prompting. A baseline in which the model is presented only with task instructions and\nthe clinical narrative, without any exemplars. The zero-shot prompt was adapted directly from the\nMEDEC benchmark paper [10] to ensure comparability with prior work, and further refined on the\nvalidation set to confirm clarity and alignment with the three subtasks. All prompting strategies\nemployed a standardized instruction template to ensure consistency across models. The complete\nprompt is provided in Appendix B.\nStatic Prompting with Random Exemplars (SPR). We evaluated static n-shot prompting with ran-\ndom exemplars as baselines. In the static n-shot setting, the model is presented with the clinical\nnarrative and n randomly sampled exemplars from the training set, appended to the same instruc-\ntions used in zero-shot prompting. Static n-shot baselines mirror prior shared-task systems [10]\nwhile allowing us to probe whether varying the number of random exemplars provides benefits.\nTo decide on the value of n, we carried out parameter tuning on the validation set (Figure 1).\nGPT-4.1 achieved the strongest overall validation performance across models and thus served as\nthe representative system for selecting n. n = 10 gave the best overall performance across subtasks\nand was adopted for all n-shot experiments.\nFigure 1: Validation performance of GPT-4.1 vs number of exemplars (n) for SPR (dashed) and RDP (solid). Error\nFlag Detection, Error Sentence Detection, and Error Correction scores all peak at n = 10.\nRetrieval-Augmented Generation (RAG)-enabled Dynamic Prompting (RDP). RDP dynamically\nselects semantically relevant exemplars per input clinical note from the training set which we em-\nbedded into a Chroma vector database using OpenAI’s text-embedding-3-large model accessed\nvia LangChain [44–46]. At inference time, the n most semantically similar training cases to the\ninput clinical note are retrieved and appended to the instructions used in zero-shot prompting. The\nvalue of n was fixed to 10, consistent with the tuning described in the SPR section, to ensure a\nfair comparison. We used cosine similarity with input clinical note for dynamic exemplar retrieval\nafter comparing with dot product on the validation set. We identified text-embedding-3-large\n7\n"}, {"page": 8, "text": "as the best embedding backbone for this purpose after evaluating BioClinicalBERT [47], SapBERT\n(PubMedBERT-fulltext) [48], and all-mpnet-base-v2 [49], which showed comparatively lower\noverall AggScores.\nTo support efficient exemplar retrieval, we compared multiple vector database backends. FAISS\n[50] offered high-speed similarity search and scalable indexing but required more engineering effort\nfor integration. Weaviate [51] provided flexible metadata handling and hybrid search capabilities,\nthough at the cost of slower runtime in our setting. Chroma [45], while simpler in scope, integrated\nseamlessly with LangChain and produced consistent nearest-neighbor retrieval across repeated tri-\nals.\n3.5. Proposed RAG-enabled Dynamic Prompting (RDP)\nOur RDP approach consists of two major steps: Vector store construction and RAG-enabled\ninference.\n3.5.1. Vector Store Construction\nPreprocessing. We normalized missing values (e.g., NA), cast sentence IDs to strings, and\npreserved expert corrections. Each record was structured into a document containing segmented\nsentences, the annotated error sentence ID, and the gold-standard correction.\nChunking and Embedding. Documents were split into semantically coherent chunks using\nLangChain’s RecursiveCharacterTextSplitter [46] to respect token limits.\nEach chunk was\nembedded using text-embedding-3-large and stored in a Chroma vector database [45] managed\nvia LangChain.\n3.5.2. RAG-Based Inference\nDuring inference (see Figure 2), (1) The input text is embedded and matched to the n=10\nnearest neighbors in the vector store using cosine similarity.\nThe vector store was constructed\nexclusively from the training split of the MEDEC corpus, ensuring strict separation from test set.\n(2) Retrieved cases are concatenated to the task instructions from zero-shot prompting.\n3.6. Evaluation Metrics\nTo assess performance on the three MEDIQA-CORR tasks we used standard evaluation metrics.\nFor error flag detection and error sentence detection, we report Accuracy, which measures the\nproportion of correctly predicted labels or sentence indices over the total test set.\nFor a more granular understanding of model behavior across error types, we additionally report\nRecall, computed on the subset of test examples where an error was present (i.e., error flag = 1),\ngrouped by error type.\nTo evaluate error correction, we used a combination of lexical and semantic similarity met-\nrics shown to correlate well with expert judgments on clinical text correction [52]. These include\nROUGE-1 [53], BERTScore (using microsoft/deberta-xlarge-mnli) [54], and BLEURT\n[55]. We also report the Aggregate Score (AggScore), calculated as the arithmetic mean of\nROUGE-1, BERTScore, and BLEURT, providing an overall indicator of correction quality. We\ncomputed error correction scores when both the reference and system corrections were available\n(i.e., not NA).\nFinally, we measured the False Positive Rate (FPR) for the best-performing model. FPR\nquantifies how often models incorrectly flag errors in sentences that are in fact correct.\n8\n"}, {"page": 9, "text": "MEDEC Vector Store\nConstruction\nMEDEC\nTraining Data\nPreprocessing\nChunking\nEmbedding\nMEDEC\nVector DB\nClinical Text\nMEDEC\nVector DB\nSimilarity\nSearch\nRelevant MEDEC\nExemplars\nRetrieval\nContext\nPrompt\nConstruction\nTask\nLLMs\nRAG Inference Phase\nCorrected\nClinical Text\nError\nDetection\nError Sentence\nDetection\nPrediction\nFigure 2: Proposed RDP framework.\n3.7. Implementation Details\nAll experiments were conducted in Python 3.10.\nRAG-enabled dynamic prompts were con-\nstructed with a maximum context window of 128k tokens where supported. Random seeds were\nfixed across runs to ensure reproducibility. Experiments were executed on a secure server with\nNVIDIA A100 GPUs (80GB memory) for vector database operations and embedding generation,\nwhile model inference was API-based and did not require local fine-tuning. On average, retrieval-\naugmented inference required 1.6 seconds per case and increased token usage by 11% compared to\nSPR ten-shot. Full evaluation required approximately 60 GPU hours for embedding generation and\n∼$100 in API usage.\nWe released the full code base on GitHub1, including (i) the RDP pipeline and implementation\ndetails, (ii) prompt templates, and (iii) evaluation scripts.\n3.8. Comparison with Physicians\nTo contextualize model performance, we reference the physician annotations reported in the\noriginal MEDEC benchmark paper [10]. In that study, two practicing physicians independently\nreviewed 569 clinical notes from the 925-note test set, with 242 cases double-annotated to estimate\ninter-annotator agreement (IAA). Agreement between the annotators was moderate, with 69.0%\naccuracy for error flag detection and 57.9% for error sentence detection; highlighting the inherent\ndifficulty of the task, even for experts.\n1Link to code: https://github.com/Farzad-1996/MedicalError\n9\n"}, {"page": 10, "text": "3.9. Ethics and Data Governance\nThe UW subset consists of de-identified notes and was used under a data usage agreement.\nData governance followed the MEDIQA-CORR guidelines, and all experiments adhered to FAIR\nprinciples for reproducibility.\n4. Results\n4.1. Comparison of Zero-shot, SPR, and RDP\nTo disentangle the role of retrieval quality from exemplar count, we evaluated five prompting\nconfigurations: (i) Zero-shot (no exemplars), (ii) static prompting with random exemplars\none-shot (SPR one-shot) (one randomly sampled exemplar), representing the previous state\nof the art[10], (iii) static prompting with random exemplars ten-shot (SPR ten-shot)\n(ten randomly sampled exemplars), (iv) RAG-enabled dynamic prompting one-shot (RDP\none-shot) (one semantically retrieved exemplar), and (v) RAG-enabled dynamic prompting\nten-shot (RDP ten-shot) (ten semantically retrieved exemplars).\nTable 2 presents the results across all evaluation metrics. Compared to the zero-shot baseline,\nSPR one-shot improved both error flag and error sentence detection (from 0.6812 to 0.7016 and\n0.6573 to 0.6670, respectively), along with corresponding gains in error correction. SPR ten-shot\nprovided only modest additional improvements (error flag accuracy 0.7124, error sentence detection\naccuracy 0.6702), suggesting that exemplar count alone offers limited benefit. By contrast, RDP\none-shot consistently outperformed its static counterpart (error flag detection accuracy 0.7168,\nerror sentence detection accuracy 0.6810), demonstrating that exemplar quality matters even when\nexemplar count is held constant. Finally, RDP ten-shot achieved the strongest overall performance\n(GPT-4.1: error flag detection accuracy 0.7286, error sentence detection accuracy 0.7037, error\ncorrection AggScore 0.6707), confirming that exemplary quality and quantity are complementary.\nTable 2: GPT-4.1 performance under zero-shot, SPR one-shot, SPR ten-shot, RDP one-shot, and RDP ten-shot.\nCondition\nError Detection Accuracy\nError Correction\nError Flag Error Sentence ROUGE-1 BERTScore BLEURT AggScore\nzero-shot\n0.6812\n0.6573\n0.6451\n0.6333\n0.6432\n0.6405\nSPR one-shot\n0.7016\n0.6670\n0.5960\n0.5915\n0.6157\n0.6010\nSPR ten-shot\n0.7124\n0.6702\n0.6380\n0.6251\n0.6285\n0.6305\nRDP one-shot\n0.7168\n0.6810\n0.6225\n0.6211\n0.6369\n0.6265\nRDP ten-shot\n0.7286\n0.7037\n0.6655\n0.6832\n0.6635\n0.6707\n4.2. Overall Performance: SPR ten-shot vs. RDP ten-shot\nTable 3 presents end-to-end results for error flag detection, error sentence detection, and error\ncorrection across nine LLMs using the best-performing RDP ten-shot and SPR ten-shot.\nIn SPR ten-shot setting, larger instruction-tuned models generally outperformed smaller variants\non both error detection and correction. For example, GPT-4.1 achieved strong error flag detection\naccuracy (71.2%) and the highest error sentence detection accuracy (67.1%), while o1-mini gave\nthe strongest error correction (AggScore of 0.6588).\nWhen enhanced with RDP ten-shot, nearly all models demonstrated consistent improvements\nin all three subtasks. For instance, GPT-4.1 improved error flag detection accuracy from 71.2% to\n10\n"}, {"page": 11, "text": "72.9% and error sentence detection accuracy from 67.1% to 70.4% (+3.3 points), while o1-mini\nachieved the strongest correction quality, increasing AggScore from 0.6588 to 0.6875 (+0.0287).\nThese improvements were statistically significant based on paired bootstrap resampling (1,000 it-\nerations, p < 0.01).\nTable 3:\nPerformance of models under SPR ten-shot and RDP ten-shot. Clinician results are reported from [10].\nModel\nError Detection Accuracy\nError Correction\nError Flag Error Sentence ROUGE-1 BERTScore BLEURT AggScore\nSPR ten-shot\nGPT-4o-mini\n0.6096\n0.4958\n0.5239\n0.5029\n0.5640\n0.5303\nGPT-4.1 mini\n0.6326\n0.5732\n0.5316\n0.5215\n0.5623\n0.5385\nGPT-4o\n0.6469\n0.5564\n0.5913\n0.5524\n0.6132\n0.5857\nGPT-4.1\n0.7124\n0.6702\n0.6380\n0.6251\n0.6285\n0.6305\nGPT-5\n0.7111\n0.6439\n0.6327\n0.6627\n0.6465\n0.6473\no1-mini\n0.6995\n0.6123\n0.6425\n0.6726\n0.6612\n0.6588\no4-mini\n0.6899\n0.5943\n0.5432\n0.5958\n0.5923\n0.5571\nClaude 3.5 Sonnet\n0.6926\n0.6612\n0.2329\n0.1237\n0.5123\n0.2896\nGemini 2.0 Flash\n0.5987\n0.3725\n0.3828\n0.3329\n0.4987\n0.4048\nRDP ten-shot\nGPT-4o-mini\n0.6182\n0.5012\n0.5235\n0.5089\n0.5723\n0.5352\nGPT-4.1 mini\n0.6508\n0.6054\n0.5568\n0.5683\n0.5736\n0.5662\nGPT-4o\n0.6811\n0.6141\n0.6644\n0.6840\n0.6641\n0.6708\nGPT-4.1\n0.7286\n0.7037\n0.6655\n0.6832\n0.6635\n0.6707\nGPT-5\n0.7405\n0.6984\n0.6142\n0.6639\n0.6575\n0.6452\no1-mini\n0.7232\n0.6562\n0.6727\n0.7065\n0.6831\n0.6875\no4-mini\n0.7081\n0.6465\n0.5527\n0.6002\n0.6189\n0.5906\nClaude 3.5 Sonnet\n0.7123\n0.6723\n0.3120\n0.2132\n0.5381\n0.3544\nGemini 2.0 Flash\n0.6013\n0.3821\n0.3927\n0.3312\n0.5125\n0.4121\nClinician Performance (from [10])\nDoctor #1\n0.7961\n0.6588\n0.3863\n0.4653\n0.5066\n0.4527\nDoctor #2\n0.7161\n0.6677\n0.7260\n0.7315\n0.6780\n0.7118\nSeveral factors contribute to these improvements.\nFirst, retrieval provides semantically and\nclinically aligned exemplars, reducing the mismatch that often limits SPR ten-shot. Second, mul-\ntiple exemplars broaden coverage of linguistic and clinical variation, offering the model alternative\npatterns that improve robustness. Third, RDP ten-shot reduces FPR in both error flag and er-\nror sentence detection; exposure to retrieved error-free exemplars helps the model better recognize\ntruly correct cases, lowering the likelihood of incorrectly flagging or labeling non-erroneous sen-\ntences. This distinction also carries over to the correction stage, where the model makes fewer\nunnecessary edits to already correct text.\nInterestingly, not all models benefit equally. Claude 3.5 Sonnet maintained relatively high\nsentence detection accuracy (71.2%) under RDP ten-shot, but its correction quality remained poor\n(AggScore of 0.3544), suggesting that its conservative generation behavior—optimized to minimize\nhallucinations—did not translate into effective correction performance.\nGemini 2.0 Flash also\nlagged behind across tasks, likely because its instruction-tuning and retrieval integration were less\n11\n"}, {"page": 12, "text": "effective in leveraging semantically aligned exemplars during inference.\nAmong clinicians, Doctor #2 achieved the strongest correction quality (AggScore of 0.7118),\noutperforming all models. However, the best-performing RDP ten-shot systems (i.e., GPT-4.1,\no1-mini) narrowed the gap. These systems demonstrated competitive performance on structured\nsubstitution-style errors while still trailing on reasoning-heavy cases.\n4.3. Error Type Performance\nWe evaluated error flag detection, error sentence detection, and error correction performance by\nerror type, focusing on the subset of test cases that contained at least one error (Table 4). Diagnosis,\ntreatment, and pharmacotherapy errors were handled best across both LLMs and clinicians, likely\nbecause of their higher lexical regularity and more stable clinical phrasing. By contrast, manage-\nment errors proved most difficult for models, reflecting their dependence on multi-sentence causal\nreasoning. Interestingly, while both clinicians performed strongly on management errors, Doctor\n#2 achieved the highest overall error correction quality across types, highlighting the continued\nadvantage of expert reasoning in complex, context-dependent settings.\nRDP ten-shot LLMs (GPT-5 RDP ten-shot, GPT-4.1 RDP ten-shot) showed selective but mean-\ningful improvements over their SPR ten-shot counterparts. For GPT-5, RDP ten-shot improved\nerror sentence detection for management, treatment, and pharmacotherapy cases, although SPR\nremained slightly stronger on error flag detection for causal organism. For GPT-4.1, RDP ten-shot\noutperformed SPR ten-shot on treatment error detection and achieved higher error correction scores\nfor all the cases. These gains suggest that retrieval particularly helps with errors requiring factual\nsubstitution or grounding. Nevertheless, both models still lagged behind clinicians on reasoning-\nheavy categories, demonstrating that retrieval enhances lexical alignment but cannot fully replace\ndomain reasoning; examples of these differences appear in Appendix H.\nWe present detailed results for GPT-5 and GPT-4.1 because they consistently ranked among the\nstrongest LLMs in the overall evaluation. Although GPT-4.1 achieved the highest error sentence\ndetection accuracy, GPT-5 delivered superior error flag detection across error types under SPR\nprompting, capturing nearly all true errors. This makes GPT-5 particularly valuable in contexts\nwhere missing an error can have greater consequences than issuing a cautious false alarm. The\nperformance of these models is shown in Table 4.\n4.4. False Positive Rate (FPR) Comparison\nIn addition to recall, which measures sensitivity to true errors, we evaluated the FPR to quantify\nhow often models incorrectly flagged error-free sentences as erroneous.\nTable 5 presents FPR values for GPT-4.1 under different prompting strategies. We focus on\nGPT-4.1 here because it achieved the strongest balance of error detection and correction quality\noverall, making it the most representative case for examining how prompting strategy affects false\npositives. Other models exhibited similar relative trends, but with less consistent improvements,\nso including them offered limited additional insight.\nIn the zero-shot condition, the model produced a relatively high FPR, reflecting a tendency to\nover-correct—that is, to flag and modify sentences that were already correct. The SPR one-shot\nbaseline reduced FPR slightly but still exhibited frequent over-corrections. Adding more context\nwithout retrieval (SPR ten-shot) lowered the FPR modestly, suggesting that exemplar count alone\nprovides limited benefit.\nRDP one-shot and RDP ten-shot both reduced FPR substantially, showing that exemplar\nquality—not only quantity—drives the improvement. RDP ten-shot yielded the lowest FPR overall,\ncorresponding to a relative reduction of nearly one-fifth compared to the SPR one-shot.\n12\n"}, {"page": 13, "text": "Table 4: Recall and error correction scores for each error-type using the subset of test examples with errors: Diagnosis\n(174 texts), Management (168), Treatment (58), Pharmacotherapy (57), and Causal Organism (18).\nError Detection Recall\nError Correction\nError Type\nError Flag\nError Sentence\nROUGE-1\nBERTScore\nBLEURT\nAggScore\nGPT-5 SPR ten-shot\nDiagnosis\n0.9655\n0.8621\n0.7413\n0.7426\n0.7286\n0.7375\nManagement\n0.9405\n0.7976\n0.4966\n0.5310\n0.5689\n0.5322\nTreatment\n0.9655\n0.9310\n0.5454\n0.5973\n0.6163\n0.5863\nPharmacotherapy\n0.9649\n0.8947\n0.6714\n0.6826\n0.6939\n0.6826\nCausal Organism\n1.0000\n1.0000\n0.7897\n0.7861\n0.7658\n0.7805\nGPT-5 RDP ten-shot\nDiagnosis\n0.9540\n0.8448\n0.7063\n0.7350\n0.7016\n0.7143\nManagement\n0.9464\n0.8452\n0.4827\n0.5548\n0.5837\n0.5404\nTreatment\n0.9483\n0.9483\n0.6040\n0.6727\n0.6679\n0.6482\nPharmacotherapy\n0.9649\n0.9123\n0.6756\n0.7138\n0.6958\n0.6951\nCausal Organism\n0.8889\n0.8889\n0.7942\n0.8074\n0.7660\n0.7892\nGPT-4.1 SPR ten-shot\nDiagnosis\n0.6782\n0.6149\n0.7558\n0.7365\n0.7154\n0.7359\nManagement\n0.6190\n0.5238\n0.4511\n0.4557\n0.5288\n0.4785\nTreatment\n0.7069\n0.6552\n0.5961\n0.5914\n0.6156\n0.6010\nPharmacotherapy\n0.7719\n0.7544\n0.6072\n0.5968\n0.6224\n0.6088\nCausal Organism\n0.7222\n0.6667\n0.7212\n0.6922\n0.6753\n0.6962\nGPT-4.1 RDP ten-shot\nDiagnosis\n0.6667\n0.6379\n0.8180\n0.8165\n0.7708\n0.8018\nManagement\n0.6071\n0.5417\n0.4996\n0.5405\n0.5619\n0.5340\nTreatment\n0.8276\n0.7759\n0.6657\n0.6832\n0.6634\n0.6708\nPharmacotherapy\n0.7193\n0.6667\n0.6152\n0.6688\n0.6344\n0.6395\nCausal Organism\n0.6667\n0.6111\n0.7436\n0.7265\n0.6888\n0.7405\nMedical Doctor #1\nDiagnosis\n0.8333\n0.6863\n0.4810\n0.5616\n0.5668\n0.5365\nManagement\n0.8267\n0.6000\n0.2788\n0.3375\n0.4371\n0.3511\nTreatment\n0.7200\n0.6800\n0.2726\n0.4032\n0.4316\n0.3691\nPharmacotherapy\n0.8000\n0.7200\n0.4377\n0.5319\n0.5371\n0.5022\nCausal Organism\n0.7273\n0.7273\n0.3664\n0.4309\n0.5090\n0.4354\nMedical Doctor #2\nDiagnosis\n0.7232\n0.6786\n0.8121\n0.8128\n0.7413\n0.7887\nManagement\n0.6893\n0.6311\n0.6763\n0.6774\n0.6487\n0.6675\nTreatment\n0.7273\n0.6970\n0.5594\n0.6147\n0.5770\n0.5837\nPharmacotherapy\n0.8182\n0.7576\n0.7592\n0.7464\n0.6774\n0.7277\nCausal Organism\n0.4286\n0.2875\n0.4474\n0.4632\n0.4141\n0.4415\n13\n"}, {"page": 14, "text": "Table 5: FPR of GPT-4.1 under different prompting approaches.\nPrompting Strategies\nFPR\nzero-shot\n0.2773\nSPR one-shot\n0.2689\nSPR ten-shot\n0.2444\nRDP one-shot\n0.2523\nRDP ten-shot\n0.2111\n4.5. Controlled Evaluation with Oracle (i.e., Gold Standard) Error Information\nTo further probe the performance ceiling and clarify which subtasks limit end-to-end perfor-\nmance, we conducted two controlled evaluations using the best-performing model, GPT-4.1 with\nRDP ten-shot. While earlier analyses evaluated end-to-end system behavior across the full test set,\nthese controlled experiments isolate contributions from the three subtasks—error flag detection,\nerror sentence detection, and error correction—by providing partial oracle information on\nportions of the test set.\nTable 6: Controlled evaluation of GPT-4.1 with RDP ten-shot on (a) the subset containing only erroneous samples\nand (b) the subset with oracle-provided error sentences.\nMetric\nComplete Test Set\nControlled Setting\n(a) Only Erroneous Samples Subset\nError Sentence Detection\n0.7037\n0.7895\n(b) Oracle Error Sentence Subset\nError Correction\nROUGE-1\n0.6655\n0.7250\nBERTScore\n0.6832\n0.7374\nBLEURT\n0.6635\n0.7027\nAggScore\n0.6707\n0.7217\n(a) Evaluation on Only Erroneous Samples. In this setting, we re-ran the model exclusively on the\nsubset of texts that were known to contain medical errors (i.e., error flag = 1). This controlled\nevaluation removes the confounding effect of misclassified error flags and focuses solely on the\nmodel’s ability to correctly identify the erroneous sentence.\nCompared to the end-to-end evaluation on the complete test set—where the model had to first\ndetermine the error flag and then locate the error sentence (Table 3)—this re-evaluation showed a\nsubstantial gain in error sentence detection accuracy.\n(b) Evaluation with Oracle Error Sentence. In this experiment, we re-ran the model with both the\nclinical text and the oracle-provided index of the sentence containing the error. This configuration\nisolates the error correction capability from both error flag detection and error sentence\ndetection, allowing direct assessment of correction quality.\nWhen provided with the oracle error sentence, GPT-4.1 with RDP ten-shot produced more\naccurate and clinically coherent corrections than the end-to-end setting. Aggregate correction qual-\nity improved from 0.6707 to 0.7217, representing a +7.6% gain. All individual metrics (ROUGE-1,\nBERTScore, and BLEURT) also improved consistently.\n14\n"}, {"page": 15, "text": "Together, these controlled re-evaluations confirm that there is room for improvement in each of\nerror flag detection, error sentence detection, and error correction.\n4.6. Analysis\nTo understand the strengths and weaknesses of RDP compared to zero-shot and SPR, we ana-\nlyzed system outputs along three research questions.\nRQ1: Does context from similar cases improve recall in error sentence detection?. In SPR, recall\nwas often limited by exemplar mismatch: a randomly chosen example rarely resembled the phras-\ning or clinical logic of the target case. Therefore, models tended to miss less common sentence\nstructures or atypical terminology. With RDP, the retrieved cases more closely mirrored the in-\nput’s linguistic style and clinical content (e.g., narratives mentioning specific lab values, imaging\nfindings, or shorthand notations). This increased the likelihood that the model recognized sub-\ntle inconsistencies, improving recall in error sentence detection. However, reasoning-heavy errors\ninvolving causal chains or multi-sentence dependencies remained.\nRQ2: Can RDP examples reduce false positives in error flag detection, thereby preventing unneces-\nsary corrections of valid sentences?. In SPR, models frequently produced false positives by labeling\ncorrect sentences as erroneous. These detection errors triggered unnecessary corrections (e.g., al-\ntering templated statements such as “The patient was started on lisinopril.”). RDP mitigated this\ntendency by surfacing correct, in-domain sentences similar to those in the test input. When re-\ntrieved context included valid, error-free examples, the model was more confident in preserving\ncorrect statements.\nRQ3: Does RDP improve handling of abbreviations and clinical shorthand?. Clinical narratives\nare rife with shorthand (e.g., “Pt c/o SOB,” “Hb 10.2 g/dL,” “ECG showed ST elevation”) and\ninstitution-specific phrasing that can confuse general-purpose LLMs. In SPR, such shorthand often\nled to misinterpretations or spurious corrections. With RDP, retrieved examples frequently con-\ntained similar abbreviations and stylistic conventions, giving the model reference points for disam-\nbiguation. For instance, exposure to prior cases where “SOB” was correctly expanded as “shortness\nof breath”. Similarly, RDP improved the interpretation of numeric ranges (e.g., lab values, vital\nsigns) by grounding them in examples with medically plausible patterns.\nOur results and analysis show that RDP automatically surfaces semantically and syntactically\naligned cases at inference time. Not only does this reduce variability from random sampling, but\nit also improves robustness by adapting to different institutions, error types, and phrasing styles\nwithout additional human intervention. This scalability is particularly important in clinical NLP,\nwhere datasets are heterogeneous and coverage of rare phenomena is critical. Thus, RDP offers a\nmore sustainable path forward for deploying LLM-based error detection and correction in diverse\nclinical environments.\n4.7. Qualitative Output Analysis\nWe conducted a qualitative output analysis over correctly and incorrectly handled samples for\nthe best models. The below patterns emerged:\n15\n"}, {"page": 16, "text": "(1) Where models perform well.\n• Error Flag Detection. Models reliably identified straightforward, localizable errors, espe-\ncially when they appeared in short, self-contained statements. Single-entity swaps or clear\nmislabeling were rarely missed.\n• Error Sentence Detection.\nSentence-level error identification was most accurate when\nthe erroneous clause was contained within a single sentence and did not depend on external\ncontext. Classic examples include clear imaging or laboratory statements that matched well-\nknown diagnostic patterns.\n• Error Correction. Medication and dose normalization were frequently handled well, espe-\ncially when errors followed conventional drug–dose–route syntax. Canonical imaging or lab\nfindings were also corrected with minimal edits, and RDP further improved performance.\n(2) Where models struggle.\n• Error Flag Detection. Models occasionally failed to flag errors when reasoning spanned\nmultiple sentences, leading to false negatives.\nFor example, errors in management plans\ndependent on prior hemodynamics or serial labs were often overlooked. False positives also\noccurred when correct sentences—especially generic management statements or templated\ndocumentation—were incorrectly flagged as erroneous. These issues were more frequent in\nlonger notes containing multiple plausible error candidates.\n• Error Sentence Detection. Errors that required linking information across sentences were\noften missed. Temporal or causal dependencies were particularly challenging, leading to near-\nmiss cases where the model selected an adjacent but incorrect sentence. Such near-misses\ndecreased under RDP.\n• Error Correction.\nAtypical phrasing, rare entities, and uncommon eponyms degraded\ncorrection performance, even when errors were correctly flagged. RDP helped but did not\neliminate the gap. Negation and scope errors also persisted, where misinterpreting phrases like\n“no evidence of...” occasionally flipped the intended meaning. Additionally, over-corrections\nwere observed when false positives at the detection stage cascaded into unnecessary edits\nof correct sentences. Rare hallucinations (e.g., inserting diagnoses not present in the note)\nfurther decreased with RDP.\nA detailed exhibit of misclassifications and their frequencies is provided in Appendix F. The\nmost common misclassifications for SPR ten-shot came from near-miss cases (16%), followed by\nnegation (8.9%) and context-related misclassifications (5.1%). RDP helped performance, with the\nlargest improvements in negation handling (−1.1%) and over-corrections (−0.8%).\nWhile rare\nentities, hallucinations, and other miscellaneous misclassifications were relatively infrequent, they\nalso saw consistent reductions under RDP. Overall, the total misclassification rate dropped from\n32.9% in the ten-shot setting to 29.6% with RDP.\n4.8. Comparison with Physicians\nWe compared system outputs with physician output on each subtask.\n16\n"}, {"page": 17, "text": "Error Flag Detection. Physicians consistently flagged notes with subtle causal inconsistencies that\nmodels often overlooked, leading to false negatives. For example, a management recommendation\ninconsistent with prior hemodynamic findings was missed by o1-mini, whereas GPT-4.1 successfully\nflagged the note when provided with relevant exemplars.\nError Sentence Detection. Frequently, LLMs identified the correct error type but assigned it to\nan adjacent sentence, i.e., a near-miss. In contrast, physicians rarely made such mistakes. RDP\nten-shot reduced such misclassifications by helping models focus on the correct sentence structures.\nError Correction. LLMs demonstrated strengths in straightforward substitutions, such as replacing\nan incorrect diagnosis with the correct one, and in recognizing canonical patterns (e.g., classic imag-\ning findings). However, they occasionally produced hallucinated edits or failed on reasoning-heavy\nerrors, especially when causal logic spanned multiple sentences. RDP often led models to propose\nclinically reasonable alternatives that diverged from the gold standard but aligned with expert\njudgments, highlighting the limitations of rigid string-based metrics for evaluating corrections.\nThese findings illustrate how RDP ten-shot improves performance across subtasks but can still\nlag behind expert reasoning in complex scenarios.\n5. Discussion\nSummary of Findings. Across all models, RDP ten-shot consistently improved performance\nby retrieving semantically similar clinical cases to guide predictions. This approach demonstrated\nrobustness in both subsets of the MEDEC dataset.\nFor error flag detection, RDP reduced FPR by up to ∼15%. This prevented unnecessary\ninterventions on sentences that were already valid.\nFor error sentence detection, GPT-4.1 RDP ten-shot achieved the strongest performance,\nshowing fewer near-misses compared to SPR. This improvement suggests that dynamically retrieved\nexemplars help models better recognize subtle clinical phrasing and shorthand.\nFor error correction, o1-mini RDP ten-shot delivered the highest correction quality (Ag-\ngScore), particularly in domains such as medication normalization and canonical lab/imaging pat-\nterns. However, reasoning-intensive errors (e.g., logical inconsistencies or inference beyond surface\ntext) remained challenging. In these cases, expert physicians continued to outperform models.\nOur evaluation confirmed that RDP improves handling of domain-specific phrasing and elimi-\nnates the need for manual exemplar selection.\nReal-world applications. These findings point toward practical hybrid systems that combine\nmodel-driven error detection with human expert review. In practice, models could serve as a first-\nline screening tool to automatically flag potential errors in real time during documentation. Even if\nimperfect, such a system would reduce the cognitive load on clinicians by highlighting likely issues,\nwhile leaving final adjudication and correction to physicians—who remain the gold standard for\ncomplex, reasoning-intensive cases. This balance could accelerate documentation review, reduce\nerror propagation, and improve overall clinical reliability without over-trusting model output.\nBeyond clinical note review, similar workflows could extend to: • Medication reconciliation,\nwhere models highlight possible inconsistencies across prescriptions, with pharmacists validating\ncorrections. • Radiology and pathology reporting, where models pre-flag likely template or phrasing\nerrors, leaving nuanced interpretation to specialists. • Procedure documentation, where models assist\nby detecting missing or inconsistent details in operative notes, discharge summaries, or follow-up\ninstructions, ensuring that key steps and safety checks are properly recorded.\n17\n"}, {"page": 18, "text": "Taken together, these results suggest that RDP ten-shot offers a path toward clinically reliable\nerror detection. The most promising deployment paradigm may not be full automation, but rather\naugmentation: models handling high-recall detection at scale, and human experts ensuring precise,\ncontext-aware correction.\nLimitations.\nThis study has several limitations.\nFirst, our evaluation was constrained by\nmodel access: we were unable to test certain premium-tier, cutting-edge reasoning models, which\nmay set a higher performance ceiling. API quota and rate limits also restricted the breadth of\nour experimentation and limited more robust analysis. Additionally, the non-deterministic nature\nof commercial LLM APIs introduced variance across runs, which limits strict reproducibility, and\nsome models’ context window sizes prevented full evaluation on very long clinical notes and limited\nreasoning over multi-sentence causal structures.\nSecond, our results depend on the quality of the RDP example retrieval. When the retriever\nsurfaces suboptimal or only loosely related examples, these mismatches propagate into prompting\nand reduce the benefits of RDP. The size of the data set further constrained the evaluation—some\nerror types had relatively few examples, reducing statistical power and making the performance on\nrare phenomena less stable.\nFinally, our evaluation metrics only partially capture clinical correctness. Standard text-similarity\nmetrics like ROUGE, BERTScore, and BLEURT may penalize clinically valid alternative correc-\ntions or fail to reflect partial correctness. Moreover, the underlying models remain opaque: the\nblack-box nature of LLM decision processes makes it difficult to fully characterize failure modes or\nguarantee reliability. Future work to address these gaps is needed. One possibility is a multi-agent\nsystem, which would help bypass context-window and model-performance limitations while also\nimproving explainability.\n6. Conclusions\nWe presented a systematic evaluation and analysis of recent LLMs on the MEDIQA-CORR 2024\nshared task for medical error detection and correction. We compared both compact LLMs (e.g.,\no1-mini) and larger frontier-scale models (e.g., GPT-4.1, GPT-4o) under three prompting strategies:\nzero-shot, SPR and RDP. Our proposed RDP consistently improved performance across all three\nsubtasks—Error Flag Detection, Error Sentence Detection, and Error Correction—by grounding\npredictions in semantically relevant clinical cases.\nKey findings include: (1) In error flag detection, RDP ten-shot prompting reduced FPR by\nup to ∼15%, thereby preventing unnecessary corrections of valid sentences; (2) In error sentence\ndetection, GPT-4.1 (RDP ten-shot) achieved the strongest error sentence detection accuracy,\nreducing near-miss sentence assignments compared to SPR; (3) In error correction, o1-mini\n(RDP ten-shot) delivered the highest correction quality (AggScore), particularly for medication\nnormalization and canonical diagnostic substitutions; and (4) Error-type analysis revealed persistent\nchallenges in handling cross-sentence reasoning, temporal logic, and rare clinical entities.\nThese results suggest that while general-purpose LLMs are promising tools for supporting clinical\ndocumentation review, their reliability remains below that of expert physicians, particularly in\ncomplex or ambiguous cases. To bridge this gap, future work should investigate: (a) specialized\nmedical LLMs fine-tuned on domain corpora, (b) hybrid systems that integrate structured clinical\nknowledge with RDP, and (c) improved evaluation metrics capable of capturing medically valid\nsynonyms and clinically acceptable alternatives.\n18\n"}, {"page": 19, "text": "Ultimately, our study demonstrates the promise of RDP for enhancing clinical text quality and\nreducing documentation errors, while underscoring the need for continued research on clinically\naligned AI systems prior to real-world deployment.\nCredit authorship contribution statement\nFA: Conceptualization, Visualization, Methodology, Analysis, Writing – original draft, review\n& editing. JAJ: Conceptualization, Visualization, Methodology, Analysis, Writing – original draft,\nWriting – review & editing. MY: Conceptualization, Review & editing. ÖU: Conceptualization,\nMethodology, Visualization, Writing - review & editing.\nDeclaration of competing interest\nThe authors declare the following financial interests/personal relationships which may be con-\nsidered as potential competing interests: MY is an Associate Editor of Journal of Biomedical\nInformatics. The remaining authors declare that they have no known competing financial interests\nor personal relationships that could have appeared to influence the work reported in this paper.\nAcknowledgments\nThis work was supported by the National Institutes of Health (NIH) - National Cancer Institute\n(Grant Nr. 1R01CA248422-01A1) and National Library of Medicine (Grant Nr. 2R15LM013209-\n02A1). The content is solely the responsibility of the authors and does not necessarily represent\nthe official views of the NIH.\nDeclaration of Generative AI and AI-assisted technologies in the writing process\nDuring the preparation of this work, the author(s) used ChatGPT to solicit editorial feedback\nregarding writing clarity and proofreading. All the scientific content and data interpretation re-\nmained solely the authors’ contributions. After using these tools/services, the authors reviewed\nand edited the content as needed and take full responsibility for the content of the publication.\nAppendix A. Subset Comparison: MS vs. UW\nTable A.7 reports performance on the MS and UW test subsets for the two strongest models,\nGPT-4.1 and o1-mini under RDP ten-shot.\nResults confirm differences between the two collections. GPT-4.1 achieved stronger performance\non MS notes, while o1-mini performed better on UW notes. UW documentation often contains\nabbreviated phrases and institution-specific terminology; grounding with retrieved exemplars helps\nnormalize this language, reducing both false positives and missed error sentences.\nIn contrast,\nMS notes are more standardized, which favors larger models like GPT-4.1 that excel at leveraging\nwell-structured exemplars.\n19\n"}, {"page": 20, "text": "Table A.7: Subset performance of GPT-4.1 and o1-mini under RDP ten-shot.\nModel\nSubset\nError Detection Accuracy\nError Correction\nError Flag\nError Sentence\nAggScore\nGPT-4.1\nMS\n0.746\n0.725\n0.701\nUW\n0.718\n0.686\n0.634\no1-mini\nMS\n0.729\n0.654\n0.683\nUW\n0.755\n0.679\n0.716\nAppendix B. Prompt Template Used for All Strategies\nThe following instruction was used as the base template for all prompting strategies (zero-shot,\nSPR, and RDP).\nThe following is a medical narrative about a patient. You are a skilled medical doctor\nreviewing the clinical text. The text is either correct or contains one error. The text has\none sentence per line. Each line starts with the sentence ID, followed by a pipe character\nthen the sentence to check. Check every sentence of the text. If the text is correct return\nthe following output: CORRECT. If the text has a medical error related to treatment,\nmanagement, cause, or diagnosis, return the sentence ID of the sentence containing the\nerror, followed by a space, and then a corrected version of the sentence. Finding and\ncorrecting the error requires medical knowledge and reasoning. Here are some general\ntips and reasoning strategies: match diagnosis with findings, check temporal and causal\nlogic, evaluate consistency, recognize typical patterns, be cautious with rare entities, and\nconfirm correct medical terminology.\nAppendix C. GPT-4.1 RDP ten-shot Performance Across Embedding Models\nWe compare GPT-4.1 under identical RDP configurations while varying the embedding model\nused for retrieval. Vector store and search parameters (e.g., cosine similarity, n = 10) are held\nconstant.\nTable C.8: Performance of different embedding backbones under RDP ten-shot prompting for error detection and\ncorrection. text-embedding-3-large serves as the strongest baseline, while domain-specific encoders such as BioClin-\nicalBERT and SapBERT underperform in this task compared to general-purpose OpenAI embeddings. All reported\nscores are strictly lower than text-embedding-3-large.\nEmbedding Model\nError Detection Accuracy\nError Correction\nError Flag Error Sentence ROUGE-1 BERTScore BLEURT AggScore\ntext-embedding-3-large\n0.7286\n0.7037\n0.6655\n0.6832\n0.6635\n0.6707\nBioClinicalBERT\n0.7048\n0.6805\n0.6359\n0.6592\n0.6390\n0.6447\nSapBERT (PubMedBERT-fulltext)\n0.7112\n0.6890\n0.6421\n0.6647\n0.6452\n0.6506\nall-mpnet-base-v2\n0.6955\n0.6712\n0.6280\n0.6528\n0.6325\n0.6378\nNotes. All runs used the same chunking strategy and retrieved the top-k = 10 neighbors to ensure\ncomparability. Each embedding model re-encoded the entire training corpus into its own vector\nspace. Reported values are averaged over multiple random seeds. Retrieval was performed with the\nChroma vector database using cosine similarity.\n20\n"}, {"page": 21, "text": "Appendix D. Zero-shot Results Across Models\nThis appendix reports zero-shot performance for all large language models (LLMs) evaluated in\nthe study, using the same test split and metrics as in the main paper.\nTable D.9: Performance of models on error detection and correction under zero-shot prompting.\nModel\nError Detection Accuracy\nError Correction\nError Flag Error Sentence ROUGE-1 BERTScore BLEURT AggScore\nGPT-4o-mini\n0.6089\n0.4757\n0.5148\n0.5089\n0.5640\n0.5292\nGPT-4.1 mini\n0.6132\n0.5121\n0.5013\n0.5213\n0.5132\n0.5119\nGPT-4o\n0.6584\n0.5665\n0.5517\n0.5373\n0.5852\n0.5581\nGPT-4.1\n0.6812\n0.6573\n0.6451\n0.6333\n0.6432\n0.6405\nGPT-5\n0.6762\n0.6014\n0.6067\n0.6242\n0.6325\n0.6211\no1-mini\n0.6908\n0.5968\n0.6052\n0.6275\n0.6246\n0.6191\no4-mini\n0.6762\n0.5523\n0.5237\n0.5911\n0.5724\n0.5624\nClaude 3.5 Sonnet\n0.7016\n0.6562\n0.2253\n0.1033\n0.5100\n0.2795\nGemini 2.0 Flash\n0.5805\n0.3535\n0.3769\n0.3127\n0.4865\n0.3920\nAppendix E. SPR one-shot Results Across Models\nThis appendix reports SPR one-shot performance for all large language models (LLMs) evaluated\nin the study, using the same test split and metrics as in the main paper.\nTable E.10: Performance of models and clinicians on error detection and correction under SPR one-shot [10].\nModel\nError Detection Accuracy\nError Correction\nError Flag Error Sentence ROUGE-1 BERTScore BLEURT AggScore\nGPT-4o-mini\n0.6092\n0.4872\n0.5238\n0.5019\n0.5540\n0.5266\nGPT-4.1 mini\n0.6213\n0.5723\n0.5267\n0.5143\n0.5631\n0.5347\nGPT-4o\n0.6368\n0.5449\n0.5805\n0.5401\n0.6022\n0.5743\nGPT-4.1\n0.7016\n0.6670\n0.5960\n0.5915\n0.6157\n0.6010\nGPT-5\n0.6811\n0.6292\n0.6255\n0.6458\n0.6565\n0.6426\no1-mini\n0.6962\n0.6086\n0.6375\n0.6619\n0.6509\n0.6501\no4-mini\n0.6866\n0.5752\n0.5327\n0.5912\n0.5813\n0.5684\nClaude 3.5 Sonnet\n0.6800\n0.6508\n0.2249\n0.1125\n0.5081\n0.2818\nGemini 2.0 Flash\n0.5906\n0.3643\n0.3770\n0.3218\n0.4975\n0.3988\nNotes. All results were obtained using the same MEDIQA-CORR test split and identical prompt\ntemplate, with zero-shot (no exemplars) and SPR one-shot prompting. BERTScore was computed\nwith the microsoft/deberta-xlarge-mnli backbone, and BLEURT with the released BLEURT check-\npoint. The AggScore is the arithmetic mean of ROUGE-1, BERTScore, and BLEURT. Reported\nvalues are averaged across multiple runs with fixed random seeds for reproducibility.\n21\n"}, {"page": 22, "text": "Appendix F. Detailed Output Analysis\nIn addition to the quantitative results presented in the main text, we examined qualitative\ndifferences in misclassification behavior between SPR and RDP prompting strategies. As shown\nin Table F.11, RDP not only reduced the overall misclassification rate but also improved the na-\nture of residual misclassifications. Near-miss cases, which were common under SPR prompting,\noften became boundary-level discrepancies rather than full misidentifications under RDP. Notably,\nRDP reduced negation-related misclassifications and over-corrections, indicating improved handling\nof sentence polarity and greater precision in distinguishing true errors from correct statements.\nFurthermore, misclassifications due to hallucination and rare-entities —though less frequent—also\ndecreased, reflecting more stable and contextually grounded model behavior. Overall, these find-\nings suggest that RDP enhances not only detection accuracy but also the semantic reliability and\ninterpretive consistency of model predictions.\nTable F.11: Misclassifications grouped by task for SPR ten-shot vs RDP ten-shot with GPT-4.1.\nTask\nMisclassification Patterns\nSPR Misclassifications\n%\nRDP Misclassifications\n%\nError Flag Detection\nOver-correction\n13\n1.4%\n6\n0.6%\nRare entities\n10\n1.1%\n6\n0.6%\nHallucination\n2\n0.2%\n1\n0.1%\nError Sentence Detection\nNear-miss\n148\n16%\n143\n15.5%\nContext\n47\n5.1%\n44\n4.8%\nNegation\n82\n8.9%\n72\n7.8%\nOther\n3\n0.3%\n2\n0.2%\nTotal Misclassifications\n305\n32.9%\n274\n29.6%\nCorrectly Identified\n620\n67.1%\n651\n70.4%\nAppendix G. Evaluation Metric Definitions\nFor completeness, we include the exact formulas for all reported metrics.\nAccuracy (Subtasks A & B)..\nAccuracy =\nTP + TN\nTP + TN + FP + FN\nRecall on error-present cases. Let the evaluation be restricted to notes with at least one error\n(error flag = 1):\nRecall =\nTP\nTP + FN\nFalse Positive Rate (FPR).\nFPR =\nFP\nFP + TN\n22\n"}, {"page": 23, "text": "ROUGE-1 (F1). Let U(S) and U(R) be the multisets of unigrams in system output S and reference\nR, and let |U(S) ∩U(R)| denote overlap count. Define precision and recall:\nPR1 = |U(S) ∩U(R)|\n|U(S)|\n,\nRR1 = |U(S) ∩U(R)|\n|U(R)|\n.\nThen the F1 variant is:\nROUGE-1 = 2 PR1 RR1\nPR1 + RR1\n.\nBERTScore (F1). Let f(·) be contextual token embeddings and cos(·, ·) cosine similarity. Define\ntoken-level precision and recall with greedy matching:\nPBS = 1\n|S|\nX\ns∈S\nmax\nr∈R cos\n\u0000f(s), f(r)\n\u0001\n,\nRBS =\n1\n|R|\nX\nr∈R\nmax\ns∈S cos\n\u0000f(r), f(s)\n\u0001\n.\nBERTScore = 2 PBS RBS\nPBS + RBS\n.\nBLEURT. BLEURT is a learned regression metric g(S, R) trained to approximate human quality\nratings; we report the model’s scalar output:\nBLEURT(S, R) = g(S, R).\nAggregate Score (AggScore).\nAggScore = ROUGE-1 + BERTScore + BLEURT\n3\n.\nAppendix H. Manual vs. Automatic Corrections\nTable H.12 presents illustrative examples comparing manual (clinician) and automatic (LLM)\ncorrections under RDP ten-shot. Incorrect annotations or outputs are highlighted in gray, following\n[10]. These examples demonstrate how LLMs approximate expert reasoning for straightforward\ndiagnostic and treatment errors but still diverge on reasoning-heavy or context-dependent cases.\nReferences\n[1] Makary MA, Daniel M. Medical error—the third leading cause of death in the US. Bmj.\n2016;353.\n[2] Rajkomar A, Dean J, Kohane I.\nMachine learning in medicine.\nNew England Journal of\nMedicine. 2019;380(14):1347-58.\n[3] Miotto R, Wang F, Wang S, Jiang X, Dudley JT.\nDeep learning for healthcare: review,\nopportunities and challenges. Briefings in Bioinformatics. 2018;19(6):1236-46.\n[4] Johnson AE, Pollard TJ, Shen L, Lehman LwH, Feng M, Ghassemi M, et al. MIMIC-III, a\nfreely accessible critical care database. Scientific data. 2016;3(1):1-9.\n23\n"}, {"page": 24, "text": "Table H.12: Examples of manual & automatic RDP ten-shot corrections. Incorrect annotations/outputs are highlighted in Grey\n[10].\nCase\nOriginal Excerpt (error in\nbold)\nGround Truth\nDoctor Corrections\nLLM Corrections\nDoctor #1\nDoctor #2\no1-mini\nGPT-4.1\nms-test-395 A\n23-year-old\nman\npresents\nwith sudden loss of conscious-\nness while pitching in a base-\nball game. There is no history\nof injury. Consciousness is re-\ngained after cardiopulmonary\nresuscitation.\nPast medical\nhistory\nis\nnegative\nfor\nany\nneurological and cardiovascu-\nlar problems. Patient is di-\nagnosed with aortic steno-\nsis after physical examina-\ntion reveals a prominent\nA wave on the jugular ve-\nnous pulse and a double\napical impulse.\nThere are\nno audible murmurs.\nAn S4\nis present.\nPatient\nis\ndi-\nagnosed\nwith\nhypertrophic\ncardiomyopathy\nafter\nphysical\nexamination\nre-\nveals a prominent\nA\nwave\non\nthe\njugular\nvenous\npulse and a double\napical impulse.\nThe\npatient\nis\ndiagnosed\nwith\nhypertrophic\ncardiomyopathy\nafter\nphysical\nexamination\nreveal a double\napical\nimpulse,\nprominent\n“a\nwave\" and S4.\nPatient\nis\ndi-\nagnosed\nwith\nhypertrophic\ncar-\ndiomyopathy after\nphysical\nexami-\nnation\nreveals\na\nprominent A wave\non\nthe\njugular\nvenous\npulse\nand\na\ndouble\napical\nimpulse.\nPatient\nis\ndi-\nagnosed\nwith\nhypertrophic\nobstructive\ncar-\ndiomyopathy after\nphysical\nexami-\nnation\nreveals\na\nprominent A wave\non\nthe\njugular\nvenous\npulse\nand\na\ndouble\napical\nimpulse.\nPatient\nis\ndi-\nagnosed\nwith\nhypertrophic\ncar-\ndiomyopathy after\nphysical\nexami-\nnation\nreveals\na\nprominent A wave\non\nthe\njugular\nvenous\npulse\nand\na\ndouble\napical\nimpulse.\nms-test-396 A 4-year-old boy presents with\na history of recurrent bacte-\nrial infections, including sev-\neral episodes of pneumococcal\nsepsis. His 2 maternal uncles\ndied after having had similar\ncomplaints. Patient is diag-\nnosed with common vari-\nable immunodeficiency af-\nter lab investigations re-\nveal an undetectable level\nof all serum immunoglobu-\nlins.\nPatient\nis\ndi-\nagnosed\nwith\nBruton\nagam-\nmaglobulinemia\nafter\nlab\ninves-\ntigations\nreveal\nan\nundetectable\nlevel of all serum\nimmunoglobulins.\nThe\ndifferen-\ntial\ndiagnosis\nincludes\nauto-\nsomal\nrecessive\nagammaglobu-\nlinemia, but fur-\nther\nevaluation\nis necessary.\nPatient\nis\ndi-\nagnosed\nwith\nX-linked\nagam-\nmaglobulinemia\nafter\nlab\ninves-\ntigations\nreveal\nan\nundetectable\nlevel of all serum\nimmunoglobulins.\nPatient\nis\ndi-\nagnosed\nwith\nX-linked\nagam-\nmaglobulinemia\nafter\nlab\ninves-\ntigations\nreveal\nan\nundetectable\nlevel of all serum\nimmunoglobulins.\nPatient\nis\ndi-\nagnosed\nwith\nX-linked\nagam-\nmaglobulinemia\nafter\nlab\ninves-\ntigations\nreveal\nan\nundetectable\nlevel of all serum\nimmunoglobulins.\nms-test-397 A\n50-year-old\nwoman\nvisits\nher primary care practitioner\nwith the complaints of gener-\nalized weakness, lightheaded-\nness, and fatigability for the\npast month.\nShe also claims\nto have epigastric pain, heart-\nburn,\nand a sensation of a\nlump\nin\nher\nthroat.\nHer\nfamily\nhistory\nis\nirrelevant,\nbut\nher\nmedical\nhistory\nis\nsignificant for a Pap smear\nthat reported atypical squa-\nmous\ncells\nof\nundetermined\nsignificance (ASCUS), which\nwas followed up with a cervi-\ncal biopsy negative for malig-\nnancy. She occasionally takes\nover-the-counter medicines to\ncope with the heartburn, and\nrecently her hemoglobin was\nfound to be 11 g/dL, for which\nshe received iron and vitamin\nB12 supplementation.\nFecal\noccult blood test is com-\npleted.\nPhysical examina-\ntion is unremarkable, except\nfor pale skin, and a pulse of\n120/min.\nThe patient is re-\nferred for an en-\ndoscopy.\nShe\nhas\nnot\nhad\nfollowup\nafter her cervi-\ncal\nbiopsy\nand\nHPV testing is\nindicated.\nText annotated as\nCORRECT\nShe\nreceived\niron\nsupplementation.\nShe\nreceived\niron\nsupplementation,\nbut\nnot\nvitamin\nB12,\nas\nthere\nis\nno\nindication\nof\nvitamin\nB12\ndeficiency.\n24\n"}, {"page": 25, "text": "[5] Lee J, Yoon W, Kim S, Kim D, So CH, Kang J. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics. 2020;36(4):1234-40.\n[6] Stanford\nCenter\nfor\nResearch\non\nFoundation\nModels.\nStanford\nCRFM\nIntroduces\nPubMedGPT-27B; 2022.\nAccessed:\n2025-10-01.\nhttps://hai.stanford.edu/news/\nstanford-crfm-introduces-pubmedgpt-27b.\n[7] Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. Palm: Scaling\nlanguage modeling with pathways. Journal of Machine Learning Research. 2023;24(240):1-113.\n[8] Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode\nclinical knowledge. Nature. 2023;620(7972):172-80.\n[9] Abacha AB, Yim Ww, Fu Y, Sun Z, Xia F, Yetisgen-Yildiz M. Overview of the mediqa-corr\n2024 shared task on medical error detection and correction. In: Proceedings of the 6th Clinical\nNatural Language Processing Workshop; 2024. p. 596-603.\n[10] Ben Abacha A, Yim Ww, Fu Y, Sun Z, Yetisgen M, Xia F, et al. MEDEC: A Benchmark for\nMedical Error Detection and Correction in Clinical Notes. In: Che W, Nabende J, Shutova E,\nPilehvar MT, editors. Findings of the Association for Computational Linguistics: ACL 2025.\nVienna, Austria: Association for Computational Linguistics; 2025. p. 22539-50. Available from:\nhttps://aclanthology.org/2025.findings-acl.1159/.\n[11] Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large lan-\nguage models in medicine. Nature medicine. 2023;29(8):1930-40.\n[12] Ji Z, Lee N, Fries J, et al. Survey of hallucination in natural language generation. arXiv\npreprint arXiv:230313848. 2023.\n[13] Gundabathula M, Kolar M.\nPromptMind at MEDIQA-CORR 2024:\nChain-of-Thought\nPrompting and Ensemble Methods for Medical Error Detection and Correction. arXiv preprint\narXiv:240508373. 2024. Available from: https://arxiv.org/abs/2405.08373.\n[14] Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, et al. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances in neural information processing systems.\n2020;33:9459-74.\n[15] Bravo-Cándel R, Carrillo-de Albornoz J, Plaza L. A Spanish Dataset for Real-Word Errors\nand Automatic Spell Checking in Clinical Text. Journal of Biomedical Informatics. 2020.\n[16] Pais V, Ko Y, Yetisgen M.\nMEDIC: Medication Direction Correction for Safer Pharmacy\nWorkflows. AMIA Annual Symposium Proceedings. 2024.\n[17] Hossain T, Logan R, Ugarte A, Matsubara S. COVID-Lies: Detecting COVID-19 Misinforma-\ntion on Social Media. arXiv preprint arXiv:200800791. 2020.\n[18] Zhou X, Mulay A, Ferrara E, Zafarani R. ReCOVery: A Multimodal Repository for COVID-\n19 News Credibility Research. In: Proceedings of the 29th ACM International Conference on\nInformation & Knowledge Management; 2020. p. 3205-12.\n[19] Cui L, Lee D.\nCoAID: COVID-19 Healthcare Misinformation Dataset.\narXiv preprint\narXiv:200600885. 2020.\n25\n"}, {"page": 26, "text": "[20] Wadden D, Leidner J, Augenstein I, Ravi S. Fact or Fiction: Verifying Scientific Claims. In:\nEMNLP; 2020. .\n[21] Sarrouti M, El Alaoui S. HealthVer: Verifying Medical Claims with Evidence. In: Proceedings\nof the 12th International Workshop on Health Text Mining and Information Analysis; 2021. .\n[22] Kotonya N, Toni F. Explainable Automated Fact-Checking for Public Health Claims. In:\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing;\n2020. p. 7740-54.\n[23] Wührl A, Klie JC, Gurevych I. BEAR-FACT: Benchmarking Biomedical Evidence-based Au-\ntomated Reasoning. In: ACL Findings; 2023. .\n[24] Pal A, Wei J, Kim HW, et al. Med-HALT: Benchmarking Hallucination in Medical Large\nLanguage Models. arXiv preprint arXiv:240108452. 2024.\n[25] Ben Abacha A, et al. Overview of the MEDIQA-CORR 2024 Shared Task on Medical Er-\nror Detection and Correction. In: Proceedings of the Clinical Natural Language Processing\nWorkshop (ClinicalNLP). Association for Computational Linguistics; 2024. Available from:\nhttps://aclanthology.org/2024.clinicalnlp-1.57.\n[26] Valiev D, Tutubalina E. HSE NLP at MEDIQA-CORR 2024: Knowledge Graph and Entity-\nAware Prompt Ensemble for Medical Error Detection and Correction. In: Proceedings of the\nClinical Natural Language Processing Workshop (ClinicalNLP). Association for Computational\nLinguistics; 2024. Available from: https://aclanthology.org/2024.clinicalnlp-1.47.\n[27] Cai S, Wang Z, et al. Train-Time and Test-Time Computation in Large Language Models for\nError Detection and Correction in Electronic Medical Records: A Retrospective Study. JMIR\nMedical Informatics. 2025;13(1):e71076. Available from: https://pmc.ncbi.nlm.nih.gov/\narticles/PMC12293163/.\n[28] Jin D, Pan E, Oufattole N, Weng WH, Fang H, Szolovits P. What disease does this patient\nhave? a large-scale open domain question answering dataset from medical exams. Applied\nSciences. 2021;11(14):6421.\n[29] OpenAI. Gpt-4o; 2024. Accessed: 08/2025. Available from: https://platform.openai.com/\ndocs/models/gpt-4o.\n[30] OpenAI. Gpt-4o mini; 2024. Accessed: 08/2025. Available from: https://platform.openai.\ncom/docs/models/gpt-4o-mini.\n[31] OpenAI. Gpt-4.1; 2024. Accessed: 08/2025. Available from: https://platform.openai.com/\ndocs/models/gpt-4.1.\n[32] OpenAI. Gpt-4.1 mini; 2024. Accessed: 08/2025. Available from: https://platform.openai.\ncom/docs/models/gpt-4.1-mini.\n[33] OpenAI. Gpt-5; 2025. Accessed: 08/2025. Available from: https://platform.openai.com/\ndocs/models/gpt-5.\n[34] OpenAI. o1-mini; 2024. Accessed: 08/2025. Available from: https://platform.openai.com/\ndocs/models/o1-mini.\n26\n"}, {"page": 27, "text": "[35] OpenAI. o4-mini; 2024. Accessed: 08/2025. Available from: https://platform.openai.com/\ndocs/models/o4-mini.\n[36] Anthropic. Claude 3.5 sonnet; 2024.\nAccessed: 08/2025.\nAvailable from: https://www.\nanthropic.com/claude/sonnet.\n[37] Google. Gemini 2.0 flash; 2024. Accessed: 08/2025. Available from: https://gemini.google.\ncom.\n[38] OpenAI. ChatGPT: Optimizing Language Models for Dialogue; 2024. Accessed: 2025-10-06.\nhttps://openai.com/blog/chatgpt.\n[39] Research M. Phi-3 Technical Report; 2024. Accessed: 2025-10-06. https://www.microsoft.\ncom/en-us/research/blog/phi-3-family-of-small-language-models.\n[40] Luo R, Yang L, Liang X, Qin Y, Wang X, Yan J, et al.\nBioGPT: Generative Pre-\ntrained Transformer for Biomedical Text Generation and Mining. Briefings in Bioinformatics.\n2022;23(6):bbac409.\n[41] Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. LLaMA: Open\nand Efficient Foundation Language Models. arXiv preprint arXiv:230213971. 2023.\n[42] Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, et al. LLaMA 2: Open\nFoundation and Fine-Tuned Chat Models. arXiv preprint arXiv:230709288. 2023.\n[43] AI@Meta. The LLaMA 3 Herd of Models. arXiv preprint arXiv:240712360. 2024. Available\nfrom: https://arxiv.org/abs/2407.12360.\n[44] OpenAI. OpenAI text-embedding-3 models; 2024. Accessed: 08/2025. https://platform.\nopenai.com/docs/guides/embeddings.\n[45] Team C. Chroma: An open-source embedding database; 2024. Accessed: 08/2025. https:\n//www.trychroma.com/.\n[46] Chase H, Gola A. LangChain; 2024. Accessed: 08/2025. https://www.langchain.com/.\n[47] Alsentzer E, Murphy J, Boag W, Weng WH, Jindi D, Naumann T, et al. Publicly available\nclinical BERT embeddings. In: Proceedings of the 2nd Clinical Natural Language Processing\nWorkshop; 2019. p. 72-8.\n[48] Liu F, Shareghi E, Meng Z, Basaldella M, Collier N. Self-Alignment Pretraining for Biomedical\nEntity Representations. In: Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies. Association\nfor Computational Linguistics; 2021. p. 4228-38.\n[49] Reimers N, Gurevych I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\nIn: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics; 2019. p. 3982-92.\n[50] Johnson J, Douze M, Jégou H. Billion-scale similarity search with GPUs. IEEE Transactions\non Big Data. 2019.\n27\n"}, {"page": 28, "text": "[51] Weaviate: An Open Source Vector Search Engine; 2024. https://weaviate.io.\n[52] Ben Abacha A, Demner-Fushman D, Roberts K, Rumshisky A, Elhadad N. MEDIQA-ANLI:\nA Natural Language Inference Dataset for the Clinical Domain. In: Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing; 2023. .\n[53] Lin CY. Rouge: A package for automatic evaluation of summaries. In: Text summarization\nbranches out; 2004. p. 74-81.\n[54] Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. Bertscore: Evaluating text generation\nwith bert. arXiv preprint arXiv:190409675. 2019.\n[55] Sellam T, Das D, Parikh AP. BLEURT: Learning robust metrics for text generation. arXiv\npreprint arXiv:200404696. 2020.\n28\n"}]}