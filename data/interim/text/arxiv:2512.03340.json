{"doc_id": "arxiv:2512.03340", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.03340.pdf", "meta": {"doc_id": "arxiv:2512.03340", "source": "arxiv", "arxiv_id": "2512.03340", "title": "PERCS: Persona-Guided Controllable Biomedical Summarization Dataset", "authors": ["Rohan Charudatt Salvi", "Chirag Chawla", "Dhruv Jain", "Swapnil Panigrahi", "Md Shad Akhtar", "Shweta Yadav"], "published": "2025-12-03T01:13:56Z", "updated": "2025-12-03T01:13:56Z", "summary": "Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.03340v1", "url_pdf": "https://arxiv.org/pdf/2512.03340.pdf", "meta_path": "data/raw/arxiv/meta/2512.03340.json", "sha256": "7c806e1a9765d9a24395149369030666c011410991c52d2e8b68ad1ff11a7de2", "status": "ok", "fetched_at": "2026-02-18T02:25:37.881443+00:00"}, "pages": [{"page": 1, "text": "PERCS: Persona-Guided Controllable Biomedical\nSummarization Dataset\nRohan Charudatt Salvi1,*, Chirag Chawla2, Dhruv Jain3, Swapnil Panigrahi3, Md Shad\nAkhtar3, and Shweta Yadav1\n1Department of Computer Science, University of Illinois, Chicago, IL, 60607, USA\n2Indian Institute of Technology, Varanasi, India\n3Department of Computer Science & Engineering, Indraprastha Institute of Information Technology, Delhi, New\nDelhi, 110020, India\n*corresponding author email: rcsalvi2@uic.edu\nABSTRACT\nAutomatic medical text simplification plays a key role in improving health literacy by making complex biomedical research\naccessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide\nvariation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS\n(PERsona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four\npersonas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent\ndifferent levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization.\nEach summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error\ntaxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along\nwith describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that\nassess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset,\nannotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication\nand controllable biomedical summarization.\nBackground & Summary\nThe internet has become a primary source of health information for the public. Surveys indicate that nearly four in five internet\nusers in the United States seek medical or health-related information online1, and an increasing proportion now rely on responses\ngenerated by large language models (LLMs) to address their healthcare questions2. While such models can produce fluent and\ninformative text, they are also prone to factual inaccuracies and hallucinations3–5. Scientific repositories such as PubMed6\nprovide a means to verify these claims, yet the biomedical literature they contain is often dense with specialized terminology\nand complex sentence structures that make it difficult for non-expert readers to comprehend and apply7,8. This gap between\nscientific communication and public understanding has long been recognized, motivating the development of plain-language\nsummaries, which translate complex biomedical findings into language that is accurate, accessible, and understandable to\ndiverse audiences9. Plain-language summarization aims to simplify scientific text while maintaining its essential meaning, but\ncurrent automatic approaches generally adopt a “one size fits all” assumption about a single “non-expert” audience. In reality,\nindividuals differ widely in their educational backgrounds, familiarity with biomedical concepts, and information needs10,11.\nAs a result, a summary appropriate for a layperson may not meet the expectations of a pre-medical student, an interdisciplinary\nresearcher, or a medical professional. Recent advances in controllable and readability-based summarization have introduced\nmechanisms for adjusting lexical complexity and sentence structure12,13, yet these systems primarily emphasize readability\nrather than content personalization10. Furthermore, the datasets required to develop and evaluate persona-specific biomedical\nsummaries remain limited.\nExisting corpora have advanced plain-language summarization but offer limited support for audience-specific adaptation.\nEarly datasets such as CDRS14, PLOS, and eLife15, PLABA16 pair biomedical research articles with plain-language summaries\nwritten for a general public audience, focusing primarily on improving readability rather than persona adaptation. More recent\nefforts have introduced elements of persona-based summarization. The WebMD-derived corpus11 categorizes readers into three\ndiscrete groups, namely doctor, patient, and general reader, enabling the creation of fine-grained, persona-specific summaries.\nSimilarly, MedEasi17 includes two personas, expert and layperson, and provides corresponding summaries of biomedical\ntexts along with edit-level annotations that capture simplification operations such as elaboration, deletion, and replacement.\nThese resources represent important progress toward personalized biomedical communication but remain limited in scope and\n1\narXiv:2512.03340v1  [cs.CL]  3 Dec 2025\n"}, {"page": 2, "text": "Persona\nLayperson\n• Avoid medical jargon.\n• Overview of purpose, method,\nresults.\n• Use short, simple sentences.\nPre-med\n• Briefly explain medical terms.\n• Clear, structured sentences.\n• Emphasize method and findings.\nResearcher\n• Accessible scientific language.\n• Clear and formal sentence.\n• Detailed design, method, and re-\nsults.\nExpert\n• Field-specific terminology.\n• Concise, professional tone.\n• Quantitative detail, design, find-\nings.\nAbstract: Purpose: To evaluate the practical value of initial C-reactive protein (CRP) in the diagnosis of\nbacterial infection in children. Methods: The subjects comprised 11 children, six boys and five girls, aged\n3 months through to 3 years (median age 16 months), whose initial CRP levels were < 1.0 mg/dL despite\nbacterial infection....\nSummary: The purpose of this study was to see how useful a certain protein, called C-reactive protein\n(CRP), is in figuring out if a child has a bacterial infection. This protein usually increases in the body when\nthere’s an infection, so doctors often measure it to help diagnose illnesses. The study looked at 11 children,\na mix of boys and girls, who were between 3 months and 3 years old. Even though these kids had bacterial\ninfections, their initial CRP levels were surprisingly low...\nSummary: The purpose of this study was to assess how useful the initial levels of C-reactive protein (CRP),\na substance produced by the liver in response to inflammation, are in diagnosing bacterial infections in chil-\ndren. The study involved 11 children (six boys and five girls, aged 3 months to 3 years), who had bacterial\ninfections but initially showed CRP levels below 1.0 mg/dL, which is considered low...\nSummary: The study aimed to assess the effectiveness of using initial C-reactive protein (CRP) levels in di-\nagnosing bacterial infections in children. CRP is a protein produced by the liver in response to inflammation,\noften used as a marker to detect infections or diseases. The study involved 11 children (six boys and five\ngirls, aged 3 months to 3 years), who had bacterial infections but initially showed low CRP levels (< 1.0\nmg/dL)...\nSummary: This study assessed the diagnostic value of initial C-reactive protein (CRP) levels in detecting\nbacterial infections in children. The study involved 11 children (six boys and five girls) aged 3 months to\n3 years (median age: 16 months), who initially presented with CRP levels below 1.0 mg/dL despite having\nbacterial infections...\nFigure 1. Persona-specific summarization of a biomedical abstract in PERCS. Green represents simplification, and Blue\nrepresents information detail.\ngranularity. They typically offer only one or two persona types per document and cover a narrow range of medical topics.\nConsequently, there is still a need for a larger, systematically curated dataset that provides multiple parallel summaries reflecting\ndistinct levels of reader expertise for the same biomedical source material.\nTo address this gap, we present the PERCS (PERsona-guided Controllable Summarization) dataset, an expert-annotated\ncorpus designed to facilitate research on audience-aware summarization in the biomedical domain. PERCS comprises 500\nbiomedical research abstracts drawn from PubMed, each paired with four distinct summaries written for readers with different\nexpertise levels: laypersons, pre-medical students, researchers from non-medical fields, and medical experts. This structure\nyields 2,000 persona-specific summaries that vary systematically in terminology, information depth, and readability. As a\nresult, PERCS summaries differ in structure, tone, informational depth, and style, as illustrated in Figure 1. Lay summaries\nemphasize clarity and minimize jargon, describing CRP as “a protein used to check for bacterial infection.” Pre-med summaries\nintroduce basic terminology, defining CRP as “a substance produced by the liver in response to inflammation.” Researcher\nsummaries focus on methods and findings, describing CRP as “a marker to detect infections,” while expert summaries retain\ndomain-specific terminology and quantitative details. Each summary was created and reviewed by medical experts following\ndetailed annotation guidelines to ensure factual consistency and appropriateness for the intended audience. By aligning multiple\nversions of the same text to clearly defined reader personas, PERCS provides an expert-curated resource for studying how\nbiomedical information can be tailored to different personas. The dataset serves as a reference for developing and evaluating\nsystems that create personalized biomedical summaries suited to readers with different backgrounds and levels of understanding.\nTo guide future research, we also evaluate several state-of-the-art language models on PERCS across multiple aspects, such as\ncomprehensiveness, readability, and factuality. Together, these contributions lay the groundwork for building and benchmarking\nmodels that produce accurate, persona-aware summaries of biomedical information.\nMethods\nProblem Statement\nTo support the creation of persona-aware biomedical summaries, we define four reader personas representing distinct levels of\nexpertise and information needs. Each biomedical abstract in the PERCS dataset is paired with four summaries, one for each\npersona:\n1. Layperson: Readers with a high school–level understanding of biology and limited medical knowledge. Summaries use\nsimple language, avoid jargon, and emphasize the study’s purpose and main findings.\n2. Pre-med: Students in the early stages of health-related higher education who are familiar with basic medical terminology.\n2/16\n"}, {"page": 3, "text": "Table 1. Different error types along with their description and examples in the persona-aware biomedical summarization.\nError\nDescription\nExample\nIncorrect Definitions\nWrong or misleading explanations of medical terms or\nconcepts.\nDefining diabetes or insulin function inaccurately.\nIncorrect Synonyms\nReplacing medical words with inaccurate or oversimpli-\nfied terms.\nUsing “painkillers” for anti-inflammatory drugs.\nIncorrect Background\nFalse or irrelevant contextual details about prevalence or\ntreatment.\nCalling a rare disorder common or misdescribing ther-\napy.\nEntity Errors\nWrong factual details like numbers, names, or dosages.\nReporting “50 mg” instead of “5 mg” or “80” vs.\n“800” patients.\nContradiction\nSummary directly opposes the abstract’s results or\nclaims.\nAbstract: “Drug A reduced symptoms.” Summary:\n“No effect.”\nOmission\nMissing key findings or results from the abstract.\nSkipping main outcomes or notable side effects.\nJumping to Conclusions\nOverstating results beyond what data supports.\nAbstract: “may help,” but summary: “proven cure.”\nMisinterpretation\nMisstating or oversimplifying meaning of the abstract.\nSaying “FDA approved” when text says “FDA al-\nlowed.”\nStructure Error\nDisorganized layout or mixing sections like methods and\nresults.\nWriting results under background or as FAQ style.\nPersona Relevance\nLanguage complexity unsuitable for target persona.\nUsing jargon for lay readers or oversimplifying expert\ntext.\nHallucination\nAdding fabricated or irrelevant content not in abstract.\nMentioning an unrelated drug or disease.\nSummaries include concise definitions of key terms and describe methods and results in clear, structured sentences.\n3. Researchers: Scientifically trained individuals without biomedical specialization, such as engineers, policymakers, or\nclinicians from other fields. Summaries describe study design and key results using accessible scientific language.\n4. Medical experts: Biomedical researchers and practitioners. Summaries remain concise and technical, retaining domain-\nspecific terminology, methodological details, quantitative results, and findings.\nThese four personas represent the major audience groups commonly addressed in science communication: the general\npublic with limited medical literacy18, learners progressing toward medical expertise, adjacent but non-specialist researchers19,\nand domain experts who require technical precision. Together, they represent the key audiences for communicating biomedical\nresearch. The task is to generate summaries that faithfully reflect the abstract while aligning with the knowledge and needs of\neach target persona. We created the dataset in a two-step process as shown in Figure 3. First, we generated the summaries using\nLLMs. Then, we employed experts to ensure the faithfulness of the summaries, followed by human evaluation of summary\nquality to assess alignment with different personas. We describe both steps in detail below.\nAutomatic Summary Generation\nTo begin dataset creation, we selected biomedical abstracts to ensure coverage of diverse medical topics. We first collected 250\nabstracts from PLOS Medicine, PLOS Biology, and PLOS Neglected Tropical Diseases using the PLOS API. Queries targeted\nchronic diseases such as cancer, diabetes, Alzheimer’s disease, and chronic kidney disease, as well as common infectious\nconditions including typhoid and pneumonia. Articles were retrieved by searching for disease-related keywords in titles and\nabstracts and were manually reviewed for relevance. To further expand topic coverage, we incorporated 250 additional abstracts\nfrom the PLABA dataset, a biomedical lay-summarization corpus that includes conditions such as COVID-19, kidney stones,\nand cystic fibrosis. Abstracts were randomly sampled from PLABA’s training split to avoid overlap with the PLOS subset. The\nfinal PERCS dataset thus contains 500 biomedical abstracts spanning a broad range of diseases and study designs.\nPrevious research has shown that few-shot prompting or fine-tuning of large language models, such as GPT-3.5, can yield\nhigh-quality lay summaries even with limited data20,21. More recent work demonstrates that modern models can also produce\nstrong summaries in zero-shot settings22,23. Studies using GPT-4 and Mistral-Large-Instruct-2407 have reported superior\nhuman-rated performance on biomedical summarization tasks compared with fine-tuned domain-specific models such as\nBioMistral24. Based on these findings, we selected GPT-4 to generate the initial persona-specific summaries in PERCS. Each\nabstract was paired with four model-generated summaries, one for each persona, using customized prompts designed to match\nthe background knowledge and information needs of the target reader. Figure 2 highlights the prompt structure used for the\nlayperson persona, and all complete persona-specific prompts are provided in Table 8 in the Appendix. This automated stage\nproduced high-quality drafts intended as starting points for expert refinement. We acknowledge that such summaries may\n3/16\n"}, {"page": 4, "text": "Figure 2. Example of a prompt design used for Lay persona summary generation in the PERCS dataset\nFigure 3. An overview of the steps involved in constructing the PERCS dataset, namely data collection, summary generation,\nand expert validation.\n4/16\n"}, {"page": 5, "text": "contain factual inaccuracies or misaligned tone, and therefore, expert review and correction were applied to ensure faithfulness\nand appropriate persona alignment.\nMedical Expert Annotation & Evaluation of Summary Quality\nWhile LLM-generated summaries are effective, prior research shows they remain prone to factual errors and hallucinations when\nadapted for public use25. To address these challenges, we examined prior work on error categorization in lay summaries26,27,\nincorporated persona-specific considerations, and developed a taxonomy of error types (Table 1) to guide evaluation of\nsummaries for factual accuracy and persona alignment. We defined 11 fine-grained error types grouped into four categories\n(Table 1): (1) New Information Errors (incorrect definitions, synonyms, or background) (2) Inference Errors (contradictions,\nomissions, misinterpretations, or entity errors) (3) Structure and Readability Errors (4) Hallucinations\nFigure 4. Example of addressing logical errors and supplying persona-specific missing information to produce an appropriate\nsummary for the persona.\nStep 1: Expert Annotation. We employed two practicing physicians (MDs) to reviewed each summary for accuracy and\nrevised it according to our taxonomy.\n• Annotation Task: Using a web-based interface, the experts reviewed each summary alongside its source abstract and\npersona type. Based on predefined guidelines (provided in the Appendix), they highlighted erroneous segments, assigned\nerror tags, and proposed corrections to improve persona relevance and faithfulness while maintaining textual coherence\n(example shown in Figure 3).\n• Training: Experts completed two rounds of calibration on 32 summaries (eight per persona) followed by reconciliation\nsessions to align interpretations of error labels and persona relevance. Consensus on labeling criteria was established\nbefore the main annotation phase.\n• Inter-Annotator Agreement: Given the open-ended nature of summarization, we assessed inter-annotator agreement by\ncomparing sentence-level error labels (as shown in Figure 4). On a random set of 40 summaries containing 420 sentences,\nwe observed strong agreement, with Krippendorff’s alpha ranging from 0.986 to 0.998 across personas and 0.992 overall\n(see Table 2 for the complete agreement scores). After confirming agreement, the experts reviewed all 2,000 summaries\nfor factual accuracy and persona alignment.\nTable 2. Agreement between expert annotators for sentence-level error analysis (Krippendorff’s Alpha)\nPersona\nSentences\nKrippendorff’s Alpha\nLAYMAN\n137\n0.995\nPREMED\n108\n0.997\nRESEARCHER\n106\n0.998\nEXPERT\n69\n0.986\nOVERALL\n420\n0.992\n5/16\n"}, {"page": 6, "text": "Table 3. Human Evaluation Scores and Krippendorff’s Alpha for Each Persona Across Four Criteria\nPersona\nMetric\nScore\nK-α\nLayman\nComprehensiveness\n4.94\n0.82\nLayness\n4.98\n1.0\nFaithfulness\n5.0\n1.0\nUsefulness\n4.98\n1.0\nPre-med\nComprehensiveness\n4.53\n0.7933\nLayness\n4.00\n0.8792\nFaithfulness\n4.34\n0.8348\nUsefulness\n4.13\n0.8353\nResearcher\nComprehensiveness\n4.52\n0.9085\nLayness\n4.41\n0.9068\nFaithfulness\n4.75\n0.8997\nUsefulness\n4.65\n0.9086\nExpert\nComprehensiveness\n4.47\n0.989\nLayness\n1.589\n0.998\nFaithfulness\n4.707\n0.991\nUsefulness\n4.568\n0.9986\nStep 2: Persona-Based Quality Evaluation. Once the faithful summaries were finalized, we evaluated their quality to ensure\nproper alignment with the personas. Following prior work on lay-summary evaluation24,28, we used a five-point Likert scale\nwith rubric-based guidance and two human raters per persona. Specifically, we adopted the rubric structure from Salvi et.al24,\nwhich assumes a target reader with high school-level biology knowledge. The rubric scores summaries on comprehensiveness,\nlayness, and factuality using a 1–5 scale:\n• Comprehensiveness: Assesses whether essential topics, approaches, and findings are present (1 = incomplete; 3 = topic\nunderstood but key details missing; 5 = fully covered).\n• Layness: Measures how well jargon is reduced and concepts are explained using simple sentences, definitions, and\nanalogies (1 = little simplification; 3 = mix of jargon and simple terms; 5 = plain, well-explained, easy to follow).\n• Factuality: Evaluates alignment with the abstract (1 = misrepresents findings or methods; 3 = mostly accurate with\nminor inconsistencies; 5 = fully faithful).\nBecause we covered diverse reader groups, we also added a usefulness dimension, which evaluates whether the summary helps\nthe intended reader grasp what was studied, why it matters, and what was found (1 = not helpful; 5 = very helpful).\nInter annotator agreement: Each persona-specific reader evaluated 50 summaries tailored to their persona. Raters first\nread the abstract, then the summary, and recorded four ratings: comprehensiveness, layness, factuality, and usefulness. Overall,\nratings were consistently high for comprehensiveness, faithfulness, and usefulness, averaging around 4.5 ± 0.3 across personas.\nLayness scores showed greater variation, as expected since the rubric was originally designed for lay summaries. Across\npersonas, Krippendorff’s alpha values ranged from approximately 0.79 to 1.0, indicating strong inter-rater reliability overall.\nFor full detailed results, including per-persona facet scores and agreement coefficients, see Table 3.\nData Records\nThe PERCS dataset is publicly available at the Open Science Framework repository at https://doi.org/10.17605/\nOSF.IO/UMBJ7 and contains 500 biomedical research abstracts paired with 2,000 expert-curated persona-specific summaries.\nEach record corresponds to a single abstract and includes four summaries tailored to distinct reader personas: layperson,\npre-medical student, researcher from a non-medical field, and medical expert.\nData structure.\nThe dataset is released in JSON format, where each entry represents one abstract with all four persona-specific\nsummaries. Each entry contains the following fields:\n• id: Unique identifier for the abstract. For PLABA abstracts, this corresponds to the PMID, and for PLOS abstracts, it is\nthe PLOS-assigned identifier.\n• abstract: Full text of the abstract.\n• summaries: Object containing four expert-corrected summaries, keyed by persona:\n6/16\n"}, {"page": 7, "text": "– layman: Summary for general public readers.\n– premed: Summary for pre-medical students.\n– researcher: Summary for researchers from non-medical fields.\n– expert: Summary for medical experts.\nAn illustrative example of persona variation in tone, terminology, and level of detail is shown in Figure 1.\nData splits\nTo support benchmarking and reproducibility, the dataset is divided into two non-overlapping splits: 350 abstracts\n(70%) for training and 150 abstracts (30%) for testing. Both subsets maintain balanced representation across sources (PLOS\nand PLABA). No abstracts appear in both splits.\nLicense\nThe dataset is released under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, allowing\nunrestricted reuse, with appropriate attribution.\nTechnical Validation\nTopic Distribution\nThe abstracts cover a wide range of diseases. PLOS sources focus on chronic conditions such as cancer, diabetes, and\nAlzheimer’s disease, while PLABA adds topics such as arthritis, kidney stones, and COVID-19. The complete topic distribution\nfor PLOS abstracts is provided in Table 7 in the Appendix.\nSummary Characteristics\nFor each persona, we measured summary length (word count), lexical diversity using Type–Token Ratio (TTR)29 and Measure\nof Textual Lexical Diversity (MTLD)30, readability using the Dale–Chall Readability Score (DCRS)31, and Coleman–Liau\nIndex (CLI)32, and semantic overlap with the abstract using BERTScore (BERT_S)33. We computed each metric for individual\nsummaries and report their mean and variation in Table 4. We found that layperson summaries were longer, with added\ndefinitions and context, but showed lower lexical diversity and the least overlap with the abstract (BERTScore). They were\nalso easier to read (lower DCRS, CLI). As expertise increased, summaries became shorter, more complex (higher DCRS,\nCLI), more diverse (higher TTR, MLTD), and closer to the abstract (higher BERT_S). Pre-med and researcher summaries\ndiffered modestly, with researchers using slightly more technical phrasing and achieving marginally higher readability scores\nand abstract overlap. Expert summaries were the most concise, lexically rich, and information-dense.\nTable 4. PERCS Dataset Statistics: Average Readability, Word Count, and Lexical Diversity by Persona\nPersona\nWord Count\nDCRS\nCLI\nBERT_S\nTTR\nMLD\nLAYMAN\n270.91\n8.17\n10.48\n0.811\n0.559\n98.159\nPREMED\n243.48\n9.96\n14.31\n0.854\n0.598\n102.845\nRESEARCHER\n246.36\n10.12\n14.58\n0.865\n0.599\n102.779\nEXPERT\n166.43\n11.76\n17.18\n0.890\n0.678\n114.860\nExperimental Benchmarking\nWe evaluated four large language models (LLMs), including GPT-4o, Mistral-8-7B-Instruct, Gemini-2.0 Flash Lite, and\nLLaMA-3 70B, on the PERCS dataset, which provides persona-specific summaries for four reader groups: laypersons,\npre-medical students, non-medical researchers, and medical experts.\nExperimental Setup\nWe benchmarked three standard prompting strategies for LLM-based summarization:\n• Zero-shot: The model receives the persona-specific prompt used to create the dataset and generates a summary without\nadditional examples.\n• Few-shot: The same persona-specific prompt is provided along with three in-domain exemplars from the dataset.\n• Self-refine: The model first produces a zero-shot summary, then critiques it for persona alignment and faithfulness to\nthe abstract, provides self-feedback, and revises iteratively until satisfied. In this setup, the LLM acts as both feedback\nprovider and evaluator.\n7/16\n"}, {"page": 8, "text": "Table 5. Selected performance on PERCS across personas using automatic evaluation metrics. The readability metrics (FKGL,\nDCRS, CLI, and LENS) directionality is not defined in a persona-based setting.\nPersona\nModel\nMethod\nR-1 ↑\nR-2 ↑\nR-L ↑\nSARI ↑\nFKGL\nDCRS\nCLI\nLENS\nSC ↑\nLayperson\nGemini\nZero-shot\n0.6029\n0.2624\n0.3691\n53.0180\n10.04\n8.71\n11.66\n52.5033\n0.3031\nGPT-4o\nFew-shot\n0.6016\n0.2476\n0.3554\n53.5134\n11.22\n8.94\n11.99\n75.0276\n0.2987\nGPT-4o\nSelf-Refine\n0.5288\n0.1727\n0.2696\n48.3749\n11.24\n9.47\n12.84\n71.6981\n0.2553\nPremed\nMistral\nZero-shot\n0.5819\n0.2629\n0.3622\n49.3190\n16.08\n11.80\n17.53\n50.3914\n0.3498\nLlama-3.1\nFew-shot\n0.6149\n0.2926\n0.3823\n51.9053\n16.72\n11.78\n17.69\n53.6619\n0.2969\nGemini\nSelf-Refine\n0.5564\n0.2084\n0.3011\n46.4857\n14.82\n10.76\n16.42\n45.4320\n0.2613\nResearcher\nLlama-3.1\nZero-shot\n0.6120\n0.2880\n0.3719\n48.4270\n15.19\n10.02\n14.95\n57.7103\n0.2769\nMistral\nFew-shot\n0.6122\n0.3072\n0.3978\n49.4988\n15.05\n10.65\n15.93\n59.3144\n0.3928\nGemini\nSelf-Refine\n0.5564\n0.2084\n0.3011\n46.6575\n14.82\n10.76\n16.42\n50.6804\n0.2803\nExpert\nGPT-4o\nZero-shot\n0.6000\n0.2905\n0.3997\n42.1944\n17.91\n12.77\n19.44\n51.2367\n0.3283\nGemini\nFew-shot\n0.6599\n0.3815\n0.4912\n44.6463\n15.43\n12.57\n18.03\n54.1410\n0.3795\nGPT-4o\nSelf-Refine\n0.5577\n0.2547\n0.3535\n42.4489\n17.30\n12.76\n19.39\n51.3194\n0.2946\nWe evaluated summaries on three key aspects: comprehensiveness, readability, and faithfulness, using automatic evalu-\nation metrics. Comprehensiveness was measured using ROUGE-1, ROUGE-2, ROUGE-L34 and SARI35; readability using\nFKGL36,37, DCRS31, CLI32, and LENS38; and faithfulness using SummaC (Conv)39. For summary generation, we accessed\nGPT-4o via the official OpenAI API platform. Other open-source models, including LLaMA3, Mistral, and Gemini, were\nqueried using the OpenRouter API interface. For automatic evaluation, we computed ROUGE, SARI, FKGL, DCRS, and CLI\nusing publicly available Python libraries. LENS and SummaC metrics, which require more computational resources, were run\non a machine equipped with a T4 GPU (16 GB VRAM).\nResults\nTo enable direct comparison within each persona and evaluation facet, all metric scores were normalized using Min–Max\nscaling after adjusting the directionality of the readability metrics. For the layperson, pre-medical, and researcher personas, the\nreadability metrics FKGL, DCRS, and CLI were inverted, since lower values indicate simpler and more accessible language,\nand were therefore treated as higher normalized scores. For the expert persona, the LENS metric was inverted instead, as lower\nraw LENS values correspond to more concise and domain-appropriate phrasing. This procedure ensured that all normalized\nmetrics were directionally aligned such that higher values consistently indicated better performance, enabling comparison of\nmodel performance across comprehensiveness, readability, and faithfulness facets. Following normalization, all models and\nprompting methods were ranked within each persona. Table 5 presents the best-performing model for each prompting strategy\nacross the personas. The complete results for each persona are provided in Tables 9, 10, 11, and 12 in the Appendix.\nAcross all personas, few-shot prompting generally achieved the best balance between comprehensiveness, readability, and\nfaithfulness. The few-shot GPT-4o model obtained the highest overall performance for the lay summaries (R-1 = 0.602, SARI\n= 53.51), producing text that was both accurate and accessible. For pre-medical readers, few-shot LLaMA-3.1 performed best\n(R-1 = 0.615, R-L = 0.382, SARI = 51.91), indicating improved content coverage while maintaining moderate readability.\nAmong researcher-level summaries, few-shot Mistral was able to provide the most precise and detailed information, achieving\nthe highest faithfulness (SummaC = 0.393) and comprehensiveness (R-L = 0.398). Finally, for expert-level outputs, zero-shot\nGPT-4o yielded the best overall alignment between coverage and readability (R-L = 0.400, SC = 0.328), demonstrating that the\nLLM could effectively control generation for an expert persona without in-context examples.\nAcross prompting strategies, few-shot configurations consistently provided higher normalized composite scores, whereas\nself-refinement offered limited gains and occasionally reduced lexical overlap. Collectively, these results indicate that in-context\nexamples substantially improve persona-aware summarization quality, with GPT-4o and LLaMA-3.1 exhibiting the most stable\nperformance across reader types, Mistral excelling in factual precision, and Gemini producing strong expert-oriented summaries\nwith competitive ROUGE scores.\nDiscussion\nThe results demonstrate that the prompting strategy has a substantial impact on persona-aligned summarization performance.\nFew-shot prompting consistently achieved the best trade-off between coverage, readability, and faithfulness across all reader\npersonas. This finding suggests that even a small number of in-context examples can effectively guide large language models\nto adjust lexical choice, syntactic complexity, and informational detail to match the intended audience persona. By contrast,\nzero-shot prompting produced strong but less consistent results, indicating that LLMs contain considerable generalization\n8/16\n"}, {"page": 9, "text": "Model\nGenerated Summary\nAbstract\nBackground: Antiepileptic drugs (AEDs) can be associated with neurotoxic side effects including cognitive dysfunction, a problem of considerable importance\ngiven the usual long-term course of treatment. Pregabalin is a relatively new AED widely used for the treatment of seizures and some types of chronic pain\nincluding fibromyalgia. We measured the cognitive effects of 12 weeks of pregabalin in healthy volunteers.[.......]. These cognitive effects were paralleled by\ncomplaints on the Portland Neurotoxicity Scale, a subjective measure of neurotoxicity (p < 0.01). Conclusion: At conventional doses and titration, pregabalin\ninduced mild negative cognitive effects and neurotoxicity complaints in healthy volunteers. These effects are one factor to be considered in the selection and\nmonitoring of chronic AED therapy. Class of Evidence: This study provides Class I evidence that pregabalin 300 mg BID negatively impacts cognition on some\ntasks in healthy volunteers.\nGPT-4o\nAntiepileptic drugs (AEDs) are medicines used to treat seizures and chronic pain like fibromyalgia. However, they can sometimes have negative effects on the\nbrain, making it hard to think clearly or remember things. This is important because people usually need these medicines for a long time. Pregabalin is a newer\nAED that we wanted to test for these effects. [.......]. This means they experienced some negative changes in how their brains felt or worked (neurotoxicity), like\nfeeling foggy or having trouble focusing. In conclusion, when pregabalin is taken at typical doses, it can mildly affect brain functions related to thinking and\nmemory in healthy people. These effects are important for doctors to consider when prescribing pregabalin, especially for people who will take it long-term to\nmanage chronic conditions.\nMistral\nAntiepileptic drugs (AEDs), used for treating seizures and certain types of chronic pain, can sometimes cause side effects like problems with thinking and\nmemory. These issues are important to consider since AEDs are often taken for long periods. Pregabalin is a relatively new AED that is widely used for seizures\nand pain management, including in a condition called fibromyalgia. [.......]. Moreover, the volunteers who received pregabalin also reported more complaints on\nthe Portland Neurotoxicity Scale, a questionnaire used to assess neurotoxicity symptoms, meaning that they noticed more issues with their thinking and memory\n(p < 0.01). In conclusion, taking pregabalin at conventional doses and titration for 12 weeks caused mild negative cognitive effects and neurotoxicity complaints\nin healthy volunteers. This information is essential when selecting and monitoring long-term AED therapy. The study provides Class I evidence that pregabalin\n300 mg twice daily negatively impacts cognition on some tasks in healthy volunteers.\nGemini\nThis research looked at how a drug called pregabalin affects the brain. Pregabalin is used to treat epilepsy (seizures) and some types of long-term pain. However,\nsome medications for epilepsy can sometimes cause problems with thinking and memory. These are known as neurotoxic side effects. [.......]. It did not have a\npositive effect for the participants. In addition, those taking pregabalin reported more common side effects related to neurotoxicity, based on their answers to the\nquestionnaire. This suggests that the drug had some unwanted effects on brain function and mental awareness. Based on the results of this research, the study\nconcluded that pregabalin, used at a typical everyday dose, caused some negative outcomes in cognition and signs of neurotoxicity among the subjects. These\nobservations are important to be taken into account when doctors choose and monitor long-term treatments using this type of medicine. The results provide\ndependable evidence that pregabalin at a dose of 300 mg twice a day negatively affects thinking in healthy individuals.\nLlama 3.1\nPregabalin is a relatively new medicine used to treat seizures and certain types of chronic pain, including a condition called fibromyalgia. However, like many\nmedicines that affect the brain, it can have side effects on cognitive function, which includes things like memory, attention, and problem-solving skills. This is a\nsignificant concern because people often take these medicines for a long time. [.......]. Additionally, people taking pregabalin reported more complaints related\nto neurotoxicity, which includes symptoms like feeling tired, having trouble concentrating, or experiencing memory problems. These findings suggest that\npregabalin, at the usual prescribed dose, can cause mild but noticeable negative effects on cognitive function in healthy individuals. This information is important\nfor doctors to consider when prescribing pregabalin for long-term treatment, as they need to weigh the benefits against the potential risks to cognitive health.\nTable 6. Example summaries for Lay persona generated by the LLMs in a few-shot setting.\ncapacity for persona-based summarization without explicit fine-tuning. Self-refinement provided smaller improvements,\nhighlighting limited gains from self-critique.\nGPT-4o and LLaMA-3.1 showed the most stable performance across all personas. They were able to balance information\ndetail and clarity while maintaining the readability level desired for each persona. Both models achieved this by using clear\nand easy-to-follow language and including technical or scientific terms only when necessary, which preserved the main ideas\nwithout overwhelming the reader. For the lay summaries, such as the example shown in Table 6, GPT-4o produced fluent and\neasy-to-follow sentences that explained the main findings in simple terms while preserving the original meaning. LLaMA-3.1\nalso used appropriate information and simplification, ensuring summaries were appropriate for the persona, which was reflected\nin its consistently good readability and comprehensiveness metric results.\nGemini’s summaries, on the other hand, were often too long. They tended to include excessive background information\nor repeat details from the abstract. This made the summaries difficult to follow, particularly for lay readers. In the example\nshown in Table 6, Gemini clearly produced the longest summary. It also used technical words such as \"neurotoxicity,\" which\nGPT-4o and LLaMA-3.1 avoided by using simpler phrases to describe the same concept. While Gemini’s summaries were\ncomprehensive, this approach reduced clarity and made the text less suitable for readers without a technical background. Mistral\ntook the opposite approach. It created short and simple summaries that were easy to read. However, Mistral often extracted\nrelevant but unnecessary details for the persona from the abstract, such as the mention of a p-value, which is not useful for\nlay readers. It also often omitted definitions or brief explanations of key terms, which could make it harder for lay readers or\npre-med students to fully understand some biomedical concepts.\nWhile few-shot prompting remains the most reliable and generalizable strategy, future work should explore hybrid methods\nthat combine in-context examples with targeted feedback mechanisms to further improve faithfulness and information control in\npersona-based summarization. The PERCS dataset, therefore, serves as a comprehensive testbed for evaluating persona-guided,\ncontrollable biomedical summarization and for developing methods that balance readability, comprehensiveness, and factuality\nacross diverse audiences.\nData Availability\nThe dataset presented in this study is openly available in the Open Science Framework repository at https://doi.org/\n10.17605/OSF.IO/UMBJ7\n9/16\n"}, {"page": 10, "text": "Code Availability\nAll code required to reproduce the dataset and the benchmarking experiments reported in this study is available on GitHub at\nhttps://github.com/rohancsalvi/PERCS-Dataset.\nReferences\n1. Fox, S. & Duggan, M. Health online 2013. Health 2013, 1–55 (2013).\n2. Adams, S. Many in u.s. consider ai-generated health information useful and reliable | the annenberg public policy center of\nthe university of pennsylvania (2025).\n3. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Med-HALT: Medical domain hallucination test for large language models. In\nJiang, J., Reitter, D. & Deng, S. (eds.) Proceedings of the 27th Conference on Computational Natural Language Learning\n(CoNLL), 314–334, DOI: 10.18653/v1/2023.conll-1.21 (Association for Computational Linguistics, Singapore, 2023).\n4. Yadav, S., Cobeli, S. & Caragea, C. Towards understanding consumer healthcare questions on the web with semantically\nenhanced contrastive learning. In Proceedings of the ACM Web Conference 2023, 1773–1783 (2023).\n5. Yadav, S., Gupta, D., Ben Abacha, A. & Demner-Fushman, D. Reinforcement learning for abstractive question sum-\nmarization with question-aware semantic rewards. In Zong, C., Xia, F., Li, W. & Navigli, R. (eds.) Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers), 249–255, DOI: 10.18653/v1/2021.acl-short.33 (Association for\nComputational Linguistics, Online, 2021).\n6. of Medicine, N. L. Pubmed. https://pubmed.ncbi.nlm.nih.gov/ (2025). Accessed: 2025-11-13.\n7. Korsch, B. M., Gozzi, E. K. & Francis, V. Gaps in doctor-patient communication: I. doctor-patient interaction and patient\nsatisfaction. Pediatrics 42, 855–871 (1968).\n8. Friedman, C., Kra, P. & Rzhetsky, A. Two biomedical sublanguages: a description based on the theories of zellig harris. J.\nbiomedical informatics 35, 222–235 (2002).\n9. Stableford, S. & Mettger, W. Plain language: a strategic response to the health literacy challenge. J. public health policy\n28, 71–93 (2007).\n10. Tran, H., Yao, Z., Li, L. & Yu, H. ReadCtrl: Personalizing text generation with readability-controlled instruction\nlearning. In Padmakumar, V. et al. (eds.) Proceedings of the Fourth Workshop on Intelligent and Interactive Writing\nAssistants (In2Writing 2025), 19–36, DOI: 10.18653/v1/2025.in2writing-1.3 (Association for Computational Linguistics,\nAlbuquerque, New Mexico, US, 2025).\n11. Mullick, A. et al. On the persona-based summarization of domain-specific documents. In Ku, L.-W., Martins, A.\n& Srikumar, V. (eds.) Findings of the Association for Computational Linguistics: ACL 2024, 14291–14307, DOI:\n10.18653/v1/2024.findings-acl.849 (Association for Computational Linguistics, Bangkok, Thailand, 2024).\n12. Luo, Z., Xie, Q. & Ananiadou, S. Readability controllable biomedical document summarization. In Goldberg, Y., Kozareva,\nZ. & Zhang, Y. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2022, 4667–4680, DOI:\n10.18653/v1/2022.findings-emnlp.343 (Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,\n2022).\n13. Zhang, Z., Goldsack, T., Scarton, C. & Lin, C. ATLAS: Improving lay summarisation with attribute-based control. In Ku,\nL.-W., Martins, A. & Srikumar, V. (eds.) Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), 337–345, DOI: 10.18653/v1/2024.acl-short.32 (Association for Computational\nLinguistics, Bangkok, Thailand, 2024).\n14. Guo, Y., Qiu, W., Wang, Y. & Cohen, T. Automated lay language summarization of biomedical scientific reviews. In\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 35, 160–168 (2021).\n15. Goldsack, T., Zhang, Z., Lin, C. & Scarton, C. Making science simple: Corpora for the lay summarisation of scientific\nliterature. In Goldberg, Y., Kozareva, Z. & Zhang, Y. (eds.) Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, 10589–10604, DOI: 10.18653/v1/2022.emnlp-main.724 (Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, 2022).\n16. Attal, K., Ondov, B. & Demner-Fushman, D. A dataset for plain language adaptation of biomedical abstracts. Sci. Data 10,\n8 (2023).\n17. Basu, C., Vasu, R., Yasunaga, M. & Yang, Q. Med-easi: Finely annotated dataset and models for controllable simplification\nof medical texts. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, 14093–14101 (2023).\n10/16\n"}, {"page": 11, "text": "18. National Academies of Sciences, Engineering, and Medicine, Division of Behavioral and Social Sciences and Education\n& Committee on the Science of Science Communication: A Research Agenda. Communicating Science Effectively: A\nResearch Agenda (National Academies Press (US), Washington, DC, 2017). Copyright 2017 by the National Academy of\nSciences. All rights reserved.\n19. van Baalen, S. & Boon, M. Understanding disciplinary perspectives: a framework to develop skills for interdisciplinary\nresearch collaborations of medical experts and engineers. BMC Med. Educ. 24, 1000 (2024).\n20. Turbitt, O., Bevan, R. & Aboshokor, M. MDC at BioLaySumm task 1: Evaluating GPT models for biomedical lay\nsummarization. In Demner-fushman, D., Ananiadou, S. & Cohen, K. (eds.) Proceedings of the 22nd Workshop on\nBiomedical Natural Language Processing and BioNLP Shared Tasks, 611–619, DOI: 10.18653/v1/2023.bionlp-1.65\n(Association for Computational Linguistics, Toronto, Canada, 2023).\n21. Goldsack, T., Scarton, C., Shardlow, M. & Lin, C. Overview of the BioLaySumm 2024 shared task on the lay summarization\nof biomedical research articles. In Demner-Fushman, D., Ananiadou, S., Miwa, M., Roberts, K. & Tsujii, J. (eds.)\nProceedings of the 23rd Workshop on Biomedical Natural Language Processing, 122–131, DOI: 10.18653/v1/2024.\nbionlp-1.10 (Association for Computational Linguistics, Bangkok, Thailand, 2024).\n22. Goldsack, T., Scarton, C. & Lin, C. Leveraging large language models for zero-shot lay summarisation in biomedicine and\nbeyond. arXiv preprint arXiv:2501.05224 (2025).\n23. Agarwal, S., Akhtar, M. S. & Yadav, S. Overview of the peranssumm 2025 shared task on perspective-aware healthcare\nanswer summarization. In Proceedings of the Second Workshop on Patient-Oriented Language Processing (CL4Health),\n445–455 (2025).\n24. Salvi, R. C., Panigrahi, S., Jain, D., Yadav, S. & Akhtar, M. S. Towards understanding LLM-generated biomedical lay\nsummaries. In Ananiadou, S., Demner-Fushman, D., Gupta, D. & Thompson, P. (eds.) Proceedings of the Second Workshop\non Patient-Oriented Language Processing (CL4Health), 260–268, DOI: 10.18653/v1/2025.cl4health-1.22 (Association for\nComputational Linguistics, Albuquerque, New Mexico, 2025).\n25. Fang, B., Dai, X. & Karimi, S. Understanding faithfulness and reasoning of large language models on plain biomedical\nsummaries. In Al-Onaizan, Y., Bansal, M. & Chen, Y.-N. (eds.) Findings of the Association for Computational Linguistics:\nEMNLP 2024, 9890–9911, DOI: 10.18653/v1/2024.findings-emnlp.578 (Association for Computational Linguistics, Miami,\nFlorida, USA, 2024).\n26. Guo, Y., August, T., Leroy, G., Cohen, T. & Wang, L. L. APPLS: Evaluating evaluation metrics for plain language summa-\nrization. In Al-Onaizan, Y., Bansal, M. & Chen, Y.-N. (eds.) Proceedings of the 2024 Conference on Empirical Methods\nin Natural Language Processing, 9194–9211, DOI: 10.18653/v1/2024.emnlp-main.519 (Association for Computational\nLinguistics, Miami, Florida, USA, 2024).\n27. Joseph, S. et al. FactPICO: Factuality evaluation for plain language summarization of medical evidence. In Ku, L.-W.,\nMartins, A. & Srikumar, V. (eds.) Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 8437–8464, DOI: 10.18653/v1/2024.acl-long.459 (Association for Computational Linguistics,\nBangkok, Thailand, 2024).\n28. Goldsack, T., Zhang, Z., Tang, C., Scarton, C. & Lin, C. Enhancing biomedical lay summarisation with external knowledge\ngraphs. In Bouamor, H., Pino, J. & Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 8016–8032, DOI: 10.18653/v1/2023.emnlp-main.498 (Association for Computational Linguistics,\nSingapore, 2023).\n29. Templin, M. C. Certain language skills in children; their development and interrelationships. (University of Minnesota\nPress, 1957).\n30. McCarthy, P. M. An assessment of the range and usefulness of lexical diversity measures and the potential of the measure\nof textual, lexical diversity (MTLD). Ph.D. thesis, The University of Memphis (2005).\n31. Chall, J. S. & Dale, E. Readability revisited: The new dale-chall readability formula. (No Title) (1995).\n32. Coleman, M. & Liau, T. L. A computer readability formula designed for machine scoring. J. Appl. Psychol. 60, 283 (1975).\n33. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y. Bertscore: Evaluating text generation with bert. arXiv\npreprint arXiv:1904.09675 (2019).\n34. Lin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 74–81 (2004).\n35. Xu, W., Napoles, C., Pavlick, E., Chen, Q. & Callison-Burch, C. Optimizing statistical machine translation for text\nsimplification. Transactions Assoc. for Comput. Linguist. 4, 401–415 (2016).\n11/16\n"}, {"page": 12, "text": "36. Kincaid, J. P., Fishburne Jr, R. P., Rogers, R. L. & Chissom, B. S. Derivation of new readability formulas (automated\nreadability index, fog count and flesch reading ease formula) for navy enlisted personnel. Tech. Rep. (1975).\n37. Flesch, R. Flesch-kincaid readability test. Retrieved Oct. 26, 2007 (2007).\n38. Maddela, M., Dou, Y., Heineman, D. & Xu, W. Lens: A learnable evaluation metric for text simplification. arXiv preprint\narXiv:2212.09739 (2022).\n39. Laban, P., Schnabel, T., Bennett, P. N. & Hearst, M. A. Summac: Re-visiting nli-based models for inconsistency detection\nin summarization. Transactions Assoc. for Comput. Linguist. 10, 163–177 (2022).\nAppendix\nEvaluation Guidelines for Expert Error-Based Summary Annotation\nObjective:\nEvaluate model-generated plain language summaries of biomedical research abstracts. Annotators assess each summary for\naccuracy, completeness, readability, and reader appropriateness, and label specific errors based on the taxonomy below.\nMaterials Provided:\n• Original abstract of the medical article\n• Model-generated plain language summary\nEvaluation Procedure:\n1. Review the Abstract: Read the abstract carefully to understand its methodology, key findings, and context.\n2. Evaluate the Summary: Compare the summary to the abstract and your medical expertise.\n3. Annotate Errors: Highlight problematic segments in the summary, tag each with the corresponding error type, and\nsuggest corrections when necessary.\nError Categories:\n1. New Information Errors (Expert Judgment Required)\n• Incorrect Definitions: Inaccurate or misleading explanations of medical terms or concepts.\n• Incorrect Synonyms: Inappropriate simplification or substitution that changes medical meaning.\n• Incorrect Background Information: Medically inaccurate or misleading added context.\n2. Inference Errors (Check Against Abstract)\n• Contradiction: Information directly conflicts with the abstract.\n• Omission: Important findings or details from the abstract are missing.\n• Jumping to Conclusions: Unsupported generalizations or causal claims.\n• Misinterpretation: Inaccurate paraphrasing or simplification of abstract content.\n• Entity Errors: Factual errors in names, numbers, dosages, or statistics.\n3. Structure and Readability Errors\n• Structure Error: Poor organization or illogical flow.\n• Grammatical Error: Issues in spelling, punctuation, or sentence structure.\n• Persona Relevance: Language is too technical or too simplistic for the target reader.\n4. Hallucinations\n• Hallucination: Fabricated, irrelevant, or nonsensical content not supported by the abstract or general medical knowledge.\n12/16\n"}, {"page": 13, "text": "Annotation Instructions:\n• Highlight the erroneous segment in the summary.\n• Tag the segment with the appropriate error label (e.g., Contradiction, Incorrect Definition).\n• Provide a corrected version using either the abstract or medical expertise.\nTable 7. Topic Distribution of PLOS abstracts in PERCS: Test Set (75 abstracts) and Entire Dataset (250 abstracts)\nDisease\nTest\nEntire\nCancer\n10\n25\nDiabetes\n10\n25\nTuberculosis\n10\n25\nHepatitis\n10\n25\nChronic Kidney Disease\n5\n25\nHIV/ AIDS\n5\n25\nCardiovascular Diseases\n5\n20\nObesity\n4\n15\nAlzheimer’s\n4\n15\nHypertension\n3\n10\nStroke\n3\n10\nPneumonia\n2\n10\nMalaria\n2\n10\nSalmonella/Typhoid\n2\n10\nTotal\n75\n250\n13/16\n"}, {"page": 14, "text": "Table 8. Initial summary generation prompts for each persona used in the creation of the PERCS dataset.\nPersona\nPrompt\nExpert\nYou are a seasoned subject matter expert in the biological sciences and are preparing a summary of a research abstract for an expert audience in\nthe same field. You decide to generate this summary with the following key principles in mind:\n1) The target audience is experts and professionals within the same scientific field who are seeking a concise overview of the study.\n2) Do not simplify technical terms or scientific concepts; preserve their complexity and meaning.\n3) Maintain a formal and objective tone suitable for a professional scientific discourse.\n4) The sentence structure should be academically appropriate and support a coherent, logical flow of information.\n5) Ensure the summary includes the core findings, methodology, and significance of the study without omitting critical data.\n6) The text must remain factually accurate, including all relevant terminology, experimental parameters, numeric results, and implications.\nThe word count should not exceed 250 words.\nResearcher\nYou are a knowledgeable science communicator with interdisciplinary expertise. Your task is to write a clear and comprehensive summary of a\nscientific abstract for researchers outside the specific field of the study but with a solid understanding of general scientific principles. When\nsummarizing, follow these key principles:\n1) The audience consists of researchers in other fields than biology and medicine who may not be familiar with domain-specific terminology or\nmethods.\n2) Use clear and accessible language. Avoid unnecessary technical jargon; when technical terms are necessary, explain them briefly in context.\n3) Accurately represent the study’s main findings, methodology, and results.\n4) Maintain a professional but approachable tone. Aim for clarity and precision without overly simple or domain-specific language.\n5) Structure the summary as a paragraph. Do not use bullet points, numbered lists, or Q&A formats.\n6) Focus on the scientific content only. Do not include commentary about the summarization process or subjective judgement of the study’s\nimportance.\nThe final summary should not exceed 350 words.\nPre-Med\nYou are an experienced academic tutor in the medical sciences. Your task is to write a summary of a research abstract specifically for pre-med\nstudents. When summarizing, follow these key principles:\n1) The audience has foundational knowledge of biology and chemistry.\n2) Use clear, simple language, and avoid technical jargon unless essential; when used, briefly explain the term.\n3) Maintain accuracy while making the information accessible and engaging for pre-med students.\n4) Ensure the summary flows logically and reads like a helpful explanation, not as dense scientific writing.\n5) Ensure the summary accurately conveys the study’s key findings, methodology, and relevance to medical sciences.\n6) The summary should be written as a continuous paragraph avoiding bullet points, lists, or Q&A formats.\nThe word count of the summary should not exceed 350 words. Do not include any concluding commentary for the students, notes on the\nsummarization process, or any self-determined importance of the study for pre-med students.\nLayman\nYou are a biology teacher in a high school and want to teach students in 10th grade about a research study. Your goal is to convey the information\nin the abstract in plain and easy-to-understand language that students can follow. You decide to generate a plain text for the same abstract keeping\nin mind what makes a text simple and easy to understand:\n1) Avoid as much scientific jargon as possible. If unavoidable, replace it with easy-to-understand synonyms.\n2) Provide explanations and definitions for complex biological terms, and include simple real-life examples to aid understanding.\n3) Keep sentence structure simple and ensure the text has a coherent flow.\n4) The word count cannot exceed 350 words.\n5) Include all important points, and if words are replaced by simpler terms, connect them to original words using brackets.\n6) Ensure factual accuracy, including definitions, synonyms, numeric figures, and findings.\nThe final summary should not exceed 350 words.\n14/16\n"}, {"page": 15, "text": "Table 9. Performance of Benchmarking Approaches for the Pre-med Persona on the PERCS Dataset Using Automatic\nEvaluation Metrics\nMethod\nModel\nR-1 ↑\nR-2 ↑\nR-L ↑\nSARI ↑\nFKGL\nDCRS\nCLI\nLENS\nSC ↑\nZero-shot\nGPT-4\n0.5916\n0.2380\n0.3434\n49.5828\n17.91\n12.77\n19.44\n46.2652\n0.2904\nGemini\n0.5960\n0.2495\n0.3424\n48.3962\n15.47\n12.69\n18.34\n49.8001\n0.2772\nMistral\n0.5819\n0.2629\n0.3622\n49.3190\n16.08\n11.80\n17.53\n50.3914\n0.3498\nLLaMA 3\n0.5900\n0.2568\n0.3429\n49.0921\n17.32\n12.28\n18.21\n49.5902\n0.2759\nFew-shot\nGPT-4\n0.6089\n0.2550\n0.3629\n50.4423\n15.74\n11.12\n16.73\n45.8277\n0.3014\nGemini\n0.6024\n0.2683\n0.3648\n48.3962\n15.43\n12.57\n18.03\n53.2864\n0.3006\nMistral\n0.5911\n0.2686\n0.3688\n49.2427\n16.04\n11.80\n17.46\n48.1884\n0.3593\nLLaMA 3\n0.6149\n0.2926\n0.3823\n51.9053\n16.72\n11.78\n17.69\n53.6619\n0.2969\nSelf-Refine\nGPT-4\n0.5624\n0.2100\n0.3126\n47.8304\n15.83\n11.43\n17.59\n47.0772\n0.2684\nGemini\n0.5564\n0.2084\n0.3011\n46.4857\n14.82\n10.76\n16.42\n45.4320\n0.2613\nMistral\n0.4980\n0.2028\n0.2800\n46.6206\n15.42\n10.52\n17.01\n45.9425\n0.3249\nLLaMA 3\n0.4973\n0.2073\n0.2748\n48.7969\n16.42\n10.07\n15.98\n46.0638\n0.2568\nTable 10. Performance of Benchmarking Approaches for the Researcher Persona on the PERCS Dataset Using Automatic\nEvaluation Metrics\nMethod\nModel\nR-1 ↑\nR-2 ↑\nR-L ↑\nSARI ↑\nFKGL\nDCRS\nCLI\nLENS\nSC ↑\nZero-shot\nGPT-4\n0.6207\n0.2742\n0.3831\n43.9918\n15.87\n11.00\n16.48\n51.2367\n0.2536\nGemini\n0.6093\n0.2838\n0.3881\n43.1912\n15.53\n11.19\n16.88\n53.5571\n0.2735\nMistral\n0.6165\n0.3039\n0.4086\n49.5102\n15.34\n10.79\n16.39\n55.6439\n0.3227\nLLaMA 3\n0.6120\n0.2880\n0.3719\n48.4270\n15.19\n10.02\n14.95\n57.7103\n0.2769\nFew-shot\nGPT-4\n0.6259\n0.2778\n0.3878\n49.1682\n16.14\n11.10\n16.86\n59.1560\n0.3074\nGemini\n0.6242\n0.2956\n0.4000\n44.9244\n15.28\n11.01\n16.42\n54.1410\n0.2769\nMistral\n0.6122\n0.3072\n0.3978\n49.4988\n15.05\n10.65\n15.93\n59.3144\n0.3928\nLLaMA 3\n0.6164\n0.2987\n0.3838\n48.2722\n15.40\n10.14\n15.23\n60.6989\n0.2710\nSelf-Refine\nGPT-4\n0.5624\n0.2100\n0.3126\n46.3540\n15.83\n11.43\n17.59\n51.3194\n0.2815\nGemini\n0.5564\n0.2084\n0.3011\n46.6575\n14.82\n10.76\n16.42\n50.6804\n0.2803\nMistral\n0.4980\n0.2028\n0.2800\n46.6433\n15.42\n10.52\n17.01\n51.2360\n0.3638\nLLaMA 3\n0.4973\n0.2073\n0.2748\n48.1174\n16.42\n10.07\n15.98\n49.0160\n0.2651\nTable 11. Performance of Benchmarking Approaches for the Lay person Persona on the PERCS Dataset Using Automatic\nEvaluation Metrics\nMethod\nModel\nR-1 ↑\nR-2 ↑\nR-L ↑\nSARI ↑\nFKGL\nDCRS\nCLI\nLENS\nSC ↑\nZero-shot\nGPT-4\n0.5972\n0.2439\n0.3390\n53.0256\n10.37\n8.54\n11.30\n51.7573\n0.2962\nGemini\n0.6029\n0.2624\n0.3691\n53.0180\n10.04\n8.71\n11.66\n52.5033\n0.3031\nMistral\n0.5235\n0.2068\n0.3186\n48.3885\n12.38\n9.37\n12.93\n62.7845\n0.3626\nLLaMA 3\n0.5900\n0.2585\n0.3527\n52.0627\n12.07\n8.59\n11.86\n53.1642\n0.2815\nFew-shot\nGPT-4\n0.6016\n0.2476\n0.3554\n53.5134\n11.22\n8.94\n11.99\n75.0276\n0.2987\nGemini\n0.6018\n0.2611\n0.3773\n53.2864\n11.00\n8.97\n12.17\n69.1026\n0.3115\nMistral\n0.5073\n0.1892\n0.3014\n47.2191\n13.08\n9.81\n13.84\n64.7531\n0.3571\nLLaMA 3\n0.5841\n0.2542\n0.3499\n53.6619\n12.06\n8.70\n11.99\n78.4790\n0.2882\nSelf-Refine\nGPT-4\n0.5288\n0.1727\n0.2696\n48.3749\n11.24\n9.47\n12.84\n71.6981\n0.2553\nGemini\n0.5422\n0.1874\n0.2756\n47.9891\n9.44\n8.45\n11.14\n71.5003\n0.2616\nMistral\n0.4750\n0.1609\n0.2523\n47.1213\n12.49\n9.55\n13.71\n57.7433\n0.2868\nLLaMA 3\n0.4233\n0.1366\n0.2008\n47.4832\n11.84\n8.32\n12.07\n62.4744\n0.2433\n15/16\n"}, {"page": 16, "text": "Table 12. Performance of Benchmarking Approaches for the Expert Persona on the PERCS Dataset Using Automatic\nEvaluation Metrics\nMethod\nModel\nR-1 ↑\nR-2 ↑\nR-L ↑\nSARI ↑\nFKGL\nDCRS\nCLI\nLENS\nSC ↑\nZero-shot\nGPT-4\n0.6000\n0.2905\n0.3997\n42.1944\n17.91\n12.77\n19.44\n51.2367\n0.3283\nGemini\n0.6348\n0.3451\n0.4511\n45.1591\n15.47\n12.69\n18.34\n53.5571\n0.4193\nMistral\n0.6478\n0.3911\n0.4758\n48.2700\n16.08\n11.80\n17.53\n55.6439\n0.4688\nLLaMA 3\n0.6594\n0.3928\n0.4738\n47.8625\n17.32\n12.28\n18.21\n57.7103\n0.3586\nFew-shot\nGPT-4\n0.5418\n0.2416\n0.3555\n41.4523\n15.57\n11.39\n16.84\n59.1560\n0.3459\nGemini\n0.6599\n0.3815\n0.4912\n44.6463\n15.43\n12.57\n18.03\n54.1410\n0.3795\nMistral\n0.6625\n0.4035\n0.4946\n48.4338\n16.04\n11.80\n17.46\n59.3144\n0.4878\nLLaMA 3\n0.6769\n0.4198\n0.5037\n45.4880\n16.72\n11.78\n17.69\n60.6989\n0.3068\nSelf-Refine\nGPT-4\n0.5577\n0.2547\n0.3535\n42.4489\n17.30\n12.76\n19.39\n51.3194\n0.2946\nGemini\n0.6010\n0.3126\n0.4184\n44.1335\n15.36\n12.49\n17.91\n50.6804\n0.3397\nMistral\n0.6552\n0.3973\n0.4852\n48.3519\n16.06\n11.80\n17.50\n57.4792\n0.4783\nLLaMA 3\n0.4732\n0.2345\n0.2981\n43.1135\n17.33\n11.95\n18.10\n49.0160\n0.2549\n16/16\n"}]}