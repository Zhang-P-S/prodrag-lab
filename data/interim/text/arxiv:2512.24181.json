{"doc_id": "arxiv:2512.24181", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.24181.pdf", "meta": {"doc_id": "arxiv:2512.24181", "source": "arxiv", "arxiv_id": "2512.24181", "title": "MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring", "authors": ["Qipeng Wang", "Rui Sheng", "Yafei Li", "Huamin Qu", "Yushi Sun", "Min Zhu"], "published": "2025-12-30T12:31:53Z", "updated": "2026-01-04T11:47:36Z", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.24181v2", "url_pdf": "https://arxiv.org/pdf/2512.24181.pdf", "meta_path": "data/raw/arxiv/meta/2512.24181.json", "sha256": "e45e5fa4325b1e2af4c1522fc1a175d8075eb075205afd95aca9972de78acae2", "status": "ok", "fetched_at": "2026-02-18T02:23:35.281689+00:00"}, "pages": [{"page": 1, "text": "MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs\nand Information-Guided Inquiring\nQipeng Wang1, Rui Sheng2, Yafei Li2, Huamin Qu2, Yushi Sun2, Min Zhu1,\n1Sichuan University, Chengdu, China, 2HKUST, Hong Kong SAR, China\nCorrespondence: zhumin@scu.edu.cn, ysunbp@connect.ust.hk\nAbstract\nRecent advancements in Large Language Mod-\nels (LLMs) have demonstrated significant\npromise in clinical diagnosis. However, cur-\nrent models struggle to emulate the iterative,\ndiagnostic hypothesis-driven reasoning of real\nclinical scenarios. Specifically, current LLMs\nsuffer from three critical limitations: (1) gen-\nerating hallucinated medical content due to\nweak grounding in verified knowledge, (2) ask-\ning redundant or inefficient questions rather\nthan discriminative ones that hinder diagnos-\ntic progress, and (3) losing coherence over\nmulti-turn dialogues, leading to contradictory\nor inconsistent conclusions. To address these\nchallenges, we propose MedKGI, a diagnos-\ntic framework grounded in clinical practices.\nMedKGI integrates a medical knowledge graph\n(KG) to constrain reasoning to validated medi-\ncal ontologies, selects questions based on infor-\nmation gain to maximize diagnostic efficiency,\nand adopts an OSCE-format structured state to\nmaintain consistent evidence tracking across\nturns.\nExperiments on clinical benchmarks\nshow that MedKGI outperforms strong LLM\nbaselines in both diagnostic accuracy and in-\nquiry efficiency, improving dialogue efficiency\nby 30% on average while maintaining state-of-\nthe-art accuracy.\n1\nIntroduction\nLarge Language Models (LLMs) are increasingly\ndemonstrating their value as powerful tools for clin-\nical diagnosis (Wang et al., 2023; Singhal et al.,\n2025; Lin et al., 2025). However, real-world clini-\ncal reasoning is an iterative process in which doc-\ntors need to strategically construct diagnostic hy-\npotheses and gather clinical information in a se-\nquential manner to make the final decision. Current\nLLMs often struggle in such iterative, hypothesis-\ndriven settings due to a fundamental discrepancy\nbetween their probabilistic, token-by-token genera-\ntion and the systematic rigor required for clinical\ndeduction (Hager et al., 2024).\nFigure 1: Comparison of diagnostic dialogues and the\nMedKGI workflow. Left: The dialogues from the base-\nline LLMs. Right: The dialogues from the proposed\nMedKGI framework. Bottom: The MedKGI workflow.\nThis gap leads to several critical limitations: a\ntendency to produce hallucinations by prioritizing\nplausible patterns over verified knowledge (Zhu\net al., 2025); ineffective questioning due to the lack\nof an explicit reasoning framework (Li et al., 2025);\nand context overloading in multi-turn dialogues\ncaused by their associative nature (Savage et al.,\n2024). As illustrated in Figure 1 (left), these limi-\ntations often result in redundant and non-strategic\ndiagnostic dialogues by baseline LLMs, in contrast\nto the structured trajectory of rigorous medical rea-\nsoning.\nTo bridge this gap, we ground our method in es-\ntablished differential diagnosis frameworks rather\nthan language patterns. Differential diagnosis is\ninherently an iterative process of systematically\nweighing competing hypotheses against clinical\nevidence (Hannigen, 2018), which is the opposite\nof associative LLM reasoning. Accordingly, we\nintegrate three key principles:\n1\narXiv:2512.24181v2  [cs.CL]  4 Jan 2026\n"}, {"page": 2, "text": "• Knowledge-Anchored Hypothesis Gener-\nation: Inspired by the clinical practice of\ngrounding initial differentials in established\nmedical knowledge (Zuo et al., 2025), we in-\ntegrate a medical knowledge graph (KG) to\ngenerate diagnostic candidates based on veri-\nfied disease–symptom relationships.\n• Strategic Uncertainty Reduction: Follow-\ning the differential diagnosis principle of prior-\nitizing high-yield findings, we adopt an infor-\nmation gain-based questioning strategy (Liu\net al., 2025) to select the most discriminative\nquestions, thereby minimizing diagnostic un-\ncertainty.\n• Iterative Evidence Refinement: To simulate\na doctor’s belief updating process as new evi-\ndence emerges, we implement a state-tracking\nmechanism that maintains a coherent diag-\nnostic record, enabling consistent hypothesis\nmanagement while mitigating context over-\nloading in long dialogues (Xu et al., 2024).\nBuilding on these principles, we propose Med-\nKGI, a diagnostic reasoning framework designed\nto emulate the systematic inquiry of human clin-\nicians within the LLM paradigm. As shown in\nFigure 1, MedKGI integrates a medical knowledge\ngraph to anchor all diagnostic reasoning in veri-\nfied medical ontologies, thereby mitigating hallu-\ncinations. Building on this grounded knowledge\n(sub-graph), it employs an information gain–based\nquestion selection strategy. This strategy evaluates\ncandidate questions by their expected reduction in\ndiagnostic uncertainty, enabling MedKGI to priori-\ntize the most discriminative inquiries and optimize\ndiagnostic efficiency. Finally, MedKGI adopts the\nObjective Structured Clinical Examination (OSCE)\nformat (Cushing et al., 2014) to maintain a struc-\ntured diagnostic state. This state tracks and updates\naccumulated evidence across dialogue turns, which\nmitigates context overloading and ensures reason-\ning consistency.\nOur key contributions are:\n• A Systematic, Hypothesis-Driven Diagnos-\ntic Framework: We propose MedKGI, a\nnovel framework that explicitly models the\niterative, hypothesis-driven process of differ-\nential diagnosis, bridging the gap between\nLLMs’ generative nature and the analytical\nrigor of differential diagnosis.\n• Knowledge-Anchored & Strategically Op-\ntimized Reasoning: MedKGI uniquely inte-\ngrates a medical knowledge graph to prevent\nhallucinations and employs an information\ngain–based questioning strategy to maximize\ndiagnostic efficiency, grounding reasoning in\nverified ontologies.\n• Superior Empirical Performance: Exten-\nsive experiments show that MedKGI outper-\nforms state-of-the-art baselines, achieving\nhigh diagnostic accuracy while improving di-\nalogue efficiency by 30%.\n2\nRelated Work\nClinical dialogue involves dynamic, multi-turn\ninformation exchange and hypothesis refine-\nment (Nori et al., 2025). We categorize existing\napproaches into LLM-driven sequential diagno-\nsis, knowledge-augmented frameworks, and agent-\nbased clinical frameworks.\nLLM-Driven Sequential Diagnosis.\nEarly\nmethods primarily leverage LLMs’ reasoning capa-\nbilities, enhanced through fine-tuning or reinforce-\nment learning (RL) to improve medical diagno-\nsis. Chain-of-Thought (CoT) prompting has been\nwidely adopted to elicit diagnostic reasoning (Dai\net al., 2025). Domain-specialized models for di-\nagnosis like Huatuo (Wang et al., 2023) and Med-\nitron (Chen et al., 2023) are pre-trained on medical\ncorpora. AgentClinic simulates doctor–patient in-\nteractions but relies on static prompting without dy-\nnamic evidence tracking (Schmidgall et al., 2024).\nRecent works have focused on inquiry strategies:\nMedAgent (Kim et al., 2025b) formulates diagno-\nsis as multi-agent collaboration while PATIENCE\n(Zhu et al., 2025) incorporates Bayesian active\nlearning for interactive questioning.\nHowever,\nthese model-centric approaches often struggle with\nhallucinations and struggle with precision in open-\nended, multi-turn scenarios due to a lack of external\ngrounding (Zuo et al., 2025).\nKnowledge-Augmented Approaches. To mit-\nigate the limitations of pure LLM-based reason-\ning, recent work has integrated external knowledge.\nRAG-based methods like MRD-RAG (Sun et al.,\n2025) leverages the tree-structure medical KG for\ndifferential diagnosis, while ClinicalRAG (Lu et al.,\n2024) fuses structured and unstructured medical\nknowledge. Beyond retrieval, some methods ex-\nplicitly model diagnostic reasoning over KGs using\nsearch or planning. For instance, Unit of Thought\n2\n"}, {"page": 3, "text": "Figure 2: An illustration of the iterative hypothesis refinement process and its corresponding clinical differential\ndiagnosis example.\n(UoT) (Hu et al., 2024) decomposes clinical rea-\nsoning into discrete, verifiable knowledge units\ngrounded in a KG. However, these approaches treat\nevidence retrieval statically, lacking dynamic state-\ntracking for handling diagnostic contexts (Wang\net al., 2025).\nAgent-based Clinical Frameworks.\nMulti-\nagent systems simulate clinical workflows by de-\ncomposing tasks across specialized agents for\nsymptom collection, evidence retrieval, and rea-\nsoning. DDO (Jia et al., 2025b) uses a diagnosis\nagent, a strategy agent, and a patient agent for stage-\nspecific inquiry, while MeDDxAgent (Rose et al.,\n2025) integrates a control agent, a history agent,\nand a knowledge agent to simulate clinical diagnos-\ntic processes with external knowledge. MEDIQ (Li\net al., 2024) introduces a query-planning agent that\nprioritizes questions based on symptom severity,\nCoD (Chen et al., 2025a) coordinates diagnostic\nagents through a consensus-driven protocol, and\nDoctorAgent-RL (Feng et al., 2025) models con-\nsultations as an RL process under uncertainty. De-\nspite these advances, existing multi-agent frame-\nworks lack criteria for question selection, relying\non heuristic role-playing rather than information-\ntheoretic objectives to optimally reduce diagnostic\nuncertainty (Chen et al., 2025b).\nSummary. Existing approaches provide flexi-\nbility, factual accuracy, and workflow simulation,\nthere is no single existing approach that effectively\nunifies: (1) KG-grounded reasoning, (2) structured\nstate tracking for context management, and (3)\ninformation-theoretic inquiry optimization. Our\nMedKGI framework addresses these challenges by\nintegrating KGs with information gain-driven selec-\ntion within a structured state tracking mechanism.\n3\nProblem Definition\nThe multi-step clinical diagnosis can be modeled\nas an iterative decision-making process that refines\nhypotheses over T dialogue turns (Figure 2). The\nprocess begins with the patient profile P, which in-\ncludes demographics and chief complaints. Over a\nsequence of dialogue turns t, the framework main-\ntains a dynamic clinical state. At turn t, given P\nand accumulated evidence Et, the objective is to es-\ntimate the posterior probability for each candidate\ndisease D ∈Dt, where Dt = {D1, D2, . . . , Dn}\nthrough evidence collection. The proactive symp-\ntom inquiry is defined as identifying the optimal\ninquiry s that maximizes the expected reduction\nin diagnostic uncertainty. This mechanism enables\nthe framework to iteratively generate and refine Dt\nuntil the target disease D∗is reached:\nP(D | P, Et) →δ(D, D∗)\n4\nMethodology\nAs illustrated in the differential diagnosis scenario\n(Figure 2), given patient profiles and symptoms,\nthe objective is to narrow a differential diagnosis\nset toward the target disease D∗through sequen-\ntial evidence collection. Unlike static classifica-\ntion, this scenario requires actively navigating a hy-\npothesis space, simulating a doctor’s cognitive pro-\ncess (Polotskaya et al., 2024). Specifically, we for-\nmulate the iterative refinement process (Figure 3)\nas a knowledge-guided active diagnostic frame-\nwork, where a doctor agent iteratively refines the\ndifferential diagnosis set by strategically gathering\ndiscriminative evidence.\nTo realize this iterative and knowledge-driven\nprocess, we design the MedKGI workflow integrat-\ning three components: (1) Entity Extraction &\nAlignment: maps the diagnosis input and gener-\nated hypothesis to a medical KG, constructing a di-\nagnostic subgraph grounded in clinically validated\ndisease-symptom relationships to mitigate hallu-\ncinations, (2) Information Gain-Based Inquiry:\ncalculates information gain to identify discrimi-\nnative symptoms, ensuring each clinical inquiry\nmaximally reduces diagnostic uncertainty and im-\nproves diagnostic efficiency, (3) OSCE-Aligned\nDiagnostic Record Management: organizes accu-\nmulated evidence into an OSCE-format diagnostic\nrecord, maintaining a consistent state to prevent\n3\n"}, {"page": 4, "text": "Figure 3: An overview of MedKGI framework. Given a patient’s chief complaint, MedKGI iteratively refines\ndifferential diagnosis through (1) medical knowledge graph alignment, (2) information gain–driven symptom inquiry\nto minimize diagnosis uncertainty, and (3) OSCE-aligned diagnostic records for coherent evidence tracking. A\nhypothesis-driven termination policy ensures diagnostic efficiency.\ncontext overloading.\n4.1\nDiagnosis Workflow of MedKGI\nAt any diagnostic turn t, the doctor agent receives\nthe following as input:\n• The Patient Profile (e.g., age, sex, and chief\ncomplaints) provided at the beginning;\n• The Patient’s Recent Utterance, which may\ncontain new symptoms or responses to prior\nquestions;\n• The Accumulated Diagnostic Record, a struc-\ntured OSCE-aligned summary (detailed in\nSection 4.4) containing confirmed symptoms,\nmedical history, and examination findings up\nto turn t −1.\nBased on this context, the doctor agent generates\ntwo key outputs:\n• A natural language Clinical Inquiry to elicit\ndiscriminative evidence;\n• A Differential Diagnosis Set outputted by doc-\ntor agent;\nTo ensure efficiency and diagnostic precision,\ndiagnostic process concludes when one of the fol-\nlowing termination conditions is met\n• Turn Limit. If the dialogue reaches Tmax\nturns, the doctor agent must issue a final diag-\nnosis.\n• Stagnation Detection. If the differential di-\nagnosis set remains unchanged for n consec-\nutive turns, doctor agent is prompted to seek\nevidence that could refute the current hypoth-\nesis. If no such symptom exists, the doctor\nagent outputs the final diagnosis.\n4.2\nEntity Extraction and Knowledge Graph\nAlignment\nAt each turn, the LLM first proposes a preliminary\ndifferential diagnosis set, which is then mapped to\nthe medical KG. To align medical terms mentioned\nin the patient utterance with standardized entities\nin the KG, we implement a multi-stage alignment\npipeline:\n• Exact Matching. For standard medical terms\nin the dialogue, we query the KG for an en-\ntity that exactly matches the candidate disease\nname.\n• Edit-Distance Matching. For minor spelling\nvariations or errors, we apply the Levenshtein\nDistance (Levenshtein, 1965) to identify ap-\nproximate matches, allowing a maximum edit\ndistance of 3.\n• Semantic Embedding Matching. For con-\nceptually equivalent but lexically divergent\nexpressions, we leverage the pre-trained Pub-\nMedBERT (Gu et al., 2021) to generate vector\nembeddings for the candidate disease name\nand all KG disease entity names. We calculate\ncosine similarity and retrieve the top-ranked\nentity, discarding matches with a similarity\n4\n"}, {"page": 5, "text": "score below a threshold τ = 0.85 to ensure\nalignment quality.\n4.3\nInformation Gain-Based Symptom\nSelection\nOnce the candidate diseases are mapped to the KG,\nwe construct a task-specific diagnostic subgraph\nGsub = (Vsub, Esub), comprising the current dif-\nferential diagnosis set and their directly connected\nsymptom nodes. To strategically reduce diagnos-\ntic uncertainty, MedKGI selects symptom queries\nthat maximize information gain (Quinlan, 1986).\nThis selection is made over the diagnostic subgraph\nGsub and is conditioned on the patient’s reported\npositive and negative symptoms (Spos, Sneg).\n4.3.1\nPosterior Disease Probability Estimation\nFirst, we establish the context by constructing a\ndiagnostic subgraph Gsub = (Vsub, Esub), where\nVsub = Sn\ni=1{Di} ∪N(Di) consists of the dif-\nferential diagnosis set D = {D1, D2, . . . , Dn}\nand all symptoms connected to them in the KG:\nN(Di) = {s ∈VKG ∨s ∈S : (Di, s) ∈EKG}.\nWe initialize the prior probability of each candidate\ndisease Di based on its average semantic similarity\nto the confirmed symptoms Spos extracted from the\ncurrent dialogue:\nP(Di) =\n1\n| Spos |\nX\nS∈Spos\nsemantic_sim(s, Di)\nwhere semantic_sim(·, ·) denotes cosine similar-\nity between the PubMedBERT (Gu et al., 2021)\nembeddings of symptom s ∈Spos and disease Di.\nGiven the accumulated observed symptoms Spos\nand Sneg, we update disease beliefs over candidate\ndisease D using Bayes’ theorem:\nP(Di | Spos, Sneg) = P(Spos, Sneg | Di) · P(Di)\nP(Spos, Sneg)\nAssuming conditional independence among\nsymptoms given a disease, the likelihood factor-\nizes as:\nP(Spos, Sneg) =\nn\nX\nj=1\nP(Spos, Sneg | Dj) · P(Dj)\nwhere we adopt a uniform conditional probability\nmodel: P(s, | Di) = 1/ |N(Di)| for all symptoms\ns ∈N(Di).\n4.3.2\nInformation Gain Computation\nFor the current differential disease set D with poste-\nrior probabilities P(Di | Spos, Sneg), we compute\nthe prior diagnostic uncertainty using the Shannon\nEntropy:\nH(D) = −\nn\nX\ni=1\nP(Di) log P(Di)\nFor any symptom s, we compute its marginal prob-\nability and the resulting posterior disease distribu-\ntions:\nP(s) =\nn\nX\ni=1\nP(s | Di)P(Di),\nP(Di | s) = P(s | Di)P(Di)\nP(s)\nFinally, the Information Gain of asking about\nsymptom s is defined as the expected reduction in\nentropy:\nIG(s) = H(D) −H(D | s)\nwhere H(D | s) represents the expected condi-\ntional entropy after observing symptom s:\nH(D | s) = P(s)H(D | s+) + P(¬s)H(D | s−)\nand H(D | s+) and H(D | s−) are the entropies if\nthe symptom is observed positive or negative, re-\nspectively. The doctor agent then selects the top-k\nsymptoms with the highest IG(s) for strategic in-\nquiry, ensuring that each subsequent question max-\nimally reduces uncertainty. Compared to methods\nthat rely on predefined question templates or fixed\ninquiry sequences, MedKGI dynamically adapts\nquestions based on the evolving diagnostic hypoth-\nesis, enabling more targeted and efficient informa-\ntion gathering.\n4.4\nConsistent State by Diagnostic Record\nManagement\nTo support coherent and consistent reasoning, we\nemploy the LLM to generate and maintain a struc-\ntured diagnostic record in JSON format, aligned\nwith the Objective Structured Clinical Examination\n(OSCE) standard. At the beginning of each dia-\nlogue session, we initialize an empty diagnostic\nrecord following a predefined schema, including\nchief complaint, symptoms, and recent medical\nexaminations.\n5\n"}, {"page": 6, "text": "At each turn, MedKGI takes the latest diagnos-\ntic record and patient utterance as input. Then,\nMedKGI outputs an updated diagnostic record that\nintegrates new evidence while preserving clinical\ncontext. This accumulated diagnostic record pre-\nvents context overloading across turns, which com-\nmonly occurs when context windows accumulate\nredundant information in vanilla prompting meth-\nods (Schmidgall et al., 2024).\n5\nEvaluation\n5.1\nExperiment Setup\nDatasets. We conducted experiments on two medi-\ncal QA benchmarks: MedQA (Jin et al., 2021) and\nCMB (Wang et al., 2024; cme, 2023). To further\nassess multi-modal clinical reasoning, we addition-\nally introduce a dataset of real-world cases from\nthe NEJM Image Challenge1. We followed the\nsettings of AgentClinic (Schmidgall et al., 2024)\nto simulate the multi-agent medical consultation\nscenarios based on the cases in MedQA and CMB.\nWe denote the processed datasets as agent-MedQA\nand agent-CMB.\nBaselines. We compared MedKGI against 12\nbaselines across four categories: (1) Dialog-Based\nMethods: AgentClinic (Schmidgall et al., 2024),\nCoT (Chain-of-Thought), Huatuo (Wang et al.,\n2023) and Meditron (Chen et al., 2023); (2) KG-\nBased Methods: MCTS-BT (Ding et al., 2025),\nMCTS-MV (Ding et al., 2025), UoT (Hu et al.,\n2024); (3) Agent-Based Methods: MEDIQ (Li\net al., 2024), CoD (Chen et al., 2025a), DDO\n(Jia et al., 2025b), and MEDDxAgent (Rose et al.,\n2025); and (4) SFT-Based Methods including mod-\nels fine-tuned on domain-specific dialogues. Spe-\ncialized medical LLMs (e.g., Huatuo and Meditron)\nare not evaluated on the NEJM benchmark if they\nlack the ability of multi-modal analysis. Further-\nmore, SFT and SFT-GT are excluded from NEJM\nevaluation due to the lack of multimodal dialogue\ntraining data required for effective fine-tuning. A\ncomplete description of all individual models and\ntheir configurations is provided in Appendix B.\nImplementation.\nDetails of our knowledge\ngraph integration are provided in the Appendix A.\nTo simulate realistic clinical interactions, we imple-\nmented a multi-agent framework with three special-\nized agents, adapted from AgentClinic (Schmidgall\net al., 2024):\n1https://www.nejm.org/image-challenge\n• Doctor agent asks up to Tmax = 20 questions.\n• Patient agent responds only with symptom\ndescriptions and never reveals diagnosis.\n• Measurement agent simulates the outcome of\nlaboratory tests or medical examinations.\nWe modified inquiry termination criteria and\nevidence-collection protocols to better align with\nclinical workflows. Detailed descriptions of the\nspecific prompt engineering for each agent are pro-\nvided in Appendix G.\nOur experiment employed Qwen3-8B (Yang\net al., 2025) and Llama3.1-8B-Instruct (AI@Meta,\n2024) for agent-MedQA and agent-CMB, and\nQwen3-VL-8B-Instruct (Yang et al., 2025) for\nNEJM. For specialized models (e.g., Huatuo and\nMeditron), we used their architectures.\nMetrics. Diagnostic accuracy (acc): The diag-\nnostic accuracy is quantified by the exact match\nbetween the final diagnostic output and the ground\ntruth D∗. Higher values indicate a more robust\nalignment with clinical benchmarks.\nDialogue\nrounds (Rounds): We also record the average dia-\nlogue turns to reach a diagnosis for each method.\nFewer Rounds indicate more efficient diagnosis.\n5.2\nMain Result\nTable 1 presents a comprehensive comparison of\nMedKGI against state-of-the-art baselines across\nthree medical consultation benchmarks, agent-\nMedQA, agent-CMB, and NEJM, using multiple\nbackbone LLMs. Our method demonstrates supe-\nrior performance in both diagnostic accuracy and\nefficiency.\nOverall Performance. MedKGI achieves supe-\nrior accuracy across three benchmarks using com-\nparable base models: 69.81% on agent-MedQA\n(Qwen3-8B), 68.21% on agent-CMB (Qwen3-8B),\nand 69.09% on NEJM (Qwen3-VL-8B-Instruct).\nNotably, these results are obtained with the fewest\ndialogue rounds, 9.11, 9.13, and 10.53 out of a\nmaximum of 20 rounds respectively.\nComparison with LLM-based Methods. Com-\npared to general LLM-based approaches, MedKGI\noutperforms methods like AgentClinic and CoT\nacross all benchmarks. It also surpasses specialized\nmedical LLMs (marked with *). For instance, on\nagent-CMB, MedKGI using Qwen3-8B achieves\nhigher accuracy (68.21%) than Medical-CoT with\nMediTron-7B (66.23%), while doing so in signifi-\ncantly fewer dialogue rounds.\n6\n"}, {"page": 7, "text": "Table 1: Comprehensive evaluation of MedKGI across three benchmarks agent-MedQA, agent-CMB, and NEJM\nusing Qwen3-8B, Llama3.1-8B-Instruct, Qwen3-VL-8B-Instruct as base language models. All methods are\nevaluated with a maximin dialogue round of 20. Reported metrics including average dialogue rounds (Rounds\n↓) and diagnostic accuracy (Acc (%) ↑). Methods marked with * employ specialized LLMs (i.e.HuatuoGPT-o1,\nMeditron-7B, and DiagnosisGPT-7B) rather than the base LLM used in our unified evaluation. Best results per\ncolumn are bolded; second-best are underlined.\nagent-MedQA\nagent-CMB\nNEJM\nBaselines\nQwen3-8B\nLlama3.1-8B-Instruct\nQwen3-8B\nLlama3.1-8B-Instuct Qwen3-VL-8B-Instruct\nRounds Acc (%) Rounds\nAcc (%)\nRounds Acc (%) Rounds\nAcc (%)\nRounds\nAcc (%)\nLLM-Based\nAgentClinic\n11.32\n59.43\n10.37\n50.00\n10.00\n58.28\n11.23\n59.60\n13.28\n54.55\nCoT\n18.92\n24.52\n17.46\n34.90\n18.32\n43.05\n16.33\n50.33\n16.46\n50.91\nHuatuo*\n(HuatuoGPT-o1)\n16.70\n56.60\n-\n-\n16.56\n62.25\n-\n-\n-\n-\nMedical-CoT*\n(MediTron-7B)\n18.94\n60.37\n-\n-\n18.34\n66.23\n-\n-\n-\n-\nKG-Based\nMCTS-BT\n14.20\n45.28\n14.98\n39.62\n13.78\n54.97\n14.21\n42.38\n11.50\n56.36\nMCTS-MV\n14.63\n53.77\n12.86\n52.83\n14.77\n60.26\n12.91\n56.95\n13.95\n67.27\nUoT\n11.47\n54.71\n11.01\n51.89\n10.71\n64.28\n10.96\n58.94\n12.32\n54.55\nAgent-Based\nMediQ\n13.80\n61.32\n13.85\n49.06\n13.76\n65.56\n15.01\n60.93\n14.48\n65.45\nDDO\n17.27\n61.32\n18.39\n50.94\n17.38\n63.58\n18.01\n60.93\n17.91\n70.73\nMEDDxAgent\n16.44\n60.38\n16.02\n49.06\n16.09\n61.59\n16.48\n57.62\n16.36\n65.45\nCoD*\n(DiagnosisGPT-7B)\n13.32\n56.60\n-\n-\n11.99\n28.50\n-\n-\n-\n-\nSFT-Based\nSFT\n11.51\n51.89\n11.21\n49.06\n12.04\n59.60\n9.95\n52.98\n-\n-\nSFT-GT\n9.35\n50.94\n10.27\n40.57\n9.93\n55.63\n10.26\n51.66\n-\n-\nOurs\n9.11\n69.81\n10.20\n53.77\n9.13\n68.21\n9.72\n60.26\n10.53\n69.09\nComparison with KG-based and Agent-based\nMethods. Among KG-based methods, MedKGI\nsurpasses even competitive approaches like MCTS-\nMV and UoT. For instance, on agent-CMB with\nQwen3-8B, it achieves 68.21% accuracy, exceed-\ning UoT’s 64.28%. In contrast to agent-based ap-\nproaches such as MediQ, DDO, and MEDDxA-\ngent, MedKGI also demonstrates superior perfor-\nmance. Furthermore, compared to the state-of-the-\nart method, MedKGI achieves comparable or better\naccuracy while typically requiring far fewer rounds\nacross all three datasets.\nComparison with SFT-based Methods. While\nSFT-based methods achieve competitive dialogue\nefficiency, their accuracy lags behind that of our\nmethod. For example, on agent-MedQA using\nQwen3-8B, SFT-GT achieves comparable effi-\nciency (9.35 average rounds vs. our 9.11) but its\naccuracy (50.94%) is significantly lower than ours\n(69.81%).\n5.3\nAnalysis of Superior Performance\nThe performance of MedKGI generalizes across\ndifferent backbone LLMs.\nFor example, with\nLlama3.1-8B-Instruct, our method achieves the\nhighest accuracy on agent-MedQA (53.77%) and\nthe second-highest on agent-CMB (60.26%), while\nconsistently requiring the fewest dialogue rounds.\nThe superiority of MedKGI across diverse bench-\nmarks and LLMs stems from three key factors:\nknowledge grounding, context-aware reasoning,\nand efficient inquiry. First, unlike methods that rely\nsolely on pre-trained LLM knowledge (e.g., Agent-\nClinic, CoT), which may lack structured clinical\n7\n"}, {"page": 8, "text": "Table 2: Ablation experiments on agent-MedQA using\nQwen3-8B, demonstrating the contribution of each com-\nponent to diagnostic performance.\nMethod\nRounds\nAcc (%)\nw/o Knowledge Graph\n13.44\n44.34\nw/o Clinical Record\n12.09\n57.55\nRandom node selection\n19.25\n31.13\nDegree-based node selection\n17.47\n47.17\nOurs\n9.11\n69.81\nreasoning, MedKGI integrates a medical knowl-\nedge graph (KG). This external grounding enables\nprecise inference and provides a structured hypothe-\nsis space for active querying (Jia et al., 2025a). Our\nablation study (Table 2) confirms that removing the\nKG leads to a significant drop in accuracy (-25.47%\non agent-MedQA). Second, compared to other KG-\nbased methods that often rely on heuristic metrics\nfor symptom selection, MedKGI selects questions\nbased on information gain that accounts for patient-\nspecific context. This approach avoids both ran-\ndom noise and popularity bias, leading to more dis-\ncriminative queries (Kim et al., 2025a). Third, in\ncontrast to agent-based methods (e.g., DDO, MED-\nDxAgent), our framework minimizes redundant\ninteractions by dynamically pruning the candidate\nsymptom set based on information gain and main-\ntaining diagnostic records. This enables MedKGI\nto achieve diagnosis in fewer rounds while main-\ntaining high symptom coverage.\n5.4\nAblation Experiments\nWe tested three variants for the ablation study: (1)\nremoving KG integration; (2) disabling the Clinical\nRecord module; and (3) replacing the information\ngain-based symptom pruning strategy with random\nor frequency-based alternatives. As shown in Ta-\nble 2, the full framework consistently achieves the\nhighest diagnostic accuracy. Removing KG inte-\ngration leads to a significant performance drop,\nunderscoring the critical role of structured exter-\nnal knowledge. Meanwhile, omitting dialogue his-\ntory summarization results in incomplete patient\nrecords, which impairs contextual coherence over\nmulti-turn interactions. Finally, both random and\nfrequency-based symptom pruning strategies re-\nsult in lower accuracy than our information-gain\napproach, confirming that targeted, discriminative\nsymptom selection is essential. Together, these\nFigure 4: (a) Impact of candidate disease settings on\naccuracy. (b) Effect of k on Accuracy where k is defined\nas the ratio of the number of candidate diseases to the\nnumber of related symptoms from the KG.\nfindings validate the necessity of each key compo-\nnent in our design.\n5.5\nHyper parameter Selection\nWe performed controlled experiments to determine\nthe optimal settings for two key hyperparameters.\nFirst, we examined how the number of candidate\ndiseases affects accuracy. Figure 4(a) shows that\naccuracy peaks at 5 candidates and declines with\nmore, as low-relevance candidates introduce noise.\nIn practice, we recommend finding the optimal\nvalue by testing on a small sampled dataset. Sec-\nond, we tested symptom sampling by defining k as\nthe average symptoms per candidate disease. Fig-\nure 4(b) indicates optimal performance at k = 1,\nimplying that focused symptom selection maxi-\nmizes discrimination while avoiding redundancy.\n6\nConclusion\nIn this work, we present MedKGI, a framework\nthat formalize multi-step clinical diagnosis as an\nactive, knowledge-guided, and iterative refinement\nprocess. By integrating a medical KG for hypothe-\nsis grounding, an information gain–driven inquiry\nstrategy for diagnostic uncertainty reduction, and a\nstructured diagnostic record aligned with clinical\nstandards, MedKGI enables systematic and effi-\ncient differential diagnosis in multi-turn dialogues.\nUnlike existing approaches that rely on static re-\ntrieval, heuristic questioning, or ungrounded LLM\nreasoning, our framework explicitly models the\nevolving clinical state and optimizes each diag-\nnostic step toward maximal discriminative power.\nExperimental results demonstrate that MedKGI\nachieves both superior diagnostic accuracy and dia-\nlogue efficiency.\n8\n"}, {"page": 9, "text": "Limitations\nWhile MedKGI demonstrates promising perfor-\nmance in differential diagnosis, there are several\nlimitations that warrant discussion. First, our pa-\ntient simulation relies on LLM-generated case de-\nscriptions that may not fully capture the ambiguity\nof real patient narratives. Critically, our patient\nagent assumes cooperative and coherent symptom\nreporting, reflecting an idealized clinical interac-\ntion. In reality, patients often exhibit cognitive or\nlinguistic biases: underreporting stigmatized symp-\ntoms, inaccurate recall, or anxiety-driven concerns\nrather than physiological reasoning. In addition,\nour information gain computations assume con-\nditional independence among symptoms given a\ndisease and employ uniform likelihood over symp-\ntom–disease edges in KG. This assumption may\nlead to suboptimal question selection when dis-\neases are distinguished primarily by complex symp-\ntom patterns.\nAcknowledgements\nThe application of AI in diagnosis support has\nraised ethical concerns that we carefully acknowl-\nedge. MedKGI is proposed as a diagnostic reason-\ning assistant rather than a replacement for licensed\ndoctors. All outputs must be validated by human\ndoctors before any clinical action is taken. Our\nevaluation data are derived from publicly available\nand anonymized datasets agent-MedQA, agent-\nCMB and NEJM Image Challenge. No real patient\nrecords are used, ensuring compliance with privacy\nstandards.\nReferences\n2023. Cmb: Chinese medical benchmark. https://\ngithub.com/FreedomIntelligence/CMB.\nAI@Meta. 2024. Llama 3 model card.\nPayal Chandak, Kexin Huang, and Marinka Zitnik.\n2023. Building a knowledge graph to enable pre-\ncision medicine. Scientific Data, 10(1):67.\nJunying Chen, Chi Gui, Anningzhe Gao, Ke Ji, Xidong\nWang, Xiang Wan, and Benyou Wang. 2025a. Cod,\ntowards an interpretable medical agent using chain of\ndiagnosis. In Findings of the Association for Compu-\ntational Linguistics: ACL 2025, pages 14345–14368.\nXi Chen, Huahui Yi, Mingke You, Weizhi Liu, Li Wang,\nHairui Li, Xue Zhang, Yingman Guo, Lei Fan, Gang\nChen, et al. 2025b. Enhancing diagnostic capabil-\nity with multi-agents conversational large language\nmodels. npj Digital Medicine, 8(1):159.\nZeming Chen, Alejandro Hern A Ndez Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas Köpf,\nAmirkeivan Mohtashami, et al. 2023. Meditron-70b:\nScaling medical pretraining for large language mod-\nels. arXiv e-prints, page arXiv:2311.16079.\nA. M. Cushing, J. S. Ker, P. Kinnersley, P. Mckeown,\nJ. Silverman, J. Patterson, and O. M. R. Westwood.\n2014.\nObjective structured clinical examination.\nAPA PsycTests Database Record.\nGuangxin Dai, Xiang Li, Lizhou Fan, and Xin Ma. 2025.\nEnhancing medical diagnostic reasoning with chain-\nof-thought in large language models. In 2025 Inter-\nnational Conference on Mechatronics, Robotics, and\nArtificial Intelligence (MRAI), pages 294–299.\nHongxin Ding, Baixiang Huang, Yue Fang, Weibin Liao,\nXinke Jiang, Zheng Li, Junfeng Zhao, and Yasha\nWang. 2025. ProMed: Shapley Information Gain\nGuided Reinforcement Learning for Proactive Medi-\ncal LLMs. arXiv e-prints, page arXiv:2508.13514.\nYichun Feng, Jiawei Wang, Lu Zhou, Zhen Lei,\nand Yixue Li. 2025.\nDoctoragent-rl:\nA multi-\nagent collaborative reinforcement learning system\nfor multi-turn clinical dialogue. arXiv e-prints, page\narXiv:2505.19630.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Trans. Comput. Healthcare,\n3(1).\nPaul Hager, Friederike Jungmann, Robbie Holland, Ku-\nnal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob\nVielhauer, Marcus Makowski, Rickmer Braren, Geor-\ngios Kaissis, and Daniel Rueckert. 2024. Evalua-\ntion and mitigation of the limitations of large lan-\nguage models in clinical decision-making. Nature\nMedicine, 30(9):2613–2622.\nSarah Hannigen. 2018. Differential diagnosis. In Jef-\nfrey S. Kreutzer, John DeLuca, and Bruce Caplan,\neditors, Encyclopedia of Clinical Neuropsychology,\nEncyclopedia of Clinical Neuropsychology, pages\n1148–1148. Springer International Publishing, Cham.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021.\nLora: Low-rank adaptation\nof large language models.\narXiv e-prints, page\narXiv:2106.09685.\nZhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao,\nSee-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei\nKoh, and Bryan Hooi. 2024. Uncertainty of thoughts:\nUncertainty-aware planning enhances information\nseeking in llms. In Advances in Neural Information\nProcessing Systems, volume 37, pages 24181–24215.\nMingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang.\n2025a. medikal: Integrating knowledge graphs as\n9\n"}, {"page": 10, "text": "assistants of llms for enhanced clinical diagnosis on\nemrs. In Proceedings of the 31st International Con-\nference on Computational Linguistics, pages 9278–\n9298, Abu Dhabi, UAE. Association for Computa-\ntional Linguistics.\nZhihao Jia, Mingyi Jia, Junwen Duan, and Jianxin Wang.\n2025b. Ddo: Dual-decision optimization for llm-\nbased medical consultation via multi-agent collabo-\nration. In Proceedings of the 2025 Conference on\nEmpirical Methods in Natural Language Processing,\npages 26380–26397.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14).\nJonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu,\nAhmed Alaa, and Danilo Bernardo. 2025a. Limita-\ntions of large language models in clinical problem-\nsolving arising from inflexible reasoning. Scientific\nReports, 15(1):39426.\nYubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu\nChan, Xuhai Xu, Daniel Mcduff, Hyeonhoon Lee,\nMarzyeh Ghassemi, Cynthia Breazeal, and Hae Won\nPark. 2025b. Mdagents: An adaptive collaboration\nof llms for medical decision-making. In Proceed-\nings of the 38th International Conference on Neural\nInformation Processing Systems.\nVladimir I. Levenshtein. 1965. Binary codes capable of\ncorrecting deletions, insertions, and reversals. Soviet\nphysics. Doklady, 10:707–710.\nShuyue Stella Li, Vidhisha Balachandran, Shangbin\nFeng, Jonathan S. Ilgen, Emma Pierson, Pang Wei\nKoh, and Yulia Tsvetkov. 2024. Mediq: Question-\nasking llms and a benchmark for reliable interactive\nclinical reasoning. In Advances in Neural Informa-\ntion Processing Systems, volume 37, pages 28858–\n28888.\nShuyue Stella Li, Jimin Mun, Faeze Brahman, Pedram\nHosseini, Bryceton G. Thomas, Jessica M. Sin, Bing\nRen, Jonathan S. Ilgen, Yulia Tsvetkov, and Maarten\nSap. 2025. Alfa: Aligning Llms to ask good ques-\ntions a case study in clinical reasoning. In Second\nConference on Language Modeling.\nYanna Lin, Shaojie Xu, Wenshuo Zhang, Yushi Sun,\nZixin Chen, Yanjie Zhang, and Rui Sheng. 2025. A\nsurvey of llm-based multi-agent systems in medicine.\nFenglin Liu, Hongjian Zhou, Boyang Gu, Xinyu Zou,\nJinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Yin-\ning Hua, Peilin Zhou, et al. 2025. Application of\nlarge language models in medicine. Nature Reviews\nBioengineering, 3(6):445–464.\nYuxing Lu, Xukai Zhao, and Jinzhuo Wang. 2024. Clin-\nicalrag: Enhancing clinical decision support through\nheterogeneous knowledge retrieval. Proceedings of\nthe 1st Workshop on Towards Knowledgeable Lan-\nguage Models (KnowLLM 2024), pages 64–68.\nHarsha Nori, Mayank Daswani, Christopher Kelly, Scott\nLundberg, Marco Tulio Ribeiro, Marc Wilson, Xi-\naoxuan Liu, Viknesh Sounderajah, Jonathan Carlson,\nMatthew P. Lungren, et al. 2025. Sequential diag-\nnosis with language models. arXiv e-prints, page\narXiv:2506.22405.\nKristina Polotskaya, Carlos S. Muñoz-Valencia, Alejan-\ndro Rabasa, Jose A. Quesada-Rico, Domingo Orozco-\nBeltrán, and Xavier Barber. 2024. Bayesian networks\nfor the diagnosis and prognosis of diseases: A scop-\ning review. Machine Learning and Knowledge Ex-\ntraction, 6(2):1243–1262.\nJ. R. Quinlan. 1986. Induction of decision trees. Ma-\nchine Learning, 1(1):81–106.\nDaniel Philip Rose, Chia-Chien Hung, Marco Lepri,\nIsraa Alqassem, Kiril Gashteovski, and Carolin\nLawrence. 2025. Meddxagent: A unified modular\nagent framework for explainable automatic differ-\nential diagnosis.\nIn Proceedings of the 63rd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13803–\n13826.\nThomas Savage, Ashwin Nayak, Robert Gallo, Ekanath\nRangan, and Jonathan H. Chen. 2024. Diagnostic\nreasoning prompts reveal the potential for large lan-\nguage model interpretability in medicine. npj Digital\nMedicine, 7(1):20.\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo\nReis, Jeffrey Jopling, and Michael Moor. 2024.\nAgentclinic: A Multimodal Agent Benchmark to\nEvaluate Ai in Simulated Clinical Environments.\narXiv e-prints, page arXiv:2405.07960.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Mohamed Amin, Le Hou, Kevin\nClark, Stephen R Pfohl, Heather Cole-Lewis, et al.\n2025. Toward expert-level medical question answer-\ning with large language models. Nature Medicine,\n31(3):943–950.\nPenglei Sun, Yixiang Chen, Xiang Li, and Xiaowen Chu.\n2025. The multi-round diagnostic rag framework for\nemulating clinical reasoning. arXiv e-prints, page\narXiv:2504.07724.\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,\nSendong Zhao, Bing Qin, and Ting Liu. 2023. Hu-\natuo: Tuning llama model with chinese medical\nknowledge. arXiv e-prints, page arXiv:2304.06975.\nXi Wang, Procheta Sen, Ruizhe Li, and Emine Yilmaz.\n2025. Adaptive retrieval-augmented generation for\nconversational systems. In Findings of the Associ-\nation for Computational Linguistics: NAACL 2025,\npages 491–503, Albuquerque, New Mexico. Associa-\ntion for Computational Linguistics.\nXidong Wang, Guiming Chen, Song Dingjie, Zhang\nZhiyi, Zhihong Chen, Qingying Xiao, Junying Chen,\nFeng Jiang, Jianquan Li, Xiang Wan, et al. 2024.\n10\n"}, {"page": 11, "text": "Cmb: A comprehensive medical benchmark in Chi-\nnese. In Proceedings of the 2024 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (Volume 1: Long Papers), pages 6184–6205.\nAssociation for Computational Linguistics.\nKaishuai Xu, Yi Cheng, Wenjun Hou, Qiaoyu Tan, and\nWenjie Li. 2024. Reasoning like a doctor: Improving\nmedical dialogue systems via diagnostic reasoning\nprocess alignment. In Findings of the Association for\nComputational Linguistics: ACL 2024, pages 6796–\n6814.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, et al. 2025.\nQwen3 Technical Report.\narXiv e-prints, page\narXiv:2505.09388.\nJiayuan Zhu, Jiazhen Pan, Yuyuan Liu, Fenglin Liu,\nand Junde Wu. 2025. Ask Patients with Patience:\nEnabling LLMs for Human-Centric Medical Dia-\nlogue with Grounded Reasoning.\narXiv e-prints,\npage arXiv:2502.07143.\nZhihong Zhu, Yunyan Zhang, Xianwei Zhuang, Fan\nZhang, Zhongwei Wan, Yuyan Chen, Qingqing Long,\nYefeng Zheng, and Xian Wu. 2025. Can we trust ai\ndoctors? a survey of medical hallucination in large\nlanguage and large vision-language models. In Find-\nings of the Association for Computational Linguistics:\nACL 2025, pages 6748–6769.\nKaiwen Zuo, Yirui Jiang, Fan Mo, and Pietro Lio. 2025.\nKg4diagnosis: A hierarchical multi-agent llm frame-\nwork with knowledge graph enhancement for medical\ndiagnosis. In Proceedings of The First AAAI Bridge\nProgram on AI for Medicine and Healthcare, volume\n281 of Proceedings of Machine Learning Research,\npages 195–204. PMLR.\n11\n"}, {"page": 12, "text": "A\nKG Implementation\nTo provide a clinically grounded foundation for\nour framework, we utilize PrimeKG (Precision\nMedicine Knowledge Graph) (Chandak et al.,\n2023), which is a comprehensive resource that in-\ntegrates over 20 high-quality primary sources, in-\ncluding Orphanet, Mayo Clinic, and DrugBank.\nPrimeKG comprises:\n• Nodes: Approximately 17,000 disease entities\nand 1,300 symptom entities.\n• Edges: We prioritize two primary relationship\ntypes:\n– Disease-Symptom: Indicating clinical\nmanifestations associated with specific\npathologies.\n– Disease-Disease: Representing comor-\nbidity links and hierarchical relationships\n(e.g., “is-a” or “part-of” relations) that as-\nsist in differential grouping.\nB\nDetailed Description of Baselines\nWe provide a comprehensive overview of the 12\nbaseline methods used in our experiments:\n• Dialog-Based\nMethods.\nAgentClinic\n(Schmidgall et al., 2024): Simulates clini-\ncian–patient dialogues for diagnosis. CoT\n(Chain-of-Thought): Appends “Let’s think\nstep by step” to encourage explicit reason-\ning. Huatuo (Wang et al., 2023) and (Chen\net al., 2023): Representative specialized medi-\ncal LLMs.\n• KG-Based Methods.\nMCTS-BT: Uses\nMonte Carlo Tree Search with backtracking\nfor hypothesis refinement. MCTS-MV: Ex-\ntends MCTS by ranking symptom queries\nbased on contextual informativeness. UoT\n(Unit of Thought) (Hu et al., 2024): Con-\nstructs symptom-centric “units” around con-\nfirmed positive symptoms and prioritizes\nstructural importance.\n• KG-Based Methods.\nMEDIQ (Li et al.,\n2024): A diagnostic agent implementing mul-\ntiple diagnostic strategies through sequential\ndialogues.\nCoD (Chen et al., 2025a): Se-\nlects questions by maximizing information\nentropy over candidate diseases. DDO (Jia\net al., 2025b): a multi-agent framework that\ndynamically chooses symptoms using diverse\nstrategies. MEDDxAgent (Rose et al., 2025):\nAdapts questioning strategy based on diagnos-\ntic uncertainty.\n• SFT-Based Methods. SFT / SFT-GT: Fine-\ntuning on Qwen3-8B using generated dia-\nlogues by AgentClinic (Schmidgall et al.,\n2024), with or without ground-truth labels re-\nspectively.\nC\nDataset and Case Generation\nC.1\nPrompt for OSCE Case Generation\nWe use the following prompt to generate standard-\nized Objective Structured Clinical Examination\n(OSCE) scenarios for evaluation. The prompt in-\nstructs the LLM to produce a structured JSON\ncontaining patient demographics, symptom history,\nphysical findings, test results, and the ground-truth\ndiagnosis while providing only the clinical objec-\ntive to the Doctor Agent.\nFigure 5: Prompt Template for OSCE Case Generation.\nPlease generate a sample Objective\nStructured Clinical Examination (OSCE) for\nthe patient actor and the doctor, including\nwhat the correct diagnosis should be as a\nstructured JSON.\nOnly provide the doctor with\nthe objective and provide \"test results\" as\na separate category. Provide these for a\nprimary care doctor exam.\nGenerate an OSCE for the\nfollowing case study. Please read the answer\ncategory for the correct diagnosis. Here is\nan example of correcting the OSCE format\n{example} . Please create a new one here:\nAn example output is shown in Figure 6.\nD\nImplementation Details\nD.1\nBase Model and Inference Configuration\n- Base models: Qwen3-8B, Meta-Llama-3.1-8B-\nInstruct, and\n- Inference temperature: 0.05\n- Max tokens: 2048\nD.2\nKnowledge Graph Statistics\nWe use PrimeKG (Chandak et al., 2023), which\ncontains:\n- 17,080 disease nodes\n12\n"}, {"page": 13, "text": "- 3,357 symptom nodes\n- 1,361 disease–disease relationships\n- 11,072 disease–symptom relationships\nD.3\nLoRA Fine-Tuning Hyperparameters\nWe fine-tune the base LLM using Low-Rank Adap-\ntation (LoRA) (Hu et al., 2021) with the following\nconfiguration:\n- Learning rate: 2e-4\n- Batch size:\n- per_device_train_batch_size = 2\n- gradient_accumulation_steps = 4\n- Effective batch size = 2 × 4 = 8\n- LoRA rank (r): 8\n- Target modules: [\"q_proj\", \"k_proj\",\n\"v_proj\", \"o_proj\"]\n- LoRA alpha: 32\n- Dropout: 0.1\n- Training epochs: 3\n- Warmup steps: 1,000\nE\nPrompt for Entity Extraction and\nEvaluation\nSymptom Entity Extraction.\nFor symptom ex-\ntraction from patient utterances, we employ the\nfollowing prompt:\nDiagnostic Accuracy Judgment.\nTo evaluate\nwhether the Doctor Agent’s final diagnosis matches\nthe ground truth, we use the judgment prompt:\nFigure 7: Prompt Template for Diagnosis Accuracy\nCheck.\nYou are responsible for determining if the\ncorrect diagnosis and the doctor's diagnosis\nare the same disease. Please respond only\nwith Yes or No. Nothing else.\nHere is the correct\ndiagnosis:\n{ground truth diagnosis}\nHere was the doctor\ndiagnosis:\n{doctor diagnosis}\nAre these the same?\nF\nPrompt for Diagnostic Record\nInitialization and Update\nTo ensure consistency of diagnostic record through-\nout the diagnosis process, we use a prompt that\nguides the LLM to perform evidence-based up-\ndates:\nFigure 6: Prompt Template for Symptom Entity Extrac-\ntion.\nYou are a helpful assistant with expertise\nin medical symptom identification.\nPlease identify and extract\nall disease and symptom entities from the\nfollowing sentence.\nEach entity must be no\nlonger than 5 characters.\nRules:\n1. Include symptoms that are\naffirmed (positive) in the \"positive\" list\n2. Include symptoms that are\nexplicitly denied (negative) in the \"\nnegative\" list\n3. Pay special attention to\nnegation words like \"no\", \"not\", \"don't\", \"\nhaven't\", \"can still\", which typically\nindicate negative symptoms\nReturn the result in valid\nJSON format as shown below (without any\nmarkdown formatting and explanation):\n{\n\"positive\": [\"\nSymptom 1\", \"Symptom 2\", ...],\n\"negative\": [\"\nSymptom 3\", \"Symptom 4\", ...]\n}\nSentence:\n{sentence} .\nFigure 8: Prompt Template for Diagnostic Record Ini-\ntialization and Update.\nYou are an experienced medical scribe.\nYour task is to read the\npatient's latest utterance and incrementally\nupdate the structured summary below.\nRules:\n1. Add or revise only facts\nconfirmed in the CURRENT utterance.\n2. Preserve all existing\ninformation that is not contradicted.\n3. Use the exact JSON schema\nthat was provided (do not create new keys).\n4. Return only the updated\nJSON object, with no extra commentary (\nwithout any markdown formatting).\nSchema:\n{schema}\nCurrent structured summary:\n{current diagnostic record}\nLatest patient utterance:\n{schema}\n13\n"}, {"page": 14, "text": "G\nPrompt for Agent Initialization and\nDiagnosis\nTo simulate realistic clinical interactions, we imple-\nment a multi-agent diagnostic framework compris-\ning three specialized agents: doctor agent, patient\nagent, and measurement agent with each guided\nby prompts to enforce specific behaviors and con-\nstraints.\nThe doctor agent adopts a constrained prompt\nspecifying question limits Tmax and tracks the\ncount of asked questions tcurrent.\nFigure 9: Prompt Template for doctor agent Initializa-\ntion.\nYou are a doctor named Dr. Agent who only\nresponds in the form of dialogue. You are\ninspecting a patient whom you will ask\nquestions in order to understand their\ndisease.\nYou are allowed to ask\n{T_max}\nquestions total before you must\nmake a decision, and have asked\n{t_current}\nquestions so far.\nAdditionally, during the doctor agent ’s differ-\nential diagnosis, we employ the following prompt\ntemplate, which integrates patient demographics,\nrecent dialogue history, structured clinical findings,\nand relevant medical knowledge extracted from the\nknowledge graph.\nFigure 10: Prompt Template for doctor agent Differen-\ntial Diagnosis.\nAge:\n{age}\nGender:\n{gender}\nChief Complaint:\n{chief complaint}\nRecent dialogue history:\n{3 recent dialogue turns}\nMedical record\nChief Complaint:\n{chief complaint}\nSymptoms:\n{symptoms}\nRecent medical examinations:\n{recent medical examinations}\nKnowledge Graph Context\nRelevant medical knowledge\nrepresented as triples:\n{simplified subgraph triples}\nThe patient agent prevents patients from reveal-\ning diagnostic results directly, forcing the doctor\nagent to make diagnoses through symptom reason-\ning.\nFigure 11: Prompt Template for patient agent Initializa-\ntion.\nYou're a patient in a clinic. The doctor\nwill ask questions or order exams to figure\nout your illness.\nBelow is all of your\ninformation:\n{patient profile}\nNever name your disease and\nonly describe symptoms naturally: how they\nfeel, when they flare, or how they affect\nyou.\nThe measurement agent adopts a standardized re-\nsult output format (RESULTS: [results here]),\nensuring parseability of medical examination re-\nsults.\n14\n"}, {"page": 15, "text": "Figure 12: Prompt Template for measurement agent\nInitialization.\nYou are a measurement reader who responds\nwith medical test results. Please respond in\nthe format \"RESULTS: [results here]\".\nBelow is all of the\ninformation you have:\n{medical examinations} . If the requested\nresults are not in your data, then you can\nrespond with NORMAL READINGS.\n15\n"}, {"page": 16, "text": "Figure 13: An example of OSCE Case.\n{\n\"OSCE Examination\": {\n\"Objective for Doctor\": \"Assess and diagnose the patient\npresenting with acute abdominal pain.\",\n\"Patient Actor\": {\n\"Demographics\": \"30-year-old female\",\n\"History\": \"The patient complains of sudden onset of\nsharp, right lower quadrant abdominal pain since last night. The pain has progressively worsened\nover the last 12 hours. She mentions that she felt nauseous this morning but has not vomited. No\nrecent changes in bowel habits or urinary symptoms have been reported.\",\n\"Symptoms\": {\n\"Primary Symptom\": \"Sharp, right lower quadrant\nabdominal pain\",\n\"Secondary Symptoms\": [\"Nausea\", \"No vomiting\", \"\nNo change in bowel habits\", \"No urinary symptoms\"]\n},\n\"Past Medical History\": \"No significant past medical\nhistory. No previous surgeries.\",\n\"Social History\": \"Non-smoker, occasional alcohol use.\nWorks as a software developer.\",\n\"Review of Systems\": \"Denies fever, vomiting, diarrhea,\ndysuria, or flank pain.\"\n},\n\"Physical Examination Findings\": {\n\"Vital Signs\": {\n\"Temperature\": \"37.2 ◦C (99 ◦F)\",\n\"Blood Pressure\": \"120/75 mmHg\",\n\"Heart Rate\": \"78 bpm\",\n\"Respiratory Rate\": \"16 breaths/min\"\n},\n\"Abdominal Examination\": {\n\"Inspection\": \"No distension or visible masses.\",\n\"Auscultation\": \"Normal bowel sounds.\",\n\"Percussion\": \"Tympanic throughout, no shifting\ndullness.\",\n\"Palpation\": \"Tenderness in the right lower\nquadrant. No guarding or rebound tenderness. Rovsing's sign positive, suggesting peritoneal\nirritation.\"\n}\n},\n\"Test Results\": {\n\"Complete Blood Count\": {\n\"WBC\": \"12,000 /µL (elevated)\",\n\"Hemoglobin\": \"13.5 g/dL\",\n\"Platelets\": \"250,000 /µL\"\n},\n\"Urinalysis\": {\n\"Appearance\": \"Clear\",\n\"WBC\": \"2-5 /HPF\",\n\"RBC\": \"0-2 /HPF\",\n\"Nitrites\": \"Negative\",\n\"Leukocyte Esterase\": \"Negative\"\n},\n\"Imaging\": {\n\"Ultrasound Abdomen\": {\n\"Findings\": \"Enlarged appendix with wall\nthickening and fluid collection. No evidence of ovarian cyst or ectopic pregnancy.\"\n}\n}\n},\n\"Correct Diagnosis\": \"Acute Appendicitis\"\n}\n}\n16\n"}]}