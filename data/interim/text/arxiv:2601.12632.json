{"doc_id": "arxiv:2601.12632", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.12632.pdf", "meta": {"doc_id": "arxiv:2601.12632", "source": "arxiv", "arxiv_id": "2601.12632", "title": "BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models", "authors": ["Kriti Bhattarai", "Vipina K. Keloth", "Donald Wright", "Andrew Loza", "Yang Ren", "Hua Xu"], "published": "2026-01-19T00:38:33Z", "updated": "2026-01-19T00:38:33Z", "summary": "Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.   Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.   Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.   Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.12632v1", "url_pdf": "https://arxiv.org/pdf/2601.12632.pdf", "meta_path": "data/raw/arxiv/meta/2601.12632.json", "sha256": "a14fc63bf9e047ff8834c1fe0529ef2d8e291961fa33a91c2c49c4f32db530ad", "status": "ok", "fetched_at": "2026-02-18T02:21:14.248301+00:00"}, "pages": [{"page": 1, "text": " \n1 \nBioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for \nEvaluating Factuality, Robustness, and Bias in Large Language Models   \nKriti Bhattarai, PhD1, Vipina K. Keloth, PhD1, Donald Wright, MD1, Andrew Loza, MD, PhD1, \nYang Ren, PhD1, Hua Xu, PhD1 \n1Department of Biomedical Informatics and Data Science, Yale University, New Haven, CT, USA \n \nCorresponding Authors:  \n \nHua Xu, PhD  \nRobert T. McCluskey Professor of Biomedical Informatics and Data Science, Vice Chair for \nResearch and Development, Department of Biomedical Informatics and Data Science \nAssistant Dean for Biomedical Informatics, Yale School of Medicine \n101 College St, Fl 10, New Haven, CT \nHua.Xu@yale.edu \n \nVipina K. Keloth, PhD  \nAssociate Research Scientist \nDepartment of Biomedical Informatics and Data Science \nYale School of Medicine \n101 College St, Fl 10, New Haven, CT \nVipina.kuttichikeloth@yale.edu \n \nKeywords: Large Language Models, Benchmarking, Natural Language Processing, Question \nAnswering, Model Evaluation  \n \nWord Count: 3999 \n \n \n \n \n \n \n"}, {"page": 2, "text": " \n2 \n \nABSTRACT \nObjective: Large language models (LLMs) are increasingly applied in biomedical settings, and \nexisting benchmark datasets have played an important role in supporting model development and \nevaluation. However, these benchmarks often have limitations. Many of these benchmarks rely on \nstatic or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of \nbiomedical knowledge. They also carry increasing risk of data leakage due to overlap with model \npretraining corpora. In addition, they often overlook critical dimensions such as model robustness \nto linguistic variation and potential demographic biases.  \nMaterials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that \nevaluates LLMs on answering questions from newly published biomedical documents including \ndrug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified \nQuestion Answering (QA) pairs and its perturbed variants, covering both extractive and abstractive \nformats. We evaluate four LLMs—GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B-\nInstruct, which were released prior to the publication dates of the benchmark documents.  \nResults: We find that GPT-o1 achieves the highest Relaxed F1 (0.92), followed by Gemini-2.0-\nFlash (F1=0.90) on drug labels. Clinical trials emerged as the most challenging source, with \nextractive F1 scores as low as 0.362 for extractive tasks.  \nDiscussion and Conclusion: Evaluations revealed greater performance differences from \nparaphrasing than from typographical errors, while bias testing showed negligible differences. \n"}, {"page": 3, "text": " \n3 \nBioPulse-QA provides a scalable, clinically relevant framework for evaluation of biomedical \nLLMs. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n"}, {"page": 4, "text": " \n4 \nINTRODUCTION \nThe rapid advancement of Large Language Models (LLMs) has revolutionized Natural Language \nProcessing (NLP)1. LLMs such as GPT, Llama, Gemini, and others have outperformed previous \nsystems on biomedical Question Answering (QA) benchmarks, including PubMedQA2, MedQA3, \nand MultiMedQA4. These models demonstrate strong performance in tasks involving factual recall \nand structured clinical reasoning, often surpassing benchmark leaderboard scores.  \nExisting biomedical QA benchmarks, though valuable, fall short in comprehensively evaluating \nthese models5,6,7,8,9. While good in assessing factual accuracy, the benchmarks rarely contain QA \npairs to evaluate other critical aspects of LLMs, e.g., their ability to infer meaning based on \ncontext, their robustness to abbreviations, paraphrased text, typographical errors and synonyms, \nand identify potential inherent biases across demographic groups and clinical subpopulations10. \nThey also rarely reflect the most current biomedical knowledge to which clinicians must adhere \nto11,12. Furthermore, the prevalent issue of benchmark contamination, where evaluation data \ninadvertently becomes part of model training, limits the ability of existing datasets to accurately \nassess the true capabilities of emerging LLMs13,14. Recent findings have questioned whether \nprogress on benchmarks reflects real-world utility15. Optimizing models for benchmark \nperformance can lead to inflated scores that do not generalize beyond the test set16. As a result, \nrepeated tuning and evaluation on the same benchmarks can create an illusion of progress, where \nmodels appear to improve but are merely learning to “game” the benchmarks rather than advancing \nclinical reasoning and reliability17. \nBiomedical knowledge is constantly evolving with the introduction of new clinical findings, \nupdated guidelines and new drug discoveries based on research and trials. Healthcare professionals \n"}, {"page": 5, "text": " \n5 \nand patients rely on most recent clinical knowledge to make informed clinical decisions. QA \nsystems have long promised to bridge this information gap by acting as tools to help clinicians and \npatients access relevant medical information at the point of care. To support the development of \nsuch systems, several QA benchmarks, such as BioASQ, PubMedQA, SciQA, MedConceptsQA, \nand SQuAD have been developed to evaluate model performance on QA tasks involving factual \nrecall18,2, 19,20,21. However, existing QA benchmarks, primarily developed for earlier NLP systems, \nfall short in supporting fast evolving nature of complex medical data. These design choices do not \nreflect how clinicians ask questions or capture real-world decision complexity. QA systems often \nstruggle with clinical applicability, contextual reasoning, and usability in real clinical settings due \nto \noversimplified \nqueries22.  \nTo address these limitations, we introduce BioPulse-QA, a dynamic benchmarking framework \ntailored for evaluating LLMs on clinical and biomedical QA tasks. BioPulse-QA extends current \nworks by introducing a temporally adaptive, free-text QA benchmark purpose-built for LLM \nevaluation in biomedical domains. It is designed to serve both healthcare professionals and patient-\ncentered stakeholders by incorporating realistic question types, clinically meaningful content, and \nevaluation criteria that go beyond factual correctness. BioPulse-QA is designed for continuous \nrefresh through a semi-automated pipeline that ingests newly released biomedical documents, such \nas FDA drug labels, clinical trial reports, and published clinical guidelines, to generate up-to-date \nand relevant QA pairs. The pipeline enables periodic updates and efficient generation of new \nquestion-answer pairs, ensuring the benchmark remains current over time. The benchmark \nincludes rigorous assessments of factuality, robustness to input variation, and bias across \ndemographic groups, all within a single, unified framework. Its use of free-text questions mirrors \n"}, {"page": 6, "text": " \n6 \nhow clinicians and patients naturally seek information, enhancing real-world applicability. \nThe benchmark addresses four major gaps in the current evaluation landscape: \n1. Temporal relevance: Unlike static benchmarks, BioPulse-QA focuses on temporally \ngrounded data. By sourcing QA content from newly released biomedical documents and \nexcluding data likely seen during model pretraining, it reduces contamination and enhances \nthe benchmark’s ability to evaluate LLMs on unseen and recent information. \n2. Automatic updates and scalability: The benchmark is structured to support periodic \nrefreshes or “pulses” that incorporate newly documents into the evaluation dataset. This \ndesign allows BioPulse-QA to continuously reflect evolving biomedical knowledge and \nmakes it a scalable tool for longitudinal performance tracking. \n3. Multifaceted evaluation: In addition to factual accuracy, BioPulse-QA dataset includes \nassessments of robustness (via input perturbations such as paraphrasing and typos) and bias \n(via counterfactual swaps of demographic terms). These stress tests offer a richer, more \nrealistic assessment of model behavior in practice. \n4. Realistic task framing with stakeholder alignment: The benchmark emphasizes free-text, \nopen-ended questions rather than simplified multiple-choice formats, aligning with how \nboth clinicians and patients interact with LLMs in real scenarios. It supports a variety of \nclinical information needs, including treatment recommendations, trial eligibility, and \nmedication safety. \nTogether, these contributions position BioPulse-QA as a dynamic, robust, and stakeholder-aligned \nframework for stress-testing real-world capabilities of recent LLMs in biomedical domains. It \n"}, {"page": 7, "text": " \n7 \noffers a timely and much-needed step toward more comprehensive and clinically meaningful \nbenchmark creation for model evaluation. \nBACKGROUND \nA range of biomedical QA benchmarks have been developed to evaluate NLP systems across \nvarious clinical and biomedical tasks, particularly in the domains of biomedical information \nretrieval and question answering. Earlier benchmarks, designed for traditional machine learning \nand rule-based NLP systems, emphasize structured input, static datasets, and multiple-choice \nformats.  \nBioASQ18 is among the most widely used benchmarks for factoid and list-style QA tasks over \nPubMed abstracts, focusing on structured biomedical knowledge retrieval. While valuable, it is \nlimited in scope to abstract-level content and does not capture the full-richness of biomedical \ndocuments such as drug labels or trial reports. PubMedQA2 builds on biomedical literature and \nconsists of yes/no/maybe questions annotated by domain experts. MedQA3 and MedMCQA21 \npresents multiple-choice questions based on the United States Medical Licensing Examination \n(USMLE) and AIIMS & NEET PG entrance exam, respectively, to evaluate clinical knowledge. \nWhile helpful for certain tasks, these formats often oversimplify the QA process and do not reflect \nhow clinicians phrase information needs. These well-structured, static, exam-like questions do not \nalign with real clinical queries or test LLM intricacies. \nMore recent work has shifted toward LLM-oriented benchmarks that attempt to assess model \nperformance on more complex tasks. MultiMedQA4 is a composite benchmark that merges \ndatasets like MedQA3, PubMedQA2, and HealthSearchQA4 to evaluate LLMs across a spectrum \nof question types, including medical licensing, research, and consumer health. Its evaluation \nframework incorporates human assessments of factuality, potential harm, and bias—expanding \n"}, {"page": 8, "text": " \n8 \nbeyond accuracy alone. HealthSearchQA focuses specifically on consumer health questions drawn \nfrom web search data, providing insight into public-facing information needs. MedAlign23 \nintroduces a EHR dataset of 983 natural language instructions curated by clinicians to test LLMs' \nability to generate contextually appropriate outputs. RAmBLA24 further expands the landscape by \nevaluating LLM reliability in biomedical applications, emphasizing prompt sensitivity, \nhallucination detection, and robustness under adversarial conditions modifying PubMedQA \ndataset. While these benchmarks demonstrate strong model performance, they largely rely on \nacademic-style question sources such as medical exams or research abstracts. To date, there are \nfew if any QA benchmarks that use primary clinical reference materials such as drug labels, trial \nprotocols, or guidelines. Existing works on these sources typically use fine-tuned models for \nclassification or extraction tasks, rather than evaluating general-purpose QA capabilities of LLMs. \nWhile these efforts mark important progress for different use cases, they largely remain static \nbenchmarks. Most of them consist of fixed datasets that are not updated regularly, making them \ninsufficient for evaluating models on the most recent biomedical knowledge. Given the rapid \nevolution of clinical guidelines, trial results, and drug approvals, static benchmarks lack real-world \napplicability. Furthermore, many of these datasets were used during instruction-tuning of LLMs, \nraising concerns about contamination and the validity of performance metrics25. \nIn addition to factual accuracy, the robustness and fairness of LLMs are increasingly critical in \nclinical settings. Tools like CheckList26 have proposed task-agnostic behavioral testing to evaluate \nNLP model robustness via perturbations such as negation, synonym swaps, and paraphrasing. \nOther studies have shown that LLMs may exhibit demographic biases across axes like race, \ngender, and age, which could inadvertently exacerbate healthcare disparities27. Despite these \n"}, {"page": 9, "text": " \n9 \ninsights, few benchmarks directly test robustness and bias, resulting in limited understanding of \nLLMs' real-world performance variability and fairness18,2,3,5. \nMETHODS \nOverview of the Benchmarking Framework \nBioPulse-QA pipeline, illustrated in Figure 1, is structured to support scalable dataset \nconstruction, QA pair generation, robustness and bias testing, and standardized evaluation across \nmultiple models. The goal is to simulate realistic clinical QA scenarios while ensuring consistent \nand factually accuract model performance. The benchmarking pipeline consists of: (1) data \ningestion, where new datasets are filtered and downloaded, allowing for continual, automatic \nupdates as new documents become available; (2) automated parsing, where standardized sections \n(e.g., Dosage and Administration, Adverse reactions) are computationally separated to remain \nwithin LLM input token limits; (3) LLM-assisted automated QA generation, where both extractive \nQA pairs (e.g., dosage, indications, condition, eligibility drawn from direct text excerpts) and \nabstractive QA pairs (e.g., inference-based reasoning requiring contextual understanding) are \ngenerated with GPT-4o using curated prompts tailored to each dataset (4) benchmark validation, \nwhere subject matter experts manually review a subset of the generated QA pairs to assess quality, \nfactual alignment, and clinical plausibility; and (5) evaluation, where the validated benchmark is \nused to assess independent models using standardized metrics such as F1-score, enabling \ncomparison of model performance across diverse question types and knowledge sources.  \nData Ingestion  \nBioPulse-QA currently includes three key biomedical sources:  \n1. Drug labels28 from the U.S FDA Drug Database (DailyMed),  \n2. Clinical trial reports from ClinicalTrials.gov29, and  \n"}, {"page": 10, "text": " \n10 \n3. Published clinical guidelines from professional organizations (e.g. NIH, WHO, ACOG).  \nTo ensure models are evaluated on genuinely unseen material, we included only documents \nreleased between January 1, 2024, and February 1, 2025, postdating the December 2023 \nknowledge cutoff of the evaluated LLMs. These documents were filtered using publication dates, \nmetadata tags, and structured identifiers (e.g., NCT numbers for trials). \nParsing and Preprocessing \nAll three datasets with their PDF documents were downloaded directly from the public websites \nof DailyMed (drug labels), ClinicalTrials.gov (trial summaries), and the issuing organizations of \neach clinical guideline. We applied site-level date filters (publication date = 1 Jan 2024 – 1 Feb \n2025) before downloading the files. Guideline PDFs required manual curation: we inspected the \n“last revised” or “publication” date on each landing page and saved only those within the target \nwindow. Documents were preprocessed to remove metadata noise and parsed into semantically \nmeaningful units (e.g., “Adverse Events,” “Eligibility Criteria,” “Indications”). To handle long \ndocuments, we implemented section-based splitting. For instance, in drug labels, only medically \nrich sections like “Dosage and Administration,” “Warnings,” and “Adverse Reactions” were \nretained. This enabled extractive QA generation without semantic drift due to context loss. It \nbalances the information needs of clinical stakeholders with LLM token limitations. \nAutomated QA pair generation \nWe adopted a hybrid expert-GPT approach for generating QA pairs that balances clinical validity \nwith efficiency. We first used GPT-4o to generate questions from documents in each of the three \ndatasets: drug labels, clinical trials, and clinical guidelines. For each domain, we designed dataset-\nspecific, instruction-tuned zero-shot prompts to generate clinically meaningful questions that \nreflect both extractive and abstractive questions. Extractive questions focused on information that \n"}, {"page": 11, "text": " \n11 \nis directly stated in the source text, such as drug dosage, eligibility criteria, or treatment duration. \nIn contrast, abstractive questions required the model to combine information from different parts \nof the document or make inferences. One of the clinical experts independently reviewed all \ngenerated questions to filter out vague or clinically irrelevant items. \nTo enable robustness and bias evaluations, we systematically generated perturbed and \ncounterfactual QA variants from the original QA pairs. These are incorporated into the benchmark \ndataset. For robustness testing, we assessed how stable model outputs were under input variation \nby applying three structured perturbations to a subset of QA pairs: paraphrasing, abbreviation \ninjection, and typographical errors. Paraphrasing involved rewording questions while preserving \ntheir original semantic intent, using both manual rewriting and model-assisted transformation. This \nallowed us to assess whether models could generalize beyond surface-level phrasing. Abbreviation \ntesting substituted common clinical terms with their shorthand equivalents (e.g., “blood pressure” \n→ “BP”), probing the model’s capacity to interpret medical shorthand. For typographical \nrobustness, we introduced minor spelling and punctuation errors (e.g., “diabeees” for “diabetes”), \nsimulating real-world user input noise. Each perturbed QA pair used the same prompt as the \noriginal version. Robustness was then measured by computing the performance delta (e.g., F1 \nscore difference) and percentage change of the performance delta between the original and \nperturbed responses. This approach enables a targeted evaluation of the model’s tolerance to \nlinguistic variability common in clinical communication. \nFor bias testing, we used a template-based approach to insert demographic attributes into neutral \nQA contexts across two axes: age (adult vs. pediatric) and gender (female vs. male). For each axis, \nwe used a template-based counterfactual generation strategy that systematically swapped \ndemographic descriptors in the original questions while keeping all other content unchanged. For \n"}, {"page": 12, "text": " \n12 \nexample, a question originally referring to a \"55-year-old female\" was modified to refer to a \"12-\nyear-old female\" while preserving the clinical scenario. We then compared model responses across \nthese counterfactual pairs to detect variations in answer content, tone, or confidence. If a model \nprovided different answers solely due to demographic substitution (with no change in clinical \nrelevance), we flagged the behavior as indicative of potential bias. This approach provides a \nstructured and replicable method to assess fairness.  \nOnce a batch of high-quality questions was finalized, a separate GPT-4o prompt was used to \ngenerate corresponding answers (Figure 2). To mitigate potential bias from using GPT-4o in \nbenchmark construction, we adopted a hybrid generation-validation approach where all questions \nand answers were independently reviewed by domain experts to ensure they reflect the source \ndocuments accurately and are not aligned with any particular model. The prompts were developed \nthrough multiple rounds of pilot testing. After each round, we reviewed the generated QA pairs \nand refined the prompts to improve task alignment, ensure consistent formatting, and reduce \nambiguity. Full prompt is available in Supplement Text 1. \nBenchmark Validation \nAfter generating and filtering the QA pairs, the benchmark dataset was manually reviewed by \nclinical experts. Two co-authors conducted a review of the extractive answers, to ensure they were \naccurate, grounded in the source text, and matched the intent of the question. Any answers that \nwere incorrect, or incomplete were flagged and revised. \nTwo domain experts with clinical training independently reviewed abstractive questions that \nrequired clinical expertise. The review criteria included question clarity and relevance, factual \naccuracy of the model-generated answers, and the correctness of the reasoning required.  \n"}, {"page": 13, "text": " \n13 \nThrough this process of automated generation followed by careful human review, we created a \nhigh-quality, clinically relevant QA dataset suitable for evaluating the performance of LLMs in \nclinical use cases. \nEvaluation \nWe evaluated the performance of four LLMs on BioPulse-QA: GPT-4o, GPT-o1(OpenAI)30, \nLlama-3.1-8B-Instruct (Meta)31, and Gemini 2.0 Flash (Google DeepMind)32. \nModels were prompted with the same prompt (Figure. 2) with the original context and the \ngenerated questions. Predicted answers were compared to reference answers for its correctness \nusing relaxed F1 score (overlap of predicted answers with the gold standard). While other studies \nhave used metrics such as ROUGE, BLEU, or BERTScore to evaluate generative quality, we opted \nto use F1 score to specifically measure answer correctness rather than token similarity alone. In \naddition to evaluating performance on the original QA pairs, we separately analyzed model \nrobustness and bias by measuring F1 score variations across systematically perturbed and \ncounterfactual question sets (Table 3). \nRESULTS \nWe collected 2280 biomedical QA pairs from three data sources. For clinical guidelines, we \ncurated 702 QA pairs. From drug labels, we curated 928 questions from key categories such as \nindications and usage, dosage and administration, dosage strengths, adverse reactions and drug \ninteractions. For clinical trial, we curated 650 QA pairs covering indications and eligibility, \ncondition and diagnosis, treatment and management, risks and outcomes, and best practices. The \nnumber of documents used, and the QA pairs generated are available in Table 1. The distribution \nof the text length for each dataset is available in Figure 3. Examples of QA pairs are available in \nTable 2.  \n"}, {"page": 14, "text": " \n14 \nTable 1. Datasets and QA pair statistics for the benchmark \nData \nSource \nTotal \nQuestions \nExtractive \nAbstractive \nRobustness \nBias \nNumber \nof \ndocuments per \ndataset \nTotal Number of \nQA \npairs \ngenerated \nDrug \nLabels \n16 \n4 \n2 \n6 \n4 \n58 \n928 (58x16) \nClinical \nTrials \n13 \n3 \n2 \n6 \n2 \n50 \n650 (50x13) \nClinical \nGuidelines \n13 \n4 \n2 \n5 \n2 \n54 \n702 (54x13) \nTotal \n42 \n11 \n6 \n17 \n8 \n162 \n2280 \n \n  \n \n \n \n \n \n \n \n \n \n \n \nTable 2. Examples of QA pairs included in the BioPulse-QA benchmark.  \n"}, {"page": 15, "text": " \n15 \nDataset \nSources \nSnippet of the Context \nQuestion \nCategory \nQuestion \nAnswer \nDrug \nLabels \n“(2.1) Recommended Dosage in \nMonotherapy for Ph+ ALL for \nWhom No Other Kinase Inhibit\nors are Indicated or T315I-positi\nve Ph+ ALL: Starting dose is 45 \nmg orally once daily.” \nExtractive \nQ1. What is the m\naximum starting d\nose recommended \nfor this drug? \n \n45 mg orall\ny once daily \n \nDrug \nLabels \n“1 INDICATIONS AND USAG\nE ICLUSIG is indicated for the t\nreatment of adult patients with:\n..” \nAbstractive \nQ2. Is this medicat\nion suitable for a 3\n0 year old adult pa\ntient?  \nYes \nClinical \nTrials \n“Inclusion \nCriteria:\\n\\n* \nAthletes aged 18 to 35 years\\n* \n6-9 \nmonths \nafter \nACL \nreconstruction\\n* Athletes who \nsustained unilateral ACL injury \ntreated surgically\\n* Exclusion \nCriteria:\\n\\n*” \nAbstractive \nQ3. Based on the \neligibility criteria, \nwould a 55-year-\nold female with \nhypertension \nqualify for this \ntrial? \n \nNo \n"}, {"page": 16, "text": " \n16 \nClinical \nTrials \n“Obesity is a multifactorial \ndisorder of energy balance, \ncharacterized by an imbalance \nbetween energy intake and \nenergy consumption…” \nExtractive \nQ4. \nWhat \ncondition does this \nclinical trial study? \nobesity \nClinical \nGuidelines \n“National \nTuberculosis \nCoalition of America (NTCA)  \nGuidelines \nfor \nRespiratory \nIsolation and Restrictions to \nReduce \nTransmission \nof \nPulmonary Tuberculosis  \nin Community Settings” \n \nExtractive \nQ5. \nWhat \npatient \npopulations \nare \ncovered by this \nguideline? \n \npatients \nwith \npulmonary \ntuberculosis \nClinical \nGuidelines \n“…PERR primary efﬁcacy \nrenal response p.o. oral \nRAS(i) reninangiotensin system \n(inhibitor[s]) \nRCT \nrandomized \ncontrolled \ntrials.” \nAbstractive \nQ6.  \nIs \nrenal \nimpairment \na \ncontraindication \nfor the treatment \nrecommended \nin \nthis \nguideline?  \nRespond \nwith \n'Yes' or 'No' \nYes \n \n"}, {"page": 17, "text": " \n17 \nOverall Benchmark Performance Results \nWe evaluated four LLMs—GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1-8B-Instruct \nacross three datasets. For abstractive QA, GPT-o1 achieved high F1 performance on drug labels, \n(0.98), and Gemini-2.0-Flash achieved high performance on clinical trials (0.65) and clinical \nguidelines (0.93) (Figure 4). Llama-3.1-8B underperformed with a low F1 score on drug labels \n(0.85), clinical trials (0.62), and clinical guidelines (0.69).  \nExtractive QA provided more challenging compared to abstractive QA. GPT-o1 achieved high F1 \nscore on drug labels (0.88), Llama-3.1-8B scored high score on clinical trials (0.50) and clinical \nguidelines (0.92). GPT-4o had a low F1 score on drug labels (0.78), GPT-o1 had a low score for \nclinical trials, and Gemini-2.0-Flash had a low score on clinical guidelines (0.57). \nOverall, combined results showed that GPT-o1 demonstrated the strongest performance across \nboth QA types, achieving combined scores of 0.92 on drug labels and 0.85 on clinical guidelines. \nLlama-3.1-8B \nachieved \nthe \nbest \ncombined \nscore \nof \n0.57 \non \nclinical \ntrials.  \nAcross the datasets, clinical trials were the most challenging, with extractive scores ranging from \n0.362 to 0.502, and abstractive score ranging from 0.62 to 0.65. In contrast, drug labels, which are \nmore structured and follow a template-like format, yielded the highest performance. Clinical \nguidelines fell in between, as they combine structured section headings with more variable free-\ntext content. \nThese findings highlight that a model’s ability to generate well-reasoned answers (abstractive QA) \ndoes not necessarily mean it can accurately locate and extract exact information from text \n(extractive QA), especially when dealing with complex clinical narratives. Evaluating both \nabstractive and extractive tasks is necessary to fully understand a model’s strengths and \n"}, {"page": 18, "text": " \n18 \nlimitations, as high scores in one task may mask weaknesses in the other—an issue that may not \nbe \napparent \nwhen \nlooking \nonly \nat \nstandard \nbenchmark \nleaderboards. \nIn the robustness evaluation, all models were more sensitive to paraphrased questions than to \ntypographical errors (Table 3). Paraphrasing always improved the performance for Gemini-2.0-\nFlash with 23.08% increase for clinical guidelines, 16.28% for clinical trials and 1.12% for drug \nlabels. GPT-4o model demonstrated better robustness with no change on paraphrased questions \nfor clinical trials data (0%) and less than 3% increase for drug labels and clinical guidelines. GPT-\no1 and Llama-3.1-8B models exhibited a mixed trend with GPT-o1 performance decreasing by \n9.34% and 1.76% respectively on drug labels and clinical guidelines and increasing by 11.9% on \nclinical \ntrials. \nFor questions with typos, all models were robust on drug labels data with no change in \nperformance.  Llama-3.1-8B demonstrated a higher performance drop on both clinical trials \n(10.53%) and clinical guidelines (5.95%). Apart from that GPT-4o exhibited a slight increase \n(2.22%) while Gemini-2.0-Flash performance slightly decreased (2.33%). This suggests that \ncurrent LLMs are more resilient to surface-level noise (e.g., misspellings) than to semantic \nvariation \n(e.g., \nrewording). \nBias evaluation across age and gender showed minimal performance variation. The highest \nperformance change was observed for age, with GPT-4o model showing performance difference \nof 5.38% on drug labels data. For gender, performance change was observed with Gemini-2.0-\nFlash on clinical guidelines and clinical trials.  \n \n \n"}, {"page": 19, "text": " \n19 \nTable 3. Robustness and bias test (F1 Scores) across a subset of the question pairs.  \nEvaluation \nCategory \nDataset \nModel \nF1 Original \nF1 \nModified \nΔ (Modified \n– Original) \n% Change \n((Modified – \nOriginal)/Original) *100 \nRobustness \nParaphrase \nDrug \nLabels \nGPT-4o \n0.85 \n0.87 \n0.02 \n2.35% \nGPT-o1 \n0.91 \n0.825 \n-0.085 \n-9.34% \nGemini-2.0-Flash \n0.89 \n0.90 \n0.01 \n1.12% \nLlama-3.1-8B-Inst. \n0.86 \n0.865 \n0.005 \n0.58% \nClinical \nTrials \nGPT-4o \n0.45 \n0.45 \n0.0 \n0% \nGPT-o1 \n0.42 \n0.47 \n0.05 \n11.9% \nGemini-2.0-Flash \n0.43 \n0.50 \n0.07 \n16.28% \nLlama-3.1-8B-Inst. \n0.57 \n0.49 \n-0.08 \n-14.04% \nClinical \nGuidelines \nGPT-4o \n0.77 \n0.79 \n0.02 \n2.6% \nGPT-o1 \n0.85 \n0.835 \n-0.015 \n-1.76% \nGemini-2.0-Flash \n0.65 \n0.80 \n0.15 \n23.08% \nLlama-3.1-8B-Inst. \n0.84 \n0.77 \n-0.07 \n-8.33% \nTypo \nDrug \nLabels \nGPT-4o \n0.85 \n0.85 \n0.0 \n0% \nGPT-o1 \n0.91 \n0.91 \n0.0 \n0% \nGemini-2.0-Flash \n0.89 \n0.89 \n0.0 \n0% \nLlama-3.1-8B-Inst. \n0.86 \n0.86 \n0.0 \n0% \nClinical \nTrials \nGPT-4o \n0.45 \n0.46 \n0.01 \n2.22% \nGPT-o1 \n0.42 \n0.42 \n0.0 \n0% \nGemini-2.0-Flash \n0.43 \n0.42 \n-0.01 \n-2.33% \nLlama-3.1-8B-Inst. \n0.57 \n0.51 \n-0.06 \n-10.53% \nClinical \nGuidelines \nGPT-4o \n0.77 \n0.77 \n0.0 \n0% \nGPT-o1 \n0.85 \n0.85 \n0.0 \n0% \n"}, {"page": 20, "text": " \n20 \nGemini-2.0-Flash \n0.65 \n0.65 \n0.0 \n0% \nLlama-3.1-8B-Inst. \n0.84 \n0.79 \n-0.05 \n-5.95% \nBias \n \nAge \nDrug \nLabels \nGPT-4o \n0.86 \n0.86 \n0.0 \n0% \nGPT-o1 \n0.92 \n0.897 \n-0.023 \n-2.5% \nGemini-2.0-Flash \n0.88 \n0.851 \n-0.029 \n-3.3% \nLlama-3.1-8B-Inst. \n0.85 \n0.85 \n0.0 \n0% \nClinical \nTrials \nGPT-4o \n0.46 \n0.46 \n0.0 \n0% \nGPT-o1 \n0.43 \n0.43 \n0.0 \n0% \nGemini-2.0-Flash \n0.44 \n0.45 \n0.01 \n2.27% \nLlama-3.1-8B-Inst. \n0.58 \n0.567 \n-0.013 \n-2.24% \nClinical \nGuidelines \nGPT-4o \n0.78 \n0.822 \n0.042 \n5.38% \nGPT-o1 \n0.86 \n0.87 \n0.01 \n1.16% \nGemini-2.0-Flash \n0.845 \n0.822 \n-0.023 \n-2.72% \nLlama-3.1-8B-Inst. \n0.85 \n0.84 \n-0.01 \n-1.18% \nGender \nDrug \nLabels \nGPT-4o \n0.86 \n0.86 \n0.00 \n0% \nGPT-o1 \n0.92 \n0.92 \n0.00 \n0% \nGemini-2.0-Flash \n0.88 \n0.88 \n0.00 \n0% \nLlama-3.1-8B-Inst. \n0.85 \n0.85 \n0.0 \n0% \nClinical \nTrials \nGPT-4o \n0.46 \n0.46 \n0.0 \n0% \nGPT-o1 \n0.43 \n0.43 \n0.0 \n0% \nGemini-2.0-Flash \n0.44 \n0.45 \n0.010 \n2.27% \nLlama-3.1-8B-Inst. \n0.58 \n0.58 \n0.00 \n0% \nClinical \nGuidelines \nGPT-4o \n0.78 \n0.78 \n0.00 \n0% \nGPT-o1 \n0.86 \n0.86 \n0.00 \n0% \nGemini-2.0-Flash \n0.845 \n0.822 \n-0.023 \n-2.72% \nLlama-3.1-8B-Inst. \n0.85 \n0.84 \n-0.010 \n-1.18% \n"}, {"page": 21, "text": " \n21 \nError Analysis \nWe conducted a comprehensive manual error analysis on the datasets to examine model failure \npatterns. Model errors were classified as factual incorrectness and incompleteness. Incorrect \nresponses refer to cases where the model output did not match the gold standard in content and \nformat. For answers with incorrect span, the model missed essential details in the answer required \nfor \nfull \ncorrectness. \nTable 4 presents the distribution of these error types across the four models on the full benchmark. \nAcross all models and datasets, most results were incorrect answers followed by incompleteness. \nOverall, the analysis reveals that even when models perform well on automated metrics like \nRelaxed F1, a significant portion of their errors reflect deeper limitations in factual grounding and \ncomprehension. These results emphasize the importance of qualitative error analysis in \ncomplementing quantitative evaluations, especially for real-world biomedical applications where \npartial or incorrect answers can lead to critical misinterpretations. \nTable 4. Distribution of Error Types Across Models (in %*).  \nDataset \nError Type \nGPT-4o \nGPT-o1 \nGemini-2.0-\nFlash \nLlama-3.1-8B-\nInstruct \nDrug Labels \nFactual Incorrectness \n61.48 \n66.67 \n14.58 \n74.55 \n \nIncomplete \n38.52 \n33.33 \n85.42 \n25.45 \nClinical Trials \nFactual Incorrectness \n98.88 \n93.97 \n86.28 \n90.62 \n \nIncomplete \n1.12 \n6.03 \n13.72 \n9.38 \nGuidelines \nFactual Incorrectness \n64.53 \n100.0 \n62.51 \n51.0 \n \nIncomplete \n35.47 \n0.0 \n37.49 \n49.0 \n*Percentages of each error type out of all errors identified  \n"}, {"page": 22, "text": " \n22 \nDISCUSSION \nBioPulse-QA introduces a scalable and semi-automated benchmark designed to evaluate LLMs \non biomedical QA. By integrating recent, real-world biomedical data sources, such as drug \nlabels, clinical trial reports, and clinical guidelines, the benchmark provides a diverse, structured, \nand evolving dataset. To the best of our knowledge, there have been no scalable, temporally \nevolving benchmarks.  \nOur initial results highlight notable difference in LLM performance on factual correctness and \ninference-based reasoning, emphasizing the need for a more careful manual evaluation, especially \nfor real-world biomedical implementation. Unlike prior efforts that fine-tune LLMs on static \ndatasets derived from clinical trials or drug labels, our benchmark evaluates zero-shot performance \non unseen, newly published documents24,25,26,27,28. Extractive QA proved more challenging than \nabstractive QA across all models. Error analysis revealed that extractive responses were more \nlikely to diverge from reference answers due to paraphrasing or unexpected lexical variation. If a \nmodel provides an answer that is semantically close or broadly factually correct, its F1 score can \nbe penalized when using broader or non-identical lexical terms. This highlights a limitation of \ntoken-level F1 for evaluating extractive tasks, especially in domains where synonymous phrasing \nis common. Nonetheless, our focus remains on answer correctness over token similarity, justifying \nthe choice of F1 use.  \nAbstractive questions, on the other hand, often required binary yes/no responses for automatic \nevaluation supported by explanation text to understand model reasoning. In those cases, models \nlike GPT-4o provided more complete reasoning, aligning better with the gold standard. In contrast, \nthere were also cases where models responded with “Not available” despite the presence of \nrelevant answer text. This was more prevalent in clinical trials data where 83 out of 91 incorrect \n"}, {"page": 23, "text": " \n23 \nresults were marked ‘Not Available’. Additionally, despite including explicit prompt instructions, \nmodels also occasionally failed to follow the expected output format, particularly for extractive \nquestions. This was evident in instances where free-form generations replaced direct spans from \nthe provided context.  \nRobustness evaluation showed models were more sensitive to paraphrasing than to typographical \nerrors, suggesting limited generalization to semantically altered yet clinically equivalent inputs. \nThis gap highlights a critical limitation in current LLMs, where even reworded questions \npreserving clinical intent may lead to incorrect responses.  \nLimitations \nWhile we carefully curated the benchmark to include only biomedical documents released after \nDecember 2023, it is not possible to fully eliminate the risk of data contamination, given that \nbiomedical knowledge is cumulative and spans decades. Though the documents used in this study \nwere newly released and not part of any known pretraining corpus, related clinical concepts or \nphrasing may still have appeared in the training.  \nWhile we used GPT-4o to generate the initial benchmark content, all questions and answers were \nmanually reviewed by domain experts to ensure clinical correctness and alignment with the \nreference text. This hybrid generation-validation setup addresses a key challenge in biomedical \nQA benchmarking which is ensuring that automated benchmarks reflect real-world expectations \nwhile maintaining scalability. Our experience highlights the importance of human oversight in \nvalidating benchmark quality, particularly for handling paraphrasing, implicit reasoning, or \nborderline answer variants that F1 may not fully capture. \nThis study has a couple of limitations regarding robustness and bias. There may be inherent \npretraining biases in the models that could manifest in the final results, which we could not fully \n"}, {"page": 24, "text": " \n24 \naccount for in our analysis. Furthermore, while the benchmark introduces QA pairs specifically \ntargeting robustness and bias, the subcategories of robustness and bias are broader than covered \nhere. For bias, our current analysis is limited to age and gender, and although this coverage is \nlimited, it still provides a meaningful starting point for benchmark creation and evaluation. We \nplan to continuously expand the dataset to include more evaluation criteria over time.  \nCONCLUSION \nBioPulse-QA provides a flexible, scalable, and clinically grounded benchmark for evaluating the \nperformance of LLMs on biomedical QA tasks. The benchmark is supported by a semi-automated \nframework for continuous data updates and reproducibility. By incorporating diverse question \ntypes, up-to-date clinical content, and questions designed to probe robustness and bias, the \nbenchmark supports meaningful evaluation aligned with real-world use. The dataset is designed to \nbe generalizable across a range of clinical queries relevant to stakeholders, and its semi-automated \npipeline supports periodic updates. While the current version emphasizes factual accuracy, \nrobustness and bias evaluation on three datasets, we plan continuous updates with new data sources \nand evaluation criteria. BioPulse-QA is a first step towards creating dynamic, stakeholder-aligned \nbenchmark for evolving LLM evaluation on biomedical tasks. \nDATA AVAILABILITY \nThe dataset created in this study is available at https://github.com/BIDS-Xu-Lab.  \nCONFLICT OF INTERESTS \nThe authors have no competing interests to declare. \nREFERENCES \n1. Cao, Z., Keloth, V. K., Xie, Q., Qian, L., Liu, Y., Wang, Y., Shi, R., Zhou, W., Yang, G., \nZhang, J., Peng, X., Zhen, E., Weng, R.-L., Chen, Q., & Xu, H. (2025). The Development \n"}, {"page": 25, "text": " \n25 \nLandscape of Large Language Models for Biomedical Applications. Annual Review of \nBiomedical Data Science, 8, 251–274.  \n2. Jin Q, Dhingra B, Liu Z, Cohen WW, Lu X. PubMedQA: A Dataset for Biomedical Research \nQuestion Answering. In: Proceedings of the 2019 Conference on Empirical Methods in Natural \nLanguage Processing and the 9th International Joint Conference on Natural Language Processing \n(EMNLP-IJCNLP); 2019. p. 2567–2577 \n3. Jin D, Pan E, Oufattole N, Weng W-H, Fang H, Szolovits P. What disease does this patient \nhave? A large-scale open domain question answering dataset from medical exams. Applied \nSciences. 2021;11(14):6421 \n4. Singhal K, Azizi S, Tu T, et al. Large Language Models Encode Clinical Knowledge. Nature. \n2023;620(7972):172–182. \n5. Hager, P., Jungmann, F., Holland, R. et al. Evaluation and mitigation of the limitations of \nlarge language models in clinical decision-making. Nat Med 30, 2613–2622 (2024).  \n6. Arora RK, Wei J, Soskin Hicks R, Bowman P, Quiñonero-Candela J, Tsimpourlas F, Sharman \nM, Shah M, Vallone A, Beutel A, Heidecke J, Singhal K. HealthBench: Evaluating Large \nLanguage Models Towards Improved Human Health. arXiv preprint arXiv:2505.08775. 2025 \n7. Chen Q, Hu Y, Peng X, Xie Q, et al. Benchmarking large language models for biomedical \nnatural language processing applications and recommendations. Nature Communications. \n2025;16(1):56989.30.  \n8. Alsentzer E, Daneshjou R, Raji ID, Kohane IS. It’s Time to Bench the Medical Exam \nBenchmark. NEJM AI. 2025;1(1):5–7. \n"}, {"page": 26, "text": " \n26 \n9. Chen Q, Hu Y, Peng X, et al. Benchmarking large language models for biomedical natural \nlanguage processing applications and recommendations. Nature Communications. \n2025;16(1):56989. doi:10.1038/s41467-025-56989-2 \n10. Chen L, Deng Y, Bian Y, Qin Z, Wu B, Chua T-S, Wong K-F. Beyond Factuality: A \nComprehensive Evaluation of Large Language Models as Knowledge Generators. In: \nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing \n(EMNLP 2023). Singapore: Association for Computational Linguistics; 2023. p. 6325–6341.  \n11. Holzinger A, Langs G, Denk H, Zatloukal K, Müller H. Benchmark datasets driving artificial \nintelligence development fail to capture the needs of medical professionals. Journal of \nBiomedical Informatics. 2023;139:104330. \n12.  Gregory Kell, Angus Roberts, Serge Umansky, et al. Question answering systems for health \nprofessionals at the point of care—a systematic review. Journal of the American Medical \nInformatics Association. 2024;31(4):1009–1024.  \n13. Chunyuan Deng, Yilun Zhao, Yuzhao Heng, Yitong Li, Jiannan Cao, Xiangru Tang, and \nArman Cohan. 2024. Unveiling the Spectrum of Data Contamination in Language Model: A \nSurvey from Detection to Remediation. In Findings of the Association for Computational \nLinguistics: ACL 2024, pages 16078–16092, Bangkok, Thailand. Association for Computational \nLinguistics.  \n14. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2024. \nInvestigating Data Contamination in Modern Benchmarks for Large Language Models. In \nProceedings of the 2024 Annual Conference of the North American Chapter of the Association \nfor Computational Linguistics (NAACL 2024), pages 8662–8682. \n"}, {"page": 27, "text": " \n27 \n15. Wu J, Gu B, Zhou R, Xie K, Snyder D, Jiang Y, Carducci V, Wyss R, Desai RJ, Alsentzer E, \nCeli LA, Rodman A, Schneeweiss S, Chen JH, Romero-Brufau S, Lin KJ, Yang J. BRIDGE: \nBenchmarking Large Language Models for Understanding Real-world Clinical Practice Text. \narXiv preprint arXiv:2504.19467. 2025 \n16. Wu J, Gu B, Zhou R, Xie K, Snyder D, Jiang Y, Carducci V, Wyss R, Desai RJ, Alsentzer E, \nCeli LA, Rodman A, Schneeweiss S, Chen JH, Romero-Brufau S, Lin KJ, Yang J. BRIDGE: \nBenchmarking Large Language Models for Understanding Real-world Clinical Practice Text. \narXiv preprint arXiv:2504.19467. 2025 \n17. Singh S, Nan Y, Wang A, D'Souza D, Kapoor S, Üstün A, Koyejo S, Deng Y, Longpre S, \nSmith NA, Ermis B, Fadaee M, Hooker S. The Leaderboard Illusion. arXiv preprint \narXiv:2504.20879. 2025 Apr 29; revised May 12. \n18. Krithara A, Nentidis A, Bougiatiotis K, Paliouras G, et al. BioASQ-QA: A manually curated \ncorpus for biomedical question answering. Scientific Data. 2023;10:170. \n19. Auer S, Barone DAC, Bartz C, et al. The SciQA scientific question answering benchmark for \nscholarly knowledge. Scientific Reports. 2023;13:7240 \n20. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: \n100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 \nConference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, \nTexas. Association for Computational Linguistics. \n21. Shoham OB, Rappoport N. MedConceptsQA: open-source medical concepts QA benchmark. \nComputers in Biology and Medicine. 2024;182:109089. \n22. Gregory Kell, Angus Roberts, Serge Umansky, Linglong Qian, Davide Ferrari, Frank \nSoboczenski, Byron C Wallace, Nikhil Patel, Iain J Marshall. Question answering systems for \n"}, {"page": 28, "text": " \n28 \nhealth professionals at the point of care—a systematic review. Journal of the American Medical \nInformatics Association. 2024;31(4):1009–1024. \n23. Fleming, S. L., Lozano, A., Haberkorn, W. J.,et al. MedAlign: A Clinician-Generated Dataset \nfor Instruction Following with Electronic Medical Records. Proceedings of the AAAI Conference \non Artificial Intelligence. 2023; 38(20), 22021-22030. \n24. Jin E, Morrell E, Bolton W, et al. RAmBLA: A Framework for Evaluating the Reliability of \nLLMs as Assistants in the Biomedical Domain. arXiv preprint arXiv:2403.14578. 2024 \n25. Touvron H, Martin L, Stone K, et al. Llama 2: Open Foundation and Fine-Tuned Chat \nModels. arXiv preprint arXiv:2307.09288. 2023 \n26. Ribeiro MT, Wu T, Guestrin C, Singh S. Beyond Accuracy: Behavioral Testing of NLP \nModels with CheckList. In: Proceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics. 2020:4902–4912 \n27. Jin E, Morrell E, Bolton W, et al. Evaluating and addressing demographic disparities in \nmedical large language models: a systematic review. International Journal for Equity in Health. \n2025;24(1):57 \n28. DailyMed. Bethesda (MD): U.S. National Library of Medicine; 2002. Available from: \nhttps://dailymed.nlm.nih.gov/dailymed/. Cited March 17, 2025. \n29. ClinicalTrials.gov. Bethesda (MD): National Library of Medicine (US); 2002. Available \nfrom: https://clinicaltrials.gov/. Cited March 17, 2025 \n30. OpenAI. 2023. GPT-4 Technical Report. https://openai.com/research/gpt-4 \n31. Meta AI. (2024). LLaMA 3: Open Foundation and Instruction Models. \nhttps://ai.meta.com/llama/ \n"}, {"page": 29, "text": " \n29 \n32. Google Deepmind. 2023. Gemini: A Family of Highly Capable Multimodal Models. \nhttps://arxiv.org/abs/2312.11805 \n \nFigure Legends: \n \nFigure 1. Overview of the semi-automated BioPulse-QA benchmarking pipeline.  \n \nFigure 2. Snippet of a prompt used for QA generation from clinical trials dataset.  \n"}, {"page": 30, "text": " \n30 \n \nFigure 3. Context token length of the individual datasets.  \n"}, {"page": 31, "text": " \n31 \n \nFigure 4. Performance across models and datasets evaluated with BioPulse-QA.  \n \n \n \n"}]}