{"doc_id": "arxiv:2601.06801", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06801.pdf", "meta": {"doc_id": "arxiv:2601.06801", "source": "arxiv", "arxiv_id": "2601.06801", "title": "Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy", "authors": ["Shujian Gao", "Yuan Wang", "Jiangtao Yan", "Zuxuan Wu", "Yu-Gang Jiang"], "published": "2026-01-11T08:25:34Z", "updated": "2026-01-11T08:25:34Z", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced reasoning capabilities in Large Language Models. However, adapting RLVR to multimodal domains suffers from a critical \\textit{perception-reasoning decoupling}. Existing paradigms, driven by text-centric outcome rewards, reasoning in language medium, inadvertently encourage models to bypass visual perception. We empirically validate this through blind experiments: state-of-the-art policies maintain or surprisingly improve performance even when visual inputs are entirely removed. This reveals that these models degenerate into \\textit{blind reasoners}, exploiting linguistic priors to generate plausible answers instead of attending to visual evidence. In response, we propose \\textbf{Thinking with Deltas}, a framework driven by a \\textbf{Differential Visual Reasoning Policy (DVRP)}. DVRP introduces intrinsic supervision via visual triplets, comprising original, masked, and perturbed inputs. It optimizes the model to maximize reasoning divergence from masked inputs (enforcing \\textit{visual sensitivity}) while minimizing divergence from perturbed inputs (ensuring \\textit{visual robustness}). By aligning reasoning variations strictly with the \\textit{Delta} of visual information, DVRP inherently bolsters visual understanding capabilities and significantly outperforms state-of-the-art methods on both general and medical benchmarks, without requiring external annotations or auxiliary tools.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06801v1", "url_pdf": "https://arxiv.org/pdf/2601.06801.pdf", "meta_path": "data/raw/arxiv/meta/2601.06801.json", "sha256": "d5aa1d0f0263f41805329790ce2b7c839c76640beb3851ba4c8791ea9a91f92a", "status": "ok", "fetched_at": "2026-02-18T02:21:43.402479+00:00"}, "pages": [{"page": 1, "text": "Thinking with Deltas: Incentivizing Reinforcement Learning\nvia Differential Visual Reasoning Policy\nShujian Gao1, Yuan Wang2, Jiangtao Yan3, Zuxuan Wu1, †, Yu-Gang Jiang1, †\n1Fudan University\n2Zhejiang University\n3Wuhan University\nAbstract\nReinforcement Learning with Verifiable Re-\nwards (RLVR) has significantly advanced rea-\nsoning capabilities in Large Language Mod-\nels. However, adapting RLVR to multimodal\ndomains suffers from a critical perception-\nreasoning decoupling.\nExisting paradigms,\ndriven by text-centric outcome rewards, rea-\nsoning in language medium, inadvertently en-\ncourage models to bypass visual perception.\nWe empirically validate this through blind ex-\nperiments: state-of-the-art policies maintain or\nsurprisingly improve performance even when\nvisual inputs are entirely removed. This reveals\nthat these models degenerate into blind reason-\ners, exploiting linguistic priors to generate plau-\nsible answers instead of attending to visual evi-\ndence. In response, we propose Thinking with\nDeltas, a framework driven by a Differential\nVisual Reasoning Policy (DVRP). DVRP in-\ntroduces intrinsic supervision via visual triplets,\ncomprising original, masked, and perturbed in-\nputs. It optimizes the model to maximize rea-\nsoning divergence from masked inputs (enforc-\ning visual sensitivity) while minimizing diver-\ngence from perturbed inputs (ensuring visual\nrobustness). By aligning reasoning variations\nstrictly with the Delta of visual information,\nDVRP inherently bolsters visual understanding\ncapabilities and significantly outperforms state-\nof-the-art methods on both general and medical\nbenchmarks, without requiring external annota-\ntions or auxiliary tools.\n1\nIntroduction\nRecent advancements in Large Language Mod-\nels (LLMs) (Chang et al., 2024; Brown et al.,\n2020) have been substantially driven by Chain-\nof-Thought (CoT) prompting and the curation of\nhigh-quality reasoning data (Kojima et al., 2022;\nYe et al., 2025).\nBuilding on this foundation,\nReinforcement Learning with Verifiable Rewards\n†Corresponding authors.\n(RLVR) (Ouyang et al., 2022; Shao et al., 2024) has\nemerged as a promising post-training paradigm. By\nincentivizing self-correction and scaling test-time\ncompute, RLVR enables models to generate rigor-\nous and self-consistent solution paths, establishing\na new standard for complex problem-solving in\ntextual domains (Wang et al., 2025e).\nHowever, a fundamental misalignment arises\nwhen directly applying the RLVR paradigm to mul-\ntimodal domains (Liu et al., 2025a). Predominant\napproaches typically mirror text-centric methodolo-\ngies (DeepSeek-AI et al., 2025) and rely heavily\non textual outcome rewards such as accuracy or\nformat constraints (Wang et al., 2025f). Further-\nmore, attempts to incorporate process supervision\nvia external judges often inherit the biases and lim-\nitations of the judge models themselves (Luo et al.,\n2025). Conceptually, if we analogize a Multimodal\nLLM (MLLM) to a human observer, the visual en-\ncoder functions as the eyes. Optimizing rewards\nbased solely on the output text sequence is equiva-\nlent to evaluating speech without verifying vision.\nThis approach neither ensures the effective percep-\ntion of visual signals nor stimulates deep visual\nunderstanding (Han et al., 2025b). Consequently,\ncurrent frameworks neglect the causal dependency\nbetween visual inputs and reasoning outcomes.\nAs illustrated in Figure 1, we empirically vali-\ndate this decoupling through blind experiments\nwhere visual inputs are either removed (Text Only)\nor replaced with blank images (Black/White). We\ninvestigate the behaviors of DAPO and GRPO un-\nder varying input configurations. For the GRPO\nbaseline (Shao et al., 2024), the observed perfor-\nmance drop is negligible. This result is counter-\nintuitive, as the absence of essential visual contexts\nshould theoretically precipitate a substantial perfor-\nmance collapse. Even more strikingly, DAPO (Yu\net al., 2025) exhibits an unexpected performance\ngain: removing visual information improves ac-\ncuracy (e.g., +3.5% on MathVista). These find-\narXiv:2601.06801v1  [cs.AI]  11 Jan 2026\n"}, {"page": 2, "text": "Figure 1: Empirical Validation of Visual Decoupling. Performance comparison in blind settings (Text Only\nor Blank Image) reveals latent reward hacking. The negligible performance drop in GRPO and the unexpected\nperformance gain in DAPO (where removing visual inputs actually improves accuracy) indicate that policies\ndegenerate into blind reasoners relying on linguistic shortcuts rather than visual evidence.\nings suggest distinct reward hacking, where the\npolicy degenerates into a blind reasoner. Rather\nthan grounding reasoning in visual perception, the\nmodel exploits linguistic shortcuts to maximize re-\nwards, effectively treating visual data as distractive\nnoise rather than indispensable evidence (Detailed\nanalysis in Section E).\nTo address this perception-reasoning decou-\npling without resorting to complex data construc-\ntion (Liu et al., 2024a) or external reward engi-\nneering (Zhang et al., 2025d), we propose Think-\ning with Deltas, a novel framework centered\naround a Differential Visual Reasoning Policy\n(DVRP). Rather than relying on proxy textual re-\nwards, DVRP introduces intrinsic visual supervi-\nsion by constructing a visual triplet input stream:\n(1) Invariant (the original image), (2) Decremental\n(masked visual input), and (3) Incremental (per-\nturbed visual input). Our core insight is that gen-\nuine visual reasoning must be strictly sensitive to\nthe Delta (∆) of visual information. Specifically,\nDVRP enforces a differential policy that maximizes\nthe divergence between the Invariant and Decre-\nmental states (proving visual sensitivity) while min-\nimizing the divergence between the Invariant and\nIncremental states (ensuring visual robustness). By\noptimizing these differential signals end-to-end,\nDVRP effectively compels the model to attend to\nvisual evidence without requiring expensive exter-\nnal tools or dense annotations.\nOur contributions are summarized as follows:\n• We identify the perception-reasoning decou-\npling in current multimodal RL as a primary\nbottleneck, arguing that standard objectives\noptimize for linguistic plausibility rather than\nvisual grounding.\n• We introduce the DVRP, an algorithm that\nleverages visual triplets to construct intrinsic\nDelta supervision signals for both visual sen-\nsitivity and robustness, inherently bolstering\nvisual perception and reasoning capabilities.\n• Extensive experiments spanning domains\nfrom general mathematical reasoning to\nspecialized medical diagnostics demonstrate\nthat DVRP significantly outperforms state-of-\nthe-art RLVR methods like GRPO (Shao et al.,\n2024) and DAPO (Yu et al., 2025), effectively\nenhancing visual perception capabilities.\n2\nRelated Work\n2.1\nMultimodal Reasoning\nRecent advancements in Multimodal Large Lan-\nguage Models (MLLMs) have significantly ex-\ntended the reasoning capabilities of LLMs to vi-\nsual domains (Wang et al., 2024a). Pioneering\nworks such as LLaVA (Liu et al., 2024a) and Qwen-\nVL (Bai et al., 2023) demonstrated that visual in-\nstruction tuning could effectively align visual en-\ncoders with LLMs, enabling strong performance\non general visual-oriented benchmarks (Yue et al.,\n2024b). Following this, the community has focused\non enhancing the reasoning depth of these models.\nInspired by the efficacy of CoT in LLMs, the inte-\ngration of Multimodal-CoT (Zhang et al., 2024c;\nSarch et al., 2025) and the curation of high-quality\nreasoning datasets (Chen et al., 2024b, 2023; Dong\net al., 2025; Sun et al.)\nhave emerged as piv-\notal strategies for bolstering visual understanding,\n"}, {"page": 3, "text": "thereby significantly mitigating linguistic halluci-\nnations (Huang et al., 2024; Leng et al., 2024).\nTo substantially enhance visual understanding, a\nnovel paradigm, which termed visual reasoning in\naction has emerged, advocating for thinking with\nimages by explicitly incorporating visual content\nwithin CoT processes (Su et al., 2025). Repre-\nsentative strategies employ visual programming\nvia code generation (Surís et al., 2023; Gupta and\nKembhavi, 2023; Lin et al., 2025), leverage exter-\nnal visual utilities (e.g., crop, zoom, rotate) (Zheng\net al., 2025b; Zhang et al., 2025a; Qi et al., 2024;\nLiu et al., 2023), or orchestrate diverse expert mod-\nels to tackle multimodal tasks (Shen et al., 2023;\nLu et al., 2023). Further explorations extend to\ntool-integrated, multi-turn, and multi-agent system\nfor interleaved reasoning (Wu et al., 2025; Chen\net al., 2024c; Wang et al., 2025b; Man et al., 2025)\nand unified frameworks (Han et al., 2025a; Li et al.,\n2025a). However, these methods rely heavily on\nexternal sources such as expensive trajectory data,\nagentic tools, or advanced teacher models. This\nincurs significant overhead and does not fundamen-\ntally improve the model’s intrinsic visual percep-\ntion and reasoning capabilities.\n2.2\nAdvancements in RLVR Frameworks\nRecent research on RLVR focuses on optimizing\nthree critical components: Data, Reward, and\nRollout.\nData Curation. High-quality Chain-of-Thought\ninitialization is a prerequisite for stable RL train-\ning (Chen et al., 2025b; Huang et al., 2025a; Zhang\net al., 2025c; Liang et al., 2025). Complementing\nthis, recent studies employ dynamic data selection\nstrategies, such as filtering trajectories based on\nvalue estimation (Wang et al., 2025d) or applying\nperplexity- and difficulty-based correction mech-\nanisms (Kong et al., 2024; Jiang et al., 2025) to\nenhance sample efficiency and diversity.\nReward Engineering. Granular reward design\nserves as a pivotal mechanism in RLVR (DeepSeek-\nAI et al., 2025). Dominant approaches rely on\noutcome-based signals, including rigorous accu-\nracy verification (Liu et al., 2025b), strict format-\nting and length constraints (Parthasarathi et al.,\n2025; Zhang and Zuo, 2025), caption alignment\nmetrics (Gou et al., 2025b) for visual-semantic\nunderstanding, and grounding constraints (Zhang\net al., 2025d). To further mitigate reasoning short-\ncuts, recent methods augment these with process-\nlevel constraints (Jiang et al., 2025), aiming to bol-\nster the procedural correctness and reliability of\nintermediate reasoning chains.\nRollout Optimization. Enhancing the efficiency\nof the exploration phase is another active frontier.\nRecent strategies focus on optimizing the rollout\nspace by dynamically adjusting sampling temper-\natures (Liao et al., 2025), expanding the sampling\nspace (Liu et al., 2025a; Li et al., 2025b), polishing\nreasoning processes (Fan et al., 2025), and em-\nploying tree-search algorithms to diversify gener-\nation paths (Ji et al., 2025). These methods aim\nto balance exploration and exploitation, ensuring\nthe policy covers a broader solution space without\ndiverging into incoherence.\nHowever, these strategies relegate the visual\nmodality to an auxiliary role relative to text, ne-\nglecting the critical need to enhance visual robust-\nness and sensitivity in MLLMs. In contrast, our\nproposed DVRP intrinsically fosters the model’s\nvisual perception capabilities.\n3\nMethod\n3.1\nProblem Formulation\nPreliminaries on GRPO.\nWe consider a mul-\ntimodal reasoning task where a policy model πθ,\nparameterized by θ, takes a visual input I and a\ntextual query q to generate a reasoning chain fol-\nlowed by a final answer, denoted as o. The training\ndataset is represented as D = {(I, q, a)}, where a\nis the ground truth answer.\nRecent advancements in RLVR have largely\nadopted GRPO (Shao et al., 2024) to enhance rea-\nsoning capabilities without the need for a separate\nvalue network. Formally, for each input instance\n(I, q), GRPO samples a group of G outputs {oi}G\ni=1\nfrom the old policy πθold. A rule-based reward\nfunction r(o, a) evaluates the correctness of each\noutput (e.g., format compliance and answer accu-\nracy), assigning a reward Ri. To reduce variance,\nthe advantage ˆAi for the i-th output is computed\nby normalizing the rewards within the group:\nˆAi =\nRi −mean({Rj}G\nj=1)\nstd({Rj}G\nj=1) + ϵ\n,\n(1)\nwhere ϵ is a small constant for numerical stability.\nThe optimization objective of GRPO is defined as:\nJGRPO(θ) = Eq∼D,{oi}∼πθold\n\"\n1\nG\nG\nX\ni=1\nLclip(oi, ˆAi)\n#\n,\n(2)\n"}, {"page": 4, "text": "Figure 2: The framework of DVRP. Our method bridges the perception-reasoning decoupling via a visual triplet\ncontrastive learning objective. The upper stream represents the standard reasoning rollout. The lower streams enforce\ntwo critical visual properties: (1) Visual Robustness: minimizing the KL-divergence (KLnoise) between predictions\non original and noise-perturbed inputs (+∆); and (2) Visual Sensitivity: maximizing the divergence (KLmask)\nwhen critical visual semantics are occluded (−∆). An entropy regularization term H prevents distribution collapse.\nwhere Lclip denotes the standard PPO clipped sur-\nrogate loss:\nLclip = min\n\u0010\nρi ˆAi, clip(ρi, 1 −ϵclip, 1 + ϵclip) ˆAi\n\u0011\n,\n(3)\nwith ρi =\nπθ(oi|I,q)\nπθold(oi|I,q) representing the probability\nratio. Following recent practices (Yu et al., 2025),\nwe remove the KL divergence penalty term to en-\ncourage sufficient exploration.\nThe Perception-Reasoning Decoupling.\nA crit-\nical limitation of current multimodal RLVR\nparadigms is their failure to enforce the causal\ndependency between visual perception and reason-\ning paths (Jiang et al., 2025). By relying solely\non outcome-based textual rewards and reasoning in\nlanguage medium, these methods treat visual inputs\nas optional context rather than necessary evidence.\nConsequently, existing frameworks largely over-\nlook the model’s sensitivity to critical visual se-\nmantics and robustness against visual perturba-\ntions, inadvertently incentivizing linguistic short-\ncuts over visually grounded reasoning (Han et al.,\n2025b).\n3.2\nVisual Triplets as Intrinsic Supervision\nTo bridge this gap, we propose DVRP, which in-\ntroduces intrinsic supervision via a Visual Triplet\nmechanism. As illustrated in Figure 2 , for each\ntraining instance, we construct three distinct views:\n• Invariant View (I): The original visual input,\nserving as the anchor for standard reasoning.\nThe policy distribution under this view is de-\nnoted as πOri\nθ\n= πθ(·|I, q).\n• Decremental View (Imask): A “−∆” state\ngenerated by masking visual regions ran-\ndomly. This creates a counterfactual scenario\nwhere visual evidence is lost. The correspond-\ning policy is πMask\nθ\n= πθ(·|Imask, q).\n• Incremental View (Inoise): A “+∆” state\ngenerated by injecting non-semantic Diffusion\nnoise. This simulates environmental instabil-\nity while preserving semantics. The corre-\nsponding policy is πNoise\nθ\n= πθ(·|Inoise, q).\nThis triplet structure allows us to operationalize two\ncomplementary learning signals: Visual Sensitiv-\nity (diverging from πMask\nθ\n) and Visual Robustness\n(aligning with πNoise\nθ\n). We conducted comprehen-\nsive experiments to investigate the insights and\napplicability of our approach, which are detailed in\nSection D.\n3.3\nOptimization Objective\nFormally, DVRP optimizes a unified objective that\nbalances task performance with intrinsic visual\ngrounding. We integrate visual triplet supervision\ninto the GRPO framework to enforce two comple-\nmentary constraints: (1) Visual Sensitivity: The\n"}, {"page": 5, "text": "Method\nGeneral Multimodal Reasoning\nMedical Multimodal Reasoning\nOverall\nGeo3k\nVista\nWeMath\nMVerse\nMVerse-V\nMMKI2\nAVG\n∆rel\nSlake\nPath\nRad\nPMC\nAVG\n∆rel\nAVG\n∆rel\nQwen2.5-VL-3B Backbone\nBase Model\n20.6\n40.6\n23.9\n30.9\n28.2\n34.8\n29.8\nRef.\n48.7\n59.2\n40.3\n46.8\n48.7\nRef.\n37.4\nRef.\nGRPO\n28.7\n59.3\n58.9\n55.3\n52.2\n57.2\n51.9\n↑74.2%\n70.9\n74.5\n71.2\n53.4\n67.5\n↑38.6%\n58.2\n↑55.6%\nDAPO\n31.2\n60.9\n60.0\n56.3\n53.0\n66.8\n54.7\n↑83.6%\n71.3\n72.4\n70.8\n56.6\n67.8\n↑39.2%\n59.9\n↑60.2%\nNoiseRollout\n32.5\n63.0\n60.1\n56.9\n53.5\n63.4\n54.9\n↑84.2%\n72.5\n76.1\n75.0\n61.0\n71.2\n↑46.2%\n61.4\n↑64.2%\nPAPO\n30.9\n61.3\n60.1\n57.1\n53.9\n57.3\n53.4\n↑79.2%\n72.1\n75.9\n74.8\n60.7\n70.9\n↑45.6%\n60.4\n↑61.5%\nDVRPD (Ours)\n35.1\n64.9\n60.5\n58.1\n54.8\n60.9\n55.7\n↑86.9%\n76.3\n78.9\n75.9\n62.2\n73.3\n↑50.5%\n62.7\n↑67.6%\nDVRPG (Ours)\n34.5\n65.5\n60.3\n57.7\n54.5\n61.2\n55.6\n↑86.6%\n74.1\n77.5\n76.2\n61.5\n72.3\n↑48.5%\n62.3\n↑66.6%\nQwen2.5-VL-7B Backbone\nBase Model\n33.8\n55.9\n41.8\n45.6\n36.9\n43.7\n43.0\nRef.\n63.7\n60.6\n61.3\n52.2\n59.5\nRef.\n49.6\nRef.\nGRPO\n40.2\n65.5\n66.1\n66.5\n61.7\n72.1\n62.0\n↑44.2%\n74.7\n75.7\n74.9\n56.2\n70.4\n↑18.3%\n65.4\n↑31.9%\nDAPO\n35.9\n61.9\n58.5\n55.6\n51.0\n71.9\n55.8\n↑29.8%\n74.9\n76.2\n67.8\n57.2\n69.0\n↑16.0%\n61.1\n↑23.2%\nNoiseRollout\n39.7\n67.8\n65.3\n66.1\n62.8\n70.5\n62.0\n↑44.2%\n72.8\n74.1\n72.2\n59.9\n69.8\n↑17.3%\n65.1\n↑31.3%\nPAPO\n40.2\n69.5\n66.7\n68.4\n64.9\n72.5\n63.7\n↑48.1%\n76.9\n73.4\n77.5\n61.6\n72.4\n↑21.7%\n67.2\n↑35.5%\nDVRPD (Ours)\n43.4\n70.9\n67.8\n68.9\n65.3\n74.1\n65.1\n↑51.4%\n80.3\n79.7\n80.5\n64.2\n76.2\n↑28.1%\n69.5\n↑40.1%\nDVRPG (Ours)\n42.3\n71.1\n68.1\n67.4\n66.7\n75.6\n65.2\n↑51.6%\n81.7\n78.3\n79.9\n65.5\n76.4\n↑28.4%\n69.7\n↑40.5%\nTable 1: Performance (avg@8 acc %) comparison of Qwen2.5-VL-3B and 7B backbones. We compare our DVRP\nagainst baselines including GRPO, DAPO, and PAPO. Subscripts D and G denote optimizations using DAPO and\nGRPO, respectively. The Base Model (highlighted) serves as the reference for calculating relative improvements\n(∆rel = Method−Base\nBase\n× 100%). For a fair comparison, all baselines are reproduced in the same environment and\nevaluated using the identical suite.\npolicy must diverge from the decremental view\n(πMask\nθ\n) where visual evidence is absent; (2) Vi-\nsual Robustness: The policy must remain con-\nsistent with the incremental view (πNoise\nθ\n) despite\nperturbations.\nHowever, directly constraining the consistency\nin the robustness and sensitivity branches can lead\nto a degenerate solution where the policy collapses\ninto a high-entropy uniform distribution to trivially\nminimize the KL divergence. To mitigate this risk,\nwe incorporate an entropy regularization term to pe-\nnalize high uncertainty, preventing the model from\nconverging to such a trivial state. Consequently, the\nfinal optimization objective JDVRP is formulated\nas:\nJDVRP(θ) =\nJGRPO(θ)\n|\n{z\n}\nMaximize Reward\n+λnec · DKL\n\u0010\nπOri\nθ\n∥πMask\nθ\n\u0011\n|\n{z\n}\nVisual Sensitivity (Max Difference)\n−λrob · DKL\n\u0000πOri\nθ\n∥πNoise\nθ\n\u0001\n|\n{z\n}\nVisual Robustness (Min Difference)\n−λent · E\nh\nH(πNoise\nθ\n) + H(πMask\nθ\n)\ni\n|\n{z\n}\nEntropy Penalty (Prevent Collapse)\n,\n(4)\nwhere πθold (implicitly used in JGRPO) serves as\nthe frozen reference distribution to stabilize the\nreward maximization, while the triplet objectives\nenforce constraints on the current policy manifold.\n4\nExperiments\n4.1\nExperiment Setup and Training Details\nImplementation Framework. Our experimental\nframework is built upon Easy-R1 (Zheng et al.,\n2025a), implemented using Python 3.10 and Py-\nTorch 2.4.0 (Paszke et al., 2019) with CUDA\n12.4 support. Following the training paradigm es-\ntablished by DeepSeek-R1 (DeepSeek-AI et al.,\n2025), we employed Direct Reinforcement Fine-\nTuning (RFT) to optimize the Qwen2.5-VL-7B and\n3B backbones (Qwen et al., 2025). All experi-\nments were conducted on a computational cluster\nequipped with 4× NVIDIA A800 GPUs.\nTraining Datasets. To foster robust reasoning\nacross domains, we employ ViRL39K (Wang et al.,\n2025a) for mathematical training, while for the\nmedical domain, we construct a composite dataset\nby amalgamating the training splits of Slake (Liu\net al., 2021), PathVQA (He et al., 2020), Rad-\nVQA (Lau et al., 2018), and PMC-VQA (Zhang\net al., 2024b).\nTraining Details. To demonstrate the versatility\nof the proposed DVRP, we integrated DVRP with\ntwo fundamental RLVR algorithms, DAPO (Yu\net al., 2025) and GRPO (Shao et al., 2024), and\nevaluated its impact on the Qwen2.5-VL-7B and\n3B backbones (Qwen et al., 2025). We conducted\nextensive ablation studies to determine the optimal\nmasking ratios and noise levels (Jiang et al., 2025).\nDetails are provided in Appendix C. Following the\nmethodology of NoiseRollout (Liu et al., 2025a),\n"}, {"page": 6, "text": "Method\nIn-Domain Datasets\nOut-of-Domain Datasets\nPMCVQA\nVQA-RAD\nSLAKE\nPathVQA\nMedXpertQA\nMMMU-Med\nProprietary Models\nGemini-2.0-flash-lite\n50.8\n59.4\n73.1\n64.9\n–\n58.7\nGPT-4.1-Nano\n53.1\n61.8\n73.1\n70.6\n–\n60.6\nGPT-4o\n–\n63.9\n71.6\n75.9\n–\n–\nGeneral-purpose Multimodal VLMs\nQwen-VL-Chat (Wang et al., 2024b)\n36.6\n47.0\n56.0\n55.1\n–\n32.7\nYi-VL-34B (AI et al., 2025)\n39.5\n53.0\n58.9\n47.3\n–\n41.5\nLLaVA-v1.6-7B (Liu et al., 2024b)\n35.5\n52.6\n57.9\n47.9\n–\n33.1\nLLaVA-v1.6-13B (Liu et al., 2024b)\n36.6\n55.8\n58.9\n51.9\n–\n39.3\nLLaVA-v1.6-34B (Liu et al., 2024b)\n44.4\n58.6\n67.3\n59.1\n–\n48.8\nLLaVA-v1.5-LLaMA3-8B (Contributors, 2023)\n36.4\n54.2\n59.4\n54.1\n–\n38.2\nMedical Multimodal VLMs\nMed-Flamingo (Moor et al., 2023)\n23.3\n45.4\n43.5\n54.7\n22.1\n28.3\nRadFM (Wu et al., 2023)\n25.9\n50.6\n34.6\n38.7\n23.4\n27.0\nLLaVA-Med-7B (Li et al., 2023)\n24.7\n51.4\n48.6\n56.8\n20.8\n36.9\nLLaVA_Med-LLaMA3-8B (Contributors, 2023)\n46.6\n60.2\n61.2\n54.5\n–\n41.1\nPubMedVision-8B (Chen et al., 2024a)\n52.7\n63.8\n74.5\n59.9\n–\n49.1\nHuatuoGPT-Vision-34B (Chen et al., 2024a)\n58.2\n68.1\n76.9\n63.5\n22.1\n54.4\nMedVLThinker-7B (Huang et al., 2025b)\n57.5\n63.7\n67.8\n65.2\n20.9\n57.0\nCAPO-7B (Jiang et al., 2025)\n55.5\n78.5\n79.1\n68.9\n–\n60.0\nMedical Agentic Systems\nMedAgents (Tang et al., 2024)\n–\n65.6\n67.9\n63.2\n–\n49.7\nMDAgents (Kim et al., 2024)\n–\n66.8\n68.2\n65.4\n–\n52.3\nAFlow (Zhang et al., 2025b)\n–\n67.3\n68.9\n66.4\n–\n53.6\nMMedAgent-RL-7B (Xia et al., 2025)\n–\n71.5\n76.2\n72.3\n–\n66.4\nOurs\nBase Model (Qwen2.5-VL-3B)\n46.8\n40.3\n48.7\n59.2\n20.7\n31.5\n+ DVRPG\n61.5\n76.2\n74.1\n77.5\n23.2\n42.4\n+14.7\n+35.9\n+25.4\n+18.3\n+2.5\n+10.9\n+ DVRPD\n62.2\n75.9\n76.3\n78.9\n24.1\n44.1\n+15.4\n+35.6\n+27.6\n+19.7\n+3.4\n+12.6\nBase Model (Qwen2.5-VL-7B)\n52.2\n61.3\n63.7\n60.6\n20.1\n54.7\n+ DVRPG\n65.5\n79.9\n81.7\n78.3\n24.0\n66.2\n+13.3\n+18.6\n+18.0\n+17.7\n+3.9\n+11.5\n+ DVRPD\n64.2\n80.5\n80.3\n79.7\n25.7\n65.9\n+12.0\n+19.2\n+16.6\n+19.1\n+5.6\n+11.2\nTable 2: Comprehensive performance comparison on 2D medical VQA benchmarks. Bold and underline denote\nbest and second-best scores. The green values indicate the absolute improvement over the corresponding Base\nModel. Subscripts D and G denote optimizations using DAPO and GRPO, respectively.\nwe adopted a sigmoid function for diffusion noise\nannealing (Detailed in Section B.4). We also adopt\nrandom patch masking as the visual sensitivity op-\neration in this work, a choice that has been dis-\ncussed in (Wang et al., 2025g). Specifically, the\nKL divergence terms are computed as the summa-\ntion of token-level KL divergences between the\ncategorical output distributions of the compared\npolicies at each generation step, calculated over\nthe trajectories sampled from the invariant policy\nπOri\nθ\n. Detailed hyperparameter configurations are\nprovided in Appendix B.2.\nEvaluation Protocol.\nWe conduct a compre-\nhensive evaluation across benchmarks spanning\nboth in-domain and out-of-domain settings. Our\ncomparative analysis includes a wide range of\nbaselines, comprising closed-source commercial\nmodels, open-source general-purpose models, and\ndomain-specific reasoning models. For detailed\nexperimental settings, please refer to Section B.1\n4.2\nMain Experiments\n4.2.1\nRLVR Experiments\nWe benchmark DVRP against representative RLVR\nbaselines (Liu et al., 2025a) (Table 1). On the\n"}, {"page": 7, "text": "Method\nMathematical Multimodal Reasoning\nGeo3k\nVista\nWeMath\nMVerse\nMVerse-V\nMMKI2\nAVG\nProprietary Models\nGPT-4o\n–\n64.7\n62.8\n50.2\n53.8\n55.8\n57.5\nGPT-4o-mini\n–\n59.9\n56.3\n42.3\n45.1\n51.9\n51.1\nGemini-2.0-flash\n–\n70.4\n47.4\n47.8\n48.7\n65.2\n55.9\nGeneral-purpose Multimodal VLMs\nQwen2.5-VL-72B (Wang et al., 2024b)\n–\n74.2\n49.1\n47.3\n48.6\n70.5\n57.9\nInternVL2.5-8B (Chen et al., 2025c)\n–\n64.9\n44.9\n37.0\n40.2\n46.8\n46.8\nInternVL2.5-VL-78B (Chen et al., 2025c)\n–\n64.9\n44.9\n37.0\n40.2\n59.8\n49.4\nLLaVA-OneVision-7B (Li et al., 2024)\n–\n58.5\n44.1\n–\n–\n–\n51.3\nLLaVA-OneVision-72B (Li et al., 2024)\n–\n67.1\n32.0\n27.2\n30.1\n–\n39.1\nLLaVA-OneVision-1.5-8B (An et al., 2025)\n–\n69.6\n61.5\n–\n–\n–\n65.6\nLLaVA-Critic-R1-7B (Wang et al., 2025c)\n35.4\n68.7\n62.6\n58.9\n53.1\n57.4\n56.0\nR1-OneVision-7B (Yang et al., 2025)\n30.6\n64.9\n55.2\n61.7\n44.3\n43.3\n50.0\nMath-Specific Multimodal VLMs\nMM-Eureka-7B (Meng et al., 2025a)\n36.4\n59.1\n45.3\n57.6\n56.4\n60.6\n52.6\nMM-Eureka-7B-CPGD (Liu et al., 2025c)\n37.6\n64.2\n64.3\n63.7\n59.2\n64.7\n59.0\nADORA-7B (Gui and Ren, 2025)\n41.2\n61.1\n53.0\n45.2\n41.8\n49.8\n48.7\nR1-VL-7B (Zhang et al., 2025c)\n31.9\n63.5\n56.1\n42.0\n43.2\n55.3\n48.7\nVLAA-Thinker-7B (Chen et al., 2025a)\n24.2\n67.4\n65.9\n47.9\n52.0\n63.2\n53.4\nVL-Rethinker-7B (Wang et al., 2025a)\n33.6\n61.3\n66.5\n64.0\n60.8\n59.8\n57.7\nRACRO-7B-CRO-GRPO (Gou et al., 2025b)\n41.4\n61.7\n68.9\n65.7\n61.7\n70.5\n61.7\nThinkLite-7B-VL (Wang et al., 2025d)\n34.4\n68.9\n63.5\n49.5\n46.0\n56.2\n53.1\nOurs\nBase Model (Qwen2.5-VL-3B)\n20.6\n40.6\n23.9\n30.9\n28.2\n34.8\n29.8\n+ DVRPD\n35.1\n64.9\n60.5\n58.1\n54.8\n60.9\n55.7\n+14.5\n+24.3\n+36.6\n+27.2\n+26.6\n+26.1\n+25.9\n+ DVRPG\n34.5\n65.5\n60.3\n57.7\n54.5\n61.2\n55.6\n+13.9\n+24.9\n+36.4\n+26.8\n+26.3\n+26.4\n+25.8\nBase Model (Qwen2.5-VL-7B)\n33.8\n55.9\n41.8\n45.6\n36.9\n43.7\n43.0\n+ DVRPD\n43.4\n70.9\n67.8\n68.9\n65.3\n74.1\n65.1\n+9.6\n+15.0\n+26.0\n+23.3\n+28.4\n+30.4\n+22.1\n+ DVRPG\n42.3\n71.1\n68.1\n67.4\n66.7\n75.6\n65.2\n+8.5\n+15.2\n+26.3\n+21.8\n+29.8\n+31.9\n+22.2\nTable 3: Performance comparison on mathematical multimodal reasoning benchmarks. Bold and underline denote\nthe best and second-best performance, respectively. The green values indicate the absolute improvement over the\ncorresponding Base Model. Subscripts D and G denote optimizations using DAPO and GRPO, respectively.\n7B scale, our approach consistently establishes\nnew state-of-the-art results. Implementing DVRP\non GRPO (DVRPG) yields an overall accuracy of\n69.7%, representing a 40.5% relative improvement\nover the base model and surpassing the strongest\nbaseline PAPO by 2.5 points. In the medical do-\nmain, DVRPG reaches 76.4% accuracy with a rela-\ntive gain of 28.4%, demonstrating differential vi-\nsual constraints effectively enhance robustness in\nspecialized reasoning tasks. For further case stud-\nies and reasoning consistency evaluation, please\nrefer to Section G.\nThe performance gain is even more pronounced\non the smaller 3B backbone. DVRPD boosts the\noverall accuracy from 37.4% to 62.7%, achiev-\ning a remarkable 67.6% relative improvement.\nCrucially, our 3B model attains 73.3% on medi-\ncal benchmarks, which surpasses the 70.4% accu-\nracy of the significantly larger 7B GRPO baseline.\nThese results validate that DVRP effectively com-\npensates for limited model capacity by maximizing\nthe utilization of visual evidence.\n"}, {"page": 8, "text": "Method\nGeneral Multimodal Reasoning\nMedical Multimodal Reasoning\nOverall\nGeo3k\nVista\nWeMath\nMVerse\nMVerse-V\nMMKI2\nAVG\nSlake\nPath\nRad\nPMC\nAVG\nAVG\nQwen2.5-VL-7B Backbone\nBase Model (Original)\n33.8\n55.9\n41.8\n45.6\n36.9\n43.7\n43.0\n63.7\n60.6\n61.3\n52.2\n59.5\n49.6\nBaseline (GRPO)\n40.2\n65.5\n66.1\n66.5\n61.7\n72.1\n62.0\n74.7\n75.7\n74.9\n56.2\n70.4\n65.4\n+ Visual Sensitivity\n41.5\n68.8\n67.2\n67.8\n64.8\n73.9\n64.0\n78.5\n79.2\n78.2\n61.4\n74.3\n68.1\n+ Visual Robustness\n40.9\n67.2\n66.6\n67.0\n63.2\n73.0\n63.0\n76.2\n76.5\n80.2\n59.1\n73.0\n67.0\nDVRPG (Full)\n42.3\n71.1\n68.1\n67.4\n66.7\n75.6\n65.2\n81.7\n78.3\n79.9\n65.5\n76.4\n69.7\nTable 4: Ablation study of Visual Sensitivity (via masking) and Visual Robustness (via noise injection) on\nthe Qwen2.5-VL-7B backbone. The Base Model (gray) represents the original pre-trained weights, while the\nBaseline utilizes the standard GRPO algorithm. While specific constraints (e.g., masking on PathVQA or noise on\nVQA-RAD) may yield marginal gains on individual datasets, the full DVRPG framework (green) achieves the best\noverall performance, demonstrating the synergy of the combined objectives.\n4.2.2\nDomain Foundation Model Experiments\nWe evaluate DVRP against a wide spectrum of\nproprietary, general-purpose, and domain-specific\nfoundation models across medical and mathemat-\nical benchmarks. In the medical domain, as de-\ntailed in Table 2, our 7B model establishes new\nstate-of-the-art performance on in-domain tasks,\nconsistently outperforming specialized baselines\nlike CAPO-7B (Jiang et al., 2025) and HuatuoGPT-\nVision-34B (Chen et al., 2024a).\nNotably, on\nPathVQA, DVRPD achieves 79.7% accuracy, sur-\npassing the proprietary GPT-4o score of 75.9% and\nrivaling complex agentic systems like MMedAgent-\nRL without requiring external tools or trajectory\ndata (Zhang et al., 2025b).\nTurning to the mathematical domain in Table 3,\nDVRP demonstrates superior generalization capa-\nbilities. Our 7B model attains an average accuracy\nof 65.2%, significantly outperforming the 72B-\nparameter Qwen2.5-VL at 57.9% and GPT-4o at\n57.5%. It also establishes a clear advantage over\nmath-specific baselines such as RACRO-7B (Gou\net al., 2025a) and MM-Eureka (Meng et al., 2025a),\nwhich score 61.7% and 59.0% respectively. These\nresults indicate that by explicitly enforcing dif-\nferential visual sensitivity and robustness, DVRP\nensures consistent generalization across diverse\ndatasets and model scales. Crucially, this approach\nempowers parameter-efficient models to rival or\neven exceed the reasoning capabilities of signifi-\ncantly larger counterparts and more sophisticated\nagentic reasoning systems (Xia et al., 2025).\n4.3\nAblation Experiments\nTo investigate the individual contributions of the\nproposed components, we conduct an ablation\nstudy on the Qwen2.5-VL-7B backbone using\nGRPO as the baseline (Table 4). The introduction\nof Visual Sensitivity yields a substantial improve-\nment over the baseline, confirming that penalizing\nblind reasoning is critical for enforcing genuine\nvisual grounding. Similarly, the Visual Robustness\nterm independently enhances performance by fos-\ntering stability against perturbations. While indi-\nvidual components may occasionally outperform\nthe unified model on specific tasks, the full DVRP\nframework achieves the best overall performance.\nThis demonstrates that sensitivity and robustness\nconstraints are complementary, effectively syner-\ngizing to maximize reasoning reliability across di-\nverse domains.\nFurthermore, we explore the impact of pertur-\nbation intensity across domains. Our experiments\nreveal a domain-specific dichotomy: general mul-\ntimodal reasoning (e.g., Math) benefits from ag-\ngressive perturbations (Pmask = 0.6, Tinit = 500)\nto enforce structural dependency, whereas medical\nreasoning requires milder regularization (Pmask =\n0.2, Tinit = 100) to preserve fine-grained patholog-\nical features. We provide a detailed discussion of\nthese hyperparameter sensitivities in Appendix C.\n5\nConclusion\nIn this work, we address the critical perception-\nreasoning decoupling in current multimodal RLVR\nparadigms by proposing Thinking with Deltas.\nDriven by the Differential Visual Reasoning Pol-\nicy, our framework leverages self-supervised vi-\nsual triplets to introduce intrinsic supervision,\ncompelling models to strictly align their reason-\ning with the presence and stability of visual evi-\ndence. This approach natively enhances perception\nwithout relying on external dependencies.Extensive\nexperiments across mathematical and medical do-\nmains demonstrate that DVRP effectively bridges\nthe perception-reasoning decoupling and empow-\n"}, {"page": 9, "text": "ers parameter-efficient models (e.g., 3B and 7B) to\nachieve state-of-the-art performance, often rivaling\nsignificantly larger commercial baselines.\nLimitations\nDespite the robust performance and generalization\ncapabilities our method demonstrates across di-\nverse domains and comparative settings, several\nlimitations remain. First, our empirical evaluation\nis currently confined to models with 3B and 7B\nparameters. Due to computational resource con-\nstraints, we have not yet scaled the proposed frame-\nwork to larger foundation models (e.g., 70B+ pa-\nrameters) or verified its efficacy across a broader\nspectrum of architectural backbones. Consequently,\nthe scalability of differential visual constraints on\nmassive-scale models and their transferability to\ndifferent model families remain to be fully char-\nacterized. Second, our current approach focuses\non intrinsic policy optimization and has not yet\nexplored integration with agentic systems. Syn-\nergizing DVRP with multi-agent frameworks or\ninteractive tool-use pipelines to achieve more so-\nphisticated reasoning strictly grounded on visual\nevidence remains an unexplored frontier. We hope\nthis work inspires future research into differential\nconstraints as a minimalist yet powerful paradigm\nfor building trustworthy and visually grounded mul-\ntimodal systems.\nReferences\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen\nHuang, Ge Zhang, Guanwei Zhang, Guoyin Wang,\nHeng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang,\nKaidong Yu, Peng Liu, Qiang Liu, Shawn Yue,\nSenbin Yang, Shiming Yang, and 14 others. 2025.\nYi: Open foundation models by 01.ai.\nPreprint,\narXiv:2403.04652.\nXiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang,\nXiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen\nXu, Changrui Chen, Chunsheng Wu, Huajie Tan,\nChunyuan Li, Jing Yang, Jie Yu, Xiyao Wang, Bin\nQin, Yumeng Wang, Zizhen Yan, Ziyong Feng, and 3\nothers. 2025. Llava-onevision-1.5: Fully open frame-\nwork for democratized multimodal training. Preprint,\narXiv:2509.23661.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,\nRunji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, and 29 others. 2023. Qwen technical\nreport. Preprint, arXiv:2309.16609.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, and 12 others. 2020.\nLan-\nguage models are few-shot learners.\nPreprint,\narXiv:2005.14165.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, and 1 others. 2024.\nA survey on evaluation of large language models.\nACM transactions on intelligent systems and technol-\nogy, 15(3):1–45.\nHardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng\nTang, Xinya Du, Yuyin Zhou, and Cihang Xie. 2025a.\nSft or rl? an early investigation into training r1-like\nreasoning large vision-language models. Preprint,\narXiv:2504.11468.\nJunying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian\nChen, Guiming Hardy Chen, Xidong Wang, Ruifei\nZhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang\nWan, and Benyou Wang. 2024a. Huatuogpt-vision,\ntowards injecting medical visual knowledge into mul-\ntimodal llms at scale. Preprint, arXiv:2406.19280.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. 2023. Shikra: Unleashing\nmultimodal llm’s referential dialogue magic. arXiv\npreprint arXiv:2306.15195.\nLin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Con-\nghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\n2024b. Sharegpt4v: Improving large multi-modal\nmodels with better captions. In European Confer-\nence on Computer Vision, pages 370–387. Springer.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,\nChenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\nLu, Yi-Hsin Hung, Chen Qian, and 1 others. 2024c.\nAgentverse: Facilitating multi-agent collaboration\nand exploring emergent behaviors. In ICLR.\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee,\nPeng Xu, Mohammad Shoeybi, Bryan Catanzaro,\nand Wei Ping. 2025b. Acereason-nemotron: Advanc-\ning math and code reasoning through reinforcement\nlearning. arXiv preprint arXiv:2505.16400.\nZhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu,\nZhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,\nHao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang,\nQingyun Li, Yiming Ren, Zixuan Chen, Jiapeng Luo,\nJiahao Wang, Tan Jiang, Bo Wang, and 23 others.\n2025c. Expanding performance boundaries of open-\nsource multimodal models with model, data, and\ntest-time scaling. Preprint, arXiv:2412.05271.\nXTuner Contributors. 2023. Xtuner: A toolkit for effi-\nciently fine-tuning llm. https://github.com/Int\nernLM/xtuner.\n"}, {"page": 10, "text": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning. Preprint,\narXiv:2501.12948.\nYuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang,\nWinston Hu, Yongming Rao, and Ziwei Liu. 2025.\nInsight-v: Exploring long-chain visual reasoning\nwith multimodal large language models. In Proceed-\nings of the Computer Vision and Pattern Recognition\nConference, pages 9062–9072.\nKaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan\nZhou, and Xiangyu Yue. 2025. Sophiavl-r1: Re-\ninforcing mllms reasoning with thinking reward.\nPreprint, arXiv:2505.17018.\nYunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Xin\nJin, Zhenguo Li, James T. Kwok, and Yu Zhang.\n2025a.\nPerceptual decoupling for scalable multi-\nmodal reasoning via reward-optimized captioning.\narXiv preprint arXiv:2506.04559.\nYunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Xin\nJin, Zhenguo Li, James T. Kwok, and Yu Zhang.\n2025b. Reasoning-aligned perception decoupling\nfor scalable multi-modal reasoning.\nPreprint,\narXiv:2506.04559.\nLujun Gui and Qingnan Ren. 2025. Training reasoning\nmodel with dynamic advantage estimation on rein-\nforcement learning. https://www.notion.so/Tr\naining-Reasoning-Model-with-Dynamic-Advan\ntage-Estimation-on-Reinforcement-Learnin\ng-1a830cc0904681fa9df3e076b6557a3e. Notion\nBlog.\nTanmay Gupta and Aniruddha Kembhavi. 2023. Vi-\nsual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recogni-\ntion, pages 14953–14962.\nFeng Han, Yang Jiao, Shaoxiang Chen, Junhao Xu,\nJingjing Chen, and Yu-Gang Jiang. 2025a. Control-\nthinker: Unveiling latent semantics for controllable\nimage generation through visual reasoning. arXiv\npreprint arXiv:2506.03596.\nJunlin Han, Shengbang Tong, David Fan, Yufan Ren,\nKoustuv Sinha, Philip Torr, and Filippos Kokkinos.\n2025b.\nLearning to see before seeing: Demysti-\nfying llm visual priors from language pre-training.\nPreprint, arXiv:2509.26625.\nXuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and\nPengtao Xie. 2020. Pathvqa: 30000+ questions for\nmedical visual question answering. arXiv preprint\narXiv:2003.10286.\nQidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,\nConghui He, Jiaqi Wang, Dahua Lin, Weiming\nZhang, and Nenghai Yu. 2024. Opera: Alleviating\nhallucination in multi-modal large language models\nvia over-trust penalty and retrospection-allocation. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 13418–\n13427.\nWenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao,\nZheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui\nLin. 2025a. Vision-r1: Incentivizing reasoning capa-\nbility in multimodal large language models. Preprint,\narXiv:2503.06749.\nXiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang,\nand Yuyin Zhou. 2025b.\nMedvlthinker: Simple\nbaselines for multimodal medical reasoning. arXiv\npreprint.\nYuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xi-\nangxiang Chu, and Liaoni Wu. 2025. Tree search\nfor llm agent reinforcement learning. arXiv preprint\narXiv:2509.21240.\nSongtao Jiang, Yuan Wang, Ruizhe Chen, Yan Zhang,\nRuilin Luo, Bohan Lei, Sibo Song, Yang Feng, Ji-\nmeng Sun, Jian Wu, and Zuozhu Liu. 2025. Capo:\nReinforcing consistent reasoning in medical decision-\nmaking. Preprint, arXiv:2506.12849.\nYubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu\nChan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee,\nMarzyeh Ghassemi, Cynthia Breazeal, and Hae Won\nPark. 2024.\nMdagents:\nAn adaptive collabora-\ntion of llms for medical decision-making. Preprint,\narXiv:2404.15155.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213.\nKeyi Kong, Xilie Xu, Di Wang, Jingfeng Zhang, and\nMohan Kankanhalli. 2024. Perplexity-aware correc-\ntion for robust alignment with noisy preferences. In\nThe Thirty-eighth Annual Conference on Neural In-\nformation Processing Systems.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nJason J Lau, Soumya Gayen, Asma Ben Abacha, and\nDina Demner-Fushman. 2018. A dataset of clini-\ncally generated visual questions and answers about\nradiology images. Scientific data, 5(1):1–10.\nSicong Leng, Hang Zhang, Guanzheng Chen, Xin\nLi, Shijian Lu, Chunyan Miao, and Lidong Bing.\n2024. Mitigating object hallucinations in large vision-\nlanguage models through visual contrastive decod-\ning. In Proceedings of the IEEE/CVF Conference\n"}, {"page": 11, "text": "on Computer Vision and Pattern Recognition, pages\n13872–13882.\nAng Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui\nCai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie\nNeiswanger, Furong Huang, and 1 others. 2025a.\nZebra-cot: A dataset for interleaved vision language\nreasoning. arXiv preprint arXiv:2507.16746.\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng\nLi, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,\nYanwei Li, Ziwei Liu, and Chunyuan Li. 2024.\nLlava-onevision: Easy visual task transfer. Preprint,\narXiv:2408.03326.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto\nUsuyama, Haotian Liu, Jianwei Yang, Tristan Nau-\nmann, Hoifung Poon, and Jianfeng Gao. 2023. Llava-\nmed: Training a large language-and-vision assis-\ntant for biomedicine in one day.\narXiv preprint\narXiv:2306.00890.\nYuting Li, Lai Wei, Kaipeng Zheng, Jingyuan Huang,\nGuilin Li, Bo Wang, Linghe Kong, Lichao Sun, and\nWeiran Huang. 2025b. Revisiting visual understand-\ning in multimodal reasoning through a lens of image\nperturbation. Preprint, arXiv:2506.09736.\nYiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin\nLiu, James Tompkin, Mengdi Xu, Mengzhou Xia,\nZhengzhong Tu, Laixi Shi, and Jiacheng Zhu.\n2025. Modomodo: Multi-domain data mixtures for\nmultimodal llm reinforcement learning. Preprint,\narXiv:2505.24871.\nMengqi Liao, Xiangyu Xi, Ruinian Chen, Jia Leng, Yan-\ngen Hu, Ke Zeng, Shuai Liu, and Huaiyu Wan. 2025.\nEnhancing efficiency and exploration in reinforce-\nment learning for llms. Preprint, arXiv:2505.18573.\nKevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dan-\ntong Zhu, Dongxing Mao, Linjie Li, Philip Torr, and\nAlex Jinpeng Wang. 2025. Vcode: a multimodal\ncoding benchmark with svg as symbolic visual repre-\nsentation. Preprint, arXiv:2511.02778.\nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and\nXiao-Ming Wu. 2021. Slake: A semantically-labeled\nknowledge-enhanced dataset for medical visual ques-\ntion answering. Preprint, arXiv:2102.09542.\nHaogeng Liu, Quanzeng You, Xiaotian Han, Yongfei\nLiu, Huaibo Huang, Ran He, and Hongxia Yang.\n2024a. Visual anchors are strong information aggre-\ngators for multimodal large language model. arXiv\npreprint arXiv:2405.17815.\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. 2024b. Llava-\nnext: Improved reasoning, ocr, and world knowledge.\nShilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng\nLi, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su,\nJun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li.\n2023. Llava-plus: Learning to use tools for creating\nmultimodal agents. Preprint, arXiv:2311.05437.\nXiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu\nDou, Haonan Wang, Tianyu Pang, and Michael Qizhe\nShieh. 2025a.\nNoisyrollout: Reinforcing visual\nreasoning with data augmentation. arXiv preprint\narXiv:2504.13055.\nZiyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xi-\naoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin,\nand Jiaqi Wang. 2025b. Visual agentic reinforcement\nfine-tuning. Preprint, arXiv:2505.14246.\nZongkai Liu, Fanqing Meng, Lingxiao Du, Zhixi-\nang Zhou, Chao Yu, Wenqi Shao, and Qiaosheng\nZhang. 2025c. Cpgd: Toward stable rule-based re-\ninforcement learning for language models. Preprint,\narXiv:2505.12504.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-\nWei Chang, Michel Galley, and Jianfeng Gao. 2024.\nMathvista: Evaluating mathematical reasoning of\nfoundation models in visual contexts.\nIn Inter-\nnational Conference on Learning Representations\n(ICLR).\nPan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. 2021.\nInter-gps: Interpretable geometry problem solving\nwith formal language and symbolic reasoning. In\nThe Joint Conference of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (ACL-IJCNLP 2021).\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and\nJianfeng Gao. 2023. Chameleon: Plug-and-play com-\npositional reasoning with large language models. Ad-\nvances in Neural Information Processing Systems,\n36:43447–43478.\nRuilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu,\nXinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang.\n2025. Ursa: Understanding and verifying chain-of-\nthought reasoning in multimodal mathematics. arXiv\npreprint arXiv:2501.04686.\nYunze Man, De-An Huang, Guilin Liu, Shiwei Sheng,\nShilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong\nWang, and Zhiding Yu. 2025. Argus: Vision-centric\nreasoning with grounded chain-of-thought. In Pro-\nceedings of the Computer Vision and Pattern Recog-\nnition Conference, pages 14268–14280.\nFanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang\nZhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han,\nBotian Shi, Wenhai Wang, Junjun He, Kaipeng\nZhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and\nWenqi Shao. 2025a. Mm-eureka: Exploring the fron-\ntiers of multimodal reasoning with rule-based rein-\nforcement learning. Preprint, arXiv:2503.07365.\nFanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang\nZhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wen-\nhai Wang, Junjun He, Kaipeng Zhang, and 1 others.\n2025b. Mm-eureka: Exploring visual aha moment\n"}, {"page": 12, "text": "with rule-based large-scale reinforcement learning.\narXiv preprint arXiv:2503.07365.\nMichael Moor, Qian Huang, Shirley Wu, Michihiro Ya-\nsunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes\nReis, Pranav Rajpurkar, and Jure Leskovec. 2023.\nMed-flamingo:\na multimodal medical few-shot\nlearner. Preprint, arXiv:2307.15189.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. Preprint, arXiv:2203.02155.\nPrasanna Parthasarathi, Mathieu Reymond, Boxing\nChen, Yufei Cui, and Sarath Chandar. 2025. Grpo-λ:\nCredit assignment improves llm reasoning. Preprint,\narXiv:2510.00194.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nand 2 others. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Preprint,\narXiv:1912.01703.\nJi Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong\nLv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao\nDong, and 1 others. 2024. Cogcom: A visual lan-\nguage model with chain-of-manipulations reasoning.\narXiv preprint arXiv:2402.04236.\nRunqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu,\nChong Sun, Xiaoshuai Song, Zhuoma GongQue,\nShanglin Lei, Zhe Wei, Miaoxuan Zhang, and 1 oth-\ners. 2024. We-math: Does your large multimodal\nmodel achieve human-like mathematical reasoning?\narXiv preprint arXiv:2407.01284.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan\nLin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jingren Zhou, and 25 oth-\ners. 2025.\nQwen2.5 technical report.\nPreprint,\narXiv:2412.15115.\nGabriel Sarch, Snigdha Saha, Naitik Khandelwal,\nAyush Jain, Michael J Tarr, Aviral Kumar, and\nKaterina Fragkiadaki. 2025. Grounded reinforce-\nment learning for visual reasoning. arXiv preprint\narXiv:2505.23678.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.\nDeepseekmath: Pushing the limits of mathemati-\ncal reasoning in open language models. Preprint,\narXiv:2402.03300.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends\nin hugging face. Advances in Neural Information\nProcessing Systems, 36:38154–38180.\nZhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu,\nYan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide\nZeng, Zhengyuan Yang, and 1 others. 2025. Think-\ning with images for multimodal reasoning: Founda-\ntions, methods, and future frontiers. arXiv preprint\narXiv:2506.23918.\nHaoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang,\nWenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, and\nXiaosong Wang. Chiron-o1: Igniting multimodal\nlarge language models towards generalizable medi-\ncal reasoning via mentor-intern collaborative search.\nIn The Thirty-ninth Annual Conference on Neural\nInformation Processing Systems.\nDídac Surís, Sachit Menon, and Carl Vondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, pages 11888–\n11898.\nXiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming\nLi, Yilun Zhao, Xingyao Zhang, Arman Cohan, and\nMark Gerstein. 2024. Medagents: Large language\nmodels as collaborators for zero-shot medical rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2024, pages 599–621.\nHaozhe Wang, Chao Qu, Zuming Huang, Wei Chu,\nFangzhen Lin, and Wenhu Chen. 2025a.\nVl-\nrethinker:\nIncentivizing self-reflection of vision-\nlanguage models with reinforcement learning. arXiv\npreprint arXiv:2504.08837.\nJiacong Wang, Zijian Kang, Haochen Wang, Haiy-\nong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao\nRan, Xiao Liang, Chao Feng, and 1 others. 2025b.\nVgr: Visual grounded reasoning.\narXiv preprint\narXiv:2506.11991.\nJiaqi Wang, Hanqi Jiang, Yiheng Liu, Chong Ma,\nXu Zhang, Yi Pan, Mengyuan Liu, Peiran Gu, Sichen\nXia, Wenjun Li, and 1 others. 2024a. A comprehen-\nsive review of multimodal large language models:\nPerformance and challenges across different tasks.\narXiv preprint arXiv:2408.01319.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-\nhao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei\nDu, Xuancheng Ren, Rui Men, Dayiheng Liu,\nChang Zhou, Jingren Zhou, and Junyang Lin. 2024b.\nQwen2-vl: Enhancing vision-language model’s per-\nception of the world at any resolution.\nPreprint,\narXiv:2409.12191.\nXiyao Wang, Chunyuan Li, Jianwei Yang, Kai Zhang,\nBo Liu, Tianyi Xiong, and Furong Huang. 2025c.\nLlava-critic-r1: Your critic model is secretly a strong\npolicy model. Preprint, arXiv:2509.00676.\n"}, {"page": 13, "text": "Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin\nLu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong\nHuang, and Lijuan Wang. 2025d. Sota with less:\nMcts-guided sample selection for data-efficient vi-\nsual reasoning self-improvement.\narXiv preprint\narXiv:2504.07934.\nYiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren,\nLiyuan Liu, Baolin Peng, Hao Cheng, Xuehai He,\nKuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang\nWang, Simon Shaolei Du, and Yelong Shen. 2025e.\nReinforcement learning for reasoning in large lan-\nguage models with one training example. Preprint,\narXiv:2504.20571.\nYuan Wang, Shujian Gao, Jiaxiang Liu, Songtao Jiang,\nHaoxiang Xia, Xiaotian Zhang, Zhaolu Kang, Yemin\nWang, and Zuozhu Liu. 2025f. Beyond n-grams: A\nhierarchical reward learning framework for clinically-\naware medical report generation.\narXiv preprint\narXiv:2512.02710.\nZhenhailong Wang, Xuehang Guo, Sofia Stoica,\nHaiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi\nChen, Yangyi Chen, Ming Yan, Fei Huang, and Heng\nJi. 2025g. Perception-aware policy optimization for\nmultimodal reasoning. Preprint, arXiv:2507.06448.\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2023. Towards generalist foundation\nmodel for radiology by leveraging web-scale 2d&3d\nmedical data. Preprint, arXiv:2308.02463.\nMingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li,\nKaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengx-\niang Zhai, and Klara Nahrstedt. 2025.\nVtool-r1:\nVlms learn to think with images via reinforcement\nlearning on multimodal tool use.\narXiv preprint\narXiv:2505.19255.\nPeng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian\nWu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu,\nYan Lu, and Huaxiu Yao. 2025. Mmedagent-rl: Op-\ntimizing multi-agent collaboration for multimodal\nmedical reasoning. Preprint, arXiv:2506.00555.\nYi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang,\nYan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin,\nFengyun Rao, Minfeng Zhu, Bo Zhang, and Wei\nChen. 2025. R1-onevision: Advancing generalized\nmultimodal reasoning through cross-modal formal-\nization. arXiv preprint arXiv:2503.10615.\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie\nXia, and Pengfei Liu. 2025. Limo: Less is more for\nreasoning. arXiv preprint arXiv:2502.03387.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,\nXiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,\nGaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin,\nZhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan\nTong, Chi Zhang, Mofan Zhang, Wang Zhang, and\n16 others. 2025.\nDapo: An open-source llm re-\ninforcement learning system at scale.\nPreprint,\narXiv:2503.14476.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao\nYu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, and\n3 others. 2024a. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark\nfor expert agi. In Proceedings of CVPR.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, and 1 others. 2024b.\nMmmu: A massive multi-discipline multimodal un-\nderstanding and reasoning benchmark for expert agi.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 9556–\n9567.\nHaoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule\nBai, Chubin Zhang, Bowen Zhang, Zhichao Zhou,\nDongliang He, and Yansong Tang. 2025a. Think-\ning with videos: Multimodal tool-augmented rein-\nforcement learning for long video reasoning. arXiv\npreprint arXiv:2508.04416.\nJiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng,\nXionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin\nCheng, Sirui Hong, Jinlin Wang, Bingnan Zheng,\nBang Liu, Yuyu Luo, and Chenglin Wu. 2025b.\nAflow: Automating agentic workflow generation.\nPreprint, arXiv:2410.10762.\nJingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu,\nXikun Zhang, Shijian Lu, and Dacheng Tao. 2025c.\nR1-vl: Learning to reason with multimodal large\nlanguage models via step-wise group relative policy\noptimization. arXiv preprint arXiv:2503.12937.\nJixiao Zhang and Chunsheng Zuo. 2025. Grpo-lead: A\ndifficulty-aware reinforcement learning approach for\nconcise mathematical reasoning in language models.\nPreprint, arXiv:2504.09696.\nRenrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun\nLin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan\nLu, Kai-Wei Chang, Peng Gao, and Hongsheng Li.\n2024a. Mathverse: Does your multi-modal llm truly\nsee the diagrams in visual math problems? Preprint,\narXiv:2403.14624.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong\nLin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024b.\nPmc-vqa: Visual instruction tuning for medical visual\nquestion answering. Preprint, arXiv:2305.10415.\nYuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jin-\ntai Chen, and Kaishun Wu. 2025d. Medtvt-r1: A\nmultimodal llm empowering medical reasoning and\ndiagnosis. Preprint, arXiv:2506.18512.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2024c.\nMulti-\nmodal chain-of-thought reasoning in language mod-\nels. Preprint, arXiv:2302.00923.\n"}, {"page": 14, "text": "Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi\nFeng, Dongdong Kuang, and Yuwen Xiong. 2025a.\nEasyr1: An efficient, scalable, multi-modality rl train-\ning framework.\nZiwei Zheng, Michael Yang, Jack Hong, Chenxiao\nZhao, Guohai Xu, Le Yang, Chao Shen, and Xing\nYu. 2025b. Deepeyes: Incentivizing\" thinking with\nimages\" via reinforcement learning. arXiv preprint\narXiv:2505.14362.\nYuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai\nZhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and\nBowen Zhou. 2025. Medxpertqa: Benchmarking\nexpert-level medical reasoning and understanding.\nPreprint, arXiv:2501.18362.\nA\nAppendix\nThis appendix provides implementation details, ad-\nditional quantitative ablations, and extensive quali-\ntative examples to support the findings in the main\npaper. The organization is as follows:\n• §BImplementation Details. We provide a\ndetailed breakdown of our experimental setup,\nincluding:\n– §B.1 Evaluation Details: Detailed pro-\ntocols for Mathematical and Medical\nbenchmarks.\n– §B.2 Hyperparameter: Comprehensive\nlists of training hyperparameters and\nDVRP coefficients.\n– §B.3 Prompt Template: The standard-\nized system prompt and reasoning tem-\nplate used across all experiments.\n– §B.4 Noise Scheduling: We detail the\nvariance-preserving diffusion formula-\ntion and the Sigmoid annealing sched-\nule employed to dynamically modulate\nnoise intensity, implementing a curricu-\nlum learning strategy for visual robust-\nness.\n• §C Ablation on Perturbation Parameters.\nWe present the sensitivity analysis for mask-\ning ratios (Pmask) and noise steps (Tinit).\n• §D Visual Robustness and Sensitivity Anal-\nysis. We analyze the contrasting behaviors of\nMLLMs under diffusion noise versus seman-\ntic masking.\n• §E RLVR Visual Dependency Experiments.\nWe detail the blind experiments (Text-Only\nand Blank Image) used to verify visual\ngrounding.\n• §F Qualitative Analysis of Visual Depen-\ndency. We provide case studies demonstrat-\ning linguistic shortcuts in baseline methods\nunder blind settings.\n• §G Qualitative Comparison of RLVR Al-\ngorithms. We offer a comparative visualiza-\ntion of reasoning trajectories between GRPO,\nDAPO, and our DVRP-D.\nB\nImplementation Details\nB.1\nEvaluation Details\nEvaluation Protocols. To comprehensively as-\nsess model performance, we conducted evaluations\nacross both general (mathematical) and medical\ndomains, distinguishing between in-domain and\nout-of-domain (OOD) settings.\n• Mathematical Evaluation: We employed\nGeo3k (Lu et al., 2021), Vista (Lu et al., 2024),\nWeMath (Qiao et al., 2024), MVerse (Zhang\net al., 2024a), MVerse-V (Zhang et al., 2024a),\nand MMKI2 (Meng et al., 2025b) as OOD\nbenchmarks to test generalization capabilities.\nFurthermore, we utilized mathruler.grader\nto facilitate precise evaluation.\n• Medical Evaluation:\nWe utilized the\ntest splits of Slake (Liu et al., 2021),\nPathVQA (He et al., 2020), RadVQA (Lau\net al., 2018), and PMC-VQA (Zhang et al.,\n2024b) for in-domain evaluation.\nFurther-\nmore, MedXpertQA (Zuo et al., 2025) and\nMMMU-Med (Yue et al., 2024a) were em-\nployed to assess OOD performance.\nTo ensure the statistical reliability of our results, we\nreport the Average Accuracy over 8 runs (AVG@8\nAcc). For inference, we deployed the vLLM en-\ngine to accelerate generation (Kwon et al., 2023).\nFor fair comparison, all models utilized a unified\nsystem prompt (reasoning template) to elicit chain-\nof-thought reasoning, with the temperature set to\n1.0 and top-p to 0.9. The specific templates used\nare detailed in Appendix B.3.\nBaselines. To conduct a comprehensive compar-\native analysis, we evaluate our proposed method\nagainst baselines categorized into two distinct di-\nmensions: RL-based optimization methods and\nfoundational MLLMs.\nRL-based Methods.\nWe benchmark our ap-\nproach against state-of-the-art RL strategies de-\n"}, {"page": 15, "text": "signed for reasoning or visual alignment, includ-\ning GRPO (Shao et al., 2024), DAPO (Yu et al.,\n2025), PAPO (Wang et al., 2025g), and NoiseRoll-\nout (Liu et al., 2025a). To ensure a fair comparison,\nwe re-trained these RL methods using the identical\ntraining set and evaluation suite.\nFoundation Models. We compare against a diverse\nspectrum of baselines ranging from proprietary\ncommercial systems to specialized open-source\nmodels:\n• Medical-Specific Models: We evaluate per-\nformant open-source models tailored for the\nbiomedical domain, such as HuatuoGPT-\nVision-34B (Chen et al., 2024a).\n• Math-Specific Models: For mathematical rea-\nsoning tasks, we include domain-expert mod-\nels like MM-Eureka (Meng et al., 2025a) and\nThinkLite-7B (Wang et al., 2025d).\n• General-Purpose Models: We also include\nwidely adopted general open-source bench-\nmarks, such as Qwen-VL (Bai et al., 2023)\nand LLaVA-v1.6 (Liu et al., 2024b).\nB.2\nHyperparameter\nTo ensure reproducibility, we provide a comprehen-\nsive overview of the training and evaluation hyper-\nparameters in Table 5 and Table 6. All models are\noptimized using AdamW with a constant learning\nrate of 1e-6 and a global batch size of 128. Consis-\ntent with recent reinforcement learning practices,\nwe remove the KL penalty relative to the reference\nmodel to facilitate broader policy exploration (Liu\net al., 2025a; Yu et al., 2025).\nCrucially, our DVRP framework introduces\nthree auxiliary objectives regulated by specific co-\nefficients: Visual Robustness (λrob = 0.01), Visual\nSensitivity (λnec = 0.01), and Entropy Regular-\nization (λent = 0.05). The above loss weights\nare consistent with previous works (Wang et al.,\n2025g; Liu et al., 2025a). To address the vary-\ning information density of visual modalities, we\nemploy domain-adaptive perturbation strategies.\nAs detailed in Table 5, we apply a higher mask\nprobability (Pmask = 0.6) and longer noise injec-\ntion steps (Tinit = 500) for mathematical reason-\ning, while adopting milder perturbations (Pmask =\n0.2, Tinit = 100) for medical tasks to preserve fine-\ngrained pathological details.\nHyperparameter\nValue\nGeneral Training\nOptimizer\nAdamW (BF16)\nLearning Rate\n1e-6\nLR Schedule\nConstant\nTotal Epochs\n3\nRL Process (GRPO)\nGlobal Batch Size\n128\nRollout Batch Size\n384\nGroup Size (G)\n5\nKL Penalty (Ref Model)\nNone (Disabled)\nReward Signal\nAccuracy\nDVRP Objectives\nVisual Robustness Loss (λrob)\n0.01\nVisual Sensitivity Loss (λnec)\n0.01\nEntropy Regularization (λent)\n0.05\nVisual Perturbation\nMath\n/\nMedical\nMask Probability (Pmask)\n0.6\n/\n0.2\nPatch Size (Pmask)\n14\n/\n14\nNoise Steps (Tinit)\n500\n/\n100\nInference\nTemperature\n1.0\nTop-p\n0.99\nMax New Tokens\n2048\nResources & Efficiency\nCompute\n4× NVIDIA A800\nStep Time\n∼1000–2000s\nTable 5: Key hyperparameters for training and eval-\nuation. DVRP parameters vary by domain (Math vs.\nMedical). Notations correspond to Eq. (4).\nB.3\nPrompt Template\nFor the prompt selection, we employ a standardized\ntemplate across all training and evaluation stages\nto elicit stable Chain-of-Thought (CoT) reasoning.\nAs illustrated in Figure 3, the template explicitly\ninstructs the model to generate an internal mono-\nlogue within <think> tags before producing the\nfinal answer, ensuring the output format remains\nconsistent for automated parsing and reward calcu-\nlation.\nB.4\nNoise Scheduling\nWe adopt a variance-preserving (VP) diffusion pro-\ncess to construct the incremental view Inoise, simu-\nlating environmental instability while maintaining\nsemantic consistency. Given an original image I\nand a noise intensity coefficient β ∈[0, 1], the\nperturbed input is formulated as:\nInoise =\np\n1 −β · I +\np\nβ · ϵ,\nϵ ∼N(0, I) (5)\n"}, {"page": 16, "text": "Hyperparameter\nValue\nGeneral Training\nOptimizer\nAdamW (BF16)\nLearning Rate\n1e-6\nLR Schedule\nConstant\nTotal Epochs\n3\nRL Process (DAPO)\nGlobal Batch Size\n128\nRollout Batch Size\n384\nMini-Rollout Batch Size\n128\nGroup Size (G)\n5\nKL Penalty (Ref Model)\nNone (Disabled)\nReward Signal\nAccuracy\nDVRP Objectives\nVisual Robustness Loss (λrob)\n0.01\nVisual Sensitivity Loss (λnec)\n0.01\nEntropy Regularization (λent)\n0.05\nVisual Perturbation\nMath\n/\nMedical\nMask Probability (Pmask)\n0.6\n/\n0.2\nPatch Size (Pmask)\n14\n/\n14\nNoise Steps (Tinit)\n500\n/\n100\nInference\nTemperature\n1.0\nTop-p\n0.99\nMax New Tokens\n2048\nResources & Efficiency\nCompute\n4× NVIDIA A800\nStep Time\n∼1000–2000s\nTable 6: Key hyperparameters for training and evalua-\ntion based on the DAPO + DVRP configuration.\nwhere ϵ represents standard Gaussian noise. The\ncoefficient β determines the signal-to-noise ratio; a\nhigher β introduces stronger perturbation.\nTo balance structural robustness learning with\nconvergence stability, we introduce a curriculum\nlearning strategy via a Sigmoid decay schedule.\nSpecifically, the noise intensity β is dynamically\nannealed based on the training progress. Let k be\nthe current training step and K be the total steps.\nWe compute the diffusion timestep t as:\nt(k) = Tinit · σ\n\u0012\nγ ·\n\u0012\n0.5 −k\nK\n\u0013\u0013\n,\n(6)\nwhere Tinit is the initial noise step (e.g., 500), σ(·)\nis the sigmoid function, and γ is a scaling factor\ncontrolling the decay steepness (set to 10 in our\nexperiments). The noise intensity is then derived\nas βk = t(k)/Tmax. This schedule imposes high-\nvariance perturbations in the early stages to enforce\nbroad structural invariance, while gradually anneal-\ning the noise to zero to refine fine-grained visual\nconsistency.\nC\nAblation on Perturbation Parameters\nWe investigate the sensitivity of the DVRP frame-\nwork to the intensity of visual perturbations. We\nperform ablations on the Qwen2.5-VL-7B (Qwen\net al., 2025) backbone by varying the masking ra-\ntio Pmask ∈{0.2, 0.4, 0.6} and the noise injection\nsteps Tinit ∈{100, 300, 500}. The results are de-\ntailed in Table 7.\nMasking Ratio.\nWe observe a distinct diver-\ngence in optimal masking ratios between domains.\nFor General Multimodal Reasoning (e.g., Math),\nperformance improves consistently as the mask-\ning ratio increases, peaking at Pmask = 0.6. This\nsuggests that geometric diagrams and natural im-\nages contain high spatial redundancy; aggressive\nmasking effectively forces the model to learn struc-\ntural dependencies rather than relying on local\ntextures. In contrast, Medical Multimodal Rea-\nsoning favors a conservative ratio (Pmask = 0.2).\nPerformance drops significantly (from 74.3% to\n71.4%) when the ratio is increased to 0.6. This\nindicates that medical diagnosis relies heavily on\nfine-grained visual details (e.g., small lesions or\nboundaries), which are easily destroyed by aggres-\nsive occlusion.\nNoise Injection Level.\nA similar trend is ob-\nserved for noise robustness. We implement noise\ninjection via a diffusion process, utilizing a sigmoid\nschedule to modulate the noise intensity. In the\ngeneral domain, the model benefits from stronger\nconsistency regularization, achieving peak accu-\nracy with higher diffusion timesteps (Tinit = 500).\nConversely, the medical domain is sensitive to high-\nintensity diffusion perturbations. Performance de-\ngrades as Tinit increases, with the optimal setting\nfound at a lower timestep (Tinit = 100). We hy-\npothesize that excessive diffusion noise in medical\nimages risks corrupting subtle pathological features\nor mimicking sensor artifacts, thereby confusing\nthe reasoning policy.\nBased on these observations, our final DVRPG\nmodel adopts domain-specific settings: we apply\nstronger perturbations (P = 0.6, T = 500) for\nmath tasks to encourage robustness, and milder\nperturbations (P = 0.2, T = 100) for medical\ntasks to preserve visual fidelity.\nImpact of Noise Structure.\nTable 8 further scru-\ntinizes the efficacy of structured Diffusion Noise\n"}, {"page": 17, "text": "Method / Setting\nGeneral Multimodal Reasoning\nMedical Multimodal Reasoning\nOverall\nGeo3k\nVista\nWeMath\nMVerse\nMVerse-V\nMMKI2\nAVG\nSlake\nPath\nRad\nPMC\nAVG\nAVG\nQwen2.5-VL-7B Backbone\nBase Model (Original)\n33.8\n55.9\n41.8\n45.6\n36.9\n43.7\n43.0\n63.7\n60.6\n61.3\n52.2\n59.5\n49.6\nBaseline (GRPO)\n40.2\n65.5\n66.1\n66.5\n61.7\n72.1\n62.0\n74.7\n75.7\n74.9\n56.2\n70.4\n65.4\nAblation I: Sensitivity to Masking Ratio (Pmask)\n(Math favors high mask, Medical favors low mask)\n+ Masking (P = 0.2)\n41.0\n68.2\n66.8\n66.9\n63.5\n73.5\n63.3\n78.5\n79.2\n78.2\n61.4\n74.3\n67.7\n+ Masking (P = 0.4)\n41.8\n69.5\n67.5\n67.1\n65.1\n74.8\n64.3\n76.8\n78.1\n77.4\n60.2\n73.1\n67.8\n+ Masking (P = 0.6)\n42.3\n71.1\n68.1\n67.4\n66.7\n75.6\n65.2\n74.5\n76.5\n75.8\n58.9\n71.4\n67.7\nAblation II: Robustness to Noise Injection (Tinit)\n(Math needs strong noise, Medical needs weak noise)\n+ Noise (T = 100)\n41.2\n68.5\n66.9\n67.0\n64.2\n73.9\n63.6\n76.2\n76.5\n80.2\n59.1\n73.0\n67.4\n+ Noise (T = 300)\n41.9\n70.1\n67.6\n67.2\n65.8\n74.9\n64.6\n75.1\n75.8\n78.4\n57.8\n71.8\n67.5\n+ Noise (T = 500)\n42.3\n71.1\n68.1\n67.4\n66.7\n75.6\n65.2\n73.5\n74.2\n76.1\n56.5\n70.1\n67.2\nDVRPG (Optimal)\n42.3\n71.1\n68.1\n67.4\n66.7\n75.6\n65.2\n81.7\n78.3\n79.9\n65.5\n76.4\n69.7\nTable 7: Comprehensive ablation study of the DVRP framework on Qwen2.5-VL-7B. We systematically vary the\nMasking Ratio (Pmask ∈{0.2, 0.4, 0.6}) and Noise Steps (Tinit ∈{100, 300, 500}). The results highlight a\ndomain-specific dichotomy: General/Math tasks benefit from aggressive perturbations (P = 0.6, T = 500) to\nenforce structural reasoning, whereas Medical tasks require milder regularization (P = 0.2, T = 100) to preserve\nfine-grained pathological features. The final DVRPG model employs these domain-optimal settings.\nMethod / Setting\nMedical Multimodal Reasoning\nSlake\nPath\nRad\nPMC\nAVG\nBase Model (Original)\n63.7\n60.6\n61.3\n52.2\n59.5\nBaseline (GRPO)\n74.7\n75.7\n74.9\n56.2\n70.4\nComparison of Noise Types\n(Fixed scheduling, varying structure)\n+ Gaussian Noise (σ = 0.1)\n75.2\n76.0\n76.5\n57.0\n71.2\n+ Diffusion Noise (T = 100)\n76.2\n76.5\n80.2\n59.1\n73.0\nTable 8: Ablation study comparing noise injection\nstrategies in the Medical domain. Under the DVRP\nframework, structured Diffusion Noise (T = 100) out-\nperforms unstructured Gaussian Noise (σ = 0.1) by\n+1.8%, demonstrating that preserving semantic struc-\nture during perturbation is critical for medical visual\nreasoning.\n(T = 100) versus unstructured Gaussian Noise\n(σ = 0.1) in the medical domain (P = 0.6). The\nresults demonstrate that Diffusion Noise consis-\ntently outperforms Gaussian Noise, achieving an\naverage gain of +1.8%. This suggests that struc-\ntured perturbations effectively regularize the pol-\nicy while preserving essential semantic integrity,\nwhereas unstructured Gaussian noise risks degrad-\ning the pixel-level fidelity required for fine-grained\nmedical diagnostics.\nD\nVisual Robustness and Sensitivity\nAnalysis\nTo investigate the reliance of MLLMs on visual\nfidelity and their resilience to perturbations, we\nconduct a comprehensive robustness analysis on\nQwen2.5-VL benchmarks (Qwen et al., 2025)\nacross general and medical domains. We introduce\ntwo distinct types of visual perturbations: Diffusion\nNoise, which represents high-frequency corruption,\nand Patch Masking, which represents semantic in-\nformation loss. As visualized in Figure 4, these\nperturbations induce contrasting reasoning behav-\niors: the model remains resilient and consistent\nunder noise (+∆) but becomes confused and diver-\ngent when visual semantics are masked (−∆). This\nqualitative observation is quantitatively confirmed\nby the results in Table 9, serving as the empirical\nfoundation for our proposed method.\nRobustness to Diffusion Noise.\nAs shown in the\nNoise rows of Table 9, MLLMs exhibit strong sta-\nbility against diffusive perturbations. We imple-\nment noise injection via a diffusion scheduler con-\ntrolled by a sigmoid function. Despite these pertur-\nbations, the Qwen2.5-VL-3B model maintains per-\nformance and even achieves a slight average gain\nof +0.6% in general reasoning tasks. Similarly, the\n7B model shows negligible performance fluctua-\ntions (∆< 0.3%) across most benchmarks. This\nindicates that current MLLMs possess inherent re-\nsilience to the high-frequency jitter introduced by\nthe diffusion scheduler, likely due to the robust fea-\nture extraction capabilities of the vision encoder.\nInsight I: This robustness validates the feasibil-\nity of our Noise Consistency objective. Since the\nmodel demonstrates invariance to diffusion noise,\nexplicitly enforcing output consistency between\nclean and noisy views acts as a safe regularization\nterm. This stabilizes the policy without degrading\nrepresentation quality.\n"}, {"page": 18, "text": "Reasoning Template\nSYSTEM:\nYou are a helpful assistant.\nUSER:\n{question}\nYou first think through the reasoning process as an internal monologue, enclosed within <think>\n</think> tags. Then, provide your final answer enclosed within \\boxed{}.\nFigure 3: The standardized prompt template employed across all training and evaluation phases. To ensure consistent\nreasoning behaviors and facilitate automated answer extraction, we explicitly instruct the model to encapsulate its\nchain-of-thought within <think> tags and place the final result inside a \\boxed{} command.\nSetting\nGeneral Multimodal Reasoning\nMedical Multimodal Reasoning\nGeo3k\nVista\nWeMath\nMVerse\nMVerse-V\nMMK12\nAVG\nSlake\nPath\nRad\nPMC\nAVG\nQwen2.5-VL-3B Backbone\nOriginal\n20.6\n40.6\n23.9\n30.9\n28.2\n34.8\n29.8\n48.7\n59.2\n40.3\n46.8\n48.8\nNoise\n22.6+2.0\n41.4+0.8\n26.5+2.6\n30.8-0.1\n26.2-2.0\n35.1+0.3\n30.4+0.6\n48.1-0.6\n58.5-0.7\n40.6+0.3\n46.2-0.6\n48.4-0.4\nMasked\n6.2-14.4\n24.2-16.4\n17.8-6.1\n21.5-9.4\n16.5-11.7\n31.6-3.2\n19.6-10.2\n31.5-17.2\n37.8-21.4\n26.2-14.1\n29.4-17.4\n31.2-17.6\nQwen2.5-VL-7B Backbone\nOriginal\n33.8\n55.9\n41.8\n45.6\n36.9\n43.7\n42.9\n63.7\n60.6\n61.3\n52.2\n59.5\nNoise\n30.5-3.3\n56.3+0.4\n41.6-0.2\n44.5-1.1\n38.5+1.6\n44.6+0.9\n42.7-0.2\n63.5-0.2\n59.8-0.8\n61.5+0.2\n51.9-0.3\n59.2-0.3\nMasked\n8.0-25.8\n32.4-23.5\n30.5-11.3\n31.2-14.4\n23.2-13.7\n39.6-4.1\n27.5-15.4\n42.1-21.6\n38.4-22.2\n40.5-20.8\n33.2-19.0\n38.6-20.9\nTable 9: Robustness evaluation under visual perturbations. Results are presented as AccuracyDifference. The\ndifference (∆) represents the gap compared to the Original setting. Red subscripts indicate performance gains\n(+∆), while Blue subscripts indicate drops (−∆).\nSensitivity to Semantic Masking.\nConversely,\nthe models demonstrate high sensitivity to patch\nmasking. When a portion of the visual input is oc-\ncluded, performance degrades sharply. Notably,\nthe 7B model suffers a severe average drop of\n−20.9% in the medical domain and −15.4% in\ngeneral tasks. This degradation is significantly\nmore pronounced than that caused by noise, in-\ndicating that reasoning depends heavily on specific\nvisual patches rather than language priors alone.\nInsight II: This sensitivity confirms that visual\ntokens are critical for correct reasoning. It moti-\nvates employing masking as a challenging view\nin the reinforcement learning loop. Exposing the\npolicy to masked inputs forces the model to max-\nimize information utilization from the remaining\nvisible patches, thereby reducing hallucination and\nenhancing the grounding of the reasoning process.\nSummary.\nMLLMs process these two perturba-\ntions through fundamentally different mechanisms.\nDiffusion noise acts as a low-level surface artifact\nto which pre-trained visual encoders exhibit strong\ninvariance. In contrast, patch masking acts as a\nhigh-level semantic disruption that breaks the vi-\nsual continuity required for logical deduction. This\ndisparity highlights a critical insight: while visual\nrepresentations are texturally robust, reasoning poli-\ncies remain fragile to structural incompleteness.\nThis observation directly underpins our method-\nology, where we leverage diffusion noise for con-\nsistency regularization and masking for hardness-\naware policy learning.\nE\nRLVR Visual Dependency Experiments\nTo rigorously investigate whether current RLVR\nmethods genuinely leverage visual information or\nmerely exploit linguistic priors, we conducted a se-\nries of blind experiments. We maintained the iden-\ntical training setup as the main experiments, train-\ning policies on the VIRL-39K (Wang et al., 2025a)\ndataset using the standard hyperparameters for both\nGRPO (Shao et al., 2024) and DAPO (Jiang et al.,\n2025). We also presented case study comparisons\nin Section F. During evaluation, we introduced two\n"}, {"page": 19, "text": "Figure 4: Visual Dependency Analysis. Divergence\nin reasoning paths. Current MLLMs show minimal\nsensitivity to masked vs. perturbed inputs (consistent in\ntop rows), but become confused when visual semantics\nare masked (divergent in bottom row), indicating a lack\nof visual grounding.\nmodality-blind settings to sever the visual semantic\nlink:\n• Blank Image (B/W): The original visual in-\nput is randomly replaced with a solid black or\nwhite image. This removes all visual seman-\ntics while preserving the multimodal input\nstructure.\n• Text Only: The visual tokens are entirely dis-\ncarded, forcing the model to rely solely on\nthe textual query and its internal parametric\nknowledge.\nVisual\nRedundancy\nin\nGRPO.\nTable\n10\npresents the comparative performance across six\ngeneral multimodal reasoning benchmarks. For\nGRPO, removing visual modalities causes a per-\nformance decline from 62.0% to 58.3%. However,\nthis drop is disproportionately small given the mul-\ntimodal nature of these tasks. The fact that the pol-\nicy retains nearly 94% of its original efficacy in the\nblind setting indicates that the model treats visual\nevidence as largely redundant. Rather than ground-\ning reasoning in visual perception, the model relies\nprimarily on linguistic priors and parametric knowl-\nedge.\nVisual Interference in DAPO.\nIn contrast,\nDAPO treats visual input as a distraction. Unex-\npectedly, ablating visual signals leads to marginal\nperformance improvements.\nAs shown in Ta-\nble 10, the Text Only configuration achieves an\naverage accuracy of 56.3%, outperforming the\noriginal multimodal baseline (55.8%).\nSpecifi-\ncally, on MathVista, replacing informative images\nwith blank frames yields a +3.5% gain (61.9% →\n65.4%), and discarding images boosts performance\non PAPO_MMK12 (Meng et al., 2025b) by +1.4%.\nThese results suggest that the current RLVR objec-\ntive encourages the policy to ignore visual infor-\nmation in favor of statistical language correlations,\neffectively treating images as noise.\nThe Cause: Linguistic Shortcuts.\nThese blind\nexperiments identify a critical limitation in current\nmultimodal RLVR paradigms. When optimizing\nsolely for outcome-based textual rewards, policies\nlearn to bypass the visual encoder to maximize re-\nward efficiency. Instead of establishing a causal\nlink between perception and reasoning, the model\noverfits to syntactic patterns in the reasoning chain.\nThis decoupling leads to hallucinations, where the\nmodel generates coherent rationales that are de-\ntached from the actual visual input.\nInsight: Enforcing Visual Dependency.\nThese\nfindings motivate our proposed framework. Scaling\nreinforcement learning on multimodal data is insuf-\nficient if the optimization does not explicitly penal-\nize the bypass of visual information. Our DVRP\nframework addresses this by introducing a visual\ntriplet constraint. By maximizing divergence on\nmasked inputs (Sensitivity) and minimizing it on\nperturbed inputs (Robustness), DVRP ensures the\npolicy treats visual tokens as necessary conditions\nfor reasoning. This mechanism re-couples per-\nception and reasoning, ensuring that performance\ngains derive from genuine visual comprehension\nrather than linguistic shortcuts.\nF\nQualitative Analysis of Visual\nDependency\nTo understand the visual bypass phenomenon ob-\nserved in Section E, we qualitatively examine the\nreasoning trajectories of the DAPO policy under\nthree inference settings: Text-Only, Blank Image,\nand the Standard (Original Image) setting. Due to\nspace constraints, the displayed CoT rollouts have\nbeen streamlined using Gemini-3-Pro to ensure\nconciseness while strictly preserving the original\nlogical flow and specific errors. We focus on cases\nwhere visual information is theoretically essential\n"}, {"page": 20, "text": "Method & Setting\nGeneral Multimodal Reasoning Benchmarks\nGeo3k\nVista\nWeMath\nMVerse\nMVer-V\nMMK12\nAVG\nBase Model (Qwen2.5-VL-7B)\n33.8\n55.9\n41.8\n45.6\n36.9\n43.7\n43.0\nGRPO Policy (Original)\n40.2\n65.5\n66.1\n66.5\n61.7\n72.1\n62.0\nw/ Black/White Image\n37.8-2.4\n64.8-0.7\n65.2-0.9\n64.2-2.3\n60.3-1.4\n66.6-5.5\n59.8-2.2\nw/ Text Only Input\n34.1-6.1\n34.1-3.5\n63.1-3.0\n63.2-3.3\n61.1-0.6\n66.5-5.6\n58.3-3.7\nDAPO Policy (Original)\n35.9\n61.9\n58.5\n55.6\n51.0\n71.9\n55.8\nw/ Black/White Image\n34.9-1.0\n65.4+3.5\n58.1-0.4\n54.2-1.4\n51.3+0.3\n68.2-3.7\n55.4-0.4\nw/ Text Only Input\n34.6-1.3\n62.5+0.6\n57.4-1.1\n55.2-0.4\n54.5+3.5\n73.3+1.4\n56.3+0.5\nTable 10: Ablation study on visual dependency. We compare the performance stability of GRPO and DAPO under\nBlack/White Image (texture-only) and Text Only (blind) settings. The Base Model serves as the zero-shot\nbaseline. For GRPO and DAPO (highlighted in blue ), the subscripts denote the performance gap (∆) relative to\ntheir respective Original settings. Note that DAPO shows counter-intuitive gains in “Text Only” mode (indicated by\nred subscripts), suggesting a potential bias towards linguistic shortcuts.\nfor deduction. As shown in the examples below, the\npolicy frequently arrives at the correct solution in\nthe blind settings (Text-Only and Blank Image) by\nhallucinating specific visual values or exploiting\nlinguistic artifacts in the questions. This qualitative\nevidence confirms that the model often treats visual\ndata as redundant, achieving high rewards through\nspurious shortcuts rather than genuine multimodal\ncomprehension.\nGEOMETRY3K\nQuestion:\nFind x.\nGT Answer:\n8\nText-Only Policy (No Vision)\nUse\ntangent\ndefinition:\ntan(60◦)\n=\nopp/adj\n=\ny/x\n=\n√\n3.\nFinding x...\nuse\ncosine:\ncos(60◦)\n=\nx/y\n=\n1/2.\nAlso sine:\nsin(60◦) = 8\n√\n3/y =\n√\n3/2.\n[Hallucinated\n8\n√\n3\nfrom\nnowhere]\nAnswer: 8\n✓\nBlank Image Policy (Visual Noise)\ntan(60◦) = y/(8\n√\n3) =\n√\n3.\nNeed adjacent x: cos(60◦) =\nx/y\n=\n1/2.\nDerive y\n=\n8\n√\n3/ cos(60◦) = 16\n√\n3. Use\nPythagoras:\ny2\n=\nx2 +\n(8\n√\n3)2. [Confused derivation]\nAnswer: 8\n✓\nOriginal Policy (DAPO - Ours)\nIdentify 30-60-90 triangle. Side\nopposite 30◦is shortest; oppo-\nsite 60◦is x; hypotenuse is 2×\nshortest. Wait, from diagram:\nside opp 60◦is x, side opp 30◦\nis 8\n√\n3. Using ratio 1 :\n√\n3,\nx = 8\n√\n3/\n√\n3 = 8.\nAnswer: 8\n✓\n"}, {"page": 21, "text": "MATHVERSE\nQuestion:\na ∥b, ∠2 =\n35◦,\n∠3\n=\n40◦. Find ∠1.\nGT Answer:\nB (105◦)\nText-Only Policy (No Vision)\nSince a ∥b and ∠2 = 35◦,\nthen ∠AEC = 35◦. ∠1 =\n∠AEC + ∠3 = 35◦+ 40◦=\n75◦. [Geometric Logic Failure]\nAnswer: B\n✓\nBlank Image Policy (Visual Noise)\nAngle 4 corresponds to Angle\n3, so Angle 4 = 40.0. Angle\n1 is on straight line: 180◦−\nAngle 2 −Angle 4 = 180◦−\n35◦−40◦= 105◦. [Correct\nlogic by chance]\nAnswer: B\n✓\nOriginal Policy (DAPO - Ours)\nSince a ∥b, Angle 4 (alternate\ninterior) = Angle 3 = 40.0. An-\ngle 1, 2, and 4 form a straight\nangle. ∠1 = 180◦−35.0 −\n40.0 = 105.0◦.\nAnswer: B\n✓\nMATHVISTA\nQuestion:\nItems sold < 5\nunits in at least\none store?\nGT Answer:\n2\nText-Only Policy (No Vision)\n\"Very\": both < 5. \"Mill\": candy\n6 (no), cook 3 (<5) -> fails ’at\nleast one’? Logic error. \"Cop\":\ncandy 9, cook 5. \"Dot\": 5 and\n7. [Hallucinated numbers]\nAnswer: 2\n✓\nBlank Image Policy (Visual Noise)\n\"Very\": both 3. \"Mill\": candy 6,\ncook 3. \"Cop\": candy 9, cook 5.\n\"Dot\": candy 5, cook 7. [Identi-\ncal Hallucination to Text-Only]\nAnswer: 2\n✓\nOriginal Policy (DAPO - Ours)\nReading chart: \"Very\" (both\nbars\n3, <5).\n\"Mill\" (Candy\n6, Cook 3 <5). \"Cop\" (Candy\n9, Cook 5). \"Dot\" (Candy 5,\nCook 7). Only Very and Mill\nsatisfy condition.\nAnswer: 2\n✓\nWE-MATH\nQuestion:\nSquare perimeter\n20.\nArea of\nshaded parallelo-\ngram?\nGT Answer:\nD (25)\nText-Only Policy (No Vision)\nPerimeter 20 →side 5. Shaded\narea is two triangles form-\ning half square?\n25/2\n=\n12.5? Options don’t match. Re-\nevaluating: assume standard\nproblem setup where shaded =\nfull area or options imply 25.\nChoosing D.\nAnswer: D\n✓\nBlank Image Policy (Visual Noise)\nPerimeter 20 →side 5. Parallel-\nogram base = side = 5. Height\n= side = 5.\nArea = base ×\nheight = 5 × 5 = 25.\nAnswer: D\n✓\nOriginal Policy (DAPO - Ours)\nPerimeter 20 →side 5.\nVi-\nsual confirmation: Shaded par-\nallelogram spans full base and\nheight of the square. Area =\n5 × 5 = 25.\nAnswer: D\n✓\nPAPO-\nMMK12\nQuestion:\nAmmonia synthe-\nsis ∆H\n<\n0.\nWhich statement\nis incorrect?\nGT Answer:\nA\nText-Only Policy (No Vision)\nA. Incorrect. Exothermic reac-\ntion (∆H < 0): high temp\nshifts equilibrium left (lower\nyield), though it improves rate.\nB. Correct: Pressure increases\nrate. C. Correct: Catalyst low-\ners activation energy. D. Cor-\nrect: Separating NH3 shifts\nequilibrium forward.\nAnswer: A\n✓\nBlank Image Policy (Visual Noise)\nA. Incorrect.\n∆H\n<\n0\nmeans exothermic. Increasing\ntemp shifts equilibrium left (Le\nChatelier), decreasing conver-\nsion. High temp is for kinetics,\nnot yield. B, C, D are standard\ncorrect statements.\nAnswer: A\n✓\nOriginal Policy (DAPO - Ours)\nA. Incorrect. High temp helps\nactivation energy but is unfa-\nvorable for exothermic equilib-\nrium yield. It’s a compromise.\nB. Correct: Pressure increases\nrate (conc. increases). C. Cor-\nrect: Catalyst lowers barrier. D.\nCorrect: Mass action law.\nAnswer: A\n✓\n"}, {"page": 22, "text": "MATHVERSE-\nVIS\nQuestion:\nl\n∥\nm.\nFind\n∠ABC.\nGT Answer:\nD (71)\nText-Only Policy (No Vision)\nl\n∥\nm, transversal AB →\n∠BAC = 38◦. Sum of angles\nin △ABC: ∠ABC + 38◦+\n· · · = 71◦? (Hallucinated cal-\nculation to match options).\nAnswer: D\n✓\nBlank Image Policy (Visual Noise)\n∠BAC = 38◦(alternate inte-\nrior). ∠ACB supplementary\nto 33? No. Calculation: 38 −\n33 = 5◦. [Hallucinated sub-\ntraction logic]\nAnswer: D\n✓\nOriginal Policy (DAPO - Ours)\nUse Exterior Angle Theorem.\n∠BAC is exterior to △ABC?\nNo, diagram shows transversal.\nLogic: 180 −38 = 142. Ex-\nterior angle summation: 38◦+\n33◦= 71◦.\nAnswer: D\n✓\nG\nQualitative Comparison of RLVR\nAlgorithms\nIn this section, we present a comparative case study\nof reasoning trajectories generated by the GRPO\nbaseline (Shao et al., 2024), the DAPO method (Yu\net al., 2025), and our proposed DVRP-D frame-\nwork. Due to space constraints, the displayed CoT\nrollouts have been streamlined using Gemini-3-Pro.\nThis process ensures conciseness while strictly pre-\nserving the original logical flow and specific errors.\nEvaluation Methodology.\nTo conduct a rigorous\nqualitative assessment, we randomly sampled 50\ndistinct cases spanning six diverse math reason-\ning datasets. We employed Gemini-3-Pro as an\nexternal evaluator to assess the quality of reasoning\nchains based on two primary criteria: visual faith-\nfulness and logical consistency. The evaluation\nspecifically focuses on identifying instances where\nthe model generates plausible text that contradicts\nvisual facts (hallucination) or ignores critical visual\ncues (blind reasoning).\nAnalysis of Baselines.\nThe comparative analy-\nsis of these sampled cases reveals that baselines\nfrequently suffer from perception-reasoning decou-\npling, manifesting in distinct failure modes. GRPO\ntends to exhibit visual hallucinations, such as de-\nscribing incorrect object colors or counts to force\na logical conclusion. It also suffers from option\nmapping failures, where valid intermediate cal-\nculations inexplicably lead to incorrect multiple-\nchoice selections.\nSimilarly, while DAPO fol-\nlows a structured format, it frequently demonstrates\nlogical inconsistencies and visual misalignment.\nSpecific errors include failing numerical compara-\ntors despite accurate counting (e.g., concluding\n2 < 4 is false) or misinterpreting graph trends\n(e.g., confusing exponential growth with linear re-\nlationships). These patterns indicate that baseline\nreasoning paths often diverge from visual reality\ndue to an over-reliance on linguistic priors.\nEfficacy of DVRP-D.\nIn contrast, the evaluation\nconfirms that DVRP-D demonstrates superior vi-\nsual grounding. Our method effectively anchors\nreasoning steps to specific visual evidence. Cru-\ncially, the assessment highlights that DVRP-D ex-\nhibits distinct self-correction behaviors. It actively\nverifies visual features during intermediate steps\nand adjusts its reasoning when initial assumptions\nconflict with visual observation. This mechanism\nsignificantly reduces the occurrence of ungrounded\nhallucinations and ensures that the final answer is\ncausally derived from the visual input.\nGEOMETRY3K\nQuestion:\nFind ∠A to near-\nest tenth.\nGT Answer:\n41.8\nGRPO (Baseline)\n...tan(∠A)\n=\n18\n9\n√\n5\n=\n2\n3 .\nNo.\ntan−1(2/3) ≈33.7◦.\nIncorrectly simplifies the fraction, lead-\ning to wrong angle.\nAnswer: 33.7\n×\nDAPO (Baseline)\n...AC = 9\n√\n5. tan(∠A) ≈0.8944.\ntan−1(0.8944) ≈41.4◦.\nCorrect approach but fails at final preci-\nsion/rounding step.\nAnswer: 41.4\n×\nDVRP-D (Ours)\nPythagoras:\nAC\n=\n9\n√\n5.\ntan(∠A) = 2\n√\n5\n5\n. Result ≈41.8◦.\nCorrect\ngeometric\nderivation\nand\ncalculation.\nAnswer: 41.8\n✓\n"}, {"page": 23, "text": "MATHVERSE\nQuestion:\nBE ∥CD. Find\nAE. (A:10, B:12,\nC:15, D:16)\nGT Answer:\nC\nGRPO (Baseline)\nSimilarity ratio 12/16 = AE/(AE +\n5). 12AE + 60 = 16AE →AE =\n15.\nCalculates 15 correctly, but hallucinates\nselecting option A.\nAnswer: A\n×\nDAPO (Baseline)\nProportion\n12\n16\n=\nAE\nAE+5 .\nSolving\ngives AE = 15.\nCorrect calculation, but fails to map re-\nsult to option C.\nAnswer: A\n×\nDVRP-D (Ours)\nBasic Proportionality Theorem. AB\nAC =\nAE\nAD . 12/16 = AE/(AE + 5) →\nAE = 15. Option C.\nRobust reasoning and correct option map-\nping.\nAnswer: C\n✓\nMATHVISTA\nQuestion:\nIs\ncount(front\nfighter)\n<\ncount(behind\nbus)?\nGT Answer:\nYes\nGRPO (Baseline)\nFront: Blue van, Bus (2 objects). Behind:\nYellow van, Plane, Bus, Bicycle (4 ob-\njects). 2 < 4? No.\nCorrect counting, but logical comparator\nfailure (2 < 4 is True).\nAnswer: No\n×\nDAPO (Baseline)\nFront: 2 objects. Behind: 4 objects. Con-\nclusion: No.\nIdentical logical failure; fails to evaluate\ninequality correctly.\nAnswer: No\n×\nDVRP-D (Ours)\nBehind bus: Yellow van, airplane, bus,\nbicycle (4 items). Comparison validates\ncondition.\nCorrectly counts and executes logical\ncomparison.\nAnswer: Y es\n✓\nWE-MATH\nQuestion:\nColor of squares\non upper half of\nblackboard 1\n⃝?\nGT Answer:\nB\nGRPO (Baseline)\nBlackboard 1\n⃝... upper half contains two\nwhite squares.\nVisual hallucination; describes wrong ob-\nject/color.\nAnswer: C\n×\nDAPO (Baseline)\nObserving upper half... Blackboard 1\n⃝\nhas three yellow squares? No, concludes\nC (White).\nConflicted reasoning leading to halluci-\nnated choice.\nAnswer: C\n×\nDVRP-D (Ours)\nFocus on Blackboard 1\n⃝. Upper half con-\ntains yellow squares. Count is 3.\nAccurate visual grounding and attribute\nrecognition.\nAnswer: B\n✓\nPAPO-\nMMK12\nQuestion:\nAmmonia\nequilibrium.\n−lg K\nvs\nT .\nWhich is\nincorrect?\nGT Answer:\nB\nGRPO (Baseline)\nAnalyzes D: vfwd > vrev. Concludes\nD is incorrect.\nMisinterprets the kinetic state at point B.\nAnswer: D\n×\nDAPO (Baseline)\nAnalyzes B: Volume fraction unchanged\nmeans equilibrium (Correct). Analyzes\nD... finds D incorrect.\nCorrectly identifies B is true, but mistak-\nenly targets D as answer.\nAnswer: D\n×\nDVRP-D (Ours)\nEndothermic reaction (K increases with\nT ). Statement B (vol fraction) implies\nequilibrium... B is the answer.\nCorrectly identifies the target statement.\nAnswer: B\n✓\n"}, {"page": 24, "text": "MATHVERSE-\nVIS\nQuestion:\nIdentify\nthe\ngraph\nrelation-\nship (Linear, Exp,\nQuad).\nGT Answer:\nB\nGRPO (Baseline)\nx = 1, y\n= 1 . . . x = 5, y\n=\n31.\nMatches y\n=\nx(x −1)/2?\n(Quadratic).\nFails to fit data points to correct function\nclass.\nAnswer: A\n×\nDAPO (Baseline)\ny = 3x? Linear relationship matches\ngraph?\nSevere visual misalignment; interprets\ncurve as linear.\nAnswer: C\n×\nDVRP-D (Ours)\nRapid increase.\nx = 5, y\n≈31.\nMatches y = 2x (Exponential).\nCorrectly recognizes exponential growth\npattern from plot.\nAnswer: B\n✓\n"}]}