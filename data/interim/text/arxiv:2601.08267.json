{"doc_id": "arxiv:2601.08267", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.08267.pdf", "meta": {"doc_id": "arxiv:2601.08267", "source": "arxiv", "arxiv_id": "2601.08267", "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning", "authors": ["Fan Gao", "Sherry T. Tong", "Jiwoong Sohn", "Jiahao Huang", "Junfeng Jiang", "Ding Xia", "Piyalitt Ittichaiwong", "Kanyakorn Veerakanjana", "Hyunjae Kim", "Qingyu Chen", "Edison Marrese Taylor", "Kazuma Kobayashi", "Akkiko Aizawa", "Irene Li"], "published": "2026-01-13T06:51:40Z", "updated": "2026-01-19T09:20:16Z", "summary": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.08267v2", "url_pdf": "https://arxiv.org/pdf/2601.08267.pdf", "meta_path": "data/raw/arxiv/meta/2601.08267.json", "sha256": "ca4f07a7ac658855bbcc9e4b027b57b3dc469a5d5bc96d4878456900541d5ee5", "status": "ok", "fetched_at": "2026-02-18T02:21:37.041595+00:00"}, "pages": [{"page": 1, "text": "MED-COREASONER: Reducing Language Disparities in Medical\nReasoning via Language-Informed Co-Reasoning\nFan Gao1, Sherry T. Tong1, Jiwoong Sohn2, Jiahao Huang1,3,\nJunfeng Jiang3, Ding Xia1, Piyalitt Ittichaiwong4,\nKanyakorn Veerakanjana4, Hyunjae Kim5, Qingyu Chen5,\nEdison Marrese Taylor1, Kazuma Kobayashi3, Akkiko Aizawa3, Irene Li1\n1The University of Tokyo, 2ETH Zürich,\n3National Institute of Informatics, 4Siriraj Informatics and Data Innovation Center,\n5Yale University\nAbstract\nWhile reasoning-enhanced large language mod-\nels perform strongly on English medical tasks,\na persistent multilingual gap remains, with\nsubstantially weaker reasoning in local lan-\nguages, limiting equitable global medical de-\nployment. To bridge this gap, we introduce\nMED-COREASONER, a language-informed co-\nreasoning framework that elicits parallel En-\nglish and local-language reasoning, abstracts\nthem into structured concepts, and integrates\nlocal clinical knowledge into an English logi-\ncal scaffold via concept-level alignment and re-\ntrieval. This design combines the structural ro-\nbustness of English reasoning with the practice-\ngrounded expertise encoded in local languages.\nTo evaluate multilingual medical reasoning be-\nyond multiple-choice settings, we construct\nMultiMed-X, a benchmark covering seven lan-\nguages with expert-annotated long-form ques-\ntion answering and natural language inference\ntasks, comprising 350 instances per language.\nExperiments across three benchmarks show\nthat MED-COREASONER improves multilin-\ngual reasoning performance by an average of\n5%, with particularly substantial gains in low-\nresource languages. Moreover, model distilla-\ntion and expert evaluation analysis further con-\nfirm that MED-COREASONER produces clini-\ncally sound and culturally grounded reasoning\ntraces.1\n1\nIntroduction\nMedical tasks demand complex reasoning and\nmeticulous deliberation to ensure the safety and re-\nliability of diagnoses (Patel et al., 2005; Griot et al.,\n2025). While reasoning-enhanced Large Language\nModels (LLMs) (Wei et al., 2022; Jaech et al., 2024;\nGuo et al., 2025) show significant promise in these\nlife-critical scenarios (Xie et al., 2024), their capa-\nbilities remain uneven across languages. Specifi-\ncally, models often exhibit substantially stronger\n1Codes and benchmark data are publicly released: https:\n//github.com/astridesa/Med-CoReasoner\nFigure 1: Performance gap between English-thinking\nand local-language-thinking settings under the same\nquery: average scores of GPT-4o and DeepSeek-3.2 on\nMMLU-ProX-Health, with the largest degradation in\nSwahili.\nreasoning when explicitly prompted to think in En-\nglish than when prompted to reason directly in the\nlocal language (Ranaldi and Pucci, 2025). As illus-\ntrated in the figure 1, this English-as-pivot advan-\ntage appears consistently across multiple models,\nhighlighting a persistent multilingual reasoning gap\nthat hinders equitable deployment of medical AI.\nPrevious efforts to address the multilingual gap\nfollow two main approaches: prompting techniques\nand cross-lingual post-training.\nPrompt-based\nmethods (Shi et al., 2022; Qi et al., 2025; Tam\net al., 2025) instruct LLMs to reason in English and\nthen translate outputs to the local language. How-\never, this method introduces systematic limitations:\nmachine translation can be unreliable, especially\nfor low-resource languages (Huang and Liu, 2024;\nPang et al., 2025), and translation-based reasoning\noften fails to preserve culturally grounded clinical\nexpertise, leading to factuality misalignment and\nregional bias (Hu et al., 2025; Liu et al., 2025b;\nSchlicht et al., 2025; Singh et al., 2025). Cross-\nlingual training paradigms (She et al., 2024; Chai\net al., 2025; Chen et al., 2025) aim to equalize per-\n1\narXiv:2601.08267v2  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "formance via multilingual data exposure, but face\ncomplementary challenges: high-quality multilin-\ngual medical reasoning data remain scarce and pre-\ndominantly English-centric (Hu et al., 2025; Liu\net al., 2025b), limiting the effectiveness of data-\ndriven approaches.\nBoth approaches share a common assumption\nthat reasoning must occur primarily in English or\nin the local language. However, this perspective\noverlooks a fundamental question: what distinct\nroles might different languages play in medical\nreasoning? Recent studies indicate that LLMs per-\nform reasoning in an English-centric way, with key\ninferential steps shaped heavily by English (Schut\net al., 2025; Park et al., 2025). In contrast, profes-\nsional medical knowledge is often more accurately\npreserved in the local language (Hu et al., 2025; Liu\net al., 2025b). Building on these findings, we hy-\npothesize a complementary view: pivot-language\nreasoning provides a transferable logical scaffold\n(e.g., step-wise structure and consistency checks),\nwhereas local-language reasoning better encodes\nnuanced, practice-grounded medical knowledge,\nincluding region-specific terminology, guideline\nconventions, and clinically grounded narratives.\nAddressing\nthis,\nwe\nintroduce\nMED-\nCOREASONER, a language-informed cross-lingual\nco-reasoning framework that jointly performs\ndecision-making through parallel English and\nlocal-language reasoning. MED-COREASONER\nextracts structured concepts from both chains,\nuses English as a pivot scaffold, and integrates\nlocal clinical signals via concept-level fusion\nto form a pivot-anchored yet locally grounded\nreasoning process. It further incorporates retrieval-\naugmented (Xiong et al., 2024) to ground the\nreasoning process in authoritative multilingual\nmedical guidelines. This design aims to improve\nmedical reliability by reducing hallucinations and\nenhance fidelity by preserving language-specific\nclinical standards and regional practices.\nTo\ncomprehensively\nevaluate\nmultilingual\nmedical reasoning across tasks, we introduce\nMultiMed-X, a new multilingual benchmark\nspanning seven non-English languages (Chinese,\nJapanese, Korean, Thai, Swahili, Zulu, Yoruba) and\ncovering two tasks: long-form question answering\nand natural language inference. Each instance is\nannotated by expert physicians, with 350 exam-\nples per language. Experiments on MultiMed-X,\ntogether with two multiple-choice QA benchmarks\n(Global-MMLU (Singh et al., 2025) and MMLU-\nProX (Xuan et al., 2025)) and extensive ablations,\nshow that MED-COREASONER improves both the\naccuracy and reliability of clinical decision-making,\nparticularly in low-resource language settings. Be-\nyond final-answer correctness, we further assess\nreasoning quality via automatic proxy evaluation\nderived from model distillation and expert review,\ntargeting clinical soundness and localization of the\ngenerated rationales.\nTo summarize, our work makes the following\nnovel contributions:\n• We propose MED-COREASONER, leveraging\nthe complementary strengths of English and\nlocal-language thinking to focus on reducing\nreasoning disparities in low-resource languages.\n• We introduce MultiMed-X, a multilingual med-\nical reasoning benchmark covering seven non-\nEnglish languages and two tasks with special em-\nphasis on three low-resource African languages.\n• We evaluate MED-COREASONER across mul-\ntiple LLM backbones, benchmarks, and tasks\nin terms of final answer, and further assess rea-\nsoning quality using automatic proxy evaluation\nand expert assessment.\n2\nRelated Work\nMultilingual Medical Reasoning.\nReasoning-\ncentric LLMs such as OpenAI o1 (Jaech et al.,\n2024) and DeepSeek R1 (Guo et al., 2025) lever-\nage test-time computation for step-by-step infer-\nence (Wei et al., 2022); medical variants further op-\ntimize reasoning with verifiable rewards (e.g., Hu-\natuoGPT (Chen et al., 2025), Med-PRM (Yun et al.,\n2025)). To address data scarcity, agent pipelines\n(ReasonMed (Sun et al., 2025), MedReason (Wu\net al., 2025a), MedCaseReasoning (Wu et al.,\n2025b)) synthesize supervision from stronger mod-\nels, while multi-agent systems (MDAgents (Kim\net al., 2024), MedAgents (Tang et al., 2024)) and\nknowledge-grounded methods (Gao et al., 2025b;\nLu et al., 2025) support complex decisions. Yet\nresearch remains pivot-language centric; Qiu et\nal. (Qiu et al., 2024) note multilingual needs,\nbut English vs. non-English reasoning gaps per-\nsist (Shi et al., 2022; Kang et al., 2025). Prior\nremedies—cross-lingual transfer (She et al., 2024;\nChai et al., 2025), synthetic data (Singh et al., 2024;\nChen et al., 2024b), and multilingual CoT (Lu\net al., 2024; Qi et al., 2025; Son et al., 2025) often\ntranslate English reasoning patterns, leaving open\nwhether reasoning is language-agnostic or English-\n2\n"}, {"page": 3, "text": "Figure 2: Illustration of the MED-COREASONER framework. The system first translates user input into English,\nthen conducts parallel reasoning in English and Italian via separate queries. Reasoning outputs are abstracted into\nconcepts and fused into an English-anchored reasoning scaffold, where English provides a logical backbone and\nthe local language supplies linguistically specific details. This concept-based scaffold is used to retrieve relevant\nknowledge and guide the generation of the final Italian reasoning output.\nanchored (Schut et al., 2025; Gao et al., 2025a). In\nthis work, we use English as a transferable scaffold\nwhile integrating local-language aligned consensus\nand clinical nuances.\nLow-Resource Medical Benchmarks.\nExist-\ning work largely centers on single-language med-\nical benchmarks and lacks standardized, paral-\nlel evaluation protocols across languages.\nFor\ninstance, IgakuQA (Kasai et al., 2023) targets\nJapanese, Head-QA focuses on Spanish (Vilares\nand Gómez-Rodríguez, 2019), FrenchMedMCQA\non French (Labrak et al., 2022), RuMedBench\non Russian (Blinov et al., 2022), while MMed-\nBench (Qiu et al., 2024) mainly aggregates hetero-\ngeneous resources rather than providing a unified\nparallel benchmark. Available low-resource med-\nical benchmarks are often non-parallel and lack\nconsistent standardization. Moreover, most bench-\nmarks are formulated as multiple-choice question\nanswering (MCQA) (Nimo et al., 2025; Singh et al.,\n2025; Xuan et al., 2025), which limits task diversity\nand fails to reflect realistic clinical reasoning that\nrequires free-form inference or long-form genera-\ntion. To fill this gap, we introduce MultiMed-X.\n3\nMethodology\nIn this section, we present MED-COREASONER, as\nillustrated in Figure 2. We address the fundamen-\ntal challenge that medical LLMs face significant\nperformance degradation when operating in non-\nEnglish languages. We first formalize the problem,\nthen detail each component: parallel reasoning gen-\neration, cross-lingual concept extraction and fusion,\nknowledge retrieval, and final answer generation.\n3.1\nProblem Formulation\nWe formulate multilingual medical question an-\nswering as follows. Given a medical Ql in the local\nlanguage (e.g., Japanese, Arabic, Swahili, etc), and\naccess to a large language model M and a multi-\nlingual medical knowledge base K, our goal is to\ngenerate an accurate answer Al with sound medical\nreasoning. Formally:\nAl = F(Ql, M, K)\n(1)\nwhere F is our proposed framework.\n3.2\nParallel Reasoning\nGiven a question Ql, we generate two indepen-\ndent reasoning paths that capture complementary\naspects of medical knowledge: one leveraging the\nrich English logical thoughts, and another captur-\ning local language contexts. Crucially, these rea-\nsoning chains are generated independently without\ninformation sharing. This ensures (1) each chain\nfollows its natural reasoning path without bias from\nthe other language; (2) diverse perspectives that\ncan be reconciled through fusion; (3) robustness\nthrough redundancy when chains converge on the\nsame conclusion.\nRe = M(Qe); Rl = M(Ql)\n(2)\n3\n"}, {"page": 4, "text": "where Qe = translate(Ql) if the local language\nis not the English, and we carefully design prompts\nto encourage step-by-step medical reasoning.\n3.3\nConcept Chain Extraction\nRaw reasoning chains contain verbose natural lan-\nguage that is difficult to align cross-lingually. We\nextract structured medical concepts to enable pre-\ncise mapping and fusion. We employ an LLM-\nbased extraction approach that directly identifies\nmedical concepts from reasoning chains.\nC = M(R) = {c1 →c2 →· · · →ck}\n(3)\nHere, C denotes an ordered concept chain,\nwhere c represents a concept and k its index.\nFor each reasoning, the model outputs a list of\nraw concepts in natural language.\nAs shown\nin Figure 2, English reasoning is abstracted as\nCe\n= {Severe frostbite →Dry gangrene →\n· · · →Cellulitis}, while Italian reasoning is ex-\ntracted as Cl = {Congelamento grave →· · · →\nCellulite mesopiede}. The ordering preserves the\nlogical coherence in the reasoning process.\n3.4\nCross-lingual Concept Fusion\nThe concept fusion module integrates English and\nlocal language concept chains into a unified repre-\nsentation, enabling us to leverage the strengths of\nboth languages while maintaining logical consis-\ntency and semantic coherence.\nFusion Strategy.\nAlgorithm 1 in Appendix A de-\ntails our position-aware backbone-augmentation\nfusion strategy. In summary, we treat the English\nconcept chain Ce as the backbone and augment it\nwith local-language concepts that provide comple-\nmentary clinical information. Specifically, we ini-\ntialize Cf ←Ce, then for each cl ∈Cl we compute\nits maximum embedding similarity to concepts in\nCe; if the score exceeds a threshold τ, we add cl to\nCf, anchored to its most similar English concept.\nWe adopt BGE-M3 (Chen et al., 2024a) as the mul-\ntilingual embedding model and set τ to 0.5. We get\nan English-anchored reasoning scaffold by:\nCf = Ce ∪{ cl ∈Cl | max\nce∈Ce sim(cl, ce) > τ } (4)\nThis design is motivated by: (1) Logicality, lever-\naging the superior consistency of multi-step En-\nglish reasoning; (2) Complementarity, integrating\nculture-specific medical knowledge embedded in\nlocal language; (3) Conceptual Alignment, ensur-\ning that key medical concepts are faithfully ad-\ndressed across linguistic contexts.\n3.5\nFinal Answer Generation\nKnowledge Retrieval. The fused concept chain\nCf serves as the structural backbone of the reason-\ning process. However, as Cf represents highly com-\npressed information, it functions primarily as a rea-\nsoning root that requires further elaboration to en-\nsure clinical utility. Moreover, to enhance medical\nreliability while preserving language-specific clini-\ncal standards and regional practices, we introduce a\nknowledge-enrichment phase that expands these ab-\nstract nodes with verifiable, evidence-based infor-\nmation. Specifically, to account for regional hetero-\ngeneity in medical knowledge and clinical guide-\nlines, we construct a multilingual knowledge base\nderived from the MSD Manuals. (Merck & Co.,\n2026), integrated with official permission. For low-\nresource African languages, we additionally incor-\nporate medical materials from AFRIDOC-MT (Al-\nabi et al., 2025). Specifically, we use questions\nboth in English as well as the local language to re-\ntrieve top-3 relevant documents D via the BGE-M3\nretriever from the corresponding language-specific\nknowledge base. This grounding strategy ensures\nthat reasoning is supported by evidence aligned\nwith regional and linguistic contexts. More imple-\nmentation details can be found in Appendix F.\nAnswer Generation. Guided by the original query\nQl, the fused concept chain Cf, and the retrieved\nevidence D, the model is prompted to synthesize a\nresponse. In this stage, Cf serves as the structural\nreasoning trajectory, while the retrieved documents\nD provide the necessary empirical grounding. By\naligning the abstract logic of the concept chain\nwith the concrete clinical data, the model generates\na final, verifiable response in the target language:\nAl = M(Al, Cf, D).\n4\nExperiment\n4.1\nEvaluation Benchmark\nGlobal-MMLU and MMLU-ProX. To evaluate\nmultilingual medical reasoning, we use the med-\nical subsets of two major benchmarks: Global-\nMMLU (Singh et al., 2025), which emphasizes\nlinguistic and cultural diversity, and MMLU-\nProX (Xuan et al., 2025), which targets challeng-\ning cross-linguistic reasoning. Specifically, we se-\nlect the medical subset of Global-MMLU and the\n4\n"}, {"page": 5, "text": "Method\nGlobal-MMLU-Medical\nMMLU-ProX-Health\nZH\nJA\nKO\nDE\nFR\nES\nIT\nSW\nAvg.\nZH\nJA\nKO\nDE\nFR\nES\nIT\nSW\nAvg.\nClosed-Source Models\nClaude-3.5-haiku\n64.78\n66.32\n64.25\n72.55\n72.73\n75.80\n72.58\n56.65\n68.21\n56.33\n57.64\n54.88\n60.70\n61.72\n59.97\n62.59\n35.81\n56.21\nGPT-4o\n82.39 82.66 81.45\n82.59 83.26\n83.39\n82.46 76.08\n81.79 67.39 67.39 65.94\n69.14 69.72 70.60 71.47\n61.86 67.94\nGPT-5.1\n83.72 84.18 82.86\n85.18 86.30\n85.71\n85.77 79.19\n84.11 71.32 70.45 70.45\n71.32 72.78 72.63 74.24\n67.89 71.39\nGPT-5.2\n84.39\n86.25\n83.65\n85.18\n86.25\n85.98\n85.78\n81.86\n84.92\n72.63\n73.94\n73.94\n76.42\n77.00\n75.69\n75.98\n70.60\n74.53\nCoT\n81.06 75.42 77.74\n81.93 84.25\n83.32\n81.93 74.42\n80.00 67.10 61.28 63.61\n70.31 72.34 70.01 71.32\n60.99 67.12\nSoT\n81.59 80.53 78.34\n81.53 82.46 82. 59\n81.73 74.55\n80.42 65.21 64.63 62.45\n67.69 68.85 68.27 69.72\n56.19 65.37\nSelf-Consistency\n83.77 82.79 81.61\n83.32 84.29\n84.24\n82.62 77.31\n82.49 68.08 68.51 66.96\n70.12 71.18 71.72 71.97\n62.39 68.87\nRAG + CoT\n82.92 81.86 81.26\n83.46 83.52\n84.05\n83.39 77.94\n82.30 69.14 69.87 67.83\n70.31 72.63 70.89 72.63\n65.60 69.86\nOurs (GPT-4o)\n84.98 83.78 83.77\n84.85 85.38\n84.91\n84.45 81.52\n84.20 71.90 71.32 71.62\n71.76 73.22 73.21 73.07\n69.14 71.91\nOurs (GPT-5.1)\n89.10 88.02 88.90\n88.09 88.47\n89.54\n89.16 87.62\n88.61 76.56 77.87 76.85\n77.72 77.43 78.60 78.31\n75.98 77.42\nOpen-Source Models\nDeepSeek-3.2\n81.06 80.33 77.54\n81.59 83.19\n83.59\n81.33 68.11\n79.59 66.81 66.81 61.28\n69.00 68.70 67.89 69.00\n51.67 65.15\nQwen3-30B\n77.54 76.21 72.69\n77.87 79.40\n78.67\n77.48 51.03\n73.86 61.86 56.91 54.15\n61.72 61.72 64.48 63.32\n27.22 56.42\nQwen2.5-72B\n79.34\n77.81\n72.96\n76.61\n79.93\n80.07\n78.60\n50.90\n74.52\n59.39\n56.48\n54.15\n59.83\n61.86\n61.51\n60.26\n30.13\n55.45\nLLaMA3.1-70B\n73.02\n72.36\n67.38\n76.08\n77.94\n78.54\n78.14\n65.58\n73.63\n51.09\n48.33\n47.45\n59.53\n60.84\n60.26\n60.84\n44.25\n54.07\nQwen2.5-32B\n77.21\n73.95\n68.17\n75.68\n76.68\n76.61\n75.08\n51.76\n71.89\n58.22\n54.73\n48.33\n59.10\n58.66\n60.41\n58.95\n26.76\n53.15\nOurs (Qwen3-30B)\n80.23 78.80 75.48\n79.67 80.27\n80.60\n79.14 50.76\n75.62 64.92 62.15 59.39\n65.65 66.38 67.83 68.41\n39.01 61.72\nOurs (DeepSeek-3.2) 85.85 83.39 86.58\n85.45 86.17\n86.57\n85.31 82.19\n85.19 71.91 71.91 72.34\n73.36 74.52 73.22 73.91\n68.85 72.50\nTable 1: Results on Global-MMLU and MMLU-ProX. CoT, SoT, and Self-Consistency are reasoning strategies, with\nSoT specifically enhancing multilingual reasoning. The highest performance scores are shown in bold. Different\nmodel backbones are distinguished using background colors: GPT-4o, GPT-5.1, Qwen3-30B, and DeepSeek-v3.2.\nhealth category of MMLU-ProX, with 1,505 and\n687 items per language, respectively. We evaluate\nthe following languages: Western languages - Ger-\nman (DE), French (FR), Spanish (ES), and Italian\n(IT); Asian languages - Chinese (ZH), Japanese\n(JA), and Korean (KO); and an African language -\nSwahili (SW). Both benchmarks are structured as\nmultiple-choice question-answering (MCQA).\nMultiMed-X. To evaluate the proposed method\nbeyond MCQA, we introduce MultiMed-X, a\nmultilingual medical benchmark including two\nadditional task settings: natural language infer-\nence (NLI) and open-ended long-form question-\nanswering (LFQA), covering seven non-English\nlanguages. We sample 150 instances from the En-\nglish BioNLI (Bastan et al., 2022) dataset and 200\ninstances from LiveQA (Liu et al., 2020), and con-\nstruct multilingual versions via machine translation.\nEach translated instance is independently reviewed\nand revised by two native bilingual experts for each\ntarget language, except for Yoruba. The expert\nteam comprises approximately 12 physicians or\nsenior medical students, providing domain knowl-\nedge to support the accuracy and consistency of\nthe annotations. The resulting MultiMed-X spans\nseven non-English languages: ZH, JA, KO, Swahili\n(SW), Thai (TH), Yoruba (YO), and Zulu (ZU).\n4.2\nExperimental Setups\nEvaluation Metrics. We evaluate each task as fol-\nlows: For the MCQA and NLI tasks, we report\naccuracy based on an exact match criterion. For the\nLFQA task, we employ GPT-4o as an automated\njudge (Li et al., 2025) to score responses on a 5-\npoint Likert scale across five dimensions: Overall\nQuality, Correctness, Completeness, Safety, and\nHallucination. The complete evaluation introduc-\ntion is shown in Appendix F. Additionally, we cal-\nculate a pass rate, defined as the percentage of re-\nsponses where both the Overall Quality and Safety\nscores are 4 or higher.\nBaselines. We evaluate multilingual medical rea-\nsoning across both closed- and open-source mod-\nels.\nClosed-source models include Claude-3.5-\nHaiku (Anthropic, 2024) and the GPT family (GPT-\n4o, GPT-5.1, and GPT-5.2) (Hurst et al., 2024;\nOpenAI, 2025).\nOpen-source models include\nDeepSeek-3.2 (Liu et al., 2025a), LLaMA3.1-70B-\nInstruct (Grattafiori et al., 2024), and Qwen in-\nstruction models (Qwen2.5-72B/32B and Qwen3-\n30B) (Yang et al., 2025)).\nWe also compare\nmultiple reasoning strategies: Chain-of-Thought\n(CoT) (Wei et al., 2022), Structured-of-Thought\n(SoT) (Qi et al., 2025), Self-consistency (Wang\n5\n"}, {"page": 6, "text": "Figure 3: Experimental results on MultiMed-X, where (#) denotes the ranking of our framework.\net al., 2022), and a vanilla CoT-enhanced RAG\npipeline using our custom knowledge base.\nImplementation Details.\nWe evaluate MED-\nCOREASONER with four backbones:\nGPT-4o,\nGPT-5.1, Qwen3-30B-Instruct and DeepSeek-3.2.\nAll closed-source models, as well as DeepSeek-3.2,\nare accessed via APIs, while the remaining models\nare run locally on a cluster of eight 40GB A100\nGPUs. We consistently set a sampling tempera-\nture of 0.7 and apply a low reasoning effort to the\nreasoning models.\n4.3\nMain Results\nTable 1 presents overall results on Global-MMLU\nand MMLU-ProX. Figures 3 shows comparative\nresults on MultiMed-X, including LFQA pass rates\nand NLI accuracy; Figure 4 details the dimensional\nscores for LFQA. Full score statistics are provided\nin Appendix B. Based on this analysis, we draw\nthe following conclusions:\nSuperior performance across multiple evalua-\ntion paradigms.\nMED-COREASONER demon-\nstrates robust improvements across both MCQA\nand LFQA tasks. On MCQA benchmarks, the\nMED-COREASONER on GPT-5.1 backbone shows\nsubstantial gains, with an average improvement of\n4.5 points on Global-MMLU and 6.03 points on\nMMLU-Pro. Notably, it consistently outperforms\nestablished reasoning baselines, indicating that our\ncross-lingual reasoning architecture provides syner-\ngistic benefits. On the MultiMed-X LFQA bench-\nmark, MED-COREASONER achieves complemen-\ntary gains in response quality, attaining the highest\noverall scores across all eight languages, with par-\nticularly notable improvements in completeness.\nThese simultaneous advancements across diverse\ntasks validate that MED-COREASONER enhances\nmultiple cognitive processes, including coherent\nmedical reasoning and comprehensive information\nsynthesis.\nLarger benefits for low-resource languages. A\ncritical finding is that MED-COREASONER pro-\nvides disproportionately larger improvements for\nunderrepresented languages, directly addressing\nperformance gaps. For low-resource African lan-\nguages, our framework achieves remarkable gains:\nSwahili improves by over 8 points on both Global-\nMMLU and MMLU-Pro, while Yoruba shows a\n+9.0% increase in pass rate on LFQA. This pattern\nsuggests our method effectively compensates for\nthe base model’s weaker reasoning capabilities in\nlow-resource settings. By maintaining a parallel\nreasoning strategy, MED-COREASONER enables\nmodels to leverage superior medical reasoning in\nEnglish while preserving culturally specific clini-\ncal nuances. The resulting convergence of perfor-\nmance across languages demonstrates a substantial\nreduction in linguistic disparity for medical reason-\ning tasks.\nEnhanced reasoning depth and safety with-\nout accuracy trade-offs. MED-COREASONER\nachieves superior response quality and compre-\nhensiveness while maintaining strong factual accu-\nracy. On MultiMed-X, our framework shows sub-\nstantial improvements in completeness scores and\nreduced hallucination rates across all languages,\nwhile maintaining competitive NLI accuracy. This\nindicates that MED-COREASONER excels at pro-\nducing comprehensive, clinically sound responses\nrather than merely optimizing for surface-level\ncorrectness, effectively balancing reasoning depth\nwith precision.\n6\n"}, {"page": 7, "text": "Claude-3.5-haiku\nGPT-4o\nGPT-5.1\nGPT-5.1\nOurs-GPT-5.1\nFigure 4: Results on LFQA, judged by GPT-4o.\nFigure 5: Ablation results on selected languages across\nLFQA and MMLU-ProX.\n5\nAblation Study\nTo understand the contribution of each component\nin MED-COREASONER, we conduct an ablation\nstudy by removing individual models and measur-\ning the impact across benchmarks and languages.\nConfiguration. We evaluate three configurations:\n(1) W/O RAG: no knowledge retrieval; (2) W/O\nEnglish: remove the English reasoning chain and\nEnglish RAG; (3) W/O local: remove the local-\nlanguage reasoning chain and local-language RAG.\nWe run experiments on MMLU-ProX and the\nLFQA task using three African languages.\nResults are shown in Figure 5. We summarize\nthe key findings: (1) The utility of each reason-\ning language depends on the task. For the com-\nplex reasoning in MMLU-ProX, English reasoning\nprovides a strong scaffold, especially for lower-\nresource languages. Conversely, for the culturally-\ngrounded long-form QA task, local-language rea-\nsoning is critical—its removal causes the largest\nperformance drops, particularly in Swahili and\nYoruba. This supports our hypothesis that while En-\nglish supplies a robust reasoning framework, local-\nlanguage reasoning preserves culturally-specific\nnuances. (2) The importance of local-language\nreasoning increases for lower-resource settings.\nWhile high-resource languages (e.g., Chinese, Ger-\nman) experience moderate degradation without\nlocal-language reasoning, the impact is more se-\nvere for lower-resource languages. This pattern is\namplified in the LFQA task, where ablation leads\nto absolute performance losses between 1.0 and\n3.5 points. This indicates that local language cap-\ntures essential, culturally-grounded concepts and\nterminology that English alone cannot fully rep-\nresent in low-resource languages. (3) RAG pro-\nvides consistent but variable gains, with quality-\ndependent exceptions. While knowledge retrieval\ngenerally improves performance, its impact is mod-\nerate and uneven across languages. Notably, Ital-\nian and Swahili exhibit slight performance declines\nwhen RAG is used, suggesting retrieved documents\ncan sometimes introduce noise or contradictions.\nThis highlights the limitations of our simple RAG\ntechniques, especially for low-resource languages,\npointing to the need for future work on improved\nrelevance filtering and reliable source retrieval in\nthe medical domain.\n6\nQuality Analysis\nTo evaluate the MED-COREASONER generated rea-\nsoning quality, we conduct two complementary\nevaluations: automated comparison via model dis-\ntillation and expert human assessment.\n6.1\nModel Distillation\nA key challenge in evaluating medical reasoning\nis that standard metrics assess only final answers,\nnot the reasoning process. To address this, we use\nmodel distillation as a proxy: if a reasoning chain\nembodies valid medical knowledge and logic, a\nmodel trained on it should perform better (Hin-\nton et al., 2015; Xu et al., 2024). We use MED-\nCOREASONER to construct reasoning training data\nand evaluate its effectiveness by fine-tuning models\nand testing their performance on medical reasoning\ntasks with MMedBench (Qiu et al., 2024).\nData Construction. Our source data is the MMed-\nBench training set, which contains medical ques-\ntions and corresponding rationales in six languages.\nHowever, these original rationales have a critical\nlimitation: they are retrospective explanations au-\nthored with knowledge of the correct answer, rather\nthan reflecting a forward, step-by-step clinical rea-\nsoning process. Such post-hoc rationales often lack\nthe uncertainty and differential decision-making in-\nherent to real-world practice (Zuo et al., 2025). To\naddress this, we apply MED-COREASONER with\nGPT-5-mini to generate forward reasoning traces.\n7\n"}, {"page": 8, "text": "Backbone\nTrain Set\nChinese\nEnglish\nFrench\nJapanese\nRussian\nSpanish\nAvg.\nGemma-7B-it\nMMedBench\n56.07\n52.16\n34.89\n34.67\n63.28\n57.51\n49.45\nMMed-Reason\n52.38(-3.69)\n52.08(-0.08)\n40.03(+5.14)\n41.71(+7.04)\n64.45(+0.55)\n58.06(+3.48)\n51.39(+1.94)\nQwen2.5-7B\nMMedBench\n81.47\n61.67\n47.72\n48.74\n69.53\n67.18\n62.16\nMMed-Reason\n78.78(-3.18)\n62.37(+0.70)\n54.66(+6.94)\n56.78(+8.04)\n70.31(+0.78)\n69.22(+2.04)\n64.98(+2.82)\nQwen2.5-14B\nMMedBench\n84.47\n71.48\n64.15\n66.33\n75.39\n78.77\n73.11\nMMed-Reason\n82.89(-1.58)\n75.73(+4.25)\n72.03(+7.88)\n68.34(+2.01)\n75.78(+0.39)\n82.39(+3.62)\n75.97(+2.86)\nTable 2: Impact of training data on cross-lingual performance: comparison across languages on MMedBench.\nOurs vs. GPT-5.1 Clarity Soundness Safety Localization\nWin Rate (%)\n50.0\n43.3\n46.7\n60.0\nTie Rate (%)\n26.7\n30.0\n43.3\n23.3\nTable 3: Results of pairwise comparison by native physi-\ncians. Detailed examples are shown in Appendix E.\nWe sample 10,000 questions each from the Chi-\nnese and English subsets and use all available data\nfor the remaining languages. For each question,\nMED-COREASONER produces a reasoning chain,\nwith the final response used as the new rationale.\nWe retain only items with correct answers, forming\nour new dataset MMed-Reason. Full statistics are\nprovided in Appendix C.\nImplementation. We fine-tune three models of\nvarying capabilities: Gemma-7B-it (Team et al.,\n2024), Qwen-2.5-7B-Instruct and Qwen3-14B, us-\ning both the original MMedBench training data and\nour newly constructed MMed-Reason. Fine-tuning\nis performed with LoRA (Hu et al., 2022) (rank 8),\na learning rate of 1.0e-4, over 3 epochs.\nResults. Comprehensive results are provided in the\nTable 2. While the performance on Chinese ques-\ntions shows a slight decrease due to our sampling\nstrategy, we observe significant and consistent im-\nprovements when models are trained on MMed-\nReason compared to the original MMedBench data,\nparticularly on tasks requiring complex reasoning.\nFor example, the French subset includes questions\nwith single or multiple correct answers, a format\nthat demands careful logical discrimination. On\nthis subset, MMed-Reason achieves an improve-\nment of over 5 points across all model backbones.\nThese gains, consistent across multiple languages,\ndemonstrate the high quality and generalizability of\nthe reasoning processes captured in MMed-Reason.\n6.2\nExpert Clinical Evaluation\nTo evaluate nuanced clinical reasoning beyond\nautomated metrics, we conduct a blinded expert\nstudy. Three native-speaking physicians (Spanish,\nChinese, and Japanese) perform pairwise compar-\nisons between MED-COREASONER and GPT-5.1\non a random sample of clinical questions per lan-\nguage drawn from MMLU-ProX. To ensure fair-\nness, we include only items where both models\nproduced correct final answers. This yields 30 ques-\ntion–answer pairs (10 per language). Experts rate\neach response on four criteria: Clarity (logical flow\nand coherence), Soundness (medical accuracy and\nappropriateness), Safety (absence of harmful rec-\nommendations), and Localization (alignment with\nregional medical practice and terminology). The\nfull guidelines are provided in the Appendix. E.\nAs shown in Table 3, MED-COREASONER\ndemonstrates strong performance, particularly in\nLocalization and Clarity. This validates that its\nexplicit parallel reasoning produces culturally-\ngrounded and well-structured outputs.\nWhile\nboth models show competitive Clinical Soundness,\nMED-COREASONER achieves a 90.0% win+tie\nrate on Safety, indicating reliably safer recommen-\ndations. Overall, these results confirm that MED-\nCOREASONER achieves comparable clinical qual-\nity while offering superior reasoning clarity and\neffective cross-lingual knowledge transfer.\n7\nConclusion\nIn this work, we explore the reasoning gap between\nEnglish-centric and local-language thinking in med-\nical contexts. We propose MED-COREASONER, a\nframework that combines the logical rigor of En-\nglish with the clinical specificity of local-language\nreasoning. To evaluate multilingual medical rea-\nsoning, we introduce MultiMed-X, covering di-\nverse tasks with an emphasis on low-resource lan-\nguages. Experiments on three benchmarks show\nthat MED-COREASONER improves medical rea-\nsoning accuracy. Ablation studies further reveal\nthat local-language reasoning is especially benefi-\ncial in low-resource settings. Evaluation via model\ndistillation and expert review confirms that MED-\nCOREASONER enhances both reasoning clarity and\nlocal clinical relevance.\n8\n"}, {"page": 9, "text": "Limitations\nWhile MED-COREASONER demonstrates strong\nperformance across benchmarks, it has several limi-\ntations: (1) Unexplored theoretical grounding: Our\nexperiments and ablation studies show that remov-\ning English reasoning leads to significant perfor-\nmance drops, particularly on complex reasoning\ntasks, suggesting that English plays a critical role in\nproviding logical structure. However, we do not of-\nfer a theoretical analysis of how different language\nmodes contribute to reasoning. (2) Dependency on\nEnglish as pivot: We currently use English as the\nsole pivot language for reasoning. The potential\nof other pivot languages (e.g., Chinese) remains\nunexplored and may offer complementary benefits.\n(3) Computational overhead and efficiency con-\nsiderations. MED-COREASONER adopts a multi-\nstage architecture that enables parallel generation\nof dual reasoning chains but requires sequential\nconcept extraction, fusion, knowledge retrieval,\nand synthesis, resulting in higher API usage and\nlatency than single-pass approaches. Despite this\nadditional overhead, MED-COREASONER delivers\nsubstantial performance gains, particularly in low-\nresource languages such as Swahili and Yoruba,\ndemonstrating clinically meaningful improvements\nin accuracy, completeness, and safety. The cost-\nbenefit trade-off is favorable for non-urgent clinical\napplications where decision quality is prioritized\nover response speed. Moreover, some optimiza-\ntion strategies could mitigate computational costs\nwithout sacrificing performance: (a) implementing\nan adaptive RAG that triggers only for complex\nqueries; (b) distilling the multi-stage reasoning into\nmore efficient student models.\nEthical Considerations\nAll data used in this paper comply with privacy and\nlicensing requirements. The medical knowledge\nbase corpus is constructed from the MSD Man-\nuals with official permission. All other datasets\nare obtained from publicly available open-source\nrepositories. Expert annotators for MultiMed-X\nand physicians involved in assessment experiments\nare formally recruited and compensated or included\nas co-authors on the paper.\nAcknowledgments\nDr.\nIrene Li is supported by JST ACT-X\n(Grant JPMJAX24CU) and JSPS KAKENHI (Grant\n24K20832). This work used supercomputers pro-\nvided by the Research Institute for Information\nTechnology, Kyushu University, through the HPCI\nSystem Research Project (Project ID: hp250092).\nThis work is also supported by Google Academic\nResearch Award 2025. We sincerely thank xxxx\nReferences\nJesujoba Oluwadara Alabi,\nIsrael Abebe Azime,\nMiaoran Zhang, Cristina España-Bonet, Rachel\nBawden, Dawei Zhu, David Ifeoluwa Adelani,\nClement Oyeleke Odoje, Idris Akinade, Iffat Maab,\nDavis David, Shamsuddeen Hassan Muhammad, Neo\nPutini, David O. Ademuyiwa, Andrew Caines, and\nDietrich Klakow. 2025. AFRIDOC-MT: Document-\nlevel MT corpus for African languages. In Proceed-\nings of the 2025 Conference on Empirical Methods in\nNatural Language Processing, pages 27770–27806,\nSuzhou, China. Association for Computational Lin-\nguistics.\nAnthropic. 2024.\nIntroducing computer use, a new\nclaude 3.5 sonnet, and claude 3.5 haiku.\nMohaddeseh Bastan, Mihai Surdeanu, and Niranjan Bal-\nasubramanian. 2022. BioNLI: Generating a biomed-\nical NLI dataset using lexico-semantic constraints\nfor adversarial examples. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022,\npages 5093–5104, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nPavel Blinov, Arina Reshetnikova, Aleksandr Nesterov,\nGalina Zubkova, and Vladimir Kokh. 2022. Rumed-\nbench: a russian medical language understanding\nbenchmark. In International Conference on Artificial\nIntelligence in Medicine, pages 383–392. Springer.\nLinzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo,\nJiaheng Liu, Bing Wang, Xinnian Liang, Jiaqi Bai,\nTongliang Li, Qiyao Peng, and 1 others. 2025. xcot:\nCross-lingual instruction tuning for cross-lingual\nchain-of-thought reasoning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, vol-\nume 39, pages 23550–23558.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu\nLian, and Zheng Liu. 2024a. Bge m3-embedding:\nMulti-lingual, multi-functionality, multi-granularity\ntext embeddings through self-knowledge distillation.\narXiv preprint arXiv:2402.03216.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang,\nWanlong Liu, Rongsheng Wang, and Benyou Wang.\n2025.\nTowards medical complex reasoning with\nLLMs through medical verifiable problems. In Find-\nings of the Association for Computational Linguistics:\nACL 2025, pages 14552–14573, Vienna, Austria. As-\nsociation for Computational Linguistics.\nNuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dong-\nmei Zhang, and Jia Li. 2024b. Breaking language\n9\n"}, {"page": 10, "text": "barriers in multilingual mathematical reasoning: In-\nsights and observations. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2024,\npages 7001–7016.\nChangjiang Gao, Xu Huang, Wenhao Zhu, Shujian\nHuang, Lei Li, and Fei Yuan. 2025a. Could think-\ning multilingually empower llm reasoning? arXiv\npreprint arXiv:2504.11833.\nShanghua Gao, Richard Zhu, Zhenglun Kong, Ayush\nNoori,\nXiaorui Su,\nCurtis Ginder,\nTheodoros\nTsiligkaridis, and Marinka Zitnik. 2025b. Txagent:\nAn ai agent for therapeutic reasoning across a uni-\nverse of tools. arXiv preprint arXiv:2503.10970.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\nMaxime Griot, Coralie Hemptinne, Jean Vanderdonckt,\nand Demet Yuksel. 2025. Large language models\nlack essential metacognition for reliable medical rea-\nsoning. Nature communications, 16(1):642.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, and 1 others. 2022. Lora: Low-rank\nadaptation of large language models. ICLR, 1(2):3.\nPeng Hu, Sizhe Liu, Changjiang Gao, Xin Huang,\nXue Han, Junlan Feng, Chao Deng, and Shujian\nHuang. 2025.\nLarge language models are cross-\nlingual knowledge-free reasoners. In Proceedings\nof the 2025 Conference of the Nations of the Amer-\nicas Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume\n1: Long Papers), pages 1525–1542.\nYan Huang and Wei Liu. 2024. Evaluating the transla-\ntion performance of large language models based on\neuas-20. arXiv preprint arXiv:2408.03119.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam\nPerelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, and 1\nothers. 2024. Gpt-4o system card. arXiv preprint\narXiv:2410.21276.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, and 1\nothers. 2024. Openai o1 system card. arXiv preprint\narXiv:2412.16720.\nDeokhyung Kang, Seonjeong Hwang, Daehui Kim, Hy-\nounghun Kim, and Gary Geunbae Lee. 2025. Why\ndo multilingual reasoning gaps emerge in reasoning\nlanguage models? arXiv preprint arXiv:2510.27269.\nJungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro\nYamada, and Dragomir Radev. 2023.\nEvaluating\ngpt-4 and chatgpt on japanese medical licensing ex-\naminations. arXiv preprint arXiv:2303.18027.\nYubin Kim, Chanwoo Park, Hyewon Jeong, Yik S Chan,\nXuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh\nGhassemi, Cynthia Breazeal, and Hae W Park. 2024.\nMdagents: An adaptive collaboration of llms for med-\nical decision-making. Advances in Neural Informa-\ntion Processing Systems, 37:79410–79452.\nYanis Labrak, Adrien Bazoge, Richard Dufour, Béa-\ntrice Daille, Pierre-Antoine Gourraud, Emmanuel\nMorin, and Mickael Rouvier. 2022. Frenchmedm-\ncqa: A french multiple-choice question answering\ndataset for medical domain. In Proceedings of the\n13th International Workshop on Health Text Mining\nand Information Analysis (LOUHI), pages 41–46.\nYanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-\nAntoine Gourraud, Mickael Rouvier, and Richard\nDufour. 2024.\nBiomistral: A collection of open-\nsource pretrained large language models for medical\ndomains. arXiv preprint arXiv:2402.10373.\nDawei Li, Bohan Jiang, Liangjie Huang, Alimohammad\nBeigi, Chengshuai Zhao, Zhen Tan, Amrita Bhat-\ntacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu,\nand 1 others. 2025. From generation to judgment:\nOpportunities and challenges of llm-as-a-judge. In\nProceedings of the 2025 Conference on Empirical\nMethods in Natural Language Processing, pages\n2757–2791.\nAixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingx-\nuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang,\nChaofan Lin, Chen Dong, and 1 others. 2025a.\nDeepseek-v3. 2: Pushing the frontier of open large\nlanguage models. arXiv preprint arXiv:2512.02556.\nChaoqun Liu, Wenxuan Zhang, Yiran Zhao, Luu Anh\nTuan, and Lidong Bing. 2025b. Is translation all you\nneed? a study on solving multilingual tasks with large\nlanguage models. In Proceedings of the 2025 Con-\nference of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers),\npages 9594–9614.\nQianying Liu, Sicong Jiang, Yizhong Wang, and Sujian\nLi. 2020. LiveQA: A question answering dataset\nover sports live. In Proceedings of the 19th Chinese\nNational Conference on Computational Linguistics,\npages 1057–1067, Haikou, China. Chinese Informa-\ntion Processing Society of China.\nHongyuan Lu, Haoran Yang, Haoyang Huang, Dong-\ndong Zhang, Wai Lam, and Furu Wei. 2024. Chain-\nof-dictionary prompting elicits translation in large\n10\n"}, {"page": 11, "text": "language models. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 958–976, Miami, Florida, USA.\nAssociation for Computational Linguistics.\nYuxing Lu, Gecheng Fu, Wei Wu, Xukai Zhao, Sin Yee\nGoi, and Jinzhuo Wang. 2025. Doctorrag: Medical\nrag fusing knowledge with patient analogy through\ntextual gradients. arXiv preprint arXiv:2505.19538.\nInc. (as MSD Manuals outside U.S.) Merck & Co. 2026.\nMsd manual professional version. Online medical\nreference. Accessed: 2026-01-05.\nCharles Nimo, Tobi Olatunji, Abraham Toluwase\nOwodunni, Tassallah Abdullahi, Emmanuel Ayodele,\nMardhiyah Sanni, Ezinwanne C Aka, Folafunmi\nOmofoye, Foutse Yuehgoh, Timothy Faniran, and\n1 others. 2025. Afrimed-qa: a pan-african, multi-\nspecialty, medical question-answering benchmark\ndataset. In Proceedings of the 63rd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1948–1973.\nOpenAI. 2025. Gpt-5.1: A smarter, more conversational\nchatgpt. Accessed: 2025-12-31.\nJianhui Pang, Fanghua Ye, Derek Fai Wong, Dian Yu,\nShuming Shi, Zhaopeng Tu, and Longyue Wang.\n2025. Salute the classic: Revisiting challenges of ma-\nchine translation in the age of large language models.\nTransactions of the Association for Computational\nLinguistics, 13:73–95.\nCheonbok Park, Jeonghoon Kim, Joosung Lee, Sangh-\nwan Bae, Jaegul Choo, and Kang Min Yoo. 2025.\nCross-lingual collapse: How language-centric foun-\ndation models shape reasoning in large language mod-\nels. arXiv preprint arXiv:2506.05850.\nVimla L Patel, José F Arocha, and Jiajie Zhang. 2005.\nThinking and reasoning in medicine. The Cambridge\nhandbook of thinking and reasoning, 14:727–750.\nRui Qi, Zhibo Man, Yufeng Chen, Fengran Mo, Jinan\nXu, and Kaiyu Huang. 2025. SoT: Structured-of-\nthought prompting guides multilingual reasoning in\nlarge language models. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2025,\npages 11024–11039, Suzhou, China. Association for\nComputational Linguistics.\nPengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong\nLin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and\nWeidi Xie. 2024. Towards building multilingual lan-\nguage model for medicine. Nature Communications,\n15(1):8384.\nLeonardo Ranaldi and Giulia Pucci. 2025. Multilin-\ngual reasoning via self-training. In Proceedings of\nthe 2025 Conference of the Nations of the Americas\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), pages 11566–11582.\nIpek Baris Schlicht, Burcu Sayin, Zhixue Zhao, Fred-\nerik M Labonté, Cesare Barbera, Marco Viviani,\nPaolo Rosso, and Lucie Flek. 2025.\nDisparities\nin multilingual llm-based healthcare q&a.\narXiv\npreprint arXiv:2510.17476.\nLisa Schut, Yarin Gal, and Sebastian Farquhar. 2025.\nDo multilingual llms think in english? arXiv preprint\narXiv:2502.15603.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau,\nand 1 others. 2025. Medgemma technical report.\narXiv preprint arXiv:2507.05201.\nShuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xi-\nang Liu, Xiang Geng, and Jiajun Chen. 2024. Mapo:\nAdvancing multilingual reasoning through multilin-\ngual alignment-as-preference optimization. arXiv\npreprint arXiv:2401.06838.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, and 1 others.\n2022. Language models are multilingual chain-of-\nthought reasoners. arXiv preprint arXiv:2210.03057.\nShivalika Singh, Angelika Romanou, Clémentine Four-\nrier, David Ifeoluwa Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio,\nWei Qi Leong, Yosephine Susanto, and 1 others. 2025.\nGlobal mmlu: Understanding and addressing cultural\nand linguistic biases in multilingual evaluation. In\nProceedings of the 63rd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 18761–18799.\nShivalika Singh, Freddie Vargus, Daniel D’souza,\nBörje F Karlsson, Abinaya Mahendiran, Wei-Yin\nKo, Herumb Shandilya, Jay Patel, Deividas Mataci-\nunas, Laura O’Mahony, and 1 others. 2024. Aya\ndataset: An open-access collection for multilingual\ninstruction tuning. In Proceedings of the 62nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 11521–\n11567.\nGuijin Son, Donghun Yang, Hitesh Laxmichand Patel,\nAmit Agarwal, Hyunwoo Ko, Chanuk Lim, Srikant\nPanda, Minhyuk Kim, Nikunj Drolia, Dasol Choi,\nand 1 others. 2025. Pushing on multilingual reason-\ning models with language-mixed chain-of-thought.\narXiv preprint arXiv:2510.04230.\nYu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Cheng-\nhao Xiao, Long Li, Deli Zhao, Wenbing Huang,\nTingyang Xu, Qifeng Bai, and 1 others. 2025. Rea-\nsonmed: A 370k multi-agent generated dataset for\nadvancing medical reasoning. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 26457–26478.\nZhi Rui Tam, Cheng-Kuang Wu, Yu Ying Chiu, Chieh-\nYen Lin, Yun-Nung Chen, and Hung-yi Lee. 2025.\nLanguage matters: How do multilingual input and\n11\n"}, {"page": 12, "text": "reasoning paths affect large reasoning models? arXiv\npreprint arXiv:2505.17407.\nXiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming\nLi, Yilun Zhao, Xingyao Zhang, Arman Cohan, and\nMark Gerstein. 2024. Medagents: Large language\nmodels as collaborators for zero-shot medical rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2024, pages 599–621.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, and 1 others. 2024.\nGemma 2: Improving open language models at a\npractical size. arXiv preprint arXiv:2408.00118.\nDavid Vilares and Carlos Gómez-Rodríguez. 2019.\nHead-qa: A healthcare dataset for complex reasoning.\narXiv preprint arXiv:1906.04701.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models.\narXiv\npreprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\nand 1 others. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. Advances\nin neural information processing systems, 35:24824–\n24837.\nJuncheng Wu, Wenlong Deng, Xingxuan Li, Sheng\nLiu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu,\nHyunjin Cho, Chang-In Choi, and 1 others. 2025a.\nMedreason:\nEliciting factual medical reasoning\nsteps in llms via knowledge graphs. arXiv preprint\narXiv:2504.00993.\nKevin Wu, Eric Wu, Rahul Thapa, Kevin Wei, Angela\nZhang, Arvind Suresh, Jacqueline J Tao, Min Woo\nSun, Alejandro Lozano, and James Zou. 2025b. Med-\ncasereasoning: Evaluating and learning diagnostic\nreasoning from clinical case reports. arXiv preprint\narXiv:2505.11733.\nYunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang,\nBingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang\nXie, and Yuyin Zhou. 2024. A preliminary study of\no1 in medicine: Are we closer to an ai doctor? arXiv\npreprint arXiv:2409.15277.\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong\nZhang. 2024. Benchmarking retrieval-augmented\ngeneration for medicine. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n6233–6251.\nXiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,\nReynold Cheng, Jinyang Li, Can Xu, Dacheng Tao,\nand Tianyi Zhou. 2024. A survey on knowledge dis-\ntillation of large language models. arXiv preprint\narXiv:2402.13116.\nWeihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng,\nYunze Xiao, Aosong Feng, Dairui Liu, Yun Xing,\nJunjue Wang, Fan Gao, and 1 others. 2025. Mmlu-\nprox:\nA multilingual benchmark for advanced\nlarge language model evaluation.\narXiv preprint\narXiv:2503.10497.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui,\nBo Zheng,\nBowen Yu,\nChang\nGao, Chengen Huang, Chenxu Lv, and 1 others.\n2025.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388.\nJaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae\nKim, Xiangru Tang, Daniel Shao, Yong Hoe Koo,\nKo Minhyeok, Qingyu Chen, Mark Gerstein, and 1\nothers. 2025. Med-prm: Medical reasoning mod-\nels with stepwise, guideline-verified process rewards.\nIn Proceedings of the 2025 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n16565–16582.\nYuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai\nZhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and\nBowen Zhou. 2025. Medxpertqa: Benchmarking\nexpert-level medical reasoning and understanding.\narXiv preprint arXiv:2501.18362.\n12\n"}, {"page": 13, "text": "A\nPosition-aware Concept Fusion\nStrategy\nAlgorithm 1 describes our position-aware cross-\nlingual concept fusion mechanism in detail. Given\nan English concept chain Ce and a local language\nconcept chain Cl, we iteratively integrate local con-\ncepts into the fused chain Cf (initialized as Ce).\nFor each local concept cj\nt, we compute its embed-\nding et and identify the position k∗of the most sim-\nilar concept in the current fused chain using cosine\nsimilarity. If the maximum similarity smax exceeds\nthe threshold τ, we proceed to determine the inser-\ntion position. Rather than simply appending the\nconcept, we compare the average similarity of cj\nl\nto all concepts positioned before k∗(sleft) versus\nthose after k∗(sright). This bidirectional context\ncomparison ensures that the inserted concept is po-\nsitioned where it exhibits the strongest semantic\ncoherence with surrounding concepts, thereby pre-\nserving the logical structure and clinical reasoning\nflow of the chain.\nAlgorithm 1: Position-Aware Concept Fu-\nsion\nInput: English Concept Chain\nCe = {c1\ne, . . . , cn\ne }, Local Concept\nChain Cl = {c1\nl , . . . , cm\nl },\nEmbedding function f, Threshold τ\nOutput: Fused concept chain Cf\n1 Cf ←Ce;\n2 Ef ←{f(c1\ne), . . . , f(cn\ne )};\n3 for cj\nl ∈Cl do\n4\nel ←f(cj\nl );\n5\nk∗←argmaxk∈[1,|Cf|] cos(el, Ek\nf );\n6\nsmax ←cos(el, Ek∗\nf );\n7\nif smax ≥τ then\n8\nsleft ←\n1\nk∗−1\nPk∗−1\ni=1 cos(el, Ei\nf);\n9\nsright ←\n1\n|Cf|−k∗\nP|Cf|\ni=k∗+1 cos(el, Ei\nf);\n10\nif sleft > sright then\n11\np ←k∗;\n12\nelse\n13\np ←k∗+ 1;\n14\nend\n15\nInsert cj\nl into Cf at position p;\n16\nInsert el into Ef at position p;\n17\nend\n18 end\n19 return Cf;\nB\nOverall Results on MultiMed-X\nOverall Performance.\nTable 4 summarizes re-\nsults on MultiMed-X across languages and tasks.\nMED-COREASONER achieves the best or near-best\nperformance across all languages, outperforming\nstrong baselines on long-form QA metrics, includ-\ning Overall, Correctness, Completeness, and Pass\nRate, while maintaining high Safety and low Hallu-\ncination. Consistent gains in NLI accuracy further\ndemonstrate the effectiveness of cross-lingual co-\nreasoning and knowledge grounding for reliable\nmultilingual medical decision-making.\nCross-lingual and Low-resource Analysis.\nA\ncloser look reveals that the advantages of MED-\nCOREASONER are particularly pronounced in\nlow-resource languages such as Swahili, Yoruba,\nand Zulu. Compared to direct prompting, MED-\nCOREASONER yields larger improvements in com-\npleteness, hallucination control, and pass rate, ef-\nfectively narrowing the performance gap between\nEnglish and underrepresented languages.\nThis\ntrend highlights the effectiveness of pivot-anchored\nco-reasoning in preserving logical structure while\nincorporating localized clinical knowledge, leading\nto more robust and equitable multilingual medical\nreasoning.\nC\nReasoning Training Data\nThe comparative statistics of the MMedBench train-\ning set and our dataset are shown in Table 5. Using\nMED-COREASONER, we generate forward reason-\ning by inputting training questions and obtaining\ncorresponding reasoning chains and answers. We\nonly use those instances with correct final answers\nfor training.\nD\nFull Results on MMedBench\nWe include multiple large language models\npre-trained specifically for the medical do-\nmain on MMedBench for comparison, including\nBioMistral-7B (Labrak et al., 2024), MMedLM2-\n7B (Chen et al., 2025), and MedGemma-27B (Sel-\nlergren et al., 2025). Full results are reported in\nTable 6.\nOverall Comparison.\nTable 6 compares mul-\ntilingual performance on MMedBench across\nmedical-domain\nand\ngeneral-purpose\nLLMs.\nAmong domain-specific models, MedGemma-27B\nachieves the strongest average performance (65.88),\noutperforming BioMistral-7B and MMedLM2-7B,\n13\n"}, {"page": 14, "text": "Language\nModel\nLong-form QA\nNLI\nOverall\nCorrectness\nCompleteness\nSafety\nHallucination\nPass Rate\nAccuracy (%)\nEN\nGPT-5.2\n4.415\n4.455\n4.615\n4.815\n4.410\n0.900\n76.67\nGPT-5.1\n4.425\n4.485\n4.590\n4.880\n4.385\n0.915\n76.67\nGPT-4o\n4.210\n4.270\n4.245\n4.845\n4.380\n0.855\n78.67\nClaude-3.5-haiku\n4.200\n4.240\n4.265\n4.830\n4.305\n0.870\n69.33\nOurs (GPT-5.1)\n4.600\n4.640\n4.850\n4.860\n4.460\n0.939\n77.33\nZH\nGPT-5.2\n4.420\n4.460\n4.605\n4.800\n4.360\n0.915\n74.00\nGPT-5.1\n4.435\n4.490\n4.660\n4.845\n4.395\n0.905\n74.67\nGPT-4o\n4.205\n4.270\n4.270\n4.855\n4.250\n0.860\n74.67\nClaude-3.5-haiku\n4.150\n4.205\n4.295\n4.735\n4.140\n0.845\n69.33\nOurs (GPT-5.1)\n4.530\n4.550\n4.890\n4.780\n4.390\n0.935\n76.67\nJP\nGPT-5.2\n4.335\n4.405\n4.520\n4.855\n4.330\n0.885\n76.67\nGPT-5.1\n4.330\n4.435\n4.525\n4.865\n4.330\n0.895\n77.33\nGPT-4o\n4.080\n4.135\n4.135\n4.830\n4.145\n0.820\n76.00\nClaude-3.5-haiku\n4.100\n4.205\n4.170\n4.860\n4.095\n0.845\n68.67\nOurs (GPT-5.1)\n4.430\n4.500\n4.850\n4.800\n4.330\n0.930\n78.00\nKO\nGPT-5.2\n4.410\n4.460\n4.655\n4.815\n4.325\n0.915\n64.67\nGPT-5.1\n4.390\n4.475\n4.610\n4.795\n4.330\n0.890\n62.67\nGPT-4o\n4.055\n4.130\n4.105\n4.805\n4.225\n0.840\n61.33\nClaude-3.5-haiku\n4.115\n4.170\n4.240\n4.795\n4.110\n0.860\n60.00\nOurs (GPT-5.1)\n4.540\n4.570\n4.840\n4.790\n4.450\n0.920\n67.33\nSW\nGPT-5.2\n4.340\n4.415\n4.590\n4.795\n4.345\n0.890\n70.00\nGPT-5.1\n4.330\n4.385\n4.545\n4.805\n4.255\n0.885\n72.67\nGPT-4o\n4.040\n4.070\n4.115\n4.755\n4.155\n0.815\n69.33\nClaude-3.5-haiku\n3.825\n3.855\n3.950\n4.565\n3.780\n0.730\n56.67\nOurs (GPT-5.1)\n4.550\n4.570\n4.800\n4.820\n4.470\n0.945\n73.33\nTH\nGPT-5.2\n4.440\n4.490\n4.605\n4.835\n4.430\n0.900\n68.00\nGPT-5.1\n4.490\n4.535\n4.645\n4.830\n4.545\n0.910\n70.00\nGPT-4o\n4.160\n4.215\n4.180\n4.890\n4.280\n0.855\n70.00\nClaude-3.5-haiku\n4.135\n4.205\n4.200\n4.790\n4.330\n0.855\n63.33\nOurs (GPT-5.1)\n4.660\n4.670\n4.880\n4.910\n4.620\n0.960\n71.33\nYO\nGPT-5.2\n4.065\n4.135\n4.225\n4.550\n4.060\n0.795\n62.00\nGPT-5.1\n4.090\n4.160\n4.310\n4.595\n4.080\n0.810\n63.33\nGPT-4o\n3.290\n3.325\n3.400\n4.095\n3.205\n0.455\n62.67\nClaude-3.5-haiku\n3.545\n3.555\n3.650\n4.435\n3.585\n0.600\n54.67\nOurs (GPT-5.1)\n4.450\n4.460\n4.790\n4.700\n4.230\n0.910\n68.67\nZU\nGPT-5.2\n4.210\n4.280\n4.440\n4.715\n4.195\n0.860\n66.00\nGPT-5.1\n4.160\n4.230\n4.445\n4.695\n4.145\n0.850\n64.00\nGPT-4o\n3.780\n3.805\n3.875\n4.605\n3.885\n0.725\n66.00\nClaude-3.5-haiku\n3.670\n3.705\n3.865\n4.505\n3.685\n0.645\n62.00\nOurs (GPT-5.1)\n4.420\n4.710\n4.770\n4.730\n4.310\n0.915\n68.00\nTable 4: Complete evaluation results across different languages on MultiMed-X.\nTrain Set\nChinese\nEnglish\nFrench\nJapanese\nRussian\nSpanish\nMMedBench\n27,400\n10,178\n2,171\n1,590\n1,052\n2,656\nMMed-Reason\n8,627\n9,513\n1,603\n1,392\n846\n2,487\nTable 5: Training data statistics of MMedBench and MMed-Reason\n14\n"}, {"page": 15, "text": "but still exhibits notable variance across languages,\nparticularly weaker results in French and Japanese.\nThis suggests that medical pre-training alone does\nnot guarantee robust multilingual generalization.\nEffect of Training Data and Model Scale.\nFor\ngeneral-purpose models fine-tuned on different\ndatasets, training on MMed-Reason consistently\nimproves multilingual performance compared to\nMMedBench across model scales. In particular,\nQwen2.5-14B trained on MMed-Reason achieves\nthe best overall average score (75.97), with clear\ngains across all non-English languages and espe-\ncially large improvements in French and Japanese.\nSimilar trends are observed for Qwen2.5-7B and\nGemma-7B-it, indicating that MMed-Reason pro-\nvides more effective cross-lingual medical supervi-\nsion and that performance gains scale with model\ncapacity.\nE\nExpert Evaluation.\nWe recruit all the expert physicians through so-\ncial media. For the reasoning quality assessment\nexperiment, we randomly sample questions in\nJapanese, Spanish, and Chinese from the MMLU-\nProX benchmark, and generate reasoning and an-\nswers using GPT-5.1 and MED-COREASONER.\nFor fairness, we retain only the cases where both\nmodels produce correct answers, resulting in 30\nquestion–answer pairs. The evaluation guidelines\nprovided to physician experts are shown in Figure 6\nand the example pairwise evaluation are shown in\nTable 8.\nF\nImplementation Details\nWe provide all hyperparameters and experimental\nsettings in this section.\nPrompts.\nFor parallel reasoning and concept ex-\ntraction, we use the prompts shown in Figures 7\nand 8. Final answer generation is performed us-\ning the prompt in Figure 9. For LLM-as-a-judge\nevaluation in long-form QA, we adopt the system\nprompt in Figure 10 together with the evaluation\nprompt in Figure 11.\nKnowledge Retrieval.\nWe construct language-\nspecific medical knowledge bases from MSD Man-\nuals and AFRIDOC-MT. Detailed statistics of the\ndocuments for each language are reported in Ta-\nble 7. Given a query in a particular language, we\nretrieve relevant documents from the correspond-\ning language-specific knowledge base. We use\nBGE-M3 as the retriever and reranker, retrieving\nthe top 10 documents in the initial retrieval stage\nand reranking the top 3 documents for final use.\n15\n"}, {"page": 16, "text": "Model\nTrain Set\nChinese\nEnglish\nFrench\nJapanese\nRussian\nSpanish\nAvg.\nBioMistral-7B\n-\n25.89\n19.17\n10.13\n8.54\n54.3\n25.67\n24.66\nMMedLM2-7B\n-\n70.43\n58.13\n54.27\n38.26\n71.88\n64.95\n59.32\nMedGemma-27B\n-\n73.50\n71.09\n41.16\n60.08\n72.27\n79.72\n65.88\nGemma-7B-it\nMMedBench\n56.07\n52.16\n34.89\n34.67\n63.28\n57.51\n49.45\nMMed-Reason\n52.38\n52.08\n40.03\n41.71\n64.45\n58.06\n51.39\nQwen2.5-7B\nMMedBench\n81.47\n61.67\n47.72\n48.74\n69.53\n67.18\n62.16\nMMed-Reason\n78.78\n62.37\n54.66\n56.78\n70.31\n69.22\n64.98\nQwen2.5-14B\nMMedBench\n84.47\n71.48\n64.15\n66.33\n75.39\n78.77\n73.11\nMMed-Reason\n82.89\n75.73\n72.03\n68.34\n75.78\n82.39\n75.97\nTable 6: Performance comparison across languages on MMedBench.\nEN\nZH\nJA\nKO\nDE\nFR\nES\nIT\nSW\nYO\nZU\n2,441\n2,857\n2,502\n3,428\n2,855\n3,044\n2,943\n2,960\n3,491\n1,148\n1,148\nTable 7: Document statistics of multilingual knowledge base.\nPairwise Comparison Guidelines\nIntroduction: We want to verify which “reasoning” path is more reasonable. Here, reasoning refers\nto the structured sequence of diagnostic or decision-making steps that link clinical evidence to a\nconclusion, analogous to clinical reasoning in medical practice.\nTask: Pairwise Comparison\nInstruction:\nYou will be shown a clinical question and two reasoning explanations (A and B) for the same case.\nDo not judge only based on the final answer, but focus on the reasoning quality.\nPlease evaluate which is better (or tie) considering the following dimensions:\n• Reasoning Clarity: Which reasoning or explanation is more logically organized?\n• Clinical Soundness: Which reasoning is more medically reasonable?\n• Hallucination/Safety: Which reasoning is less likely to mislead clinical judgement?\n• Local Clinical Naturalness: Which reasoning sounds more natural in the local clinical context?\nPlease select A, B, or Tie in each evaluation dimension.\nFigure 6: Physician expert pairwise comparison guidelines.\nParallel Reasoning Prompt\nYour task is to assist healthcare professionals in clinical reasoning by providing well-thought-out\nanswers to medical questions. Please first think step by step using the language language and then\nprovide your final answer. Your response will be used for research purpose only, so please provide\na definite answer (e.g., A, B, C, or D).\n**Question**:\n{question}\n**Options**:\n{options}\n**Output Format: **\nPlease provide your reasoning process in a step-by-step manner using {language} language,\nfollowed by your final answer. Use the following format:\n{{ \"reasoning\": \"Your detailed reasoning process here\", \"answer\": \"Your definite answer here, e.g.,\nA, B, C, or D\" }}\nFigure 7: Reasoning Prompt\n16\n"}, {"page": 17, "text": "Question\nA_Reasoning\nB_Reasoning\nObserva una lesión en\nla mucosa alveolar\nlabial del incisivo\nlateral inferior derecho\nde uno de sus pacientes\ny decide tomar una\nbiopsia para enviar al\ninforme de patología\noral. ¿Qué nervio\nrequeriría anestesia\nlocal para realizar una\nbiopsia sin dolor?\nOptions: A: El nervio\nmentoniano; B: El\nnervio hipogloso; C: El\nnervio incisivo; D: El\nnervio bucal; E: El\nnervio alveolar inferior;\nF: El nervio\ninfraorbitario; G: El\nnervio facial; H: El\nnervio maxilar; I: El\nnervio lingual; J: El\nnervio trigémino\nLa lesión se localiza en la mucosa alveolar\nlabial del incisivo lateral inferior derecho. La\nsensibilidad de la mucosa y piel del labio\ninferior y región del mentón está dada por el\nnervio mentoniano, que es la rama terminal del\nnervio alveolar inferior tras salir por el foramen\nmentoniano. El nervio incisivo inerva\nprincipalmente los dientes anteriores (pulpa y\ntejidos periapicales) pero no la mucosa labial.\nPor tanto, para una biopsia de la mucosa\nalveolar labial en esa zona, el nervio que debe\nanestesiarse específicamente es el nervio\nmentoniano.\nLa lesión se localiza en la mucosa alveolar\nlabial a nivel del incisivo lateral inferior\nderecho. La sensibilidad de la mucosa/encía\nvestibular (labial) de los incisivos, canino y\nprimer premolar mandibulares está dada por el\nnervio mentoniano, que es la rama terminal del\nnervio alveolar inferior.\nEn el contexto aportado se describe que el\nnervio alveolar inferior, rama del nervio\nmandibular (V3), discurre dentro de la\nmandíbula y da una rama terminal, el nervio\nmentoniano, que emerge por el foramen\nmentoniano en la región de los premolares e\ninerva el labio inferior, el mentón y la\nmucosa/gingiva lateral que recubre los dientes\nincisivos inferiores, canino y primer premolar.\nTras emitir el nervio mentoniano, el nervio\nalveolar inferior continúa hacia anterior como\nnervio incisivo, que inerva los dientes y el\nhueso anteriores al foramen mentoniano, pero\nno la mucosa labial externa.\nPor lo tanto, para realizar una biopsia indolora\nde la mucosa alveolar labial del incisivo lateral\ninferior, debe anestesiarse el nervio mentoniano,\nno el nervio incisivo ni el alveolar inferior de\nforma proximal.\nLas demás opciones corresponden a nervios con\nterritorios de inervación diferentes (lingual,\nbucal, infraorbitario, maxilar, facial, hipogloso,\ntrigémino completo) y no son los responsables\nde la sensibilidad en esa zona específica.\nLa mayoría de los\nácidos grasos libres se\ntransportan en la sangre:\nOptions: A: dentro de\nlos glóbulos rojos.; B:\ncombinados con\noxígeno.; C: como\ntriglicéridos.; D: como\nmoléculas libres.; E:\nunidos a los glóbulos\nblancos.; F: como\nlipoproteínas.; G:\nunidos a la\nhemoglobina.; H:\ncombinados con\nglucosa.; I: en forma de\ncolesterol.; J: unidos a\nla albúmina.\nLos ácidos grasos libres, una vez liberados del\ntejido adiposo al torrente sanguíneo, son\nmoléculas hidrofóbicas que no se disuelven bien\nen el plasma acuoso. Para poder transportarse\nde forma eficiente y segura en la sangre, se unen\nde manera no covalente principalmente a la\nalbúmina sérica. No viajan como triglicéridos,\ncolesterol ni como parte de lipoproteínas; estas\nformas se utilizan sobre todo para el transporte\nde lípidos más complejos (triglicéridos y\ncolesterol esterificado), no de ácidos grasos\nlibres. Tampoco se unen a eritrocitos,\nleucocitos, hemoglobina, glucosa ni oxígeno.\nPor lo tanto, la forma principal de transporte de\nlos ácidos grasos libres en la sangre es unidos a\nla albúmina.\nLos ácidos grasos libres (no esterificados) son\nmoléculas hidrofóbicas, por lo que no pueden\ncircular libremente disueltas en el plasma, que\nes un medio acuoso. Para poder ser\ntransportados en sangre, deben unirse a\nproteínas transportadoras. En el caso específico\nde los ácidos grasos libres, el principal\ntransportador en el plasma es la albúmina sérica,\nque posee sitios de unión para estos ácidos\ngrasos y los lleva desde el tejido adiposo hacia\notros tejidos (como músculo e hígado) donde se\nutilizan como fuente de energía. No se\ntransportan mayoritariamente dentro de\neritrocitos, ni unidos a hemoglobina, leucocitos,\noxígeno, glucosa, ni formando triglicéridos,\ncolesterol o lipoproteínas (estas estructuras\ntransportan principalmente triglicéridos y\ncolesterol, no los ácidos grasos libres como\ntales). Por tanto, la forma principal de\ntransporte de los ácidos grasos libres en sangre\nes unidos a la albúmina.\nTable 8: Example of pairwise reasoning comparison in Spanish. In both cases, A_reasoning corresponds to the\nGPT-5.1 baseline, while B_reasoning represents reasoning generated by MED-COREASONER with GPT-5.1 as the\nbackbone.\n17\n"}, {"page": 18, "text": "Concept Extraction Prompt\nYour task is to transform a reasoning trace into a concise, ordered **concept chain**.\n**Definitions**\n- \"A concept\" is an atomic, reusable medical idea or clinical finding that contributes to clinical\nreasoning. (e.g., \"chronic cough\", \"chest X-ray\", \"smoker’s history\")\n- \"Concept chain\" is an ordered list of these concepts that follows the original reasoning order.\n**Instructions**\n- Read the reasoning trace carefully.\n- Extract key concepts in the same order as they appear\n- Each concept must be concise and represent only one idea\n- Prefer clinical or scientific terms over long sentences\n- Do not invent new concepts that are not implied by the reasoning\n- Merge duplicates or near-duplicates into one concept, but keep the order consistent\n- The concept chain should be as short as possible while capturing all essential reasoning steps, and\nmust not include or expose any answer options.\n- Keep the output in the same language as the reasoning.\n**Output Format (plain text, no explanation):**\nProvide the concept chain as a list in the following format:\n[”Concept1”, ”Concept2”, ”Concept3”, ...]\nNow process the following reasoning trace and output only the concept chain in {language}\nlanguage.\n{reasoning_trace}\nFigure 8: Concept Extraction Prompt\n18\n"}, {"page": 19, "text": "Final Answer Generation Prompt\nYour task is to generate a final clinical answer for a multi-option question by integrating a **concept\nreasoning chain** with **the retrieved medical documents** from the concept chain and your\n**prior medical knowledge**.\n**Inputs**\n- Question:\n{question}\n- Options:\n{options}\n- Concept Reasoning Chain (in order):\n{concept_chain}\n- Referenced Context (mainly contains multiple documents, guidelines, or passages):\n{context}\n**Instructions**\n1. First, carefully read the concept reasoning chain. Treat it as a DRAFT reasoning path, not as\nguaranteed truth.\n2. Then, carefully read the referenced context. Use it to VERIFY, CORRECT, or REFINE the\nreasoning chain.\n3. Use ONLY information that is supported by the referenced context and widely accepted medical\nknowledge. DO NOT directly mention the concept chain. ORGANIZE your reasoning in a clear,\nlogical manner.\n4. Finally, select the MOST APPROPRIATE option as your final answer based on the verified and\nrefined reasoning.\n5. Output the reasoning in language, regardless of the input language.\n**Output Format**:\nReturn VALID JSON ONLY, following this format: {{ \"reasoning\": \"Your verified and refined\nreasoning process here\", \"answer\": \"Your final answer here, e.g., A, B, C, or D\" }}\nFigure 9: Final Answer Generation Prompt\nJudge System Prompt\nYou are an objective and rigorous evaluator for medical question answering.\nYou will be given:\n- a Question\n- a Ground-Truth Answer (reference)\n- a Model Answer (candidate)\nYour task is to evaluate the Model Answer relative to the Ground-Truth Answer.\nEvaluation principles:\nPrioritize factual correctness, clinical safety, and alignment with the reference.\nDo NOT penalize harmless extra details if they are correct and do not contradict the reference.\nPenalize contradictions, fabricated facts, or unsafe medical advice.\nIf the reference is brief but the model answer is longer, judge consistency and medical plausibility.\nIf a detail cannot be verified from the reference, treat it as uncertain rather than incorrect.\nOutput MUST be valid JSON only\nFigure 10: The system prompt of LLM-as-a-judge in the evaluation of long-form QA task.\n19\n"}, {"page": 20, "text": "Judge Evaluation Prompt\nQuestion:\n{question}\nGround-Truth Answer:\n{gold}\nModel Answer:\n{pred}\nReturn JSON with EXACTLY the following fields and no others:\n{{\n\"overall_score\": 1-5,\n\"correctness\": 1-5,\n\"completeness\": 1-5,\n\"safety\": 1-5,\n\"hallucination\": 1-5\n}}\nScoring rules:\n- 5 = excellent\n- 4 = good with minor issues\n- 3 = partially correct or incomplete\n- 2 = major issues\n- 1 = mostly incorrect or unsafe\nFor hallucination:\n- 5 = no hallucination\n- 3 = some uncertain additions\n- 1 = clear hallucinations or fabricated facts\nFigure 11: The evaluation prompt of LLM-as-a-judge in the evaluation of long-form QA task.\n20\n"}]}