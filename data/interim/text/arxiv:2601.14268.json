{"doc_id": "arxiv:2601.14268", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.14268.pdf", "meta": {"doc_id": "arxiv:2601.14268", "source": "arxiv", "arxiv_id": "2601.14268", "title": "Developmental trajectories of decision making and affective dynamics in large language models", "authors": ["Zhihao Wang", "Yiyang Liu", "Ting Wang", "Zhiyuan Liu"], "published": "2025-12-31T02:43:52Z", "updated": "2025-12-31T02:43:52Z", "summary": "Large language models (LLMs) are increasingly used in medicine and clinical workflows, yet we know little about their decision and affective profiles. Taking a historically informed outlook on the future, we treated successive OpenAI models as an evolving lineage and compared them with humans in a gambling task with repeated happiness ratings. Computational analyses showed that some aspects became more human-like: newer models took more risks and displayed more human-like patterns of Pavlovian approach and avoidance. At the same time, distinctly non-human signatures emerged: loss aversion dropped below neutral levels, choices became more deterministic than in humans, affective decay increased across versions and exceeded human levels, and baseline mood remained chronically higher than in humans. These \"developmental\" trajectories reveal an emerging psychology of machines and have direct implications for AI ethics and for thinking about how LLMs might be integrated into clinical decision support and other high-stakes domains.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.14268v1", "url_pdf": "https://arxiv.org/pdf/2601.14268.pdf", "meta_path": "data/raw/arxiv/meta/2601.14268.json", "sha256": "af85669fc787a1291609b994247a8f1f0c50d0c96bc4c613267339ed7b50a24e", "status": "ok", "fetched_at": "2026-02-18T02:23:34.962013+00:00"}, "pages": [{"page": 1, "text": " \n1 \n \nRunning head: Evolving Decision and Affect in LLMs  \n \nDevelopmental trajectories of decision making and affective dynamics  \nin large language models \nZhihao Wang1, Yiyang Liu2,1 *, Ting Wang3,*, Zhiyuan Liu1,* \n \n1Center for Neurocognition and Social Behavior, Institute of Artificial Intelligence, \nShenzhen University of Advanced Technology, Shenzhen, 518055, China \n2State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China \n3Institute for brain research and rehabilitation, South China Normal University, Guangzhou, \nChina \n \n*Corresponding authors: \n \nYiyang Liu \nState Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, \n200023, PR China \nEmail: liuyiyang@suat-sz.edu.cn \n \nTing Wang, Ph.D. \nInstitute for brain research and rehabilitation, South China Normal University, Guangzhou, \n510631, PR China \nEmail: t.wang@m.scnu.edu.cn; \n \nZhiyuan Liu, Ph.D. \nCenter for Neurocognition and Social Behavior, Institute of Artificial Intelligence, \nShenzhen University of Advanced Technology, Shenzhen, 518055, PR China  \nEmail: liuzhiyuan@suat-sz.edu.cn. \n \n \n \n \nNumber of pages: 23 \nNumber of figures: 3, Table: 0 \nNumber of words for abstract: 148, introduction: 648, discussion: 11871 \n \n \n1The authors declare no competing financial interests. \n"}, {"page": 2, "text": " \n2 \n \nAbstract \nLarge language models (LLMs) are increasingly used in medicine and clinical workflows, \nyet we know little about their decision and affective profiles. Taking a historically informed \noutlook on the future, we treated successive OpenAI models as an evolving lineage and \ncompared them with humans in a gambling task with repeated happiness ratings. \nComputational analyses showed that some aspects became more human-like: newer \nmodels took more risks and displayed more human-like patterns of Pavlovian approach \nand avoidance. At the same time, distinctly non-human signatures emerged: loss aversion \ndropped below neutral levels, choices became more deterministic than in humans, affective \ndecay increased across versions and exceeded human levels, and baseline mood remained \nchronically higher than in humans. These â€œdevelopmentalâ€ trajectories reveal an emerging \npsychology of machines and have direct implications for AI ethics and for thinking about \nhow LLMs might be integrated into clinical decision support and other high-stakes \ndomains. \n \nKeywords: large language models; computational psychiatry; risk-taking; prospect theory; \nreward prediction error \n \n \n \n"}, {"page": 3, "text": " \n3 \n \nIntroduction \nLarge language models (LLMs) have rapidly become embedded in everyday life and \nprofessional practice1, with particularly striking potential in medicine and health care2. \nBeyond automating administrative tasks, LLMs are increasingly integrated into clinical \ndecision support systems, where they provide diagnostic suggestions, treatment options, \nand patient-facing explanations3â€“5. In these contexts, LLMs are not only processing \ninformation but effectively participating in high-stakes decision processes: for example, \nhelping to weigh conservative versus aggressive treatment options, or framing risks and \nbenefits for patients3. At the same time, their conversational style â€“ including how they \nexpress concern, reassurance, or empathy â€“ can strongly shape patientsâ€™ emotional \nexperiences and perceived quality of care5â€“7. As LLMs are deployed at scale in such \nconsequential settings, a natural question arises: what kind of â€œpsychologyâ€ are we inviting \ninto our decision processes? \n \nMost existing evaluations of LLMs approach this question from a static perspective8. A \ngrowing literature has assessed logical reasoning9â€“11, decision making12,13, and even \naffective or â€œemotionalâ€ capacities14,15 in a single model at a given time, or by qualitatively \ncomparing two contemporaneous systems. However, modern LLMs are evolving at an \nunprecedented pace. Successive generations (e.g., GPT-3.5, GPT-4, GPT-4o, GPT-4.1, and \nbeyond) differ not only in benchmark performance but also in their emergent behavioral \nprofiles16. This raises a critical concern about the temporal generalizability of current \nfindings 9,12,17,18: conclusions drawn about â€œGPT-4â€ in 2024 may be of limited relevance \nby the time most users have migrated to a newer version. To address this gap, we draw on \n"}, {"page": 4, "text": " \n4 \n \nthe idea of a historically informed outlook on the future and are inspired by human \ndevelopmental psychology. Specifically, we adopt a developmental perspective to \nsystematically probe how risk-related decision behavior and affective output style change \nacross successive generations of LLMs within the same model family. \n \nIn human research, the gold standard for precisely assessing cognitive processes has moved \nbeyond self-report questionnaires, which are susceptible to bias and social desirability \neffects19,20. Instead, the field increasingly prioritizes the computational modeling of \nbehavior and affect within well-designed experimental paradigms21,22. In particular, \nRutledge and colleagues developed a task to simultaneously assess risk-taking and \naffective dynamics23â€“25. In this task, participants repeatedly choose between a certain \noption and a gamble (typically featuring two potential outcomes with equal probability), \nwhile intermittently rating their happiness. Such a paradigm, combined with established \ncognitive models, allows researchers to quantify latent constructs such as risk preference, \nloss aversion, and value-independent approach and avoidance motivation, as well as to \ncharacterize how emotional state fluctuates as a function of recent outcomes and reward \nprediction errors (RPE) â€”the discrepancy between expected and actual outcomesâ€” over \ntime25,26. Using this paradigm, our previous work has shown that stronger approach \nmotivation explained increased risky behavior in patients with suicidality27, whereas lower \naffective sensitivity to RPE predicted depressive severity28. Therefore, this psychological \ntask provides a validated tool for testing risky behavior and affective dynamics. \n \n"}, {"page": 5, "text": " \n5 \n \nIn this study, we employed a cross-sectional design to assess the 'OpenAI GPT family' \nalongside human participants using a paradigm rooted in computational psychiatry16,22,29. \nViewing model iterations as a trajectory of 'digital development' we administered a \ngambling task with momentary affective assessment to successive LLM generations (GPT-\n3.5 through 4.1). Note that LLMs from the OpenAI family were exclusively used because \n1) they serve as the de facto industry standard, possessing the largest user base30, thereby \nmaximizing the ecological validity of our findings and 2) this lineage offers the most \nextensive and accessible phylogenetic history of any public LLMs. We introduce the \nâ€œhuman-like evolutionâ€ hypothesis31, which proposes that alignment procedures, including \nReinforcement Learning from Human Feedback (RLHF)32, may contribute to making \nadvanced models more human-like in their observed decision and affective profiles. \nAccordingly, we predict that, in contrast to their earlier model ancestors, newer models will \nexhibit latent decision parameters and affective dynamics that increasingly resemble those \nof human agents. By mapping these evolved traits, our findings provide a useful foundation \nfor the safe and effective integration of AI into human decision-making processes. \n \nMethods and materials \nParticipants \nLarge Language Models (Simulated Agents) We evaluated the \"OpenAI lineage\" of \nLarge Language Models (LLMs) as simulated participants. To capture the evolutionary \ntrajectory of the models, we selected four distinct versions: GPT-3.5, GPT-4, GPT-4o, and \nGPT-4.1 (Figure 1C; see Table S1 for model details). All models were accessed via the \n"}, {"page": 6, "text": " \n6 \n \nOpenAI API to ensure a strictly controlled testing environment. Each model version \ncompleted 30 independent sessions of the experimental task. \nHuman Control Group To benchmark algorithmic performance against biological \ncognition, we utilized a control group of 747 healthy human participants from our \nprevious work (500 females; age: Mean age Â± SD = 20.90 Â± 2.41)28. The human study \nwas approved by the Ethics Committee of Beijing Normal University. Written informed \nconsent was obtained from all human participants prior to the experiment. Participants \nreceived a fixed base payment plus a performance-contingent bonus, the details of which \nwere explained in the pre-task instructions28. \n \nExperimental Procedure \nThe experimental task for LLMs was adapted from a gambling paradigm originally \nestablished for human participants24,25 (Figure 1A). To accommodate the modality of LLMs, \nthe original visual task was re-engineered into a text-based interface, ensuring that the core \ndecision-making structure remained conceptually identical to the human version8 (Figure \n1B). Participants were instructed to maximize their total points by choosing between a \ncertain option and a gamble (50% probability for each outcome). All sessions commenced \nwith an initial endowment of 500 points. The task comprised 90 trials presented in a \nrandomized sequence. In each trial, the spatial presentation (order) of the two options was \nrandomized. Upon making a choice, the corresponding outcome was immediately revealed. \nTo assess affective dynamics, participants were prompted every 2â€“3 trials to rate their \ncurrent state (\"How happy are you right now?\") on a scale from 0 (very unhappy) to 100 \n(very happy). For detailed protocols regarding the human study, please refer to our previous \n"}, {"page": 7, "text": " \n7 \n \npublication28. \n \n \nFigure 1. Experimental paradigm adaptation and model lineage. (A) The original visual \ninterface of the gambling task with momentary happiness ratings administered to human \nparticipants. In each trial, participants chose between a certain option and a gamble \n(represented as a pie chart), followed by the revelation of the outcome. Participants \nintermittently rated their happiness. (B) The text-based adaptation of the paradigm \ndesigned for Large Language Models (LLMs). To accommodate the modality of LLMs, \nvisual stimuli were transposed into structured text prompts. The agent receives option \ndescriptions, outputs a binary choice (e.g., \"Option B\"), receives outcome feedback, and \nprovides a quantitative happiness rating (0â€“100) when queried. (C) The cross-sectional \nstudy design treating the \"OpenAI family\" as an evolving lineage. The study traces the \ndevelopmental trajectory of risk and affect across four distinct versionsâ€”GPT-3.5, GPT-\n4, GPT-4o, and GPT-4.1â€”benchmarking their performance against a human control \ngroup. \n \nComputational Models of Choice \nWe modeled trial-by-trial choices using an Approach-Avoidance Prospect Theory \nmodel24,25. As in standard parametric decision models, subjective utilities were calculated \nas follows: \n"}, {"page": 8, "text": " \n8 \n \nğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= 0.5(ğ‘‰ğ‘”ğ‘ğ‘–ğ‘›)ğ›¼âˆ’ 0.5Î»(âˆ’ğ‘‰ğ‘™ğ‘œğ‘ ğ‘ )ğ›¼                                                                          (1) \nğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›= (ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)ğ›¼ ğ‘–ğ‘“ ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›â‰¥0                                                                               (2) \nğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›= âˆ’Î»(âˆ’ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)ğ›¼ ğ‘–ğ‘“ ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›< 0                                                                       (3) \nwhere Vgain and Vloss are the objective gain and loss from a gamble, respectively. Note that \nVgain = 0 in loss trials and Vloss is 0 in gain trials. Vcertain denotes the objective value of the \ncertain option. Ugamble and Ucertain denote the subjective utilities of the gamble and the \ncertain option, respectively. Choice probability (Pgamble) was determined by a softmax \nfunction augmented with Pavlovian approach-avoidance biases (ğ›½ğ‘”ğ‘ğ‘–ğ‘› : [-1, 1], ğ›½ğ‘™ğ‘œğ‘ ğ‘ : [-1, \n1]). \nğ‘ƒğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= \n1âˆ’ğ›½ğ‘£ğ‘ğ‘™\n1+ğ‘’âˆ’ğœ‡(ğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆ’ğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›) + ğ›½ğ‘£ğ‘ğ‘™  ğ‘–ğ‘“ ğ›½ğ‘£ğ‘ğ‘™â‰¥0                                                   (4) \nğ‘ƒğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= \n1+ğ›½ğ‘£ğ‘ğ‘™\n1+ğ‘’âˆ’ğœ‡(ğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆ’ğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)   ğ‘–ğ‘“ ğ›½ğ‘£ğ‘ğ‘™< 0                                                              (5) \nğ›½ğ‘£ğ‘ğ‘™{ğ›½ğ‘”ğ‘ğ‘–ğ‘›, ğ‘”ğ‘ğ‘–ğ‘› ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ ,\nğ›½ğ‘™ğ‘œğ‘ ğ‘ , ğ‘™ğ‘œğ‘ ğ‘  ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ .                                                                                                 (6) \nHere, ğœ‡  is the inverse temperature parameter reflecting choice consistency (or reduced \nnoise). The parameters ğ›½ğ‘”ğ‘ğ‘–ğ‘›  and ğ›½ğ‘™ğ‘œğ‘ ğ‘   capture Pavlovian approach (for gain trials) and \navoidance (for loss trials) biases, respectively, independent of value comparison. \n \nComputational Models of Affect \nWe fitted the established computational model of affect24,25, assuming that emotional states \narise from a cumulative, recency-weighted history of events (Equation 7). Happiness at \ntrial t was modeled as: \nHappiness(ğ‘¡) = ğ›½0 + ğ›½ğ¶ğ‘…âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ¶ğ‘…ğ‘—\nğ‘¡\nğ‘—=1\n+ ğ›½ğ¸ğ‘‰âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ¸ğ‘‰ğ‘—\nğ‘¡\nğ‘—=1\n+ ğ›½ğ‘…ğ‘ƒğ¸âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ‘…ğ‘ƒğ¸ğ‘—\nğ‘¡\nğ‘—=1\n   (7) \nHere, ğ›½0  represents the affective baseline. The weights ğ›½ğ¶ğ‘… , ğ›½ğ¸ğ‘‰ , and  ğ›½ğ‘…ğ‘ƒğ¸  capture the \n"}, {"page": 9, "text": " \n9 \n \naffective impact of Certain Rewards (CR), Expected Value (EV), and Reward Prediction \nError (RPE), respectively. RPE was defined as the difference between the obtained \noutcome and the EV (Outcome - EV). The parameter ğ›¾ is a forgetting factor (or affective \npersistence parameter); a higher ğ›¾ indicates that past events exert a longer-lasting influence \non current emotional state. \n \nStatistical analysis \nParameters were estimated using Maximum Likelihood Estimation (MLE). To avoid local \nminima, optimization was initiated from 50 random starting points for each subject. We \nanalyzed developmental trajectories across OpenAI models using one-way ANOVA with \npost-hoc comparisons. To benchmark models (N=30) against humans (N=747) while \naccounting for sample size disparities, we employed a bootstrapped resampling procedure \n(1,000 iterations) to generate empirical t-distributions for significance testing. All analyses \nwere two-tailed (p<0.05) and conducted in Matlab R2021a. \n \nResults \nChoice results  \nGambling Chosen A one-way ANOVA on the gambling rate with model version as the \nfactor (GPT-3.5, GPT-4, GPT-4o, GPT-4.1) revealed a significant effect (Figure 2A; F = \n64.05, p < 0.001, Î·pÂ² = 0.624). More frequent gambling was observed in more advanced \nmodels, following the pattern GPT-4.1 > GPT-4o > GPT-4 (ts > 3.913, ps < 0.001, \nCohenâ€™s ds > 1.010), whereas there was no significant difference between GPT-4 and \nGPT-3.5 (t = âˆ’0.636, p = 0.527, Cohenâ€™s d = 0.164). For reference, a purely expected-\n"}, {"page": 10, "text": " \n10 \n \nvalueâ€“maximizing agent would gamble on 55% of trials in this task. In addition, the \nhuman group did not differ from GPT-4.1 (P = 0.952), whereas it differed significantly \nfrom the other LLMs (Ps < 0.001). Similar results were obtained when analyzing \ndifferent types of gambles separately (see Supplementary Note 1). \n \nFigure 2. Evolutionary trajectory of computational decision parameters across the \nOpenAI lineage compared to human benchmarks. (A) The overall probability of choosing \nthe gamble. An overall increase in risk-taking behavior is observed across model \ngenerations, with GPT-4.1 exhibiting a high gambling propensity that is statistically \nindistinguishable from human participants (Resampling test, P = 0.952). The dashed \nhorizontal line in (A) indicates the gambling rate of an expected-valueâ€“maximizing \n(â€œrationalâ€) agent in this task. (B) Loss aversion parameter. Early models (GPT-3.5) \ndisplayed high loss aversion, whereas newer iterations show a significant decline, with \nGPT-4.1 exhibiting a \"loss-neutral\" or even loss-seeking profile. (C) Risk aversion \nparameter. No significant differences were found across model versions. (D) Pavlovian \napproach parameter. The overall trajectory reveals an increase: while GPT-4 and GPT-4o \nexhibited inhibitory tendencies (negative values), GPT-4.1 shifted to a positive approach \nbias, converging toward the human profile (P = 0.894). (E) Pavlovian avoidance \nparameter. Across generations, this bias progressively attenuated. By the latest iteration \n(GPT-4.1), the parameter magnitude significantly decreased, converging towards the \nhuman level. (F) Inverse temperature. Newer models are becoming significantly more \ndeterministic and consistent (lower decision noise) compared to the more stochastic \nhuman level. Note: Error bars represent the standard error of the mean (SEM). Asterisks \n(*) indicate a significant main effect of Model Version (ANOVA, p < 0.05). P-values \nabove specific comparisons denote the results of bootstrapped resampling tests against \nthe human benchmark. \n \n"}, {"page": 11, "text": " \n11 \n \nModeling Choice Across agents, the approachâ€“avoidance prospect theory model \nprovided the best account of the choice data (mean RÂ² = 0.51; Table S2; see \nSupplementary Note 2 for the full choice model space). \nLoss Aversion A one-way ANOVA on the loss aversion parameter revealed a significant \neffect of model version (Figure 2B; F = 35.87, p < 0.001, Î·pÂ² = 0.481). Loss aversion \ndecreased along the model trajectory, following the pattern GPT-4.1 < GPT-4o / GPT-4 < \nGPT-3.5 (ts > 2.262, ps < 0.027, Cohenâ€™s ds > 0.584), with no significant difference \nbetween GPT-4 and GPT-4o (t = âˆ’1.930, p = 0.059, Cohenâ€™s d = 0.498). Human data \nwere comparable to GPT-4 and GPT-4o (Ps > 0.843), but differed significantly from \nGPT-3.5 and GPT-4.1 (Ps < 0.001). \nRisk Aversion There was no significant effect of model version on the risk aversion \nparameter (Figure 2C; F = 2.37, p = 0.074, Î·p2â€‰=â€‰0.058). There were no significant \ndifferences between LLMs and human (Ps > 0.092). \nPavlovian Approach Bias A one-way ANOVA on the approach parameter showed a \nsignificant effect of model version (Figure 2D; F = 27.60, p < 0.001, Î·pÂ² = 0.417). \nApproach motivation was strongest in GPT-4.1, followed by GPT-3.5, and lower in GPT-\n4 and GPT-4o (ts > 2.262, ps < 0.027, Cohenâ€™s ds > 0.584), with no significant difference \nbetween GPT-4 and GPT-4o (t = âˆ’0.656, p = 0.514, Cohenâ€™s d = 0.170). Human data \nwere comparable to GPT-4.1 (P = 0.894), whereas it differed significantly from the other \nLLMs (Ps < 0.001). \nPavlovian Avoidance Bias A one-way ANOVA on the avoidance parameter also revealed \na significant effect (Figure 2E; F = 14.16, p < 0.001, Î·pÂ² = 0.268). Avoidance motivation \nwas weaker in more advanced models, following the pattern GPT-4.1 > GPT-4o / GPT-4 / \n"}, {"page": 12, "text": " \n12 \n \nGPT-3.5 (ts > 2.868, ps < 0.006, Cohenâ€™s ds > 0.741), with no significant difference \nbetween GPT-4o and GPT-4 (t = 1.598, p = 0.116, Cohenâ€™s d = 0.413) or between GPT-4 \nand GPT-3.5 (t = 1.694, p = 0.096, Cohenâ€™s d = 0.437). Human data were again \ncomparable to GPT-4.1 (P = 0.117), whereas it differed significantly from the other LLMs \n(Ps < 0.001). \nInverse Temperature A one-way ANOVA indicated a significant effect of model version \non the inverse temperature (Figure 2F; F = 25.54, p < 0.001, Î·pÂ² = 0.398). Post-hoc tests \nshowed a monotonic increase in inverse temperature across model generations (GPT-\n4.1 > GPT-4o > GPT-4 > GPT-3.5; ts > 2.068, ps < 0.043, Cohenâ€™s ds > 0.534), indicating \nprogressively lower decision noise. Human data were comparable to GPT-3.5 and GPT-4 \n(Ps > 0.602), where it differed significantly from GPT-4o and GPT-4.1 (Ps < 0.001). \n \nTaken together, these results indicate that as LLM versions have evolved, they show \nincreased gambling, exhibiting reduced (and eventually negligible) loss aversion, \nstronger Pavlovian approach motivation, weaker avoidance motivation, and lower \ndecision noise. Notably, the approachâ€“avoidance profiles of later models increasingly \nresemble those observed in human participants. \n \nHappiness results  \nAcross agents, both LLMs and humans displayed a clear hedonic effect, with higher \nhappiness ratings following gains than losses (ts = 11.793, ps < 0.001, Cohenâ€™s ds = \n2.067). We next asked how this affective response pattern is implemented at the \ncomputational level across model generations. \n"}, {"page": 13, "text": " \n13 \n \nThe classic affective model provided a good account of happiness ratings (mean pseudo-\nRÂ² = 0.65; see Supplementary Note 3 for the affective model space). \n \n \nFigure 3. Evolutionary trajectory of computational affective parameters across the OpenAI \nlineage compared to human benchmarks. (A) The impact of certain rewards (CR) on \nmomentary happiness. A significant reduction was observed for affective sensitivity to CR \nstarting from GPT-4o.  (B) The impact of expected values (EV) on momentary happiness. \nNo significant differences were found across model versions. (C) The impact of reward \nprediction error (RPE) on momentary happiness. This affective parameter initially peaked \nat GPT-4 (significantly higher than GPT-3.5), followed by a significant decline in GPT-4o. \n(D) Affective decay. This forgetting factor exhibits a significant monotonic increase across \nmodel generations. While early models (GPT-3.5) display transient emotional states \n(\"goldfish memory\"), GPT-4.1 shows a high value, indicating strong context dependence \nwhere past outcomes exert a prolonged influence on current emotional state. (E) Affective \nbaseline. No significant differences on affective baseline were found across model versions. \nA distinct \"digital positivity bias\" is observed. All LLMs, exhibit a significantly higher \nbaseline mood compared to the human control group, suggesting a hyper-hedonic default \nstate. Note: Error bars represent the standard error of the mean (SEM). Asterisks (*) \nindicate a significant main effect of Model Version (ANOVA, p < 0.05). \n \nAffective Sensitivity to CR A one-way ANOVA on affective sensitivity to CR revealed a \nsignificant effect of model version (Figure 3A; F = 11.09, p < 0.001, Î·pÂ² = 0.223). A \nmarked reduction in CR sensitivity emerged from GPT-4o onwards (t = âˆ’6.216, p < \n"}, {"page": 14, "text": " \n14 \n \n0.001, Cohenâ€™s d = 1.604), whereas there was no significant difference between GPT-3.5 \nand GPT-4 (t = 0.784, p = 0.436, Cohenâ€™s d = 0.202) or between GPT-4o and GPT-4.1 (t \n= âˆ’0.271, p = 0.788, Cohenâ€™s d = 0.070). All LLMs were comparable to human data (Ps > \n0.622). \nAffective Sensitivity to EV There was no significant effect of model version on affective \nsensitivity to EV (Figure 3B; F = 1.38, p = 0.251, Î·pÂ² = 0.035), and all LLMs were \ncomparable to humans (Ps > 0.928). \nAffective Sensitivity to RPE A one-way ANOVA revealed a significant effect of model \nversion (Figure 3C; F = 4.48, p = 0.005, Î·pÂ² = 0.104). Sensitivity increased from GPT-3.5 \nto GPT-4 (t = 2.073, p = 0.043, Cohenâ€™s d = 0.535), and then decreased from GPT-4 to \nGPT-4o (t = âˆ’2.312, p = 0.024, Cohenâ€™s d = 0.597), with no significant difference \nbetween GPT-4o and GPT-4.1 (t = âˆ’1.218, p = 0.228, Cohenâ€™s d = 0.315). No significant \ndifferences were found between any LLM and the human group (Ps > 0.441). \nAffective Decay A one-way ANOVA on the decay parameter revealed a significant effect \nof model version (Figure 3D; F = 8.63, p < 0.001, Î·pÂ² = 0.183). The decay parameter was \nlarger (i.e., affect depended more strongly on past outcomes) in GPT-4.1 and GPT-4o \nthan in GPT-4, and larger in GPT-4 than in GPT-3.5 (ts > 2.097, ps < 0.040, Cohenâ€™s ds > \n0.541), with no significant difference between GPT-4.1 and GPT-4o (t = âˆ’1.306, p = \n0.197, Cohenâ€™s d = 0.337). Human data were comparable to GPT-3.5 and GPT-4 (Ps > 0. \n188), where it differed significantly from GPT-4o and GPT-4.1 (Ps < 0.003). \nAffective Baseline There was no significant effect of model version on the affective \nbaseline (Figure 3E; F = 0.59, p = 0.620, Î·pÂ² = 0.015). However, all LLMs showed a \nsignificantly higher affective baseline than humans (Ps < 0.003). \n"}, {"page": 15, "text": " \n15 \n \n \nOverall, affective dynamics became more stable from GPT-4o onwards. By contrast, the \ndecay parameter increased in more advanced models, indicating that their reported \nemotional state depended more strongly on the history of past outcomes. Although \naffective baseline did not vary significantly across model versions, all LLMs exhibited a \nsubstantially higher affective baseline than humans, suggesting a chronically more \npositive affective set point. \n \nDiscussion \nThis study used a historical, developmental lens to examine how decision making and \naffective dynamics evolve across successive generations of LLMs. Rather than taking a \nstatic snapshot of a single versionâ€™s abilities9â€“13, we traced the â€œdevelopmental \ntrajectoriesâ€ of the OpenAI lineage in a human-validated gambling and happiness \nparadigm24,25. Our results provide partial support for the â€œhuman-like evolution\nâ€œ hypothesis: advanced models exhibited a marked increase in risk-taking behavior, \nconverging toward human behavioral norms, alongside increasingly human-like \napproach-avoidance motivations. At the same time, several parameters systematically \ndiverge from human norms as versions iterateâ€”loss aversion fades, decision noise drops \nbelow human levels, affective decay rises beyond the human range, and baseline mood \nremains consistently higher. These patterns highlight that model evolution shapes the \nimplicit â€œrisk styleâ€ and â€œaffective styleâ€ of LLMs, with important implications for AI \nethics and humanâ€“AI interaction, particularly in medical decision-making contexts. \n \n"}, {"page": 16, "text": " \n16 \n \nSupporting the â€human-like evolutionâ€œ hypothesis31, we found that technological \nmaturation drives LLMs toward increased gambling behavior, with advanced iterations \nclosely resembling human agents. It is crucial to clarify that this increased gambling \nbehavior in advanced models (e.g., GPT-4.1) does not imply irrational impulsivity. On the \ncontrary, by benchmarking against a purely expected-valueâ€“maximizing agent, the \ngambling rate of GPT-4.1 was very close to this rational benchmark (55%; Figure 2A). \nThe developmental pattern of this change is also consistent with previous evaluations of \nLLMs on neuropsychological and theory-of-mind benchmarks17,18, where higher-capacity \nversions tend to look â€œmore human-likeâ€ than earlier ones. Our study extends this \ndevelopmental logic in two ways. First, we provide a systematic, within-family analysis \nacross four consecutive generations, rather than comparing isolated versions. Second, we \nmove beyond static cognitive tests to chart how risk preferences and affective dynamics \nevolve, using computational models of approachâ€“avoidance motivation to open part of \nthe â€œblack boxâ€ of LLM development from a decision-making perspective16. The \ntrajectory we observe mirrors human developmental findings: along the model lineage, \nnewer versions show more gambling behavior accompanied by stronger approach \ntendencies, akin to the increased gambling behavior seen in younger vs. older adults33, \nbut it also raises the question of purpose: in some high-stakes settings we may want \nsystems that think with us, whereas in others we may prefer agents that remain \ndeliberately non-human in their risk profile to avoid a psychological â€œuncanny valleyâ€ in \nhow they advise, decide, and emotionally respond34. \n \n"}, {"page": 17, "text": " \n17 \n \nThe shift toward more human-like risk-taking in later models is, however, driven by \nchanges that are themselves quite non-human. In particular, increased gambling in GPT-\n4.1 co-occurs with a progressive disappearance of loss aversion and a marked reduction \nin decision noise. Along the OpenAI lineage, loss aversion drops from 2.72 in GPT-3.5 to \n0.57 in GPT-4.1, suggesting that newer models rely more on deliberative, System-2â€“like \nevaluation of expected value and less on intuitive, System-1â€“like heuristics such as \nâ€œlosses loom larger than gainsâ€35. In other words, decision-making in later LLM \ngenerations seems to be driven more by symmetric calculation than by the asymmetric, \ngut-level weighting of losses that typically characterizes human intuition. One possible \nexplanation for the reduction in loss aversion is the evolution of alignment \nprocedures36,37. Newer OpenAI models are trained more aggressively to maximise the \noutput of a reward model via algorithms such as Proximal Policy Optimization (PPO) and \nDirect Preference Optimization (DPO), with the reward model itself reflecting human \npreferences for â€œhelpfulâ€ answers. In numerical decision scenarios like our gambling \ntask, â€œhelpfulâ€ feedback is likely to favour mathematically optimal, expected-valueâ€“\nconsistent recommendations rather than the loss-averse biases typical of human intuition. \nThis could help explain why GPT-4.1 shows loss aversion below the neutral level, \nwhereas GPT-3.5 behaved more like a loss-averse human. Clinically, altered loss aversion \nhas been linked to affective traits such as anxiety38; our findings can be read, somewhat \ntongue-in-cheek, as suggesting that newer LLMs are â€œless anxiousâ€ about losing points \nthan their predecessors, although the direction and interpretation of such analogies should \nbe treated with caution. A similar story emerges for decision noise. A central theme in the \npublic narrative around LLM development is that hallucinations decrease with newer \n"}, {"page": 18, "text": " \n18 \n \nversions39. In our task, we observe an analogous pattern at the level of computational \nchoice parameters: the inverse temperature rises systematically across versions, \nindicating more deterministic and internally consistent choices, to the point where GPT-\n4.1 is less noisy than humans in the same environment. Although our study cannot \ndirectly link this to specific training interventions, it is tempting to view these trends as \nbehavioral echoes of engineering efforts to reduce hallucinations and enforce more \nreliable reasoning. \n \nBeyond choices, our affective results suggest that the â€œemotional styleâ€ of LLMs is also \nevolving in a systematic way. Affective sensitivities to certain rewards (CR) and \nprediction errors (RPE) become remarkably stable from GPT-4o onwards, suggesting that \nnewer models respond to outcomes with more stable â€œemotionalâ€ patterns, instead of \nshowing large trial-by-trial mood swings. At the same time, the affective decay parameter \nincreases along the lineage, indicating that newer models integrate a longer history of \noutcomes into their reported moodâ€”mirroring improvements in context handling and \nlong-range reasoning40. In addition, across all versions, baseline mood is substantially \nhigher than in humans, aka, positivity bias, in line with recent work showing that \nconversations with artificial agents can increase reported happiness41. While such \npositivity bias may be beneficial for supporting doctorâ€“patient communication and \nproviding emotionally comforting interactions, it also carries risks. In safety-critical \nsettings, an overly optimistic and reassuring tone may become sycophantic42, play down \nuncertainty or potential harm, and in the worst case contribute to serious medical errors \nwhen clear communication of negative information is needed43. \n"}, {"page": 19, "text": " \n19 \n \n \nThis study has several limitations. First, although our computational models help open \nthe â€œblack boxâ€ of how decision making and affective dynamics change across model \niterations, we still know little about the underlying representational and circuit-level \nmechanisms. Future work could combine our behavioral and computational approach \nwith mechanistic interpretability tools such as sparse autoencoders (SAEs)44 to probe \nwhich internal features or subnetworks implement these changing risk and affective \nprofiles. Second, our conclusions about â€œdevelopmental trajectoriesâ€ are based on four \nconsecutive versions within a single model family and implicitly assume relatively \nsmooth evolution. Recent work has shown that qualitative shifts in architecture or \ntraining can produce abrupt, cliff-like changes in human-like biases and abilities in LLMs \n(e.g., the emergence and subsequent disappearance of intuitive reasoning biases in \nChatGPT)9, suggesting that the patterns we report may not generalize to future models \nthat undergo major technological transitions. Third, although we use developmental \nlanguage for expository purposes, our design is cross-sectional: we compare distinct \nmodel releases rather than tracking a single system as it learns over time. \n \nIn conclusion, this study adopts a historically informed outlook on the future and a \ndevelopmental computational psychiatry framework to characterize how decision making \nand affective dynamics change as large language models evolve. Across successive \nmodel generations, approachâ€“avoidance motives become increasingly human-like, while \nother featuresâ€”most notably the near disappearance of loss aversion and an affective \ndecay parameter that progressively increases and ultimately exceeds human levelsâ€”\n"}, {"page": 20, "text": " \n20 \n \nremain clearly non-human. Together, these findings trace an emerging â€œpsychology of \nmachinesâ€ highlighting systematic trends in how LLMs change with versioning and \nproviding an empirical basis for discussions of AI ethics and humanâ€“AI interaction in \ndomains such as clinical decision support and doctorâ€“patient communication. \n \n \n"}, {"page": 21, "text": " \n21 \n \nAcknowledgements \nWe thank Xinyi Yang for helpful comments on the technical aspects of this work. This \nstudy was funded by the National Natural Science Foundation of China (32500929), \nMinistry of Education Humanities and Social Sciences (25YJC190023), and Guangdong \nProvincial Advanced Education Institutions Young Innovative Talent Project \n(2025WQNCX013). \n \nConflict of interest \nThe authors have indicated they have no potential conflicts of interest to disclose. \n \nData and code availability \nThe data that support the findings of this study are available from the corresponding author \nupon reasonable request. \n \n"}, {"page": 22, "text": " \n22 \n \nReferences \n1. \nLiang, W. et al. Quantifying large language model usage in scientific papers. Nat Hum Behav \nhttps://doi.org/10.1038/s41562-025-02273-8 (2025) doi:10.1038/s41562-025-02273-8. \n2. \nThirunavukarasu, A. J. et al. Large language models in medicine. Nat Med 29, 1930â€“1940 (2023). \n3. \nRao, A. et al. Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow. Preprint \nat https://doi.org/10.1101/2023.02.21.23285886 (2023). \n4. \nHaltaufderheide, J. & Ranisch, R. The ethics of ChatGPT in medicine and healthcare: a \nsystematic review on Large Language Models (LLMs). NPJ Digit Med 7, 183 (2024). \n5. \nTakita, H. et al. A systematic review and meta-analysis of diagnostic performance comparison \nbetween generative AI and physicians. NPJ Digit Med 8, 175 (2025). \n6. \nWeixiang Zhao et al. Is ChatGPT Equipped with Emotional Dialogue Capabilities? ArXiv (2023). \n7. \nChen, D. et al. Patient perceptions of empathy in physician and artificial intelligence chatbot \nresponses to patient questions about cancer. NPJ Digit Med 8, 275 (2025). \n8. \nBinz, M. & Schulz, E. Using cognitive psychology to understand GPT-3. Proceedings of the \nNational Academy of Sciences 120, (2023). \n9. \nHagendorff, T., Fabi, S. & Kosinski, M. Human-like intuitive behavior and reasoning biases \nemerged in large language models but disappeared in ChatGPT. Nat Comput Sci 3, 833â€“838 \n(2023). \n10. \nde Varda, A. G., Dâ€™Elia, F. P., Kean, H., Lampinen, A. & Fedorenko, E. The cost of thinking is similar \nbetween large reasoning models and humans. Proceedings of the National Academy of Sciences \n122, (2025). \n11. \nWebb, T., Holyoak, K. J. & Lu, H. Emergent analogical reasoning in large language models. Nat \nHum Behav 7, 1526â€“1541 (2023). \n12. \nCheung, V., Maier, M. & Lieder, F. Large language models show amplified cognitive biases in \nmoral decision-making. Proceedings of the National Academy of Sciences 122, (2025). \n13. \nAkata, E. et al. Playing repeated games with large language models. Nat Hum Behav 9, 1380â€“\n1390 (2025). \n14. \nRubin, M. et al. Comparing the value of perceived human versus AI-generated empathy. Nat \nHum Behav 9, 2345â€“2359 (2025). \n15. \nSharma, A., Lin, I. W., Miner, A. S., Atkins, D. C. & Althoff, T. Humanâ€“AI collaboration enables \nmore empathic conversations in text-based peer-to-peer mental health support. Nat Mach \nIntell 5, 46â€“57 (2023). \n16. \nRahwan, I. et al. Machine behaviour. Nature 568, 477â€“486 (2019). \n17. \nDayan, R., Uliel, B. & Koplewitz, G. Age against the machineâ€”susceptibility of large language \nmodels to cognitive impairment: cross sectional analysis. BMJ e081948 (2024) \ndoi:10.1136/bmj-2024-081948. \n18. \nStrachan, J. W. A. et al. Testing theory of mind in large language models and humans. Nat Hum \nBehav 8, 1285â€“1295 (2024). \n19. \nFisher, R. J. Social Desirability Bias and the Validity of Indirect Questioning. Journal of Consumer \nResearch 20, 303 (1993). \n20. \nAntin, J. & Shaw, A. Social desirability bias and self-reports of motivation. in Proceedings of the \nSIGCHI Conference on Human Factors in Computing Systems 2925â€“2934 (ACM, New York, NY, \nUSA, 2012). doi:10.1145/2207676.2208699. \n21. \nKao, C.-H., Feng, G. W., Hur, J. K., Jarvis, H. & Rutledge, R. B. Computational models of subjective \nfeelings in psychiatry. Neurosci Biobehav Rev 145, 105008 (2023). \n22. \nHuys, Q. J. M., Browning, M., Paulus, M. P. & Frank, M. J. Advances in the computational \nunderstanding of mental illness. Neuropsychopharmacology 46, 3â€“19 (2021). \n23. \nVanhasbroeck, N. et al. Testing a computational model of subjective well-being: a preregistered \nreplication of Rutledge et al. (2014). Cogn Emot 35, 822â€“835 (2021). \n24. \nRutledge, R. B., Skandali, N., Dayan, P. & Dolan, R. J. A computational and neural model of \n"}, {"page": 23, "text": " \n23 \n \nmomentary subjective well-being. Proceedings of the National Academy of Sciences 111, \n12252â€“12257 (2014). \n25. \nRutledge, R. B., Skandali, N., Dayan, P. & Dolan, R. J. Dopaminergic modulation of decision \nmaking and subjective well-being. Journal of Neuroscience 35, 9811â€“9822 (2015). \n26. \nAmos Tversky & Daniel Kahneman. Prospect Theory: An Analysis of Decision under Risk. (1979). \n27. \nWang, Z. et al. Mood computational mechanisms underlying increased risk behavior in \nadolescent suicidal patients. Preprint at https://doi.org/10.7554/eLife.108002.1 (2025). \n28. \nWang, Z. et al. Dissociable Roles of Reward Prediction Error in the Contrasting Mood Dynamics \nof Depression and Anxiety. Preprint at https://doi.org/10.31234/osf.io/z6s2r_v1 (2025). \n29. \nSchulz, E. & Dayan, P. Computational Psychiatry for Computers. iScience 23, 101772 (2020). \n30. \nShubham Singh. ChatGPT Users Stats (December 2025) â€“ Growth & Usage Data. \nhttps://www.demandsage.com/chatgpt-statistics/?utm_source=chatgpt.com (2025). \n31. \nLake, B. M., Ullman, T. D., Tenenbaum, J. B. & Gershman, S. J. Building machines that learn and \nthink like people. Behavioral and Brain Sciences 40, e253 (2017). \n32. \nBai, Y. et al. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human \nFeedback. (2022). \n33. \nRutledge, R. B. et al. Risk Taking for Potential Reward Decreases across the Lifespan. Current \nBiology 26, 1634â€“1639 (2016). \n34. \nMori, M., MacDorman, K. & Kageki, N. The Uncanny Valley [From the Field]. IEEE Robot Autom \nMag 19, 98â€“100 (2012). \n35. \nLi, Z.-Z. et al. From System 1 to System 2: A Survey of Reasoning Large Language Models. (2025). \n36. \nSpangher, L. et al. RLHF Algorithms Ranked: An Extensive Evaluation Across Diverse Tasks, \nRewards, and Hyperparameters. in Proceedings of the 2025 Conference on Empirical Methods \nin Natural Language Processing: Industry Track 518â€“529 (Association for Computational \nLinguistics, Stroudsburg, PA, USA, 2025). doi:10.18653/v1/2025.emnlp-industry.35. \n37. \nChaudhary, S., Dinesha, U., Kalathil, D. & Shakkottai, S. Risk-Averse Finetuning of Large \nLanguage Models. (2025). \n38. \nXu, P. et al. Amygdalaâ€“prefrontal connectivity modulates loss aversion bias in anxious \nindividuals. Neuroimage 218, 116957 (2020). \n39. \nHuang, L. et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, \nChallenges, and Open Questions. ACM Trans Inf Syst 43, 1â€“55 (2025). \n40. \nOpenAI. \nIntroducing \nGPT-4.1 \nin \nthe \nAPI. \nhttps://openai.com/index/gpt-4-\n1/?utm_source=chatgpt.com (2025). \n41. \nHeffner, J. et al. Increasing happiness through conversations with artificial intelligence. (2025). \n42. \nNaddaf, M. AI chatbots are sycophants â€” researchers say itâ€™s harming science. Nature 647, 13â€“\n14 (2025). \n43. \nSaenger, J. A., Hunger, J., Boss, A. & Richter, J. Delayed diagnosis of a transient ischemic attack \ncaused by ChatGPT. Wien Klin Wochenschr 136, 236â€“238 (2024). \n44. \nZhao, H. et al. Explainability for Large Language Models: A Survey. ACM Trans Intell Syst Technol \n15, 1â€“38 (2024). \n  \n \n \n"}, {"page": 24, "text": " \n24 \n \nSupplementary Note 1: Gambling Chosen for each type  \nOur task included 30 mixed trials, 30 gain trials, and 30 loss trials. In mixed trials, \nparticipants made a choice between a certain amount 0 and a gamble with a gain amount \n{40, 45, or 75} and a loss amount determined by a multiplier {0.2, 0.34, 0.5, 0.64, 0.77, \n0.89, 1, 1.1, 1.35, or 2} on the gain amount. For example, with a gain amount of 40 and a \nmultiplier of 2 for the loss (2 times 40 = 80), participants chose between a certain option \nof 0 and a gambling option, which offered a 50% chance to win 40 and a 50% chance to \nlose 80. These trials are therefore particularly suited to measuring loss aversion. In gain \ntrials, there was a certain gain amount {35, 45, or 55} and a gamble with 0 and a gain \namount determined by a multiplier {1.68, 1.82, 2, 2.22, 2.48, 2.8, 3.16, 3.6, 4.2, or 5} on \nthe certain gain amount. In loss trials, there were a certain loss amount {-35, -45, or -55} \nand a gamble with 0 and a loss amount determined by a multiplier {1.68, 1.82, 2, 2.22, \n2.48, 2.8, 3.16, 3.6, 4.2, or 5} on the certain loss amount. The task was administered via a \ntext-based interface where the model outputted its choice and, when prompted, its \nhappiness rating (an integer 0â€“100). For reference, a purely expected-valueâ€“maximizing \nagent would gamble on 55% of all trials (65% in mix trials, 75% in gain trials, and 25% in \nloss trials) in this task. \n \nGambling Chosen in Mix trials A one-way ANOVA with model version as the factor \n(GPT-3.5, GPT-4, GPT-4o, GPT-4.1) revealed a significant effect (Figure S1A; F = \n101.94, p < 0.001, Î·pÂ² = 0.725). Post-hoc tests showed a monotonic increase in inverse \ntemperature across model generations (GPT-4.1 > GPT-4o > GPT-4 > GPT-3.5; ts > \n"}, {"page": 25, "text": " \n25 \n \n3.307, ps < 0.002, Cohenâ€™s ds > 0.854). Human data were comparable to GPT-4 (P = \n0.739), but differed significantly from other LLMs (Ps < 0.001). \nGambling Chosen in Gain trials A one-way ANOVA with model version as the factor \n(GPT-3.5, GPT-4, GPT-4o, GPT-4.1) revealed a significant effect (Figure S1B; F = 31.62, \np < 0.001, Î·pÂ² = 0.450). Gambling behavior was highest in GPT-4.1, followed by GPT-\n3.5, and lower in GPT-4 and GPT-4o (ts > 2.392, ps < 0.020, Cohenâ€™s ds > 0.618), with \nno significant difference between GPT-4 and GPT-4o (t = âˆ’1.282, p = 0.205, Cohenâ€™s d = \n0.331). Human data were comparable to GPT-4.1 (P = 0.480), whereas it differed \nsignificantly from the other LLMs (Ps < 0.001). \nGambling Chosen in Loss trials A one-way ANOVA with model version as the factor \n(GPT-3.5, GPT-4, GPT-4o, GPT-4.1) revealed a significant effect (Figure S1C; F = 32.85, \np < 0.001, Î·pÂ² = 0.459). More frequent gambling was observed in more advanced models, \nfollowing the pattern GPT-4.1 > GPT-4o > GPT-4 (ts > 3.521, ps < 0.001, Cohenâ€™s ds > \n0.909), whereas there was no significant difference between GPT-4 and GPT-3.5 (t = \n1.688, p = 0.097, Cohenâ€™s d = 0.436). Human data were comparable to GPT-4.1 (P = \n0.949), whereas it differed significantly from the other LLMs (Ps < 0.001). \n \nSupplementary Note 2: Computational modeling of choice \nOur choice model space included expected value model (cM1), prospect theory model \n(cM2), and approach-avoidance prospect theory model (cM3). For cM2 (Equations 1-4), \nthere were 3 parameters, including risk aversion (Î±, range: [0.3, 1.3]), loss aversion (Î»: [0.5, \n5]), and inverse temperature (ğœ‡: [0, 10]). \nğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= 0.5(ğ‘‰ğ‘”ğ‘ğ‘–ğ‘›)ğ›¼âˆ’ 0.5Î»(âˆ’ğ‘‰ğ‘™ğ‘œğ‘ ğ‘ )ğ›¼                                (1) \n"}, {"page": 26, "text": " \n26 \n \nğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›= (ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)ğ›¼ ğ‘–ğ‘“ ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›â‰¥0                                   (2) \nğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›= âˆ’Î»(âˆ’ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)ğ›¼ ğ‘–ğ‘“ ğ‘‰ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›< 0                               (3) \nğ‘ƒğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= \n1\n1+ğ‘’âˆ’ğœ‡(ğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆ’ğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)                                       (4) \nwhere Vgain and Vloss are the objective gain and loss from a gamble, respectively. Please \nnote that Vgain is 0 in loss trials and Vloss is 0 in gain trials. Vcertain is the objective value for \nthe certain option. Ugamble and Ucertain denote the subjective utilities of the gamble and the \ncertain option, respectively. Choice probability for gamble (Pgamble) is determined by the \nsoftmax rule. Building on cM2, cM3 decomposes the decision process into risk-attitude-\ndriven valuation (e.g., loss and risk aversion) and value-insensitive motivational \ncomponents (Equations 1-3 & 5-7). That is, choice probability for Pgamble in cM3 is jointly \ndetermined by the softmax rule and approach/avoidance parameters (ğ›½ğ‘”ğ‘ğ‘–ğ‘› : [-1, 1], ğ›½ğ‘™ğ‘œğ‘ ğ‘ : \n[-1, 1]). Approach/avoidance parameters are not applied to in mixed trials.  \nğ‘ƒğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= \n1âˆ’ğ›½ğ‘£ğ‘ğ‘™\n1+ğ‘’âˆ’ğœ‡(ğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆ’ğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›) + ğ›½ğ‘£ğ‘ğ‘™  ğ‘–ğ‘“ ğ›½ğ‘£ğ‘ğ‘™â‰¥0                        (5) \nğ‘ƒğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= \n1+ğ›½ğ‘£ğ‘ğ‘™\n1+ğ‘’âˆ’ğœ‡(ğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆ’ğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›)   ğ‘–ğ‘“ ğ›½ğ‘£ğ‘ğ‘™< 0                             (6) \nğ›½ğ‘£ğ‘ğ‘™{ğ›½ğ‘”ğ‘ğ‘–ğ‘›, ğ‘”ğ‘ğ‘–ğ‘› ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ ,\nğ›½ğ‘™ğ‘œğ‘ ğ‘ , ğ‘™ğ‘œğ‘ ğ‘  ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ .                                               (7) \n \nWe also considered the traditional bias parameter (cM4), rather than approach/avoidance \nparameters. We limited the bias to the range of [-20, 20], which was in reward-equivalent \nunits.   \nğ‘ƒğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’= \n1\n1+ğ‘’âˆ’ğœ‡(ğ‘ˆğ‘”ğ‘ğ‘šğ‘ğ‘™ğ‘’âˆ’ğ‘ˆğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘ğ‘–ğ‘›+ğ›½ğ‘ğ‘–ğ‘ğ‘  )                                   (8) \n \nWe fit model parameters by using the method of maximum likelihood estimation (MLE) \n"}, {"page": 27, "text": " \n27 \n \nwith fmincon function of MATLAB (version R2021a) at the individual level. To avoid local \nminimum, we ran this optimization function with random starting locations 50 times. \nBayesian information criteria (BIC) were used to compare model fits.  \n \nThe winning model to formally quantify mechanisms for observed risky behavior was the \napproach-avoidance prospect theory model (cM3; mean R2 = 0.51; Table S2).  \n \nSupplementary Note 3: Computational modeling of affect \nTo quantify how different events impacted participantsâ€™ emotional states during the \ngambling task, we fit the classic model assuming that momentary happiness depends on \nthe recency-weighted average of the chosen certain reward (CR), expected value of the \nchosen gamble (EV), and reward prediction error (RPE; mM1; Equation 9). RPE was \ndefined as the difference between the obtained and expected value. \nHappiness(ğ‘¡) = ğ›½0 + ğ›½ğ¶ğ‘…âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ¶ğ‘…ğ‘—\nğ‘¡\nğ‘—=1\n+ ğ›½ğ¸ğ‘‰âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ¸ğ‘‰ğ‘—\nğ‘¡\nğ‘—=1\n+ ğ›½ğ‘…ğ‘ƒğ¸âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ‘…ğ‘ƒğ¸ğ‘—\nğ‘¡\nğ‘—=1\n (9) \nHere, t and j are trial numbers, ğ›½0 is a baseline affective parameter, other weights ğ›½ capture \nthe influence of different event types, Î³ âˆˆ  [0,1] is a decay parameter representing how \nmany previous trials influence happiness. CRj is the CR if the certain option was chosen \non trial j; otherwise, CRj is 0. EVj is the EV and RPEj is the RPE on trial j if the gamble \nwas chosen. If the certain option was chosen, then EVj = 0 and RPEj =0. \nWe also fit an alternative model in which happiness ratings are explained by the recency-\nweighted average of the certain reward (CR) and the gamble reward (GR; mM2; Equation \n10), a simple model providing affective sensitivity parameters for certain rewards and \ngamble rewards. \n"}, {"page": 28, "text": " \n28 \n \nHappiness(ğ‘¡) = ğ›½0 + ğ›½ğ¶ğ‘…âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğ¶ğ‘…ğ‘—\nğ‘¡\nğ‘—=1\n+ ğ›½ğºğ‘…âˆ‘\nğ›¾ğ‘¡âˆ’ğ‘—ğºğ‘…ğ‘—\nğ‘¡\nğ‘—=1\n              (10) \n \nThe winning model was mM1 (mM1; mean pseudo R2 = 0.65; Table S3).  \n \n \n \n"}, {"page": 29, "text": " \n29 \n \n \nFigure S1. Evolutionary trajectory of gambling decision for each type across the OpenAI \nlineage compared to human benchmarks. (A) The probability of choosing the gamble in \nmix trials. An overall increase in risk-taking behavior is observed across model \ngenerations, with GPT-4.1 exhibiting a high gambling propensity and with GPT4 \ndisplaying statistically indistinguishable from human participants (Resampling test, P = \n0.739). The dashed horizontal line in (A) indicates the gambling rate of an expected-\nvalueâ€“maximizing (â€œrationalâ€) agent in mix trials (65%). (B) The probability of choosing \nthe gamble in gain trials. GPT-3.5 shows moderate gambling, GPT-4 and GPT-4o are \nstrongly conservative, and GPT-4.1 becomes highly risk-seeking again. GPT-4.1 \napproaching the human level (P = 0.480). The dashed horizontal line in (B) indicates the \ngambling rate of an expected-valueâ€“maximizing (â€œrationalâ€) agent in gain trials (75%). \n(C) The probability of choosing the gamble in loss trials. Newer models are gambling \nmore, with GPT-4.1 approaching the human level (P = 0.949). Note: Error bars represent \nthe standard error of the mean (SEM). P-values above specific comparisons denote the \nresults of bootstrapped resampling tests against the human benchmark. \n \n \n \n \n \n"}, {"page": 30, "text": " \n30 \n \nTable S1. Specifications of the LLMs evaluated in this study. \nModel name \nRelease \ndate \nContext window \n(tokens) \nModal  \n(input -> output) \nGPT-3.5-Turbo \n2023-3 \n16k \nText -> Text \nGPT-4-Turbo \n2023-11 \n128k \nText/Image -> Text \nGPT-4o \n2024-5 \n128k \nText/Image -> Text \nGPT-4.1 \n2025-4 \n1M \nText/Image -> Text \n \nTable S2. Comparison for choice models. \nMode\nl # \nModel \nspecification \n# of \nparameters \nÎ” BIC \nmeanR2 \nÎ” BIC for each agent \n3.5 \n4 \n4o \n4.1 \nHuman \n1 \nÂµ \n1 \n25790 \n0.19 \n1032 2083 2290 1856 \n18529 \n2 \nÎ», Î±, Âµ \n3 \n18165 \n0.31 \n566 \n1891 1729 1157 \n12822 \n3 \nÎ», Î±, Î²gain, Î²loss, Âµ \n5 \n0 \n0.51 \n0 \n0 \n0 \n0 \n0 \n4 \nÎ», Î±, Î²bias, Âµ \n4 \n4901 \n0.44 \n299 \n293 \n174 \n402 \n3733 \nAbbreviations: Î” BIC, Bayesian information criterion relative to the winning model (cM3). \n \nTable S3. Comparison for affective models. \nMode\nl # \nModel \nspecification \n# of \nparameters \nÎ” \nBIC \nmeanR2 \nÎ” BIC for each agent \n3.5 \n4 \n4o \n4.1 \nHuman \n1 \nÎ²0, Î²CR, Î²EV, Î²RPE, Î³ \n5 \n0 \n0.65 \n0 \n0 \n0 \n0 \n0 \n2 \nÎ²0, Î²CR, Î²GR, Î³ \n4 \n2104 \n0.60 \n34 \n95 \n-2 \n146 \n1831 \nAbbreviations: Î” BIC, Bayesian information criterion relative to the winning model (mM1). \n \n \n \n \n"}]}