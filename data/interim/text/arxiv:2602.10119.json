{"doc_id": "arxiv:2602.10119", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.10119.pdf", "meta": {"doc_id": "arxiv:2602.10119", "source": "arxiv", "arxiv_id": "2602.10119", "title": "Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke", "authors": ["Anjali K. Kapoor", "Anton Alyakin", "Jin Vivian Lee", "Eunice Yang", "Annelene M. Schulze", "Krithik Vishwanath", "Jinseok Lee", "Yindalon Aphinyanaphongs", "Howard Riina", "Jennifer A. Frontera", "Eric Karl Oermann"], "published": "2026-01-18T04:18:42Z", "updated": "2026-01-18T04:18:42Z", "summary": "Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.10119v1", "url_pdf": "https://arxiv.org/pdf/2602.10119.pdf", "meta_path": "data/raw/arxiv/meta/2602.10119.json", "sha256": "246493e01a21feecdc9bba7818f8b779eafb46635ee3a8e37fb1eb39f7dad4a0", "status": "ok", "fetched_at": "2026-02-18T02:21:20.027792+00:00"}, "pages": [{"page": 1, "text": "Large Language Models Predict Functional Outcomes  \nafter Acute Ischemic Stroke \n \nAnjali K. Kapoor, A.B.1,*, Anton Alyakin, M.S.E1-3, Jin Vivian Lee, M.D. M.S.,1-3, Eunice Yang, \nB.S.1,4, Annelene M. Schulze, M.S.1, Krithik Vishwanath1,5, Jinseok Lee, Ph.D.2,6, Yindalon \nAphinyanaphongs, M.D., Ph.D.7,8, Howard Riina, M.D.1,9, Jennifer A. Frontera, M.D.10, Eric Karl \nOermann, M.D.1,2,8,11,‡ \n \n1Department of Neurosurgery, NYU Langone Health; New York, NY 10016, USA \n2Global AI Frontier Lab, New York University; Brooklyn, NY 11201, USA \n3Department of Neurosurgery, Washington University in Saint Louis; Saint Louis, MO 63110, \nUSA \n4Columbia University Vagelos College of Physicians and Surgeons; New York, NY, USA \n5Department of Aerospace Engineering & Engineering Mechanics, The University of Texas at \nAustin; Austin, TX 78705, USA \n6Department of Biomedical Engineering, Kyung Hee University, Yongin, South Korea \n7Department of Population Health, NYU Langone Health, New York, NY 10016, USA \n8Division of Applied AI Technologies, NYU Langone Health, New York, NY 10016, USA \n9Department of Radiology, NYU Langone Health; New York, NY 10016, USA \n10Department of Neurology, NYU Langone Health; New York, NY 10016, USA \n11Center for Data Science, New York University; New York, NY 10011, USA \n \n*Corresponding author (review): Anjali K. Kapoor, Department of Neurosurgery, NYU \nLangone Medical Center, New York University, 550 First Ave, MS 3 205, New York, NY 10016, \nUSA. Email: anjali.kapoor@nyulangone.org \n \n‡Corresponding author (post-publication): Eric K. Oermann, MD, Department of \nNeurosurgery, NYU Langone Medical Center, New York University, 550 First Ave, MS 3 205, \nNew York, NY 10016, USA. Email: Eric.Oermann@nyulangone.org \n \nAuthor Contributions: Conception and design: Frontera, Oermann. Registry maintenance: \nFrontera. Data acquisition: Kapoor, Frontera. Model training and evaluation: Kapoor. Baseline \nmodel reproduction: Kapoor. Analysis and interpretation of data: Kapoor, Alyakin, Oermann. \nStatistical analysis: Kapoor, Alyakin. Figure creation: Kapoor, Alyakin, Yang, Schulze, \nOermann. Manuscript drafting: Kapoor, Alyakin, Lee, Oermann. Critical revision of manuscript: \nAll authors. Study supervision: Frontera, Oermann. Resources and funding: Frontera, Oermann. \n \n"}, {"page": 2, "text": "1 \nAbstract \nBackground: Accurate prediction of functional outcomes measured after acute ischemic stroke \ncan guide clinical decision-making and resource allocation. Prior work on predicting modified \nRankin Scale has relied predominantly on structured variable predictors (age, NIHSS) and \nconventional machine learning. The ability of large language models (LLMs) to infer future mRS \nscores from routine clinical admission notes is largely unexplored. \nMethods: We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, \nMedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS \nprediction using a large, real-world stroke registry. We retrospectively analyzed 11,472 \nadmission History and Physical notes from 8,723 ischemic stroke admissions in the NYU \nLangone Get With The Guidelines–Stroke registry (2016–2025). The discharge outcome dataset \nincluded 9,485 notes, while the 90-day outcome dataset included 1,898 notes. Data were \ntemporally split with the most recent 12 months held out for testing. Performance was assessed \nusing exact (7-class) mRS accuracy and binary functional outcome (mRS 0–2 vs. 3–6) accuracy \nand compared against established structured-data baselines incorporating NIHSS and age. \nResults: The best performing model was fine-tuned Llama, which achieved the highest 90-day \nexact mRS accuracy of 33.9% [95% CI, 27.9%-39.9%]. It also achieved 90-day binary functional \noutcome accuracy of 76.3% [95% CI, 70.7%-81.9%], discharge exact mRS accuracy of 42.0% \n[95% CI, 39.0%-45.0%], and discharge binary accuracy of 75.0% [95% CI, 72.4%-77.6%]. For \n90-day prediction, Llama performed comparably to structured-data baselines (exact: 28.1%; \nbinary: 72.6–73.8%). \nConclusions: Fine-tuned LLMs can predict post-stroke functional outcomes from admission \nnotes alone, achieving performance comparable to models requiring structured variable \nabstraction. Task-specific fine-tuning altered the relative performance of model classes, with \ngenerative models outperforming encoders after adaptation. Our findings support the \ndevelopment of text-based prognostic tools that integrate seamlessly into clinical workflows \nwithout manual data extraction. \nKeywords: Artificial Intelligence; Natural Language Processing; Clinical Decision Support; \nElectronic Health Records; modified Rankin Scale \nNon-standard Abbreviations and Acronyms: ASTRAL, Acute Stroke Registry and Analysis of \nLausanne; BERT, Bidirectional Encoder Representations from Transformers; CI, confidence \ninterval; CORAL, COntinuous RAnked Logits; CT, computed tomography; EHR, electronic \nhealth record; EMR, electronic medical record; GWTG, Get With The Guidelines; H&P, history \nand physical; HPI, history of present illness; IQR, interquartile range; IV, intravenous; LLM, \nlarge language model; LoRA, Low-Rank Adaptation; MAE, mean absolute error; MER, \nmechanical thrombectomy; MRI, magnetic resonance imaging; mRS, modified Rankin Scale; \nNIHSS, National Institutes of Health Stroke Scale; PHI, Protected Health Information; PLAN, \nPrestroke modified Rankin Scale, Level of consciousness, Age, NIHSS; AUPRC, area under the \nprecision–recall curve; AUROC, area under the receiver operating characteristic curve\n \n"}, {"page": 3, "text": "2 \n \nIntroduction \nStroke is a leading cause of disability worldwide, with the modified Rankin Scale (mRS) serving \nas the standard clinical measure for assessing post-stroke functional outcomes1. Accurate \nprediction of mRS scores can aid in clinical decision-making, goal setting, and resource \nallocation in stroke care. Traditionally, stroke outcome prediction has relied on structured clinical \ndata such as National Institutes of Health Stroke Scale (NIHSS) scores2–5, age2–5, sex5,6, \ncomorbidities5,7,8, laboratory values5,9, length of stay10, and discharge disposition10–12. These \nmodels can be effective, but they often require resource-intensive data abstraction and complex \npreprocessing, making them difficult to deploy in practice.13,14 Additionally, the majority of these \nstudies depend on information not available at admission, such as discharge NIHSS4 and mRS11,15 \nscores, disposition10–12, or even later assessments such as 1-week16 or 30-day mRS17, in order to \npredict 90-day mRS scores. By contrast, models that use unstructured clinical notes from \nadmission support low-friction deployment and provide prognostic insight early in the \nhospitalization to support clinical care.14  \nUnstructured clinical text has been shown to improve the prediction of functional outcomes after \nstrokes. One study demonstrated that incorporating History of Present Illness (HPI) notes and CT \nreports significantly improved established baseline scores such as PLAN and ASTRAL.18 \nAnother study showed that using ClinicalBERT on the admission HPI performs similarly to \nusing NIHSS for binarized 90-day mRS prediction.19 A third study compared the predictive \npower of clinical text information, radiomics features, and DeepSurv-derived survival features \nusing machine learning classifiers and found that text was the strongest single predictor of \n90-day functional outcome.20 Unstructured clinical text carries rich prognostic signal—often \nmatching or exceeding the predictive value of traditional structured variables—while being \nreadily available at admission and requiring minimal preprocessing or workflow disruption. \nHowever, the optimal approach for extracting prognostic information from text remains \nunclear.21,22 Large language models (LLMs) have recently shown strong promise for analyzing \ncomplex clinical text.14,23 Unlike earlier natural language processing models that require \nextensive feature engineering, LLM encoders, such as BERT24, can automatically generate \ncontextualized representations that capture semantic relationships across text24, whereas \ngenerative LLMs, such as GPT25, can perform complex multi-step reasoning without \ntask-specific training25. This makes them particularly well-suited for outcome prediction from \nheterogeneous and unstructured clinical text.14,23 Yet, to date, no study has evaluated how \ngenerative LLMs perform on mRS prediction using unstructured clinical documentation as the \nsole input.  \nWe conducted a comprehensive evaluation of LLM architectures, pretraining approaches, and \nfine-tuning configurations for predicting both the discharge and the 90-day mRS scores. An \noverview of our approach is displayed in Figure 1. Analyses were performed using clinical data \nfrom 10,302 stroke patient admissions collected at NYU Langone Health as part of the Get With \nThe Guidelines initiative26,27. We assessed encoders (BERT24 and NYUTron14) and generative \nmodels (Llama-3.1-8B28 and MedGemma-4B29) under both frozen and fine-tuned conditions. \nThese model choices allow us to directly compare generalist and clinically pretrained models of \ncomparable size. We also reproduced the structured-data approach of Zhang et al.4 and the \ncombined structured- and unstructured-data approach of Sung et al.19 to provide meaningful  \n"}, {"page": 4, "text": "3 \n \nFigure 1. Overview of stroke outcome prediction. H&P notes from stroke patients were input \ninto frozen and fine-tuned encoder and generative models to predict the mRS score at discharge \nand 90-day. The predicted mRS scores range from 0 (no symptoms) to 6 (death), with scores 0–2 \nbinarized to represent good outcomes and 3–6 to represent poor outcomes. \nAbbreviations: BERT, Bidirectional Encoder Representations from Transformers; H&P, History \nand Physical; mRS, modified Rankin Scale \nbaseline comparisons against established 90-day mRS predictive models. This work seeks to \nclarify the potential and limitations of LLMs for predicting post-stroke functional outcomes and \nto inform their future integration into clinical decision-making. \nMethods \nStudy Design \nThis study was approved by the NYU Institutional Review Board (i22-0302). The need for \ninformed consent was waived. The study was reported in accordance with the Transparent \nReporting of Multivariable Prediction Model for Individual Prognosis or Diagnosis using \nArtificial Intelligence (TRIPOD+LLM) guidelines30. Data cannot be shared due to PHI, but code \ncan be shared upon request. The corresponding author had full access to all study data and takes \nresponsibility for the integrity of the data and the accuracy of the data analysis. \nThe study aims to predict both the discharge mRS and 90-day post-discharge mRS scores \nutilizing admission H&P notes, representing routine clinical documentation that captures \npertinent patient information at the time of admission. All consecutive patients admitted for acute \nischemic stroke between January 2016 and February 2025 in the NYU Langone Health \n(NYULH) Hospital System were identified from an institutional Get With The \nGuidelines–Stroke (GWTG-Stroke) registry31. NYULH is a university-affiliated medical center \ncomprised of three certified Comprehensive Stroke Centers, located at NYU Langone Tisch \nHospital in Manhattan, NYU Langone Hospital—Brooklyn, and NYU Langone Hospital—Long \nIsland. All cases of stroke were prospectively registered in the stroke registries based on the \nfollowing criteria: 1) patients with a diagnosis of stroke at the emergency department or during \n"}, {"page": 5, "text": "4 \n \nhospitalization, 2) daily screening of all patients receiving head CT, or 3) screening for a \ndiagnosis at discharge using the International Classification of Diseases, Ninth and Tenth \nRevisions (ICD‐9 and ICD‐10 revisions with clinical modification) codes. Data regarding the \ndemographics, cause, risk factor profiles, intervention, and outcomes of patients with stroke were \ncollected. Stroke severity was assessed using the NIHSS and functional outcome was measured \nusing the mRS. \nOnly cases with complete mRS labels (discharge and/or 90-day post-discharge) and valid H&P \nnotes were included in this study. A flowchart with inclusion and exclusion criteria is provided in \nFigure 2. We extracted their discharge and 90-day post-discharge mRS scores from the registry \nand unstructured clinical notes (all H&P notes at admission) from the electronic medical record \n(EMR) via NYUMC Neuro Data Hub32.  \n \n \nFigure 2: Flowchart depicting construction of the discharge and 90-day datasets derived \nfrom the GWTG–Stroke registry and linked admission H&P notes. A total of 10302 stroke \nadmissions corresponding to 10,271 patients from the registry were identified. For the discharge \nanalysis, admissions missing the discharge mRS score or H&P note were excluded, yielding a \nDischarge Dataset of 9,485 notes from 6,884 patients. For the 90-day analysis, admissions \nmissing the 90-day mRS score or H&P note were excluded, resulting in a 90-day Dataset of \n1,898 notes from 1,400 patients. A subset of the 90-day dataset excluding cases missing NIHSS \nvalues produced the 90-day Baseline Comparison Dataset (1,637 notes, 1,170 patients). These \nfinal cohorts were temporally split into train and test sets and were used for model development, \ncross-validation, and evaluation. \nAbbreviations: GWTG: Get With The Guidelines; H&P, History and Physical; mRS, modified \nRankin Scale; NIHSS, National Institutes of Health Stroke Scale\n \n"}, {"page": 6, "text": "5 \nOutcomes \nThe primary outcomes were the exact 90-day mRS score (total 7 classes: 0, 1, 2, 3, 4, 5, and 6) \nand good vs. poor functional outcome after acute stroke (mRS 0-2 vs. 3-6). The secondary \noutcomes were the exact discharge mRS score (total 7 classes: 0, 1, 2, 3, 4, 5, and 6) and good \nvs. poor functional status at discharge (mRS 0-2 vs. 3-6). \nData \nPatients with available H&P notes and corresponding mRS scores were included in the discharge \nand/or 90-day datasets, depending on which outcome timepoints were available. The discharge \ndataset consists of 9,485 H&P notes corresponding to 6,888 admissions and 6,884 patients. The \n90-day post-discharge dataset, which is restricted to patients who underwent thrombectomy or \nthrombolysis, consists of 1,898 notes corresponding to 1,400 admissions and 1,400 patients. For \npatients with multiple admission notes, each note was treated as an independent sample \n(Supplementary Fig. S1). To simulate prospective real-world deployment, we adopted a \ntemporal split in which the most recent 12 months of admissions were reserved as the held-out \ntest set. This yielded 8,408 training/validation notes and 1,077 test notes for the discharge \ndataset, and 1,674 training/validation notes and 224 test notes for the 90-days post-discharge \ndataset (train/test split: 90/10). \nPreprocessing \nH&P notes were preprocessed using each model’s native tokenizer, which segments text into \nmodel-specific tokens (e.g., words or subwords). To mimic real-world deployment scenarios, no \nmanual text selection or processing was performed. Figure S2 displays the distribution of H&P \ntoken lengths across all admission notes. \nFor encoder models (BERT24 and NYUTron14), we truncated the H&P notes to a maximum of \n512 tokens (approximately two paragraphs, roughly corresponding to the HPI) to conform to the \ninput length limitations of these models; after testing three different strategies of converting the \nH&P notes into 512 tokens—first 512, last 512, and summarization— we chose to truncate each \nnote to the first 512 tokens, as depicted in Figure S3. Meanwhile, generative models (Llama28 \nand MedGemma29) had larger context windows, enabling input sequences up to 8192 tokens. \nClinical Features \nTo compare with other baseline prognostic models, we additionally collected age and NIHSS \nscore from the structured registry dataset for 90-day mRS prediction and reproduced the Sung et \nal. model19 and the Zhang et al. model4. \nEncoder Models \nWe evaluated two encoder models: BERT24 and NYUTron14. BERT24 is a widely used \ngeneral-purpose encoder transformer model pretrained on large corpora of text. NYUTron14 is a \nclinical adaptation of the BERT architecture that is specifically pretrained on a corpus of hospital \nnotes. \nBoth models were evaluated in frozen and fine-tuned configurations. In the frozen configuration, \nwe used the embeddings produced by the models as input for downstream prediction tasks. We \ncompared four classifier heads: linear regression, L2-regularized logistic regression, CORAL \nordinal regression, and a random forest classifier. For each encoder, the classifier head that \nachieved the highest performance during cross-validation was selected and subsequently applied \n"}, {"page": 7, "text": "6 \n \nto the held-out test set, as depicted in Figure S4. In the fine-tuned configuration, we fine-tuned \nBERT and NYUTron for the specific task of mRS prediction. We systematically unfroze varying \nnumbers of transformer layers—from 0 (frozen embeddings) to 12 (full fine-tuning)—and \nidentified the optimal fine-tuning depth for each mode (Fig. S5). The embeddings were passed \nthrough a linear regression head to predict continuous mRS scores (range 0-6). We optimized the \nmodel using mean absolute error (MAE) loss. Predictions were clipped to the 0-6 range to match \nthe mRS scale and then rounded to the nearest integer for discrete mRS classification. All \nfine-tuned encoder models were trained on a single NVIDIA A100 GPU with a consistent \noptimization setup of five epochs, batch size = 8, optimizer = AdamW, learning rate = 2×10⁻⁵. \nGenerative Models \nWe also evaluated two generative LLMs: Llama-3.1-8B28 and MedGemma-4B29. As \nautoregressive transformers, these models generate text sequentially while drawing on \ninformation from across the entire H&P note.25 Llama-3.1-8B is a general-purpose model \npretrained on vast internet data28, whereas MedGemma-4B is a clinical model pretrained on \nmedical text29. Both models were evaluated using zero-shot prompting and parameter-efficient \nfine-tuning.  \n \nFigure 3. Overview of the stroke cohort (Discharge: N = 6,888 admissions; 90-day: N = \n1,400 admissions).  Panels A–D summarize characteristics of the discharge cohort at the \nadmission level—including admission counts by year (A), age distribution (B), sex distribution \n(C), and discharge mRS (D). Panel E depicts the 90-day mRS distribution for admissions with \navailable follow-up (limited to those treated with thrombectomy or thrombolysis).\n \n"}, {"page": 8, "text": "7 \nFor the zero-shot prompting, we used the -Instruct variants28,29 of the models. We tested two \nversions of the prompt, provided in Figure S6: Prompt A only instructed the model to predict the \nmRS score, whereas Prompt B also provided the mRS definitions. We found no significant \ndifference between two prompts in our experiments, suggesting that the models are familiar with \nthe concept of mRS (Fig. S7). Results throughout the study are reported for Prompt B. \nFor fine-tuning, we used the base variants of each model. We converted them into seven-class \nmRS classifiers by attaching a task-specific softmax classification head. We used Low-Rank \nAdaptation (LoRA)33 to efficiently fine-tune the models with a reduced number of trainable \nparameters. All generative models were fine-tuned on a single NVIDIA A100 GPU using an \nidentical set of hyperparameters: three epochs, batch size of 1 with four gradient-accumulation \nsteps for an effective batch size of 4, AdamW optimizer, with a learning rate of 5×10⁻⁵ and  \nweight decay of 0.01. We used a uniform LoRA configuration (r = 16, α = 32, dropout = 0.05, \ntarget modules = [q_proj, v_proj]). \nBaseline Prognostic Models \nWe reproduced two established 90-day mRS prediction baseline models: the structured-data \nmodel from Zhang et al.4 and the text-augmented structured model from Sung et al.19 \nFor the structured-data baseline, we implemented the ordinal logistic regression model \nintroduced by Zhang et al.4. Their model identified age and NIHSS as independent predictors of \n90-day mRS. Following their approach, we trained a proportional-odds regression model using \nthese two variables. \nFor the hybrid baseline, we replicated the best-performing architecture described by Sung et al.19, \nwhich integrates unstructured clinical text with key structured predictors. In their approach, \nmodels using the HPI text were enhanced by combining text-derived predictions with age and \nNIHSS. Without having their model weights, we approximated this setup by fine-tuning \nBio-ClinicalBERT on H&P notes to generate predicted probabilities of poor outcome (mRS ≥ 3) \nand then incorporating these probabilities together with age and NIHSS into a logistic regression \nclassifier. \nOur best-performing model—fine-tuned Llama—was compared against these two baseline \nmodels. For exact 90-day mRS accuracy, only the models producing ordinal or multiclass mRS \noutputs (Zhang et al. and Llama) were evaluated, as Sung et al. report binary outcomes only. All \nthree models (Zhang et al., Sung et al., and Llama) were also assessed on binary accuracy (mRS \n0–2 vs. 3–6). \nEvaluation Metrics \nModel performance was assessed using both exact and binary accuracy. Exact accuracy was \ndefined as the proportion of predictions in which the model’s mRS output (total 7 classes: 0-6) \nmatched the true discharge or 90-day mRS score exactly. Binary accuracy was defined as the \nproportion of predictions in which the model’s mRS output (total 7 classes: 0-6) matched the \nfavorable outcome category (mRS scores ranging 0–2) or the unfavorable outcome category \n(mRS scores ranging 3–6).\n \n"}, {"page": 9, "text": "8 \n \nTable 1. Baseline demographic and clinical characteristics of the discharge and 90-day \ncohorts.  \nCharacteristic \nDischarge cohort (N = 6,888 \nadmissions) \n90-day cohort (N = 1,400 \nadmissions) \nAge (median [IQR]) \n71 [61-81] \n73 [61-83] \nSex \nFemale, 47.6% \nMale, 52.4% \nFemale, 48.5% \nMale, 51.5% \nAdmission NIHSS (median \n[IQR]) \n3 [1-8]  \n(Excluding missing data for \n1.1% of patients) \n10 [4-19] \n(Excluding missing data for \n3.3% of patients) \nDischarge Disposition \nHome, 55.0% \nOther Health Care Facility, \n38.5% \nAcute Care Facility, 0.6% \nLeft AMA, 0.8% \nHospice - Health Care \nFacility, 2.9% \nHospice - Home, 0.7% \nExpired, 1.6%  \nHome, 44.8% \nOther Health Care Facility, \n41.0% \nAcute Care Facility, 0.5% \nLeft AMA, 0.4% \nHospice - Health Care \nFacility, 7.7% \nHospice - Home, 0.7% \nExpired, 3.9%  \n(Excluding missing data for \n1% of patients) \nIV Thrombolytic Initiated \n11.5%  \n(Excluding missing data for \n.4% of patients) \n52.0% \n(Excluding missing data for \n1.5% of patients) \nMechanical Thrombectomy \n(MER) Initiated \n14.2%  \n(Excluding missing data for \n12.5% of patients) \n58.1% \n(Excluding missing data for \n3.6% of patients) \nFinal Stroke Diagnosis \nIschemic stroke only \nIschemic stroke only \nmRS (median [IQR]) \nDischarge mRS: 2 [1-4] \n90-day mRS: 2 [0-4] \nBinary mRS \nDischarge mRS < 3, 52.6% \nDischarge mRS ≥ 3, 47.4% \n90-day mRS < 3, 54.9% \n90-day mRS ≥ 3, 45.1% \nAbbreviations: AMA, against medical advice; IQR, interquartile range; IV, intravenous; MER, \nmechanical thrombectomy; mRS, modified Rankin Scale; NIHSS, National Institutes of Health \nStroke Scale \n \n"}, {"page": 10, "text": "9 \nStatistical Analysis \nMetrics were reported as point estimates, with cross-validation results averaged across folds and \ntest-set performance bootstrapped (1000 resamples) to obtain 95% confidence intervals. Model \ncomparisons were performed on paired predictions using McNemar’s test. All tests were \ntwo-sided, with the significance level of 0.05. Statistical analyses were conducted in Python \n(v3.9) using SciPy, scikit-learn, statsmodels, and PyTorch (2.7.1) with Hugging Face \nTransformers. \nResults  \nStudy population \nA total of 10,271 patients were registered to the study cohort during the study period. After \nexcluding 1556 patients with missing H&P notes, 8715 patients were included. Of these, 1831 \npatients (21%) were missing discharge mRS scores and 7315 patients (84%) were missing \n90-day post-discharge mRS scores. Patients with available mRS scores were allocated to two \ngroups: discharge mRS (N=6884) and 90-days post-discharge mRS (N=1400). Baseline \ndemographics and clinical characteristics are detailed in Table 1 and Figure 3. Among the 1400 \npatients in the 90-day dataset, approximately 52.0% received IV thrombolytic therapy, and \n58.1% received thrombectomy (Table 1). The distribution of mRS scores at discharge and at 90 \ndays post-discharge is provided in Figure 3D and 3E.  \nDischarge mRS prediction with frozen models \nAs displayed in Figure 4, in the frozen configuration, the clinically pretrained NYUTron \nperformed best. For the exact discharge mRS prediction, NYUTron achieved the highest \naccuracy of 35.6% [95% CI, 32.8%-38.4%], significantly outperforming general-purpose BERT \nat 30.6% [95% CI, 28.0%-33.2%] (P = 2.32×10⁻⁵), MedGemma at 25.6% [95% CI, \n23.0%-28.2%] (P = 5.62x10-8), and Llama at 17.6% [95% CI, 15.3%-19.9%] (P = 5.31x10-17). \nBoth MedGemma and Llama were also significantly outperformed by BERT (P = 7.43x10-3 and \nP = 1.88x10-10, respectively).  \nFor the binary discharge mRS prediction, NYUTron (72.0% [95% CI, 69.3%-74.7%]), \nsignificantly outperformed MedGemma (66.4% [95% CI, 63.5%-69.3%], P = 2.61x10-3), Llama \n(65.2% [95% CI, 62.4%-68.0%], P = 2.61x10-3), and BERT (59.8% [95% CI, 56.9%-62.7%]; P = \n1.17×10-13). Consistent with these results, NYUTron achieved the highest area under the receiver \noperating characteristic curve (AUROC) of 0.79 and area under the precision–recall curve \n(AUPRC) of 0.78 (Fig. S8A and S8B).  \n90-day mRS prediction with frozen models \n90-day mRS prediction results followed a similar pattern. NYUTron again achieved the highest \nexact accuracy of 31.7% [95% CI, 25.7%-37.7%], followed by BERT (28.6% [95% CI, \n22.8%-34.4%]), MedGemma (24.1% [95% CI, 18.7%-29.5%]), and Llama (20.1% [95% CI, \n15.0%-25.2%]). Only the NYUTron versus Llama comparison reached statistical significance (P \n= 1.15×10-2).\n \n"}, {"page": 11, "text": "10 \n \n \nFigure 4. Frozen model comparison for discharge and 90-day mRS prediction. Panels A–D \nshow exact accuracy (left column) and binary accuracy (right column; mRS 0–2 vs 3–6) with \n95% confidence intervals derived from bootstrapped test-set estimates for four frozen models \nevaluated on discharge (Panels A–B) and 90-day (Panels C–D) outcomes. Encoder models \n(BERT and NYUTron) use random-forest classifier heads trained on frozen embeddings, \nwhereas generative models (Llama and MedGemma) are evaluated in their frozen zero-shot \nconfigurations using the definition-augmented prompt. Shared significance-group lettering (a–d) \nabove each bar denotes CLD groupings: models that do not share a letter differ significantly (p < \n0.05), whereas models that share at least one letter are not significantly different. \nAbbreviations: CLD, compact letter display; mRS, modified Rankin Scale; RF, random forest \n \n"}, {"page": 12, "text": "11 \nFor binary 90-day accuracy, the pattern was similar to that of the binary discharge mRS \nprediction. NYUTron achieved the highest accuracy of 79.9% [95% CI, 74.8%-85.0%], \nsignificantly outperforming MedGemma (67.0% [95% CI, 60.7%-73.3%], P = 1.05×10-3), Llama \n(65.6% [95% CI, 58.9%-72.3%], P = 1.31×10-4), and BERT (65.2% [95% CI, 53.3%-77.1%]; P = \n2.25×10-5). No other comparisons were found to be significant. NYUTron again exhibited \nsuperior AUROC of 0.86 and AUPRC of 0.80 (Fig. S8C and S8D).  \nDischarge mRS prediction with fine-tuned models \nAs shown in Figure 5, after fine-tuning, generative models outperformed encoders for the \ndischarge mRS prediction. For the exact mRS prediction, MedGemma achieved the highest \naccuracy of 42.7% [95% CI, 39.7%-45.7%], followed by Llama at 42.0% [95% CI, \n39.0%-45.0%], NYUTron at 32.9% [95% CI, 30.0%-35.8%], and BERT at 27.4% [95% CI, \n24.8%-30.0%]. All pairwise comparisons between generative and encoder models reached \nstatistical significance (all pairwise P < 9.08×10-8).  \nFor the binary discharge functional outcome accuracy, Llama performed best at 75.0% [95% CI, \n72.4%-77.6%], followed by MedGemma (74.7% [95% CI, 72.1%-77.3%]), NYUTron (74.6% \n[95% CI, 72.0%-77.2%]), and BERT (68.9% [95% CI, 66.1%-71.7%]). BERT performed worse \nthan all other models (all pairwise P < 1.07x10-4), but no other comparisons were significant. In \nline with these findings, Llama and MedGemma achieved higher AUROC and AUPRC (Llama: \nAUROC = 0.84; AUPRC = 0.81; MedGemma: AUROC = 0.84; AUPRC = 0.82) than NYUTron \nand BERT (NYUTron: AUROC = 0.81; AUPRC = 0.79; BERT: AUROC = 0.74; AUPRC = \n0.72) (Fig. S9A and S9B). \n90-day mRS prediction with fine-tuned models \nFor the exact 90-day mRS prediction, Llama achieved the highest accuracy of 33.9% [95% CI, \n27.9%-39.9%], followed by MedGemma at 31.7% [95% CI, 25.5%-37.9%], NYUTron at 25.4% \n[95% CI, 19.8%-31.0%], and BERT at 25.0% [95% CI, 19.2%-30.8%]. Only Llama significantly \noutperforms encoders (P = 3.82x10-2 and P = 4.28x10-2 against BERT and NYUTron, \nrespectively).  \nFor the binary 90-day functional outcome prediction, performance differences were modest and \nno pairwise comparisons between generative and encoder models reached statistical significance. \nNYUTron was highest at 76.8% [95% CI, 71.8%-81.8%], followed by Llama (76.3% [95% CI, \n70.7%-81.9%]), MedGemma (75.9% [95% CI, 70.5%-81.3%]), and BERT (70.1% [95% CI, \n64.1%-76.1%]). However, Llama achieved the highest AUROC and AUPRC (AUROC = 0.87; \nAUPRC = 0.84) (Fig. S9C and S9D). \nError analysis \nTo characterize model behavior across the full mRS spectrum, we examined confusion matrices \nfor each model. The confusion matrix for the best-performing fine-tuned model, Llama, is \npresented in Figure S10. These matrices exhibit a clear diagonal pattern, with 55.0% of \ndischarge and 39.19% of 90-day misclassifications occurring within ±1 mRS level, \ndemonstrating a tendency toward near-neighbor rather than large-magnitude errors.\n \n"}, {"page": 13, "text": "12 \n \nFigure 5. Fine-tuned model comparison for discharge and 90-day mRS prediction. Panels \nA–D show exact accuracy (left column) and binary accuracy (right column; mRS 0–2 vs 3–6) \nwith 95% confidence intervals derived from bootstrapped test-set estimates for four fine-tuned \nmodels evaluated on discharge (Panels A–B) and 90-day (Panels C–D) outcomes. All models \nwere fine-tuned on patient admission H&P notes. Encoder models (BERT and NYUTron) were \nfine-tuned to a depth of 10 layers—identified as the most effective depth across both encoders \nand timepoints—using linear classification heads trained jointly with the fine-tuned encoder. \nGenerative models (Llama and MedGemma-4B) were fine-tuned using LoRA. Shared \nsignificance-group lettering (a–d) above each bar denotes CLD groupings: models that do not \nshare a letter differ significantly (p < 0.05), whereas models sharing at least one letter are not \nsignificantly different. \nAbbreviations: CLD, compact letter display; H&P, History and Physical; LoRA, Low-Rank \nAdaptation; mRS, modified Rankin Scale, RF, random forest \n \n"}, {"page": 14, "text": "13 \nStructured-data baselines comparison \nThe best-performing model—fine-tuned Llama— achieved performance comparable to \nestablished baselines, including the NIHSS+age ordinal-regression model of Zhang et al.4 and \nthe HPI+NIHSS+age binary-classification model of Sung et al19 (Fig. 6). For the exact 90-day \nmRS accuracy, Llama achieved 34.6% [95% CI, 27.9%-41.3%] compared with 28.1% [95% CI, \n21.8%-34%, P = .08] for the proportional-odds baseline of Zhang et al.4 For binary 90-day mRS \naccuracy, Llama again achieved the highest performance (77.1% [95% CI, 70.9%-83.2%]), \nfollowed by the Sung et al. (73.8% [95% CI, 67%-80.4%], P = .85) and the Zhang et al. models4 \n(72.6% [95% CI, 65.9%-78.8%], P = .43).  \nDiscussion \nWe present the first comprehensive evaluation of modern LLMs for fine-grained mRS prediction \nusing unstructured clinical text from early admission. While the majority of past studies were \nlimited to datasets of fewer than 500 patients4,11,34, we employed a dataset of over 6,000 stroke \nadmissions from the Get With The Guidelines–Stroke registry initiative. Unlike prior approaches \nthat relied on structured variables5, manual text feature selection35, or post-discharge data4,11, our \nmethod leverages unmodified H&P notes—routinely created at every admission—to predict both \ndischarge and 90-day functional outcomes. This task has some similarities to but is substantially \ndifferent from mRS identification, where the current score is inferred from the note, rather than \npredicted into the future.35 We demonstrated that text-based models can achieve predictive \nperformance comparable to established structured-data models, offering a practical and scalable \nfoundation for clinical decision support. \nUnlike most prior work, which reports either binary10,11,18,19,36 or one-off accuracy4,37, we \nevaluated both exact and binarized mRS prediction accuracies to better characterize model \nbehavior. In doing so, we observed that relative performance differences between models were \nmore pronounced for exact mRS accuracy than for binary accuracy, reflecting the greater \ndiscriminative resolution of ordinal prediction compared with coarse dichotomization. \nAdditionally, fine-grained 0-6 mRS predictions provide actionable patient-specific prognostic \ndetail for counseling and rehabilitation planning.4,37 Although exact accuracy was modest—as \nexpected for a seven-class ordinal outcome—error analysis revealed that most misclassifications \nfell within ±1 mRS point, indicating near-neighbor rather than clinically implausible errors. \nOur findings extend and clarify mixed results from prior text-based stroke outcomes \nprognostication studies. Heo et al. (2019) concluded that deep learning outperformed traditional \nmachine learning for mRS prediction from MRI reports21, whereas Su et al. (2025) found the \nopposite using admission notes22. Our fine-tuned Llama model achieved numerical \nimprovements over both the proportional-odds model inspired by Zhang et al. (2021)4 and the \nstructured-plus-text approach similar to that of Sung et al. (2021)19. While these differences did \nnot reach statistical significance in our 90-day cohort, the results suggest that early clinical \nnarratives contain sufficient prognostic signals to inform long-term functional outcome \nprediction. Text-based prediction avoids the resource-intensive data abstraction that limits \nstructured-variable approaches13,14, offering a lower-friction path to deployment. \n \n"}, {"page": 15, "text": "14 \n \n \nFigure 6: Comparison of fine-tuned Llama-3.1-8B and baseline models for 90-day mRS \nprediction. Panels A–B present mean predictive performance for three prior approaches: a \nfine-tuned Llama model, the proportional-odds ordinal regression model from Zhang et al., and \nthe structured-plus-text binary classifier from Sung et al. (A) Exact accuracy for 90-day mRS \nprediction comparing Llama with the ordinal regression model; the Sung et al. model is omitted \nfrom this panel because it produces binary-only outputs. (B) Binary accuracy for predicting poor \nfunctional outcome (mRS ≥ 3) across all three models. Error bars denote bootstrapped 95% \nconfidence intervals computed on the 90-day test set restricted to patients with available NIHSS \nand age. \nAbbreviations: mRS, modified Rankin Scale, NIHSS, National Institutes of Health Stroke Scale \nIt is important to note that our implementations of the Sung et al. and Zhang et al. baselines are \nsimilar but not exact replications of the original methods. Sung et al. used the HPI section of the \nadmission note, whereas we approximated this by extracting the first 512 tokens of the H&P \nnote. We also did not apply pre-processing steps, such as spell-correction and acronym expansion \nnor a hyperparameter search. Zhang et al. utilized discharge NIHSS while our implementation \nused the admission value. Unlike their study, we also did not need to upsample our dataset due to \nsufficient size. There is also a natural dataset shift compared to these methods: Sung et al.’s data \noriginated from Taiwanese hospitals, which may have different documentation styles. Zhang et \nal.’s dataset also had specific inclusion criteria such as eligibility for endovascular therapy and a \nbaseline NIHSS above 5, and excluded patients who died during hospitalization. Meanwhile, our \n90-day dataset only consists of patients who underwent thrombectomy or thrombolysis. While \nthe lack of exact weights and differences in methodology and datasets prevent a direct \ncomparison to these studies, our findings suggest that fine-tuned Llama performs at least on par \nwith structured and semi-structured approaches. \nThe relative performance of encoder versus generative architectures differed between frozen and \nfine-tuned conditions. Before fine-tuning, encoders tended to outperform generative models. \nHowever, this comparison is complicated by structural asymmetries: frozen encoders use \n"}, {"page": 16, "text": "15 \ntrainable classifier heads that effectively function as a small task-supervised fine-tuning, whereas \ngenerative models rely on prompt-engineering alone. After fine-tuning, generative models \nachieved the highest exact mRS accuracies, which could reflect their capacity to model \nlong-range dependencies across full H&P notes. Importantly, our encoders were limited to 512 \ntokens, whereas generative models processed sequences up to 8,192. We also did not control for \nthe model parameter size between the encoder and generative models. Future work using \nlong-context encoders such as ModernBERT38 or BigBird39 could clarify whether this \nperformance gap reflects architectural differences, model weight class, or input length \nconstraints.  \nOur results underscore the importance of task-specific pretraining and fine-tuning for clinical \nprediction tasks. In the frozen setting, NYUTron—the only model pretrained on clinical \nnotes—achieved the highest accuracy, outperforming generative models that are an order of \nmagnitude larger. This is consistent with recent findings showing that general-purpose \nfoundation models underperform on EHR-based hospital operations tasks, such as readmission \nand length of stay predictions, without supervised adaptation.23 Notably, for the binary discharge \ntask, the generative models outperformed BERT, suggesting that NYUTron’s advantage stems \nfrom its clinical pretraining rather than its encoder architecture. Furthermore, NYUTron \nremained consistently more accurate than BERT even after fine-tuning, matching generative \nmodels on the binary task, suggesting that domain-specific pretraining on clinical text confers a \ndurable performance advantage that persists even after supervised fine-tuning. \nHowever, we found that parameter-efficient fine-tuning via LoRA can effectively adapt general \nmodels to specialized clinical tasks. Once LoRA-fine-tuned, generative models achieved \ncomparable binary accuracy and surpassed NYUTron in exact mRS prediction. This finding \nsuggests that supervised adaptation can enable general-purpose models to match or exceed the \nperformance of encoders pretrained on institutional EHR data, offering a practical alternative to \ndeveloping and maintaining domain-specific pretrained models that require larger-scale datasets \nand substantial pretraining resources. In this study, we limited our investigation to smaller local \nmodels. As larger HIPAA-compliant generalist foundation models such as GPT-5 become \nincreasingly available across health systems, comparing generalist and clinical specialist systems \nacross tasks remains an important direction for future investigation.23,40,41 \nOur study is not without limitations. As mentioned above, we did not control for the model size \nwhen comparing encoder and generative models. We also did not incorporate imaging reports18, \nwhich have been shown to contribute complementary prognostic information such as infarct \nvolume and location. The choice to use a predetermined temporal split resulted in a limited size \nof the 90-day cohort with complete structured variables, potentially underpowering our \ncomparison against established baselines. Additionally, as the 90-day cohort was limited to \npatients who underwent thrombectomy or thrombolysis and had documented follow-up, it's \nunclear whether our 90-day performance estimates will generalize to the broader stroke \npopulation. Lastly, although mRS is a well-established measure of functional outcome, it remains \na subjective and coarse-grained scale that can introduce labeling variability.42  \nSeveral next steps emerge from our findings and limitations. Future work should pursue external \nvalidation across diverse hospital systems, explicitly evaluate equity and documentation-related \nbiases, and assess robustness to variation in writing style and workflow. Multimodal approaches \n"}, {"page": 17, "text": "16 \n \nintegrating text with imaging41,43–45 or structured variables5 may improve performance, though \ndoing so necessitates careful consideration of admission-time data availability. Comparing \ntrainable local models against HIPAA-compliant frontier models is important to establish the \nbenefits of the biomedical and/or clinical pretraining and fine-tuning for this task.23,40,41,46 \nProspective studies assessing the impact on clinical decision-making and algorithmic fairness \nevaluation are essential before deployment.47 Additionally, the development of lightweight AI \nsystems demonstrates the potential for deploying efficient clinical prediction models directly on \nedge devices, which could further reduce infrastructure barriers and enable real-time \nprognostication at the point of care. Taken together, these directions can help translate text-based \nprognostic models from research settings into real-world clinical environments. \nIn conclusion, we evaluated modern large language models on a real-world clinical prediction \ntask—forecasting post-stroke functional outcomes from admission documentation. Unlike \nstandardized multiple-choice benchmarks commonly used to assess medical AI47–49, this task \nreflects the complexity inherent in clinical prognostication: incomplete information, variable \ndocumentation quality, and outcomes shaped by factors not yet observable at admission. While \nEHR-pretrained NYUTron performed best in the frozen setting, task-specific fine-tuning enabled \ngenerative models to surpass encoders. Our findings demonstrate that meaningful stroke outcome \nprediction can be achieved using routinely documented clinical text alone, supporting the \ndevelopment of prognostic tools that integrate seamlessly into existing clinical workflows \nwithout requiring manual data extraction. \nAcknowledgements: We would like to acknowledge Nader Mherabi, Dafna Bar-Sagi, PhD, and \nPaul Testa, MD for their continued support of medical AI research at NYU. We thank Michael \nConstantino, Kevin Yie, and the NYU Langone High-Performance Computing (HPC) Team for \nsupporting computing resources fundamental to our work. \nSources of Funding: This work was supported by the Institute for Information & \nCommunications Technology Promotion (IITP) grant funded by the Korea government (MSIT) \n(No. RS-2019-II190075 Artificial Intelligence Graduate School Program (KAIST); No. \nRS-2024-00509279, Global AI Frontier Lab). \n \nDisclosures: EKO has equity in Delvi, MarchAI, and Artisight, income from Merck & Co. and \nMirati Therapeutics, employment at Eikon Therapeutics, and consults for Sofinnova Partners and \nGoogle.  \n \nConflict of Interest: None \nSupplemental Material: Figures S1-S10 \n \n"}, {"page": 18, "text": "17 \nReferences \n1.​\nSaver, J. L. et al. Standardized nomenclature for modified Rankin Scale global disability \noutcomes: Consensus recommendations from Stroke Therapy Academic Industry \nRoundtable XI. Stroke 52, 3054–3062 (2021). \n2.​\nRost, N. S. et al. Stroke severity is a crucial predictor of outcome: An international \nprospective validation study. J. Am. Heart Assoc. 5, (2016). \n3.​\nWouters, A., Nysten, C., Thijs, V. & Lemmens, R. Prediction of outcome in patients with \nacute ischemic stroke based on initial severity and improvement in the first 24 h. Front. \nNeurol. 9, 308 (2018). \n4.​\nZhang, M. Y., Mlynash, M., Sainani, K. L., Albers, G. W. & Lansberg, M. G. Ordinal \nprediction model of 90-day modified Rankin Scale in ischemic stroke. Front. Neurol. 12, \n727171 (2021). \n5.​\nKlug, J., Leclerc, G., Dirren, E. & Carrera, E. Machine learning for early dynamic \nprediction of functional outcome after stroke. Commun. Med. (Lond.) 4, 232 (2024). \n6.​\nPaulus, J. K. et al. Field synopsis of the role of sex in stroke prediction models. J. Am. \nHeart Assoc. 5, (2016). \n7.​\nPhan, T. G., Clissold, B. B., Ma, H., Van Ly, J. & Srikanth, V. Predicting disability after \nischemic stroke based on comorbidity index and stroke severity-from the Virtual \nInternational Stroke Trials Archive-acute collaboration. Front. Neurol. 8, 192 (2017). \n8.​\nBonkhoff, A. K. et al. Outcome after acute ischemic stroke is linked to sex-specific lesion \npatterns. Nat. Commun. 12, 3289 (2021). \n9.​\nHe, Q. et al. Predicting a favorable (mRS 0-2) or unfavorable (mRS 3-6) stroke outcome by \narterial spin labeling and amide proton transfer imaging in post-thrombolysis stroke \n"}, {"page": 19, "text": "18 \n \npatients. J. Pers. Med. 13, 248 (2023). \n10.​ Zhang, Q., Yang, Y. & Saver, J. L. Discharge destination after acute hospitalization strongly \npredicts three month disability outcome in ischemic stroke. Restor. Neurol. Neurosci. 33, \n771–775 (2015). \n11.​ ElHabr, A. K. et al. Predicting 90-day modified Rankin Scale score with discharge \ninformation in acute ischaemic stroke patients following treatment. BMJ Neurol. Open 3, \ne000177 (2021). \n12.​ Qureshi, A. I., Chaudhry, S. A., Sapkota, B. L., Rodriguez, G. J. & Suri, M. F. K. Discharge \ndestination as a surrogate for Modified Rankin Scale defined outcomes at 3- and 12-months \npoststroke among stroke survivors. Arch. Phys. Med. Rehabil. 93, 1408–1413.e1 (2012). \n13.​ Capurro, D., Yetisgen, M., van Eaton, E., Black, R. & Tarczy-Hornoch, P. Availability of \nstructured and unstructured clinical data for comparative effectiveness research and quality \nimprovement: a multisite assessment. EGEMS (Wash., DC) 2, 1079 (2014). \n14.​ Jiang, L. Y. et al. Health system-scale language models are all-purpose prediction engines. \nNature 619, 357–362 (2023). \n15.​ Thompson, M. P. & Reeves, M. Abstract 168: Assessing the utility of the modified Rankin \nScale (mRS) at discharge to predict day 90 outcomes in acute stroke registries. Circ. \nCardiovasc. Qual. Outcomes 5, (2012). \n16.​ Ovbiagele, B. & Saver, J. L. Day-90 acute ischemic stroke outcomes can be derived from \nearly functional activity level. Cerebrovasc. Dis. 29, 50–56 (2010). \n17.​ Ovbiagele, B., Lyden, P. D., Saver, J. L. & VISTA Collaborators. Disability status at 1 \nmonth is a reliable proxy for final ischemic stroke outcome. Neurology 75, 688–692 (2010). \n18.​ Sung, S.-F., Hsieh, C.-Y. & Hu, Y.-H. Early prediction of functional outcomes after acute \n"}, {"page": 20, "text": "19 \nischemic stroke using unstructured clinical text: Retrospective cohort study. JMIR Med. \nInform. 10, e29806 (2022). \n19.​ Sung, S.-F., Chen, C.-H., Pan, R.-C., Hu, Y.-H. & Jeng, J.-S. Natural language processing \nenhances prediction of functional outcome after acute ischemic stroke. J. Am. Heart Assoc. \n10, e023486 (2021). \n20.​ Guo, Y. et al. Novel survival features generated by clinical text information and radiomics \nfeatures may improve the prediction of ischemic stroke outcome. Diagnostics (Basel) 12, \n1664 (2022). \n21.​ Heo, J. et al. Machine learning-based model for prediction of outcomes in acute stroke. \nStroke 50, 1263–1265 (2019). \n22.​ Su, Y.-H. & Tsai, C.-F. Predicting functional outcomes after a stroke event by clinical text \nnotes: A comparative study of traditional machine learning and deep learning methods. \nHealth Informatics J. 31, 14604582251381194 (2025). \n23.​ Jiang, L. Y. et al. Generalist foundation models are not clinical enough for hospital \noperations. arXiv [cs.CL] (2025) doi:10.48550/arXiv.2511.13703. \n24.​ Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional \nTransformers for language understanding. arXiv [cs.CL] (2018). \n25.​ Brown, T. B. et al. Language Models are Few-Shot Learners. arXiv [cs.CL] (2020). \n26.​ Fonarow, G. C. et al. Characteristics, performance measures, and in-hospital outcomes of \nthe first one million stroke and transient ischemic attack admissions in get with the \nguidelines-stroke. Circ. Cardiovasc. Qual. Outcomes 3, 291–302 (2010). \n27.​ Xian, Y. et al. Twenty years of sustained improvement in quality of care and outcomes for \npatients hospitalized with stroke or transient ischemic attack: Data from The Get With The \n"}, {"page": 21, "text": "20 \n \nGuidelines-stroke program. Stroke 55, 2599–2610 (2024). \n28.​ Grattafiori, A. et al. The Llama 3 herd of models. arXiv [cs.AI] (2024). \n29.​ Sellergren, A. et al. MedGemma Technical Report. arXiv [cs.AI] (2025). \n30.​ Gallifant, J. et al. The TRIPOD-LLM reporting guideline for studies using large language \nmodels. Nat. Med. 31, 60–69 (2025). \n31.​ Reeves, M. J. et al. Twenty years of Get With The Guidelines-stroke: Celebrating past \nsuccesses, lessons learned, and future challenges. Stroke 55, 1689–1698 (2024). \n32.​ Han, X. et al. Neuro Data Hub: A new approach for streamlining medical clinical research. \nNeurosurg. Pract. 6, e000162 (2025). \n33.​ Hu, E. J. et al. LoRA: Low-Rank Adaptation of large language models. arXiv [cs.CL] \n(2021). \n34.​ Samak, Z. A., Clatworthy, P. & Mirmehdi, M. Automatic prediction of stroke treatment \noutcomes: latest advances and perspectives. Biomed. Eng. Lett. 15, 467–488 (2025). \n35.​ Silva, L. et al. Assessment of the modified Rankin scale in electronic health records with a \nfine-tuned large language model. medRxiv (2025) doi:10.1101/2025.04.30.25326777. \n36.​ Heo, T. S. et al. Prediction of stroke outcome using natural language processing-based \nmachine learning of radiology report of brain MRI. J. Pers. Med. 10, 286 (2020). \n37.​ Stinear, C. M., Smith, M.-C. & Byblow, W. D. Prediction tools for stroke rehabilitation. \nStroke 50, 3314–3322 (2019). \n38.​ Finally, a Replacement for BERT. https://huggingface.co/blog/modernbert. \n39.​ BigBird. https://huggingface.co/docs/transformers/en/model_doc/big_bird. \n40.​ Vishwanath, K. et al. Generalist large language models outperform clinical tools on medical \nbenchmarks. arXiv [cs.CL] (2025) doi:10.48550/arXiv.2512.01191. \n"}, {"page": 22, "text": "21 \n41.​ Alyakin, A. et al. CNS-Obsidian: A Neurosurgical Vision-Language Model Built From \nScientific Publications. arXiv [cs.AI] (2025). \n42.​ Wilson, J. T. L. et al. Reliability of the modified Rankin Scale across multiple raters: \nbenefits of a structured interview: Benefits of a structured interview. Stroke 36, 777–781 \n(2005). \n43.​ Erdoğan, M. Ş. et al. Predicting modified Rankin scale scores of ischemic stroke patients \nusing radiomics features and machine learning. in Advances in Intelligent Systems and \nComputing 204–213 (Springer Nature Switzerland, Cham, 2024). \n44.​ Alyakin, A. et al. CNS-CLIP: Transforming a neurosurgical journal into a multimodal \nmedical model. Neurosurgery (2024) doi:10.1227/neu.0000000000003297. \n45.​ Kondepudi, A. et al. Health system learning achieves generalist neuroimaging models. \narXiv [cs.CV] (2025) doi:10.48550/arXiv.2511.18640. \n46.​ Pedro, T. et al. Exploring the use of ChatGPT in predicting anterior circulation stroke \nfunctional outcomes after mechanical thrombectomy: a pilot study. J. Neurointerv. Surg. 17, \n261–265 (2025). \n47.​ Chen, S. F. et al. LLM-assisted systematic review of large language models in clinical \nmedicine. Nat. Med. (2026). \n48.​ Singh, S. et al. The pitfalls of multiple-choice questions in generative AI and medical \neducation. Sci. Rep. 15, 42096 (2025). \n49.​ Vishwanath, K. et al. Medical large language models are easily distracted. arXiv [cs.CL] \n(2025). \n \n"}, {"page": 23, "text": "22 \n \nSupplemental Figures \n \n \nFigure S1. Distribution of H&P notes per patient at discharge and 90-day follow-up. Panels \nA and B summarize the number of notes per patient in the discharge cohort (A) and the subset \nwith available 90-day follow-up (B). \nAbbreviations: H&P, History and Physical\n \n"}, {"page": 24, "text": "23 \n \nFigure S2. Distribution of H&P token lengths across all admission notes. Token lengths were \ncomputed using the BERT WordPiece tokenizer. Dashed vertical lines indicate the 512-token \nlimit of encoder models and the 8192-token context window typical of generative LLMs. \nAbbreviations: H&P, History and Physical\n \n"}, {"page": 25, "text": "24 \n \nFigure S3. Comparison of input strategies for NYUTron and BERT encoders. Panels A–B \nshow mean accuracy for frozen NYUTron (Panel A) and frozen BERT (Panel B) embeddings \nderived using three input strategies: the first 512 tokens of the admission H&P note, the last 512 \ntokens, and a Llama-generated clinical summary. All models use a random forest classifier \ntrained on frozen encoder embeddings. Error bars indicate 95% confidence intervals obtained via \n10-fold cross-validation on the training set. \nAbbreviations: BERT, Bidirectional Encoder Representations from Transformers; H&P, History \nand Physical; LLM, large language model. \n"}, {"page": 26, "text": "25 \n \nFigure S4. Comparison of classifier heads for NYUTron and BERT encoders. Panels A–D \nshow mean accuracy for four downstream classifier heads—logistic regression (LogReg), \nrandom forest (RF; 500 trees, max depth = 7), linear regression (Linear; predictions rounded and \nclipped to the valid mRS range 0–6), and CORAL ordinal regression—trained on frozen \nembeddings extracted from admission H&P notes. Panels A–B present discharge accuracy for \nNYUTron (Panel A) and BERT (Panel B), while Panels C–D present 90-day accuracy for \nNYUTron (Panel C) and BERT (Panel D). Error bars indicate 95% confidence intervals obtained \nvia 10-fold cross-validation on the training set. \nAbbreviations: BERT, Bidirectional Encoder Representations from Transformers; CORAL, \nCOntinuous RAnked Logits; H&P, History and Physical; mRS, modified Rankin Scale; LogReg, \nLogistic Regression; RF, Random Forest\n \n"}, {"page": 27, "text": "26 \n \n \nFigure S5. Fine-tuning depth comparison for NYUTron and BERT encoders. Panels A–D \nshow mean accuracy for NYUTron (Panels A, C) and BERT (Panels B, D) when fine-tuned to \ndifferent depths. Models were fine-tuned by unfreezing between 2 and 12 transformer layers. For \neach depth, the encoder’s [CLS] representation was passed through a jointly trained linear \nregression head to predict continuous mRS scores (0–6), optimized using MAE loss. Depth \n10—identified as the optimal fine-tuning depth across both encoders and timepoints—is \nhighlighted in dark purple. Panels A–B report discharge accuracy for NYUTron and BERT, \nrespectively, and Panels C–D report 90-day accuracy. Error bars indicate 95% confidence \nintervals obtained via 10-fold cross-validation on the training set. \nAbbreviations: BERT, Bidirectional Encoder Representations from Transformers; CLS, \nclassification token; MAE, mean absolute error; mRS, modified Rankin Scale\n \n"}, {"page": 28, "text": "27 \n \nFigure S6. Prompting strategy comparison for zero-shot mRS prediction. Two prompting \nstrategies were evaluated for zero-shot discharge mRS prediction using generative models \n(Llama and MedGemma). Prompt A provides a concise instruction asking the model to output a \nsingle integer mRS score (0–6) based solely on the H&P note. Prompt B augments this \ninstruction by including the full mRS definitions (0–6) before requesting the prediction. Both \nprompts explicitly require the model to return only the integer label with no additional text. As \nthere was no significant difference between prompts, Prompt B was used.  \nAbbreviations: H&P, History and Physical; mRS, modified Rankin Scale\n \n"}, {"page": 29, "text": "28 \n \n \n \nFigure S7. Comparison of zero-shot prompt designs for MedGemma and Llama. Panels \nA–D show zero-shot accuracy for MedGemma (Panels A and C) and Llama (Panels B and D) on \ndischarge (Panels A–B) and 90-day (Panels C–D) mRS prediction using two prompt variants. \nPrompt A instructed the model only to predict the mRS score, whereas Prompt B additionally \nprovided the mRS category definitions. Error bars indicate 95% confidence intervals obtained \nvia bootstrapping. Results throughout the study are reported using Prompt B. \nAbbreviations: mRS, modified Rankin Scale \n"}, {"page": 30, "text": "29 \n \nFigure S8. Discrimination performance of frozen encoder models for binary mRS \nprediction. Panels A–D show receiver operating characteristic (ROC) curves (Panels A and C) \nand precision–recall (PR) curves (Panels B and D) for frozen NYUTron and BERT models in \npredicting unfavorable functional outcome (mRS ≥ 3) at hospital discharge (Panels A–B) and 90 \ndays post-discharge (Panels C–D). Continuous confidence scores for binary mRS ≥ 3 \nclassification were obtained from random forest classifiers trained on frozen encoder \nembeddings. Legends report area-under-the-curve metrics for each model. Zero-shot prompted \ndecoder models (Llama and MedGemma) were not included, as they produced only discrete \nmRS predictions without continuous confidence scores, precluding ROC and precision–recall \nanalysis. \nAbbreviations: AUC, area under the curve; mRS, modified Rankin Scale; PR, precision–recall; \nROC, receiver operating characteristic\n \n"}, {"page": 31, "text": "30 \n \n \nFigure S9. Discrimination performance of fine-tuned encoder and decoder models for \nbinary mRS prediction. Panels A–D show receiver operating characteristic (ROC) curves \n(Panels A and C) and precision–recall (PR) curves (Panels B and D) for fine-tuned NYUTron, \nBERT, MedGemma, and Llama models in predicting unfavorable functional outcome (mRS ≥ 3) \nat hospital discharge (Panels A–B) and 90 days post-discharge (Panels C–D). Continuous \nconfidence scores for binary mRS ≥ 3 classification were obtained from fine-tuned model \noutputs; for encoder models, confidence scores were derived from regression outputs, while \ndecoder model scores were obtained by aggregating predicted class probabilities for mRS ≥ 3. \nLegends report area-under-the-curve metrics for each model. \nAbbreviations: AUC, area under the curve; mRS, modified Rankin Scale; PR, precision–recall; \nROC, receiver operating characteristic\n \n"}, {"page": 32, "text": "31 \n \nFigure S10. Normalized confusion matrices for the best-performing model, fine-tuned \nLlama-3.1-8B, at discharge and 90-day follow-up. Panels A and B show normalized confusion \nmatrices for discharge (Panel A) and 90-day (Panel B) mRS prediction. Each matrix displays \nper-class prediction accuracy for mRS scores (0–6), normalized by the true label distribution. \nAbbreviations: mRS, modified Rankin Scale \n \n"}]}