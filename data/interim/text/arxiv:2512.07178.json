{"doc_id": "arxiv:2512.07178", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.07178.pdf", "meta": {"doc_id": "arxiv:2512.07178", "source": "arxiv", "arxiv_id": "2512.07178", "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "published": "2025-12-08T05:18:15Z", "updated": "2025-12-08T05:18:15Z", "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.07178v1", "url_pdf": "https://arxiv.org/pdf/2512.07178.pdf", "meta_path": "data/raw/arxiv/meta/2512.07178.json", "sha256": "a342524ded6b40d47edaa28dc8dbc56339eba10628e89d794f12f304eb74e83e", "status": "ok", "fetched_at": "2026-02-18T02:24:50.184000+00:00"}, "pages": [{"page": 1, "text": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language\nGeneration\nLATIFA DWIYANTI, Kanazawa University, Japan and Institut Teknologi Bandung, Indonesia\nSERGIO RYAN WIBISONO, Institut Teknologi Bandung, Indonesia\nHIDETAKA NAMBO, Kanazawa University, Japan\nExplainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning\nmodels are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained\nprominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP\neffectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those\nwithout technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large\nlanguage model (LLM), specifically OpenAIâ€™s GPT, to generate contextualized textual explanations. This integration is guided by\nuser-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the modelâ€™s\ncontext and the userâ€™s perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP\nexplanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted\nuser evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the\ngenerated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While\nthe findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and\ntrustworthy model explanations.\nAdditional Key Words and Phrases: XAI, SHAP, Context-Aware Explanation, LLM, OpenAI GPT, Human-Centered AI, Perceived\nUnderstandability\n1\nIntroduction\nAs noted by Molnar [1], interpretability plays a crucial role in encouraging the adoption of machine learning (ML) and\nartificial intelligence (AI), since transparency helps build trust and supports deployment in high-stakes domains. In\nline with the goals of AI interpretability, DARPA launched the Explainable AI (XAI) program in 2015 to help users\nunderstand, trust and manage AI systems more effectively [2]. In 2021, they published a comprehensive summary that\nsummarizes the programâ€™s goals, structure, and research progress, further amplifying interest in XAI and inspiring\nwidespread research in the field.\nUsing the open API of the Semantic Scholar platform, which leverages AI through the Semantic Scholar Academic\nGraph (S2AG) to improve relevance in academic searches [3], the author conducted a query to retrieve articles related\nto XAI. A total of 5,544 papers were retrieved using three filtering parameters: the keyword \"XAI,\" publication years\nbetween 2020 and 2024, and inclusion in the field of Computer Science. The yearly distribution of these publications is\nshown in Figure 1, highlighting the continued growth and increasing interest in this research area. Currently, there is\nno universally accepted definition of XAI. The term is often used interchangeably with others such as Interpretable\nMachine Learning, Reasonable AI, or Understandable AI [4]. The goals of XAI can vary widely, as outlined by Arrieta\net al. in their comprehensive review [5], ranging from enhancing trustworthiness, causality, and transferability to\npromoting informativeness, fairness, interactivity, and awareness of privacy. Among these, the goal most consistently\nAuthorsâ€™ Contact Information: Latifa Dwiyanti, latifa@stu.kanazawa-u.ac.id, Kanazawa University, Kanazawa, Ishikawa, Japan and Institut Teknologi\nBandung, Bandung, Indonesia; Sergio Ryan Wibisono, Institut Teknologi Bandung, Bandung, Indonesia; Hidetaka Nambo, Kanazawa University, Kanazawa,\nIshikawa, Japan.\narXiv:2512.07178v1  [cs.AI]  8 Dec 2025\n"}, {"page": 2, "text": "Fig. 1. XAI Growing Papers\nemphasized in all studies is informativeness, the idea that explanations should provide users with sufficient information\nto support decision making [5]. Arun Rai beautifully captures the essence of XAI by likening it to transforming a â€™black\nboxâ€™ into a â€™glass boxâ€™, allowing users to understand the rationale behind AI predictions [6].\nXAI has been applied in a wide range of domains, including medicine, healthcare, law, civil engineering, marketing,\neducation, cybersecurity, transportation, and agriculture [7, 8]. However, several challenges persist, such as the trade-off\nbetween explainability and model performance, varying human interpretations, the absence of universal standards,\nbiased data leading to unfair outcomes, and the lack of consensus on how to evaluate XAI methods [7]. To address\nhuman-centric challengesâ€”particularly users with varying abilities to understand explanationsâ€”recent efforts have\nfocused on developing context-aware XAI, which generates explanations tailored to both the model and the userâ€™s\ncontext [8]. This paper aims to contribute to this direction by introducing a tool for AI researchers to develop models\nthat integrate XAI principles while enhancing the contextual relevance of their explanations.\nThe structure of this paper is as follows: an introduction, a review of related work, a detailed explanation of our\nPython library contextualSHAP, followed by a case study and evaluation.\n2\nRelated Works\n2.1\nSHAP (Shapley Additive exPlanation)\nIn general, machine learning interpretability methods can be categorized into two types. The first involves models that\nare inherently interpretable, while the second refers to post hoc interpretability, where explanations are generated after\nthe model has been trained [1]. Post hoc interpretability is further divided into two categories: model-agnostic and\nmodel-specific approaches. Model-agnostic methods can be applied to any machine learning model, regardless of its\ninternal structure or processing mechanisms, whereas model-specific methods are designed to explain particular types\nof models [5].\nSHAP is among the most commonly used model-agnostic approaches [7, 9â€“11]. It enables visualization of the\nrationale behind model predictions by illustrating feature interactions and importance, both locally and globally, and\ncan be applied to any model or data type [11]. Introduced in 2017, SHAP is based on Lloyd Shapleyâ€™s 1953 concept of\n2\n"}, {"page": 3, "text": "Shapley Values, which quantify a playerâ€™s average marginal contribution across all possible coalitions [7, 12]. Equation\n(1) is the classic Shapley value estimation:\nğœ™ğ‘–=\nâˆ‘ï¸\nğ‘†âŠ†ğ¹\\{ğ‘–}\n|ğ‘†|!(|ğ¹| âˆ’|ğ‘†| âˆ’1)!\n|ğ¹|!\n\u0002\nğ‘“ğ‘†âˆª{ğ‘–} (ğ‘¥ğ‘†âˆª{ğ‘–}) âˆ’ğ‘“ğ‘†(ğ‘¥ğ‘†)\n\u0003\n(1)\nLegend:\nğœ™ğ‘–: The Shapley value for feature ğ‘–, representing its\noverall contribution to the prediction.\nğ¹: The set of all features.\nğ‘†: A subset of features not including ğ‘–.\nğ‘“ğ‘†âˆª{ğ‘–} (ğ‘¥ğ‘†âˆª{ğ‘–}): The model prediction when feature ğ‘–\nis added to subset ğ‘†.\nğ‘“ğ‘†(ğ‘¥ğ‘†): The model prediction using only the subset ğ‘†.\nThis equation forms the theoretical foundation for SHAP values used in model explanations. However, computing\nexact Shapley values is computationally expensive, especially for models with many features. To address this, Lundberg\net al [13] proposed TreeExplainer, a specialized SHAP implementation for tree-based models such as decision trees,\nrandom forests, and gradient-boosted trees. TreeExplainer exploits the internal structure of trees to compute exact\nShapley values efficiently, reducing computational complexity to low-order polynomial time [13].\nAlgorithm 1 presents a simplified version of the TreeSHAP algorithm, which computes SHAP values under *inter-\nventional semantics*. Specifically, it estimates the interventional expectation E[ğ‘“(ğ‘‹) | ğ‘‘ğ‘œ(ğ‘‹ğ‘†= ğ‘¥ğ‘†)], where ğ‘“(ğ‘‹) is\nthe model output and ğ‘‘ğ‘œ(ğ‘‹ğ‘†= ğ‘¥ğ‘†) denotes an intervention on the feature subset ğ‘‹ğ‘†that breaks dependencies with\nother features [13]. This approach differs from the observational conditional expectation E[ğ‘“(ğ‘‹) | ğ‘‹ğ‘†= ğ‘¥ğ‘†], which\npreserves the natural dependencies between features.\nAlgorithm 1 Estimating ğ¸[ğ‘“(ğ‘‹) | ğ‘‘ğ‘œ(ğ‘‹ğ‘†= ğ‘¥ğ‘†)]\n1: procedure EXPVALUE(ğ‘¥,ğ‘†, tree = (ğ‘,ğ‘,ğ‘¡,ğ‘Ÿ,ğ‘‘))\n2:\nprocedure G(ğ‘—)\n3:\nif ğ‘—is internal then\n4:\nreturn ğ‘Ÿğ‘—\n5:\nelse\n6:\nif ğ‘‘ğ‘—âˆˆğ‘†then\n7:\nreturn ğº(ğ‘ğ‘—) if ğ‘¥ğ‘‘ğ‘—= ğ‘¡ğ‘—else ğº(ğ‘ğ‘—)\n8:\nelse\n9:\nreturn ğº(ğ‘ğ‘—) Â· ğ‘¡ğ‘ğ‘—+ ğº(ğ‘ğ‘—) Â· ğ‘¡ğ‘ğ‘—\n10:\nend if\n11:\nend if\n12:\nend procedure\n13:\nreturn ğº(1)\n14: end procedure\nLegend:\nğ‘¥: The input instance (a complete set of feature values).\nğ‘†: The subset of features being conditioned on (i.e.,\nfeatures whose values are known).\nğ‘ğ‘—,ğ‘ğ‘—: The left and right children of node ğ‘—in the\nfollowing component.\nğ‘¡ğ‘—: The fraction of data reaching node ğ‘—that goes into\nchild ğ‘ğ‘—.\nğ‘Ÿğ‘—: The output of node ğ‘—if it is a leaf.\nğ‘‘ğ‘—: The feature index used for splitting at node ğ‘—.\n2.2\nGenerative Language Model\nThe effort to enable machines to communicate naturally with humans has been ongoing for decades. Early advancements\nin natural language processing (NLP) relied heavily on rule-based systems [14].\nEarly systems were resource-intensive and poorly scalable, motivating the development of pretrained language\nmodels (PLMs). While PLMs improved performance by leveraging large corpora, they often required task-specific\nfine-tuning. Large language models (LLMs) addressed this by enabling strong zero-shot and few-shot performance,\nwith GPT-3 marking a key milestone [15].\nLLMs are based on the Transformer architecture, which uses self-attention to capture long-range dependencies, and\nare pretrained with self-supervised learning (SSL) through causal language modeling [14â€“16]. Subsequent variants\n(InstructGPT, ChatGPT, GPT-4) incorporated reinforcement learning from human feedback (RLHF) to enhance alignment\n3\n"}, {"page": 4, "text": "with human intent [17]. The GPT family has since shown substantial advances in natural language generation, with\nGPT-3 establishing the foundation for human-like text generation [15, 16].\n2.3\nUnderstandability of Explanation\nAs mentioned in the introduction, there is no universally accepted definition of XAI, nor is there a rigid framework for\nevaluating it. At its core, an explanation generated by an XAI approach serves as an interface between humans and the\nsystem, aiming to accurately reflect how a model makes decisions while remaining accessible and understandable to\nusers [18].\nTo fulfill XAIâ€™s first goalâ€”describing the reasoning behind a modelâ€™s decisionâ€”SHAP offers a partial solution by\nproviding both global and local explanations of feature importance. It visualizes how specific features contribute to the\nmodelâ€™s output and enables users to see their correlation with the target variable. SHAP operates through a method\nsimilar to partial dependence, treating the predictive model as a black box and observing how variations in input\nfeatures influence predictions [19].\nHowever, addressing the second goalâ€”making explanations understandable to usersâ€”remains more complex and\nunderexplored. Some studies argue that understandability is mainly intended for developers, who need to interpret and\nfine-tune models [20, 21]. But when it comes to end users, there is no standard metric for measuring how well they\ncomprehend model explanations.\nTo begin, we must consider what â€œunderstandingâ€ actually means. This question goes beyond computer science and\nXAI should be approached as a human-agent interaction problem, intersecting artificial intelligence, social sciences,\nand human-computer interaction [21]. The concept of understanding itself can vary. In this paper, we adopt the view\nthat understanding is inseparable from the process of explanation. Drawing on the works of Salmon, Strevens, and\nKhalifa, we highlight a shared view that scientific understanding fundamentally relies on the ability to construct and\nassess explanations [22].\nIn the context of XAI, especially when evaluating user comprehension, perceived understandability becomes a\ncrucial concept. It is inherently subjective, depending on each userâ€™s individual perception. Sindra Naveed defines\nperceived understandability as a userâ€™s subjective evaluation of how well they grasp an explanation and how that\nexplanation contributes to their broader understanding of the systemâ€™s functionality [23]. In our previous research,\nwe compared user comprehension across three types of visualizations: (1) a baseline display showing only the input\ndata and classification result, without XAI; (2) a display showing only LIME or SHAP visualizations; and (3) a display\nintegrating the visual explanation with a summary text generated by ChatGPT. Based on a Likert-scale survey measuring\nperceived understandability, we found that users strongly preferred the combined visual-text explanation. The basic\nnon-XAI output scored the lowest, while the integrated visualization and textual explanation significantly enhanced\nuser comprehension [24]. These findings are consistent with the cognitive theory of multimedia learning, which\nsuggests that presentations combining words and images are more effective for understanding than those using only\none modality [25].\n3\nContextualSHAP\nMotivated by the potential of combining SHAP visualizations with generative text capabilities offered by ChatGPT, we\ndeveloped a Python library named ContextualSHAP (https://github.com/latifadwi/contextualshap). Initial experiments\nusing SHAP outputs directly as prompts in ChatGPT revealed limitations in contextual understanding, often resulting\n4\n"}, {"page": 5, "text": "in inaccurate or misleading summaries. Therefore, the library was designed to help developers generate more context-\naware explanations by offering an easy-to-use, off-the-shelf tool. We hypothesize that enhancing contextual relevance\nin explanations can lead to improved user comprehension.\nThe â€™contextualSHAPâ€™ package extends the functionality of the widely adopted SHAP library, which is primarily\nused to provide explainability for AI models as part of the broader field of XAI. While the original SHAP library\noffers robust tools for calculating and visualizing SHAP values, contextualSHAP enhances these capabilities by inte-\ngrating contextualized natural language explanations through ChatGPT using the OpenAI API. In its initial release,\ncontextualSHAP leverages three key components from the public SHAP API [26]: shap_values, which retrieves shapley\nvalues generated by a SHAP explainer; shap.plots.bar,which produces a bar plot representing a set of SHAP values;\nand shap.plots.waterfall, to visualizes the explanation of a single prediction using a waterfall plot. These components\nare wrapped into higher-level utility functions within contextualSHAP to enrich the context provided to ChatGPT,\nensuring the resulting explanations are more accurate and user-friendly as depict in Figure 2. The package currently\nsupports the following three core functions.\nFig. 2. ContextualSHAP\nTo provide richer contextual information and enhance the quality of the ChatGPT-generated explanations, the\ncontextualSHAP functions are equipped with five optional parameters. These inputs serve as complementary additions\nto the SHAP-generated visualizations and help generated explanations more comprehensive, accurate, and user-relevant.\n(1) Feature renaming (feature_aliases) Feature names inherited from raw datasets or preprocessing are often\ncryptic or domain-irrelevant. Prior work emphasizes the need for domain-relevant terminology in explanations\n[27]. This parameter enables mapping to meaningful aliases, improving the readability of visual and textual\noutputs.\n(2) Feature description (feature_descriptions) Numerical encoding of categorical variables (e.g., sex â†’0/1)\ncan obscure their meaning. This parameter provides semantic context for such encodings and clarifies feature\nunits or measurement scales (e.g., height in cm vs. ft), ensuring more interpretable explanations.\n(3) Background context (additional_background) Model explanations are more useful when grounded in\napplication context. This parameter allows users to specify the modelâ€™s purpose or task, guiding ChatGPT\ntoward contextually relevant explanations.\n(4) Language choices (language) Explanations are more accessible in a userâ€™s native language [28]. This parameter\nspecifies the preferred output language, broadening accessibility across user groups.\n(5) Reader level (reader) Explanations are tailored to audience expertise. Two levels are supported:\n5\n"}, {"page": 6, "text": "(a) General: Non-technical users, with emphasis on intuitive, everyday language.\n(b) Expert: Domain specialists, emphasizing critical assessment of data, methods, and results.\nPrompts are sent in Markdown format to ChatGPT in the background with enough data to ask ChatGPT to return an\nexplanation that is appropriate for the user. The full text prompts can be found in the package repository, however they\ncontain a few basic aspects needed for the functions provided by the package to work:\n(1) A tabular list of feature names, aliases, and descriptions. These are sent as part of the prompt to let ChatGPT\nunderstand the available feature names. When feature aliases/descriptions are not provided, ChatGPT is told\nto interpret them themselves. ChatGPT has been shown to be able to interpret MedInc as Median Income or\nLat/Long as Latitude/Longitude given the proper background context supplied through additional_background\nparameter. However, it is still recommended to add per-feature descriptions in addition to background to\nimprove clarity.\n(2) A limited list of per-feature SHAP values. Although the user of the package can generate multiple SHAP values\nfor multiple instances of model predictions, only limited number of samples can be used. This is because\nChatGPT imposes token limit for each call. SHAP values are presented as a table embedded within the prompt\nto let ChatGPT understand the numbers without having to read them through images/plots.\n(3) Plot images. The plot images (waterfall or bar) are also included in the prompt. ChatGPT is also explained\nwhether the image is a waterfall plot of a single instance of prediction (a single SHAP value per feature) or a\nbar plot containing an average SHAP value for each feature.\n(4) Additional prompt to format the response. ChatGPT is told to return the response in the exact format to be\nparsed programmatically.\nIn addition to the contextual and technical parameters, we embedded a â€œguard promptâ€ within the contextualshap\npackage. This prompt acts as a safeguard to ensure that the generative explanation provided by ChatGPT remains\nwithin appropriate boundaries and avoids potential misinterpretations or overconfident claims.\nThe first purpose of guard prompt is to clarify the nature of predictions. The prompt explicitly reminds the model\nthat it is generating explanations based on predictions from a statistical or machine learning model, not deterministic\nconclusions. This is especially important in sensitive domains such as healthcare, where outputs from a model (e.g.,\ndisease risk predictions) must not be treated as definitive diagnoses. Users are advised to consult with a qualified\nprofessional (e.g., a doctor) to interpret any predictive results correctly and responsibly.\nThe second one, to ensure accurate interpretation of SHAP-specific symbols. During the development and early\nexperimentation with contextualshap, we found that ChatGPT occasionally misinterpreted elements of SHAP plots,\nparticularly the waterfall plot. One recurring issue was confusion between the symbols ğ¸[ğ‘“(ğ‘‹)] and ğ‘“(ğ‘¥). While\nğ¸[ğ‘“(ğ‘‹)] represents the expected prediction value, i.e., the average model output across the entire dataset, the ğ‘“(ğ‘¥) is the\nmodelâ€™s prediction for the specific instance being explained. This kind of confusion can mislead users and compromise\nthe reliability of the explanation. To mitigate this, the guard prompt provides explicit definitions of key symbols and\nconcepts before generating the final explanation.\n4\nImplementation and Validation\nTo validate the proposed package, we applied it to a real-world case study in the healthcare domain. This domain is\nparticularly appropriate due to its high sensitivity, prediction outcomes can directly influence a patientâ€™s understanding\nof their health condition. In such scenarios, explanations must go beyond technical accuracy to ensure they are\n6\n"}, {"page": 7, "text": "understandable and trustworthy. Moreover, medical datasets often involve specialized terminology, reinforcing the\nneed for contextualized and domain-aware explanations.\n4.1\nImplementation : Liver Diseases Classification\nWe used the HCV dataset from the UCI Machine Learning Repository. This dataset contains laboratory test results and\ndemographic data from blood donors and patients with Hepatitis C [29]. The goal of the model is to predict whether an\nindividual has liver disease based on these features.\nThe raw dataset includes 615 instances and 12 input features, with a target column labeled Category. The Category\ncolumn includes five values: 0 = Blood Donor (no liver disease), 0s = Suspect Blood Donor (requires further medical\nassessment), 1 = Hepatitis, 2 = Fibrosis, 3 = Cirrhosis. We grouped the 1, 2, and 3 labels under a single class called\n\"Liver Disease\". Entries labeled 0s were excluded, as they represent uncertain cases requiring expert review. We also\nremoved rows with missing values. After preprocessing, the dataset consisted of 608 complete records. The final class\ndistribution was imbalanced, with 90.4% labeled as healthy and 9.6% as liver disease cases. We trained the dataset using\nXGBoost, which achieved an F1 score of 0.78.\nNext, we interpreted the trained model using TreeSHAP, a SHAP-based method tailored for tree-based models. SHAP\nvalues were visualized using shap.plots.bar and shap.plots.waterfall. These plots were then passed to ChatGPT with a\nsimple prompt: â€œExplain this SHAP visualization.â€ The result depict in Table 1\nTable 1. Image and Explanation without ContextualSHAP\nThis SHAP plot shows feature importance based on their average impact on the modelâ€™s predictions:\nâ€¢ AST, ALT, and GGT have the highest influence.\nâ€¢ ALP and BIL have moderate influence.\nâ€¢\nThe rest of the features contribute very little.\nâ€¢ AST is the top feature, with a mean SHAP value of 0.06.\nâ€¢\nSome features like ALB have almost no impact.\nIn short: the model relies most on a few top features, especially AST\nThis SHAP plot shows how each feature influenced the modelâ€™s prediction of 0.76 (starting from a base of\n0.131):\nâ€¢\nTop positive contributors:\nâ€“ AST (324): +0.3\nâ€“\nGGT (392.2): +0.2\nâ€“\nBIL (67): +0.1\nâ€¢\nTop negative contributors:\nâ€“ ALT (27.8):-0.03\nâ€“\nCREA (97.7): -0.02\nRed means features pushed the prediction higher, blue means they pulled it lower.\nWe then applied the contextualSHAP package to the trained model. As a first step, we defined the additional context\nby specifying the feature_aliases, feature_descriptions, and additional_background parameters. With these contextual\ninputs configured, we utilized the contextualshap.plots.bar and contextualshap.plots.waterfall functions, setting the\nlanguage parameter to English, the reader level to general, and using the gpt4o model with both explanation and\nchart display enabled. These functions automatically trigger a call to the OpenAI API to generate the corresponding\nexplanation texts. Using this model, the estimated time per request is approximately 14.9 seconds, with an average\ncost of about $0.017 per request. Table 2 depict the plot bar, waterfall bar, and text explanation for this liver disease\nclassification after using the ContextualSHAP library.\n7\n"}, {"page": 8, "text": "Table 2. Image and Explanation with ContextualSHAP\nThe bar plot you see represents the impact of different features (or patient information) on a model that predicts\nwhether a person has liver disease. These features include various blood test results and demographic information.\nThe longer the bar, the more influence that feature has on the modelâ€™s decision:\n(1) AST (Aspartate Aminotransferase): This enzyme is found in the liver and muscles. High levels might\nindicate liver damage. It has the biggest SHAP value, meaning itâ€™s very important in predicting liver\ndisease.\n(2) ALT (Alanine Aminotransferase): Like AST, this is another liver enzyme. High levels suggest liver cell\ndamage. Itâ€™s the second most important factor according to the SHAP value.\n(3)\nGGT (Gamma-Glutamyl Transferase): An enzyme related to the liver and bile ducts. Its SHAP value shows\nit significantly contributes to the modelâ€™s prediction, often relating to alcohol use or bile duct issues.\n(4)\nALP (Alkaline Phosphatase): An enzyme found in various tissues like the liver and bones. Its value suggests\nmoderate importance, usually in liver or bone disorders.\n(5)\nBIL (Bilirubin): Itâ€™s a pigment from red blood cell breakdown. High levels can indicate liver or bile duct\nproblems. The model finds it moderately significant.\n(6)\nCHOL (Cholesterol): Though known for heart disease risk, its role in the model suggests its levels might\nhave some correlation with liver health.\n(7)\nCHE (Cholinesterase): A liver-produced enzyme, and low levels can indicate liver issues. It appears mildly\nsignificant.\n(8)\nCREA (Creatinine): High levels indicate kidney function problems, which indirectly relates to overall health\nstatus the model considers.\n(9)\nALB (Albumin): A protein by the liver. Surprisingly, the model doesnâ€™t find it very influential, as shown by\nits low SHAP value, but it could still be relevant in specific cases.\nOther Features: Include the rest of the features like Age and Sex, which the model uses but finds less critical than the\nmain ones listed.\nThe waterfall plot explains how different features contribute to predicting the likelihood of a patient having liver\ndisease. The prediction starts at an average baseline value and gets adjusted up or down depending on certain factors,\ncalled SHAP values.\n(1) AST (Aspartate Aminotransferase): This is a liver enzyme. Here, with a high SHAP value of +0.30, it\nstrongly pushes the prediction towards indicating liver disease. The patientâ€™s AST level is 324, much higher\nthan normal, suggesting possible liver damage.\n(2)\nGGT (Gamma-Glutamyl Transferase): Related to bile ducts, this enzymeâ€™s SHAP value of +0.20 also\nsignificantly pushes the prediction towards liver disease, with a patientâ€™s level of 392.2, indicating potential\nliver or bile duct problems, possibly due to alcohol use.\n(3)\nBIL (Bilirubin): High bilirubin levels can indicate liver problems. With a SHAP value of +0.10, this feature\nalso contributes to a higher chance of liver disease, given the patientâ€™s level of 67.\n(4)\nCHOL (Cholesterol): It slightly pushes the prediction towards a liver disease probability with a SHAP value\nof +0.05. The patientâ€™s value is 3.09, which needs further context to interpret fully.\n(5) ALT (Alanine Aminotransferase): With a SHAP value of -0.03, this enzyme actually pulls the prediction\naway from liver disease, indicating the patientâ€™s level (27.8) might be in a safer range.\n(6)\nCREA (Creatinine): This helps evaluate kidney function. A SHAP value of -0.02 means it slightly reduces\nthe chance of liver disease. The patientâ€™s level is 97.7.\n(7)\nCHE (Cholinesterase): With a slight SHAP value of +0.02, it leans towards liver disease, as a low level can\nindicate liver issues. The level here is 5.75.\n(8) ALB (Albumin): A protein made by the liver, it marginally indicates liver disease with a SHAP value of\n+0.01 as the patientâ€™s level is 37.\n(9)\nALP (Alkaline Phosphatase): It has a small SHAP value of -0.01, which lightly pushes the prediction away\nfrom liver disease. The patientâ€™s level is 114.\nEach feature in the waterfall plot shows its influence, positive or negative, on the prediction. Features adding positively\nincrease the likelihood of predicting liver disease, whereas negative ones suggest otherwise. In this case, the model\nprediction finally settles at 0.76, indicating a relatively high risk of liver disease for the patient.\n4.2\nValidation\nTo validate whether the proposed approach improves end-user understandability, we conducted two types of evaluations:\na Likert-scale survey and semi-structured deep interviews. These methods were applied to seven participants with\ndifferent backgrounds, as shown in Table 3.\nTable 3. Group Participant Demographics\nLaypersons\nExperts (Doctors)\nNumber of participants\n5\n2\nAge (years)\n24â€“39 (average 31.8)\n31, 34\nNationality\nJapanese (2), Indonesian (2), Chinese (1)\nIndonesian (2)\n8\n"}, {"page": 9, "text": "In the Likert-scale survey, users were asked to rate their agreement (1 = Strongly Disagree to 5 = Strongly Agree) on\nfive questions measuring perceived understandability across three types of explanations: (i) SHAP-only visualization,\n(ii) SHAP + non-contextual explanation, (iii) contextualSHAP: SHAP + contextual generative text. The result of the\naverage likert-scale survey depict in Table 4.\nTable 4. User Perceived Understandability (Average Likert Scores)\nQuestions\nMean Score Group 1\nMean Score Group 2\ni\nii\niii\ni\nii\niii\nThe explanation helped me understand how the model arrived at its decision.\n1.8\n2.8\n3.2\n3\n4\n4\nThe explanation was clear and easy to comprehend.\n1.6\n2.6\n3.2\n3\n4\n4.5\nThe explanation felt relevant to my knowledge and background.\n1.2\n1.6\n2.6\n4\n4\n4.5\nThe combination of text and visual elements improved my understanding.\nNA*\n3.2\n3.6\nNA*\n3.5\n4\nI feel confident that I understood the modelâ€™s reasoning after reviewing the expla-\nnation.\n1.6\n3\n3.8\n3\n4\n4\nAverage\n1.55\n2.64\n3.28\n3.25\n3.9\n4.2\nNote: NA indicates the question was not applicable to SHAP-only visualization due to the absence of explanatory text.\nBased on the Likert-scale responses and interviews, the layperson group showed the clearest improvement in\nperceived understandability when moving from SHAP-only visualizations (i) to explanations with text (ii), and further\nto contextual explanations (iii). While generic explanatory text (ii) already improved clarity compared to raw SHAP\noutputs, participants reported that contextualized text (iii) was significantly more effective in bridging the gap between\ntechnical information and their background knowledge. This progression highlights the importance of contextualization\nin addition to textual support. Consistent with the cognitive theory of multimedia learning, the combination of visuals\nand tailored text substantially improved comprehension. Furthermore, some participants emphasized the added value\nof receiving explanations in their native language, reinforcing the importance of localization for accessibility.\nIn contrast, the doctor group reported consistently high levels of understandability across all formats. For them, the\ndifference between generic (ii) and contextual explanations (iii) was marginal, as their domain expertise enabled them\nto interpret SHAP outputs directly. They noted that while textual explanations are not harmful, conciseness and focus\nare more valuable than elaboration. This indicates that explanation strategies should be user-adaptive, in this case\nlaypersons benefit most from contextualized, localized explanations, whereas experts prefer succinct interpretations.\n5\nResult\nThis study aimed to improve the perceived clarity of model explanations by combining SHAP visualizations with\ncontext-aware natural language outputs generated via a custom Python package, contextualSHAP, which uses OpenAIâ€™s\nlanguage model. Effectiveness was assessed through a Likert-scale survey and interviews with a layperson and a\nmedical expert, comparing three conditions: (i) SHAP only, (ii) SHAP with generic explanation, and (iii) SHAP with\ncontext-aware explanation from contextualSHAP.\nThe results showed that contextualSHAP consistently improved understandability scores, particularly for the\nlayperson group, who considered the added context essential for interpreting the modelâ€™s decisions. Their average\nscores rose from 1.55 (SHAP-only) and 2.64 (SHAP+generic) to 3.28 (contextualSHAP), demonstrating a clear gain\nin perceived clarity. Participants also emphasized that receiving explanations in their native language significantly\nenhanced comprehension, underscoring the role of localization in accessibility. In contrast, the doctor group reported\nhigh scores across all conditions, with only marginal differences between generic and contextual explanations (3.25, 3.9,\n9\n"}, {"page": 10, "text": "and 4.2, respectively). Their domain expertise allowed them to interpret SHAP outputs without additional context, and\nthey expressed a preference for concise, focused outputs over elaboration.\nThese findings highlight the importance of user-tailored, context-aware explanations, as different user groups\nperceive them differently. This paper reports a preliminary experiment, and further studies are needed to confirm that\ncontextualSHAP definitively improves the understandability of machine learning model predictions. Future research\nshould involve larger participant groups across diverse domains to validate the effectiveness of contextual explanations\nand to further assess the flexibility and impact of contextualSHAP in supporting human-centric AI explainability.\nReferences\n[1] Christoph Molnar. Interpretable Machine Learning. Christoph Molnar, 2022.\n[2] David Gunning, Eric Vorm, Jerry Y. Wang, and Mark Turek. Darpaâ€™s explainable ai (xai) program: A retrospective. Applied AI Letters, 2(4), 2021.\n[3] Robert M. Kinney, Christos Anastasiades, Richard Authur, Iz Beltagy, Jonathan Bragg, Adam Buraczynski, et al. The semantic scholar open data\nplatform, 2023.\n[4] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on explainable artificial intelligence (xai). IEEE Access, 6, 2018.\n[5] Alejandro Barredo Arrieta et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai.\nInformation Fusion, 58, 2020.\n[6] Arun Rai. Explainable ai: from black box to glass box. Journal of the Academy of Marketing Science, 48(1):137â€“141, 2020.\n[7] G. P. Reddy and Y. V. P. Kumar. Explainable ai (xai): Explained. In IEEE Open Conference of Electrical, Electronic and Information Sciences (EStream),\n2023.\n[8] Wei Yang et al. Survey on explainable ai: From approaches, limitations and applications aspects. Human-Centric Intelligent Systems, 3(3), 2023.\n[9] Shuai Ma. Towards human-centered design of explainable artificial intelligence (xai): A survey of empirical studies, 2024.\n[10] Yunfeng Zhang et al. Effect of confidence and explanation on accuracy and trust calibration in ai-assisted decision making. In Proceedings of the\n2020 Conference on Fairness, Accountability, and Transparency, 2020.\n[11] P. Linardatos, V. Papastefanopoulos, and S. Kotsiantis. Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1):1â€“45,\n2021.\n[12] Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions, 2017.\n[13] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair, Rich Caruana, Jonathan Himmelfarb, Nisha Bansal, and\nSu-In Lee. From local explanations to global understanding with explainable ai for trees. Nature Machine Intelligence, 2(1), 2020.\n[14] Kartik S. Kalyan, Ajit Rajasekharan, and Sangeetha S. Ammus: A survey of transformer-based pretrained models in natural language processing,\n2021.\n[15] Tom B. Brown et al. Language models are few-shot learners, 2020.\n[16] Kartik S. Kalyan. A survey of gpt-3 family large language models including chatgpt and gpt-4, 2023.\n[17] Long Ouyang et al. Training language models to follow instructions with human feedback, 2022.\n[18] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, and Fosca Giannotti. A survey of methods for explaining black box models,\n2018.\n[19] Josua Krause, Adam Perer, and Kenney Ng. Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of\nthe CHI Conference on Human Factors in Computing Systems, pages 5686â€“5697, 2016.\n[20] Tim Miller, Paul Howe, and Leon Sterling. Explainable ai: Beware of inmates running the asylum. In IJCAI Workshop on Explainable Artificial\nIntelligence (XAI), 2017.\n[21] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267, 2019.\n[22] AndrÃ©s PÃ¡ez. The pragmatic turn in explainable artificial intelligence (xai). Minds and Machines, 29(3):441â€“459, 2019.\n[23] Sana Naveed, Gregor Stevens, and David Robin-Kern. An overview of the empirical evaluation of explainable ai (xai): A comprehensive guideline\nfor user-centered evaluation in xai. Applied Sciences, 14(23), 2024.\n[24] Latifa Dwiyanti, Hidenori Nambo, and Nur Hamid. Leveraging explainable artificial intelligence (xai) for expert interpretability in predicting rapid\nkidney enlargement risks in autosomal dominant polycystic kidney disease (adpkd). AI, 5(4):2037â€“2065, 2024.\n[25] Richard E. Mayer. Multimedia Learning. Cambridge University Press, 2002.\n[26] Shap documentation. Accessed: 2025-05-02.\n[27] Kacper Sokol and Peter Flach. Explainability fact sheets: A framework for systematic assessment of explainable approaches. In Proceedings of the\n2020 Conference on Fairness, Accountability, and Transparency, pages 56â€“67, 2020.\n[28] Monica Melby-LervÃ¥g and Arne LervÃ¥g. Reading comprehension and its underlying components in second-language learners: A meta-analysis of\nstudies comparing first- and second-language learners. Psychological Bulletin, 140(2):409â€“433, 2014.\n[29] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The uci machine learning repository, 2025. Accessed: 2025-05-12.\n10\n"}]}