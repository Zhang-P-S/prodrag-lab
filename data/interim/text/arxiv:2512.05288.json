{"doc_id": "arxiv:2512.05288", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.05288.pdf", "meta": {"doc_id": "arxiv:2512.05288", "source": "arxiv", "arxiv_id": "2512.05288", "title": "Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification", "authors": ["Feijiang Han"], "published": "2025-12-04T22:26:30Z", "updated": "2025-12-04T22:26:30Z", "summary": "Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.05288v1", "url_pdf": "https://arxiv.org/pdf/2512.05288.pdf", "meta_path": "data/raw/arxiv/meta/2512.05288.json", "sha256": "77a9d60541b34a4adbb648f0d8ecff083325efdef64326a1985d1c863204fa3e", "status": "ok", "fetched_at": "2026-02-18T02:25:19.936567+00:00"}, "pages": [{"page": 1, "text": "Beyond Detection: A Comprehensive Benchmark and Study on Representation\nLearning for Fine-Grained Webshell Family Classification\nFeijiang Han\nDepartment of Computer and Information Science, University of Pennsylvania\nPhiladelphia, PA, USA\nfeijhan@seas.upenn.edu\nAbstract\nMalicious WebShells pose a significant and evolving threat\nby compromising critical digital infrastructures and endan-\ngering public services in sectors such as healthcare and fi-\nnance. While the research community has made significant\nprogress in WebShell detection (i.e., distinguishing malicious\nsamples from benign ones), we argue that it is time to tran-\nsition from passive detection to in-depth analysis and proac-\ntive defense. One promising direction is the automation of\nWebShell family classification, which involves identifying\nthe specific malware lineage in order to understand an adver-\nsary’s tactics and enable a precise, rapid response. This cru-\ncial task, however, remains a largely unexplored area that cur-\nrently relies on slow, manual expert analysis. To address this\ngap, we present the first systematic study to automate Web-\nShell family classification. Our method begins with extract-\ning dynamic function call traces to capture inherent behav-\niors that are resistant to common encryption and obfuscation.\nTo enhance the scale and diversity of our dataset for a more\nstable evaluation, we augment these real-world traces with\nnew variants synthesized by Large Language Models. These\naugmented traces are then abstracted into sequences, graphs,\nand trees, providing a foundation to benchmark a comprehen-\nsive suite of representation methods. Our evaluation spans\nclassic sequence-based embeddings (CBOW, GloVe), trans-\nformers (BERT, SimCSE), and a range of structure-aware\nalgorithms, including Graph Kernels, Graph Edit Distance,\nGraph2Vec, and various Graph Neural Networks. Through\nextensive experiments on four real-world, family-annotated\ndatasets under both supervised and unsupervised settings, we\nestablish a robust baseline and provide practical insights into\nthe most effective combinations of data abstractions, repre-\nsentation models, and learning paradigms for this challenge.\nThis foundational work is a crucial step toward automating\nthreat intelligence, accelerating incident response, and ulti-\nmately enhancing the resilience of the digital services that\nsociety depends on.\nIntroduction\nMalicious WebShells have evolved from simple scripts into\nstrategic assets used in sophisticated attacks that directly\nthreaten critical public services in sectors like healthcare\nand finance, endangering the sensitive data of millions.\nTo counter this pervasive threat, the research community\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nhas achieved considerable success in developing automated\ntechniques for WebShell detection (Tu et al. 2014; Aboaoja\net al. 2022; Ma, Han, and Zhou 2024; Feng et al. 2024; Han\net al. 2025c).\nWhile successful, this focus on binary classification (ma-\nlicious vs. benign) provides only a foundational first line of\ndefense and offers limited actionable intelligence for sub-\nsequent security operations. A more proactive and robust\nsecurity posture requires not just knowing that a server is\ncompromised, but understanding the specific nature of the\nthreat itself. This necessitates WebShell family classifica-\ntion: the task of identifying the specific variant or lineage\nof the malware. Automating this process is crucial as it un-\nlocks a deeper level of threat intelligence, helping security\nteams attribute attacks, anticipate an adversary’s next moves,\nand mount a faster, more targeted incident response (Zhao\net al. 2024). For instance, an automated system can reduce\nresponse time from hours of manual expert analysis to mere\nseconds, enabling security operation centers (SOCs) to trig-\nger specific defense playbooks tailored to a family’s known\ntactics before significant damage, like data exfiltration, oc-\ncurs. This critical task, however, remains largely unexplored\nin the research community, with current practices relying on\ntime-consuming manual analysis.\nWe argue that automating this task is technically feasible\nfor two primary reasons. First, WebShells within the same\nfamily often share distinct behavioral characteristics due to\ncode reuse (Wrench and Irwin 2015; Starov et al. 2016).\nSecond, this malicious behavior can be captured in the pro-\ngram’s dynamic function call traces even when the source\ncode is obfuscated (De Go¨er et al. 2018; Xu and Chen 2023).\nThis insight forms our core hypothesis: by learning to rec-\nognize these fundamental behavioral patterns, a model can\neffectively group and track WebShell families, even when\nthey are protected by surface-level obfuscation.\nHowever, family classification is inherently more chal-\nlenging than binary detection, as it requires models that\ncan capture the nuanced behavioral patterns that differenti-\nate families, not just generic malicious traits. This challenge\nmotivates the foundational research question of our work:\nWhat data structures and representation methods are most\neffective for capturing these family-specific behaviors?\nTo answer this question, this paper presents the first sys-\ntematic study to benchmark WebShell family classification.\narXiv:2512.05288v1  [cs.CR]  4 Dec 2025\n"}, {"page": 2, "text": "We conduct a large-scale empirical evaluation of diverse\ndata abstractions and representation learning methods de-\nsigned to capture WebShell behavior. Our goal is to estab-\nlish a robust foundation and a practical guide for this critical\ntask.\nOur contributions are as follows:\n• A Comprehensive Methodological Framework. We\ndesign and execute the first large-scale benchmark for\nthis task. To ensure a robust evaluation, we introduce a\ndata synthesis framework leveraging a Large Language\nModel (LLM) to augment our real-world data with di-\nverse, behaviorally-consistent function call traces. We\nabstract this enriched dataset into three fundamental data\nstructures (sequences, graphs, and trees) and systemati-\ncally evaluate a diverse spectrum of representation learn-\ning methods, from classic word embeddings and trans-\nformers to structure-aware algorithms like Graph Kernels\nand various Graph Neural Networks (GNNs).\n• A Robust Empirical Baseline. Through extensive ex-\nperiments on four real-world datasets with both super-\nvised and unsupervised classification, we establish the\nfirst robust, data-driven performance baseline for Web-\nShell family classification. This provides a crucial point\nof comparison for all future work in this emerging area.\n• Actionable Insights for the Security Community.\nOur analysis delivers a clear hierarchy of performance,\ndemonstrating that structural representations (especially\ntrees) are decisively more effective than sequential ones,\nand that GNNs are the premier modeling architecture.\nThese findings offer immediate, practical guidance for\npractitioners and researchers aiming to build effective\nclassification systems.\n• A Practical Guide to Implementation. We distill our\nfindings into a set of best practices for implementation,\ndetailing optimal strategies for model selection and hy-\nperparameter configuration.\nUltimately, this work provides both a foundational bench-\nmark and a practical guide, empowering the community to\nmove beyond simple detection and build the next generation\nof intelligent, fine-grained defense systems.\nProblem Formulation\nThe primary goal of WebShell family classification is to au-\ntomatically categorize a given malicious WebShell into one\nof several predefined families. Our central research objec-\ntive is to fundamentally understand which data abstractions\nand representation methods are most effective at capturing\nthese family-specific features from raw data. We therefore\nadopt a two-stage framework that decouples representation\nlearning from classification, enabling a fair and standardized\nbenchmark of different encoders.\nStage 1: Representation Learning.\nThe input to this\nstage is a raw, unstructured function call trace from a sin-\ngle WebShell. An encoder model, g, maps this trace into a\nfixed-dimensional numerical vector x = g(trace) ∈Rd.\nThis vector x, or embedding, is a structured summary of\nthe WebShell’s runtime behavior. Our core investigation lies\nin comparing various designs for this encoder g.\nStage 2: Benchmarking via Classification.\nIn this stage,\nwe use a suite of standard classifiers to benchmark the qual-\nity of the embeddings (x) produced in Stage 1. The cen-\ntral principle is that a higher-quality representation will be\nmore separable in vector space and thus yield better perfor-\nmance on classification tasks, providing an objective mea-\nsure of the encoder’s effectiveness. Formally, the classifica-\ntion task is defined as follows: given a dataset of embeddings\nD = {(x1, y1), . . . , (xn, yn)}, where yi ∈{1, . . . , K} is\nthe family label for one of K families. The objective is to\nlearn a classifier f : Rd →{1, . . . , K} that accurately pre-\ndicts the label ˆy = f(x) for any given embedding x.\nDataset Collection\nData Acquisition and Annotation.\nOur dataset construc-\ntion follows an established pipeline for creating high-quality,\nreal-world WebShell datasets (Zhao et al. 2024). The pro-\ncess begins with suspicious files flagged by a large-scale\ncloud provider’s malware detection system. Each potential\nWebShell is executed in a secure sandbox to capture its dy-\nnamic function call trace—a chronologically ordered log of\nits runtime behavior. This dynamic approach offers a sig-\nnificant advantage over static analysis, bypassing common\nevasion techniques like obfuscation and encryption, thereby\nrevealing a well-defined structure of operational behaviors\nideal for extracting family-specific features. Besides, this\ntrace is essentially language-agnostic. By discarding noise\nfrom programming syntax and idiosyncratic coding habits,\nit focuses on the core operational logic, which makes our\nevaluation broadly generalizable to different server-side lan-\nguages. An example of this raw data is shown in Table 1.\nTable 1: An Example of raw dynamic function call trace cap-\ntured from sandboxed execution. Each record consists of a\nunique identifier and the corresponding function calls.\nFilemd5\nDynamic Function Calls\n191c2...3b01\n[main, zend compile file, main,\nbase64 decode, main, assert, assert,\nzend compile string, assert,\nzend fetch r post, assert, eval, eval,\nzend compile string, ...]\nSecurity experts then manually review these traces to fil-\nter out false positives. The verified malicious samples sub-\nsequently undergo a human-machine collaborative process\nfor family annotation. Samples that cannot be confidently\nassigned to any established family are designated as outliers\nand assigned a ‘Family ID‘ of -1. The final labeled data for-\nmat is presented in Table 2.\nLLM-Powered Data Augmentation.\nTo address the in-\nherent limitations of real-world data collection, such as data\nscarcity for rare families and the absence of novel, zero-\nday threats, we introduce an LLM-augmented data synthesis\n"}, {"page": 3, "text": "Table 2: Examples of annotated samples with their identi-\nfiers and assigned family labels.\nFilemd5\nFamily ID\n12b7340d1b8acf0fe2d78fce84bccf8c\n1\n1aba8701dcab6629caa9e21fc772b50e\n2\n28c5678442c6a3ee17290ece4d1c8904\n3\n00cd0f1bfda4903dba26541301c686ec\n5\n01625e53cb2d1275fbf4b2af0f6946e3\n-1\nframework to enrich and expand our dataset. This frame-\nwork enables us to scale our data collection efforts and en-\nhance the diversity of the samples. The specific prompt tem-\nplates used in this process are detailed in the Appendix.\nSpecifically, we employed a two-pronged strategy. First,\nfor Intra-Family Data Augmentation, we used a few-shot\nprompts, providing the LLM with a high-level description of\na family’s behavior along with several canonical examples of\nits function call traces. This enabled the model to generate\na large volume of new samples that are behaviorally con-\nsistent with existing families but syntactically unique. This\ntechnique effectively addresses the class imbalance problem\nby augmenting underrepresented families.\nSecond, building on this, we introduced a New Family &\nZero-Day Simulation. This stage simulates the adversarial\ntactic of creating novel variants by blending the behavioral\ncharacteristics of different malware families. The resulting\nsynthetic traces can be labeled either as entirely new families\nor as adversarial outliers, which are designed to challenge\nthe classifier’s robustness.\nTo ensure the fidelity and logical soundness of the aug-\nmented synthetic data, all LLM-generated traces underwent\na rigorous two-stage, human-in-the-loop verification and\nsanitization process.\n1. Automated Filtering: All generated traces first pass\nthrough an automated filter to ensure they are well-\nformed (e.g., proper list format and valid function names\nfrom our vocabulary). Malformed traces are immediately\ndiscarded.\n2. Human-in-the-Loop Verification: We then use a visual-\nization platform to project and plot the embeddings of the\nfiltered synthetic samples alongside the real-world sam-\nples from the same family. For intra-family data augmen-\ntation samples, we manually review these clusters and\ndiscard any synthetic samples that significantly deviate\nfrom their family’s core cluster center.\nDataset Details.\nThrough this process, we constructed\nfour distinct datasets for our experiments, labeled DS1\nthrough DS4. These datasets feature progressively increas-\ning scale and complexity in terms of sample size, the num-\nber of families, and the quantity of outliers. This graduated\ndesign allows for a robust and thorough evaluation of our\nrepresentation methods across diverse conditions. Table 3\nsummarizes the key statistics of each dataset.\nTable 3: Details of our dataset. The complexity increases\nfrom DS1 to DS4.\nDataset\n# Samples\nComplexity\n# Families\n# Outliers\nDS1\n452\nLow\n21\n1\nDS2\n553\nMedium\n37\n10\nDS3\n1125\nHigh\n48\n23\nDS4\n1617\nHigh\n81\n28\nBehavioral Data Abstraction\nTo make raw function call traces amenable to machine learn-\ning, we abstract this sequential data into three distinct struc-\ntural representations: sequences, graphs, and trees. As illus-\ntrated in Figure 1, each representation captures a different\naspect of a WebShell’s runtime behavior, providing a unique\nlens through which to analyze its characteristics.\nSequence Model.\nThe most direct abstraction treats a\nfunction call trace as a sequence of discrete tokens, where\neach function name becomes a token in the execution order\n(Figure 1a). This linear representation is compatible with a\nwide range of natural language processing models. A trace\ncan be represented as S = (t1, t2, . . . , tn), where ti is the\ni-th function called.\nGraph Model.\nTo capture more complex, non-sequential\ninteractions, we model each trace as a Function Call Graph\n(FCG), shown in Figure 1b. An FCG, G = (V, E), provides\na static, aggregate view of the program’s behavior, where\neach unique function is a node v ∈V , and a directed edge\n(u, v) ∈E exists if function u ever calls function v. The\nedges can be weighted by call frequency to represent the\nstrength of the interaction. This model effectively captures\nall calling relationships, including loops and indirect calls.\nTree Model.\nTo preserve the hierarchical nature of pro-\ngram execution, we also represent each trace as a Function\nCall Tree (FCT), illustrated in Figure 1c. The FCT, T =\n(V, E), is a rooted tree where the entry point (e.g., ‘ main ‘)\nis the root and edges represent direct parent-child call rela-\ntionships. Unlike the graph model, the FCT is acyclic and\npreserves the specific execution path and context; a function\ncalled multiple times in different contexts appears as distinct\nnodes in the tree.\nRepresentation and Benchmarking\nRepresentation Learning Methods\nWe apply a diverse set of foundational and widely adopted\nrepresentation learning techniques tailored to each data ab-\nstraction. Table 4 provides a complete overview.\nFor sequence models,\nwe evaluate two distinct cate-\ngories: classic context-free methods (CBOW (Mikolov et al.\n2013), GloVe (Pennington, Socher, and Manning 2014)) and\nmodern context-aware transformers (BERT (Devlin et al.\n2018), SimCSE (Gao, Yao, and Chen 2021)). To produce a\nsingle fixed-dimensional vector for each function call trace,\nwe employ several aggregation strategies tailored to each\nmodel type. For the static embeddings produced by CBOW\n"}, {"page": 4, "text": "(a) Sequence Model\n(b) Graph Model\n(c) Tree Model\nFigure 1: The visualization of three data abstractions. (a) The Sequence Model visualizes the chronological execution flow. (b)\nThe Graph Model provides a static, aggregate view of all calling relationships. (c) The Tree Model preserves the hierarchical\ncall structure and execution context.\nand GloVe, we investigate three strategies: averaging all\nfunction call vectors to create a mean representation (avg);\nconcatenating the vectors of each function call sequentially\n(concat); and a TF-IDF weighted average that emphasizes\nmore discriminative functions. For the transformer models,\nwe leverage their deep, contextualized hidden states by: av-\neraging the hidden states across all tokens (avg); concatenat-\ning the hidden state vectors from different layers (concat);\nand using the final hidden state of the dedicated classifica-\ntion token, [CLS].\nFor graph and tree models,\nwe employ two classic meth-\nods for direct structural comparison. First, Graph/Tree Ker-\nnels (Shervashidze et al. 2011) measure similarity by count-\ning shared substructures, such as common call sequences\n(paths), randomly generated traversals (random walks), and\nidentical small-scale call hierarchies (subtrees). Second, we\ncompute the Graph/Tree Edit Distance (Marzal and Vi-\ndal 1993), which quantifies dissimilarity by calculating the\nminimum cost of operations (e.g., node insertion, dele-\ntion, and substitution) required to transform one structure\ninto another. For learning-based approaches, we benchmark\nthree prominent Graph Neural Network (GNN) architectures\nwhich learn representations via message passing: Graph\nConvolutional Network (GCN) (Kipf and Welling 2016),\nGraph Attention Network (GAT) (Veliˇckovi´c et al. 2018),\nand Graph Isomorphism Network (GIN) (Xu et al. 2018).\nFinally, we include Graph2Vec (Narayanan et al. 2017), an\nunsupervised method that learns whole-graph embeddings.\nBenchmarking Classifiers.\nTo assess the quality of the\nlearned representations, we benchmark the resulting embed-\ndings using a suite of four standard classifiers. For unsuper-\nvised evaluation, we use K-Means (MacQueen 1967) and\nMean-Shift (Comaniciu and Meer 2002) clustering. For su-\npervised evaluation, we employ two widely-used models:\nRandom Forest (Breiman 2001) and the Support Vector Ma-\nchine (SVM) (Cortes and Vapnik 1995). This comprehensive\nTable 4: Overview of the representation methods and their\nimplementation variants evaluated for each data abstraction.\nRepresentation Method\nImplementation Variants\nSequence-Based Models\nWord2Vec (CBOW)\nConcat, Avg, Concat & Avg\nGloVe\nConcat, Avg, Concat & Avg\nBERT\nConcat, Avg, CLS\nSimCSE\nConcat, Avg, CLS\nGraph-Based Models\nGraph Kernel\nPath, Walk, Subtree\nGraph Edit Distance\n–\nGraph Neural Networks\nGCN, GAT, GIN\nGraph Embedding (Graph2Vec)\n–\nTree-Based Models\nTree Kernel\nPath, Walk, Subtree\nTree Edit Distance\n–\nGraph Neural Networks\nGCN, GAT, GIN\nTree Embedding (Graph2Vec)\n–\nframework allows us to measure the effectiveness of each\nrepresentation in both labeled and unlabeled settings.\nExperimental Setup\nImplementation Details\nRepresentation Models.\nFor all representation learning\nmodels, we standardized the output embedding dimension\nto 128 to balance expressiveness and computational effi-\nciency. The input dimensions were dynamically set based\non the function vocabulary size of each specific dataset. To\nestablish a consistent and reproducible baseline, we utilized\nthe default hyperparameter settings (e.g., optimizer, learn-\ning rate, loss function) recommended for each model during\n"}, {"page": 5, "text": "Models\nRepresentation Methods\nDataset 4\nKM-ACC\nKM-NMI\nMS-ACC\nMS-NMI\nRF-ACC\nRF-F1\nSVM-ACC\nSVM-F1\nSequence-\nBased\nWord2Vec\nConcat\n0.651 \n0.859 \n0.288 \n0.526 \n0.919 \n0.871 \n0.915 \n0.887 \nAvg\n0.620 \n0.856 \n0.383 \n0.662 \n0.934 \n0.907 \n0.838 \n0.751 \nConcat & Avg\n0.496 \n0.791 \n0.396 \n0.673 \n0.942 \n0.908 \n0.418 \n0.185 \nGloVe\nConcat\n0.750 \n0.890 \n0.403 \n0.657 \n0.946 \n0.899 \n0.942 \n0.918 \nAvg\n0.792 \n0.920 \n0.445 \n0.717 \n0.941 \n0.917 \n0.844 \n0.745 \nConcat & Avg\n0.648 \n0.860 \n0.421 \n0.700 \n0.943 \n0.912 \n0.489 \n0.219 \nBert\nConcat\n0.594 \n0.817 \n0.416 \n0.688 \n0.933 \n0.897 \n0.893 \n0.875 \nAvg\n0.603 \n0.822 \n0.368 \n0.669 \n0.853 \n0.795 \n0.821 \n0.756 \nCLS\n0.579 \n0.813 \n0.388 \n0.669 \n0.861 \n0.816 \n0.822 \n0.762 \nSimCSE\nConcat\n0.631 \n0.846 \n0.402 \n0.651 \n0.843 \n0.730 \n0.905 \n0.834 \nAvg\n0.612 \n0.821 \n0.332 \n0.604 \n0.826 \n0.730 \n0.877 \n0.799 \nCLS\n0.577 \n0.794 \n0.316 \n0.547 \n0.729 \n0.584 \n0.832 \n0.737 \nGraph-Based\nGraph \nKernel\nPath\n0.837 \n0.926 \n0.346 \n0.612 \n0.947 \n0.930 \n0.943 \n0.928 \nWalk\n0.768 \n0.896 \n0.375 \n0.633 \n0.946 \n0.927 \n0.946 \n0.922 \nSubTree\n0.829 \n0.925 \n0.426 \n0.705 \n0.925 \n0.905 \n0.895 \n0.851 \nGraph Edit Distance\n0.802 \n0.915 \n0.350 \n0.650 \n0.961 \n0.935 \n0.967 \n0.953 \nGNN\nGCN\n0.821 \n0.933 \n0.413 \n0.703 \n0.963 \n0.955 \n0.949 \n0.928 \nGAT\n0.872 \n0.945 \n0.335 \n0.634 \n0.958 \n0.945 \n0.934 \n0.904 \nGIN\n0.536 \n0.831 \n0.309 \n0.591 \n0.944 \n0.931 \n0.953 \n0.944 \nGraph Embedding (Graph2Vec)\n0.515 \n0.847 \n0.329 \n0.686 \n0.963 \n0.953 \n0.972 \n0.965 \nTree-Based\nTree Kernel\nPath\n0.728 \n0.876 \n0.360 \n0.597 \n0.942 \n0.902 \n0.940 \n0.891 \nWalk\n0.843 \n0.934 \n0.410 \n0.705 \n0.942 \n0.912 \n0.935 \n0.898 \nSubTree\n0.895 \n0.938 \n0.432 \n0.657 \n0.968 \n0.949 \n0.960 \n0.935 \nTree Edit Distance\n0.848 \n0.929 \n0.330 \n0.652 \n0.948 \n0.928 \n0.947 \n0.939 \nGNN\nGCN\n0.835 \n0.936 \n0.338 \n0.613 \n0.963 \n0.958 \n0.938 \n0.918 \nGAT\n0.879 \n0.952 \n0.371 \n0.674 \n0.959 \n0.957 \n0.967 \n0.939 \nGIN\n0.558 \n0.842 \n0.332 \n0.620 \n0.948 \n0.931 \n0.954 \n0.939 \nTree Embedding (Graph2Vec)\n0.763 \n0.902 \n0.341 \n0.637 \n0.963 \n0.944 \n0.969 \n0.959 \nFigure 2: Performance comparison of representation methods on the DS4 dataset. Columns denote classifiers (KM: K-Means;\nMS: Mean-Shift; RF: Random Forest; SVM) and metrics.\nthe representation learning phase. Detailed configurations\nfor each model are provided in the Appendix.\nBenchmarking Classifiers.\nTo ensure a fair comparison\nacross different representations, we employed a grid search\nwith cross-validation to tune the hyperparameters for each\nrepresentation-classifier pair. To ensure statistical robust-\nness, all reported results are the average of 10 independent\nruns, each with a different seed.\nEvaluation Metrics\nSupervised Classification.\nWe assess the performance of\nsupervised models using standard metrics: Accuracy and F1-\nscore. For the multi-class setting, F1-score is reported as\nmacro-averaged values. This approach computes the metric\nindependently for each family and then calculates the un-\nweighted mean, ensuring that all families, regardless of their\nsize, contribute equally to the final score.\nUnsupervised Clustering.\nWe evaluate clustering quality\nusing two primary metrics: Accuracy and Normalized Mu-\ntual Information (NMI). Accuracy is computed by first find-\ning the optimal mapping between cluster assignments and\nground-truth labels via the Hungarian algorithm and then\ncalculating the percentage of correctly assigned samples.\nNMI measures the agreement between the assigned clusters\nC and true labels Y , correcting for chance:\nNMI(Y, C) =\n2 × I(Y ; C)\nH(Y ) + H(C),\n(1)\nwhere I(Y ; C) is the mutual information between the true\nand predicted labels, while H(Y ) and H(C) are their re-\nspective entropies.\nResults and Analysis\nWe present our experimental findings for the most complex\nDS4 datasets in Figures 2. The full results for DS1, DS2,\nand DS3 are in the Appendix. Performance is visualized us-\ning a blue-to-red color gradient, where blue signifies higher\nscores. The top results in each column are highlighted.\n"}, {"page": 6, "text": "Key Insight 1: Structural Semantics Definitively\nOutperform Sequential Syntax\nThe most striking result from our benchmark is the sig-\nnificant performance gap between structural (graph and\ntree) and sequential representations. As shown in Figures 2,\nGNNs and even classic methods like Tree Edit Distance\nconsistently achieve F1-scores exceeding 0.9, while the per-\nformance of advanced sequence models like BERT is both\nlower and more volatile. Notably, as dataset complexity in-\ncreases, structural methods exhibit a much more graceful\nperformance degradation, reinforcing their robustness and\nscalability for this task.\nThis performance delta highlights a fundamental limita-\ntion of sequential models. They are designed to capture lin-\near dependencies, treating a function call trace as a syntactic\nsentence. However, a WebShell family’s signature lies not in\ncall adjacency but in the overarching control-flow topology.\nMalicious actors frequently reuse a core set of utility func-\ntions (e.g., for execution, encoding, or file access) but invoke\nthem from different program locations, often inserting non-\nfunctional ”junk” calls to thwart simple pattern matching.\nStructural representations, by abstracting the trace into\na graph or tree, capture these complex, non-local relation-\nships. They model the program’s who-calls-whom relation-\nships which is a far more fundamental and stable indicator\nof shared malicious logic. This inherent robustness to super-\nficial code reordering and obfuscation is the primary reason\nfor their superior performance.\nTable 5: The top 3 representation methods for each classifier,\nranked by overall performance across all datasets.\nClassifier\nTop 3 Optimal Representation Methods\nK-Means\nTree-GAT, Graph-GAT, Tree-Kernel\nMean Shift\nTree-GAT, CBOW, GloVe\nRandom Forest\nTree-GCN, Graph-GCN, Tree-GAT\nSVM\nTree-GAT, Graph-GIN, Tree-GIN\nKey Insight 2: Hierarchical Context is Crucial,\nGranting Trees an Edge\nWhile both graph and tree models prove effective, our results\nshow that tree-based representations yield better overall per-\nformance (Table 5). This underscores the critical importance\nof hierarchical context in distinguishing WebShell fami-\nlies.\nThe reason for this advantage lies in what each structure\npreserves. A standard FCG is an aggregate view, merging\nall invocations of a function into a single node. It shows that\nfunction A() called B(), but loses the context of how that\ncall occurred. In contrast, a FCT is acyclic and preserves the\nprecise execution path. Each node in an FCT represents a\nunique function invocation within a specific call stack.\nThis distinction is vital. A polymorphic function like\neval() might be used for different purposes depending on\nits caller. An FCT disambiguates these cases, representing\nhandler 1() →eval() and handler 2() →eval() as dis-\ntinct nodes with different parent-child relationships. This\nfine-grained, contextual fingerprint provides a more potent\nfeature set for learning.\nKey Insight 3: GNNs are the Premier Architecture\nfor Learning Behavioral Topologies\nAmong all models, GNNs emerge as the most powerful\nand stable architecture, particularly the GAT and GCN.\nThe theoretical underpinning is clear: GNNs are purpose-\nbuilt to learn from relational data via a message-passing\nparadigm that explicitly models network topology. Unlike\nclassic methods (e.g., Graph Kernels) that count predefined\nsubstructures, GNNs automatically learn the most discrimi-\nnative structural motifs.\nThe particular strength of GAT stems from its attention\nmechanism. In a WebShell’s call graph, not all function\ncalls are equally important; calls to system(), assert(), or\nbase64 decode() are far more salient than generic opera-\ntions. GAT learns to assign higher attention weights to these\ndiagnostically critical nodes and edges, effectively focusing\non the parts of the call graph that best define a family’s ma-\nlicious signature.\nTable 6: Optimal implementation strategies for sequence-\nbased methods.\nMethod\nClassifier\nOptimal Strategy\nCBOW/GloVe\nKM/MS/RF\nAvg\nSVM\nConcat\nBERT/SimCSE\nAll Classifiers\nConcat\nTable 7: Optimal implementation strategies for graph- and\ntree-based methods.\nMethod\nClassifier\nOptimal Strategy\nGraph Kernel\nUnsupervised\nSubtree Kernel\nSupervised\nPath Kernel\nTree Kernel\nAll Classifiers\nSubtree Kernel\nGNNs\nUnsupervised\nGCN, GAT\nRandom Forest\nGAT\nSVM\nGIN\nPractical Implications and Guidance\nOur findings offer a clear, actionable guide for building more\nintelligent malware defense systems.\nImplications for Threat Discovery and Operational Use.\nAs expected, supervised classifiers achieve higher overall\nperformance than unsupervised clustering algorithms, high-\nlighting the value of high-quality labels for building high-\nprecision models. Thus, when a sufficient corpus of labeled\ndata is available, supervised classification is the preferred\napproach. However, in real-world security operations, labels\nfor emerging threats are often scarce or unavailable. This\nis where unsupervised methods become indispensable, as\n"}, {"page": 7, "text": "their ability to group samples by intrinsic behavioral simi-\nlarity provides a direct pathway for discovering new or un-\nknown WebShell families. Our results show that in this set-\nting, the performance gap between structural and sequential\nrepresentations is magnified, making the choice of a robust\nstructural representation even more critical. Security teams\ncan leverage this to automatically group new malware sam-\nples, flagging emergent clusters as potential zero-day threats\nrequiring expert analysis.\nOptimal Implementation Strategies.\nAchieving these re-\nsults requires pairing the right abstraction with the right\nmodel variant. Our benchmark provides a clear roadmap\n(summarized in Tables 5, 6, and 7).\n• For overall performance, a Tree-GAT model is the most\nconsistent top performer across both supervised and un-\nsupervised tasks.\n• For GNNs, GAT and GCN are best for clustering, while\nGIN shows strength with SVMs in supervised settings.\n• For Graph Kernels, Subtree Kernels are generally supe-\nrior, especially for Tree Kernels where they are the opti-\nmal choice for all classifiers.\n• For sequence models, the optimal aggregation strategy\ndepends on the model architecture. For context-free em-\nbeddings like CBOW and GloVe, averaging the token\nembeddings of a trace is effective. For context-aware\ntransformers like BERT and SimCSE, more sophisticated\nstrategies like concatenating hidden states or using the fi-\nnal [CLS] token representation are superior.\nRelated Work\nWebShell Detection Research on WebShell detection has\npredominantly focused on binary classification, distin-\nguishing malicious from benign scripts. Early efforts re-\nlied on rule-based methods using signature matching, which\nproved ineffective against obfuscated or novel threats (Le\net al. 2021; Hannousse and Yahiouche 2021). Subsequently,\nmachine learning and deep learning techniques became\nmainstream, extracting lexical, statistical, or semantic fea-\ntures from source code or opcodes to train classifiers (Jin-\nping et al. 2020; Pu et al. 2022; Shang et al. 2024; Zhang,\nKang, and Wang 2025). Recently, Large Language Mod-\nels (LLMs) have demonstrated strong zero-shot capabilities\nin this domain (Han et al. 2025a,b), achieving competitive\nperformance without task-specific fine-tuning (Han et al.\n2025c).\nHowever, while binary detection is well-studied, research\non the more granular task of WebShell family multi-\nclassification remains scarce. This gap is significant, as\nidentifying the specific family of a WebShell is crucial for\nthreat intelligence and targeted defense. A foundational con-\ntribution in this area is the MWF dataset (Zhao et al. 2024),\nwhich provided the first publicly available, family-annotated\ndataset of malicious WebShells, thereby enabling systematic\nresearch into multi-class classification, including ours.\nRepresentation Learning for Program Behavior Our\nwork is grounded in representation learning, which aims\nto transform complex, unstructured data like function call\ntraces into meaningful vector embeddings. Inspired by natu-\nral language processing, early methods treat program traces\nas sentences. Classic techniques like Word2Vec (specifi-\ncally, CBOW and Skip-gram) (Mikolov et al. 2013) and\nGloVe (Pennington, Socher, and Manning 2014) learn static,\ncontext-independent embeddings for each function. The ad-\nvent of transformers led to powerful contextual models like\nBERT (Devlin et al. 2018), which capture deeper semantic\nrelationships. More recently, contrastive learning methods\nsuch as SimCSE (Gao, Yao, and Chen 2021) have further\nimproved the quality of sentence-level embeddings.\nTo capture the rich relational structure of function calls,\nwe also explore graph-based methods. Traditional ap-\nproaches include Graph Kernels, such as the Weisfeiler-\nLehman (WL) kernel, which measure graph similarity by\ncounting shared substructures (Shervashidze et al. 2011).\nUnsupervised methods like Graph2Vec learn embeddings\nfor entire graphs by treating them as documents and their\nsubgraphs as words (Narayanan et al. 2017). The cur-\nrent state-of-the-art, however, is dominated by Graph Neu-\nral Networks, which learn node and graph representations\nthrough iterative message passing. Our work benchmarks\nseveral prominent GNN architectures: Graph Convolutional\nNetworks (Kipf and Welling 2016), Graph Attention Net-\nworks (Veliˇckovi´c et al. 2018), and Graph Isomorphism Net-\nworks (Xu et al. 2018).\nConclusion\nIn this work, we presented a systematic benchmark and\ncomprehensive study for fine-grained WebShell family clas-\nsification, a critical and underexplored task in cybersecu-\nrity. By abstracting dynamic function call traces into se-\nquences, graphs, and trees, we conducted a large-scale\nevaluation of diverse representation learning methods. Our\nempirical results are conclusive: structural representations\ndefinitively outperform sequential models, demonstrating\nthat a family’s behavioral signature lies in its call topol-\nogy, not its syntactic order. We further identified that tree-\nbased abstractions, which preserve hierarchical execution\ncontext, provide a consistent performance advantages. Fi-\nnally, we demonstrated that Graph Neural Networks, partic-\nularly GAT, are the premier architecture for this task, offer-\ning the most robust and high-performing models across both\nsupervised and unsupervised settings. This study moves the\nfield beyond simple binary detection by establishing a ro-\nbust baseline and providing actionable guidance for build-\ning the next generation of automated threat intelligence sys-\ntems. Our findings offer a practical framework for enabling\nfaster, more precise incident response and a more proactive\ndefense against the evolving landscape of critical infrastruc-\nture threats.\nEthical Statement\nThis research is fundamentally aimed at generating a posi-\ntive societal impact by enhancing cybersecurity against ma-\nlicious WebShells, a class of malware that poses a direct\nthreat to critical infrastructure, including government, finan-\ncial, and healthcare systems. The primary benefit of this\n"}, {"page": 8, "text": "work is empowering security organizations to move beyond\nsimple detection to a more nuanced, family-level under-\nstanding of threats. This capability translates directly into\ntangible societal goods: it enables faster incident response\nto minimize data breaches of sensitive records, aids law en-\nforcement in attributing attacks, and helps preserve the in-\ntegrity and public trust in essential digital services.\nA core component of our ethical methodology was the\ndeliberate decision to release only the dynamic function call\ntraces, not the underlying source code. This approach pro-\nvides the research community with a rich behavioral sum-\nmary for analysis while intentionally withholding the full,\nexecutable malicious code. By doing so, we prevent the di-\nrect redistribution or weaponization of the original malware,\nensuring that our dataset serves to strengthen defenses with-\nout creating new security risks.\nWe acknowledge the dual-use nature of cybersecurity re-\nsearch, where publicizing effective methods could inform\nadversarial strategies. However, we contend that the net ef-\nfect of this open research is overwhelmingly positive for de-\nfenders. Our focus on dynamic behavior is inherently more\nrobust against the common obfuscation techniques used by\nattackers. By providing a systematic framework and shar-\ning our findings, we aim to level the playing field, giving\ndefenders, especially those at smaller or under-resourced or-\nganizations, the tools and knowledge to adapt more quickly.\nWe believe the societal benefits of advancing defensive capa-\nbilities through open, responsible research significantly out-\nweigh the inherent risks.\nReferences\nAboaoja, F. A.; Zainal, A.; Ghaleb, F. A.; Al-Rimy, B. A. S.;\nEisa, T. A. E.; and Elnour, A. A. H. 2022. Malware detection\nissues, challenges, and future directions: A survey. Applied\nSciences, 12(17): 8482.\nBreiman, L. 2001.\nRandom forests.\nMachine learning,\n45(1): 5–32.\nComaniciu, D.; and Meer, P. 2002. Mean shift: A robust\napproach toward feature space analysis. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 24(5): 603–\n619.\nCortes, C.; and Vapnik, V. 1995. Support-vector networks.\nMachine learning, 20(3): 273–297.\nDe Go¨er, F.; Rawat, S.; Andriesse, D.; Bos, H.; and Groz,\nR. 2018. Now you see me: Real-time dynamic function call\ndetection. In Proceedings of the 34th Annual Computer Se-\ncurity Applications Conference, 618–628.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nFeng, P.; Wei, D.; Li, Q.; Wang, Q.; Hu, Y.; Xi, N.; and Ma,\nJ. 2024. GlareShell: Graph learning-based PHP webshell\ndetection for web server of industrial internet. Computer\nNetworks, 245: 110406.\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple con-\ntrastive learning of sentence embeddings. In Proceedings of\nthe 2021 conference on empirical methods in natural lan-\nguage processing, 6894–6910.\nHan, F.; Cui, H.; Guo, L.; Wang, Z.; and Lyu, Z. 2025a.\nRead Before You Think: Mitigating LLM Comprehension\nFailures with Step-by-Step Reading. arXiv:2504.09402.\nHan, F.; Yu, X.; Tang, J.; Rao, D.; Du, W.; and Un-\ngar, L. 2025b. ZeroTuning: Unlocking the Initial Token’s\nPower to Enhance Large Language Models Without Train-\ning. arXiv:2505.11739.\nHan, F.; Zhang, J.; Deng, C.; Tang, J.; and Liu, Y. 2025c.\nCan llms handle webshell detection? overcoming detec-\ntion challenges with behavioral function-aware framework.\narXiv preprint arXiv:2504.13811.\nHannousse, A.; and Yahiouche, S. 2021. Handling webshell\nattacks: A systematic mapping and survey. Computers &\nSecurity, 108: 102366.\nJinping, L.; Zhi, T.; Jian, M.; Zhiling, G.; and Jiemin, Z.\n2020. Mixed-models method based on machine learning in\ndetecting webshell attack. In Proceedings of the 2020 Inter-\nnational Conference on Computers, Information Processing\nand Advanced Education, 251–259.\nKipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nLe, H. V.; Nguyen, T. N.; Nguyen, H. N.; and Le, L. 2021.\nAn efficient hybrid webshell detection method for webserver\nof marine transportation systems. IEEE Transactions on In-\ntelligent Transportation Systems, 24(2): 2630–2642.\nMa, M.; Han, L.; and Zhou, C. 2024.\nResearch and\napplication of artificial intelligence based webshell de-\ntection model: A literature review.\narXiv preprint\narXiv:2405.00066.\nMacQueen, J. 1967. Some methods for classification and\nanalysis of multivariate observations.\nIn Proceedings of\nthe fifth Berkeley symposium on mathematical statistics and\nprobability, volume 1, 281–297. University of California\nPress.\nMarzal, A.; and Vidal, E. 1993. Computation of normalized\nedit distance and applications. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 15(9): 926–932.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-\nficient estimation of word representations in vector space.\narXiv preprint arXiv:1301.3781.\nNarayanan, A.; Chandramohan, M.; Venkatesan, R.; Chen,\nL.; and Liu, Y. 2017. graph2vec: Learning distributed repre-\nsentations of graphs. arXiv preprint arXiv:1707.05005.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nPu, A.; Feng, X.; Zhang, Y.; Wan, X.; Han, J.; and Huang,\nC. 2022. BERT-Embedding-Based JSP Webshell Detection\non Bytecode Level Using XGBoost. Security and Commu-\nnication Networks, 2022(1): 4315829.\nShang, M.; Han, X.; Zhao, C.; Cui, Z.; Du, D.; and Jiang, B.\n2024. Multi-language webshell detection based on abstract\nsyntax tree and treelstm. In 2024 27th International Confer-\nence on Computer Supported Cooperative Work in Design\n(CSCWD), 377–382. IEEE.\n"}, {"page": 9, "text": "Shervashidze, N.; Schweitzer, P.; van Leeuwen, E. J.;\nMehlhorn, K.; and Borgwardt, K. M. 2011.\nWeisfeiler-\nlehman graph kernels.\nJournal of Machine Learning Re-\nsearch, 12(9): 2539–2561.\nStarov, O.; Dahse, J.; Ahmad, S. S.; Holz, T.; and Niki-\nforakis, N. 2016. No honor among thieves: A large-scale\nanalysis of malicious web shells. In Proceedings of the 25th\nInternational Conference on World Wide Web, 1021–1032.\nTu, T. D.; Guang, C.; Xiaojun, G.; and Wubin, P. 2014. Web-\nshell detection techniques in web applications. In Fifth In-\nternational Conference on Computing, Communications and\nNetworking Technologies (ICCCNT), 1–7. IEEE.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y. 2018. Graph attention networks. stat,\n1050(20): 10–48550.\nWrench, P. M.; and Irwin, B. V. 2015. Towards a PHP web-\nshell taxonomy using deobfuscation-assisted similarity anal-\nysis. In 2015 Information Security for South Africa (ISSA),\n1–8. IEEE.\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2018.\nHow powerful are graph neural networks? arXiv preprint\narXiv:1810.00826.\nXu, Y.; and Chen, Z. 2023. Family classification based on\ntree representations for malware. In Proceedings of the 14th\nACM SIGOPS Asia-Pacific Workshop on Systems, 65–71.\nZhang, Y.; Kang, H.; and Wang, Q. 2025. MMFDetect: Web-\nshell Evasion Detect Method Based on Multimodal Feature\nFusion. Electronics, 14(3): 416.\nZhao, Y.; Lv, S.; Long, W.; Fan, Y.; Yuan, J.; Jiang, H.; and\nZhou, F. 2024. Malicious webshell family dataset for web-\nshell multi-classification research. Visual Informatics, 8(1):\n47–55.\n"}, {"page": 10, "text": "Implementation Details\nThis appendix provides detailed hyperparameter settings for our representation methods and downstream classifiers to ensure\nthe reproducibility of our experiments. All models were implemented using Python 3.8 with PyTorch 1.12 and Scikit-learn 1.1.\nExperiments were conducted on a server equipped with an Intel Xeon Gold 6248R CPU, 256GB of RAM, and an NVIDIA\nA100 GPU.\nHyperparameters for Representation Methods\nCBOW.\nWe use the Word2Vec implementation from the Gensim library. The model is configured with a vector dimensionality\nof 128, a context window size of 5, 10 negative samples, and is trained for 100 epochs. A minimum word count of 2 was\nenforced. For the ‘concat‘ aggregation strategy, sequences longer than the maximum length of 256 are truncated.\nGloVe.\nWe use the official GloVe implementation. The model is configured with a context window size of 10 and an embed-\nding dimensionality of 128. The weighting function parameter ‘xmax‘ is set to 100, and the exponent ‘alpha‘ is set to 0.75. The\nmodel was trained for 100 epochs using the Adam optimizer with a learning rate of 0.001 and a batch size of 512.\nBERT & SimCSE.\nWe utilize the ‘bert-base-uncased‘ architecture from the Hugging Face Transformers library as the foun-\ndation for both BERT and SimCSE. The model consists of 12 transformer layers, 12 attention heads, and a hidden size of 768,\nwhich is then projected to a final embedding of 128 dimensions via a linear layer. For pre-training, we construct a domain-\nspecific corpus where each line contains two randomly selected function call sequences. The model is trained for 10 epochs\nwith a batch size of 64, a learning rate of 2e-5, and the AdamW optimizer. For SimCSE, we use a dropout rate of 0.1 as the\nnoise operator for the contrastive learning objective.\nGraph Kernels.\nWe use the ‘gklearn‘ library. For the Weisfeiler-Lehman (WL) subtree kernel, the number of iterations was\nset to 5. For the Random Walk (RW) kernel, the random walk length was set to 10.\nGraph Edit Distance (GED).\nOur GED computation is implemented using the ‘gklearn‘ library with the following settings:\n• Edit Costs: We define a constant edit cost vector of ‘[1, 1, 1, 1, 1, 1]‘ for node/edge deletion, insertion, and substitution.\nThis uniform cost treats all structural changes equally.\n• GED Algorithm: We use the BIPARTITE graph matching algorithm for its efficiency on large-scale graph data.\nGraph2Vec.\nWe use the official implementation of Graph2Vec. The model is configured with an embedding dimensionality\nof 128, 10 negative samples, and a WL subtree height of 3. It was trained for 100 epochs with a learning rate of 0.025.\nGraph Neural Networks (GNNs).\nAll GNN models (GCN, GAT, GIN) were implemented using PyTorch Geometric. Each\nmodel consists of 3 GNN layers followed by a global mean pooling layer and a 2-layer MLP head to produce the final 128-\ndimensional embedding. We trained for 200 epochs using the Adam optimizer with a learning rate of 0.001 and a batch size of\n64. A dropout rate of 0.5 was applied after each GNN layer. For GAT, we used 4 attention heads.\nHyperparameters for Downstream Classifiers\nTo ensure a fair and robust comparison, we used a fixed set of optimized hyperparameters for our downstream classifiers,\nidentified via grid search on a validation set.\nK-Means.\nThe number of clusters (‘k‘) is set to the ground-truth number of families in each dataset. We used the ”k-means++”\ninitialization method and set ‘n init‘ to 10 to ensure stability.\nMean-Shift.\nThe bandwidth parameter was automatically estimated using the ‘estimate bandwidth‘ function from Scikit-\nlearn on a sample of the data.\nRandom Forest.\nThe model is configured with 100 estimators (trees), a maximum depth of 10, and a minimum of 5 samples\nrequired to split an internal node.\nSupport Vector Machine (SVM).\nWe use an SVM with a radial basis function (RBF) kernel. The regularization parameter\n‘C‘ is set to 1.0, and the kernel coefficient ‘gamma‘ is set to ‘scale‘.\n"}, {"page": 11, "text": "Prompt Templates for LLM-Powered Data Augmentation\nPrompt for Intra-Family Augmentation\nThe following prompt was used to generate new, diverse samples for existing WebShell families. The goal was to create traces\nthat are behaviorally consistent with the target family while introducing syntactic variations.\nPrompt for Intra-Family Data Augmentation\nSystem Prompt: You are a cybersecurity expert specializing in malware analysis. Your task is to generate new, plausible\ndynamic function call traces for a specific WebShell family. The generated traces must be behaviorally consistent with\nthe provided description and examples, but should introduce minor variations to enhance data diversity.\nUser Prompt: Based on the following behavioral profile and examples for the [Family Name] WebShell family, please\ngenerate 10 new and unique dynamic function call traces.\n[Behavioral Description]\nFor Example: This family typically uses base64 decoding on POST data and then executes the result using an ‘eval‘ or\n‘assert‘ call. It often includes file manipulation functions like ‘fopen‘ and ‘fwrite‘ for persistence.\n[Example Traces]\nFor Example:\n1. [” main ”, ”base64 decode”, ”eval”, ”zend fetch r post”, ...]\n2. [” main ”, ”zend compile string”, ”assert”, ”base64 decode”, ...]\nOutput:\nPrompt for New Family Simulation\nThis prompt was designed to simulate adversarial innovation by instructing the LLM to create a novel WebShell family. This is\nachieved by blending the characteristics of two existing families, thereby generating data for zero-day threat scenarios.\nPrompt for New Family & Zero-Day Simulation\nSystem Prompt: You are an expert malware author. Your objective is to design a novel WebShell family by creatively\nblending the characteristics of two existing malware families. First, describe the core behavior and tactics of your new\ncreation. Then, generate function call traces that reflect this new, hybrid behavior.\nUser Prompt: Design a new WebShell family that combines the stealth techniques of [Family A Name] with the\ncommand execution capabilities of [Family B Name].\n1. Provide a short description of the new family’s behavior.\n2. Generate 10 dynamic function call traces for this new family.\n[Family A Profile: Name and Behavioral Description]\ne.g., Family A (Stealthy Dropper): Focuses on obfuscation using string manipulation functions and avoids direct execu-\ntion calls. It writes payloads to temporary files.\n[Family B Profile: Name and Behavioral Description]\ne.g., Family B (Powerful C2): Uses direct command execution via ‘shell exec‘ and ‘system‘ and communicates over raw\nsockets.\n[Example Traces from Family A & B]\nOutput:\n"}, {"page": 12, "text": "Results for DS1, DS2, and DS3\nModels\nRepresentation Methods\nDataset 1\nKM-ACC\nKM-NMI\nMS-ACC\nMS-NMI\nRF-ACC\nRF-F1\nSVM-ACC\nSVM-F1\nSequence-\nBased\nWord2Vec\nConcat\n0.801 \n0.916 \n0.640 \n0.828 \n0.977 \n0.950 \n0.980 \n0.960 \nAvg\n0.870 \n0.936 \n0.677 \n0.834 \n0.967 \n0.922 \n0.782 \n0.627 \nConcat & Avg\n0.810 \n0.895 \n0.640 \n0.809 \n0.971 \n0.952 \n0.416 \n0.247 \nGloVe\nConcat\n0.817 \n0.923 \n0.666 \n0.841 \n0.980 \n0.964 \n0.975 \n0.953 \nAvg\n0.874 \n0.940 \n0.704 \n0.846 \n0.973 \n0.952 \n0.758 \n0.629 \nConcat & Avg\n0.846 \n0.908 \n0.677 \n0.835 \n0.977 \n0.953 \n0.322 \n0.209 \nBert\nConcat\n0.782 \n0.905 \n0.598 \n0.807 \n0.979 \n0.961 \n0.977 \n0.954 \nAvg\n0.882 \n0.936 \n0.674 \n0.828 \n0.980 \n0.968 \n0.973 \n0.944 \nCLS\n0.859 \n0.930 \n0.746 \n0.872 \n0.977 \n0.953 \n0.979 \n0.964 \nSimCSE\nConcat\n0.849 \n0.932 \n0.715 \n0.880 \n0.962 \n0.910 \n0.969 \n0.950 \nAvg\n0.881 \n0.939 \n0.746 \n0.877 \n0.939 \n0.883 \n0.969 \n0.956 \nCLS\n0.860 \n0.927 \n0.647 \n0.835 \n0.911 \n0.853 \n0.969 \n0.934 \nGraph-Based\nGraph \nKernel\nPath\n0.973 \n0.977 \n0.738 \n0.860 \n0.965 \n0.937 \n0.979 \n0.965 \nWalk\n0.825 \n0.925 \n0.662 \n0.817 \n0.962 \n0.925 \n0.971 \n0.935 \nSubTree\n0.916 \n0.959 \n0.674 \n0.854 \n0.971 \n0.935 \n0.926 \n0.902 \nGraph Edit Distance\n0.911 \n0.947 \n0.658 \n0.830 \n0.969 \n0.943 \n0.977 \n0.949 \nGNN\nGCN\n0.980 \n0.985 \n0.621 \n0.798 \n0.988 \n0.988 \n0.979 \n0.951 \nGAT\n0.980 \n0.985 \n0.693 \n0.848 \n0.980 \n0.968 \n0.984 \n0.981 \nGIN\n0.787 \n0.917 \n0.602 \n0.804 \n0.965 \n0.926 \n0.980 \n0.975 \nGraph Embedding (Graph2Vec)\n0.846 \n0.933 \n0.681 \n0.854 \n0.975 \n0.949 \n0.960 \n0.919 \nTree-Based\nTree Kernel\nPath\n0.973 \n0.977 \n0.711 \n0.859 \n0.965 \n0.922 \n0.977 \n0.950 \nWalk\n0.646 \n0.818 \n0.670 \n0.821 \n0.958 \n0.908 \n0.963 \n0.932 \nSubTree\n0.916 \n0.958 \n0.628 \n0.823 \n0.969 \n0.939 \n0.952 \n0.923 \nTree Edit Distance\n0.841 \n0.921 \n0.647 \n0.831 \n0.962 \n0.920 \n0.975 \n0.951 \nGNN\nGCN\n0.958 \n0.976 \n0.681 \n0.866 \n0.984 \n0.981 \n0.980 \n0.975 \nGAT\n0.980 \n0.985 \n0.681 \n0.851 \n0.977 \n0.961 \n0.988 \n0.988 \nGIN\n0.752 \n0.891 \n0.643 \n0.833 \n0.971 \n0.936 \n0.980 \n0.969 \nTree Embedding (Graph2Vec)\n0.874 \n0.941 \n0.677 \n0.856 \n0.977 \n0.958 \n0.967 \n0.936 \nFigure .3: Performance comparison of all representation methods on the DS1 dataset.\n"}, {"page": 13, "text": "Models\nRepresentation Methods\nDataset 2\nKM-ACC\nKM-NMI\nMS-ACC\nMS-NMI\nRF-ACC\nRF-F1\nSVM-ACC\nSVM-F1\nSequence-\nBased\nWord2Vec\nConcat\n0.785 \n0.910 \n0.529 \n0.732 \n0.967 \n0.944 \n0.966 \n0.928 \nAvg\n0.878 \n0.935 \n0.547 \n0.753 \n0.973 \n0.960 \n0.917 \n0.834 \nConcat & Avg\n0.715 \n0.865 \n0.487 \n0.722 \n0.971 \n0.932 \n0.506 \n0.265 \nGloVe\nConcat\n0.784 \n0.904 \n0.516 \n0.714 \n0.962 \n0.929 \n0.970 \n0.935 \nAvg\n0.914 \n0.954 \n0.603 \n0.803 \n0.975 \n0.967 \n0.961 \n0.941 \nConcat & Avg\n0.709 \n0.866 \n0.487 \n0.722 \n0.966 \n0.923 \n0.499 \n0.227 \nBert\nConcat\n0.772 \n0.891 \n0.439 \n0.669 \n0.972 \n0.945 \n0.973 \n0.956 \nAvg\n0.823 \n0.911 \n0.543 \n0.760 \n0.955 \n0.942 \n0.967 \n0.941 \nCLS\n0.839 \n0.917 \n0.582 \n0.782 \n0.959 \n0.939 \n0.965 \n0.949 \nSimCSE\nConcat\n0.748 \n0.880 \n0.539 \n0.703 \n0.906 \n0.805 \n0.937 \n0.869 \nAvg\n0.803 \n0.904 \n0.456 \n0.694 \n0.918 \n0.845 \n0.933 \n0.871 \nCLS\n0.743 \n0.887 \n0.454 \n0.598 \n0.908 \n0.794 \n0.933 \n0.877 \nGraph-Based\nGraph \nKernel\nPath\n0.846 \n0.935 \n0.508 \n0.727 \n0.977 \n0.969 \n0.979 \n0.973 \nWalk\n0.667 \n0.843 \n0.466 \n0.663 \n0.972 \n0.941 \n0.973 \n0.959 \nSubTree\n0.884 \n0.944 \n0.491 \n0.711 \n0.975 \n0.952 \n0.971 \n0.960 \nGraph Edit Distance\n0.752 \n0.890 \n0.531 \n0.754 \n0.967 \n0.931 \n0.971 \n0.931 \nGNN\nGCN\n0.903 \n0.956 \n0.543 \n0.756 \n0.980 \n0.975 \n0.972 \n0.939 \nGAT\n0.908 \n0.966 \n0.545 \n0.755 \n0.983 \n0.980 \n0.980 \n0.967 \nGIN\n0.624 \n0.832 \n0.500 \n0.707 \n0.968 \n0.953 \n0.984 \n0.983 \nGraph Embedding (Graph2Vec)\n0.785 \n0.894 \n0.524 \n0.749 \n0.962 \n0.922 \n0.971 \n0.948 \nTree-Based\nTree Kernel\nPath\n0.848 \n0.936 \n0.516 \n0.706 \n0.973 \n0.951 \n0.977 \n0.973 \nWalk\n0.471 \n0.704 \n0.394 \n0.621 \n0.952 \n0.902 \n0.969 \n0.945 \nSubTree\n0.917 \n0.959 \n0.551 \n0.774 \n0.978 \n0.966 \n0.980 \n0.974 \nTree Edit Distance\n0.734 \n0.882 \n0.518 \n0.759 \n0.960 \n0.920 \n0.960 \n0.900 \nGNN\nGCN\n0.860 \n0.947 \n0.562 \n0.757 \n0.983 \n0.983 \n0.973 \n0.955 \nGAT\n0.924 \n0.967 \n0.562 \n0.758 \n0.984 \n0.983 \n0.980 \n0.967 \nGIN\n0.619 \n0.833 \n0.524 \n0.743 \n0.974 \n0.964 \n0.985 \n0.985 \nTree Embedding (Graph2Vec)\n0.671 \n0.864 \n0.493 \n0.738 \n0.973 \n0.962 \n0.959 \n0.942 \nFigure .4: Performance comparison of all representation methods on the DS2 dataset.\n"}, {"page": 14, "text": "Models\nRepresentation Methods\nDataset 3\nKM-ACC\nKM-NMI\nMS-ACC\nMS-NMI\nRF-ACC\nRF-F1\nSVM-ACC\nSVM-F1\nSequence-\nBased\nWord2Vec\nConcat\n0.742 \n0.869 \n0.492 \n0.681 \n0.974 \n0.970 \n0.972 \n0.942 \nAvg\n0.819 \n0.917 \n0.536 \n0.704 \n0.963 \n0.940 \n0.895 \n0.725 \nConcat & Avg\n0.709 \n0.844 \n0.517 \n0.721 \n0.959 \n0.942 \n0.655 \n0.331 \nGloVe\nConcat\n0.768 \n0.880 \n0.625 \n0.773 \n0.969 \n0.955 \n0.968 \n0.954 \nAvg\n0.806 \n0.907 \n0.541 \n0.707 \n0.964 \n0.953 \n0.903 \n0.784 \nConcat & Avg\n0.727 \n0.857 \n0.611 \n0.768 \n0.961 \n0.933 \n0.710 \n0.365 \nBert\nConcat\n0.719 \n0.852 \n0.534 \n0.691 \n0.973 \n0.971 \n0.962 \n0.949 \nAvg\n0.665 \n0.817 \n0.534 \n0.694 \n0.942 \n0.904 \n0.938 \n0.895 \nCLS\n0.640 \n0.812 \n0.570 \n0.705 \n0.945 \n0.915 \n0.929 \n0.843 \nSimCSE\nConcat\n0.687 \n0.859 \n0.586 \n0.741 \n0.934 \n0.890 \n0.956 \n0.932 \nAvg\n0.635 \n0.826 \n0.587 \n0.735 \n0.916 \n0.824 \n0.943 \n0.906 \nCLS\n0.610 \n0.802 \n0.482 \n0.696 \n0.849 \n0.689 \n0.942 \n0.890 \nGraph-Based\nGraph \nKernel\nPath\n0.808 \n0.896 \n0.426 \n0.659 \n0.963 \n0.945 \n0.965 \n0.948 \nWalk\n0.818 \n0.905 \n0.500 \n0.671 \n0.960 \n0.932 \n0.965 \n0.945 \nSubTree\n0.876 \n0.933 \n0.397 \n0.638 \n0.959 \n0.935 \n0.936 \n0.867 \nGraph Edit Distance\n0.744 \n0.878 \n0.493 \n0.672 \n0.960 \n0.925 \n0.962 \n0.930 \nGNN\nGCN\n0.786 \n0.911 \n0.621 \n0.788 \n0.978 \n0.975 \n0.969 \n0.962 \nGAT\n0.926 \n0.960 \n0.586 \n0.739 \n0.978 \n0.974 \n0.969 \n0.958 \nGIN\n0.628 \n0.857 \n0.465 \n0.692 \n0.971 \n0.964 \n0.978 \n0.977 \nGraph Embedding (Graph2Vec)\n0.792 \n0.897 \n0.619 \n0.769 \n0.961 \n0.947 \n0.957 \n0.927 \nTree-Based\nTree Kernel\nPath\n0.809 \n0.897 \n0.392 \n0.643 \n0.956 \n0.933 \n0.965 \n0.952 \nWalk\n0.792 \n0.888 \n0.518 \n0.654 \n0.951 \n0.915 \n0.954 \n0.914 \nSubTree\n0.895 \n0.938 \n0.432 \n0.657 \n0.968 \n0.949 \n0.960 \n0.935 \nTree Edit Distance\n0.741 \n0.878 \n0.499 \n0.697 \n0.964 \n0.947 \n0.967 \n0.948 \nGNN\nGCN\n0.850 \n0.939 \n0.611 \n0.784 \n0.979 \n0.978 \n0.973 \n0.965 \nGAT\n0.943 \n0.965 \n0.637 \n0.797 \n0.979 \n0.979 \n0.976 \n0.966 \nGIN\n0.586 \n0.829 \n0.453 \n0.638 \n0.964 \n0.936 \n0.976 \n0.974 \nTree Embedding (Graph2Vec)\n0.788 \n0.896 \n0.638 \n0.786 \n0.964 \n0.955 \n0.956 \n0.931 \nFigure .5: Performance comparison of representation methods on the DS3 dataset.\n"}]}