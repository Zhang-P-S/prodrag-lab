{"doc_id": "arxiv:2601.17284", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.17284.pdf", "meta": {"doc_id": "arxiv:2601.17284", "source": "arxiv", "arxiv_id": "2601.17284", "title": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering", "authors": ["Yaokun Liu", "Yifan Liu", "Phoebe Mbuvi", "Zelin Li", "Ruichen Yao", "Gawon Lim", "Dong Wang"], "published": "2026-01-24T03:44:08Z", "updated": "2026-01-24T03:44:08Z", "summary": "The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided \"Clarify-Before-Answer\" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.17284v1", "url_pdf": "https://arxiv.org/pdf/2601.17284.pdf", "meta_path": "data/raw/arxiv/meta/2601.17284.json", "sha256": "617df430ebff4fb2782b5fdfd123fa72cd7a86f6d74726fcd49720ffe7f9eebb", "status": "ok", "fetched_at": "2026-02-18T02:20:36.288687+00:00"}, "pages": [{"page": 1, "text": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in\nLLMs for Safe Medical Question Answering\nYaokun Liu\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United State\nyaokunl2@illinois.edu\nYifan Liu\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United States\nyifan40@illinois.edu\nPhoebe Mbuvi\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United States\npmbuvi2@illinois.edu\nZelin Li\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United States\nzelin3@illinois.edu\nRuichen Yao\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United States\nryao8@illinois.edu\nGawon Lim\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United States\ngawonl2@illinois.edu\nDong Wang\nUniversity of Illinois\nUrbana-Champaign\nChampaign, IL, United States\ndwang24@illinois.edu\nAbstract\nThe deployment of Large Language Models in Medical Question\nAnswering is severely hampered by ambiguous user queries, a sig-\nnificant safety risk that demonstrably reduces answer accuracy in\nhigh-stakes healthcare settings. In this paper, we formalize this\nchallenge by linking input ambiguity to aleatoric uncertainty (AU),\nwhich is the irreducible uncertainty arising from underspecified\ninput. To facilitate research in this direction, we construct CV-\nMedBench, the first benchmark designed for studying input ambi-\nguity in Medical QA. Using this benchmark, we analyze AU from\na representation engineering perspective, revealing that AU is lin-\nearly encoded in LLM‚Äôs internal activation patterns. Leveraging this\ninsight, we introduce a novel AU-guided ‚ÄúClarify-Before-Answer‚Äù\nframework, which incorporates AU-Probe‚Äîa lightweight module\nthat detects input ambiguity directly from hidden states. Unlike\nexisting uncertainty estimation methods, AU-Probe requires nei-\nther LLM fine-tuning nor multiple forward passes, enabling an\nefficient mechanism to proactively request user clarification and\nsignificantly enhance safety. Extensive experiments across four\nopen LLMs demonstrate the effectiveness of our QA framework,\nwith an average accuracy improvement of 9.48% over baselines.\nOur framework provides an efficient and robust solution for safe\nMedical QA, strengthening the reliability of health-related appli-\ncations. The code is available at https://github.com/yaokunliu/AU-\nMed.git, and the CV-MedBench dataset is released on Hugging Face\nat https://huggingface.co/datasets/yaokunl/CV-MedBench.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nWWW ‚Äô26, Dubai, United Arab Emirates\n¬© 2026 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-2307-0/2026/04\nhttps://doi.org/10.1145/3774904.3793027\nCCS Concepts\n‚Ä¢ Computing methodologies ‚ÜíNatural language processing;\nUncertainty quantification; ‚Ä¢ Applied computing ‚ÜíHealth\ninformatics.\nKeywords\nLarge Language Models, Medical Question Answering, Aleatoric\nUncertainty, Representation Engineering, Ambiguity Detection\nACM Reference Format:\nYaokun Liu, Yifan Liu, Phoebe Mbuvi, Zelin Li, Ruichen Yao, Gawon Lim,\nand Dong Wang. 2026. Mind the Ambiguity: Aleatoric Uncertainty Quan-\ntification in LLMs for Safe Medical Question Answering. In Proceedings of\nthe ACM Web Conference 2026 (WWW ‚Äô26), April 13‚Äì17, 2026, Dubai, United\nArab Emirates. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/\n3774904.3793027\n1\nIntroduction\nLarge Language Models (LLMs) are increasingly adopted as conve-\nnient and affordable tools for medical question answering (Medical\nQA) [28, 39, 42]. In 2024, around 31% of U.S. adults reported using\nLLMs to seek preliminary explanations or advice about symptoms\nrather than immediately consulting healthcare professionals [5].\nAlthough LLMs broaden access to medical knowledge, they are\nalso exposed to a critical safety issue: users often submit highly\nambiguous queries [4]. For example, a user might ask ‚ÄúCan I take\nibuprofen for pain?‚Äù without mentioning essential demographics or\nsymptom details. This input ambiguity prevents LLM from identify-\ning patient-specific contraindications and raises the risk of unsafe\nmedical advice [40], as shown in Figure 1. Our empirical evidence\nfurther confirms this safety concern: we observe a significant drop\nin response accuracy across LLMs when queries are ambiguous\n(Figure 2). In high-stakes medical decision support, these unsafe\narXiv:2601.17284v1  [cs.CL]  24 Jan 2026\n"}, {"page": 2, "text": "WWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nYaokun Liu et al.\nAmbiguous Query\nYes, ibuprofen is generally \neffective for pain relief. \nClear Query\nI am 45, on blood thinners. Can \nI take ibuprofen for back pain?\nNo. Ibuprofen can increase \nbleeding risk when combined \nwith blood thinners.\nCan I take ibuprofen for pain?\nUnsafe Response - High Risk\nReliable Output ‚Äì Low Risk\nFigure 1: Effect of input ambiguity on medical LLM re-\nsponses. Ambiguous queries lead to unsafe outputs, while\nclear queries enable reliable reasoning.\nQwen2.5-7B-Instruct Llama-3.1-8B-Instruct\nBioMistral-7B\nBio-Medical-Llama-3-8B\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Accuracy\n59%\n49%\n56%\n50%\n73%\n64%\n58%\n48%\n54%\n48%\n68%\n61%\n41%\n36%\n41%\n36%\n58%\n46%\n77%\n61%\n74%\n56%\n70%\n59%\nMedQA-Clear\nMedQA-Ambiguous\nMedMCQA-Clear\nMedMCQA-Ambiguous\nMedExQA-Clear\nMedExQA-Ambiguous\nFigure 2: Input ambiguity reduces answer accuracy across\nmodels and datasets.\nresponses are consequential, as they may mislead users and endan-\nger patient health. Therefore, there is an urgent need for methods\nto detect and manage the risks posed by ambiguous queries.\nA promising solution to detect ambiguity risks lies in Aleatoric\nUncertainty (AU) quantification, which captures the uncertainty\ninherently in the input. Specifically, AU represents the intrinsic\nvariability in the underlying answer distribution, which constitutes\nan important component of model uncertainty [15]. In Medical QA,\naleatoric uncertainty directly reflects the degree of ambiguity in\nuser queries [20, 24]. For example, when a user asks \"Can I take\nibuprofen to relieve pain?\" without specifying the type of pain\nor clinical context, the model may generate a variety of possible\nanswers, resulting in a higher AU. In this work, we aim to quantify\nAU for user queries and leverage it as a principled indicator for\nidentifying inputs whose ambiguity poses a risk to answer safety.\nHowever, existing methods for uncertainty quantification (UQ) in\nLLMs face two major challenges when applied to AU quantification\nin medical QA. Firstly, many UQ methods estimate total predictive\nuncertainty rather than isolating the aleatoric component [2, 7, 8,\n17]. These methods are validated on controlled benchmarks where\ninput ambiguity (i.e., AU) is minimal or absent. While these methods\nperform well under such idealized conditions, prior studies reveal\nthat their performance degrades sharply when confronted with\nreal-world query ambiguity [35]. Secondly, existing methods that\nexplicitly quantify AU in LLMs are computationally prohibitive,\nmaking them unsuitable for time-sensitive clinical scenarios where\nresponse latency is critical. Specifically, existing methods can be\ncategorized into sampling-based [6, 10, 14, 21] and ensemble-based\nmethods [1, 9, 13]. Sampling-based methods require multiple rounds\nof generation to capture variability across outputs, while ensemble-\nbased methods compare predictions across several independently\ntrained QA models. Both categories require repeated forward passes\nor model replicas, making them computationally expensive and\nimpractical for deployment. These limitations motivate our key\nresearch question: How can the AU of medical QA be quantified both\naccurately and efficiently to proactively detect ambiguity risks?\nRecent advances in representation engineering show that LLMs\nencode high-level semantic attributes, such as sentiment and factu-\nality, along approximately linear directions in their activation space,\nalso referred to as the Linear Representation Hypothesis [26, 27, 44].\nInspired by these findings, in this paper, we examine whether AU\nis also linearly encoded in the internal states of LLMs. Our analy-\nsis reveals a striking pattern: activations from low-AU (clear) and\nhigh-AU (ambiguous) medical queries can be separated by a linear\nboundary. This observation indicates that AU can be estimated\ndirectly from internal activations of LLMs, allowing inference-time\nAU quantification before answer generation.\nBuilding on this observation, we propose the AU-Guided ‚ÄúClarify-\nBefore-Answer‚Äù Medical QA Framework, which detects AU directly\nfrom the internal states of LLMs before answer generation, enabling\ntimely detection and proactive mitigation of ambiguity-induced\nrisks. This framework is enabled by three core contributions:\n‚Ä¢ Clear-to-Vague Benchmark (CV-MedBench). We construct\nCV-MedBench, the first Medical QA benchmark that includes\nsystematically generated ambiguous questions, addressing the\ngap of existing datasets that do not capture realistic user input\nambiguity. This benchmark undergoes rigorous human verifi-\ncation, validating its quality and suitability for the systematic\nstudies of input ambiguity in Medical QA.\n‚Ä¢ Aleatoric Uncertainty Probe (AU-Probe). We introduce the\nAU-Probe, a lightweight plug-in module designed to quantify\nAU directly from the LLM‚Äôs internal activations. By operating on\nhidden states, the AU-Probe requires no repeated forward passes\nor LLM fine-tuning, ensuring real-time and easily deployable AU\nestimation before model answer generation.\n‚Ä¢ AU-Guided ‚ÄúClarify-Before-Answer‚Äù Pipeline. We propose\nthe first framework that employs AU as an explicit guiding signal\nfor a \"Clarify-Before-Answer\" pipeline. In the first stage, AU-\nProbe estimates the AU score of each incoming question before\nanswer generation, providing an early indicator of input ambigu-\nity. When the AU score exceeds the safety threshold, the system\nproactively requests clarification from the user. Once the query is\nsufficiently clarified, in the second stage, the LLM proceeds with\nanswer generation. This pipeline enables early risk detection,\nreduces ambiguity-induced errors, and substantially enhances\nthe safety and reliability of Medical QA systems.\nExtensive experiments across four open-source LLMs demonstrate\nthe effectiveness of our methods: AU-Probe achieves substantially\nstronger ambiguity discrimination with an average AUROC im-\nprovement of 49.24% over baselines, while the AU-guided clarifi-\ncation pipeline improves answer accuracy of LLMs by 9.48% with\nhigh efficiency. Overall, our framework strengthens the safety and\nreliability of Medical QA systems, supporting more trustworthy\nhealth-related applications and clinical decision support.\n2\nRelated Works\n2.1\nLLMs for Medical QA\nMedical QA is a particularly demanding task that requires pre-\ncise domain-specific reasoning under strict accuracy requirements,\nas even minor errors in clinical scenarios can lead to unsafe or\n"}, {"page": 3, "text": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nmisleading guidance, especially for non-expert users. To address\nthese challenges, recent efforts have applied general-purpose and\ndomain-adapted LLMs to medical QA [32, 43]. Med-PaLM [29]\nadapts instruction-tuned models to medical domains and achieves\nexpert-level accuracy on benchmark exams. PMC-LLaMA [42] lever-\nages PubMed Central articles to enhance domain alignment, while\nBioGPT [23] pretrains on biomedical corpora for improved biomed-\nical reasoning. MedGemma [28] optimizes a general-purpose LLM\nfor multimodal medical text and image comprehension.\nWhile these models focus on enhancing knowledge coverage\nand generation accuracy, little attention has been paid to input\nambiguity‚Äìa common source of risk in real-world applications,\nleaving a critical gap in the safe and robust deployment of LLM-\nbased medical QA systems. Our method directly bridges this gap\nby enabling early detection of input ambiguity.\n2.2\nUncertainty Quantification in LLMs\nUncertainty quantification aims to assess the reliability of model\noutputs and has become an essential component for detecting hal-\nlucinations and improving the trustworthiness of LLMs. The total\nuncertainty of LLMs can be decomposed into epistemic uncertainty\n(EU) and aleatoric uncertainty (AU). A variety of UQ methods\nhave been explored to quantify total uncertainty, including log-\nprobability entropy [8, 30, 36], self-consistency across sampled\ngenerations [7, 20, 22], LLM-as-a-judge [17, 33], and internal repre-\nsentation‚Äìbased estimators [19, 37]. While effective when inputs\nare clear, these methods for total uncertainty estimation have been\nshown to degrade significantly when queries contain ambiguity,\nrevealing their inability to effectively model aleatoric uncertainty\narising from vague user inputs [35]. To explicitly quantify aleatoric\nuncertainty, recent studies have proposed sampling-based [6, 10,\n14, 21] and ensemble-based [1, 9, 13] strategies. Sampling-based\nmethods estimate aleatoric uncertainty by generating multiple clar-\nifications or perturbations of the same input to measure output\nvariability, while ensemble-based approaches aggregate predictions\nacross auxiliary LLMs to isolateinput-driven uncertainty from epis-\ntemic uncertainty. However, both categories require multiple for-\nward passes or additional model replicas, introducing substantial\ncomputational cost and latency that make them unsuitable for real-\ntime medical QA systems.\nThese limitations highlight the need for an efficient aleatoric\nuncertainty quantification method that can operate within the\nresponse-time constraints of medical QA.\n2.3\nRepresentation Engineering\nRecent advances in representation engineering reveal that LLMs\nencode high-level semantic and behavioral properties along ap-\nproximately linear directions in the hidden activations, enabling\ndetection and manipulation of complex attributes through simple\nlinear operations [26, 27, 44]. Prior studies have demonstrated this\nfor diverse attributes: Tigges et al. [34] show that sentiment is cap-\ntured by a single direction in the residual stream, with ablation\ndegrading prediction accuracy; Waldis et al. [38] show that toxic\nvs. non-toxic content is linearly separable in hidden activations,\nenabling detoxification through vector-based steering; and Arditi et\nal. [3] demonstrate that safety-aligned refusal behavior is governed\nby a dominant direction, allowing precise behavioral control.\nDespite this progress, the linear representational structure of\naleatoric uncertainty, which arises from input ambiguity, remains\nunexplored. Building on these insights, we present the first analysis\nexamining whether representations of clear versus ambiguous medi-\ncal questions form linearly separable patterns in hidden activations.\nOur analyses confirm such geometry, motivating a lightweight\nprobe for efficient AU quantification directly from internal states.\n3\nClear-to-Vague Medical QA Benchmark\nExisting public medical QA datasets predominantly contain ques-\ntions written in clear and detailed clinical language. While suitable\nfor evaluating diagnostic reasoning, such questions do not reflect\nhow ordinary users typically express medical concerns in real-world\nsettings, where queries are often vague, incomplete, or semantically\nimprecise. To address this gap and enable the study of aleatoric\nuncertainty in medical QA, we construct the Clear-to-Vague Med-\nical QA Benchmark (CV-MedBench), which pairs existing clear\nmedical query resources with their ambiguous counterparts.\nIn particular, we collect clear questions from three public medical\nQA datasets MedQA [16], MedMCQA [25], and MedExQA [18], and\nprompt a LLM (e.g., GPT-4o) to rewrite each clear question xclr\ninto an underspecified variant xamb that better reflects real-world\nmedical query ambiguity while preserving the original medical\ntopic. The rewriting process applies the following transformation\ntypes that reflect major sources of real-world input ambiguity:\n‚Ä¢ Context omission: removing essential clinical details that are\nnecessary for precise medical interpretation;\n‚Ä¢ Semantic vagueness: replacing specific descriptions with broader\nor less informative expressions;\n‚Ä¢ Logical inconsistency: introducing mild internal contradictions\nwhile keeping surface-level grammaticality.\nThe selection of transformation types is decided autonomously by\nthe LLM based on its evaluation of question content. The full prompt\ntemplate is provided in Appendix 8.2, enabling the generation of\nambiguous counterparts to be easily extended to other datasets\nand LLMs in practical use. In Figure 12, we show an example clear\nquestion paired with its rewritten ambiguous form.\nTo ensure the quality of the generated ambiguous questions, we\nconducted a human verification study that evaluated each rewritten\nvariant along three dimensions: topic fidelity, ambiguity validity,\nand linguistic fluency. Agreement rates exceeding 95% confirmed\nthat our method successfully introduced real-world ambiguity while\npreserving the original clinical context. Details of the validation\nprotocol and quantitative results are reported in Appendix 8.3.\nUltimately, CV-MedBench provides contrastive question pairs\n(xclr, xamb) that differ only in ambiguity level, which isolate the\neffect of input ambiguity from other linguistic or clinical factors.\nTo the best of our knowledge, this is the first dataset designed to\nsupport systematic investigation of input ambiguity in Medical QA.\n4\nMethodology\nIn this section, we introduce our AU-guided ‚ÄúClarify-Before-Answer‚Äù\nframework for Medical QA, designed to mitigate safety risks caused\nby ambiguous inputs. Figure 3 provides an overview of the two-\nstage pipeline guided by the plug-in AU-Probe.\n"}, {"page": 4, "text": "WWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nYaokun Liu et al.\nAU-probe Training\nTwo-Stage \"Clarify-Before-Answer\" Framework\nAU-probe\nProceed. LLM generates\nmedical response.\nSTAGE 2\nAnswer Generation\nYes - High Ambiguity\nNo - Sufficient Clarity\n>\nThreshold?\nAU Score\n0.85\nAU Quantification\nInternal\nActivation\n‚Ä¶\nLLM\nUser Question: I have a headache, what should I do?\nFlag input. Prompt user\nfor providing details.\nSTAGE 1\nAU-Guided Clarification\n‚Ä¶\nSeparating \nclear vs. amb activations\nRewrite\nClear Question: My 5-year-old\nson developed a rash that started \non his face and quickly spread to \nhis whole body. He has had a \nrunny nose, cough, and red eyes \nfor about a week, and today he \nhas a very high fever. We \nrecently moved here from ‚Ä¶\nAmbiguous Question: My son has a \nrash that got worse over a couple of \ndays. What should I do?\nFigure 3: Overview of the proposed AU-guided ‚ÄúClarify-Before-Answer‚Äù framework. Left: AU-Probe learns a linear separation\nbetween clear and ambiguous activations to predict AU. Right: the two-stage ‚ÄúClarify-Before-Answer‚Äù pipeline detects ambigu-\nous inputs with AU score before answer generation, triggering clarification when needed or answering when the input is clear.\n4.1\nDefinition and Problem Formulation\nLet x denote the input medical question and y denote its correct\nanswer. Formally, aleatoric uncertainty is defined as the entropy of\nthe underlying answer distribution for x:\nùëàaleatoric(x) = H (ùëù(y | x)) ,\n(1)\nwhere ùëù(y | x) denote the answer distribution. Aleatoric uncertainty\ncharacterizes the inherent stochasticity within the data-generating\nprocess and is irreducible even for a perfectly trained model. In\nthis work, we operationalize ùëàaleatoric(x) as the uncertainty arising\nfrom input ambiguity. In the Medical QA task, AU increases when\nunderspecified symptom descriptions cause the answer distribution\nùëù(y | x) to disperse across multiple possible answers. Given a user\ninput x, we aim to estimate its aleatoric uncertainty ÀÜùëàaleatoric(x),\nwhich serves as an indicator of input ambiguity to safeguard the\nreliability of downstream answer generation.\n4.2\nLinear Encoding of Aleatoric Uncertainty\nExisting UQ methods often fail to provide robust AU estimation or\nincur high computational overhead (discussed in Sections 1 and 2.2).\nTo overcome these limitations, we draw inspiration from the Linear\nRepresentation Hypothesis [26] and quantify aleatoric uncertainty\nfrom intermediate LLM activations. Specifically, we utilize the LLM\nas a calibrated instrument to extract input ambiguity features, yield-\ning an AU estimator that is both efficient and accurate.\n4.2.1\nInternal Activation Extraction. To investigate how aleatoric\nuncertainty is encoded within the LLM, we first extract hidden\nrepresentations elicited by clear and ambiguous versions of the\nsame medical question. For each contrastive pair (xclr, xamb) in\nCV-MedBench, we independently feed both inputs into the LLM\nfor medical QA and record the residual-stream activations:\n\u0000a(ùëô) (xclr), a(ùëô) (xamb)\u0001\nfor ùëô= 1, . . . , ùêø,\n(2)\nwhere a(ùëô) (x) ‚ààRùëëdenotes the activation of the final prompt token\nat layer ùëô, and ùêøis the total number of transformer layers. Layers\nare indexed from 1, and activations correspond to the outputs of\ntransformer blocks. We specifically extract the final prompt token‚Äôs\nactivation because it captures the model‚Äôs integrated semantic state\nfor the entire input before answer generation, a representation\nknown to be salient for high-level semantic signals of LLMs [44].\nLayer 1\nLayer 7\nLayer 14\nLayer 21\nLayer 28\nFigure 4: PCA visualization of activations for Qwen2.5-7B-\nInstruct demonstrate layer-dependent linear separability.\nBlue: clear questions. Red: ambiguous questions.\n4.2.2\nLinear Separability Analysis. Having obtained layer-wise\nactivations for clear and ambiguous inputs, we next examine how\nthese activations are organized within the LLM‚Äôs hidden space and\nwhether they form distinguishable geometric patterns that reflect\ndifferences in aleatoric uncertainty.\nIn particular, we project the activations of contrastive question\npairs into a two-dimensional subspace using PCA. Figure 4 shows\nresults for Qwen2.5-7B-Instruct, where clear and ambiguous ques-\ntions form well-separated clusters in several layers of the network.\nThis visual separation provides initial evidence that the internal rep-\nresentations are sensitive to input ambiguity. Critically, this linear\nseparability suggests that the distinction between low-AU (clear)\nand high-AU (ambiguous) states can be defined by a simple hyper-\nplane. Therefore, we posit that aleatoric uncertainty is encoded\nalong a linear direction within the residual stream, a structure that\nenables quantification via a lightweight linear classifier.\nDue to space constraints, we only display a subset of layers and\nonly one model. Similar separability patterns are observed across\nall models evaluated in our experiments (details in Section 8.1.1). A\nquantitative analysis of layer-wise linear separability for all models\nis provided in Section 4.3.2, which identifies the most separable\nlayer for robust AU estimation.\n4.3\nAleatoric Uncertainty Probe\nBuilding on the linear encoding of AU within hidden states, we in-\ntroduce the Aleatoric Uncertainty Probe (AU-Probe), a lightweight\nplug-in module that estimates AU directly from internal activations\nof LLMs and requires no model fine-tuning or additional sampling,\nenabling inference-time AU quantification for real-time medical\nQA systems.\n4.3.1\nProbe Design and Training. For each transformer layer ùëô,\nwe train an independent linear probe to map the activation a(ùëô) (x) ‚àà\n"}, {"page": 5, "text": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\n1\n4\n7\n10\n13\n16\n19\n22\n25\n28\n31\nLayer\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUROC\nLlama-3.1-8B-Instruct\nQwen2.5-7B-Instruct\nBioMistral-7B\nBio-Medical-Llama-3-8B\nFigure 5: Layer-wise AUROC of the AU-Probe across LLMs.\nRùëëto a scalar AU score:\nÀÜùë¢(ùëô) = ùúé\n\u0010\nw(ùëô)‚ä§a(ùëô) (x) + ùëè(ùëô)\u0011\n,\n(3)\nwhere ùúé(¬∑) is the sigmoid function, and w(ùëô),ùëè(ùëô) are the layer-\nspecific learnable parameters. The output ÀÜùë¢(ùëô) serves as the pre-\ndicted aleatoric uncertainty score at layer ùëô. Training is performed\nindependently for all layers ùëô= 1, . . . , ùêø. We optimize the probe\nparameters using the Binary Cross-Entropy (BCE) loss:\nL(ùëô) = ‚àíùë¢log ÀÜùë¢(ùëô) ‚àí(1 ‚àíùë¢) log\n\u0010\n1 ‚àíÀÜùë¢(ùëô)\u0011\n,\n(4)\nwhere clear questions are assigned the labelùë¢= 0 (low AU), whereas\nambiguous questions are assigned the label ùë¢= 1 (high AU).\nNotably, we adopt binary labels (ùë¢‚àà{0, 1}) instead of manually\nassigned continuous AU scores for two reasons. First, fine-grained\naleatoric uncertainty annotation is inherently subjective and prone\nto high inter-annotator variability and label noise [31, 41]. Binary\nlabels, conversely, provide a more stable training signal and substan-\ntially higher inter-annotator agreement in practice (Appendix 8.3).\nSecond, despite binary supervision, the probe‚Äôs sigmoid output pro-\nduces continuous scores in (0, 1), naturally allowing the probe to\nreflect the varying degrees of AU learned implicitly from the data\ndistribution. This design thus ensures stable supervision without\nsacrificing the ability to model graded aleatoric uncertainty.\n4.3.2\nLayer Selection. The strength of AU signals varies across\nlayers. Therefore, we identify the most informative layer by evaluat-\ning linear separability across all layers using a per-layer linear probe\ntrained on the training set. Performance is measured by AUROC,\nwhere higher values indicate stronger linear separability between\nclear and ambiguous questions and thus a more informative AU\nsignal. As shown in Figure 5, the optimal layer differs across mod-\nels. On Llama-3.1-8B-Instruct and BioMistral-7B, the linear probe\nachieves peak AUROC in deeper layers (layers 28‚Äì32), whereas on\nQwen2.5-7B-Instruct and Bio-Medical-Llama-3-8B, the strongest\nseparability is observed in intermediate layers (layers 8‚Äì12).\nAfter selecting the most informative layer ùëô‚àó, the linear probe\ntrained on this layer serves as the AU-Probe. The final AU score\nfor input question x is computed as:\nÀÜùëàaleatoric(x) = ùúé\n\u0010\nw(ùëô‚àó)‚ä§a(ùëô‚àó) (x) + ùëè(ùëô‚àó)\u0011\n,\n(5)\nyielding a continuous value in (0, 1). Higher AU scores indicate\na greater likelihood of input ambiguity, while lower AU scores\nindicate clearer questions. Operating solely on hidden activations\nof LLMs, the AU-Probe functions as a lightweight plug-in module\nthat enables efficient and easily deployable AU quantification.\n4.4\nAU-Guided ‚ÄúClarify-Before-Answer‚Äù\nMedical QA Framework\nTo address the safety risks posed by ambiguous user queries in\nmedical QA, we integrate the AU-Probe into an AU-guided two-\nstage ‚ÄúClarify-Before-Answer\" framework. As shown in Figure 3,\nour framework uses the AU score as an early signal to proactively\ndetect ambiguous questions and ensure necessary clarification is\nobtained before generating a final response.\nStage 1: Ambiguity Detection and Clarification. Given a user\nquery x, the AU-Probe computes ÀÜùëàaleatoric(x) directly from the hid-\nden activation a(ùëô‚àó) (x), providing the model‚Äôs internal estimate of\ninput ambiguity before answer generation. We trigger a clarification\nrequest by comparing the AU score with a threshold ùúè:\nÀÜùëàaleatoric(x) > ùúè\n=‚áí\nflag as ambiguous and request clarification.\nEmpirically, we search thresholds in {0.1, 0.3, 0.5, 0.7, 0.9} based\non ambiguity‚Äìdetection accuracy of AU-probe and find that ùúè= 0.5\nperforms best across datasets and models (Fig. 11 in Appendix).\nThe threshold remains tunable to balance safety and efficiency‚Äîfor\nexample, increasing ùúèreduces false alarms, whereas lowering it\nincreases sensitivity to ambiguity.\nStage 2: Safe Answer Generation. Once the query exhibits\nsufficiently low AU, either because the original input is already\nwell-specified or because clarification has reduced the ambiguity,\nthe system proceeds to answer generation:\nÀÜùëàaleatoric(x) ‚â§ùúè\n=‚áí\nproceed to answer generation.\nIn this stage, the LLM executes answer generation, providing the\nfinal medical response to the well-specified query. This AU-guided\n\"Clarify-Before-Answer\" pipeline offers three advantages:\n1. Plug-in and model-agnostic. AU-Probe operates on hidden\nactivations and requires no LLM fine-tuning, making the framework\ncompatible and easily applied to any model used for medical QA.\n2. Early safety intervention. Ambiguity is detected before\nanswer generation, preventing misinterpretation-induced errors\nand reducing the computational overhead of unnecessary decoding.\n3. Improved reliability for real-world medical QA. By prompt-\ning clarification when ambiguity is detected, the system delivers\nsafer and more contextually grounded answers.\n5\nExperiments\n5.1\nExperimental Setup\nFull details of datasets, models, baselines, and evaluation protocols\nare provided in the Appendix 8.1.\n5.2\nAU Quantification Performance\nSince AU stems from underspecified inputs, an effective AU quantifi-\ncation method should output higher scores to ambiguous questions\nand lower scores to clear ones, allowing the system to identify\nqueries that pose ambiguity-related risk.\nAs shown in Table 1, AU-Probe achieves near-perfect AUROC\nacross all models, demonstrating its strong discriminative ability\nin in-distribution and out-of-distribution settings. These results\nverify that AU forms a linearly separable signal within the internal\nspace of LLMs, and AU-Probe leverages this property effectively to\n"}, {"page": 6, "text": "WWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nYaokun Liu et al.\nTable 1: Performance of UQ methods on medical QA datasets. Best AUROC (higher is better), ECE and Brier (lower is better) per\nmodel and dataset are in bold; second best are underlined.\nModel\nUQ Method\nCV-MedQA\nCV-MedMCQA\nCV-MedExQA\nAUROC ‚Üë\nECE ‚Üì\nBrier ‚Üì\nAUROC ‚Üë\nECE ‚Üì\nBrier ‚Üì\nAUROC ‚Üë\nECE ‚Üì\nBrier ‚Üì\nQwen2.5-7B-Instruct\nMSP\n0.5573\n0.4865\n0.4850\n0.5303\n0.4816\n0.4800\n0.5299\n0.4276\n0.4359\nMean Token Entropy\n0.5877\n0.3029\n0.3514\n0.5421\n0.2778\n0.3433\n0.5215\n0.2902\n0.3542\nSemantic Entropy\n0.5479\n0.4825\n0.4810\n0.5233\n0.4586\n0.4594\n0.5305\n0.4395\n0.4452\nSAR\n0.5546\n0.3599\n0.3882\n0.5316\n0.3164\n0.3669\n0.5376\n0.3422\n0.3819\nRAUQ\n0.5069\n0.2487\n0.3177\n0.4938\n0.2563\n0.3256\n0.5111\n0.2455\n0.3147\nASK4CONF\n0.8281\n0.2429\n0.2529\n0.7190\n0.1837\n0.2540\n0.7056\n0.2184\n0.2693\nAU-probe\n0.9998\n0.0515\n0.0109\n0.9999\n0.1035\n0.0207\n0.9987\n0.2488\n0.1277\nLlama-3.1-8B-Instruct\nMSP\n0.4971\n0.4826\n0.4820\n0.5292\n0.4797\n0.4786\n0.5514\n0.4752\n0.4737\nMean Token Entropy\n0.3169\n0.3084\n0.3599\n0.4900\n0.2690\n0.3354\n0.5401\n0.3317\n0.3612\nSemantic Entropy\n0.5601\n0.4412\n0.4400\n0.5560\n0.4245\n0.4260\n0.5768\n0.4369\n0.4359\nSAR\n0.4865\n0.2329\n0.3186\n0.5083\n0.2005\n0.3117\n0.5364\n0.2150\n0.3110\nRAUQ\n0.6405\n0.1595\n0.2540\n0.5520\n0.1160\n0.2659\n0.5600\n0.2246\n0.3014\nASK4CONF\n0.7487\n0.3840\n0.3726\n0.6011\n0.3665\n0.3788\n0.6497\n0.3886\n0.3872\nAU-probe\n0.9891\n0.0411\n0.0424\n0.9690\n0.0984\n0.0794\n0.8867\n0.1839\n0.1793\nBio-Medical-Llama-3-8B\nMSP\n0.6341\n0.4277\n0.4280\n0.5833\n0.4297\n0.4331\n0.5905\n0.4268\n0.4287\nMean Token Entropy\n0.4535\n0.3168\n0.3399\n0.5645\n0.1814\n0.2876\n0.5812\n0.1619\n0.2752\nSemantic Entropy\n0.6121\n0.3329\n0.3502\n0.5308\n0.2314\n0.3117\n0.5611\n0.2657\n0.3206\nSAR\n0.5317\n0.0810\n0.2618\n0.5441\n0.0806\n0.2671\n0.5572\n0.0949\n0.2613\nRAUQ\n0.4993\n0.1624\n0.2838\n0.5168\n0.1293\n0.2791\n0.5371\n0.1385\n0.2763\nASK4CONF\n0.5458\n0.3985\n0.4005\n0.5582\n0.3845\n0.3942\n0.5546\n0.3905\n0.4000\nAU-probe\n0.9977\n0.1908\n0.0595\n0.9911\n0.2131\n0.0848\n0.9553\n0.2516\n0.1847\nBioMistral-7B\nMSP\n0.5853\n0.4839\n0.4847\n0.4846\n0.4585\n0.4640\n0.6410\n0.4644\n0.4661\nMean Token Entropy\n0.5221\n0.3270\n0.3596\n0.4928\n0.2599\n0.3310\n0.5981\n0.2925\n0.3301\nSemantic Entropy\n0.4453\n0.2760\n0.3493\n0.5361\n0.2102\n0.3074\n0.6188\n0.2168\n0.2907\nSAR\n0.5490\n0.1244\n0.2687\n0.4960\n0.1195\n0.2739\n0.6349\n0.0741\n0.2434\nRAUQ\n0.5713\n0.1913\n0.2829\n0.4844\n0.1687\n0.2891\n0.6274\n0.1951\n0.2778\nASK4CONF\n0.7554\n0.1259\n0.2058\n0.4356\n0.3423\n0.3910\n0.5051\n0.3365\n0.3712\nAU-probe\n0.9997\n0.0379\n0.0102\n0.9958\n0.1063\n0.0388\n0.9780\n0.2282\n0.1551\nachieve reliable detection of input ambiguity across diverse test sets.\nRegarding calibration, AU-Probe remains highly competitive. While\nmethods like SAR sometimes yield lower ECE in OOD settings due\nto their tendency toward more conservative AU estimates, AU-\nProbe‚Äôs consistently superior Brier scores confirm that its sharper\nprobability assignments can achieve a more accurate AU estimation.\nOverall, AU-Probe benefits from direct access to internal hidden\nstates. By operating on contextualized residual-stream activations,\nit bypasses noisy output-space signals, such as generation entropy\nor output likelihood utilized by methods like RAUQ. This allows\nAU-Probe to quantify AU more faithfully and robustly.\n5.3\nEffectiveness of AU-Guided Clarification\nIn this section, we evaluate whether AU-guided clarification driven\nby AU-Probe improves LLM answer accuracy and validate the\ntwo-stage ‚Äúclarify-before-answer‚Äù framework. Experiments are con-\nducted on the ambiguous versions of questions in the test set, rep-\nresenting realistic, underspecified user inputs in medical QA. We\nreport the QA accuracy of the base model (No Clarify.), ASK4CONF,\nwhich is the strongest baseline from our AUROC evaluation, and\nAU-Probe. The base model establishes a lower bound that strug-\ngles to resolve ambiguous queries. Both ASK4CONF and AU-Probe\nadopt 0.5 as the threshold for ambiguity detection and triggering\nTable 2: QA Accuracy Improvement after AU-Guided Clarifi-\ncation (Threshold = 0.5). Improv. = +AU-probe over No Clar.\nDataset\nNo Clar.\n+ASK4CONF\n+AU-probe\nImprov.\nQwen2.5-7B\nCV-MedQA\n0.4855\n0.5169\n0.5939\n+10.84%\nCV-MedMCQA\n0.5050\n0.5130\n0.5660\n+6.10%\nCV-MedExQA\n0.6415\n0.6585\n0.7309\n+8.94%\nLlama-3.1-8B\nCV-MedQA\n0.4815\n0.5232\n0.5719\n+9.03%\nCV-MedMCQA\n0.4810\n0.4970\n0.5410\n+6.00%\nCV-MedExQA\n0.6074\n0.6170\n0.6830\n+7.55%\nBio-Medical-Llama-3-8B\nCV-MedQA\n0.6080\n0.6866\n0.7643\n+15.63%\nCV-MedMCQA\n0.5640\n0.6340\n0.7390\n+17.50%\nCV-MedExQA\n0.5926\n0.6521\n0.6957\n+10.32%\nBioMistral-7B\nCV-MedQA\n0.3637\n0.3841\n0.4116\n+4.79%\nCV-MedMCQA\n0.3620\n0.3880\n0.4140\n+5.20%\nCV-MedExQA\n0.4596\n0.5277\n0.5787\n+11.91%\nclarification. For ASK4CONF, this threshold is explicitly defined\nin its prompt (Figure 9); for AU-Probe, 0.5 is evaluated as the op-\ntimal decision boundary (Figure 11). In particular, if a question‚Äôs\n"}, {"page": 7, "text": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nTable 3: ID (underlined) and OOD AUROC results.\nTraining\nTest sets\nsets\nCV-MedQA\nCV-MedMCQA\nCV-MedExQA\nQwen2.5-7B-Instruct\nCV-MedQA\n0.9998\n0.9965\n0.9996\nCV-MedMCQA\n0.9980\n0.9998\n0.9996\nLlama-3.1-8B-Instruct\nCV-MedQA\n0.9959\n0.8944\n0.8639\nCV-MedMCQA\n0.9785\n0.9993\n0.9927\nBio-Medical-Llama-3-8B\nCV-MedQA\n0.9962\n0.8742\n0.8564\nCV-MedMCQA\n0.9860\n0.9971\n0.9661\nBioMistral-7B\nCV-MedQA\n0.9995\n0.9262\n0.9209\nCV-MedMCQA\n0.9380\n0.9922\n0.9224\npredicted AU exceeds 0.5, it is treated as ambiguous and replaced\nwith its clear counterpart to simulate user-provided clarification;\notherwise, the original ambiguous question is retained. The QA\naccuracy of the LLM is computed on this clarified input stream.\nAs shown in Table 2, AU-guided clarification consistently and\nsignificantly improves QA accuracy over the base model, confirm-\ning the efficacy of the two-stage paradigm: proactive ambiguity\ndetection followed by targeted clarification substantially enhances\nresponse reliability in safety-critical medical QA. Moreover, AU-\nProbe consistently outperforms ASK4CONF in guiding clarification\ndecisions. This advantage stems from its stronger discriminative\ncapability, achieved through a learned linear direction that more\nprecisely separates clear and ambiguous representations in the ac-\ntivation space. As a result, AU-Probe triggers clarification more ac-\ncurately, avoiding unnecessary interventions on clear inputs while\nreliably resolving genuinely ambiguous queries. These findings\nhighlight the advantage of our AU-guided \"clarify-before-answer\"\nframework in enabling safer medical QA systems.\n5.4\nRobustness Analysis\n5.4.1\nOut-of-Distribution Evaluation. A key challenge for su-\npervised uncertainty estimators is robustness under distribution\nshift. We evaluate AU-Probe in both in-distribution (ID) and out-\nof-distribution (OOD) settings. AU-Probe is trained separately on\nthe training splits of CV-MedQA and CV-MedMCQA and evalu-\nated on three held-out test splits (CV-MedQA, CV-MedMCQA, and\nCV-MedExQA). The CV-MedExQA dataset is used exclusively for\ninference as an OOD benchmark. Training and test splits remain\nstrictly disjoint to avoid leakage. Results are summarized in Table 3.\nIn ID evaluation, AU-Probe achieves AUROC values close to 1.0\nacross all models, demonstrating that LLM hidden states encode\na highly discriminative, linearly separable signal of AU, allowing\nthe probe to accurately distinguish between clear and vague in-\nputs. Under OOD evaluation, performance decreases slightly but\nremains strong, with AUROC consistently above 0.85. The result\nsuggests that although the types of ambiguity differ across datasets,\nLLMs encode aleatoric uncertainty in a largely dataset-agnostic\nmanner within their internal representations. As a result, AU-Probe\ngeneralizes effectively across datasets and maintains reliable AU\nquantification under distribution shift as detailed in Section 8.1.1.\n2%\n5% 10% 20% 30% 40% 50% 70%100%\nTraining data percentage\n0.700\n0.800\n0.900\n1.000\nAUROC\nLlama-3.1-8B-Instruct\n2%\n5% 10% 20% 30% 40% 50% 70%100%\nTraining data percentage\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAUROC\nBio-Medical-Llama-3-8B\n2%\n5% 10% 20% 30% 40% 50% 70%100%\nTraining data percentage\n0.900\n0.925\n0.950\n0.975\n1.000\nAUROC\nBioMistral-7B\n2%\n5% 10% 20% 30% 40% 50% 70%100%\nTraining data percentage\n0.994\n0.996\n0.998\n1.000\nAUROC\nQwen2.5-7B-Instruct\nCV-MedQA\nCV-MedMCQA\nCV-MedExQA\nFigure 6: AU-Probe under different training sizes.\nAmong the tested models, Qwen2.5-7B-Instruct exhibits excep-\ntional robustness, maintaining AUROC values near 1.0 in both ID\nand OOD settings. The results suggest that internal representations\nof Qwen2.5-7B-Instruct display a discriminative linear separation\nbetween clear and ambiguous questions, resulting in superior gen-\neralization and highlighting the probe‚Äôs effectiveness when trained\non high-quality internal representations.\n5.4.2\nLow-Data Regime Performance. We further evaluate the\nrobustness of AU-Probe under limited labeled data. The goal is\nto determine how much supervision is needed for the probe to\nachieve stable AU quantification, providing practical guidance for\nlow-resource settings. Starting from the full CV-MedBench training\nsplits (1,600 instances), we progressively reduce the training size\nto 70%, 50%, 40%, 30%, 20%, 10%, 5%, and 2%. Notably, the primary\nresults reported in Tables 1, 2, 3 utilize only the 30% training size.\nAs shown in Figure 6, AU-Probe exhibits rapid convergence and\nhigh data efficiency. On ID subsets (CV-MedQA and CV-MedMCQA),\nnear-optimal AUROC is achieved with as little as 5‚Äì10% of the train-\ning data for most models, and saturation occurs by 30% at the latest.\nOn the OOD CV-MedExQA subset, performance improves more\ngradually but stabilizes between 20% and 40% of the full training\nset. These results indicate that only a small fraction of training\ndata is needed to achieve strong AU quantification. Across models,\nQwen2.5-7B-Instruct achieves peak performance with the fewest\nsamples, suggesting that its hidden representations better capture\nthe distinction between clear and ambiguous inputs. This finding\nis consistent with its strong robustness in the OOD evaluation.\nOverall, using 30% of the training data (960 samples) achieves\nperformance nearly identical to full-data training, so we we use\nthis reduced set for all AU-Probe models to improve computational\nefficiency. Remarkably, even with as little as 2% of the data (64 sam-\nples), AU-Probe remains substantially stronger than the baseline\nmethods in Table 1, indicating strong data efficiency and reliable\nperformance in low-data settings with limited annotations.\n5.5\nComputational Efficiency Analysis\nFor clinical deployment, an AU estimator must impose low compu-\ntational overhead to preserve real-time interactivity. In Table 4, we\nbenchmark all UQ methods on an NVIDIA A40 GPU, measuring\nper-sample answer generation time on the CV-MedQA test set.\n"}, {"page": 8, "text": "WWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nYaokun Liu et al.\nTable 4: Per-sample inference latency (s) of UQ methods.\nMethod\nQwen2.5\nLlama3.1\nBioMed-Llama\nBioMistral\nMSP\n1.142\n1.143\n0.977\n1.210\nMTE\n1.185\n1.148\n0.982\n1.214\nSE\n10.445\n10.222\n20.970\n26.179\nSAR\n10.971\n10.819\n25.879\n34.622\nASK4CONF\n0.996\n0.969\n0.716\n1.096\nRAUQ\n11.847\n11.350\n27.054\n35.480\nAU-Probe\n1.217\n1.181\n1.050\n1.222\nUser Input (original): I have a red itchy rash on my arm. What should\nI use to treat it?\nAU Score: 0.82 ‚ÜíAmbiguous, clarification requested\nLLM Response: Your question lacks essential clinical details for safe\nguidance. Please provide additional information such as age, symptom\nduration, key associated symptoms, relevant medical history, current\nmedications, and any recent exposure risks.\nUser Input (clarified): The rash appeared yesterday after I tried a new\nscented lotion. It is itchy but not painful, and it is not spreading. No\nswelling or fever. What should I use to treat it?\nAU Score: 0.39 ‚ÜíClear, proceed to Answer Generation\nLLM Response: Given the information you‚Äòve provided, it‚Äôs highly\nlikely that the rash is an allergic contact dermatitis reaction to the\nscented lotion. Here are safe, over-the-counter suggestions: 1. Apply a\ntopical cream: You can try applying an over-the-counter hydrocortisone\ncream to reduce itching and inflammation. Apply a thin layer 2-3 times\na day. 2. Use an oral antihistamine: If the itching is severe, taking an\noral antihistamine like diphenhydramine (e.g., Benadryl) may help‚Ä¶\nStage 1: AU-guided\nClarification\nStage 2: Answer Generation\nFigure 7: Case study of AU-guided clarification.\nLikelihood-based methods (MSP and MTE) and ASK4CONF are\nthe fastest among baselines, with latencies around 0.9‚Äì1.2 seconds.\nHowever, despite their relatively low cost, these methods exhibit\nmarkedly weaker discrimination (Table 1), limiting their utility in\nsafety-critical settings where missed ambiguity directly impacts\npatient risk. Consistency-based methods (SE and SAR) and RAUQ\nincur substantially higher latency, ranging from 10 to over 35 sec-\nonds, depending on the model. Their cost is dominated by multi-\nsample generation (SE, SAR) or attention-head aggregation during\ndecoding (RAUQ), making them impractical for real-time inter-\nactive clinical applications. By contrast, AU-Probe maintains low\nlatency across all four models (1.05‚Äì1.22 seconds), comparable to\nthe fastest baselines while avoiding the substantial overhead seen\nin multi-round methods. Combined with its significantly superior\ndiscrimination performance (Table 1), AU-Probe offers an advanta-\ngeous balance of accuracy and efficiency, making it well-suited for\ntimely ambiguity detection in medical QA workflows.\n5.6\nCase Study\nIn this section, we present a real user case to illustrate how the AU-\nguided ‚Äúclarify-before-answer‚Äù framework detects input ambiguity\nand enhances the safety of Medical QA.\nAs shown in Figure 7, the user inputs the initial query (‚ÄúI have\na red itchy rash on my arm. What should I use to treat it?‚Äù) into\nLlama-3.1-8B-Instruct. This query lacks key diagnostic information,\nand the AU-Probe successfully assigns a high uncertainty score\n(0.82, above the ùúè= 0.5 threshold), triggering user clarification.\nThe system requests essential clinical details before attempting to\nanswer. After the user provides the detailed information, the AU\nscore decreases substantially (0.39), indicating that the input is now\nsufficiently well-specified. At this point, the model proceeds to the\nanswer-generation stage and delivers a safe, context-aware recom-\nmendation consistent with standard care for contact dermatitis.\nThis case demonstrates the key benefit of the AU-guided ‚Äúclarify-\nbefore-answer‚Äù framework: ambiguous queries are detected and\nintercepted successfully by the AU-probe before answer generation,\nclarification requests supply the missing context, and the final\nresponse becomes both safer and more clinically reliable.\n6\nLimitations\nOur work presents two limitations that should be considered during\nthe practical deployment of the AU-Probe.\nFirst, the AU-Probe assesses ambiguity solely based on the lin-\nguistic formulation of the user query (i.e., how underspecified the\nprompt is). It does not detect factual inconsistencies or a seman-\ntic‚Äìreality mismatch where a clearly phrased question is based on\nincorrect underlying medical information from the user. Address-\ning this limitation would require incorporating external validation\nmechanisms (e.g., real-time physiological data), which is beyond the\nscope of our methods. Users should therefore interpret the uncer-\ntainty estimates as reflecting question ambiguity, not the objective\nfactual completeness of the user‚Äôs medical context.\nSecond, the AU-Probe requires white-box access to the internal\nhidden-state representations of the target LLM. This dependency\nrestricts its application to models where internal activations are\naccessible and precludes its direct use with black-box commercial\nAPIs that only expose the final output. Extending this methodology\nto black-box models remains an area for future research.\n7\nConclusion\nThis paper tackles the critical safety risks posed by input ambiguity\nin Medical QA by formulating it as quantifiable aleatoric uncertainty.\nThrough a novel representation engineering perspective on uncer-\ntainty quantification, we introduce AU-Probe‚Äîa computationally\nlightweight module that estimates AU in a single pass‚Äîand inte-\ngrate it into the first two-stage ‚Äúclarify-before-answer‚Äù framework\nfor proactive input ambiguity mitigation. Extensive evaluations\ndemonstrate AU-Probe‚Äôs superior ambiguity quantification capabil-\nity and the AU-guided QA framework‚Äôs substantial accuracy gains.\nBy enhancing the reliability of LLMs in medical QA systems without\ncompromising real-time inference efficiency, our work advances\nthe broader vision of AI for social good.\nAcknowledgments\nThis research is supported in part by the National Science Founda-\ntion under Grant No. CNS-2427070, IIS-2331069, IIS-2202481, IIS-\n2130263, CNS-2131622. The views and conclusions contained in this\ndocument are those of the authors and should not be interpreted as\nrepresenting the official policies, either expressed or implied, of the\nU.S. Government. The U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes notwithstanding\nany copyright notation here on.\n"}, {"page": 9, "text": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nReferences\n[1] Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L. Edelman.\n2024. Distinguishing the Knowable from the Unknowable with Language Models.\nIn Forty-first International Conference on Machine Learning, ICML 2024, Vienna,\nAustria, July 21-27, 2024. OpenReview.net.\n[2] Lukas Aichberger, Kajetan Schweighofer, and Sepp Hochreiter. 2024. Rethink-\ning uncertainty estimation in natural language generation.\narXiv preprint\narXiv:2412.15176 (2024).\n[3] Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes\nGurnee, and Neel Nanda. 2024. Refusal in language models is mediated by a\nsingle direction. Advances in Neural Information Processing Systems 37 (2024),\n136037‚Äì136083.\n[4] Zahra Atf, Seyed Amir Ahmad Safavi-Naini, Peter R Lewis, Aref Mahjoubfar,\nNariman Naderi, Thomas R Savage, and Ali Soroush. 2025. The challenge of\nuncertainty quantification of large language models in medicine. arXiv preprint\narXiv:2504.05278 (2025).\n[5] Vanessa Choy, Sara Martin, and Ashley Lumpkin. 2024. Can we rely on generative\nAI for healthcare information?| Ipsos.\n[6] Jeremy Cole, Michael Zhang, Dan Gillick, Julian Eisenschlos, Bhuwan Dhingra,\nand Jacob Eisenstein. 2023.\nSelectively answering ambiguous questions. In\nProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing. 530‚Äì543.\n[7] Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu,\nBhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards\nthe predictive uncertainty quantification of free-form large language models.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 5050‚Äì5063.\n[8] Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr√©d√©ric Blain, Francisco\nGuzm√°n, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia.\n2020. Unsupervised quality estimation for neural machine translation. Transac-\ntions of the Association for Computational Linguistics 8 (2020), 539‚Äì555.\n[9] Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. 2019. Deep ensembles:\nA loss landscape perspective. arXiv preprint arXiv:1912.02757 (2019).\n[10] Xiang Gao, Jiaxin Zhang, Lalla Mouatadid, and Kamalika Das. 2024.\nSpuq:\nPerturbation-based uncertainty quantification for large language models. arXiv\npreprint arXiv:2403.02509 (2024).\n[11] W Brier Glenn et al. 1950. Verification of forecasts expressed in terms of proba-\nbility. Monthly weather review 78, 1 (1950), 1‚Äì3.\n[12] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of\nmodern neural networks. In International conference on machine learning. PMLR,\n1321‚Äì1330.\n[13] Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. 2020. Bayesian deep en-\nsembles via the neural tangent kernel. Advances in neural information processing\nsystems 33 (2020), 1010‚Äì1022.\n[14] Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang\nZhang. 2023. Decomposing uncertainty for large language models through input\nclarification ensembling. arXiv preprint arXiv:2311.08718 (2023).\n[15] Eyke H√ºllermeier and Willem Waegeman. 2021. Aleatoric and epistemic uncer-\ntainty in machine learning: An introduction to concepts and methods. Machine\nlearning 110, 3 (2021), 457‚Äì506.\n[16] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter\nSzolovits. 2021. What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Applied Sciences 11, 14 (2021),\n6421.\n[17] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,\nEthan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-\nJohnson, et al. 2022. Language models (mostly) know what they know. arXiv\npreprint arXiv:2207.05221 (2022).\n[18] Yunsoo Kim, Jinge Wu, Yusuf Abdulle, and Honghan Wu. 2024. MedExQA: Med-\nical question answering benchmark with multiple explanations. arXiv preprint\narXiv:2406.06331 (2024).\n[19] Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik,\nand Yarin Gal. 2024. Semantic entropy probes: Robust and cheap hallucination\ndetection in llms. arXiv preprint arXiv:2406.15927 (2024).\n[20] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty:\nLinguistic Invariances for Uncertainty Estimation in Natural Language Genera-\ntion. In The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\n[21] Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun,\nMika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, et al. 2024. Uncertainty\nquantification for in-context learning of large language models. arXiv preprint\narXiv:2402.10189 (2024).\n[22] Yifan Liu, Aliya Maussymbayeva, Aslanbek Murzakhmetov, Alexandra Ne-\nmerenco, Yike Li, and Dong Wang. 2025. Reasoning-Based Uncertainty Esti-\nmation for Scalable Multidimensional Media Bias Annotation: A Benchmark\nAcross Diverse Media Spaces. Knowledge-Based Systems (2025), 115058.\n[23] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing\nNie. 2023. Biomedgpt: Open multimodal generative pre-trained transformer for\nbiomedicine. arXiv preprint arXiv:2308.09442 (2023).\n[24] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\nAmbigQA: Answering ambiguous open-domain questions.\narXiv preprint\narXiv:2004.10645 (2020).\n[25] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.\nMedmcqa: A large-scale multi-subject multi-choice dataset for medical domain\nquestion answering. In Conference on health, inference, and learning. PMLR, 248‚Äì\n260.\n[26] Kiho Park, Yo Joong Choe, and Victor Veitch. 2023.\nThe linear representa-\ntion hypothesis and the geometry of large language models. arXiv preprint\narXiv:2311.03658 (2023).\n[27] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexan-\nder Turner. 2024. Steering llama 2 via contrastive activation addition. In Proceed-\nings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers). 15504‚Äì15522.\n[28] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine\nTraverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, C√≠an Hughes, Charles Lau,\net al. 2025. Medgemma technical report. arXiv preprint arXiv:2507.05201 (2025).\n[29] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won\nChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.\n2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023),\n172‚Äì180.\n[30] Junya Takayama and Yuki Arase. 2019. Relevant and informative response gen-\neration using pointwise mutual information. In Proceedings of the First Workshop\non NLP for Conversational AI. 133‚Äì138.\n[31] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander,\nand Nathan Silberman. 2019. Learning from noisy labels by regularized estimation\nof annotator confusion. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. 11244‚Äì11253.\n[32] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura\nGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models\nin medicine. Nature medicine 29, 8 (2023), 1930‚Äì1940.\n[33] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu\nYao, Chelsea Finn, and Christopher D Manning. 2023. Just ask for calibration:\nStrategies for eliciting calibrated confidence scores from language models fine-\ntuned with human feedback. arXiv preprint arXiv:2305.14975 (2023).\n[34] Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. 2023.\nLinear representations of sentiment in large language models. arXiv preprint\narXiv:2310.15154 (2023).\n[35] Tim Tomov, Dominik Fuchsgruber, Tom Wollschl√§ger, and Stephan G√ºnnemann.\n2025. The Illusion of Certainty: Uncertainty quantification for LLMs fails under\nambiguity. arXiv preprint arXiv:2511.04418 (2025).\n[36] Liam Van der Poel, Ryan Cotterell, and Clara Meister. 2022.\nMutual infor-\nmation alleviates hallucinations in abstractive summarization. arXiv preprint\narXiv:2210.13210 (2022).\n[37] Artem Vazhentsev, Lyudmila Rvanova, Gleb Kuzmin, Ekaterina Fadeeva, Ivan\nLazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Mrinmaya\nSachan, Preslav Nakov, et al. 2025. Uncertainty-Aware Attention Heads: Ef-\nficient Unsupervised Uncertainty Quantification for LLMs.\narXiv preprint\narXiv:2505.20045 (2025).\n[38] Andreas Waldis, Vagrant Gautam, Anne Lauscher, Dietrich Klakow, and Iryna\nGurevych. 2025. Aligned probing: Relating toxic behavior and model internals.\narXiv preprint arXiv:2503.13390 (2025).\n[39] Dandan Wang and Shiqing Zhang. 2024. Large language models in medical and\nhealthcare fields: applications, advances, and challenges. Artificial intelligence\nreview 57, 11 (2024), 299.\n[40] Xiaoye Wang, Nicole Xi Zhang, Hongyu He, Trang Nguyen, Kun-Hsing Yu, Hao\nDeng, Cynthia Brandt, Danielle S Bitterman, Ling Pan, Ching-Yu Cheng, et al.\n2024. Safety challenges of AI in medicine in the era of large language models.\narXiv preprint arXiv:2409.18968 (2024).\n[41] Qi Wei, Lei Feng, Haoliang Sun, Ren Wang, Chenhui Guo, and Yilong Yin. 2023.\nFine-grained classification with noisy labels. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. 11651‚Äì11660.\n[42] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng\nWang. 2024. PMC-LLaMA: toward building open-source language models for\nmedicine. Journal of the American Medical Informatics Association 31, 9 (2024),\n1833‚Äì1843.\n[43] Ran Xu, Patrick Jiang, Linhao Luo, Cao Xiao, Adam Cross, Shirui Pan, Jimeng\nSun, and Carl Yang. 2025. A survey on unifying large language models and\nknowledge graphs for biomedicine and healthcare. In Proceedings of the 31st ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining V. 2. 6195‚Äì6205.\n[44] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren,\nAlexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al.\n2023. Representation engineering: A top-down approach to ai transparency.\narXiv preprint arXiv:2310.01405 (2023).\n"}, {"page": 10, "text": "WWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nYaokun Liu et al.\nPrompt for Multiple-Choice Medical QA\nGiven the following medical question and four candidate answers\n(A, B, C, and D), choose the best answer. You should provide a\nconcise answer and start your response with the letter of the\nselected option (A, B, C, or D).\nInput question:\n<QUESTION TO ANSWER>\nFigure 8: Prompt used for answering multiple-choice medical\nquestions.\nPrompt for ASK4CONF (AU Estimation)\nEstimate the clarity probability (0.0 to 1.0) for the following medi-\ncal question, which will be converted to aleatoric uncertainty\nas 1 ‚àíclarity probability. Higher clarity values indicate less am-\nbiguity, while lower clarity values indicate greater ambiguity or\nunderspecification.\nGive ONLY the probability, with no additional words, explana-\ntion, or reasoning. For example:\nProbability: <a value between 0.0 and 1.0>\nReminder: Always output only ‚ÄúProbability: <value>‚Äù ‚Äî no com-\nmentary or justification.\nInput question:\n<QUESTION TO EVALUATE>\nFigure 9: Prompt used in the ASK4CONF setting for estimat-\ning aleatoric uncertainty (1 ‚àíclarity probability).\n8\nAppendix\n8.1\nExperimental Setup\n8.1.1\nDataset and Models. Our experiments are conducted on\nthe Clear-to-Vague Medical QA Benchmark (CV-MedBench), con-\nstructed on questions from three publicly available multiple-choice\nmedical QA datasets: MedQA [16], MedMCQA [25], and MedExQA\n[18]. These question sources span a wide range of clinical spe-\ncialties, with MedQA and MedMCQA covering common subjects\n(e.g., pathology, physiology) and MedExQA focusing on underrepre-\nsented areas (e.g., Biomedical Engineering, Occupational Therapy).\nFor the training split, we randomly sampled 800 questions each\nfrom the original MedQA and MedMCQA training splits. For test-\ning, we adopt the original test splits of the three datasets, totaling\n5,029 questions (1,273 from MedQA, 2,816 from MedMCQA, and 940\nfrom MedExQA). All questions include both clear and ambiguous\nversions, and training and test splits are strictly disjoint. Test sets\nfrom MedQA and MedMCQA are treated as in-distribution (ID),\nand MedExQA serves as out-of-distribution (OOD) due to its dis-\ntinct specialty coverage and absence from training. For clarity, we\nrefer to the corresponding CV-MedBench test subsets as CV-MedQA,\nCV-MedMCQA, and CV-MedExQA according to their original sources.\nWe conduct experiments using four open LLMs without fine-\ntuning, including both general-purpose instruction-tuned models\n(Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct) and special-\nized biomedical models (Bio-Medical-Llama-3-8B and BioMistral-\n7B) that are equipped with strong medical domain knowledge. Our\ninstruction prompts are in the Appendix 8.2.\nNotably, AU-Probe is only trained on 240 questions (each paired\nwith clear and ambiguous versions) randomly sampled from the\nCV-MedBench training split, except for the low-data experiment in\nSection 5.4.1, where all 800 questions are used to assess the amount\nof training data required. For AU quantification, each model uses the\nlayer with the strongest AUROC signal identified in Section 4.3.2:\nLlama-3.1-8B-Instruct (layer 32), Qwen2.5-7B-Instruct (layer 9),\nBioMistral-7B (layer 30), and Bio-Medical-Llama-3-8B (layer 11).\n8.1.2\nBaselines. We compare AU-Probe against a comprehensive\nset of UQ methods covering likelihood-based, consistency-based,\nLLM-as-a-judge, and internal-state-based methods: Likelihood-\nbased baselines estimate uncertainty directly from the model‚Äôs out-\nput distribution using a single forward pass, including Maximum\nSequence Probability (MSP) and Max Token Entropy (MTE) [8].\nConsistency-based methods assess uncertainty by measuring dis-\nagreement across multiple sampled generations. We evaluate Se-\nmantic Entropy (SE) [20] and SAR [7]. LLM-as-a-judge methods\nrely on the model to explicitly score its own confidence under the\ndefault temperature. We adopt ASK4CONF [33], using an adapted\nprompt that instructs the model to directly produce aleatoric un-\ncertainty in the form of a clarity probability; the full prompt is\nprovided in Appendix 8.2. Internal-state-based baselines infer\nuncertainty from hidden states. We consider RAUQ [37] as a repre-\nsentative baseline. RAUQ differs fundamentally from our method:\n(1) it models total uncertainty rather than isolating aleatoric uncer-\ntainty, and (2) RAUQ combines attention-head features with output\ndistributions and functions essentially as a likelihood-based method\naugmented by attention signals. In contrast, AU-Probe operates\nsolely on residual-stream activations of the prompt token before\nany answer generation. This design decouples aleatoric uncertainty\nestimation from the generation process, enabling proactive detec-\ntion of ambiguous inputs.\n8.1.3\nEvaluation Metrics. We evaluate AU quantification using\nboth discrimination and calibration metrics. For discrimination,\nwe report AUROC, which measures how effectively a method\nranks ambiguous inputs above clear ones; higher values indicate a\nstronger discriminative ability. For calibration, we use Expected\nCalibration Error (ECE) [12] to quantify the deviation between\npredicted AU scores and observed ambiguity frequencies, where\nlower values indicate better calibration. We additionally report the\nBrier score [11], which measures the mean squared error between\npredicted AU scores and ground-truth ambiguity, reflecting both\ncalibration quality and prediction sharpness.\n8.2\nPrompt Templates\nWe use the prompt in Figure 10 to guide the LLM in rewriting\nclear medical questions into controlled ambiguous variants while\npreserving the original clinical topic. Figure 8 shows the prompt\nused for answering multiple-choice questions, and Figure 9 presents\nthe ASK4CONF prompt for estimating aleatoric uncertainty from\nclarity probability.\n"}, {"page": 11, "text": "Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering\nWWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nPrompt for Generating Ambiguous Medical Questions\nYou are a medical data modifier. Your task is to rewrite clear and precise medical questions to simulate real-world user queries that contain natural\nambiguity (aleatoric uncertainty).\nYou can apply one or more of the following uncertainty types, whichever are appropriate for the given question:\n1. Context omission ‚Äî remove important background information that makes the question less clear or underspecified.\nExample 1:\nOriginal: ‚ÄúA 60-year-old diabetic man with foot ulcer presents with fever and chills. What is the next best step in management?‚Äù\nRewritten: ‚ÄúA man with an infected wound on his foot feels unwell. What should the doctor do next?‚Äù\nApplied types: [‚Äúcontext omission‚Äù]\nExample 2:\nOriginal: ‚ÄúA 25-year-old woman with asthma develops sudden shortness of breath after taking aspirin.‚Äù\nRewritten: ‚ÄúA woman has trouble breathing after taking a medicine.‚Äù\nApplied types: [‚Äúcontext omission‚Äù]\n2. Semantic vagueness ‚Äî wording becomes less precise or more general, while keeping the medical topic unchanged.\nExample 1:\nOriginal: ‚ÄúA 45-year-old man with pneumonia presents with productive cough and fever for 3 days. What antibiotic should be started?‚Äù\nRewritten: ‚ÄúA middle-aged man with a lung infection and fever for a few days needs treatment. What medicine should be given?‚Äù\nApplied types: [‚Äúsemantic vagueness‚Äù]\nExample 2:\nOriginal: ‚ÄúA patient with rheumatoid arthritis is started on methotrexate. What lab tests should be monitored?‚Äù\nRewritten: ‚ÄúA patient on a new medication for joint pain needs regular tests. What kind of tests are usually done?‚Äù\nApplied types: [‚Äúsemantic vagueness‚Äù]\n3. Logical inconsistency ‚Äî introduce a small internal contradiction that may cause confusion but still reads naturally.\nExample 1:\nOriginal: ‚ÄúA newborn has jaundice with total bilirubin of 20 mg/dL. What is the next best step?‚Äù\nRewritten: ‚ÄúA newborn appears healthy but has a very high bilirubin level that is probably not serious. What should be done?‚Äù\nApplied types: [‚Äúlogical inconsistency‚Äù]\nExample 2:\nOriginal: ‚ÄúA patient with chest pain and elevated troponin is diagnosed with myocardial infarction.‚Äù\nRewritten: ‚ÄúA patient with chest pain is told that their heart test is normal, but troponin is very high. What does this mean?‚Äù\nApplied types: [‚Äúlogical inconsistency‚Äù]\nDo not change the main medical topic or intent of the question. If none of these types fit, return the original question unchanged.\nFor each output, include:\n- The rewritten question\n- The applied uncertainty types (a list from [semantic vagueness, context omission, logical inconsistency])\nInput question:\n<QUESTION TO MODIFY>\nFigure 10: Prompt used to rewrite clear medical questions into ambiguous variants.\n10%\n30%\n50%\n70%\n90%\nThreshold ( )\n0.70\n0.80\n0.90\n1.00\nAccuracy\nQwen2.5-7B\n10%\n30%\n50%\n70%\n90%\nThreshold ( )\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nAccuracy\nLlama-3.1-8B\n10%\n30%\n50%\n70%\n90%\nThreshold ( )\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nAccuracy\nBioMed-Llama-3-8B\n10%\n30%\n50%\n70%\n90%\nThreshold ( )\n0.60\n0.70\n0.80\n0.90\n1.00\nAccuracy\nBioMistral-7B\nCV-MedQA\nCV-MedMCQA\nFigure 11: AU-Probe ambiguity‚Äìdetection accuracy across four LLMs under different ambiguity thresholds (ùúè) on validation\nsets. Our threshold search shows that ùúè= 0.5 leads to the highest validation detection accuracy across different models.\n8.3\nHuman Evaluation of Ambiguous Question\nGeneration\nTo validate the quality of the Clear-to-Vague Medical QA bench-\nmark, we conducted a comprehensive human evaluation. The pri-\nmary objective is to ensure the LLM successfully introduces ambi-\nguity into the clear medical questions while maintaining the core\n"}, {"page": 12, "text": "WWW ‚Äô26, April 13‚Äì17, 2026, Dubai, United Arab Emirates\nYaokun Liu et al.\nTable 5: Agreement rates across two human judges.\nDimension\nAgreement Rate\nHuman 1\nHuman 2\nCombined\nTF\n195/200 (97.5%)\n195/200 (97.5%)\n390/400 (97.5%)\nAV\n192/200 (96.0%)\n192/200 (96.0%)\n384/400 (96.0%)\nLF\n198/200 (99.0%)\n198/200 (99.0%)\n396/400 (99.0%)\nClear Question: A 23-year-old pregnant woman at 22 weeks gesta-\ntion presents with burning upon urination. She states it started 1 day\nago and has been worsening despite drinking more water and taking\ncranberry extract. She otherwise feels well and is followed by a doctor\nfor her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure\nis 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen\nsaturation is 98% on room air. Physical exam is notable for an absence\nof costovertebral angle tenderness and a gravid uterus. Which of the\nfollowing is the best treatment for this patient?\nAmbiguous Question: A pregnant woman feels pain while urinating\nand it‚Äôs getting worse even after trying home remedies. She seems\ngenerally okay and has been seeing a doctor for her pregnancy. What\nis the best way to help her?\nFigure 12: Example question from CV-MedBench.\nclinical context and linguistic quality necessary for realistic sce-\nnarios. We evaluate each rewritten question along three binary\ndimensions (0 = fail, 1 = pass):\n‚Ä¢ Topic Fidelity (TF): whether the rewritten question preserves\nthe same underlying medical topic or scenario as the original.\n‚Ä¢ Ambiguity Validity (AV): whether the rewritten question con-\ntains genuine, real-world ambiguity, consistent with context\nomission, semantic vagueness, or mild logical inconsistency.\n‚Ä¢ Linguistic Fluency (LF): whether the rewritten question is\ngrammatical, coherent, and natural in English.\nTwo independent annotators reviewed each sample by comparing\nthe original and rewritten questions and then assigning binary\nscores for all three evaluation dimensions.\nTable 5 reports the agreement rates between the two annotators\nacross all evaluation dimensions. Agreement is high across all di-\nmensions, with an overall agreement exceeding 95%. The strong\nconsensus validates that the rewritten variants successfully pre-\nserve clinical intent, introduce ambiguity, and maintain linguistic\nquality. Consequently, the Clear-to-Vague Medical QA dataset pro-\nvide a solid resource for studying input ambiguity in medical QA.\n"}]}