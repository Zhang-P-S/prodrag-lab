{"doc_id": "arxiv:2512.10996", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.10996.pdf", "meta": {"doc_id": "arxiv:2512.10996", "source": "arxiv", "arxiv_id": "2512.10996", "title": "MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA", "authors": ["Seonok Kim"], "published": "2025-12-10T15:43:25Z", "updated": "2025-12-10T15:43:25Z", "summary": "Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.10996v1", "url_pdf": "https://arxiv.org/pdf/2512.10996.pdf", "meta_path": "data/raw/arxiv/meta/2512.10996.json", "sha256": "01a6e6877c8bd070c8d83ddca8eca49e993a15d648eaa0dadf417c4d56c0c140", "status": "ok", "fetched_at": "2026-02-18T02:24:30.889098+00:00"}, "pages": [{"page": 1, "text": "MedBioRAG: Semantic Search and Retrieval-Augmented Generation with\nLarge Language Models for Medical and Biological QA\nSeonok Kim\nMazelone\n161 Hakdong-ro, Gangnam-gu, Seoul\nseonokrkim@gmail.com\nAbstract\nRecent advancements in retrieval-augmented\ngeneration (RAG) have significantly enhanced\nthe ability of large language models (LLMs)\nto perform complex question-answering (QA)\ntasks. In this paper, we introduce MedBioRAG,\na retrieval-augmented model designed to im-\nprove biomedical QA performance through a\ncombination of semantic and lexical search,\ndocument retrieval, and supervised fine-tuning.\nMedBioRAG efficiently retrieves and ranks\nrelevant biomedical documents, enabling pre-\ncise and context-aware response generation.\nWe evaluate MedBioRAG across text retrieval,\nclose-ended QA, and long-form QA tasks using\nbenchmark datasets such as NFCorpus, TREC-\nCOVID, MedQA, PubMedQA, and BioASQ.\nExperimental results demonstrate that Med-\nBioRAG outperforms previous state-of-the-art\n(SoTA) models and the GPT-4o base model\nin all evaluated tasks. Notably, our approach\nimproves NDCG and MRR scores for docu-\nment retrieval, while achieving higher accu-\nracy in close-ended QA and ROUGE scores in\nlong-form QA. Our findings highlight the effec-\ntiveness of semantic search-based retrieval and\nLLM fine-tuning in biomedical applications.\n1\nIntroduction\nRecent advancements in large language models\n(LLMs) have significantly expanded their appli-\ncations in biomedical domains, demonstrating\nstrong performance in structured and open-ended\nquestion-answering (QA) tasks (McDuff et al.,\n2023; Singhal et al., 2023a). However, biomed-\nical QA presents unique challenges due to its do-\nmain specificity, complexity, and factual accuracy\nrequirements. Unlike general-purpose QA, medi-\ncal QA demands high precision and interpretability,\nmaking domain adaptation and retrieval-based en-\nhancements essential.\nLLMs such as GPT-4o show strong zero-shot rea-\nsoning capabilities but rely on static pre-training\ndata, making them prone to hallucination and out-\ndated information. Retrieval-augmented generation\n(RAG) addresses this limitation by dynamically re-\ntrieving external biomedical knowledge (Zhu et al.,\n2024). However, its effectiveness depends on re-\ntrieval quality, document ranking, and model fine-\ntuning.\nWe\nintroduce\nMedBioRAG,\na\nretrieval-\naugmented framework integrating semantic search,\ndocument retrieval, and fine-tuned LLM-based\nanswer generation to enhance biomedical QA.\nTraditional\nkeyword-based\nretrieval\nmethods\n(e.g., BM25, TF-IDF) struggle with semantic\nunderstanding, often leading to irrelevant results.\nMedBioRAG improves upon these by leveraging\nsemantic search for precise retrieval and fine-tuned\nLLMs for factually accurate responses.\nWe\nsystematically evaluate MedBioRAG across three\nmajor categories of biomedical QA:\n• Text Retrieval Performance, where we assess\nthe effectiveness of semantic search vs. lex-\nical search using NDCG and MRR scores\non datasets such as NFCorpus and TREC-\nCOVID.\n• Close-ended QA, which requires models to\nselect the correct answer from predefined op-\ntions, as seen in datasets like MedQA, Pub-\nMedQA, and BioASQ.\n• Long-form QA, which involves generating de-\ntailed explanations based on biomedical lit-\nerature, evaluated using ROUGE and BLEU\nscores on datasets such as LiveQA, Medica-\ntionQA, and PubMedQA.\nBeyond retrieval quality, prompt engineering and\nsupervised fine-tuning play critical roles in ensur-\ning that LLMs generate coherent, well-structured,\nand clinically meaningful responses. Prompt en-\ngineering allows models to adapt their response\narXiv:2512.10996v1  [cs.CL]  10 Dec 2025\n"}, {"page": 2, "text": "NFCorpus\n31.8\n30.4\nMedBioRAG\n85.0\nMeditron-70B\n81.6\nMed-PaLM 2\n79.2\nFlan-PaLM\n79.0\nBioGPT\n78.2\nBioLinkBERT\n72.2\nBioELECTRA\n64.2\n58.9\n70.5\nPrevious SoTA\nMedBioRAG\nTREC-COVID\nNDCG\nJun 21\nMay 23\nJun 22\nNov 23\nOct 22\nFeb 25\nDec 22\nNDCG and MRR (%)\nAccuracy (%)\nNDCG\nMRR\nMRR\nText Retrieval Performance\nSoTA on PubMedQA\n37.9\n64.3\n61.9\n88.2\nFigure 1: MedBioRAG performance highlights. The\ntop plot compares scores of MedBioRAG against pre-\nvious state-of-the-art (SoTA) methods (Sawarkar et al.,\n2024; Lù, 2024). The bottom plot shows the histori-\ncal accuracy progression on PubMedQA, demonstrating\nMedBioRAG’s advancement over prior methods. (Chen\net al., 2023; Singhal et al., 2023b,a; Luo et al., 2022;\nYasunaga et al., 2022; Kanakarajan et al., 2021)\nstyle based on user intent, while fine-tuning en-\nsures that models develop specialized domain ex-\npertise, improving factual consistency and reducing\nhallucinations. By systematically analyzing these\ncomponents, this study provides a comprehensive\nframework for optimizing biomedical QA systems.\nOur contributions can be summarized as follows:\n• We introduce MedBioRAG, a retrieval-\naugmented generation (RAG) framework for\nbiomedical QA, integrating semantic search,\ndocument ranking, and fine-tuned LLM-based\nresponse synthesis. Our approach enhances\nfactual accuracy, contextual relevance, and\nstructured inference across diverse biomedi-\ncal tasks.\n• We show that fine-tuning GPT-4o signifi-\ncantly improves biomedical QA performance.\nFine-tuned GPT-4o, combined with retrieval-\naugmented generation, outperforms zero-shot\nGPT-4o and other fine-tuned LLMs across\nmultiple benchmarks. Our results indicate that\ndomain-specific fine-tuning enhances factual\naccuracy, response coherence, and overall QA\neffectiveness in biomedical applications.\n• We systematically evaluate MedBioRAG\nacross retrieval performance, close-ended rea-\nsoning, and long-form synthesis, demonstrat-\ning its superiority over traditional lexical re-\ntrieval techniques and SoTA models like GPT-\n4o in terms of NDCG, MRR, and overall QA\nperformance. Our model achieves state-of-\nthe-art (SoTA) results on PubMedQA and\nBioASQ, surpassing previous benchmarks\nFigure 1.\nBy addressing challenges unique to medical and\nbiological domains, such as domain specificity, fac-\ntual accuracy, and contextual depth, MedBioRAG\nbridges the gap between general-purpose LLMs\nand domain-specific biomedical AI applications.\nOur results highlight the importance of integrating\nretrieval mechanisms with fine-tuned LLMs, pro-\nviding valuable insights for developing AI-powered\nmedical assistants and research tools. Through\na rigorous evaluation of retrieval strategies, re-\nsponse generation techniques, and domain adap-\ntation mechanisms, this work contributes to ad-\nvancing the field of biomedical AI, setting a new\nbenchmark for future research in medical question-\nanswering systems.\n2\nRelated Work\n2.1\nLLMs in Biomedical Domains\nThe increasing adoption of large language models\n(LLMs) in biomedical research has led to signifi-\ncant advancements in reasoning-based question an-\nswering (McDuff et al., 2023; Singhal et al., 2023a;\nSaab et al., 2024; Luo et al., 2022; Jeong et al.,\n2024; Zhang et al., 2024). LLMs have demon-\nstrated strong capabilities in retrieving medical\nknowledge, structured reasoning, and evidence-\nbased response generation. However, challenges\npersist in ensuring factual accuracy, mitigating hal-\nlucinations, and adapting these models for domain-\nspecific applications.\nFine-tuning (Ouyang et al., 2022; Nori et al.,\n2023) has played a critical role in enhancing\nLLM performance for biomedical QA. Domain-\nspecific models have leveraged task-specific fine-\n"}, {"page": 3, "text": "Figure 2: Overview of MedBioRAG. We perform semantic and lexical search for document retrieval, supervised\nfine-tuning of an LLM, and answer generation. The left section depicts semantic search (top) using vector-based\nretrieval and lexical search (bottom) using keyword-based retrieval. Retrieved documents are re-ranked and passed\nto a fine-tuned LLM for response generation. The right section illustrates answer generation, supporting both\nclose-ended QA (e.g., multiple-choice and yes/no questions) and long-form QA with structured responses.\ntuning to improve accuracy and contextual un-\nderstanding. Some models integrate uncertainty-\nguided search strategies, allowing them to refine re-\nsponses using external retrieval mechanisms. Oth-\ners employ preference-based optimization frame-\nworks, iteratively refining generated responses\nthrough synthetic preference datasets. These ap-\nproaches have achieved state-of-the-art perfor-\nmance by leveraging retrieval-based adaptation and\nhuman-aligned evaluation methodologies (Saab\net al., 2024; Frisoni et al., 2024).\n2.2\nRetrieval-Augmented Generation\nRAG (Lewis et al., 2020) has emerged as a pivotal\nframework for enhancing the reliability of LLMs in\nbiomedical applications (Saab et al., 2024). Unlike\nconventional parametric models, RAG-based sys-\ntems dynamically retrieve relevant documents from\nexternal knowledge sources, allowing for more con-\ntextually relevant and up-to-date responses. This is\nparticularly valuable in medicine, where accurate\ninformation retrieval is critical for clinical decision-\nmaking and evidence-based practice.\nLexical Search Lexical search is one of the most\nwidely used techniques in biomedical information\nretrieval, relying on exact keyword matching and\nstatistical ranking methods such as BM25 (Robert-\nson and Zaragoza, 2009). These approaches rank\ndocuments based on term frequency and inverse\ndocument frequency, enabling efficient retrieval\nfrom structured databases. Lexical search meth-\nods are well-suited for retrieving documents that\ncontain exact term matches and are widely used in\ntraditional biomedical search engines.\nHowever, lexical search faces significant limita-\ntions in handling the complexity of medical termi-\nnology. Challenges such as synonymy (e.g., \"heart\nattack\" vs. \"myocardial infarction\") and polysemy\n(words with multiple meanings) often lead to in-\ncomplete or suboptimal retrieval results. Addition-\nally, keyword-based methods struggle with con-\ntextual variability, limiting their ability to retrieve\ndocuments that convey conceptually relevant infor-\nmation without explicit keyword overlap.\nSemantic Search Semantic search methods\n(Muennighoff, 2022) have been developed to ad-\ndress the limitations of lexical search by leveraging\ndense vector representations and similarity-based\nretrieval. Instead of relying on exact term matches,\nsemantic search encodes medical texts into high-\ndimensional embeddings, enabling the retrieval of\ncontextually relevant documents even when exact\n"}, {"page": 4, "text": "terms are not present. This is particularly benefi-\ncial in biomedical domains, where concept-based\nretrieval is essential for improving response quality.\nPre-trained embedding models, such as those\ntrained on biomedical corpora, have significantly\nimproved the performance of semantic retrieval.\nThese models enable LLMs to retrieve semanti-\ncally similar documents based on conceptual rela-\ntionships rather than explicit term matching. Ad-\nvances in contrastive learning and hybrid retrieval\nstrategies have further optimized semantic search\nby refining ranking mechanisms and improving re-\ntrieval accuracy.\nSemantic search is particularly advantageous in\ncomplex biomedical QA tasks, where capturing\ncontextual meaning is essential. However, its ef-\nfectiveness depends on the quality of embeddings,\nthe robustness of ranking algorithms, and domain-\nspecific training objectives. MedBioRAG employs\nsemantic search as its primary retrieval mechanism,\nrefining its ranking strategies and retrieval effective-\nness to optimize biomedical information access.\n3\nMethod\nOur approach optimizes large language models\n(LLMs) for biomedical question answering (QA)\nby integrating supervised fine-tuning, semantic re-\ntrieval, and structured prompt engineering Figure 2.\nInstead of relying solely on parametric knowledge\nwithin an LLM, we enhance factual accuracy by re-\ntrieving relevant documents using a high-precision\nretrieval mechanism before generating responses.\nThe retrieval module is designed to fetch domain-\nspecific information, which is then processed and\npassed to a fine-tuned LLM for response genera-\ntion.\nThe proposed model operates in three main\nstages:\n1. Retrieval Mechanism: A hybrid search frame-\nwork incorporating both lexical and semantic\nsearch, with semantic search playing a domi-\nnant role.\n2. LLM-Based Answer Generation: Fine-tuned\nLLMs synthesize retrieved information into\ncoherent and contextually relevant answers.\n3. Prompt Engineering and Content Filtering:\nOptimized prompts structure the input to\nguide the model towards well-formed and fac-\ntually precise outputs.\nThis methodology ensures that the model ben-\nefits from external knowledge while maintaining\nstructured response generation.\n3.1\nRetrieval Mechanism\nThe retrieval component plays a crucial role in\nfetching the most relevant biomedical documents\nto enhance answer quality. We incorporate both\nlexical search (Robertson and Zaragoza, 2009) and\nsemantic search, with an emphasis on semantic\nsearch for higher retrieval precision.\nLexical Search Lexical retrieval is based on\nterm-frequency methods, utilizing BM25 as the\ncore ranking function. Given a query Q and a doc-\nument Di, BM25 ranks documents based on:\nIDF(t) = log\n\u0012N −nt + 0.5\nnt + 0.5\n+ 1\n\u0013\n(1)\nTF(t, Di) =\n(k1 + 1)ft,Di\nk1(1 −b + b ·\n|Di|\navgDL) + ft,Di\n(2)\nBM25(Di, Q) =\nX\nt∈Q\nIDF(t) × TF(t, Di)\n(3)\nwhere nt is the number of documents containing\nterm t, N is the total number of documents, ft,Di\nis the frequency of t in Di, and |Di| represents the\ndocument length. avgDL is the average document\nlength in the collection.\nSemantic Search Unlike lexical search, seman-\ntic search retrieves documents based on contextual\nsimilarity rather than exact term matching. This ap-\nproach employs dense vector representations, map-\nping queries and documents into a shared embed-\nding space.\nA given query Q and document Di are first trans-\nformed into vector representations using an encoder\nfunction ϕ:\nvQ = ϕ(Q),\nvDi = ϕ(Di)\n(4)\nwhere vQ and vDi are the dense vector represen-\ntations of the query and document, respectively.\nTo determine document relevance, the similarity\nscore between the query and a document is com-\nputed using the cosine similarity:\nSim(Q, Di) =\nvQ · vDi\n∥vQ∥∥vDi∥\n(5)\n"}, {"page": 5, "text": "Figure 3: Task-wise Performance Comparison across Retrieval and QA Tasks. This figure compares the performance\nof MedBioRAG, Previous State-of-the-Art (SoTA), and GPT-4o (Base Model) across three major categories: Text\nRetrieval Performance, Close-ended QA, and Long-form QA. The left section evaluates retrieval effectiveness on\nNFCorpus and TREC-COVID using NDCG and MRR scores. The middle section presents accuracy scores for\nMedQA, PubMedQA, and BioASQ, demonstrating improvements achieved with MedBioRAG. The right section\nassesses response quality using ROUGE scores for LiveQA, PubMedQA, BioASQ, and MedicationQA, highlighting\nMedBioRAG’s effectiveness in generating structured long-form answers. Across all tasks, MedBioRAG consistently\noutperforms previous SoTA models (Sawarkar et al., 2024; Chen et al., 2023; Yasunaga et al., 2022) and the GPT-4o\nbase model(OpenAI, 2024).\nwhere · represents the dot product, and ∥vQ∥and\n∥vDi∥denote the Euclidean norms of the respective\nvectors.\nThe retrieval system ranks documents based on\ntheir similarity scores, selecting the top k docu-\nments:\nDtop-k = argmaxk Sim(Q, Di)\n(6)\nThis process allows the system to retrieve doc-\numents that are semantically relevant, even when\nexact keyword matches are absent. The effective-\nness of semantic search depends on the quality of\nthe embedding model ϕ, the retrieval ranking mech-\nanism, and domain-specific pretraining.\n3.2\nLLM-Based Answer Generation\nOnce relevant documents are retrieved, the next\nstep involves generating well-structured and con-\ntextually relevant responses.\nThis is achieved\nthrough a combination of supervised fine-tuning\nand structured prompt construction.\nSupervised Fine-tuning LLMs To adapt large\nlanguage models (LLMs) for biomedical question\nanswering (QA), we employ supervised fine-tuning.\nFine-tuning ensures that the model aligns with\ndomain-specific knowledge and exhibits higher fac-\ntual accuracy when generating responses. We train\nthe model using a dataset consisting of (x, y) pairs,\nwhere x represents the input query and retrieved\ndocument context, and y is the expected answer:\nLLM = −\n|y|\nX\nt=1\nlog Pθ(yt|y<t, x)\n(7)\nwhere Pθ denotes the probability distribution of\nthe model’s next token prediction, and yt represents\nthe t-th token of the target response.\nFine-tuning enables the model to develop a\nstronger understanding of biomedical terminolo-\ngies, clinical reasoning, and literature-based ques-\ntion answering.\nContextual Prompt Construction To further\nguide response generation, we employ prompt en-\ngineering techniques that structure the input for\noptimal output quality. A well-designed prompt\nensures factual consistency and coherence while\nmitigating hallucinations.\nTo further enhance reliability, we apply content\nfiltering techniques to remove redundant, irrelevant,\nor low-confidence outputs. The model assigns a\nconfidence score sc to each generated response:\nsc = softmax(WohT )\n(8)\nwhere hT represents the final hidden state of\nthe output sequence, and Wo is a learned projec-\ntion matrix. Responses with confidence scores be-\nlow a predefined threshold are discarded or revised\nthrough iterative refinement.\nBy integrating retrieval-augmented generation\n(RAG), fine-tuning, and structured prompt engi-\n"}, {"page": 6, "text": "neering, our approach optimizes LLMs for biomed-\nical QA, ensuring that generated responses are both\ncontextually appropriate and factually accurate.\n4\nExperiments\n4.1\nExperimental Setups\nTo evaluate the effectiveness of MedBioRAG, we\nconduct comprehensive experiments across multi-\nple biomedical QA benchmarks. Our evaluation\nconsists of three major experimental settings: (1)\nretrieval performance, (2) close-ended QA, and (3)\nlong-form QA. We compare our method against\nseveral baselines, including both general-purpose\nand fine-tuned large language models.\nBaselines To evaluate the effectiveness of Med-\nBioRAG, we conduct experiments across various\nbiomedical question-answering (QA) tasks, com-\nparing different model configurations and retrieval\nstrategies. Our evaluation framework includes com-\nparisons between a base model in a zero-shot set-\nting and a fine-tuned LLM, as well as LLMs with\nand without retrieval augmentation. Specifically,\nwe compare a fine-tuned LLM without retrieval-\naugmented generation (RAG) to the same model\nwith RAG enabled, allowing us to assess the impact\nof external document retrieval on answer genera-\ntion.\nRetrieval Evaluation For document retrieval,\nwe evaluate MedBioRAG’s performance on the\nNFCorpus (Boteva et al., 2016) and TREC-COVID\n(Voorhees et al., 2020) datasets, comparing lexical\nand semantic search methods. Lexical retrieval re-\nlies on BM25, while semantic search utilizes dense\nembeddings for vector-based document retrieval.\nWe measure retrieval effectiveness using standard\ninformation retrieval metrics, including Discounted\nCumulative Gain (DCG), Normalized Discounted\nCumulative Gain (NDCG), Mean Reciprocal Rank\n(MRR), Precision@10, Recall@10, F1-score@10,\nand Mean Average Precision (MAP). These metrics\nassess the ranking quality of retrieved documents,\nwith higher scores indicating better alignment be-\ntween retrieved content and the user’s query. The\nresults demonstrate that semantic search consis-\ntently outperforms lexical search across all metrics,\nhighlighting its ability to capture contextual mean-\ning more effectively.\nClose-ended QA Evaluation For multiple-\nchoice biomedical QA, we evaluate MedBioRAG\non MedQA, PubMedQA, and BioASQ (Jin et al.,\n2020, 2019; Nentidis et al., 2023; Vilares and\nGómez-Rodríguez, 2019). These datasets test the\nmodel’s ability to select the correct answer from\npredefined options based on medical knowledge\nand retrieved evidence. Accuracy is used as the\nprimary evaluation metric, measuring the percent-\nage of correctly answered questions. MedBioRAG\ndemonstrates significant improvements over both\nzero-shot and fine-tuned LLM baselines, partic-\nularly when retrieval is incorporated. By lever-\naging external knowledge sources, MedBioRAG\nmitigates hallucinations and improves answer re-\nliability, outperforming previous state-of-the-art\n(SoTA) models.\nLong-form QA Evaluation To assess Med-\nBioRAG’s ability to generate detailed, structured\nresponses, we conduct long-form QA experiments\non LiveQA, MedicationQA, PubMedQA, and\nBioASQ. These tasks require the model to generate\nfree-form explanations based on retrieved biomed-\nical literature. The performance of long-form an-\nswer generation is measured using ROUGE scores,\nBLEU scores, BERTScore, and BLEURT. ROUGE\nevaluates the overlap between generated responses\nand reference answers, BLEU measures n-gram\nprecision, BERTScore assesses semantic similarity\nusing contextual embeddings, and BLEURT cap-\ntures fluency and coherence in model outputs. The\nresults indicate that MedBioRAG achieves substan-\ntial gains in factual accuracy and coherence, consis-\ntently outperforming GPT-4o and fine-tuned LLMs\nwithout retrieval.\n4.2\nExperimental Results\nRetrieval Performance\nTo evaluate retrieval performance, we compare\nMedBioRAG’s lexical and semantic search com-\nponents on NFCorpus and TREC-COVID datasets\nusing standard retrieval metrics. Table 3 compares\nretrieval performance across NFCorpus and TREC-\nCOVID datasets using lexical and semantic search.\nResults indicate that semantic search consistently\noutperforms lexical retrieval across all evaluation\nmetrics, including NDCG@10, MRR@10, and\nPrecision@10.\nSpecifically, on NFCorpus, se-\nmantic search achieves an NDCG@10 score of\n37.91, significantly higher than lexical search at\n31.34. Similarly, MRR@10 improves from 51.63\nin lexical search to 64.29 in semantic retrieval.\nThe same trend is observed in TREC-COVID,\nwhere MedBioRAG’s semantic search component\nattains an MRR@10 of 89.17, surpassing the lexi-\ncal search performance of 82.50. These improve-\n"}, {"page": 7, "text": "Dataset\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU\nBERTScore\nBLEURT\nLiveQA\nFine-Tuned GPT-4o\n24.12\n6.18\n13.31\n1.63\n1.10\n-46.48\n+ MedBioRAG\n15.73\n4.58\n10.74\n1.20\n2.29\n-86.99\nGPT-4o\n26.96\n5.80\n13.42\n1.41\n-2.93\n-34.79\n+ MedBioRAG\n27.33\n6.39\n13.42\n15.29\n-1.60\n-29.99\nMedicationQA\nFine-Tuned GPT-4o\n24.69\n8.80\n17.61\n2.49\n8.98\n-33.82\n+ MedBioRAG\n27.73\n15.09\n22.72\n7.24\n8.79\n-33.63\nGPT-4o\n22.92\n13.69\n18.70\n7.89\n8.55\n-6.92\n+ MedBioRAG\n19.85\n4.20\n10.97\n0.98\n-7.63\n-33.21\nPubMedQA\nFine-Tuned GPT-4o\n35.82\n13.55\n26.09\n4.34\n35.33\n-9.23\n+ MedBioRAG\n37.49\n14.78\n27.89\n6.11\n37.02\n-3.89\nGPT-4o\n25.72\n9.02\n17.05\n2.48\n17.04\n-9.04\n+ MedBioRAG\n26.39\n9.55\n17.47\n2.73\n18.10\n-7.86\nBioASQ\nFine-Tuned GPT-4o\n32.69\n16.84\n25.11\n6.52\n32.97\n-2.41\n+ MedBioRAG\n34.30\n18.81\n27.74\n6.12\n35.43\n-15.44\nGPT-4o\n13.97\n5.51\n10.08\n1.27\n0.22\n-24.84\n+ MedBioRAG\n22.29\n8.21\n15.64\n2.27\n11.60\n-12.50\nTable 1: Performance comparison of various models on long-form QA tasks across different datasets (LiveQA,\nMedicationQA, PubMedQA, and BioASQ). The evaluation metrics include ROUGE scores, BLEU, BERTScore,\nand BLEURT. The highest value for each dataset and metric is highlighted in bold to indicate the best-performing\nconfiguration.\nMethod\nMedQA\nPubMedQA\nBioASQ\nFine-Tuned GPT-4o\n87.88\n80.70\n97.06\n+ MedBioRAG\n89.47\n85.00\n98.32\nGPT-4o\n81.82\n44.74\n96.12\n+ MedBioRAG\n86.86\n66.67\n97.06\nGPT-4o-mini\n67.68\n77.55\n96.32\n+ MedBioRAG\n70.71\n76.32\n97.06\nGPT-4\n66.67\n52.63\n96.32\n+ MedBioRAG\n78.79\n72.81\n97.79\nGPT-3.5\n51.52\n19.30\n88.24\n+ MedBioRAG\n45.36\n38.60\n66.91\nTable 2: Performance comparison of various models on\nclose-ended QA tasks. Fine-tuning GPT-4o with Med-\nBioRAG achieves outperforming other methods across\nMedQA, PubMedQA, and BioASQ datasets.\nMed-\nBioRAG significantly improves retrieval-augmented\ngeneration (RAG) performance, particularly in close-\nended QA. Bold values indicate the best performance\nfor each dataset.\nments demonstrate the effectiveness of semantic re-\ntrieval in identifying contextually relevant biomed-\nical literature.\nFigure 4 illustrates the effect of increasing Top-\nK retrieval on MedQA and PubMedQA. As the\nnumber of retrieved documents increases, perfor-\nmance initially improves but deteriorates beyond\nan optimal threshold due to noise and conflicting\nDataset\nNFCorpus\nTREC-COVID\nMetric\nLexical\nSemantic\nLexical\nSemantic\nDCG@10\n2.65\n3.27\n4.39\n5.55\nNDCG@10\n31.34\n37.91\n48.35\n61.02\nMRR@10\n51.63\n64.29\n82.50\n89.17\nPrecision@10\n23.04\n27.88\n49.60\n64.20\nRecall@10\n15.95\n18.70\n0.43\n0.54\nF1-score@10\n12.61\n14.99\n0.85\n1.07\nMAP@10\n46.01\n56.15\n72.31\n82.19\nTable 3: Comparison of MedBioRAG with Lexical and\nSemantic Search across NFCorpus and TREC-COVID\ndatasets. The results indicate that MedBioRAG with Se-\nmantic Search consistently outperforms Lexical Search\nacross all metrics for both datasets.\ninformation. This highlights the importance of a\nbalanced retrieval strategy in biomedical QA.\nFine-tuned LLMs with MedBioRAG demon-\nstrate superior retrieval capabilities compared to\nmodels relying solely on parametric knowledge.\nThe integration of MedBioRAG enables fine-tuned\nmodels to access up-to-date biomedical literature,\nimproving their ability to generate factually accu-\nrate and contextually relevant responses.\nClose-ended QA Performance\nFor close-ended QA, we compare MedBioRAG\nagainst prior state-of-the-art (SoTA) models, in-\n"}, {"page": 8, "text": "Figure 4: Impact of increasing Top-K on MedQA short-\nform QA. As the number of retrieved documents in-\ncreases, the performance of all evaluation metrics de-\ncreases. Given the nature of the task, which expects\nconcise short-form answers, retrieving more documents\nintroduces noise and conflicting information, negatively\naffecting answer quality.\ncluding GPT-4o and fine-tuned biomedical LLMs.\nResults indicate that MedBioRAG achieves su-\nperior performance across multiple benchmarks.\nOn MedQA, MedBioRAG improves accuracy to\n90%, outperforming the previous SoTA model\nat 82%. Similarly, in PubMedQA, MedBioRAG\nattains an accuracy of 85%, exceeding the 82%\nachieved by previous models. The largest improve-\nment is observed in BioASQ, where MedBioRAG\nachieves 96% accuracy, significantly higher than\nthe prior SoTA score of 94%. These results confirm\nthat integrating retrieval-based augmentation with\nfine-tuned LLMs enhances factual consistency and\ndomain-specific reasoning in biomedical QA.\nOverall, our experimental results validate the ef-\nfectiveness of MedBioRAG in enhancing biomedi-\ncal QA by integrating semantic retrieval and fine-\ntuned LLM-based answer generation.\nLong-form QA Performance For long-form\nQA, we evaluate MedBioRAG on LiveQA, Medica-\ntionQA, PubMedQA, and BioASQ using ROUGE\nscores, BLEU, and BERTScore Table 1. Med-\nBioRAG consistently outperforms fine-tuned GPT-\n4o across all datasets. In LiveQA, MedBioRAG\nachieves a ROUGE-1 score of 27.33 and a BLEU\nscore of 15.29, outperforming both fine-tuned GPT-\n4o and base GPT-4o models. Similar improvements\nare seen in MedicationQA, where MedBioRAG at-\ntains the highest BLEU score of 7.89, surpassing\nprevious approaches. In PubMedQA, MedBioRAG\nimproves ROUGE-L to 27.89 and BERTScore to\n37.02, indicating enhanced response coherence and\nfactuality.\nBioASQ results further highlight MedBioRAG’s\neffectiveness, achieving the highest BLEURT score\namong all models. These improvements demon-\nstrate that retrieval-augmented fine-tuning signifi-\ncantly enhances response fluency and factual cor-\nrectness in long-form biomedical QA tasks.\nFine-tuned LLMs with MedBioRAG achieve\nsubstantial gains in long-form answer generation\nby leveraging real-time document retrieval. Com-\npared to models without retrieval augmentation,\nMedBioRAG-enhanced fine-tuned LLMs produce\nresponses that are more structured, informative,\nand aligned with expert-reviewed biomedical liter-\nature.\n5\nConclusion\nIn this work, we introduce MedBioRAG, a retrieval-\naugmented generation (RAG) framework designed\nto enhance biomedical question answering (QA) by\nintegrating semantic retrieval, document ranking,\nand fine-tuned large language models (LLMs). Our\napproach improves factual accuracy by retrieving\nrelevant biomedical literature, enabling more pre-\ncise and contextually aware response generation.\nExperiments show that MedBioRAG outper-\nforms both fine-tuned LLMs and previous state-of-\nthe-art (SoTA) models. Semantic retrieval signifi-\ncantly improves NDCG, MRR, and Precision@10\ncompared to lexical search. In close-ended QA,\nMedBioRAG achieves higher accuracy on MedQA,\nPubMedQA, and BioASQ, surpassing previous\nbenchmarks. For long-form QA, it consistently\nimproves ROUGE, BLEU, and BERTScore, en-\nhancing response fluency and factual accuracy.\nKey contributions include hybrid retrieval that\nbalances precision and recall, fine-tuned LLMs that\nreduce hallucinations, and prompt engineering for\nimproved response structure. Future work will fo-\ncus on refining retrieval ranking, optimizing infer-\nence speed, and adapting to specialized biomedical\ndomains.\nLimitations\nMedBioRAG’s key limitation is the lack of valida-\ntion by medical professionals, making it unclear\nhow well the model aligns with expert reasoning.\n"}, {"page": 9, "text": "While it enhances biomedical QA through retrieval-\naugmented generation, its effectiveness depends\non retrieval quality, and unresolved contradictions\nin retrieved documents raise concerns about fac-\ntual accuracy. Real-time retrieval also increases\ncomputational overhead, limiting applicability in\ntime-sensitive settings. Additionally, further fine-\ntuning is needed for specialized domains like clini-\ncal diagnosis. Broader evaluation on real-world\ndatasets, such as clinical case reports and elec-\ntronic health records (EHRs), is necessary to as-\nsess its practical utility. Despite these challenges,\nMedBioRAG highlights the potential of retrieval-\naugmented LLMs in biomedical AI.\nReferences\nVera Boteva, Demian Gholipour Ghalandari, Artem\nSokolov, and Stefan Riezler. 2016. A full-text learn-\ning to rank dataset for medical information retrieval.\nIn Advances in Information Retrieval - 38th Euro-\npean Conference on IR Research, ECIR 2016, Padua,\nItaly, March 20-23, 2016. Proceedings, volume 9626\nof Lecture Notes in Computer Science, pages 716–\n722. Springer.\nZeming Chen, Alejandro Hernández Cano, Angelika\nRomanou, Antoine Bonnet, Kyle Matoba, Francesco\nSalvi, Matteo Pagliardini, Simin Fan, Andreas\nKöpf, Amirkeivan Mohtashami, Alexandre Sallinen,\nAlireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk,\nDeniz Bayazit, Axel Marmet, Syrielle Montariol,\nMary-Anne Hartley, Martin Jaggi, and Antoine\nBosselut. 2023.\nMeditron-70b: Scaling medical\npretraining for large language models.\nPreprint,\narXiv:2311.16079.\nGiacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gi-\nanluca Moro, and Zaiqiao Meng. 2024. To generate\nor to retrieve? on the effectiveness of artificial con-\ntexts for medical open-domain question answering.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 9878–9919, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nMinbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Tae-\nwhoo Lee, and Jaewoo Kang. 2024.\nOlaph: Im-\nproving factuality in biomedical long-form question\nanswering. Preprint, arXiv:2405.12701.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2020. What dis-\nease does this patient have? a large-scale open do-\nmain question answering dataset from medical exams.\nPreprint, arXiv:2009.13081.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. Pubmedqa: A dataset for\nbiomedical research question answering. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2567–2577. Asso-\nciation for Computational Linguistics (ACL).\nKamal raj Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. 2021.\nBioELEC-\nTRA:pretrained biomedical text encoder using dis-\ncriminators. In Proceedings of the 20th Workshop\non Biomedical Language Processing, pages 143–154,\nOnline. Association for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS 2020), volume 33,\npages 9459–9474. Curran Associates, Inc.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBiogpt:\ngenerative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6):bbac409.\nXing Han Lù. 2024. Bm25s: Orders of magnitude faster\nlexical search via eager sparse scoring. Preprint,\narXiv:2407.03618.\nDaniel McDuff, Mike Schaekermann, Tao Tu, Anil\nPalepu, Amy Wang, Jake Garrison, Karan Singhal,\nYash Sharma, Shekoofeh Azizi, Kavita Kulkarni,\nLe Hou, Yong Cheng, Yun Liu, S Sara Mahdavi,\nSushant Prakash, Anupam Pathak, Christopher Sem-\nturs, Shwetak Patel, Dale R Webster, Ewa Domi-\nnowska, Juraj Gottweis, Joelle Barral, Katherine\nChou, Greg S Corrado, Yossi Matias, Jake Sunshine,\nAlan Karthikesalingam, and Vivek Natarajan. 2023.\nTowards accurate differential diagnosis with large\nlanguage models. Preprint, arXiv:2312.00164.\nNiklas\nMuennighoff.\n2022.\nSgpt:\nGpt\nsen-\ntence embeddings for semantic search.\nPreprint,\narXiv:2202.08904.\nAnastasios Nentidis, Georgios Katsimpras, Anasta-\nsia Krithara, Salvador Lima López, Eulália Farré-\nMaduell, Luis Gasco, Martin Krallinger, and Geor-\ngios Paliouras. 2023. Overview of BioASQ 2023: The\nEleventh BioASQ Challenge on Large-Scale Biomedi-\ncal Semantic Indexing and Question Answering, page\n227–250. Springer Nature Switzerland.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan,\nRichard Edgar, Nicolo Fusi, Nicholas King, Jonathan\nLarson, Yuanzhi Li, Weishung Liu, Renqian Luo,\nScott Mayer McKinney, Robert Osazuwa Ness, Hoi-\nfung Poon, Tao Qin, Naoto Usuyama, Chris White,\nand Eric Horvitz. 2023. Can generalist foundation\nmodels outcompete special-purpose tuning? case\nstudy in medicine. Preprint, arXiv:2311.16452.\nOpenAI. 2024.\nGpt-4o system card.\nPreprint,\narXiv:2410.21276.\n"}, {"page": 10, "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. Preprint, arXiv:2203.02155.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333–389.\nKhaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno,\nDavid Stutz, Ellery Wulczyn, Fan Zhang, Tim\nStrother, Chunjong Park, Elahe Vedadi, Juanma Zam-\nbrano Chaves, Szu-Yeu Hu, Mike Schaekermann,\nAishwarya Kamath, Yong Cheng, David G. T. Bar-\nrett, Cathy Cheung, Basil Mustafa, Anil Palepu,\nDaniel McDuff, Le Hou, Tomer Golany, Luyang\nLiu, Jean baptiste Alayrac, Neil Houlsby, Nenad\nTomasev, Jan Freyberg, Charles Lau, Jonas Kemp,\nJeremy Lai, Shekoofeh Azizi, Kimberly Kanada, Si-\nWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shak-\neri, Luheng He, Ben Caine, Albert Webson, Natasha\nLatysheva, Melvin Johnson, Philip Mansfield, Jian\nLu, Ehud Rivlin, Jesper Anderson, Bradley Green,\nRenee Wong, Jonathan Krause, Jonathon Shlens,\nEwa Dominowska, S. M. Ali Eslami, Katherine\nChou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu,\nJames Manyika, Jeff Dean, Demis Hassabis, Yossi\nMatias, Dale Webster, Joelle Barral, Greg Corrado,\nChristopher Semturs, S. Sara Mahdavi, Juraj Got-\ntweis, Alan Karthikesalingam, and Vivek Natarajan.\n2024. Capabilities of gemini models in medicine.\nPreprint, arXiv:2404.18416.\nKunal Sawarkar, Abhilasha Mangal, and Shivam Raj\nSolanki. 2024.\nBlended rag:\nImproving rag\n(retriever-augmented generation) accuracy with se-\nmantic search and hybrid query-based retrievers. In\n2024 IEEE 7th International Conference on Multi-\nmedia Information Processing and Retrieval (MIPR),\npages 155–161.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara\nMahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen\nPfohl, Perry Payne, Martin Seneviratne, Paul Gam-\nble, Chris Kelly, Abubakr Babiker, Nathanael Schärli,\nAakanksha Chowdhery, Philip Mansfield, Dina\nDemner-Fushman, Blaise Agüera y Arcas, Dale\nWebster, Greg S. Corrado, Yossi Matias, Kather-\nine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu,\nAlvin Rajkomar, Joelle Barral, Christopher Semturs,\nAlan Karthikesalingam, and Vivek Natarajan. 2023a.\nLarge language models encode clinical knowledge.\nNature, 620(7972):172–180.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, Mike Schaek-\nermann, Amy Wang, Mohamed Amin, Sami Lach-\ngar, Philip Mansfield, Sushant Prakash, Bradley\nGreen, Ewa Dominowska, Blaise Aguera y Arcas,\nNenad Tomasev, Yun Liu, Renee Wong, Christo-\npher Semturs, S. Sara Mahdavi, Joelle Barral, Dale\nWebster, Greg S. Corrado, Yossi Matias, Shekoofeh\nAzizi, Alan Karthikesalingam, and Vivek Natara-\njan. 2023b. Towards expert-level medical question\nanswering with large language models. Preprint,\narXiv:2305.09617.\nDavid Vilares and Carlos Gómez-Rodríguez. 2019.\nHead-qa: A healthcare dataset for complex reason-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics (ACL\n2019), pages 960–966, Florence, Italy. Association\nfor Computational Linguistics.\nEllen M. Voorhees, Tasmeer Alam, Steven Bedrick,\nDina Demner-Fushman, William R. Hersh, Kyle Lo,\nKirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2020.\nTREC-COVID: constructing a pandemic information\nretrieval test collection. SIGIR Forum, 54(1):1:1–\n1:12.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. In Association for Computational\nLinguistics (ACL).\nKai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling\nYan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen,\nBrian D. Davison, Hui Ren, Jing Huang, Chen\nChen, Yuyin Zhou, Sunyang Fu, Wei Liu, Tian-\nming Liu, Xiang Li, Yong Chen, Lifang He, James\nZou, Quanzheng Li, Hongfang Liu, and Lichao\nSun. 2024. A generalist vision–language foundation\nmodel for diverse biomedical tasks. Nature Medicine,\n30(11):3129–3141.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Haonan Chen, Zheng\nLiu, Zhicheng Dou, and Ji-Rong Wen. 2024. Large\nlanguage models for information retrieval: A survey.\nPreprint, arXiv:2308.07107.\n"}, {"page": 11, "text": "A\nExperimental Details\nTable 4: Experimental details of the fine-tuning process for various biomedical question-answering tasks. The table\noutlines the dataset used, training duration, base model, number of epochs, and batch size for each task. Fine-tuning\nwas performed across closed-ended, long-form, and short-form QA datasets to optimize model performance. All\nbase models used for fine-tuning were GPT-4o.\nTask\nTrain Dataset\nTrain Samples\nTraining Duration\nEpochs\nBatch Size\nClosed-ended QA\nMedQA\n10,178\n3h 25m 33s\n2\n13\nPubMedQA (PQA-L)\n552\n7h 46m 44s\n3\n1\nBioSQA\n5,049\n3h 10m 7s\n3\n2\nLong-form QA\nPubMedQA (PQA-A)\n196,144\n1d 6h 29m 20s\n1\n64\nMedicationQA\n551\n1h 44m 18s\n3\n1\nLiveQA\n500\n1h 46m 17s\n3\n1\nBioSQA\n5,049\n2h 36m 49s\n3\n10\nCombined Custom Dataset\n6,652\n2h 6m 1s\n3\n13\nShort-form QA\nMedQA\n10,178\n1h 49m 44s\n2\n13\nTable 5: Comparison of prompting strategies and decoding parameters for Closed-Ended, Long-Form, and Short-\nForm Question Answering. The table outlines system messages, token limits, and decoding parameters optimized\nfor different QA formats.\nParameter\nClosed-Ended QA\nLong-Form QA\nShort-Form QA\nSystem Message\n\"You are an expert medical AI\nassistant. Answer the following\nquestion using only one letter:\nA, B, C, or D.\"\n\"You are a biomedical research\nexpert. Generate precise and\nwell-structured answers.\"\n\"You are an expert medical AI\nassistant. Provide concise and\naccurate answers.\"\nMax Tokens\n2\n300\n50\nTemperature\n0.1\n0.2\n0.2\nTop P\n0.7\n0.8\n0.85\nFrequency Penalty\n0.5\n0.0\n0.2\nPresence Penalty\n0.1\n0.0\n0.0\nStop Sequence\n[\"\\n\"]\n-\n-\n"}]}