{"doc_id": "arxiv:2512.04834", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.04834.pdf", "meta": {"doc_id": "arxiv:2512.04834", "source": "arxiv", "arxiv_id": "2512.04834", "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case", "authors": ["Vignesh Kumar Kembu", "Pierandrea Morandini", "Marta Bianca Maria Ranzini", "Antonino Nocera"], "published": "2025-12-04T14:17:46Z", "updated": "2025-12-04T14:17:46Z", "summary": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.04834v1", "url_pdf": "https://arxiv.org/pdf/2512.04834.pdf", "meta_path": "data/raw/arxiv/meta/2512.04834.json", "sha256": "067768b6255d5ea8bc7222c079b6d3c0029dcaf8e38d8bfefc4e2474b73cc458", "status": "ok", "fetched_at": "2026-02-18T02:25:24.782024+00:00"}, "pages": [{"page": 1, "text": "Are LLMs Truly Multilingual?\nExploring Zero-Shot Multilingual Capability of\nLLMs for Information Retrieval: An Italian\nHealthcare Use Case\nVignesh Kumar Kembu1,2[0009-0002-3782-8111],\nPierandrea Morandini2[0000-0002-8615-3766],\nMarta Bianca Maria Ranzini2[0000-0001-8275-6028], and\nAntonino Nocera1[0000-0003-2120-2341]\n1 Department of Electrical, Computer and Biomedical Engineering,\nUniversity of Pavia, Italy\nvigneshkumar.kembu01@universitadipavia.it, antonino.nocera@unipv.it\n2 IRCCS Humanitas Research Hospital, Milan, Italy\n{vignesh.kembu,pierandrea.morandini,marta.ranzini}@humanitas.it\nAbstract. Large Language Models (LLMs) have become a key topic in\nAI and NLP, transforming sectors like healthcare, finance, education, and\nmarketing by improving customer service, automating tasks, providing\ninsights, improving diagnostics, and personalizing learning experiences.\nInformation extraction from clinical records is a crucial task in digital\nhealthcare. Although traditional NLP techniques have been used for this\nin the past, they often fall short due to the complexity, variability of clin-\nical language, and high inner semantics in the free clinical text. Recently,\nLarge Language Models (LLMs) have become a powerful tool for better\nunderstanding and generating human-like text, making them highly ef-\nfective in this area. In this paper, we explore the ability of open-source\nmultilingual LLMs to understand EHRs (Electronic Health Records) in\nItalian and help extract information from them in real-time. Our detailed\nexperimental campaign on comorbidity extraction from EHR reveals that\nsome LLMs struggle in zero-shot, on-premises settings, and others show\nsignificant variation in performance, struggling to generalize across var-\nious diseases when compared to native pattern matching and manual\nannotations.\nKeywords: LLMs · Multilingual · Information Retrieval · Healthcare ·\nEHRs.\n1\nIntroduction\nLarge Language Models (LLMs) have revolutionized the field of natural language\nprocessing, showcasing impressive capabilities in text generation, comprehension,\narXiv:2512.04834v1  [cs.AI]  4 Dec 2025\n"}, {"page": 2, "text": "2\nVK. Kembu et al.\nand conversational interaction. The models, such as GPT’s and Google’s Bard\netc., are based on advanced neural networks with billions of parameters. They\ncan grasp context, semantics, and intricate language nuances, which allows them\nto perform exceptionally across diverse applications—from chatbots and virtual\nassistants to content creation and programming assistance. However, despite\ntheir strengths, LLMs encounter several challenges [21,16]. They can generate\nincorrect information, may misinterpret subtle input variations and have the\npotential to produce biased or inappropriate content. Ongoing research seeks to\naddress these issues through improved training techniques, fine-tuning processes\nand the establishment of ethical guidelines. Today, LLMs play a crucial role in\nvarious sectors, including education, healthcare and customer service, offering in-\nnovative solutions while also raising important questions about security, privacy\nand the future of human-computer interactions. As these models continue to\nevolve, their potential to transform communication and creativity appears huge,\npresenting both exciting opportunities and complex challenges for society [6,28].\nIn context of healthcare LLMs are revolutionizing healthcare by enhanc-\ning clinical decision-making, automating administrative processes and improv-\ning patient engagement. These advanced AI systems, trained on large datasets,\ncan interpret and generate human-like text, making them valuable for various\napplications in healthcare. One significant area of impact is in clinical deci-\nsion support. LLMs can analyze medical literature, patient records and research\ndata to provide evidence-based recommendations to healthcare professionals.\nBy synthesizing complex information quickly, they aid clinicians in diagnosing\nconditions and selecting appropriate treatments, ultimately improving patient\noutcomes and are streamlining administrative tasks such as scheduling, billing\nand documentation [17,11,24].\nScenario. In a clinical context, we consider a clinician aiming to extract co-\nmorbidities from EHRs using LLMs, focusing on a simple and direct approach,\nwhich does not need any manipulation of the LLM prompt(Zero-shot). Consid-\nering this, we formulate three key questions to be addressed in the evaluation of\nthe scenario.\n– Q1. Can we use LLMs in Zero-shot to extract comorbidities from EHRs?\n– Q2. Can LLMs substitute a regular expression-based approach?\n– Q3. Can we find a best LLM among the chosen ones?\nExtending from the scenario, we evaluate LLMs (Large Language Models)\nin multilingual settings within digital health and clinical decision support. This\nresearch explores the use of different LLMs in the healthcare domain, specifically\nfocusing on the classification tasks involving Italian patient EHRs. The objective\nwas to design, develop a generalized data gathering ETL of patient electronic\nhealth records, LLM pipeline and framework which can handle different data\nformats and ranges of models with different parameters, capable of extracting\ndiverse types of information from clinical records. In context with this, our cur-\nrent study leverages usage of 6 open-source models from three model families.\nThe inference was carried out in Zero-shot setting on free EHR text of patients,\nwhich are discussed below in detail.\n"}, {"page": 3, "text": "Are LLMs Truly Multilingual?\n3\n2\nPreliminaries\nLarge Language Models (LLMs) are advanced models designed to process\nand generate human language, trained on vast amount of text. These models\nuse network architecture called transformers [25], which help them manage and\nproduce text as in human communication. These models are typically built with\nbillions of parameters and more, allowing them to capture internal patterns in\nlanguage, context and reasoning [15].\nClosed source models, such as GPT [5,1], Gemini [23,7], and Claude [4,3],\nare proprietary and accessible via APIs, enabling businesses to integrate ad-\nvanced AI without building models from scratch. But their closed nature raises\nsecurity concerns, particularly in sensitive areas like healthcare. On the other\nhand, Open source models like Qwen’s [20,27], Bloom’s [26], Llama’s [2,10], and\nMistral’s [13,14], offer transparency and customization, allowing fine-tuning for\nspecific tasks, such as healthcare applications, while providing better control\nover data privacy and regulatory compliance.\nInformation Extraction (IE) is a crucial task in natural language process-\ning (NLP) that involves automatically extracting structured information from\nunstructured text. This process is key for converting large volumes of textual\ndata, such as news articles, medical text (EHRs) and social media posts, into\nusable information for further analysis or decision-making[18]. Large Language\nModels (LLMs), such as GPT and BERT, have significantly advanced the field\nof Information Extraction (IE). These models, trained on massive corpora, excel\nat identifying entities, relationships and other structured information in diverse\nand unstructured text without requiring task-specific training data [8]. By lever-\naging their deep contextual understanding, LLMs have shown superior perfor-\nmance in extracting nuanced and complex information, enabling more accurate\nand adaptable IE systems across various domains [5].\nMultilingual understanding and generation have made notable progress\nthrough models trained on large and diverse multilingual data combined with\nadvanced training techniques. Large language models demonstrate impressive ro-\nbustness in English, leveraging abundant data and resources. Evidence on their\nperformance and reliability in other languages remains limited and underex-\nplored [19]. LLMs show potential in healthcare, helping in medical Q&A, diag-\nnosis, counseling, and EHR data extraction, even in multilingual settings [29].\nElectronic Health Records (EHRs) are digital information that contains\npatients medical history, such as diagnoses, treatments, medications, laboratory\nresults and documentation notes from clinicians. EHRs improve continuity of\ncare and efficiency by allowing the update of patient information in real time [12].\nHowever, most EHR data is unstructured, making it difficult to extract mean-\ningful insights. Information extraction on EHR data was traditionally addressed\nby regular expressions, however building the pattern is complicated, not gen-\neralizable and requires deep domain knowledge with all linguistics nuances of\nthe domain. Currently, Large Language Models (LLMs) have great potential to\nprocess both structured and unstructured data, enabling them to extract valu-\nable insights for clinical decision-making and research [17]. LLMs can extract\n"}, {"page": 4, "text": "4\nVK. Kembu et al.\ndata from clinical notes with high accuracy, outperforming traditional pattern-\nmatching methods. They are particularly effective in retrieving data, such as lab\nresults and vital signs, which are crucial for clinical analysis. [11].\n3\nMethodology\nThe methodology comprises of EHR data gathering pipeline, first we applied\nregular expressions to automatically annotate the texts, establishing a measure\nfor comparison. Then we leveraged a LLMs to extract comorbidities from the\nsame set of EHRs, comparing the LLMs outputs to the Regexp annotations.\nTo manage inaccuracies in the regexp annotations, we chose 100 regexp-false\nclassified EHRs and proceeded with manual annotation by clinicians, creating\na ground truth. Finally, further comparison of the LLMs performance against\nthese manually annotated labels was carried out. Figure 1 presents the layout of\nthe methodological approach.\n3.1\nData Pipeline & description\nThe protocol required collection of different data from the patient EHR from\ndifferent hospital areas. For this, a large ETL using Oracle and Python has been\nimplemented for the collection of the cardiological risk factors, presence of pre-\nvious cardiac interventions, treatment received during the hospitalization, labs,\ndischarge diagnosis, at-home treatment, and different dataframes have been cre-\nated: Anamnesis, Interventions, Labs, Procedures, Procedures ICD9, Therapies.\nWe will focus solely on Anamnesis data which has been obtained by performing\na large ETL process because it represents unstructured text with the comorbid-\nity information of interest. The dataframe consists of three columns, which are:\nNosologico (hospital admission ID), Ente (hospital identifier), Anamnesis (EHR\nfree text). With a total of 8223 patient records.\nTo emphasize the IR part, we focused on the following comorbidities which\nare clinically relevant for patient evaluation in the cardiac domain: Fibrillazione\natriale (Atrial fibrillation), Insufficienza Renale (Kidney failure), BPCO - Bron-\ncopneumopatia cronica ostruttiva (COPD-Chronic obstructive pulmonary dis-\nease), Diabete mellito (Diabetes mellitus) and Ipertensione arteriosa (Hyper-\ntension) will be the key focus of our research. Since the EHR is in the Italian\nlanguage, all the keys shall be used as per it.\n3.2\nAutomated Data Annotation using Regular expression\nBuilding the regexp for the key comorbidities was done in collaboration with the\nhelp of clinicians, since the electronic health records are created and updated by\nthem for each patient. So, to create the best regexp classifier, domain experts\nhave been involved in developing the patterns. An example of why we need do-\nmain experts for this regexp creation is as follows, Different clinical mentions for\n"}, {"page": 5, "text": "Are LLMs Truly Multilingual?\n5\nComorbidities\nClassification\nAutomated Data\nAnnotation\nManual Data Annotation\nEvaluation & Analysis \nLLMs \nvs\nManual Data Annotation\nLLMs \nvs\nAutomated Data\nAnnotation (Regexp)\nD\nA\nT\nA\nS\nO\nU\nR\nC\nE\nETL\nFig. 1: Methodology - Data gathering pipeline, data annotation and comparison:\nfrom regexp based automatic classification and clinicians validated ground truth\nto LLM extraction.\nthe same term: Term: “Diabetes” - Diabetes mellitus - refers to the general dis-\nease entity, DM - Abbreviation for Diabetes Mellitus, often used in medical notes\nand prescriptions, Type 1 diabetes mellitus, Type 2 diabetes mellitus, Insulin-\ndependent diabetes mellitus (IDDM), Non-insulin-dependent diabetes mellitus\n(NIDDM), Diabetes mellitus with nephropathy, Diabetes mellitus with retinopa-\nthy, Diabetes mellitus with neuropathy, Gestational diabetes mellitus (GDM). A\ndomain expert is essential for identifying key comorbidities and creating accurate\nregex patterns. These patterns are then applied to EHRs to classify comorbidi-\nties, producing baseline data for comparison with LLM results.\n3.3\nManual Data Annotation\nTo be more precise on the created pattern for data classification using Auto-\nmated Data Annotation using Regular expression with the help of clinicians,\n100 “false\" classified records (i.e., regexp classified as negative(0) for the pres-\nence of the comorbidities) of the patient on all five key comorbidities have been\nmanually annotated with help of clinicians (Doctors). Although EHRs aim to im-\nprove patient care, they also pose several challenges. One major concern is data\nquality, as EHRs are only as good as the data entered by clinicians, which can\nbe prone to errors and inaccuracies. EHRs can lack standardization, resulting\nin varying formats and terminology across different systems and organizations,\nmaking it challenging for healthcare providers to share information and coordi-\nnate care. Considering this, the regexp created with the help of clinicians infor-\nmation might not be able to capture all the patterns of the comorbidities. So,\nmanual annotation was carried out to double-check the “false\" classified records\nof the patient. In the process of manual annotations, two clinicians annotated\nall five key comorbidities for each selected EHR. They annotated all five key\ncomorbidities discussing case by case to reach a agreement of the class.\n"}, {"page": 6, "text": "6\nVK. Kembu et al.\n4\nExperimental Setup\nThere are various factors to consider when designing the research setup. For our\nexperiment, the primary criteria we have considered are discussed below.\nPrivacy concerns & Licensing. Data privacy is a concern with Large\nLanguage Models (LLMs), and organizations must address these issues when\ndeploying them. Humanitas AI Center and Humanitas Research Hospital priori-\ntize data privacy, healthcare patient data should be securely managed and used\nfor research only on-premises. Recalling the difference between “Closed-source”\nand “Open-Source” models, only “Open-Source” ones have been selected as they\nallow for deployment in the “on-premises” environment.\nLanguage Support. Language support in large language models (LLMs)\nrefers to the number of languages a model can understand and generate text,\nas well as its proficiency in those languages. The models capabilities can vary\nsignificantly depending on their training data, architecture and intended use\ncases. They can be broadly classified as “Multilingual Models” and “Language-\nSpecific Models”. Multilingual models are explicitly designed to handle multiple\nlanguages and often support dozens or even more languages. In our case, patients\nEHR is in the Italian language, thus only \"Multilingual Models\" have been\nchosen and the experiments have been carried out.\nSize and Resource Requirements. The size and resource requirements of\nLLMs significantly influence their accessibility and usability. Large models typi-\ncally yield impressive capabilities but come with high demands for computational\npower, memory and storage, making them more costly and complex to operate.\nPrior understanding of these requirements was crucial for us to adopt LLMs\nfor the proposed information classification task. High-Performance Computing\n(HPC) with Multiple GPUs was used to carry out the proposed research.\nSelected LLMs. Considering the points stated before, chosen models show\ngreat level of accuracy in the leaderboard [9], especially in consideration with the\nItalian language score. From OpenLLaMA family 3B & 7B models, from Mistral\nfamily 7B & 8x7B models and from Qwen2.5 family 3B & 7B models have been\nselected for this study.\n5\nResult Analysis\nThis section examines and compares the performance of cutting-edge large lan-\nguage models (LLMs) towards regular expression and human annotation in the\nIR task discussed in section 3.\n5.1\nPerformance Comparison: LLMs vs Regexp\nTo evaluate the performance of various large language models (LLMs), it is\nessential to establish a reference dataset for comparison. With the availability\nof annotated data through automated methods, this dataset can serve as the\n"}, {"page": 7, "text": "Are LLMs Truly Multilingual?\n7\n0\n1\nBinary Value\n0\n2000\n4000\n6000\nCount\n7367\n856\nregexp\n(a)\n0\n1\nBinary Value\n0\n2000\n4000\n6000\nCount\n7126\n1097\nregexp\n(b)\n0\n1\nBinary Value\n0\n2000\n4000\n6000\nCount\n7501\n722\nregexp\n(c)\n0\n1\nBinary Value\n0\n1000\n2000\n3000\n4000\n5000\nCount\n5363\n2860\nregexp\n(d)\n0\n1\nBinary Value\n0\n1000\n2000\n3000\n4000\n5000\n6000\nCount\n2225\n5998\nregexp\n(e)\nFig. 2: Classification using regular expressions for the chosen comorbidities\n- a)Fibrillazione atriale, b)Insufficienza Renale, c)BPCO-Broncopneumopatia\ncronica ostruttiva, d)Diabete mellito and e)Ipertensione arteriosa.\nreference. Consequently, the results generated by different LLMs can be com-\npared against these actual values. The automated annotation is carried out by\napplying a series of different combinations of match patterns for the discussed\ncomorbidities, utilizing regular expressions. From the Figure 2 we could see the\nclassification, in which 0 class represents that the comorbidities is not found\nin the EHRs and 1 class represents the availability of the comorbidities in the\nEHRs. Diabete mellito and ipertensione arteriosa are most positive classified\nfield by the regexp, comorbidities like Fibrillazione atriale, Insufficienza Renale\nand BPCO were the most negative classified.\nLLMs. A standard prompt in a zero-shot setting has been used across all the\nLLMs in context with the scenario 1. To avoid multiple classifications in a single\ninference, each comorbidity has been classified in an individual inference for each\nEHR. Assigning one comorbidity per inference per task maximizes the accuracy\nof large language models (LLMs) and leads to more precise predictions. This\nwill avoid the misinterpretation of data, especially medical information, which\nis prone to bias when several tasks are handled by one inference. In executing\none task at a time, the model is able to give complete attention to that one task,\nthereby optimizing the performance. In the following, the classification results of\ndifferent LLM family models against the regular expressions have been discussed.\nOpenLLaMA model family generally show a increase in the classification\naccuracy w.r.t regular expression results of the comorbidities as the model size\nincreases, which can been seen from the Figure 3. In particular with OpenLLaMA\n3B, the model tends to perform well in terms of comorbidities like Diabete mellito\nand Ipertensione arteriosa with an accuracy of 34.78 % and 72.86 % compared\nto the other comorbidities which have an classification accuracy of less than\n15 %. In contrast OpenLLaMA 7B has an accuracy of 50 % and above across all\nthe comorbidities, Insufficienza Renale and BPCO have been among the top in\nclassification with 83.73 % and 81.95 %. Mistral model family generally show\na decrease in the classification accuracy w.r.t regular expression results of the\ncomorbidities as the model size increases, which is shown in Figure 3. Mistral\n7B shows a accuracy of 75 % above across all the comorbidities except Iperten-\nsione arteriosa which is at 57.33 %, Fibrillazione atriale and BPCO have shown\na accucary of 90 % above which is the highest among all the models. Mixtral\n8x7B shows a varied performance, comorbidities like BPCO, Diabete mellito\n"}, {"page": 8, "text": "8\nVK. Kembu et al.\nand Ipertensione arteriosa classification metrics are more equivalent to the per-\nformance OpenLLaMA 3B. Fibrillazione atriale and Insufficienza Renale show\na slight increase in accuracy when compared to OpenLLaMA 3B. In contrast\nQwen2.5 model family does not show much of a increase or decrease with in-\ncrease in the model parameters, which is shown in the Figure 3. Qwen2.5 7B\nwhich shows a very small marginal difference w.r.t to the 3B model, especially\nonly for comorbidities like Fibrillazione atriale, Insufficienza Renale and BPCO.\nThis shows that the domain-specific features in the dataset are being handled\nsimilarly by both models and the differences in model size is not substantial\nenough to affect their performance.\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n10\n20\n30\n40\n50\n60\n70\nAccuracy (%)\n10.43%\n13.34%\n8.78%\n34.78%\n72.86%\nAccuracy Comparison for each comorbidities\n(a)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n76.70%\n83.73%\n81.95%\n62.63%\n55.84%\nAccuracy Comparison for each comorbidities\n(b)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n95.60%\n88.12%\n92.51%\n79.57%\n57.53%\nAccuracy Comparison for each comorbidities\n(c)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n10\n20\n30\n40\n50\n60\n70\nAccuracy (%)\n33.97%\n15.25%\n8.78%\n34.78%\n72.89%\nAccuracy Comparison for each comorbidities\n(d)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\nAccuracy (%)\n89.55%\n86.65%\n91.21%\n65.22%\n27.06%\nAccuracy Comparison for each comorbidities\n(e)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\nAccuracy (%)\n89.52%\n86.66%\n91.22%\n65.22%\n27.06%\nAccuracy Comparison for each comorbidities\n(f)\nFig. 3: LLMs accuracy compared to regular expression -a)OpenLLaMA 3B,\nb)OpenLLaMA 7B, c)Mistral 7B, d)Mixtral 8x7B, e)Qwen2.5 3B and e)Qwen2.5\n7B\nFrom the Figure 4, we could compare the overall accuracy of the selected\nmodels to the a) automated and b) manual annotated data as the reference\nvalues. It shows the mean accuracy of the models across all the defined comor-\nbidities. From a) of the Figure 4 compared to the automated annotation, we\ncould see that OpenLlama 3B and Mixtral 8x7B overall accuracies were lower\nthan 35 %. In contrast, OpenLlama 7B, Mistral 7B, Qwen2.5 3B and 7B both\nhad an overall accuracy of more than 70 %, which is nearly two times the perfor-\nmance of the other two models. Mistral 7B performed very well when compared\nto the other models, with an overall accuracy of 82.67 % when compared to\nregular expression (automated annotation).\nWhile the classification accuracy provides a general insight into the LLMs\nperformance, it is important to examine the classification reports for a deeper\n"}, {"page": 9, "text": "Are LLMs Truly Multilingual?\n9\nunderstanding. Metrics such as F1-score, precision and recall help to assess how\nwell the model performs across different comorbidities especially in the presence\nof data imbalance, and highlight potential areas where it may be struggling. This\nis needed for evaluating the LLMs true effectiveness.\nLLMs and Metrics\nAutomated (8223 data records)\nFibrillazione Atriale Insufficienza Renale BPCO Diabete Mellito Ipertensione Arteriosa\nOpenLLaMA 3B\nPrecision\n0.1\n0.13\n0.09\n0.35\n0.73\nRecall\n1\n1\n1\n1\n1\nF1-score\n0.19\n0.24\n0.16\n0.52\n0.84\nOpenLLaMA 7B\nPrecision\n0.23\n0.19\n0.22\n0.46\n0.8\nRecall\n0.52\n0.07\n0.42\n0.47\n0.53\nF1-score\n0.31\n0.1\n0.29\n0.47\n0.64\nMistral 7B\nPrecision\n0.86\n0.96\n1\n0.99\n0.99\nRecall\n0.69\n0.11\n0.15\n0.42\n0.42\nF1-score\n0.77\n0.2\n0.26\n0.59\n0.59\nMixtral 8x7B\nPrecision\n0.10\n0.13\n0.09\n0.35\n0.73\nRecall\n0.69\n0.98\n1\n1\n1\nF1-score\n0.18\n0.24\n0.16\n0.52\n0.84\nQwen2.5 3B\nPrecision\n0\n0\n0\n0\n0\nRecall\n0\n0\n0\n0\n0\nF1-score\n0\n0\n0\n0\n0\nQwen2.5 7B\nPrecision\n0.25\n0\n0\n0\n0\nRecall\n0\n0\n0\n0\n0\nF1-score\n0.1\n0\n0\n0\n0\nTable 1: Precision, Recall and F1 Score for LLMs vs. Regular Expressions in\nIdentifying Comorbidities (Class 1).\nFrom the Table 1 we could compare comorbidity-wise LLMs performance.\nOpenLLama 3B has perfect recall (1.0) but struggles with precision, leading to\nmany false positives. OpenLLaMA 7B model shows low precision in most of the\nconditions i.e, ranging from 0.19 for Insufficienza Renale to 0.8 for Ipertensione\nArteriosa. OpenLLama 3B with high recall, low precision and low F1, the model\nstruggles to generalization across comorbidities. OpenLLama 7B shows better\ngeneralization than 3B, but still has low scores across various comorbidities.\nMistral 7B demonstrates outstanding precision across all comorbidities, which\nshows it is very effective at avoiding false positives. But recall is low compared\nto high precision, indicating the model often misses many cases of the comor-\nbidities. In terms of F1-score, the model shows better balanced results across\nall comorbidities, compared to other LLMs. Mixtral 8x7B model exhibits low\nprecision and high recall across all comorbidities. Due to this, the F1-scores are\nlow for all except for Ipertensione Arteriosa is 0.84, showing the model achieves\na better balance between precision and recall for this comorbidity. Mistral 7B\nshows better generalization, but still has low scores across a few comorbidities.\nMixtral 8x7B has low scores in most cases and struggles to generalize across co-\nmorbidities, even though it is the most advanced among all the chosen models.\nBoth Qwen2.5 3B and Qwen2.5 7B models, despite their parameter differ-\nences, seem to produce classes that do not match the true positive class at all\nin most of the cases. The precision is 0 in most of the cases because there are\nno positive predictions. Similarly, recall is also 0 because the model does not\ncorrectly identify any of the actual positive samples in the data. In context with\n"}, {"page": 10, "text": "10\nVK. Kembu et al.\nthese, F1-score is also 0 for most of the comorbidities. Qwen2.5 7B exhibits a\nslight difference for Fibrillazione Atriale with precision(0.25) and F1-score(0.1).\nOverall, Qwen2.5 family could not produce true positive class when automated\ndata was compared.\nLLMs in a zero-shot setting struggle to extract comorbidities from EHRs\nand do not match the accuracy of regular expression-based extractions,\nmaking them unsuitable as a substitute. These results are linked to re-\nsearch questions Q1) and Q2).\nOpenLlama 3B OpenLlama 7B\nMistral 7B\nMixtral 8x7B\nQwen2.5 3B\nQwen2.5 7B\n30\n40\n50\n60\n70\n80\nAccuracy (%)\n28.04%\n72.17%\n82.67%\n33.13%\n71.94%\n71.93%\nOverall Accuracy Across Different Models\noverall accuracy\n(a)\nOpenLlama 3B OpenLlama 7B\nMistral 7B\nMixtral 8x7B\nQwen2.5 3B\nQwen2.5 7B\n20\n40\n60\n80\nAccuracy (%)\n7.8%\n81.4%\n92.6%\n80.6%\n92.2%\n92.0%\nOverall Accuracy Across Different Models\noverall accuracy\n(b)\nFig. 4: Overall accuracy across different models compared to a)regular expression\nannotation and b) manual annotation.\n5.2\nPerformance Comparison: LLMs vs Humans\nAs discussed in section 3 100 false classified comorbidities has been manually\nannotated by clinical experts. This is because positive classifications contain\ncomorbidity information in the EHRs. False negatives in regexp are critical in\nhealthcare, as missed diagnoses can delay treatment, progression of disease and\nlead to poor patient outcomes. By annotating these false negatives, we can ensure\nthat critical misclassifications are corrected, improving the ability to identify\nimportant conditions.\nDuring this process, clinicians were not informed about the nature of the\ndataset provided i.e., with only the false class from the regex annotations pro-\nvided. This was done to avoid bias or prejudgment from influencing the anno-\ntation. The guidance provided to the annotators is same as before, 0 class to\nrepresent the comorbidities is not found and 1 class represent the availability\nthe comorbidities in the EHRs. From the Figure 5 we could see that Iperten-\nsione arteriosa is the one with more false negatives, followed by the Fibrillazione\natriale. Both of these comorbidities have 10 % or more false classification by the\nregular expression. In the other hand Insufficienza Renale, BPCO and Diabete\n"}, {"page": 11, "text": "Are LLMs Truly Multilingual?\n11\n0\n1\nBinary Value\n0\n20\n40\n60\n80\nCount\n90\n10\nManual\n(a)\n0\n1\nBinary Value\n0\n20\n40\n60\n80\n100\nCount\n96\n4\nManual\n(b)\n0\n1\nBinary Value\n0\n20\n40\n60\n80\n100\nCount\n99\n1\nManual\n(c)\n0\n1\nBinary Value\n0\n20\n40\n60\n80\n100\nCount\n96\n4\nManual\n(d)\n0\n1\nBinary Value\n0\n20\n40\n60\n80\nCount\n80\n20\nManual\n(e)\nFig. 5:\nManual\nannotation\nclassification\nof\nthe\nchosen\ncomorbidities\n-\na)Fibrillazione atriale, b)Insufficienza Renale, c)BPCO-Broncopneumopatia\ncronica ostruttiva, d)Diabete mellito and e)Ipertensione arteriosa.\nmellito have 4 % or less false negative compared to the counterpart. In addition\nto this we can derive the performance of the regular expression created for each\ncomorbidities when it is compared to the manual annotation, in particular pat-\ntern created for BPCO has the higher accuracy of 99 % and 80 % for Ipertensione\narteriosa being the least accuracy. Overall comorbidities classification accuracy\nusing regular expression when compared to the manual annotation was 92.2 %.\nLLMs. With this new set of data, a zero-shot setting with a standard prompt\nis used across all the selected LLMs. As per previous experiment, to prevent\nmultiple classifications in a single inference, each comorbidity is classified indi-\nvidually for each EHR. In the following we discusses the classification results of\nvarious LLM families against manual annotations.\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nAccuracy (%)\n10.00%\n4.00%\n1.00%\n4.00%\n20.00%\nAccuracy Comparison for each comorbidities\n(a)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\nAccuracy (%)\n74.00%\n94.00%\n85.00%\n81.00%\n73.00%\nAccuracy Comparison for each comorbidities\n(b)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n90.00%\n96.00%\n99.00%\n96.00%\n82.00%\nAccuracy Comparison for each comorbidities\n(c)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\nAccuracy (%)\n86.00%\n87.00%\n94.00%\n63.00%\n73.00%\nAccuracy Comparison for each comorbidities\n(d)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n90.00%\n96.00%\n99.00%\n96.00%\n80.00%\nAccuracy Comparison for each comorbidities\n(e)\nfibrillazione atriale\ninsufficienza renale\nbpco\ndiabete mellito\nipertensione arteriosa\nComorbidities\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n89.00%\n96.00%\n99.00%\n96.00%\n80.00%\nAccuracy Comparison for each comorbidities\n(f)\nFig. 6: LLMs accuracy compared to manual annotation. (a)OpenLLaMA 3B,\n(b)OpenLLaMA 7B, (c)Mistral 7B, (d)Mixtral 8x7B, (e)Qwen2.5 3B and\n(f)Qwen2.5 7B\n"}, {"page": 12, "text": "12\nVK. Kembu et al.\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n0\n90\n0\n10\nConfusion Matrix of fibrillazione atriale\n(a)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n0\n96\n0\n4\nConfusion Matrix of insufficienza renale\n(b)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n0\n99\n0\n1\nConfusion Matrix of bpco\n(c)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n0\n96\n0\n4\nConfusion Matrix of diabete mellito\n(d)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n0\n80\n0\n20\nConfusion Matrix of ipertensione arteriosa\n(e)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n90\n0\n10\n0\nConfusion Matrix of fibrillazione atriale\n(f)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n96\n0\n4\n0\nConfusion Matrix of insufficienza renale\n(g)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n99\n0\n1\n0\nConfusion Matrix of bpco\n(h)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n96\n0\n4\n0\nConfusion Matrix of diabete mellito\n(i)\nPredicted 0\nPredicted 1\nPredicted\nActual 0\nActual 1\nActual\n80\n0\n18\n2\nConfusion Matrix of ipertensione arteriosa\n(j)\nFig. 7: Confusion matrix for LLMs when compared to manual annotation. The\nfirst row represents OpenLLaMA 3B and the second, Mistral 7B. Comor-\nbidities are (a,f) Fibrillazione atriale, (b,g) Insufficienza Renale, (c,h) BPCO-\nBroncopneumopatia cronica ostruttiva, (d,i) Diabete mellito and (e,j) Iperten-\nsione arteriosa.\nOpenLLaMA family of models shows nearly the same kind of behavior as\nseen with the regular expression, increase in the model accuracy w.r.t the model\nsize which can be seen in the Figure 6. OpenLLaMA 3B has varying range\nof classification accuracy when compared to the manual annotation set, start-\ning from BPCO as low as 1 % to 20 % for Ipertensione arteriosa. In contrast\nOpenLLaMA 7B shows a extreme level of accuracy increase compared to the\n3B version. In particular the model shows classification accuracy of greater than\n70 % across all the comorbidities, notably 94 % accuracy for Insufficienza Renale\nand 85 % for BPCO. These results show that the model are mostly in alignment\nwith the human annotation. Mistral family shows increased level of accuracy\nw.r.t to the previous regular expression case, the model Mixtral 8x7B showed a\ndrastic performance increase in manual annotation. Mistral 7B shows 90 % and\nabove classification accuracy in most of the comorbidities except for Iperten-\nsione arteriosa which is 82 % and reaching 99 % for BPCO. Mistral 8x7B has a\n94 % for BPCO and 63 % for Diabete mellito being the least, the model shows\na overall performance increase across all the comorbidities when compared to\nthe manual annotation. Qwen2.5 family shows increased level of accuracy w.r.t\nto the regular expression case, notably a huge increase 52.94% for Ipertensione\narteriosa and 30.78% for Diabete mellito. Qwen2.5 3B and 7B differ slightly in\nthe Fibrillazione atriale and other comorbidities classification accuracy being\nidentical.\nFrom b) of the Figure 4 compared to the manual annotation, we could see that\nOpenLlama 3B shows a very low overall performance among all the models with\na overall accuracy of less than 10 % when compared to the manual annotation.\nOpenLlama 7B, Mistral 7B, Mixtral 8x7B, Qwen2.5 3B and 7B showed a overall\naccuracy of more than 80 %. With mistral 7B and openllama 7B we see an\n"}, {"page": 13, "text": "Are LLMs Truly Multilingual?\n13\naccuracy increase of about ≈10 % when compared to the automated annotation,\nQwen2.5 3B and 7B we see an accuracy increase of about ≈20 % when compared\nto the automated annotation and Mixtral 8x7B we see a whooping 47.48 % of\naccuracy increase in the manual annotation when compared to the automated\nannotation. Regular expression had a accuracy of 92 % when compared to manual\nannotation, llama and mixtral 8x7B are below that and the others three reports a\nsimilar performance that of regexp. In consideration of the huge unbalanced class\ndistribution post the manual annotation, we use confusion matrix to compare\nthe results with the LLMs instead of classification report. From b) of the Figure 4\nwe take the best and least accuracy model and matrix is created and shown in\nFigure 7. This is done to evaluate whether the model is generalizing effectively\nor simply making random predictions. Openllama 3B being we could clearly\nsee a pattern that the model always classifies the comorbidities as positives on\ngathered subset of data for manual annotation. Mistral 7B shows a pattern, by\nclassifying most of comorbidities as negatives on the gathered subset. This shows\nhow the LLMs classify the selected comorbidities from the EHRs, showcasing a\nNO better performance compared to the regular expression approach.\nEven though we see varied performance of LLMs across the comorbidi-\nties in this setting, we could conclude that Mistral 7B seems to generalize\nconsiderably well across three out of five comorbidities compared to the\nother LLMs when regular expression is considered. However, when direct\ncomparison with manual annotation is considered the model always pre-\ndicts class 0 (the majority class), thus not showing real understanding on\nthe semantics of the comorbidities (Q3).\n6\nDiscussion\nThe results of the scenario 1 considered in our study, and the extended technical\nevaluation of the same, provide valuable insights into the usage of LLMs in zero-\nshot, on-premises settings for comorbidities extraction from Italian EHRs. The\ninitial accuracy results of the LLMs when compared to the regular expression\nor the manual annotation seem to be good, but when a classification report\nand confusion matrix are constructed and analyzed, they show how these LLMs\nstruggle to generalize across all the chosen comorbidities.\nThe results from our evaluation demonstrate that Mistral 7B can achieve\nperformance in extracting a few comorbidities from EHRs among the selected\nLLMs when regular expression is considered for comparison, but cannot gen-\neralize across all the chosen comorbidities in accordance with manual annota-\ntion. Adding to this, neither of the chosen LLMs could reach the level of the\nregular expression-based pattern matching approach. It is evident that the se-\nlected multilingual LLMs face serious challenges and trustworthiness related is-\nsues when used in a zero-shot setting with no technical expertise in high-risk\ndomains as Healthcare. In this state of usage, considering LLMs in place of a\n"}, {"page": 14, "text": "14\nVK. Kembu et al.\npattern-matching approach in extraction pipelines would be problematic, since\nit would create uncertainty in the extracted information.\nIn context with the proposed scenario, when a clinician without much techni-\ncal knowledge of prompt tuning techniques adheres to a simple and direct LLM\nprompt (zero-shot), aiming to extract comorbidities from EHRs, the overall re-\nsults show that this approach would not be advisable in this setting. Although\ndifferent prompt engineering approaches [22] could be employed to increase the\naccuracy of LLMs usage and making them more adaptive to the task intended,\nwe on purpose did not attempt to optimize or engineer the use of LLMs, thus\nreplicating a simulated environment, where a clinician or doctor would be ex-\npected to use these models directly.\nOverall, we want to highlight that the increasing use of LLMs without tech-\nnical understanding in high-risk domains like healthcare, law, finance, and au-\ntonomous systems etc., brings concern. In these high-stakes environments, mis-\ninformation, biased decision-making, lack of accountability and security vulner-\nabilities are significant risks when deploying LLMs in critical fields, as mistakes\ncould lead to dangerous outcomes. These analyses showed that LLMs must be\nproperly tested before deployment into production, global metrics should be\naccurately selected to avoid misleading conclusions and continuous close moni-\ntoring should be done for hallucinations and other deviations.\n7\nConclusion\nIn this paper, we explored the potential of six multilingual general-purpose open-\nsource large language models (LLMs) in understanding and extracting valuable\ninformation from Electronic Health Records (EHRs) in Italian. Our study, fo-\ncused on the real-time retrieval of comorbidities from EHRs, shows important\ninsights into the LLMs ability to handle non-English language understanding\nand processing. Our findings show how LLMs struggle in extracting comorbidi-\nties and their difficulty in understanding Italian language EHR when used in a\nzero-shot approach. LLMs in the zero-shot setting cannot be used as a substitute\nfor traditional pattern matching in the extraction pipelines. In the future, we will\nexplore In-Context learning (ICL) approaches and will also consider fine-tuning\nan LLM model for IR from EHRs to improve its language processing capabilities\nand enhance trustworthiness in handling healthcare-related tasks.\nReferences\n1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774 (2023)\n2. AI, M.: Llama 3.2: Connect 2024 vision for edge and mobile devices. https:\n//ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\n(2024)\n3. Anthropic: Introducing claude 4, https://www.anthropic.com/news/claude-4\n"}, {"page": 15, "text": "Are LLMs Truly Multilingual?\n15\n4. Anthropic: Introducing the next generation of claude, https://www.anthropic.\ncom/news/claude-3-family\n5. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win-\nter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,\nJ., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language\nmodels are few-shot learners (2020), https://arxiv.org/abs/2005.14165\n6. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi,\nX., Wang, C., Wang, Y., Ye, W., Zhang, Y., Chang, Y., Yu, P.S., Yang, Q.,\nXie, X.: A survey on evaluation of large language models. ACM Trans. Intell.\nSyst. Technol. 15(3) (Mar 2024). https://doi.org/10.1145/3641289, https:\n//doi.org/10.1145/3641289\n7. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon,\nI., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al.: Gemini 2.5: Pushing the\nfrontier with advanced reasoning, multimodality, long context, and next generation\nagentic capabilities. arXiv preprint arXiv:2507.06261 (2025)\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Burstein, J., Doran,\nC., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers). pp. 4171–4186. Associ-\nation for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https:\n//doi.org/10.18653/v1/N19-1423, https://aclanthology.org/N19-1423/\n9. EuroLingua-GPT: Internal european leaderboard - a hugging face space by eurolin-\ngua, https://huggingface.co/spaces/Eurolingua/european-llm-leaderboard\n10. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Let-\nman, A., Mathur, A., Schelten, A., Vaughan, A., et al.: The llama 3 herd of models\n(2024)\n11. Gu, B., Shao, V., Liao, Z., Carducci, V., Romero-Brufau, S., Yang, J., Desai, R.:\nScalable information extraction from free text electronic health records using large\nlanguage models. medrxiv (2024)\n12. Häyrinen, K., Saranto, K., Nykänen, P.: Definition, structure, content, use and im-\npacts of electronic health records: a review of the research literature. International\njournal of medical informatics 77(5), 291–304 (2008)\n13. Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., de las Casas,\nD., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L.R., Lachaux,\nM.A., Stock, P., Scao, T.L., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mistral\n7b (2023), https://arxiv.org/abs/2310.06825\n14. Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C.,\nChaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G.,\nLample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian,\nS., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix,\nT., Sayed, W.E.: Mixtral of experts (2024), https://arxiv.org/abs/2401.04088\n15. Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray,\nS., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language models\n(2020), https://arxiv.org/abs/2001.08361\n16. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,\nStoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension (2019), https:\n//arxiv.org/abs/1910.13461\n"}, {"page": 16, "text": "16\nVK. Kembu et al.\n17. Li, L., Zhou, J., Gao, Z., Hua, W., Fan, L., Yu, H., Hagen, L., Zhang, Y., Assimes,\nT.L., Hemphill, L., Ma, S.: A scoping review of using large language models (llms)\nto investigate electronic health records (ehrs) (2024), https://arxiv.org/abs/\n2405.03066\n18. Nadeau, D., Sekine, S.: A survey of named entity recognition and classification.\nLingvisticae Investigationes 30, 3–26 (2007), https://api.semanticscholar.org/\nCorpusID:8310135\n19. Qin, L., Chen, Q., Zhou, Y., Chen, Z., Li, Y., Liao, L., Li, M., Che, W., Yu, P.S.:\nA survey of multilingual large language models. Patterns 6(1), 101118 (2025).\nhttps://doi.org/https://doi.org/10.1016/j.patter.2024.101118, https://\nwww.sciencedirect.com/science/article/pii/S2666389924002903\n20. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D.,\nHuang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou,\nJ., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang,\nP., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y.,\nSu, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., Qiu, Z.: Qwen2.5 technical\nreport (2025), https://arxiv.org/abs/2412.15115\n21. Radford, A., Narasimhan, K., Salimans, T., et al.: Improving language understand-\ning by generative pretraining. OpenAI (2019), https://openai.com/research/\nlanguage-unsupervised\n22. Sahoo, P., Singh, A.K., Saha, S., Jain, V., Mondal, S., Chadha, A.: A systematic\nsurvey of prompt engineering in large language models: Techniques and applica-\ntions (2025), https://arxiv.org/abs/2402.07927\n23. Team, G., Anil, R., Borgeaud, S., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk,\nJ., Dai, A.M., Hauth, A., Millican, K., et al.: Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805 (2023)\n24. Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting,\nD.S.W.: Large language models in medicine. Nature medicine 29(8), 1930–1940\n(2023)\n25. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need (2023), https://arxiv.org/abs/1706.\n03762\n26. Workshop, B., Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D.,\nCastagné, R., Luccioni, A.S., Yvon, F., et al.: Bloom: A 176b-parameter open-\naccess multilingual language model. arXiv preprint arXiv:2211.05100 (2022)\n27. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang,\nC., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin,\nH., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin,\nJ., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang,\nP., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin,\nW., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y.,\nWan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., Qiu, Z.: Qwen3 technical\nreport (2025), https://arxiv.org/abs/2505.09388\n28. Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,\nZhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li,\nY., Tang, X., Liu, Z., Liu, P., Nie, J.Y., Wen, J.R.: A survey of large language\nmodels (2025), https://arxiv.org/abs/2303.18223\n29. Zhu, S., Supryadi, Xu, S., Sun, H., Pan, L., Cui, M., Du, J., Jin, R., Branco,\nA., Xiong, D.: Multilingual large language models: A systematic survey (2024),\nhttps://arxiv.org/abs/2411.11072\n"}]}