{"doc_id": "arxiv:2601.11567", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.11567.pdf", "meta": {"doc_id": "arxiv:2601.11567", "source": "arxiv", "arxiv_id": "2601.11567", "title": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology", "authors": ["Vanessa D'Amario", "Randy Daniel", "Alessandro Zanetti", "Dhruv Edamadaka", "Nitya Alaparthy", "Joshua Tarkoff"], "published": "2025-12-26T14:30:53Z", "updated": "2025-12-26T14:30:53Z", "summary": "Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.11567v1", "url_pdf": "https://arxiv.org/pdf/2601.11567.pdf", "meta_path": "data/raw/arxiv/meta/2601.11567.json", "sha256": "d9eeec403a89fc0aff7a8d150621217996187535bf6a8a3d005d2b2d64c1f35e", "status": "ok", "fetched_at": "2026-02-18T02:23:46.778579+00:00"}, "pages": [{"page": 1, "text": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large\nLanguage Models for Pediatric Endocrinology\nVanessa D’Amario1, Randy Daniel1, Alessandro Zanetti1, Dhruv Edamadaka1, Nitya Alaparthy1,\nJoshua Tarkoff2\n1Nova Southeastern University\n3300 S University Dr, Fort Lauderdale, FL 33328 USA\n2Nicklaus Children’s Hospital\n3100 SW 62nd Ave, Miami, FL 33155 USA\nvdamario@nova.edu\nAbstract\nSmall open-source medical large language models (LLMs)\noffer promising opportunities for low-resource deployment\nand broader accessibility. However, their evaluation is of-\nten limited to accuracy on medical multiple choice question\n(MCQ) benchmarks, and lacks evaluation of consistency, ro-\nbustness, or reasoning behavior. We use MCQ coupled to\nhuman evaluation and clinical review to assess six small\nopen-source medical LLMs (HuatuoGPT-o1 (Chen et al.\n2024), Diabetica-7B (Wei et al. 2024), Diabetica-o1 (Wei\net al. 2024), Meditron3-8B (Sallinen et al. 2025), MedFound-\n7B (Liu et al. 2025), and ClinicaGPT-base-zh (Wang et al.\n2023)) in pediatric endocrinology. In deterministic settings,\nwe examine the effect of prompt variation on models’ out-\nput and self-assessment bias. In stochastic settings, we evalu-\nate output variability and investigate the relationship between\nconsistency and correctness. HuatuoGPT-o1-8B achieved the\nhighest performance. The results show that high consistency\nacross the model response is not an indicator of correctness,\nalthough HuatuoGPT-o1-8B showed the highest consistency\nrate. When tasked with selecting correct reasoning, both\nHuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment\nbias and dependency on the order of the candidate explana-\ntions. Expert review of incorrect reasoning rationales identi-\nfied a mix of clinically acceptable responses and clinical over-\nsight. We further show that system-level perturbations, such\nas differences in CUDA builds, can yield statistically signifi-\ncant shifts in model output despite stable accuracy. This work\ndemonstrates that small, semantically negligible prompt per-\nturbations lead to divergent outputs, raising concerns about\nreproducibility of LLM-based evaluations and highlights the\noutput variability under different stochastic regimes, empha-\nsizing the need of a broader diagnostic framework to under-\nstand potential pitfalls in real-world clinical decision support\nscenarios.\nIntroduction\nLarge Language Models (LLMs) hold significant potential\nin healthcare, particularly to improve chronic disease man-\nagement and clinical decision support (Tu et al. 2024; Mc-\nDuff et al. 2025). High-performance medical LLMs have\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nAccepted at the AAAI 47th Workshop on Reproducible Artificial\nIntelligence (RAI2026). Singapore (January 27th, 2026).\nshowed strong results on different aspects of clinical care,\nfrom diagnostics to patient interfacing (Singhal et al. 2025;\nLiu et al. 2025; Mukherjee et al. 2024). However, these mod-\nels are often large, exceeding 70 billions parameters, and\nproprietary, making them inaccessible for independent val-\nidation or adaptation. They become impracticable solutions\nin resource-constrained or low-income healthcare settings.\nIn contrast, small open-source medical LLMs offer the\npotential for lightweight, locally deployable, and transparent\ntools that could reduce barriers to access. Nonetheless, their\nreliability and reproducibility have not been systematically\naddressed. A critical question to address is understanding if\nthese models, as released and without fine-tuning, encode\nsufficient clinical knowledge to support real-world use.\nPediatric endocrinology presents a unique testbed for the\nevaluation of medical LLMs. Pediatric endocrinology is fac-\ning growing patient volume, longer waiting times, and a\nlarger range of conditions in the last years (Kumar et al.\n2021; Aye et al. 2024). At the same time, access to pe-\ndiatric endocrinology expertise is limited in many regions\n(Aye et al. 2024), creating a potential role for AI-driven deci-\nsion support, provided that they can deliver trustworthy out-\nputs.\nIn this study, we explore this question by evaluating\na selection of small (under 10 billion parameters) previ-\nously fine-tuned open-source medical LLMs in pediatric en-\ndocrinology. Given the lack of evaluation of medical LLMs\nin this field, our work fills this gap by systematically assess-\ning model behavior on a curated set of pediatric endocrinol-\nogy cases, offering insights into both the capabilities and\nlimitations of LLM tools in this clinical subspecialty.\nTo evaluate model performance in pediatric endocrinol-\nogy, we leverage Multiple Choice Questions (MCQs) drawn\nfrom real-world clinical content. MCQs are a widely\nused strategy for evaluating medical LLMs, particularly\nsmall, domain-specific models. While convenient and well-\nstructured for automated scoring and scalable benchmark-\ning, MCQ-based evaluation has well-documented limita-\ntions. Question difficulty varies widely, distractors may lack\nclinical plausibility, and accuracy alone often obscures rea-\nsoning quality, robustness, or consistency. Recent studies\noutside of the healthcare context (Li et al. 2024; Khatun\nand Brown 2024) have also highlighted response instability\narXiv:2601.11567v1  [cs.CL]  26 Dec 2025\n"}, {"page": 2, "text": "among LLMs in MCQ settings and self-assessment bias (Xu\net al. 2024; Wataoka, Takahashi, and Ri 2024; Panickssery,\nBowman, and Feng 2024).\nDespite these challenges, MCQs remain one of the most\naccessible formats for systematic model evaluation. In this\nwork, we embrace their practicality while addressing their\nshortcomings: we propose a multidimensional evaluation\nframework that goes beyond standard accuracy-based eval-\nuation and measures how stable model outputs are under\nclinically irrelevant input variations, parameter changes, and\ncomputational variability. Specifically, we evaluate six small\nmedical LLMs and assess consistency, by comparing deci-\nsions across prompt variations, including syntactical varia-\ntions and the removal of answer option labels. We also eval-\nuate response consistency and correctness across stochastic\nsampling, where the degree of stochasticity of the inference\nprocess is determined by different temperature values and\nself-assessment bias, by presenting models with a choice be-\ntween their own rationale and a gold-standard clinical expla-\nnation. We lastly investigate numerical stability, examining\nhow changes in software environments affect model outputs\ndespite identical inputs.\nThis multidimensional analysis provides a structured di-\nagnostic of small medical LLM readiness and exposes fail-\nure modes that accuracy metrics alone would not be able\nto capture. Our goal is not only to compare models but to\ndefine reproducibility-oriented evaluation criteria for trust-\nworthy, accessible LLMs in medicine. In doing so, we high-\nlight which models show the greatest promise for practical\nclinical integration and identify areas requiring further im-\nprovement.\nMaterials and Methods\nMultiple Choice Questions from Pediatric ESAP\n2021-2022\nTo evaluate the reasoning capabilities of small open-source\nmedical LLMs we rely on the Pediatric Endocrine Self-\nAssessment Program 2021-2022 (ESAP) (Pesce and Sisto\n2021). The program is a question bank containing 100\namong clinical cases and factual questions based on clinical\nguidelines. Clinical cases items present a patient scenario,\nincluding relevant medical history, examination findings,\nand laboratory results, and typically ask to determine the\nmost likely diagnosis or the best course of action in patient\nmanagement. Other more factual items focus on guideline-\nbased knowledge such as indications for specific medica-\ntions, appropriate monitoring strategies, or recommended\nreferral to a subspecialist. Each ESAP item includes a de-\nscription of the clinical case, a question, five possible an-\nswers (each comes with a letter from A to E), an educational\nobjective, the correct answer, and the clinical explanation for\neach answer. Drawing on the expertise of leading pediatric\nendocrinologists, the program is highly technical and aimed\nat medical specialists. The program’s learning objectives in-\nclude recognizing clinical features of endocrine, growth, and\nmetabolic disorders; applying current diagnostic and treat-\nment options; identifying risk factors; and evaluating en-\ndocrine manifestations of systemic diseases. As such, we\nconsidered it as an optimal starting point for knowledge as-\nsessment in pediatric endocrinology.\nWe\nexcluded\nnine\nESAP\nitems\nfrom\nthe\nanaly-\nsis, since those rely on images and graphs. For each\nclinical\ncase\ndescribed,\nwe\nextracted\nthe\nfollowing\nfields: <description>, <question>, <options>,\n<correct answer>, and <explanation>.\nSmall Open-Source Medical LLMs\nWe evaluate six small-sized, open-source medical LLMs de-\nsigned for biomedical reasoning.\nHuatuoGPT-o1 (Chen et al. 2024) is based on the LLaMA\n3.1 8B architecture (Dubey et al. 2024) and is trained using\ncontinued pre-training and supervised fine-tuning in a bilin-\ngual corpus of 1.1TB (Chinese and English), with bench-\nmarks including MedQA, MedMCQA, CMB, CMExam,\nand CMMLU.\nDiabetica-7B (Wei et al. 2024) uses the Qwen2-7B-\nInstruct model (Yang et al. 2024), fine-tuned through low-\nrank adaptation and self-distillation on public medical QA\nsets, SFT datasets, and internal diabetes-specific data, eval-\nuated through MCQs, fill-in-the-blank and dialogue tasks.\nDiabetica-o1 is the result of self-distillation of Diabetica-\n7B.\nMeditron3-8B (Sallinen et al. 2025), using LLaMA 3.1-\n8B, underwent a continuous pre-training and supervised in-\nstruction fine-tuning in PubMed Central, validated medical\ntextbooks, and more than 46,000 clinical practice guidelines,\nand was tested on MedQA, MedMCQA, and PubMedQA.\nMedFound-7B (Liu et al. 2025) and ClinicalGPT-based-\nzh (Wang et al. 2023) are both based on the BLOOM-\n7B (Workshop et al. 2022). The latter was developed through\nsupervised fine-tuning and reinforcement learning using\na range of Chinese electronic health records, multi-turn\npatient-doctor dialogues, standardized medical examination\nquestions, and question-answer pairs derived from struc-\ntured medical knowledge graphs.\nExperimental Design\nTo assess the stability and reasoning capabilities of small\nopen-source medical LLMs, we prepared our dataset from\nthe Pediatric ESAP 2021-2022 as a stepping-stone in the de-\nsign of three main experiments. We describe the inferential\nsetup for all the experiments in Appendix Experimental Se-\ntup/Inference and Hyperparameters.\nExperiment 1: LLMs Accuracy and Response Stabil-\nity to Prompt Variation\nWe evaluate the performance\nof the models under deterministic conditions (temperature\nT = 0). For each ESAP item, including the clinical de-\nscription, question and multiple-choice options, each model\nis instructed to select the correct option and provide support-\ning clinical reasoning. Three prompt strategies are tested: (1)\nprompt A (Appendix Prompting/Prompts in Experiment 1),\n(2) prompt B, which introduces minor syntactical variations\nto prompt A and is used to assess prompt sensitivity; and (3)\nprompt A without letter token, which is identical to prompt\nA except that the option letter (A to E) are omitted from the\n<options> field, separating each option with a semicolon.\n"}, {"page": 3, "text": "Beyond accuracy, the main objective of this experiment is\nto assess the stability of model outputs to prompt variations.\nWe apply McNemar’s exact test to compare accuracy across\n(a) syntactic variations (prompt A vs prompt B) and (b) let-\nter omission (prompt A vs prompt A without letters). To\ncapture variability in model behavior independently of cor-\nrectness, we also compute the pairwise response match-rate\nand Cohen’s κ coefficient, each accompanied by 95% confi-\ndence intervals obtained via Wilson and bootstrap methods\n(B = 10000), respectively. A Stuart-Maxwell test is per-\nformed to evaluate whether the distribution of categorical re-\nsponses differs significantly between prompt strategies, pro-\nviding complementary evidence of systematic response shift\nbeyond overall accuracy.\nExperiment 2: Stability in non-deterministic setting\nWe examine the reliability of the prediction under stochastic\ndecoding, for increasing temperature values. Despite finding\nstable performance of commercial models (ChatGPT, Llama\n70B) across different temperatures (Renze 2024; Patel et al.\n2024), recent studies have showed how model performance\ndegrades for LLMs of limited size (< 13 B) over T ≥1, es-\npecially for tasks that require instruction following (Li et al.\n2025). In our experiments, we evaluate the output to prompt\nA in three different settings T = [0.3, 0.6, 1.0]. We focus on\nthe models that perform the best in Experiment 1. We define\nconsistency as the frequency of majority vote and correct-\nness as the frequency of correct answers in ten runs. This\nsetup allows us to assess whether high-confidence predic-\ntions consistently correspond to correct answers. We also\ndetermine the model stability and best performing model at\ndifferent stochastic regimes.\nExperiment 3: Self-Bias and Discerning Gold-Standard\nReasoning\nWe provide the LLM with the ESAP item de-\nscription, question, and multiple choice options. We ask the\nLLM to select the most clinically correct solution between\ntwo candidate explanations, with one being the model’s own\noutput from Experiment 1, the other being the gold-standard\nexplanation authored by ESAP instructors (prompt in App.\nPrompting/Prompt in Experiment 3). For this task, we used\nboth responses from prompt A and prompt B. We addition-\nally ask the LLM to output a degree of agreement between\nthe two explanations on a scale from 0 to 5. To evaluate the\npresence of positional bias, we prompted the model in two\nways: once with the gold-standard explanation listed first\nand the model’s reasoning second, and then with reversed or-\nder. During the analysis, we focus on the ESAP items where\nthe model selected an incorrect response in Experiment 1,\nsince those necessarily diverge from the indicated best an-\nswer. Incorrect might reflect factually wrong statements, as\nwell as overly cautious management plan, which do not align\nwith clinical expectations.\nGiven the time-consuming nature of this task, the clini-\ncal review focuses on the most critical cases. The pediatric\nendocrinology expert is asked to evaluate the clinical reason-\ning of the best-performing model on items that were consis-\ntently answered incorrectly, independent of the position bias.\nThe evaluation is based on one of the items of the CLEVER\nframework proposed for clinical reasoning, in (Liu et al.\n2025), using a Likert scale from 1 to 5. Clinical reasoning\nis defined as the alignment of the LLM’s content with the\ndiagnostic reasoning process used in medical practice.\nReproducibility Considerations in Model Implementa-\ntion\nExperiment 1-2-3 are run on a single NVIDIA\nL40S (CUDA 11.8, cuBLAS 11.11.3.6, cuDNN 9.1.0.70,\nNCCL 2.21.5). To assess implementation-level reproducibil-\nity, the best performing models in Experiment 1 were\nre-run on prompt A on a second system NVIDIA RTX\n5090 (CUDA 12.8, cuBLAS 12.8.3.14, cuDNN 9.7.1.26,\nNCCL 2.26.2). Both systems used the Hugging Face\ntransformers (4.51.3) library under determinis-\ntic conditions (do sample=False), with identical soft-\nware dependencies and decoding parameters. Although\nboth setups were functionally equivalent, it is common\nknowledge that GPU-level floating-point operations and\ncuBLAS/cuDNN kernel differences across CUDA versions\ncan introduce minor nondeterminism in matrix computa-\ntions (He and Lab 2025). We include this cross-system com-\nparison to quantify the impact of hardware-software stack\nvariability on model stability.\nOutput Evaluation\nIn our initial design, we planned to\nautomate the output evaluation using GPT-4o, but we aban-\ndoned this approach due to the lack of consistency in GPT-\n4o responses. The output evaluation is conducted separately\nby three non-medical reviewers. Beyond the five standard\nESAP options (A-E), we include two further categories:\nMultiple Selection and Hallucinate or None. We resume to\nMultiple Selection whenever a model selects two or more op-\ntions. In cases where the model response includes conflict-\ning, missing, or ambiguous selections (e.g., indicating one\nanswer, then contradicting itself), the output is marked as\nHallucinate or None. Hallucinate or None reflects the diffi-\nculty of non-endocrinology reviewers to draw a line between\nvalid but non-assertive medical verbiage and subtle medical\nerrors.\nFor instances where non-medical reviewers could not at-\ntribute one of the categories, two clinical experts, one in\nemergency medicine and one in pediatric endocrinology, are\nconsulted to assist with classification.\nThe code to reproduce LLMs responses and results is\navailable at\nhttps://github.com/vanessadamario/SmallMedLLMs-\nPedEndo\nResults\nBefore showing the results of each experiment, we comment\non the overall performance and hallucinations observed dur-\ning output evaluation.\nAmong the models, HuatuoGPT-o1-8B, Diabetica-7B,\nand Meditron3-8B exhibited the most consistent reasoning\npatterns. Their responses generally adhered to the multiple-\nchoice format, often discussing the given options and weigh-\ning their clinical likelihood before reaching a conclusion.\nHuatuoGPT-o1-8B occasionally reversed this order, present-\ning a final answer first, followed by a discussion of each\noption as requested in the prompt. In contrast, Diabetica-\no1 produced outputs that were more difficult to interpret.\n"}, {"page": 4, "text": "Model\nPrompt A\nPrompt B (similar to A)\nPrompt A without Letter Token\naccuracy (95% CI)\n%u\naccuracy (95% CI)\n%u\naccuracy (95% CI)\n%u\nHuatuoGPT-o1-8B\n0.35[0.26, 0.45]\n100\n0.35[0.26, 0.45]\n100\n0.33[0.24, 0.43]\n96.7\nDiabetica-o1\n0.33[0.24, 0.43]\n100\n0.34[0.25, 0.44]\n100\n0.27[0.19, 0.37]\n94.5\nDiabetica-7B\n0.30[0.21, 0.40]\n98.9\n0.32[0.23, 0.42]\n100\n0.27[0.19, 0.37]\n87.9\nMeditron3-8B\n0.33[0.24, 0.43]\n97.8\n−\n−\n0.34[0.25, 0.44]\n90.1\nMedFound-7B\n0.04[0.02, 0.11]\n18.6\n0.12[0.07, 0.20]\n34.1\n0.04[0.02, 0.11]\n22.0\nClinicalGPT-base-zh\n0.20[0.13, 0.29]\n79.1\n0.20[0.13, 0.29]\n94.5\n0.20[0.13, 0.29]\n79.1\nTable 1: Assessment of correctness and usability in experiment 1. Legend. For the three prompting strategies we report accuracy\nand 95% confidence interval. %u: percentage of usable responses, where the decision falls in one of the five ESAP options.\nLeft section: prompt A. Center section: prompt B, similar to prompt A. Right section: prompt A without letter (A-E) in the\nmultiple options.\nModel\nPrompt A vs Prompt B\nPrompt A with vs without Letter Token\nStuart-\nκ\nmatch-rate\nStuart-\nκ\nmatch-rate\nMaxwell\n95% CI\n95% CI\nMaxwell\n95% CI\n95% CI\nHuatuoGPT-o1-8B\n< 10−4\n0.55\n0.64\n< 10−4\n0.35\n0.48\n[0.42, 0.67]\n[0.53, 0.73]\n[0.23, 0.48]\n[0.38, 0.58]\nDiabetica-o1\n< 10−4\n0.40\n0.52\n< 10−4\n0.35\n0.48\n[0.27, 0.52]\n[0.42, 0.62]\n[0.23, 0.48]\n[0.38, 0.58]\nDiabetica-7B\n< 10−4\n0.63\n0.70\n< 10−4\n0.38\n0.51\n[0.51, 0.74]\n[0.60, 0.79]\n[0.26, 0.51]\n[0.40, 0.61]\nMeditron3-8B\n−\n−\n−\n0.006\n0.19\n0.35\n[0.07, 0.31]\n[0.26, 0.45]\nMedFound-7B\n−\n−0.18\n0.05\n0.72\n−0.20\n0.04\n[−0.24, −0.11]\n[0.02, 0.12]\n[−0.24, −0.14]\n[0.02, 0.11]\nClinicalGPT-base-zh\n0.19\n0.15\n0.32\n0.005\n0.18\n0.34\n[0.04, 0.27]\n[0.23, 0.42]\n[0.05, 0.30]\n[0.25, 0.44]\nTable 2: Assessment of stability and reproducibility in experiment 1. McNemar’s test results in p-value > 0.4 across all compar-\nisons with exception of MedFound-7B Prompt A vs Prompt B. No significant differences between accuracies across pairwise\ncomparison of prompting strategy. Beyond accuracy, stability is measured by Stuart-Maxwell test, to compare model output\nacross different prompting strategies, under the hypothesis of no difference. p-value show significant differences. Cohen’s κ\ncoefficient and match-rate are reported with their respective confidence intervals.\nWhile the initial portion of the response is often coherent,\nthe output frequently degenerates into simulated dialogues\nbetween a fictitious user and an assistant. These interactions\nincluded follow-up questions and, in some cases, newly gen-\nerated MCQs. Hallucinations also included emojis, Chinese\nideograms, LaTeX or Python code fragments, or statistical\nreferences disconnected from the task at hand. For the eval-\nuation, we considered the first part of the output, and dis-\ncarded what was unrelated to the analysis of the clinical sce-\nnario. MedFound-7B’s output also showed signs of degener-\nation. The model frequently hallucinated content; when not\nhallucinating, it was hesitant to provide a definitive answer\nand tended to suggest a wide array of additional diagnostic\ntests before committing to a conclusion. The ClinicalGPT-\nbase-zh model produced shorter responses, typically only a\nfew sentences, and rarely addressed all options. It often dis-\nplayed a strong preference for a single option without dis-\ncussing alternatives. For quantitative evaluations of option\ndistributions, see App. Output Evaluation.\nLLMs Performance, Prompt Stability, and\nAgreement\nIn Table 1, for each LLM we report accuracy, confidence\ninterval, and usability rate. The left, middle, and right panels\nshow respectively the performance under prompt A, prompt\nB, and prompt A without letter token.\nHuatuoGPT-o1-8B emerges as the model with the highest\nnumber of correct responses across prompt A and prompt B.\nAlthough HuatuoGPT-o1-8B is slightly more accurate than\nDiabetica-o1, a closer look at the LLMs outputs reveals im-\nportant differences. A breakdown on Diabetica-o1 perfor-\nmance shows that under prompt A and B the model out-\nputs respectively only 12 and 13 fully interpretable answers,\nthe output contains emojis in 12 and 14 cases, the output\nis truncated in 34 and 45 cases, the output contains LaTeX,\nPython, or Markdown code in 27 and 23 cases, and Chinese\ncharacters in 3 cases, across the two prompts. As expected,\nDiabetica-7B performs slightly worse than Diabetica-o1.\nMeditron3-8B shows relatively high performance compared\nto other models for prompt A, but it fails to produce an\noutput under prompt B. Models with lower overall perfor-\n"}, {"page": 5, "text": "10\n9\n8\n7 or \nless\nT=0.3\nHuatuoGPT-o1-8B\nDiabetica-o1\nDiabetica-7B\nMeditron3-8B\n10\n9\n8\n7 or \nless\nT=0.6\n0 1 2 3 4 5 6 7 8 910\n10\n9\n8\n7 or \nless\nT=1.0\n0 1 2 3 4 5 6 7 8 910\n0 1 2 3 4 5 6 7 8 910\n0 1 2 3 4 5 6 7 8 910\n0\n5\n10\n15\n20\n25\n# questions\nFrequency of correct option selection\n     Correctness\n                  Consistency\nFrequency of majority vote selection\nFigure 1: Consistency and correctness in the stochastic setting. From left to right, representations for HuatuoGPT-o1-8B,\nDiabetica-o1, Diabetica-7B, and Meditron 3-8B. From top to bottom, results across different temperature setting with T = 0.3\n(blue frame), T = 0.6 (gray frame), and T = 1.0 (yellow frame). In each plot, the y-axis reports consistency as majority vote\nselected ≥7 times, up to ten. Correctness varies from 0 to total number of runs (= 10), on the x-axis. The size and color\nintensity of the blob show the number of cases that fall into a specific category. Ideally, a perfect-scoring model would show all\nthe density concentrated in a single blob at the top right corner of the plot. We excluded all ESAP items where the model lacks\na majority class.\nmance, such as ClinicalGPT-base-zh, exhibit close to chance\nperformance (chance = 18.2 correct answer) and a strong\nbias toward a particular response (often option A). For fur-\nther discussion, refer to App. Output Evaluation.\nEvaluations of model stability are in Table 2. The ta-\nble does not contain statistics and p-value for the McNe-\nmar’s test. This resulted in p-values > 0.4 across all models\nand configurations (prompt A vs prompt B, prompt A with\nletter option and without), indicating that overall accuracy\nremained stable despite syntactic and formatting changes.\nNevertheless, subsequent analyses (Stuart-Maxwell test, Co-\nhen’s κ coefficient, and match-rate) revealed variations in re-\nsponse patterns, suggesting that accuracy stability does not\nnecessarily imply identical reasoning or output distributions.\nStuart-Maxwell tests revealed significant distributional\nshifts between prompt variants for all models (p < 0.05),\nwith the highest performing models showing extremely low\np-values (<\n10−4). This suggests that well-performing\nmodels, while maintaining stable accuracy, exhibit sys-\ntematic shifts in their response distribution when prompt\nsyntax changes. Models with lower performance, such as\nMedFound-7B and ClinicalGPT-base-zh show less coherent\nchanges and potentially more diffuse answers. From a repro-\nducibility perspective, this pattern highlights that stability in\naccuracy does not guarantee stability in decision behavior to\nminimal prompt perturbation.\nT = 0.3\nT = 0.6\nT = 1.0\n✓\n×\n✓\n×\n✓\n×\nHuatuoGPT-o1-8B\n14\n11\n11\n7\n10\n4\nDiabetica-o1\n5\n4\n7\n3\n5\n3\nDiabetica-7B\n13\n16\n6\n13\n3\n7\nMeditron3-8B\n2\n0\n0\n0\n0\n0\nTable 3: Number of ESAP items where the model consis-\ntently selected the same option 10 out of 10 runs. The count\nof correct (✓) and incorrect (×) responses is reported at dif-\nferent temperature regimes.\nAcross all models, the match rate exceeded Cohen’s κ by\napproximately 0.1 −0.2, indicating that a portion of ob-\nserved agreement reflects shared answer preferences over\nconsistency. Interestingly, Cohen’s κ and match-rate de-\ngraded more strongly when option letters were removed\n(Prompt A w vs w/o Letter Token) than when prompt syn-\ntax was altered (Prompt A vs Prompt B), suggesting that\nmodel consistency depends more on the formatting structure\nof multiple-choice than on linguistic changes.\nThe evaluation of LLMs performance under removal of\noption letters highlights a strong sensitivity bias toward spe-\ncific options. For such experiment, Cohen’s κ ≤0.4 for all\nmodels. With syntactic variations, only HuatuoGPT-o1-8B\n"}, {"page": 6, "text": "10\n15\n20\n25\n30\n35\n40\n# correct responses\nHuatuoGPT-o1-8B\nDiabetica-o1\nDiabetica-7B\nMeditron3-8B\nT=0\nFigure 2: Box-plot showing the distribution of correct responses across runs, at three temperatures (color codes as in Figure 1).\nThe solid horizontal line shows performance under deterministic condition for each model.\nand Diabetica-7B show a moderate level of agreement, re-\nspectively, with κ = 0.55 and 0.63.\nStability in Non-Deterministic Setting\nHuatuoGPT-o1-8B,\nDiabetica-o1,\nDiabetica-7B,\nand\nMeditron3-8B emerged as best performing models from\nExperiment 1 and were used for follow-up analysis assess-\ning the stability of their responses in the non-deterministic\nsetting.\nHuatuoGPT-o1\nHuatuoGPT-o1\nA\nB\n8%\n14%\n17%\n15%\n46%\n12%\n7%\n31%\n15%\n36%\nHall / None\nESAP \nnever\nESAP \nsecond\nESAP \nfirst\nESAP \nboth\nDiabetica-o1\nDiabetica-o1\n0\n10\n20\n30\n40\n50\n60\n# Questions - Only Those Answered Incorrectly from Exp 1\nA\nB\n31%\n16%\n20%\n31%\n10%\n23%\n8%\n32%\n27%\nFigure 3: HuatuoGPT-o1-8B and Diabetica-o1 selection of\ngold-standard explanation against their own reasoning under\nprompt A and prompt B. Each bar reports the percentage of\ntimes each model selected the gold standard explanation, re-\ngardless of the presented order (ESAP both in green), if only\nin one of the two positions (ESAP first in khaki or ESAP sec-\nond in yellow), in neither (ESAP never in red). Cases where\na model did not make a valid selection, either by failing\nto choose between the two explanations or by introducing\na new and unrelated option are indicated as Hallucinate /\nNone, in gray.\nIn Figure 1, we excluded ESAP items for which the model\nfails to produce a majority class and report how often a\nmodel selected the correct answer versus how often the same\nmodel selected its majority class. An ideal model would\nshow all the density concentrated in a single blob at the top-\nright corner of the plot. In practice, we observe that all tested\nmodels deviate from this ideal behavior following different\nPrompt\nCase\nScore\nMain Observations\n1\n27\n3\nMissed Factual Details\nand Key Diagnosis\n36\n4\nAcceptable Reasoning\n37\n4\nAcceptable Reasoning\n57\n4\nAcceptable Reasoning\nand Clinical Deviation\n60\n3\nPartially Correct\n71\n2\nMisclassification of Risk\n75\n2\nMissed Factual Details\n94\n3\nIncomplete Reasoning\n2\n27\n2\nWrong Statements\nMissed Factual Details\n34\n2\nWrong Statements\n49\n2\nMisinterpreted Labs\n94\n2\nFactual Recall Failure\nTable 4: Endocrinologist evaluation of HuatuoGPT-o1-8B\noutput on questions where model failed to identify the cor-\nrect reasoning. Responses are evaluated based on clinical\nreasoning on a Likert scale from 1 to 5.\ntrends. HuatuoGPT-o1-8B shows the highest overall num-\nber of consistently correct responses across ten runs for all\ntemperatures (Table 3, T = 0.3). At maximum consistency,\nHuatuoGPT-o1-8B is the only model for which we observe\nan increasing rate of correct responses over incorrect (Ta-\nble 3, first row). By contrast, Diabetica-o1 and Diabetica-7B\nexhibit a sharp decline in model consistency for increasing\ntemperature, with Diabetica-7B showing strong consistency\nfor incorrect responses at T = 0.3. Meditron3-8B becomes\nmore reluctant in providing a final answer at higher temper-\nature values. At T = 1.0, Meditron3-8B is the only case\nwhere the majority vote corresponds to the Hallucination /\nNone, for 71 out of 91 outputs, explaining the high density\ntowards the left side of the plot. We report frequencies of\nthe majority class in Supplemental Materials (Tables 8, 9,\n10). Table 3 further highlights how for HuatuoGPT-o1-8B\nthe temperature increase affects more abruptly the consis-\ntency of wrong answers, leaving a relatively higher number\nof correct answers still highly consistent. Overall, these re-\nsults highlight that consistency, as reflected by output fre-\n"}, {"page": 7, "text": "Model\nPrompt A: CUDA 11.8 vs CUDA 12.8\n∆accuracy\nMcNemar\nStuart-Maxwell\nκ (95% CI)\nmatch-rate (95% CI)\nHuatuoGPT-o1-8B\n+0.044\n0.5\n< 10−4\n0.51[0.38, 0.63]\n0.60[0.50, 0.70]\nDiabetica-o1\n−0.022\n0.8\n< 10−4\n0.53[0.41, 0.66]\n0.63[0.52, 0.72]\nDiabetica-7B\n+0.011\n1.\n< 10−4\n0.68[0.57, 0.79]\n0.75[0.65, 0.83]\nMeditron3-8B\n+0.011\n1.\n< 10−4\n0.31[0.19, 0.44]\n0.45[0.35, 0.55]\nTable 5: Comparison of LLMs response using same libraries and different CUDA version. ∆accuracy = accuracyCUDA 12.8 −\naccuracyCUDA 11.8. All other metrics are described in Experiment 1 section.\nquency across runs, does not necessarily map to correctness,\nleading to potential risks of over-relying on output frequency\nas a proxy for reliability and reproducible behavior.\nWe lastly show in Figure 2 how the distribution of the\nnumber of correct responses (across ten runs) changes for\nincreasing temperatures. While HuatuoGPT-o1-8B accu-\nracy remains stable, the other models show a decline, with\nMeditron3-8B not providing final answers at T = 1 and jus-\ntifying the sharp decline in performance.\nDiscerning Gold-Standard Reasoning\nOur analysis focuses on the cases where HuatuoGPT-o1-8B\nand Diabetica-o1 responded incorrectly: 61 (prompt A) and\n60 (prompt B) for Diabetica-o1, and 59 for both prompts in\nHuatuoGPT-o1-8B. Figure 3\nHuatuoGPT-o1-8B shows higher preference for the gold\nstandard explanation than Diabetica-o1. Under prompt A\n(and B), the HuatuoGPT-o1-8B selected the gold standard\nexplanation in both positions for 27 (21) cases, selected it in\nonly one position for 19 (26) cases, and it failed to select it\nin either position for 8 (4) cases. There were 5 (7) cases that\ncould not be classified due to the ambiguous or irrelevant\noutput. In comparison, Diabetica-o1 selected the gold stan-\ndard explanation in both positions for 19 (16) cases, in only\none position for 22 (24) cases, and never for 19 (14) cases,\nwith 1 (6) non-classifiable responses under prompt 1 (and 2).\nThese differences highlight behavioral reproducibility gaps\nbetween the two models: while both show comparable over-\nall accuracy, HuatuoGPT-o1-8B exhibits a greater ability in\nreasoning alignment with the clinical reference.\nA pediatric endocrinology expert reviewed HuatuoGPT-\no1-8B outputs where the gold-standard was never selected\n(12 ESAP items; App. Integral Evaluations from Pediatric\nEndocrinologists and HuatuoGPT-o1-8B. For more detailed\nclinical comments and comparison with HuatuoGPT-o1-8B\noutput, see Table 4.\nThe physician noted that HuatuoGPT-o1-8B often over-\nlooked key biochemical findings that would guide a clini-\ncian’s reasoning and would lead a clinician to immediately\nconsider or rule out specific options. However, only two of\nthe twelve reviewed cases contain factually incorrect state-\nments. This pattern suggests that reasoning might be coher-\nent but incomplete, pointing to systematic rather than ran-\ndom reasoning errors.\nAdditional agreement analyses between the two explana-\ntions as performed by the LLM are illustrated in Figure 8-\n9 for HuatuoGPT-o1-8B, and Figure 10-11 for Diabetica-\no1. Those seem to reinforce this view: for HuatuoGPT-\no1-8B, the spread of evaluation scores is wider when the\nmodel recognizes the ESAP explanation as superior than\nwhen it favors its own, suggesting a correlation between\nself-consistency (selection of ESAP explanation at both po-\nsitions) and interpretive coherence (evaluation of explana-\ntion agreement). We do not observe a similar behavior for\nDiabetica-o1.\nWe further refer the reader to Table 11 and Table 12 to ex-\namine the endocrinologist’s evaluation on HuatuoGPT-o1-\n8B responses. Overall, HuatuoGPT-o1-8B’s consider its re-\nsponses to be more similar to the correct ESAP explanation\nthan the expert physician. This finding is consistent with the\nmodel’s tendency to overlook key clinical information for\ncertain items.\nOverall, the results suggest a high variability for superior\nexplanation selection, dependent on the explanation position\nwithin the prompt, but also a coherent, even if flawed, rather\nthan randomic, evaluation of model response against gold-\nstandard for HuatuoGPT-o1-8B.\nNumerical Instability\nIn Table 5, we report the comparison of LLMs outputs un-\nder prompt A, given two different CUDA frameworks. This\nresult shows how across two CUDA versions, all models\nexhibited statistically significant distributional shifts (Stu-\nart–Maxwell p < 10−4), despite changes in mean accuracy\nare still within confidence intervals from Table 1. Agreement\nbetween runs remained moderate at best (κ = 0.31–0.68),\nwith match-rates spanning only 0.45–0.75. This divergence,\nobserved in the absence of architectural or dataset changes,\nsuggests that differences in numerical precision and kernel\nbehavior across CUDA builds propagate to measurable in-\nconsistencies in model predictions which lead to completely\ndifferent clinical outcomes.\nDiscussion\nInterpretation of Findings\nWhile the tested small open-source medical LLMs exhibit\ncomparable overall accuracy trends (Experiment 1), they\nlargely differ in the quality, consistency, and behavioral sta-\nbility of their output under stress-testing. We argue that\nmodel assessment should move beyond aggregate perfor-\nmance metrics (such as accuracy) toward finer-grained be-\nhavioral diagnostics that capture reasoning stability and be-\nhavioral reproducibility, essential for reliable and repro-\nducible science. Despite their medical fine-tuning, the eval-\nuated models show a pronounced bias toward option letters,\n"}, {"page": 8, "text": "which becomes particularly problematic in the tested con-\ntext, suggesting that superficial cues can override semantic\nreasoning.\nAs expected, increasing temperature values degraded\nmodel performance, reflecting higher creativity and reduced\noutput stability under stochastic sampling (Renze 2024;\nLi et al. 2025). Although some variation is inherent to\nLLMs (Atil et al. 2024), the degree and pattern of in-\nconsistency varies greatly between models, suggesting that\nHuatuoGPT-o1-8B could be a better fit to clinical tasks\nthan its counterparts. Our results further suggest that over-\nreliance on response frequency as a measure of correctness\nwould be misleading. In clinical contexts, such behavior\ncould translate into models persistently recommending in-\nappropriate actions, with the risk of reproducible, yet poten-\ntially harmful recommendations.\nEven the top-performing model, HuatuoGPT-o1-8B,\nehibited self-assessment bias, favoring its own explanations\nover a provided gold-standard rationale, even when incor-\nrect. Expert review in Experiment 3 showed that some of\nthese outputs were clinically acceptable though not strictly\ncorrect or incomplete, whereas in other situations the LLM\nmade critical factual errors. This pattern underscores that\ncurrent small medical LLMs can exhibit flawed reasoning,\nemphasizing the need for reproducibility-oriented evalua-\ntion frameworks that diagnose not only what LLMs answer,\nbut how they reason.\nLastly, a further dimension of instability emerged from\nour numerical reproducibility tests. Even when prompts,\nmodel weights, and HuggingFace libraries were held\nconstant, changes in CUDA versions yielded statisti-\ncally significant distributional shifts in model responses,\nwith moderate rate of agreement. This finding exposes\nhow hardware-level and software-level numerical differ-\nences, invisible to end users, can propagate to clinically\nmeaningful output changes. Adding to the multifaceted\ntheme of reproducibility, in exploratory runs we qualita-\ntively observed that identical model checkpoints (CUDA\n11.8 setup) produced non-identical outputs when exe-\ncuted through different inference functions within the Hug-\ngingFace transformers library, specifically Pipeline\nand AutoTokenizer+AutoModelForCausalLM. This incon-\nsistency, observed under determistic decoding conditions,\nhighlights how reproducibility can be affected by back-\nend library implementation details within the same soft-\nware framework. We notice how while the initial tokens\nfor each generation are identical, small syntactic varia-\ntions appeared later in the sequence, in some cases prop-\nagating into semantically distinct completions and dif-\nferent option selection. One example is reported in the\nAppendix Reproducibility Issues: pipeline vs AutoTok-\nenizer+AutoModelForCausalLM.generate()).\nLimitations\nFirst, this work focused on prompt variations and temper-\nature changes, which are proven to affect LLM responses\neven for large models (Jeon and Kim 2025; Wang et al. 2024;\nAzimi et al. 2024; Li et al. 2025; Renze 2024). These pa-\nrameters were prioritized since they have more widely been\ndiscussed and directly linked to reasoning consistency and\nstochastic robustness. However, other inference hyperpa-\nrameters were held constant throughout all the experiments,\nas detailed in the appendix, yet they may also influence\nmodel performance. While a more extensive hyperparame-\nter exploration would be valuable, such undertaking remains\nextremely time-consuming due to human-review, which re-\nquires approximately 60 minutes for the evaluation 91 cases.\nAdditionally, while MCQs provide a scalable frame-\nwork for benchmarking, our results highlight that LLMs are\nhighly sensitive to prompting. Our analysis of numerical in-\nstability was limited in scope and restricted to a subset of\ncases, nonetheless the results highlight statistical shifts in\nmodels behavior. Expanding this line of inquiry will be es-\nsential to establish reproducibility standards for clinical AI\nsystems.\nLastly, we acknowledge that non-expert reviewers may\noverlook details of medical relevance or subtle hallucina-\ntions. Nonetheless, our hybrid strategy mixing non-expert\nreviews with targeted expert evaluation on responses of\ndifficult interpretation offers a practical compromise be-\ntween rigor and feasibility. A deeper qualitative assess-\nment of the medical content across all model outputs would\nbe extremely valuable but remains prohibitively resource-\nintensive.\nConclusion\nThis study highlights that evaluation of small open-source\nmedical LLMs need to move beyond traditional accuracy\nmetrics alone. While traditional metrics remain useful, they\nobscure deeper sources of variability that affect how reliably\nLLMs work. By systematically probing prompt sensitivity,\nresponse stability, reasoning coherence, and numerical in-\nstability we expose multiple dimensions of reproducibility\nthat current evaluation frameworks largely overlook.\nOur findings reveal that even when absolute accuracy ap-\npears similar, models differ substantially in their behavioral\nreproducibility as the ability to produce consistent reasoning\nand explanation under semantically equivalent conditions.\nDespite low absolute performance, difficult to contextualize\nwithout a human baseline, our analysis reveals how variabil-\nity in model outputs underscores the need for rigorous and\ninterpretability-focused evaluation frameworks.\nThese results also raise concerns about the over-reliance\non MCQ-based evaluations which may mask reasoning in-\nconsistency and favor surface-level alignment. Future as-\nsessments should incorporate scenario-based assessment or\nfree-text justification grading, allowing for richer and more\ninterpretable insights into model decision processes. Ope-\nnAI’s recent release of HealthBench (Arora et al. 2025) rep-\nresents a potential area of exploration.\nCurrently, none of the tested small open-source mod-\nels demonstrated sufficient reliability for clinical decision-\nmaking “as-is”. However, their accessibility underscores the\npotential for transparent, reproducible, and lightweight ar-\nchitectures when paired with retrieval-augmentation. In fu-\nture work, we aim to extend these reproducibility-oriented\nmethods toward supporting safe and interpretable model use\nin low-resource settings.\n"}, {"page": 9, "text": "Acknowledgments\nThis work was supported by the National Artificial Intelli-\ngence Research Resource Pilot (NAIRR Grant No. 240463)\nand the Nova Southeastern University President’s Research\nGrant (NSU-PRG 2026). We gratefully acknowledge Paul\nGerbino, Daniel Schaible, and Robert Pursell of the En-\ndocrine Society for their interest in this study, and Dr. Veron-\nica Tozzo for her insightful contributions throughout this re-\nsearch development.\nReferences\nArora, R. K.; Wei, J.; Hicks, R. S.; Bowman, P.; Qui˜nonero-\nCandela, J.; Tsimpourlas, F.; Sharman, M.; Shah, M.; Val-\nlone, A.; Beutel, A.; et al. 2025.\nHealthbench: Evaluat-\ning large language models towards improved human health.\narXiv preprint arXiv:2505.08775.\nAtil, B.; Chittams, A.; Fu, L.; Ture, F.; Xu, L.; and Bald-\nwin, B. 2024. LLM Stability: A detailed analysis with some\nsurprises. arXiv e-prints, arXiv–2408.\nAye, T.; Boney, C. M.; Orr, C. J.; Leonard, M. B.; Leslie,\nL. K.; and Allen, D. B. 2024. Child Health Needs and the\nPediatric Endocrinology Workforce: 2020-2040. Pediatrics,\n153(Supplement 2): e2023063678J.\nAzimi, I.; Qi, M.; Wang, L.; Rahmani, A. M.; and Li, Y.\n2024.\nAccuracy and Consistency of LLMs in the Regis-\ntered Dietitian Exam: The Impact of Prompt Engineering\nand Knowledge Retrieval. arXiv preprint arXiv:2408.02964.\nChen, J.; Cai, Z.; Ji, K.; Wang, X.; Liu, W.; Wang, R.; Hou,\nJ.; and Wang, B. 2024. Huatuogpt-o1, towards medical com-\nplex reasoning with llms. arXiv preprint arXiv:2412.18925.\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle,\nA.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\net al. 2024. The llama 3 herd of models. arXiv e-prints,\narXiv–2407.\nHe, H.; and Lab, T. M. 2025.\nDefeating Nondetermin-\nism in LLM Inference.\nThinking Machines Lab: Con-\nnectionism.\nHttps://thinkingmachines.ai/blog/defeating-\nnondeterminism-in-llm-inference/.\nJeon, S.; and Kim, H.-G. 2025. A comparative evaluation of\nchain-of-thought-based prompt engineering techniques for\nmedical question answering.\nComputers in Biology and\nMedicine, 196: 110614.\nKhatun, A.; and Brown, D. G. 2024. A study on large lan-\nguage models’ limitations in multiple-choice question an-\nswering. arXiv preprint arXiv:2401.07955.\nKumar, S.; Ashraf, A. P.; Lteif, A.; Lynch, J.; and Aye, T.\n2021. Pediatric endocrinology: perspectives of pediatric en-\ndocrinologists regarding career choice and recruitment of\ntrainees. Endocrine Practice, 27(7): 743–748.\nLi, L.; Sleem, L.; Nichil, G.; State, R.; et al. 2025. Exploring\nthe Impact of Temperature on Large Language Models: Hot\nor Cold? Procedia Computer Science, 264: 242–251.\nLi, W.; Li, L.; Xiang, T.; Liu, X.; Deng, W.; and Garcia, N.\n2024. Can Multiple-choice Questions Really Be Useful in\nDetecting the Abilities of LLMs? In Proceedings of the 2024\nJoint International Conference on Computational Linguis-\ntics, Language Resources and Evaluation (LREC-COLING\n2024), 2819–2834.\nLiu, X.; Liu, H.; Yang, G.; Jiang, Z.; Cui, S.; Zhang, Z.;\nWang, H.; Tao, L.; Sun, Y.; Song, Z.; et al. 2025. A general-\nist medical language model for disease diagnosis assistance.\nNature Medicine, 1–11.\nMcDuff, D.; Schaekermann, M.; Tu, T.; Palepu, A.; Wang,\nA.; Garrison, J.; Singhal, K.; Sharma, Y.; Azizi, S.; Kulkarni,\nK.; et al. 2025. Towards accurate differential diagnosis with\nlarge language models. Nature, 1–7.\nMukherjee, S.; Gamble, P.; Ausin, M. S.; Kant, N.; Aggar-\nwal, K.; Manjunath, N.; Datta, D.; Liu, Z.; Ding, J.; Busacca,\nS.; et al. 2024. Polaris: A safety-focused llm constellation ar-\nchitecture for healthcare. arXiv preprint arXiv:2403.13313.\nPanickssery, A.; Bowman, S.; and Feng, S. 2024. Llm evalu-\nators recognize and favor their own generations. Advances in\nNeural Information Processing Systems, 37: 68772–68802.\nPatel, D.; Timsina, P.; Raut, G.; Freeman, R.; levin, M. A.;\nNadkarni, G. N.; Glicksberg, B. S.; and Klang, E. 2024. Ex-\nploring temperature effects on large language models across\nvarious clinical tasks. medRxiv, 2024–07.\nPesce, L. M.; and Sisto, P. A. P. 2021. Pediatric ESAP 2021-\n2022 pediatric endocrine self-assessment program ques-\ntions, answers, discussions. Endocrine Society.\nRenze, M. 2024.\nThe effect of sampling temperature on\nproblem solving in large language models. In Findings of\nthe association for computational linguistics: EMNLP 2024,\n7346–7356.\nSallinen, A.; Solergibert, A.-J.; Zhang, M.; Boy´e, G.;\nDupont-Roc, M.; Theimer-Lienhard, X.; Boisson, E.;\nBernath, B.; Hadhri, H.; Tran, A.; et al. 2025.\nLlama-3-\nMeditron: An Open-Weight Suite of Medical LLMs Based\non Llama-3.1. In Workshop on Large Language Models and\nGenerative AI for Health at AAAI 2025.\nSinghal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn, E.;\nAmin, M.; Hou, L.; Clark, K.; Pfohl, S. R.; Cole-Lewis, H.;\net al. 2025. Toward expert-level medical question answering\nwith large language models. Nature Medicine, 1–8.\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\n2024. Towards generalist biomedical AI. Nejm Ai, 1(3):\nAIoa2300138.\nWang, G.; Yang, G.; Du, Z.; Fan, L.; and Li, X. 2023.\nClinicalGPT: large language models finetuned with diverse\nmedical data and comprehensive evaluation. arXiv preprint\narXiv:2306.09968.\nWang, L.; Chen, X.; Deng, X.; Wen, H.; You, M.; Liu, W.;\nLi, Q.; and Li, J. 2024. Prompt engineering in consistency\nand reliability with the evidence-based guideline for LLMs.\nNPJ digital medicine, 7(1): 41.\nWataoka, K.; Takahashi, T.; and Ri, R. 2024. Self-preference\nbias in llm-as-a-judge. arXiv preprint arXiv:2410.21819.\nWei, L.; Ying, Z.; He, M.; Chen, Y.; Yang, Q.; Hong, Y.; Lu,\nJ.; Li, X.; Huang, W.; and Chen, Y. 2024. An adapted large\nlanguage model facilitates multiple medical tasks in diabetes\ncare. arXiv preprint arXiv:2409.13191.\n"}, {"page": 10, "text": "Workshop, B.; Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.;\nIli´c, S.; Hesslow, D.; Castagn´e, R.; Luccioni, A. S.; Yvon,\nF.; et al. 2022. Bloom: A 176b-parameter open-access mul-\ntilingual language model. arXiv preprint arXiv:2211.05100.\nXu, W.; Zhu, G.; Zhao, X.; Pan, L.; Li, L.; and Wang, W. Y.\n2024. Pride and prejudice: LLM amplifies self-bias in self-\nrefinement. arXiv preprint arXiv:2402.11436.\nYang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.;\nLi, C.; Li, C.; Liu, D.; Huang, F.; Dong, G.; Wei, H.; Lin,\nH.; Tang, J.; Wang, J.; Yang, J.; Tu, J.; Zhang, J.; Ma, J.;\nYang, J.; Xu, J.; Zhou, J.; Bai, J.; He, J.; Lin, J.; Dang, K.;\nLu, K.; Chen, K.; Yang, K.; Li, M.; Xue, M.; Ni, N.; Zhang,\nP.; Wang, P.; Peng, R.; Men, R.; Gao, R.; Lin, R.; Wang, S.;\nBai, S.; Tan, S.; Zhu, T.; Li, T.; Liu, T.; Ge, W.; Deng, X.;\nZhou, X.; Ren, X.; Zhang, X.; Wei, X.; Ren, X.; Liu, X.;\nFan, Y.; Yao, Y.; Zhang, Y.; Wan, Y.; Chu, Y.; Liu, Y.; Cui,\nZ.; Zhang, Z.; Guo, Z.; and Fan, Z. 2024. Qwen2 Technical\nReport. arXiv:2407.10671.\n"}, {"page": 11, "text": "Reproducibility Checklist\n1. General Paper Structure\n1.1. Includes a conceptual outline and/or pseudocode de-\nscription of AI methods introduced (yes/partial/no/NA)\nyes\n1.2. Clearly delineates statements that are opinions, hypoth-\nesis, and speculation from objective facts and results\n(yes/no) yes\n1.3. Provides well-marked pedagogical references for less-\nfamiliar readers to gain background necessary to repli-\ncate the paper (yes/no) yes\n2. Theoretical Contributions\n2.1. Does\nthis\npaper\nmake\ntheoretical\ncontributions?\n(yes/no) no\nIf yes, please address the following points:\n2.2. All assumptions and restrictions are stated clearly\nand formally (yes/partial/no) Type your response\nhere\n2.3. All novel claims are stated formally (e.g., in theorem\nstatements) (yes/partial/no) Type your response here\n2.4. Proofs of all novel claims are included (yes/par-\ntial/no) Type your response here\n2.5. Proof sketches or intuitions are given for complex\nand/or novel results (yes/partial/no) Type your re-\nsponse here\n2.6. Appropriate citations to theoretical tools used are\ngiven (yes/partial/no) Type your response here\n2.7. All theoretical claims are demonstrated empirically\nto hold (yes/partial/no/NA) Type your response here\n2.8. All experimental code used to eliminate or disprove\nclaims is included (yes/no/NA) Type your response\nhere\n3. Dataset Usage\n3.1. Does this paper rely on one or more datasets? (yes/no)\nyes\nIf yes, please address the following points:\n3.2. A motivation is given for why the experiments\nare conducted on the selected datasets (yes/par-\ntial/no/NA) yes\n3.3. All novel datasets introduced in this paper are in-\ncluded in a data appendix (yes/partial/no/NA) NA\n3.4. All novel datasets introduced in this paper will be\nmade publicly available upon publication of the pa-\nper with a license that allows free usage for research\npurposes (yes/partial/no/NA) yes\n3.5. All datasets drawn from the existing literature (po-\ntentially including authors’ own previously pub-\nlished work) are accompanied by appropriate cita-\ntions (yes/no/NA) yes\n3.6. All datasets drawn from the existing literature\n(potentially\nincluding\nauthors’\nown\npreviously\npublished work) are publicly available (yes/par-\ntial/no/NA) no\n3.7. All datasets that are not publicly available are de-\nscribed in detail, with explanation why publicly\navailable alternatives are not scientifically satisficing\n(yes/partial/no/NA) yes\n4. Computational Experiments\n4.1. Does this paper include computational experiments?\n(yes/no) yes\nIf yes, please address the following points:\n4.2. This paper states the number and range of values\ntried per (hyper-) parameter during development of\nthe paper, along with the criterion used for selecting\nthe final parameter setting (yes/partial/no/NA) yes\n4.3. Any code required for pre-processing data is in-\ncluded in the appendix (yes/partial/no) yes\n4.4. All source code required for conducting and analyz-\ning the experiments is included in a code appendix\n(yes/partial/no) yes\n4.5. All source code required for conducting and ana-\nlyzing the experiments will be made publicly avail-\nable upon publication of the paper with a license\nthat allows free usage for research purposes (yes/-\npartial/no) yes\n4.6. All source code implementing new methods have\ncomments detailing the implementation, with refer-\nences to the paper where each step comes from (yes/-\npartial/no) yes\n4.7. If an algorithm depends on randomness, then the\nmethod used for setting seeds is described in a way\nsufficient to allow replication of results (yes/par-\ntial/no/NA) yes\n4.8. This paper specifies the computing infrastructure\nused for running experiments (hardware and soft-\nware), including GPU/CPU models; amount of\nmemory; operating system; names and versions of\nrelevant software libraries and frameworks (yes/par-\ntial/no) yes\n4.9. This paper formally describes evaluation metrics\nused and explains the motivation for choosing these\nmetrics (yes/partial/no) yes\n4.10. This paper states the number of algorithm runs used\n"}, {"page": 12, "text": "to compute each reported result (yes/no) yes\n4.11. Analysis\nof\nexperiments\ngoes\nbeyond\nsingle-\ndimensional summaries of performance (e.g., aver-\nage; median) to include measures of variation, con-\nfidence, or other distributional information (yes/no)\nyes\n4.12. The significance of any improvement or decrease in\nperformance is judged using appropriate statistical\ntests (e.g., Wilcoxon signed-rank) (yes/partial/no)\nyes\n4.13. This paper lists all final (hyper-)parameters used\nfor each model/algorithm in the paper’s experiments\n(yes/partial/no/NA) yes\n"}, {"page": 13, "text": "Prompting\nIn this Section, we include the prompting strategies used in the experiments. The arguments <description>,\n<question>, <options> are filled with content from the ESAP 2021-2022 program, with <explanation *> show-\ning two reasoning paths, either the gold-standard explanation extracted from the ESAP or alternatively a model’s output from\nExperiment 1.\nPrompts in Experiment 1\nPrompt A\n‘Given the following clinical case, consider the question, and provide the most likely answer among the provided\noptions and the reasoning process behind your choice.\n<description>\n<question>\nOptions: <options>\nPlease, reduce the output to a paragraph and report the logic steps that led to your answer.’\nPrompt B\n‘Consider this clinical case, question, and options available.\n<description>\n<question>\nOptions: <options>\nPlease do provide your choice among the possible answers. Then provide the reasoning steps behind your choice.’\nPrompt in Experiment 3\n‘Given the case the following clinical case and multiple choice question, I will provide you two potential explanations. Can you\nplease report the level of agreement from a scale from 0 (null agreement) to 5 (perfect agreement) and select which response\nmakes more clinical sense between the two explanations?\nClinical case: <description>\nQuestions: <question>\nOptions: <options>\nExplanation 1: <explanation 1>\nExplanation 2: <explanation 2>\nPlease state clearly which is the most clinical correct option between explanation 1 and explanation 2. Then specify\nthe agreement level between explanation 1 and explanation 2.’\nExperimental Setup\nInference and Hyperparameters\nWe perform the model inference sequentially, with each model loaded once and used to process the full set of 91 ESAP\nitems in a single session, each as a single batch. By calling the function for output generation separately for each prompt,\nwe assure that the model output is independent from previous prompts. All the experiments are performed using AutoTok-\nenizer+AutoModelForCausalLM+.generate() from the transformers library (version 4.51.3). The experiments ran on a single\nNvidia L40S, cuda 11.8. More details on libraries and software versions are available at\nhttps://github.com/vanessadamario/SmallMedLLMs-PedEndo/blob/main/requirements.txt.\nExperiment 1\ndo sample set to False, temperature, top p, and top k set to None, length penalty = 1.0, repetition penalty =\n1.2, max tokens=1500, max new tokens = 1500, num return sequences = 1. pad token id = 128009, eos token id = 128009,\nand attention mask are output of the default tokenizer setting, with padding=True and truncation=True. All other arguments are\nleft as default from the AutoTokenizer+AutoModelForCausalLM+.generate().\n"}, {"page": 14, "text": "Experiment 2\nSame hyperparameters of Experiment 1, with exception of do sample = True, temperature=0.6, top p=0.9,\ntop k=75.\nExperiment 3\nThis shares identical hyperparameters of Experiment 1, with exception of max new tokens = 2048.\nOutput Evaluation\nModel\nPrompt\nA\nB\nC\nD\nE\nHall / None\nMultiple\nHuatuoGPT-o1-8B\nA\n19\n24\n23\n13\n12\n0\n0\nB\n21\n19\n22\n17\n12\n0\n0\nA No Letter\n25\n20\n19\n16\n8\n1\n2\nDiabetica-o1\nA\n24\n24\n20\n17\n6\n0\n0\nB\n15\n23\n21\n20\n12\n0\n0\nA No Letter\n22\n16\n22\n17\n9\n3\n2\nDiabetica-7B\nA\n13\n19\n28\n19\n11\n1\n0\nB\n13\n21\n26\n19\n12\n0\n0\nA No Letter\n16\n14\n18\n20\n12\n5\n6\nMeditron3-8B\nA\n18\n21\n18\n19\n13\n2\n0\nB\n-\n-\n-\n-\n-\n-\n-\nA No Letter\n17\n18\n12\n19\n16\n7\n2\nMedFound-7B\nA\n13\n1\n1\n1\n1\n72\n2\nB\n30\n3\n1\n1\n0\n56\n0\nA No Opt Letter\n13\n3\n1\n1\n2\n64\n7\nClinicalGPT-base-zh\nA\n23\n19\n17\n7\n6\n16\n3\nB\n69\n2\n8\n5\n2\n2\n3\nA No Opt Letter\n46\n7\n6\n7\n6\n18\n1\nTable 6: Experiment 1: Responses distribution for the three prompts tested. MedFound-7B and ClinicalGPT-base-zh show the\nhighest degree of hallucination. Meditron3-8B does not output a response under prompt B.\nModel\nTemperature\nA\nB\nC\nD\nE\nHall / None\nMultiple\nHuatuoGPT-o1-8B\n0.3\n172\n236\n206\n168\n120\n5\n3\n0.6\n171\n231\n201\n171\n124\n12\n0\n1.0\n171\n234\n196\n170\n120\n17\n2\nDiabetica-o1\n0.3\n203\n210\n219\n172\n106\n0\n0\n0.6\n204\n210\n215\n173\n97\n11\n0\n1.0\n181\n213\n224\n175\n98\n12\n7\nDiabetica-7B\n0.3\n146\n170\n276\n194\n113\n8\n3\n0.6\n170\n166\n252\n198\n110\n12\n2\n1.0\n158\n169\n237\n162\n123\n53\n8\nMeditron3-8B\n0.3\n127\n195\n202\n195\n164\n26\n1\n0.6\n149\n202\n182\n182\n156\n39\n0\n1.0\n82\n111\n109\n70\n56\n476\n6\nTable 7: Experiment 2: Responses distribution across ten runs of prompt A.\nA\nB\nC\nD\nE\nNo Majority\nHuatuoGPT-o1-8B\n17\n24\n17\n16\n10\n7\nDiabetica-o1\n18\n20\n22\n13\n8\n10\nDiabetica-7B\n13\n18\n29\n17\n9\n5\nMeditron3-8B\n11\n19\n20\n17\n13\n11\nTable 8: Frequency of the majority class for T = 0.3.\n"}, {"page": 15, "text": "A\nB\nC\nD\nE\nNo Majority\nHuatuoGPT-o1-8B\n16\n22\n21\n17\n12\n3\nDiabetica-o1\n22\n18\n23\n16\n7\n5\nDiabetica-7B\n16\n15\n27\n19\n9\n5\nMeditron3-8B\n9\n23\n16\n17\n11\n15\nTable 9: Frequency of majority class for T = 0.6.\nA\nB\nC\nD\nE\nHall / None\nNo Majority\nHuatuoGPT-o1-8B\n17\n23\n20\n15\n11\n0\n5\nDiabetica-o1\n13\n19\n25\n16\n8\n0\n10\nDiabetica-7B\n14\n19\n26\n15\n11\n0\n6\nMeditron3-8B\n3\n4\n0\n2\n2\n71\n9\nTable 10: Frequency of majority class for T = 1.0. Meditron3-8B avoid option selection for most cases (71 out of 91).\nVisualization of Model Consistency Across Temperatures\nFigures 4-7 show how model correctness changes across different temperatures, for the three T values tested. ESAP items are\nordered based on each model performance at the lowest temperature tested (T = 0.3). HuatuoGPT-o1-8B exhibits the highest\ncoherence. Diabetica-7B and Diabetica-o1 represent the intermediate case. Interestingly, Diabetica-7B shows much higher\nconsistency and correctness for T = 0.3 but its performance degrades quite abruptly for increasing temperature. Meditron3-8B,\nas discussed in the main body of the paper, become more resistant in providing a response for T = 1.0, which explains the lack\nof higher numerical values.\n51 65 29 30 31 90 34 37 38 39 60 59 42 43 45 93 48 50 53 94 27 23 2 82 21 75 89 8 11 71 80 57 7 58 81 52 36 16 20 22 26 97 78 72 56 14 86 84 15 85 12 13 96 49 74 73 19 95 98 87 55 33 91 9 17 5 66 69 41 44 28 64 61 40 67 92 70 1 79 77 76 63 62 47 46 25 24 18 10 83 99\nCase ID\n0.3\n0.6\n1.0\nLab Tests\nTemperature\nHuatuoGPT-o1-8B at Different Temperatures\n# Times Correct Option is Selected\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nYes\nNo\nFigure 4: Graphical representation of HuatuoGPT-o1-8B consistency and correctness at different temperature values.\n37 69 64 83 60 84 53 87 50 90 42 39 94 20 19 71 13 14 15 12 11 80 95 8 59 38 86 31 30 92 93 74 75 23 9 21 7 49 5 47 17 82 36 24 22 65 67 18 44 34 33 66 63 61 78 72 55 48 46 79 40 29 85 73 1 45 26 16 96 56 41 51 43 62 58 25 89 28 27 81 57 52 97 70 2 91 76 77 98 10 99\nCase ID\n0.3\n0.6\n1.0\nLab Tests\nTemperature\nDiabetica-o1 at Different Temperatures\n# Times Correct Option is Selected\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nYes\nNo\nFigure 5: Graphical representation of Diabetica-o1 consistency and correctness at different temperature values.\nFurther Results for Experiment 3: Discerning Gold-Standard Reasoning\nAgreement between Gold Standard and Models Outputs from a Model’s Perspective\nWe extracted agreement scores from the model outputs on a 0–5 scale. For each case, we calculated the average agreement by\ncombining the scores of two runs: one in which the gold standard explanation was presented first and the model’s reasoning\n"}, {"page": 16, "text": "50 74 49 72 71 69 30 31 33 44 36 64 38 39 60 59 24 45 22 87 2 47 8\n9 90 89 42 55 14 15 16 83 82 81 80 84 52 53 1 75 78 95 97 65 73 51 13 21 12 28 37 34 48 5 43 20 11 91 18 17 61 93 63 79 96 67 27 41 62 7 92 85 66 23 86 94 99 40 77 98 25 26 10 70 29 58 57 56 46 19 76\nCase ID\n0.3\n0.6\n1.0\nLab Tests\nTemperature\nDiabetica-7B at Different Temperatures\n# Times Correct Option is Selected\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nYes\nNo\nFigure 6: Graphical representation of Diabetica-7B consistency and correctness at different temperature values.\n84 30 29 60 56 71 48 21 86 16 75 37 2 81 97 14 33 31 5 59 7 38 79 82 50 34 80 85 13 8 53 72 65 66 67 73 89 91 51 94 22 24 96 36 23 43 42 12 74 17 27 95 58 49 15 90 87 41 78 18 40 9 61 20 93 26 98 46 69 64 83 19 63 62 39 55 11 99 57 47 45 44 28 76 70 52 25 77 1 92 10\nCase ID\n0.3\n0.6\n1.0\nLab Tests\nTemperature\nMeditron3-8B at Different Temperatures\n# Times Correct Option is Selected\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nYes\nNo\nFigure 7: Graphical representation of Meditron3-8B consistency and correctness at different temperature values.\nsecond, and another with the order reversed. Since agreement is less informative when the model already selects the correct\nanswer, our analysis focuses on cases where the model’s selected option disagreed with the gold standard choice. The goal\nof the following evaluation is to observe whether the level of agreement reflects the discrimination capabilities of the model\nto select the gold standard explanation against the model’s own reasoning. We excluded the clinical cases where the model’s\nreasoning selects a different answer from the one in explanation 1 and explanation 2.\nFigures 8-9 and Figures 10-11 show respectively the average agreement score for HuatuoGPT-o1-8B and Diabetica-o1 for\nprompt A and prompt B.\nESAP\nboth\n0\n1\n2\n3\n4\n5\nAgreement Level\nn = 27\nESAP\nsecond\nn = 10\nESAP\nfirst\nn = 9\nESAP\nnever\nn = 8\nComparing ESAP with \nWrong Outcomes (Prompt A)\nPicking ESAP Explanation as Best\nFigure 8: Distribution of agreement scores for HuatuoGPT-o1-8B: Prompt A\nThe analysis can only be speculative given the small sample size. While Diabetica-o1 does not show any specific trend,\nthe box-plot distributions for HuatuoGPT-o1-8B suggest something interesting. In Figures 8-9, agreement scores are more\nwidely distributed for cases where the model selected the gold standard explanation in both runs (Both), indicating greater\nvariability of the agreement score and the potential capability of discerning clinical differences between the gold standard and\nthe misleading response of the model itself. Interestingly, when the model favored its own reasoning over the gold standard\n(Never) (n = 8 in prompt 1 and n = 4 in prompt 2), the box plots are generally more skewed towards higher agreement values,\npotentially reflecting a lack of sensitivity to the subtle distinctions in the gold standard explanation versus the model reasoning.\n"}, {"page": 17, "text": "ESAP\nboth\n0\n1\n2\n3\n4\n5\nAgreement Level\nn = 21\nESAP\nsecond\nn = 18\nESAP\nfirst\nn = 9\nESAP\nnever\nn = 4\nComparing ESAP with \nWrong Outcomes (Prompt B)\nPicking ESAP Explanation as Best\nFigure 9: Distribution of agreement scores for HuatuoGPT-o1-8B: Prompt B\nESAP\nboth\n0\n1\n2\n3\n4\n5\nAgreement Level\nn = 19\nESAP\nsecond\nn = 10\nESAP\nfirst\nn = 12\nESAP\nnever\nn = 19\nComparing ESAP with \nWrong Outcomes (Prompt A)\nPicking ESAP Explanation as Best\nFigure 10: Distribution of agreement scores for Diabetica-o1: Prompt A\nThe values in the right plot (Never) of Figure 8 and Figure 9 can be qualitatively compared to the scores in Table 4, where\nthe evaluation based on clinical reasoning is on a 1-5 scale. HuatuoGPT-o1-8B shows higher levels of agreement than the ones\nassigned by the pediatric endocrinology expert. Evaluation of experiment 1 outcomes is given by the pediatric endocrinologist\nand HuatuoGPT-o1-8B (from Experiment 3).\nIntegral Evaluations from Pediatric Endocrinologist and HuatuoGPT-o1-8B\nTable 11 and Table 12 show the clinician’s evaluation of HuatuoGPT-o1-8B responses of Experiment 1 and their evaluation\nin Likert scale (1-5) based on the clinical reasoning scale in CLEVER. The right side of the tables shows the evaluation of\nHuatuoGPT-o1-8B’s previous explanation against the gold standard. In general, we notice that HuatuoGPT-o1-8B tends to be\nmore forgiving, losing some important details related to biochemical and genetic mechanisms which are key for the diagnosis.\nReproducibility Issues: pipeline vs AutoTokenizer + AutoModelForCausalLM.generate()\nIn this study, all small open-source LLMs were run using the HuggingFace transformers library. Text generation was per-\nformed using two approaches: the high-level pipeline() interface and a lower-level setup involving AutoTokenizer ,\nAutoModelForCausalLM , and manual use of .generate() . Both were configured with do sample=False and a\ntemperature=0, which should theoretically result in deterministic result. Between the two workflows, only the pad token id\nand eos token id parameters are different, but our understanding is that, since we avoid the use of batches and call the function\nfor output generation separately for each prompt, those tokens should not affect the model generation.\nAlthough in some cases the two methods produced identical responses, we observed instances where the generated outputs\ndiverged after the initial few tokens, despite identical prompts and generation settings. In the manual .generate() setup,\nwe explicitly set pad token id and eos token id to the model’s end-of-sequence token. However, these parameters are\nnot exposed or controlled in the same way through pipeline() , which can lead to implicit differences in how padding or\nattention masks are handled during inference.\nThis divergence highlights how small numerical variations might cause significant differences in auto-regressive generation,\neven under deterministic decoding conditions. This invites further considerations when evaluating model consistency and results\n"}, {"page": 18, "text": "ESAP\nboth\n0\n1\n2\n3\n4\n5\nAgreement Level\nn = 16\nESAP\nsecond\nn = 5\nESAP\nfirst\nn = 19\nESAP\nnever\nn = 14\nComparing ESAP with \nWrong Outcomes (Prompt B)\nPicking ESAP Explanation as Best\nFigure 11: Distribution of agreement scores for Diabetica-o1: Prompt B\nreproducibility.\nAutoTokenizer + AutoModelForCausalLM:\n## Thinking\nAlright, let’s think about this little guy who just came in with some\npretty interesting symptoms. So first off, we’ve got him having trou-\nble with his eyes - specifically, there’s something going on with his\nvision where one eye turns inward. That happened right after he\ntook quite a tumble when he fell while trying to stand up by himself.\nNow, they did an MRI and found that both of his optic nerves look\nswollen, kind of like they’re twisted around too much, and even the\nback part of his eyeballs seem a bit flat. This all sounds like it could\nbe related to increased pressure inside his skull, maybe because of\nfluid buildup or swelling somewhere.\nNow, moving onto what else is happening with him... Oh, I see! They\nmentioned he has an unusual head shape. It seems like his skull isn’t\ndeveloping as expected, especially across the top where you’d expect\ntwo separate parts but instead, everything fused together into more\nof a long oval shape. And then there’s how tall he is – way shorter\nthan other kids his age, definitely below the fifth percentile. Plus,\nhe’s got these issues with his legs, like bowing and a funny walk.\nHmm, those sound familiar...\n[..]\n## Final Response\nThe clinical findings described in the scenario point towards a skele-\ntal disorder characterized by disproportionate short stature, limb\ndeformities, and craniofacial anomalies. Given the presence of short\nstature (< 5 th percentile), mild genu varum (bow-leggedness) defor-\nmity, and a waddling gait, along with the distinct head shape indi-\ncating complete fusion of the sagittal suture and dolichocephaly,\nthe most likely underlying skeletal disorder is **C. Hypochon-\ndroplasia**. This diagnosis aligns with the characteristic features\nobserved in the patient, particularly the absence of dental abnor-\nmalities and the nature of the limb deformities. Other potential di-\nagnoses, such as vitamin D deficiency or genetic syndromes, do not\nfully account for the specific constellation of symptoms presented.\nTherefore, hypochondroplasia emerges as the most plausible expla-\nnation for the child’s condition.\nPipeline:\n## Thinking\nAlright, let’s think about this little guy who just came in with some\npretty interesting symptoms. So first off, we’ve got him having trou-\nble with his eyes - specifically, there’s something going on with his\nvision where one eye turns inward. That happened right after he\ntook quite a tumble when he fell while trying to stand up by himself.\nNow, they did an MRI and found that both of his optic nerves look\nswollen, kind of like they’re twisted around too much, and even the\nback part of his eyeballs seem a bit flat. This all sounds like it could\nbe related to increased pressure inside his skull, maybe because of\nfluid buildup or swelling somewhere.\nNow, moving onto what else is happening with him... Oh, I see!\nThere’s more than just the eyes. They mentioned he has a funny-\nshaped head – not round but sort of long and narrow, almost like\na football. And then there’s how tall he isn’t growing compared to\nother kids his age. Plus, he’s got these bowed legs and walks with\na weird, wobbly gait. It seems like everything points towards some\nbone issues here. Let me think...\n[..]\n## Final Response\nThe clinical presentation described suggests several key features:\nbilateral eye issues post-fall, unusual cranial shape, short stature,\nleg bowing, and a distinctive gait. Given these details, the condi-\ntion that aligns most closely with these findings is **D. Severe vi-\ntamin D deficiency**. This conclusion is supported by the presence\nof rachitic changes seen on x-ray, indicating poor bone mineraliza-\ntion typically caused by insufficient vitamin D intake during critical\ngrowth periods. While other conditions such as hypochondroplasia\nmay share similar skeletal manifestations, the specific context of in-\nadequate vitamin D supplementation early in life strongly supports\nsevere vitamin D deficiency as the primary concern. Other options\nlisted do not fully account for the comprehensive symptom set pre-\nsented. Therefore, option D is the most fitting diagnosis based on\nthe information provided.\n"}, {"page": 19, "text": "Case\nPediatric\nEndocrinology\nExpert\n(1-5)\nHuatuoGPT-o1-8B\nEx-\nplanation\n1:\nGold-Standard;\nExplanation 2: Model Reason-\ning\nExplanation\n1:\nModel\nRea-\nsoning; Explanation 2: Gold-\nStandard\n(0-5)\n27\nReasoning is generally correct,\nbut did not get to the biochem-\nical diagnosis of 5-α reductase\ndeficiency.\n3\n[..]. explanation 1 focusing on\ngenetic considerations and ex-\nplanation 2 emphasizing phys-\niological mechanisms.\n[..]\ntheir\napproaches\ndiffer\nslightly\nin\ninterpreting\nthe\nsignificance of each detail.\n4\n36\nAnswer is actually not totally\nunreasonable and honestly I\nwould consider doing this as\nwell.\n4\n[..] While Explanation 1 priori-\ntizes safety, Explanation 2 con-\nsiders ongoing treatment effi-\ncacy.\n[..] While explanation 1 focuses\nprimarily on adjusting the med-\nication dosage, explanation 2\nprovides additional context and\npotential treatment alternatives.\n4\n37\nThis question is a bit tricky as\nit asks about order. I may do an\nACTH stimulation test as well.\n[..] The model actually consid-\nered the correct answer initially,\nso the thinking is generally cor-\nrect here.\n4\n[..] while Explaining 2 offers\npractical insights quickly, Ex-\nplaining 1 covers broader di-\nagnostic ground. They comple-\nment each other nicely.\n[..] There is partial overlap\nin recommendations regarding\nthe necessity of initial adrenal\nfunction testing, leading to a\nmoderate level of agreement.\n4\n57\nAgain things were generally\ncorrect\nhere.\nSome\nmedical\ncenters and if the patient was\nin the hospital, honestly we\nwould probably do the stimula-\ntion test.\n4\n[..] Both perspectives recog-\nnize the critical nature of as-\nsessing adrenal status post-long\nterm steroid exposure but differ\nslightly in methodology prefer-\nence.\n[..] Explanation 1 better suits\nscenarios demanding prompt\nresolution, whereas Explana-\ntion\n2\nfits\nbetter\nin\nnon-\nemergency settings where grad-\nual tapering is feasible.\n4\n60\nHormone profile should be that\nof someone in the fed state, but\nthe hypothalamus cannot see\nthose signals. It got some of it\ncorrect, but not all.\n3\n[..] They differ in specific detail\n[..]. Although both capture key\naspects of the condition, they\npresent distinct interpretations\nof the same underlying prob-\nlem.\n[..]\nWhile\ninsightful,\nthis\napproach\n(ESAP\ngold-standard\nresponse)\ndoes\nnot\ndi-\nrectly address the immediate\nconcerns presented in the initial\nscenario.\n3.5\n71\nModel did not assign that de-\nspite the large thyroid nodule,\nit is actually a low risk cancer\nand therefore the TSH does not\nneed to be as aggressively sup-\npressed.\n2\nExplanation 1 [..] provides con-\ntext essential for understanding\nthe nuances involved in risk cat-\negorization. Meanwhile, expla-\nnation 2 distills this complexity\ninto actionable steps, emphasiz-\ning the critical balance needed\nin real-world scenarios.\n[..] **Explanation 1 is slightly\nmore directly applicable** due\nto its focus on general princi-\nples rather than strict catego-\nrization rules. However, **both\nexplanations share significant\nrelevance** [..].\n4\n75\nThe model did not get to the\nkey part of this question which\nis the actual biochemical profile\nthat was described. I would not\nbe able to get this correct if I\ncould not have access to the in-\nternet as this is really a question\nabout facts.\n2\n[..] Explanation 1 provides a\ndetailed background on each\ngene. [..] However, it feels a\nbit dry and lacks the real-world\napplication aspect. Meanwhile,\nexplanation 2 gets straight to\nthe point.\n[..] Explanation 1 zeroes in\non the specifics of the case,\nwhereas Explanation 2 takes a\nbroader view of genetic consid-\nerations.\n3\n94\nThere\nwere\nscattered\ntruths\nhere. Again, this is a tough\nquestion at the end of the day\nthat I would struggle with. I just\ndon’t think that it got to the\ncrux of the question, what is the\nmost common pathogenic ge-\nnetic change that we see in kids.\n3\n[..] Initially, I leaned towards\nESAP gold-standard\nresponse due to its general\npopularity, but upon further\nconsideration,\nthe\nstronger\nlink to pediatric cases made\nmodel’s response\nthe\nbetter choice.\n[..]\nBoth\nexplanations\nrec-\nognize\nthe\nimportance\nof\n(ESAP gold-standard\nresponse) and (model’s\nresponse)\nin\npapillary\nthyroid cancer, but differ in\ntheir prioritization based on\nthe patient’s age and disease\ncharacteristics.\n4\nTable 11: HuatuoGPT-o1-8B reasoning for incorrect answers to prompt 1 where model consistently failed to identify gold-\nstandard reasoning (both). On the left, pediatric endocrinology expert evaluation, on the right, Huatuo-o1-8B evaluation.\n"}, {"page": 20, "text": "Case\nPediatric\nEndocrinology\nExpert\n(1-5)\nHuatuoGPT-o1-8B\nEx-\nplanation\n1:\nGold-Standard;\nExplanation 2: Model Reason-\ning\nExplanation\n1:\nModel\nRea-\nsoning; Explanation 2: Gold-\nStandard\n(0-5)\n27\nThere is worse reasoning here\nand wrong statements about\nhormone levels that are in the\nquestion stem. Again, it does\nnot get to the 10:1 ratio that is\nneeded for 5-α RD.\n2\n[..] Both explanations touch on\nsimilar themes regarding an-\ndrogen function, but they dif-\nfer slightly in their interpreta-\ntions of the exact mechanisms\ninvolved [..] they diverge some-\nwhat in interpreting the precise\nmechanisms.\n[..] I’d say explanation 1 cap-\ntures the essence of the bio-\nchemical puzzle better, focus-\ning precisely on the enzyme-\nrelated aspect. Yet, explanation\n2 offers a comprehensive view\nincorporating familial patterns\nand clinical manifestations.\n4\n34\nHonestly, strange question. In-\nsulinoma is extremely rare at\nthis age. Diazoxide challenge\nreally is not useful that it\npicked. There are physiology\nstatements that are not exactly\ntrue. It did have the correct an-\nswer interestingly and almost\nchose it.\n2\n[..] However, slight variations\nexist in the emphasis placed\non each detail, reflecting nu-\nanced differences in interpreta-\ntion. Overall, the conclusions\ndrawn share significant overlap.\nAlthough both paths explore the\nsame core issue, they differ in\nemphasis and scope. Explana-\ntion 1 prioritizes functional di-\nagnostics, while Explanation 2\nleans towards structural assess-\nments and genetic underpin-\nnings.\n4\n49\nThere was some misinterpre-\ntation of the lab results here.\nThere is no evidence of hy-\npothyroidism at this point. It\nsounds like the model had con-\ncerns about the large drop in\nT4. There is no evidence of\nover-treatment.\n2\n[..] I’d say Explanation 2 is the\nmore clinically sound choice.\nIts emphasis on timely interven-\ntion to maintain optimal thy-\nroid function resonates with\nreal-world\nmedical\ndecision-\nmaking.\n[..] Both explanations recognize\nthe significance of the T4 de-\ncline and its implications for\nthe patient’s treatment trajec-\ntory. They share similar in-\nsights regarding the necessity of\navoiding over-treatment and en-\nsuring safety, although they dif-\nfer slightly in their prioritiza-\ntion of specific diagnostic steps.\n4\n94\nThere was some misinterpre-\ntation of the lab results here.\nThere is no evidence of hy-\npothyroidism at this point. It\nsounds like the model had con-\ncerns about the large drop in\nT4. There is no evidence of\nover-treatment.\n2\n[..] the most clinically sensi-\nble conclusion regarding the\ngenetic alteration causing this\npapillary\nthyroid\ncarcinoma\nin the 16-year-old patient is\nmodel’s response,\ndue\nto\nits\nhigher\nfrequency\nin\npediatric cases compared to\nESAP gold-standard\nresponse.\n[..]\nit\nbecomes\nclear\nthat\nESAP gold-standard\nresponse remains the top\ncontender\nfor\nthe\nunder-\nlying\ngenetic\nissue\nin\nthis\nadolescent\ncase.\nDespite\ninitial\nthoughts\nfavoring\nmodel’s response,\nthe\ndata\npoints\nstrongly\ntoward\nESAP gold-standard\nresponseas the most clin-\nically relevant factor. [..] the\nmost clinically sensible expla-\nnation is **Explanation 1**.\n4\nTable 12: HuatuoGPT-o1-8B reasoning for incorrect answers to prompt 2 where model consistently failed to identify gold-\nstandard reasoning (both). On the left, pediatric endocrinology expert evaluation, on the right, Huatuo-o1-8B evaluation.\n"}]}