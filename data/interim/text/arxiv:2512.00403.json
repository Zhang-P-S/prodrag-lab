{"doc_id": "arxiv:2512.00403", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.00403.pdf", "meta": {"doc_id": "arxiv:2512.00403", "source": "arxiv", "arxiv_id": "2512.00403", "title": "SelfAI: Building a Self-Training AI System with LLM Agents", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Xiaobing Yu", "Yu Zhong", "Shangqi Deng", "Ufaq Khan", "Jianghao Wu", "Xiaofeng Liu", "Imran Razzak", "Xiaojun Chang", "Yutong Xie"], "published": "2025-11-29T09:18:39Z", "updated": "2025-11-29T09:18:39Z", "summary": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.00403v1", "url_pdf": "https://arxiv.org/pdf/2512.00403.pdf", "meta_path": "data/raw/arxiv/meta/2512.00403.json", "sha256": "6160b0aaf09de3e61df28bb25dd06d2aa0de21205caea4aa6b4877461c02b81f", "status": "ok", "fetched_at": "2026-02-18T02:25:45.176167+00:00"}, "pages": [{"page": 1, "text": "SelfAI: Building a Self-Training AI System with\nLLM Agents\nXiao Wu1,2, Ting-Zhu Huang1*, Liang-Jian Deng1, Xiaobing Yu5,\nYu Zhong1, Shangqi Deng3, Ufaq Khan2, Jianghao Wu6,\nXiaofeng Liu4, Imran Razzak2, Xiaojun Chang2, Yutong Xie2*\n1University of Electronic Science and Technology of China, 2Mohamed\nbin Zayed University of Artificial Intelligence, 3Xian Jiaotong University,\n4Yale University, 5Washington University in St. Louis, 6Monash\nUniversity .\n*Corresponding author(s). E-mail(s): tingzhuhuang@126.com;\nyutong.xie@mbzuai.ac.ae;\nAbstract\nRecent work on autonomous scientific discovery has leveraged LLM-based agents\nto integrate problem specification, experiment planning, and execution into end-to-\nend systems. However, these frameworks are often confined to narrow application\ndomains, offer limited real-time interaction with researchers, and lack principled\nmechanisms for determining when to halt exploration, resulting in inefficiencies,\nreproducibility challenges, and under-utilized human expertise. To address these\ngaps, we propose SelfAI, a general multi-agent platform that combines a User\nAgent for translating high-level research objectives into standardized experimen-\ntal configurations, a Cognitive Agent powered by LLMs with optimal stopping\ncriteria to iteratively refine hyperparameter searches, and an Experiment Man-\nager responsible for orchestrating parallel, fault-tolerant training workflows across\nheterogeneous hardware while maintaining a structured knowledge base for con-\ntinuous feedback. We further introduce two novel evaluation metrics, Score and\nAUPD, to quantify discovery efficiency and search diversity. Across regression,\nNLP, computer vision, scientific computing, medical imaging, and drug discovery\nbenchmarks, SelfAI consistently achieves strong performance and reduces redun-\ndant trials compared to classical Bayesian optimization and LLM-based baselines,\nwhile enabling seamless interaction with human researchers.\n1\narXiv:2512.00403v1  [cs.LG]  29 Nov 2025\n"}, {"page": 2, "text": "1 Main\nIn recent years, large language models (LLMs) [1–3] have fundamentally reshaped the\nlandscape of AI-driven research. Advances in reasoning [4, 5], multimodal understand-\ning [6, 7], and autonomous tool use [8–13] have positioned LLMs as central components\nfor accelerating scientific workflows.\nEarly LLM-based scientific research demonstrated that LLMs can extract actionable\nscientific knowledge to guide experiments [14–17] or answer professional questions [18–\n20]. As reasoning and tool-interaction capabilities matured, research expanded to cross-\nstage planning [21–23], hypothesis and idea generation [24–27], multimodal knowledge\nintegration [28], and conducting experiments [17, 29]. Meanwhile, LLMs have driven\nthe development of tasks such as reaction prediction [30–32], material discovery [18, 33–\n35], chemical synthesis design and molecular property optimization [29, 30, 36, 37],\nand biomedicine research [17, 38–41]. Building on these advances, a new generation\nof scientific discovery systems has emerged, spanning AI scientific assistants across\nthe full spectrum of scientific workflows [16, 18, 42, 43], automated scientific discovery\nsystems (ASDSs) specialized for code generation and experiment design [29, 36, 44], and\nincreasingly capable research agents that incorporate automated debugging, iterative\nidea refinement, and scientific writing [26, 45, 46]. Along this evolutionary trajectory,\nLLM-centered ASDS have begun to integrate inference, execution, and feedback\nmechanisms into unified closed-loop pipelines, achieving varying degrees of autonomy\nacross scientific domains.\nDespite rapid progress, existing ASDS frameworks still exhibit fundamental lim-\nitations. Many systems [24, 44, 45, 47] (such as AIRA and Scientist-V2) excel at\ntranslating research intent into executable code or experimental procedures, thereby\nvalidating the “executability” of ASDS and primarily relying on final success rate (medal\nrate) or best performance as metrics. More autonomous platforms like MLGym [48]\nand MLAgentBench [49] enable system-level control, allowing agents to execute end-\nto-end benchmarks, collect results, and conduct standardized evaluations. Moreover,\nwhile recent LLM-driven optimization methods [50–53] such as LLM4EO showcase\nthat LLMs can infer evolutionary tendencies and synthesize more effective operators,\nthey operate at the level of local modification rules rather than scientific reasoning\nin trajectories. Overall, these approaches do not explicitly enhance scientific reason-\ning during experimentation, such as identifying optimal stopping points, discovering\nefficient trial trajectories, or assessing the structural quality of reasoning.\nBuilding reasoning-centric ASDS therefore requires a shift from execution-oriented\nautonomy toward cognitive autonomy: systems must not only carry out experiments\nbut also analyze, reflect, and revise their reasoning throughout the experimental\nprocess. However, current ASDS systems provide little support for these cognitive-\nlayer behaviors. The absence of mechanisms for trajectory-level reasoning assessment\nand LLM-driven decision-making limits both efficiency and accuracy, especially in\nsettings with large search spaces, stochastic outcomes, or complex interdependencies\namong hypotheses, resulting in inefficient exploration, elevated computational costs,\nand limited real-time adaptability. These limitations highlight a fundamental need\nfor ASDS architectures that incorporate reasoning-driven search, adaptive learning\nmechanisms, and structural evaluation of scientific trajectories.\n2\n"}, {"page": 3, "text": "Information \nRetrieve\nIdeas & Experiment \nSchemes\nHypothesis \nGeneration\n1\nOrchestration\n & Tracking\nResults Collection & Reporting\nStrategic  Planning\n3\nStopping \nJudgement\n2\na\nb\n•\nUse ResNet for ImageNet image classification with \nresidual blocks and shortcut connections.\n•\nResNet-18 and 34 employ basic blocks with two 3×3 \nconvolutions, while deeper versions such as ResNet-50, \n101, and 152 leverage bottleneck blocks to scale depth.\n•\nResNet supports different shortcut types: identity \nmappings for all shortcuts, projection shortcuts \nfor dimension changes, projections for all \nshortcuts, or no shortcuts at all.\n•\nRun experiments\nExperiment: Method comparisons / SOTA performance / \nAblation studies / Detailed discussions / …\n•\nDesign high-performance deep learning for Image \nclassification task on ImageNet dataset \n•\n……\nUser Agent:\nReporting\nBest Results: ….\nTrajectories: …\n \nInsights: …\n1\nHypothesis Generation\nStrategic Planning\n3\nStopping Judgement\n2\n⚫Analysis of Current Tasks\n⚫Analysis of Completed Trials\nSummarize performance trends\nEvaluate trajectories\nHighlight potential trajectories\nExploitation and Exploration: exploit promising configurations and explore uncertain regions\n⚫Test all promising configurations \n⚫Evaluate Unexplored configurations\n⚫Has the best metric improved \nsignificantly\nCognitive Agent: Hyperparameter Optimization\nExploration\nSearch Space\nReasoning Trajectories\nExploitation\nParams A: ... \nParams B: ...\n……\nMetrics: ...\nTrials\nTrial 1\nParams A: ... \nParams B: ...\n……\nMetrics: ...\nTrial 2\nParams A: ... \nParams B: ...\n……\nMetrics: ...\nTrial N\nExperiment Manager\nCognitive Agent\nScore \nAUPD \nBest-time \nStop-time \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nc\nGS (1.00)\nBS (1.00)\nLLM (1.00)\nLLM-ES (0.99)\nSelfAI (1.00)\nSolver (Best Result)\n0.0\n0.5\nDensity\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nd\nGS\n(n=417)\nBS\n(n=416)\nLLM\n(n=385)\nLLM-ES\n(n=180)\nSelfAI\n(n=131)\n0.167\n0.543\n0.212\n0.476\n0.229\n0.467\n0.245\n0.465\n0.172\n0.509\n0.318\n0.311\n0.323\n0.350\n0.294\nFig. 1: SelfAI Framework for Automated Scientific Experimentation. a,\nHolistic architecture of the multi-agent system, which transforms various experiments\nin the research process into a structured workflow. b, User intentions, comprising\nideas and experiment schemes, are transformed into structured configurations via a\npredefined prompt. These inputs are processed through successive stages: hypothesis\ngeneration, strategic planning, trial execution, and result collection. c, Performance\ndistribution across 11 tasks demonstrates the framework’s ability, when powered by\nGPT-4o-mini, to prioritize high-performance regions without sacrificing exploration.\nd, Trial counts for each solver shown in c, accompanied by quantile lines, density\ndistributions, and performance variability across the global and two evaluation regions.\nHigher values in low-performance regions promote rapid escape, while lower values in\nhigh-performance regions enable localized refinement.\n3\n"}, {"page": 4, "text": "To address these issues, we propose SelfAI, a unified multi-agent self-training\npipeline for continuous adaptability and transparent collaboration. As shown in Fig. 1,\nSelfAI forms a closed loop that integrates user intent, cognitive reasoning, and exper-\nimental orchestration. Upstream, a User Agent converts high-level objectives and\nexploratory questions into standardized experiment configurations. These configura-\ntions enable the Cognitive Agent to analyze performance metrics, conduct reasoning\nover historical outcomes, and adjust the search trajectory, driving gradual improve-\nment [54, 55]. A resource-aware Experiment Manager ensures efficient execution\nby handling resource scheduling, environment provisioning, adaptive checkpointing,\nand comprehensive experiment logging. During high-level parameter optimization, the\nCognitive Agent and Experiment Manager jointly estimate the potential benefit of\ncontinued training and apply an optimal-stopping criterion to terminate unpromising\ntrials. To evaluate scientific reasoning quality, we introduce two complementary evalu-\nation metrics, Score (Discovery Efficiency) and AUPD (Area Under the Performance\nDiversity). Score aggregates, across tasks, the normalized improvement over the search\nspace together with penalties for discovering good configurations late and for stopping\nfar from the best-found point. AUPD explicitly encodes how broadly a solver explores\nthe search space by summarizing the performance-diversity tradeoffs across the entire\ntrajectory, enabling detailed analysis of exploration behavior and stopping decisions in\nlong-term searches. Together, these metrics enable quantitative assessment of explo-\nration behavior, reasoning structure, and stopping decisions in long-term autonomous\nexperiments.\nIn summary, SelfAI provides a continuously self-training system that autonomously\niterates through cycles of intent-centric understanding, trajectory-aware reasoning,\nand adaptive strategic planning, enabling sustained performance gains with minimal\nhuman intervention. Through checkpoint tolerance, zero-code parallelization, and\ndynamic resource management, SelfAI substantially reduces redundant computation,\nimproves resource efficiency, and accelerates search convergence in long-horizon scientific\nworkflows. In addition, SelfAI integrates seamlessly with modern MLOps pipelines [56–\n58], facilitating collaborative experiment coordination, end-to-end traceability of model\nevolution, and robust privacy protection for sensitive configurations and user data.\nCollectively, these capabilities make SelfAI a practical, scalable, and operationally\nmature platform for rapid scientific discovery. The key contributions of this paper are\ngiven as follows:\n1. Autonomous Scientific Discovery System: We design a cohesive framework\nof specialized LLM agents, including the User Agent, Cognitive Agent Framework,\nand Experiment Manager, which collaboratively translate high-level research\nobjectives into iteratively refined experimental workflows, including trajectory\nanalysis, hypothesis generation, and best stopping decision-making.\n2. Novel Evaluation Metrics and Optimal Stopping: We propose two new met-\nrics, Score (discovery efficiency) and AUPD (area under the performance–diversity\ncurve), to jointly quantify the efficiency and diversity of hyperparameter\nexploration. An embedded optimal stopping criterion automatically terminates\n4\n"}, {"page": 5, "text": "Fig. 2: Scores among all solvers across different tasks to measure the best stopping\ncriterion.\nunpromising trials, effectively balancing exploration and exploitation while\nminimizing computational overhead.\n3. Comprehensive Validation of LLM-based Reasoning in Scientific Dis-\ncovery: We evaluate SelfAI across diverse tasks, including regression, sentiment\nanalysis, computer vision, medical imaging, and drug discovery. The results show\nthat SelfAI effectively improves upon excellent baseline models in both results\nexploration and performance.\n2 Performance of SelfAI on Benchmark Data\nScientific discovery is driven by diverse physical environments and user intents. To\neffectively reason across diverse scientific discovery tasks, our approach integrates\nmultiple LLM agents and a suite of tools: User Agent, Cognitive Framework, and\nExperiment Management, addressing the complex reasoning and planning problem of\nscientific discovery. In our experiments, we evaluate the SelfAI across 6 domains and\n12 tasks. The compared methods contain different suites of LLMs, such as OpenAI-\no3 [1], Llama3.3 [59], Qwen2.5 [2], and DeepSeek-R1 [3]. We used grid search and the\nTree Parzen Estimator (TPE) optimizer [60, 61] (referred to as BS), which applies a\nmodified Bayesian inference method using a spiral search. All agents are tested on four\nNvidia A100 GPUs. A public implementation of SelfAI will be available on the project\nwebsite. We also provide reasoning processes for reproducibility.\n5\n"}, {"page": 6, "text": "Fig. 3: Diverse Metrics (AUPD) among all solvers across different tasks to evaluate\ntrajectory diversity.\nMetrics. We introduce four metrics: Score, AUPD, tbest, tstop. Score measures a\ncomprehensive search performance that combines optimality and stopping efficiency.\nAUPD quantifies the diversity of explored high-quality solutions.tbest represents the\nnormalized time to first find the best result, while tstop indicates when the search\nterminates. Specifically, the reasoning process that finds the best result but delays\nstopping exhibits low tbest and high tstop. Conversely, late discovery with immediate\nstopping yields high values for both. An ideal solver should minimize both times.\nIn addition, a high AUPD reflects broad exploration, whereas a low value indicates\nfocused search, which may also result from rapid convergence near the starting point.\nThus, the most valuable optimization strategy achieves a high Score with a low AUPD,\nenabling fast discovery of optimal configurations with minimal wasted exploration.\nMore details are illustrated in the Supplementary Section A.4.\nBenchmarks. We collect benchmarks composed of multiple tasks of 6 primary\ncategories, designed to simulate challenging scenarios in scientific discovery. The\nbenchmark requires the solver to engage in scientific reasoning by analyzing experiments,\ninterpreting observations, and adapting strategies, uncovering novel patterns, functional\nrelationships, or optimal experimental configurations. Compared with hyperparameter\nsearch, these processes reflect the inductive reasoning essential to real-world scientific\ninquiry. Therefore, this benchmark assesses the model’s reasoning ability, not just its\nnumerical optimization capabilities. All benchmark data is collected through actual\nexperimental runs, and some results are reported in published literature, spanning\nboth discrete and continuous hyperparameter search spaces, from low-dimensional to\nhigh-dimensional settings (see Appendix B for more details).\n6\n"}, {"page": 7, "text": "Existing methods [24, 45, 49] explore final performance or medal rates using\nreward functions and prompts specifically designed for innovative idea generation,\ncode generation, and pass@K strategies. In the line of LLM-driven optimization,\nLLM4EO [50] uses LLMs exclusively to synthesize and evolve evolutionary operators\nthat determine which parts of a candidate solution should be modified during the\nsearch. The LLM is used to generate and update operators to shape the local search\nbehavior of the evolutionary algorithm. These implementations do not focus on scientific\nreasoning across trials during the exploration process, making direct comparisons with\nour benchmark infeasible. Consequently, we used the readily accessible GPT4-o3-mini\nmodel and compared SelfAI with LLM and LLM-ES methods on 11 small/medium-\nsized tasks across different domains in Fig. 1c. The LLM and LLM-ES methods\nare fine-tuned by using the structured prompts in Supplementary Section D. The\nLLM solver uses a search prompt that recommends new trials based on prior results,\nwhile LLM-ES augments this with an early-stopping module that decides whether\nto terminate exploration. Fig. 1c presents that our SelfAI outperforms LLM and\nLLM-ES in all four metrics, validating SelfAI’s effectiveness in integrating domain\nknowledge to accelerate scientific discovery. Furthermore, we focus on the reasoning\ncapability of the original LLM model because it is difficult to be fair when comparing\ndifferent methods. Fig. 1d shows that SelfAI promptly focuses on exploiting higher-\nperformance regions while maintaining a balanced distribution for effective exploration.\nFigs. 2 and 3 summarize the benchmark performance based on these two metrics. The\nbenchmarks show that SelfAI consistently achieves excellent results across various\ndomains and tasks for different LLM models. Detailed comparisons covering Score,\nAUPD, Best-Time, Stop-Time, and Hit Rate are provided in Supplementary Table B2.\nFailure cases. Several key failure modes of LLM-based solvers are shown in\nSupplementary Section B.1. First, the hit rate for identifying the best result varies\nsubstantially across models, with some solvers (particularly DeepSeek-R1 series) failing\nto reach the global optimum due to premature stopping. Second, limited context\nwindows cause incomplete reasoning over long trajectories, preventing solvers from\nfully incorporating earlier experimental signals. Third, computational fragility leads to\nnon-monotonic performance under seemingly minor perturbations in reasoning strategy.\nThese findings indicate that while SelfAI improves overall efficiency, reasoning-driven\nexploration remains sensitive to solver stability and stopping behavior.\n2.1 Machine Learning\nBoston house pricing prediction. We evaluate SelfAI on the Boston hous-\ning price prediction task [62] using a random forest regression model. A total of\n162 trials were conducted across a search space defined by five hyperparameters:\ni.e., n-estimators = [100, 200, 300], max-depth = [None, 10, 20], min-samples-split =\n[2, 5, 10], min-samples-leaf=[1,2,5] and max-features = [“sqrt”,“log2”]). As shown in\nFigs. 2 and 3 (and Supplementary Table C3), GPT-4o-mini achieves by far the highest\nScore (0.9811) and the 1st ranking, indicating outstanding optimization efficiency.\nWithin the DeepSeek-R1 family, the 32B and 7B variants secure 2nd and 3rd places\nrespectively, demonstrating the best balance between rapid convergence (low Best-Time)\n7\n"}, {"page": 8, "text": "and reasonable search diversity among open-source models. In contrast, DeepSeek-R1-\n70B and Llama3.3-70B exhibited consistent exploration but lacked effective stopping\ncriteria, leading to longer convergence times. Notably, nearly all models achieved an\nidentical and high Best Result ( 0.841), indicating that while most can find a correct\nsolution, they differ substantially in optimization efficiency and reliability.\nSentiment analysis. We perform experiments for sentiment analysis that focus on\nidentifying opinions, emotions, and attitudes expressed in text. All experiments are\nfound in [63] with standard experimental settings, where the pre-trained Word2Vec\nembeddings [64] are used as input features. An LSTM network is then employed to\nmodel sentence sequences, converting them into a multi-class classification problem.\nThis setup serves to evaluate the agent’s reasoning and optimization capabilities in\ntextual domains. As shown in Supplementary Table C4, GPT-4o-mini outperforms\nall other LLMs, achieving the highest Score (0.8824) by discovering the optimal\nconfiguration extremely early while maintaining reasonable search diversity. DeepSeek-\nR1 series and Qwen2.5 series exhibit significantly lower Scores, with many failing\nto converge quickly or occasionally missing the global optimum. The largest frontier\nmodel GPT-4o also underperforms markedly (Score 0.2745, rank 11), suggesting that\nscale alone is insufficient for effective iterative hyperparameter reasoning in recurrent\nneural architectures.\n2.2 Scientific Computing for Image Completion\nFor scientific computing fields, we selected the tensor decomposition method [65].\nThe method is an important tool for high-dimensional data analysis and is crucial in\napplications such as data compression [66], computational acceleration [67], and multi-\nmodal data fusion [28]. All LLMs are provided with identical mathematical knowledge\nabout TW decomposition generated by GPT-4o. As shown in Supplementary Table C5,\nQwen2.5 (7b and 14b) and GPT4-o3-mini perform well and rank highly. Qwen2.5-72b,\non the other hand, may overly rely on general reasoning capabilities and fail to balance\nmathematical knowledge and reasoning capabilities. In the DeepSeek series, although\nthe DeepSeek-R1-7b model can’t find the optimal solution, its search strategy was\nacceptable with a moderate Score, demonstrating its ability to perform mathematical\nreasoning. DeepSeek-R1-70b and GPT-4o, despite excellent search diversity and early\nstopping, received a mediocre score, reflecting that their exploration strategy was not\nwell aligned with the optimization goal of tensor decomposition, possibly failing to\nfind the optimal performance between data compression and computational efficiency.\n2.3 Computer Vision\nA. SIREN We employed SIREN (Sinusoidal Representation Networks) [68] to eval-\nuate our framework on image segmentation and denoising tasks. Leveraging sine\nactivations and coordinate-based inputs, SIREN excels at representing high-frequency\nsignals through continuous implicit representations, making it widely used in scientific\ncomputing and physics-based problems. However, its performance is highly sensitive\nto hyperparameters (e.g., learning rate and regularization strength, etc.), requiring\ncareful tuning per dataset. The unsupervised nature of SIREN further increases the\n8\n"}, {"page": 9, "text": "risk of training instability or divergence with improper settings, making it a strong\ntest case for SelfAI’s hyperparameter optimization capability.\nFig. 4 illustrates hyperparameter search trajectories for image segmentation using\nSIREN (see also Supplementary Figs. B4, B5 and B6), where the surface, smoothed\nfrom the original steep, multi-peak data, reveals distinct search behaviors. Given the\nsame three initial points, the tree-structured Bayesian optimizer (BS) follows a spiral\ntrajectory that underutilizes promising starting regions and fails to explore broadly,\noften converging to local minima. In contrast, LLM-based optimizers infer trends from\nevaluated points, incorporate causal reasoning, and explore more broadly to efficiently\nlocate the global optimum. They also monitor progress and halt when improvements\nplateau, a capability absent in traditional methods.\nQuantitative results (Supplementary Tables C6–C7) reveal distinct task-dependent\nperformance profiles. In segmentation, DeepSeek-R1-7B achieves the highest Score\n(0.693) and ranks first, followed by Qwen2.5-7B and GPT-4-o3-mini. Larger models such\nas Qwen2.5-72B and Llama-3.3-70B rank only mid-range, indicating that scale alone\ndoes not ensure effective optimization. In denoising, the landscape shifts: Qwen2.5-72B\ndelivers the best performance (Score 0.761), while DeepSeek-R1-14B and DeepSeek-R1-\n32B jointly occupy second place. Notably, models that perform well in segmentation\ntasks do not necessarily perform well in denoising tasks, suggesting that the effectiveness\nof the solver largely depends on the degree of matching between its inference strategy\nand the task.\nB. Image Classification We also benchmark two commonly used supervised learn-\ning methods in computer vision: Masked Autoencoder (MAE) [69] and ResNet [70].\nThese represent fundamentally different learning paradigms where hyperparameter\noptimization plays a crucial role in achieving the latest performance.\nMask Autoencoder (MAE) MAE introduces two key hyperparameters: training\nstrategies (linear detection vs. fine-tuning) and masking rate, which are explored over a\nrange: [0.10, 0.90] with an interval of 0.1. The masking rate directly affects the difficulty\nof the reconstruction task and the quality of the learning representation, while the\ntraining strategy determines how to adapt to the downstream task. In our experiments\n(see Supplementary Table C8 for more details), GPT-4o-mini achieved explicit per-\nformance with efficiency, demonstrating the extraordinary ability to identify the best\nmask configuration and training strategies. Qwen2.5-7B achieves early convergence and\ndisciplined stopping. The DeepSeek-R1 series shows consistent scaling behavior: perfor-\nmance improves from 7B to 14B to 32B, but slightly drops at 70B. In traditional solvers,\nGS and BS lag significantly behind LLM-based solvers, highlighting the advantages\nof intelligent hyperparameter optimization methods. Apart from Qwen2.5-14b and\nDeepSeek-R1-7b, most solvers can find the optimal solution, but significant differences\nexist among the various methods in terms of search efficiency and convergence speed.\nResNet Family On the ImageNet ResNet hyperparameter benchmark, LLMs in\nthe 7B–32B range demonstrate superior optimization efficacy. The top-performing\nsolvers, such as Qwen2.5-14B (1st), DeepSeek-R1-32B (2nd), and Qwen2.5-7B with\nDeepSeek-R1-14B (tied 3rd), consistently identify the optimal ResNet configuration\nwithin one or two trials, halting immediately with near-perfect sample efficiency. In\ncontrast, larger variants from the same model families drop to middle or lower rankings,\n9\n"}, {"page": 10, "text": "consuming over half the search budget with notably higher exploration overhead.\nWhile GPT-series models, as large-scale counterparts, still achieve competitive results,\nthe 7B–32B class exhibits a clear advantage in balancing accuracy and efficiency. For\nthe architectural search on ImageNet, we explored key dimensions such as depth and\nbottleneck design. Qwen2.5-14B attains theoretical peak performance, with its 7B and\n32B versions completing further exploration efficiently. These results reflect an ability\nto identify ideal network configurations with minimal wasted exploration.\nLCBench We evaluate SelfAI on the standard AutoML benchmark [71] using Bayesian\noptimization over 2000 rounds of hyperparameter search, covering critical parameters\nincluding learning rate, batch size, network depth, and dropout rate. As shown in\nSupplementary Table C10, DeepSeek-R1-70B, Qwen2.5-14B, and Llama3.3-70B secure\nthe top three positions in the ranking, with DeepSeek-R1-70B attaining the highest\noverall score and the lowest search diversity. Notably, leading models in the 7B–32B\nparameter range efficiently identify near-optimal hyperparameter configurations by\nevaluating only a minimal set of candidate solutions, achieving an order-of-magnitude\nimprovement in stop-time compared to classical methods. These findings demonstrate\ndiminishing returns with scaling, as larger models within the same series (e.g., Qwen2.5-\n72B) typically underperform comparable models of medium size, while the 7B model\ncan achieve robust performance comparable to models several times its size.\n2.4 Medical Image Analysis\nIn medical image analysis, nnU-Net [72] is a landmark framework known for its\nstrong generalization and automated design. Despite its adaptive network structure,\npreprocessing, and training strategies for various segmentation tasks, the exploration\nof novel architectures persists. To address this, nnU-Net-Revisited [73] establishes\na comprehensive benchmark including 19 mainstream models (CNN-based [74–76],\nTransformer-based [77, 78], and Mamba-based [79, 80]), offering a solid basis for fair\nevaluation. Results are shown in Figs. 2-3 and Supplementary Tables C11- C12. On\nthe BTCV dataset [81], GPT4-o3-mini achieves the highest Score (0.6875) and Rank\n1, demonstrating strong optimization efficacy, while Qwen2.5-7B secures second place,\noutperforming all larger variants in its family. In nnU-Net hyperparameter tuning\nfor the BraTS dataset [82], Qwen2.5-14B attains the top ranking (Score 0.4333),\nfollowed closely by GPT4-o3-mini, with Qwen2.5-32B and DeepSeek-R1-70B tying for\nthird. Notably, the 7B and 72B models outperformed similar models of medium size\n(14B–32B), indicating that larger model size does not guarantee better optimization.\n2.5 Imbalanced Node Classification\nGraph neural networks (GNNs), as powerful tools for processing relational data,\nare playing an increasingly critical role in numerous cutting-edge scientific fields. In\nbiomedicine, GNNs have become a core component of protein structure prediction\nmodels such as AlphaFold3 [39], enabling precise modeling of spatial interactions\nbetween amino acid residues and ushering in a new era in structural biology. In drug\ndiscovery, they are widely used for molecular property prediction [39, 83, 84], compound-\ntarget interaction identification [85, 86], and novel drug generation [87]. Similarly,\n10\n"}, {"page": 11, "text": "log10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nBS\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nLlama3.3-70b\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nGPT4-o3-mini\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nGPT4-o3\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nRandom Path\nOptimization Path\nInitial Point\nTrial Point\nTrial After Best\nBest Point\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nQwen2.5-7b\n1\n2\n3\n4\n5\n6\n7\n8\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nQwen2.5-14b\n1\n2\n3\n4\n5\n6\n7\n89\n10\n11\n1213\n14\n15\n16\n17\n18\n19\n20\n21\n22\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nQwen2.5-32b\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nQwen2.5-72b\n1\n2\n3\n4\n5\n6\n7\n8\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nDeepSeek-r1-7b\n1\n2\n3\n4\n5\n6\n7\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nDeepSeek-r1-14b\n1\n2\n3\n4 5\n6\n7\n8\n9\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nDeepSeek-r1-32b\n1\n2\n3\n4\n5\n6\n7\n8\nlog10 (lr)\n1\n2\n3\n4\nlog10 (k2)\n1\n2\n3\n4\nPSNR (dB)\n5\n10\n15\nDeepSeek-r1-70b\n1\n2\n3\n4\n5 6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n5\n10\n15\n20\n25\nPSNR (dB)\nFig. 4: Illustration of the optimized trajectory for the SIREN method for image\nsegmentation. Green points are suggested points before reaching the optimal points. Red\npoints are redundant suggestions when reaching out to the optimal points and failing\nto stop trials. The ⋆is the optimal point. We show the serialization recommendations\nprovided by LLM through the labeled numbers.\nGNNs demonstrate exceptional performance in engineering domains, including traffic\nprediction [88, 89], cybersecurity, and chip design [90, 91]. Given this broad utility\nand the sensitivity of GNN performance to hyperparameter settings, we evaluate\nGraphSAGE on the imbalanced Cora benchmark [92] under the setup of [93].\n11\n"}, {"page": 12, "text": "Figs. 2 and 3 represent that the GPT4-o3 series delivered near-optimal results,\nachieving the fastest discovery of optimal solutions and efficient stopping decision\nwhile maintaining high search diversity, underscoring its robust optimization capability\nin scientific computing scenarios. In contrast, Qwen2.5 models, (except 72b), easily\ndelivered consistent but middling results across scales, while larger models generally\nexhibit late convergence and higher unnecessary exploration, reflecting potential\ninherent limitations in understanding the structural properties and optimization\nrequirements of specialized domains such as protein interaction networks and molecular\ngraph structures (see Supplementary Table C13 for more results). These results indicate\nthat SelfAI can effectively address challenges in GNN hyperparameter optimization.\nThe framework’s strong performance on graph learning tasks suggests its potential as\na general-purpose optimizer for scientific domains, including protein engineering and\ndrug development.\n2.6 Drug Discovery\nIn drug discovery, accurate bioactivity prediction is vital for early-stage virtual screening\nand compound prioritization, enabling shorter development cycles and lower preclinical\ncosts. Following the evaluation practices and modeling standards summarized by\nKorotcov et al. [94], this study employs the Chagas EP20 dataset [95], which measures\nthe activity of compounds against Trypanosoma cruzi, a neglected but medically\nsignificant target in tropical disease research. The core challenge lies in learning an\neffective mapping from molecular representations, such as SMILES sequences [96] or\ngraph-based encodings [97] to experimentally measured bioactivity [98]. We adopt\nSelfAI to adaptively refine performance, assess search trajectories, and determine\nprincipled stopping points for this task.\nExperimental results (see Supplementary Table C14) reveal distinct performance\npatterns among solvers in hyperparameter optimization. Qwen2.5-7B and 14B secure\nfirst and second place, respectively, both achieving high Scores with minimal search\ndiversity. GPT-4o ranks third, while DeepSeek-R1-7B and 70B tie for fourth. A key\nfinding is the clear performance drop at larger scales; Qwen2.5-32B, Qwen2.5-72B, and\nDeepSeek-R1-32B rank in the bottom half, demonstrating that increased parameters\ndo not guarantee better optimization. In contrast, traditional optimization methods\n(such as GS and BS) ranked poorly in this task, underscoring the clear advantage\nof large language model-based intelligent optimizers in bioactivity prediction. These\nresults collectively demonstrate that in high-dimensional, noisy biochemical scenarios,\nlanguage model-driven optimizers offer efficient convergence and a superior balance\nbetween exploration and exploitation.\nFinally, the ranking heatmap (Fig. B3) and average performance analysis (Supple-\nmentary Table B2) reveal several consistent trends. GPT-4o-mini emerges as the most\nconsistently effective solver, strong inductive reasoning, rapid adaptation to emerging\nevidence, and stable progression across diverse scientific domains. Mid-sized models\nsuch as Qwen2.5-7B and Qwen2.5-14B display competitive and robust performance\nrelative to their scale, often exhibiting disciplined evidence-based refinement during\nexploration. In contrast, larger models (Qwen2.5-72B and DeepSeek-R1-70B) exhibit\n12\n"}, {"page": 13, "text": "higher variance: they perform well on specific tasks but frequently show delayed break-\nthroughs, unstable trajectories, and inconsistent cumulative progress. These patterns\nsuggest that larger models tend to spend more time exploring alternative possibilities\nrather than leveraging early evidence, resulting in delayed commitment and reduced\nadaptability during the search process.\n3 Discussion\nThe emergence of the autonomous scientific discovery system (ASDS) represents\na paradigm shift in scientific research, with the potential to accelerate discovery\nby delegating structured experimental processes to artificial intelligence. Yet most\nexisting agentic ML frameworks and benchmark-driven ASDS, such as MLE-bench [99],\nemphasize an agent’s ability to generate and execute candidate solutions, offering\nlimited support for adaptive trajectory management, such as reasoning about when\nexploration should continue, how search strategies should evolve, or why certain regions\nof the space warrant further investigation. SelfAI addresses this gap by integrating\nstructured intent interpretation, multi-agent reasoning, and principled stopping criteria\ninto a general reasoning framework. Across diverse domains, this design enables the\nsystem to allocate computational effort where it is most informative, rather than\nexhaustively enumerating the search space.\nUnder this goal, SelfAI is meticulously designed as a general-purpose platform capa-\nble of performing high-level reasoning across diverse tasks, thereby accelerating the\nexperimental search process. Evaluations across a comprehensive benchmark (12 tasks\nacross 6 different domains) uncover that SelfAI strategically allocates limited compu-\ntational resources to the most promising performance spaces or research directions\nand terminates unproductive trajectories, exhibiting human-like cognitive flexibility in\ndynamically adjusting the search trajectory. This optimal stopping behavior effectively\nalleviates the most persistent inefficiencies in current ASDS.\nThe newly proposed Score and AUPD metrics are crucial for quantifying this advan-\ntage, revealing SelfAI’s remarkable balance between rapidly finding discovery efficiency\n(Score) and maintaining healthy exploration diversity (AUPD). These metrics further\nilluminate the limitations of traditional methods like grid search and Bayesian opti-\nmization, which often lack the adaptive reasoning required to escape local optima or\nterminate unfruitful trials. Our findings also suggest that in scientific domains, effec-\ntive scientific discovery demands specialized reasoning capabilities beyond mere model\nscale. Although large-scale models like GPT-4o and DeepSeek-R1-70B occasionally\nexcel in broad exploration, their performance is frequently surpassed by more compact,\npurpose-driven models such as Qwen2.5-7B and DeepSeek-R1-7B, which demonstrate\nsuperior stopping efficiency and search consistency. This indicates that for navigating\ncomplex non-convex optimization environments, the structured reasoning and hypoth-\nesis generation implemented in SelfAI’s cognitive agent may play a more decisive role\nthan model scale alone. The framework demonstrates that purpose-driven architectural\ndesign can effectively harness the capabilities of moderately-sized models to deliver\nrobust scientific discovery performance.\n13\n"}, {"page": 14, "text": "The implications of this framework extend far beyond accelerated experimental\nsearch and improved benchmark performance. First, the architecture of SelfAI embodies\na practical blueprint for human-AI symbiosis in science. The User Agent acts as a\nseamless translator, allowing human researchers to interact with the system at the\nlevel of scientific intent and high-level questions, rather than through low-level code or\nconfiguration files. This design formally integrates human expertise as a guiding input,\ncreating a collaborative loop where the AI manages the scale and complexity of the\nsearch while the human provides strategic direction and domain insight. This addresses\na critical shortcoming of \"black-box\" automation by fostering reproducibility and\ntrust, as every action of the AI is traceable to a human-defined objective. Furthermore,\nthis closed-loop process allows researchers to shift from using AI for scientific research\nto exploring how to conduct scientific research optimally and to investigate efficient\nsearch strategies and problem-related endpoints under controlled conditions.\nSelfAI has the potential to evolve from a tool into a collaborative research partner,\nenhancing the cognitive and experimental capabilities of human scientists. This evolu-\ntion would be driven by several key advancements. For instance, the User Agent could\nbe enhanced to incorporate autonomous code generation, thereby broadening the range\nof experimental configurations without requiring explicit user pre-definition. Further-\nmore, while the current iteration of SelfAI relies on the static knowledge embedded\nwithin its underlying LLM (Large Language Model), future versions could leverage\nadvanced memory management and RAG approaches to alleviate context length limi-\ntations and dynamically integrate a richer, more up-to-date body of domain-specific\nknowledge. Collectively, these envisioned capabilities underscore SelfAI’s foundational\nrole in shaping a more adaptive, collaborative, and intelligent paradigm for autonomous\nscientific discovery.\n4 The Design of SelfAI for Scientific Discovery\nSelfAI is a general scientific discovery system designed to automate and accelerate the\nend-to-end scientific discovery process. Its core function is to transform users’ high-\nlevel research intentions (hyperparameter optimization, novel algorithm design, model\narchitecture validation, or ablation studies) into structured, executable workflows. The\nsystem operates in an integrated loop, comprising three core agents: a user agent, a\ncognitive agent, and an experiment manager, which together constitute the overall\nexperimental workflow in Fig. 1.\n4.1 User Agent\nThe User Agent, as the user interface of SelfAI, is designed to be interactive, user-\ncontrollable, and adaptable to evolving research needs. It interprets high-level scientific\nintent and translates natural-language objectives into structured, machine-readable\nexperimental configurations. Through iterative clarification, it helps researchers specify\nobjectives, constraints, and search spaces, thereby establishing the experimental envi-\nronment in which the Cognitive Agent operates. For example, requests such as “achieve\nstate-of-the-art accuracy on CIFAR-10 using a CNN” or “identify the most influential\n14\n"}, {"page": 15, "text": "method for image classification” are reformulated into precise experimental specifi-\ncations (see Supplementary Material A.1). Unlike static intent-generation prompts,\nthe User Agent supports continuous intervention: researchers may pause execution,\ninspect reasoning traces, adjust constraints, or redesign experimental protocols at any\nstage without restarting the workflow.\n4.2 Cognitive Agent\nSelfAI’s cognitive agent is central to accelerating scientific discovery. After receiving\nstructured experimental configurations from the user agent and accumulated exper-\nimental history from the experiment manager, the agent synthesizes all available\ninformation, analyzes experimental trajectories, evaluates observed performance trends,\nand identifies promising regions in the search space. This achieves a delicate balance\nbetween developing these high-potential regions and exploring areas of greater uncer-\ntainty, ultimately deriving the next optimal search strategy. A significant feature of\nour cognitive agent is its introduction of principled stopping criteria, addressing a\nkey limitation of traditional AI systems, which often continue exploring under con-\nditions of diminishing returns. By conducting evidence-based evaluations of research\nprogress, the agent can determine when further experiments on a particular hypothesis\nare unlikely to yield significant new insights, thereby reallocating resources to more\npromising research directions (see Supplementary Material A.2 for more details). This\ncapability improves resource utilization efficiency in the automated discovery process.\n4.3 Experiment Manager\nThe Experiment Manager executes the experimental plans proposed by the Cognitive\nAgent, each of which has already been structured as executable training jobs, specifying\nmodel architectures, datasets, and hyperparameters. For every job, the manager\ninitializes these specified configurations, launches the training and evaluation pipeline,\nand records all associated outputs. It tracks runtime status, handles execution failures,\nand captures performance metrics as well as relevant logs and metadata. These records\nconstitute a continuously updated structured knowledge base, providing a reliable\nfoundation for the cognitive agent to analyze search progress, identify promising regions\nof the search space, and decide when to stop exploration. Comprehensive execution\nprocedures and implementation details are provided in Supplementary Material A.3.\nBeyond executing trials, the Experiment Manager also manages the computational\nenvironment in which scientific experiments are carried out. Through the Experiment\nManager, resources are allocated reliably, execution workloads are scheduled efficiently,\nand checkpointing and recovery mechanisms are coordinated seamlessly. Anomalies\nor failed runs are surfaced to the Cognitive Agent as feedback for complex adaptive\nreasoning. As a result, the overall research process is conducted within an efficient,\nreliable, and reasoning-driven environment.\n15\n"}, {"page": 16, "text": "Table 1: Comparison of SelfAI with related AI research frameworks and benchmarks\nacross system-level, agent-specific, and task-specific capabilities.\nFunctions\nOurs\nCode LLaMA [53]\nMLGym [48]\nAI Co-Scientist [45]\nAIRA [24]\nMLAgentBench [49]\nOptuna [61]\nInteractive Research\n✓\n✗\n✓\n✗\n✗\n✓\n✓\nFlexible Artifacts\n✓\n✗\n✓\n✓\n✗\n✓\n✗\nPrivacy and Security\n✓\n✗\n✗\n✗\n✗\n✗\n✓\nTrajectory Analysis\n✓\n✗\n✗\n✗\n✗\n✗\n✗\nHypothesis Generation\n✓\n✗\n✓\n✓\n✓\n✗\n✗\nStrategic Planning\n✓\n✗\n✓\n✗\n✗\n✗\n✗\nCausal Inference\n✓\n✗\n✓\n✗\n✗\n✗\n✗\nAdaptive Learning\n✓\n✓\n✓\n✗\n✗\n✗\n✗\nJob Scheduling\n✓\n✗\n✓\n✗\n✗\n✗\n✓\nCheckpoint Management\n✓\n✗\n✗\n✗\n✗\n✗\n✗\nExperiment Tracking\n✓\n✗\n✗\n✗\n✗\n✗\n✓(*)\nZero-Code Parallelization\n✓\n✗\n✗\n✗\n✗\n✗\n✗\nHypothesis Optimization\n✓\n✓(*)\n✗\n✗\n✗\n✗\n✓\nSelf-Evaluation\n✓\n✗\n✗\n✗\n✗\n✗\n✗\nBenchmark Suite\n✓\n✗\n✗\n✗\n✗\n✓\n✗\nNote: The comparison is structured in four blocks: (1) System-level functions, (2) Cognitive\nfunctions primarily handled by the Cognitive Agent, (3) Execution functions managed by\nthe Experiment Manager, and (4) Performance in optimization tasks. ✓(*) denotes basic\nsupport.\n4.4 Comparison of AI research systems\nWe contextualize SelfAI’s capabilities within the current landscape of AI-assisted\nresearch tools. Table 1 presents a systematic comparison against representative frame-\nworks across multiple dimensions of scientific functionality. Existing ASDSs often\nspecialize in code- or idea-level hypothesis generation under fully autonomous pipelines\nthat primarily optimize medal rates within 24 hours. In contrast, SelfAI integrates\nintent interpretation, iterative scientific reasoning, and adaptive trajectory optimiza-\ntion into a unified discovery workflow. These capabilities allow the system to refine\nstrategic planning based on accumulated experimental evidence, rather than executing\npredetermined and static generalization procedures. Overall, these advantages posi-\ntion SelfAI as a general-purpose framework capable of supporting efficient, reliable,\nand reasoning-driven scientific workflows that extend the scope of existing agent or\nhyperparameter optimization benchmarks.\n5 Data availability\nAll datasets used in this study are available for download at https://github.com/\nXiaoXiao-Woo/SelfAI.\n6 Code availability\nThe SelfAI repository is available as Supplementary Software. Updated versions can\nbe found at https://github.com/XiaoXiao-Woo/SelfAI.\n16\n"}, {"page": 17, "text": "Appendix A\nLLM Agents in SelfAI System for\nScientific Discovery\nThe SelfAI framework supports the complete lifecycle of AI systems, covering design,\ntraining, evaluation, and deployment. It integrates three core agents: the User Agent,\nthe Cognitive Agent, and the Experiment Manager, and provides an integrated toolkit\nfor autonomous model training, experiment orchestration, and advanced reasoning,\nenabling a scalable and adaptive workflow.\nA.1\nUser Agent: Idea Interaction and Experiment\nConfiguration\nIn autonomous research systems, the User Agent is typically powered by a general-\npurpose LLM and acts as the primary interface between human researchers and the\nsystem. These systems build an “idea-to-experiment” autonomous research process\nwhere user access to detailed experimental workflows is limited. Such approaches\ncan streamline research execution but may also restrict researchers’ flexibility in\niteratively shaping and adapting experiments. By contrast, our User Agent unifies\nidea generation and experiment configuration into a continuous, interactive process,\nempowering researchers to refine initial concepts, explore alternative approaches, and\nadjust experimental parameters at any stage.\nDuring the idea interaction process, the User Agent understands the task back-\nground, dissects method details, and defines precise experimental objectives across\ndiverse scientific domains. At this stage, the User Agent can help users pinpoint inno-\nvative research directions and generate field-relevant ideas. Once an idea emerges, the\nconfiguration interaction phase leverages prompt templates to clarify user objectives\nand formulate strategic experimental plans, including initial hypotheses and search\nconfigurations. This process converts high-level manual queries into standardized and\nreproducible experimental configurations, as illustrated in the following examples. We\ninsert the specified template into the prompt and trigger the prompt to fill out the\nspecified configuration file.\nConsequently, the User Agent can transform ideas into system and user information.\nThe system information provides the Cognitive Agent with essential content, including\nrole-playing instructions and task-specific knowledge (such as task benchmarking,\nmethod evaluation, comparative analysis, ablation studies, and experimental designs).\nThis information equips the Cognitive Agent to interpret high-level research objectives\naccurately and to execute relevant reasoning and planning steps effectively.\nIn parallel, the user information focuses on the practical aspects of experimental\nplanning by supplying detailed specifications and keywords, such as search space\ndefinitions, the number of trials, optimization criteria, and resource constraints, which\nare critical for configuring and running experiments. Together, these two types of\ninformation enable seamless coordination between conceptual ideation and concrete\nexecution, supporting a workflow in which researchers can iteratively refine their\nexperiments and monitor progress in real time.\n17\n"}, {"page": 18, "text": "User Agent Prompt for Configuration Interaction\nSystem: ...\nUser:\n{{SUMMARIZED_CONTENT}}\nPlease fill out the content following the YAML format:\n1\n# Template\nfor\nConfiguration\nInteraction\n2\n# to convert\nIdea\nInteraction\ninto YAML\nformat\n3\n- role: system\n4\ncontent:\n5\nmodel: $modelName\n6\ndescription: You are a $role\nspecializing in\n7\nstudying\n$taskName. Please\nprovide\n8\nprofessional\nand\ndetailed\nanswers.\n9\ntask: $taskName\n10\nbasic_idea: $basic_idea\n11\nsearch_space:\n12\n$hyper_name1: []\n13\n$hyper_name2: []\n14\n...\n15\nlink: $paperLink\n16\ninstrustion: Complete\ninstructions\nunder\nlimited\ntrials.\n17\n- role: user\n18\ncontent:\n19\nmax_trials: $maxTrials\n20\ntrials: []\nA.2\nCognitive Agent\nIn our SelfAI framework, the Cognitive Agent continuously ingests and adapts domain\nknowledge to tackle complex reasoning problems during experiments, much like a\nhuman researcher. At each stage, the agent first conducts a comprehensive analysis\nof the task and completed experiments to clarify the purpose and key information\nof the task and generates preliminary hypotheses. Then, it evaluates whether these\nhypotheses are worth trying through stop judgment to inform strategic planning. It\nthen generates specific schemes to probe unexplored regions of the solution space,\ncreating a non-Markovian chain of evolving plans grounded in prior states.\nA.2.1\nReasoning for Scientific Discovery\nLLM agents exhibit remarkable reasoning capabilities, making them well-suited for\nscientific discovery. As illustrated in Fig. A1, we present an advanced reasoning\nframework for generating optimal stopping solutions, an approach that can be extended\nto various scientific discovery tasks.\n18\n"}, {"page": 19, "text": "<Objective> The current task involves image classification using the ResNet architecture on the ImageNet dataset ......\n<Keys> The ResNet family includes various architectures. Shallower models like ResNet-18 and ResNet-34 ...... deeper \nmodels such as ResNet-50, 101, and 152 ......\nCompleted Trials:     {{TRIALS}}\nStep 3: Promising Hyperparameter \nCombinations\n•\nTop Performer: ResNet-101 due to depth, \nbottleneck blocks, and projection shortcuts.\n•\nFuture Exploration: Plain-34's performance \nmay not yield better results. Configurations that \ncombine deeper architectures with effective \nblock designs and shortcut types warrant further \ninvestigation.\nStep 2: Performance Trends Evaluation\n1.Depth: ResNet-101, Plain-34 architecture suggesting that deeper \nnetworks can capture more complex features and lead to improved \naccuracy.\n2.Block Type: The bottleneck block in ResNet-101 may facilitate \nbetter feature extraction and optimization in ResNet-18 and Plain-34. \n3.Shortcut Type:  B_proj_dimchange enhances performance \ncompared to identity shortcuts or no shortcuts at all, indicating that \ndimensional adjustments in deeper networks can be beneficial.\nStep 1: Performance Metrics Summary\nResNet-18 A: 27.88%,  \nPlain-34: 28.54%,  \nResNet-101 (Bottleneck): 21.75%,\n…\nParameters: \n•\nArchitecture: ResNet-50\n•\nDepth Layers: 50\n•\nBlock Type: Bottleneck\n•\nShortcut Type: Projection (Dim Change)\nReasoning: \nThe ResNet-50 architecture offers \na strong balance between depth \nand computational efficiency. \nWhile deeper models like \nResNet-101 show improved \nJSON Format\nTask 1: Analysis of Current Task\nHypothesis \nGeneration\nStopping \nJudgment\nTask 2: Analysis of Completed Trials\nStrategic Planning\n[\n{\n\"number\": 6,\n\"params\": {\n\"architecture\": \"ResNet-50\",\n……\n},\n\"top1_error_10crop\": 24.52\n},\n{…}, {…}\n]\nFig. A1: Illustration of the Cognitive Agent. The overall reasoning process\ninvolves several key steps: Hypothesis Generation (analysis of the current task and\ncompleted trials), Stopping Judgment, and Strategic Planning. Strategic Planning\ndevelops experimental schemes based on the analyzed hypotheses.\nA. Trajectory Analysis and Hypothesis Generation\nSpecifically, the agent begins with Task 1: Analyze the Current Task, where it\nmeticulously interprets the prompt to establish the task’s core objectives, constraints,\nand hyperparameters. This initial analysis is used to perform a structured trajectory\nevaluation, creating a coherent chain of thought that guides all subsequent strategy and\nexploration. This initial analysis is used to perform a structured trajectory evaluation,\ncreating a coherent chain of thought that guides all subsequent reasoning and planning\nstrategies.\nNext, in Task 2: Analysis of Completed Trials, the agent systematically\nevaluates previous outcomes to generate new hypotheses. It specifically analyzes the\nperformance trends of individual parameters and their combinations to identify promis-\ning directions in the broader hyperparameter space and propose novel configurations.\nThis deep trend analysis allows the agent to anticipate promising regions of the search\nspace and detect valuable reasoning trajectories that might otherwise be overlooked.\nCognitive Prompt for Trajectory Analysis and Hypothesis Generation\nSystem:\n{{SYSTEM_PART_OF_CONFIGURATION}}\nUser: Completed trials:\n{{COMPLETED_TRIALS}}\n19\n"}, {"page": 20, "text": "Task 1: Analyze the current task\nUnderstand current tasks, basic ideas, objectives, and hyperparameters.\nTask 2: Analysis of Completed Trials\nStep 1: Summarize performance metrics for completed trials.\nStep 2: Evaluate performance trends for hyperparameters.\nStep 3: Highlight promising hyperparameter combinations.\nB. Best Stopping Judgement As described in Sect. ??, we introduce the optimal\nstopping criterion to guide the prompt design that balances exploration and exploitation.\nDuring Best Stopping Judgement, the cognitive agent first determines whether the\ncurrent performance metrics surpass those of the initial configuration and conducts\nan optimal stopping judgment. Building on the prior analysis of completed trials, the\nagent systematically evaluates stopping conditions to avoid testing all configurations.\nBy using prompt instructions, the optimal stopping judgment evaluates all com-\npleted trials by leveraging insights from trial analysis, observed performance trends,\nand identified key findings. In practice, we implement this judgment process via a\nstructured prompt template comprising two main tasks and explicit stopping crite-\nria. The language model then analyzes the relationship between completed trials and\nunexplored areas. The prompt is organized as follows:\nCognitive Prompt for Best Stopping Judgement\nUser: Completed trials:\n{{COMPLETED_TRIALS}}\nThe following **Search Space** contains **unexplored** trials. {{TRIALS}}\nInstructions: Task 1: Review Analysis of Completed Trials (trial analysis,\nperformance trends, highlights, and other insights)\nTask 2: Decide Whether to Stop Optimization\nBased on the above analysis and **Completed Trials**, determine whether the\noptimization process should be stopped.\nCarefully analyze each of the following stop rules and provide a short (1-2\nsentences) justification for whether it is met:\n1. Have all promising configurations identified based on performance trends\nbeen tested?\n2. Is it unlikely that unexplored configurations will perform better based on\nthe observed trends and the law of diminishing returns?\n3. Has the best metric improved significantly?\nStep 2: Decide whether **all** conditions are met.\nIf **all** criteria in Step 1 are met, Answer: Yes, with confidence\nscore: {{CONFIDENCE_SOCRE}}. Otherwise, Answer: No with confidence score:\n{{CONFIDENCE_SOCRE}}.\nC. Strategic Planning Following the reasoning and stopping judgment phases shown\nin Fig. A1, the agent reviews all executed trials to synthesize key observations on\nperformance trends, notable configurations, and areas of consistent improvement. Next,\n20\n"}, {"page": 21, "text": "it translates its abstract hypotheses into a concrete experimental plan. This strategic\nprocess balances two competing objectives: exploiting promising configurations and\nexploring uncertain regions. Exploitation focuses on refining or building upon previously\nsuccessful configurations, whereas exploration targets under-examined areas that show\nsignificant potential for performance gains. Each proposed trial is explicitly justified\nby insights from the preceding reasoning phase, ensuring the search is deliberate and\nwell-founded. Finally, each recommendation is formalized as an intended trial and\ndelegated to the Experiment Manager for execution.\nAs trials advance and incremental improvements diminish, the Experiment Manager\nmaintains a dynamic record of completed actions and continuously refines the remaining\nsearch space. Strategic planning directs the optimization trajectory toward convergence,\nand the system employs an effective stopping strategy to terminate exploration in a\nrational and efficient manner.\nNotably, the LLM-based agent demonstrates human-like exploratory behavior: it\ninitially focuses on refining hypotheses around high-performing regions and gradually\nextends attention to areas of greater uncertainty. This pattern reflects a synergy of\nintelligence-driven reasoning and adaptive planning.\nCognitive Prompt for Strategic Planning\nUser: Instructions:\nTask 1: Review Analysis of All Completed Trials\nCompleted trials:\n{{COMPLETED_TRIALS}}\nThe following **Search Space** contains **unexplored** trials:\n{{TRIALS}}\nInstructions:\nTask 2: Optimization Recommendation\nRecommend exactly {{N_JOBS}} promising trials from the provided **Search\nSpace** (include both number and params).\nRules:\n1. “params” MUST include:\n{{HYPERNAME}}\n2. All selected ‘params‘ must match exactly with the provided **Search Space**.\nDo NOT leave out any key.\n3. Use the analysis in **Task 1** (trial analysis, performance trends, highlights,\nand other insights) to guide selection.\n4. Based on the above analysis, explore under-explored regions only when there\nis clear evidence of potential performance gain.\n5. Do not mix, modify, or create new values.\n6. You MUST not output any JSON blocks in this part.\n7. You MUST provide reasoning for each recommendation.\n21\n"}, {"page": 22, "text": "A.3\nExperiment Manager\nThe Experiment Manager is responsible for experiment orchestration and recovery,\nincluding resource management, task allocation, and progress tracking. These capabil-\nities enable efficient coordination of multi-instance parallel optimization, maximize\nresource utilization, and enhance training robustness:\n1) Resource Management. The Experiment Manager monitors user program\nresource consumption and dynamically allocates available GPU, TPU, and memory\nresources. This granular allocation optimizes workload distribution across computing\nunits and ensures stable execution of all trials.\n2) Fault recovery and Checkpoint Reconnection. In case of system inter-\nruptions or suboptimal model performance, the Experiment Manager reports failures\nto the cognitive agent. The Experiment Manager performs preliminary diagnostics,\nidentifies potential issues, adjusts training parameters, and resumes training from the\nlatest checkpoint.\n3) Multi-Instance Parallel Optimization. SelfAI instantiates each user pro-\ngram to run across diverse physical environments, independent of the target program\nframework. The Experiment Manager coordinates multi-instance parallel training, syn-\nchronizes execution, and concurrently tests various configurations, thereby shortening\noverall training time and improving generalization across datasets and model parame-\nters. For each parallel experiment, the Experiment Manager identifies and supplies\nnecessary runtime parameters, ensuring experiments are conducted under the same\nenvironment optimized by the cognitive agent.\nSelfAI maintains detailed logs of all configurations, training runs, and evaluation\nmetrics, and provides targeted optimization suggestions for each completed experiment.\nThis intelligent decision-making refines experimental design and reduces redundant\nsearch in automated scientific discovery. The collaboration between the Experiment\nManager and the cognitive agent establishes a self-improving workflow. By conducting\nadvanced reasoning and analyzing trial result patterns through hypothesis-driven\nexploration, they uncover relationships between settings and model performance,\nsteering the search toward optimal configurations. This ensures that parallel training\nis not merely a brute-force process, but an intelligent exploration of the training space\nthat adapts based on real-time outcomes.\nA.4\nEvaluation for Reasoning Trajectories\nWhile the reasoning process of LLMs in problem-solving often generates a diverse\nrange of discrete insights and multiple potential chains of thought, this diversity,\nwhile valuable for exploration and exploitation, can pose a challenge to coherent\nreasoning evaluation across various experimentation and discovery phases. We propose a\nsystematic evaluation metric that captures both the diversity of reasoning perspectives\nand the overall coherence of the reasoning process, ensuring a more robust and\ncomprehensive assessment of reasoning capabilities.\nOptimal Stopping Criteria In this work, we collected the best value point and the\nstop point from trials. Based on Optimal Stopping Criteria [100, 101], we can define\n22\n"}, {"page": 23, "text": "the following measure formulas,\nGain = 1\nN\nN−1\nX\ni=0\nv∗\ni −vi,min\nvi,max −vi,min\n(A1)\ntbest = 1\nN\nN−1\nX\ni=0\ntbest\ni\n= 1\nN\nN−1\nX\ni=0\nmi\nMi\n(A2)\ntstop = 1\nN\nN−1\nX\ni=0\ntstop\ni\n= 1\nN\nN−1\nX\ni=0\nni\nMi\n(A3)\nand where N is the number of tasks. For the i-th task, Mi is the number of completed\ntrials. mi is the best value point index. tbest\ni\nis the cost of the best value. In addition,\nwe set the stop point index, ni, where tstop\ni\nis better when ni is closer to the best value\npoint index. Si means that the binarized value.\nTo obtain a comprehensive measure, we combine the last three measures, i.e., Rel,\ntbest, and tstop, where we utilize Rel in underestimated penalty, then tbest and tstop\nare the time penalty (Pbest and Pstop). Thus, the total penalty is\nPtotal = tstop\ni\n+ tbest\ni\n2\n(A4)\nFinally, the score is denoted as\nScore = 1\nN\nN−1\nX\ni=0\nGain · (1 −Ptotal)\n(A5)\nBest Approximation/Candidate In [48], performance profiles and the AUP aim to\nmeasure available rates across m tasks, where all performance metrics are threshold\nτ, performance profiles (ρm(τ)) are computed as thresholds in all metrics (sorted by\nascending) in current task.\nAUPm =\nZ τmax\n1\nρm(τ)dτ\n(A6)\nIt is noted that the above performance profile and AUPm score cannot measure the\ndiversity of reasoning. Therefore, we rewrite the performance profile and AUP score:\nFirst, the performance profile is defined in all completed trials Mi in i-th task and\nthe overall search space H, as follows\nri =\n\n\n\n\n\nmax {vk: k∈H}\nvi\n, ascend\nvi\nmin {vk: k∈H}, descend\n(A7)\n23\n"}, {"page": 24, "text": "where ascend/descend denotes that the value is larger/smaller, the performance is\nbetter. For all trials, τ is the set of all obtained ri values.\nThen, we consider all completed trials Mi in i-th task,\nρi(τ) =\n(\n|{k ∈Mi : rk >= τ}|,\nascend\n|{k ∈Mi : rk <= τ}|,\ndescend\n(A8)\nwhich captures how many evaluated configurations exceed a given performance\nthreshold τ. ρi(τ) is the cumulative distribution curve of the trajectory.\nThe area term aggregates the overall concentration of strong configurations along\nthe performance axis:\nA = 1\nN\nN−1\nX\ni=0\nZ τmax\nτmin\nρi(τ)dτ.\n(A9)\nTo capture the temporal asymmetry of discovery, we compute the centroid\nG = 1\nA\nZ τmax\nτmin\nx · ρi(τ) dτ,\n(A10)\nand define the skewness\nS =\nZ τmax\nτmin\n\u0010 x\nG\n\u00113\nρ(x) dτ.\n(A11)\nSince S may be unbounded and may take both positive (left-skewed) and negative\n(right-skewed) values, we normalize it via a reference skewness value Sbase from the\nGS method and a smooth monotonic mapping:\nS′ = 1 −S −Sbase\nSbase\n,\n(A12)\nS′ = tanh(S) + 1\n2\n, S′ ∈(0, 1).\n(A13)\nFinally, the Area Under the Performance–Diversity curve (AUPD) is defined as\nAUPD = A/S′,\n(A14)\nwhere trajectories that exhibit earlier concentration of high-performing configurations\nobtain larger and thus smaller AUPD value, whereas trajectories that concentrate\nimprovements later yield smaller and therefore larger value.\nAppendix B\nDetails of SelfAI Benchmark\nThis section provides the complete technical definitions of the metrics used in the main\ntext, including all normalization procedures, ranking aggregation, and scoring rules.\nThese details are omitted from the main text for clarity and conciseness.\nTable B1 summarizes the 12 tasks spanning six scientific categories used to evaluate\nthe reasoning capacity of different solvers. For each dataset, we supply background\n24\n"}, {"page": 25, "text": "information to the LLMs to simulate the user-intent interpretation process (see\nSect. A.1). Most datasets are generated through systematic grid-based exploration,\nwhereas LCBench and the Chagas EP20 drug-discovery dataset are constructed from\nBayesian optimization trajectories. LCBench, in particular, is a widely used AutoML\nbenchmark [71]. Supplementary Fig. B2 provides an additional structural analysis of\nthe LCBench search space. Accuracy depends on interacting hyperparameters such\nas weight decay, hidden-layer width, and depth, and the high-performance region is\nsharply localized. This structure highlights why solvers must possess strong reasoning\ncapabilities to efficiently locate these promising configurations. Supplementary Fig. B3\nreports solver rankings based on the Score metric. These rank summaries allow cross-\ntask comparisons of solver consistency; detailed interpretation of performance trends\nis provided in the main text.\nIn Table B2, we summarize averaged performance across all benchmark tasks.\nClassical baselines such as grid search and TPE-based Bayesian optimization achieve\ncompetitive final objective values but receive low Score and high AUPD, reflecting\nredundant late-stage exploration and delayed stopping. Naive LLM solvers improve\nearly discovery but remain unstable across tasks. In contrast, SelfAI-driven solvers\nbased on mid-sized models such as Qwen2.5-7b and GPT4-o3-mini attain the highest\nScores with substantially lower AUPD and earlier stopping, indicating more focused\nand efficient trajectories. Larger models (e.g., Qwen2.5-72b, Llama3.3-70b) do not\nnecessarily yield better efficiency, highlighting that structured reasoning and trajectory\ncontrol within SelfAI are more critical than sheer model scale. In addition, as discussed\nin Sect. B.1, failure analyses are also included for completeness. These examples\nillustrate typical breakdowns in reasoning processes across long trajectories, such\nas premature stopping, limited incorporation of earlier observations, or sensitivity\nto minor perturbations. They inform the broader evaluation of solver stability and\nsupport the claims presented in the main text.\nTable B1: List of tasks in SelfAI with different hyperparameters for multiple\ntasks and datasets.\nCategory\nMethod\nTask\nDim\nCount\nRef.\nScientific Computing\nTensor Network\nImage Completion\n3\n64\n[65]\nRandom Forest\nRegression\n5\n162\n[62]\nDeep Learning\nLSTM\nSentiment Analysis\n2\n20\n[63]\nGraphSAGE\nNode Classification\n22\n25\n[93]\nComputer Vision\nSIREN\nImage Denoising\n2\n25\n[68]\nSIREN\nImage Segmentation\n2\n25\n[68]\nResNet\nImage Classification\n4\n9\n[70]\nMAE\nImage Classification\n2\n20\n[69]\nFashionMnist-NN\nImage Classification\n5\n2000\n[71]\nMedical Image Analysis\nnnUnet\nBraTS [82]\n3\n18\n[72]\nnnUnet-revisited\nBTCV [81]\n5\n19\n[73]\nDrug Discovery\nDNN\nBioactivity Prediction\n4\n30\n[94]\n25\n"}, {"page": 26, "text": "Fig. B2: The relationship between different parameter settings and accuracy. a A\nhexbin plot showing the joint density of learning rate (log-scaled) and accuracy. b\nBox plots illustrating the distribution of dropout rates used for models of different\ndepths. c Mean accuracy and standard error across logarithmically binned batch sizes.\nd Performance comparison of different architectural configurations (layers and units).\ne A scatter plot of weight decay against momentum, colored by accuracy. f The\noptimization trajectory, showing accuracy improvement over successive trials, with\na moving average trend line. g A correlation matrix quantifying linear relationships\nbetween all parameters and the accuracy.\nB.1\nFailure Cases\nComparison of Best Result Hit Rate. Table B2 compares the experimental\nresults of traditional solvers and LLM-based solvers. Early-stopping LLM-ES partially\nalleviates redundant trials but remains less stable and less efficient than SelfAI-\nintegrated variants. In contrast, SelfAI consistently enhances trajectory-level reasoning\nacross all LLM families, reducing unnecessary exploration (lower AUPD), improving\nstopping behavior (lower Stop-Time), and yielding higher overall Score. Mid-sized\nmodels such as Qwen2.5-7B and DeepSeek-R1-32B demonstrate particularly favorable\n26\n"}, {"page": 27, "text": "Table B2: Averaged performance comparison of SelfAI across different tasks.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Hit-Rate↑Rank\nGS\n0.2453\n1.0000\n0.5094\n1.0000\n1.0000\n1.0000\n14\nBS\n0.1927\n0.8106\n0.6265\n0.9881\n1.0000\n1.0000\n15\nLLM\n0.3526\n0.7638\n0.2949\n1.0000\n1.0000\n0.9286\n13\nLLM-ES\n0.5294\n0.2349\n0.4691\n0.4582\n0.9981\n0.6429\n3\nQwen2.5-7b\n0.5562\n0.2154\n0.4805\n0.3945\n0.9957\n0.7857\n2\nQwen2.5-14b\n0.5015\n0.3310\n0.4926\n0.4997\n0.9969\n0.7857\n5\nQwen2.5-32b\n0.4287\n0.4684\n0.5252\n0.6133\n0.9972\n0.7857\n10\nQwen2.5-72b\n0.4189\n0.5358\n0.4531\n0.7087\n0.9995\n0.8571\n11\nDeepSeek-r1-7b\n0.4769\n0.1802\n0.7020\n0.3093\n0.9927\n0.5000\n8\nDeepSeek-r1-14b 0.4793\n0.3956\n0.4953\n0.5100\n0.9948\n0.7143\n7\nDeepSeek-r1-32b 0.4989\n0.3476\n0.4535\n0.5433\n0.9933\n0.7857\n6\nDeepSeek-r1-70b 0.4556\n0.3513\n0.5392\n0.5299\n0.9962\n0.7143\n9\nLlama3.3-70b\n0.3625\n0.5483\n0.5099\n0.7271\n0.9683\n0.7143\n12\nGPT4-o3-mini\n0.6433\n0.2259\n0.3168\n0.3961\n0.9989\n0.8571\n1\nGPT4-o3\n0.5140\n0.2284\n0.5477\n0.3961\n0.9966\n0.6429\n4\nexploration and exploitation trade-offs. Although LLMs possess advanced reasoning\ncapabilities and can identify optimal results more efficiently, we found that LLM-based\nsolvers still face challenges related to context limitations and computational fragility.\n1) Context limitations. The LCBench dataset exceeds the maximum token limit\nof GPT-family models, which is why SelfAI does not employ GPT4-o3-mini or GPT4-\no3. In practice, search spaces rarely contain thousands of trials, and this issue can be\nmitigated in future work. For instance, memory management and Retrieval-Augmented\nGeneration (RAG) can help reduce token requirements by introducing additional steps\nto the experimental recommendation workflow, such as embedding, chunking, and\nretrieval.\n2) Computational fragility. Table B2 compares the best result hit rates across\nsolvers. The performance of LLM-based solvers is highly sensitive to the reasoning\nstrategy, and early stopping often leads to a decline in hit rate. In the supplementary\ntables, we report the best result tbest for each solver. tbest = 1 indicates that the\noptimal result was not found. Moreover, when tbest = 1, the score metric improves\nsignificantly, highlighting the solver’s weakness in optimal stopping. As illustrated in\nFig. B7, computational fragility also hinders reliable performance improvement through\nparameter tuning, as seen in the non-monotonic performance of the DeepSeek-R1\nmodel series. Table C5 summarizes the metrics corresponding to Fig. B7.\nFig. B7 provides a detailed visualization of solver behavior. The Qwen2.5 models\nconsistently identify high-performing configurations early in the search, with Qwen2.5-\n7B reaching near-optimal regions within the first 10–15 trials. Their trajectories exhibit\nstable refinement and limited oscillation, suggesting consistent integration of prior\nexperimental evidence. In contrast, the DeepSeek-R1 models show pronounced non-\nmonotonic behavior. DeepSeek-R1-7B frequently oscillates between high and low values,\nreflecting sensitivity to minor variations in its reasoning process. DeepSeek-R1-70B\n27\n"}, {"page": 28, "text": "Image Classification\nMedical Image Analysis\nSolver\nTask\nFig. B3: Performance ranking of different methods across multiple tasks. The heatmap\ndisplays the rank of each method (rows) for every task (columns), where lower numbers\n(darker colors) indicate better performance (e.g., 1st place). The final average rank is\nsummarized on the right.\nachieves early improvements but then terminates prematurely, failing to further refine\npromising regions. Although the GS and BS reach high-performance regions, their\ntrajectories lack the rapid breakthroughs observed in LLM-based solvers. Overall, these\ntrajectory comparisons provide visual evidence of differences in exploration stability,\nbreakthrough efficiency, and stopping behavior across solver families, complementing\nthe aggregate metrics reported in the main text.\n28\n"}, {"page": 29, "text": "BS\nGPT4-o3-mini\nGPT4-o3\nLlama3.3-70B\nFig. B4: Illustration of the optimized trajectory using the SIREN method for additional\ncases in image denoising (first two columns) and segmentation (last two columns).\nBlue points are initial points. Green points represent suggested points before reaching\nthe optimum. Red points indicate redundant suggestions generated after the optimal\npoint has been reached. The ⋆marks the optimal point. The numbered labels indicate\nthe sequence of recommendations provided by LLMs.\n29\n"}, {"page": 30, "text": "Qwen2.5-7B\nQwen2.5-14B\nQwen2.5-72B\nQwen2.5-72B\nFig. B5: More optimized trajectories of Fig. B4. These LLMs have different suggestions\nfor non-convex hyperparameter optimization.\n30\n"}, {"page": 31, "text": "DeepSeek-R1-7B\nDeepSeek-R1-14B\nDeepSeek-R1-32B\nDeepSeek-R1-70B\nFig. B6: More optimized trajectories of Fig. B4. These LLMs have different suggestions\nfor non-convex hyperparameter optimization.\n31\n"}, {"page": 32, "text": "0\n10\n20\n30\n40\n50\n60\nTrial Number\n25.41\n30.41\n35.41\n40.40\n40.60\n40.80\n41.00\n41.20\n41.40\n41.60\nPerformance\n41.50\nQwen2.5-7b\nQwen2.5-7b\nGS\nBS\n0\n10\n20\n30\n40\n50\n60\nTrial Number\n25.41\n30.41\n35.41\n40.40\n40.60\n40.80\n41.00\n41.20\n41.40\n41.60\nPerformance\n41.50\nQwen2.5-72b\nQwen2.5-72b\nGS\nBS\n0\n10\n20\n30\n40\n50\n60\nTrial Number\n25.41\n30.41\n35.41\n40.40\n40.60\n40.80\n41.00\n41.20\n41.40\n41.60\nPerformance\n41.21\nDeepSeek-r1-7b\nDeepSeek-r1-7b\nGS\nBS\n0\n10\n20\n30\n40\n50\n60\nTrial Number\n25.41\n30.41\n35.41\n40.40\n40.60\n40.80\n41.00\n41.20\n41.40\n41.60\nPerformance\n41.34\nDeepSeek-R1-70b\nDeepSeek-R1-70b\nGS\nBS\nFig. B7: Failure Cases for the DeepSeek-R1 Family in the scientific computing field.\nBoth DeepSeek-R1-7B and DeepSeek-R1-70B fail to reach the optimal results, marked\nwith a star symbol (⋆). Notably, DeepSeek-R1-7B fails to improve beyond the initial\nresults, as indicated by the red dotted line, suggesting limited exploration capability\nin the search space.\n32\n"}, {"page": 33, "text": "Appendix C\nSupplementary Tables\nTable C3: Performance comparison of different solvers for the Boston\nhousing prices based on the Random Forest Regressor.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.4969\n1.0000\n0.0062\n1.0000\n0.841\n9\nBS\n0.4654\n0.9745\n0.0755\n0.9937\n0.841\n11\nLlama3.3-70b\n0.3831\n0.1330\n1.0000\n0.1509\n0.837\n13\nQwen2.5-7b\n0.7893\n0.3463\n0.0943\n0.3270\n0.841\n5\nQwen2.5-14b\n0.7013\n0.6108\n0.0063\n0.5912\n0.841\n7\nQwen2.5-32b\n0.6981\n0.4081\n0.2579\n0.3459\n0.841\n8\nQwen2.5-72b\n0.7862\n0.5250\n0.0126\n0.4151\n0.841\n6\nDeepSeek-R1-7b\n0.9057\n0.0910\n0.0881\n0.1006\n0.841\n3\nDeepSeek-R1-14b\n0.8522\n0.3401\n0.0063\n0.2893\n0.841\n4\nDeepSeek-R1-32b\n0.9403\n0.1618\n0.0063\n0.1132\n0.841\n2\nDeepSeek-R1-70b\n0.3948\n0.0215\n1.0000\n0.0189\n0.833\n12\nGPT4-o3-mini\n0.9811\n0.0062\n0.0189\n0.0189\n0.841\n1\nGPT4-o3\n0.4694\n0.0560\n1.0000\n0.0377\n0.840\n10\nAppendix D\nPrompts of Compared Methods\nSearch Prompt of LLM solver\nInstructions:\n**Search Space** (Numbering starts from 0, excluding Completed Trials):\n{{TRIALS}}\nTask 1: Optimization Recommendation Recommend exactly {{N_JOBS}}\npromising trials from the provided **Search Space** (include both number and\nparams).\nRules:\n1. “params” MUST include:\n{{HYPERNAME}}\n2. All selected ‘params‘ must match exactly with the provided **Search Space**.\nDo NOT leave out any key.\n3. Use the analysis in **Task 1** (trial analysis, performance trends, highlights,\nand other insights) to guide selection.\n33\n"}, {"page": 34, "text": "Table C4: Performance comparison of different solvers for the sentiment\nanalysis task based on the LSTM model.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.4750\n1.0000\n0.0500\n1.0000\n0.96\n2\nBS\n0.0294\n0.8817\n0.9412\n1.0000\n0.96\n13\nLlama3.3-70b\n0.3824\n0.8817\n0.2353\n1.0000\n0.96\n4\nQwen2.5-7b\n0.1830\n0.2857\n1.0000\n0.5294\n0.94\n12\nQwen2.5-14b\n0.3529\n0.8318\n0.4118\n0.8824\n0.96\n7\nQwen2.5-32b\n0.3529\n0.6383\n0.5882\n0.7059\n0.96\n8\nQwen2.5-72b\n0.4412\n0.8220\n0.2353\n0.8824\n0.96\n3\nDeepSeek-R1-7b\n0.2157\n0.1419\n1.0000\n0.3529\n0.93\n11\nDeepSeek-R1-14b\n0.2353\n0.8817\n0.5294\n1.0000\n0.96\n10\nDeepSeek-R1-32b\n0.3824\n0.8817\n0.2353\n1.0000\n0.96\n4\nDeepSeek-R1-70b\n0.3824\n0.8817\n0.2353\n1.0000\n0.96\n4\nGPT4-o3-mini\n0.8824\n0.1494\n0.0588\n0.1765\n0.96\n1\nGPT4-o3\n0.2745\n0.0469\n1.0000\n0.1765\n0.93\n9\n4. Based on the above analysis, explore under-explored regions only when there\nis clear evidence of potential performance gain.\n5. Do not mix, modify, or create new values.\n6. You MUST not output any JSON blocks in this part.\n7. You MUST provide reasoning for each recommendation.\nEarly-Stopping Prompt of LLM-ES solver\nCompleted trials: {{COMPLETED_TRIALS}}\nThe following **Search Space** contains **unexplored** trials. {{TRIALS}}\nIf the optimization process should be stopped, Answer: Yes with confidence\nscore: {{CONFIDENCE_SOCRE}}. Otherwise, Answer: No with confidence score:\n{{CONFIDENCE_SOCRE}}.\nFinally,\nyou\nMUST\noutput\n’Answer:\nNo/Yes’\nwith\nconfidence\nscore:\n{{CONFIDENCE_SOCRE}}.\nReferences\n[1] OpenAI, R.: Gpt-4 technical report. arxiv 2303.08774. View in Article 2(5)\n(2023)\n34\n"}, {"page": 35, "text": "Table C5: Performance comparison of different solvers for scientific\ncomputing fields, where we evaluate the tensor wheel decomposition\nmethod on the multi-spectral image completion dataset.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.2656\n1.0000\n0.4688\n1.0000\n41.50\n11\nBS\n0.1066\n0.9638\n0.7869\n1.0000\n41.50\n13\nLlama3.3-70b\n0.4016\n0.8979\n0.2623\n0.9344\n41.50\n8\nQwen2.5-7b\n0.7377\n0.2904\n0.2295\n0.2951\n41.50\n1\nQwen2.5-14b\n0.5410\n0.2779\n0.4262\n0.4918\n41.50\n2\nQwen2.5-32b\n0.2869\n0.9638\n0.4262\n1.0000\n41.50\n10\nQwen2.5-72b\n0.1803\n0.9638\n0.6393\n1.0000\n41.50\n12\nDeepSeek-R1-7b\n0.3216\n0.3009\n1.0000\n0.3443\n41.21\n9\nDeepSeek-R1-14b\n0.4426\n0.8687\n0.2295\n0.8852\n41.50\n4\nDeepSeek-R1-32b\n0.4426\n0.6645\n0.4426\n0.6721\n41.50\n4\nDeepSeek-R1-70b\n0.4136\n0.1477\n1.0000\n0.1639\n41.34\n7\nGPT4-o3-mini\n0.5410\n0.2623\n0.4590\n0.4590\n41.50\n3\nGPT4-o3\n0.4217\n0.0539\n1.0000\n0.1475\n41.34\n6\n[2] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y.,\nHuang, F., et al.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023)\n[3] Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S.,\nWang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948 (2025)\n[4] Huang, J., Chang, K.C.-C.: Towards reasoning in large language models: A\nsurvey. arXiv preprint arXiv:2212.10403 (2022)\n[5] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models\nare zero-shot reasoners. Advances in neural information processing systems 35,\n22199–22213 (2022)\n[6] Wu, S., Fei, H., Qu, L., Ji, W., Chua, T.-S.: Next-gpt: Any-to-any multimodal\nllm. In: Forty-first International Conference on Machine Learning (2024)\n[7] McKinzie, B., Gan, Z., Fauconnier, J.-P., Dodge, S., Zhang, B., Dufter, P., Shah,\nD., Du, X., Peng, F., Belyi, A., et al.: Mm1: methods, analysis and insights from\nmultimodal llm pre-training. In: European Conference on Computer Vision, pp.\n304–323 (2024). Springer\n[8] Ferrag, M.A., Tihanyi, N., Debbah, M.: From llm reasoning to autonomous ai\n35\n"}, {"page": 36, "text": "Table C6: Performance comparison of different solvers for the image\nsegmentation task based on the SIREN model.\nSolver\nScore↑\nAUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.1100\n1.000000\n0.780000\n1.000000\n16.63\n13\nBS\n0.329545 0.761200\n0.477273\n0.863636\n16.63\n11\nQwen2.5-7b\n0.602273 0.487619\n0.363636\n0.431818\n16.63\n2\nQwen2.5-14b\n0.454545 0.708497\n0.272727\n0.818182\n16.63\n4\nQwen2.5-32b\n0.374388 0.538240\n0.613636\n0.636364\n16.60\n9\nQwen2.5-72b\n0.408757 0.536178\n0.636364\n0.545455\n16.60\n5\nDeepSeek-r1-7b\n0.693182 0.318656\n0.227273\n0.386364\n16.63\n1\nDeepSeek-r1-14b 0.408534 0.471791\n0.704545\n0.477273\n16.60\n6\nDeepSeek-r1-32b 0.312477 0.198417\n1.000000\n0.340909\n15.77\n12\nDeepSeek-r1-70b 0.374555 0.576330\n0.590909\n0.659091\n16.60\n8\nLlama3.3-70b\n0.380344 0.285865\n0.590909\n0.295455\n10.79\n7\nGPT4-o3-mini\n0.477273 0.517709\n0.454545\n0.590909\n16.63\n3\nGPT4-o3\n0.363636 0.671366\n0.477273\n0.795455\n16.63\n10\nagents: A comprehensive review. arXiv preprint arXiv:2504.19678 (2025)\n[9] Ghafarollahi, A., Buehler, M.J.: Sciagents: automating scientific discovery\nthrough bioinspired multi-agent intelligent graph reasoning. Advanced Materials\n37(22), 2413523 (2025)\n[10] Chen, D., Bai, Y., Ament, S., Zhao, W., Guevarra, D., Zhou, L., Selman, B.,\nDover, R.B., Gregoire, J.M., Gomes, C.P.: Automating crystal-structure phase\nmapping by combining deep learning with constraint reasoning. Nature Machine\nIntelligence 3(9), 812–822 (2021)\n[11] Lin, J., Guo, Y., Han, Y., Hu, S., Ni, Z., Wang, L., Chen, M., Liu, H., Chen,\nR., He, Y., et al.: Se-agent: Self-evolution trajectory optimization in multi-step\nreasoning with llm-based agents. arXiv preprint arXiv:2508.02085 (2025)\n[12] Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X.,\nQian, B., et al.: Toolllm: Facilitating large language models to master 16000+\nreal-world apis. arXiv preprint arXiv:2307.16789 (2023)\n[13] Xu, J., Du, W., Liu, X., Li, X.: Llm4workflow: An llm-based automated workflow\nmodel generation tool. In: Proceedings of the 39th IEEE/ACM International\nConference on Automated Software Engineering, pp. 2394–2398 (2024)\n36\n"}, {"page": 37, "text": "Table C7: Performance comparison of different solvers for the image\ndenoising task based on the SIREN model.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.1500\n1.0000\n0.7000\n1.0000\n24.78\n12\nBS\n0.1477\n0.8819\n0.7045\n1.0000\n24.78\n13\nQwen2.5-7b\n0.7045\n0.1802\n0.1364\n0.4545\n24.78\n4\nLlama3.3-70b\n0.5706\n0.1105\n0.5455\n0.2955\n24.27\n8\nQwen2.5-14b\n0.4205\n0.5205\n0.5455\n0.6136\n24.78\n10\nQwen2.5-32b\n0.5795\n0.4762\n0.3182\n0.5227\n24.78\n7\nQwen2.5-72b\n0.7614\n0.1543\n0.1364\n0.3409\n24.78\n1\nDeepSeek-R1-7b\n0.5341\n0.4193\n0.3864\n0.5455\n24.78\n9\nDeepSeek-R1-14b\n0.7159\n0.1639\n0.0909\n0.4773\n24.78\n2\nDeepSeek-R1-32b\n0.7159\n0.1284\n0.2045\n0.3636\n24.78\n2\nDeepSeek-R1-70b\n0.6818\n0.1359\n0.2955\n0.3409\n24.78\n5\nGPT4-o3-mini\n0.2703\n0.4389\n0.7955\n0.6591\n24.48\n11\nGPT4-o3\n0.6023\n0.4786\n0.2045\n0.5909\n24.78\n6\n[14] Gupta, S., Mahmood, A., Shetty, P., Adeboye, A., Ramprasad, R.: Data extrac-\ntion from polymer literature using large language models. Communications\nmaterials 5(1), 269 (2024)\n[15] Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S.,\nVan Katwyk, P., Deac, A., et al.: Scientific discovery in the age of artificial\nintelligence. Nature 620(7972), 47–60 (2023)\n[16] Darvish, K., Skreta, M., Zhao, Y., Yoshikawa, N., Som, S., Bogdanovic, M., Cao,\nY., Hao, H., Xu, H., Aspuru-Guzik, A., et al.: Organa: A robotic assistant for\nautomated chemistry experimentation and characterization. Matter 8(2) (2025)\n[17] Huang, K., Zhang, S., Wang, H., Qu, Y., Lu, Y., Roohani, Y., Li, R., Qiu, L.,\nZhang, J., Di, Y., et al.: Biomni: A general-purpose biomedical ai agent. bioRxiv,\n2025–05 (2025)\n[18] Alampara, N., Schilling-Wilhelmi, M., Ríos-García, M., Mandal, I., Khetarpal,\nP., Grover, H.S., Krishnan, N.A., Jablonka, K.M.: Probing the limitations\nof multimodal language models for chemistry and materials research. Nature\ncomputational science, 1–10 (2025)\n[19] Polak, M.P., Morgan, D.: Extracting accurate materials data from research\npapers with conversational language models and prompt engineering. Nature\nCommunications 15(1), 1569 (2024)\n37\n"}, {"page": 38, "text": "Table C8: Performance comparison of different solvers for Mask AutoEn-\ncoders (MAE) [69].\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.1000\n1.0000\n0.8000\n1.0000\n85.0\n12\nBS\n0.0294\n0.8093\n0.9412\n1.0000\n85.0\n13\nLlama3.3-70b\n0.2941\n0.8093\n0.4118\n1.0000\n85.0\n10\nQwen2.5-7b\n0.7647\n0.0112\n0.2353\n0.2353\n85.0\n2\nQwen2.5-14b\n0.3471\n0.1513\n1.0000\n0.2941\n84.5\n9\nQwen2.5-32b\n0.2647\n0.8093\n0.4706\n1.0000\n85.0\n11\nQwen2.5-72b\n0.4412\n0.8093\n0.1176\n1.0000\n85.0\n7\nDeepSeek-R1-7b\n0.4104\n0.0080\n1.0000\n0.1765\n84.9\n8\nDeepSeek-R1-14b\n0.6176\n0.3853\n0.2941\n0.4706\n85.0\n5\nDeepSeek-R1-32b\n0.6765\n0.1108\n0.2941\n0.3529\n85.0\n3\nDeepSeek-R1-70b\n0.5588\n0.2235\n0.1765\n0.7059\n85.0\n6\nGPT4-o3-mini\n0.8235\n0.0088\n0.1176\n0.2353\n85.0\n1\nGPT4-o3\n0.6471\n0.1141\n0.3529\n0.3529\n85.0\n4\n[20] Dagdelen, J., Dunn, A., Lee, S., Walker, N., Rosen, A.S., Ceder, G., Persson,\nK.A., Jain, A.: Structured information extraction from scientific text with large\nlanguage models. Nature communications 15(1), 1418 (2024)\n[21] Mu, C., Zhang, X., Wang, H.: Planning of heuristics: Strategic planning on\nlarge language models with monte carlo tree search for automating heuristic\noptimization. arXiv preprint arXiv:2502.11422 (2025)\n[22] Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., Wang, Y.-X.: Language\nagent tree search unifies reasoning acting and planning in language models. arXiv\npreprint arXiv:2310.04406 (2023)\n[23] Hao, S., Gu, Y., Ma, H., Hong, J.J., Wang, Z., Wang, D.Z., Hu, Z.: Reasoning with\nlanguage model is planning with world model. arXiv preprint arXiv:2305.14992\n(2023)\n[24] Toledo, E., Hambardzumyan, K., Josifoski, M., Hazra, R., Baldwin, N., Audran-\nReiss, A., Kuchnik, M., Magka, D., Jiang, M., Lupidi, A.M., et al.: Ai research\nagents for machine learning: Search, exploration, and generalization in mle-bench.\narXiv preprint arXiv:2507.02554 (2025)\n[25] Zweiger, A., Pari, J., Guo, H., Akyürek, E., Kim, Y., Agrawal, P.: Self-adapting\nlanguage models. arXiv preprint arXiv:2506.10943 (2025)\n38\n"}, {"page": 39, "text": "Table C9: Performance comparison of different solvers for the image\nclassification task on the ImageNet dataset reported from the ResNet\nbenchmark.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.0000\n1.0000\n1.0000\n1.0000\n21.43\nBS\n0.0833\n0.6394\n0.8333\n1.0000\n21.43\nLlama3.3-70b\n0.3333\n0.6394\n0.3333\n1.0000\n21.43\nQwen2.5-7b\n0.6667\n0.0865\n0.3333\n0.3333\n21.43\nQwen2.5-14b\n0.8333\n0.0000\n0.1667\n0.1667\n21.43\nQwen2.5-32b\n0.5833\n0.2192\n0.3333\n0.5000\n21.43\nQwen2.5-72b\n0.4167\n0.2988\n0.5000\n0.6667\n21.43\nDeepSeek-R1-7b\n0.4167\n0.3322\n0.5000\n0.6667\n21.43\nDeepSeek-R1-14b\n0.6667\n0.0865\n0.3333\n0.3333\n21.43\nDeepSeek-R1-32b\n0.7500\n0.0865\n0.1667\n0.3333\n21.43\nDeepSeek-R1-70b\n0.5000\n0.2443\n0.5000\n0.5000\n21.43\nGPT4-o3-mini\n0.5000\n0.2192\n0.5000\n0.5000\n21.43\nGPT4-o3\n0.5833\n0.2192\n0.3333\n0.5000\n21.43\n[26] Team, N., Zhang, B., Feng, S., Yan, X., Yuan, J., Yu, Z., He, X., Huang, S., Hou,\nS., Nie, Z., et al.: Novelseek: When agent becomes the scientist–building closed-\nloop system from hypothesis to verification. arXiv preprint arXiv:2505.16938\n(2025)\n[27] Baek, J., Jauhar, S.K., Cucerzan, S., Hwang, S.J.: Researchagent: Iterative\nresearch idea generation over scientific literature with large language models. In:\nProceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pp. 6709–6738 (2025)\n[28] Steyaert, S., Pizurica, M., Nagaraj, D., Khandelwal, P., Hernandez-Boussard,\nT., Gentles, A.J., Gevaert, O.: Multimodal data fusion for cancer biomarker\ndiscovery with deep learning. Nature machine intelligence 5(4), 351–362 (2023)\n[29] Gao, F., Li, H., Chen, Z., Yi, Y., Nie, S., Cheng, Z., Liu, Z., Guo, Y., Liu, S.,\nQin, Q., et al.: A chemical autonomous robotic platform for end-to-end synthesis\nof nanoparticles. Nature Communications 16(1), 7558 (2025)\n[30] Zhang, Y., Han, Y., Chen, S., Yu, R., Zhao, X., Liu, X., Zeng, K., Yu, M.,\nTian, J., Zhu, F., et al.: Large language models to accelerate organic chemistry\nsynthesis. Nature Machine Intelligence, 1–13 (2025)\n39\n"}, {"page": 40, "text": "Table C10: Performance comparison of different solvers for the image\nclassification task on the LCBench dataset reported from [71].\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.1750\n1.0000\n0.6500\n1.0000\n88.29\nBS\n0.4189\n0.9985\n0.1622\n1.0000\n88.29\nLlama3.3-70b\n0.4738\n0.0052\n1.0000\n0.0486\n87.98\nQwen2.5-7b\n0.4619\n0.0047\n1.0000\n0.0451\n85.78\nQwen2.5-14b\n0.4739\n0.0017\n1.0000\n0.0180\n85.62\nQwen2.5-32b\n0.4718\n0.0022\n1.0000\n0.0225\n85.62\nQwen2.5-72b\n0.4599\n0.0081\n1.0000\n0.0766\n87.98\nDeepSeek-R1-7b\n0.4691\n0.0027\n1.0000\n0.0280\n85.62\nDeepSeek-R1-14b\n0.4579\n0.0086\n1.0000\n0.0806\n87.98\nDeepSeek-R1-32b\n0.4677\n0.0035\n1.0000\n0.0331\n85.78\nDeepSeek-R1-70b\n0.4739\n0.0014\n1.0000\n0.0180\n85.62\nGPT4-o3-mini\n-\n-\n-\n-\n-\n-\nGPT4-o3\n-\n-\n-\n-\n-\n-\n[31] Xiong, J., Zhang, W., Wang, Y., Huang, J., Shi, Y., Xu, M., Li, M., Fu, Z., Kong,\nX., Wang, Y., et al.: Bridging chemistry and artificial intelligence by a reaction\ndescription language. Nature Machine Intelligence, 1–12 (2025)\n[32] Jablonka, K.M., Schwaller, P., Ortega-Guerrero, A., Smit, B.: Leveraging large\nlanguage models for predictive chemistry. Nature Machine Intelligence 6(2),\n161–169 (2024)\n[33] Miret, S., Krishnan, N.A.: Enabling large language models for real-world materials\ndiscovery. Nature Machine Intelligence, 1–8 (2025)\n[34] Kang, Y., Kim, J.: Chatmof: an artificial intelligence system for predicting\nand generating metal-organic frameworks using large language models. Nature\ncommunications 15(1), 4705 (2024)\n[35] Stach, E., DeCost, B., Kusne, A.G., Hattrick-Simpers, J., Brown, K.A., Reyes,\nK.G., Schrier, J., Billinge, S., Buonassisi, T., Foster, I., et al.: Autonomous\nexperimentation systems for materials development: A community perspective.\nMatter 4(9), 2702–2726 (2021)\n[36] Boiko, D.A., MacKnight, R., Kline, B., Gomes, G.: Autonomous chemical research\nwith large language models. Nature 624(7992), 570–578 (2023)\n[37] Özçelik, R., Ruiter, S., Criscuolo, E., Grisoni, F.: Chemical language modeling\n40\n"}, {"page": 41, "text": "Table C11: Performance comparison of different solvers for the medical\nimage segmentation task on the BTCV dataset reported from the nnUnet\nbenchmark [73].\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.4211\n1.0000\n0.1579\n1.0000\n85.04\n3\nBS\n0.0625\n0.6453\n0.8750\n1.0000\n85.04\n13\nLlama3.3-70b\n0.3750\n0.6453\n0.2500\n1.0000\n85.04\n5\nQwen2.5-7b\n0.4688\n0.3441\n0.5000\n0.5625\n85.04\n2\nQwen2.5-14b\n0.2813\n0.6453\n0.4375\n1.0000\n85.04\n7\nQwen2.5-32b\n0.2813\n0.4742\n0.6875\n0.7500\n85.04\n7\nQwen2.5-72b\n0.1875\n0.6453\n0.6250\n1.0000\n85.04\n11\nDeepSeek-R1-7b\n0.3680\n0.0813\n1.0000\n0.1250\n83.69\n6\nDeepSeek-R1-14b\n0.1983\n0.0505\n1.0000\n0.1875\n80.69\n10\nDeepSeek-R1-32b\n0.1875\n0.6453\n0.6250\n1.0000\n85.04\n11\nDeepSeek-R1-70b\n0.2813\n0.6453\n0.4375\n1.0000\n85.04\n7\nGPT4-o3-mini\n0.6875\n0.1466\n0.2500\n0.3750\n85.04\n1\nGPT4-o3\n0.4063\n0.5081\n0.4375\n0.7500\n85.04\n4\nwith structured state space sequence models. Nature Communications 15(1),\n6176 (2024)\n[38] Ghareeb, A.E., Chang, B., Mitchener, L., Yiu, A., Szostkiewicz, C.J., Laurent,\nJ.M., Razzak, M.T., White, A.D., Hinks, M.M., Rodriques, S.G.: Robin: A multi-\nagent system for automating scientific discovery. arXiv preprint arXiv:2505.13400\n(2025)\n[39] Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ron-\nneberger, O., Willmore, L., Ballard, A.J., Bambrick, J., et al.: Accurate structure\nprediction of biomolecular interactions with alphafold 3. Nature 630(8016),\n493–500 (2024)\n[40] Brahmavar, S.B., Srinivasan, A., Dash, T., Krishnan, S.R., Vig, L., Roy, A.,\nAduri, R.: Generating novel leads for drug discovery using llms with logical\nfeedback. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol.\n38, pp. 21–29 (2024)\n[41] Jiménez-Luna, J., Grisoni, F., Schneider, G.: Drug discovery with explainable\nartificial intelligence. Nature Machine Intelligence 2(10), 573–584 (2020)\n[42] King, R.D., Whelan, K.E., Jones, F.M., Reiser, P.G., Bryant, C.H., Muggleton,\nS.H., Kell, D.B., Oliver, S.G.: Functional genomic hypothesis generation and\n41\n"}, {"page": 42, "text": "Table C12: Performance comparison of different solvers for the nnUnet\nmodel on the BraTS dataset [82].\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.1667\n1.0000\n0.6667\n1.0000\n82.45\n10\nBS\n0.0333\n0.3786\n0.9333\n1.0000\n82.45\n13\nLlama3.3-70b\n0.1333\n0.3786\n0.7333\n1.0000\n82.45\n11\nQwen2.5-7b\n0.1000\n0.3786\n0.8000\n1.0000\n82.45\n12\nQwen2.5-14b\n0.4333\n0.0698\n0.5333\n0.6000\n82.45\n1\nQwen2.5-32b\n0.3333\n0.3118\n0.5333\n0.8000\n82.45\n3\nQwen2.5-72b\n0.2333\n0.0883\n0.7333\n0.8000\n82.45\n9\nDeepSeek-R1-7b\n0.2995\n0.0481\n1.0000\n0.4000\n82.41\n5\nDeepSeek-R1-14b\n0.2940\n0.0406\n1.0000\n0.4000\n82.00\n7\nDeepSeek-R1-32b\n0.2667\n0.0839\n0.6667\n0.8000\n82.45\n8\nDeepSeek-R1-70b\n0.3333\n0.3126\n0.5333\n0.8000\n82.45\n3\nGPT4-o3-mini\n0.4000\n0.2347\n0.6000\n0.6000\n82.45\n2\nGPT4-o3\n0.2984\n0.0484\n1.0000\n0.4000\n82.33\n6\nexperimentation by a robot scientist. Nature 427(6971), 247–252 (2004)\n[43] Mandal, I., Soni, J., Zaki, M., Smedskjaer, M.M., Wondraczek, K., Wondraczek,\nL., Gosvami, N.N., Krishnan, N.A.: Evaluating large language model agents for\nautomation of atomic force microscopy. Nature Communications 16(1), 9104\n(2025)\n[44] Audran-Reiss, A., EstapÃŠ, J.A., Hambardzumyan, K., Budhiraja, A., Josifoski,\nM., Toledo, E., Hazra, R., Magka, D., Shvartsman, M., Pathak, P., et al.: What\ndoes it take to be a good ai research agent? studying the role of ideation diversity.\narXiv preprint arXiv:2511.15593 (2025)\n[45] Gottweis, J., Weng, W.-H., Daryin, A., Tu, T., Palepu, A., Sirkovic, P.,\nMyaskovsky, A., Weissenberger, F., Rong, K., Tanno, R., et al.: Towards an ai\nco-scientist. arXiv preprint arXiv:2502.18864 (2025)\n[46] Li, X., Wang, S., Zeng, S., Wu, Y., Yang, Y.: A survey on llm-based multi-agent\nsystems: workflow, infrastructure, and challenges. Vicinagearth 1(1), 9 (2024)\n[47] Yamada, Y., Lange, R.T., Lu, C., Hu, S., Lu, C., Foerster, J., Clune, J., Ha, D.:\nThe ai scientist-v2: Workshop-level automated scientific discovery via agentic\ntree search. arXiv preprint arXiv:2504.08066 (2025)\n[48] Nathani, D., Madaan, L., Roberts, N., Bashlykov, N., Menon, A., Moens, V.,\n42\n"}, {"page": 43, "text": "Table C13: Performance comparison of different solvers for the Graph-\nSAGE model on the imbalanced node classification task.\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.1000\n1.0000\n0.8000\n1.0000\n89.28\n13\nBS\n0.2727\n0.8982\n0.4545\n1.0000\n89.28\n8\nLlama3.3-70b\n0.1591\n0.8982\n0.6818\n1.0000\n89.28\n9\nQwen2.5-7b\n0.2880\n0.0928\n1.0000\n0.4091\n89.12\n6\nQwen2.5-14b\n0.2894\n0.0890\n1.0000\n0.4091\n89.15\n5\nQwen2.5-32b\n0.2880\n0.3564\n1.0000\n0.4091\n89.12\n6\nQwen2.5-72b\n0.1364\n0.8982\n0.7273\n1.0000\n89.28\n12\nDeepSeek-R1-7b\n0.3116\n0.3083\n1.0000\n0.3636\n89.15\n3\nDeepSeek-R1-14b\n0.1591\n0.8982\n0.6818\n1.0000\n89.28\n9\nDeepSeek-R1-32b\n0.1591\n0.8657\n0.7273\n0.9545\n89.28\n11\nDeepSeek-R1-70b\n0.2955\n0.7096\n0.5909\n0.8182\n89.28\n4\nGPT4-o3-mini\n0.7727\n0.1463\n0.0455\n0.4091\n89.282\n1\nGPT4-o3\n0.7727\n0.2540\n0.1818\n0.2727\n89.282\n1\nBudhiraja, A., Magka, D., Vorotilov, V., Chaurasia, G., et al.: Mlgym: A new\nframework and benchmark for advancing ai research agents. arXiv preprint\narXiv:2502.14499 (2025)\n[49] Huang, Q., Vora, J., Liang, P., Leskovec, J.: Mlagentbench: Evaluating language\nagents on machine learning experimentation. arXiv preprint arXiv:2310.03302\n(2023)\n[50] Liao, R., Qiu, J., Chen, X., Li, X.: Llm4eo: Large language model for evolutionary\noptimization in flexible job shop scheduling. arXiv preprint arXiv:2511.16485\n(2025)\n[51] Jiang, Q., Karniadakis, G.: Agenticsciml: Collaborative multi-agent systems for\nemergent discovery in scientific machine learning. arXiv preprint arXiv:2511.07262\n(2025)\n[52] Meng, S., Wang, Y., Yang, C.-F., Peng, N., Chang, K.-W.: Llm-a*: Large language\nmodel enhanced incremental heuristic search on path planning. In: EMNLP\n(Findings) (2024)\n[53] Kochnev, R., Goodarzi, A.T., Bentyn, Z.A., Ignatov, D., Timofte, R.: Optuna vs\ncode llama: Are llms a new paradigm for hyperparameter tuning? arXiv preprint\narXiv:2504.06006 (2025)\n43\n"}, {"page": 44, "text": "Table C14: Performance comparison of different solvers for bioactivity\nprediction on Chagas EP20 dataset [95]\nSolver\nScore↑AUPD ↓Best-Time↓Stop-Time↓Best Result↑Rank\nGS\n0.4833\n1.0000\n0.0333\n1.0000\n0.754\n11\nBS\n0.3333\n0.8941\n0.3333\n1.0000\n0.754\n13\nLlama3.3-70b\n0.4630\n0.8941\n0.0741\n1.0000\n0.754\n12\nQwen2.5-7b\n0.9074\n0.0772\n0.0741\n0.1111\n0.754\n1\nQwen2.5-14b\n0.8889\n0.0649\n0.1111\n0.1111\n0.754\n2\nQwen2.5-32b\n0.6296\n0.4231\n0.0741\n0.6667\n0.754\n9\nQwen2.5-72b\n0.5741\n0.6799\n0.0741\n0.7778\n0.754\n10\nDeepSeek-R1-7b\n0.7778\n0.1104\n0.2222\n0.2222\n0.754\n4\nDeepSeek-R1-14b\n0.7037\n0.5510\n0.0741\n0.5185\n0.754\n7\nDeepSeek-R1-32b\n0.6852\n0.3405\n0.0741\n0.5556\n0.754\n8\nDeepSeek-R1-70b\n0.7778\n0.3160\n0.1111\n0.3333\n0.754\n4\nGPT4-o3-mini\n0.7407\n0.3546\n0.1852\n0.3333\n0.754\n6\nGPT4-o3\n0.8148\n0.0622\n0.0370\n0.3333\n0.754\n3\n[54] Hao, S., Gu, Y., Ma, H., Hong, J.J., Wang, Z., Wang, D.Z., Hu, Z.: Reasoning with\nlanguage model is planning with world model. arXiv preprint arXiv:2305.14992\n(2023)\n[55] Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang, P.: Generative agents:\nInteractive simulacra of human behavior. arXiv preprint arXiv:2304.03442 (2023)\n[56] Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A next-\ngeneration hyperparameter optimization framework. In: The 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pp. 2623–2631\n(2019)\n[57] Yadan, O.: Hydra - A framework for elegantly configuring complex applications.\nGithub (2019). https://github.com/facebookresearch/hydra\n[58] Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., Hutter,\nF.: Efficient and robust automated machine learning. In: Advances in Neural\nInformation Processing Systems 28 (2015), pp. 2962–2970 (2015)\n[59] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n[60] Watanabe, S.: Tree-structured parzen estimator: Understanding its algorithm\n44\n"}, {"page": 45, "text": "components and their roles for better empirical performance. arXiv preprint\narXiv:2304.11127 (2023)\n[61] Akiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M.: Optuna: A next-\ngeneration hyperparameter optimization framework. In: Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery and Data\nMining (2019)\n[62] Huang, B.: [03/24] Boston Housing Dataset. https://kaggle.com/competitions/\n2403-boston-housing-dataset. Kaggle (2020)\n[63] Kandhro, I.A., Jumani, S.Z., Ali, F., Shaikh, Z.U., Arain, M.A., Shaikh, A.A.: Per-\nformance analysis of hyperparameters on a sentiment analysis model. Engineering,\nTechnology & Applied Science Research 10(4), 6016–6020 (2020)\n[64] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781 (2013)\n[65] Orús, R.: A practical introduction to tensor networks: Matrix product states\nand projected entangled pair states. Annals of physics 349, 117–158 (2014)\n[66] Li, Z., Huang, C., Wang, X., Hu, H., Wyeth, C., Bu, D., Yu, Q., Gao, W., Liu, X.,\nLi, M.: Lossless data compression by large models. Nature Machine Intelligence,\n1–6 (2025)\n[67] Berezutskii, A., Liu, M., Acharya, A., Ellerbrock, R., Gray, J., Haghshenas, R.,\nHe, Z., Khan, A., Kuzmin, V., Lyakh, D., et al.: Tensor networks for quantum\ncomputing. Nature Reviews Physics, 1–13 (2025)\n[68] Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit\nneural representations with periodic activation functions. Advances in neural\ninformation processing systems 33, 7462–7473 (2020)\n[69] He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders\nare scalable vision learners. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16000–16009 (2022)\n[70] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 770–778 (2016)\n[71] Zimmer, L., Lindauer, M., Hutter, F.: Auto-pytorch: Multi-fidelity metalearning\nfor efficient and robust autodl. IEEE transactions on pattern analysis and machine\nintelligence 43(9), 3079–3090 (2021)\n[72] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a\nself-configuring method for deep learning-based biomedical image segmentation.\n45\n"}, {"page": 46, "text": "Nature methods 18(2), 203–211 (2021)\n[73] Isensee, F., Wald, T., Ulrich, C., Baumgartner, M., Roy, S., Maier-Hein, K.,\nJaeger, P.F.: nnu-net revisited: A call for rigorous validation in 3d medical image\nsegmentation. In: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, pp. 488–498 (2024). Springer\n[74] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for\nbiomedical image segmentation. In: International Conference on Medical Image\nComputing and Computer-assisted Intervention, pp. 234–241 (2015). Springer\n[75] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a\nself-configuring method for deep learning-based biomedical image segmentation.\nNature methods 18(2), 203–211 (2021)\n[76] Roy, S., Koehler, G., Ulrich, C., Baumgartner, M., Petersen, J., Isensee, F.,\nJaeger, P.F., Maier-Hein, K.H.: Mednext: transformer-driven scaling of convnets\nfor medical image segmentation. In: International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pp. 405–415 (2023). Springer\n[77] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation processing systems 30 (2017)\n[78] Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V.,\nHatamizadeh, A.: Self-supervised pre-training of swin transformers for 3d medi-\ncal image analysis. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 20730–20740 (2022)\n[79] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state\nspaces. arXiv preprint arXiv:2312.00752 (2023)\n[80] Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for\nbiomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024)\n[81] Landman, B., Xu, Z., Igelsias, J., Styner, M., Langerak, T., Klein, A.: Miccai\nmulti-atlas labeling beyond the cranial vault–workshop and challenge. In: Proc.\nMICCAI Multi-Atlas Labeling Beyond Cranial Vault—Workshop Challenge, vol.\n5, p. 12 (2015)\n[82] LaBella, D., Adewole, M., Alonso-Basanta, M., Altes, T., Anwar, S.M., Baid, U.,\nBergquist, T., Bhalerao, R., Chen, S., Chung, V., Conte, G.-M., Dako, F., Eddy,\nJ., Ezhov, I., Godfrey, D., Hilal, F., Familiar, A., Farahani, K., Iglesias, J.E.,\nJiang, Z., Johanson, E., Kazerooni, A.F., Kent, C., Kirkpatrick, J., Kofler, F.,\nLeemput, K.V., Li, H.B., Liu, X., Mahtabfar, A., McBurney-Lin, S., McLean,\nR., Meier, Z., Moawad, A.W., Mongan, J., Nedelec, P., Pajot, M., Piraud, M.,\nRashid, A., Reitman, Z., Shinohara, R.T., Velichko, Y., Wang, C., Warman, P.,\n46\n"}, {"page": 47, "text": "Wiggins, W., Aboian, M., Albrecht, J., Anazodo, U., Bakas, S., Flanders, A.,\nJanas, A., Khanna, G., Linguraru, M.G., Menze, B., Nada, A., Rauschecker,\nA.M., Rudie, J., Tahon, N.H., Villanueva-Meyer, J., Wiestler, B., Calabrese,\nE.: The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023:\nIntracranial Meningioma (2023)\n[83] Jiang, H., Wang, J., Cong, W., Huang, Y., Ramezani, M., Sarma, A., Dokholyan,\nN.V., Mahdavi, M., Kandemir, M.T.: Predicting protein–ligand docking structure\nwith graph neural network. Journal of chemical information and modeling 62(12),\n2923–2932 (2022)\n[84] Jiang, D., Hsieh, C.-Y., Wu, Z., Kang, Y., Wang, J., Wang, E., Liao, B., Shen, C.,\nXu, L., Wu, J., et al.: Interactiongraphnet: a novel and efficient deep graph repre-\nsentation learning framework for accurate protein–ligand interaction predictions.\nJournal of medicinal chemistry 64(24), 18209–18232 (2021)\n[85] Shi, W., Yang, H., Xie, L., Yin, X.-X., Zhang, Y.: A review of machine learning-\nbased methods for predicting drug–target interactions. Health Information\nScience and Systems 12(1), 30 (2024)\n[86] Gu, S., Bao, L., Yang, Y., Zhao, Y., Tong, H.H.Y., Liu, L., Liu, H., Hou, T.,\nKang, Y.: Amgc is a multiple-task graph neutral network for epigenetic target\nprofiling. Cell Reports Physical Science 5(3) (2024)\n[87] Bongini, P., Bianchini, M., Scarselli, F.: Molecular generative graph neural\nnetworks for drug discovery. Neurocomputing 450, 242–252 (2021)\n[88] Wang, X., Ma, Y., Wang, Y., Jin, W., Wang, X., Tang, J., Jia, C., Yu, J.: Traffic\nflow prediction via spatial temporal graph neural network. In: Proceedings of\nthe Web Conference 2020, pp. 1082–1092 (2020)\n[89] Sharma, A., Sharma, A., Nikashina, P., Gavrilenko, V., Tselykh, A., Bozhenyuk,\nA., Masud, M., Meshref, H.: A graph neural network (gnn)-based approach for\nreal-time estimation of traffic speed in sustainable smart cities. Sustainability\n15(15), 11893 (2023)\n[90] Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J.W., Songhori, E., Wang, S., Lee,\nY.-J., Johnson, E., Pathak, O., Nova, A., et al.: A graph placement methodology\nfor fast chip design. Nature 594(7862), 207–212 (2021)\n[91] Yang, S., Yang, Z., Li, D., Zhang, Y., Zhang, Z., Song, G., Hao, J.: Versatile\nmulti-stage graph neural network for circuit representation. Advances in Neural\nInformation Processing Systems 35, 20313–20324 (2022)\n[92] Zhao, T., Zhang, X., Wang, S.: Graphsmote: Imbalanced node classification\non graphs with graph neural networks. In: Proceedings of the 14th ACM\nInternational Conference on Web Search and Data Mining, pp. 833–841 (2021)\n47\n"}, {"page": 48, "text": "[93] Yan, L., Zhang, S., Li, B., Zhou, M., Huang, Z.: Unreal: Unlabeled nodes\nretrieval and labeling for heavily-imbalanced node classification. arXiv preprint\narXiv:2303.10371 (2023)\n[94] Korotcov, A., Tkachenko, V., Russo, D.P., Ekins, S.: Comparison of deep learning\nwith multiple machine learning methods and metrics using diverse drug discovery\ndata sets. Molecular pharmaceutics 14(12), 4462–4475 (2017)\n[95] Ekins, S., Siqueira-Neto, J., McCall, L.-I., Sarker, M., Yadav, M., Ponder, E.L.,\nKallel, E.A., Kellar, D., Chen, S., Arkin, M., et al.: Machine learning models\nand pathway genome data base for trypanosoma cruzi drug discovery. PLoS\nneglected tropical diseases 9(6), 0003878 (2015)\n[96] Weininger, D.: Smiles, a chemical language and information system. 1. introduc-\ntion to methodology and encoding rules. Journal of chemical information and\ncomputer sciences 28(1), 31–36 (1988)\n[97] Duvenaud, D.K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T.,\nAspuru-Guzik, A., Adams, R.P.: Convolutional networks on graphs for learning\nmolecular fingerprints. Advances in neural information processing systems 28\n(2015)\n[98] Fliri, A.F., Loging, W.T., Thadeio, P.F., Volkmann, R.A.: Biological spectra\nanalysis: linking biological activity profiles to molecular structure. Proceedings\nof the National Academy of Sciences 102(2), 261–266 (2005)\n[99] Chan, J.S., Chowdhury, N., Jaffe, O., Aung, J., Sherburn, D., Mays, E., Starace,\nG., Liu, K., Maksin, L., Patwardhan, T., et al.: Mle-bench: Evaluating machine\nlearning agents on machine learning engineering. arXiv preprint arXiv:2410.07095\n(2024)\n[100] Hill, T.P.: Knowing when to stop: How to gamble if you must-the mathematics\nof optimal stopping. American Scientist 97(2), 126–133 (2009)\n[101] Ferguson, T.S.: Who solved the secretary problem? Statistical science 4(3),\n282–289 (1989)\n48\n"}]}