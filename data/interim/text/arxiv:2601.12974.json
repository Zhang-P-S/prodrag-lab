{"doc_id": "arxiv:2601.12974", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.12974.pdf", "meta": {"doc_id": "arxiv:2601.12974", "source": "arxiv", "arxiv_id": "2601.12974", "title": "Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios", "authors": ["Hongyang Ma", "Tiantian Gu", "Huaiyuan Sun", "Huilin Zhu", "Yongxin Wang", "Jie Li", "Wubin Sun", "Zeliang Lian", "Yinghong Zhou", "Yi Gao", "Shirui Wang", "Zhihui Tang"], "published": "2026-01-19T11:36:39Z", "updated": "2026-01-19T11:36:39Z", "summary": "The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping \"Guideline Adherence\" versus \"Decision Quality\" reveals a prevalent \"High Efficacy, Low Safety\" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.12974v1", "url_pdf": "https://arxiv.org/pdf/2601.12974.pdf", "meta_path": "data/raw/arxiv/meta/2601.12974.json", "sha256": "bc3fb09b8e79844b9a1af68d28230d72e0a5879aece00a69ef629471cafffc9e", "status": "ok", "fetched_at": "2026-02-18T02:21:09.605976+00:00"}, "pages": [{"page": 1, "text": "Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic\nDental Clinical Scenarios\nHongyang Ma1, 2#, Tiantian Gu3#, Huaiyuan Sun3#, Huilin Zhu2, 4#, Yongxin Wang3#, Jie Li1, Wubin Sun3, Zeliang Lian3,\nYinghong Zhou4, Yi Gao5*, Shirui Wang3*, Zhihui Tang2*\n1The Second Affiliated Hospital of Harbin Medical University, Heilongjiang Province, China\n2Peking University School and Hospital of Stomatology, Beijing, China\n3Medlinker Intelligent and Digital Technology Co., Ltd, Beijing, China\n4School of Dentistry, Centre for Orofacial Regeneration, Rehabilitation and Reconstruction (COR3), The University of\nQueensland, Brisbane, Australia\n5Department of Stomatology, Beijing Xicheng District Health Care Hospital for Mothers and Children, Beijing, China\n*Corresponding authors:\nYi Gao (13439023411@126.com),\nShirui Wang (wsr@medlinker.com),\nZhihui Tang (zhihui_tang@126.com),\n# These authors contributed equally to this work\nAbstract\nThe transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents\ndemands a shift in evaluation—from static accuracy to dynamic behavioral reliability. To explore this boundary in\ndentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we\npresent\nthe\nStandardized\nClinical\nManagement\n&\nPerformance\nEvaluation\n(SCMPE)\nbenchmark,\nwhich\ncomprehensively\nassesses\nperformance\nfrom\nknowledge-oriented\nevaluations\n(static\nobjective\ntasks)\nto\nworkflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models\ndemonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues,\nidentifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active\ninformation gathering and dynamic state tracking. Mapping \"Guideline Adherence\" versus \"Decision Quality\" reveals\na prevalent \"High Efficacy, Low Safety\" risk in general models. Furthermore, we quantify the impact of\nRetrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic\nworkflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge\nalone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the\ncapability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge\nand safe, autonomous clinical practice.\n"}, {"page": 2, "text": "Introduction\nIn recent years, the trajectory of Large Language Models (LLMs) in healthcare has rapidly shifted from passive\ninformation retrieval to the development of autonomous clinical agents capable of managing critical stages,\nincluding history taking, diagnostic suggestions, treatment planning, and follow-up management. Notably, the\nemergence of specialized multimodal models, such as DentVLM, has demonstrated the potential to elevate junior\npractitioners' diagnostic performance to expert levels and significantly reduce diagnostic time by integrating visual\nand linguistic information1. Concurrently, comprehensive evaluations indicate that LLMs perform strongly in clinical\nnote generation and patient communication2. These capabilities align naturally with dentistry, a field characterized\nby its predominantly outpatient nature, highly standardized workflows, and frequent interdisciplinary collaboration,\nmaking it an ideal testbed for agentic deployment3. Beyond operational efficiency, the deployment of AI agents in this\nfield holds profound significance for patients: it enables them to obtain high-quality diagnostic and treatment\nrecommendations directly via online platforms. This accessibility breaks down information barriers, empowering\npatients to truly participate in clinical decision-making alongside professionals.\nTo rigorously assess the foundational competencies required for such agents, the academic community has\nestablished a series of benchmarks evolving from static knowledge to dynamic reasoning. Foundational datasets\nlike MedQA (USMLE-based) and MedMCQA have served as the gold standard for evaluating the \"knowledge brain\"\nof potential agents through large-scale multiple-choice questions4, 5. Moving beyond rote memorization, benchmarks\nsuch as PubMedQA have been introduced to evaluate models' abilities to reason over biomedical literature and\nevidence, a crucial skill for evidence-based practice6. Furthermore, leading research initiatives, such as the\nMultiMedQA suite used to evaluate Med-PaLM, have aggregated these datasets to set new standards for\n\"expert-level\" performance in general medicine7.\nHowever, a critical \"Knowledge-Action Gap\" remains unaddressed in current evaluation paradigms. Existing\nsystems predominantly focus on general Q&A or single-point tasks, failing to capture the performance degradation\nthat occurs when models transition from selecting static options to managing dynamic patient interactions8.\nConsequently, two critical questions regarding the feasibility of dental agents remain unresolved: First, does the LLM\npossess a robust \"knowledge brain\" that is factually accurate? Second, can this knowledge be safely translated into\n\"clinical execution\" within a multi-turn workflow?\nRegarding the first question—knowledge retention versus application—literature reveals a paradox of \"high exam\nscores versus limited practical capability.\" Systematic reviews indicate that high-performance models like GPT-4\nhave achieved accuracy rates in dental licensing examinations that surpass the average human candidate9, 10.\nFurthermore, in the domain of educational content production, board-style questions generated by LLMs have\nachieved difficulty and discrimination indices comparable to those created by human experts11. Yet, these \"high\nscores\" do not directly translate to reliable agentic competence. Studies highlight that once tasks involve\nopen-ended clinical inquiries, significant disparities emerge, often accompanied by \"confident but wrong\"\nhallucinations12-14. This necessitates a re-evaluation of the \"knowledge brain\" not just by score, but by its stability\nacross guidelines and languages.\nRegarding the second question—the balance between safety and efficacy in execution—current evaluations reveal\nthe limitations of pursuing \"accuracy\" alone. In clinical practice, safety is embodied in identifying contraindications\n"}, {"page": 3, "text": "(e.g., antibiotic prophylaxis for infective endocarditis), while efficacy focuses on regimen optimization. The\nCRAFT-MD framework15, found that even diagnostically accurate models exhibit critical limitations in \"conversational\nreasoning,\" often failing to ask follow-up questions to elicit key information. Such passivity poses unacceptable risks\nin real-world flows, where performance is often unstable and dependent on prompt engineering16. Furthermore,\nwhile specific models demonstrate strong reasoning in complex implantology cases, the risk of misleading advice\nremains high without a mechanism to define safe operating boundaries17. This suggests that current assessments\nhave not yet sufficiently decomposed and weighted \"safety-critical points\" against \"efficacy optimization points.\"\nTo bridge this gap, this study presents the Standardized Clinical Management & Performance Evaluation (SCMPE)\nbenchmark. Addressing the linguistic bias inherent in mainstream benchmarks9, 18-21, we construct a bilingual\n(Chinese-English)\nevaluation\nprotocol.\nDrawing\non\nthe\ninteractive\nconcepts\nof\nCRAFT-MD\nand\nthe\nhallucination-focus of Med-HALT, we seek not only to quantify the \"Knowledge-Action Gap\" by contrasting objective\nexam performance with multi-turn dialogue scores, but also to map the decision boundaries of LLMs using\nfine-grained safety and effectiveness metrics, and to validate the impact of Retrieval-Augmented Generation (RAG)\nin optimizing these boundaries. This approach intends to provide empirical evidence with greater external validity,\nfacilitating the transition of LLMs from \"exam high-achievers\" to \"reliable clinical copilots”.\nMethods\nData Acquisition and Construction of the Bilingual Repository\nTo empirically chart the capability boundaries of LLMs in dentistry, we first engineered a comprehensive bilingual\nrepository designed to stress-test clinical-agent readiness from knowledge-oriented evaluations to workflow-based\nsimulations, under a unified clinical-alignment rubric. This repository was curated from three distinct sources to\nensure coverage from theoretical foundations to real-world complexities. For the assessment of static memory and\nreasoning, we aggregated a vast collection of Bilingual MCQs from the MedMCQA5 dental subset and the Chinese\nNational Dental Licensing Examination database. Following a rigorous filtration process to remove non-dental\nentries and verify explanation logic, we retained 2,192 English and 2,066 Chinese items. To evaluate\nevidence-based retrieval capabilities, we compiled 60 authoritative clinical practice guidelines, comprising 19\nChinese and 41 English documents. For workflow-based simulation aimed at approximating real-world deployment,\nwe collected 118 Real-world Case Reports (68 Chinese, 50 English) covering diverse dental sub-specialties. Notably,\nto minimize data contamination, the collected clinical guidelines and real-world case reports were not sourced from\npublic general datasets or common internet medical corpora. This reduces the likelihood of their presence in the\nLLMs' pre-training data, ensuring the evaluation targets generalization capability rather than rote memorization.\nThese raw data sources (eTable 1) formed the foundational substrate for our subsequent structured annotation.\nEstablishment of Clinical Alignment Criteria via Delphi Consensus\nTo transform raw medical data into a rigorous evaluation standard, we defined the operational boundaries and safety\nguardrails required for an autonomous agent through a three-round Delphi consensus process. The objective was to\nestablish a \"Clinical Alignment\" framework that quantifies the consequence of agentic error. The initial protocol\nproposed 39 distinct performance indicators. In each round, an expert panel provided retention judgments,\nquantitative criticality weights, and qualitative refinement proposals. We synthesized feedback between rounds,\n"}, {"page": 4, "text": "integrating statistical stability indicators to iteratively refine the agent’s compliance requirements. The process\nconverged after three rounds, crystallizing into 37 validated alignment metrics. Based on the consensus median\nscores, we instantiated a Risk Stratification Framework that maps each metric to a 1–5 severity scale. This scale\ndelineates the impact of error, ranging from Weight 1 (Context-dependent/limited risk) to Weight 5 (Life-threatening\nor catastrophic failure), thereby providing a weighted ground truth for the subsequent scoring of agent behaviors\n(detailed in eTable 2). High-risk and emergency topics achieved the highest scores with stable consensus:\n“Emergency management of airway obstruction,” “Contraindications for extraction,” “Bone volume assessment for\nimplants,” “Decision for tooth preservation vs extraction,” “Radiographic assessment for impacted tooth extraction,”\nand “Indications assessment for implant surgery” fell within [4.5,5] (score 5). Items such as “Drug–drug interaction\nalerts,” “Early signs of LAST,” and “Informed consent essentials” fell within [4,4.5) (score 4). Some effectiveness or\nfoundational diagnostic items (e.g., “Guideline-based differential diagnosis,” “Accuracy of caries grading,”\n“Systemic–oral interaction risk assessment”) had medians in [0,3) or [3,3.5), scoring 1–2. Most items achieved\nIQR≤1 by rounds two to three, with the consensus median as the final representative weight, indicating good\nconvergence and clear prioritization.\nStructured Annotation and Test Set Generation\nLeveraging the 37 validated alignment metrics, we employed a \"Human-in-the-loop\" knowledge engineering\nworkflow to convert the raw repository into a structured evaluation suite. For knowledge-oriented evaluations\n(Guideline-based Open QA), the collected guidelines underwent semantic chunking and were mapped to the\nconsensus checkpoints. We utilized Qwen (qwen3-max-preview) to draft initial QA pairs per chunk, which then\nunderwent rigorous clinician review to ensure the reference answers aligned strictly with the 37 consensus metrics.\nThis yielded 205 complex queries covering 443 checkpoint instances and 1,225 rubric points. For workflow-based\nsimulations (simulated deployment based on real-world cases), the real-world case reports were destructured into\nclinical facts (case vignette) and re-mapped to the consensus metrics to create high-fidelity simulation scenarios. All\nmappings were clinician-verified, resulting in a dataset of 498 checkpoint instances and 1,318 rubric points. This\nrigorous annotation process ensured that every evaluation point in the test set was directly traceable to the\nexpert-defined safety and effectiveness standards (eTable 3).\nExperimental Setup: Multi-turn Dialogue Simulation\nTo assess the feasibility of autonomous deployment, we configured an experimental environment that mirrors the\ndynamic nature of clinical practice. We deployed Qwen as a Simulated Patient (SP) LLM to engage the target LLM\nin multi-turn dialogues. The SP LLM ingested the structured case vignette and operated under strict prompt\nconstraints to prevent role-breaking or information leakage. The target LLM was tasked with executing a complete\nclinical workflow—from history taking to iterative planning. The target LLM was instructed to autonomously terminate\nthe dialogue only when it had gathered sufficient information and was 100% confident in the diagnosis (see Prompts\nin Supplementary Note 1), subsequently outputting a structured management plan. This mechanism forces the\nmodel to judge information saturation, thereby directly evaluating its active history-taking and information-gathering\ncapabilities.\nFurthermore, to deconstruct the capability gap between \"inquiry\" and \"reasoning,\" we designed a control experiment.\nIn this setting, the full Case Vignette held by the Simulated Patient (SP) was directly fed into the target LLM, allowing\n"}, {"page": 5, "text": "it to bypass the inquiry phase and generate a management plan immediately. By comparing scores between the\n\"Multi-turn Inquiry Mode\" and the \"Direct Input Mode,\" we can isolate performance deficits caused by information\ngaps due to poor inquiry strategies from those stemming from intrinsic flaws in clinical reasoning logic.\nUnified Scoring Mechanism: Safety Guardrails versus Clinical Effectiveness\nWe implemented a unified rubric-based scoring framework tailored to the dual nature of our evaluation metrics. For\nSafety Checkpoints, we adopted a \"zero-tolerance\" principle to evaluate the agent's adherence to critical guardrails.\nThis was operationalized via a binary constraint mechanism where a score of 1 is awarded only if pass criteria are\nmet and no fail criteria are triggered; otherwise, the score is 0. For Effectiveness Checkpoints, we utilized a dynamic\nscoring scheme to measure the quality of clinical execution. Each checkpoint contained multiple rule-based criteria\nweighted from -10 to 10. These scores were aggregated and normalized to a range of [0, 1] at the checkpoint level.\nWhile the weighted normalized total score reflects the model's macro-logical capability in handling clinical tasks, we\nemphasize the independence of Safety Guardrails. A safety score of 0 serves as a \"clinical veto,\" indicating that\nregardless of how high the efficacy score is, the model poses an unacceptable risk in that scenario. This\ndual-scoring approach allows for a granular analysis of the trade-off between \"Guideline Adherence\" (Safety) and\n\"Clinical Decision Quality\" (Effectiveness).\nNative-Language Evaluation and Regional Alignment\nDistinct from standard multilingual benchmarks that rely on parallel translation, our protocol emphasizes\nNative-Source Data Curation. We aggregated independent datasets for Chinese and English contexts—spanning\nexams, guidelines, and case reports—to explicitly evaluate the LLM's mastery of region-specific medical norms and\ndistinct clinical practice guidelines. This design avoids translation artifacts and ensures the Agent is tested against\nthe specific regulatory and cultural frameworks of each healthcare system. During deployment simulations, we\nenforced a Language-Concordant Protocol: the LLM was constrained to process inputs and generate reasoning\nchains strictly in the source language. This methodological separation enables a stratified analysis of the LLM's\n\"Bilingual Clinical Competency,\" verifying its feasibility as a localized clinical assistant capable of adhering to the\nspecific standards of care in diverse linguistic environments.\nExternal Knowledge Integration via RAG\nTo mitigate the inherent limitations of parametric memory and enhance adherence to clinical protocols, we\nimplemented a Retrieval-Augmented Generation (RAG) architecture as a core \"External Tool\" for the Agent. The\nexternal knowledge base was constructed using the semantic guideline chunks derived during the Data\nConstruction phase, ensuring a closed-loop validation of authoritative sources. We evaluated the impact of this\nknowledge augmentation from knowledge-oriented evaluations (Guideline-based Open QA) to workflow-based\nsimulations (real-world-case-based dialogue simulations). Specifically, within the multi-turn dialogue simulations, the\nRAG mechanism was triggered at each interaction turn, retrieving relevant clinical protocols to inform the LLM's\nimmediate response. This granular integration allows us to examine whether real-time knowledge injection\nenhances the agent's active inquiry and history-taking capabilities, rather than merely refining the final diagnostic\noutput. This comparative analysis aims to quantify the efficacy of external retrieval in grounding the Agent’s\ndecision-making process, specifically assessing whether dynamic access to guideline protocols can optimize\nperformance in complex, multi-turn clinical workflows compared to the Agent's intrinsic reasoning capabilities alone.\n"}, {"page": 6, "text": "Agent Reliability and Stochasticity Profiling\nGiven the inherent non-determinism of generative agents, ensuring consistent performance is critical for clinical\nsafety. We employed a Worst-at-k methodology to quantify the lower bound of the LLM's performance reliability and\nassess the risk of sporadic, low-quality outputs (hallucinations or reasoning failures). We constructed a stratified\nreliability probe set by randomly selecting 74 representative cases (2 per assessment criterion) from the broader\nrepository. To capture the distribution of potential behaviors, the LLM was tasked with independently resolving each\ncase 10 times, generating a spectrum of 10 distinct performance scores per scenario: {s1, s2,…, s10}.\nTo rigorously stress-test the LLM's safety floor, we computed the Worst@k metric. For a given iteration count\nk(ranging from 1 to 10), we simulated a \"worst-case\" deployment scenario by randomly sampling k outcomes from\nthe generated set and isolating the minimum score. The aggregate reliability metric is defined as:\n�����@�=\n1\n�∑�=1\n�[����∈�������(��)(�)]\nwhere M denotes the total test cases (M=74), Rj represents the response set for the j-th case, and Samplek(Rj)\nindicates the subset of k scores sampled without replacement. By analyzing the performance degradation curves\nacross increasing k values, we derived a quantitative measure of the LLM's stability, distinguishing between robust\nclinical reasoning and fragile, high-variance behavior.\nResults\nEstablishment of the Evaluation Framework and Global Performance Overview\nTo empirically chart the capability boundaries of dental LLMs, we executed a three-stage research design as\nillustrated in Figure 1, progressing from raw data acquisition to structured evaluation. First, regarding Data\nAcquisition, we aggregated a comprehensive bilingual repository comprising 4,258 standardized exam items (2,192\nEnglish, 2,066 Chinese), 60 authoritative clinical practice guidelines, and 118 real-world case reports (eTable 1).\nSecond, to establish a \"Clinical Alignment\" standard, a three-round Delphi Consensus process was conducted. This\nprocess crystallized the evaluation criteria into 37 validated metrics, stratified into 16 Safety-Binary guardrails and\n21 Effectiveness-Quantitative indicators, and instantiated a Risk Stratification Framework (Weights 1–5) (eTable 2).\nThird, during Test Set Construction, a \"Human-in-the-loop\" annotation workflow mapped the raw repository to these\nconsensus metrics. This yielded a structured evaluation suite consisting of 205 guideline-based queries (covering\n443 checkpoint instances and 1,225 rubric points) for knowledge evaluation and destructured the case reports into\n498 checkpoint instances and 1,318 rubric points for dynamic clinical simulation (eTable 3), both achieving 100%\ncoverage of the 37 consensus indicators. Subsequently, in the LLM Response Generation phase, the workflow\nbifurcated: MCQs and Open-end QAs were directed to the Target LLM to elicit choices with reasoning, while Case\nVignettes initiated a multi-turn dialogue where the Target LLM (acting as a Doctor) interacted with a Simulated\nPatient under restrictive prompts to formulate a treatment plan. Finally, strictly aligned with the Scoring Metrics, a\ndual-scoring mechanism was implemented to operationalize these metrics: a \"zero-tolerance\" binary score (0/1) for\nsafety checkpoints, and a normalized weighted score (mapped to [0, 1]) for effectiveness checkpoints. These criteria\nscores were then synthesized into weighted averages to generate comprehensive analysis charts.\n"}, {"page": 7, "text": "Figure 1. Overview of the SCMPE Framework. The figure and all its elements were designed and assembled using\nMicrosoft PowerPoint graphics and publicly available vector icons obtained from https://www.svgrepo.com/, which\nprovides free and open-license SVG resources that allow both academic and commercial use with modification.\nTo investigate the performance of various LLMs on this framework, we employed DeepSeek-V3.2-Exp (20251117),\nGPT-5.2 (GPT-5.2-2025-12-11), Claude-Sonnet-4.5 (20250929), Gemini-3-Pro (gemini-3-pro-preview-11-2025),\nBaichuan-M2 (Baichuan-M2-32B) and MedGPT (MedGPT-251130, Medlinker) as the test models. Notably, MedGPT\nserves as the representative vertical domain model in this study. We explicitly verified that its pre-training corpus did\nnot include the specific MCQs, guidelines, or case reports constructed for this benchmark. This exclusion is critical\nto rule out data leakage, ensuring that the subsequent analysis of its performance trends reflects genuine domain\ngeneralization and reasoning capabilities rather than rote memorization. All evaluations were conducted within a\ncomparable time window, specifically between November 2025 and December 2025. During the experiments, all\nparameters were kept at their default configurations.\nWe employed GPT-4.1 (gpt-4.1-2025-04-14) as the automated scoring engine. Figure 2 presents the global\nperformance overview across the three resulting task types: MCQs, Open QA (Guideline), and Multi-turn Dialogue\n(Clinical Case). In MCQs, MedGPT achieved the highest Accuracy of 0.944 and Macro-F1 score with 0.855 (Figure\n2a-b). Similarly, in Open QA (Guideline) (Figure 2c), models maintained competitive weighted normalized scores,\nwith MedGPT (0.751) and GPT-5.2 (0.697) demonstrating proficiency in retrieving and synthesizing static protocols.\nHowever, the global statistics reveal a divergent trend in Multi-turn Dialogue (Clinical Case) (Figure 2d): despite the\nhigh theoretical baseline, weighted normalized total scores dropped notably in dynamic interactions. MedGPT—as a\ndomain-specialized (vertical) medical model—still shows the same “Knowledge-Action Gap” degradation pattern\nwhen shifting from static knowledge tasks to dynamic clinical execution, although it remained among the top\n"}, {"page": 8, "text": "performers (0.503), followed closely by GPT-5.2 (0.494). Taken together, these results motivate a cross-task\nanalysis to quantify how performance degrades when moving from MCQs and Open QA (Guideline) to Multi-turn\nDialogue (Clinical Case), i.e., the Knowledge-Action Gap.\nNotably, the generally low ROUGE-L scores observed in eFigure 1 (peaking at only ~0.314) highlight a critical\nmethodological fallacy: evaluating \"reasoning\" capabilities through text overlap metrics is fundamentally flawed.\nSuch text-matching assessments neither provide interpretability for the models' high accuracy in standardized\nexams (MCQs) nor possess any predictive validity for their ability to solve actual problems in complex clinical\nscenarios. This confirms a substantial disconnect between such abstract \"reasoning evaluations\" and true clinical\ncompetence, proving that these metrics fail to reflect the models' decision-making value in real-world medical\nenvironments.\nFigure 2. Comparative Evaluation of LLMs across MCQs, Open QA (Guideline), and Multi-turn Dialogue\n(Clinical Case). MedGPT refers to the MedGPT-251130 version. (a-b) Performance on the Standardized Exam\n(MCQ) task. Panel (a) displays the Accuracy scores. Panel (b) shows the Macro-F1 scores. (c) Performance on the\nGuideline-based Open QA task, measured by weighted normalized scores. (d) Performance on Clinical Case\nsimulations (multi-turn dialogue). Models are ordered by their respective scores in each panel.\nWe assessed the agreement between model-generated scores and human expert ratings using standard\nconcordance statistics. Overall, the alignment was strong, with Spearman ρ in [0.8286–0.9429] across experts,\nsuggesting that the model-based scoring preserves both linear consistency and rank-order consistency with expert\njudgment. These results indicate that the model’s scoring is well-calibrated to expert assessment and can serve as\na reliable proxy for evaluating diagnostic reasoning quality.\nFurthermore, to characterize agentic stochasticity and quantify a conservative“performance floor”for deployment,\nwe conducted reliability profiling using the Worst@k metric (eFigure 2). Across k from 1 to 10, Worst@k scores\ndecrease monotonically for all models, indicating that increasing the number of sampled responses exposes\nnon-negligible low-end failures even when average performance appears competitive. The degradation pattern is\ntask-dependent: models are comparatively more stable in MCQs, show a clearer drop in Guideline-based Open QA,\nand exhibit the steepest deterioration in Multi-turn Dialogue (Clinical Case) — suggesting that long-horizon\ninteraction amplifies variance through missed information gathering, imperfect state tracking, and occasional\nreasoning breakdowns. Notably, top-tier models maintain a higher Worst@k trajectory across tasks, whereas\nweaker models show faster collapse as k increases, highlighting reliability—rather than point estimates alone—as a\n"}, {"page": 9, "text": "key differentiator for safety-critical use.\nQuantifying the Knowledge-Action Gap\nDespite the high baseline competence in static tasks, a cross-task analysis revealed a precipitous decline in\nperformance as models progressed from knowledge-oriented evaluations to workflow-based simulations. Figure 3a\nvisualizes this \"Knowledge-Action Gap\" through a stratified performance comparison across languages. Regardless\nof whether the context is English or Chinese, average scores exhibit a consistent stepwise degradation from MCQs,\nto Open QA (Guideline), and finally to Multi-turn Dialogue (Clinical Case). This degradation is further quantified in\nthe model-specific trajectory analysis (Figure 3b). Without exception, all evaluated models exhibited a steep\nnegative slope when moving from single-turn selection tasks to multi-turn clinical management. The scores dropped\nsignificantly, with leading models falling from the 0.90–0.95 range in MCQs to approximately 0.50 in Multi-turn\nDialogue. This trend empirically confirms the \"high scores, limited capability\" paradox: the ability to select the correct\noption in a structured exam does not linearly translate to the dynamic state-tracking and decision-making required in\na clinical workflow. Importantly, this persistent decline across both general-purpose and domain-specialized models\nhighlights the universality of the Knowledge-Action Gap. Moreover, performance in Multi-turn Dialogue suggests that\ncurrent LLMs still operate primarily in a “Co-pilot / Assisted Mode” rather than being ready for unsupervised\nend-to-end clinical decision-making, leaving a substantial gap before independent clinical deployment is feasible.\nFigure 3. The \"Knowledge-Action Gap\": Performance Degradation from Static Exams to Dynamic Clinical\nSimulations. MedGPT refers to the MedGPT-251130 version. (a) Average performance comparison across three\ntask types (MCQs, Open QA, and Multi-turn Dialogue) stratified by language (English vs. Chinese). A consistent\ndecline in scores is observed as task complexity increases, regardless of the language. (b) Trajectory analysis of\nindividual models across the three task types. All models display a steep negative slope, illustrating the sharp drop\nin performance when transitioning from standardized single-turn tasks to multi-turn clinical dialogue.\nDialogue Efficiency and Information-Gathering Patterns in Multi-turn Dialogue\nWe adopted an interaction efficiency metric, the Efficiency Coefficient, to quantify information-gathering efficiency in\nmulti-turn clinical dialogues. By jointly accounting for dialogue quality (Clinical Case score) and interaction cost (turn\ncount), this metric directly captures score gain per turn in simulated history taking.\nAs shown in Figure 4, the top-performing models exhibit distinct high-performance strategies. MedGPT achieves\n"}, {"page": 10, "text": "strong Clinical Case scores with fewer turns, resulting in a higher Efficiency Coefficient, consistent with structured,\ngoal-directed questioning that rapidly covers key history elements and converges to an actionable management\nplan. In contrast, GPT-5.2 also attains high Clinical Case scores but with more turns and a comparatively lower\nEfficiency Coefficient, suggesting a “thorough-accurate” strategy in which more granular information acquisition\nsupports downstream decision quality; this may explain why a general-purpose model remains competitive in\ncomplex\nclinical\ndialogues.\nOverall,\nother\nmodels\nshow\nlower\nefficiency\ndistributions,\nspecifically\nwith\nDeepSeek-V3.2-Exp and Baichuan-M2 exhibiting notably fewer dialogue turn. This is mechanistically consistent\nwith either premature closure due to incomplete history taking or diminished effective use of accumulated context in\nlonger dialogues.\nFigure 4. Efficiency Coefficient and Performance Score in Multi-turn Clinical Dialogue. MedGPT refers to the\nMedGPT-251130 version. The chart illustrates the trade-off between interaction cost and quality. The bottom x-axis\nrepresents the Efficiency Coefficient (calculated as Score / Turns), where the scatter points and violin plots depict\nthe distribution of efficiency across individual simulation cases for each model. The top x-axis represents the\nMulti-turn Dialogue (Clinical Case) Score (Weighted Mean), which corresponds to the position of the central markers\n(stars and diamonds) for each model. MedGPT (Star) demonstrates an optimal balance with high scores and high\nefficiency, whereas other models show varying degrees of efficiency loss or premature closure.\nTo further localize the bottleneck in multi-turn performance, we conducted a control experiment designed to\ndisentangle inquiry/action from reasoning/knowledge. In this setting, the full Case Vignette was directly provided to\nthe target LLM, bypassing the inquiry phase and requiring immediate plan generation. Table 1 shows a consistent\nand substantial score increase for all models in the Direct Input Mode compared with the Multi-turn Inquiry Mode.\nThis uniform uplift indicates that the primary limitation in multi-turn dialogues is not the availability of clinical\nknowledge or the ability to generate a coherent plan when information is complete, but rather deficits in proactive\ninformation gathering and dynamic state tracking during interaction; consequently, the Knowledge-Action Gap in this\n"}, {"page": 11, "text": "benchmark is driven predominantly by Information Gathering (Action) rather than Clinical Reasoning (Knowledge).\nTable 1. Performance Comparison of LLMs in Multi-turn Inquiry Mode versus Direct Input Mode.\n●All models show a statistically significant performance improvement in the Direct Input Mode compared to the Multi-turn Inquiry Mode\n( p ≤0.0001). P-values are derived from weighted, bootstrap tests for all pairwise comparisons, adjusted using the Holm correction.\n●MedGPT refers to the MedGPT-251130 version.\nMetric-level Disparities and Safety Vetoes in Dynamic Dialogues\nAt the metric level, the radar plots in Figure 5 show broad coverage across Safety and Effectiveness checkpoints in\nstatic Guideline QA, whereas the polygons contract and distort in multi-turn Clinical Case simulations, indicating a\nsystematic failure to translate static knowledge into dynamic execution. The dot-plot distributions in Figure 6 further\ndemonstrate that this degradation is most pronounced on high-weight checkpoints and is frequently triggered by\nmissing key questions during interaction. Because Safety is evaluated via a zero-tolerance binary veto, a single\nomission of safety-critical information can yield Safety=0 even when subsequent text appears plausible, thereby\nseparating fluent-but-unsafe outputs from clinically acceptable behaviors.\n"}, {"page": 12, "text": "Figure 5. Radar Chart Analysis of Consensus Metrics Coverage. MedGPT refers to the MedGPT-251130\nversion. (a-b) Performance coverage in Guideline QA tasks for Safety (a) and Effectiveness (b) metrics. Note the\nbroad, nearly complete polygons indicating strong theoretical retrieval. (c-d) Performance coverage in Clinical Case\nsimulations. The significant shrinkage of the polygons illustrates the degradation in applying safety (c) and\neffectiveness (d) protocols in dynamic dialogue settings.\nRepresentative low-score failure modes are observed in S-15 (Indications Assessment for Dental Implant Surgery,\nWeight 5) and S-13 (Prevention of Complications in Root Canal Therapy, Weight 2). In S-15, models often fail to\nsystematically assess bone volume and anatomical constraints during the dialogue, preventing appropriate risk\nstratification and triggering a safety veto. In S-13, models may omit critical preoperative imaging assessments or\nprocedural precautions, resulting in\nSafety=0 despite possessing theoretical knowledge. These\npatterns\ndemonstrate that the scoring framework can surface potentially fatal decision errors that arise specifically from\ninformation gaps in multi-turn inquiry.\nIn contrast, several checkpoints remain comparatively stable in dynamic dialogues, suggesting safer zones for\nclinical copilot use. S-12 (Identification of Contraindications for Tooth Extraction, Weight 5) and S-11 (Recognition\nand Emergency Management of Anaphylactic Shock, Weight 4) more consistently follow standardized pathways,\nsupporting more robust safety performance. Similarly, E-06 (Early Clinical Indicators of Oral Neoplasms, Weight 4)\nis often supported by distinct red-flag recognition (e.g., non-healing ulcers), yielding more reliable effectiveness in\nmulti-turn settings. Collectively, Figure 5 and Figure 6 identify a mechanism by which specific high-complexity\n"}, {"page": 13, "text": "checkpoints are disproportionately vulnerable to omission-driven failures, while also delineating a subset of metrics\nthat appear more reliable for risk-stratified human–AI collaboration.\nFigure 6. Metric-Level Performance Discrepancy Analysis. MedGPT refers to the MedGPT-251130 version. (a-b)\nDot plots showing the distribution of model scores for each metric in Guideline QA. Models consistently score high\non static knowledge metrics (e.g., S-11, E-03). (c-d) Dot plots for Clinical Case simulations. The \"Average\" dot\nrepresents the mean score across all models per metric.\nMapping the Safe-Operating Boundaries and Clinical Trade-offs\nTo rigorously delineate the operational boundaries of autonomous dental agents, we mapped the capability space of\nLLMs regarding \"Safety Guardrails\" versus \"Clinical Effectiveness,\" visualizing the performance trajectory from static\nOpen QA (circles) to dynamic multi-turn simulations (triangles). This dual-axis analysis reveals a critical performance\ndegradation, demonstrating that a model's ability to retrieve knowledge in static tasks does not inherently guarantee\nexecution quality in dynamic workflows. As illustrated in Figure 7, all evaluated models retreat from the\n\"Safe-Operating Region\" (upper-right quadrant) observed in Open QA tasks. In the dynamic setting, models often\nexhibit a \"Safety-Biased, Low Efficacy\" profile, where the ability to synthesize comprehensive treatment plans\n(Effectiveness) collapses more precipitously than adherence to safety protocols. Notably, only select models such\nas GPT-5.2 and MedGPT manage to maintain a relatively robust safety floor (Safety > 0.6), separating themselves\nfrom the lower-left cluster. However, even these leading models display a substantial \"Knowledge-Action Gap,\"\ncharacterized by a sharp decline in Effectiveness scores compared to their static baselines. This distribution\nunderscores that the primary bottleneck in current architectures is not the trade-off of safety for efficacy, but rather\nthe inability to maintain active state tracking and information gathering required to sustain high clinical effectiveness\nin multi-turn interactions.\n"}, {"page": 14, "text": "Figure 7. Mapping the Knowledge-Action Gap: Safety vs. Effectiveness Trade-off. MedGPT refers to the\nMedGPT-251130 version. The scatter plot visualizes the performance trajectory of Large Language Models from\nstatic knowledge tasks to dynamic clinical simulations. The X-axis represents the Clinical Effectiveness Score, and\nthe Y-axis represents the Safety Score. Distinct markers indicate task types: circles represent Open QA (Guideline)\nand triangles represent Multi-turn Dialogue (Clinical Case), with dashed lines illustrating the performance shift for\neach model. The upper-right quadrant denotes the ideal \"Safe-Operating Region.\" While models generally inhabit\nthis high-performance zone in static QA tasks, a significant regression is observed in dynamic scenarios.\nExternal Knowledge Integration and Agentic Reliability\nWe further evaluated the impact of Retrieval-Augmented Generation (RAG) as a remediation strategy. As presented\nin Table2, the integration of an external knowledge base containing the 60 semantic guideline chunks resulted in\nstatistically significant improvements in Guideline-based Open QA tasks, with performance gains ranging from\n0.0125 to 0.1626 across evaluated models. Notably, general models with lower initial baselines, such as\nClaude-Sonnet-4.5 and Baichuan-M2, exhibited the most substantial benefits in this static retrieval setting (0.1626\nand 0.1318, respectively). However, the benefit in the Case Dialogue tasks was markedly heterogeneous and\ngenerally more modest for top-performing models. While DeepSeek-V3.2-Exp achieved a significant boost (0.1391),\n"}, {"page": 15, "text": "other leading models like MedGPT and GPT-5.2 showed limited improvements (0.0413 and 0.0357), and\nBaichuan-M2 even experienced performance degradation (-0.0147). This divergence demonstrates that while RAG\nsuccessfully mitigates specific factual hallucinations—pulling the \"Safety\" coordinates rightward in the capability\nmap—it fails to consistently bridge the \"Knowledge-Action Gap.\" The inability of RAG to universally improve, or in\nsome cases even maintain, performance in dynamic workflows underscores that external knowledge injection\ncannot fully compensate for intrinsic deficits in reasoning logic or state tracking required for complex clinical\nsimulations.\nTable 2. Impact of Retrieval-Augmented Generation (RAG) on LLM performance across Open QA and Multi-turn\nDialogue tasks.\nImpact of Language Concordance on Performance\nTo evaluate the holistic robustness of LLMs across diverse medical environments, we analyzed performance on our\nnatively sourced bilingual datasets (eFigure 3). Critically, this stratification extends beyond mere translation\nproficiency; since the Chinese and English samples were independently collected, they represent distinct\nmedical-cultural contexts and guideline ecosystems. The results revealed a divergent pattern based on model\narchitecture. While top-tier global generalist models (e.g., GPT-5.2, Gemini-3-Pro) exhibited a relatively balanced\nproficiency across languages, domain-specific and regional models (e.g., MedGPT, DeepSeek) displayed a\ndiscernible \"Native-Language Alignment.\" Specifically, MedGPT achieved peak performance in its primary linguistic\ncontext (Chinese) but experienced a notable decline in the non-native setting (English), particularly in Multi-turn\nDialogue (dropping from ~0.58 to ~0.41). Similarly, regional models like DeepSeek showed significant disparities in\nMCQs. This suggests that while general clinical reasoning capabilities are transferable, the deep cultural and\nguideline-specific nuances in dentistry still impose a dependency on the pre-training distribution, challenging the\nnotion of universal robustness for specialized agents. Thus, true clinical robustness requires not just reasoning\ncapability, but explicit alignment with the heterogeneous medical standards of diverse linguistic ecosystems.\nDiscussion\n"}, {"page": 16, "text": "Our study proposes a paradigm shift in evaluating LLMs in dentistry, moving beyond the traditional \"Knowledge\nEngine\" perspective to a comprehensive \"Clinical Agent\" framework. The observed performance drop from\nknowledge-oriented evaluations to workflow-based simulations offers critical insights into the architectural\nrequirements of autonomous dental agents. Conceptually, our evaluation framework probes the same underlying\nagentic\ncompetencies\nunder\ntwo\ntask\nregimes—from\nknowledge-oriented\nevaluations\nto\nworkflow-based\nsimulations—thereby stress-testing not only recall of protocols but also interactive planning, state tracking, and\ndecision safety. While recent studies confirm that LLMs can achieve passing scores in dental licensing\nexaminations9, 10, our results demonstrate that high proficiency in standardized exams (MCQs) does not guarantee\nthe successful execution of complex, multi-step clinical workflows. We attribute the precipitous performance decline\nobserved in our results to the escalation of \"Task Complexity\" and the challenge of \"Information Asymmetry,\" rather\nthan merely an increase in question difficulty. Unlike static exams where all conditions are provided, clinical\nscenarios require the Agent to actively bridge the information gap through strategic inquiry in an incomplete\ninformation setting. This finding corroborates the \"High scores vs. Limited capability\" paradox and highlights the\ncritical challenge of the \"Faithfulness-Plausibility Gap,\" where models may generate plausible outputs without\nfaithful underlying reasoning logic22. Future development must prioritize the integration of Tool Use capabilities—\nspecifically, the ability to actively query and synthesize external guidelines—rather than merely scaling parameter\nsize for rote memorization11.\nThe introduction of the Simulated Patient (SP) multi-turn dialogue assessment establishes a higher standard of\necological validity compared to single-turn QA benchmarks. In our end-to-end simulations, the LLM is required not\nonly to diagnose but to maintain State Tracking across the entire clinical trajectory—from history taking to treatment\nplanning and follow-up. This distinction is vital for differentiating a conversational \"chatbot\" from a task-oriented\n\"Clinical Agent\" capable of handling long-horizon tasks23. The performance drop observed when moving from\nknowledge-oriented evaluations to workflow-based simulations, particularly in long-context scenarios, highlights the\nfragility of current LLMs in maintaining coherent clinical logic over time. Furthermore, by utilizing Native-Source Data\nrather than translated datasets, we further substantiate that these performance deficits are intrinsic to the model's\nactive inquiry and strategic planning capabilities rather than artifacts of linguistic misalignment.\nThe integration of RAG transforms the LLM's operation from opaque generation to a transparent, evidence-based\nprocess. By anchoring responses to specific guideline chunks, the Traceable RAG mechanism significantly\nmitigates the hallucination of non-existent treatment protocols. However, our results reveal that this benefit does not\nuniformly translate to workflow-based simulations. The observation that RAG yielded negligible gains—or even\nperformance regression in models with weaker reasoning cores (e.g., Baichuan-M2)—indicates that simply\naccessing external guidelines is insufficient. It suggests that without a stable \"Decision Core\" to filter and utilize\nretrieved information, external knowledge can become cognitive noise rather than aid. Thus, external knowledge\nintegration requires a stronger reasoning or Agentic framework to effectively solve clinical problems. Furthermore,\ncurrent implementations reveal structural limitations: the retrieval component often relies on English-centric\nembedding models, while the generation component predominantly utilizes generic LLMs rather than medically\noptimized ones, with applications largely restricted to QA, report generation, and summarization. Crucially, existing\nevaluations disproportionately focus on accuracy and fluency, leaving bias and safety considerations severely\n"}, {"page": 17, "text": "underaddressed. Medical RAG remains in its nascent stage, necessitating future efforts to strengthen validation on\nprivate data, enhance multilingual capabilities, and establish stricter ethical and safety evaluation standards24.\nAccess to external knowledge alone is insufficient without robust reasoning engines to interpret that knowledge\nwithin a specific patient context25, 26.\nThe consistent superiority of MedGPT across the full evaluation continuum provides compelling evidence for the\nnecessity of domain-adaptive pre-training in constructing autonomous Clinical Agents. This finding aligns with recent\nobservations that general medical models perform suboptimally in the highly specialized field of dentistry due to a\nlack of deep domain knowledge. Consequently, efforts such as DentalBench highlight the critical importance of\nconstructing sub-field specific benchmarks and corpora (e.g., for dentistry) for developing trustworthy medical AI27.\nMedGPT’s dominance suggests that medical domain specificity extends beyond mere vocabulary retention to the\ninternalization of \"clinical syntax\"—the implicit logical structures governing diagnosis and treatment planning. In the\ncontext of our agentic evaluation framework, this finding has profound implications for the Architectural Alignment of\ndental agents. A domain-specific model does not simply act as a larger database; it functions as a more stable\nDecision Core, capable of executing State Tracking in multi-turn dialogue simulations with higher fidelity28.\nOur study acknowledges several methodological constraints inherent to the evaluation framework. The composition\nand size of the expert panel may limit the generalizability of the consensus-based metrics, potentially introducing\nselection bias regarding specific clinical sub-specialties. Furthermore, the structured feedback mechanism\nemployed during the Delphi process could introduce conformity or anchoring bias, leading to an artificial\nconvergence of expert opinions. Regarding the inevitable subjectivity in human evaluation of long-context dialogues,\nwe defend our strategy of employing \"fine-grained checkpoint mapping + automated scoring\" to maximize \"Machine\nConsistency.\" Nevertheless, as LLM tasks evolve into open-ended, complex Agentic workflows, human evaluation\nbecomes prohibitively expensive and hard to scale, while traditional single \"LLM-as-a-judge\" approaches often\nsuffer from bias and limited perspective. In contrast, the \"Agent-as-a-Judge\" paradigm—capable of using tools,\nmaintaining memory, and evaluating intermediate steps like a human—represents a critical path for scalable,\nlow-cost, and nuanced assessment29. Although the Agent-as-a-Judge paradigm cannot fully replace human\nsupervision, it serves as a necessary evolution for evaluating next-generation clinical agents. Future work must\nincorporate real-world evidence (RWE) and multicenter validation to further optimize these thresholds and conduct\nrigorous sensitivity analyses to ensure the robustness of the Safety Checkpoints across diverse clinical settings30.\nCrucially, while this study explores the feasibility of autonomous agents, the evaluation is strictly confined to the\nDynamic Reasoning Core— effectively validating the \"cognitive brain\" of the agent rather than a fully functional\n\"embodied agent.\" The current framework assesses text-based decision-making but does not evaluate the Action\nExecution capabilities required for physical intervention or direct integration with Electronic Health Record (EHR)\nsystems. Consequently, the \"autonomy\" demonstrated here represents a potential for clinical reasoning rather than\na readiness for physical clinical practice.\nAlthough our Simulated Patient scenarios are derived from real-world reports, they inherently undergo a degree of\nstructuring and sanitization during the prompt engineering process. In actual clinical practice, agents must contend\nwith unstructured, high-noise inputs, including vague patient descriptions, colloquialisms, and ambiguous intent. Our\nresults, therefore, may underestimate the difficulty of State Tracking in raw, noisy clinical environments. The\n"}, {"page": 18, "text": "robustness of these models against non-standard, noisy patient narratives remains a critical frontier. Future\nbenchmarks must evolve from structured simulations to \"in-the-wild\" stress tests to fully verify the agent's resilience\nagainst the unpredictability of real-world patient interactions31.\nThe SCMPE benchmark proposed in this study not only reveals the \"Knowledge-Action Gap\" but also provides an\nempirical blueprint for transitioning from mere LLM benchmarking to practical Clinical Agent deployment. Between\n2024 and 2025, related research has grown exponentially, marking a technological shift from single models to\nsystems capable of autonomy, planning, and tool use. However, Medical Agents differ significantly from\ngeneral-purpose agents; they must operate under strict safety, privacy regulations, and ethical constraints,\ncharacterized by a \"zero-tolerance\" high-risk profile32.\nIn this context, AI-driven medical software projects represent not just a technological upgrade but a reshaping of\nclinical workflows. The core challenge lies in balancing agent autonomy with controllability and trust. Driven by AI\nalgorithm models, these projects achieve diagnostic support through data integration, logical computation, and\ndecision assistance, with their core value lying in the balance between algorithm precision and clinical adaptability.\nBased on our findings, we delineate the application scenarios as follows:\nDoctor Side (Decision Support): The Agent can function as a \"clinical copilot,\" assisting physicians in\ncomprehensive history taking to improve efficiency, and performing real-time guideline checks to prevent\nomissions, thereby enhancing diagnostic efficiency and optimizing judgment accuracy.\nPatient Side (Health Education): In direct-to-patient scenarios, the Agent's function must be strictly limited to\nhealth education and consultation, explicitly prohibiting autonomous diagnosis or treatment to maintain safety\nboundaries.\nCheckpoint Side (Human-AI Collaboration): Based on our scoring mapping, clinicians can place higher trust in\n\"high-scoring\ncheckpoints,\"\nwhereas\n\"low-scoring\ncheckpoints\"\ninvolving\ncomplex\nreasoning\nrequire\na\n\"Human-in-the-loop\" mechanism for manual review and intervention.\nThis stratified strategy aims to extend the boundaries of medical services while ensuring the safety of clinical\npractice by clearly defining service limits.\nIn conclusion, this study marks a pivotal step in redefining the evaluation of Large Language Models in dentistry,\ntransitioning from the static paradigm of a Knowledge Engine to the dynamic, functional assessment of a Clinical\nAgent. The results reveals a critical dichotomy: while current models demonstrate impressive proficiency in the\nStatic Knowledge Module, their capability within the Dynamic Reasoning Core remains fragile. Crucially, our findings\nindicate that simply accessing external guidelines via RAG is insufficient to bridge the \"Knowledge-Action Gap\" in\ncomplex state tracking; instead, domain-adaptive pre-training proves essential for constructing a stable \"Decision\nCore\" capable of adhering to clinical syntax. The paradox of \"High scores vs. Limited capability\" underscores that\npassing a board exam is a necessary but insufficient condition for autonomous practice, and safety guardrails must\nbe enforced as zero-tolerance constraints rather than trade-offs for efficacy. Ultimately, this work provides not just a\nbenchmark, but a foundational roadmap for aligning the \"silicon reasoning\" of AI with the \"biological safety\" required\nin patient care, establishing the rigorous boundaries within which Dental Agents can safely evolve. Only through\nsuch rigorous validation can we ensure that the promise of AI-driven patient empowerment—providing accessible,\nhigh-quality guidance for shared decision-making—is realized safely and effectively.\n"}, {"page": 19, "text": "Data availability\nAll the Supplymentary Tables and Appendix Tables used in the study are also available in the following repository：\nhttps://github.com/Medlinker-MG/SCMPE\nCode availability\nAll\ncode\nfor\nreproducing\nour\nanalysis\nis\navailable\nin\nthe\nfollowing\nrepository:\nhttps://github.com/Medlinker-MG/SCMPE\nAcknowledgements\nWe would also like to thank all the physicians for their contributions. This work would not have been possible without\nthe insight and generosity of the physicians who contributed their time and expertise to SCMPE Benchmark.\nAuthor contributions\nHM, TG, HS, ZL, YG, SW and ZT designed and supervised the study. HM, TG, HS, HZ and YW established clinical\ndata standardization and performed clinical data collection. HM, HZ, JL, YZ, YG and ZT contributed to the\nacquisition, analysis, or interpretation of data. HM, HZ, YG and ZT drafted the manuscript. TG, HS, YW, WS, ZL, YG,\nSW and ZT critically reviewed the manuscript for important intellectual content. ZT and YG had full access to all of\nthe data in the study and take responsibility for the integrity of the data and the accuracy of the data analysis. All\nauthors have read and approved the final version of the manuscript.\nConflict of interest\nTG, HS, YW, WS, ZL and SW are employees of Medlinker Intelligent and Digital Technology Co., Ltd, Beijing, China.\nAll other authors have declared no conflicts of interest.\nFunding\nFunding\nfor\nthis\nstudy\nwas\nsupported\nby\nBeijing\nChaoyang\nDigital\nHealth\nProof\nof\nConcept\nCenter\nNo.2025SLZZ030 and the National Natural Science Foundation of China for Young Scientists (82401196).\nEthics Statement\nAll data sources we use to construct the SCMPE benchmark dataset are publicly available and free to use without\ncopyright infringement. All questions in the SCMPE dataset have been appropriately anonymized so that they do not\ncontain sensitive private information about patients. We do not foresee any other possible negative societal impacts\nof this work.\nReferences\n1. Meng, Z. et al. DentVLM: A multimodal vision-language model for comprehensive dental diagnosis and enhanced clinical\npractice. arXiv:2509.23344 (2025).\n2. Bedi, S. et al. MedHELM: Holistic evaluation of large language models for medical tasks. arXiv:2505.23802 (2025).\n3. Farhadi Nia, M., Ahmadi, M. & Irankhah, E. Transforming dental diagnostics with artificial intelligence: advanced integration of\n"}, {"page": 20, "text": "ChatGPT and large language models for patient care. Front Dent Med 5, 1456208, doi:10.3389/fdmed.2024.1456208 (2024).\n4. Jin, D. et al. What disease does this patient have? a large-scale open domain question answering dataset from medical\nexams. Applied Sciences 11, 6421 (2021).\n5. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain\nquestion answering. In Conference on Health, Inference, and Learning 248-260 (PMLR, 2022).\n6. Jin, Q. et al. Pubmedqa: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) 2567-2577 (2019).\n7. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172-180 (2023).\n8. Gong, E. J., Bang, C. S., Lee, J. J. & Baik, G. H. Knowledge-practice performance gap in clinical large language models: systematic\nreview of 39 benchmarks. J Med Internet Res 27, e84120, doi:10.2196/84120 (2025).\n9. Liu, M. et al. Large language models in dental licensing examinations: systematic review and meta-analysis. Int Dent J 75,\n213-222, doi:10.1016/j.identj.2024.10.014 (2025).\n10. Sabri, H. et al. Performance of three artificial intelligence (AI)-based large language models in standardized testing; implications\nfor AI-assisted dental education. J Periodontal Res 60, 121-133, doi:10.1111/jre.13323 (2025).\n11. Kim, H.-S. & Kim, G.-T. Can a large language model create acceptable dental board-style examination questions? A\ncross-sectional prospective study. J Dent Sci 20, 895-900, doi:10.1016/j.jds.2024.08.020 (2025).\n12. Nguyen, H. C., Dang, H. P., Nguyen, T. L., Hoang, V. & Nguyen, V. A. Accuracy of latest large language models in answering\nmultiple choice questions in dentistry: A comparative study. PLoS One 20, e0317423, doi:10.1371/journal.pone.0317423 (2025).\n13. Ozdemir, Z. M. & Yapici, E. Evaluating the accuracy, reliability, consistency, and readability of different large language models in\nrestorative dentistry. J Esthet Restor Dent 37, 1740-1752, doi:10.1111/jerd.13447 (2025).\n14. Thaliyil, K. P. et al. Evaluating the evidence-based potential of six large language models in paediatric dentistry: a comparative\nstudy on generative artificial intelligence. Eur Arch Paediatr Dent, doi:10.1007/s40368-025-01012-x (2025).\n15. Johri, S. et al. An evaluation framework for clinical use of large language models in patient interaction tasks. Nat Med 31, 77-86,\ndoi:10.1038/s41591-024-03328-5 (2025).\n16. Rewthamrongsris, P., Burapacheep, J., Trachoo, V. & Porntaveetus, T. Accuracy of large language models for infective\nendocarditis prophylaxis in dental procedures. Int Dent J 75, 206-212, doi:10.1016/j.identj.2024.09.033 (2025).\n17. Wu, X. et al. A multi-dimensional performance evaluation of large language models in dental implantology: comparison of\nChatGPT,\nDeepSeek,\nGrok,\nGemini\nand\nQwen\nacross\ndiverse\nclinical\nscenarios.\nBMC\nOral\nHealth\n25,\n1272,\ndoi:10.1186/s12903-025-06619-6 (2025).\n18. Liu, J. et al. Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. Advances in\nNeural Information Processing Systems 36, 52430-52452 (2023).\n19. Wang, X. et al. Cmb: A comprehensive medical benchmark in chinese. In Proceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers) 6184-6205 (2024).\n20. Umapathi, L. K., Pal, A. & Sankarasubbu, M. Med-HALT: Medical Domain Hallucination Test for Large Language\nModels. arXiv:2307.15343 (2023).\n21. Fleming, S. L. et al. MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records.\n"}, {"page": 21, "text": "In Thirty-Eighth AAAI Conference on Artificial Intelligence, doi:10.1609/AAAI.V38I20.30205 (2024).\n22. Wang, W. et al. Medical reasoning in the era of LLMs: a systematic review of enhancement techniques and applications.\narXiv:2508.00669 (2025).\n23. Singh, S. & Beniwal, H. A survey on near-human conversational agents. Journal of King Saud University - Computer and\nInformation Sciences 34, 8852-8866, doi:10.1016/j.jksuci.2021.10.013 (2022).\n24. Yang, R. et al. Retrieval-augmented generation in medicine: a scoping review of technical implementations, clinical\napplications, and ethical considerations. arXiv:2511.05901 (2025).\n25. Pingua, B. et al. Medical LLMs: Fine-Tuning vs. Retrieval-Augmented Generation. Bioengineering (Basel) 12, 687,\ndoi:10.3390/bioengineering12070687 (2025).\n26. Teo, Z. L. et al. Generative Artificial Intelligence in Medicine. Nat Med 31, 3270-3282, doi:10.1038/s41591-025-03983-2 (2025).\n27. Zhu, H., Xu, Y., Li, Y., Meng, Z. & Liu, Z. DentalBench: benchmarking and advancing LLMs capability for bilingual dentistry\nunderstanding. arXiv:2508.20416 (2025).\n28. Chen, A. et al. Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks.\nIn Proceedings of the 42nd International Conference on Machine Learning (2025).\n29. Yu, F. When AIs judge AIs: the rise of agent-as-a-judge evaluation for LLMs. arXiv:2508.02994 (2025).\n30. Kolbinger, F. R. & Kather, J. N. Adaptive validation strategies for real-world clinical artificial intelligence. Nat Comput Sci 5,\n980-986, doi:10.1038/s43588-025-00901-x (2025).\n31. Javed, H., El-Sappagh, S. & Abuhmed, T. Robustness in deep learning models for medical diagnostics: security and adversarial\nchallenges towards robust AI applications. Artif Intell Rev 58, 12, doi:10.1007/s10462-024-11005-9 (2025).\n32. Xu, G. et al. A comprehensive survey of AI agents in healthcare. TechRxiv (2025).\n"}, {"page": 22, "text": "Supplementary information\neTable 1. Data collection of the Bilingual Repository\n"}, {"page": 23, "text": "eTable 2. Three-round scoring and final weights for Safety and Effectiveness metrics.\n●The scale delineates the consequence of agent error: scores in [0, 3) were mapped to Weight 1 (Context-dependent/limited risk); [3, 3.5)\n"}, {"page": 24, "text": "to Weight 2 (Foundational/low-to-moderate risk); [3.5, 4) to Weight 3 (Moderate risk with meaningful impact); [4, 4.5) to Weight 4\n(High-risk and time-sensitive); and [4.5, 5] to Weight 5 (Life-threatening or catastrophic failure).\n●S-xx and E-xx denote metrics in the Safety Gate and Effectiveness Gate, respectively.\n"}, {"page": 25, "text": "eTable 3. Test set with structured evaluation suite\n"}, {"page": 26, "text": "Supplementary Note 1. Prompts for Multi-turn Dialogue Simulation\nSimulated Patient (SP) prompt:\nYou are a patient.\nYou do not have any medical knowledge.\nYou have to describe your symptoms from the given case vignette based on the questions asked.\nDo not break character and reveal that you are describing symptoms from the case vignette.\nDo not generate any new symptoms or knowledge, otherwise you will be penalized.\nDo not reveal more information than what the question asks.\nKeep your answer short, to only 1 sentence.\nSimplify terminology used in the given paragraph to layman language.\n**Case Vignette**: {{ case }}\nTarget LLM prompt:\nYou are an AI doctor specializing in {{ specialty }}.\nArrive at a diagnosis of a patient's medical condition.\nAsk only one question at a time, and it should not be more than 1 line.\nContinue asking questions until you're 100% confident of the diagnosis.\nDo not ask the same question multiple times. Ask different questions to cover more information.\nThe questions should cover age and sex of the patient, current symptoms, medical history of illness and medications, and\nrelevant family history if necessary.\nKeep your questions short and brief to not confuse the patient.\nAfter completing all questions, provide a comprehensive plan including assessment, treatment decisions, and patient\nmanagement.\nYour response must begin with \"**Final Treatment Plan:**\".\n"}, {"page": 27, "text": "eFigure 1. Assessment of Rationale Generation Quality via ROUGE-L Metric.\nThis figure illustrates the ROUGE-L scores for the rationales generated by each model during the Standardized Exam task. The\nmetric evaluates the linguistic overlap and coherence against reference explanations.\n"}, {"page": 28, "text": "eFigure 2. Worst@k reliability profiling across tasks (MCQs, Open QA, and Multi-turn Dialogue).\nThis figure quantifies model stability across distinct clinical tasks:\n(Left) MCQs - Accuracy: Models exhibit relative stability in static multiple-choice questions, with a gentle decline in the\nperformance floor as k increases.\n(Middle) Open QA (Guideline): Stochasticity becomes apparent in guideline-based Q&A, evidenced by a distinct drop in\nscores.\n(Right) Multi-turn Dialogue (Clinical Case): The curves display the steepest deterioration trend in dynamic multi-turn\ndialogues, indicating significant instability in long-horizon state tracking and information gathering.\nOverall, MedGPT (red line) and Gemini-3-Pro (green line) maintain a higher performance floor across tasks, whereas other\nmodels exhibit a more rapid performance collapse.\n"}, {"page": 29, "text": "eFigure 3. Assessment of Cross-Lingual Robustness on Natively Sourced Datasets.\nPerformance comparison stratified by language (English vs. Chinese) across three task types. The bars represent mean scores\nwith 95% confidence intervals. The variation in performance highlights the models' differing capabilities in adapting to distinct\nlinguistic-cultural medical environments.\n"}]}