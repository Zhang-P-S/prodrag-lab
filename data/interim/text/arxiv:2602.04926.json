{"doc_id": "arxiv:2602.04926", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.04926.pdf", "meta": {"doc_id": "arxiv:2602.04926", "source": "arxiv", "arxiv_id": "2602.04926", "title": "Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation", "authors": ["Ning Wang", "Kuanyan Zhu", "Daniel Yuehwoon Yee", "Yitang Gao", "Shiying Huang", "Zirun Xu", "Sainyam Galhotra"], "published": "2026-02-04T08:48:11Z", "updated": "2026-02-04T08:48:11Z", "summary": "Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.04926v1", "url_pdf": "https://arxiv.org/pdf/2602.04926.pdf", "meta_path": "data/raw/arxiv/meta/2602.04926.json", "sha256": "e4878dd0164a4179fed4a6394e149fb26fe98e98ccef7bb1c6b9730a766d2510", "status": "ok", "fetched_at": "2026-02-18T02:19:48.705210+00:00"}, "pages": [{"page": 1, "text": "Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented\nGeneration\nNing Wang*1\nKuanyan Zhu*2\nDaniel Yuehwoon Yee*3\nYitang Gao4\nShiying Huang1\nZirun Xu5\nSainyam Galhotra†1\n1Cornell University\n2University of Cambridge\n3The University of Hong Kong\n4HKUST\n5University of British Columbia\nnw366@cornell.edu\nkz345@cam.ac.uk\nu3636035@connect.hku.hk\nsg@cs.cornell.edu\nAbstract\nRetrieval-augmented\ngeneration\n(RAG)\nis\nnow standard for knowledge-intensive LLM\ntasks, but most systems still treat every query\nas fresh, repeatedly re-retrieving long pas-\nsages and re-reasoning from scratch, inflating\ntokens, latency, and cost. We present Auto-\nPrunedRetriever, a graph-style RAG system\nthat persists the minimal reasoning subgraph\nbuilt for earlier questions and incrementally\nextends it for later ones. AutoPrunedRetriever\nstores entities and relations in a compact, ID-\nindexed codebook and represents questions,\nfacts, and answers as edge sequences, enabling\nretrieval and prompting over symbolic struc-\nture instead of raw text. To keep the graph\ncompact, we apply a two-layer consolidation\npolicy (fast ANN/KNN alias detection plus\nselective k-means once a memory threshold is\nreached) and prune low-value structure, while\nprompts retain only overlap representatives\nand genuinely new evidence. We instantiate\ntwo front ends: AUTOPRUNEDRETRIEVER-\nREBEL, which uses REBEL (Huguet Cabot\nand Navigli, 2021) as a triplet parser, and\nAUTOPRUNEDRETRIEVER-LLM,\nwhich\nswaps in an LLM extractor. On GraphRAG-\nBenchmark\n(Medical\nand\nNovel),\nboth\nvariants achieve state-of-the-art complex\nreasoning accuracy, improving over Hip-\npoRAG2 (Jim´enez Guti´errez et al., 2025) by\nroughly 9–11 points, and remain competitive\non contextual summarize and generation.\nOn our harder STEM and TV benchmarks,\nAutoPrunedRetriever again ranks first, while\nusing up to two orders of magnitude fewer\ntokens than graph-heavy baselines, making it\na practical substrate for long-running sessions,\nevolving corpora, and multi-agent pipelines.\n1\nIntroduction\nRetrieval-augmented generation (RAG) grounds\nLLMs in external knowledge, reducing halluci-\n*Equal contribution.\n†Corresponding author.\nnations, enabling citation, and allowing updates\nwithout full retraining. Recent advances in dense\nretrieval and generation have yielded strong per-\nformance on open-domain question answering and\ntool-augmented assistants (Lewis et al., 2020;\nKarpukhin et al., 2020a; Izacard et al., 2022; Ram\net al., 2023).\nHowever, moving from retriev-\ning relevant text to solving complex, knowledge-\nintensive tasks still requires multi-hop reasoning:\ncomposing evidence across documents, enforcing\ntemporal or structural constraints, and maintaining\nconsistency across repeated or related queries.\nIn practice, most RAG systems treat each query\nindependently. Even when multiple questions are\nclosely related—or arise sequentially in agentic\nworkflows, systems repeatedly re-retrieve overlap-\nping passages and re-reason from scratch. This\nleads to substantial redundancy in retrieved con-\ntext, inflated token usage, higher latency, and in-\ncreased cost. These inefficiencies are especially\npronounced in long-running sessions and multi-\nagent settings (e.g., planner–researcher–verifier\npipelines), where similar reasoning chains are re-\nvisited many times (Yao et al., 2023; Wu et al.,\n2024)..\nGraph-based RAG methods address some of\nthese issues by lifting retrieval from flat text\npassages to structured representations over enti-\nties and relations (Han et al., 2024; Sun et al.,\n2022; Baek et al., 2023; Wang et al., 2023).\nBy explicitly modeling compositional structure,\nGraphRAG systems improve multi-hop reasoning\nand disambiguation. Yet existing approaches still\nface three fundamental bottlenecks when deployed\nover evolving corpora and long reasoning chains.\nWe demonstrate these challenges with an example.\nExample 1. Consider a small corpus containing\nfive documents describing (i) a corporate acquisi-\ntion, (ii) regulatory status under the EU Digital\nServices Act, (iii) GDPR incident histories, (iv)\narXiv:2602.04926v1  [cs.DB]  4 Feb 2026\n"}, {"page": 2, "text": "2024 vendor contracts, and (v) subsidiary rela-\ntionships (Fig. 1). From this corpus, we ask three\nrelated questions:\nQ1: Which subsidiaries acquired after Jan.\n1,\n2021 are subject to the EU Digital Services Act?\nQ2: For those subsidiaries, did GDPR incident\nrates decrease post-acquisition?\nQ3: Which 2024 vendor contracts involve those\nsame subsidiaries?\nA standard GraphRAG\npipeline constructs an entity–relation graph over\nthe corpus and answers each query via neighbor-\nhood expansion.\nEven in this minimal setting,\nthree limitations emerge. (M1) Graph construc-\ntion and maintenance: entity aliases and nam-\ning variants require global checks and relinking\nas new evidence arrives. (M2) Reasoning gran-\nularity: neighborhood-based expansion retrieves\nbroad subgraphs around central entities (e.g., the\nparent company or regulator), rather than the few\nedges that realize each reasoning chain. (M3) Re-\ndundant retrieval: when Q1–Q3 are issued se-\nquentially or by multiple agents, largely overlap-\nping subgraphs are repeatedly retrieved and seri-\nalized, compounding token and latency costs.\nNotably, the answers to Q2 and Q3 reuse much\nof the reasoning structure required for Q1. How-\never, existing systems fail to exploit this overlap,\nrepeatedly reconstructing context instead of per-\nsisting and extending prior reasoning.\nThese observations suggest a shift in perspec-\ntive: retrieval should not aim to recover all poten-\ntially relevant context, but instead identify, cache,\nand reuse the minimal reasoning structure needed\nto answer a query, and incrementally extend that\nstructure as new questions arrive. These limita-\ntions motivate three design principles: local incre-\nmental structure, path-centric retrieval, and exact\nsymbolic reuse, which guide the design of Auto-\nPrunedRetriever.\nOur Approach.\nWe introduce AutoPrune-\ndRetriever, a structure-first RAG system that treats\nreasoning paths, rather than passages or neighbor-\nhoods, as the primary retrieval unit. AutoPrune-\ndRetriever converts text into symbolic triples and\nrepresents questions, facts, and answers as com-\npact sequences of entity–relation edges. The sys-\ntem persists only the minimal subgraphs that sup-\nport successful reasoning and reuses them across\nlater queries, avoiding repeated re-retrieval and re-\nprompting.\nTo keep memory compact and stable over time,\nAutoPrunedRetriever applies a two-layer consol-\nidation policy: a lightweight, continuous alias-\ndetection pass using approximate nearest neigh-\nbors, and a periodic, budget-triggered consolida-\ntion step that merges aliases and prunes low-value\nstructure. Retrieval is explicitly path-centric, scor-\ning candidate reasoning chains rather than ex-\npanding broad neighborhoods. Prompts are con-\nstructed from a compact symbolic codebook that\nincludes only novel or non-redundant evidence,\nsubstantially reducing token usage while preserv-\ning grounding in source text.\nWe\ninstantiate\nAutoPrunedRetriever\nwith\ntwo\nfront\nends:\nAutoPrunedRetriever-\nREBEL,\nwhich\nuses\na\nfixed\ntriplet\nparser,\nand AutoPrunedRetriever-LLM, which replaces\nit with an LLM-based extractor.\nAcross the\nGraphRAG benchmark as well as harder STEM\nand TV reasoning datasets, both variants achieve\nstate-of-the-art\ncomplex\nreasoning\naccuracy\nwhile using up to two orders of magnitude fewer\ntokens than graph-heavy baselines. These results\ndemonstrate that pruned,\npersistent reasoning\nstructure, not larger graphs or longer prompts, is\nthe key substrate for efficient, long-running, and\nagentic RAG systems.\n1.1\nDesign Principles\nThe design of AutoPrunedRetriever is guided by\nthree principles, each directly addressing one of\nthe limitations illustrated in Example 1.\nP1.\nLocal, incremental structure (addresses\nM1).\nTo avoid the cost and brittleness of\nglobal graph maintenance, AutoPrunedRetriever\nbuilds reasoning structure locally and incremen-\ntally. Text is encoded into symbolic triples and\ngrouped into small, coherent graphs that can be ex-\ntended over time. Entity consolidation is applied\nselectively at the symbol level, allowing aliases to\nbe merged without re-extracting text or relinking\nglobal structure.\nP2. Path-centric retrieval (addresses M2). Rea-\nsoning is realized by short chains of entities and\nrelations, not broad neighborhoods. AutoPrune-\ndRetriever therefore treats edge sequences as the\nprimary retrieval unit and scores candidate reason-\ning paths directly, avoiding the retrieval of large\nsubgraphs that do not contribute to the required in-\nference.\nP3. Exact symbolic reuse (addresses M3). To\nprevent repeated serialization of overlapping con-\ntext, reuse across queries is exact and symbolic\n"}, {"page": 3, "text": "Corpus\nTriples\nGraphs\nMeta \nCodebook\n...\nRetrieved \nTriples\nPruned\nRetrieved \nTriples\nQuery\nAI\nLanguage \nModel\nAnswer\nFigure 1:\nAutoPrunedRetriever pipeline:\n(1) en-\ncode into symbols and edges, (2) build chunked small\ngraphs, (3) coarse→fine retrieval, (4) selector + com-\npact prompt packing, (5) entity-only consolidation with\na DPO wrapper to trade accuracy vs. tokens.\nrather than textual. AutoPrunedRetriever caches\nreasoning subgraphs as compact sequences of en-\ntity–relation identifiers and constructs prompts\nthat include only novel or non-redundant evidence,\nensuring that token usage scales with new reason-\ning rather than repeated context.\nWe discuss more details about these principles\nin Section 2.\n2\nMethod\n2.1\nOverview\nFree text is a noisy, length-biased substrate for\nreasoning: it repeats the same facts in many sur-\nface forms, conflates content with string realiza-\ntion, and penalizes reuse by charging tokens re-\npeatedly for identical information. These proper-\nties make it ill-suited for persistent, multi-hop rea-\nsoning across related queries.\nAutoPrunedRetriever addresses these issues\nwith a symbol-first pipeline that implements the\nthree design principles introduced in Section 1.1.\nSpecifically, we: (1) encode questions, answers,\nand facts into a shared symbolic codebook of\nentities and relations (Sec. 2.2); (2) build local,\ncoherent reasoning graphs that serve as retrieval\natoms (Sec. 2.3); (3) retrieve paths rather than\nneighborhoods via coarse-to-fine scoring in sym-\nbol space (Sec. 2.4); (4) select and package only\nnon-redundant structure into compact prompts\n(Secs. 2.5, 2.6); and (5) consolidate entities incre-\nmentally to keep the persistent graph compact over\ntime (Sec. 2.7).\nFinally, we adapt compression\nbehavior to accuracy–efficiency tradeoffs using a\nlightweight DPO-trained policy (Sec. 2.8).\nEach component is designed to ensure that re-\ntrieval, reasoning, and prompting scale with new\nreasoning rather than repeated context.\n2.2\nStep 1: Symbolic Encoding\nIntuition.\nWe normalize free-form text into a\nsmall set of symbols and the edges they instanti-\nate. This exposes shared structure across questions\nand facts and makes reuse exact via IDs rather\nthan brittle string matches. Because natural lan-\nguage is heavy-tailed, most informational mass is\ncarried by a small “core” vocabulary; encoding\nthose into a codebook yields large compression\ngains (see Lemma 1). In App. A.2 (Lemma 14,\nProposition 16) also shows that such E–R–E code-\nbooks mitigate the “token dilution” effect of long\nsequence embeddings.\nFormulation.\nFor a text span y (question, an-\nswer, fact), a parser (REBEL or an LLM) produces\ntriples\nτ(y) ⊆UE × UR × UE.\nWe maintain a meta-codebook\nC = (E, R, M, Q, A, F, Eemb, Remb),\nwhere E, R are entity and relation dictionaries,\nM ⊆E × R × E is the sparse set of unique edges,\nand Q, A, F store sequences of edge IDs for ques-\ntions, answers, and facts.\nAn INDEXIFY map\ny = INDEXIFY\n\u0000τ(y); E, R, M\n\u0001\n∈M⋆\nserializes\ntext\ninto\nedge\nindices,\nextending\n(E, R, M) only when new symbols/edges appear.\nWe append y to the appropriate store in Q, A, F.\nThe embedding tables Eemb : E →Rd and\nRemb : R →Rd give us E–R–E embeddings for\neach edge (e, r, e′) ∈M.\n2.3\nStep 2: Chunked Small Graphs\n(Local-First Construction)\nIntuition.\nInstead of inserting every triple into\na global graph, we build small, locally coherent\ngraphs in a single pass. The question is no longer\n“where in the global graph does this triple live?”\nbut “does this triple fit the current small graph?”\nThis keeps working sets small and updates lo-\ncal, while preserving global consistency via shared\nIDs in M.\nRuns and modalities.\nFor each modality y ∈\n{Q, A, F} we build a run repository\nRuns(y) ∈\n\u0000M⋆\u0001⋆,\na list of edge-ID sequences (runs) that correspond\nto small graphs.\n"}, {"page": 4, "text": "Streaming construction.\nWe maintain a current\nsmall graph Gk = (Vk, Ek) with an embedding\ncentroid. For each incoming triple, we compute a\nfit score that combines:\n1.\nSemantic cohesion: cosine similarity be-\ntween the triple embedding and the centroid of Gk.\n2. Structural continuity: a bonus when the\ntriple continues a path (e.g., tailprev=headnew) or\nreuses nodes in Vk.\nIf the (bonus-adjusted) score exceeds a thresh-\nold τ, we append to Gk; otherwise we close Gk,\nlinearize its edges into gk ∈M⋆, store gk ∈\nRuns(y), and start Gk+1. This yields maximal lo-\ncally coherent segments (Lemma 2).\nBoundary refinement.\nA single pass can over-\ncut near boundaries, so we run a local merge-if-\nnot-a-true-cut test on adjacent runs: we re-embed\nthe concatenation of the boundary region, re-run\nthe same segmenter, and merge if the new segmen-\ntation does not place a cut at the original bound-\nary. Surviving boundaries are self-consistent fixed\npoints of the segmenter (Lemma 3).\nThe result is a sequence of compact, coherent\nruns that serve as retrieval atoms.\nWe quantify\ntheir intra-run cohesion and the induced working-\nset reduction for retrieval in Lemmas 4 and Lem-\nmas 5.\n2.4\nStep 3: Coarse to Fine Path Retrieval\nIntuition.\nNaively\nembedding\nevery\nquery\nagainst every run is slow and favors long, noisy\nchunks. We instead first work in symbol space to\nget a small shortlist and then do a more detailed\ntriple-level check on that shortlist. This two-layer\nscheme keeps cost near O(k) rather than O(n)\nwhile preserving precision (Lemma 7).\nCoarse stage (symbol-space recall).\nEach run\ndecodes to triples (h, ρ, t). We pool entity embed-\ndings into E(·) and relation embeddings into R(·).\nFor a query q and candidate run f we compute a\nsimple max-pair score over entities and relations,\nscoarse(q, f) = went max\ni,j cos\n\u0000E(q)i, E(f)j\n\u0001\n+ wrel max\np,r cos\n\u0000R(q)p, R(f)r\n\u0001\n,\nand keep the top-k runs as a high-recall shortlist\nIk. This stage only touches small entity/relation\nsets and is therefore very fast.\nFine stage (triple-aware re-ranking).\nFor each\ncandidate in Ik, we linearize triples (h ρ t) into\nshort lines, embed them, and build a similarity ma-\ntrix S between query and candidate lines. The final\nscore is a weighted sum of five simple terms com-\nputed on S: a top-t mean over the best entries (re-\nlational strength), a coverage term counting how\nmany query triples are well matched, a soft many-\nto-many overlap term, a greedy 1:1 match encour-\naging parsimonious alignment, and a small whole-\nchunk bonus gated by full-text similarity to avoid\n“longer is better”. We then take the global TopM\nruns across all queries and channels (answers /\nfacts / prior questions). Exact formulas and hy-\nperparameters are deferred to App. A.3.\n2.5\nStep 4: Knowledge Selection\nIntuition.\nReal corpora have heavy-tailed reuse\nof motifs, so naive “pull everything similar” either\nmisses paraphrased support or floods the prompt\nwith near-duplicates. We therefore operate on runs\nand, for each channel (answers, facts, related ques-\ntions), choose an action\ns ∈{include all, unique, not include}.\nHere include all keeps all surviving runs (e.g.,\nfor safety/audit regimes), unique keeps one repre-\nsentative per semantic cluster to avoid echoing and\ntoken bloat, and not include drops the channel\nwhen it adds little beyond context. Semantic clus-\nters are defined in run-embedding space; we pick\na consensus representative per cluster, and show\nin Lemma 8 that this representative stays close to\nparaphrastic variants.\n2.6\nStep 5: Compact prompt Construction\nGiven the selected runs, we assemble the minimal\nsymbolic state the LLM needs:\nU = q ∪a ∪f,\nE′ = {h : t : (h, ρ, t) ∈U},\nR′ = {ρ : (h, ρ, t) ∈U}.\nWe maintain two equivalent encodings and\nchoose the cheaper at query time:\n(1) word\ntriples,\nwhich list (h, ρ, t) directly for low-\nredundancy regimes; and (2) compact indices,\nwhich map E′, R′ to short IDs and represent\nquery/answer/fact sequences as ID triples.\nThe\nprompt payload is Π = (E′, R′, q, a, f, rules),\nwith a brief textual header explaining the ID for-\nmat. Token cost scales with |E′|+|R′|+|q|+|a|+\n|f|, typically far below concatenated passages.\n"}, {"page": 5, "text": "2.7\nStep 6: Entity-Only Consolidation\nIntuition.\nLong-running deployments accumu-\nlate aliases, misspellings, and near-duplicates\n(IBM vs. International Business Machines). We\nconsolidate entities only, then remap all edges and\nsequences.\n• Layer 1 (ANN+KNN). Continuously build\nan ANN-backed k-NN graph over entity embed-\ndings; connect pairs with cosine above a con-\nservative threshold τE and form provisional alias\ngroups.\n• Layer 2 (on-demand k-means).\nWhen\nmemory or |E| exceeds a budget, refine groups\nwith k-means and choose medoid representatives\nmE(·), which minimize within-cluster distortion\n(Lemma 11). Each edge (u, r, v) is remapped to\n(mE(u), r, mE(v)), and duplicates are removed.\nBecause questions/answers/facts are stored as\nedge-ID sequences,\nthis remap automatically\ncleans them as well. This quotienting can only re-\nduce edge and sequence cardinalities (Lemma 9)\nand does not increase the number of sentence-level\ntext encodings (Lemma 10).\n2.8\nStep 7: Adaptive Compression via DPO\nThe “right” selector choice depends on the query\n(ambiguity, hops), model (context length, robust-\nness), domain (redundancy), and user goals (accu-\nracy vs. latency/tokens). We learn a small categor-\nical policy πθ over the selector actions per chan-\nnel, conditioned on features such as query length,\nambiguity scores, model ID, and token budget.\nOffline, for each query we evaluate several se-\nlector configurations y and compute a utility\nU(x, y) = α · Acc + δ · Faithfulness−\nβ · Tokens −γ · Latency.\nWe form preference pairs (x, y+, y−) whenever\nU(x, y+) > U(x, y−), and train πθ(y | x) with\nDPO against a fixed reference policy.\nUnder a\nBradley–Terry preference model, DPO aligns pol-\nicy log-odds with utility differences up to a scal-\ning and reference correction (Lemma 12), and a\nsimple action lattice yields monotone token con-\ntrol under a budget constraint (Lemma 13).\nAt inference time, the policy picks (include\nall, unique, or not include) per channel, steer-\ning AutoPrunedRetriever to the appropriate oper-\nating point—e.g., overlap-heavy scaffolds for am-\nbiguous multi-hop questions vs. aggressive dedu-\nplication under tight budgets—while retaining the\nsame symbolic infrastructure.\n3\nExperiments\nWe study:\n• RQ1 (§3.2): complex reasoning performance\non Medical, Novel, STEM, TV.\n• RQ2 (§3.3):\nefficiency (tokens, latency,\nworkspace) on STEM/TV.\n• RQ3 (§3.4): overall performance on the full\nGraphRAG benchmark.\n3.1\nExperimental Settings\nDevices and models.\nGraphRAG experiments\n(§3.4) run on an Intel i9-13900KF, 64 GB RAM,\nRTX 4090 (24 GB); STEM/TV experiments (§3.2,\n§3.3) use an A100 (40 GB). All LLM-backed pars-\ning/judging uses the gpt-4o-mini API.\nDatasets.\nMedical and Novel are from the\nGraphRAG-Benchmark (Xiang et al., 2025) with\nFact Retrieval, Complex Reasoning, Contextual\nSummarize, and Creative Generation.\nWe ad-\nditionally construct TV and STEM from the\nHotpotQA Wikipedia pipeline (HotpotQA Team,\n2019), grouped into 47 TV and 32 STEM micro-\ncorpora; TV questions focus on character/episode\nrelations, STEM on cross-sentence scientific infer-\nence.\nEvaluation and parsers. We follow the LLM-\njudge protocol of Xiang et al. (Xiang et al.,\n2025).\nAUTOPRUNEDRETRIEVER is evaluated\nwith two front ends:\na REBEL-based triplet\nparser (Huguet Cabot and Navigli, 2021) and\nan LLM-based parser (gpt-4o-mini); both use\nthe same pruning and indices-only prompting\npipeline.\n3.2\nFull Complex-Reasoning Evaluation\nWe aggregate all complex-reasoning sources:\nMedical-CR, Novel-CR, STEM, TV, to test a\nsingle pruned, symbolic pipeline across technical,\nnarrative, and pop-culture reasoning.\nQuantitative results. Across all four sets, AU-\nTOPRUNEDRETRIEVER is consistently strongest\n(Fig. 2).\nOn Medical-CR and Novel-CR, the\nREBEL variant reaches 72.49% and 63.02%\nACC vs. HIPPORAG2 (61.98%, 53.38%), i.e.,\n+10.51/+9.64 points.\nThe LLM-parser vari-\nant is close (71.59%, 62.80%), indicating that\nthe gain mainly comes from symbolic pruning\n"}, {"page": 6, "text": "and retrieval.\nOn STEM, REBEL/LLM obtain\n81.4%/78.1% vs. HIPPORAG2 (69.9%); on TV,\n68.2%/65.2% vs. HIPPORAG2 (59.5%). The or-\ndering is the same on all four: APR-REBEL >\nAPR-llm > HippoRAG2 > LightRAG.\nMedical Novel STEM\nTV\n0.0\n0.2\n0.4\n0.6\n0.8\nAnswer Correctness\n0.725\n0.630\n0.814\n0.682\n0.716\n0.628\n0.781\n0.650\n0.620\n0.534\n0.699\n0.595\n0.613\n0.491\n0.171\n0.465\nMedical Novel STEM\nTV\n0.0\n0.1\n0.2\n0.3\n0.4\nROUGE-L\n0.308\n0.312\n0.308\n0.184\n0.311\n0.354\n0.304\n0.191\n0.370\n0.334\n0.317\n0.195\n0.250\n0.242\n0.000\n0.117\nAutoPrunedRetriever-REBEL\nAutoPrunedRetriever-llm\nHippoRAG2\nLightRAG\nFigure 2: Average answer correctness on all complex-\nreasoning sets (Medical-CR, Novel-CR, STEM, TV)\nfor HippoRAG2, LightRAG, AutoPrunedRetriever-\nREBEL, AutoPrunedRetriever-llm.\nCase study (STEM): reasoning over ecologi-\ncal chains. Consider the STEM question “How\ndoes the variability in the size of brown bears\nacross different regions serve as evidence for un-\nderstanding their adaptability in various environ-\nments?” In our run this was a question where AU-\nTOPRUNEDRETRIEVER beat both HIPPORAG2\nand LIGHTRAG. What our retriever actually sur-\nfaced was a compact symbolic subgraph centered\non three functional edges: (1) region →resource\navailability, (2) resources →body size, and (3)\nbody size →environmental adaptability. Because\nthose three edges were present together, the LLM\ncould reconstruct the full causal chain:\nregion ⇒food ⇒size ⇒adaptability.\nThe generated answer therefore explained that\nlarge coastal/Kodiak bears reflect high salmon\n(high calories), whereas smaller inland bears re-\nflect limited resources, and that this variation\nitself is evidence of species-level adaptability.\nBy contrast, HIPPORAG2 retrieved a broader,\ntaxonomy-oriented context about brown-bear sub-\nspecies (“the taxonomy of brown bears remains\nsomewhat bewildering”), which led the model to\nproduce a descriptive answer (“there are many va-\nrieties, so sizes differ”) but not a mechanistic one\n(no resource →size link). LIGHTRAG retrieved\ntopic-level chunks like “Brown bears vary greatly\nin size depending on where they live,” which was\nenough for correlation but not for causation. This\nillustrates the core advantage: our pruning keeps\nonly the minimal but functional intermediates, so\nthe model can follow the hops in order instead of\nguessing them.\nCase study (TV): retrieving the exact two\npieces. A similar pattern appears on TV-style, en-\ntangled questions, e.g., “In The Simpsons minor-\ncharacter descriptions, what in-universe line ex-\nplains the Yes Guy’s stretched-out ‘Ye-e-e-s?!’,\nand what does the entry say about Wiseguy not\nactually having a single fixed name?” This ques-\ntion is hard not because the language is long, but\nbecause the answer lives in two separate men-\ntions: one that gives the in-universe justification\n(“I had a stro-o-o-oke”) and another that clarifies\nthe meta-labeling of the Wiseguy character. AU-\nTOPRUNEDRETRIEVER retrieved precisely those\ntwo pieces as separate edges/nodes and presented\nthem together, so the LLM could output both the\nin-universe gag and the meta-level note about the\ncharacter not having a fixed proper name. HIP-\nPORAG2, which tends to over-expand its graph,\npulled a broader “recurring jokes / minor charac-\nters” context and produced a generic “it’s a run-\nning gag” answer that failed to name the stroke\nline. LIGHTRAG, which collapses to topic-level\nchunks, also stayed at the descriptive level (“re-\ncurring jokes create humor”) and missed the ex-\nact line. This shows that for entangled narrative\nquestions, the benefit is not “more graph,” but “the\nright two edges at once.”\nOverall,\ncomplex reasoning is where the\npruned, indices-only pipeline shows the clearest\nadvantage over prior GraphRAG variants.\n3.3\nEfficiency on Instrumented Corpora\n(STEM, TV)\nWe measure efficiency on STEM and TV, where\nwe fully control build-time and storage logging.\nRetrieval prompt tokens and latency.\nFig-\nure 3 shows average query-time input tokens.\nAUTOPRUNEDRETRIEVER-REBEL\nis\nmost\ncompact (about 1,090 tokens on STEM and 523\non TV), followed by AUTOPRUNEDRETRIEVER-\nLLM (3,027 / 592).\nHIPPORAG2 and LIGH-\nTRAG send much larger contexts (1,898/1,589\nand 8,846/2,964).\nEnd-to-end latency (Fig. 4)\nroughly tracks this token ordering: methods with\nlonger prompts are slower, while APR-REBEL\nremains competitive.\nBuild-time\ngraph/prompt\ntokens\nand\nworkspace. Figure 5 reports serialized graph-side\npayloads and workspace size.\nAPR-REBEL is\n"}, {"page": 7, "text": "STEM\nTV\n0\n2000\n4000\n6000\n8000\n10000\nAvg Input Tokens\n1090.161\n523.196\n3026.610\n591.863\n1897.680\n1589.000\n8846.083\n2964.392\nSTEM\nTV\n0\n10\n20\n30\n40\n50\n60\nAvg Output Tokens\n53.195\n50.598\n56.212\n49.441\n52.375\n48.890\n3.250\n56.287\nAutoPrunedRetriever-REBEL\nAutoPrunedRetriever-llm\nHippoRAG2\nLightRAG\nFigure 3: Input and output token usage on STEM and\nTV.\nSTEM\nTV\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nAvg gen Latency (s)\n3.091\n2.607\n2.955\n2.601\n0.180\n0.199\n1.653\n0.687\nSTEM\nTV\n0\n5\n10\n15\n20\nAvg Retrieval Latency (s)\n8.948\n14.254\n11.524\n18.062\n0.214\n0.212\n3.744\n1.210\nAutoPrunedRetriever-REBEL\nAutoPrunedRetriever-llm\nHippoRAG2\nLightRAG\nFigure 4: End-to-end latency on STEM and TV.\nsmallest on both corpora; APR-llm stores more\nLLM-extracted triples (≈1.2 × 106 graph/prompt\ntokens on STEM and 2.84 × 106 on TV), but still\nbelow HIPPORAG2 and LIGHTRAG. Figure 6\nshows that APR’s codebook / graph-size growth\nhas plateaus where new items merge into existing\nentities, reflecting the two-layer entity-pruning\nstep.\nSTEM\nTV\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nGraph Prompt Tokens\n1e7\n0\n0\n1,204,757\n2,835,689\n2,212,066\n5,016,941\n4,822,577\n11,161,008\nSTEM\nTV\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nGraph Completion Tokens\n1e6\n0\n0\n761,816\n2,494,504\n624,078\n1,957,095\n764,044\n1,980,769\nSTEM\nTV\n0\n500\n1000\n1500\n2000\n2500\nWorkspace Size (MB)\n68\n542\n705\n2,292\n259\n443\n154\n370\nAutoPrunedRetriever-REBEL\nAutoPrunedRetriever-llm\nHippoRAG2\nLightRAG\nFigure\n5:\nBuild-time\ngraph/prompt\ntokens\nand\nworkspace usage on STEM and TV.\n3.4\nFull GraphRAG Benchmark\nWe now evaluate on the full GraphRAG bench-\nmark (Xiang et al., 2025), i.e., Medical and Novel\nacross Fact Retrieval, Complex Reasoning, Con-\ntextual Summarize, and Creative Generation (Ta-\nble 1).\nOn Contextual Summarize, APR transfers\nwell:\non Medical it reaches 68.78%/70.14%\nACC\n(REBEL/LLM),\nslightly\nabove\nFAST-\nGRAPHRAG (67.88%), and on Novel it reaches\n0\n20\n40\n60\n80\n100\n120\nQuestion Index\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n MB from first\n0\n20\n40\n60\n80\n100\nQuestion Index\n0.0\n0.5\n1.0\n1.5\n2.0\n MB from first\nAutoPrunedRetriever-REBEL\nAutoPrunedRetriever-llm\nFigure 6: APR codebook / graph-size evolution (left:\nSTEM, right: TV).\n82.55%/83.10%,\nfar above MS-GRAPHRAG\n(LOCAL) (64.40%).\nOn Fact Retrieval, APR-\nREBEL matches or exceeds strong baselines in\nROUGE-L (e.g., 38.02 on Novel) while keep-\ning ACC competitive with classic RAG and\nHippo-style systems.\nFor Creative Genera-\ntion, APR-llm attains 62.97% ACC on Novel\nand 65.02% on Medical, near or above other\ngraph-based methods.\nToken usage on GraphRAG (Fig. 7) remains\nlow: APR-REBEL uses 1,110 tokens on Novel\nand 1,341 on Medical; APR-llm uses 956 and\n2,234, placing both among the most compact\ngraph-aware systems while staying competitive or\nSOTA on complex reasoning and summarization.\nNovel\nMedical\n0\n50000\n100000\n150000\n200000\n250000\n300000\n350000\nAvg Tokens\n1,110\n1,341\n956\n2,243\n879\n954\n38,707\n39,821\n331,375\n332,881\n1,008\n1,020\n100,832\n100,310\n4,204\n4,298\n3,441\n3,510\n7,208\n7,342\nAutoPrunedRetriever-REBEL\nAutoPrunedRetriever-llm\nV-RAG\nMS-GraphRAG(local)\nMS-GraphRAG(global)\nHippoRAG2\nLightRAG\nFast-GraphRAG\nRAPTOR\nHippoRAG\nFigure 7: Average input token usage on the GraphRAG\nbenchmark (Medical, Novel).\n4\nRelated Work\nRetrieval-Augmented\nGeneration\n(RAG).\nRAG grounds LLMs in external knowledge via\nretriever–reader pipelines such as DrQA (Chen\net al., 2017), DPR (Karpukhin et al., 2020b), and\nthe original RAG model (Lewis et al., 2020).\nREALM integrates retrieval into pre-training with\na non-parametric memory (Guu et al., 2020),\nwhile FiD performs passage-wise encoding and\nfusion for strong open-domain QA at higher\ndecoding cost (Izacard and Grave, 2021). Atlas\nand follow-ups show that retrieval-augmented\n"}, {"page": 8, "text": "Table 1: GraphRAG benchmark results on Novel and Medical.\nCategory\nModel\nFact Retrieval\nComplex Reasoning Contextual Summarize\nCreative Generation\nACC\nROUGE-L\nACC\nROUGE-L\nACC\nCov\nACC\nCov\nFS Cov\nNovel Dataset\nRAG (w/o rerank)\n58.76\n37.35\n41.35\n15.12\n50.08\n82.53\n41.52 47.46\n37.84\nRAG (w/ rerank)\n60.92\n36.08\n42.93\n15.39\n51.30\n83.64\n38.26 49.21\n40.04\nMS-GraphRAG (local) (Edge et al., 2025)\n49.29\n26.11\n50.93\n24.09\n64.40\n75.58\n39.10 55.44\n35.65\nHippoRAG (Guti´errez et al., 2025)\n52.93\n26.65\n38.52\n11.16\n48.70\n85.55\n38.85 71.53\n38.97\nHippoRAG2 (Jim´enez Guti´errez et al., 2025) 60.14\n31.35\n53.38\n33.42\n64.10\n70.84\n48.28 49.84\n30.95\nLightRAG (Guo et al., 2025a)\n58.62\n35.72\n49.07\n24.16\n48.85\n63.05\n23.80 57.28\n25.01\nFast-GraphRAG (CircleMind AI, 2024)\n56.95\n35.90\n48.55\n21.12\n56.41\n80.82\n46.18 57.19\n36.99\nRAPTOR (Sarthi et al., 2024)\n49.25\n23.74\n38.59\n11.66\n47.10\n82.33\n38.01 70.85\n35.88\nLazy-GraphRAG (Edge et al., 2024a)\n51.65\n36.97\n49.22\n23.48\n58.29\n76.94\n43.23 50.69\n39.74\nAutoPrunedRetriever-REBEL\n49.25\n38.02\n63.02\n31.25\n82.55\n83.95\n59.94 25.78\n21.21\nAutoPrunedRetriever-llm\n45.99\n26.99\n62.80\n35.35\n83.10\n83.86\n62.97 34.40\n22.13\nMedical Dataset\nRAG (w/o rerank)\n63.72\n29.21\n57.61\n13.98\n63.72\n77.34\n58.94 35.88\n57.87\nRAG (w/ rerank)\n64.73\n30.75\n58.64\n15.57\n65.75\n78.54\n60.61 36.74\n58.72\nMS-GraphRAG (local) (Edge et al., 2025)\n38.63\n26.80\n47.04\n21.99\n41.87\n22.98\n53.11 32.65\n39.42\nHippoRAG (Guti´errez et al., 2025)\n56.14\n20.95\n55.87\n13.57\n59.86\n62.73\n64.43 69.21\n65.56\nHippoRAG2 (Jim´enez Guti´errez et al., 2025) 66.28\n36.69\n61.98\n36.97\n63.08\n46.13\n68.05 58.78\n51.54\nLightRAG (Guo et al., 2025a)\n63.32\n37.19\n61.32\n24.98\n63.14\n51.16\n-\n-\n-\nFast-GraphRAG (CircleMind AI, 2024)\n60.93\n31.04\n61.73\n21.37\n67.88\n52.07\n65.93 56.07\n44.73\nRAPTOR (Sarthi et al., 2024)\n54.07\n17.93\n53.20\n11.73\n58.73\n78.28\n-\n-\n-\nLazy-GraphRAG (Edge et al., 2024a)\n60.25\n31.66\n47.82\n22.68\n57.28\n55.92\n62.22 30.95\n43.79\nAutoPrunedRetriever-REBEL\n61.28\n32.96\n72.49\n30.79\n68.78\n40.15\n64.04 32.19\n11.12\nAutoPrunedRetriever-llm\n61.25\n34.69\n71.59\n31.11\n70.14\n40.59\n65.02 33.06\n28.62\nLMs with updatable indices and reranking can\nreach\nstrong\nfew-shot\nperformance\n(Izacard\net al., 2022), but these text-first systems still treat\nqueries independently and remain passage-centric\nand token-heavy for multi-hop or long-context\nreasoning.\nVector Retrieval and Re-ranking.\nDense\nretrieval embeds queries and documents into a\nshared space (Lee et al., 2019; Karpukhin et al.,\n2020b) and is typically served by ANN in-\ndexes such as FAISS (Johnson et al., 2017) or\nHNSW (Malkov and Yashunin, 2016).\nTwo-\nstage pipelines then apply stronger rerankers:\nBERT-based cross-encoders (Nogueira and Cho,\n2019)\nand\nlate-interaction\nmodels\nlike\nCol-\nBERT/ColBERTv2 (Khattab and Zaharia, 2020;\nSanthanam et al., 2022) improve ranking quality\nwhile trading off latency and indexability.\nGraph-based\nRAG\n(GraphRAG).\nGraph-\nbased RAG replaces flat passages with entity–\nrelation structure. MS-GraphRAG builds graphs\nover private corpora and retrieves summarized\nsubgraphs, with a local variant that builds per-\nsession graphs (Edge et al., 2024c).\nLazy-\nGraphRAG defers graph construction to query\ntime (Edge et al., 2024b).\nHippoRAG and\nHippoRAG2 introduce neuro-inspired long-term\nmemory that consolidates and reuses reasoning\npaths (Guti´errez et al., 2025a,b). LightRAG and\nFast-GraphRAG simplify graphs into key–value\nforms for low-latency retrieval (Guo et al., 2025b;\nCircleMind AI, 2024), while RAPTOR uses a\ntree of recursive summaries instead of an explicit\ngraph (Sarthi et al., 2024).\nTogether, these il-\nlustrate the shift from passage-centric to struc-\ntured retrieval, but still pay substantial graph-\nserialization and token cost.\nMemory-Augmented\nArchitectures.\nExternal-memory\nLMs\nmaintain\npersistent,\nupdatable stores alongside parameters, exploring\nscalable memory (Wu et al., 2022), efficient\nretrieval (Liu et al., 2024), and continual con-\nsolidation (Dao et al., 2023).\nThese build on\ndifferentiable memory architectures such as mem-\nory networks (Weston et al., 2014) and neural\nTuring machines (Graves et al., 2014).\nEfficient Model Deployment. LLM efficiency\nis improved by quantization and low-rank adap-\ntation (e.g., QLoRA) (Dettmers et al., 2023),\nmixed-precision training (Micikevicius et al.,\n2018), and hardware-aware optimization (Ai et al.,\n2024). Adaptive computation and dynamic rout-\ning (Schuster et al., 2022; Li et al., 2023) further\ntrade depth and routing complexity against accu-\nracy, complementing retrieval- and memory-side\nefforts to cut tokens and latency.\n"}, {"page": 9, "text": "5\nLimitations\nOur study has several limitations.\nFirst, we\nevaluate AUTOPRUNEDRETRIEVER primarily on\nEnglish, knowledge-intensive QA benchmarks\n(GraphRAG-Benchmark, STEM, TV); it is un-\nclear how well the same pruning and symbol-\nization scheme transfers to other languages, do-\nmains, or noisy user logs. Second, our pipeline\ndepends on upstream triple extractors (REBEL or\nan LLM); systematic extraction errors or missing\nrelations can still harm downstream reasoning, and\nwe do not jointly train extraction and retrieval. Fi-\nnally, we focus on text-only corpora and single-\nturn question answering in agentic settings, leav-\ning multimodal inputs, tool-use workflows, and\nhuman-in-the-loop updates to future work.\nReferences\nQinbin Ai, Mingxuan Chen, Wenhu Chen, Tri Dao,\nDaniel Fu, Albert Gu, and 1 others. 2024.\nEffi-\ncient inference for large language models: A sur-\nvey. Foundations and Trends in Machine Learning,\n17(2):145–387.\nJinheon Baek, Myeongho Jeong, Sung Ju Hwang, and\nJungwoo Park. 2023. Knowledge graph grounded\nquestion answering with graph neural networks. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n’23.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In ACL.\nCircleMind AI. 2024.\nFast graphrag:\nHigh-speed\ngraph-based retrieval-augmented generation. Ana-\nlytics Vidhya Blog.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and\nChristopher R´e. 2023. Hungry hungry hippos: To-\nwards language modeling with state space models.\nIn International Conference on Learning Represen-\ntations, ICLR ’23.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. In Advances in Neural Informa-\ntion Processing Systems, volume 36 of NeurIPS ’23.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua\nBradley, Alex Chao, Apurva Mody, Steven Truitt,\nDasha Metropolitansky, Robert Osazuwa Ness, and\nJonathan Larson. 2025.\nFrom local to global: A\ngraph rag approach to query-focused summariza-\ntion. Preprint, arXiv:2404.16130.\nDarren Edge, Ha Trinh, and Jonathan Larson. 2024a.\nLazyGraphRAG: Setting a New Standard for Qual-\nity and Cost. Microsoft Research Blog.\nDarren Edge, Ha Trinh, and Jonathan Larson. 2024b.\nLazyGraphRAG: Setting a New Standard for Qual-\nity and Cost. Microsoft Research Blog.\nDarren Edge, Ha Trinh, Jonathan Larson, Alex Chao,\nand Robert Osazuwa Ness. 2024c.\nFrom local to\nglobal: A graph rag approach to query-focused sum-\nmarization. ArXiv:2404.16130 [cs.CL].\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. Preprint, arXiv:1410.5401.\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao\nHuang. 2025a. Lightrag: Simple and fast retrieval-\naugmented generation. Preprint, arXiv:2410.05779.\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao\nHuang. 2025b. Lightrag: Simple and fast retrieval-\naugmented generation. ArXiv:2410.05779 [cs.IR].\nBernal Jim´enez Guti´errez, Yiheng Shu, Yu Gu, Michi-\nhiro Yasunaga, and Yu Su. 2025a.\nHippoRAG:\nNeurobiologically Inspired Long-Term Memory\nfor Large Language Models.\nArXiv:2405.14831\n[cs.CL].\nBernal Jim´enez Guti´errez, Yiheng Shu, Weijian Qi,\nSizhe Zhou, and Yu Su. 2025b. From rag to mem-\nory: Non-parametric continual learning for large\nlanguage models. ArXiv:2502.14802 [cs.CL].\nBernal Jim´enez Guti´errez, Yiheng Shu, Yu Gu, Michi-\nhiro Yasunaga, and Yu Su. 2025. Hipporag: Neu-\nrobiologically inspired long-term memory for large\nlanguage models. Preprint, arXiv:2405.14831.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020.\nREALM:\nRetrieval-augmented language model pre-training.\nIn ICML.\nHaoyu Han, Yue Wang, Hagai Shomer, Kai Guo,\nJiawei Ding, Yang Lei, Bo Li, and Jie Tang.\n2024. Retrieval-augmented generation with graphs\n(graphrag). Preprint, arXiv:2404.16130.\nHotpotQA Team. 2019.\nPreprocessed wikipedia\nfor hotpotqa.\nhttps://hotpotqa.github.io/\nwiki-readme.html.\nPere-Llu´ıs Huguet Cabot and Roberto Navigli. 2021.\nREBEL: Relation extraction by end-to-end language\ngeneration. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, pages 2370–\n2381, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open-\ndomain question answering. In ICLR.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022.\nAtlas: Few-shot learning\nwith retrieval augmented language models. Journal\nof Machine Learning Research, 23(251):1–43.\n"}, {"page": 10, "text": "Bernal Jim´enez Guti´errez, Yiheng Shu, Weijian Qi,\nSizhe Zhou, and Yu Su. 2025. From rag to mem-\nory: Non-parametric continual learning for large\nlanguage models. arXiv preprint arXiv:2502.14802.\nPoster at ICML 2025.\nJeff Johnson, Matthijs Douze, and Herv´e J´egou.\n2017.\nBillion-scale similarity search with GPUs.\narXiv:1702.08734.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020a.\nDense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020b.\nDense passage retrieval for\nopen-domain question answering.\narXiv preprint\narXiv:2004.04906.\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\nficient and effective passage search via contextual-\nized late interaction over BERT. In SIGIR.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open-\ndomain question answering. In ACL.\nPatrick\nLewis,\nEthan\nPerez,\nAleksandra\nPiktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich\nK¨uttler,\nMike\nLewis,\nWen-tau\nYih,\nTim Rockt¨aschel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive nlp tasks. In Advances in Neu-\nral Information Processing Systems, volume 33 of\nNeurIPS ’20, pages 9459–9474.\nYuhong Li, Daniel Fu, Wenhu Chen, Manoj Kumar,\nNoah Smith, Ming-Wei Chen, and Christopher R´e.\n2023. Efficient streaming language models with at-\ntention sinks. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning, ICML ’23.\nZhenghao Liu, Jianfeng Wang, Jie Tang, Jie Zhou, and\nAshwin Kalyan. 2024. Memory-efficient large lan-\nguage models via dynamic memory allocation. In\nInternational Conference on Learning Representa-\ntions, ICLR ’24.\nYury A. Malkov and Dmitry A. Yashunin. 2016. Ef-\nficient and robust approximate nearest neighbor\nsearch using hierarchical navigable small world\ngraphs. arXiv:1603.09320.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg,\nMichael Houston,\nOleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\nsion training. In International Conference on Learn-\ning Representations, ICLR ’18.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. In arXiv:1901.04085.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nKeshav Santhanam, Omar Khattab, and et al. 2022.\nColbertv2:\nEffective and efficient retrieval via\nlightweight late interaction. In NAACL.\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\nKhanna, Anna Goldie, and Christopher D. Manning.\n2024. Raptor: Recursive abstractive processing for\ntree-organized retrieval. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR). ArXiv:2401.18059 [cs.CL].\nTal Schuster, Adam Fisch, Tommi Jaakkola, and\nRegina Barzilay. 2022. Confident adaptive language\nmodeling. In Advances in Neural Information Pro-\ncessing Systems, volume 35 of NeurIPS ’22, pages\n17456–17472.\nZeyu\nSun,\nHongyang\nYang,\nRui\nZhou,\nChen\nWang, Xiaodong Liu, and Xuanjing Huang. 2022.\nGraphene: Retrieval-augmented generation for effi-\ncient knowledge-intensive reasoning. In Advances\nin Neural Information Processing Systems, vol-\nume 35 of NeurIPS ’22.\nXiaorui Wang, Zhiyuan Zhang, Yizheng Hao, Lei Li,\nLei Hou, Zhiyuan Liu, Xuan Song, and Maosong\nSun. 2023. Graph-based memory for large language\nmodels.\nIn Proceedings of the 40th International\nConference on Machine Learning, ICML ’23.\nJason Weston, Sumit Chopra, and Antoine Bordes.\n2014. Memory networks. In International Confer-\nence on Learning Representations, ICLR ’15.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nBeibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\nShaokun Zhang, Jiale Liu, Ahmed Hassan Awadal-\nlah, Ryen W White, Doug Burger, and Chi Wang.\n2024. Autogen: Enabling next-gen LLM applica-\ntions via multi-agent conversation.\nYuhuai Wu, Markus Rabe, DeLesley Hutchins, and\nChristian Szegedy. 2022. Memorizing transformer.\nIn International Conference on Learning Represen-\ntations, ICLR ’22.\nZhishang Xiang, Chuanjie Wu, Qinggang Zhang,\nShengyuan Chen, Zijin Hong, Xiao Huang, and Jin-\nsong Su. 2025. When to use graphs in rag: A com-\nprehensive analysis for graph retrieval-augmented\ngeneration. Preprint, arXiv:2506.05690.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. Preprint, arXiv:2210.03629.\n"}, {"page": 11, "text": "A\nappendix\nA.1\nTheoretical Properties\nA.1.1\nEncoding and Heavy-Tailed Repetition\nLemma 1 (Concentration of repetition). Let V be\nthe corpus vocabulary and f : V →N token fre-\nquencies. In typical language corpora f is heavy-\ntailed: there is a core set Vcore ⊂V such that\nX\nv∈Vcore\nf(v) ≈Θ(|C|),\nwhere |C| is the total token count. Thus, indexing\nrecurrent entities/relations captures most informa-\ntional mass while reducing redundancy.\nSketch. Empirical token distributions in large cor-\npora follow Zipf-like laws; a small set of types\naccounts for most occurrences. Mapping these to\nIDs and sharing them across queries/facts removes\nrepeated surface forms without losing the domi-\nnant mass.\nA.1.2\nChunked Small Graphs (Local-First\nConstruction)\nLemma 2 (Maximal local-coherence partition).\nFix a threshold τ, a bounded continuity bonus\n0 ≤b < ∞, and a fit rule that is monotone\nnonincreasing as the centroid drifts away from a\ntriple. The one-pass rule “append if fit-score≥τ,\nelse cut” yields a partition G = (G1, . . . , GK) in\nwhich each Gk is maximal: no additional triple\ncan be appended without violating the fit test.\nSketch. Within a segment, appending triples can\nonly decrease (or leave unchanged) the fit of future\ntriples because the centroid moves away from any\nfixed candidate. Once a triple fails the test, any\nlarger graph containing it would also fail. Thus,\neach cut point defines a maximal prefix w.r.t. the\nlocal rule.\nLemma 3 (Boundary-consistency merge). Let Θ\ndenote the segmenter parameters (threshold, con-\ntinuity bonus, etc.). For adjacent runs (L, R), de-\nfine MERGE(L, R) as true iff the segmenter ap-\nplied to the concatenation L ∥R either (i) pro-\nduces a single chunk or (ii) places its first cut away\nfrom the original boundary at |L|. Then a bound-\nary is removed iff the segmenter, when given both\nsides at once, would not cut at that location. Sur-\nviving boundaries are fixed points of the segmenter\nunder local re-evaluation.\nSketch. The merge test re-runs exactly the same\nalgorithm and hyperparameters on the local win-\ndow. If the “true” segmentation prefers a differ-\nent cut, we merge; otherwise we keep the bound-\nary. This is equivalent to requiring boundary self-\nconsistency under the same rule.\nLemma 4 (Intra-chunk cohesion bound). Assume\nunit-normalized triple embeddings (vi) and an ac-\nceptance rule cos(¯c, vi) + δi ≥τ with 0 ≤δi ≤b,\nwhere ¯c is the running centroid. For any completed\nsmall graph Gk with |Gk| ≥2,\n2\n|Gk|(|Gk|−1)\nX\np<q\ncos(vp, vq) ≥τ −b.\nSketch. The centroid always lies in the convex hull\nof the triple embeddings. The acceptance condi-\ntion ensures each new triple is not too far (in co-\nsine) from the current centroid, up to the continu-\nity bonus b. Averaging pairwise cosines and using\ntriangle-type inequalities yields the bound.\nLemma 5 (Working-set reduction for retrieval).\nLet M = |M| be the number of distinct edges,\nand suppose runs have lengths ℓ1, . . . , ℓK with\nL = maxk ℓk. If symbolic pre-filtering selects H\ncandidate runs for re-ranking, then the fine stage\ntouches at most H · L edges. Compared to scan-\nning all edges, this yields an asymptotic reduction\nfactor Ω(M/(H · L)).\nSketch. Each candidate run contributes at most L\nedges to be scored. Bounding H and L indepen-\ndently of M (corpus size) gives sublinear depen-\ndence on M.\nCorollary 6 (Precision–recall / segmentation\ntradeoff). Increasing τ (or decreasing the continu-\nity bonus b) shortens runs, improves cohesion, and\nreduces retrieval latency at the cost of recall. De-\ncreasing τ lengthens runs, improves recall, but in-\ncreases candidate sizes. The boundary merge step\ncounteracts over-segmentation by removing unsta-\nble cuts.\nSketch. Higher thresholds cause earlier cuts;\nlower thresholds allow more heterogeneous con-\ntent in each run.\nThe merge rule prunes cuts\nthat the full-window segmenter would not repro-\nduce.\n"}, {"page": 12, "text": "A.1.3\nCoarse retrieve\nLemma 7 (Efficiency of Coarse→Fine with Max–\nPair Filtering). Let n be corpus size, d the em-\nbedding dimension, and k\n≪\nn the coarse\nshortlist size.\nThe coarse stage computes, per\nquery–candidate pair, constants over small en-\ntity/relation sets; the fine stage evaluates only k\nitems with sentence embeddings. Thus cost drops\nfrom O(n·d) to O(k·d) while preserving precision\nprovided k retains high-overlap candidates.\nA.1.4\nSemantic Selection and Consensus\nLemma 8 (Semantic consensus is close to its\nmembers). Let runs r1, . . . , rm have embed-\ndings ψ(ri) and cosine similarity sim(ra, rb) =\ncos(ψ(ra), ψ(rb)).\nLet ¯r maximize the average\nsimilarity\n¯r ∈arg max\nr\nm\nX\ni=1\nsim(r, ri).\nIf all runs in the cluster are mutually similar, i.e.,\nsim(ra, rb) ≥θ for some θ close to 1, then ¯r is\nalso close to every member:\n1 −sim(ra, ¯r) ≤ε(θ)\nfor all a,\nfor a function ε(θ) →0 as θ →1.\nSketch. On the unit sphere, cosine similarity de-\nfines a bounded distance. If ¯r were far from some\nmember ra while all runs are mutually close, mov-\ning ¯r toward ra would increase its average simi-\nlarity to the cluster, contradicting maximality. The\nbound ε(θ) follows from standard geometric argu-\nments.\nA.1.5\nConsolidation of Entities and Edge Se-\nquences\nLemma 9 (Quotient consolidation reduces edge\ncardinality). Let G = (V, R, E) be a directed\nmultigraph with labeled edges E ⊆V × R × V .\nLet ∼be the equivalence relation on V induced\nby entity consolidation and π : V →V/∼the\nprojection. Define ϕ : E →E′ by ϕ(u, r, v) =\n(π(u), r, π(v)) and let E′ = uniq(ϕ(E)). Then:\n1. |E′| ≤|E|, with equality iff ϕ is injective on\nE.\n2. For any edge sequence σ = (ei1, . . . , eiT ),\nthe remapped-and-deduped sequence σ′ =\nuniq(ϕ(σ)) satisfies |σ′| ≤|σ|.\nSketch. E′ is the image of E under ϕ followed by\ndeduplication, so it cannot have more elements.\nSequences inherit this property pointwise; collaps-\ning equal edges cannot increase length.\nLemma 10 (Vectorization cost is preserved).\nLet vs be a schema vectorizer depending only\non symbolic indices and precomputed vectors\n(Eemb, Remb).\nThen the number of sentence-\nlevel text encodings required by the pipeline is un-\nchanged by applying ϕ and deduplicating edges.\nSketch. Consolidation changes only which indices\npoint to which vectors. All sentence encodings are\ndone before consolidation (for triples and chunks),\nso later index manipulation reuses them.\nLemma 11 (Medoid representatives minimize\nwithin-cluster distortion). Let C\n⊆\nV\nbe a\ncluster of entities and define cosine dissimilarity\nd(x, y) = 1 −cos(x, y). A medoid r⋆∈C satis-\nfies\nr⋆∈arg min\nr∈C\nX\ni∈C\nd(ei, er),\nand for any other representative r ∈C,\nX\ni∈C\nd(ei, er⋆) ≤\nX\ni∈C\nd(ei, er).\nSketch. This is the standard property of medoids:\nby definition they minimize total dissimilarity\nwithin the cluster.\nA.1.6\nDPO Wrapper and Policy Behavior\nLemma 12 (DPO aligns policy log-odds with util-\nities). Assume preferences follow a Bradley–Terry\nmodel:\nPr(y+ ≻y−| x) = σ\n\u0000λ [U(x, y+) −U(x, y−)]\n\u0001\nfor some λ > 0, where U is a latent utility. Let\nπref be fixed. Then any minimizer πθ of the DPO\nobjective satisfies, up to a normalization constant\nCx,\nlog πθ(y | x) −log πθ(y′ | x)\n=\nλ\nβdpo\n\u0002\nU(x, y) −U(x, y′)\n\u0003\n+ ∆ref(y, y′ | x) + Cx.\nwhere ∆ref depends only on πref.\nSketch. DPO maximizes the conditional log-\nlikelihood of observed pairwise preferences under\n"}, {"page": 13, "text": "a logistic link, with a reference correction. At op-\ntimum, gradient stationarity enforces proportion-\nality between policy log-odds and utility differ-\nences, offset by the fixed reference.\nLemma 13 (Monotone token control via ac-\ntion lattice). Suppose each channel’s action set\n{include all, unique, not include} forms a\nlattice under ⪰with\ninclude all ⪰unique ⪰not include,\nsuch that Tokens(x, y) is monotone nonincreasing\ndown the lattice and Acc(x, y) is Lipschitz in a\ntask metric. Then there exists a Lagrange mul-\ntiplier η⋆≥0 such that the DPO-trained policy\nwith penalty η⋆· Tokens attains a target budget B,\nand tighter budgets B′ < B can be achieved by\nincreasing η (eventually collapsing to always not\ninclude).\nSketch. On a finite action set, the mixed policy\nover actions yields a convex set of achievable (to-\nkens, accuracy) pairs. A standard Lagrangian ar-\ngument with a monotonically ordered action set\ngives existence of a multiplier realizing each feasi-\nble budget, and increasing the penalty pushes mass\ntoward cheaper actions.\nA.2\nEffect of Token Length and Benefits of\nEntity–Relation Factorization\nSequence embeddings.\nFor simplicity, we ap-\nproximate the embedding of a text span S of\nlength n tokens by the mean of its token embed-\ndings:\nz(S) ≈1\nn\nn\nX\ni=1\nxi ∈Rd.\nEach token embedding decomposes into\nxi = si + εi,\nwhere si is the semantic signal and εi is zero-mean\nnoise with E[εi] = 0 and Var(⟨q, εi⟩) = σ2 for\nany unit query vector q.\nWe partition the tokens into: (i) relevant tokens\nR (carry information the query cares about) and\n(ii) irrelevant tokens I (background, boilerplate,\nnarration), with |R| = m and |I| = n −m.\nLemma 14 (Token dilution). Let q be a unit query\nvector aligned with the average relevant signal\nµrel := 1\nm\nX\ni∈R\nsi\nwith\n⟨q, µrel⟩= α > 0.\nAssume irrelevant tokens have no systematic\nalignment with q, i.e., E[⟨q, si⟩] = 0 for i ∈I.\nThen the signal-to-noise ratio (SNR) of the pas-\nsage embedding in the direction q decays as\nSNR(S) :=\n\u0000E[⟨q, z(S)⟩]\n\u00012\nVar(⟨q, z(S)⟩) ∝1\nn.\nProof. We have\nz(S) = 1\nn\nn\nX\ni=1\nxi = 1\nn\nX\ni∈R\n(si+εi)+ 1\nn\nX\ni∈I\n(si+εi).\nTaking inner product with q and expectation, and\nusing E[⟨q, εi⟩] = 0 for all i and E[⟨q, si⟩] = 0 for\ni ∈I,\nE[⟨q, z(S)⟩] = 1\nn\nX\ni∈R\n⟨q, si⟩= m\nn α.\nThus, for fixed m and α, the expected signal scales\nas mα/n.\nFor the variance, by independence and identical\nvariance of the noise:\nVar(⟨q, z(S)⟩) = Var\n\u0010 1\nn\nn\nX\ni=1\n⟨q, εi⟩\n\u0011\n= 1\nn2\nn\nX\ni=1\nVar(⟨q, εi⟩) = σ2\nn .\nTherefore\nSNR(S) = (mα/n)2\nσ2/n\n= m2α2\nσ2\n· 1\nn,\nwhich shows SNR(S) ∝1/n as claimed.\nCorollary 15 (Length bias). Consider two pas-\nsages Sshort and Slong that contain the same m rel-\nevant tokens (same fact, same α) but have lengths\nns and nℓwith nℓ> ns. Then\nE[⟨q, z(Sshort)⟩] = m\nns α >\nm\nnℓα = E[⟨q, z(Slong)⟩]\nThus, even for equally relevant content, longer\npassages tend to produce lower expected similar-\nity to q and are systematically disadvantaged in\nretrieval.\nImplication.\nLemma 14 and Corollary 15 for-\nmalize a “token dilution” effect: when we em-\nbed entire passages, the representation of a fact is\nweakened by irrelevant tokens, and the SNR de-\ncreases as 1/n with passage length. Consequently,\n"}, {"page": 14, "text": "retrieval quality depends not only on what is said,\nbut also on how long and where it is written.\nEntity–relation factorization.\nIn our system,\nwe instead represent knowledge as graph edges\n(e, r, e′) ∈M ⊆E × R × E and store embed-\ndings for entities and relations via the codebook\nEemb : E →Rd,\nRemb : R →Rd.\nThus each fact f = (e, r, e′) is encoded by the\ntriple\nEemb(e), Remb(r), Eemb(e′),\nderived from short token sequences for entity\nnames and relation labels, rather than from whole\npassages.\nProposition 16 (Advantages of E–R–E embed-\ndings). Let E, R, M, Eemb, Remb be as in Sec-\ntion 2.2.\nUnder the averaging+noise model of\nLemma 14, the following hold:\n1. (High-SNR micro-embeddings) There exist\nconstants c1, c2 > 0, independent of the pas-\nsage length n, such that for any fact f =\n(e, r, e′) ∈M,\nc1 ≤SNR(v) ≤c2\n∀v ∈{Eemb(e), Remb(r), Eemb(e′)}.\nIn particular, E–R–E embeddings do not suf-\nfer the 1/n decay of Lemma 14.\n2. (Compositional query scoring) Let a query q\ninduce components qE, qR ∈Rd. For suit-\nable nonnegative weights λs, λr, λt we can\nscore an edge (e, r, e′) ∈M via\nscore(e, r, e′ | q) = λs ⟨qE, Eemb(e)⟩+\nλr ⟨qR, Remb(r)⟩+ λt ⟨qE, Eemb(e′)⟩,\ni.e., as an inner product between a structured\nquery and short, high-SNR E–R–E embed-\ndings, instead of a single inner product with\na noisy passage embedding z(S).\n3. (Localized interference and updates) Each\nfact f has its own edge (e, r, e′); inter-\nference between facts arises only through\nshared Eemb(·) or Remb(·). Updating a sin-\ngle fact changes O(1) vectors instead of re-\nembedding entire passages containing many\nunrelated facts.\nProof sketch. (1) Let the surface string for e ∈E\nhave nE tokens, of which mE are semantically rel-\nevant. By construction nE is bounded by a small\nconstant (e.g., 1–3), so mE ≈nE = O(1). Ap-\nplying the same calculation as in Lemma 14 with\nn = nE gives\nSNR\n\u0000Eemb(e)\n\u0001\n∝m2\nE\nnE\n= Θ(1),\nwith constants independent of the passage length\nn in which f appears. The same argument applies\nto Remb(r) and Eemb(e′), yielding the claimed\nbounds c1, c2.\n(2) Because we store Eemb(e), Remb(r), and\nEemb(e′) separately, any query q that decomposes\ninto components (qE, qR) admits the factorized\nscore above.\nAlgebraically, this is a weighted\nsum of inner products between short E–R–E vec-\ntors and corresponding query components, rather\nthan a single inner product ⟨q, z(S)⟩with a length-\ndependent passage embedding.\n(3) The representation of a fact f is the triple\n(Eemb(e), Remb(r), Eemb(e′)).\nAdding, remov-\ning, or modifying f only affects these embed-\ndings and other edges sharing e, r, or e′.\nNo\nre-embedding of unrelated tokens is required, in\ncontrast to passage-level embeddings that entan-\ngle many facts in the same z(S).\nA.3\nRetrieval Details\nFor completeness we summarize the exact scoring\nterms used in the coarse and fine stages.\nCoarse score.\nAn indexed run y decodes to\ntriples S(y) = {(h, ρ, t)} ⊆M. We collect en-\ntity and relation embeddings into matrices E(y) ∈\nRne×d and R(y) ∈Rnr×d. For a query q and can-\ndidate run f,\nscoarse(q, f) = went max\ni,j cos\n\u0000E(q)i, E(f)j\n\u0001\n+ wrel max\np,r cos\n\u0000R(q)p, R(f)r\n\u0001\n,\nand we take Ik = TopKf scoarse(q, f).\nFine score from triple lines.\nFor each candidate\nf ∈Ik, we linearize triples to short lines “h ρ t”,\nembed query and candidate lines into Q ∈Rnq×d\nand C ∈Rnc×d, and form the cosine matrix\nS = bQbC⊤∈[−1, 1]nq×nc.\nAll fine-stage terms are computed on S:\n"}, {"page": 15, "text": "• RelTopT: flatten S, take the top-t entries, and\naverage.\n• Coverage: Cov(τcov) = P\ni 1[maxj Sij ≥\nτcov].\n• Many-to-many\n(MP):\napply\nσ\n\u0000(Sij −\nτpair)/Tpair\n\u0001\nelementwise and normalize by\n√nqnc or log(1 + nqnc).\n• Distinct 1:1: greedily select the largest un-\nused entries above τdist and average them\nwith a 1a)Raw −textprompt/√m factor.\n• Whole-chunk gate: compute a full-chunk\ncosine between concatenated query and can-\ndidate text, normalize by a length term, and\ngate with a sigmoid so very long but off-topic\nchunks do not get extra credit.\nThe final semantic score is a weighted sum\nsfine = RelTopT + λcovCov + λmpMP\n+ λ1:1Distinct + λwholeWholeGate.\nwith one set of weights and thresholds per\ndataset/model, reused across all experiments.\nA.4\nPrompt Format and Input Encoding\nWe encode each input as a compact, graph-\nstructured prompt rather than a long text block.\nConcretely, a prompt consists of:\n• A codebook of entities and relations, E′ and\nR′ (either as words or short IDs).\n• Edge sequences for the query, prior knowl-\nedge, and facts: query edges q, knowledge\nedges k, and fact edges f.\n• A short instruction block describing how to\ninterpret each tuple (h, ρ, t) or ID triple.\nFigure 8 contrasts a conventional raw-text\nprompt with our ID-based and word-based encod-\nings. The ID variants use a JSON-style schema:\nOne of three formats depends on which one has\nfewer tokens.\n(a) Raw-text prompt\nQ: Which subsidiaries acquired since 2021\nare exposed to new EU rules?\nContext:\n- In 2022, AlphaCorp acquired BetaLtd...\n- EU Regulation 2024/12 applies to...\n- Post-merger reports indicate ...\n(plus additional retrieved passages ...)\n(b) ID-referenced codebook with edge matrix\n{\n\"e\": [\"AlphaCorp\",\"BetaLtd\",\n\"EUReg2024_12\",\"2021+\"],\n\"r\": [\"acquired_in\",\"exposed_to\",\n\"subject_to\"],\n\"edge_matrix\": [[0,0,3],\n[1,1,2],\n[0,2,2]],\n\"questions(edges[i])\":[0,1],\n\"facts(edges[i])\": [0,2]\n\"rules\":\"<KB schema string>\"\n}\n(c) ID-referenced compact triples\n{\n\"e\": [\"AlphaCorp\",\"BetaLtd\",\n\"EUReg2024_12\",\"2021+\"],\n\"r\": [\"acquired_in\",\"exposed_to\",\n\"subject_to\"],\n\"questions([[e,r,e], ...]):\": [[0,0,3], [1,1,2]],\n\"facts([[e,r,e], ...]):\": [[0,0,3], [0,2,2]],\n\"rules\":\"<KB schema string>\"\n}\n(d) Word-level triples (no IDs)\n{\n\"questions(words)\": [[AlphaCorp,acquired_in,2021+],\n[BetaLtd,exposed_to,EUReg2024_12]],\n\"facts(words)\": [[AlphaCorp,acquired_in,2021+],\n[AlphaCorp,subject_to,EUReg2024_12]],\n\"rules\":\"<KB schema string>\"\n}\nFigure 8:\nAutoPrunedRetriever input encodings.\nPanel (a) shows a conventional long-context prompt;\n(b) encodes the same information via an entity/relation\ncodebook and an edge matrix; (c) uses explicit triple\nlists with IDs; (d) uses full-word triples.\n(a) Edge-matrix JSON schema\n---Knowledge Base---\n[JSON format]\n- e: list of entities (e[i] = entity string)\n- r: list of relations (r[j] = relation string)\n- edge_matrix: [[head_e_idx, r_idx, tail_e_idx]]\n* NOTE: edges[i] is just shorthand for edge_matrix[i]\n- questions(edges[i]): questions linked by edge i\n- given knowledge(edges[i]): prior answers linked by edge i\n- facts(edges[i]): facts linked by edge i\n(b) ID-based triple JSON schema\n---Knowledge Base---\n[JSON format]\n- e: list of entities (e[i] = entity string)\n- r: list of relations (r[j] = relation string)\n- [e,r,e]: triple [head_e_idx, r_idx, tail_e_idx]\n- questions([[e,r,e], ...]): question triples\n- given knowledge([[e,r,e], ...]): prior answer triples\n- facts([[e,r,e], ...]): fact triples\n(c) Word-level triple schema\n---Knowledge Base---\n[JSON format]\n- questions(words): question triples\n- given knowledge(words): prior answer triples\n- facts(words): fact triples\nFigure 9:\nKnowledge-base JSON specifications\n(“rules”) used by AutoPrunedRetriever. The con-\ncrete encodings in Fig. 8 all instantiate one of these\nschemas.\n• e: entity vocabulary (either strings or IDs).\n"}, {"page": 16, "text": "• r: relation vocabulary.\n• edge matrix or triple lists: [h, r, t] indices\ninto e/r or E/R.\n• questions, knowledge, facts: subsets of\nedges tagged as questions, prior knowledge,\nor background facts.\n"}, {"page": 17, "text": "A.5\nCross-domain qualitative comparison on STEM and TV\nDomain /\nID\nQuestion\n(abridged)\nAutoPrunedRetriever\nResult\nAnalysis (abridged)\nHippoRAG2\nResult\nAnalysis\n(abridged)\nLightRAG\nResult\nAnalysis\n(abridged)\nSTEM-\n5c755e96\nBrown\nbear\nsize\nvariation and adapt-\nability\nContext:\n“Kodiak\nbears\nare\nlargest due to high salmon avail-\nability; inland bears smaller with\nlimited resources.”\nReasoning: geography →food\nabundance →body-mass shift →\nadaptability.\nAnswer:\nLarger coastal/Kodiak\nbears\nreflect\nrich\ncaloric\nin-\ntake;\nsmaller inland bears re-\nflect scarcity ⇒size variance ev-\nidences environmental adaptabil-\nity.\nError: — (correct)\nContext:\n“The\ntaxonomy\nof\nbrown bears remains bewilder-\ning; multiple subspecies identi-\nfied.”\nReasoning: taxonomy →mor-\nphological variation (no environ-\nmental cause).\nAnswer:\nSize variability indi-\ncates subspecies diversity.\nError:\nMisses\ncausal\ndriver\n(resources) ⇒descriptive, not\nmechanistic.\nContext:\n“Brown bears vary\ngreatly in size depending on\nwhere they live.”\nReasoning:\nregion →size →\nadaptation (shallow).\nAnswer:\nBears adapt to local\nconditions, so sizes differ.\nError: Correlation only; lacks re-\nsource/metabolic link.\nSTEM-\n4e26ae6d\nHistorical range →\necological role\nContext:\n“Mexican grizzly /\nKodiak / Himalayan subspecies;\napex predators affecting vegeta-\ntion and prey.”\nReasoning: historical range →\ndiversification →habitat adapta-\ntion →modern trophic role.\nAnswer: Past range shaped re-\ngional lineages whose adapta-\ntions underwrite today’s grizzly\napex role.\nError: — (correct)\nContext:\n“Pleistocene lineage\nprior to demise.”\nReasoning: lineage timeline →\nextinction (no present ecology).\nAnswer:\nHistorical divergence\nexplains current bears (vague).\nError: Lacks link to present-day\necological function.\nContext:\n“Ecological dynamics\nof predator–prey systems.”\nReasoning: ecosystem complex-\nity →generic role.\nAnswer: Grizzlies play roles in\necosystems (generic).\nError: No entity-level or causal\npath from range to role.\nSTEM-\n1b8f5662\nPhysical\nadapta-\ntions\n→\nhunting\nsuccess (moose)\nContext:\n“Charge and scent-\nbased ambush tactics; terrain af-\nfects prey choice.”\nReasoning: morphology + habi-\ntat →tactic →success vs large\nprey.\nAnswer:\nBears’ strength/claws\n+ terrain-leveraged tactics raise\nsuccess on moose.\nError: — (correct)\nContext: “Brown bears as apex\nomnivores in ecosystems.”\nReasoning: apex predator →sur-\nvival (no tactics).\nAnswer: As apex predators they\ncan hunt large prey.\nError: Omits behavioral mecha-\nnism/tactical link.\nContext:\n“Bears interact with\ndiverse ecosystems.”\nReasoning:\nenvironment\n→\nadaptation (broad).\nAnswer: Environmental adapta-\ntion enables hunting.\nError: High-level summary; no\ntactic/terrain edge.\nTV-\n29d2f5b1\nCaboose\nand\nOmega possession\n(Red vs Blue)\nContext:\n“Caboose’s abnormal\nbehavior linked to Omega pos-\nsession and oxygen deprivation\nafter suit reboot.”\nReasoning: possession + hypoxia\n→erratic acts →friendly-fire.\nAnswer:\nCaboose;\naccidents\nstem from AI control + hypoxia.\nError: — (correct)\nContext: “Carolina’s body taken\nby Omega; personality changes.”\nReasoning:\nAI possession →\nbehavior\nchange\n(host\nmisat-\ntributed).\nAnswer: Carolina behaves abnor-\nmally due to Omega.\nError:\nEntity confusion; tem-\nporal mismatch; misses hypoxia\nfactor.\nContext: “Omega AI causes ag-\ngression.”\nReasoning: AI influence →ab-\nnormal behavior (partial).\nAnswer: Omega explains erratic\nacts.\nError: Omits oxygen-deprivation\ncomponent; partial causality.\nTV-\n33a6bd74\nSarge’s Season 15\ndepression and re-\ndemption\nContext: “Sarge creates fake en-\nemies, betrays Reds/Blues, later\nsaves them.”\nReasoning:\ndepression →be-\ntrayal →redemption (temporal).\nAnswer: Depression triggers be-\ntrayal; later redemption by saving\nteam.\nError: — (correct)\nContext:\n“Sarge\nexperiences\nburnout.”\nReasoning:\ndepression →low\nmorale (truncated).\nAnswer: Sarge acts poorly due to\nburnout.\nError:\nMisses\nbe-\ntrayal–redemption arc.\nContext: “Acts irrationally after\nlong missions.”\nReasoning: fatigue →misbehav-\nior (truncated).\nAnswer: Irrational actions due to\nfatigue.\nError:\nOmits betrayal + later\nchange-of-heart.\nTV-\n1e87f0a2\nThe\nSimpsons\n–\nYes Guy / Wiseguy\nmeta humor\nContext:\n“Yes Guy’s ‘Ye-e-e-\ns?!’ explained by ‘I had a stro-o-\no-oke’; Wiseguy labeled stereo-\ntype.”\nReasoning: stroke gag →speech\nquirk →meta-reference.\nAnswer: The gag is justified in-\nuniverse; Wiseguy isn’t a fixed\nname.\nError: — (correct)\nContext: “Running joke across\nepisodes.”\nReasoning: repetition →humor\n(no causal quote).\nAnswer: It’s a recurring joke.\nError:\nLacks textual evidence\nexplaining the quirk.\nContext: “Minor characters re-\ncurring jokes.”\nReasoning:\ntrope repetition →\nhumor (generic).\nAnswer: Recurring jokes create\nhumor.\nError: Descriptive only; misses\nexplicit line/second part.\nTable 2: Cross-domain qualitative comparison on STEM and TV questions. Each model column includes its\nretrieved context, reconstructed reasoning chain, final answer, and an error note (if any). AutoPrunedRetriever pre-\nserves minimal but functional causal paths; HippoRAG2 tends to over-expand associatively; LightRAG collapses\nto topic-level summaries.\n"}]}