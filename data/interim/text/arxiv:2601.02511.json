{"doc_id": "arxiv:2601.02511", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.02511.pdf", "meta": {"doc_id": "arxiv:2601.02511", "source": "arxiv", "arxiv_id": "2601.02511", "title": "LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection", "authors": ["Bahareh Golchin", "Banafsheh Rekabdar", "Danielle Justo"], "published": "2026-01-05T19:33:30Z", "updated": "2026-01-05T19:33:30Z", "summary": "Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.02511v1", "url_pdf": "https://arxiv.org/pdf/2601.02511.pdf", "meta_path": "data/raw/arxiv/meta/2601.02511.json", "sha256": "cef00e67d7939e83f1f12bbeb8ebee3724eab3d57c47b26951ab480b371df5a6", "status": "ok", "fetched_at": "2026-02-18T02:23:11.354868+00:00"}, "pages": [{"page": 1, "text": "LLM-Enhanced Reinforcement Learning for\nTime Series Anomaly Detection\n1st Bahareh Golchin\ndept. Computer Science\nPortland State University\nPortland, OR, USA\nbgolchin@pdx.edu\n2nd Banafsheh Rekabdar\ndept. Computer Science\nPortland State University\nPortland, OR, USA\nrekabdar@pdx.edu\n3rd Danielle Justo\ndept. Computer Science\nSmith College\nNorthampton, MA, USA\ndjusto@smith.edu\nAbstract—Detecting anomalies in time series data is crucial for\nfinance, healthcare, sensor networks, and industrial monitoring\napplications. However, time series anomaly detection often suffers\nfrom sparse labels, complex temporal patterns, and costly expert\nannotation. We propose a unified framework that integrates\nLarge Language Model (LLM)–based potential functions for\nreward shaping with reinforcement learning (RL), Variational\nAutoencoder (VAE)–enhanced dynamic reward scaling, and active\nlearning with label propagation. An LSTM-based RL agent\nleverages LLM-derived semantic rewards to guide exploration,\nwhile VAE reconstruction errors add unsupervised anomaly\nsignals. Active learning selects the most uncertain samples, and\nlabel propagation expands labeled data efficiently. Evaluations on\nYahoo-A1 and SMD benchmarks demonstrate that our method\nachieves state-of-the-art detection accuracy under limited labeling\nbudgets and operates effectively in data-constrained settings. This\nstudy highlights the promise of combining LLMs with RL and\nadvanced unsupervised techniques for robust, scalable anomaly\ndetection in real-world applications.1\nI. INTRODUCTION\nTime series anomaly detection represents a critical challenge\nin modern data analysis, spanning applications from healthcare\nmonitoring to industrial surveillance [1]. Traditional approaches\nface significant limitations when dealing with sparse labeled\ndata, complex temporal dependencies, and real-time decision-\nmaking requirements [2]. While deep learning methods show\npromise, they struggle with sample efficiency and adapting\nto novel anomaly patterns without extensive retraining. The\nconvergence of reinforcement learning (RL) and anomaly de-\ntection represents a paradigm shift, treating anomaly detection\nas an interactive learning process where agents learn optimal\ndetection policies through environmental feedback [3].\nExisting RL-based anomaly detection systems face critical\nchallenges, including sparse reward signals, limited exploration\nefficiency, and difficulty incorporating domain knowledge [4],\n[5]. The scarcity and high cost of labeled anomaly data\ncompound these limitations, as traditional RL methods require\nsubstantial labeled data to achieve satisfactory performance.\nThese challenges require innovative approaches to operate\neffectively under data-constrained conditions while maintaining\nhigh detection accuracy [6].\n1Code is available at GitHub Repository.\nThe emergence of Large Language Models (LLMs) has\nrevolutionized complex reasoning tasks across diverse domains,\noffering new possibilities for enhancing RL-based anomaly\ndetection systems. Recent research demonstrates that LLMs\npossess remarkable capabilities in understanding temporal\npatterns and can effectively serve as semantic anomaly detectors\nby leveraging their pre-trained knowledge to assess data patterns\n[7].\nPotential-based reward shaping (PBRS) provides a theoret-\nically sound framework for incorporating additional reward\nsignals without altering optimal policies [8]. This can prove\nto be particularly valuable in sparse reward environments\nwhere traditional RL algorithms struggle [9]. Active learning\nmaximizes labeled sample informativeness through strategic\nselection, while techniques like label propagation expand\nlabeled datasets. Variational Autoencoders (VAE) complement\nthese approaches by providing unsupervised anomaly scoring\nthrough reconstruction error analysis [10], [11].\nThis work presents a novel integrated framework combining:\nan LSTM-based RL agent, VAE-enhanced reward mecha-\nnisms, LLM-based potential functions for semantic reward\nshaping, and active learning with label propagation. Our\ncontributions include: (1) LLM-generated semantic potential\nfunctions incorporating domain knowledge without manual\nengineering, (2) dynamic reward scaling balancing supervised\nand unsupervised signals, and (3) comprehensive evaluation\ndemonstrating superior performance on both univariate and\nmultivariate datasets.\nThis comprehensive approach simultaneously addresses\nthe sparse reward problem through LLM-based potential\nfunctions, sample efficiency through active learning, temporal\ndependencies through LSTM representation, and exploration-\nexploitation trade-offs through dynamic reward scaling. The\nresulting framework offers a robust solution for real-world\nanomaly detection where labeled data is scarce and timely, and\naccurate decision-making is critical.\nII. BACKGROUND\nIn what follows, we review the background related to our\nstudy.\narXiv:2601.02511v1  [cs.LG]  5 Jan 2026\n"}, {"page": 2, "text": "A. Extrinsic and Intrinsic Rewards\nIn RL, rewards are categorized into extrinsic signals defined\nby the task designer and intrinsic signals generated internally to\nfoster exploration [12], [13]. Extrinsic rewards drive the agent\ntoward its primary objective, such as receiving positive feedback\nfor correctly identifying anomalies or penalties for false alarms\n[8]. Although these signals provide specific feedback, they are\noften sparse or delayed, which can hinder effective learning.\nConversely, intrinsic rewards motivate the agent to investigate\nunfamiliar states based on factors like novelty or prediction\nerror, independent of external feedback [14]. For anomaly\ndetection tasks, these intrinsic signals are crucial for guiding\nthe agent toward unusual patterns that may not yet be labeled.\nBy integrating both reward types, the agent effectively balances\nexploiting known strategies with exploring new situations,\nsignificantly enhancing both learning efficiency and adaptability\n[15].\nB. Potential-Based Reward Shaping\nReward shaping accelerates learning by modifying the reward\nsignal while strictly preserving the optimal policy. A prominent\napproach, Potential-Based Reward Shaping (PBRS), augments\nthe standard reward using a potential function Φ(s) defined as:\nr′(s, a, s′) = r(s, a, s′) + γΦ(s′) −Φ(s)\n(1)\n[8]. This formulation guarantees policy invariance, ensuring\nthe agent’s optimal behavior remains consistent with the\noriginal objective [11]. By incorporating domain heuristics\nor data-driven insights into Φ(s), PBRS guides the agent\ntoward desirable states, such as identifying severe anomalies,\nmore efficiently [16]. Consequently, the method improves\nconvergence rates in complex environments without biasing\nthe final learned policy [17].\nC. Variational Autoencoder for Anomaly Detection\nVAEs map input features to latent Gaussian distributions\nusing a neural encoder-decoder architecture [18]. The model\nmaximizes the Evidence Lower Bound (ELBO) as a tractable\nproxy for the intractable marginal likelihood:\nL(θ, ϕ; x) = Eqϕ(z|x)\n\u0002\nlog pθ(x | z)\n\u0003\n−KL(qϕ(z | x) ∥p(z)) ,\n(2)\nwhere the objective balances reconstruction fidelity with the\nKullback–Leibler divergence between the approximate posterior\nqϕ(z | x) and the prior p(z).\nIII. PROPOSED METHOD\nOur proposed framework illustrates in Figure 1. The system\nconsists of four main components: (1) an LSTM-based RL\nagent for sequential anomaly detection, (2) a VAE to estimate\nanomaly magnitude via reconstruction error with dynamic\nreward scaling, (3) an LLM-based semantic potential function\nfor reward shaping, and (4) an active learning and label\npropagation module to minimize labeling cost.\nA. LSTM-based RL agent\nEnvironment and States. Our framework models anomaly\ndetection as a sequential decision-making process within an\nRL environment. The environment processes time series data\nsequentially, where at each time step t, the agent observes\na state st, selects a binary action at ∈{0, 1} (normal\nor anomalous), and receives a reward signal. Each episode\ncorresponds to processing a complete time series, with the\nagent’s objective being to classify each time step while\nminimizing false detections accurately.\nThe state at time t consists of a sliding window of\nnsteps consecutive observations from the time series, st =\n{xt−nsteps+1, xt−nsteps+2, . . . , xt}. This enables the agent to\ndistinguish between different action contexts; each state is\naugmented with an action indicator flag. This creates two\npossible state representations at each time step: s0\nt = [st; 0] for\npredicting normal, and s1\nt = [st; 1] for predicting anomalous.\nwhere each augmented state sa\nt ∈Rnsteps×(d+1) contains both\nthe raw sensor values and the action flag.\nThis dual-component design preserves essential temporal\ndependencies while providing the Q-network with action-\nspecific context for learning distinct value functions. The\nsliding window length is set to nsteps\n= 25, providing\nsufficient historical context for temporal pattern recognition\nwhile maintaining computational efficiency.\nExtrinsic Reward. The extrinsic reward mechanism provides\ndirect feedback on the agent’s classification decisions, guiding\nlearning through immediate reinforcement based on prediction\naccuracy. At each time step, the agent selects a binary action\nat ∈{0, 1} representing predict normal or predict anomaly,\nrespectively, and receives reward feedback based on the ground\ntruth label yt.\nClassification-Based Reward Structure. The primary ex-\ntrinsic reward R1(st, at) follows a confusion matrix-based\nformulation, giving +5 for true positives, +1 for true neg-\natives, −1 for false positives, and −5 for false negatives\n(R1(st, at) ∈{5, 1, −1, −5}).\nB. VAE-Enhanced Reward Augmentation\nTo complement the classification feedback, we incorporate\nreconstruction error from a VAE trained exclusively on normal\npatterns. The VAE provides an unsupervised anomaly score\nthat captures deviations from learned normal behavior:\nR2(st) = MSE(xt, ˆxt) = 1\nn\nn\nX\ni=1\n(xt,i −ˆxt,i)2,\n(3)\nwhere xt represents the current window, ˆxt is its VAE\nreconstruction, and n is the window dimensionality. Higher\nreconstruction errors indicate greater deviation from normal\npatterns, providing additional evidence for anomaly presence.\nDynamic Reward Integration. The final extrinsic reward\ncombines classification accuracy with reconstruction-based\nguidance through a dynamically scaled formulation:\nRtotal(st, at) = R1(st, at) + λ(t) · R2(st),\n(4)\n"}, {"page": 3, "text": "Fig. 1: Our proposed framework: an LSTM-based RL agent operates on sliding windows of the time series; the reward merges\na VAE-based reconstruction term and an LLM potential for semantic shaping with labels supplied via active learning, producing\nanomaly predictions.\nwhere λ(t) is a time-varying coefficient that balances supervised\nclassification feedback (R1) with unsupervised anomaly scoring\n(R2). The coefficient λ(t) is updated after each episode using\nproportional control:\nλt+1 = clip (λt + α (Rtarget −Repisode) , λmin, λmax) , (5)\nwhere α is the adjustment rate, Rtarget is the desired episode\nreward, and Repisode is the actual episode reward. If perfor-\nmance drops (Repisode < Rtarget), λ increases to give more\nweight to the VAE anomaly signal; if performance improves, λ\ndecreases to focus more on classification. Bounds [λmin, λmax]\nkeep updates stable.\nC. LLM-Based Potential Function\nTo enrich the reward signal with semantic context, we\nintroduce a potential-based shaping term derived from an LLM.\nThis component leverages the LLM’s ability to assess complex\ntemporal patterns and provide an interpretable anomaly severity\nscore, which serves as the potential function ϕ(s) in the reward\nshaping framework as shown in Equation (1).\nSeverity Scoring via LLM. For a given state st, represented\nas a sliding window of nsteps consecutive sensor readings, the\nvalues are formatted into a structured prompt and provided to\nthe LLM (e.g., models GPT-3.5, Llama-3.2-3B, and Phi-2). The\nmodel is instructed to output a single JSON object: {”severity”:\nv}. Where v ∈[0, 1] quantifies the likelihood of the current\nwindow containing an anomaly, with 0 representing “certainly\nnormal” and 1 representing “certainly anomalous”. This output\nis parsed to extract the scalar potential ϕ(st) = v.\nFew-Shot Prompt Design. The LLM prompt shows a few\nexample sensor readings with their severity scores to help it\ngive consistent and accurate results. For instance:\n• Sensor readings: [0.0, 0.0, 0.0, 0.0,\n...] →{\"severity\": 0.00}\n• Sensor readings: [0.0, 0.0, 0.0, 5.0,\n5.0, ...] →{\"severity\": 0.75}\nThese examples act as anchors, which help the LLM to map\ndiverse patterns to a bounded and meaningful anomaly severity\nscale. The prompt structure includes system-level instructions\nenforcing JSON-only output to prevent parsing errors and\nensure reliable extraction of severity scores.\nIntegration into Reward Shaping. Recall Equation (1) (i.e.,\nr′(st, at) = r(st, at) + γ ϕ(st+1) −ϕ(st)). In our proposed\nmethod, we define: 1) r(st, at) as the extrinsic reward from\nclassification and VAE-based penalties, and 2) ϕ(s) as the\nLLM-generated potential. Next, similar to the literature, we\ndefine γ as the discount factor. This approach preserves the\noptimal policy while providing a richer training signal that\nguides the agent toward states with higher anomaly likelihood.\nSemantic Context Enhancement. Unlike traditional meth-\nods that only look for statistical patterns in data, the LLM\npotential function uses advanced reasoning to understand\ntime-based patterns. The LLM can identify complex, unusual\nbehaviors like sudden jumps, irregular changes, or subtle\nshifts in trends that purely numerical methods might miss.\nBy combining basic numerical features with the LLM’s high-\nlevel understanding, the potential function connects statistical\nanomaly detection with contextual reasoning. This helps the\nRL agent work more efficiently by better exploring and using\ninformative areas of the data space.\nD. Active Learning Integration\nTo reduce annotation effort while maintaining strong detec-\ntion performance, the framework integrates an AL module\nthat works alongside the RL agent. This module selects\nthe most informative samples for human labeling and uses\nsemi-supervised learning to spread labels to similar unlabeled\ninstances. At the end of each episode, uncertainty is measured\nby the margin Margin(s) = |Q(s, a0) −Q(s, a1)|, and the\ntop NAL states with the smallest margins are sent for manual\nannotation. A label propagation step then assigns pseudo-labels\nto additional unlabeled samples based on similarity weights\nwij, typically computed with a Gaussian kernel in feature\nspace. The KLP most confident pseudo-labels are added to the\nlabeled set, further improving the training signal while limiting\nhuman labeling effort.\n"}, {"page": 4, "text": "IV. EXPERIMENTS\nThis section describes our experiments. We start by describ-\ning the datasets and then address three key research questions:\n(Q1) Does using LLMs for potential-based reward shaping\nperform better than existing methods? (Q2) Which LLM (GPT\n3.5, Llama-3, Phi-2) works best in our anomaly detection\napproach and why? (Q3) How does our method perform on\nunivariate versus multivariate time series datasets?\nA. Datasets\nWe evaluate our framework on two widely used benchmarks:\nYahoo-A1 (67 univariate series from real Yahoo website traffic)\nand SMD (28 multivariate series with 38 sensors from server\nmachines). Yahoo-A1 contains hourly collected data with\nlabeled anomaly points, while SMD includes 10 days of sensor\ndata with normal patterns in the first 5 days and random\nanomalies in the last 5 days [19], [20]. Table I summarizes\nthe key dataset statistics.\nTABLE I: Key Statistics for Yahoo-A1 and SMD\nBenchmark\n# series\n# dims\nAnomaly %\nYahoo-A1\n67\n1\n1.76%\nSMD\n28\n38\n4.16%\nB. Results and Discussions\nTo answer Q1, Table II (left) summarizes performance\non Yahoo A1. Our method with Llama-3 achieves an F1 of\n0.7413 with precision/recall of 0.6051/0.9565, essentially on\npar with the strongest non-LLM baseline (CARLA: 0.7233 F1).\nCompared with earlier deep baselines such as TimesNet (0.5135\nF1) and TranAD (0.5654 F1), the gain is sizeable. The high\nrecall indicates the LLM-shaped potential guides the agent to\nconsistently surface true anomalies, while maintaining moderate\nprecision. In contrast, the GPT-3.5 variant attains very high\nrecall (0.9130) but extremely low precision (0.0742), leading\nto a poor F1 of 0.1372. This reflects an over-eager anomaly\nprior to GPT that over-flags peaks. The Phi-2 variant shows\nthe opposite behavior, 0.6666 precision but only 0.4761 recall,\nsuggesting a conservative prior that misses many positives; its\nF1 is 0.5555.\nOn SMD (right side of the Table II), Llama-3 again provides\nthe best trade-off, reaching an F1 of 0.5300 with precision/recall\nof 0.3813/0.8685. This outperforms CARLA (0.5114 F1)\nand other non-LLM baselines. The Industrial SMD series\ncontains strong seasonality and gradual drifts; the Llama-\nshaped potential appears calibrated enough to keep recall high\nwithout flooding detections. GPT-3.5 performs better than on\nYahoo (precision/recall 0.5370/0.4061; F1 = 0.4625), but it\nstill underperforms Llama-3, likely because its anomaly prior\nis too permissive. Phi-2 is highly precise on SMD (0.8461) yet\nrecalls only 0.2541 of anomalies (F1 0.3908), again consistent\nwith a cautious prior.\nTo answer Q2, we identify three practical reasons. First,\nLlama-3 follows instructions well and produces smooth severity\nFig. 2: SMD anomaly detection — Llama. Colors: blue =\noriginal signal; green = true (ground-truth) anomalies; red =\ndetected anomalies. Panels: top-left shows ground truth over\nthe signal; top-right shows ground truth + detected anomalies;\nbottom-left and bottom-right are two zoomed-in regions.\nscores within the required [0, 1] range, creating balanced\nsignals that work effectively for potential-based reward shaping.\nSecond, Llama-3 shows better pattern recognition compared\nto other models. GPT often overreacts to minor changes\nand assigns high scores too broadly. An example of this\nphenomenon is illustrated in Figure 2, where Llama-3 properly\nidentifies significant changes and outliers without being overly\nsensitive to normal variations. Third, Llama-3 produces more\nconsistent outputs across different time windows, resulting in\nless noisy shaped rewards and helping the RL agent learn\nclearer decision boundaries.\nTo answer Q3, Yahoo A1 is a simple dataset where each time\nseries has only one variable and anomalies are typically sudden\nspikes or shifts in the mean value. These types of anomalies\nare easy to detect using short time windows because an LLM\ncan identify obvious patterns like spikes or jumps, leading to\ngood detection performance. In contrast, SMD is a complex\nmultivariate dataset with many related sensors per machine\nthat show seasonal patterns and gradual changes over time.\nIn SMD, real anomalies usually occur when multiple sensors\nchange together or when deviations continue for extended\nperiods, making individual sensors appear normal despite the\noverall anomaly. This complexity makes detection more difficult\nbecause methods need to understand relationships between\ndifferent sensors and track changes over time, or they will\neither miss real anomalies or incorrectly flag normal seasonal\nvariations. Our approach computes LLM severity scores for\neach window and uses them for reward shaping, which works\nwell for Yahoo’s simple, single-variable anomalies but struggles\nwith SMD’s complex multi-sensor patterns unless the model\ncan capture cross-sensor relationships and temporal consistency.\nV. CONCLUSION\nThis study presents a novel framework integrating LLM-\nbased potential functions with reinforcement learning for\ntime series anomaly detection, addressing sparse rewards and\nlimited labeled data challenges. Our approach demonstrates that\nsemantic reward shaping through LLMs, particularly Llama-\n"}, {"page": 5, "text": "TABLE II: Comparison of our framework on two benchmark time series datasets: Yahoo A1 and SMD\nYahoo A1\nSMD\nModel\nPrec\nRec\nF1\nPrec\nRec\nF1\nTHOC [21]\n0.1495\n0.8326\n0.2534\n0.0997\n0.5307\n0.1679\nTranAD [22]\n0.4185\n0.8712\n0.5654\n0.2649\n0.5661\n0.3609\nTS2Vec [23]\n0.3929\n0.6305\n0.4841\n0.1033\n0.5295\n0.1728\nDCdetector [24]\n0.0598\n0.9434\n0.1124\n0.0432\n0.9967\n0.0828\nTimesNet [25]\n0.3808\n0.7883\n0.5135\n0.2450\n0.5474\n0.3385\nCARLA [26]\n0.5747\n0.9755\n0.7233\n0.4276\n0.6362\n0.5114\nProposed Method + GPT-3.5\n0.0742\n0.9130\n0.1372\n0.5370\n0.4061\n0.4625\nProposed Method + Llama-3.2-3B\n0.6051\n0.9565\n0.7413\n0.3813\n0.8685\n0.5300\nProposed Method + Phi-2\n0.6666\n0.4761\n0.5555\n0.8461\n0.2541\n0.3908\n3, effectively guides RL agents while maintaining policy\ninvariance, with VAE-enhanced dynamic reward scaling and\nactive learning achieving competitive performance on Yahoo-\nA1 and SMD datasets. Experimental results reveal distinct\nLLM behavioral patterns in anomaly assessment, with Llama-3\nproviding optimal precision-recall balance across univariate and\nmultivariate scenarios. The framework’s effectiveness under\ndata-constrained conditions makes it valuable for real-world\napplications, with future work exploring complex multivariate\nrelationships and additional LLM architectures.\nACKNOWLEDGMENT\nThis material is based on work supported by the National\nScience Foundation under grant no. 2244551.\nREFERENCES\n[1] B. Kim, M. A. Alawami, E. Kim, S. Oh, J. Park, and H. Kim, “A\ncomparative study of time series anomaly detection models for industrial\ncontrol systems,” Sensors, vol. 23, no. 3, p. 1310, 2023.\n[2] M. Zhang, Y. Sun, and F. Liang, “Sparse deep learning for time series\ndata: Theory and applications,” in Proc. 37th Conf. Neural Inf. Process.\nSyst. (NeurIPS 2023), Poster, 2023.\n[3] H. Bastani, O. Bastani, and W. S. P. Sinchaisri, “Improving human\nsequential decision-making with reinforcement learning,” Management\nScience, 2025, articles in Advance.\n[4] L. Kweider, M. A. Kassem, and U. Sandouk, “Anomalous state sequence\nmodeling to enhance safety in reinforcement learning,” IEEE Access,\nvol. 12, pp. 157 140–157 148, 2024.\n[5] X. Chen, R. Xiao, Z. Zeng, Z. Qiu, S. Zhang, and X. Du, “Semi-\nsupervised anomaly detection via adaptive reinforcement learning-enabled\nmethod with causal inference for sensor signals,” 2024.\n[6] F. Khanizadeh, A. Ettefaghian, G. Wilson, A. Shirazibeheshti, T. Radwan,\nand C. Luca, “Smart data-driven medical decisions through collective\nand individual anomaly detection in healthcare time series,” Int. J. Med.\nInformatics, vol. 194, p. 105696, 2025.\n[7] S. Bhambri, A. Bhattacharjee, D. Kalwar, L. Guan, H. Liu, and\nS. Kambhampati, “Extracting heuristics from large language models\nfor reward shaping in reinforcement learning,” 2024.\n[8] A. Y. Ng, D. Harada, and S. J. Russell, “Policy invariance under reward\ntransformations: Theory and application to reward shaping,” in Proc.\n16th Int. Conf. Mach. Learn. (ICML), 1999, pp. 278–287.\n[9] G. C. Forbes, N. Gupta, L. Villalobos-Arias, C. M. Potts, A. Jhala, and\nD. L. Roberts, “Potential-based reward shaping for intrinsic motivation,”\nin Proc. 23rd Int. Conf. Autonomous Agents and Multiagent Systems\n(AAMAS), Auckland, New Zealand, 2024, pp. 589–597.\n[10] B. Golchin and B. Rekabdar, “Anomaly detection in time series data using\nreinforcement learning, variational autoencoder, and active learning,” in\nProc. Conf. AI, Science, Engineering, and Technology (AIxSET). Laguna\nHills, CA, USA: IEEE, 2024, pp. 1–8.\n[11] E. Wiewiora, “Potential-based shaping and q-value initialization are\nequivalent,” J. Artif. Intell. Res., vol. 19, pp. 205–208, 2003.\n[12] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction,\n2nd ed.\nCambridge, MA, USA: MIT Press, 2018.\n[13] N. Chentanez, A. G. Barto, and S. P. Singh, “Intrinsically motivated\nreinforcement learning,” in Advances in Neural Information Processing\nSystems 17.\nCambridge, MA, USA: MIT Press, 2005, pp. 1281–1288.\n[14] A. G. Barto, “Intrinsic motivation and reinforcement learning,” in Intrinsic\nMotivation and Reinforcement Learning, G. Baldassarre and M. Mirolli,\nEds.\nBerlin, Germany: Springer, 2013, pp. 17–47.\n[15] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven\nexploration by self-supervised prediction,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. (CVPR), 2017, pp. 2778–2787.\n[16] S. Devlin and D. Kudenko, “Dynamic potential-based reward shaping,”\nin Proc. 11th Int. Conf. Autonomous Agents and Multiagent Systems\n(AAMAS), 2012, pp. 433–440.\n[17] M. Grzes, “Reward shaping in episodic reinforcement learning,” in Proc.\n16th Int. Conf. Autonomous Agents and MultiAgent Systems (AAMAS),\n2017, pp. 565–573.\n[18] E. A. Elaziz, R. Fathalla, and M. Shaheen, “Deep reinforcement learning\nfor data-efficient weakly supervised business process anomaly detection,”\nJournal of Big Data, vol. 10, no. 1, p. 33, 2023.\n[19] N.\nLaptev,\nB.\nY.,\nand\nS.\nAmizadeh.\n(2015)\nA\nbenchmark\ndataset\nfor\ntime\nseries\nanomaly\ndetection.\nYahoo!\nResearch\nBlog.\n[Online].\nAvailable:\nhttps://yahooresearch.tumblr.com/post/\n114590420346/a-benchmark-dataset-for-time-series-anomaly\n[20] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, “Robust anomaly\ndetection for multivariate time series via stochastic recurrent neural\nnetwork,” in Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.\n(KDD), 2019, pp. 2828–2837.\n[21] L. Shen, Z. Li, and J. T. Kwok, “Time-series anomaly detection\nusing temporal hierarchical one-class network,” in Advances in Neural\nInformation Processing Systems 33 (NeurIPS), 2020, pp. 13 016–13 026.\n[22] S. Tuli, G. Casale, and N. R. Jennings, “Tranad: Deep transformer\nnetworks for anomaly detection in multivariate time series data,” Proc.\nVLDB Endow., vol. 15, no. 6, pp. 1201–1214, 2022.\n[23] Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu,\n“Ts2vec: Towards universal representation of time series,” in Proc. AAAI\nConf. Artif. Intell. (AAAI), 2022, pp. 8980–8987.\n[24] Y. Yang, C. Zhang, T. Zhou, Q. Wen, and L. Sun, “DCdetector: Dual\nattention contrastive representation learning for time series anomaly\ndetection,” in Proc. 29th ACM SIGKDD Int. Conf. Knowl. Discov. Data\nMin. (KDD), Long Beach, CA, USA, 2023.\n[25] H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long, “Timesnet:\nTemporal 2d-variation modeling for general time series analysis,” in Int.\nConf. Learn. Representations (ICLR), 2023.\n[26] Z. Z. Darban, G. I. Webb, S. Pan, C. C. Aggarwal, and M. Soleimani,\n“Deep learning for time series anomaly detection: A survey,” ACM Comput.\nSurv., vol. 57, no. 1, pp. Article 15, 1–42, 2025.\n"}]}