{"doc_id": "arxiv:2602.08997", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.08997.pdf", "meta": {"doc_id": "arxiv:2602.08997", "source": "arxiv", "arxiv_id": "2602.08997", "title": "Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs", "authors": ["Lavender Y. Jiang", "Xujin Chris Liu", "Kyunghyun Cho", "Eric K. Oermann"], "published": "2026-02-09T18:43:19Z", "updated": "2026-02-09T18:43:19Z", "summary": "Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.08997v1", "url_pdf": "https://arxiv.org/pdf/2602.08997.pdf", "meta_path": "data/raw/arxiv/meta/2602.08997.json", "sha256": "567fbde828b3ce8d995cbeb2d0132ed2c6ec2be9b6b39c74fef8df0b4111f5bf", "status": "ok", "fetched_at": "2026-02-18T02:19:27.808144+00:00"}, "pages": [{"page": 1, "text": "Paradox of De-identification:\nA Critique of HIPAA Safe Harbour in the Age of LLMs\nLavender Y. Jiang\nlavender.jiang@nyu.edu\nXujin Chris Liu\nxl3942@nyu.edu\nKyunghyun Cho\nkyunghyun.cho@nyu.edu\nEric K. Oermann\nEric.Oermann@nyulangone.org\nNew York University, New York, NY, USA\nAbstract\nPrivacy is a human right that sustains patient-\nprovider trust.\nClinical notes capture a pa-\ntient’s private vulnerability and individuality,\nwhich are used for care coordination and re-\nsearch.\nUnder HIPAA Safe Harbor, these\nnotes are de-identified to protect patient pri-\nvacy. However, Safe Harbor was designed for\nan era of categorical tabular data, focusing on\nthe removal of explicit identifiers while ignoring\nthe latent information found in correlations be-\ntween identity and quasi-identifiers, which can\nbe captured by modern LLMs.\nWe first for-\nmalize these correlations using a causal graph,\nthen validate it empirically through individ-\nual re-identification of patients from scrubbed\nnotes. The paradox of de-identification is fur-\nther shown through a diagnosis ablation: even\nwhen all other information is removed, the\nmodel can predict the patient’s neighborhood\nbased on diagnosis alone. This position paper\nraises the question of how we can act as a com-\nmunity to uphold patient-provider trust when\nde-identification is inherently imperfect.\nWe\naim to raise awareness and discuss actionable\nrecommendations.\nData and Code Availability\nFrom a large, ur-\nban, academic hospital, we collected 222,949 identi-\nfied clinical notes from 170,283 patients (3.34 times\nmore patients than MIMIC-IV, the largest publicly\navailable EHR dataset).\nDue to the sensitive na-\nture of protected health information and the inherent\nrisks associated with re-identification, neither the raw\nnor de-identified data can be made publicly available.\nTo support reproducibility, our code is accessible via\nhttps://github.com/nyuolab/paradox_of_deid\nentification.\nInstitutional Review Board (IRB)\nThis study\nwas approved by the Institutional Review Board\n(IRB) at NYU Langone Health (study protocol s21-\n01189). The methods were carried out in accordance\nwith the IRB’s relevant guidelines and regulations.\ninference\nLinkage Attack\nPatient was born on\n[DOB (redact)] and\nlives in [ZIP\n(redact)]. Patient is \npregnant and\nloves dressage. \ncorrelation\ncorrelation\nmodel\nDOB\nZIP\n12300\nPredicted attributes\nLinkage Attack\nDOB\nZIP\n12345\nTrue attributes\nTabular ReID (traditional)\nPatient\nText ReID (ours)\nFigure 1: Linkage attack of texts is more challenging\nyet possible with modern language models.\n1. Introduction\nPrivacy is a fundamental pillar of human rights and\ndemocratic participation. Although people increas-\ningly sacrifice privacy in exchange for convenience of\ndigital services, studies show that people continue to\nvalue privacy and expect protections (Madden, 2015).\n© L.Y. Jiang, X.C. Liu, K. Cho & E.K. Oermann.\narXiv:2602.08997v1  [cs.CY]  9 Feb 2026\n"}, {"page": 2, "text": "Paradox of De-id\nHealthcare is an exception where people knowingly\nshare the most intimate details of their lives with\nproviders, a disclosure predicated on confidentiality.\nThis is not just a legal requirement but also a clin-\nical necessity, since the quality of care relies on the\npatient’s willingness to be honest (California Health-\ncare Foundation, 2017; Birkh¨auer et al., 2017). In the\nUnited States, this trust is codified by the Health In-\nsurance Portability and Accountability Act (HIPAA).\nHIPAA allows for sharing and sale of patient notes\nafter “de-identification”. However, the term increas-\ningly becomes a misnomer since it does not imply the\nabsence of re-identification risk. Under Safe Harbor,\n“de-identification” typically involves the removal of\n18 categories of Protected Health Information (PHI)\n(Office of Civil Rights, 2022). In practice, this is usu-\nally achieved via Named Entity Recognition (NER)\nand rule-based systems (Norgeot et al., 2020).\nHIPAA’s attempt to balance utility and privacy us-\ning scrubbed notes has become an impossible object\nin the era of LLMs. Much like the Penrose triangle,\nthe redaction appears coherent in isolation but rep-\nresents a structural paradox when viewed together\nas a causal graph. Language models can learn nu-\nanced correlations between seemingly non-sensitive\ninformation and the patient’s identity through back-\ndoor paths that remains after de-identification.\nHere we show both principally and empirically that\ncurrent “de-identification” practices are insufficient\nin the presence of LLMs. Through ablation studies,\nwe show that keeping diagnosis alone results in iden-\ntity leakage.\nThis suggests that “de-identification”\nis a paradox: how can clinically useful notes be\nsafely shared if the very core medical informa-\ntion enables re-identification?\nOur goal is to raise awareness and call for culture\nchanges in Health AI community to reflect the capa-\nbilities of modern AI. While we do not propose an im-\nmediate solution, we posit that the first step toward\na remedy is a rigorous redefinition of the problem.\n2. Related Work\nLinkage Attacks and Regulatory Origins\nThe\nvulnerability of de-identified data to linkage attacks is\nwell-documented. Seminal work by Sweeney (2000)\ndemonstrated that scrubbed tabular datasets could\nbe linked to individuals using public records, such as\nthe re-identification of the Massachusetts Governor’s\nmedical records. These findings directly influenced\nthe design of the HIPAA Safe Harbor provisions,\nwhich mandate the removal of specific identifiers to\nmitigate re-identification risks. However, in the era\nof Large Language Models (LLMs), the assumption\nthat removing explicit identifiers is sufficient is in-\ncreasingly challenged. LLMs can exploit latent corre-\nlations between non-scrubbed concepts (e.g., stylistic\nnuances) and the underlying patient identity, weak-\nening the established privacy guarantees.\nCritiques of De-Identification\nWe are not the\nfirst to question the robustness of de-identification.\nOhm (2009) argued that “data can be either use-\nful or perfectly anonymous but never both,” describ-\ning the “broken promises of privacy.” Similarly, the\ndifferential privacy community has long argued that\ntraditional de-identification fails to provide rigorous\nmathematical privacy guarantees (Dwork and Roth,\n2014). Despite these academic critiques, the health-\ncare industry continues to rely heavily on rule-based\nde-identification tools such as Philter (Norgeot et al.,\n2020) and treats de-identified clinical notes as a trad-\nable commodity. This suggests a critical gap between\ntheoretical privacy limitations and practical deploy-\nment.\nTechnical Defenses and Legal Frontiers\nCon-\nsiderable research has focused on improving de-\nidentification techniques, including NER-based ap-\nproaches to tag explicit identifiers (Neamatullah\net al., 2008; Johnson et al., 2016; Norgeot et al., 2020;\nYang et al., 2019; Johnson et al., 2023) and adver-\nsarial training and text rewriting (Friedrich et al.,\n2019; Morris et al., 2022). While these technical ad-\nvancements are important pieces of the puzzles, our\nwork suggests that improving de-identification under\nthe current “scrub-and-share” framing may be insuf-\nficient. Legal scholarship has also increasingly scru-\ntinized the regulation of highly personal data in the\ncontext of LLM interactions (Narayanan and Kapoor,\n2024; Nolte et al., 2025; Bommasani et al., 2025). We\nargue that the research community should go beyond\nminimal legal compliance (specifically the static re-\nquirements of Safe Harbor) and adopt a proactive\nethical stance that upholds the spirit of patient pri-\nvacy in the face of increasingly powerful inference\ntechnologies.\n3. The paradox from causal graphs\nBy constructing a causal graph using our assumptions\non the data-generating process, we can identify the\nbackdoors that are left open for linkage attacks. A\n2\n"}, {"page": 3, "text": "Paradox of De-id\nPatient was born on\n[DOB (redact)] and lives\nin [ZIP (redact)]. Patient\nis pregnant and loves\ndressage. \nNote Generation\nDe-identification\n\"deidentified\" notes\nmedical\nidentity\nnonsensitive\nsensitive\nbackdoor\n\"deid\"\nFigure 2: The current de-identification frameworks (purple) leaves two backdoors open for linkage attacks\nvia non-sensitive (blue) and medical (yellow) information.\ncausal graph is a type of probabilistic graphical model\nin which each node is a random variable, and the\nparents of a node are “direct causes” of that node.\nWe say u causes v, if changes in u result in changes\nin v when all other variables remain constant. For\ninstance, Figure 3 uses I to represent the identity of\na patient (green) and M to represent the patient’s\nmedical information (yellow). For illustration, let’s\nassume M represents pregnancy status. If we change\nthe patient’s biological sex from female to male, then\nM must be “not pregnant”. Therefore, we draw a\ndirected edge from I to M that encodes a causation\nrelationship.\nNote Generation\nFigure 3 (left) illustrates the\ngenerating process of real-world clinical notes based\non our assumptions.\nThe green node I represents\nthe patient’s identity (e.g., address and occupation).\nAssociated with this identity are sensitive informa-\ntion S (red node; e.g., DOB and ZIP code), medi-\ncal information M (yellow node; e.g., diagnosis), and\nnon-sensitive information N (blue node; e.g., a hobby\nsuch as breakdancing). The clinical note X (orange\nnode) is derived from these three variables (S, M, N).\nWe assume that only the clinical note X is observed\nand shared, while the other latent variables remain\nunobserved.\nDe-identification\nFigure 3 (right) illustrates how\nthe current de-identification paradigm leaves two\nbackdoors open for linkage attacks. This paradigm\nproduces “de-identified” notes X′ by detecting and\nremoving HIPAA-protected attributes,\neffectively\nsevering the red edge from S to X.\nHowever, the\ncorrelation between X′ and I persists, mediated by\nbackdoor paths (represented by dotted red arrows).\nFirst, the blue path (X′ ←N ←I →S) shows the\ncorrelation via non-sensitive attributes N. Second,\nthe yellow path (X′ ←M ←I →S) shows the cor-\nrelation via medical information M.\nExamples\nTo make this concrete, let’s examine\nboth a synthetic and a real-world example.\nCon-\nsider the clinical note snippet in the center of Fig-\nure 3: “Patient was born on [DOB] and lives at [ZIP\ncode]. Patient is pregnant and loves dressage.” Al-\nthough the protected attributes (DOB and ZIP code)\nare redacted, we can still infer that the patient is\nan adult female based on the pregnancy, and resides\nin an affluent neighborhood given the hobby of dres-\nsage. Regarding real-world data, consider MIMIC-III\nand MIMIC-IV, which were de-identified by mask-\ning out HIPAA-protected attributes (Johnson et al.,\n2016, 2023).\nThese datasets contain 7-12 years of\nelectronic health records (EHR) from the ICU (or\nICU and ED) of Beth Israel Hospital, covering 38,597\n- 50,920 adult patients.\nDe-identification in these\ndatasets employed regular expression filters, language\nmodel-based detectors, or a combination of both to\ndetect and replace sensitive attributes such as name\nand address.\nThe Paradox\nOur causal graph reveals that “de-\nidentification” is fundamentally a misnomer.\nCur-\nrent practices define success by severing the direct\nlink (S →X) via identifier removal, while systemati-\ncally ignoring backdoor paths persisting through non-\nsensitive (N) and medical (M) attributes. This cre-\nates an inescapable paradox: even under perfect Safe\nHarbor compliance, “de-identified” notes remain sta-\ntistically tethered to identity through the very corre-\nlations that confirm their clinical utility. The conflict\n3\n"}, {"page": 4, "text": "Paradox of De-id\nData\nPath\nAUC\nRandom Guess\nNone\n50.00\nDiagnosis Only\nX′ →M ←I →S only\n58.57\nDe-identified Note\nX′ →M ←I →S and\nX′ →N ←I →S\n78.35\nIdentified Note\nAll open paths (via M, N, S)\n82.78\nTable 1: Borough prediction AUC shows that diagnosis alone leaks identity, with non-sensitive information\ncontributing most to re-identification risk.\nis structural instead of technical. While Scaiano et al.\n(2016) warned of quasi-identifiers, we argue the ten-\nsion is intrinsic: complete privacy demands severing\nall pathways to identity, but clinical utility requires\npreserving the medical content that leaks it. Table 1\nexposes this paradox empirically. A model predict-\ning patient neighborhood achieves above-random ac-\ncuracy from diagnosis alone (AUC 58.57%), rising to\n78.35% with de-identified notes, which is startlingly\nclose to the 82.78% achieved with fully identified\nnotes.\nThis narrow 4.43 point gap reveals a trou-\nbling truth: the vast majority of re-identification risk\nstems not from Protected Health Information, but\nfrom the non-sensitive and medical content we deem\nsafe to share. Appendix C visualizes this geographic\nvariation, showing that health outcomes and demo-\ngraphics cluster by neighborhood. See section 4.1 for\nexperimental details.\n4. Empirical Evidence from\nRe-identification\nThe backdoor paths identified in section 3 suggest\nthat patients can be re-identified from standard de-\nidentified notes with accuracy significantly exceed-\ning random chance. To validate this, we conduct a\ntwo-stage attack: first inferring sensitive attributes\nfrom redacted notes, and then using these predic-\ntions to link individuals. This approach goes beyond\ntypical privacy studies by demonstrating actual indi-\nvidual linkage rather than just attribute inference.\nWe benchmark performance against random base-\nlines: for attribute prediction, the baseline is uniform\nprobability.\nFor re-identification, the baseline con-\nsists of guessing the majority class of each attribute\nand drawing uniformly from the matched group.\nFigure 3 illustrates the re-identification attack pro-\ncess.\nFirst, an adversary trains a model to learn\ncorrelations between sensitive attributes and the re-\nmaining content (medical and non-sensitive) in de-\nidentified notes. Second, the adversary applies this\nmodel to unseen notes to predict the redacted sensi-\ntive attributes (e.g., DOB and zip code). Finally, us-\ning the predicted attributes, the adversary queries an\nexternal database to identify individuals with match-\ning attributes and randomly selects one person from\nthe matching group. If the selected person is the orig-\ninal patient, the attack succeeds; otherwise, it fails.\nFor simplicity, we assume the adversary has access\nto the complete patient population in the external\ndatabase. In practice, the adversary may only have\naccess to a subset of the population, which would\nmake the attack more difficult. However, even with\nthis assumption, we can still demonstrate that de-\nidentified notes are not as private as they seem.\n4.1. Experiment Setup\nData.\nWe collected 222,949 identified clinical notes\nfrom 170,283 patients at NYU Langone, a large, ur-\nban, academic hospital.\nThis cohort is 3.34 times\nlarger than MIMIC-IV, the largest publicly available\nEHR dataset. For each patient, we curated six de-\nmographic attributes: biological sex, note year, note\nmonth, borough, zip code median income, and in-\nsurance type (see Table 2).\nThese attributes were\nselected to approximate the unique identifier trio\n(Sweeney, 2000) consisting of biological sex, birth\ndate, and zip code, which can collectively isolate over\n87% of the U.S. population. In our work, birth date\nis proxied by note year and month (combined with\nage mentions), and zip code is inferred from borough,\narea income, and insurance type (see Appendix C for\ngeographic distributions of these attributes). Clinical\nnotes were de-identified using UCSF philter (Nor-\ngeot et al., 2020). We partitioned the dataset by pa-\n4\n"}, {"page": 5, "text": "Paradox of De-id\ntient into training (80%), validation (10%), and test\n(10%) splits, ensuring all notes from a given patient\nremain in the same split.\nFinetuning\nWe\nfine-tuned\nBERT-base-uncased\n(110M parameters) (Devlin et al., 2018), pretrained\nonly on general text to prevent clinical information\nleakage, to predict each attribute separately. See Ap-\npendix B for details.\nPrediction Evaluation.\nWe evaluate the general-\nization capability of our fine-tuned models on the test\nset using two complementary metrics: Accuracy and\nweighted Area Under the Receiver Operating Char-\nacteristic Curve (ROC-AUC). While Accuracy mea-\nsures the absolute hit rate, ROC-AUC provides a ro-\nbust estimate of discriminative power, particularly\nvaluable for attributes with class imbalance.\nRe-identification Strategy.\nWe simulate an ad-\nversarial linkage attack where fine-tuned models\nrecover redacted demographic attributes from de-\nidentified notes to pinpoint individuals in a patient\ndatabase.\nWe employ a top-k matching strategy:\nfor the i-th attribute, we select the top ki predicted\nclasses and filter the database for patients match-\ning any of these values.\nTo rigorously analyze the\ntrade-off between prediction accuracy and the speci-\nficity of the matched group, we exhaustively sweep\nki ∈{1, . . . , ci} for attributes with ci classes.\nRe-identification Evaluation.\nWe assess risk us-\ning three metrics:\n1. Group\nRe-identification\nSuccess\nRate\n(PG = Πa∈APa): the frequency with which the\ntrue patient is captured within the predicted can-\ndidate set Ω. This requires correct top-k predic-\ntions for all attributes a ∈A such as neighbor-\nhood.\n2. Individual Re-identification from Group\n(PI|G), the probability of identifying the true pa-\ntient from a correctly identified group\n3. Probability\nof\nUnique\nRe-identification\n(Preid = PI|G × PG), the overall likelihood that\nan adversary uniquely identify a patient from de-\nidentified notes.\nNote that Preid is a conservative lower bound be-\ncause we model PI|G as a uniform 1/|Ω| chance, where\n|Ω| is the size of the candidate pool. This however\nlikely underestimates real-world risks, because adver-\nsaries often leverage auxiliary knowledge such as so-\ncial media to prune the candidate pool and achieve\nhigher individual matching accuracy.\n4.2. Experimental Results\nAttribute prediction exceeds random chance\nusing as few as one thousand examples.\nAs\nillustrated in Figure 4, de-identified clinical notes re-\nmain vulnerable to attribute prediction. Across all\nsix attributes and all data regimes (1k to 177k exam-\nples), the language model (red) consistently outper-\nform random baselines (grey). These results empir-\nically supports that de-identification process retains\nexploitable signals in the two backdoor paths.\nThe privacy risk is immediate:\nmodels achieve\nabove-random performance with as few as 1,000\ntraining examples. While biological sex is the most\nexposed attribute (recovered with > 99.7% accu-\nracy), even the subtlest signals (month of note) are\npredicted with better-than-random accuracy.\nIndividual\nre-identification significantly ex-\nceeds random baseline.\nWe evaluate the risk of\nindividual re-identification (Preid = PI|G ×PG) as the\nproduct of the probability of correct group identifica-\ntion (PG) and the probability of selecting the true pa-\ntient from that identified group (PI|G ≈1/|Ω|). Fig-\nure 5 visualizes this relationship, comparing language\nmodel inferences (circles) against guessing majority-\nclass (crosses).\nThe language model predictions shift clearly to-\nward the top-right of the plot.\nThe top direction\nshows that for any given group success rate (PG), LM\nidentifies smaller group (higher PI|G). The right di-\nrection shows that for any fixed identified group size\n(PI|G), LM has a higher group success rate (PG). In\nfact, the max re-identification risk for the language\nmodel (0.34%) is approximately 37 times higher than\nthe highest majority-class baseline (0.0091%).\nInterpretation of the risks.\nWhile a 0.34% re-\nidentification risk may appear numerically small, the\ndetermination of “acceptable” risk is a complex pol-\nicy question that statistics alone cannot resolve. For\npatients with rare medical histories or marginalized\nidentities, even a low probability of exposure consti-\ntutes a significant breach of safety and trust. Fur-\nthermore, this risk should be viewed at scale: when\napplied to a corpus the size of MIMIC-IV, a 0.34%\nrisk implies the potential re-identification of roughly\n5\n"}, {"page": 6, "text": "Paradox of De-id\nAttribute\n# Classes\nPotential Values\nBiological Sex\n2\nFemale, Male\nNeighborhood\n6\nManhattan, Brooklyn, Bronx, Queens, Staten Island, Others\nNote Year\n10\n2012–2021\nNote Month\n12\nJanuary–December\nArea Income\n2\nPoor (below median), Rich (above median)\nInsurance Type\n2\nPublic (Medicare/Medicaid), Private\nTable 2: Demographic attributes predicted from de-identified clinical notes. These attributes approximate\nthe unique identifier trio from Sweeney (2000).\nPatient was born on\n[DOB (redact)] and lives\nin [ZIP (redact)]. Patient\nis pregnant and\nloves dressage. \nmodel\ninference\n2000\nrichville\nPredicted attributes\nmatch\ncandidate\ndb\n \n \nFigure 3: Re-identification process overview. As a simplified example, a single de-identified notes is\npassed through a LM for DOB and ZIP prediction. The predictions are used to match patients in\na candidate database, resulting in a candidate set Ω. In this case, the re-identification risk is zero\nif either of the prediction is wrong. Otherwise, the risk is 1/|Ω| based on random draw from the\ncorrectly identified group.\n170 patients. The question then becomes how many\nleaks are acceptable, particularly when data subjects\nare not aware of the possibility of re-identification via\nsecondary model inference.\n5. A Call to Action\nOur findings challenge the core premise of the HIPAA\nSafe Harbor standard, which operates on a binary\ndefinition of privacy: data is either “identified” or\n“de-identified.”\nHIPAA assumes that removing a\nstatic list of tokens renders data “safe”, effectively de-\ncoupling the clinical narrative from the patient’s iden-\ntity. However, our causal graph analysis and empiri-\ncal results suggests that this decoupling is a mirage.\nClinical notes are inherently entangled with identity.\nA patient’s medical diagnosis and non-redacted nar-\nratives are direct products of their unique life trajec-\ntory, creating high-dimensional signature that can be\nmapped back to the individual.\nBy treating de-identification as a deterministic pro-\ncess of redaction, the current framework ignores the\ncorrelation remaining in the residual text. In the era\nof LLMs, which excel at synthesizing subtle and het-\nerogeneous patterns, the distinction between “iden-\ntifiers” and “content” collapses. A specific diagnosis\npaired with a niche hobby is no longer just medical\ndata but also a smeared fingerprint.\nThis reveals a sociotechnical disconnect: policy ac-\ncepts a method (scrubbing 18 identifiers) as a proxy\nfor a goal (anonymity), but advancing technology\ncontinuously degrades that proxy’s validity.\nIf we\naccept a 0.34% re-identification risk as “safe”, we\nare implicitly approving the exposure of over 800,000\nindividuals, a figure derived from applying our em-\npirically observed rate (measured on a diverse urban\nhospital population) to the 278 million US urban resi-\ndents. For vulnerable populations, this is not a statis-\ntic but a breach of trust.\nThe commodification of\n“de-identified” data thus rests on a precarious ethi-\ncal foundation, trading on a promise of privacy that\nmodern LM is uniquely equipped to break.\n6\n"}, {"page": 7, "text": "Paradox of De-id\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nTarget: Biological sex\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nTarget: Neighborhood\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nTarget: Year\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nTarget: Month\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nTarget: Income\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAccuracy (%)\nTarget: Insurance\nSignal from 'De-identified' Notes\nPrediction based on Random Guess\nFigure 4: Attribute prediction exceeds random chance using as few as 1k examples. LM’s accuracy\n(red bars) is better than random guess (grey bars) and improve with more data. AUCs shows a\nsimilar above-random pattern whose gap increases with more data (Appendix A).\n0%\n10%\n20%\n30%\n40%\n50%\nProbability that patient is in the identified group\n0.0%\n1.0%\n2.0%\n3.0%\n4.0%\nProbability of random draw\nfrom the identified group\nRisk of Individual Re-identification from De-identified Data\nRandom Guess\nLM Guess\n0.05%\n0.10%\n0.15%\n0.20%\n0.25%\n0.30%\nProbability of individual re-identification (%)\nFigure 5: Individual re-identification risk significantly exceeds random guess. The scatter plot\nshows the relationship between group re-identification probability(PG, x axis) and the the proba-\nbility of identifying the correct individual within that group (PI|G ≈1/|Ω|, y axis). More yellow\npoints toward the upper right represents higher overall re-identification risks. LM predictions (cir-\ncles) consistently outperform majority class guesses (crosses).\n7\n"}, {"page": 8, "text": "Paradox of De-id\n5.1. The Urgency\nThe acceleration of the AI arms race has transformed\nhigh-quality domain-specific text into one of the\nworld’s most sought-after commodities.\nLLMs are\ndata-hungry, requiring trillions of tokens to achieve\nemergent capabilities (Groeneveld et al., 2024; Dubey\net al., 2024). As the low-hanging fruit of public web\ndata is exhausted, protected and high-quality corpora\nsuch as clinical notes become the next frontier.\nHealthcare already accounts for about 30% of the\nglobal data volume (Wiederrecht et al., 2020), yet\nit remains relatively untouched due to privacy con-\nstraints. However, this is ending as major AI com-\npanies such as OpenAI and Anthropic pivot to-\nwards clinical integration (OpenAI, 2026a,b; An-\nthropic, 2026). The current legal framework relies on\nSafe Harbor to justify healthcare data transfer, but\nour results demonstrate that its privacy guarantee is\ninsufficient, especially in the face of modern LLMs.\nThe persistence of Safe Harbor despite known lim-\nitations is not an oversight but a feature of a sys-\ntem optimized for data liquidity rather than patient\nprotection (Ohm, 2009). De-identified clinical notes\nrepresent a multi-billion dollar market (Grand View\nResearch, 2026), creating structural disincentives for\nhealthcare institutions to adopt privacy-preserving\nalternatives that might reduce data utility or require\ncostly infrastructure investments.\nThere is an ur-\ngency to carefully investigate, understand and ad-\ndress this disincentive.\n5.2. Policy Recommendations\nWe propose a shift from binary privacy definitions to\na risk-aware governance framework:\nTiered Access and Accountability:\nWe call for\na regulatory framework where access controls are pro-\nportional to re-identification risk. High-risk data ne-\ncessitates stringent vetting of researchers and regular,\nrenewable access audits by trusted third parties. Fur-\nthermore, shared datasets should implement digital\nwatermarking to ensure provenance and traceability,\ndiscouraging negligence.\nTransparency\nand\nPatient\nRights:\nPatients\nshould be informed that “de-identification” is a prob-\nabilistic risk reduction, not a guarantee of anonymity.\nWe advocate for the right to know when and how\none’s data is being utilized, shifting the paradigm\nfrom implicit waivers to active engagement.\nQuantifiable\nUtility\nChecks:\nData\nrelease\nshould be governed by rigorous pre-sharing evalua-\ntions. We recommend adopting utility quantification\nmetrics such as SecureKL (Fuentes et al., 2025) to\nensure that privacy risks are justified by tangible\nscientific value.\nData that fails to meet utility\nthresholds should not be exposed to privacy risks.\n5.3. Research Recommendations\nSafeguarding sensitive clinical data requires moving\nbeyond a purely technocentric view. We call for in-\nterdisciplinary research that bridges engineering with\nlegal and social governance.\nRejecting Technical Hubris:\nIt is time to aban-\ndon the assumption that de-identification is a prob-\nlem solvable through engineering alone. As our causal\nanalysis demonstrates, perfect de-identification of\nhigh-dimensional clinical text is a technically impossi-\nble goal. Research should pivot towards co-designing\nsystems where technical safeguards are reinforced by\nlegal liability and social contracts, rather than ex-\npecting software to replace the rule of law.\nAdvancing\nDe-identification\nStandards:\nWhile technical solutions alone are insufficient, they\nremain a critical line of defense.\nFuture works\nshould move beyond heuristics to more principled\napproaches.\nPromising directions include differen-\ntially private synthetic data generation (Near, 2021;\nLi, 2022; Lin et al., 2023).\nCultivating Data Transparency:\nWe advocate\nfor the adoption of “data nutrition labels” (Sun et al.,\n2019) to transparently communicate the privacy limi-\ntations of datasets. We should further recognize that\nprivacy risks propagate to downstream models. Since\nlanguage models can memorize training data (Carlini\net al., 2022), models finetuned on “de-identified” or\nsynthetic data should be subject to the same privacy\nconsiderations as the underlying data.\n6. Conclusion\nMedical privacy is foundational to patient-provider\ntrust, quality of care, and individual dignity. Cur-\nrently, this protection relies on HIPAA Safe Harbor,\na standard we argue is increasingly fragile in the era\nof LLMs. By reducing privacy to the removal of 18\nexplicit identifiers, current regulations fail to account\nfor the high-dimensional correlations that permeate\nclinical narratives. We frame de-identification as a\n8\n"}, {"page": 9, "text": "Paradox of De-id\nparadox: much like a Penrose triangle, the redac-\ntion process appears sound locally but proves struc-\nturally impossible when viewed as a whole. Through\ncausal analysis and empirical validation (including\nneighborhood prediction from diagnosis and an end-\nto-end re-identification attack on ∼17k patients), we\ndemonstrate that seemingly de-identified notes retain\nnontrivial signals for individual re-identification. As\ndata-hungry AI systems expand into healthcare, the\nurgency of addressing this vulnerability cannot be\noverstated. We offer concrete recommendations for\npolicymakers and researchers to move beyond “scrub-\nand-share” models.\nWhile no immediate technical\npanacea exists, acknowledging this paradox is the\nfirst step toward a more robust framework for data\ngovernance and patient protection.\n7. Limitations\nOur re-identification attack assumes access to la-\nbeled training data to learn the correlations between\nredacted notes and sensitive attributes.\nWhile our\nstudy utilized internal hospital records, real-world\nadversaries can leverage vast repositories of public\nand illicitly obtained data. For instance, voter reg-\nistration lists publicly disclose names, addresses (in-\ncluding zip codes), birth dates, biological sex, and\npolitical affiliations. Furthermore, sensitive personal\ndata is increasingly exposed through frequent data\nbreaches (Landi, 2020; Bruce, 2024; Office for Civil\nRights, 2024), providing adversaries with rich auxil-\niary datasets for linkage attacks.\nThis study purposefully establishes a conservative\nbaseline by simplifying the experimental setting. In\npractice, the re-identification risk is likely higher, mo-\ntivated adversaries could employ more sophisticated\ntechniques for finding the true patient in an iden-\ntified group.\nFor instance, an adversary could de-\nploy autonomous agent built on trillion-parameter\nbackbones, capable of cross-referencing clinical notes\nwith live web search, social media footprints, and\nother digital traces to achieve high-precision re-\nidentification.\nFinally, our empirical evaluation is restricted to a\nsingle healthcare system. Although the study is lim-\nited to data from a single health system, this sys-\ntem comprises three distinct locations with highly\ndiverse patient demographic. While evaluating multi-\ninstitutional datasets could further strengthen our\nempirical results, the marginal utility of such data\naccess is outweighed by the ethical and privacy risks\nassociated with accessing additional raw identified\nrecords. Furthermore, our theoretical framework (il-\nlustrated by Figure 3) shows that the persistence of\nbackdoor paths is a structural property of the redac-\ntion process itself. Given that the feasibility of link-\nage attack is well-documented in the literature, these\nstructural vulnerabilities suggest that our findings are\nbroadly applicable to other clinical environments uti-\nlizing similar HIPAA Safe Harbor protocols.\nAcknowledgments\nL.Y.J. is supported by Apple AIML PhD fellow-\nship. L.Y.J. and K.C. are supported by NSF Award\n1922658. K.C., E.K.O. and L.Y.J. are supported by\nInstitute for Information & communications Technol-\nogy Promotion (IITP) grant funded by the Korea\ngovernment (MSIT) (No. RS-2019-II190075 Artificial\nIntelligence Graduate School Program (KAIST); No.\nRS-2024-00509279, Global AI Frontier Lab). E.K.O.\nis supported by the National Cancer Institute’s\nEarly Surgeon Scientist Program (3P30CA016087-\n41S1) and the W.M. Keck Foundation.\nWe would\nlike to thank Mimee Xu, Lucas Rosenblatt, Daniel Al-\nber, Gavin Zihao Yang, Karl Lee Sangwon, Zachary\nHorvitz, Hilal Asi, Julia Stoyanovich, Saadia Gabriel,\nA. Feder Cooper, Hima Lakkaraju, Angelica Chen,\nStephanie Milani, Irene Chen, Falaah Arif Khan, Al-\nice Oh for valuable discussion.\nGemini-3-pro and GPT-5.2 were used for revising\nmanuscript and improving figure. Claude Opus 4.6\nwere used for resolving outdated dependencies for\nopen-source code and improving documentation.\nAuthor’s contribution\nK.C. and E.K.O. super-\nvised the project.\nL.Y.J., K.C. and E.K.O. con-\nceptualized the project.\nL.Y.J collected data and\nengineered the software for training and evaluation.\nL.Y.J., X.C.L and K.C. debug the software. L.Y.J.,\nK.C., X.C.L., E.K.O. created figures. L.Y.J., X.C.L.\nand K.C. wrote the initial draft. All authors edited\nand revised the manuscript.\n9\n"}, {"page": 10, "text": "Paradox of De-id\nReferences\nAnthropic. Advancing claude in healthcare and the\nlife sciences. https://www.anthropic.com/ne\nws/healthcare-life-sciences, January 2026.\nAccessed: 2026-2-4.\nJohanna Birkh¨auer, Jens Gaab, Joe Kossowsky, Se-\nbastian Hasler, Peter Krummenacher, Christoph\nWerner, and Heike Gerger. Trust in the health care\nprofessional and health outcome: A meta-analysis.\nPLoS One, 12(2):e0170988, February 2017.\nRishi Bommasani, Sanjeev Arora, Jennifer Chayes,\nYejin Choi, Mariano-Florentino Cu´ellar, Li Fei-\nFei, Daniel E Ho, Dan Jurafsky, Sanmi Koyejo,\nHima Lakkaraju, Arvind Narayanan, Alondra Nel-\nson, Emma Pierson, Joelle Pineau, Scott Singer,\nGa¨el Varoquaux, Suresh Venkatasubramanian, Ion\nStoica, Percy Liang, and Dawn Song. Advancing\nscience- and evidence-based AI policy. Science, 389\n(6759):459–461, July 2025.\nGiles Bruce.\nHackers leak change healthcare con-\ntracts, patient data. https://www.beckershos\npitalreview.com/cybersecurity/hackers-lea\nk-change-healthcare-contracts-patient-dat\na.html, April 2024. Accessed: 2024-9-10.\nCalifornia Healthcare Foundation.\nNational con-\nsumer health privacy survey 2005: Executive sum-\nmary. https://www.chcf.org/wp-content/upl\noads/2017/12/PDF-ConsumerPrivacy2005Exec\nSum.pdf, 2017. Accessed: 2026-1-22.\nNicholas Carlini, Daphne Ippolito, Matthew Jagiel-\nski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang.\nQuantifying memorization across neural\nlanguage models. Feb 2022. URL http://arxi\nv.org/abs/2202.07646.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova.\nBert: Pre-training of deep\nbidirectional transformers for language under-\nstanding. Oct 2018. URL http://arxiv.org/\nabs/1810.04805.\nAbhimanyu\nDubey,\nAbhinav\nJauhri,\nAbhinav\nPandey,\nAbhishek\nKadian,\nAhmad\nAl-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten,\nAmy Yang, Angela Fan, Anirudh Goyal, An-\nthony\nHartshorn,\nAobo\nYang,\nArchi\nMitra,\nArchie Sravankumar,\nArtem Korenev,\nArthur\nHinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\ndriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Roziere, Bethany Biron, Binh Tang, Bob-\nbie Chern, Charlotte Caucheteux, Chaya Nayak,\nChloe Bi, Chris Marra, Chris McConnell, Christian\nKeller, Christophe Touret, Chunyang Wu, Corinne\nWong,\nCristian\nCanton\nFerrer,\nCyrus\nNiko-\nlaidis, Damien Allonsius, Daniel Song, Danielle\nPintz,\nDanny Livshits,\nDavid Esiobu,\nDhruv\nChoudhary, Dhruv Mahajan, Diego Garcia-Olano,\nDiego Perino, Dieuwke Hupkes, Egor Lakomkin,\nEhab AlBadawy, Elina Lobanova, Emily Dinan,\nEric Michael Smith, Filip Radenovic, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis\nAnderson, Graeme Nail, Gregoire Mialon, Guan\nPang, Guillem Cucurell, Hailey Nguyen, Hannah\nKorevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee,\nJan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jen-\nnifer Billock, Jenny Hong, Jenya Lee, Jeremy\nFu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\nWang, Jiecao Yu, Joanna Bitton, Joe Spisak,\nJongsoo Park, Joseph Rocca, Joshua Johnstun,\nJoshua Saxe, Junteng Jia, Kalyan Vasuden Al-\nwala, Kartikeya Upasani, Kate Plawiak, Ke Li,\nKenneth Heafield, Kevin Stone, Khalid El-Arini,\nKrithika Iyer, Kshitiz Malik, Kuenley Chiu, Ku-\nnal Bhalla, Lauren Rantala-Yeary, Laurens van der\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins,\nLouis Martin, Lovish Madaan, Lubo Malo, Lukas\nBlecher, Lukas Landzaat, Luke de Oliveira, Made-\nline Muzzi, Mahesh Pasupuleti, Mannat Singh,\nManohar Paluri, Marcin Kardas, Mathew Oldham,\nMathieu Rita, Maya Pavlova, Melanie Kambadur,\nMike Lewis, Min Si, Mitesh Kumar Singh, Mona\nHassan, Naman Goyal, Narjes Torabi, Nikolay\nBashlykov, Nikolay Bogoychev, Niladri Chatterji,\nOlivier Duchenne, Onur C¸elebi, Patrick Alrassy,\nPengchuan Zhang, Pengwei Li, Petar Vasic, Peter\nWeng, Prajjwal Bhargava, Pratik Dubal, Praveen\nKrishnan, Punit Singh Koura, Puxin Xu, Qing He,\nQingxiao Dong, Ragavan Srinivasan, Raj Gana-\npathy, Ramon Calderer, Ricardo Silveira Cabral,\nRobert Stojnic, Roberta Raileanu, Rohit Gird-\nhar, Rohit Patel, Romain Sauvestre, Ronnie Poli-\ndoro, Roshan Sumbaly, Ross Taylor, Ruan Silva,\nRui Hou, Rui Wang, Saghar Hosseini, Sahana\nChennabasappa, Sanjay Singh, Sean Bell, Seo-\nhyun Sonia Kim, Sergey Edunov, Shaoliang Nie,\n10\n"}, {"page": 11, "text": "Paradox of De-id\nSharan Narang, Sharath Raparthy, Sheng Shen,\nShengye Wan, Shruti Bhosale, Shun Zhang, Si-\nmon Vandenhende, Soumya Batra, Spencer Whit-\nman, Sten Sootla, Stephane Collot, Suchin Guru-\nrangan, Sydney Borodinsky, Tamar Herman, Tara\nFowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov,\nTong Xiao, Ujjwal Karn, Vedanuj Goswami, Vib-\nhor Gupta, Vignesh Ramanathan, Viktor Kerkez,\nVincent Gonguet, Virginie Do, Vish Vogeti, Vladan\nPetrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu,\nWhitney Meers, Xavier Martinet, Xiaodong Wang,\nXiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia,\nXuewei Wang, Yaelle Goldschlag, Yashesh Gaur,\nYasmine Babaei, Yi Wen, Yiwen Song, Yuchen\nZhang, Yue Li, Yuning Mao, Zacharie Delpierre\nCoudert, Zheng Yan, Zhengxing Chen, Zoe Pa-\npakipos, Aaditya Singh, Aaron Grattafiori, Abha\nJain, Adam Kelsey, Adam Shajnfeld, Adithya\nGangidi, Adolfo Victoria, Ahuva Goldstand, Ajay\nMenon,\nAjay Sharma,\nAlex Boesenberg,\nAlex\nVaughan, Alexei Baevski, Allie Feinstein, Amanda\nKallet, Amit Sangani, Anam Yunus, Andrei Lupu,\nAndres Alvarado, Andrew Caples, Andrew Gu, An-\ndrew Ho, Andrew Poulton, Andrew Ryan, Ankit\nRamchandani,\nAnnie Franco,\nAparajita Saraf,\nArkabandhu Chowdhury, Ashley Gabriel, Ash-\nwin Bharambe, Assaf Eisenman, Azadeh Yazdan,\nBeau James, Ben Maurer, Benjamin Leonhardi,\nBernie Huang, Beth Loyd, Beto De Paola, Bhar-\ngavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden\nHancock, Bram Wasti, Brandon Spence, Brani\nStojkovic, Brian Gamido, Britt Montalvo, Carl\nParker, Carly Burton, Catalina Mejia, Chang-\nhan Wang, Changkyu Kim, Chao Zhou, Chester\nHu, Ching-Hsiang Chu, Chris Cai, Chris Tin-\ndal, Christoph Feichtenhofer, Damon Civin, Dana\nBeaty, Daniel Kreymer, Daniel Li, Danny Wy-\natt, David Adkins, David Xu, Davide Testuggine,\nDelia David, Devi Parikh, Diana Liskovich, Didem\nFoss, Dingkang Wang, Duc Le, Dustin Holland,\nEdward Dowling, Eissa Jamil, Elaine Montgomery,\nEleonora Presani, Emily Hahn, Emily Wood, Erik\nBrinkman, Esteban Arcaute, Evan Dunbar, Evan\nSmothers, Fei Sun, Felix Kreuk, Feng Tian, Firat\nOzgenel, Francesco Caggioni, Francisco Guzm´an,\nFrank Kanayet, Frank Seide, Gabriela Medina\nFlorez, Gabriella Schwarz, Gada Badeer, Geor-\ngia Swee, Gil Halpern, Govind Thattai, Grant\nHerman, Grigory Sizov, Guangyi, Zhang, Guna\nLakshminarayanan, Hamid Shojanazeri, Han Zou,\nHannah Wang, Hanwen Zha, Haroun Habeeb,\nHarrison Rudolph, Helen Suk, Henry Aspegren,\nHunter Goldman, Ibrahim Damlaj, Igor Molybog,\nIgor Tufanov, Irina-Elena Veliche, Itai Gat, Jake\nWeissman, James Geboski, James Kohli, Japhet\nAsher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang,\nJennifer Chan, Jenny Zhen, Jeremy Reizenstein,\nJeremy Teboul, Jessica Zhong, Jian Jin, Jingyi\nYang, Joe Cummings, Jon Carvill, Jon Shep-\nard, Jonathan McPhie, Jonathan Torres, Josh\nGinsburg, Junjie Wang, Kai Wu, Kam Hou U,\nKaran Saxena, Karthik Prasad, Kartikay Khan-\ndelwal, Katayoun Zand, Kathy Matosich, Kaushik\nVeeraraghavan, Kelly Michelena, Keqian Li, Kun\nHuang, Kunal Chawla, Kushal Lakhotia, Kyle\nHuang, Lailin Chen, Lakshya Garg, Lavender\nA, Leandro Silva, Lee Bell, Lei Zhang, Liang-\npeng Guo, Licheng Yu, Liron Moshkovich, Luca\nWehrstedt, Madian Khabsa, Manav Avalani, Man-\nish Bhatt, Maria Tsimpoukelli, Martynas Mankus,\nMatan Hasson, Matthew Lennie, Matthias Reso,\nMaxim Groshev, Maxim Naumov, Maya Lathi,\nMeghan Keneally, Michael L Seltzer, Michal Valko,\nMichelle Restrepo, Mihir Patel, Mik Vyatskov,\nMikayel Samvelyan, Mike Clark, Mike Macey, Mike\nWang, Miquel Jubert Hermoso, Mo Metanat, Mo-\nhammad Rastegari, Munish Bansal, Nandhini San-\nthanam, Natascha Parks, Natasha White, Navy-\nata Bawa, Nayan Singhal, Nick Egebo, Nicolas\nUsunier, Nikolay Pavlovich Laptev, Ning Dong,\nNing Zhang, Norman Cheng, Oleg Chernoguz,\nOlivia Hart,\nOmkar Salpekar,\nOzlem Kalinli,\nParkin Kent, Parth Parekh, Paul Saab, Pavan Bal-\naji, Pedro Rittner, Philip Bontrager, Pierre Roux,\nPiotr Dollar, Polina Zvyagina, Prashant Ratan-\nchandani, Pritish Yuvraj, Qian Liang, Rachad\nAlao, Rachel Rodriguez, Rafi Ayub, Raghotham\nMurthy, Raghu Nayani, Rahul Mitra, Raymond\nLi, Rebekkah Hogan, Robin Battey, Rocky Wang,\nRohan Maheswari,\nRuss Howes,\nRuty Rinott,\nSai Jayesh Bondu, Samyak Datta, Sara Chugh,\nSara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru\nPan, Saurabh Verma, Seiji Yamamoto, Sharadh\nRamaswamy,\nShaun\nLindsay,\nShaun\nLindsay,\nSheng Feng, Shenghao Lin, Shengxin Cindy Zha,\nShiva Shankar, Shuqiang Zhang, Shuqiang Zhang,\nSinong Wang, Sneha Agarwal, Soji Sajuyigbe,\nSoumith Chintala, Stephanie Max, Stephen Chen,\nSteve Kehoe, Steve Satterfield, Sudarshan Govin-\ndaprasad, Sumit Gupta, Sungmin Cho, Sunny\nVirk, Suraj Subramanian, Sy Choudhury, Sydney\n11\n"}, {"page": 12, "text": "Paradox of De-id\nGoldman, Tal Remez, Tamar Glaser, Tamara Best,\nThilo Kohler, Thomas Robinson, Tianhe Li, Tian-\njun Zhang, Tim Matthews, Timothy Chou, Tzook\nShaked, Varun Vontimitta, Victoria Ajayi, Vic-\ntoria Montanez, Vijai Mohan, Vinay Satish Ku-\nmar, Vishal Mangla, V´ıtor Albiero, Vlad Ionescu,\nVlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir\nIvanov, Wei Li, Wenchen Wang, Wenwen Jiang,\nWes Bouaziz, Will Constable, Xiaocheng Tang, Xi-\naofang Wang, Xiaojian Wu, Xiaolan Wang, Xide\nXia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu,\nYe Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang,\nYossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao,\nYundi Qian, Yuzi He, Zach Rait, Zachary DeVito,\nZef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and\nZhiwei Zhao. The llama 3 herd of models. arXiv\n[cs.AI], July 2024.\nCynthia Dwork and Aaron Roth.\nThe algorithmic\nfoundations of differential privacy.\nFoundations\nand Trends in Theoretical Computer Science, 9\n(3–4):211–407, 2014. ISSN 1551-305X.\nMax Friedrich, Arne K¨ohn, Gregor Wiedemann, and\nChris Biemann.\nAdversarial learning of privacy-\npreserving text representations for de-identification\nof medical records. Jun 2019. URL http://arxi\nv.org/abs/1906.05000.\nKeren Fuentes, Mimee Xu, and Irene Chen. Privacy-\npreserving dataset combination. arXiv [cs.LG], Oc-\ntober 2025.\nGrand View Research. De-identified health data mar-\nket (2026 - 2033). https://www.grandviewresea\nrch.com/industry-analysis/de-identifie\nd-health-data-market-report, 2026. Accessed:\n2026-2-4.\nDirk Groeneveld,\nIz Beltagy,\nPete Walsh,\nAk-\nshita Bhagia, Rodney Kinney, Oyvind Tafjord,\nAnanya Harsh Jha, Hamish Ivison, Ian Magnus-\nson, Yizhong Wang, Shane Arora, David Atkin-\nson, Russell Authur, Khyathi Raghavi Chandu, Ar-\nman Cohan, Jennifer Dumas, Yanai Elazar, Yuling\nGu, Jack Hessel, Tushar Khot, William Merrill,\nJacob Morrison, Niklas Muennighoff, Aakanksha\nNaik, Crystal Nam, Matthew E Peters, Valentina\nPyatkin, Abhilasha Ravichander, Dustin Schwenk,\nSaurabh Shah, Will Smith, Emma Strubell, Nis-\nhant Subramani,\nMitchell Wortsman,\nPradeep\nDasigi, Nathan Lambert, Kyle Richardson, Luke\nZettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini,\nNoah A Smith, and Hannaneh Hajishirzi. OLMo:\nAccelerating the science of language models. arXiv\n[cs.CL], February 2024.\nAlistair E. W. Johnson, Tom J. Pollard, Lu Shen, Li-\nwei H. Lehman, Mengling Feng, Mohammad Ghas-\nsemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G. Mark.\nMIMIC-III, a\nfreely accessible critical care database. Scientific\nData, 3(160035), 2016. doi: https://doi.org/10.1\n038/sdata.2016.35.\nAlistair E. W. Johnson, Lucas Bulgarelli, Lu Shen,\nAlvin Gayles, Ayad Shammout, Steven Horng,\nTom J. Pollard, Sicheng Hao, Benjamin Moody,\nBrian Gow, Li-Wei H. Lehman, Leo A. Celi, and\nRoger G. Mark. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):\n1, Jan 2023. ISSN 2052-4463.\nHeather Landi. UCSF pays hackers $1.1M to regain\naccess to medical school servers. https://www.fi\nercehealthcare.com/tech/ucsf-pays-hackers\n-1-14m-to-regain-access-to-medical-schoo\nl-servers, September 2020. Accessed: 2024-9-10.\nNinghui Li.\nDifferentially private data synthesis:\nState of the art and challenges. ACM Asia Confer-\nence on Computer and Communications Security,\nMay 2022. doi: 10.1145/3488932.3522771. URL\nhttps://www.usenix.org/conference/usenix\nsecurity21/presentation/zhang-zhikun.\nZinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Har-\nsha Nori, and Sergey Yekhanin. Differentially pri-\nvate synthetic data via foundation model apis 1:\nImages. May 2023. URL http://arxiv.org/abs/\n2305.15560.\nIlya Loshchilov and Frank Hutter. Decoupled weight\ndecay regularization. Nov 2017. URL http://ar\nxiv.org/abs/1711.05101.\nMary Madden. Americans’ attitudes about privacy,\nsecurity and surveillance. https://www.pewresea\nrch.org/internet/2015/05/20/americans-att\nitudes-about-privacy-security-and-surveil\nlance/, May 2015. Accessed: 2026-1-22.\nJohn X. Morris, Justin T. Chiu, Ramin Zabih, and\nAlexander M. Rush. Unsupervised text deidentifi-\ncation. Oct 2022. URL http://arxiv.org/abs/\n2210.11528.\n12\n"}, {"page": 13, "text": "Paradox of De-id\nArvind Narayanan and Sayash Kapoor. AI safety is\nnot a model property. https://www.normaltech\n.ai/p/ai-safety-is-not-a-model-property,\nMarch 2024. Accessed: 2026-1-28.\nIshna Neamatullah, Margaret M Douglass, Li-Wei H\nLehman, Andrew Reisner, Mauricio Villarroel,\nWilliam J Long, Peter Szolovits, George B Moody,\nRoger G Mark, and Gari D Clifford. Automated\nde-identification of free-text medical records. BMC\nMed. Inform. Decis. Mak., 8:32, July 2008.\nJoseph Near. Differentially private synthetic data —\nnist. Joseph Near, May 2021. URL https://www.\nnist.gov/blogs/cybersecurity-insights/di\nfferentially-private-synthetic-data.\nHenrik Nolte, Mich`ele Finck, and Kristof Meding.\nMachine learners should acknowledge the legal im-\nplications of large language models as personal\ndata. arXiv [cs.LG], June 2025.\nBeau Norgeot, Kathleen Muenzen, Thomas A Pe-\nterson, Xuancheng Fan, Benjamin S Glicksberg,\nGundolf Schenk, Eugenia Rutenberg, Boris Os-\nkotsky, Marina Sirota, Jinoos Yazdany, Gabriela\nSchmajuk, Dana Ludwig, Theodore Goldstein, and\nAtul J Butte. Protected health information filter\n(philter):\naccurately and securely de-identifying\nfree-text clinical notes. NPJ Digit Med, 3:57, April\n2020.\nOffice for Civil Rights.\nHHS’ office for civil rights\nsettles malicious insider cybersecurity investigation\nfor $4.75 million. https://www.hhs.gov/about/\nnews/2024/02/06/hhs-office-civil-rights-s\nettles-malicious-insider-cybersecurity-i\nnvestigation.html, February 2024.\nAccessed:\n2024-9-10.\nOffice of Civil Rights. Summary of the hipaa privacy\nrule, 2022. URL https://www.hhs.gov/hipaa/\nfor-professionals/privacy/laws-regulatio\nns/index.html. Accessed: 2023-11-7.\nPaul Ohm. Broken promises of privacy: Responding\nto the surprising failure of anonymization. UCLA\nlaw review. University of California, Los Angeles.\nSchool of Law, 5:1701, Aug 2009. ISSN 0041-5650.\nOpenAI. Introducing ChatGPT health. https://op\nenai.com/index/introducing-chatgpt-healt\nh/, January 2026a. Accessed: 2026-2-4.\nOpenAI. Introducing OpenAI for healthcare. https:\n//openai.com/index/openai-for-healthcare/,\nJanuary 2026b. Accessed: 2026-2-4.\nMartin Scaiano, Grant Middleton, Luk Arbuckle,\nVarada Kolhatkar, Liam Peyton, Moira Dowling,\nDebbie S Gipson, and Khaled El Emam.\nA\nunified framework for evaluating the risk of re-\nidentification of text de-identification tools.\nJ.\nBiomed. Inform., 63:174–183, October 2016.\nChenkai Sun, Abolfazl Asudeh, H V Jagadish, Bill\nHowe, and Julia Stoyanovich. MithraLabel. In Pro-\nceedings of the 28th ACM International Conference\non Information and Knowledge Management, New\nYork, NY, USA, November 2019. ACM.\nLatanya Sweeney. Simple demographics often iden-\ntify people uniquely. Health, 671(2000):1–34, 2000.\nISSN 1949-4998.\nGreg Wiederrecht, Sasson Darwish, and Andrew\nCallaway.\nThe healthcare data explosion, 2020.\nURL https://www.rbccm.com/en/gib/healthc\nare/episode/the_healthcare_data_explosion.\nAccessed: 2023-11-7.\nXi Yang, Tianchen Lyu, Chih-Yin Lee, Jiang Bian,\nWilliam R. Hogan, and Yonghui Wu.\nA study\nof deep learning methods for de-identification of\nclinical notes at cross institute settings.\nIEEE\nInternational Conference on Healthcare Informat-\nics. IEEE International Conference on Healthcare\nInformatics, 2019, Jun 2019.\nISSN 2575-2626.\ndoi: 10.1109/ICHI.2019.8904544.\nURL http:\n//dx.doi.org/10.1109/ICHI.2019.8904544.\n13\n"}, {"page": 14, "text": "Paradox of De-id\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAUC (%)\nTarget: Biological sex\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAUC (%)\nTarget: Neighborhood\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAUC (%)\nTarget: Year\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAUC (%)\nTarget: Month\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAUC (%)\nTarget: Income\n1k\n10k\n100k\n178k\nTraining Samples\n0\n20\n40\n60\n80\n100\nAUC (%)\nTarget: Insurance\nSignal from 'De-identified' Notes\nPrediction based on Random Guess\nFigure 6: Attribute predictor’s AUC (red bars) is better than random guess (blue bars).\n14\n"}, {"page": 15, "text": "Paradox of De-id\nAppendix A. Above-random AUCs\nFigure 6 shows that similar to accuracy, the AUC of\neach attribute predictor is above random guess and\nimproves with more training data.\nAppendix B. Training Details\nFinetuning.\nTo prevent information leakage from\nclinical pre-training, we initialized our approach with\nbert-base-uncased (110M parameters), which is\npre-trained solely on general domain text (Devlin\net al., 2018). We fine-tuned a separate model for each\nattribute using eight NVIDIA A100 GPUs (40GB)\nor H100 GPUs (80GB) for up to 10 epochs, with\nearly stopping and a random seed of 0.\nWe used\nthe AdamW optimizer (Loshchilov and Hutter, 2017)\nwith a learning rate of 2 × 10−5, no weight decay, an\neffective batch size of 256, and a linear decay sched-\nule without warmup. Checkpoints were evaluated ev-\nery half-epoch, and the model achieving the highest\nweighted validation ROC-AUC was selected for infer-\nence. Code is available on Anonymous Github.\nAppendix C. Geographic Distribution\nof Health Outcomes\nFigures 7–10 show the geographic distribution of\nhealth outcomes and demographic attributes across\nNYC zip codes. These maps illustrate that both clin-\nical outcomes (mortality, length of stay, comorbidity)\nand demographic attributes (sex, income, age, insur-\nance type) vary substantially by neighborhood, re-\ninforcing the structural link between geography and\npatient identity exploited by the backdoor paths in\nFigure 3.\nMaps\nwere\ngenerated\nusing\ngeopandas\nand\nmatplotlib by joining per-zip-code aggregate statis-\ntics with an NYC zip code shapefile. Aggregates were\nprotected with differential privacy via the Laplace\nmechanism (ϵ = 1.0, split evenly across columns) and\nsmall-group suppression (zip codes with fewer than\n10 records were excluded). A quantile classification\nscheme with five bins was used for visualization.\n15\n"}, {"page": 16, "text": "Paradox of De-id\nIn-Hospital Mortality Rate\nMortality Rate\n0.00, 0.01\n0.01, 0.02\n0.02, 0.02\n0.02, 0.02\n0.02, 0.05\nNo data\nMean Length of Stay\nDays\n3.73, 5.48\n5.48, 5.75\n5.75, 6.20\n6.20, 6.78\n6.78, 8.71\nNo data\nFigure 7: NYC zip code heatmaps of in-hospital mortality rate (left) and mean length of stay (right).\nProportion Female\nProportion\n0.38, 0.50\n0.50, 0.52\n0.52, 0.55\n0.55, 0.59\n0.59, 0.69\nNo data\nPer Capita Income\nUSD\n 13543.00,  24752.00\n 24752.00,  29572.00\n 29572.00,  36764.00\n 36764.00,  63561.00\n 63561.00, 174922.00\nNo data\nFigure 8: NYC zip code heatmaps of proportion female (left) and per capita income (right).\n16\n"}, {"page": 17, "text": "Paradox of De-id\nMean Patient Age\nYears\n26.10, 42.18\n42.18, 46.74\n46.74, 50.65\n50.65, 55.27\n55.27, 83.58\nNo data\nGovernment Payer Rate\nRate\n0.06, 0.21\n0.21, 0.26\n0.26, 0.31\n0.31, 0.39\n0.39, 0.88\nNo data\nFigure 9: NYC zip code heatmaps of mean patient age (left) and government payer rate (right).\nMean Charlson Comorbidity Index\nCCI Score\n0.12, 0.57\n0.57, 0.67\n0.67, 0.76\n0.76, 0.83\n0.83, 1.05\nNo data\nFigure 10: NYC zip code heatmap of mean Charlson Comorbidity Index.\n17\n"}]}