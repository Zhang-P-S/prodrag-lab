{"doc_id": "arxiv:2511.13107", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.13107.pdf", "meta": {"doc_id": "arxiv:2511.13107", "source": "arxiv", "arxiv_id": "2511.13107", "title": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study", "authors": ["Zhichao He", "Mouxiao Bian", "Jianhong Zhu", "Jiayuan Chen", "Yunqiu Wang", "Wenxia Zhao", "Tianbin Li", "Bing Han", "Jie Xu", "Junyan Wu"], "published": "2025-11-17T08:05:15Z", "updated": "2025-11-17T08:05:15Z", "summary": "The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.13107v1", "url_pdf": "https://arxiv.org/pdf/2511.13107.pdf", "meta_path": "data/raw/arxiv/meta/2511.13107.json", "sha256": "1c9af2b8025f57f9cfaf71cad89d87c98c9e8ba26b335137efcdb782a3c12206", "status": "ok", "fetched_at": "2026-02-18T02:26:55.473461+00:00"}, "pages": [{"page": 1, "text": "EVALUATING THE ABILITY OF LARGE LANGUAGE MODELS TO\nIDENTIFY ADHERENCE TO CONSORT REPORTING GUIDELINES\nIN RANDOMIZED CONTROLLED TRIALS: A METHODOLOGICAL\nEVALUATION STUDY\nZhichao He1,†, Mouxiao Bian2,†, Jianhong Zhu1,†, Jiayuan Chen2, Yunqiu Wang1,3, Wenxia Zhao1, Tianbin Li2, Bing\nHan2, Jie Xu2,*, and Junyan Wu1,*\n1 SUN YAT-SEN MEMORIAL HOSPITAL , Guangdong, China\n2 Shanghai Artificial Intelligence Laboratory, Shanghai, China\n3 Imperial College London, London, United Kingdom\n4 University of Washington, Washington, USA\nABSTRACT\nBackground: The Consolidated Standards of Reporting Trials (CONSORT) statement is the global\nbenchmark for transparent and high-quality reporting of randomized controlled trials (RCTs). Manual\nverification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant\nbottleneck in peer review and evidence synthesis. While Large Language Models (LLMs) have\ndemonstrated transformative potential in natural language understanding, their proficiency in the\nnuanced, high-stakes task of methodological assessment remains unverified.\nObjective: This study aimed to systematically evaluate the accuracy and reliability of contemporary\nLLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a\nzero-shot setting.\nMethods: We constructed a gold-standard dataset of 150 published RCTs spanning diverse medical\nspecialties. Each article was independently assessed against 37 CONSORT sub-items by two trained\nmethodologists, with discrepancies resolved by a senior arbitrator to establish a consensus label\n(\"Compliant,\" \"Non-Compliant,\" or \"Not Applicable\"). A suite of 16 leading LLMs, including GPT,\nGemini, Claude, and Qwen series, were evaluated. The primary outcome was the macro-averaged\nF1-score for the three-class classification task, supplemented by item-wise performance metrics and\nqualitative error analysis.\nResults: Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and\nDeepSeek-R1, achieved nearly identical macro F1-scores of 0.634 and Cohen’s Kappa coefficients of\n0.280 and 0.282, respectively, indicating only \"fair\" agreement with expert consensus. A striking\nperformance disparity was observed across classes: while most models could identify \"Compliant\"\nitems with high accuracy (F1-score > 0.850), they struggled profoundly with identifying \"Non-\nCompliant\" and \"Not Applicable\" items, where F1-scores rarely exceeded 0.400. Notably, some\nhigh-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. A\nprimary systematic error was the misclassification of \"Not Reported\" as \"Not Applicable,\" particularly\nfor items concerning changes to trial methods or outcomes.\nConclusion: LLMs show potential as preliminary screening assistants for CONSORT checks, capably\nidentifying well-reported items. However, their current inability to reliably detect reporting omissions\nor methodological flaws (\"Non-Compliant\" items) makes them unsuitable for replacing human\nexpertise in the critical appraisal of trial quality. Their utility is presently confined to augmenting, not\nautomating, the work of researchers, editors, and systematic reviewers.\n1†These authors contributed equally.\n2*Correspondence: Junyan Wu(wujunyan@mail.sysu.edu.cn), Jie Xu (xujie@pjlab.org.cn)\narXiv:2511.13107v1  [cs.CL]  17 Nov 2025\n"}, {"page": 2, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nKeywords Benchmark · RCT · CONSORT · Large language Model\n1\nIntroduction\nRandomized controlled trials (RCTs) represent the pinnacle of study design for evaluating the efficacy and safety\nof health interventions, forming the bedrock of evidence-based medicine [1][2]. The validity of their conclusions\nand their utility in clinical decision-making and health policy are, however, critically dependent on the transparent,\ncomplete, and accurate reporting of their design, conduct, and analysis [3][4]. Deficiencies in reporting can obscure\nmethodological flaws, introduce bias, and impede the replication of research, ultimately leading to a waste of valuable\nresearch resources[5][6].\nTo address these pervasive issues, the Consolidated Standards of Reporting Trials (CONSORT) statement was developed.\nThis evidence-based set of recommendations, provides a 25-item checklist designed to guide authors in reporting RCTs,\nthereby improving their quality and transparency [7]. The endorsement of CONSORT by hundreds of leading medical\njournals has been associated with modest to significant improvements in reporting quality [8][9], yet adherence remains\nsuboptimal across the medical literature[10][11].\nManually verifying adherence to CONSORT guidelines is a standard but demanding component of the peer-review\nprocess for journals and a foundational step in conducting systematic reviews and meta-analyses[12][13]. This process\nis not only time-consuming and resource-intensive but also requires substantial methodological expertise and can\nbe subject to inter-rater variability[14][15]. The increasing volume of published research exacerbates this challenge,\ncreating a significant bottleneck in the evidence synthesis pipeline[16].\nThe recent advent of large language models (LLMs) such as OpenAI’s GPT series, Google’s Gemini, and Anthropic’s\nClaude has revolutionized the field of artificial intelligence[17][18][19]. Their sophisticated capabilities in under-\nstanding, summarizing, and reasoning over complex text have opened new frontiers for their application in medicine\nand scientific research [20]. From assisting in clinical documentation and drafting manuscripts to accelerating drug\ndiscovery, LLMs are poised to reshape the research landscape [21][22][23]. The potential to automate the assessment of\nscientific literature for methodological rigor is particularly tantalizing, promising to enhance efficiency and consistency\n[24][25].\nHowever, assessing CONSORT adherence transcends simple keyword searching or text summarization. It demands a\nnuanced understanding of complex epidemiological and biostatistical concepts such as allocation concealment, blinding,\nand intention-to-treat analysis[26][27]. It requires the ability to infer information, identify omissions, and critically\nevaluate the adequacy of reported details[28]. The extent to which current LLMs possess this specialized reasoning\ncapability, particularly without domain-specific fine-tuning (i.e., in a \"zero-shot\" context), is unknown.\nThis study, therefore, seeks to address a critical question: How accurately and reliably can state-of-the-art LLMs\nidentify adherence to CONSORT reporting guidelines in published RCTs compared to a gold standard of human expert\nconsensus? We hypothesized that LLMs would perform well on straightforward, descriptive items but would struggle\nwith items requiring deep methodological inference. By establishing a rigorous benchmark—RCTBench, this research\naims to provide comprehensive evidence on the capabilities and limitations of LLMs for this critical task, informing\ntheir potential role as an assistive tool for researchers, peer reviewers, and systematic review teams.\n2\nMethods\n2.1\nStudy Design\nThis was a methodological evaluation study designed to benchmark the performance of multiple LLMs against a\nhuman-expert-derived gold standard for the task of CONSORT 2010 guideline adherence checking.\n2.2\nGold Standard Dataset Construction\nA dataset of 150 full-text articles of RCTs published between 2020 and 2025 was compiled. The articles were sourced\nfrom PubMed and the Cochrane Central Register of Controlled Trials (CENTRAL), ensuring a diverse sample across\nvarious medical and surgical specialties, journal impact factors, trial designs (e.g., parallel, crossover), and sample sizes.\nA rigorous annotation process was implemented to create the gold standard. Ten methodologists with postgraduate\ntraining in epidemiology or biostatistics served as expert annotators. Each article was independently assessed by two\nannotators against the 37 sub-items of the CONSORT 2010 checklist. For each item, a three-way classification was\nmade: \"Compliant\" (the item was adequately reported), \"Non-Compliant\" (the item was inadequately reported or\n2\n"}, {"page": 3, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nomitted), or \"Not Applicable\" (the item was not relevant to the specific trial design, e.g., Item 11 on blinding in an\nopen-label trial). All discrepancies between the two primary annotators were reviewed by a third, senior methodologist\nwho arbitrated a final consensus label. For each \"Non-Compliant\" or \"Not Applicable\" judgment, annotators provided a\nconcise, standardized justification and answer point.\n2.3\nLarge Language Models and Evaluation Task\nWe evaluated a panel of 16 prominent LLMs representing the state-of-the-art as of early 2024. This included models\nfrom OpenAI (GPT-4o, GPT-5 series), Google (Gemini series), Anthropic (Claude series), Alibaba (Qwen series),\nMeta (Llama-4 series), DeepSeek AI, and Mistral AI. The evaluation was conducted in a zero-shot setting to assess the\nmodels’ intrinsic capabilities without task-specific examples. A detailed prompt was engineered to instruct the models\nto act as an evidence-based medicine expert. The prompt required the models to read the full text of an RCT and output\na structured JSON object containing two arrays: one for \"non_compliant_items\" and one for \"not_applicable_items.\"\nEach entry in these arrays was required to include the CONSORT item number and a brief reason for the classification.\nCompliant items were to be omitted from the output for brevity and efficiency. This structured output format facilitated\nautomated parsing and scoring, prompt can be found in appendix 2.\n2.4\nOutcome Measures and Statistical Analysis\nThe primary outcome measure was the macro-averaged F1-score (Macro-F1), which computes the F1-score for each\nclass (\"Compliant,\" \"Non-Compliant,\" \"Not Applicable\") independently and then averages them, giving equal weight to\neach class..\nF1 = 2 × Precision × Recall\nPrecision + Recall\n(1)\nWhere:\n- Precision represents the proportion of samples predicted as positive that are actually positive(Equation 2):\nPrecision =\nTP\nTP + FP\n(2)\n- Recall represents the proportion of actual positive samples that are correctly predicted as positive(Equation 3):\nRecall =\nTP\nTP + FN\n(3)\nIn the above formulas, TP stands for True Positives, FP stands for False Positives, and FN stands for False Negatives.\nThis metric is robust to class imbalance. Macro-averaged precision and recall were also calculated. Cohen’s Kappa\ncoefficient was used to measure the agreement between model predictions and the gold standard, accounting for chance\nagreement. Secondary outcomes included item-wise F1-scores and class-specific performance metrics (precision,\nrecall, F1) to identify specific areas of model strength and weakness. A qualitative error analysis was performed by\ncategorizing the justifications provided by models for their incorrect predictions, particularly focusing on systematic\nerror patterns. All statistical analyses were descriptive, with results presented in tables and figures.\n3\nResults\n3.1\nGold Standard Dataset Characteristics\nRCTBench consists of 150 RCTs comprising 5,550 assessable CONSORT items and contains articles from 83\njournals, 38 disciplines (Figure 1) . Expert consensus classified 3,868 (69.7%) as \"Compliant,\" 847 (15.0%) as\n\"Non-Compliant,\" and 835 (15.3%) as \"Not Applicable.\" This distribution highlights the prevalence of reporting\ndeficiencies and the necessity of handling \"Not Applicable\" cases in real-world scenarios. Items with the highest\nnon-compliance rates were Item 10 (randomization implementation; 87.3%), Item 9 (allocation concealment; 62.0%),\nand Item 14a (recruitment dates; 56.0%), indicating these are common areas of poor reporting.\n3.2\nOverall Model Performance\nThe performance of the 16 LLMs was modest and highly variable, with a clear distinction between identifying\ncompliance and detecting non-compliance. As shown in Table 1, Gemini-2.5-Flash and DeepSeek-R1 were the top\nperformers, achieving nearly identical macro F1-scores of 0.634. Their Cohen’s Kappa values of 0.280 and 0.282,\nrespectively, indicate only \"fair\" agreement with the gold standard.\n3\n"}, {"page": 4, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nFigure 1: Characteristics of RCTBench\nTable 1: Overall Performance of Selected Large Language Models on CONSORT Adherence Classification\nModel Name\nMacro Precision\nMacro Recall\nMacro F1-Score\nCohen’s Kappa\nGemini-2.5-Flash\n0.699\n0.496\n0.634\n0.280\nDeepSeek-R1\n0.700\n0.496\n0.634\n0.282\nGPT-5mini\n0.714\n0.514\n0.633\n0.277\nGemini-2.5-Pro\n0.670\n0.470\n0.630\n0.279\nGPT-5\n0.677\n0.553\n0.617\n0.262\ngpt-oss-20b\n0.691\n0.485\n0.613\n0.251\nClaude Sonnet4-20250514\n0.645\n0.423\n0.581\n0.172\nDeepSeek-V3.1\n0.588\n0.441\n0.577\n0.166\nDeepSeek-V3\n0.590\n0.423\n0.564\n0.135\nQwen3_32b\n0.585\n0.399\n0.557\n0.122\nMistral-Small-3.1-24B-Instruct-2503\n0.559\n0.423\n0.553\n0.121\nQwen3-235B-A22B\n0.531\n0.408\n0.542\n0.097\nGPT-4o\n0.549\n0.400\n0.521\n0.097\nLlama-4-Scout-17B-16E\n0.459\n0.354\n0.482\n-0.016\nQwen2.5-72B-instruct\n0.558\n0.376\n0.476\n0.019\nLlama-4-Maverick-17B-128E-Instruct\n0.520\n0.377\n0.473\n-0.009\nA striking finding was the underperformance of several highly anticipated models. GPT-4o, for example, achieved\na macro F1-score of only 0.521 and a Kappa of 0.097, indicating slight agreement.\nModels like Llama-4-\nMaverick and Llama-4-Scout performed at or below the level of chance, with negative Kappa coefficients.\n3.3\nClass-Specific Performance and Item-wise Analysis\nThe modest macro-F1 scores obscure a critical performance dichotomy between classes (Figure 2). All models were\nproficient at identifying \"Compliant\" items. Top models like Gemini-2.5-Flash and GPT-5mini achieved F1-scores\nof 0.887 and 0.881 for this class, respectively, driven by high recall (>0.900 for many). This suggests models can\neffectively recognize when information is present.\nHowever, performance collapsed when attempting to identify \"Non-Compliant\" or \"Not Applicable\" items. For the\ncrucial \"Non-Compliant\" class, the highest F1-score was only 0.385 (GPT-5mini), with most models scoring well below\n0.350. The F1-score for the \"Not Applicable\" class was similarly poor, with the best model (GPT-5) reaching 0.579\nand many falling below 0.400. GPT-4o, for instance, scored a mere 0.166 F1 for \"Non-Compliant\" and 0.244 for \"Not\nApplicable,\" indicating it missed the vast majority of these cases.\nThis pattern explains why models with very high overall accuracy (e.g., Gemini-2.5-Flash at 80.6%) still have low\nKappa and Macro F1 scores: they default to predicting the majority class (\"Compliant\"), thereby failing at the primary\ntask of flagging deficiencies.\n4\n"}, {"page": 5, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nFigure 2: Performance Dichotomy (F1-Score) by Class for Top and Notable Models\n3.4\nQualitative Error Analysis\nQualitative analysis of errors provided a clear explanation for the poor quantitative results. The most frequent\nsystematic error was the conflation of \"information not reported\" with \"not applicable.\" For items requiring a statement\nabout whether a procedure occurred (e.g., Item 3b: changes to methods; Item 7b: interim analyses; Item 14b: trial\ndiscontinuation), models consistently provided justifications like, \"The article does not mention any important changes\nto the methods,\" to support an incorrect \"Not Applicable\" classification. The correct judgment, according to CONSORT,\nis \"Non-Compliant\" because the omission itself is a reporting failure.\nAnother major error involved a shallow interpretation of methodology. Models frequently cited text describing random\nsequence generation (Item 8a) as evidence for adequate allocation concealment (Item 9), failing to distinguish these\ndistinct concepts. This indicates a reliance on keyword proximity rather than a procedural understanding of bias\nmitigation techniques.\n4\nDiscussion\nThis comprehensive evaluation reveals that while LLMs possess some capability for automating CONSORT checks,\ntheir performance is modest and their limitations are profound. Our central finding is a stark performance dichotomy:\nmodels are adept at confirming the presence of reported information but are largely incapable of reliably identifying\nreporting omissions or methodological flaws. The overall performance, with the best models achieving only \"fair\"\n5\n"}, {"page": 6, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nagreement (Kappa ≈0.28), falls far short of the threshold required for autonomous use in high-stakes scientific\nappraisal.\nThe results challenge the optimistic assumption that larger or more recent models are universally better for specialized\ntasks. The surprising underperformance of a prominent model like GPT-4o (Macro F1 = 0.521) compared to Gemini-\n2.5-Flash (Macro F1 = 0.634) highlights that model architecture, training data composition, and fine-tuning for\ninstruction-following can lead to idiosyncratic strengths and weaknesses. GPT-4o’s very low recall for non-compliant\nand not-applicable items suggests it may be \"over-calibrated\" to avoid making definitive negative claims in the absence\nof explicit negative evidence, a behavior that is detrimental in a prescriptive checking task.\nThe near-total failure to identify \"Non-Compliant\" items (F1 < 0.400) is the most critical limitation uncovered. The\nprimary value of a CONSORT checking tool is not to confirm what is present, but to flag what is missing or inadequately\nreported, as these omissions are central to assessing risk of bias[26] [29]. The models’ tendency to default to the\n\"Compliant\" class makes them unreliable for this core function. This aligns with findings from other domains showing\nthat LLMs struggle with tasks requiring \"negative evidence\" or reasoning about omissions[30][31].\nOur qualitative analysis pinpoints the root cause: models lack a deep, normative understanding of reporting guidelines.\nThey operate on a descriptive logic (\"what is in the text?\") rather than a prescriptive one (\"what should be in the\ntext according to the standard?\"). The recurring error of misclassifying \"not reported\" as \"not applicable\" is a direct\nconsequence of this logical gap [2]. This suggests that simply scaling up existing model architectures may be insufficient;\nnew training paradigms that instill normative reasoning are needed[31][32].\nDespite these limitations, our findings do not dismiss the potential of LLMs. Their high recall for compliant items\nsuggests they can be used as effective \"rule-out\" or preliminary screening tools. A workflow where an LLM first\nperforms a rapid scan to identify and clear obviously well-reported items could allow human experts to focus their\nlimited time on the more ambiguous and methodologically complex aspects of a paper [15][33]. This human-in-the-loop\nmodel appears to be the most viable application for the current generation of LLMs [34].\nThis study has several strengths, including its rigorous gold-standard, comprehensive model testing, and detailed error\nanalysis. Limitations include the zero-shot setting, which may understate the potential of fine-tuned or few-shot models\n[35][36]; the exclusion of information from figures and tables, which future multimodal models may address [37][38];\nand the rapidly evolving nature of LLM technology [39].\n5\nConclusion\nLarge language models, in their current state, are not reliable autonomous agents for assessing CONSORT reporting\nguideline adherence. While they can efficiently identify adequately reported information, their profound inability to\ndetect reporting omissions or methodological flaws renders them unsafe for standalone use in critical appraisal. Their\nimmediate value lies in a more modest role: as assistive tools in a human-supervised workflow, helping to streamline the\ninitial, more tedious aspects of the review process. The significant gap between human expert performance and current\nAI capabilities underscores the enduring necessity of deep methodological expertise in safeguarding the integrity of\nclinical evidence.\n6\nData Availability Statement\nThe dataset supporting the findings of this study has been made publicly available through the MedBench repository. It\ncan be accessed at https://medbench.opencompass.org.cn/home.\nReferences\n[1] G. Guyatt, J. Cairns, D. Churchill, D. Cook, B. Haynes, J. Hirsh, J. Irvine, M. Levine, M. Levine, J. Nishikawa\net al., “Evidence-based medicine: a new approach to teaching the practice of medicine,” jama, vol. 268, no. 17, pp.\n2420–2425, 1992.\n[2] D. Moher, S. Hopewell, K. F. Schulz, V. Montori, P. C. Gøtzsche, P. J. Devereaux, D. Elbourne, M. Egger,\nand D. G. Altman, “Consort 2010 explanation and elaboration: updated guidelines for reporting parallel group\nrandomised trials,” Bmj, vol. 340, 2010.\n[3] J. P. Ioannidis, “Why most published research findings are false,” PLoS medicine, vol. 2, no. 8, p. e124, 2005.\n[4] I. Chalmers and P. Glasziou, “Avoidable waste in the production and reporting of research evidence,” The Lancet,\nvol. 374, no. 9683, pp. 86–89, 2009.\n6\n"}, {"page": 7, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\n[5] P. Glasziou, D. G. Altman, P. Bossuyt, I. Boutron, M. Clarke, S. Julious, S. Michie, D. Moher, and E. Wager,\n“Reducing waste from incomplete or unusable reports of biomedical research,” The Lancet, vol. 383, no. 9913, pp.\n267–276, 2014.\n[6] D. Moher, P. Glasziou, I. Chalmers, M. Nasser, P. M. Bossuyt, D. A. Korevaar, I. D. Graham, P. Ravaud, and\nI. Boutron, “Increasing value and reducing waste in biomedical research: who’s listening?” The Lancet, vol. 387,\nno. 10027, pp. 1573–1586, 2016.\n[7] K. F. Schulz, D. G. Altman, D. Moher, C. Group et al., “Consort 2010 statement: updated guidelines for reporting\nparallel group randomised trials,” Journal of clinical epidemiology, vol. 63, no. 8, pp. 834–840, 2010.\n[8] L. Turner, L. Shamseer, D. G. Altman, L. Weeks, J. Peters, T. Kober, S. Dias, K. F. Schulz, A. C. Plint, and\nD. Moher, “Consolidated standards of reporting trials (consort) and the completeness of reporting of randomised\ncontrolled trials (rcts) published in medical journals,” Cochrane database of systematic reviews, no. 11, 2012.\n[9] L. Turner, L. Shamseer, D. G. Altman, K. F. Schulz, and D. Moher, “Does use of the consort statement impact the\ncompleteness of reporting of randomised controlled trials published in medical journals? a cochrane reviewa,”\nSystematic reviews, vol. 1, no. 1, p. 60, 2012.\n[10] V. Chhapola, S. Tiwari, R. Brar, and S. Kanwal, “Reporting quality of trial abstracts-improved yet suboptimal: A\nsystematic review and meta-analysis. j evid based med. 2018; 11 (2): 89-94.”\n[11] C. Han, K.-p. Kwak, D. M. Marks, C.-U. Pae, L.-T. Wu, K. S. Bhatia, P. S. Masand, and A. A. Patkar, “The impact\nof the consort statement on reporting of randomized clinical trials in psychiatry,” Contemporary Clinical Trials,\nvol. 30, no. 2, pp. 116–122, 2009.\n[12] M. J. Page, J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C. Hoffmann, C. D. Mulrow, L. Shamseer, J. M. Tetzlaff,\nE. A. Akl, S. E. Brennan et al., “The prisma 2020 statement: an updated guideline for reporting systematic\nreviews,” bmj, vol. 372, 2021.\n[13] M. Cumpston, T. Li, M. J. Page, J. Chandler, V. A. Welch, J. P. Higgins, and J. Thomas, “Updated guidance for\ntrusted systematic reviews: a new edition of the cochrane handbook for systematic reviews of interventions,” The\nCochrane database of systematic reviews, vol. 2019, no. 10, p. ED000142, 2019.\n[14] S. Chan and M. Bhandari, “The quality of reporting of orthopaedic randomized trials with use of a checklist for\nnonpharmacological therapies.” journal of bone & joint surgery american volume, vol. 89, no. 9, pp. 1970–8,\n2007.\n[15] G. L. Clayton, H. E. Jones, I. Boutron, D. L. T. Laursen, M. F. Olsen, P. Ravaud, J. Savovi, J. A. C. Sterne,\nA. Hróbjartsson, and L. Jrgensen, “Impact of blinding on estimated treatment effects in randomised clinical trials:\nmeta-epidemiological study,” RMD Open, vol. 368, pp. –, 2020.\n[16] H. Bastian, P. Glasziou, and I. Chalmers, “Seventy-five trials and eleven systematic reviews a day: How will we\never keep up?” Plos Medicine, vol. 7, no. 9, p. e1000326, 2010.\n[17] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell et al., “Language models are few-shot learners,” Advances in neural information processing systems,\nvol. 33, pp. 1877–1901, 2020.\n[18] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican\net al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\n[19] A. L. Martínez, A. Cano, and A. Ruiz-Martínez, “Generative artificial intelligence-supported pentesting: a\ncomparison between claude opus, gpt-4, and copilot,” arXiv preprint arXiv:2501.06963, 2025.\n[20] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting, “Large language\nmodels in medicine,” Nature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[21] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl\net al., “Large language models encode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–180, 2023.\n[22] A. Madani, B. Krause, E. R. Greene, S. Subramanian, B. P. Mohr, J. M. Holton, J. L. Olmos Jr, C. Xiong, Z. Z.\nSun, R. Socher et al., “Large language models generate functional protein sequences across diverse families,”\nNature biotechnology, vol. 41, no. 8, pp. 1099–1106, 2023.\n[23] C. Galli, A. V. Gavrilova, and E. Calciolari, “Large language models in systematic review screening: Opportunities,\nchallenges, and methodological considerations,” Information, vol. 16, no. 5, p. 378, 2025.\n[24] L. Yan, L. Sha, L. Zhao, Y. Li, R. Martinez-Maldonado, G. Chen, X. Li, Y. Jin, and D. Gaševi´c, “Practical\nand ethical challenges of large language models in education: A systematic scoping review,” British Journal of\nEducational Technology, vol. 55, no. 1, pp. 90–112, 2024.\n7\n"}, {"page": 8, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\n[25] C. Lin and C.-F. Kuo, “Roles and potential of large language models in healthcare: a comprehensive review,”\nBiomedical Journal, p. 100868, 2025.\n[26] D. G. Altman, K. F. Schulz, D. Moher, M. Egger, F. Davidoff, D. Elbourne, P. C. Gøtzsche, T. Lang, and C. Group,\n“The revised consort statement for reporting randomized trials: explanation and elaboration,” Annals of internal\nmedicine, vol. 134, no. 8, pp. 663–694, 2001.\n[27] K. F. Schulz, I. Chalmers, R. J. Hayes, and D. G. Altman, “Empirical evidence of bias: dimensions of methodolog-\nical quality associated with estimates of treatment effects in controlled trials,” Jama, vol. 273, no. 5, pp. 408–412,\n1995.\n[28] J. P. Higgins, D. G. Altman, P. C. Gøtzsche, P. Jüni, D. Moher, A. D. Oxman, J. Savovi´c, K. F. Schulz, L. Weeks,\nand J. A. Sterne, “The cochrane collaboration’s tool for assessing risk of bias in randomised trials,” bmj, vol. 343,\n2011.\n[29] L. Wood, M. Egger, L. L. Gluud, K. F. Schulz, P. Jüni, D. G. Altman, C. Gluud, R. M. Martin, A. J. Wood, and J. A.\nSterne, “Empirical evidence of bias in treatment effect estimates in controlled trials with different interventions\nand outcomes: meta-epidemiological study,” bmj, vol. 336, no. 7644, pp. 601–605, 2008.\n[30] R. Bommasani, “On the opportunities and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.\n[31] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston, “Chain-of-verification\nreduces hallucination in large language models,” arXiv preprint arXiv:2309.11495, 2023.\n[32] B. Shneiderman, “Human-centered artificial intelligence: Reliable, safe & trustworthy,” International Journal of\nHuman–Computer Interaction, vol. 36, no. 6, pp. 495–504, 2020.\n[33] S. Spillias, P. Tuohy, M. Andreotta, R. Annand-Jones, F. Boschetti, C. Cvitanovic, J. Duggan, E. A. Fulton,\nD. B. Karcher, C. Paris et al., “Human-ai collaboration to identify literature for evidence synthesis,” Cell Reports\nSustainability, vol. 1, no. 7, 2024.\n[34] S. Amershi, D. Weld, M. Vorvoreanu, A. Fourney, B. Nushi, P. Collisson, J. Suh, S. Iqbal, P. N. Bennett, K. Inkpen\net al., “Guidelines for human-ai interaction,” in Proceedings of the 2019 chi conference on human factors in\ncomputing systems, 2019, pp. 1–13.\n[35] S. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith, “Don’t stop\npretraining: Adapt language models to domains and tasks,” arXiv preprint arXiv:2004.10964, 2020.\n[36] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray\net al., “Training language models to follow instructions with human feedback,” Advances in neural information\nprocessing systems, vol. 35, pp. 27 730–27 744, 2022.\n[37] S. Hopewell, A. Hirst, G. S. Collins, S. Mallett, L.-M. Yu, and D. G. Altman, “Reporting of participant flow\ndiagrams in published reports of randomized trials,” Trials, vol. 12, no. 1, p. 253, 2011.\n[38] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen image\nencoders and large language models,” in International conference on machine learning.\nPMLR, 2023, pp.\n19 730–19 742.\n[39] M. Shamsabadi, J. D’Souza, and S. Auer, “Large language models for scientific information extraction: An\nempirical study for virology,” arXiv preprint arXiv:2401.10040, 2024.\n7\nAppendix\n7.1\nCONSORT 2010 Checklist\nSection/Topic\nItem No\nChecklist item\nTitle and abstract\n1a\nIdentification as a randomised trial in the title\n1b\nStructured summary of trial design, methods, re-\nsults, and conclusions (for specific guidance see\nCONSORT for abstracts)\nIntroduction: Background and\nobjectives\n2a\nScientific background and explanation of rationale\n2b\nSpecific objectives or hypotheses\n8\n"}, {"page": 9, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nSection/Topic\nItem No\nChecklist item\nMethods: Trial design\n3a\nDescription of trial design (such as parallel, facto-\nrial) including allocation ratio\n3b\nImportant changes to methods after trial com-\nmencement (such as eligibility criteria), with rea-\nsons\nMethods: Participants\n4a\nEligibility criteria for participants\n4b\nSettings and locations where the data were col-\nlected\nMethods: Interventions\n5\nThe interventions for each group with sufficient de-\ntails to allow replication, including how and when\nthey were actually administered\nMethods: Outcomes\n6a\nCompletely defined pre-specified primary and sec-\nondary outcome measures, including how and\nwhen they were assessed\n6b\nAny changes to trial outcomes after the trial com-\nmenced, with reasons\nMethods: Sample size\n7a\nHow sample size was determined\n7b\nWhen applicable, explanation of any interim anal-\nyses and stopping guidelines\nMethods: Randomisation:\nSequence generation\n8a\nMethod used to generate the random allocation\nsequence\n8b\nType of randomisation; details of any restriction\n(such as blocking and block size)\nMethods: Allocation\nconcealment mechanism\n9\nMechanism used to implement the random allo-\ncation sequence (such as sequentially numbered\ncontainers), describing any steps taken to conceal\nthe sequence until interventions were assigned\nMethods: Implementation\n10\nWho generated the random allocation sequence,\nwho enrolled participants, and who assigned par-\nticipants to interventions\nMethods: Blinding\n11a\nIf done, who was blinded after assignment to inter-\nventions (for example, participants, care providers,\nthose assessing outcomes) and how\n11b\nIf relevant, description of the similarity of interven-\ntions\nMethods: Statistical methods\n12a\nStatistical methods used to compare groups for\nprimary and secondary outcomes\n12b\nMethods for additional analyses, such as subgroup\nanalyses and adjusted analyses\nResults: Participant flow (a\ndiagram is strongly\nrecommended)\n13a\nFor each group, the numbers of participants who\nwere randomly assigned, received intended treat-\nment, and were analysed for the primary outcome\n13b\nFor each group, losses and exclusions after ran-\ndomisation, together with reasons\nResults: Recruitment\n14a\nDates defining the periods of recruitment and\nfollow-up\n14b\nWhy the trial ended or was stopped\nResults: Baseline data\n15\nA table showing baseline demographic and clinical\ncharacteristics for each group\n9\n"}, {"page": 10, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\nSection/Topic\nItem No\nChecklist item\nResults: Numbers analysed\n16\nFor each group, number of participants (denomi-\nnator) included in each analysis and whether the\nanalysis was by original assigned groups\nResults: Outcomes and\nestimation\n17a\nFor each primary and secondary outcome, results\nfor each group, and the estimated effect size and\nits precision (such as 95% confidence interval)\n17b\nFor binary outcomes, presentation of both absolute\nand relative effect sizes is recommended\nResults: Ancillary analyses\n18\nResults of any other analyses performed, including\nsubgroup analyses and adjusted analyses, distin-\nguishing pre-specified from exploratory\nResults: Harms\n19\nAll important harms or unintended effects in each\ngroup (for specific guidance see CONSORT for\nharms)\nDiscussion: Limitations\n20\nTrial limitations, addressing sources of potential\nbias, imprecision, and, if relevant, multiplicity of\nanalyses\nDiscussion: Generalisability\n21\nGeneralisability (external validity, applicability) of\nthe trial findings\nDiscussion: Interpretation\n22\nInterpretation consistent with results, balancing\nbenefits and harms, and considering other relevant\nevidence\nOther information: Registration\n23\nRegistration number and name of trial registry\nOther information: Protocol\n24\nWhere the full trial protocol can be accessed, if\navailable\nOther information: Funding\n25\nSources of funding and other support (such as sup-\nply of drugs), role of funders\n7.2\nPrompt designed for RCTBench\nAct as a rigorous systematic review and evidence-based medicine expert, and evaluate the provided clinical randomized\ncontrolled trial (RCT) literature strictly in accordance with the CONSORT Statement.\nYour tasks:\n1. Carefully read the full text of the literature provided by the user.\n2. Verify each of the following CONSORT checklist items one by one.\n3. For each item, make one of the three judgments below:\n– Compliant: The literature clearly reports all requirements of the item.\n– Non-Compliant: The literature does not report or reports incompletely.\n– Not Applicable: The item is not applicable to this study (e.g., items 11a and 11b are \"Not Applicable\" in an\nopen-label trial without blinding).\n4. You need to output a structured JSON object containing two keys:\nnon_compliant_items and\nnot_applicable_items.\n5. For items judged as \"Non-Compliant\", fill their numbers and reasons into the non_compliant_items array.\n6. For items judged as \"Not Applicable\", fill their numbers and brief reasons into the not_applicable_items\narray.\n7. Do not output items judged as \"Compliant\".\nOutput format requirements:\nThe output must be pure, directly parsable JSON without any additional Markdown\nformatting or explanatory text..The JSON object structure must strictly follow the example below:\n10\n"}, {"page": 11, "text": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in\nRandomized Controlled Trials\n{\n\"non_compliant_items\": [\n{\"item_number\": \"3b\", \"reason\": \"Failed to state any modifications to eligibility\ncriteria after the trial started and the reasons for such modifications\"},\n{\"item_number\": \"9\", \"reason\": \"Failed to describe the mechanism of allocation\nconcealment (e.g., type of container used) and implementation steps\"}\n],\n\"not_applicable_items\": [\n{\"item_number\": \"11b\", \"reason\": \"This trial adopted an open-label design\nwithout blinding, so there is no need to describe the similarity of interventions.\"}\n]\n}\n- If an array is empty (i.e., no \"Non-Compliant\" or \"Not Applicable\" items), the array shall be an empty array [].\nCONSORT Checklist:\n...(CONSORT 2010 Checklist,table 7.1)...\n11\n"}]}