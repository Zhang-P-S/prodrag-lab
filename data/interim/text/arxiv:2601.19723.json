{"doc_id": "arxiv:2601.19723", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.19723.pdf", "meta": {"doc_id": "arxiv:2601.19723", "source": "arxiv", "arxiv_id": "2601.19723", "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes", "authors": ["Yifan Wang", "Jichen Zheng", "Jingyuan Sun", "Yunhao Zhang", "Chunyu Ye", "Jixing Li", "Chengqing Zong", "Shaonan Wang"], "published": "2026-01-27T15:47:22Z", "updated": "2026-01-27T15:47:22Z", "summary": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.19723v1", "url_pdf": "https://arxiv.org/pdf/2601.19723.pdf", "meta_path": "data/raw/arxiv/meta/2601.19723.json", "sha256": "87b8d9a1775a4bbe5b9a0b011f56fb8f3fd4e82a089f241fc85c6434fad53490", "status": "ok", "fetched_at": "2026-02-18T02:20:19.521289+00:00"}, "pages": [{"page": 1, "text": "Component-Level Lesioning of Language Models Reveals Clinically Aligned\nAphasia Phenotypes\nYifan Wang1∗, Jichen Zheng2,3∗, Jingyuan Sun1∗, Yunhao Zhang2,3 , Chunyu Ye2,3 , Jixing\nLi4 , Chengqing Zong2,3 , Shaonan Wang5\n1Department of Computer Science, The University of Manchester, UK\n2State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS,\nBeijing, China\n3School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n4Department of Linguistics and Translation, City University of Hong Kong, Hong Kong\n5Department of Language Science and Technology, Hong Kong Polytechnic University, Hong Kong\nyifan.wang-38@postgrad.manchester.ac.uk, zhengjichen2023@ia.ac.cn,\njingyuan.sun@manchester.ac.uk, {zhangyunhao2021, yechunyu2023}@ia.ac.cn, jixingli@cityu.edu.hk,\ncqzong@nlpr.ia.ac.cn, shaonan.wang@polyu.edu.hk\nAbstract\nLarge language models (LLMs) increasingly ex-\nhibit human-like linguistic behaviors and internal\nrepresentations that they could serve as compu-\ntational simulators of language cognition.\nWe\nask whether LLMs can be systematically manip-\nulated to reproduce language-production impair-\nments characteristic of aphasia following focal\nbrain lesions.\nSuch models could provide scal-\nable proxies for testing rehabilitation hypotheses,\nand offer a controlled framework for probing the\nfunctional organization of language.\nWe intro-\nduce a clinically grounded, component-level frame-\nwork that simulates aphasia by selectively per-\nturbing functional components in LLMs, and ap-\nply it to both modular Mixture-of-Experts mod-\nels and dense Transformers using a unified in-\ntervention interface.\nOur pipeline (i) identi-\nfies subtype-linked components for Broca’s and\nWernicke’s aphasia, (ii) interprets these compo-\nnents with linguistic probing tasks, and (iii) in-\nduces graded impairments by progressively per-\nturbing the top-k subtype-linked components, eval-\nuating outcomes with Western Aphasia Battery\n(WAB) subtests summarized by Aphasia Quotient\n(AQ). Across architectures and lesioning strategies,\nsubtype-targeted perturbations yield more system-\natic, aphasia-like regressions than size-matched\nrandom perturbations, and MoE modularity sup-\nports more localized and interpretable phenotype-\nto-component mappings.\nThese findings sug-\ngest that modular LLMs, combined with clini-\ncally informed component perturbations, provide\na promising platform for simulating aphasic lan-\nguage production and studying how distinct lan-\nguage functions degrade under targeted disruptions.\n1\nIntroduction\nLarge language models (LLMs) have advanced rapidly,\nachieving strong performance across a wide range of lan-\nguage tasks. Beyond task accuracy, they increasingly repro-\nduce human-like linguistic behaviors [Park et al., 2023; Mou\net al., 2024] and exhibit internal representations that align\nwith human neural responses during language processing\n[Schrimpf et al., 2021; Caucheteux and King, 2022; Toneva\nand Wehbe, 2019; Goldstein et al., 2024]. These develop-\nments position LLMs not only as engineering artifacts, but\nalso as candidate computational models for investigating the\nmechanisms of human language—and, crucially, for simulat-\ning disordered language in a controlled and scalable way.\nAphasia is an especially compelling testbed for this goal.\nAs a heterogeneous language disorder caused by focal neural\ninjury, aphasia presents with dissociable impairments in com-\nprehension and/or production that vary systematically across\nsubtypes.\nIf we can manipulate an LLM’s internal com-\nponents to reproduce the linguistic profiles observed in pa-\ntients—capturing both the type and severity of deficits—then\nLLMs could serve as computational proxies for individu-\nals with aphasia. Such proxies would enable efficient, re-\nproducible testing of therapeutic hypotheses and could sup-\nport the development of personalized rehabilitation strategies.\nMore broadly, they would offer a new paradigm for probing\nfunctional organization in both biological and artificial lan-\nguage systems.\nThe scientific motivation for this approach draws on the\nlong-standing view that language is supported by a modu-\nlar or partially specialized architecture in the brain [Dronkers\nand Ivanova, 2023; Meunier et al., 2010; Bertolero et al.,\n2015]. Under this framework, different neural substrates con-\ntribute different linguistic functions, and damage to particular\nregions yields relatively predictable patterns of impairment.\nClassic descriptions of Broca’s and Wernicke’s aphasias, for\nexample, link deficits in speech production and comprehen-\nsion to lesions in distinct cortical territories. Early compu-\narXiv:2601.19723v1  [cs.CL]  27 Jan 2026\n"}, {"page": 2, "text": "tational accounts pursued this logic using small-scale con-\nnectionist models, showing that “lesioning” specific parts of\na network could reproduce select patient-like error patterns\n[Dell et al., 1997; Farah, 1991; Hinton and Shallice, 1991].\nWhile foundational, these models lacked the scale and lin-\nguistic expressivity needed to approximate the breadth and\ncomplexity of human language, limiting their value as realis-\ntic proxies for aphasic syndromes.\nIn this paper, we leverage modular LLMs based on\na Mixture-of-Experts (MoE) architecture,\nwhere differ-\nent experts often capture different types of informa-\ntion—suggesting a form of functional specialization analo-\ngous to that observed in the human brain [Chen et al., 2022;\nZhang et al., 2023]. We also study dense Transformer models\nas baselines. To enable a fair comparison, we treat all inter-\nvention targets as functional components: in MoE models, a\ncomponent is an expert, whereas in dense models, a com-\nponent corresponds to a fine-grained neuron group or fea-\nture channel within a layer.\nWe first identify components\nmost associated with two clinically salient aphasia subtypes,\nBroca’s and Wernicke’s. We then interpret these components\nusing linguistic probing tasks. Finally, we induce graded im-\npairments by perturbing the top-2% subtype-linked compo-\nnents and evaluate the resulting deficits using Western Apha-\nsia Battery (WAB) subtests summarized by the Aphasia Quo-\ntient (AQ), a standard clinical measure. Across experiments,\nprogressive impairment yields graded, clinically measurable\ndeclines on WAB subtests and AQ. Together, these results\nsuggest that LLMs—particularly modular architectures—can\nserve as scalable experimental platforms for modeling apha-\nsia, enabling controlled studies of how distinct language func-\ntions degrade under targeted perturbations.\n2\nRelated Work\nOur research integrates insights from three distinct but con-\nverging domains: the neuroscience of aphasia, the history\nof computational disorder simulation, and the architecture of\nmodern language models. A central gap is that most recent\n“LLM damage” studies emphasize general robustness, but do\nnot align to standardized clinical scoring or separate clinically\nmeaningful aphasia subtypes.\n2.1\nAphasia and the Neural Basis of Language\nAphasia offers a clinically grounded lens on the functional\norganization of language.\nBroca’s aphasia is typically as-\nsociated with effortful, agrammatical production with rela-\ntively preserved comprehension, whereas Wernicke’s aphasia\nis characterized by fluent but semantically disrupted speech\n[Lichtheim and others, 1885; Damasio, 1992]. Lesion and\nneuroimaging studies support partially dissociable substrates:\nthe left inferior frontal gyrus (IFG) is consistently linked\nto syntactic processing [Friederici, 2011], while temporal\nregions contribute to semantic integration [Dronkers et al.,\n2004; Friederici, 2011]. These findings motivate a computa-\ntional analogue where localized perturbations produce inter-\npretable linguistic deficits and subtype-specific profiles.\n2.2\nSimulating Language Disorders with\nComputational Models\nLesion-based simulation has a long tradition in cognitive\nmodeling. Early connectionist models reproduced naming er-\nrors and grammatical impairments by selectively damaging\nmodel components [Joanisse and Seidenberg, 1999; Thomas\nand Karmiloff-Smith, 2002]. More recently, LLMs have been\nused to elicit aphasia-like outputs via controlled prompting\n[Manir et al., 2024] or by perturbing high-level structures\nsuch as layers [Wang et al., 2025]. While promising, these\napproaches are often coarse-grained: they typically assess\ndegradation with broad Natural Language Processing (NLP)\nbenchmarks, making it difficult to connect a perturbed com-\nponent to a specific linguistic function, and rarely provide\nstandardized clinical evaluation or clear subtype separation.\nOur work builds on this tradition but seeks a model with a\nmore direct and biologically plausible analogy to the brain’s\nmodularity.\n2.3\nMixture-of-Experts Models and\nComponent-Level Lesioning\nMixture-of-Experts models activate only a subset of param-\neters (experts) per token [Shazeer et al., 2017; Fedus et\nal., 2022; Jiang et al., 2024].\nBeyond efficiency, experts\ncan develop emergent specialization for tasks or linguistic\nphenomena [Chen et al., 2022; Zhang et al., 2023], which\nmakes MoE architectures well-suited for component-level le-\nsion studies. However, the broader LLM impairment litera-\nture has mainly focused on general capability loss, safety, or\nrobustness [Sharma et al., 2023; Wang et al., 2023; Gong et\nal., 2025], and often lacks clinical alignment and subtype-\nsensitive analysis.\nIt is also unclear whether the apparent\nlocality of MoE lesions reflects functional modularity or is\npartly shaped by MoE routing.\nTo address these issues, we study both MoE and dense\nTransformer baselines under a unified notion of functional\ncomponents (experts in MoE; neuron groups/feature chan-\nnels in dense models). This allows architecture-independent\nperturbation and direct comparison, while clinical evaluation\nprovides a standardized target for assessing subtype-specific\nimpairments.\n3\nMethod\nWe apply the framework to two transformer LMs: a dense\nbaseline (OLMo) and its Mixture-of-Experts variant (OL-\nMoE). To make the two models comparable, all interven-\ntions operate on a shared notion of functional components\n(units)—a layer–expert pair in OLMoE, and a hidden dimen-\nsions in the Feedforward Neural Network (FFN) for OLMo.\nThe methodology follows a single pipeline for both models:\n(1) attribute fine-grained linguistic phenomena to units us-\ning BLiMP, (2) identify phenotype-linked units from Aphasi-\naBank fine-tuning signals and validate subtype separability\nwith an external CAP classifier, (3) select a stable top-p%\nthreshold via p-sweep, (4) align phenotype-linked units with\nlinguistic phenomena through a rank-percentile heatmap, and\n(5) induce graded impairments by progressively lesioning\nthe top-ranked units and quantify clinical degradation with\n"}, {"page": 3, "text": "Functional Profiling and Phenotype–Phenomenon Alignment\nAphasia \nBank\nWernicke\nBroca\n…\nMoE\nNon-\nMoE\nFitting\nAphasia phenotype-specific fine-tuning\nLinguistic phenomenon probing\nWernicke\nBroca\n…\nBLiMP\nMoE\nNon-\nMoE\nZero\nAblation\nHeatmap \nComparison\nBroca\nWernicke\nUnit-Level Phenomenon–Subtype Alignment\nAphasia-Classifier\nCAP\nTrain RoBERTa\nBroca\nWernicke\nAphasia phenotype \nModel Output\n&-uh well ‚the \nfirst one\nhe he [: she] had \n&-um &-um &-uh\n…\nProgressive Lesioning and Clinical Validation\nPhenotype Component \nContribution\nWernicke\nBroca\nSelect top-k% Unit \nfor Damage\nTop 0.5% Damage\nTop 1.0% Damage\nTop 2.0% Damage\nTop 1.5% Damage\nProgressive Lesioning Schedule\nAphasia Quotient\nWAB Test\nSpontaneo\nus Speech\nComprehe\nnsion\nRepetition\nNaming\nClinical Evaluation\nFigure 1: Details of the analysis pipeline. Functional Profiling and Phenotype–Phenomenon Alignment: BLiMP probing and Aphasi-\naBank fine-tuning yield unit-importance rankings; a heatmap links linguistic phenomena to Broca/Wernicke units, with CAP as an external\nphenotype check. Progressive Lesioning and Clinical Validation: Top-k% units are progressively lesioned and evaluated on WAB/AQ,\nenabling a matched comparison between MoE and dense models.\nWestern Aphasia Battery (WAB), summarized as the Aphasia\nQuotient (AQ) (Figure 1).\n3.1\nModel Architecture\nWe use OLMoE [Muennighoff et al., 2024], specifically\nOLMoE-1B-7B-0924-Instruct.\nIt keeps a standard Trans-\nformer backbone but replaces the FFN with MoE layers,\nwhere a lightweight router selects 8 of 64 experts per token.\nThe model has 16 layers, activates about 1B parameters per\nforward pass, and is fine-tuned on OLMo-Instruct.\nAs a non-MoE control, we use the matched dense OLMo-\nINSTRUCT checkpoint [Groeneveld et al., 2024]. OLMo has\nthe same tokenizer, depth, and training corpus as OLMoE, but\nuses a conventional dense FFN in each layer with no routing\nor sparse expert activation, allowing us to isolate the effect of\nMoE-style modularity. These two models therefore support a\ncontrolled architectural comparison under the same unit-level\nprobing and lesion interface introduced next.\n3.2\nUnit Attribution via BLiMP Zero-Ablation\nThis section produces a task-wise unit attribution map that\nlinks internal units to specific linguistic phenomena. We use\nBLiMP’s minimal-pair evaluation to estimate the causal im-\nportance of model units for specific linguistic phenomena.\nFor each BLiMP subtask, we ablate one unit at a time by\nzeroing its output and record the resulting accuracy. In OL-\nMoE, the ablated unit is a single expert, with the router and\nall other experts left unchanged. In the dense OLMo base-\nline, we treat FFN hidden dimensions within each layer as\nparallel neuron groups and apply the same output-zeroing in-\ntervention, enabling a comparable unit-level analysis across\narchitectures.\nWe define the importance of an expert’s specific task as the\nchange in accuracy:\n∆t(u) = Acct(u) −Acct\n(1)\nNegative ∆t(u) indicates that ablation harms performance,\nsuggesting that model expert or neurons (u) contributes pos-\nitively to phenomenon t. Repeating this procedure yields a\ntask-specific attribution map ∆t ∈Ru for each BLiMP sub-\ntask. These BLiMP-derived rankings are later used to com-\npute rank-percentiles for the alignment heatmaps and to con-\nstruct task profiles for threshold robustness analysis.\n3.3\nAphasia Phenotype-Specific Unit Contribution\nWhile Section 3.2 attributes phenomena to units, this section\nattributes clinical phenotypes (Broca vs. Wernicke) to units\nusing AphasiaBank supervision. The BLiMP analysis links\nunits to specific syntactic/semantic phenomena. To identify\nwhich units are most involved in fitting aphasia subtypes,\nwe fine-tune the same base model (OLMoE or OLMo) on\nthe Broca and Wernicke subsets of AphasiaBank, producing\ntwo phenotype-specific models. We then rank units by their\ntraining-time contribution using gradient-based statistics ag-\ngregated at the unit level (experts in OLMoE; neuron groups\nin OLMo), following Zhang et al. [2024].\nFor phenotype c ∈Broca, Wernicke, we define the cu-\nmulative gradient–weight importance for a parameter tensor\nΘ as:\nIc(θ) =\nS\nX\ns=1\n\f\f\f g(s)\nθ\n⊙θ(s) \f\f\f\n(2)\nwhere ⊙is element-wise multiplication. θ(s) is the weight\nat step s, and g(s)\nθ\nis the gradient of this weight at step s. We\nthen aggregate this quantity over all parameters belonging to a\ncomponent u to obtain a component-level contribution score:\nScorec(u) =\nX\nθ∈Θ(u)\nX\nIc(θ)\n(3)\nRanking experts by Scorec(u) yields two phenotype-specific\ncomponent contribution tables (Broca and Wernicke), which\nwe use for downstream alignment with BLiMP and progres-\nsive lesion experiments.\nTo check that the fine-tuned models produce subtype-\nconsistent language rather than simply memorizing aphasia-\nrelated content, we add an external style validator.\nA\nlightweight RoBERTa-base [Liu et al., 2019] classifier is\ntrained on TalkBank’s Comparative Aphasia Project English\ndata (CAP) [cap, 2025; Bates and Wulfeck, 1989] to predict\nwhether a sentence is more Broca-like or Wernicke-like, re-\nturning subtype probabilities from text alone. We run this\n"}, {"page": 4, "text": "classifier on model generations and report style consistency\nas the fraction of outputs assigned to the intended subtype,\nproviding an independent corpus-based verification that the\ntwo fine-tuned models are separable in phenotype. Together,\nthe phenotype-linked unit ranking and the CAP-based sep-\narability check provide the lesion candidates and validation\nsignals for the subsequent threshold selection and alignment\nanalyses.\n3.4\nThreshold Selection and Robustness\nBecause both visualization (alignment heatmaps) and pro-\ngressive lesioning require selecting the top-p% units, a ro-\nbustness protocol is needed to avoid arbitrary threshold\nchoices.\nTo keep the analysis comparable across MoE\nand dense models, all ablatable components are treated as\nunits. For each BLiMP subtask, units are converted to rank-\npercentiles based on the attribution scores from Section 3.2.\nFor each subtype (Broca/Wernicke), units are ranked by the\ngradient-based contribution scores (Section 3.3) and the top-\np% are selected. Since p acts as a visualization threshold,\nwe run a p-sweep over p ∈0.5, 1, 2, 3, 5, 10%: for each p,\nwe compute a BLiMP task-profile vector by averaging the se-\nlected units’ rank-percentiles within each subtask, and then\nmeasure stability across thresholds using Spearman correla-\ntion:\nρ(p1, p2) = Spearman(mp1, mp2)\n(4)\nIf ρ(p1, p2) is consistently high within the range of p men-\ntioned above, it indicates that the observed cross-task hotspot\npattern is not sensitive to the threshold selection. For read-\nability, we will choose an intermediate threshold for visual-\nization. In the following analyses, an intermediate p = 2%\nis adopted for visualization and lesion budgets, supported by\nthe observed stability under p-sweep.\n3.5\nAlignment of Language Phenomena with\nClinical Phenotypes\nWith the BLiMP unit rankings (Section 3.2) and the subtype-\nlinked contribution tables from AphasiaBank fine-tuning\n(Section 3.3), the alignment is made explicit using a rank-\npercentile heatmap. For each subtype c ∈Broca, Wernicke,\nthe top-p% units are selected by their contribution scores. For\neach BLiMP subtask tj, units are ordered by their BLiMP\nimportance ranking, giving the rank rtj(ui) for each selected\nunit ui. These ranks are then normalized by the number of\nunits in that subtask to form a percentile matrix, where each\nentry indicates how strongly a subtype-linked unit is associ-\nated with a specific linguistic phenomenon.\nHc(i, j) = rtj(ui)\nNtj\n(5)\nNtj represents the number of components in subtask tj. A\nlower Hc(i, j) indicates that the component is more important\nin the BLiMP subtask, and the closer it is to 1, the lower its\nimportance. We visualized the matrix as a heatmap to com-\npare the distribution of Broca syndrome-related experts and\nWernicke syndrome-related experts on syntactic and seman-\ntic subtasks.\n3.6\nDamage Model for Progressive Aphasia\nLesions\nTo mimic focal, lasting damage, we disable selected units\nwhile keeping the overall network structure intact. In OL-\nMoE this targets experts; in dense OLMo it targets the corre-\nsponding FFN neuron groups. Two lesion schemes are com-\npared: activation zeroing, which forces the unit’s output to\nzero, and Xavier re-initialization [Glorot and Bengio, 2010],\nwhich overwrites the unit’s weights to erase its learned com-\nputation. In the Xavier-based lesion step, we traverse the se-\nlected units and replace their weight parameters with new val-\nues sampled from the Xavier uniform distribution, as formal-\nized in Equation 6:\na =\nr\n6\nnin + nout\n(6)\nnin represents the number of input neurons, and nout repre-\nsents the number of output neurons. We overwrite the original\nweights with a random value with a mean of 0 and a moderate\nvariance, thereby completely destroying the original function\nand rendering it unable to perform its specific task.\nActivation zeroing implements a strict silencing lesion by\nremoving a unit’s output, whereas Xavier re-initialization cre-\nates a scrambling lesion by overwriting its weights, disrupting\nthe learned computation while keeping the rest of the network\nintact. Together, they span two complementary impairment\nmodes that better mirror clinical observations where dam-\naged tissue may remain metabolically active yet functionally\ndegraded [Kiran and Thompson, 2019; Gleichgerrcht et al.,\n2015; Wilson and Schneck, 2020]. To model graded severity,\nunits are ranked by the phenotype-linked contribution scores\n(Section 3.3) and the top-p% are lesioned with increasing p;\nthe resulting models are then evaluated with WAB in Section\n3.7.\n3.7\nWAB-Based Clinical Evaluation and Aphasia\nQuotient\nAfter constructing graded lesions on phenotype-linked units,\nWestern Aphasia Battery (WAB) [Kertesz, 2007] provides a\nstandardized clinical endpoint to test whether targeted per-\nturbations translate into aphasia-level functional loss. To as-\nsess whether our injury experiment would induce aphasia-\nlike language changes, we evaluate models with four WAB\nsubtests—Spontaneous Speech (SS), Comprehension (C),\nRepetition (R), and Naming (N)—which probe fluency, un-\nderstanding, repetition, and lexical retrieval. Each item is\nformatted as a standardized text prompt, and all models are\ntested with the same question set, prompting templates, and\ndecoding settings; only minimal post-processing is applied.\nScores from the four subtests are combined into the Apha-\nsia Quotient (AQ). Since some WAB items require mul-\ntimodal input or clinician-driven procedures that text-only\nmodels cannot perform, we keep only text-equivalent items\n(See the supplementary material wab-text.json file for de-\ntails.). Each subtest is scored on this executable subset, nor-\nmalized by the subset maximum, and then linearly rescaled\nto the original WAB subtest maximum to keep subtests com-\n"}, {"page": 5, "text": "parable.\nAQ = (SS + C\n12 + R\n10 + N\n4 ) × 2\n(7)\nWe used the WAB standard weighting for the four subtests to\nderive an AQ score on a 0-100 scale. According to WAB doc-\numentation [Barfod, 2013], a score above 93.8 is considered\nnormal, while a score below 93.8 is diagnosed as aphasia.\nWe tested the AQ scores of a baseline model (no impairment)\nand a progressive impairment model, and observed how the\nAQ score changed with increasing impairment severity. We\nused AQ as our primary outcome measure for assessing the\nseverity of aphasia.\n4\nExperimental Setup\n4.1\nDataset\nIn this paper, we use four datasets:\n• Benchmark of Linguistic Minimal Pairs:\nBLiMP\n[Warstadt et al., 2020] evaluates grammatical knowledge\nusing minimal sentence pairs. We use its syntax, seman-\ntics, and syntax–semantics subsets, and group examples\ninto nine issue types using the linguistics term\nfield.\n• Comparative Aphasia Project English Data (CAP):\nCAP is an English aphasia corpus released via Talk-\nBank, with transcribed speech from 11 participants col-\nlected under a shared elicitation protocol for compara-\ntive analyses of aphasic language [cap, 2025; Bates and\nWulfeck, 1989].\n• Aphasia Bank:\nAphasiaBank [MacWhinney et al.,\n2011] is a corpus of conversations, narratives, and\nQ&A sessions from speakers with aphasia. To simulate\nBroca’s and Wernicke’s aphasia, 313 Broca samples and\n63 Wernicke samples were selected.\n• Western Aphasia Battery (WAB): WAB is a widely\nused standardized clinical test for assessing language\nfunction in aphasia. It quantifies subtype and severity,\nand summarizes core oral subtests into a 0–100 Aphasia\nQuotient (AQ) [Kertesz, 2007].\n4.2\nEvaluation Metrics\nIn the evaluation of BLiMP language phenomena, we com-\npute accuracy based on average log probability. Each sample\ncontains a grammatically correct and incorrect sentence. If\nthe model assigns higher average log probability to the cor-\nrect sentence, it is counted as accurate.\nIn the style fidelity of the CAP classifier, we use a\nRoBERTa-based binary classifier. We report average classi-\nfier confidence (the predicted probability of the expected sub-\ntype). These metrics provide a corpus-based auxiliary valida-\ntion demonstrating that phenotypic fine-tuning can produce\ndiscriminative subtype output patterns.\nIn the WAB clinical assessment, we consider four core sub-\ntests when evaluating the model—spontaneous speech, com-\nprehension, repetition, and naming—and combine them into\nthe 0-100 Aphasia Quotient (AQ) (see section 3.7 for details).\n4.3\nImplementation Details\nFor BLiMP zero-ablation, inference-only evaluation is con-\nducted with activation patching using DeepSpeed on NVIDIA\nA100-SXM-64GB GPUs (global batch size 256; seed 42).\nThe CAP-based subtype classifier is a RoBERTa-base se-\nquence classifier fine-tuned with DeepSpeed and AdamW (lr\n2e-5) for 10 epochs (batch size 16; seed 42). AphasiaBank\nmodel training and gradient-based attribution use DeepSpeed\nwith AdamW (lr 5e-5) for 1 epoch (batch size 8; seed 1234),\nwith ZeRO Stage-3 enabled for memory efficiency.\n5\nResults and Discussion\nThis section presents empirical evidence for the proposed\ncomponent-level aphasia framework in both model architec-\ntures. It first tests whether subtype-linked components map\nonto fine-grained linguistic phenomena, providing an inter-\npretable bridge between clinical phenotypes and behavioral\nsignatures. It then motivates the choice of the top-p% thresh-\nold with a p-sweep robustness analysis.\nFinally, it exam-\nines whether progressively lesioning these components yields\ngraded declines on WAB/AQ, and illustrates the effects with\nqualitative examples.\n5.1\nStyle Consistency (CAP Classifier)\nTo evaluate whether the model can retain the expected apha-\nsia phenotype beyond task scores after fitting a corpus of real\naphasia patients, we introduce an independent subtype con-\nsistency test based on a supervised classifier (RoBERTa base\nmodel) trained on the CAP dataset.\nAs shown in Table 1, the CAP classifier achieved an over-\nall consistency of 98.25% (OLMoE) and 92% (OLMo) on\nthe CAP validation set. The OLMoE outputs exhibited strong\nphenotypic consistency (85% for the Broca subtype and 90%\nfor the Wernicke subtype). Notably, the non-MoE baseline\nmodel OLMo also achieved similarly high consistency (80%\nfor the Broca subtype and 95% for the Wernicke subtype).\nThis indicates that the observed aphasia subtype consistency\nis not an artifact caused by the MoE routing itself. In sum-\nmary, these results establish that subtype-specific fine-tuning\nyields separable outputs, enabling downstream lesion selec-\ntion and clinical evaluation.\nOLMoE-output\nOLMo-output\nCAP Classifier Score\n98.25%\n92%\nBroca\n85%\n80%\nWernicke\n90%\n95%\nTable 1: CAP - classifier scores on model outputs. CAP Clas-\nsifier Score is evaluated on the held-out CAP validation split.\nBroca/Wernicke are subtype consistency of model generations that\nis the fraction of generations classified as the target subtype.\n5.2\nPhenomenon–Subtype Unit Alignment and\np-sweep Robustness\nHaving verified subtype separability, we next ask whether\nsubtype-linked units correspond to interpretable linguistic\n"}, {"page": 6, "text": "[a]\n[b]\n[c]\n[d]\nL10-E22\nL4-E54\nL10-E61\nL11-E35\nL10-E58\nL10-E21\nL8-E61\nL15-E36\nL11-E24\nL15-E12\nL15-E29\nL14-E7\nL15-E57\nL11-E52\nL15-E17\nL13-E54\nL11-E44\nL14-E4\nL9-E6\nL13-E12\nnpi_licensing\nquantifiers\nargument_structure\nellipsis\nfiller\nisland\ns_selection\nbinding\ncontrol_raising\nnpi_licensing\nSemAvg\nSynAvg\nSynSemAvg\nnpi_licensing\nquantifiers\nargument_structure\nellipsis\nfiller\nisland\ns_selection\nbinding\ncontrol_raising\nnpi_licensing\nSemAvg\nSynAvg\nSynSemAvg\nL10-E34\nL12-E51\nL9-E20\nL13-E10\nL12-E45\nL9-E11\nL15-E0\nL15-E36\nL14-E39\nL15-E12\nL15-E29\nL14-E7\nL14-E24\nL15-E60\nL15-E57\nL15-E58\nL15-E1\nL15-E17\nL13-E54\nL8-E48\nOLMo - Wernicke \nOLMo - Broca\nOLMoE - Wernicke\nOLMoE - Broca \nrank / total  ( rank percentile clipped to 0.12 )\nrank / total\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n20.80%\n7.91%\n20.60%\n5.04%\n2.36%\n21.79%\n4.86%\n6.01%\n10.63%\nOLMo - Broca \n20.09%\n8.91%\n19.47%\n5.39%\n2.56%\n20.80%\n6.40%\n6.96%\n9.43%\nOLMo - Wernicke\n20%\n5%\n20%\n20%\n10%\n25%\nOLMoE - Broca\n10%\n15%\n5%\n5%\n15%\n15%\n15%\n20%\nOLMoE - Wernicke\nnpi_licensing\nquantifiers        argument_structure\nellipsis        filler        island        s-selection          binding           control_raising\n[e]\nFigure 2: Rank-percentile heatmaps and pie chart for top-2%\nsubtype-relevant units. [a–b] Dense (OLMo) neurons (Wernicke,\nBroca). [c–d] MoE (OLMoE) experts (Wernicke, Broca). Color\nshows rank percentile (lower = more important) across BLiMP sub-\ntasks and summary columns. [e] Proportion of dominant BLiMP\nsub-tasks among the top-2% units (by each unit’s maximum contri-\nbution).\nphenomena.\nWe test whether subtype-linked components\nmap to specific linguistic behaviors by aligning their impor-\ntance with BLiMP phenomenon rankings.\nFor each sub-\ntype (Broca, Wernicke), the top 2% components are se-\nlected (dense: neuron groups/feature channels; MoE: ex-\nperts) and their relative importance across BLiMP subtasks\nis visualized (Figure 2).\nIn the dense (non-MoE) base-\nline, the top 2% subtype-associated components exhibit dif-\nfuse importance spread across many BLiMP phenomena, and\nBroca- and Wernicke-aligned sets yield highly similar pro-\nfiles (Figure 2a,b,e). This dispersion suggests that subtype-\nrelated signals are encoded in a distributed manner, limit-\ning phenomenon-specific interpretability and making unit-to-\nfunction correspondences difficult to localize. In contrast,\nthe MoE model shows a more structured and phenotype-\nconsistent alignment: the top 2% subtype-associated experts\nform clearer block patterns across BLiMP phenomena (Fig-\nure 2c,d,e), indicating selective sensitivity to particular sub-\nsets of linguistic attributes rather than uniform contribution\nacross tasks. This structured selectivity supports the hypothe-\nsis that MoE modularity facilitates a more localized mapping\nfrom subtype-linked training signals to observable language\nbehaviors, whereas dense models tend to distribute these sig-\n[a]\n[b]\nOLMoE - Robustness\nOLMo - Robustness\nFigure 3:\nRobustness of subtype task profiles around the\n2% lesion choice.\n[a] MoE (OLMoE) and [b] dense (OLMo)\nshow Spearman similarity between profiles at lesion ratio p ∈\n(0.5%, 1%, 2%, 3%, 5%, 10%) and the 2% profile for Broca- and\nWernicke-targeted units.\nnals across many units.\nAcross both architectures, subtype-linked components\nconcentrate their importance in a subset of BLiMP phenom-\nena, most prominently argument structure, ellipsis, filler, is-\nland constraints, and s-selection, while showing compara-\ntively weaker associations with NPI licensing, quantifiers,\nbinding, control/raising, and aggregate measures (e.g., se-\nmavg, synavg, synsemavg). This indicates that the clinically\nrelevant perturbation targets identified by our subtype attribu-\ntion are not uniformly distributed over linguistic knowledge,\nbut instead disproportionately implicate structure-building\nand constraint-sensitive phenomena. We also observe sub-\ntype differences within this subset: Broca-linked components\nplace relatively greater weight on argument structure than\nWernicke-linked components, consistent with the view that\nBroca-like deficits are more tightly tied to production-side\nstructural planning and predicate–argument realization (in-\nterpreted here as a model-side correspondence rather than a\ndirect claim about neuroanatomical localization).\nFinally, our visualization is robust to the choice of the top-\np threshold. Figure 3 shows that phenomenon profiles remain\nhighly stable for p ∈[1%, 5%] (Spearman ρ > 0.85 rela-\ntive to the 2% reference) across both architectures and both\nsubtypes. We therefore use p = 2% as a principled compro-\nmise that preserves subtype-specific structure while avoiding\ndilution from lower-signal components. Overall, while both\narchitectures can exhibit phenotype-targeted regressions un-\nder a unified component-level analysis, the MoE model con-\ncentrates subtype-linked effects into more separable compo-\nnents: perturbing a small subset of experts yields coherent,\nsubtype-consistent shifts that scale with lesion strength. In\nthe dense model, similar regressions likely arise from broader\ndistributed disruption, reflecting weaker modular separability\nand reduced component-level interpretability.\n5.3\nProgressive Lesioning Causes Graded Drops in\nWAB/AQ\nTo test whether phenotype-related unit impairments in-\nduce aphasia-like behavior, the most Broca- and Wernicke-\nassociated units are lesioned at increasing ratios (p\n∈\n0.5, 1, 1.5, 2%) and evaluated with WAB, reporting the Apha-\nsia Quotient (the “WAB test score” in Figure 4). Controls\ninclude the intact base model and a random-lesion baseline\nwith the same lesion budget. In both the non-MoE OLMo\nmodel (Figure 4a) and the MoE-based OLMoE model (Fig-\n"}, {"page": 7, "text": "ure 4b), the baseline remained stable (93.97–94.82), above\nthe WAB-defined threshold score for aphasia (93.8). This\nconfirms that the assessment is insensitive to non-lesion-\nrelated confounding factors.\nConversely, both phenotype-\ntargeted lesions and size-matched random lesions exhibited\na clear dose-response relationship: larger perturbations led\nto a systematic decrease in WAB scores, indicating that our\nlesion management approach induces graded, clinically inter-\npretable deficits rather than a fragile failure mode.\nCrucially, under the same lesion budget, phenotype-\ntargeted lesions consistently exhibited greater destructive\npower than random lesions, demonstrating that the units\nselected through phenotype-unit mapping capture function-\nally and clinically relevant unit components. This effect was\nmost pronounced at high lesion proportions, where random\nlesions maintained fairly high performance, while phenotype-\ntargeted lesions led to a significant decrease in WAB scores.\nFurthermore, we observed a separation of model archi-\ntecture dependencies between Broca’s and Wernicke’s func-\ntional impairments. In the non-MoE OLMo model (Figure\n4a), the dysfunction curves for Broca and Wernicke con-\nverge rapidly with lesion ratio p, eventually reaching simi-\nlar low performance levels. This convergence indicates that\nin dense Transformer models, the computations supporting\nthese clinical phenotypes are more dispersed and overlapping.\nThis makes it difficult to clearly distinguish subtype-specific\ndysfunctions under unit ablation. In contrast, the OLMoE\nmodel of the MoE architecture (Figure 4b) exhibits more\npronounced dysfunction characteristics: as the p-value in-\ncreases, ablation targeting Wernicke leads to a faster decrease\nin the WAB score than ablation targeting Broca’s (and the gap\nwidens around p = 2%). This difference aligns with the hy-\npothesis that the modular MoE architecture enables more\nlocalized and subtype-informative lesion-phenotype map-\nping, while dense models tend to encode language func-\ntions in a more distributed manner.\n[a]\n[b]\nFigure 4: WAB Test Score. Progressive lesioning (Xavier) yields\ngraded clinical degradation on the WAB evaluation (Aphasia Quo-\ntient, AQ). [a] Dense OLMo and [b] MoE OLMoE.\n5.4\nQualitative Case Study: Targeted Lesions vs\nRandom and Base\nTo complement quantitative results, we compare all the per-\nformance of the base model with random lesions and pheno-\ntypic targeted lesions (p=2%) under the same WAB prompt.\nTable 2 shows the results for the WAB prompt (“How are you\ntoday?”). The base model produces coherent, socially ap-\npropriate responses, and random lesions largely preserve co-\nherence, suggesting that sparse unstructured damage induces\nonly mild noise.\nIn contrast, phenotype-targeted lesions yield subtype-\nconsistent regressions. Broca-targeted lesions compress pro-\nduction (e.g., OLMoE collapses to minimal affirmations;\ndense OLMo becomes repetitive), whereas Wernicke-targeted\nlesions primarily disrupt semantics (e.g., repetitive low-\ninformation fragments or off-topic assertions), resulting in\na marked loss of meaningful communicative content across\nboth architectures.\nOverall, this case study provides a clear behavioral valida-\ntion of our component-level mapping: with a constant pro-\nportion of lesions, only lesions targeting subtype-related\nunits reliably produce unique, aphasia-like output pat-\nterns, while random lesions tend not to affect general conver-\nsational ability. This qualitative evidence is consistent with\nour broader findings.\nPrompt: How Are You Today?\nModel\nAphasia\nType\nOutput\nBase Model\n/\nI am fine, thank you.\nOLMoE\nDamage\nTop2%\nRandom\nDamage\nYES, I am doing well\ntoday.\nBroca\nYes\nWernicke\nAa Aa\nOLMo\nDamage\nTop2%\nRandom\nDamage\nI am fine, thank you.\nBroca\nHow are you today?\nWernicke\nYou are not an AI model.\nTable 2: Qualitative case study of lesioned generations.\n6\nConclusion\nThis work asks whether targeted, component-level perturba-\ntions in large language models can produce clinically mean-\ningful analogs of functional specialization in human lan-\nguage. We introduce a clinically grounded framework that (i)\nidentifies subtype-linked functional components for Broca’s\nand Wernicke’s aphasia, (ii) relates these components to inter-\npretable linguistic phenomena via BLiMP, and (iii) evaluates\ninduced impairments using Western Aphasia Battery (WAB)\nsubtests summarized by the Aphasia Quotient (AQ). Across\nmodels, AQ decreases monotonically with increasing lesion\nstrength, demonstrating graded impairment that is measur-\nable in a clinical metric rather than reflecting only generic\ncapability degradation.\nSubtype-targeted perturbations yield more systematic,\naphasia-like regressions than size-matched random per-\nturbations.\nBroca-targeted lesions preferentially reduce\nproduction-related quality (e.g., reduced output and informa-\ntion content), whereas Wernicke-targeted lesions more of-\nten compromise semantic coherence and relevance. These\nsubtype-distinct effects are observed in both the modular OL-\nMoE model and a dense OLMo baseline, addressing a gap in\n"}, {"page": 8, "text": "prior LLM lesioning work that rarely aligns impairments with\nWAB/AQ or explicitly distinguishes aphasia subtypes.\nFinally, architecture matters for interpretability. MoE mod-\nels provide more localized and separable damage–phenotype\nmappings: subtype-linked experts align more selectively with\nspecific BLiMP phenomena (e.g., argument structure, ellip-\nsis, island constraints, and s-selection) and can be perturbed\nto produce coherent, subtype-consistent shifts. Dense models\nshow similar high-level trends but with more diffuse compo-\nnent signatures, making unit-level attribution less localized.\nTaken together, our results suggest that modular LLMs, com-\nbined with clinically informed component targeting and eval-\nuation, offer a promising and scalable platform for simulat-\ning aphasic language production and for probing how distinct\nlanguage functions degrade under controlled disruptions.\nReferences\nVanessa Barfod. Western aphasia battery (wab). Stroke En-\ngine. Retrieved January, 10:2021, 2013.\nElizabeth Bates and Beverly Wulfeck. Comparative aphasiol-\nogy: A cross-linguistic approach to language breakdown.\nAphasiology, 3(2):111–142, 1989.\nMaxwell\nA\nBertolero,\nBT\nThomas\nYeo,\nand\nMark\nD’Esposito. The modular and integrative functional archi-\ntecture of the human brain. Proceedings of the National\nAcademy of Sciences, 112(49):E6798–E6807, 2015.\nAphasiabank english non-protocol CAP corpus. TalkBank,\n2025. Comparative Aphasia Project (CAP), English data.\nCharlotte Caucheteux and Jean-R´emi King. Brains and algo-\nrithms partially converge in natural language processing.\nCommunications biology, 5(1):134, 2022.\nTianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao,\nDaxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei.\nTask-specific expert pruning for sparse mixture-of-experts.\narXiv preprint arXiv:2206.00277, 2022.\nAntonio R Damasio.\nAphasia.\nNew England Journal of\nMedicine, 326(8):531–539, 1992.\nGary S Dell, Myrna F Schwartz, Nadine Martin, Eleanor M\nSaffran, and Deborah A Gagnon.\nLexical access in\naphasic and nonaphasic speakers. Psychological review,\n104(4):801, 1997.\nNina F Dronkers and Maria V Ivanova. The neuroscience of\nlanguage and aphasia. 2023.\nNina F Dronkers, David P Wilkins, Robert D Van Valin Jr,\nBrenda B Redfern, and Jeri J Jaeger. Lesion analysis of the\nbrain areas involved in language comprehension. Cogni-\ntion, 92(1-2):145–177, 2004.\nMartha J Farah. Cognitive neuropsychology: Patterns of co-\noccurrence among the associative agnosias: Implications\nfor visual object representation. Cognitive Neuropsychol-\nogy, 8(1):1–19, 1991.\nWilliam Fedus, Barret Zoph, and Noam Shazeer.\nSwitch\ntransformers: Scaling to trillion parameter models with\nsimple and efficient sparsity. Journal of Machine Learn-\ning Research, 23(120):1–39, 2022.\nAngela D Friederici. The brain basis of language process-\ning: from structure to function.\nPhysiological reviews,\n91(4):1357–1392, 2011.\nEzequiel Gleichgerrcht, Madison Kocher, Travis Nesland,\nChris Rorden, Julius Fridriksson, and Leonardo Bonilha.\nPreservation of structural brain network hubs is associated\nwith less severe post-stroke aphasia. Restorative neurology\nand neuroscience, 34(1):19–28, 2015.\nXavier Glorot and Yoshua Bengio. Understanding the dif-\nficulty of training deep feedforward neural networks. In\nProceedings of the thirteenth international conference on\nartificial intelligence and statistics, pages 249–256. JMLR\nWorkshop and Conference Proceedings, 2010.\nAriel Goldstein, Avigail Grinstein-Dabush, Mariano Schain,\nHaocheng\nWang,\nZhuoqiao\nHong,\nBobbi\nAubrey,\nSamuel A Nastase, Zaid Zada, Eric Ham, Amir Feder,\net al. Alignment of brain embeddings and artificial con-\ntextual embeddings in natural language points to common\ngeometric patterns. Nature communications, 15(1):2768,\n2024.\nEun Jeong Gong, Chang Seok Bang, Jae Jun Lee, and\nGwang Ho Baik.\nKnowledge-practice performance gap\nin clinical large language models: Systematic review of\n39 benchmarks.\nJournal of Medical Internet Research,\n27:e84120, 2025.\nDirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia,\nRodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivi-\nson, Ian Magnusson, Yizhong Wang, et al. Olmo: Acceler-\nating the science of language models. In Proceedings of the\n62nd annual meeting of the association for computational\nlinguistics (volume 1: Long papers), pages 15789–15809,\n2024.\nGeoffrey E Hinton and Tim Shallice. Lesioning an attractor\nnetwork: investigations of acquired dyslexia. Psychologi-\ncal review, 98(1):74, 1991.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux,\nArthur Mensch, Blanche Savary, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,\nFlorian Bressand, et al. Mixtral of experts. arXiv preprint\narXiv:2401.04088, 2024.\nMarc F Joanisse and Mark S Seidenberg.\nImpairments\nin verb morphology after brain injury: A connectionist\nmodel. Proceedings of the National Academy of Sciences,\n96(13):7592–7597, 1999.\nAndrew Kertesz. Western aphasia battery–revised. 2007.\nSwathi Kiran and Cynthia K Thompson. Neuroplasticity of\nlanguage networks in aphasia: Advances, updates, and fu-\nture challenges. Frontiers in neurology, 10:295, 2019.\nLudwig Lichtheim et al. On aphasia. Broca’s Region, pages\n318–347, 1885.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov.\nRoberta:\nA ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n"}, {"page": 9, "text": "Brian MacWhinney, Davida Fromm, Margaret Forbes, and\nAudrey Holland. Aphasiabank: Methods for studying dis-\ncourse. Aphasiology, 25(11):1286–1307, 2011.\nShamiha Binta Manir, KM Sajjadul Islam, Praveen Madiraju,\nand Priya Deshpande. Llm-based text prediction and ques-\ntion answer models for aphasia speech. IEEE Access, 2024.\nDavid Meunier, Renaud Lambiotte, and Edward T Bullmore.\nModular and hierarchically modular organization of brain\nnetworks. Frontiers in neuroscience, 4:200, 2010.\nXinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong\nLiang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xu-\nanjing Huang, et al. From individual to society: A survey\non social simulation driven by large language model-based\nagents. arXiv preprint arXiv:2412.03563, 2024.\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle\nLo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh,\nOyvind Tafjord, Nathan Lambert, et al.\nOlmoe: Open\nmixture-of-experts language models.\narXiv preprint\narXiv:2409.02060, 2024.\nJoon Sung Park, Joseph O’Brien, Carrie J. Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S. Bernstein.\nGenerative agents: Interactive simulacra of human behav-\nior. In Proceedings of the 36th Annual ACM Symposium\non User Interface Software and Technology, pages 1–22.\nACM, October 2023.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina\nKauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B\nTenenbaum, and Evelina Fedorenko. The neural architec-\nture of language: Integrative modeling converges on pre-\ndictive processing. Proceedings of the National Academy\nof Sciences, 118(45):e2105646118, 2021.\nPratyusha Sharma, Jordan T Ash, and Dipendra Misra. The\ntruth is in there: Improving reasoning in language mod-\nels with layer-selective rank reduction.\narXiv preprint\narXiv:2312.13558, 2023.\nNoam Shazeer,\nAzalia Mirhoseini,\nKrzysztof Maziarz,\nAndy Davis,\nQuoc Le,\nGeoffrey Hinton,\nand Jeff\nDean.\nOutrageously large neural networks:\nThe\nsparsely-gated mixture-of-experts layer.\narXiv preprint\narXiv:1701.06538, 2017.\nMichael SC Thomas and Annette Karmiloff-Smith.\nMod-\nelling typical and atypical cognitive development. Hand-\nbook of childhood development, pages 575–599, 2002.\nMariya Toneva and Leila Wehbe. Interpreting and improv-\ning natural-language processing (in machines) with natural\nlanguage-processing (in the brain). Advances in neural in-\nformation processing systems, 32, 2019.\nJindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai\nZheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye,\nXiubo Geng, et al. On the robustness of chatgpt: An adver-\nsarial and out-of-distribution perspective. arXiv preprint\narXiv:2302.12095, 2023.\nChengcheng Wang, Zhiyu Fan, Zaizhu Han, Yanchao Bi, and\nJixing Li. Emergent modularity in large language models:\nInsights from aphasia simulations. bioRxiv, pages 2025–\n02, 2025.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R Bow-\nman. Blimp: The benchmark of linguistic minimal pairs\nfor english. Transactions of the Association for Computa-\ntional Linguistics, 8:377–392, 2020.\nStephen M Wilson and Sarah M Schneck.\nNeuroplastic-\nity in post-stroke aphasia: A systematic review and meta-\nanalysis of functional imaging studies of reorganization of\nlanguage processing. Neurobiology of Language, 2(1):22–\n82, 2020.\nZhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao,\nXiaozhi Wang, Xu Han, Zhiyuan Liu, Ruobing Xie,\nMaosong Sun, and Jie Zhou. Emergent modularity in pre-\ntrained transformers.\narXiv preprint arXiv:2305.18390,\n2023.\nZhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, and Xuan-Jing\nHuang. Unveiling linguistic regions in large language mod-\nels. In Proceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long\nPapers), pages 6228–6247, 2024.\n"}]}