{"doc_id": "arxiv:2511.14255", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.14255.pdf", "meta": {"doc_id": "arxiv:2511.14255", "source": "arxiv", "arxiv_id": "2511.14255", "title": "AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR", "authors": ["Gabrial Zencha Ashungafac", "Mardhiyah Sanni", "Busayo Awobade", "Alex Gichamba", "Tobi Olatunji"], "published": "2025-11-18T08:44:17Z", "updated": "2025-11-18T08:44:17Z", "summary": "Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.14255v1", "url_pdf": "https://arxiv.org/pdf/2511.14255.pdf", "meta_path": "data/raw/arxiv/meta/2511.14255.json", "sha256": "c2c6470dff2da3dd08f7208ad203e5be2eaeec890e8d17f78d5b849e58bd5dec", "status": "ok", "fetched_at": "2026-02-18T02:26:40.827590+00:00"}, "pages": [{"page": 1, "text": "AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry\nBenchmark Suite for African Accented English ASR\nGabrial Zencha Ashungafac1, Mardhiyah Sanni1, Busayo Awobade1,\nAlex Gichamba1, Tobi Olatunji1\n1 Intron Health\ntobi@intron.io\nAbstract\nRecent advances in speech-enabled AI, in-\ncluding Google’s NotebookLM and OpenAI’s\nspeech-to-speech API, are driving widespread\ninterest in voice interfaces globally.\nDe-\nspite this momentum, there exists no pub-\nlicly available application-specific model evalu-\nation that caters to Africa’s linguistic diversity.\nWe present AfriSpeech-MultiBench, the first\ndomain-specific evaluation suite for over 100\nAfrican English accents across 10+ countries\nand seven application domains: Finance, Le-\ngal, Medical, General dialogue, Call Center,\nNamed Entities and Hallucination Robustness.\nWe benchmark a diverse range of open, closed,\nunimodal ASR and multimodal LLM-based\nspeech recognition systems using both spon-\ntaneous and non-spontaneous speech conversa-\ntion drawn from various open African accented\nEnglish speech datasets. Our empirical analysis\nreveals systematic variation: open-source ASR\nmodels excels in spontaneous speech contexts\nbut degrades on noisy, non-native dialogue;\nmultimodal LLMs are more accent-robust yet\nstruggle with domain-specific named entities;\nproprietary models deliver high accuracy on\nclean speech but vary significantly by country\nand domain. Models fine-tuned on African En-\nglish achieve competitive accuracy with lower\nlatency, a practical advantage for deployment,\nhallucinations still remain a big problem for\nmost SOTA models. By releasing this compre-\nhensive benchmark, we empower practitioners\nand researchers to select voice technologies\nsuited to African use-cases, fostering inclusive\nvoice applications for underserved communi-\nties.\n1\nIntroduction\nAutomatic Speech Recognition (ASR) has become\na foundational technology across numerous do-\nmains. In customer-support environments, ASR\npowers real-time call routing, intent detection, and\nagent assistance, substantially reducing response\ntimes and improving user satisfaction (Wang et al.,\n2023). In healthcare, voice-enabled digital scribes\ntranscribe clinician-patient interactions on the fly,\nalleviating documentation burdens and cutting\ndownstream transcription costs (van Buchem et al.,\n2021). Emerging applications in legal transcription\n(Saadany et al., 2023), financial trading desktops,\nand live subtitling further demonstrate the broad\nimpact of ASR systems in both enterprise and con-\nsumer settings.\nSelecting the optimal ASR model for a given\ntask now often means choosing among powerful,\npre-trained foundation systems rather than train-\ning bespoke models from scratch. Self-supervised\nmodels such as wav2vec 2.0 (Baevski et al., 2020)\nlearn rich audio features from large amounts of un-\nlabeled speech and can be applied in a zero-shot\nor few-shot manner, achieving near state-of-the-art\nWER rates on standard benchmarks (Baevski et al.,\n2020). Large multitask models such as Whisper\n(Radford et al., 2023), trained on hundreds of thou-\nsands of hours of multilingual and multitask data,\nexhibit strong zero-shot transfer across domains\nand languages without additional fine-tuning (Rad-\nford et al., 2023). However, computational bud-\ngets, latency requirements, and domain mismatches\nmean that one foundation model may outperform\nanother depending on the target task, be it medical\ndictation, legal proceedings, or informal conversa-\ntional speech.\nAccented speech, particularly non-Western and\nunderrepresented varieties, remains a persistent\nblind spot in mainstream evaluation suites. African\naccents, with their rich phonetic and prosodic diver-\nsity, often lead to significant word error rate dispar-\nities compared to North-American or British En-\nglish (Dossou, 2025). Without a dedicated bench-\nmark, practitioners lack a reliable way to assess\nwhich off-the-shelf ASR system can meet accuracy,\nlatency, or robustness requirements on African-\naccented speech.\narXiv:2511.14255v1  [cs.CL]  18 Nov 2025\n"}, {"page": 2, "text": "Accordingly, we present a unified evaluation\nsuite that benchmarks leading ASR systems,\nAfriSpeech-MultiBench in zero-shot mode across\nmedical, legal, conversational, entity-rich, call\ncenter, finance and robustness-diagnostic African-\naccented English speech (short, silence, and no-\nspeech conditions). The suite supplies standard-\nized test sets, and transparent scoring protocols en-\nabling practitioners to compare models and select\nthe architecture most appropriate for their target\napplication or for finetuning. We publicly release\nthe benchmark suite on Hugging Face with CC\nBY-NC-SA 4.0 License 1\n2\nRelated Work\nIrokoBench introduced a comprehensive text-based\nevaluation across seventeen low-resource African\nlanguages, revealing significant performance gaps\nbetween large language models and human com-\npetence on tasks such as natural language infer-\nence, reasoning and question answering (Adelani\net al., 2025). The study underscores the necessity\nof domain-specific evaluation: without targeted test\nsuites, systematic deficiencies remain undetected.\nWithin automatic speech recognition (ASR),\nprogress\nis\noften\nmeasured\nthrough\nthe\ncommunity-maintained\nOpen\nASR\nLeader-\nboard, which continuously reports word-error\nrate (WER) and real-time factor on LibriSpeech\n(Panayotov et al., 2015), TED-LIUM 3 (Hernandez\net al., 2018), GigaSpeech (Chen et al., 2021),\nVoxPopuli (Wang et al., 2021), AMI (Carletta\net al., 2005), Earnings22 (Andrew et al., 2022),\nSPGISpeech (Guo et al., 2022), and Common\nVoice (Ardila et al., 2020). Although these datasets\ncover a range of domains, from read audiobooks\nto meeting-room recordings, they remain dom-\ninated by North-American and British English,\nproviding limited insight into performance on\nAfrican-accented English.\nEmpirical investigations confirm the practical\nconsequences of this imbalance. Koenecke et al.,\n2020 documented a twofold increase in WER\nfor African American Vernacular English rela-\ntive to Standard American English across mul-\ntiple commercial recognizers.\nA global audit\ninvolving speakers from 171 birth countries ob-\nserved the largest error rates for sub-Saharan partic-\nipants(DiChristofano et al., 2022). In the absence\n1https://huggingface.co/collections/\nintronhealth/afrispeechmultibench\nof African-accented evaluation sets, leaderboard\nrankings therefore offer an incomplete picture for\nstakeholders on the continent.\nModern recognizers are architecturally diverse.\nThey include multilingual encoders such as Whis-\nper (Radford et al., 2023) and XLS-R, proprietary\ncloud services (Microsoft Azure Speech-to-Text,\nGoogle Speech-to-Text), Conformer-based systems\nlike Canary (Puvvada et al., 2024) and Parakeet\n(Rekesh et al., 2023), Speech-Augmented Lan-\nguage Models (SALMs) (Chen et al., 2023), and\nmultimodal architectures such as SeamlessM4T\n(Schwenk et al., 2023). Their heterogeneous train-\ning regimes and objectives complicate any attempt\nto infer accent robustness from results on existing\nbenchmarks alone.\nSeveral African-accented corpora have been\nreleased to mitigate data scarcity. AfriSpeech-200\nprovides roughly 200 hours of read speech from\nmore than 100 indigenous accents (Olatunji et al.,\n2023).\nAfriSpeech-Dialog adds spontaneous\ntwo-speaker conversations (Sanni et al., 2025);\nAfriSpeech-Parliament\ncaptures\nparliamentary\ndebates (Intron Health, 2025a); Med-Convo-Nig\nfocuses on Nigerian clinical tele-consultations\n(Intron\nHealth,\n2025c);\nAfri-Names\ntargets\nnamed-entity-rich\nprompts\n(Intron\nHealth,\n2025b);\nand AfriSpeech-Countries assembles\ncross-regional accents under consistent recording\nconditions (Intron Health, 2025). Existing baseline\nevaluations do not cover modern speech recogni-\ntion systems or lack broad application-specific\nresults.\nThis study contributes three key advances. First,\n7 publicly available African-accented corpora are\nharmonised into AfriSpeech-MultiBench, an eval-\nuation suite spanning medical, legal, conversa-\ntional , named-entity-rich and noise robustness\nspeech. Second, 18 contemporary recognizers cov-\nering multilingual, proprietary, Conformer-based,\nSpeechLLMs and multimodal architectures are\nevaluated in zero-shot mode, with WER reported.\nThird, a fine-grained error analysis disaggregates\nresults by accent cluster, phonetic context and do-\nmain, elucidating systematic failure modes and in-\nforming future data collection and model selection.\n"}, {"page": 3, "text": "3\nBenchmark Methodology\n3.1\nSource Datasets\nWe\nassemble\nseven\ncorpora\nto\nform\nAfriSpeech-MultiBench,\ncovering\ndiverse\nAnglophone African English accents.\nThe\ndistribution of sources is shown in Table 1.\n• AfriSpeech-200: (Afri) a 200-hour, 67,577\nclip dataset, 2,463 speakers across 120 indige-\nnous accents from 13 African countries, span-\nning clinical and general domain read speech\n(Olatunji et al., 2023).\n• AfriSpeech-Dialog: (Diag) about 50 long-\nform medical and nonmedical conversational\nsessions with African-accented spontaneous\nEnglish (about 7 hrs) (Sanni et al., 2025).\n• AfriSpeech-Parliamentary: (Parl) A real-\nworld noisy, multi-speaker dataset of tran-\nscribed parliamentary speech (about 35.86\nhours, 8,068 clips) sampled from Nigeria,\nGhana, South Africa, and Kenya.\n(In-\ntron Health, 2025a).\n• Med-Conv-Nig: (Med.Conv) about 25 long-\nform simulated doctor-patient conversations\ncapturing multispecialty clinical interactions\nin Nigeria, featuring both male and female\nspeakers and rich in medical vocabulary tai-\nlored for evaluating domain-specific ASR in\nhealthcare settings (Intron Health, 2025c).\n• AfriNames:\n(Names) A read-speech cor-\npus with subsets focused on African names\n(Name), numbers (Nums), and voice com-\nmands (Commands), e.g.\n\"transfer $500\nto my HSBC account\"; comprising 6,307\nsingle-speaker samples (about 8.92 hours), en-\nriched with named entities and number utter-\nances, spanning 12 distinct accents across four\ncountries, particularly suited for evaluating\nASR performance on entity-rich transcription\ntasks (Intron Health, 2025b)\n• AfriSpeech-Countries:\nA\nmixture\nof\nAfriSpeech-200, AfriSpeech-Parliamentary,\nAfriNames and North African accented\nspeech samples (Ctry-NA), totaling approxi-\nmately 67 hours and 21,581 clips. The dataset\nspans seven African regions and includes both\nread and conversational speech. All samples\nare annotated by domain and country.\n• Afro-Call-Centers:\n(Call)\nA\nprivate\nunreleased\ndataset\ncapturing\nreal-world\nagent-customer voice interactions rich in\ndomain-specific vocabulary across finance,\nhealth, and customer support domains\n3.2\nDomains Studied\nWe define seven domain categories for evaluation\nwith dataset details described in Table 1:\n• Medical: health-related medical speech and\nclinician–patient dialogues.\n• General:\nread-speech\nsourced\nfrom\nWikipedia and non-spontaneous multispeaker\ndialogues.\n• Legal: noisy parliamentary proceeding with\noverlapping speech.\n• Finance: read speech enriched with numbers\nsuch as currencies, decimals, dates, measure-\nments, locations, trading volumes, and finan-\ncial institutions.\n• Call Center / Customer Support: real-world\nagent–customer interactions\n• Named-Entities: Named-Entity-Rich Gen-\neral clips with dense mentions of African per-\nson names, locations, organizations, and dates\n• Noise Robustness:\nThis diagnostic sub-\nset evaluates ASR stability under challeng-\ning acoustic conditions, including short ut-\nterances (under 3.5 s) from AMI, VoxPop-\nuli, AfriSpeech, and AfriNames datasets. It\nalso includes Intervening Silence clips from\nAfriNames with deliberate pauses to test con-\ntextual continuity, and a No Speech subset\nfrom AfriSpeech-Parliament to measure false-\ntrigger resistance when no speech is present.\n3.3\nModels\nWe evaluate 19 modern ASR systems partly\nsourced from the top twenty entries on the Hug-\nging Face Open ASR Leaderboard (snapshot: July\n2025)2 categorized into model families represent-\ning architectural breadth Conformer, RNN-T, CTC,\ntransducer hybrids, and speech-augmented lan-\nguage models (SpeechLLMs) and include both\nfully open-source checkpoints and proprietary ser-\nvices already deployed in commercial workflows.\n2Leaderboard URL: https://huggingface.co/spaces/\nhf-audio/open_asr_leaderboard.\n"}, {"page": 4, "text": "Domain\nData Source\nSamples\nHours\nCountries\nAccents\nSpeakers\nMedical\nAfri (clinical), Dialog (medical), Med.Conv\n3651\n29.88\n10\n95\n519\nGeneral\nAfri (general), Dialog (general)\n2741\n13.06\n9\n84\n455\nLegal\nParl\n8068\n35.86\n4\n–\n–\nNamed Entities\nNames (names)\n3121\n2.18\n3\n6\n–\nFinance\nNames (numbers), Names (commands)\n3186\n6.73\n4\n9\n–\nCall Center\nCall (Private)\n16\n0.80\n2\n3\n32\nRobustness\nShort Speech, No Speech, Intervening Silence\n2067\n3.74\n-\n-\n-\nTotal Unique\n20093\n79.19\n11\n108\n859\nTable 1: Domain-wise breakdown of the AfriSpeech-Multibench benchmark. Parentheses denote domain-specific\nsubsets. Full names of the datasets - Afri:AfriSpeech, Dialog:AfriSpeech-Dialog, Med.Conv:Med-Conv-Nig,\nNames: AfriNames. The Call Center source is private and not disclosed.\nArchitecture\nModel\nSize\nConformer\nNvidia Parakeet-tdt-0.6B-v2\n0.6B\nNvidia Parakeet-tdt-1.1B\n1.1B\nNvidia Parakeet-rnnt-1.1B\n1.1B\nNvidia Canary-1B-flash\n1B\nWhisper\nOpenAI Whisper-large-v3\n1.54B\nDistil-Whisper-v3.5\n756M\nNyra Health CrisperWhisper\n1.54B\nSpeechLLMs IBM Granite-3.3-2B\n2B\nMistral Voxtral-Mini-3B\n3B\nNvidia Canary-Qwen-2.5B\n2.5B\nMicrosoft Phi-4 MM-Instruct\n5.6B\nProprietary\nIntron-Sahara\n–\nIntron-Sahara-V2\n–\nOpenAI GPT-4o Transcribe\n–\nGoogle Gemini-2.0 Flash\n–\nGoogle Gemini-2.5 Flash\n–\nAWS Transcribe\n–\nMicrosoft Azure Speech\n–\nGoogle Chirp v3\n–\nTable 2: Descriptions of evaluated models, including\nmodel size, core architecture, and provider. Model sizes\nare in billions (B) of parameters when known.\n• NVIDIA’s open models: Open-source ASR\nmodels based on the FastConformer (Rekesh\net al., 2023) such as the Parakeet variants:\nCTC, RNN-T and TDT (Galvez et al., 2024)\nin sizes of 0.6B and 1.1B, and the 1 billion\nparameter Canary-flash model pairing a Fast-\nConformer encoder with a transformer de-\ncoder (Puvvada et al., 2024).\n• Whisper Variants:\nTransformer encoder\ndecoder models based on Whisper (Rad-\nford et al., 2023).\nWe consider the vari-\nants: Whisper-large-v3 (Radford et al., 2023),\nDistil-Whisper-v3.53, and CrisperWhisper\n(Zusag et al., 2024).\n• Open SpeechLLMs: Multimodal LLMs and\n3https://huggingface.co/distil-whisper/\ndistil-large-v3.5\nSpeech-Augmented LLMs including IBM\nGranite-3.3-2B4, Phi-4 Multimodal Instruct\n(Abdin et al., 2024), Nvidia Canary-Qwen5,\nand Mistral’s Voxtral Mini-3B (Liu et al.,\n2025).\n• Proprietary cloud ASR services: OpenAI’s\nGPT-4o transcribe6, Google’s Gemini-2.0- &\nGemini-2.0- flash7, Google’s Chirp V3 8,\nAWS Transcribe9, Azure Speech Recogni-\ntion10 and Intron Sahara (V1 and V2)11. Mod-\nels are evaluated in zero-shot mode, with nei-\nther demonstrations (Min et al., 2022) nor\ndomain-specific fine-tuning.\nThis broad selection of modern ASR systems\nfacilitate an empirical comparison between com-\nmercially deployed services and publicly available\ncheckpoints, capturing the architectural and com-\nmercial diversity of leading ASR systems, provid-\ning a realistic basis for accent-aware model selec-\ntion.\n3.4\nEvaluation Protocol\n• Primary metric: Word Error Rate (WER) mea-\nsured per model, per domain, per country, and\nper dataset.\n4https://huggingface.co/ibm-granite/\ngranite-speech-3.3-2b\n5https://huggingface.co/nvidia/canary-qwen-2.\n5b\n6https://platform.openai.com/docs/models/\ngpt-4o-transcribe\n7https://cloud.google.com/vertex-ai/\ngenerative-ai/docs/models/gemini/2-0-flash\n8https://cloud.google.com/speech-to-text/v2/\ndocs/chirp_3-model\n9https://aws.amazon.com/transcribe/\n10https://azure.microsoft.com/en-us/products/\nai-services/ai-speech\n11https://www.intron.io/\n"}, {"page": 5, "text": "• Error analysis: Breakdown by domain, accent\ngroup (native vs non-native), named-entity er-\nrors, noise robustness; Open-source vs propri-\netary models, unimodal vs multimodal, large\nvs compact variants.\n4\nExperiments\n• Dataset splits: We use held-out test sets per\ncorpus, ensuring some accents appear only\nin testing to evaluate zero-shot generalization\n(e.g. 41 accents exclusively in test partition of\nAfriSpeech-200)\n• Transcript Pre- and Post-processing: Model-\nspecific transcript pre- and post-processing\n(described in Appendix section 7) normal-\nized inputs, removed filler words, and mapped\nnumber words to their digit form,\ne.g.\n\"twenty\" to \"20\" and \"first\" to \"1st\".\n• Inference setup: Uniform audio input prepro-\ncessing (16 kHz mono, no diarization) with\ndefault hyperparameters and decoding settings\nfor ASR models and proprietary API calls. Lo-\ncal runs were on single T4 GPU (16GB).\n• Prompting: We use consistent prompts for\nopen and closed LLMs, e.g. \"Transcribe this\nENGLISH audio\". Prompt details are pro-\nvided in Appendix section 7.\nWe provide results for single runs.\n5\nResults\n5.1\nOverall Results\nAs shown in Table 3, model performance on widely\nused ASR benchmarks such as LibriSpeech, TED-\nLIUM, and AMI does not reliably translate to\naccuracy on African-accented or domain-specific\nspeech. Models that dominate global leaderboards\nexhibit substantial degradation under accent, noise,\nand contextual variability characteristic of African\ndatasets. For instance, leading open-source systems\nlike Parakeet-tdt-0.6B-v2 and Whisper-large-\nv3, which achieve sub-4% WER on LibriSpeech,\nexperience error rates between 30-45% on gen-\neral African speech and exceed 70% on medi-\ncal dialogue or named-entity-rich inputs within\nAfriSpeech-MultiBench.\nThis discrepancy holds consistently across ar-\nchitecture families including transformer-based,\ntransducer, and instruction-tuned multimodal mod-\nels highlighting a persistent generalization gap of\nroughly 2-5× between leaderboard metrics and\nAfrican deployment conditions. Notably, these er-\nrors compound in domain-specific contexts where\npronunciation diversity, code-switching, and out-\nof-vocabulary proper nouns are prevalent.\nIn contrast, Intron-Sahara V2 a successor\nto Intron-Sahara and a regionally tuned model\ntrained on diverse African English data demon-\nstrates markedly superior transferability.\nIt\nachieves WERs below 15% across all benchmarked\ndomains, with a modest increase to 18.09% in med-\nical conversation, still outperforming all other mod-\nels by a wide margin.\nBeyond accuracy, Intron-Sahara V2 exhibits\nexceptional robustness under challenging acous-\ntic conditions. Across the robustness diagnostic\nsubsets (Silence, Short Samples, and Intervening\nSilence), it consistently delivers the lowest error\nand false-trigger rates, underscoring its resilience\nto pause-filled, truncated, or low-energy speech.\nThese findings suggest that regionally tuned ASR\nsystems can close much of the performance gap on\nAfrican speech, outperforming larger yet globally\ntrained models across both accuracy and robustness\ndimensions.\n5.2\nDomain Performance\n5.2.1\nMedical\nAs shown in Table 4, the medical domain remains\none of the most challenging settings, with average\nWERs exceeding 40% for most open and propri-\netary systems. Intron-Sahara V2 achieves the\nbest overall performance across all three medical\ndatasets on Afri-Med, Afri-Diag, and Med.Conv\nyielding an overall average of 15.26%. Open mod-\nels such as Whisper-large-v3 and Parakeet-tdt-\n0.6B-v2 perform moderately (25-27%), while pro-\nprietary systems like Gemini-2.0, GPT-4o, and\nAzure range from 24–30%. Large multimodal\nLLMs such as Phi-4 MM-Instruct and IBM Gran-\nite exceed 80% WER, underscoring that leader-\nboard success on clean English benchmarks does\nnot generalize to accented or domain-specific med-\nical speech. These results highlight the advantage\nof regionally tuned, domain-adapted ASR systems\nfor low-resource healthcare applications in Africa.\n5.2.2\nFinance\nThe finance domain-represented by the Afri-Names\nsubsets for numerals and spoken commands shows\n"}, {"page": 6, "text": "Model\nOpen ASR Benchmarks\nAfriSpeech-MultiBench\nLib-S\nTED-3\nGiga\nVoxP\nAMI\nEarn22\nSPGI\nAfri\nDiag\nParl\nMedC\nNames\nCall\nRob\nParakeet-tdt-0.6B-v2\n1.69\n3.38\n9.74\n5.95\n11.16\n11.15\n2.17\n30.20\n11.23\n18.45\n29.41\n41.88\n20.96\n40.89\nParakeet-tdt-1.1B\n1.40\n3.59\n9.52\n5.49\n15.87\n14.49\n3.16\n28.45\n15.14\n27.14\n29.98\n45.66\n25.26\n44.62\nParakeet-rnnt-1.1B\n1.45\n3.83\n9.89\n5.44\n17.01\n13.94\n2.93\n28.18\n15.08\n26.75\n30.59\n46.70\n28.93\n90.30\nCanary-1B-flash\n1.48\n3.12\n9.85\n5.63\n13.11\n12.77\n1.95\n29.77\n48.50\n19.13\n93.62\n44.10\n88.71\n51.32\nWhisper-large-v3\n2.01\n3.86\n10.02\n9.54\n15.95\n11.29\n2.94\n26.49\n13.49\n19.99\n31.76\n43.23\n24.69\n33.79\nDistil-Whisper-v3.5\n2.37\n3.64\n9.84\n8.04\n14.63\n11.29\n2.87\n27.58\n11.50\n18.00\n30.41\n45.80\n21.65\n34.00\nCrisperWhisper\n1.82\n3.20\n10.24\n9.82\n8.71\n12.89\n2.70\n63.80\n72.72\n79.35\n83.12\n70.14\n35.52\n38.82\nIBM Granite-3.3-2B\n1.64\n4.12\n11.05\n6.55\n10.22\n13.86\n3.96\n34.38\n99.59\n20.67\n96.30\n49.51\n27.10\n45.86\nVoxtral (Mistral)\n1.86\n–\n10.04\n6.78\n–\n12.18\n2.04\n20.17\n68.42\n21.10\n78.73\n49.36\n29.20\n43.51\nCanary-Qwen-2.5B\n1.61\n1.90\n9.43\n5.66\n10.19\n10.45\n1.90\n29.87\n96.64\n18.18\n97.89\n42.91\n39.09\n41.15\nPhi-4 MM-Instruct\n1.68\n2.89\n9.77\n5.93\n11.45\n10.50\n3.11\n26.48\n88.91\n36.73\n130.17\n44.28\n24.99\n122.45\nIntron-Sahara\n–\n–\n–\n–\n–\n–\n–\n16.35\n14.26\n15.41\n27.92\n8.17\n20.08\n18.91\nIntron-Sahara V2\n–\n–\n–\n–\n–\n–\n–\n11.83\n12.02\n13.01\n18.09\n11.66\n13.45\n7.86\nGPT-4o Transcribe\n–\n–\n–\n–\n–\n–\n–\n24.66\n15.03\n64.39\n30.80\n52.49\n23.20\n33.59\nGoogle Gemini-2.0 Flash\n–\n–\n–\n–\n–\n–\n–\n27.80\n12.02\n20.51\n27.59\n50.12\n22.39\n33.91\nGoogle Gemini-2.5 Flash\n–\n–\n–\n–\n–\n–\n–\n26.54\n12.48\n19.42\n27.3\n37.81\n23.41\n32.04\nAWS Transcribe\n–\n–\n–\n–\n–\n–\n–\n32.77\n14.02\n18.50\n30.08\n36.70\n23.51\n33.98\nAzure Speech Recognition\n–\n–\n–\n–\n–\n–\n–\n28.41\n13.29\n18.75\n29.17\n35.69\n24.95\n32.61\nGoogle Chirp V3\n–\n–\n–\n–\n–\n–\n–\n35.03\n17.53\n28.18\n38.57\n52.45\n29.60\n31.70\nTable 3: Word Error Rate (WER %) for each model on standard open ASR benchmarks and subsets of the\nAfriSpeech-MultiBench dataset. Dashes represent results that were not available. Full names of datasets: Lib-S: Lib-\nriSpeech; TED-3: TED-LIUM 3; Giga: GigaSpeech; VoxP: VoxPopuli; AMI: AMI Meeting Corpus; Earn22: Earn-\nings22; SPGI: SPGISpeech; Afri: AfriSpeech-200; Diag: AfriSpeech-Dialogue; Parl: AfriSpeech-Parliamentary;\nMedC: Med-Conv-Nig; Nam: AfriNames; Call: Afro-Call-Centers.; Robustness a combination of No Speech from\nAfrispeech-Parliamentary; Short Samples from AMI, VoxP and Afrispeech-Names; and Intervening Silence samples\nfrom AMI, VoxP, Afrispeech-Names\nFigure 1: Average of Open ASR Leaderboard vs AfriSpeech-Multibench for top models from Table 3\nstrong gains for regionally tuned ASR. Intron-\nSahara V2 achieves a WER of 8.20%, compared\nto 35-50% for most open-source and proprietary\nsystems. Lightweight conformer models such as\nParakeet-tdt-1.1B and Whisper-large-v3 reach\naround 43-46%, while Azure and AWS achieve\nmid-30% accuracy. This domain highlights Sa-\nhara’s superior handling of short, context-free ut-\nterances and accent-driven variations in number\npronunciation critical in the financial domain.\n5.2.3\nNames\nPerformance on African named entities remains a\nmajor differentiator. As shown in Table 3, Intron-\nSahara V2 again leads with 12.4% WER, com-\npared to 40-70% for open-source models and over\n50% for proprietary systems. The failure modes\nof large general-purpose LLMs (e.g., hallucinating\nor anglicizing names) emphasize the need for pho-\nnetic grounding and localized lexicons. Despite\nlimited scale, Sahara’s region-specific acoustic and\nlanguage modeling yields more accurate entity re-\ncovery.\n"}, {"page": 7, "text": "Figure 2:\nWord Error Rate (WER %) for each model across different African English accents in\nAfriSpeech-MultiBench. The average is computed across all listed accent categories.\nFigure 3: Model Sizes vs. Performance on Open ASR Benchmark (Blue) and AfriSpeech-Multibench (Orange)\n5.2.4\nLegal\nTable 3 summarizes performance on the Parliamen-\ntary dataset, which features overlapping speakers\nand high background noise. Among open-source\nsystems, Distil-Whisper-v3.5 performs best with\n11.5% WER, showing that smaller distilled vari-\nants can generalize better to real conversational\noverlap.\nIntron-Sahara V2 follows closely at\n12.01%, outperforming all proprietary and large\nmultimodal models, whose WERs remain above\n20%. This demonstrates that domain-adapted en-\ncoders trained on regional conversational data can\nsurpass even the largest general-purpose models\nunder high-noise conditions.\n5.2.5\nCall Center\nIn the call-center domain, Intron-Sahara V2\nagain leads, achieving 13.45% WER, followed\nby Whisper-large-v3 at 24.7% and Parakeet-\ntdt-0.6B-v2 at 20.96%. Proprietary models such\nas GPT-4o Transcribe and Gemini-2.0 record\naround 22-23% WER, showing that Sahara’s tun-\ning for multi-speaker interaction, turn-taking, and\naccent robustness provides a tangible advantage in\nnoisy, dialogue-heavy environments.\n5.2.6\nNoise Robustness\nThe Noise Robustness evaluation combines the\nSilence, Short, and Intervening Silence diagnostics\nto assess model stability under pauses and non-\nspeech conditions. Most global ASR systems de-\ngrade sharply (20–70% WER), often hallucinating\nspeech. In contrast, Intron-Sahara V2 achieves 0\n% false triggers on silence, 10.15% on short clips,\nand 11.23% under pauses, outperforming Whisper-\nlarge-v3 and GPT-4o. Detailed results are provided\nin Appendix 7\n"}, {"page": 8, "text": "Model\nAfri-Med\nDiag\nMed.Conv\nAverage\nParakeet-tdt-0.6B-v2\n34.55\n11.23\n29.41\n25.06\nParakeet-tdt-1.1B\n33.79\n15.14\n29.98\n29.98\nParakeet-rnnt-1.1B\n33.45\n15.08\n30.59\n30.59\nCanary-1B-flash\n34.77\n72.23\n78.92\n78.92\nWhisper-large-v3\n32.59\n17.22\n31.76\n27.19\nDistil-Whisper-v3.5\n32.18\n16.77\n30.63\n26.53\nCrisperWhisper\n66.66\n78.92\n83.12\n76.23\nIBM Granite-3.3-2B\n40.28\n99.53\n96.30\n78.70\nVoxtral (Mistral)\n30.75\n56.32\n78.73\n55.27\nCanary-Qwen-2.5B\n32.04\n93.08\n97.92\n74.35\nPhi-4 MM-Instruct\n31.74\n88.91\n130.17\n83.61\nIntron-Sahara\n19.20\n13.44\n29.10\n19.46\nIntron-Sahara V2\n15.24\n12.02\n18.51\n15.25\nGPT-4o Transcribe\n28.54\n15.03\n30.80\n24.79\nGoogle Gemini-2.0 Flash\n31.13\n12.02\n27.59\n23.58\nGoogle Gemini-2.5 Flash\n30.66\n12.48\n27.30\n23.48\nAWS Transcribe\n42.22\n14.02\n30.08\n28.77\nAzure Speech Recognition\n32.90\n13.29\n26.17\n24.12\nGoogle Chirp V3\n39.60\n17.53\n38.57\n31.90\nAverage\n34.59\n39.51\n53.83\n42.89\nTable 4: Word Error Rate (WER %) for each model on\nthe medical domain subsets of AfriSpeech-MultiBench,\nincluding clinical notes, medical dialogues, and doc-\ntor–patient conversations. Dataset full name mappings:\nAfri-Med: AfriSpeech Medical; Diag: AfriSpeech-\nDialogue; Med.Conv: Med-Conv-Nig.\nModel\nName\nCommands\nNums\nParakeet-tdt-0.6B-v2\n65.55\n32.65\n22.57\nParakeet-tdt-1.1B\n76.44\n33.67\n26.47\nParakeet-rnnt-1.1B\n75.78\n35.36\n26.66\nCanary-1B-flash\n75.69\n30.05\n20.15\nWhisper-large-v3\n73.10\n31.58\n18.11\nDistil-Whisper-v3.5\n68.15\n37.28\n15.28\nCrisperWhisper\n70.14\n70.35\n71.18\nIBM Granite-3.3-2B\n78.97\n49.03\n23.45\nVoxtral (Mistral)\n69.17\n41.77\n25.42\nCanary-Qwen-2.5B\n69.79\n31.44\n20.15\nPhi-4 MM-Instruct\n78.09\n104.13\n51.28\nIntron-Sahara\n24.24\n1.81\n14.81\nIntron-Sahara V2\n12.40\n7.92\n4.27\nGPT-4o Transcribe\n67.43\n46.67\n17.45\nGoogle Gemini-2.0-Flash\n74.12\n40.77\n18.23\nGoogle Gemini-2.5-Flash\n67.44\n36.89\n16.43\nAWS Transcribe\n60.07\n27.60\n20.21\nAzure Speech Recognition\n67.15\n23.42\n22.43\nGoogle Chirp V3\n84.91\n40.22\n24.83\nTable 5: Word Error Rate (WER %) for each model on\nAfrican named entites and Financial domain subsets of\nAfriSpeech-MultiBench. Dashes represent results that\nwere not available.\n5.3\nAccent and country variations\nAs shown in Table 6 and Figure 2, most models\nshow pronounced degradation in Nigeria, South\nAfrica, and Ghana (about 30%), relative to East and\nNorth Africa (about 24%). Most models perform\ncomparably except GPT-4o and CrisperWhisper\nwith WERs above 60% and Sahara models with\nWER less than 15%.\n5.4\nModel size vs performance\nFigure 3 and Table 3 show that, in a handful of do-\nmains, larger Speech LLMs (Granite, Phi-4, Vox-\ntral, Canary-Qwen) only marginally outperform\nsmaller architectures like conformer and Whisper\nvariants half their sizes. In conversational speech,\nthey are worse overall. Figure 3 indicates overall\nworse performance for open models with increas-\ning size.\n6\nDiscussion\nThis study yields a number of key insights that\nilluminate performance gaps and opportunities for\nadvancing ASR systems in African settings:\n6.1\nGlobal benchmarks misrepresent African\nrealities.\nLeading models like Whisper and Parakeet achieve\nWERs below 10% on LibriSpeech and GigaSpeech,\nyet degrade to over 20-40% on African-accented\ndata in AfriSpeech-MultiBench. This mismatch\nunderscores the limits of current leaderboards in\nguiding ASR adoption across low-resource geogra-\nphies.\n6.2\nAccent diversity drives large performance\nvariance.\nWhile models performed well on Kenyan and Ugan-\ndan English (average WERs as low as 12-18%),\nWERs doubled or tripled for West African and\nNorth African accents-exceeding 25% for many\nsystems. This highlights the phonetic and prosodic\ndiversity across the continent and the inadequacy\nof accent-agnostic training.\n6.3\nConversational speech remains a major\nbottleneck.\nCompared to read speech, performance worsened\nsignificantly on conversational corpora AfriSpeech-\nDialog Medical, Medical Conversations (Med\nConvo), and Parliamentary speech. These mirror\nWestern benchmarks, where models also struggle\non AMI and Earnings22 relative to LibriSpeech\nor SPGISpeech. However, the drop-off in African\nconversational domains is more severe, revealing\ncompound challenges likely due to accent, prosody,\nand domain shift.\n6.4\nNamed entities and structured commands\nstill confound models.\nMost models scored above 40% WER on the Afri-\nNames dataset, numbers, and financial voice com-\nmands, often failing to distinguish culturally unique\nor phonetically similar terms. This raises usabil-\nity concerns in domains requiring accurate name\ncapture or transactional integrity.\n"}, {"page": 9, "text": "Model\nNigeria\nGhana\nKenya\nRwanda\nUganda\nSouth Africa\nNorth Africa\nAverage\nParakeet-tdt-0.6B-v2\n32.60\n26.27\n21.78\n23.92\n9.38\n21.08\n22.76\n22.54\nParakeet-tdt-1.1B\n28.65\n22.08\n21.21\n20.39\n21.35\n25.96\n24.13\n23.40\nParakeet-rnnt-1.1B\n32.76\n23.69\n23.19\n24.71\n23.08\n27.59\n24.42\n25.63\nCanary-1B-flash\n29.00\n25.06\n18.25\n20.39\n23.65\n26.30\n24.04\n23.81\nCanary-Qwen-1B\n28.10\n23.90\n19.20\n20.60\n20.00\n24.80\n22.10\n22.81\nWhisper-large-v3\n26.53\n27.22\n16.70\n21.18\n16.16\n19.85\n20.72\n21.19\nDistil-Whisper-v3.5\n26.69\n22.16\n18.51\n23.53\n19.22\n24.45\n21.43\n22.28\nCrisperWhisper\n74.50\n80.99\n74.72\n40.00\n58.29\n72.11\n62.64\n66.18\nIBM Granite-3.3-2B\n33.05\n25.27\n21.33\n21.18\n21.55\n26.16\n30.25\n25.54\nVoxtral (Mistral)\n27.84\n22.49\n21.50\n23.53\n17.57\n26.96\n20.17\n22.87\nCanary-Qwen-2.5B\n26.82\n22.10\n18.49\n21.57\n21.45\n25.19\n20.99\n22.37\nPhi-4 MM-Instruct\n27.73\n18.86\n27.92\n17.25\n18.49\n51.26\n20.03\n25.93\nIntron-Sahara\n15.85\n15.93\n12.48\n18.04\n12.26\n17.65\n13.14\n15.05\nIntron-Sahara V2\n12.83\n12.02\n13.01\n18.09\n11.66\n13.45\n15.98\n13.86\nGPT-4o Transcribe\n40.03\n60.41\n58.38\n15.69\n15.22\n60.43\n22.40\n38.94\nGoogle Gemini-2.0 Flash\n26.47\n24.54\n21.29\n21.57\n19.61\n27.59\n22.11\n23.31\nGoogle Gemini-2.5 Flash\n25.90\n23.10\n20.40\n20.85\n18.70\n26.10\n21.50\n22.36\nGoogle Chirp V3\n27.10\n24.00\n21.20\n21.90\n19.60\n27.30\n22.60\n23.67\nAWS Transcribe\n28.16\n22.59\n18.18\n30.98\n24.11\n25.05\n22.23\n24.47\nAzure Speech Recognition\n26.41\n23.01\n17.59\n25.49\n19.31\n25.10\n25.46\n23.20\nAverage\n30.99\n27.54\n24.36\n23.01\n21.25\n29.65\n24.38\n25.31\nTable 6: Word Error Rate (WER %) for each model across African accents in AfriSpeech-MultiBench, extended to\ninclude Voxtral (Mistral) and Intron-Sahara V2.\n6.5\nModel size and architecture don’t predict\nreliability.\nSmaller models like Parakeet-tdt-0.6B and Distil-\nWhisper sometimes matched larger peers on global\nbenchmarks but showed inconsistent gains on\nAfrican test sets. By contrast, Sahara a region-\nally optimized model consistently delivered best-\nin-class results across medical, legal, and conversa-\ntional tasks.\n6.6\nBenchmarking must evolve beyond\naverage-case accuracy.\nAfriSpeech-MultiBench\nenables\nfine-grained,\ndomain-aware evaluation that reflects real-world\ndeployment conditions.\nIt provides not only\nmodel ranking, but also insight into where and\nwhy systems fail offering practical guidance for\nbuilding domain and region-specific ASR solutions\nin healthcare, law, finance, and public service\ndelivery across Africa.\n7\nConclusion\nThis study set out to address the gap between\nglobal ASR benchmarks and real-world per-\nformance on African-accented, domain-specific\nspeech.\nThrough AfriSpeech-MultiBench, we\nreveal that top-performing models on standard\ndatasets like LibriSpeech and TED-3 achieving sub-\n5% WER can exhibit 5-10X higher error rates on\nAfrican speech, especially in medical, financial,\nand conversational domains. These disparities are\nconsistent across open-source and proprietary sys-\ntems, highlighting persistent geographic, linguistic,\nand domain biases in existing ASR development\nand evaluation pipelines.\nOur findings underscore the need for regionally\ngrounded benchmarks and models. Intron-Sahara,\na model trained with African-specific data, consis-\ntently outperformed global leaders across domains\nand accents, particularly in name recognition, doc-\ntor patient dialogue, and financial commands. By\nbenchmarking 19 models across 8 African coun-\ntries and 7 key domains, AfriSpeech-MultiBench\nprovides actionable insights for building inclusive\nASR systems. This work lays the foundation for fu-\nture research and deployment efforts in healthcare,\nlegal transcription, customer service, and multilin-\ngual voice applications across the African conti-\nnent.\nLimitations\nWhile AfriSpeech-MultiBench offers a broad and\ndiverse benchmark across African-accented En-\nglish, several limitations warrant consideration.\nFirst, despite including over 10 countries and six\ndomains, the benchmark does not yet cover all ma-\njor linguistic regions in Africa or fully represent\nunder-resourced countries with limited public data\navailability. Certain domains such as manufactur-\ning, education, and public safety are not currently\nincluded, and even within included sectors like\nhealthcare and finance, dataset sizes remain mod-\nest compared to global corpora, which may limit\nfine-grained error analysis and generalization of\nresults.\nSecondly, some datasets used are proxies rather\nthan fully representative of their target verticals.\nFor instance, parliamentary proceedings may not\nfully capture the legal domain’s complexity, such\nas courtroom vernacular, legalese, or multilingual\n"}, {"page": 10, "text": "code-switching common in legal aid and judicial\nsettings. Similarly, due to privacy constraints, cus-\ntomer support datasets from private call centers\nwere not included, limiting direct benchmarking\nfor commercial deployments. These gaps high-\nlight both the urgent need and the opportunity for\ncontinued investment in domain-specific and geo-\ngraphically expansive data collection to build more\ncomprehensive benchmarks for inclusive speech\ntechnologies.\nReferences\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien\nBubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J. Hewett, Mojan Javaheripi, Piero\nKauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li,\nWeishung Liu, Caio C. T. Mendes, Anh Nguyen,\nEric Price, Gustavo de Rosa, Olli Saarikivi, and\n8 others. 2024. Phi-4 technical report. Preprint,\narXiv:2412.08905.\nDavid Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Az-\nime, Jian Yun Zhuang, Jesujoba Oluwadara Alabi,\nXuanli He, and 1 others. 2025. Irokobench: A bench-\nmark for african languages in the age of large lan-\nguage models. In Proceedings of the 2025 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL), pages\n2732–2757, Mexico City, Mexico. Association for\nComputational Linguistics.\nGalen Andrew, Mingqing Chen, Jinyu Lu, and Kevin\nSim. 2022. Earnings22: A 100-hour benchmark cor-\npus for earnings-call ASR. In Proceedings of Inter-\nspeech 2022, pages 3158–3162.\nRosana Ardila, Megan Branson, Kelly Davis, Michael\nKohler, Josh Meyer, Michael Henretty, and 1 others.\n2020. Common voice: A massively-multilingual\nspeech corpus. In Proceedings of the 12th Language\nResources and Evaluation Conference (LREC), pages\n4218–4222.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nIn Advances in Neural Information Processing Sys-\ntems (NeurIPS), pages 12449–12460.\nJean Carletta, Simone Ashby, Séverine Bourban,\nMike Flynn, Mael Guillemot, Thomas Hain, and\n1 others. 2005.\nThe AMI meeting corpus:\nA\npre-announcement.\nIn Proceedings of the Sec-\nond International Conference on Machine Learning\nfor Multimodal Interaction (MLMI), pages 30–44.\nSpringer.\nChang Chen, Yiming Peng, Yuan Guo, Nanxin Yang,\nShuai Zhang, Yongqiang Cui, and 1 others. 2021. Gi-\ngaspeech: An evolving, multi-domain ASR training\ncorpus with 10,000 hours of audio. In Proceedings\nof Interspeech 2021, pages 3790–3794.\nZhehuai Chen, He Huang, Andrei Andrusenko, Oleksii\nHrinchuk, Krishna Puvvada, Jason Li, and 1 others.\n2023. SALM: Speech-augmented language model\nwith in-context learning for speech recognition and\ntranslation. arXiv preprint arXiv:2310.09424.\nAlex DiChristofano, Henry Shuster, Shefali Chandra,\nand Neal Patwari. 2022. Global performance dis-\nparities between english-language accents in au-\ntomatic speech recognition.\nIn arXiv preprint\narXiv:2208.01157.\nP. Dossou, Bonaventure F.˙2025. Advancing african-\naccented english speech recognition:\nEpistemic\nuncertainty-driven data selection for generalizable\nASR models. In Proceedings of the 63rd Annual\nMeeting of the Association for Computational Lin-\nguistics: Student Research Workshop, Bangkok, Thai-\nland. Association for Computational Linguistics.\nDaniel Galvez, Vladimir Bataev, Hainan Xu, and Tim\nKaldewey. 2024. Speed of light exact greedy decod-\ning for rnn-t speech recognition models on gpu. In\nInterspeech 2024, pages 277–281.\nCong Guo, Jing Zhang, Xiaohui Ma, Yongqiang Huang,\nMike Lewis, Zhe Wei, and Shiliang Chen. 2022.\nSPGISpeech: 5,000 hours of transcribed financial\naudio for self-supervised speech representation learn-\ning. In Proceedings of Interspeech 2022, pages 3663–\n3667.\nFrançois Hernandez, Vincent Nguyen, Sahar Ghan-\nnay, Natalia Tomashenko, and Yannick Estève. 2018.\nTED-LIUM 3: Twice as much data and corpus repar-\ntition for experiments on speaker adaptation.\nIn\nSpeech and Computer (SPECOM), pages 198–208.\nIntron\nHealth.\n2025.\nAfrispeech-countries:\nCross-regional\nafrican-accented\nen-\nglish\nspeech\nbenchmark.\nhttps://\nhuggingface.co/datasets/intronhealth/\nafrispeech-countries.\nIntron Health. 2025a. Afrispeech-parliament: Tran-\nscribed parliamentary sessions from four african\nnations.\nhttps://huggingface.co/datasets/\nintronhealth/afrispeech-parliament.\nIntron\nHealth.\n2025b.\nAfri-names:\nAfrican\nnamed-entity\nread-speech\ncorpus.\nhttps:\n//huggingface.co/datasets/intronhealth/\nafri-names.\nIntron Health. 2025c.\nMed-convo-nig:\nNige-\nrian\ndoctor–patient\ntele-consultation\nspeech\ndataset.\nhttps://huggingface.co/datasets/\nintronhealth/med-convo-nig.\nAllison Koenecke, Andrew Nam, Emily Lake, Joe\nNudell, Minnie Quartey, Zion Mengesha, and 1 oth-\ners. 2020. Racial disparities in automated speech\nrecognition. Proceedings of the National Academy\nof Sciences, 117(14):7684–7689.\n"}, {"page": 11, "text": "Alexander H. Liu, Andy Ehrenberg, Andy Lo, Clément\nDenoix, Corentin Barreau, Guillaume Lample, Jean-\nMalo Delignon, Khyathi Raghavi Chandu, Patrick\nvon Platen, Pavankumar Reddy Muddireddy, Sanchit\nGandhi, Soham Ghosh, Srijan Mishra, Thomas Fou-\nbert, Abhinav Rastogi, Adam Yang, Albert Q. Jiang,\nAlexandre Sablayrolles, Amélie Héliou, and 87 oth-\ners. 2025. Voxtral. Preprint, arXiv:2507.13264.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nTobi Olatunji, Tejumade Afonja, Aditya Yadavalli,\nChris Chinenye Emezue, Sahib Singh, Bonaven-\nture F. P. Dossou, and 1 others. 2023. Afrispeech-200:\nPan-african accented speech dataset for clinical and\ngeneral domain asr. Transactions of the Association\nfor Computational Linguistics, 11:1599–1617.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur. 2015. Librispeech: An ASR corpus\nbased on public domain audio books. In Proceedings\nof the 2015 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n5206–5210.\nKrishna C. Puvvada, Piotr ˙Zelasko, He Huang, Olek-\nsii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan,\nSomshubra Majumdar, Elena Rastorgueva, Zhehuai\nChen, Vitaly Lavrukhin, Jagadeesh Balam, and Boris\nGinsburg. 2024.\nLess is more: Accurate speech\nrecognition & translation without web-scale data.\narXiv preprint arXiv:2406.19674.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine Mcleavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In Proceedings of the 40th International\nConference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pages\n28492–28518. PMLR.\nDima Rekesh, Nithin Rao Koluguri, Samuel Kriman,\nSomshubra Majumdar, Vahid Noroozi, He Huang,\nOleksii Hrinchuk, Krishna C. Puvvada, Ankur Ku-\nmar, Jagadeesh Balam, and Boris Ginsburg. 2023.\nFast conformer with linearly scalable attention\nfor efficient speech recognition.\narXiv preprint\narXiv:2305.05084.\nHadeel Saadany, Catherine Breslin, Constantin Orasan,\nand Sophie Walker. 2023. Better transcription of uk\nsupreme court hearings. In AI4AJ@ICAIL.\nMardhiyah Sanni, Tassallah Abdullahi, Devendra\nKayande, Emmanuel Ayodele, Naome Etori, Michael\nMollel, and 1 others. 2025. Afrispeech-dialog: A\nbenchmark dataset for spontaneous english conver-\nsations in healthcare and beyond. In Proceedings of\nthe 2025 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nHolger Schwenk, Loïc Barrault, Yu-An Chung, Fran-\ncisco Guzmán, Juan Pino, and the Seamless Com-\nmunication Team. 2023. Seamlessm4t: Massively\nmultilingual and multimodal machine translation. In\nProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nMarieke M. van Buchem, Hileen Boosman, Martijn P.\nBauer, Ilse M. J. Kant, Simone A. Cammel, and\nEwout W. Steyerberg. 2021. The digital scribe in clin-\nical practice: A scoping review and research agenda.\nnpj Digital Medicine, 4:57.\nLingli Wang, Ni Huang, Yili Hong, Luning Liu, Xun-\nhua Guo, and Guoqing Chen. 2023. Voice-based AI\nin call center customer service: A natural field ex-\nperiment. Production and Operations Management,\n32(4):1002–1018.\nWeiyi Wang, Chau Tran, Fahim Azhar, Henrik\nRottmann, Armand Joulin, and 1 others. 2021. Vox-\npopuli: A large-scale multilingual speech corpus for\nrepresentation, semi-, and self-supervised learning.\nIn Proceedings of Interspeech 2021, pages 993–997.\nMario Zusag, Laurin Wagner, and Bernhad Thallinger.\n2024. Crisperwhisper: Accurate timestamps on ver-\nbatim speech transcriptions. In Interspeech 2024,\npages 1265–1269.\n"}, {"page": 12, "text": "Appendix\nNoise Robustness\nModel\nSIL Short Int.SIL\nAvg.\nParakeet-tdt-0.6B-v2\n73.12\n34.50\n15.05\n40.89\nParakeet-tdt-1.1B\n78.31\n33.20\n22.35\n44.62\nParakeet-rnnt-1.1B\n211.62\n35.41\n23.86\n90.30\nCanary-1B-flash\n71.02\n54.56\n30.39\n51.32\nWhisper-large-v3\n41.90\n36.58\n22.89\n33.79\nDistil-Whisper-v3.5\n43.31\n37.38\n23.28\n34.00\nCrisperWhisper\n53.62\n36.07\n24.76\n38.82\nIBM Granite-3.3-2B\n43.90\n60.75\n32.92\n45.86\nVoxtral (Mistral)\n40.77\n57.57\n32.18\n43.51\nCanary-Qwen-2.5B\n37.63\n54.39\n31.43\n41.15\nPhi-4 MM-Instruct\n154.05 136.60\n75.71 122.45\nIntron-Sahara\n25.49\n21.47\n9.77\n18.91\nIntron-Sahara V2\n0.00\n15.98\n7.59\n7.86\nGPT-4o Transcribe\n44.48\n37.80\n18.48\n33.59\nGoogle Gemini-2.0 Flash\n45.45\n37.80\n18.48\n33.91\nGoogle Gemini-2.5 Flash\n44.27 19.964\n31.875\n32.04\nAWS Transcribe\n43.45\n35.36\n21.12\n33.98\nAzure Speech Recog.\n44.27\n32.52\n21.04\n32.61\nGoogle Chirp V3\n31.37\n40.62\n23.10\n31.70\nAverage\n58.86\n45.62\n26.18\n43.55\nTable 7: Robustness evaluation across SIL, Short, and\nInt.SIL subsets. The final column shows their mean\n(Avg.).\nPre- and Post-Processing\nAudio pre-processing\nAudio files are used exactly as distributed by the\nsource datasets; no further segmentation or con-\ncatenation is performed. A single exception con-\ncerns the NVIDIA NeMo checkpoints (parakeet-*,\ncanary-1B), which require 16kHz mono input.\nWhen a file is multi-channel or sampled above\n16kHz, it is down-mixed to mono and re-sampled\nwith sox prior to inference.\nAll other engines\n(Whisper variants, API endpoints) accept the origi-\nnal wave-forms without modification.\nTranscript pre-processing\nReference and hypothesis strings undergo a\nthree-stage normalisation pipeline, implemented\nexactly as in the public evaluation script:\n1. clean_text — lower-cases, trims whites-\npace, removes punctuation, deletes 32 variants\nof [inaudible], and removes frequent filler\nwords (uh, hmm,mmhmm,. . . ).\n2. text_to_numbers — maps number words\n(“twenty” →20) and ordinal words (“first”\n→1st) to their digit form.\n3. EnglishTextNormalizer —\napplies the\nWhisper normaliser for final case-folding and\nwhitespace cleanup.\nA sentinel token abcxyz replaces empty strings\nto avoid undefined denominators in word-error cal-\nculations.\nPost-processing for Nemo models\nNeMo/Parakeet outputs include automatically gen-\nerated punctuation. Before the three-stage nor-\nmaliser, inverse text normalisation is applied to re-\nstore standard spacing around commas and periods,\nensuring a fair comparison with punctuation-free\nreference strings.\nMetric\nWord-error rate (WER) is computed with JIWER\nWER(r, h) = S + D + I\n|r|\n,\nwhere S, D and I count substitutions, deletions\nand insertions needed to transform hypothesis h\ninto reference r.\nPrompting for Speech Augmented Language\nModels\nDefault prompts for open source speech augmented\nlanguage models where used:\n• Canary-Qwen-2.5B :\n\" T r a n s c r i b e\nthe\nf o l l o w i n g :\n{model . audio \\ _\n• Mixtral (Voxtral-Mini-3B-2507): We used its\napply_transcription_request function which\ntakes an audio file and wraps it with inbuilt\nprompts for speech transcription.\n• Google Gemini 2.0 Flash:\nThe required\nprompt according to Google API documen-\ntation was used, prompt = \"\"\" Transcribe this\nENGLISH audio. \"\"\"\n• Phi-4\nMultimodal\nInstruct:\n<|user|><|audio_1|>Transcribe\nthe\naudio\nto text<|end|><|assistant|>\n"}]}