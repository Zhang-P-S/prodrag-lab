{"doc_id": "arxiv:2601.20375", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20375.pdf", "meta": {"doc_id": "arxiv:2601.20375", "source": "arxiv", "arxiv_id": "2601.20375", "title": "LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning", "authors": ["Wei Huang", "Anda Cheng", "Yinggui Wang", "Lei Wang", "Tao Wei"], "published": "2026-01-28T08:37:34Z", "updated": "2026-01-28T08:37:34Z", "summary": "Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20375v1", "url_pdf": "https://arxiv.org/pdf/2601.20375.pdf", "meta_path": "data/raw/arxiv/meta/2601.20375.json", "sha256": "d14d4336065c5e5e1157d36b88767ddb0d83300f2fcf13544c5540081ae80fcd", "status": "ok", "fetched_at": "2026-02-18T02:20:15.120700+00:00"}, "pages": [{"page": 1, "text": "LLM-AutoDP: Automatic Data Processing via LLM Agents for\nModel Fine-tuning\nWei Huang*\nAnt Group\nBeijing, China\nhw378176@antgroup.com\nAnda Cheng*\nAnt Group\nBeijing, China\nandacheng.cad@gmail.com\nYinggui Wang\nAnt Group\nBeijing, China\nwyinggui@gmail.com\nLei Wang\nAnt Group\nBeijing, China\nshensi.wl@antgroup.com\nTao Wei\nAnt Group\nBeijing, China\nlenx.wei@antgroup.com\nABSTRACT\nLarge Language Models (LLMs) can be fine-tuned on domain-specific\ndata to enhance their performance in specialized fields. However,\nsuch data often contains numerous low-quality samples, necessi-\ntating effective data processing (DP). In practice, DP strategies are\ntypically developed through iterative manual analysis and trial-\nand-error adjustment. These processes inevitably incur high labor\ncosts and may lead to privacy issues in high-privacy domains like\nhealthcare due to direct human access to sensitive data. Thus, achiev-\ning automated data processing without exposing the raw data has\nbecome a critical challenge. To address this challenge, we propose\nLLM-AutoDP, a novel framework that leverages LLMs as agents\nto automatically generate and optimize data processing strategies.\nStarting from an initial prompt, our method generates multiple can-\ndidate strategies and iteratively refines them using feedback sig-\nnals and comparative evaluations. This iterative in-context learn-\ning mechanism enables the agent to converge toward high-quality\nprocessing pipelines without requiring direct human intervention\nor access to the underlying data. To further accelerate strategy\nsearch, we introduce three key techniques: (1) Distribution Preserv-\ning Sampling, which reduces data volume while maintaining distri-\nbutional integrity; (2) Processing Target Selection, which uses a bi-\nnary classifier to identify low-quality samples for focused process-\ning; and (3) Cache-and-Reuse Mechanism, which minimizes redun-\ndant computations by reusing prior processing results. We eval-\nuate LLM-AutoDP on five medical datasets across three model ar-\nchitectures. Results show that models trained on data processed by\nour framework achieve over 80% win rates against models trained\non unprocessed data. Compared to AutoML baselines based on\nLLM agents, LLM-AutoDP achieves approximately a 65% win rate.\nMoreover, our acceleration techniques reduce the total searching\ntime by up to 10Ã—, demonstrating both effectiveness and efficiency.\nPVLDB Reference Format:\nWei Huang*, Anda Cheng*, Yinggui Wang, Lei Wang, and Tao Wei.\nLLM-AutoDP: Automatic Data Processing via LLM Agents for Model\nFine-tuning. PVLDB, 19(5): XXX-XXX, 2026.\ndoi:XX.XX/XXX.XX\n* Equal contribution.\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication\nPVLDB Artifact Availability:\nThe source code, data, and/or other artifacts have been made available at\nhttps://github.com/secretflow/ACoLab/tree/main/Autodp-paper-code.\n1\nINTRODUCTION\nThe remarkable success of large language models (LLMs) has led to\ntheir widespread adoption across diverse domains [16, 48]. While\napplying LLMs in specialized fields such as healthcare [29, 58],\ndomain-specific fine-tuning is often necessary to achieve optimal\nperformance. A critical challenge in domain-specific fine-tuning\nlies in data acquisition: domain-specific datasets are typically col-\nlected through web scraping, crowdsourcing, or other automated\nmethods [9, 52], which frequently introduce substantial noise and\nrender the raw data unsuitable for direct use. Consequently, data\nprocessing (DP) emerges as a pivotal step in the fine-tuning pipeline.\nTraditionally, DP strategies are manually designed by iterative\nhuman observation and trial-and-error adjustments. While this can\nyield usable datasets, it suffers from two issues: (1) it incurs sig-\nnificant labor costs, and (2) more critically, it could lead to pri-\nvacy issues, especially when handling sensitive data (e.g., medi-\ncal records). As a result, an ideal solution is to generate DP strate-\ngies automatically. Automatically generating DP strategies mainly\nfaces two issues: (1) Which DP operators need to be used? (2) What\nis the execution order of these operators? Existing approaches to-\nwards automatic data processing typically leverage AutoML tech-\nnologies [11, 14, 21, 32, 34, 37], employing optimization algorithms\nsuch as three search, Bayesian optimization, or evolutionary algo-\nrithms to identify the most effective combination of basic data pro-\ncessing operations. Despite their sophistication, these methods suf-\nfer from two major challenges: (1) These optimization algorithms\ndo not comprehend the intrinsic meanings or semantics behind\nvarious DP strategies, which generally results in slow convergence;\n(2) They are not tailored for data processing in LLM fine-tuning\ntasks, thereby ignoring the problem of the large computational\noverhead associated with LLM data processing and fine-tuning.\nTo address these challenges, we propose LLM-AutoDP to lever-\nage LLMs as agents for automatic data processing. LLM-AutoDP\nrights licensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 19, No. 5 ISSN 2150-8097.\ndoi:XX.XX/XXX.XX\narXiv:2601.20375v1  [cs.LG]  28 Jan 2026\n"}, {"page": 2, "text": "operates iteratively through two interactive modules: strategy gen-\neration and strategy evaluation. The strategy generation module\nemploys LLMs as agents to create semantically meaningful and ef-\nfective DP strategies by using the initial prompt to inject domain-\nspecific knowledge into the generation process. The strategies are\nthen passed to the strategy evaluation module to get feedback for\nfurther strategy refinement. To effectively utilize feedback signals\nto improve subsequent generations, we propose a group relative\ncomparison mechanism. Specifically, we refine the prompt by com-\nbinations of multiple generated strategies and their corresponding\nfeedback signals as in-context information, guiding the LLMs to-\nward more effective strategy formulation. The evaluation module\napplies generated strategies to the raw data, followed by model\nfine-tuning and testing. To ensure both speed and reliability of\nevaluation, we introduce three key techniques: (1) Distribution-\npreserving Sampling, to reduce the amount of data in fine-tuning\nwhile maintaining the original distribution characteristics; (2) Pro-\ncessing Target Selection, to identify the most informative subsets\nof data for processing; and (3) Cache-and-Reuse Mechanism, to\nminimize redundant computation by reusing previously processed\nresults. Together, these components enable LLM-AutoDP to effi-\nciently explore the space of DP strategies and converge to high-\nquality solutions that enhance model fine-tuning performance while\nmaintaining computational efficiency.\nWe employed two state-of-the-art (SOTA) LLMs as agents and\nconducted extensive experiments on five medical datasets and three\nmodels. The experimental results show that, compared to models\ntrained on unprocessed data, the models trained on data processed\nby LLM-AutoDP achieve a win rate of over 80%. Additionally, our\nmethod achieves approximately a 65% win rate when compared to\nAutoML baseline approaches based on LLM agents. Furthermore,\nwe demonstrated through ablation experiments that the LLM-AutoDP\nframework maintains result stability even when faced with vary-\ning numbers of strategies generated in the initial rounds, and dif-\nferent models used by the agents. This also proves the robustness\nof our method. The three acceleration strategies that we propose\nreduce the total processing data time by up to 10Ã—, demonstrating\nboth effectiveness and efficiency.\n2\nRELATED WORK\nLLM Agent. The era of intelligent agents has arrived, driven by\nrevolutionary advancements in LLMs. LLM agents, characterized\nby their goal-driven behaviors and dynamic adaptation capabili-\nties, potentially represent a critical pathway toward achieving arti-\nficial general intelligence (AGI). Both academia and industry have\napplied LLM agents to a wide range of fields, including code gener-\nation [18], data security [23], social sciences [25, 31], game genera-\ntion [7], tool creation [54], model training [11], and more. MetaGPT [18]\nencodes Standard Operating Procedures (SOPs) into prompt sequences\nto enable a more streamlined workflow, allowing agents with human-\nlike domain expertise to validate intermediate results and reduce\nerrors. SELA [11] introduces Tree-Search Enhanced LLM Agents,\nan agent-based system that leverages Monte Carlo Tree Search\n(MCTS) to optimize the AutoML process. By representing pipeline\nconfigurations as trees, agents can conduct experiments intelligently\nand iteratively refine their strategies, facilitating a more effective\nexploration of the machine learning solution space. GameGPT [7]\nleverages a dual-agent collaboration and a hierarchical approach,\nusing multiple internal dictionaries to automate and enhance the\ngame development process. Currently, there is a notable lack of re-\nsearch on using LLM agents for systematic, automated processing\nof data for LLM fine-tuning. Our work aims to fill this gap.\nAutomated Data Preprocessing with AutoML. Data prepro-\ncessing can be automated to varying degrees, depending on the na-\nture of the operations and the implementation of the underlying\nmodels. The most basic level of automation still relies on traditional\nhard-coded techniques [33], while more advanced approaches lever-\nage AutoML technologies [2, 13, 27]. Although many preprocess-\ning methods are specifically designed to handle a single prepro-\ncessing task [19, 55], numerous studies [3, 20, 26, 30] aim to achieve\nmultiple preprocessing functions simultaneously. For instance, Auto-\nPrep [3] performs automatic missing data imputation, data type\ndetection, duplicate removal, categorical data encoding, and fea-\nture scaling. Some newer methods, such as AutoData [28], DataAs-\nsist [15], BioAutoMATED [42], Atlantic [36], and DiffPrep, are de-\nsigned as dedicated preprocessing plugins that can be integrated\ninto standard AutoML frameworks [17, 38]. For example, Auto-\nData [28] implements a specialized data processing module that\noperates as an end-to-end mechanism within a standard AutoML\nframework, enabling the execution of various data preprocessing\nand augmentation tasks, including data acquisition, labeling, clean-\ning, and enhancement. These methods are typically applied to tra-\nditional models and small datasets, where dozens of iterations are\noften required to derive high-quality data processing strategies.\nHowever, when applied to LLMs, these methods face significant\nchallenges. Although there are some recent automated data pro-\ncessing frameworks for LLM training [5, 6], these methods do not\nfully use LLM as an agent for fully automated strategy selection.\nLLMs often require substantial time to process data, making the\napplication of these preprocessing techniques for automatic data\nprocessing on LLMs highly time-consuming and thus impractical.\n3\nFRAMEWORK AND KEY MECHANISMS\n3.1\nFramework of AutoDP\nThe overall framework of LLM-AutoDP is depicted in Figure 1. This\nframework comprises two interrelated modules: (1) a strategy gen-\neration module that employs LLMs as agents to automatically de-\nsign DP strategies, and (2) a strategy evaluation module that as-\nsesses the strategies through model training and evaluation.\nLLM-based Data Processing Strategy Generation. As shown\nin the left part of Figure 1, LLM-AutoDP leverages pre-trained LLMs\nas agents to automatically generate DP strategies. We design a set\nof prompt templates and employ prompt engineering techniques to\ninject domain-specific knowledge into the LLMsâ€™ generation pro-\ncess. Guided by the prompts, LLMs produce diverse DP strategies\nthat specify both the selection and sequencing of data processing\ntechniques. These strategies are then executed in the model train-\ning environment, where they are iteratively refined based on per-\nformance feedback. A key challenge of this module is how the\nagent can effectively utilize feedback to improve strategy gener-\nation. To address this, we propose a novel feedback mechanism\n"}, {"page": 3, "text": "Meta Agent\nDataset screening \nmodel F\nDistribution-Preserving Sampling (DPS)\nğ·!\"#$%\nğ·&'()*\nğ·),!\"#$%\nğ·),&'()*\nsampling\nğ·)\nMerging\nPrompt\n1. Input initial prompt\nOperator pool\nDP strategies 1\nDP strategies N\nâ€¦â€¦\n2. Generate multiple \ndistinct DP strategies\n3. Process the data \naccording to different DP\nstrategies.\nDataset\nDataset\nScore\nScore\n4. Execute the evaluation \nstep to obtain feedback \nscores.\nPrompt\n5. Group different DP strategies and \ntheir corresponding scores into a set \nto refine the prompt.\nLoop ( step 2 -> 3 -> 4 -> 5 ) until the meta agent \noutputs the optimal DP strategies.\nCache-and-Reuse Mechanism (CRM)\nDP strategies 1\nDP strategies N\nIterative feedback and strategy generation\nEvaluation\nDataset\nTraining\nInference\nScore\nAcceleration methods and evaluation\nCleaning\nGeneration\nSelection\nOptimization\nunprocessed \nDataset\nTraining\nInference\ncomputation\nF\nğ·)\nğ·,-.,&'()/\nğ·,-.\nF\nğ·,-.,&'()/\nProcessing Target Selection(PTS)\nğ·,-.,,0*1&\nğ·,-.,,0*1&\nDataset pool (Cache)\na new strategy\nsearch\nReuse\nBased on Strategy 1, only the\nDP strategy needs to be executed.\nFigure 1: The overall framework of LLM-AutoDP. The left part utilizes an LLM as an agent to iteratively refine the prompt,\nenabling the agent to generate high-quality data processing strategies. The right part consists of an evaluation module for\ngenerating strategy feedback scores and an acceleration module for speeding up data processing. The LLM iteratively controls\nthe interaction between the two parts to select high-quality processing strategies.\nTable 1: Operator taxonomy with four categories\nData Processing\nOperations\nCleaning\nApply MinHashLSH to eliminate duplicate\nsamples. [40]\nRemove low-level noise, such as HTML tags.\nMaintain the ratio of special characters in the\nsample within a specific range.\nKeep the number of tokens in the sample\nwithin a specific range.\nKeep the sample of word-level n-gram repeti-\ntion ratios within a specific range.\nOptimization\nOptimize the questions in the sample. [4]\nOptimize the answer in the sample. [4]\nOptimize both the questions and answers in the\nsample. [4]\nGeneration\nGenerate the missing questions based on the\nshots. [44]\nGenerate the missing answer based on the\nshots. [44]\nGenerate question-answer pairs based on the\nshots. [44]\nSelection\nSelect high-quality data based on gradient in-\nformation. [45]\nbased on group relative comparison, integrated with in-context\nlearning, to guide LLMs toward effective strategy formulation.\nStrategy Evaluation via Model Training and Evaluation.\nAs shown in the right part of Figure 1, to evaluate the sampled DP\nstrategies, LLM-AutoDP applies the generated strategies to process\nthe give data, followed by finetuning and evaluating a pre-trained\nmodel on the processed data. The evaluation results are then fed\nback to the strategy generation module to inform the next iteration\nof strategy optimization. This closed-loop interaction enables the\nDP strategies to dynamically adapt to the model training dynamics,\nthereby improving alignment between data processing and learn-\ning objectives. The central challenge of the evaluation module is\nhow to achieve fast yet reliable strategy evaluation. To this end,\nin section 3.3, we introduce three key techniques to accelerate the\nevaluation process while preserving its fidelity.\n3.2\nLLM-based Strategy Generation\nLLM-AutoDP leverages pre-trained LLMs as agents to automati-\ncally generate DP strategies. To achieve this, we employ prompt\nengineering techniques and carefully design a series of instruc-\ntion templates. By incorporating task descriptions, input-output\nformats, and domain-specific knowledge through prompts, we con-\nstrain the LLMsâ€™ generation process within a meaningful and prac-\ntically feasible search space. The strategy generation process runs\niteratively. In the initial round, strategies are generated based solely\non our predefined instructions. In subsequent rounds, feedback\nfrom the strategy evaluation module is incorporated into the prompt\nto refine and optimize the generated strategies.\n"}, {"page": 4, "text": "Search Space. Considering four data processing teams, we ex-\nplored the various permutation-combination scenarios when se-\nlecting 1, 2, 3, or 4 teams while taking order into account. Specif-\nically, when selecting only one team, there are 4 options; when\nselecting two teams, there are 12 possibilities; when selecting three\nteams, there are 24 permutations; and when selecting all four teams,\nthere are also 24 permutations. In total, this amounts to 64 differ-\nent selection spaces. In addition, we have added an extra option:\nâ€No Processing Required for Original Data.â€\nInitial Round. In the initial prompt, we first specify the ob-\njective: to design optimal DP strategies that improve data quality\nto achieve best model performance. We then describe the set of\navailable DP operators and their corresponding functional groups,\nalong with detailed explanations of each operatorâ€™s purpose and\nexpected behavior. See Table 1 for more details on these opera-\ntors. Next, we instruct the LLMs to initialize multiple distinct DP\nstrategies. Each DP strategy must clearly define the sequence of\nDP operators, resulting in a diverse set of initial DP strategies. We\nencourage the LLM to start with relatively small team composi-\ntions across different strategies to facilitate early exploration and\nunderstand the impact of different data processing methods and\ntheir orderings. This allows us to gather preliminary insights into\nthe effectiveness of individual components and their interactions,\nforming a foundation for further optimization. For a detailed initial\nprompt, please refer to section 6.1.\nIterative Optimization Rounds. Starting from the second round,\nthe prompts provided to the LLMs not only include task instruc-\ntions and operator descriptions but also incorporate performance\nfeedback from previously evaluated strategies. Let S(ğ‘¡) = {ğ‘“(ğ‘¡)\nğ‘˜\n|\nğ‘˜= 1, . . . , ğ¾(ğ‘¡)} denote the set of ğ¾(ğ‘¡) DP strategies generated in\nthe ğ‘¡-th round. Let ğ‘…(Â·) represent the strategy evaluation function.\nThen, the evaluation results for all strategies in S(ğ‘¡) are R (ğ‘¡) =\n{ğ‘Ÿ(ğ‘¡)\nğ‘˜\n= ğ‘…(ğ‘“(ğ‘¡)\nğ‘˜\n) | ğ‘˜= 1, . . . , ğ¾(ğ‘¡)}. Additionally, we compute the\nbaseline performance ğ‘Ÿ0 = ğ‘…(None), which corresponds to train-\ning the model on the unprocessed dataset. In the (ğ‘¡+ 1)-th round,\nthe agent receives both R (ğ‘¡) and ğ‘Ÿ0, and computes a normalized\nrelative improvement score for each strategy as:\nğ‘ (ğ‘¡)\nğ‘˜\n= ğ‘Ÿ(ğ‘¡)\nğ‘˜\nâˆ’ğ‘Ÿ0, âˆ€1 â‰¤ğ‘˜â‰¤ğ¾(ğ‘¡).\n(1)\nThis score quantifies how much a given DP strategy improves\nor degrades model performance compared to the baseline. A pos-\nitive score (ğ‘ (ğ‘¡)\nğ‘˜\n> 0) indicates a beneficial effect, while a nega-\ntive score (ğ‘ (ğ‘¡)\nğ‘˜\n< 0) suggests degradation. The higher the score,\nthe more effective the strategy is at enhancing data quality for\ndownstream training. We refine the prompt by injecting different\nDP strategies and their corresponding feedback scores as a group\ninto the next round of prompting. Using inter-group relative com-\nparison, we require the agent to compare and analyze the differ-\nent DP strategy configurations and their feedback scores within\nthe group. Through this process, the agent can identify superior\ndata processing strategy configurations and, based on the insights\ngained, adjust the DP strategies, execution order, and the number\nof strategies in the next round. The goal is to progressively refine\nthe DP strategies toward optimal performance. When generating\nnew strategies, the agent not only considers the absolute perfor-\nmance of previous strategies but also learns from their relative\nimprovements. If the agent determines that no further significant\nimprovements can be made by adjusting group composition or or-\ndering, it may terminate the iteration and return the most effective\nstrategy found so far. An example of a prompt for the iterative op-\ntimization round can be found in section 6.1.\n3.3\nEfficient Strategy Evaluation\nThe strategy evaluation module aims to evaluate the effectiveness\nof the generated strategies and feed back the results to the agent\nfor iterative optimization of the strategy. To evaluate the perfor-\nmance of a candidate data processing strategy ğ‘“, we insert it into\nthe finetuning process of a pre-trained LLMğ‘Šğ‘ğ‘Ÿğ‘’, conduct finetun-\ning for ğ‘ğ‘“ğ‘¡epochs on the training set D that needs to be precessed,\nand then evaluate model performance on the validation set Dğ‘£ğ‘ğ‘™\nusing some evaluation function Eğ‘£ğ‘ğ‘™to get results as feedback to\nagent. This process can be formulated as follows:\nğ‘…(ğ‘“) = Eval\n(\nFT (ğ‘Šğ‘ğ‘Ÿğ‘’, ğ‘“(D), ğ‘ğ‘“ğ‘¡\n), Dğ‘£ğ‘ğ‘™\n)\n(2)\nHowever, since the evaluation process includes LLM training,\nit is very time-consuming, especially when the model has a large\nnumber of parameters. In addition, since we need to evaluate the\nstrategy in each round of iterative optimization, which involves\nfine-tuning and testing of the LLM in each round, it is easy for\nthe entire process to become unacceptably time-consuming. There-\nfore, the key problem of the evaluation module is how to achieve\nfast yet reliable strategy evaluation. To solve this problem, we pro-\npose three technologies: Processing Target Selection, Distribution-\nPreserving Sampling, and Cache-and-Reuse Mechanism to reduce\nthe time consumption of the strategy evaluation process.\n3.3.1\nTraining of the Binary Screening Model. A general approach\nto evaluating the quality of conversational data is to use prompt\nengineering to elicit answers from SOTA LLMs. However, due to\ntheir typically large parameter sizes, these models often have long\ninference times, which could introduce additional time overhead\nwhen applied within our framework. Inspired by knowledge distil-\nlation [47], we collected approximately 2 million pieces of medical\nand general conversational open-source data, which do not overlap\nwith our training data. Following the prior work [10], we scored\nthe data on a scale of 1 to 5. To ensure diversity in the training\ndata, we randomly sampled a certain amount of data from each\nscoring interval, forming a total of 200,000 training samples. Next,\nwe used Qwen3-32B [48] to annotate these 200,000 samples, assign-\ning one of two labels: â€High-quality data, no further processing re-\nquiredâ€ or â€Low-quality data, further processing needed.â€ Finally,\nwe trained Qwen2.5-7B [48] on this dataset, minimizing the fol-\nlowing loss function to obtain the final binary classification model\nfor assessing data quality. This process aims to provide an efficient\nmechanism for accurately screening the quality of conversational\ndata within a relatively short time frame.\nL(ğœƒ) = âˆ’E(ğ‘¥,ğ‘¦)âˆ¼Dğ‘ğ‘™ğ‘œ\n[ ğ‘‡âˆ‘\nğ‘¡=1\nlog ğ‘ƒğœƒ(ğ‘¦ğ‘¡| ğ‘¥,ğ‘¦<ğ‘¡)\n]\n(3)\nWhere ğ‘‡represents the length of the target output sequence, and\nğ·ğ‘ğ‘™ğ‘œdenotes the 200,000 training samples we collected.\n"}, {"page": 5, "text": "3.3.2\nDistribution-Preserving Sampling (DPS). We denote the raw\nand unprocessed dataset as Dğ‘œğ‘Ÿğ‘–. The key question of our evalu-\nation is whether it is necessary to use the entire original dataset\nDğ‘œğ‘Ÿğ‘–for model training. The goal of model training and evaluation\nis to assess the performance of a given DP strategy and compare\nit against other candidates, to identify the most effective strategy.\nTherefore, rather than requiring the full dataset, we only need a\nsubset of samples that sufficiently approximates the underlying\ndata distribution. To this end, we propose a sampling method that\npreserves the statistical properties of the original data distribution.\nGiven a target sample size ğ‘and a source dataset Ëœ\nD, we construct\na reduced dataset Dğ‘ through iterative sampling until it contains\nexactly ğ‘samples. Initially, Dğ‘ is empty. At each iteration, we\nselect one sample from Ëœ\nD \\ Dğ‘ and add it to Dğ‘ . To ensure the\nsampled subset retains the characteristics of the original data dis-\ntribution, we employ an embedding model ğ¸[46] to map each sam-\nple ğ‘¥âˆˆËœ\nD to a high-dimensional embedding vector eğ‘¥= ğ¸(ğ‘¥). We\nthen select the sample whose embedding maximizes the similarity\nwith all unselected samples, defined as:\nËœ\nD (ğ‘–)\nğ‘ \n= Ëœ\nD (ğ‘–âˆ’1)\nğ‘ \nâˆª{ğ‘¥ğ‘–},\nğ‘¥ğ‘–= arg max\nğ‘¥\nâˆ‘\nğ‘âˆˆËœ\nD\\ Ëœ\nD (ğ‘–âˆ’1)\nğ‘ \neâŠ¤\nğ‘¥eğ‘\nâˆ¥eğ‘¥âˆ¥Â· âˆ¥eğ‘âˆ¥\n(4)\nWe use the binary screening model ğ¹to classify Ëœ\nD into Dğ‘ğ‘™ğ‘’ğ‘ğ‘›or\nDğ‘›ğ‘œğ‘–ğ‘ ğ‘¦. This process can be formulated as follows:\nDğ‘ğ‘™ğ‘’ğ‘ğ‘›=\n{\nğ‘¥|ğ¹(ğ‘¥) = 0, âˆ€ğ‘¥âˆˆËœ\nD\n}\n,\nDğ‘›ğ‘œğ‘–ğ‘ ğ‘¦=\n{\nğ‘¥|ğ¹(ğ‘¥) = 1, âˆ€ğ‘¥âˆˆËœ\nD}\n(5)\nTo ensure that the sampled dataset accurately reflects the propor-\ntion of noisy samples of original data, we perform sampling sepa-\nrately on Dğ‘ğ‘™ğ‘’ğ‘ğ‘›and Dğ‘›ğ‘œğ‘–ğ‘ ğ‘¦using Eq. 4. Let Ëœ\nDğ‘ ,clean and Ëœ\nDğ‘ ,noisy\ndenote the subsets sampled from Dğ‘ğ‘™ğ‘’ğ‘ğ‘›and Dğ‘›ğ‘œğ‘–ğ‘ ğ‘¦, respectively.\nThe final sampled dataset Dğ‘ is then constructed as Dğ‘ = Ëœ\nDğ‘ ,noisyâˆª\nËœ\nDğ‘ ,clean. This mechanism enables the preservation of the overall\nstructure of the data manifold on both Dğ‘ğ‘™ğ‘’ğ‘ğ‘›and Dğ‘›ğ‘œğ‘–ğ‘ ğ‘¦. As a re-\nsult, Dğ‘ provides a compact yet representative approximation of\nDğ‘œğ‘Ÿğ‘–, enabling efficient and reliable DP strategy evaluation.\n3.3.3\nProcessing Target Selection (PTS). When the agent generates\na strategy, such as â€Data Optimization â€“> Data Selection,â€ we need\nto utilize an SOTA LLM to optimize all the data, and then compute\ngradient information, which inevitably leads to longer processing\ntimes. We consider that when sequentially processing the data ac-\ncording to the DP strategy, not all the data needs to be processed.\nUsually, there are some clean and high-quality data, which can\nbe directly used for model training without additional processing.\nTherefore, we consider finding this part of the data in advance and\nskipping the processing on this data, thereby saving the time spent\non processing this part of the data. To this end, we use the binary\nscreening model ğ¹, which is trained using pipeline in section 3.3.1,\nto determine whether certain data in the current dataset needs to\nbe optimized. Using this model, we can divide the dataset into two\ndisjoint datasets, which contain clean data and data that need to\nbe cleaned and optimized, respectively, as follows:\nDğ‘ğ‘¢ğ‘Ÿ,ğ‘ğ‘™ğ‘’ğ‘ğ‘›=\n{\nğ‘¥|ğ¹(ğ‘¥) = 0, âˆ€ğ‘¥âˆˆDğ‘ğ‘¢ğ‘Ÿ\n}\n,\nDğ‘ğ‘¢ğ‘Ÿ,ğ‘›ğ‘œğ‘–ğ‘ ğ‘¦=\n{\nğ‘¥|ğ¹(ğ‘¥) = 1, âˆ€ğ‘¥âˆˆDğ‘ğ‘¢ğ‘Ÿ}\n(6)\nwhere ğ·ğ‘ğ‘¢ğ‘Ÿrepresents the data processed in the current step ac-\ncording to the DP strategy. When using a DP strategy ğ‘“for pro-\ncessing, we only process the data in Dğ‘ğ‘¢ğ‘Ÿ,ğ‘›ğ‘œğ‘–ğ‘ ğ‘¦or its subset one by\none. The data in Dğ‘ğ‘¢ğ‘Ÿ,ğ‘ğ‘™ğ‘’ğ‘ğ‘›can be directly used for model training.\n3.3.4\nCache-and-Reuse Mechanism (CRM). LLM-AutoDP employs\nan iterative optimization framework to discover effective data pro-\ncessing strategies. During this process, we observe that the same\nor partially overlapping strategies may be sampled across different\nrounds of optimization. This motivates us to design a cache-and-\nreuse mechanism to reduce redundant computation and accelerate\nstrategy evaluation.\nSpecifically, in the ğ‘¡-th optimization round, the agent generate\nğ¾(ğ‘¡) DP strategies as S(ğ‘¡) = {ğ‘“(ğ‘¡)\nğ‘˜\n| ğ‘˜= 1, . . . , ğ¾(ğ‘¡)}. For each\nstrategy ğ‘“(ğ‘¡)\nğ‘˜\nâˆˆ{ğ‘“(ğ‘¡)\nğ‘˜\n| ğ‘˜= 1, . . . , ğ¾(ğ‘¡)}, we apply it to the sampled\ndataset Dğ‘ to obtain the processed dataset D (ğ‘¡)\nğ‘˜\n= ğ‘“(ğ‘¡)\nğ‘˜\n(Dğ‘ ), which\nis stored in the dataset pool C(ğ‘¡) = {D (ğ‘¡)\nğ‘˜\n| ğ‘˜= 1, . . . , ğ¾(ğ‘¡)}. In the\n(ğ‘¡+ 1)-th round, when evaluating a new strategy ğ‘“(ğ‘¡+1)\nğ‘˜\n, we first\nsearch the previously recorded strategy pool {ğ‘“(ğ‘–)\nğ‘—\n| 1 â‰¤ğ‘–â‰¤ğ‘¡, 1 â‰¤\nğ‘—â‰¤ğ¾(ğ‘–)} for the longest prefix of ğ‘“(ğ‘¡+1)\nğ‘˜\n. If such a prefix strategy\nğ‘“(ğ‘–)\nğ‘—\nexists and is identical to the prefix of ğ‘“(ğ‘¡+1)\nğ‘˜\n, we can decompose\nğ‘“(ğ‘¡+1)\nğ‘˜\nas:\nğ‘“(ğ‘¡+1)\nğ‘˜\n= ğ‘“(ğ‘¡+1)\nğ‘˜,prefix + ğ‘“(ğ‘¡+1)\nğ‘˜,suffix = ğ‘“(ğ‘–)\nğ‘—\n+ ğ‘“(ğ‘¡+1)\nğ‘˜,suffix,\n(7)\nwhere ğ‘“(ğ‘¡+1)\nğ‘˜,prefix corresponds to the existing strategy ğ‘“(ğ‘–)\nğ‘—\n, and ğ‘“(ğ‘¡+1)\nğ‘˜,suffix\ndenotes the newly added operations. Given this decomposition, the\nprocessed dataset ğ‘“(ğ‘¡+1)\nğ‘˜\n(Dğ‘ ) is equivalent to applying the suffix\nğ‘“(ğ‘¡+1)\nğ‘˜,suffix to the precomputed dataset D (ğ‘–)\nğ‘—\nas:\nğ‘“(ğ‘¡+1)\nğ‘˜\n(Dğ‘ ) = ğ‘“(ğ‘¡+1)\nğ‘˜,suffix(D (ğ‘–)\nğ‘—).\n(8)\nTherefore, instead of reprocessing the entire dataset from scratch\nusing ğ‘“(ğ‘¡+1)\nğ‘˜\n, we can directly reuse D (ğ‘–)\nğ‘—\nand only apply the suffix\noperations. This cache-and-reuse mechanism enables LLM-AutoDP\nto efficiently explore complex DP strategies while minimizing re-\npeated computation, leading to lower overall training cost.\n4\nEXPERIMENTS\n4.1\nExperimental Settings\n4.1.1\nDatasets, Metrics, and Models. We conduct experiments on\nmedical QA datasets, as healthcare-related data is typically large\nin scale, highly sensitive, and often noisy, thereby presenting a\npressing need for automatic data processing. Specifically, we use\nfour medical dialogue and QA datasets: cMedQA2 [56], Chinese-\nmedical-dialogue (CMD) [41], Huatuo-26M-Lite(Huatuo) [24],\nand Medical-O1-Reasoning-SFT(Medical-O1) [8]. Although these\ndata have been simply cleaned during the collection process, our\nexperiments show that it is still necessary to comprehensively pro-\ncess these data before using them. For each dataset, we randomly\nsample 20,000 instances for training and 1,000 for evaluation. In\n"}, {"page": 6, "text": "Table 2: This table demonstrates the optimal data process-\ning strategy output by the agent after several rounds of it-\neration. In most experiments, LLM-AutoDP autonomously\nterminates after only 4 or 5 rounds.\nDatasets\nTermination Round\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nCMD\n4\n4\ncMedQA2\n5\n4\nMedical-O1\n5\n5\nHuatuo\n4\n4\nHuatuo-100\n4\n3\naddition, we randomly sampled 100 data points from Huatuo-26M-\nLite to form a small dataset, Huatuo-26M-Lite-100(Huatuo-100), to\nverify the effect of our method on a small dataset. On the training\nsets, we apply various automatic data processing methods, and sub-\nsequently fine-tune three SOTA pre-trained LLMs on the processed\ndata: Qwen2.5-7B-Instruct [48], Llama3.1-8B-Instruct [16], and\nGemma-2-9B-Chat [39]. The fine-tuned models are then used\nto generate answers to the questions in the test sets. We follow\nthe evaluation methods of medical LLMs as described in [35, 49,\n53]. We report win/tie/loss rates of LLM-AutoDP against different\nbaseline methods across all test sets. In the results, â€Our Winsâ€,\nâ€Tiesâ€, and â€Our Lossesâ€ represent the proportion of samples where\nour method performs better, the same, or worse than the baseline,\nrespectively. The prompt for the win/tie/loss rates follows prior\nwork [49] and details are shown in section 6.1. Specifically, we pair\nmodels fine-tuned on data processed by LLM-AutoDP with those\ntrained on data processed by other methods, and use two judge\nmodels GPT-4 [1] and Baichuan-M1-14B-Instruct [43] to de-\ntermine which model performs better on each question.\n4.1.2\nBaselines. We compare LLM-AutoDP with several approaches\nfor automated DP:\n(1) No-process. Using original unprocessed data.\n(2) All-process. Using all data processing operators. We use all\noperators in Table 1 to form a strategy in the order of data\ndata cleaning â†’data optimization â†’data generation â†’\ndata selection.\n(3) RS. Random search. Randomly selecting 5 strategies from\nall possible strategy combinations for evaluation and using\nthe best one as the RS result.\n(4) SELA [11], an AutoML framework that integrates LLMs\nwith Monte Carlo Tree Search to automate traditional ma-\nchine learning workflows. We modify the search target of\nSELA and only perform strategy search for the data pro-\ncessing process.\n4.1.3\nImplementation Details. We utilize two agent models for strat-\negy generation: Qwen3-32B [48] and DeepSeek-R1-Distill-Llama-\n70B [12] (Due to the potential privacy concerns associated with\nthe data, we prefer high-performance LLMs that can be deployed\nlocally, rather than relying on callable APIs). The sampling temper-\nature is set to 0.6. We sample 4 strategies at the initial round.\nFigure 2: Visualization of the distribution of the original\ndata and the data sampled by 20% on Huatuoâ€‘26Mâ€‘Lite\ndataset. It can be seen that the distribution of the sampled\ndata is consistent with that of the original data.\nFor sampling rate in DPS, previous study [59] have shown that\nusing 20% of the data is sufficient to ensure the performance of\nthe trained model. Below 20% will negatively affect performance.\nTherefore, we ensure that 20% of the data from each dataset is sam-\npled for DPS. We visualize the distribution of the sampled data and\nthe original data in Figure 2. It can be seen that the two sets of dis-\ntributions are consistent, indicating that our sampling process does\nnot reduce the generalization ability.\nThe fine-tuning process is conducted on Qwen2.5-1.5B-Instruct\nmodel. For fine-tuning, we set the learning rate to 1e-5 and the\nbatch size to 64. Each dataset was trained for 3 epochs. The AdamW\noptimizer was used for fine-tuning. We employed Swift [57] as the\ntraining platform and vLLM [22] for inference. For iteration-based\nmethods, SELA and our proposed LLM-AutoDP, we restrict the\nnumber of policy selection iterations to at most 5 rounds. This\nlimitation is imposed due to the high computational cost for fine-\ntuning LLMs. Exceeding this number would result in prohibitive\ntime and resource consumption, making the policy selection pro-\ncess impractical. All experiments are conducted with 16 NVIDIA\nA100 GPUs with 80G memory.\n4.2\nMain Results Comparison\nIn Table 3 and Table 4, we report the win/tie/loss rates of our pro-\nposed method, LLM-AutoDP, compared to other approaches, using\nGPT-4 and Baichuan-M1-14B-Instruct as judges, respectively. The\nresults demonstrate that LLM-AutoDP significantly outperforms\nthe unprocessed baseline No-Process across most datasets. For in-\nstance, when fine-tuning various models on the Chinese-medical-\ndialogue, cMedQA2, and Huatuo-26M-Lite-100 datasets under GPT-\n4 evaluation, the model trained on data processed by LLM-AutoDP\nachieves a win rate exceeding 80% over the one trained on raw data.\nSimilar trends are observed with Baichuan-based judgments. An\nexception occurs in the Medical-O1-Reasoning-SFT dataset com-\nparison, where all comparisons against No-Process result in ties\n(Tie = 1.0). This is attributed to the high quality of the original data,\nwhich leads the LLM to determine that no additional processing is\nnecessary, resulting in identical training inputs. Compared to the\niterative method SELA, LLM-AutoDP achieves a higher win rate\n"}, {"page": 7, "text": "Table 3: Win/tie/loss results of LLM-AutoDP against different automatic data processing baselines using GPT-4 as judge. â€Our\nWinsâ€,â€Tiesâ€, and â€Our Lossesâ€ represent the proportion of samples where our method performs better, the same, or worse\nthan the baselines, respectively. Results against No-Process result on Medical-O1-Reasoning-SFT are ties (Tie = 1.0), which is\ndue to the high quality of the original data, leading the agents to determine that no additional processing is required.\nDatasets\nLLM-AutoDP vs.\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\nOur Wins\nTies\nOur Losses\nOur Wins\nTies\nOur Losses\nOur Wins\nTies\nOur Losses\nChinese-\nmedical-\ndialogue\nNo-Process\n0.8937\n0.0057\n0.1005\n0.8549\n0.0000\n0.1451\n0.8718\n0.0247\n0.1035\nAll-Process\n0.5015\n0.0922\n0.4063\n0.5062\n0.0917\n0.4021\n0.5543\n0.1217\n0.3240\nRS\n0.7089\n0.0461\n0.2450\n0.6931\n0.0395\n0.2674\n0.7228\n0.0288\n0.2484\nSELA\n0.6945\n0.0404\n0.2651\n0.6587\n0.0606\n0.2807\n0.7125\n0.0404\n0.2471\ncMedQA2\nNo-Process\n0.8463\n0.0130\n0.1407\n0.8380\n0.0087\n0.1533\n0.8818\n0.0087\n0.1094\nAll-Process\n0.6583\n0.0477\n0.2940\n0.6137\n0.0359\n0.3504\n0.7130\n0.0719\n0.2151\nRS\n0.6733\n0.0302\n0.2965\n0.6777\n0.0302\n0.2921\n0.7335\n0.0559\n0.2106\nSELA\n0.5953\n0.0377\n0.3670\n0.6233\n0.0422\n0.3345\n0.6337\n0.0325\n0.3338\nMedical-O1-\nReasoning-\nSFT\nNo-Process\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\nAll-Process\n0.4724\n0.0944\n0.4332\n0.4538\n0.0713\n0.4749\n0.4766\n0.0919\n0.4315\nRS\n0.4969\n0.0925\n0.4106\n0.5111\n0.0833\n0.4056\n0.5252\n0.0749\n0.3999\nSELA\n0.4748\n0.0933\n0.4319\n0.4698\n0.0969\n0.4333\n0.5339\n0.0738\n0.3923\nHuatuo-26M-\nLite\nNo-Process\n0.6125\n0.0225\n0.3650\n0.6250\n0.0050\n0.3700\n0.6638\n0.0174\n0.3188\nAll-Process\n0.5325\n0.0475\n0.4175\n0.5418\n0.0325\n0.4257\n0.5724\n0.0475\n0.3801\nRS\n0.5150\n0.0500\n0.4350\n0.5590\n0.0500\n0.3910\n0.5743\n0.0500\n0.3757\nSELA\n0.5325\n0.0475\n0.4175\n0.5418\n0.0325\n0.4257\n0.5724\n0.0475\n0.3801\nHuatuo-26M-\nLite-100\nNo-Process\n0.8312\n0.0250\n0.1438\n0.8325\n0.0025\n0.1650\n0.8237\n0.0000\n0.1763\nAll-Process\n0.5575\n0.1150\n0.3275\n0.5750\n0.0275\n0.3975\n0.5773\n0.0225\n0.4002\nRS\n0.5950\n0.1100\n0.2950\n0.6000\n0.0250\n0.3725\n0.6137\n0.0275\n0.3588\nSELA\n0.6125\n0.0825\n0.3025\n0.5475\n0.0275\n0.4250\n0.5766\n0.0425\n0.3809\non the majority of datasets and models. Only in a few casesâ€”such\nas fine-tuning Llama3.1-8B on Medical-O1-Reasoning-SFTâ€”does\nLLM-AutoDP slightly underperform. Notably, despite both being\niterative strategy selection methods, LLM-AutoDP converges faster\nthan SELA. In most experiments, LLM-AutoDP autonomously ter-\nminates after only 4 or 5 rounds, experiment results reported in ta-\nble 2. In contrast, SELA remains far from convergence after 5 itera-\ntions [11] and performs only marginally better than random search.\nThis highlights a key advantage of using LLMs as decision agents\nover traditional AutoML optimization techniques: LLMs can rapidly\nconverge to more effective strategies. This efficiency is particularly\nvaluable in LLM fine-tuning scenarios, where each iteration incurs\nsubstantial time and computational cost. Taken together, these re-\nsults confirm that LLM-AutoDP offers a faster and more effective\nautomated data processing solution.\n4.3\nEfficiency Improvement of Acceleration\nTechniques\nTo comprehensively evaluate the speedup effects of our proposed\nacceleration techniques (DPS, PTS, and CRM), we conduct ablation\nstudies on four datasets. The results in Table 5, demonstrate the\nimpact of different combinations of these techniques on the total\ntime cost of the strategy search process. As shown in Table 5, the\nbaseline without any acceleration (w/o DPS, PTS, CRM) requires\napproximately 40â€“50 hours to complete the full search across all\ndatasets. This highlights the high computational burden associated\nwith exhaustive exploration of the data processing strategy space.\nIn contrast, introducing DPS alone significantly reduces the time\nrequired for strategy search by dynamically pruning unpromising\nbranches early in the search process. For example, on the Chinese-\nmedical-dialogue dataset, the total search time drops from 41.7 to\n9.6 hoursâ€”a reduction of 76.9%. Across all datasets, DPS consis-\ntently reduces the time cost by 76.3%â€“78.7%, demonstrating its ef-\nfectiveness in efficiently narrowing down the dataset size for strat-\negy searching. When PTS is further introduced alongside DPS, we\nobserve an additional reduction in search time by 2.2%â€“3.3%. No-\ntably, on the Medical-O1-Reasoning-SFT dataset, the combination\nof DPS and PTS reduces the total search time from 50.0 to 4.3 hours,\nachieving a cumulative saving of 91.4%. Similarly, on Huatuo-26M-\nLite, the time is reduced to 6.7 hours, yielding an 81.9% saving.\nThe results indicate that PTS effectively reduces the time for\ndata processing by filtering the samples that do not need process-\ning. Finally, integrating all three components (DPS, PTS, and CRM)\nachieves the highest speedup. On the Medical-O1-Reasoning-SFT\ndataset, the total search time is further reduced to just 2.5 hours\nâ€”an impressive 95.0% saving compared to the baseline. Across all\nfour datasets, the combined use of the three techniques reduces the\ntotal search time by 88.2%â€“95.0%, clearly demonstrating their com-\nplementary roles in enhancing efficiency. In summary, these re-\nsults validate the effectiveness of our proposed acceleration meth-\nods in substantially reducing the computational overhead of the\nstrategy search process, without compromising the quality of dis-\ncovered data processing pipelines.\n"}, {"page": 8, "text": "Table 4: Win/tie/loss results of LLM-AutoDP against different automatic data processing baselines using Baichuan-M1-14B-\nInstruct as judge. â€Our Winsâ€,â€Tiesâ€, and â€Our Lossesâ€ represent the proportion of samples where our method performs better,\nthe same, or worse than the baselines, respectively. Results against No-Process result on Medical-O1-Reasoning-SFT are ties\n(Tie = 1.0), which is due to the high quality of the original data, leading the agents to determine that no processing is needed.\nDatasets\nLLM-AutoDP vs.\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\nOur Wins\nTies\nOur Losses\nOur Wins\nTies\nOur Losses\nOur Wins\nTies\nOur Losses\nChinese-\nmedical-\ndialogue\nNo-Process\n0.8694\n0.0288\n0.1018\n0.8794\n0.0288\n0.0918\n0.8709\n0.0158\n0.1133\nAll-Process\n0.4021\n0.2060\n0.3919\n0.4125\n0.2488\n0.3387\n0.3967\n0.2562\n0.3471\nRS\n0.6816\n0.1153\n0.2031\n0.6516\n0.1334\n0.2150\n0.6771\n0.1013\n0.2216\nSELA\n0.6441\n0.1311\n0.2248\n0.6693\n0.1346\n0.1661\n0.6661\n0.1214\n0.2125\ncMedQA2\nNo-Process\n0.8165\n0.0778\n0.1057\n0.8429\n0.0452\n0.1119\n0.8965\n0.0166\n0.0869\nAll-Process\n0.5955\n0.1520\n0.2525\n0.6069\n0.1430\n0.2501\n0.6327\n0.1541\n0.2132\nRS\n0.6003\n0.1683\n0.2313\n0.6148\n0.1593\n0.2259\n0.6137\n0.1889\n0.1974\nSELA\n0.4584\n0.2437\n0.2979\n0.4345\n0.2297\n0.3358\n0.4467\n0.2488\n0.3045\nMedical-O1-\nReasoning-\nSFT\nNo-Process\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\nAll-Process\n0.5127\n0.0963\n0.3910\n0.4769\n0.0873\n0.4358\n0.5066\n0.1003\n0.3931\nRS\n0.4688\n0.1017\n0.4295\n0.4793\n0.0811\n0.4396\n0.4842\n0.1178\n0.3980\nSELA\n0.4421\n0.0889\n0.4690\n0.4330\n0.0108\n0.5562\n0.4688\n0.0889\n0.4423\nHuatuo-26M-\nLite\nNo-Process\n0.5401\n0.1287\n0.3312\n0.5637\n0.0925\n0.3437\n0.5931\n0.0842\n0.3227\nAll-Process\n0.3913\n0.1963\n0.4124\n0.4269\n0.1717\n0.4014\n0.4543\n0.1763\n0.3694\nRS\n0.4525\n0.2025\n0.345\n0.4444\n0.1833\n0.3723\n0.4897\n0.1818\n0.3285\nSELA\n0.3913\n0.1963\n0.4124\n0.4269\n0.1717\n0.4014\n0.4543\n0.1763\n0.3694\nHuatuo-\n26M-Lite-\n100\nNo-Process\n0.6400\n0.2537\n0.10625\n0.9137\n0.0425\n0.0437\n0.9300\n0.020\n0.050\nAll-Process\n0.5213\n0.19\n0.2888\n0.5525\n0.1238\n0.3237\n0.5732\n0.1168\n0.3100\nRS\n0.5038\n0.2912\n0.205\n0.565\n0.1937\n0.2413\n0.5820\n0.2027\n0.2153\nSELA\n0.505\n0.2662\n0.2288\n0.5475\n0.1275\n0.325\n0.5690\n0.1800\n0.2510\nTable 5: Acceleration effects of the proposed efficient evaluation methods in reducing the time cost of the strategy search\nprocess. â€w/o DPS, PTS, CRMâ€ represents the baseline without any acceleration techniques applied. The column â€Time (h)â€\nindicates the total duration of the search process in hours, while â€Save (%)â€ reflects the percentage reduction in search time\nachieved by using the acceleration method.\nMethod\nChinese-medical-dialogue\ncMedQA2\nMedical-O1-Reasoning-SFT\nHuatuo-26M-Lite\nTime (h)\nSave (%)\nTime (h)\nSave (%)\nTime (h)\nSave (%)\nTime (h)\nSave (%)\nw/o DPS, PTS, CRM\n41.7\n0\n48.9\n0.\n50.0\n0\n37.2\n0\n+ DPS\n9.6\n76.9\n10.4\n78.7\n11.2\n77.6\n8.8\n76.3\n+ DPS, PTS\n8.7\n79.1\n9.4\n80.8\n4.3\n91.4\n6.7\n81.9\n+ DPS, PTS, CRM\n3.5\n91.6\n4.8\n90.2\n2.5\n95.0\n4.4\n88.2\n4.4\nEffects of Iterative Optimization\nTo valid the effects of iterative optimization in LLM-AutoDP, we\ncompare our LLM-AutoDP with a one-step strategy selection method\nthat employs LLM for generation with greedy search for selec-\ntion, denoted as LLM+greedy. In this approach, the LLM gener-\nates multiple candidate strategies at once, and the best-performing\none is selected based on initial evaluation scores without any sub-\nsequent refinement or feedback-driven iteration. Table 6 presents\nthe win/tie/loss rates of LLM-AutoDP versus LLM+greedy across\nmultiple datasets. The results show that LLM-AutoDP consistently\noutperforms the one-step method, achieving significantly higher\nwin rates (ranging from 60% to over 85%) on most tasks. Tie rates\nare relatively low, indicating clear performance differentiation be-\ntween the two approaches. This comparison demonstrates the ef-\nfectiveness of multi-round optimization in LLM-AutoDP. Unlike\ngreedy search, which lacks iterative refinement, our method lever-\nages feedback from previous rounds to progressively improve data\nprocessing strategies. This iterative mechanism enables the system\nto converge toward more effective pipelines, especially in complex\nand noisy data environments such as medical language modeling.\nThese findings underscore the importance of feedback loops in au-\ntomated data processing for LLM fine-tuning, where a single static\ndecision may fall short of capturing optimal solutions.\n4.5\nEffects of using different LLMs as agents\nIn the above experiments, we employ Qwen3-32B [48] as the agent\nmodel for strategy generation. To assess whether LLM-AutoDPâ€™s\nperformance is sensitive to the choice of LLM agent, we replace\nQwen3-32B with DeepSeek-R1-Distill-Llama-70B [12]. We re-\npeat the experiments on both models 5 times and record the win\n"}, {"page": 9, "text": "CMD\ncMedQA2\nHuatuo\nHuatuo-100\n50\n60\n70\n80\n90\n100\nOur Win Rate (%)\n89.4\n84.6\n61.3\n83.1\n88.8\n83.0\n61.3\n80.0\nQwen2.5-7B\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nCMD\ncMedQA2\nHuatuo\nHuatuo-100\n50\n60\n70\n80\n90\n100\n87.9\n84.3\n62.5\n83.2\n87.1\n84.9\n62.5\n80.5\nLlama3.1-8B\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nCMD\ncMedQA2\nHuatuo\nHuatuo-100\n50\n60\n70\n80\n90\n100\n87.2\n88.2\n66.4\n82.4\n85.5\n87.2\n66.4\n80.1\nGemma-2-9B\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nCMD\ncMedQA2\nHuatuo\nHuatuo-100\n50\n60\n70\n80\n90\n100\nOur Win Rate (%)\n86.9\n81.7\n54.0\n64.0\n86.8\n81.2\n54.0\n61.7\nQwen2.5-7B\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nCMD\ncMedQA2\nHuatuo\nHuatuo-100\n50\n60\n70\n80\n90\n100\n87.9\n84.3\n56.4\n91.4\n88.0\n86.6\n56.4\n89.9\nLlama3.1-8B\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nCMD\ncMedQA2\nHuatuo\nHuatuo-100\n50\n60\n70\n80\n90\n100\n87.1\n89.6\n59.3\n93.0\n87.4\n88.7\n59.3\n92.5\nGemma-2-9B\nQwen3-32B\nDeepSeek-R1-Distill-Llama-70B\nFigure 3: Results of using Qwen3-32B and DeepSeek-R1-Distill-Llama-70B as the agent models. We repeat the experiments on\neach agent 5 times and record the win rate of the resulting fine-tuned models versus the models fine-tuned on the original,\nunprocessed data. The colored columns and black bars indicate the average and standard deviation of repeated experiment\nresults. The top row and bottom row show results of using GPT-4 and Baichuan-M1-14B-Instruct as the judges, respectively.\nTable 6: The effects of iterative optimization in LLM-AutoDP. We compare LLM-AutoDP with a one-step strategy selection\nmethod (LLM+greedy) that employs LLM for strategy generation with greedy search for strategy selection. We report the\nwin/tie/loss rates of LLM-AutoDP versus LLM+greedy across multiple datasets and models.\nDatasets\nJudge\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\nOur Wins\nTies\nOur Losses\nOur Wins\nTies\nOur Losses\nOur Wins\nTies\nOur Losses\nChinese-medical-\ndialogue\nGPT-4\n0.7205\n0.0288\n0.2507\n0.7011\n0.0317\n0.2672\n0.7693\n0.0258\n0.2049\nBaichuan\n0.6427\n0.1326\n0.2247\n0.6442\n0.1095\n0.2463\n0.6713\n0.0832\n0.2455\ncMedQA2\nGPT-4\n0.7714\n0.0251\n0.2035\n0.8266\n0.0025\n0.1709\n0.8307\n0.0025\n0.1668\nBaichuan\n0.7023\n0.1369\n0.1608\n0.7815\n0.0979\n0.1206\n0.7875\n0.0764\n0.1361\nMedical-O1-Reasoning-\nSFT\nGPT-4\n0.4693\n0.1064\n0.4243\n0.4612\n0.0915\n0.4473\n0.4459\n0.0907\n0.4634\nBaichuan\n0.4725\n0.0904\n0.4371\n0.4642\n0.0842\n0.4516\n0.4777\n0.1074\n0.4149\nHuatuo-26M-Lite\nGPT-4\n0.575\n0.065\n0.36\n0.6053\n0.0475\n0.3472\n0.643\n0.0493\n0.3077\nBaichuan\n0.4538\n0.205\n0.3412\n0.45\n0.2263\n0.3237\n0.4927\n0.1967\n0.3106\nHuatuo-26M-Lite-100\nGPT-4\n0.6825\n0.05\n0.2675\n0.6075\n0.0075\n0.385\n0.6415\n0.0125\n0.346\nBaichuan\n0.6188\n0.1925\n0.1887\n0.6813\n0.1237\n0.195\n0.6717\n0.1461\n0.1822\nrate of the resulting fine-tuned models when strategies were gen-\nerated using each agent, relative to models trained on the original,\nunprocessed data. We present the average and standard deviation\nof repeated experiment results in Figure 3. The results in Figure 3\ndemonstrate that both LLMs are effective agents for generating\nhigh-quality DP strategies across various datasets and base mod-\nels. While Qwen3-32B achieves slightly better performance than\nDeepSeek-R1-Distill-Llama-70B, especially on the Huatuo-100\ndataset, the average performance difference across other datasets\nis negligible. This indicates that LLM-AutoDP is relatively robust\nto the choice of agent model, suggesting a low dependency on spe-\ncific LLM architectures. Furthermore, we observe that the standard\ndeviation across multiple experimental runs is consistently small.\nThis suggests that LLM-AutoDP also exhibits robustness to the\nsampling process of the agent LLMsâ€™ strategy generation, demon-\nstrating stability in its automated pipeline.\n"}, {"page": 10, "text": "2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\nOur Win Rate (%)\nCMD\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\ncMedQA2\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\nHuatuo\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\nHuatuo-100\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\nOur Win Rate (%)\nCMD\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\ncMedQA2\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\nHuatuo\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\n2\n4\n6\nStrategy Numbers at Initial Round\n50\n60\n70\n80\n90\n100\nHuatuo-100\nQwen2.5-7B\nLlama3.1-8B\nGemma-2-9B\nFigure 4: Results of using different numbers of strategies in the initial round. We use Qwen3-32B as the agent model and report\nthe win rate of the resulting fine-tuned models versus the models fine-tuned on the original, unprocessed data. The top row\nand bottom row show results of using GPT-4 and Baichuan-M1-14B-Instruct as the judges, respectively.\n4.6\nImpact of initial strategy numbers\nIn the first round of LLM-AutoDP, we inform the agents of the\ninitial strategy numbers via the prompt. This parameter directly\ninfluences the exploration space during the initial phase. To in-\nvestigate whether this parameter affects the final strategy quality,\nwe conducted experiments with different initial strategy sampling\ncounts {2, 4, 6}. Figure 4 demonstrate the performance across var-\nious fine-tuned models and datasets. We find that varying the ini-\ntial sampling count has minimal impact on the final strategy ef-\nfectiveness. Specifically, all fine-tuned models show performance\ndifferences of no more than 0.5% across the three sampling settings.\nAcross four datasets, the performance fluctuations due to different\ninitial sampling counts remain below 1%. The key reason for this ro-\nbustness lies in the adaptive nature of LLM-AutoDP during subse-\nquent iterations. Although the initial exploration space differs, we\nobserve that the framework automatically adjusts both the num-\nber of exploration rounds and the number of strategies explored\nper round. For instance, with an initial sampling of 6 strategies,\nLLM-AutoDP converges within 3 iterations, whereas it requires 5\niterations when starting with only 2 strategies. These observations\nhighlight the robustness of the LLM-AutoDP framework to the ini-\ntial exploration configuration, demonstrating its strong adaptabil-\nity to diverse initial settings without compromising the quality of\nthe resulting data processing strategies.\n5\nCONCLUSION AND FUTURE WORK\nWe propose LLM-AutoDP to address the challenge of automating\ndata processing for LLM fine-tuning without exposing raw, poten-\ntially sensitive training data. Our method apply LLMs as agents to\niteratively generate and refine DP strategies through alternating\nphases of strategy generation and evaluation. To accelerate the\ncomputationally expensive evaluation phase, we introduce three\nkey techniques. These techniques significantly reduce time con-\nsumption while preserving the effectiveness of the discovered strate-\ngies. Experiments across various datasets and models show that\nLLMs fine-tuned on data processed by our framework achieve over\n80% win rates compared to models trained on unprocessed data\nand outperform AutoML baselines based on LLM agents with ap-\nproximately a 65% win rate. Our proposed acceleration techniques\nreduce the total search time by up to 10Ã—, demonstrating both high\nefficiency and strong performance. As future work, we plan to ex-\ntend LLM-AutoDP to support multi-modal data and explore its ap-\nplicability in other high-stakes domains. Another future direction\nto combine privacy-preserving mechanisms into the framework to\nfurther enhance data security. For instance, differential privacy\ncan be incorporated into the training process to prevent the model\nfrom leaking sensitive information. In such cases, the effectiveness\nof LLM-AutoDP warrants further investigation.\n6\nAPPENDIX\n6.1\nPrompts used in LLM-AutoDP\n(1) Prompt for agent LLMs at initial round:\nYou are an intelligent assistant specializing in handling training\ndata for large models. The current task is to achieve a specified level\nof accuracy by adjusting the quality of the data, but this must be\ndone in â€black-box modeâ€ â€”meaning you cannot view the specific\ncontent of the data or know in advance what issues exist within\n"}, {"page": 11, "text": "it. To accomplish the goal, you can coordinate the following four\nprofessional teams to work collaboratively:\n(1) Data Cleaning Team: Responsible for basic data purification.\n(2) Data Generation Team: Responsible for supplementing new\ndata.\n(3) Data Optimization Team: Responsible for improving data\nquality.\n(4) Data Selection Team: Responsible for filtering high-quality\ndata.\nFor more details on each teamâ€™s specific functions, refer to the\nâ€Team Capability Overview.â€\nAlthough you cannot preview the problems within the data, you\ncan combine the teams like assembling building blocks â€”deciding\nwhich teams to use and planning the sequence of their work (the\norder will affect the final outcome). Through continuous iteration,\nyou need to identify the optimal combination of teams to process\nthe training data.\n### Team Capability Overview:\n(1) ** Data Cleaning Team **\nâ€¢ Capabilities: Data purification, including deduplication, re-\nmoval of special characters, filtering data with inappropriate\nlength, noise reduction, and elimination of repeated words within\nindividual data entries.\n(2) ** Data Generation Team **\nâ€¢ Capabilities: Generating missing questions or answers, as well\nas creating new data.\n(3) ** Data Optimization Team **\nâ€¢ Capabilities: Improving data quality by optimizing existing\ndata, enhancing the accuracy of Q&A pairs, improving text flu-\nency, readability, conciseness, and ensuring compliance with con-\ntent standards.\n(4) ** Data Selection Team **\nâ€¢ Capabilities: Selecting high-quality data.\n### Task Execution Instructions:\nFirst Round (Initialization): Initialize multiple different team\ncombinations (no more than four)\nâ€¢ Based on your experience, initialize multiple distinct combi-\nnations of teams.\nâ€¢ Each combination can consist of a single team or multiple\nteams.\nâ€¢ The sequence of the teamsâ€™ work must be clearly defined for\neach combination.\nâ€¢ During the first round of initialization, it is recommended to\ntest combinations with fewer teams to determine the initial impact\nof different teams and their working order. This will help accumu-\nlate baseline data for subsequent iterative optimization.\n### Example of Combinations:\n###Combination[1]###\nâ€¢ Data Cleaning Team\n###Combination[2]###\nâ€¢ Data Cleaning Team, Data Generation Team\nOutput Format:\nPlease strictly follow the format below to output the results:\n###Combination[1]###\nâ€¢ List the team names in order of execution, separated by com-\nmas.\nâ€¦\n###Combination[n]###\nâ€¢ List the team names in order of execution, separated by com-\nmas.\n###Reasons for Different Combinations###\nâ€¢ Explain why these combinations were chosen.\nRound 1:\nNow, proceed with the first step and initialize multiple different\nteam combinations (no more than four).\n(2) Prompt for agent LLMs at iterative optimization rounds:\nRound 2 (Iterative Optimization): Evaluate Feedback Scores of\nCombinations and Adjust Team Configurations and Work Order;\nChanges in the Number of Combinations Are Allowed\nâ€¢ Obtain feedback scores for different combinations. A higher\nscore for a combination indicates that the data processed by this\ncombination is more suitable for large model training.\nâ€¢ The feedback scores reflect the performance differences be-\ntween the processed training data and the original data. Therefore,\nthe score results may indicate either positive improvement (posi-\ntive values) or performance degradation (negative values).\nâ€¢ Adjust team configurations and work order based on the feed-\nback scores from the previous round. The goal is to progressively\noptimize the data processing effect to achieve the best possible out-\ncome.\nâ€¢ You are allowed to change the number of combinations in this\nround. That is, the number of combinations in this round can differ\nfrom the previous round.\nâ€¢ When adjusting team configurations, pay attention not only to\nthe scores of different combinations but also to the relative changes\nin scores among different combinations.\nâ€¢ If, during the iteration process, you determine that no further\nadjustments to team configurations or work order are needed, you\ncan stop the iteration and provide the most suitable team combina-\ntion. Mark this combination with the labelã€Best Teamã€‘to indicate\nit as the final choice.\nâ€¢ If, during the iteration process, you attempt different combi-\nnations but the feedback scores are consistently close to zero, it\nindicates that the original data does not require any processing. In\nthis case, outputã€No Processing Required for Original Dataã€‘and\nstop the iteration.\nâ€¢ Duplicate teams are not allowed within the same combination,\nand the number of teams in a single combination must not exceed\nfour.\nRepeat Round 2 until you determine that no further adjustments\nto team configurations or work order are necessary.\nThe feedback scores for the different team combinations in\nRound are as follows:\n1.###Combination[1]###\nFeedback Score:\n2.###Combination[2]###\nFeedback Score:\n"}, {"page": 12, "text": "3.###Combination[3]###\nFeedback Score:\n4.###Combination[4]###\nFeedback Score:\n### Round :\nNow, proceed with Step , evaluate the feedback scores of the\ncombinations, and adjust the team configurations and work order.\nChanges in the number of combinations are allowed.\n(3) Prompt for win/wie/loss rate judge models:\nAs a professional medical evaluator, please evaluate the follow-\ning two doctorsâ€™ responses to the same medical question.\nQuestion:\nResponse 1:\nResponse 2:\nThe evaluation criteria are prioritized in the following order: Ac-\ncuracy of the doctorâ€™s response, Safety, Fluency, and Conciseness.\nThe specific definitions are as follows:\nEvaluation Criteria:\n1. Accuracy of the Doctorâ€™s Response: The doctor should accu-\nrately understand the patientâ€™s question and provide a scientific\nand correct answer.\n2. Safety: The doctor must adhere to laws, regulations, ethics,\nand professional conduct when answering.\n3. Fluency: Ensure semantic coherence, with no logical errors or\nirrelevant information. Maintain a friendly and enthusiastic tone\nin the response.\n4. Conciseness: Clearly and concisely explain complex medical\nknowledge. Avoid overly redundant content in the dialogue.\nNote: The evaluation must be based on the importance ranking\nof Accuracy > Safety > Fluency > Conciseness. In case of conflicts,\nprioritize the higher-ranking criterion.\nYou need to select your evaluation result from the following\nthree options: [Response 1 wins compared to Response 2, Response\n1 ties with Response 2, Response 1 loses compared to Response 2].\nYour output must strictly follow the format below:\nEvaluation Result:\nOnly provide the selected evaluation result here.\n6.2\nExperiments on Law Dataset\nTable 7: The results of LLM-AutoDP on DISC-Law-SFT\ndataset. We report the win/tie/loss rates of LLM-AutoDP ver-\nsus No-process baseline.\nModels\nJudge\nOur Wins\nTies\nOur Losses\nQwen2.5-7B\nGPT-4\n0.9007\n0.0145\n0.0848\nLawLLM\n0.8419\n0.0625\n0.0956\nLlama3.1-8B\nGPT-4\n0.8466\n0.000\n0.1534\nLawLLM\n0.8664\n0.0394\n0.0942\nGemma-2-9B\nGPT-4\n0.8941\n0.0024\n0.1035\nLawLLM\n0.8327\n0.1049\n0.0624\nIn addition to medical data, we also validate the effectiveness\nof LLM-AutoDP on law dataset. We evaluate LLM-AutoDP on the\nDISC-Law-SFT dataset [50]. We compare the performance of mod-\nels trained on data processed by LLM-AutoDP with those trained\non the original data. The evaluation is conducted using GPT-4 and\na domain-specialized legal model, LawLLM-7B [51], as judges. As\nshown in Table 7, the models trained on LLM-AutoDP-processed\ndata outperform those trained on the original data in approximately\n90% of the queries.\n6.3\nAn Example of Strategy Optimizations\nWe present an example of the step-by-step optimization process by\nLLM-AutoDP on cMedQA2 dataset in Figure 5.\nFigure 5: A step-by-step example of strategy optimization\nusing LLM-AutoDP. A total of 5 rounds of iterations are per-\nformed, but only the details of 2 rounds are shown due to\nspace imitation.\n"}, {"page": 13, "text": "REFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-\nrencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shya-\nmal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Lalmohan Behera, Vishnu Vardhan, and Reddy Chilukoori. [n.d.]. Automation\nin Data Engineering: Challenges and Opportunities in Building Smart Pipelines.\nhttps://api.semanticscholar.org/CorpusID:277572683\n[3] Mehwish Bilal, Ghulam Ali, Muhammad Waseem Iqbal, Muhammad Anwar,\nMuhammad Sheraz Arshad Malik, and Rabiah Abdul Kadir. 2022. Auto-prep:\nefficient and automated data preprocessing pipeline.\nIEEE Access 10 (2022),\n107764â€“107784.\n[4] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge,\nDawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, et al. 2024. Data-juicer:\nA one-stop data processing system for large language models. In Companion of\nthe 2024 International Conference on Management of Data. 120â€“134.\n[5] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge,\nDawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding,\nand Jingren Zhou. 2023. Data-Juicer: A One-Stop Data Processing System for\nLarge Language Models. Companion of the 2024 International Conference on Man-\nagement of Data (2023). https://api.semanticscholar.org/CorpusID:261530723\n[6] Daoyuan Chen, Yilun Huang, Xuchen Pan, Nana Jiang, Haibin Wang, Ce Ge,\nYushuo Chen, Wenhao Zhang, Zhijian Ma, Yilei Zhang, Jun Huang, Wei Lin,\nYaliang Li, Bolin Ding, and Jingren Zhou. 2024. Data-Juicer 2.0: Cloud-Scale\nAdaptive Data Processing for Foundation Models. ArXiv abs/2501.14755 (2024).\nhttps://api.semanticscholar.org/CorpusID:275921171\n[7] Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. 2023.\nGamegpt: Multi-agent collaborative framework for game development. arXiv\npreprint arXiv:2310.08067 (2023).\n[8] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng\nWang, Jianye Hou, and Benyou Wang. 2024. HuatuoGPT-o1, Towards Medical\nComplex Reasoning with LLMs. arXiv:2412.18925 [cs.CL] https://arxiv.org/abs/\n2412.18925\n[9] Junying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen,\nHongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, et al. 2023. Huatuogpt-ii,\none-stage training for medical adaption of llms. arXiv preprint arXiv:2311.09774\n(2023).\n[10] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav,\nZheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023. Alpagasus:\nTraining a better alpaca with fewer data. arXiv preprint arXiv:2307.08701 (2023).\n[11] Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei,\nBangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, et al. 2024. SELA: Tree-\nSearch Enhanced LLM Agents for Automated Machine Learning. arXiv preprint\narXiv:2410.17238 (2024).\n[12] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs\nvia Reinforcement Learning.\narXiv:2501.12948 [cs.CL] https://arxiv.org/abs/\n2501.12948\n[13] Dinesha Dissanayake, Rajitha Navarathna, Praveen Ekanayake, and Suma-\nnaruban Rajadurai. 2025. A Survey of Evaluating AutoML and Automated Fea-\nture Engineering Tools in Modern Data Science. In International Conference\non Enterprise Information Systems.\nhttps://api.semanticscholar.org/CorpusID:\n277715348\n[14] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and\nFrank Hutter. 2022.\nAuto-sklearn 2.0: Hands-free automl via meta-learning.\nJournal of Machine Learning Research 23, 261 (2022), 1â€“61.\n[15] Kartikay Goyle, Quin Xie, and Vakul Goyle. 2024. Dataassist: A machine learn-\ning approach to data cleaning and preparation. In Intelligent Systems Conference.\nSpringer, 476â€“486.\n[16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-\nhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-\nten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783 (2024).\n[17] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2019.\nAutoML: A Survey of the\nState-of-the-Art. ArXiv abs/1908.00709 (2019). https://api.semanticscholar.org/\nCorpusID:199405568\n[18] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023.\nMetagpt: Meta programming for multi-agent collaborative framework. arXiv\npreprint arXiv:2308.00352 3, 4 (2023), 6.\n[19] Daniel Jarrett, Bogdan C Cebere, Tennison Liu, Alicia Curth, and Mihaela\nvan der Schaar. 2022. Hyperimpute: Generalized iterative imputation with auto-\nmatic model selection. In International Conference on Machine Learning. PMLR,\n9916â€“9937.\n[20] Haifeng Jin, FranÃ§ois Chollet, Qingquan Song, and Xia Hu. 2023. AutoKeras:\nAn AutoML Library for Deep Learning. J. Mach. Learn. Res. 24 (2023), 6:1â€“6:6.\nhttps://api.semanticscholar.org/CorpusID:259149826\n[21] Aristeidis Karras, Christos N. Karras, Nikolaos V. Schizas, Markos Avlonitis,\nand Spyros Sioutas. 2023. AutoML with Bayesian Optimizations for Big Data\nManagement. Inf. 14 (2023), 223.\nhttps://api.semanticscholar.org/CorpusID:\n257995586\n[22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.\nEfficient\nmemory management for large language model serving with pagedattention.\nIn Proceedings of the 29th Symposium on Operating Systems Principles. 611â€“626.\n[23] Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, and\nMicah Goldblum. 2025. Commercial LLM Agents Are Already Vulnerable to\nSimple Yet Dangerous Attacks. arXiv preprint arXiv:2502.08586 (2025).\n[24] Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu,\nPrayag Tiwari, Xiang Wan, and Benyou Wang. 2023. Huatuo-26M, a Large-scale\nChinese Medical QA Dataset. arXiv:2305.01526 [cs.CL]\n[25] Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. 2023. Econagent:\nlarge language model-empowered agents for simulating macroeconomic activi-\nties. arXiv preprint arXiv:2310.10436 (2023).\n[26] Peng Li, Zhiyi Chen, Xu Chu, and Kexin Rong. 2023. Diffprep: Differentiable\ndata preprocessing pipeline search for learning over tabular data. Proceedings\nof the ACM on Management of Data 1, 2 (2023), 1â€“26.\n[27] Jiabin Liu, Fu Zhu, Chengliang Chai, Yuyu Luo, and Nan Tang. 2021. Automatic\nData Acquisition for Deep Learning. Proc. VLDB Endow. 14 (2021), 2739â€“2742.\nhttps://api.semanticscholar.org/CorpusID:236995528\n[28] Jiabin Liu, Fu Zhu, Chengliang Chai, Yuyu Luo, and Nan Tang. 2021. Automatic\ndata acquisition for deep learning. Proceedings of the VLDB Endowment 14, 12\n(2021), 2739â€“2742.\n[29] Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang,\nPeng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, et al. 2024. A survey on medical\nlarge language models: Technology, application, trustworthiness, and future di-\nrections. arXiv preprint arXiv:2406.03712 (2024).\n[30] Zac Yung-Chun Liu, Shoumik Roychowdhury, Scott Tarlow, Akash Nair, Shweta\nBadhe, and Tejas Shah. 2021. AutoDC: Automated data-centric processing. arXiv\npreprint arXiv:2111.12548 (2021).\n[31] Zilin Ma, Yiyang Mei, and Zhaoyuan Su. 2024. Understanding the benefits and\nchallenges of using large language model-based conversational agents for men-\ntal well-being support. In AMIA Annual Symposium Proceedings, Vol. 2023. 1105.\n[32] Tran Ngoc Minh, Mathieu Sinn, Hoang Thanh Lam, and Martin Wistuba.\n2018. Automated Image Data Preprocessing with Deep Reinforcement Learn-\ning. ArXiv abs/1806.05886 (2018).\nhttps://api.semanticscholar.org/CorpusID:\n49271795\n[33] Alhassan G. Mumuni and Fuseini Mumuni. 2024.\nAutomated data process-\ning and feature engineering for deep learning and big data applications: a sur-\nvey. ArXiv abs/2403.11395 (2024).\nhttps://api.semanticscholar.org/CorpusID:\n266884632\n[34] Randal S. Olson and Jason H. Moore. 2016. TPOT: A Tree-based Pipeline Op-\ntimization Tool for Automating Machine Learning. In AutoML@ICML.\nhttps:\n//api.semanticscholar.org/CorpusID:12399099\n[35] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wul-\nczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. 2024. Capa-\nbilities of gemini models in medicine. arXiv preprint arXiv:2404.18416 (2024).\n[36] LuÃ­s Santos and LuÃ­s Ferreira. 2023. Atlanticâ€”Automated data preprocessing\nframework for supervised machine learning. Software Impacts 17 (2023), 100532.\n[37] Mayur Kishor Shende, Andres E Feijoo-Lorenzo, and Neeraj Dhanraj Bokde.\n2022. cleanTS: Automated (AutoML) tool to clean univariate time series at mi-\ncroscales. Neurocomputing 500 (2022), 155â€“176.\n[38] Qitao Shi, Ya-Lin Zhang, Longfei Li, Xinxing Yang, Meng Li, and Jun Zhou. 2020.\nSAFE: Scalable Automatic Feature Engineering Framework for Industrial Tasks.\n2020 IEEE 36th International Conference on Data Engineering (ICDE) (2020), 1645â€“\n1656. https://api.semanticscholar.org/CorpusID:212414797\n[39] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy\nHardin, Surya Bhupatiraju, LÃ©onard Hussenot, Thomas Mesnard, Bobak Shahri-\nari, Alexandre RamÃ©, et al. 2024. Gemma 2: Improving open language models at\na practical size. arXiv preprint arXiv:2408.00118 (2024).\n[40] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. 2023. D4:\nImproving llm pretraining via document de-duplication and diversification. Ad-\nvances in Neural Information Processing Systems 36 (2023), 53983â€“53995.\n[41] Toyhom. 2023. Chinese-medical-dialogue-data.\nhttps://github.com/Toyhom/\nChinese-medical-dialogue-data\n[42] Jacqueline A Valeri, Luis R Soenksen, Katherine M Collins, Pradeep Ramesh,\nGeorge Cai, Rani Powers, Nicolaas M Angenent-Mari, Diogo M Camacho, Felix\nWong, Timothy K Lu, et al. 2023. BioAutoMATED: an end-to-end automated\nmachine learning tool for explanation and design of biological sequences. Cell\nsystems 14, 6 (2023), 525â€“542.\n[43] Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei\nCheng, Xiangrong Zeng, Yupeng Zhang, Yuqi Huo, Zecheng Wang, et al. 2025.\nBaichuan-m1: Pushing the medical capability of large language models. arXiv\npreprint arXiv:2502.12671 (2025).\n"}, {"page": 14, "text": "[44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage models with self-generated instructions. arXiv preprint arXiv:2212.10560\n(2022).\n[45] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi\nChen. 2024. Less: Selecting influential data for targeted instruction tuning. arXiv\npreprint arXiv:2402.04333 (2024).\n[46] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.\nC-Pack: Packaged Resources To Advance General Chinese Embedding.\narXiv:2309.07597 [cs.CL]\n[47] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li,\nCan Xu, Dacheng Tao, and Tianyi Zhou. 2024. A survey on knowledge distilla-\ntion of large language models. arXiv preprint arXiv:2402.13116 (2024).\n[48] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical\nreport. arXiv preprint arXiv:2505.09388 (2025).\n[49] Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang\nJia, and Hongying Zan. 2024. Zhongjing: Enhancing the chinese medical capabil-\nities of large language model through expert feedback and real-world multi-turn\ndialogue. In Proceedings of the AAAI conference on artificial intelligence, Vol. 38.\n19368â€“19376.\n[50] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun\nLiu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, and Zhongyu Wei.\n2023. DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal\nServices. arXiv:2309.11325 [cs.CL]\n[51] Shengbin Yue, Shujun Liu, Yuxuan Zhou, Chenchen Shen, Siyuan Wang, Yao\nXiao, Bingxuan Li, Yun Song, Xiaoyu Shen, Wei Chen, et al. 2024. LawLLM:\nIntelligent Legal System with Legal Reasoning and Verifiable Retrieval. In In-\nternational Conference on Database Systems for Advanced Applications. Springer,\n304â€“321.\n[52] Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi\nZhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, et al. 2020. MedDi-\nalog: Large-scale medical dialogue datasets. In Proceedings of the 2020 conference\non empirical methods in natural language processing (EMNLP). 9241â€“9250.\n[53] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan\nLi, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. 2023.\nHuatuogpt, towards taming language model to be a doctor.\narXiv preprint\narXiv:2305.15075 (2023).\n[54] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian McAuley,\nWayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2024. Agentcf: Collaborative learn-\ning with autonomous language agents for recommender systems. In Proceedings\nof the ACM Web Conference 2024. 3679â€“3689.\n[55] Shuo Zhang, Jinyi Chen, Jiayuan Chen, Xiaofei Chen, and Hejiao Huang. 2023.\nData imputation in IoT using spatio-temporal variational auto-encoder. Neuro-\ncomputing 529 (2023), 23â€“32.\n[56] S. Zhang, X. Zhang, H. Wang, L. Guo, and S. Liu. 2018. Multi-Scale Attentive\nInteraction Networks for Chinese Medical Question Answer Selection. IEEE\nAccess 6 (2018), 74061â€“74071. https://doi.org/10.1109/ACCESS.2018.2883637\n[57] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang,\nZeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda\nChen. 2024.\nSWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning.\narXiv:2408.05517 [cs.CL] https://arxiv.org/abs/2408.05517\n[58] Yanxin Zheng, Wensheng Gan, Zefeng Chen, Zhenlian Qi, Qian Liang, and\nPhilip S Yu. 2025. Large language models for medicine: a survey. International\nJournal of Machine Learning and Cybernetics 16, 2 (2025), 1015â€“1040.\n[59] Daquan Zhou, Kaixin Wang, Jianyang Gu, Xiang Peng, Dongze Lian, Yifan\nZhang, Yang You, and Jiashi Feng. 2023. Dataset Quantization. 2023 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) (2023), 17159â€“17170. https:\n//api.semanticscholar.org/CorpusID:261049434\n"}]}