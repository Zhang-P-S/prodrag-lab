{"doc_id": "arxiv:2512.12858", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.12858.pdf", "meta": {"doc_id": "arxiv:2512.12858", "source": "arxiv", "arxiv_id": "2512.12858", "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization", "authors": ["Sonal Prabhune", "Balaji Padmanabhan", "Kaushik Dutta"], "published": "2025-12-14T21:52:31Z", "updated": "2025-12-14T21:52:31Z", "summary": "Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.12858v1", "url_pdf": "https://arxiv.org/pdf/2512.12858.pdf", "meta_path": "data/raw/arxiv/meta/2512.12858.json", "sha256": "6fdc5547b64ba9908bd83af2ac5f2232fb86246d4e0d97996a900992347f92c9", "status": "ok", "fetched_at": "2026-02-18T02:24:18.486283+00:00"}, "pages": [{"page": 1, "text": "INFORMATION-CONSISTENT LANGUAGE MODEL\nRECOMMENDATIONS THROUGH GROUP RELATIVE POLICY\nOPTIMIZATION\nSonal Prabhune, Balaji Padmanabhan, and Kaushik Dutta\n{saprabhune@usf.edu, bpadmana@umd.edu, duttak@usf.edu}\nABSTRACT\nLarge Language Models (LLMs) are increasingly deployed in business-critical domains such as\nfinance, education, healthcare, and customer support, where users expect consistent and reliable\nrecommendations. Yet LLMs often exhibit variability when prompts are phrased with minor dif-\nferences, even when semantically equivalent. Such inconsistency undermines trust, complicates\ncompliance, and disrupts user experience. While personalization is desirable in certain contexts,\nmany enterprise scenarios—such as HR onboarding, customer support, or policy disclosure—require\ninvariant information delivery regardless of phrasing or prior conversational history. Existing ap-\nproaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality\nor reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we\npropose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO)\nto directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to\nreasoning and code generation, we adapt GRPO to enforce stability of information content across\ngroups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability\nrewards, treating prompt variants as groups and resetting conversational context to isolate phrasing\neffects. Experiments on investment and job recommendation tasks show that our GRPO-trained\nmodel reduces variability more effectively than fine-tuning or decoding-based baselines. To our\nknowledge, this is a novel application of GRPO for aligning LLMs toward information consistency,\nreframing variability not as an acceptable feature of generative diversity but as a correctable flaw in\nenterprise deployments.\n1\nINTRODUCTION\nLarge Language Models (LLMs) such as Llama-3 are increasingly deployed in domains requiring decision support\nand recommendations. A critical requirement in these deployments is that the AI provides consistent and reliable\noutputs regardless of how a user phrases a prompt. Consistency is not merely a technical consideration—it underpins\ntrust, usability, and compliance in business applications. In practice, organizations depend on stable outputs to ensure\noperational reliability, regulatory adherence, brand integrity, and user satisfaction. At the same time, consistent behavior\nis also essential to safeguard fairness and prevent systemic harms.\nIt is important to acknowledge that consistency is not always universally desirable. In some settings, such as personalized\nlearning platforms or health coaching, users benefit when the system tailors its responses to their unique profiles and\nhistories. Here, variation in outputs can reflect meaningful personalization rather than unreliability. However, there\nare many business-critical scenarios where consistency must prevail regardless of how well the LLM knows the\nuser or their prior interaction context. For example, the use of LLMs for answering organizational policy-related\nquestions, personal financial planning questions, and educational planning questions are common applications. In such\napplications, providing a consistent answer is important in building the user trust on LLM based answers. In these cases,\npersonalization should not alter the essential information content of the response. For example, in human resources\nonboarding, new employees should always receive the same explanation of company policies; in customer support,\nanswers about product warranty coverage should remain unchanged no matter how the question is phrased; and in\ncompliance-driven settings, financial disclosures or insurance terms must be delivered identically to every user. These\narXiv:2512.12858v1  [cs.LG]  14 Dec 2025\n"}, {"page": 2, "text": "Sonal Prabhune et al.\nare cases where personalization may add conversational nuance but should never compromise the consistency of core\ninformation.\nSolutions such as RAG [1] have been proposed as a way to support consistency by grounding answers in external\nknowledge. While effective in many contexts, RAG does not fully eliminate inherent inconsistencies in LLM behavior.\nFor instance, consider two applicants preparing for the same job interview and querying an LLM-powered assistant: one\nasks, “What are the most common data structures I should review?” while another asks, “Which data structures should I\nprepare for in coding interviews?” Even when both queries are backed by the same retrieved documents, the LLM may\ngenerate divergent lists or emphasize different topics. Such inconsistencies create confusion and reduce trust in the\nsystem, especially in high-stakes contexts like interview preparation. Literature has often embraced this variability\nas an acceptable property of generative models, framing it as diversity in responses [2] and proposing workarounds\nsuch as RAG or temperature tuning to control stochasticity. However, this acceptance does not resolve the fundamental\nissue. Simply lowering temperature or relying on retrieval does not guarantee that semantically equivalent prompts\nwill produce consistent outputs. For many business and educational applications, overlooking this inconsistency is\nuntenable.\nAcross industries such as finance, education, healthcare, and customer services, unpredictable or inconsistent responses\ncan have significant consequences and may not help build the required user trust in using such systems. A bank\nproviding different disclosures depending on phrasing, risks compliance failures; a chatbot that answers the same\nquestion differently for two customers undermines confidence in customer support; and in educational or hiring contexts,\ndemographic attributes such as gender introducing unintended inconsistencies result in ethical issues. In particular, job\nrecommendation scenarios highlight both the operational and ethical risks of inconsistency and demonstrate the need\nfor robust methods to enforce stable and equitable outputs.\nWhile contextual methods such as RAG can help improve consistency by grounding model responses in relevant\ndocumentation, their applicability depends on whether contextual retrieval is available at query time. In enterprise\ndeployments, RAG can reduce hallucinations and tie answers to authoritative sources, thereby increasing factual\nconsistency. However, many user interactions occur without such context—when individuals query general-purpose\nassistants directly, without attached documents or retrieval layers. In such settings, the model must still produce\ninternally consistent responses across semantically equivalent prompts, regardless of who is asking the question. In\nthis paper, we focus on this latter class of scenarios—direct, context-free user interactions—and leave the extension to\ncontextual or retrieval-grounded querying as a direction for future work.\nIn addition to operational and reputational incentives, legal risk increasingly drives the demand for consistency in AI\nsystems. Recent lawsuits highlight how inconsistent model behavior can expose organizations to liability. In Mobley v.\nWorkday [3] and Harper v. Sirius XM [4], allegations of hiring bias highlight how inconsistent or opaque decision\npatterns can be interpreted as unlawful disparate treatment. Cases involving chatbot misinformation, such as the British\nColumbia tribunal’s finding of liability for an AI-generated misstatement [5], further illustrate that even occasional\nunreliability can create legal exposure. Likewise, the wrongful-death claim in Raine v. OpenAI [6] points to the\nrisks posed by inconsistent adherence to safety-critical behaviors. These examples demonstrate that inconsistency\nundermines fairness, defensibility, and regulatory compliance, turning consistency from a technical preference into a\nlegal imperative.\nIt is important to note that in certain cases, context can assist consistency. RAG frameworks, for example, can\nanchor model responses to verified documentation or structured data, reducing factual drift and stabilizing answers\nacross paraphrased prompts. When properly implemented, RAG can effectively “ground” responses in organizational\ncontext—such as product manuals, policy databases, or compliance archives—ensuring that the same underlying\ninformation supports each response. However, not all scenarios can rely on contextual retrieval. In many real-world\ninteractions, users directly query an LLM without supplementary context, expecting consistent and accurate answers\npurely from the model’s internal knowledge. These situations include public-facing chatbots, educational advisors,\nor general-purpose assistants where users pose free-form questions. For such cases, contextual grounding cannot be\nassumed, and the challenge becomes ensuring that the LLM’s intrinsic generation process itself remains stable across\nsemantically equivalent inputs. If a professional gave different answers to the same question on different days, we\nwould question their reliability. LLMs must show this same level of consistency for organizations to trust them in\noperational decision-making.\nUltimately, businesses value consistency in LLM behavior not only to safeguard fairness but also to guarantee operational\nreliability, regulatory compliance, brand integrity, user trust and user satisfaction. In this paper, we focus specifically on\ndirect, user-facing LLM interactions without external retrieval context, examining how reinforcement learning—via\nGroup Relative Policy Optimization (GRPO)—can promote consistent behavior even when no grounding information is\navailable. We leave the exploration of context-augmented consistency through RAG and other retrieval mechanisms to\nfuture work.\n2\n"}, {"page": 3, "text": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization\n2\nLITERATURE REVIEW\nLLMs often exhibit sensitivity to superficial differences in prompts. Even when inputs are semantically equivalent,\nminor changes in form can cause divergent responses. Gan et al. [7] document cross-lingual prompt sensitivity, while\nSharma et al. [8] propose explicit measures for quantifying consistency across paraphrases. Beyond phrasing effects, Liu\net al. [9] highlight logical invariances—such as negation or transitivity—as proxies for stable reasoning, emphasizing\nthat inconsistency is a pervasive issue for model reliability.\nA number of low-cost fixes attempt to mitigate inconsistency through decoding controls. Temperature tuning is\nfrequently used to reduce variability, but work by Zhang et al. [10] and Schmalbach [11] shows that deterministic\ndecoding (e.g., setting temperature to zero) does not guarantee identical completions across semantically equivalent\ninputs. Thus, simple sampling adjustments cannot enforce true information consistency.\nRAG provides a popular strategy to stabilize outputs by grounding them in external sources [1]. While RAG improves\nfactuality and reduces hallucination, it does not guarantee consistent outputs across paraphrased prompts. Even when\nretrieval results are held constant, divergence arises from stochasticity in generation and retrieval noise. This limitation\nis evident in contexts such as interview preparation, where two semantically equivalent queries may yield different\nrecommended topics despite access to the same evidence. Consequently, RAG is complementary but insufficient for\nenforcing invariance.\nFine-tuning has been explored as a more direct means of promoting consistency. Sun et al. [12] find that instruction-\ntuned models improve robustness overall but still exhibit notable brittleness under rephrasings. Zhao et al. [13] propose\na two-stage method combining instruction augmentation with consistency alignment training, showing reductions in\nparaphrase-induced variability. Wu et al. [14] introduce prompt perturbation consistency learning, using divergence-\nbased regularization to align outputs across perturbed variants. Raj et al. [15] extend this with Chain of Guidance\n(CoG), generating synthetic variants for fine-tuning to enforce semantic agreement. These studies demonstrate that\nfine-tuning can help, but their approaches are primarily data-augmentation or loss-based, without making consistency\nthe primary optimization goal. Recent approaches have addressed information consistency in retrieval-augmented\ngeneration (RAG) systems by optimizing similarity across paraphrased queries, often evaluated relative to retrieved\nevidence or task-specific ground truth answers [16]. While effective for knowledge-grounded QA, such formulations\nassume a well-defined notion of correctness. Our work instead focuses on business-critical recommendation and\nadvisory settings, where ground truth is inherently ambiguous and the primary requirement is invariance of information\ncontent across user characteristics (e.g., gender or other attributes), independent of phrasing or retrieval context. This\ndistinction motivates the need for consistency objectives that are independent of retrieval context and ground-truth\nsupervision, particularly in enterprise-facing deployments.\nDespite these advances, important gaps remain. Most approaches either measure consistency post-hoc or encourage\nit indirectly. Few works define consistency as a direct training objective, and reinforcement learning has rarely been\napplied to explicitly minimize cross-variant dispersion. Furthermore, much of the literature evaluates on classification\nor reasoning tasks, leaving generative recommendation settings—such as investment or job advising—underexplored.\nDemographic attributes like gender have been used to highlight unequal opportunities [17], but less often as structured\ntestbeds for prompt consistency itself.\n2.1\nIs Consistency Always Desirable in Business Scenarios?\nWhile much of the literature frames inconsistency as a flaw, it is important to recognize that uniformity is not always\nthe optimal outcome, as we know from the literature of personalization and recommendation engines in traditional\nsystems. In many enterprise applications, certain degree of variation—when grounded in contextually appropriate,\ntask-relevant, or user-declared attributes—can enhance usefulness, personalization, and fairness.\nFor example, adaptive tutoring systems or AI-assisted learning environments benefit from adjusting responses to\nindividual knowledge states, pacing preferences, or learning goals. A recent meta-analysis confirms that AI-driven\npersonalized learning interventions yield consistently positive effects on student achievement across diverse contexts [18].\nLikewise, in healthcare, medical AI systems are expected to tailor guidance based on clinically relevant variables—such\nas age, comorbidities, or pregnancy status—where differentiation is justified by medical evidence rather than unintended\ndemographic disparity [19].\nHowever, consistency becomes critical when the task involves factual correctness, safety, compliance, or fairness\nobligations. In such cases, equivalent inputs—regardless of how they are phrased or who provides them—should yield\ninvariant outputs. Examples include banking disclosures, loan eligibility decisions, scientific or mathematical facts, and\ncompliance-driven policies. This distinction aligns with principles in the NIST AI Risk Management Framework and\n3\n"}, {"page": 4, "text": "Sonal Prabhune et al.\nthe EU AI Act, which emphasize trustworthiness, reliability, and non-discrimination as essential for high-risk systems\n[20, 21].\n2.2\nBalancing Fairness, Cultural Context, and Personalization\nThe need for consistency is also culturally contextual. Some societies prioritize equal treatment across gender\nor ethnicity, while others incorporate differentiated norms into occupational or social expectations. For example, in\ngendered labor markets, recommendation systems might tailor job suggestions differently in regions where cultural\nnorms shape acceptable roles. Yet, in contexts such as scientific or mathematical facts, or compliance disclosures,\noutputs must remain invariant across gender, religion, or ethnicity to ensure fairness and epistemic neutrality.\nEthical AI frameworks (e.g., UNESCO’s Recommendation on AI Ethics and the OECD AI Principles) advise that\ndifferentiation should be user-driven and contextually justified, not emergent from model inconsistencies [22, 23]. This\nmeans cultural adaptation may be acceptable when explicitly requested by users (e.g., dietary guidance sensitive to\nreligious practices) but not when identical prompts from users of different demographics elicit divergent factual or\nprescriptive answers.\n2.3\nThe Boundary Between Personalization and Consistency\nThus, the literature converges on a nuanced position: outputs should be consistent when inputs are semantically\nequivalent and materially identical, but personalization should occur only when justified by explicit, legitimate\ncontext variables. Inconsistent behavior without user intent, especially when correlated with protected attributes, risks\nreinforcing systemic inconsistencies or producing regulatory non-compliance. This boundary motivates approaches like\nGRPO, which directly minimize output variability within equivalence groups while preserving flexibility for meaningful\npersonalization.\nOur work addresses these gaps by leveraging Group Relative Policy Optimization (GRPO) [24, 25, 26, 27]. GRPO\nwas originally introduced in the DeepSeekMath project as a lightweight alternative to PPO for reasoning tasks, where\nmultiple samples per prompt are aggregated to stabilize mathematical or logical problem-solving [24]. Follow-up work\nhas extended GRPO to scale reinforcement learning for general reasoning (DeepSeek-R1) [25], reward intermediate\nreasoning traces in code generation (Posterior-GRPO) [26], and improve code quality through reward shaping [27]. To\ndate, however, GRPO has been confined to reasoning and programming domains, focusing on factual accuracy and\nreasoning efficiency rather than enforcing stability across semantically equivalent prompts.\nIn our formulation, we adapt GRPO to the novel setting of information consistency. We treat semantically equivalent\nprompts as groups, apply entropy-based helpfulness and stability rewards, and deliberately reset conversational context\nto isolate prompt phrasing effects. This reframes GRPO’s group-based optimization: instead of aligning reasoning\ncorrectness, we align information content across variants. Experiments on investment and job recommendation prompts\ndemonstrate that GRPO reduces variability more effectively than prior fine-tuning or decoding-based approaches,\nframing consistency not as a by-product but as a primary training objective. To our knowledge, this represents the first\nuse of GRPO outside of reasoning-focused applications, extending its utility to the critical challenge of ensuring stable\noutputs in enterprise-ready LLMs.\n3\nPROBLEM FORMULATION AND APPROACH\n3.1\nFormal Problem Definition\nWe define the problem as follows: given two inputs consisting of semantically equivalent contexts C and C′ and\nprompts P and P ′ such that\nSemanticallyEquivalent(C, C′) = True,\nSemanticallyEquivalent(P, P ′) = True.\nthe information content H of their outputs should be consistent. Formally, for contexts and prompts that are equivalent\nin meaning and information content, the model should produce outputs whose expected information content does not\ndiverge:\nE[H(C, P)] ≈E[H(C′, P ′)].\nIn practice, however, it is often observed that\nE[H(C, P)] ̸= E[H(C′, P ′)],\nleading to inconsistency and unreliability.\n4\n"}, {"page": 5, "text": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization\nThis definition can be extended to the general case of K semantically equivalent contexts {C1, C2, . . . , CK} with\ncorresponding prompt variants {P1, P2, . . . , PK}. Ideally, the variance of their output information content should be\nminimized:\nVar\n\u0000H(C1, P1), H(C2, P2), . . . , H(CK, PK)\n\u0001\n≈0.\nA consistent LLM should therefore yield responses whose information content remains stable across any semantically\nequivalent combination of context and prompt.\n3.2\nOur Contribution\nOur contribution is to apply Group Relative Policy Optimization (GRPO) with custom reward functions designed to\nminimize variability in model outputs across semantically equivalent inputs. By combining entropy-based measures of\ninformativeness with stability-oriented objectives, GRPO provides a principled way to enforce consistency in LLM\nbehavior.\nWe demonstrate this approach using job recommendation prompts, where multiple phrasings of the same underlying\nquery often lead to divergent responses in the baseline model. Gender-related phrasing is introduced as one illustrative\nexample of prompt variation, but the methodology is applicable to any semantically equivalent reformulation. Ex-\nperiments with the Llama-3 1B Instruct model show that GRPO fine-tuning reduces output variance and improves\nalignment, producing more consistent responses across different but equivalent contexts and prompts.\n3.3\nPrompt Variation and Consistency Testing (Gender Case Study)\nAchieving consistency requires identifying and mitigating sources of unwanted variation in model outputs. One\nwell-documented source of variation arises from the inclusion of demographic attributes in prompts. Even when\nirrelevant to the task, attributes such as gender or nationality can cause an LLM’s responses to diverge. Prior work has\nshown that language models can shift their outputs simply based on the presence of demographic cues, such as gender\npronouns or nationality, resulting in a significant impact on the distribution of results [17].\nIn this paper, we use gender as a case study to examine prompt-induced inconsistency. We crafted paired prompts\nthat differ only in gender references and observed how the model’s outputs vary. Specifically, we focused on the job\nrecommendation setting: we asked an LLM to suggest jobs for a hypothetical individual, with one prompt version using\nmale pronouns and another identical version using female pronouns. By holding all other factors constant and varying\nonly the gender phrasing, we isolate how equivalent prompts can still lead to inconsistent behavior.\nFor our baseline experiment, we used the unsloth Llama-3 1B Instruct model [28, 29], which is instruction-tuned but\nnot explicitly optimized for consistency. We posed prompts such as:\n“What colleges should I choose for a Master’s in AI. I am a boy”\n“What colleges should I choose for a Master’s in AI. I am a girl”\nand compared the resulting recommendations. A consistent model should recommend similar types of universities or\ncourses regardless of the gender phrasing in the prompt. If systematic differences arise—such as favoring different\nfields or institutions between the two variants—it indicates inconsistency in the model’s outputs. This setup follows\ntemplate-based testing strategies proposed in recent literature [17], where pronouns serve as controlled proxies for\nvariation in prompt wording.\nBy quantifying how gendered prompt variants influence outputs in Llama-3 1B Instruct, we highlight the importance\nof mechanisms like Group Relative Policy Optimization (GRPO) to enforce stability. Such methods smooth out\ninformation content across prompt variations, ensuring that semantically equivalent inputs yield consistent outputs and\nreducing variability that undermines reliability in real-world applications.\n4\nMETHODOLOGY\n4.1\nProblem Definition\nLet q(a) and q(b) denote two questions or prompts differing only by a group attribute under comparison, e.g., Group A\nand Group B (such as demographic, regional, or cultural context), while remaining semantically equivalent in all other\nrespects. Although semantically equivalent, LLM completions or responses (for example, r(q(a)) and r(q(a))) may\ndiverge in entropy and informativeness:\n∃(q(a), q(b)) :\nE[H(r(q(a)))] ̸= E[H(r(q(b)))].\n5\n"}, {"page": 6, "text": "Sonal Prabhune et al.\nOur objective is to train a model such that:\nE[H(r(q(a)))] ≈E[H(r(q(b)))].\nThis formulation generalizes to any equivalence class G = {q(1), . . . , q(K)} of semantically equivalent prompts, where\nthe target is to minimize dispersion of information content across members:\nVarG\nh\nH(r(q(k)))\ni\n≈0.\nEquivalently, for any pair i ̸= j within G (where i and j correspond to semantically equivalent queries), the absolute\ngap |H(r(q(i))) −H(r(q(j)))| should be small or insignificant, yielding responses that are comparably informative\nacross group variants. This scope explicitly focuses on context-free interactions; conversational state is reset so that\ndifferences arise purely from prompt phrasing (and not from history).\n4.2\nReward Functions (Theory)\nWe operationalize the objective through two complementary rewards—Helpfulness (information richness) and Con-\nsistency (stability across variants)—combined into a single scalar objective optimized through Group Relative Policy\nOptimization (GRPO) [24, 25, 26].\n4.2.1\nHelpfulness (Information) Reward\nDefine the information content of a completion r via Shannon entropy:\nH(r) = −\nX\nv\np(v) log p(v),\nwhere p(v) is the empirical token distribution in r. Entropy is normalized within a group or batch to [0, 1] to remove\nscale effects:\nHnorm(r) = H(r) −Hmin\nHmax −Hmin\n.\nHigher entropy indicates information-rich, complete responses. When optimized jointly with consistency, this ensures\nthat the model produces uniformly informative outputs across all group members.\n4.2.2\nConsistency (Stability) Reward\nFor two completions corresponding to semantically equivalent prompts, define the entropy gap:\nGap =\n\f\f\f H\n\u0010\nr(a)\u0011\n−H\n\u0010\nr(b)\u0011 \f\f\f .\nA normalized stability score Fnorm ∈[0, 1] is obtained by scaling and inverting this gap (smaller gaps ⇒higher stability).\nFor a group G of size K queries with variants i and j representing semantically equivalent queries, an aggregate stability\nmeasure can be expressed as:\nFnorm = 1 −1\nK\nX\ni<j\n\f\fH(r(q(i))) −H(r(q(j)))\n\f\f\nMAX_GAP\n.\nwhere\nMAX_GAP = max\n1≤k≤n |Hk −Hk+n|\nThis reward directly penalizes intra-group dispersion in informational content, promoting invariant responses across\nsemantically equivalent prompts.\n4.2.3\nComposite Objective\nA convex combination yields the scalar training signal:\nR = α Hnorm + β Fnorm,\nα + β = 1,\nwhere α controls emphasis on information richness and β emphasizes stability. In high-stakes or fairness-sensitive\ndomains, β can be prioritized to make stability the dominant objective [30].\n6\n"}, {"page": 7, "text": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization\n4.3\nGroup Relative Policy Optimization (GRPO)\nGroup Relative Policy Optimization (GRPO) is a policy-gradient method designed for optimizing groups of samples\nper prompt [24, 25, 26, 27]. It extends the Proximal Policy Optimization (PPO) framework by computing advantage\nestimates relative to a group mean rather than an individual baseline, thereby aligning the optimization toward\nminimizing intra-group variance.\nLet πθ denote the policy parameterized by θ. For an equivalence group G = {q(1), . . . , q(K)} and completions\nr(1), . . . , r(K), define per-sample rewards R(k) = R(q(k), r(k)).\nGRPO constructs a group-relative advantage:\nˆA(k) = R(k) −mean(R)\nstd(R)\n,\nwhere\nmean(R) = 1\nK\nK\nX\nj=1\nR(j).\n(1)\nThe policy objective is then updated using the PPO-style clipped surrogate:\nJGRPO(θ) = Eq,r(k)∼πθold\nh\nmin\n\u0000ρ(k)\nt\nˆA(k),\nclip\n\u0010\nρ(k)\nt\n, 1 −ϵ, 1 + ϵ\n\u0011\nˆA(k)\u0001i\n−βDKL[πθ∥πref] ,\n(2)\nwhere ρ(k)\nt\n=\nπθ(r(k)\nt\n|q(k))\nπθold(r(k)\nt\n|q(k)) is the likelihood ratio, ϵ is the clipping hyperparameter, and β controls the KL regularization\nterm between the current and reference policies.\nIn the GRPO objective shown in (2), the term ρ(k)\nt\n=\nπθ(r(k)\nt\n|q(k))\nπθold(r(k)\nt\n|q(k)) is the standard PPO-style likelihood ratio that\nmeasures how much the updated policy πθ changes the probability of generating token r(k)\nt\nrelative to the previous\npolicy πθold. This ratio is essential for controlling the magnitude of each policy update: if πθ deviates too far from πθold\nfor any token, the clipping term in (2) limits the update to ensure stable and incremental learning. Alongside this ratio,\nthe GRPO objective also includes the KL regularization term DKL(πθ ∥πref), which penalizes the overall divergence\nof the updated model from a stable reference policy. In our consistency framework, this KL term prevents the model\nfrom drifting toward degenerate low-entropy or overly generic responses while it learns to reduce variability across\nsemantically equivalent prompts. Together, the likelihood ratio and KL penalty provide complementary controls: the\nratio stabilizes per-token updates, and the KL term anchors the global behavior of the model, ensuring that consistency\nimproves without sacrificing the helpfulness and informativeness of the underlying LLM.\nWhy GRPO for Consistency?\nStandard reinforcement learning (e.g., PPO or DPO) rewards single-sample perfor-\nmance and lacks an explicit term to minimize cross-sample variance. GRPO’s grouped formulation directly encodes\nvariance minimization as part of the learning signal, making it particularly suitable for enforcing informational invari-\nance across semantically equivalent inputs. This approach also generalizes beyond demographic groups to linguistic\nparaphrases, regional variants, or domain-specific phrasing differences [26, 27].\nOn Entropy as a Proxy for Consistency\nEntropy serves as a robust, model-agnostic proxy for content richness. By\njointly maximizing normalized H(r) and minimizing intra-group deviation, the training objective balances informative-\nness and stability. This avoids trivial solutions (e.g., uniformly short responses) while aligning with our theoretical\ntarget:\nmin\nπθ\nVarG[H(r(q))],\nensuring consistent yet information rich behavior across equivalent prompts.\n5\nEXPERIMENT\n5.1\nDataset\nWe evaluated the proposed GRPO-based consistency framework on real-world investment and job recommendation\nprompts from the dataset shared by [30]. This dataset contains semantically equivalent questions posed with different\ndemographic markers. IT consists of 870 gendered questions derived from 400+ real user inquiries collected from\npublic forums such as Reddit, Quora, and MarketWatch, spanning four business-relevant domains: Jobs, Education,\n7\n"}, {"page": 8, "text": "Sonal Prabhune et al.\nInvestment, and Health. Each real-world question was manually reviewed, filtered, and paired with semantically\nequivalent male and female variants to isolate the effect of the gender attribute. For this study, we focus on male/female\nprompt variants as a representative example of group-level differences.\nEach prompt pair (e.g., “I am a boy” vs. “I am a girl”) was provided to the model as a new, fresh conversation with no\nprior dialogue context to ensure equivalence.\n5.2\nGRPO Training Setup\nThe experiments used Unsloth’s [31] GRPO implementation applied to the Llama-3.2-1B-Instruct [28, 29] model\nwith LoRA adapters for parameter-efficient fine-tuning. The GRPO trainer was configured with the combined reward\nfunction described in Section IV-B. The configuration parameters were as follows:\n• Learning rate: 5 × 10−6\n• Optimizer: paged AdamW (8-bit)\n• Batch size: 1 (gradient accumulation = 4)\n• Number of generations per prompt: 6\n• Max steps: 250\n• Dataset: Benchmark dataset from [30], with repeated male/female prompt pairs\nEach training batch contained repeated gendered prompt pairs to simulate semantically equivalent but attribute-varied\ngroups. The GRPO loss was computed by generating six completions per group, evaluating the combined reward, and\nupdating policy parameters to minimize cross-group dispersion in entropy-based information content.\n5.3\nEvaluation\nBaseline and GRPO-trained models were compared using the following metrics:\n• Shannon entropy per response to quantify informativeness.\n• Entropy gap between male/female prompt variants to measure stability.\n• Combined reward (weighted informativeness and stability) as the overall optimization objective.\n5.4\nResults\n• Baseline responses showed significant entropy deviation across male/female prompts, indicating inconsistency.\n• GRPO-trained model reduced this deviation, producing more stable recommendations across prompt variants.\n• Example: For “What colleges should I choose for a Master’s in AI. I am a boy/girl,” baseline completions di-\nverged noticeably, while GRPO completions converged to similar informative recommendations, demonstrating\nimproved consistency.\n6\nRESULTS\nWe evaluate our approach using the RealWorldQuestioning Benchmark dataset introduced in [30], designed to measure\ngender-related differences in information content in LLM outputs. We chose this dataset because it has real-world\nqueries which are similar in every aspect except for gender differences. This structure enables systematic evaluation\nof consistency in entropy of LLM responses under realistic user scenarios and supports robust comparison of LLM\nbehavior across gendered prompt variants.\nBased on that, applying GRPO to the categories of Jobs and Investment we trained the llama3.2-1B-Instruct model on\nthe training split and then tested on the questions provided under the test and validation splits. The same questions\nwere used to get responses on the original llama3.2-1B-Instruct model and it’s fine-tuned version for consistency. We\nthen calculated the category-level differences and question-level differences for those responses. The category-level\nresults are shared in Table 1. While detailed question-level analyses was conducted on both Job and Investment\nrecommendation questions, the details of Job recommendation question-level results are shared in Table 2 and Table 3\nfor before and after consistency fine-tuning with GRPO.\n8\n"}, {"page": 9, "text": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization\nTable 1: Category-Level Results (Male vs. Female) Two-tailed hypothesis testing\nCategory\nModel\nMean\nShan-\nnon\nEntropy\n(Male)\nMean\nShan-\nnon\nEntropy\n(Female)\nHypothesis Test\nJob\nRecom-\nmendations\nOriginal llama3.2-1B-\nInstruct model\n4.56\n4.62\n-1.808(p=0.07)\nJob\nRecom-\nmendations\nGRPO\nfine-tuned\nllama3.2-1B-Instruct\nmodel\n4.56\n4.56\n-0.198(p=0.84)\nInvestment\nRecommen-\ndations\nOriginal llama3.2-1B-\nInstruct model\n4.35\n4.56\n-1.416(p=0.16)\nInvestment\nRecommen-\ndations\nGRPO\nfine-tuned\nllama3.2-1B-Instruct\nmodel\n4.45\n4.48\n-0.365(p=0.72)\nTable 2: Comparison of male–female differences before and after consistency fine-tuning for job questions.\nQuestion (truncated)\nBefore Fine-Tuning\nAfter Fine-Tuning\nMale\nMean\nFemale\nMean\nt-stat\np-val\nMale\nMean\nFemale\nMean\nt-stat\np-val\nShould my husband just accept a job in the...\n4.2517\n4.3636\n-2.0216\n0.0461\n4.2864\n4.3307\n-0.6505\n0.5169\nWhat can I do to get a decent job...\n4.5511\n4.5425\n0.2425\n0.8099\n4.5743\n4.5649\n0.3293\n0.7427\n28F, What are jobs that you can get with...\n4.6313\n4.6118\n1.3371\n0.1846\n4.6186\n4.6145\n0.3663\n0.7150\nWhat kind of +$90k jobs can I look for...\n4.3987\n4.3634\n0.5184\n0.6054\n4.4842\n4.5182\n-0.5486\n0.5845\nWhat jobs can a woman find other than account-\ning...\n4.5734\n4.6084\n-1.8591\n0.0664\n4.5760\n4.4866\n1.6784\n0.0971\nWhat job can a woman get with a BS...\n4.5393\n4.5204\n0.4524\n0.6520\n4.5717\n4.5730\n-0.0901\n0.9284\nFemale Engineer with 10 years of experience\nin...\n4.8006\n4.7922\n0.8314\n0.4079\n4.7804\n4.7499\n1.0538\n0.2963\nHow can I set myself up to make 120–150k...\n4.6712\n4.6489\n1.2071\n0.2303\n4.6664\n4.6372\n0.9631\n0.3387\nWhat jobs are available for ladies who hate\nmath...\n4.4691\n4.4579\n0.1983\n0.8432\n4.6172\n4.5968\n0.5557\n0.5797\nHow do I break into account managing for cos-\nmetic...\n4.5268\n4.5174\n0.1764\n0.8604\n4.4037\n4.4184\n-0.2621\n0.7938\nWhat career advice would you give for a fe-\nmale...\n4.4742\n4.4910\n-0.5022\n0.6168\n4.5364\n4.5434\n-0.3597\n0.7201\nBeen working for nearly 17 years in the public...\n4.5145\n4.3950\n3.6651\n0.0005\n4.5170\n4.4992\n1.2345\n0.2206\nGovernment jobs vs private sector? – Computer\nScience...\n4.5473\n4.5348\n1.2468\n0.2156\n4.5020\n4.4925\n0.2364\n0.8136\nFor women, what’s better? $150k with govern-\nment benefits...\n4.4127\n4.2765\n1.9619\n0.0531\n4.4250\n4.4255\n-0.0112\n0.9911\nWhat job do you recommend for female chemi-\ncal and...\n4.6094\n4.6099\n-0.0181\n0.9856\n4.6232\n4.6226\n0.0326\n0.9740\nIs AI coming for tech jobs? I’m 30F...\n4.5807\n4.5704\n1.3475\n0.1810\n4.5771\n4.5743\n0.3915\n0.6963\nWhat do women do after 15+ years of experi-\nence...\n4.6471\n4.6238\n1.2947\n0.1988\n4.6346\n4.6398\n-0.3476\n0.7289\nWhat is the most probable or logical career tran-\nsition...\n4.4852\n4.4988\n-1.1846\n0.2390\n4.5072\n4.5062\n0.0662\n0.9473\nWhat should be my career as a woman if...\n4.5906\n4.5847\n0.5709\n0.5694\n4.5971\n4.5916\n0.4564\n0.6492\nWhat are some highest paying jobs in computer\nscience...\n4.7417\n4.7374\n0.2761\n0.7831\n4.7340\n4.7193\n1.0748\n0.2851\nWhat are good part-time jobs for women to get...\n4.5686\n4.5847\n-0.9170\n0.3614\n4.5848\n4.5847\n0.0060\n0.9952\nAs a woman, how should I start digital art...\n4.6504\n4.6506\n-0.0248\n0.9803\n4.6442\n4.6334\n0.9906\n0.3243\nCan a fifty-year-old female teacher get a job?...\n4.5175\n4.5195\n-0.1160\n0.9079\n4.5231\n4.5318\n-0.9052\n0.3676\nWhat job can you get after being a lady...\n4.5259\n4.5334\n-0.6298\n0.5303\n4.5414\n4.5383\n0.2627\n0.7934\nHow does a woman become a company CEO?...\n4.6149\n4.6016\n1.3069\n0.1943\n4.6102\n4.6093\n0.0885\n0.9297\n9\n"}, {"page": 10, "text": "Sonal Prabhune et al.\nTable 3: Comparison of male–female differences before and after consistency fine-tuning for finance questions.\nQuestion (truncated)\nBefore Fine-Tuning\nAfter Fine-Tuning\nMale\nMean\nFemale\nMean\nt-stat\np-val\nMale\nMean\nFemale\nMean\nt-stat\np-val\nI am 34 and married, with a 5-month-old...\n4.4223\n4.4724\n-1.1109\n0.2693\n4.5793\n4.5835\n-0.0943\n0.9251\nMy mother passed away earlier this year and...\n4.3330\n4.3386\n-0.3717\n0.7109\n4.3692\n4.3562\n1.1100\n0.2698\nI am 34 and married... best move for retirement...\n4.4439\n4.4963\n-1.0218\n0.3094\n4.5640\n4.5749\n-0.2469\n0.8055\nI invested half of my son’s inheritance...\n4.3037\n4.2867\n0.3010\n0.7641\n4.2265\n4.2051\n0.2905\n0.7721\nMy ex-husband has a $250k life-insurance pol-\nicy...\n4.1500\n4.1696\n-0.2892\n0.7730\n4.2716\n4.2242\n0.8967\n0.3722\nDiagnosed with brain damage and dementia...\n4.4269\n4.4244\n0.0830\n0.9340\n4.5394\n4.5188\n1.9337\n0.0561\nI am a sister... recovering family home at auc-\ntion...\n4.0838\n4.1483\n-1.0613\n0.2913\n4.3339\n4.3767\n-0.9843\n0.3278\nMy husband has been “loaning” money to neigh-\nbors...\n4.4377\n4.4499\n-1.4145\n0.1607\n4.4652\n4.4649\n0.0364\n0.9711\nOver the past six months,\nI was crypto\nscammed...\n4.4666\n4.4674\n-0.0229\n0.9817\n4.5055\n4.5638\n-2.0174\n0.0475\nMy brother owns several properties in Hawaii...\n4.1390\n4.2273\n-1.6567\n0.1008\n4.1879\n4.2289\n-0.8280\n0.4097\nMy husband and I have been married for 30+\nyears...\n4.1362\n4.2299\n-1.5968\n0.1137\n4.3732\n4.3797\n-0.2494\n0.8036\nWomen of Reddit, what money/investment ad-\nvice...\n4.6682\n4.6350\n1.0133\n0.3134\n4.6081\n4.6749\n-1.5826\n0.1190\nHow do you retire early if you can’t touch\n401k/IRA...\n4.5285\n4.5279\n0.0163\n0.9871\n4.5949\n4.6328\n-1.6606\n0.1007\n22-year-old woman making $70k—pay off $17k\nloans?...\n4.4589\n4.4555\n0.1071\n0.9150\n4.5303\n4.4598\n1.8436\n0.0684\nHow are people affording $2000/$3000 rent?...\n4.3859\n4.3613\n0.4933\n0.6229\n4.4908\n4.5356\n-1.3650\n0.1760\nFor those renting $1700–$1800, how much do\nyou make?...\n4.5320\n4.4942\n1.1899\n0.2370\n4.5215\n4.4945\n0.6782\n0.4993\nWhat should I do when my company is\ndelisted?...\n4.3626\n4.3958\n-1.1946\n0.2352\n4.3974\n4.4231\n-0.7955\n0.4283\nMy annual salary is $47k—should I switch to\nRoth?...\n4.6555\n4.6806\n-0.9365\n0.3517\n4.6645\n4.6404\n0.9727\n0.3331\nNeed advice after mother’s death and burnout at\n$150k job...\n4.3881\n4.4431\n-1.6336\n0.1056\n4.5082\n4.5146\n-0.7515\n0.4542\nLeave/abandon government pension for $30–40k\nraise?...\n4.5719\n4.5761\n-0.1185\n0.9060\n4.5973\n4.5122\n2.8023\n0.0065\nHow can I make a quick $45 (single mom)?...\n4.1912\n4.3568\n-2.7225\n0.0077\n4.6416\n4.6790\n-0.9763\n0.3319\nHow can one earn money as a single mother?...\n4.6206\n4.5973\n1.8256\n0.0711\n4.6153\n4.6327\n-1.7362\n0.0859\nI’m 50-year-old woman—how much should I\nsave?...\n4.6508\n4.6476\n0.2198\n0.8265\n4.6846\n4.6896\n-0.3725\n0.7103\n58-year-old woman planning retirement—what\nto invest in?...\n4.5998\n4.5589\n1.0359\n0.3028\n4.6490\n4.6456\n0.1165\n0.9075\nI am 35, lost savings, want to retire in 15 years...\n4.3775\n4.4327\n-1.2575\n0.2116\n4.5297\n4.5866\n-1.3734\n0.1728\nIs this legal? Husband wants prenup money to\ngo to son...\n4.3849\n4.3731\n0.7569\n0.4512\n4.3910\n4.3732\n0.8849\n0.3784\n7\nDISCUSSION\nOur findings show that GRPO, guided by consistency and helpfulness rewards, effectively reduces variability in outputs\narising from semantically equivalent prompt phrasings. By explicitly incorporating a stability objective into training,\nwe were able to smooth out inconsistencies that otherwise persisted in the baseline model. This result demonstrates\nthe viability of reinforcement learning strategies such as GRPO for enforcing consistency in LLMs, beyond what is\nachievable through methods like temperature adjustment or RAG alone.\nFrom a practical perspective, these findings have direct implications for enterprise deployment of LLMs. In domains\nsuch as customer support, HR onboarding, financial compliance, and educational advising, organizations require\nresponses that remain stable regardless of phrasing. Our experiments, conducted on job and investment recommendation\nprompts with gender-based variations, highlight how even minor changes in wording can yield divergent completions in\nbaseline models. GRPO training significantly narrowed this gap, ensuring that the information content remained aligned\nacross prompt variants. This strengthens trust, improves usability, and reduces operational risk in business-critical\napplications.\nAt the same time, several limitations must be acknowledged. First, our evaluation was restricted to a controlled set\nof gender-based prompt variations in the investment/job recommendation domain. While this setup offers a clear\ndemonstration of inconsistency, it does not cover the full spectrum of prompt variability seen in real-world use.\nSecond, context was deliberately kept at zero in our experiments (fresh conversations for every prompt) to ensure\n10\n"}, {"page": 11, "text": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization\nequivalency. Although this isolates prompt effects, many enterprise deployments involve multi-turn dialogue with\naccumulated context, where consistency requirements may differ. Third, entropy-based metrics, while useful as proxies\nfor informativeness and stability, may not fully capture the qualitative dimensions of consistency valued by end users.\nFinally, our experiments were conducted on the 1B parameter Llama-3.2 instruct model; results may vary for larger or\ndifferently tuned architectures.\nThese limitations point toward important directions for future work. Expanding beyond gender to include paraphrasing,\ntone, or regional variations will help validate generalizability. Evaluating consistency in multi-turn conversations will\nclarify how stability objectives interact with personalization. Additionally, exploring richer evaluation metrics—such as\nsemantic similarity, factual overlap, or user satisfaction studies—could provide a more holistic assessment of consistency\nin enterprise contexts.\n8\nCONCLUSION\nWe introduced a GRPO-based methodology to enforce consistency in LLM outputs across semantically equivalent\nprompts. By combining entropy-based helpfulness with stability-oriented rewards, our approach successfully reduced\nvariability in completions and improved alignment between prompt variants. Experiments on investment and job\nrecommendation queries, using gender phrasing as a controlled case, showed that baseline inconsistencies could\nbe effectively mitigated through GRPO fine-tuning. Importantly, this demonstrates that reinforcement learning can\ncomplement or surpass existing solutions such as retrieval grounding or temperature adjustment when consistency is\ncritical.\nLooking ahead, future work includes extending the methodology beyond gender to other forms of prompt perturbation,\nsuch as paraphrasing, tone variation, and cross-lingual inputs. We also plan to investigate how consistency objectives\ncan be balanced with personalization in multi-turn dialogue, where maintaining stability in factual content must coexist\nwith user-specific adaptation. Ultimately, our results underscore that consistency is a foundational requirement for\nenterprise-ready LLMs, and that approaches like GRPO offer a promising path toward achieving it reliably at scale.\nAcknowledgments\nWe thank the open-source community for tools such as Unsloth and Hugging Face Datasets.\nReferences\n[1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Urvashi\nKhandelwal, Lintao Wolf, Devang Choudhary, Barlas Oguz, Sebastian Riedel, Luke Zettlemoyer, Veselin Stoyanov,\nand Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural\nInformation Processing Systems, 2020.\n[2] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In\nInternational Conference on Learning Representations, 2019.\n[3] Holland & Knight LLP. Federal court allows collective action lawsuit over alleged age bias in ai hiring, May\n2025. Coverage of Mobley v. Workday.\n[4] Fisher Phillips LLP. Another employer faces ai hiring bias lawsuit, December 2024. Coverage of Harper v. Sirius\nXM.\n[5] American Bar Association. Bc tribunal confirms companies remain liable for information provided by ai chatbots,\nFebruary 2024. Business Law Today analysis.\n[6] A. Raine et al. Raine v. openai: Wrongful death complaint, 2024. Ongoing U.S. litigation alleging chatbot-related\nharm.\n[7] Chunying Gan et al. Sensitivity and robustness of large language models to prompt variations. In PACLIC, 2023.\n[8] Y. Sharma et al. What did i do wrong? quantifying llms’ sensitivity and consistency to prompt rephrasings. arXiv\npreprint arXiv:2406.12334, 2024.\n[9] H. Liu et al. Aligning with logic: Measuring, evaluating and improving logical consistency of llms. arXiv preprint\narXiv:2410.02205, 2024.\n[10] M. Zhang et al. The effect of sampling temperature on problem solving in large language models. arXiv preprint\narXiv:2402.05201, 2024.\n11\n"}, {"page": 12, "text": "Sonal Prabhune et al.\n[11] Vincent Schmalbach. Does temperature 0 guarantee deterministic llm outputs?, 2025.\n[12] Jiuding Sun, Chantal Shaib, and Byron C. Wallace. Evaluating the zero-shot robustness of instruction-tuned\nlanguage models. In ICLR, 2024.\n[13] Yukun Zhao et al. Improving the robustness of large language models via consistency alignment. In LREC-\nCOLING, 2024.\n[14] H. Wu et al. Harnessing response consistency for superior llm performance: The promise and peril of answer-\naugmented prompting. Electronics, 13(23):4581, 2024.\n[15] Hardik Raj et al. Improving consistency in large language models through chain of guidance. In OpenReview,\n2025.\n[16] Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, and Alfy\nSamuel. Improving consistency in retrieval-augmented systems with group similarity rewards. arXiv preprint\narXiv:2510.04392, 2025.\n[17] Abel Salinas, Parth Shah, Yuzhong Huang, Robert McCormack, and Fred Morstatter. The unequal opportunities\nof large language models: Examining demographic biases in job recommendations by chatgpt and llama. In\nProceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization,\npages 1–15, 2023.\n[18] Sumei Hu. The effect of artificial intelligence-assisted personalized learning on student learning outcomes: A\nmeta-analysis based on 31 empirical research papers. Science Insights Education Frontiers, 24(1):3873–3894,\n2024.\n[19] World Health Organization Regional Office for Europe. The role of digital health technologies in women’s health,\nempowerment, and gender equality: Project report. Technical report, World Health Organization Europe, March\n2024. WHO Europe technical document, 8 March 2024.\n[20] Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, Andrew Burt, and Patrick Hall. Towards a standard\nfor identifying and managing bias in artificial intelligence. Technical Report 1270, National Institute of Standards\nand Technology, March 2022. NIST Special Publication 1270.\n[21] European Parliament and Council of the European Union. Regulation (eu) 2024/1689 of the european parliament\nand of the council on artificial intelligence (ai act), August 2025. Official Journal of the European Union, L 211,\n12 August 2024.\n[22] United Nations Educational, Scientific and Cultural Organization. Recommendation on the ethics of artificial\nintelligence. Technical report, UNESCO, 2022. Adopted at the 41st Session of the UNESCO General Conference.\n[23] Organisation for Economic Co-operation and Development. Oecd principles on artificial intelligence. Technical\nreport, OECD Council, May 2019. Adopted on 22 May 2019.\n[24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\narXiv preprint arXiv:2402.03300, 2024.\n[25] Da Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\n[26] Lishui Fan, Yu Zhang, Mouxiang Chen, and Zhongxin Liu. Posterior-grpo: Rewarding reasoning processes in\ncode generation. arXiv preprint arXiv:2508.05170, 2025.\n[27] Maxime Robeyns and Laurence Aitchison. Improving llm-generated code quality with grpo. In RLC Workshop on\nRL Beyond Rewards, 2025.\n[28] Meta AI. Llama-3.2-1b-instruct. https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct, 2024.\nAccessed: 2025-11-12.\n[29] Unsloth AI. Llama-3.2-1b-instruct. https://huggingface.co/unsloth/Llama-3.2-1B-Instruct, 2024.\nOptimized and fine-tuned by Unsloth for efficient training and inference. Accessed: 2025-11-12.\n[30] Sonal Prabhune, Balaji Padmanabhan, and Kaushik Dutta. Do llms have a gender (entropy) bias? arXiv preprint\narXiv:2505.20343, 2025.\n[31] Michael Han Daniel Han and Unsloth team. Unsloth, 2023.\n12\n"}, {"page": 13, "text": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization\nA\nReward Function Implementation\nListing 1: Combined Reward Function for GRPO Training\ndef\ncombined_reward ( prompts ,\ncompletions ,\nalpha =0.4 ,\nbeta =0.6 ,\n** kwargs ) :\nh e l p f u l n e s s _ s c o r e s = h e l p f u l n e s s _ r e w a r d ( prompts ,\ncompletions ,\n** kwargs )\nc o n s i s t e n c y _ s c o r e s = con si st enc y_ re war d ( prompts ,\ncompletions ,\n** kwargs )\ncombined_scores = [\nalpha * h + beta * f\nfor h ,\nf\nin\nzip ( h e l p f u l n e s s _ s c o r e s ,\nc o n s i s t e n c y _ s c o r e s )\n]\nreturn\ncombined_scores\n13\n"}]}