{"doc_id": "arxiv:2602.02731", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.02731.pdf", "meta": {"doc_id": "arxiv:2602.02731", "source": "arxiv", "arxiv_id": "2602.02731", "title": "Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors", "authors": ["Rohan Pandey", "Haijuan Yan", "Hong Yu", "Jack Tsai"], "published": "2026-02-02T19:46:46Z", "updated": "2026-02-02T19:46:46Z", "summary": "Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.02731v1", "url_pdf": "https://arxiv.org/pdf/2602.02731.pdf", "meta_path": "data/raw/arxiv/meta/2602.02731.json", "sha256": "28825f8063fb0db9454297873b49a54e14c522caf9933b15ac961afdc98bc326", "status": "ok", "fetched_at": "2026-02-18T02:20:00.180795+00:00"}, "pages": [{"page": 1, "text": "Predicting first-episode homelessness among US\nVeterans using longitudinal EHR data: time-varying\nmodels and social risk factors\nRohan Pandey1,2, Haijuan Yan1, Hong Yu1,2,3, and Jack Tsai4,5,6*\n1Center for Healthcare Organization and Implementation Research, VA Bedford Health Care, MA, USA\n2Manning College of Information and Computer Sciences, UMass Amherst, MA, USA\n3Miner School of Computer and Information Sciences, UMass Lowell, MA, USA\n4National Center on Homelessness among Veterans, VA Homeless Programs Office, Washington, DC, USA\n5School of Public Health, University of Texas Health Science Center at Houston, Houston, TX, USA\n6Department of Psychiatry, Yale University School of Medicine, New Haven, CT, USA\n**Corresponding author: Jack Tsai (Jack.Tsai@uth.tmc.edu)\nABSTRACT\nHomelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive\nintervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans\nAffairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017\n(prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to\nmodel the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine\nlearning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that\nincorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC)\nby 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30%\nat 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models\nunderperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups.\nThese results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable\nstrata, enabling targeted and data-informed prevention strategies for at-risk veterans.\n1 Introduction\nHomelessness remains a major public health challenge in the United States (U.S.). On a single night in January 2024, an\nestimated 771,480 people were experiencing homelessness, an 18% increase from 2023. Veterans account for about 5% of all\nUS adults experiencing homelessness, with 32,882 veterans experiencing homelessness on a single night in 2024, underscoring\npersistent risk in this population1. While there has been substantial progress in reducing veteran homelessness in the past 15\nyears, veteran homelessness remains one of the top priorities of the U.S. Department of Veterans Affairs (VA)2. In the broader\nU.S. population, homelessness is at record high levels1, underscoring the need for scalable prediction and prevention strategies.\nElectronic health records (EHRs) offer system-wide visibility into clinical, utilization, and social-needs data that could\nenable earlier identification of patients at risk for first-episode homelessness. The VA operates the largest integrated healthcare\nsystem in the U.S. and has maintained a national EHR system for more than four decades. Prior EHR-based studies in the\nVA have documented associations between physical comorbidities, mental health conditions (e.g., PTSD), and substance use\ndisorders among veterans3,4. Yet most of this work has been cross-sectional or relied on regression-based analyses with limited\nuse of predictive modeling.\nOnly a handful of studies have focused on predicting homelessness in the veteran population5–8, and these studies have not\ncompared diverse modeling approaches using contemporary machine-learning methods. In addition, although longitudinal\nEHR data are available, time-varying representations of patient histories are rarely used, and key social factors, including\nsocioeconomic status, education, housing, and access to healthy food, remain inconsistently captured and underused at scale9–11.\nHomelessness risk and access to preventive services vary across sociodemographic groups, including racial, age, and gender\nsubgroups among Veterans12, raising concerns that prediction models could either exacerbate or mitigate existing inequities.\nThese gaps are particularly important for clinical translation, as models must inform proactive outreach and triage. Early,\naccurate identification of patients at elevated risk for first-episode homelessness could enable targeted outreach, social-work\nreferral, and prevention services within large health systems such as the VA.\n1\narXiv:2602.02731v1  [cs.CL]  2 Feb 2026\n"}, {"page": 2, "text": "To address these gaps, we used national VA EHR data to conduct a retrospective prognostic modeling study. We\ncompared the performance of different models to predict first-episode homelessness among veterans over 3, 6, 9, and\n12 months (multiple future horizons) following a 1-year observation. Our contributions are threefold: (1) construction of\ntime-varying EHR representations using clinically informed condition persistence rules that govern how long diagnoses and\nsocial/behavioral problems remain active after they are last recorded; (2) explicit incorporation of social and behavioral\nfactors of health (SBFH) alongside demographics, service utilization, and medical and behavioral health conditions; and (3) a\ncomparison of architecturally distinct models under a consistent evaluation protocol. A schematic overview of the study design,\nobservation/prediction windows, and longitudinal representation construction is shown in Fig. 1.\nFigure 1. Study schematic: longitudinal EHR representations and homelessness risk prediction.\nRaw visit-level EHR data for an example patient are aggregated into fixed half-year intervals to form a baseline longitudinal\nrepresentation without persistence (H1: January-June 2016; H2: July-December 2016). A condition persistence framework\napplies domain-specific persistence policies (chronic persistent, ever-history, recurrent time-limited, episodic) to carry forward\nor limit condition activity across intervals, yielding a longitudinal representation in which shaded cells indicate values added by\nthe framework. The resulting patient-level profiles are converted into natural-language prompts organized by clinical domain\n(example shown) and used as inputs to three model classes: machine-learning models, masked-language models, and\nlarge-language models to independently predict the risk of first-episode homelessness within M months after baseline. The\nexample illustrates the half-year temporal representation; analogous representations were also constructed at quarterly and\nyearly aggregation levels for comparative analyses.\n2 Results\n2.1 Prevalence of Homelessness\nOf 6,105,401 veterans with a VA visit in 2016, 4,276,403 (70.0%) met eligibility for prediction (Supplementary Figure 1).\nWithin the 3-month window, 13,728 of 4,276,403 veterans experienced homelessness (0.32%). Prevalence increased with\nlonger windows: 26,818 (0.63%) within 6 months, 39,420 (0.92%) within 9 months, and 51,002 (1.19%) within 12 months.\nThese four cumulative prediction cohorts therefore contained 13,728, 26,818, 39,420, and 51,002 cases, respectively, with the\nremainder serving as non-cases. Detailed cohort characteristics are presented in Supplementary Table 1.\n2/43\n"}, {"page": 3, "text": "Model Class\nModel\nInput Representation\nPR AUC, %\nROC AUC, %\nPrediction Window = 3 Months\nMachine Learning\nElastic Net Logistic Regression\nStatic\n1.73 (1.44, 2.19)\n76.88 (75.03, 78.64)\nTime Varying\n1.87 (1.54, 2.45)\n76.71 (74.79, 78.46)\nRandom Forest\nStatic\n1.89 (1.57, 2.36)\n78.38 (76.57, 80.04)\nTime Varying\n2.11 (1.76, 2.68)\n79.27 (77.54, 80.94)\nXGBoost\nStatic\n2.08 (1.71, 2.8)\n79.57 (77.87, 81.22)\nTime Varying\n2.22 (1.83, 2.91)\n79.73 (77.95, 81.37)\nMasked Language\nModels\nModernBERT\nStatic\n1.91 (1.57, 2.43)\n77.84 (76, 79.56)\nTime Varying\n2.39 (1.8, 3.34)\n77.28 (75.39, 79)\nBioClinical ModernBERT\nStatic\n1.96 (1.63, 2.45)\n78.74 (76.93, 80.42)\nTime Varying\n2.34 (1.91, 3.09)\n78.78 (76.97, 80.53)\nLarge Language\nModels\nLlama-3.1-8B\nStatic\n1.75 (1.47, 2.21)\n78.36 (76.59, 80.04)\nTime Varying\n2.16 (1.77, 2.76)\n79.12 (77.43, 80.79)\nOpenBioLLM-8B\nStatic\n1.42 (1.23, 1.76)\n76.97 (75.11, 78.82)\nTime Varying\n2.32 (1.81, 3.1)\n78.31 (76.49, 79.95)\nPrediction Window = 6 Months\nMachine Learning\nElastic Net Logistic Regression\nStatic\n3.23 (2.8, 3.84)\n74.82 (73.45, 76.18)\nTime Varying\n3.28 (2.83, 3.97)\n74.83 (73.46, 76.18)\nRandom Forest\nStatic\n3.73 (3.24, 4.42)\n77.82 (76.59, 79.05)\nTime Varying\n3.86 (3.32, 4.61)\n77.98 (76.77, 79.24)\nXGBoost\nStatic\n3.62 (3.16, 4.28)\n78.53 (77.3, 79.75)\nTime Varying\n4.13 (3.48, 5.01)\n78.45 (77.2, 79.67)\nMasked Language\nModels\nModernBERT\nStatic\n3.43 (2.98, 4.03)\n77.32 (76.11, 78.59)\nTime Varying\n3.54 (3.07, 4.14)\n77.49 (76.23, 78.73)\nBioClinical ModernBERT\nStatic\n3.43 (2.96, 4.03)\n77.02 (75.75, 78.22)\nTime Varying\n3.58 (3.12, 4.21)\n76.6 (75.33, 77.93)\nLarge Language\nModels\nLlama-3.1-8B\nStatic\n3.44 (3.01, 4.02)\n77.26 (76.02, 78.53)\nTime Varying\n4.12 (3.52, 4.98)\n78.43 (77.21, 79.66)\nOpenBioLLM-8B\nStatic\n3.55 (3.15, 4.13)\n79.02 (77.83, 80.19)\nTime Varying\n3.65 (3.19, 4.36)\n78.33 (77.11, 79.54)\nPrediction Window = 9 Months\nMachine Learning\nElastic Net Logistic Regression\nStatic\n4.47 (4.02, 5.11)\n76.4 (75.32, 77.39)\nTime Varying\n4.52 (4.05, 5.22)\n76.3 (75.22, 77.32)\nRandom Forest\nStatic\n5.07 (4.56, 5.78)\n78.97 (78.02, 79.87)\nTime Varying\n5.23 (4.67, 5.95)\n79.03 (78.11, 79.95)\nXGBoost\nStatic\n4.93 (4.43, 5.62)\n78.62 (77.64, 79.55)\nTime Varying\n5.14 (4.59, 5.89)\n78.37 (77.39, 79.31)\nMasked Language\nModels\nModernBERT\nStatic\n4.95 (4.44, 5.63)\n78.38 (77.35, 79.32)\nTime Varying\n5.27 (4.68, 6.01)\n78.08 (77.08, 79.03)\nBioClinical ModernBERT\nStatic\n5.14 (4.62, 5.84)\n78.7 (77.75, 79.64)\nTime Varying\n5.16 (4.65, 5.84)\n78.72 (77.75, 79.68)\nLarge Language\nModels\nLlama-3.1-8B\nStatic\n4.03 (3.65, 4.53)\n77.47 (76.46, 78.38)\nTime Varying\n5.19 (4.64, 5.92)\n78.56 (77.6, 79.46)\nOpenBioLLM-8B\nStatic\n5.07 (4.58, 5.66)\n80.2 (79.28, 81.11)\nTime Varying\n4.42 (3.97, 5.03)\n76.68 (75.64, 77.68)\nPrediction Window = 12 Months\nMachine Learning\nElastic Net Logistic Regression\nStatic\n5.66 (5.17, 6.33)\n75.75 (74.77, 76.71)\nTime Varying\n5.72 (5.22, 6.41)\n75.75 (74.74, 76.72)\nRandom Forest\nStatic\n6.21 (5.68, 6.85)\n78.32 (77.43, 79.16)\nTime Varying\n6.39 (5.83, 7.06)\n78.39 (77.5, 79.24)\nXGBoost\nStatic\n6.47 (5.86, 7.23)\n78.1 (77.2, 78.96)\nTime Varying\n6.72 (6.06, 7.53)\n78.05 (77.14, 78.92)\nMasked Language\nModels\nModernBERT\nStatic\n6.11 (5.62, 6.74)\n78.5 (77.59, 79.35)\nTime Varying\n6.29 (5.76, 6.97)\n77.84 (76.9, 78.75)\nBioClinical ModernBERT\nStatic\n6.19 (5.65, 6.89)\n78.65 (77.78, 79.5)\nTime Varying\n6.65 (6.03, 7.4)\n77.99 (77.07, 78.86)\nLarge Language\nModels\nLlama-3.1-8B\nStatic\n5.01 (4.57, 5.54)\n73.72 (72.69, 74.74)\nTime Varying\n5.66 (5.16, 6.32)\n77.3 (76.38, 78.15)\nOpenBioLLM-8B\nStatic\n5.65 (5.17, 6.28)\n77.85 (76.93, 78.69)\nTime Varying\n5.99 (5.49, 6.66)\n78.06 (77.18, 78.93)\nTable 1. Performance of predictive models across prediction windows for static and proposed time-varying EHR\nrepresentations. Bold indicates highest PR-AUC within each prediction window.\n3/43\n"}, {"page": 4, "text": "Model Class\nModel\nRisk Group\nSize, P\nSensitivity (%)\n@ top-P\nSpecificity (%)\n@ top-P\nPPV (%)\n@ top-P\nO/E Ratio\n@ top-P\nPrediction Window = 3 Months\nMachine Learning\nElastic Net Logistic Regression\n0.01\n12.68 (10.35, 15.16)\n99.04 (99.03, 99.05)\n4.07 (3.32, 4.86)\n12.68 (10.35, 15.15)\n0.05\n30.76 (27.26, 34.11)\n95.08 (95.07, 95.09)\n1.97 (1.75, 2.19)\n6.15 (5.45, 6.82)\nRandom Forest\n0.01\n13.41 (10.93, 16.03)\n99.04 (99.03, 99.05)\n4.30 (3.51, 5.14)\n13.41 (10.93, 16.03)\n0.05\n33.09 (29.59, 36.59)\n95.09 (95.08, 95.10)\n2.12 (1.90, 2.35)\n6.62 (5.92, 7.32)\nXGBoost\n0.01\n12.83 (10.64, 15.45)\n99.04 (99.03, 99.05)\n4.12 (3.41, 4.96)\n12.83 (10.64, 15.45)\n0.05\n32.36 (29.01, 35.71)\n95.09 (95.08, 95.10)\n2.08 (1.86, 2.29)\n6.47 (5.80, 7.14)\nMasked Language\nModels\nModernBERT\n0.01\n12.24 (9.77, 14.58)\n99.04 (99.03, 99.04)\n3.93 (3.13, 4.68)\n12.25 (9.76, 14.58)\n0.05\n30.17 (26.96, 33.53)\n95.08 (95.07, 95.09)\n1.94 (1.73, 2.15)\n6.04 (5.39, 6.70)\nBioClinical ModernBERT\n0.01\n14.72 (12.39, 17.35)\n99.04 (99.04, 99.05)\n4.72 (3.97, 5.56)\n14.72 (12.39, 17.34)\n0.05\n32.80 (29.30, 36.30)\n95.09 (95.08, 95.10)\n2.10 (1.88, 2.33)\n6.56 (5.86, 7.26)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n13.85 (11.37, 16.33)\n99.04 (99.03, 99.05)\n4.44 (3.65, 5.24)\n13.85 (11.37, 16.32)\n0.05\n32.51 (29.15, 36.15)\n95.09 (95.08, 95.10)\n2.09 (1.87, 2.32)\n6.50 (5.83, 7.23)\nOpenBioLLM-8B\n0.01\n13.41 (11.37, 16.62)\n99.04 (99.03, 99.05)\n4.30 (3.65, 5.33)\n13.41 (11.37, 16.61)\n0.05\n31.92 (28.57, 35.57)\n95.09 (95.08, 95.10)\n2.05 (1.83, 2.28)\n6.38 (5.71, 7.11)\nPrediction Window = 6 Months\nMachine Learning\nElastic Net Logistic Regression\n0.01\n11.78 (10.14, 13.50)\n99.07 (99.06, 99.08)\n7.39 (6.36, 8.46)\n11.78 (10.14, 13.49)\n0.05\n27.59 (25.21, 29.90)\n95.14 (95.13, 95.16)\n3.46 (3.16, 3.75)\n5.52 (5.04, 5.98)\nRandom Forest\n0.01\n12.53 (10.81, 14.24)\n99.07 (99.06, 99.08)\n7.86 (6.78, 8.93)\n12.53 (10.81, 14.24)\n0.05\n29.53 (27.22, 31.99)\n95.15 (95.14, 95.17)\n3.70 (3.41, 4.01)\n5.91 (5.44, 6.40)\nXGBoost\n0.01\n12.53 (10.81, 14.32)\n99.07 (99.06, 99.08)\n7.86 (6.78, 8.98)\n12.53 (10.81, 14.31)\n0.05\n29.31 (26.92, 31.69)\n95.15 (95.14, 95.17)\n3.68 (3.38, 3.98)\n5.86 (5.38, 6.34)\nMasked Language\nModels\nModernBERT\n0.01\n12.53 (10.81, 14.32)\n99.07 (99.06, 99.08)\n7.86 (6.78, 8.98)\n12.53 (10.81, 14.31)\n0.05\n30.05 (27.67, 32.44)\n95.16 (95.14, 95.17)\n3.77 (3.47, 4.07)\n6.01 (5.53, 6.49)\nBioClinical ModernBERT\n0.01\n13.27 (11.41, 14.91)\n99.08 (99.07, 99.09)\n8.33 (7.15, 9.35)\n13.27 (11.40, 14.91)\n0.05\n29.53 (27.07, 31.99)\n95.15 (95.14, 95.17)\n3.70 (3.39, 4.01)\n5.91 (5.41, 6.40)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n12.83 (11.04, 14.47)\n99.07 (99.06, 99.08)\n8.04 (6.92, 9.07)\n12.83 (11.03, 14.46)\n0.05\n30.95 (28.49, 33.41)\n95.16 (95.15, 95.18)\n3.88 (3.57, 4.19)\n6.19 (5.70, 6.68)\nOpenBioLLM-8B\n0.01\n11.63 (10.37, 14.10)\n99.07 (99.06, 99.08)\n7.30 (6.50, 8.84)\n11.63 (10.36, 14.09)\n0.05\n30.28 (28.11, 32.96)\n95.16 (95.15, 95.18)\n3.80 (3.53, 4.13)\n6.06 (5.62, 6.59)\nPrediction Window = 9 Months\nMachine Learning\nElastic Net Logistic Regression\n0.01\n10.71 (9.34, 12.02)\n99.09 (99.08, 99.10)\n9.87 (8.60, 11.08)\n10.71 (9.33, 12.02)\n0.05\n26.69 (24.71, 28.61)\n95.20 (95.18, 95.22)\n4.92 (4.55, 5.27)\n5.34 (4.94, 5.72)\nRandom Forest\n0.01\n12.23 (10.86, 13.65)\n99.10 (99.09, 99.12)\n11.27 (10.00, 12.58)\n12.23 (10.85, 13.64)\n0.05\n29.17 (27.19, 31.05)\n95.22 (95.21, 95.24)\n5.38 (5.01, 5.72)\n5.83 (5.44, 6.21)\nXGBoost\n0.01\n11.97 (10.55, 13.39)\n99.10 (99.09, 99.11)\n11.04 (9.72, 12.34)\n11.97 (10.55, 13.39)\n0.05\n28.92 (26.89, 30.95)\n95.22 (95.20, 95.24)\n5.33 (4.96, 5.71)\n5.78 (5.38, 6.19)\nMasked Language\nModels\nModernBERT\n0.01\n12.08 (10.65, 13.50)\n99.10 (99.09, 99.12)\n11.13 (9.82, 12.44)\n12.08 (10.65, 13.49)\n0.05\n29.98 (27.90, 32.01)\n95.23 (95.21, 95.25)\n5.53 (5.14, 5.90)\n6.00 (5.58, 6.40)\nBioClinical ModernBERT\n0.01\n11.52 (10.25, 12.99)\n99.10 (99.09, 99.11)\n10.62 (9.44, 11.97)\n11.52 (10.24, 12.98)\n0.05\n30.44 (28.36, 32.32)\n95.24 (95.22, 95.25)\n5.61 (5.23, 5.96)\n6.09 (5.67, 6.46)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n12.38 (11.06, 13.90)\n99.11 (99.09, 99.12)\n11.41 (10.19, 12.81)\n12.38 (11.06, 13.90)\n0.05\n29.07 (27.09, 30.95)\n95.22 (95.21, 95.24)\n5.36 (4.99, 5.71)\n5.81 (5.42, 6.19)\nOpenBioLLM-8B\n0.01\n10.10 (9.28, 12.03)\n99.08 (99.08, 99.10)\n9.31 (8.56, 11.08)\n10.10 (9.28, 12.02)\n0.05\n27.45 (26.18, 30.19)\n95.21 (95.20, 95.23)\n5.06 (4.83, 5.56)\n5.49 (5.24, 6.04)\nPrediction Window = 12 Months\nMachine Learning\nElastic Net Logistic Regression\n0.01\n10.90 (9.76, 11.96)\n99.12 (99.11, 99.13)\n13.00 (11.64, 14.26)\n10.90 (9.76, 11.96)\n0.05\n26.39 (24.78, 28.00)\n95.26 (95.24, 95.28)\n6.30 (5.91, 6.68)\n5.28 (4.96, 5.60)\nRandom Forest\n0.01\n11.29 (10.12, 12.47)\n99.12 (99.11, 99.14)\n13.47 (12.06, 14.87)\n11.30 (10.11, 12.47)\n0.05\n28.12 (26.47, 29.84)\n95.28 (95.26, 95.30)\n6.71 (6.31, 7.12)\n5.62 (5.29, 5.97)\nXGBoost\n0.01\n11.57 (10.43, 12.78)\n99.13 (99.11, 99.14)\n13.80 (12.44, 15.24)\n11.57 (10.43, 12.78)\n0.05\n27.88 (26.20, 29.57)\n95.28 (95.26, 95.30)\n6.65 (6.25, 7.05)\n5.58 (5.24, 5.91)\nMasked Language\nModels\nModernBERT\n0.01\n11.10 (10.00, 12.24)\n99.12 (99.11, 99.14)\n13.24 (11.92, 14.59)\n11.10 (10.00, 12.23)\n0.05\n29.69 (27.88, 31.29)\n95.30 (95.28, 95.32)\n7.08 (6.65, 7.46)\n5.94 (5.58, 6.26)\nBioClinical ModernBERT\n0.01\n11.41 (10.24, 12.59)\n99.13 (99.11, 99.14)\n13.61 (12.20, 15.01)\n11.41 (10.24, 12.58)\n0.05\n29.18 (27.41, 30.78)\n95.29 (95.27, 95.31)\n6.96 (6.54, 7.34)\n5.84 (5.48, 6.16)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n9.76 (8.71, 10.90)\n99.11 (99.09, 99.12)\n11.65 (10.38, 13.00)\n9.77 (8.70, 10.90)\n0.05\n26.67 (25.14, 28.47)\n95.26 (95.24, 95.28)\n6.36 (6.00, 6.79)\n5.33 (5.03, 5.69)\nOpenBioLLM-8B\n0.01\n10.63 (10.04, 12.39)\n99.12 (99.11, 99.14)\n12.68 (11.97, 14.77)\n10.63 (10.04, 12.39)\n0.05\n28.00 (26.71, 30.04)\n95.28 (95.26, 95.30)\n6.68 (6.37, 7.16)\n5.60 (5.34, 6.01)\nTable 2. Performance of time-varying models by risk tier (top 1% and 5%) and prediction window.\n4/43\n"}, {"page": 5, "text": "2.2 Overall Model Performance\nAcross prediction windows, PR-AUCs increased with longer prediction horizons, mirroring higher event prevalence. For the\n3-month window (prevalence 0.32%), models achieved PR-AUCs of 1.42-2.39%. For 6, 9, and 12 months (prevalence rates of\n0.63%, 0.92%, and 1.19%, respectively), the ranges were 3.23-4.13%, 4.03-5.27%, and 5.01-6.72%, respectively (Table 1).\nThe best-performing models by window were ModernBERT with time-varying features at 3 months (PR-AUC 2.39%, 95% CI\n1.80-3.34) and 9 months (5.27%, 4.68-6.01), and XGBoost with time-varying features at 6 months (4.13%, 3.48-5.01) and\n12 months (6.72%, 6.06-7.53). Compared with static representations, incorporating clinically informed time-varying features\nimproved PR-AUCs in all but one model-window comparison. In ablation analyses, adding condition-persistence rules on top\nof the time-varying representation improved PR-AUC for most model-window combinations, with the largest average gains at\n6- and 12-month horizons (Supplementary Table 2). This supports the added value of clinically informed condition persistence.\nBecause time-varying representations with our condition persistence framework yielded the highest PR-AUCs across models\nand prediction windows, we focused subsequent analyses on the time-varying versions of each model.\n2.3 Concentration of Risk\nWe compared PR-AUC and ROC-AUC across three model classes (machine learning, masked language models, large language\nmodels) using both static and time-varying representations. Across prediction windows, time-varying models identified\n9.76-14.72% of all future homelessness cases when screening only the top 1% of patients by predicted risk, with a specificity\nof 99.04-99.13% (Table 2). Expanding the risk tier to the top 5% increased case capture to 26.39-33.09% (specificity 95.08-\n95.30%). Consistent with increasing outcome prevalence over longer horizons, PPV rose with longer windows and narrower\nrisk tiers. Among the best-performing time-varying models at each horizon (ModernBERT at 3 months and XGBoost at 12\nmonths), PPV at the 1% tier ranged from 3.93% (95% CI, 3.13-4.68) at 3 months to 13.80% (12.44-15.24) at 12 months, and at\nthe 5% tier from 1.94% (1.73-2.15) to 6.65% (6.25-7.05). The highest PPV observed was 18.71% (16.36-20.84) for XGBoost at\n12 months, when screening the top 0.5% at a specificity of 99.59% (Supplementary Table 3). This means that, in the observed\ndata, approximately 1 in 5 patients flagged subsequently experienced homelessness within 12 months. Risk enrichment within\nthese tiers was substantial: observed-to-expected (O/E) ratios ranged from 9.76 to 14.72 at the 1% tier and from 5.28 to 6.62 at\nthe 5% tier, indicating that flagged groups are roughly 5-15 times more likely to become homeless than the cohort average\n(Supplementary Table 3). Together, these results suggest that small, high-risk tiers (top 1-5%) can substantially concentrate\nfuture homelessness risk while preserving very high specificity, making them promising targets for proactive outreach and\nprevention efforts.\n2.4 Impact of predictors\nFigure 2. Model performance by predictor set and prediction window.\nPR-AUC, area under the precision-recall curve; ROC-AUC, area under the receiver operating characteristic curve; SD, standard\ndeviation; Demo, demographics; Codes, clinical and utilization codes (diagnosis indicators and service-utilization variables\nderived from ICD-10 diagnostic codes and VA outpatient stop codes, excluding SBFH); SBFH, social and behavioral factors;\nLR, Elastic Net logistic regression; RF, random forest; XGB, XGBoost; ModernBERT, ModernBERT-large model;\nBioClinBERT, BioClinical-ModernBERT model; LLaMA, LLaMA-3.1-8B model; OpenBio, OpenBioLLM-8B model.\n5/43\n"}, {"page": 6, "text": "Performance generally improved as predictor sets expanded from demographics alone to demographics plus clinical and\nutilization codes (diagnosis indicators and service-utilization variables derived from ICD-10 and VA stop codes, excluding\nSBFH) and then to demographics, codes, and social and behavioral factors of health (SBFH) across models and prediction\nhorizons (Figure. 2). For example, in Elastic Net Logistic Regression at the 3-month window, adding codes to demographics\nincreased PR-AUC from 0.60% to 1.60%, and adding SBFH increased PR-AUC to 1.87%. At 12 months, PR-AUC increased\nfrom 2.35% with demographics alone to 5.16% with the addition of codes, and a further jump to 6.67% with the addition of\nSBFH. Performance gains from including SBFH were observed across most models and prediction windows and tended to\nbe largest at longer horizons; for instance, for XGBoost, adding SBFH to demographics plus codes increased PR-AUC from\n3.97% to 5.14% and from 5.43% to 6.72% at the 9- and 12-month windows, respectively (Supplementary Table 4). These\nimprovements from adding SBFH represent relative gains of 15-30% for traditional ML models. Overall, Supplementary Table\n4 demonstrates a consistent pattern: adding clinical codes boosts discrimination over demographics alone, and adding SBFH\nusually yields a further gain, particularly at longer prediction windows.\nWe also assessed predictor importance using the Kernel SHAP method for the classical machine-learning models (e.g.,\nElastic Net Logistic Regression, Random Forest, and XGBoost; Supplementary Figure 2). Based on SHAP values, we identified\npredictors that increased model predictions toward a higher estimated risk of homelessness and those that decreased predictions,\nwhich we refer to as positive and negative predictors, respectively. Among the most influential positive predictors, older age\n(especially 70-79 and ≥80 years), 0% service-connected disability rating, and selected state-of-residence indicators were\nconsistently selected across models and prediction windows. In contrast, being married, divorced, or separated, higher VISN\n(Veterans Integrated Service Network) counts, and indicators of emergency or urgent care use were among the most consistent\nnegative predictors. Among diagnosis-related predictors, posttraumatic stress disorder (PTSD), sleep disorders, psychoses,\nand chronic obstructive pulmonary disease (COPD) frequently appeared as positive predictors. For substance use disorders,\ncannabis use disorder was a common positive predictor, whereas broader drug abuse diagnoses often had negative SHAP values.\nSocial and behavioral factors (SBFH) showed heterogeneous patterns: government insurance status and documented housing or\nviolence-related problems frequently appeared as positive predictors, particularly at shorter prediction windows, whereas coded\nemployment or financial problems more often appeared with negative SHAP values. Patterns of service utilization characterized\nby zero mental health, emergency/urgent care, or substance use-related visits were also common negative predictors. These\npatterns reflect how the models use available codes in combination with other covariates rather than indicating causal risk\nor protective effects. Accordingly, SHAP values should be interpreted as ranking predictors by their contribution to model\npredictions for this homelessness-prediction task, not as evidence of causal risk or protective factors.\n2.5 Subgroup Analyses\nFigure 3. Subgroup-specific performance of prediction models for homelessness risk for the 9-month prediction\nwindow.\nPR-AUC precision-recall area under the curve, CI confidence interval, RF random forest, ModernBERT ModernBERT-base\nmodel, Llama Llama-3.1-8B model. Subgroup estimates labeled “Unreliable” did not meet pre-specified reliability criteria\n(fewer than 20 positive or 20 negative cases in the subgroup and/or bootstrap CI width > 0.12).\nTo illustrate subgroup performance, we focused on the 9-month prediction window and three representative models corre-\nsponding to the best performers from each model class: Random Forest (ML), ModernBERT (MLM), and Llama-3.1-8B\n(LLM). Detailed subgroup results for all prediction windows and best-in-class models are reported in Supplementary Table 5.\nAcross race groups, estimated discrimination was highest in Black and White veterans (Figure 3, panel A). For ModernBERT,\nPR-AUCs were 5.34% (95% CI, 4.75-6.11%) in Black veterans and 4.92% (95% CI, 4.19-5.88%) in White veterans, with\n6/43\n"}, {"page": 7, "text": "similar patterns for Llama-3.1-8B and Random Forest. Performance estimates for American Indian, Asian, and Native Hawaiian\nveterans were lower and more variable; for instance, ModernBERT PR-AUCs ranged from approximately 2.5% to 6.3%, with\nwide confidence intervals extending up to about 15% reflecting small sample sizes and few events. Likelihood ratio tests (LRTs)\nindicated significant heterogeneity in race-specific PR-AUCs for ModernBERT and Llama-3.1-8B, with borderline evidence for\nRandom Forest (Supplementary Table 5). These patterns suggest modest but non-negligible racial performance differences,\nwith the greatest uncertainty in smaller racial subgroups.\nWe also observed age-related heterogeneity in discrimination (Figure 3, panel B). PR-AUCs were highest in younger and\nmid-life adults and lower at the oldest ages. For example, ModernBERT achieved PR-AUCs of 8.92% (95% CI, 6.59-12.89%)\nin veterans aged 18-29 years and 8.74% (95% CI, 7.06-11.09%) in those aged 50-59 years, compared with 1.78% (95% CI,\n0.55-6.13%) in veterans aged 80-100 years; Llama-3.1-8B and Random Forest showed similar gradients, and LRTs rejected the\nnull hypothesis of homogeneous performance across age groups for all three models (Supplementary Table 5).\nIn contrast, we observed only small differences in performance by ethnicity (Figure 3, panel C). For ModernBERT, PR-\nAUCs were 8.34% (95% CI, 3.96-20.12%) in non-Hispanic veterans and 8.00% (95% CI, 4.03-18.94%) in Hispanic veterans,\nwith comparable estimates for Random Forest and Llama-3.1-8B. Veterans with unknown ethnicity had PR-AUCs around\n7.31% (95% CI, 6.20-9.10%), similar in magnitude to the other groups. Although one LRT for ModernBERT reached nominal\nstatistical significance, the absolute PR-AUC gap across ethnic subgroups was small (on the order of 1-3 percentage points;\nSupplementary Table 5).\nAs a complementary fairness analysis, we examined both the maximum gap in PR-AUC across racial subgroups and the\nminimum (worst-group) PR-AUC (Supplementary Table 6). For the 12-month prediction window and our three primary models\n(Random Forest, ModernBERT, and Llama-3.1-8B), Llama-3.1-8B showed the smallest race gap (5.58 %), followed closely by\nRandom Forest (5.92 %), whereas ModernBERT had the largest gap (9.24 %). Random Forest achieved the highest worst-group\nPR-AUC (5.06%), compared with 4.18% for ModernBERT and 2.47% for Llama-3.1-8B. These patterns highlight a trade-off\nbetween similarity of performance across race groups (smaller gap) and protection of the worst-performing group (higher\nworst-group PR-AUC), suggesting that Random Forest offers the strongest worst-group performance, whereas Llama-3.1-8B\ntends to minimize PR-AUC gaps between race groups. Across age and ethnicity, LLM and MLM models often exhibited gaps\nof similar or smaller magnitude than Random Forest and comparable worst-group performance, although no single model was\nuniformly most equitable across all subgroups and prediction windows.\n3 Discussion\nUsing a national VA cohort, this study employed several advanced analytic methods to predict first-episode homelessness\nand revealed five major findings. First, across prediction windows, near-term prediction of first-episode homelessness was\nconsistently more difficult than longer-horizon prediction: all models performed worst at 3 months (prevalence 0.32%), and\nPR-AUCs increased as the prediction window and outcome prevalence rose (to 0.63%, 0.92%, and 1.19% at 6, 9, and 12\nmonths, respectively). The best-performing models, time-varying ModernBERT at 3 and 9 months and time-varying XGBoost\nat 6 and 12 months, achieved PR-AUCs of 2-7%, which are well above the corresponding outcome prevalences in this\nextreme class-imbalance setting (for example, a PR-AUC of 2% at 3 months is roughly six times the base rate). Incorporating\ntime-varying features with clinically informed condition persistence rules improved PR-AUCs in all but one of the head-to-head\ncomparisons with static representations, and ablating the condition persistence rules (while retaining time-varying structure)\nreduced performance in the majority of model-window combinations. These ablation findings suggest that clinically informed\ncondition persistence rules are an important representational choice that typically improves performance, particularly at\nlonger prediction horizons. Incorporating domain knowledge about the expected course of mental health conditions, physical\ncomorbidities, and social risk yields more informative trajectories than purely naive time-varying representations and is likely to\nbe important for other EHR-based prediction tasks as well. In other words, how well clinicians document changing symptoms\nand conditions in the data can greatly influence performance of these models.\nRather than treating the framework as a way to deal with model under-specification, we viewed it as an explicit, clinically\ninterpretable feature-engineering step that encodes persistence and recurrence patterns not captured by raw visit-level indicators\nalone. Together, these findings underscore the value of representing how conditions and social risks remain active over time\nrather than as static ever/never indicators. Discrimination was broadly similar between the best time-varying tabular models\nand transformer encoders, with ModernBERT outperforming XGBoost at 3- and 9-month horizons and XGBoost performing\nbest at 6 and 12 months, whereas large generative LLMs did not improve on either approach. Notably, gains were more\nconsistently reflected in PR-AUC than ROC-AUC, as expected under extreme class imbalance (in our VA cohort, prevalence was\n0.32–1.19% across 3–12 months). ROC-AUC summarizes global ranking across the full score distribution, whereas PR-AUC\nis more sensitive to improvements in precision among the highest-risk patients. This divergence supports that homelessness\nonset can be precipitated by short-horizon destabilizing events and rapid changes in utilization and clinical status (e.g., acute\nemergency/urgent care use, substance-related encounters, psychiatric crises), where time-varying features capture more directly\n7/43\n"}, {"page": 8, "text": "than static indicators, thereby improving the detection of homelessness even when ROC-AUC changes modestly.\nSecond, expanding the predictor set from demographics alone to clinical codes and then to social and behavioral factors\n(SBFH) produced a consistent stepwise gain in discrimination across model classes and prediction horizons, with the largest\nabsolute improvements in PR-AUC when SBFH were added at longer prediction windows. To our knowledge, no prior work\nin homelessness risk prediction, particularly among US veterans, has explicitly quantified the incremental value of SBFH\nbeyond demographics and clinical comorbidity, even though prior studies have identified social factors such as low income,\nunemployment, legal problems, and prior housing instability as key correlates of homelessness risk13–15. Our findings extend\nthis literature by showing that routinely collected SBFH can yield consistent gains in PR-AUC, sensitivity, and PPV when\nincorporated into EHR-based risk models, supporting ongoing efforts to systematically capture social risk data in VA and other\nhealthcare systems. At the same time, in some cases, effect sizes were small and confidence intervals overlapped in some\nsettings, likely reflecting extreme outcome rarity and incomplete capture of social risks in structured EHR fields, underscoring\nthe need for richer SBFH ascertainment (for example, via NLP and LLM-based extraction from clinical text) to fully realize the\npotential of social data for homelessness prevention16. As various screeners, like the Assessing Circumstance and Offering\nResources for Needs (ACORN) used in the VA17 or the various SBFH screeners that many states are mandating or incentivizing\nmanaged care organizations to use18 are implemented, further research is needed on determining which are most effective in\ncapturing the important factors.\nThird, using Kernel SHAP, an additive feature-attribution method based on Shapley values, we ranked predictors across\nmodels and prediction windows. The most influential positive predictors were demographic and clinical markers of vulnerability,\nolder age, lower income, and 0% service-connected disability; serious mental illnesses, including PTSD; sleep disorders;\ncardiometabolic and pulmonary comorbidities; and substance use disorders, together with SBFH features such as housing or\nviolence-related problems, neighborhood deprivation (ADI), and non-specific psychosocial needs. These patterns closely align\nwith prior evidence that substance use disorders, mental illness, and income-related disadvantages are among the strongest\nand most consistent risk factors for homelessness among US veterans15,19,20. In contrast, marital status, more regular primary\ncare or mental health contact, and the absence of recent emergency/urgent-care or substance-use visits frequently appeared\nas negative predictors, consistent with studies showing that veterans experiencing homelessness disproportionately rely on\nemergency departments and that engagement in ongoing outpatient care may reduce acute utilization21–23. Some SBFH features\nwith known adverse associations, such as employment/financial and legal problems, were negative predictors in certain windows,\nlikely reflecting multivariable interactions, selection into intensive VA or legal-aid services, and documentation patterns rather\nthan true protective effects. Taken together, these findings underscore that SHAP values highlight variables that are most useful\nfor discrimination within our multivariable models, not necessarily causal risk or protective factors, and should be interpreted in\nconjunction with substantive knowledge of veteran homelessness and social factors of health.\nFourth, we evaluated model fairness by comparing discrimination across racial, age, and ethnic subgroups using PR-AUC,\nlikelihood ratio tests for subgroup heterogeneity (with false discovery rate-adjusted P values), and worst-case (“minimum\nsubgroup”) performance. Consistent with prior work on subgroup performance in clinical risk scores, we observed only modest\nrace-related differences between the largest groups, Black and White veterans, with overlapping confidence intervals and similar\nPR-AUCs across models, whereas estimates for American Indian, Asian, and Native Hawaiian veterans were more variable\nand often unreliable because of very small sample sizes. Age-related heterogeneity was more pronounced: all three models\nperformed best in mid-life adults and worse at the extremes of age, a pattern aligned with reports that one-size-fits-all models\noften underperform in underrepresented or clinically distinct subpopulations24. In contrast, we found no statistically significant\ndifferences in PR-AUC by ethnicity, aside from lower and more uncertain estimates among veterans with unknown ethnicity,\nwhich may reflect differential documentation rather than true performance differences. Subgroup analyses also revealed that\nfairness is multidimensional: some models narrowed gaps in PR-AUC between race groups but offered lower performance for\nthe worst-performing group, whereas others improved worst-group performance at the cost of larger gaps. Across race, age,\nand ethnicity, no single model was uniformly most equitable across all metrics and horizons, implying that model selection\nwill necessarily depend on how health systems prioritize different fairness objectives, for example, minimizing disparities\nbetween groups versus maximizing protection for the most disadvantaged subgroup, rather than on overall model performance.\nImportantly, studies that have compared demographic groups on their rates of homelessness and utilization of VA homeless\nservices suggest no clear disparities by race or sex12. But prediction models should be combined with clinician judgment and\nreceive periodic evaluations to avoid any disparate impacts.\nFifth, risk of future homelessness was highly concentrated within small high-risk tiers, such that a substantial share of\nevents occurred among veterans in the top 1-5% of predicted risk, implying that targeted outreach to a relatively small segment\nof the population could meaningfully increase the efficiency of prevention efforts. This has also been reported in previous\nstudies6,8 and underscore the value of having a strong prediction model that can be used to guide efficient use of outreach\nresources. Although understanding risk factors and identifying current homelessness has been an active area of research, our\nwork extends prior studies by predicting future first-episode homelessness, explicitly modeling temporality, and quantifying the\n8/43\n"}, {"page": 9, "text": "added value of SBFH across diverse modeling approaches in a national VA cohort. Because homelessness is a rare outcome,\neven well-calibrated models inevitably yield modest PPVs, and translating risk scores into practice raises challenges around\nlogistics, generalizability, cost-effectiveness, and the risk-benefit balance of preventive interventions. From a programmatic\nperspective, these risk tiers could therefore be used to prioritize more intensive assessment and services, while keeping the\noverall screening burden manageable.\nAt the same time, low PPV and high false-positive rates carry important ethical, psychological, and resource implications.\nLabeling someone as “high risk for homelessness” may contribute to distress, mistrust, or stigma, and misdirect scarce\nhousing and social-service resources if not paired with careful, person-centered assessment. These concerns underscore that\nhomelessness risk scores should function as decision-support tools rather than standalone gatekeepers, embedded within\ntransparent workflows that emphasize voluntary engagement, shared decision making, and clear benefit to veterans. Future work\nshould include decision-analytic evaluation such as decision curve analysis and net-benefit frameworks to quantify whether\nmodels with modest PPV yield a favorable trade-off between correctly identified high-risk cases and unnecessary outreach\nunder explicit assumptions about intervention burden, cost, and benefit. Prior evaluations of VA and other health-system risk\nmodels (for example, for suicide risk and high-cost utilization) suggest that, under reasonable assumptions about intervention\nburden and benefit, prediction tools can deliver positive net benefit even when PPV remains in the single or low double\ndigits25–27. Similar impact evaluations co-designed with veterans, clinicians, and housing partners will be essential to ensure\nthat homelessness risk prediction is implemented in ways that are equitable, clinically useful, and aligned with prevention goals.\nOur study has several limitations. First, the VA population’s demographic composition and patterns of care differ from\nthose of the overall US population, and VHA data do not fully capture community-based hospitalizations, which may limit\ngeneralizability to non-VA settings. Second, temporal generalizability is uncertain: our models were developed using pre-\npandemic data, and secular changes such as the COVID-19 pandemic and recent transitions in VHA EHR systems may alter\nboth homelessness risk and its documentation. Future work should evaluate similar approaches in post-pandemic cohorts to\nassess robustness over time. Third, we restricted our analysis to a one-year observation window before prediction; longer\nobservation periods or alternative look-back windows could further improve performance and warrant exploration. Fourth,\nhomelessness outcomes and SBFH were ascertained from structured EHR fields, which likely undercapture housing instability\nand social risks and may lead to outcome and predictor misclassification. Finally, we evaluated only a limited set of temporal\ngranularities and condition persistence rules; although these rules improved discrimination in most settings, they remain\napproximations and may over-smooth some conditions. Refining and externally validating persistence choices is an important\narea for future work.\nTo our knowledge, this is one of the first large-scale national studies of predictive modeling for first-episode homelessness\namong US veterans using EHR data. This study identified five key findings regarding the prediction of first-episode homelessness.\nFirst, homelessness was rare and especially difficult to predict over short horizons, although the best time-varying models still\nclearly outperformed the base rate. Second, clinically informed longitudinal representations generally improved or matched\nperformance relative to static or naive time-varying features. Third, expanding predictors from demographics to clinical\ndiagnoses and routinely collected social and behavioral factors yielded consistent gains and, together with SHAP analyses,\nhighlighted clinically and socially plausible patterns of vulnerability and engagement in care. Fourth, the risk of future\nhomelessness was highly concentrated in small high-risk tiers, suggesting that targeted outreach to a small share of veterans\ncould capture a substantial fraction of future events. Fifth, discrimination was broadly similar across major demographic\nsubgroups, but the heterogeneity we observed by race and age underscores that risk scores should be deployed as equity-aware\ntools paired with explicit fairness objectives and ongoing monitoring rather than as one-size-fits-all solutions. Together, these\nfindings suggest that EHR-based risk models that incorporate longitudinal trajectories and social risk information may help\nhealth systems support more targeted, prevention-oriented responses to homelessness among veterans.\n4 Methods\n4.1 Data Source and Study Design\nWe conducted a retrospective prognostic modeling study using inpatient and outpatient records from the U.S. Department of\nVeterans Affairs (VA) Corporate Data Warehouse (CDW). The CDW contains comprehensive clinical and administrative data\nfrom the Veterans Health Administration (VHA), the largest integrated healthcare system in the U.S., encompassing over 1,200\nmedical centers and clinics nationwide. Available information includes demographics, diagnoses, procedures, medications,\nclinical notes, and health services utilization, making it a robust resource for large-scale health research.\nThe study protocol was approved by the VA Bedford Healthcare System Institutional Review Board (IRB #1652850-23),\nwith a waiver of informed consent due to minimal risk. We adhered to the Transparent Reporting of a Multivariable Prediction\nModel for Individual Prognosis or Diagnosis (TRIPOD) guidelines, and the study was conducted in accordance with the\nprinciples of the Declaration of Helsinki.\n9/43\n"}, {"page": 10, "text": "The unit of analysis was the individual VA patient, with each patient contributing a single observation based on data\naggregated over a 1-year observation window. We defined the observation window as January 1-December 31, 2016, and the\nindex date as January 1, 2017. We predicted the first episode of homelessness over four cumulative risk windows: 3 months\n(Q1 2017), 6 months (Q1-Q2 2017), 9 months (Q1-Q3 2017), and 12 months (Q1-Q4 2017). We selected the 2016-2017 period\nto ensure sufficient follow-up time while utilizing contemporary data that predates COVID-19-related disruptions and follows\nthe transition to ICD-10 coding.\nHomelessness was defined using a composite measure used by the VA Homeless Programs Office and defined in previous\nwork28. Briefly, patients were classified as experiencing homelessness if they met any of the following criteria in CDW during\nthe prediction window: (1) International Classification of Diseases, 10th Revision (ICD-10) diagnostic code Z59.0× in inpatient\nor outpatient records; (2) a positive response on part 2 of the annual Homeless Screening Clinical Reminder (HSCR); (3) a\nhomelessness-related record in the Homeless Operations, Management, and Evaluation System (HOMES) (e.g., participation\nin programs such as Grant and Per Diem [GPD], Health Care for Homeless Veterans [HCHV]/Health Care for Re-Entry\nVeterans [HCMI], Domiciliary Care for Homeless Veterans [DCHV], or Compensated Work Therapy/Transitional Residence\n[CWT/TR]); or (4) relevant outpatient stop codes (504, 508, 511, 528, 529, 555, 556, 590) or inpatient specialty codes (28,\n29, 37, 39) indicating receipt of homelessness-specific services. VA stop codes designate the type of care delivered during\noutpatient visits and the associated workload.\nEligible participants were U.S. veterans nationwide who met the following criteria: (1) aged 18-100 years; (2) had at\nleast one VA health care encounter during the observation window. Patients were excluded if they had (1) any homelessness\nindicators in the five years before or during the observation window, or (2) conflicting demographic information. The cohort\nflow diagram is provided in Supplementary Figure 1.\n4.2 Predictor Construction\nPredictor Type\nPredictors\nDemographics\nAge, Body Mass Index (BMI), Combat Exposure, Education, Ethnicity, Gender, Income, Marital\nStatus, Military Sexual Trauma (MST), Race, Rural/Urban, State, Service-connected disability,\nVeterans Integrated Service Network (VISN)\nService Utilization\nOutpatient Visits: Primary care, Mental health, Substance abuse, Speciality care, Rehabilitation,\nDiagnostic / Ancillary,\nInpatient Visits: Total Visits, Visit Days,\nEmergency / Urgent-care Visits,\nVeterans Integrated Service Network (VISN) count\nMental Health Disorders\nAnxiety Disorder, Bipolar Disorder, Dementia, Depression, Other Neurological Disorders,\nPosttraumatic Stress Disorder, Psychoses, Sleep Disorder\nPhysical Health Disorders\nBlood Loss Anemia, Cardiac Arrhythmia, Cardiovascular Disease, Chronic Pulmonary Disease,\nCirrhosis, Coagulopathy, Congestive Heart Failure, Deficiency Anemia, Diabetes, Fluid and\nElectrolyte Disorders, Hepatitis, HIV, Hypertension, Hypothyroidism, Influenza, Liver Disease,\nLymphoma, Metastatic Cancer, Obesity, Pain, Paralysis, Peptic Ulcer Disease, Peripheral Vascular\nDisorders, Pulmonary Circulation Disorders, Renal Failure, Rheumatoid Arthritis/collagen, Solid\nTumor without Metastasis, Traumatic brain injury, Valvular Disease, Weight Loss\nSubstance Abuse Disorders\nAlcohol use disorder, Cannabis, Cocaine, Drug Abuse, Hallucinogen, Nicotine dependence, Opioid\nuse disorder, Other stimulant\nSocial and Behavioral Factors\nof Health\nArea Deprivation Index (ADI), Employment or financial problems, Food insecurity, Housing\nproblems, Insurance, Legal problems, Non-specific psychosocial needs, Social or familial problems,\nViolence problems\nTable 3. List of predictors considered in our study.\nPredictors were organized into six domains: demographics, service utilization, mental health disorders, physical health\ndisorders, substance use disorders, and social and behavioral factors of health (SBFH). Our selection of variables was informed\nby epidemiological studies that have identified risk factors of homelessness among both Veterans and non-veterans15,29. All\nfeatures were extracted during the observation window (January 1-December 31, 2016), except military sexual trauma (MST)\nand combat exposure, which were derived from any historical record before the index date.\n10/43\n"}, {"page": 11, "text": "Patient-level demographic variables included age, body mass index (BMI), sex, gender, race, ethnicity, marital status,\neducation, income, state of residence, and rural/urban status. Three indicators specific to military service were also included:\ncombat exposure, MST, and service-connected disability status. Service use variables captured categorized counts of inpatient\nadmissions, outpatient visits, emergency/urgent care visits, and unique Veterans Integrated Service Network (VISN) encounters:\ncounts of unique VISNs visited in the observation window, which we included in all models to partially account for differ-\nences in healthcare utilization intensity that could otherwise confound associations between persistence-filled diagnoses and\nhomelessness risk. VISNs are regional administrative networks that coordinate care across VA facilities. Major diagnoses were\nidentified from ICD-10 codes, which were aggregated into clinically meaningful categories capturing mental health, physical\nhealth, and substance abuse disorders.\nStructured ICD-10 and VA stop codes captured seven SBFH factors: employment or financial problems, food insecurity,\nhousing instability, legal problems, non-specific psychosocial needs, social/familial problems, and exposure to violence.\nNeighborhood-level socioeconomic status was represented using the national Area Deprivation Index (ADI), linked via patient\nZIP code. We assigned the maximum ADI observed during the observation window, based on the rationale that exposure to\nthe most socioeconomically deprived environment may have the largest and most enduring impact on risk for homelessness.\nInsurance information was also added from the EHR database.\nIn total, we used 79 predictors (Table 3): 14 demographic variables, 10 service utilization variables, 8 indicators of mental\nhealth disorders, 30 indicators of physical health disorders, 8 indicators of substance use disorders, and 9 SBFH. Collectively,\nmental health, physical health, and substance use disorder indicators constituted the diagnostic feature set. All diagnostic and\nSBFH variables were binary, while demographic and service utilization features were categorical (except combat exposure and\nMST, which were binary). Missing values on demographic variables were coded as “unknown”. For diagnostic and SBFH\nvariables, absence of a code during a period was treated as no recorded evidence of that condition. Full variable-level definitions\nare provided in Supplementary Note 1.\nDiagnostic and SBFH features were modeled using two complementary representations to examine the impact of temporal\ndynamics on prediction performance. The first, static, representation used binary indicators for conditions observed at any point\nduring the 2016 observation window. The second, time-varying, representation aggregated each feature across multiple temporal\nperiods and applied clinically informed condition persistence rules that specify how long diagnoses and social/behavioral\nproblems remain active after they are last recorded (only within the 2016 observation window). We examined two temporal\ngranularities, quarterly (Q1-Q4 2016) and half-year (H1-H2 2016), which was treated as a hyperparameter and selected using\nvalidation PR-AUC within each model and prediction horizon; selected values are reported in Supplementary Table 8. We\nselected half-year and quarterly aggregation because these intervals capture clinically meaningful changes in conditions and\nservice use over time while avoiding the extreme sparsity and noise that would arise with finer-grained (e.g., monthly/weekly)\nwindows.\nTime-varying features were assigned one of four condition persistence rules based on their clinical course and likelihood of\nrecurrence: (1) chronic persistent: once observed, the feature remains active for all subsequent intervals, reflecting long-term or\nlifelong conditions; (2) recurrent time-limited: the feature remains active for T intervals after observation, capturing conditions\nthat may remit or recur with treatment; (3) ever-history: the feature is marked active in all periods once recorded, representing\ndiagnoses with enduring prognostic relevance regardless of current status; and (4) episodic: the feature is active only in the\nperiod in which it was recorded, suitable for acute or self-limited conditions. These persistence rules were specified a priori\nin consultation with VA clinicians and were intended as pragmatic approximations of underlying clinical courses rather than\ndata-driven optimizations. For recurrent time-limited conditions, we used a default timeout of two quarters (≈6 months)\nunless clinicians recommended a shorter horizon. The assignment of persistence modes and timeouts (T) for each predictor is\nprovided in Supplementary Table 9 (Fill strategy). To guard against over-correction, we additionally evaluated models using\ntime-varying features without persistence and compared performance gains attributable to the framework in ablation analyses\n(Supplementary Table 2).\nPersistence timeouts (T) were defined in quarterly periods and converted proportionally for half-year aggregation (e.g., a\ntwo-quarter timeout equals one half-year period), with timeouts rounded to whole half-year intervals. This condition persistence\nframework was developed through iterative consultation with VA clinicians specializing in homelessness prevention, primary\ncare, and psychiatry. Because the observation window covered only one year, T values necessarily spanned relatively short\nhorizons and were designed to approximate, rather than precisely model, long-term temporal patterns, providing a structured way\nto evaluate whether incorporating clinically informed condition persistence improves predictive performance. An illustrative\nexample of the data representation and model inputs is shown in Supplementary Note 3.\n4.3 Statistical Analyses\nWe evaluated predictive performance using three classes of models: classical machine learning (ML), masked language\nmodels (MLMs), and large language models (LLMs). Classical ML models included Elastic Net logistic regression, Random\n11/43\n"}, {"page": 12, "text": "Forest, and XGBoost, with hyperparameters selected via grid search on validation data. For MLMs, ModernBERT and\nBioClinical-ModernBERT were fine-tuned for binary sequence classification using a standard classification head. LLMs\nincluded LLaMA-3.1-8B and OpenBioLLM-8B, both fine-tuned using Low-Rank Adaptation (LoRA)30 with a modified\nlanguage-modeling head. We replaced the standard vocabulary prediction layer with a 2-class classification head that outputs\n“Yes” (homeless) or “No” (not homeless). LoRA fine-tunes only a small subset (<1%) of parameters while keeping the base\nmodel otherwise frozen, reducing computational requirements. During training, only the final predicted token contributed\nto the loss (answer-only training), focusing optimization on the classification task. Proprietary models such as GPT-4 were\nexcluded because VHA privacy restrictions prohibit data transfer outside the VINCI environment. Full model architectures and\nhyperparameters are provided in Supplementary Note 2.\nTabular models used categorical demographic and utilization variables and binary diagnosis and social and behavioral\nfactors of health (SBFH) features, represented either statically or aggregated across quarterly/half-year intervals using the\ntime-varying representation described above. MLMs and LLMs received an equivalent text rendering that grouped features\nby clinical domain and time (Q1-Q4 or H1-H2) and produced binary Yes/No outputs, aligning information content across\nmodeling approaches.\nPredictive performance was assessed on the held-out test set using the area under the receiver operating characteristic curve\n(ROC-AUC), area under the precision-recall curve (PR-AUC), sensitivity, specificity, positive predictive value (PPV), and\nobserved-to-expected (O/E) ratios. The O/E ratio represents the observed incidence in a risk group divided by the expected\nincidence if the same number of patients were selected at random from the population. Because homelessness is a rare event,\nwe calculated each metric for multiple risk-group sizes. For a given model, a risk-group size P corresponds to the top P fraction\nof patients by predicted risk. Following prior work and our cohort characteristics, we considered risk groups corresponding to\nthe top 0.5%, 1%, 5%, 10%, 25%, 50%, and 75% of patients by predicted risk.\nModels were evaluated under varying predictor and temporal representations to assess their contributions to performance.\nWe compared three predictor configurations: (1) demographics only; (2) demographics plus clinical and utilization codes\n(diagnosis indicators and service-utilization variables derived from ICD-10 diagnostic codes and VA outpatient stop codes); and\n(3) demographics plus clinical/utilization codes and SBFH, to quantify the incremental value of each predictor class.\nTo evaluate generalizability and fairness, we conducted subgroup analyses stratified by race, age, and ethnicity for the\n9-month prediction window. We pre-specified reliability criteria requiring at least 20 positive and 20 negative cases in the\nsubgroup and a bootstrap CI width ≤0.12; estimates not meeting these criteria were flagged as unreliable in figures and\nsupplementary tables. To formally assess heterogeneity in performance across subgroups, we used likelihood-ratio tests\ncomparing models with and without subgroup-specific performance parameters and controlled for multiple testing using the\nBenjamini-Hochberg false-discovery rate procedure, reporting FDR-adjusted q values. We additionally summarized fairness\ntrade-offs using the maximum gap in PR-AUC across racial subgroups and the minimum (worst-group) PR-AUC.\nTraining data were split at the patient level into 92% training, 3% validation, and 5% test sets to prevent data leakage. To\naddress severe class imbalance while preserving realistic evaluation, we applied stratified downsampling only to the training set\n(matched on gender, age group, and race) to balance outcome classes; validation and test sets retained the original prevalence\ndistribution. Given the large cohort size (n = 4,276,403) and event prevalence across prediction windows (0.32-1.19%), the\ntraining set contained tens of thousands of positive cases, and the validation and test sets contained several hundred to a\nfew thousand positives each, sufficient for hyperparameter tuning and independent evaluation. This allocation maximized\ntraining data for model optimization while maintaining adequately powered subsets for unbiased performance assessment\n(Supplementary Table 7). Random seeds were fixed to ensure reproducibility, and all models were trained, validated, and tested\non identical splits to ensure comparability.\nStratified bootstrapping with 2,000 iterations was used to estimate 95% confidence intervals (CIs) for all metrics. In each\niteration, the test set was resampled with replacement while preserving the original prevalence of homelessness. The 95% CI\nwas defined as the 2.5th and 97.5th percentiles of the bootstrapped metric distribution. For each model, the hyperparameter\nconfiguration yielding the highest PR-AUC on validation data was selected, prioritizing performance on this rare-event outcome.\n5 Data Availability\nThe VHA EHR data used in this study are available under restricted access because of veterans’ privacy and data security\nregulations. Access may be obtained with appropriate approvals through the VA Informatics and Computing Infrastructure\n(VINCI; contact: VINCI@va.gov). Individuals who wish to use these data for research purposes must meet the research\ncredentialing requirements outlined by the VA Office of Research and Development.\n12/43\n"}, {"page": 13, "text": "6 Code Availability\nCode for the prediction models is available from the corresponding author upon reasonable request. Relevant software tools and\nlibraries include: PEFT (for Low-Rank Adaptation fine-tuning)31 , LLaMA-3.1-8B32, OpenBioLLM-8B33, ModernBERT34,\nBioClinical-ModernBERT35, scikit-learn36, XGBoost37, PyTorch38, and HuggingFace Transformers39. All natural language\nprompts used for MLM and LLM model training are provided in the Supplementary Material (Supplementary Note 2). Analyses\nwere conducted using Python 3.10.\n7 Acknowledgements\nResearch reported in this study was supported by the National Center on Homelessness Among Veterans (NCHAV) and by\nthe National Institutes of Health (NIH) under award number 1R01NR020868. This study was also in part supported by NIH\nunder award numbers R01DA056470-A1 and 1R01AG080670-01, and by the U.S. Department of Veterans Affairs (VA) Health\nSystems Research. We thank Joel Reisman for guidance on data access and interpretation and for helpful discussions. The\nfunder played no role in study design, data collection, analysis and interpretation of data, or the writing of this manuscript.\n8 Author contributions\nRohan Pandey, Hong Yu, and Jack Tsai conceived and designed the study. Rohan Pandey and Haijuan Yan performed data\ncollection. Rohan Pandey implemented the code, conducted experiments, and performed analyses. Rohan Pandey drafted the\nmanuscript. Hong Yu and Jack Tsai contributed to manuscript revision and provided critical feedback. Hong Yu and Jack Tsai\nsupervised the study. All authors contributed to interpretation of results, approved the final manuscript, and take responsibility\nfor the submission.\n9 Competing interests\nThe authors declare no competing interests.\nReferences\n1. The 2024 Annual Homelessness Assessment Report (AHAR to Congress) Part 1: Point-In-Time Estimates of Homelessness,\nDecember 2024. https://www.huduser.gov/portal/sites/default/files/pdf/2024-AHAR-Part-1.pdf. Accessed: 2025-12-15.\n2. Tsai, J., Pietrzak, R. H. & Szymkowiak, D. The problem of veteran homelessness: An update for the new decade. Am.\njournal preventive medicine 60, 774–780 (2021).\n3. Saleem, J. J., Flanagan, M. E., Wilck, N. R., Demetriades, J. & Doebbeling, B. N. The next-generation electronic\nhealth record: perspectives of key leaders from the us department of veterans affairs. J. Am. Med. Informatics Assoc. 20,\ne175–e177 (2013).\n4. Fink, D. S. et al. Comparing mental and physical health of us veterans by va healthcare use: implications for generalizability\nof research in the va electronic health records. BMC health services research 22, 1500 (2022).\n5. Tsai, J., Hoff, R. A. & Harpaz-Rotem, I. One-year incidence and predictors of homelessness among 300,000 us veterans\nseen in specialty mental health care. Psychol. Serv. 14, 203 (2017).\n6. Elbogen, E. B. et al. Identifying prevention targets for homelessness among recently discharged us veterans across systems.\nHeal. Serv. Insights 18, 11786329251375179 (2025).\n7. Tsai, J. & Szymkowiak, D. Retrospective study of homelessness among transitioning service members within two years\nafter military service. Adm. Policy Mental Heal. Mental Heal. Serv. Res. 1–10 (2025).\n8. Tsai, J. et al. Predicting homelessness among transitioning us army soldiers. Am. journal preventive medicine 66, 999–1007\n(2024).\n9. Chatterjee, P., Macneal, E. & Roberts, E. T. Measurement bias in documentation of social risk among medicare beneficiaries.\nIn JAMA Health Forum, vol. 6, e251923–e251923 (American Medical Association, 2025).\n10. Devanarayan, P., Farber, C., Stancliff, H. & Marco, C. A. Association of icd-10 z code-documented social determinants of\nhealth with emergency department outcomes in ectopic pregnancy. The Am. J. Emerg. Medicine (2025).\n11. Hau, C. et al. Social determinants of health icd-10 code use by a large integrated healthcare system. In Healthcare, vol. 13,\n2710 (MDPI, 2025).\n13/43\n"}, {"page": 14, "text": "12. Montgomery, A. E., Szymkowiak, D. & Tsai, J. Housing instability and homeless program use among veterans: The\nintersection of race, sex, and homelessness. Hous. Policy Debate 30, 396–408 (2020).\n13. O’Toole, T. P., Johnson, E. E., Aiello, R., Kane, V. & Pape, L. Tailoring care to vulnerable populations by incorporating\nsocial determinants of health: the veterans health administration’s “homeless patient aligned care team” program. Prev.\nchronic disease 13, E44 (2016).\n14. Montgomery, A. E., Tsai, J. & Blosnich, J. R. Demographic correlates of veterans’ adverse social determinants of health.\nAm. journal preventive medicine 59, 828–836 (2020).\n15. Tsai, J. & Rosenheck, R. A. Risk factors for homelessness among us veterans. Epidemiol. reviews 37, 177–195 (2015).\n16. Yang, Z., Mitra, A., Hu, W., Berlowitz, D. & Yu, H. Predicting suicide death among veterans after psychiatric hospitalization\nusing transformer based models with social determinants and nlp. Sci. Reports (2025).\n17. Russell, L. E. et al. Implementing a social needs screening and referral program among veterans: Assessing circumstances\n& offering resources for needs (acorn). J. general internal medicine 38, 2906–2913 (2023).\n18. Davidson, K. W. & McGinn, T. Screening for social determinants of health: the known and unknown. Jama 322, 1037–1038\n(2019).\n19. Washington, D. L. et al. Risk factors for homelessness among women veterans. J. health care for poor underserved 21,\n82–91 (2010).\n20. Tsai, J. Homelessness among US veterans: Critical perspectives (Oxford University Press, 2019).\n21. O’Toole, T. P. et al. Population-tailored care for homeless veterans and acute care use, cost, and satisfaction: a prospective\nquasi-experimental trial. Prev. chronic disease 15, E23 (2018).\n22. Gundlapalli, A. V. et al. Characteristics of the highest users of emergency services in veterans affairs hospitals: homeless\nand non-homeless. Stud. health technology informatics 238, 24 (2017).\n23. Tsai, J. & Rosenheck, R. A. Risk factors for ed use among homeless veterans. The Am. journal emergency medicine 31,\n855–858 (2013).\n24. Chen, R. J. et al. Algorithmic fairness in artificial intelligence for medicine and healthcare. Nat. biomedical engineering 7,\n719–742 (2023).\n25. Harris, A. H., Finlay, A. K. & Meerwijk, E. L. Evaluating the accuracy of the veterans health administration’s reach vet\nsuicide prediction model for legal involved veterans. npj Mental Heal. Res. 4, 53 (2025).\n26. Kessler, R. C. et al. Evaluation of a model to target high-risk psychiatric inpatients for an intensive postdischarge suicide\nprevention intervention. JAMA psychiatry 80, 230–240 (2023).\n27. Vickers, A. J., Van Calster, B. & Steyerberg, E. W. Net benefit approaches to the evaluation of prediction models, molecular\nmarkers, and diagnostic tests. bmj 352 (2016).\n28. Tsai, J., Szymkowiak, D. & Jutkowitz, E. Developing an operational definition of housing instability and homelessness in\nveterans health administration’s medical records. PLoS One 17, e0279973 (2022).\n29. Nilsson, S. F., Nordentoft, M. & Hjorthøj, C. Individual-level predictors for becoming homeless and exiting homelessness:\na systematic review and meta-analysis. J. urban health 96, 741–750 (2019).\n30. Hu, E. J. et al. Lora: Low-rank adaptation of large language models. ICLR 1, 3 (2022).\n31. Peft: Parameter-efficient fine-tuning. https://huggingface.co/docs/peft/en/index. Accessed: 2025-12-15.\n32. Meta-llama/Llama-3.1-8B · Hugging Face. https://huggingface.co/meta-llama/Llama-3.1-8B (2024). Accessed: 2025-12-\n15.\n33. Aaditya/Llama3-OpenBioLLM-8B · Hugging Face. https://huggingface.co/aaditya/Llama3-OpenBioLLM-8B. Accessed:\n2025-12-15.\n34. Warner, B. et al. Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context\nfinetuning and inference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 2526–2547 (2025).\n35. Sounack, T. et al. Bioclinical modernbert: A state-of-the-art long-context encoder for biomedical and clinical nlp. arXiv\npreprint arXiv:2506.10896 (2025).\n36. Pedregosa, F. et al. Scikit-learn: Machine learning in python. J. machine Learn. research 12, 2825–2830 (2011).\n14/43\n"}, {"page": 15, "text": "37. Chen, T. Xgboost: A scalable tree boosting system. Cornell Univ. (2016).\n38. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Adv. neural information processing\nsystems 32 (2019).\n39. Transformers documentation. https://huggingface.co/docs/transformers/en/index. Accessed: 2025-12-15.\n15/43\n"}, {"page": 16, "text": "Supplementary Figure 1. Cohort flow diagram\na) Construction of the cohort and b) the study timeline; M = {3,6,9,12} (in months)\n16/43\n"}, {"page": 17, "text": "Supplementary Table 1. Cohort characteristics\nPredictor\nType\nPredictor\nLevel\nTotal\n(n = 4,276,403)\n3 Months Homeless\n(n = 13,728; 0.32%)\n6 Months Homeless\n(n = 26,818; 0.63%)\n9 Months Homeless\n(n = 39,420; 0.92%)\n12 Months Homeless\n(n = 51,002; 1.19%)\nDemographics\nAge\n18-29\n142204 (3.33%)\n1146 (8.35%)\n2225 (8.3%)\n3260 (8.27%)\n4183 (8.2%)\n30-39\n339880 (7.95%)\n2301 (16.76%)\n4468 (16.66%)\n6443 (16.34%)\n8161 (16.0%)\n40-49\n372185 (8.7%)\n1860 (13.55%)\n3574 (13.33%)\n5201 (13.19%)\n6737 (13.21%)\n50-59\n580750 (13.58%)\n3192 (23.25%)\n6113 (22.79%)\n8961 (22.73%)\n11591 (22.73%)\n60-69\n1280756 (29.95%)\n3459 (25.2%)\n6836 (25.49%)\n10207 (25.89%)\n13325 (26.13%)\n70-79\n921838 (21.56%)\n1251 (9.11%)\n2525 (9.42%)\n3778 (9.58%)\n4996 (9.8%)\n80-100\n638790 (14.94%)\n519 (3.78%)\n1077 (4.02%)\n1570 (3.98%)\n2009 (3.94%)\nBody Mass Index\n(BMI)\nUnknown\n1187477 (27.77%)\n3757 (27.37%)\n7296 (27.21%)\n10591 (26.87%)\n13617 (26.7%)\nObese\n1412355 (33.03%)\n4123 (30.03%)\n8163 (30.44%)\n12154 (30.83%)\n15867 (31.11%)\nOverweight\n1101176 (25.75%)\n3319 (24.18%)\n6574 (24.51%)\n9679 (24.55%)\n12516 (24.54%)\nNormal\n547678 (12.81%)\n2388 (17.4%)\n4515 (16.84%)\n6603 (16.75%)\n8493 (16.65%)\nUnderweight\n27717 (0.65%)\n141 (1.03%)\n270 (1.01%)\n393 (1.0%)\n509 (1.0%)\nCombat Exposure\n102679 (2.4%)\n556 (4.05%)\n1067 (3.98%)\n1590 (4.03%)\n2003 (3.93%)\nEducation\nCompleted\nHigh School\n1636116 (38.26%)\n4709 (34.3%)\n9412 (35.1%)\n13914 (35.3%)\n17911 (35.12%)\nUnknown\n1156717 (27.05%)\n5708 (41.58%)\n10996 (41.0%)\n16091 (40.82%)\n20838 (40.86%)\nCompleted\nCollege\n1073517 (25.1%)\n2527 (18.41%)\n4867 (18.15%)\n7199 (18.26%)\n9361 (18.35%)\nCompleted\nGraduate\nSchool\n377028 (8.82%)\n674 (4.91%)\n1333 (4.97%)\n1910 (4.85%)\n2488 (4.88%)\nAttended\nVocational\nSchool\n33025 (0.77%)\n110 (0.8%)\n210 (0.78%)\n306 (0.78%)\n404 (0.79%)\nEthnicity\nNot Hispanic\n3887177 (90.9%)\n12416 (90.44%)\n24162 (90.1%)\n35581 (90.26%)\n45917 (90.03%)\nHispanic\n266998 (6.24%)\n1010 (7.36%)\n2059 (7.68%)\n3023 (7.67%)\n4059 (7.96%)\nUnknown\n122228 (2.86%)\n302 (2.2%)\n597 (2.23%)\n816 (2.07%)\n1026 (2.01%)\nGender\nMale\n3936231 (92.05%)\n12001 (87.42%)\n23444 (87.42%)\n34380 (87.21%)\n44443 (87.14%)\nFemale\n324152 (7.58%)\n1635 (11.91%)\n3215 (11.99%)\n4829 (12.25%)\n6296 (12.34%)\nUnknown\n16020 (0.37%)\n92 (0.67%)\n159 (0.59%)\n211 (0.54%)\n263 (0.52%)\nIncome\n7,500\n249116 (5.83%)\n932 (6.79%)\n1806 (6.73%)\n2670 (6.77%)\n3403 (6.67%)\n17,500\n190481 (4.45%)\n613 (4.47%)\n1233 (4.6%)\n1796 (4.56%)\n2319 (4.55%)\n25,000\n361004 (8.44%)\n1144 (8.33%)\n2260 (8.43%)\n3351 (8.5%)\n4382 (8.59%)\n35,000\n466936 (10.92%)\n1436 (10.46%)\n2734 (10.19%)\n4085 (10.36%)\n5257 (10.31%)\n45,000\n486317 (11.37%)\n1325 (9.65%)\n2692 (10.04%)\n3924 (9.95%)\n5114 (10.03%)\n55,000\n195326 (4.57%)\n701 (5.11%)\n1406 (5.24%)\n2087 (5.29%)\n2644 (5.18%)\n65,000\n408420 (9.55%)\n1100 (8.01%)\n2099 (7.83%)\n3127 (7.93%)\n4032 (7.91%)\n75,000\n369208 (8.63%)\n840 (6.12%)\n1602 (5.97%)\n2341 (5.94%)\n3087 (6.05%)\n85,000\n155571 (3.64%)\n339 (2.47%)\n661 (2.46%)\n969 (2.46%)\n1239 (2.43%)\n95,000\n193460 (4.52%)\n425 (3.1%)\n858 (3.2%)\n1217 (3.09%)\n1564 (3.07%)\n112,500\n202596 (4.74%)\n395 (2.88%)\n814 (3.04%)\n1191 (3.02%)\n1519 (2.98%)\n137,500\n53525 (1.25%)\n92 (0.67%)\n198 (0.74%)\n304 (0.77%)\n393 (0.77%)\n175,000\n161797 (3.78%)\n270 (1.97%)\n532 (1.98%)\n752 (1.91%)\n979 (1.92%)\nUnknown\n782646 (18.3%)\n4116 (29.98%)\n7923 (29.54%)\n11606 (29.44%)\n15070 (29.55%)\nMarital Status\nMarried\n2469391 (57.74%)\n3953 (28.8%)\n7886 (29.41%)\n11508 (29.19%)\n15124 (29.65%)\nDivorced\n904106 (21.14%)\n4954 (36.09%)\n9625 (35.89%)\n14163 (35.93%)\n18186 (35.66%)\nNot Married\n475057 (11.11%)\n3215 (23.42%)\n6156 (22.95%)\n9025 (22.89%)\n11538 (22.62%)\nWidowed\n288709 (6.75%)\n614 (4.47%)\n1197 (4.46%)\n1834 (4.65%)\n2414 (4.73%)\nSeparated\n116933 (2.73%)\n945 (6.88%)\n1860 (6.94%)\n2752 (6.98%)\n3562 (6.98%)\nUnknown\n22207 (0.52%)\n47 (0.34%)\n94 (0.35%)\n138 (0.35%)\n178 (0.35%)\nMilitary Sexual\nTrauma (MST)\n33931 (0.79%)\n284 (2.07%)\n536 (2.0%)\n754 (1.91%)\n994 (1.95%)\nRace\nWhite\n3196183 (74.74%)\n8263 (60.19%)\n16310 (60.82%)\n23972 (60.81%)\n30970 (60.72%)\nBlack\n679256 (15.88%)\n4214 (30.7%)\n7997 (29.82%)\n11827 (30.0%)\n15370 (30.14%)\nUnknown\n289755 (6.78%)\n822 (5.99%)\n1674 (6.24%)\n2423 (6.15%)\n3078 (6.04%)\nAsian\n42507 (0.99%)\n116 (0.84%)\n227 (0.85%)\n312 (0.79%)\n414 (0.81%)\nNative Hawaii\n37427 (0.88%)\n139 (1.01%)\n245 (0.91%)\n354 (0.9%)\n482 (0.95%)\nAmerican\nIndian\n31275 (0.73%)\n174 (1.27%)\n365 (1.36%)\n532 (1.35%)\n688 (1.35%)\nRural/urban\nUrban\n2669610 (62.43%)\n10051 (73.22%)\n19633 (73.21%)\n28906 (73.33%)\n37397 (73.32%)\nRural\n1598419 (37.38%)\n3663 (26.68%)\n7143 (26.64%)\n10448 (26.5%)\n13510 (26.49%)\nUnknown\n8374 (0.2%)\n14 (0.1%)\n42 (0.16%)\n66 (0.17%)\n95 (0.19%)\nTexas\n347078 (8.12%)\n1168 (8.51%)\n2323 (8.66%)\n3457 (8.77%)\n4440 (8.71%)\nFlorida\n344221 (8.05%)\n938 (6.83%)\n1816 (6.77%)\n2745 (6.96%)\n3609 (7.08%)\nCalifornia\n279602 (6.54%)\n1253 (9.13%)\n2491 (9.29%)\n3806 (9.65%)\n4936 (9.68%)\nUnknown\n212773 (4.98%)\n621 (4.52%)\n1241 (4.63%)\n1877 (4.76%)\n2581 (5.06%)\nOhio\n170848 (4.0%)\n503 (3.66%)\n984 (3.67%)\n1505 (3.82%)\n1939 (3.8%)\nNorth\nCarolina\n169954 (3.97%)\n453 (3.3%)\n855 (3.19%)\n1295 (3.29%)\n1655 (3.24%)\nGeorgia\n150705 (3.52%)\n759 (5.53%)\n1390 (5.18%)\n1990 (5.05%)\n2578 (5.05%)\nPennsylvania\n144747 (3.38%)\n414 (3.02%)\n799 (2.98%)\n1189 (3.02%)\n1510 (2.96%)\nNew York\n139224 (3.26%)\n487 (3.55%)\n917 (3.42%)\n1361 (3.45%)\n1734 (3.4%)\nIllinois\n117953 (2.76%)\n413 (3.01%)\n775 (2.89%)\n1128 (2.86%)\n1402 (2.75%)\nTennessee\n111183 (2.6%)\n334 (2.43%)\n601 (2.24%)\n836 (2.12%)\n1098 (2.15%)\nContinued on next page\n17/43\n"}, {"page": 18, "text": "Supplementary Table 1 – continued\nVirginia\n110521 (2.58%)\n276 (2.01%)\n564 (2.1%)\n864 (2.19%)\n1142 (2.24%)\nMichigan\n107271 (2.51%)\n344 (2.51%)\n660 (2.46%)\n946 (2.4%)\n1258 (2.47%)\nSouth\nCarolina\n105475 (2.47%)\n278 (2.03%)\n541 (2.02%)\n808 (2.05%)\n1046 (2.05%)\nArizona\n105156 (2.46%)\n365 (2.66%)\n733 (2.73%)\n1068 (2.71%)\n1389 (2.72%)\nMissouri\n99668 (2.33%)\n220 (1.6%)\n476 (1.77%)\n705 (1.79%)\n956 (1.87%)\nIndiana\n95681 (2.24%)\n262 (1.91%)\n514 (1.92%)\n759 (1.93%)\n977 (1.92%)\nWashington\n87701 (2.05%)\n340 (2.48%)\n654 (2.44%)\n915 (2.32%)\n1191 (2.34%)\nWisconsin\n86173 (2.02%)\n214 (1.56%)\n397 (1.48%)\n541 (1.37%)\n702 (1.38%)\nAlabama\n84011 (1.96%)\n273 (1.99%)\n528 (1.97%)\n724 (1.84%)\n924 (1.81%)\nMinnesota\n78824 (1.84%)\n116 (0.84%)\n235 (0.88%)\n331 (0.84%)\n418 (0.82%)\nKentucky\n77327 (1.81%)\n182 (1.33%)\n345 (1.29%)\n536 (1.36%)\n702 (1.38%)\nOklahoma\n70552 (1.65%)\n221 (1.61%)\n411 (1.53%)\n588 (1.49%)\n759 (1.49%)\nOregon\n66454 (1.55%)\n521 (3.8%)\n1013 (3.78%)\n1273 (3.23%)\n1475 (2.89%)\nArkansas\n66055 (1.54%)\n155 (1.13%)\n325 (1.21%)\n516 (1.31%)\n647 (1.27%)\nColorado\n63300 (1.48%)\n209 (1.52%)\n397 (1.48%)\n598 (1.52%)\n755 (1.48%)\nLouisiana\n61675 (1.44%)\n226 (1.65%)\n468 (1.75%)\n653 (1.66%)\n876 (1.72%)\nNevada\n52287 (1.22%)\n231 (1.68%)\n454 (1.69%)\n672 (1.7%)\n876 (1.72%)\nMaryland\n50707 (1.19%)\n168 (1.22%)\n347 (1.29%)\n506 (1.28%)\n675 (1.32%)\nMississippi\n50562 (1.18%)\n148 (1.08%)\n256 (0.95%)\n382 (0.97%)\n484 (0.95%)\nMassachusetts\n49617 (1.16%)\n189 (1.38%)\n428 (1.6%)\n587 (1.49%)\n768 (1.51%)\nNew Jersey\n47907 (1.12%)\n157 (1.14%)\n313 (1.17%)\n455 (1.15%)\n572 (1.12%)\nIowa\n46185 (1.08%)\n120 (0.87%)\n259 (0.97%)\n389 (0.99%)\n471 (0.92%)\nWest Virginia\n42326 (0.99%)\n109 (0.79%)\n218 (0.81%)\n301 (0.76%)\n376 (0.74%)\nKansas\n38515 (0.9%)\n104 (0.76%)\n195 (0.73%)\n282 (0.72%)\n382 (0.75%)\nNew Mexico\n36755 (0.86%)\n85 (0.62%)\n186 (0.69%)\n268 (0.68%)\n367 (0.72%)\nIdaho\n34911 (0.82%)\n82 (0.6%)\n158 (0.59%)\n256 (0.65%)\n345 (0.68%)\nNebraska\n32154 (0.75%)\n55 (0.4%)\n136 (0.51%)\n218 (0.55%)\n282 (0.55%)\nConnecticut\n31737 (0.74%)\n86 (0.63%)\n178 (0.66%)\n284 (0.72%)\n380 (0.75%)\nMontana\n26790 (0.63%)\n79 (0.58%)\n142 (0.53%)\n208 (0.53%)\n272 (0.53%)\nUtah\n26016 (0.61%)\n77 (0.56%)\n149 (0.56%)\n230 (0.58%)\n291 (0.57%)\nMaine\n22395 (0.52%)\n59 (0.43%)\n115 (0.43%)\n162 (0.41%)\n205 (0.4%)\nSouth Dakota\n21384 (0.5%)\n42 (0.31%)\n91 (0.34%)\n154 (0.39%)\n208 (0.41%)\nNew\nHampshire\n19668 (0.46%)\n79 (0.58%)\n142 (0.53%)\n201 (0.51%)\n246 (0.48%)\nHawaii\n17112 (0.4%)\n56 (0.41%)\n113 (0.42%)\n151 (0.38%)\n194 (0.38%)\nWyoming\n13787 (0.32%)\n43 (0.31%)\n76 (0.28%)\n98 (0.25%)\n115 (0.23%)\nNorth Dakota\n13753 (0.32%)\n37 (0.27%)\n74 (0.28%)\n98 (0.25%)\n136 (0.27%)\nRhode Island\n12636 (0.3%)\n29 (0.21%)\n61 (0.23%)\n93 (0.24%)\n135 (0.26%)\nDelaware\n11172 (0.26%)\n39 (0.28%)\n82 (0.31%)\n119 (0.3%)\n159 (0.31%)\nAlaska\n10696 (0.25%)\n34 (0.25%)\n67 (0.25%)\n96 (0.24%)\n114 (0.22%)\nVermont\n9554 (0.22%)\n39 (0.28%)\n60 (0.22%)\n93 (0.24%)\n123 (0.24%)\nDistrict Of\nColumbia\n3645 (0.09%)\n36 (0.26%)\n65 (0.24%)\n103 (0.26%)\n127 (0.25%)\nService connected\ndisability\n0%\n1481706 (34.65%)\n4291 (31.26%)\n8289 (30.91%)\n12137 (30.79%)\n15595 (30.58%)\n10%\n267544 (6.26%)\n828 (6.03%)\n1681 (6.27%)\n2462 (6.25%)\n3187 (6.25%)\n20%\n131359 (3.07%)\n328 (2.39%)\n685 (2.55%)\n1023 (2.6%)\n1352 (2.65%)\n30%\n204318 (4.78%)\n679 (4.95%)\n1290 (4.81%)\n1920 (4.87%)\n2517 (4.94%)\n40%\n130930 (3.06%)\n337 (2.45%)\n672 (2.51%)\n1020 (2.59%)\n1299 (2.55%)\n50%\n114961 (2.69%)\n331 (2.41%)\n644 (2.4%)\n945 (2.4%)\n1204 (2.36%)\n60%\n187984 (4.4%)\n495 (3.61%)\n968 (3.61%)\n1413 (3.58%)\n1845 (3.62%)\n70%\n256197 (5.99%)\n1058 (7.71%)\n2099 (7.83%)\n3088 (7.83%)\n3978 (7.8%)\n80%\n282927 (6.62%)\n1036 (7.55%)\n1971 (7.35%)\n2880 (7.31%)\n3749 (7.35%)\n90%\n298501 (6.98%)\n1042 (7.59%)\n2020 (7.53%)\n3013 (7.64%)\n3903 (7.65%)\n100%\n919976 (21.51%)\n3303 (24.06%)\n6499 (24.23%)\n9519 (24.15%)\n12373 (24.26%)\nVISN\n8\n409694 (9.58%)\n1022 (7.44%)\n2038 (7.6%)\n3137 (7.96%)\n4346 (8.52%)\n10\n344367 (8.05%)\n1043 (7.6%)\n2023 (7.54%)\n3004 (7.62%)\n3878 (7.6%)\n22\n319139 (7.46%)\n1385 (10.09%)\n2783 (10.38%)\n4049 (10.27%)\n5252 (10.3%)\n7\n305471 (7.14%)\n1217 (8.87%)\n2278 (8.49%)\n3256 (8.26%)\n4219 (8.27%)\n16\n289020 (6.76%)\n840 (6.12%)\n1679 (6.26%)\n2528 (6.41%)\n3334 (6.54%)\n17\n271647 (6.35%)\n868 (6.32%)\n1711 (6.38%)\n2506 (6.36%)\n3163 (6.2%)\n6\n257847 (6.03%)\n690 (5.03%)\n1342 (5.0%)\n2037 (5.17%)\n2625 (5.15%)\n21\n225793 (5.28%)\n898 (6.54%)\n1761 (6.57%)\n2724 (6.91%)\n3517 (6.9%)\n23\n214793 (5.02%)\n410 (2.99%)\n861 (3.21%)\n1308 (3.32%)\n1693 (3.32%)\n19\n208984 (4.89%)\n608 (4.43%)\n1164 (4.34%)\n1769 (4.49%)\n2245 (4.4%)\n20\n202901 (4.74%)\n1023 (7.45%)\n1978 (7.38%)\n2635 (6.68%)\n3254 (6.38%)\n9\n197768 (4.62%)\n530 (3.86%)\n989 (3.69%)\n1440 (3.65%)\n1900 (3.73%)\n12\n197604 (4.62%)\n635 (4.63%)\n1173 (4.37%)\n1680 (4.26%)\n2126 (4.17%)\n2\n188982 (4.42%)\n659 (4.8%)\n1239 (4.62%)\n1846 (4.68%)\n2374 (4.65%)\n4\n184479 (4.31%)\n566 (4.12%)\n1101 (4.11%)\n1582 (4.01%)\n1969 (3.86%)\n15\n170893 (4.0%)\n406 (2.96%)\n823 (3.07%)\n1225 (3.11%)\n1611 (3.16%)\n1\n158642 (3.71%)\n522 (3.8%)\n1058 (3.95%)\n1524 (3.87%)\n1976 (3.87%)\n5\n128379 (3.0%)\n406 (2.96%)\n817 (3.05%)\n1170 (2.97%)\n1520 (2.98%)\nOutpatient Visits:\nPrimary care\n0\n208605 (4.88%)\n1559 (11.36%)\n2864 (10.68%)\n4023 (10.21%)\n5000 (9.8%)\n1\n902376 (21.1%)\n2149 (15.65%)\n4176 (15.57%)\n6109 (15.5%)\n7907 (15.5%)\n2\n847575 (19.82%)\n2056 (14.98%)\n4075 (15.2%)\n6033 (15.3%)\n7879 (15.45%)\n3\n623847 (14.59%)\n1767 (12.87%)\n3493 (13.02%)\n5152 (13.07%)\n6708 (13.15%)\n4\n439139 (10.27%)\n1311 (9.55%)\n2589 (9.65%)\n3886 (9.86%)\n5130 (10.06%)\n5+\n1254861 (29.34%)\n4886 (35.59%)\n9621 (35.88%)\n14217 (36.07%)\n18378 (36.03%)\nOutpatient Visits:\nMental health\n0\n3015033 (70.5%)\n5752 (41.9%)\n11531 (43.0%)\n17272 (43.82%)\n22665 (44.44%)\n1\n264321 (6.18%)\n1404 (10.23%)\n2737 (10.21%)\n3988 (10.12%)\n5067 (9.93%)\n2\n198294 (4.64%)\n955 (6.96%)\n1857 (6.92%)\n2770 (7.03%)\n3640 (7.14%)\n3\n160804 (3.76%)\n730 (5.32%)\n1483 (5.53%)\n2189 (5.55%)\n2899 (5.68%)\n4\n128044 (2.99%)\n641 (4.67%)\n1303 (4.86%)\n1868 (4.74%)\n2410 (4.73%)\n5+\n509907 (11.92%)\n4246 (30.93%)\n7907 (29.48%)\n11333 (28.75%)\n14321 (28.08%)\nContinued on next page\n18/43\n"}, {"page": 19, "text": "Supplementary Table 1 – continued\nOutpatient Visits:\nSubstance abuse\n0\n4193235 (98.06%)\n11972 (87.21%)\n23709 (88.41%)\n35129 (89.11%)\n45721 (89.65%)\n1\n26372 (0.62%)\n420 (3.06%)\n809 (3.02%)\n1133 (2.87%)\n1386 (2.72%)\n2\n9769 (0.23%)\n191 (1.39%)\n340 (1.27%)\n471 (1.19%)\n591 (1.16%)\n3+\n47027 (1.1%)\n1145 (8.34%)\n1960 (7.31%)\n2687 (6.82%)\n3304 (6.48%)\nOutpatient Visits:\nSpecialty care\n0\n1376548 (32.19%)\n4740 (34.53%)\n9167 (34.18%)\n13293 (33.72%)\n16929 (33.19%)\n1\n645905 (15.1%)\n1967 (14.33%)\n3790 (14.13%)\n5578 (14.15%)\n7306 (14.32%)\n2\n450014 (10.52%)\n1256 (9.15%)\n2568 (9.58%)\n3804 (9.65%)\n4953 (9.71%)\n3\n325704 (7.62%)\n933 (6.8%)\n1840 (6.86%)\n2715 (6.89%)\n3593 (7.04%)\n4\n248658 (5.81%)\n808 (5.89%)\n1542 (5.75%)\n2292 (5.81%)\n2989 (5.86%)\n5+\n1229574 (28.75%)\n4024 (29.31%)\n7911 (29.5%)\n11738 (29.78%)\n15232 (29.87%)\nOutpatient Visits:\nRehabilitation\n0\n2705599 (63.27%)\n8331 (60.69%)\n16445 (61.32%)\n24139 (61.24%)\n31366 (61.5%)\n1\n548830 (12.83%)\n1751 (12.75%)\n3454 (12.88%)\n5198 (13.19%)\n6733 (13.2%)\n2\n326780 (7.64%)\n959 (6.99%)\n1858 (6.93%)\n2696 (6.84%)\n3469 (6.8%)\n3\n194497 (4.55%)\n569 (4.14%)\n1115 (4.16%)\n1648 (4.18%)\n2114 (4.14%)\n4\n120933 (2.83%)\n392 (2.86%)\n738 (2.75%)\n1085 (2.75%)\n1401 (2.75%)\n5+\n379764 (8.88%)\n1726 (12.57%)\n3208 (11.96%)\n4654 (11.81%)\n5919 (11.61%)\nOutpatient Visits:\nDiagnostic /\nAncillary\n0\n287421 (6.72%)\n1134 (8.26%)\n2119 (7.9%)\n3003 (7.62%)\n3738 (7.33%)\n1\n635550 (14.86%)\n1395 (10.16%)\n2720 (10.14%)\n4027 (10.22%)\n5211 (10.22%)\n2\n579045 (13.54%)\n1324 (9.64%)\n2729 (10.18%)\n4015 (10.19%)\n5278 (10.35%)\n3\n457623 (10.7%)\n1186 (8.64%)\n2407 (8.98%)\n3563 (9.04%)\n4683 (9.18%)\n4\n354861 (8.3%)\n1046 (7.62%)\n2063 (7.69%)\n3060 (7.76%)\n4019 (7.88%)\n5+\n1961903 (45.88%)\n7643 (55.67%)\n14780 (55.11%)\n21752 (55.18%)\n28073 (55.04%)\nEmergency /\nUrgent-care\n0\n3353468 (78.42%)\n8046 (58.61%)\n15979 (59.58%)\n23708 (60.14%)\n30991 (60.76%)\n1\n520258 (12.17%)\n2713 (19.76%)\n5208 (19.42%)\n7539 (19.12%)\n9646 (18.91%)\n2\n204184 (4.77%)\n1291 (9.4%)\n2493 (9.3%)\n3650 (9.26%)\n4640 (9.1%)\n3+\n198493 (4.64%)\n1678 (12.22%)\n3138 (11.7%)\n4523 (11.47%)\n5725 (11.23%)\nInpatient Visits:\nTotal\n0\n3950804 (92.39%)\n11407 (83.09%)\n22567 (84.15%)\n33475 (84.92%)\n43602 (85.49%)\n1\n108068 (2.53%)\n786 (5.73%)\n1469 (5.48%)\n2071 (5.25%)\n2590 (5.08%)\n2\n73840 (1.73%)\n597 (4.35%)\n1037 (3.87%)\n1451 (3.68%)\n1796 (3.52%)\n3+\n143691 (3.36%)\n938 (6.83%)\n1745 (6.51%)\n2423 (6.15%)\n3014 (5.91%)\nInpatient Visits: No.\nof Days\n0\n3950834 (92.39%)\n11407 (83.09%)\n22567 (84.15%)\n33475 (84.92%)\n43602 (85.49%)\n1\n35603 (0.83%)\n160 (1.17%)\n335 (1.25%)\n485 (1.23%)\n622 (1.22%)\n2\n40262 (0.94%)\n246 (1.79%)\n445 (1.66%)\n657 (1.67%)\n829 (1.63%)\n3+\n249704 (5.84%)\n1915 (13.95%)\n3471 (12.94%)\n4803 (12.18%)\n5949 (11.66%)\nVISN count\n1\n2034356 (47.57%)\n4516 (32.9%)\n8772 (32.71%)\n12789 (32.44%)\n16500 (32.35%)\n2\n1282822 (30.0%)\n4208 (30.65%)\n8326 (31.05%)\n12241 (31.05%)\n15763 (30.91%)\n3\n587032 (13.73%)\n2597 (18.92%)\n5032 (18.76%)\n7494 (19.01%)\n9763 (19.14%)\n4\n232504 (5.44%)\n1304 (9.5%)\n2536 (9.46%)\n3729 (9.46%)\n4857 (9.52%)\n5+\n139689 (3.27%)\n1103 (8.03%)\n2152 (8.02%)\n3167 (8.03%)\n4119 (8.08%)\nMental\nHealth\nDisorders\nAnxiety Disorder\n369175 (8.63%)\n2320 (16.9%)\n4498 (16.77%)\n6462 (16.39%)\n8313 (16.3%)\nBipolar Disorder\n79218 (1.85%)\n829 (6.04%)\n1619 (6.04%)\n2331 (5.91%)\n2979 (5.84%)\nDementia\n125731 (2.94%)\n262 (1.91%)\n522 (1.95%)\n732 (1.86%)\n917 (1.8%)\nDepression\n727811 (17.02%)\n4628 (33.71%)\n8954 (33.39%)\n12939 (32.82%)\n16591 (32.53%)\nOther Neurological\nDisorders\n168145 (3.93%)\n661 (4.81%)\n1272 (4.74%)\n1815 (4.6%)\n2282 (4.47%)\nPosttraumatic Stress\nDisorder\n612738 (14.33%)\n3348 (24.39%)\n6423 (23.95%)\n9358 (23.74%)\n11940 (23.41%)\nPsychoses\n67160 (1.57%)\n741 (5.4%)\n1408 (5.25%)\n2060 (5.23%)\n2587 (5.07%)\nSleep Disorder\n728343 (17.03%)\n2453 (17.87%)\n4766 (17.77%)\n6976 (17.7%)\n9095 (17.83%)\nPhysical\nHealth\nDisorders\nBlood Loss Anemia\n13449 (0.31%)\n52 (0.38%)\n103 (0.38%)\n150 (0.38%)\n190 (0.37%)\nCardiac Arrhythmia\n441826 (10.33%)\n1000 (7.28%)\n1974 (7.36%)\n2869 (7.28%)\n3692 (7.24%)\nCardiovascular\nDisease\n41516 (0.97%)\n140 (1.02%)\n275 (1.03%)\n385 (0.98%)\n507 (0.99%)\nChronic Pulmonary\nDisease\n484324 (11.33%)\n1547 (11.27%)\n3014 (11.24%)\n4415 (11.2%)\n5642 (11.06%)\nCirrhosis\n36720 (0.86%)\n195 (1.42%)\n406 (1.51%)\n586 (1.49%)\n724 (1.42%)\nCoagulopathy\n54368 (1.27%)\n200 (1.46%)\n361 (1.35%)\n504 (1.28%)\n647 (1.27%)\nCongestive Heart\nFailure\n202163 (4.73%)\n611 (4.45%)\n1223 (4.56%)\n1787 (4.53%)\n2234 (4.38%)\nDeficiency Anemia\n114764 (2.68%)\n355 (2.59%)\n689 (2.57%)\n1007 (2.55%)\n1300 (2.55%)\nDiabetes\n1132896 (26.49%)\n2685 (19.56%)\n5259 (19.61%)\n7886 (20.01%)\n10375 (20.34%)\nFluid and\nElectrolyte\nDisorders\n142822 (3.34%)\n679 (4.95%)\n1313 (4.9%)\n1845 (4.68%)\n2321 (4.55%)\nHepatitis\n86619 (2.03%)\n701 (5.11%)\n1356 (5.06%)\n2006 (5.09%)\n2543 (4.99%)\nHIV\n16553 (0.39%)\n138 (1.01%)\n260 (0.97%)\n404 (1.02%)\n501 (0.98%)\nHypertension\n1971448 (46.1%)\n4966 (36.17%)\n9703 (36.18%)\n14281 (36.23%)\n18605 (36.48%)\nContinued on next page\n19/43\n"}, {"page": 20, "text": "Supplementary Table 1 – continued\nHypothyroidism\n269726 (6.31%)\n572 (4.17%)\n1103 (4.11%)\n1678 (4.26%)\n2156 (4.23%)\nInfluenza\n7221 (0.17%)\n30 (0.22%)\n64 (0.24%)\n110 (0.28%)\n149 (0.29%)\nLiver Disease\n62564 (1.46%)\n289 (2.11%)\n568 (2.12%)\n804 (2.04%)\n1045 (2.05%)\nLymphoma\n25603 (0.6%)\n41 (0.3%)\n77 (0.29%)\n114 (0.29%)\n159 (0.31%)\nMetastatic Cancer\n30586 (0.72%)\n62 (0.45%)\n118 (0.44%)\n168 (0.43%)\n202 (0.4%)\nObesity\n535709 (12.53%)\n1642 (11.96%)\n3270 (12.19%)\n4890 (12.4%)\n6329 (12.41%)\nPain\n2285963 (53.46%)\n8152 (59.38%)\n15986 (59.61%)\n23504 (59.62%)\n30432 (59.67%)\nParalysis\n23737 (0.56%)\n101 (0.74%)\n176 (0.66%)\n261 (0.66%)\n320 (0.63%)\nPeptic Ulcer\nDisease\n10804 (0.25%)\n40 (0.29%)\n81 (0.3%)\n121 (0.31%)\n152 (0.3%)\nPeripheral Vascular\nDisorders\n188758 (4.41%)\n475 (3.46%)\n939 (3.5%)\n1350 (3.42%)\n1762 (3.45%)\nPulmonary\nCirculation\nDisorders\n34361 (0.8%)\n107 (0.78%)\n224 (0.84%)\n324 (0.82%)\n410 (0.8%)\nRenal Failure\n240999 (5.64%)\n619 (4.51%)\n1189 (4.43%)\n1692 (4.29%)\n2140 (4.2%)\nRheumatoid\nArthritis/collagen\n62803 (1.47%)\n166 (1.21%)\n329 (1.23%)\n481 (1.22%)\n614 (1.2%)\nSolid Tumor\nwithout Metastasis\n237679 (5.56%)\n484 (3.53%)\n989 (3.69%)\n1480 (3.75%)\n1915 (3.75%)\nTraumatic brain\ninjury\n53858 (1.26%)\n366 (2.67%)\n729 (2.72%)\n1069 (2.71%)\n1331 (2.61%)\nValvular Disease\n86468 (2.02%)\n182 (1.33%)\n349 (1.3%)\n514 (1.3%)\n654 (1.28%)\nWeight Loss\n61445 (1.44%)\n276 (2.01%)\n520 (1.94%)\n749 (1.9%)\n954 (1.87%)\nSubstance\nAbuse\nDisorders\nAlcohol use\ndisorder\n263595 (6.16%)\n2753 (20.05%)\n5105 (19.04%)\n7290 (18.49%)\n9158 (17.96%)\nCannabis\n58342 (1.36%)\n1006 (7.33%)\n1920 (7.16%)\n2708 (6.87%)\n3423 (6.71%)\nCocaine\n23833 (0.56%)\n702 (5.11%)\n1249 (4.66%)\n1747 (4.43%)\n2184 (4.28%)\nDrug Abuse\n96809 (2.26%)\n1915 (13.95%)\n3517 (13.11%)\n4966 (12.6%)\n6213 (12.18%)\nHallucinogen\n591 (0.01%)\n16 (0.12%)\n31 (0.12%)\n43 (0.11%)\n53 (0.1%)\nNicotine\ndependence\n315904 (7.39%)\n2047 (14.91%)\n3912 (14.59%)\n5659 (14.36%)\n7194 (14.11%)\nOpioid use disorder\n33303 (0.78%)\n654 (4.76%)\n1206 (4.5%)\n1695 (4.3%)\n2066 (4.05%)\nOther stimulant\n8220 (0.19%)\n294 (2.14%)\n541 (2.02%)\n733 (1.86%)\n907 (1.78%)\nSocial and\nBehavioral\nFactors of Health\nADI\n0-9.9\n132341 (3.09%)\n428 (3.12%)\n884 (3.3%)\n1330 (3.37%)\n1686 (3.31%)\n10-19.9\n243070 (5.68%)\n820 (5.97%)\n1606 (5.99%)\n2369 (6.01%)\n3070 (6.02%)\n20-29.9\n342748 (8.01%)\n974 (7.09%)\n1976 (7.37%)\n2918 (7.4%)\n3697 (7.25%)\n30-39.9\n454757 (10.63%)\n1269 (9.24%)\n2454 (9.15%)\n3594 (9.12%)\n4684 (9.18%)\n40-49.9\n501180 (11.72%)\n1361 (9.91%)\n2767 (10.32%)\n4007 (10.16%)\n5137 (10.07%)\n50-59.9\n524171 (12.26%)\n1544 (11.25%)\n2941 (10.97%)\n4265 (10.82%)\n5462 (10.71%)\n60-69.9\n520226 (12.17%)\n1492 (10.87%)\n2947 (10.99%)\n4395 (11.15%)\n5672 (11.12%)\n70-79.9\n507166 (11.86%)\n1614 (11.76%)\n3084 (11.5%)\n4565 (11.58%)\n5873 (11.52%)\n80-89.9\n488948 (11.43%)\n1651 (12.03%)\n3211 (11.97%)\n4743 (12.03%)\n6168 (12.09%)\n90-100\n482767 (11.29%)\n2070 (15.08%)\n4055 (15.12%)\n5994 (15.21%)\n7967 (15.62%)\nUnknown\n79029 (1.85%)\n505 (3.68%)\n893 (3.33%)\n1240 (3.15%)\n1586 (3.11%)\nEmployment or\nfinancial problems\n71035 (1.66%)\n1808 (13.17%)\n3150 (11.75%)\n4281 (10.86%)\n5181 (10.16%)\nFood insecurity\n178 (0.0%)\n12 (0.09%)\n17 (0.06%)\n24 (0.06%)\n29 (0.06%)\nHousing problems\n16388 (0.38%)\n746 (5.43%)\n1380 (5.15%)\n1804 (4.58%)\n2129 (4.17%)\nInsurance\nUnknown\n1965156 (45.95%)\n8529 (62.13%)\n16416 (61.21%)\n24105 (61.15%)\n31068 (60.92%)\nGovernment\n1155865 (27.03%)\n2699 (19.66%)\n5337 (19.9%)\n7890 (20.02%)\n10247 (20.09%)\nPrivate\n600385 (14.04%)\n960 (6.99%)\n1965 (7.33%)\n2845 (7.22%)\n3734 (7.32%)\nSpecialized\n554997 (12.98%)\n1540 (11.22%)\n3100 (11.56%)\n4580 (11.62%)\n5953 (11.67%)\nLegal problems\n24419 (0.57%)\n905 (6.59%)\n1532 (5.71%)\n1990 (5.05%)\n2363 (4.63%)\nNon-specific\npsychosocial needs\n364068 (8.51%)\n2872 (20.92%)\n5286 (19.71%)\n7279 (18.47%)\n9098 (17.84%)\nSocial or familial\nproblems\n54496 (1.27%)\n339 (2.47%)\n631 (2.35%)\n876 (2.22%)\n1186 (2.33%)\nViolence problems\n347251 (8.12%)\n2912 (21.21%)\n5353 (19.96%)\n7373 (18.7%)\n9102 (17.85%)\n20/43\n"}, {"page": 21, "text": "Supplementary Table 2. Ablation of the condition persistence framework\nModel Class\nModel\nInput Representation\nPR-AUC, %\nROC AUC, %\nPrediction Window = 3 Months\nMachine Learning\nElastic Net LR\nTime Varying\n1.87 (1.54, 2.45)\n76.71 (74.79, 78.46)\nTV w/o Fill Strategy\n1.75 (1.45, 2.22)\n76.71 (74.82, 78.51)\nRandom Forest\nTime Varying\n2.11 (1.76, 2.68)\n79.27 (77.54, 80.94)\nTV w/o Fill Strategy\n1.87 (1.54, 2.44)\n78.76 (77.03, 80.39)\nXGBoost\nTime Varying\n2.22 (1.83, 2.91)\n79.73 (77.95, 81.37)\nTV w/o Fill Strategy\n2.22 (1.81, 2.98)\n79.72 (77.99, 81.34)\nMasked Language\nModels\nModernBERT\nTime Varying\n2.39 (1.8, 3.34)\n77.28 (75.39, 79.00)\nTV w/o Fill Strategy\n2.03 (1.64, 2.74)\n79.36 (76.57, 80.11)\nBioClinical ModernBERT\nTime Varying\n2.34 (1.91, 3.09)\n78.78 (76.97, 80.53)\nTV w/o Fill Strategy\n2.61 (2.01, 3.55)\n78.94 (77.03, 80.59)\nLarge Language\nModels\nLlama-3.1-8B\nTime Varying\n2.16 (1.77, 2.76)\n79.12 (77.43, 80.79)\nTV w/o Fill Strategy\n1.93 (1.59, 2.51)\n78.49 (76.74, 80.17)\nOpenBioLLM-8B\nTime Varying\n2.32 (1.81, 3.10)\n78.31 (76.49, 79.95)\nTV w/o Fill Strategy\n2.36 (1.81, 3.10)\n77.87 (76.01, 79.62)\nPrediction Window = 6 Months\nMachine Learning\nElastic Net LR\nTime Varying\n3.28 (2.83, 3.97)\n74.83 (73.46, 76.18)\nTV w/o Fill Strategy\n3.14 (2.73, 3.76)\n74.66 (73.32, 76.01)\nRandom Forest\nTime Varying\n3.86 (3.32, 4.61)\n77.98 (76.77, 79.24)\nTV w/o Fill Strategy\n3.86 (3.32, 4.60)\n78.02 (76.82, 79.26)\nXGBoost\nTime Varying\n4.13 (3.48, 5.01)\n78.45 (77.20, 79.67)\nTV w/o Fill Strategy\n3.97 (3.38, 4.77)\n78.47 (77.25, 79.68)\nMasked Language\nModels\nModernBERT\nTime Varying\n3.54 (3.07, 4.14)\n77.49 (76.23, 78.73)\nTV w/o Fill Strategy\n3.43 (3.01, 4.00)\n77.21 (75.95, 78.47)\nBioClinical ModernBERT\nTime Varying\n3.58 (3.12, 4.21)\n76.60 (75.33, 77.93)\nTV w/o Fill Strategy\n3.53 (3.05, 4.20)\n77.21 (75.93, 78.46)\nLarge Language\nModels\nLlama-3.1-8B\nTime Varying\n4.12 (3.52, 4.98)\n78.43 (77.21, 79.66)\nTV w/o Fill Strategy\n3.49 (3.06, 4.11)\n76.85 (75.56, 78.12)\nOpenBioLLM-8B\nTime Varying\n3.65 (3.19, 4.36)\n78.33 (77.11, 79.54)\nTV w/o Fill Strategy\n2.76 (2.44, 3.25)\n75.31 (73.98, 76.67)\nPrediction Window = 9 Months\nMachine Learning\nElastic Net LR\nTime Varying\n4.52 (4.05, 5.22)\n76.30 (75.22, 77.32)\nTV w/o Fill Strategy\n4.41 (3.95, 5.07)\n76.18 (75.09, 77.19)\nRandom Forest\nTime Varying\n5.23 (4.67, 5.95)\n79.03 (78.11, 79.95)\nTV w/o Fill Strategy\n5.21 (4.67, 5.90)\n78.94 (78.02, 79.84)\nXGBoost\nTime Varying\n5.14 (4.59, 5.89)\n78.37 (77.39, 79.31)\nTV w/o Fill Strategy\n5.08 (4.53, 5.90)\n78.14 (77.15, 79.06)\nMasked Language\nModels\nModernBERT\nTime Varying\n5.27 (4.68, 6.01)\n78.08 (77.08, 79.03)\nTV w/o Fill Strategy\n5.04 (4.39, 5.70)\n78.18 (76.22, 78.22)\nBioClinical ModernBERT\nTime Varying\n5.16 (4.65, 5.84)\n78.72 (77.75, 79.68)\nTV w/o Fill Strategy\n4.92 (4.41, 5.60)\n76.94 (75.89, 77.97)\nLarge Language\nModels\nLlama-3.1-8B\nTime Varying\n5.19 (4.64, 5.92)\n78.56 (77.60, 79.46)\nTV w/o Fill Strategy\n5.01 (4.51, 5.68)\n79.10 (78.11, 80.02)\nOpenBioLLM-8B\nTime Varying\n4.42 (3.97, 5.03)\n76.68 (75.64, 77.68)\nTV w/o Fill Strategy\n4.62 (4.17, 5.23)\n79.24 (78.28, 80.15)\nPrediction Window = 12 Months\nMachine Learning\nElastic Net LR\nTime Varying\n5.72 (5.22, 6.41)\n75.75 (74.74, 76.72)\nTV w/o Fill Strategy\n5.57 (5.11, 6.21)\n75.59 (74.60, 76.54)\nRandom Forest\nTime Varying\n6.39 (5.83, 7.06)\n78.39 (77.50, 79.24)\nTV w/o Fill Strategy\n6.48 (5.92, 7.18)\n78.56 (77.70, 79.41)\nXGBoost\nTime Varying\n6.72 (6.06, 7.53)\n78.05 (77.14, 78.92)\nTV w/o Fill Strategy\n6.58 (5.95, 7.35)\n78.02 (77.12, 78.90)\nMasked Language\nModels\nModernBERT\nTime Varying\n6.29 (5.76, 6.97)\n77.84 (76.90, 78.75)\nTV w/o Fill Strategy\n6.30 (5.78, 6.96)\n78.09 (77.14, 78.97)\nBioClinical ModernBERT\nTime Varying\n6.65 (6.03, 7.40)\n77.99 (77.07, 78.86)\nTV w/o Fill Strategy\n6.39 (5.85, 7.12)\n77.91 (76.98, 78.83)\nLarge Language\nModels\nLlama-3.1-8B\nTime Varying\n5.66 (5.16, 6.32)\n77.30 (76.38, 78.15)\nTV w/o Fill Strategy\n6.19 (5.61, 6.92)\n76.96 (76.04, 77.89)\nOpenBioLLM-8B\nTime Varying\n5.99 (5.49, 6.66)\n78.06 (77.18, 78.93)\nTV w/o Fill Strategy\n4.37 (4.04, 4.77)\n75.39 (74.49, 76.26)\n21/43\n"}, {"page": 22, "text": "Supplementary Table 3. Performance of time-varying models by risk tier (top 1% and 5%)\nand prediction window\nModel Class\nModel\nRisk Group\nSize, P\nSensitivity\n@ top-P, %\nSpecificity\n@ top-P, %\nPPV\n@ top-P, %\nO/E Ratio\n@ top-P\nPrediction Window = 3 Months\nMachine Learning\nElastic Net\nLogistic Regression\n0.01\n12.68 (10.35, 15.16)\n99.04 (99.03, 99.05)\n4.07 (3.32, 4.86)\n12.68 (10.35, 15.15)\n0.05\n30.76 (27.26, 34.11)\n95.08 (95.07, 95.09)\n1.97 (1.75, 2.19)\n6.15 (5.45, 6.82)\nRandom Forest\n0.01\n13.41 (10.93, 16.03)\n99.04 (99.03, 99.05)\n4.30 (3.51, 5.14)\n13.41 (10.93, 16.03)\n0.05\n33.09 (29.59, 36.59)\n95.09 (95.08, 95.10)\n2.12 (1.90, 2.35)\n6.62 (5.92, 7.32)\nXGBoost\n0.01\n12.83 (10.64, 15.45)\n99.04 (99.03, 99.05)\n4.12 (3.41, 4.96)\n12.83 (10.64, 15.45)\n0.05\n32.36 (29.01, 35.71)\n95.09 (95.08, 95.10)\n2.08 (1.86, 2.29)\n6.47 (5.80, 7.14)\nMasked Language\nModels\nModernBERT\n0.01\n12.24 (9.77, 14.58)\n99.04 (99.03, 99.04)\n3.93 (3.13, 4.68)\n12.25 (9.76, 14.58)\n0.05\n30.17 (26.96, 33.53)\n95.08 (95.07, 95.09)\n1.94 (1.73, 2.15)\n6.04 (5.39, 6.70)\nBioClinical ModernBERT\n0.01\n14.72 (12.39, 17.35)\n99.04 (99.04, 99.05)\n4.72 (3.97, 5.56)\n14.72 (12.39, 17.34)\n0.05\n32.80 (29.30, 36.30)\n95.09 (95.08, 95.10)\n2.10 (1.88, 2.33)\n6.56 (5.86, 7.26)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n13.85 (11.37, 16.33)\n99.04 (99.03, 99.05)\n4.44 (3.65, 5.24)\n13.85 (11.37, 16.32)\n0.05\n32.51 (29.15, 36.15)\n95.09 (95.08, 95.10)\n2.09 (1.87, 2.32)\n6.50 (5.83, 7.23)\nOpenBioLLM-8B\n0.01\n13.41 (11.37, 16.62)\n99.04 (99.03, 99.05)\n4.30 (3.65, 5.33)\n13.41 (11.37, 16.61)\n0.05\n31.92 (28.57, 35.57)\n95.09 (95.08, 95.10)\n2.05 (1.83, 2.28)\n6.38 (5.71, 7.11)\nPrediction Window = 6 Months\nMachine Learning\nElastic Net\nLogistic Regression\n0.01\n11.78 (10.14, 13.50)\n99.07 (99.06, 99.08)\n7.39 (6.36, 8.46)\n11.78 (10.14, 13.49)\n0.05\n27.59 (25.21, 29.90)\n95.14 (95.13, 95.16)\n3.46 (3.16, 3.75)\n5.52 (5.04, 5.98)\nRandom Forest\n0.01\n12.53 (10.81, 14.24)\n99.07 (99.06, 99.08)\n7.86 (6.78, 8.93)\n12.53 (10.81, 14.24)\n0.05\n29.53 (27.22, 31.99)\n95.15 (95.14, 95.17)\n3.70 (3.41, 4.01)\n5.91 (5.44, 6.40)\nXGBoost\n0.01\n12.53 (10.81, 14.32)\n99.07 (99.06, 99.08)\n7.86 (6.78, 8.98)\n12.53 (10.81, 14.31)\n0.05\n29.31 (26.92, 31.69)\n95.15 (95.14, 95.17)\n3.68 (3.38, 3.98)\n5.86 (5.38, 6.34)\nMasked Language\nModels\nModernBERT\n0.01\n12.53 (10.81, 14.32)\n99.07 (99.06, 99.08)\n7.86 (6.78, 8.98)\n12.53 (10.81, 14.31)\n0.05\n30.05 (27.67, 32.44)\n95.16 (95.14, 95.17)\n3.77 (3.47, 4.07)\n6.01 (5.53, 6.49)\nBioClinical ModernBERT\n0.01\n13.27 (11.41, 14.91)\n99.08 (99.07, 99.09)\n8.33 (7.15, 9.35)\n13.27 (11.40, 14.91)\n0.05\n29.53 (27.07, 31.99)\n95.15 (95.14, 95.17)\n3.70 (3.39, 4.01)\n5.91 (5.41, 6.40)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n12.83 (11.04, 14.47)\n99.07 (99.06, 99.08)\n8.04 (6.92, 9.07)\n12.83 (11.03, 14.46)\n0.05\n30.95 (28.49, 33.41)\n95.16 (95.15, 95.18)\n3.88 (3.57, 4.19)\n6.19 (5.70, 6.68)\nOpenBioLLM-8B\n0.01\n11.63 (10.37, 14.10)\n99.07 (99.06, 99.08)\n7.30 (6.50, 8.84)\n11.63 (10.36, 14.09)\n0.05\n30.28 (28.11, 32.96)\n95.16 (95.15, 95.18)\n3.80 (3.53, 4.13)\n6.06 (5.62, 6.59)\nPrediction Window = 9 Months\nMachine Learning\nElastic Net\nLogistic Regression\n0.01\n10.71 (9.34, 12.02)\n99.09 (99.08, 99.10)\n9.87 (8.60, 11.08)\n10.71 (9.33, 12.02)\n0.05\n26.69 (24.71, 28.61)\n95.20 (95.18, 95.22)\n4.92 (4.55, 5.27)\n5.34 (4.94, 5.72)\nRandom Forest\n0.01\n12.23 (10.86, 13.65)\n99.10 (99.09, 99.12)\n11.27 (10.00, 12.58)\n12.23 (10.85, 13.64)\n0.05\n29.17 (27.19, 31.05)\n95.22 (95.21, 95.24)\n5.38 (5.01, 5.72)\n5.83 (5.44, 6.21)\nXGBoost\n0.01\n11.97 (10.55, 13.39)\n99.10 (99.09, 99.11)\n11.04 (9.72, 12.34)\n11.97 (10.55, 13.39)\n0.05\n28.92 (26.89, 30.95)\n95.22 (95.20, 95.24)\n5.33 (4.96, 5.71)\n5.78 (5.38, 6.19)\nMasked Language\nModels\nModernBERT\n0.01\n12.08 (10.65, 13.50)\n99.10 (99.09, 99.12)\n11.13 (9.82, 12.44)\n12.08 (10.65, 13.49)\n0.05\n29.98 (27.90, 32.01)\n95.23 (95.21, 95.25)\n5.53 (5.14, 5.90)\n6.00 (5.58, 6.40)\nBioClinical ModernBERT\n0.01\n11.52 (10.25, 12.99)\n99.10 (99.09, 99.11)\n10.62 (9.44, 11.97)\n11.52 (10.24, 12.98)\n0.05\n30.44 (28.36, 32.32)\n95.24 (95.22, 95.25)\n5.61 (5.23, 5.96)\n6.09 (5.67, 6.46)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n12.38 (11.06, 13.90)\n99.11 (99.09, 99.12)\n11.41 (10.19, 12.81)\n12.38 (11.06, 13.90)\n0.05\n29.07 (27.09, 30.95)\n95.22 (95.21, 95.24)\n5.36 (4.99, 5.71)\n5.81 (5.42, 6.19)\nOpenBioLLM-8B\n0.01\n10.10 (9.28, 12.03)\n99.08 (99.08, 99.10)\n9.31 (8.56, 11.08)\n10.10 (9.28, 12.02)\n0.05\n27.45 (26.18, 30.19)\n95.21 (95.20, 95.23)\n5.06 (4.83, 5.56)\n5.49 (5.24, 6.04)\nPrediction Window = 12 Months\nMachine Learning\nElastic Net\nLogistic Regression\n0.01\n10.90 (9.76, 11.96)\n99.12 (99.11, 99.13)\n13.00 (11.64, 14.26)\n10.90 (9.76, 11.96)\n0.05\n26.39 (24.78, 28.00)\n95.26 (95.24, 95.28)\n6.30 (5.91, 6.68)\n5.28 (4.96, 5.60)\nRandom Forest\n0.01\n11.29 (10.12, 12.47)\n99.12 (99.11, 99.14)\n13.47 (12.06, 14.87)\n11.30 (10.11, 12.47)\n0.05\n28.12 (26.47, 29.84)\n95.28 (95.26, 95.30)\n6.71 (6.31, 7.12)\n5.62 (5.29, 5.97)\nXGBoost\n0.01\n11.57 (10.43, 12.78)\n99.13 (99.11, 99.14)\n13.80 (12.44, 15.24)\n11.57 (10.43, 12.78)\n0.05\n27.88 (26.20, 29.57)\n95.28 (95.26, 95.30)\n6.65 (6.25, 7.05)\n5.58 (5.24, 5.91)\nMasked Language\nModels\nModernBERT\n0.01\n11.10 (10.00, 12.24)\n99.12 (99.11, 99.14)\n13.24 (11.92, 14.59)\n11.10 (10.00, 12.23)\n0.05\n29.69 (27.88, 31.29)\n95.30 (95.28, 95.32)\n7.08 (6.65, 7.46)\n5.94 (5.58, 6.26)\nBioClinical ModernBERT\n0.01\n11.41 (10.24, 12.59)\n99.13 (99.11, 99.14)\n13.61 (12.20, 15.01)\n11.41 (10.23, 12.58)\n0.05\n29.18 (27.41, 30.78)\n95.29 (95.27, 95.31)\n6.96 (6.54, 7.34)\n5.84 (5.48, 6.16)\nLarge Language\nModels\nLlama-3.1-8B\n0.01\n9.76 (8.71, 10.90)\n99.11 (99.09, 99.12)\n11.65 (10.38, 13.00)\n9.77 (8.70, 10.90)\n0.05\n26.67 (25.14, 28.47)\n95.26 (95.24, 95.28)\n6.36 (6.00, 6.79)\n5.33 (5.03, 5.69)\nOpenBioLLM-8B\n0.01\n10.63 (10.04, 12.39)\n99.12 (99.11, 99.14)\n12.68 (11.97, 14.77)\n10.63 (10.04, 12.39)\n0.05\n28.00 (26.71, 30.04)\n95.28 (95.26, 95.30)\n6.68 (6.37, 7.16)\n5.60 (5.34, 6.01)\n22/43\n"}, {"page": 23, "text": "Supplementary Table 4. Model performance by predictor set and prediction window\nModel Class\nModel\nFeature Group\nPR-AUC, %\nROC AUC, %\nPrediction Window = 3 Months\nMachine Learning\nElastic Net\nLogistic Regression\nDemographic\n0.6 (0.55, 0.69)\n68.31 (66.44, 70.08)\nDemographic + Code\n1.6 (1.31, 2.11)\n76.06 (74.28, 77.81)\nDemographic + Code + SBFH\n1.87 (1.54, 2.45)\n76.71 (74.79, 78.46)\nRandom Forest\nDemographic\n0.6 (0.55, 0.68)\n68.77 (66.99, 70.45)\nDemographic + Code\n1.66 (1.36, 2.14)\n77.67 (75.93, 79.31)\nDemographic + Code + SBFH\n2.11 (1.76, 2.68)\n79.27 (77.54, 80.94)\nXGBoost\nDemographic\n0.65 (0.58, 0.80)\n69.79 (68.00, 71.49)\nDemographic + Code\n2.22 (1.63, 3.14)\n78.15 (76.46, 79.75)\nDemographic + Code + SBFH\n2.22 (1.83, 2.91)\n79.73 (77.95, 81.37)\nMasked Language\nModels\nModernBERT\nDemographic\n0.62 (0.56, 0.71)\n68.80 (66.93, 70.61)\nDemographic + Code\n1.78 (1.46, 2.35)\n77.97 (76.20, 79.64)\nDemographic + Code + SBFH\n2.39 (1.80, 3.34)\n77.28 (75.39, 79.00)\nBioClinical ModernBERT\nDemographic\n0.61 (0.55, 0.72)\n67.40 (65.49, 69.29)\nDemographic + Code\n1.60 (1.28, 2.22)\n75.71 (73.87, 77.43)\nDemographic + Code + SBFH\n2.34 (1.91, 3.09)\n78.78 (76.97, 80.53)\nLarge Language\nModels\nLlama-3.1-8B\nDemographic\n0.58 (0.52, 0.72)\n66.36 (64.43, 68.22)\nDemographic + Code\n1.42 (1.18, 1.93)\n75.44 (73.54, 77.18)\nDemographic + Code + SBFH\n2.16 (1.77, 2.76)\n79.12 (77.43, 80.79)\nOpenBioLLM-8B\nDemographic\n0.65 (0.55, 1.01)\n68.42 (66.66, 70.13)\nDemographic + Code\n1.26 (1.08, 1.52)\n76.10 (74.40, 77.83)\nDemographic + Code + SBFH\n2.32 (1.81, 3.10)\n78.31 (76.49, 79.95)\nPrediction Window = 6 Months\nMachine Learning\nElastic Net\nLogistic Regression\nDemographic\n1.27 (1.16, 1.49)\n67.15 (65.73, 68.51)\nDemographic + Code\n2.76 (2.38, 3.40)\n74.96 (73.67, 76.27)\nDemographic + Code + SBFH\n3.28 (2.83, 3.97)\n74.83 (73.46, 76.18)\nRandom Forest\nDemographic\n1.24 (1.16, 1.37)\n68.22 (66.88, 69.60)\nDemographic + Code\n2.98 (2.56, 3.67)\n76.41 (75.20, 77.65)\nDemographic + Code + SBFH\n3.86 (3.32, 4.61)\n77.98 (76.77, 79.24)\nXGBoost\nDemographic\n1.34 (1.24, 1.52)\n69.87 (68.57, 71.17)\nDemographic + Code\n2.90 (2.53, 3.47)\n76.69 (75.47, 77.91)\nDemographic + Code + SBFH\n4.13 (3.48, 5.01)\n78.45 (77.20, 79.67)\nMasked Language\nModels\nModernBERT\nDemographic\n1.29 (1.19, 1.45)\n68.66 (67.30, 69.94)\nDemographic + Code\n2.92 (2.53, 3.50)\n76.54 (75.31, 77.78)\nDemographic + Code + SBFH\n3.54 (3.07, 4.14)\n77.49 (76.23, 78.73)\nBioClinical ModernBERT\nDemographic\n1.36 (1.24, 1.54)\n68.87 (67.54, 70.16)\nDemographic + Code\n2.82 (2.47, 3.36)\n76.02 (74.75, 77.32)\nDemographic + Code + SBFH\n3.58 (3.12, 4.21)\n76.60 (75.33, 77.93)\nLarge Language\nModels\nLlama-3.1-8B\nDemographic\n1.16 (1.07, 1.33)\n65.95 (64.53, 67.36)\nDemographic + Code\n2.60 (2.31, 3.05)\n76.28 (75.04, 77.50)\nDemographic + Code + SBFH\n4.12 (3.52, 4.98)\n78.43 (77.21, 79.66)\nOpenBioLLM-8B\nDemographic\n1.26 (1.16, 1.41)\n68.05 (66.71, 69.31)\nDemographic + Code\n2.00 (1.79, 2.26)\n73.97 (72.65, 75.19)\nDemographic + Code + SBFH\n3.65 (3.19, 4.36)\n78.33 (77.11, 79.54)\nPrediction Window = 9 Months\nMachine Learning\nElastic Net\nLogistic Regression\nDemographic\n1.82 (1.71, 1.95)\n67.88 (66.71, 69.00)\nDemographic + Code\n3.93 (3.53, 4.51)\n76.11 (75.11, 77.10)\nDemographic + Code + SBFH\n4.52 (4.05, 5.22)\n76.30 (75.22, 77.32)\nRandom Forest\nDemographic\n1.86 (1.76, 1.99)\n70.28 (69.24, 71.20)\nDemographic + Code\n4.27 (3.86, 4.87)\n78.03 (77.11, 78.96)\nDemographic + Code + SBFH\n5.23 (4.67, 5.95)\n79.03 (78.11, 79.95)\nXGBoost\nDemographic\n1.97 (1.86, 2.12)\n71.27 (70.24, 72.23)\nDemographic + Code\n3.97 (3.57, 4.56)\n77.60 (76.68, 78.50)\nDemographic + Code + SBFH\n5.14 (4.59, 5.89)\n78.37 (77.39, 79.31)\nMasked Language\nModels\nModernBERT\nDemographic\n1.86 (1.75, 1.99)\n68.41 (67.29, 69.47)\nDemographic + Code\n4.60 (4.17, 5.16)\n78.93 (77.95, 79.89)\nDemographic + Code + SBFH\n5.27 (4.68, 6.01)\n78.08 (77.08, 79.03)\nBioClinical ModernBERT\nDemographic\n1.90 (1.79, 2.05)\n69.43 (68.28, 70.45)\nDemographic + Code\n4.31 (3.86, 4.91)\n77.84 (76.89, 78.79)\nDemographic + Code + SBFH\n5.16 (4.65, 5.84)\n78.72 (77.75, 79.68)\nLarge Language\nModels\nLlama-3.1-8B\nDemographic\n1.74 (1.64, 1.88)\n67.27 (66.13, 68.32)\nDemographic + Code\n2.20 (2.04, 2.40)\n70.35 (69.31, 71.44)\nDemographic + Code + SBFH\n5.19 (4.64, 5.92)\n78.56 (77.60, 79.46)\nOpenBioLLM-8B\nDemographic\n1.80 (1.71, 1.99)\n69.31 (68.27, 70.29)\nDemographic + Code\n3.09 (2.85, 3.47)\n75.15 (74.12, 76.14)\nDemographic + Code + SBFH\n4.42 (3.97, 5.03)\n76.68 (75.64, 77.68)\nContinued on next page\n23/43\n"}, {"page": 24, "text": "Supplementary Table 4 – continued\nPrediction Window = 12 Months\nMachine Learning\nElastic Net\nLogistic Regression\nDemographic\n2.35 (2.23, 2.51)\n68.41 (67.38, 69.44)\nDemographic + Code\n5.16 (4.70, 5.80)\n75.94 (74.95, 76.86)\nDemographic + Code + SBFH\n5.72 (5.22, 6.41)\n75.75 (74.74, 76.72)\nRandom Forest\nDemographic\n2.40 (2.28, 2.57)\n69.99 (69.04, 70.94)\nDemographic + Code\n5.52 (5.03, 6.12)\n77.86 (76.97, 78.69)\nDemographic + Code + SBFH\n6.39 (5.83, 7.06)\n78.39 (77.50, 79.24)\nXGBoost\nDemographic\n2.49 (2.37, 2.65)\n70.63 (69.72, 71.53)\nDemographic + Code\n5.43 (4.93, 6.08)\n77.31 (76.43, 78.14)\nDemographic + Code + SBFH\n6.72 (6.06, 7.53)\n78.05 (77.14, 78.92)\nMasked Language\nModels\nModernBERT\nDemographic\n2.42 (2.29, 2.57)\n69.05 (68.05, 70.08)\nDemographic + Code\n4.88 (4.44, 5.42)\n76.18 (75.26, 77.06)\nDemographic + Code + SBFH\n6.29 (5.76, 6.97)\n77.84 (76.90, 78.75)\nBioClinical ModernBERT\nDemographic\n2.44 (2.31, 2.61)\n69.11 (68.13, 70.12)\nDemographic + Code\n5.57 (5.06, 6.19)\n77.27 (76.33, 78.12)\nDemographic + Code + SBFH\n6.65 (6.03, 7.40)\n77.99 (77.07, 78.86)\nLarge Language\nModels\nLlama-3.1-8B\nDemographic\n2.18 (2.08, 2.30)\n67.59 (66.60, 68.58)\nDemographic + Code\n4.73 (4.35, 5.19)\n75.39 (74.41, 76.30)\nDemographic + Code + SBFH\n5.66 (5.16, 6.32)\n77.30 (76.38, 78.15)\nOpenBioLLM-8B\nDemographic\n2.49 (2.37, 2.63)\n71.03 (70.12, 71.91)\nDemographic + Code\n4.78 (4.40, 5.22)\n76.40 (75.47, 77.30)\nDemographic + Code + SBFH\n5.99 (5.49, 6.66)\n78.06 (77.18, 78.93)\n24/43\n"}, {"page": 25, "text": "Supplementary Figure 2. Predictor importance estimated using SHAP values\nSHAP (SHapley Additive exPlanations) values for top 100 positive (A) and negative (B) predictors across all three models\n(Elastic Net Logistic Regression [ENL], Random Forest [RF], and XGBoost) and four prediction windows (3, 6, 9, and\n12 months). Columns represent predictors shown \"as is\" with time-stamped variants preserved (e.g., PTSD_2016_Q1 and\nPTSD_2016_H1 are displayed as separate features), reflecting the actual feature space used by each model-window combination.\nPredictors are grouped by category with visual separators and prefixed as follows: [D] Demographics, [D-MH] Mental Health\nDisorders, [D-PH] Physical Health Disorders, [D-SA] Substance Abuse Disorders, [U] Service Utilization, [P] Procedures, and\n[S] Social and Behavioral Factors of Health.\n(A)\n25/43\n"}, {"page": 26, "text": "(B)\n26/43\n"}, {"page": 27, "text": "Supplementary Table 5. Subgroup-specific PR-AUC by prediction window, model, and\ndemographic subgroup\nPR-AUC for the best models from each class at each of the prediction windows (3-, 6-, 9-, and 12-months), stratified by age,\nrace, and ethnicity. * Estimates did not meet pre-specified reliability criteria (fewer than 20 positive or negative cases, or 95%\nCI width > 0.12).\nSubgroup\nLevel\nN\nEvents\nModel 1\nPR-AUC, %\nModel 2\nPR-AUC, %\nModel 3\nPR-AUC, %\nPrediction Window = 3 Months\nXGBoost\nModernBERT\nLlama\nAge\n18–29\n7195\n63\n4.78 (2.98, 9.38)\n5.52 (2.69, 12.59)\n4.15 (2.79, 7.36)\n30–39\n17206\n114\n4.06 (2.56, 7.74)\n4.63 (2.45, 8.94)\n4.01 (2.63, 7.05)\n40–49\n18605\n83\n2.76 (1.81, 4.69)\n2.75 (1.56, 6.35)\n2.54 (1.65, 4.35)\n50–59\n29066\n178\n3.19 (2.30, 5.34)\n3.00 (2.13, 4.88)\n2.98 (2.11, 5.01)\n60–69\n63966\n155\n1.96 (1.24, 3.59)\n2.21 (1.09, 4.31)\n1.72 (1.10, 3.26)\n70–79\n45844\n62\n0.63 (0.43, 1.14)\n0.68 (0.35, 2.21)\n0.50 (0.35, 0.84)\n80–100\n31939\n31\n0.52 (0.28, 1.14)\n0.36 (0.21, 0.77)\n0.41 (0.22, 0.94)\nEthnicity\nHispanic\n13344\n52\n3.80 (1.67, 9.24)\n4.26 (1.38, 9.48)\n3.95 (1.76, 10.34)\nNot Hispanic\n194335\n621\n2.18 (1.78, 2.88)\n2.32 (1.73, 3.27)\n2.08 (1.70, 2.71)\nUnknown\n6142\n13\n2.68 (0.46, 16.35)*\n7.43 (0.39, 30.51)*\n4.08 (0.49, 19.97)*\nRace\nAmerican Indian\n1555\n9\n11.31 (2.41, 39.16)*\n25.56 (6.36, 56.06)*\n24.49 (4.66, 54.27)*\nBlack\n33902\n223\n3.21 (2.28, 5.14)\n2.78 (1.81, 4.42)\n2.87 (2.01, 4.64)\nNative Hawaiian\n1796\n7\n8.83 (2.05, 34.73)*\n6.35 (1.90, 23.47)*\n9.43 (2.28, 31.64)*\nUnknown\n14526\n39\n1.92 (0.86, 6.34)\n1.58 (0.67, 5.52)\n1.86 (0.83, 6.72)\nWhite\n159932\n408\n1.98 (1.52, 2.86)\n2.49 (1.63, 3.98)\n1.95 (1.52, 2.69)\nPrediction Window = 6 Months\nXGBoost\nClin. ModernBERT\nLlama\nAge\n18–29\n7137\n107\n7.43 (4.78, 12.41)\n6.07 (4.17, 10.03)\n7.56 (4.56, 12.59)\n30–39\n16885\n220\n6.13 (4.32, 9.27)\n5.25 (3.93, 7.78)\n5.25 (3.84, 7.96)\n40–49\n18571\n183\n8.15 (5.26, 12.47)\n6.50 (4.24, 10.71)\n6.81 (4.46, 10.54)\n50–59\n29023\n307\n4.29 (3.40, 6.02)\n3.54 (2.84, 4.73)\n4.08 (3.14, 5.95)\n60–69\n64276\n339\n4.36 (3.12, 6.27)\n3.22 (2.49, 4.32)\n3.98 (2.92, 6.03)\n70–79\n45969\n125\n2.12 (1.47, 3.84)\n3.05 (1.74, 6.18)\n3.81 (1.81, 8.17)\n80–100\n31960\n60\n2.59 (1.05, 7.36)\n0.93 (0.51, 2.19)\n1.04 (0.67, 1.92)\nEthnicity\nHispanic\n13339\n93\n5.19 (3.06, 9.79)\n5.14 (2.98, 10.07)\n5.35 (3.07, 10.63)\nNot Hispanic\n194320\n1217\n4.14 (3.47, 5.09)\n3.55 (3.07, 4.20)\n4.10 (3.52, 5.04)\nUnknown\n6162\n31\n3.16 (1.50, 8.45)\n4.06 (0.89, 13.99)\n5.17 (1.07, 15.33)\nRace\nAmerican Indian\n1617\n17\n10.89 (2.10, 28.08)*\n11.07 (2.12, 28.36)*\n4.87 (2.03, 16.20)*\nAsian\n2075\n12\n6.94 (1.78, 27.70)*\n11.59 (1.38, 32.42)*\n7.95 (1.92, 29.16)*\nBlack\n33879\n409\n5.10 (3.99, 7.08)\n4.09 (3.33, 5.32)\n4.53 (3.68, 6.08)\nNative Hawaiian\n1954\n10\n13.46 (1.02, 37.51)*\n3.58 (0.77, 16.22)*\n8.65 (1.01, 33.57)*\nUnknown\n14460\n91\n3.31 (2.30, 5.38)\n3.80 (2.43, 7.30)*\n4.12 (2.53, 8.32)*\nWhite\n159836\n802\n4.12 (3.36, 5.27)\n3.56 (2.96, 4.41)\n4.23 (3.38, 5.56)\nPrediction Window = 9 Months\nRandom Forest\nModernBERT\nLlama\nAge\n18–29\n7105\n174\n10.30 (7.16, 14.70)\n8.92 (6.59, 12.89)\n8.38 (6.48, 11.89)\n30–39\n16922\n335\n6.83 (5.62, 8.86)\n7.39 (5.92, 9.59)\n7.15 (5.83, 9.16)\n40–49\n18618\n268\n6.63 (4.96, 9.29)\n6.75 (5.23, 9.48)\n6.94 (5.27, 9.54)\n50–59\n29241\n436\n8.24 (6.55, 10.40)\n8.74 (7.06, 11.09)\n8.96 (7.06, 11.40)\n60–69\n63979\n513\n4.19 (3.54, 5.23)\n4.64 (3.83, 5.95)\n4.62 (3.86, 5.94)\n70–79\n45875\n178\n2.53 (1.55, 4.35)\n2.21 (1.48, 4.21)\n2.19 (1.58, 3.78)\n80–100\n32081\n67\n2.20 (0.76, 6.55)\n1.78 (0.55, 6.13)\n4.34 (1.09, 11.02)\nEthnicity\nHispanic\n13148\n163\n6.40 (4.18, 12.77)\n8.00 (4.03, 18.94)\n10.44 (4.45, 21.34)\nNot Hispanic\n194631\n1773\n13.63 (4.06, 31.10)\n8.34 (3.96, 20.12)\n8.87 (3.71, 23.71)\nUnknown\n6042\n35\n6.97 (6.03, 8.40)\n7.31 (6.20, 9.10)\n7.42 (6.31, 9.15)\nRace\nAmerican Indian\n1682\n32\n4.71 (3.43, 7.45)\n5.01 (3.54, 7.83)*\n5.15 (3.53, 8.49)*\nAsian\n2133\n21\n2.50 (1.58, 5.89)*\n2.54 (1.62, 5.86)*\n3.21 (1.79, 8.58)*\nBlack\n34121\n623\n5.35 (4.76, 6.17)\n5.34 (4.75, 6.11)\n5.34 (4.74, 6.15)\nNative Hawaiian\n1807\n21\n3.97 (2.23, 10.42)\n6.27 (2.05, 14.87)\n3.08 (1.71, 8.03)\nUnknown\n14488\n128\n3.38 (2.45, 5.65)\n3.37 (2.39, 5.49)*\n3.58 (2.50, 6.54)\nWhite\n159590\n1146\n5.17 (4.40, 6.27)\n4.92 (4.19, 5.88)\n4.81 (4.11, 5.78)\nContinued on next page\n27/43\n"}, {"page": 28, "text": "Supplementary Table 5 – continued\nPrediction Window = 12 Months\nXGBoost\nClin. ModernBERT\nOpenBio\nAge\n18–29\n7138\n202\n8.77 (6.29, 12.37)\n8.97 (6.90, 12.55)\n9.57 (7.19, 13.31)\n30–39\n16921\n397\n9.95 (7.80, 12.61)\n8.12 (6.67, 10.22)\n9.73 (7.95, 12.28)\n40–49\n18743\n341\n8.77 (7.01, 11.35)\n8.47 (6.85, 11.03)\n9.14 (7.30, 11.77)\n50–59\n28788\n582\n9.74 (8.27, 11.93)\n8.92 (7.72, 10.89)\n9.40 (8.08, 11.29)\n60–69\n63785\n660\n6.17 (5.08, 7.83)\n6.74 (5.39, 8.54)\n6.39 (5.18, 7.99)\n70–79\n46107\n259\n2.92 (2.15, 4.59)\n2.67 (2.03, 3.79)\n3.03 (2.29, 4.30)\n80–100\n32339\n109\n2.67 (1.29, 5.29)\n2.96 (1.12, 6.30)\n2.18 (1.31, 4.67)\nEthnicity\nHispanic\n13434\n198\n6.04 (4.14, 9.13)\n5.33 (3.95, 8.20)\n4.75 (3.72, 6.74)\nNot Hispanic\n194328\n2298\n6.90 (6.18, 7.77)\n6.91 (6.20, 7.76)\n6.21 (5.62, 6.94)\nUnknown\n6059\n54\n4.02 (2.49, 8.43)\n3.73 (2.28, 7.62)\n3.82 (2.44, 7.07)\nRace\nAmerican Indian\n1535\n27\n5.96 (2.99, 15.77)*\n5.43 (3.44, 12.51)*\n6.23 (3.01, 16.54)*\nAsian\n2076\n14\n7.72 (3.18, 18.81)*\n5.74 (2.55, 17.22)*\n5.71 (2.14, 20.34)*\nBlack\n33963\n811\n8.69 (7.46, 10.34)\n8.84 (7.69, 10.54)\n9.03 (7.90, 10.58)\nNative Hawaiian\n1862\n22\n12.81 (5.70, 29.87)*\n10.42 (4.87, 24.27)*\n11.19 (5.51, 26.90)*\nUnknown\n14447\n139\n4.55 (3.26, 7.41)\n3.90 (3.08, 5.60)\n3.97 (2.96, 5.93)\nWhite\n159938\n1537\n6.54 (5.66, 7.70)\n6.47 (5.64, 7.48)\n5.71 (5.01, 6.61)\n28/43\n"}, {"page": 29, "text": "Supplementary Table 6. Fairness summary: maximum PR-AUC gap and worst-group\nPR-AUC across demographic subgroups and prediction windows\nThe Likelihood Ratio Test evaluates whether the relationship between model predictions and outcomes varies across demo-\ngraphic subgroups. We apply the Benjamini-Hochberg procedure to control the FDR when testing heterogeneity across multiple\nsubgroup variables (typically 5 per model-time window). The q-FDR value is the FDR-adjusted p-value; q-FDR < 0.05 indicates\nstatistically significant heterogeneity after accounting for multiple comparisons.\nSubgroup\nModel Class\nModel\nPR-AUC Gap\n(max–min), %\nWorst-Group\nPR-AUC, %\nLRT q-FDR\nPrediction Window = 3 Months\nAge\nMachine Learning\nElastic Net Logistic Regression\n4.30%\n0.41%\n0.398\nRandom Forest\n4.25%\n0.61%\n0.661\nXGBoost\n4.26%\n0.52%\n0.848\nMasked Language\nModels\nModernBERT\n5.16%\n0.36%\n0.786\nClinicalModernBERT\n4.68%\n0.56%\n0.757\nLarge Language\nModels\nLlama-3.1-8B\n3.74%\n0.41%\n0.982\nOpenBioLLM-8B\n4.46%\n0.58%\n0.647\nEthnicity\nMachine Learning\nElastic Net Logistic Regression\n1.98%\n1.81%\n0.823\nRandom Forest\n1.08%\n1.86%\n0.810\nXGBoost\n1.61%\n2.18%\n0.939\nMasked Language\nModels\nModernBERT\n5.10%\n2.32%\n0.786\nClinicalModernBERT\n1.42%\n2.30%\n0.921\nLarge Language\nModels\nLlama-3.1-8B\n2.00%\n2.08%\n0.982\nOpenBioLLM-8B\n2.10%\n1.91%\n0.818\nRace\nMachine Learning\nElastic Net Logistic Regression\n15.34%\n1.60%\n0.010\nRandom Forest\n12.77%\n1.95%\n0.018\nXGBoost\n9.39%\n1.92%\n0.228\nMasked Language\nModels\nModernBERT\n23.99%\n1.58%\n0.033\nClinicalModernBERT\n13.59%\n2.16%\n0.001\nLarge Language\nModels\nLlama-3.1-8B\n22.63%\n1.86%\n0.038\nOpenBioLLM-8B\n17.92%\n1.66%\n0.211\nPrediction Window = 6 Months\nAge\nMachine Learning\nElastic Net Logistic Regression\n6.84%\n0.95%\n< 0.001\nRandom Forest\n6.95%\n1.32%\n0.013\nXGBoost\n6.03%\n2.12%\n0.016\nMasked Language\nModels\nModernBERT\n6.15%\n0.85%\n0.002\nClinicalModernBERT\n5.57%\n0.93%\n< 0.001\nLarge Language\nModels\nLlama-3.1-8B\n6.52%\n1.04%\n< 0.001\nOpenBioLLM-8B\n5.33%\n1.22%\n0.032\nEthnicity\nMachine Learning\nElastic Net Logistic Regression\n2.21%\n2.08%\n0.810\nRandom Forest\n2.69%\n2.20%\n0.869\nXGBoost\n2.04%\n3.16%\n0.935\nMasked Language\nModels\nModernBERT\n2.00%\n3.34%\n0.503\nClinicalModernBERT\n1.58%\n3.55%\n0.309\nLarge Language\nModels\nLlama-3.1-8B\n1.25%\n4.10%\n0.427\nOpenBioLLM-8B\n3.41%\n2.40%\n0.538\nRace\nMachine Learning\nElastic Net Logistic Regression\n2.37%\n2.93%\n< 0.001\nRandom Forest\n10.27%\n3.09%\n0.008\nXGBoost\n10.15%\n3.31%\n0.043\nMasked Language\nModels\nModernBERT\n7.81%\n3.46%\n< 0.001\nClinicalModernBERT\n8.03%\n3.56%\n< 0.001\nLarge Language\nModels\nLlama-3.1-8B\n4.53%\n4.12%\n0.427\nOpenBioLLM-8B\n9.74%\n3.03%\n0.062\nPrediction Window = 9 Months\nContinued on next page\n29/43\n"}, {"page": 30, "text": "Supplementary Table 6 – continued\nAge\nMachine Learning\nElastic Net Logistic Regression\n6.62%\n1.48%\n< 0.001\nRandom Forest\n8.10%\n2.20%\n0.004\nXGBoost\n7.14%\n1.97%\n0.012\nMasked Language\nModels\nModernBERT\n7.14%\n1.78%\n< 0.001\nClinicalModernBERT\n7.22%\n1.24%\n< 0.001\nLarge Language\nModels\nLlama-3.1-8B\n6.77%\n2.19%\n0.009\nOpenBioLLM-8B\n6.01%\n1.91%\n0.011\nEthnicity\nMachine Learning\nElastic Net Logistic Regression\n0.06%\n4.50%\n0.113\nRandom Forest\n1.38%\n3.97%\n0.147\nXGBoost\n2.70%\n4.75%\n0.110\nMasked Language\nModels\nModernBERT\n1.27%\n5.01%\n0.041\nClinicalModernBERT\n1.51%\n4.56%\n0.110\nLarge Language\nModels\nLlama-3.1-8B\n2.26%\n3.08%\n0.366\nOpenBioLLM-8B\n0.95%\n3.61%\n0.003\nRace\nMachine Learning\nElastic Net Logistic Regression\n7.51%\n2.56%\n< 0.001\nRandom Forest\n11.12%\n2.50%\n0.067\nXGBoost\n9.27%\n2.98%\n0.001\nMasked Language\nModels\nModernBERT\n5.80%\n2.54%\n< 0.001\nClinicalModernBERT\n8.11%\n3.22%\n< 0.001\nLarge Language\nModels\nLlama-3.1-8B\n7.23%\n3.21%\n0.011\nOpenBioLLM-8B\n6.50%\n1.99%\n0.001\nPrediction Window = 12 Months\nAge\nMachine Learning\nElastic Net Logistic Regression\n7.51%\n1.21%\n< 0.001\nRandom Forest\n7.18%\n1.80%\n< 0.001\nXGBoost\n7.28%\n2.67%\n< 0.001\nMasked Language\nModels\nModernBERT\n6.74%\n2.24%\n< 0.001\nClinicalModernBERT\n6.30%\n2.67%\n< 0.001\nLarge Language\nModels\nLlama-3.1-8B\n5.70%\n4.16%\n< 0.001\nOpenBioLLM-8B\n7.22%\n3.97%\n0.187\nEthnicity\nMachine Learning\nElastic Net Logistic Regression\n2.09%\n3.77%\n0.055\nRandom Forest\n2.62%\n3.94%\n0.304\nXGBoost\n2.88%\n4.02%\n0.116\nMasked Language\nModels\nModernBERT\n2.07%\n4.39%\n0.083\nClinicalModernBERT\n3.18%\n3.73%\n0.019\nLarge Language\nModels\nLlama-3.1-8B\n1.31%\n4.37%\n0.189\nOpenBioLLM-8B\n2.39%\n3.82%\n0.183\nRace\nMachine Learning\nElastic Net Logistic Regression\n9.51%\n3.94%\n< 0.001\nRandom Forest\n5.92%\n5.06%\n< 0.001\nXGBoost\n8.26%\n4.55%\n< 0.001\nMasked Language\nModels\nModernBERT\n9.24%\n4.18%\n< 0.001\nClinicalModernBERT\n6.52%\n3.90%\n< 0.001\nLarge Language\nModels\nLlama-3.1-8B\n5.58%\n2.47%\n< 0.001\nOpenBioLLM-8B\n7.55%\n2.18%\n0.183\n30/43\n"}, {"page": 31, "text": "Supplementary Note 1. ICD codes and categorization of predictors\nDiagnosis-based predictors were defined using ICD-10-CM codes extracted from the VA Corporate Data Warehouse and\ngrouped into clinically coherent domains. Mental health conditions (e.g., depressive disorders, PTSD, psychotic disorders),\nsubstance use disorders (alcohol, opioids, stimulants, cannabis, other drugs), and physical health conditions (e.g., cardiovascular,\nmetabolic, respiratory, neurologic) were categorized using code lists developed with clinical input and aligned to prior VA and\nhomelessness literature. Social and behavioral factors of health (SBFH) predictors were derived from ICD-10-CM Z-codes and\nrelated flags capturing housing instability, financial and employment problems, interpersonal violence, and other social needs.\nPhysical Health Disorders\nPredictor\nICD-10 Codes\nAIDS/HIV\nB20, B21, B22, B24\nBlood Loss Anemia\nD50.0\nCardiac Arrhythmia\nI44.1, I44.2, I44.3, I45.6, I45.9, I47, I48, I49, R00.0, R00.1, R00.8, T82.1, Z45.0, Z95.0\nCardiovascular Disease\nI48, I49, I70\nChronic Pulmonary Disease\nI27.8, I27.9, J40, J41, J42, J43, J44, J45, J46, J47, J60, J61, J62, J63, J64, J65, J66, J67,\nJ68.4, J70.1, J70.3\nCirrhosis\nK70.3, K71.7, K74, K76.1\nCoagulopathy\nD65, D66, D67, D68, D69.1, D69.3, D69.4, D69.5, D69.6\nCongestive Heart Failure\nI09.9, I11.0, I13.0, I13.2, I25.5, I42.0, I42.5, I42.6, I42.7, I42.8, I42.9, I43, I50, P29.0\nDeficiency Anemia\nD50.8, D50.9, D51, D52, D53\nDiabetes\nE10.2, E10.3, E10.4, E10.5, E10.6, E10.7, E10.8, E11.2, E11.3, E11.4, E11.5, E11.6,\nE11.7, E11.8, E12.2, E12.3, E12.4, E12.5, E12.6, E12.7, E12.8, E13.2, E13.3, E13.4,\nE13.5, E13.6, E13.7, E13.8, E14.2, E14.3, E14.4, E14.5, E14.6, E14.7, E14.8, E10.0,\nE10.1, E10.9, E11.0, E11.1, E11.9, E12.0, E12.1, E12.9, E13.0, E13.1, E13.9, E14.0,\nE14.1, E14.9\nFluid and Electrolyte Disorders\nE22.2, E86, E87\nHepatitis\nB17.1, B18.2, B19.2, Z22.52, B15.0, B15.9, B16, B16.0, B16.1, B16.2, B16.9, B17.0,\nB17.1, B17.10, B17.11, B17.2, B17.8, B17.9, B18.0, B18.1, B18.2, B18.8, B18.9, B19,\nB19.0, B19.1, B19.10, B19.11, B19.2, B19.20, B19.21, B19.9, B25.1\nHypertension\nI11, I12, I13, I15, I10\nHypothyroidism\nE00, E01, E02, E03, E89.0\nInfluenza\nJ09.X1, J09.X2, J09.X3, J09.X9, J10.00, J10.01, J10.08, J10.1, J10.2, J10.81, J10.82,\nJ10.83, J10.89, J11.00, J11.08, J11.1, J11.2, J11.81, J11.82, J11.83, J11.89\nLiver Disease\nB18, I85, I86.4, I98.2, K70, K71.1, K71.3, K71.4, K71.5, K71.7, K72, K73, K74,\nK76.0, K76.2, K76.3, K76.4, K76.5, K76.6, K76.7, K76.8, K76.9, Z94.4\nLymphoma\nC81, C82, C83, C84, C85, C88, C96, C90.0, C90.2\nMetastatic Cancer\nC77, C78, C79, C80\nObesity\nE66\nContinued on next page\n31/43\n"}, {"page": 32, "text": "Table – continued\nPain\nA02.%, A18.%, A39.%, A52.%, A54.%, A59.%, A69.%, B0%, B02.%, B06.%, B1%,\nC0%, C1%, D0%, D1%, D57.%, D86.%, E08.%, E10.%, E11.%, E13.%, F45.%,\nG25.%, G43%, G43.%, G44.%, G50.%, G52.%, G54.%, G56.%, G57.%, G58.%,\nG59%, G60.%, G61.%, G62.%, G63%, G72.%, G89.%, G90.%, G96.%, G99.%,\nH46.%, H47.%, H57.%, I20.%, I77.%, K40.%, K41.%, K42.%, K43.%, K44.%, K45.%,\nK46.%, K58.%, K80.%, L40.%, L97.%, L98.%, M00.%, M01%, M02.%, M05.%,\nM06.%, M07.%, M08.%, M10.%, M11.%, M12.%, M13.%, M14.%, M15.%, M16.%,\nM17.%, M18.%, M19.%, M1A%, M20.%, M21.%, M22.%, M23.%, M24.%, M25.%,\nM26.%, M27.%, M32.%, M33.%, M34.%, M35.%, M43.%, M45.%, M46.%, M47.%,\nM48.%, M49.%, M50.%, M51.%, M53.%, M54.%, M60.%, M62.%, M65.%, M66.%,\nM67.%, M70.%, M71.%, M72.%, M75.%, M76.%, M77.%, M79.%, M80.%, M84.%,\nM86.%, M87.%, M89.%, M90.%, M94.%, M95.%, M96.%, M99.%, N20.%, N21.%,\nN22%, N30.%, N41.%, N42.%, N45.%, N50.%, N51%, N53.%, N71.%, N72%, N73.%,\nN80.%, N94.%, O26.%, Q66.%, Q67.%, Q74.%, Q76.%, R07.%, R10.%, R25.%,\nR51%, R52%, R68.%, S00.%, S02.%, S03.%, S05.%, S06.%, S09.%, S10.%, S12.%,\nS13.%, S14.%, S16.%, S19.%, S20.%, S22.%, S23.%, S24.%, S29.%, S30.%, S32.%,\nS33.%, S34.%, S39.%, S30.%, S32.%, S33.%, S34.%, S39.%, S40.%, S42.%, S43.%,\nS46.%, S49.%, S50.%, S52.%, S53.%, S56.%, S59.%, S60.%, S62.%, S63.%, S66.%,\nS70.%, S72.%, S73.%, S76.%, S79.%, S80.%, S82.%, S83.%, S86.%, S89.%, S90.%,\nS92.%, S93.%, S96.%, T07%, T14.%, T83.%, T84.%, T85.%, X0%, X11%, X12%,\nX19%, X21%, X22%, X29%, X31%, X32%, X39%, X41%, X42%, X49%, X51%,\nX52%, X59%, X61%, X62%, X69%, X71%, X72%, X79%, X8%, X9%\nParalysis\nG04.1, G11.4, G80.1, G80.2, G81, G82, G83.0, G83.1, G83.2, G83.3, G83.4, G83.9\nPeptic Ulcer Disease\nK25.7, K25.9, K26.7, K26.9, K27.7, K27.9, K28.7, K28.9\nPeripheral Vascular Disorders\nI70, I71, I73.1, I73.8, I73.9, I77.1, I79.0, I79.2, K55.1, K55.8, K55.9, Z95.8, Z95.9\nPulmonary Circulation Disorders\nI26, I27, I28.0, I28.8, I28.9\nRenal Failure\nI12.0, I13.1, N18, N19, N25.0, Z49.0, Z49.1, Z49.2, Z94.0, Z99.2\nRheumatoid Arthritis/collagen\nL94.0, L94.1, L94.3, M05, M06, M08, M12.0, M12.3, M30, M31.0, M31.1, M31.2,\nM31.3, M32, M33, M34, M35, M45, M46.1, M46.8, M46.9\nSolid Tumor without Metastasis\nC00, C01, C02, C03, C04, C05, C06, C07, C08, C09, C10, C11, C12, C13, C14, C15,\nC16, C17, C18, C19, C20, C21, C22, C23, C24, C25, C26, C30, C31, C32, C33, C34,\nC37, C38, C39, C40, C41, C43, C45, C46, C47, C48, C49, C50, C51, C52, C53, C54,\nC55, C56, C57, C58, C60, C61, C62, C63, C64, C65, C66, C67, C68, C69, C70, C71,\nC72, C73, C74, C75, C76, C97\nTraumatic brain injury\nF07.81, S02.0, S02.1, S02.8, S02.9, S04.2, S04.3, S04.4, S06., S07.1, Z87.820\nValvular Disease\nA52.0, I05, I06, I07, I08, I09.1, I09.8, I34, I35, I36, I37, I38, I39, Q23.0, Q23.1, Q23.2,\nQ23.3, Z95.2, Z95.3, Z95.4\nWeight Loss\nE40, E41, E42, E43, E44, E45, E46, R63.4, R64\n32/43\n"}, {"page": 33, "text": "Mental Health Disorders\nPredictor\nICD-10 Codes\nAnxiety Disorder\nF40, F41, F01.54, F01.A4, F01.B4, F01.C4, F02.84, F02.A4, F02.B4, F02.C4, F03.94,\nF03.A4, F03.B4, F03.C4, F06.4\nBipolar disorder\nF30, F31\nDementia\nF01, F02, F03, G30, G31.0, G31.83\nDepression\nF20.4, F31.3, F31.4, F31.5, F32, F33, F34.1, F41.2, F43.2\nOther Neurological Disorders\nG10, G11, G12, G13, G20, G21, G22, G25.4, G25.5, G31.2, G31.8, G31.9, G32, G35,\nG36, G37, G40, G41, G93.1, G93.4, R47.0, R56\nPosttraumatic Stress Disorder\nF43.10, F43.11, F43.12\nPsychoses\nF20, F22, F23, F24, F25, F28, F29, F30.2, F31.2, F31.5\nSleep Disorder\nF51.9, F51.8, F51.19, F51.09, F51.1, G47.27, G47.24, G47.22, G47.26, G47.12, G47.54,\nG47.25, G47.11, F51.13, G47.69, F51.05, G47.29, G47.14, G47.8, G47.53, G47.01,\nG47.20, G47.6, G47.9, G47.61, F51.4, G47.21, G47.23, F51.12, F51.3, G47.52, F51.04,\nG47.2, G47.10, G47.62, G47.50, G25.81, G47.00, G47.13, G47.63, F51.03, G47.51,\nF51.02, F51.11, F51.5, G47.19, F51.01, G47.59, G47, G47.09, G47.1, G47.5, G47.0,\nG25.81\n33/43\n"}, {"page": 34, "text": "Substance Abuse Disorders\nPredictor\nICD-10 Codes\nAlcohol Use Disorder\nF10, G62.1, I42.6, K29.2, K70, T51.0\nCannabis\nF12\nCocaine\nF14\nDrug Abuse\nF12, F13, F14, F15, F16, F18, F19, Z71.5, Z72.2\nHallucinogen\nF16\nNicotine dependence\nF17\nOpioid Use Disorder\nF11.10, F11.11, F11.120, F11.121, F11.122, F11.129, F11.13, F11.14, F11.150, F11.151, F11.159,\nF11.181, F11.182, F11.188, F11.19, F11.20, F11.21, F11.220, F11.221, F11.222, F11.229, F11.23,\nF11.24, F11.250,F11.251, F11.259, F11.281, F11.282, F11.288, F11.29\nOther stimulant\nF15\n34/43\n"}, {"page": 35, "text": "Social and Behavioral Factors of Health\nPredictor\nICD-10 Codes\nStop Codes\nEmployment or financial problem\nZ56, Z59.5-9\n208, 222, 535, 555, 568, 574\nFood insecurity\nZ59.4\n—\nHousing instability\nZ59.1, Z59.81 (CDW)\n504, 507, 508, 511, 522, 528, 529, 530,\n555, 556, 590\nLegal problems\nZ65.0-4, Y92.14\n591, 592\nNon-specific psychosocial needs\nR41.83, Z53.1, Z55.0-4, Z55.8-9, Z60.0,\nZ60.3-5, Z60.8-9, Z56.1, Z64.4, Z65.0-4,\nZ65.8-9, Z73.4-6\n—\nSocial or familial problems\nZ59.2, Z59.3, Z55, Z60\n—\nViolence Problems\nO9A.3%, O9A.4%, O9A.5%, T74%,\nT76%, X92%, X93%, X94%, X95%,\nX96%, X97%, X98%, X99%, Y00%,\nY01%, Y02%, Y03%, Y04%, Y07%,\nY08%, Y09%, Y35%, Y36%, Y37%,\nY38%,\nZ04.4%,\nZ04.7%,\nZ04.81%,\nZ65.0%,\nZ65.1%,\nZ65.2%,\nZ65.3%,\nZ65.4%,\nZ65.5%,\nZ65.8%,\nZ65.9%,\nZ69%, Z91.4%\n524\n35/43\n"}, {"page": 36, "text": "Service Utilization Predictors\nPredictor\nStop Codes\nOutpatient Visits: Primary care\n170, 171, 172, 301, 318, 322, 323, 338, 348, 350, 704\nOutpatient Visits: Mental health\n156, 157, 292, 464, 502, 509, 510, 516, 524, 525, 527, 531, 533, 534, 535, 536,\n538, 539, 542, 546, 547, 550, 552, 561, 562, 564, 565, 566, 567, 568, 571, 572,\n573, 574, 575, 576, 577, 579, 582, 583, 584, 586, 587, 593, 596, 597, 598, 599,\n713\nOutpatient Visits: Substance abuse\n513, 514, 519, 523, 545, 547, 548, 560, 586, 587, 593, 596, 597, 598, 599, 706,\n707, 721, 722, 723, 724\nOutpatient Visits: Specialty care\n120, 290, 324, 331, 336, 143, 231, 293, 302, 303, 304, 305, 306, 307, 308, 309,\n310, 311, 312, 313, 314, 315, 316, 317, 321, 325, 327, 329, 330, 333, 334, 335,\n337, 339, 340, 344, 345, 346, 349, 356, 369, 391, 392, 394, 420, 602, 603, 604,\n606, 607, 608, 611, 118, 119, 121, 173, 174, 175, 176, 177, 178, 190, 191, 319,\n326, 347, 351, 352, 353, 354, 658, 680, 682, 291, 401, 402, 403, 404, 405, 406,\n407, 408, 409, 410, 411, 413, 414, 415, 418, 419, 424, 427, 428, 429, 430, 432,\n434, 435, 441, 486, 487, 488, 489, 718\nOutpatient Visits: Diagnostic / Ancillary\n192, 651, 674, 669, 103, 123, 124, 125, 139, 142, 147, 159, 160, 166, 167, 169,\n180, 181, 182, 328, 332, 372, 373, 436, 683, 685, 686, 104, 105, 106, 107, 108,\n109, 110, 111, 115, 116, 126, 128, 144, 145, 146, 148, 149, 150, 151, 155, 158,\n212, 421, 703\nOutpatient Visits: Rehabilitation\n195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n213, 214, 215, 216, 217, 218, 220, 221, 222, 224, 225, 229, 230, 240, 241, 250,\n417, 423, 425, 437, 438, 439\nEmergency / Urgent-care\n130, 131\nInpatient Visits: Total\n25, 26, 33, 38, 39, 45, 68, 70, 71, 75, 76, 77, 79, 88, 89, 91, 92, 93, 94, 109, 110,\n27, 29, 72, 73, 74, 84, 86, 90, 111, 1-19, 24, 30, 31, 34, 83, 48-63, 65, 78, 97,\n106, 107, 108, 32, 34, 40, 42, 43, 44, 46, 47, 64, 66, 67, 68, 69, 80, 81, 95, 96,\n100, 101, 102, 103, 104, 105, 20, 35, 41, 82, 21, 36, 112, 22, 23, 85\n36/43\n"}, {"page": 37, "text": "Supplementary Table 7. Data-split characteristics across prediction windows\n*Training set values shown before stratified downsampling. The training set was downsampled to balance classes (matched on\ngender, age group, and race). validation and test sets retained original prevalence distribution. Data was split at the patient level\nto prevent data leakage.\nPrediction Window\nData Split\nN\nN (%)\nEvents (n)\n3 Months\nTraining*\n3,934,290\n92%\n12,630\nValidation\n128,292\n3%\n412\nTest\n213,820\n5%\n686\n6 Months\nTraining*\n3,934,290\n92%\n24,672\nValidation\n128,292\n3%\n805\nTest\n213,820\n5%\n1,341\n9 Months\nTraining*\n3,934,290\n92%\n36,266\nValidation\n128,292\n3%\n1,183\nTest\n213,820\n5%\n1,971\n12 Months\nTraining*\n3,934,290\n92%\n46,921\nValidation\n128,292\n3%\n1,531\nTest\n213,820\n5%\n2,550\n37/43\n"}, {"page": 38, "text": "Supplementary Note 2. Model hyperparameters and training configuration\nComputational Resources\nAll language models (ModernBERT, Bioclinical ModernBERT, Llama-3.1-8B, OpenBio) were trained on NVIDIA A100 80GB\nGPUs. Training did not utilize the full server capacity, with resources allocated as needed for each model’s requirements.\nLogistic regression (elastic net)\nWe fit an elastic net logistic regression model that combines L1 and L2 penalties. We used randomized search with 100\niterations. The search varied the inverse regularization strength C over the values [0.03, 0.1, 0.3, 1, 3, 10, and 30], and varied\nthe elastic net mixing parameter (l1_ratio) over [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, and 1.0]. The maximum number of iterations was\nallowed to be [2,000, 4,000, or 6,000]. We used the SAGA solver with an elastic net penalty.\nRandom forest\nFor the random forest model, we used randomized search with 100 iterations. The search space included the number of\ntrees (n_estimators) with candidate values of [400, 800, 1,200, and 1,600]. We also varied maximum tree depth [None, 10,\n20, 40, 80], the minimum number of samples required to split a node (min_samples_split) [2, 5, 10, 20, 50], the minimum\nnumber of samples required at a leaf node (min_samples_leaf) [1, 2, 5, 10, 20], the number of features considered at each\nsplit (max_features) [sqrt, log2, 0.2, 0.4, 0.6, 0.8], the complexity parameter (ccp_alpha) [0.0, 1e-4, 1e-3], and the fraction of\nsamples used per tree (max_samples) [None, 0.5, 0.8]. Bootstrap sampling was enabled.\nXGBoost\nFor gradient boosted trees (XGBoost), we used randomized search with 100 iterations. We varied the number of boosting\nrounds (n_estimators) with candidate values of [300, 600, 1,000, and 1,500]. Other tuned hyperparameters included maximum\ntree depth [3, 6, 9, 12, 15], learning rate [0.01, 0.05, 0.1, 0.2], row subsampling ratio (subsample) [0.6, 0.8, 0.9, 1.0], feature\nsubsampling ratio (colsample_bytree) [0.6, 0.8, 0.9, 1.0], L1 regularization (reg_alpha) [0, 0.1, 1, 10], L2 regularization\n(reg_lambda) [0, 0.1, 1, 10], minimum child weight [1, 3, 5, 7], and minimum loss reduction (gamma) [0, 0.1, 0.5, 1]. The\nobjective was binary logistic regression, and we used the \"hist\" tree method for efficiency.\nModernBERT\nWe fine-tuned the ModernBERT-large model as a sequence classification model using a fixed configuration without hyperpa-\nrameter search. We trained for 10 epochs with a learning rate of 0.00005 (5e-5). We used a batch size of 16 with gradient\naccumulation of 2, resulting in an effective batch size of 32. Optimization used the AdamW optimizer with weight decay of\n0.05, cosine learning rate scheduling, and a warmup ratio of 0.1. We set the maximum sequence length to 512 tokens. Training\nwas performed in mixed fp16 precision without quantization. Early stopping was applied based on precision-recall area under\nthe curve (PR-AUC) on the validation set, with a patience of 20 evaluation intervals. A standard classification head was added\non top of the encoder\nBioClinical-ModernBERT\nWe also fine-tuned a clinically pre-trained variant, BioClinical-ModernBERT-large. All training hyperparameters and settings\nwere identical to those used for ModernBERT, including optimizer, learning rate, weight decay, number of epochs, batch size,\ngradient accumulation, scheduler, sequence length, precision, and early stopping strategy.\nLlama-3.1-8B\nWe fine-tuned the Meta-Llama-3.1-8B base model using Low-Rank Adaptation (LoRA) with a fixed configuration and no\nhyperparameter search. The LoRA rank was 16, the LoRA alpha parameter was 32, and the LoRA dropout rate was 0.1. LoRA\nadapters were applied to the attention and feed-forward projection modules, including query, key, value, output, and intermediate\nprojections (q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, gate_proj), as well as to the language modeling head (lm_head).\nWe trained for 10 epochs with a learning rate of 0.0001 (1e-4) and an effective batch size of 64 (batch size of 4 multiplied by\ngradient accumulation steps of 16). We used the AdamW optimizer in its fused implementation (adamw_torch_fused), with\na linear learning rate schedule, a warmup ratio of 0.03, a maximum gradient norm of 1.0, and a maximum sequence length\nof 512 tokens. Training used mixed bfloat16 precision without additional quantization. Gradient checkpointing was enabled\nto reduce memory usage. Early stopping was based on validation PR-AUC with a patience of 10 evaluation intervals. The\nlanguage modeling head was fully fine-tuned together with the LoRA adapters. Training uses answer-only loss calculation,\nwhere only the final predicted token contributes to the loss, focusing the model on the classification task.\nOpenBio LLM\nWe also fine-tuned a clinically oriented model, OpenBioLLM-8B, using the same LoRA and training configuration as for\nLlama-3.1-8B. Keeping these settings identical ensured comparability between the two large language models.\n38/43\n"}, {"page": 39, "text": "Supplementary Note 3. Illustrative example of EHR representation and model inputs\nIllustrative example of how a single patient’s electronic health record (EHR) data are transformed into the representations used\nby our machine learning (ML), masked language model (MLM), and large language model (LLM) approaches. The schematic\nversion of this pipeline appears in Figure 1 of the main text. The example is purely illustrative and does not correspond to a real\npatient.\nStage 1: Raw longitudinal EHR table\nConsider a hypothetical patient (Patient 001) with three visits during the observation year 2016. For simplicity, we show only\nage, gender, three diagnosis groups (Anxiety disorder, Cancer, Influenza), and an indicator for legal problems. The first two\nvisits (February and April) fall in the first time interval (H1; e.g., January-June 2016), and the September visit falls in the\nsecond interval (H2; e.g., July-December 2016).\nFor Patient 001 (Age 32, Male), the visit-level records are:\nVisit Date\nAge\nGender\nAnxiety\nDisorder\nCancer\nInfluenza\nLegal\nProblems\n10/02/16\n32\nM\n1\n0\n1\n1\n22/04/16\n32\nM\n1\n0\n0\n0\n15/09/16\n32\nM\n0\n1\n0\n0\nStage 2: Longitudinal representation without condition persistence rules\nWe first aggregate visit-level information within each interval. A value of 1 indicates at least one occurrence of the diag-\nnosis/event in that period. This preserves temporal ordering (H1 vs H2) but does not yet incorporate condition persistence\nrules.\nVariable\nH1\n(2016 Jan - Jun)\nH2\n(2016 Jul - Dec)\nAnxiety Disorder\n1\n0\nCancer\n0\n1\nInfluenza\n1\n0\nLegal Problems\n1\n0\nStage 3: Applying the condition persistence framework\nAs described in Supplementary Table 9, each predictor is assigned a condition persistence rule (e.g., ever-history, recurrent\ntime-limited, episodic). In this toy example:\n• Anxiety disorder: recurrent time-limited\n• Legal problems: recurrent time-limited\n• Cancer: ever-history\n• Influenza: episodic\nBoth Anxiety disorder and Legal problems use a recurrent time-limited rule with persistence for T = 2 time intervals. Cancer\nuses an ever-history rule, and Influenza has no persistence beyond the interval in which it is recorded.\nApplying these rules to the interval-level representation yields (with * indicating values updated by persistence):\n39/43\n"}, {"page": 40, "text": "Variable\nH1\n(2016 Jan - Jun)\nH2\n(2016 Jul - Dec)\nAnxiety Disorder\n1\n1*\nCancer\n1*\n1\nInfluenza\n1\n0\nLegal Problems\n1\n1*\nIn the actual pipeline, these condition persistence rules are applied at the end of the observation window to construct features\nfor prediction and do not introduce information from future time periods into earlier prediction windows.\nStage 4: Construction of ML feature representation\nFor Elastic Net Logistic Regression, random forest, and related tabular models, we collapse the longitudinal information into a\nsingle row per patient at the end of the observation window, using interval-specific indicators (e.g., anxiety_H1, anxiety_H2).\nThe full models use a much richer feature set spanning demographics, mental and physical health, substance use, social factors\nof health, and utilization. A simplified subset of the resulting feature vector for Patient 001 is:\nFeature\nValue\nage_30_39\n1\nsex_male\n1\nanxiety_H1\n1\nanxiety_H2\n1\ncancer_H1\n1\ncancer_H2\n1\ninfluenza_H1\n1\ninfluenza_H2\n0\nlegal_problems_H1\n1\nlegal_problems_H2\n1\nStage 5: Text Prompt for MLM and LLMs\nThe same patient profile is converted into a compact natural language prompt. For this toy example (3-month prediction\nwindow), the prompt is:\nPatient Prediction Task: 3-Month Homelessness Risk\nPatient Information:\ndemographics:\n{Gender:\nmale, Age:\n32};\nmental_health_disorders:\n{H1:\nAnxiety disorder; H2:\nAnxiety disorder};\nphysical_health:\n{H1:\nInfluenza; H2:\nCancer};\nsocial_and_behavioral_factors:\n{H1-H2:\nLegal problems}.\nTask: Given the patient information, predict yes if this patient will be homeless in the next 3 months, no otherwise.\nThe prompt structure follows this template, where features are organized by clinical domain and time periods:\n40/43\n"}, {"page": 41, "text": "Patient Information:\n[domain1]:\n{[static features] or {[time_period]:\n[features]}};\n[domain2]:\n{[time_period]:\n[features]}; ...\nGiven the patient information, predict\nyes if this patient will be homeless in the next [M] months, no otherwise.\nWhere:\n• [domain] represents one of: demographics, utilization, mental_health_disorders, physical_health, substance_abuse, military_-\nhistory, or social_and_behavioral_factors\n• [time_period] is one of: Q1, Q2, Q3, Q4 (for quarterly aggregation) or H1, H2 (for half-year aggregation)\n• [M] is the prediction window: 3 months, 6 months, 9 months, or 12 months\n• Static features (e.g., demographics) appear as {Feature: value, Feature: value}\n• Time-varying features appear as {Q1: feature1, feature2; Q2: feature3} with features listed within each time period\n• Only present features are included (absent features are omitted)\n• Count/utilization features include their values (e.g., Primary care visits: 3), while binary features appear as names only\nwhen present\n41/43\n"}, {"page": 42, "text": "Supplementary Table 8. Selected aggregation level for time-varying representation\nTemporal aggregation granularity (quarterly vs half-year) for the time-varying representation was treated as a hyperparameter\nand selected using validation PR-AUC within each model and prediction horizon. The table reports the selected aggregation\nlevel.\nModel Class\nModel\n3 Months\n6 Months\n9 Months\n12 Months\nLarge Language\nModels\nLlama-3.1-8B\nQuarterly\nHalf-Year\nQuarterly\nQuarterly\nOpenBioLLM-8B\nHalf-Year\nQuarterly\nHalf-Year\nQuarterly\nMachine Learning\nElastic Net LR\nQuarterly\nQuarterly\nHalf-Year\nQuarterly\nRandom Forest\nQuarterly\nHalf-Year\nHalf-Year\nQuarterly\nXGBoost\nQuarterly\nQuarterly\nQuarterly\nQuarterly\nMasked Language\nModels\nBioClinical ModernBERT\nQuarterly\nHalf-Year\nQuarterly\nQuarterly\nModernBERT\nQuarterly\nHalf-Year\nQuarterly\nQuarterly\n42/43\n"}, {"page": 43, "text": "Supplementary Table 9. Fill strategy rules used to construct time-varying predictors\nPredictor Type\nPredictor\nMode\nQuarters\nMental Health\nAnxiety Disorder\nRecurrent Time-Limited\n2\nBipolar Disorder\nChronic Persistent\nDementia\nChronic Persistent\nDepression\nRecurrent Time-Limited\n2\nOther Neurological Disorders\nChronic Persistent\nPosttraumatic Stress Disorder\nRecurrent Time-Limited\n2\nPsychoses\nChronic Persistent\nSleep Disorder\nRecurrent Time-Limited\n2\nPhysical Health\nAIDS/HIV\nChronic Persistent\nBlood Loss Anemia\nRecurrent Time-Limited\n2\nCardiac Arrhythmia\nChronic Persistent\nCardiovascular Disease\nChronic Persistent\nChronic Pulmonary Disease\nChronic Persistent\nCirrhosis\nChronic Persistent\nCoagulopathy\nChronic Persistent\nCongestive Heart Failure\nChronic Persistent\nDeficiency Anemia\nRecurrent Time-Limited\n2\nDiabetes\nChronic Persistent\nFluid And Electrolyte Disorders\nEpisodic\nHepatitis\nRecurrent Time-Limited\n2\nHypertension\nChronic Persistent\nHypothyroidism\nChronic Persistent\nInfluenza\nEpisodic\nLiver Disease\nChronic Persistent\nLymphoma\nEver-History\nMetastatic Cancer\nEver-History\nObesity\nChronic Persistent\nPain\nRecurrent Time-Limited\n1\nParalysis\nChronic Persistent\nPeptic Ulcer Disease\nRecurrent Time-Limited\n2\nPeripheral Vascular Disorders\nChronic Persistent\nPulmonary Circulation Disorders\nRecurrent Time-Limited\n2\nRenal Failure\nChronic Persistent\nRheumatoid Arthritis/Collagen\nChronic Persistent\nSolid Tumor Without Metastasis\nEver-History\nTraumatic Brain Injury\nRecurrent Time-Limited\n2\nValvular Disease\nChronic Persistent\nWeight Loss\nRecurrent Time-Limited\n2\nSubstance Abuse\nAlcohol Use Disorder\nChronic Persistent\nCannabis\nRecurrent Time-Limited\n2\nCocaine\nRecurrent Time-Limited\n2\nDrug Abuse\nRecurrent Time-Limited\n2\nHallucinogen\nRecurrent Time-Limited\n2\nNicotine Dependence\nChronic Persistent\nOpioid Use Disorder\nChronic Persistent\nOther Stimulant\nRecurrent Time-Limited\n2\nSDOH Factors\nEmployment Or Financial Problems\nRecurrent Time-Limited\n2\nFood Insecurity\nRecurrent Time-Limited\n2\nHousing Problems\nRecurrent Time-Limited\n2\nLegal Problems\nRecurrent Time-Limited\n2\nNon Specific Psychosocial Needs\nRecurrent Time-Limited\n2\nSocial Or Familial Problems\nRecurrent Time-Limited\n2\nViolence Problems\nEpisodic\n43/43\n"}]}