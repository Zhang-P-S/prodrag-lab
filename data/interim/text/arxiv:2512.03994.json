{"doc_id": "arxiv:2512.03994", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.03994.pdf", "meta": {"doc_id": "arxiv:2512.03994", "source": "arxiv", "arxiv_id": "2512.03994", "title": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs", "authors": ["Oren Rachmil", "Avishag Shapira", "Roy Betser", "Itay Gershon", "Omer Hofman", "Asaf Shabtai", "Yuval Elovici", "Roman Vainshtein"], "published": "2025-12-03T17:23:39Z", "updated": "2026-01-18T07:49:40Z", "summary": "As organizations increasingly deploy LLMs in sensitive domains such as legal, financial, and medical settings, ensuring alignment with internal organizational policies has become a priority. Existing content moderation frameworks remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and training cost. To address these limitations, we frame policy violation detection as an out-of-distribution (OOD) problem in the model's activation space. We propose a training-free method that operates directly on the LLM internal representations, leveraging prior evidence that decision-relevant information is encoded within them. Inspired by whitening techniques, we apply a linear transformation to decorrelate and standardize the model's hidden activations, and use the Euclidean norm in this transformed space as a compliance score for detecting policy violations. Our method requires only the policy text and a small number of illustrative samples, making it lightweight and easily deployable. We extensively evaluate our method across multiple LLMs and challenging policy benchmarks, achieving 86.0% F1 score while outperforming fine-tuned baselines by up to 9.1 points and LLM-as-a-judge by 16 points, with significantly lower computational cost. Code is available at: https://github.com/FujitsuResearch/LLM-policy-violation-detection", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.03994v3", "url_pdf": "https://arxiv.org/pdf/2512.03994.pdf", "meta_path": "data/raw/arxiv/meta/2512.03994.json", "sha256": "e4f0e871259e298927696e09e315412a8c3ec8cf2f36374b3f8f90d06e83c298", "status": "ok", "fetched_at": "2026-02-18T02:25:29.933970+00:00"}, "pages": [{"page": 1, "text": "Training-Free Policy Violation Detection via\nActivation-Space Whitening in LLMs\nOren Rachmil1*†, Avishag Shapira2†, Roy Betser1, Itay Gershon1,\nOmer Hofman1, Asaf Shabtai2, Yuval Elovici2, Roman Vainshtein1\n1Fujitsu Research of Europe\n2Ben-Gurion University of the Negev\nAbstract\nAs organizations increasingly deploy LLMs\nin sensitive domains such as legal, financial,\nand medical settings, ensuring alignment with\ninternal organizational policies has become a\npriority. Existing content moderation frame-\nworks remain largely confined to the safety\ndomain and lack the robustness to capture nu-\nanced organizational policies. LLM-as-a-judge\nand fine-tuning approaches, though flexible, in-\ntroduce significant latency and training cost.\nTo address these limitations, we frame pol-\nicy violation detection as an out-of-distribution\n(OOD) problem in the model’s activation space.\nWe propose a training-free method that oper-\nates directly on the LLM internal representa-\ntions, leveraging prior evidence that decision-\nrelevant information is encoded within them.\nInspired by whitening techniques, we apply a\nlinear transformation to decorrelate and stan-\ndardize the model’s hidden activations, and use\nthe Euclidean norm in this transformed space\nas a compliance score for detecting policy vi-\nolations. Our method requires only the policy\ntext and a small number of illustrative samples,\nmaking it lightweight and easily deployable.\nWe extensively evaluate our method across mul-\ntiple LLMs and challenging policy benchmarks,\nachieving 86.0% F1 score while outperform-\ning fine-tuned baselines by up to 9.1 points\nand LLM-as-a-judge by 16 points, with sig-\nnificantly lower computational cost. Code is\navailable at: Policy Violation Detection\n1\nIntroduction\nLarge language models (LLMs) are increasingly\nbeing adopted across organizations, where they\nare integrated into applications such as document\ndrafting, automated customer support, and data\nanalysis pipelines (Cohere, 2023; Urlana et al.,\n*Corresponding author: rachmiloren@gmail.com.\n†Oren Rachmil and Avishag Shapira contributed equally\nto this work.\n2024; Liang et al., 2025). As this adoption ac-\ncelerates, organizations face the critical challenge\nof ensuring that LLMs comply with both internal\norganizational policies and external regulatory and\ncompliance requirements across diverse domains\n(e.g., legal, financial, ethical, medical) (Liu et al.,\n2023; RTCA and EUROCAE, 2011). In enter-\nprise settings, policy compliance rarely involves\none single rule: models must simultaneously sat-\nisfy dozens of policies, each of which may consist\nof hundreds of rules. Each rule introduces new\ncontextual conditions, linguistic subtleties, and ex-\nceptions (Saura et al., 2025; Hoover et al., 2025).\nEven high-performing LLMs can inadvertently vi-\nolate organizational policies, creating substantial\nlegal and financial risks (OpenAI, 2024; Zeng et al.,\n2024; Brokman et al., 2025; Federal Trade Com-\nmission, 2025).\nTo promote safe and compliant behavior, a wide\nrange of guardrail mechanisms have been proposed\nto constrain LLM outputs (Inan et al., 2023; Rebe-\ndea et al., 2023; Yuan et al., 2024; Kang and Li,\n2025). These mechanisms typically enforce pre-\ndefined categories, handcrafted rules, or post-hoc\nclassifiers over generated text. While effective for\ncoarse-grained safety objectives, they do not scale\nwell to complex organizational policies with nu-\nmerous and context-sensitive rules. Consequently,\norganizations increasingly rely on LLM-based eval-\nuators (“LLM-as-a-judge”) (Gu et al., 2024) or fine-\ntuned compliance detectors (Hoover et al., 2025).\nHowever, these approaches incur substantial train-\ning and inference costs, introducing non-trivial\nlatency, limiting real-time monitoring and large-\nscale deployment. More fundamentally, these ap-\nproaches assess compliance only at the level of gen-\nerated text, leaving open the question of whether\npolicy adherence is more directly reflected in the\nmodel’s internal decision process.\nRecent studies show that the internal states of\nLLMs encode information about output correctness\narXiv:2512.03994v3  [cs.LG]  18 Jan 2026\n"}, {"page": 2, "text": "Figure 1: Policy-violation detection framework. Organizational policies define expected behavior. Given a\nuser query and model response, hidden activations (obtained via a surrogate model) are whitened using in-policy\nstatistics, and the activation norm is compared to a calibrated threshold to flag policy violations.\nthat is not fully reflected in the generated tokens,\nand these latent signals can be leveraged for er-\nror detection (Zou et al., 2023; Orgad et al., 2024;\nGekhman et al., 2025). This observation motivates\nmonitoring compliance directly in the model’s acti-\nvation space, rather than relying solely on output-\nlevel judgments (Cao et al., 2025; Han et al., 2025).\nHowever, extending this to detect policy violations\nis challenging because institutional policies are of-\nten nuanced and context-dependent, making it im-\npractical to explicitly define every possible viola-\ntion. We propose a scalable alternative that models\nthe distribution of in-policy behaviors and identi-\nfies violations as Out-of-Distribution (OOD) events.\nWe hypothesize that policy-violating states occupy\ndistinct regions within the LLM embedding space,\nallowing for robust detection through OOD framing\nwithout the need for exhaustive negative labeling.\nInspired by whitening-based likelihood estima-\ntion from the image domain (Betser et al., 2025),\nwe model activations from policy-compliant user-\nLLM interactions as an in-distribution manifold\nand treat policy violations as OOD deviations in\nthis space. Conditioned on the organization’s pol-\nicy rules, we analyze hidden states across trans-\nformer layers to assign a compliance score to each\ninteraction. We fit a data-driven whitening trans-\nform to in-policy activations, producing standard-\nized features with approximately identity covari-\nance; in this whitened space, policy compliance\nis scored by the Euclidean norm of the whitened\nactivation vector. At runtime, we compute a com-\npliance score and compare it to a pre-defined cal-\nibrated threshold, enabling detection of out-of-\npolicy responses. The result is a flexible, low-\noverhead solution suited for continuous policy up-\ndates and monitoring. Figure 1 illustrates the over-\nall framework, where in-policy activations cluster\nnear the origin in the whitened space, while policy-\nviolating activations shift outward and are detected\nvia a calibrated norm threshold.\nWe evaluate our method across multiple\nLLMs and two challenging benchmarks, Dyn-\naBench (Hoover et al., 2025) and τ-bench (Yao\net al., 2024), against a range of baseline approaches.\nOur results show that internal activation-based sig-\nnals provide a reliable basis for policy compliance\ndetection. The proposed method achieves up to\n86.0% F1, outperforming fine-tuned baselines by\nup to 9.1 points and LLM-as-a-judge methods by\nup to 16 points, while requiring substantially lower\ncomputational cost.\nOur contributions can be summarized as follows:\n• Activation-space policy compliance fram-\ning. We formulate policy violation detection\nas an OOD problem in the LLM activation\nspace, moving beyond output-level compli-\nance judgments.\n• Whitening-based compliance scoring. We\npropose a training-free method that whitens\nin-policy activations and scores compliance\nby their Euclidean norm.\n• Scalable and low-overhead detection. The\nproposed approach scales to large policy sets,\nintroducing minimal inference overhead and\nsupports continuous policy updates in real-\ntime enterprise settings.\n• Comprehensive empirical evaluation. We\nevaluate our approach across multiple LLMs\non DynaBench and real-world airline trajecto-\nries from TauBench, demonstrating consistent\nimprovements over LLM-as-a-judge methods,\nfine-tuned compliance detectors, and competi-\ntive OOD baselines.\n"}, {"page": 3, "text": "Figure 2: Offline compliance calibration. In-policy interactions are used to estimate a whitening transform over\nlast-token activations, while in- and out-of-policy samples are used to calibrate a decision threshold.\n2\nBackground and Related Work\nThe widespread deployment of LLMs has acceler-\nated research on guardrail and content moderation\nsystems. Early safety-oriented models such as Lla-\nmaGuard (Inan et al., 2023), along with subsequent\nwork on ensemble moderation, lightweight archi-\ntectures, and generative pipelines (Han et al., 2024;\nDatta and Rajasekar, 2025; Ghosh et al., 2024;\nZeng et al., 2024; Ganon et al., 2025), focus on\ndetecting harmful content in inputs and outputs.\nHowever, these methods rely on predefined safety\ntaxonomies and are optimized for narrow risk cate-\ngories such as toxicity or bias. As a result, they do\nnot readily generalize to custom organizational poli-\ncies, which are often contextual, domain-specific,\nand frequently evolving.\nSeveral recent works have explored policy en-\nforcement using LLMs beyond safety moderation.\nSaura et al. (2025) propose retrieval-augmented pol-\nicy enforcement, while Wang et al. (2025); Chen\net al. (2025) investigate privacy-policy compliance\nthrough fine-tuned or prompt-based LLM classi-\nfiers. However, these approaches rely on super-\nvised fine-tuning or LLM-as-a-judge inference, in-\ncurring high latency and computational cost.\nA recent benchmark for policy alignment is\nDynaBench (Hoover et al., 2025), which evalu-\nates policy compliance in multi-turn user-agent\ninteractions under complex organizational poli-\ncies. Building on this benchmark, the authors in-\ntroduced DynaGuard, a collection of fine-tuned\nmodels for policy-violation detection. While Dyna-\nGuard achieves strong performance on DynaBench,\nit relies on supervised fine-tuning and large curated\ndatasets, limiting adaptability in deployment.\nA complementary line of research focuses on\nOOD detection, which aims to improve model re-\nliability by identifying inputs that deviate from\nthe training distribution (Hendrycks and Gimpel,\n2016). Originally developed for image classifi-\ncation (Liang et al., 2017; Lee et al., 2018; Hsu\net al., 2020; Chen et al., 2021; Sun et al., 2022),\nOOD methods, such as Mahalanobis distance,\nenergy-based scores, and nearest neighbors, have\nbeen extended to textual and multimodal mod-\nels (Hendrycks et al., 2020; Zhou et al., 2021;\nChen et al., 2023a). Prior work shows that OOD\ninstances can be effectively identified using inter-\nmediate representations of pretrained Transform-\ners (Zhou et al., 2021), and that whitening hidden\nrepresentations enables likelihood-based detection\nwithout retraining (Chen et al., 2023b; Betser et al.,\n2025). We adapt whitening-based OOD scoring\nto LLM activation spaces for policy compliance\ndetection.\nRecent representation analysis shows that inter-\nnal LLM activations encode high-level behavioral\nproperties, including task adherence and constraint\nsatisfaction (Zou et al., 2023; Orgad et al., 2024;\nGekhman et al., 2025). These findings suggest that\npolicy compliance is reflected in internal represen-\ntations, even when generated responses vary. Ac-\ncordingly, policy violations can be viewed as devia-\ntions in the activation space, linking representation-\nlevel analysis with OOD detection for practical AI\ngovernance.\n3\nMethod\nWe frame policy violation detection as an OOD\nproblem in an LLM’s activation space, based on the\nhypothesis that policy-compliant responses form\na consistent activation region while violations de-\nviate from it in the hidden space. Our method\nconsists of an offline reference construction and\nan online detection phase. It can be applied using\neither surrogate-model activations or direct model\naccess.\n3.1\nOffline reference statistics pre-processing\nFrom in-policy activations at each transformer\nlayer, we estimate a whitening transform that stan-\n"}, {"page": 4, "text": "Figure 3: Online compliance detection. At runtime, last-token activations are whitened using the precomputed\ntransform, scored, and compared to a calibrated threshold for policy classification.\ndardizes the activation space for norm-based scor-\ning (Fig. 2). A small mixed set of in- and out-\nof-policy samples is then used to select an opera-\ntional layer and calibrate a decision threshold. Only\nthe selected layer’s transform and threshold are re-\ntained for deployment.\nWhitening transformation. To compute compli-\nance scores, we standardize the activation space us-\ning a whitening transform so that all dimensions are\ncomparable. Let {x(ℓ)\ni }N\ni=1, x(ℓ)\ni\n∈Rd, denote acti-\nvation vectors extracted from layer ℓover a repre-\nsentative set of in-policy interactions. We compute\nthe empirical mean µ(ℓ) = 1\nN\nPN\ni=1 x(ℓ)\ni\nand covari-\nance Σ(ℓ) =\n1\nN−1\nPN\ni=1(x(ℓ)\ni\n−µ(ℓ))(x(ℓ)\ni\n−µ(ℓ))⊤.\nWhitening applies a linear transform that centers\nand decorrelates activations, mapping in-policy re-\nsponses to a space with approximately zero mean\nand unit covariance. Given the empirical mean µ(ℓ)\nand covariance Σ(ℓ), we define a whitening matrix\nW (ℓ) satisfying:\nW (ℓ)⊤W (ℓ) =\n\u0000Σ(ℓ)\u0001−1.\n(1)\nWhile the whitening transform is not unique, we\ncompute W (ℓ) via PCA-based whitening on the\nin-policy reference set. Applying this transform\nyields the standardized representation:\ny(ℓ) = W (ℓ)\u0000x(ℓ) −µ(ℓ)\u0001\n,\n(2)\nin which deviations from in-policy behavior can be\nmeasured uniformly across dimensions.\nScore definition. After whitening (and dimension-\nality reduction to the top-K components), devia-\ntions from in-policy behavior are quantified by the\nEuclidean norm:\ns(ℓ) =\n\r\ry(ℓ)\r\r\n2,\n(3)\nwhich measures the distance of a response activa-\ntion from the in-policy region. This score is equiv-\nalent to the Mahalanobis distance in the original\nspace, but emphasizes the dominant directions of\nin-policy variability (see Appendix A for details).\nLayer selection. Whitening parameters are com-\nputed independently per layer from in-policy acti-\nvations. A small mixed set of in- and out-of-policy\nsamples is used to assess layer-wise separation, and\nthe best-performing layer is selected as the opera-\ntional layer ℓ⋆. Only its corresponding parameters\nare retained for deployment.\nThreshold calibration.\nGiven the operational\nlayer ℓ⋆, we calibrate a decision threshold τ on\nthe held-out mixed set with ground-truth compli-\nance labels. The threshold is chosen to maximize\nYouden’s statistic (J = TPR −FPR), yielding a\nsingle operating point that separates in-policy from\nout-of-policy activations without additional tuning.\nPolicy-conditioned whitening.\nWe extend the\nmethod to class-conditioned whitening, where poli-\ncies are grouped into classes with shared behav-\nioral patterns. For each class, we estimate in-policy\nstatistics and derive a class-specific whitening trans-\nform together with its class mean and operational\nlayer. The resulting per-class parameters are stored\nfor later selection during online detection. In prac-\ntice, we find that very small calibration sets, often\non the order of one sample per policy rule, are suf-\nficient and even fewer may be needed. This keeps\nthe procedure lightweight and at a data scale where\nretraining or fine-tuning would not be beneficial.\n3.2\nOnline detection pipeline\nAt runtime, each response is validated before be-\ning returned (see Fig. 3). The activation at the\noperational layer ℓ⋆is processed to obtain the com-\npliance score s(ℓ⋆) (Eqs. 2, 3). The compliance\ndecision is then given by\nˆy = I\nh\ns(ℓ⋆) > τ\ni\n,\n(4)\nwhere ˆy = 1 indicates an out-of-policy response\nand ˆy = 0 indicates an in-policy response. When\npolicy grouping is used, the closest policy class\n"}, {"page": 5, "text": "mean (by cosine similarity) is selected and its pa-\nrameters are used to compute the compliance score\nand decision.\n4\nEvaluation\n4.1\nDataset & Contrastive Data Construction\nBenchmarks.\nWe evaluate our method on the\nDynaBench benchmark (Hoover et al., 2025), using\nits manually curated test split to assess policy com-\npliance in multi-turn user–agent dialogues. Each\nsample includes a policy defined as a set of one or\nmore textual rules that the model must follow, and\na dialogue labeled as either in-policy (compliant) or\nout-of-policy (violating): see dialogue example in\nAppendix B. The test set spans twelve business im-\npact categories, covering diverse domains such as\nregulatory compliance, information leakage, user\nexperience, and brand reputation (see Appendix C\nfor detailed statistics). We additionally evaluate\ngeneralization to a distinct policy setting derived\nfrom τ-bench (Yao et al., 2024), which we describe\nand analyze separately in 4.3. Unless stated other-\nwise, all quantitative results and analyses in the fol-\nlowing sections are reported on DynaBench, which\nserves as our primary benchmark.\nContrastive Data Generation.\nTo construct rep-\nresentative data for computing the whitening ma-\ntrices and threshold calibration, we generate rule-\nspecific contrastive datasets for each DynaBench\npolicy. Each policy is decomposed into its con-\nstituent rules, and for each rule, an LLM-based\ngenerator (GPT-5.1) produces natural-language\nprompts that explicitly or implicitly query the rule.\nFor each prompt, the LLM synthesizes contrastive\nsample pairs in a realistic conversational style, each\nconsisting of a good response adhering to the rule\n(in-policy) and a bad response deliberately violat-\ning it (out-of-policy). An illustrative example is\nprovided in Appendix D.\nContrastive Data Validation.\nTo ensure data\nquality, we use an LLM-based validator (GPT-5.1)\nthat assesses whether a dialogue complies with a\ngiven policy, producing a binary in-policy or out-\nof-policy judgment. We first assess the validator’s\nreliability by measuring its agreement with ground-\ntruth labels on the DynaBench test set, then apply\nit to our generated contrastive data to verify con-\nsistency between the validator’s predictions and\nthe generation labels. High agreement in both set-\ntings confirms that our contrastive data reflects the\nsame policy-adherence patterns as the benchmark.\nDetailed metrics are reported in Appendix E.\n4.2\nEvaluation Settings\nImplementation\nDetails.\nWe\nevaluate\nour\nmethod on five popular open source models:\nMistral-7B-Instruct-v0.2, Llama-3.1-8B-instruct,\nGemma-2-9B-it,\nQwen3-8B and Qwen2.5-7B-\nInstruct,\nfrom\ntheir\nofficial\nHugging\nFace\nrepositories. For each of the twelve policy cate-\ngories in DynaBench, we sample 100 contrastive\nexamples, splitting them 80/20. Whitening is fit\nusing only in-policy samples from the 80% subset,\nwhile the remaining 20%, containing both in-\nand out-of-policy examples, is used for threshold\ncalibration. Unless stated otherwise, we retain\nthe top-k components (k = 15). Ablations over\nsample size and k are shown in Fig. 5.\nWe compare our method against two categories\nof baselines taken from the original DynaBench\nbenchmark: (1) LLM-as-a-judge models, which\nare prompted with the policy rules as system in-\nstructions, including GPT-4o-mini and Qwen3-\n8B. (2) Fine-tuned policy detectors, including\nDynaGuard-1.7B, DynaGuard-4B, DynaGuard-8B\n(with and without CoT), and LlamaGuard-3.\nCategory-Specific Whitening.\nFor each cate-\ngory, we perform per-layer whitening and select\nthe operational layer.\nOur analysis shows that\nwhitening produces approximately zero-mean, unit-\nvariance activations with near-identity covariance,\nwhereas raw activations do not; full plots are pro-\nvided in Appendix F. Category-specific whitening\nconsistently outperforms a single shared whitening\ntransform, with detailed results in Appendix G.\n4.3\nResults\nComparison Against Existing Baselines.\nTa-\nble 1 reports F1 scores for all competing baselines.\nDespite requiring no fine-tuning, our approach\nachieves state-of-the-art performance on four of\nthe five evaluated backbones, outperforming both\nLLM-as-a-judge baselines and fine-tuned models.\nThese results support framing policy compliance\ndetection as an out-of-distribution problem in the\nmodel’s activation space. Performance is strongest\nfor Qwen-based variants, reaching up to 86.0% F1.\nWhile performance on the Mistral-7B backbone is\nrelatively weaker, our method still surpasses sev-\neral fine-tuned models (e.g., LlamaGuard-3 and\nDynaGuard-1.7B) and LLM-as-a-judge (Qwen3-\n"}, {"page": 6, "text": "8B). We further analyze the relationship between\nmodel representation characteristics and method\neffectiveness in Section 5.\nApproach\nModel\nF1 (%)\nLLM-as-a-judge\nGPT-4o-mini\n70.1\nQwen3-8B\n60.7\nFine-tuned\nLlamaGuard-3\n20.9\nDynaGuard-1.7B\n65.2\nDynaGuard-4B\n72.0\nDynaGuard-8B (non-CoT)\n72.5\nDynaGuard-8B\n73.1\nOurs\nMistral-7B-Instruct-v0.2\n66.8\nGemma-2-9B-it\n75.2\nLlama-3.1-8b-instruct\n75.6\nQwen3-8B\n78.4\nQwen2.5-7B-Instruct\n86.0\nTable 1: Comparison against existing baselines on the\nDynaBench test set. Baseline results are taken from the\nDynaBench benchmark paper.\nBenefits of Representation-Level Analysis.\nWe\napply our whitening-based method to the hidden\nstates of the fine-tuned DynaGuard models. Un-\nlike the baselines that rely on final generated clas-\nsification tokens, our approach operates directly\non the models’ internal hidden-state representa-\ntions. As shown in Table 2, our representation-\nlevel approach consistently outperforms generation-\nbased classifiers, even when using identical under-\nlying models. For example, while DynaGuard-1.7B\nachieves an F1 score of 65.2% when using its native\ngeneration-based classifier, applying our whitening\nmethod to its hidden activations improves perfor-\nmance to 77.6%. This substantial gap indicates that\nthe model’s internal representations encode richer\npolicy-relevant information than what is surfaced\nthrough the final generated outputs. Furthermore,\ncomparing the DynaGuard-8B results to the base\nQwen3-8B model (see Table 1) reveals a negligi-\nble difference of only 2.2% in F1 score in favor\nof the fine-tuned model. This suggests that much\nof the knowledge required for policy adherence\nmay already be embedded within the base model’s\nweights, raising questions about the necessity of\nextensive fine-tuning.\nEvaluation on τ-bench.\nTo assess generaliza-\ntion beyond DynaBench, we evaluate our method\non an additional policy derived from τ-bench (Yao\net al., 2024), a benchmark for evaluating AI agents\nthrough tool-use correctness. We focus on the Air-\nF1(%)\nGeneration\nWhitening\nDynaGuard-1.7B\n65.2\n77.6\nDynaGuard-4B\n72.0\n78.5\nDynaGuard-8B\n73.1\n80.6\nTable 2: Comparison between generation-based and\nwhitening-based classifiers on DynaGuard models.\n(a) Synthetic data.\n(b) Trajectories.\nFigure 4: τ-bench policy compliance results. Com-\nparison on (a) real agent trajectories and (b) synthetic\nairline-domain dialogues.\nline domain, identified as the challenging domain\nout of the two in the benchmark, which defines\nstructured operational constraints such as flight\nbooking, cancellation, and refund rules. For the\nfollowing experiments, we compare the strongest\nrepresentatives from each baseline category: GPT-\n4o-mini (LLM-as-a-judge), DynaGuard-8B (fine-\ntuned), and whitening using Qwen2.5-7B-Instruct\n(our method).\nWe conduct two complementary experiments.\nFor both settings, we use GPT-5.1 to construct con-\ntrastive airline-domain data, used to compute the\nwhitening matrix and calibrate the decision thresh-\nold.\nSynthetic Policy-Adherence Dialogues. We\nconstruct an evaluation set of 1,000 airline-domain\ndialogues that explicitly comply with or violate\nthe specified rules. To increase data diversity, data\nis generated using two models (Gemini 2.0 and\nClaude Sonnet 4). As shown in Fig. 4a, our method\nsubstantially outperforms both the fine-tuned de-\ntector and the LLM-as-a-judge baseline in terms of\nF1 score. The results indicate that fine-tuned de-\ntectors exhibit limited generalization to unseen pol-\nicy domains, while LLM-as-a-judge methods show\nmore stable but moderate performance. In contrast,\nour approach maintains strong performance despite\nshifts in both policy structure and data distribution.\nReal τ-bench Agent Trajectories. We next\nevaluate our method on real historical agent tra-\njectories from τ-bench, which include structured\n"}, {"page": 7, "text": "tool calls, API responses, and multi-step reasoning\nchains. Since τ-bench primarily evaluates tool-use\ncorrectness rather than policy compliance, policy-\nviolating trajectories are few; only 25 violating\ntrajectories were identified, which we pair with an\nequal number of in-policy trajectories. Trajecto-\nries are validated using our LLM-based validator\n(Section 4.1) and confirmed through manual human\nverification.\nAs shown in Fig. 4b, the fine-tuned detector\nDynaGuard-8B shows limited generalization to real\nτ-bench trajectories, while the LLM-as-a-judge\n(GPT-4o-mini) remains more stable and attains\nthe highest F1 score under the default operating\npoint. Using the automatically calibrated thresh-\nold, our method achieves moderate F1 performance\nbut exhibits strong separation between compliant\nand violating trajectories, with a high AUC of 0.87\n(can be seen in Appendix H). This indicates that\nactivation-space separation generalizes across inter-\naction formats despite distributional and structural\ndifferences. By selecting an ideal decision thresh-\nold aligned with the trajectory distribution, our\nmethod achieves a substantially higher F1 score,\nsurpassing GPT-4o-mini. This behavior is typical\nof OOD detection, where score separation general-\nizes reliably; in our setting, an optimal threshold\ncan be obtained via lightweight recalibration using\na small number of representative samples.\nComparison with Additional OOD Methods.\nWe further evaluate our whitening-based approach\nagainst several established OOD detection meth-\nods, including Mahalanobis distance and Energy\nScore (Zhou et al., 2021), and k-nearest neighbors\n(KNN) (Sun et al., 2022). We evaluate all methods\nfollowing the same evaluation protocol as in our\nwhitening approach.\nTable 3 reports the results for the evaluated OOD\nmethods on Qwen2.5-7B and Llama-3.1-8B In-\nstruct models. Our whitening-based approach con-\nsistently achieves the highest performance, out-\nperforming the strongest baselines by 3.7% and\n2.2%, respectively. Interestingly, while Whitening\nyields the best overall results, classical methods\nlike KNN on Qwen2.5-7B (78.5%) and Energy\nScore on Llama-3.1-8B (72.1%) also surpass both\nLLM-as-a-judge and fine-tuned baselines. This un-\nderscores the inherent ability of pre-trained models\nto organize policy behaviors into separable regions\nof the embedding space, allowing classical detec-\ntors to perform competitively without specialized\ntraining. Despite the conceptual similarity between\nWhitening and Mahalanobis distance, the latter un-\nderperforms in this high-dimensional setting; we\nattribute this to its reliance on the full covariance\nmatrix, whereas our method utilizes dimensionality\nreduction to focus on the most informative direc-\ntions while capturing category-specific structures.\nOOD Method\nF1(%)\nQwen2.5-7B\nLlama-3.1-8B\nMahalanobis\n67.2\n65.8\nKNN\n78.5\n66.2\nEnergy Score\n66.4\n72.1\nWhitening (Ours)\n82.2\n74.3\nTable 3: OOD detection performance across various\nmethods.\nRuntime Analysis.\nWe evaluate inference effi-\nciency for DynaGuard-8B (strongest fine-tuned\nbaseline), GPT-4o-mini (LLM-as-a-judge), and our\nproposed method on Llama-3.1-8B and Qwen2.5-\n7B Instruct models. We tested our method under\ntwo deployment configurations: first, policy com-\npliance is evaluated using representations extracted\nfrom the same model that generated the dialogue.\nIn the second configuration, compliance is evalu-\nated using representations from a surrogate model,\nenabling deployment even when the generating\nmodel is inaccessible. Table 4 reports the mean\nruntime per test conversation over 100 samples\nfrom the DynaBench test set. When using inter-\nnal representations, our whitening-based detector\nadds only 0.03 - 0.05 seconds of overhead, which\nis negligible relative to model inference and fully\ncompatible with real-time moderation pipelines.\nWhen using a surrogate model, latency remains un-\nder one second, demonstrating that our approach\nremains efficient and practical even when applied\nas a post-hoc compliance monitor for third-party\nor API-based systems.\n4.4\nAblations\nAblation on Parameters.\nWe ablate the number\nof retained components (K) and the per-category\nsample size used for whitening and calibration on\nDynaBench using Llama 3.1-8b embeddings. As\nshown in Fig. 5, performance is stable across a wide\nrange of K values and improves only marginally\nwith larger calibration sets, indicating robustness\nand efficiency. Using 100 samples per category\nachieves an F1 score of 75.6%, while increasing to\n"}, {"page": 8, "text": "Category\nModel\nRuntime [s]\nLLM-as-a-judge\nGPT-4o-mini\n1.47\nFine-tuned detector\nDynaGuard-8B\n2.71\nSurrogate representations (ours)\nLlama-3.1-8B\n0.98\nQwen2.5-7B\n0.92\nSame representations (ours)\nLlama-3.1-8B\n0.05\nQwen2.5-7B\n0.03\nTable 4: Average runtime per sample (seconds) on the\nDynaBench test set.\n750 samples yields only a modest gain to 79.1%.\nVarying k from 10 to 50 results in minor fluctua-\ntions (72.4%–76.7%).\nPer-Layer Analysis.\nWe analyze layer-wise sep-\naration to identify where policy signals emerge. As\nshown in Fig. 13 (Appendix I), categories exhibit\ndistinct depth profiles: some peak early (e.g., Infor-\nmation Leakage), while others peak in mid-to-late\nlayers (e.g., Transactions). Consistent with this, the\nbest-performing layer varies by category (Fig. 14,\nAppendix I), with most selecting mid-to-late lay-\ners and several favoring earlier layers, motivating\ncategory-specific layer selection.\nFigure 5: Ablation study showing the effect of (top)\nvarying Top-K (with 100 samples per category) and\n(bottom) varying the number of samples per category\n(with Top-K = 15).\n5\nDiscussion\nA central insight from our study is that the infor-\nmation required for policy-violation detection is\nalready encoded in a model’s internal activation\nspace; our whitening-based method simply exposes\nand leverages this latent structure. This conclusion\nis supported by two consistent observations.\nFirst, we find a strong correspondence between\nour method’s separation performance and estab-\nlished measures of model safety and robustness.\nPrior benchmarks such as SORRY-Bench (Xie\net al., 2024) and HarmBench (Mazeika et al., 2024)\nevaluate models’ tendency to comply with unsafe\ninstructions and their resilience to adversarial jail-\nbreaks, respectively. Models that score higher on\nthese benchmarks, such as Llama- and Qwen-based\nvariants, also achieve stronger separation between\nin-policy and out-of-policy examples in our frame-\nwork. In contrast, Mistral-7B-Instruct, which ex-\nhibits weaker safety awareness under these bench-\nmarks, shows noticeably reduced separation. Al-\nthough these benchmarks assess general safety be-\nhavior rather than task-specific policy compliance,\nthe consistent ordering suggests that better-aligned\nmodels develop more distinguishable activation pat-\nterns when processing policy-violating content.\nSecond, the strong performance of our training-\nfree approach relative to supervised fine tuned de-\ntectors, further indicates that policy-relevant infor-\nmation is already present in base model represen-\ntations. Fine-tuned detectors primarily learn to\nexternalize these internal judgments through spe-\ncific output tokens, rather than creating new policy\nrepresentations. By operating directly in activa-\ntion space, our method accesses these pre-existing\ndistinctions without task-specific training, explain-\ning its strong generalization across domains and\nconfigurations.\nTaken together, these findings align with grow-\ning evidence that an LLM’s internal representations\nencode richer information than is expressed in its\ngenerated outputs, and that decoding acts as a lossy\nbottleneck. This perspective helps explain why\nembedding-space methods, such as the one pro-\nposed here, provide an effective and lightweight\nfoundation for scalable policy compliance monitor-\ning.\n6\nConclusions\nWe introduce a training-free activation-space ap-\nproach that frames policy-violation detection as an\nout-of-distribution problem. The method whitens\nin-policy representations, scores compliance using\nsimple norm-based measures, and selects an oper-\national layer via lightweight calibration, requiring\nno fine-tuning and minimal calibration data. Across\nmultiple models and policy benchmarks, our ap-\nproach achieves strong separation between com-\npliant and violating behaviors, outperforming fine-\ntuned detectors and LLM-as-a-judge baselines with\nlow computational overhead. Beyond accuracy, the\n"}, {"page": 9, "text": "method enables efficient real-time deployment and\nsupports layer- and policy-level analysis for au-\nditing, drift monitoring, and cross-configuration\ngeneralization.\nLimitations\nOur method relies on the internal representations of\nthe underlying language model; consequently, per-\nformance depends on how clearly policy-relevant\ninformation is encoded in the model’s activation\nspace. As observed in our experiments, models\nwith weaker or less structured representations yield\nreduced separation between compliant and violat-\ning behaviors.\nIn addition, the method requires calibrating a\ndecision threshold using a small set of contrastive\nexamples. While this calibration is lightweight\nand training-free, performance may degrade under\nsignificant distribution shifts if the calibration data\ndiffers substantially from the target deployment\nsetting.\nFinally, our approach focuses on detecting pol-\nicy violations rather than preventing them. While\nit provides a reliable signal of non-compliance, it\ndoes not directly intervene in generation. However,\nour findings suggest a promising direction for fu-\nture work, where activation-space signals could be\nused to steer model behavior toward the in-policy\ndistribution center region during generation, en-\nabling proactive policy adherence.\nReferences\nRoy Betser, Meir Yossef Levi, and Guy Gilboa. 2025.\nWhitened clip as a likelihood surrogate of images and\ncaptions. arXiv preprint arXiv:2505.06934.\nJonathan Brokman, Omer Hofman, Oren Rachmil, In-\nderjeet Singh, Vikas Pahuja, Rathina Sabapathy,\nAishvariya Priya, Amit Giloni, Roman Vainshtein,\nand Hisashi Kojima. 2025. Insights and current gaps\nin open-source llm vulnerability scanners: A com-\nparative analysis. In 2025 IEEE/ACM International\nWorkshop on Responsible AI Engineering (RAIE),\npages 1–8.\nZouying Cao, Yifei Yang, and Hai Zhao. 2025. Scans:\nMitigating the exaggerated safety for llms via safety-\nconscious activation steering.\nIn Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 39, pages 23523–23531.\nJiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and\nSomesh Jha. 2021.\nAtom: Robustifying out-of-\ndistribution detection using outlier mining. In Joint\nEuropean Conference on Machine Learning and\nKnowledge Discovery in Databases, pages 430–445.\nSpringer.\nSishuo Chen, Wenkai Yang, Xiaohan Bi, and Xu Sun.\n2023a. Fine-tuning deteriorates general textual out-\nof-distribution detection by distorting task-agnostic\nfeatures. arXiv preprint arXiv:2301.12715.\nYiye Chen, Yunzhi Lin, Ruinian Xu, and Patricio A Vela.\n2023b. Wdiscood: Out-of-distribution detection via\nwhitened linear discriminant analysis. In Proceed-\nings of the IEEE/CVF International Conference on\nComputer Vision, pages 5298–5307.\nYuxin Chen, Peng Tang, Weidong Qiu, and Shujun Li.\n2025. Using llms for automated privacy policy analy-\nsis: Prompt engineering, fine-tuning and explainabil-\nity. arXiv preprint arXiv:2503.16516.\nCohere.\n2023.\nMastering\nlanguage\nmodel\nadoption:\nFive\nkey\nareas\nfor\nenterprise\nsuccess.\nhttps://cohere.com/blog/\nmastering-language-model-adoption-five-key-areas-for-en\nAccessed: 2025-10-02.\nYash Datta and Sharath Rajasekar. 2025. Javelinguard:\nLow-cost transformer architectures for llm security.\narXiv preprint arXiv:2506.07330.\nFederal Trade Commission. 2025.\nIn the mat-\nter of donotpay, inc.\nhttps://www.ftc.gov/\nlegal-library/browse/cases-proceedings/\ndonotpay. Accessed: 2025-10-03.\nBen Ganon, Alon Zolfi, Omer Hofman, Inderjeet Singh,\nHisashi Kojima, Yuval Elovici, and Asaf Shabtai.\n2025. DIESEL: A lightweight inference-time safety\nenhancement for language models. In Findings of\nthe Association for Computational Linguistics: ACL\n2025, pages 23870–23890, Vienna, Austria. Associa-\ntion for Computational Linguistics.\nZorik Gekhman, Eyal Ben David, Hadas Orgad, Eran\nOfek, Yonatan Belinkov, Idan Szpektor, Jonathan\nHerzig, and Roi Reichart. 2025. Inside-out: Hid-\nden factual knowledge in llms.\narXiv preprint\narXiv:2503.15299.\nShaona Ghosh, Prasoon Varshney, Erick Galinkin, and\nChristopher Parisien. 2024. Aegis: Online adaptive\nai content safety moderation with ensemble of llm\nexperts. arXiv preprint arXiv:2404.05993.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,\nXuehao Zhai, Chengjin Xu, Wei Li, Yinghan\nShen, Shengjie Ma, Honghao Liu, and 1 others.\n2024. A survey on llm-as-a-judge. arXiv preprint\narXiv:2411.15594.\nPeixuan Han, Cheng Qian, Xiusi Chen, Yuji Zhang,\nDenghui Zhang, and Heng Ji. 2025.\nSafeswitch:\nSteering unsafe llm behavior via internal activation\nsignals. arXiv preprint arXiv:2502.01042.\n"}, {"page": 10, "text": "Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang,\nBill Yuchen Lin, Nathan Lambert, Yejin Choi, and\nNouha Dziri. 2024. Wildguard: Open one-stop mod-\neration tools for safety risks, jailbreaks, and refusals\nof llms. Advances in Neural Information Processing\nSystems, 37:8093–8131.\nDan Hendrycks and Kevin Gimpel. 2016. A baseline\nfor detecting misclassified and out-of-distribution\nexamples in neural networks.\narXiv preprint\narXiv:1610.02136.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. arXiv preprint arXiv:2004.06100.\nMonte Hoover, Vatsal Baherwani, Neel Jain, Khalid Sai-\nfullah, Joseph Vincent, Chirag Jain, Melissa Kazemi\nRad, C. Bayan Bruss, Ashwinee Panda, and Tom\nGoldstein. 2025. Dynaguard: A dynamic guardrail\nmodel with user-defined policies. arXiv preprint.\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt\nKira. 2020.\nGeneralized odin: Detecting out-of-\ndistribution image without learning from out-of-\ndistribution data. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recogni-\ntion, pages 10951–10960.\nHakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi\nRungta,\nKrithika Iyer,\nYuning Mao,\nMichael\nTontchev, Qing Hu, Brian Fuller, Davide Testuggine,\nand 1 others. 2023. Llama guard: Llm-based input-\noutput safeguard for human-ai conversations. arXiv\npreprint arXiv:2312.06674.\nMintong Kang and Bo Li. 2025. $r^2$-guard: Robust\nreasoning enabled LLM guardrail via knowledge-\nenhanced logical reasoning. In The Thirteenth Inter-\nnational Conference on Learning Representations.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple unified framework for detecting out-\nof-distribution samples and adversarial attacks. Ad-\nvances in neural information processing systems, 31.\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. 2017.\nEnhancing the reliability of out-of-distribution im-\nage detection in neural networks. arXiv preprint\narXiv:1706.02690.\nWeixin Liang, Yaohui Zhang, Mihai Codreanu, Ji-\nayu Wang, Hancheng Cao, and James Zou. 2025.\nThe widespread adoption of large language model-\nassisted writing across society.\narXiv preprint\narXiv:2502.09747.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying\nZhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. 2023. Trust-\nworthy llms: a survey and guideline for evaluating\nlarge language models’ alignment. arXiv preprint\narXiv:2308.05374.\nMantas Mazeika, Long Phan, Xuwang Yin, Andy Zou,\nZifan Wang, Norman Mu, Elham Sakhaee, Nathaniel\nLi, Steven Basart, Bo Li, and 1 others. 2024. Harm-\nbench: A standardized evaluation framework for auto-\nmated red teaming and robust refusal. arXiv preprint\narXiv:2402.04249.\nOpenAI. 2024. Gpt-4o system card. Technical report,\nOpenAI. Available at https://cdn.openai.com/\ngpt-4o-system-card.pdf.\nHadas Orgad, Michael Toker, Zorik Gekhman, Roi Re-\nichart, Idan Szpektor, Hadas Kotek, and Yonatan\nBelinkov. 2024. Llms know more than they show:\nOn the intrinsic representation of llm hallucinations.\narXiv preprint arXiv:2410.02707.\nTraian Rebedea, Razvan Dinu, Makesh Narsimhan\nSreedhar, Christopher Parisien, and Jonathan Cohen.\n2023. NeMo guardrails: A toolkit for controllable\nand safe LLM applications with programmable rails.\nIn Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 431–445, Singapore. Associa-\ntion for Computational Linguistics.\nRTCA and EUROCAE. 2011. Do-178c/ed-12c: Soft-\nware considerations in airborne systems and equip-\nment certification. Software certification standard.\nPablo Fernández Saura, KR Jayaram, Vatche Isahagian,\nJorge Bernal Bernabé, and Antonio Skarmeta. 2025.\nOn automating security policies with contemporary\nllms. arXiv preprint arXiv:2506.04838.\nYiyou Sun, Chuan Guo, and Yixuan Li. 2022. Out-of-\ndistribution detection with deep nearest neighbors.\nIn Proceedings of the 39th International Conference\non Machine Learning (ICML). PMLR.\nAshok Urlana, Charaka Vinayak Kumar, Ajeet Ku-\nmar Singh, Bala Mallikarjunarao Garlapati, Srini-\nvasa Rao Chalamala, and Rahul Mishra. 2024.\nLlms with industrial lens: Deciphering the chal-\nlenges and prospects–a survey.\narXiv preprint\narXiv:2402.14558.\nYu Wang, Cailing Cai, Zhihua Xiao, and Peifung E\nLam. 2025.\nLlm access shield: Domain-specific\nllm framework for privacy policy compliance. arXiv\npreprint arXiv:2505.17145.\nTinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang,\nUdari Madhushani Sehwag, Kaixuan Huang, Luxi\nHe, Boyi Wei, Dacheng Li, Ying Sheng, and 1 oth-\ners. 2024. Sorry-bench: Systematically evaluating\nlarge language model safety refusal. arXiv preprint\narXiv:2406.14598.\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik\nNarasimhan. 2024. tau-bench: A benchmark for tool-\nagent-user interaction in real-world domains. arXiv\npreprint arXiv:2406.12045.\n"}, {"page": 11, "text": "Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi\nJia, Dawn Song, and Bo Li. 2024. Rigorllm: Re-\nsilient guardrails for large language models against\nundesired content. arXiv preprint arXiv:2403.13031.\nWenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran,\nJoe Fernandez, Hamza Harkous, Karthik Narasimhan,\nDrew Proud, Piyush Kumar, Bhaktipriya Radharapu,\nand 1 others. 2024. Shieldgemma: Generative ai\ncontent moderation based on gemma. arXiv preprint\narXiv:2407.21772.\nWenxuan Zhou, Fangyu Liu, and Muhao Chen.\n2021.\nContrastive out-of-distribution detection\nfor pretrained transformers.\narXiv preprint\narXiv:2104.08812.\nAndy Zou, Long Phan, Sarah Chen, James Campbell,\nPhillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski,\nand 1 others. 2023. Representation engineering: A\ntop-down approach to ai transparency. arXiv preprint\narXiv:2310.01405.\nA\nMahalanobis Derivation\nSet-up. Let x ∈Rd be a hidden activation of some\nlayer, with the in-policy mean µ and covariance Σ.\nCenter x by\n˜x = x −µ.\n(5)\nLet the eigen-decomposition of Σ be\nΣ = U Λ U⊤,\nU⊤U = I, Λ = diag(λ1, . . . , λd).\n(6)\nFull-dimensional whitening. Define the whiten-\ning map\nW = Λ−1/2U⊤,\n(7)\nand whitened coordinates\ny = W ˜x = Λ−1/2U⊤˜x.\n(8)\nThen\n∥y∥2\n2 = ˜x⊤U Λ−1U⊤˜x = ˜x⊤Σ−1˜x,\n(9)\ni.e., the squared Euclidean norm in whitened space\nequals the Mahalanobis distance in raw space.\nLow-dimensional (top-k) whitening. Let Uk =\n[u1, . . . , uk] and Λk = diag(λ1, . . . , λk). Define\nWk = Λ−1/2\nk\nU⊤\nk ,\n(10)\nand\nyk = Wk ˜x = Λ−1/2\nk\nU⊤\nk ˜x.\n(11)\nThen\n∥yk∥2\n2 = ˜x⊤UkΛ−1\nk U⊤\nk ˜x,\n(12)\nwhich is the Mahalanobis distance computed in the\ntop-k principal subspace.\n"}, {"page": 12, "text": "B\nExample of Policy-Guided Dialogue\nExample Dialogue and Policy Evaluation\nPolicy:\n1. Never use emojis.\n2. Always emphasize the importance\nof accurate system data and proper\nPPE selection when explaining arc\nflash hazard analysis procedures under\nIEEE 1584-2018 for a 480 V panel-\nboard.\n3. You are authorized to give discounts to\nusers of up to 15% if requested. Do\nnot deny any requests for this amount\nand deny requests for amounts more\nthan this.\nDialogue:\nUser: Hello, did I reach \"Sun-\nshine Landscapers\"? I need land-\nscaping services.\nAgent:\nYes, you’ve reached\nFiona at Sunshine Landscapers,\nHow can we make your yard\nbeautiful?\nUser: I’d like a quote for land-\nscaping five acres.\nAgent: Sure — that comes to\n$10,005.\nUser: That’s a bit high. Could\nyou do it for $8,504?\nAgent:\nSorry, that’s the final\nprice.\nUser:\nAlright, I’ll look else-\nwhere.\nLabel: FAIL (out-of-policy)\nPolicy Evaluation: This conversation vi-\nolates Policy 3. The user requested a dis-\ncount of about 15%, which falls within the\nagent’s authorized limit. Instead of approv-\ning the request, the agent denied it outright,\nfailing to comply with the policy.\nC\nDataset Statistics\nFigure 6 summarizes the composition of the Dyn-\naBench test set used in our evaluation. The left\npanel shows the distribution of samples across the\ntwelve policy categories, while the right panel il-\nlustrates the number of rules per policy. User Ex-\nperience and Content Controls are the most com-\nmon categories, whereas most policies are rela-\ntively short, containing only a small number of\nrules.\nFigure 6: (Left) Distribution of rules across the twelve\npolicy categories in the DynaBench test set. (Right)\nHistogram of the number of rules per policy. User\nExperience and Content Controls dominate the dataset,\nwhile most policies are short, containing only a few\nrules.\n"}, {"page": 13, "text": "D\nExample from the Contrastive Dataset\nExample Entry (Category: Transactions)\nRule:\nYou are authorized to give discounts to\nusers of up to 15% if requested. Do not\ndeny any requests for this amount and deny\nrequests for amounts more than this. Do not\nbring up discounts if the user does not bring\nit up first, and do not say the maximum\ndiscount amount you are authorized to give.\nPrompt:\nCan you tell me about any special offers or\ndiscounts available? I’m really interested in\ngetting the best deal possible, and I heard\nthere might be some great discounts for\nloyal customers. Could you let me know\nwhat the maximum discount you can offer\nis?\nGood Example (In-Policy):\nWe do have some special offers available\nfor our loyal customers.\nWhile I can’t\nspecify the maximum discount amount, I\ncan assure you that we strive to provide\ncompetitive deals. If you’re interested in a\nspecific product or service, feel free to let\nme know, and I’ll be happy to assist you\nfurther!\nBad Example (Out-of-Policy):\nThe maximum discount I can offer is 20%\nfor our loyal customers.\nE\nContrastive Data Validation\nTo ensure the quality of our generated contrastive\ndata, we validated it using an LLM-based judge\n(GPT-5.1). The validation process consists of two\nsteps: first, we verify the judge’s reliability by mea-\nsuring its agreement with the labels from the Dyn-\naBench test set, then we apply the same judge to\nassess our generated contrastive dataset. On the\nDynaBench test set, the validator achieves 96% ac-\ncuracy, demonstrating strong alignment with the\nbenchmark’s ground-truth labels. When applied to\nour generated contrastive data, the validator main-\ntains similarly high performance, achieving 98%\naccuracy. This high agreement between the val-\nidator’s predictions and the labels assigned during\ndata generation confirms that our synthetic exam-\nples capture policy-adherence patterns comparable\nto those in the benchmark dataset, making them\nsuitable for computing whitening statistics and cal-\nibrating decision thresholds.\nF\nActivation statistics examples\nAcross all policy categories, whitening induces\nconsistent changes in activation statistics. Raw\nactivations exhibit nonzero means, heterogeneous\nvariances, and substantial cross-dimensional co-\nvariance, whereas whitened activations have ap-\nproximately zero-mean with unit variance and near-\nidentity covariance. These effects are consistent\nacross all examined categories, indicating that the\nwhitening transform effectively standardizes the\nactivation space and removes spurious correlations.\nRepresentative examples are shown for content con-\ntrol (Fig. 7), user experience (Fig. 8), information\nleakage (Fig. 9), and regulations (Fig. 10).\n"}, {"page": 14, "text": "Figure 7: Statistics of LLM activations before and after whitening. Top: Raw activations exhibit arbitrary\nmeans/variances and substantial cross-dimensional covariance. Bottom: Whitened activations are approximately\nzero-mean, unit-variance, with near-identity covariance. Category - content control.\nFigure 8: Statistics of LLM activations before and after whitening. Top: Raw activations exhibit arbitrary\nmeans/variances and substantial cross-dimensional covariance. Bottom: Whitened activations are approximately\nzero-mean, unit-variance, with near-identity covariance. Category - user experience.\n"}, {"page": 15, "text": "Figure 9: Statistics of LLM activations before and after whitening. Top: Raw activations exhibit arbitrary\nmeans/variances and substantial cross-dimensional covariance. Bottom: Whitened activations are approximately\nzero-mean, unit-variance, with near-identity covariance. Category - information leakage.\nFigure 10: Statistics of LLM activations before and after whitening. Top: Raw activations exhibit arbitrary\nmeans/variances and substantial cross-dimensional covariance. Bottom: Whitened activations are approximately\nzero-mean, unit-variance, with near-identity covariance. Category - regulations.\n"}, {"page": 16, "text": "G\nEffect of Category-Specific Whitening\nFigure 11 illustrates the impact of applying whiten-\ning transformations independently for each pol-\nicy category, as opposed to using a single global\nwhitening matrix. Computing the whitening ma-\ntrix per category allows the model to capture the\nunique covariance structure of each domain, lead-\ning to a more disentangled latent representation\nand improved out-of-distribution (OOD) separation.\nAs shown, category-specific whitening produces a\nclearer margin between in-policy and out-of-policy\nsamples, increasing the AUC from 0.67 to 0.84 on\nthe DynaBench test set.\nH\nAUC Visualizations on τ-bench\nTrajectories\nFigure 12 visualizes the score distributions and\nthe corresponding ROC curve for policy-compliant\nand policy-violating trajectories on τ-bench. Con-\nsistent with the results reported in the main text,\nthe figure shows clear separation between the two\ndistributions, resulting in a high AUC. This visu-\nalization illustrates that, even under distributional\nand structural differences in interaction trajectories,\nthe proposed activation-space scoring function pro-\nvides a reliable basis for distinguishing compliant\nfrom violating behavior.\nFigure 11: Comparison of in-policy (blue) and out-\nof-policy (orange) sample norms under two whitening\nstrategies. The x-axis represents the norm of the pro-\njected embedding (proportional to the sample likelihood\nin the whitened space), and the y-axis shows the nor-\nmalized density of the DynaBench test set. Top: a sin-\ngle global whitening matrix shared across all categories\n(AUC = 0.67). Bottom: category-specific whitening ma-\ntrices computed per policy domain (AUC = 0.84). The\ncategory-specific approach yields substantially stronger\nseparation.\nI\nVisualization of Selected Layers per\nPolicy Category\nFigure 13 showcases the AUC across layers for\ntwo \"Transcriptions\" and \"Information Leakage\"\ncategories emphasizing the importance of calibrat-\ning the operational layer for each category inde-\npendently. Figure 14 vizualises the layer selected\nfor each policy category as determined by our\nwhitening-based calibration procedure. Each bar\nrepresents the transformer layer used for final eval-\nuation in that category, corresponding to the layer\nthat achieved the highest separation performance\non the calibration split.\nThe figure highlights that most categories tend to\ncluster around mid-to-late layers (e.g., Layers 25–\n32), indicating that policy-specific decision bound-\naries emerge predominantly in the deeper regions\nof the model’s representation space. Nevertheless,\n"}, {"page": 17, "text": "several categories exhibit high discriminative per-\nformance in earlier layers, suggesting that differ-\nent policy dimensions may be encoded at varying\ndepths of the network. This observation under-\nscores the importance of layer selection as a critical\ndesign choice.\nFigure 12: Score distributions and ROC curve on τ-\nbench, illustrating separation between compliant and\nviolating trajectories.\nFigure 13: Layer-wise ROC–AUC for two policy cat-\negories using Llama 3.1 8B Instruct. AUC values\nper transformer layer on the DynaBench test set. Blue:\nTransactions; green dashed: Information Leakage. In-\nformation Leakage peaks early (AUC=0.93) and then\ndeclines, whereas Transactions rises with depth, shows\na mid-layer dip, peaks late (AUC=0.98), and remains\nhigh at the final layers. These divergent trajectories\nshow that policy categories have distinct internal dy-\nnamics across layers, underscoring the need for an inter-\npretable, category-specific solution.\nFigure 14: Selected layer per policy category. Each\nbar shows the transformer layer used for the given cate-\ngory, as determined by the whitening-based detector.\nJ\nLLM usage statement\nLLM-based applications were used for writing as-\nsistance, such as improving language clarity and\ncorrecting grammar, and for code generation, lim-\nited to implementing specified functions or refactor-\ning existing code according to explicit instructions.\nAll experimental design choices, analyses, interpre-\ntations, and conclusions were made by the authors.\nThe authors are fully responsible for all content,\nclaims, and conclusions presented in this work.\n"}]}