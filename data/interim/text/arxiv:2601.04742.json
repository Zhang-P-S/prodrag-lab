{"doc_id": "arxiv:2601.04742", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.04742.pdf", "meta": {"doc_id": "arxiv:2601.04742", "source": "arxiv", "arxiv_id": "2601.04742", "title": "Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval", "authors": ["Seyeon Jeong", "Yeonjun Choi", "JongWook Kim", "Beakcheol Jang"], "published": "2026-01-08T09:07:41Z", "updated": "2026-01-08T09:07:41Z", "summary": "Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.04742v1", "url_pdf": "https://arxiv.org/pdf/2601.04742.pdf", "meta_path": "data/raw/arxiv/meta/2601.04742.json", "sha256": "e6726b362c114007601c70fb63f2436014a09ebdd18b6cf31d4cce8213e57271", "status": "ok", "fetched_at": "2026-02-18T02:22:24.767253+00:00"}, "pages": [{"page": 1, "text": "1\nTool-MAD: A Multi-Agent Debate Framework for Fact Verification\nwith Diverse Tool Augmentation and Adaptive Retrieval\nSeyeon Jeong1, Yeonjun Choi1, JongWook Kim2, Beakcheol Jang1\n1Graduate School of Information, Yonsei University, Seoul, Republic of Korea\n2Department of Computer Science, Sangmyung University, Seoul, Republic of Korea\nAbstract—Large Language Models (LLMs) suffer from halluci-\nnations and factual inaccuracies, especially in complex reasoning\nand fact verification tasks. Multi-Agent Debate (MAD) systems\naim to improve answer accuracy by enabling multiple LLM\nagents to engage in dialogue, promoting diverse reasoning and\nmutual verification. However, existing MAD frameworks primar-\nily rely on internal knowledge or static documents, making them\nvulnerable to hallucinations. While MADKE introduces external\nevidence to mitigate this, its one-time retrieval mechanism limits\nadaptability to new arguments or emerging information during\nthe debate. To address these limitations, We propose Tool-\nMAD, a multi-agent debate framework that enhances factual\nverification by assigning each agent a distinct external tool,\nsuch as a search API or RAG module. Tool-MAD introduces\nthree key innovations: (1) a multi-agent debate framework where\nagents leverage heterogeneous external tools, encouraging diverse\nperspectives, (2) an adaptive query formulation mechanism that\niteratively refines evidence retrieval based on the flow of the\ndebate, and (3) the integration of Faithfulness and Answer\nRelevance scores into the final decision process, allowing the\nJudge agent to quantitatively assess the coherence and question\nalignment of each response and effectively detect hallucina-\ntions. Experimental results on four fact verification benchmarks\ndemonstrate that Tool-MAD consistently outperforms state-of-\nthe-art MAD frameworks, achieving up to 5.5% accuracy\nimprovement. Furthermore, in medically specialized domains,\nTool-MAD exhibits strong robustness and adaptability across\nvarious tool configurations and domain conditions, confirming\nits potential for broader real-world fact-checking applications.\nIndex Terms—LLM, Multi-Agent Debate, Fact Verification.\nI. INTRODUCTION\nL\nARGE Language Models (LLMs) have recently achieved\nstrong performance across various NLP tasks [1]–[3],\nsuch as dialogue generation, summarization, and knowl-\nedge extraction. However, they often suffer from hallucina-\ntion—producing confident yet factually incorrect content [4].\nTo mitigate this issue, prompt-based single-agent meth-\nods such as Chain-of-Thought (CoT) [5] and self-reflection\n[6] guide reasoning or incorporate external tools like the\nWikipedia API. While simple and effective, these approaches\noften fail to revise incorrect answers, a limitation referred to\nas Degeneration of Thought [7].\nRecently, multi-agent debate frameworks have been pro-\nposed to overcome this by enabling multiple LLMs to en-\ngage in argumentation and mutual verification [7], [8]. While\npromising, existing methods typically rely on static documents\nor internal model knowledge and still base final decisions\nThis work was supported by the National Research Foundation of Korea\n(NRF) funded by Korean Government under Grant RS-2023-00273751.\nFig. 1.\nComparison of MAD [7], MADKE [9] and the proposed Tool-\nMAD: Unlike MAD and MADKE, which relies on a fixed document pool\nfor retrieval, Tool-MAD dynamically retrieves external documents at each\nround through query formulation.\non LLM-generated debate history, which remains vulnerable\nto hallucination. To enhance factual grounding, MADKE [9]\nintroduced static external retrieval before the debate. However,\nthis evidence remains fixed during discussion, limiting the\nagents’ ability to adapt to new claims or knowledge gaps that\nemerge throughout the debate.\nA key challenge underlying these limitations is that fact\nverification in practical scenarios rarely depends on a sin-\ngle, uniform source of evidence. Different external tools\nprovide distinct advantages: search APIs offer broad cover-\nage and access to real-time information, whereas Retrieval-\nAugmented Generation (RAG) modules retrieve semantically\naligned and context-rich documents from curated corpora such\nas Wikipedia. Tasks involving breaking news or fast-evolving\nevents require up-to-date external information, while tasks\ninvolving historically stable knowledge demand high-precision\nretrieval. Relying on a single type of retrieval often leads\nto systematic blind spots. Moreover, prior debate frameworks\ntypically assume that evidence retrieved before the discussion\nis sufficient for the entire process, overlooking the dynamic na-\nture of argumentation. In human debates, participants routinely\nrefine their claims and gather new evidence when confronted\nwith counterarguments or inconsistencies. Without a mecha-\nnism for iterative retrieval, debate agents remain restricted by\nthe limitations of their initial evidence set.\nIn this paper, we introduce Tool-MAD, a dynamic multi-\nagent debate framework designed to enhance factual veri-\nfication by enabling agents to leverage heterogeneous ex-\narXiv:2601.04742v1  [cs.CL]  8 Jan 2026\n"}, {"page": 2, "text": "2\nternal tools such as search APIs and Retrieval-Augmented\nGeneration (RAG) modules, thereby promoting diverse per-\nspectives [10]. Unlike prior multi-agent debate frameworks\nthat rely on static retrieval or internally stored knowledge\nwithin the model, Tool-MAD is designed to allow agents\nto dynamically and iteratively retrieve new evidence as the\ndebate progresses. Specifically, after each debate round, agents\nupdate their queries based on previous arguments exchanged\nduring the discussion, enabling adaptive retrieval of relevant\ndocuments. This iterative knowledge acquisition process sig-\nnificantly enhances the adaptability of the framework, reduces\nthe likelihood of hallucinations, and contributes to improving\nthe factual reliability of the final decision. This mechanism is\nillustrated in Figure 1, which compares MAD, MADKE and\nTool-MAD.\nIn addition, we evaluate the responses generated in each\ndebate round using two independent metrics from the RAGAS\nframework [11]: faithfulness, which measures how well an\nagent’s claim is grounded in the retrieved evidence, and\nanswer relevance, which assesses how directly the response\naddresses the original question. As these two metrics capture\ncomplementary aspects of response quality, we collectively\nrefer to them as the stability score, which serves as an auxiliary\nsignal for the Judge agent to assess the factual consistency and\ntrustworthiness of each response.\nWe conduct extensive experiments across four fact verifica-\ntion benchmark datasets to evaluate the effectiveness of Tool-\nMAD. The results show that Tool-MAD consistently outper-\nforms competitive multi-agent debate framworks such as MAD\n[7] and MADKE [9], achieving performance improvements\nof up to 35.5 % and 5.5 %, respectively. Tool-MAD further\ndemonstrates its flexibility in medical QA settings, maintaining\nrobust performance under different retrieval tools and corpus\nconfigurations.\nThe main contributions of this paper can be summarized as\nfollows:\n• We propose Tool-MAD, a novel multi-agent debate\nframework that empowers agents to verify factual claims\nadaptively by leveraging a diverse set of external tools,\nincluding real-time search APIs and Retrieval-Augmented\nGeneration (RAG) modules.\n• We introduce an adaptive query formulation mechanism,\nenabling agents to iteratively refine their evidence re-\ntrieval based on evolving debate contexts and previous\narguments, leading to more informed and reliable judg-\nments.\n• We incorporate faithfulness and answer relevance as a\nstability score in Tool-MAD to assess how well responses\nalign with evidence and address the original question.\nThis helps detect hallucinations and guide final decisions.\n• We comprehensively evaluate Tool-MAD on four bench-\nmark datasets for fact verification, as well as two ad-\nditional datasets for medical QA tasks, consistently sur-\npassing competitive multi-agent debate baselines.\nII. RELATED WORKS\nMulti-Agent Debate. Multi-agent debate frameworks have\nemerged as a promising direction for strengthening LLM rea-\nsoning by leveraging adversarial or collaborative interactions\nbetween agents. Early theoretical foundations stem from Min-\nsky’s “society of minds” theory [12], which views intelligence\nas the emergent result of multiple interacting subsystems. This\nidea has resurfaced in modern LLM settings, where multiple\nagents exchange arguments, challenge assumptions, and refine\nintermediate reasoning.\nDu et al. [8] demonstrated that multi-round debate enables\nLLMs to correct each other’s errors and improve logical\nconsistency. Liang et al. [7] introduced a tit-for-tat debate\nstructure to combat the Degeneration of Thought problem\nidentified in self-reflection methods [13]. Other works have\nexplored more complex multi-agent coordination strategies,\nsuch as majority voting, argument diversification, or struc-\ntured deliberation trees. RECONCILE [14] positions agents as\nparticipants in a roundtable discussion, integrating confidence-\nweighted consensus to avoid dominance by a single agent.\nBeyond general reasoning, multi-agent frameworks have\nalso been applied in specialized settings such as planning,\nsafety evaluation, and code generation. For instance, debate-\nbased self-correction mechanisms have been explored in math-\nematical reasoning, where agents critique intermediate steps,\nand in safety-alignment contexts, where disagreement is used\nto uncover unsafe model [8]. These efforts show that debate\ncan create richer reasoning traces, but they still rely heavily\non internal knowledge.\nMADKE [9] introduced the idea of knowledge-enhanced\ndebate by injecting externally retrieved documents before\ndiscussion; however, the evidence pool remains unchanged\nthroughout the debate process. Similarly, debate-enhanced\nRAG systems retrieve supporting evidence prior to deliberation\nand then freeze the document set. While these systems show\nempirical gains, the fixed-evidence assumption limits adapt-\nability when new arguments emerge during the debate.\nFact Verification and Factuality Evaluation. Ensuring\nthe factual correctness of LLM outputs is a long-standing\nchallenge [4], [15]. Early fact verification approaches relied\non supervised classification models over structured evidence\nsources such as Wikipedia or news corpora. With the rise\nof generative models, research has shifted toward grounding\ngeneration in external corpora through retrieval-augmented\ngeneration (RAG) [10]. RAG-based fact-checkers [16] and\nhybrid evidence selection frameworks have shown that inte-\ngrating retrieval significantly reduces hallucination in open-\ndomain QA and claim verification.\nFact verification evaluation has similarly diversified. Works\nsuch as FEVER and its successors introduced label-based\nevaluation\npipelines.\nMore\nrecent\nframeworks\nsuch\nas\nFactScore [17] attempt to quantify factuality at the claim level\nby decomposing model outputs. Consistency-based factuality\napproaches, including self-consistency [18] and re-asking [19],\nevaluate factual correctness by measuring the stability of\nmodel outputs rather than relying solely on external grounding.\nRAGAS [11] bridged retrieval-based reasoning and factu-\nality evaluation by introducing two complementary metrics:\nfaithfulness, measuring whether claims match retrieved evi-\ndence, and answer relevance, measuring whether responses\naddress the question directly. Several subsequent works have\n"}, {"page": 3, "text": "3\nadopted faithfulness for hallucination detection in RAG sys-\ntems or used relevance-based filtering to identify unusable\ngenerations. However, these metrics have predominantly been\napplied as post-hoc evaluators for fully generated outputs, not\nas real-time signals that influence multi-step reasoning.\nFurthermore, no prior work integrates metric-guided eval-\nuation into a multi-agent debate structure, nor uses factuality\nsignals to modulate argument selection, judge decisions, or\nevidence refinement across rounds. Tool-MAD incorporates\nthese signals internally, using faithfulness and answer rele-\nvance as round-level stability indicators, which is distinct from\nexisting factuality scoring frameworks.\nTool-Augmented and Retrieval-Augmented Agents. A\ngrowing line of research explores augmenting LLMs with\nexternal tools, enabling them to perform tasks requiring\nspecialized knowledge or computation. Toolformer [20] en-\nables models to learn API-calling behaviors, while Hugging-\nGPT [21] and GEAR [22] treat the LLM as a coordinator for\nheterogeneous models or systems. Tool-use frameworks have\nsince evolved into modular agent pipelines that combine in-\nformation extraction, symbolic reasoning, multi-step planning,\nor domain-specific simulators.\nIn scientific and professional domains, tool-augmented\nagents such as ChemCrow [23] and biomedical retrieval\nagents\n[24]\ndemonstrate\nthat\ndomain-specific\ntools\ncan\ndramatically\nimprove\nreasoning\nfidelity.\nRetrieval\nmod-\nules—including dense retrievers, hybrid rankers, and cross-\nencoder rerankers—enable models to ground their reasoning\nin curated evidence.\nHowever, existing tool-augmented systems overwhelmingly\nadopt a single-agent perspective and use tools in a one-shot or\nsequential fashion. They do not exploit multi-agent interaction\nas a mechanism for tool selection, evidence diversification,\nor iterative grounding. Likewise, retrieval systems typically\nperform a single retrieval step at the beginning of a task,\nwithout adapting to evolving reasoning trajectories or emerg-\ning counterarguments. As a result, these systems often fail\nto revisit or refine earlier tool calls when new uncertainties\narise, leading to brittle reasoning paths. Moreover, the lack of\ninteraction between agents prevents the system from leverag-\ning disagreement or complementary viewpoints to guide more\ntargeted retrieval or tool use.\nIII. TOOL-MAD\nIn this section, we introduce Tool-MAD, a novel multi-agent\ndebate framework designed to enhance the factual reliability\nof LLMs for claim verification tasks. Unlike previous methods\nthat rely primarily on static evidence or single-agent reasoning,\nTool-MAD incorporates iterative retrieval of external evidence\nand dynamic interactions among multiple specialized agents.\nBy repeatedly updating evidence and challenging one another’s\nconclusions, the system progressively refines its reasoning,\nmitigates hallucinations, and promotes more reliable consensus\nformation.\nA. External Tools\nTo enable dynamic, context-aware fact verification, Tool-\nMAD equips agents with external retrieval tools that provide\naccess to relevant and timely evidence during debates. Specifi-\ncally, we integrate two complementary retrieval mechanisms: a\nRAG module leveraging a static corpus, and a live web Search\nAPI for real-time information access.\nRetrieval-Augmented Generation We employ the RAG\nframework [25] to augment agent reasoning with relevant\ndocuments retrieved from an embedded vector store. For this\npurpose, we use Milvus [26], a scalable and efficient vector\ndatabase optimized for high-dimensional corpus management.\nWe index a corpus constructed from Wikipedia articles, en-\nabling rapid semantic retrieval. At inference time, each query\nreturns the top three most semantically relevant documents,\nproviding agents with targeted supporting evidence.\nSearch API Complementing the static RAG corpus, Tool-\nMAD also incorporates a real-time Search API, enabling\nagents to access up-to-date information directly from the web.\nFor this, we utilize the Tavily Search API 1, known for its\neffective integration with language models. Similar to the\nRAG system, the Search API retrieves the three most relevant\ndocuments per query, ensuring comprehensive coverage of\nevolving knowledge demands during the debate.\nAlgorithm 1 Tool-MAD Framework\nInput: claim c, debater set A, max rounds T, stability\nthresholds St = (f, ar)\nOutput: final decision answer\nInitialize history H ←∅\nfor each A ∈A do\nSA ←0\nend for\nfor r = 1 to T do\nfor each agent A ∈A do\nLet ¯A be the opponent of A\nif r = 1 then\nq(r)\nA ←Query(A, c)\nD(r)\nA ←Retrieve(A, q(r)\nA )\na(r)\nA ←Respond(A, D(r)\nA , c)\nelse\nq(r)\nA ←Query(A, c, a(r−1)\n¯\nA\n, q(r−1)\nA\n)\nD(r)\nA ←Retrieve(A, q(r)\nA )\na(r)\nA ←Respond(A, D(r)\nA , c, a(r−1)\n¯\nA\n)\nend if\nf (r)\nA\n←faithfulness(a(r)\nA , D(r)\nA )\nar(r)\nA ←answerRelevance(c, a(r)\nA )\nSA ←SA + (f (r)\nA , ar(r)\nA )\nend for\nAppend round-r info to H\nif aR = aS and SR > St and SS > St then\nreturn aR\nend if\nend for\nreturn Judge(AJ, H, {SA}A∈A, c)\n1https://tavily.com/\n"}, {"page": 4, "text": "4\nRashida Jones was a cast member in Our Idiot Brother.\nGT : SUPPORTS\nClaim\nQuery Formulation\nRetrieve Documents\nResponse Answer\nThe judgment is determined as SUPPORTS \nbased on the debate history and faithfulness and answer relevance.\nRAG\nDB\nSearch\nAPI\nInit Debate (r = 1)\nSUPPORTS\nREFUTES\nCheck Faithfulness and Answer Relevance\nCheck Consensus\nSUPPORTS\nREFUTES\nQuery Formulation\nRetrieve Documents\nResponse Answer\nRAG\nDB\nSearch\nAPI\nSUPPORTS\nREFUTES\nwithout other's response\nwith other's response\nRAG Agent\nSearch Agent\nJudge Agent\nDebate\nHistory\nFaithfulness &\nAnswer Relevance\nJudge\nDebate (r  <       )\nFaithfulness : 0.8\nAnswer Relevance : 0.9\nFaithfulness : 0.83\nAnswer Relevance : 0.6\nFig. 2.\nGiven a claim, two agents (RAG and Search) engage in multi-round debates, where r denotes the current round and T is the predefined round\nthreshold. If no consensus is reached or the stability score falls below the threshold by round T, the Judge Agent issues a final verdict based on the debate\nhistory and the stability score.\nB. Debate Participants\nDebater Agents Tool-MAD involves two specialized de-\nbater agents: a RAG-based agent (AR) that retrieves evidence\nfrom a static vector-based corpus, and a Search-based agent\n(AS) that accesses live web documents via a search API.\nBoth agents operate under a shared prompt template to ensure\nconsistent behavior across rounds. In the first round, each\nagent independently generates a response based solely on the\ninput claim. In subsequent rounds, agents refine their responses\nby incorporating the previous response from their opponent,\nenabling richer reasoning and exposure to diverse viewpoints.\nJudge Agent If the two debater agents fail to reach con-\nsensus within a predefined maximum number of rounds, the\nfinal decision is made by a third agent, the Judge (AJ). The\nJudge determines the outcome based on three primary inputs:\n(1) the original claim, (2) the full debate history—including\nagent responses and retrieved evidence from each round, and\n(3) a stability score that assesses the factual consistency and\nrelevance of each agent’s arguments.\nC. Stability Score\nTo quantitatively evaluate the reliability of each agent’s re-\nsponse, Tool-MAD adopts two core metrics from the RAGAS\nframework [11]: faithfulness and answer relevance. These two\nmetrics jointly form the Stability Score used throughout the\ndebate.\nFaithfulness measures how accurately an agent’s response\nreflects the content of the retrieved evidence. Each response\nas(q) is decomposed into a set of factual statements:\nS(as(q)) = {s1, s2, . . . , s|S|}.\nFor each statement si, the LLM determines whether it can\nbe inferred from the retrieved context c(q). We define a\nverification function v(si, c(q)) such that v(si, c(q)) = 1 if\nsi is supported by c(q), and v(si, c(q)) = 0 otherwise. The\nfaithfulness score F is then computed as the proportion of\nsupported statements:\nF =\nP|S|\ni=1 v(si, c(q))\n|S|\n.\nA high faithfulness score indicates that the response is well-\ngrounded and factually consistent with the retrieved evidence.\nAnswer relevance assesses how directly the agent’s re-\nsponse addresses the original question. For the given answer\nas(q), the LLM generates a set of n potential questions:\nQ′ = {q1, q2, . . . , qn},\nwhere each qi is intended to represent a question that the an-\nswer could plausibly correspond to. We compute embeddings\nfor the original question q and each generated question qi, and\ncalculate their cosine similarity:\nsim(q, qi) = cos\n\u0000e(q), e(qi)\n\u0001\n.\nThe answer relevance score is then obtained by averaging the\nsimilarities:\nAR = 1\nn\nn\nX\ni=1\nsim(q, qi).\n"}, {"page": 5, "text": "5\n0.0-0.2\n0.2-0.4\n0.4-0.6\n0.6-0.8\n0.8-1.0\n0\n50\n100\n150\nCount\n5\n9\n24\n37\n74\n1\n8\n16\n22\nAverage RAG Faithfulness\nCorrect\nIncorrect\n0.0-0.2\n0.2-0.4\n0.4-0.6\n0.6-0.8\n0.8-1.0\n0\n50\n100\n150\n10\n139\n19\n28\nAverage RAG Answer Relevance\nCorrect\nIncorrect\n0.0-0.2\n0.2-0.4\n0.4-0.6\n0.6-0.8\n0.8-1.0\n0\n50\n100\n150\nCount\n2\n5\n10\n33\n99\n1\n4\n6\n15\n21\nAverage Search Faithfulness\nCorrect\nIncorrect\n0.0-0.2\n0.2-0.4\n0.4-0.6\n0.6-0.8\n0.8-1.0\n0\n50\n100\n150\n6\n143\n14\n33\nAverage Search Answer Relevance\nCorrect\nIncorrect\nFig. 3.\nEmpirical distributions of faithfulness and answer relevance for selecting stability-score thresholds. Answer relevance is concentrated above 0.8,\nwhile faithfulness shows a wider spread around 0.7–0.8. We therefore set thresholds to 0.7 (faithfulness) and 0.8 (answer relevance) to balance precision and\nefficiency\nA high relevance score suggests that the answer remains fo-\ncused on the intended question, whereas a low score indicates\nincompleteness or topic drift.\nBoth metrics are computed at every debate round. If either\nfaithfulness or answer relevance falls below a predefined\nthreshold, the round is marked as inconclusive and the debate\ncontinues.\nThreshold\nof\nStability\nScore To determine suitable\nstability-score thresholds, we analyzed the empirical distribu-\ntions of faithfulness and answer relevance (Figure 3). Answer\nrelevance was strongly skewed toward high values, with most\nscores above 0.8, making 0.8 a natural cutoff that filters low-\nquality outputs without affecting the majority of valid ones.\nIn contrast, faithfulness exhibited a wider spread, with many\nresponses falling between 0.7 and 0.8; thus, using a stricter\nthreshold would unnecessarily reject reasonable, evidence-\naligned answers and prolong debates.\nThese observations reveal a key trade-off: overly strict\nthresholds increase precision but hurt efficiency, especially\nwhen models produce borderline yet acceptable outputs.\nTherefore, we adopt 0.7 for faithfulness and 0.8 for answer rel-\nevance, which empirically provides stable convergence while\nmaintaining decision quality.\nD. Tool-MAD Procedure\nTool-MAD proceeds in multiple rounds of interaction be-\ntween two debater agents, each equipped with distinct external\ntools. Let c denote the input claim, and r ∈{1, . . . , T} be\nthe current debate round, where T is the maximum allowed\nnumber of rounds. The framework involves three agents: a\nQuery Selection\nYou are a participant in a fact-checking debate. Our goal is to reach a\nconsensus with an accurate answer. You are an agent utilizing RAG.\nYou are in query select page, you can choose change query or continue use\nyour query.\nOnly output the query, and wrap it in square brackets like this: [your query\nhere]. Do not include anything else.\nOther debaters answer: {other_answer}\nBefore you used query : {used_query}\nClaim : {prompt}\nFig. 4. Prompt of query selection\nretrieval-based debater AR, a search-based debater AS, and\na Judge agent AJ. Each debater generates arguments based\non its retrieved evidence, and the responses in each round\nare quantitatively evaluated using faithfulness and answer rele-\nvance scores. If either agent’s response fails to meet predefined\nthresholds for these scores, or if no consensus is reached, the\ndebate proceeds to the next round. If consensus is not achieved\nby the final round T, the Judge agent produces a final verdict\nbased on the accumulated dialogue and retrieved evidence. The\noverall framework overview is presented in Figure 2, and the\ncomplete algorithmic details are provided in the algorithm 1.\nInitialization Round (r = 1). In the first round, each\ndebater independently constructs an initial query based solely\non the input claim c, without reference to the opponent’s\nargument. This query is submitted to the agent’s designated\nretrieval tool, which returns a set of top-k relevant documents.\n"}, {"page": 6, "text": "6\nN Round's Debate\nIf the claim is correct, you must first explain why it is correct based on the\ndocument, then output **SUPPORTS**. \nIf the claim is incorrect, you must first explain why it is incorrect based on\nthe document, then output **REFUTES**.\nIf the evidence is unclear or inconsistent, you must first explain the\nuncertainty, then output **NOT ENOUGH INFO**.\nYour final answer (**SUPPORTS**, **REFUTES**, or **NOT ENOUGH\nINFO**) must appear on the last line only, after your reasoning.\nClaim : {prompt} \nOther debaters answer : {other_answer}\nDocument : {search_result}\nFig. 5. Prompt of round debate\nJudge\nYou are the judge for the fact-checking debate. This is the final round of the\ndebate. If no consensus has been reached, you must determine the correct\nanswer\nThe faithfulness score measures how accurately the agent's answer reflects\nthe retrieved documents, while the answer relevancy score indicates how\nwell the answer addresses the original question.\nA higher score means better alignment and greater reliability\nBased on the debate history and faithfulness and answer relevancy score, \nplease determine the correctness of the claim as follows:\nif the claim is correct, output **SUPPORTS**\nif the claim is incorrect, output **REFUTES**\nif it is uncertain whether the claim is correct, output **NOT ENOUGH\nINFO**\nDebate History : {debate_history}\nRAG Agent's Faithfullness Score : {rag_faithfullness / 3}\nRAG Agent's Relevancy Score : {rag_answer_relevance / 3\nSearch Agent's Faithfullness Score : {search_faithfullness / 3}\nSearch Agent's Relevancy Score : {search_answer__relevance / 3}\nClaim : {prompt}\nFig. 6. Prompt of judge\nThe agent then composes its response by reasoning over the\nclaim and the retrieved evidence. For agent A ∈{AR, AS},\nthe process is formally defined as:\nq1\nA = Query(A, c)\n(1)\nD1\nA = Retrieve(A, q1\nA)\n(2)\na1\nA = Respond(A, D1\nA, c)\n(3)\nwhere q1\nA is the retrieval query, D1\nA is the set of retrieved\ndocuments, and a1\nA is the generated response for round 1.\nDebate Rounds (r\n>\n1). In each subsequent round,\nagents refine their queries and responses by incorporating the\nopponent’s previous answer. This enables dynamic evidence\nupdates and encourages more robust reasoning. Each round\nconsists of three steps: query formulation, evidence retrieval,\nand response generation. Specifically:\nTABLE I\nDATASETS USED IN TOOL-MAD, CATEGORIZED BY TASK TYPE AND\nCORRESPONDING REFERENCES.\nDataset\nTask\nFEVER [27]\nFact Verification\nFEVEROUS [28]\nFact Verification\nFAVIQ [29]\nFact Verification\nAVERITEC [30]\nFact Verification\nMEDQA [31]\nMedical\nPUBMEDQA [32]\nMedical\nqr\nA = Query(A, c, ar−1\n¯\nA\n)\n(4)\nDr\nA = Retrieve(A, qr\nA)\n(5)\nar\nA = Respond(A, Dr\nA, c, ar−1\n¯\nA\n)\n(6)\nwhere ar−1\n¯\nA\nis the previous response from the opposing agent\n¯A.\nIf both agents produce the same response in any round,\nthe debate terminates early with a consensus, considering\npredefined thresholds of the stability score. Otherwise, the\ncurrent round is appended to the debate history. If consensus\nis not reached or the responses fall below the thresholds\nfor faithfulness or answer relevance, the debate continues\nuntil round T. The Judge agent AJ then determines the final\noutcome.\nJudge Decision(triggered only when the debate reaches\nthe final round) If the agents fail to reach a consensus by the\nfinal round T, the Judge agent AJ makes the final decision\nbased on the original claim c, the complete debate history\nH, and the stability score S. Here, S is composed of the\naverage faithfulness and answer relevance scores across all\nrounds, capturing the overall quality of the agents’ responses.\nThis decision process is formalized as:\nfinal answer = Judge(AJ, H, S, c)\n(7)\nThe LLM-based Judge agent receives the stability score S\nand debate history H as part of its prompt and makes the final\ndecision by jointly considering both.\nFor reproducibility, we provide the full prompts used for\nthe debate round, query selection, and judge components in\nFigures\n4,\n5, and\n6. Note that the prompt for the initial\nround corresponds to the same template with the opponent’s\nresponse removed.\nIV. EXPERIMENTS\nWe evaluate the effectiveness of a Tool-MAD framework\nthat leverages external tools in performing the fact verifica-\ntion task. Unless specified otherwise, all experiments were\nconducted on 200 randomly sampled instances per dataset,\nevaluated using an Exact Match (EM) criterion, where a\nprediction is considered correct if it exactly matches the\nground truth labels.\n"}, {"page": 7, "text": "7\nTABLE II\nSTRUCTURAL COMPARISON OF BASELINE REASONING FRAMEWORKS.\nModel\nReasoning\nMulti-Agent\nRetrieval\nQuery Formulation\nCoT (Zero-shot)\nO\nX\nX\nX\nSingle-Agent (ReAct)\nO\nX\nO\nO\nMAD\nO\nO\nX\nX\nMADKE\nO\nO\nO\nX\nTool-MAD\nO\nO\nO\nO\nTABLE III\nMAIN RESULTS ON FOUR FACT VERIFICATION DATASETS (FEVER, FEVEROUS, FAVI-Q, AND AVERITEC). TOOL-MAD CONSISTENTLY IMPROVES\nPERFORMANCE ACROSS BOTH PROPRIETARY (E.G., GPT-4O, DEEPSEEKR1) AND OPEN-SOURCE (E.G., LLAMA-3.3-70B) MODELS, OUTPERFORMING\nOTHER MULTI-AGENT DEBATE FRAMEWORKS. AVERAGE SCORES ARE REPORTED FOR OVERALL COMPARISON. BOLD INDICATES THE HIGHEST EXACT\nMATCH SCORE WITHIN EACH BASE MODEL GROUP (ABOVE: GPT-4 VARIANTS AND DEEPSEEKR1; BELOW: LLAMA).\nModel\nFEVER\nFEVEROUS\nFAVIQ\nAVERITEC\nAverage\nDeepseekR1\n71.0\n67.5\n77.0\n49.0\n66.1\nGPT-4o\n69.5\n54.5\n68.0\n43.5\n58.9\nGPT-4o-mini\n62.0\n37.0\n56.0\n33.0\n47.0\n+ CoT(Zero-shot)\n66.5\n31.0\n67.0\n33.5\n49.5\n+ Single Agent(ReAct)\n62.0\n24.0\n66.0\n24.0\n44.0\n+ MAD\n71.0\n36.5\n68.0\n36.0\n52.9\n+ MADKE\n72.0\n66.0\n75.5\n58.5\n68.0\n+ Tool-MAD\n73.0\n71.5\n77.5\n62.0\n71.0\nLlama-3.3-70B(Inst)\n69.5\n49.0\n64.0\n51.0\n58.4\n+ CoT(Zero-shot)\n71.0\n51.0\n68.5\n32.0\n55.6\n+ Single Agent(ReAct)\n73.5\n51.0\n65.5\n35.5\n56.4\n+ MAD\n54.0\n31.5\n58.5\n39.5\n45.9\n+ MADKE\n62.5\n61.0\n71.5\n31.0\n56.5\n+ Tool-MAD\n74.0\n77.0\n78.5\n66.5\n74.0\nA. Datasets\nWe evaluate our method across a wide range of tasks\nto assess both factual accuracy and cross-task flexibility.\nFor fact verification, we utilize four widely used benchmark\ndatasets: FEVER [27] and FEVEROUS [28], which are based\non Wikipedia-derived claims, as well as FAVIQ [29] and\nAVERITEC [30], which include claims from real world con-\ntexts. These datasets collectively span diverse domains and\nclaim structures, allowing for a robust assessment of factual\nverification capabilities. To evaluate the flexibility of our\nframework beyond fact verification, we additionally include\nmedical QA datasets such as MEDQA [31] and PubMedQA\n[32], which focus on domain-specific clinical and biomedical\nreasoning. A detailed summary of all datasets, including scale,\ndomain, and task type, is provided in Table I.\nB. Models\nWe employ GPT-4o-mini [33] and Llama-3.3-70B-Instruct-\nTurbo [34] as backbone models in our Tool-MAD framework.\nWe also utilize GPT-4o [33], a larger variant of GPT-4o-mini,\nfor performance comparison. To further assess the effective-\nness of the proposed framwork, we conduct a performance\ncomparison with DeepseekR1 [35], a representative reasoning\nbased model.\nBaseline Model We evaluate Tool-MAD by comparing it\nwith single agent reasoning and multi-agent debate baselines.\nA brief summary is provided below:\n• Zero-shot CoT [36] : Zero-shot CoT is to induce rea-\nsoning with the prompt “Let’s think step by step”.\n• Single Agent [37] : The single-agent baseline is based on\nthe ReAct framework, using RAG for external knowledge\nTABLE IV\nACCURACY AND 95% CONFIDENCE INTERVALS ACROSS DATASETS.\n(BACKBONE MODEL : GPT-4O-MINI)\nDataset\nAcc.\n95% C.I\nFEVER\n0.73\n(0.67, 0.79)\nFEVEROUS\n0.72\n(0.66, 0.78)\nFAVIQ\n0.78\n(0.72, 0.83)\nAVeriTeC\n0.62\n(0.56, 0.69)\nretrieval. ReAct enables iterative reasoning via tool use\nand feedback.\n• MAD [7] : MAD is a framework that uses an interactive\n“tit for tat” debate structure, motivated by the Degener-\nation of Thought observed in single agents, even when\nself-reflection is applied.\n• MADKE [9] : MADKE addresses the limitations of\ntraditional MAD methods that rely solely on internal\nknowledge by incorporating a static evidence pool re-\ntrieved prior to the debate.\nTable II provides a structural comparison of these baselines,\nhighlighting their differences across four key dimensions:\nintrinsic reasoning capability, multi-agent interaction, retrieval\nusage, and query formulation. As shown in the table, Tool-\nMAD is the only framework that simultaneously supports\nall four components by integrating structured debate, itera-\ntive retrieval, and adaptive query rewriting. In contrast, prior\nbaselines typically cover only a subset of these capabilities,\nresulting in more limited overall functionality.\nC. Fact Verification Experiments\nWe evaluate multi-agent debate models on four fact verifi-\ncation benchmark datasets. Table III presents the experimental\n"}, {"page": 8, "text": "8\nTABLE V\nEXACT MATCH (EM) SCORES OF DIFFERENT MULTI-AGENT DEBATE\nFRAMEWORKS ON MEDQA AND PUBMEDQA. BOLD INDICATES THE\nHIGHEST SCORE.\nModel\nMedQA\nPubMedQA\nMAD\n58.0\n22.5\nMADKE\n74.0\n21.5\nTool-MAD\n77.0\n29.0\nresults. Tool-MAD, using the lightweight GPT-4o-mini as its\nbackbone, outperforms the more powerful GPT-4o across all\nevaluated benchmark datasets. Notably, it shows a 18.5%\nimprovement on AVERITEC, the dataset with the highest label\ncomplexity, demonstrating the robustness of our framework\nunder more challenging verification settings. Compared to\nDeepSeekR1, Tool-MAD consistently achieves higher accu-\nracy across all tasks, with improvements of up to 13%. Tool-\nMAD achieves comparable performance with the open-source\nLlama-3.3-70B backbone, confirming its robustness across\nmodel backbones.\nTool-MAD outperforms other multi-agent debate frame-\nworks, achieving up to 35.0% and 5.5% improvements over\nMAD and MADKE, respectively, with average gains of 18.1%\nand 3.0%. These results demonstrate that Tool-MAD delivers\nsuperior performance in fact verification tasks compared to\nexisting multi-agent frameworks, and further highlight its\narchitectural advantage over MADKE, which also leverages\nexternal knowledge.\nTo better understand the observed performance gap, we\nanalyze the limitations of each baseline system. The Single\nAgent, despite leveraging the ReAct framework [37] and ex-\nternal tools, frequently generates incorrect answers due to the\nretrieval of irrelevant or incomplete documents. MAD [7]im-\nproves upon this through multi-agent debate, but still struggles\nwhen evidence is insufficient, often defaulting to a generic\n“Not Enough Info” response. MADKE [9] achieves relatively\nstable performance by relying on a static evidence pool re-\ntrieved prior to the debate, yet fails to generalize well on more\ncomplex datasets such as AVERITEC and FEVEROUS, where\ndynamic adaptation is required. These results highlight the\narchitectural advantage of Tool-MAD, which enables agents\nto iteratively retrieve new and contextually relevant evidence\nthroughout the debate, leading to improved factual reasoning.\nAdditionally, we report 95% bootstrap confidence intervals\nfor Tool-MAD using GPT-4o-mini as the backbone. Due to the\ncomputational cost of multi-round multi-agent debate—where\neach query triggers multiple LLM inferences—conducting\nlarge-scale repeated trials is prohibitively expensive. The\nreported confidence intervals (Table II) therefore serve to\nsupplement the robustness of our findings by quantifying the\nvariability of the results under resampling. The corresponding\nresults are presented in Table IV.\nD. Flexibility\nThis experiment focuses on evaluating whether Tool-MAD\ncan effectively adapt and maintain consistent performance\nwhen external tools are changed or applied to different do-\nmains. For comparison, we conduct flexibility experiments\nusing the frameworks that showed competitive performance in\nFact Verification Experiments section, in place of single-agent\nbaselines that demonstrated relatively lower performance. We\nevaluate our framework on two medical QA datasets: MEDQA,\nwhich focuses on clinical multiple-choice questions, and Pub-\nMedQA, which targets biomedical fact verification. To test\nthe framework’s extensibility across tools, we configure the\nMEDQA experiment using a PubMed-based RAG corpus,\nwhile the PubMedQA setting replaces the standard search API\nwith OpenAlex [38], an open scholarly database.\nIn the PubMedQA pipeline, the agent extracts keywords\nfrom each query, retrieves three relevant abstracts, summarizes\nthem using BERT [39], and incorporates the summaries into\nthe debate. Detailed information on the PubMedQA workflow\nis provided in the figure 7.\nAs shown in Table V, Tool-MAD outperforms other multi-\nagent debate frameworks on both datasets. Notably, while\nMADKE performs worse than MAD on PubMedQA despite\nincorporating external knowledge, Tool-MAD achieves strong\nresults and maintains consistent performance even when the\nunderlying tools are changed.\nBeyond these quantitative results, the strong performance of\nTool-MAD on both MEDQA and PubMedQA can be attributed\nto the complementary nature of its heterogeneous retrieval\nsources. Clinical QA tasks often require both up-to-date evi-\ndence, such as newly published studies or evolving treatment\nguidelines, and structured biomedical knowledge grounded in\nestablished definitions or mechanistic explanations. Search-\nbased retrieval is well suited for capturing the former, whereas\nRAG-based retrieval excels at providing the latter. By inte-\ngrating both types of information within the debate, Tool-\nMAD enables agents to form more comprehensive and reliable\narguments compared to methods that rely on a single evidence\nmodality.\nAnother noteworthy aspect is that Tool-MAD maintains\nstable performance despite the domain shift from Wikipedia-\nstyle fact verification to biomedical question answering. This\nrobustness arises from the framework’s iterative retrieval\nmechanism, which allows agents to refine their queries and\nadjust their evidence sources as domain-specific reasoning\nchallenges emerge. The ability to dynamically cross-validate\nretrieved evidence across rounds helps mitigate errors intro-\nduced by unfamiliar terminology or dataset-specific retrieval\nnoise.\nFinally, the debate structure used in Tool-MAD naturally\naligns with multi-source reasoning patterns observed in clini-\ncal decision-support workflows. In practice, healthcare profes-\nsionals often synthesize information from both standardized\nguidelines and recent studies, and Tool-MAD mirrors this\nprocess by enabling different agents to contribute distinct\nyet complementary evidence. The use of faithfulness and\nanswer relevance as stability indicators further helps filter\nout unsupported or clinically unreliable responses, providing\nan additional layer of safety when operating in high-stakes\nmedical domains.\n"}, {"page": 9, "text": "9\nTABLE VI\nPERFORMANCE COMPARISON OF DIFFERENT AGENT COMBINATIONS ON FEVER, FEVEROUS, FAVIQ AND AVERITEC DATASETS. TOOL-MAD\nACHIEVES THE BEST OVERALL RESULTS ACROSS COMBINATIONS OF RAG, SEARCH, AND VANILA AGENTS. BOLD INDICATES THE HIGHEST EXACT\nMATCH SCORE.\nModel\nFEVER\nFEVEROUS\nFAVIQ\nAVERITEC\nVANILA + VANILA\n62.0\n40.0\n60.0\n45.0\nRAG + VANILA\n65.5\n57.5\n63.5\n58.0\nSEARCH + VANILA\n60.5\n43.0\n63.5\n53.5\nRAG + RAG\n67.5\n60.5\n68.5\n56.5\nSEARCH + SEARCH\n67.0\n60.5\n68.5\n55.5\nTool-MAD (RAG + SEARCH)\n73.0\n71.5\n77.5\n62.0\nOpenAlex API\nTool-MAD\n DistilBERT\n2. Retrieve Top-k Documents\n(Paper abstract)\n1. Keyword Selection\n3. Summerization\nFig. 7. Workflow diagram of the paper retrieval API used in the PubMedQA experiment\n1\n2\n3\n4\nDebate Rounds\n65.0\n66.0\n67.0\n68.0\n69.0\n70.0\n71.0\n72.0\n73.0\nExact match score (%)\nAccuracy\nPeak\nFig. 8. Exact Match performance of Tool-MAD on the FEVER dataset across\ndifferent debate rounds.\nE. Ablation Study\nCombination of Agents To evaluate the impact of different\nagent configurations, we compare their performance across the\nFEVER, FEVEROUS, and FAVIQ benchmark datasets. All\nagents follow the same Tool-MAD procedure, with the only\nvariation being the external tools they employ. Specifically,\nwe define three types of agents: a base agent without any\nexternal tools (VANILLA), an agent using retrieval-augmented\ngeneration (RAG), and an agent equipped with a web-based\nsearch API (SEARCH).\nThe results are presented in Table VI. As shown in the ta-\nble, configurations that incorporate external tools consistently\noutperform those without tools, indicating that tool integra-\ntion improves performance in fact verification tasks. Among\nall agent combinations tested, the original Tool-MAD setup\n(RAG + SEARCH) achieved the highest average accuracy,\ndemonstrating its robustness and effectiveness.\nFEVER\nFEVEROUS\nFAVIQ\nAVERITEC\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\nExact Match Score (%)\n73.0\n71.5\n77.5\n62.0\n71.0\n69.0\n77.5\n61.0\nw QF\nw/o QF\nFig. 9.\nExact Match performance comparison with and without query\nformulation (QF) across four benchmark datasets.\nIn\nparticular,\nconfigurations\nlike\nRAG+RAG\nor\nSEARCH+SEARCH outperformed the VANILLA setup, but\nstill fell short of the hybrid configuration (RAG+SEARCH).\nThis suggests that relying on a single type of tool can lead\nto redundant retrievals and limit the diversity of evidence,\nthereby constraining overall performance improvements.\nTo further interpret these results, we note that the hybrid\nconfiguration benefits from the complementary characteris-\ntics of the two external tools. RAG-based retrieval provides\nsemantically aligned, context-rich documents, while search-\nbased retrieval offers broader coverage and access to rapidly\nupdated information. These heterogeneous evidence sources\nallow the agents to explore distinct perspectives during the\ndebate, reducing shared blind spots and producing more re-\nliable conclusions. In contrast, homogeneous configurations\ntend to retrieve overlapping content, which limits the diversity\nof arguments and constrains potential performance gains.\n"}, {"page": 10, "text": "10\nFEVER\nFEVEROUS\nFAVIQ\nAVERITEC\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\nExact Match Score (%)\n73.0\n71.5\n77.5\n62.0\n70.5\n69.0\n73.0\n61.5\nw Stability Score\nw/o Stability Score\nFig. 10. Faithfulness and Answer Relevance scores across four datasets, based\non the final predictions used for Exact Match evaluation, comparing models\nwith and without scoring-based feedback.\nNumber of Debate Rounds We investigate how the number\nof debate rounds affects the overall performance of Tool-\nMAD. To analyze the effect of debate round, we conduct\nan experiment on the FEVER dataset. The results are shown\nin Figure\n8. We observe that performance is lowest when\nonly the Init Round (Round 1) is used. Accuracy gradually\nimproves up to round 3 but slightly declines at round 4.\nBased on this observation, we set the threshold for the number\nof debate rounds to 3 in order to balance performance and\nefficiency.\nThis pattern suggests that although additional rounds help\nrefine arguments and retrieve more relevant evidence, exces-\nsive debate depth may introduce unnecessary speculation or\namplify intermediate reasoning errors. Such behavior aligns\nwith observations in prior multi-step reasoning research, where\nprolonged interactions can lead to unstable or redundant\nargumentation. Therefore, three rounds provide sufficient op-\nportunity for evidence refinement without causing degradation\nin the quality of the debate.\nEffect of Query Formulation We conducted ablation\nexperiments to evaluate the effectiveness of dynamic query\nformulation. In Tool-MAD, each agent can independently\nrevise its query at every debate round to retrieve more relevant\ndocuments. To isolate the impact of this mechanism, we\ncompare the standard Tool-MAD setup with dynamic query\nupdates at each round (w/ Query Formulation) to a variant that\nuses only the initial claim for retrieval throughout all rounds\n(w/o Query Formulation).\nAs shown in Figure 9, dynamic query formulation improves\nperformance on all datasets except FAVIQ, where the score re-\nmains unchanged. The most notable improvement is observed\non FEVER (+2.0) and FEVEROUS (+2.5), followed by a\nmodest gain on AVERITEC (+1.0). These results demonstrate\nthat formulating queries in response to the opponent’s argu-\nment helps uncover more relevant evidence and contributes to\nimproved fact verification accuracy.\nOne possible explanation for this behavior is the difference\nin dataset characteristics. FEVER and FEVEROUS primarily\ncontain entity-centric factual claims where emphasizing spe-\ncific entities or relations during query reformulation leads to\nmore targeted retrieval. In contrast, FAVIQ includes broader\nFEVER\nFEVEROUS\nFAVIQ\nAVERITEC\n0.6 0.7 0.8 0.9 1.0\nFaithfulness\nCorrect\nIncorrect\nFEVER\nFEVEROUS\nFAVIQ\nAVERITEC\n0.6 0.7 0.8 0.9 1.0\nAnswer Relevance\nCorrect\nIncorrect\nFig. 11. The figure presents the average Faithfulness and Answer Relevance\nscores for both correct and incorrect answers across four datasets.\ninformation-seeking questions that often require diverse or\nmulti-hop evidence, meaning that the initial query already\ncaptures most of the relevant search space. As a result, further\nrefinement has limited impact on FAVIQ, whereas datasets\nwith more structured claim formulations benefit more from\nadaptive querying.\nEffect of Stability Score We conducted ablation experi-\nments to assess the effectiveness of incorporating faithfulness\nand answer relevance (stability score) as dynamic evaluation\nsignals. In Tool-MAD, each agent’s response is evaluated at\nevery debate round using these two metrics, and if either score\nfalls below predefined thresholds, the round is considered\ninconclusive and is repeated. This mechanism enables the\nframework to dynamically filter out low-quality responses\nthroughout the debate.\nTo isolate the impact of this score-based filtering, we\ncompare the standard Tool-MAD configuration that utilizes\nfaithfulness and answer relevance scores to guide debate pro-\ngression (w/ Scoring Feedback) with a variant that skips score-\nbased validation and progresses through all rounds regardless\nof response quality (w/o Scoring Feedback).\nThe results in Figure 10 highlight that scoring feedback\nconsistently improves EM performance across all datasets,\nwith the most significant gain observed on FAVIQ (+4.5%).\nThese findings suggest that faithfulness and answer relevance\nfunction not merely as evaluation metrics but as crucial control\nsignals that help maintain factual consistency and question\nrelevance throughout multi-round debates.\nA notable aspect of this trend is that FAVIQ benefits\nparticularly strongly from stability-based filtering. Because\nmany FAVIQ questions are open-ended and sensitive to topical\nalignment, answers that deviate slightly from the intended\nquestion are often incorrect even when they contain fac-\ntually valid information. In such cases, answer relevance\nbecomes a highly predictive indicator of correctness, while\nfaithfulness prevents unsupported reasoning from propagating\nacross rounds. Together, these signals help stabilize the debate\nprocess and reduce both topic drift and hallucinated content.\nF. Analysis of Stability Score\nFigure 11 compares the average Faithfulness and Answer\nRelevance scores across four datasets, based on whether the\n"}, {"page": 11, "text": "11\nagent’s prediction was correct or incorrect. In all datasets, re-\nsponses classified as correct consistently exhibit higher scores\non both metrics, indicating a strong correlation between these\nscores and the Exact Match (EM) accuracy. These findings\nsuggest that the scores can be used not only as post-hoc\nevaluation metrics but also as internal signals for determining\nanswer correctness or filtering responses in real-time systems.\nIn particular, the FAVIQ and AVERITEC datasets show a\npronounced gap in Answer Relevance scores between correct\nand incorrect responses, suggesting that alignment with the\noriginal user query plays a crucial role in predicting cor-\nrectness for complex question answering tasks. These results\nhighlight that Faithfulness and Answer Relevance serve as\nstrong and reliable indicators for evaluating response quality\nand factual consistency. They can also contribute to quality\ncontrol and final decision-making mechanisms in multi-agent\ndebate systems.\nV. DISCUSSION\nA. Structural Effects of Query Rewriting\nOne of the most salient patterns observed in our analysis\nis that the gradual query rewriting performed throughout the\ndebate plays a crucial role in improving verification accuracy.\nInitial queries typically take a broad, holistic form that mirrors\nthe surface structure of the claim, but as the debate progresses,\nthe agents refine their queries by decomposing attributes,\nrestructuring entity relations, or explicitly specifying missing\ndetails. For instance, in the Trinidad and Tobago Guardian\nownership case, the first-round query asked about the newspa-\nper’s overall “ownership history,” which retrieved only partial\ninformation. In later rounds, the agents rewrote the query to\nisolate the key relation: “the relationship between Guardian\nMedia Limited and ANSA McAL,” which surfaced docu-\nments describing the hierarchical ownership chain. This shift\ntransformed an initially ambiguous or contradictory prediction\ninto a stable SUPPORTS decision. A similar phenomenon\noccurred in the Mel Ott RBI case. Early queries retrieved\nonly season-level statistics, leading to NOT ENOUGH INFO\n(NEI), but rewriting the query into “total career RBIs” enabled\nretrieval of the exact aggregate value (1860), allowing the\nsystem to recover from partial evidence and converge on the\ncorrect label. These examples demonstrate that the multi-round\nquery rewriting process is not merely repeated retrieval, but\na structured mechanism that incrementally decomposes and\nreconstructs the claim, guiding the system toward increasingly\nprecise evidence.\nB. Structural Effects of Stability scores\nA second notable characteristic identified in the analysis\nis the stabilizing effect of round-wise Stability scores, which\nevaluate each answer’s faithfulness (document–answer consis-\ntency) and answer relevance (question–answer consistency).\nIt is common for the RAG-based answer and the Search-\nbased answer to propose different labels for the same claim.\nRather than selecting based on majority or repetition, the\nsystem computes stability scores for both answers at every\nround and tends to choose the label whose supporting answer\nmaintains consistently higher semantic alignment with the\nretrieved evidence and the original question. For example,\nin the Stomatochaeta classification case, the RAG answer\nrepeatedly predicted SUPPORTS while the Search answer\npredicted REFUTES. Across multiple rounds, however, the\nSearch answer exhibited higher faithfulness scores and more\nstable question–answer relevance, prompting the model to\nadopt REFUTES as the final label. The Obispo first ascent\ncase further illustrates this effect. Although the first round\nproduced NEI due to insufficient evidence, subsequent rounds\nproduced SUPPORTS answers with steadily increasing sta-\nbility scores, eventually overriding the initial uncertainty and\nyielding a confident final decision. Because stability score\ndirectly measures whether an answer is grounded in retrieved\nevidence, it remains robust even under noisy retrieval con-\nditions, assigning greater weight to evidence-aligned answers\nthat consistently satisfy semantic constraints. Thus, stability\nscore provides a meta-evaluative layer that re-assesses each\ncandidate answer based on evidence quality and coherence,\nsubstantially enhancing convergence reliability compared to\nsingle-agent RAG systems.\nC. Multi-Agent Debate as a Structural Exploration Mecha-\nnism\nFinally, the analysis shows that the debate process ex-\nhibits an emergent capacity to explore structural properties\nof evidence, temporal shifts, and conflicting information even\nwithout an explicit moderator. A representative example is\nthe Fort Myers Police Department case, where the Search\ntool retrieved up-to-date information indicating that the police\nchief had recently passed away, implying REFUTES, while\nthe ground-truth label, anchored to an earlier timestamp,\nremained SUPPORTS. In such scenarios, the agents did not\nmerely default to the labeled answer but instead engaged\nin a comparative evaluation of evidence recency, reliability,\nand contextual relevance. In other cases, the agents grounded\ntheir arguments in different retrieval sources and iteratively\ncritiqued one another’s evidence, adjusting their interpretations\nin later rounds. This behavior suggests that the system is not\nsimply producing parallel answers but is performing a form\nof quasi-critical reasoning that assesses the coherence, con-\nflict, and incompleteness of available evidence. Such patterns\nindicate that multi-agent debate has the potential to evolve\nbeyond static fact verification toward a richer inference system\ncapable of handling temporal drift, evidence conflicts, and\nmulti-faceted claims, conditions that more closely resemble\nreal-world reasoning tasks.\nVI. CONCLUSION\nIn this work, we introduced Tool-MAD, a multi-agent de-\nbate framework that enhances factual verification by assigning\ndistinct external tools to each agent and enabling adaptive,\ncontext-aware retrieval throughout the debate. Across diverse\nfact-verification and medical QA benchmarks, Tool-MAD\nachieved up to 35% performance gains over existing debate\nsystems, with ablation studies confirming the importance of\ntool diversity, dynamic query formulation, and our Stability\n"}, {"page": 12, "text": "12\nScore, which functions as an internal control signal rather than\na retrospective metric. These results demonstrate that com-\nbining structured debate with adaptive tool use significantly\nimproves factual grounding and robustness. Tool-MAD’s sta-\nble performance across domains and retrieval configurations\nindicates strong potential for extension to richer tool ecosys-\ntems and more advanced judge models. Overall, Tool-MAD\nprovides a unified and evidence-sensitive framework for trans-\nparent and reliable multi-agent reasoning in high-stakes factual\nverification tasks.\nREFERENCES\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[2] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\nCheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language models\nfor dialog applications,” arXiv preprint arXiv:2201.08239, 2022.\n[3] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid,\nJ. Tompson, Q. Vuong, T. Yu, W. Huang et al., “Palm-e: An embodied\nmultimodal language model,” 2023.\n[4] J. Li, J. Chen, R. Ren, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen,\n“The dawn after the dark: An empirical study on factuality hallucination\nin large language models,” arXiv preprint arXiv:2401.03205, 2024.\n[5] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[6] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflex-\nion: Language agents with verbal reinforcement learning,” Advances in\nNeural Information Processing Systems, vol. 36, pp. 8634–8652, 2023.\n[7] T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, S. Shi,\nand Z. Tu, “Encouraging divergent thinking in large language models\nthrough multi-agent debate,” in Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing, Y. Al-Onaizan,\nM. Bansal, and Y.-N. Chen, Eds.\nMiami, Florida, USA: Association\nfor Computational Linguistics, Nov. 2024, pp. 17 889–17 904. [Online].\nAvailable: https://aclanthology.org/2024.emnlp-main.992/\n[8] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, “Improving\nfactuality and reasoning in language models through multiagent debate,”\nin Forty-first International Conference on Machine Learning, 2023.\n[9] H. Wang, X. Du, W. Yu, Q. Chen, K. Zhu, Z. Chu, L. Yan, and\nY. Guan, “Learning to break: Knowledge-enhanced reasoning in multi-\nagent debate system,” Neurocomputing, vol. 618, p. 129063, 2025.\n[10] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nneural information processing systems, vol. 33, pp. 9459–9474, 2020.\n[11] S. Es, J. James, L. E. Anke, and S. Schockaert, “Ragas: Automated\nevaluation of retrieval augmented generation,” in Proceedings of the 18th\nConference of the European Chapter of the Association for Computa-\ntional Linguistics: System Demonstrations, 2024, pp. 150–158.\n[12] M. Minsky, Society of mind.\nSimon and Schuster, 1986.\n[13] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao,\n“Reflexion: language agents with verbal reinforcement learning,”\nin Advances in Neural Information Processing Systems, A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,\nEds.,\nvol.\n36.\nCurran\nAssociates,\nInc.,\n2023,\npp.\n8634–8652.\n[Online].\nAvailable:\nhttps://proceedings.neurips.cc/paper files/paper/\n2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf\n[14] J. C.-Y. Chen, S. Saha, and M. Bansal, “Reconcile: Round-table con-\nference improves reasoning via consensus among diverse llms,” arXiv\npreprint arXiv:2309.13007, 2023.\n[15] Y. Chen, Q. Fu, Y. Yuan, Z. Wen, G. Fan, D. Liu, D. Zhang, Z. Li,\nand Y. Xiao, “Hallucination detection: Robustly discerning reliable\nanswers in large language models,” in Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management,\n2023, pp. 245–255.\n[16] Y. Zhao, J. Zhang, I. Chern, S. Gao, P. Liu, J. He et al., “Felm:\nBenchmarking factuality evaluation of large language models,” Advances\nin Neural Information Processing Systems, vol. 36, pp. 44 502–44 523,\n2023.\n[17] S. Min, K. Krishna, X. Lyu, M. Lewis, W.-t. Yih, P. W. Koh, M. Iyyer,\nL. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic\nevaluation of factual precision in long form text generation,” arXiv\npreprint arXiv:2305.14251, 2023.\n[18] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdh-\nery, and D. Zhou, “Self-consistency improves chain of thought reasoning\nin language models,” arXiv preprint arXiv:2203.11171, 2022.\n[19] P. Manakul, A. Liusie, and M. Gales, “Selfcheckgpt: Zero-resource\nblack-box hallucination detection for generative large language models,”\nin Proceedings of the 2023 conference on empirical methods in natural\nlanguage processing, 2023, pp. 9004–9017.\n[20] T.\nSchick,\nJ.\nDwivedi-Yu,\nR.\nDessi,\nR.\nRaileanu,\nM.\nLomeli,\nE.\nHambro,\nL.\nZettlemoyer,\nN.\nCancedda,\nand\nT.\nScialom,\n“Toolformer: Language models can teach themselves to use tools,”\nin Advances in Neural Information Processing Systems, A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,\nEds., vol. 36.\nCurran Associates, Inc., 2023, pp. 68 539–68 551.\n[Online].\nAvailable:\nhttps://proceedings.neurips.cc/paper files/paper/\n2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf\n[21] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face,” Advances\nin Neural Information Processing Systems, vol. 36, pp. 38 154–38 180,\n2023.\n[22] Y. Lu, H. Yu, and D. Khashabi, “Gear: Augmenting language mod-\nels with generalizable and efficient tool resolution,” arXiv preprint\narXiv:2307.08775, 2023.\n[23] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and\nP. Schwaller, “Chemcrow: Augmenting large-language models with\nchemistry tools,” arXiv preprint arXiv:2304.05376, 2023.\n[24] S. Gao, A. Fang, Y. Huang, V. Giunchiglia, A. Noori, J. R. Schwarz,\nY. Ektefaie, J. Kondic, and M. Zitnik, “Empowering biomedical discov-\nery with ai agents,” Cell, vol. 187, no. 22, pp. 6125–6151, 2024.\n[25] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval\naugmented language model pre-training,” in International conference\non machine learning.\nPMLR, 2020, pp. 3929–3938.\n[26] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li,\nX. Xu et al., “Milvus: A purpose-built vector data management system,”\nin Proceedings of the 2021 International Conference on Management of\nData, 2021, pp. 2614–2627.\n[27] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever:\na large-scale dataset for fact extraction and verification,” arXiv preprint\narXiv:1803.05355, 2018.\n[28] R.\nAly,\nZ.\nGuo,\nM.\nSchlichtkrull,\nJ.\nThorne,\nA.\nVlachos,\nC. Christodoulopoulos, O. Cocarascu, and A. Mittal, “Feverous: Fact\nextraction and verification over unstructured and structured information,”\narXiv preprint arXiv:2106.05707, 2021.\n[29] J. Park, S. Min, J. Kang, L. Zettlemoyer, and H. Hajishirzi, “Faviq:\nFact verification from information-seeking questions,” arXiv preprint\narXiv:2107.02153, 2021.\n[30] M. Schlichtkrull, Z. Guo, and A. Vlachos, “Averitec: A dataset for\nreal-world claim verification with evidence from the web,” Advances\nin Neural Information Processing Systems, vol. 36, pp. 65 128–65 167,\n2023.\n[31] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits,\n“What disease does this patient have? a large-scale open domain question\nanswering dataset from medical exams,” Applied Sciences, vol. 11,\nno. 14, p. 6421, 2021.\n[32] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, “Pubmedqa:\nA dataset for biomedical research question answering,” arXiv preprint\narXiv:1909.06146, 2019.\n[33] OpenAI, “Introducing gpt-4o: our fastest and most affordable flagship\nmodel,” https://platform.openai.com/docs/guides/vision, 2024, accessed:\n2025-05-19.\n[34] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,\nA. Letman, A. Mathur, A. Schelten, A. Vaughan et al., “The llama 3\nherd of models,” arXiv preprint arXiv:2407.21783, 2024.\n[35] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma,\nP. Wang, X. Bi et al., “Deepseek-r1: Incentivizing reasoning capability\nin llms via reinforcement learning,” arXiv preprint arXiv:2501.12948,\n2025.\n[36] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large lan-\nguage models are zero-shot reasoners,” Advances in neural information\nprocessing systems, vol. 35, pp. 22 199–22 213, 2022.\n[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” in\nInternational Conference on Learning Representations (ICLR), 2023.\n"}, {"page": 13, "text": "13\n[38] J. Priem, H. Piwowar, and R. Orr, “Openalex: A fully-open index\nof scholarly works, authors, venues, institutions, and concepts,” arXiv\npreprint arXiv:2205.01833, 2022.\n[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” in Pro-\nceedings of the 2019 conference of the North American chapter of the\nassociation for computational linguistics: human language technologies,\nvolume 1 (long and short papers), 2019, pp. 4171–4186.\n"}]}