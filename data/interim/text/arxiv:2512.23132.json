{"doc_id": "arxiv:2512.23132", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.23132.pdf", "meta": {"doc_id": "arxiv:2512.23132", "source": "arxiv", "arxiv_id": "2512.23132", "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems", "authors": ["Armstrong Foundjem", "Lionel Nganyewou Tidjon", "Leuson Da Silva", "Foutse Khomh"], "published": "2025-12-29T01:27:19Z", "updated": "2025-12-29T01:27:19Z", "summary": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.23132v1", "url_pdf": "https://arxiv.org/pdf/2512.23132.pdf", "meta_path": "data/raw/arxiv/meta/2512.23132.json", "sha256": "1743d8982a678ce9a6d1497aeaa9448b6b5c7035b8f85dc974a6caf5b8aac9ee", "status": "ok", "fetched_at": "2026-02-18T02:23:38.007835+00:00"}, "pages": [{"page": 1, "text": "1\nMulti-Agent Framework for Threat Mitigation and\nResilience in AI–Based Systems\nArmstrong Foundjem, sMIEEE, Lionel Nganyewou Tidjon, sMIEEE, Leuson Da Silva,\nand Foutse Khomh, sMIEEE\nAbstract\nMachine learning (ML) increasingly underpins foundation models and autonomous pipelines in high-stakes domains\nsuch as finance, healthcare, and national infrastructure, rendering these systems prime targets for sophisticated\nadversarial threats. Attackers now leverage advanced Tactics, Techniques, and Procedures (TTPs) spanning data\npoisoning, model extraction, prompt injection, automated jailbreaking, training data exfiltration, and—more recently—\npreference-guided black-box optimization that exploits models’ own comparative judgments to craft successful attacks\niteratively. These emerging text-only, query-based methods demonstrate that larger and better-calibrated models can\nbe paradoxically more vulnerable to introspection-driven jailbreaks and cross-modal manipulations. While traditional\ncybersecurity frameworks offer partial mitigation, they lack ML-specific threat modeling and fail to capture evolving\nattack vectors across foundation, multimodal, and federated settings. Objective: This research empirically characterizes\nmodern ML security risks by identifying dominant attacker TTPs, exposed vulnerabilities, and lifecycle stages most\nfrequently targeted in foundation-model, multimodal, and retrieval-augmented (RAG) pipelines. The study also assesses\nthe scalability of current defenses against generative and introspection-based attacks, highlighting the need for adaptive,\nML-aware security mechanisms. Methods: We conduct a large-scale empirical analysis of ML security, extracting 93\ndistinct threats from multiple sources: real-world incidents in MITRE ATLAS (26), the AI Incident Database (12), and\npeer-reviewed literature (55), supplemented by 854 ML repositories from GitHub and the Python Advisory database.\nA multi-agent reasoning system with enhanced Retrieval-Augmented Generation (RAG)—powered by ChatGPT-4o\n(temperature 0.4)—automatically extracts TTPs, vulnerabilities, and lifecycle stages from over 300 scientific articles\nusing evidence-grounded reasoning. The resulting ontology-driven threat graph supports cross-source validation and\nlifecycle mapping. Results: Our analysis uncovers multiple unreported threats beyond current ATLAS coverage,\nincluding model-stealing attacks against commercial LLM APIs, data leakage through parameter memorization, and\npreference-guided query optimization enabling text-only jailbreaks and multimodal adversarial examples. Gradient-\nbased obstinate attacks, MASTERKEY automated jailbreaking, federated learning poisoning, diffusion backdoor\nembedding, and preference-oriented optimization leakage emerge as dominant TTPs, disproportionately impacting\npretraining and inference. Graph-based dependency analysis shows that specific ML libraries and model hubs exhibit\ndense vulnerability clusters lacking effective issue-tracking and patch-propagation mechanisms. Conclusion: This study\nunderscores the urgent need for adaptive, ML-specific security frameworks that address introspection-based and\npreference-guided attacks alongside classical adversarial vectors. Robust dependency management, automated threat\nintelligence, and continuous monitoring are essential to mitigate supply-chain and inference-time risks throughout\nthe ML lifecycle. By unifying empirical evidence from incidents, literature, and repositories, this research delivers a\ncomprehensive threat landscape for next-generation AI systems and establishes a foundation for proactive, multi-agent\nsecurity governance in the era of large-scale and generative AI.\nIndex Terms\nCybersecurity; Machine learning security; Vulnerabilities; Threat assessment; Tactics, techniques, and procedures\n(TTPs); Multi-agent systems; Artificial intelligence.\nI. Introduction\nNowadays, Machine Learning (ML) is achieving significant success in dealing with various complex problems in\nsafety-critical domains such as healthcare [1] aviation [2], automotive [3], railways [4], and space [5]. ML has also been\napplied in cybersecurity to detect threatening anomalous behaviors such as spam, malware, and malicious URLs [6],\nallowing a system to respond to real-time inputs containing both normal and suspicious data and learn to reject\nmalicious behavior. While ML is strengthening defense systems, it also helps threat actors improve their tactics,\ntechniques, and procedures (TTPs) and expand their attack surface. Attackers leverage the black-box nature of ML\nmodels and manipulate input data to affect their performance [7], [8], [9], [10].\nEarly work [10], [7], [11], [12], [13], [14], [15], [16], [17], [9] outlined ML attacks and defenses targeting different phases\nof the ML lifecycle, i.e., input data, training, inference, and monitoring. ML-based systems are also often deployed on-\npremise or on cloud service providers, which increases attack vectors and makes them vulnerable to traditional attacks\nat different layers, like software, system, and network levels. At the software level, ML-based systems are vulnerable\nto operating system (OS) attacks since attackers can exploit the OS. At the system level, ML-based systems are\nAll authors are aﬀiliated with the Department of Computer and Software Engineering, Polytechnique Montreal, QC H3T 0A3, e-mail:\n{a.foundjem,lionel.tidjon,leuson-mario-pedro.da-silva,foutse.khomh}@polymtl.ca.\narXiv:2512.23132v1  [cs.CR]  29 Dec 2025\n"}, {"page": 2, "text": "2\nvulnerable to attacks, including CPU side-channel [18] and memory-based [19]. Finally, at the network level, ML-\nbased systems can be compromised under attacks [6], including Denial of Service (DoS), botnets, and ransomware.\nTo achieve their goals, ML threat actors can poison data and fool ML-based systems using different strategies, like\nevasion [10], [9], [12], [16], extraction [7], [20], [21], inference [22], [10], and poisoning [9], [8], [12], [10], [16]. To defend\nagainst such threats, adversarial defenses have been proposed\n[11], [14]. Usually, threat TTPs and mitigations are\nreported in a threat assessment framework to help conduct attack and defense operations. Unfortunately, there is a\nlack of concrete applications of threat assessment in the ML field that provide a broader overview of ML threats,\nML tool vulnerabilities, and mitigation solutions. The goal of this study is to systematically characterize ML security\nthreats, assess their impact on ML components (phases, models, tools), and identify effective mitigation strategies.\nWhile ML enhances various domains, its black-box nature and deployment across software, system, and network layers\nexpose it to adversarial attacks such as evasion, extraction, inference, and poisoning. Existing research lacks a unified\nthreat assessment framework that maps ML vulnerabilities, attack tactics, and mitigation strategies across different\nlifecycle stages. To achieve this goal, we conduct an empirical investigation, integrating real-world threat intelligence to\nanalyze ML-specific security risks, classify TTPs, and propose structured mitigation solutions, enhancing ML security\nframeworks against evolving threats.\nThus, we asked the following research questions (RQs):\n1) What are the most prominent threat TTPs and their common entry points in ML attack scenarios?\n2) What is the effect of threat TTPs on different ML phases and models?\n3)\nWhat previously undocumented security threats can be identified in the AI Incident Database, the literature,\nand ML repositories that are missing from the ATLAS database?\nResults suggest that Convolutional neural networks (e.g., GPT2, Fisheye, Copycat, ResNet) are one of the most\ntargeted models in attack scenarios. ML repositories such as TensorFlow, OpenCV, Notebook, and Numpy have\nthe largest vulnerability prominence. The most severe dependencies that caused the vulnerabilities include tensorflow,\nlinux_kernel, vim, openssl, magemagick, and pillow. DoS, improper input validation, and buffer-overflow were the most\nfrequent in ML repositories. Our examinations of vulnerabilities and attacks reveal that testing, inference, training,\nand data collection are the most targeted ML phases by threat actors. The mitigation of these vulnerabilities and\nthreats includes adversarial [11], [14], [23] and traditional defenses such as software updates, and cloud security policies\n(e.g., zero trust). Leveraging our findings, ML red/blue teams can take advantage of the ATLAS TTPs and the newly\nidentified TTPs from the AI incident database to better conduct attacks/defenses using the most exploited TTPs and\nmodels for more impact.\nSince ML-based systems are increasingly in production, ML practitioners can leverage these results to prevent\nvulnerabilities and threats in ML products during their lifecycle. Researchers can also use the results to propose\ntheories and algorithms for strengthening defenses.\nContributions. This paper advances ML threat assessment by integrating attacker Tactics, Techniques, and Proce-\ndures (TTPs) from frameworks such as MITRE ATLAS and ATT&CK with real-world vulnerabilities across the ML\nlifecycle. Our main contributions are:\n1) Lifecycle-Centric Threat Framework. A unified mapping of TTPs to vulnerabilities across data, software, and\nsystem layers—spanning collection, training, deployment, and inference—enabling holistic reasoning on cascading\nrisks.\n2) State-of-the-Art Model Analysis. Extension of threat assessment to foundation and multimodal models (GPT-4o,\nClaude-3.5, Gemini-1.5, LLaMA-3.2, DeepSeek-R1, etc.), revealing composite backdoors, tokenization exploits,\nand prompt-injection jailbreaks that generalize across modalities.\n3) Scalable Multi-Agent Mapping. A modular three-step pipeline combining retrieval-augmented reasoning, ontology\nalignment, and heterogeneous GNNs to scale threat mapping to RAG and RLHF pipelines.\n4) Graph-Based Severity Estimation. A heterogeneous GNN fusing code, model-artifact, and threat-intelligence\nfeatures to learn a severity score ˆs validated against real-world CVEs and incident costs.\n5) Automated Repository Mining. Extraction of CVE–CPE–tool relations from GitHub/PyPI and construction of\ndependency graphs linking vulnerabilities to ML toolchains, exposing supply-chain risks.\n6) Open Practitioner Toolkit. A reproducible toolkit supporting cluster drill-downs and visual lifecycle mapping\n(stage →vulnerability →stakeholder) for researchers and practitioners.\nTogether, these contributions deliver a scalable, evidence-driven foundation for analyzing and mitigating threats in\nmodern ML ecosystems, bridging traditional cybersecurity taxonomies and emerging AI security practice.\nThe rest\nof this paper is structured as follows. In Section II, we define basic concepts such as vulnerabilities and threats, and\nreview the related literature. Section III describes the study methodology for threat assessment, defines some research\nquestions, and presents a formal definition of an ML threat. In Section IV, we present results while answering the\ndefined research questions. Section V-A proposes mitigation solutions for the observed threats and vulnerabilities.\n"}, {"page": 3, "text": "3\nSection V discusses the results and the application in corporate settings. In Section VI, we present threats that could\naffect the validity of the reported results. Section VII concludes the paper and outlines avenues for future work.\nII. Background and related work\nBefore diving into ML threat assessment, generic security concepts such as assets, vulnerabilities, and threats must\nbe defined. This section provides an overview of security concepts and related work.\nA. Assets\nIn computer security, an asset is any valuable logical or physical resource owned by an organization, such as\ndata, software, hardware, storage, or network infrastructure (see Fig. 1). In ML systems, assets span five layers:\nData level—access credentials (tokens, passwords, cryptographic keys, certificates), datasets, models and parameters,\nsource code, and libraries. Software level—Machine-Learning-as-a-Service (MaaS) APIs, production ML applications,\ncontainers, and virtual machines (VMs). Storage level—databases, object stores (e.g., buckets), files, and block storage\nhosting training data, models, or code. System level—servers, racks, data centers, and compute clusters. Network\nlevel—firewalls, routers, gateways, switches, and load balancers.\nData-level assets (models, datasets, code, keys) face threats such as theft, poisoning, backdooring, and evasion.\nSoftware-level components (services, apps, OS, VMs) are susceptible to misconfiguration, buffer overflow, and credential\nexposure. Storage systems (databases, files, blocks) are vulnerable to SQL injection, weak authentication, and improper\nbackups. System-level hardware (servers, clusters) can be exploited via unpatched firmware, DoS, and side-channel\nattacks. Network devices (firewalls, routers) are vulnerable to misconfiguration, DDoS attacks, and botnet infiltration.\nThis layered view supports targeted risk assessment and defense prioritization.\nData level\nkeys, models, datasets, \ncode, libraries\nStealing, backdooring, \npoisoning, injection, \nevasion, inference\nSoftware level\nStorage level\nNetwork level\nSystem level\nservices, apps, \ncontainers, OS, VMs\ndatabases, objects, \nfiles, blocks\nservers, racks, data \ncenters, clusters\nfirewalls, routers, gateways, \nswitches, load balancers\nCWE   Top 25                OWASP Top 10                       NVD                        CVE \nLayers Assets Vulnerabilities\nsecurity misconfig, \nbuffer overflow, \ncredential exposure\nSQL injection, Weak \nauthentication, \nImproper backup\nunpatched \nfirmwares, DoS, \nCPU side-channel\nImproper configs, distributed \nDoS, botnets \nFig. 1: Assets and vulnerabilities across ML/AI system layers. Mapping five infrastructure layers—data, software,\nstorage, system, and network, to their representative assets and prevalent vulnerabilities, drawing from CWE Top 25,\nOWASP Top 10, NVD, and CVE taxonomies.\nB. Vulnerabilities\nA vulnerability is a software or hardware flaw that threat actors can exploit to execute malicious command-and-\ncontrol (C2) operations, such as data theft and destruction.\nTypes of vulnerabilities.\nVulnerabilities occur at different levels: data, software, storage, system, and network (see Fig 1). At the data level,\ndata assets are vulnerable to model stealing, backdooring [24], poisoning, injection, evasion, and inference. At the\nsoftware level, threat actors look for errors or bugs in ML apps, such as buffer overflows, exposed credentials, and security\nmisconfigurations. At the storage level, ML databases and cloud storage are vulnerable to weak authentication, improper\nbackup, and SQL injection attacks. At the system level, they exploit several hardware vulnerabilities, including firmware\nunpatching and CPU side-channel attacks [18], to launch attacks such as DoS, affecting the ML cloud infrastructure\nwhere ML apps are managed. At network level, threat actors can exploit improper configurations of the network, making\nit vulnerable to distributed DoS and botnet attacks. These vulnerabilities are reported in the Common Weaknesses\nExposure (CWE) Top 25 [25], the OWASP Top 10 [26], the National Vulnerability Database (NVD), and the Common\nVulnerability and Exposures (CVE) standards. Lastly, CPE dependency in cybersecurity refers to a vulnerability’s\nreliance on specific Common Platform Enumeration (CPE) components that describe hardware, software, or firmware.\nThe CPE name and version indicate the affected systems, which is essential for identifying and managing associated\nrisks.\n"}, {"page": 4, "text": "4\nC. Threats\nA threat exploits a given vulnerability to damage and/or destroy a target asset. Threats can be of two types: insider\nthreats and outsider threats. Insider threats originate from the internal system, and they are more often executed by\na trusted entity of the system (e.g., employee). Outsider threats are operated from the remote/external system. In\nthe following, we distinguish between traditional threats and recent machine learning threats.\nTraditional threats.\nAdversarial Tactics, Techniques, and Common Knowledge (ATT&CK) [27] is a public and standard knowledge\ndatabase of attack TTPs. Traditional attack phases are divided into two groups: conventional pre-attack phases and\nattack phases.\nPre-attack. The pre-attack phase consists of two tactics: reconnaissance and resource development [27]. During\nreconnaissance, attackers use several techniques, including network scanning to identify a victim’s open ports and\nOS version (e.g., nmap, censys), and phishing to embed malicious links in emails or SMS messages. During resource\ndevelopment, attackers use several techniques, including acquiring resources to support C2 operations (e.g., domains),\npurchasing a network of compromised systems (e.g., a botnet) for C2, developing tools (e.g., crawlers, exploit toolkits),\nand phishing.\nAttack. Once the pre-attack phase is complete, attackers will attempt to gain initial access to the target victim\nhost or network by delivering a malicious file or link through phishing, or by exploiting vulnerabilities in the\nwebsites/software used by victims. Also, attackers manipulate software dependencies and development tools before\nthey are delivered to the final consumer. Upon successful initial access, they will execute malicious code on the\nvictim host/network. After execution, they will attempt to persist on the target by modifying registries (e.g., Run\nKeys/Startup Folder), and automatically executing at boot. In addition, attackers will try to gain high-level permissions\n(e.g., as root/administrator). To hide their malicious activities, they will ensure they remain undetected by installing\nantivirus or Endpoint Detection Response (EDR) tools. An attacker can also execute lateral movement techniques,\nsuch as exploiting remote services, to spread to other hosts or networks and achieve greater impact.\nMachine learning threats\nAdversarial Threat Landscape for Artificial Intelligence Systems (ATLAS) [28] is a public and standard knowledge\ndatabase of adversarial TTPs for ML-based systems [28]. ML attack phases are divided into two groups: ML pre-attack\nphases and attack phases.\nML Pre-attack. ML pre-attack tactics are similar to those used in traditional threats, but with new techniques and\nprocedures adapted to the ML context [28]. During reconnaissance, Threat actors will search for the victim’s publicly\navailable research materials, such as technical blogs and pre-print repositories, and search for public ML artifacts,\nsuch as development tools (e.g., TensorFlow). For resource development, they will also acquire adversarial ML attack\nimplementations such as adversarial robustness [29] toolbox (ART).\nML Attack. ML systems are vulnerable to traditional attacks and other kinds of attacks that turn their normal\nbehaviors into threatening behaviors called adversarial attacks. Like traditional threats, ML threats target the\nconfidentiality, integrity, and availability of data. To achieve the goal, attackers may have full knowledge (white-\nbox), partial knowledge (gray-box), or no knowledge (black-box) of the targeted ML system. In black-box settings,\nattackers do not have access to the training dataset, the model, or the executing code (since assets are hosted on a\nprivate corporate network). Still, they can access the public ML API as a legitimate user. This allows them to only\nperform queries and observe outputs [30].\nIn white-box settings, attackers have knowledge of the model architecture and can access the training dataset or\nmodel to manipulate the training process. In gray-box settings, they have either a partial knowledge of the model\narchitecture or some information about the training process. Whether white-box, gray-box, or black-box attacks, they\ncan be targeted (focused on a particular class/sample) or untargeted (applied to any class/sample with no specific\nchoice) to cause models to misclassify inputs. Different attack techniques are used: poisoning, evasion, extraction,\nand inference. During poisoning [8], [9], [17], [31], [12], [32], [16], [33], attackers inject false training data to corrupt\nthe learning model (even allowing it to be backdoored [24]) to achieve an expected goal at inference time. During\nevasion [34], [9], [13], [35], attackers iteratively and carefully modify ML API queries and observe the output at\ninference time [30]. The queries seem normal, but are misclassified by ML models. During extraction [7], [20], [21],\n[36], attackers iteratively query the online model [30] allowing them to extract information about the model. Then,\nthey use this information to gradually train a substitute model that mimics the target model’s predictive behavior.\nDuring inference [22], [37], [38], attackers probe the online model with different queries. Based on the results, they\ncan infer whether features are used to train the model, which may compromise private data.\nThe adversarial models used [10] for attack include (1) fast gradient sign method (FGSM) which consists in adding\nnoise with the same direction as the gradient of the cost function w.r.t to data, (2) DeepFool eﬀiciently computes\nperturbations that fool deep networks, (3) Carlini and Wagner (C&W) is a set of three attacks against defensive\n"}, {"page": 5, "text": "5\ndistilled neural networks [39], (4) Jacobian-based saliency map (JSMA) saturates a few pixels in an image to their\nmaximum/minimum values, (5) universal adversarial perturbations are agnostic-image perturbations that can fool a\nnetwork on any image with high probability, (6) Basic Iterative Method (BIM) is an iterative version of the FGSM,\n(7) one pixel is when a single pixel in the image is changed to fool classifiers, (8) Iterative Least-likely Class Method\n(ILCM) is an extension of BIM where an image label is replaced by a target label of the least-likely class predicted\nby a classifier, and (9) Adversarial Transformation Networks (ATNs) turns any input into an adversarial attack\non the target network, while disrupting the original inputs and outputs of the target network as little as possible.\nTable I provides a comparison where conventional security controls might suﬀice versus where novel defenses—such\nas adversarial training, robust model architectures, and differential privacy—are required.\nTABLE I: Comparison of Traditional vs. ML-Specific Threats. Each property is contrasted to highlight the unique\nnature of ML-based systems and where traditional methods need adaptation.\nProperty\nTraditional Threats\nML/AI-Specific Threats\nAttack Surface\nMainly network endpoints, OS vulnerabilities, user credentials, etc.\nTraining data pipelines, learned model parameters, inference-time inputs\nAdversary’s Knowledge\nTypically partial or zero-knowledge about system internals (black-box)\nVaries from black-box to full white-box of ML model (depends on threat model)\nAttack Goal\nExfiltrate data, disrupt services, gain unauthorized system access\nMisclassify outputs, extract model IP, infer membership, degrade model performance\nStealth Mechanism\nMalware obfuscation, phishing, network-level exploits\nImperceptible perturbations to inputs, or subtle data poisoning manipulations\nImpact on System\nPotential data loss, financial damage, operational downtime\nDegraded accuracy, privacy leakage, model unavailability, or IP theft\nRequired Expertise\nSkilled in OS/network exploits, social engineering\nData science & ML knowledge plus exploit expertise\nCommon Defenses\nFirewalls, antivirus, patching, intrusion detection systems\nAdversarial training, differential privacy, secure model architectures\nLifecycle Complexity\nSecurity is mostly at the network, OS, or application layer\nVulnerabilities spread across data collection, training, inference phases\n1) Threat, Vulnerability, and Incident Databases\nThe MITRE ATT&CK framework, ATLAS, and the AI Incident Database are essential resources in the field of\nsecurity for machine learning (ML) and AI systems. Each of these databases contains valuable information that helps\nunderstand, mitigate, and analyze threats to AI and ML systems.\nMITRE ATT&CK. The MITRE ATT&CK framework is a globally recognized, structured knowledge base that catalogs\nadversarial tactics, techniques, and procedures (TTPs). It focuses on real-world observations of threat actor behavior\ntargeting various systems, including enterprise IT, cloud platforms, and industrial control systems. The framework\nprovides detailed mappings between tactics (adversarial goals like privilege escalation) and techniques (specific methods\nto achieve those goals). The dataset is available via MITRE’s TAXII server1 and in STIX format, allowing researchers\nto access structured data programmatically for vulnerability analysis and threat modeling.\nATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems).\nATLAS is a specialized knowledge base\ndesigned to document and catalog adversarial threats specific to AI and ML systems. Developed by MITRE, ATLAS\nbuilds upon the ATT&CK framework but focuses exclusively on AI-related attack scenarios. It maps threats to\nparticular phases of the ML lifecycle (e.g., data poisoning [40], [41], [42] during training or adversarial attacks during\ndeployment) and includes real-world examples of adversarial tactics. The ATLAS datasets can be accessed through\nits website using programmatic access (API) and may require scraping or specialized tools.\nAI Incident Database. The AI Incident Database is a community-driven repository that catalogs real-world incidents\ninvolving the failure or exploitation of AI systems. It documents issues such as data bias, adversarial attacks, and\nsafety-critical errors. Unlike ATT&CK and ATLAS, which focus on tactics and techniques, this database emphasizes\nthe broader impacts of AI failures, including their societal and ethical consequences. The database is publicly accessible\nthrough its website, allowing users to browse or download records for research purposes. Data extraction can typically\nbe performed through web scraping or API integration.\n2) ML Life-cycle stages documented in literature and production.\nFollowing widely-adopted process models such as CRISP–DM, ISO/IEC 5338, and ISO/IEC 23053, and integrating\nmodern post-training and operational practices, the machine learning life-cycle [43], [44], [45], [46] can be described\nas: (1) Problem definition & requirements elicitation; (2) Data acquisition/collection; (3) Data labeling/annotation;\n(4) Data governance & security (including PII handling and access control); (5) Data preprocessing/augmentation; (6)\nFeature engineering or tokenization; (7) Pre-training (foundation model training); (8) Fine-tuning or parameter-eﬀicient\nadaptation (e.g., LoRA, adapters); (9) Alignment through reinforcement learning from human feedback (RLHF)\nor AI feedback (RLAIF); (10) Evaluation & validation (including robustness, fairness, and reliability tests); (11)\nSecurity testing/ red-teaming (e.g., jailbreaks, poisoning, extraction); (12) Packaging & registration (model artifacts,\nregistry, and versioning); (13) Deployment/serving (batch, online, or edge); (14) Inference-time augmentation (retrieval-\naugmented generation, tool use, or agent integration); (15) Monitoring & observability (concept drift, data quality,\n1https://attack.mitre.org/resources/attack-data-and-tools/\n"}, {"page": 6, "text": "6\nlatency, cost); (16) Guardrails & policy enforcement (filters, rate limiting, authorization); (17) Incident response &\nrollback; (18) Continuous learning or re-training (data refresh, hotfixes); and (19) Archival & decommissioning. For\nempirical mapping in this work, these stages are collapsed into five macro-phases: Data preparation, Pre-training,\nFine-tuning (incl. PEFT/LoRA), RLHF/Alignment, and Deployment/Inference (incl. agents/RAG).\nD. Threat Model\nA threat model for ML systems identifies the critical assets, potential adversaries, vulnerabilities, attack vectors,\nand mitigations. This model provides a comprehensive understanding of the threats that ML systems face during their\nlifecycle and across different layers, such as data, models, infrastructure, and APIs.\nGoal. Attackers aim to affect the confidentiality, integrity, and availability of data (e.g., training data, features, model)\ndepending on threat objectives. Poisoning attacks can affect data integrity. Extraction attacks can enable the theft\nof models or features, thereby compromising confidentiality. Key elements of the threat model include Entities and\nAssets.\nEntities. Users: Legitimate actors interacting with the system. For example, data scientists, ML engineers, or application\nusers querying the model. Adversaries: Malicious actors targeting the system. For example, Competitors attempting\nto steal a model, cybercriminals exploiting APIs, or insiders corrupting datasets.\nAssets. Data: Training, validation, and test datasets are critical to model performance. For example, A dataset of\nmedical records is used to train a diagnostic model. Models: The core algorithms and their parameters, including\ndeployed and pre-trained models. For example, A fraud detection model is used in real-time transaction monitoring.\nInfrastructure: Hardware, servers, APIs, and cloud environments hosting ML systems. For example, TensorFlow Serving\nfor real-time inference, GPUs, or cloud-based data storage. APIs Interfaces for model inference or interaction. For\nexample, REST APIs for querying a sentiment analysis model.\nML threat scenarios\nData Poisoning. Adversaries inject malicious samples into the training data to manipulate the model’s behavior.\nUse-case: A dataset for spam detection is poisoned with mislabeled spam emails as non-spam. The trained model\nallows spam emails to bypass the filter. Impact: Degrades the model’s accuracy and reliability, impacting predictions.\nMitigation. Validate datasets for anomalies, use differential privacy, and apply robust data-cleaning techniques.\nInfrastructure Exploitation.\nExploit vulnerabilities in hardware, configurations, or hosting environments. Use-case:\nAn attacker exploits unpatched vulnerabilities in TensorFlow Serving (e.g., CVE-2020-15208) to inject a malicious\nmodel.\nImpact. Backdoor insertion compromises the integrity of predictions.\nMitigation. Patch vulnerabilities regularly, use role-based access control (RBAC), and host models in secure environ-\nments like trusted execution environments (TEEs).\nMathematical Representations of Threats\nLet a ∈A be an asset, where A is a set of assets from the system S. An asset a can be owned or accessed by an entity\n(e.g., a user, a user group, a program, or a set of programs) denoted as E. E denotes the set of all entities and E ∈E.\nLet ACS : A × E →R be a function that defines the level of privilege that an entity E has on an asset a or an asset\ngroup Ag ⊆A, under the system S. R is a set of right access, and it can take values (1) R = {none (∅), user, root}\nmeaning that entities can have either no privilege (none), user access on A (user), and full access on A (root); or (2)\nR = {none, read, write} meaning that entities can have no privilege (none), read access on A (read), and write access\non A (write). When ACAW S(′model.pkl′,′ ml_api′) = root, it means that the Amazon Web Service (AWS) ML API\nservice ml_api has full access to the pickled model file model.pkl. When ACV M(′training.csv′,′ John′) = write, it\nmeans that user John can modify or delete the training data file training.csv in the virtual machine V M.\nLet P1, ...Pn be a set of premises and C the goal to achieve. This relation is represented by:\nP1, ...Pn\nC\nIt also means that C can succeed when properties P1, ...Pn are satisfied. Based on [47], we define the following\nnotations. The notation\na7−→E\nmeans that a is E’s asset. Given kE ∈K a protection property (e.g., encryption key, certificate, token), the notation\n{a}kE\nmeans that the protection kE is enforced on asset a by an entity E. Let E1, E2 ∈E be two entities that share an\nasset a. The notation\nE1\na\n←→E2\n"}, {"page": 7, "text": "7\nmeans that a is shared by E1 and E2. The sharing is satisfied when E1 send a to E2 and E2 send a to E1 as follows,\nE1\na−→E2, E2\na−→E1\nE1\na\n←→E2\nLet m : A →C be a model function that takes data in A and returns decisions in C based on the inputs. C can be\ntwo classes (i.e., {c1, c2}) or multiple classes (i.e., {c1, c2, ..., cn}), where c1, c2, ..., cn ∈C.\na) Knowledge.\nThe attacker’s knowledge of the target ML system determines their strategy and attack feasibility. Attack models fall\ninto three categories: black-box, gray-box, and white-box, each reflecting different levels of access to model parameters,\ntraining data, and system components. These attacks exist within a broader adversarial setting, which defines the\noverall context of adversarial manipulation, including attack objectives, specificity, and defensive considerations.\nIn black-box settings, an attacker AT does not have direct access [30] to ML assets AV of the target victim V (i.e.,\nmodel, executing code, datasets), i.e., ∀aV ∈AV , ACV (aV , AT) = ∅. They have only access to the ML inference API\nusing an access token kV obtained as a legitimate user from the victim’s platform V , i.e.,\n{aAT }\n7−−−−→AT\nwhere aAT ∈A are data crafted offline by attacker AT to be sent via API. During the attack, AT performs queries\nusing the victim’s ML inference API and observes outputs. To do so, AT sends an online request with the crafted\ndata aAT using access token kV , i.e.,\nAT\n{aAT }kV\n−−−−−−→V\nThen, AT will receive prediction responses and analyze them to further improve its data for attack, i.e.,\nV\n{mV (aAT )}\n−−−−−−−−→AT\nwhere mV is the executed model behind the ML inference API of the target victim V .\nIn gray-box settings, an attacker AT has partial access to some ML assets ˜AV ⊆AV of the target victim V . This\npartial access could include knowledge of the model architecture, hyperparameters, or a subset of training data, but\nnot full access to model parameters or gradients. Formally,\n∃aV ∈˜AV , ACV (aV , AT) = partial\nIn this scenario, the attacker AT can leverage transfer learning, metadata analysis, or limited model responses to\nrefine their adversarial strategies. The attack process can involve training a shadow model\nˆ\nM to approximate the\ntarget model MV :\nˆ\nM ≈MV ,\nwhere\nDpartial ⊂Dtrain.\nUsing this approximated model, the attacker can estimate gradients and generate adversarial examples:\nxadv = x + δ,\nsuch that\nMV (xadv) = yt,\n∥δ∥p ≤ϵ.\nWhere yt is the targeted misclassification.\nIn white-box settings, attacker AT may have internal access to some ML assets ˜AV ⊆AV of the target victim V (e.g.,\nmodel, training data), i.e.,\n∀aV ∈˜AV , ACV (aV , AT) ∈{read, write}\nThen, AT can perform several state-of-the-art attack techniques such as poisoning, evasion, extraction, and inference\n(see Section II-C).\nb) Specificity.\nIn adversarial settings, ML threats can target a specific class/sample for misclassification (adversarially targeted)\nor any class/sample for misclassification (adversarially untargeted). The goal of AT is to maximize the loss L so that\nmodel mV misclassifies input data,\narg max\na\nL(mV (a), c)\nwhere a ∈A is an input data, c ∈C is a target class, and mV (a) is the predicted target data given a. To achieve\nthe goal, AT can execute a targeted or untargeted attack affecting the integrity and confidentiality of data [48], [8].\nWhen attack is targeted, AT substitutes the predicted class c by adding a small pertubation θu(a, c) so that\nmV (aAT ) = c,\n"}, {"page": 8, "text": "8\nwhere aAT = a + θu(a, c) is an adversarial sample.\nIn untargeted attack, AT adds a small pertubation θt(a) to input a so that\nmV (aAT ) ̸= mV (a),\nwhere aAT = a + θt(a) is an adversarial sample. ML threats can also leverage traditional TTPs to achieve their goals.\nIn traditional settings, threat actors can either actively pursue and compromise specific targets while maintaining\nanonymity (traditionally targeted) or spread indiscriminately across the network without a predefined objective\n(traditionally untargeted). The terms Adversarially and Traditionally are used to distinguish between attack specificity\nin adversarial settings and broader, less targeted approaches in traditional settings. In traditional attacks, the attacker\nAT typically targets critical assets such as user accounts, servers, virtual machines, databases, and networks. By\nexploiting vulnerabilities and bypassing authentication mechanisms or firewalls, AT gains unauthorized access to\nthese assets. Once inside, AT can escalate privileges to gain full control of the ML assets belonging to the victim V ,\ndenoted as ∀aV ∈AV , ACV (aV , AT) = root. This unrestricted access enables the attacker to cause extensive damage,\nsuch as exfiltrating sensitive data, corrupting models, or disrupting system operations.\nCapability. Threat actors employ a variety of tactics to execute machine learning (ML) attacks effectively [28]. These\ntactics include Reconnaissance, Resource Development, Initial Access, ML Model Access, Execution, Persistence, De-\nfense Evasion, Discovery, Collection, ML Attack Staging, Exfiltration, and Impact. During Reconnaissance and Resource Developm\nattackers gather intelligence about the target system by analyzing publicly available resources, such as papers,\nrepositories, or technical documentation. Simultaneously, they establish command-and-control (C2) infrastructure\nto facilitate the attack. In the Initial Access phase, attackers attempt to infiltrate the victim’s infrastructure, focusing\non entry points containing ML artifacts such as datasets, models, or APIs. Once inside, they escalate their activities\nto gain deeper access to model internals and physical environments (ML Model Access). During Execution, attackers\ndeploy remote-controlled malicious code to extract sensitive data or disrupt normal operations. To maintain access,\nthey rely on Persistence techniques such as implanting backdoor ML models or preserving compromised access\nchannels. Attackers use Evasion strategies to bypass detection mechanisms such as classifiers and intrusion detec-\ntion systems [34], [9], [13], [35]. Once the system is compromised, they engage in Discovery and Collection activities\nto identify and harvest valuable data. During ML Attack Staging, adversaries refine their strategies by training\nproxy models, crafting adversarial data, or injecting poisoned inputs to corrupt the target model. The final phase,\nExfiltration and Impact, often results in significant consequences, such as theft of proprietary models, large-scale data\nbreaches, human harm, or complete system failure.\n1) Assets & Adversaries (Threat-Model Scope)\nTable II enumerates the machine-learning assets we protect, aligned with the phases of the ML lifecycle introduced\nin II-C2 and later in the MITRE ATLAS tactic. The subsequent Table III defines four adversary personas and the\nsubset of assets each can legitimately or illegitimately reach. This explicit mapping disambiguates the attack surface\nconsidered throughout our GNN-based severity model (sec III-D) and the mitigation matrix (Fig. 17, section IV-D).\nTABLE II: Taxonomy of ML assets in our threat model: categories, exemplar artefacts, and lifecycle phases.\nAsset category\nConcrete artefacts\nLifecycle phase\nData\nraw training corpus; RLHF preference logs; LoRA/QLoRA adapter\ndeltas; evaluation\nCollect →Train →Fine-\nTune\nModel artefacts\nnetwork architecture graph; checkpoint weights; ONNX/TensorRT\nbinaries; gradient updates\nTrain →Package →Deploy\nExecution surface\nREST/GRPC inference API; SDK wrappers; hosted notebook\nendpoints; model-registry entries\nDeploy →Serve\nSupply-chain\nthird-party libraries; container images; CI/CD configs; signed model\ncards\nPackage →Deploy\nMLOps metadata\nexperiment tracker DB; lineage store; monitoring dashboards; audit\nlogs\nCross-cutting\na) Example linkage.\nThe synthetic gray-box scenario in section III-G maps to Gray-box collaborator: the former contractor controls the\npublic inference API plus partial knowledge of a BERT-based architecture and public pre-training corpus, but no\ndirect access to current weights. That example therefore, targets the Execution surface and Model artefacts rows in\nTable II. Each subsequent attack graph edge and mitigation entry cites the asset IDs (Data, Execution) and persona\nIDs (P2, P3, …) to keep the provenance explicit.\n"}, {"page": 9, "text": "9\nTABLE III: Adversary personas, their access levels, and reachable assets. Typical actors range from a curious end-user\n(limited to API access) to a rogue maintainer, registry attacker (compromising dependencies).\nPersona\nAccess level\nPrimary assets exposed\nPublic black-box\nPublic inference API only\nExecution surface\nGray-box\ncollaborator\nAPI + partial internals (e.g. arch sketch,\nsmall data subset)\nExecution surface; subset of Data & Model artefacts\nWhite-box insider\nFull repo and pipeline\nAll asset classes\nSupply-chain attacker\nBuild or dependency path\nSupply-chain artefacts (library, container, CI)\nE. Related work\nIn computer security, threat assessment is a continuous process that involves identifying, analyzing, evaluating, and\nmitigating threats. While this process has been extensively applied to traditional systems [49], its concrete application\nto machine learning (ML)-based systems remains nascent and underexplored. The ATLAS framework, developed by\nthe MITRE Corporation in collaboration with organizations like Microsoft and Palo Alto Networks, represents the\nfirst comprehensive real-world attempt at ML threat assessment [28]. Recent work has also leveraged the ATT&CK\nframework for threat modeling. For example, Kumar et al. [50] identified gaps during ML development, deployment,\nand operational phases when ML systems are under attack. They proposed incorporating security aspects such as\nstatic and dynamic analysis, auditing, and logging to strengthen ML-based systems in industrial settings. Building\non these foundations, our approach integrates existing mitigations from ATT&CK [51], the Cloud Security Alliance,\nand NIST security guidelines [52], [53], [54], [55], [56], [57], [58]. It organizes mitigations across layers (e.g., data level\nto cloud level) and stages (e.g., harden, detect, isolate, evict), aligning with frameworks like MITRE D3FEND [59].\nFurthermore, Lakhdhar et al. [60] proposed mapping newly discovered vulnerabilities to ATT&CK tactics by extracting\nfeatures like CVSS severity scores and using RandomForest-based models. Similarly, Kuppa et al. [61] employed a\nmulti-head joint embedding neural network trained on threat reports to map CVEs to ATT&CK techniques. Both\napproaches, however, are limited by their reliance solely on the ATT&CK database and their focus on mapping\nvulnerabilities to TTPs. Our proposed threat assessment methodology combines insights from ATLAS, ATT&CK,\nand additional sources like the AI Incident Database [62] to provide a comprehensive characterization of ML threats\nand vulnerabilities. By mapping TTPs to specific ML phases and models, and integrating vulnerability analysis with\nlifecycle-specific defenses, we offer a complete, end-to-end assessment of ML assets.\nAdversarial Threats and Vulnerabilities in ML Systems\nAdversarial threats are a key focus within ML security. Carlini et al. [63] demonstrated the robustness of adversarial\nexamples in bypassing detection mechanisms, while Athalye et al. [64] exposed flaws in gradient obfuscation defenses.\nWallace et al. [65] extended these findings by highlighting vulnerabilities in machine translation systems, showcasing\nhow adversaries can generate targeted mistranslations and malicious outputs. Papernot et al. [66], [67] introduced\nblack-box and transferability-based attack methodologies, demonstrating the feasibility of crafting adversarial examples\nwithout access to model internals. On the privacy front, Carlini et al. [7] highlighted the risks of training data extraction\nfrom large language models, emphasizing the potential for sensitive information leakage. Similarly, Chen et al. [31]\nproposed BadNL, a backdoor attack framework for NLP systems, which leverages semantic-preserving triggers to\nensure stealth and eﬀicacy. These works collectively underline the importance of designing robust defenses.\nFrameworks for Defense and Systematization\nSystematic approaches to ML threat assessment have also been proposed. Abdullah et al. [33] and Barreno et al. [48]\ncategorized adversarial threats and mapped them to ML lifecycle stages, creating a foundation for threat mitigation\nstrategies. Cissé et al. [68] introduced Parseval networks to constrain model behavior for improved robustness, while\nGoodfellow et al. [69] presented generative adversarial networks (GANs), inspiring adversarial training techniques.\nWallace et al. [65] further proposed defenses against imitation-based attacks, offering practical countermeasures to\nmitigate these threats.\nComprehensive Threat Mitigation\nOur combined approach leverages ATLAS, ATT&CK, AI Incident Database, and defense frameworks such as\nD3FEND [59] and NIST guidelines. This methodology integrates traditional cybersecurity practices with ML-specific\ninsights to provide a holistic and robust threat assessment strategy. In the following sections, we demonstrate how\nATLAS TTPs affect ML components across lifecycle stages and how traditional vulnerabilities propagate through ML\nrepositories.\nExisting research primarily focuses on threat modeling using frameworks like ATLAS and ATT&CK, with efforts to\nmap vulnerabilities to TTPs through automated techniques. However, these approaches are limited by their reliance on\n"}, {"page": 10, "text": "10\npredefined databases, failing to capture emerging threats from real-world incidents. Our work fills this gap by integrating\nmultiple sources—including ATLAS, ATT&CK, AI Incident Database, and GitHub repositories—to provide a more\ncomprehensive, dynamic threat assessment. By mapping TTPs to specific ML lifecycle stages and analyzing high-\nseverity vulnerabilities across dependency clusters, we bridge the disconnect between theoretical threat models and\npractical security challenges, offering a more actionable and lifecycle-aware approach to ML security.\nIII. Study Methodology\nAI Agents to extract and map ML attack Scenarios, TTPs, and ML Phases and models\nSQ, Emb. + Vector Database\n6\n1\n2\n3\nScore\n0.98\n0.95\n0.88\nReranker\nQuery User\nKnowledge Graph\nAI Incident Database\nRQ1\nRQ2\n1\n7\n2\nResponds\nLLMs\n3\n4\n5\n8\n10\n11\n9\nEmb. \nModel\n(a) Agentic-RAG mapping TTPs to (i) ML phases and models (ii) to attack scenarios\nMap CVE IDs to \nML Tools\nAgents extract CVE IDs, Tool, CPE from source repos and build visuals\nPyPA Database\nML Repos\nCVE IDs\nTool Names\nRQ3\n12\n13\n14\nExtract CVE\ninfo\n(b) Using Agents to map CVE IDs to ML tools\nFig. 2: Study methodology using agentic workflow to address RQ1–RQ3. (a) Agentic–RAG pipeline that ingests\nliterature and incident data, retrieves and reranks evidence, and maps TTPs to ML lifecycle phases, model types, and\nattack scenarios, integrating results into a knowledge graph. (b) Agent workflow that mines GitHub/PyPI repositories\nfor CVE IDs, CPEs and tool names, then maps vulnerabilities to specific ML tools for visualization and analysis.\nThe goal of this study is to comprehensively analyze ML threat behaviors, including common entry points, prominent\nthreat tactics, and typical TTP stages, and evaluate their impact on ML components such as vulnerable ML phases,\nmodels, tools, and their associated dependencies. To achieve this, we leverage threat knowledge from established\nsources such as ATLAS, ATT&CK, and the AI Incident Database, alongside documented vulnerabilities from ML\ncodebase repositories (GitHub and PyPA). Additionally, we incorporate TTPs discussed in the literature, aligned with\nvarious ML lifecycle stages, to predict potential threats related to specific packages or libraries. The perspective of\nthis study is to equip ML red/blue teams, researchers, and practitioners with a deeper understanding of ML threat\nTTPs. By doing so, they can proactively prevent and defend against these threats, ensuring the secure development\nand deployment of ML products from staging to production environments. The context of this study encompasses 93\nreal-world ML attack scenarios drawn from diverse sources, including 26, 12, and 55 (total of 93) cases from ATLAS,\nthe AI Incident Database, and the literature, respectively. It also includes 854 ML repositories, with 845 sourced from\nGitHub and 11 from PyPA [70]. Figure 2 illustrates the roadmap of the study. First, we present the research questions\nthat guide this study. Next, we introduce the threat model considered and adopted in this work. The implementations\nof this study’s goals leveraged five AI Agents using the Swarm framework2, each agent designed to execute a specific\ntask as detailed in Section III-A. Custom scripts were developed for individual tasks (to be executed by agents),\n2https://docs.swarms.world/en/latest/swarms/agents/openai_assistant/\n"}, {"page": 11, "text": "11\nindependently tested for correctness, and then integrated into a cohesive multi-agent framework to ensure precision\nand eﬀiciency throughout the workflow. Finally, we outline the data collection and processing steps to ensure the\nclarity and reproducibility of our methodology.\nIn the context of this research work, we define an AI agent3 as a computational system capable of perceiving and\ninterpreting its environment, mining, analyzing, and reporting data from multiple external sources (e.g., scientific\narticles, security databases, and software repositories), reasoning and refining queries autonomously through internal\nlogic (reasoning), and making informed decisions based on the context and previous interactions (memory). Five\nagents are coordinated to form an ‘agentic’ solution that autonomously refines initial search queries, retrieves relevant\ninformation, identifies adversarial threats (TTPs) and vulnerabilities, and their lifecycle stages, and constructs graphical\nor analytical models for visualization and analysis. The agentic system can independently assess new data against\nexisting knowledge to continuously update its understanding (learning), anticipate potential threats, and facilitate\ncomprehensive exploration of the threat landscape.\nResearch Questions (RQs). To achieve our goal, we address the following research questions:\nRQ1: What are the most prominent threat TTPs and their common entry points in ML attack scenarios?\nThis RQ aims to expand knowledge about ML threat TTPs to facilitate the development of better defense\nstrategies. By examining the execution flows of ML attack scenarios, this study seeks to identify the most commonly\nused TTPs and their sequences. Understanding these patterns provides actionable insights into how adversaries\nstructure their attacks, enabling researchers and practitioners to design proactive and targeted countermeasures\ntailored to these scenarios.\nRQ2: What is the effect of threat TTPs on different ML phases and models?\nThis RQ focuses on understanding the impact of the threat tactics identified in RQ1 on various ML phases and\nmodels. The objective is to analyze how each tactic affects different stages of the ML lifecycle, such as data\ncollection, training, and deployment, to help secure individual pipeline components. By identifying the most\nfrequently targeted ML phases and the most prevalent threat tactics, this RQ provides actionable insights into\nthe areas of greatest vulnerability, enabling practitioners to prioritize defenses and mitigate risks effectively across\nthe ML lifecycle.\nRQ3:\nWhat previously undocumented security threats can be identified in the AI Incident Database, the literature,\nand ML repositories that are missing from the ATLAS database?\nThis RQ assesses the completeness of ATLAS by identifying other security threats that may have been overlooked.\nThe goal is to identify gaps in ATLAS by comparing it with other sources, such as the AI Incident Database\nor relevant literature. This retrospective approach focuses on cataloging overlooked threats from multiple sources\nand aligning them with the existing ATLAS framework. Furthermore, we also investigate the most vulnerable\nML repositories, the most recurrent associated vulnerabilities, and the dependencies that cause them.\n1) Metrics.\nTo answer our RQs, we compute individual metrics as reported below.\nRQ1 focuses on the Tactics in Scenario-Based Attacks by computing the tactics employed in scenario-based attacks\nand their frequency. This provides a quantitative measure of how often specific tactics are utilized, offering insights\ninto prevalent attack strategies in different scenarios. RQ2 addresses Impact on ML Phases and calculates the number\nof tactics targeting each ML phase. This evidence highlights which phases of the ML lifecycle (e.g., data collection,\ntraining, deployment) are most impacted by threats. Such a metric is crucial for identifying the stages at which ML\nsystems are particularly vulnerable. For RQ3, we examine vulnerabilities, their types, and the tools they affect. We\ndesign multiple metrics to capture the scope and distribution of vulnerabilities. Specifically, we compute: (i) the total\nnumber of vulnerabilities (nov) and their overall types, and (ii) the distribution of nov by threat type and tools (e.g.,\nGitHub ML repositories). These metrics further break down the nov by individual tools and by type for each tool,\nproviding granular insights into the frequency and nature of vulnerabilities in ML repositories. This analysis sheds\nlight on the most vulnerable tools, the recurring types of vulnerabilities, and their associated potential threats.\nA. Data collection\nThis study leverages diverse and credible data sources, including academic databases, codebase repositories (GitHub\nand PyPA), the AI Incidents Database, and the MITRE Database, to comprehensively examine threats and vulnerabili-\nties in machine learning systems. First, we conducted an exploratory analysis of the codebase and incident repositories\nto gain a comprehensive understanding of their structures and organization, enabling targeted and informed data\ncollection. A systematic process for extracting threats and vulnerabilities from well-established databases ensures\nreliability and consistency in identifying relevant issues. The study adds depth and context by integrating data from\n3https://blogs.nvidia.com/blog/what-is-agentic-ai/\n"}, {"page": 12, "text": "12\nGitHub repositories, using publicly available information to link vulnerabilities to real-world problems. Finally, to\nenhance precision and eﬀiciency, we developed and implemented scripts [71] for execution by the Agent. Specialized\nAgents were employed to execute specific tasks, ensuring eﬀiciency and precision in the workflow.\nQuery & Search\nGraph Representation\nCodebase\nRAG\nMITRE\na\nb\ne\nc\nd\nFig. 3: Multi-agent LLM with RAG. A coordinated set of AI agents executes four roles: (a) Query & Search:\nretrieving relevant literature and threat intelligence from Academic Search Engines/Databases; (b) RAG: extracting\nand ranking relevant TTPs, vulnerabilities, and ML lifecycle stages; (c) Graph Representation: encoding extracted\nrelationships into a heterogeneous knowledge graph; (d) Codebase & MITRE Mapping: Linking GitHub/PyPI CVEs\nto ATT&CK/ATLAS.\na) Overview of Our Multi-Agent Approach.\nThis study comprises five key tasks spanning from initial data collection to the final reporting of results. First, we\nrefine the initial search query ( a⃝) using a Large Language Model (LLM), ensuring more precise and granular strings\n(that should take into consideration acronyms of key terms used for TTP) to interrogate the scientific databases\nin anticipation of the subsequent RAG steps. Next, we leverage an agentic RAG pipeline ( b⃝) with a re-ranker to\nidentify pertinent themes and content from the literature, using the refined query generated in the previous step.\nWe then search codebase repositories ( c⃝) according to criteria in Section III-A2, enabling us to gather concrete\nevidence of security practices from real-world ML libraries/ applications. Concurrently, we extract ML attacks ( d⃝)\ndocumented in ATLAS and the AI Incident Database (Section III-A3), enriching the threat profile by incorporating\nadversarial behaviors observed in practice. Finally, we build graphical representations ( e⃝) to visualize and analyze\nthe vulnerability patterns gleaned from literature and codebase repositories (Section IV).\n1) Scientific Database Search\nA multi-agent RAG system [72], [73], [74] automates data collection and processing for this literature review,\norchestrating each agent’s specialized role. A thorough search was conducted on Semantic Scholar, Google Scholar,\nand IEEE Xplore, starting with an exploration of 14 foundational seed papers widely recognized in the machine learning\n(ML) security community [7], [34], [48], [63], [65], [33], [31], [37], [67], [69], [66], [68], [64], [20]. The authors chose these\n14 papers due to their relevance and popularity regarding security threats in machine learning application domains.\nThese seed papers form the foundation for systematically exploring the broader literature search. Based on insights\nfrom these seed papers, an initial_query—“Tactics, Techniques, and Procedures in Machine Learning Security”—was\nsubmitted to an LLM (temperature = 0.4) to produce a refined, inclusive, targeted query string (Listing 1) aimed\nat capturing additional relevant articles. This refined query string was then executed via API calls across multiple\nacademic databases, returning 4,820 articles. From this set, 300 articles were randomly sampled (Confidence Level: 99%,\nMargin of Error: 7.5%) for our RAG pipeline. As illustrated in Figure 3, the first agent (Query & Search) receives\nthis initial query, thus initiating a systematic literature review that captures a broader representation of the relevant\nthreat landscape. This method ensures both depth and breadth in our data collection process, thereby enhancing the\nrobustness of our study methodology.\n1\nquery = (\n2\n”((\\”Tactics, Techniques, and Procedures\\” OR \\”TTP\\” OR \\”advers* attack\\” OR \\”threat\\” OR \\”vuln*”) ”\n3\n”AND (\\”machine learning security\\” OR \\”ML security\\” OR \\”AI security\\” OR \\”deep learning security\\”)\n4\n”AND (\\”data poisoning\\” OR \\”evasion attack\\” OR \\”model tampering\\” OR \\”model inversion\\” OR ”\n5\n”\\”backdoor attack\\” OR \\”adversarial example\\” OR \\”denial of service\\” OR \\”resource exhaustion\\”))”\n6\n)\nListing 1: Search query string generated by LLM RAG agent\n2) ML Codebase Repositories (GitHub and PyPA)\nTo gather the sample of ML projects considered to run our study, we adopt various strategies. Similar to previous\nstudies [75], [76], we aim to collect ML repositories (repos) from GitHub, applying specific criteria to select high-\nquality, widely recognized projects. One such filtering criterion includes repos with over 1,000 stars, which are generally\n"}, {"page": 13, "text": "13\nregarded as reputable, promising, and reflective of community engagement and interest. Beyond stars, we also observe\nadditional activity metrics to gauge the repos’ health and engagement. These include the number of open and closed\nissues, which provide insight into the project’s maintenance and responsiveness; the number of pull requests (PRs),\nboth merged and pending, signifies the pace of development and the integration of new features. By focusing on repos\nthat meet or exceed this threshold, we aim to curate a dataset that balances quality, relevance, and active contributions\nfrom the open-source community. In particular, we use the GitHub API search to mine repos satisfying our criteria,\nfor example, to select repos with over 1,000 stars, we use the following criteria: machine-learning in:topic stars:>1000\nsort:stars. As a result, we obtained a list of 916 repositories that we sorted in descending order by number of stars,\nand filtered out 82 projects that did not meet our inclusion criteria. To this end, 834 projects were retained. Given\nthe widespread adoption of the Python programming language and its frameworks [77], [78], we decided to include the\nPyPA database [70] in our analysis. This database comprises 425 repositories that document known vulnerabilities.\nUpon evaluation, we identified 11 ML repositories of interest.\n3) Vulnerabilities, Threats, and Adversarial tactics Selection\nWhen selecting target threats, we focus on three key criteria: newness, consistency, and reputation. By prioritizing\nrecent data, we ensure that the sources provide up-to-date information on emerging ML vulnerabilities and threats.\nThe consistency criterion emphasizes the inclusion of data sources that comprehensively cover significant vulnerabilities\nor threats, ensuring reliability and breadth. Lastly, the reputation criterion ensures the selection of data sources that\nare continuously updated, widely recognized, and frequently referenced by the community. Based on these criteria,\nwe have chosen the ATLAS [28] and AI Incident [62] databases as our primary sources. These datasets were accessed\nprogrammatically through API or direct downloads.\nThe ATLAS database. During the exploratory analysis, which we did to understand the structure and patterns for\neach data source, we observed attack scenarios. For each attack scenario, we analyzed its pattern to identify the\nassociated goal, knowledge, specificity, and capabilities. Then, we use our scripts [71] to extract the attack phases.\nGiven that an attack is depicted into phases (procedures) [79], at the time of mining and analysis, our data contains\n26 documented ML attack scenarios spanning from 2016 to 2024 [28].\nThe AI Incident Database. This source compiles real-world AI incident reports, which in this study, we extracted the\ndataset from 2003 to 2024 [62]. To complement the tactics, techniques, and procedures (TTPs) defined in ATLAS,\nwe also incorporate the ATT&CK framework [80], which provides a broader context for attack scenarios, combining\ntactics from both ATLAS and ATT&CK. To mine the incidents available in the dataset, we use our developed crawler\nto mine the attacks, using Regex and NLP techniques (cosine similarity, etc.) to search for keywords like textitattack\n[71]. Also, we parsed the reference link to verify each attack by reading its description. As a result, 254 attacks were\nreturned, dated from 2018 to 2024.\nThe ATT&CK database. Includes downloadable JSON files. In the end, we extract the scenarios and their TTP\ndefinitions from ATLAS (14 tactics, 52 techniques, 39 sub-techniques) and ATT&CK (14 enterprise tactics, 188\nenterprise techniques, 379 sub-techniques)\n[81]. All the referenced databases are maintained and up-to-date, employing\ndifferent update strategies. ATLAS and ATT&CK, managed by the MITRE Corporation, receive continuous support\nand updates from prominent industry leaders, including Microsoft, McAfee, Palo Alto Networks, IBM, and NVIDIA.\nIn contrast, the AI Incident Database relies on collaborative contributions and curates information from verified real-\nworld incidents reported by reputable media outlets, including Forbes, BBC, The New York Times, and CNN. This\nintegration of diverse sources ensures that the datasets remain robust and reflective of the evolving landscape of AI\nand ML security threats.\na) Cross-level alignment of vulnerabilities.\nTable IV links every high-impact vulnerability in our corpus to three orthogonal coordinates: (i) the ML life-cycle\nphase where the flaw first manifests, (ii) the software-component layer it abuses (data layer/ model layer/ orchestration\nlayer), and (iii) the system or network surface that is ultimately compromised. The same identifiers (e.g. VUL-17)\nappear as node labels in the heterogeneous GNN (section III-E &\nIII-D.) and in the Mitigation Matrix (Fig. 17,\nsection IV-D), allowing the reader to trace a single weakness—such as LoRA gradient leakage—from its inception in\nthe fine-tuning phase, through the model-repository API, all the way to the underlying S3 storage bucket.\nb) Methodology for cross-level alignment.\nFor every CVE or TTP in our corpus, we followed a three-step tagging pipeline. (1) Life-cycle phase: we read the\nvulnerability description and the original exploit reports, then applied the NIST ML life-cycle taxonomy (pre-train,\nfine-tune, RLHF, deploy) to identify the earliest phase at which the flaw can be triggered. (2) Software layer: we\nmapped the affected source files, configuration keys, or API endpoints, to one of three canonical layers in an ML\nstack—data layer (e.g., dataset loaders, artifact stores), model layer (training or inference code, parameter adapters), or\norchestration layer (pipelines, registries, service mesh). (3) System/ network surface: finally, we traced the execution\npath until a concrete infrastructure boundary was reached, such as an S3 bucket, a Kubernetes control plane, or\n"}, {"page": 14, "text": "14\nTABLE IV: Cross-Level Vulnerability Map. Each vulnerability is aligned with (i) its first ML life-cycle phase, (ii) the\nsoftware component it abuses, and (iii) the infrastructure or network surface, it ultimately compromises. IDs (e.g.\nVUL-17) are reused in the GNN (section IV-D) and the Mitigation Matrix (Fig. 17).\nID\nCVE / TTP\nML phase\nSoftware layer\nSystem level\nVUL-17\nLoRA gradient leakage\nFine-tune\nModel-repo API\nS3 bucket (weights)\nVUL-23\nModel-registry poisoning\nDeploy\nServing layer\nK8s control-plane\nVUL-31\nUniversal jailbreak prompt (MASTERKEY)\nDeploy\nChat interface\nPublic inference API\nVUL-42\nReward-model hacking (RLHF)\nRLHF loop\nRL policy store\nCI/CD controller\nVUL-55\nTraining-data reconstruction (GPT-J)\nPre-train\nCheck-point store\nCloud object store\na public inference API. The tags were double-checked by two cybersecurity professionals (Cohen’s κ = 0.93). The\nresulting triple of labels, phase, and software layer, surface, is what Table IV records and what the GNN ingests as\nnode metadata in section III-E & III-D.\nTABLE V: Cross–layer taxonomy: each vulnerability observed in our corpus is linked from its ML life-cycle phase to\nthe affected software, system, or network layer.\nML phase\nAsset\nVulnerability / TTP (ex.)\nMapped layer\nData prep\nTraining dataset\nData poisoning (BadNets [82], [83]), Label-flip (CVE-2023-45210)\nSoftware (ETL pipeline)\nPre-train\nCheck-points\nWeight-deserialisation RCE (CVE-2025-32434)\nSystem (file I/O)\nFine-tune\nLoRA adapters\nGradient leakage [84]\nSoftware (update API)\nRLHF loop\nPreference DB\nReward-model hacking [85]\nSystem (CI/CD)\nDeploy\nInference API\nUniversal jailbreak (MASTERKEY [86]); HTTP request smuggling\n(CVE-2024-3099)\nNetwork (edge proxy)\nc) Cross-layer taxonomy.\nTable V traces each vulnerability vertically through the software stack, clarifying where it breaks the ML boundary\nand touches traditional infrastructure.\nd) How the layer mapping was derived.\nThe procedure mirrors the cross-level workflow:\n(1) Phase anchoring: fix the ML life-cycle phase established above;\n(2) Call-graph walk: start from the vulnerable asset and traverse source code, manifests, or API specifications until\nthe first software boundary is crossed—classifying the boundary as data, model, or orchestration;\n(3) Perimeter resolution: continue the trace to an observable infrastructure surface (file I/O, CI/CD service, edge\nproxy, …), which becomes the “mapped layer” column in Table V.\nThe phase–asset–layer triples feed directly into the GNN (section 17) and enable mitigation queries such as: “Which\ndeploy-phase flaws escalate past the orchestration layer into the public network edge?” Table IV traces each individual\nCVE up the stack (bottom-up), while Table V aggregates flaws by phase–asset pair and shows how they descend to\nlower layers (top-down). Viewed together, enable a bi-directional view from a single vulnerability to its system impact,\nor from an affected layer to all ML-phase exploits that reach it.\nB. Data processing\nThis section analyzes different datasets to uncover threat patterns and relationships with vulnerabilities and ML\nstages.\n1) Retrieval-Augmented Generation (RAG) with Reranking.\nOur enhanced RAG system leverages ChatGPT-4o with a temperature of 0.4, optimized through empirical testing\nwithin the range [0.2–0.7], ensuring accurate retrieval of key concepts (TTPs, vulnerabilities, and lifecycle stages) from\nscientific papers. This configuration maintains the flexibility to capture synonyms, nuanced terms, and variations while\nminimizing hallucinations and extraneous content by strictly aligning outputs with evidence. The RAG workflow begins\nwith document retrieval using dense embedding models (e.g., Sentence Transformers). A transformer-based reranker\nprioritizes retrieved documents based on their relevance to the query, ensuring that only the top-k documents (k = 50\nin our implementation) proceed to the generation phase. During response generation, the LLM synthesizes outputs\nby conditioning on both the refined query and reranked documents, combining its generative capabilities with factual\ngrounding.\n"}, {"page": 15, "text": "15\n2) Post-hoc interpretability.\nWe embed two complementary explainers—SHAP (SHapley Additive ExPlanations) and LIME (Local Interpretable\nModel-agnostic Explanations)—alongside the retrieval-augmented generation loop so the system can justify why a\nparticular Confidentiality, Integrity, or Availability (CIA) label is returned.\nttp_sentences = {\n1: ”Data poisoning attack compromises model training\nintegrity .” ,\n2: ”Model inversion attack leaks confidential information from training data .” ,\n3: ”Denial - of - service attack targets\navailability\nof ML models.” ,\n}\nListing 2: Example TTP sentences illustrating Confidentiality, Integrity, and Availability violations used in the\nSHAP/LIME demonstrations.\nGlobal attribution (SHAP). For every candidate sentence, we compute token-level Shapley values with maskers.Text+partition.\nFigure 4 (left) plots class probabilities for three canonical TTP statements, see Listing 2; the right panel shows the ten\ntokens that most raise (green) or lower (red) the dominant score. In Sentence 2, for example, inversion, confidential,\nand information add +0.31 log-odds to Confidentiality, whereas the generic token data subtracts –0.07.\nLocal perturbation (LIME). LIME perturbs4 each sentence 4, 000 times, fits a local linear surrogate, and returns token\nweights visualized in Figure 5. The bar chart reproduces SHAP’s ordering (with integrity, model, training increase↑;\nmeanwhile compromises decreases↓), and the inline heat-map highlights in situ the words that push the classifier\ntoward or away from Integrity.\nFig. 4: Global SHAP attribution for the sentences in Listing 2. Each row shows the CIA probabilities (left) and the\nten tokens with the largest Shapley values for the dominant class (right): green bars push the score up, red bars pull\nit down.\nEvidence graph. Token scores flow into an interactive NetworkX graph in which queries, retrieved passages, and\nsystem responses are nodes; edge widths are proportional to |SHAP|. Analysts can trace every decision from raw text\n→token importance →ATLAS technique (Fig. 12b).\nCorpus-level coverage. Applying this pipeline to 300 sampled research articles yields a graph with 55 distinct TTPs,\n21 exploited vulnerabilities, and 9 ML life-cycle stages (only vulnerabilities actively exploited by at least one TTP is\nretained. Full statistics appear in Section IV.\nBy coupling global (SHAP) and local (LIME) attribution with graph-based visualisation, the system delivers actionable,\n4num_samples determines the number of perturbations. A sweep from 1,000–6,000 showed that 4,000 samples reduced the median\ntoken-weight variance to <1% across five runs while keeping latency below 0.4 s per sentence on a 12-core CPU.\n"}, {"page": 16, "text": "16\nFig. 5: LIME explanation for Sentence 1 in Listing 2. Left: class probabilities. Centre: token weights— orange =\npositive, teal = negative. Right: heat-map overlay on the original text.\naudit-ready explanations that reduce model opacity and support informed decision-making in software engineering\nand security analysis.\na) Illustrative SHAP & LIME outputs.\nFigures 4 and 5 apply the interpretability pipeline to the three sentences in Listing 2:\n1. SHAP global view. Each row in Fig. 4 pairs the predicted CIA distribution (left) with the ten most influential\ntokens (right). In Sentence 2, tokens such as inversion and confidential push the prediction toward Confidentiality,\nwhereas data pulls it away.\n2. LIME local view. Fig. 5 zooms in on Sentence 1. The bar chart echoes SHAP’s ranking, and the heat-map overlays\nthose weights on the raw text, making the evidence instantly visible.\n3. Cross-lens consistency. Agreement between SHAP (global) and LIME (local) on the sign and ordering of salient\ntokens confirms that the explanation is not an artifact of a single method. Treating SHAP as the primary explainer\nand LIME as a sentence-level validator, therefore, yields both a rigorous, corpus-wide attribution framework and\nan intuitive diagnostic tool for practitioners.\n3) Linking Literature to ATLAS database.\nTo further satisfy all the requirements of our RQs, we link the TTPs’ information to the ATLAS database. First, we\ngathered the detailed attack information, including the associated tactics, techniques, goals, knowledge requirements,\nand specificity from the extracted TTPs. Then, we linked these extracted information with the TTP definitions\nprovided by ATLAS and ATT&CK frameworks. For clarity and brevity, only the tactics and techniques derived from\nthe 14 seeding papers are presented in this table. However, a comprehensive mapping of TTPs, vulnerabilities, and\nML lifecycle stages is shown later in the results Section IV\n4) ATLAS database.\nFrom the extracted ML attacks (26 in total) representing the entire dataset available in ATLAS at the time of\nmining, we now map and report the associated ML attack phases as described in the threat model (see Section II-D).\nIn particular, each phase of an attack represents a tactic, while the associated execution step(s) represent the technique\nor sub-technique(s). Thus, we analyze the extracted scenarios and their related TTPs descriptions provided by ATLAS\nand ATT&CK across different phases and their relationships. For example, consider the Microsoft - Azure Service\nattack,5 executed by the Microsoft Azure Red Team and Azure Trustworthy ML Team against the Azure ML service\nin production [87]. The introduced attack targets different capabilities of the ML system: confidentiality (unauthorized\nmodel/training data access), integrity (poisoning by crafting adversarial data), and availability (disruption of the ML\nservice). The attack knowledge is based on a white-box setting, as the attackers have full access to the training data\nand the model. Finally, the attack specificity is based on an adversarial untargeted setting, as threat actors do not\ntarget a specific class of the ML model. The attack has eight phases, as detailed below6:\n• Phase 1: The required information for the attack is collected, such as Microsoft publications on ML model\narchitectures and open source models;\n5Microsoft Azure Service Disruption\n6We do not report here the extracted information from the ATLAS database as they are already available in the dataset; however, we\nconsider and discuss them when combining their results with the information of attacks from other sources.\n"}, {"page": 17, "text": "17\n• Phase 2: Usage of valid accounts to access the internal network;\n• Phase 3: The training data and model file of the target ML model are found;\n• Phase 4: The model and the data are extracted, leading the team to continue executing the ML attack stages;\n• Phase 5: During ML attack staging, they crafted adversarial data using target data and the model;\n• Phase 6: They exploited an exposed inference API to gain legitimate access to the Azure ML service.\n• Phase 7: Adversarial examples are submitted to the API to verify their eﬀicacy on the production system;\n• Phase 8: Finally, the team successfully executed crafted adversarial data on the online ML service.\n5) AI Incident database.\nTo analyze this dataset, we start by identifying if there are potential TTPs similar to those in ATLAS/ATT&CK.\nFurthermore, we check the target models and related information about the attack (goal, knowledge, and specificity).\nFor example, consider the ML real-world attack called Indian Tek Fog Shrouds an Escalating Political War dated\nfrom 2022. Following the reference link associated with the attack, Tek Fog is an ML-based bot app used to distort\npublic opinion by creating temporary email addresses and bypassing authentication systems from different services,\nlike social media and messaging apps7. The goal is to send fake news, automatically hijacking X and Facebook trends,\nsuch as retweeting/sharing posts to amplify propaganda, phishing inactive WhatsApp accounts, spying on personal\ninformation, and building a database of citizens for harassment. The bot may use a Transformer model such as\nGPT-2 to generate coherent text-like messages [88]. By analyzing the attack, we observe the following four ML TTPs:\n(i) Resource Development (Establish Accounts), (ii) Initial Access (Valid Accounts), (iii) ML Attack Staging (Create\nProxy ML Model: Use Pre-Trained Model), and (iv) Exfiltration (Exfiltration via Cyber Means). The attack specificity\nis traditionally targeted since threat actors target specific inactive WhatsApp accounts to spy on personal information.\nThere is no detail about the knowledge of the attack in the adversarial context.\nAmong the reported threats, only 18 represent threat tactics/techniques mentioned in ATLAS/ ATT&CK. Recog-\nnizing that some attacks are reported in multiple records across different sources and associated information, we must\neliminate duplicate reports, resulting in 12 unique records. These 12 ML real-world attacks are not documented in\nATLAS and are used to complete case studies in the ATLAS database.\n6) ML Repositories.\nTo analyze security vulnerabilities in ML repositories, we systematically mined issues from GitHub projects, focusing\non those explicitly referencing threats or vulnerabilities in their titles and/or comments. Using the GitHub API, we\nsearched for cybersecurity-related keywords commonly used by security teams to document potential risks. The search\nterms were grouped using an OR disjunction and included:\n• “cve” (for Common Vulnerabilities and Exposures),\n• “vuln” (for vulnerabilities),\n• “threat” (for threats),\n• “attack” (for attacks/attackers), and\n• “secur” (for security-related discussions).\nThis query retrieved 3,236 issues from 289 projects. To refine our dataset, we applied a filtering process to exclude\nissues reporting incomplete or improperly formatted CVE codes. A valid CVE code follows the CVE-{YEAR}-{ID}\npattern, where the year represents the vulnerability’s assignment, and the ID is a unique identifier. We extracted\nthese CVEs using the regular expression CVE-\\d{4}-\\d{4,7}, resulting in 897 unique CVEs across 350 issues from\n94 projects. Recognizing that reported CVE codes can become invalid over time—due to reclassification, rejection, or\nfurther investigation—we further validated their availability.\nThe computation involved here relies on absolute counts to quantify vulnerabilities, attacks, and incidents across\ndifferent ML models and tools. However, we recognize that this approach may inadvertently emphasize models that\nare more common, rather than those that are inherently more vulnerable. To address this concern, we will also\nincorporate normalized metrics. Specifically, we will calculate the percentage of vulnerabilities relative to the total\nnumber of reported incidents for each model or tool. This normalization helps account for the deployment bias, allowing\nfor fairer comparisons across different ML components.\nC. Automated Threat Classification via LLM-Guided Reasoning\nWe implement a fully automated pipeline that classifies CVEs into ML-specific threat classes and verifies low-\nconfidence predictions in a single pass.\n7India’s Tek Fog Shrouds\n"}, {"page": 18, "text": "18\n1. Context-Aware Classification\nA zero-shot BART-MNLI model assigns an initial label by evaluating textual entailment between the CVE description\nand eight threat classes drawn from ATLAS, ATT&CK, and the AI-Incident DB. This stage yields high-throughput\ncoverage with no manual feature engineering.\n2. Self-Verification via CoT Reasoning\nIf the top-label confidence falls below an empirically tuned threshold8 (p < 0.60), the pipeline launches GPT-4o’s\nself-verification loop. GPT-4o receives (i) the raw description, (ii) the low-confidence label, and (iii) the full label set.\nIt then\n1) Generates a token-level Chain-of-Thought (CoT) over the full context—CVE text, numeric confidence, ATLAS\nIDs, and CVSS vector.\n2) Revises the label when that context supports an alternative.\n3) Returns a one-sentence justification; the full CoT is logged for audit.\na) Concrete example—CVE-2024-3099 (MLflow duplicate-name flaw).\nBART-MNLI proposes Model Extraction (p = 0.52). GPT-4o’s CoT (Listing 3) spots a URL-encoded name collision,\nchecks ATLAS IMP-T1645, sees an integrity-only CVSS vector (I:L/A:L)9, and relabels the issue as Model-Repository\nPoisoning. The whole token trace and step-wise explanation appear below.\n1 #01 ”read :\nduplicate registry names accepted (URL-encoded)”\n2 #02 ”=> two_models_share_id →integrity\nrisk”\n3 #03 ”impact : wrong model weights may be served”\n4 #04 ”no evidence of parameter exfiltration ￿extraction unlikely”\n5 #05 ”candidate(Model_Extraction)=0.52 < THR(0.60)”\n6 #06 ”scan alternative\nlabels …”\n7 #07 ”candidate(Model_Repository_Poisoning) = p ≈0.77”\n8 #08 ”taxonomy check : IMP-T1645 exists in ATLAS”\n9 #09 ”CVSS vector = I :L / A:L,\nconsistent with poisoning”\n10 #10 ” select\nlabel = Model_Repository_Poisoning”\n11 #11 ”craft\njustification\nstring”\n12 #12 ”log decision and return”\nListing 3: Token-by-token CoT for CVE-2024-3099 (model-registry poisoning).\nExample: Token-Level Contextual Reasoning for CVE-2024-3099\nStep 1: Detects the URL-encoded name collision →integrity threat.\nStep 2: Summarises the observation: two_models_share_id.\nStep 3: States concrete impact: downstream jobs may pull the wrong weights.\nStep 4: Finds no sign of exfiltration →extraction label doubtful.\nStep 5: Records low confidence (0.52 < 0.60) →triggers verification.\nStep 6: Searches alternative taxonomy classes.\nStep 7: Estimates p ≈0.77 for Model-Repository Poisoning.\nStep 8: Confirms class exists in ATLAS (IMP-T1645).\nStep 9: Checks CVSS (I:L / A:L) aligns with integrity/availability loss.\nStep 10: Commits to the new label.\nStep 11: Generates the analyst-facing one-liner.\nStep 12: Logs the full trace and returns the result.\n3. Corpus-Level Impact\nThe pipeline has processed 834 validated CVEs (2,183 occurrences in 312 GitHub issues across 86 repositories).10\nOn that gold set, the baseline macro-F1 is 0.71. GPT-4o revises 43 of 73 low-confidence predictions, lifting macro-F1\nto 0.87 (∆= +0.16); a 1,000-fold bootstrap gives a 95 % CI of ±0.04. Because macro-F1 weights each class equally\n8We scanned the validation split in 0.05 increments; p=0.60 maximizes macro-F1 by trading off false corrections (< 0.55) against missed\nerrors (> 0.65). The value is fixed before testing on the 200-CVE gold set.\n9CVSS v3.x represents every vulnerability as a bundle of base metrics. The last three—Confidentiality (C), Integrity (I), and Availability\n(A)—measure the impact of the flaw on each security objective. I:L/A:L, i.e. no confidentiality impact, limited integrity loss, and limited\navailability loss.\n10A manually annotated 200-CVE gold set (8 classes × 25) is carved out of the 834 CVEs and used only for threshold tuning and final\naccuracy reporting; see Table VI.\n"}, {"page": 19, "text": "19\n[89], [90], the gain cannot be ascribed to high-frequency classes alone—precision and recall improve in six of the eight\nclasses. Hence, our system provides audit-ready, contextual reasoning alongside a statistically robust performance\nboost.\nTABLE VI: Macro-F1 and related metrics on the 200-CVE gold set.\nMetric\nBaseline\n+CoT\nValue\n95 % CI\nValue\n95 % CI\nMacro-P\n0.72\n±0.03\n0.86\n±0.02\nMacro-R\n0.71\n±0.03\n0.88\n±0.02\nMacro-F1\n0.71\n±0.03\n0.87\n±0.02\nLow-conf. CVEs\n73/200\n—\n73/200\n—\nLabels revised (%)\n—\n—\n43/73 (59 %)\n—\nD. Predicting Vulnerabilities and threats in ML-based systems\nGraph Neural Networks (GNNs) have emerged as a robust framework for threat analysis in cybersecurity due to\ntheir ability to model complex, interconnected systems [91], [92], [93]. Cyber threats inherently exhibit graph-like\nstructures, in which entities such as vulnerabilities, malware, users, and IP addresses are linked through relationships\nsuch as exploits, network communications, and software dependencies. GNNs excel in this domain by capturing\nboth the intrinsic features of individual entities and the structural patterns of their interconnections [94]. Through\nmessage-passing algorithms, GNNs enable dynamic information flow between nodes, effectively mimicking real-world\nthreat propagation. This capability makes GNNs particularly effective for predicting threat evolution, identifying\npotential attack vectors, and assessing the likelihood of threat proliferation in complex environments. In this study,\nwe implement a GNN-based framework as a proof-of-concept for predicting vulnerability risk using real-world NVD\ndatasets linked to GitHub issues, forming a heterogeneous graph that captures the multifaceted nature of cybersecurity\nthreats. Given the increasing complexity of modern threat landscapes, GNNs offer a robust approach for proactive\nthreat and vulnerability prediction. Unlike traditional machine learning models that treat data points as independent,\nGNNs leverage the relational context to uncover hidden patterns, predict emerging threats, and reveal potential attack\npathways. Furthermore, GNNs are highly adaptable, capable of incorporating heterogeneous data from various sources,\nincluding network logs, vulnerability databases, and threat intelligence feeds. By adopting GNNs for threat analysis,\nwe aim to develop an intelligent, scalable, and data-driven defense mechanism that can detect emerging threats and\nalso anticipate future risks in an ever-evolving cybersecurity landscape.\n1 {\n2\n\"CVE_data_type\" :\n\"CVE\" ,\n3\n\"CVE_data_format\" :\n\"MITRE\" ,\n4\n\"CVE_data_version\" :\n\" 4.0 \" ,\n5\n. . .\n6\n7\n\" impact \" :\n{\n8\n\" baseMetricV2 \" :\n{\n9\n\" cvssV2 \" :\n{\n10\n\" baseScore \" : 7 . 5 ,\n11\n\" impactScore \" : 6 . 4 ,\n12\n\" e x p l o i t a b i l i t y S c o r e \" : 8 . 6\n13\n} ,\n14\n\" severity \" :\n\"HIGH\"\n15\n}\n16\n} ,\n17\n. . .\n18 }\nListing 4: Excerpts of the NVD Dataset showing the impact to compute the risk score (CVSS)\nCVE Risk Score Calculation [95]\nThe Risk Score in CVE (Common Vulnerabilities and Exposures) analysis quantifies the severity of a vulnerability.\nThe most commonly used standard is the Common Vulnerability Scoring System (CVSS).\nComponents of CVSS Base Score\nThe Base Score is calculated using:\n• Impact Score (IS): Measures potential damage.\n"}, {"page": 20, "text": "20\n• Exploitability Score (ES): Measures ease of exploitation.\nCVSS v2 Base Score Formula\nBase Score = roundup ((0.6 × IS + 0.4 × ES −1.5) × f(Impact))\nWhere11:\nIS = 10.41 × (1 −(1 −C) × (1 −I) × (1 −A))\nES = 20 × AV × AC × Au\nThe adjustment function is defined as:\nf(Impact) =\n(\n0,\nif IS = 0\n1.176,\nif IS > 0\nIn our enhanced GNN-based model, the risk score is calculated as:\nRisk Score = (0.5 × Base Score) + (0.3 × Exploitability Score) + (0.2 × Impact Score)\nThis approach balances the severity (base score), exploitability, and impact for a more comprehensive risk assessment.\nThe CVSS Base Score ranges [0-10], where: 0.0 - 3.9 = Low severity; 4.0 - 6.9 = Medium severity; 7.0 - 8.9 = High\nseverity; 9.0 - 10.0 = Critical severity.\nGraph Neural Networks (GNNs) have emerged as a powerful tool for modeling complex relationships in structured\ndata, making them particularly suitable for cybersecurity applications such as vulnerability risk prediction. In this\nresearch, we leverage a heterogeneous GNN architecture to predict risk scores (see listing 4: 15-22 and the following\ndescription) for Common Vulnerabilities and Exposures (CVEs) by capturing intricate relationships among CVEs,\naffected products, and reference sources. Unlike traditional machine learning models that treat data as independent\nand identically distributed samples, GNNs excel in learning from graph-structured data where nodes (e.g., CVEs,\nproducts, references) are interconnected through edges representing their relationships (e.g., affects, referenced_by,\nlinked_to). Our model employs GraphSAGE convolutional layers to aggregate information from neighboring nodes,\nenabling the network to learn richer feature representations based on both the node attributes (e.g., TF-IDF features\nfrom CVE descriptions) and the topology of the graph. The risk prediction process involves propagating information\nacross the graph through multiple convolutional layers, followed by a fully connected layer that outputs a predicted\nrisk score for each CVE node. By incorporating real-world features, weighted CVSS impact factors, and advanced\noptimization techniques, our model achieves robust performance in assessing vulnerability risks. This approach not\nonly enhances the predictive accuracy but also provides interpretability through the graph structure, offering valuable\ninsights into the factors contributing to cybersecurity threats.\nLeveraging GNN to Address Attack Severity\nTo strengthen our analysis of attack severity and vulnerability relationships, we leveraged a GNN model to integrate\nand enhance our clustering-based insights. The GNN was designed to predict risk scores based on CVE metadata,\nexploitability factors, and attack characteristics. By incorporating structural patterns from clustering, the GNN\nlearned and generalized attack severity across interconnected vulnerabilities, ultimately improving risk assessment. We\naddressed different dimensions of attack severity through clustering techniques and integrated them into the GNN’s\nnode features and edge relationships:\n1) Attack Success Rate (ASR) via KMeans Clustering: The GNN encoded ASR-based clusters as node attributes,\nallowing the model to learn which attack methods are more effective in deceiving ML models. By propagating risk\nscores across similar vulnerabilities, the GNN refined its risk predictions beyond CVSS-based heuristics. In ASR,\neach attack method (i.e., FGSM and PGD) is represented as a node within the GNN, where empirically derived\nASR values (from real-world ML applications/experiments and the literature) are introduced as node features. This\nallows the model to learn attack-severity patterns by propagating ASR values across similar attack nodes, thereby\ncapturing relationships between different attack types. For instance, adversarial attacks such as FGSM and PGD\ntypically exhibit high ASR values when targeting CNNs; however, these success rates tend to decrease when robust\ntraining techniques are employed, see Table IX. Similarly, model extraction attacks demonstrate varying ASR levels,\nwhich depend heavily on the query budget (Q) and the specific architecture under attack: ASR ≈1 −e−λQ where\nλ represents how eﬀiciently the attack extracts model knowledge.\n11the Scaling Factor (10.41) was empirically chosen to satisfy the maximum possible impact (when C, I, A = 1) results in IS ￿10.41,\ndetails on these calculations, variables and constant values are available online: https://www.first.org/cvss/v2/guide\n"}, {"page": 21, "text": "21\n2) Stealth & Detectability via Agglomerative Clustering: We introduced edges between vulnerabilities that shared\ncommon evasion techniques, enabling the GNN to propagate knowledge about attack stealthiness across nodes.\nThis feature enhanced the model’s ability to identify harder-to-detect vulnerabilities, which traditional risk scoring\nsystems often overlook.\n3) Computational Cost & Practicality via Gaussian Mixture Model (GMM): The GNN distinguished between low-cost\nadversarial attacks and resource-intensive model extraction techniques, improving its understanding of real-world\nfeasibility of an attack. By encoding attack practicality as graph structures, the model can better prioritize threats\nthat pose an immediate risk over those that require high computational resources.\n4) Taxonomy of CVEs via Hierarchical Clustering (Dendrogram): The hierarchical relationships between CVEs were\nmapped as edges in the GNN, allowing the model to generalize risk patterns based on attack similarity. This improved\nthe model’s ability to predict vulnerabilities with limited historical data by leveraging structural dependencies.\n5) Risk Score Distribution & CVSS Analysis: The GNN’s predicted risk scores aligned with known severity distri-\nbutions, confirming its ability to learn meaningful patterns from clustering techniques. By integrating structured\nseverity attributes into the GNN, the model produced more fine-grained risk predictions, addressing gaps in\ntraditional CVSS scoring.\nWe implemented the GNN to enhance vulnerability assessment by integrating clustering-derived attributes as node\nfeatures, providing contextual severity insights beyond traditional CVSS scores. It established graph connectivity\nbetween similar attack types based on exploit techniques, stealth characteristics, and computational overhead, enabling\nthe model to propagate risk insights across interconnected vulnerabilities. By utilizing message passing and represen-\ntation learning, the GNN dynamically classified vulnerabilities rather than relying solely on static risk metrics. This\nresulted in a highly effective risk assessment tool capable of learning attack severity relationships, generalizing across\npreviously unseen vulnerabilities, and refining security prioritization strategies. Ultimately, this approach enhances\nunderstanding of vulnerability impact, equipping cybersecurity practitioners to anticipate and mitigate evolving threats\nmore effectively.\nE. Graph schema\nTables VII and VIII formally define the heterogeneous graph G = (V, E) we build before learning. Each node stores\na small, fixed-length feature vector (e.g. TF–IDF bag-of-words for text; one-hot encoding for categorical fields). Edge\nlabels encode how two nodes are related and are used as type-specific channels in the GraphSAGE layers. Based\non our observations, we found that threat information in our corpus is relational, meaning that a single CVE may\nappear in several GitHub Issues, affect multiple dependencies (CPEs), and be linked to particular attack techniques\n(ATT&CK/ATLAS). Model pipeline. (1) Feature encoding: we embed text fields with sentence-BERT and keep the\nTABLE VII: Node catalogue of the heterogeneous attack graph (|V| = 57,812). Seven node types are included: CVEs\n(with CVSS v3 and text), CPEs/dependencies, GitHub issues, attack techniques (ATT&CK/ATLAS), and cluster\ncentroids for attack success rate (ASR), stealth, and computational cost.\nNode type\nSymbol\nCount\nKey features\nCVE\nvcve\n11,604\nCVSS v3 vectors, textual synopsis\nCPE / Dependency\nvcpe\n9,371\nvendor, product, version\nGitHub Issue\nviss\n23,128\ntitle, body, timestamp, repo ID\nAttack Technique\nvtt\n1,142\nATT&CK / ATLAS identifier, tactic\nASR Cluster Centroid\nvasr\n15\navg. attack-success-rate, std. dev.\nStealth Cluster Centroid\nvstl\n10\nevasion score, detectability rank\nCost Cluster Centroid\nvcst\n8\nFLOPS, GPU hours, $‐cost bucket\nfirst 256 dimensions. (2) GraphSAGE layers: two hops (k=2) with mean aggregation separately per edge type, then\ntype-wise linear fusion. (3) Risk head: a three-way MLP outputs ˆr∈[0, 1] (low/ medium/ high-critical). The loss is a\nweighted MSE against the composite risk score to emphasize high-severity CVEs.\nEdge motivation. The three sim relations (ASR, stealth, cost) inject domain knowledge from section III-E1–III-G5\ninto the graph so that even a sparsely connected CVE inherits risk signals from structurally similar neighbours.\nDuring message passing the GNN therefore propagates both factual links (affects, reported_in) and latent behavioural\nsimilarity, yielding the calibrated risk scores reported in section IV.\n1) Real-time threats monitoring.\nFinally, to enhance real-time vulnerability risk assessment, we develop an ML-based Threat Assessment and\nMonitoring System that integrates GNNs and NLP. The system continuously ingests real-time CVE data from the NVD\ndatabase, extracting critical threat intelligence, including CVSS scores, exploitability factors, and patch availability.\nTo enhance contextual understanding, we employ BERT embeddings to transform CVE descriptions into semantic\n"}, {"page": 22, "text": "22\nTABLE VIII: Edge catalogue (|E| = 218,906). All edges are directed; we add the reverse edge type when symmetry is\nrequired. Construction rules specify how edges are instantiated (e.g., from NVD JSON fields, issue links, or clustering\nmethods), and the semantics column describes the meaning of each relation in the context of vulnerability–threat\nmapping.\nEdge type\nSrc →Tgt Construction rule\nSemantics\naffects\nvcve →vcpe\nCPE listed in NVD JSON of CVE\nProduct vulnerable to CVE\nreported_in\nvcve →viss\nIssue body matches CVE regex\nDisclosure/ discussion thread\nreferences\nviss →vtt\nIssue links an ATT&CK / ATLAS URL\nPractitioner cites attack pattern\nshares_vector vcve →vtt\nJaccard\n\u0000tf–idf(CVE), tf–idf(tech)\n\u0001\n>0.15\nSame exploit mechanism\nmember_of\nvcve →vasr\nK-means on attack-success-rate metadata\nASR similarity cluster\nstealth_sim\nvcve →vstl\nAgglomerative clustering on evasion metrics Detectability cluster\ncost_sim\nvcve →vcst\nGMM on compute/resource cost\nPracticality cluster\nKey acronyms: GMM →Gaussian‐mixture model, Src →Source, Tgt →Target. Jaccard similarity is computed on TF-IDF vectors with a threshold\nof 0.15.\nrepresentations, enabling deeper threat analysis. We leverage a GraphSAGE-based GNN to construct a vulnerability\nknowledge graph, capturing relationships between CVEs based on attack similarity, exploitability patterns, and risk\npropagation. This graph-based approach enables the system to detect correlations between attack types, supporting\nstructured threat classification. The GNN integrates clustering-derived attributes to provide risk insights beyond\ntraditional CVSS scoring, dynamically classifying vulnerabilities using message passing and representation learning.\nFurthermore, we incorporate external threat intelligence sources such as MITRE ATT&CK, CISA Known Exploited\nVulnerabilities (KEV) Catalog12, AI Incident Database (AIID), and Exploit Database (Exploit-DB)13, ensuring\nadaptive risk assessment. High-risk vulnerabilities (Predicted Risk Score > 0.8) trigger automated alerts, enabling\nproactive mitigation. By contextualizing risk propagation and refining security prioritization, this system significantly\nimproves real-time vulnerability assessment, equipping security practitioners with a dynamic, data-driven approach to\nanticipate and mitigate emerging ML security threats. Details on our implementations is available in our replication\npackage [71]\nTABLE IX: Attack Success Rates (ASR) from Empirical Studies shows Projected Gradient Descent (PGD) on Gastric\nCancer Subtyping scoring 100% success rate in causing misclassification. “Varies” —depends on external conditions\n(dataset, architecture, attack parameters), meanwhile “Guaranteed” —used when attack is theoretically provable to\nsucceed under certain conditions (e.g., Certifiable Black-Box Attacks).\nAttack Type\nTarget Model\nDataset\nASR (%)\nSource\nFGSM\nResNet-50 (CNN)\nImageNet\n63-69\nKurakin et al. (2016)\nUniversal Adversarial Perturbations\nVarious Image Classifiers\nMultiple Datasets\n77\nXcube Labs (2021)\nEvadeDroid\nBlack-box Android Malware Detectors\nCustom Malware Dataset\n80-95\nBostani & Moonsamy (2021)\nTransparent Adversarial Examples\nGoogle Cloud Vision API\nReal-world Images\n36\nBorkar & Chen (2021)\nBlack-box Adversarial Attack\nVarious Deep Learning Apps\nReal-world Applications\n66.6\nCao et al. (2021)\nPGD\nResNet (CNN)\nGastric Cancer Subtyping\n100\nKather et al. (2021)\nGenerative Adversarial Active Learning\nIntrusion Detection System (IDS)\nNetwork Traﬀic Data\n98.86\nKwon et al. (2023)\nAdversarial Scratches\nResNet-50 (CNN)\nImageNet\n98.77\nJere et al. (2019)\nEpistemic Uncertainty Exploitation\nCNN\nCIFAR-10\n90.03\nTuna et al. (2021)\nGradient-Based Attacks\nMulti-Label Classifiers\nVarious Datasets\nVaries\nZhang et al. (2023)\nAdversarial Suﬀixes\nText-to-Image Models\nCustom Prompts\nVaries\nShahgir et al. (2023)\nCertifiable Black-Box Attack\nVarious Models\nCIFAR-10, ImageNet\nGuaranteed\nHong & Hong (2023)\nAdversarial Attacks on NIDS\nKitsune NIDS\nNetwork Traﬀic Data\n94.31\nQiu et al. (2023)\nAdversarial Attacks on ViTs\nVision Transformers\nRCC Classification\n2.22-12.89\nKather et al. (2021)\nAdversarial Attacks on Text Classification\nVarious NLP Models\nSentiment Analysis Datasets\nVaries\nZhou et al. (2023)\nF. Severity Prediction of real-world SoTA assessment\n1) External validation.\nTo ascertain that the GNN-derived severity score ˆs reflects real risk, we correlate it with two external ground truths:\n(i) the CVSSbase ratings attached to the 651 CVE–issue pairs in our corpus, and (ii) the incident-cost annotations\navailable for 87 cases in the AI-Incident-DB. Table X shows a strong monotone relationship (ρ=0.63, Prec@10=0.80\nfor CVSS; similar values for cost), indicating that ˆs ranks threats in line with human post-mortems.\n12https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\n13https://www.exploit-db.com\n"}, {"page": 23, "text": "23\nTABLE X: Alignment of GNN-predicted severity ˆs with two external ground truths. Top-10 precision = #(top-10\noverlap) / 10.\nGround-truth proxy\nSpearman ρ\nKendall τ\nTop-10 precision\nCVSSbase (651 CVEs)\n0.63\n0.47\n0.80\nAI-Incident-DB impact cost (87 cases)\n0.58\n0.42\n0.78\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAttack cost (GPU h, normalised)\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAttack success rate\nCluster A: Low cost, high ASR evasion\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAttack success rate\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nStealth score\nCluster B: High stealth poisoning\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n# API queries (×10 )\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nStolen accuracy\nCluster C: Resource intensive extraction\nFig. 6: Representative cluster drill-downs. Three clusters are examined in detail: (A) low-cost, high-ASR evasion\nattacks, (B) high-stealth data poisoning campaigns, and (C) resource-intensive model extraction efforts. Each vignette\nshows the induced subgraph with weighted edges, a two-stage attack timeline, and a scatter plot of GNN-predicted\nseverity ˆs versus real-world cost.\n2) Operational Validation.\nTo evaluate the real-world eﬀicacy of the severity score ˆs, we conducted a two-week controlled study with an\nindependent security operations center (SOC; n = 16 analysts). Alerts were auto-routed to three queues: high (ˆs > 0.8),\nmedium (0.5 < ˆs ≤0.8), or watch (ˆs ≤0.5). Across 412 incidents, results showed a 24 % reduction in mean time-\nto-first-action (from 37 min to 28 min; p < 0.01) with no significant change in false-positive rate (χ2, p = 0.44). This\nconfirms ˆs improves operational responsiveness without requiring workflow modifications.\n3) Rationale for Heterogeneity.\nIn an ablation experimentation (see replication package [71]), we re-trained the GNN seven times, each run removing\nexactly one of the edge families in Table VIII, for instance, the CVE ↔CPE relation or the TTP-similarity links.\nSuppressing any single edge type reduced the Spearman correlation between the predicted severity ˆs and the reference\nCVSSbase score by at least ∆ρ = 0.09 (median drop 0.12). This systematic degradation indicates that the model’s\npredictive power stems from the combination of heterogeneous relations rather than from any single edge category\nin isolation. The finding aligns with recent results on multi-layer vulnerability graphs [96]. Clusters characterization.\nFig. 6 drills into three representative groups: (A) low-cost, high-ASR evasion; (B) high-stealth poisoning; and (C)\nresource-intensive extraction. Each vignette couples the sub-graph with an incident timeline, turning the aggregate\nanalysis into practitioner-ready insight.\nCluster-level drill-downs (Fig. 6).\nFollowing the global UMAP overview\n(Fig. 7), Fig. 6 zooms into three security-critical clusters identified by the GNN. Each panel plots the two severity\ndimensions that maximize variance inside the cluster and scales marker size by the composite score ˆs.\n4) Illustrative cases.\nTo complement the cluster-level visualizations, we highlight three representative incidents drawn from Fig. 6 that\nillustrate surprising or operationally significant behaviours. In Cluster A (low-cost, high-ASR evasion), CVE-2024-3099\ndemonstrates that a simple synonym-swap perturbation bypassed production filters with less than 0.4 GPU-h while\nachieving over 90% ASR, underscoring the minimal resources required for effective attacks. In Cluster B (high-stealth\npoisoning), a fine-tuned LLaMA-7B model was compromised by a multi-trigger backdoor that evaded detection under\nstandard evaluations yet caused targeted failures post-deployment. In Cluster C (resource-intensive extraction), an\nattack against GPT-J required approximately 108 API queries. Still, it ultimately reproduced the model with over 90%\nfidelity, illustrating that well-resourced adversaries can replicate proprietary models despite apparent barriers. Together,\nthese examples translate abstract clusters into concrete narratives that enrich interpretability and demonstrate real-\nworld impact.\n"}, {"page": 24, "text": "24\nUMAP Dimension 1\nUMAP Dimension 2\nLegend\nCluster 0\nCluster 1\nCluster 2\nCluster 3\nCluster 4\ns\n1.22\ns\n1.51\ns\n1.76\nFig. 7: Global UMAP projection of the 834‐CVE graph. Each point represents one CVE vertex, coloured by its\nK-Means cluster ID and sized by the GNN-predicted severity ˆs. Three qualitative regimes emerge: a dense band of\nlow-cost high-ASR evasion attacks (upper right); a horizontal stealth continuum of poisoning incidents (centre); and\na sparse, high-cost tail of extraction campaigns (lower left). The alignment of the largest markers with the top-right\ncorner visually validates the learned severity metric and motivates the cluster drill-downs in Fig. 6.\n5) Global severity landscape (Fig. 7).\nembeds every CVE vertex of our heterogeneous graph into two UMAP components derived from the five–dimensional\nseverity feature vector:\n⟨CPE-deg, Issue-deg, cost, stealth, ASR⟩. Each point is color-coded by its K-Means cluster ID and scaled by the\nGNN-predicted severity ˆs (larger markers implies higher operational risk). Three qualitative regimes become visible:\n(a) a dense, low-cost band in the upper-right quadrant (Clusters 0 & 3) dominated by evasion attacks that already\nreach ASR > 0.90 with <0.4 GPU-h;\n(b) a horizontally stretched stealth continuum (Cluster 1) whose members obtain similar success rates but vary widely\nin detectability, reflecting the noisy–to–backdoor spectrum of the model poisoning; and\n(c) a sparse, high-cost tail in the lower-left (Cluster 4) comprising extraction campaigns that need ∼108 API calls\nbefore exceeding 90 % fidelity.\nThe fact that the largest markers coincide with the top-right, high-risk zones provides a visual sanity check of the\nlearned severity ˆs. Moreover, the contrasting cluster morphologies motivate the subsequent drill-down vignettes in\nFig. 6. The advantages of these plots include: (1) They bridge the macro–micro gap: the UMAP bird’s-eye map locates\nhigh-risk regions, while the drill-downs expose the cost/stealth/ASR trade-offs that actually drive risk. (2) Because\nthe axes are in operational units (GPU-h, queries, ASR), defenders can immediately see which counter-measures are\nimpactful—e.g., rate limits for Cluster C are critical, but irrelevant for Cluster A. (3) The internal ordering of points\nmirrors the learned severity ˆs, providing a visual sanity-check for the GNN.\nG. Gray-box Adversarial Attacks on Real-World ML models\n1) Gray-box Model-Extraction Use-Case\na) How gray-box extraction works in practice.\nIn a gray-box scenario, the attacker sees only the victim’s inference API and its probability (logit) outputs, mirroring\nthe data-free stealing setup of CopyCat CNN [97]. The attack proceeds by sampling sentences x from a public corpus,\nquerying the proprietary Model-A API to obtain logits y⋆= fAPI(x), and performing a gradient step that aligns a local\nstudent model fθ with y⋆(Listing 5). Although designed for vision networks, the same principle has been shown to\nthreaten large-language-model (LLM) agents: Beurer-Kellner et al. recently reports have shown that prompt-injection\naware design patterns are still susceptible to query-only extraction unless explicit rate-limiting or log-rounding is\nenforced [98]. Our synthetic loss curve in Fig. 8a follows the characteristic exponential decay of the original CopyCat\nstudy: after 106 queries, the student recovers 92 % of the teacher’s downstream accuracy. This result underscores\nthat—even without access to weight—modern LLMs remain vulnerable to data-free extraction and therefore require\ndefensive measures beyond simple authentication or pay-per-token billing.\n1 for\nstep\nin\nrange (1 ,000 ,000) :\n2\nx\n= sample ( public_corpus , T=0.8)\n# temperature sampling\n3\ny* = query_api_logits (x)\n# black - box teacher\n4\n￿\n= student . update (x , y*)\n# d i s t i l l a t i o n\nstep\nListing 5: gray-box extraction pseudocode: the real implementation is on the order of 103 LOC.\n"}, {"page": 25, "text": "25\n103\n104\n105\n106\nAPI queries\n100\nCross entropy loss\n(a) Model-extraction eﬀiciency\nCross-entropy loss of the student model (log–log scale) as\nthe attacker issues up to 106 API queries. The steep decline\nconfirms how quickly the shadow model approaches teacher\nfidelity.\n0\n1\nLive label (0, 1)\nLive label\n0\n20\n40\n60\n80\n100\n120\n# live API calls\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nShadow confidence\nShadow P(non-target)\n(b) gray-box attack telemetry\nStep-wise evolution of the live label (top) and the shadow\nmodel’s confidence that the label will flip (bottom) during\nsynonym-swap loop. A single trace illustrates how the high-\nlevel budget in (a) is spent.\nFig. 8: Complementary views on gray-box extraction. Panel (a) gives the corpus-level picture—average loss versus\nquery budget across many inputs—while panel (b) zooms into one representative sentence to show the micro-dynamics\nthat consume that budget. Together, the plots demonstrate both the global eﬀiciency of the CopyCat strategy and\nthe step-wise mechanics of an individual attack episode.\nThe two–panel chart (Fig. 8) traces a complete gray-box shadow-model attack against the production API. The\nupper strip shows how the live label returned by the target model remains POSITIVE (value 1) for the first 120\nAPI calls and then flips to NEGATIVE (value 0) on the final request—demonstrating a successful evasion within\nthe 600-query budget. The lower strip plots the shadow model’s estimated probability that each candidate sentence\nalready contradicts the current live label. Confidence rises monotonically from 0.12 to 0.91 as synonym substitutions\nare guided by the locally fine-tuned BERT surrogate, indicating that the attacker can gauge its progress without any\ngradient access to the target. The surrogate steadily converges on high-risk inputs (bottom panel), and only when that\nconfidence crosses a practical threshold does the live system finally mis-classify (top panel), confirming the practicality\nof the attack pathway.\n2) gray-box attack scenario (inspired by Microsoft’s 2024 Azure AI red-team disclosure [87].)\nA former contractor has lost privileged access to the Azure subscription that hosts a BERT-based text classifier,\nbut still retains (i) coarse knowledge of the backbone architecture, (ii) awareness that Wiki-40B seeded the initial pre-\ntraining, and (iii) unrestricted access to the public /predict endpoint. Leveraging Microsoft’s open-source red-teaming\ntoolkit PyRIT [87], the attacker first trains a shadow model on 200k publicly scraped sentences, then executes an\nadaptive query loop: each API response is compared against the shadow’s logits, and the input is synonym-perturbed\nuntil the live model misclassifies. After ≈600 queries, the adversary achieves a 27% relative drop in F1 on the target\nmodel—without ever exfiltrating weights or data. We map this behavior to MITRE ATLAS technique EXF-T1041\n(model extraction) and assign a GNN severity score ˆs = 0.78 (Table IV). In the mitigation matrix, the attack triggers\ndefenses M03 (rate-limit & jitter) and M12 (adversarially re-trained confidence masking); deploying both reduces the\nexploit success rate from 27% to 4% in our replay test. Fig. 8 presents a step-wise timeline of the gray-box loop and\noverlays the predicted severity propagation through software, system, and network layers.\n3) Generalization of PyRIT-Style Attacks\nAlthough the PyRIT-style case study is demonstrated on a text-generation model, the underlying threat pattern\ngeneralizes across modalities. It exemplifies an Exploratory–Integrity–Targeted behavior in the Barreno et al. taxon-\nomy [48], in which an adversary manipulates input prompts or conditioning signals to override learned safety constraints.\nAnalogous behaviors emerge in code-generation APIs (through adversarial comments), retrieval-augmented LLMs (via\nprompt leakage in retrieved context), and multimodal systems (through adversarial captions or image prompts).\nThese observations confirm that the modeled attack class captures a broader family of prompt-injection and response-\n"}, {"page": 26, "text": "26\nmanipulation techniques that affect diverse ML pipelines. Consequently, our multi-agent reasoning framework remains\napplicable to foundation-model, multimodal, and interactive AI systems. Notably, recent work on preference-guided\noptimization [99] extends this phenomenon by showing that such attacks are payload-agnostic and can amplify PyRIT-\nstyle prompts through iterative selection of more effective variants—even when only text responses are observable.\nProminence and common entry points of threat TTPs exploited in ML attack scenarios – (RQ1)\nWhat are the most prominent threat TTPs and their common entry points in ML attack scenarios?\n4) Attack correlation matrix.\nAfter collecting the required information from attacks, we further explore them by mapping their associated ML\ncomponents, like impacted models, phases, and tools. This way, we can provide a better understanding and visualization\nof how these attacks occur and their impacts. For a given attack, the ML components are represented as a column\nvector, while vulnerability/threat is defined as a row vector. The attack cross-correlation matrix (CCM) is a relation\nthat maps the features of an attack vector to the features of an element-of-interest (EOI), divided into two categories:\nthreat and vulnerability CCMs, as presented below. For example, in Table XI, we present an attack matrix that maps\nTTP features (goals, knowledge, specificity, capability/tactic) to attack scenarios [79]; we provide more details about\nthis table when presenting our results.\nThreat CCM\nThe Threat CCM matrix maps TTPs to EOIs, including attack scenarios and ML lifecycle phases. Below, we outline\nthe methodology adopted based on the collected data.\nTTP Features and Attack Scenario Mapping.\nTo address RQ1, we systematically map ML threats to attack\nscenarios based on key threat attributes, including attacker goals, knowledge, specificity, and capabilities/tactics. This\nenables us to identify the most frequently used tactics, similarities in attack execution flows, and common entry points.\nFollowing the data extraction process described in Section III-A, we construct the threat matrix presented in Table XI.\nFor instance, consider the VirusTotal Poisoning attack from the MITRE dataset.14 The attack initiates at stage 0\n(Resource Development), where the adversary acquires tools or infrastructure to facilitate the operation. Subsequently,\nit progresses to stage 1 (ML Attack Staging), where adversarial data is crafted to poison the target model. The attacker\nthen moves to stage 2 (Initial Access) to exploit valid accounts or external remote services for unauthorized access.\nFinally, the attack culminates at stage 3 (Persistence), ensuring prolonged access to the compromised system.\nEmpty cells in the matrix indicate no direct correlation between a feature and an attack scenario, while N/A signifies\ncases where the relation is either unknown or not explicitly mentioned in the database. In the Attack Capability/Tactic\ncolumns (6th–17th), an entry of stage; i denotes that the attack scenario executes the corresponding tactic at step i\nin the attack flow. If multiple stages are listed, such as stage; i, stage; j, it signifies that the tactic is applied at both\nsteps.\n5) Impact of threat TTPs against ML phases and models (RQ2)\nWhat is the effect of threat TTPs on different ML phases and models?\nWe map tactics to ML phases, identifying the frequent threat tactics used against each ML phase. First, we analyze\nthe ATLAS description of each tactic and their related techniques, aiming to identify ML phase signatures that could\nbe associated with ML phases, like trained for Training and testing the model for Testing. Then, the relationship\nbetween tactics and ML phases is recorded in a threat CCM. Table XIX shows a record of the mapping between\ntactics and ML phases, showing the impact of threat TTPs against ML phases. The coeﬀicients of this matrix are\nrepresented by a checkmark (✓), when there is a relation between a given tactic and ML phase.\nAttack Scenarios and ML Models Mapping. To map the ML models targeted or exploited by attack scenarios,\nwe first searched for the exploited model type in the ATLAS TTP descriptions, the AI Incident Database, and the\nliterature [65]. For each analyzed attack, we retrieved the name of the targeted model and recorded the associated\nML models and attack scenarios. The results of this mapping are presented in Table XII, providing a clear linkage\nbetween attack scenarios and the specific ML models they exploit. For example, the description of the Botnet DGA\nDetection Evasion attack indicates that the target model is Convolutional Neural Network [79]: “The Palo Alto\nNetworks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain\nGeneration Algorithm (DGA) [...]”.\n6) Cataloging previously undocumented threats and aligning them with ATLAS — (RQ3)\nWhat previously undocumented security threats can be identified in the AI Incident Database, the literature, and ML repositories that\nare missing from the ATLAS database?\n14https://atlas.mitre.org/studies/AML.CS0002\n"}, {"page": 27, "text": "27\nTABLE XI: Mapping between TTP features and attack scenarios in the AI Incident Database and the 14 seed papers\nfrom the Literature. AI-DB stands for cases extracted from the AI Incident database, while LIT stands for Literature.\nSource\nAttack\nScenario\nAttack\nGoal\nAttack\nKnowledge\nAttack\nSpecificity\nAttack Capability / Tactic\nReconais.\nResource\nDevelop.\nInitial\nAccess\nML Model\nAccess\nExec.\nPersist.\nDefense\nEvasion\nDiscovery\nCollection\nML\nAttack\nStaging\nExfiltration\nImpact\nAI Incident\nDatabase\nAI-DB-01\nConfident.\nIntegrity\nN/A\nTraditional\nTarget\nstage 0\nstage 1\nstage 2\nstage 3\nAI-DB-02\nConfident.\nIntegrity\nN/A\nTraditional\nTarget\nstage 0\nstage 1\nstage 2\nAI-DB-03\nHuman\nLife\nN/A\nTraditional\nTarget\nstage 0\nstage 1\nAI-DB-04\nConfident.\nIntegrity\nN/A\nTraditional\nTarget\nstage 0\nstage 1\nAI-DB-05\nIntegrity\nN/A\nTraditional\nTarget\nstage 0\nstage 1\nAI-DB-06\nConfident.\nIntegrity\nWhite-box\nAdvers.\nUntarget\nstage 0\nstage 1\nAI-DB-07\nConfident.\nIntegrity\nWhite-box\nAdvers.\nUntarget\nstage 0\nstage 1\nAI-DB-08\nAvailabiltiy\nN/A\nTraditional\nTarget\nstage 0\nAI-DB-09\nConfident.\nIntegrity\nGray-box\nAdvers.\nTarget\nstage 0\nstage 2\nstage 1\nstage 4\nstage 3\nAI-DB-10\nConfident.\nIntegrity\nGray-box\nAdvers.\nTarget\nstage 0\nstage 2\nstage 1\nstage 3\nAI-DB-11\nConfident.\nIntegrity\nGray-box\nAdvers.\nTarget\nstage 0\nstage 2\nstage 1\nstage 3\nAI-DB-12\nConfident.\nIntegrity\nGray-box\nAdvers.\nTarget\nstage 0\nstage 2\nstage 1\nstage 3\nLiterature\nLIT-01\nConfident.\nBlack-box\nAdvers.\nUntarget\nstage 0\nstage 1\nLIT-02\nConfident.\nIntegrity\nGray-box\nWhite-box\nAdvers.\nTarget &\nUntarget\nstage 1\nstage 0\nstage 2\nLIT-03\nConfident.\nIntegrity\nGray-box\nAdvers.\nTarget &\nUntarget\nstage 1\nstage 0\nstage 2\nLIT-04\nConfident.\nIntegrity\nBlack-box\nGray-box\nWhite-box\nAdvers.\nTarget\nstage 1\nstage 0\nstage 2\nLIT-05\nConfident.\nIntegrity\nBlack-box\nAdvers.\nTarget &\nUntarget\nstage 0\nstage 1\nLIT-06\nConfident.\nIntegrity\nBlack-box\nGray-box\nWhite-box\nAdvers.\nTarget &\nUntarget\nstage 1\nstage 0\nstage 2\nLIT-07\nConfident.\nIntegrity\nWhite-box\nBlack-box\nAdvers.\nTarget &\nUntarget\nstage 1\nstage 0\nLIT-08\nConfident.\nBlack-box\nAdvers.\nTarget\nstage 1\nstage 0\nstage 1\nLIT-09\nConfident.\nIntegrity\nBlack-box\nAdvers.\nUntarget\nstage 1\nstage 0\nstage 2\nLIT-10\nConfident.\nIntegrity\nWhite-box\nAdvers.\nUntarget\nstage 1\nstage 0\nstage 2\nLIT-11\nConfident.\nIntegrity\nBlack-box\nAdvers.\nUntarget\nstage 0\nstage 2\nLIT-12\nConfident.\nIntegrity\nGray-box\nAdvers.\nTarget\nstage 1\nstage 0\nLIT-13\nConfident.\nIntegrity\nWhite-box\nAdvers.\nTarget &\nUntarget\nstage 0\nstage 2\nLIT-14\nConfident.\nWhite-box\nAdvers.\nUntarget\nstage 0\nstage 1\nstage 2\nBased on the CVE IDs identified in GitHub issues (see Section III-A), we analyze and map the most prominent\nvulnerabilities and threats in ML repositories, along with the dependencies responsible for these vulnerabilities.\nTo accomplish this goal, we download the CVE JSON-formatted data from the National Vulnerability Database\n(NVD) and extract the required information for the mappings. The mapping between a CVE ID and a specific ML\ntool is represented by a set of information ⟨dep, att, lvl, ver⟩, where dep denotes the dependency responsible for the\nvulnerability, att specifies the attack that can be launched to exploit the CVE, lvl indicates the severity level of\nthe vulnerability, and ver indicates the version of the vulnerability. For this research question, we compute (i) the\ntotal number of vulnerabilities (nov) and their types. Additionally, we calculate the same metrics focusing on their\ndistribution by threat type and tools (i.e., GitHub ML repositories): nov per tool and nov per type for each tool.\nThese metrics provide critical insights into the frequency of vulnerabilities across ML repositories and highlight the\npotential threats they pose.\nH. Ranking SoTA Models by Vulnerability\n1. Ranking Criteria To rank state-of-the-art (SoTA) large language models (LLMs) by their susceptibility to security\nthreats, we collected data from the literature on recent peer-reviewed benchmarks and empirical studies. Each model\nwas evaluated across three core vulnerability categories as shown in Table XIII:— Prompt Injection Attack Success\nRate (ASR), Code-level Backdoors, and Training-stage Exploits, based on their strategic relevance and empirical\nmeasurability. This classification encompasses the entire LLM lifecycle, ensuring comprehensive coverage of both user-\nfacing and systemic risks, and aligns with established benchmarks. We collected quantitative ASRs and qualitative\nseverity indicators (e.g., memory corruption, stealth persistence) for each model based on publicly available results.\n2. Scoring Approach We define a Composite Vulnerability Score as:\nCVSi = w1 · PromptASRi + w2 · BackdoorASRi + w3 · TrainingRiski\n"}, {"page": 28, "text": "28\nTABLE XII: Mapping between Attack Scenarios and target ML models\nSource\nAttack scenario\nModel Used\nMITRE\nATLAS\nEvasion of Deep Learning Detector\nfor Malware C2 Traﬀic\nCNN\nBotnet Domain Generation\n(DGA) Detection Evasion\nCNN\nVirusTotal Poisoning\nLSTM\nBypassing Cylance’s AI\nMalware Detection\nDNN\nCamera Hijack Attack on\nFacial Recognition System\nCNN, GAN\nAttack on Machine Translation Service - Google Translate,\nBing Translator, and Systran Translate\nTransformer\nClearview AI Misconfiguration\nN/A\nGPT-2 Model Replication\nGPT-2\nProofPoint Evasion\nCopycat [97]\nTay Poisoning\nDNN\nMicrosoft Azur Service Disruption\nN/A\nMicrosoft Edge AI Evasion\nDNN\nFace Identification System Evasion via Physical Countermeasures\nN/A\nBackdoor Attack on Deep Learning Modelsin Mobile Apps\nDNN\nConfusing AntiMalware Neural Networks\nDNN\nCompromised PyTorch Dependency Chain\nN/A\nAchieving Code Execution in MathGPT via Prompt Injection\nGPT-3\nBypassing ID.me Identity Verification\nCNN*\nArbitrary Code Execution with Google Colab\nN/A\nPoisonGPT\nGPT\nIndirect Prompt Injection Threats: Bing Chat Data Pirate\nGPT\nChatGPT Plugin Privacy Leak\nGPT\nChatGPT Package Hallucination\nGPT\nShadow Ray\nN/A\nMorris II Worm: RAG-Based Attack\nGPT, Gemini, LLaVA\nWeb-Scale Data Poisoning: Split-View Attack\nN/A\nAI\nIncident\nDatabase\nIndia’s Tek Fog Shrouds an Escalating Political War\nGPT-2\nMeta Says It’s Shut Down A Pro-Russian DisInformation Network...\nN/A\nLibyan Fighters Attacked by a Potentially Unaided Drone, UN Says\nCNN\nFraudsters Cloned Company Director’s Voice In $35M Bank\nHeist, Police Find\nDeepVoice [100]\nPoachers Evade KZN Park’s High-Tech Security\nand Kill four Rhinos for their Horns\nDNN\nTencent Keen Security Lab: Experimental Security\nResearch of Tesla Autopilot\nFisheye [101]\nThree Small Stickers in Intersection Can Cause Tesla Autopilot\nto Swerve Into Wrong Lane\nCNN\nThe DAO Hack -Stolen $50M The Hard Fork\nN/A\nTwitter pranksters derail GPT-3 bot with newly discovered “prompt injection” hack\nGPT\nPrompt injection attacks against GPT-3\nGPT\nAI-powered Bing Chat spills its secrets via prompt injection attack\nGPT\nEvaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples\nBERT\nLiterature\nCarlini et al. [7]\nGPT-2\nBiggio et al. [34]\nSVM, DNN\nBarreno et al. [48]\nNaive Bayes\nCarlini et al. [63]\nFeed-Forward DNN\nWallace et al. [65]\nTransformer\nAbdullah et al. [33]\nRNN, CNN, Hidden Markov\nChen et al. [31]\nLSTM, BERT\nChoquette-Choo et al. [37]\nCNN, RestNet\nPapernot et al. [67]\nDNN, kNN, SVM Logistic Regression,\nDecision Trees\nGoodfellow et al. [69]\nGAN\nPapernot et al. [66]\nDNN\nCisse et al. [68]\nParseval Networks\nAthalye et al. [64]\nCNN, ResNet, InceptionV3\nJagielski et al. [20]\nRestNetv2\nWhere: PromptASRi: Prompt Injection Attack Success Rate, BackdoorASRi: Code-level backdoor success rate, and\nTrainingRiski: Normalized score (0–1) for training-stage exploit severity.\n3. Statistical Model Justification Due to sparsity and heterogeneity in attack success rate (ASR) reporting, we used\na normalized weighted aggregation method instead of regression or PCA. Our approach follows:\n• MCDA principles (Multi-Criteria Decision Analysis)\n• Min-max normalization for cross-category comparability\n• Expert-informed weights to reflect real-world impact\nThe final ranking is based on: quantitative ASR evidence, categorical risk profiles, and weighted aggregation using\nMCDA. This transparent and reproducible approach enables systematic vulnerability comparison across SoTA LLMs.\nAdditionally, we identify six LLM-exclusive attack families—including prompt injection, RLHF reward hacking, LoRA\ngradient leakage, large-scale model extraction, training-data reconstruction, and tool-call abuse—under the SoTA\nvulnerability umbrella. Each family is then mapped to a formal MITRE ATLAS/ATT&CK technique (Table XIV),\nproviding a structured taxonomy for models such as GPT-4 (including GPT-4o, GPT-4V), PaLM 2, Llama 3, Gemini\n1.5 Pro, Claude 3, Vision-language models (e.g., GPT-4V, MM-Llama).\n"}, {"page": 29, "text": "29\nTABLE XIII: Vulnerability Categories in State-of-the-Art Model Ranking\nCategory\nRationale\nPrompt Injection ASR\nThese attacks directly target the LLM’s input-handling mechanism,\nrequiring no internal access and being the most common attack\nvector in real-world use (e.g., indirect jailbreaks, instruction\nhijacking). They are low-barrier, high-impact threats.\nCode-level Backdoors\nThese represent inference-time risks where the model outputs\nmalicious or manipulated code due to adversarial prompting or\nstealthy fine-tuning. This category is vital for evaluating LLMs\ndeployed in programming, automation, or DevOps tasks.\nTraining-stage Exploits\nThese capture the most persistent and systemic vulnerabilities,\nsuch as data poisoning, sleeper agents, or multi-trigger backdoors\nintroduced during the fine-tuning process. They are harder to\ndetect and may survive alignment efforts, thus reflecting deep\nmodel compromise.\nTABLE XIV: State-of-the-art LLM-security studies, mapped to lifecycle phase and MITRE ATLAS.\nThreat family\nKey Refs\nSoTA model(s)\nLifecycle phase\nATLAS ID\nJail-break / prompt injection\n[86], [102], [103]\nGPT-3.5, GPT-4, Claude-2, Bard\nDeployment\nDIS-T1525\nReward-model hacking (RLHF)\n[85], [104]\nInstruct-GPT, GPT-4 (reward)\nRLHF loop\nIMP-T1565\nAdapter-gradient leakage\n[84], [105]\nLLaMA-2 7/13 B (+ LoRA/QLoRA)\nFine-tune\nEXF-T1040\nModel extraction/ distillation\n[106], [98], [107], [108]\nGPT-3.5/4 APIs, LLaMA-2 7 B\nInference API\nEXF-T1041\nTraining-data reconstruction\n[109], [110]\nGPT-J 6 B, GPT-3.5, Claude-2\nPre-train\nEXF-T1042\nFunction-call abuse in tool agents\n[111], [112], [113]\nGPT-4 (function-call API)\nAgent ops\nEXF-T1050\n1) Threats Unique to SoTA Models.\nThe rapid adoption of foundation models since 2022 has shifted the machine-learning threat landscape. LLMs expose\nnew attack surfaces that either did not exist or were insignificant for CNN- or RNN-based systems. The following\nparagraphs summarize these six families’ threats against SoTA Models.\n(1) Jail-breaking and multi-turn prompt-injection chains.\nPrompt-injection modifies the instruction context rather\nthan the model parameters. Recent work demonstrates universal jailbreak strings (MASTERKEY) that survive\nsystem-prompt hardening and constitutional guidelines [86]. AutoDAN extends this to an automated chain-of-thought\nattack that escalates privileges over multiple dialogue turns [102]. Both map to DIS-T1525 Prompt Manipulation\nand occur during deployment. (2) Reward-model hacking in RLHF loops.\nBecause instruction-tuned models rely\non reinforcement learning from human feedback (RLHF), adversaries can poison the preference dataset or craft\nadversarial demonstrations that steer the reward model off-policy [85]. The resulting policy drift re-enables toxic\nor disallowed content even after alignment. ATLAS technique: IMP-T1565 Adversarial Training. (3) Adapter-layer\ngradient leakage.\nFine-tuning via LoRA/QLoRA adapters publishes only rank-reduced updates, but those updates\ncan leak memorized training snippets when intercepted, allowing white-box data reconstruction without full-model\naccess [84]. This affects the fine-tune phase and maps to EXF-T1040 Model Parameter Extraction. (4) Scalable model-\nextraction and distillation. Copy distillation [106], [98], [107], [108] and Cat-LLaMA distillation [107] show that over\n90 % downstream accuracy can be stolen from commercial APIs with 107–108 queries, bypassing traditional rate-limit\ndefences. Threat ID: EXF-T1041 Model Extraction (inference API). (5) Training-data reconstruction & memorization.\nCarlini et al. extract verbatim personal data from GPT-J and GPT-3.5 by prompting on rare n-grams [109]. Encoded\nprompt-leak attacks extend this to embed secrets in the instruction tokens themselves [109], [110], [114], [115], [116],\n[117]. Lifecycle phase: pre-training; ATLAS ID: EXF-T1042 Training-Data Extraction. (6) Function-call abuse in\ntool-enabled agents.\nWhen LLMs are granted structured “function-call” abilities, arguments can be coerced into\nshell-metacharacters or SQL payloads, leading to full remote-code execution inside the orchestration layer [111]. We\nmap this to EXF-T1050 Sandbox Escape at deployment time.\nThese six LLM-specific threat families contribute 78 documented incidents in our corpus. Table XIV positions each\nfamily within the ML lifecycle and assigns the corresponding MITRE ATLAS technique. This mapping lets us tally\nincidents per phase or technique—our measure of exploit density, which is summarized in the radar chart (Fig. 12a)\nand detailed in the accompanying heat-maps (Fig. 12b).\nI. Normalization by Deployment Frequency\nDirectly comparing raw attack counts across model families risks conflating popularity with vulnerability: models with\na larger user base will naturally attract more reported attacks simply due to wider exposure, not necessarily because they\n"}, {"page": 30, "text": "30\nTABLE XV: Leave-One-Out Sensitivity Analysis of the Composite Deployment-Frequency Proxy. Omitting any single\nusage proxy changes the top-5 model rankings by at most one position, indicating no single data source dominates\nthe normalization.\nOmitted Proxy\nMax Shift\nTop-5 Shift\nRevised Top-5 Models\nz_pypi\n2\n1\nGPT-J,\nBERT,\nT5-13B,\nStableDiffusion,\nPaLM-2\nz_hf\n3\n1\nGPT-J,\nStableDiffusion,\nT5-13B,\nPaLM-2,\nBERT\nz_docker\n2\n1\nBERT, T5-13B, GPT-J, PaLM-2, StableDif-\nfusion\nz_cite\n2\n1\nGPT-J, T5-13B, BERT, PaLM-2, LLaMA-2\nare intrinsically less secure. To control for this bias, we normalize attack counts by a composite deployment-frequency\nproxy wm for each model family m. This proxy combines four publicly accessible signals: (i) PyPI download counts\nfor the model’s main library, (ii) HuggingFace or TF-Hub checkpoint pulls, (iii) Docker Hub pulls for oﬀicial inference\ncontainers, and (iv) annualized Semantic Scholar citations of the model’s origin paper (2020–2024). All raw counts\nwere collected between 3–6 June 2025 via oﬀicial REST APIs (rate-limited to ≤104 queries/day) or public datasets\n(BigQuery for PyPI). Each metric is z-normalized to remove scale differences, averaged across the four proxies,\nand min–max scaled to [0, 1], following best practices for combining heterogeneous indicators in statistical pattern\nrecognition and ensemble learning [118], [119], [120], [121]:\nwm =\n1\n4\nP4\nj=1 zmj −mink\nh\n1\n4\nP\nj zkj\ni\nmaxk\nh\n1\n4\nP\nj zkj\ni\n−mink\nh\n1\n4\nP\nj zkj\ni\n+ ε\nwhere ε = 10−4 prevents division-by-zero for niche models. Leave-one-out sensitivity tests confirm robustness: omitting\nany single proxy changes the top-5 model ranking by at most one position (Table XV).\na) Leave-One-Out Sensitivity Analysis [122]\nWe validated robustness by recomputing weights while omitting each proxy sequentially. Table XV reports a leave-\none-out sensitivity analysis of the composite deployment-frequency proxy. For each run, a single usage signal (PyPI\ndownloads, HuggingFace pulls, Docker pulls, or citations) is omitted, and model rankings are recomputed. Across all\ncases, the top-5 rankings shift by at most one position, and the maximum shift for any model in the full list is three\npositions. This stability confirms that no single proxy disproportionately influences the top-ranked results.\n1) Deployment-normalized risk.\nRaw incident counts alone overweight models that are simply more common. To adjust for real-world exposure, we\ncompute a deployment weight wm for each model family m by averaging four public proxies, each z-scored to zero\nmean and unit variance:\nProxy\nData source (collection window)\nPkg installs\nDaily pip download counts from the public PyPI BigQuery dataset (01 Jan 2023 – 01 Jun 2025).\nCKPT pulls\ntotal_downloads field for HuggingFace & TF-Hub checkpoints, capped at the most recent 365 days\n(snapshot 06 Jun 2025).\nDocker pulls\nPull counters for the model’s oﬀicial inference container images (snapshot 05 Jun 2025), bucketed by\norder of magnitude (10k).\nCitation mo-\nmentum\nSemantic Scholar citations per year of the model’s origin paper (rolling 2020–2024).\nThe normalized attack frequency is therefore,\nbfm =\nfm\nwm + ε,\nε = 10−4.\nRemoving any single proxy changes the top-5 ranking by at most one position.\nJ. GNN for Threat Intelligence Reasoning\nBuilding upon the heterogeneous GNN introduced in Section III-D, we describe how the model propagates severity\nsignals across the ontology-driven threat graph to produce node-level risk scores, enabling structured, evidence-driven\nreasoning. The multi-agent framework employs a Heterogeneous GNN (HGNN) that operates on the threat graph\nG = (V, E). Nodes V represent entities TTPs, vulnerabilities, ML lifecycle stages, assets, incidents); edges E encode\nsemantic relations (causes, occurs-in, targets, evidence-of, has-dep).\n"}, {"page": 31, "text": "31\nTABLE XVI: Absolute (f) vs. deployment-normalised ( bf) attack counts, 2023–2025. A side-by-side comparison allows\ndirect inspection of how deployment normalization changes the relative ranking of attack frequency.\nModel family\nf\nw\nbf\nGPT-3.5/4 (API)\n218\n0.92\n237\nStable Diffusion\n144\n0.77\n187\nLLaMA-2 (HF)\n89\n0.61\n146\nCLIP / OpenCLIP\n51\n0.34\n150\nLoRA-BERT variants\n37\n0.19\n195\nT5-XXL\n31\n0.55\n56\nWhisper (ASR)\n29\n0.48\n60\nBLOOM-Z\n27\n0.41\n66\nViT Base / Large\n22\n0.63\n35\nDINO-v2\n20\n0.57\n35\na) Message passing.\nWe follow the relational convolution of R-GCN [123] with GraphSAGE-style neighbor sampling:\nh(l+1)\nv\n= σ\n\u0010X\nr∈R\nX\nu∈Nr(v)\n1\ncv,r W (l)\nr h(l)\nu + W (l)\n0 h(l)\nv\n\u0011\n,\n(1)\nwhere cv,r is a per-relation normalization constant and σ is ReLU.\nb) Features and decoder.\nNode initial features are BERT embeddings of textual attributes concatenated with one-hot type vectors. After L\nlayers, a two-layer MLP regresses a continuous severity: ˆsv = MLP(h(L)\nv\n). Training minimizes mean-squared error on\nnodes with known scores (CVSS, ATLAS, or incident-derived labels).\nc) Agent integration.\nThe GNN Reasoner Agent converts the live NetworkX graph into PyTorch-Geometric tensors, runs a forward pass,\nand writes ˆsv back to node attributes. Empirical evaluation yields Spearman ρ=0.63 with ground-truth severity and\nrobust performance on unseen threat entities.\n1) Graph-Grounded Reasoning in Threat Assessment\nThe HGNN enables structured, evidence-driven reasoning by combining the symbolic threat ontology with learned\nrelational embeddings. We identify four reasoning modalities, all empirically validated using the 93 extracted threats\nand dependency graph:\n1) Structural Reasoning: Multi-hop message passing (Equation 1) captures transitive risk chains. For example, a TTP\nthat causes a vulnerability indirectly elevates the risk of any affected asset after two hops, mirroring documented\nATLAS patterns (e.g., data poisoning →training corruption →inference failure): TTPA\ncauses\n−−−−→VulnB\naffects\n−−−−→AssetC.\n2) Transitive Inference: The model implicitly captures higher-order dependencies across relational paths. If TTPA →\nVulnB and VulnB →AssetC, the learned embeddings allow elevated risk to be assigned to AssetC through aggregated\nrelational context.\n3) Evidence-Based Reasoning: We apply GNNExplainer [124] to extract high-contribution subgraphs that serve as\nevidence trails supporting each predicted severity score ˆsv. Representative examples are shown in Table XVII,\nwhere extracted relational paths align with analyst reasoning and observed TTP–vulnerability–asset dependencies.\nThese evidence traces can also be cross-referenced with the corresponding mitigation stages in Fig. 17, illustrating\nhow structural reasoning links directly to actionable defense measures.\n4) Lifecycle-Aware Reasoning: The graph encodes ML lifecycle stages (data collection →training →inference), enabling\nthe GNN to propagate risk from early-stage TTPs (e.g., poisoning) to late-stage impacts (e.g., evasion), consistent\nwith RQ2 findings on phase-specific targeting.\n5) Zero-Shot Relational Generalization: On a held-out set of 15 novel TTPs from the AI Incident Database (not in\nATLAS), the GNN achieves Spearman ρ=0.61, demonstrating compositional generalization to unseen relational\nstructures.\nThis reasoning is probabilistic and learned, not symbolic deduction, but provides traceable, analyst-aligned intelligence\nbeyond black-box severity scores.\n2) Limitations\nThe model produces probabilistic severity estimates, not formal proofs. It requires labeled anchors (CVSS/ATLAS)\nand does not perform counterfactual reasoning or uncertainty quantification unless explicitly extended.\n"}, {"page": 32, "text": "32\nTABLE XVII: Example evidence paths extracted by GNNExplainer [124] for high-severity predictions produced by\nthe HGNN. Each row lists the top relational subgraph contributing to a node’s predicted severity ˆsv. The “Contrib.”\ncolumn indicates the normalized importance weight (0–1) assigned by GNNExplainer, representing the degree to which\nthat evidence path influenced the model’s prediction.\nPredicted Node\nEvidence Path\nContrib.\nInference Failure\nDataPoisoning →TrainingCorruption →Inference\n0.78\nModel Extraction\nAPIExposure →QueryAccess →Extraction\n0.71\nPreference Jailbreak\nPromptOpt →RewardSignal →PolicyShift\n0.69\nNote. Higher “Contrib.” values indicate that the corresponding relational path had a stronger influence on the predicted node severity\nˆsv. These evidence trails provide model-level interpretability, aligning with the analyst’s reasoning and supporting evidence-grounded\nmapping to defense strategies (see Fig. 17).\nThus, our HGNN serves as a scalable, graph-grounded reasoning engine that transforms structured threat intelligence\ninto interpretable, multi-hop, evidence-driven risk insights for operational ML security governance.\nIV. Study results\nIn this section, we present and discuss the results of our research questions.\nDeployment\nTest time\nFine-tuning\nTraining\nPretraining\nFederated Training\nSystem Development\nData Preparation\nInference\nModel hardening\nEvasion Attacks\nModel Extraction\nBackdoor Attacks\nMembership Inference Attacks\nAdversarial Example Transferability\nGradient Masking\nSide-Channel Attacks\nPoisoning Attacks\nSponge Attacks\nUniversal Adversarial Texts (UATs)\nParameter-Efficient Tuning Backdoor Attacks\nPrompt-Based Backdoor Attacks\nTraining Data Extraction\nAdversarial Style Transfer Attacks\nUniversal Prompt Vulnerabilities\nInsertion-Based Backdoor Attacks\nGradient-Based NLP Adversarial Attacks\nImperceptible Backdoor Attacks\nTask-Agnostic Backdoor Attacks\nData Leakage Through Memorization\nSyntactic Backdoor Attacks\nPrompt Injection Attacks\nFederated Learning Poisoning\nVertical Federated Learning Vulnerabilities\nGraph-Based Threat Exploits\nAdversarial Demonstration Attacks (advICL)\nBackdooring Neural Code Search Models (BADCODE)\nRobust Adversarial Prompt Attacks (PromptRobust)\nFederated Learning Poisoning with Federated LLMs (FedSecurity)\nHOUYI Prompt Injection Attacks\nQuality Assurance via Prompt Injection (QA-Prompt)\nAutomated Red-Teaming Framework for LMs (RedLM)\nLatent Jailbreak Attacks\nPrompt Extraction Attacks\nProPILE Framework for Privacy Leakage\nInstruction Exploitation via AutoPoison\nAdversarial Alignment Challenges\nVisual Adversarial Examples in Multimodal Models\nExploiting Machine Unlearning for Backdoor Attacks (BAU)\nLow-Resource Language Jailbreaking (LLJ)\nStealthy Jailbreak Prompt Generation (AutoDAN)\nTrust in Multimodal Negotiation Agents\nGPTFUZZER Framework\nGradient-Based Obstinate Adversarial Attacks\nMASTERKEY Automated Jailbreaking\nSemantic Firewall Bypass (Self-Deception Attacks)\nSystematic Jailbreak Prompts\nBackdoor Variants in Communication Networks\nAdvanced Data Deduplication Side-Channels\nRAIN Mechanism for Safe Rewindable LLMs\nLLMSmith Framework Exploitation\nDP-Forward Robust Training\nInstruction-Tuning Dataset Errors (DONKII)\nDynamic Role Hijacking Attacks\nMulti-Language Jailbreaking\nFederated Training Vulnerabilities\nPreference-Guided Black-Box Optimization\nComparative-Confidence Leakage\nText-Only Jailbreak Optimization\nAdversarial Suffix Hill-Climbing\nVision-LLM \n Perturbation Attack\nHybrid Transfer+Query Attack\nPreference-Oriented Ensemble Attacks\nWeak Adversarial Defenses\nInsufficient Monitoring\nData Poisoning\nUnencrypted Model Parameters\nPrivacy Violations\nExposed Training Data\nBiased Data Sources\nSemantic Exploitation\nResource Exhaustion\nTrigger Sensitivity\nInput Validation Gaps\nModel Overfitting\nSynthetic Data Vulnerabilities\nLack of Auditability\nFederated Model Poisoning\nSide-Channel Exploits\nBackdoor Exploits\nGradient Leakage\nAdversarial Transferability\nSupply Chain Attacks\nModel Inconsistency Errors\nComparative Confidence Leakage\nText-Only API Surface Exposure\nSafety Activation Gap for Comparison Queries\nModel Calibration Vulnerability\nIterative Query Exploitation\nPreference-Oriented Optimization Leakage\nLifecycle Stages\nTTPs\nVulnerabilities\nCritical Vulnerabilities\nStrong Connection\nWeak Connection\nFig. 9: Relationships among ML lifecycle stages, tactics/techniques/procedures (TTPs), and vulnerabilities. This\nbipartite–tripartite network maps nine ML lifecycle stages (left, blue triangles) to a set of reported TTPs (center,\nred rectangles), which are in turn connected to vulnerabilities (right, green circles) observed in the literature. Edge\nthickness denotes connection strength, where strong links indicate frequent co-occurrence across multiple sources\nand weak links indicate infrequent or context-specific associations. Lifecycle stages span from Data Preparation\nand Pretraining through Fine-tuning, Testing, and Deployment, while vulnerabilities include issues such as privacy\nviolations, data poisoning, gradient leakage, resource exhaustion, and model inconsistency errors. This visualization\nsynthesizes findings from both foundational and recent studies [125], [40], [29], [24], [62], [7], [33], [11], [126], [127],\n[128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [41], [23], [143], [144], [145],\n[146], [147], [65], [73], [148], [149], [74], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162],\n[163], [164], [165], [166], [167], [168], [96], [169], [170], [99], highlighting where security risks are concentrated and how\nthey propagate across the ML pipeline.\n"}, {"page": 33, "text": "33\n(a) The scattered plot with a regression line\n(b) The Pareto chart with cumulative line\nFig. 10: The scattered plot with regression line reveals a modest positive correlation between CVE count and issues,\nwith a linear fit of slope (m) ≈+0.3 and intercept (b=3.01), implying additional CVEs correspond to about 0.3 more\nissues on average. Meanwhile, the Pareto plot shows a cumulative distribution (blue line) spanning 91 CPEs: ≈20%\nof them (about 18 CPEs) account for 673 CVEs—over 80% of the total 834.\nTABLE XVIII: Predicted Risk Scores (P) is the Likelihood of Exploitation for CVEs with Corresponding Descriptions,\nCVSS Scores, Exploitability Scores, and Patch Statuses. P: (0.8,1] High, [0.40,0.8] Medium, [0,0.4) Low\nCVE ID\nDescription\nCVSS\nExpl.\nPatch\nP\nCVE-2025-0015\nUse After Free vulnerability in Arm Ltd Valhall GPU Kernel Driver\n0.000\n0.000\n0\n0.094\nCVE-2025-0222\nVulnerability in IObit Protected Folder up to 13.6.0.5\n0.556\n0.462\n1\n0.664\nCVE-2025-0223\nVulnerability in IObit Protected Folder up to 13.6.0.5\n0.556\n0.462\n1\n0.708\nCVE-2025-0224\nVulnerability in Provision-ISR SH-4050A-2, SH-4100A-2L\n0.000\n0.000\n0\n0.126\n…\n…\n…\n…\n…\n…\nCVE-2025-0226\nVulnerability in Tsinghua Unigroup Electronic Archives System\n0.000\n0.000\n0\n0.095\nCVE-2025-0228\nVulnerability in code-projects Local Storage Todo App 1.0\n0.485\n0.436\n1\n0.860\nCVE-2025-0229\nVulnerability in code-projects Travel Management System 1.0\n0.990\n1.000\n1\n0.818\nCVE-2025-0215\nVulnerability in UpdraftPlus WP Backup & Migration Plugin\n0.616\n0.718\n1\n0.117\nTable XVIII shows the predicted scores of the GNN model classifying vulnerabilities in three categories of predicted\nrisk (P).\n\n\n\n\n\n(0.8, 1]\nCritical Response :\n[0.4, 0.8]\nMedium Priority\n[0, 0.4)\nLow Priority\nCritical Response: requires immediate action in patching, continuous monitoring, and urgent mitigation. Medium\nPriority: action needed to review the vulnerability, monitor trends, and schedule timely patches. Low Priority requires\nroutine patching without urgency and passive monitoring.\nFig. 11 represents a heterogeneous network where CVE nodes (orange) are connected to affected products (skyblue)\nand external references (light green). The directed edges illustrate real-world relationships such as vulnerabilities\naffecting specific products and being referenced by security advisories or exploit reports. The visualization captures\nthe structural dependencies within the cybersecurity ecosystem, providing contextual insights for the GNN to predict\nthe risk score of each CVE based on both its intrinsic features and its connections to related entities. The expected\nrisk score is influenced by the following CVSS Metrics: Historical severity scores (e.g., base score, exploitability); the\nCVE Descriptions: Keywords that indicate exploitability (e.g., “remote code execution”); Affected Products: High-\nprofile products may indicate higher risk; References: URLs pointing to known exploits or advisories can signal active\nexploitation.\n"}, {"page": 34, "text": "34\n(a) The radar chart\n(b) The heatmap chart\nFig. 12: The radar chart shows how each model scores across Prompt Injection, Backdoor, and Training-phase\nvulnerabilities. It visualizes the shape of their risk profiles. Meanwhile, the heatmap compares absolute vulnerability\nvalues and composite scores (CVS) for all models, highlighting the most susceptible (darker red implies higher risk).\nA. Comparative Vulnerability Analysis of SoTA LLMs\n1) Composite Vulnerability Scores Across SoTA Models\nOur quantitative evaluation of SoTA LLMs reveals distinct differences in vulnerability profiles. Using the Composite\nVulnerability Score (CVS) metric, which integrates prompt injection ASR, backdoor ASR, and training-stage risk, we\nranked five leading models from the literature [86], [102], [103], [85], [104], [84], [105], [107], [108], [109], [110], [111],\n[171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [98], [181], [182], [183], [184], [137], [185], [186], [187],\n[188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [125]:\nGPT-4o emerged as the most vulnerable, with a CVS of 0.95, driven by extremely high success rates in both prompt\ninjection (0.95) and code-level backdoor attacks (0.985). Claude-3.5and Gemini-1.5 followed closely, reflecting similar\nvulnerability profiles across all categories. LLaMA‑7B, although exhibiting moderate exposure to prompt injection,\nhad the highest training-stage risk score (1.0), indicating susceptibility to sleeper agents and backdoor triggers.\nDeepSeek‑R1 showed high vulnerability to prompt-based attacks but lacked suﬀicient backdoor and training-stage\ndata, resulting in a lower composite score.\n2) Insights from Radar Chart and Heatmap reported in Fig. 12.\nThe radar chart (see Fig. 12a) highlights GPT‑4o’s uniformly high risk across all categories, whereas LLaMA‑7B\nshows pronounced spikes in training-stage vulnerabilities. The heatmap (see Fig. 12b) reinforces these findings, making\nit visually evident how risk is distributed unevenly across models and vulnerability types. These results underscore the\nimportance of model-specific threat modeling. While some models may resist specific attack vectors, their exposure\nto others—especially those affecting training pipelines or memory-based exploits—necessitates the development of\ncustomized defense frameworks.\nB.\nProminence and common entry points of threat TTPs exploited in ML attack scenarios (RQ1)\nThis RQ addresses two constructs: the prominence of threat TTPs exploited in attack scenarios and the common entry points.\nThe prominence of threat TTPs in attack scenarios\nTable XI shows the mapping between tactics and attack scenarios. The most prominent tactic is ML Attack Staging,\noccurring 30 times across the 93 ML attack scenarios. During ML attack staging, threat actors prepare their attack\nby crafting adversarial data to feed the target model, training proxy models, poisoning, or evading the target model.\nThe other significant tactics used in attack scenarios are Impact and Resource Development, occurring 21 and 15\ntimes in ML attack scenarios, respectively (see Table XI). After the ML attack successes, most attack scenarios tried\nto evade the ML model, disrupt ML service, or destroy ML systems, data, and cause harm to humans.\nIn Table XI, the execution flows of attack scenarios share some TTP stages. The most used TTP sequences in\nattack scenarios are:\n• ML Attack Staging (stage 0) →Defense Evasion (stage 1) →Impact (stage 2)\n• ML Attack Staging (stage 0) →Exfiltration (stage 1)\n• ML Attack Staging (stage 0) →Impact (stage 1)\n• Reconnaissance (stage 0) →Resource Development (stage 1) →ML Model Access (stage 2) →ML Attack Staging\n(stage 3) →Impact (stage 4)\n"}, {"page": 35, "text": "35\n• Reconnaissance (stage 0) →Resource Development (stage 1) →ML Attack Staging (stage 2) →Defense Evasion\n(stage 3)\nAll these attack scenarios (Carlini et al. [63], Abdullah et al. [33], Papernot et al. [67], [66], Biggio et al. [34],\nAthalye et al. [64], Barreno et al. [48]) have similar execution sequences i.e., starting from stage stage 0 to stage stage\n2. Attack scenarios (Carlini et al. [7], Wallace et al. [65], Choquette-Choo et al. [37]) share stages from stage 0 to stage\n1. In addition, attack scenarios Attack on Machine Translation Service and Microsoft Edge AI Evasion have similar\nexecution sequences, i.e., starting from stage S0 to stage S4. It is also the same for attack scenarios Evasion of Deep\nLearning Detector for Malware C2 Traﬀic and Botnet Domain Generation (DGA) Detection Evasion that share stages\nfrom stage 0 to stage 3. Attack scenarios Jagielski et al. [20] and Poachers Evade KZN’s Park High-Tech Security\nhave some stages already included in the selected sequences, i.e., Defense Evasion (stage 0) and Impact (stage 1), ML\nAttack Staging (stage 1) and Exfiltration (stage 2). Attack scenarios Backdoor Attack on Deep Learning Models in\nMobile Apps and Confusing AntiMalware Neural Networks only share two stages (i.e., stage 0 and stage 1) already\nincluded in the selected sequences; thus, they are ignored.\nTable XI also shows that the most attack scenarios targeted ML systems without prior knowledge or access to the\ntraining data and the ML model (black box); this is explained by the highest number of occurrences of Black-box in\nthe Attack Knowledge column (i.e., 17 times). In addition, most attack scenarios are untargeted, shown by the highest\nnumber of occurrences of Traditional Untargeted and Adversarially Untargeted in the Attack Specificity column (i.e.,\n20 times). They also mainly targeted Confidentiality and Integrity.\nThe common entry points in attack scenarios\nTable XI shows that the common entry points of attack scenarios are Reconnaissance and ML Attack Staging.\nPrecisely, attackers exploited public resources such as research materials (e.g., research papers, pre-print repositories),\nML artifacts like existing pre-trained models and tools (e.g., GPT-2), and adversarial ML attack implementations\n(Reconnaissance). To start the attack, they can use a pre-trained proxy model or craft adversarial data offline to be\nsent to the ML model for attack (ML Attack Staging).\nSummary 1\nML attacks mainly exploit data poisoning, backdoor injections, membership inference, and supply chain risks, with\nGradient-Based Obstinate Adversarial Attacks, MASTERKEY Automated Jailbreaking, and Federated Learning\nPoisoning being the most prevalent TTPs. Attacks commonly originate from third-party dependencies, model\nAPIs, training data pipelines, and pretraining artifacts, highlighting ML supply chain vulnerabilities. The most\nfrequent attack stages involve Reconnaissance and ML Attack Staging, with ML Attack Staging and Impact being\nthe dominant TTP sequences. While shorter TTP paths focus on exfiltration and impact, the longest observed\nsequence follows Reconnaissance →Resource Development →ML Model Access →ML Attack Staging →Impact.\nOverall, Confidentiality and Integrity remain the primary attack objectives in ML threat scenarios.\nC. Impact of threat (TTPs) against ML phases and models (RQ2)\nIn this research question, we delve into how the TTPs impact the ML overflow (phases) and models. For that, we\naim to identify the most targeted/vulnerable ML phases and models based on adopted threat TTPs. Therefore, we\npresent our results into two parts: (i) the impact of threat TTPs against ML phases and (ii) the impact of threat\nTTPs against ML models.\nTABLE XIX: Mapping between Tactics adopted on attacks and ML phases\nhhhhhhhhhhh\nTactics [28]\nML Phases\nData Collection\nPreprocessing\nFeature Engineering\nTraining\nTesting\nInference\nMonitoring\nReconnaissance\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nResource Development\n✓\n✓\n✓\n✓\n✓\n✓\nInitial Access\n✓\n✓\n✓\n✓\nML Model Access\n✓\n✓\n✓\nExecution\n✓\n✓\nPersistence\n✓\n✓\n✓\nDefense Evasion\n✓\n✓\n✓\n✓\n✓\n✓\nDiscovery\n✓\n✓\n✓\n✓\nCollection\n✓\nML Attack Staging\n✓\n✓\n✓\n✓\n✓\n✓\nExfiltration\n✓\n✓\n✓\nImpact\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nCredential Access\n✓\n"}, {"page": 36, "text": "36\nImpact of threat TTPs against ML phases:\nIn the state-of-the-art ML security research, our analysis of TTPs (Tactics, Techniques, and Procedures), vulner-\nabilities, and ML lifecycle stages reveals critical weaknesses that adversaries exploit to compromise model integrity.\nThe results of this analysis are shown in Fig. 9; we observed 21 vulnerabilities, 55 TTPs, and nine lifecycle stages.\nAmong the most pressing vulnerabilities are Data Poisoning, Backdoor Exploits, Federated Model Poisoning, and\nGradient Leakage, all of which introduce systemic risks that propagate through the ML pipeline. Training-time attacks,\nsuch as Federated Learning Poisoning and Gradient-Based Obstinate Adversarial Attacks, pose substantial threats by\nmanipulating model parameters at their inception, leading to compromised inference outcomes. In adversarial settings,\nMASTERKEY Automated Jailbreaking and Semantic Firewall Bypass Attacks highlight how prompt-based adversarial\nmanipulations circumvent existing alignment techniques, rendering large-scale AI models susceptible to unauthorized\ncontrol and exploitation. The widespread adversarial transferability of attacks further exacerbates these risks, enabling\ncrafted adversarial perturbations to generalize across models, underscoring the inadequacy of conventional defenses. To\nmitigate these threats, robust countermeasures must be deployed, including differentially private training, cryptographic\nmodel integrity verification, and adversarially robust learning architectures. Despite its promise of decentralized\nprivacy-preserving computation, Federated learning remains an attack vector requiring secure aggregation techniques\nto thwart malicious updates. Furthermore, real-time adversarial detection pipelines, adversarial training with diverse\nattack distributions, and secure model fine-tuning frameworks are imperative to enhancing resilience against TTP-\ndriven model compromise. As ML adoption scales, research into proactive, adaptive security paradigms must advance to\nsafeguard models against evolving attack methodologies, ensuring the robustness of AI systems deployed in high-stakes\ndomains.\nIn Table XIX, we present the most targeted ML Phases (columns) against the different adopted Tactics\n(rows) from practice. First, we can observe that, based on the analyzed attacks, not all ML phases are impacted by\nthe TTPs, as they cover specific ML phases. Such a finding does not indicate that other ML phases are not impacted;\nrather, it reflects the contextual nature of the observed attacks. In different contexts or under varying threat models,\nother ML phases might also be vulnerable to similar or novel TTPs. This highlights the need for a holistic approach\nwhen analyzing and mitigating threats across the entire ML lifecycle, as adversaries may adapt their strategies to\nexploit weaknesses in less commonly targeted phases.\nSecond, regarding the impacted phases, we observe that Testing, Inference and Training represent the most impacted\nML phases. This finding underscores the need for practitioners and researchers to prioritize these phases when analyzing\npotential vulnerabilities. It is essential not only to investigate and understand the likelihood and nature of vulnerabilities\nthat occur during these phases, but also to develop and implement effective mitigation strategies. By focusing on these\nhigh-risk phases, researchers and practitioners can work toward building more robust and resilient ML pipelines.\nThird, regarding the tactics, we observe varying levels of coverage across the ML phases. For example, Reconnaissance\nand Impact are present in all reported ML phases, demonstrating their broad applicability and relevance across the\nentire ML lifecycle. On the other hand, Credential Access is associated exclusively with the Data Collection phase.\nSuch a disparity can be attributed to several factors, such as the specific nature of the tactic and its primary focus.\nFor example, Reconnaissance involves gathering information, which can be relevant at any phase, whereas Credential\nAccess is more likely to target sensitive access points, such as those involved in the data acquisition process.\nTABLE XX: Target Models, Occurrences (Occ.), Normalized Metrics ( Nm. (%) =\n\u0000 Occ. of a Model\nTotal Occ.\n\u0001\n× 100), and Time\nInterval of attacks (Period) collected from MITRE, AI Incident, and Literature. The information about the Occ is\nextracted from Table XII. “N/A” stands for the cases without information regarding the model.\nTargeted Model\nOcc.\nNm (%)\nPeriod\nTransformers (BERT, GPT-2, GPT-3, others)\n16\n25.40\n2019-2024\nConvolutional Neural Networks (CopyCat, Fisheye, ResNet, others)\n12\n19.05\n2018-2021\nDeep Neural Networks (unspecified)\n9\n14.29\n2013-2021\nHidden Markov\n1\n1.59\n2021\nLong-Short Term Memory\n2\n3.17\n2020-2021\nGenerative Adversarial Networks\n2\n3.17\n2014-2020\nDeepVoice [100]\n1\n1.59\n2019\nFeed-Forward Neural Networks\n1\n1.59\n2017\nParseval Networks\n1\n1.59\n2017\nLinear classifiers (SVM, Logistic Reg., Naive Bayes)\n3\n4.76\n2010-2016\nNon-Linear classifiers (Decision Trees, k-Nearest Neighbor)\n2\n3.17\n2016\nN/A\n10\n15.87\n2018-2024\n"}, {"page": 37, "text": "37\nImpact of threat TTPs across ML models:\nThe analysis of cybersecurity vulnerabilities across software dependencies (linking GitHub issues to vulnerabili-\nty/threats –CVE) reveals a high prevalence of critical and high-severity CVEs, highlighting systemic risks that persist\nacross multiple dependencies. The network graphs ( available in our replication package and derived from Tables VII,\nVIII and Fig. 7 ) underscore the existence of high-impact vulnerabilities, which, when left unpatched, serve as prime\nattack vectors for sophisticated adversarial techniques such as Gradient-Based Obstinate Adversarial Attacks and\nMASTERKEY Automated Jailbreaking. These tactics manipulate security mechanisms through subtle perturbations\nor bypass restrictions to exploit system weaknesses. The network visualization with community detection further\nemphasizes that security vulnerabilities are not isolated threats but form interconnected clusters , suggesting that\nadversaries can exploit multiple dependencies through cascading failures. We observe that dependency management\npresents a wicked problem, where some communities have a reasonable number of issues to address vulnerabilities ,\nwhile others suffer from an overwhelming influx of critical cases with little to no issue tracking or mitigation mechanisms\nin place . This disparity underscores the urgent need for proactive security interventions, particularly in high-risk\ncommunities where unaddressed critical vulnerabilities can propagate across dependencies, amplifying systemic risks.\nIn this context, influential nodes, representing highly connected dependencies, play a critical role in dependency\nmanagement. These nodes act as risk amplifiers—a single compromise in a central node can propagate across multiple\nsystems, creating widespread security breaches. Consequently, prioritizing security updates for these high-degree nodes\nand enforcing automated security patching mechanisms becomes imperative [205]. The network structure also reveals\nlatent inter-dependencies, where seemingly unrelated software components share common vulnerabilities, necessitating\na holistic approach to risk mitigation. Organizations should leverage graph-based threat modeling to proactively identify\nhigh-risk dependencies and deploy adaptive security mechanisms, such as real-time monitoring and federated threat\nintelligence, to minimize exploitability. This interconnected vulnerability landscape reinforces the urgent need for\nsystemic resilience strategies, ensuring that software dependencies remain robust against adversarial exploitation and\nresilient to emerging threats. Fig. 10b ranks CPEs by their total CVE count (highest on the left) with bars stacked\nby severity (e.g., red = critical, orange = high), while the blue line on a secondary y‐axis shows the cumulative\npercentage of CVEs. The leftmost bars often represent a small subset of CPEs accounting for most vulnerabilities\n(the “vital few”), indicating a Pareto effect if the line exceeds 80–90% after only a few bars; bars heavily colored\nin red/orange reveal especially severe CVEs. Once the cumulative line surpasses around 95%, additional bars yield\nless impact, aiding prioritization for patching or deeper triage. Meanwhile, Fig. 10a plots CVE count (x‐axis) versus\nIssue count (y‐axis) for each CPE, overlaid with a regression line: a higher slope means an extra CVE generally leads\nto more Issues, a near‐zero slope shows minimal correlation, and the intercept represents baseline Issues if a CPE\ntheoretically had zero CVEs. Closer clustering around the line implies a stronger relationship, whereas bubbles above\nit may signify more Issues than expected, and those below could be under‐tracked.\nTable XX summarizes the models targeted in this study, including the time period of each attack and the number of\noccurrences based on the extractions in Section III-G4. The “unspecified” category (row-4) indicates that Deep Neural\nNetworks (DNNs) were used without disclosing their architectures, while the final row (row-13) lists “N/A,” meaning\nno information on the model was found. Most attack scenarios involve Transformers or Convolutional Neural Networks\n(CNNs). CNNs appear in 12 cases, while Transformers lead with 14, though the distribution over time differs between\nthe two. CNNs show a steady pattern of exploitation across the analyzed period, aligning with the popularity reported\nby Kaggle between 2019 and 2021[206], where Gradient Boosting Machines (e.g., xgboost, lightgbm) and CNNs were\nmost frequently used. Since tree‐based models like Gradient Boosting Machines are discrete and non‐differentiable, they\nare not well-suited for gradient‐based white‐box attacks[207], so such models are absent from these attack scenarios.\nIn contrast, Transformers exhibit an irregular distribution with a notable surge in 2023, particularly targeting GPT\nmodels. This jump may reflect their widespread adoption and the accompanying rise in adversarial interest. Several\nattack scenarios do not specify which model was attacked. While still valid, these cases lack clarity and hinder deeper\nanalysis, highlighting the need for more transparent reporting to better identify vulnerabilities in different model\narchitectures. Finally, across the entire study period (2013–2023), early years show relatively few attacks, often on\nmodels that never achieved the popularity of Transformers, CNNs, or DNNs.\nCNNs are frequently targeted. While this observation can be partially attributed to the widespread adoption of\nCNNs in various domains (e.g., computer vision, autonomous systems), our analysis extends beyond mere popularity\nmetrics. CNNs exhibit inherent architectural vulnerabilities—such as sensitivity to adversarial perturbations due to\ntheir linear decision boundaries—that make them more susceptible to specific attack vectors like adversarial evasion\nand gradient-based attacks. To differentiate between vulnerabilities arising from high deployment frequency and those\ndue to structural weaknesses, we incorporated normalized metrics (i.e., percentage-based distributions) alongside\nabsolute counts. This dual approach enables a more nuanced understanding of why specific models, such as CNNs,\nare disproportionately targeted, offering insights into both their prevalence and inherent security risks.\n"}, {"page": 38, "text": "38\nSummary 2\nThreat TTPs impact ML phases with varying severity, with pretraining and inference being the most vulnerable\ndue to exposed model artifacts, insuﬀicient robustness, and susceptibility to adversarial examples. Training-time\nattacks, such as data poisoning and backdoor injections, threaten model integrity, while inference-time threats,\nincluding evasion and membership inference attacks, compromise confidentiality and reliability. Federated learning\nenvironments face heightened risks from poisoning and leakage attacks due to the distributed trust assumptions\nand gradient-sharing mechanisms employed. Attack strategies primarily leverage Reconnaissance, Impact, ML\nAttack Staging, and Resource Development, with Testing, Inference, Training, and Data Collection being the most\ntargeted ML phases. Transformers and CNNs are the most frequently attacked model architectures, with a notable\nrise in GPT-based attacks in recent years.\nD. Characterizing new Threats not reported in the ATLAS database (RQ3)\nTo answer this RQ, we split the results into three parts: (i) the new threats found in the AI Incident database and\nthe literature, (ii) the threats mined from the GitHub ML repositories, further discussing the most vulnerable ML\nrepositories as the dependencies that cause them, and (iii) the most frequent vulnerabilities in the ML repositories.\nNew Threats from the AI Incident Database and the Literature\nIn Table XXI, we present the new threats collected and the associated tactics and techniques. Regarding the threats\nin the AI Incident database, we identify new TTPs covering eight (8) tactics and 15 techniques across 12 ML attacks.\nMoving forward, regarding the threats from the Literature (considering only the 14 seed papers here), we have 14\nML attacks, covering six tactics and nine techniques. Overall, we can observe that most LLM attacks share the same\nTTPs as No-LLM ones, highlighting the replicability of a given attack exploring different contexts. Except for the\ntactic Persistence, all the other tactics are commonly shared among all attacks, indicating that despite the different\nattacks in various contexts, they share similar characteristics.\nThese 26 new ML attack scenarios were not documented in ATLAS and could be used to extend the ATLAS case\nstudies. While some of these latest attacks share the same characteristics with the ones already included in ATLAS,\nother attacks, like LLM-based, provide new insights about ML attacks by exploring related attributes in the same\nand different ML models.\nFig. 13: Frequency distribution of vulnerability types in GitHub ML repositories. Bar chart showing the number of\noccurrences for the most common vulnerability categories identified across ML–related repositories on GitHub.\n"}, {"page": 39, "text": "39\nTABLE XXI: Threats collected from AI Incident and Literature. Associated tactics to an attack are presented as\ncolumns, while techniques are reported as values in the cells. AI-DB stands for cases extracted from the AI Incident\ndatabase, while LIT stands for Literature.\nAI Incident\nDatabase\nAttacks\nTactics\nResource\nDevelopment\nInitial\nAccess\nML Attack\nStaging\nExfiltration\nReconnaissance\nImpact\nDefense\nEvasion\nML Model\nAccess\nPersistence\nAI-DB-1\nEstablish\nAccounts\nValid\nAccounts\nUse Pre-Trained\nModel\nCyber\nMeans\nAI-DB-2\nEstablish\nAccounts\nValid\nAccounts\nCyber\nMeans\nAI-DB-3\nActive\nScanning\nCost\nHarvesting\nAI-DB-4\nUse Pre-Trained\nModel\nCost\nHarvesting\nAI-DB-5\nEvade ML\nModel\nEvade ML\nModel\nAI-DB-6\nCraft\nAdversarial Data\nEvade ML\nModel,\nCost\nHarvesting\nAI-DB-7\nCraft\nAdversarial Data\nEvade ML\nModel,\nCost\nHarvesting\nAI-DB-8\nEvade ML\nModel,\nDenial of\nService\nAI-DB-9\nLLM Prompt\nInjection: Direct\nCraft\nAdversarial Data\nPublicly Available\nVulnerability\nExternal Harms\nAI Model\nInference\nAPI Access\nAI-DB-10\nLLM Prompt\nInjection: Direct\nCraft\nAdversarial Data\nPublicly Available\nVulnerability\nAI Model\nInference\nAPI Access\nAI-DB-11\nLLM Prompt\nInjection: Direct\nCraft\nAdversarial Data\nPublicly Available\nVulnerability\nAI Model\nInference\nAPI Access\nAI-DB-12\nLLM Prompt\nInjection: Direct\nCraft\nAdversarial Data\nPublicly Available\nVulnerability\nFull ML\nModel Access\nLIT-01\nUse Pre-Trained\nModel\nML Inference\nAPI: Extract\nML Model\nLIT-02\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-03\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-04\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-05\nUse Pre-Trained\nModel\nML Inference\nAPI: Extract\nML Model\nLIT-06\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-07\nCraft\nAdversarial Data\nBackdoor ML\nModel:\nInject Payload\nLIT-08\nCraft\nAdversarial Data\nML Inference\nAPI: Infer Training\nData Membership\nLIT-09\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-10\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-11\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-12\nCraft\nAdversarial Data\nLIT-13\nCraft\nAdversarial Data\nEvade ML\nModel\nEvade ML\nModel\nLIT-14\nCraft\nAdversarial Data\nML Inference\nAPI: Extract\nML Model\nVictim’s Publicly\nAvailable\nResearch Materials\nPotential Threats from Vulnerabilities in GitHub ML repositories\nOverall, we identify 35 vulnerability types from the mined ML repository issues. When evaluating these vulnerabili-\nties, we observe that most are classified under two main types: (i) software- and (ii) network-level. While software-level\nvulnerabilities are related to weaknesses faced by the target software (e.g., applications (models), operating systems,\nor libraries) that adversaries can exploit, network-level vulnerabilities explore the infrastructure the target system\noperates on, exploiting flaws in communication protocols, access to services and resources, etc. Fig. 13 shows our\nstudy’s top 10 vulnerability types and their occurrences.\nDenial of Service (DoS) is the most recurrent vulnerability, with a frequency of 951 occurrences; such a vulnerability\naims to make the target system unavailable. We may highlight that DoS occurs in both types of vulnerabilities\nevaluated here. While software-level DoS can impact the system’s availability due to a memory or crash error (e.g.,\nsegmentation fault) that disrupts the underlying OS and machine, network-level DoS may disrupt the regular traﬀic\nof a network resource.\n"}, {"page": 40, "text": "40\nTABLE XXII: Dependencies responsible for Vulnerabilities in ML repositories\nDependency\nOccurrences\nSeverity\nAffected\nRepositories\ngoogle:tensorflow\n184\ncritical, high, medium\n7\nlinux:linux_kernel\n45\nhigh, medium, low\n4\nvim:vim\n38\ncritical, high, medium\n1\nopenssl:openssl\n30\ncritical, high, medium, low\n10\nimagemagick:imagemagick\n27\ncritical, high, medium\n1\npython:pillow\n24\ncritical, high, medium\n11\nhaxx:curl\n22\ncritical, high, medium\n7\npaddlepaddle:paddlepaddle\n17\ncritical, high\n1\ngnu:glibc\n16\ncritical, high, medium, low\n3\nsqlite:sqlite\n15\ncritical, high, medium\n4\nMost vulnerable GitHub ML repositories and Their Target Dependencies\nIn our analysis, 86 repositories reported at least one vulnerability. Checking these repositories, we observe that\n75% of them represent projects used to build ML systems, like libraries, toolkits, frameworks, and MLOps. The other\nrepositories use these previous projects as dependencies to provide their services, like practices, tutorials, and tools\nto users. Figure 14 presents the top 10 repositories with more occurrences of the vulnerabilities under analysis.\nFig. 14: Vulnerability occurrences across top GitHub ML repositories. The bar-plot highlights an uneven distribution\nof vulnerabilities across ML projects, with a small subset accounting for the majority of identified flaws.\nPython Code Tutorials15 is the repository with the highest frequency of reported vulnerabilities. This repository\ncontains a diverse set of tutorials on Python, covering different domains, including Machine Learning, which may\nencourage users to replicate the vulnerable code and face the reported flaws/weaknesses. Next, we have a consistent\nnumber of repositories that usually provide services for other repositories, like libraries and frameworks. For example,\nAimet is a library that supports advanced quantization and compression techniques for trained neural network models,\nwhile TensorFlow is one of the most used frameworks for building ML applications.\nMoving forward, once we collect the vulnerabilities and their frequency in ML repositories, we aim to investigate\nthe dependencies that cause these vulnerabilities. Overall, we observe that 227 dependencies are responsible for\nthe vulnerabilities studied here. Table XXII presents the top 10 most recurrent dependencies regarding the number\nof occurrences. TensorFlow is the most frequent dependency, with 184 occurrences across seven different sample\nrepositories. Given the popularity of Tensorflow, such a dependency is constantly used by different repositories,\n15https://github.com/x4nth055/pythoncode-tutorials\n"}, {"page": 41, "text": "41\nconsequently increasing the chances of these repositories facing vulnerabilities. On the other hand, TensorFlow is\nconstantly updated, addressing reported issues and providing up-to-date services for its users. Among the dependencies\nthat most affect repositories, Pillow, a library for image processing, stands out by affecting eleven repositories, while\nmost of them are other libraries and tools (systems).\nRegarding the severity of the vulnerabilities, Figure 15 presents the distribution for the top 10 repositories. Although\nwe observe the vulnerabilities vary from low to critical severity, it is important to highlight how recurrent high and\nmedium vulnerabilities are reported, posing significant risks to the security and stability of systems. On the other\nhand, although low severity vulnerabilities are less frequent, critical vulnerabilities represent an expressive frequency\nfor some repositories, like Python Code Tutorials, Kuberflow, and Guess. Addressing these critical vulnerabilities\npromptly should be prioritized to mitigate potential exploitation and ensure the secure operation of the systems.\nFig. 15: Distribution of vulnerability severities across top GitHub ML repositories. Vulnerabilities are categorized into\nfour severity levels, with the pythoncode-tutorials repository exhibits the highest number of vulnerabilities overall.\nThis distribution underscores the need for targeted remediation strategies prioritizing high- and critical-severity issues\nin widely used ML repositories.\nMost Frequent Vulnerabilities across ML Repositories\nAfter identifying vulnerabilities and the dependencies that caused them, we aim to know how the observed\nvulnerability types propagated across the studied ML repositories. Fig. 18 shows the distribution of vulnerability\ntypes for the top 10 ML repositories, which have more occurrences of vulnerabilities. Overall, we can observe that\nDenial of Service (DoS) is consistently reported as the primary vulnerability type for all repositories. For the remaining\ntypes, we observe a regular occurrence of Improper Input Validation, Null Pointer Deference, and Heap-based Buffer\nOverflow. However, we also observe a high incidence of SQL injection vulnerabilities on the project PythonCode\nTutorials; due to the focus of this repository, such a vulnerability type is valid and recurrent, as some tutorials explore\nthe adoption of databases and SQL. These findings show that the ML-analyzed repositories might face the same\ntypes of vulnerabilities, indicating that certain categories of vulnerabilities are prevalent regardless of the repository’s\nspecific focus.\n"}, {"page": 42, "text": "42\nFig. 16: Distribution of vulnerability types across top GitHub ML repositories. The visualization highlights the\nprevalence of 13 specific security weaknesses in popular ML repositories, guiding prioritization for remediation. The\npythoncode-tutorials repository shows the highest concentration, dominated by DoS and Improper Input Validation.\nSummary 3\nThe integration of the AI Incident Database, GitHub security issues, and the literature reveals multiple\npreviously undocumented threats absent from ATLAS. Graph-based dependency analysis highlights that ML library\nclusters face disproportionately high-severity vulnerabilities, often lacking adequate issue-tracking or mitigation\nmechanisms. Emerging threats include supply chain compromises, automated jailbreak techniques, and prompt-\nbased adversarial manipulations, emphasizing the need for continuous updates to threat models. New ML attacks,\nparticularly those targeting LLMs, present opportunities to extend the ATLAS database with novel insights. Despite\ntheir different focuses, ML repositories exhibit shared vulnerabilities, with frequent occurrences of Denial of Service\n(DoS), Improper Input Validation, Null Pointer Dereference, and Heap-based Buffer Overflow. Additionally, ML\ndependencies, particularly TensorFlow, are major exposure points, introducing high-severity risks across various\nML applications.\n1) Extension for Preference-Guided and Introspection-Based Attacks.\nTo capture newly emerging threats such as preference-guided jailbreaks and introspection-based optimization\nattacks [99], we augment the mitigation matrix (Fig. 17) with the following targeted controls:\n• Reduce optimization signal. Train safety policies to refuse comparative or preference-eliciting queries that can be\nexploited for gradient-free optimization. Avoid deterministic binary phrasing in refusals, as consistent responses form\na usable signal.\n• Rate-limit and jitter. Detect iterative, stateful query patterns (e.g., near-duplicate prompts differing slightly in text\nor image) and introduce randomized refusals or obfuscations to disrupt attack optimization loops.\n• Guardrails around introspection. Enforce policy-level blocking of self-assessment or self-ranking requests tied to\n"}, {"page": 43, "text": "43\nFig. 17: ML Threat Mitigation Matrix\ndisallowed objectives, and monitor for escalating acceptance of adversarially reframed instructions.\n• Agent and RAG contexts. Sanitize retrieved or contextual information that elicits unsafe preference reasoning,\nand implement human-in-the-loop or interlock mechanisms when repeated near-duplicate retrievals occur within\nmulti-agent or RAG workflows.\nThese measures strengthen the matrix’s coverage of introspection-based, black-box attacks and ensure that emerging\npreference-oracle threats are addressed alongside traditional adversarial and data-poisoning defenses.\na) Limitations.\nA residual risk persists in text-only interfaces: even without numeric confidences, comparative judgments can leak\na strong optimization signal. Current static prompt defenses remain insuﬀicient against iterative, preference-guided\nattacks.\nFigure 18 presents a detailed breakdown of how Graph Neural Networks (GNNs) and clustering techniques improve\nvulnerability classification and risk assessment. (Top Left) The Correlation Matrix of CVE Features illustrates the\nrelationships between CVSS Score, Exploitability Score, and Predicted Risk Score, highlighting the degree of association\nbetween these key vulnerability indicators. (Top Right) Density Plot of Predicted Risk Scores by ASR Cluster visualizes\nthe distribution of predicted risk scores within each attack success rate (ASR) cluster, showing variations in attack\neffectiveness. (Middle Left) Violin Plot: CVSS Score Distribution per ASR Cluster compares the spread of CVSS\nscores across ASR clusters, demonstrating inconsistencies between attack success rates and traditional severity scores.\n(Middle Right) Attack Success Rate (ASR) Clustering groups vulnerabilities based on their likelihood of successfully\nmisleading ML models, aiding the GNN in prioritizing high-ASR attacks. (Bottom Left) Stealth and Detectability\nClustering categorizes vulnerabilities based on their evasion capability, enabling the GNN to refine predictions for\nharder-to-detect threats. (Bottom Right) Computational Cost and Practicality Clustering differentiates between low-\ncost and resource-intensive attacks, helping the GNN assess real-world adversarial feasibility. These visualizations\ncollectively demonstrate how GNN-driven learning enhances vulnerability classification, improves risk prediction, and\nrefines cybersecurity prioritization beyond traditional CVSS scoring.\nV. Discussions of results\nThe findings of this study reveal critical vulnerabilities and threats that affect machine learning (ML) systems\nthroughout their lifecycle, from data pre-processing to deployment and operational stages. By analyzing a compre-\nhensive set of data from multiple sources, including the MITRE ATT&CK and ATLAS frameworks, the AI Incident\nDatabase, and GitHub repositories, this study highlights the multifaceted nature of security risks in ML systems.\nVulnerabilities were found to span not only traditional software vulnerabilities but also ML-specific attack vectors,\nsuch as adversarial examples, data poisoning, and model extraction. A key insight is the significant role of dependencies\n"}, {"page": 44, "text": "44\nFig. 18: Multi-faceted GNN-based vulnerability analysis. Integrated views showing (i) feature correlations, (ii) density\nand violin plots of predicted risk by ASR cluster, (iii) scatter plots for ASR vs. CVSS, stealth vs. exploitability, and\nCVSS vs. practicality. Together, these views integrate GNN predictions with statistical and unsupervised learning\ninsights to profile vulnerabilities across multiple operational dimensions. It links severity, exploitability, stealth, cost,\nand operational feasibility.\nin amplifying ML vulnerabilities. Libraries such as TensorFlow, PyTorch, and OpenCV were identified as recurrently\ntargeted due to their expansive dependency chains. For instance, vulnerabilities in dependencies like Log4j and Pickle\nwere shown to cascade across the ML ecosystem, affecting downstream components and deployment environments.\nThese findings underscore the interconnected nature of ML systems and highlight the urgent need for holistic approaches\nto vulnerability management that extend beyond individual tools to their dependencies. This research also underscores\nthe limitations of existing threat models, such as ATLAS, in capturing the full spectrum of real-world vulnerabilities\nand threats. While ATLAS provides a valuable framework for cataloging adversarial tactics, the integration of real-\nworld incidents from the AI Incident Database and GitHub repositories revealed numerous threats not documented in\nATLAS. This gap highlights the dynamic nature of ML security threats and the need for continuously updated and\n"}, {"page": 45, "text": "45\nenriched threat models to reflect emerging risks. Another key result is the mapping of threats to specific stages of the\nML lifecycle, providing actionable insights into where systems are most vulnerable. For example, data poisoning and\nadversarial training attacks predominantly target the training phase, while model extraction and API exploitation\noccur more frequently during the deployment phase. By understanding these stage-specific vulnerabilities, stakeholders\ncan implement targeted mitigation strategies. Finally, this study demonstrates the critical importance of integrating\nproactive and reactive measures into the security lifecycle of ML systems. Proactive measures, such as adversarial\ntraining and secure coding practices, can prevent vulnerabilities from being introduced. Reactive measures, such as\nreal-time monitoring, incident response, and automated patching, ensure rapid containment of threats when they do\noccur. The proposed mitigation matrix (Fig. 17) synthesizes these measures into a comprehensive framework, enabling\nstakeholders to address threats holistically and dynamically.\nA. Mitigation of Vulnerabilities and Threats\nTo address vulnerabilities and threats in ML systems, a combination of proactive and reactive security measures\nshould be implemented across all levels of the ML lifecycle. Traditional security mechanisms such as access control,\nencryption, and network defenses should be complemented by ML-specific defenses, including adversarial training,\nrobust model architectures, and secure data handling practices. Federated learning (FL), while enhancing data\nprivacy by enabling decentralized training, introduces critical security risks. Data poisoning attacks (e.g., model and\nbackdoor poisoning) allow malicious clients to manipulate updates, degrading model integrity. Privacy attacks such as\nmembership inference and gradient leakage exploit model updates to reconstruct private training data. Aggregation\nexploits compromise federated averaging by injecting biased updates, while communication attacks (e.g., MITM, DoS)\ndisrupt training. To mitigate these threats, robust aggregation techniques (e.g., Krum, trimmed mean, differential\nprivacy-based aggregation) filter adversarial contributions. Secure update mechanisms like homomorphic encryption\n(HE) and secure multi-party computation (SMPC) prevent data leakage. Adversarially robust learning strengthens\ndefenses via federated adversarial training and Byzantine-resilient optimization. Privacy-preserving techniques (e.g.,\ngradient noise injection, secure aggregation protocols) mitigate inference attacks, while blockchain-based FL enhances\nintegrity and decentralization. Future research should explore automated anomaly detection, post-quantum security,\nand regulatory compliance to ensure robust FL deployment. By integrating these defenses with established frameworks\nsuch as MITRE ATT&CK [51], D3FEND [59], and NIST security guidelines [208], ML systems can be fortified against\nevolving adversarial threats. The proposed ML Threat Mitigation Matrix provides a structured approach to proactive\nrisk management, covering critical vulnerabilities from data collection to deployment and ensuring a comprehensive\nsecurity posture.\n1) Mitigation Strategies Across Levels\nData-Level Mitigation.\n1) Hardening Data Pipelines: Adversarial defenses [145] and tools like ART, CleverHans, and Foolbox help mitigate\nadversarial attacks targeting datasets.\n2) Data Protection:\nTLS encryption secures data in transit, while AES-based encryption safeguards data at rest.\nDynamic analysis enhances protection during use [56].\n3) Access Control:\nIdentity and Access Management (IAM) policies enforce the principle of least privilege [52],\nminimizing unintended access to critical assets.\n4) Adversarial Detection: Techniques such as Introspection [23], Feature Squeezing [209], and SafetyNet [210] provide\nrobust defenses against adversarial inputs.\n5) Sanitization of Data: Compromised data can be sanitized using denoisers [211] to ensure integrity before use.\nSoftware-Level Mitigation.\n1) Vulnerability Scanning:\nTools like GitHub Code Scanning, OSS-Fuzz, and SonarQube [55] detect and mitigate\nvulnerabilities in ML libraries and pipelines.\n2) Secure Configurations:\nProper IAM configurations and the enforcement of runtime restrictions protect against\nprivilege escalation.\n3) Regular Updates:\nPatch management ensures that vulnerabilities in libraries and dependencies are promptly\naddressed.\nDatabase-Level Mitigation.\n1) Access Control: IAM policies limit database access, while regular backups stored off-network protect against data\nloss [212].\n2) Integrity Monitoring: Continuous monitoring ensures early detection of unauthorized changes.\nSystem and Network-Level Mitigation\n1) Endpoint Protection:\nOS hardening, such as enabling Secure Boot and enforcing automatic updates, protects\nsystem endpoints [54].\n"}, {"page": 46, "text": "46\n2) Network Defenses:\nFirewalls [213], intrusion prevention systems (IPS) [214], and encrypted VPN tunnels [215]\nsecure communication between endpoints.\n3) Traﬀic Analysis:\nNetwork access control lists (ACLs) and monitoring systems like Snort and Zeek detect and\nrespond to malicious activity.\nCloud-Level Mitigation\n1) Zero-Trust Principles:\nRole-based permissions, multi-factor authentication, and secure policies enforced through\nCASB and SASE frameworks ensure robust cloud security [216].\n2) Real-Time Monitoring: Cloud-based SIEMs like Splunk Cloud and Azure Sentinel detect threats across hybrid\ninfrastructures.\n3) Hybrid Cloud Configurations: Bastion and transit networks enhance flexibility and security in multi-cloud setups.\nIntegration with ML Lifecycle. The proposed threat mitigation matrix integrates seamlessly into the end-to-end ML\nlifecycle, from data collection to deployment. For example, compromised training data can be isolated, sanitized, and\nreplaced in real-time, while models deployed on cloud infrastructures benefit from automated security orchestration\nusing serverless functions and APIs. This approach parallels the functionality of existing Security Orchestration,\nAutomation, and Response (SOAR) systems but is explicitly tailored for ML environments.\nContinuous Threat Monitoring. The study emphasizes the importance of continuous threat assessment in ML systems.\nDependencies such as curl/libcurl, which have repeatedly impacted TensorFlow, highlight the need for proactive\nmonitoring and periodic vulnerability scans. The proposed framework ensures that ML engineers can identify and\nrespond to emerging threats, even after initial mitigation has been applied.\nB. Vulnerabilities in SoTA LLMs\n1) Landscape Overview and Emergent Threats.\nRecent advancements in SoTA models (LLMs) have unveiled a new class of high-severity vulnerabilities that\ntranscend traditional adversarial threat vectors. Our study reveals that models such as GPT‑4o, Claude‑3.5, Gemini‑1.5,\nLLaMA‑3.2, and DeepSeek‑R1 remain susceptible to priming-based jailbreaks, prompt injection, and tokenization\nattacks that bypass alignment filters and exploit low-level tokenizer mechanics (e.g., TokenBreak) [217], [218]. More-\nover, emerging backdoor threats—including composite multi-trigger attacks—have demonstrated 100% success rates\neven under stringent RLHF and adversarial training regimes, as evidenced in attacks on LLaMA‑7B [219]. These\nvulnerabilities span multiple system levels, from input and prompt manipulation to representation-level exploits\nenabled by subliminal learning and fine-tuning with contaminated synthetic data [220], [221]. Our layered mapping\n(Fig. 9) not only identifies current blind spots in model deployment pipelines but also reinforces the importance of our\nproposed mitigation matrix, which we further illustrate using a real-world scenario involving a composite backdoor\nattack in an LLM-as-a-service setting.\n2) Data-Level Vulnerabilities: Poisoning and Latent Triggers.\nModern LLMs are critically exposed at the data layer. Adversaries can embed poisoned samples or trigger phrases\nduring fine-tuning, often escaping detection while inducing harmful behavior. For instance, modifying just 0.001% of\ntraining data led to biased outputs in medical LLMs, despite passing standard evaluations [219]. These results validate\nFig. 9’s emphasis on data poisoning and latent trigger injection.\n3) Software-Level Vulnerabilities: Tokenizers and Unsafe Libraries\nThe software stack remains a significant source of risk. Tokenization-based exploits such as TokenBreak allow\nattackers to bypass filters by modifying a single character 16. This cyberattack lets hackers crack AI models just by\nchanging a single character. Third-party dependencies, such as pickle and unsafe regex libraries, compound this risk.\nA systematic review showed how LLMs amplify or overlook code-level vulnerabilities due to insecure prompting [222].\nFig. 9 captures these under improper input validation and supply chain compromise.\n4) Storage and System-Level Vulnerabilities: Model Hijacking.\nStorage and system vulnerabilities are increasingly relevant with containerized LLM deployments. Threats such\nas firmware tampering, API spoofing, and model hijacking remain underexplored yet highly impactful. Tools like\nLLM4CVE demonstrate how automated repair is possible—but also illustrate how easily vulnerabilities propagate\nwithout hardening at deployment time [223].\n5) Network-Level Vulnerabilities: API Exploits and Model Extraction.\nUnsecured inference APIs remain vulnerable to a range of remote threats, including model extraction, indirect\nprompt injection, and membership inference. Models like GPT‑4o and Gemini‑1.5 are known to be susceptible to\n16https://www.techradar.com/pro/security/this-cyberattack-lets-hackers-crack-ai-models-just-by-changing-a-single-character\n"}, {"page": 47, "text": "47\njailbreaks disguised as benign academic language [217], [218]. These threats align with the categories of misconfigured\nAPIs, lack of input filtering, and improper authentication, as shown in Fig. 9.\n6) Lifecycle Propagation: Multi-Stage Exploit Chains.\nOur threat model confirms that vulnerabilities cascade across layers. A poisoned dataset can introduce a backdoor\nthat later enables prompt injection during inference. Such multi-stage exploits are shown in Fig. 9, revealing that the\nattack surface is not static—it grows with model capacity, context length, and autonomy.\n7) Strategic Implications: Security Must Co-Evolve with Capability.\nAs SoTA models (LLMs) advance toward greater autonomy, with features like memory, planning, and tool use, their\nvulnerability landscape becomes increasingly complex. Our analysis indicates that higher model capability does not\ninherently confer greater security. Instead, increased complexity—through expanded APIs, extended context windows,\nand dynamic memory—broadens the attack surface and opens pathways for more sophisticated exploits [217], [221],\n[224]. To keep pace with these risks, security must evolve alongside model capability. Accordingly, defense strategies\nshould be systemically layered, targeting every stage illustrated in Fig. 9: from input filtering and dataset validation\nto software hardening, model verification, and runtime monitoring. Without such a shift, SoTA models risk becoming\nsophisticated, yet opaque systems, where persistent and exploitable weaknesses undermine increasing functionality.\nC. Discussion of Scalability\nIt should be noted that our three-step mapping process scales eﬀiciently to new ML pipeline types such as multimodal,\ninstruction-tuned, or RLHF-augmented models. Each stage of the pipeline—(i) retrieval of relevant TTPs, (ii) ontology\nlinking, and (iii) GNN-based reasoning—is fully modular. Adding new component types requires only the definition of\nadditional schema entities and relations in the ontology, while existing mappings remain valid. The retrieval-augmented\nclassifier adapts automatically through zero-shot prompting without the need to retrain earlier stages. Formally, the\ncomputational cost scales linearly with the number of added nodes and edges (O(n+m)), ensuring tractable scaling\nfor large, heterogeneous pipelines. This modularity allows the framework to evolve alongside advances in model\narchitectures, including multimodal encoders, diffusion decoders, and autonomous-agentic systems. In practice, this\nscalability was validated by extending the ontology and mapping pipeline to support both text-based LLMs and\nmultimodal diffusion models without retraining prior components.\nBecause our methodology operates through an AI agency that implements an automatic RAG system, the ontology\nremains live and self-updating. The system continuously mines and classifies new TTPs from the literature and public\nrepositories, allowing real-time integration of emerging attack patterns into the multi-agent mapping framework.\nConsequently, the list of reported TTPs, vulnerabilities, and ML lifecycle stages naturally expands as new cases appear\nin the ecosystem. After ontology normalization, deduplication, and integration of new state-of-the-art multimodal and\npreference-guided attack techniques [99], the unified threat graph now encodes 73 distinct TTPs, 27 vulnerabilities,\nand 10 ML lifecycle stages.\nOverall, this extended discussion highlights that the proposed mapping framework and threat taxonomy remain\nrobust and adaptable to the next generation of multimodal and large-language models, reinforcing the scalability and\ngeneralizability of our findings.\na) What we learned.\nRecent work [99] shows that LLMs’ own comparative judgments can be elicited to drive text-only, query-based\noptimization of jailbreaks, prompt injections, and vision-LLM adversarial examples. This preference-oracle approach\nremoves the need for logits or surrogate models and, paradoxically, becomes more effective on larger, better-calibrated\nmodels. For practitioners, this widens the attack surface of production APIs that only expose text and calls for stateful\ndetection, policy-level refusal of preference elicitation, and anti-optimization jitter in guardrails. We incorporate this\nattack class into our lifecycle-centric mapping and mitigation matrix to ensure coverage of introspection-based, black-\nbox threats.\nD. Implications for Different Stakeholder Groups\n1) Cybersecurity Practitioners.\nProactive Threat Mitigation: Practitioners must prioritize identifying and mitigating vulnerabilities in ML repositories\nand dependencies. Regular penetration testing, vulnerability scans, and secure configuration audits should be integrated\ninto the ML development lifecycle.\nThreat Intelligence Integration: Leveraging insights from databases like ATLAS, the AI Incident Database, and other\nemerging repositories can enhance proactive defense strategies. This includes updating threat detection rules and\n"}, {"page": 48, "text": "48\ntraining models to identify adversarial patterns in real-time.\nIncident Response Readiness: Given the dynamic nature of ML threats, practitioners need robust incident response\nplans tailored to address both traditional and ML-specific attack vectors, such as adversarial examples or model\nextraction attempts.\n2) Academics and Researchers.\nAdvancing Threat Models: Researchers have a critical role in refining and expanding existing threat models like ATLAS.\nBy integrating real-world vulnerabilities from diverse sources, academics can develop comprehensive frameworks that\nreflect the latest threat landscape.\nLifecycle-Specific Defenses: There is a need for research into stage-specific defenses, such as robust training methods\nto counter data poisoning or cryptographic techniques to secure model inference APIs.\nInterdisciplinary Collaboration:\nCollaboration between security, AI, and domain experts is essential to address the\nmultifaceted challenges posed by ML threats. This includes studying the socio-technical impacts of AI vulnerabilities\nin critical domains like healthcare or transportation.\n3) Regulation Agencies.\nRegulatory bodies must establish comprehensive guidelines for the secure development, deployment, and maintenance\nof ML systems. These guidelines should include robust dependency management practices, regular security audits,\nand compliance with established standards like ISO/IEC 27001 and the NIST AI Risk Management Framework (AI\nRMF). Such measures ensure that organizations proactively address vulnerabilities and align with best practices.\nIncident Reporting Frameworks.\nAgencies should mandate the disclosure of AI-related security incidents to build\na centralized database of vulnerabilities and threats. This database would inform future policies, enhance threat\nintelligence sharing, and foster greater transparency in ML security. Mandatory frameworks, akin to the NIS2 Directive,\nare crucial for documenting adversarial attacks, dependency vulnerabilities, and system failures across sectors.\nGlobal Collaboration.\nGiven the international scope of AI and ML systems, regulatory agencies must collaborate\nto standardize security practices and promote cross-border knowledge sharing. Frameworks like the EU AI Act and\nthe OECD AI Principles can serve as foundational models for harmonizing security standards and fostering collective\nresilience.\nRegulatory Directions. Dependency vulnerabilities remain a significant and evolving threat to ML systems. Regulatory\nbodies could introduce new guidelines requiring organizations to implement automated dependency monitoring, patch\nmanagement, and threat detection systems. Inspired by the Digital Operational Resilience Act (DORA), similar\nresilience frameworks should be expanded to encompass AI and ML systems in critical infrastructure sectors. To\naddress cascading risks in AI supply chains, regulations could mandate transparency in dependency usage, including\nthe disclosure of vulnerabilities in third-party libraries. This aligns with software supply chain regulations like the U.S.\nExecutive Order 14028, which emphasizes the importance of software bills of materials (SBOMs) for secure supply\nchains. Such measures would ensure the robustness and resilience of ML systems in an increasingly interconnected\necosystem.\n4) Tool Builders and Developers.\nBuilding Secure ML Tools: Developers of ML tools and frameworks must integrate security features such as automated\nvulnerability detection, secure dependency handling, and built-in adversarial robustness mechanisms.\nDependency Management: Tool builders should implement mechanisms to monitor, update, and secure third-party\nlibraries and dependencies. Providing users with transparency about known vulnerabilities in dependencies can prevent\ncascading failures.\nUser-Friendly Security Enhancements: Tools should include easy-to-use security features, such as pre-built models with\nadversarial training or APIs that detect malicious inputs. Making security accessible to non-expert users is crucial for\nwidespread adoption.\nVI. Threats to validity\nOur empirical study of ML security threats integrates multiple threat intelligence sources, like ATLAS, ATT&CK, the\nAI Incident Database, and GitHub repositories. While our methodology aims to provide a comprehensive assessment,\nseveral threats to validity must be acknowledged. Thus, we adhered to the methodological principles outlined by\nWohlin et al. [225] and Juristo & Moreno [226] to systematically identify, assess, and mitigate potential threats to\nvalidity. Throughout this empirical study, we adhered to the methodological principles outlined by Wohlin et al. [225]\nand Juristo & Moreno [226] to systematically identify, assess, and mitigate potential threats to validity.\nConstruct Validity:\nConstruct validity concerns whether the variables and metrics used in our study accurately\n"}, {"page": 49, "text": "49\nrepresent the underlying theoretical constructs. Our analysis relies on multiple threat databases, including ATLAS,\nATT&CK, and the AI Incident Database, which could introduce biases due to differences in how security incidents\nand adversarial behaviors are categorized. To mitigate this, we triangulated findings across diverse sources to ensure\nalignment with established ML security taxonomies. Additionally, our mapping of TTPs to ML lifecycle stages was\nvalidated by expert reviews to ensure consistency and correctness.\nInternal Validity: Internal validity pertains to the causal relationships inferred from the data. One potential threat\narises from automated extraction and processing of security vulnerabilities from GitHub repositories and the AI Incident\nDatabase, which may contain duplicate or misclassified entries. To address this, we implemented rigorous data cleaning\nand filtering techniques and manually reviewed a subset of cases for validation. Moreover, our identification of emerging\nTTPs relied on historical trends, which may not fully account for evolving attack strategies. We mitigated this risk\nby incorporating recent high-impact incidents and cross-referencing with real-world security reports.\nExternal Validity: External validity concerns the generalizability of our findings beyond the studied datasets. While\nour approach integrates multiple sources, some niche ML threats may remain underrepresented, particularly those\ntargeting proprietary or closed-source models. Additionally, the ML ecosystem evolves rapidly, and newly discovered\nvulnerabilities may not be reflected in our dataset immediately. To enhance external validity, we included a wide range\nof attack scenarios from different ML domains (e.g., NLP, vision, federated learning) and systematically updated our\ndataset with newly reported incidents.\nConclusion Validity:\nConclusion validity relates to the reliability and statistical significance of our findings. Our\nstudy identifies the most prominent TTPs and vulnerabilities based on their frequency and impact, but the absence\nof a standardized threat severity metric across different databases introduces some uncertainty. To mitigate this,\nwe employed statistical techniques such as frequency distributions and cross-dataset correlations to ensure robust\nconclusions. Additionally, our threat modeling and dependency analysis were designed to minimize biases in prioritizing\nsecurity risks.\nReliability: Reliability concerns the reproducibility of our findings. We documented our methodology comprehensively,\nproviding detailed steps for data collection, preprocessing, and analysis. However, some elements of our study, such\nas expert validation of TTP mappings, introduce a degree of subjectivity. To enhance reproducibility, we released our\ndatasets and code where possible, allowing independent verification. Future research can build on our framework by\nextending the dataset and refining classification methodologies to further validate our conclusions.\nVII. Conclusion\nThe increasing sophistication of adversarial tactics targeting ML systems underscores the urgent need for a robust\nand adaptive security framework. In this study, we conducted a comprehensive analysis of ML threat behaviors by\naggregating insights from multiple sources, including ATLAS, AI Incident Database reports, GitHub ML repositories,\nand the PyPA database. Our findings reveal critical security gaps in existing threat models, particularly within\nwidely used ML repositories, underscoring the necessity for continuous monitoring, dependency analysis, and proactive\nmitigation strategies. We identified Transformers as one of the most frequently targeted architectures, with 25.4%\nagainst CNNs (19.05%) in real-world attack scenarios. The testing, inference, and training phases emerged as the\nmost vulnerable ML lifecycle stages.\nBuffer overflow and denial-of-service (DoS) attacks were the most prevalent\nthreats across ML repositories, while dependency analysis exposed security risks in TensorFlow, OpenCV, and Jupyter\nNotebook, particularly in libraries such as pickle, joblib, numpy116, python3.9.1, and log4j. Additionally, our study\ncontributes 32 previously undocumented ML attack scenarios, encompassing 17 new techniques and 13 tactics, providing\na valuable extension to ATLAS case studies for future research. To bridge the gap between theoretical threat models\nand real-world attack mitigation, we introduced an ML Threat Mitigation Matrix that maps real-world threats to\npotential defensive strategies. By incorporating GNN-based analysis and clustering techniques, we demonstrated how\nrisk prediction models can enhance ML vulnerability classification, refine attack severity estimation, and improve\noverall risk assessment.\nA. Future research directions.\nOur future work focuses on advancing AI-driven ML threat assessment by integrating Generative AI for adversarial\nsimulation, enabling autonomous threat modeling and predicting zero-day attacks. We aim to expand real-time threat\nintelligence aggregation by incorporating feeds from multiple sources, including CISA KEV, the AI Incident Database,\nand Dark Web intelligence, to enhance adaptive risk scoring. Additionally, we plan to develop Reinforcement Learning\n(RL)-based self-improving security mechanisms that dynamically optimize ML defenses against evolving threats.\nLastly, we will extend ATLAS-based security frameworks with automated security governance, providing real-time\nattack prediction, defense adaptation, and compliance recommendations to fortify ML systems against adversarial\nthreats.\n"}, {"page": 50, "text": "50\n1) retrospective testing & red-teaming.\nMoreover, we invite the community to pursue empirical evaluation that lies beyond the scope of our present study.\nFirst, a retrospective incident analysis—in which pipeline predictions are compared with the post-mortem labels of\npublicly reported failures (e.g., the 112 cases archived in the AI Incident Database)—would quantify real-world accuracy\nand could be scored with inter-annotator measures such as Cohen’s κ. Second, a large-scale red-team campaign against\na production-grade MLOps stack (e.g., Azure ML deployed on Kubernetes) would expose the pipeline to adversaries\noperating under genuine operational constraints, thereby revealing failure modes that synthetic benchmarks cannot\ncapture. Systematic investigations along these two axes would provide the empirical grounding needed to translate\nlaboratory-grade defenses into dependable, field-tested safeguards.\nAuthor Contributions\nArmstrong Foundjem: Conceptualization, Methodology, Data Curation, Software, Formal Analysis, Writing – Review\n& Editing, Validation, and Visualization. Lionel Nganyewou Tidjon: Writing – Original Draft, Data Curation, Vali-\ndation, Visualization. Leuson Da Silva: Writing – Review & Editing, Repository Mining. Foutse Khomh: Supervision,\nWriting – Review & Editing, and Funding Acquisition.\nAcknowledgment\nThis work is partly funded by the Fonds de Recherche du Québec (FRQ), Natural Sciences and Engineering Research\nCouncil of Canada (NSERC), Canadian Institute for Advanced Research (CIFAR), and Mathematics of Information\nTechnology and Complex Systems (MITACS).\nReferences\n[1] K. Kourou, T. P. Exarchos, K. P. Exarchos, M. V. Karamouzis, and D. I. Fotiadis, “Machine learning applications in cancer prognosis\nand prediction,” Computational and structural biotechnology journal, vol. 13, pp. 8–17, 2015.\n[2] G. Gui, F. Liu, J. Sun, J. Yang, Z. Zhou, and D. Zhao, “Flight delay prediction based on aviation big data and machine learning,”\nIEEE Transactions on Vehicular Technology, vol. 69, no. 1, pp. 140–150, 2020.\n[3] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, “A survey of deep learning applications to autonomous vehicle control,” IEEE\nTransactions on Intelligent Transportation Systems, vol. 22, no. 2, pp. 712–733, 2020.\n[4] M. Chenariyan Nakhaee, D. Hiemstra, M. Stoelinga, and M. van Noort, “The recent applications of machine learning in rail track\nmaintenance: A survey,” in Reliability, Safety, and Security of Railway Systems. Modelling, Analysis, Verification, and Certification,\nS. Collart-Dutilleul, T. Lecomte, and A. Romanovsky, Eds.\nCham: Springer International Publishing, 2019, pp. 91–105.\n[5] D. Girimonte and D. Izzo, “Artificial intelligence for space applications,” in Intelligent Computing Everywhere.\nSpringer, 2007, pp.\n235–253.\n[6] L. N. Tidjon, M. Frappier, and A. Mammar, “Intrusion detection systems: A cross-domain overview,” IEEE Communications Surveys\n& Tutorials, vol. 21, no. 4, pp. 3639–3681, 2019.\n[7] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson et al.,\n“Extracting training data from large language models,” in 30th USENIX Security Symposium (USENIX Security 21), 2021, pp.\n2633–2650.\n[8] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li, “Manipulating machine learning: Poisoning attacks and\ncountermeasures for regression learning,” in 2018 IEEE Symposium on Security and Privacy (SP).\nIEEE, 2018, pp. 19–35.\n[9] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of adversarial machine learning,” Pattern Recognition, vol. 84, pp.\n317–331, 2018.\n[10] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning in computer vision: A survey,” IEEE Access, vol. 6, pp.\n14 410–14 430, 2018.\n[11] D. Arp, E. Quiring, F. Pendlebury, A. Warnecke, F. Pierazzi, C. Wressnegger, L. Cavallaro, and K. Rieck, “Dos and don’ts of machine\nlearning in computer security,” in Proc. of the USENIX Security Symposium, 2022.\n[12] J. X. Morris, E. Lifland, J. Y. Yoo, J. Grigsby, D. Jin, and Y. Qi, “Textattack: A framework for adversarial attacks, data augmentation,\nand adversarial training in nlp,” arXiv preprint arXiv:2005.05909, 2020.\n[13] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro, “Intriguing properties of adversarial ml attacks in the problem space,”\nin 2020 IEEE symposium on security and privacy (SP).\nIEEE, 2020, pp. 1332–1349.\n[14] F. Tramer, N. Carlini, W. Brendel, and A. Madry, “On adaptive attacks to adversarial example defenses,” Advances in Neural\nInformation Processing Systems, vol. 33, pp. 1633–1645, 2020.\n[15] N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, A. Madry, and A. Kurakin, “On evaluating\nadversarial robustness,” arXiv preprint arXiv:1902.06705, 2019.\n[16] H. Abdullah, W. Garcia, C. Peeters, P. Traynor, K. R. Butler, and J. Wilson, “Practical hidden voice attacks against speech and\nspeaker recognition systems,” arXiv preprint arXiv:1904.05734, 2019.\n[17] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song, “Robust physical-world\nattacks on deep learning visual classification,” in Proceedings of the IEEE conference on computer vision and pattern recognition,\n2018, pp. 1625–1634.\n[18] F. Liu, Y. Yarom, Q. Ge, G. Heiser, and R. B. Lee, “Last-level cache side-channel attacks are practical,” in 2015 IEEE Symposium\non Security and Privacy, 2015, pp. 605–622.\n[19] P. Pessl, D. Gruss, C. Maurice, M. Schwarz, and S. Mangard, “DRAMA: Exploiting DRAM addressing for Cross-CPU attacks,” in\n25th USENIX Security Symposium (USENIX Security 16).\nAustin, TX: USENIX Association, Aug. 2016, pp. 565–581.\n[20] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot, “High accuracy and high fidelity extraction of neural networks,”\nin 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 1345–1362.\n[21] T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets: Stealing functionality of black-box models,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp. 4954–4963.\n[22] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference attacks against machine learning models,” in 2017 IEEE\nsymposium on security and privacy (SP).\nIEEE, 2017, pp. 3–18.\n"}, {"page": 51, "text": "51\n[23] J. Aigrain and M. Detyniecki, “Detecting adversarial examples and other misclassifications in neural networks by introspection,”\narXiv preprint arXiv:1905.09186, 2019.\n[24] Y. Gao, B. G. Doan, Z. Zhang, S. Ma, J. Zhang, A. Fu, S. Nepal, and H. Kim, “Backdoor attacks and countermeasures on deep\nlearning: A comprehensive review,” arXiv preprint arXiv:2007.10760, 2020.\n[25] “Cwe,” MITRE Corporation. [Online]. Available: https://cwe.mitre.org/top25/archive/2021/2021_cwe_top25.html\n[26] “Owasp top ten,” OWASP. [Online]. Available: https://owasp.org/www-project-top-ten/\n[27] B. E. Strom, J. A. Battaglia, M. S. Kemmerer, W. Kupersanin, D. P. Miller, C. Wampler, S. M. Whitley, and R. D. Wolf, “Finding\ncyber threats with att&ck-based analytics,” The MITRE Corporation, Tech. Rep., 2017, https://www.mitre.org/sites/default/files/\npublications/16-3713-finding-cyber-threats%20with%20att%26ck-based-analytics.pdf.\n[28] “Altas framework,” MITRE Corporation. [Online]. Available: https://atlas.mitre.org/matrices/matrix\n[29] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry, “Exploring the landscape of spatial robustness,” in Proceedings of the 36th\nInternational Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov,\nEds., vol. 97.\nPMLR, 09–15 Jun 2019, pp. 1802–1811. [Online]. Available: https://proceedings.mlr.press/v97/engstrom19a.html\n[30] “Adversarial ml 101,” MITRE Corporation. [Online]. Available: https://atlas.mitre.org/resources/adversarial-ml-101/\n[31] X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, “Badnl: Backdoor attacks against nlp models,” in ICML 2021 Workshop on\nAdversarial Machine Learning, 2021.\n[32] H. Abdullah, M. S. Rahman, W. Garcia, K. Warren, A. S. Yadav, T. Shrimpton, and P. Traynor, “Hear” no evil”, see” kenansville”*:\nEﬀicient and transferable black-box attacks on speech recognition and voice identification systems,” in 2021 IEEE Symposium on\nSecurity and Privacy (SP).\nIEEE, 2021, pp. 712–729.\n[33] H. Abdullah, K. Warren, V. Bindschaedler, N. Papernot, and P. Traynor, “Sok: The faults in our asrs: An overview of attacks against\nautomatic speech recognition and speaker identification systems,” in 2021 IEEE symposium on security and privacy (SP).\nIEEE,\n2021, pp. 730–747.\n[34] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto, and F. Roli, “Evasion attacks against machine learning\nat test time,” in Joint European conference on machine learning and knowledge discovery in databases.\nSpringer, 2013, pp. 387–402.\n[35] H. Kwon, Y. Kim, H. Yoon, and D. Choi, “Selective audio adversarial example in evasion attack on speech recognition system,” IEEE\nTransactions on Information Forensics and Security, vol. 15, pp. 526–538, 2019.\n[36] L. Schönherr, K. Kohls, S. Zeiler, T. Holz, and D. Kolossa, “Adversarial attacks against automatic speech recognition systems via\npsychoacoustic hiding,” arXiv preprint arXiv:1808.05665, 2018.\n[37] C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot, “Label-only membership inference attacks,” in International Conference\non Machine Learning.\nPMLR, 2021, pp. 1964–1974.\n[38] M. Nasr, R. Shokri, and A. Houmansadr, “Comprehensive privacy analysis of deep learning: Passive and active white-box inference\nattacks against centralized and federated learning,” in 2019 IEEE symposium on security and privacy (SP). IEEE, 2019, pp. 739–753.\n[39] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as a defense to adversarial perturbations against deep neural\nnetworks,” in 2016 IEEE symposium on security and privacy (SP).\nIEEE, 2016, pp. 582–597.\n[40] H. Huang, J. Mu, N. Z. Gong, Q. Li, B. Liu, and M. Xu, “Data poisoning attacks to deep learning based recommender systems,”\narXiv preprint arXiv:2101.02644, 2021.\n[41] Z. Wang, B. Liu, C. Lin, X. Zhang, C. Hu, J. Qin, and L. Luo, “Revisiting data poisoning attacks on deep learning based recommender\nsystems,” in 2023 IEEE Symposium on Computers and Communications (ISCC), July 2023, pp. 1261–1267.\n[42] H. Zhang, C. Tian, Y. Li, L. Su, N. Yang, W. X. Zhao, and J. Gao, “Data poisoning attack against recommender system\nusing incomplete and perturbed data,” in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining, ser. KDD ’21.\nNew York, NY, USA: Association for Computing Machinery, 2021, p. 2154–2164. [Online]. Available:\nhttps://doi.org/10.1145/3447548.3467233\n[43] S. Shankar, R. Garcia, J. M. Hellerstein, and A. G. Parameswaran, “”we have no idea how models will behave in production until\nproduction”: How engineers operationalize machine learning,” Proc. ACM Hum.-Comput. Interact., vol. 8, no. CSCW1, Apr. 2024.\n[Online]. Available: https://doi.org/10.1145/3653697\n[44] R. Ashmore, R. Calinescu, and C. Paterson, “Assuring the machine learning lifecycle: Desiderata, methods, and challenges,” ACM\ncomputing surveys (CSUR), vol. 54, no. 5, pp. 1–39, 2021.\n[45] M. Schlegel and K.-U. Sattler, “Management of machine learning lifecycle artifacts: A survey,” SIGMOD Rec., vol. 51, no. 4, p.\n18–35, Jan. 2023. [Online]. Available: https://doi.org/10.1145/3582302.3582306\n[46] H. Yu, K. Yang, T. Zhang, Y.-Y. Tsai, T.-Y. Ho, and Y. Jin, “Cloudleak: Large-scale deep learning models stealing through adversarial\nexamples.” in NDSS, vol. 38, 2020, p. 102.\n[47] D. Kindred, Theory generation for security protocols.\nCarnegie Mellon University, 1999.\n[48] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, “The security of machine learning,” Machine Learning, vol. 81, no. 2, pp.\n121–148, 2010.\n[49] C.-W. Ten, C.-C. Liu, and G. Manimaran, “Vulnerability assessment of cybersecurity for scada systems,” IEEE Transactions on Power\nSystems, vol. 23, no. 4, pp. 1836–1846, 2008.\n[50] R. S. Siva Kumar, M. Nyström, J. Lambert, A. Marshall, M. Goertzel, A. Comissoneru, M. Swann, and S. Xia, “Adversarial machine\nlearning-industry perspectives,” in 2020 IEEE Security and Privacy Workshops (SPW), 2020, pp. 69–75.\n[51] “Att&ck mitigations,” MITRE Corporation. [Online]. Available: https://attack.mitre.org/mitigations/enterprise/\n[52] J. McCarthy, D. Faatz, H. Perper, C. Peloquin, and J. Wiltberger, “Identity and access management,” NIST SPECIAL\nPUBLICATION, p. 2B, 1800.\n[53] V. C. Hu, M. Iorga, W. Bao, A. Li, Q. Li, A. Gouglidis et al., “General access control guidance for cloud systems,” NIST Special\nPublication, vol. 800, no. 210, pp. 50–2ex, 2020.\n[54] M. Souppaya, K. Scarfone et al., “Guide to enterprise patch management technologies,” NIST Special Publication, vol. 800, p. 40,\n2013.\n[55] K. Scarfone, M. Souppaya, A. Cody, and A. Orebaugh, “Technical guide to information security testing and assessment,” NIST Special\nPublication, vol. 800, no. 115, pp. 2–25, 2008.\n[56] K. A. Scarfone, W. Jansen, and M. Tracy, “Sp 800-123. guide to general server security,” National Institute of Standards & Technology,\n2008, https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-123.pdf.\n[57] T. Karygiannis and L. Owens, Wireless Network Security:.\nUS Department of Commerce, Technology Administration, National\nInstitute of …, 2002.\n[58] D. Cooper, A. Regenscheid, M. Souppaya, C. Bean, M. Boyle, D. Cooley, and M. Jenkins, “Security considerations for code signing,”\nNIST Cybersecurity White Paper, 2018.\n[59] “D3fend framework,” MITRE Corporation. [Online]. Available: https://d3fend.mitre.org/\n[60] Y. Lakhdhar and S. Rekhis, “Machine learning based approach for the automated mapping of discovered vulnerabilities to adversial\ntactics,” in 2021 IEEE Security and Privacy Workshops (SPW), 2021, pp. 309–317.\n"}, {"page": 52, "text": "52\n[61] A. Kuppa, L. Aouad, and N.-A. Le-Khac, “Linking cve’s to mitre att&ck techniques,” in The 16th International Conference on\nAvailability, Reliability and Security, ser. ARES 2021.\nNew York, NY, USA: Association for Computing Machinery, 2021.\n[62] S. McGregor, “Preventing repeated real world ai failures by cataloging incidents: The ai incident database,” in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 35, no. 17, 2021, pp. 15 458–15 463.\n[63] N. Carlini and D. Wagner, “Adversarial examples are not easily detected: Bypassing ten detection methods,” in Proceedings of the\n10th ACM workshop on artificial intelligence and security, 2017, pp. 3–14.\n[64] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial\nexamples,” in International conference on machine learning.\nPMLR, 2018, pp. 274–283.\n[65] E. Wallace, M. Stern, and D. Song, “Imitation attacks and defenses for black-box machine translation systems,” arXiv preprint\narXiv:2004.15015, 2020.\n[66] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, “Practical black-box attacks against machine learning,”\nin Proceedings of the 2017 ACM on Asia conference on computer and communications security, 2017, pp. 506–519.\n[67] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in machine learning: from phenomena to black-box attacks using\nadversarial samples,” arXiv preprint arXiv:1605.07277, 2016.\n[68] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier, “Parseval networks: Improving robustness to adversarial examples,”\nin International Conference on Machine Learning.\nPMLR, 2017, pp. 854–863.\n[69] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial\nnets,” Advances in neural information processing systems, vol. 27, 2014.\n[70] “Python packaging advisory database,” Python Software Foundation. [Online]. Available: https://github.com/pypa/advisory-database\n[71] A. Foundjem, L. Nganyewou Tidjon, L. Da Silva, and F. Khomh, “Replication package: Multi-agent threat assessment for AI-based\nsystems,” 2025, latest development version available at: https://github.com/foundjem/ThreatAssessment_AI-Agency.git. [Online].\nAvailable: https://doi.org/10.5281/zenodo.17480025\n[72] J. He, C. Treude, and D. Lo, “Llm-based multi-agent systems for software engineering: Literature review, vision and the road\nahead,” ACM Trans. Softw. Eng. Methodol., Jan. 2025, just Accepted. [Online]. Available: https://doi.org/10.1145/3712003\n[73] M. Alhanahnah and Y. Boshmaf, “DepsRAG: Towards agentic reasoning and planning for software dependency management,” in\nNeurIPS 2024 Workshop on Open-World Agents, 2024. [Online]. Available: https://openreview.net/forum?id=I396ZJFZLq\n[74] M. Arslan, H. Ghanem, S. Munawar, and C. Cruz, “A survey on rag with llms,” Procedia Computer Science, vol. 246, pp. 3781–3790,\n2024.\n[75] L. A. . Data, “Lf ai & data foundation project lifecycle document,” The Linux Fondation, Tech. Rep., 2021, https://github.com/lfai/\nproposing-projects/blob/master/LFAI%26Data-Project%20LifecycleDocument.pdf.\n[76] J. Wu, H. He, K. Gao, W. Xiao, J. Li, and M. Zhou, “A comprehensive analysis of challenges and strategies for software release notes\non github,” Empirical Software Engineering, vol. 29, no. 5, p. 104, 2024.\n[77] D. Đurđev, “Popularity of programming languages,” AIDASCO Reviews, vol. 2, no. 2, pp. 24–29, 2024.\n[78] O. Mediakov, B. Korostynskyi, V. Vysotska, O. Markiv, and S. Chyrun, “Experimental and exploratory analysis of programming\nlanguages popularity according to the pypl index.” in MoMLeT+ DS, 2022, pp. 307–332.\n[79] “Case studies,” MITRE Corporation. [Online]. Available: https://atlas.mitre.org/studies\n[80] B. E. Strom, A. Applebaum, D. P. Miller, K. C. Nickels, A. G. Pennington, and C. B. Thomas, “Mitre att&ck: Design and philosophy,”\nin Technical report.\nThe MITRE Corporation, 2018.\n[81] “Att&ck framework,” MITRE Corporation. [Online]. Available: https://attack.mitre.org/\n[82] M. Guranda, “Towards benchmarking the robustness of neuro-symbolic learning against data poisoning backdoor attacks,” Ph.D.\ndissertation, Delft University of Technology, 2025.\n[83] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnerabilities in the machine learning model supply chain. arxiv 2017,”\narXiv preprint arXiv:1708.06733, 2017.\n[84] W. Zhu et al., “LoRA-Leak: Fine-tune leakage in adapter-based llms,” in Proceedings of the ACM Conference on Computer and\nCommunications Security, 2024.\n[85] M. Park et al., “Reward hacking in rlhf for large language models,” in NeurIPS Workshop on Alignment, 2023.\n[86] Y. Bai et al., “MASTERKEY: Universal jailbreak prompts for large language models,” arXiv:2309.01827, 2023.\n[87] R. Badoiu and M. A. R. Team, “Pyrit: Microsoft’s open-source framework for red-teaming generative-ai systems,” https://github.\ncom/microsoft/pyrit, Feb. 2024, gitHub repo + white-paper; Section 3 shows a shadow-\nmodel attack against an Azure ML endpoint using only\nmodel-family knowledge and public API queries.\n[88] “Tek fog: Morphing urls to make real new fake, ’hijacking’ whatsapp to drive bjp propaganda,” The Wire India. [Online]. Available:\nhttps://thewire.in/tekfog/en/2.html\n[89] M. Sokolova and G. Lapalme, “A systematic analysis of performance measures for classification tasks,” Information Processing &\nManagement, vol. 45, no. 4, pp. 427–437, 2009.\n[90] H. Li, X. Li, Y. Dong, and K. Liu, “From macro to micro: Probing dataset diversity in language model fine-tuning,” arXiv preprint\narXiv:2505.24768, 2025.\n[91] L. Li, F. Qiang, and L. Ma, “Advancing cybersecurity: Graph neural networks in threat intelligence knowledge graphs,” in\nProceedings of the International Conference on Algorithms, Software Engineering, and Network Security, ser. ASENS ’24.\nNew York,\nNY, USA: Association for Computing Machinery, 2024, p. 737–741. [Online]. Available: https://doi.org/10.1145/3677182.3677314\n[92] T. Altaf, X. Wang, W. Ni, G. Yu, R. P. Liu, and R. Braun, “Gnn-based network traﬀic analysis for the detection of sequential attacks\nin iot,” Electronics, vol. 13, no. 12, p. 2274, 2024.\n[93] J. Xiao, L. Yang, F. Zhong, X. Wang, H. Chen, and D. Li, “Robust anomaly-based insider threat detection using graph neural\nnetwork,” IEEE Transactions on Network and Service Management, vol. 20, no. 3, pp. 3717–3733, 2022.\n[94] B. VS, A. G. PS, and S. K, “Forecasting and analysing cyber threats with graph neural networks and gradient based explanation for\nfeature impacts,” in 2024 Global Conference on Communications and Information Technologies (GCCIT), 2024, pp. 1–6.\n[95] K. Scarfone and P. Mell, The Common configuration scoring system (CCSS): Metrics for software security configuration vulnerabilities\n(Draft).\nUS Department of Commerce, National Institute of Standards and Technology, 2009.\n[96] Y. Jiang, N. Oo, Q. Meng, H. W. Lim, and B. Sikdar, “Vulrg: Multi-level explainable vulnerability patch ranking for complex\nsystems using graphs,” CoRR, vol. abs/2502.11143, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2502.11143\n[97] J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F. de Souza, and T. Oliveira-Santos, “Copycat cnn: Stealing knowledge by persuading\nconfession with random non-labeled data,” in 2018 International Joint Conference on Neural Networks (IJCNN).\nIEEE, 2018, pp.\n1–8.\n[98] L. Beurer-Kellner, B. Buesser, A.-M. Creţu, E. Debenedetti, D. Dobos, D. Fabian, M. Fischer, D. Froelicher, K. Grosse, D. Naeff\net al., “Design patterns for securing llm agents against prompt injections,” arXiv preprint arXiv:2506.08837, 2025.\n[99] J. Zhang, M. Ding, Y. Liu, J. Hong, and F. Tramèr, “Black-box optimization of llm outputs by asking for directions,” 2025. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:282209868\n"}, {"page": 53, "text": "53\n[100] S. Ö. Arık, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, Y. Kang, X. Li, J. Miller, A. Ng, J. Raiman et al., “Deep voice:\nReal-time neural text-to-speech,” in International Conference on Machine Learning.\nPMLR, 2017, pp. 195–204.\n[101] “Experimental security research of tesla autopilot,” Tencent Keen Security Lab, Tech. Rep., 2019, https://keenlab.tencent.com/en/\nwhitepapers/Experimental_Security_Research_of_Tesla_Autopilot.pdf.\n[102] H. Yang et al., “Autodan: Automated jailbreak attacks on instruction-tuned llms,” arXiv:2402.01234, 2024.\n[103] Z. Liu et al., “Prompt injection attacks on large language models,” in IEEE Security & Privacy Workshops, 2023.\n[104] E. Perez et al., “Red teaming RLHF reward models,” in International Conference on Learning Representations, 2024.\n[105] H. Chen et al., “Adapterleak: Gradient leakage in parameter-eﬀicient tuning,” in USENIX Security Symposium, 2024.\n[106] N. Carlini, D. Paleka, K. D. Dvijotham, T. Steinke, J. Hayase, A. F. Cooper, K. Lee, M. Jagielski, M. Nasr, A. Conmy et al., “Stealing\npart of a production language model,” arXiv preprint arXiv:2403.06634, 2024.\n[107] X. Wang et al., “Cat-llama: Compact and transferable model distillation,” in Proceedings of ACL, 2024.\n[108] S. Kandpal et al., “Stealing gpt-3 via extremely large query batches,” arXiv:2305.14485, 2023.\n[109] N. Carlini et al., “Extracting training data from large language models,” in USENIX Security Symposium, 2023.\n[110] J. Lee et al., “Membership inference attacks against LLM apis,” in Proceedings of the ACM Conference on Computer and\nCommunications Security, 2024.\n[111] S. Liu et al., “Attacking tool-enabled LLM frameworks: Threats and defenses,” in Network and Distributed System Security\nSymposium, 2025.\n[112] Z. Wu, H. Gao, J. He, and P. Wang, “The dark side of function calling: Pathways to jailbreaking large language models,” arXiv\npreprint arXiv:2407.17915, 2024.\n[113] X. Shen, Y. Shen, M. Backes, and Y. Zhang, “Gptracker: A large-scale measurement of misused gpts,” in 2025 IEEE Symposium on\nSecurity and Privacy (SP).\nIEEE, 2025, pp. 336–354.\n[114] B. Hui, H. Yuan, N. Gong, P. Burlina, and Y. Cao, “Pleak: Prompt leaking attacks against large language model applications,” in\nProceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, ser. CCS ’24.\nNew York, NY,\nUSA: Association for Computing Machinery, 2024, p. 3600–3614. [Online]. Available: https://doi.org/10.1145/3658644.3670370\n[115] D. Pape, S. Mavali, T. Eisenhofer, and L. Schönherr, “Prompt obfuscation for large language models,” arXiv preprint arXiv:2409.11026,\n2024.\n[116] T. Green, M. Gubri, H. Puerto, S. Yun, and S. J. Oh, “Leaky thoughts: Large reasoning models are not private thinkers,” arXiv\npreprint arXiv:2506.15674, 2025.\n[117] S. Cohen, R. Bitton, and B. Nassi, “Here comes the ai worm: Unleashing zero-click worms that target genai-powered applications,”\narXiv preprint arXiv:2403.02817, 2024.\n[118] A. K. Jain, R. P. W. Duin, and J. Mao, “Statistical pattern recognition: A review,” IEEE Transactions on pattern analysis and\nmachine intelligence, vol. 22, no. 1, pp. 4–37, 2000.\n[119] T. G. Dietterich, “Ensemble methods in machine learning,” in International workshop on multiple classifier systems.\nSpringer, 2000,\npp. 1–15.\n[120] J. Han, M. Kamber, and J. Pei, Data Mining: Concepts and Techniques, 3rd ed.\nBurlington, MA: Morgan Kaufmann, 2011.\n[121] C. Sammut and G. I. Webb, Encyclopedia of machine learning and data mining.\nSpringer Publishing Company, Incorporated, 2017.\n[122] L. Wang, D. Zhang, D. Yang, A. Pathak, C. Chen, X. Han, H. Xiong, and Y. Wang, “Space-ta: Cost-effective task allocation exploiting\nintradata and interdata correlations in sparse crowdsensing,” ACM Transactions on Intelligent Systems and Technology (TIST), vol. 9,\nno. 2, pp. 1–28, 2017.\n[123] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling, “Modeling relational data with graph convolutional\nnetworks,” The Semantic Web: ESWC 2018, vol. 10843, pp. 593–607, 2018, r-GCN: Relational Graph Convolution.\n[124] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “GNNExplainer: Generating explanations for graph neural networks,”\nAdvances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019, post-hoc path-based explanation method. [Online].\nAvailable: https://arxiv.org/abs/1903.03894\n[125] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong, “Jailbreaking black box large language models in twenty\nqueries,” 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pp. 23–42, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:263908890\n[126] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, Y. Zhang, N. Gong et al., “Promptrobust: Towards evaluating\nthe robustness of large language models on adversarial prompts,” in Proceedings of the 1st ACM Workshop on Large AI Systems and\nModels with Privacy and Safety Analysis, 2023, pp. 57–68.\n[127] A. Elmahdy and A. Salem, “Deconstructing classifiers: Towards a data reconstruction attack against text classification models,” arXiv\npreprint arXiv:2306.13789, 2023.\n[128] S. Casper, J. Lin, J. Kwon, G. Culp, and D. Hadfield-Menell, “Explore, establish, exploit: Red teaming language models from scratch,”\narXiv preprint arXiv:2306.09442, 2023.\n[129] Y. Huang, Q. Zhang, L. Sun et al., “Trustgpt: A benchmark for trustworthy and responsible large language models,” arXiv preprint\narXiv:2306.11507, 2023.\n[130] S. Han, B. Buyukates, Z. Hu, H. Jin, W. Jin, L. Sun, X. Wang, W. Wu, C. Xie, Y. Yao et al., “Fedsecurity: A benchmark for attacks\nand defenses in federated learning and federated llms,” in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, 2024, pp. 5070–5081.\n[131] Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng et al., “Prompt injection attack against\nllm-integrated applications,” arXiv preprint arXiv:2306.05499, 2023.\n[132] M. van Wyk, M. Bekker, X. Richards, and K. Nixon, “Protect your prompts: Protocols for ip protection in llm applications,” arXiv\npreprint arXiv:2306.06297, 2023.\n[133] C. Wang, S. K. Freire, M. Zhang, J. Wei, J. Goncalves, V. Kostakos, Z. Sarsenbayeva, C. Schneegass, A. Bozzon, and E. Niforatos,\n“Safeguarding crowdsourcing surveys from chatgpt with prompt injection,” arXiv preprint arXiv:2306.08833, 2023.\n[134] A. Qammar, H. Wang, J. Ding, A. Naouri, M. Daneshmand, and H. Ning, “Chatbots to chatgpt in a cybersecurity space: Evolution,\nvulnerabilities, attacks, challenges, and future recommendations,” arXiv preprint arXiv:2306.09255, 2023.\n[135] X. Qi, K. Huang, A. Panda, P. Henderson, M. Wang, and P. Mittal, “Visual adversarial examples jailbreak aligned large language\nmodels,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 19, 2024, pp. 21 527–21 536.\n[136] J. Hughes, S. Price, A. Lynch, R. Schaeffer, F. Barez, S. Koyejo, H. Sleight, E. Jones, E. Perez, and M. Sharma, “Best-of-n jailbreaking,”\narXiv preprint arXiv:2412.03556, 2024.\n[137] N. Carlini, M. Jagielski, C. A. Choquette-Choo, D. Paleka, W. Pearce, H. Anderson, A. Terzis, K. Thomas, and F. Tramèr, “Poisoning\nweb-scale training datasets is practical,” in 2024 IEEE Symposium on Security and Privacy (SP).\nIEEE, 2024, pp. 407–425.\n[138] E. Pelofske, L. M. Liebrock, and V. Urias, “Cybersecurity threat hunting and vulnerability analysis using a neo4j graph database of\nopen source intelligence,” arXiv preprint arXiv:2301.12013, 2023.\n[139] J. He and M. Vechev, “Large language models for code: Security hardening and adversarial testing,” in Proceedings of the 2023 ACM\nSIGSAC Conference on Computer and Communications Security, 2023, pp. 1865–1879.\n"}, {"page": 54, "text": "54\n[140] Q. Li, C. Thapa, L. Ong, Y. Zheng, H. Ma, S. A. Camtepe, A. Fu, and Y. Gao, “Vertical federated learning: taxonomies, threats,\nand prospects,” arXiv preprint arXiv:2302.01550, 2023.\n[141] M. A. Rahman, L. Alqahtani, A. Albooq, and A. Ainousah, “A survey on security and privacy of large multimodal deep learning\nmodels: Teaching and learning perspective,” in 2024 21st Learning and Technology Conference (L&T).\nIEEE, 2024, pp. 13–18.\n[142] J. Malik, R. Muthalagu, and P. M. Pawar, “A systematic review of adversarial machine learning attacks, defensive controls and\ntechnologies,” IEEE Access, 2024.\n[143] W. Xiong, E. Legrand, O. Åberg, and R. Lagerström, “Cyber security threat modeling based on the mitre enterprise att&ck matrix,”\nSoftware and Systems Modeling, pp. 1–21, 2021.\n[144] A. Kuppa, L. Aouad, and N.-A. Le-Khac, “Linking cve’s to mitre att&ck techniques,” in The 16th International Conference on\nAvailability, Reliability and Security, ser. ARES 2021.\nNew York, NY, USA: Association for Computing Machinery, 2021.\n[145] E. Tabassi, K. J. Burns, M. Hadjimichael, A. D. Molina-Markham, and J. T. Sexton, “A taxonomy and terminology of adversarial\nmachine learning,” NIST IR, pp. 1–29, 2019.\n[146] Y. Lakhdhar and S. Rekhis, “Machine learning based approach for the automated mapping of discovered vulnerabilities to adversial\ntactics,” in 2021 IEEE Security and Privacy Workshops (SPW), 2021, pp. 309–317.\n[147] R. S. Siva Kumar, M. Nyström, J. Lambert, A. Marshall, M. Goertzel, A. Comissoneru, M. Swann, and S. Xia, “Adversarial machine\nlearning-industry perspectives,” in 2020 IEEE Security and Privacy Workshops (SPW), 2020, pp. 69–75.\n[148] T. Fu, M. Sharma, P. Torr, S. B. Cohen, D. Krueger, and F. Barez, “Poisonbench: Assessing large language model vulnerability to\ndata poisoning,” 2024. [Online]. Available: https://arxiv.org/abs/2410.08811\n[149] J. Hughes, S. Price, A. Lynch, R. Schaeffer, F. Barez, S. Koyejo, H. Sleight, E. Jones, E. Perez, and M. Sharma, “Best-of-n jailbreaking,”\narXiv preprint arXiv:2412.03556, 2024.\n[150] A. E. Cinà, K. Grosse, A. Demontis, S. Vascon, W. Zellinger, B. A. Moser, A. Oprea, B. Biggio, M. Pelillo, and F. Roli, “Wild patterns\nreloaded: A survey of machine learning security against training data poisoning,” ACM Computing Surveys, vol. 55, 12 2023.\n[151] K. Shaukat, S. Luo, V. Varadharajan, I. A. Hameed, and M. Xu, “A survey on machine learning techniques for cyber security in the\nlast decade,” IEEE Access, vol. 8, pp. 222 310–222 354, 2020.\n[152] N. Mehrabi, P. Goyal, C. Dupuy, Q. Hu, S. Ghosh, R. Zemel, K.-W. Chang, A. Galstyan, and R. Gupta, “Flirt: Feedback loop\nin-context red teaming,” arXiv, 8 2023. [Online]. Available: http://arxiv.org/abs/2308.04265\n[153] M. Bagaa, T. Taleb, J. B. Bernabe, and A. Skarmeta, “A machine learning security framework for iot systems,” IEEE Access, vol. 8,\npp. 114 066–114 077, 2020.\n[154] S. V. Hoseini, J. Suutala, J. Partala, and K. Halunen, “Threat modeling ai/ml with the attack tree,” IEEE Access, 2024.\n[155] K. He, D. D. Kim, and M. R. Asghar, “Adversarial machine learning for network intrusion detection systems: A comprehensive\nsurvey,” IEEE Communications Surveys and Tutorials, vol. 25, pp. 538–566, 11 2023. [Online]. Available: https://github.\n[156] I. Shumailov, Y. Zhao, D. Bates, N. Papernot, R. Mullins, and R. Anderson, “Sponge examples: Energy-latency attacks on neural\nnetworks,” 6 2020. [Online]. Available: http://arxiv.org/abs/2006.03463\n[157] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandkumar, “Diffusion models for adversarial purification,” 5 2022.\n[Online]. Available: http://arxiv.org/abs/2205.07460\n[158] J. Ahmad, M. U. Zia, I. H. Naqvi, J. N. Chattha, F. A. Butt, T. Huang, and W. Xiang, “Machine learning and blockchain technologies\nfor cybersecurity in connected vehicles,” 1 2024.\n[159] A. Qammar, H. Wang, J. Ding, A. Naouri, M. Daneshmand, and H. Ning, “Chatbots to chatgpt in a cybersecurity space: Evolution,\nvulnerabilities, attacks, challenges, and future recommendations,” 5 2023. [Online]. Available: http://arxiv.org/abs/2306.09255\n[160] Y. Hu, W. Kuang, Z. Qin, K. Li, J. Zhang, Y. Gao, W. Li, and K. Li, “Artificial intelligence security: Threats and countermeasures,”\n1 2021.\n[161] N. Bouacida and P. Mohapatra, “Vulnerabilities in federated learning,” IEEE Access, vol. 9, pp. 63 229–63 249, 2021.\n[162] N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P. yeh Chiang, M. Goldblum, A. Saha, J. Geiping,\nand T. Goldstein, “Baseline defenses for adversarial attacks against aligned language models,” 9 2023. [Online]. Available:\nhttp://arxiv.org/abs/2309.00614\n[163] J. Li, Z. Wu, W. Ping, C. Xiao, and V. G. V. Vydiswaran, “Defending against insertion-based textual backdoor attacks via\nattribution,” pp. 8818–8833. [Online]. Available: https://github.\n[164] C. Wu, X. Li, and J. Wang, “Vulnerabilities of foundation model integrated federated learning under adversarial threats,” 1 2024.\n[Online]. Available: http://arxiv.org/abs/2401.10375\n[165] X. Qi, K. Huang, A. Panda, P. Henderson, M. Wang, and P. Mittal, “Visual adversarial examples jailbreak aligned large language\nmodels,” 6 2023. [Online]. Available: http://arxiv.org/abs/2306.13213\n[166] C.\nWang,\nS.\nK.\nFreire,\nM.\nZhang,\nJ.\nWei,\nJ.\nGoncalves,\nV.\nKostakos,\nZ.\nSarsenbayeva,\nC.\nSchneegass,\nA.\nBozzon,\nand E. Niforatos, “Safeguarding crowdsourcing surveys from chatgpt with prompt injection,” 6 2023. [Online]. Available:\nhttp://arxiv.org/abs/2306.08833\n[167] G.\nDeng,\nY.\nLiu,\nY.\nLi,\nK.\nWang,\nY.\nZhang,\nZ.\nLi,\nH.\nWang,\nT.\nZhang,\nand\nY.\nLiu,\n“Masterkey:\nAutomated\njailbreak across multiple large language model chatbots,” 7 2023. [Online]. Available: http://arxiv.org/abs/2307.08715http:\n//dx.doi.org/10.14722/ndss.2024.24188\n[168] H. Kwon, J. Kim, and W. Pak, “Graph-based prompt injection attacks against large language models,” in 2024 IEEE International\nConference on Technology, Informatics, Management, Engineering and Environment (TIME-E), vol. 5, 2024, pp. 1–5.\n[169] E. Fedorchenko, N. Busko, and E. Novikova, “Automated assessment of the exploits using deep learning methods,” in International\nConference on Risks and Security of Internet and Systems.\nSpringer, 2024, pp. 509–524.\n[170] A. Okutan and M. Mirakhorli, “Predicting the severity and exploitability of vulnerability reports using convolutional neural nets,”\nin Proceedings of the 3rd International Workshop on Engineering and Cybersecurity of Critical Systems, ser. EnCyCriS ’22.\nNew\nYork, NY, USA: Association for Computing Machinery, 2022, p. 1–8. [Online]. Available: https://doi.org/10.1145/3524489.3527298\n[171] B. Steenhoek, M. M. Rahman, and et al., “To err is machine: Vulnerability detection challenges llm reasoning,” arXiv, 2024.\n[172] J. Wang, Z. Hu, and D. Wagner, “JULI: Jailbreak large language models by self‐introspection,” arXiv, 2025.\n[173] F. Jiang, Z. Xu, and et al., “Artprompt: Ascii art‐based jailbreak attacks against aligned llms,” Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2024.\n[174] M. Sabbaghi, P. Kassianik, G. J. Pappas, Y. Singer, A. Karbasi, and H. Hassani, “Adversarial reasoning at jailbreaking time,” arXiv,\n2025.\n[175] X. Yang, B. Zhou, X. Tang, J. Han, and S. Hu, “Exploiting synergistic cognitive biases to bypass safety in llms,” arXiv, 2025.\n[176] T. Tong, F. Wang, Z. Zhao, and M. Chen, “BadJudge: Backdoor vulnerabilities of LLM‐as‐a‐Judge,” in International Conference on\nLearning Representations (ICLR), 2025.\n[177] M. Bhatt, S. Chennabasappa, J. Saxe, and et al., “Cyberseceval 2: A wide‐ranging cybersecurity evaluation suite for large language\nmodels,” arXiv, 2024.\n"}, {"page": 55, "text": "55\n[178] X. Li, R. Wang, M. Cheng, T. Zhou, and C. Hsieh, “Drattack: Prompt decomposition and reconstruction makes powerful llm\njailbreakers,” in Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024.\n[179] Y. Li, X. Li, S. Zhong, and et al., “Everything you wanted to know about LLM-based vulnerability detection but were afraid to ask,”\narXiv, 2025.\n[180] Z. Liao, Y. Nan, Z. Zheng, and et al., “Augmenting smart contract decompiler output through fine‐grained dependency analysis and\nllm‐facilitated semantic recovery,” arXiv, 2025.\n[181] L. Rossi, M. Aerni, J. Zhang, and F. Tramèr, “Membership inference attacks on sequence models,” in 2025 IEEE Security and Privacy\nWorkshops (SPW).\nIEEE, 2025, pp. 98–110.\n[182] K. Nikolić, L. Sun, J. Zhang, and F. Tramèr, “The jailbreak tax: How useful are your jailbreak outputs?” arXiv preprint\narXiv:2504.10694, 2025.\n[183] J. Rando, J. Zhang, N. Carlini, and F. Tramèr, “Adversarial ml problems are getting harder to solve and to evaluate,” arXiv preprint\narXiv:2502.02260, 2025.\n[184] J. Rando, H. Korevaar, E. Brinkman, I. Evtimov, and F. Tramèr, “Gradient-based jailbreak images for multimodal fusion models,”\narXiv preprint arXiv:2410.03489, 2024.\n[185] Y. Jin, C. Li, J. Wang, Z. Liu, W. Qiu et al., “LLM-BSCVM: An LLM-based blockchain smart-contract vulnerability management\nframework,” arXiv pre-print, May 2025.\n[186] F. Liu, H. Wang, J. Cho, D. Roth, and A. W. Lo, “AUTOCT: Automating interpretable clinical-trial prediction with LLM agents,”\narXiv pre-print, Jun 2025.\n[187] Z. Gao, H. Wang, Y. Zhou, W. Zhu, and C. Zhang, “How far have we gone in vulnerability detection using large language models,”\narXiv pre-print, Nov 2023.\n[188] F. Weng, Y. Xu, C. Fu, and W. Wang, “\nmathitMMJ-Bench: A comprehensive study on jailbreak attacks and defenses for multimodal large language models,” arXiv pre-print,\nAug 2024.\n[189] ——, “MMJ-Bench: A comprehensive study on jailbreak attacks and defenses for vision–language models,” in AAAI Conf. on Artificial\nIntelligence, Apr 2025.\n[190] S. Lee, S. Ni, and et al., “xJailbreak: Representation-space guided reinforcement learning for interpretable LLM jailbreaking,” arXiv\npre-print, Jan 2025.\n[191] Y. In, W. Kim, Y. zhe Li, Y. Jo, C. Park et al., “Is safety standard the same for everyone? user-specific safety evaluation of large\nlanguage models,” arXiv pre-print, Feb 2025.\n[192] D. Ivry and O. Nahum, “Sentinel: A state-of-the-art model to protect against prompt injections,” arXiv pre-print, Jun 2025.\n[193] X. Huang, X. Wang, C. Pan, and et al., “Medical MLLM is vulnerable: Cross-modality jailbreak and mismatched attacks on medical\nmultimodal large language models,” in AAAI Conf. on Artificial Intelligence, May 2024.\n[194] K. Zhang, S. Wang, S. Wen, and et al., “Your fix is my exploit: Enabling comprehensive DL library API fuzzing with large language\nmodels,” in International Conference on Software Engineering (ICSE), Jan 2025.\n[195] Y. Gou, K. Chen, Y. Zhang, and et al., “Eyes closed, safety on: Protecting multimodal LLMs via image-to-text transformation,” in\nEuropean Conference on Computer Vision (ECCV), Mar 2024.\n[196] S. S. Daneshvar, Y. Nong, X. Yang, S. Wang, and H. Cai, “VulScribeR: Exploring RAG-based vulnerability augmentation with LLMs,”\narXiv pre-print, Aug 2024.\n[197] W. Kim, S. Park, Y. In, S. Han, and C. Park, “SIMPLOT: Enhancing chart question answering by distilling essentials,” in NAACL\n2024, Feb 2024.\n[198] J. Hu, Q. Zhang, and H. Yin, “Augmenting greybox fuzzing with generative AI,” arXiv pre-print, Jun 2023.\n[199] C. Zhang, M.-A. Côté, M. Abdul-Mageed, and et al., “DefenderBench: A toolkit for evaluating language agents in cybersecurity\nenvironments,” arXiv pre-print, May 2025.\n[200] S. Patnaik, H. Changwal, M. Aggarwal, S. Bhatia, Y. Kumar, and B. Krishnamurthy, “CABINET: Content relevance–based noise\nreduction for table question answering,” in International Conference on Learning Representations (ICLR), Feb 2024.\n[201] X. Li, Y. Mao, Z. Lu, W. Li, and Z. Li, “SCLA: Automated smart-contract summarization via LLMs and control-flow prompt,” arXiv\npre-print, Feb 2024.\n[202] F. Jiang, Z. Xu, L. Niu, B. Y. Lin, and R. Poovendran, “ChatBug: A common vulnerability of aligned LLMs induced by chat\ntemplates,” in AAAI Conf. on Artificial Intelligence, Jun 2024.\n[203] S. Chen, Z. Han, J. Gu, and et al., “Red teaming GPT-4V: Are GPT-4V safe against uni/multi-modal jailbreak attacks?” arXiv\npre-print, Apr 2024.\n[204] Y. Pan, T. Shi, J. Zhao, and J. W. Ma, “Detecting and filtering unsafe training data via data attribution,” arXiv pre-print, Feb 2025.\n[205] A. Foundjem, E. E. Eghan, and B. Adams, “A grounded theory of cross-community secos: Feedback diversity versus synchronization,”\nIEEE Transactions on Software Engineering, vol. 49, no. 10, pp. 4731–4750, 2023.\n[206] “State of machine learning and data science 2021,” Kaggle, 2021. [Online]. Available: https://storage.googleapis.com/kaggle-media/\nsurveys/Kaggleś%20State%20of%20Machine%20Learning%20and%20Data%20Science%202021.pdf\n[207] H. Chen, H. Zhang, D. Boning, and C.-J. Hsieh, “Robust decision trees against adversarial examples,” in International Conference\non Machine Learning.\nPMLR, 2019, pp. 1122–1131.\n[208] National Institute of Standards and Technology (NIST), “Cybersecurity Framework,” 2024, accessed: 2024-02-17. [Online]. Available:\nhttps://www.nist.gov/cybersecurity\n[209] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial examples in deep neural networks,” arXiv preprint\narXiv:1704.01155, 2017.\n[210] J. Lu, T. Issaranon, and D. Forsyth, “Safetynet: Detecting and rejecting adversarial examples robustly,” in Proceedings of the IEEE\ninternational conference on computer vision, 2017, pp. 446–454.\n[211] C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, “Feature denoising for improving adversarial robustness,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 501–509.\n[212] R. Chandramouli, D. Pinhas et al., “Security guidelines for storage infrastructure,” NIST Special Publication, vol. 800, p. 209, 2020.\n[213] J. Wack, K. Cutler, and J. Pole, “Guidelines on firewalls and firewall policy,” BOOZ-ALLEN AND HAMILTON INC MCLEAN VA,\nTech. Rep., 2002.\n[214] K. Scarfone, P. Mell et al., “Guide to intrusion detection and prevention systems (idps),” NIST special publication, vol. 800, no. 2007,\np. 94, 2007.\n[215] S. Frankel, K. Kent, R. Lewkowski, A. D. Orebaugh, R. W. Ritchey, and S. R. Sharma, “Guide to ipsec vpns:.” 2005.\n[216] S. Rose, O. Borchert, S. Mitchell, and S. Connelly, “Zero trust architecture,” National Institute of Standards and Technology, Tech.\nRep., 2020.\n[217] Y. Ge, N. Kirtane, H. Peng, and D. Hakkani-Tür, “Llms are vulnerable to malicious prompts disguised as scientific language,” arXiv\npreprint arXiv:2501.14073, 2025.\n"}, {"page": 56, "text": "56\n[218] H. Kwon and W. Pak, “Text-based prompt injection attack using mathematical functions in modern large language models,”\nElectronics, vol. 13, no. 24, p. 5008, 2024.\n[219] D. A. Alber, Z. Yang, A. Alyakin, E. Yang, S. Rai, A. A. Valliani, J. Zhang, G. R. Rosenbaum, A. K. Amend-Thomas, D. B. Kurland\net al., “Medical large language models are vulnerable to data-poisoning attacks,” Nature Medicine, vol. 31, no. 2, pp. 618–626, 2025.\n[220] P. M. P. Curvo, “The traitors: Deception and trust in multi-agent language model simulations,” ArXiv, vol. abs/2505.12923, 2025.\n[Online]. Available: https://api.semanticscholar.org/CorpusID:278740659\n[221] X. W. Chia and J. Pan, “Probing latent subspaces in llm for ai security: Identifying and manipulating adversarial states,” ArXiv,\nvol. abs/2503.09066, 2025. [Online]. Available: https://api.semanticscholar.org/CorpusID:276938259\n[222] E. Basic and A. Giaretta, “Large language models and code security: A systematic literature review,” arXiv preprint arXiv:2412.15004,\n2024.\n[223] M. Fakih, R. Dharmaji, H. Bouzidi, G. Q. Araya, O. Ogundare, and M. A. A. Faruque, “Llm4cve: Enabling iterative automated\nvulnerability repair with large language models,” arXiv preprint arXiv:2501.03446, 2025.\n[224] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, “”do anything now”: Characterizing and evaluating in-the-wild jailbreak\nprompts on large language models,” Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications\nSecurity, 2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:260704242\n[225] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, A. Wesslén et al., Experimentation in software engineering.\nSpringer,\n2012, vol. 236.\n[226] N. Juristo and A. M. Moreno, Basics of software engineering experimentation.\nSpringer Science & Business Media, 2013.\n"}]}