{"doc_id": "arxiv:2601.17952", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.17952.pdf", "meta": {"doc_id": "arxiv:2601.17952", "source": "arxiv", "arxiv_id": "2601.17952", "title": "A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models", "authors": ["Michail Mamalakis", "Tiago Azevedo", "Cristian Cosentino", "Chiara D'Ercoli", "Subati Abulikemu", "Zhongtian Sun", "Richard Bethlehem", "Pietro Lio"], "published": "2026-01-25T19:03:04Z", "updated": "2026-01-25T19:03:04Z", "summary": "Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.17952v1", "url_pdf": "https://arxiv.org/pdf/2601.17952.pdf", "meta_path": "data/raw/arxiv/meta/2601.17952.json", "sha256": "50f54b5249f3fd27150ff1f99b7b13cbe963c6fff9441f71880056695d5bd828", "status": "ok", "fetched_at": "2026-02-18T02:20:34.846384+00:00"}, "pages": [{"page": 1, "text": "A MONOSEMANTIC ATTRIBUTION FRAMEWORK FOR STABLE\nINTERPRETABILITY IN CLINICAL NEUROSCIENCE LARGE LANGUAGE\nMODELS\nMichail Mamalakis\nDepartment of Computer Science and Technology\nCancer Research UK Cambridge Institute\nUniversity of Cambridge\nUnited Kingdom\nmm2703@cam.ac.uk\nTiago Azevedo\nDepartment of Computer Science and Technology\nUniversity of Cambridge\nUnited Kingdom\nCristian Cosentino\nDIMES\nUniversity of Calabria\nItaly\nChiara D’Ercoli\nDepartment of Computer\nAutomatic and Management Engineering (DIAG)\nSapienza Università di Roma\nItaly\nSubati Abulikemu\nDepartment of Psychiatry\nUniversity of Cambridge\nUnited Kingdom\nZhongtian Sun\nSchool of Computing\nUniversity of Kent\nUnited Kingdom\nRichard Bethlehem\nDepartment of Psychology\nUniversity of Cambridge\nUnited Kingdom\nPietro Lio\nDepartment of Computer Science and Technology\nUniversity of Cambridge\nUnited Kingdom\nJanuary 27, 2026\nABSTRACT\nInterpretability remains a key challenge for deploying large language models (LLMs) in clinical set-\ntings such as Alzheimer’s disease progression diagnosis, where early and trustworthy predictions\nare essential. Existing attribution methods exhibit high inter-method variability and unstable\nexplanations due to the polysemantic nature of LLM representations, while mechanistic inter-\npretability approaches lack direct alignment with model inputs and outputs and do not provide\nexplicit importance scores. We introduce a unified interpretability framework that integrates attri-\nbutional and mechanistic perspectives through monosemantic feature extraction. By constructing\na monosemantic embedding space at the level of an LLM layer and optimizing the framework to\nexplicitly reduce inter-method variability, our approach produces stable input-level importance\nscores and highlights salient features via a decompressed representation of the layer of interest,\nadvancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative\ndisease.\nKeywords Mechanistic Interpretability · explainable AI · Atributional Interpretability · Alzheimer · monosemantic ·\npolysemantic\n1\nIntroduction\nExplainable Artificial Intelligence (XAI) plays a crucial role in building trust in machine learning systems, especially\nin sensitive and high-stakes areas such as finance, climate, and healthcare [1, 2]. In medical settings, interpretability\nis essential for clinical integration and regulatory approval, especially in complex diseases such as Alzheimer’s\nDisease (AD) progression [3], where early and accurate detection can substantially alter treatment results [4].\narXiv:2601.17952v1  [cs.CL]  25 Jan 2026\n"}, {"page": 2, "text": "A PREPRINT - JANUARY 27, 2026\nAlthough machine learning has shown promise in Alzheimer’s progression (AD) diagnostics using multimodal\nclinical data [5], the application of large language models (LLMs) such as GPT-4 and LLaMA-2 in structured\nclinical settings remains limited [6, 7]. A key obstacle is the polysemanticity of internal neural representations—\nindividual neurons or features often encode multiple, semantically unrelated concepts [8, 9, 10]. This entanglement\nundermines the interpretability of standard attribution techniques such as gradients, perturbations, and integrated\npaths, which typically assume one-to-one correspondence between features and meanings. To this end, existing\nattribution methods assign importance scores to input features (e.g., words or tokens), yet they fall short in\naddressing the polysemantic nature of internal representations. This limitation often leads to ambiguous or\nmisleading explanations—particularly problematic in clinical applications, where interpretability is critical [11, 12,\n13, 14]. Moreover, the inter-method variability of attributional techniques is another limitation of dataset-driven\nexplanations, as it reduces trust in the interpretability of AI models [15].\nIn contrast, mechanistic interpretability aims to uncover the internal structure of neural computation by identifying\nsemantically coherent components within the model. Sparse Autoencoders (SAE) have played a pivotal role in\nadvancing our understanding of such representations in both language and vision domains [16]. SAEs aim to\nsolve the superposition problem in neural feature representations by mapping the model’s activations into a more\nmonosemantic latent space, where individual features are better aligned with specific concepts in the network [9].\nHowever, these mechanistic tools typically lack attributional resolution, limiting their utility for explaining how\nspecific inputs contribute to model predictions in real-world, decision-critical scenarios [10].\nThis reveals a critical gap in the current landscape: attributional techniques offer only surface-level explanations\nusing polysemantic features, while mechanistic methods provide structural insights based on monosemantic\nfeature alignment but lack attributional grounding. To date, no unified framework successfully integrates both\nparadigms—particularly in the domain of LLM-based clinical inference.\nTo address these challenges and produce stable, clinically meaningful explanations from large language models,\nwe propose a unified monosemantic attribution framework that integrates both attributional and mechanistic\ninterpretability approaches (Figure 1). Our approach first employs sparse autoencoders (SAEs) to transform\npolysemantic LLM activations into a more monosemantic latent space, where individual features are encouraged to\ncorrespond to disentangled semantic factors. This bottleneck substantially reduces representational complexity\nand enables classical attribution methods to assign more precise and semantically coherent importance scores.\nFor the attribution component, we apply six established techniques—Feature Ablation, Layer Activations, Layer\nConductance, Layer Gradient SHAP [17], Layer Integrated Gradients [18], and Layer Gradient×Activation—both\nin the native activation space (polysemantic features) and in the SAE-induced latent space (more monosemantic\nfeatures).\nTo address the inter-method variability problem [15], we combine the resulting attribution vectors and introduce the\nTransformer Explanation Optimizer (TEO), a learning-based optimization mechanism that selects explanations with\nmaximal alignment to model behavior and dataset-level consistency [15]. We experiment with encoder–decoder\narchitectures based on 1D transformers (TEO) and diffusion networks (DEO). For meta-level assessment and\nvisualization, we embed these optimized attributions into a 2D manifold using UMAP [19] and principal component\nanalysis (PCA) [20], and impose a global coherence constraint by evaluating their linear structure along the primary\nUMAP components. This geometry-aware constraint acts as an additional regularizer, imposing a global linear\nstructure along the dominant statistical directions of the explanation space (the first two principal components\nfrom PCA). By doing so, the principal global feature patterns exhibit similar behavior across the majority of the\ncohort, facilitating the extraction of robust, population-level patterns.\n2\nHypothesis and Evaluation Protocol\nThe central hypothesis of this work is that as the embedding layer approaches a more monosemantic representation,\nattribution scores become more stable, less complex, and more diagnostically informative compared to those derived\nfrom polysemantic features. This hypothesis is evaluated through a series of experiments in which different LLMs\nare trained and evaluated on ADNI [21] (Independent and Identically Distributed; IID) and subsequently tested\non BrainLAT [22] (Out-of-Distribution; OOD) to assess robustness under demographic and protocol shifts. We\nstudy both binary classification (Control vs. Alzheimer’s disease (AD)) and three-class classification settings\n(Control, Early Mild Cognitive Impairment (EMCI), and Late Mild Cognitive Impairment (LMCI) in ADNI; Control,\nFrontotemporal Dementia (FTD), and AD in BrainLAT).\nThe data are partitioned into 80% training and 20% validation within the training portion (which itself constitutes\n80% of the full dataset), with the remaining 20% held out for testing. The last-layer embeddings of the LLM achieving\nthe strongest baseline performance are subsequently used to evaluate the proposed interpretability framework.\n2\n"}, {"page": 3, "text": "A PREPRINT - JANUARY 27, 2026\nLLM\ni-Layer\nSAE\nMonosemantic feature space\nk-Attributional interpretability methods\nencoder\ndecoder\nUMAP constraint\nExplanation optimizer\nn-Optimized attributions\nGlobal pattern discovery\nGenerated Multimodal Clinical Patient Text\nThe participant's sex is Male. Their Date of Birth is 03/1933. Their Year of Birth is 1933.0. Their Handedness is Right. Their Marital status at baseline is Married. Their education in years is 18.0. Participant Retired? \nYes. Type of Participant residence House (owned or rented). Language to be used for testing the Participant English. Participant's Primary Language English. The participant's Ethnicity is Not Hispanic or Latino. Trail \nMaking Test:  Race 5. Information Source Participant Visit. The participant's weight is 164.2. The weight was measured in pounds. The participant's  Systolic - mmHg 118.0. The participant's  Diastolic - mmHg \n67.0. The participant's  Seated Pulse Rate (per minute) is 53.0. The participant's Respirations (per minute) are 20.0. The participant's Temperature is 97.0. The Temperature Source was Oral. The Temperature Units \nwere Fahrenheit. Depressive symptoms present? No. On the Clock Drawing Test the partecipant answered the following questions in this way:  Approximately circular face Correct. Symmetry of number placement \nIncorrect. Correctness of numbers Correct. Presence of the two hands Correct. Presence of the two hands, set to ten after eleven Correct. Clock Drawing Test:  Total Score 4.0. On the Clock copying task the \nparticipant scored as follows:  Approximately circular face Yes. Symmetry of number placement Yes. Correctness of numbers Yes. Presence of the two hands Yes. Presence of the two hands, set to ten after eleven \nYes. Clock copying task:  Total Score 5.0. On the Auditory Verbal Learning Test the participant scored as follows in each trial:  Trial 1 Total 4.0. Total Intrusions 0.0. Trial 2 Total 5.0. Total Intrusions 0.0. Trial 3 Total \n5.0. Total Intrusions 0.0. Trial 4 Total 5.0. Total Intrusions 0.0. Trial 5 Total 5.0. Total Intrusions 1.0. Trial 6 Total 0.0. Total Intrusions 0.0. List B Total 5.0. Total Intrusions 1.0. On the Category Fluency Test Animals \nthe scores were: - Total Correct 12.0. Perseverations 0.0. Intrusions 2.0. Part A - Time to Complete 32.0. Errors of Commission 0.0. Errors of Omission 0.0. Part B - Time to complete 144.0. Errors of Commission 1.0. \nErrors of Omission 0.0. On the Auditory Verbal Learning Test the participant scored as follows: 30 Minute Delay Total 0.0. Total Intrusions 1.0. Recognition Score 8.0. Total Intrusions 5.0. American National Adult \nReading Test:  ANART Total Score (Total # of errors) 19.0. For the Functional Activities Questionnaire the participant scored as follows for each question:  Writing checks, paying bills, or balancing checkbook. Never \ndid, would have difficulty now (1). Assembling tax records, business affairs, or other papers. Never did, would have difficulty now (1). Shopping alone for clothes, household necessities, or groceries. Never did, \nwould have difficulty now (1). Playing a game of skill such as bridge or chess, working on a hobby. Normal (0). Heating water, making a cup of coffee, turing off the stove. Normal (0). Preparing a balanced meal. \nNever did, would have difficulty now (1). Keeping track of current events. Never did, would have difficulty now (1). Paying attention to and understanding a TV program, book, or magazine. Has difficulty, but does by \nself (1). Remembering appointments, family occasions, holidays, medications. Requires assistance (2). Trail Making Test for FAQ score:  Traveling out of the neighborhood, driving, or arranging to take public \ntransportation. Dependent (3). Total Score for FAQ is 11.0\nn-Inputs\n n-Optimized attributions\nFigure 1: Proposed interpretability framework for LLM in Alzheimer’s diagnosis. The model integrates k-attributional\nmethods with a SAE to generate a monosemantic feature space. An explanation optimizer refines attribution outputs,\nenhancing clarity and reducing variability. Global explanation quality is visualized and assessed using UMAP and a\nlinear meta-rule, supporting both individual prediction interpretability and cohort-level pattern discovery.\nImportantly, both the SAE bottleneck architectures (TopK, JumpReLU, and Gated-SAEs) and TEO (with and without\nthe SAE bottleneck) are trained exclusively on IID explanation cohorts using only these training and validation splits\nand are then evaluated on the OOD dataset without any additional training or adjustments, enabling a rigorous\nassessment of out-of-distribution robustness under strict generalization conditions.\n3\nA Monosemantic Attribution Framework\n3.1\nAttributional Theory and Methods\nAttribution explainability methods follow the framework of additive feature attribution, where the explanation\nmodel g(f ,x) is represented as a linear function of simplified input features:\ng(f ,x) = φ0 +\nM\nX\ni=1\nφi xi\n(1)\nHere, f is the predictive model, φi ∈R is the attribution (importance) assigned to feature xi, and M is the number of\nsimplified input features (here 512).\nFor this study, we employed six well-established attributional interpretability methods applied to large language\nmodels (LLMs), denoted as K = 6: Feature Ablation, Layer Activations (which capture the embedding activation\nspace of a specific layer of interest within the LLM), Layer Conduction, Layer Gradient-SHAP [23], Layer Integrated\nGradients [18], and Layer Gradient × Activation (For analytical mathematic formulation see Appendix A.1.). To\nalign these layer-wise interpretability methods with the additive feature attribution framework, we reinterpret the\ninternal activations (i.e., latent units) of a network layer L as simplified input features. The objective is to estimate\nan attribution score φi for each unit, where φi ∈R quantifies the contribution of the corresponding neuron to the\nmodel’s prediction.\nAll attribution methods were applied to the final (22nd) layer of the MODERN-BERT LLM—the model variant that\nachieved the highest classification accuracy in our evaluations (see Supplementary section 1.2). These formulations\nallow us to ground various neural attribution techniques within a unified additive explanation model, facilitating\ntheir comparison and hybridization under shared theoretical assumptions.\n3\n"}, {"page": 4, "text": "A PREPRINT - JANUARY 27, 2026\n3.2\nAttributional Explanation Optimizer Framework\nLet A = {A1, A2,..., AK } denote the set of K = 6 attribution methods applied to the final layer L of the model f .\nEach method Ak generates an attribution vector φ(k) = [φ(k)\n1 ,φ(k)\n2 ,...,φ(k)\nM ], where M is the number of latent features\n(neurons) in layer L. The goal is to derive a unified attribution vector ¯φ that captures the consensus explanation\nacross methods. Each attribution vector φ(k) is evaluated using the Relative Input Stability (RIS), Relative Output\nStability (ROS) [24], and Sparseness [25] metrics (Appendix A.2.). The weighted average attribution vector ¯φ serves\nas the target explanation for the optimization process and it is calculated as:\n¯φ =\nKX\nk=1\nwk ·φ(k)\n(2)\nAn encoder–decoder model is trained to generate a reconstructed explanation ˆφ from the original input x. Two ar-\nchitectures are considered the Diffusion UNet1D [26] (Diffusion Explanation Optimizer, DEO) and the x-transformer\nautoencoder [27, 28] (Transformer Explanation Optimizer, TEO). For the analytical mathematical formulation, see\nAppendix A.2.4. As previously highlighted, the reconstruction of the optimal explanation and the associated cost\nfunction adhere to the same principles and architectural design outlined in [15]. The cost function consists of three\nkey components: sparseness, as defined in [25]; ROS and RIS scores [24]; and similarity. The integration of these\ncomponents ensures a robust and interpretable evaluation. The total cost function for training the reconstruction\nmodel is:\nLtotal(φ(k), ˆφ) =λ1 ·\n1\nMRIS(f , ˆφ)\n+λ2 ·\n1\nMROS(f , ˆφ)\n+λ3 · Msparse(f , ˆφ)+λ4 ·Lsimilarity( ˆφ, ¯φ)\n(3)\nwhere:λ1,λ2,λ3,λ4 are hyperparameters controlling the influence of each loss term. This formulation enables a prin-\ncipled and quantitative integration of multiple attribution methods, optimizing toward a robust and interpretable\nexplanation.\n3.3\nUMAP Linear Constraint\nTo obtain a unified and comparable low-dimensional representation of attribution scores across all tokenizer\nfeatures, we apply a feature-wise UMAP projection to the normalized attribution tensor of shape RM×T , where\nM is the number of test samples and T denotes the dimensionality of the tokenizer embedding space. For each\nfeature j, the attribution vector x(j) ∈RM is min–max normalized and embedded using a one-dimensional UMAP\ntransformation into a two-dimensional space y(j) ∈RM×2. The resulting coordinates are normalized to [0,1] so\nthat all feature-wise embeddings share a common bounded range. This nonlinear projection preserves the local\nneighborhood structure of each attribution distribution while mapping all T tokenizer features into an aligned and\ndirectly comparable representation space across attribution methods (Appendix A.4).\nLinear Constraint on UMAP Embeddings: To encourage structural consistency in the UMAP [19] embeddings, we\nintroduce a linear constraint requiring equality between the first and second embedding components for each point,\nexpressed as ui1 = ui2. This can be written equivalently as ui1 −ui2 = 0 for all i. We incorporate this constraint into\nthe overall optimization objective via a penalty term,\nλ5\nnX\ni=1\n(ui1 −ui2)2,\nwhere λ5 controls the strength of the constraint. This formulation enforces consistency of the reconstructed\nembeddings while preserving flexibility in the learned attribution representation.\n3.4\nThe SAE Approach and Architectures\nThe mathematical formulation situates SAE architectures within the theoretical framework of superposition and\nsemantic disentanglement (for an analytical mathematical formulation, see Appendix A.5). By expressing hidden\nstates as sparse linear combinations of interpretable features, SAEs bridge the gap between low-level activations\nand human-understandable concepts. Let x ∈Rd denote a layer’s neuron activation vector in a pretrained model. A\nSparse Autoencoder learns a sparse feature representation a ∈RF such that:\nˆx = W a+b,\n(4)\n4\n"}, {"page": 5, "text": "A PREPRINT - JANUARY 27, 2026\nwhere W ∈Rd×F is the decoder (dictionary) matrix and b ∈Rd is a learned bias term. Each column W·,i represents\nthe direction of feature i in neuron space, and ai is its activation. This linear mapping enables complex activations\nto be expressed as combinations of more interpretable features. If F > d, then the feature space is overcomplete,\nand W cannot be full-rank. This leads to superposition, where multiple features overlap in the same subspace,\nand individual neurons encode multiple unrelated concepts [29]. If W is invertible and aligned to a basis, each\nneuron corresponds to a single feature. The representation is monosemantic and disentangled [8]. When W has\noverlapping columns, neurons can respond to multiple features, yielding polysemantic behavior. That is, for some\nj, x j = P\ni Wj,i ai involves multiple nonzero terms [30]. Variants of SAEs like TopK, JumpReLU, and Gated-SAEs offer\nincreasingly precise control over the mapping between low-level activations and human-understandable concepts,\nenabling fine-grained analysis and intervention (analytical mathematical formulation, see Appendix A.6.).\n3.5\nAttribution from Sparse Feature Space to Input Tokens\nLet xinput ∈Rdinput denote the input embedding vector (e.g., LLM token embeddings), x = f (xinput) ∈Rd the hidden\nlayer activation of the LLM, a = Encoder(x) ∈RF the SAE sparse feature vector, and ˆx = W a+b the reconstructed\nactivation from the SAE decoder. Now suppose we have a sparse attribution vector ψi over features a, i.e., ψ ∈RF ,\nwhere each ψi reflects the importance of SAE feature ai. We aim to assign importance Φk to each input token\ndimension xinput,k.\nWe propagate the feature attributions backward through the encoder to the input. Using the chain rule:\nΦk =\nFX\ni=1\nψi ·\n∂ai\n∂xinput,k\n=\nFX\ni=1\nψi · ∂ai\n∂x ·\n∂x\n∂xinput,k\n(5)\nwhere ∂ai\n∂x is the encoder Jacobian (SAE layer), and\n∂x\n∂xinput,k is the LLM gradient from input token to hidden layer. This\ngives us a scalar attribution Φk ∈R for each token/input embedding dimension k. This represents how much each\ninput token contributes to the sparse SAE features identified as important. In doing so, we assess the contribution\nof input features based on the monosemantic behavior of the trained network’s internal mechanisms. Building on\nour study, we apply the six previously discussed attribution methods at two levels: from the SAE feature space to the\nencoder layer, and from the encoder layer to the input embedding space. This dual-level attribution analysis enables\nus to investigate how interpretable sparse features relate to model internals and ultimately shape the input-level\nrepresentations. Attribution methods (e.g., Integrated Gradients, SHAP) can directly estimate:\nφinput = AttributionMethod(f ,xinput,φSAE)\n(6)\nwhere φSAE denotes the monosemantic feature space of the SAE network. Thus, the dual-level approach allows us\nto connect semantically meaningful sparse features to the raw input representation space.\n3.6\nLLM Networks and Hyperparameter Tuning\nWe evaluate encoder-based LLMs (BERT, RoBERTa, DistilBERT, ALBERT, BioBERT, ModernBERT) on ADNI (IID)\nand BRAINLAT (OOD) under a unified protocol spanning full fine-tuning, zero-shot, few-shot with temperature\ncontrol, and parameter-efficient LoRA. ModernBERT outperformed all other networks on ADNI: in the binary\ntask it achieved the highest F1 (75.89%), AUC-PR (86.41%), ROC-AUC (83.95%), and Accuracy (72.37%), and it\nremained strongest in the three-class setting (F1 68.80%, AUC-PR 78.48%, ROC-AUC 78.67%, Accuracy 65.05%).\nFor OOD model adaptation, ModernBERT zero-shot yielded 55% Accuracy, few-shot/LoRA provided modest gains\n(62%), while full fine-tuning peaked at ∼84% Accuracy but lies outside our scope (Supplementary 1.2). Accordingly,\nwe use ModernBERT fine-tuned on ADNI for IID and zero-shot on BRAINLAT for OOD, and all interpretability\nanalyses are conducted on the 22nd layer of ModernBERT. We conducted extensive hyperparameter tuning for all\ncomponents. The explanation optimizer performed best at a learning rate of 2e-4, with the optimal weight config-\nuration (λ1,λ2,λ3,λ4) = (0.1,0.3,0.1,0.5). UMAP constraints were most effective at the 4× batch size level (4×64)\n(Supplementary 1.3; Figures 3). Among the four SAE variants, TopK produced the strongest overall performance\n(Supplementary 1.3; Figures 2, 4, 5), using a 32× feature depth. All models were trained using the AdamW optimizer\nwith early stopping and standard evaluation metrics (Supplementary 1.3).\n3.7\nDataset and Code Availability\nThe dataset used in this study originates from the ADNI cohort [21] and is represented as text generated from\nmultiple modalities, serving as the in-distribution (IID) dataset. We further split the generated text into nine\n5\n"}, {"page": 6, "text": "A PREPRINT - JANUARY 27, 2026\nTable 1: Unified attribution performance across tasks and evaluation settings. Values report mean±std per class.\nBinary tasks use (A/C); three-class tasks use (L/M/C) and (A/F/C). Bold indicates the best performance within each\nsetting.\nTask & Setting\nMethod (variant)\nSparseness\nRIS\nROS\nBinary — IID (ADNI)\nColumns: (A/C)\nGradient Activation\n0.3277±0.0384/0.2500±0.0230\n5.6149±0.0193/5.6170±0.0221\n16.9303±0.0034/16.9347±0.0\nGradient Activation-SAE\n0.2035±0.0117/0.1668±0.0072\n5.6252±0.0213/5.6173±0.0221\n16.9343±6.8e −5/16.9347±4.0e −5\nDEO\n0.3383±0.0033/0.3377±0.0017\n9.2839±0.0800/9.3131±0.1427\n20.6342±0.0866/20.6159±0.2026\nDEO-SAE\n0.3374±0.0029/0.3140±0.0010\n9.2790±0.0646/9.1750±0.1088\n20.6150±0.0880/20.5150±0.1299\nTEO\n0.4220±0.0003/0.4199±0.0005\n5.0520±0.0192/5.0688±0.0184\n16.3529±0.0056/16.3777±0.0011\nTEO-SAE\n0.2672±0.0010/0.2682±0.0007\n1.6227±0.1708/0.9964±0.2639\n12.9250±0.1703/12.2983±0.2613\nTEO-UMAP (SAE)\n0.3989±0.0004/0.4057±0.0003\n5.4394±0.0332/5.4709±0.1746\n16.3037±0.0033/16.2102±0.0079\nBinary — OOD (BrainLAT)\nColumns: (A/C)\nGradient Activation-SAE\n0.1140±0.0177/0.0630±0.0069\n6.0328±0.0277/6.0339±0.0398\n16.9347±3.6e −6/16.9348±3.7e −5\nTEO-SAE\n0.2691±0.0016/0.2725±0.0004\n0.6835±0.6676/0.4734±0.2801\n11.5236±0.6591/11.2130±0.5150\nTEO-UMAP (SAE)\n0.3989±0.0005/0.4043±0.0029\n5.4394±0.0383/5.4282±0.1944\n16.3037±0.0039/16.1577±0.1054\nThree-class — IID (ADNI)\nColumns: (L/M/C)\nGradient Activation\n0.3839±0.0177/0.2917±0.0200/0.2697±0.0061\n5.6272±0.0212/5.6269±0.0193/5.6290±0.0225\n16.9339±0.0008/16.9340±0.0006/16.9347±0\nGradient Activation-SAE\n0.4310±0.1156/0.2579±0.1095/0.2296±0.0036\n5.6297±0.9478/5.6172±1.1684/5.6210±0.0194\n16.9347±2.8625/16.9347±3.5241/16.9348±1.22e −5\nTEO\n0.4131±0.0003/0.3909±0.0047/0.3918±0.0008\n5.0938±0.0188/4.8283±0.0377/4.8080±0.0184\n16.4043±0.0024/16.1354±0.0324/16.1172±0.0090\nTEO-SAE\n0.2860±0.0374/0.2838±0.0523/0.2682±0.0649\n2.2642±0.4877/2.1617±0.4547/1.5468±0.1171\n13.5646±2.2745/13.4676±2.7641/12.8570±0.1179\nTEO-UMAP (SAE)\n0.4161±0.0870/0.4172±0.2372/0.3973±0.0749\n5.1017±0.1697/5.1116±0.1072/5.1086±0.2083\n16.4031±3.8616/16.4088±0.4924/16.4123±6.8439\nThree-class — OOD (BrainLAT)\nColumns: (A/F/C)\nGradient Activation-SAE\n0.1836±0.0016/0.4303±0.0022/0.1772±0.0112\n6.0269±0.0302/6.1450±0.1210/6.1234±0.1912\n16.9348±2.6e −6/16.9345±2.8e −5/16.9346±1.5e −5\nTEO-SAE\n0.3716±0.0009/0.4224±0.0002/0.4162±0.0029\n4.9396±0.0148/5.5421±0.0611/5.7520±0.3645\n15.8121±0.0099/16.2773±0.0010/16.3792±0.0034\nTEO-UMAP (SAE)\n0.4239±5.0e −5/0.4246±2.4e −4/0.4238±5.6e −4\n5.4583±0.0297/5.5576±0.0954/5.5525±0.1862\n16.3726±0.0017/16.3572±0.0035/16.3661±0.0073\nsubgroups based on input modality, as detailed in Supplementary Material 1.1.5, for pattern analysis and biomarker\nresearch purposes. For the out-of-distribution (OOD) cohort, we used text generated from multimodal sources (MRI\nand clinical files) in the Latin American Brain Health Institute (BrainLat) dataset, a multi-site initiative providing\nneuroimaging, cognitive, and clinical data across several Latin American countries [22]. Additional demographic\nand preprocessing details are provided in Supplementary Sections 1.1.1–1.1.5. The code is implemented in Python\nusing PyTorch and runs on an NVIDIA cluster in one A100-SXM-80GB GPU. It leverages SAE_LENs [31] for SAE\ntraining, quantus [32] for evaluation, and captum for attribution analysis.\n4\nResults of Experiments\n4.1\nAblation Analysis of the Monosemantic Attribution Framework\nIn this study, sparseness is defined such that higher values correspond to more selective and concentrated at-\ntributions across input features—that is, greater sparseness. However, sparseness alone is insufficient to assess\nexplanation quality, as it does not account for robustness or stability. Therefore, the most effective explanation\nmethod is one that simultaneously achieves high sparseness and low RIS and ROS values.\nAcross IID, OOD datasets and for both binary and three-class classification tasks, Table 1 shows a consistent stability–\nsparsity frontier governed by the proposed optimizers. In the binary IID case, SAE substantially improves stability\nfor feature-learning explainers, most notably: Layer Conductance and especially TEO, with large drops in RIS/ROS\nfor both Alzheimer’s and Control, whereas Activation–SAE increases RIS/ROS relative to its no-SAE variant and is\ntherefore less robust. In the binary OOD case, this pattern persists and strengthens: TEO–SAE achieves the lowest\nRIS/ROS overall (strong cross-dataset stability), while TEO–UMAP recovers higher sparsity (> 0.40) at a modest\nstability cost versus TEO–SAE, offering a tunable sparsity–stability trade-off. In the three-class IID setting, Feature\nAblation is the sparsity leader across Control/LMCI/MCI (∼0.52–0.53) with moderate, steady yet still high RIS/ROS\nvalues; Layer Conductance–SAE markedly reduces RIS/ROS for LMCI/MCI; and TEO–SAE again delivers the most\nstable attributions across all classes, albeit with reduced sparsity compared with no-SAE variants. The same rank\nordering holds OOD. Across all blocks, gradient-formulaic methods (Grad-SHAP, Guided Backprop, Integrated\nGradients) show near-invariant RIS/ROS (∼5.6/ ∼16.93) regardless of SAE, class, or domain, indicating that SAE\nchiefly benefits learned-attribution methods. Further analyses are provided in Supplementary §1.4, Tables 2–5.\nTo validate that the proposed UMAP linear constraint effectively linearizes the attribution space and yields robust\nscores within tokenized features, we performed an additional PCA analysis. We extracted the top eight principal\ncomponents and visualized the first two, which capture the majority of variance in the cohort distribution. Under\nthe UMAP constraint, these components reveal an approximately linear structure along the dominant statistical\n6\n"}, {"page": 7, "text": "A PREPRINT - JANUARY 27, 2026\nLayer Conduction SAE\nFeature Ablation SAE\nGradient SHAP SAE\nGradient Activation SAE\nIntegrated Gradient SAE\nActivation SAE\nTEO-SAE\nTEO-UMAP (SAE)\nLMCI class of ADNI test cohort\nFigure 2: PCA of token-level attribution representations (LMCI, ADNI test set). The first two principal components\nare shown, computed from the top eight PCA components of the attribution matrix.\ndirections (Figure 2), inducing a coherent global organization of the feature space and promoting consistent\npopulation-level attribution patterns across subjects (Appendix B.3).\n4.2\nIndividual- and Cohort-Level Quality of Explanations\nFigure 3 presents qualitative local-level and cohort-level attributions for the LMCI class in the three-class classifica-\ntion task on both IID and OOD settings using our proposed optimisers TEO, TEO-SAE, and TEO-UMAP (SAE). At the\nlocal level, the heatmaps illustrate feature importance, with tokens colour-coded to indicate relevance (green = posi-\ntive relevance; red = negative relevance). Visually, TEO-SAE produces the tightest, least noisy explanations—fewer\nspurious highlights and clearer token groupings—consistent with its lowest RIS/ROS in Table 1. Adding the UMAP\nconstraint restores higher sparseness while preserving much of TEO-SAE stability: TEO-UMAP (SAE) yields compact,\nwell-separated patterns that remain clinically interpretable across IID and OOD. Across classes and datasets, higher\nSparseness corresponds to less diffuse maps with balanced positive/negative highlights, whereas low Sparseness\nwith high RIS/ROS manifests as saturated red/green patches and unstable saliency (see Supplementary Figures\n7–16). Among the six classical attribution methods (Activation, Layer Conductance, Feature Ablation, Gradient\nSHAP, Gradient Activation, Integrated Gradient), Feature Ablation attains the highest Sparseness but exhibits poorer\nstability (elevated RIS/ROS), a gap that worsens with SAE due to decoder-driven “decompression” (Supplementary\nFigures 7–8). Layer Conductance shows the opposite trade-off: SAE reduces Sparseness but improves stability (lower\nRIS/ROS), with similar stability gains observed for Gradient Activation, Integrated Gradient, and Gradient SHAP\n(Supplementary Figures 9–10). Overall, none of these classical methods match the proposed framework as TEO-SAE\nis consistently most stable, and TEO-UMAP (SAE) offers a tunable sparsity–stability compromise that generalises\nfrom ADNI to BrainLat (for extended analyses see Supplementary §1.6).\nAt the cohort level, we extracted UMAP embeddings to visualise patterns and text–category clusters in 2D space,\nobserving any spreading effects or homogeneous clustering. We also applied PCA to identify high-contributing\nfeatures (threshold 0.6 on the first component; Figure 3). Moving from TEO to TEO–SAE produces tighter, more\nhomogeneous low-to-high attribution and the lowest RIS/ROS (highest stability), but also a marked reduction in\nsparseness, evident as broader token spread in the 2D manifold (see Figure 3). In some cases, this stabilisation\nconcentrates signals so strongly that few features exceed the significance threshold (square box in the 1D scatter\nplot where PCA first component ≥0.6), and not all subgroups are represented (Figure 3, Supplementary Figure\n32). Imposing a linear UMAP constraint (TEO–UMAP) mitigates this effect by restoring sparsity in significant\nattributions while retaining stability, yielding compact, clinically interpretable maps with more uniform subgroup\ncoverage (Figure 3; Supplementary Figures 33–35). The behaviour of the proposed framework (TEO, TEO–SAE,\n7\n"}, {"page": 8, "text": "A PREPRINT - JANUARY 27, 2026\nTEO\nTEO-SAE\n60% threshold of \nfeature attribution \n512 tokens inputs\nTEO-UMAP (SAE)\nADNI test cohort\nLate MCI class\nTEO\nTEO-UMAP (SAE)\nTEO-SAE\n0.25\n0.27\n0.29\n0.31\n0.33\n0.35\n0.37\n0.39\n0.41\n0.43\n0.45\nSparseness\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nExplanation Satbility (RIS/ROS)\nROS\nRIS\nPCA component 1\nPCA component 1\nPCA component 1\nFigure 3: Left: Stability–sparsity frontier for explanation optimizers on ADNI (Late MCI) in the testing cohort. Scatter\npoints show TEO, TEO–SAE, and TEO–UMAP (SAE) on ADNI (IID) and BrainLat (OOD). Metrics: Sparseness (higher\nis better) vs. RIS/ROS stability (lower is better). Middle: Token-level heatmap produced by the proposed framework,\nwith feature-attribution scale (green: positive relevance; red: negative relevance; white: neutral). Right: 1st PCA\nand 2D UMAP projections of the full testing cohort. Thresholding uses 60% feature attribution over 512 tokens.\nThe generated input text is split into nine subgroups (different colours) based on input modality, as detailed in\nSupplementary Material 1.1.5, for pattern analysis and biomarker identification.\nTEO–UMAP) shows that higher sparseness corresponds to less diffuse, more balanced highlights, whereas lower\nsparseness with higher RIS/ROS results in saturated red/green patches. This mirrors the patterns observed across\nthe six classical methods (Activation, Layer Conduction, Feature Ablation, Gradient SHAP, Gradient Activation,\nIntegrated Gradient; Apendix A.2). With SAE, feature-learning explainers such as Layer Conduction generally gain\nstability (lower RIS/ROS) at some sparsity cost, while Feature Ablation maintains high sparsity but remains unstable.\nNone, however, match the stability–sparsity trade-off achieved by TEO–SAE and TEO–UMAP (box plots in Figure 3;\nTable 1). Supplementary §1.7 (Figures 22–31) provides more details about cohort-level attributions.\n4.3\nAre Monosemantic Representation–Based Attribution Methods Statistically Distinct from Standard\nAttribution Techniques?\nA statistical evaluation of interpretability metrics (sparseness, RIS, and ROS, see Supplementary §1.5) across methods\nwith and without the SAE layer was computed. In the binary ADNI task, paired t-tests with FDR correction showed\nthat adding an SAE bottleneck significantly reduced Complexity (p < 10−10) and RIS (p < 10−4) in both groups, while\nROS changes were small and inconsistent (marginal for Control, non-significant for Alzheimer’s). The strongest\nSAE effects appeared in attribution metrics, with Gradient SHAP (p < 10−45), Layer Conduction (p = 3.2×10−7),\nIntegrated Gradients (p < 10−55), and the TEO (p < 10−95) all showing decisive reductions, confirming robust\nstability gains under SAE. In the three-class ADNI task, paired t-tests and Wilcoxon signed-rank tests (BH-FDR)\nindicated that the MCI group showed the clearest improvement: ROS decreased strongly (t(17) = −10.12, p =\n1.30×10−8; W = 0, p = 8.0×10−6, q = 2.3×10−5), RIS showed a smaller reduction detected non-parametrically\n(W = 19, p = 0.00117), and Complexity increased modestly by Wilcoxon (W = 17, p = 7.9 × 10−4) while paired\nt-tests were non-significant. Control and LMCI had incomplete pairs, preventing matched testing with correction.\nOverall, SAE reliably improves attribution stability (lower RIS/ROS) and increase sparseness in binary tasks, with the\nthree-class MCI group showing the most consistent ROS gains.\n8\n"}, {"page": 9, "text": "A PREPRINT - JANUARY 27, 2026\n4.4\nClinical Impact and Diagnostic Outcomes in Alzheimer’s Disease\nBiomarker Identification: Our findings demonstrate that both TEO-SAE and TEO-UMAP yield the most reliable\nand consistent identification of informative sources across the nine multimodal subgroups. Using a threshold of\n0.6 on the first principal component (PCA; Figure 3), we observe systematic and class-specific biomarker patterns\nin the IID binary task (ADNI). For the Control group, TEO-SAE is primarily driven by FAQ, whereas TEO-UMAP\nshifts emphasis toward DEM, AVLT2, and FAQ. For the Alzheimer’s group, TEO prioritises FAQ, AVLT1, and CFA,\nwhile TEO-UMAP highlights ANART, FAQ, and DEM. In the three-class setting, TEO identifies AVLT1, CDT, and\nANART as most influential for Controls, while TEO-UMAP selects AVLT1, CDT, and CFA. For MCI, TEO assigns\nhighest relevance to CCT, AVLT2, and FAQ, whereas TEO-UMAP favours AVLT2, ANART, and CFA. For LMCI, TEO\nelevates AVLT1, FAQ, and CDT, while TEO-UMAP elevates FAQ, ANART, and AVLT2. These trends are summarised in\nAppendix Table 2 (Section B.2), with acronym definitions provided in Sections 1.1.5 (Table 1).\nImportantly, these results illustrate how our proposed mechanistic attribution framework can disentangle which\ndemographic/vital features and, crucially, which cognitive assessments contribute most strongly to each diagnostic\ncategory. This is particularly relevant in clinical neuroscience, where cognitive tests are often time-consuming,\nresource-intensive, and susceptible to demographic or cultural bias. Our framework provides a principled approach\nfor identifying the most informative cognitive biomarkers-such as AVLT, FAQ, CDT, and ANART-which have been\nrepeatedly linked to early Alzheimer’s progression in the literature [33], [34]. By pinpointing the minimal set of\nhigh-yield assessments for each classification task, the method supports more efficient screening pipelines, reduces\nclinical burden, and enables scalable application in larger cohorts where repeated or comprehensive testing is\nimpractical.\n5\nExternal Evaluation of Clinically Coherent Explanations\nSetup. To assess the clinical relevance of our attribution framework and, in particular, the contribution of the SAE\nlayer in producing monosemantic and diagnostically coherent explanations, we performed an auxiliary evaluation\nacross three cohorts (Binary ADNI, Binary BrainLAT, and three-class ADNI). For each test sample, we extracted the\ntop 50% most influential token attributions from TEO without SAE, TEO with SAE, and TEO-UMAP, and constructed\nclass-specific CSV files in which highlighted characters for each attribution method were arranged column-wise,\nwith the full character sequence (including the CLS token) provided in the first column to ensure clear sample-\nlevel distinction. These CSVs were then presented to a large language model (ChatGPT-5.1 [35]) under a fixed\nprompting protocol to yield an external, model-agnostic assessment of the interpretability structure encoded by\neach explanation space.\nResults. Across both binary tasks, all three attribution methods correctly identified the pathological Alzheimer’s\nclass; however, TEO without SAE consistently fixated on task artefacts (e.g., instruction counts or label-like patterns)\nrather than clinically meaningful biomarkers, whereas TEO-SAE and TEO-UMAP surfaced coherent neurocognitive\nindicators, demographic risk factors, and processing-speed impairments. In the more challenging three-class ADNI\nsetting, the SAE-enabled variants again produced clearer diagnostic separation and more structured biomarker\nprofiles, while TEO without SAE failed to identify pathology or highlight relevant clinical features. These results\nshow that using an SAE-driven monosemantic representation substantially strengthens the reliability, diagnostic\nvalidity, and clinical interpretability of the resulting attribution signals (see Appendix B.1.; Figure 4).\n6\nConclusion\nWe introduced a unified interpretability framework that combines monosemantic feature extraction with learning-\nbased explanation optimization, optionally augmented with a geometry-aware constraint. Across IID and OOD\nsettings and multiple classification tasks, the proposed methods yield more stable and robust explanations than clas-\nsical attribution approaches, defining a tunable sparsity–stability trade-off that generalizes under distribution shift.\nClinically, the framework identifies coherent diagnostic markers and enables low-dimensional cohort-level explana-\ntions that support efficient assessment. Overall, this approach advances robust and trustworthy interpretability for\nLLM-based Alzheimer’s disease analysis.\nImpact Statement\nThis paper presents work whose goal is to advance the field of machine learning. There are many potential societal\nconsequences of our work, none of which we feel must be specifically highlighted here. The paper discusses several\npotential positive societal impacts, particularly emphasizing its relevance to clinical applications such as the early\n9\n"}, {"page": 10, "text": "A PREPRINT - JANUARY 27, 2026\ndiagnosis and treatment planning of Alzheimer’s Disease. By proposing a unified interpretability framework that\ncombines attributional and mechanistic techniques, the authors aim to enhance the trustworthiness, consistency,\nand human alignment of large language model (LLM) outputs. This improved interpretability is presented as a\nmeans to support safer and more effective integration of LLMs into cognitive health and clinical decision-making,\nwith the potential to uncover clinically meaningful patterns and ultimately improve patient outcomes. However, the\npaper does not explicitly address possible negative societal impacts of the work. It does not discuss risks such as the\nmisinterpretation of model ex-677 planations, over-reliance on machine-generated insights in high-stakes medical\ncontexts, or the potential for the framework to inadvertently reinforce biases embedded in training data. Societal\nimpacts can be better established through future work, in which we plan to incorporate clinician-in-the-loop\nevaluation and patients.\nAcknowledgments\nThis acknowledgment complies with the dataset license and does not affect the anonymization of the authors. Data\ncollection and sharing for the Alzheimer’s Disease Neuroimaging Initiative (ADNI) is funded by the National Institute\non Aging (National Institutes of Health Grant U19AG024904). The grantee organization is the àNorthern California\nInstitute for Research and Education. Past funding was obtained from: the National Institute of Biomedical Imaging\nand Bioengineering, the Canadian Institutes of Health Research, and private sector contributions through the\nFoundation for the National Institutes of Health (FNIH) including generous contributions from the following:\nAbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen;\nBristolMyers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company;\nEuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO\nLtd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical\nResearch & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx\nResearch; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier;\nTakeda Pharmaceutical Company; and Transition Therapeutics.\nReferences\n[1] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv preprint\narXiv:1702.08608, 2017.\n[2] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, and Jiliang Tang.\nTrustworthy ai: A computational perspective. ACM Trans. Intell. Syst. Technol., 14(1), November 2022.\n[3] Alireza A. Tahami Monfared, Melissa J. Byrnes, Lauren A. White, Jianwei Zhang, Eric Yu, and Jeng Lin.\nAlzheimer’s disease: Epidemiology and clinical progression. Neurology and Therapy, 11:553–569, 2022.\n[4] Clifford R Jack et al.\nNia-aa research framework: Toward a biological definition of alzheimer’s disease.\nAlzheimer’s & Dementia, 14(4):535–562, 2018.\n[5] Garam Lee, Kwangsik Nho, Byungkon Kang, Kyung-Ah Sohn, Dokyoon Kim, Michael W. Weiner, and Alzheimer’s\nDisease Neuroimaging Initiative. Predicting alzheimer’s disease progression using multi-modal deep learning\napproach. Scientific Reports, 9(1):1952, 2019.\n[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in\nneural information processing systems, volume 33, pages 1877–1901, 2020.\n[7] Hugo Touvron et al. Llama 2: Open foundation and fine-tuned chat models. https://ai.meta.com/llama/,\n2023.\n[8] Chris Olah, Arvind Satyanarayan, Ludwig Schubert Wusser, and Shan Carter. Zoom in: An introduction to\ncircuits. Distill, 2020.\n[9] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models, 2023.\n[10] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-\nDodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei,\nMartin Wattenberg, and Christopher Olah. Toy models of superposition, 2022.\n[11] Wojciech Samek, Grégoire Montavon, Sebastian Lapuschkin, Christopher J. Anders, and Klaus-Robert Müller.\nExplaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE,\n109(3):247–278, 2021.\n10\n"}, {"page": 11, "text": "A PREPRINT - JANUARY 27, 2026\n[12] Bas H.M. van der Velden, Hugo J. Kuijf, Kenneth G.A. Gilhuijs, and Max A. Viergever. Explainable artificial\nintelligence (xai) in deep learning-based medical image analysis. Medical Image Analysis, 79:102470, 2022.\n[13] Gwenolé Quellec, Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze, Pascale Massin, and Béatrice Coch-\nener. Explain: Explanatory artificial intelligence for diabetic retinopathy diagnosis. Medical Image Analysis,\n72:102118, 2021.\n[14] Michail Mamalakis, Krit Dwivedi, Michael Sharkey, Samer Alabed, David Kiely, and Andrew J. Swift. A trans-\nparent artificial intelligence framework to assess lung disease in pulmonary hypertension. Scientific Reports,\n13(1):3812, 2023.\n[15] Michail Mamalakis, Antonios Mamalakis, Ingrid Agartz, Lynn Egeland Mørch-Johnsen, Graham K. Murray, John\nSuckling, and Pietro Lio. Solving the enigma: Enhancing faithfulness and comprehensibility in explanations of\ndeep networks. AI Open, 6:70–81, 2025.\n[16] Liv Gorton. The missing curve detectors of inceptionv1: Applying sparse autoencoders to inceptionv1 early\nvision. arXiv preprint arXiv:2406.03662, 2024.\n[17] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural\ninformation processing systems, 30, 2017.\n[18] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks, 2017.\n[19] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for\ndimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n[20] Karl Pearson F.R.S. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh,\nand Dublin Philosophical Magazine and Journal of Science, 2(11):559–572, 1901.\n[21] Susanne G. Mueller, Michael W. Weiner, Leon J. Thal, Ronald C. Petersen, Clifford Jack, William Jagust, John Q.\nTrojanowski, Arthur W. Toga, and Laurel Beckett. The alzheimer’s disease neuroimaging initiative. Neuroimag-\ning Clinics of North America, 15(4):869–877, 2005. Alzheimer’s Disease: 100 Years of Progress.\n[22] Pavel Prado, Vicente Medel, Agustín Sainz-Ballesteros, Hernando Santamaría-García, Sebastián Moguilner,\nJhony Mejía, Raúl González-Gómez, Andrea Slachevsky, María Isabel Behrens, David Aguillón, Francisco Lopera,\nMario A. Parra, Diana Matallana, Marcelo Adrián Maito, Adolfo M. García, Nilton Custodio, Alberto Ávila Funes,\nStefanie Piña-Escudero, Agustina Birba, Sol Fittipaldi, Agustina Legaz, and Agustín Ibáñez. The brainlat project:\na multimodal neuroimaging dataset of neurodegeneration from underrepresented backgrounds. Scientific\nData, 10:889, 2023.\n[23] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. CoRR, abs/1705.07874,\n2017.\n[24] Chirag Agarwal, Nari Johnson, Martin Pawelczyk, Satyapriya Krishna, Eshika Saxena, Marinka Zitnik, and\nHimabindu Lakkaraju. Rethinking stability for attribution-based explanations, 2022.\n[25] Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Xi Wu, and Somesh Jha. Concise explanations of\nneural networks using adversarial training. In Hal Daumé III and Aarti Singh, editors, Proceedings of the\n37th International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning\nResearch, pages 1383–1391, Virtual Event, online, July 13–18 2020. PMLR. Originally released as arXiv:1810.06583\n(2018).\n[26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention\n(MICCAI), pages 234–241. Springer, 2015.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need, 2017.\n[28] Toan Q. Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention.\narxiv, 2019.\n[29] Nelson Elhage, Neel Nanda, et al. A mechanistic interpretability analysis of superposition in neural networks.\nTransformer Circuits Thread, 2022.\n[30] Wes Gurnee and Max Tegmark. Language models represent space and time, 2024.\n[31] Joseph Bloom, Curt Tigges, Anthony Duong, and David Chanin. Saelens. https://github.com/jbloomAus/\nSAELens, 2024.\n11\n"}, {"page": 12, "text": "A PREPRINT - JANUARY 27, 2026\n[32] Anna Hedström, Leander Weber, Daniel Krakowczyk, Dilyara Bareeva, Franz Motzkus, Wojciech Samek, Se-\nbastian Lapuschkin, and Marina Marina M.-C. Höhne. Quantus: An explainable ai toolkit for responsible\nevaluation of neural network explanations and beyond. Journal of Machine Learning Research, 24(34):1–11,\n2023.\n[33] Ronald C. Petersen, Glenn E. Smith, Susan C. Waring, Robert J. Ivnik, Eric G. Tangalos, and Emre Kokmen. Mild\ncognitive impairment: clinical characterization and outcome. Archives of Neurology, 56(3):303–308, 1999.\n[34] Mark W. Bondi, Emily C. Edmonds, Amy J. Jak, Lindsay R. Clark, Lisa Delano-Wood, Carrie R. McDonald,\nDaniel A. Nation, David J. Libon, Rhoda Au, Douglas Galasko, and David P. Salmon and. Neuropsychological\ncriteria for mild cognitive impairment improves diagnostic precision, biomarker associations, and progression\nrates. Journal of Alzheimer’s Disease, 42(1):275–289, 2014. PMID: 24844687.\n[35] OpenAI. ChatGPT-4o. https://openai.com/chatgpt, 2024. Accessed May 2025.\n[36] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\narXiv preprint\narxiv:2006.11239, 2020.\n[37] Rodrigo Quiroga et al.\nInvariant visual representation by single neurons in the human brain.\nNature,\n435(7045):1102–1107, 2005.\n[38] Mattia Rigotti, Omri Barak, Melissa Warden, et al. The importance of mixed selectivity in complex cognitive\ntasks. Nature, 497(7451):585–590, 2013.\n[39] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An\nintroduction to circuits. Distill, 5(3):e00024.001, 2020.\n[40] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature cir-\ncuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647,\n2024.\n[41] Callum McDougall. Sae visualizer, 2024.\n[42] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem\nAnil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with\ndictionary learning. Transformer Circuits Thread, 2023.\n[43] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff\nWu, and William Saunders. Language models can explain neurons in language models. openaipublic, 2023.\n[44] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce,\nCraig Citro, Emmanuel Ameisen, Andy Jones, et al. Scaling monosemanticity: Extracting interpretable features\nfrom claude 3. Transformer Circuits Thread, 2024.\n[45] Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár, Rohin\nShah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders, 2024.\n[46] Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safety – a review, 2024.\n[47] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should i trust you?\": Explaining the predictions\nof any classifier, 2016.\n[48] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Woj-\nciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.\nPLOS ONE, 10(7):1–46, 07 2015.\n[49] Amitojdeep Singh, Sourya Sengupta, and Vasudevan Lakshminarayanan. Explainable deep learning models in\nmedical image analysis, 2020.\n[50] Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, and Eyke Hüllermeier. Approximating the shapley\nvalue without marginal contributions, 2024.\n[51] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-\nDodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei,\nMartin Wattenberg, and Christopher Olah. Toy models of superposition, 2022.\n[52] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. Transformer Circuits\nThread, 2022.\n12\n"}, {"page": 13, "text": "A PREPRINT - JANUARY 27, 2026\nA\nTechnical Appendices\nA.1\nAttributional theory and methods\nAttribution explainability methods follow the framework of additive feature attribution, where the explanation\nmodel g(f ,x) is represented as a linear function of simplified input features:\ng(f ,x) = φ0 +\nM\nX\ni=1\nφi xi\n(7)\nHere, f is the predictive model, φi ∈R is the attribution (importance) assigned to feature xi, and M is the number of\nsimplified input features.\nFor this study, we employed six well-established attributional interpretability methods applied to large language\nmodels (LLMs), denoted as K = 6: Feature Ablation, Layer Activations (which capture the embedding activation\nspace of a specific layer of interest within the LLM), Layer DeepLIFT SHAP, Layer Gradient SHAP [23], Layer Integrated\nGradients [18], and Layer Gradient × Activation.\nTo align these layer-wise interpretability methods with the additive feature attribution framework, we reinterpret the\ninternal activations (i.e., latent units) of a network layer L as simplified input features. The objective is to estimate\nan attribution score φi for each unit, where φi ∈R quantifies the contribution of the corresponding neuron to the\nmodel’s prediction.\nLayer SHAP implementations: This directly corresponds to the Shapley formulation:\nφi =\nX\nS⊆F\\{i}\n|S|!(|F|−|S|−1)!\n|F|!\n£\nfS∪{i}(xS∪{i})−fS(xS)\n¤\n(8)\nIn practice, Deep SHAP approximates this using sampling and a chain-rule based linearization over network layers\n[23]. Gradient SHAP assumes that input features are independent and that the explanation model is linear, allowing\nexplanations to be expressed as an additive composition of feature contributions. Under these assumptions, SHAP\nvalues [23] can be approximated by computing the expected gradients over a distribution of perturbed inputs.\nSpecifically, Gaussian noise is added to each input feature to generate multiple baseline samples, and the resulting\ngradients are averaged to approximate SHAP attributions.\nActivation Attribution: This method treats the raw activation aL\ni (x) as proportional to its importance in the output.\nIn the additive form:\nφi = aL\ni (x)\n(9)\nAssuming linearity between layer L and the output, activations themselves serve as proxy contributions.\nGradient × Activation Attribution: This method computes the element-wise product between the activation values\nand the gradients of the model output with respect to those activations, thereby capturing the first-order sensitivity\nof the output to the neurons in the layer. To this end, the method estimates the first-order sensitivity of the output\nwith respect to the activation:\nφi = aL\ni (x)· ∂f\n∂aL\ni\n(x)\n(10)\nThis corresponds to a local linear approximation (first-order Taylor expansion) of the model at x, akin to DeepLIFT\nand the SHAP linearization used in DeepLift SHAP [23].\nFeature Ablation Attribution: This attributional interpretability technique is a perturbation-based approach to\nestimating attributions. It involves replacing the input or output values of a selected layer with a given baseline\nor reference value and computing the resulting change in the model’s output. By default, each neuron (i.e., scalar\ninput or output value) within the layer is ablated independently. For neuron group S ⊆{1,...,dL}, the perturbed\nactivation is:\n˜aL\ni =\n(\nbL\ni &if i ∈S,\naL\ni (x)&otherwise,\n(11)\n13\n"}, {"page": 14, "text": "A PREPRINT - JANUARY 27, 2026\nand the attribution is the marginal effect:\nφS = f\n¡\nx; ˜aL\nS\n¢\n−f (x)\n(12)\nAll attribution methods were applied to the final (22nd) layer of the MODERN-BERT LLM—the model variant that\nachieved the highest classification accuracy in our evaluations (see Suplementary material section 1.1). These\nformulations allow us to ground various neural attribution techniques within a unified additive explanation model,\nfacilitating their comparison and hybridization under shared theoretical assumptions.\nA.2\nAttributional explanation optimizer framework\nLet A = {A1, A2,..., AK } denote the set of K = 6 attribution methods applied to the final layer L of the model f .\nEach method Ak generates an attribution vector φ(k) = [φ(k)\n1 ,φ(k)\n2 ,...,φ(k)\nM ], where M is the number of latent features\n(neurons) in layer L. The goal is to derive a unified attribution vector ¯φ that captures the consensus explanation\nacross methods.\nA.2.1\nScoring and Weighting Attribution Methods\nEach attribution vector φ(k) is evaluated using the following quality metrics:\nA.2.2\nEvaluation Interpretability Metrics\nWe evaluate the robustness of each attribution method Ak using the following stability metrics:\nRelative Input Stability (RIS):\nM(k)\nRIS = RIS(f ,φ(k);x) =\n∥x∥p\n∥φ(k)(x)∥p\nmax\nx′∈Nx, ˆyx′= ˆyx\n∥φ(k)(x)−φ(k)(x′)∥p\n∥x−x′∥p\n(13)\nRelative Output Stability (ROS):\nM(k)\nROS = ROS(f ,φ(k);x) =\n∥f (x)∥p\n∥φ(k)(x)∥p\nmax\nx′∈Nx, ˆyx′= ˆyx\n∥φ(k)(x)−φ(k)(x′)∥p\n∥f (x)−f (x′)∥p\n(14)\nHere, Nx denotes a neighborhood of perturbed inputs x′ around x, and ˆyx is the predicted class label. Both metrics\nmeasure the relative sensitivity of the attribution vector φ(k) to perturbations in the input or output space.\nSparseness Metric: We quantify the sparseness of the attribution vector φ(k) ∈Rd using the Gini Index, a measure of\ninequality that has been shown to satisfy several desirable properties for evaluating sparseness [25]. This formulation\nis adopted in the context of explaining neural network predictions [25].\nLet v ∈Rd\n≥0 be a non-negative vector. Denote by v(k) the k-th smallest element in v after sorting it in non-decreasing\norder. Then, the Gini Index G(v) ∈[0,1] is defined as:\nG(v) = 1−2\ndX\nk=1\nv(k)\n∥v∥1\n·\nµd −k +0.5\nd\n¶\n,\n(15)\nwhere ∥v∥1 = Pd\ni=1 vi is the ℓ1-norm of v. To evaluate the sparseness of an attribution vector φ(k), we apply the Gini\nIndex to the vector of its absolute values:\nSparseness\n³\nφ(k)´\n= G\n³¯¯¯φ(k)¯¯¯\n´\n,\nwhere\n¯¯φ(k)¯¯ =\n³\n|φ(k)\n1 |,|φ(k)\n2 |,...,|φ(k)\nd |\n´\n.\nHigher values of G\n¡¯¯φ(k)¯¯¢\nindicate greater sparseness. In the extreme case, if only one component is non-zero, the\nGini Index reaches its maximum value of 1, indicating perfect sparseness. If all components are equal, the Gini\nIndex is 0.\n14\n"}, {"page": 15, "text": "A PREPRINT - JANUARY 27, 2026\nA.2.3\nAggregation of Attributions\nThe weighted average attribution vector ¯φ is calculated as:\n¯φ =\nKX\nk=1\nwk ·φ(k)\n(16)\nThis vector serves as the target explanation for the optimization process.\nA.2.4\nExplanation Reconstruction via Encoder–Decoder Models\nAn encoder–decoder model is trained to generate a reconstructed explanation ˆφ from the original input x. Two\narchitectures are considered the Diffusion UNet1D [26] and the x-transformer autoencoder [27, 28].\nDiffusion model:\nThe diffusion model follows the basic structure of a 1-dimensional U-Net and is trained using\ndiffusion principles. In this framework, diffusion models [36] are latent variable models in which the observed data\nφ(k)\n0\nis gradually corrupted through a forward noising process, producing a sequence of latent variables φ(k)\n1:T . A\ncorresponding reverse process is then learned to recover the original data from noise. The mathematical formulation\nis as follows:\nFORWARD PROCESS: A fixed Markov chain progressively adds Gaussian noise to the data:\nq(φ(k)\n1:T |φ(k)\n0 ) :=\nTY\nt=1\nq(φ(k)\nt |φ(k)\nt−1),\nq(φ(k)\nt |φ(k)\nt−1) := N (φ(k)\nt ;\nq\n1−βtφ(k)\nt−1,βtI)\n(17)\nAlternatively, sampling from the forward process at an arbitrary timestep t is possible in closed form:\nq(φ(k)\nt |φ(k)\n0 ) = N (φ(k)\nt ;\np\n¯αtφ(k)\n0 ,(1−¯αt)I),\n(18)\nwhere αt := 1−βt and ¯αt := Qt\ns=1 αs.\nREVERSE PROCESS: A learned time-reversal model with Gaussian transitions:\npθ(φ(k)\n0:T ) := p(φ(k)\nT )\nTY\nt=1\npθ(φ(k)\nt−1|φ(k)\nt ),\npθ(φ(k)\nt−1|φ(k)\nt ) := N (φ(k)\nt−1;µθ(φ(k)\nt ,t),Σθ(φ(k)\nt ,t)),\n(19)\nwhere p(φ(k)\nT ) := N (φ(k)\nT ;0,I).\nTRAINING OBJECTIVE: The training objective of diffusion models is based on a variational bound, which includes\nKullback–Leibler (KL) divergence terms. The KL term comparing the true posterior from the forward process and\nthe model’s learned reverse process is written as:\nKL\n³\nq(φ(k)\nt−1 | φ(k)\nt ,φ(k)\n0 )∥pθ(φ(k)\nt−1 | φ(k)\nt )\n´\n(20)\nBoth distributions are Gaussian:\nq(φ(k)\nt−1 | φ(k)\nt ,φ(k)\n0 ) = N (φ(k)\nt−1; ˜µt(φ(k)\nt ,φ(k)\n0 ), ˜βtI)\n(21)\npθ(φ(k)\nt−1 | φ(k)\nt ) = N (φ(k)\nt−1;µθ(φ(k)\nt ,t),σ2\nt I)\n(22)\nThe closed-form KL divergence between two Gaussians N (µ1,σ2\n1I) and N (µ2,σ2\n2I) in d-dimensions is:\nKL = 1\n2\n\"\nlog\nÃ\nσ2\n2\nσ2\n1\n!\n+\nσ2\n1 +∥µ1 −µ2∥2\nσ2\n2\n−d\n#\n(23)\nIn our setting, this term is computed for each timestep t and summed across all steps:\nL1:T −1 =\nTX\nt=2\nEq(φ(k)\n0 ,φ(k)\nt\n)\nh\nKL\n³\nq(φ(k)\nt−1 | φ(k)\nt ,φ(k)\n0 )∥pθ(φ(k)\nt−1 | φ(k)\nt )\n´i\n(24)\n15\n"}, {"page": 16, "text": "A PREPRINT - JANUARY 27, 2026\nThis forms a core part of the evidence lower bound (ELBO) optimized during training. Using variational inference,\nwe minimize the negative ELBO:\nL = Eq\n\"\n−logp(φ(k)\nT )+\nTX\nt=1\nKL\n³\nq(φ(k)\nt−1|φ(k)\nt ,φ(k)\n0 )∥pθ(φ(k)\nt−1|φ(k)\nt )\n´\n−logpθ(φ(k)\n0 |φ(k)\n1 )\n#\n.\n(25)\nEach KL term compares Gaussian distributions and can be computed in closed form. The posterior q(φ(k)\nt−1|φ(k)\nt ,φ(k)\n0 )\nis also Gaussian:\nq(φ(k)\nt−1|φ(k)\nt ,φ(k)\n0 ) = N (φ(k)\nt−1; ˜µt(φ(k)\nt ,φ(k)\n0 ), ˜βtI),\n(26)\nwith:\n˜µt(φ(k)\nt ,φ(k)\n0 ) =\np ¯αt−1βt\n1−¯αt\nφ(k)\n0 +\npαt(1−¯αt−1)\n1−¯αt\nφ(k)\nt ,\n(27)\n˜βt = 1−¯αt−1\n1−¯αt\nβt.\n(28)\nSIMPLIFIED TRAINING LOSS: The common parameterization rewrites the objective as denoising score matching:\nLsimple(θ) := Et,φ(k)\n0 ,ϵ\n·°°°ϵ−ϵθ(\np\n¯αtφ(k)\n0 +\np\n1−¯αtϵ,t)\n°°°\n2¸\n,\n(29)\nwhere ϵ ∼N (0,I) and ϵθ is the neural network trained to predict noise.\nIn our implementation we compute the total loss for the diffusion model as:\nLsimilarity( ˆφ, ¯φ) = Lsimilarity(θ) =\n1\nK +1\nKX\nl=0\nL (l)\nsimple(θ)\n(30)\nx-Transformer:\nLet the input sequence be:\nφ(k) = [φ(k)\n1 ,φ(k)\n2 ,...,φ(k)\nT ] ∈RT ×din\nwhere din = 7 is the input dimensionality and T = 512 is the sequence length. We consider a Transformer-based\nencoder-decoder architecture operating on input sequences Φ(k) ∈RB×T ×din at diffusion step k, where: B is the\nbatch size, T is the sequence length, din is the input feature dimension, and Φ(k) is the input sequence at step k.\nThe processing pipeline is mathematically formulated as follows:\nINPUT PROJECTION AND POSITIONAL ENCODING: We first project the input to the model dimension d and add\npositional encodings:\nX0 = WinΦ(k) +P,\nX0 ∈RB×T ×d\n(31)\nwhere: Win ∈Rdin×d is a learnable linear projection matrix, and P ∈R1×T ×d is a learnable positional embedding\nmatrix.\nENCODER: MULTI-HEAD SELF-ATTENTION LAYERS: The encoder consists of Le stacked multi-head self-attention\n(MHSA) layers:\nHenc = MHSALe ◦···◦MHSA1(X0)\n(32)\nwhere each MHSA layer performs:\nMHSA(X) = Softmax\nÃ\nQK⊤\np\ndh\n!\nV\n(33)\nwith: Q,K,V: Query, Key, and Value matrices obtained via learned linear projections, and dh: the dimensionality of\neach attention head.\nDECODER INPUT PROJECTION: During training, the decoder may receive the ground-truth output Φ(k)\ntarget ∈RB×T ×1:\nY0 = WdecΦ(k)\ntarget +P\n(34)\nwhere Wdec ∈R1×d is a projection matrix.\nIf no decoder input is available (e.g., during inference), Φ(k)\ntarget is initialized to a zero tensor.\n16\n"}, {"page": 17, "text": "A PREPRINT - JANUARY 27, 2026\nDECODER MHSA + CROSS-ATTENTION LAYERS: The decoder consists of Ld layers of MHSA followed by cross-\nattention (CA) using the encoder context:\nHdec = CALd ◦···◦CA1\n¡\nMHSALd ◦···◦MHSA1(Y0)\n¯¯Henc\n¢\n(35)\nEach cross-attention (CA) layer uses the decoder hidden state as the query and encoder output as the key and value:\nCA(Y,Henc) = Softmax\nÃ\nQdecK⊤\nenc\np\ndh\n!\nVenc\n(36)\nOUTPUT PROJECTION: Finally, the decoder output is projected back to the target dimension:\nˆΦ(k) = WoutHdec,\nˆΦ(k) ∈RB×T ×1\n(37)\nwhere Wout ∈Rd×1 is a linear projection matrix.\nThe similarity cost function is given by the Mean Squared Error (MSE) loss between the predicted output of the\nx-Transformer and the target weighted attribution vector as follow:\nLsimilarity( ˆφ, ¯φ) = LMSE = 1\nT\nTX\nt=1\n°° ˆφt −¯φt\n°°2 ,\n(38)\nA.3\nThe total cost function of the optimizer\nAs previously highlighted, the reconstruction of the optimal explanation and the associated cost function adhere to\nthe same principles and architectural design outlined in [15]. The cost function consists of three key components:\nsparseness, as defined in [25]; ROS and RIS scores [24]; and similarity. The integration of these components ensures\na robust and interpretable evaluation. The total cost function for training the reconstruction model is:\nLtotal(φ(k), ˆφ) =λ1 ·\n1\nMRIS(f , ˆφ)\n+λ2 ·\n1\nMROS(f , ˆφ)\n+λ3 · Msparse(f , ˆφ)+λ4 ·Lsimilarity( ˆφ, ¯φ)\n(39)\nwhere:λ1,λ2,λ3,λ4 are hyperparameters controlling the influence of each loss term. This formulation enables a prin-\ncipled and quantitative integration of multiple attribution methods, optimizing toward a robust and interpretable\nexplanation.\nA.4\nThe UMAP extraction and the linear constrain\nGiven a dataset ˆΦ = { ˆφ1, ˆφ2,..., ˆφn} ⊂RD, UMAP aims to find a low-dimensional embeddingU = {u1,u2,...,un} ⊂Rd\nwhere typically d = 2 or d = 3, such that the local topological structure of the data in ˆΦ is preserved in U.\nHIGH-DIMENSIONAL GRAPH CONSTRUCTION: First, the algorithm constructs a k-nearest neighbors graph in the\nhigh-dimensional space ˆΦ. The distance metric used to calculate the pairwise distances is typically Euclidean:\nd( ˆφi, ˆφj ) = ∥ˆφi −ˆφj ∥2\nNext, a conditional probability is defined between points ˆφi and ˆφj using a Gaussian distribution:\npi j = exp\nÃ\n−\n∥ˆφi −ˆφj ∥2\nσ2\ni\n!\nwhere σi is the bandwidth for the Gaussian distribution, determined through a binary search to match a fixed\nperplexity.\nThe graph is symmetrized:\nPi j =\npi j + p ji\n2\n17\n"}, {"page": 18, "text": "A PREPRINT - JANUARY 27, 2026\nLOW-DIMENSIONAL EMBEDDING GRAPH: In the low-dimensional space, a similar probability is defined between\npoints ui and u j :\nqi j =\n1\n1+ a∥ui −u j ∥2b\nwhere a and b are hyperparameters that control the shape of the distribution, and ∥ui −u j ∥2 is the Euclidean\ndistance between points in the low-dimensional embedding.\nOBJECTIVE FUNCTION: The optimization process involves minimizing the cross-entropy between the high-\ndimensional and low-dimensional probability distributions:\nL =\nX\ni<j\n£\nPi j log(Qi j )+(1−Pi j )log(1−Qi j )\n¤\nThis loss function encourages points that are close in the high-dimensional space to be close in the low-dimensional\nspace, and points that are distant to remain distant.\nOPTIMIZATION PROCESS: The optimization is carried out using stochastic gradient descent (SGD), updating the\nembedding points {ui} iteratively based on the gradient of the loss function L . The gradient updates for the\nlow-dimensional embedding ui are computed as follows:\n∂L\n∂ui\n= −\nX\nj̸=i\n¡\nPi j −Qi j\n¢\nui −u j\n∥ui −u j ∥2\n2\nREGULARIZATION CONSTRAINT: To prevent the embedding from collapsing to a single point, we introduce a variance\nconstraint to ensure that the variance of the embedding does not approach zero:\nVar(U) = 1\nn\nnX\ni=1\n∥ui −¯u∥2\n2 ≥ϵ\nwhere ¯U = 1\nn\nPn\ni=1 ui is the mean of the embeddings, and ϵ > 0 is a small constant that enforces a lower bound on\nthe variance.\nAPPLICATION OF UMAP IN OUR PROBLEM: To obtain a comparable low-dimensional representation of the attribution\nscores across all tokenizer features, we applied a feature-wise UMAP projection procedure to the normalized attri-\nbution matrix. For each attribution method, the attribution tensor has shape RM×T , where M denotes the number\nof test samples in the evaluation cohort and T corresponds to the dimensionality of the tokenizer embedding space\nof the input text. For each feature j ∈{1,...,T }, we first applied min–max normalization to the feature-specific\nattribution vector\nx(j) ∈RM,\nand subsequently performed a one-dimensional UMAP projection to obtain a two-dimensional embedding\ny(j) ∈RM×2.\nThe resulting coordinates were then normalized to the interval [0,1] to ensure that all feature-wise embeddings share\na common bounded range. This procedure preserves the relative neighborhood structure of the M-sample attribu-\ntion distribution for each feature while mapping all T features into a comparable two-dimensional representation\nspace.\nThe motivation for applying UMAP independently to each of the T tokenizer features is to ensure that all attribution\nmethods are projected into an aligned and comparable representation space. Since each attribution method\nproduces values defined over the same token embedding dimensions, a feature-wise nonlinear projection enables\nconsistent cross-method comparison of attribution patterns within the shared tokenizer feature space.\nLINEAR CONSTRAINT FOR EQUAL COMPONENTS IN UMAP: Let ui = (ui1,ui2,...,uid) denote the embedding of the\ni-th data point in a d-dimensional space. The requirement that the first and second embedding components are\nequal can be written as:\nui1 = ui2\n∀i ∈{1,2,...,n}.\n18\n"}, {"page": 19, "text": "A PREPRINT - JANUARY 27, 2026\nEquivalently, this can be expressed as the linear equality constraint:\nui1 −ui2 = 0\n∀i ∈{1,2,...,n}.\nThis constraint enforces that, for each data point i, the first and second coordinates of the embedding vector ui are\nidentical.\nWithin the total objective Ltotal(φ(k), ˆφ) of Eq. 35, an additional penalty term may be introduced to enforce this\nconstraint. The penalty can be written as:\nλ5\nnX\ni=1\n(ui1 −ui2)2 ,\nwhere λ5 is a regularization parameter controlling the strength of the constraint. This term encourages the first and\nsecond components of each reconstructed embedding point from the optimizer ( ˆφ) to be equal, while still allowing\nflexibility depending on the value of λ5.\nA.5\nThe superposition and the monosemantic representations\nWe model an embedding space as a real vector space Rd, where a hidden activation vector h ∈Rd represents a\ncombination of underlying semantic features. By the linear representation hypothesis, each interpretable feature\ncorresponds to a fixed direction in Rd [8, 29].\nLet a ∈RF be a sparse feature activation vector and W ∈Rd×F be a linear transformation such that:\nh = W a =\nFX\ni=1\naiwi,\nwhere wi denotes the i-th column of W , corresponding to the direction of the feature i.\nIf F > d, the map W cannot be invertible, and thus different combination of characteristics can map to the same\nembedding. This gives rise to superposition, where multiple semantic features are embedded into shared subspaces\nor overlapping neuron activations [29].\nMONOSEMANTIC REPRESENTATIONS: A representation is called monosemantic when each neuron corresponds to a\nsingle interpretable feature [8]. Mathematically, this corresponds to the case where W is full-rank and aligned with\nthe identity matrix (or a rotation of it):\nW = I ⇒h = a.\nThis implies that each feature ai is represented by a unique dimension hi, with no overlap. Each neuron responds\nto a single, isolated concept, akin to “grandmother cells” in neuroscience [37].\nPOLYSEMANTIC REPRESENTATIONS: In contrast, polysemantic neurons represent multiple, distinct concepts. For-\nmally, if neuron h j computes:\nh j =\nFX\ni=1\nWj,i ai,\nand two or more Wj,i ̸= 0, then neuron j encodes multiple features simultaneously, exhibiting polysemanticity\n[29, 30].\nMore generally, a polysemantic embedding may be viewed as a mixture:\nh =\nKX\nk=1\nαkck,\nK > 1,\nwhere ck are concept vectors and αk are scalar weights.\nThis behavior is prevalent in both neural network activations and in biological neurons that exhibit mixed selectivity\n[38].\nMonosemantic representations arise from disentangled bases, where neurons correspond to isolated features.\nSuperposition emerges from dimensionality compression and necessarily leads to polysemantic neurons, each\nencoding a combination of features. Spare auto-encoder is a way to try to solve the polysemantic neurons—each\nencoding problem.\n19\n"}, {"page": 20, "text": "A PREPRINT - JANUARY 27, 2026\nA.6\nThe SAE approach and architectures\nSparse Autoencoder (SAE) architectures have advanced our understanding of how language and vision models\nrepresent features [16]. Neural network behavior is often explained via computational circuits—collections of\nneurons that together compute meaningful functions. Classical circuit analysis has identified key components such\nas edge detectors [39] or word-copying units [30]. By using features derived from SAEs rather than raw neurons,\nresearchers have improved the interpretability of circuits related to complex behaviors [40].\nFeature discovery can involve visual analysis [41], manual inspection [42], and even assistance from large language\nmodels [43]. Their causal role is often validated via activation interventions: modifying a feature activation vector a\nand observing predictable changes in model output [44].\nThe mathematical formulation situates SAE architectures within the theoretical framework of superposition and\nsemantic disentanglement. By expressing hidden states as sparse linear combinations of interpretable features,\nSAEs bridge the gap between low-level activations and human-understandable concepts.\nLINEAR FORMULATION OF SAES: Let x ∈Rd denote a layer’s neuron activation vector in a pretrained model. A Sparse\nAutoencoder learns a sparse feature representation a ∈RF such that:\nˆx = W a+b,\n(40)\nwhere W ∈Rd×F is the decoder (dictionary) matrix and b ∈Rd is a learned bias term. Each column W·,i represents\nthe direction of feature i in neuron space, and ai is its activation. This linear mapping enables complex activations\nto be expressed as combinations of more interpretable features.\nIf F > d, then the feature space is overcomplete, and W cannot be full-rank. This leads to superposition, where\nmultiple features overlap in the same subspace, and individual neurons encode multiple unrelated concepts\n[29]. If W is invertible and aligned to a basis, each neuron corresponds to a single feature. The representation is\nmonosemantic and disentangled [8]. When W has overlapping columns, neurons can respond to multiple features,\nyielding polysemantic behavior. That is, for some j, x j = P\ni Wj,i ai involves multiple nonzero terms [30].\nVARIANTS OF SAES: Variants of SAEs like TopK, JumpReLU, and Gated-SAEs offer increasingly precise control over\nthe mapping between low-level activations and human-understandable concepts, enabling fine-grained analysis\nand intervention.\nTopK-SAEs: Instead of using a soft sparsityconstraint (e.g., L1 regularization), TopK-SAEs enforce hard sparsity\nusing a top-K activation function:\na = TopK(Wenc(x−bdec)),\n(41)\nwhich retains only the K largest entries of the preactivation and zeros out the rest. This promotes discrete sparsity\nand avoids complex hyperparameter tuning.\nJumpReLU-SAEs: JumpReLU replaces ReLU with a thresholded step function:\nJumpReLUθ(x) = x · H(x −θ),\n(42)\nwhere H(·) is the Heaviside step function and θ is a learnable threshold. This allows neurons to activate only\nabove a semantic threshold, aligning with binary behavior observed in some interpretable features. However, the\ndiscontinuity makes training difficult due to non-differentiability.\nGated-SAEs: Gated-SAEs introduce a gating mechanism that decouples activation magnitude and presence. Let\nWmag and Wgate be two encoders. Then the feature activation is computed as:\na =\n¡\nWmag(x)\n¢\n⊙H(Wgate(x)−θ),\n(43)\nwhere ⊙denotes elementwise multiplication. This enables better control over when and how strongly a feature\nactivates, making them easier to train than JumpReLU-SAEs [45].\nIn this study we utilize two different architectures of SAEs the standard SAE and TopK-SAE.\nA.7\nAttribution from Sparse Feature Space to Input Tokens\nLet xinput ∈Rdinput denote the input embedding vector (e.g., LLM token embeddings), x = f (xinput) ∈Rd the hidden\nlayer activation of the LLM, a = Encoder(x) ∈RF the SAE sparse feature vector, and ˆx = W a+b the reconstructed\nactivation from the SAE decoder. Now suppose we have a sparse attribution vector ψi over features a, i.e., ψ ∈RF ,\n20\n"}, {"page": 21, "text": "A PREPRINT - JANUARY 27, 2026\nwhere each ψi reflects the importance of SAE feature ai. We aim to assign importance Φk to each input token\ndimension xinput,k.\nATTRIBUTION FLOW THROUGH THE ENCODER: We propagate the feature attributions backward through the encoder\nto the input. Using the chain rule:\nΦk =\nFX\ni=1\nψi ·\n∂ai\n∂xinput,k\n=\nFX\ni=1\nψi · ∂ai\n∂x ·\n∂x\n∂xinput,k\n(44)\nwhere ∂ai\n∂x is the encoder Jacobian (SAE layer), and\n∂x\n∂xinput,k is the LLM gradient from input token to hidden layer.\nThis gives us a scalar attribution Φk ∈R for each token/input embedding dimension k.\nThis represents how much each input token contributes to the sparse SAE features that have been identified as\nimportant. In this way, we evaluate the contribution of input features based on the monosemantic behavior of the\ntrained network’s mechanism. Based on our study thus far, we will apply the six attribution methods previously\ndiscussed at two levels: from the SAE feature space to the encoder layer, and from the encoder layer to the input\nembedding space. This dual-level attribution analysis enables us to investigate how interpretable sparse features\nrelate to model internals and ultimately influence the input-level representations.\nTo this end, we define a two-step attribution mechanism:\nStep 1: Attribution from Sparse Features to Encoder Layer\nLet ψ ∈RF represent the importance scores of sparse features (obtained via attribution methods). We propagate\nthese to the encoder layer as:\nφenc = W ψ ∈Rd,\n(45)\nwhere φenc quantifies the contribution of each encoder neuron to the important SAE features.\nStep 2: Attribution from Encoder Layer to Input\nTo assign attribution scores to input dimensions, we propagate φenc to the input embedding via the gradient of the\nencoder:\nφinput =\nµ\n∂x\n∂xinput\n¶⊤\nφenc ∈Rdinput.\n(46)\nAlternatively, attribution methods (e.g., Integrated Gradients, SHAP) can directly estimate:\nφinput = AttributionMethod(f ,xinput,φenc)\nThis dual-level attribution analysis allows us to connect semantically meaningful sparse features to the raw input\nrepresentation space.\nB\nClinical Validation and External Interpretability Analysis\nB.1\nClinical relevance in Alzheimer’s diagnosis progression: Evidence that SAE-guided attribution yields more\nreliable explanations than traditional attribution.\nTo further validate the role of the SAE layer in shaping the attribution space into a more monosemantic and clinically\ncoherent feature representation, we conducted an auxiliary evaluation. Specifically, for each test input, we extracted\nthe top 50% most influential token attributions produced by our attribution framework—TEO without SAE, TEO with\nSAE, and TEO-UMAP. We then generated a CSV file for each classification class, in which the highlighted characters\nfor the three attribution methods were organized column-wise. The complete character sequence for each sample,\nbeginning with the CLS token, was included in the first column to ensure clear sample-level distinction. These CSV\n21\n"}, {"page": 22, "text": "A PREPRINT - JANUARY 27, 2026\nfiles were subsequently provided as input to a large language model (ChatGPT-5.1 [35]) using a fixed prompt to\nobtain an external, model-agnostic assessment of the interpretability structure encoded by each explanation space.\nThree experiments were performed:\n1. Binary ADNI (Control vs. Alzheimer’s disease), with each class provided as a separate CSV file.\n2. Binary BrainLAT (Control vs. Alzheimer’s disease), also split into two class-specific CSV files.\n3. Three-class ADNI (Control, MCI, LMCI), with each diagnostic category represented in its own CSV file.\nWe evaluated two primary criteria: (i) whether the language model could distinguish, based solely on the highlighted\nfeatures, which CSV corresponded to the pathological versus the healthy control class; and (ii) whether the model\ncould identify meaningful pathology-related biomarkers.\nFor the first two experiments, the model was prompted with:\nGiven the two CSV files, and recognizing that medical biases exist in the char column with each\nsample beginning with the character sequence [CLS], determine how each of the three attribution\nmethods (attr1, attr2, attr3) highlight features associated with healthy or unhealthy interpretations,\nand analyze the reasons for these differences. Predict the pathology and specify which of the two\nCSV files corresponds to the pathological case for each attribution technique.\nFor the three-class ADNI experiment, we used:\nGiven the three CSV files, and recognizing that medical biases exist in the char column with each\nsample beginning with the character sequence [CLS], determine how each attribution method (attr1,\nattr2, attr3) highlights features associated with healthy or pathological interpretations. Predict the\npathology or pathologies, identify which CSVs correspond to the pathological and healthy groups\nfor each attribution method, and specify the associated conditions.\nThe resulting GPT-generated interpretations, shown in Figures 4, provide an external linguistic lens on each\nexplanation space.\nIn the binary ADNI experiment, all the three attribution frameworks, TEO without SAE, TEO with SAE, and TEO-\nUMAP, correctly identified the pathological file (CSV1) and associated it with Alzheimer’s disease, noting that the\nsignal was more consistent with late-stage rather than early cognitive impairment. A similar pattern emerged for\nthe binary BrainLAT experiment: the pathological CSV was attributed to Alzheimer’s disease rather than MCI, with\nclear differentiation from the healthy control (Figures 4a,b).\nAcross both binary tasks, TEO without SAE exhibited erratic and clinically uninformative behavior, frequently\nattending to task labels, instruction counts, or other artefactual patterns rather than neurocognitive biomarkers\n(Figures 4a,b). In contrast, TEO-SAE and TEO-UMAP consistently highlighted clinically meaningful domains,\nincluding demographic risk factors, processing-speed impairments, and neurophysiological indicators.\nIn the more complex three-class ADNI experiment, the advantages of the SAE-induced monosemantic structure\nbecame even clearer (Figure 4c). TEO without SAE failed to correctly identify pathological classes and did not surface\nmeaningful biomarkers. Conversely, TEO-SAE achieved clearer diagnostic separation and more coherent feature\nconcentrations, while TEO-UMAP further emphasized structured biomarkers, particularly within demographics\nand vitals, and also provided correct class-level predictions.\nCollectively, these findings demonstrate that incorporating the SAE layer—thereby enforcing a more monoseman-\ntic, disentangled attribution representation—substantially enhances the clinical meaningfulness, stability, and\ndiagnostic alignment of the resulting explanations.\nB.2\nThe clinical impact and outcome in the diagnosis of Alzheimer, early MCI and late MCI.\nThis study shows that the Transformer Explanation Optimizer (TEO) with a Sparse Autoencoder (SAE) and TEO-\nUMAP provide the most reliable identification of informative sources across nine multimodal subgroups: Demo-\ngraphics (DEM), Vital Signs (VS), Clock Drawing Test (CDT), Clock Copying Test (CCT), Auditory Verbal Learning\nTest v1 (AVLT1), Category Fluency—Animals (CFA), Auditory Verbal Learning Test v2 (AVLT2), American National\nAdult Reading Test (ANART), and Functional Activities Questionnaire (FAQ). Using a significance threshold of 0.6\non PCA principal components PC1, we observe in the binary task that, for Control, TEO-SAE is dominated by FAQ,\nwhereas TEO-UMAP emphasises DEM, AVLT2, and FAQ; for Alzheimer’s, TEO prioritises FAQ, AVLT1, and CFA,\nwhile TEO-UMAP highlights ANART, FAQ, and DEM. In the three-class task, for Control the main contributors are\n22\n"}, {"page": 23, "text": "A PREPRINT - JANUARY 27, 2026\nTable 2: Relative contribution of multimodal assessment groups across diagnostic categories and attribution\nmethods.\nGroup\nDem\nVS\nCDT CCT AVLT1 CFA AVLT2 ANART FAQ\nBinary classification (Control vs. Alzheimer’s disease)\nControl (TEO)\n0.00\n0.00\n0.00\n0.00\n0.02\n0.00\n0.00\n0.00\n0.11\nControl (TEO-UMAP)\n0.33\n0.21\n0.29\n0.29\n0.12\n0.30\n0.32\n0.30\n0.36\nAlzheimer (TEO)\n0.05\n0.00\n0.09\n0.01\n0.12\n0.12\n0.00\n0.00\n0.13\nAlzheimer (TEO-UMAP)\n0.33\n0.29\n0.30\n0.32\n0.22\n0.30\n0.32\n0.50\n0.35\nThree-class classification (Control / MCI / LMCI)\nControl (TEO)\n0.74\n0.55\n0.87\n0.63\n0.88\n0.68\n0.20\n0.80\n0.38\nControl (TEO-UMAP)\n0.51\n0.61\n0.67\n0.60\n0.72\n0.66\n0.56\n0.60\n0.45\nMCI (TEO)\n0.23\n0.30\n0.29\n0.38\n0.22\n0.30\n0.36\n0.10\n0.31\nMCI (TEO-UMAP)\n0.31\n0.26\n0.21\n0.22\n0.22\n0.36\n0.40\n0.40\n0.23\nLMCI (TEO)\n0.19\n0.19\n0.20\n0.10\n0.22\n0.16\n0.24\n0.10\n0.22\nLMCI (TEO-UMAP)\n0.32\n0.27\n0.23\n0.25\n0.24\n0.28\n0.40\n0.40\n0.43\nAbbreviations: Dem = Demographics; VS = Vital Signs; CDT = Clock Drawing Test; CCT = Clock Copying Test; AVLT1/2 = Auditory\nVerbal Learning Test (v1/v2); CFA = Category Fluency (Animals); ANART = American National Adult Reading Test; FAQ =\nFunctional Activities Questionnaire.\nAVLT1, CDT, and ANART under TEO, and AVLT1, CDT, and CFA under TEO-UMAP; for MCI, TEO favours CCT, AVLT2,\nand FAQ, whereas TEO-UMAP favours AVLT2, ANART, and CFA; and for LMCI, TEO elevates AVLT1, FAQ, and CDT,\nwhile TEO-UMAP elevates FAQ, ANART, and AVLT2. These patterns, summarised in Table 2, support the clinical\ninterpretability of the proposed optimisers.\nAcross ADNI cohorts, the most stable signals for clinical stratification are functional status (FAQ) and memory\nmeasures (AVLT1/AVLT2), with visuospatial performance (CDT) recurrent in Control/LMCI. TEO+SAE preferentially\nelevates neuropsychological performance features (AVLT1/2, CDT, CCT), while TEO-UMAP surfaces complementary\ncontextual and language markers (DEM, ANART, CFA), yielding class-specific, interpretable profiles: Control—\nFAQ/AVLT1/CDT; Alzheimer’s—FAQ with AVLT1/CFA (TEO) or ANART/DEM (TEO-UMAP); MCI—AVLT2 with\nCCT/FAQ (TEO) or ANART/CFA (TEO-UMAP); and LMCI—FAQ with AVLT1/CDT (TEO) or ANART/AVLT2 (TEO-\nUMAP). Using a simple PC1 ≥0.6 significance rule, these optimisers provide actionable attribution maps that\ncan prioritise assessments, reduce testing burden, support trial enrichment, and guide personalised monitoring.\nTogether, they offer a practically deployable and transparent framework for clinically meaningful multimodal\nreasoning in neurodegenerative disease.\nB.3\nValidation of the UMAP Linear Constraint via PCA Structure Analysis\nTo verify the claim that the proposed UMAP linear constraint effectively linearizes the majority of the attribution\nspace and yields robust attribution scores within the same tokenized feature, we performed an additional PCA\nanalysis. Specifically, we extracted the top eight principal components from the attribution matrix and visualized\nthe first two components, which capture the highest proportion of variance in the data. This allows us to assess\nwhether the tokenized features exhibit an approximately linear structure in their dominant statistical directions.\nThe results (Figure 2) demonstrate that the proposed method indeed produces an embedding that approaches a\nlinear configuration, thereby supporting our hypothesis that the UMAP linear constraint leads to more stable and\nstructurally consistent attribution representations.\nB.4\nRelated Work\nB.4.1\nAttributional Interpretabillity\nAttributional interpretability (AtI), a branch of explainable AI (XAI), focuses on explaining model outputs by tracing\npredictions back to individual input contributions, often using gradient-based methods [46]. While gradients provide\ninsights into the relationship between inputs and outputs, they can be sensitive to perturbations or discontinuities,\nposing challenges for reliable interpretation.\nAtI encompasses various methods for interpreting complex, nonlinear models, including techniques like Local\nInterpretable Model-agnostic Explanations (LIME; [47]) and SHapley Additive exPlanations (SHAP; [23]). In medical\n23\n"}, {"page": 24, "text": "A PREPRINT - JANUARY 27, 2026\nimaging, popular attribution techniques include SHAP, Layer-wise Relevance Propagation (LRP; [48]), and gradient-\nbased methods like GRAD-CAM ([49]). These methods aim to enhance trust in models and provide valuable insights\ninto decision-making processes. However, they face limitations. For instance, LRP emphasizes positive preactiva-\ntions, often yielding less precise explanations, while SHAP is computationally intensive due to the complexity of\ncalculating Shapley values [23]. Adaptations like Monte Carlo methods and stratified sampling (e.g., SVARM) have\nimproved the efficiency and precision of certain techniques [50].\nB.4.2\nMechanistic Interpretability and Sparse Autoencoder\nMechanistic interpretability (MI), a key area of explainable AI (XAI), focuses on understanding the internal acti-\nvation patterns of AI models by analyzing their fundamental components, such as features, neurons, layers, and\nconnections. Unlike AtI, MI takes a bottom-up approach, aiming to uncover the causal relationships and precise\ncomputations that transform inputs into outputs. This method identifies specific neural circuits driving behavior\nand provides a reverse-engineering perspective. Insights from fields like physics, neuroscience, and systems biology\nfurther guide the development of transparent and value-aligned AI systems.\nA core principle of MI is the concept of polysemanticity, where individual neurons encode multiple concepts,\ncontrasted with monosemanticity, where neurons correspond to a single semantic concept. Polysemanticity\nreduces interpretability, as neurons represent overlapping features. Structures like sparse autoencoders (SAEs)\naddress this by leveraging the superposition hypothesis, which posits that neural networks use high-dimensional\nspaces to represent more features than the number of neurons, encoding them in nearly orthogonal directions. SAEs\ndecompose embeddings from deep layers, such as MLPs or transformer attention layers, into higher-dimensional\nmonosemantic representations, aligning activation patterns with specific concepts of interest [9, 51].\nSparse Autoencoder architectures have significantly advanced our understanding of feature representations in\nlanguage and vision models [16]. Neural network behavior is often interpreted through computational circuits—\ngroups of neurons that compute meaningful functions, such as edge detectors [39] or word-copying units [52].\nLeveraging SAE-derived features instead of raw neurons has improved the interpretability of circuits associated with\ncomplex behaviors [40]. This shift enables clearer mappings between neuron activations and high-level functions,\nfacilitating validation of model behavior [46]. By aligning internal representations with privileged basis directions—\ndistinct semantic vectors within network layers—researchers further enhance monosemanticity and advance the\ninterpretability of deep models.\n24\n"}, {"page": 25, "text": "A PREPRINT - JANUARY 27, 2026\nTwo-class ADNI classification experiment\nBinary ADNI Control (0)\nBinary ADNI Alzheimer (1)\nGiven the two CSV files, and recognizing that medical biases exist in the char column with each of samples \nbeginning with the character sequence [C L S], determine how each of the three attribution methods (attr1, \nattr2, attr3) highlights features that may be associated with healthy or unhealthy interpretations, and analyze \nthe reasons for these differences. Try to predict the pathology. Mention which of the two csv files highlight \nthe pathological case for each of the three attr techniques and try to specify the pathology.\nTEO-No-SAE\nTEO-UMAP\nTEO-SAE\nTwo-class BrainLAT classification experiment\nBinary BrainLAT Control (BL_0)\nBinary BrainLAT Alzheimer (BL_1)\nGiven the two CSV files, and recognizing that medical biases exist in the char column with each of samples \nbeginning with the character sequence [C L S], determine how each of the three attribution methods (attr1, \nattr2, attr3) highlights features that may be associated with healthy or unhealthy interpretations, and analyze \nthe reasons for these differences. Try to predict the pathology. Mention which of the two csv files highlight \nthe pathological case for each of the three attr techniques and try to specify the pathology.\nTEO-No-SAE\nTEO-UMAP\nTEO-SAE\nFigure 4: GPT-5.1 generated global explanations characterizing attribution-score performance in biomarker identifi-\ncation for TEO, both with and without the SAE layer, and for the TEO-UMAP model. (a) Binary ADNI: Control vs.\nAlzheimer’s disease. (b) Binary BrainLAT: Control vs. Alzheimer’s disease.\n25\n"}, {"page": 26, "text": "A PREPRINT - JANUARY 27, 2026\nThree-class ADNI classification experiment\nGiven the three CSV files, and recognizing that medical biases exist in the char column with each of samples \nbeginning with the character sequence [C L S], determine how each of the three attribution methods (attr1, \nattr2, attr3) highlights features that may be associated with healthy or unhealthy interpretations, and analyze \nthe reasons for these differences. Try to predict the pathology or pathologies and the healthy if exist. Mention \nwhich of the three csv files highlight the pathological case/s for each of the three attr techniques and try to \nspecify the pathology/ies and the same for the healthy (control if exist).\nThree class ADNI Control (0)\nThree class ADNI MCI (1)\nThree class ADNI LMCI (2)\nTEO-No-SAE\nTEO-UMAP\nTEO-SAE\nFigure 5: (continued) (c) Three-class ADNI: Control, MCI, and LMCI categories.\n26\n"}]}