{"doc_id": "arxiv:2511.20718", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.20718.pdf", "meta": {"doc_id": "arxiv:2511.20718", "source": "arxiv", "arxiv_id": "2511.20718", "title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training", "authors": ["Chenliang Li", "Adel Elmahdy", "Alex Boyd", "Zhongruo Wang", "Alfredo Garcia", "Parminder Bhatia", "Taha Kass-Hout", "Cao Xiao", "Mingyi Hong"], "published": "2025-11-25T05:54:02Z", "updated": "2025-11-25T05:54:02Z", "summary": "PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.20718v1", "url_pdf": "https://arxiv.org/pdf/2511.20718.pdf", "meta_path": "data/raw/arxiv/meta/2511.20718.json", "sha256": "401102fb439acc864c7e9575dba83d9459cb862c517ad4f2d8efbad9934bd334", "status": "ok", "fetched_at": "2026-02-18T02:26:23.118817+00:00"}, "pages": [{"page": 1, "text": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization\nfor Multi-Turn Agents\nChenliang Li1∗, Adel Elmahdy2, Alex Boyd2, Zhongruo Wang3,\nAlfredo Garcia1, Parminder Bhatia2, Taha Kass-Hout2, Cao Xiao2, Mingyi Hong4\n1Texas A&M University\n2GE HealthCare\n3Independent Researcher\n4University of Minnesota\nchenliangli@tamu.edu,\n{adel.elmahdy, alex.boyd}@gehealthcare.com\nwangleft@gmail.com\nalfredo.garcia@tamu.edu\n{parminder.bhatia, taha.kass-hout, cao.xiao}@gehealthcare.com\nmhong@umn.edu\nNovember 27, 2025\nAbstract\nProximal policy optimization (PPO) has been widely adopted for training large language\nmodels (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its\nperformance is often unstable and prone to collapse. Through empirical analysis, we identify\ntwo main sources of instability in this setting: (1) token-level importance sampling, which is\nmisaligned with the natural granularity of multi-turn environments that have distinct turn-\nlevel stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic\nhas not learned to evaluate certain state-action pairs, resulting in high-variance gradients and\nunstable updates. To address these challenges, we introduce two complementary stabilization\ntechniques: (1) turn-level importance sampling, which aligns optimization with the natural\nstructure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients\nby downweighting unreliable, highly off-policy samples. Depending on how these components are\ncombined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias\ncorrection applied to token-level PPO), and ST-PPO (turn-level sampling combined with\nclipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which\ntogether demonstrate how the two stabilization mechanisms address complementary sources\nof instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and\nmedical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the\nperformance collapses observed in large-model training, maintain lower clipping ratios throughout\noptimization, and achieve higher task performance than standard token-level PPO. These results\ndemonstrate that combining turn-level importance sampling with clipping-bias correction provides\na practical and scalable solution for stabilizing multi-turn LLM agent training.\n∗This work was done during an internship at GE HealthCare, Bellevue, WA.\nCorresponding authors: Chenliang Li and Adel Elmahdy.\n1\narXiv:2511.20718v1  [cs.LG]  25 Nov 2025\n"}, {"page": 2, "text": "→ωJToken→PPO(ω) = E\n\n1\n|y|\n|y|\n#\nt=1\n1t↑Bc\ntoken wt(ω) →ω log εω(yt|x, y<t) ˆAt\n\n.\n→ωJTurn-PPO(ω) = E\n! 1\n|y|\nK\n\"\nk=1\nwturn\nk\n(ω)\nˆAk\n|yk|\n#\n$%\n&\nturn-level credit\n→ω log εω(yk | x, y<k)\n'\n.\n→ωJS-PPO(ω) :=\n1\n↑Ctoken(ω)↑2\n→ωJPPO(ω).\n→ωJST-PPO(ω) :=\n1\n↑Cturn(ω)↑2\n→ωJTurn-PPO(ω).\n0\n50\n100\n150\n200\n250\nPolicy Optimization Steps\n100\n102\n104\n106\n108\n1010\n1012\nGradient Norm (log scale)\nST-PPO\nS-PPO\nToken-PPO\nTurn-PPO\nFigure 1: Illustration of the four PPO variants. Token-level PPO becomes Turn-level PPO by\napplying turn-level importance sampling (Eq. 4). Further adding the clipping bias to normalize\ngradients yields S-PPO and ST-PPO (Eq. 8 and Eq. 7). Both variants significantly reduce the\nprobability of extreme gradient spikes, leading to more stable training.\n1\nIntroduction\nReinforcement learning (RL) has significantly advanced the reasoning capabilities of large language\nmodels (LLMs), enabling strong performance in domains such as mathematical problem solving\n(Jaech et al., 2024; Liu et al., 2024; Yu et al., 2025) and code generation (El-Kishky et al., 2025;\nCui et al., 2025). Beyond these applications, RL has also shown promise in more agentic settings\nsuch as tool learning (Qian et al., 2025; Feng et al., 2025), where models learn to invoke external\ntools (e.g., web search engines), execute actions, and interact with real-world environments. Recent\nsystems such as Deepseek V3 (Liu et al., 2024) and Kimi V2 (Team et al., 2025) have achieved\nstate-of-the-art performance on both mathematical reasoning (e.g., AIME, Math-500) and agentic\nbenchmarks (Jimenez et al., 2023). Despite these successes, the computational demands of multi-turn\nRL training pose significant practical challenges. These gains rely on numerous interaction samples,\nwhich are costly due to the large number of rollouts and multi-turn tool calls required during training.\nIn practice, hardware and memory limits force each batch of collected samples to be split into\nseveral mini-batches (Schulman et al., 2017) and updated sequentially. This naturally induces a\nhybrid update mechanism where later updates become increasingly off-policy (Chen et al., 2023). To\nmaximize sample efficiency under computational constraints, practitioners often adopt off-policy\npipelines, and the resulting distribution mismatch is typically corrected using importance sampling\n(Nachum et al., 2017).\nThe shift to off-policy methods, while necessary for computational efficiency, introduces high\nvariance and can destabilize training (Munos et al., 2016; Precup et al., 2000). To address this\nchallenge, proximal policy optimization (PPO) (Schulman et al., 2017) constrains policy updates\nwith a clipped surrogate objective. By limiting the impact of importance sampling ratios, PPO\nstabilizes learning even when mini-batch reuse makes later updates effectively off-policy. Building on\nthis principle, recent advances such as TOPR (Roux et al., 2025), LUFFY (Yan et al., 2025), and\nGSPO (Zheng et al., 2025) refine importance sampling and clipping strategies to further enhance\nstability and efficiency.\nDespite these advances, PPO training on multi-turn tasks still suffers from two core deficiencies:\n(1) token-level importance sampling misaligns with the natural granularity of multi-turn environments\n2\n"}, {"page": 3, "text": "(Zeng et al., 2025), where reasoning unfolds through distinct turn-level stages (e.g., problem analysis,\nquery formulation, information processing), and (2) off-policy updates rely on critic-based advantage\nestimates that are often unreliable for out-of-distribution tokens (Dorka et al., 2022), leading to\nhigh-variance gradients and potential collapse even under clipping.\nTo address the first deficiency in off-policy RL, we propose turn-level importance sampling,\nas formalized in Lemma 4.1, which establishes the mathematical foundation for turn-level credit\nassignment. This formulation aligns optimization with the natural boundaries of reasoning and\nretrieval phases, providing more precise credit assignment while preserving stability. It enables\nthe model to treat different stages of a reasoning trajectory with appropriate emphasis. Such\nstructure-aware optimization proves particularly valuable for search-augmented reasoning, where the\nquality of intermediate decisions directly impacts final outcomes. The intuition behind turn-level\nimportance sampling lies in balancing granularity and stability: token-level methods are overly noisy,\nsequence-level methods overly coarse, while turn-level ratios strike the middle ground by capturing\nsub-goal level contributions without amplifying token-level variance.\nTo further stabilize off-policy training and address the second deficiency, we employ an adjust-\nment for clipping-induced bias as detailed in Lemma 4.2, which decomposes PPO’s gradient to\nisolate the clipping bias term. Retaining token-level information constrains high-variance updates,\nand turn-level importance sampling guides credit assignment. This hybrid design uses turn-level\ncredit assignment and token-level clipping bias for stabilization, achieving a balance between stability\nand fidelity and ensuring more conservative and robust policy updates.\nOur key contributions are as follows:\n1. We empirically diagnose why vanilla PPO becomes unstable when applied to multi-turn LLM\nagent training under off-policy updates. Our observations reveal two root causes unique to the\nagentic setting: (i) the granularity mismatch between token-level optimization and turn-level\ninteractions, and (ii) the accumulation of variance from unreliable critic estimates on off-policy\nsamples, where state-action pairs are poorly evaluated.\n2. We propose a PPO variant based on turn-level importance sampling (Turn-PPO) that better\nmatches the structure of multi-turn reasoning and enables turn-level credit assignment.\n3. We propose stabilized turn-level PPO (ST-PPO, see Fig. 1, Section 4), a hybrid algorithm that\ncombines turn-level ratios for optimization with a clipping-bias correction to normalize gradient\nupdates. This design reduces highly off-policy updates and leads to more conservative and robust\npolicy learning.\nThese contributions constitute the first tailored adaptation of PPO for multi-turn LLM agents,\nintegrating theoretical insights with algorithmic design. By addressing both the granularity mismatch\nand the instability of off-policy updates, our approach provides a principled framework for stable\noptimization. Empirically, it mitigates the collapse observed in large-model training and consistently\nimproves task performance, offering a practical and scalable solution for reinforcement learning with\nmulti-turn LLM agents.\n2\nRelated Works\n2.1\nReinforcement Learning with LLM Agents\nRecent advances in reinforcement learning for large language models (LLMs) have primarily progressed\nalong two main directions: (1) feedback-driven alignment and policy optimization, and (2) agentic\n3\n"}, {"page": 4, "text": "tool use with long-horizon reasoning.\nOn the alignment side, RLHF (Ouyang et al., 2022) translates human or rule-based preferences into\nreward signals and optimizes policies using policy gradient methods, as exemplified by InstructGPT.\nSubsequent approaches such as direct preference optimization (DPO) (Rafailov et al., 2023) avoid\nexplicit reward modeling, while other RL algorithms—including PPO (Schulman et al., 2017),\nGRPO (Shao et al., 2024), and RLOO (Ahmadian et al., 2024)—have been adapted to large models\nto balance stability and efficiency.\nOn the agentic side, methods such as ReAct (Yao et al., 2023), Give (He et al., 2024, 2025)\nintegrate reasoning and acting through interleaved thought–action steps, while Reflexion (Shinn et al.,\n2023) leverages self-reflection and memory to enhance multi-turn decision making. Toolformer (Schick\net al., 2023) demonstrates self-supervised learning of tool usage, and WebGPT (Nakano et al., 2021)\nintroduces browser-based agents trained with human feedback. More recent systems, such as Search-\nR1 (Jin et al., 2025b), WebAgent-R1 (Wei et al., 2025), and WEBRL (Qi et al., 2024), extend this\nline of research to end-to-end reinforcement learning for web-based agents, where models are trained\nto perform retrieval, reasoning, and action within real-world interactive environments.\n2.2\nOff Policy Reinforcement Learning with LLM Agents\nOff-policy reinforcement learning methods differ fundamentally from on-policy approaches by allowing\nthe agent to learn from data generated by a policy different from the one currently being improved.\nThis characteristic enables greater flexibility and data efficiency, as off-policy algorithms can reuse\npast experiences or observations collected under different policies. This property is particularly\nimportant in scenarios where data collection is costly or limited.\nRecent work has extended these ideas to policy gradient methods tailored for off-policy learning in\ncomplex environments. The tapered off-policy REINFORCE (TOPR) algorithm (Roux et al., 2025)\nbuilds on the classic REINFORCE method (Williams, 1992) by introducing a tapered importance\nsampling scheme for the policy gradient. This tapering reduces the variance and instability that\ncommonly arise in naive off-policy gradient methods, as it asymmetrically adjusts the importance\nweights to better balance learning from positive and negative examples. However, because TOPR\nrelies heavily on trajectory-level rewards, it is not well-suited for scenarios that require more\nfine-grained credit assignment.\nAnother research line explores off-policy extensions of GRPO. ARPO (Lu et al., 2025) incorporates\na replay buffer into GRPO’s sampling process, mixing on-policy and replayed samples to alleviate\nthe zero-advantage issue, while RePO (Li et al., 2025a) reuses past outputs as off-policy samples\nwith tailored replay strategies (e.g., recency-based, reward-oriented) to improve data efficiency\nand stability. Although these methods enhance sample utilization, they still require substantial\ntraining time, and their trajectory-level credit assignment often leads to slower convergence on\nmulti-turn tasks.\n2.3\nReinforcement Learning in Multi-turn Tasks\nRecent advances in LLMs have increasingly leveraged reinforcement learning to enhance multi-turn\nreasoning and tool use capabilities (Chen et al., 2025; Cheng et al., 2025; Li et al., 2025c). The work\non turn-level credit assignment by Zeng et al. (2025) introduces a novel RL framework that models\nmulti-turn interactive tasks as Markov decision processes, enabling fine-grained credit allocation\nto individual reasoning and tool-use steps within a trajectory. Complementing this, the Search-\n4\n"}, {"page": 5, "text": "R1 framework proposed by Jin et al. (2025b,a) similarly harnesses RL to train LLMs to reason\nabout when and how to use external search engines interactively. By framing the reasoning and\nsearch process as an end-to-end decision-making problem, Search-R1 encourages LLMs to generate\ninformative queries and iteratively refine answers based on real-time feedback from the search\ntool. This work emphasizes the importance of multi-turn interactions to bridge the gap between\ninternal textual reasoning and external information retrieval, demonstrating improved reasoning\nrobustness and answer quality over traditional single-turn or passive retrieval approaches. Despite\nthese advances, there remains a lack of algorithms specifically tailored to stabilize PPO in multi-turn\nLLM off-policy training. Our work addresses this gap by introducing a customized PPO variant that\nexplicitly leverages turn-level structure, leading to more stable and scalable training.\n3\nPreliminaries\nNotation. An autoregressive language model parameterized by θ is defined as a policy πθ. We use\nx to denote a query and D as the query set. Given a response y to a query x, its likelihood under\nthe policy πθ is denoted as πθ(y | x) = Q|y|\nt=1 πθ(yt | x, y<t), where |y| denotes the number of tokens\nin y. A query-response pair (x, y) can be scored by a verifier r, resulting in a reward r(x, y) ∈[0, 1].\nTurn-level\nMDP. We model multi-turn interactions as a Markov decision process\nM = (S, A, P, R, γ) defined at the granularity of turns. A turn is denoted by yk := (ytstart\nk\n, . . . , ytend\nk ),\nand a full trajectory can be written as y := (y1; . . . ; yn) for n turns, where ‘;’ denotes token con-\ncatenation. Each state sk ∈S corresponds to the dialogue context at the beginning of turn k,\nwhich we define as sk := (x, y1, y2, . . . , yk−1). The transition P(sk+1 | sk, ak) updates the context\nwith environment feedback (e.g., results from a tool call), and the reward R(sk, ak) evaluates the\nquality of the turn. The discount factor γ ∈(0, 1] balances present and future rewards. Unlike\ntoken-level MDPs, the unit of interaction here is the turn, with boundaries [tstart\nk\n, tend\nk\n], aligning\ncredit assignment with the natural structure of reasoning and retrieval phases.\nProximal Policy Optimization (PPO). Using samples generated from the old policy πθold,\nPPO constrains the policy update within a proximal region of the old policy through the clipping\nmechanism. This allows for multiple gradient updates to the policy using the same batch of samples.\nSpecifically, PPO employs the following objective for policy optimization:\nJPPO(θ) = Ex∼D, y∼πθold(·|x)\n\n1\n|y|\n|y|\nX\nt=1\nmin\n\u0010\nwt(θ) ˆAt, clip (wt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011\n\n,\n(1)\nwhere the importance ratio of the token yt is defined as\nwt(θ) =\nπθ(yt | x, y<t)\nπθold(yt | x, y<t),\n(2)\nwhere ˆAt denotes the token-level advantage estimated by GAE based on the critic model ˆV . As\ndiscussed in related work, PPO has been widely used in many agentic tasks and performs well in\nsingle-turn settings. However, its performance in multi-turn settings is often unstable and prone\nto collapse; for example, Yuan et al. (2025) show that PPO can fail in long reasoning tasks. In\nmulti-turn settings, reward signals are typically provided only at the end of a long interaction.\nThis delayed feedback leaves value predictions at intermediate states poorly constrained, and the\n5\n"}, {"page": 6, "text": "0\n50\n100\n150\n200\n250\n300\n350\nPolicy Optimization Steps\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nGAE Advantage\n(a) Advantage.\n0\n50\n100\n150\n200\n250\n300\n350\nPolicy Optimization Steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRatio of Valid Action\n(b) Ratio of valid actions.\n0\n50\n100\n150\n200\n250\n300\n350\nPolicy Optimization Steps\n100\n101\n102\n103\n104\n105\n106\n107\n108\nGradient Norm (log scale)\n(c) Policy gradient.\n0\n50\n100\n150\n200\n250\n300\n350\nPolicy Optimization Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSuccess Rate\n(d) Success rate.\nFigure 2: Observations from a failed run with Qwen2.5-7B base model when running token-level\nPPO. From left to right, we show the estimated advantage, the ratio of valid actions (whether the\ntool is successfully called), the L2 norm of the policy gradient, and the success rate of the search\ntask. Each metric is recorded for every training batch.\nresulting errors propagate backward through temporal-difference updates, compounding over many\nsteps (Arjona-Medina et al., 2019). Such delayed and sparse rewards substantially increase bias\nand variance in value estimation. In addition, under off-policy updates the critic must evaluate\nstate–action pairs that may be rarely or never visited by the current policy, further exacerbating\nestimation errors (Munos et al., 2016; Dorka et al., 2022). To summarize, PPO deficiencies in\nmulti-turn environments can be summarized as follows:\n1. PPO trains at the token level, which does not match the turn-level structure of multi-turn\nreasoning. Under off-policy updates, this mismatch becomes more pronounced and often increases\nthe variance of the importance weights.\n2. Off-policy samples in multi-turn tasks frequently contain state-action pairs that the critic has not\nlearned to evaluate well. This leads to unreliable advantage estimates, highly off-policy gradients,\nand in practice, unstable training even with clipping.\nTo further probe instability, we examine a failed run of Qwen2.5-7B under token-level PPO\n(Fig. 2) in multi-turn search tasks. The results highlight two critical issues: (1) around steps 250–300,\nthe critic assigns highly variable advantages while the ratio of valid actions collapses, a typical\nsignature of reward hacking; and (2) these unreliable samples trigger an explosion in the policy\ngradient norm, which rapidly degrades performance and causes a collapse in task success rate. This\nevidence confirms that critic estimation errors and variance accumulation can destabilize off-policy\nupdates in multi-turn settings, motivating the need for stabilization mechanisms beyond turn-level\nscaling.\n4\nProposed Algorithm\nIn this section, we present our approach for stabilizing PPO in multi-turn LLM training. Based on\nthe deficiencies identified in Section 3, we first introduce a turn-level variant of PPO that better\nmatches the granularity of multi-turn reasoning and interaction (Section 4.1). We then propose\na stabilization mechanism that leverages clipping bias to adaptively downweight highly off-policy\nupdates (Section 4.2).\n6\n"}, {"page": 7, "text": "4.1\nTurn-Level PPO Formulation\nWe begin by examining how these deficiencies limit the performance of standard PPO and then\nintroduce a turn-level variant designed to address them For K total turns, with [tstart\nk\n, tend\nk\n] denoting\nthe token positional boundaries for turn k ∈{1, . . . , K}, we propose the turn-level PPO formulation:\nJTurn−PPO(θ) = Ex∼D,y∼πθold(·|x)\n\n1\n|y|\nK\nX\nk=1\ntend\nkX\nt=tstart\nk\nmin\nn\nwturn\nk\n(θ) ˆAt, clip(wturn\nk\n(θ), 1−ϵ, 1+ϵ) ˆAt\no\n\n, (3)\nwhere wturn\nk\nis a turn-level importance sampling weight. Inspired by the sequence-level weight used\nby Zheng et al. (2025), we define a turn-level variant as:\nwturn\nk\n(θ) :=\n\u0012 πθ(yk | x, y<k)\nπθold(yk | x, y<k)\n\u0013\n1\n|yk|\n= exp\n\n1\n|yk|\ntend\nkX\nt=tstart\nk\nlog πθ(yt | x, y<t)\nπθold(yt | x, y<t)\n\n,\n(4)\nwhere the ratio is scaled by the length of the kth turn, |yk|, to stabilize the objective across turns of\nvarying lengths. At first glance, this modification appears minimal, as it only alters the computation\nof importance weights relative to the original PPO formulation in Eq. 1. However, as we formalize\nbelow, it fundamentally changes the structure of credit assignment. To prepare for this analysis, we\ndefine the events under which clipping is inactive for the turn-level objective in Eq. 3:\nB+\nturn := { (k, t) : ˆAt ≥0, wturn\nk\n(θ) ≤1 + ϵ },\nB−\nturn := { (k, t) : ˆAt < 0, wturn\nk\n(θ) ≥1 −ϵ }.\nLet Bturn := B+\nturn ∪B−\nturn. When Bturn holds, the PPO objective reduces to the unclipped form wt ˆAt,\nso the update is fully determined by the critic’s advantage estimates. In contrast, when Bc\nturn occurs\n(clipping is active), it indicates a mismatch between the reference and current policies, and gradients\nof the corresponding tokens are set to zero.\nLemma 4.1. The gradient of the objective function in Eq. 3 is given by\n∇θJTurn−PPO(θ) = E\n\n\n1\n|y|\nK\nX\nk=1\nwturn\nk\n(θ)\nˆAk\n|yk|\n|\n{z\n}\nTurn-Level Credit\n∇θ log πθ(yk|x, y<k)\n\n,\n(5)\nwhere |yk| = tend\nk\n−tstart\nk\n+ 1 and ˆAk := Ptend\nk\nt=tstart\nk\n1{(k,t)∈Bturn} ˆAt.\nProof. We refer to Appendix A.1 for the proof of Lemma 4.1.\nRemark. Lemma 4.1 highlights the effect of modifying the importance sampling ratio from the\ntoken level to the turn level. As shown in Eq. 5, all tokens within a turn share the same aggregated\nadvantage ˆAk, even though both the advantages and the clipping mechanism are still computed\ntoken-wise. The turn-level structure arises because tokens in the same turn use a common geometric\nmean importance ratio, and the final gradient is obtained by aggregating the unclipped token-level\ncontributions within Bturn. Consistent with our following experiments and prior studies (Zhou et al.,\n2024; Zeng et al., 2025), incorporating turn-level credit assignment generally leads to improved\nperformance on multi-turn tasks.\n7\n"}, {"page": 8, "text": "0\n50\n100\n150\n200\n250\n300\nPolicy Optimization Steps\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccess Rate\nTurn-PPO\nToken-PPO\n(a) Success rate.\n0\n50\n100\n150\n200\n250\n300\nPolicy Optimization Steps\n10\n1\n100\n101\n102\n103\nGradient Norm (log scale)\nTurn-PPO\nToken-PPO\n(b) Policy gradient.\n0\n100\n200\n300\n400\n500\nPolicy Optimization Steps\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nGradient Norm\n(c) Norm of PPO loss.\n0\n100\n200\n300\n400\n500\nPolicy Optimization Steps\n100\n101\n102\n103\n104\n105\n106\n107\nGradient Norm (log scale)\n(d) Norm of clipping bias.\nFigure 3: Comparison of token-level versus turn-level PPO training on Qwen2.5-1.5B for the search\ntask (results averaged over 5 runs). (a) Success rates demonstrate that turn-level PPO outperforms\ncompared to token-level PPO. (b) L2 norms of policy gradients show that turn-level PPO exhibits\ngreater training stability. (c–d) Additional diagnostic metrics for token-level PPO: (c) the L2 norm\nof the PPO loss function remains stable throughout training due to gradient clipping, while (d) the\nL2 norm of the clipping bias term grows exponentially over time, validating Lemma 4.2.\nValidation. We first evaluate the performance of token-level PPO in Eq. 1 and turn-level PPO\nin Eq. 3 on the search task using the Qwen2.5-1.5B base model. Fig. 3 reports the average results\nover five runs. As shown in Fig. 3a, turn-level PPO achieves a higher performance (success rate) than\ntoken-level PPO, confirming that eliminating the granularity mismatch improves task performance.\nMoreover, Fig. 3b shows that the average policy gradient norm of turn-level PPO is consistently\nlower, suggesting that turn-level scaling helps stabilize training.\nThese results demonstrate that turn-level PPO alleviates part of the instability. However, when\ntested with larger models such as Qwen2.5-7B, we observe that both token-level and turn-level PPO\nstill suffer from collapse. This observation motivates the development of additional stabilization\nmechanisms, which we introduce in the next section.\n4.2\nStabilizing Off-Policy Training via Clipping-Bias Correction\nWhile turn-level importance sampling improves stability, it does not fully prevent collapse, as highly\noff-policy or unreliable samples can still destabilize updates. To address this, we decompose the\ngradient of the token-level PPO objective by first defining the events under which clipping is inactive.\nSpecifically, we begin by defining the events under which clipping is inactive:\nB+\ntoken := { t : ˆAt ≥0, wt(θ) ≤1 + ϵ },\nB−\ntoken := { t : ˆAt < 0, wt(θ) ≥1 −ϵ }.\nLet Btoken := B+\ntoken∪B−\ntoken. With this notation in place, we can formally decompose the PPO\ngradient as follows:\nLemma 4.2. The PPO gradient can be decomposed as\n∂JPPO(θ)\n∂log πθ\n= E\n\n1\n|y|\n|y|\nX\nt=1\nwt Aπ\nt\n\n\n|\n{z\n}\nOff-Policy term\n+ E\n\n1\n|y|\n|y|\nX\nt=1\nwt\n\u0010\nˆAt −Aπ\nt\n\u0011\n\n\n|\n{z\n}\nAdvantage Estimation Error Term\n−E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Bc\ntoken wt ˆAt\n\n\n|\n{z\n}\nC(θ): Clipping Bias Term\n,\n(6)\nwhere ˆAt denotes the advantage estimate by the critic using the GAE method, while Aπ denotes the\nadvantage estimated by the ground-truth value function V π.\n8\n"}, {"page": 9, "text": "Algorithm 1: Multi-turn PPO with Token- and Turn-Level Importance Sampling (ST-PPO)\nInput: Initialize policy π0, step size η, dataset D, and maximum generation length L.\nfor iteration k = 0, 1, . . . , K −1 do\nData Sampling I: Sample a batch of queries x ∼D and generate agent trajectories τ := y0:L ∼πk.\nData Sampling II: For each trajectory, split it into turn-level state–action pairs (s0, a0), (s1, a1), . . .\n(e.g., using a loss mask or <eot> tokens).\nGradient Estimation: Compute the Turn-PPO gradient as in Eq. 3.\nSurrogate Gradient: Use token-level importance sampling to compute the clipping-error norm and\nform the surrogate gradient in Eq. 7.\nPolicy Improvement: Update the policy using the surrogate gradient in Eq. 7.\nPolicy Evaluation: Update the critic using the temporal difference (TD) error.\nend for\nProof. We refer to Appendix A.2 for the proof of Lemma 4.2.\nFrom the gradient decomposition, we see that the PPO gradient consists of three terms. The\nfirst is the off-policy term, corresponding to the gradient of the vanilla policy gradient algorithm\nwithout clipping, where all tokens contribute equally to the update. The second is the advantage\nestimation error term, which captures the discrepancy between the critic’s estimated advantages\nand the ground-truth ones, thereby introducing both variance and systematic bias. The third is\nthe clipping bias term, arising directly from the indicator 1t∈Bc\ntoken in the clipped objective: while\nclipping stabilizes training by suppressing large updates, it also discards certain token contributions\nand hence introduces bias.\nWe call the third term in Eq. 6 the clipping bias term. To examine its behavior, we conduct\nan experiment with token-level PPO on the Qwen2.5-7B base model and track the L2 norm of this\nterm across training iterations. For comparison, we also report the L2 norm of PPO’s standard loss\n(Fig. 3c) and the clipping bias norm (Fig. 3d). As expected, the standard PPO loss remains relatively\nstable across iterations since clipping suppresses extreme updates. In contrast, the clipping bias\ngrows exponentially and exhibits large oscillations, indicating that a subset of generations contain\noutlier tokens with extreme values, rendering these samples unreliable even after clipping.\nBy focusing updates on more stable, low-error samples, the optimization becomes conservative\nand avoids destabilizing spikes. Motivated by this observation, we propose a surrogate gradient\nestimator that reweights each sampled generation according to its clipping error, thereby ensuring\nstable policy updates. From a scaling perspective, turn-level importance sampling treats each turn\nas a whole, with a weight given by the turn-level importance ratio multiplied by the cumulative\nunclipped advantage and normalized by the turn length. To further enhance stability, we incorporate\nan additional scaling factor derived from the clipping bias term, which downweights highly off-policy\nor risk-prone samples. This leads to a surrogate objective with distinct clipping-bias corrections for\ntoken-level and turn-level PPO, denoted Ctoken(θ) and Cturn(θ), respectively:\nCtoken(θ) := E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Bc\ntoken wt ˆAt\n\n,\nCturn(θ) := E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Bc\nturn wturn\nt\nˆAt\n\n.\n9\n"}, {"page": 10, "text": "Based on these definitions, the surrogate gradients are given by\n∇θJST-PPO(θ) :=\n1\n∥Cturn(θ)∥2\n∇θJTurn-PPO(θ),\n(7)\n∇θJS-PPO(θ) :=\n1\n∥Ctoken(θ)∥2\n∇θJPPO(θ).\n(8)\nFig. 1 illustrates the relations among the four PPO variants we study. Together with S-PPO\n(clipping-bias correction at the token level), these variants complete the set of four algorithms: Token-\nPPO, Turn-PPO, S-PPO, and ST-PPO. Since Turn-PPO mainly serves as an intermediate variant,\nour experiments focus on Token-PPO, S-PPO, and ST-PPO. The overall pipeline is summarized in\nAlgorithm 1.\nTurn-Level Credit Assignment. From Lemma 4.1, after revising the importance sampling,\nour algorithm is able to assign turn-level credit to each interaction, independent of the specific choice\nof advantage estimator (e.g., GAE or Monte Carlo). This removes the mismatch between the level\nof interaction (turns) and the level at which credit is assigned.\nOptimality. In Eqs. 7 and 8, we stabilize the gradient by normalizing it with the clipping\nbias term. For each sample, both the original turn-level PPO and ST-PPO update along gradient\ndirections that are aligned, differing only by normalization. As a result, ST-PPO preserves the same\noptimality conditions as Turn-PPO while providing more stable policy improvement.\nGenerality. The formulation is flexible enough to recover existing LLM training methods (e.g.,\nGRPO, RLOO) through specific choices of advantage design. This generality allows algorithms\ndeveloped for advantage estimation to be directly applied as special cases within our framework.\n5\nExperiments\nWe conduct comprehensive numerical evaluations of our proposed ST-PPO algorithm (Algorithm 1)\nand compare its performance against the state-of-the-art baseline, Search-R1 Jin et al. (2025b). Our\nexperimental results demonstrate two key advantages of the proposed approach: (1) the combination\nof turn-level importance sampling and clipping-bias correction significantly enhances training stability\ncompared to standard PPO, preventing the performance collapses commonly observed in multi-turn\nagent training, and (2) ST-PPO and S-PPO maintain consistently lower clipping ratios throughout\ntraining, indicating more reliable gradient updates and improved sample efficiency.\nExperimental Setup. Following the experimental setup of Search-R1 Jin et al. (2025b), we\nconduct our experiments using the Qwen-2.5-7B (Base) model (Bai et al., 2023) as the base policy.\nFor retrieval, we employ the 2018 Wikipedia dump (Karpukhin et al., 2020) as our knowledge source\nand the E5 embeddings (Wang et al., 2022) as the retriever. To ensure fair comparison with existing\nmethods, we follow Lin et al. (2023) and consistently retrieve three passages across all retrieval-\nbased approaches. We evaluate ST-PPO on three benchmark datasets: (1) Natural Questions\n(NQ) (Kwiatkowski et al., 2019), (2) HotpotQA (Ho et al., 2020) and (3) Medical Multiple-Choice\nQuestions (Jin et al., 2020; Pal et al., 2022; Jin et al., 2019; Wang et al., 2024; Zuo et al., 2025).\nThese datasets present diverse multi-hop reasoning and search challenges that require effective\ncoordination between information retrieval and reasoning steps, making them ideal testbeds for\nevaluating the stability and effectiveness of our turn-level optimization approach. For complete\nimplementation details, refer to Appendix A.3.\n10\n"}, {"page": 11, "text": "0\n50\n100\n150\n200\n250\n300\n350\n400\nPolicy Optimization Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSuccess Rate\nST-PPO\nS-PPO\nToken-PPO\nGRPO\n(a) Scores on the NQ dataset.\n0\n50\n100\n150\n200\n250\n300\n350\n400\nPolicy Optimization Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSuccess Rate\nST-PPO\nS-PPO\nToken-PPO\nGRPO\n(b) Scores on the HotpotQA dataset.\nFigure 4: Experimental results of Qwen-2.5-7B policy models, with the value model also trained\nfrom Qwen-2.5-7B. Results are averaged over three trials. We report the average success rate on the\nNQ and HotpotQA datasets.\nEvaluation. For evaluation, we evaluate on the test sets of the NQ and HotpotQA datasets to\nassess model performance on each domain. Exact Match (EM) is used as the primary evaluation\nmetric, which measures the percentage of predictions that exactly match the ground-truth answers\nafter standard normalization. EM provides a strict assessment of whether our multi-turn approach\nsuccessfully retrieves and synthesizes correct information through the search and reasoning process.\nTraining Stability and Performance. Fig. 4 compares the training dynamics of GRPO(Liu\net al., 2024), Token-level PPO, ST-PPO, and S-PPO on the NQ and HotpotQA datasets. All\nmethods demonstrate rapid initial improvement, confirming that PPO can identify effective policies\nearly in training. However, their long-term behaviors diverge dramatically. Token-level PPO exhibits\nmarked instability: after reaching peak performance, we observe sharp performance collapses that\nrender the learned policy significantly worse than earlier checkpoints. GRPO shows a similar trend\nand collapses even earlier, with training breaking down midway and subsequent performance becoming\nunstable, indicating that trajectory-level credit assignment still fails to prevent variance-driven\ncollapse in multi-turn settings. This degradation pattern aligns with our theoretical analysis, where\nwe identified that critic estimation errors on tokens with high importance weights can destabilize\ngradient updates. Such instability necessitates careful early stopping to preserve the best-performing\ncheckpoint, complicating the training process. In contrast, ST-PPO and S-PPO maintain consistent\nupward progress throughout the entire optimization horizon without performance collapses.\nFig. 5a further shows that both ST-PPO and S-PPO achieve significantly lower clipping ratios\nthan PPO, while Fig. 5b demonstrates consistently smaller KL divergence during optimization.\nThese results confirm that our methods reduce the magnitude of unstable updates and improve\ntraining safety. In Appendix A.4, we conduct an experiment under a more off-policy setting, and the\nresults show that our method remains more stable than the baseline algorithm.\nThis stability stems from two key mechanisms: (1) the turn-level importance sampling aligns\ncredit assignment with the natural structure of reasoning-search interactions, reducing the granularity\nmismatch that contributes to instability, and (2) the clipping-bias correction adaptively scales gradient\nupdates based on sample reliability, reducing the influence of tokens with high variance or poor\ncritic estimates. As a result, ST-PPO and S-PPO achieve both competitive peak performance and\nrobust convergence without requiring early stopping or careful checkpoint selection.\n11\n"}, {"page": 12, "text": "0\n50\n100\n150\n200\n250\n300\n350\nPolicy Optimization Steps\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nClipping Ratio\nST-PPO\nS-PPO\nToken-PPO\nGRPO\n(a) Clipping Ratio on the HotpotQA dataset.\n0\n50\n100\n150\n200\n250\n300\n350\nPolicy Optimization Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nKL Divergence\nST-PPO\nS-PPO\nToken-PPO\nGRPO\n(b) KL divergence on the HotpotQA dataset.\nFigure 5: We also report (a) the clipping ratio and (b) the KL divergence during policy optimization.\nBoth ST-PPO and S-PPO achieve lower clipping ratios and KL divergence compared to vanilla PPO,\nindicating more stable training dynamics. For GRPO, we do not report values beyond the 160th\nstep because the algorithm collapses around that point, after which the metrics become NaN.\n5.1\nExperimental Setup on Medical Multiple-Choice Benchmarks\nIn this section, we further evaluate the performance of ST-PPO on medical tasks to assess its\ngeneralization ability in a domain distinct from open-domain search.\nSpecifically, we use the\nAlphaMed19K (Liu et al., 2025) dataset for training, with Wikipedia serving as the retrieval source.\nTo ensure consistency across experiments and datasets, we adopt the prompt template shown in\nAppendix A.5 for all training runs. The hyperparameter configuration for the medical dataset follows\nthe same settings used in the NQ and HotpotQA experiments.\nBaselines. We compare our method against several 8B-scale baselines commonly used in\nagentic tasks: (1) RAG: Retrieval-augmented generation (Glass et al., 2022; Li et al., 2025b)\nusing the tool-following instructions, where external documents are retrieved to supplement the\nmodel’s responses. Note that the Llama-3.1-8B-Instruct model is used without additional training\nin this setting. (2) COT: Chain-of-Thought prompting (Wei et al., 2022), which encourages the\nmodel to generate explicit reasoning steps before producing an answer. (3) Search-R1: Search-R1\nframework Jin et al. (2025b), where we train the model using PPO and evaluate using the checkpoint\nat the 150th optimization step (2 epochs). (4) ST-PPO: Our proposed algorithm, evaluated using\nthe checkpoint at the 150th step (2 epochs).\nEvaluation Datasets. To comprehensively evaluate the algorithm’s performance on medical\ntasks, we consider both in-domain and out-of-domain benchmarks. For in-domain evaluation, we\nuse MedQA (Jin et al., 2020) and MedMCQA (Pal et al., 2022). For out-of-domain evaluation,\nwe include PubMedQA (Jin et al., 2019), the medical subset of MMLU (Wang et al., 2024), and\nMedXpert (Zuo et al., 2025). We exclude GPQA (Rein et al., 2024) as it does not contain a medically\nrelevant subset.\nPerformance. Table 1 shows that our training method not only stabilizes multi-turn optimization\nbut also enables a model that initially lacks tool-use ability to reliably perform retrieval and acquire\ndomain knowledge. The poor performance of RAG demonstrates this initial limitation. As a result,\nST-PPO achieves the strongest overall performance on medical QA benchmarks, with the best\naverage accuracy (49.90%) among retrieval-augmented and RL-enhanced methods, outperforming\n12\n"}, {"page": 13, "text": "Table 1: Performance of 8B LLMs across in-domain† and out-of-domain∗benchmarks (accuracy %).\nBold numbers indicate the best performance within each model family. Inference-based methods\nrely solely on the model’s internal knowledge, while RL methods retrieve external information or use\nreinforcement learning to improve answers.\nModel\nIn-Domain†\nOut-of-Domain∗\nAvg.\nMedQA\nMedMCQA\nPubMedQA\nMMLU-M\nMedXpert\nInference-Based Methods (No Retrieval)\nLlama-3.1-8B-Instruct (Direct Inference)\n45.20\n52.40\n62.00\n40.70\n13.00\n42.66\nLlama-3.1-8B-Instruct (CoT)\n48.62\n59.80\n64.40\n54.20\n13.02\n48.01\nRetrieval-Augmented and RL-Enhanced Methods\nLlama-3.1-8B-Instruct (RAG)\n8.90\n11.30\n16.80\n4.90\n9.20\n10.22\nLlama-3.1-8B-Instruct (Search-R1)\n53.40\n54.00\n60.20\n49.50\n9.77\n45.37\nLlama-3.1-8B-Instruct (ST-PPO, Ours)\n58.90\n57.90\n64.80\n53.50\n14.40\n49.90\n†In-domain benchmarks. ∗Out-of-domain benchmarks.\nvanilla PPO in Search-R1 framework across both in-domain and out-of-domain evaluations.\n6\nConclusion\nIn this work, we analyzed why PPO becomes unstable in multi-turn LLM agent training and identified\ntwo main causes: the mismatch between token-level optimization and turn-level credit assignment,\nand the high variance from off-policy tokens. Through a theoretical decomposition of PPO’s gradient\nupdates, we showed how these factors lead to unstable convergence and occasional collapse. To\naddress this, we proposed ST-PPO, which integrates token- and turn-level importance sampling\nwith clipping-bias correction to yield more conservative and stable updates. Empirically, ST-PPO\nproduced consistently smooth training curves, whereas PPO often required early stopping. These\nresults highlight the value of matching optimization granularity with task structure and point toward\nsafer reinforcement learning methods for multi-turn LLM agents.\n13\n"}, {"page": 14, "text": "References\nArash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,\nAhmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning\nfrom human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.\nJose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brand-\nstetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. Advances in\nNeural Information Processing Systems, 32, 2019.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen\nZhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement\nlearning. arXiv preprint arXiv:2503.19470, 2025.\nXing Chen, Dongcui Diao, Hechang Chen, Hengshuai Yao, Haiyin Piao, Zhixiao Sun, Zhiwei Yang,\nRandy Goebel, Bei Jiang, and Yi Chang. The sufficiency of off-policyness and soft clipping: Ppo\nis still insufficient according to an off-policy measure. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pp. 7078–7086, 2023.\nJie Cheng, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Gang Xiong, Yisheng Lv, and Fei-Yue Wang.\nStop summation: Min-form credit assignment is all process reward model needs for reasoning.\narXiv preprint arXiv:2504.15275, 2025.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu\nYu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint\narXiv:2502.01456, 2025.\nNicolai Dorka, Tim Welschehold, Joschka Bödecker, and Wolfram Burgard. Adaptively calibrated\ncritic estimates for deep reinforcement learning. IEEE Robotics and Automation Letters, 8(2):\n624–631, 2022.\nAhmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan,\nFrancis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming\nwith large reasoning models. arXiv preprint arXiv:2502.06807, 2025.\nJiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,\nJinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms.\narXiv preprint arXiv:2504.11536, 2025.\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and\nAlfio Gliozzo. Re2g: Retrieve, rerank, generate. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 2701–2715, 2022.\nJiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, and Alejandro Ribeiro. Give:\nStructured reasoning of large language models with knowledge graph inspired veracity extrapolation.\narXiv preprint arXiv:2410.08475, 2024.\n14\n"}, {"page": 15, "text": "Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, and Alejandro Ribeiro. Self-\ngive: Associative thinking from limited structured knowledge for enhanced large language model\nreasoning. arXiv preprint arXiv:2505.15062, 2025.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop\nqa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060,\n2020.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\nNarasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint\narXiv:2310.06770, 2023.\nBowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O Arik, and Jiawei Han. An empirical study on\nreinforcement learning for reasoning-search interleaved llm agents. arXiv preprint arXiv:2505.15117,\n2025a.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and\nJiawei Han. Search-r1: Training LLMs to reason and leverage search engines with reinforcement\nlearning. In Second Conference on Language Modeling (COLM), 2025b.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have. A Large-scale Open Domain Question Answering Dataset from\nMedical Exams. arXiv [cs. CL], 2020.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A dataset\nfor biomedical research question answering. In Proceedings of the 2019 conference on empirical\nmethods in natural language processing and the 9th international joint conference on natural\nlanguage processing (EMNLP-IJCNLP), pp. 2567–2577, 2019.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP\n(1), pp. 6769–6781, 2020.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, 7:453–466, 2019.\nSiheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, and Chaochao Lu. Repo: Replay-enhanced policy\noptimization. arXiv preprint arXiv:2506.09340, 2025a.\nXiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Ye Qi, and Zhicheng Dou. Retrollm:\nEmpowering large language models to retrieve fine-grained evidence within generation. In Pro-\nceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 16754–16779, 2025b.\n15\n"}, {"page": 16, "text": "Xuefeng Li, Haoyang Zou, and Pengfei Liu.\nTorl: Scaling tool-integrated rl.\narXiv preprint\narXiv:2503.23383, 2025c.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro\nRodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual\ninstruction tuning. In The Twelfth International Conference on Learning Representations, 2023.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437, 2024.\nChe Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel\nRueckert, and Rossella Arcucci. Beyond distillation: Pushing the limits of medical llm reasoning\nwith minimalist rule-based rl. arXiv preprint arXiv:2505.17952, 2025.\nFanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy\noptimization for gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025.\nRémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy\nreinforcement learning. Advances in neural information processing systems, 29, 2016.\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust\nregion method for continuous control. arXiv preprint arXiv:1707.01891, 2017.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems, 35:27730–\n27744, 2022.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Conference on health,\ninference, and learning, pp. 248–260. PMLR, 2022.\nDoina Precup, Richard S Sutton, and Satinder Singh. Eligibility traces for off-policy policy evaluation.\n2000.\nZehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang,\nJiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum\nreinforcement learning. arXiv preprint arXiv:2411.02337, 2024.\nCheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur,\nand Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\nin neural information processing systems, 36:53728–53741, 2023.\n16\n"}, {"page": 17, "text": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R Bowman.\nGpqa: A graduate-level google-proof q&a\nbenchmark. In First Conference on Language Modeling, 2024.\nNicolas Le Roux, Marc G Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves,\nAlex Fréchette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sándor Toth, and Sam Work. Ta-\npered off-policy reinforce: Stable and efficient reinforcement learning for llms. arXiv preprint\narXiv:2503.14286, 2025.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. Advances in Neural Information Processing Systems, 36:68539–68551,\n2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in\nopen language models. arXiv preprint arXiv:2402.03300, 2024.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36:8634–8652, 2023.\nKimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen,\nYanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv\npreprint arXiv:2507.20534, 2025.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint\narXiv:2212.03533, 2022.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-\ntask language understanding benchmark. Advances in Neural Information Processing Systems, 37:\n95266–95290, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837, 2022.\nZhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang\nXu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn\nreinforcement learning. arXiv preprint arXiv:2505.16421, 2025.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3):229–256, 1992.\n17\n"}, {"page": 18, "text": "Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang.\nLearning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations (ICLR), 2023.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian\nFan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system\nat scale. arXiv preprint arXiv:2503.14476, 2025.\nYufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in\nlong-cot? value optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025.\nSiliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong.\nReinforcing multi-turn reasoning in llm agents via turn-level credit assignment. arXiv preprint\narXiv:2505.11821, 2025.\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong\nLiu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization.\narXiv preprint arXiv:2507.18071, 2025.\nYifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language\nmodel agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.\nYuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding,\nand Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding.\narXiv preprint arXiv:2501.18362, 2025.\nA\nAppendix\nA.1\nProof of Lemma 4.1\nThe proof establishes turn-level credit assignment by first reformulating the PPO objective with\nindicator functions, then computing how the gradient with respect to a single token depends only on\nits containing turn due to locality. The key insight is that the turn-level importance weight derivative\nyields a scaling factor of wturn\nk\n(θ)\n|yk|\n, where the\n1\n|yk| term arises from the geometric mean structure of\nthe turn-level importance ratio. Finally, aggregating over all tokens via the multivariable chain rule\nproduces the claimed gradient form where each turn receives credit proportional to wturn\nk\n(θ) ˆ\nAk\n|yk|,\ncombining importance weighting, accumulated advantage, and length normalization.\nWe now proceed with the lemma proof. We derive the gradient of the Turn-PPO objective\n18\n"}, {"page": 19, "text": "JTurn−PPO(θ) defined in Eq. 3. Beginning with the objective in expanded form:\nJTurn−PPO(θ)\n= Ex∼D, y∼πθold(·|x)\n\n1\n|y|\nK\nX\nk=1\ntend\nkX\nt=tstart\nk\nmin\nn\nwturn\nk\n(θ) ˆAt, clip(wturn\nk\n(θ), 1 −ϵ, 1 + ϵ) ˆAt\no\n\n\n= Ex∼D, y∼πθold(·|x)\n\n1\n|y|\nK\nX\nk=1\ntend\nkX\nt=tstart\nk\n\u0010\n1(k,t)∈Bturnwturn\nk\n(θ) ˆAt + 1(k,t)∈Bc\nturnclip(wturn\nk\n(θ), 1−ϵ, 1+ϵ) ˆAt\n\u0011\n\n,\n(9)\nwhere1 Eq. 9 follows from decomposing the min function using indicator functions. The indicator\nfunction 1(k,t)∈Bturn = 1, if the token index t within turn k belongs to the set Bturn and 0 otherwise.\nThe set Bturn = B+\nturn ∪B−\nturn represents the indices of tokens within turns where clipping is inactive.\nTogether, the set Bturn and its complement Bc partition the space of token indices across all turns,\nsatisfying Bturn ∪Bc\nturn = {1, 2, . . . , |y|} and Bturn ∩Bc\nturn = ∅. For token indices (k, t) ∈Bturn, the\nmin operation selects the unclipped term wturn\nk\n(θ) ˆAt; otherwise, for t ∈Uc, it selects the clipped\nterm clip(wturn\nk\n(θ), 1 −ϵ, 1 + ϵ) ˆAt.\nTaking the gradient with respect to log πθ(yt′|x, y<t′) where t′ ∈[tstart\nk′\n, tend\nk′ ] (i.e., token t′ belongs\nto turn k′), we get\n∂JTurn−PPO(θ)\n∂log πθ(yt′|x, y<t′) = E\n\n1\n|y|\nK\nX\nk=1\ntend\nkX\nt=tstart\nk\n1(k,t)∈Bturn\n∂wturn\nk\n(θ)\n∂log πθ(yt′|x, y<t′)\nˆAt\n\n\n(10)\n= E\n\n1\n|y|\ntend\nk′\nX\nt=tstart\nk′\n1(k,t)∈Bturn\n∂wturn\nk′\n(θ)\n∂log πθ(yt′|x, y<t′)\nˆAt\n\n,\n(11)\nwhere Eq. 10 follows since clipped terms have zero gradients by definition of the clipping function;\nand Eq. 11 follows since wturn\nk\n(θ) depends only on tokens within turn k. Since token t′ belongs to\nturn k′, we have\n∂wturn\nk\n(θ)\n∂log πθ(yt′|x,y<t′) = 0 for all k ̸= k′ due to disjoint turn boundaries.\nNext, we evaluate the derivative of wturn\nk′\n(θ) with respect to the log probability. Recall that\n1For notational brevity, we drop the expectation subscript in what follows, with all expectations taken over the\nsame joint distribution where x ∼D and y ∼πθold(·|x).\n19\n"}, {"page": 20, "text": "wturn\nk′\n(θ) is defined as\nwturn\nk′\n(θ) =\n \nπθ(yk′|x, y<k′)\nπθold(yk′|x, y<k′)\n!\n1\n|yk′ |\n=\n\n\ntend\nk′\nY\nt=tstart\nk′\nπθ(yt|x, y<t)\nπθold(yt|x, y<t)\n\n\n1\n|yk′ |\n= exp\n\n1\n|yk′|\ntend\nk′\nX\nt=tstart\nk′\n[log πθ(yt|x, y<t) −log πθold(yt|x, y<t)]\n\n\n= exp\n\n−1\n|yk′|\ntend\nk′\nX\nt=tstart\nk′\nlog πθold(yt|x, y<t)\n\n· exp\n\n1\n|yk′|\ntend\nk′\nX\nt=tstart\nk′\nlog πθ(yt|x, y<t)\n\n,\n(12)\nwhere the first exponential term in Eq. 12 is constant with respect to θ.\nWe can write\nwturn\nk′\n(θ) = κ · exp (f(θ)) where\nκ = exp\n\n−1\n|yk′|\ntend\nk′\nX\nt=tstart\nk′\nlog πθold(yt|x, y<t)\n\n,\nf(θ) =\n1\n|yk′|\ntend\nk′\nX\nt=tstart\nk′\nlog πθ(yt|x, y<t).\n(13)\nConsequently, the derivative of wturn\nk′\n(θ) with respect to the log probability is evaluated as\n∂wturn\nk′\n(θ)\n∂log πθ(yt′|x, y<t′) = κ · exp (f(θ)) ·\n∂f(θ)\n∂log πθ(yt′|x, y<t′)\n(14)\n= wturn\nk′\n(θ) ·\n∂\n∂log πθ(yt′|x, y<t′)\n\n1\n|yk′|\ntend\nk′\nX\nt=tstart\nk′\nlog πθ(yt|x, y<t)\n\n\n= wturn\nk′\n(θ) ·\n1\n|yk′| · ∂log πθ(yt′|x, y<t′)\n∂log πθ(yt′|x, y<t′)\n(15)\n= wturn\nk′\n(θ) ·\n1\n|yk′|,\n(16)\nwhere Eq. 14 follows from the chain rule; and Eq. 15 follows since only the term with index t = t′ in\nthe summation depends on log πθ(yt′|x, y<t′). Substituting Eq. 16 back into Eq. 11, we get\n∂JTurn−PPO(θ)\n∂log πθ(yt′|x, y<t′) = E\n\nwturn\nk′\n(θ)\n|y| · |yk′|\ntend\nk′\nX\nt=tstart\nk′\n1(k,t)∈Bturn ˆAt\n\n.\n(17)\n20\n"}, {"page": 21, "text": "Finally, from Eq. 9, the full gradient is given by\n∇θJTurn−PPO(θ) =\nK\nX\nk=1\ntend\nkX\nt=tstart\nk\n∂JTurn−PPO(θ)\n∂log πθ(yt|x, y<t)∇θ log πθ(yt|x, y<t)\n(18)\n=\nK\nX\nk=1\ntend\nkX\nt=tstart\nk\nE\n\nwturn\nk\n(θ)\n|y| · |yk|\ntend\nkX\nj=tstart\nk\n1(k,j)∈Bturn ˆAj\n\n∇θ log πθ(yt|x, y<t)\n(19)\n= E\n\n\nK\nX\nk=1\nwturn\nk\n(θ)\n|y| · |yk|\n\n\ntend\nkX\nj=tstart\nk\n1(k,j)∈Bturn ˆAj\n\n\ntend\nkX\nt=tstart\nk\n∇θ log πθ(yt|x, y<t)\n\n\n= E\n\n\nK\nX\nk=1\nwturn\nk\n(θ)\n|y| · |yk|\n\n\ntend\nkX\nj=tstart\nk\n1(k,j)∈Bturn ˆAj\n\n∇θ log πθ(yk|x, y<k)\n\n\n(20)\n= E\n\"\n1\n|y|\nK\nX\nk=1\nwturn\nk\n(θ)\nˆAk\n|yk|∇θ log πθ(yk|x, y<k)\n#\n,\n(21)\nwhere Eq. 18 applies the multivariable chain rule; Eq. 19 follows from Eq. 17; Eq. 20 uses the chain\nrule identity ∇θ log πθ(yk|x, y<k) = Ptend\nk\nt=tstart\nk\n∇θ log πθ(yt|x, y<t); and Eq. 21 follows by defining\nˆAk := Ptend\nk\nj=tstart\nk\n1(k,j)∈Bturn ˆAj. This completes the proof of Lemma 4.1.\n■\nA.2\nProof of Lemma 4.2\nThe token-level PPO objective function can be expressed as\nJPPO(θ) = Ex∼D, y∼πθold(·|x)\n\n1\n|y|\n|y|\nX\nt=1\nmin(wt(θ) ˆAt, clip(wt(θ), 1 −ϵ, 1 + ϵ) ˆAt)\n\n\n= Ex∼D, y∼πθold(·|x)\n\n1\n|y|\n|y|\nX\nt=1\n\u0010\n1t∈Btoken · wt(θ) ˆAt + 1t∈Bc\ntoken · clip(wt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011\n\n,\n(22)\nwhere Eq. 22 follows from the definition of the event Btoken and\nclip(w, 1 −ϵ, 1 + ϵ) = max(min(w, 1 + ϵ), 1 −ϵ) =\n\n\n\n\n\n1 −ϵ\nif w < 1 −ϵ,\nw\nif 1 −ϵ ≤w ≤1 + ϵ,\n1 + ϵ\nif w > 1 + ϵ.\n(23)\n21\n"}, {"page": 22, "text": "Taking the gradient with respect to θ, we obtain\n∇θJPPO(θ) = E\n\n1\n|y|\n|y|\nX\nt=1\n\u0010\n1t∈Btoken · ∇θwt(θ) ˆAt + 1t∈Bc\ntoken · ∇θ clip(wt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011\n\n\n= E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Btoken · ∇θwt(θ) ˆAt\n\n\n(24)\n= E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Btokenwt(θ)∇θ log πθ(yt|x, y<t) ˆAt\n\n,\n(25)\nwhere Eq. 24 follows from the definition of the event Bc that yields\n1t∈Bc\ntoken · ∇θclip(w, 1 −ϵ, 1 + ϵ) ˆAt =\n(\n∇θ(1 −ϵ) ˆAt = 0\nif ˆAt < 0 and wt < 1 −ϵ,\n∇θ(1 + ϵ) ˆAt = 0\nif ˆAt ≥0 and wt > 1 + ϵ;\n(26)\nand Eq. 25 readily follows from the definition of wt(θ) that leads to\n∇θwt(θ) = ∇θ\nπθ(yt|x, y<t)\nπold(yt|x, y<t) = wt(θ)∇θ log πθ(yt|x, y<t).\n(27)\nFinally, we verify that RHS of Lemma 4.2 is equal to Eq. 25 as follows:\nE\n\n1\n|y|\n|y|\nX\nt=1\nwt∇θ log πθAπ\nt\n\n+ E\n\n1\n|y|\n|y|\nX\nt=1\nwt∇θ log πθ( ˆAt−Aπ\nt )\n\n−E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Bc\ntokenwt∇θ log πθ ˆAt\n\n\n= E\n\n1\n|y|\n|y|\nX\nt=1\nwt∇θ log πθ ˆAt\n\n−E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Bc\ntokenwt∇θ log πθ ˆAt\n\n\n= E\n\n1\n|y|\n|y|\nX\nt=1\n1t∈Btokenwt∇θ log πθ ˆAt\n\n,\n(28)\nwhere Eq. 28 follows from (1 −1t∈Bc\ntoken) = 1t∈Btoken. This completes the proof of Lemma 4.2.\n■\nA.3\nImplementation Details\nRetrieval System. A local retriever service is deployed and accessed via an HTTP interface.\nFor each user query, we consistently return the top three passages. The dialogue is restricted to a\nmaximum of three interaction turns.\nTraining Process. All experiments are run on 8 NVIDIA H100 GPUs. We activate gradient\ncheckpointing and train under a Fully Sharded Data Parallel (FSDP) setup with offloading enabled\nfor parameters, gradients, and optimizers. The policy network is optimized with a learning rate\nof 1 × 10−6, while the critic is trained with 1 × 10−5. Optimization proceeds for 4 epochs, with\nwarm-up ratios of 0.285 (policy) and 0.015 (critic). The effective batch size is 512, subdivided into\nmini-batches of 256 and micro-batches of 64 for policy updates and 8 for critic updates. Generalized\n22\n"}, {"page": 23, "text": "Advantage Estimation (GAE) is employed with λ = 1 and γ = 1. Input sequences are truncated to\nat most 4,096 tokens, with limits of 500 tokens for responses, 2,048 tokens for the initial context,\nand 500 tokens for retrieved passages. We adopt turn-level importance sampling combined with\nvariance-reduction detachment to improve training stability. Regularization follows PPO conventions\nwith a KL coefficient of 0.001 and a clipping threshold of 0.2. Rollouts are generated using vLLM\nwith tensor parallel size 4, GPU memory utilization set to 0.6, temperature fixed at 1.0, and top-p\nsampling of 1.0. The rollout and reference log-probability calculations both use a micro-batch size\nof 128.\nTurn Boundary Identification. To implement turn-level importance sampling, we need\nto identify turn boundaries within the multi-turn trajectories. In our search task setup, we use\nthe loss mask to distinguish between agent-generated content and environment responses: LLM-\ngenerated reasoning and query formulation steps are marked with 1 in the loss_mask, while retrieved\ndocument content is marked with 0. We define turn boundaries by grouping consecutive tokens with\nloss_mask = 1 as complete turns (corresponding to the agent’s actions in our turn-level MDP),\nwhile consecutive 0s represent states (i.e., retrieved content that provides environmental feedback).\nThese identified turn boundaries enable our algorithm to apply turn-level importance sampling and\ncredit assignment.\nA.4\nSupplementary Experiments\nIn the following experiments, we set the training batch size to 512 and the mini-batch size to 128,\nwhich results in one on-policy update and three subsequent off-policy updates per batch. Under this\nmore off-policy setting, our method demonstrates greater stability and more controlled performance\ncompared to the baseline.\nFigure 6 shows that the clipping bias grows steadily throughout training, indicating that the\ninfluence of off-policy data becomes increasingly significant. This growth is mainly driven by two\nfactors: the variance of importance sampling ratios increases as the training distribution drifts away\nfrom the current policy, and the critic provides less accurate estimates on off-policy batches. As\na result, the discrepancy between the true gradient and the clipped surrogate objective becomes\namplified, leading to large and persistent bias in later stages of training. This observation highlights\nthe necessity of employing the clipping ratio to reweight samples, as it counteracts the variance\nexplosion of importance sampling and mitigates the critic’s misestimation on off-policy data, thereby\nmaintaining more stable optimization.\nA.5\nDemonstration of Medical Task\nPrefix Prompt for the Medical Task\nAnswer the given medical multiple choice question. Think step-by-step inside <think> and\n</think> tags.\nWhen you encounter:\n• Unfamiliar medical terminology or drug names\n• Complex disease mechanisms you’re uncertain about\n• Specific treatment protocols or guidelines you need to verify\n23\n"}, {"page": 24, "text": "0\n50\n100\n150\n200\n250\n300\n350\n400\nPolicy Optimization Steps\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nSuccess Rate\nST-PPO\nS-PPO\nToken-PPO\n0\n50\n100\n150\n200\n250\n300\n350\n400\nPolicy Optimization Steps\n100\n101\n102\n103\n104\n105\n106\n107\nClipping Bias\nST-PPO\nS-PPO\nFigure 6: Experimental results of Qwen-2.5-7B policy models, with the value model also trained\nfrom Qwen-2.5-7B. Results are averaged over three trials. Left: Success Rate, Right: The norm of\nClipping Bias.\n• Any information where you lack confidence\nYou can search for clarification using <search> query </search>, and results will appear between\n<information> and </information>. Use search strategically to fill knowledge gaps and improve\nanswer accuracy. After sufficient reasoning and any necessary searches, provide your final answer\ninside <answer> and </answer> with ONLY the letter (e.g., <answer>A</answer>).\n24\n"}, {"page": 25, "text": "Table 2: A rollout example of answering a medical question.\nQuestion: Which one of the following is not included in the treatment of malignant melanoma?\nOptions:\nA: Radiation\nB: Surgical excision\nC: Chemotherapy\nD: Immunotherapy\n<think> Let’s break down the question step by step! We’re looking for the option that is not\nincluded in the treatment of malignant melanoma. </think>\n<think> A: Radiation - Hmm, radiation therapy is sometimes used in combination with surgery\nor as palliative care for symptomatic relief. Not sure if it’s the odd one out.\nB: Surgical excision - Ah, surgery is a standard treatment for removing the primary tumor and\nany involved lymph nodes. Yes, that’s a correct treatment option.\nC: Chemotherapy - Chemotherapy can be used in some cases of malignant melanoma, particularly\nfor patients with advanced disease or those who have recurrence after initial treatment. Yes,\nthat’s a valid treatment option.\nD: Immunotherapy - Immunotherapy, such as pembrolizumab (Keytruda) or nivolumab (Opdivo),\nhas shown significant efficacy in treating advanced melanoma by enhancing the body’s immune\nresponse against cancer cells. Yes, that’s a commonly used treatment option!\nI’m getting confused about the radiation option, so let me search for clarification. </think>\n<search> What is the role of radiation therapy in treating malignant melanoma? </search>\n<information> Doc 1 (Melanoma): Radiation therapy is often used after surgical resection for\nlocally or regionally advanced or unresectable metastases. It may reduce local recurrence but\ndoes not prolong survival. Doc 2 (Radiation therapy): Melanoma is generally radioresistant;\nradiation is mainly palliative for metastatic disease. Doc 3 (Radiation therapy): Radiation\nrequires higher doses for melanoma than most cancers and is not curative. </information>\n<think> Ah-ha! It seems that radiation therapy has a limited role in treating malignant\nmelanoma and is mostly used in palliative care settings. It’s not the first-line treatment for the\ndisease. So, I think I can confidently say that... </think>\n<answer> A </answer>\n25\n"}]}