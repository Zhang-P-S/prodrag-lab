{"doc_id": "arxiv:2601.18796", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.18796.pdf", "meta": {"doc_id": "arxiv:2601.18796", "source": "arxiv", "arxiv_id": "2601.18796", "title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models", "authors": ["Brian Ondov", "Chia-Hsuan Chang", "Yujia Zhou", "Mauro Giuffrè", "Hua Xu"], "published": "2026-01-26T18:58:46Z", "updated": "2026-01-26T18:58:46Z", "summary": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.18796v1", "url_pdf": "https://arxiv.org/pdf/2601.18796.pdf", "meta_path": "data/raw/arxiv/meta/2601.18796.json", "sha256": "b61625acf6385d0400823a231fcf9df330a4bd4845fe55e3eebc38c975ca6616", "status": "ok", "fetched_at": "2026-02-18T02:20:27.277271+00:00"}, "pages": [{"page": 1, "text": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with\nEmbedding Language Models\nBrian Ondov*, Chia-Hsuan Chang*, Yujia Zhou, Mauro Giuffrè and Hua Xu\nDepartment of Biomedical Informatics & Data Science\nYale School of Medicine\nNew Haven, CT, USA\n{brian.ondov,chia-hsuan.chang,yujia.zhou,mauro.giuffre,hua.xu}@yale.edu\nAbstract\nText embeddings have become an essential part\nof a variety of language applications. However,\nmethods for interpreting, exploring and revers-\ning embedding spaces are limited, reducing\ntransparency and precluding potentially valu-\nable generative use cases. In this work, we\nalign Large Language Models to embeddings\nof clinical trials using the recently reported Em-\nbedding Language Model (ELM) method. We\ndevelop an open-source, domain-agnostic ELM\narchitecture and training framework, design\ntraining tasks for clinical trials, and introduce\nan expert-validated synthetic dataset. We then\ntrain a series of ELMs exploring the impact of\ntasks and training regimes. Our final model,\nctELM, can accurately describe and compare\nunseen clinical trials from embeddings alone\nand produce plausible clinical trials from novel\nvectors. We further show that generated trial\nabstracts are responsive to moving embeddings\nalong concept vectors for age and sex of study\nsubjects. Our public ELM implementation and\nexperimental results will aid the alignment of\nLarge Language Models to embedding spaces\nin the biomedical domain and beyond.\n1\nIntroduction\nText embeddings map variable-length text docu-\nments to fixed-length vector spaces, capturing rich\nsemantic information. These embeddings have be-\ncome ubiquitous in Natural Language Processing,\nowed to the utility of embedding spaces for sim-\nilarity scoring, classification, and other applica-\ntions (Muennighoff et al., 2023). However, em-\nbedding of text is typically a one-way process,\nand the resulting embeddings are treated as ‘black\nboxes,’ useful for applications but uninterpretable\nand irreversible. In addition to making embedding\nspaces more interpretable, reversing these spaces\ncan aid in producing more creative content (Yeh\n*These authors contributed equally to this work\net al., 2025; Zhang et al., 2025). Yet, existing meth-\nods for inverting embeddings are extremely limited\nin length do not invert arbitrary vectors well, and\ncannot perform more advanced reasoning over one\nor more vectors. (Song and Raghunathan, 2020;\nMorris et al., 2023; Tennenholtz et al., 2024).\nA promising potential solution is training Em-\nbedding Language Models (ELMs) to allow interac-\ntion with embeddings via natural language (Tennen-\nholtz et al., 2024). ELMs extend language models\nby adding an adapter layer that aligns an embed-\nding space of interest to the model’s own token\nembedding space, allowing prompts to contain mix-\ntures of tokens and complete text embeddings. In\naddition to enabling operations over arbitrary vec-\ntors, ELMs have been shown to more faithfully\nrepresent interpolated embedding vectors than text-\nonly LLMs prompted to combine original text in-\nputs. Still, ELMs have only been reported for the\nnarrow domain of film reviews, and for proprietary\nbase language models, with no open-source code-\nbase for implementing or training them. Questions\nalso remain around optimal training procedures.\nIn this work, we seek to advance methods for\nmaking embeddings and embedding spaces more\ntransparent. We build on the work of Tennenholtz\net al. (2024) by creating an open-source ELM ar-\nchitecture and training framework and by explor-\ning the viability of ELMs in the biomedical do-\nmain. We use our new implementation to align\nan ELM to embeddings of clinical trials, design-\ning domain-specific training tasks and constructing\nan expert-validated dataset. In extensive experi-\nments, we demonstrate that our model, ctELM, can\nreconstruct abstracts more reliably than Vec2Text\nand can perform additional tasks requiring reason-\ning over multiple embeddings. We show ctELM\ncan produce plausible, hypothetical clinical trials\nfrom novel embedding vectors obtained by inter-\npolating or perturbing embeddings derived from\ntext sources. Further, we show that the generated\n1\narXiv:2601.18796v1  [cs.CL]  26 Jan 2026\n"}, {"page": 2, "text": "abstracts are responsive to clinically meaningful\ndirections identified in the embedding space us-\ning Concept Activation Vectors (Kim et al., 2018),\nnamely those representing the sex and age of trial\nsubjects. Finally, we advance general knowledge of\nELMs by performing extensive ablations showing\nthe effects of tasks, training regimes, embedding\nmodels, and generation parameters. The main con-\ntributions of this work are: (1) the first open-source\nELM architecture and training framework; (2) an\nexpert-validated dataset for training ELMs to in-\nterpret embeddings of clinical trials; (3) a trained\nELM that can interpret embeddings of clinical tri-\nals; and (4) ablation studies adding to knowledge\nof optimal ELM training.\n2\nBackground\n2.1\nText Embeddings\nEarly, “shallow” embeddings operated on individ-\nual tokens and were learned using neural networks\nwith single hidden layers or matrix factorization\nmethods (Mikolov et al., 2013; Pennington et al.,\n2014). Following the introduction of pretrained\ntransformers (Vaswani et al., 2017; Radford et al.,\n2019; Devlin et al., 2019), sentence-level, and sub-\nsequently document-level text embeddings became\nviable using contrastive learning on pooled contex-\ntual embeddings for individual tokens (Reimers and\nGurevych, 2019; Gao et al., 2021; BehnamGhader\net al., 2024; Xiao et al., 2024).\n2.2\nInversion as Vulnerability\nOne line of research on embeddings assumes an\nattacker trying to access private information that\nwould have been thought to be inherently secure\ndue to the ‘black box’ nature of embeddings (Song\nand Raghunathan, 2020).\nThis line gave rise\nto GEIA (Generative Embedding Inversion At-\ntack) (Li et al., 2023a).\nGEIA projects a vec-\ntor embedding to the token embedding layer in\nplace of the first token of input to a decoder-only\ntransformer-based language model, in this case us-\ning the GPT-2 architecture (Radford et al., 2019).\nThe model is then trained from random weights us-\ning teacher forcing to recover the original sentence\none token at a time.\nBuilding on this work, Vec2Text (Morris et al.,\n2023) fine-tunes encoder-decoder transformer lan-\nguage models to become an ‘inverter’ and a ‘correc-\ntor,’ in this case both based on pretrained T5 (Raffel\net al., 2020). Given an embedding, the inverter is\ntrained to make an initial hypothesis of the original\ntext, and the corrector is trained to move the hypoth-\nesis text closer to the target embedding by making\ndiscrete updates, based on both the target embed-\nding and the embedding of the current hypothesis.\nGiven enough iterative updates, this method can\naccurately recover short text sequences from em-\nbeddings alone. The introduction of Vec2Text has\nspurred subsequent research on defending against\nembedding inversion attacks (Zhuang et al., 2024).\nA reproduction study of Vec2Text by Seputis et al.\n(2025) also confirmed the major findings of Morris\net al. (2023). Aside from fixed-length semantic em-\nbeddings, Kugler et al. (2024) showed that text can\nalso be recovered from the token-level contextual\nembeddings of BERT (Devlin et al., 2019).\n2.3\nVector-Controlled Generation\nAnother line of work seeks to understand and re-\nverse embedding spaces in order to use directions\nin those spaces to control content. The beginnings\nof this paradigm can be seen in Bolukbasi et al.,\n2016, in which a gender axis is identified in a shal-\nlow word embedding space, and this axis neutral-\nized in word embeddings that are undesirably gen-\ndered. More recently, Tennenholtz et al. (2024) in-\ntroduce Embedding Language Models, partly with\nthe aim of exploring embedding spaces to generate\nmore novel text content. This work explored mov-\ning embeddings of film plots with reviews along\naxes (representing attributes such as comedy or\ndrama) identified in the embedding space using\nConcept Activation Vectors (CAVs) (Kim et al.,\n2018). CAVs are essentially vectors orthogonal\nto the decision plane of a linear classifier for a\nconcept of interest and were originally developed\nfor explaining predictions of vision models based\non internal activations. Steerability in LLMs has\nalso been explored using Concept Bottlenecks (Sun\net al., 2025). However, these require labels during\nLLM training, making them less flexible than align-\ning to an embedding space, and the training process\nsignificantly degrades language modeling ability.\n3\nMethods\n3.1\nPreliminaries\nLet the embedding model Eemb denote a map-\nping from a sequence of language tokens to an\nembedding space, formally expressed as: Eemb :\nX 7→Zemb, where the length of the token se-\nquence X is bounded by the context length lim-\n2\n"}, {"page": 3, "text": "Abstract\n       embeddings\nExpert\n       review\nOracle\n      model ❄\nEmbedding\n       model ❄\nStructured\n         abstract\n         sections\nUnstructured\n         clinical trial\n        abstracts\nSimilarities\n      & differences\nConcatenate\n      sections\nPlain\n   Language\n   Summaries\nTask prompts\nWrite the {background, \nmethods, …} section for the \nabstract <emb>.\nProvide the text of the \nabstract <emb>.\nWrite a plain language summary \nof the abstract <emb>.\nList five commonalities \nbetween the first abstract \n<emb> and the second abstract \n<emb>.\nList five differences between \nthe first abstract <emb> and \nthe second abstract <emb>.\nInterface\nBase chat model\n   transformer layers\n!\nctELM\nAdapter !\nBase token\nembedder ❄\nLLM\nFFN\nVector data\nText data\nUnstructured\n         clinical trial\n        abstracts\nSimilarities\n      & differences\nPlain\n   Language\n   Summaries\nNegative log-likelihood loss\nPubMedRCT\nStructured\n         abstract\n         sections\nFigure 1: The data generation and training pipeline for ctELM.\nitation inherent to Eemb. The base chat model,\nrepresented by M, is a text-only, instruction-tuned\nlarge language model (LLM). Formally, this model\ntranslates one sequence of language tokens into\nanother: M : X 7→X. The chat model com-\nprises two primary components, namely the to-\nken embedding layer Ebase and the transformer\nlayers Mbase. The token embedding layer Ebase\nmaps input tokens to a token embedding space:\nEbase : X 7→Zbase, where each token x ∈X is\nencoded into a token embedding vector z ∈Zbase.\nThe transformer layers Mbase subsequently trans-\nforms these token embeddings into output token\nsequences: Mbase : Zbase 7→X.\n3.2\nArchitecture\nOur objective is to extend the base model M to\na target model (Mtgt) capable of processing and\ninterpreting embeddings produced by the external\nembedding model Eemb, alongside standard lan-\nguage tokens. Specifically, Mtgt operates on a\ncombination of tokens and embeddings to produce\ntext output: Mtgt : (X, Zemb) 7→X. To accom-\nplish this, we adapt the approach proposed by Ten-\nnenholtz et al. (2024). We introduce an adapter\nmodule A to align the embedding spaces of Zemb\nand Zbase. Consequently, the target model is de-\nfined as: Mtgt = (A, Ebase, Mbase). The adapter\nA is a two-layer multilayer perceptron (MLP):\nW1(σ(W0Zemb + b0)) + b1,\n(1)\nwhere W0, b0, W1, b1 are learnable weights and σ\nis a non-linear activation function. The adapter\nensures that the embeddings Zemb produced by\nthe external embedding model Eemb are projected\ninto the embedding space Zbase, thereby enabling\nMbase to generate output texts by effectively lever-\naging both token-derived contextual information\n(Zbase) and the semantic content encoded within\nZemb. In this sense, the ELM architecture has simi-\nlarities with Vision Language Models (Zhang et al.,\n2024a), which must also use adapters to align lan-\nguage models to a dense vector space containing\nvisual information.\n3.3\nData & Task Preparation\nWe use PubMed 200K RCT (Dernoncourt and Lee,\n2017), which contains 190,654, 2,500, and 2,500\nabstracts for training, validation, and testing sets,\nrespectively. This dataset was created to aid in\nclassifying structured abstract sections; here we\nwill use it to generate those sections, and as a clean\ncollection of randomized controlled trials. Since\neach abstract is structured and separated by section,\nwe concatenate all sections into an unstructured\nclinical trial abstract. A selected embedding model\nEemb is then employed to generate embedding z ∈\nZemb for each abstract.\n3\n"}, {"page": 4, "text": "ELMs are aligned to embedding spaces by per-\nforming various tasks that would require detailed\nknowledge of the content of the embeddings. To\ntrain ctELM using the Mtgt model architecture, we\nprepare five diverse tasks relevant to clinical trial\nabstracts, as illustrated in Fig. 1. A training in-\nstance is formulated as the input p and the output o.\nThe input p is constructed as a prompt combining\ntext tokens X (i.e., the task instruction) and abstract\nembedding z ∈Zemb. The output o is the targeted\ntext. Both input and output vary depending on the\ntask. The following are the details of five different\ntasks (see Table 1 for statistics):\nemb2abs: Decode an abstract embedding back to\nan abstract; p is “Provide the text of the abstract z”,\no is the original abstract text.\nemb2sec: Decode an abstract embedding to a spe-\ncific section. The input p is asks to generate texts\nfor a section from an abstract embedding z: “Write\nthe {background, objective, method, result, or con-\nclusion} section for the abstract z”; o is the cor-\nresponding section text. To balance the size of\ntraining samples across tasks, for each abstract we\nrandomly sample a section from the abstract.\nemb2pls: Generates a plain language summary\nfrom an abstract embedding z with input prompt\np: “Write a plain language summary of the abstract\nz”; o is the plain language summary generated by\nan oracle model (see Appendix A).\nemb2com: Analyzes two abstract embeddings and\nlists five commonalities. The prompt p is thus\ncrafted as “List five commonalities between the\nfirst abstract zi and the second abstract zj”, where\nboth zi, zj ∈Zemb are abstract embeddings; o is\nthe commonality analysis generated by the oracle\nmodel. We use topic modeling to select abstract\npairs from the same topic and across different top-\nics to ensure diversity (see Appendix B for details).\nemb2dif: Lists five differences for two given ab-\nstract embeddings. The prompt is: “List five differ-\nences between the first abstract zi and the second\nabstract zj”; o is the difference analysis generated\nby the oracle model.\n3.4\nTraining Procedure\nAlthough we prompt ctELM with both text instruc-\ntions and abstract embedding(s), its training is sim-\nilar to text-only language models in that it aims to\npredict the next word in a sequence. Therefore, we\nTable 1: Statistics for five tasks.\nTraining\nValidation\nTesting\nTask1: emb2abs\n190,654\n2,500\n2,500\nTask2: emb2sec\n190,654\n2,500\n2,500\nTask3: emb2pls\n190,654\n2,500\n2,500\nTask4: emb2com\n241,794\n3,126\n3,126\nTask5: emb2dif\n241,794\n3,180\n3,180\ncan optimize the ctELM by minimizing the nega-\ntive log-likelihood loss versus training outputs. We\nkeep the token embedding layer Ebase frozen and\noptimize embedding adapter A (with its parame-\nter W0, b0, W1, b1) and transformer layers Mbase.\nFor efficient fine-tuning, in practice we employ\nLow-Rank Adaptation (LoRA) (Hu et al., 2022)\nto optimize Mbase, thus tuning low-rank adapters\nrather than the original weights. We explore both\none-phase training (1P), which only jointly opti-\nmizes A and Mbase, and two-phase training (2P),\nwhich adds an initial step in which Mbase is frozen.\n4\nExperiments\n4.1\nImplementation & Settings\nFor\nexperiments,\nwe\nset\nM\nto\nbe\nLlama-3.1-8B-Instruct (Dubey et al., 2024),\nEemb to be BAAI/bge-large-en-v1.5, and adopt\ngpt-4o-mini as the oracle model to generate\nthe synthetic data for emb2pls, emb2com, and\nemb2dif. We load the base model via Hugging-\nface’s transformers package (Wolf et al., 2020)\nand extend it by introducing an adapter A, which\nis a 2-layer MLP with 2,048 neurons in the\nhidden layer, 4,096 in the second layer (to match\nLLama 3.1’s token embedding dimension), and\nReLU activation between the layers. The training\nprocedure is implemented using the HuggingFace\ntrl and peft packages to ensure reproducibility.\nAll the training hyperparameters are reported in\nAppendix C.\nFor inference, we set the temperature as 1 for\nall tasks. However, we find that repetition occurs\nwhen generating entire abstracts. Therefore, we\nexperiment with imposing a repetition penalty us-\ning the methods of Keskar et al. (2019), with the\nauthors’ recommended penalty strength of 1.2 for\nemb2abs task. For other tasks, we set the penalty\nat 1.0 (no penalization), as we do not observe this\nbehavior for these tasks.\n4\n"}, {"page": 5, "text": "4.2\nModel Variants\nWe explore various training configurations of\nctELM to assess the effects of data scale, task diver-\nsity, and training procedure. Data scale: We train\nctELM on two dataset sizes, 190K and 1.2M train-\ning instances. Task diversity: We construct three\ndatasets to analyze the influence of task diversity\nwhile keeping the training size fixed at 190K:\n1-task: Only emb2abs training instances.\n3-task:\nAn equal number of instances from\nemb2abs, emb2sec, and emb2pls.\n5-task: Equally sampled instances from emb2abs,\nemb2sec, emb2pls, emb2com, and emb2dif.\nFor the 1.2M configuration, we interleave train-\ning instances from all five tasks, sampling until\neach instance from every task has been included\nat least once. Training procedure: We further\nexamine the effect of training strategy by varying\nthe number of training phases. Each model config-\nuration is denoted as xP-yE, where x indicates the\nnumber of distinct training phases and y denotes\nthe number of training epochs.\n4.3\nBaselines\nAs ctELM is trained on clinical trial studies, direct\ncomparison with the originally reported ELM of\nTennenholtz et al. (2024) is not feasible, since (1)\nit was trained on a movie review dataset and (2) the\narchitecture is not publicly available for retraining.\nHowever, as the emb2abs task is direct inversion\nof an embedding, we can compare ctELM’s perfor-\nmance for this task against Vec2Text (Morris et al.,\n2023), to our knowledge the state-of-the art sys-\ntem for embedding inversion. We first compare\nagainst the published GTR-base model, trained on\nWikipedia passages. Though not trained specifi-\ncally on clinical trial abstracts, Morris et al. (2023)\ndemonstrate generalization of this model to various\nbiomedical corpora. As a stronger baseline, we also\nuse the published GTR-base weights as initializa-\ntions for further training on our corpus. We use the\nmodel weights and implementation from the offi-\ncial repository.1 Additionally, published Vec2Text\nmodels were only trained with a maximum of 128\ntokens and are known to perform poorly beyond\nthis length (Seputis et al., 2025). As our abstracts\nhave a mean length of 304 tokens, we thus also\nexperiment with using Vec2Text to invert embed-\n1https://github.com/vec2text/vec2tex\ndings of individual sections, then concatenating the\nresults to reconstruct the complete abstract. In total,\nwe test four configurations:\nVec2Text: The published model directly inverts an\nabstract embedding into its corresponding text.\nVec2Text-ft:\nThe published model is further\ntrained on our training abstracts, with the maximum\ntokens increased from 128 to 512, then inverts an\nembedding of a full abstract.\nVec2Text-sect: Each section of an abstract is em-\nbedded and decoded with the published model, and\nthe resulting outputs concatenated.\nVec2Text-sect-ft: The published model is further\ntrained to invert embeddings of individual sections\nfrom PubMedRCT abstracts, leaving the maximum\ntokens at 128. At test time, each section of an\nabstract is independently embedded and decoded,\nand the results concatenated, as in Vec2Text-sect.\nTo approximately match the number of training\nsteps of our 1.2M ctELM models, we train the fine-\ntuned models (Vec2Text-ft and Vec2Text-sect-ft)\nfor 7 epochs on the 190K abstracts from the Pub-\nMedRCT training set.\n4.4\nMetrics\nWe adopt Semantic Consistency (SC) (Tennenholtz\net al., 2024) as our primary metric. SC measures\nthe semantic closeness between two embeddings.\nFormally, assuming that the generated text is ˆo, we\nembed it and compare with the embedding of the\ntarget text o (or for novel embeddings, compare\nwith the novel embedding directly):\nSC(ˆo, o) = δ(Eemb(ˆo), Eemb(o)),\n(2)\nwhere δ measures cosine similarity between em-\nbedding in the semantic space Zemb produced by\nthe Eemb. As a result, the higher SC suggests the\nmodel generates texts that better align with the se-\nmantical concept of the target text.\n4.5\nResults\nTable 2 reports the performance analysis of seman-\ntic consistency (SC) across model variants and\ntasks. Improved performance over baselines:\nAcross all tasks, ctELM consistently outperforms all\nVec2Text baselines. For example, on abstract recon-\nstruction (emb2abs with penalty=1.2), Vec2Text-\nsect-ft achieves 0.82, while ctELM models achieve\n5\n"}, {"page": 6, "text": "Table 2: Semantic Consistency for 5 tasks on the test set, across training tasks and strategies. ctELM is trained on\neither 190K or 1.2M data using the bge-large-en-v1.5 embedding model. (xP-yE) represents x-phase training\nprocedure is adopted for y epochs. Mean values over the test set are shown with standard deviations. Best\nperformance for each task is marked in bold.\nData Size\nModel\nemb2abs\n(penalty=1.2)\nemb2abs\n(penalty=1.0)\nemb2sec\nemb2pls\nemb2com\nemb2dif\nBaseline\nVec2Text\n0.70±0.08\n-\n-\n-\n-\n-\nVec2Text-ft\n0.77±0.08\n-\n-\n-\n-\n-\nVec2Text-sect\n0.82±0.07\n-\n-\n-\n-\n-\nVec2Text-sect-ft\n0.82±0.06\n-\n-\n-\n-\n-\nctELM on\n190K\n1-task (1P-1E)\n0.83±0.05\n0.82±0.05\n-\n-\n-\n-\n3-task (1P-1E)\n0.83±0.05\n0.81±0.05\n0.73±0.07\n0.77±0.05\n-\n-\n3-task (1P-2E)\n0.84±0.05\n0.83±0.05\n0.74±0.07\n0.78±0.05\n-\n-\n3-task (2P-1E)\n0.86±0.05\n0.84±0.05\n0.75±0.07\n0.80±0.05\n-\n-\n5-task (1P-1E)\n0.83±0.05\n0.82±0.05\n0.73±0.07\n0.77±0.05\n0.87±0.04\n0.86±0.04\nctELM on\n1.2M\n5-task (1P-1E)\n0.86±0.05\n0.84±0.05\n0.76±0.07\n0.80±0.05\n0.88±0.04\n0.88±0.04\n5-task (1P-2E)\n0.87±0.05\n0.85±0.05\n0.76±0.07\n0.81±0.05\n0.88±0.04\n0.89±0.03\n5-task (2P-1E)\n0.87±0.04\n0.86±0.05\n0.77±0.07\n0.81±0.05\n0.88±0.04\n0.89±0.03\nup to 0.87, indicating the effectiveness of the pro-\nposed architecture in capturing semantic content\nfrom embeddings. Impact of repetition penalty:\nApplying a repetition penalty of 1.2 yields better\nsemantic consistency than no penalty (1.0), suggest-\ning that penalizing repetitive token generation en-\ncourages more faithful and coherent text generation.\nEffect of task diversity: Increasing the number of\ntasks does not degrade performance on individual\ntasks despite fewer training samples per task. For\ninstance, the SC scores for emb2abs (penalty=1.2)\nare 0.83 for 1-task, 0.83 for 3-task, and 0.83 for\n5-task (all trained with 1P-1E on 190K), indicating\nthat ctELM generalizes well across multitask train-\ning regimes without sacrificing single-task quality.\nEffect of data scale: Scaling the training data from\n190K to 1.2M results in consistent improvements\nacross all tasks. For example, emb2sec improves\nfrom 0.73–0.75 (190K) to 0.76–0.77 (1.2M), and\nemb2dif improves from 0.86 to 0.89. This demon-\nstrates the scalability of ctELM and its capacity to\nbenefit from larger datasets. Effect of training\nprocedures: On the smaller 190K dataset, the two-\nphase training procedure yields superior results.\nFor instance, 3-task (2P-1E) outperforms both 1P-\n1E and 1P-2E configurations in most tasks. How-\never, on the larger 1.2M dataset, the gap between\ntraining procedures narrows. Both 1P-2E and 2P-\n1E achieve similar top performance (e.g., 0.87 on\nemb2abs, 0.81 on emb2pls, 0.89 on emb2dif). No-\ntably, 1P-1E requires approximately half the train-\ning time and still delivers competitive results, mak-\ning it a practical alternative for large-scale deploy-\nment. We thus use the 5-task 1P-1E model for\nfurther experiments.\nWhile Table 2 evaluates outputs using held-out\n(but real) test abstracts, Appendix D investigates\nctELM’s ability to generalize to novel or hypothet-\nical abstracts. Appendix L presents decoded ex-\namples from interplolated embeddings. For further\ninsight, we analyze consistency (versus the original\nabstract) and fluency, both quantitatively—using\nG-Eval (Liu et al., 2023)— and qualitatively in Ap-\npendix E. To test the generalizability of ctELM’s\narchitecture, we also explore the effect of different\nbase chat models in Appendix F and embedding\nmodels in Appendix G.\n5\nValidation\nThe high Semantic Consistency for clinical tri-\nals generated by ctELM from novel embeddings\ndemonstrates that the model has successfully\nlearned a data manifold for a set of abstracts. How-\never, it cannot tell us how well this learned man-\nifold corresponds with a theoretical distribution\nof parameters of clinical trials.\nSpecifically, it\ndoes not tell us (1) whether the abstracts gener-\nated from novel embeddings describe clinical trials\nwith likely parameters (in other words, trials that\nare plausible), or (2) whether directions along the\nmanifold correspond with clinical trial parameters\n(in other words, whether the geometry of the mani-\nfold is clinically meaningful). To further validate\nctELM for generative and explanatory use cases, we\nthus ask two research questions:\nRQ1: Can ctELM map novel points in the embed-\n6\n"}, {"page": 7, "text": "ding space to plausible clinical trials?\nRQ2: Are clinical trials generated by ctELM re-\nsponsive to clinically meaningful directions in the\nembedding space?\nWe seek to answer these questions in the space\nof abstracts, as, among the tasks ctELM can per-\nform, abstracts most specifically layout the details\nof one clinical trial. For both questions, we will\nhave ctELM perform the emb2abs task for novel\nembeddings (those not directly generated from text\nsources by the mapping X 7→Zemb).\n5.1\nClinical Trial Plausibility\nTo answer RQ1 (plausibility of clinical trials\ngenerated from novel embeddings), we task hu-\nman experts with discriminating real (test set) ab-\nstracts from hypothetical ones generated using the\nemb2abs prompt and novel embedding vectors. As\nin Tennenholtz et al. (2024), we generate novel\nvectors via interpolation, by averaging randomly\nselected pairs of test set abstract embeddings. Our\nmain metric is win rate, which is the fraction of hy-\npothetical abstracts that successfully fool the expert\nwhen paired with a random test abstract. In expecta-\ntion, the highest achievable win rate is 0.5, meaning\ngenerated abstracts are indistinguishable from real\nones. As a baseline, we compare to Vec2Text-sect-\nft performing the easier task of generating abstracts\nfrom original (not interpolated) embeddings, as its\nsection-wise nature precludes direct interpolations.\nTwo experts, both authors, perform the annotation;\none an MD (Doctor of Medicine) and one an MBBS\n(Bachelor of Medicine, Bachelor of Surgery).We\nrandomly select 50 real abstracts from the test set.\nThe first 25 are paired with Vec2Text-sect-ft ab-\nstracts for expert 1 and ctELM abstracts for expert\n2, while the second 25 are paired with ctELM ab-\nstracts for expert 1 and Vec2Text-sect-ft abstracts\nfor expert 2, resulting in 50 single-annotated pairs\nfor each system. Order within pairs is randomized.\n5.2\nConcept Activation Vectors\nTo answer RQ2 (responsiveness of ctELM outputs\nto clinically meaningful directions in the embed-\nding space), we follow Tennenholtz et al., 2024 in\nmoving embeddings along Concept Activation Vec-\ntors (CAVs). We train CAVs to identify two axes\nrepresenting demographics of clinical trial subjects:\n(1) sex (male vs. female), and (2) age (children vs.\nolder adults). Details of data collection for CAV\ntraining can be found in Appendix J. The model\nTable 3: Win rates for human experts\nWin rate\nEmbedding\nExp. 1\nExp. 2\nAvg.\nVec2Text-sect-ft\nOriginal\n0.00\n0.04\n0.02\nctELM\nInterpolated\n0.48\n0.40\n0.44\nused to identify the CAVs is a linear kernel SVM,\nimplemented with Scikit-learn (Pedregosa et al.,\n2011). Once CAVs are identified, we add them\nto embeddings of single sex or single age group\nclinical trials, with a signed coefficient α determin-\ning the strength and direction of modification. The\nresulting vectors are then normalized to length 1,\nas other BAAI/bge-large-en-v1.5 embeddings,\nand used to generate new clinical trial abstracts by\nprompting ctELM to perform the emb2abs task. To\ndetermine responsiveness, we employ an extraction\nagent to label sex or age of the subjects in the re-\nsulting abstracts (see Appendix K). The complete\npipeline is depicted in Figure 13.\n5.3\nResults\nFor RQ1, the win rates of systems vs. human ex-\nperts are shown in Table 3. Vec2Text-sect-ft (the\nhighest performing baseline) fooled experts only\nonce in the 50 pairs, even using unmodified embed-\ndings. On the other hand, ctELM fools the experts\n44% of the time, close to the theoretical limit of\n50%, even though it is performing the more diffi-\ncult task of generating hypothetical abstracts from\nnovel (interpolated) embeddings. This shows that\nctELM outputs are not only fluent but also describe\nclinically plausible trials with coherent scientific\ndetails. We also develop an automated win rate\nexperiment using an LLM discriminator in order to\nscale to more conditions (see Appendix H).\nFor RQ2, Figures 2 and 3 show results for modifi-\ncation along sex and age CAVs, respectively. Modi-\nfication successfully changes subject demographics\nalong the expected axes. Both CAVs can even in-\nduce intermediate values. For sex, lower values of\nα produce some abstracts of neutral sex (meaning\nthey include both or do not mention sex), as well as\na mixture of male-only and female-only abstracts.\nFor age, lower values of α produce some abstracts\nwith subject ages between children and older adults,\nas well as abstracts with both extremes. In both\ncases, semantic consistency remains relatively high\nas α is increased, though there is some drop-off\nat full saturation (complete change of subjects),\nwhich happens when α is around 1 or -1.\n7\n"}, {"page": 8, "text": "REF\n-0.0625\n-0.125\n-0.25\n-0.5\n-1\n0\n10\n20\n30\n40\n50\nTrial subject sex (count)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSC mean ± std\nMale to female\nMale\nNeutral\nFemale\nREF\n0.0625\n0.125\n0.25\n0.5\n1\n0\n10\n20\n30\n40\n50\nTrial subject sex (count)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSC mean ± std\nFemale to male\nMale\nNeutral\nFemale\nFigure 2: Moving embeddings along a Concept Activation Vector for sex of trial subjects changes the observed sex\nin abstracts generated by ctELM. The value α is the coefficient of the added sex vector and thus represents concept\nstrength. Trial subject sex (y-axis, left) refers to the number of trials identified as each sex among a group of 50.\nREF is sex extracted from original abstracts. Semantic Consistency is shown in black lines (y-axis, right).\nREF\n0.0625\n0.125\n0.25\n0.5\n1\n0\n20\n40\n60\n80\nTrial subject age\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSC mean ± std\nChild to aged\nREF\n-0.0625\n-0.125\n-0.25\n-0.5\n-1\n0\n20\n40\n60\n80\nTrial subject age\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSC mean ± std\nAged to child\nFigure 3: Moving embeddings along Concept Activation Vectors for age of trial subjects changes the observed age\nin abstracts generated by ctELM. The value α is the coefficient of the added age vector. Trial subject age (y-axis, left)\nrefers to the identified age of each trial (each depicted as a point). Box and whisker plots show minima, maxima,\nmedians, and inter-quartile ranges of identified age. Note that horizontal jitter is employed for each discrete α value;\nthe x position of each point within its strip is thus not meaningful. REF is the original abstracts with age extracted\ndirectly. Semantic Consistency is shown in black lines (y-axis, right).\n6\nDiscussion & Conclusion\nIn this work, we advance the capability of re-\nsearchers to interpret, explore, and reverse seman-\ntic embedding spaces. We show that Embedding\nLanguage Models (ELMs), formerly only demon-\nstrated for the domain of film reviews and for pro-\nprietary models, generalize to the biomedical do-\nmain, specifically for embeddings of clinical trial\nabstracts, and that lightweight, open-source LLMs\ncan be used as base models for ELMs. We fur-\nther provide the research community with an open-\nsource architecture and training framework, and\nwe use it to create ctELM, an ELM that can in-\nterpret embeddings of clinical trial abstracts. We\nshow that capable ELMs can be trained with very\nfew tasks (1–5 as opposed to 24), and with sim-\npler single-phase training, skipping the adapter pre-\ntraining that Tennenholtz et al. (2024) claimed was\nnecessary. Our validation experiments show that,\neven for novel points with no original text abstract,\nctELM can describe clinical trials plausible enough\nto deceive human experts tasked with discriminat-\ning them from real clinical trial abstracts. Our\nexperiments further show that generated abstracts\nare responsive to changes along clinically mean-\ningful directions in the embedding space. This\nshows the robustness of the learned mapping and\nopens many possibilities for language-based in-\nterpretation of embedding spaces and controlled\ngeneration for diverse synthetic data. Taken in to-\ntal, we expect this work will make aligning LLMs\nto embedding spaces vastly more accessible, en-\nabling a wide array of downstream applications.\nWe provide our code under the MIT license at\n8\n"}, {"page": 9, "text": "https://github.com/BIDS-Xu-Lab/OpenELM.\nLimitations\nFirst, we acknowledge that the domain and for-\nmat of our training data is relatively narrow. As\nELMs inherently train to specific data manifolds,\nit is not clear how well ctELM would generalize to\nother data from the biomedical domain (such as\nfull articles or clinical notes), let alone data from\nother domains. For now, we consider ELMs to be\ncorpus- and task-specific (as originally introduced\nin Tennenholtz et al., 2024), and we release our\narchitecture and training code with the hopes that\nresearchers in other domains can easily train be-\nspoke ELMs for other corpora and domains. Future\nwork should test the limits of ELMs for generaliz-\ning across domains and tasks.\nSecond,\nthough\nwe\nexplored\nfine-tuning\nVec2Text, it is still not a perfect baseline since\nit is based on T5, which has fewer parameters than\nctELM’s Llama 3 base model. More in-depth explo-\nration of Vec2Text’s iterative correction method of\ngeneration, with larger models and domain-specific\ntraining data, may improve the viability of this\nmethod for exploring embedding spaces and should\nbe explored in the future.\nFinally, though many generated abstracts were\nable to deceive human experts when compared to\nreal abstracts, we are still far from translating these\ninto real-world studies. In particular, the applica-\nbility of a specific combination of drug, disease,\nand population would require deep expertise in the\nrelevant field of medicine to validate as a clinical\ntrial. As an example of an ethical hazard, chang-\ning of study participants may introduce popula-\ntions with further protections under Common Rule\n(such as children or pregnant women) that systems\nmay not account for. Translating our methods and\nfindings into downstream applications will thus re-\nquire large-scale collaboration with clinicians and\nbioethicists.\nReferences\nParishad BehnamGhader, Vaibhav Adlakha, Marius\nMosbach, Dzmitry Bahdanau, Nicolas Chapados, and\nSiva Reddy. 2024. Llm2vec: Large language models\nare secretly powerful text encoders. arXiv preprint\narXiv:2404.05961.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nneural information processing systems, 29.\nFranck Dernoncourt and Ji Young Lee. 2017. Pubmed\n200k rct: a dataset for sequential sentence clas-\nsification in medical abstracts.\narXiv preprint\narXiv:1710.06071.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 conference of the\nNorth American chapter of the association for com-\nputational linguistics: human language technologies,\nvolume 1 (long and short papers), pages 4171–4186.\nAdji B. Dieng, Francisco J. R. Ruiz, and David M. Blei.\n2020. Topic modeling in embedding spaces. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:439–453.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The llama 3 herd of models.\narXiv e-prints, pages arXiv–2407.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,\nXuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,\nShengjie Ma, Honghao Liu, and 1 others. 2024. A\nsurvey on llm-as-a-judge. CoRR.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nBeen Kim, Martin Wattenberg, Justin Gilmer, Carrie\nCai, James Wexler, Fernanda Viegas, and 1 others.\n2018.\nInterpretability beyond feature attribution:\nQuantitative testing with concept activation vectors\n(tcav). In International conference on machine learn-\ning, pages 2668–2677. PMLR.\n9\n"}, {"page": 10, "text": "Kai Kugler, Simon Münker, Johannes Höhmann, and\nAchim Rettinger. 2024. Invbert: Reconstructing text\nfrom contextualized word embeddings by inverting\nthe bert pipeline. Journal of Computational Literary\nStudies, 2(1).\nHaoran Li, Mingshi Xu, and Yangqiu Song. 2023a. Sen-\ntence embedding leaks more information than you\nexpect: Generative embedding inversion attack to\nrecover the whole sentence. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 14022–14040.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, and Meishan Zhang. 2023b. Towards\ngeneral text embeddings with multi-stage contrastive\nlearning. arXiv preprint arXiv:2308.03281.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2511–2522.\nH. B. Mann and D. R. Whitney. 1947. On a test of\nwhether one of two random variables is stochastically\nlarger than the other. The Annals of Mathematical\nStatistics, 18(1):50–60.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas\nGrossberger. 2018. Umap: Uniform manifold ap-\nproximation and projection. The Journal of Open\nSource Software, 3(29):861.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in neural information processing systems,\n26.\nJohn Morris, Volodymyr Kuleshov, Vitaly Shmatikov,\nand Alexander Rush. 2023. Text embeddings reveal\n(almost) as much as text. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12448–12460, Singapore.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and\nNils Reimers. 2023. Mteb: Massive text embedding\nbenchmark. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 2014–2037.\nArjun Panickssery, Samuel Bowman, and Shi Feng.\n2024. Llm evaluators recognize and favor their own\ngenerations. Advances in Neural Information Pro-\ncessing Systems, 37:68772–68802.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, and 1 others. 2011. Scikit-learn: Ma-\nchine learning in python. the Journal of machine\nLearning research, 12:2825–2830.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, and 1 others. 2019.\nLanguage models are unsupervised multitask learn-\ners. OpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research,\n21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nDominykas Seputis, Yongkang Li, Karsten Langerak,\nand Serghei Mihailov. 2025. Rethinking the privacy\nof text embeddings: A reproducibility study of “text\nembeddings reveal (almost) as much as text”. In\nProceedings of the Nineteenth ACM Conference on\nRecommender Systems, pages 822–831.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. In Pro-\nceedings of the 2020 ACM SIGSAC conference on\ncomputer and communications security, pages 377–\n390.\nChung-En Sun, Tuomas P. Oikarinen, Berk Ustun, and\nTsui-Wei Weng. 2025. Concept bottleneck large lan-\nguage models. In The Thirteenth International Con-\nference on Learning Representations, ICLR 2025,\nSingapore, April 24-28, 2025. OpenReview.net.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, Alexandre Ramé, Morgane\nRivière, and 1 others. 2025. Gemma 3 technical\nreport. arXiv preprint arXiv:2503.19786.\nGuy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Ji-\nhwan Jeong, Lior Shani, Azamat Tulepbergenov,\nDeepak Ramachandran, Martin Mladenov, and Craig\nBoutilier. 2024. Demystifying embedding spaces\nusing large language models. In The Twelfth Inter-\nnational Conference on Learning Representations,\nVienna, Austria. OpenReview.net.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand 1 others. 2020. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the\n10\n"}, {"page": 11, "text": "2020 conference on empirical methods in natural\nlanguage processing: system demonstrations, pages\n38–45.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings.\nIn Proceedings of the 47th International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval, page 641–649, New York, NY,\nUSA. Association for Computing Machinery.\nWenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming\nPan, Lei Li, and William Wang. 2024. Pride and\nprejudice: Llm amplifies self-bias in self-refinement.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 15474–15492.\nCatherine Yeh, Donghao Ren, Yannick Assogba, Do-\nminik Moritz, and Fred Hohman. 2025. Exploring\nempty spaces: Human-in-the-loop data augmenta-\ntion. In Proceedings of the 2025 CHI Conference on\nHuman Factors in Computing Systems, pages 1–19.\nJingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu.\n2024a. Vision-language models for vision tasks: A\nsurvey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence.\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen\nXie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong\nYang, Pengjun Xie, Fei Huang, and 1 others. 2024b.\nmgte: Generalized long-context text representation\nand reranking models for multilingual text retrieval.\narXiv preprint arXiv:2407.19669.\nXingjian Zhang, Ziyang Xiong, Shixuan Liu, Yutong\nXie, Tolga Ergen, Dongsub Shim, Hua Xu, Honglak\nLee, and Qiaozhu Mei. 2025. Mapexplorer: New\ncontent generation from low-dimensional visualiza-\ntions. In Proceedings of the 31st ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining V.\n2, pages 3843–3854.\nShengyao Zhuang, Bevan Koopman, Xiaoran Chu, and\nGuido Zuccon. 2024. Understanding and mitigating\nthe threat of vec2text to dense retrieval systems. In\nProceedings of the 2024 Annual International ACM\nSIGIR Conference on Research and Development\nin Information Retrieval in the Asia Pacific Region,\npages 259–268.\nA\nData Preparation for emb2pls\nWe generate the plain language summary for\neach abstract using gpt-4o-mini with the prompt\nshown in Fig. 4. To measure the quality of the\ngenerated summaries, we had two physicians (both\nwomen) review 20 sampled summaries generated\nfrom gpt-4o-mini. The physicians had a stand-\ning contract with our organization to annotate data\nand had expectations to do similar work and were\ncompensated according to skill level. We discussed\nwith the physicians the goals of the project and\nrole of their evaluations, and defined annotation\nguidelines (see the attached supplementary file)\nmeasuring four aspects: simplicity, accuracy, com-\npleteness, and relevance. Each metric is measured\nusing a 5-point Likert scale (1=Poor, and 5=Excel-\nlent). To further contextualize these scores, we\nalso have the physicians rate 20 expert-written\nsummaries (which we expect to be of high qual-\nity) and 20 summaries of poor quality. Expert\nand poor-quality summaries were derived from the\nTREC Plain Language Adaptation of Biomedical\nAbstracts (PLABA) task (data obtained upon re-\nquest to organizers). Poor summaries were those\nwith the lowest scores from PLABA’s human eval-\nuators. All 60 summaries were shuffled, and their\nsources were blind to the two physicians. Fig. 5\npresents the scatter plot for scores between two\nphysicians, where the score of each summary is\nthe sum of four metrics.\nThe Pearson correla-\ntion coefficient (r = 0.52) suggests a moderate\ncorrelations in two physicians’ annotated scores.\nWe conduct the Wilcoxon rank-sum test (Mann\nand Whitney, 1947) to test whether the median is\ndifferent between groups (gpt-4o-mini summary\nversus human-written summary and gpt-4o-mini\nversus poor summary). Fig. 6 shows that there\nis no significant difference between median score\nof gpt-4o-mini summaries and that of human-\nwritten summaries. At the same time, there is a\nsignificant difference between gpt-4o-mini and\nthe poor summaries, validating the evaluation pro-\ncess. These results suggest the plain language sum-\nmaries generated by gpt-4o-mini are of sufficient\nquality for training ctELM.\nB\nData Preparation for emb2com and\nemb2dif\nWe use gpt-4o-mini to generate commonality and\ndifference analyses for each abstract pair, using\nprompts shown in Fig. 7 and Fig. 8. To construct\ndiverse and meaningful abstract pairs, we apply\nBERTopic (Grootendorst, 2022), a Python-based\ntopic modeling framework, to cluster abstracts in\nthe embedding space.\nThe procedure involves\nthree main steps. First, all abstracts are embed-\nded using BAAI/bge-large-en-v1.5 model (Xiao\net al., 2024).\nSecond, we reduce the high-\ndimensional embeddings into a five-dimensional\nspace using UMAP (McInnes et al., 2018) with\n11\n"}, {"page": 12, "text": "You are a medical writing assistant\nwith\nexpertise in creating plain\nlanguage summaries of scientific\nresearch. Your goal is to translate\ncomplex scientific abstracts into\nsimple , concise summaries\nunderstandable by a general audience.\nProvide only the plain language\nsummary , without any additional words ,\ninstructions , or formatting.\nTranslate the following PubMed article\nabstract into a plain language summary:\n\"{ abstract }\"\nFigure 4: The prompt template for generating plain\nlanguage summary.\n8\n10\n12\n14\n16\n18\n20\nScore from Physician 2\n6\n8\n10\n12\n14\n16\n18\n20\nScore from Physician 1\nr = 0.52, p-value = 0.0000\nFigure 5: The scatter plot between two physicians’ an-\nnotated scores with Pearson correlation coefficient (r)\nresults.\nthe following hyperparameters: n_neighbors=15,\nn_components=5, and min_dist=0.1. Third, HDB-\nSCAN is employed to identify topic clusters\nwithin the reduced space.\nTo determine the\noptimal number of clusters, we search for the\nmin_cluster_size value that yields the highest topic\nquality, measured using the criteria proposed in Di-\neng et al. (2020). As shown in Fig. 9, we select\nmin_cluster_size=250, resulting in 121 topic clus-\nters with the best topic quality. Using these clus-\nters, we sample abstract pairs either from within the\nsame topic or across different topics. Table 4 sum-\nmarizes the pair distribution used for the emb2com\nand emb2dif tasks across training, validation, and\ntesting datasets.\nC\nTraining Hyperparameters & Details\nFor the first phase of two-phase training procedure,\nwe freeze all model parameters except for embed-\ngpt-4o-mini\nhuman-written summary\npoor summary\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nScore\np = 0.538\np = 0.001\nScore Distribution by Summary Source\nFigure 6: The boxplot for scores by different sources\nwith Wilcoxon rank-sum test results.\nYou are an expert in biomedical\nliterature analysis.\nYou are asked to compare two PubMed\nabstracts and identify their\ncommonalities. Please use concise\nlanguage. Please directly list five\ncommonalities between two abstracts.\nHere are two abstracts:\n1. \"{ abstract1 }\"\n2. \"{ abstract2 }\"\nFigure 7: The prompt template for listing five common-\nalities for a abstract pair.\nding adapter A. We then use SFTTrainer in the\ntrl package to optimize the adapter. In the SFT-\nConfig, we set the learning rate as 1e-3, batch size\nas 4, gradient accumulation steps as 8, and max\nsequence length as 2,048. With this settings, we\ntrain the adapter using AdamW optimizer (default\nparameters), linear scheduler with a warmup phase,\nmixed precision (i.e., bfloat16) for one epoch.\nAs for the second phase of two-phase training\nprocedure and one-phase training procedure, we\nuse SFTTrainer with LoraConfig from peft pack-\nage. We set the learning rate as 5e-5, batch size\nas 4, gradient accumulation steps as 8, max grad\nnorm as 1, and max sequence length as 2,048. On\nthe other hand, we set r as 16, lora alpha as 32,\nlora dropout as 0.05, and bias as none. We only\noptimze q_proj and k_proj in Mbase and set A as\nthe module to save. We train the adapter as well\nas LoRA parameters using AdamW optimizer (de-\nfault parameters), linear scheduler with a warmup\nphase, mixed precision (i.e., bfloat16) for one or\ntwo epoch(s).\n12\n"}, {"page": 13, "text": "You are an expert in biomedical\nliterature analysis.\nYou are asked to compare two PubMed\nabstracts and identify their\ndifferences. Please use concise\nlanguage. Please directly list five\ndifferences between two abstracts.\nHere are two abstracts:\n1. \"{ abstract1 }\"\n2. \"{ abstract2 }\"\nFigure 8: The prompt template for listing five differ-\nences for a abstract pair.\nNumber of Topics\nTopic Quality\n0.1\n0.12\n0.14\n0.16\n625\n293\n192\n153\n121\n104\n90\n70\n56\n57\nFigure 9: The line plot for helping identify the best\nnumber of topics.\nWe train the model with the above settings on\none Nvidia H100 GPU. The training time for ctELM\non 1.2M data using one-phase training procedure\nfor one epoch (1P-1E) takes around 13 hours. On\nthe other hand, the training time for ctELM on 1.2M\ndata using two-phase training procedure for one\nepoch (2P-1E) takes around 26 hours.\nD\nInterpolation Semantic Consistency\nWe construct an interpolated testing set by aver-\naging embeddings from randomly selected pairs\nof test abstracts. This simulates new abstract em-\nbeddings that lie between known examples in the\nsemantic space. Across all configurations, ctELM\nmaintains the same observations, such as a repeti-\ntion penalty of 1.2 leading to better performance\nthan penalty of 1.0 in emb2abs, and model perfor-\nmance benefiting from more training data. When\nwe compare to the performance with real abstracts\n(Table 2), although slightly lower, with a drop of\nroughly 0.02–0.04 points (e.g., 0.87 →0.83 on\nemb2abs, 0.81 →0.77 on emb2pls), the scores\nremain stable and consistent, demonstrating the\nTable 4: Distribution of abstract pairs used for the\nemb2com and emb2dif tasks across training, validation,\nand testing datasets. Pairs are constructed based on topic\nassignment using BERTopic. Each dataset contains a\nbalanced number of same-topic and different-topic pairs\nto ensure diversity and control for topic-based variation.\nTraining Dataset\nemb2com\nemb2dif\nPairs from the Same Topic\n120,897\n120,897\nPairs from Different Topics\n120,897\n120,897\nValidation Dataset\nemb2com\nemb2dif\nPairs from the Same Topic\n1,562\n1,562\nPairs from Different Topics\n1,564\n1,564\nTesting Dataset\nemb2com\nemb2dif\nPairs from the Same Topic\n1,589\n1,589\nPairs from Different Topics\n1,591\n1,591\nctELM’s robustness in handling unseen, interpo-\nlated representations.\nE\nConsistency and Fluency\nWe measure consistency and fluency quantitatively\nwith G-Eval (Liu et al., 2023), using the open-\nsource DeepEval framework.2 To define metrics,\nthe framework requires ‘criteria’ and ‘evaluation\nsteps,’ which we provide for consistency and flu-\nency in Figures 10 and 11, respectively.\nTable 7 shows G-Eval scores for the best-\nperforming baseline and the ctELM 5-task 1P-1E\nmodel. Though consistency of both models can\nbe improved, we find that ctELM has 31% higher\nconsistency and 317% higher fluency.\nTo further investigate these scores, we manually\nreview 25 outputs from Vec2Text-sect-ft and ctELM\nfor errors in consistency and fluency. We analyze\nthese qualitatively by extracting common themes\nand finding representative examples, as shown in\nTable 6.\nF\nEffect of Base Chat Model\nIn addition to Llama-3.1-8B-Instruct, we also\nexplore 3 variants of Gemma 3 (Team et al., 2025),\nspanning different model sizes and domain-specific\ntraining:\n• gemma-3-1b-it: 1B parameters, instruction-\ntuned, text-only, open-domain training.\n2https://github.com/confident-ai/deepeval\n13\n"}, {"page": 14, "text": "Consistency\nCriteria: “Determine whether the actual output describes the same clinical trial as the input.”\nEvaluation steps:\n• “Check whether the medical condition in ‘actual output’ reflects that of ‘input’.”\n• “Check whether the study design (e.g., randomized controlled trial, observational study) in ‘actual output’ reflects\nthat of ‘input’.”\n• “Check whether the intervention (e.g., drug, therapy) in ‘actual output’ reflects that of ‘input’.”\n• “Check whether the population (e.g., age group, sex, health status) in ‘actual output’ reflects that of ‘input’.”\nFigure 10: Prompts to evaluate consistency using G-Eval via the DeepEval framework.\nFluency\nCriteria: “Determine the quality of the ’actual output’ in terms of grammar, spelling, punctuation, word choice, and\nsentence structure.”\nEvaluation steps:\n• “Check whether ‘actual output’ follows standard grammar rules”\n• “Check whether ‘actual output’ is free of spelling and punctuation errors”\n• “Check whether ‘actual output’ uses appropriate word choice”\n• “Check whether ‘actual output’ has a coherent sentence structure”\nFigure 11: Prompts to evaluate fluency using G-Eval via the DeepEval framework.\n14\n"}, {"page": 15, "text": "Table 5: Performance of semantic consistency on inter-\npolated testing set. ctELM is trained on either 190K or\n1.2M data. (xP-yE) represents x-phase training proce-\ndure is adopted for y epochs. Best performance for each\ntask is marked in bold.\nModel\nemb2abs\n(pen.=1.2)\nemb2abs\n(pen.=1.0)\nemb2pls\nLlama-3.1-8B-Instruct (190K training pairs)\n1-task (1P-1E)\n0.80±0.03\n0.79±0.04\n-\n3-task (1P-1E)\n0.80±0.04\n0.79±0.04\n0.74±0.04\n3-task (1P-2E)\n0.81±0.03\n0.80±0.04\n0.75±0.04\n3-task (2P-1E)\n0.82±0.03\n0.81±0.03\n0.76±0.03\n5-task (1P-1E)\n0.80±0.03\n0.79±0.04\n0.74±0.04\nLlama-3.1-8B-Instruct (1.2M training pairs)\n5-task (1P-1E)\n0.82±0.03\n0.81±0.03\n0.76±0.04\n5-task (1P-2E)\n0.83±0.03\n0.81±0.04\n0.77±0.04\n5-task (2P-1E)\n0.83±0.03\n0.82±0.03\n0.77±0.04\nGemma 3 (1.2M training pairs, 5-task 1P-1E)\ngemma-3-1b-it\n0.77±0.04\n-\n0.72±0.04\ngemma-3-1b-it\n0.78±0.04\n-\n0.73±0.04\nmedgemma-4b-it\n0.80±0.03\n-\n0.74±0.04\n• gemma-3-4b-it: 4B parameters, instruction-\ntuned, multi-modal, open-domain training.\n• medgemma-4b-it:\n4B parameters, multi-\nmodal, instruction-tuned, further pretraining\nand finetuning on biomedical data starting\nfrom gemma-3-4b-it checkpoint.\nFor multimodal models (gemma-3-4b-it and\nmedgemma-4b-it), we load only the text module\nwithout the vision module. Results are shown in Ta-\nble 8. Performance generally increases with either\nmodel size (in parameters) or domain-specific train-\ning. Though Llama 3.1 8B performs better than\nGemma 3 models, it is not clear if this is due only\nto number of parameters, as models of equivalent\nsizes are not available for these two architectures.\nWe also include Gemma 3 models in our analyses\nin Appendices D and H.\nG\nEffect of Embedding Model\nTo explore how well ctELM generalizes over\nembedding models, we compare our original\nBAAI/bge-large-en-v1.5 model to two addi-\ntional options for Eemb:\n• BAAI/bge-large-en-v1.5 (original): open-\ndomain, 1,024 dimensions, 335M parame-\nters (Xiao et al., 2024).\n• Alibaba-NLP/gte-large-en-v1.5:\nopen-\ndomain, 1,024 dimensions, 434M parame-\nters (Zhang et al., 2024b; Li et al., 2023b).\n• NeuML/pubmedbert-base-embeddings:\nbiomedical,\n768 dimensions,\n109M pa-\nrameters;\nmodel\nof\nGu\net\nal.\n(2021)\ncontrastively\nfine-tuned\nusing\nSentence\nTransformers (Reimers and Gurevych, 2019).\nFor\nall\nembedding\nmodels,\nwe\nuse\nLlama-3.1-8B-Instruct as the base model\nand the 1P-1E training procedure on the 1.2M\n5-task training set.\nWe find that ctELM gen-\neralizes well across embedding models, with\nbge-large-en-v1.5 and gte-large-en-v1.5\nboth exceeding the Vec2Text baselines (Table 9).\nHowever, we find here no benefit from the\ndomain-specific pubmedbert-base-embeddings\nmodel. This may be due to the smaller number of\nparameters and fewer embedding dimensions.\nH\nAutomatic Plausibility Analysis\nTo measure plausibility of generated clinical trial\nabstracts under more conditions than feasible with\nhuman experts, we develop an LLM-based win rate\nexperiment to mirror the expert version in §5.1.\nH.1\nMethods\nAs\nthe\ndiscriminator\nagent,\nwe\nemploy\ngpt-4o-2024-11-20 with the prompt shown\nin Fig. 12. Though an LLM discriminator may be\nsubject to systematic self-preference (Panickssery\net al., 2024), using a different model to judge\nthan to generate may ameliorate this problem (Xu\net al., 2024). To avoid the known phenomenon\nof positional bias in LLMS (Gu et al., 2024), we\nrandomize whether the real abstract is first or\nsecond. Further, each win rate for each system is\ncomputed 5 times with different random seeds to\nensure aggregation over different orderings of each\npair.\nAdditionally, many clinical trial abstracts (real\nand generated) contain clinical trial registry identi-\nfiers. As identifiers may be memorized and as-\nsociated with study details in the discriminator\nagent’s model, we redacted any such identifiers\nfrom both real and generated abstracts. We crafted\nregular expressions to capture each format appear-\ning in World Health Organization’s International\nClinical Trials Registry Platform3 as of February\n10, 2025 (Table 10) and replaced matches with\n“[redacted]”.\n3https://www.who.int/tools/\nclinical-trials-registry-platform\n15\n"}, {"page": 16, "text": "Table 6: Common types of Consistency and Fluency errors.\nConsistency\nModel\nError type\nExamples\nctELM\nImprecision of\ndrugs\n“Tropisetron” →“Granisetron” (both 5-HT3 antago-\nnists used as antiemetics)\n“Telithromycin” →“Clarithromycin” (both antibi-\notics used to treat pneumonia)\nIncorrect patient\ncounts\n“463 patients” →“1,000 patients”\n“176 eyes of 152 patients” →“100 eyes from 50\npatients”\nSimplification of\nmulti-arm studies\nRemoving a third “minimal contact CB bibliother-\napy” group from a study that also included (1) a\n6-session Cognitive Behavioral (CB) group and (2) a\ncontrol group that just got educational brochures.\nVec2Text-sect-ft\nDropping\nimportant words\n“daily interruption of sedation” →“daily sedation”\nDropping of statisti-\ncal results\nJumbling of words\nand roots\n“Acupuncture and needle contact were superior to\ncontrol in reducing the muscle hypertonicity of all\nmuscles except SCM”→\"Muscle contact and hyper-\ntouch were superior to needle contact in reducing\nsclerotherapy\"\nNumerical impreci-\nsion\n“age=15.5 years, SD=1.2” →“mean age, 22.5+/-2.5\nyears”\nHallucination of\nnonsensical,\nirrelevant phrases\n“resected apnea”\n“pharmacokinetics of nisoplaban”\n“after initial cigarette-dosing”\n“a single dose of myosinophils (Meltz, n=30)”\n“cumulative femur relapse”\nFluency\nModel\nError type\nExamples\nctELM\nSpacing errors\n“Patients’satisfaction”\nPunctuation errors\n“p<001” (should have a decimal point)\nVec2Text-sect-ft\nIncoherent acronyms “low-grade tuberculosis (LO)”\n“early hip osteoarthritis (EE)”\nLow-complexity\nstretches\n“S-s-s-s-s-s-s-s-s-s-s-s”\n“100/100/100/100/100 ml”\n“a single dose of a single dose of a single dose [...]”\nIllogical phrases\n“Vitamin D deficiency is a risk factor for developing\nsun exposure”\nSpelling errors\n“acanemia”\nPunctuation errors\n“P.05” (should have a “<”)\n16\n"}, {"page": 17, "text": "Table 7: G-Eval consistency and fluency for the best-\nperforming baseline and our model.\nModel\nConsistency\nFluency\nVec2Text-sect-ft\n0.26±0.08\n0.29±0.12\nctELM (1.2M, 5-task, 1P-1E)\n0.34±0.12\n0.92±0.08\nWhich of the following abstracts is\nmore likely to be a real abstract\ndescribing a clinical trial? Return\nonly \"1\" or \"2\".\n1. \"{ abstract1 }\"\n2. \"{ abstract2 }\"\nFigure 12: The discriminator agent prompt. Order of\nreal and generated abstracts is randomized.\nH.2\nResults\nThe LLM-based win rates of abstracts from inter-\npolated embeddings and from embeddings moved\nalong CAVs are shown in Table 11. First, we note\nthe overall similarity in the automatic results to\nhuman results for the conditions tested by both.\nSpecifically, Vec2Text-sect-ft (with original em-\nbeddings) achieves a win rate of 0.02 vs. experts\nand 0.01 vs. the LLM, whereas ctELM based on\nLlama-3.1-8B-Instruct (with interpolated em-\nbeddings) achieves a win rate of 0.44 vs. experts\nand 0.40 vs. the LLM. LLM-based win rates are in\nfact slightly lower than their human counterparts,\nmeaning they were better able to discriminate. This\nsuggests the LLM discriminator agent is reliable\nenough to provide useful results for other condi-\ntions.\nSecond, we note that using novel embeddings\nhas little on affect plausibility of abstracts gener-\nated by ctELM. For Llama-3.1-8B-Instruct, in-\nterpolated embeddings and those produced using\nage CAVs in fact have slightly higher win rates\nthan for original embeddings. Though there are\ndrops for interpolated vs. original embeddings for\nGemma 3 models, their inter-quartile ranges still\noverlap. These results further suggests that ctELM\nhas learned not only to create fluent text descrip-\ntions of existing clinical trials, but has learned a\nmanifold of plausible clinical trials.\nFinally, we note that win rates are lower for\nGemma 3 models. This is not unexpected, given\ntheir lower Semantic Consistency. It is also in\nline with expectations that the large 4B parame-\nter model performs better than the 1B parameter\nmodel. However, we see no benefit here of the con-\ntinued domain-based pretraining and fine-tuning of\nmedgemma-4b-it, which is suprising. It is possi-\nble that the narrow focus of this models additional\ntraining tasks (mostly multiple choice questions)\naffected their general language modeling or instruc-\ntion following capabilities without benefiting this\nparticular biomedical task.\nI\nData Collection for Sex Concept\nActivation Vector\nTo identify a symmetric axis of cohort sex, we col-\nlected clinical trials describing interventions that\ncould apply to both sexes, but that happened to\nonly include one sex in the study cohort. We thus\nsearched PubMed for randomized controlled tri-\nals with only one of the MeSH terms ‘Male’ and\n‘Female,’ and excluding studies with MeSH terms\nrelated to sex-specific conditions (such as prostate\ncancer) or procedures (such as hysterectomy). We\nfurther required one of four gendered nouns (‘men,’\n‘women,’ ‘boys,’ ‘girls’) to appearing in the ‘Ti-\ntle/Abstract field’ to filter out studies with single-\nsex MeSH terms but no mention of cohort sex in\nabstracts. The complete PubMed search strings are\nprovided in Figs. 14 and 15. Search results were\nsorted using the ‘Best Match’ option, and, for each\nsex, the first 25 were collected that satisfied two\nmanually verified criteria: (1) cohort sex was men-\ntioned in the abstract (not just the title), and (2)\nthere were no implied participants of the opposite\nsex, for example, “Study participants: 50 adults\n(23 women; 46%).” We then augmented the data\nto ensure semantically symmetrical pairs by using\ngpt-4o-2024-11-20 (depicted as ‘Augmentation\nagent’ in Fig. 13) to reverse the sex of study partici-\npants in these initial 50 abstracts, using the prompt\nin Fig. 16. This created a total of 100 abstracts\ndescribing clinical trials: 25 real male, 25 real fe-\nmale, 25 synthetic male, and 25 synthetic female.\nAll synthetic samples were manually reviewed for\nsuccessful change of cohort sex and consistency of\nother study details.\nJ\nData Collection for Age Concept\nActivation Vector\nSimilarly to the sex Concept Activation Vector,\nwe collected clinical trials describing interventions\nthat could apply to both children and aged sub-\njects, but that happened to only include one of\n17\n"}, {"page": 18, "text": "Table 8: Effect of base chat models.\nSemantic Consistency is shown for 5 tasks on the test set.\nThe\nbge-large-en-v1.5 embedding model is used with the 1P-1E training procedure on the 1.2M 5-task dataset.\nMean values over the test set are shown with standard deviations. A repetition penalty of 1.2 is used during inference\nfor emb2abs. Best performance for each task is marked in bold.\nBase model\nemb2abs\nemb2sec\nemb2pls\nemb2com\nemb2dif\nLlama-3.1-8B-Instruct\n0.86±0.05\n0.76±0.07\n0.80±0.05\n0.88±0.04\n0.88±0.04\ngemma-3-1b-it\n0.76±0.05\n0.69±0.07\n0.72±0.05\n0.84±0.04\n0.79±0.04\ngemma-3-4b-it\n0.79±0.05\n0.72±0.07\n0.75±0.05\n0.86±0.04\n0.84±0.04\nmedgemma-4b-it\n0.81±0.05\n0.73±0.07\n0.76±0.05\n0.86±0.04\n0.85±0.04\nTable 9: Effect of embedding model. Semantic Consistency is shown for 5-task 1P-1E model across embedding\nmodels. Mean values over the test set are shown with standard deviations. A repetition penalty of 1.2 is used during\ninference for emb2abs. Best performance for each task is marked in bold.\nEmbedding model\nemb2abs\nemb2sec\nemb2pls\nemb2com\nemb2dif\nbge-large-en-v1.5\n0.86±0.05\n0.76±0.07\n0.80±0.05\n0.88±0.04\n0.88±0.04\ngte-large-en-v1.5\n0.83±0.06\n0.76±0.08\n0.76±0.06\n0.85±0.04\n0.89±0.03\npubmedbert-base-embeddings\n0.81±0.09\n0.65±0.14\n0.69±0.10\n0.82±0.07\n0.83±0.07\nRegex\nRegistry\nACTRN[0-9]+\nAustralian New Zealand Clinical Trials Registry\nChiCTR[A-Z0-9-]+\nChinese Clinical Trials Register\nCTIS[0-9-]+\nEuropean Union Clinical Trials Information System\nCTRI[0-9/]+\nClinical Trials Registry - India\nDRKS[0-9]+\nGerman Clinical Trials Register\nEUCTR[0-9a-zA-Z-]+\nEuropean Clinical Trials Register\nIRCT[0-9]+N[0-9]+\nIranian Registry of Clinical Trials\nISRCTN[0-9]+\nUK Clinical Study Register\nITMCTR[0-9]+\nInternational Traditional Medicine Clinical Trial Registry\nJPRN-[a-zA-Z0-9]+\nJapan Primary Registries Network\nKCT[0-9]{7}\nKorean Clinical Research Information Service\nLBCTR[0-9]+\nLebanese Clinical Trials Registry\nNCT[0-9]{8}\nUS National Clinical Trial\nNL-OMON[0-9]+\nOverview of Medical Research in the Netherlands\nPACTR[0-9]+\nPan African Clinical Trials Registry\nRBR-[a-z0-9]+\nBrazilian Clinical Trials Registry\nRPCEC[0-9]{4}\nCuban Registry of Clinical Trials\nSLCTR/\\d+/\\d+\nSri Lanka Clinical Trials Registry\nTCTR[0-9]+\nThai Clinical Trials Registry\nTable 10: Regular Expressions for identifying Clinical Trial Registry identifiers.\nthese groups in the trial. We thus searched PubMed\nfor randomized controlled trials with only one of\nthe top-level MeSH age groups ‘Child’ (defined in\nMeSH as ages 6-12) and ‘Aged’ (defined as age 65\nand above), further excluding the other top-level\ngroups (‘Infant’, ‘Child, Preschool’, ‘Adolescent’,\n‘Adult’, and ‘Middle Aged’). To find studies appli-\ncable across ages, we also excluded age-specific\nstudy elements, such as ‘Schools’ for child stud-\nies or ‘Dementia’ for aged studies. The complete\nPubMed search strings are provided in Figs. 17 and\n18. Again, search results were sorted using the\n‘Best Match’ option, and, for each age group, the\nfirst 25 were collected that satisfied two manually\nverified criteria: (1) subject age was mentioned in\nthe abstract, and (2) there were no implied partici-\npants of other ages. We again augmented the data\nto ensure semantically symmetrical pairs by using\n18\n"}, {"page": 19, "text": "Table 11: Win Rate of abstracts generated from original or modified embedding vectors when presented to an LLM\ndiscriminator along with a real abstract. For ctELM, the 1P-1E training procedure is used with the 5-task 1.2M\ndataset, and all CAVs are applied with |α| = 0.5.\nWin rate by embedding type\nMethod\nBase model\nOrig.\nInterp.\nCAV-sex\nCAV-age\nVec2Text\n-\n0.00±0.00\n-\n-\n-\nVec2Text-ft\n-\n0.01±0.00\n-\n-\n-\nVec2Text-sect\n-\n0.00±0.00\n-\n-\n-\nVec2Text-sect-ft\n-\n0.01±0.00\n-\n-\n-\nctELM\ngemma-3-1b-it\n0.12±0.02\n0.13±0.05\n-\n-\nctELM\ngemma-3-4b-it\n0.31±0.07\n0.29±0.06\n-\n-\nctELM\nmedgemma-4b-it\n0.22±0.05\n0.16±0.02\n-\n-\nctELM\nLlama-3.1-8B-Instruct\n0.39±0.06\n0.40±0.06\n0.38±0.07\n0.40±0.08\ngpt-4o-2024-11-20 to reverse the age of study\nparticipants in these initial 50 abstracts, using the\nprompt in Fig. 19. This created a total of 100 ab-\nstracts describing clinical trials: 25 real child, 25\nreal aged, 25 synthetic child, and 25 synthetic aged.\nAll synthetic samples were manually reviewed for\nsuccessful change of subject age and consistency\nof other study details.\nK\nExtraction of Subject Demographics\nTo evaluate responsiveness of ctELM to CAVs,\nwe employed an extraction agent comprising\ngpt-4o-2024-11-20 with the system messages de-\npicted in Figures 20 and 21 for sex and age, respec-\ntively. For both, the user prompt template was\n“Now process the following abstract: {abstract}\".\nL\nGenerated Examples of Interpolated\nEmbedding\nTo illustrate the generative capabilities of ctELM\non interpolated embeddings, we present three ex-\namples of generated texts (background, objective,\nand result) derived from the average of two distinct\nabstract embeddings. As shown in Fig. 22, Fig. 23,\nand Fig. 24, ctELM successfully synthesizes se-\nmantically coherent sentences that reflect thematic\noverlaps between the paired source abstracts. This\ndemonstrates the model’s capacity to interpolate\nmeaningfully between known research areas. Im-\nportantly, this capability suggests a promising av-\nenue for exploratory scientific hypothesis genera-\ntion. For instance, by sampling embeddings from\nunderrepresented or “empty” regions of the seman-\ntic space (i.e., areas not directly covered by existing\ntraining data), ctELM could be prompted to gener-\nate novel study hypotheses, bridging previously\nunconnected biomedical concepts. This highlights\nthe potential of embedding-based generation as a\ntool for ideation and discovery in literature-based\nresearch.\n19\n"}, {"page": 20, "text": "Augmentation\n    agent ❄\nMale\n      cohort\n       embeddings\nctELM\n❄\nLLM\nLinear model\nVector data\nText data\nMale\n         cohort\n        abstracts\nFemale\n        cohort\n        abstracts\nFemale\n      cohort\n       embeddings\nEmbedding\n       model ❄\nLinear\n   classifier !\nConcept\n       Activation\n      Vector\nMeSH\n               queries\n⊕\nSex\n    extraction\n   agent\n❄\nSynthetic\n         male\n        abstracts\nSynthetic\n        female\n        abstracts\nCAV-guided\n      male\n      abstracts\nCAV-guided\n       female\n        abstracts\nFigure 13: The workflow for modifying clinical trial embeddings with Concept Activation Vectors, including data\ncollection, augmentation, linear model training, generation from modified embeddings, and evaluation. Depicted\nhere is the sex CAV; the same workflow applies to age, except with child and aged instead of male and female.\n20\n"}, {"page": 21, "text": "(\nEnglish[Language]\nAND (Randomized Controlled Trial [Publication Type])\nAND (Male[MeSH Terms])\nNOT (Female[MeSH Terms])\nNOT (Genitalia[MeSH Terms])\nNOT (Urogenital Diseases[MeSH Terms ])\nNOT (Pelvic Neoplasms[MeSH Terms ])\nNOT (Urogenital Surgical Procedures[MeSH Terms ])\nNOT (Fertility Preservation[MeSH Terms ])\nNOT (Contraceptive Devices[MeSH Terms ])\nNOT (Alopecia[MeSH Terms])\nNOT (Gonadal Disorders[MeSH Terms ])\nNOT (Gonadal Hormones[MeSH Terms ])\n) AND\n(\n(\"2024/01/01\"[ EPDAT] : \"2024/12/31\"[ EPDAT ])\n) AND\n(\n(men[Title/Abstract ]) OR (boys[Title/Abstract ])\n)\nFigure 14: The PubMed search string for male single-sex clinical trials.\n(\nEnglish[Language]\nAND (Randomized Controlled Trial[Publication Type])\nAND (Female[MeSH Terms])\nNOT (Male[MeSH Terms])\nNOT (Pregnancy[MeSH Terms])\nNOT (Menopause[MeSH Terms])\nNOT (Genitalia[MeSH Terms])\nNOT (Urogenital Diseases[MeSH Terms ])\nNOT (Breast Neoplasms[MeSH Terms ])\nNOT (Pelvic Neoplasms[MeSH Terms ])\nNOT (Urogenital Surgical Procedures[MeSH Terms ])\nNOT (Menstruation Disturbances[MeSH Terms ])\nNOT (Osteoporosis , Postmenopausal[MeSH Terms ])\nNOT (Fertility Preservation[MeSH Terms ])\nNOT (Contraceptive Devices[MeSH Terms ])\nNOT (Gonadal Disorders[MeSH Terms ])\nNOT (Gonadal Hormones[MeSH Terms ])\n) AND\n(\n(\"2024/01/01\"[ EPDAT] : \"2024/12/31\"[ EPDAT ])\n) AND\n(\n(women[Title/Abstract ]) OR (girls[Title/Abstract ])\n)\nFigure 15: The PubMed search string for female single-sex clinical trials.\n21\n"}, {"page": 22, "text": "Modify this abstract so the subjects are {'male ','female '} rather than\n{'female ','male '}. Output only the abstract , with no quotes or formatting.\n\"{ abstract }\"\nFigure 16: The prompt template for symmetric augmentation of abstracts for the sex Concept Activation Vector.\n(\nEnglish[Language]\nAND (Randomized Controlled Trial[Publication Type])\nAND (Child[MeSH Terms])\nNOT (Child , Preschool[MeSH Terms ])\nNOT (Infant[MeSH Terms])\nNOT (Adolescent[MeSH Terms])\nNOT (Adult[MeSH Terms])\nNOT (Middle Aged[MeSH Terms])\nNOT (Aged[MeSH Terms])\nNOT (Immunization Schedule[MeSH Terms ])\nNOT (Child Behavior[MeSH Terms)\nNOT (Growth Disorders[MeSH Terms ])\nNOT (Growth Hormone[MeSH Terms])\nNOT (Growth and Development[MeSH Terms ])\nNOT (Tooth , Deciduous[MeSH Terms ])\nNOT (Child Abuse[MeSH Terms])\nNOT (Family[MeSH Terms])\nNOT (Schools[MeSH Terms])\nNOT (Curriculum[MeSH Terms])\nNOT (Congenital , Hereditary , and Neonatal Diseases and Abnormalities\n[MeSH Terms])\nNOT (Neurodevelopmental Disorders[MeSH Terms ])\n) AND\n(\n(\"2024/01/01\"[ EPDAT] : \"2024/12/31\"[ EPDAT ])\n)\nFigure 17: The PubMed search string for child single-age-group clinical trials.\n(\nEnglish[Language]\nAND (Randomized Controlled Trial[Publication Type])\nAND (Aged[MeSH Terms])\nNOT (Child[MeSH Terms])\nNOT (Child , Preschool[MeSH Terms ])\nNOT (Infant[MeSH Terms])\nNOT (Adolescent[MeSH Terms])\nNOT (Middle Aged[MeSH Terms])\nNOT (Breast Neoplasms[MeSH Terms ])\nNOT (Dementia[MeSH Terms])\nNOT (Polypharmacy[MeSH Terms])\nNOT (Activities of Daily Living[MeSH Terms ])\n) AND\n(\n(\"2024/01/01\"[ EPDAT] : \"2024/12/31\"[ EPDAT ])\n)\nFigure 18: The PubMed search string for aged single-age-group clinical trials.\n22\n"}, {"page": 23, "text": "Modify this abstract so the subjects are {'children ','older adults '} rather\nthan {'older adults ','children '}. Include specific ages. Output only the\nabstract , with no quotes or formatting.\n\"{ abstract }\"\nFigure 19: The prompt template for symmetric augmentation of abstracts for the age Concept Activation Vector.\nYou are a biomedical natural language processing assistant. Given the abstract\nof a clinical trial study , your task is to identify the gender of the study\npopulation.\nYour output must be in the following JSON format:\n{\n\"gender \": \"female\"\n// or \"male\" or \"neutral\"\n}\nGuidelines:\n- If the abstract mentions that the study participants are women or females ,\noutput \"female \".\n- If the abstract mentions men or males , output \"male\".\n- If the abstract only mentions the number of participants without specifying\ngender , output \"neutral \".\n- If both male and female participants are mentioned and the study includes\nboth , still output \"neutral \".\n- Do not infer gender based on disease or context. Only use explicit statements.\nFigure 20: The subject sex extraction agent system message.\n23\n"}, {"page": 24, "text": "You are a biomedical natural language processing assistant. Given the abstract\nof a clinical trial study , your task is to extract or infer the average age (in\nyears) of the study population.\nYour output must be in the following JSON format:\n{\n\"age\": 54.3\n// numerical value only\n}\nGuidelines:\n1. If the study mentions the **mean or average age**, extract and return that\nvalue.\n2. If the study mentions an **age range ** (e.g., \"30 to 50 years\"), compute the\naverage (e.g., (30+50)/2 = 40.0) and return that value.\n3. If no explicit age value is mentioned , infer the most likely average age\nbased on population group terms in the text , using this mapping:\n- \"Child , Preschool \": 2-5 years\n-> 3.5\n- \"Child \": 6-12 years\n-> 9\n- \"Adolescent \": 13-18 years\n-> 15.5\n- \"Adult \": 19-44 years\n-> 31.5\n- \"Middle Aged\": 45-64 years\n-> 54.5\n- \"Aged\": 65+ years\n-> 75\n- \"Aged , 80 and over\": 80+ years\n-> 85\n- \"Octogenarians \": 80-89 years\n-> 84.5\n- \"Nonagenarians \": 90-99 years\n-> 94.5\n- \"Centenarians \": 100+ years\n-> 100\nChoose the most appropriate inferred value if only a population label is\npresent.\nOnly include the JSON output. Do not explain or add commentary.\nFigure 21: The subject age extraction agent system message.\n24\n"}, {"page": 25, "text": "Example 1: Generated Background of Interpolated Embedding\nGenerated Background Section of Interpolated Embedding between PMID=24099432 and\nPMID=17064200: “This paper presents baseline data from a randomized clinical trial examining\nthe effectiveness of a cognitive behavioral intervention (CBI) for improving medication adherence\nand depression outcomes among patients with poorly controlled hypertension.”\nPartial Abstract for PMID=24099432 (Cognitive Behavioral Therapy, Depressive disorder):\n“[Objective] We tested whether a brief cognitive behavioral (CB) group and bibliotherapy prevention\nreduce major depressive disorder (MDD) onset, depressive symptoms, and secondary outcomes\nrelative to brochure controls in adolescents with self-reported depressive symptoms when school\npersonnel recruit participants and deliver the intervention. ... [Results] The finding that a brief\nCB group intervention delivered by real-world providers significantly reduced MDD onset relative\nto both brochure control and bibliotherapy is very encouraging, although effects on continuous\noutcome measures were small or nonsignificant and approximately half the magnitude of those\nfound in efficacy research, potentially because the present sample reported lower initial depression.”\nPartial Abstract for PMID=17064200 (Hypertension): “[Objective] To examine potential\nthreats to internal and external study validity caused by differential patient withdrawal from a\nrandomized controlled trial evaluating pharmacist management of hypertension, to compare the\ncharacteristics of patients who withdrew with those of patients who completed the study, and to\nidentify characteristics that predispose patients to withdraw from hypertension management. ...\n[Results] Therefore, internal validity was preserved, and outcomes from the study groups could\nbe reliably compared. A lack of significant differences between patients who withdrew versus\nthose who completed, with the exception of insurance status, suggests that external validity was\nnot jeopardized.”\nFigure 22: Example of generated background from interpolated embedding between two clinical trials. The\ngenerated text reflects a synthesis of themes related to cognitive behavioral therapy and hypertension.\n25\n"}, {"page": 26, "text": "Example 2: Generated Objective Section of Interpolated Embedding\nGenerated\nObjective\nof\nInterpolated\nEmbedding\nbetween\nPMID=15914575\nand\nPMID=9777179: “A study was conducted to determine if irritable bowel syndrome (IBS) patients\nwith depressive symptoms have a better response to a selective serotonin reuptake inhibitor (SSRI)\nthan those without depressive symptoms.”\nPartial Abstract for PMID=15914575 (Irritable bowel syndrome): “[Background] Melatonin,\na sleep promoting agent, is involved in the regulation of gastrointestinal motility and sensation.\n[Objective] We aimed to determine if melatonin was effective in improving bowel symptoms and\nsleep disturbances in irritable bowel syndrome (IBS) patients with sleep disturbance. ... [Results]\nThe findings suggest that the beneficial effects of melatonin on abdominal pain in IBS patients with\nsleep disturbances are independent of its action on sleep disturbances or psychological profiles.”\nPartial Abstract for PMID=9777179 (Chronic mental illness): “[Objective] We sought to\nidentify baseline predictors of response to clozapine. Data were from a 15-site randomized clinical\ntrial comparing clozapine and haloperidol in hospitalized patients with refractory schizophrenia\n(n = 423). Three-month outcomes were analyzed with the full sample (n=368 due to attrition).\n... [Results] Although high levels of symptoms were associated with greater improvement on\nclozapine, these findings are not robust enough to suggest that any specific, clinically defined\nsubgroup of refractory patients should be preferentially targeted for clozapine treatment.”\nFigure 23: Generated objective from the interpolation of abstract embeddings concerning irritable bowel syndrome\nand chronic mental illness. The output proposes a novel trial scenario integrating elements of both parent studies.\nExample 3: Generated Results Section of Interpolated Embedding\nGenerated\nResult\nof\nInterpolated\nEmbedding\nbetween\nPMID=15166570\nand\nPMID=12860586:\n“The results of this study demonstrate that patients with functional\nconstipation have a better outcome after treatment with a single dose of subcutaneous diamorphine\nthan after treatment with a single dose of subcutaneous hyoscine butylbromide.”\nPartial Abstract for PMID=15166570 (Opioids, Morphine, Urinary tract dysfunction):\n“[Background] Intrathecal administration of opioids may cause lower urinary tract dysfunction.\nIn this study, the authors compared the effects of morphine and sufentanil administered intrathe-\ncally in a randomized double-blind fashion (two doses each) on lower urinary tract function in\nhealthy male volunteers. ... [Conclusion] Intrathecal opioids decrease bladder function by causing\ndose-dependent suppression of detrusor contractility and decreased sensation of urge. Recovery\nof normal lower urinary tract function is significantly faster after intrathecal sufentanil than after\nmorphine, and the recovery time is clearly dose dependent.”\nPartial Abstract for PMID=12860586 (Dyspepsia): “[Background] The value of the test-and-\ntreat strategy in the approach to dyspepsia has been evaluated only in a few secondary care studies.\nMost patients with dyspepsia, however, are treated by their primary care physician ... [Conclusion]\nThe test-and-treat strategy proved to be as effective and safe as prompt endoscopy. Only a minority\nof patients were referred for endoscopy after the test-and-treat approach.”\nFigure 24: Example of a generated result sentence from interpolated embeddings of abstracts on opioids and\ndyspepsia. The output blends insights into drug response and gastrointestinal outcomes, demonstrating semantic\nconsistency across domains.\n26\n"}]}