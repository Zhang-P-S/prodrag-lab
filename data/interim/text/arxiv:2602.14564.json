{"doc_id": "arxiv:2602.14564", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.14564.pdf", "meta": {"doc_id": "arxiv:2602.14564", "source": "arxiv", "arxiv_id": "2602.14564", "title": "Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation", "authors": ["Shefayat E Shams Adib", "Ahmed Alfey Sani", "Ekramul Alam Esham", "Ajwad Abrar", "Tareque Mohmud Chowdhury"], "published": "2026-02-16T08:53:23Z", "updated": "2026-02-16T08:53:23Z", "summary": "Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.14564v1", "url_pdf": "https://arxiv.org/pdf/2602.14564.pdf", "meta_path": "data/raw/arxiv/meta/2602.14564.json", "sha256": "8f0609a93a897332665fb81c8556dbd9a23c2ba14e4942d525395e1ae94edc1c", "status": "ok", "fetched_at": "2026-02-18T02:19:13.305520+00:00"}, "pages": [{"page": 1, "text": "2025 28th International Conference on Computer and Information Technology (ICCIT)\n19-21 December 2025, Cox’s Bazar, Bangladesh\nAssessing Large Language Models for Medical QA:\nZero-Shot and LLM-as-a-Judge Evaluation\nShefayat E Shams Adib, Ahmed Alfey Sani, Ekramul Alam Esham, Ajwad Abrar, Tareque Mohmud Chowdhury\nDepartment of Computer Science and Engineering, Islamic University of Technology, Gazipur, Bangladesh\nEmail: {shefayatadib, ahmedalfey, ekramulalam, ajwadabrar, tareque}@iut-dhaka.edu\nAbstract—Recently, Large Language Models (LLMs) have\ngained significant traction in medical domain, especially in devel-\noping a QA systems to Medical QA systems for enhancing access\nto healthcare in low-resourced settings. This paper compares\nfive LLMs deployed between April 2024 and August 2025 for\nmedical QA, using the iCliniq dataset, containing 38,000 medical\nquestions and answers of diverse specialties. Our models include\nLlama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct,\nLlama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are\nusing a zero-shot evaluation methodology and using BLEU and\nROUGE metrics to evaluate performance without specialized\nfine-tuning. Our results show that larger models like Llama\n3.3 70B Instruct outperform smaller models, consistent with\nobserved scaling benefits in clinical tasks. It is notable that,\nLlama-4-Maverick-17B exhibited more competitive results, thus\nhighlighting evasion efficiency trade-offs relevant for practical\ndeployment. These findings align with advancements in LLM\ncapabilities toward professional-level medical reasoning and re-\nflect the increasing feasibility of LLM-supported QA systems in\nthe real clinical environments. This benchmark aims to serve\nas a standardized setting for future study to minimize model\nsize, computational resources and to maximize clinical utility in\nmedical NLP applications.\nIndex Terms—Large Language Models, Medical Question An-\nswering, Zero-shot Evaluation, Healthcare NLP, LLM-as-a-Judge\nI. INTRODUCTION\nRecent advances in Large Language Models (LLMs) have\ndemonstrated promising capabilities in medical research,\nshowing enhanced decision-making and assessment abilities\nwithin healthcare domains [1], [2]. While achieving strong\nperformance across various biomedical tasks [3], LLMs have\nfaced criticism for displaying unfairness in broader NLP ap-\nplications [4]. Rigorous testing to ensure accuracy, safety, and\nclinical utility becomes essential before widespread healthcare\nadoption of these technologies [5], [6].\nCurrent evaluation practices attempt to capture healthcare\nrealities by leveraging datasets derived from multiple-choice\nmedical examinations, which provide strong benchmarks for\nassessing precision [2], [7]. Notably, DeepSeek-R1 has per-\nformed on par with, if not better, than the proprietary models\non such benchmarks, while also revealing considerable perfor-\nmance variation across different model architectures [7].\nMedical applications continue to present considerable chal-\nlenges for LLMs despite technological advances. A persistent\ngap between benchmark performance and real-world clinical\neffectiveness has been highlighted in recent studies [5], [6].\nCurrent models exhibit competence in medical question an-\nswering; however, they frequently lack the specialized medical\nknowledge depth required for complex decision-making [2],\n[5]. Given the accelerating pace of LLM development and\nincreasing demand for AI-assisted healthcare in resource-\nlimited contexts, comprehensive evaluation of contemporary\nmodel generations becomes essential for determining their\npractical applicability in medical settings.\nWe selected five state-of-the-art LLMs with diverse ar-\nchitectural approaches and parameter scales to address this\ncritical gap: Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3\n70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-\n5-mini. Released between April 18, 2024, and August 7, 2025,\nthese models range from efficiency-focused 3B parameter\nvariants to large-scale 70B parameter architectures, featuring\nvarious innovations including mixture-of-experts design and\nenhanced instruction-following capabilities [8], [8]. For our\nevaluation, we utilized the iCliniq 1 dataset, which contains\nwell-structured medical questions and answers covering di-\nverse medical specialties. The dataset provides accurate clin-\nical contexts across a wide range of medical domains. This\ncomprehensive coverage enables proper assessment of LLMs\nacross various medical knowledge and reasoning tasks [9].\nThis evaluation aims to compare the performance of the\nfive mentioned well-capable LLMs across the iCliniq dataset,\nby applying zero-shot evaluation methodology and computing\nthe BLEU and ROUGE scores to assess the performance of\neach of them [2]. This will help us understand the current\ncapabilities and limitations of LLMs in the healthcare sector so\nthat research for development in AI-assisted healthcare appli-\ncations becomes more reachable. The study’s key contributions\nare as follows:\n• Comprehensive\nMulti-Dimensional\nBenchmarking:\nWe provide zero-shot evaluation of five advanced LLMs\n(Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B\nInstruct, Llama-4-Maverick-17B-128E-Instruct, GPT-5-\nmini) on the iCliniq medical QA dataset using traditional\nautomatic metrics (BLEU, ROUGE) and LLM-as-a-Judge\nframework evaluating medical accuracy, completeness,\nsafety, clarity, and helpfulness.\n• Parameter Scaling and Clinical Deployment Analysis:\nWe demonstrate correlations between model size and\nmedical QA performance, revealing Llama 3.3 70B’s su-\n1https://www.kaggle.com/datasets/henry41148/icliniq-medical-qa-38k\nAccepted in 28th ICCIT, 2025\narXiv:2602.14564v1  [cs.CL]  16 Feb 2026\n"}, {"page": 2, "text": "Models\nCollected\nData from\niCliniq\nDataset\nRandomly\nSelected 3K Q&A\nZero-Shot\nPrompts\nResult\nAnalysis\nLLM\nGenerated\nOutputs\nReference\nAnswers\n(Ground Truth)\nLlama-3-8B\nLlama-3.2-3B\nLlama-3.3-70B\nLlama-4-Maverick\nGPT-5-mini\nLLM as a\nJudge\nStandard \nMetrics\nBLEU Score\nROUGE Score\nFig. 1. Methodology pipeline for evaluating LLM performance on medical question answering tasks.\nperior performance and Llama-4-Maverick 17B’s compet-\nitive efficiency with fewer parameters. The analysis pro-\nvides actionable deployment guidance for high-accuracy\nclinical environments versus resource-constrained set-\ntings.\n• Standardized Dual-Evaluation Framework: A method-\nology combining automatic metrics with LLM-as-a-Judge\nclinical quality assessment using five specialized criteria\naddresses n-gram limitations and establishes benchmark-\ning protocols for future medical LLM research.\nII. LITERATURE REVIEW\nThe rapid progress of Large Language Models (LLMs)\nhas driven expanding adoption in the biomedical healthcare\ndomain, particularly for medical question answering (QA).\nEarly developments focused on domain-specific pretraining,\nwith models like BioBERT and ClinicalBERT demonstrating\nimproved performance on clinical notes and structured data.\nMed-BERT further expanded this approach to electronic health\nrecords [10]. However, when applied to generative QA tasks\nwhere precision and security are paramount, these early mod-\nels remained constrained in their capabilities.\nThe establishment of more complex models marked a\ndefining shift in medical LLM development. Med-PaLM 2\nachieved expert-level clinical reasoning with 86.5% accu-\nracy on the MedQA dataset, demonstrating unprecedented\nsuccess on medical license exams [1]. The introduction of\ngeneral-purpose models like GPT-3 and GPT-4 brought notable\nadvancements, with Med-PaLM and Med-PaLM-2 achiev-\ning near-expert performance on medical licensing tests and\nbenchmarks [1]. Domain-specific applications further demon-\nstrated efficiency gains through targeted customization, such as\nVaxBot-HPV, which achieved excellent accuracy and fidelity\nscores for vaccine-related queries while reducing hallucina-\ntions through specialized fine-tuning [11].\nResearch evolution has continued with reasoning-enhanced\nmodels and collaborative frameworks across multiple LLMs,\nshowing promise in reducing inter-model disagreement and\nincreasing diagnostic precision [12]. However, subsequent\nresearch revealed persistent challenges: hallucinations and\ninconsistent reasoning remained problematic in real-world\nclinical datasets despite strong benchmark performance [6],\n[13]. MedLM research highlighted that while prompt engi-\nneering and fine-tuning enhanced output quality, they could\nnot completely resolve reliability and factual grounding issues\n[10].\nRecent research has emphasized broader, more comprehen-\nsive evaluations. Ethics and reasoning-focused datasets like\nMedEthicsQA and MEDIQ have exposed critical flaws in\nsensitive clinical decision-making scenarios [14], [15]. Bench-\nmarking efforts have revealed significant performance gaps\nacross model sizes and architectures [2], [5], with perfor-\nmance varying considerably across languages and contexts,\nas demonstrated by multilingual benchmarks such as BRIDGE\n[7]. Parallel investigations have explored domain-targeted fine-\ntuning approaches and collaborative methods combining mul-\ntiple models to minimize inconsistency [12]. Knowledge-\naugmented techniques like KoSEL have aimed to improve\nfactual accuracy, though often at the cost of computational\nefficiency [16].\nCurrent research indicates that larger models, such as Llama\n3.3 70B, consistently outperform smaller variants [8]. How-\never, the overall advancement in biomedical QA has been\nless dramatic than anticipated. Despite steadily rising bench-\nmark scores, fundamental challenges persist: hallucinations,\ninconsistent ethical reasoning, and limited applicability to real\npatient inquiries remain problematic. This gap emphasizes the\nnecessity of thorough, dataset-driven assessments using real-\nworld medical data, such as those conducted with iCliniq,\nto evaluate genuine performance improvements rather than\nrelying solely on benchmark achievements. Our study directly\naddresses this critical gap by benchmarking recent models, in-\ncluding Llama 3.x series, Llama-4 Maverick, and GPT-5-mini,\nto assess whether newer model generations deliver measurable\nenhancements in practical medical QA applications.\nIII. METHODOLOGY\nThis paper presents a thorough zero-shot evaluation frame-\nwork to evaluate five state-of-the-art large language models’\nperformance on answering medical questions. Our adopted\nmethodology adheres to established standards in medical NLP\nassessment [2], [5] while modifying the unique requirements\nof zero-shot situations.\nA. Dataset\nWe have used the iCliniq Medical QA dataset, an extensive\ncollection of 38,000 real-world medical questions and expert-\nvalidated responses from a variety of medical specialties.\n"}, {"page": 3, "text": "TABLE I\nSAMPLE QUESTION-ANSWER PAIR FROM THE ICLINIQ DATASET\nQuestion: Hello doctor, I have recently been diagnosed with type 2 diabetes. Could you provide me with a simplified explanation of how this condition\naffects my body and what steps I can take to keep my blood sugar levels stable throughout the day?\nAnswer: Hello, Welcome to icliniq.com. I read your query and understand your concern. If you have already been diagnosed with diabetes, it is\nmandatory to keep it under control at all times with the help of diet. Exercise and maintain an active lifestyle and medications prescribed by the\ntreating doctor. Walking, yoga, and exercise as health permits started gradually will help. Check your blood sugar levels and other parameters like liver\nfunction test, kidney function test, and lipid profile regularly. Watch what you eat. Small frequented meals might help. Stay away from sweets, sugar,\nhoney, jaggery, marmalade, coconut water, juices, biscuits, rusk, and junk packaged food. Avoid potatoes, white rice, green peas, beetroot, and carrot.\nTake the dieticians opinion for a detailed diet chart depending on height weight religion culture body mass index likes and dislikes. Avoid stress and\ntension. Deep breathing and meditation will help. Go for green leafy vegetables, salad, and fruits take only papaya, guava, and kiwi. Replace white\nrice with brown rice and bread and flour with multi-grains. Avoid fried oily junk alcohol smoking. Take foods with low glycemic index. I hope I\ncleared all your doubts. Thank you.\nThe dataset, covering a wide range of medical illnesses,\ntreatments and other health concerns, represents authentic\npatient inquiries made to medical practitioners. From this\nlarge dataset, to ensure computational feasibility, we randomly\nselected 3,000 question-answer pairs for our evaluation while\nretaining statistical significance and representativeness across\nmedical fields.\nThe chosen subset preserves the original distribution of\nmedical specialties included in the entire dataset, such as\ngeneral medicine, cardiology, dermatology, gastroenterology\nand pediatrics. Each question-answer pair in our evaluation\nset consists of a natural language patient query and an expert-\nvalidated response. Given the complexity and variability of\nmedical consultations, the average question length is approxi-\nmately 126 tokens, while reference answers average length is\n192 tokens2.\nB. Models\nWe selected five well-known large language models that\nrepresent various architectural philosophies, parameter sizes,\nand release schedules to provide a thorough evaluation of the\ncurrent landscape of LLMs:\n1) Llama-3-8B-Instruct\n(April\n2024):\nAn\n8\nbillion-\nparameter version of Meta’s Llama-3 architecture. It is\ninstruction-tuned and optimized for following detailed\ninstructions and maintaining conversational context [8].\n2) Llama 3.2 3B (September 2024): A smaller version\nof the Llama architecture with 3 billion parameters. It\nis very good at interpreting language and is meant for\neffective use [8].\n3) Llama 3.3 70B Instruct (December 2024): This is\nthe largest model in our study, featuring 70 billion\nparameters. This one is one the of the most advanced\nopen-source language models, offering better reasoning\nskills [8].\n4) Llama 4 Maverick 17B-128E Instruct (April 2025):\nThis is an updated version with 17 billion parameters\nthat aims to improve instruction-following abilities and\ncontext understanding [17].\n5) GPT-5-mini (August 2025): This is one of the latest\ncompact models from OpenAI’s GPT series. It uses\n2https://www.kaggle.com/code/ahmedalfeysani/calculating-the-tokens\nadvanced training methods and provides better efficiency\nfor different NLP tasks [18].\nC. Zero-Shot Evaluation Framework\nFollowing established practices in medical NLP evaluation\n[2], [19], we implemented a zero-shot learning approach\nwhere models generate responses without task-specific fine-\ntuning or in-context examples. This methodology evaluates\nthe models’ inherent medical knowledge and reasoning ca-\npabilities acquired during pre-training, providing insights into\ntheir practical applicability in real-world medical consultation\nscenarios [1], [7].\nFor each model, we used a standardized prompt template\ndesigned to elicit comprehensive and medically appropriate\nresponses:\nAs a medical AI assistant, please provide a thorough and\naccurate answer to the following medical question. Your\nresponse should be informative, evidence-based, and ap-\npropriate for patient education while maintaining medical\naccuracy.\nQuestion: [MEDICAL-QUESTION]\nThis prompt design ensures consistency across model eval-\nuations while encouraging responses that balance medical\naccuracy with patient comprehensibility, following recommen-\ndations from medical AI evaluation studies [6], [14].\nD. Evaluation Metrics\n1) Traditional Automatic Metrics: We employed standard-\nized automatic evaluation metrics commonly used in medical\nNLP research to assess model performance across multiple\ndimensions [2], [15]:\n• BLEU Scores: Measures n-gram overlap between gener-\nated and reference answers, providing insights into lexical\nsimilarity and content coverage [2].\n• ROUGE Scores: Evaluates recall-oriented overlap, as-\nsessing how well generated answers capture essential in-\nformation from reference responses. ROUGE-1 measures\nunigram overlap, ROUGE-2 focuses on bigram over-\nlap for contextual coherence, and ROUGE-L evaluates\nlongest common subsequence overlap [2].\n"}, {"page": 4, "text": "These metrics provide complementary perspectives on answer\nquality, with BLEU focusing on precision and ROUGE em-\nphasizing recall, together offering a comprehensive view of\nmodel performance in medical question answering tasks [15],\n[20].\n2) LLM-as-a-Judge Evaluation Framework: While tradi-\ntional n-gram based metrics like BLEU and ROUGE provide\nvaluable insights into lexical similarity, they have significant\nlimitations in evaluating medical question answering systems\n[2], [15]. These metrics cannot assess the factual correctness\nof medical claims, evaluate whether responses include ap-\npropriate medical disclaimers, or measure clinical utility in\nactual medical scenarios [21]. To address these limitations, we\nimplemented an LLM-as-a-Judge evaluation framework that\nprovides more nuanced assessment of medical QA quality,\ncomplementing our traditional automatic metrics [21], [22].\nWe employed a systematic evaluation approach based on\nClaude Sonnet 4 as our judge model framework. The evalu-\nation assesses responses across five key dimensions using a\n5-point Likert scale with detailed scoring rubrics to ensure\nconsistent evaluation:\n1) Medical Accuracy (1-5): Factual correctness of med-\nical information provided, including appropriate use of\nmedical qualifiers and avoidance of definitive diagnoses\nwithout proper context.\n2) Completeness (1-5): Whether the response adequately\naddresses all aspects of the medical question, consider-\ning both breadth and depth of coverage relative to the\nreference answer.\n3) Safety (1-5): Appropriateness of safety disclaimers,\navoidance of harmful advice, and inclusion of recom-\nmendations to consult healthcare professionals when\nappropriate.\n4) Clarity (1-5): How clearly and understandably the\nmedical information is communicated, including sen-\ntence structure, use of medical terminology, and patient-\nfriendly language.\n5) Helpfulness (1-5): Practical utility of the response to the\npatient, including actionable advice, next steps guidance,\nand overall practical value.\nThe overall score is calculated using weighted clinical\nimportance: Medical Accuracy (30%), Safety (25%), Com-\npleteness (20%), Helpfulness (15%), and Clarity (10%). This\nweighting prioritizes medical correctness and patient safety\nwhile acknowledging the importance of comprehensive and\npractical guidance. The evaluation criteria and scoring method-\nology were reviewed and validated by a qualified medical\nprofessional to ensure clinical relevance and appropriateness.\nThe evaluation was conducted on the same 300 responses per\nmodel used for BLEU and ROUGE metrics to ensure direct\ncomparability across evaluation methodologies.\nIV. RESULTS AND DISCUSSION\nWe present the performance evaluation of five state-of-the-\nart LLMs on medical question answering using the iCliniq\ndataset, employing zero-shot evaluation methodology with\nBLEU and ROUGE metrics as shown in Table I.\nA. Performance Evaluation\nTo evaluate the performance of the models on medical ques-\ntion answering, we used BLEU-1, ROUGE-1, and ROUGE-L\nmetrics [2]. The evaluation results indicate that among the\ntested models, Llama 3.3 70B Instruct achieved the highest\nperformance across all metrics, despite not being fine-tuned\nspecifically for medical question answering tasks.\nLlama 3.3 70B Instruct surpassed all other models, attaining\nthe top scores in BLEU-1 (0.2207), ROUGE-1 (0.2761),\nand ROUGE-L (0.1306). This superior performance can be\nattributed to its large parameter count of 70 billion, which\nenables better capture of complex medical knowledge and rea-\nsoning patterns [8]. The model’s instruction-tuned architecture\nalso contributes to its ability to generate more coherent and\ncontextually appropriate medical responses [1].\nLlama-4-Maverick 17B demonstrated competitive perfor-\nmance, ranking second in most metrics with ROUGE-1\n(0.2597) and ROUGE-L (0.1260) scores. Despite having sig-\nnificantly fewer parameters (17 billion) compared to Llama\n3.3 70B, this model achieved remarkably close performance,\nhighlighting the efficiency gains from newer architectural\ninnovations and the mixture-of-experts approach [17]. Other\nmodels like Llama 3.2 3B and Llama-3-8B-Instruct also\nperformed competitively across metrics, demonstrating the\neffectiveness of the Llama family in medical QA tasks [8],\n[8].\nHowever, GPT-5-mini exhibited suboptimal performance\nacross all metrics, recording a BLEU-1 score of 0.0124 and the\nlowest ROUGE-L score of 0.0914 among the assessed models.\nThis poor performance may be attributed to potential issues\nin the evaluation setup, model configuration, or the specific\nimplementation of GPT-5-mini used in our experiments [18].\nThe relatively lower performance also reflects difficulties with\nlexical overlap and fluency in medical question answering\ntasks.\nThe relatively lower BLEU-1 scores across all models\ncompared to ROUGE-1 scores can be explained by the strict\nnature of precision-based evaluation. BLEU-1 emphasizes\nexact unigram matches, which are critical for evaluating\nlexical precision but may be overly restrictive in medical\nQA, where multiple valid phrasings can convey the same\nclinical information [2]. As noted in prior work [5], zero-\nshot LLMs, while strong in general language understanding,\nare not fine-tuned for medical contexts. This domain gap\ncan reduce exact lexical matching but still allows models to\nproduce semantically coherent responses, which is reflected in\nrelatively higher ROUGE scores [6].\nB. Parameter Scaling and Architecture Analysis\nThe results of our study show a clear correlation between\nthe size of the model and its performance in medical question-\nanswering tasks. The analysis reveals that parameter scaling\nsignificantly impacts model capabilities, with Llama 3.3 70B’s\n"}, {"page": 5, "text": "TABLE II\nZERO-SHOT PERFORMANCE EVALUATION OF LLMS ON THE ICLINIQ MEDICAL QA DATASET\nModel\nBLEU-1\nBLEU-4\nROUGE-1\nROUGE-2\nROUGE-L\nLlama-3-8B-Instruct\n0.1739\n0.0127\n0.2419\n0.0379\n0.1219\nLlama 3.2 3B\n0.2012\n0.0122\n0.2588\n0.0355\n0.1258\nLlama 3.3 70B Instruct\n0.2207\n0.0141\n0.2761\n0.0404\n0.1306\nLlama-4-Maverick 17B 128E Instruct\n0.2089\n0.0132\n0.2597\n0.0381\n0.1260\nGPT-5-mini\n0.0124\n0.0065\n0.2024\n0.0290\n0.0914\nsuperior performance largely attributable to its 70 billion\nparameters [8]. This finding aligns with previous research\non scaling laws in language models and their application to\nmedical domains [1], [5].\nInterestingly, Llama-4-Maverick 17B achieved competitive\nresults with less than a quarter of the parameters of Llama\n3.3 70B, suggesting that architectural innovations can partially\ncompensate for parameter limitations. The mixture-of-experts\narchitecture employed in Maverick models appears to provide\nefficiency benefits while maintaining competitive performance\nin medical reasoning tasks [17]. This efficiency makes it\nparticularly suitable for deployment in clinical settings with\nlimited computational infrastructure [7].\nThe standard deviations observed across all models (ranging\nfrom ±0.0622 to ±0.0956 for BLEU-1) indicate considerable\nvariability in performance across different medical questions.\nThis variability suggests that model performance is highly\ndependent on question complexity, medical specialty, and the\nspecific medical knowledge required for accurate responses\n[10], [15].\nWe present the performance evaluation of five state-of-the-\nart LLMs on medical question answering using the iCliniq\ndataset, employing zero-shot evaluation methodology with\nBLEU and ROUGE metrics as shown in Table II. For compar-\native context, we also include baseline results reported in prior\nwork (MedLM) using GPT-3.5, T5, and GPT-2 (Table III).\nComparison with MedLM Baselines\nWhen compared to the MedLM baseline models with\ncorrected decimal values, our evaluated Llama variants\ndemonstrate substantial improvements across all metrics. The\nMedLM baseline BLEU-1 scores show GPT-2 (0.0998), T5\n(0.0984), and GPT-3.5 (0.0243), while our best-performing\nLlama 3.3 70B achieved 0.2207, representing a 2.2x improve-\nment over the strongest MedLM baseline. Even our smallest\nTABLE III\nBASELINE MEDLM MODEL PERFORMANCE ON THE ICLINIQ DATASET\n(FROM PRIOR WORK) [23]\nModel\nBLEU-1\nBLEU-4\nROUGE-1\nROUGE-L\nGPT-3.5\n0.0243\n0.0009\n0.0022\n0.0019\nT5\n0.0984\n0.0080\n0.0017\n0.0014\nGPT-2\n0.0998\n0.0085\n0.0019\n0.0012\nmodel, GPT-5-mini (0.0124), performs comparably to GPT-\n3.5, while our mid-range models substantially outperform all\nbaselines [2].\nThe performance gap is even more pronounced in ROUGE\nmetrics. Our Llama 3.3 70B achieved ROUGE-1 of 0.2761\ncompared to MedLM’s highest score of 0.0022 (GPT-3.5),\nrepresenting a 125x improvement. For ROUGE-L, Llama 3.3\n70B (0.1306) vastly outperforms the best MedLM baseline\nof 0.0019 (GPT-3.5), showing a 68x improvement. This dra-\nmatic performance enhancement extends across all our models,\nwith even Llama-3-8B-Instruct (ROUGE-1: 0.2419) achieving\nover 100x better performance than MedLM baselines. These\nsubstantial improvements demonstrate the significant advance-\nment in medical QA capabilities achieved by modern LLMs\ncompared to earlier generation models [21].\nC. LLM-as-a-Judge Evaluation Results\nThe LLM-as-a-Judge evaluation provides complementary\ninsights to traditional BLEU and ROUGE metrics, revealing\ncritical aspects of medical QA quality not captured by n-\ngram overlap measures. Table IV presents the comprehensive\nevaluation results combining both traditional automatic metrics\nand judge assessment across all five models.\nThe LLM-as-a-Judge evaluation demonstrates remarkable\nconsistency with traditional BLEU and ROUGE metrics, with\nLlama 3.3 70B achieving the highest overall score (4.40/5.00),\nfollowed by Llama-4-Maverick (4.23/5.00), and GPT-5-mini\nrecording the lowest performance (3.16/5.00). This alignment\nvalidates our comprehensive assessment approach [21]. Llama\n3.3 70B demonstrated exceptional medical accuracy (4.83/5),\naligning with its highest BLEU-4 and ROUGE-2 scores, while\nLlama-4-Maverick achieved the highest clarity score (4.97/5),\nTABLE IV\nLLM-AS-A-JUDGE OVERALL PERFORMANCE RANKINGS\nModel\nOverall Score\nLlama 3.3 70B Instruct\n4.40\nLlama-4-Maverick 17B 128E\n4.23\nLlama-3-8B-Instruct\n3.77\nLlama 3.2 3B\n3.20\nGPT-5-mini\n3.16\n"}, {"page": 6, "text": "reflecting structured presentation and patient-friendly commu-\nnication [8], [17]. GPT-5-mini achieved the highest safety\nscore (3.80/5) despite poor traditional metrics, highlighting the\nimportance of multi-dimensional evaluation [18].\nROUGE-1 shows strong positive correlation (r = 0.89) with\noverall judge scores, while BLEU metrics show moderate\ncorrelation (r = 0.67) with medical accuracy. Quality dis-\ntributions mirror traditional rankings: Llama 3.3 70B (88%\nhigh-quality responses ≥4.0), Llama-4-Maverick (84%),\nLlama-3-8B (65%), Llama-3.2-3B (25%), and GPT-5-mini\n(23%). Llama 3.3 70B emerges as most suitable for high-\naccuracy scenarios with available computational resources,\nwhile Llama-4-Maverick presents an attractive balance for\nresource-constrained environments, achieving 94% of Llama\n3.3 70B’s ROUGE-1 performance with significantly fewer\nparameters [9], [17], [19]. The convergence between evalu-\nation methodologies strengthens confidence in performance\nhierarchies and highlights the potential of large pre-trained\nmodels in medical QA tasks without task-specific training [11],\n[16].\nV. CONCLUSION\nThis study presented a comparative zero-shot evaluation\nof five advanced LLMs on the iCliniq medical QA dataset\nusing BLEU and ROUGE metrics. Our findings demonstrate\nthat Llama 3.3 70B Instruct provides the most accurate and\nconsistent responses, while Llama-4 Maverick 17B shows\ncompetitive efficiency-oriented performance. In contrast, GPT-\n5-mini underperformed across metrics, highlighting limita-\ntions of compact architectures in specialized domains such as\nmedicine.\nOverall, the results reinforce the scaling benefits of larger\nLLMs but also reveal promising architectural improvements\nthat enable smaller models to achieve near-comparable ac-\ncuracy. Importantly, our study underscores that automatic\nmetrics like BLEU and ROUGE alone are insufficient to assess\nclinical reliability, pointing to the necessity of incorporating\nfactual correctness, ethical compliance, and explainability in\nfuture evaluations. This benchmark provides a standardized\nframework to guide ongoing research toward deploying LLMs\nsafely and effectively in real-world medical decision support.\nREFERENCES\n[1] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin,\nL. Hou, K. Clark, S. R. Pfohl, H. Cole-Lewis et al., “Toward expert-\nlevel medical question answering with large language models,” Nature\nMedicine, vol. 31, no. 3, pp. 943–950, 2025.\n[2] Q. Chen, Y. Hu, X. Peng, Q. Xie, Q. Jin, A. Gilson, M. B. Singer,\nX. Ai, P.-T. Lai, Z. Wang et al., “Benchmarking large language models\nfor biomedical natural language processing applications and recommen-\ndations,” Nature communications, vol. 16, no. 1, p. 3280, 2025.\n[3] A. Abrar, N. T. Oeshy, P. Maheru, F. Tabassum, and T. M. Chowdhury,\n“Faithful summarization of consumer health queries: A cross-lingual\nframework with llms,” in 5th Muslims in ML Workshop co-located with\nNeurIPS 2025, 2025.\n[4] A. Abrar, N. T. Oeshy, M. Kabir, and S. Ananiadou, “Religious bias\nlandscape in language and text-to-image models: Analysis, detection,\nand debiasing strategies,” AI & SOCIETY, pp. 1–27, 2025.\n[5] S. Shool, S. Adimi, R. Saboori Amleshi, E. Bitaraf, R. Golpira, and\nM. Tara, “A systematic review of large language model (llm) evaluations\nin clinical medicine,” BMC Medical Informatics and Decision Making,\nvol. 25, no. 1, p. 117, 2025.\n[6] S. Bedi, Y. Liu, L. Orr-Ewing, D. Dash, S. Koyejo, A. Callahan, J. A.\nFries, M. Wornow, A. Swaminathan, L. S. Lehmann et al., “A systematic\nreview of testing and evaluation of healthcare applications of large\nlanguage models (llms),” medRxiv, pp. 2024–04, 2024.\n[7] J. Wu, B. Gu, R. Zhou, K. Xie, D. Snyder, Y. Jiang, V. Carducci,\nR. Wyss, R. J. Desai, E. Alsentzer et al., “Bridge: Benchmarking large\nlanguage models for understanding real-world clinical practice text,”\narXiv preprint arXiv:2504.19467, 2025.\n[8] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of\nmodels,” arXiv e-prints, pp. arXiv–2407, 2024.\n[9] J. Abrantes, “Assessing large language models for medical question\nanswering in portuguese: Open-source versus closed-source approaches,”\nCureus, vol. 17, no. 5, 2025.\n[10] S. Narula, S. Karkera, R. Challa, S. Virmani, N. Chilukuri, M. Elkas,\nN. Thammineni, A. Kamath, P. Jaiswal, and A. Krishnan, “Testing the\naccuracy of modern llms in answering general medical prompts,” Int. J.\nSoc. Sci. Econ. Res, vol. 8, 2023.\n[11] Y. Li, J. Li, M. Li, E. Yu, D. Rhee, M. Amith, L. Tang, L. S. Savas,\nL. Cui, and C. Tao, “Vaxbot-hpv: a gpt-based chatbot for answering hpv\nvaccine-related questions,” JAMIA open, vol. 8, no. 1, p. ooaf005, 2025.\n[12] K. Shang, C.-H. Chang, and C. C. Yang, “Collaboration among multiple\nlarge language models for medical question answering,” arXiv preprint\narXiv:2505.16648, 2025.\n[13] M. M. Lucas, J. Yang, J. K. Pomeroy, and C. C. Yang, “Reasoning\nwith large language models for medical question answering,” Journal\nof the American Medical Informatics Association, vol. 31, no. 9, pp.\n1964–1975, 2024.\n[14] J. Wei, Z. Meng, Z. Xiao, T. Hu, Y. Feng, Z. Zhou, J. Wu, and\nZ. Liu, “Medethicsqa: A comprehensive question answering benchmark\nfor medical ethics evaluation of llms,” arXiv preprint arXiv:2506.22808,\n2025.\n[15] S. Li, V. Balachandran, S. Feng, J. Ilgen, E. Pierson, P. W. W. Koh,\nand Y. Tsvetkov, “Mediq: Question-asking llms and a benchmark for\nreliable interactive clinical reasoning,” Advances in Neural Information\nProcessing Systems, vol. 37, pp. 28 858–28 888, 2024.\n[16] Z. Zeng, Q. Cheng, X. Hu, Y. Zhuang, X. Liu, K. He, and Z. Liu,\n“Kosel: Knowledge subgraph enhanced large language model for medi-\ncal question answering,” Knowledge-Based Systems, vol. 309, p. 112837,\n2025.\n[17] B. Tang, C. C. Fu, F. Kou, G. Sizov, H. Zhang, J. Park, J. Liu, J. You,\nQ. Yang, S. Mehta et al., “Efficient speculative decoding for llama\nat scale: Challenges and solutions,” arXiv preprint arXiv:2508.08192,\n2025.\n[18] M. Hu, Z. Eidex, S. Wang, M. Safari, Q. Li, and X. Yang, “Benchmark-\ning gpt-5 for zero-shot multimodal medical reasoning in radiology and\nradiation oncology,” arXiv preprint arXiv:2508.13192, 2025.\n[19] A. Arias-Duart, P. A. Martin-Torres, D. Hinjos, P. Bernabeu-Perez, L. U.\nGanzabal, M. G. Mallo, A. K. Gururajan, E. Lopez-Cuena, S. Alvarez-\nNapagao, and D. Garcia-Gasulla, “Automatic evaluation of healthcare\nllms beyond question-answering,” arXiv preprint arXiv:2502.06666,\n2025.\n[20] Y. Zhu, W. Tang, H. Yang, J. Niu, L. Dou, Y. Gu, Y. Wu, W. Zhang,\nY. Sun, and X. Yang, “The potential of llms in medical education: gen-\nerating questions and answers for qualification exams,” arXiv preprint\narXiv:2410.23769, 2024.\n[21] H. Li, Q. Dong, J. Chen, H. Su, Y. Zhou, Q. Ai, Z. Ye, and Y. Liu, “Llms-\nas-judges: a comprehensive survey on llm-based evaluation methods,”\narXiv preprint arXiv:2412.05579, 2024.\n[22] J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen,\nS. Ma, H. Liu et al., “A survey on llm-as-a-judge,” arXiv preprint\narXiv:2411.15594, 2024.\n[23] N. Yagnik, J. Jhaveri, V. Sharma, and G. Pila, “Medlm: Exploring lan-\nguage models for medical question answering systems,” arXiv preprint\narXiv:2401.11389, 2024.\n"}]}