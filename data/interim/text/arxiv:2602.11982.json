{"doc_id": "arxiv:2602.11982", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.11982.pdf", "meta": {"doc_id": "arxiv:2602.11982", "source": "arxiv", "arxiv_id": "2602.11982", "title": "Automatic Simplification of Common Vulnerabilities and Exposures Descriptions", "authors": ["Varpu Vehomäki", "Kimmo K. Kaski"], "published": "2026-02-12T14:12:58Z", "updated": "2026-02-12T14:12:58Z", "summary": "Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on investigating how large language models (LLMs) could be utilized in automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions. Automatic text simplification has been studied in several contexts, such as medical, scientific, and news texts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of cyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions, evaluated by two groups of cyber security experts in two survey rounds. We have found that while out-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation. Code and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification\\_nmi.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.11982v1", "url_pdf": "https://arxiv.org/pdf/2602.11982.pdf", "meta_path": "data/raw/arxiv/meta/2602.11982.json", "sha256": "9882d78c25f1638eaf4c7a68b14fc6151369b0f5078c19e54b82da15c95cb4f4", "status": "ok", "fetched_at": "2026-02-18T02:19:21.446048+00:00"}, "pages": [{"page": 1, "text": "AUTOMATIC SIMPLIFICATION OF COMMON VULNERABILITIES\nAND EXPOSURES DESCRIPTIONS\nVarpu Vehomäki, Kimmo Kaski\nDepartment of Computer Science\nAalto University School of Science\nEspoo\nFinland\nfirstname.lastname@aalto.fi\nABSTRACT\nUnderstanding cyber security is increasingly important for individuals and organizations. However, a\nlot of information related to cyber security can be difficult to understand to those not familiar with the\ntopic. In this study, we focus on investigating how large language models (LLMs) could be utilized\nin automatic text simplification (ATS) of Common Vulnerability and Exposure (CVE) descriptions.\nAutomatic text simplification has been studied in several contexts, such as medical, scientific, and news\ntexts, but it has not yet been studied to simplify texts in the rapidly changing and complex domain of\ncyber security. We created a baseline for cyber security ATS and a test dataset of 40 CVE descriptions,\nevaluated by two groups of cyber security experts in two survey rounds. We have found that while\nout-of-the box LLMs can make the text appear simpler, they struggle with meaning preservation.\nCode and data are available at https://version.aalto.fi/gitlab/vehomav1/simplification_nmi.\nKeywords artificial intelligence · natural language processing · cyber security · text simplification\n1\nIntroduction\nText simplification is the process of re-writing a natural language text in a less complicated manner, while preserving\nthe original meaning of the text. For the rapidly increasing and specialised text corpora of today, there is growing need\nto perform this process automatically or semi-automatically using Artificial Intelligence (AI) and Large Language\nModels (LLMs). This process is called Automatic Text Simplification (ATS).\nText simplification is often used to make information accessible to language learners, people with cognitive disabilities,\nor those without sufficient contextual expertise, which divides the goal of text simplification into two main styles,\nnamely Plain Language and easy-read [1]. Plain Language is aimed at people without domain expertise, while easy-read\ntext also attempts to be understandable for those with cognitive disabilities [1]. Here we will focus on the Plain\nLanguage approach while leaving the easy-read approach outside the scope of the present investigation.\nSome of the fields of research in which ATS is actively studied are scientific [2, 3] and medical [4, 5] texts, but not yet\nin the complex context of cyber security. It often happens that even experts in one field of research will have difficulty\nunderstanding research articles somewhat outside of their own field [6], which has recently served as motivation for\nmany research-related ATS studies. To make interdisciplinary communication easier, the authors of [7] studied making\ntexts more understandable through the detection of personalized jargon. Their system takes into account the background\nof the user and modifies the text to make it more easily understandable. It was found that those less familiar with\nthe domain preferred term explanations and that those with some more knowledge found contextualizing complex\nterminology useful.\nAs it comes to using LLMs in ATS, it should be noted that they can also cause harm even when used for non-malicious\npurposes. LLMs’ biases can also show in text simplification and, for certain groups of end users, simplified text may\nturn out to be misleading [1]. If important information is incorrectly simplified, there is a risk of harm, which without\nproper human oversight can result in a responsibility gap and thus needing extra attention to be paid in the ATS process.\narXiv:2602.11982v1  [cs.CL]  12 Feb 2026\n"}, {"page": 2, "text": "For example, a question can be raised that who is held responsible if an ATS system omits a key detail in a simplified\nversion of a cyber security report? In addition to the audience of the simplified reports possibly being misled, such\nincidents can erode people’s trust in the ATS system and in the organization using it.\nTo avoid these problems in text simplification, collaboration with the intended audience, domain experts, and developers\nhas been proposed [1]. One of the problems is factuality in commonly used text simplification corpora and in the\noutput of text simplification models. It has also been shown that the metrics used to evaluate the performance of\ntext simplification models do not capture factuality [1]. In addition to human oversight, collaborative systems and\ntransparency about the use of ATS are ways to make information more reliable [8].\nResponsibility in text simplification does not only cover the possible mistakes in the simplified text. Certain ATS\nsolutions can shift the responsibility of understanding to the reader. It is important that there is an attempt to write\naccessible source text and that ATS is only used as a tool to assist in writing text so that it is accessible to a wider\naudience. Writing original texts in complex language because an ATS system is available for the intended audience to\nuse should not be the goal [9].\nIn todays cyber security attacks, written reports of incidents are an important part of a security analyst’s job. These\nreports bring together important information on attempted and successful attacks against businesses, infrastructures,\nand other entities. However, these reports tend to be technical and thus hard to understand for those who are not experts\nin cyber security. This poses a major challenge especially to strategy level decision makers of e.g. infrastructure\norganisations. However, making a simplified version of such a report can be time-consuming and challenging for\nsomeone not familiar with the task. In addition, a rapid increase in the number of reports of cybersecurity incidents,\nvulnerabilities, and exploits calls for the development of methods or tools for automatic text simplification (ATS).\nIn order to make these cyber security reports better accessible for non-expert audiences, we introduce an AI-assisted\nATS method for simplifying descriptions of Common Vulnerabilities and Exposures (CVE) descriptions. Furthermore,\nwe create a small human-evaluated semi-synthetic dataset of simplified CVE descriptions to be used for testing and\ncomparison purposes of ATS systems.\n2\nMaterials and methods\nFor the dataset to be used in this study, we randomly chose 100 Common Vulnerability and Exposure (CVE) descriptions\npublished in 2025 from the CVElistV5 repository1. Of these 100 CVE descriptions, we chose 40 for human evaluation,\nand the remaining 60 descriptions were used as an evaluation set in the development of the local simplification model.\nBefore simplifying the 40 CVEs in the human evaluation set, we manually cleaned up the CVE descriptions, i.e.,\nremoving log excerpts and other non-natural language texts. We excluded non-natural language because it is outside the\nscope of this study.\nThe test set of 40 CVEs was first simplified with the GPT4o model, using the Azure OpenAI API2. These initial\nsimplifications were made in April 2025 using the GPT version gpt-4o-2024-11-20. After the first round of human\nevaluation, we resimplified these 40 CVE for the second round of human evaluation. These resimplifications were\nperformed using the ChatGPT GUI to enable real-time interaction with the model. Here we used the same model as in\nthe case of initial simplifications, namely GPT-4o, and these resimplifications were performed in July 2025.\nThe initial simplifications were performed at the sentence level, as many of the evaluation metrics commonly used in\nATS are based on it. The CVEs were split into sentences using the sentencizer from the spaCY library3. Then these\nsentences were given to the GPT4o model one by one, and then the simplification was aligned with the original sentence.\nSubsequent improvements to the initial simplifications were made at the document level. The reason for this was that\nthe human evaluation was done at the document level, as assessing the quality and meaning preservation is more easily\ndone at the document level than at the sentence level.\nAs commercial LLMs lack transparency and user control, we tested and trained some open-source models specifically\nfor this task. We experimented with several different approaches, like training a BART model using domain-adaptive\npre-training (DAPT) and running open-source LLMs locally. After some initial testing, we settled on an agent system\nusing the 4 billion parameter version of Gemma 3 [10]. We chose this model mainly for its good performance in relation\nto size, as it can be run on a single GPU system yielding a coherent output.\nFigure 1 shows the pipeline structure of the agent-based Gemma model system or, in short, GemmaAgent. As a first\nstep of the process, the original CVE is fed into a term extraction agent that outputs a list of terms in the text. For\n1https://github.com/CVEProject/cvelistV5\n2https://learn.microsoft.com/en-us/azure/ai-foundry/openai/overview\n3https://spacy.io/api/sentencizer\n2\n"}, {"page": 3, "text": "the term extraction stage, we experimented with the Gemma base model and several named entity recognition (NER)\nmodels. For the final system to extract terms, we used the AITSecNER4 model. Here we restricted the term extraction\nto terms with labels ’CON’, ’MALWARE’, ’TACTIC’, ’TECHNIQUE’ or ’TOOL’, because we found these categories\nto be most relevant for the understandability of the CVE. The terms extracted by the NER model are then fed into a\nretrieval augmented generation (RAG) agent with access to a database containing cybersecurity lexica and dictionaries.\nThe RAG agent writes explanations of the extracted terms based on the information found in the database. Finally, the\nsimplification agent receives the original CVE description along with the term explanations and instructions to simplify\nthe original CVE using the term explanations as support.\nFigure 1: A figure illustrating the structure of the GemmaAgent.\n2.1\nEvaluation\nSeveral metrics have been created to assess the performance of text simplification methods. One of them is SARI [11]\nwhich is a sentence-level metric to evaluate the quality of a simplified sentence by comparing it to the original sentence\nand a simplified reference sentence. In this paper, we use D-SARI, a version of SARI to evaluate document-level text\nsimplification [12]. Like the original SARI metric, D-SARI requires a reference to perform the evaluation. The formula\nfor D-SARI is the following:\nD-SARI = Dkeep + Ddel + Dadd\n3\nwhere Dkeep, Dadd and Ddel are the addition, deletion and keep operations defined in [11] multiplied by the penalty\nscores defined in [12].\n4https://huggingface.co/selfconstruct3d/AITSecNER\n3\n"}, {"page": 4, "text": "The Flesch-Kincaid Grade Level (FKGL) is another common ATS evaluation metric [13], which is calculated as follows\nFKGL = 0.39 · ASL + 11.8 · ASW −15.59.\nwhere ASL = average sentence length in words and ASW = average number of syllables per word. In [14] it was argued\nthat FKGL is not a suitable metric for automatic text simplification and that it is better to separately report the average\nsentence length and the average number of syllables, as they are the components of FKGL.\nThe other important dimension of text simplification is the preservation of meaning, which can be evaluated, for\nexample, by using BERTScore [15] and semantic similarity. The BERTScore is calculated as follows:\nP = 1\nm\nm\nX\ni=1\nmax\nj∈[1,n] cos (h(xi), h(yj))\nR = 1\nn\nn\nX\nj=1\nmax\ni∈[1,m] cos (h(xi), h(yj))\nBERTScoreF 1 = 2PR\nP + R\nwhere P = token level precision, R = token level recall and h = embedding function. For more details on the BERTScore\nformula, refer to [15]. Semantic similarity is yet another metrics to measure how close two pieces of text are in meaning.\nThe formula for the semantic similarity is as follows\nSemantic similarity(sc, sr) = cos (e(sc), e(sr))\nwhere e = embedding function, sc = candidate sentence, sr = reference sentence and cos(x, y) = cosine similarity\nbetween vectors x and y. We use two different embedding models and libraries to calculate semantic similarity:\nMeaningBERT [16] and Sentence-BERT [17] with the nomic-embed-text-v1.55 model.\nIn this study we have also performed evaluations of the 40 randomly chosen CVE descriptions, simplified by GPT-4o,\nin the form of a human survey using a 3-point Likert scale. We conducted two rounds of human surveys: the first round\nwith 12 participants and the second round with 10 participants, all with some level of cyber security expertise.\nParticipants in the first survey were shown the original and simplified CVE descriptions and asked to respond to two\nstatements: \"The simplification is easier to understand than the original\" and \"The simplification preserves the meaning\nof the original text\". The respondents were given the same three response options for both statements: agree, neither\nagree nor disagree, and disagree. There was also an optional comment field where respondents could add comments\nabout the simplifications.\nThe first round or initial simplifications that received more than 80% \"agree\" responses and no \"disagree\" responses to\nboth statements above were automatically considered high-quality and were left out of the second human evaluation\nround. For the second round of simplifications, we prompted the GPT-4o model to improve the initial simplifications.\nWe presented the model with the original prompt, instructions on what to do next, the original CVE, the initial\nsimplification, and the comments given on each CVE by the participants of the first survey.\nIn the second round of human survey, we asked the participants to evaluate the quality of the first and second text\nsimplification versions and also compare the first and second round simplification, adding the statement: \"The second\nround simplification is of better quality than the first\".\n3\nResults\nTable 1 shows the results of the D-SARI scores for the GPT4o, Gemma3:4b, and GemmaAgent models. For all of these\nmodels, the scores turned out to be overall low, but among them Gemma3:4b got the highest score of 0.14. As D-SARI\nis the only reference-based metric used, the low scores can be attributed to differences between the simplifications and\nthe references. However, we expect a much larger number of verified reference simplifications to improve the values of\nthese scores.\n5https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\n4\n"}, {"page": 5, "text": "Table 2 shows the results of BERTScore, MeaningBERT, and SentenceBERT scores for the Semi-synthetic, GPT4o,\nGemma3:4b, and GemmaAgent models. It turned out that Semi-synthetic and GemmaAgent simplifications performed\nbest on the meaning preservation metrics. The semi-synthetic data got the best BERTScore, while GemmaAgent got a\nslightly higher MeaningBERT score. The semantic similarity values for both models were the same. Gemma 3:4b got\nthe lowest BERTScore and Semantic similarity score, so we can conclude that adding retrieval augmented generation\n(RAG) in the simplification process helps with meaning preservation. The MeaningBERT score for Gemma3:4b was\nslightly higher than for GPT-4o.\nTable 3 shows the results for the system level FKGL scores as well as the average word, syllable, and sentence counts\nof the original and simplified CVEs. For the calculations of these scores, we used the lftk-library6. The simplifications\nproduced by GPT-4o achieved somewhat better scores for referenceless automatic simplicity metrics than the original\ndescriptions. The FKGL score for the original descriptions was 12.45, while GPT-4o reduced the score to 9.49.\nGemma3:4b shortened the descriptions, based on the average number of words and sentences, while the other models\nproduced simplifications that were longer than the original text. In addition, we calculated the average number of named\nentities for the original and simplification CVEs, using the AITSecNER NER model. All other models turned out to\nreduce the number of named entities, except the GemmaAgent model.\nModel\nD-SARI\nGPT-4o\n0.09\nGemma3:4b\n0.14\nGemmaAgent\n0.09\nTable 1: System level D-SARI scores, test set results.\nModel\nBERTScore\nMeaningBERT\nSBERT\nSemi-synthetic\n0.60\n76.62\n0.92\nGPT-4o\n0.58\n71.66\n0.90\nGemma3:4b\n0.50\n72.80\n0.86\nGemmaAgent\n0.56\n76.94\n0.92\nTable 2: System level scores for meaning preservation, test set results.\nModel\nFKGL\nAverage number\nof words\nAverage number\nof sentences\nAverage number of\nsyllables per word\nAverage number of\nnamed entities\nOriginal\n12.45\n43.68\n2.08\n1.62\n2.6\nSemi-synthetic\n10.74\n59.58\n2.55\n1.39\n2.23\nGPT-4o\n9.49\n54.85\n2.53\n1.35\n1.65\nGemma3:4b\n14.50\n34.65\n1.3\n1.61\n1.33\nGemmaAgent\n14.20\n55.3\n2.1\n1.61\n2.85\nTable 3: System level scores for simplicity, test set results.\nIn addition, it turned out that out of 40 CVE simplifications, only five simplifications were accepted from the first round\nof human evaluations without modifications to the second round. Based on these evaluations, the remaining 35 were\nmodified and passed on to the second round of human evaluation.\n4\nDiscussion\nIn the first round of simplifications, it was found out that the GPT-4o would not return simplifications for certain types\nof prompt, like \"Exploitation of this issue requires user interaction in that a victim must open a malicious file\". We\nsuspect that this is caused by GPT-4o guardrails that prevent it from outputting the so-called \"harmful content\". To\nensure that the evaluations would not be affected by such a technical issue, the prompts with which a simplification\nwas not returned were manually substituted with the original sentence before the evaluations. Furthermore, GPT-4o\nturned out to be the only model in our experiments that reduced the FKGL scores of the descriptions from the originals’,\nreaching even a better FKGL score than the semi-synthetic data. The GPT-4o produced longer simplifications with more\nsentences and less syllables per word than the other fully automatic systems, which explains the lower FKGL score.\n6https://github.com/brucewlee/lftk\n5\n"}, {"page": 6, "text": "Although GPT-4o mostly preserved information that should be left untouched, such as version numbers, without specific\ninstructions to do so, there was at least one instance in which the model did not correctly deal with the version number.\nFor example, in CVE-2025-322027 GPT-4o incorrectly rounded the version number from 4.3000000025 to 4.3. Stating\nthat version information should be left untouched in the prompt could help avoid problems like this.\nOne of the interesting findings is that the GemmaAgent model performed well in the meaning preservation metrics, but\ndid not lower the complexity of the text, based on the metrics in Table 3. In addition, it turned out to be the only model\nproducing a higher average number of named entities than the original CVEs. This could be caused by the focus on\nexplaining terms in the prompt given to the model.\nThe semi-synthetic data went through multiple iterations, and the simplifications were improved on the basis of the\nfeedback from domain experts, as can be expected. These referenceless automatic metrics show that the test set we have\nproduced turned out to be a good reference for ATS testing in cybersecurity. The biggest challenge in the simplifications\nturned out to be the preservation of meaning, which caused the most disagreement between human evaluators.\nOne of the key observations of this study is that it is important to consider the audience or end users to whom the\nsimplifications are aimed. Here, our objective was to produce text that would be generally understandable to people\nwith limited understanding of the cyber security domain. We found that even within an audience, there is variation on\nthe level of understanding and that needs to be taken into consideration when developing text simplification systems.\nEqually important is that, if the simplifications produced by LLMs are used in the decision-making process of a\ncompany, it is crucial that the information in the simplified report is factual and that no important points are left out.\n5\nConclusions\nOur study shows that automatic evaluation metrics do not consistently follow human understanding of text simplicity. It\nhas also shown that meaning preservation is a big challenge in ATS. Some of the problems with meaning preservation\ncan be dealt with by improving prompting, but there are also aspects of meaning that are not easily measured and thus\nare harder to evaluate.\nMore research is needed on human-in-the-loop simplification systems and proper human evaluations of these systems.\nOur surveys were targeted towards cyber security professionals, but it can be very challenging for them to evaluate how\nunderstandable the text would be to someone without their background, while a non-expert will not be able to reliably\nevaluate the meaning preservation.\nA larger database of cyber security information could improve the performance of the RAG model and make simplifica-\ntions more reliable. More detailed investigation of the effects of RAG in ATS is needed.\n5.1\nConflict of interest\nThe authors of this study declare no conflict of interest.\nReferences\n[1] Nils Freyer, Hendrik Kempt, and Lars Klöser. Easy-read and large language models: on the ethical dimensions of\nLLM-based text simplification. 26(3):50.\n[2] Syed Muhammad Ali, Hammad Sajid, Owais Aijaz, Owais Waheed, Faisal Alvi, and Abdul Samad. Team\nsharingans at simpletext: fine-tuned llm based approach to scientific text simplification. In Working Notes of the\nConference and Labs of the Evaluation Forum (CLEF 2024), pages 3174–3181, 2024.\n[3] Andrianos Michail, Pascal Severin Andermatt, and Tobias Fankhauser. Simpletext best of labs in clef-2023:\nScientific text simplification using multi-prompt minimum bayes risk decoding. In Lorraine Goeuriot, Philippe\nMulhem, Georges Quénot, Didier Schwab, Giorgio Maria Di Nunzio, Laure Soulier, Petra Galušˇcáková, Alba\nGarcía Seco de Herrera, Guglielmo Faggioli, and Nicola Ferro, editors, Experimental IR Meets Multilinguality,\nMultimodality, and Interaction, pages 227–253, Cham, 2024. Springer Nature Switzerland.\n[4] Laurens Van Den Bercken, Robert-Jan Sips, and Christoph Lofi. Evaluating neural text simplification in the\nmedical domain. In The World Wide Web Conference, pages 3286–3292. ACM.\n[5] Brian Ondov, Kush Attal, and Dina Demner-Fushman. A survey of automated methods for biomedical text\nsimplification. Journal of the American Medical Informatics Association, 29(11):1976–1988, 2022.\n7https://nvd.nist.gov/vuln/detail/CVE-2025-32202\n6\n"}, {"page": 7, "text": "[6] Liana Ermakova, Eric SanJuan, Stéphane Huet, Hosein Azarbonyad, Olivier Augereau, and Jaap Kamps. Overview\nof the clef 2023 simpletext lab: Automatic simplification of scientific texts. In International Conference of the\nCross-Language Evaluation Forum for European Languages, pages 482–506. Springer, 2023.\n[7] Yue Guo, Joseph Chee Chang, Maria Antoniak, Erin Bransom, Trevor Cohen, Lucy Lu Wang, and Tal August.\nPersonalized Jargon Identification for Enhanced Interdisciplinary Communication. Proceedings of the conference.\nAssociation for Computational Linguistics. North American Chapter. Meeting, 2024:4535–4550, June 2024.\n[8] Ashwin Devaraj, William Sheffield, Byron Wallace, and Junyi Jessy Li. Evaluating factuality in text simplification.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 7331–7345, Dublin, Ireland,\nMay 2022. Association for Computational Linguistics.\n[9] Jenia Kim, Stefan Leijnen, and Lisa Beinborn. Considering Human Interaction and Variability in Automatic Text\nSimplification. In Proceedings of the Third Workshop on Text Simplification, Accessibility and Readability (TSAR\n2024), pages 52–60, Miami, Florida, USA, 2024. Association for Computational Linguistics.\n[10] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron,\nJean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu,\nFrancesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng,\nNoveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan\nEyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran\nKazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen,\nAbhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng,\nAlexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil\nDas, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal\nPiot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo,\nC. J. Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas,\nDivyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga,\nEugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-\nPluci´nska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor,\nIvan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph\nFernandez, Josh Newlan, Ju-yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus\nGreff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank\nChaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev,\nNilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil\nCulliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie\nWu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan\nGirgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan\nEiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent\nRoseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein\nZhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila\nBabar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher,\nTris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell,\nYossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis,\nKoray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin,\nSebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and\nLéonard Hussenot. Gemma 3 Technical Report, March 2025. arXiv:2503.19786 [cs].\n[11] Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. Optimizing statistical machine\ntranslation for text simplification. 4:401–415, 2016.\n[12] Renliang Sun, Hanqi Jin, and Xiaojun Wan. Document-level text simplification: Dataset, criteria and baseline.\n[13] Rudolph Flesch. A new readability yardstick. Journal of applied psychology, 32(3):221, 1948.\n[14] Teerapaun Tanprasert and David Kauchak. Flesch-Kincaid is Not a Text Simplification Evaluation Metric. In\nProceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages\n1–14, Online, 2021. Association for Computational Linguistics.\n[15] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating Text\nGeneration with BERT, February 2020. arXiv:1904.09675 [cs].\n[16] David Beauchemin, Horacio Saggion, and Richard Khoury. MeaningBERT: assessing meaning preservation\nbetween sentences. Frontiers in Artificial Intelligence, 6, September 2023. Publisher: Frontiers.\n7\n"}, {"page": 8, "text": "[17] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,\nAugust 2019. arXiv:1908.10084 [cs].\n8\n"}]}