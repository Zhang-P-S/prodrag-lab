{"doc_id": "arxiv:2512.06812", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.06812.pdf", "meta": {"doc_id": "arxiv:2512.06812", "source": "arxiv", "arxiv_id": "2512.06812", "title": "Large Language Model-Based Generation of Discharge Summaries", "authors": ["Tiago Rodrigues", "Carla Teixeira Lopes"], "published": "2025-12-07T12:14:41Z", "updated": "2025-12-07T12:14:41Z", "summary": "Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.06812v1", "url_pdf": "https://arxiv.org/pdf/2512.06812.pdf", "meta_path": "data/raw/arxiv/meta/2512.06812.json", "sha256": "a0d1c39cf7f753b551c4f90607c319f6f725f80cc38a55957be168da6f362dfb", "status": "ok", "fetched_at": "2026-02-18T02:25:02.991238+00:00"}, "pages": [{"page": 1, "text": "Large Language Model–Based Generation of Discharge Summaries\nTiago Rodrigues1,2* and Carla Teixeira Lopes1,2\n1Faculty of Engineering, University of Porto, Porto, Portugal.\n2INESC TEC, Porto, Portugal.\n*Corresponding author(s). E-mail(s): up201907021@edu.fe.up.pt;\nAbstract\nDischarge Summaries are documents written by medical professionals that detail a patient’s visit to a care\nfacility. They contain a wealth of information crucial for patient care, and automating their generation could\nsignificantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical\npatient information is easily accessible and actionable. In this work, we explore the use of five Large Language\nModels on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gem-\nini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and\nreference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting,\noutperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source\nmodels, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallu-\ncinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the\nsummaries generated by proprietary models. Despite the challenges, such as hallucinations and missing infor-\nmation, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic\ndischarge summary generation as long as data privacy is ensured.\n1 Introduction\nWhen a patient is admitted to a hospital, critical information is documented in clinical notes. These notes are\ncrucial for tracking a patient’s issues and progress and facilitate information sharing among medical professionals.\nUpon discharge, these notes are compiled into a discharge summary, a comprehensive document that includes all the\nessential details captured during a patient’s admission, his hospital course, final diagnosis, and follow-up care [1].\nThis process, however, is cumbersome and time-intensive, requiring clinicians to invest significant amounts of time\nanalyzing and summarizing vast amounts of information, an activity which can be prone to errors or misjudgment [2].\nThis workload adds to the already high demands on healthcare providers, contributing to exhaustion and reducing\nthe time available for direct patient care [3, 4].\nFortunately, in recent years, advancements in Large Language Models (LLMs) have made them highly compet-\nitive in a myriad of Natural Language Processing tasks, including Information Extraction, Text Generation, and\nSummarization [5]. These advancements have made them competent in the problems clinicians face, making it possi-\nble to automate this process. While limited context windows previously restricted their applicability, recent models\nsuch as GPT-4 [6] and Google’s Gemini 1.5 Pro [7] improved their limits significantly, enabling the processing of\nentire sets of notes in one go.\nThis work explores the ability of five recently developed LLMs to generate a discharge summary from the set\nof notes written during a patient’s admission. We cover two main classes of LLMs: smaller yet powerful open-\nsource models: Mistral-7B [8], and Llama2-7B [9]; and more powerful but proprietary models: GPT-3.5, GPT-4,\nand Gemini 1.5 Pro. We experimented with two main techniques for improving the results, fine-tuning and one-\nshot prompting, and conducted an extensive analysis of their capabilities. Proprietary models show overall superior\n1\narXiv:2512.06812v1  [cs.CL]  7 Dec 2025\n"}, {"page": 2, "text": "performance, but the fine-tuned version of Mistral produces competitive results. Still, significant advancements are\nneeded for real-world applicability. We release the source code used to conduct the experiments1.\n2 Related Work\nSummarization is a Natural Language Processing (NLP) task that aims to distill key information from a larger body\nof text by preserving its essential content. Approaches can be extractive, selecting key sentences from the original\ntext, or abstractive, generating new sentences that convey the original meaning in a condensed format [10–13]. This\ntask can also be classified as either single-document or multi-document summarization [12], depending on the data\nsource.\nSummarization has a growing prevalence in the clinical field [14], where it has been applied to medical evidence\nfrom research articles [15], synthesizing a discharge report into a discharge summary [16], and generating brief\nhospital course sections of discharge summaries [17]. Popular models include decoder-only architectures such as\nGPT-3.5 [15], and seq2seq models like BART [2, 16, 18], T5 [2, 16], and BERT2BERT [2, 16].\nDischarge summary generation has been a topic of rising research interest [19], but it doesn’t yet have a significant\nbody of work [12]. An article by Adams et al. [17] established in 2021 the task of hospital-course summarization\nand identified important features to be considered when approaching it. In the same year, one of the first works\ngenerating discharge summaries from prior notes was published by Shing et al. [4]. More recently, the shared task\nby Xu et al. [20] extended this line of work with new developments. Some attempts to automatically generate a\ndischarge summary focused on specific parts of the report, such as the Chief Complaint [21] or the Brief Hospital\nCourse [2].\nSimplifying and summarizing the information on discharge reports is a task that has also gained traction [22, 23].\nFor example, the work of Goswami et al. [23] is a recent attempt to tackle the summarization of the entire discharge\nsummary, leveraging Llama-2 to perform the task. They attempt to summarize 700 discharge records, creating\ncondensed and easier-to-understand reports for patients. For this, they first generate reference summaries for each\nreport and train the model with different techniques, of which QLoRA stands as the most effective. While the task\nis similar to ours, it serves a different purpose and does not tackle multi-document summarization, as they leverage\nthe already compiled report instead of the collection of notes.\nA paper that works on the same objective is that of Ellershaw et al. [24], who propose a system leveraging GPT-\n4 Turbo for this task. Additionally, they also used MIMIC-III’s data to conduct the experiments and presented\nthe entire set of notes to the model at once. However, their evaluation was only qualitative and conducted on 53\nsummaries, a figure that makes it challenging to generalize the results. Moreover, their tests were only conducted on\nGPT-4 Turbo using few-shot prompting and a structured output, which complicates the assessment of how much\nof the scores are due to these restrictions over the model’s inherent capability.\nIn general, approaches revolve around two methods: Purely abstractive methods [2, 18, 22–25], where a single\nmodel processes the input and generates a discharge summary, and Extract-then-Abstract methods [2, 4], where\nkey sentences are first selected by an “extractor” model, and then summarized by an “abstractor”.\nBoth approaches show promise, although the extract-then-abstract workflow tends to have issues with sentence\nselection besides inheriting the same problems of the purely abstractive approach, such as repeating words and\nsentences and generating inconsistencies between the reported information and the true summary [2, 4]. Moreover,\nthe sentences tend to be less coherent, and the abstraction models require a deeper understanding of the language\nto interpret the disconnected sentences [2].\nOne advantage of extract-then-abstract systems is the easier handling of long text sequences, an issue common\nwith multi-document summarization [2, 4, 12]. As the extractor only selects meaningful sentences, it reduces the size\nof the document to an appropriate context length. Authors opting for the purely abstractive approach either try to\napply a model to the full sequence [18] risking not being able to process it correctly or crop irrelevant information\nfrom the source to generate the summary [25], possibly missing critical information from the clipped sources.\n3 Methodology\nOur goal is to explore a set of LLMs for discharge summary generation. We begin by giving an overview of the\nmodels we focused on and how they were adapted to our specific domain. Then, we detail the data used in our\nexperiments. Finally, we describe the metrics used to assess performance.\n1Available on https://github.com/1Krypt0/clinical-summarization-llm.\n2\n"}, {"page": 3, "text": "3.1 Model Selection\nWe evaluated two main classes of models: smaller, open-source models and larger, proprietary ones. All models are\ndecoder-only and have promising results in various NLP tasks [6, 8, 26–28].\n3.1.1 Open-Source Models\nTwo criteria were prioritized to select suitable open-source models: model size and context window. Too large models\nthat would not fit on our NVIDIA RTX 3090 (24GB of vRAM) were excluded, and we required a context window\nsufficiently large to accommodate our data, which we fixed at a minimum of 8,192 tokens.\nWe leveraged models from the Llama-2 [26] and Mistral [8] families. Llama-2 models come in 3 sizes: 7B, 13B,\nand 70B parameters. Even though they have a context window of 4,096 tokens, they can be extended to our 8,192\ntoken limit due to Rotary Position Embeddings (RoPE) [29], enabling an increase in context length with additional\ntraining. Due to the hardware restrictions mentioned above, we opted for the 7B version.\nMistral-7B is a model developed by Mistral AI that reportedly outperforms the bigger Llama-2-13B on various\ntasks at a much smaller size and faster inference speed [8]. Moreover, it natively supports 8,192 tokens, so no\nadaptation is needed.\nWe utilized the Instruction-Tuned version of both models to optimize their performance on this task. The specific\nmodels we used are available as ‘mistralai/Mistral-7B-Instruct-v0.2’2 and as ‘meta-llama/Llama-2-7b-chat-hf’3.\n3.1.2 Proprietary Models\nFor proprietary language models, we opted to include OpenAI’s GPT-3.5 [27], GPT-4 [6], and Google’s Gemini 1.5\nPro [28]. GPT-4 has been widely regarded as the state-of-the-art in various NLP tasks [6], while Gemini 1.5 Pro,\nan improvement over the previous Gemini 1.0 Pro, appears to have better performance than even the much larger\n1.0 Ultra [28], which in itself showed better performance than GPT-4 in many tasks [7]. These models tend to have\nmuch larger context lengths than the open-source ones, with GPT-3.5 having 16,384 tokens, GPT-4 increasing this\nfigure to 128,000 tokens, and Gemini 1.5 Pro to 1 million tokens, allowing for large collections of text to be processed\nsimultaneously.\n3.2 Model Adaptation\nAlthough the base models have good results in various NLP tasks, one of the standout features of open-source\nmodels is their adaptability through fine-tuning. Taking advantage of this potential, we adapted our models to the\nclinical domain.\nFor the open-source models, we employed Quantized Low-Rank Adaptation (QLoRA) [30], a resource-efficient\nvariation of LoRA [31]. QLoRA reduces memory requirements by applying a memory-efficient data type, 4-bit\nNormalFloat (NF4), and Double Quantization, enabling the training of large models using fewer resources [30].\nTo implement it, we utilized torchtune4, a PyTorch [32] library for fine-tuning and experimenting with LLMs.\nMinor modifications were made to the library’s source code to accommodate our preformatted instruction templates\nwithout additional text alterations.\nBoth models were fine-tuned using the AdamW optimizer [33] with a learning rate of 2e−5. We used a cosine\nlearning rate scheduler with 100 warmup steps and four gradient accumulation steps. Due to memory restrictions,\nwe only applied 1 item per batch.\nThe training outcomes varied between models. While Mistral’s loss showed only slight improvement (hovering\naround 0.8 after training), its starting point was already promising. Llama-2 exhibited substantial progress, with\nloss values decreasing from over 4.0 to around 1.15. Training durations were similar: Mistral required 3 days, 19\nhours, and 53 minutes, while Llama-2 took 3 days, 16 hours, and 39 minutes.\nWe could not apply QLoRA to the proprietary models because these do not have their weights publicly available.\nInstead, we opted for one-shot prompting to guide their generation, a proven technique for improving perfor-\nmance [34], as it reduces the dependency on model pre-training and guides the model to a more well-defined\nanswer.\nExcept for GPT-3.5, each proprietary model was tested in zero and one-shot scenarios. We opted not to include\nGPT-3.5 in the one-shot generation because its context window, despite expansive, is not large enough to accommo-\ndate more than a single set of notes consistently. For the one-shot scenario, we used the same notes and discharge\n2https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n3https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n4https://github.com/pytorch/torchtune\n3\n"}, {"page": 4, "text": "summary in all tests obtained from a random admission from the training data. To guide the model’s response, we\nasked it to output it in a structured format.\nAs a starting point, we devised the prompt format for the zero-shot prompt, of which a snippet is given below. For\na full version, refer to the accompanying repository5. The headers used in this formatted prompt were obtained using\nregular expressions over the entire dataset and are the most prominent ones across all summaries, encapsulating all\nthe relevant details of a discharge summary.\nYou are an expert clinical assistant. You will receive a collection of clinical notes. You will summarize them\nin the style of a discharge summary, outputting the following fields, and no additional information:\nAdmission Date: [Admission Date]\nDischarge Date: [Discharge Date]\nDate of Birth: [Date of Birth]\nSex: [Sex]\nService: [Hospital Service used]\nChief Complaint: [Chief Complaint]\n...\nFor the one-shot variant, after supplying the text above, we append the following:\nFor example, given the notes:\n### NOTES START ###\n{notes content}\n### NOTES END ###\nYou should summarize them as:\n### SUMMARY START ###\n{associated summary content}\n### SUMMARY END ###\nWe use the titles “NOTES START” and “SUMMARY START” to highlight the position of the notes to the model and\ntheir respective ends. Also, to prompt the open-source models, we used the following simplified version of the above\nprompt:\nYou are an expert clinical assistant. You will receive a collection of clinical notes. You will summarize them\nin the style of a discharge summary.\nWe passed only this sentence, which we called the system prompt, to avoid exceeding the token limits. An\noverview of all the models used, as well as their potential for adaptation, is given in Table 1.\nTable 1: Models selected for experimentation\nModel\nParameters\nContext (tokens)\nProprietary?\nTrainable?\nPrompt Types\nLlama-2\n7B\n4,0961\nNo\nYes\nSystem Prompt Only\nMistral\n7B\n8,192\nNo\nYes\nSystem Prompt Only\nGPT-3\n175B\n16,384\nYes\nNo\nZero-Shot\nGPT-4\nUnknown\n128,000\nYes\nNo\nZero-Shot, One-Shot\nGemini 1.5 Pro\nUnknown\n1 Million\nYes\nNo\nZero-Shot, One-Shot\n1 Extensible via RoPE embeddings.\n3.3 Dataset Pre-Processing and Characterization\nTo conduct our experiments, we needed a large body of clinical data that included discharge summaries. For this, we\nopted for MIMIC-III [35], a large, publicly available database of de-identified health data from over 40,000 patients\n5https://github.com/1Krypt0/clinical-summarization-llm\n4\n"}, {"page": 5, "text": "gathered from 2001 to 2012 from critical care units of the Beth Israel Deaconess Medical Center. For our work,\nthe standout feature of this database is the addition of clinical notes taken during a patient’s stay, including the\ndischarge summaries written upon discharge, that act as a ground truth in our experiments, or, as Adams et al.\n[17] suggested, as a silver truth.\nDuring our experimentation with proprietary LLMs such as GPT-3.5, GPT-4, and Gemini 1.5 Pro, we ensured\ncompliance with both the terms of the MIMIC-III Data Use Agreement 6 and applicable European ethical frame-\nworks on the use of AI and Gen AI in research, such as the European Code of Conduct for Research Integrity [36]\nand ERA Guidelines on responsible use of Gen AI in research [37]. Specifically, we selected API providers that\nassure that input data is not used for purposes (e.g., models’ training) other than the intended computation. These\npractices align with the guidelines outlined in the article “Responsible use of MIMIC data with online services like\nGPT” 7.\nAlthough we aimed to use the entire MIMIC-III dataset, we found noise in the data that we needed to filter\nbefore proceeding. In this process, shown in Figure 1, we started by removing all the notes flagged as errors by\nphysicians. Next, all the discharge summaries that included additions to the original version, stored as addendums,\nwere also removed to cut down on the larger summaries. This process left us with 1,527,197 notes from 47,006\nunique admissions.\nFig. 1: Data Filtering pipeline used to construct our dataset\nAfterward, we considered the context length of the models and ensured that the data did not surpass the smallest\ncontext window of 8,192 tokens. Since both Llama and Mistral use tokenizers based on Sentencepiece [38], a Byte-\nPair Encoding (BPE) tokenizer, we used it to filter out collections of notes related to an admission that exceeded\n7,600 tokens. This value was chosen for added assurance and to accommodate hardware restrictions since, at this\nsize, admissions could still cause Out-of-Memory errors, impeding our evaluation process. This left us with 21,339\ntotal admissions. These were then randomly split into a train and test set, with 982 admissions reserved as the test\nset and the remaining 20,357 as training data.\nWe now examine some general features of our dataset. Figure 2 shows a histogram of the word count in both\nthe collection of notes and the summary. On average, a collection of notes on the training set has 1,553 words; on\nthe test set, this number is 2,078. A summary on the training set has an average of 1,803 words, and that value\nrises to 2,081 on the test set. This indicates that the summaries tend to be larger than the collection of notes on\nwhich they were based, which might suggest that they are much more text-heavy than the notes and possibly less\ninformation-dense. In contrast, the notes rely on having a lot of information in a more condensed format. This is to\nbe expected, as the discharge summary draws upon information from all notes and possibly contains information\nnot present anywhere else.\n6https://physionet.org/content/mimiciii/view-dua/1.4/\n7https://physionet.org/news/post/gpt-responsible-use\n5\n"}, {"page": 6, "text": "Fig. 2: Histogram with word count per document\nLooking deeper at the collection of notes, we can see their main constituents in Figure 3a. Nursing, Radiology,\nand Electrocardiogram (ECG) notes comprise the bulk of our collection. Other categories of notes are much less\nprevalent, making them less relevant to the overall generation process. Figure 3b shows a boxplot with the number\nof notes per admission.\nIf we investigate which documents contain more words or sentences, the trend reverses, with Figure 3c showing\nthat the most common documents do not have the most words overall. In fact, the ECG notes contain the least\namount of information per note, at just 36 words on average. Nursing and Radiology notes also reverse their trend,\nand physician notes, much rarer in both datasets, present the most information overall. The amount of sentences,\nshown in Figure 3d, also corroborates this trend, although nursing notes begin to become more prominent in this\nmatter. The data indicates a trend for many short sentences in these types of notes, while, for example, a Physician’s\nnote seems like a more detailed report, with more sentences and words overall.\n3.4 Experiments Evaluation\nTo assess the performance of each model, we rely on two types of metrics: quantitative metrics from the summa-\nrization domain, which will provide a numerical and comparable measure of performance, and a set of qualitative\nmetrics that will offer insights into the relevance, correctness, and reliability of the produced summaries.\n3.4.1 Quantitative Analysis\nTo evaluate the quality of the generated summaries, we utilized several well-known metrics. For exact matching,\nwe report BLEU [39], a precision-oriented metric that measures the overlap between a generated summary and the\nreference text, and ROUGE [40], a recall-oriented metric also assessing overlap similarity. We report ROUGE-1 and\nROUGE-2, focusing on unigrams and bigrams, and ROUGE-L using the Longest Common Subsequence (LCS) [40].\nSince these metrics are limited to exact match scoring, synonyms or slightly different sentences will be disre-\ngarded [11]. To address this, we leveraged BERTScore [41], which uses BERT embeddings to evaluate semantic\nsimilarity, and BLEURT [42], which operates similarly but with a model pre-trained on detecting similar sentence\nstructures.\nTo analyze sentence repetitions, we used the Type-to-Token Ratio (TTR), defined by\nNunique words\nNwords\n, which\nmeasures vocabulary diversity.\n6\n"}, {"page": 7, "text": "(a) Percentage of notes per category on our dataset\n(b) Distribution of notes per Admission on our dataset\n(c) Average amount of words per type of note on our\ndataset\n(d) Average amount of sentences per type of note on our\ndataset\nWe also included reference-free metrics, which compare summaries directly with source documents without\nrelying on a ground truth. Specifically, we used BLANC [43], which measures how well a pre-trained model improves\nits understanding of source documents after training on the generated summary. This metric correlates with human\njudgment while bypassing the need for a reference summary.\nFor factual consistency, we employed SummaC Conv [44], a metric that uses a pre-trained Natural Language\nInference (NLI) model and a Convolutional Neural Network (CNN) to detect inconsistencies by comparing sentences\nfrom the source document and summary.\nMetrics were calculated considering the entire document as a single answer. For proprietary models, a structure to\nthe output was specified, so we also evaluated results by individual sections to evaluate the adherence to the format\nand identify challenging sections. Such a restriction was not applied to the open-source models due to input size\nconstraints, so a per-section evaluation was not conducted on them. All metrics range from 0 to 1, while BLEURT,\naccording to the implementation description [45], mostly varies from -2 to 1, with some outlier values being possible.\n7\n"}, {"page": 8, "text": "3.4.2 Qualitative Analysis\nTo complement the quantitative analysis, we also performed a qualitative evaluation of the results with the help of\na clinical expert who read the summaries and evaluated them against three criteria: Completeness, by determining\nwhether the summary captured all relevant information; Correctness, by verifying the factual accuracy of the\ninformation presented; and Conciseness, by evaluating whether the summary included any unnecessary information.\nThe expert evaluated summaries from three randomly selected admissions. The answers to the questions were\ngiven using a 5-point Likert scale, where 1 indicated a “very bad” result and 5 a “very good” one.\nAdditionally, we asked the expert to identify which information was detected as incorrect to possibly determine a\npattern of hallucinations in the answers. Moreover, we asked for additional comments the expert deemed necessary.\nGiven its small scale, this evaluation should be interpreted as preliminary, and it primarily serves as an initial\nindication to guide future research directions.\n4 Results and Discussion\nThis section examines the main findings of our experiments.\n4.1 Similarity Between Produced and Original Summaries\nWe examine and discuss the performance of all models when evaluating against the gold-standard summaries.\nTable 2 presents the scores obtained for each model.\nTable 2: Scores obtained by each model on the metrics described in Subsection 3.4.1. Best scores are in bold\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU\nTTR1\nBERTScore\nBLEURT\nBLANC\nSummaC\nLlama - B2\n14.16%\n2.72%\n8.22%\n1.30%*\n21.96%\n77.83%\n-1.085\n6.95%\n53.14%*\nLlama - F.T3\n13.99%\n2.68%\n8.05%\n1.23%\n22.81%\n77.89%\n-1.084*\n6.67%\n52.87%\nMistral - B\n21.55%\n6.14%\n10.00%\n0.20%\n47.93%*\n79.52%\n-1.085\n14.11%*\n51.89%\nMistral - F.T\n22.29%*\n6.34%*\n10.19%*\n0.19%\n47.46%\n79.66%*\n-1.094\n13.92%\n50.24%\nGPT-3 - Zero4\n19.65%\n7.07%\n10.63%\n0.04%\n51.63%\n81.87%\n-0.440\n8.87%\n50.71%\nGPT-4 - Zero\n20.97%\n7.17%\n10.89%\n0.11%\n51.77%**\n81.75%\n-0.538\n9.12%\n51.24%\nGemini - Zero\n23.33%\n9.21%\n12.81%\n0.32%\n46.98%\n82.58%\n-0.469\n11.69%**\n64.94%**\nGPT-4 - One4\n22.30%\n7.42%\n11.40%\n0.33%\n49.85%\n82.53%\n-0.484\n9.90%\n53.0%\nGemini - One\n30.90%**\n10.97%**\n15.36%**\n3.07%**\n47.14%\n85.09%**\n0.0847**\n10.41%\n57.07%\nA single asterisk (*) denotes the best score on an open-source model, whilst a double asterisk (**) denotes the best score on a proprietary model.\n1 TTR = Type to Token Ratio\n2 B = Baseline\n3 F.T = Fine-Tuned\n4 Zero = Zero-Shot\n5 One = One-Shot\n4.1.1 Open-Source Models\nWe begin by analyzing open-source models, starting with the baseline versions and then moving to the fine-tuned\nones, assessing performance improvements.\nA first and very noticeable observation, applicable to all models, is that the exact-match scores are low, with\nthe highest barely surpassing 30%. This is, however, common in similar studies [18, 46] as large, multi-document\nsummarization remains a challenge even for state-of-the-art models [12].\nMistral outperforms Llama on most metrics, only trailing behind in BLEU and SummaC and achieving the\nsame score on BLEURT. Despite underperforming on several other metrics, Llama’s baseline model achieved the\nbest BLEU score among the open-source models. However, Llama’s vocabulary appears less diverse, as evidenced\nby the poor TTR score. This is a possible indicative of repeated sentences, a problem common in abstractive\nsummarization [2]. Overall, Mistral takes a significant advantage over Llama, as its results are similar even to those\nof state-of-the-art proprietary models.\nSummaC results indicate Llama is marginally more factually consistent, with differences of 1.25% (baseline)\nand 2.63% (fine-tuned) over Mistral. However, as SummaC evaluates entailment rather than strict accuracy, its\neffectiveness diminishes for longer documents [11], marking the need for deeper qualitative analysis.\nMistral showed minor improvement post-fine-tuning, while Llama’s performance declined on most metrics. The\noverall impact of fine-tuning was negligible; Mistral-7B saw only a 0.74% gain in ROUGE-1 and a slight 0.01% drop\n8\n"}, {"page": 9, "text": "in BLEU. For Llama, ROUGE-1 dropped by 0.17%, and BLEU by 0.07%. Changes to the other metrics were also\nnegligible and did not contribute to any significant alteration in performance.\nFine-tuning did not yield significant improvements, likely due to two key factors. First, the 4,096-token limit\non generation constrained the output, potentially reducing information richness and structure. However, with an\naverage discharge summary length of 2,081 words, this token limit should accommodate most cases. Second, the lack\nof a structured output prompt, which aimed to reduce input size due to hardware limitations, may have hindered\nthe models’ ability to generate comprehensive outputs.\nOverall, fine-tuning provided little to no performance gains, emphasizing the need to address input and output\nconstraints for better results.\n4.1.2 Proprietary Models\nMoving now to the proprietary models, we start our analysis with the results of zero-shot generation, and then we\ncheck if moving to a one-shot scenario aids the models.\nTable 2 reveals a trend in model performance. GPT-3.5, with 175B parameters [27], performs slightly worse\nthan the baseline Mistral model, which has only 7B parameters. GPT-4 outperforms GPT-3.5 but is on par with\nfine-tuned Mistral, with notable differences in TTR and BLEURT.\nGemini, however, emerges as the top performer in zero-shot, establishing high scores in most categories, par-\nticularly in exact-match metrics. Surprisingly, no model surpasses the BLEU score of the baseline Llama, which is\nmore than four times higher than Gemini’s.\nDespite improvements, BLANC scores remain largely unchanged for the heavier models. Mistral remains the\nmost consistent here, with proprietary models lagging behind open-source alternatives. In SummaC, only Gemini\nshows significant improvement in zero-shot, correlating with its strong BLANC performance. Both metrics show\nsimilar trends across models, improving or decreasing together.\nWhen moving to one-shot results, performance notably improves. GPT-4 surpasses previous scores except for\nTTR, marginally outperforming fine-tuned Mistral. Gemini, however, shows a substantial performance jump, achiev-\ning the best overall score, topping six of the eight benchmarks, and nearly tripling Llama’s BLEU score. It’s also the\nonly model with a positive BLEURT score, making it the top performer despite having subpar results in most areas.\nThe subpar exact-match performance can possibly be attributed to the structured format in which they were\nforced to output. While this may prompt models to include overlooked information, the order of sections may not\nalign with most summaries, affecting exact-match scores. However, relevant information might still be present, as\nsupported by BERTScore, which will be further explored in Section 4.4.\n4.2 Per-Section Similarity Between Produced and Original Summaries\nWe now look deeper into the proprietary models and consider each section separately. Since each proprietary model\nwas asked to output in a structured format, it is reasonable to assume the answer was generated with this structure,\na hypothesis we corroborated when analyzing the results with regular expressions. If a section was missing in the\noriginal or generated summary, an empty string was used for comparison. For BLANC and SummaC, we used the\nentire source document as a reference to evaluate the impact of individual sections on model performance.\nTable 3 presents the analysis, showing the best scores per section in bold. Gemini, using one-shot prompting,\nemerged as the top performer with 73 best results (including ties), followed by GPT-3 with 53 best results. The\nother models showed more consistent but lower performance. Notably, while Gemini excelled in exact-match metrics\n(ROUGE, BLEU), GPT-3 performed better in TTR, BERTScore, BLANC, and SummaC. Gemini’s strong exact-\nmatch scores suggest it closely mimics discharge summary language, whereas GPT-3 scores better in vocabulary\ndiversity.\nIn soft-overlap metrics, BLEURT and BERTScore, both models were competitive, but BERTScore favored GPT-\n3, while BLEURT gave a slight edge to Gemini. BLANC corroborated this trend, with GPT-3 scoring highest in\nreference-free evaluations, suggesting it enhances understanding of the original text. However, many BLANC scores\nwere negative, indicating some sections degraded understanding. SummaC also favored GPT-3, with 11 of the 18\nbest scores, indicating its consistency across sections.\nIn comparison with the “Discharge Me!” task from Xu et al. [20], which focused on the “Brief Hospital Course”\nand “Discharge Instructions” sections, we computed the average scores between the relevant sections for our models.\nThe top performer remains Gemini with One-Shot prompts, which showed subpar results on the exact-match\nmetrics, with 10.55% ROUGE-1, 2.41% ROUGE-2, 6.83% ROUGE-L, and 2.025% BLEU. Such scores would place\nit among the last positions on the leaderboard. However, our BERTScore was better than the top performer of the\n9\n"}, {"page": 10, "text": "Table 3: Scores (ROUGE-{1/2/L}/BLEU/TTR/BertScore/BLEURT/BLANC/SummaC) obtained by proprietary models under each section from the zero-\nshot prompt. Best scores per section and metric are in bold\nSection\nGPT-3 - Zero-Shot\nGPT-4 - Zero-Shot\nGemini - Zero-Shot\nGPT-4 - One-Shot\nGemini - One-Shot\nService\n7.42%,\n0.42%,\n7.35%,\n0.18%,\n99.45%, 80.10%, -0.470, -0.27%,\n56.87%\n15.55%, 1.26%, 15.49%, 0.10%,\n98.05%, 80.10%, -0.295, -0.31%,\n58.99%\n11.04%,\n0.51%,\n10.95%,\n0%,\n95.79%, 78.87%, -0.510, -0.05%,\n50.85%\n28.29%,\n0.66%,\n28.11%,\n0%,\n99.71%, 83.11%, -0.026, -0.28%,\n46.07%\n43.37%,\n0.10%,\n43.38%,\n0%,\n99.60%, 87.93%, 0.250, -0.32%,\n38.24%\nChief Complaint\n12.75%,\n5.54%,\n12.24%,\n1.08%,\n96.33%, 60.57%, -1.021, 0.25%,\n70.69%\n14.71%, 6.53%, 14.16%, 1.01%,\n96.33%, 60.73%, -0.951, 0.05%,\n70.57%\n14.12%,\n6.68%,\n13.69%,\n1.08%,\n90.47%, 54.75%, -0.920, -0.05%,\n69.94%\n14.33%,\n6.54%,\n13.87%,\n0.93%,\n96.02%, 60.79%, -0.966, -0.04%,\n70.16%\n14.57%, 7.74%, 14.40%, 1.96%,\n94.10%, 59.72%, -0.788, -0.25%,\n61.89%\nMajor Surgical or Inva-\nsive Procedure\n20.77%,\n2.90%,\n20.01%,\n1.10%,\n96.72%, 61.25%, -0.896, -0.04%,\n60.65%\n20.73%, 4.27%, 19.53%, 2.53%,\n94.75%, 60.45%, -0.970, 0.03%,\n63.13%\n13.26%,\n3.27%,\n12.71%,\n2.02%,\n88.83%, 53.84%, -0.978, 0.03%,\n59.75%\n23.23%, 4.39%, 22.41%, 1.83%,\n95.28%, 61.48%, -0.878, -0.07%,\n60.06%\n25.02%, 3.49%, 24.60%, 0.90%,\n96.62%, 61.64%, -0.705, -0.19%,\n52.09%\nHistory\nof\nPresent\nIll-\nness\n12.52%,\n3.08%,\n8.83%,\n0.41%,\n81.78%, 58.93%, -1.028, 3.03%,\n52.61%\n10.93%,\n2.88%,\n7.98%,\n0.11%,\n83.82%, 56.72%, -1.060, 1.04%,\n45.10%\n13.52%, 4.09%, 9.50%, 1.27%,\n74.27%,\n53.84%,\n-1.172,\n1.94%,\n49.60%\n9.22%,\n2.58%,\n6.98%,\n0.06%,\n69.02%,\n47.14%,\n-1.085,\n1.06%,\n41.53%\n9.86%,\n2.44%,\n6.40%,\n1.45%,\n42.74%,\n36.30%,\n-1.098,\n1.03%,\n35.40%\nPast Medical History\n6.27%,\n1.96%.\n5.52%,\n0.02%,\n92.23%,\n57.84%,\n-1.312,\n-\n0.04%, 60.12%\n6.43%,\n1.98%,\n5.47%,\n0.08%,\n86.91%, 55.33%, -1.248, -0.24%,\n53.01%\n6.94%, 2.28%, 6.09%, 0.25%,\n84.26%, 53.72%, -1.294, -0.10%,\n55.09%\n5.19%,\n1.70%,\n4.38%,\n0.05%,\n71.91%, 46.43%, -1.222, -0.15%,\n46.98%\n3.84%,\n1.07%,\n3.27%,\n0.08%,\n39.99%, 26.25%, -1.118, -0.24%,\n33.86%\nAllergies\n2.63%,\n0.57%,\n2.48%,\n0%,\n98.94%,\n58.13%,\n-0.961,\n-\n0.23%, 75.86%\n12.56%,\n5.73%,\n12.37%,\n0%,\n97.56%, 58.94%, -1.132, -0.41%,\n65.96%\n3.56%,\n0.89%,\n3.39%,\n0.64%,\n91.64%, 51.74%, -1.049, -0.36%,\n55.53%\n9.60%,\n5.11%,\n9.47%,\n0.82%,\n97.62%, 58.57%, -1.102, -0.42%,\n61.74%\n12.27%, 7.02%, 11.91%, 5.27%,\n97.76%, 58.46%, -1.144, -0.55%,\n48.23%\nMedications\non\nAdmis-\nsion\n1.32%,\n0.06%,\n1.15%,\n0%,\n92.28%, 54.15%, -1.500, -0.03%,\n68.49%\n1.73%,\n0.10%,\n1.51%,\n0%,\n92.25%, 54.07%, -1.488, -0.14%,\n63.08%\n1.18%,\n0.06%,\n1.00%,\n0%,\n89.66%, 48.53%, -1.407, -0.38%,\n54.67%\n1.50%,\n0.07%,\n1.28%,\n0%,\n93.53%, 54.28%, -1.459, -0.13%,\n62.45%\n4.94%, 0.78%, 4.05%, 0.30%,\n62.06%,\n43.00%,\n-1.21,\n-0.44%,\n38.26%\nFamily History\n0.45%,\n0.05%,\n0.45%,\n0%,\n99.31%,\n57.79%,\n-0.895,\n-\n0.17%, 77.54%\n1.47%, 0.29%, 1.39, 0%, 98.16%,\n56.97%, -1.076, -0.14%, 66.37%\n1.13%,\n0.30%,\n1.06%,\n0%,\n91.48%, 50.98%, -0.984, -0.30%,\n55.83%\n0.81%,\n0.06%,\n0.77%,\n0%,\n97.22%, 56.75%, -1.006, -0.24%,\n66.34%\n4.62%, 0.96%, 3.99%, 0.53%,\n86.66%, 54.47%, -1.204, -0.26%,\n48.64%\nSocial History\n3.06%,\n0.51%,\n2.74%,\n0.05%,\n96.72%, 57.53%, -1.367, 0.12%,\n65.21%\n3.53%,\n0.63%,\n3.09%,\n0.09%,\n96.69%, 57.31%, -1.400, -0.04%,\n60.66%\n3.85%,\n0.87%,\n3.35%,\n0.31%,\n88.54%, 52.06%, -1.395, -0.10%,\n53.08%\n2.85%,\n0.53%,\n2.44%,\n0.07%,\n97.11%, 57.42%, -1.380, -0.04%,\n62.60%\n8.99%, 1.75%, 7.24%, 1.42%,\n91.17%, 58.13%, -1.262, -0.03%,\n47.76%\nPhysical Exam\n3.61%,\n0.26%,\n2.77%,\n0.08%,\n84.53%, 57.10%, -1.294, 1.68%,\n54.37%\n3.59%,\n0.31%,\n2.68%,\n0.09%,\n86.47%, 56.65%, -1.371, 0.64%,\n52.53%\n3.51%,\n0.32%,\n2.35%,\n0.26%,\n78.29%,\n50.88%,\n-1.480,\n1.10%,\n67.42%\n3.93%,\n0.31%,\n2.77%,\n0.19%,\n83.59%,\n56.72%,\n-1.398,\n1.05%,\n55.02%\n8.33%, 1.36%, 5.85%, 2.19%,\n75.84%,\n56.82%,\n-1.308,\n0.7%,\n66.33%\nPertinent Results\n2.31%,\n0.77%,\n1.89%,\n0%,\n88.16%, 54.47%, -1.433, 1.99%,\n46.20%\n3.57%,\n1.20%,\n2.59%,\n0.14%,\n81.83%,\n54.28%,\n-1.415,\n3.86%,\n47.99%\n5.10%,\n2.50%,\n4.11%,\n2.19%,\n68.24%, 49.71%, -1.410, 5.66%,\n65.51%\n3.98%,\n1.31%,\n2.86%,\n0.34%,\n78.85%,\n54.50%,\n-1.409,\n4.72%,\n47.75%\n7.79%, 2.74%, 5.57%, 3.17%,\n74.46%, 54.53%, -1.329, 3.96%,\n67.96%\nBrief Hospital Course\n8.39%,\n1.15%,\n5.86%,\n0.03%,\n82.32%, 58.64%, -1.023, 1.42%,\n51.84%\n8.49%,\n1.06%,\n5.69%,\n0.04%,\n82.68%, 57.99%, -1.074, 1.08%,\n47.97%\n11.49%.\n2.84%,\n7.59%,\n0.31%,\n71.52%,\n53.91%,\n-1.154,\n1.70%,\n46.72%\n10.07%,\n1.29%,\n6.39%,\n0.16%,\n77.87%,\n57.87%,\n-1.033,\n1.77%,\n48.74%\n14.57%, 3.45%, 9.10%, 1.87%,\n69.13%, 58.69%, -1.084, 1.90%,\n43.83%\nDischarge Diagnosis\n10.11%,\n3.38%,\n9.11%,\n0.65%,\n93.11%, 59.48%, -1.124, 1.08%,\n57.62%\n12.78%,\n4.75%,\n11.32%,\n0.98%,\n90.52%, 59.42%, -1.110, -0.45%,\n56.10%\n9.16%,\n4.05%,\n8.50%,\n0.87%,\n87.75%, 53.07%, -1.204, -0.27%,\n63.49%\n13.02%,\n4.85%,\n11.47%,\n1.13%,\n86.17%, 58.98%, -1.079, 0.34%,\n49.97%\n13.52%,\n5.42%,\n12.19%,\n1.86%, 88.78%, 59.24%, -1.094,\n-0.43%, 69.47%\nDischarge Medications\n0.75%,\n0.23%,\n0.73%,\n0%,\n96.52%, 52.60%, -1.428, -0.11%,\n70.86%\n1.00%,\n0.12%,\n0.89%,\n0%,\n92.14%, 52.81%, -1.529, -0.18%,\n60.77%\n0.56%,\n0.08%,\n0.49%,\n0%,\n90.65%,\n47.54%,\n-1.344,\n-0.33,\n54.39%\n0.81%,\n0.11%,\n0.71%,\n0%,\n92.43%, 52.63%, -1.474, -0.08%,\n58.67%\n3.59%,\n0.84%,\n2.83%,\n0%,\n47.73%, 34.15%, -1.028, -0.39%,\n32.51%\nDischarge Disposition\n8.06%,\n0.04%,\n8.09%,\n0%,\n98.97%, 60.04%, -1.385, -0.18%,\n66.10%\n7.45%,\n0.68%,\n7.45%,\n0%,\n97.88%,\n59.30%,\n-1.195,\n-\n0.42%%, 62.86%\n8.63%,\n0.65%,\n8.62%,\n0%,\n91.14%, 53.99%, -0.835, -0.38%,\n49.68%\n10.72%,\n0.74%,\n10.67%,\n0%,\n97.03%, 59.77%, -1.110, -0.13%,\n52.92%\n20.71%,\n4.80%,\n20.72%,\n0%,\n71.07%, 47.56%, -0.389, -0.13%,\n45.94%\nDischarge Instructions\n3.82%,\n0.76%,\n3.12%,\n0.01%,\n94.90%, 57.38%, -1.278, 0.15%,\n58.71%\n3.99%,\n0.44%,\n3.13%,\n0.08%,\n92.07%, 56.82%, -1.210, -0.22%,\n61.58%\n1.12%,\n0.06%,\n0.99%,\n0%,\n91.14%, 50.64%, -1.323, -0.36%,\n54.78%\n5.63%,\n0.74%,\n3.97%,\n0.39%,\n85.71%, 57.22%, -1.191, -0.05%,\n55.66%\n6.53%, 1.37%, 4.56%, 2.18%,\n81.03%, 55.79%, -1.295, 0.19%,\n43.68%\nDischarge Condition\n10.12%,\n0.08%,\n10.13%,\n0%,\n99.07%,\n59.79%,\n-0.868,\n-\n0.43%, 34.36%\n4.39%,\n0.12%,\n4.06%,\n0%,\n96.24%, 58.98%, -1.440, 0.12%,\n62.69%\n6.24%, 0%, 6.17%, 0%, 91.05%,\n52.80%, -0.956, -0.48%, 43.47%\n4.67%,\n0.08%,\n4.40%,\n0%,\n94.11%, 58.91%, -1.449, -0.01%,\n62.05%\n9.77%,\n0.07%,\n9.75%,\n0%,\n85.50%, 52.19%, -0.908, -0.52%,\n38.53%\nFollow-up Instructions\n4.06%,\n1.20%,\n3.70%,\n0%,\n97.76%, 55.58%, -1.409, 0.18%,\n54.36%\n4.62%,\n1.56%,\n4.07%,\n0.05%,\n93.07%,\n55.44%,\n-1.392,\n0.10%,\n50.09%\n1.42%,\n0.49%,\n1.30%,\n0%,\n90.79%, 49.12%, -1.396, -0.23%,\n54.36%\n4.79%,\n1.44%,\n4.17%,\n0.06%,\n91.47%,\n55.33%,\n-1.390,\n0.12%,\n47.91%\n3.98%,\n1.45%,\n3.29%,\n0.40%,\n41.92%, 29.91%, -1.072, -0.06%,\n30.85%\n10\n"}, {"page": 11, "text": "challenge, with an improvement of 13.44%. This indicates that Gemini is providing correct output, with a meaning\nsimilar to the ground-truth, but will tend to use starkly different wording compared to what is expected.\nConsidering section complexity, sections like “Service,” “Major Surgical or Invasive Procedure”, and “Discharge\nDisposition” yielded good scores, while categories like “Discharge Medications”, “Follow-up Instructions”, and “Past\nMedical History” were more challenging. This aligns with expectations, as these sections often include information\nomitted during the hospital stay, with discharge details typically written at the time of discharge rather than during\nthe hospital course.\n4.3 Similarity Between Produced Summaries and Clinical Notes\nThe lower scores of Tables 2 and 3 prompted us to examine the degree of overlap between our produced summaries\nand the original clinical notes instead of comparing them against the gold-standard discharge summaries. The\nresults are presented in Table 4. Since we measured only the overlap, we omitted BLANC, SummaC, and TTR, all\nreference-free metrics, from this evaluation.\nTable 4: Overlap metrics of each model against the original notes. Best scores are in bold\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nBLEU\nBERTScore\nBLEURT\nLlama - B\n19.26%\n6.29%\n11.86%\n1.88%\n78.41%\n-0.904\nLlama - F.T\n19.77%\n6.62%\n12.09%\n1.99%\n78.44%\n-0.900\nMistral - B\n31.19%\n15.87%\n19.02%\n0.1%\n80.12%\n-0.927\nMistral - F.T\n31.13%\n15.49%\n18.57%\n0.09%\n80.14%\n-0.937\nGPT-3 - Zero\n20.36%\n8.07%\n10.60%\n0%\n79.89%\n-0.776\nGPT-4 - Zero\n21.55%\n7.80%\n10.49%\n0.02%\n79.95%\n-0.829\nGemini - Zero\n24.87%\n13.41%\n14.59%\n0.09%\n80.82%\n-0.824\nGPT-4 - One\n23.68%\n8.67%\n11.35%\n0.06%\n80.92%\n-0.837\nGemini - One\n26.31%\n11.99%\n13.93%\n1.38%\n81.78%\n-0.808\nOverlap is more prominent in the open-source models, with Mistral having the highest scores overall and con-\nsiderable results in the exact-match metrics. BLEU also is comparatively high for Llama, with Mistral falling short\nin this score. This trend in performance leads us to believe that, despite the fine-tuning process not bringing any\nsignificant advantage to generate a discharge summary, it led to them modeling the text better and becoming more\nefficient at transcribing features from the text. Despite these improvements in extracting information from the text,\ntheir performance was worse when reasoning over the information and synthesizing it correctly into a discharge\nsummary, as evidenced by the contrast in scores.\nSurprisingly, the proprietary models show little improvement compared to the open-source ones. In the exact-\nmatch metrics, almost all results from the GPT models are only slightly above Llama’s. One hypothesis that might\nexplain this is that, because of their size and reasoning ability, these models could distill the data from the notes\ninto a discharge summary, inferring from it and giving results akin to an actual document. The structured output\ncould have also helped in this process by guiding the output and forcing the models to adapt to a defined structure.\nConsidering the soft-overlap metrics of BERTScore and BLEURT, the evaluations show a stark contrast.\nAlthough BERTScore remained stable for all models, BLEURT gives lower scores in the proprietary models. In\nTable 2, an improvement is seen in BLEURT when moving to the proprietary variants, with Gemini even reaching\na positive result. However, in Table 4, the scores don’t vary substantially and are consistent throughout all models.\nOverall, the results indicate that the open-source models tend to better transcribe information directly from the\nsource text into the discharge summary, whilst the proprietary models tend to generate sentences that convey the\nsame meaning as the original notes but use different terms.\n4.4 Completeness, Correctness, and Conciseness of the Produced Summaries\nWe now look at the results of the qualitative evaluation conducted by our medical expert. We used a 5-point Likert\nscale, where the worst score was a 1, and the best possible score was a 5. Table 5 presents the scores of each criterion\nfor each document.\nAcross three samples, GPT-4 with one-shot prompting consistently achieved the highest scores across all cat-\negories despite overall subpar results. Both Llama versions and baseline Mistral performed poorly, often scoring\n11\n"}, {"page": 12, "text": "Table 5: Results of the qualitative evaluation per document. Best\nscores are in bold\nModel\nCompleteness\nCorrectness\nConciseness\nLlama - Baseline\n{1, 1, 1}\n{1, 1, 1}\n{1, 1, 1}\nLlama - Finetuned\n{1, 1, 1}\n{1, 1, 1}\n{1, 1, 1}\nMistral - Baseline\n{2, 1, 1}\n{3, 1, 1}\n{2, 1, 1}\nMistral - Finetuned\n{2, 2, 2}\n{3, 2, 2}\n{2, 2, 2}\nGPT3 - Zero-Shot\n{2, 2, 2}\n{2, 1, 2}\n{2, 2, 2}\nGPT4 - Zero-Shot\n{2, 2, 2}\n{2, 2, 2}\n{2, 2, 2}\nGemini - Zero-Shot\n{2, 2, 2}\n{2, 2, 1}\n{2, 2, 2}\nGPT-4 - One-Shot\n{3, 3, 3}\n{3, 3, 3}\n{3, 3, 3}\nGemini - One-Shot\n{3, 2, 3}\n{2, 2, 2}\n{3, 2, 3}\nat the lowest end of the scale. Fine-tuned Mistral showed improvement, achieving scores comparable to zero-shot\napproaches, but one-shot strategies remained the best performers.\nOne major issue noted by the clinician was the lack of completeness in summaries. No model reliably included\nall information from the gold-standard summaries. Looking deeper into this issue, we see that it is likely a problem\nwith the data itself, as some necessary information is missing from the notes. This notion, tied with the results from\nTable 3, reinforces our belief that information such as what to do upon discharge and any follow-up necessary is\nonly determined when the summary is written and not commonly present in the remaining notes. This statement is\nfurther reinforced by the work of Adams et al. [17], which mentions that clinicians receive little formal instruction\nin documenting patient information and are pressed for time when doing so. Nevertheless, based on the expert’s\nnotes, GPT-4 and Gemini, with one-shot prompting, did a reasonable job of capturing all available information,\neven though some critical details might be absent.\nCorrectness was another frequent issue. Llama often produced repetitive, incoherent, or irrelevant outputs despite\nreasonable BLEU and BERTScore metrics, likely due to its use of medical terminology in the produced sentences.\nMistral showed a slight improvement but missed key details, though it reliably extracted relevant information from\nnotes. However, it sometimes deviated from standard summary formats, presenting outputs as letters to physicians.\nConciseness appears to follow the trend of completeness. Proprietary models, particularly in one-shot scenarios,\nshowed substantial improvements in correctness. However, they occasionally included hallucinated details not found\nin the notes, such as a recommendation for a patient to follow a “low-residue diet” when no such information was\ngiven. Gemini was more prone to such hallucinations, possibly due to data leaking from the example summary into\nthe response, leading to incoherence.\nOverall, this analysis was consistent with our previous results, giving an advantage to the proprietary models\nthat use a one-shot approach. Although our quantitative metrics gave the edge to Gemini, meaning that it is the\nmost similar to the actual summaries, this analysis preferred GPT-4 by some margin, making them the two most\nviable and most promising approaches to clinicians.\n4.5 Inference Speed Analysis\nQuick generation is a desirable and important factor when dealing with multiple requests. Therefore, we also\nevaluated the models’ inference speed, relying on the methodology from Deci AI [47]. In Table 6, we report the\nresults after five warmup steps, using 100 repetitions of an example chosen randomly. For the proprietary models,\nas these rely on APIs, such an analysis is impossible.\nTable 6: Inference times for the open-source\nmodels\nModel\ntinference\nσinference\nLlama-Baseline\n84,052,84ms\n60,369.35ms\nLlama-Finetuned\n71,694.74ms\n59,991.02ms\nMistral-Baseline\n18,389.22ms\n24.07ms\nMistral-Finetuned\n17,732.36ms\n33.46ms\n12\n"}, {"page": 13, "text": "There is a clear advantage in Mistral, which presented the best scores by far, with our fine-tuned version being\nfour times faster on average than Llama. Surprisingly, besides being very slow in its generation, Llama also presents a\nconsiderably higher degree of variability in the scores. A probable answer to this is the use of RoPE embeddings [29],\nwhich possibly slows down the computations where the length exceeds the model’s predefined context window of\n4,096 tokens.\n5 Considerations for Real-World Deployment\nConsidering the results from the previous sections, we can consider the two most viable approaches, which we deem\nto be Gemini 1.5 Pro and GPT-4, using a one-shot prompt approach. However, there are some caveats to these\noptions that we can further explore.\nAt the time of writing, the prices each model charges are available in Table 7. Gemini 1.5 Pro costs 1.5$ per\nmillion tokens of input and 5$ per million tokens of output, considering that the total amount of tokens does not\nexceed 128,000 tokens. The version of GPT-4 that we tested on, GPT-4 Turbo, is even more expensive, at 10$ per\nmillion tokens on input and 30$ on output. Besides, both options are rate-limited, with Gemini maxing out at 1,000\nrequests per minute and 4 million tokens per minute. GPT-4, despite allowing 10,000 requests per minute, limits\nthe input to 2 million tokens per minute in its highest tier. These figures would have to be considered carefully in\nany real-world deployment.\nTable 7: Pricing and Rate Limits of proprietary models\nModel\nPrice / 1M tokens\nT.P.M\nR.P.M\nR.P.D\nGPT-3\n0.5$ / 1.5$\n50M\n10,000\nNone\nGPT-4\n10$ / 30$\n2M\n10,000\nNone\nGemini 1.5 Pro\n1.5$ / 5$\n4M\n1000\nNone\nA possible alternative would be to use a self-hosted model like the fine-tuned Mistral, which offers cost benefits\nby eliminating generation fees but with the tradeoff of reduced performance, more frequent hallucinations, and\nmissing information compared to the proprietary systems. Llama 2, in the tested size, is not recommended due to\nits poor performance and inefficiency. Another issue with self-hosting is that it requires significant infrastructure to\nhandle long inputs and scale for hospital-wide use, creating significant maintenance costs and speed tradeoffs. For\ninstance, Mistral took over eight hours to generate 982 summaries, while Llama was over four times slower, making\nboth impractical for clinical use without substantial resources. Additionally, rapid innovation in AI means models\ncan become obsolete within a year, limiting long-term viability.\nIt is also important to consider privacy and data handling in closed-source models. Clinical notes are classified as\nsensitive information, protected under the General Data Protection Regulation (GDPR) [48]. Unless these models\nensure GDPR compliance, they cannot be used for automatic generation.\nNonetheless, although it is possible to use both alternatives in a real-world setting, this problem still needs\nsignificant research efforts and advancements to be considered feasible for adoption. Currently, a hybrid approach\nwhere AI generates a draft for physicians to review and edit appears as a more reliable option, reducing clinician\nworkload while improving care quality without compromising accuracy.\nOur study has limitations, which we will now highlight. Hardware was a significant hindrance, as it restricted\nthis study regarding the data and model selection. Due to this, many data points were discarded from MIMIC,\nand model selection and training were also impacted. Additionally, only MIMIC-III data was used, which may\nnot generalize to other hospitals with different summarization styles. In this topic, we also did not consider the\ninherently subjective nature of importance. What is critical in one care unit may not be relevant in another, and\ndifferent professionals have unique summarisation styles, which can vary significantly between them.\nOur human analysis was conducted by a medical expert, using just three samples from each model, limiting\nthe generalizability of the results. Additionally, what might be relevant for one clinician can be negligible for\nanother, so having the evaluation done by more people would be an advantage, as the scores would be more\nreliable. Furthermore, language barriers and unfamiliarity with American hospital abbreviations may have negatively\nimpacted the assessment, as our clinical expert is Portuguese and works within Portugal’s health service.\nFinally, it is also important to note that we did not thoroughly engineer our hyperparameters when fine-tuning\nour models, nor did we thoroughly evaluate settings such as temperature in the proprietary models, which can lead\nto changes in the generated responses.\n13\n"}, {"page": 14, "text": "6 Conclusions and Future Work\nThis work addressed the challenge of generating discharge summaries from prior clinical notes, aiming to improve\nthe efficiency of patient documentation for clinicians. We evaluated five models, ranging from small open-source\nversions to large proprietary systems, using a comprehensive suite of metrics alongside analyses of document overlap\nand inference speed. While proprietary models showed superior performance, certain open-source options, such\nas the fine-tuned Mistral, demonstrated competitive results. However, the experiments indicate that significant\nadvancements are still needed for real-world applicability.\nFuture research could explore a broader range of models, including other open-source options like Gemma or\nlarger variants such as Llama-3 70B or Llama-3 400B [49]. Using locally sourced data and experimenting with\ndatasets in different languages, particularly low-resource ones, could enhance adaptability and robustness in diverse\nclinical settings. On the topic of data, creating synthetic datasets would also enable unrestricted distribution,\nfacilitating research.\nAddressing missing data in clinical notes presents another avenue for improvement. Introducing alternative\nmethods for capturing detailed information, such as voice recordings, could enrich the dataset. With advancements\nin multimodal data research, exploring new approaches to discharge summary generation, including multimodal\nsystems, holds significant potential [10].\nAcknowledgements\nThis work is co-financed by Component 5 - Capitalization and Business Innovation, inte-\ngrated in the Resilience Dimension of the Recovery and Resilience Plan within the scope of the Recovery and\nResilience Mechanism (MRR) of the European Union (EU), framed in the Next Generation EU, for the period 2021\n- 2026, within project HfPT, with reference 41.\nReferences\n[1] Wimsett, J., Harper, A., Jones, P.: Review article: Components of a good quality discharge summary: A\nsystematic review. Emergency Medicine Australasia 26(5), 430–438 (2014) https://doi.org/10.1111/1742-6723.\n12285\n[2] Searle, T., Ibrahim, Z., Teo, J., Dobson, R.J.B.: Discharge summary hospital course summarisation of in patient\nElectronic Health Record text with clinical concept guided deep pre-trained Transformer models. Journal of\nBiomedical Informatics 141, 104358 (2023) https://doi.org/10.1016/j.jbi.2023.104358\n[3] West, C.P., Dyrbye, L.N., Shanafelt, T.D.: Physician burnout: Contributors, consequences and solutions.\nJournal of Internal Medicine 283(6), 516–529 (2018) https://doi.org/10.1111/joim.12752\n[4] Shing, H.-C., Shivade, C., Pourdamghani, N., Nan, F., Resnik, P., Oard, D., Bhatia, P.: Towards Clinical\nEncounter Summarization: Learning to Compose Discharge Summaries from Prior Notes. arXiv (2021)\n[5] Naveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., Mian, A.: A\nComprehensive Overview of Large Language Models. arXiv (2024). https://doi.org/10.48550/arXiv.2307.06435\n[6] OpenAI: GPT-4 Technical Report. arXiv (2023)\n[7] Team, G.: Gemini: A Family of Highly Capable Multimodal Models. arXiv (2024). https://doi.org/10.48550/\narXiv.2312.11805\n[8] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D., Bressand, F., Lengyel, G.,\nLample, G., Saulnier, L., Lavaud, L.R., Lachaux, M.-A., Stock, P., Scao, T.L., Lavril, T., Wang, T., Lacroix,\nT., Sayed, W.E.: Mistral 7B. arXiv (2023)\n[9] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava,\nP., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J.,\nFu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M.,\nKerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich,\nD., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein,\nJ., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor,\nR., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S.,\n14\n"}, {"page": 15, "text": "Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv (2023). https://doi.org/10.48550/arXiv.2307.09288\n[10] Jangra, A., Mukherjee, S., Jatowt, A., Saha, S., Hasanuzzaman, M.: A Survey on Multi-modal Summarization.\nACM Computing Surveys 55(13s), 296–129636 (2023) https://doi.org/10.1145/3584700\n[11] Koh, H.Y., Ju, J., Liu, M., Pan, S.: An Empirical Survey on Long Document Summarization: Datasets, Models,\nand Metrics. ACM Computing Surveys 55(8), 154–115435 (2022) https://doi.org/10.1145/3545176\n[12] Ma, C., Zhang, W.E., Guo, M., Wang, H., Sheng, Q.Z.: Multi-document Summarization via Deep Learning\nTechniques: A Survey. ACM Computing Surveys 55(5), 1–37 (2023) https://doi.org/10.1145/3529754\n[13] Yadav, A.K., Ranvijay, Yadav, R.S., Maurya, A.K.: State-of-the-art approach to extractive text summarization:\nA comprehensive review. Multimedia Tools and Applications 82(19), 29135–29197 (2023) https://doi.org/10.\n1007/s11042-023-14613-9\n[14] Rodrigues, T., Teixeira Lopes, C.: Harnessing large language models for clinical information extraction: A\nsystematic literature review. ACM Trans. Comput. Healthcare 6(4) (2025) https://doi.org/10.1145/3744660\n[15] Tang, L., Sun, Z., Idnay, B., Nestor, J.G., Soroush, A., Elias, P.A., Xu, Z., Ding, Y., Durrett, G., Rousseau,\nJ.F., Weng, C., Peng, Y.: Evaluating large language models on medical evidence summarization. npj Digital\nMedicine 6(1), 1–8 (2023) https://doi.org/10.1038/s41746-023-00896-7\n[16] Zhu, Y., Yang, X., Wu, Y., Zhang, W.: Leveraging Summary Guidance on Medical Report Summarization.\narXiv (2023)\n[17] Adams, G., Alsentzer, E., Ketenci, M., Zucker, J., Elhadad, N.: What’s in a summary? laying the groundwork\nfor advances in hospital-course summarization. In: Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-\nTur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y. (eds.) Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pp. 4794–4811. Association for Computational Linguistics, Online (2021). https://doi.org/10.\n18653/v1/2021.naacl-main.382 . https://aclanthology.org/2021.naacl-main.382/\n[18] Alaei, S.: An Automated Discharge Summary System Built for Multiple Clinical English Texts by Pre-trained\nDistilBART Model. Master’s thesis, Stockholm University (2023)\n[19] Lu, Z., Peng, Y., Cohen, T., Ghassemi, M., Weng, C., Tian, S.: Large language models in biomedicine\nand health: Current research landscape and future directions. Journal of the American Medical Informatics\nAssociation 31(9), 1801–1811 (2024) https://doi.org/10.1093/jamia/ocae202\n[20] Xu, J., Chen, Z., Johnston, A., Blankemeier, L., Varma, M., Hom, J., Collins, W.J., Modi, A., Lloyd, R.,\nHopkins, B., Langlotz, C., Delbrouck, J.-B.: Overview of the first shared task on clinical text generation:\nRRG24 and “discharge me!”. In: Demner-Fushman, D., Ananiadou, S., Miwa, M., Roberts, K., Tsujii, J. (eds.)\nProceedings of the 23rd Workshop on Biomedical Natural Language Processing, pp. 85–98. Association for\nComputational Linguistics, Bangkok, Thailand (2024). https://doi.org/10.18653/v1/2024.bionlp-1.7 . https:\n//aclanthology.org/2024.bionlp-1.7/\n[21] Lee, S.H.: Natural language generation for electronic health records. npj Digital Medicine 1(1), 1–7 (2018)\nhttps://doi.org/10.1038/s41746-018-0070-0\n[22] Hanjae, K., Min, J.H., Bin, J.Y., Chan, Y.S.: Patient-friendly discharge summaries in korea based on chatgpt:\nSoftware development and validation. jkms 39(16), 148–0 (2024) https://doi.org/10.3346/jkms.2024.39.e148\nhttp://www.e-sciencecentral.org/articles/?scid=1516086925\n[23] Goswami, J., Prajapati, K.K., Saha, A., Saha, A.K.: Parameter-efficient fine-tuning large language model\napproach for hospital discharge paper summarization. Applied Soft Computing 157, 111531 (2024) https:\n//doi.org/10.1016/j.asoc.2024.111531\n[24] Ellershaw, S., Tomlinson, C., Burton, O.E., Frost, T., Hanrahan, J.G., Khan, D.Z., Horsfall, H.L., Little, M.,\n15\n"}, {"page": 16, "text": "Malgapo, E., Starup-Hansen, J., et al.: Automated Generation of Hospital Discharge Summaries Using Clinical\nGuidelines and Large Language Models. OpenReview (2024)\n[25] Pal, K.: Summarization and Generation of Discharge Summary Medical Reports. Master’s thesis, Brown\nUniversity, Providence, Rhode Island (2022)\n[26] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro,\nE., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation\nLanguage Models. arXiv (2023)\n[27] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\nMcCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models Are Few-Shot Learners. arXiv (2020)\n[28] Team, G.: Gemini 1.5: Unlocking Multimodal Understanding across Millions of Tokens of Context. arXiv (2024).\nhttps://doi.org/10.48550/arXiv.2403.05530\n[29] Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., Liu, Y.: RoFormer: Enhanced Transformer with Rotary Position\nEmbedding. arXiv (2023)\n[30] Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: QLoRA: Efficient Finetuning of Quantized LLMs.\narXiv (2023)\n[31] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-Rank Adaptation\nof Large Language Models. arXiv (2021)\n[32] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,\nAntiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner,\nB., Fang, L., Bai, J., Chintala, S.: PyTorch: An Imperative Style, High-Performance Deep Learning Library.\nIn: Advances in Neural Information Processing Systems, vol. 32. Curran Associates, Inc., Vancouver, British\nColumbia, Canada (2019)\n[33] Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. arXiv (2019). https://doi.org/10.48550/\narXiv.1711.05101\n[34] Wang, J., Shi, E., Yu, S., Wu, Z., Ma, C., Dai, H., Yang, Q., Kang, Y., Wu, J., Hu, H., Yue, C., Zhang, H.,\nLiu, Y., Pan, Y., Liu, Z., Sun, L., Li, X., Ge, B., Jiang, X., Zhu, D., Yuan, Y., Shen, D., Liu, T., Zhang, S.:\nPrompt Engineering for Healthcare: Methodologies and Applications. arXiv (2024). https://doi.org/10.48550/\narXiv.2304.14670\n[35] Johnson, A.E.W., Pollard, T.J., Shen, L., Lehman, L.-W.H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P.,\nCeli, L.A., Mark, R.G.: MIMIC-III, a freely accessible critical care database. Scientific Data 3, 160035 (2016)\nhttps://doi.org/10.1038/sdata.2016.35\n[36] ALLEA - All European Academies: The European Code of Conduct for Research Integrity. ALLEA - All\nEuropean Academies, DE (2023)\n[37] Research,\nD.-G.,\nInnovation:\nGuidelines\non\nthe\nResponsible\nUse\nof\nGenerative\nAI\nin\nResearch\nDeveloped\nby\nthe\nEuropean\nResearch\nArea\nForum\n-\nEuropean\nCommission.\nhttps://research-and-\ninnovation.ec.europa.eu/news/all-research-and-innovation-news/guidelines-responsible-use-generative-ai-\nresearch-developed-european-research-area-forum-2024-03-20 en (2024)\n[38] Kudo, T., Richardson, J.: SentencePiece: A simple and language independent subword tokenizer and detokenizer\nfor Neural Text Processing. In: Blanco, E., Lu, W. (eds.) Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations, pp. 66–71. Association for Computational\nLinguistics, Brussels, Belgium (2018). https://doi.org/10.18653/v1/D18-2012\n16\n"}, {"page": 17, "text": "[39] Papineni, K., Roukos, S., Ward, T., Zhu, W.-J.: BLEU: A method for automatic evaluation of machine transla-\ntion. In: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. ACL ’02, pp.\n311–318. Association for Computational Linguistics, USA (2002). https://doi.org/10.3115/1073083.1073135\n[40] Lin, C.-Y.: ROUGE: A Package for Automatic Evaluation of Summaries. In: Text Summarization Branches\nOut, pp. 74–81. Association for Computational Linguistics, Barcelona, Spain (2004)\n[41] Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Evaluating text generation with BERT.\nIn: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net, Addis Ababa, Ethiopia (2020). https://openreview.net/forum?id=SkeHuCVFDr\n[42] Sellam, T., Das, D., Parikh, A.: BLEURT: Learning Robust Metrics for Text Generation. In: Jurafsky, D.,\nChai, J., Schluter, N., Tetreault, J. (eds.) Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 7881–7892. Association for Computational Linguistics, Online (2020). https:\n//doi.org/10.18653/v1/2020.acl-main.704\n[43] Vasilyev, O., Dharnidharka, V., Bohannon, J.: Fill in the BLANC: Human-free quality estimation of document\nsummaries. In: Eger, S., Gao, Y., Peyrard, M., Zhao, W., Hovy, E. (eds.) Proceedings of the First Workshop\non Evaluation and Comparison Of NLP Systems, pp. 11–20. Association for Computational Linguistics, Online\n(2020). https://doi.org/10.18653/v1/2020.eval4nlp-1.2\n[44] Laban, P., Schnabel, T., Bennett, P.N., Hearst, M.A.: SummaC: Re-Visiting NLI-based Models for Inconsistency\nDetection in Summarization. Transactions of the Association for Computational Linguistics 10, 163–177 (2022)\nhttps://doi.org/10.1162/tacl a 00453\n[45] Sellam, T.: BLEURT Issue #1: How to run evaluation? https://github.com/google-research/bleurt/issues/1.\nGitHub issue in google-research/bleurt (2020). https://github.com/google-research/bleurt/issues/1\n[46] Pal, K.: Summarization and Generation of Discharge Summary Medical Reports. Master’s thesis, Brown\nUniversity, Providence, Rhode Island (2022)\n[47] Deci: The Correct Way to Measure Inference Time of Deep Neural Networks. https://deci.ai/blog/measure-\ninference-time-deep-neural-networks/ (2023)\n[48] Parliament, E., European Union, C.: Regulation (EU) 2016/679 of the European Parliament and of the Council\nof 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on\nthe Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation)\n(Text with EEA Relevance) (2016)\n[49] AI@Meta: Llama 3 Model Card (2024). https://github.com/meta-llama/llama3/blob/main/MODEL CARD.\nmd\n17\n"}]}