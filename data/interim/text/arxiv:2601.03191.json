{"doc_id": "arxiv:2601.03191", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.03191.pdf", "meta": {"doc_id": "arxiv:2601.03191", "source": "arxiv", "arxiv_id": "2601.03191", "title": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation", "authors": ["Anees Ur Rehman Hashmi", "Numan Saeed", "Christoph Lippert"], "published": "2026-01-06T17:13:23Z", "updated": "2026-01-06T17:13:23Z", "summary": "Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.03191v1", "url_pdf": "https://arxiv.org/pdf/2601.03191.pdf", "meta_path": "data/raw/arxiv/meta/2601.03191.json", "sha256": "2f46160d35d847f7e6c4801cbea82392416c6327df45ab884891fc2cbb0cf822", "status": "ok", "fetched_at": "2026-02-18T02:23:00.258130+00:00"}, "pages": [{"page": 1, "text": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model\nfor Chest X-Ray Interpretation\nAnees Ur Rehman Hashmi\nHasso Plattner Institute\nPotsdam, Germany\nanees.hashmi@hpi.de\nNuman Saeed\nMBZUAI\nAbu Dhabi, UAE\nnuman.saeed@mbzuai.ac.ae\nChristoph Lippert\nHasso Plattner Institute\nPotsdam, Germany\nchristoph.lippert@hpi.de\nAbstract\nMultimodal medical large language models have shown\nimpressive progress in chest X-ray interpretation but con-\ntinue to face challenges in spatial reasoning and anatomical\nunderstanding. Although existing grounding techniques im-\nprove overall performance, they often fail to establish a true\nanatomical correspondence, resulting in incorrect anatom-\nical understanding in the medical domain.\nTo address\nthis gap, we introduce AnatomiX, a multitask multimodal\nlarge language model explicitly designed for anatomically\ngrounded chest X-ray interpretation. Inspired by the radi-\nological workflow, AnatomiX adopts a two stage approach:\nfirst, it identifies anatomical structures and extracts their\nfeatures, and then leverages a large language model to\nperform diverse downstream tasks such as phrase ground-\ning, report generation, visual question answering, and im-\nage understanding. Extensive experiments across multiple\nbenchmarks demonstrate that AnatomiX achieves superior\nanatomical reasoning and delivers over 25% improvement\nin performance on anatomy grounding, phrase grounding,\ngrounded diagnosis and grounded captioning tasks com-\npared to existing approaches. Code and pretrained model\nare available at github.com/aneesurhashmi/anatomix\n1. Introduction\nMultimodal Large Language Models (MLLMs) are being\nincreasingly applied in the natural and medical imaging do-\nmain to perform multiple tasks using a single model [36].\nThese models typically consist of an image encoder and a\nLarge Language Model (LLM), and utilize the pretrained\nLLM’s strengths by passing image embeddings along with\na text prompt into the LLM to perform downstream tasks\n[20]. The pretrained LLMs are generally trained on very\nlarge text corpora and therefore demonstrate strong text\ngeneration capabilities, making them suitable for a diverse\nset of downstream tasks after supervised fine-tuning and\ninstruction tuning [1, 24]. However, owing partly to their\nautoregressive design and the challenges of merging vision\nand language modalities, MLLMs still struggle with fine-\ngrained spatial understanding, for instance, when reasoning\nabout positions of multiple objects or their relative spatial\nrelations in a scene [21, 34].\nThis issue of MLLMs has previously been addressed by\nintroducing object grounding, which aligns the text con-\ncepts with the objects in the image [28, 33]. Grounding\nin MLLMs is usually achieved by training the model on a\ndataset containing the names or descriptions of local objects\nin the image plus their bounding boxes or segmentation\nmasks as spatial markers. This in turn improves the reason-\ning abilities of MLLMs with better concept understanding,\nmaking them applicable in the medical domain, where spa-\ntial reasoning is essential. In particular, chest X-ray (CXR)\ninterpretation greatly benefits from such multimodal rea-\nsoning, as accurate localization and semantic alignment be-\ntween textual findings and radiographic regions are crucial\nfor diagnosis. MLLMs like ViviMed [23], ChexAgent [7],\nRadVLM [10] and MAIRA-2 [4] show that grounding via\nspecial tokens yields consistent performance gains on CXR\nimage tasks.\nAlthough incorporating grounding through additional to-\nkens has improved MLLMs’ spatial reasoning, it remains\ninsufficient for the fine-grained localization and differenti-\nation required in medical imaging, where anatomically dis-\ntinct regions often exhibit highly similar visual textures and\nappearances [34]. As illustrated in Fig. 1, current state-of-\nthe-art (SOTA) MLLM fails to correctly localize lesions or\nidentify the correct anatomical objects when presented with\nflipped images - where the left and right sides are switched.\nThese models may perform well on standard orientations\nbut fail when spatial cues are inverted, revealing that they\noverly rely on spatial correlations rather than recognition of\nanatomical structures, exposing a critical gap between vi-\nsual grounding and medical comprehension.\nThis weak anatomical understanding in current medical\nMLLMs is likely due to their single-step visual grounding\narXiv:2601.03191v1  [cs.CV]  6 Jan 2026\n"}, {"page": 2, "text": "Identify the position of the\nfollowing finding in the CXR:\nleft lung atelectasis\nIdentify the position of the\nfollowing finding in the CXR:\nright lung atelectasis\nIdentify the position of the\nfollowing finding in the CXR:\nright lung atelectasis\nLocate left clavical and lower\nright lung\nLocate right clavical and left\nlung\nAnatomiX (ours)\nRadVLM\nPrompt\nFigure 1. Comparison between AnatomiX and RadVLM [10] in anatomy understanding. (a) and (b) show both models predicting the\ndisease on the correct side (color scheme: red for model’s output, green for all ground truth locations). (c), (d) and (e) show models’\noutputs for the same image flipped on the vertical axis (left ↔right), where RadVLM completely fails to recognize the correct anatomical\nobject, while AnatomiX successfully recognizes the correct anatomies, showcasing high anatomical understanding\nprocess. Specifically, these models must implicitly detect\nthe correct anatomical objects within an image before per-\nforming the downstream task. This one-step process differs\nfundamentally from the workflow of radiologists, who itera-\ntively identify, localize, and evaluate each anatomical struc-\nture before drawing diagnostic conclusions. To address this\nissue, we introduce AnatomiX, an anatomy-aware grounded\nMLLM for chest X-Ray interpretation. AnatomiX uses a\ntwo stage process to first identify different thoracic anatom-\nical objects (organs) before performing the task; thereby,\nshowing a high anatomical understanding compared to ex-\nisting CXR grounding MLLMs as shown in Fig. 1. Our\nproposed model significantly outperforms SOTA models on\nfour grounding tasks and shows SOTA or on-par perfor-\nmance on report generation, VQA and image understand-\ning tasks. Extensive experiments on a large collection of\ndatasets show the high reasoning and anatomical under-\nstanding capabilities of AnatomiX. In summary, our work\nmakes the following contributions:\n• We introduce AnatomiX, an anatomy-aware grounded\nmultimodal large language model for chest X-Ray inter-\npretation.\n• AnatomiX achieves SOTA performance on a CXR related\ngrounding task, while maintaining on-par or better perfor-\nmance on report-generation, VQA and image understand-\ning tasks.\n• We demonstrate the robustness of AnatomiX across dif-\nferent datasets and perform ablations to validate the con-\ntribution of each component.\n2. Related Work\nEarly adaptations of MLLMs to radiology primarily involve\nfine-tuning general-domain models on medical datasets.\nLLaVA-Med [16] and RadVLM [10] extend LLaVA [20]\nfor multi-task CXR benchmarks, improving both report\ngeneration and VQA. Several works introduce explicit\ngrounding for CXR tasks: ViviMed [23] and MedRG [38]\npair the Segment-Anything model [15] with an LLM for\ndetection and segmentation, while MAIRA-2 [4] enables\ngrounded report generation through additional tokens. Sim-\nilarly, RadVLM [10] constructs a large instruction dataset\nfor diverse CXR tasks, whereas CheXagent [7] applies con-\ntrastive learning and instruction tuning to enhance phrase\ngrounding and CXR report generation. Radialog [27] sup-\nports multi-turn CXR conversations, and MedGemma [30]\nadapts Gemma-3 for general purpose medical tasks by fine-\ntuning on large-scale medical datasets. More recently, AOR\n[17] introduced region level information in LLM for CXR\ninterpretation; however, this model is not yet publicly avail-\nable for testing and comparison.\nMost prior efforts adapt general-domain MLLMs rather\nthan developing domain-specific architectures. CheX [25]\nadvances toward anatomy-aware modeling by incorporat-\ning anatomical objects from CXR reports but lacks prompt-\nbased interaction and generative flexibility.\nOverall, ex-\n"}, {"page": 3, "text": "Self Similarity \nLoss Matrix\nK\nV\nQ\nEncoder\nDecoder\nSentence \nEncoder\nSimilarity\nSearch\nBest matching Sentences\nImage Tokens\nAnatomy Tokens\nTraining\nFrozen\nOutput for       \n\"Right lung is healthy.\"\n\"Left lung shows\npleural effusion and\ncardiomegaly.\"\n...\nContrastive Alignment\nContrastive Retrieval\nTraining\nInference\nAPM Architecture\nFigure 2. Anatomy Perception Module (APM) architecture (a): The encoder E outputs image embedding Ip, while the decoder D and\nfeature extraction module M output object bounding boxes ˆybox, and anatomical object tokens ˆOA, respectively. Different colors in ˜O,\nO and ˆOA represent specific anatomical objects. (b) shows the contrastive alignment using frozen sentence encoder S and self-similarity\nloss. (c): The vector database (VDB) contains the text sentences and embeddings used for contrastive retrieval. (Bottom right): APM uses\n(a) and (b) during training, and replaces (b) with (c) during inference. The P represent different FC projectors described in section 3.1.\nisting CXR MLLMs rely on instruction tuning and large-\nscale radiology datasets, achieving strong benchmark per-\nformance yet showing limited reasoning and anatomical\nunderstanding [34]. In contrast, our approach introduces\na two-stage anatomy-aware pipeline that explicitly models\nthoracic structures before performing downstream tasks.\n3. Methodology\nThis section describes the architecture of AnatomiX, which\ncomprises two primary components: The Anatomy Percep-\ntion Module and a large language model outlined below.\n3.1. Anatomy Perception Module\nGiven an input CXR image I, the objective of the Anatomy\nPerception Module (APM) is to extract a global image\nrepresentation along with fine-grained features corre-\nsponding to N thoracic anatomical objects. The resulting\nrepresentations are subsequently used by an LLM for\ndownstream tasks. The APM adopts a multi-task learning\nframework that jointly learns global image features, object\nlocalization through bounding boxes, and their detailed\nanatomical representations, while also retrieving textual\ndescriptions associated with each anatomical object. Fig. 2\nshows the detailed architecture of the APM, which consists\nof an image encoder (E), a decoder (D), a feature extraction\nmodule (M) and a sentence encoder (S). The details of\neach component are given below.\nImage Encoder E and Decoder D:\nThe input image I is first encoded by the image encoder E\nproducing the image representation Ip ∈RP ×d which con-\nsists of P patch embedding vectors of dimension d. To-\ngether, these form a global representation of the image.\nThese embeddings serve as the shared representation of vi-\nsual information for subsequent modules. Specifically, Ip\nis provided both to the feature extractor M, which focuses\non semantic anatomy cues, and to the decoder D, which\nis inspired by DETR [6]. The decoder processes Ip jointly\nwith N learnable object tokens ˜O = [˜o1, . . . , ˜oN], using\ntransformer blocks to perform cross-attention between to-\nkens and image patches. Through this interaction, each ob-\nject token learns to attend to relevant anatomical regions,\nresulting in updated token embeddings O that encode the\nlocalization of the N anatomical objects.\nO = D(Ip, ˜O)\n(1)\nwhere O ∈RN×d = [o1, ..., oN; oi ∈Rd]\nThe object tokens O are projected using a fully con-\nnected (FC) projector Pbox to predict a bounding box for\neach anatomical object (ˆybox = Pbox(O)), as illustrated in\nFig. 2. Unlike DETR [6], O is not permutation invariant,\nwhich means that each element of O corresponds to exactly\none predefined anatomical object. This design enables each\ntoken oi to focus on and extract information related to the\nith anatomical object from Ip. To effectively learn the ˆybox\nlocalization, we use a combination of L1 and intersection\nover union (IoU) losses as shown in eq. 2 and 3.\nLIoU(ˆybox, ybox) = 1 −|ˆybox ∩ybox|\n|ˆybox ∪ybox|\n(2)\nLbox = λ1(|ˆybox −ybox|) + λ2LIoU(ˆybox, ybox)\n(3)\n"}, {"page": 4, "text": "LoRA\nMultimodal Prompt Template\nAnatomy Perception\nModule\nImage Tokens\nAnatomy Tokens\nTraining\nFrozen\nFigure 3. Overall architecture of AnatomiX. The outputs of the\nAPM and the user prompt U are added to a structured multimodal\nprompt template before being passed to the LM, which generates\nthe response T. Pim and PA represent FC projectors as described\nin section 3.2.\nwhere ybox represents ground truth bounded boxes, λ1 = 5\nand λ2 = 2 are the weightings of the L1 Loss and the LIoU\nLoss, respectively, set to the default values used in DETR.\nFeature Extraction Module M:\nIn addition to predictions of the bounding boxes in the\ndecoder output, we leverage the spatial information en-\ncoded in O to extract fine-grained representations of each\nanatomical object through the feature extraction module M.\nWithin M, cross-attention is computed between O and im-\nage patches Ip as shown in eq. 4, where the anatomical\nobject tokens O serve as queries (Q), and the image embed-\nding Ip provides the keys (K) and values (V ). Conceptu-\nally, M can be viewed as an extension of the decoder D,\nwhere the image representation Ip is re-included via a skip\nconnection.\nOA = M(Q, K, V ) = Softmax\n\u0012QKT\n√\nd\n\u0013\nV,\n(4)\nwhere Q = OWQ is the query matrix, K = IpWK is the\nkey matrix and V = IpWV is the value matrix.\nThe output OA represent localized image features for the\nanatomical objects at the corresponding positions specified\nby the predicted bounding boxes ˆybox. Subsequently, these\nfeatures are projected to a lower-dimensional space (d →s)\nusing the projection module PM as: ˆOA = PM(OA) ,\nwhich yields ˆOA ∈RN×s.\nContrastive Alignment with S:\nDuring APM training, we perform localized contrastive\nalignment (Fig. 2-b) between each anatomical feature token\nˆoi\nA ∈ˆOA and its corresponding textual description si ∈St,\nwhere si specifies the radiological findings in the associ-\nated anatomical region (e.g. Right Lung shows pneumo-\nnia, pleural effusion and atelectasis.). Since ˆOA encodes\nfine-grained visual representations of N anatomical objects\nderived from the image embedding Ip, aligning these to-\nkens with the corresponding text sentence embeddings en-\nables the model to establish correspondences between vi-\nsual and semantic representations. This alignment ensures\nthat the visual tokens capture each anatomical region’s spa-\ntial and structural properties while linking them to clinically\nrelevant textual concepts. The textual descriptions St are\nfirst encoded using the frozen sentence encoder S (Biomed-\nBERT [11]) to obtain embeddings ˆSE ∈RN×768, which are\nthen projected to a lower-dimensional space (768 →s) us-\ning a fully connected projector PS for efficient contrastive\nlearning.\nˆSE = S(St)\n(5)\nˆSA = PS(ˆSE)\n(6)\nwhere ˆSA ∈RN×s represents projected text embeddings.\nRadiological findings across thoracic anatomical objects\noften overlap or appear together, making it uncommon for\nan image to contain a finding in only one region. In such\ncases, using standard CLIP-style contrastive loss [29] can\nintroduce many false negatives, as it assumes only one cor-\nrect (positive) text–image pair per sample. To overcome\nthis, we employ a soft contrastive loss that allows multi-\nple degrees of similarity across anatomical regions. Specif-\nically, we introduce a self-similarity matrix Sself (eq. 7),\nwhich permits non-zero similarity values for off-diagonal\nentries, reflecting the natural co-occurrence of anatomi-\ncal observations (see the supplementary material section\nS1 for details).\nWe optimize this alignment using Kull-\nback–Leibler (KL) divergence as shown in eq. 8, which\nmeasures distributional differences instead of enforcing dis-\ncrete class boundaries [13].\nThis makes it well-suited\nfor overlapping or correlated anatomical feature represen-\ntations and preserving partial similarities among different\nanatomical objects’ features.\nSself = ˆSEˆST\nE\n(7)\nLCL(ˆSA, Sself) =\nX\ni\nˆSA(i) log\nˆSA(i)\nSself(i)\n(8)\nwhere Sself ∈RN×N\nFinally, we train APM end-to-end using a combination of\nbounding box prediction and contrastive alignment losses.\nLAP M = Lbox + LCL\n(9)\nContrastive Retrieval with VDB:\n"}, {"page": 5, "text": "Since the textual descriptions St associated with anatomical\nobjects are available only during APM training, we replace\nthe sentence encoder S with a compact vector database\nVDB during inference as shown in Fig. 2-(c). This database\nstores all unique textual sentences corresponding to each\nof the N anatomical regions, along with their precomputed\nembeddings. At inference time, each anatomical object to-\nken in ˆOA is compared against the sentence embeddings in\nVDB to retrieve the most semantically similar sentence ˆSt\nfor that anatomical region as ˆSt = VDB(ˆOA). The retrieved\nsentences are then passed to LLM, along with the anatom-\nical object tokens ˆOA, the predicted bounding boxes ˆybox,\nand the image embeddings Ip. Additional implementation\ndetails of VDB are provided in the supplementary material\nsection S2.\n3.2. Large Language Model\nGiven the outputs of APM — image embeddings Ip, pre-\ndicted locations of N anatomical objects ˆybox and their cor-\nresponding features ˆOA and retrieved text descriptions ˆSt -\nand the user prompt U, the large language model LM gen-\nerates a textual response T performing the task specified in\nU. To achieve this, we construct a multimodal prompt tem-\nplate (shown in supplementary Fig. S1) that integrates the\nfine grained anatomical and textual information extracted in\nAPM, and passes it to LM. Fig. 3 shows the overall archi-\ntecture of AnatomiX. Firstly, to align the embedding spaces\nof APM and LM, we project both the image embedding\nIp and anatomical object tokens embedding ˆOA into LM’s\nembedding space using FC projectors. Specifically, Pim\n(d →l) maps the image embedding to Il = Pim(Ip), where\nIl ∈RP ×l, and PA (s →l) maps the anatomical object\ntoken embedding to Ol = PA(ˆOA), where Ol ∈RN×l.\nThe language model LM is based on MedGemma-4b-it\n[30] LLM architecture (excluding vision encoder). To en-\nable anatomy aware reasoning, we extend the model’s vo-\ncabulary by introducing N special tokens for anatomical\nobjects (<obj i> for i ∈[0, N]) and four additional to-\nkens for spatial grounding (<box>, </box>, <ref>, and\n</ref>). Each <obj i> token corresponds to the fea-\nture representation of the ith anatomical object, oi\nl, where\nOl = [o1\nl , . . . , oN\nl ]. By explicitly providing these object-\nspecific tokens, the LLM can directly access the fine grained\nvisual features of each anatomical object. This design al-\nlows LM to directly reason over anatomical objects rather\nthan implicitly inferring them from global image represen-\ntations and then performing the task given in U. Finally,\nthe LM is trained for the next token prediction using Low-\nRank Adaptation (LoRA) [12] and optimized with standard\ncross-entropy loss as shown in eq. 10.\nReport\nGeneration\nGrounding\nVQA\nImage Und-\nerstanding\nReport\nGeneration\nPhrase\nGrounding\nGrounded\nDiagnosis\nAnatomy-\nGrounding\nGrounded\nCaptioning\nClose-Ended\nVQA\nOpen-Ended\nVQA\nAbnormality\nDetection\nImage Classi\nfication\nMIMIC-Report-Gen.\nVinDr-Instruct\nPatchChest-Gr.\nMS-CXR\nVinDr-Instruct\nMS-CXR\nAnatomy Grounding\nMS-CXR\nMIMIC-VQA\nSLAKE\nRadialog-Instruct\nMIMIC-VQA\nSLAKE\nRadialog-Instruct\nVinDr-Instruct\nMIMIC-CXR-Clf.\nFigure 4. Set of 9 radiology tasks (middle circle) spanning 4 cat-\negories (inner circle) done by AnatomiX and the datasets used\n(outer circle).\nLLM = −1\nT\nT\nX\nt=1\nX\nw∈V\nyt(w) log pLM(w | x<t)\n(10)\npLM(w | x<t) = Softmax(LM(x<t))\n(11)\nwhere T is the total number of tokens in the sequence, V\nis the vocabulary, x<t represents the context tokens before\nposition t, yt(w) is the one-hot ground truth distribution at\nstep t, w denotes a token in the vocabulary V , and pLM(w |\nx<t) is the probability assigned by LM for token w given\nx<t.\n4. Experiments\n4.1. Dataset\nWe train APM of AnatomiX using more than 237,000 sam-\nples from the Chest ImaGenome [35] dataset, which extends\nMIMIC-CXR [14] with detailed spatial and semantic anno-\ntations, providing localized information for 36 anatomical\nobjects and their observations. We used the given object-\nwise phrases for St and the given object bounding boxes\nfor ybox prediction.\nSupplementary Table S1 provides the summary of nine\ndatasets used for LM training.\nThese datasets con-\ntain instruction-response pairs derived from eight publicly\navailable CXR datasets including MIMIC-CXR-JPG [14],\nVinDr-CXR [26], MS-CXR [5], PadChest-Grounding [8],\n"}, {"page": 6, "text": "Table 1. Performance on four grounding tasks. For Grounded Diagnosis (GD) and Grounded Captioning (GC), results are shown as GD /\nGC. AnatomiX-Ip and AnatomiX-ˆOA represent the ablations discussed in section 6.\nModel\nNLG Metrics (GD / GC)\nClinical Metrics (GD / GC)\nPhrase Grounding\nAnatomy Grounding\nBERTScore\nROUGE\nMETEOR\nRadGraph-\nF1\nChexbert-14-\nF1\nIoU\nmAP\nIoU\nmAP\nMAIRA-2\n0.01 / 0.08\n0.01 / 0.06\n0.01 / 0.04\n0.00 / 0.02\n0.03 / 0.02\n0.32\n0.24\n0.35\n0.24\nRadVLM\n0.15 / 0.27\n0.06 / 0.11\n0.05 / 0.07\n0.00 / 0.12\n0.32 / 0.40\n0.39\n0.30\n0.60\n0.49\nCheXagent\n0.49 / 0.56\n0.43 / 0.44\n0.29 / 0.37\n0.40 / 0.39\n0.40 / 0.61\n0.33\n0.24\n0.18\n0.09\nAnatomiX (ours)\n0.63 / 0.65\n0.60 / 0.56\n0.42 / 0.48\n0.58 / 0.50\n0.54 / 0.78\n0.46\n0.35\n0.73\n0.66\nAnatomiX-Ip\n0.10 / 0.06\n0.11 / 0.04\n0.07 / 0.04\n0.08 / 0.05\n0.25 / 0.21\n0.10\n0.03\n0.04\n0.01\nAnatomiX- ˆOA\n0.42 / 0.17\n0.38 / 0.12\n0.26 / 0.06\n0.35 / 0.08\n0.42 / 0.23\n0.24\n0.16\n0.36\n0.27\nSLAKE [19], MIMIC-CXR-VQA [2], RaDialog-Instruct\n[27] and Chest-ImaGenome [35].\nVinDr-Instruct, and\nAnatomy Grounding instruction-response datasets were\ncreated using the VinDr-CXR [26] and Chest ImaGenome\n[35] datasets, respectively.\n4.2. Radiology Tasks\nAnatomiX is trained on a diverse set of nine CXR-related\ntasks, spanning four categories:\nimage understanding,\ngrounding, report generation, and visual question answer-\ning (VQA), as illustrated in Fig. 4. The details of each task\nare provided in the supplementary material section S3.\n4.3. Training Scheme\nAnatomiX is trained in three steps that focus on different\narchitectural components and tasks. The first step focuses\non the end-to-end training of APM for anatomical object\ndetection and contrastive alignment. We set the number of\nanatomical objects (N) = 36 and train APM for 45 epochs\nwith 0.0001 learning rate. Followed by an alignment step\n(step 2), where we align the embedding space of LM and\nAPM by unfreezing the Pim and PA projectors while keep-\ning all other components frozen. This step uses the report\ngeneration dataset for 2 epoch training with 0.0002 learn-\ning. The third and final step focuses on instruction tuning,\nwhere we train LM (using LoRA [12]) along with Pim and\nPA for 4 epochs while keeping APM frozen. This step in-\ncludes supervised fine-tuning on all nine tasks discussed in\nsection 4.2. All training steps use AdamW [22] optimizer\nand were trained on 4 NVIDIA H100 GPUs with 80GB\nmemory for approximately 125 hours.\n5. Results and Discussion\n5.1. Grounding\nWe begin by evaluating the model’s visual grounding ca-\npabilities. Specifically, we examine its ability to highlight\nrelevant regions or pathologies on CXRs and to describe\nthe features present within those regions.\nThis informa-\ntion allows clinicians to visually verify the model’s pre-\ndictions and gain insight into its decision making process.\nTo assess these capabilities, we evaluate AnatomiX on four\nchallenging grounding tasks using a combination of natu-\nral language generation (NLG), clinical, and detection met-\nrics.\nWe primarily compare our model against existing\nMLLMs with grounding capabilities, including RadVLM\n[10], Maira-2 [4], and CheXagent [7].\nThe phrase grounding and anatomy grounding tasks re-\nquire localizing entities using bounding boxes.\nAccord-\ningly, we evaluate these tasks using IoU and mean av-\nerage precision (mAP) metrics.\nAs shown in Table 1,\nAnatomiX significantly outperforms all other models by\nup to 15% in phrase grounding and over 25% in anatomy\ngrounding. This substantial improvement stems from the\nanatomy-oriented design of AnatomiX, which enables the\nmodel to focus more effectively on specific anatomical\nstructures.\nFig.\n5 illustrates representative samples of\nphrase and anatomy grounding tasks, where our model ac-\ncurately localizes both anatomical regions and pathologies\ncompared to the second best model RadVLM [10]. To fur-\nther study the anatomical understanding, we perform phrase\nand anatomy grounding tasks on a flipped image as shown\nin Fig. 1. RadVLM [10] performs well on standard input\nbut fails on flipped images, confusing left–right structures\nand relying on orientation cues. In contrast, AnatomiX pre-\nserves accurate grounding under flipping, demonstrating ro-\nbust and spatially consistent anatomical reasoning—an es-\nsential capability for reliable medical image interpretation.\nThe grounded diagnosis and grounded captioning tasks,\nin contrast, require the model to identify pathologies or\ndescribe image content within a user-specified region.\nWe evaluate performance on these tasks using both NLG\n(ROUGE [18], BERTScore [37], and METEOR [3]) and\nclinical (RadGraph-F1 [9] and CheXbert-14-F1 [32]) met-\nrics. As shown in Table 1, AnatomiX consistently achieves\nthe highest scores across all metrics providing upto 30%\ngains in grounded diagnosis and over 25 % improvement\nin grounded captioning tasks, further underscoring the ef-\nfectiveness of its anatomy-aware architecture.\nFig.\n5\nshow sample input-outputs of AnatomiX and the second\n"}, {"page": 7, "text": "Identify the position of the following\nfinding in the CXR: small left\npneumothorax\n<ref>small left pneumothorax</ref><box>\n(560,133),(840,310)</box>\n<ref>small left pneumothorax</ref><box>\n(598,147),(812,324)</box>\nThis finding is located at [0.58, 0.14, 0.83,\n0.25] in the image.\nWhere on this image is the Structure of\nleft margin of heart located?\nThe <ref>Structure of left margin of\nheart</ref> is at <box>(451,422),(660,667)\n</box>\nFor the <ref>Structure of left margin of\nheart</ref>, the coordinates are <box>\n(452,420),(672,658)</box> on the X-ray.\nThe location for the <ref>Structure of left\nmargin of heart</ref> is marked at [0.48,\n0.49, 0.68, 0.71] on the Chest X-ray.\nPhrase Grounding\nDescribe the content of the following\nregion(s): <box>(178,508),(403,735)\n</box><box>(612,552),(845,784)</box>\nBibasilar pneumonia\nBibasilar pneumonia\nbilateral lower lobe opacities\nPlease give the corresponding\ndiagnosis for the following region(s):\n<box>(550,490),(823,699)</box>\nPneumonia\nPneumonia\nPneumothorax\nAnatomy Grounding\nGrounded Captioning\nGrounded Diagnosis\nGround\nTruth\nRadVLM / CheXagent\nAnatomiX (ours)\nPrompt\nTask\nFigure 5. Sample input–output pairs and comparison with second best models on grounding tasks. The upper panels show outputs from our\nmodel across four tasks. The lower panels compare AnatomiX with RadVLM [10] for phrase and anatomy grounding, and with CheXagent\n[7] for grounded diagnosis and captioning. Box colors: blue = user input, green = ground truth, red = model output.\nbest model CheXagent [7] for these two tasks, showcas-\ning our model’s capacity to generate precise and clinically\nmeaningful descriptions aligned with the specified regions.\nNotably, MAIRA-2 [4] model completely fails to perform\nthese grounding tasks, as it was not trained to incorporate\nspatial or region-specific input.\n5.2. Report Generation\nThe automatic generation of radiology reports is an impor-\ntant task that significantly reduces the time required for\nCXR interpretation and reporting. We evaluate this task\nusing a set of NLG (ROUGE [18], BERTScore [37], ME-\nTEOR [3]) and clinical (RadGraph-F1 [9], and CheXbert-\n14-F1 [32]) metrics. We compare our model against sev-\neral SOTA CXR report generation models, including Rad-\nVLM [10], Maira-2 [4], CheXagent [7], Radialog [27], and\nMedgemma [30].\nTable 2 shows that AnatomiX consis-\ntently outperforms competing approaches across metrics,\ndemonstrating its strong capability in producing both lin-\nguistically coherent and clinically accurate reports.\nThe\nonly exception occurs in CheXbert-14-F1 [32], where\nAnatomiX attains an F1 score of 0.42, compared to 0.48 for\nRadialog and 0.45 for Maira-2. Importantly, both of these\nmodels contain approximately 1.5 × more parameters than\nAnatomiX, emphasizing the efficiency and scalability of our\napproach. These results collectively highlight AnatomiX’s\nbalance between performance and computational efficiency,\nsuggesting that it effectively captures domain-specific med-\nical semantics without relying on excessively large model\narchitectures.\n5.3. Image Understanding\nFor the image understanding tasks, image classification\nand abnormality detection, we evaluate performance us-\ning AUROC, CheXbert-14-F1 [32], IoU, and mAP scores.\nAnatomiX outperforms all compared models in the image\nclassification task and achieves the highest IoU for abnor-\nmality detection, while maintaining competitive mAP per-\nformance with leading approaches. The results in Table 3\nhighlight the model’s strong visual reasoning capabilities\n"}, {"page": 8, "text": "Table 2. Report Generation performance grouped by NLG and\nClinical metrics. AnatomiX-Ip and AnatomiX-ˆOA represent the\nablations.\nModel\nNLG Metrics\nClinical Metrics\nROUGE\nBERTScore\nMETEOR\nRadGraph\nChexbert-14 F1\nMAIRA-2\n0.43\n0.25\n0.12\n0.17\n0.45\nRadialog\n0.51\n0.35\n0.18\n0.24\n0.48\nMedGamma\n0.37\n0.29\n0.18\n0.20\n0.40\nRadVLM\n0.45\n0.27\n0.12\n0.19\n0.32\nCheXagent\n0.32\n0.16\n0.06\n0.15\n0.31\nAnatomiX (ours)\n0.53\n0.38\n0.21\n0.26\n0.42\nAnatomiX-Ip\n0.15\n0.18\n0.09\n0.15\n0.24\nAnatomiX- ˆOA\n0.05\n0.08\n0.02\n0.06\n0.12\nTable 3.\nPerformance on image classification and abnormality\ndetection tasks. AnatomiX-Ip and AnatomiX-ˆOA represent ab-\nlations discussed in section 6.\nModel\nImage Classification (IC)\nAbnormality Detection (AD)\nAUROC\nChexbert-14-\nF1\nIoU\nmAP\nMAIRA-2\n0\n0\n0.16\n0.01\nRadialog\n0.65\n0.47\n0\n0\nMedGamma\n0.58\n0.40\n0.04\n0\nRadVLM\n0.63\n0.43\n0.28\n0.12\nCheXagent\n0.92\n0.85\n0.31\n0.22\nAnatomiX (ours)\n0.92\n0.85\n0.31\n0.20\nAnatomiX-Ip\n0.85\n0.75\n0.08\n0.06\nAnatomiX- ˆOA\n0.88\n0.77\n0.15\n0.10\nand reliable localization performance.\n5.4. Visual Question Answering (VQA)\nWe benchmark AnatomiX on both open- and close-ended\nVQA tasks using BERTScore [37] and CheXbert-14-F1\n[32]. AnatomiX demonstrates strong performance across\nboth settings (Table 4), outperforming all models and\nmatching or exceeding CheXagent [7]. Minor score dif-\nferences mainly arise from keywords mismatch with the\nground truth, for example when the ground truth for the\nquestion “Is there any pneumonia in the image?” is “yes,\npneumonia is present” but the model outputs only “yes”,\nor vice versa, these metrics assign a lower score despite\nboth being clinically correct. Overall, these results high-\nlight AnatomiX’s high VQA capabilities.\n6. Ablation\nLLM: We conduct two ablations to analyze the effect of\nanatomical and textual inputs. In AnatomiX-Ip, only im-\nage embeddings Ip are provided to LM, excluding anatom-\nical tokens, bounding boxes, and retrieved sentences. In\nAnatomiX-ˆOA, we add ˆOA alongside Ip to isolate the in-\nfluence of ybox and ˆSt. As shown in Table 1, removing\nanatomical information causes a major performance drop,\nwhile adding ˆOA partially recovers results but remains lim-\nited on grounded captioning, which requires detailed de-\nTable 4.\nOpen and close ended VQA task performance.\nAnatomiX-Ip and AnatomiX-ˆOA represent ablation experiments.\nModel\nOpen-Ended VQA\nClose-Ended VQA\nBERTScore\nChexbert-14 F1\nBERTScore\nChexbert-14 F1\nMAIRA-2\n0.07\n0.31\n0.10\n0.81\nRadialog\n0.08\n0.43\n0.03\n0.92\nMedGemma\n0.03\n0.44\n0.02\n0.38\nRadVLM\n0.07\n0.04\n0.23\n0.67\nCheXagent\n0.86\n0.87\n0.90\n0.97\nAnatomiX (ours)\n0.86\n0.86\n0.89\n0.95\nAnatomiX-Ip\n0.38\n0.64\n0.61\n0.87\nAnatomiX- ˆOA\n0.39\n0.52\n0.48\n0.79\nTable 5. Ablation experiments for APM. AnatomiX-wo-M rep-\nresents experiment without M while AnatomiX-Dino shows the\nexperiment with DinoV3 as image encoder.\nybox Metrics\nˆSt Metrics\nModel\nIoU\nChexbert-14-\nF1\nRadGraph-F1\nMETEOR\nAnatomiX-wo-M\n0.781\n0.627\n0.689\n0.71\nAntomiX-Dino\n0.792\n0.614\n0.673\n0.694\nAnatomiX\n0.812\n0.634\n0.709\n0.727\nscriptions. For report generation (Table 2), including ˆOA\nwithout retrieved sentences reduces performance, indicat-\ning that ˆSt is essential for descriptive tasks. Similar trends\nin VQA and image understanding (Tables 3 and 4) suggest\nthat ˆOA aids spatial reasoning, whereas ˆSt primarily bene-\nfits linguistically rich tasks.\nAPM: We conduct two ablations to assess the architectural\ncomponents of APM. First, we replace the image encoder\nE with a pretrained DinoV3 encoder [31], referred to as\nAnatomiX-Dino. Second, we evaluate the role of M by per-\nforming contrastive alignment directly using the output of\nthe decoder D and removing M, denoted as AnatomiX-wo-\nM. For object detection (ybox prediction), we report IoU,\nand for contrastive retrieval, we report CheXbert-14-F1\n[32], RadGraph-F1 [9], and METEOR [3] scores by com-\nparing the retrieved text ˆSt with ground truth sentences. As\nshown in Table 5, DinoV3 yields competitive IoU but lower\ntextual correctness, while removing M leads to an overall\nperformance drop—indicating that M enhances anatomical\nfeature extraction and improves sentence retrieval.\n7. Conclusion\nIn conclusion, AnatomiX shows significant improvements\nin CXR interpretation, especially in tasks that require a di-\nrect anatomical understanding. Our results highlight that\nanatomy-oriented design is the key to accurate spatial rea-\nsoning in medical MLLMs. While finetuning natural image\nMLLMs on large medical datasets can help, it can create\nfalse spatial correspondences. Future work could extend\nanatomy oriented architectures to other modalities such as\n"}, {"page": 9, "text": "MRI and echocardiography. This study focuses on single\nturn interactions; extending it to multi turn setups would\nenhance flexibility and applicability.\nOverall, this work\nmarks an important step toward domain-specific design in\nMLLMs.\nReferences\n[1] DM Anisuzzaman, Jeffrey G Malins, Paul A Friedman, and\nZachi I Attia.\nFine-tuning llms for specialized use cases.\nMayo Clinic Proceedings: Digital Health, 2024. 1\n[2] Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho,\nGyubok Lee, Sunjun Kweon, Jungwoo Oh, Lei Ji, Eric\nChang, Tackeun Kim, et al. Mimic-ext-mimic-cxr-vqa: A\ncomplex, diverse, and large-scale visual question answering\ndataset for chest x-ray images, 2024. 6\n[3] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, pages 65–72, 2005. 6, 7, 8\n[4] Shruthi Bannur, Kenza Bouzid, Daniel C Castro, Anton\nSchwaighofer, Anja Thieme, Sam Bond-Taylor, Maximilian\nIlse, Fernando P´erez-Garc´ıa, Valentina Salvatelli, Harshita\nSharma, et al. Maira-2: Grounded radiology report genera-\ntion. arXiv preprint arXiv:2406.04449, 2024. 1, 2, 6, 7\n[5] Benedikt Boecking,\nNaoto Usuyama,\nShruthi Bannur,\nDaniel C Castro, Anton Schwaighofer, Stephanie Hyland,\nMaria Wetscherek, Tristan Naumann, Aditya Nori, Javier\nAlvarez-Valle, et al. Making the most of text semantics to\nimprove biomedical vision–language processing. In Euro-\npean conference on computer vision, pages 1–21. Springer,\n2022. 5\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213–229. Springer, 2020. 3\n[7] Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Mag-\ndalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya\nMaria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen,\nEduardo Pontes Reis, et al.\nChexagent: Towards a foun-\ndation model for chest x-ray interpretation. arXiv preprint\narXiv:2401.12208, 2024. 1, 2, 6, 7, 8\n[8] Daniel Coelho de Castro, Aurelia Bustos, Shruthi Ban-\nnur, Stephanie L Hyland, Kenza Bouzid, Maria Teodora\nWetscherek, Maria Dolores S´anchez-Valverde, Lara Jaques-\nP´erez, Lourdes P´erez-Rodr´ıguez, Kenji Takeda, et al.\nPadchest-gr: A bilingual chest x-ray dataset for grounded\nradiology report generation. NEJM AI, 2(7):AIdbp2401120,\n2025. 5\n[9] Jean-Benoit Delbrouck, Pierre Chambon, Christian Blueth-\ngen,\nEmily Tsai,\nOmar Almusa,\nand Curtis P Lan-\nglotz.\nImproving the factual correctness of radiology re-\nport generation with semantic rewards.\narXiv preprint\narXiv:2210.12186, 2022. 6, 7, 8\n[10] Nicolas Deperrois, Hidetoshi Matsuo, Samuel Ruip´erez-\nCampillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser,\nKoji Fujimoto, Mizuho Nishio, Thomas M Sutter, Ju-\nlia E Vogt, et al.\nRadvlm:\nA multitask conversa-\ntional vision-language model for radiology. arXiv preprint\narXiv:2502.03333, 2025. 1, 2, 6, 7\n[11] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao,\nand Hoifung Poon. Domain-specific language model pre-\ntraining for biomedical natural language processing. ACM\nTransactions on Computing for Healthcare (HEALTH), 3(1):\n1–23, 2021. 4\n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\nLora: Low-rank adaptation of large language models. ICLR,\n1(2):3, 2022. 5, 6\n[13] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-\nsoning and aligning for text-to-image person retrieval.\nIn\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 2787–2797, 2023. 4\n[14] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-ying\nDeng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-\nidentified publicly available database of chest radiographs\nwith free-text reports. Scientific data, 6(1):317, 2019. 5\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In Proceedings of the IEEE/CVF international confer-\nence on computer vision, pages 4015–4026, 2023. 2\n[16] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\nand Jianfeng Gao. Llava-med: Training a large language-\nand-vision assistant for biomedicine in one day. Advances\nin Neural Information Processing Systems, 36:28541–28564,\n2023. 2\n[17] Qingqiu Li, Zihang Cui, Seongsu Bae, Jilan Xu, Runtian\nYuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang,\nJunjun He, et al. Aor: Anatomical ontology-guided reason-\ning for medical large multimodal model in chest x-ray inter-\npretation. arXiv preprint arXiv:2505.02830, 2025. 2\n[18] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, pages\n74–81, 2004. 6, 7\n[19] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and\nXiao-Ming Wu. Slake: A semantically-labeled knowledge-\nenhanced dataset for medical visual question answering. In\n2021 IEEE 18th international symposium on biomedical\nimaging (ISBI), pages 1650–1654. IEEE, 2021. 6\n[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36:34892–34916, 2023. 1, 2\n[21] Jingping Liu, Ziyan Liu, Zhedong Cen, Yan Zhou, Yinan\nZou, Weiyan Zhang, Haiyun Jiang, and Tong Ruan.\nCan\nmultimodal large language models understand spatial rela-\ntions? arXiv preprint arXiv:2505.19015, 2025. 1\n[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[23] Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han,\nand Ting Chen.\nVividmed: Vision language model with\n"}, {"page": 10, "text": "versatile visual grounding for medicine.\narXiv preprint\narXiv:2410.12694, 2024. 1, 2\n[24] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiao-\njuan Qi. Groma: Localized visual tokenization for grounding\nmultimodal large language models. In European Conference\non Computer Vision, pages 417–435. Springer, 2024. 1\n[25] Philip M¨uller, Georgios Kaissis, and Daniel Rueckert. Chex:\nInteractive localization and region description in chest x-\nrays. In European Conference on Computer Vision, pages\n92–111. Springer, 2024. 2\n[26] Ha Q Nguyen, Khanh Lam, Linh T Le, Hieu H Pham, Dat Q\nTran, Dung B Nguyen, Dung D Le, Chi M Pham, Hang TT\nTong, Diep H Dinh, et al. Vindr-cxr: An open dataset of\nchest x-rays with radiologist’s annotations. Scientific Data,\n9(1):429, 2022. 5, 6\n[27] Chantal Pellegrini, Ege ¨Ozsoy, Benjamin Busam, Nassir\nNavab, and Matthias Keicher. Radialog instruct dataset. 2,\n6, 7\n[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 1\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PmLR, 2021. 4\n[30] Andrew Sellergren,\nSahar Kazemzadeh,\nTiam Jaroen-\nsri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, C´ıan Hughes, Charles Lau,\net al.\nMedgemma technical report.\narXiv preprint\narXiv:2507.05201, 2025. 2, 5, 7\n[31] Oriane Sim´eoni, Huy V Vo, Maximilian Seitzer, Federico\nBaldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov,\nMarc Szafraniec, Seungeun Yi, Micha¨el Ramamonjisoa,\net al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 8\n[32] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek,\nAndrew Y Ng, and Matthew P Lungren. Chexbert: com-\nbining automatic labelers and expert annotations for accu-\nrate radiology report labeling using bert.\narXiv preprint\narXiv:2004.09167, 2020. 6, 7, 8\n[33] Andrew Szot, Bogdan Mazoure, Harsh Agrawal, R Devon\nHjelm, Zsolt Kira, and Alexander Toshev. Grounding multi-\nmodal large language models in actions. Advances in Neural\nInformation Processing Systems, 37:20198–20224, 2024. 1\n[34] Daniel Wolf, Heiko Hillenhagen, Billurvan Taskin, Alex\nB¨auerle, Meinrad Beer, Michael G¨otz, and Timo Ropinski.\nYour other left! vision-language models fail to identify rel-\native positions in medical images. In International Confer-\nence on Medical Image Computing and Computer-Assisted\nIntervention, pages 691–701. Springer, 2025. 1, 3\n[35] Joy T Wu, Nkechinyere N Agu, Ismini Lourentzou, Arjun\nSharma, Joseph A Paguio, Jasper S Yao, Edward C Dee,\nWilliam Mitchell, Satyananda Kashyap, Andrea Giovannini,\net al. Chest imagenome dataset for clinical reasoning. arXiv\npreprint arXiv:2108.00316, 2021. 5, 6\n[36] Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tianqi Liu,\nZhipeng Li, Xin Liu, and Xiaoxuan Huang. A comprehen-\nsive survey of large language models and multimodal large\nlanguage models in medicine.\nInformation Fusion, page\n102888, 2024. 1\n[37] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. Bertscore: Evaluating text genera-\ntion with bert. arXiv preprint arXiv:1904.09675, 2019. 6, 7,\n8\n[38] Ke Zou, Yang Bai, Zhihao Chen, Yang Zhou, Yidi Chen,\nKai Ren, Meng Wang, Xuedong Yuan, Xiaojing Shen,\nand Huazhu Fu.\nMedrg:\nMedical report grounding\nwith multi-modal large language model.\narXiv preprint\narXiv:2404.06798, 2024. 2\n"}, {"page": 11, "text": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model\nfor Chest X-Ray Interpretation\nSupplementary Material\nMultimodal Prompt\nUser:\nYou are a professional radiologist. I will provide you with context containing likely features about different parts of the chest\nX-rays.\nImage:\n<image start> <image> ...\n<image> <image end>\nLikely findings:\n<emb><obj 0></emb> <box>(0,387),(670,1024)</box> Abdominal cavity shows enteric tube.\n<emb><obj 1></emb> <box>(300,118),(394,207)</box> Aortic arch structure is healthy.\n...\n<emb><obj N></emb> <box>(398,391),(517,518)</box> Right cardiophrenic sulcus is\nhealthy.\nTask: [TASK]\nModel Response: [MODEL RESPONSE]\nFigure S1. Multimodal prompt template used in LM. Colored tags (<emb> and <box>) denote special tokens corresponding to anatom-\nical object embeddings and bounding boxes, respectively. Each <obj i> token represents the embedding of the ith anatomical object,\nwhile <image> indicates image patch embeddings.\nS1. Self-Similarity Loss Matrix\nThe Contrastive Alignment stage of the Anatomy Perception Module (APM) utilizes the Self-Similarity matrix Sself to\nmodel fine grained semantic relations among anatomical descriptions. In this stage, a pretrained sentence encoder S provides\nindirect supervision by embedding textual inputs into a continuous semantic space that captures linguistic and clinical similar-\nities. Given a set of input sentences St, each sentence is encoded through S to obtain text embeddings SE ∈RN×768, where\neach row corresponds to the representation of one sentence in a 768-dimensional embedding space. To ensure consistency\nand comparability across representations, the embeddings are first normalized using ℓ2 normalization:\n¯SE =\nSE\n|SE|2\n(S1)\nThis normalization projects the embeddings onto a unit hypersphere, ensuring that they encode directional (semantic)\ndifferences rather than magnitude based variations. The normalized embeddings are then used to compute the Self-Similarity\nmatrix:\nSself = Softmax(¯SE) · Softmax(¯SE)T\n(S2)\nwhere Sself ∈RN×N encodes pairwise similarity scores between all sentences in St. The softmax operation (applied row\nwise) ensures these similarities are smooth and probabilistically interpretable.\nNext, to align anatomical and textual semantics, we compute a projected similarity matrix ˆKA between the projected\nanatomical features ˆOA and projected text embeddings ˆSA:\n"}, {"page": 12, "text": "K\nV\nQ\nEncoder\nDecoder\nSimilarity\nSearch\nBest matching Sentences\nImage Tokens\nAnatomy Tokens\nTraining\nFrozen\nOutput for       \nContrastive Retrieval\nFigure S2. APM architecture during inference, where the Contrastive Alignment’s components are replaced with vector database for the\nContrastive Retrieval. See Fig. 2 for training architecture.\nˆKA = Softmax\n ˆOAˆST\nA\nτ\n!\n(S3)\nwhere ˆKA ∈RN×N, and τ is the temperature coefficient (set to 0.01) that controls the sharpness of the similarity distribution.\nThe final contrastive alignment loss is defined as the averaged KL-divergence between the anatomical–textual similarity\nmatrix ˆKA and the self-similarity matrix Sself, computed in both row-wise and column-wise directions to enforce mutual\nconsistency:\nLCL = 1\n2KL\n\u0010\nˆKA, Sself\n\u0011\n+ 1\n2KL\n\u0010\nˆKT\nA, ST\nself\n\u0011\n(S4)\nThis formulation encourages ˆOA and ˆSA to maintain pairwise relationships that reflect the semantic structure captured\nin Sself. As a result, the APM preserves semantic coherence and clinical consistency across related sentences, capturing\noverlapping anatomical features rather than enforcing strict one-to-one alignments.\nS2. Vector Database\nDuring APM inference, we replace the Contrastive Alignment with the Contrastive Retrieval (see Fig. S2) to identify the se-\nmantically most similar sentences to the anatomical object tokens ˆOA. These retrieved sentences represent the most probable\nobservations for each anatomical object and thus provide important contextual information for downstream descriptive tasks\nin LM, as discussed in ablations.\nThe vector database, denoted as VDB, stores all unique sentences associated with each anatomical object from the valida-\ntion set of the Chest-ImaGenome dataset. For every object in an image, we construct a concise descriptive sentence using the\ncorresponding phrases and attributes given in the original dataset (example sentence: “Right lower lung shows pleural effu-\nsion and atelectasis”). To build VDB, we first compile the set of unique sentences for each anatomical object. Each sentence\nis then encoded using the sentence encoder S and the trained projection head PS, producing s-dimensional embeddings (see\nFig. 2 for PS). These embeddings, along with their corresponding sentences, are stored as key–value pairs in VDB, with a\ndistinct sub-database allocated to each anatomical object. Consequently, VDB comprises N independent sub-databases.\nThe compact nature of both the embeddings and the sentences ensures that VDB remains lightweight, enabling efficient\nretrieval at inference time. During inference, a similarity search is performed between each anatomical object token ˆOi\nA\n"}, {"page": 13, "text": "Table S1. Number of training, validation, and test samples for the nine datasets used in LM training and validation. The “Source” column\nindicates the original public dataset used directly or as the basis for dataset creation.\nDataset\nSource\nTest\nTrain\nVal\nMIMIC-VQA\nMIMIC-CXR-VQA\n5,497 101,963\n4,926\nRaDialog Instruct\nRaDialog Instruct\n799\n6,274\n822\nSLAKE\nSLAKE\n298\n1,175\n285\nAnatomy Grounding\nChest-ImaGenome\n3,403 237,938\n1,959\nMS-CXR\nMS-CXR\n528\n2,445\n507\nVinDr-Instruct\nVinDr-CXR\n6,166\n38,122\n4,099\nPadChest-Grounding\nPadChest-Grounding\n1,121\n3,920\n558\nMIMIC-CXR Classification MIMIC-CXR\n2,957 182,425\n1,666\nMIMIC Report Gen\nMIMIC-CXR\n1,722 135,049\n1,078\nTotal\n–\n22,491 709,311 15,900\nand the sentence embeddings within the corresponding sub-database of VDB, thereby retrieving the most relevant descriptive\nsentences for each object. These sentences are then passed to LM using a multimodal prompt template shown in Fig. S1,\nwhere they provide important contextual information about each anatomy.\nS3. Radiology Tasks\nAnatomiX is trained and evaluated on nine CXR-related tasks, spanning four categories: image understanding, grounding,\nreport generation, and visual question answering (VQA). Each of these tasks is focused on specific aspect of the CXR\ninterpretation and uses different dataset(s).\nS3.1. Image Understanding\nThis category includes multi-label image classification across 14 classes using the MIMIC-CXR dataset, as well as CXR\nabnormality detection leveraging the VinDr-Instruct dataset. Fig. S4 shows sample input-output samples for classification\nand abnormality detection tasks along with the output of our model.\nS3.2. Grounding\nWe include four challenging grounding tasks in this work, namely: Phrase Grounding, Grounded Diagnosis, Grounded\nCaptioning, and Anatomy Grounding. In Phrase Grounding, the model identifies the spatial location of a given phrase\nwithin an input image, utilizing the MS-CXR, PadChest-Gr, and VinDr-Instruct datasets. Grounded Diagnosis and Grounded\nCaptioning require the model to infer a diagnosis and generate a textual description for a specified image region, respectively;\nwe use VinDr-Instruct and MS-CXR for Grounded Diagnosis and MS-CXR for Grounded Captioning. Finally, Anatomy\nGrounding uses the Anatomy-Grounding dataset to localize anatomical structures based on user-provided textual prompts.\nFig. 5 shows sample input and output pairs for these tasks, along with the output of our model.\nS3.3. Report Generation\nThis task involves generating the full report, including both the findings and impression sections using MIMIC-CXR dataset.\nA sample image-report is shown in Fig. S3 along with the output of AnatomiX.\nS3.4. Visual Question Answering\nThe VQA category consists of open-ended and closed-ended question answering tasks, derived from a combination of the\nMIMIC-VQA, SLAKE, and Radialog-Instruct datasets.\n"}, {"page": 14, "text": "Findings:\nThe patient is status post median sternotomy and CABG. Left-sided AICD/pacemaker device is noted with leads\nterminating in the right atrium and right ventricle. Heart size remains mildly enlarged. Mediastinal and hilar\ncontours are unchanged. There is mild pulmonary vascular congestion. Patchy opacities in the lung bases may\nreflect atelectasis. No pleural effusion or pneumothorax is present. There are no acute osseous abnormalities.\nImpression:\nMild pulmonary vascular congestion and bibasilar atelectasis.\nWrite a comprehensive report:\nFindings:\nLeft-sided pacer is re- demonstrated with leads terminating in the right atrium and right ventricle. The patient is\nstatus post median sternotomy, aortic valve replacement, and CABG. Heart size is mildly enlarged, unchanged.\nMediastinal and hilar contours are similar. Mild upper zone pulmonary vascular redistribution is likely chronic\nwithout overt pulmonary edema. Lung volumes remain low with streaky opacities in the lung bases suggestive of\natelectasis. No large pleural effusion or pneumothorax is present. Fusion hardware within the lumbar spine is\npartially imaged as well as hardware within the right humeral head.\nImpression:\nChronic mild pulmonary vascular congestion without overt pulmonary edema. Bibasilar atelectasis.\nFigure S3. Sample report generation task. Color scheme: Blue color for user input prompt, red shows the report generated by AnatomiX,\nand green represents the ground truth report.\n Locate areas in the chest X-ray where\nabnormalities are present, using bounding box\ncoordinates\n<ref>Cardiomegaly</ref><box>(399,455),\n(835,605)</box><ref>Aortic enlargement</ref>\n<box>(504,252),(620,375)</box>\n<ref>Cardiomegaly</ref><box>(421,478),\n(788,588)</box><ref>Aortic enlargement</ref>\n<box>(454,280),(637,434)</box>\nGiven the CXR, identify any diseases. Options:\n(o) Lung Lesion\n(v) No Finding\n(p) Pleural Effusion\n(a) Support Devices\n(r) Edema\n(x) Pneumothorax\n(c) Atelectasis\n(n) Enlarged Cardiomediastinum\n(j) Pleural Other\n(k) Pneumonia\n(w) Lung Opacity\n(q) Fracture\n(u) Consolidation\n(l) Cardiomegaly\nThere may be more than one answer.\n(p) Pleural Effusion, (c) Atelectasis\n(p) Pleural Effusion, (c) Atelectasis\nImage Classification\nAbnormality Detection\nDoes the image show pneumothorax?\nNo, there is no evidence of that in the image.\nNo, there is not.\nClose-Ended VQA\nWhat are the key observations derived from\nthis chest x-ray?\nThe patient has lung opacity, atelectasis and\npleural effusion.\nSure, atelectasis and pleural effusion.\nOpen-Ended VQA\nFigure S4. Example input-output-ground truth for image understanding and visual question answering tasks. User input prompt is shown\nin blue color, red color represents AnatomiX’s output, while ground truth is shown in green.\n"}]}