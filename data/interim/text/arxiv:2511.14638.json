{"doc_id": "arxiv:2511.14638", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.14638.pdf", "meta": {"doc_id": "arxiv:2511.14638", "source": "arxiv", "arxiv_id": "2511.14638", "title": "A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases", "authors": ["Tao Yang", "Dandan Huang", "Yunting Lin", "Pengfei Wu", "Zhikun Wu", "Gangyuan Ma", "Yulan Lu", "Xinran Dong", "Dingpeng Li", "Junshuang Ge", "Zhiyan Zhang", "Xuanzhao Huang", "Wenyan Nong", "Yao Zhou", "Hui Tang", "Hongxi Yang", "Shijie Zhang", "Juan Li", "Xiaojun Cao", "Lin Yang", "Xia Gao", "Kaishou Xu", "Xiaoqiong Gu", "Wen Zhang", "Huimin Xia", "Li Liu", "Wenhao Zhou", "Mulin Jun Li"], "published": "2025-11-18T16:29:19Z", "updated": "2025-11-18T16:29:19Z", "summary": "Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.14638v1", "url_pdf": "https://arxiv.org/pdf/2511.14638.pdf", "meta_path": "data/raw/arxiv/meta/2511.14638.json", "sha256": "e2d0e30ea21f3ef8bee401530eed67c52a312a9c5e483bbfd823bea9ead25483", "status": "ok", "fetched_at": "2026-02-18T02:26:38.168717+00:00"}, "pages": [{"page": 1, "text": "A Specialized Large Language Model for Clinical Reasoning and \nDiagnosis in Rare Diseases \nTao Yang1,#, Dandan Huang2,#, Yunting Lin1,#, Pengfei Wu1, Zhikun Wu1, Gangyuan Ma3, Yulan Lu1, \nXinran Dong4, Dingpeng Li2, Junshuang Ge1, Zhiyan Zhang1, Xuanzhao Huang1, Wenyan Nong1, \nYao Zhou1, Hui Tang1, Hongxi Yang2, Shijie Zhang2, Juan Li1, Xiaojun Cao1, Lin Yang4, Xia Gao1, \nKaishou Xu1, Xiaoqiong Gu1, Wen Zhang1, Huimin Xia1, Li Liu1,*, Wenhao Zhou1,*, Mulin Jun Li1,* \n \n1Guangzhou Women and Children's Medical Center, Guangzhou Medical University, Guangzhou, China. \n2Center for Cardiovascular Diseases, The Province and Ministry Co-sponsored Collaborative Innovation \nCenter for Medical Epigenetics, School of Basic Medical Sciences, Tianjin Medical University, Tianjin, \nChina. \n3Guangzhou National Laboratory, Guangzhou, China. \n4Center for Molecular Medicine, Children's Hospital of Fudan University, Shanghai, China. \n \n#The authors contributed equally to this work  \n*Correspondence: \nmulinli@connect.hku.hk \n(M.J.L), \nzhouwenhao@fudan.edu.cn \n(W.H.Z), \nliliuchina@qq.com (L.L) \n \nAbstract: \nRare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. \nConvectional pipelines decouple noisy evidence extraction from downstream inferential \ndiagnosis, and general/medical large language models (LLMs) face scarce real-world electronic \nhealth records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, \ndomain-specialized clinical corpus and a clinician-validated reasoning set, and develop \nRareSeek-R1 via staged instruction tuning, chain-of-thought learning, and graph-grounded \nretrieval. Across multicenter EHR narratives and public benchmarks, RareSeek-R1 attains \nstate-of-the-art accuracy, robust generalization, and stability under noisy or overlapping \nphenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized \nvariants by resolving ambiguity and aligning candidates to mechanisms. Human studies show \n"}, {"page": 2, "text": "performance on par with experienced physicians and consistent gains in assistive use. Notably, \ntransparent reasoning highlights decisive non-phenotypic evidence (median 23.1%, such as \nimaging, interventions, functional tests) underpinning many correct diagnoses. This work \nadvances a narrative-first, knowledge-integrated reasoning paradigm that shortens the \ndiagnostic odyssey and enables auditable, clinically translatable decision support. \n \n \n"}, {"page": 3, "text": "Introduction \nRare diseases, typically defined by a prevalence below 1/20001,2, now encompass well \nover 10,000 distinct disorders and affect hundreds of millions worldwide, yet patients \ncommonly endure a prolonged “diagnostic odyssey,” with the average time for an accurate \ndiagnosis is 4-8 years3. Contemporary differential diagnostics seek to integrate longitudinal \nclinical evidence, for example standardized phenotypes extracted from highly heterogeneous \nelectronic health records (EHRs)4 and clinically pathogenic variants/genes prioritized by \nhigh‑throughput family‑based sequencing5,6, thereby incrementally improving diagnostic yield. \nIn practice, achieving efficient and accurate clinical diagnosis still hinges on two tightly \ncoupled steps: (1) faithful acquisition and standardization of complete clinical evidence, \nespecially phenotype terms formulated as Human Phenotype Ontology (HPO)7 and molecular \ndiagnostic reports (such as pathogenic/likely pathogenic variants documented in ClinVar8), and \n(2) rigorous diagnostic reasoning grounded in the patient’s causal evidence chain, culminating \nin clinical decision-making supported by computational algorithms9. However, nearly all \nstate‑of‑the‑art clinical workflows and computational decision‑support strategies operationalize \nthese two challenging tasks in isolation, treating key phenotypes/variants extraction as a \npre‑processing task and downstream inference as a largely separate ranking exercise10. This \nstructural decoupling injects biases and uncertainty at each step, leaving the diagnostic odyssey \nlargely unabated. \nLarge language models (LLMs) are advancing rapidly in medicine11,12. General models, \nincluding GPT series13, LLaMA14, PaLM15, and DeepSeek16, increasingly support clinical tasks. \nBuilding on these, specialized medical LLMs have emerged, such as AMIE17, MedPaLM-218, \nMedFound19, Baichuan-M220, MMedLM21, Meditron22 and Clinical Camel23. Collectively, \nthese systems demonstrate strong performance in medical QA and decision support, \nhighlighting the promise of domain adapted LLMs for differential diagnostics of rare disease. \nPioneer attempts to apply LLMs to rare disease diagnosis show mixed, often contradictory \nresults. Some studies show GPT‑4/4o outperforming general baselines and, in limited settings, \neven surpassing existing phenotype‑driven pipelines24. Other studies report that general LLMs, \nsuch as OpenAI o1 and GPT-4o, underperform bioinformatic tools like Exomiser when limited \nto extracted HPO terms25. Agentic hybrids like DeepRare yield modest gains yet remain \n"}, {"page": 4, "text": "dependent on accurate phenotype extraction and normalization26. These inconsistencies leave \nan open question on whether LLMs can truly surpass current strategies for rare disease \ndifferential diagnosis. \nTraining domain‑specialized LLMs for rare diseases faces intertwined data and technical \nchallenges. First, data scarcity and distributional mismatch limit both training and evaluation. \nAvailable clinical corpora are relatively small, fragmented, and heterogeneously labeled, while \npublic benchmarks skew toward structured phenotypes rather than full clinical narratives, \nwhich inflates performance on curated inputs and undermines external validity in real clinics24. \nSecond, phenotype‑centric pipelines compress clinical evidence and lose diagnostic signal. \nPhenotype extraction and HPO normalization fail to capture non‑phenotypic cues relevant to \nrare disease inference, such as pathology and histology results, environmental and lifestyle \nfactors, exposure history, and clinical interventions27. Moreover, well‑annotated HPO‑mapped \nresources such as Phenopacket28 and RareBench24 are predominantly retrospective, \nground‑truth datasets that do not reflect the difficulty, incompleteness, and noise of evidence \nacquisition during real‑time diagnosis. Third, LLMs may hallucinate findings and struggle with \ncausal and temporal reasoning in the absence of latest variant-gene-phenotype-disease \nrelationships24,29, especially when evidence is incomplete. Together these challenges and \nremedies chart a course beyond current best clinical practices toward reliable and scalable rare \ndisease diagnosis with LLMs. \nIn this study, we systematically assembled three rare disease-focused resources, including \na large‑scale clinical corpus (RareMed‑Corpus), a clinician‑validated diagnostic reasoning \ndataset (RareMed‑CoT), and a graph‑grounded retrieval resource (RareMed‑RAG). Building \non this foundation, we developed RareSeek‑R1, a domain‑specialized LLM tailored for rare \ndisease diagnostic reasoning. The model employs a Progressive Parameter‑Efficient Transfer \nLearning strategy in three stages30. First, domain‑specific instruction tuning on the \nDeepSeek‑R1 injects rare disease knowledge while preserving general linguistic competence. \nSecond, chain‑of‑thought (CoT) fine‑tuning strengthens multi‑step clinical reasoning and \nintegration \nof \nheterogeneous \nevidence. \nThird, \nintegrative \nreasoning \ncouples \nretrieval‑augmented generation (RAG) with a curated rare disease knowledge graph (KG), \naligning variant-gene-phenotype-gene-disease relations to improve factual fidelity, calibration, \n"}, {"page": 5, "text": "and interpretability. We then benchmarked RareSeek-R1 against leading LLMs and \nstate-of-the-art phenotype-driven tools across five public and multi-center datasets, forming the \nlargest evaluation to date in rare disease diagnosis. RareSeek-R1 consistently delivered superior \naccuracy and remained robust under noisy or atypical phenotypes. Human-AI comparative \nstudies and AI-assisted trials in real clinical workflows confirmed its applicability and \nreliability for decision support. Finally, through an LLM clinical readiness assessment and \ninterpretability analyses, we characterized RareSeek‑R1’s reasoning process and rationality, \nelucidated its internal inference mechanisms, and identified potential missing elements in \nconventional rare disease differential diagnostics pipelines. \n \nResults \nLarge-scale multidimensional rare disease knowledgebase integration and RareSeek-R1 \nmodel development \nRareSeek-R1 was trained through a systematic, three-stage pipeline that integrates \nlarge-scale data collection, standardized curation, and graph-grounded digitization (Fig. 1a). In \nstage one, domain-specific instruction tuning on RareMed-Corpus (149,341 independent rare \ndisease-oriented texts, ~500M tokens) injected consolidated rare disease knowledge while \npreserving general language capability endowed by the DeepSeek‑R1‑70B base model. The \nrare disease-focused corpus comprised 48,852 de-identified, definitively diagnosed EHRs, \n35,722 medical texts and guidelines, 30,101 PubMed case reports, and 34,666 synthetic cases, \nensuring diversity and clinical relevance. In stage two, CoT fine‑tuning on RareMed‑CoT, \nwhich contains 17,477 high‑quality reasoning chains, strengthened multi‑step reasoning and \nevidence integration. This training shifted the model from outcome‑only prediction to \nprocess‑explainable diagnosis and improved robustness and interpretability. In stage three, \nGraphRAG (Graph Retrieval-Augmented Generation) ‑based integrative reasoning reduced \nhallucinations and aligned inference with up‑to‑date biomedical knowledge by coupling \nretrieval‑augmented generation to a curated rare disease KG. RareMed‑RAG harmonizes \ndefinitions and relations from commonly-used resources, such as ClinVar8, HGMD31, HPO7, \nOMIM32 and Orphanet33, enabling precise traversal of variant-gene-phenotype-disease links \n(see details in Methods). \n"}, {"page": 6, "text": "To enable a systematic evaluation of RareSeek‑R1, we assembled a unified suite of five \nhigh‑quality, digitized benchmarks, the largest rare disease diagnostic cohort to date (Fig. 1b). \nThese include: 1) EHR‑Internal includes 4,306 de‑identified, definitively diagnosed clinical \nnarratives from the same institutions as the training data, supporting internal validation on \nmatched settings. 2) EHR‑External includes 283 multi‑center cases from independent hospitals, \nenabling out‑of‑distribution testing across institutions. 3) RareBench includes 1,197 public \nphenotype‑only cases drawn from sources such as MME34, LIRICAL35, HMS36, RAMEDIS37, \nand PUMCH_ADM24; cases are standardized to HPO terms with OMIM/Orphanet diagnoses, \nassessing generalization on curated phenotypic profiles. 4) MedEHR‑Variant includes 147 \ncases pairing full EHRs with whole‑exome sequencing (WES), enabling joint clinical-genetic \ninference and quantifying the incremental value of genomic data and structured knowledge \naugmentation. 5) Phenopacket‑Store includes 5,213 GA4GH‑conformant, case‑level records \nfollowing the Phenopacket schema, with harmonized HPO phenotypes, variant annotations, and \nreference diagnoses, further supporting evaluation of phenotype‑ and genotype‑aware pipelines \n(see details in Methods).  \nBy leveraging above large and high-quality benchmarks, we evaluated RareSeek‑R1 \nagainst five general LLMs (LLaMA 3.3-70B38, GPT-4o13, GPT-539, OpenAI o140,41 and QwQ-\n32B42), six medical LLMs (including Meditron-70B22, MMedLM-70B21, Clinical Camel-70B23, \nHuatuoGPT-o1-70B43, Baichuan-M220 and Baichuan-M144), and five phenotype‑driven tools \n(including Exomiser5, PhenoDP45, Phen2Disease46, Base_IC46 and Phrank47) across differential \ndiagnostic accuracy, sensitivity to phenotypic noise/complexity, sequential clinical evidence \nvalue, gains from GraphRAG and genomic data, human-AI comparative performance, and \ninterpretability/clinical readiness (Fig. 1c). \n"}, {"page": 7, "text": " \nFig. 1: Schematic illustration of the development and evaluation of the RareSeek-R1 \nframework for rare disease diagnosis. (a) Model development. RareSeek-R1 was built upon \nDeepSeek-R1-Distill-LLaMA-70B through domain-specific instruction tuning on RareMed-\nCorpus, followed by chain-of-thought fine-tuning on the RareMed-CoT dataset to improve \ndiagnostic reasoning in rare diseases. The model further incorporated GraphRAG-based \nintegrative reasoning grounded in a curated rare disease knowledge graph to ensure factual \nconsistency and biomedical alignment. (b) Benchmark datasets. Comprehensive multi-source \nbenchmark datasets were established to evaluate diagnostic performance across diverse case \ndistributions, data origins, disease categories, and genetic annotation statuses. (c) Clinical \nevaluation of the AI system. The model was systematically evaluated for diagnostic accuracy, \n               \n                  \n                           \n                             \n                      \n       \n          \n            \n           \n            \n         \n            \n                \n            \n                                      \n                                 \n                                    \n                  \n \n                \n              \n                                              \n                    \n             \n         \n                       \n              \n               \n              \n       \n    \n        \n   \n    \n          \n                                           \n                          \n                      \n                  \n        \n            \n     \n       \n       \n         \n    \n        \n    \n        \n      \n   \n       \n   \n       \n   \n    \n   \n  \n   \n  \n   \n  \n   \n    \n     \n   \n   \n  \n  \n  \n   \n  \n  \n  \n        \n         \n        \n        \n      \n       \n        \n      \n       \n         \n        \n        \n      \n         \n          \n          \n        \n      \n       \n        \n      \n       \n      \n         \n   \n         \n         \n         \n         \n         \n   \n   \n    \n   \n  \n   \n   \n   \n   \n   \n  \n  \n      \n   \n   \n  \n  \n  \n  \n  \n  \n  \n               \n \n \n \n       \n                \n      \n               \n             \n             \n                   \n                          \n                                      \n                                  \n            \n                                             \n                 \n               \n                                 \n  \n  \n         \n  \n                 \n          \n \n                          \n       \n           \n                            \n                             \n \n         \n     \n                                              \n    \n   \n                                     \n       \n       \n        \n           \n      \n       \n         \n           \n     \n         \n         \n     \n                   \n"}, {"page": 8, "text": "robustness to phenotypic variability, reasoning with sequential clinical evidence, benefits from \nGraphRAG and genomic information, human-AI performance, and clinical interpretability. \n \nComprehensive evaluation of rare disease differential diagnosis across LLMs and \nconventional methods \nTo ensure fair and reproducible assessment, we implemented a unified evaluation protocol. \nFor each case, models generated up to 20 candidate diagnoses, subsequently normalized to \nOrphanet identifiers via the MONDO API48. Accuracy was computed at multiple rank cutoffs \nand at two levels: (i) exact, when the predicted Orphanet code matched the reference; and (ii) \nhierarchical, when the prediction mapped to a parent concept of the reference diagnosis. Given \nthe clinical actionability of higher‑level categories in rare disease workflows, hierarchical \nmatches were treated as correct in secondary analyses. This protocol was applied uniformly \nacross all models and baselines (see details in Methods). Across EHR‑Internal, EHR‑External, \nand RareBench benchmarks emphasizing full EHR narratives or core phenotypes, \nDeepSeek‑R1 achieved the strongest overall diagnostic performance among foundation models. \nThis superior performance motivated our selection of DeepSeek‑R1 as the base model for \ndeveloping RareSeek‑R1. \nWe first evaluated RareSeek‑R1 against multiple baselines and competing tools on the \nEHR‑Internal test set. RareSeek-R1 demonstrated the best overall diagnostic performance \nacross all methods, with a Top-1 accuracy of 0.684 ± 0.014, significantly outperforming both \ngeneral LLMs and phenotype-based diagnostic tools (Fig. 2a). Generally, Reasoning-enhanced \nLLMs outperformed general-purpose models, OpenAI o1 achieved the highest accuracy among \nnon-specialized models (0.487 ± 0.016), followed by Baichuan-M2 (0.438 ± 0.017). \nComparable trends were observed under hierarchical evaluation. Across 28 Orphanet-defined \ndisease categories (noting that a single disease may map to multiple categories), RareSeek-R1 \ndemonstrated consistently balanced performance, achieving > 0.80 accuracy in rare neoplastic \nor tumoral, rheumatologic, and urogenital diseases, but lower accuracy in rare immune (0.286), \nmetabolic (0.498), and infertility disorders (0.477), delineating its current diagnostic boundaries. \nBecause conventional rare disease diagnostic tools largely rely on phenotype extraction \nfrom clinical evidence, particularly EHRs, we applied three established extractors \n"}, {"page": 9, "text": "(PhenoTagger49, PhenoBERT4, ClinPhen50) for HPO normalization and compared their \ndownstream differential diagnostic performance. Using PhenoTagger for HPO extraction \nyielded the highest accuracy across five downstream phenotype‑based diagnostic methods \n(Exomiser, PhenoDP, Phen2Disease, Base_IC, Phrank) under both exact and hierarchical \ncriteria. Accordingly, we used PhenoTagger to extract HPO terms for all subsequent analyses. \nAmong conventional phenotype-driven approaches, Exomiser showed the highest accuracy \n(Top-1 = 0.057 ± 0.008; Top-3 = 0.138 ± 0.006), while PhenoDP and Phen2Disease performed \nsimilarly or worse, indicating that these approaches struggle without expert-curated HPOs and \nlack robust direct EHR parsing. To quantify the value of clinical context, we re-evaluated \nRareSeek-R1 on EHR-Internal using phenotype-only input. Compared with a Top-1 accuracy \nof 0.684 ± 0.014 using full text, phenotype-only input achieved a markedly lower Top-1 \naccuracy of 0.192 ± 0.012 (Fig. 2b). This gap suggests routine EHRs contain clinically decisive \nsignals that automated HPO pipelines often miss or cannot map (e.g., personal genetic findings, \ndisease trajectory, treatment response, longitudinal investigations, family history, imaging and \nlaboratory evidence, and therapeutic exposures). \nTo evaluate robustness across heterogeneous data sources, RareSeek-R1 was further tested \non the EHR-External dataset comprising multi-center real-world cases. RareSeek-R1 \nconsistently outperformed all baselines, achieving a Top-1 accuracy of 0.719 ± 0.025, \ndemonstrating strong generalization beyond the training distribution (Fig. 2c). In comparison, \nGPT-5 achieved 0.578 ± 0.004, while phenotype-based tools such as Exomiser showed limited \nperformance (Top-1 accuracy of 0.046 ± 0.035). Across disease categories, RareSeek-R1 \nreached > 0.80 accuracy in rare bone, developmental defect, genetic, and ophthalmic disorder. \nOn the independent RareBench24 dataset curated around core phenotypes rather than full \nnarratives, RareSeek-R1 achieved 0.392 ± 0.025 Top-1 accuracy (Fig. 2d), surpassing GPT-5 \n(0.353 ± 0.049). Among phenotype-based methods, Exomiser performed best (Top-1 = 0.232 ± \n0.016), surpassing several medical LLMs (e.g., Meditron-70B, MMedLM-70B, Clinical \nCamel-70B). Notably, its Top-10 accuracy (0.575 ± 0.022) was comparable to RareSeek-R1 \n(0.577 ± 0.027). Across disease categories, RareSeek-R1 achieved >0.65 accuracy in rare \ngynecologic or obstetric, infertility, cardiac surgical, and urogenital diseases. Because most \nRareBench cases contain only key phenotypic information, this benchmark, unlike \n"}, {"page": 10, "text": "EHR‑Internal and EHR‑External which emphasize comprehensive clinical narratives, further \nillustrates RareSeek‑R1’s stable performance across heterogeneous data regimes. \n \nOur training framework adopts a Progressive Parameter‑Efficient Transfer Learning \nparadigm, enabling stepwise knowledge infusion and targeted adaptation for rare disease \ndiagnosis. To determine whether domain‑specific instruction tuning effectively injects rare \ndisease knowledge and whether CoT‑based integration of heterogeneous evidence further \nenhances LLM diagnostic reasoning, we conducted component‑wise ablations by \nsystematically fine‑tuning publicly available general and medical LLMs. As shown in Fig. 2e, \nall models demonstrated consistent performance gains across datasets after progressive fine-\ntuning. Following the rare disease knowledge infusion stage, the models achieved average \nimprovements in diagnostic accuracy of 11.8%, 9.6%, and 4.4% on EHR-Internal, EHR-\nExternal, and RareBench, respectively. Building upon these gains, CoT fine-tuning, designed \nto enhance reasoning coherence and diagnostic reliability, yielded an additional average gain \nof 9.5%, 7.9%, and 3.9% across the three datasets. In summary, these results demonstrate that \nour stepwise training framework markedly strengthens LLM reasoning robustness and cross-\ndomain generalizability in rare disease diagnosis. \n"}, {"page": 11, "text": " \nFig. 2: Performance analysis of rare disease diagnostic methods and LLM training \nstrategy components across datasets. (a) Comparative diagnostic performance of rare disease \nmethods on EHR-Internal. (b) Diagnostic performance of LLMs comparing diagnoses based \non phenotype only and full EHR narratives (A: Meditron-70B; B: MMedLM-70B; C: LLaMA \n3.3-70B; D: Clinical Camel-70B; E: HuatuoGPT-o1-70B; F: Baichuan-M2; G: GPT-4o; H: \nOpenAI o1; I: GPT-5; J: RareSeek-R1). (c, d) Comparative diagnostic performance of rare \ndisease methods on EHR-External (c) and RareBench (d). (e) Contribution of LLM training \ncomponents to diagnostic performance across datasets (left: EHR-Internal; middle: EHR-\nExternal; right: RareBench). The short horizontal line represents the mean performance across \nmodels. Percentage increases indicate improvements achieved through knowledge infusion \nfine-tuning and CoT fine-tuning. \n \nImpact of phenotypic noise and complexity on model performance \n \n \n \n \n \n \n \n                     \n"}, {"page": 12, "text": "In the aforementioned analysis, we found that when all EHR-extracted phenotypes were \nused, RareSeek-R1 and most LLMs sustained strong differential diagnostic performance on \nboth routine EHR narratives and phenotype-dominant cases, whereas traditional phenotype-\ndriven tools such as Exomiser were competitive with some general and medical LLMs only \nwhen phenotypic signals were highly specific and well curated, as in RareBench (Fig. 2d). This \nobservation prompted two questions: whether conventional phenotype extraction and \ndiagnostic pipelines lack the capacity to automatically process complex and heterogeneous \nclinical narratives due to limits in phenotype granularity, ontology coverage, and \ncontextualization, and whether full clinical narratives contain additional unmapped phenotypes \nor non-HPO signals that materially facilitate correct diagnosis. To investigate these issues, we \nquantified within clinical EHRs the proportion of key phenotypes, defined as features that \nexactly match HPO terms for the confirmed diagnosis, and evaluated their impact on \ndifferential diagnostic accuracy. Given its size and disease diversity, all subsequent phenotype-\nbased analyses were conducted on EHR-Internal (n = 4,306). \nUsing Human Phenotype Ontology Annotation (HPO‑A), we defined key phenotypes as \nEHR‑extracted features that exactly matched HPO‑A annotations for the confirmed diagnosis. \nCases were stratified by the per‑patient count of key phenotypes into three tiers: Few (0-1, N = \n2,368), Moderate (2-4, N = 1,675), and Rich (>4, N = 263). The Top‑5 diagnostic accuracy of \nLLMs (e.g., RareSeek‑R1, Baichuan‑M2, OpenAI o1) increased with the number of key \nphenotypes, whereas traditional phenotype‑driven methods (e.g., Exomiser) declined slightly, \nattaining only 0.145, 0.134, and 0.134 across tiers, versus RareSeek‑R1 at 0.787, 0.819, and \n0.840 (Fig. 3a). Notably, the proportion of key phenotypes among all extracted phenotypes \ndecreased as tier increased (Few: 0.312 ± 0.011; Moderate: 0.235 ± 0.011; Rich: 0.136 ± 0.009; \nFig. 3b), indicating that noise or unmapped phenotypes are pervasive in real‑world EHRs and \nmay constitute a larger share in records that also contain more key phenotypes. \nTo assess RareSeek‑R1’s differential diagnostic capability under well‑annotated, \nHPO‑mapped conditions, we re‑evaluated performance on EHR‑Internal using only key \nphenotypes. Under this setting, Exomiser’s Top-5 accuracy improved markedly to 0.127, 0.369, \nand 0.427 across the Few, Moderate, and Rich tiers, respectively (Fig. 3c). This improvement \nindicates that Exomiser’s diagnostic performance is highly sensitive to phenotype quality, with \n"}, {"page": 13, "text": "noisy or imprecise HPO terms substantially diminishing its accuracy. By comparison, \nRareSeek‑R1 achieved Top‑5 accuracies of 0.401, 0.721, and 0.734 using only key phenotypes, \nstill largely outperforming phenotype‑driven methods, yet remaining below its performance \nwith full EHR narratives (Fig. 3d). These findings indicate that RareSeek‑R1 perform well \nwhen restricted to key HPO‑mapped features, but they derive additional gains from the richer \nclinical context present in unstructured EHR text. \nReal‑world EHRs for rare disease patients frequently contain low‑information phenotypes \nthat recur across many conditions and have limited discriminative value. We defined cases with \nhighly overlapping or low‑information features as complex‑phenotype cases and constructed \ntwo cohorts, including the 300 cases with the lowest mean information content (Mean IC) and \nthe 300 with the highest composite difficulty scores (see details in Methods). RareSeek‑R1 \nachieved the highest accuracy in both cohorts (Top‑1 = 0.723 and 0.520; Fig. 3e). Under the \nMean IC criterion, Baichuan‑M2 ranked second (Top‑1 = 0.510); under the composite difficulty \ncriterion, OpenAI o1 ranked second (Top‑1 = 0.423). These findings delineate graded \ndiagnostic complexity and underscore the robustness and generalizability of LLMs to \noverlapping, ambiguous phenotypes in real‑world settings. Taken together, conventional \nphenotype extraction/normalization and phenotype‑driven tools falter on complex, \nheterogeneous narratives. By reasoning directly over raw EHR text and leveraging stepwise \nrare disease knowledge infusion with CoT‑based integrative reasoning, RareSeek‑R1 achieves \nstate‑of‑the‑art performance across diverse benchmarks. \n \nSequential effect and importance of clinical evidence categories for rare disease diagnosis \nIn rare disease diagnosis, clinical evidence is accumulated sequentially rather than \nobtained in a single encounter, typically beginning with the chief complaint, followed by the \nhistory of present illness and family history, then proceeds through multiple rounds of physical \nexamination, specialty assessments, molecular testing, and ancillary investigations. As \ninformation accrues, diagnostic judgments are progressively refined. To quantify the \ncontribution of each EHR category to model performance of RareSeek-R1, we conducted an \nIncremental Information Addition Analysis and single‑field ablations. In the incremental \nanalysis, using the chief complaint as baseline, Top‑1 accuracy was 0.475 ± 0.012; sequentially \n"}, {"page": 14, "text": "adding history of present illness, family history, physical examination, specialty assessments, \nand ancillary tests yielded Top‑1 accuracies of 0.561 ± 0.009, 0.520 ± 0.015, 0.543 ± 0.020, \n0.523 ± 0.009, and 0.684 ± 0.014, respectively (left panel of Fig. 3f). In the ablation experiments, \nstarting from the complete EHR (Top‑1 = 0.684 ± 0.014), removing individual fields produced \nthe largest drops when omitting the chief complaint or ancillary examinations (0.463 ± 0.015 \nand 0.523 ± 0.017), followed by history of present illness, family history, physical examination, \nand specialist examination (0.552 ± 0.015, 0.567 ± 0.017, 0.579 ± 0.014, and 0.578 ± 0.017; \nright panel of Fig. 3f). These results indicate that the chief complaint, present illness, and \nancillary tests are the most critical clinical categories for rare disease diagnosis, whereas family \nhistory, physical examination, and specialty assessments provide complementary value. \nOverall, the superior performance of complete EHR-based diagnosis reflects not only the \nsequential accrual of clinical evidence but also the synergistic contribution of heterogeneous \nsignals that RareSeek‑R1 can exploit. Crucially, richer early‑visit information enables earlier, \nmore confident triage and differential diagnosis, helping to shorten the prolonged diagnostic \nodyssey in rare diseases. Moreover, key attributes, such as temporal course, demographics, \ntreatment response, longitudinal workups, family history, exposures, and detailed tests, are \noften absent from or poorly captured by standardized HPO representations, limiting \nphenotype‑driven tools compared with models reasoning directly over full EHR narratives. \n"}, {"page": 15, "text": " \nFig. 3: Performance analysis under phenotypic noise, complexity, and sequential clinical \nevidence in rare disease diagnosis. (a) Diagnostic performance of different methods across \nthe Few (left), Moderate (middle), and Rich (right) key phenotype strata. (b) Proportion of key \nversus noise phenotypes across Few, Moderate, and Rich key-phenotype groups. (c-d) \nComparison of diagnostic performance between Exomiser (left) and RareSeek-R1 (right) under \nFew, Moderate, and Rich key-phenotype strata using all and key phenotypes. (e) Diagnostic \nperformance comparison for the 300 cases with the lowest Mean IC (left) and the 300 complex \ncases with the highest composite difficulty scores (right). (f) Effects of sequential addition of \ninformation on diagnostic accuracy (left) and contributions of individual clinical evidence \n \n \n \n \n \n \n \n                            \n"}, {"page": 16, "text": "categories evaluated through single-field ablation experiments (right). \n \nEnhancing diagnostic accuracy through integration of augmented retrieval and genetic \ninformation \nTo mitigate the limitations of LLMs that rely solely on parametric knowledge and to \nminimize hallucinations, we integrated a graph‑based RAG framework. GraphRAG injects \nstructured variant-gene-phenotype-disease relations, harmonized from widely used, up‑to‑date \nresources including ClinVar8, HGMD31, HPO7, OMIM32, and Orphanet 33, directly into the \nmodel’s reasoning pipeline (see details in Methods). By supplying precise, contextually \nrelevant knowledge, GraphRAG enables faithful traversal of biomedical links, reduces retrieval \nnoise, and improves diagnostic accuracy in rare disease cases. Beyond HPO‑based phenotypes, \ngenetic findings and putative pathogenic variants constitute core clinical evidence in \ncontemporary rare disease workflows. As whole‑genome sequencing (WGS), long‑read \nsequencing, and ongoing discovery of disease genes/variants expand the knowledge landscape, \ntimely graph updates further amplify GraphRAG’s contribution to LLM reasoning. Accordingly, \nwe evaluated whether integrating GraphRAG into RareSeek‑R1 enhances diagnostic \nperformance in cases with complete clinical evidence, encompassing both comprehensive \nphenotypic profiles and prioritized genetic variant data (see details in Methods). \nOn the MedEHR‑Variant benchmark, which comprises 147 cases pairing full EHRs with \nprioritized putative pathogenic variants from trio whole‑exome sequencing, we evaluated the \nadded value of GraphRAG and the integration of genomic evidence. Using Exomiser as a \nreference, phenotype‑only Top‑1 accuracy was 0.129 ± 0.029 and increased to 0.408 ± 0.083 \nafter incorporating variants, confirming the incremental utility of genetic data. We then \nquantified GraphRAG’s contribution within RareSeek‑R1 across input modalities. With \nEHR‑only input, Top‑1 accuracy rose from 0.312 ± 0.060 to 0.332 ± 0.085. With variant‑only \ninput, the gain was larger, from 0.477 ± 0.070 to 0.557 ± 0.084. The greatest improvement \noccurred when phenotypic and genomic evidence were combined, with Top‑1 accuracy \nincreasing from 0.575 ± 0.057 to 0.770 ± 0.055, underscoring both the incremental benefit of \nGraphRAG and the synergistic boost from integrating phenotypes with variants (Fig. 4a). \nOn the Phenopacket‑Store benchmark28, which includes 5,213 GA4GH conformant case \n"}, {"page": 17, "text": "level records with harmonized HPO annotations, variant metadata, and reference diagnoses, \nenabling evaluation of phenotype and genotype aware pipelines. Because raw VCFs were \nunavailable, Exomiser was assessed under a phenotype only baseline with Top-1 accuracy of \n0.270 ± 0.011. RareSeek-R1 without GraphRAG reached 0.210 ± 0.007, whereas GraphRAG \naugmentation improved performance to 0.414 ± 0.014, indicating that structured retrieval and \nranking enhance phenotype driven diagnosis. With variant only input, GraphRAG likewise \nraised accuracy from 0.479 ± 0.018 to 0.615 ± 0.019. When both phenotypic and genetic \nevidence were combined, RareSeek-R1 achieved its highest accuracy, increasing from 0.550 ± \n0.016 to 0.734 ± 0.010, with Top-10 reaching 0.910 ± 0.007 after GraphRAG (Fig. 4b). \nCollectively, integrating GraphRAG into RareSeek-R1 delivers clear gains, with the largest lift \nwhen full EHRs or standardized phenotypes are paired with prioritized variant evidence. We \npropose that graph-grounded retrieval injects explicit, current variant-gene-phenotype-disease \nlinks into the reasoning process, reducing hallucinations, improving factual calibration, and \npreserving knowledge currency. The structured context resolves overlapping phenotypes, aligns \ncandidates with underlying genetic mechanisms, and deprioritizes spurious matches typical of \nphenotype-only inputs. Empirically, GraphRAG increases accuracy across input settings, with \nthe largest gains in combined EHR and variant scenarios where clinical narratives and genomic \nsignals act synergistically. \n \nHuman-AI comparative evaluation and LLM diagnostic capability framework \nTo evaluate RareSeek‑R1’s contribution to real‑world clinical decision‑making, we \nconducted a human-AI comparative evaluation focused on rare neurologic and congenital \nmetabolic disorders. These two disease categories were selected because they encompass \ndiverse clinical manifestations and diagnostic modalities, and are supported by well-established \nmultidisciplinary care systems, thereby enabling rigorous and reproducible evaluation of \ndiagnostic performance. Participating physicians were stratified by clinical seniority into three \ncohorts (including Junior (n = 3), Middle (n = 3), and Senior (n = 3)), and assessed on 12 \nrepresentative diseases among neurologic and congenital metabolic disorders using 110 de-\nidentified EHRs. All clinicians independently reviewed the full records with diagnostic labels \nremoved and produced ranked differential diagnoses under identical conditions, without \n"}, {"page": 18, "text": "external tools or references (see details in Methods). Under these standardized conditions, \nRareSeek‑R1 achieved a Top‑1 accuracy of 0.473 (Fig. 4c), outperforming the Junior group and \naligning with the Middle group, thereby situating the model within the range of mid‑level \nclinical performance for these task domains. When augmented GraphRAG, the model’s Top‑1 \naccuracy rose to 0.582, approaching Senior‑level performance (Fig. 4c). These findings indicate \nthat RareSeek‑R1 can serve as a reliable decision‑support system across clinician experience \nlevels, and that GraphRAG further enhances diagnostic calibration and reliability by curbing \nhallucinations and aligning inferences with structured biomedical knowledge. \nWe further quantified the assistive value of RareSeek‑R1 with GraphRAG in clinician \nworkflows. Two weeks after the initial unaided round, nine physicians re‑evaluated the same \nEHRs (110 cases), this time informed by the model’s GraphRAG‑augmented reasoning outputs. \nTop‑1 accuracy increased consistently across experience tiers: Junior rose from 0.376 to 0.545 \n(Δ = 0.169), Middle from 0.479 to 0.597 (Δ = 0.118), and Senior from 0.576 to 0.670 (Δ = 0.094) \n(Fig. 4c). Improvements reflected tighter evidence integration and better‑calibrated differentials. \nFor example, a case first labeled Angelman syndrome was corrected to Prader-Willi syndrome \nafter the AI highlighted rapid weight gain with abdominal obesity, absence of familial obesity \nor diabetes, and hypotonia, consistent with the reference. Overall, GraphRAG‑enhanced \nRareSeek‑R1 markedly improves clinician performance across seniority, with the largest gains \nin Junior physicians, while providing traceable rationales that enable targeted diagnostic \nrevision. \nConventional metrics such as accuracy and text scores like BLEU51 or ROUGE52 \ninsufficiently reflect the clinical quality of LLM diagnostic reasoning. To provide a clinically \ngrounded assessment, we developed FINDER (Framework for Inference and Diagnosis \nEvaluation in Rare Diseases), a Likert‑based rubric constructed via literature review and expert \nconsultation that organizes LLM performance into eight clinical dimensions and is applied to \nboth the model’s reasoning process and diagnostic outputs (see details in Methods). Using the \ntwo datasets described above, RareSeek‑R1 significantly outperformed OpenAI o1 in Medical \nCase Comprehension (3.963 vs. 3.584, P < 0.001), Medical Guidelines and Consensus (3.888 \nvs. 3.697, P < 0.001), Sensitivity to Key Clinical Features (4.067 vs. 3.729, P < 0.001), and \nClinical Reasoning (4.003 vs. 3.850, P < 0.001), indicating more accurate capture and \n"}, {"page": 19, "text": "integration of critical evidence. RareSeek‑R1 also scored higher for Differential Diagnosis \nRelevance (4.002 vs. 3.933, P < 0.001) and Diagnostic Acceptability (4.019 vs. 3.773, P < \n0.001), reflecting better clinical alignment, and for Bias and Fairness (3.923 vs. 3.764, P < \n0.001) and Potential for Harm (3.954 vs. 3.769, P < 0.001), indicating improved safety and \nreliability (Fig. 4d). \n \nFig. 4: Diagnostic performance integrating GraphRAG and genetic information, and \ncomparison of AI-assisted diagnosis with human physicians. (a) Impact of GraphRAG-\nbased retrieval and genetic information on diagnostic performance. Left: MedEHR‑Variant \n \n \n \n                           \n"}, {"page": 20, "text": "dataset; right: Phenopacket‑Store dataset. (b) Diagnosis comparison between AI and clinicians, \nand effect of AI assistance on clinicians for rare neurologic and congenital metabolic disorders \n(n = 110). (c) Human evaluation of RareSeek-R1 versus OpenAI o1 across eight clinical \ndimensions. Bar graphs represent mean ± 95% confidence intervals. Statistical significance was \ndetermined using two-sided t-tests: ***P < 0.001. \n \nModel interpretability and contribution of non-HPO clinical evidence in RareSeek-R1 \nmodel \nTo assess interpretability in rare disease diagnosis, we systematically analyzed RareSeek-\nR1’s case‑specific CoT reasoning, which provides a stepwise exposition of diagnostic logic and \nelucidates mechanisms underlying predicted outcomes. We selected four representative \ndisorders from the EHR-Internal dataset, including Wilson disease (ORPHA:905), Prader-Willi \nsyndrome (ORPHA:739), Tuberous sclerosis complex (ORPHA:805), and Langerhans cell \nhistiocytosis (ORPHA:389), and we applied word cloud visualization to highlight high-\nfrequency clinical features emphasized during reasoning. These disorders had sufficient well-\ndocumented cases within the EHR-Internal dataset, allowing comprehensive assessment of the \nmodel’s reasoning interpretability and underlying diagnostic rationale. RareSeek-R1 \nconsistently prioritized clinically salient cues concordant with established literature and disease \ndefinitions. For example, low ceruloplasmin and hepatic dysfunction in Wilson disease, and \ncortical tubers and seizures in Tuberous sclerosis complex, demonstrating precise attention to \ncore \ndiagnostic \ncharacteristics. To \nfurther \nprobe \nRareSeek‑R1’s \ndisease‑specific \ndecision‑making, we built a global surrogate model using extracted HPO‑annotated phenotypes \nas features and the model’s predicted diagnoses as targets. Feature importances derived from \nregression coefficients reliably recapitulated disease‑defining signals. For Wilson disease, the \ntop contributors were elevated hepatic transaminases, jaundice, vomiting, hepatitis, and Kayser-\nFleischer ring. For Tuberous sclerosis complex, the leading features were seizure, \nsubependymal nodules, sleep disturbance, status epilepticus, and cardiac rhabdomyoma, each \nconcordant with HPO annotations and canonical disease profiles.  \nIn analyzing RareSeek‑R1’s reasoning process, we found that HPO‑unmappable, \nnon‑phenotypic clinical evidence, alongside standardized phenotypes, is pivotal for rare disease \n"}, {"page": 21, "text": "inference. For example, Sildenafil therapy, commonly used to treat pulmonary arterial \nhypertension, cannot be directly mapped to HPO terms, yet it provides valuable diagnostic clues \nfor rare diseases such as Idiopathic/heritable pulmonary arterial hypertension. To quantify this \ncontribution, we examined all correct diagnostic CoT outputs in EHR‑Internal, extracting \ndiagnostic features and mapping them to HPO, and unmapped items were annotated as \nnon‑HPO evidence (see details in Methods). Overall, the median per‑patient proportion of \nnon‑HPO features was 23.1% (interquartile range [IQR] 0.118-0.400; Fig. 5a). Among correctly \ndiagnosed conditions with more than 30 cases, non‑HPO features averaged over 40% in \nLangerhans cell histiocytosis, retinoblastoma, and neuroblastoma, with IQRs of 0.400-0.600, \n0.375-0.600, and 0.297-0.586, respectively (Fig. 5c). By Orphanet category, averages exceeded \n30% in rare gastroenterologic, respiratory, hematologic, ophthalmic, and neoplastic diseases, \nwith IQRs of 0.239-0.441, 0.200-0.533, 0.229-0.520, 0.250-0.500, and 0.300-0.583, \nrespectively (Fig. 5d). These findings underscore that non‑HPO evidence is substantial and \nclinically consequential, and can be effectively leveraged by LLMs to enhance diagnostic \nreasoning. \nWe further conducted a systematic categorization of non‑HPO features extracted from \nEHR narratives. Leveraging DeepSeek‑R1 for semantic classification (see details in Methods), \nthese signals predominately mapped to seven categories: Imaging findings (26.5%), Clinical \ninterventions or procedures (23.0%), Functional assessments (17.7%), Laboratory test results \n(9.1%), Genetic or molecular test results (8.2%), Pathology or histology results (7.2%), and \nEnvironmental, lifestyle, or exposure factors (5.4%) (Fig. 5b). This distribution indicates that \nrare disease inference with LLMs hinges not only on standardized phenotypes but also on rich, \nnon‑phenotypic clinical evidence that complements HPO‑mapped features and frequently \ncarries decisive signal, particularly in oncology, ophthalmology, and multisystem disorders. In \ncomplex presentations, integrating both phenotypic and non‑phenotypic inputs aligns more \nclosely with clinician reasoning, improves calibration of differential diagnoses, and enhances \ninterpretability through traceable evidence categories. These findings also motivate future \nAI‑enabled rare disease pipelines to incorporate multi‑source clinical data and structured \ncategorization of non‑HPO evidence, thereby strengthening robustness, clinical validity, and \nreal‑world applicability. \n"}, {"page": 22, "text": " \nFig. 5: Non-HPO clinical evidence of RareSeek-R1 in rare disease inference. (a) Proportion \nof phenotype versus non-phenotype features per case (median and interquartile range); (b) \nDistribution of non-phenotype features across categories in the full chain-of-thought reasoning \ndataset; (c) Distribution of non-phenotype features by category in representative disease cases; \n(d) Proportion of phenotype and non-phenotype features across rare disease system categories \nin representative cases. \n \nDiscussion \nWe present RareSeek‑R1, a domain‑specialized LLM for rare disease diagnostic reasoning \ntrained via Progressive Parameter‑Efficient Transfer Learning, pairing instruction tuning on a \nlarge, clinically grounded RareMed‑Corpus deeply integrated from diverse texts and real‑world \nEHRs with high‑fidelity fine‑tuning on RareMed‑CoT to cultivate explicit, stepwise clinical \nreasoning. Across six internal and external independent benchmarks, RareSeek‑R1 delivered \nconsistently strong differential diagnosis performance under heterogeneous case distributions. \n \n    \n \n \n \n"}, {"page": 23, "text": "Then, human-AI comparisons positioned the model between middle‑ and senior‑level clinicians, \nand AI‑assisted rereads improved accuracy across all tiers, most notably for junior physicians. \nGraphRAG further reduced hallucinations, improved factual calibration, and aligned inference \nwith latest variant-gene-phenotype-disease relations, with the considerable gains when EHR \nnarratives were combined with prioritized variants. Finally, interpretability analyses confirmed \nclinical face validity by showing that case‑specific CoT traces and a global surrogate model \nrecovered that non‑HPO evidence, such as imaging findings, laboratory tests and other clinical \nprocedures, forms a diagnostically consequential component of the model’s rationale. Together, \nRareSeek‑R1 reasons directly over full EHR text, leverages graph‑grounded retrieval, and \nmeasurably augments clinician performance, advancing reliable and scalable AI support for \nrare disease diagnosis. \nPrior work shows LLMs degrade on phenotype‑only inputs versus full narratives, \nindicating that phenotype extraction alone underestimates diagnostic potential27,53. Curated \nbenchmarks like RareBench privilege tidy HPO profiles while omitting decisive non‑HPO \nsignals, like disease trajectory, treatment response, and laboratory or imaging findings54,55, thus \nfavoring phenotype‑driven tools in artificial settings. In routine care, etiologies are uncertain56, \nphenotypes are noisy or overlapping35,45,57, and multimorbidity is common58, demanding \nsystems that synthesize heterogeneous evidence rather than operate on phenotypes in isolation. \nRareSeek‑R1 reasons directly over full EHR narratives and preserves non‑phenotypic \ninformation that phenotype pipelines compress or discard, maintaining superior accuracy under \nphenotypic noise and complexity where conventional tools degrade. Sensitivity analyses show \nthat even within few key phenotypes, substantial diagnostic signal lies outside HPO‑A, \nexplaining consistent gains with narrative inputs. These capabilities both upgrade and disrupt \npractice. As an upgrade, RareSeek‑R1 transforms messy, incomplete notes into actionable \ndifferentials, reducing dependence on perfect HPO normalization and mitigating failure modes \nfrom ontology gaps, term granularity, and context loss. Operationally, RareSeek‑R1 acts as a \nfront‑door assistant surfacing high‑yield tests/referrals and a back‑end auditor reconciling \nnarrative, phenotype, and genomic evidence, shortening the diagnostic odyssey and improving \nequity. These results support a paradigm shift from phenotype‑centric pipelines to \nnarrative‑first, graph‑grounded clinical reasoning that better mirrors real‑world decision. \n"}, {"page": 24, "text": "Although RareSeek‑R1 delivers high diagnostic accuracy and interpretability, performance \nmay decline in ultra‑rare disorders or in presentations with substantial phenotypic overlap that \ncan be mistaken for common diseases, reflecting limited training data, ambiguous clinical \nsignals, and non‑deterministic factors59,60. KGs provide structured priors linking diseases, \nphenotypes, genes, and variants, compensating for long‑tail gaps and constraining candidates \nby genetic and molecular mechanisms to efficiently narrow the search space55,61. For example, \nKCNQ2‑related developmental and epileptic encephalopathy (ORPHA:439218) and \nSTXBP1‑related encephalopathy (ORPHA:599373) share global developmental delay \n(HP:0001263), seizures (HP:0001250), and intellectual disability (HP:0001249), making \nphenotype‑only discrimination difficult. Thus, integrating graph‑based genetic context removes \nmechanism‑inconsistent candidates and speeds convergence to the correct diagnosis. In practice, \nthis both upgrades existing workflows and enables a more disruptive, narrative‑native, \ngraph‑constrained approach to end‑to‑end reasoning. Overall, combining semantic reasoning \nwith KG priors improves accuracy and reliability and underscores the distinctive value of \nGraphRAG for complex rare disease diagnosis61. \nInterpretability and reliability are prerequisites for clinical use62-64. RareSeek‑R1 natively \nproduces explicit diagnostic reasoning chains, offering transparent insight into how conclusions \nare reached. The highlighted cues in its outputs align with established knowledge, for instance, \ncopper accumulation and liver dysfunction supporting Wilson’s disease65, and cryptorchidism \nwith developmental delay supporting Prader-Willi syndrome66. Feature relevance from a global \nsurrogate model was concordant with HPO‑based disease-phenotype associations, providing \nconvergent validation of mechanism‑consistent reasoning. Beyond phenotypes, the chains \nincorporate non‑phenotypic evidence, such as imaging findings (intraocular mass on CT) and \ngenetic data (A3243G mtDNA mutation), that match known disease characteristics. Clinically, \nthis dual validation (reasoning trace plus external priors) both upgrades current workflow \ntransparency and auditability and enables a more disruptive pathway in which narrative‑native \nreasoning is continuously checked against structured knowledge. \nRareSeek‑R1 adds practical value across care and research while enabling a potential shift \nin workflows. For clinicians, explicit reasoning chains streamline information capture, integrate \nmultimodal data, and guide interpretation, improving decision quality and auditability. In \n"}, {"page": 25, "text": "preclinical triage, it ranks likely diagnoses from narratives and history, and recommends \ntargeted tests or referrals, helping primary care identify complex rare diseases early and \ncoordinate multidisciplinary care. For patients with multisystem or ambiguous presentations, it \nprovides cross‑disciplinary support that unifies diverse evidence, and in remote or \nresource‑limited settings it improves early access through personalized testing and referral \npathways, potentially reducing delays. For researchers, structured reasoning outputs and a \ngraph‑grounded knowledge base aid discovery of disease, phenotype, gene, and variant links, \ninforming mechanistic studies and trial design. Collectively, these roles strengthen clinical \nmanagement and open a path toward narrative‑first and graph‑constrained diagnostic \nworkflows that are auditable, updatable, and scalable. \nThis study has limitations that reflect enduring challenges in the rare disease domain. Over \nhalf of rare diseases lack known etiologies, yet training and evaluation labels largely reflect \nclinical diagnoses rather than etiologic or molecular confirmations, constraining etiologic \ninference and gene level prediction that are essential for sequencing optimization and discovery \nof phenotype gene associations6,67-69. The current focus on textual and structured data without \ncomprehensive integration of imaging and laboratory signals may limit applicability in complex \nscenarios where multimodal synthesis is required55,70-73. RareSeek‑R1 also lacks explicit \nexclusion mechanisms for common diseases, a choice that may increase false negatives in rare \ndisease detection while still aiding differential diagnosis. In addition, the knowledge landscape \nevolves rapidly, and existing graphs incompletely capture newly described entities and \nphenotypic traits, creating gaps in coverage and update cadence. Notwithstanding these \nconstraints, RareSeek‑R1 demonstrates robust performance in cases with established etiologic \nor molecular diagnoses, indicating alignment with clinically meaningful mechanisms. Future \nwork will expand cohorts with etiologic confirmation to strengthen inference at the mechanism \nlevel, integrate multimodal evidence across narratives, genetics, imaging, and laboratories to \nimprove performance in complex presentations, thereby advancing RareSeek‑R1 from clinical \nlevel interpretation toward etiologic level understanding and improving readiness for real world \ndeployment. \n \n \n"}, {"page": 26, "text": "Methods \nData collection \n1. Domain-specific instruction tuning dataset \nTo enable domain-specific knowledge infusion for RareSeek-R1, we constructed the \nRareMed-Corpus, a multilingual rare disease dataset comprising four component datasets, \nRareMedEHR, RareMedText, PubMed-CR, and RareMedSynthetic, which collectively contain \napproximately 500 million tokens (Fig. 1a). \nRareMedEHR comprises 48,852 de-identified electronic health records collected from \nmultiple hospitals across China, including Guangzhou Women and Children’s Medical Center \n(GWCMC) and the Tianjin Healthcare Big Data Super Platform. The dataset encompasses 263 \nrare diseases, with patients having a mean age of 3.83 years (SD = 5.42), reflecting the \npredominantly pediatric nature of the cohort. GWCMC, as the largest women and children’s \nhospital in South China and the first in the region with Level-7 EHR capability, contributes a \nlarge volume of rare pediatric disease cases, providing extensive longitudinal EHR data for \nearly diagnosis and comprehensive disease characterization. The latter aggregates clinical and \npublic health data from 43 tertiary and 39 secondary hospitals in Tianjin, which, after \nstandardization and de-identification, form an interoperable, research-ready database. The \ndataset covers the period from January 2016 to April 2024. The study was conducted under a \nwaiver of written informed consent approved by the Institutional Review Board (IRB), and \nethical approval was obtained from the IRB and ethics committees of all participating \ninstitutions (no.2025[379A01]). All EHR data were rigorously de-identified to remove any \npatient-identifiable information. Each record includes demographic information, chief \ncomplaint, present illness, ancillary tests, family history, physical examination, and specialty \nassessments. To ensure diagnostic accuracy and consistency, we included only records in which \nthe rare disease diagnosis was explicitly documented in the discharge summary and \ncorroborated by the responsible clinician or by a specialist/multidisciplinary review. For \npatients with multiple admissions, only the first hospitalization was retained to avoid \nredundancy and potential bias introduced by longitudinal follow-up. In addition, eligible \nrecords were required to contain at least one disease-associated phenotype or equivalent \nobjective diagnostic evidence, including genetic or molecular test results, pathology or \n"}, {"page": 27, "text": "histology reports, imaging findings, and other relevant diagnostic evidence. Records were \nexcluded if they lacked essential clinical sections, contained uncertain diagnoses, or provided \ninsufficient information for reliable assessment. Disease names were standardized using the \nMonarch Initiative v3 API (https://api-v3.monarchinitiative.org/v3/docs#/) to map each \ndiagnosis to the corresponding Orphanet (ORPHAcode) identifier. All mappings were manually \nreviewed by experienced physicians to resolve inconsistencies. To ensure dataset completeness \nand cross-source consistency, the data underwent validation, duplicate detection, and random \nmanual review of 5% of the records, achieving an inter-reviewer agreement of κ = 0.91. \nRareMedText was developed as a comprehensive corpus of textual resources on rare \ndiseases, compiled from authoritative medical textbooks, specialty monographs, evidence-\nbased clinical practice guidelines, and disease encyclopedic knowledge sources. The collection \ncomprises 35,722 multilingual entries covering 6,820 distinct rare diseases, systematically \nencompassing disease definitions, etiologies, clinical manifestations, diagnostic criteria, \ndifferential diagnoses, and related clinical knowledge. Source materials were obtained from \nwell-recognized repositories and institutions, including the China Alliance for Rare Diseases \n(ChARD, https://www.chard.org.cn/), the National Organization for Rare Disorders (NORD, \nhttps://rarediseases.org/), Orphanet (https://www.orpha.net/), the Online Mendelian Inheritance \nin \nMan \n(OMIM, \nhttps://www.omim.org/), \nthe \nChinese \nMedical \nAssociation \n(https://www.yiigle.com/index), the Rare Disease Wikipedia (https://en.wikipedia.org/wiki), \nand other authoritative databases across diverse medical disciplines such as pediatrics, genetics, \nneurology, immunology, and endocrinology. To ensure consistency, all disease names in \nRareMedText were standardized to ORPHAcode following the same curation and verification \nprocess applied to RareMedEHR. \nThe PubMed-CR dataset comprises 30,101 abstracts and full-text case reports of rare \ndiseases collected from PubMed Central (PMC, https://www.ncbi.nlm.nih.gov/)74, the largest \npublicly accessible digital repository of biomedical and life sciences literature. The reports \nprovide detailed descriptions of patients’ symptoms, signs, diagnoses, treatments, and follow-\nup outcomes, with a particular focus on rare, diagnostically challenging, or newly recognized \ndisorders. To identify relevant case reports, we performed systematic searches in PubMed using \nOrphanet disease names combined with the term “case report” in article titles. All articles and \n"}, {"page": 28, "text": "abstracts were sourced from the PubMed Central Open Access Subset (knowledge cutoff: \nOctober 2024), with corresponding PMIDs retrieved from the National Library of Medicine \ndatabase and full texts automatically downloaded. During curation, extraneous content such as \nreferences, tables, figures, and supplementary materials was removed, and duplicated or \nincomplete reports were excluded. All disease names in the extracted reports were subsequently \nstandardized to ORPHAcode, following the same curation and verification process applied to \nRareMedEHR. This dataset provides a rich source of authentic clinical narratives, enabling the \nmodel to learn diverse real-world diagnostic patterns and reasoning processes across complex \nand low-frequency rare diseases. \nThe RareMedSynthetic dataset comprises 34,666 phenotype-driven synthetic cases \ngenerated from HPO annotations (https://hpo.jax.org/data/annotations) and Orphanet disease-\nphenotype \nassociations (https://www.orphadata.com/), \ndesigned \nto supplement \nthe \nrepresentation of ultra-rare or underreported diseases that are insufficiently captured in real-\nworld datasets. For each disease, associated phenotypes were systematically combined to \ngenerate plausible case scenarios. To increase the complexity and realism of synthetic cases, \nadditional phenotypes unrelated to the disease were randomly sampled and incorporated as \ndistractor features, simulating noisy or confounding clinical information commonly \nencountered in practice. Each synthetic case therefore consists of a set of disease-specific \nphenotypes alongside potential distractor phenotypes, reflecting both characteristic clinical \nfeatures and the variability of real-world patient presentations. By providing structured and \ncomprehensive synthetic case representations, RareMedSynthetic enables the model to learn \nricher phenotypic patterns and disease-phenotype associations, particularly for conditions with \nextremely limited or absent clinical documentation. This synthetic augmentation mitigates data \nsparsity and enhances the model’s capacity to generalize diagnostic reasoning across low-\nfrequency and highly heterogeneous rare disease presentations. \n2. Chain-of-Thought fine-tuning dataset \nTo improve the reasoning and generalization capabilities of large language models in rare \ndisease diagnosis, we constructed the RareMed-CoT dataset, a systematically curated resource \ncomprising 17,477 high-quality diagnostic reasoning samples. The dataset construction began \nwith manual annotation of 500 EHRs by clinical experts, which served as a high-fidelity seed \n"}, {"page": 29, "text": "dataset19,75. For each record, clinicians provided stepwise diagnostic reasoning, documenting \nkey clinical features, laboratory and imaging findings, pathological results, and candidate \ndiseases for differential consideration. Critical decision points and supporting evidence were \nexplicitly recorded to ensure clinical validity, logical rigor, and reproducibility. These seed \ncases not only captured the complexity of real-world rare disease diagnosis but also served as \nstructured exemplars for subsequent large-scale reasoning generation. \nBuilding on this foundation, 20,000 additional candidate diagnostic reasoning samples \nwere generated using DeepSeek-R116, guided by standardized prompts and authoritative rare \ndisease clinical guidelines. The generated dataset spanned 157 distinct rare diseases across \nmultiple organ systems, including endocrine, respiratory, neurological, hematologic, \nmusculoskeletal, and dermatologic disorders, thereby ensuring broad phenotypic coverage and \nrepresentative clinical scenarios. This strategy allowed the inclusion of both common and ultra-\nrare disease manifestations, ensuring that the resulting reasoning chains reflect realistic \nvariability in clinical presentations, laboratory patterns, imaging findings, and histopathological \ncharacteristics. \nTo maintain data quality and reliability, we implemented a multi-tiered quality control \nprocess. Initially, all generated samples underwent automated pre-screening using ChatGPT-\n4o13, which evaluated format integrity, logical coherence, and consistency with established \nmedical knowledge, automatically flagging outputs with evident contradictions or clinical \ninaccuracies. Samples identified as high-risk or exhibiting residual uncertainty were \nsubsequently manually reviewed by clinical experts, who verified whether each reasoning chain \nappropriately covered key diagnostic evidence, maintained logical consistency, and avoided \ncritical medical errors. Discrepancies between reviewers were resolved by senior clinical \nexperts, ensuring that only reasoning chains meeting stringent clinical and logical standards \nwere retained. Through this rigorous process, 16,977 high-quality generated reasoning chains \nwere ultimately selected and merged with the original 500 manually annotated seed cases to \nform the final RareMed-CoT dataset75. \nThis comprehensive dataset provides a robust training resource for chain-of-thought (CoT) \nfine-tuning, enabling large language models to move beyond purely outcome-oriented \npredictions toward process-explainable diagnostic reasoning. By capturing the stepwise clinical \n"}, {"page": 30, "text": "logic applied in real-world rare disease diagnosis, the dataset enhances model interpretability, \nreliability, and generalization across diverse clinical scenarios, supporting downstream \napplications in both research and clinical decision support systems. \n3. Evaluation dataset \nTo systematically evaluate the rare disease diagnostic performance of LLMs against \ntraditional phenotype-driven approaches in real-world clinical settings, we constructed a \ncomprehensive evaluation dataset comprising 11,429 rare disease test cases from multiple \nsources and data types. \nEHR-Internal comprises 4,306 rare disease electronic health records covering 75 distinct \ndiseases that were not used in model training and serve as an internal test set to evaluate \nRareSeek-R1’s diagnostic performance. Each record contains comprehensive clinical \ninformation, including chief complaints, history of present illness, family history, physical \nexamination, specialty assessments, and ancillary tests such as laboratory and imaging reports. \nEHR-External comprises 283 cases spanning 21 rare diseases, collected from hospitals within \nthe Tianjin Healthcare Big Data Super Platform that are different from those contributing to the \ntraining dataset. This dataset provides an independent evaluation to assess the model’s \nrobustness and generalization across different hospitals, despite originating from the same \noverarching platform. Both datasets adopted the same physician-confirmed and documentation-\nbased diagnostic verification criteria as RareMedEHR, ensuring consistency and reliability of \ndiagnostic labeling. RareBench, a publicly available benchmark, integrates five rare disease \nrepositories, including MME34 (40 cases, 17 diseases), LIRICAL35 (370 cases, 252 diseases), \nHMS36 (88 cases, 46 diseases), RAMEDIS37 (624 cases, 74 diseases), and PUMCH_ADM24 \n(75 cases, 16 diseases), totaling 1,197 phenotypic-only cases, which are used to evaluate the \nmodel’s generalization ability on standardized benchmarks. \nTo investigate the contribution of genetic variation and GraphRAG-enhanced reasoning to \ndiagnostic performance, two additional datasets were included. MedEHR-Variant comprises \n147 cases covering 109 rare diseases from Guangzhou Women and Children’s Medical Center, \neach with complete EHRs and WES data, representing the only test set with both full clinical \nand genomic information. Phenopackets, from the GA4GH Phenopacket-store repository76, \nincludes 5,213 standardized cases covering 378 rare diseases, each containing patient \n"}, {"page": 31, "text": "phenotypes, variant annotations, and clinical diagnoses in a uniform structured format, enabling \ncross-study and cross-model comparisons. Collectively, these datasets provide a comprehensive \nevaluation framework for RareSeek-R1, spanning internal and external validation, phenotypic-\nonly benchmarks, and genotype-inclusive cohorts, thereby allowing assessment of both \ndiagnostic accuracy and generalization across diverse clinical and genomic scenarios. \n \nModel overview \nHere we developed RareSeek-R1, a domain-specialized large language model for rare \ndisease diagnosis, using a Progressive Parameter-Efficient Transfer Learning framework30. This \ntwo-stage paradigm integrates domain-specific instruction tuning to inject rare disease \nknowledge77 and chain-of-thought fine-tuning to cultivate explicit clinical reasoning75. The \napproach preserves the base model’s robust language understanding and contextual modeling \nability while systematically instilling expert medical knowledge and reasoning patterns. \nConsequently, RareSeek-R1 achieves interpretable and clinically grounded diagnostic \ninferences across complex rare disease cases. \n1. Domain-specific instruction tuning for rare diseases \nIn this study, we selected DeepSeek-R1-Distill-Llama-70B as the base model for \ninstruction tuning. This model uses the Llama-3.3-70B architecture as the student model and is \ntrained via knowledge distillation from the more powerful DeepSeek-R1-671B teacher model. \nCompared with the original teacher model, DeepSeek-R1-Distill preserves the core reasoning \ncapabilities of large models while substantially reducing computational and storage costs16. In \nmedical applications, particularly for multi-turn reasoning and diagnostic tasks, DeepSeek-R1 \nhas demonstrated robust performance and reliability78,79, making it an ideal base model for fine-\ntuning in rare disease diagnosis. \nFor instruction tuning, we utilized RareMed-Corpus, a multilingual rare disease dataset \nintegrating \nfour \ncomponents-RareMedEHR, \nRareMedText, \nPubMed-CR, \nand \nRareMedSynthetic-collectively encompassing approximately 500 million tokens derived from \nstructured and unstructured clinical data, standardized disease descriptions, and curated \nphenotype-disease associations. The fine-tuning process systematically infused domain-\nspecific rare disease knowledge into the model77, enabling it to interpret complex clinical \n"}, {"page": 32, "text": "instructions, reason over heterogeneous data modalities, and accurately recognize specialized \nmedical terminology. This knowledge integration substantially enhances RareSeek-R1’s ability \nto perform diagnostic reasoning across diverse clinical contexts, including ultra-rare or \nunderrepresented conditions with limited real-world documentation. \n2. CoT fine-tuning for enhanced diagnostic reasoning \nTo further enhance the model’s diagnostic reasoning capabilities in complex clinical cases, \nwe employed a CoT fine-tuning strategy75,80. Unlike direct prediction of final diagnoses, CoT \nfine-tuning requires the model to explicitly generate clinically coherent reasoning chains prior \nto producing diagnostic outputs. These chains encompass reasoning steps from symptoms to \ncandidate diseases, interpretation of genetic variants, differential diagnosis logic, and \nexplanatory notes on exclusion processes. This approach better emulates the cognitive \nworkflow of clinicians, improving both interpretability and diagnostic accuracy. \nThe training data were derived from two sources. First, a small set of clinician-annotated \ndiagnostic process records served as high-quality seed data75. Second, leveraging the \nDeepSeek-R1 teacher model in combination with authoritative rare disease diagnostic \nguidelines, we automatically generated a large-scale set of diagnostic reasoning samples to \nexpand the training corpus. Each training instance consists of three components: the input \npatient record xi, the diagnostic label yi, and the corresponding clinician-annotated reasoning \nchain 𝑟𝑖, with reference to the relevant disease guideline gi. Based on this structure, the large \nlanguage model is trained to generate coherent diagnostic reasoning chains prior to final \ndiagnosis prediction: \n𝑟̂𝑖= [(𝑥𝑖, 𝑦𝑖, 𝑟𝑖, 𝑔𝑖)]𝑖=1\n𝑁 \nsubsequently, the data were organized into standardized prompt-completion pairs: \n𝑆𝑖\n′ = (𝑝𝑖, 𝑐𝑖), 𝑝𝑖= (𝑥𝑖, 𝑔𝑖), 𝑐𝑖= (𝑟̂𝑖, 𝑦𝑖) \nwhere 𝑝𝑖 represents the input prompt encompassing patient case information and relevant \nclinical guidelines, and 𝑐𝑖 denotes the corresponding generated reasoning chain and final \ndiagnosis. Based on this formulation, the fine-tuning procedure first integrates each patient case \nand associated guideline into the input prompt 𝑝𝑖, under which the model generates a complete \ndiagnostic reasoning chain 𝑟̂𝑖 and the corresponding diagnosis 𝑦𝑖. Supervised learning is then \n"}, {"page": 33, "text": "applied by aligning the model outputs with the standardized completion 𝑐𝑖, enabling the model \nto progressively learn the comprehensive reasoning pathway from clinical and genetic \ninformation to candidate diseases and differential diagnoses, thereby systematically enhancing \nits clinical reasoning capabilities. To further improve training efficiency and reduce \ncomputational overhead, this fine-tuning process was implemented using Low-Rank \nAdaptation (LoRA)81. \n \nKnowledge Graph construction and retrieval-augmented generation \nTo enable structured integration of genetic and phenotypic knowledge for rare diseases, \nwe constructed RareMed‑RAG, a domain-specific biomedical KG designed to support \ndownstream diagnostic reasoning, knowledge retrieval, and retrieval-augmented generation \n(RAG) tasks82-84. The KG was implemented using Neo4j, providing scalable storage, querying, \nand traversal capabilities for heterogeneous entities and relationships. It encompasses four core \nentity types-diseases, phenotypes, genes, and variants-forming the foundational infrastructure \nfor comprehensive cross-entity reasoning. \nData integration leveraged multiple authoritative biomedical databases and standardized \nontology resources. ClinVar8 (20250209 version) and the Human Gene Mutation Database31 \n(HGMD Professional 2024.2) provided variant-level knowledge, including pathogenic variants, \ngenomic coordinates, and clinical significance. After deduplication of overlapping loci between \nthe two sources, a total of 635,439 unique variant sites were retained. OMIM contributed 7,534 \nentries, supplying phenotype-gene associations that link genetic mechanisms to clinical \nmanifestations32. Orphanet provided comprehensive disease definitions, hierarchical \nrelationships among disease entities, and curated disease-phenotype associations annotated \nwith frequency attributes describing their occurrence in patient populations (e.g., always \npresent, very frequent, frequent, occasional, rare), encompassing 4,281 annotated disease-\nphenotype relationships33. These resources collectively ensured disease-level standardization \nand enriched the integration of complementary knowledge sources. The HPO played a central \nrole in the workflow by standardizing phenotypic terms and integrating multiple internal \nrelational resources, including HPO Annotation (HPOA) files such as disease-phenotype \nmappings, gene-phenotype associations, and gene-disease links7. In total, the HPO-based \n"}, {"page": 34, "text": "integration captured 9,625 OMIM-coded diseases, 8,217 Orphanet-coded diseases, 16,694 \nunique phenotypes, and 5,186 genes, enhancing cross-entity connectivity and facilitating \ncomprehensive linkage across genetic and phenotypic data. All source databases are \nperiodically synchronized, and the KG is regularly updated to incorporate the latest biomedical \nand genomic information. \nThe construction workflow followed several sequential stages. First, raw source datasets \nwere parsed and transformed into a unified data schema. Second, entity normalization was \nperformed: disease concepts were harmonized to authoritative identifiers and terms (Orphanet \nand OMIM), phenotypic terms were mapped to HPO entries, and genes were standardized using \nHGNC nomenclature. Variants were normalized using transcript-level identifiers along with \nstandardized nucleotide (c.) and protein (p.) notations, and were linked to genomic coordinates \nwhere available. Third, semantic relationships were instantiated across resources, including \ndisease-phenotype, \ndisease-gene, \ngene-variant, \nand \nphenotype-gene \nedges, \nwhile \nsupplementary relations from HPO and curated external resources were incorporated to \nenhance connectivity and coverage. Finally, the integrated KG was imported into Neo4j and \nsubjected to automated and manual quality-control procedures, including relationship \nconsistency checks, redundancy removal, and structural integrity validation, ensuring \ncoherence, reliability, and traceable provenance of the graph. \nIn the knowledge retrieval stage, this study employed Graph Cypher Retriever as the core \nretrieval mechanism instead of conventional vector-based retrieval. Traditional vector retrieval \nmethods typically rely on semantic similarity measures, which are prone to introducing \nsubstantial noise. For entities requiring high precision, such as genes and variants, this often \nresults in irrelevant or even incorrect matches. In contrast, the Cypher-based retrieval approach, \nleveraging the query language of the graph database, directly operates on the explicit indices \nand relational structures within the KG, thereby achieving high determinism and precision in \nentity localization and relationship extraction. This characteristic aligns well with the stringent \nrequirements of rare disease diagnostic reasoning, providing a reliable and verifiable \nknowledge foundation for subsequent inference and generation tasks. \nThe retrieval process proceeds as follows: first, the input query objects (e.g., diseases, \nphenotypes, genes, or variants) are mapped to standardized identifiers consistent with the KG \n"}, {"page": 35, "text": "schema. Second, Cypher queries are dynamically constructed according to the entity type to \naccurately locate target nodes and their associated relationships within Neo4j. For phenotype-\nbased retrieval, which may yield a large number of candidate diseases, the results are further \nfiltered and ranked using IC metrics to prioritize candidates with higher informativeness and \nstronger discriminative power. Finally, the processed subgraph is returned in a structured format \nand incorporated as external knowledge input to the language model, supporting downstream \nreasoning and generation processes. \n \nBaselines for rare disease diagnosis \nTo comprehensively evaluate the performance of RareSeek-R1, two categories of baseline \nmethods were selected: \n1. Phenotype‑driven tools \nWe incorporated multiple phenotype-based diagnostic tools, including Exomiser5, \nPhen2Disease46 (both double and patient modes), Base_IC46, PhenoDP45, and Phrank47. Among \nthem, Exomiser is a widely adopted tool in rare disease research, originally designed for the \njoint prioritization of genes and variants by integrating phenotypic and sequencing data. In this \nstudy, we employed its “phenotype_only” mode to enable disease ranking purely based on \nphenotypic inputs. Phen2Disease is a recently proposed phenotype-driven diagnostic approach \nthat applies IC-weighted HPO features and a bidirectional patient-disease similarity metric for \ndisease prioritization; we evaluated it under its phenotype-only configuration as well. Base_IC \nperforms ranking by identifying overlapping phenotypic terms between a patient and candidate \ndiseases and summing their IC values. PhenoDP, a deep learning-based approach, generates \nphenotypic summaries, recommends relevant symptoms, and optimizes disease ranking to \nimprove the diagnostic efficiency of Mendelian disorders. Phrank represents a classical gene \nand disease prioritization algorithm whose phenotype-only mode extends disease-associated \nphenotypes and applies Bayesian network-based and information-theoretic scoring to perform \nranking. \n2. Large language models \nTo systematically assess the diagnostic potential of LLMs in rare disease diagnosis, we \nselected two presentative categories of models. First, medical LLMs include Meditron-70B22, \n"}, {"page": 36, "text": "MMedLM-70B21, Clinical Camel-70B23, HuatuoGPT-o1-70B43, Baichuan-M144, and \nBaichuan-M220. These models were pretrained or instruction-tuned on large-scale medical \ncorpora and clinical datasets to enhance medical knowledge coverage and clinical reasoning \ncapabilities. Second, general LLMs, including LLaMA 3.3-70B38, QwQ-32B42, GPT-4o13, \nOpenAI o141, and GPT-539, were trained on diverse cross-domain corpora to achieve broad \nlanguage understanding and generative competence. Their inclusion allows for a systematic \ncomparison between domain-specific and general-purpose models in rare disease diagnostic \ntasks, highlighting their differences and complementarity. Notably, several models, such as \nQwQ-32B, Baichuan-M2, and OpenAI o1, incorporate reasoning-enhanced mechanisms within \ntheir architectural design or training strategies. These mechanisms aim to strengthen complex \nlogical reasoning and multi-step inference, thereby enabling the evaluation of the effectiveness \nof reasoning enhancement in improving diagnostic performance under complex clinical \nscenarios. \n \nEvaluation of LLM-based diagnostic results \n1. Standardization of disease names \nIn evaluating the diagnostic performance of LLMs, standardizing disease nomenclature is \na prerequisite for achieving objective comparison and automated evaluation. Since model-\ngenerated disease names are typically expressed in natural language, direct text matching is \nsusceptible to variations in phrasing and semantic ambiguity, potentially introducing evaluation \nbias. To address this issue, all model outputs were first processed to extract disease names from \nthe unstructured diagnostic text responses. Given that the outputs of LLMs often contain \ncontextual reasoning, justification, or ranked disease lists, we employed ChatGPT-4o to \nperform structured disease name extraction. A rule-based prompt template was designed to \ninstruct the model to identify and output only the disease entities mentioned, while ignoring \ndescriptive reasoning or unrelated text. This step ensured that the subsequent normalization \nprocess operated on a consistent and unambiguous set of disease terms. \nFollowing extraction, all disease names were mapped to standardized ORPHAcodes. \nCompared with other ontological systems such as OMIM or MONDO, Orphanet provides both \nunique identifiers for rare diseases and a comprehensive hierarchical classification system, \n"}, {"page": 37, "text": "facilitating subsequent system-level statistical analyses. For implementation, the Monarch \nInitiative v3 API (https://api-v3.monarchinitiative.org/v3/docs#/) was used to map natural \nlanguage disease names to their corresponding Orphanet entities. Model-predicted names that \ncould not be mapped to any valid Orphanet entry were excluded to maintain the reliability and \nreproducibility of diagnostic accuracy evaluation. \n2. Definition of correct diagnosis \nDuring evaluation, model outputs were compared against the ground truth based on \nstandardized ORPHAcodes. A prediction was considered correct if the model-predicted code \nexactly matched the true diagnosis. Furthermore, recognizing the hierarchical nature of disease \nontologies and the variability in clinical naming conventions, we additionally considered \npredictions corresponding to the parent term (i.e., a higher-level disease category) of the true \ndiagnosis as correct. For example, if the ground-truth diagnosis was Mixed cryoglobulinemia \ntype II (ORPHA:93554) and the model predicted Cryoglobulinemic vasculitis (ORPHA:91138), \nthe prediction was still regarded as correct because the latter represents the parent concept of \nthe former in the Orphanet hierarchy. For each diagnostic task, the model was required to \ngenerate the top 20 most probable disease predictions for subsequent ranking-based evaluation. \n \nExploration and analysis of phenotype information in rare disease diagnosis \nWithin the aforementioned evaluation framework, the EHR-Internal dataset, derived from \nthe Guangzhou Women and Children’s Medical Center, represents the largest collection in \nterms of both disease diversity and number of cases. Given its representativeness and data \nsufficiency, this dataset was selected as the primary analytical cohort for subsequent \nexperiments and analyses to ensure the robustness and generalizability of the evaluation results. \n1. Phenotype extraction \nIn the domain of clinical natural language processing (NLP), most existing phenotype \nextraction tools are optimized for English-language corpora. To address this limitation, the \noriginal Chinese EHR texts were first translated into English using the DeepSeek-R1 model16. \nSubsequently, three widely used automated phenotype extraction tools, including \nPhenoTagger49, PhenoBERT4, and ClinPhen50, were applied to the translated texts, each \nexecuted with their default parameter configurations. Model selection was guided by \n"}, {"page": 38, "text": "downstream diagnostic performance within phenotype-driven diagnostic pipelines. Specifically, \nthe accuracy of rare disease prediction using extracted phenotypes served as the principal \nevaluation metric, allowing for a comparative assessment of the three extraction tools and the \nidentification of the most effective phenotype extraction strategy. \n2. Diagnostic comparison between EHR and phenotype-based inputs \nTo systematically evaluate the impact of different input formats on the diagnostic \nperformance of LLMs, two controlled experimental settings were established. (1) In the EHR-\ntext condition, the original unstructured clinical narratives were directly input into the model, \nsimulating real-world diagnostic reasoning without prior data curation. (2) In the phenotype-\nonly condition, the input consisted of phenotypic features automatically extracted from the \nEHR text and subsequently standardized using HPO terms. This setting enabled the evaluation \nof diagnostic performance when LLMs operate on structured, ontology-aligned phenotype \nrepresentations. For each clinical case, diagnostic predictions were generated under both \nconditions, and the resulting disease rankings and candidate lists were compared. This design \nenabled a quantitative assessment of how structured phenotypic representations influence the \ndiagnostic accuracy and interpretability of LLMs. \n3. Evaluation of model robustness under phenotypic noise and complexity \nTo investigate how the Noise phenotypic affects the diagnostic performance of LLMs, we \nfurther stratified cases according to the number of key phenotypes. Here, key phenotypes are \ndefined as phenotypic features extracted from the EHR text that match the known disease-\nassociated phenotypes annotated in the HPOA dataset, representing features with direct \ndiagnostic relevance. For each case, the number of key phenotypes was quantified and \ncategorized into three levels: Few key phenotypes (0 ≤ n ≤ 1), Moderate key phenotypes \n(1 < n ≤ 4), and Rich key phenotypes (n > 4). Diagnostic predictions were then generated \nwithin each stratum using the LLM, and both disease ranking accuracy and stability metrics \nwere recorded. By comparing performance across these strata, we systematically evaluated how \nthe abundance or scarcity of key phenotypic information affects model behavior. This analysis \nprovides insight into the model’s robustness under heterogeneous clinical data conditions and \nits adaptability to varying levels of phenotypic completeness in real-world rare disease \ndiagnosis. \n"}, {"page": 39, "text": "Building on this analysis, we further examined phenotypic complexity, reflecting the \nintrinsic heterogeneity and specificity of phenotype distributions in rare disease diagnosis. In \nrare disease research, IC is commonly employed to quantify phenotype specificity: phenotypic \nconcepts that appear infrequently in the corpus are assigned higher IC values, whereas lower \nIC values correspond to more general terms within the ontology hierarchy. The average IC \nmetric provides a measure of phenotype sparsity and diagnostic complexity within a dataset, \nserving as a reference for task difficulty when interpreting model performance. In this study, \nwe first computed the average IC for all cases in the EHR-Internal dataset and selected the Top-\n300 cases with the lowest average IC as samples representing high-complexity phenotypic \nprofiles. Given that some cases contain a large number of extracted phenotypes, including many \nirrelevant or noisy features, we further defined a composite difficulty score that integrates both \nthe average IC and the number of phenotypes, normalized to form a unified difficulty metric. \nIn this metric, higher scores indicate greater diagnostic difficulty, which is inversely related to \nthe average IC alone. Using this criterion, the Top-300 cases ranked by composite difficulty \nwere designated as high-complexity samples, enabling the evaluation of model performance in \nscenarios characterized by both noisy phenotypic inputs and low phenotype specificity. \nnorm_mean_IC(𝑠) = Mean_IC(𝑠) −Mean_IC𝑚𝑖𝑛\nMean_IC𝑚𝑎𝑥−Mean_IC𝑚𝑖𝑛\n \nnorm_cnt(𝑠) =\n𝑛−𝑛min\n𝑛max −𝑛𝑚𝑖𝑛\n \ncomposite_difficulty(s) = (1 −norm_mean_IC(s)) × norm_cnt(s) \n \nMean_IC(s): the average information content of disease s, computed over all phenotypes \nannotated for that disease. Mean_ICmin: the minimum average information content across all \ndiseases in the dataset. Mean_ICmax: the maximum average information content across all \ndiseases in the dataset. norm_mean_IC(s): the normalized average information content of \ndisease s, scaled between mean_ICmin and mean_ICmax. n: the number of phenotypes associated \nwith disease s. nmin: the minimum number of phenotypes observed for any disease in the dataset. \nnmax: the maximum number of phenotypes observed for any disease in the dataset. norm_cnt(s): \nthe normalized phenotype counts for disease s, scaled between nmin and nmax. \ncomposite_difficulty(s): the composite difficulty score for disease s, integrating both the \n"}, {"page": 40, "text": "normalized average information content and the normalized phenotype count to quantify \noverall diagnostic complexity. \n \nClinical study \n1. Sequential contribution of clinical evidence categories in rare disease diagnosis \nIn the clinical diagnosis of rare diseases, patient information is typically acquired in a \nsequential and progressive manner, beginning with the chief complaint, followed by the history \nof present illness, family history, physical examination, specialist examination, and auxiliary \ninvestigations. To systematically evaluate how each type of clinical information contributes to \nthe diagnostic reasoning process of LLMs, we designed two complementary experiments: \nIncremental Information Addition Analysis and Single-Field Ablation Analysis. \nIn the incremental analysis, the chief complaint was used as the baseline input. Additional \ninformation fields, including history of present illness, family history, physical examination, \nspecialist examination, and auxiliary tests, were progressively added in the natural order of \nclinical information acquisition. After each addition, model inference was performed to \nmeasure the marginal improvement in diagnostic accuracy attributable to the newly introduced \ninformation. In the ablation analysis, the full EHR was used as the baseline. In each iteration, \none specific field was removed while keeping all others intact, and diagnostic performance was \nre-evaluated. The relative decline in accuracy upon the removal of each field was used to \nquantify its importance to the overall diagnostic capability of the model. \nIn summary, these complementary analyses provide a fine-grained understanding of how \nstructured EHR components contribute to rare disease diagnosis, offering empirical evidence \nto guide the optimization of input structures and information utilization strategies for LLM-\nbased clinical reasoning. \n2. Comparison of diagnostic accuracy between AI and physicians \nTo systematically assess the role of the AI system in real-world clinical decision-making, \nwe conducted a human-AI comparative experiment evaluating diagnostic performance across \nclinicians with different levels of experience and an AI system integrating RareSeek-R1 with \nGraphRAG in rare neurological and congenital metabolic disorders. Participating clinicians \nwere categorized into three groups according to their clinical seniority: junior (n = 3), \n"}, {"page": 41, "text": "intermediate (n = 3), and senior (n = 3). A total of 110 clinical cases covering 12 representative \nrare diseases were included for evaluation. \nAll clinicians independently formulated diagnostic conclusions under identical conditions, \nbased solely on standardized EHR data, including the chief complaint, history of present illness, \nfamily history, physical examination, specialist examination, and ancillary investigations, \nwithout access to external references or tools. The AI model was provided with the same \nstructured EHR inputs to generate ranked candidate diagnoses, enabling direct comparison of \ndiagnostic accuracy and agreement across clinician groups and the AI system. \n3. Assisted diagnostic accuracy with the LLM in the workflow \nTo further evaluate the assistive role of the AI model in clinical diagnostic workflows, we \nconducted an extended assessment based on the preceding human-AI comparative experiment. \nTwo weeks after the initial independent diagnosis session, all three clinician groups (junior, \nmiddle, and senior) re-evaluated the same 110 rare disease cases, this time with access to the \nmodel’s reasoning rationales and diagnostic suggestions. Each clinician independently \nreviewed and revised their diagnostic conclusions using the AI-generated outputs as \nsupplementary references. The final diagnoses were then compared with those obtained during \nthe initial unaided phase. By analyzing the change in diagnostic accuracy across clinician \ngroups before and after AI assistance, this study systematically quantified the diagnostic gain \nconferred by the large language model and elucidated its potential utility as an augmentative \ntool in rare disease clinical decision-making.  \n4. FINDER: A human evaluation framework for rare disease diagnosis with LLMs \nTo systematically evaluate the diagnostic capability and potential limitations of LLMs for \nrare diseases in real-world clinical settings, we developed FINDER (Framework for Inference \nand Diagnosis Evaluation in Rare Diseases). This framework was designed with reference to \nexpert consensus in rare disease medicine, domain-specific literature, and established LLM \nevaluation methodologies, aiming to comprehensively assess the model’s medical consistency \nand clinical feasibility in generating accurate and reliable diagnostic outputs19,51,52. \nFINDER provides a multidimensional quantitative evaluation of rare disease LLMs across \neight key dimensions, covering case understanding, diagnostic reasoning, and safety \nconsiderations: (1) Medical Case Comprehension: evaluates the completeness and accuracy of \n"}, {"page": 42, "text": "the model’s semantic understanding and extraction of key information from clinical narratives; \n(2) Medical Guideline and Consensus Compliance: examines whether the model’s reasoning \nand diagnostic conclusions align with established medical guidelines and expert consensus; (3) \nSensitivity to Key Clinical Features: measures the model’s ability to identify both phenotypic \nand non-phenotypic features essential for rare disease diagnosis; (4) Clinical Reasoning \nConsistency: assesses whether the model’s diagnostic logic is consistent with clinical reasoning \npathways and physician thought processes; (5) Relevance of Differential Diagnosis: evaluates \nthe model’s capability to differentiate among multiple candidate diseases and identify the most \nplausible etiologies; (6) Acceptability of Diagnostic: determined by clinical experts, this \ndimension measures the practical feasibility and reliability of model-generated diagnoses in \nreal-world clinical settings; (7) Bias and Fairness: assesses potential biases related to age, sex, \nculture, or ethnicity that may affect diagnostic outcomes; and (8) Possibility of Harm: detects \nthe presence of false, misleading, or potentially harmful information that could contribute to \nmisdiagnosis or clinical risk. \nIn summary, the FINDER framework provides a standardized and systematic foundation \nfor revealing the comprehensive capabilities of rare disease LLMs in medical understanding, \nclinical reasoning, fairness, and safety. \n \nModel interpretability and non-HPO clinical evidence in rare disease diagnosis \nTo systematically elucidate the reasoning logic and decision rationale of the model in rare \ndisease diagnosis, we conducted a comprehensive interpretability analysis of its generated \ndiagnostic reasoning process. Given the wide heterogeneity of rare diseases and substantial \nvariability in clinical features, we selected four representative disorders with well-documented \nphenotypic and diagnostic information-Wilson disease, Prader-Willi syndrome, Tuberous \nsclerosis complex, and Langerhans cell histiocytosis-as analytical cases to ensure \nrepresentativeness and interpretability. These diseases exhibit complex, multisystem \nphenotypes and diagnostic logic, making them well suited for both reasoning visualization and \nglobal surrogate modeling analyses. \nIn this analysis, the model not only generated final diagnostic outputs but also \nautomatically produced complete CoT reasoning traces75. Based on these reasoning texts, we \n"}, {"page": 43, "text": "conducted a feature-level visualization using word clouds to intuitively illustrate the model’s \nsalient attention to key clinical features across different disease types. Furthermore, a Global \nSurrogate Modeling approach was employed to transparently reveal the relationship between \ninput features and model predictions85. Specifically, a linear regression model was used as a \nsurrogate to approximate the diagnostic behavior of the LLM, with phenotypic features \nextracted from electronic health records and the model’s predicted outputs as inputs. By \nvisualizing the regression coefficients of the surrogate model, we identified the phenotypic \nfactors most strongly associated with diagnostic predictions, thereby providing a global-level \ninterpretation of the LLM’s reasoning logic and decision mechanisms. \nIn addition, we systematically investigated the underlying mechanisms through which \nphenotypic and non-phenotypic features contribute to the model’s diagnostic reasoning process. \nSpecifically, for cases in which the model produced correct diagnostic predictions, we \nemployed the DeepSeek-R1 model by providing, as input, the diagnostic reasoning chain of \neach case, the model’s predicted diagnosis, and the corresponding Orphanet disease definition \nand description. Leveraging its intrinsic reasoning capability and the standardized disease \ndefinitions, the model extracted clinically relevant information that substantially contributed to \nthe diagnostic decision-making process. Subsequently, all extracted features were semantically \nmapped to the corresponding Human Phenotype Ontology (HPO) terms using the HPO \nOntology Service (https://ontology.jax.org/api/hp/docs#/default/search). Features that could \nnot be successfully mapped were designated as non-phenotypic features. For these non-\nphenotypic features, we further applied the DeepSeek-R1 model to perform semantic \ncategorization, classifying them into multiple categories, such as Imaging findings, Clinical \ninterventions or procedures, Functional assessments, Genetic or molecular test results, and \nother relevant categories. Through this systematic pipeline, we quantitatively analyzed the \nmodel’s dependency patterns on non-phenotypic clinical information within the diagnostic \nreasoning process, thereby elucidating the latent mechanisms by which large language models \nincorporate diverse clinical evidence, beyond standardized HPO phenotypes, into rare disease \ndiagnostic decision-making. \n \nImplementation \n"}, {"page": 44, "text": "We implemented large LLM training by integrating LoRA81 and ZeRO-3 with the \nDeepSpeed framework86. Both the Domain-specific instruction tuning and CoT fine-tuning \nstages employed the same training strategy, ensuring consistent optimization and parameter-\nefficient adaptation across the two fine-tuning phases. LoRA reduces the number of trainable \nparameters by freezing the original pre-trained weights and injecting trainable low-rank \ndecomposition matrices into each layer of the Transformer architecture. Parameter-efficient \nadaptation was achieved through LoRA with a rank of 8 and α = 32 applied to all linear layers. \nThe model was fine-tuned for 3 epochs with a per-device batch size of 4 and a learning rate of \n1e-4. Attention computations utilized the FlashAttention implementation to optimize efficiency \nand stability, and training was performed in bfloat16 precision. A warmup ratio of 0.05 was \nused to gradually adapt the learning rate at the start of training, promoting stable convergence. \nFor inference, we utilized the vLLM library 87, which offers high efficiency in both memory \nutilization and computational resource management. \n \nStatistical analysis \n \nWe evaluated the diagnostic performance of RareSeek-R1 using Top-20 accuracy. For each \nmetric, the mean and standard error were calculated. Confidence intervals (CIs) were estimated \nusing a non-parametric bootstrap procedure with 1,000 resamples. In all analyses, a two-sided \nP value < 0.05 was considered statistically significant. Comparisons between models or input \nsettings were performed using two-sided t-tests to assess whether differences in diagnostic \naccuracy were statistically significant across datasets. \n \nData availability \nPublicly available data were collected from authoritative biomedical sources, including \nCHARD \n(https://www.chard.org.cn/), \nNORD \n(https://rarediseases.org/), \nOrphadata \n(https://www.orphadata.com/), \nOMIM \n(https://www.omim.org/), \nPubMed \nCentral \n(https://www.ncbi.nlm.nih.gov/), HPO annotations (https://hpo.jax.org/data/annotations) and \nthe Chinese Medical Association database (https://www.yiigle.com/index), as well as publicly \naccessible encyclopedic resources (https://en.wikipedia.org/wiki). Additional variant-level \ninformation was obtained from ClinVar (https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/) \n"}, {"page": 45, "text": "and the Human Gene Mutation Database (https://www.hgmd.cf.ac.uk/ac/index.php). Real-\nworld EHR data were obtained from clinical institutions with institutional review board \napproval. Due to privacy regulations and institutional policies, the de-identified EHRs cannot \nbe made publicly available. De-identified subsets for academic, non-commercial research may \nbe provided upon formal request to the corresponding author (Mulin Jun Li), following a \ndefined protocol for data request approval. \n \nCode availability \nThe LLMs were developed and deployed in Python (3.10) using PyTorch (2.6.0). The \nfollowing standard model libraries were used: numpy (1.26.4), scipy (1.15.2), matplotlib \n(3.10.1), transformers (4.51.3), tokenizers (0.21.1), vLLM (0.8.4) and DeepSpeed (0.16.5).  \n \n \n \n \n \n"}, {"page": 46, "text": "Reference \n1. \nThe Lancet Global, H. The landscape for rare diseases in 2024. Lancet Glob Health 12, \ne341 (2024). \n2. \nRehm, H.L. Time to make rare disease diagnosis accessible to all. Nat Med 28, 241-\n242 (2022). \n3. \nMarwaha, S., Knowles, J.W. & Ashley, E.A. A guide for the diagnosis of rare and \nundiagnosed disease: beyond the exome. Genome Med 14, 23 (2022). \n4. \nFeng, Y., Qi, L. & Tian, W. PhenoBERT: A Combined Deep Learning Method for \nAutomated Recognition of Human Phenotype Ontology. IEEE/ACM Trans Comput \nBiol Bioinform 20, 1269-1277 (2023). \n5. \nSmedley, D., et al. Next-generation diagnostics and disease-gene discovery with the \nExomiser. Nat Protoc 10, 2004-2015 (2015). \n6. \nMao, D., et al. AI-MARRVEL - A Knowledge-Driven AI System for Diagnosing \nMendelian Disorders. NEJM AI 1(2024). \n7. \nGargano, M.A., et al. The Human Phenotype Ontology in 2024: phenotypes around the \nworld. Nucleic Acids Res 52, D1333-D1346 (2024). \n8. \nLandrum, M.J., et al. ClinVar: improving access to variant interpretations and \nsupporting evidence. Nucleic Acids Res 46, D1062-D1067 (2018). \n9. \nDe La Vega, F.M., et al. Artificial intelligence enables comprehensive genome \ninterpretation and nomination of candidate diagnoses for rare genetic diseases. Genome \nMed 13, 153 (2021). \n10. \nMao, X., et al. A phenotype-based AI pipeline outperforms human experts in \ndifferentially diagnosing rare diseases using EHRs. NPJ Digit Med 8, 68 (2025). \n11. \nThirunavukarasu, A.J., et al. Large language models in medicine. Nat Med 29, 1930-\n1940 (2023). \n12. \nSinghal, K., et al. Large language models encode clinical knowledge. Nature 620, 172-\n180 (2023). \n13. \nAchiam, \nJ., \net \nal. \nGPT-4 \ntechnical \nreport. \nPreprint \nat \narXiv \nhttps://arxiv.org/abs/2303.08774 (2023). \n14. \nTouvron, H., et al. Llama 2: Open foundation and fine-tuned chat models. Preprint at \narXiv https://arxiv.org/abs/2307.09288 (2023). \n15. \nChowdhery, A., et al. Palm: Scaling language modeling with pathways. Preprint at \narXiv https://arxiv.org/abs/2204.02311 (2023). \n16. \nGuo, D., et al. DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement \nlearning. Nature 645, 633-638 (2025). \n17. \nTu, T., et al. Towards conversational diagnostic artificial intelligence. Nature 642, 442-\n450 (2025). \n18. \nSinghal, K., et al. Toward expert-level medical question answering with large language \nmodels. Nat Med 31, 943-950 (2025). \n19. \nLiu, X., et al. A generalist medical language model for disease diagnosis assistance. \nNat Med 31, 932-942 (2025). \n20. \nDou, C., et al. Baichuan-M2: Scaling Medical Capability with Large Verifier System. \nPreprint at arXiv https://arxiv.org/abs/2509.02208 (2025). \n21. \nQiu, P., et al. Towards building multilingual language model for medicine. Nat \n"}, {"page": 47, "text": "Commun 15, 8384 (2024). \n22. \nChen, Z., et al. Meditron-70b: Scaling medical pretraining for large language models. \nPreprint at arXiv https://arxiv.org/abs/2311.16079 (2023). \n23. \nToma, A., et al. Clinical Camel: An open expert-level medical language model with \ndialogue-based \nknowledge \nencoding. \nPreprint \nat \narXiv \nhttps://arxiv.org/abs/2305.12031 (2023). \n24. \nChen, X., et al. RareBench: can LLMs serve as rare diseases specialists? in Proceedings \nof the 30th ACM SIGKDD conference on knowledge discovery and data mining 4850-\n4861 (2024). \n25. \nReese, J.T., et al. Systematic benchmarking demonstrates large language models have \nnot reached the diagnostic accuracy of traditional rare-disease decision support tools. \n2024.2007. 2022.24310816 (2025). \n26. \nZhao, W., et al. An Agentic System for Rare Disease Diagnosis with Traceable \nReasoning. Preprint at arXiv https://arxiv.org/abs/2506.20430 (2025). \n27. \nReese, J.T., et al. On the limitations of large language models in clinical diagnosis. \n2023.2007. 2013.23292613 (2024). \n28. \nDanis, D., et al. A corpus of GA4GH phenopackets: Case-level phenotyping for \ngenomic diagnostics and discovery. HGG Adv 6, 100371 (2025). \n29. \nKim, J., Wang, K., Weng, C. & Liu, C. Assessing the utility of large language models \nfor phenotype-driven gene prioritization in the diagnosis of rare genetic disease. Am J \nHum Genet 111, 2190-2202 (2024). \n30. \nZhou, N., Wang, H., Zheng, Y. & Huang, D. Progressive Parameter Efficient Transfer \nLearning for Semantic Segmentation. In Proc. 13th International Conference on \nLearning Representations https://openreview.net/pdf?id=YNbLUGDAX5 (2025). \n31. \nStenson, P.D., et al. Human Gene Mutation Database (HGMD): 2003 update. Hum \nMutat 21, 577-581 (2003). \n32. \nHamosh, A., Scott, A.F., Amberger, J.S., Bocchini, C.A. & McKusick, V.A. Online \nMendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic \ndisorders. Nucleic Acids Res 33, D514-517 (2005). \n33. \nWeinreich, S.S., Mangon, R., Sikkens, J., Teeuw, M.E. & Cornel, M.J.N.t.v.g. Orphanet: \na European database for rare diseases. 152, 518-519 (2008). \n34. \nPhilippakis, A.A., et al. The Matchmaker Exchange: a platform for rare disease gene \ndiscovery. Hum Mutat 36, 915-921 (2015). \n35. \nRobinson, P.N., et al. Interpretable Clinical Genomics with a Likelihood Ratio \nParadigm. Am J Hum Genet 107, 403-417 (2020). \n36. \nRonicke, S., et al. Can a decision support system accelerate rare disease diagnosis? \nEvaluating the potential impact of Ada DX in a retrospective study. Orphanet J Rare \nDis 14, 69 (2019). \n37. \nTöpel, T., Scheible, D., Trefz, F. & Hofestädt, R. RAMEDIS: a comprehensive \ninformation system for variations and corresponding phenotypes of rare metabolic \ndiseases. Hum Mutat 31, E1081-1088 (2010). \n38. \nGrattafiori, A., et al. The llama 3 herd of models. Preprint at arXiv \nhttps://arxiv.org/abs/2407.21783 (2024). \n39. \nHandler, R., Sharma, S. & Hernandez-Boussard, T. The fragile intelligence of GPT-5 \n"}, {"page": 48, "text": "in medicine. Nat Med (2025). \n40. \nTemsah, M.H., Jamal, A., Alhasan, K., Temsah, A.A. & Malki, K.H. OpenAI o1-\nPreview vs. ChatGPT in Healthcare: A New Frontier in Medical AI Reasoning. Cureus \n16, e70640 (2024). \n41. \nZhong, T., et al. Evaluation of OpenAI o1: Opportunities and Challenges of AGI. \nPreprint at arXiv https://arxiv.org/abs/2409.18486 (2024). \n42. \nTeam, \nQ.J.a.p.a. \nQwen2 \ntechnical \nreport. \nPreprint \nat \narXiv \nhttps://arxiv.org/abs/2407.10671 (2024). \n43. \nChen, J., et al. Huatuogpt-o1, towards medical complex reasoning with llms. Preprint \nat arXiv https://arxiv.org/abs/2412.18925 (2024). \n44. \nWang, B., et al. Baichuan-m1: Pushing the medical capability of large language models. \nPreprint at arXiv https://arxiv.org/abs/2502.12671 (2025). \n45. \nWen, B., Shi, S., Long, Y., Dang, Y. & Tian, W. PhenoDP: leveraging deep learning for \nphenotype-based case reporting, disease ranking, and symptom recommendation. \nGenome Med 17, 67 (2025). \n46. \nZhai, W., Huang, X., Shen, N. & Zhu, S. Phen2Disease: a phenotype-driven model for \ndisease and gene prioritization by bidirectional maximum matching semantic \nsimilarities. Brief Bioinform 24, bbad172 (2023). \n47. \nJagadeesh, K.A., et al. Phrank measures phenotype sets similarity to greatly improve \nMendelian diagnostic disease prioritization. Genet Med 21, 464-470 (2019). \n48. \nShefchek, K.A., et al. The Monarch Initiative in 2019: an integrative data and analytic \nplatform connecting phenotypes to genotypes across species. Nucleic Acids Res 48, \nD704-D715 (2020). \n49. \nLuo, L., et al. PhenoTagger: a hybrid method for phenotype concept recognition using \nhuman phenotype ontology. Bioinformatics 37, 1884-1890 (2021). \n50. \nDeisseroth, C.A., et al. ClinPhen extracts and prioritizes patient phenotypes directly \nfrom medical records to expedite genetic disease diagnosis. Genet Med 21, 1585-1593 \n(2019). \n51. \nPapineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: a method for automatic \nevaluation of machine translation. In Proc. 40th annual meeting of the Association for \nComputational Linguistics (eds Isabelle, P. et al.) 311–318 (ACL, 2002). \n52. \nLin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Proc. Text \nSummarization Branches Out 74–81 (ACL, 2004). \n53. \nGarcia, B.T., et al. Improving automated deep phenotyping through large language \nmodels using retrieval-augmented generation. Genome Med 17, 91 (2025). \n54. \nTambuyzer, E., et al. Therapies for rare diseases: therapeutic modalities, progress and \nchallenges ahead. Nat Rev Drug Discov 19, 93-111 (2020). \n55. \nZheng, Q., et al. Large-scale long-tailed disease diagnosis on radiology images. Nat \nCommun 15, 10147 (2024). \n56. \nBauskis, A., Strange, C., Molster, C. & Fisher, C.J.O.j.o.r.d. The diagnostic odyssey: \ninsights from parents of children living with an undiagnosed condition. 17, 233 (2022). \n57. \nAlsentzer, E., Finlayson, S.G., Li, M.M., Kobren, S.N. & Kohane, I.S. Simulation of \nundiagnosed patients with novel genetic conditions. Nat Commun 14, 6403 (2023). \n58. \nHalverson, C.M., Cao, S., Perkins, S.M. & Francomano, C.A.J.G.i.M.O. Comorbidity, \n"}, {"page": 49, "text": "misdiagnoses, and the diagnostic odyssey in patients with hypermobile Ehlers-Danlos \nsyndrome. 1, 100812 (2023). \n59. \nSchmidt, A., et al. Next-generation phenotyping integrated in a national framework for \npatients with ultrarare disorders improves genetic diagnostics and yields new molecular \nfindings. Nat Genet 56, 1644-1653 (2024). \n60. \nCrooke, S.T. Meeting the needs of patients with ultrarare diseases. Trends Mol Med 28, \n87-96 (2022). \n61. \nSong, J., Xu, Z., He, M., Feng, J. & Shen, B. Graph retrieval augmented large language \nmodels for facial phenotype associated rare genetic disease. NPJ Digit Med 8, 543 \n(2025). \n62. \nThorsen-Meyer, H.C., et al. Dynamic and explainable machine learning prediction of \nmortality in patients in the intensive care unit: a retrospective study of high-frequency \ndata in electronic patient records. Lancet Digit Health 2, e179-e191 (2020). \n63. \nLauritsen, S.M., et al. Explainable artificial intelligence model to predict acute critical \nillness from electronic health records. Nat Commun 11, 3852 (2020). \n64. \nRajpurkar, P., Chen, E., Banerjee, O. & Topol, E.J.J.N.m. AI in health and medicine. \n28, 31-38 (2022). \n65. \nShribman, S., Poujois, A., Bandmann, O., Czlonkowska, A. & Warner, T.T. Wilson's \ndisease: update on pathogenesis, biomarkers and treatments. J Neurol Neurosurg \nPsychiatry 92, 1053-1061 (2021). \n66. \nTauber, M. & Hoybye, C. Endocrine disorders in Prader-Willi syndrome: a model to \nunderstand and treat hypothalamic dysfunction. Lancet Diabetes Endocrinol 9, 235-\n246 (2021). \n67. \nBoycott, K.M., et al. International Cooperation to Enable the Diagnosis of All Rare \nGenetic Diseases. Am J Hum Genet 100, 695-705 (2017). \n68. \nGreene, D., et al. Genetic association analysis of 77,539 genomes reveals rare disease \netiologies. Nat Med 29, 679-688 (2023). \n69. \nTurro, E., et al. Whole-genome sequencing of patients with rare diseases in a national \nhealth system. Nature 583, 96-102 (2020). \n70. \nLu, M.Y., et al. A visual-language foundation model for computational pathology. Nat \nMed 30, 863-874 (2024). \n71. \nHuang, Z., Bianchi, F., Yuksekgonul, M., Montine, T.J. & Zou, J. A visual-language \nfoundation model for pathology image analysis using medical Twitter. Nat Med 29, \n2307-2316 (2023). \n72. \nZhou, H.Y., et al. A transformer-based representation-learning model with unified \nprocessing of multimodal input for clinical diagnostics. Nat Biomed Eng 7, 743-755 \n(2023). \n73. \nNguyen, E., et al. Sequence modeling and design from molecular to genome scale with \nEvo. Science 386, eado9336 (2024). \n74. \nCanese, K. & Weis, S. PubMed: the bibliographic database. In The NCBI Handbook \n2nd edn (National Center for Biotechnology Information, 2013). \n75. \nWei, J., et al. Chain-of-thought prompting elicits reasoning in large language models. \nIn Proc. 36th International Conference on Neural Information Processing Systems (eds \nKoyejo, S. et al.) 24824–24837 (Curran Associates, 2022). \n"}, {"page": 50, "text": "76. \nJacobsen, J.O.B., et al. The GA4GH Phenopacket schema defines a computable \nrepresentation of clinical data. Nat Biotechnol 40, 817-820 (2022). \n77. \nMecklenburg, N., et al. Injecting new knowledge into large language models via \nsupervised fine-tuning. Preprint at arXiv https://arxiv.org/abs/2404.00213 (2024). \n78. \nSandmann, S., et al. Benchmark evaluation of DeepSeek large language models in \nclinical decision-making. Nat Med 31, 2546-2549 (2025). \n79. \nTordjman, M., et al. Comparative benchmarking of the DeepSeek large language model \non medical tasks and clinical reasoning. Nat Med 31, 2550-2555 (2025). \n80. \nChung, H.W., et al. Scaling instruction-finetuned language models. J. Mach. Learn. \nRes. 25, 1-53 (2024). \n81. \nHu, E.J., et al. Lora: Low-rank adaptation of large language models. In Proc. 10th \nInternational \nConference \non \nLearning \nRepresentations \nhttps://openreview.net/forum?id=nZeVKeeFYf9 (ICLR, 2022). \n82. \nLewis, P., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. In \nProc. 34th International Conference on Neural Information Processing Systems 9459-\n9474 (Curran Associates, 2020). \n83. \nGao, Y., et al. Retrieval-augmented generation for large language models: A survey. \nPreprint at arXiv https://arxiv.org/abs/2312.10997 (2023). \n84. \nEdge, D., et al. From local to global: A graph rag approach to query-focused \nsummarization. Preprint at arXiv https://arxiv.org/abs/2404.16130 (2024). \n85. \nSigut, J., et al. Interpretable surrogate models to approximate the predictions of \nconvolutional neural networks in glaucoma diagnosis. Mach. Learn. Sci. Technol. 4, \n045024 (2023). \n86. \nRasley, J., Rajbhandari, S., Ruwase, O. & He, Y. Deepspeed: System optimizations \nenable training deep learning models with over 100 billion parameters. In Proc. 26th \nACM SIGKDD international conference on knowledge discovery & data mining \nhttps://openreview.net/forum?id=uRJ2qirIHs (2020). \n87. \nKwon, W., et al. Efficient memory management for large language model serving with \npagedattention. In Proc. 29th symposium on operating systems principles 611–626 \n(Association for Computing Machinery, 2023). \n \n"}]}