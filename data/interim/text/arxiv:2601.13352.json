{"doc_id": "arxiv:2601.13352", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.13352.pdf", "meta": {"doc_id": "arxiv:2601.13352", "source": "arxiv", "arxiv_id": "2601.13352", "title": "LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction", "authors": ["Yuxing Lu", "J. Ben Tamo", "Weichen Zhao", "Nan Sun", "Yishan Zhong", "Wenqi Shi", "Jinzhuo Wang", "May D. Wang"], "published": "2026-01-19T19:41:39Z", "updated": "2026-01-19T19:41:39Z", "summary": "Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.13352v1", "url_pdf": "https://arxiv.org/pdf/2601.13352.pdf", "meta_path": "data/raw/arxiv/meta/2601.13352.json", "sha256": "2ac4ab826965f99f6e60de38253f4ec4424b12fe732fefacdfe58ce1bd261bdf", "status": "ok", "fetched_at": "2026-02-18T02:21:04.508817+00:00"}, "pages": [{"page": 1, "text": "LLM-as-RNN: A Recurrent Language Model for Memory Updates and\nSequence Prediction\nYuxing Lu*12, J. Ben Tamo*1, Weichen Zhao3, Nan Sun4, Yishan Zhong1, Wenqi Shi5,\nJinzhuo Wang†2, May D. Wang†1\n1Georgia Institute of Technology, 2Peking University, 3Shandong University,\n4Huazhong University of Science and Technology, 5UT Southwestern Medical Center\nAbstract\nLarge language models are strong sequence\npredictors, yet standard inference relies on im-\nmutable context histories. After making an\nerror at generation step t, the model lacks an\nupdatable memory mechanism that improves\npredictions for step t+1. We propose LLM-as-\nRNN, an inference-only framework that turns\na frozen LLM into a recurrent predictor by rep-\nresenting its hidden state as natural-language\nmemory. This state, implemented as a struc-\ntured system-prompt summary, is updated at\neach timestep via feedback-driven text rewrites,\nenabling learning without parameter updates.\nUnder a fixed token budget, LLM-as-RNN cor-\nrects errors and retains task-relevant patterns,\neffectively performing online learning through\nlanguage. We evaluate the method on three\nsequential benchmarks in healthcare, meteorol-\nogy, and finance across Llama, Gemma, and\nGPT model families. LLM-as-RNN signifi-\ncantly outperforms zero-shot, full-history, and\nMemPrompt baselines, improving predictive\naccuracy by 6.5% on average, while producing\ninterpretable, human-readable learning traces\nabsent in standard context accumulation.\n1\nIntroduction\nLearning from sequential feedback is fundamental\nto adaptive prediction (Zhang et al., 2024b; Jiang\net al., 2024). Historically, this requirement was\nmet by Recurrent Neural Networks (RNNs) and\nLong Short-Term Memory (LSTMs), which main-\ntained a compact, evolving hidden state to capture\ntemporal dependencies and adapt to shifting data\ndistributions (Hochreiter and Schmidhuber, 1997;\nCho et al., 2014). The advent of Transformer archi-\ntectures revolutionized this landscape by replacing\nrecurrence with large-scale parallel attention mech-\nanisms. In this paradigm, \"memory\" is no longer a\ncompressed state, but an explicit history of tokens\n*Equal contribution.\n†Corresponding author.\nprocessed via In-Context Learning (ICL) (Brown\net al., 2020). This shift has enabled remarkable ad-\nvances in reasoning and generation across diverse\nopen and proprietary model families (Wu et al.,\n2025; Du et al., 2025).\nHowever, this architectural trade-off introduces\na critical limitation in long-horizon settings. Dur-\ning standard inference, Large Language Models\n(LLMs) operate in a largely stateless way: with\nfrozen parameters, the system lacks a mutable\nmemory to internalize past mistakes (Shinn et al.,\n2023; Packer et al., 2023). Instead of updating a\nbelief state, the model relies on an append-only con-\ntext window, carrying errors forward without cor-\nrection (Wang et al., 2024; Muhoberac et al., 2025).\nThis limitation becomes acute in domains such as\nlongitudinal clinical prediction, weather forecast-\ning, and financial time-series modeling, where task-\nrelevant signals accumulate over time, and the data\ndistribution may drift.\nA common solution is to encode the entire past\ndirectly in the prompt. One approach, Full His-\ntory Concatenation (FHC) (Ascoli and Choi, 2025),\nappends all raw observations, while methods like\nMemPrompt (Madaan et al., 2022) append a step-\nwise summary. As the sequence grows, concate-\nnation suffers from attention dilution and ’lost-in-\nthe-middle’ phenomena (Liu et al., 2024), while\nappend-only summaries are prone to error cascad-\ning (Zhang et al., 2024a). Once a misconception is\nwritten into the context, it becomes an immutable\nground truth; later evidence often fails to override\nit, causing errors to persist despite contradictory\nsignals (Turpin et al., 2023).\nWe address this gap with LLM-as-RNN, an\ninference-only framework that reframes sequential\nprediction as a recurrent process with a natural-\nlanguage state. Unlike append-only approaches,\nour method updates a structured memory at each\nstep using feedback, derived from ground-truth la-\nbels or an LLM critic, to correct errors and refine\n1\narXiv:2601.13352v1  [cs.CL]  19 Jan 2026\n"}, {"page": 2, "text": "Figure 1: Illustrative comparison. (a) Simple LLM lacks memory. (b) LLM with Context suffers from growing input\nsize. (c) LLM-as-RNN uses an iterative memory state to summarize historical information from evaluating outputs.\nstrategies under a fixed token budget. We evaluate\nthis approach on three diverse benchmarks: clinical\nprediction (MIMIC-IV), meteorology (Weather),\nand financial forecasting (S&P 500). Across all do-\nmains, LLM-as-RNN outperforms zero-shot, full-\nhistory concatenation, and MemPrompt baselines,\nwith especially large gains on long sequences. It\nachieves improvements of 10.8% on MIMIC-IV,\n1.6% on Weather, and 4.8% on S&P 500, while\nproducing interpretable learning traces that make\nthe model’s adaptation process transparent.\nThis work makes three contributions: (1) We\nformalize recurrent inference for LLMs, treating\ntextual state as an explicit, mutable memory. This\nperspective fundamentally distinguishes revisable\nmemory updates from standard unbounded history\naccumulation. (2) We introduce LLM-as-RNN, an\ninference-only framework that enables online adap-\ntation in frozen models. By iteratively rewriting a\nbounded natural-language state using per-timestep\nfeedback, the model corrects errors without pa-\nrameter access. (3) We demonstrate across three\ndomains and multiple model families that outcome-\ndriven state updates consistently outperform strong\nprompt-based baselines. Furthermore, by exposing\nthe adaptation process as human-readable state evo-\nlution, our framework facilitates safety audits and\nbuilds trust, ensuring that the model’s reasoning\ntrajectory is transparent rather than implicit.\n2\nRelated Work\n2.1\nRecurrent and Memory Models\nClassical sequence modeling uses recurrent neu-\nral networks (RNNs), long short-term memory\n(LSTMs), and gated recurrent units (GRUs) to\nmaintain a vector-valued hidden state that evolves\nover time (Elman, 1990; Hochreiter and Schmid-\nhuber, 1997; Chung et al., 2014; Graves et al.,\n2013; Sutskever et al., 2014; Bahdanau et al., 2015).\nWhile efficient, these dense vector states often\nact as an information bottleneck. To address this,\nmemory-augmented architectures, such as Neural\nTuring Machines and Differentiable Neural Com-\nputers, separated the controller from an external\ndifferentiable memory bank to support algorith-\nmic reasoning and long-term dependencies (We-\nston et al., 2014; Graves et al., 2014, 2016; Santoro\net al., 2016).\nThe Transformer architecture replaced this ex-\nplicit recurrence with self-attention over a global\ncontext (Vaswani et al., 2017). While powerful, the\nquadratic cost of attention has sparked a resurgence\nof interest in linear-time recurrent architectures.\nRecent models like RWKV (Peng et al., 2023),\nMamba (Gu and Dao, 2024), and linear attention\nvariants (Katharopoulos et al., 2020) effectively\nreintroduce recurrence into the Transformer back-\nbone, formalizing decoder-only models as multi-\nstate RNNs (Arora et al., 2024; Oren et al., 2024).\nHowever, these approaches typically require\ntraining custom architectures from scratch. In the\nregime of frozen large language models, recurrence\nis simulated via prompt management. Recurrent-\nGPT (Zhou et al., 2023) and MemGPT (Packer\net al., 2023) emulate RNNs by treating the context\nwindow as a short-term buffer and offloading his-\ntory to external storage. LLM-as-RNN builds upon\nthis stateful perspective but distinguishes itself by\nrepresenting the recurrent state not as a latent vector\nor a static storage log, but as an evolving, natural-\nlanguage system prompt.\n2\n"}, {"page": 3, "text": "Time Step t-1\nContextualization\nReflection\nMemory Update\nTime Step t+1\nTime Step t\nMemory State ht\n(updated summary to \nnext step)\nMemory State\nht-1 (e.g., historical \nsummary, key facts)\nInput Text xt\n(e.g., user query, \ndocument snippet)\nContext\nCt\nOutput Text\nyt (e.g., generated \nanswer, response)\nFeedback\nRefection \nCriterion Rt\n(e.g., reference answer)\nFeedback Text et\n(e,g. natural language \ncritique, score)\nLLM\nfθ\nContext \nFusion\n(integrates \nhistory & input)\n(generate text)\nMemory State\nUpdate fθ\n(computes new state based \non feedback)\nEvaluation LLM\ngeval (assesses quality, \ngenerates critique)\nFigure 2: Overview of LLM-as-RNN framework. At each time step, the system fuses the previous memory state\nwith new input to generate a response, evaluates that response to create a feedback signal, and then updates the\nnatural language memory state to guide future interactions.\n2.2\nInference-Time Adaptation in LLMs\nLLMs exhibit strong in-context learning capability,\nadapting to new tasks from a handful of demonstra-\ntions without parameter updates. This behavior has\nbeen interpreted as implicit optimization or meta-\nlearning implemented in the forward pass (Brown\net al., 2020; Akyürek et al., 2022; Garg et al., 2022;\nVon Oswald et al., 2023; Dai et al., 2023). To\nextend this beyond the context window, retrieval-\naugmented generation (RAG) and non-parametric\nsystems utilize external memory banks to query\nrelevant history at inference time (Khandelwal\net al., 2020; Lewis et al., 2020; Borgeaud et al.,\n2022; Izacard et al., 2023). Recent agentic frame-\nworks extend this by storing high-level “events” or\nuser profiles to support long-horizon personaliza-\ntion (Park et al., 2023; Zhong et al., 2024; Das\net al., 2024; Wang et al., 2023). However, these\napproaches are primarily retrieval-based: they se-\nlect relevant past information but do not necessarily\nupdate a belief state to correct errors.\nA complementary stream of research treats nat-\nural language itself as an optimization variable.\nMethods like Self-Refine (Madaan et al., 2023),\nReflexion (Shinn et al., 2023), and ReAct (Yao\net al., 2022) introduce iterative feedback loops\nwhere the model critiques its own output to im-\nprove performance. This concept has been formal-\nized in frameworks like OPRO (Yang et al., 2024)\nand TextGrad (Yuksekgonul et al., 2024), which\nperform \"optimization via prompting\", effectively\nbackpropagating textual feedback to refine system\nprompts or solutions.\nUnlike RAG, which retrieves static history, and\ntraditional continual learning, which relies on ex-\npensive parameter updates (Wu et al., 2024), our\napproach performs adaptation purely at inference\ntime. By treating the system prompt as a recurrent\nhidden state and updating it through error-driven\nfeedback, we combine recurrent statefulness with\nfeedback-based adaptation in frozen LLMs.\n3\nMethods\n3.1\nPreliminaries and Problem Formulation\nWe consider sequential generation over a horizon\nT, where the cumulative full history HT may ex-\nceed the LLM’s context window Lmax. At each\nstep t, the model (parameterized by θ) receives\nan observation xt, the previous history Ht−1 =\n{x1:t−1, ˆy1:t−1}, and generates a response ˆyt.\nˆyt ∼fθ(· | Ht−1, xt)\n(1)\nThis formulation suffers from two limitations:\n(1) Computational: |Ht−1 ⊕xt| grows linearly,\neventually violating Lmax; and (2) Statelessness:\nHt−1 is an append-only log. Errors in early out-\nputs ˆy1:t−1 are frozen in the context, permanently\nbiasing future predictions.\nTo address this, we propose LLM-as-RNN (Fig-\nure 2), which reformulates inference as a recurrent\nprocess over a mutable textual state ht. Our goal is\nto maintain a bounded memory ht−1, constrained\nby a fixed token budget λ (where λ ≪Lmax), that\nacts as a semantic sufficient statistic for the full\nhistory. We seek a memory state that minimizes\ninformation loss, ensuring the state-conditioned\nprediction approximates the full-history:\nPθ(ˆyt|ht−1, xt) ≈Pθ(ˆyt|Ht−1, xt)\n(2)\n3\n"}, {"page": 4, "text": "3.2\nRecurrent Inference Mechanism\nThe inference process at step t is decomposed into\nthree atomic operations: Contextualization, Reflec-\ntion, and Memory Update.\n3.2.1\nStep 1: Contextualization\nUnlike vector-based RNNs, which fuse inputs\nvia matrix multiplication, LLM-as-RNN performs\nstate mixing directly in the token space. We define\na prompt template Pgen that constructs a local con-\ntext Ct by concatenating (⊕) the system instruc-\ntions Isys, the prior memory state ht−1, and the\nnew observation xt:\nCt = Isys ⊕ht−1 ⊕xt\n(3)\nThe model then samples a response ˆyt conditioned\non this bounded context:\nˆyt ∼fθ(·|Pgen(Ct))\n(4)\nThis formulation ensures that the context size\nremains constant (O(1)) with respect to the se-\nquence index t. Unlike full-history methods where\nattention costs grow linearly with time, our input\nlength depends only on the bounded memory size λ\nand current observation |xt|, preventing throughput\ndegradation in long-horizon tasks.\n3.2.2\nStep 2: Reflection\nTo ensure the memory ht remains accurate over\nlong horizons, we introduce a feedback mecha-\nnism that acts as a \"semantic gradient\", guiding\nthe evolution of the memory state. We define a\nCritic function geval that evaluates the prediction\nˆyt against a reference criterion Rt, producing a\nnatural language feedback signal et:\net = geval(ˆyt, Rt)\n(5)\nWe formulate geval to handle 2 supervision modes:\nSupervised mode: Rt contains the ground truth la-\nbel yt. Here, geval computes the semantic residual:\net ←“Error: Expected yt but generated ˆyt.”\nOpen-Ended\nmode:\nRt\nrepresents\na\nset\nof\nquality\nheuristics\n(e.g.,\nrelevance,\nco-\nherence).\nHere,\ngeval\nacts\nas\nan\nLLM-\nas-a-Judge,\nproducing\na\ncritique:\net\n←\n“Reasoning flaw: ˆyt contradicts prior fact . . . ”\nThe signal et guides the subsequent memory up-\ndate, analogous to the backpropagated error term\nin differentiable memory networks.\n3.2.3\nStep 3: Memory Update\nThe final step is the memory update, where we\ntransition from ht−1 →ht to incorporate new in-\nformation while satisfying the token budget λ. This\noperation is modeled not just as summarization, but\nas a feedback-guided rewrite.\nUsing a specific prompt template Pmem, the\nmodel generates the new state conditioned on the\nprevious state, current events, and the feedback\nsignal st:\nht ∼fθ(· | Pmem(ht−1, xt, ˆyt, et))\n(6)\nThe prompt Pmem explicitly instructs the model\nto:\n1. Compress xt and ˆyt into the summary.\n2. Apply the Critique: Use et to identify and\nrewrite incorrect beliefs in ht−1 rather than\nsimply appending new tokens.\nThis formulation ensures the memory is self-\nhealing: the state evolves to correct misconceptions\nbased on the \"semantic gradient\" et.\n3.3\nAlgorithm\nThe complete inference procedure, integrating the\nsemantic gradient loop and constraint enforcement,\nis detailed in Algorithm 1.\nAlgorithm 1 LLM-as-RNN Inference Process\nRequire: Sequence {xt}T\nt=1, Frozen LLM θ, Eval-\nuation Function geval, Max Memory λ\n1: h0 ←∅\n2: for t = 1 to T do\n3:\n// Phase 1: Contextualization\n4:\nCt ←Isys ⊕ht−1 ⊕xt\n5:\nˆyt ∼fθ(·|Pgen(Ct)) {Generate Prediction}\n6:\n// Phase 2: Reflection\n7:\nRetrieve reference/criteria Rt\n8:\net ←geval(ˆyt, Rt) {Semantic Evaluation}\n9:\n// Phase 3: Memory Update\n10:\nUpdate ht ←fθ(·|Pmem(ht−1, xt, ˆyt, et))\n11:\nif |ht| > λ then\n12:\nht ←Compress(ht, λ)\n13:\nend if\n14: end for\n15: return {ˆyt}T\nt=1\n4\n"}, {"page": 5, "text": "4\nExperiments\n4.1\nDatasets\nWe evaluate LLM-as-RNN on three sequential\nbenchmarks spanning healthcare, meteorology,\nand finance: the MIMIC-IV dataset, the Weather\ndataset, and the S&P 500 with Financial News\nHeadlines dataset. Following a unified protocol,\nwe structure the weather and finance benchmarks\nas continuous temporal streams rather than inde-\npendent samples. Additional dataset statistics and\npreprocessing details are provided in Appendix A.\nMIMIC-IV.\nThe MIMIC-IV dataset is a deiden-\ntified EHR dataset containing ICU admissions with\nstructured clinical variables (e.g., diagnoses, pro-\ncedures, labs, treatments) (Johnson et al., 2023).\nWe construct longitudinal patient trajectories con-\nsisting of sequential clinical notes and lab events\n(Appendix B). The task is to predict the primary di-\nagnosis for the next hospital visit given the history\nof prior admissions.\nWeather.\nThe Weather dataset is a meteorolog-\nical time series containing mixed modalities, in-\ncluding textual descriptors and continuous mea-\nsurements (e.g., temperature, humidity, wind, pres-\nsure) (Muthukumar, 2023). We employ a sequen-\ntial sliding-window protocol: at each timestep t,\nthe observation xt is restricted to a fixed 5-day win-\ndow. However, the model predicts the condition for\nday t by conditioning on both this local window\nand the recurrent memory ht−1.\nS&P 500.\nThe S&P 500 dataset aligns daily\nmarket closing prices with financial news head-\nlines (Mahaptra, 2024). Under the same protocol,\nthe model receives a 5-day lookback of price and\nnews as input xt. The task is to forecast the closing\nprice for day t, synthesizing quantitative signals\nwith qualitative sentiment accumulated over the\nentire sequence.\n4.2\nBaselines\nWe compare LLM-as-RNN against three baselines.\nAll methods share the same input/output formatting\nand evaluation protocol; they differ only in how\nthey encode history.\nZero-shot.\nThis baseline (Figure 1a) predicts the\ntarget at time t using only the most recent observa-\ntion (e.g., the last visit/day) without any additional\nhistory. It serves as a lower bound that tests the\nLLM’s raw single-step predictive ability.\nFull History Concatenation (FHC).\nFHC (As-\ncoli and Choi, 2025) (Figure 1b) conditions the\nLLM on the entire available history by directly con-\ncatenating all past observations into the prompt at\neach timestep:\nCt = x1 ⊕x2 ⊕· · · ⊕xt−1 ⊕xt.\n(7)\nFHC is the most straightforward strategy for lever-\naging temporal context and is commonly used as\na long-context baseline, but its input length grows\nwith t and can exceed the model’s context window,\nnecessitating truncation and potentially degrading\nperformance.\nMemPrompt.\nMemPrompt (Madaan et al., 2022)\nsummarizes each past observation into a short tex-\ntual memory unit and concatenates the accumulated\nsummaries as a compact proxy for the full history:\nmi = Summarize(xi),\nCt = m1 ⊕m2 ⊕· · · ⊕mt−1 ⊕xt.\n(8)\nUnlike FHC, MemPrompt bounds history growth\nvia per-step compression. However, the memory\nis append-only, previously written summaries are\nnot revised in light of new evidence or prediction\nerrors, so misconceptions can persist over time.\n4.3\nLLM Backbones\nWe evaluate LLM-as-RNN with multiple back-\nbone LLMs to assess how backbone capabil-\nity affects sequential prediction performance\n(Llama (Grattafiori et al., 2024), Gemma (Team\net al., 2025), and GPT (Agarwal et al., 2025) fam-\nilies). To ensure fair comparisons across back-\nbones, we use temperature = 0.7, top-p = 0.9,\nand max_tokens = 4096 for all LLM calls, and\nkeep these settings unchanged across timesteps in\nthe sequential stream. All LLM-based evaluations\nare computed using the same judge model, Claude\nSonnet 4.5 (Anthropic, 2025).\n4.4\nMetrics\nWe employ task-specific metrics:\n• MIMIC-IV (Semantic Accuracy): We report\nLLM-Judged Accuracies (Acc@1 and Acc@5).\nThe LLM-Judge determines if the generated di-\nagnosis is semantically equivalent to the ground\ntruth, avoiding the pitfalls of string matching.\n• Weather (Alignment): The LLM judge eval-\nuates whether the generated summary factually\n5\n"}, {"page": 6, "text": "Table 1: Overall performance. Green = best backbone within each method; Yellow = best overall in the full table.\nMethod\nLLM backbones\nMIMIC-IV\nWeather\nS&P 500\nAcc@1(↑)\nAcc@5(↑)\nAlign(↑)\nMAE(↓)\nMSE(↓)\nZero-shot\nLlama-3.2-1B\n0.1538\n0.3846\n0.6783\n1.598\n6.273\nLlama-3.2-3B\n0.1958\n0.4545\n0.6923\n1.585\n6.080\nLlama-3.1-8B\n0.2797\n0.5594\n0.7063\n1.499\n5.343\nLlama-3.1-70B\n0.3287\n0.6503\n0.7203\n1.493\n5.232\nGemma-3-1B\n0.1608\n0.3986\n0.6853\n1.612\n6.347\nGemma-3-4B\n0.2238\n0.5175\n0.7063\n1.548\n5.982\nGemma-3-12B\n0.2937\n0.6014\n0.7273\n1.438\n5.214\nGemma-3-27B\n0.3427\n0.6713\n0.7552\n1.402\n4.972\nGPT-oss-20B\n0.3357\n0.6643\n0.7413\n1.305\n4.590\nGPT-oss-120B\n0.3636\n0.7063\n0.7483\n1.312\n4.612\nFHC\nLlama-3.2-1B\n0.2517\n0.6434\n0.7133\n1.609\n5.738\nLlama-3.2-3B\n0.3357\n0.7552\n0.7133\n1.578\n5.503\nLlama-3.1-8B\n0.3497\n0.8392\n0.7203\n1.440\n5.317\nLlama-3.1-70B\n0.4126\n0.8601\n0.7273\n1.464\n5.211\nGemma-3-1B\n0.2587\n0.6503\n0.7133\n1.603\n6.042\nGemma-3-4B\n0.3566\n0.8112\n0.7273\n1.511\n5.541\nGemma-3-12B\n0.4056\n0.8601\n0.7483\n1.412\n5.067\nGemma-3-27B\n0.4476\n0.9021\n0.7832\n1.386\n4.881\nGPT-oss-20B\n0.4126\n0.8671\n0.7902\n1.280\n4.420\nGPT-oss-120B\n0.4406\n0.8881\n0.7972\n1.290\n4.440\nMemPrompt\nLlama-3.2-1B\n0.3147\n0.7203\n0.7203\n1.478\n5.879\nLlama-3.2-3B\n0.4126\n0.7902\n0.7273\n1.426\n5.148\nLlama-3.1-8B\n0.4336\n0.8462\n0.7343\n1.309\n4.768\nLlama-3.1-70B\n0.4615\n0.9231\n0.7413\n1.292\n4.417\nGemma-3-1B\n0.3217\n0.7273\n0.7273\n1.491\n6.118\nGemma-3-4B\n0.4196\n0.8392\n0.7483\n1.392\n5.012\nGemma-3-12B\n0.4755\n0.8951\n0.7832\n1.298\n4.716\nGemma-3-27B\n0.4965\n0.9091\n0.8182\n1.264\n4.090\nGPT-oss-20B\n0.4895\n0.9021\n0.8112\n1.230\n4.120\nGPT-oss-120B\n0.5175\n0.9161\n0.8322\n1.198\n4.162\nLLM-as-RNN\n(Ours)\nLlama-3.2-1B\n0.3427\n0.7552\n0.7343\n1.531\n5.297\nLlama-3.2-3B\n0.4545\n0.7972\n0.7413\n1.347\n5.025\nLlama-3.1-8B\n0.5524\n0.8531\n0.7483\n1.287\n4.517\nLlama-3.1-70B\n0.5804\n0.9510\n0.7622\n1.211\n4.313\nGemma-3-1B\n0.3497\n0.7622\n0.7413\n1.533\n5.541\nGemma-3-4B\n0.4685\n0.8601\n0.7622\n1.321\n4.754\nGemma-3-12B\n0.5594\n0.9161\n0.8252\n1.237\n4.406\nGemma-3-27B\n0.6434\n0.9301\n0.8182\n1.186\n4.072\nGPT-oss-20B\n0.5874\n0.9231\n0.8112\n1.105\n3.900\nGPT-oss-120B\n0.6294\n0.9441\n0.8182\n1.112\n3.821\ncontradicts the ground truth on key variables, pro-\nducing a binary success score.\n• S&P 500 (Forecasting Error): We measure the\ndeviation between predicted and actual closing\nprices using Mean Absolute Error (MAE) and\nMean Squared Error (MSE).\n5\nResults\nOur evaluation is guided by 4 research questions:\n• RQ1 (Efficacy): Does the LLM-as-RNN frame-\nwork outperform other strategies, and how does\ndifferent LLM backbones behave?\n• RQ2 (Temporal Dynamics): Does the model\nlearn and reduce error over long sequences?\n• RQ3 (Autonomy): Can the framework operate\neffectively using intrinsic self-correction (LLM-\nas-a-Judge) in the absence of ground truth labels?\n• RQ4 (Mechanism): What are the contributions\nof the memory to the overall performance?\n5.1\nOverall Performance Analysis (RQ1)\nWe analyze whether outcome-driven memory up-\ndates can effectively overcome the shortcomings\nof static context accumulation. As shown in Ta-\nble 1, LLM-as-RNN achieves the strongest overall\nperformance across most backbones and tasks, sup-\nporting our core hypothesis: achieving robustness\nover long horizons requires a mutable belief state\nrather than a monotonically growing context buffer.\nThe advantage is largest in settings where the\nground truth changes over time or conflicts with\n6\n"}, {"page": 7, "text": "1\n2\n3\n4\n5\nTime\n0.60\n0.70\n0.80\n0.90\n1.00\nAccuracy (\n)\nMIMIC-IV\nAcc@1 (\n)\nAcc@5 (\n)\n1\n2\n3\n4\n5\nTime\n0.10\n0.20\n0.30\n0.40\nAlign (\n)\nWeather\nAlign (\n)\n5.50\n5.75\nS&P 500\nMAE (\n)\nMSE (\n)\n1\n2\n3\n4\n5\nTime\n1.60\n1.70\nError (\n)\nFigure 3: Temporal dynamics across iterative timesteps (t=1 . . . 5) for three datasets. As feedback-driven memory\nupdates accumulate, the performance increase.\nearlier evidence. On MIMIC-IV, where a patient’s\ncondition can change abruptly, LLM-as-RNN im-\nproves Acc@1 by 12.6% (absolute) compared to\nthe best MemPrompt variant (0.6434 vs. 0.5175).\nSimilarly, on S&P 500, where shifts in market\nregimes invalidate prior “trends,” our method low-\ners MSE by roughly 6.6% (3.821 vs. 4.090). These\nresults suggest that standard append-only methods\n(FHC and MemPrompt) are prone to error persis-\ntence: once an incorrect diagnosis or outdated senti-\nment is written into the context, it continues to bias\nfuture predictions. In contrast, LLM-as-RNN can\nactively “forget” or revise these obsolete patterns\nthrough its update mechanism.\nA direct comparison between LLM-as-RNN and\nFHC underscores the inefficiency of raw context.\nAlthough FHC receives a complete, lossless history,\nit consistently underperforms our compressed-state\napproach (e.g., for Llama-3-70B, FHC Acc@1 is\n0.4126 vs. 0.5804 for LLM-as-RNN). This indi-\ncates that the bottleneck in sequential prediction\nis not how much information is available, but how\nwell that information is curated. FHC is vulnerable\nto attention dilution and noise, whereas LLM-as-\nRNN acts as an information filter that preserves\nonly the signals relevant for the next prediction.\nAlthough performance generally improves with\nmodel size, LLM-as-RNN disproportionately bene-\nfits smaller backbones. On the clinical prediction\ntask, the Llama-3.2-3B model with LLM-as-RNN\n(Acc@1: 0.4545) surpasses the significantly larger\nLlama-3.1-70B with FHC (Acc@1: 0.4126). This\nsuggests that the recurrent update loop acts as a\nstrong inductive bias for sequential reasoning, en-\nabling smaller models to approximate the long-\nhorizon tracking capabilities typically associated\nwith many more parameters.\nOn the Weather benchmark, while LLM-as-RNN\noutperforms baselines for most backbones (7/10),\nthe gains are smaller, and MemPrompt achieves\nthe single highest alignment score (0.8322 with\nGPT-oss-120B). We hypothesize that meteorolog-\nical data, which is physically continuous and less\nsemantic, derives limited benefit from “verbal cor-\nrection” compared to semantic tasks such as diag-\nnosis. In these high-entropy physical processes,\nan append-only memory scheme like MemPrompt\nmay already be adequate, since weather trajectories\nseldom exhibit the sort of logical inconsistencies\nor conceptual errors that our textual update mecha-\nnism is designed to repair.\n5.2\nTemporal Dynamics and Learning (RQ2)\nA key hypothesis of LLM-as-RNN is that the frame-\nwork does not merely condition on history but\nlearns from it through recurrence. To validate this,\nwe analyze the model’s performance as a function\nof the sequence length t.\nPerformance Gains Over Time.\nFigure 3 shows\na clear improvement trend as LLM-as-RNN up-\ndates its recurrent memory state across timesteps.\nImportantly, at each timestep t we evaluate the\nmodel’s prediction ˆyt against the corresponding\nground-truth target yt for that timestep. The re-\nported curves reflect how performance evolves\nfrom the first prediction through later steps using\nfeedback from previous steps.\nOn MIMIC-IV, both Acc@1 and Acc@5 in-\ncrease substantially from early to late time steps,\nwith the largest jump occurring after the first up-\ndate and continued gains thereafter. On Weather,\nalignment rises rapidly in the first few steps and\nthen plateaus, suggesting the state quickly captures\nthe key short-horizon signals. On S&P 500, MAE\nand MSE decrease steadily but more modestly, in-\ndicating incremental calibration of numerical fore-\ncasts. Overall, these curves support the core hy-\npothesis that feedback-driven state rewrites enable\nonline improvement under a fixed budget, rather\nthan merely accumulating history.\n7\n"}, {"page": 8, "text": "Table 2: Performance comparison between supervised\nfeedback (ground-truth) and self-supervised feedback\n(LLM-as-a-Judge). Results use Llama-3.1-8B.\nDataset\nMetric\nGround Truth\nLLM-Judge\nMIMIC-IV\nAcc@1 (↑)\n0.5524\n0.3077\nAcc@5 (↑)\n0.8531\n0.8042\nWeather\nAlign (↑)\n0.7483\n0.7203\nS&P 500\nMAE (↓)\n1.287\n1.545\nMSE (↓)\n4.517\n5.533\nRecovery from Error.\nWe quantify recovery\nfrom error as an incorrect to correct transition be-\ntween consecutive visits (Appendix Figure 5). Con-\nditioned on being incorrect at time t, LLM-as-RNN\ncorrects its prediction at t+1 in 54.8% of cases\nafter incorporating the feedback signal et, while\n45.2% of errors persist. Overall, these transition\ndynamics support a feedback-driven “self-healing”\nbehavior enabled by memory updates, which is\nharder to obtain in static baselines that lack an ex-\nplicit recurrent state update.\n5.3\nRobustness to Feedback Source (RQ3)\nThe standard configuration of LLM-as-RNN re-\nlies on ground-truth supervision to generate the\nfeedback signal et. However, in many real-world\ndeployment scenarios, immediate ground truth is\nunavailable. We evaluate the more common “Open-\nEnded” mode where the feedback et is generated\nby an LLM-Critic. This critic is grounded in a\nspecific set of domain guidelines.\nTable 2 shows the performance gap when re-\nplacing the ground truth geval(ˆyt, yt) with a neural\ncritic geval(ˆyt, Rt). Overall, switching to open-\nended, self-supervised feedback degrades perfor-\nmance, but still achieving acceptable results. On\nMIMIC-IV, Acc@1 drops from 0.5524 to 0.3077,\nwhile Acc@5 remains comparatively robust, sug-\ngesting that the LLM-Judge can preserve a high-\nquality candidate set but provides a weaker fine-\ngrained ranking signal. On the Weather dataset, the\nAlignment Rate decreases marginally from 0.7483\nto 0.7203, retaining 96.3% of the fully supervised\nperformance. On the S&P 500 forecasting task, the\ndegradation is larger: MAE increases from 1.287 to\n1.545 and MSE increases from 4.517 to 5.533. Nev-\nertheless, the model with LLM judges significantly\noutperforms the zero-shot baseline, validating that\nthe iterative reflection process itself, even without\nperfect labels, induces a form of self-consistency\nthat stabilizes long-horizon generation.\nTable 3: Effect of context window token budget λ on\nS&P 500 dataset using Llama-3.1-8B. Performance im-\nproves with increased budget up to a threshold, beyond\nwhich additional capacity yields diminishing returns.\nContext window λ\nMAE (↓)\nMSE (↓)\n512\n1.564\n5.462\n1024\n1.499\n5.292\n2048\n1.443\n5.167\n4096\n1.287\n4.517\n8192\n1.248\n4.366\n5.4\nAblation Studies (RQ4)\nTo isolate the effect of the context window budget\navailable to the recurrent state in LLM-as-RNN,\nwe conducted a controlled ablation on the S&P 500\ndataset. Specifically, we varied the maximum con-\ntext window length λ for each LLM call across 512,\n1024, 2048, 4096, and 8192, and report forecasting\nerrors in Table 3.\nAs λ increases, both MAE and MSE consistently\ndecrease, indicating that allocating a larger context\nwindow to the recurrent state enables the model to\nretain more relevant historical signals and reduce\nprediction error. The gains are most pronounced\nwhen scaling from λ = 512 to λ = 4096 (MAE:\n1.564 →1.287, MSE: 5.462 →4.517). Further\nincreasing the budget to λ = 8192 yields only\nmarginal additional improvement (MAE: 1.287 →\n1.248, MSE: 4.517 →4.366), suggesting dimin-\nishing returns at larger context windows. Overall,\nthese results support the importance of sufficient\ncontext window budget for trajectory modeling\nwhile indicating that performance saturates once λ\nis large enough to preserve the time-series signals.\n6\nConclusion\nWe presented LLM-as-RNN, an inference-time\nframework that makes a frozen LLM behave like\na recurrent predictor by maintaining a natural-\nlanguage memory state and rewriting it at each\ntimestep using feedback on prior predictions, rather\nthan accumulating an immutable context history.\nThis revisable, structured state enables online error\ncorrection under a fixed token budget while pro-\nducing transparent, human-readable learning traces.\nAcross time sequential benchmarks in healthcare,\nmeteorology, and finance, our results indicate that\nfeedback-driven state rewriting offers a simple,\nmodel-agnostic route to long-horizon adaptation\nwithout parameter updates.\n8\n"}, {"page": 9, "text": "7\nLimitation\nLLM-as-RNN increases inference cost because\neach timestep typically requires multiple model\ncalls (prediction, reflection, and memory update),\nwhich can be prohibitive for latency-sensitive de-\nployments compared to single-pass inference. The\nquality of memory updates is bounded by the back-\nbone model’s ability to diagnose failures, translate\nerrors into actionable guidance, and compress in-\nformation without losing critical signals; smaller\nbackbones can be more prone to unstable or lossy\nupdates. The framework also depends on the avail-\nability and reliability of feedback: ground-truth\nlabels may be delayed or unavailable, and LLM-\nbased critics can introduce noise or inconsistency\nthat leads to memory drift or self-reinforcing mis-\ntakes. Finally, while the core algorithm is domain-\nagnostic, strong performance often requires careful\nprompt/schema design and selecting an appropri-\nate memory budget, and fully automating prompt\ndesign and ensuring robust long-horizon behavior\nremain open challenges.\n8\nPotential risks\nBecause LLM-as-RNN accumulates state over\ntime, incorrect predictions or memory updates can\ncompound across timesteps, and the interpretability\nof the memory may create an illusion of reliability\nthat increases automation bias; in clinical decision\nsupport, the system should be used strictly as an\nassistive tool with clinician oversight and rigorous\nevaluation across diverse subpopulations, and out-\nputs should not be treated as medical advice. Simi-\nlar concerns apply to financial forecasting: markets\nare influenced by exogenous factors and regime\nshifts, and sequential “learning from feedback” can\noverfit to noise or recent trends, so the framework\nshould not serve as the sole basis for investment\ndecisions. The memory mechanism also raises pri-\nvacy and security concerns because the state may\ninadvertently retain sensitive or re-identifying de-\ntails if not carefully controlled, and untrusted inputs\nor adversarial feedback could corrupt the memory\nand steer future predictions; mitigations include\ndata-minimization and redaction for stored state,\naccess controls, separating trusted feedback chan-\nnels from user content, enforcing structured up-\ndate schemas, monitoring for anomalous memory\nchanges, and providing reset/rollback mechanisms.\nReferences\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam Alt-\nman, Andy Applebaum, Edwin Arbus, Rahul K\nArora, Yu Bai, Bowen Baker, Haiming Bao, and 1\nothers. 2025. gpt-oss-120b & gpt-oss-20b model\ncard. arXiv preprint arXiv:2508.10925.\nEkin Akyürek and 1 others. 2022. What learning al-\ngorithm is in-context learning? investigations with\nlinear models. arXiv preprint.\nAnthropic. 2025. System card: Claude sonnet 4.5. Tech-\nnical report, Anthropic.\nSimran Arora,\nSabri Eyuboglu,\nMichael Zhang,\nAman Timalsina, Silas Alberti, Dylan Zinsley,\nJames Zou, Atri Rudra, and Christopher Ré. 2024.\nSimple linear attention language models balance\nthe recall-throughput tradeoff.\narXiv preprint\narXiv:2402.18668.\nBenjamin G Ascoli and Jinho D Choi. 2025. Advanc-\ning conversational text-to-sql: Context strategies and\nmodel integration with large language models. Fu-\nture Internet, 17(11):527.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015.\nNeural machine translation by jointly\nlearning to align and translate. In International Con-\nference on Learning Representations.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, and 1 others.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International conference on\nmachine learning, pages 2206–2240. PMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, and 1 oth-\ners. 2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings\nof the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming\nMa, Zhifang Sui, and Furu Wei. 2023. Why can gpt\nlearn in-context? language models secretly perform\ngradient descent as meta-optimizers. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 4005–4019.\n9\n"}, {"page": 10, "text": "Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Mel-\nnyk, Sarathkrishna Swaminathan, Sihui Dai, Aurelie\nLozano, Georgios Kollias, Vijil Chenthamarakshan,\nJiri Navratil, and 1 others. 2024. Larimar: Large lan-\nguage models with episodic memory control. In In-\nternational Conference on Machine Learning, pages\n10109–10126. PMLR.\nYiming Du, Wenyu Huang, Danna Zheng, Zhaowei\nWang, Sebastien Montella, Mirella Lapata, Kam-Fai\nWong, and Jeff Z Pan. 2025. Rethinking memory in\nai: Taxonomy, operations, topics, and future direc-\ntions. arXiv preprint arXiv:2505.00675.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gre-\ngory Valiant. 2022. What can transformers learn\nin-context? a case study of simple function classes.\nAdvances in neural information processing systems,\n35:30583–30598.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. In Proc. IEEE Intl. Conf. on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 6645–6649.\nAlex Graves,\nGreg Wayne,\nand Ivo Danihelka.\n2014.\nNeural turing machines.\narXiv preprint\narXiv:1410.5401.\nAlex Graves,\nGreg Wayne,\nMalcolm Reynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio Gómez Colmenarejo, Tiago Ra-\nmalho, John Agapiou, and 1 others. 2016. Hybrid\ncomputing using a neural network with dynamic ex-\nternal memory. Nature, 538(7626):471–476.\nAlbert Gu and Tri Dao. 2024. Mamba: Linear-time\nsequence modeling with selective state spaces. In\nFirst conference on language modeling.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2023. Atlas: Few-shot learning with retrieval\naugmented language models. Journal of Machine\nLearning Research, 24(251):1–43.\nYushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, An-\nderson Schneider, Yuriy Nevmyvaka, and Dongjin\nSong. 2024. Empowering time series analysis with\nlarge language models: a survey. In Proceedings of\nthe Thirty-Third International Joint Conference on\nArtificial Intelligence, pages 8095–8103.\nAlistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Francois Fleuret. 2020. Transformers are\nRNNs: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, and 1 others. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. Advances\nin neural information processing systems, 33:9459–\n9474.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics, 12:157–173.\nAman Madaan, Niket Tandon, Peter Clark, and Yiming\nYang. 2022. Memory-assisted prompt editing to im-\nprove gpt-3 after deployment. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2833–2861.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nand 1 others. 2023. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information\nProcessing Systems, 36:46534–46594.\nDyutidas Mahaptra. 2024. S&p 500 with financial news\nheadlines (2008–2024). Kaggle Dataset. Accessed:\n2026-01-02.\nMatthew Muhoberac, Atharva Parikh, Nirvi Vakharia,\nSaniya Virani, Aco Radujevic, Savannah Wood,\nMeghav Verma, Dimitri Metaxotos, Jeyaraman\nSoundararajan, Thierry Masquelin, and 1 others.\n2025.\nState and memory is all you need for\nrobust and reliable ai agents.\narXiv preprint\narXiv:2507.00081.\nJ. Muthukumar. 2023. Weather dataset. Kaggle Dataset.\nAccessed: 2026-01-02.\nMatanel Oren, Michael Hassid, Nir Yarden, Yossi Adi,\nand Roy Schwartz. 2024. Transformers are multi-\nstate rnns. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 18724–18741.\n10\n"}, {"page": 11, "text": "Charles Packer, Vivian Fang, Shishir_G Patil, Kevin\nLin, Sarah Wooders, and Joseph_E Gonzalez. 2023.\nMemgpt: Towards llms as operating systems.\nJoon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2023. Generative agents: Interactive simulacra\nof human behavior. In Proceedings of the 36th an-\nnual acm symposium on user interface software and\ntechnology, pages 1–22.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Al-\nbalak, Samuel Arcadinho, Stella Biderman, Huanqi\nCao, Xin Cheng, Michael Chung, Leon Derczynski,\nXingjian Du, Matteo Grella, Kranthi Gv, Xuzheng\nHe, Haowen Hou, Przemyslaw Kazienko, Jan Ko-\ncon, Jiaming Kong, Bartłomiej Koptyra, and 13 oth-\ners. 2023. RWKV: Reinventing RNNs for the trans-\nformer era. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, pages 14048–\n14077, Singapore. Association for Computational\nLinguistics.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick,\nDaan Wierstra, and Timothy Lillicrap. 2016. Meta-\nlearning with memory-augmented neural networks.\nIn Proceedings of The 33rd International Conference\non Machine Learning, volume 48 of Proceedings of\nMachine Learning Research, pages 1842–1850, New\nYork, New York, USA. PMLR.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. Advances in Neural Information Process-\ning Systems, 36:8634–8652.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Se-\nquence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems,\nvolume 27.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, Alexandre Ramé, Morgane\nRivière, and 1 others. 2025. Gemma 3 technical\nreport. arXiv preprint arXiv:2503.19786.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models don’t\nalways say what they think: Unfaithful explanations\nin chain-of-thought prompting. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nJohannes Von Oswald, Eyvind Niklasson, Ettore Ran-\ndazzo, João Sacramento, Alexander Mordvintsev, An-\ndrey Zhmoginov, and Max Vladymyrov. 2023. Trans-\nformers learn in-context by gradient descent. In In-\nternational Conference on Machine Learning, pages\n35151–35174. PMLR.\nPeng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi\nYao, Yong Jiang, Pengjun Xie, Fei Huang, and Hua-\njun Chen. 2024. Wise: Rethinking the knowledge\nmemory for lifelong model editing of large language\nmodels. Advances in Neural Information Processing\nSystems, 37:53764–53797.\nWeizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,\nXifeng Yan, Jianfeng Gao, and Furu Wei. 2023. Aug-\nmenting language models with long-term memory.\nAdvances in Neural Information Processing Systems,\n36:74530–74543.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2014.\nMemory networks. arXiv preprint arXiv:1410.3916.\nTongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan,\nThuy-Trang Vu, and Gholamreza Haffari. 2024. Con-\ntinual learning for large language models: A survey.\nPreprint, arXiv:2402.01364.\nYaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang,\nYongyue Zhang, Huifeng Guo, Ruiming Tang, and\nYong Liu. 2025. From human memory to ai memory:\nA survey on memory mechanisms in the era of llms.\narXiv preprint arXiv:2504.15965.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\nLiu, Quoc V Le, Denny Zhou, and Xinyun Chen.\n2024.\nLarge language models as optimizers.\nIn\nThe Twelfth International Conference on Learning\nRepresentations.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. In The eleventh international conference on\nlearning representations.\nMert Yuksekgonul, Federico Bianchi, Joseph Boen,\nSheng Liu, Zhi Huang, Carlos Guestrin, and James\nZou. 2024. Textgrad: Automatic\" differentiation\" via\ntext. arXiv preprint arXiv:2406.07496.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu,\nand Noah A. Smith. 2024a. How language model\nhallucinations can snowball. In Proceedings of the\n41st International Conference on Machine Learning,\nvolume 235 of Proceedings of Machine Learning\nResearch, pages 59670–59684. PMLR.\nXiyuan Zhang, Ranak Roy Chowdhury, Rajesh K Gupta,\nand Jingbo Shang. 2024b. Large language models for\ntime series: a survey. In Proceedings of the Thirty-\nThird International Joint Conference on Artificial\nIntelligence, pages 8335–8343.\nWanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and\nYanlin Wang. 2024. Memorybank: Enhancing large\nlanguage models with long-term memory. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 38, pages 19724–19731.\nKai Zhou and 1 others. 2023. Recurrentgpt: Interactive\ngeneration and reasoning with recurrent language\nmodels. arXiv preprint.\n11\n"}, {"page": 12, "text": "A\nDataset Details\nMIMIC-IV (clinical EHR).\nMIMIC-IV 1 (John-\nson et al., 2023) is a large, deidentified electronic\nhealth record (EHR) database of patients treated\nat the Beth Israel Deaconess Medical Center, cov-\nering both intensive care unit (ICU) admissions\nand emergency department (ED) visits. It contains\nstructured clinical information such as demograph-\nics, diagnoses, procedures, laboratory measure-\nments, and treatment/medication-related variables.\nIn our experiments, we represent each patient as\na chronologically ordered sequence of visits/ad-\nmissions, and we apply deterministic visit filtering\n(Appendix B) to reduce clinically heterogeneous\ntimelines.\nWeather (meteorological time series).\nThe\nWeather dataset 2 is a multivariate time series with\n96,453 timestamped observations and 12 columns,\nmixing categorical descriptors and continuous me-\nteorological variables.\nTypical fields include a\ntimestamp (Formatted Date), textual descriptors\n(e.g., Daily Summary), precipitation type, and nu-\nmeric measurements such as temperature and appar-\nent temperature (◦C), humidity, wind speed, wind\nbearing, visibility, and pressure.\nS&P 500 with Financial News Headlines (finan-\ncial time series).\nS&P 500 dataset 3 dataset cou-\nples a historical S&P 500 market time series clos-\ning prices with one or more daily financial news\nheadlines. We align market records and headlines\nby trading date. This benchmark emphasizes non-\nstationarity and temporal dependence typical of\nfinancial markets, while also testing whether tex-\ntual news can help guide sequential prediction and\nmemory updates.\nB\nVisit Filtering for MIMIC-IV\nPatient timelines in MIMIC-IV may contain admis-\nsions that are clinically heterogeneous across time\n(e.g., unrelated comorbid events). We apply a de-\nterministic, lexicon-driven filtering procedure that\nretains, for each patient, a temporally contiguous\nsubsequence of visits whose inferred coarse topics\nare mutually consistent. The procedure does not\nlearn from labels or train a model; it only prunes\n1https://physionet.org/content/mimiciv/3.1/\n2https://www.kaggle.com/datasets/muthuj7/weather-\ndataset\n3https://www.kaggle.com/datasets/dyutidasmahaptra/s-\nand-p-500-with-financial-news-headlines-20082024\nvisits while preserving all original structured fields\nof the retained records.\nB.1\nInputs, Outputs, and Cohort Preselection\nInputs.\nThe filtering consumes the full parsed\nMIMIC-IV dataset in JSON format, where each\npatient record contains a chronologically ordered\nlist of visit records.\nCohort\nrestriction.\nOnly\npatients\nwhose\nvalid_visits fall within a fixed range are consid-\nered. In the reported configuration, we restrict to\npatients with 5–20 valid visits (inclusive).\nOutputs.\nThe procedure produces a filtered\nJSON dataset in which each retained patient record\npreserves all original fields but replaces the visit\nlist with the selected subsequence.\nB.2\nVisit Text Construction\nFor each visit, we construct a single lowercased text\nstring by concatenating multiple free-text sources:\n• Clinical sections: all section values; if a section\nvalue is a list, all elements are included.\n• Notes: if a structured notes field is present, the\nfull note text is used when available.\n• Additional fields:\nlike chief_complaint,\nallergies and service.\nThis aggregated text is used only for topic match-\ning, it does not alter the stored visit content.\nWe define a small set of coarse medical topics,\neach represented by a set of keywords. Topic ev-\nidence is computed by substring matches in the\naggregated visit text. The full lexicon is shown in\nTable 6.\nB.3\nPer-Visit Topic Assignment\nLet v denote a visit and t a topic with keyword set\nKt. We compute a topic score as the number of\nkeywords that appear in the visit text:\nst(v) =\nX\nk∈Kt\n1\n\u0002\nk in the aggregated text of v\n\u0003\n(9)\nTopics with st(v) = 0 are ignored. The visit is\nassigned up to the top three topics by st(v) (ties\nbroken by the sorting order):\nTopics(v) = Top-3{t : st(v) > 0}.\n(10)\nIf no keywords match, then Topics(v) = ∅.\n12\n"}, {"page": 13, "text": "Table 4: Dataset summary (appendix). Fill in cohort-specific counts after preprocessing where applicable.\nDataset\nDomain\nTimestep\nInputs (modalities)\nPrediction target\nMetric(s)\nMIMIC-IV\nHealthcare\nVisit\nStructured EHR notes (labs, di-\nagnoses, treatments)\nClinical diagnosis\nAcc@1, Acc@5 (↑)\nWeather\nMeteorology\nHourly\nNumeric variables and categori-\ncal / text descriptors\nWeather summary\nAlignment Rate (↑)\nS&P 500\nFinance\nDaily\nNews headlines text\nClosing prices\nMAE, MSE (↓)\nTable 5: Dataset statistics.\nDataset\nSize\nMIMIC-IV\n7,128 patients, 37,536 visits\nWeather\n4,019 days, 96,453 observations\nS&P 500\n3,507 trading days, 19,127 headlines\nFor two visits vi and vj, we compute Jaccard\nsimilarity over topic sets:\nSim(vi, vj) =\n\u001a|Topics(vi) ∩Topics(vj)|\n|Topics(vi) ∪Topics(vj)|\n(11)\nThis definition ensures that visits with no matched\ntopics do not spuriously increase coherence.\nB.4\nConsecutive Group Discovery\nFor each patient with visits (v1, . . . , vn) in chrono-\nlogical order, we partition the sequence into con-\nsecutive groups using a single left-to-right pass.\nWe maintain a current group G (initialized with\nthe first visit). For each subsequent visit vi, we\ncompute its average similarity to the visits already\nin the current group:\nSim(vi, G) =\n1\n|G|\nX\nvj∈G\nSim(vi, vj).\n(12)\nIf Sim(vi, G) ≥τ, we append vi to G; otherwise\nwe close G and start a new group at vi. Only groups\nof length at least m are kept as candidate coherent\ngroups. If no candidate group exists (i.e., no con-\nsecutive segment reaches length m), we fall back to\ntreating the entire visit sequence as a single group.\nGiven the candidate coherent groups for a pa-\ntient, we select which visits to keep by retaining\nthe largest group. The resulting filtered visit list\nis the chronologically ordered subsequence corre-\nsponding to that group.\nFinally, we enforce a minimum number of re-\ntained visits: if a patient has fewer than r visits\nafter filtering, the patient is removed from the fil-\ntered dataset. In our experiments, we use τ = 0.6,\nm = 2 and r = 3.\nTable 6: Coarse topic lexicon used for visit topic as-\nsignment. Abbreviations such as CHF, COPD, HbA1c,\nCKD, and AKI are matched as substrings.\nTopic\nKeywords\nCardiovascular\nheart, cardiac, cardiovascular, hyper-\ntension, chf, myocardial, coronary,\nartery, atrial, ventricular, angina, infarc-\ntion, fibrillation, blood pressure\nRespiratory\nlung, pulmonary, respiratory, pneumo-\nnia, copd, asthma, bronchitis, dyspnea,\nbreathing, oxygen\nDiabetes\ndiabetes, diabetic, glucose, insulin,\nhyperglycemia, hypoglycemia, hba1c,\nblood sugar\nRenal\nkidney, renal, nephro, dialysis, creati-\nnine, ckd, aki, urinary, urine\nNeurological\nneuro, brain, stroke, seizure, demen-\ntia, alzheimer, parkinson, headache, mi-\ngraine\nGastrointestinal\ngastro, intestinal, liver, stomach, bowel,\ngi, hepatic, cirrhosis, abdominal\nOncology\ncancer, tumor, malignancy, metasta-\nsis, oncology, chemotherapy, radiation,\nneoplasm\nInfectious\ninfection, sepsis, bacterial, viral, antibi-\notic, fever, inflammatory\nFor all retained patients, all original patient-level\nfields are preserved unchanged; only the visit list\nis replaced by the selected subsequence.\nC\nPrompts for MIMIC-IV datasets\nIn this section, we provide the full prompt tem-\nplates used for the MIMIC-IV clinical diagnosis\ntask.\nC.1\nMethod: Initialization and Generation\nSystem Initialization\nYou are an expert clinical AI assistant analyzing\nlongitudinal patient records.\nPatient Information:\n• Patient ID: {patient_id}\n• Age: {age} years\n13\n"}, {"page": 14, "text": "1\n3\n8\n70\n0.2\n0.3\n0.4\n0.5\n0.6\nMIMIC-IV Acc@1 (\n)\n1\n3\n8\n70\nWeather Align (\n)\n1\n3\n8\n70\nS&P 500 MSE (\n)\n1\n4\n12\n27\n0.2\n0.3\n0.4\n0.5\n0.6\n1\n4\n12\n27\n1\n4\n12\n27\n20\n120\nModel size (B params, log scale)\n0.2\n0.3\n0.4\n0.5\n0.6\n20\n120\nModel size (B params, log scale)\n20\n120\nModel size (B params, log scale)\nLlama\nGemma\nGPT\nZero-shot\nFHC\nMemPrompt\nLLM-as-RNN\nFigure 4: Scaling across backbone families. Performance vs. model size (B params, log-scale) for Zero-shot, FHC,\nMemPrompt, and LLM-as-RNN. Rows: Llama/Gemma/GPT backbones; columns: MIMIC Acc@1, Weather Align,\nS&P 500 MSE. LLM-as-RNN yields consistent gains across sizes and families.\nFigure 5: Transition heatmap of primary-diagnosis cor-\nrectness between visits. Rows indicate correctness at\ntime t and columns indicate correctness at time t+1.\n• Gender: {gender}\n• Primary Conditions: {primary_conditions}\nEvolving Clinical Summary: {evolving_summary}\nYour task is to provide accurate clinical predictions\nbased on the current visit information and patient\nhistory.\nDefault Initialization Value (First Visit)\nCLINICAL HISTORY: This is the patient’s first visit.\nNo previous medical history available in the system.\nCURRENT STATUS & FOCUS AREAS:\n• Establishing baseline assessment\n• Gathering initial clinical presentation\nKEY CONSIDERATIONS FOR FUTURE VISITS:\n• Baseline health status to be established after first\nvisit\n• Future assessments will track disease progression\nPrediction Prompt (Pgen)\nYou are a clinical AI assistant predicting discharge\ndiagnoses for the current visit.\nCURRENT VISIT SNAPSHOT:\n• Chief Complaint: {chief_complaint}\n• Vital Signs: {vitals}\n• Lab Results: {labs}\n• Medications on Admission: {medications}\n• Recent Procedures: {procedures}\n• History of Present Illness:\n{history_present_illness}\n• Past Medical History: {past_medical_history}\n• Social History: {social_history}\n• Family History: {family_history}\n• Physical Exam: {physical_exam}\n• Pertinent Results: {pertinent_results}\n• Hospital Course: {hospital_course}\n• Allergies: {allergies}\nTASK: Provide ONLY the discharge diagnosis\npredictions. Return JSON with top-5 diagnoses and\n14\n"}, {"page": 15, "text": "the primary diagnosis:\n{\n\"top_5_diagnoses\": [\n{\"name\": \"diagnosis1\"},\n{\"name\": \"diagnosis2\"},\n{\"name\": \"diagnosis3\"},\n{\"name\": \"diagnosis4\"},\n{\"name\": \"diagnosis5\"}\n],\n\"primary_diagnosis\": {\"name\": \"diagnosis1\n\"}\n}\nC.2\nMethod: Reflection and Memory Update\nEvaluation/Reflection Prompt (geval)\nYou are evaluating predicted discharge diagnoses\nagainst ground truth to guide learning.\nPREDICTED OUTPUT (JSON): {prediction}\nGROUND TRUTH DIAGNOSES:\n{ground_truth_diagnoses}\nEvaluate with SEMANTIC MATCHING (consider\nbroader/narrower terms as correct if clinically\nequivalent).\nReturn JSON with ONLY these fields. Please respond\nin JSON format:\n{\n\"diagnosis_evaluation\": {\n\"primary_correct\": true/false,\n\"any_top5_correct\": true/false,\n\"missed_diagnoses\": [\"diagnosis1\", \"\ndiagnosis2\"],\n\"why_missed\": \"brief reason for misses\"\n},\n\"improvement_suggestions\": \"one concise\nsentence on how to improve future\npredictions\"\n}\nMemory Update Prompt (Pmem)\nYou are updating a clinical AI system’s memory to\nlearn from prediction errors.\nCURRENT EVOLVING SUMMARY:\n{current_evolving_summary}\nPATIENT’S ACTUAL MEDICAL HISTORY (from\nall previous visits): {patient_history}\nEVALUATION FEEDBACK (Prediction vs Ground\nTruth): {evaluation_feedback}\nVISIT CONTEXT: {visit_summary}\nCRITICAL INSTRUCTIONS:\n• The evaluation feedback contains LLM JUDGE\nanalysis with: what diagnoses were MISSED and\nWHY, what clinical signs were OVERLOOKED,\nand specific improvement suggestions.\n• The PATIENT HISTORY shows the ACTUAL\ndiagnoses and treatments across visits.\n• Your task: Update the evolving summary to\nincorporate past learnings, current status, and future\nconsiderations.\nPAY SPECIAL ATTENTION TO:\n• \"missed_diagnoses\" - what we failed to predict\n• \"why_missed\" - root cause of our errors\n• \"improvement_suggestions\" - single concise\nsentence\nCreate a CONCISE evolving summary (MAX 200\nwords total) with three short sections:\n1. CLINICAL HISTORY (2-3 sentences max): List\nkey diagnoses from patient history with visit numbers.\nNote recurring conditions.\n2. FOCUS AREAS (2-3 key points): What diagnoses\nwere missed in this visit? Why were they missed?\n3. FUTURE CONSIDERATIONS (1-2 key points):\nWhat to watch for based on patient’s actual conditions.\nCRITICAL: Keep it brief and focused. Stop after\ncompleting the JSON structure.\nProvide the updated evolving summary as JSON:\n{\n\"evolving_summary\": \"unified summary with\nthree sections: CLINICAL HISTORY,\nCURRENT STATUS & FOCUS AREAS, and KEY\nCONSIDERATIONS FOR FUTURE VISITS\"\n}\nC.3\nBaselines\nBaseline: Zero-Shot\nYou are an expert clinical AI assistant. Analyze the\nfollowing patient visit and provide:\n1. Top-5 diagnoses (ranked by likelihood)\n2. Primary diagnosis\nCurrent Visit Information:\n• Patient Demographics: Age {age}, Gender\n{gender}\n• Chief Complaint: {chief_complaint}\n• Vital Signs: {vital_signs}\n• Lab Results: {lab_results}\n• Current Medications: {medications}\n• Recent Procedures: {procedures}\nTASK: Provide ONLY the discharge diagnosis\npredictions. Return JSON with top-5 diagnoses and\nthe primary diagnosis:\n{\n\"top_5_diagnoses\": [\n{\"name\": \"diagnosis1\"},\n{\"name\": \"diagnosis2\"},\n{\"name\": \"diagnosis3\"},\n{\"name\": \"diagnosis4\"},\n{\"name\": \"diagnosis5\"}\n],\n\"primary_diagnosis\": {\"name\": \"diagnosis1\n\"}\n}\nMemPrompt Summarization\nThe following system prompt has become too long and\nneeds compression:\n{long_system_prompt}\nTask: Create a compressed version that:\n1. Retains all critical clinical information\n2. Removes redundancy\n3. Keeps the most recent and relevant updates\n4. Maintains the same structure (Focus Areas, Status,\nConsiderations)\n15\n"}, {"page": 16, "text": "5. Stays under 250 words\nProvide the compressed prompt maintaining the same\nJSON structure.\nBaseline: FHC and MemPrompt\nYou are a clinical decision support system. You have\naccess to a patient’s complete visit history with known\noutcomes (diagnoses and treatments).\nPATIENT VISIT HISTORY (with known\noutcomes): {patient_history}\nCURRENT VISIT: {current_visit}\nTASK: Based on the patient’s complete history and\ncurrent presentation, provide Top 5 most likely\ndischarge diagnoses (ranked by likelihood, from most\nlikely to least likely).\nConsider:\n• Patterns from previous visits and their outcomes\n• Disease progression and comorbidities\n• Current clinical presentation\nTASK: Provide ONLY the discharge diagnosis\npredictions. Return JSON with top-5 diagnoses and\nthe primary diagnosis:\n{\n\"top_5_diagnoses\": [\n{\"name\": \"diagnosis1\"},\n{\"name\": \"diagnosis2\"},\n{\"name\": \"diagnosis3\"},\n{\"name\": \"diagnosis4\"},\n{\"name\": \"diagnosis5\"}\n],\n\"primary_diagnosis\": {\"name\": \"diagnosis1\n\"}\n}\nD\nQualitative Analysis: Memory Trace\nTo illustrate the recurrent inference mechanism, we\npresent a step-by-step trace of Patient 10035631.\nThis example demonstrates how the Memory State\n(ht) evolves to correct errors and accumulate clini-\ncal context over time.\nStep 1: Initialization and Initial Feedback (V1)\nThe patient presents with Leukemia. The model\npredicts the primary condition correctly but misses\nsecondary electrolyte abnormalities. The memory\nis updated to watch for these in the future.\nInput (x1) & Prediction (ˆy1)\nChief Complaint: Leukemia\nPrediction: Acute Myeloid Leukemia (AML) in remis-\nsion.\nGround Truth: Leukemia, Aspergillosis, Pancytopenia,\nHypokalemia.\nReflection/Feedback (s1)\nError Analysis: Primary diagnosis correct. However,\nelectrolyte abnormalities like hypokalemia were missed.\nGuidance: Include common metabolic/electrolyte abnor-\nmalities in the differential for hematologic malignancy\npatients.\nMemory Update (h0 →h1)\nPrevious Memory: [Empty/Default Initialization]\nUpdated Memory (h1):\nClinical History: Leukemia (V1), Aspergillosis (V1),\nHypokalemia (V1).\nFuture Focus: Watch for metabolic and electrolyte\nabnormalities (specifically Potassium) in hematologic\npatients.\n... Visits 2, 3, and 4 processed (Memory\naccumulates breast cancer, pneumonia) ...\nStep 2: Error Correction via Recurrence (V5)\nIn Visit 5, the model falsely predicts \"Remission\"\nwhen the patient has \"Relapsed\". The reflection\nmodule catches this, and the memory explicitly\nencodes this correction to prevent future compla-\ncency.\nInput (x5) & Prediction (ˆy5)\nChief Complaint: Admission for cycle 1 of Dacogen\n(chemotherapy).\nPrediction: Leukemia (AML) in remission.\nGround Truth: Relapsed AML.\nReflection/Feedback (s5)\nError Analysis: Primary diagnosis incorrectly identified\nAML as ’in remission’ when patient actually had relapsed\ndisease.\nGuidance: Prioritize active disease states over remission\nstatus when clinical evidence (Dacogen) suggests relapse.\nMemory Update (h4 →h5)\nPrevious Focus: Watch for pulmonary complications,\nGVHD.\nUpdated Focus (h5):\nMissed Diagnosis Note: Primary diagnosis incorrectly\nidentified AML as ’in remission’.\nImprovement Strategy: Prioritize active disease states\nover remission status. Ensure most acute condition is\nselected.\nStep 3: Long-Term Memory Retention (V8)\nBy the end of the sequence, the memory state (hT )\nhas become a comprehensive summary of the pa-\ntient’s complex trajectory, far exceeding the context\nwindow of a standard zero-shot prompt.\n16\n"}, {"page": 17, "text": "Final Memory State (h8)\nCLINICAL HISTORY:\n• Oncology: Leukemia (V1, V8), Invasive Ductal\nCarcinoma (V2), Relapsed AML (V5), Multiple\nMyeloma (V6).\n• Infectious: Aspergillosis (V1), Sepsis (V6),\nNeutropenic fever (V7).\n• Comorbidities: Hypokalemia (V1), Acute Kidney\nFailure (V6), GVHD (V3), Orthostatic Hypotension\n(V4).\nCURRENT STATUS & FOCUS AREAS:\n• Learned Patterns: Patient has recurring Heart\nFailure (V1, V3, V5).\n• Correction History: Watch for specific organisms\nin bacteremia (missed in V7); distinguish between\nactive relapse vs remission.\nKEY CONSIDERATIONS FOR FUTURE VISITS:\n• Watch for pulmonary complications in hematologic\nmalignancy.\n• Monitor electrolyte levels (Potassium) carefully.\n• Distinguish between Leukemia vs Myeloma\npresentation.\nE\nError Analysis\nE.1\nNon-parsable outputs.\nWith smaller backbones or higher sampling ran-\ndomness, generations more often violate strict\nJSON-only constraints (e.g., emitting extra natural-\nlanguage text, markdown code fences, trailing com-\nmas, or missing braces). These formatting failures\nbreak automatic parsing and can halt the recurrent\npipeline, since both the evaluator and the memory\nupdate step depend on structured fields to propa-\ngate feedback across timesteps.\nE.2\nOverlong generations causing truncation.\nPrediction, critique, or memory-update outputs\ncan exceed max_tokens or the memory budget λ,\nleading to truncation. This is particularly damag-\ning when truncation cuts off JSON closures (mak-\ning outputs non-parsable) or removes critical su-\npervision signals such as missed_diagnoses and\nwhy_missed. In a recurrent setting, losing these\nfields not only degrades the current timestep but\nalso weakens the next memory update, compound-\ning error over long horizons.\nE.3\nNoisy or biased feedback.\nWhen ground truth labels are unavailable and feed-\nback is generated by an LLM judge, the critique\ncan be noisy, inconsistent, or biased (e.g., over-\npenalizing acceptable synonyms, missing clinically\nequivalent diagnoses, or providing spurious ratio-\nnales). Because the memory update treats this feed-\nback as a “semantic gradient,” systematic judge\nerrors can cause the state to internalize incorrect\nlessons, inducing memory drift and potentially re-\ninforcing mistakes across subsequent timesteps.\n17\n"}]}