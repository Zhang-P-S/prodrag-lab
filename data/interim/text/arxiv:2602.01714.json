{"doc_id": "arxiv:2602.01714", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.01714.pdf", "meta": {"doc_id": "arxiv:2602.01714", "source": "arxiv", "arxiv_id": "2602.01714", "title": "MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark", "authors": ["Mouath Abu-Daoud", "Leen Kharouf", "Omar El Hajj", "Dana El Samad", "Mariam Al-Omari", "Jihad Mallat", "Khaled Saleh", "Nizar Habash", "Farah E. Shamout"], "published": "2026-02-02T06:52:20Z", "updated": "2026-02-02T06:52:20Z", "summary": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.01714v1", "url_pdf": "https://arxiv.org/pdf/2602.01714.pdf", "meta_path": "data/raw/arxiv/meta/2602.01714.json", "sha256": "a5260e0d8e3adc0101dc3bd5fba398dab6c01225ad659c32f5ec18168837a688", "status": "ok", "fetched_at": "2026-02-18T02:20:03.622857+00:00"}, "pages": [{"page": 1, "text": "Published as a conference paper at ICLR 2026\nMedAraBench: Large-Scale Arabic Medical\nQuestion Answering Dataset and Benchmark\nMouath Abu-Daoud1, Leen Kharouf1, Omar El Hajj1, Dana El Samad1,\nMariam Al-Omari1, Jihad Mallat3, Khaled Saleh3, Nizar Habash2, Farah E.\nShamout1\n1Engineering Division, New York University Abu Dhabi, UAE\n2Science Division, New York University Abu Dhabi, UAE\n3Cleveland Clinic Abu Dhabi, UAE\n{mma9138, lk2713, ose6432, dae7168, ma8058, nh48, fs999}@nyu.edu\n{mallatj, salehk}@ccad.ae\nAbstract\nArabic remains one of the most underrepresented languages in natural\nlanguage processing research, particularly in medical applications, due to\nthe limited availability of open-source data and benchmarks. The lack of\nresources hinders efforts to evaluate and advance the multilingual capa-\nbilities of Large Language Models (LLMs).\nIn this paper, we introduce\nMedAraBench, a large-scale dataset consisting of Arabic multiple-choice\nquestion-answer pairs across various medical specialties. We constructed\nthe dataset by manually digitizing a large repository of academic materials\ncreated by medical professionals in the Arabic-speaking region. We then\nconducted extensive preprocessing and split the dataset into training and\ntest sets to support future research efforts in the area. To assess the quality\nof the data, we adopted two frameworks, namely expert human evaluation\nand LLM-as-a-judge. Our dataset is diverse and of high quality, spanning\n19 specialties and five difficulty levels.\nFor benchmarking purposes, we\nassessed the performance of eight state-of-the-art open-source and propri-\netary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our\nfindings highlight the need for further domain-specific enhancements. We\nrelease the dataset and evaluation scripts to broaden the diversity of medi-\ncal data benchmarks, expand the scope of evaluation suites for LLMs, and\nenhance the multilingual capabilities of models for deployment in clinical\nsettings.\n1\nIntroduction\nThe emergence of Large Language Models (LLMs) has driven transformative progress in\nNatural Language Processing (NLP) in recent years. They have demonstrated exceptional\nperformance across various renowned benchmarks due to their powerful understanding and\nreasoning abilities, grounded in the vast amount of knowledge in their training corpora\n(Brown et al., 2020; Bommasani et al., 2022; Chowdhery et al., 2022). This includes general\nand domain-specific benchmarks (Wang et al., 2021; Stahlberg, 2020).\nHowever, performance improvements remain variable across underrepresented languages and\ndomains, particularly in high-stakes applications like medicine (Jiang et al., 2025; Yang et al.,\n2025). For example, Arabic is among the most spoken languages in the world, with over\n400 million speakers across the globe. However, it remains underrepresented in the medical\ndomain, mainly due to limited expert-annotated resources (Habash, 2005). Arabic NLP\ngenerally presents inherent language-specific linguistic challenges (Habash, 2005; Farghali\n& Shaalan, 2009; AlMoaiad, 2024), making the availability of such resources essential for\n1\narXiv:2602.01714v1  [cs.CL]  2 Feb 2026\n"}, {"page": 2, "text": "Published as a conference paper at ICLR 2026\nTable 1: Comparison of MedAraBench with existing medical QA benchmarks, including\nestimated dataset size.\nBenchmark\nLanguage(s)\nType\nSize\nExpert\nAnnotation\nDifficulty\nMapping\nSpecialty\nCoverage\nArabic\nPublic\nMedQA\nEnglish, Chinese\nMCQs\n60,000\n✓\n×\n✓\n×\n✓\nMedMCQA\nEnglish\nMCQs\n193,000\n✓\n×\n✓\n×\n✓\nMMLU (USMLE)\nEnglish\nMCQs\n1,800\n×\n×\n✓\n×\n✓\nMMLU Translation\n14 incl. Arabic\nMCQs\n15,000\n✓\n×\n✓\n✓\n✓\nAraMed\nArabic\nQA\n270,000\n✓\n×\n✓\n✓\n×\nMedArabiQ\nArabic\nQA and MCQs\n700\n×\n×\n✓\n✓\n✓\nMedAraBench (Ours)\nArabic\nMCQs\n24,000\n✓\n✓\n✓\n✓\n✓\nassessing the performance of LLMs, especially as they are being deployed in diverse medical\ncontexts.\nSeveral benchmarks have been recently introduced in the medical domain, as summarized in\nTable 1. However, most of them focus almost exclusively on English (Jin et al., 2021; 2019).\nRecent work began to address this need but remain limited in scope and size (Abu Daoud\net al., 2025). Thus, there is a pressing need for large-scale benchmarks to assess and improve\nLLMs for Arabic-language medical reasoning.\nTo address those gaps, in this paper, we present MedAraBench, a comprehensive benchmark\nfor evaluating and advancing LLMs on Arabic medical tasks. The dataset consists of curated\nMultiple-Choice Questions (MCQs) spanning different specialties and difficulty tiers aligned\nwith stages of medical education. We propose a standardized development and evaluation\nprotocol to enable reproducible and clinically meaningful assessment of LLMs. Our key\ncontributions are as follows:\n• We introduce MedAraBench, a large-scale Arabic medical benchmark featuring\n24,883 MCQs across 19 medical specialties and five difficulty levels. The bench-\nmark includes standardized training and test sets to enable systematic evaluation\nand advancement of LLMs.\n• We perform extensive quality assessment via human expert evaluation, focusing on\nquestion clarity, clinical relevance, and medical correctness, as well as automated\nLLM-as-a-judge analysis.\n• We benchmark 16 state-of-the-art proprietary and open-source LLMs across three\ncategories (general purpose, Arabic-centric, medical) on the MedAraBench test set\nin the zero-shot setting to establish baseline performance for future research.\n2\nRelated Work\nIn recent years, several benchmark datasets have been developed to assess the capabilities\nof LLMs in the medical domain, driven by the expanding demand for applications that can\nstreamline clinical workflows. Despite this progress, Arabic remains underrepresented in\nclinical NLP, mainly due to the lack of high-quality data to support building clinical ap-\nplications in Arabic (Abdelaziz et al., 2025). As such, most existing benchmarks focus on\nEnglish. For instance, the Massive Multitask Language Understanding (MMLU) benchmark\nincludes question-answer pairs from the US Medical Licensing Exam (USMLE) (Hendrycks\net al., 2021). Jin et al. (2020) introduce MedQA, a multilingual benchmark dataset con-\nsisting of multiple-choice questions sourced from medical licensing exams in English and\nChinese. Pal et al. (2022) extend these benchmarks in MedMCQA to a multilingual evalu-\nation framework but remains limited in Arabic.\nRecent work have introduced new resources for medical evaluation in Arabic. Translations\nof existing datasets, such as of MMLU into 42 languages, including Arabic, provide valuable\ndata but lack the necessary nuances for proper integration into clinical practice (Singh,\n2025). AraMed presents an Arabic medical corpus and an annotated Arabic QA dataset\nsourced from online medical platforms (Alasmari et al., 2024). MedArabiQ presents one of\n2\n"}, {"page": 3, "text": "Published as a conference paper at ICLR 2026\nFigure 1: Overview of MedAraBench.\nthe first Arabic medical MCQ datasets, yet lacks specialty coverage, difficulty mapping, and\nexpert evaluation (Abu Daoud et al., 2025).\nSeveral evaluation frameworks have been proposed to evaluate the performance of clinical\nAI models. Kanithi et al. (2024) introduce ‘MEDIC’, a framework for evaluating LLMs from\nmedical reasoning andethics, to in-context learning and clinical safety. Wang et al. (2024)\npropose testing models on real-world input noise, dialogue interruptions, and reasoning jus-\ntifications. Despite the growing interest in multilingual evaluation, there remains a critical\ngap in comprehensive, high-quality, and clinically relevant benchmarks for under-served\nlanguages. We aim to address this gap by introducing a comprehensive Arabic benchmark.\nTable 1 provides a structured comparison between MedAraBench and other existing medical\nbenchmarks, covering key dimensions such as language coverage, specialty diversity, expert\nannotation, and public availability.\n3\nMethodology\nHere, we describe the steps pertaining to data collection, processing, evaluation, bench-\nmarking, few-shot learning, and finetuning, to facilitate proper reuse and fair comparison,\naligning with best practices for benchmark construction and evaluation. An overview is\nprovided in Figure 1.\n3.1\nData Collection and Pre-processing\nWe compiled a large repository of scanned paper-based exams, hosted on student-led social\nplatforms of regional medical schools. The dataset did not include any personal or real\npatient data, so anonymization was not necessary, and our data collection complied with\nprivacy and ethical standards. Considering the nature of the documents, we recruited pro-\nfessional typists to digitize the data. We then aggregated the paper-based exam documents\nto build a single MCQ dataset.\nUpon manual inspection by NLP researchers, we observed that several documents exhib-\nited issues such as missing or malformed correct answers, incomplete or duplicated answer\nchoices, non-standard formatting or misaligned fields, and ambiguous answer keys or extra-\n3\n"}, {"page": 4, "text": "Published as a conference paper at ICLR 2026\nneous non-MCQ content. To ensure dataset quality and model compatibility, we applied\nstrict filtering criteria to remove any questions with such issues. The filtering process was\nperformed manually by five NLP researchers. While data acquisition and preprocessing re-\nquired extensive effort, it highlights that the dataset is not publicly accessible in structured\nformats, thus reducing the likelihood of data contamination.\nEach question is associated with several annotations: (i) number of answer choices (i.e.,\nABCD (4 choices), ABCDE (5 choices), and ABCDEF (6 choices)), (ii) difficulty level cor-\nresponding to five years of study (Y1 - Y5), and (iii) medical specialty.\nThe questions\nfall under 19 medical specialties: Anatomy, Anesthesia, Biochemistry, Cell and Molecu-\nlar Biology, Chemistry and Physics, Embryology, Emergency Medicine, Internal Medicine,\nMedical Ethics, Microbiology, Ophthalmology, Pathology, Pediatrics, Pharmacology, Phys-\niology, Preventive Medicine, Psychiatry, Statistics, and Surgery. The scanned documents\ncollected from the repositories were originally categorized by specialty, which allowed us to\ndirectly inherit the medical specialty classification for each question.\nWe started by omitting all questions with 6 answer choices due to the small size of this subset\n(9 questions), allowing us to categorize our data into two categories (4 answer choices and\n5 answer choices). We then performed a stratified random split of the dataset into training\n(80%) and test sets (20%). This was to ensure that the medical specialties are represented\nevenly across the training and test sets (i.e., if the dataset contains 100 Cardiology questions,\n80 would be randomly included in the training set, while the remaining 20 would be in the\ntest set). A summary of the dataset in terms of token length distribution, medical specialty\ndistribution, and difficulty level distribution is presented in Appendix A.\nIt is important to note that the homogeneity regarding medical terminology consistency\nis essential in assessing the capabilities of LLMs on standardized Arabic medical tasks.\nAs it pertains to MedAraBench, we did not perform any external standardization across\nthe dataset as we acknowledge that this may not be a major issue from a benchmarking\nperspective since real-world medical QA may not necessarily be standardized in terms of\nterminology.\nWe acknowledge that Arabic medical terminology standardization remains an ongoing chal-\nlenge, with unified vocabulary frameworks still under development across Arabic-speaking\nregions (Amar, 2022), (Attia, 2024). The expert clinician evaluations described in Section\n3.2 serve as quality control, with reviewers achieving high agreement on all metrics despite\nthe absence of standardization.\n3.2\nQuality Assessment\nTo further assess the quality of our dataset, we conducted two analysis: human expert\nevaluation and using LLM-as-a-judge.\n3.2.1\nHuman Expert Evaluation\nWe designed our expert evaluation protocol to assess the data according to the following\ncriteria:\n1. Medical Accuracy: the extent to which the question, options, and correct answer\nreflect current, evidence-based medical knowledge (Scale: high or low) (Olatunji\net al., 2024; Iskander et al., 2024; Rejeleene et al., 2024).\n2. Clinical Relevance: the practical importance and applicability of the question\ncontent to real-world medical practice or education (Scale: high or low) (Iskander\net al., 2024; Olatunji et al., 2024).\n3. Question Difficulty: the complexity required to answer the question correctly\n(Scale: high or low) (Iskander et al., 2024).\n4. Question Quality: assessment of the MCQ construction quality (Scale: high or\nlow) following established medical education standards (Al-Rukban, 2006):\n• Clarity: question is clear, complete, and unambiguous.\n4\n"}, {"page": 5, "text": "Published as a conference paper at ICLR 2026\n• Option Homogeneity: all distractors are plausible and of similar type.\n• Single Best Answer one clearly correct option exists.\n• No Clueing: options do not provide clues to other answers.\nWe selected samples for review from the test set and determined the sample size based on\nCochran’s formula (Cochran, 1977), to create a representative sample size (Hosseini, 2024).\nWe estimated a single proportion at 95% confidence with a ±5 percentage-point margin of\nerror, using p = 0.5 as a conservative assumption when the true quality rate is unknown\nbecause it maximizes variance and therefore yields a safe upper bound on sample size. We\nfirst calculated an estimated sample size assuming an infinite population using\nn0 = z2p(1 −p)\ne2\nwith z = 1.96, p = 0.5, and e = 0.05. However, since the dataset is finite, we then applied\nthe finite population correction using\nn =\nn0\n1 + n0−1\nN\nWe recruited two board-certified clinicians specializing in Anesthesiology and Internal\nMedicine with Arabic clinical fluency and over 20 years of experience each. The reviews were\ndouble-blinded to model outputs and data provenance, and were conducted independently\nwith pre-registered instructions on Qualtrics (Provo, UT, https://www.qualtrics.com), a\nweb-based survey platform allowing for standardized presentation of evaluation criteria and\nindependent response collection across both reviewers.\n3.2.2\nLLM-as-a-Judge\nConsidering that only a subset of the test set was considered for the human expert evaluation,\nwe further introduced an LLM-as-a-judge evaluation protocol as an additional rating of data\nquality. Motivated by previous literature (Abu Daoud et al., 2025), we prompted four of\nour best-performing SOTA LLMs (gpt-03, gemini-2.0-flash, and claude-4-sonnet) to act as\nmedical education experts and to evaluate the MCQs along the same metrics used in our\nexpert quality evaluation: Medical Accuracy, Clinical Relevance, Question Difficulty, and\nQuestion Quality, on a binary (0 or 1) scale for the entire test set. Additionally, we calculated\nPearson Correlation coefficients for each model and the expert reviewers on the 378-question\ndataset evaluated by our medical experts. This approach gives us the advantage of providing\na more nuanced evaluation across a broader set of our data, while also providing insights\ninto the efficacy of LLMs in evaluating the quality of Arabic medical data relative to expert\nannotators.\n3.3\nBenchmarking Protocol\nTo introduce new benchmark results, we evaluated 16 proprietary and open-source models\non the test set of the MedAraBench benchmark. We set the models’ temperature as 0 to\nensure stable outputs as shown in previous work for classification of MCQs (Abu Daoud\net al., 2025). We selected one letter response per question.\n• Open-source models: Llama-3.3-70b-instruct (Grattafiori et al., 2024a), Llama-\n3.1-8b-instruct (Grattafiori et al., 2024b), Deepseek-chat-v3-0324 (DeepSeek-AI,\n2024), Allam-7b-instruct (Bari, 2024), Cohere c4ai-comman-r7b-arabic-02-2025\n(Alnumay, 2025), Medgemma-4b-it (Sellergren, 2025), Apollo-7b (Wang, 2024),\nFanar-C-1-8.7b (Abbas, 2025), BiMedix-Bi-27B (Pieri et al., 2024), and Med42-8b\n(Christophe, 2024).\n• Proprietary models: Claude-sonnet-4-20250514 (Anthropic, 2025), Gemini-2.0-\nflash (Google Cloud, 2025), GPT-4.1 (OpenAI, 2025a), GPT-5 (OpenAI, 2025b),\nGPT-o3 (OpenAI, 2025c), and Qwen-plus (Yang & et al., 2024).\n5\n"}, {"page": 6, "text": "Published as a conference paper at ICLR 2026\nOur prompt is shown in Appendix B below.Answers were extracted via post-processing,\nwhereas models were prompted to output the answer choice letter (A-D) directly, and we\nparsed their text responses using pattern matching to extract the answer. No set language\nparameters in API and model calls were used since models automatically detect Arabic text\nfrom the input. The results of our benchmarking experiments are shown in Table 4 below.\n3.4\nFew-shot Learning Protocol\nBuilding upon our zero-shot benchmarking protocol, we conducted few-shot experiments on\nLLaMa-3.1-8B-instruct to assess in-context learning capabilities. We provided 3 high-quality\nsample questions that were rated highly across all evaluation metrics (question quality, clin-\nical relevance, difficulty, and medical accuracy) by expert evaluators. These exemplar ques-\ntions were carefully selected from the training split and omitted from the test set to ensure\nfair evaluation. The few-shot examples covered diverse medical topics including anatomy,\nbiochemistry, and physiology, formatted with questions, options, and correct answers in\nArabic to maintain linguistic consistency with the test items. The chosen questions and\nfew-shot prompt are provided in Appendix B below.\n3.5\nLow-rank Adaptation\nWe further investigated parameter-efficient fine-tuning using QLoRA (Quantized Low-Rank\nAdaptation) on the Llama-3.1-8B-instruct model. The model was loaded in 4-bit precision\nand trained on the MedAraBench training split, formatted as Arabic prompt-response pairs\nfor multiple-choice question answering. LoRA adapters were applied to key attention mod-\nules (q proj, k proj, v proj, o proj) with standard hyperparameters, training for up to 800\nsteps with batched gradient accumulation. This approach enabled efficient adaptation to\nArabic medical terminology and reasoning patterns while preserving the models’ general ca-\npabilities. The fine-tuned models were evaluated on the same test set used in our zero-shot\nand few-shot experiments to directly measure the impact of MedAraBench data on model\nperformance.\n4\nResults\nIn this section, we present a summary of our dataset and the results of our expert quality\nevaluation, LLM-as-a-judge experiments, and benchmarking experiments.\n4.1\nDataset Summary\nThe initial dataset consisted of 34,333 MCQs. The manual filtering process resulted in a\nreduction of approximately 29% of the initial dataset, yielding 24,883 samples overall. The\ntraining set consisted of 19,894 samples, and the test set of 4,989 samples. An overview of\nthe dataset is shown in Figure A1 in Appendix A below, along with additional statistical\nsummaries of the dataset.\n4.2\nExpert Quality Assessment\nAt 95% confidence and ±5% margin, Cochran’s formula initially yielded n0 = 384 questions.\nThe final sample size was 378 after adjusting for a finite sample. Hence, our two annotators\ncompleted a review of 378 questions as a representative sample of the entire dataset. The\nresults of our data quality assessment and the inter-annotator agreement are summarized in\nTable 2. Our results show slight to fair levels of agreement across all metrics, with Medical\nAccuracy having the highest level of agreement with a Cohen’s Kappa score of 0.555 and a\npercentage agreement of 82%.\nAdditionally, we provide a detailed per-specialty breakdown of the evaluation results in\nAppendix C. To better assess the results, we investigate individual average and agreement\nscores for each specialty. Specifically, Figures C1, C2, C3, and C4 show the average accuracy\nper specialty for each metric, while Tables C1, C2, C3, and C4 show the average accuracy\n6\n"}, {"page": 7, "text": "Published as a conference paper at ICLR 2026\nTable 2: Expert quality assessment results for the representative data subset.\nMetric\nAverage [standard deviation]\nPercent Agreement\nCohen’s Kappa\nMedical Accuracy\n0.722 [0.448]\n82.0%\n0.555\nClinical Relevance\n0.653 [0.476]\n65.6%\n0.275\nQuestion Difficulty\n0.669 [0.471]\n65.6%\n0.233\nQuestion Quality\n0.767 [0.423]\n68.3%\n0.152\nTable 3: Evaluation results of LLM-as-a-judge applied to the test set.\nModel\nEvaluation Metric (average)\nMedical Accuracy\nClinical Relevance\nQuestion Difficulty\nQuestion Quality\nGPT-o3\n0.673 [0.469]\n0.827 [0.378]\n0.588 [0.492]\n0.841 [0.366]\nGemini 2.0 Flash\n0.717 [0.450]\n0.565 [0.496]\n0.815 [0.388]\n0.774 [0.366]\nClaude-4-Sonnet\n0.711 [0.453]\n0.749 [0.434]\n0.576 [0.494]\n0.764 [0.425]\nGPT-5\n0.533 [0.499]\n0.610 [0.488]\n0.597 [0.490]\n0.476 [0.499]\nand agreement results per specialty for each metric. All in all, our expert quality evaluations\nindicate that the data is of high quality with fair levels of agreement across a random sample\nof our test set.\nTable 4: Benchmark accuracies, model sizes, and training dataset size for all evaluated\nLLMs.\nModel Type\nModel Category\nModel\nModel Size\nTraining Dataset Size\nOverall Accuracy\nProprietary\nGeneral-purpose\nclaude-sonnet-4-20250514\nUnknown\nUnknown\n0.694\ngemini-2.0-flash\nUnknown\nUnknown\n0.654\ngpt-4.1\nUnknown\nUnknown\n0.673\ngpt-5\nUnknown\nUnknown\n0.764\ngpt-o3\nUnknown\nUnknown\n0.765\nOpen-source\nGeneral-purpose\ndeepseek-chat-v3-0324\n8B parameters\n1.8 trillion\n0.620\nqwen-plus\n8B parameters\n18 trillion\n0.618\nllama-3.3-70b-instruct\n70B parameters\n15 trillion\n0.547\nllama-3.1-8b-instruct\n8B parameters\n15 trillion\n0.170\nArabic-centric\nfanar-c-1-8.7b\n8.7B parameters\n1 trillion\n0.498\nallam-7b-instruct\n7B parameters\n5.2 trillion\n0.447\nc4ai-command-r7b-arabic-02-2025\n7B parameters\nUnknown\n0.381\nMedical\nmedgemma-4b-it\n4B parameters\n4 trillion\n0.390\napollo-7b\n7B parameters\nUnknown\n0.238\nmed42-8b\n8B parameters\n15T + 1B\n0.318\nbimedix-bi\n27B parameters\n632 million\n0.390\n4.3\nLLM-as-a-Judge Assessment\nThe results of our LLM-as-a-judge experiments are summarized in Table 3 and Table D1 in\nAppendix D across all four evaluation metrics. Our results show moderate agreement among\ndifferent LLMs and comparable results with the expert evaluation scores. To better under-\nstand the results in comparison with expert evaluations, we provide a detailed breakdown\nof the LLM-as-a-judge results in Appendix D.\n4.4\nBenchmarking SOTA LLMs\nWe report all benchmarking results in Table 4.\nOur results show that general-purpose\nand reasoning-optimized models consistently outperform specialized or smaller open-source\nmodels on MedAraBench. GPT-o3 and GPT-5 both demonstrate top overall accuracy (0.765\nand 0.764, respectively), while models like med42-8b and apollo-7b achieve the lowest scores\n(0.0.318 and 0.238, respectively). Tables E1 and E2 in Appendix E showcase a more detailed\nbreakdown of the accuracy scores per specialty and level for each of the 15 evaluated models.\n7\n"}, {"page": 8, "text": "Published as a conference paper at ICLR 2026\n4.5\nFew-shot Learning and QLoRA\nThe results of our few-shot learning and QLoRa protocols on Llama-3.1-8B-instruct are\nshown in Table 5 below.\nTable 5: Fewshot, and QLoRA fine-tuning performance compared to baseline zero-shot\naccuracy for Llama-8.1B-instruct\nModel\nBaseline Accuracy\nFewshot Accuracy\nLoRA Accuracy\nllama-3.1-8b-instruct\n0.170\n0.191\n0.320\nThe few-shot and QLoRA fine-tuning experiments demonstrated substantial improvements\nover the baseline zero-shot performance. Few-shot learning provided a modest gain of 12.4%\n(from 0.170 to 0.191), while QLoRA fine-tuning, boosted accuracy by 88.2% to 0.320 - nearly\ndoubling the model’s performance.\n5\nDiscussion\nOverall, in this study, we present MedAraBench, a new 25k question dataset consisting of\nboth training and test sets. We report performance baseline results for SOTA LLMs and\nhighlight critical differences in model capabilities under zero-shot settings. By exposing gaps\nin Arabic medical understanding, MedAraBench offers useful insights for the development\nof more inclusive, multilingual, and domain-specialized language models.\nThe expert quality assessment and LLM-as-a-judge experiments provide valuable insights\ninto the quality of our data and the plausibility of using LLMs to evaluate the quality\nof medical datasets. Namely, the expert quality evaluation yields average scores ranging\nfrom 0.653 - 0.767 across all 4 evaluation metrics, with percent agreements and Cohen’s\nKappa scores ranging from 0.656 - 0.820 and 0.152 - 0.555, respectively, indicating slight\nto fair levels of agreement across all metrics. The average evaluation metric scores indicate\nmoderate to high-quality data and fair agreement across annotators, but they indicate the\nneed for the curation of more benchmark datasets of higher quality and clinical relevance\nto properly assess the readiness of LLMs for clinical deployment. Generally, we see weak\nto moderate alignment between LLM-as-a-judge evaluation and expert evaluation of the\nquality of our dataset on all metrics, except for question difficulty. Specifically, GPT-o3 has\nthe highest Pearson Correlation scores with both experts A and B across Medical Accuracy\n(0.577 and 0.505, respectively), Clinical Relevance (0.252 and 0.0.377, respectively), and\nQuestion Quality (0.407 and 0.336, respectively). As for Question Difficulty, we see weak\nto no alignment across all models, with Gemini-2.0-Flash scoring highest with Expert A\n(0.019) and Claude-4-Sonnet scoring higher with Expert B (0.039). Our results indicate\nthat LLMs are yet to be reliable for proper evaluation of Arabic medical data, and highlight\nthe need for further training to better align with expert human standards.\nOur benchmark evaluation results coincide with prior research on LLM benchmarking in\nthe medical domain, whereas proprietary models typically outperform open-access models\nin structured tasks such as multiple-choice QA. This was previously demonstrated by Chen\net al. (2025) and Alonso et al. (2024), who demonstrated superior accuracy performance\nby proprietary models relative to open-source models in medical QA tasks across multiple\nlanguages. This was further shown by Abu Daoud et al. (2025), who demonstrated supe-\nrior performance by proprietary models such as Gemini 1.5 Pro, Claude 3.5 Sonnet, and\nGPT-4 in Arabic medical MCQ. Our results reinforce those findings, with all proprietary\nmodels performing at significantly higher or similar accuracy scores to open-source models.\nNamely, the proprietary models GPT-o3 and GPT-5 achieved the highest overall accuracies\non MedAraBench (0.765 and 0.764, respectively), with Claude-Sonnet-4 also performing\nstrongly (0.694). By contrast, even the largest open-source general-purpose models such as\nllama-3.3-70b-instruct and qwen-plus only reached 0.547 and 0.618, while domain-focused\nArabic-centric and medical models, such as allam-7b-instruct (0.447), fanar-c-1-8.7b (0.498),\nand medgemma-4b-it (0.390), remained below 0.5 accuracy. While training dataset size and\n8\n"}, {"page": 9, "text": "Published as a conference paper at ICLR 2026\nmodel scale correlate with performance (e.g., larger models like llama-3.3-70b-instruct and\ndeepseek-chat-v3-0324 outperformed smaller Arabic-centric or medical-only models), this\nwas not sufficient for open-source models to match proprietary LLMs. We also observe that\ngeneral-purpose models fine-tuned for reasoning consistently outperform Arabic-focused or\nmedical-specific models, underscoring the importance of both architectural and data scale,\nas well as transfer learning capabilities honed in proprietary labs.\nThe superior performance by proprietary models is possibly due to the larger training cor-\npora, stronger pretraining on structured datasets, more extensive instruction tuning, and\nspecialized reinforcement learning pipelines that proprietary models undergo relative to their\nopen-source counterparts. Additionally, our results show significantly higher performance\nby reasoning models (gpt-5 and gpt-o3) relative to other models, showing the promise and\nimportance of incorporating reasoning and explainability into medical NLP as a whole and\nArabic medical NLP specifically.\nThe few-shot learning and QLoRA experiments in Table 5 above revealed substantial im-\nprovements over the baseline zero-shot performance for Llama-3.1-8B-instruct, though with\nnotable differences in effectiveness between the two approaches. Few-shot learning provided\na modest gain of 12.4% (from 0.170 to 0.191), suggesting that in-context examples helped\nthe model better understand the medical question format and reasoning patterns. Addition-\nally, QLoRA fine-tuning, boosted accuracy by 88.2% to 0.320, nearly doubling the model’s\nperformance. The dramatic superiority of QLoRA fine-tuning highlights that parameter-\nefficient adaptation using domain-specific medical data is substantially more effective than\nin-context learning alone for adapting general-purpose models to specialized medical QA\ntasks in Arabic. These results emphasize the importance of targeted training protocols over\nprompt engineering for achieving competent performance in specialized medical language\nunderstanding tasks.\nWe further evaluate the evolution of models across generations by comparing contempo-\nrary and legacy model performance on the MedArabiQ benchmark (Abu Daoud et al.,\n2025). Our results, presented in Table G1 in Appendix G below, show that all contempo-\nrary models outperform legacy models on the MCQ task. Additionally, this analysis allows\nus to compare the merit of both the MedArabiQ and MedAraBench benchmarks, with\ngemini-2.0-flash, gpt-4.1, gpt-5, gpt-o3, and qwen-plus performing better on the MedAra-\nbiQ benchmark, while claude-sonnet-4-20250514 and llama-3.3-70b-instruct perform better\non MedAraBench. This indicates that the MedAraBench benchmark is more challenging\noverall, highlighting its value in advancing medical Arabic NLP.\nHowever, despite the significant improvement across generations, the highest performing\nmodel achieved an accuracy score of 0.765, which does not match expert-level performance\nand indicates clear headroom for improvement before being ready for deployment in clinical\nsettings. Furthermore, this allows us to raise important questions about what exactly LLMs\nare learning. High accuracy does not necessarily indicate deep understanding or clinical rea-\nsoning. Instead, models may be leveraging statistical associations and lexical patterns to\neliminate implausible answers. For example, frequent exposure to certain disease-treatment\npairs during pretraining may allow models to make educated guesses without reasoning\nthrough symptom progression or differential diagnosis. This distinction is crucial, particu-\nlarly in high-stakes applications such as medical education or decision support. Future work\nshould consider evaluating not just answer correctness, but also the rationale behind model\nchoices, possibly through explanation-based tasks or clinician scoring of model justifications.\n6\nLimitations and Future Work\nWhile our study provides a comprehensive evaluation of LLMs on Arabic medical MCQs and\nrepresents a substantial advancement in benchmarking capabilities, several limitations exist.\nFirst, the dataset is limited in its capability to evaluate LLMs on classification tasks only\ndue to the nature of the MCQ dataset, preventing the evaluation of LLMs in generative\ntasks.\nAdditionally, although the data source was not available in a structured digital\nformat and required extensive digitization and cleaning efforts, we cannot certainly rule out\ncontamination. Furthermore, our data assumes fluency in Modern Standard Arabic, which,\n9\n"}, {"page": 10, "text": "Published as a conference paper at ICLR 2026\ndespite being common in formal settings, may not fully align with linguistic realities. This\ncan affect the generalizability of MedAraBench to learners or practitioners accustomed to\ndialectal or mixed-language instruction.\nAnother limitation emerges from our expert quality evaluation experiments. Although our\ndataset was reviewed by two expert clinicians, we observed occasional inconsistencies across\ntheir assessments. We acknowledge that expert disagreement and inherent subjectivity are\ncommon in clinical judgment, but recognize the need for broader consensus in future vali-\ndation efforts. Furthermore, our data is largely skewed toward easier difficulty levels (Y1 at\n56% and Y2 at 22.89%) relative to more advanced levels (Y3 at 12.04%, Y4 at 3.38%, and\nY5 at 5.14%. This distribution is due to the availability of source materials collected from\nregional medical repositories, which contained disproportionately more early-year content.\nWhile this evaluation limits the benchmark’s capacity to assess advanced clinical reasoning,\nit is representative of the present educational resource landscape in Arabic, and provides\nvalue by establishing a baseline performance for Arabic medical understanding across the\npresent difficulty levels and specialties, revealing that even basic medical reasoning remains\nchallenging for current LLMs. Moreover, our data is text-only in its format, limiting its ap-\nplicability to accommodating image-based reasoning required in specialties such as radiology\nand dermatology, and warranting expansions to other data modalities in future work.\nIn this study, we primarily focused on zero-shot evaluation of model performance, providing\nan assessment without further adaptation. While this is an important evaluation framework,\nfuture work could explore the impact of few-shot and chain-of-thought prompting strategies,\nas well as fine-tuning as an opportunity to improve model performance. Furthermore, future\nwork could warrant the incorporation of dialectal data to enhance model adaptability across\ndiverse Arabic clinical settings. Another important area of future work is to investigate the\nuse of Arabic-based lecture notes to advance medical Arabic NLP beyond MCQs.\n7\nConclusions\nTo conclude, we introduced a large-scale Arabic medical benchmark designed to evaluate the\nzero-shot performance of LLMs on curated MCQs. Covering 19 medical specialties and span-\nning five difficulty levels, MedAraBench provides a comprehensive and fine-grained lens for\nassessing Arabic medical reasoning in LLMs. Our benchmark provides an advancement for\ndeveloping benchmarks in the Arabic language and exposes limitations in the performance\nof current LLMs in low-resource language tasks and the need for robust multilingual train-\ning strategies. Future work should explore fine-tuning strategies and the curation of larger\nand higher-quality datasets tailored to Arabic medical contexts. We release MedAraBench\nin hopes of supporting downstream clinical applications, and we hope that it serves as a\ncatalyst for continued research at the intersection of Arabic NLP and medical AI.\nEthics Statement\nThe authors disclose the use of generative AI tools to assist with LaTeX code cleanup and\nformatting only, with all content, analyses, and conclusions authored and verified by the\nresearchers involved in this project.\nReproducibility Statement\nThe MedAraBench dataset is available at https://github.com/nyuad-cai/MedAraBench\nAcknlowedgements\nThis work was supported by the Meem Foundation, the NYUAD Center for Artificial In-\ntelligence and Robotics, funded by Tamkeen under the NYUAD Research Institute Award\nCG010, the Center for Cyber Security (CCS), funded by Tamkeen under the NYUAD Re-\nsearch Institute Award G1104, and the Center for Interdisciplinary Data Science & AI\n(CIDSAI), funded by Tamkeen under the NYUAD Research Institute Award CG016. The\nresearch was carried out on the High Performance Computing resources at New York Uni-\nversity Abu Dhabi (Jubail).\n10\n"}, {"page": 11, "text": "Published as a conference paper at ICLR 2026\nReferences\nUmar Abbas. Fanar: An arabic-centric multimodal generative ai platform. arXiv preprint\narXiv:2501.13944, 2025.\nMariam Essam Abdelaziz, Mohanad A. Deif, Shabbab Ali Algamdi, and Rania Elgohary.\nA benchmark arabic dataset for arabic question classification using aafaq framework.\nScientific Data, 2025.\nMouath Abu Daoud, Chaimae Abou Zahir, Leen Kharouf, Walid AlEisawi, Nizar Habash,\nand Farah Shamout. Medarabiq: Benchmarking large language models on arabic medical\ntasks. PMLR, 2025.\nMohammed Al-Rukban. Guidelines for the construction of multiple choice questions tests.\nJournal of Family Community Medicine, 2006. URL https://pmc.ncbi.nlm.nih.gov/\narticles/PMC3410060/.\nAshwag Alasmari, Sarah Alhumoud, and Waad Alshammari.\nAramed: Arabic medical\nquestion answering using pretrained transformer language models. In Proceedings of the\n6th Workshop on Open-Source Arabic Corpora and Processing Tools (OSACT), pp. 50–56,\n2024.\nYazeed AlMoaiad.\nChallenges in natural arabic language processing.\nEdelweiss Applied\nScience and Technology, 2024.\nYazeed Alnumay.\nCommand r7b arabic: A small, enterprise focused, multilingual, and\nculturally aware arabic llm. arXiv preprint arXiv:2503.14603v1, 2025.\nI˜nigo Alonso, Maite Oronoz, and Rodrigo Agerri. Medexpqa: Multilingual benchmarking of\nlarge language models for medical question answering. Artificial Intelligence in Medicine,\n155:102938, September 2024. ISSN 0933-3657. doi: 10.1016/j.artmed.2024.102938. URL\nhttp://dx.doi.org/10.1016/j.artmed.2024.102938.\nKhatra Amar. Arabic morphological productivity in the translation of medical terminology.\nPhD Thesis - UNIVERSITI SAINS MALAYSIA, 2022.\nAnthropic.\nClaude\nopus\n4\n&\nclaude\nsonnet\n4\n—\nsystem\ncard.\nhttps://www.anthropic.com/claude-4-system-card, 2025.\nYasser Mohamed Attia.\nThe challenges of achieving dynamic equivalence in the arabic\ntranslation of medical terminology for heart and brain diseases: Difficulties and recom-\nmendations. Journal of Faculty of Arts, Port Said University, 30(30):14–31, 2024.\nM Saiful Bari.\nAllam: Large language models for arabic and english.\narXiv preprint\narXiv:2407.15390, 2024.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,\nErik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, An-\nnie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa\nDoumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-\nFei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E.\nHo, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,\nPratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab,\nPang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal\nLadhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li,\nTengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele\nMunyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie,\nJuan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel\nPapadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher R´e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih,\n11\n"}, {"page": 12, "text": "Published as a conference paper at ICLR 2026\nKrishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram`er,\nRose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie,\nMichihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun\nZhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities\nand risks of foundation models, 2022. URL https://arxiv.org/abs/2108.07258.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.\nHanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language\nmodels on answering and explaining challenging medical questions, 2025. URL https:\n//arxiv.org/abs/2402.18060.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with\npathways, 2022. URL https://arxiv.org/abs/2204.02311.\nClement Christophe. Med42-v2: A suite of clinical llms. arXiv preprint arXiv:2408.06142,\n2024.\nWilliam Gemmell Cochran. Sampling techniques. john wiley & sons, 1977.\nDeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.\nAli Farghali and Khaled Shaalan.\nArabic natural language processing: Challenges and\nsolutions. ACM Transactions on Asian Language Information Processing (TALIP), 2009.\nGoogle\nCloud.\nGemini\n2.0\nflash\n—\ngenerative\nai\non\nvertex\nai.\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash,\nMay\n2025.\nA. Grattafiori et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024a.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, and Akhil Mathur et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024b.\nNizar Habash. Introduction to arabic natural language processing. ACL Tutorial, 2005.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL\nhttps://arxiv.org/abs/2009.03300.\nPedram Hosseini. A benchmark for long-form medical question answering. arXiv preprint,\n2024.\n12\n"}, {"page": 13, "text": "Published as a conference paper at ICLR 2026\nShadi Iskander, Nachshon Cohen, and Zohar Karnin. Quality matters: Evaluating synthetic\ndata for tool-using llms. arXiv preprint arXiv:2409.16341, 2024. URL https://arxiv.\norg/abs/2409.16341.\nLuyi Jiang, Jiayuan Chen, Lu Lu, Xinwei Peng, Lihao Liu, Junjun He, and Jie Xu. Bench-\nmarking chinese medical llms: A medbench-based analysis of performance gaps and hier-\narchical optimization strategies, 2025. URL https://arxiv.org/abs/2503.07306.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.\nWhat disease does this patient have?\na large-scale open domain question answering\ndataset from medical exams, 2020. URL https://arxiv.org/abs/2009.13081.\nDi Jin, Yansong Pan, Tao Ouyang, and Pascale Fung.\nWhat disease does this patient\nhave? a large-scale open domain question answering dataset from medical exams. arXiv\npreprint arXiv:2009.13081, 2021. URL https://arxiv.org/abs/2009.13081. Submitted\nto AAAI 2021.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A\ndataset for biomedical research question answering. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 2567–2577, 2019.\nPraveen K. Kanithi, Cl´ement Christophe, Marco A. F. Pimentel, Tathagata Raha, Nada\nSaadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, and Shadab Khan.\nMedic: Towards a comprehensive framework for evaluating llms in clinical applications.\narXiv preprint arXiv:2409.07314, 2024.\nTobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayo-\ndele, Mardhiyah Sanni, and Chinemelu Aka. Afrimed-qa: A pan-african, multi-specialty,\nmedical question-answering benchmark dataset. arXiv preprint arXiv:2411.15640, 2024.\nURL https://arxiv.org/html/2411.15640.\nOpenAI. Gpt-4 technical report. https://openai.com/index/gpt-4-1/, April 2025a.\nOpenAI. GPT-5 system card. https://cdn.openai.com/gpt-5-system-card.pdf, 2025b.\nOpenAI. Openai o3 and o4-mini system card. https://cdn.openai.com/pdf/o3-and-o4-mini-\nsystem-card.pdf, 2025c.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-\nscale multi-subject multi-choice dataset for medical domain question answering. In Con-\nference on health, inference, and learning, pp. 248–260. PMLR, 2022.\nSara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman\nKhan, Timothy Baldwin, and Hisham Cholakkal. Bimedix: Bilingual medical mixture of\nexperts llm. arXiv preprint arXiv:2402.13253, 2024.\nRick Rejeleene, Xiaowei Xu, and Johm Talburt. Towards trustable language models: Inves-\ntigating information quality of large language models. arXiv preprint arXiv:2401.13086,\n2024. URL https://arxiv.org/abs/2401.13086.\nAndrew Sellergren. Medgemma technical report. arXiv preprint arXiv:2507.05201, 2025.\nShivalika Singh. Global mmlu: Understanding and addressing cultural and linguistic biases\nin multilingual evaluation. Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics, 2025.\nFelix Stahlberg.\nNeural machine translation: A review and survey, 2020.\nURL https:\n//arxiv.org/abs/1912.02047.\nBin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, and Nancy Chen. Resilience of large\nlanguage models for noisy instructions. EMNLP, 2024.\n13\n"}, {"page": 14, "text": "Published as a conference paper at ICLR 2026\nCunxiang Wang, Pai Liu, and Yue Zhang.\nCan generative pre-trained language models\nserve as knowledge bases for closed-book qa?, 2021.\nURL https://arxiv.org/abs/\n2106.01561.\nXidong Wang. Apollo: A lightweight multilingual medical llm towards democratizing med-\nical ai to 6b people. arXiv preprint arXiv:2403.03640, 2024.\nAn Yang and et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL\nhttps://arxiv.org/abs/2412.15115.\nHan Yang, Mingchen Li, Huixue Zhou, Yongkang Xiao, Qian Fang, and Rui Zhang. Large\nlanguage model synergy for ensemble learning in medical question answering: Design and\nevaluation study. J Med Internet Res, 2025.\n14\n"}, {"page": 15, "text": "Published as a conference paper at ICLR 2026\nA\nData Analysis\nA.1\nData Summary\nAn overview of the MedAraBench dataset according to the distribution of questions per\ndifficulty level and speialty is shown in Figure A1 below.\n(a)\n(b)\nFigure A1: Overview of dataset according to difficulty level and specialties.\nIn terms of specialty, the largest subsets of the dataset fall under Anatomy (6100 questions\n- 24.61% of the dataset) and Physiology (3302 - 13.32%), while the smallest subset falls\nunder Pathology (56 - 0.23%). A more detailed breakdown of the distribution of questions\naccording to specialty can be shown in Table A1 and Figure A1 (b) above.\nOn the other hand, in terms of difficulty level, the largest subset of the dataset fall under\nY1 (15095 questions - 60.89% of the dataset) while the smallest subset falls under Y4 (1033\n- 4.17%). A more detailed breakdown of the distribution of questions according to level\ncan be shown in Table A2 and Figure A1 (a) above. Additionally, Figure A2 shows the the\ncomposition of the five levels across the 16 specialties included in MedAraBench.\n15\n"}, {"page": 16, "text": "Published as a conference paper at ICLR 2026\nTable A1: Distribution of questions per medical specialty.\nMedical Specialty\nNumber of Questions Percentage\nAnatomy\n6100\n24.61%\nAnesthesia\n405\n1.63%\nBiochemistry\n1387\n5.59%\nCell and Molecular Biology\n3155\n12.73%\nChemistry and Physics\n3894\n15.71%\nEmbryology\n184\n0.74%\nEmergency Medicine\n120\n0.48%\nInternal Medicine\n762\n3.07%\nMicrobiology\n750\n3.03%\nOphthalmology\n1318\n5.32%\nPathology\n56\n0.23%\nPharmacology\n329\n1.33%\nPhysiology\n3302\n13.32%\nStatistics\n1795\n7.24%\nSurgery\n357\n1.44%\nTable A2: Distribution of questions per difficulty level.\nDifficulty Level Number of Questions Percentage\nY1\n15095\n60.89%\nY2\n4954\n19.98%\nY3\n2313\n9.33%\nY4\n1033\n4.17%\nY5\n1396\n5.63%\nFigure A2: Level composition of the 5 different difficulty levels (Y1 - Y5) within each\nspecialty in the MedAraBench dataset.\n16\n"}, {"page": 17, "text": "Published as a conference paper at ICLR 2026\nA.2\nToken Length Distribution\nThere are 24,791 questions in our dataset. The questions are moderate in length, with a total\naverage length of 37.86 characters. The answers vary in format, whereas 18554 questions\nhave four answer choices (A, B, C, and D) and 6228 questions have five answer choices\n(A, B, C, D, and E). The average answer length across the datasets is 171.09 characters.\nFigure A3 below gives a detailed breakdown of the distributions of text lengths of the entire\ndataset.\nFigure A3: Distribution of text lengths in the MedAraBench dataset: (a) Distribution of\nquestion length; (b) Distribution of answer length; (c) Distribution of Option A length; (d)\nDistribution of Option B length; (e) Distribution of Option C length; (f) Distribution of\nOption D length; and (g) Distribution of Option E length.\n17\n"}, {"page": 18, "text": "Published as a conference paper at ICLR 2026\nB\nBenchmarking and Few-shot Learning Prompts\nB.1\nBenchmarking Prompt\nListing 1: Benchmarking prompt for medical MCQ evaluation\n\"You are an expert\nmedical\nvirtual\nassistant.\nPlease\nprovide\nthe\ncorrect\nanswer\nletter (A, B, C, or D)\nfor the\nfollowing\nArabic\nmedical\nmultiple -choice\nquestion.\nQuestion:\n{ question_text_in_Arabic }\nOptions:\nA:{ option_A_in_Arabic }\nB:{ option_B_in_Arabic }\nC:{ option_C_in\nArabic}\nD:{ option_D_in\nArabic}\nAnswer:\"\nB.2\nFew-Shot Learning Prompt\nListing 2: Few-shot prompt for medical MCQ evaluation\n\"You are an expert\nmedical\nassistant. You will be provided\nwith\na few Arabic\nmedical\nsample\nquestions\nand their\ncorrect\nanswers , then a new\nquestion\nthat you must\nanswer.\nSample\nQuestions:\n{ sample_questions }\nNow , the new\nquestion:\nQuestion: {question}\nOptions:\n{options_text}\n(Please\nanswer\nwith the letter\nchoice { allowed_letters} ONLY):\"\nThe sample questions are as follows:\n18\n"}, {"page": 19, "text": "Published as a conference paper at ICLR 2026\nC\nExpert Quality Assessment Details\nIn this appendix, we provide a detailed breakdown of the expert evaluation results introduced\nin Section 2. While the main text summarizes overall averages and agreement levels across\nall specialties, here we report per-specialty results to give a more fine-grained view of model\nperformance and annotator consistency.\nFigures C1–C4 present the distribution of annotator scores across specialties for each of\nthe four evaluation metrics (Medical Accuracy, Clinical Relevance, Question Difficulty, and\nQuestion Quality). The corresponding Tables C1– C4 report the average scores, number of\nevaluated questions, percentage agreement, and Cohen’s Kappa values for each specialty.\nTogether, these results highlight the variability across domains and provide context for\ninterpreting the aggregate quality metrics shown in the main text.\nFigure C1: Overall annotator accuracy scores per specialty.\nFigure C2: Overall annotator relevance scores per specialty.\n19\n"}, {"page": 20, "text": "Published as a conference paper at ICLR 2026\nFigure C3: Overall annotator difficulty scores per specialty.\nFigure C4: Overall annotator quality scores per specialty.\nTable C1: Annotator accuracy scores per specialty.\nSpecialty\nAverage [standard deviation] Number of Questions\nPercent Agreement\nCohen’s Kappa\nAnatomy\n0.689 [0.464]\n103\n0.748\n0.431\nAnesthesia\n0.600 [0.503]\n10\n0.600\n0.167\nBiochemistry\n0.625 [0.489]\n24\n0.750\n0.471\nCell and Molecular Biology\n0.613 [0.489]\n62\n0.774\n0.529\nChemistry and Physics\n0.708 [0.457]\n48\n0.917\n0.798\nEmbryology\n1.000 [0.000]\n1\n1.000\n-\nEmergency Medicine\n1.000 [0.000]\n2\n1.000\n-\nInternal Medicine\n0.917 [0.280]\n18\n0.944\n0.640\nMicrobiology\n0.400 [0.498]\n15\n0.733\n0.455\nOphthalmology\n0.820 [0.388]\n25\n0.880\n0.603\nPathology\n0.625 [0.518]\n4\n0.750\n-\nPharmacology\n1.000 [0.000]\n4\n1.000\n-\nPhysiology\n0.895 [0.308]\n43\n0.884\n0.380\nStatistics\n0.947 [0.226]\n19\n1.000\n1.000\n20\n"}, {"page": 21, "text": "Published as a conference paper at ICLR 2026\nTable C2: Annotator relevance scores per specialty.\nSpecialty\nAverage [standard deviation] Number of Questions\nPercent Agreement\nCohen’s Kappa\nAnatomy\n0.699 [0.479]\n103\n0.641\n0.225\nAnesthesia\n1.000 [0.000]\n10\n1.000\n-\nBiochemistry\n0.667 [0.494]\n24\n0.708\n0.400\nCell and Molecular Biology\n0.726 [0.501]\n62\n0.435\n0.095\nChemistry and Physics\n0.333 [0.470]\n48\n0.688\n0.286\nEmbryology\n1.000 [0.000]\n1\n1.000\n-\nEmergency Medicine\n1.000 [0.000]\n2\n1.000\n-\nInternal Medicine\n1.000 [0.280]\n18\n0.833\n0.000\nMicrobiology\n1.000 [0.407]\n15\n0.600\n0.000\nOphthalmology\n1.000 [0.303]\n25\n0.800\n0.000\nPathology\n1.000 [0.463]\n4\n0.500\n-\nPharmacology\n1.000 [0.000]\n4\n1.000\n-\nPhysiology\n0.930 [0.275]\n43\n0.884\n0.224\nStatistics\n0.947 [0.504]\n19\n0.211\n0.021\nTable C3: Annotator difficulty scores per specialty.\nSpecialty\nAverage [standard deviation] Number of Questions\nPercent Agreement\nCohen’s Kappa\nAnatomy\n0.816 [0.464]\n103\n0.650\n0.240\nAnesthesia\n0.700 [0.510]\n10\n0.500\n0.074\nBiochemistry\n0.708 [0.476]\n24\n0.750\n0.442\nCell and Molecular Biology\n0.661 [0.463]\n62\n0.710\n0.320\nChemistry and Physics\n0.958 [0.408]\n48\n0.625\n0.027\nEmbryology\n1.000 [0.000]\n1\n1.000\n-\nEmergency Medicine\n1.000 [0.577]\n2\n0.000\n-\nInternal Medicine\n0.722 [0.487]\n18\n0.611\n0.182\nMicrobiology\n0.933 [0.305]\n15\n0.800\n-0.098\nOphthalmology\n0.880 [0.418]\n25\n0.720\n0.229\nPathology\n0.500 [0.535]\n4\n1.000\n-\nPharmacology\n0.500 [0.518]\n4\n0.750\n-\nPhysiology\n0.372 [0.494]\n43\n0.651\n0.281\nStatistics\n0.316 [0.500]\n19\n0.368\n-0.009\nTable C4: Annotator quality scores per specialty.\nSpecialty\nAverage [standard deviation] Number of Questions\nPercent Agreement\nCohen’s Kappa\nAnatomy\n0.573 [0.451]\n103\n0.573\n0.044\nAnesthesia\n0.600 [0.489]\n10\n0.700\n0.348\nBiochemistry\n0.583 [0.449]\n24\n0.625\n0.143\nCell and Molecular Biology\n0.548 [0.448]\n62\n0.484\n-0.120\nChemistry and Physics\n0.708 [0.457]\n48\n0.792\n0.496\nEmbryology\n1.000 [0.000]\n1\n1.000\n-\nEmergency Medicine\n1.000 [0.000]\n2\n1.000\n-\nInternal Medicine\n0.889 [0.280]\n18\n0.944\n0.640\nMicrobiology\n0.533 [0.490]\n15\n0.533\n0.037\nOphthalmology\n0.760 [0.370]\n25\n0.840\n0.432\nPathology\n0.750 [0.518]\n4\n0.750\n-\nPharmacology\n1.000 [0.000]\n4\n1.000\n-\nPhysiology\n0.884 [0.292]\n43\n0.860\n0.178\nStatistics\n0.895 [0.273]\n19\n0.842\n-0.075\n21\n"}, {"page": 22, "text": "Published as a conference paper at ICLR 2026\nD\nLLM-as-a-Judge\nThe models were provided with the full test set of MCQ (stem, options, and correct answer)\nand instructed to return only valid JSON output. The exact prompt was:\nListing 3: Prompt provided to LLMs\nYou are a medical\neducation\nexpert. Evaluate\nthe\nfollowing\nmultiple -choice\nquestion (MCQ) on a binary\nscale (0 or 1) for each of the\nfollowing\nmetrics:\n1. Medical\nAccuracy\n(1=high , 0=low)\n2. Clinical\nRelevance\n(1=high , 0=low)\n3. Question\nDifficulty\n(1=high , 0=low)\n4. Question\nQuality\n(1=high , 0=low)\nImportant: Return\nONLY\nvalid\nJSON. No explanations , no markdown\n, no text.\nThe\nresponse\nmust be exactly\nlike this:\n{\n\" medical_accuracy \": <0-1>,\n\" clinical_relevance \": <0-1>,\n\" question_difficulty \": <0-1>,\n\" question_quality \": <0-1>\n}\nQuestion\nstem: {row[’Question ’]}\nOptions:\n{options_text}\nCorrect\nanswer: {row[’Correct\nAnswer ’]}\nThis setup ensured that LLM outputs were standardized and machine-readable. By ag-\ngregating scores across thousands of test questions, we obtained descriptive statistics and\nmodel-wise distributions that enabled a more fine-grained analysis than binary human rat-\nings alone.\nTo examine agreement between models and expert evaluators, Pearson correlation coeffi-\ncients were calculated on a per-question basis. The results of this analysis are shown in\nTable D1 below. Our Pearson Correlation results show GPT-o3 was the best performing\nmodel in terms of alignment with expert evaluations. As such, we opted to use GPT-o3 to\ncomplete LLM-as-aJudge evaluations over the entire training set of MedAraBench, allowing\nus to have a full evaluation of the entire dataset. Additionally, we ran t-tests to compare\nGPT-o3 scores on the training and test sets and found no significant difference on all 4 eval-\nuation metrics, indicating consistent quality across splits. The results of this experiment\nare shown in table D2 below.\nTable D1: Pearson correlation coefficients between model and expert ratings.\nExpert A\nExpert B\nModel\nGPT-o3\nClaude-4-Sonnet\nGemini-2.0-Flash\nGPT-5\nGPT-o3\nClaude-4-Sonnet\nGemini-2.0-Flash\nGPT-5\nMedical Accuracy\n0.577\n0.065\n0.023\n0.043\n0.505\n0.053\n0.053\n0.068\nClinical Relevance\n0.252\n0.165\n0.176\n0.071\n0.377\n0.131\n0.131\n0.104\nQuestion Quality\n0.407\n0.007\n0.023\n0.044\n0.336\n0.062\n0.062\n0.033\nQuestion Difficulty\n-0.114\n-0.070\n0.019\n-0.116\n-0.018\n0.039\n0.039\n-0.049\n22\n"}, {"page": 23, "text": "Published as a conference paper at ICLR 2026\nTable D2: GPT-o3 LLM-as-a-judge scores on training and test sets (mean [std]) with\nt-test p-values.\nDataset\nMedical Accuracy\nClinical Relevance Question Quality\nQuestion Difficulty\nTraining set\n0.638 [0.481]\n0.821 [0.383]\n0.839 [0.367]\n0.561 [0.496]\nTest set\n0.673 [0.469]\n0.827 [0.378]\n0.588 [0.492]\n0.841 [0.366]\nP-value\n0.0001\n0.0362\n0.0001\n0.0001\n23\n"}, {"page": 24, "text": "Published as a conference paper at ICLR 2026\nE\nPerformance per Specialty and Level\nTo complement the overall evaluation, we analyze model accuracy across different medical\nspecialties and difficulty levels. This breakdown highlights domain-specific strengths and\nweaknesses, as well as how performance varies across the five curriculum levels (Y1—Y5).\nWe provide a detailed breakdown of model accuracies per specialty and levels in Tables E1\nand E2 below.\nIn terms of specialty performance, our analysis reveals clear superior performance across\nmodels, with GPT-5 and GPT-o3 dominating most domains while apollo-7b and med42-8b\nconsistently underperformed. Notably, the smaller medgemma-4b-it excelled in Pathology\n(0.574), demonstrating that specialized training can sometimes overcome scale limitations.\nAs for difficulty level performance, all models exhibited an accuracy drop from Y1 to Y3\nbefore recovering in Y4-Y5. GPT-5 maintained the highest performance across four of five\nlevels, while weaker models showed steeper declines at intermediate levels. This consistent\npattern indicates that Y3 questions, which likely require more complex clinical reasoning,\npresent the greatest challenge across all model architectures.\nThe significant performance variation between difficulty levels highlights critical limitations\nin current LLMs for medical applications. The average 15-20% performance drop from Y1 to\nY3 suggests that while models excel at factual recall in early curriculum years, they struggle\nwith the integrative clinical reasoning required at intermediate levels. This difficulty scaling\neffect was most pronounced for smaller models, indicating that scale and sophisticated\ntraining are crucial for handling complex medical reasoning tasks. These findings suggest\nLLMs may be better suited for foundational knowledge and specialized domains than for\ncomplex clinical decision-making.\n24\n"}, {"page": 25, "text": "Published as a conference paper at ICLR 2026\nTable E1: Overall Model Accuracy per Specialty.\nSpecialty\nModel\nAnatomy\nAnesthesia\nBiochemistry\nEmbryology\nEmergency\nMedicine\nInternal\nMedicine\nMicrobiology\nOphthalmology\nOther\nPathology\nPharmacology\nPhysiology\nStatistics\nSurgery\nallam-7b-instruct\n0.402\n0.469\n0.458\n0.333\n0.478\n0.351\n0.338\n0.320\n0.151\n0.463\n0.409\n0.484\n0.624\n0.430\napollo-7b\n0.315\n0.198\n0.237\n0.000\n0.435\n0.175\n0.192\n0.178\n0.076\n0.259\n0.106\n0.183\n0.253\n0.109\nc4ai-command-r7b-arabic-02-2025\n0.322\n0.173\n0.415\n0.333\n0.304\n0.361\n0.265\n0.239\n0.131\n0.333\n0.303\n0.442\n0.571\n0.391\nclaude-sonnet-4-20250514\n0.691\n0.741\n0.720\n0.333\n0.739\n0.784\n0.483\n0.691\n0.206\n0.722\n0.727\n0.747\n0.794\n0.703\ndeepseek-chat-v3-0324\n0.596\n0.593\n0.652\n0.333\n0.565\n0.649\n0.404\n0.568\n0.191\n0.630\n0.667\n0.693\n0.750\n0.562\nfanar-c-1-8.7b\n0.415\n0.309\n0.499\n0.500\n0.565\n0.392\n0.258\n0.355\n0.152\n0.500\n0.364\n0.496\n0.621\n0.391\ngemini-2.0-flash\n0.645\n0.679\n0.698\n0.500\n0.652\n0.701\n0.483\n0.668\n0.193\n0.630\n0.636\n0.702\n0.772\n0.648\ngpt-4.1\n0.681\n0.580\n0.712\n0.333\n0.652\n0.711\n0.404\n0.653\n0.197\n0.722\n0.561\n0.757\n0.794\n0.656\ngpt-5\n0.795\n0.778\n0.768\n0.667\n0.739\n0.866\n0.709\n0.792\n0.215\n0.759\n0.864\n0.822\n0.810\n0.766\ngpt-o3\n0.797\n0.802\n0.792\n0.500\n0.783\n0.835\n0.689\n0.772\n0.215\n0.759\n0.864\n0.823\n0.835\n0.727\nllama-3.1-8b-instruct\n0.335\n0.296\n0.332\n0.167\n0.348\n0.175\n0.225\n0.286\n0.116\n0.315\n0.227\n0.349\n0.478\n0.328\nllama-3.3-70b-instruct\n0.487\n0.556\n0.585\n0.333\n0.435\n0.577\n0.325\n0.506\n0.177\n0.463\n0.576\n0.598\n0.695\n0.539\nmed42-8b\n0.304\n0.235\n0.348\n0.167\n0.043\n0.309\n0.185\n0.220\n0.107\n0.389\n0.242\n0.321\n0.475\n0.297\nmedgemma-4b-it\n0.365\n0.272\n0.380\n0.500\n0.391\n0.247\n0.305\n0.375\n0.125\n0.574\n0.273\n0.424\n0.549\n0.320\nqwen-plus\n0.579\n0.630\n0.658\n0.500\n0.478\n0.670\n0.397\n0.544\n0.193\n0.593\n0.636\n0.682\n0.766\n0.648\nTable E2: Overall Model Accuracy per Difficulty Level\nModel\nLevel\nallam-7b-instruct\napollo-7b\nc4ai-command-r7b-arabic-02-2025\nclaude-sonnet-4-20250514\ndeepseek-chat-v3-0324\nfanar-c-1-8.7b\ngemini-2.0-flash\ngpt-4.1\ngpt-5\ngpt-o3\nllama-3.1-8b-instruct\nllama-3.3-70b-instruct\nmed42-8b\nmedgemma-4b-it\nqwen-plus\nY1\n0.475\n0.263\n0.418\n0.707\n0.648\n0.488\n0.667\n0.699\n0.769\n0.773\n0.366\n0.566\n0.339\n0.416\n0.644\nY2\n0.430\n0.215\n0.350\n0.664\n0.578\n0.433\n0.618\n0.635\n0.723\n0.730\n0.324\n0.510\n0.322\n0.357\n0.581\nY3\n0.367\n0.172\n0.306\n0.548\n0.490\n0.341\n0.534\n0.485\n0.664\n0.652\n0.253\n0.439\n0.244\n0.297\n0.490\nY4\n0.439\n0.179\n0.291\n0.745\n0.582\n0.372\n0.673\n0.638\n0.770\n0.750\n0.316\n0.571\n0.260\n0.321\n0.638\nY5\n0.316\n0.184\n0.253\n0.698\n0.562\n0.347\n0.656\n0.667\n0.799\n0.781\n0.271\n0.507\n0.226\n0.354\n0.549\n25\n"}, {"page": 26, "text": "Published as a conference paper at ICLR 2026\nF\nAnswer Choice Distribution Balance\nAfter constructing the dataset, we observed small but notable imbalances in answer distri-\nbutions after constructing our dataset.\nThis could lead to model bias toward any specific answer position (e.g., always selecting\n“A”). As such, to address this, we analyzed and adjusted the distribution of correct answer\nchoices across all subsets and splits by making minimal targeted adjustments (e.g., reorder-\ning options when possible) to bring the correct answer frequencies closer to uniformity. This\nrefinement was done independently for the training and test sets across each answer format\ngroup (ABCD, ABCDE, ABCDEF). This adjustment ensures a balanced representation of\ncorrect answers and helps reduce the likelihood that models learn position-based heuristics.\nFigures F1–F3 visualize the resulting distributions.\nFigure F1: Answer Choice Distribution for ABCD format (Train/Test)\nFigure F2: Answer Choice Distribution for ABCDE format (Train/Test)\nWe further validated this balance using chi-square goodness-of-fit tests.\nChi-square\ngoodness-of-fit tests confirmed no significant deviation from uniformity in ABCD and\nABCDE splits (p > 0.97), indicating that the observed answer distributions do not sig-\nnificantly deviate from a uniform distribution. The ABCDEF format was excluded from\n26\n"}, {"page": 27, "text": "Published as a conference paper at ICLR 2026\nFigure F3: Answer Choice Distribution for ABCDEF format (Train/Test)\nthis analysis in the test set due to insufficient sample size (n = 2). All tests support the\nconclusion that the distributions do not significantly differ from uniformity.\nThe resulting χ2 values and p-values were as follows:\n• ABCD (Train): χ2 = 0.099, p = 0.992\n• ABCD (Test): χ2 = 0.212, p = 0.976\n• ABCDE (Train): χ2 = 0.422, p = 0.981\n• ABCDE (Test): χ2 = 0.226, p = 0.994\n• ABCDEF (Train): χ2 = 0.714, p = 0.982\n• ABCDEF (Test): χ2 = 6.02, p = 0.304\nThis adjustment ensures a balanced representation of correct answers and helps reduce the\nlikelihood that models learn position-based heuristics.\n27\n"}, {"page": 28, "text": "Published as a conference paper at ICLR 2026\nG\nContemporary vs Legacy Model Assesment\nIn this section, we evaluate the evolution of contemporary models against their legacy coun-\nterparts by evaluating them on the MedArabiQ Abu Daoud et al. (2025) dataset and com-\nparing their accuracy scores. Additionally, this analysis allows us to compare model scores\non the MedArabiQ and MedAraBench benchmarks, providing valuable insights into the\nvalue of each benchmark.\nThe results are shown in Table G1 below. Our experimental results on the MedArabiQ\nbenchmark show that all contemporary models outperform legacy models on the MCQ task.\nAdditionally, our results show that gemini-2.0-flash, gpt-4.1, gpt-5, gpt-o3, and qwen-plus\nperform better on the MedArabiQ benchmark, while claude-sonnet-4-20250514 and llama-\n3.3-70b-instruct perform better on MedAraBench. This indicates that the MedAraBench\nbenchmark is more challenging overall, and thus provides larger value for the advancement of\nmedical Arabic NLP. It is important to note that legacy models gemini-1.5-pro and claude-\n3.5-sonnet were deprecated, which prevented us from evaluating them on MedAraBench to\nprovide a better comparison between benchmarks.\nTable G1: Legacy vs contemporary model performance on MedArabiQ and MedAraBench\nMedArabiQ\nMedAraBench\nLegacy Model\nAccuracy\nContemporary Model\nAccuracy\nContemporary Model\nAccuracy\nclaude-sonnet-3.5\n0.535\nclaude-sonnet-4-20250514\n0.6869\nclaude-sonnet-4-20250514\n0.6937\ngemini-1.5-pro\n0.575\ngemini-2.0-flash\n0.7273\ngemini-2.0-flash\n0.6539\ngpt-4\n0.535\ngpt-4.1\n0.8081\ngpt-4.1\n0.6733\ngpt-5\n0.8586\ngpt-5\n0.7642\ngpt-o3\n0.8384\ngpt-o3\n0.7652\nllama-3.1-8b\n0.262\nllama-3.3-70b-instruct\n0.4949\nllama-3.3-70b-instruct\n0.5466\nqwen-2.5-7b\n0.380\nqwen-plus\n0.6566\nqwen-plus\n0.6177\n28\n"}]}