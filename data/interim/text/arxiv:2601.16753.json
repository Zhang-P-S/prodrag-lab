{"doc_id": "arxiv:2601.16753", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.16753.pdf", "meta": {"doc_id": "arxiv:2601.16753", "source": "arxiv", "arxiv_id": "2601.16753", "title": "Standardizing Longitudinal Radiology Report Evaluation via Large Language Model Annotation", "authors": ["Xinyi Wang", "Grazziela Figueredo", "Ruizhe Li", "Xin Chen"], "published": "2026-01-23T13:57:09Z", "updated": "2026-01-23T13:57:09Z", "summary": "Longitudinal information in radiology reports refers to the sequential tracking of findings across multiple examinations over time, which is crucial for monitoring disease progression and guiding clinical decisions. Many recent automated radiology report generation methods are designed to capture longitudinal information; however, validating their performance is challenging. There is no proper tool to consistently label temporal changes in both ground-truth and model-generated texts for meaningful comparisons. Existing annotation methods are typically labor-intensive, relying on the use of manual lexicons and rules. Complex rules are closed-source, domain specific and hard to adapt, whereas overly simple ones tend to miss essential specialised information. Large language models (LLMs) offer a promising annotation alternative, as they are capable of capturing nuanced linguistic patterns and semantic similarities without extensive manual intervention. They also adapt well to new contexts. In this study, we therefore propose an LLM-based pipeline to automatically annotate longitudinal information in radiology reports. The pipeline first identifies sentences containing relevant information and then extracts the progression of diseases. We evaluate and compare five mainstream LLMs on these two tasks using 500 manually annotated reports. Considering both efficiency and performance, Qwen2.5-32B was subsequently selected and used to annotate another 95,169 reports from the public MIMIC-CXR dataset. Our Qwen2.5-32B-annotated dataset provided us with a standardized benchmark for evaluating report generation models. Using this new benchmark, we assessed seven state-of-the-art report generation models. Our LLM-based annotation method outperforms existing annotation solutions, achieving 11.3\\% and 5.3\\% higher F1-scores for longitudinal information detection and disease tracking, respectively.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.16753v1", "url_pdf": "https://arxiv.org/pdf/2601.16753.pdf", "meta_path": "data/raw/arxiv/meta/2601.16753.json", "sha256": "758d1ec650fd250855aeee580df160d008b7a0551d64b7fff27609dd863335c6", "status": "ok", "fetched_at": "2026-02-18T02:20:38.264435+00:00"}, "pages": [{"page": 1, "text": "Standardizing Longitudinal Radiology Report\nEvaluation via Large Language Model Annotation\nXinyi Wang1, Grazziela Figueredo2, Ruizhe Li1, Xin Chen1*\n1School of Computer Science, The University of Nottingham,\nNottingham, NG7 2RD, United Kingdom.\n2School of Medicine, The University of Nottingham, Nottingham, NG7\n2RD, United Kingdom.\n*Corresponding author(s). E-mail(s): Xin.Chen@nottingham.ac.uk;\nContributing authors: Xinyi.Wang4@nottingham.ac.uk;\nG.Figueredo@nottingham.ac.uk; Ruizhe.Li@nottingham.ac.uk;\nAbstract\nLongitudinal information in radiology reports refers to the sequential tracking of\nfindings across multiple imaging-based examinations over time, which is crucial\nfor monitoring disease progression and guiding clinical decisions. Many recent\nautomated radiology report generation methods are designed to capture longitu-\ndinal information; however, validating their performance is challenging. There is\nno proper tool to consistently label temporal changes in both ground-truth and\nmodel-generated texts for meaningful comparisons. Existing annotation meth-\nods are typically labor-intensive, relying on the use of manual lexicons and rules.\nComplex rules are closed-source, domain specific and hard to adapt, whereas\noverly simple ones tend to miss essential specialised information. Large language\nmodels (LLMs) offer a promising annotation alternative, as they are capable of\ncapturing nuanced linguistic patterns and semantic similarities without exten-\nsive manual intervention. They also adapt well to new contexts. In this study, we\ntherefore propose an LLM-based pipeline to automatically annotate longitudinal\ninformation in radiology reports. The pipeline first identifies sentences containing\nrelevant information and then extracts the progression of diseases. We evalu-\nate and compare five mainstream LLMs on these two tasks using 500 manually\nannotated reports. Considering both efficiency and performance, Qwen2.5-32B\nwas subsequently selected and used to annotate another 95,169 reports from the\npublic MIMIC-CXR dataset. Our Qwen2.5-32B-annotated dataset provided us\nwith a standardized benchmark for evaluating report generation models. Using\nthis new benchmark, we assessed seven state-of-the-art report generation models.\nOur LLM-based annotation method outperforms existing annotation solutions,\n1\narXiv:2601.16753v1  [cs.CL]  23 Jan 2026\n"}, {"page": 2, "text": "achieving 11.3% and 5.3% higher F1-scores for longitudinal information detec-\ntion and disease tracking, respectively. In conclusion, this work demonstrates the\npotential of LLMs for efficient and effective medical report annotation and estab-\nlishes a standardized framework to evaluate the capability of report generation\nmodels in producing longitudinal information.\nKeywords: Automated annotation, large language model, longitudinal information,\ndeep learning, radiology report generation and evaluation\n1 Introduction\nRadiological imaging is a key non-invasive tool used in routine clinical practice to assess\npatient condition and guide clinical decisions [1]. In clinical practice, serial images of a\nspecific anatomical region are often acquired at multiple time points to monitor known\nconditions or detect new findings. Information derived from comparing such images\nover time is termed longitudinal information, which is essential for tracking progression\nof diseases and injuries, for instance, and for evaluating treatment response [2–6].\nRadiologists often summarize longitudinal information in unstructured free-text\nreports. This leads to follow-up radiology reports containing two sentence types. Cross-\nsectional statements describe findings derived from a single time-point visit, while\nlongitudinal sentences integrate information across sequential studies. Figure 1a illus-\ntrates a follow-up report that includes a cross-sectional sentence on lung inflation (i.e.,\nsentence A) and a longitudinal sentence documenting the improvement in opacity\ncompared to the initial study (i.e., sentence B).\nGiven the substantial workload associated with manual radiology report writing,\nthe development of automatic report generation models has been widely researched\n[7]. Early report generation models primarily processed single time-point images,\noverlooking the sequential changes intrinsic to longitudinal image series. Recent auto-\nmated models capture this longitudinal information by leveraging multiple sequential\nimages or reports [5, 6, 8–12, 12–18]. There is however a gap in the current literature,\nwhich is the lack of a standardized framework for assessing longitudinal report gener-\nation [17]. This gap precludes systematic comparison of models and their constituent\nmethodological components.\nQuantitative and reproducible assessment requires automated annotation tools\nthat can reliably annotate changes over time in both ground-truth and model-\ngenerated reports for valid comparisons. Specifically, the coherency of longitudinal\ndescriptions can be evaluated by extracting and comparing longitudinal sentences in\nclinical and generated reports, while the capturing of disease changes can be assessed\nby extracting disease progression labels. Such assessments are very common for cross-\nsectional content. For instance, prevalent Clinical Efficacy metrics employ automated\nlabelers (e.g., CheXbert [19]) to detect the presence of 14 specific diseases in both\nreference and generated reports for comparison [7]. However, the assessment of lon-\ngitudinal information has received scant attention, largely because existing tools are\n2\n"}, {"page": 3, "text": "Example Sentences in the Report\n>       The lungs are well inflated. \n>       There has been significant interval improvement\nin the diffuse alveolar opacities previously seen. \nA\nB\nA\nB\nLongitudinal Information\na\nb\n1st\n2nd\n3rd\nPatient Visit Record\nExisting Annotation Methods\nOur LLM-based Annotation Methods\nHeavy reliance on manual review & expertise\nMissing critical information\nClosed-source methods\nLimited scalability and generalizability\nRigid, rule-based understanding of nuance\nLongitudinal or cross-\nsectional sentence\nDisease\nProgression\nCross-sectional\nLongitudinal\nOpacity\nImproved\n_\n_\nFully automated & intelligent processing\nComprehensive contextual understanding &\nflexible, deep semantic comprehension\nPerformance that surpasses  traditional methods\nOpen & accessible ecosystem\nExceptional scalability & generalizability\nFig. 1\nThe longitudinal information in the report and the advantages of LLM-based annotation.\na. Take sentences from the second-visit report of a patient to illustrate cross-sectional sentences\nand longitudinal information in free-text reports. b. Comparing existing longitudinal information\nannotation methods with large model-based annotation methods.\ninadequate for establishing a standardized evaluation framework. In addition, an effi-\ncient longitudinal annotation tool can help structure reports and provide annotated\ntraining data for deep learning models particularly in medical image analysis. Structur-\ning reports avoids problems like ambiguous expressions and inconsistent terminology\nin unstructured text, reducing diagnostic and potential therapeutic errors caused by\nmisinterpretation; the annotated data can be used for model pre-training or auxiliary\ntasks to facilitate model learning.\nExisting longitudinal annotation tools mainly include MS-CXR-T [20], ImaGenome\nsilver [3], MIMIC-Diff-VQA [21], and the Temporal Entity Matching (TEM) score\n[13]. Among them, MS-CXR-T, ImaGenome silver and MIMIC-Diff-VQA refer to the\nmethods used in annotating these three datasets. MS-CXR-T is manually annotated,\nmaking it infeasible to keep pace with the newly generated reports continuously pro-\nduced by report generation models. ImaGenome silver provides structured annotations\nfor objects, attributes, and disease status changes in reports. However, it only releases\nthe annotation results on MIMIC-CXR [22] and does not open-source the underlying\nmethodology. Consequently, the tool cannot be applied to annotate generated or other\nreports, which severely limits its usability. As for MIMIC-Diff-VQA [21], it presents\ninformation in unstructured question-answer pairs (e.g., Question: “What has changed\nin the right lung area?” Answer: “The level of pleural effusion has changed from small\nto moderate”), which lacks the structured labels necessary for automated evaluation.\n3\n"}, {"page": 4, "text": "The TEM score [13] evaluates generated reports by matching temporal expres-\nsions against ground-truth reports. It extracts these expressions using both temporal\nlexicon matching and a deep-learning-based entity recognition model. A key limita-\ntion of this approach is its focus on extracting temporal expressions (TE) rather than\nsummarizing disease progression (DP). While TE captures discrete descriptions of\nmedical states (e.g., “nodule increased”), DP requires the analytical synthesis of such\ninformation to characterize clinical change (e.g., “nodule worsened”). Consequently, it\npenalizes semantically equivalent expressions that differ lexically (e.g., “enlarged” vs.\n“increased in size”). Furthermore, temporal keyword matching is too rigid to capture\nall longitudinal descriptions. For example, the changing positions of support devices\nacross sequential imaging cannot be captured by the TEM lexicon.\nBeyond the limitations in evaluating generated reports, current longitudinal infor-\nmation annotation approaches also face scalability and generalizability issues because\nthey depend on either manual annotation [20] or lexicon-based rule-driven methods\n[3, 13, 21]. Both manual annotation and the development of lexicons/rules require\nextensive specialized human effort from medically trained annotators, making the pro-\ncess costly, time-consuming, and difficult to adapt to other diseases. For instance, the\nImaGenome silver constructed an initial vocabulary of 8,752 phrases for chest X-rays\n[3, 23], which need to be refined through a semi-automatic filtering process.\nRecent advances in large language models (LLMs) offer a scalable alternative for\nautomated radiology report annotation. Unlike previous methods, LLMs are capa-\nble of both summarizing disease progression and generalizing across diverse linguistic\nexpressions without predefined lexicons or task-specific rules. These capabilities make\nLLMs particularly suitable for performing longitudinal annotation of reports and, sub-\nsequently, facilitating the evaluation of generated reports based on these annotations.\nPrior studies have applied LLMs to transfer free-text reports into structured templates\nfor cross-sectional findings [24–26] or specific disease-diagnosis tasks [27–29], but their\npotential for extracting longitudinal clinical information remains unexplored.\nIn this work, we propose an LLM-based framework for extracting longitudinal\ninformation from radiology reports. The framework comprises two main tasks: (1)\nlongitudinal sentence identification, which determines whether a sentence contains\nlongitudinal information, and (2) disease progression tracking, which categorizes the\nchange in the medical condition described in longitudinal sentences as “improved”,\n“no change”, or “worsened”. Figure 1b illustrates the superiority of the LLM-based\napproach over existing methods, and Table 1 presents a detailed comparison between\nour annotation method and existing annotation methods.\nTable 1 Our longitudinal information annotation method compared with other annotation\nmethods. TDM: traditional deep-learning model; TE: temporal expressions; DP: disease\nprogression. Eval refers to whether this method can be used for generated report evaluation.\nAnnotation method\nLexicon\nManual review\nTDM\nLLM\nTE or DP\nEval\nTEM\n✓\n–\n✓\n–\nTE\n✓\nMIMIC-Diff-VQA\n✓\n–\n–\n–\nTE\n–\nMS-CXR-T\n–\n✓\n–\n–\nDP\n–\nImaGenome silver\n✓\n–\n–\n–\nDP\n–\nOurs\n–\n–\n–\n✓\nDP\n✓\n4\n"}, {"page": 5, "text": "To find the most suitable LLMs for this given task, we compare the perfor-\nmance of five mainstream LLMs on a subset of the Chest ImaGenome dataset [3].\nThe selected models vary in size and application scenario, including both general-\npurpose models and those fine-tuned for medical applications: MedGemma-27B [30],\nMedResearcher-R1-32B [31], Qwen2.5-32B [32], LLama3.3-70B [33], and Qwen2.5-72B\n[32]. The evaluation subset comprises 1,975 manually annotated sentences extracted\nfrom 500 reports. Subsequently, to establish an evaluation benchmark for report gen-\neration models, we annotate the follow-up reports in the MIMIC-CXR dataset [22, 34]\nusing Qwen2.5-32B based on its annotation effectiveness and efficiency. We termed the\nresulting annotated dataset L-MIMIC. Details of the model selection and new bench-\nmark dataset construction processes are provided in Appendix A.2 and Section 2.1.3,\nrespectively.\nBased on the LLM annotation pipeline, we developed an evaluation framework to\nassess whether report generation models can effectively capture longitudinal informa-\ntion. This evaluation examines performance along two key aspects: (1) the coherency\nof longitudinal descriptions, and (2) the capability to accurately capture disease\nprogression. Finally, we evaluated seven mainstream report generation models (i.e.,\nR2Gen[35], MedVersa[11], L R2Gen[16], MLRG[14], HC-LLM[17], Maira2[12], and\nLibra[5]) on the L-MIMIC test dataset using the evaluation framework.\nOur work makes three key contributions:\n• First LLM-based longitudinal annotation: We leverage LLMs to automati-\ncally identify longitudinal content and disease progression in radiology reports. We\ncompare five mainstream LLMs and show substantial improvements over traditional\nmethods through comprehensive validation.\n• A unified evaluation framework for longitudinal report generation: We\ncreated a longitudinal annotation for the MIMIC-CXR dataset and named this\nnewly annotated dataset L-MIMIC, which serves as a benchmark for evaluating how\nwell report generation models capture longitudinal information. Based on the LLM-\ndriven annotation framework, we also propose a set of metrics specifically designed\nfor this assessment.\n• Evaluation of automated report generation models: We assessed seven main-\nstream models, highlighting their strengths and limitations in capturing longitudinal\ninformation.\n2 Method\nThis section describes the proposed pipeline for using LLMs to automatically anno-\ntate radiology reports, including LLM model selection, prompt design, and evaluation\nmethodology. Based on a comparative evaluation, the best-performing LLM is selected\nto generate a large benchmark dataset, referred to as L-MIMIC. The second part of\nthe method outlines the evaluation pipeline and metrics used to assess the perfor-\nmance of seven state-of-the-art radiology report generation models on the L-MIMIC\ndataset. The code and L-MIMIC dataset will be released upon paper acceptance.\n5\n"}, {"page": 6, "text": "2.1 LLM-based longitudinal annotation\n2.1.1 Large language model selection\nWe selected five widely used LLMs, covering both medical-specialized and general-\npurpose\nfamilies:\nMedGemma-27B\n[30](link),\nMedResearcher-R1-32B\n[31](link),\nQwen2.5-32B\n[32](link),\nLlama3.3-70B\n[33](link),\nand\nQwen2.5-72B\n[32](link).\nMedGemma-27B and MedResearcher-R1-32B represent clinically adapted models\ntrained on substantial medical corpora, whereas Qwen2.5-32B, Llama3.3-70B, and\nQwen2.5-72B serve as state-of-the-art general LLM baselines. All models are released\nas open-source and freely available. All experiments used deterministic greedy\ndecoding and were run on NVIDIA A100 GPUs.\n2.1.2 Report annotation pipeline and LLM prompt design\nReports were first segmented into sentences using Stanza [36]. Each sentence was\nsubsequently classified by an LLM into either a cross-sectional sentence, describing\nthe patient’s static condition without temporal comparison, or a longitudinal sentence.\nFor sentences identified as longitudinal, the model extracted disease-related keywords\n(e.g., atelectasis) and assigned a progression label. The design of the progression label\nfollows previous work [3, 5, 13, 20] and annotates three categories, i.e., improved, no\nchange and worsened. An additional unmentioned label is introduced to account for\nirrelevant content potentially generated by generative models. The overall processing\nworkflow is summarized in Fig. 2a.\nFollowing previous works [29], prompts were designed by combining concise task\ndescriptions with illustrative examples. The prompts used for longitudinal classifi-\ncation, keyword extraction, and progression labeling are provided below. Bold text\nindicates placeholders that are replaced with task-specific content.\n“You are a medical AI assistant. Given a radiology report sentence sentence, determine\nif it compares the current image with prior studies (e.g., remain, compare, similar, stable,\nincreased, still, new, again). If yes, return <1>. If no, return <0>. Examples: “Cardiac\nand mediastinal silhouettes are stable.” returns <1>, “No larger pleural effusions.” returns\n<1>,“Increased retrocardiac opacity may reflect atelectasis.” returns <1>.”\n“Which item in the list most related to the following sentence sentence? The list is disease\nlist. Please reply with the answer enclosed in <>, for example <spinal fracture>. Return\n<support devices> for any changes in support device positions.\n“The following sentence sentence describes the status of disease as <no change>(e.g.,\nsimilar, unchanged), <improved>(e.g., resolved), or <worsened>. Return the answer in\nthe form of <no change>, <improved>, or <worsened>. If this condition is not mentioned,\nreturn <unmentioned>. Note that if ‘change’ or ‘new’ is mentioned along with a newly\nappearing lesion, it usually implies worsening. Example: in cases of ‘pleural effusion’, ‘New\nsmall bilateral pleural effusions.’ means <worsened>. In cases of ‘low lung volumes’, ‘lower’\nmeans <worsened>; ‘increased’ means <improved>.\n6\n"}, {"page": 7, "text": "> There is no pneumothorax.\n> Bilateral pleural effusions are\nsmall.\n> Bibasilar atelectasis have\nimproved, larger on the right. \nEach Sentence in Report\nLLM\n> Bilateral pleural effusions are small.\n> Bibasilar atelectasis have improved,\nlarger on the right. \nGround\ntruth \nreport\nGenerated\nreport\nLongitudinal\nannotation prompt\n> Bibasilar\natelectasis have\nimproved, larger\non the right. \na\nb\nLongitudinal\nsentences\nBLEU/\nMETEOR/\nROUGE-L/\nKeyword\nannotation\nDisease\nprogression \nannotation\nDisease progression \nannotation\nAccuracy/\nPrecision/\nRecall/\nF1 score\n> There is no pneumothorax.\n<Improved>\n<No change>\n<Worsened>\n<Unmentioned>\nLongitudinal\nsentences\n<Keyword>\n<Status>\n<Status>\n<cross-sectional>\n<longitudinal>\nLongitudinal\nannotation\nLongitudinal\nannotation\n<cross-sectional>\nKeyword \nannotation prompt\nLLM\nLLM\n<Atelectasis>\nDisease\nprogression\nannotation prompt\nFig. 2\nAnnotation and evaluation pipeline. a. Longitudinal, keyword, and disease progression anno-\ntations. Longitudinal annotation identifies sentences containing longitudinal information; keyword\nannotation captures the main content; disease progression annotation captures changes in status (i.e.,\nimproved, no change, worsened, or unmentioned) related to the main content. b. An evaluation frame-\nwork for the report generation model based on the proposed annotation method.\nThe above prompts apply to all models, except that MedGemma requires “Please\noutput <0> or <1>.” at the end of the prompt when performing longitudinal anno-\ntation to enforce binary outputs. The disease list design is provided in Appendix\nA.1.\n2.1.3 Dataset and evaluation metrics\nThe test set for assessing LLMs annotation performance was derived from ImaGenome.\nLongitudinal annotations in ImaGenome were provided in two forms: manually curated\n(i.e., gold) and automatically generated (i.e., silver). The gold subset included only\nlongitudinal sentences from second-visit reports of 500 patients, whereas the silver\nannotations covered the majority of sentences across all reports. We constructed a\ncombined dataset of 1,975 sentences sourced from 500 reports. Specifically, this dataset\ncomprises all sentences from the gold subset, augmented with non-longitudinal sen-\ntences sampled from the same reports in the silver annotations. Model performance\nfor both longitudinal annotation and disease-progression annotation was quantified\nusing standard evaluation metrics, including accuracy, precision, recall, and the F1\nscore. The evaluation results are presented in Figure 3, and Qwen2.5-32B was selected\nto construct a benchmark dataset (i.e. L-MIMIC) for report generation method\nevaluation.\n7\n"}, {"page": 8, "text": "2.2 Evaluation of report generation models\n2.2.1 L-MIMIC Dataset\nThe L-MIMIC dataset was constructed by annotating radiology reports from the\nMIMIC-CXR database using Qwen2.5-32B, which was chosen for its favorable balance\nbetween performance and computational efficiency (Appendix A.2). Following Zhu et\nal. [16], we selected reports containing a “Findings” section from patients’ second or\nlater visits, and retaining all images for each patient across scan positions. We also\nseparated report sections into “Indication”, “Comparison”, “Findings”, “Impression”,\n“Technique”, “History”, and “Examination”, leveraging the official MIMIC-CXR code.\nEach sentence in the “Findings” section was then annotated with Qwen2.5-32B to\nidentify longitudinal information, relevant clinical keywords, and indications of disease\nprogression. Overall, the L-MIMIC dataset consists of 73,093 training, 588 validation,\nand 1,863 test reports, which can be used as a benchmark for method comparison.\n2.2.2 Radiology report generation models\nWe assessed seven radiology report generation models, including two baselines:\nR2Gen, representing conventional deep-learning approaches, and MedVersa, represent-\ning large-scale models (≥7B parameters). Neither baseline incorporates prior clinical\nhistory. The other five models—L R2Gen, MLRG, HC-LLM, Maira2 and Libra—are\ndesigned to exploit patients’ historical information to generate longitudinally informed\nreports. Among them, L R2Gen and MLRG do not incorporate LLMs, whereas\nHC-LLM includes a frozen LLM; all remaining models are LLM-based. This set com-\nprehensively spans models of varying scales, architectures, and capacities to leverage\nlongitudinal clinical data. Full model configurations are provided in Appendix B.1.\n2.2.3 Evaluation pipeline and metrics\nFigure 2b summarizes the evaluation framework for assessing whether the report-\ngeneration models faithfully capture longitudinal disease progression. Using the\ngenerated L-MIMIC dataset as an example, ground-truth reports were first anno-\ntated with longitudinal sentences, disease-specific keywords, and progression labels.\nGenerated reports were then processed to extract longitudinal sentences, which were\nmatched to their ground-truth counterparts. Linguistic similarity was quantified using\nstandard natural-language metrics [7]—BLEU-1–4, ROUGE-L, and METEOR—to\nevaluate the fluency and lexical correspondence of the generated longitudinal descrip-\ntions. For each disease instance with a reference progression label, progression status\nwas inferred from the generated longitudinal sentences. Notably, progression labels\nexcluded support devices. Given that changes in support devices are predominantly\npositional, their progression trends (improved, worsened or no change) cannot be reli-\nably classified, and no corresponding annotations were provided by ImaGenome. Model\nperformance in predicting progression was then evaluated using accuracy, precision,\nrecall, and F1-score against the ground-truth annotations.\n8\n"}, {"page": 9, "text": "3 Results\n3.1 LLM-based annotation performance\nWe compared the annotation performance of the five LLMs (i.e. MedGemma-27B [30],\nMedResearcher-R1-32B [31], Qwen2.5-32B [32], LLama3.3-70B [33], and Qwen2.5-72B\n[32]) with ImaGenome silver [3] (traditional rule-based method used as baseline),\nas shown in Figure 3. On the task of identifying sentences containing longitudinal\ninformation, all LLMs substantially outperformed ImaGenome silver. LLaMA3.3-70B\nachieved the highest F1 score of 94.0%, representing an 11.3% improvement over\nImaGenome silver (82.7%). MedGemma-27B and Qwen2.5-32B also showed signifi-\ncant gains, with F1 scores of 93.2% (+10.5%) and 91.8% (+9.1%), respectively, while\nMedResearcher-R1-32B and Qwen2.5-72B scored 90.5% (+7.8%) and 89.5% (+6.8%)\nrespectively.\nThe gains in F1 were primarily driven by increased recall rather than precision.\nThe rule-based method exhibited very high precision (98.5%) but extremely low recall\n(71.2%), resulting in many missed positive cases. In contrast, LLaMA3.3-70B achieved\na recall of 96.8% (+25.6%) while maintaining high precision (91.4%). Similar recall\nimprovements were observed across other LLMs, including MedGemma-27B (93.7%,\n+22.5%) and Qwen2.5-32B (88.8%, +17.6%), showing that LLMs improve sensitivity\nwithout sacrificing precision to a degree that harms overall performance. Consistent\nwith this trend, LLM-based models also showed higher accuracy than the rule-based\nmethod (90.2%), with LLaMA3.3-70B reaching 95.9% (+5.7%) and MedGemma-27B\n95.5% (+5.3%).\n89.5\n85.1\n94.4\n93.4\n94\n96.8\n91.4\n95.9\n91.8\n88.8\n95.1\n94.8\n90.5\n86\n95.4\n94\n93.2\n93.7\n92.7\n95.5\n82.7\n71.2\n98.5\n90.2\n95.8\n98.4\n95.8\n91.2\n94.9\n97.3\n93.7\n90.5\n95.4\n98.7\n95.3\n91.9\n95.1\n97.7\n94.4\n90.1\n96.3\n97.7\n95.6\n91.4\n95.7\n97.8\n94.5\n88.7\n76.2\n83.7\n87.7\n83.8\n77.4\n79.4\n79.8\n78.9\n72.2\n87.1\n87\n83.2\n70.4\n74.9\n84.6\n79.5\n79.6\n76\n87.9\n83.7\n72.5\n76.6\n82.1\n78.5\nF1\nRecall\nPrecision\nAccuracy\n70\n80\n90\n100\nLongitudinal annotation\n ImaGenome silver \n MedGemma-27B \n MedResearcher-R1-32B \n Qwen2.5-32B \n LLama3.3-70B \n Qwen2.5-72B\nDisease progression annotation\nWorsened\nImproved\nNo change\nAverage\n90\n100\nAccuracy\nWorsened\nImproved\nNo change\nAverage\n70\n80\n90\nF1 score\nFig. 3\nPerformance of large language models on longitudinal annotation and disease progression\nannotation. For disease progression annotation, the average refers to the micro-average across three\nclasses: no change, improved, and worsened.\n9\n"}, {"page": 10, "text": "In disease progression prediction, LLMs consistently outperformed rule-based\nImaGenome silver. Overall, micro-average accuracy ranged from 90.1–91.9%, rep-\nresenting improvements of 1.4–3.2% over ImaGenome Silver (88.7%). Qwen2.5-\n32B achieved the highest accuracy (91.9%). Micro-average F1 scores ranged from\n78.9–83.8%, corresponding to gains of 0.4–5.3% over ImaGenome Silver (78.5%).\nQwen2.5-72B (83.8%) showed the strongest performance.\nDifferent models excelled in different categories. In the “no change” category,\nMedGemma-27B achieved an F1 score of 87.9%, outperforming ImaGenome by 5.8%\n(82.1%), while Qwen2.5-32B reached 87%, an improvement of 4.9%. In the “improved”\ncategory, Qwen2.5-32B performed the best, achieving 87.1% and leading ImaGenome\nby 10.5%. In the “worsened” category, Qwen2.5-32B, which performed well in the first\ntwo categories, is comparable to ImaGenome, whereas MedGemma-27B outperformed\nImaGenome by 7.1%.\n3.2 L-MIMIC for Report Generation Evaluation\nThe follow-up reports in the MIMIC-CXR dataset comprise 92,374 reports for training,\n737 for validation, and 2,058 for testing, following its official split. Figure 4 shows its\nannotation statistics. While most follow-up reports include disease progression infor-\nmation. About 20% of training/validation and 10% of test reports lack such details\n(Figure 4a). Among disease progression, 59%, 10%, and 11% indicate “no change”,\n“improved”, and “worsened”, respectively. Furthermore, variations in the support\ndevice, such as its position, accounted for 18% (Figure 4b). We extracted cases whose\nreports were labeled as containing longitudinal information, forming the L-MIMIC\ndataset with 73,093 training, 588 validation, and 1,863 test cases. The training and\nvalidation sets include longitudinal annotations to support the development of medical\nimage analysis models, and are suitable for tasks such as pre-training, hyperparame-\nter setting, and fine-tuning. The test set is designed for evaluating model performance\nparticularly in report generation and disease progression tracking. Detailed application\nscenarios for longitudinal annotation are discussed in Section 4.4.\nTrain dataset\n#Reports: 92,374 \n#Sentences (L_report): 409,194\nValidation dataset\n#Reports: 737 \n#Sentences (L_report): 3226\nTest dataset\n#Reports: 2058 \n#Sentences (L_report): 11004\nL_report\nC_report\nL_sentence\nC_sentence\n79.1%\n79.8%\n90.5%\n20.9%\n20.2%\n9.5%\n58.8%\n58.1%\n57%\n43%\n41.9%\n41.2%\na\nNo change\nWorsened\nImproved\nUnmentioned\n59%\n18%\n10%\n2%\n11%\nSupport\ndevices\nb\nFig. 4 Annotation statistics for follow-up reports in MIMIC-CXR. a. Proportion of sentences and\nreports with longitudinal information. L report/L sentence: reports/sentences with longitudinal infor-\nmation. C report/C sentence: reports/sentences that convey only cross-sectional information and\nexclude any longitudinal information. b. Proportion of disease progression categories in L-MIMIC.\n10\n"}, {"page": 11, "text": "We evaluated seven report-generation systems (i.e., R2Gen[35], MedVersa[11],\nL R2Gen[16], MLRG[14], HC-LLM[17], Maira2[12], and Libra[5]) on the L-MIMIC test\nset to assess their ability to capture longitudinal information, therefore demonstrating\nthe advantages of using L-MIMIC for method evaluation. Table 2 summarizes the key\nmetrics: BLEU-4 [37] and METEOR [38] for language quality, micro-average F1 for\ndisease diagnosis, and both micro-average and class-level F1 for disease-progression\ncategories. The table also indicates whether models use prior imaging (P I) or tex-\ntual inputs (P T). Full model configurations and all language and clinical metrics are\nprovided in Appendix B.\nTable 2 Performance of report generation models on the L-MIMIC test set. P I, prior image\ninput; P T, prior text input; A F1, micro-average F1 score; N F1, I F1, W F1, F1 scores for\nno-change, improved, and worsened classes, respectively. Higher values are better for all indicators.\nModels\nInputs\nLanguage-based metrics\nMedical correctness-based metrics (%)\nP I\nP T\nWhole report\nLongitudinal\nsentences\nDiagnosis\nProgression\nBLEU-\n4\nMET\nEOR\nBLEU-\n4\nMET\nEOR\nF1\nA F1 N F1 I F1\nW F1\nR2Gen\n✗\n✗\n9.3\n13.3\n2.4\n5.9\n32.5\n33.6\n44.6\n0.2\n2.7\nMedVersa\n✗\n✗\n15.0\n16.5\n2.9\n7.3\n52.8\n33.9\n41.4\n9.4\n14.0\nL R2Gen\n✓\n✓\n9.0\n12.9\n3.4\n6.5\n42.1\n38.8\n51.5\n2.3\n2.1\nMLRG\n✓\n✓\n15.0\n16.8\n6.9\n9.7\n52.8\n35.7\n44.4\n10.5\n12.5\nHC-LLM\n✓\n✓\n11.6\n15.6\n5.4\n9.0\n43.4\n38.0\n48.6\n11.2\n5.3\nMaira2\n✓\n✓\n18.9\n19.5\n9.6\n12.5\n58.2\n41.4\n49.7\n17.7\n18.3\nLibra\n✓\n✗\n23.6\n22.6\n12.7\n13.9\n57.6\n41.0\n50.5\n13.9\n14.9\nAll report-generation models exhibited considerable scope for improvement in cap-\nturing longitudinal information. Libra outperformed counterparts on language-based\nmetrics, achieving BLEU-4 and METEOR scores of 23.6 and 22.6 for full reports\nversus only 12.7 and 13.9 for longitudinal sentences. Maira2, meanwhile, topped all\nmodels in diagnostic and overall disease progression prediction, with a diagnostic F1\nof 58.2% versus a substantially lower 41.4% micro-average F1 for progression. Other\nmodels performed even poorer on longitudinal sentence language metrics and pro-\ngression micro-average F1; most yielded a progression F1<40%, METEOR<10, and\nBLEU-4<7.\nModels with longitudinal priors achieved higher performance on longitudinal infor-\nmation generation than models without priors. R2Gen and MedVersa, which lack input\nof prior information (text report and/or images), obtained BLEU-4 scores of 2.4 and\n2.9, METEOR scores of 5.9 and 7.3, and progression F1 scores of 33.6% and 33.9%.\nThe other methods with priors reached BLEU-4 scores of 3.4–12.7, METEOR scores of\n6.5–13.9, and progression F1 scores of 35.7–41.4%. The longitudinally enhanced vari-\nant L R2Gen achieved higher scores on longitudinal sentences (BLEU-4: 3.4 vs. 2.4;\nMETEOR: 6.5 vs. 5.9) and progression F1 (38.8% vs. 33.6%) compared with R2Gen,\nwhile its overall report performance was lower (BLEU-4: 9.0 vs. 9.3; METEOR: 12.9\nvs. 13.3).\nAcross the three progression categories, all models show substantial imbalance,\nwith the no-change class consistently outperforming the improved and worsened\n11\n"}, {"page": 12, "text": "classes. R2Gen and L R2Gen exhibit the largest performance difference between\nthe no-change (44.6% and 51.5%) class and the improved/worsened classes (<3%).\nOther models mitigate this imbalance, yet gaps still remain. For example, Maira2\nachieves 17.7% and 18.3% on the improved and worsened classes, respectively, while\nmaintaining a score of 49.7% on the no-change class.\n4 Discussion\nIn this study, we benchmark LLMs’ ability to annotate longitudinal information in\nradiology reports, and further scale this annotation pipeline to annotate a large public\ndataset. Building on this validated LLM-based annotation methodology, we develop a\nsystematic evaluation framework tailored to assessing the capacity of radiology report\ngeneration models to capture longitudinal information, and apply this framework to\nevaluate seven mainstream report generation models, revealing their limitations. In\nthe following discussion, we elaborate the advantages of LLM-based annotation, the\ninfluence of LLM scale and training dataset, potential application scenarios of auto-\nmated longitudinal information extraction, and the performance of report generation\nmodels. Finally, we discuss the limitations and future work of this study.\n4.1 Advantages of LLM-based longitudinal information\nannotation\nWe evaluated five mainstream LLMs on their ability to identify longitudinal descrip-\ntions and track disease progression. In the task of identifying longitudinal descrip-\ntions, all LLMs significantly outperformed the conventional rule-based method (i.e.,\nImaGenome silver), with MedGemma-27B, LLaMA3.3-70B, and Qwen2.5-32B achiev-\ning the highest performance. For disease progression, MedGemma-27B, Qwen2.5-72B,\nand Qwen2.5-32B demonstrated a clear advantage over traditional approaches,\nwhereas other models achieved only modest improvements. Considering both anno-\ntation accuracy and efficiency, Qwen2.5-32B was selected as a practical tool for\nlarge-scale annotation of the MIMIC-CXR dataset, facilitating the construction of\na longitudinally labeled resource for downstream studies of longitudinal description\nidentification and disease progression.\nLarge models offer advantages for medical annotation beyond performance met-\nrics. Conventional annotation methods typically rely on extensive manually curated\nvocabularies, which require labor-intensive human verification. Meanwhile, these\nvocabularies are often disease-specific, which limits their general applicability. While\nLLMs enable automatic and scalable annotation by interpreting context-rich clinical\nnarratives without reliance on predefined lexicons, thereby reducing manual effort and\nenhancing adaptability across diverse clinical domains.\nSome existing approaches, such as TEM and MIMIC-Diff-VQA, extract temporal\ndescriptive terms directly from radiology reports, but do not summarize information\non disease progression. This limitation restricts their utility across diverse application\nscenarios. In contrast, LLMs, with their strong language comprehension and flexibility\nin understanding clinical narratives, offer a promising avenue for summarizing disease\nprogression from radiology reports.\n12\n"}, {"page": 13, "text": "In conclusion, our experiments show that LLMs significantly outperform traditional\nrule-based methods in identifying longitudinal descriptions and disease progression in\nradiology reports. Unlike conventional approaches that require laborious construction\nand maintenance of large domain-specific lexicons, LLMs reduce human resource costs\nand enhance task scalability, making them a promising and robust tool for this task.\n4.2 Influence of LLMs scale and training dataset\nLarger LLMs do not necessarily outperform smaller ones in terms of both longitudinal\nannotation and disease progression. Within the Qwen2.5 series, Qwen2.5-32B and\nQwen2.5-72B achieved comparable performance, while across series, LLaMA3.3-70B\nslightly underperformed compared with Qwen2.5-32B and MedGemma-27B. These\nfindings show that 32B-scale LLMs are sufficient to meet the demands of specific\nclinical scenarios, offering strong performance alongside greater ease of deployment\nand lower GPU requirements.\nConsistent with the observations of Kim et al. [27] in disease diagnosis, medically\nfine-tuned language models did not outperform their base models in our annotation\ntasks. MedResearcher-R1-32B, a medically fine-tuned variant of Qwen2.5-32B, showed\nlower performance than its base model, suggesting that domain-specific fine-tuning\ndoes not always enhance task-specific capabilities. This may reflect limitations in the\ndesign of fine-tuning tasks or the coverage of domain-specific data.\n4.3 Evaluation of medical report generation models\nWe evaluate the progress and limitations of current report generation models in\nlongitudinal information generation by validating seven mainstream models on the L-\nMIMIC dataset, based on our evaluation framework (Table 2). Overall, recent models\nhave incorporated prior image and text data via diverse strategies, achieving notable\nprogress in longitudinal information capture. However, considerable room for enhance-\nment remains, specifically in the coverage and fluency of longitudinal sentences, and\nthe ability to identify disease improvement and deterioration.\nReport generation models typically exhibit three types of errors when handling lon-\ngitudinal information: missing comparison, redundant comparison, and wrong trend\nprediction. A missing comparison occurs when documented lesion changes are omitted;\na redundant comparison introduces unreported alterations; and a wrong trend pre-\ndiction misrepresents the direction of disease progression. Our evaluation framework\ncan detect all these errors. Specifically, progression metrics provide an intuitive reflec-\ntion of wrong trend errors and missing comparison errors. Language-based metrics\nfor longitudinal sentences can distinguish linguistic descriptive discrepancies, thereby\ncapturing redundant comparisons and missing comparison errors, as well as partially\nindicating wrong trend errors.\nTrend errors stem from the limitations of the model’s capabilities, whereas redun-\ndant and missing comparisons arise when the model fails to recognize the target\ndisease. Even radiologists interpreting identical imaging data may produce reports\nwith inconsistent documentation priorities without explicit guidance—this underscores\n13\n"}, {"page": 14, "text": "the necessity of integrating keyword guidance into report generation models to delin-\neate required information for extraction. While keyword-driven information retrieval\nhas been embedded in general report generation frameworks [39, 40], analogous\nresearch focusing on longitudinal information capture remains sparse.\nReport generation models show a significant performance gap in longitudinal\nmetrics compared with language-based and diagnostic metrics for full reports. This\nsuggests that the models can capture clinical findings; however, they struggle to cor-\nrelate these with patients’ historical records. Previous report generation models (e.g.,\nR2Gen and MedVersa) typically take single-time-point images as input, which do not\ncontain longitudinal information. Longitudinal descriptions generated by these models\nare hallucinations that lack clinical value.\nRecent models for medical report generation incorporate prior images and reports\nas inputs, achieving improved performance on longitudinal metrics compared to base-\nline approaches. However, existing fusion strategies remain rudimentary. Most methods\nrely on cross-attention mechanisms to retrieve relevant information in prior inputs\n[5, 14, 16]. Yet, none of these methods introduce constraints when associating cur-\nrent observations with historical patient data. Liu et al. [17] attempt to compare the\ncurrent image with prior images, as well as the current report with prior reports, by\nimposing consistency constraints, but they keep the LLM frozen and only train a linear\nprojection layer, potentially limiting expressive power.\nIn conclusion, our evaluation of the longitudinal information generation capabil-\nity refines the assessment framework for report generation models and identifies the\nlimitations of current models. Incorporating additional constraints, such as keywords,\nto better integrate prior images and reports into the training process represents a\npromising direction for future report generation research.\n4.4 Other application scenarios of automated longitudinal\ninformation annotation in radiology reports\nBesides the evaluation scenario, automated longitudinal information extraction has\ntwo primary application scenarios that support both clinical practice and research.\nFirst, it enables structured characterization of disease trajectories from free-text radi-\nology reports by converting narrative descriptions into progression categories (e.g.,\nimproved, no change, worsened). Structuring longitudinal and disease-tracking infor-\nmation can support quantification of population-level trends, identification of clinically\nmeaningful temporal changes, and retrieval of cases for large-scale analysis.\nSecond, automatic longitudinal annotation can enhance the training of automated\nreport generation models by providing additional supervisory signals that indicate\nwhether prior examinations should be referenced, more accurately reflecting real-world\nclinical workflows. Current models treat longitudinal and cross-sectional sentences\nidentically, using the same input. In reality, longitudinal sentences require reference to\nprevious images and reports, whereas cross-sectional sentences can be generated from\nsingle time-point images. Zhu et al. [16] selected longitudinal reports from patients\nwith more than two visits, assuming these contain longitudinal information. However,\nour annotations (Figure 4) reveal that 10–20% of such reports lack longitudinal content\nand around 58% of sentences in longitudinal reports are cross-sectional sentences.\n14\n"}, {"page": 15, "text": "Therefore, explicit differentiation of cross-sectional from true longitudinal sentences is\ncritical to enabling models to prioritize high-value input data.\nAdditionally, automatic longitudinal annotation supplies high-quality training data\nfor model pre-training, which is crucial for deep learning, especially for large-scale\nmodels. Pre-training transforms model parameters from random initialization into a\nsemantically informed starting point, laying a robust foundation for effective fine-\ntuning. For example, Zhang et al. [5] utilized VQA annotations from MIMIC-Diff-VQA\n[21] to pre-train their report generation model.\n4.5 Limitations and future works\nWhen tracking longitudinal disease progression, LLMs performed suboptimally in\nidentifying “worsened” cases. A key limitation was their tendency to misinterpret\nsentences describing worsened status as cross-sectional status, due to difficulty dis-\ncerning whether terms like “increased” referred to the current scan alone or implied\na comparison with prior imaging. For instance, Qwen2.5-32B failed to identify 46 out\nof 190 longitudinal sentences describing “worsened” cases, compared to missing only\n9 sentences describing “improved” cases. Additionally, LLMs often overlooked newly\nemerged diseases as “worsened” progression indicators. Future improvements could\ninvolve more nuanced prompt engineering, such as integrating prior reports to provide\nessential clinical context.\nThe validation in this study is limited to chest X-ray reports. Research on longitudi-\nnal progression in radiology remains nascent, and datasets with expert annotations for\nother imaging modalities and anatomical regions are scarce. Therefore, constructing\nthe necessary datasets and further evaluating the reliability of LLM-based annotations\nacross a broader range of clinical scenarios are critical next steps.\nThe reliability of LLMs is constrained by their inherent tendencies toward hallu-\ncination and bias, particularly in domains like rare diseases where training data is\nlimited. Recent advances, such as enabling LLMs to access the internet, have helped\nmitigate hallucinations and outdated knowledge. Thus, a promising research direc-\ntion lies in developing LLM-based annotation methods that allow online querying or\nreal-time knowledge updates without compromising patient privacy or significantly\nincreasing inference time. Finally, it is worth noting that our annotation and evalu-\nation framework is sufficiently flexible and general to accommodate newly developed\nLLMs.\nReferences\n[1] Liao, Y., Liu, H., Spasi´c, I.: Deep learning approaches to automatic radiology\nreport generation: A systematic review. Informatics in Medicine Unlocked 39,\n101273 (2023)\n[2] Kalil, A.C., Metersky, M.L., Klompas, M., Muscedere, J., Sweeney, D.A., Palmer,\nL.B., Napolitano, L.M., O’Grady, N.P., Bartlett, J.G., Carratal`a, J., et al.: Man-\nagement of adults with hospital-acquired and ventilator-associated pneumonia:\n15\n"}, {"page": 16, "text": "2016 clinical practice guidelines by the infectious diseases society of america and\nthe american thoracic society. Clinical infectious diseases 63(5), 61–111 (2016)\n[3] Wu, J., Agu, N., Lourentzou, I., Sharma, A., Paguio, J., Yao, J.S., Dee, E.C.,\nMitchell, W., Kashyap, S., Giovannini, A., et al.: Chest imagenome dataset.\nPhysio Net (2021)\n[4] Li, M.D., Chang, K., Bearce, B., Chang, C.Y., Huang, A.J., Campbell, J.P.,\nBrown, J.M., Singh, P., Hoebel, K.V., Erdo˘gmu¸s, D., et al.: Siamese neural net-\nworks for continuous disease severity evaluation and change detection in medical\nimaging. NPJ digital medicine 3(1), 48 (2020)\n[5] Zhang, X., Meng, Z., Lever, J., Ho, E.S.L.: Libra: Leveraging temporal images for\nbiomedical radiology analysis. In: Findings of the Association for Computational\nLinguistics: ACL 2025, pp. 17275–17303. Association for Computational Linguis-\ntics, Vienna, Austria (2025). https://aclanthology.org/2025.findings-acl.888/\n[6] Hou, W., Cheng, Y., Xu, K., Li, W., Liu, J.: Recap: Towards precise radiology\nreport generation via dynamic disease progression reasoning. In: 2023 Findings\nof the Association for Computational Linguistics: EMNLP 2023, pp. 2134–2147\n(2023). Association for Computational Linguistics (ACL)\n[7] Wang, X., Figueredo, G., Li, R., Zhang, W.E., Chen, W., Chen, X.: A sur-\nvey of deep-learning-based radiology report generation using multimodal inputs.\nMedical Image Analysis, 103627 (2025)\n[8] Nicolson, A., Dowling, J., Anderson, D., Koopman, B.: Longitudinal data and\na semantic similarity reward for chest x-ray report generation. Informatics in\nMedicine Unlocked 50, 101585 (2024)\n[9] Wang, F., Du, S., Yu, L.: Hergen: Elevating radiology report generation with\nlongitudinal data. In: European Conference on Computer Vision, pp. 183–200\n(2024). Springer\n[10] Dalla Serra, F., Wang, C., Deligianni, F., Dalton, J., O’Neil, A.Q.: Controllable\nchest x-ray report generation from longitudinal representations. In: The 2023\nConference on Empirical Methods in Natural Language Processing (2023)\n[11] Zhou, H.-Y., Acosta, J.N., Adithan, S., Datta, S., Topol, E.J., Rajpurkar, P.:\nMedversa: A generalist foundation model for medical image interpretation. arXiv\npreprint arXiv:2405.07988 (2024)\n[12] Bannur, S., Bouzid, K., Castro, D.C., Schwaighofer, A., Thieme, A., Bond-Taylor,\nS., Ilse, M., P´erez-Garc´ıa, F., Salvatelli, V., Sharma, H., et al.: Maira-2: Grounded\nradiology report generation. arXiv preprint arXiv:2406.04449 (2024)\n[13] Bannur, S., Hyland, S., Liu, Q., Perez-Garcia, F., Ilse, M., Castro, D.C., Boecking,\n16\n"}, {"page": 17, "text": "B., Sharma, H., Bouzid, K., Thieme, A., et al.: Learning to exploit temporal struc-\nture for biomedical vision-language processing. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 15016–15027\n(2023)\n[14] Liu, K., Ma, Z., Kang, X., Li, Y., Xie, K., Jiao, Z., Miao, Q.: Enhanced contrastive\nlearning with multi-view longitudinal data for chest x-ray report generation. In:\nProceedings of the Computer Vision and Pattern Recognition Conference, pp.\n10348–10359 (2025)\n[15] Wang, Z., Sun, Y., Li, Z., Yang, X., Chen, F., Liao, H.: Llm-rg4: Flexible and\nfactual radiology report generation across diverse input contexts. In: Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 39, pp. 8250–8258 (2025)\n[16] Zhu, Q., Mathai, T.S., Mukherjee, P., Peng, Y., Summers, R.M., Lu, Z.: Utilizing\nlongitudinal chest x-rays and reports to pre-fill radiology reports. In: International\nConference on Medical Image Computing and Computer-Assisted Intervention,\npp. 189–198 (2023). Springer\n[17] Liu, T., Wang, J., Hu, Y., Li, M., Yi, J., Chang, X., Gao, J., Yin, B.: Hc-llm:\nHistorical-constrained large language models for radiology report generation. In:\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 39, pp. 5595–\n5603 (2025)\n[18] Yang, Y., You, X., Zhang, K., Fu, Z., Wang, X., Ding, J., Sun, J., Yu, Z., Huang,\nQ., Han, W., et al.: Spatio-temporal and retrieval-augmented modelling for chest\nx-ray report generation. IEEE Transactions on Medical Imaging (2025)\n[19] Smit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A.Y., Lungren, M.P.: Chexbert:\nCombining automatic labelers and expert annotations for accurate radiology\nreport labeling using bert. In: EMNLP 2020-2020 Conference on Empirical\nMethods in Natural Language Processing, Proceedings of the Conference, pp.\n1500–1519 (2020)\n[20] Bannur, S., Hyland, S., Liu, Q., P´erez-Garc´ıa, F., Ilse, M., Castro, D.C., Boecking,\nB., Sharma, H., Bouzid, K., Schwaighofer, A., et al.: MSCXR-T: Learning to\nexploit temporal structure for biomedical vision-language processing. PhysioNet\n(2023)\n[21] Hu, X., Gu, L., An, Q., Zhang, M., Liu, L., Kobayashi, K., Harada, T., Summers,\nR.M., Zhu, Y.: Expert knowledge-aware image difference graph representation\nlearning for difference-aware medical visual question answering. In: Proceedings\nof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\npp. 4156–4165 (2023)\n[22] Johnson, A.E., Pollard, T.J., Berkowitz, S.J., Greenbaum, N.R., Lungren, M.P.,\nDeng, C.-y., Mark, R.G., Horng, S.: Mimic-cxr, a de-identified publicly available\n17\n"}, {"page": 18, "text": "database of chest radiographs with free-text reports. Scientific data 6(1), 317\n(2019)\n[23] Wu, J.T., Syed, A., Ahmad, H., Pillai, A., Gur, Y., Jadhav, A., Gruhl, D., Kato,\nL., Moradi, M., Syeda-Mahmood, T.: Ai accelerated human-in-the-loop structur-\ning of radiology reports. In: AMIA Annual Symposium Proceedings, vol. 2020, p.\n1305 (2021)\n[24] Wo´znicki, P., Laqua, C., Fiku, I., Hekalo, A., Truhn, D., Engelhardt, S., Kather,\nJ., Foersch, S., D’Antonoli, T.A., Santos, D., et al.: Automatic structuring of\nradiology reports with on-premise open-source large language models. European\nRadiology 35(4), 2018–2029 (2025)\n[25] Hein, D., Christie, A., Holcomb, M., Xie, B., Jain, A., Vento, J., Rakheja, N.,\nShakur, A.H., Christley, S., Cowell, L.G., et al.: Iterative refinement and goal\narticulation to optimize large language models for clinical information extraction.\nnpj Digital Medicine 8(1), 301 (2025)\n[26] Grothey, B., Odenkirchen, J., Brkic, A., Sch¨omig-Markiefka, B., Quaas, A.,\nB¨uttner, R., Tolkach, Y.: Comprehensive testing of large language models for\nextraction of structured data in pathology. Communications Medicine 5(1), 96\n(2025)\n[27] Kim, S.H., Schramm, S., Adams, L.C., Braren, R., Bressem, K.K., Keicher, M.,\nPlatzek, P.-S., Paprottka, K.J., Zimmer, C., Hedderich, D.M., et al.: Benchmark-\ning the diagnostic performance of open source llms in 1933 eurorad case reports.\nnpj Digital Medicine 8(1), 97 (2025)\n[28] Le Guellec, B., Lef`evre, A., Geay, C., Shorten, L., Bruge, C., Hacein-Bey, L.,\nAmouyel, P., Pruvo, J.-P., Kuchcinski, G., Hamroun, A.: Performance of an open-\nsource large language model in extracting information from free-text radiology\nreports. Radiology: Artificial Intelligence 6(4), 230364 (2024)\n[29] Nowak, S., Wulff, B., Layer, Y.C., Theis, M., Isaak, A., Salam, B., Block, W.,\nKuetting, D., Pieper, C.C., Luetkens, J.A., et al.: Privacy-ensuring open-weights\nlarge language models are competitive with closed-weights gpt-4o in extract-\ning chest radiography findings from free-text reports. Radiology 314(1), 240895\n(2025)\n[30] Sellergren, A., Kazemzadeh, S., Jaroensri, T., Kiraly, A., Traverse, M.,\nKohlberger, T., Xu, S., Jamil, F., Hughes, C., Lau, C., et al.: Medgemma technical\nreport. arXiv preprint arXiv:2507.05201 (2025)\n[31] Yu, A., Yao, L., Liu, J., Chen, Z., Yin, J., Wang, Y., Liao, X., Ye, Z., Li, J., Yue,\nY., et al.: Medreseacher-r1: Expert-level medical deep researcher via a knowledge-\ninformed trajectory synthesis framework. arXiv preprint arXiv:2508.14880 (2025)\n18\n"}, {"page": 19, "text": "[32] Team, Q.: Qwen2.5: A Party of Foundation Models (2024). https://qwenlm.\ngithub.io/blog/qwen2.5/\n[33] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur,\nA., Schelten, A., Yang, A., Fan, A., et al.: The llama 3 herd of models. arXiv\ne-prints, 2407 (2024)\n[34] Johnson, A.E., Pollard, T.J., Greenbaum, N.R., Lungren, M.P., Deng, C.-y., Peng,\nY., Lu, Z., Mark, R.G., Berkowitz, S.J., Horng, S.: Mimic-cxr-jpg, a large publicly\navailable database of labeled chest radiographs. arXiv preprint arXiv:1901.07042\n(2019)\n[35] Chen, Z., Song, Y., Chang, T.-H., Wan, X.: Generating radiology reports via\nmemory-driven transformer. arXiv preprint arXiv:2010.16056 (2020)\n[36] Qi, P., Zhang, Y., Zhang, Y., Bolton, J., Manning, C.D.: Stanza: A python natural\nlanguage processing toolkit for many human languages. In: Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics: System\nDemonstrations, pp. 101–108 (2020)\n[37] Papineni, K., Roukos, S., Ward, T., Zhu, W.-J.: Bleu: a method for automatic\nevaluation of machine translation. In: Proceedings of the 40th Annual Meeting of\nthe Association for Computational Linguistics, pp. 311–318 (2002)\n[38] Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with\nimproved correlation with human judgments. In: Proceedings of the Acl Workshop\non Intrinsic and Extrinsic Evaluation Measures for Machine Translation And/or\nSummarization, pp. 65–72 (2005)\n[39] Tanida, T., M¨uller, P., Kaissis, G., Rueckert, D.: Interactive and explainable\nregion-guided radiology report generation. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 7433–7442 (2023)\n[40] M¨uller, P., Kaissis, G., Rueckert, D.: Chex: Interactive localization and region\ndescription in chest x-rays. In: European Conference on Computer Vision, pp.\n92–111 (2024). Springer\nAppendix A\nDetails on longitudinal information\nannotation with LLMs\nA.1\nDesign of the disease list for prompt construction\nThe disease list was generated by deduplicating all disease annotations in the\nImaGenome gold set, yielding 51 unique terms. The terms “normal” and “abnormal”\nwere excluded because they do not correspond to specific disease entities. A category\nfor support devices was subsequently added. Changes in support devices, particularly\n19\n"}, {"page": 20, "text": "positional changes, do not indicate disease progression and are therefore not anno-\ntated as disease status changes in ImaGenome. Nevertheless, information on changes\nin support devices often requires reference to prior cases, justifying its inclusion.\nThe final list contains 50 terms, as follows:\n[‘support devices’,‘airspace opacity’, ‘alveolar hemorrhage’, ‘aspiration’, ‘atelec-\ntasis’, ‘bone lesion’, ‘bronchiectasis’, ‘calcified nodule’, ‘clavicle fracture’, ‘consoli-\ndation’, ‘copd/emphysema’, ‘costophrenic angle blunting’, ‘elevated hemidiaphragm’,\n‘enlarged cardiac silhouette’, ‘enlarged hilum’, ‘fluid overload/heart failure’, ‘goiter’,\n‘granulomatous disease’, ‘hernia’, ‘hydropneumothorax’, ‘increased reticular mark-\nings/ild pattern’, ‘infiltration’, ‘interstitial lung disease’, ‘lobar/segmental collapse’,\n‘low lung volumes’, ‘lung cancer’, ‘lung lesion’, ‘lung opacity’, ‘mass/nodule (not\notherwise specified)’, ‘mediastinal displacement’, ‘mediastinal widening’, ‘opacity’,\n‘pericardial effusion’, ‘pleural effusion’, ‘pleural/parenchymal scarring’, ‘pneumonia’,\n‘pneumothorax’, ‘pulmonary edema/hazy opacity’, ‘rib fracture’, ‘scoliosis’, ‘shoul-\nder osteoarthritis’, ‘spinal degenerative changes’, ‘spinal fracture’, ‘sub-diaphragmatic\nair’, ‘subcutaneous air’, ‘superior mediastinal mass/enlargement’, ‘tortuous aorta’,\n‘vascular calcification’, ‘vascular congestion’, and ‘vascular redistribution’].\nA.2\nAnalysis of large models selection\nDeep learning models typically require large-scale datasets for effective training and\nevaluation. Therefore, when selecting a LLM for report annotation, it is essential to\nbalance accuracy and inference efficiency: accuracy determines the reliability of anno-\ntations, whereas efficiency affects the feasibility of deployment in real-world clinical\nworkflows.\nAs shown in Figure 3, Qwen2.5-32B and MedGemma-27B outperform other evalu-\nated models as described in Section 3. In this section, we benchmarked their inference\nspeeds using the longitudinal annotation task as an example (Table A1). Specifically,\n100 sentences were randomly sampled from the test set, and all experiments were\nconducted on NVIDIA A100 GPUs with FlashAttention2 acceleration under identical\nconfigurations.\nThe results indicate that MedGemma-27B generates substantially longer reasoning\nchains per query, resulting in an overall runtime 15.7× longer than that of Qwen2.5-\n32B. This finding suggests that Qwen2.5-32B offers a more favorable trade-off between\naccuracy and efficiency, making it the more practical choice for real-world applications.\nAppendix B\nReport generation model configurations\nand full performance results\nB.1\nModel configurations\nGenerated reports used in this study were obtained in three ways: (1) officially provided\ngenerated outputs, (2) running inference from provided model checkpoints, and (3)\nreimplementing models using the provided code. Table B2 lists the official links, the\nsources of generated reports, and details of the model inputs. All experiments were\nrun on one NVIDIA A100 GPU.\n20\n"}, {"page": 21, "text": "Table A1 Inference speed of each model on the longitudinal annotation task. Time and tokens are\naverages per query.\nModel\nGPU#\nSpeed (tokens/s)\nTokens#\nTime (s)\nMedGemma-27B\n1\n11.34\n358.58\n31.62\nMedResearcher-R1-\n32B\n1\n19.38\n67.79\n3.50\nQwen2.5-32B\n1\n18.45\n38.23\n2.07\nLLama3.3-70B\n1\n0.42\n276.73\n658.88\n2\n24.53\n11.28\nQwen2.5-72B\n1\n0.13\n56.02\n430.9\n2\n9.40\n5.96\nTable B2 Summary of models, links, generated-report sources, image inputs, and text inputs.\n* indicates that, given the model’s large and diverse training corpus, only sources relevant to\nreport generation are listed.\nModels\nLinks\nGenerated-\nreport sources\nImage inputs\nText inputs\nR2Gen\nGitHub\nCheckpoint\nCurrent frontal or lateral view\n–\nL R2Gen\nGitHub\nReproduced\nresults\nCurrent frontal or lateral view,\nprevious frontal or lateral view\nPrevious report\nMLRG\nGitHub\nReleased results\nCurrent frontal and lateral view,\nprevious frontal view\nPrevious report,\nindication\nHC-\nLLM\nGitHub\nReproduced\nresults\nCurrent frontal or lateral view,\nprevious frontal or lateral view\nPrevious report\nLibra\nGitHub\nCheckpoint\nCurrent frontal view,\nprevious frontal view\nIndication,\nhistory,\ncomparison,\ntechnology\nMaira2*\nHugging\nFace\nCheckpoint\nCurrent frontal and lateral view,\nprevious frontal view\nPrevious report,\nindication,\ncomparison,\ntechnology\nMedVersa* Hugging\nFace\nCheckpoint\nCurrent frontal or lateral view\nIndication,\ncomparison\nPrior to language-based evaluation, data are typically preprocessed to remove\nirrelevant information, which can substantially affect evaluation outcomes. To ensure\nconsistency across models, we adopted the preprocessing procedures used in R2Gen\nfor all models, to enable fair comparisons. Detailed preprocessing steps are provided\nin Table B3.\nB.2\nFull performance results\nThe following tables provide detailed evaluation results of the report generation mod-\nels on the L-MIMIC dataset. Table B4 presents comparisons based on language-based\nmetrics, including BLEU1-4, ROUGE-L, and METEOR. Table B5 summarizes model\n21\n"}, {"page": 22, "text": "Table B3 Text Data Preprocessing Strategy in R2Gen.\nProcessing Steps\nDescription\nExample\nRemove line breaks\nReplace all newline characters\nwith spaces to form a single\ncontinuous text.\nOriginal: \"heart size normal\\nlungs clear\"\nProcessed: \"heart size normal lungs clear\"\nNormalize\nunderscores\nReplace consecutive underscores\nwith a single underscore.\nOriginal: \"right lung\"\nProcessed: \"right lung\"\nCollapse multiple\nspaces\nReplace consecutive spaces with\na single space.\nOriginal: \"heart\nsize\nnormal\"\nProcessed: \"heart size normal\"\nNormalize periods\nReplace consecutive periods\nwith a single period.\nOriginal: \"opacity.. noted\"\nProcessed: \"opacity. noted\"\nRemove numbering\nRemove common section or list\nnumbers (e.g., “1.”, “2.”).\nOriginal: \"1. lungs clear. 2. heart normal.\"\nProcessed: \"lungs clear. heart normal.\"\nLowercasing\nConvert all characters to\nlowercase for normalization.\nOriginal: \"Heart Size Normal\"\nProcessed: \"heart size normal\"\nSentence\nsegmentation\nSplit text into sentences using “.\n” as the delimiter.\nOriginal: \"heart size normal. lungs clear.\"\nProcessed: [\"heart size normal\", \"lungs clear\"]\nRemove\npunctuation\nRemove punctuation and special\ncharacters.\nOriginal: \"no acute disease!\"\nProcessed: \"no acute disease\"\nTrim whitespace\nRemove leading and trailing\nspaces in each sentence.\nOriginal: \" lungs clear \"\nProcessed: \"lungs clear\"\nReconstruct report\nRejoin cleaned sentences using\nperiods.\nOriginal: [\"lungs clear\", \"heart normal\"]\nProcessed: \"lungs clear . heart normal .\"\nperformance according to clinical efficacy metrics, including precision, recall, and F1-\nscore for both disease diagnosis and overall disease progression tracking. Table B6\nreports detailed results of disease progression tracking across individual classes, cor-\nresponding to three progression states: improved, no change, and worsened, reflecting\nthe models’ performance on different disease progression states. Together, these tables\nprovide a comprehensive overview of model performance from both linguistic and\nclinical perspectives.\nTable B4 Comparisons of model performance on the L-MIMIC dataset based on the\nlanguage-based metric. B1, B2, B3, B4, RL, ME denote BLEU-1, BLEU-2, BLEU-3, BLEU-4,\nROUGE-L and METEOR, respectively.\nPaper\nWhole report\nLongitudinal sentences\nB1\nB2\nB3\nB4\nRL\nME\nB1\nB2\nB3\nB4\nRL\nME\nR2Gen\n35.1\n20.6\n13.3\n9.3\n25.8\n13.3\n9.3\n4.9\n3.3\n2.4\n17.0\n5.9\nMedVersa\n39.0\n26.6\n19.6\n15.0\n30.8\n16.5\n7.5\n4.9\n3.7\n2.9\n18.3\n7.3\nL R2Gen\n31.5\n19.3\n12.8\n9.0\n26.1\n12.9\n11.6\n6.6\n4.5\n3.4\n18.3\n6.5\nMLRG\n39.4\n26.5\n19.4\n15.0\n31.6\n16.8\n17.6\n11.5\n8.7\n6.9\n22.6\n9.7\nHC-LLM\n40.2\n24.8\n16.5\n11.6\n27.2\n15.6\n19.6\n11.4\n7.5\n5.4\n19.7\n9.0\nMaira2\n43.2\n30.9\n23.7\n18.9\n35.1\n19.5\n23.9\n16.1\n12.1\n9.6\n26.0\n12.5\nLibra\n49.7\n36.9\n29.0\n23.6\n38.1\n22.6\n28.6\n20.2\n15.7\n12.7\n28.1\n13.9\n22\n"}, {"page": 23, "text": "Table B5 Comparisons of report generation model performance on the L-MIMIC dataset based\non clinical efficacy metrics.\nPaper\nDisease diagnosis (%)\nDisease progression (%)\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nR2Gen\n44.1\n25.7\n32.5\n33.6\n33.7\n33.6\nMedVersa\n59.1\n47.7\n52.8\n35.3\n32.5\n33.9\nL R2Gen\n54.3\n34.4\n42.1\n38.5\n39.0\n38.8\nMLRG\n59.0\n47.9\n52.8\n33.0\n39.0\n35.7\nHC-LLM\n50.6\n38.0\n43.4\n34.4\n42.5\n38.0\nMaira2\n61.5\n55.3\n58.2\n35.2\n50.4\n41.4\nLibra\n60.7\n54.8\n57.6\n35.1\n49.3\n41.0\nTable B6 Comparison of report generation model performance on the L-MIMIC dataset across\ndifferent classes of disease progression metrics based on disease progression metrics.\nPaper\nNo change (%)\nImproved (%)\nWorsened (%)\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nR2Gen\n44.9\n44.3\n44.6\n0.2\n0.2\n0.2\n2.8\n2.7\n2.7\nMedVersa\n43.4\n39.5\n41.4\n9.9\n8.9\n9.4\n14.4\n13.6\n14.0\nL R2Gen\n51.8\n51.2\n51.5\n2.2\n2.3\n2.3\n2.0\n2.1\n2.1\nMLRG\n41.6\n47.6\n44.4\n9.4\n11.9\n10.5\n11.0\n14.4\n12.5\nHC-LLM\n44.6\n53.5\n48.6\n10.0\n12.8\n11.2\n4.7\n6.1\n5.3\nMaira2\n42.5\n59.8\n49.7\n14.8\n21.9\n17.7\n15.5\n22.1\n18.3\nLibra\n43.8\n59.5\n50.5\n11.7\n17.1\n13.9\n12.5\n18.4\n14.9\n23\n"}]}