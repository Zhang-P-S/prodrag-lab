{"doc_id": "arxiv:2601.06979", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06979.pdf", "meta": {"doc_id": "arxiv:2601.06979", "source": "arxiv", "arxiv_id": "2601.06979", "title": "MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education", "authors": ["Dongsuk Jang", "Ziyao Shangguan", "Kyle Tegtmeyer", "Anurag Gupta", "Jan Czerminski", "Sophie Chheang", "Arman Cohan"], "published": "2026-01-11T16:27:21Z", "updated": "2026-01-31T01:49:10Z", "summary": "The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06979v2", "url_pdf": "https://arxiv.org/pdf/2601.06979.pdf", "meta_path": "data/raw/arxiv/meta/2601.06979.json", "sha256": "ff138c5c6e16346a3765ae101d151b606ddadf1609059813b26eedfc79c66936", "status": "ok", "fetched_at": "2026-02-18T02:21:40.575583+00:00"}, "pages": [{"page": 1, "text": "MedTutor: A Retrieval-Augmented LLM System for\nCase-Based Medical Education\nDongsuk Jang1,3\nZiyao Shangguan1\nKyle Tegtmeyer2\nAnurag Gupta2\nJan Czerminski2\nSophie Chheang2\nArman Cohan1\n1Department of Computer Science, Yale University\n2Department of Radiology and Biomedical Imaging, Yale School of Medicine\n3Interdisciplinary Program for Bioengineering, Seoul National University\n{james.jang,ziyao.shangguan,arman.cohan}@yale.edu\n§ Code\nÅ Demo Video\nDataset\nAbstract\nThe learning process for medical residents\npresents significant challenges, demanding\nboth the ability to interpret complex case re-\nports and the rapid acquisition of accurate medi-\ncal knowledge from reliable sources. Residents\ntypically study case reports and engage in dis-\ncussions with peers and mentors, but finding\nrelevant educational materials and evidence to\nsupport their learning from these cases is often\ntime-consuming and challenging. To address\nthis, we introduce MedTutor, a novel system\ndesigned to augment resident training by au-\ntomatically generating evidence-based educa-\ntional content and multiple-choice questions\nfrom clinical case reports. MedTutor lever-\nages a Retrieval-Augmented Generation (RAG)\npipeline that takes clinical case reports as input\nand produces targeted educational materials.\nThe system’s architecture features a hybrid re-\ntrieval mechanism that synergistically queries a\nlocal knowledge base of medical textbooks and\nacademic literature (using PubMed, Semantic\nScholar APIs) for the latest related research, en-\nsuring the generated content is both foundation-\nally sound and current. The retrieved evidence\nis filtered and ordered using a state-of-the-art\nreranking model and then an LLM generates\nthe final long-form output describing the main\neducational content regarding the case-report.\nWe conduct a rigorous evaluation of the system.\nFirst, three radiologists assessed the quality of\noutputs, finding them to be of high clinical and\neducational value. Second, we perform a large-\nscale evaluation using an LLM-as-a Judge to\nunderstand if LLMs can be used to evaluate\nthe output of the system. Our analysis using\ncorrelation between LLMs outputs and human\nexpert judgments reveals a moderate alignment\nand highlights the continued necessity of expert\noversight.\n1\nIntroduction\nThe training of medical residents is an intensive\nlearning process, built upon the foundation of\nstudying and interpreting thousands of case re-\nports. Residents routinely engage with clinical\ncases through discussions with peers and mentors,\nanalyzing findings and differential diagnoses to\ndeepen their understanding. However, while direct\nfeedback from attending physicians is invaluable,\nthe process of finding relevant educational material\nand supporting evidence for specific cases is of-\nten time-consuming and inconsistent (Rogers et al.,\n2019; Daniel et al., 2020). The sheer volume of\nmedical literature and the challenge of identifying\npertinent resources for each case can limit the depth\nof learning that residents achieve from their clin-\nical experiences (Anderson and Anderson, 2019;\nBednarczyk et al., 2014). There exists a significant\nopportunity to augment this traditional learning pro-\ncess with AI tools that can efficiently retrieve and\nsynthesize educational content from clinical cases,\ndrawing upon vast archives of medical knowledge.\nLLMs present a promising avenue for this augmen-\ntation, but their application in high-stakes medical\ndomains is fraught with challenges, most notably\nthe risk of factual inaccuracy (or hallucination) and\nthe use of outdated knowledge (Abd-alrazaq et al.,\n2023; Li et al., 2023; Xie and Wang, 2023).\nTo overcome these challenges, we develop Med-\nTutor, a system that grounds LLM generation in\nverifiable, contextually relevant medical knowledge\nto case reports through a RAG pipeline. Our pri-\nmary goal is to provide medical residents with a\nreliable tool that transforms any given clinical re-\nport into a concise, and highly relevant educational\nmodule. We focus on radiology as the domain of\nstudy, although, our techniques are generalizable\nto other domains. The system begins by decom-\nposing clinical reports into actionable diagnostic\nqueries and keywords that can be effectively is-\nsued to a search index, enabling targeted retrieval\nof relevant educational material. It then initiates a\nhybrid retrieval process that simultaneously queries\na curated database of medical textbooks, and per-\n1\narXiv:2601.06979v2  [cs.CL]  31 Jan 2026\n"}, {"page": 2, "text": "1. Query \ngeneration\nClinical report\nKeyword 1\nKeyword 2\n…\nTextbook local DB\n2. Retrieval\nLive API\nRerank\n3. Generation\nEducational \nmaterial\nMultiple \nchoice Q&A\nFindings:\nFrontal and \nlateral views \nof the chest \nwere \nobtained.\n…  \nImpression:\nPersistent \ncardiomegaly,  \n…\nEducational \nmaterial\nMultiple \nchoice Q&A\nTextbook\nsummary\nOriginal report\nLearning \nmodule\n4. Evaluation\nLLM-as-a-Judge\nHuman specialist \nevaluation\nTextbook \nsummary\nTextbook snippets\nReranked \npapers\nGenerator \nmodel\nOriginal \nreport\nOriginal clinical report\nTextbook summary\nEducational material\nMultiple choice Q&A\nFINDINGS: \nLIVER: The liver is coarsened and nodular in \nechotexture. The contour of the liver is nodular, \nconsistent with cirrhosis. There is no focal liver \nmass. The main portal vein is patent with \nhepatopetal flow. (The rest omitted)\nIMPRESSION: \nCirrhotic liver with increased nodularity of the \nliver compared to the study from (anonymized)\nCirrhosis is characterized by a hallmark \nhistological pattern of fibrotic bridges carving \nthe liver into regenerative nodules, with key \nradiologic findings including surface nodularity, \nwidening of fissures, and expansion of the \ngallbladder fossa. Imaging features include \nmicronodules or macronodules, caudate lobe \nhypertrophy, and signs of portal hypertension \nsuch as splenomegaly, ascites, and varices, \nwith diagnostic considerations varying based on \netiology, such as ethanol, hemochromatosis, or \nchronic viral hepatitis…(The rest omitted)\n1. Definition and Pathophysiology: Cirrhosis is a \nlate stage of scarring of the liver caused by \nmany forms of liver diseases and condition…\n2. Imaging Features: Ultrasound: The liver \nappears coarsened and nodular in echotexture \nwith a nodular contour. …\n3. Diagnostic Considerations: …\n4. Complications: …\n5. Surveillance and Monitoring: …\n6. Cardiac Implications: …\n7. Imaging Modalities for Surveillance: …\nQ1. What is a characteristic ultrasound finding \nin a liver affected by cirrhosis?\nA. Smooth liver surface\nB. Coarsened and nodular liver echotexture\nC. Focal liver mass\nD. Absence of portal vein flow\nAnswer: B\nExplanation: The ultrasound findings in the \nprovided context describe the liver as \n\"coarsened and nodular in echotexture\" and the \ncontour of the liver as …\nFigure 1: The overall architecture of the MedTutor system.\nforms live searches on academic search engines\n(i.e., PubMed and Semantic Scholar) for current\npublished literature related to the case.\nThe retrieved evidence undergoes a multi-\nfaceted processing step: academic articles and text-\nbook snippets are reranked for relevance to the case,\nusing a state-of-the-art reranking model, Qwen3-\nReranker-8B (Zhang et al., 2025b). Finally, all\nprocessed evidence—the original report, keywords,\ntop-ranked articles, and textbook summaries—is\nsynthesized by a generator LLM into two distinct\noutputs: a comprehensive set of educational mate-\nrial and a set of multiple-choice questions (MCQs)\ndesigned to test understanding. The overview of\nthe system is illustrated in Figure 1.\nThis work makes three primary contributions:\n• We detail the design and implementation of\nthe MedTutor system, a scalable and efficient ar-\nchitecture that leverages asynchronous I/O, paral-\nlel multi-GPU inference with vLLM (Kwon et al.,\n2023), and optimized batch processing to handle\nlarge workloads.\n• We introduce a new, expert annotated bench-\nmark dataset for evaluating the quality of AI-\ngenerated educational content. We run our pipeline\nwith 6 LLMs(see Appendix A for details) across\n2,000 clinical reports per each 5 major radiol-\nogy datasets (i.e., Yale Hospital Internal, MIMIC-\nCXR (Johnson et al., 2019, 2024), MIMIC-IV-note\n(Johnson et al., 2023), CheXpert Plus (Chambon\net al., 2024), and ReXGradient-160K (Zhang et al.,\n2025a)).\n• We collected comprehensive evaluations from\nthree radiologists, alongside a LLM-as-a-Judge\nevaluation with four models for all experiments.\nThis dataset, which we are planning to publicly\nrelease, will be a valuable resource for evaluating\nthe quality and clinical utility of generative models\nin medicine.\nOur analysis provides insights about the use-\nfulness of our system to users and highlight the\nstrengths and weaknesses of LLMs in evaluating\neducational content in our setting.\n2\nRelated Work\nOur work is situated at the intersection of RAG,\nthe application of LLMs in medicine, and the criti-\ncal need for trustworthy medical AI systems. We\nstructure our review accordingly.\n2.1\nLLMs and RAG in the Medical Domain\nThe application of LLMs in medicine has shown\nimmense promise. General-purpose foundation\nmodels have demonstrated impressive capabilities\non standardized medical exams and complex diag-\nnostic problems (Nori et al., 2023; Singhal et al.,\n2023). This has fueled a broader vision for general-\nist biomedical AI that can assist with a wide range\nof clinical tasks (Tu et al., 2023). However, the\n“black-box” nature of these models and their po-\ntential for factual errors or \"hallucinations\" remain\n2\n"}, {"page": 3, "text": "significant barriers to clinical adoption, necessitat-\ning robust evaluation frameworks (Huang et al.,\n2024; Li et al., 2023; Xie and Wang, 2023).\nTo mitigate these risks, RAG has emerged as\na key paradigm for building dependable clinical\ntools (Lewis et al., 2021). By grounding LLM\noutputs in external, verifiable evidence from reli-\nable medical literature, RAG provides a pathway\nto trustworthy AI. A recent perspective in Nature\nMedicine strongly advocates for RAG as a prereq-\nuisite for the responsible deployment of generative\nAI in healthcare (Yang et al., 2024). The field is\nnow maturing to a point where standardized bench-\nmarks for medical RAG are being established, al-\nlowing for more rigorous evaluation of these sys-\ntems (Xiong et al., 2024). Our work contributes\nto this growing body of literature by presenting a\nnovel RAG-based system specifically designed for\nmedical education, a domain where accuracy and\nreliability are paramount.\n2.2\nLLMs for Medical Education\nLLMs show promise in generating medical exam\ncontent, though concerns about accuracy necessi-\ntate expert oversight (Zhu et al., 2025). Integrat-\ning RAG improves reliability by grounding out-\nput in external sources, with studies reporting no-\ntable gains in question-answering accuracy using\nmedical textbooks (Chen et al., 2025; Wang et al.,\n2025b). Benchmarks like MIRAGE further vali-\ndate RAG’s role in medical QA tasks (Xiong et al.,\n2024).\nFor resident training, LLMs can assess skills and\nprovide feedback, but expert review remains vital\n(Atsukawa et al., 2025). Systems enabling citation\ngeneration enhance factuality (Wang et al., 2025a),\nwhile evaluation frameworks like LLM-as-a-Judge\noffer scalability despite only moderate alignment\nwith human judgment (Zheng et al., 2025). New\napproaches continue to embed evidence-based\nmedicine principles into RAG pipelines for clini-\ncally accurate educational content (Lu et al., 2025).\nOur system, MedTutor, is distinct in its focus\non transforming a single clinical report into a com-\nprehensive educational module, featuring synthe-\nsized educational material and MCQs grounded\nin a hybrid retrieval from both medical textbooks\nand the latest academic literature. This approach is\ndesigned not to replace expert judgment but to aug-\nment it, fostering the self-directed learning skills\nthat are crucial for lifelong professional develop-\nment (Bravata et al., 2003; Williams and Ntiri,\n2018).\n3\nMedTutor\nMedTutor is a RAG system designed to support\nmedical residents on case-based education. It in-\nvolves a pipeline approach in retrieving highly rel-\nevant educational content from both textbooks and\nliterature and produces a coherent educational ma-\nterial as well as multiple-choice questions related\nto a case. While MedTutor’s design is general and\ncan be applicable to many clinical practices, we fo-\ncus our domain on radiology due to availability of\npublic datasets and our access to domain experts.\n3.1\nThe MedTutor Pipeline Stages\nThe input to MedTutor is a case report, which\nwill be processed through a sequence of automated\nstages, each designed for parallel execution.\nCase decomposition into search queries: The\nprocess begins with a source radiology report. Then\nwe use an Llama-3.3-70B-Instruct(Meta, 2024) to\nprocess the radiology report and decompose it into\nmultiple keyword based queries that will be used\nfor retrieval. These queries are key diagnostic terms\nand findings. Prompts for case decomposition into\nsearch queries are shown in Appendix D.1.\nHybrid Evidence Retrieval: For each search\nquery, the system performs a hybrid retrieval\nprocess in parallel described below: (1) Local\nretrieval for textbook material: Textbooks and\nnotes are essential resources for medical educa-\ntion.\nIn our MedTutor system, we first apply\nOCR to a radiology textbook (Dähnert, 2017) us-\ning the mistral-ocr-2503 model, then segment\nand index the material by page.\nWe generate\ndense embeddings for these materials with the\nQwen3-Embedding-8B model, which has demon-\nstrated state-of-the-art performance in embedding\nand retrieval tasks on the MTEB benchmark (Muen-\nnighoff et al., 2023) among models of compara-\nble size. These embeddings are stored in a pre-\ncomputed vector database for subsequent queries.\nFor local database search, we employ a bi-encoder\narchitecture to generate dense vector representa-\ntions for both the query and the pre-indexed text-\nbook pages, subsequently identifying the most rel-\nevant page using cosine similarity. (2) Retrieval\nusing academic APIs: Some case reports are more\nspecialized or rare, requiring retrieving knowledge\nfrom latest academic literature. Therefore, we also\nemploy retrieval from academic search engines.\n3\n"}, {"page": 4, "text": "We use PubMed and Semantic Scholar APIs, two\ncommonly used and freely available scholarly sys-\ntems, to fetch the latest relevant research papers.\nTo prevent rate-limiting, API calls are managed\nby an asyncio.Semaphore. If pre-fetched results\nfor the queries are available, this step is skipped to\nimprove efficiency.\nEvidence Processing: The retrieved evidence is\nthen processed through two concurrent tasks: (1)\nReranking: As the search engine results using key-\nword queries can be noisy, we employ a reranking\nstage to prioritize the most relevant (top 2) doc-\numents to the case report. This is handled by a\ndedicated service running the Qwen3-Reranker-8B\nmodel, a strong reranker according to the MTEB\nbenchmark. The reranker is given a contextual-\nized query containing both the original report’s text\nand the specific search keyword to improve rele-\nvance. (2) Query-focused Summarization: Concur-\nrently, the content retrieved from the local textbook\ndatabase is summarized with respect to the query\nby a generator LLM to distill key information re-\nlated to the keywords into a concise way.\nGenerating Learning Modules: Finally, the\noriginal case report, the top retrieved content in-\ncluding the textbook snippets and abstracts of re-\nlated papers, and the search keywords are passed\nto a generator LLM to generate a concise learning\nmodule. These learning modules contain compre-\nhensive explanatory material contextualizing the\ncase within broader medical knowledge, followed\nby multiple-choice questions designed to test un-\nderstanding of key concepts. Prompts used for\ngenerating learning modules are in Appendix D.\nOptimized Multi-Task Generation: The gen-\neration step is heavily optimized for efficiency. In-\nstead of generating outputs sequentially, the system\nfirst constructs prompts for all cases received, and\nall sub-tasks.\nBatch Construction: Two distinct batches of in-\nput prompts to LLMs are created: one for gener-\nating the final educational modules and another\nfor generating multiple-choice questions. These\ninput prompts are long-context (3530 tokens for\nMCQ, 3463 tokens for Educational module in av-\nerage), containing the original report, the list of\nkeywords, the abstracts of the top-ranked papers\nafter reranking, and the generated textbook sum-\nmaries.\nConcurrent Batch Inference: The two\nbatches are sent concurrently to the generation ser-\nvice. The generate_text_batch method in our\nVLLMHandler passes the entire list of prompts to\nthe vLLM engine in a single call. This fully lever-\nages vLLM’s continuous batching capability, al-\nlowing the GPU to process multiple requests simul-\ntaneously without padding, dramatically increasing\nthroughput and reducing overall processing time.\nThis architecture, particularly the use of batch gen-\neration with vLLM, allows MedTutor to process\nhundreds of complex reports far more efficiently\nthan a naive, sequential approach, making it a prac-\ntical tool for large-scale educational content cre-\nation.\nLocal Deployment: We deploy MedTutor com-\npletely locally using locally served open-source\nLLMs, without reliance on any cloud-based LLM\nAPIs. This allows responsible and private handling\nof medical data.\n3.2\nSystem Design Details\nThe MedTutor pipeline is an asynchronous, multi-\nstage system designed for efficiency, scalability,\nand modularity. The architecture leverages par-\nallel processing across multiple GPUs and opti-\nmized batching to handle large-scale report gen-\neration. The entire workflow is orchestrated by a\ncentral asyncio event loop, which communicates\nwith dedicated ModelWorker processes via multi-\nprocessing queues. A conceptual overview of the\narchitecture is shown in Figure 1.\n3.3\nArchitecture for Scalability\nAt the core of our system is a hybrid concurrency\nmodel designed to maximize throughput and re-\nsource utilization.\nAsynchronous Orchestration: The main pro-\ncess runs on an asyncio event loop, managing I/O-\nbound tasks such as live API calls for literature re-\ntrieval and orchestrating the overall pipeline. This\nallows the system to handle thousands of concur-\nrent operations efficiently without being blocked\nby network latency.\nParallel Multi-GPU Inference: To handle the\ncomputationally intensive model inference, we\nspawn separate ModelWorker processes for each\nrequired service (e.g., reranking, generation). Each\nworker is pinned to a specific GPU or set of GPUs\nas defined in the configs.json file. Within each\nworker, we use the vLLM engine, a state-of-the-art\nserving library that employs techniques like Page-\ndAttention to achieve high-throughput, low-latency\ninference.\nInter-Process\nCommunication:\nThe\nmain asyncio loop communicates with the\n4\n"}, {"page": 5, "text": "ModelWorker processes using a robust queue-\nbased system (multiprocessing.Queue).\nA\nrequest, tagged with a unique ID, is placed on\na request queue. The main loop then awaits an\nasyncio.Future associated with that ID. The\nworker process retrieves the request, performs\nthe inference, and places the result on a response\nqueue. A dedicated listener task in the main loop\nlistens for responses and resolves the correspond-\ning Future, seamlessly bridging the asynchronous\nand multi-process components.\n4\nSystem Evaluation\nWe conduct a multi-faceted evaluation to assess\nthe quality of our MedTutor system. Given our\nfocus on the radiology domain, the evaluation is\ndone by three radiologists who scored the outputs\non a 5-point Likert scale (1=Poor, 5=Excellent).\nAnnotation guidelines and the annotation interface\ndesign are detailed in Appendices F and G. We\nevaluate both the intermediate “upstream” compo-\nnents of our pipeline and the final “downstream”\ngenerated content. Furthermore, we investigate the\nfeasibility of using an LLM-as-a-Judge as a proxy\nfor human evaluation of the AI generated educa-\ntional content by analyzing its agreement with our\nhuman experts.\n4.1\nUpstream Component Quality\nFirst, we evaluate the quality of the upstream com-\nponents that feed into the final generator: search\nquery extraction and retrieved paper relevance.\nThis evaluation was conducted on a set of 50 clin-\nical cases. As shown in Table 1, human experts\nfound the search queries extracted by the system to\nbe highly appropriate (Human Avg. score of 3.73).\nHowever, they were more critical of the relevance\nof the retrieved academic papers, giving an average\nscore of 2.88. This suggests that while the system\ncorrectly identifies the main topics, the unfiltered,\nlive-retrieved literature can contain articles that are\nnot perfectly aligned with the specific clinical con-\ntext of the report. In contrast, the LLM judges rated\nthe paper relevance significantly higher (LLM Avg.\n4.20), indicating a divergence in the assessment of\ncontextual relevance between human experts and\nautomated metrics.\n4.2\nDownstream Generation Quality\nThe primary evaluation focused on the quality of\nthe final, user-facing outputs: textbook summaries,\nEvaluator\nQuery\nPaper\nAppropriateness\nRelevance\nHuman Evaluators\n3.73\n2.88\nMedGemma-27B\n3.73\n4.34\nGPT-4.1-mini\n4.15\n4.52\nGemini-2.5-Flash\n4.27\n4.58\nGemini-2.5-Pro\n4.03\n3.37\nLLM Avg.\n4.05\n4.20\nTable 1: Comparison of evaluator scores. The “ Eval-\nuators” row represents the combined results from two\nindependent radiologists (n=50 each).\nMCQs, and educational material. Three radiol-\nogists annotated the outputs from two generator\nmodels, Llama-3.3-70B-Instruct and MedGemma-\n27B(Sellergren et al., 2025). The detailed results\nare presented in Table 2.\nBoth models produced high-quality outputs ac-\ncording to our expert evaluators. Llama 3.3-70B-\nInstruct achieved a respectable average human\nscore of 3.44, demonstrating its capability in syn-\nthesizing complex medical information into educa-\ntional content. MedGemma-27B, a model more\nspecialized for the medical domain, performed\nslightly better, with an average human score of 3.65.\nThe experts particularly noted the higher quality of\nthe MCQs generated by MedGemma-27B (3.53)\ncompared to those from Llama-3.3-70B-Instruct\n(3.11). This suggests that the domain-specific na-\nture of MedGemma-27B provides a distinct ad-\nvantage in generating educational content, such as\nplausible distractors for multiple-choice questions.\nWhen comparing human evaluations to the LLM-\nas-a-Judge scores, we note an interesting trend. The\nLLM judges also preferred MedGemma-27B over\nLlama 3.3, aligning with the relative ranking of the\nhuman experts. However, the LLMs consistently\nassigned higher absolute scores than the human\nradiologists. This suggests that while LLM-as-a-\nJudge can be a valuable tool for scalable, relative\ncomparisons between models, its scoring calibra-\ntion differs from that of human experts, indicating a\ntendency for score inflation. These findings suggest\na promising path toward semi-automated evalua-\ntion while reinforcing the role of human experts as\nthe gold standard for assessing clinical utility. Full\nLLM-as-a-Judge results are in Tab A.\n5\n"}, {"page": 6, "text": "Model\nEvaluator\nTextbook\nSummary\nEducational\nMaterial\nMCQ\nQuality\nOverall\nAverage\nLlama 3.3-70B-Instruct\nHuman Evaluators\n3.43\n3.78\n3.11\n3.44\nMedGemma-27B\n3.64\n3.66\n3.79\n3.70\nGPT-4.1-mini\n4.34\n4.50\n4.19\n4.34\nGemini-2.5-Flash\n2.82\n3.58\n4.08\n3.49\nGemini-2.5-Pro\n3.95\n4.28\n4.14\n4.12\nLLM Avg.\n3.69\n4.01\n4.05\n3.91\nMedGemma-27B\nHuman Evaluators\n3.58\n3.84\n3.53\n3.65\nMedGemma-27B\n3.65\n4.09\n4.22\n3.99\nGPT-4.1-mini\n4.21\n4.79\n4.60\n4.53\nGemini-2.5-Flash\n3.05\n4.61\n4.47\n4.04\nGemini-2.5-Pro\n3.84\n4.18\n4.15\n4.06\nLLM Avg.\n3.69\n4.42\n4.36\n4.16\nTable 2: Main Generation Task Quality: Direct Comparison of Human Expert and LLM-as-a-Judge Evaluations.\nThe “Human Evaluators” scores represent the combined results from three independent radiologists (n=50 each).\nAll scores are on a 1-5 scale (5=best).\n4.3\nInter-Annotator Agreement\nTo ensure the reliability of our human evaluations,\nwe measured the inter-annotator agreement (IAA)\nbetween the two board-certified radiologists using\nKrippendorff’s Alpha (Krippendorff, 2011).\nThe alpha coefficient is calculated as: α =\n1 −Do\nDe Here, Do is the observed disagreement,\ncalculated as the average difference between the\nratings from each human annotator, A1 and A2,\nacross all M evaluated items. Specifically, if ri,1\nand ri,2 are the ratings for item i from A1 and A2\nrespectively, then:\nDo = 1\nM\nM\nX\ni=1\nδ2(ri,1, ri,2)\nDe represents the disagreement expected by\nchance, calculated based on the individual rating\ndistributions of A1 and A2. For the difference func-\ntion δ2, we first recoded the 1-to-5 Likert scale\nratings into a 3-point interval scale (1-2 →1; 3 →\n2; 4-5 →3) and then applied a squared difference:\nδ2(u, v) = (u −v)2.\nThe results, presented in Table 4, show a range\nof agreement levels. We observed good agreement\nfor the Textbook Summary from MedGemma-27B\n(α = 0.661) and fair agreement for Paper Rele-\nvance (α = 0.493).\nOverall, our annotators demonstrated moderate\nto good agreement across most tasks (with the ex-\nception of MCQ quality), which is in line with\nagreement levels reported in prior work on high-\nquality datasets (Liu et al., 2024; Bavaresco et al.,\n2025). The lower agreement for MCQ evaluation\n(α = 0.048) suggests that the criteria for this spe-\ncific task may require more detailed guidelines to\nimprove consistency.\n5\nConclusion\nIn this work, we introduce MedTutor, a novel,\nopen-source system designed to augment clini-\ncal education by transforming clinical reports into\nstructured, evidence-backed learning modules. Our\nsystem addresses the critical challenges of fac-\ntual accuracy and knowledge freshness in medi-\ncal AI by employing a sophisticated RAG pipeline.\nThis pipeline features a hybrid retrieval mechanism\nthat synthesizes knowledge from both foundational\nmedical textbooks and real-time academic litera-\nture, ensuring the generated educational modules\nare both reliable and current.\nOur rigorous evaluation, conducted by board-\ncertified radiologists, confirmed that MedTutor can\nproduce high-quality, clinically valuable educa-\ntional content. Furthermore, our large-scale LLM-\nas-a-Judge analysis revealed a moderate but promis-\ning correlation with human expert judgments, sug-\ngesting a viable path toward scalable automated\nevaluation while underscoring the continued impor-\ntance of expert oversight.\nBy publicly releasing the MedTutor system, its\nuser interface, and the comprehensive evaluation\ndataset, we make two key contributions. First, we\nprovide a practical tool that can be immediately\nadapted by other institutions to enhance their own\ntraining programs. Second, we offer a valuable\nbenchmark and framework for future research into\n6\n"}, {"page": 7, "text": "building trustworthy and effective generative AI\nsystems for the high-stakes medical domain. We be-\nlieve this work represents a significant step toward\nfostering more effective and efficient clinician-AI\ncollaboration in medical education.\n6\nLimitations\nWhile MedTutor demonstrates a promising ap-\nproach to augmenting medical education, we ac-\nknowledge several limitations that offer avenues\nfor future work.\nFirst, our evaluation is primarily focused on the\ndomain of radiology. Although the system’s archi-\ntecture is designed to be generalizable, its effec-\ntiveness and the nuances of its application in other\nmedical specialties with different reporting styles\nand knowledge structures, such as pathology or car-\ndiology, have not yet been explored. Future studies\nshould assess the adaptability and performance of\nMedTutor across a broader range of clinical do-\nmains.\nSecond, the human evaluation, while rigorous\nand conducted by domain experts, was performed\non a dataset of 50 clinical cases. A larger-scale\nstudy involving a greater number of cases and a\nmore diverse cohort of radiologists would be ben-\neficial to further validate our findings and provide\nmore robust statistical power to the conclusions\ndrawn.\nFinally, our analysis of inter-annotator agree-\nment and the LLM-as-a-Judge evaluations high-\nlights challenges in consistently generating high-\nquality subjective content. The lower agreement\nscores for MCQs, for instance, suggest that these\noutputs require further refinement. This indicates\nthat more advanced prompting techniques, fine-\ntuning of the generator models, or more sophisti-\ncated evaluation guidelines may be necessary to\nimprove the reliability and educational value of\nthese more complex, creative tasks.\nAcknowledgments\nThis research was supported by a grant of the Korea\nHealth Technology R&D Project through the Korea\nHealth Industry Development Institute (KHIDI),\nfunded by the Ministry of Health & Welfare, Re-\npublic of Korea (grant number: HI19C1352).\n7\n"}, {"page": 8, "text": "References\nAlaa Abd-alrazaq, Rawan AlSaad, Dari Alhuwail, Ar-\nfan Ahmed, Padraig Mark Healy, Syed Latifi, Sarah\nAziz, Rafat Damseh, Sadam Alabed Alrazak, and\nJavaid Sheikh. 2023. Large language models in med-\nical education: Opportunities, challenges, and future\ndirections. JMIR Medical Education, 9(1):e48291.\nMichael Anderson and Susan Leigh Anderson. 2019.\nHow should ai be developed, validated, and imple-\nmented in patient care?\nAMA Journal of Ethics,\n21(2):E125–130.\nNatsuko Atsukawa, Hiroyuki Tatekawa, Tatsushi Oura,\nShu Matsushita, Daisuke Horiuchi, Hirotaka Takita,\nYasuhito Mitsuyama, Ayako Omori, Taro Shimono,\nYukio Miki, and Daiju Ueda. 2025. Evaluation of\nradiology residents’ reporting skills using large lan-\nguage models: an observational study. Japanese\nJournal of Radiology, 43(7):1204–1212.\nAnna Bavaresco, Raffaella Bernardi, Leonardo Berto-\nlazzi, Desmond Elliott, Raquel Fernández, Albert\nGatt, Esam Ghaleb, Mario Giulianelli, Michael\nHanna, Alexander Koller, André F. T. Martins,\nPhilipp Mondorf, Vera Neplenbroek, Sandro Pezzelle,\nBarbara Plank, David Schlangen, Alessandro Sug-\nlia, Aditya K Surikuchi, Ece Takmaz, and Alberto\nTestoni. 2025. Llms instead of human judges? a\nlarge scale empirical study across 20 nlp evaluation\ntasks. Preprint, arXiv:2406.18403.\nJoseph Bednarczyk, Merril Pauls, Jason Fridfinnson,\nand Erin Weldon. 2014. Characteristics of evidence-\nbased medicine training in royal college of physicians\nand surgeons of canada emergency medicine residen-\ncies - a national survey of program directors. BMC\nMedical Education, 14(1).\nDawn MT Bravata, Stephen J Huot, Hadley S Abernathy,\nKelley M Skeff, and Dena MC Bravata. 2003. The\ndevelopment and implementation of a curriculum to\nimprove clinicians’ self-directed learning skills: a\npilot project. BMC Medical Education, 3(1).\nPierre Chambon, Jean-Benoit Delbrouck, Thomas\nSounack, Shih-Cheng Huang, Zhihong Chen, Maya\nVarma, Steven QH Truong, Chu The Chuong, and\nCurtis P. Langlotz. 2024. Chexpert plus: Augment-\ning a large chest x-ray dataset with text radiology\nreports, patient demographics and additional image\nformats. Preprint, arXiv:2405.19538.\nRong Chen, Siyun Zhang, Yiyi Zheng, Qiuhua Yu,\nand Chuhuai Wang. 2025.\nEnhancing treatment\ndecision-making for low back pain: a novel frame-\nwork integrating large language models with retrieval-\naugmented generation technology.\nFrontiers in\nMedicine, 12.\nWolfgang Dähnert. 2017. Radiology review manual.\nWolters Kluwer, Philadelphia.\nDennis A Daniel, Sue E Poynter, Christopher P Landri-\ngan, Charles A Czeisler, Jeffrey P Burns, and Traci A\nWolbrink. 2020. Pediatric resident engagement with\nan online critical care curriculum during the inten-\nsive care rotation*. Pediatric Critical Care Medicine,\n21(11):986–991.\nYining Huang, Keke Tang, Meilian Chen, and Boyuan\nWang. 2024. A comprehensive survey on evaluating\nlarge language model applications in the medical\nindustry. Preprint, arXiv:2404.15777.\nA. Johnson, T. Pollard, R. Mark, S. Berkowitz, and\nS. Horng. 2024. Mimic-cxr database (version 2.1.0).\nAlistair Johnson, Tom Pollard, Steven Horng, Leo An-\nthony Celi, and Roger Mark. 2023. Mimic-iv-note:\nDeidentified free-text clinical notes (version 2.2).\nAlistair E W Johnson, Tom J Pollard, Seth J Berkowitz,\nand 1 others. 2019. Mimic-cxr, a de-identified pub-\nlicly available database of chest radiographs with\nfree-text reports. Scientific data, 6(1):317.\nKlaus Krippendorff. 2011. Computing Krippendorff’s\nAlpha-Reliability.\nTechnical report, Annenberg\nSchool for Communication, University of Pennsylva-\nnia.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Ef-\nficient memory management for large language\nmodel serving with pagedattention.\nPreprint,\narXiv:2309.06180.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2021.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. Preprint, arXiv:2005.11401.\nDongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu,\nZiyang Chen, Baotian Hu, Aiguo Wu, and Min\nZhang. 2023. A survey of large language models\nattribution. Preprint, arXiv:2311.03731.\nYixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun\nZhao, Simeng Han, Shafiq Joty, Pengfei Liu,\nDragomir Radev, Chien-Sheng Wu, and Arman Co-\nhan. 2024.\nBenchmarking generation and eval-\nuation capabilities of large language models for\ninstruction controllable summarization.\nPreprint,\narXiv:2311.09184.\nKeer Lu, Zheng Liang, Da Pan, Shusen Zhang, Gu-\nosheng Dong, Zhonghai Wu, Huang Leng, Bin\nCui, and Wentao Zhang. 2025.\nMed-r2: Craft-\ning trustworthy llm physicians via retrieval and\nreasoning of evidence-based medicine.\nPreprint,\narXiv:2501.11885.\nMeta. 2024. Llama 3.3 70b instruct.\nMeta.\n2025.\nThe\nllama\n4\nherd:\nThe\nbe-\nginning of a new era of natively multimodal\nai\ninnovation.\nhttps://ai.meta.com/blog/\n8\n"}, {"page": 9, "text": "llama-4-multimodal-intelligence/. Accessed:\n2025-05-20.\nNiklas Muennighoff, Nouamane Tazi, Loïc Magne, and\nNils Reimers. 2023. Mteb: Massive text embedding\nbenchmark. Preprint, arXiv:2210.07316.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabilities\nof gpt-4 on medical challenge problems. Preprint,\narXiv:2303.13375.\nMiranda J. Rogers, Michelle Zeidan, Zachary S.\nFlinders, Angela P. Presson, and Robert Burks.\n2019. Educational resource utilization by current\northopaedic surgical residents. JAAOS: Global Re-\nsearch and Reviews, 3(4):e041.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri,\nAtilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, Cían Hughes, Charles\nLau, Justin Chen, Fereshteh Mahvar, Liron Yatziv,\nTiffany Chen, Bram Sterling, Stefanie Anna Baby, Su-\nsanna Maria Baby, Jeremy Lai, Samuel Schmidgall,\nand 62 others. 2025. Medgemma technical report.\nPreprint, arXiv:2507.05201.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, Mike Schaeker-\nmann, Amy Wang, Mohamed Amin, Sami Lachgar,\nPhilip Mansfield, Sushant Prakash, Bradley Green,\nEwa Dominowska, Blaise Aguera y Arcas, and 12\nothers. 2023. Towards expert-level medical ques-\ntion answering with large language models. Preprint,\narXiv:2305.09617.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaek-\nermann, Mohamed Amin, Pi-Chuan Chang, Andrew\nCarroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, Basil\nMustafa, Aakanksha Chowdhery, Yun Liu, Simon\nKornblith, David Fleet, Philip Mansfield, Sushant\nPrakash, Renee Wong, Sunny Virmani, and 13 others.\n2023. Towards generalist biomedical ai. Preprint,\narXiv:2307.14334.\nXiao Wang, Mengjue Tan, Qiao Jin, Guangzhi Xiong,\nYu Hu, Aidong Zhang, Zhiyong Lu, and Minjia\nZhang. 2025a.\nMedcite: Can language models\ngenerate verifiable text for medicine?\nPreprint,\narXiv:2506.06605.\nYubo Wang, Xueguang Ma, and Wenhu Chen. 2025b.\nAugmenting black-box llms with medical text-\nbooks for biomedical question answering. Preprint,\narXiv:2309.02233.\nAdrienne Williams and Shana Ntiri. 2018. An online,\nself-directed curriculum of core research concepts\nand skills. MedEdPORTAL, 14.\nQianqian Xie and Fei Wang. 2023. Faithful ai in health-\ncare and medicine. medRxiv.\nGuangzhi\nXiong,\nQiao\nJin,\nZhiyong\nLu,\nand\nAidong Zhang. 2024.\nBenchmarking retrieval-\naugmented generation for medicine.\nPreprint,\narXiv:2402.13178.\nRui Yang, Yilin Ning, Emilia Keppo, Mingxuan\nLiu, Chuan Hong, Danielle S Bitterman, Jasmine\nChiat Ling Ong, Daniel Shu Wei Ting, and Nan\nLiu. 2024. Retrieval-augmented generation for gen-\nerative artificial intelligence in medicine. Preprint,\narXiv:2406.12449.\nXiaoman Zhang, Julián N. Acosta, Josh Miller, Ouwen\nHuang, and Pranav Rajpurkar. 2025a. Rexgradient-\n160k: A large-scale publicly available dataset of\nchest radiographs with free-text reports. Preprint,\narXiv:2505.00228.\nYanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang,\nHuan Lin, Baosong Yang, Pengjun Xie, An Yang,\nDayiheng Liu, Junyang Lin, Fei Huang, and Jingren\nZhou. 2025b. Qwen3 embedding: Advancing text\nembedding and reranking through foundation models.\nPreprint, arXiv:2506.05176.\nWeibing Zheng, Laurah Turner, Jess Kropczynski, Mu-\nrat Ozer, Tri Nguyen, and Shane Halse. 2025. Llm-as-\na-fuzzy-judge: Fine-tuning large language models as\na clinical evaluation judge with fuzzy logic. Preprint,\narXiv:2506.11221.\nYunqi Zhu, Wen Tang, Huayu Yang, Jinghao Niu,\nLiyang Dou, Yifan Gu, Yuanyuan Wu, Wensheng\nZhang, Ying Sun, and Xuebing Yang. 2025. The po-\ntential of llms in medical education: Generating ques-\ntions and answers for qualification exams. Preprint,\narXiv:2410.23769.\n9\n"}, {"page": 10, "text": "Appendix Contents\nA LLM-as-a-Judge Evaluation\n11\nB\nDetailed Inter-Annotator Agreement\n12\nC MedTutor Dataset Samples and Public Dataset Information\n14\nC.1\nHighest-Scoring Case with Llama-3.3-70B-Instruct . . . . . . . . . . . . . . . . . . . .\n15\nC.2\nLowest-Scoring Case with Llama-3.3-70B-Instruct\n. . . . . . . . . . . . . . . . . . . .\n19\nC.3\nHighest-Scoring Case with MedGemma-27B-text-it . . . . . . . . . . . . . . . . . . . .\n22\nC.4\nLowest-Scoring Case with MedGemma-27B-text-it . . . . . . . . . . . . . . . . . . . .\n24\nD Default System Prompts for MedTutor\n27\nD.1\nKeyword Generation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nD.2 Textbook Summary Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nD.3\nMCQ Generation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.4\nEducational Material Generation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nE\nMedTutor System UI\n30\nF\nHuman Annotation Guideline\n32\nF.1\nRetrieved & Reranked Academic Papers . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nF.2\nGenerated Textbook Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nF.3\nExample Multiple Choice Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nG Human Annotator System UI\n33\n10\n"}, {"page": 11, "text": "A\nLLM-as-a-Judge Evaluation\nModel\nJudge\nTextbook\nSummary\nEducational\nMaterial\nMCQ\nAverage\nLlama 3.1-8B-Instruct\nMedGemma-27B\n3.64 (±0.95)\n3.49 (±1.10)\n3.69 (±1.05)\n3.61\nGPT-4.1-mini\n4.06 (±0.85)\n4.18 (±0.90)\n3.86 (±0.92)\n4.03\nGemini-2.5-Pro\n3.59 (±1.15)\n3.55 (±1.20)\n3.92 (±1.18)\n3.69\nGemini-2.5-Flash\n3.64 (±1.30)\n3.49 (±1.25)\n3.88 (±1.28)\n3.67\nAvg. (Judges)\n3.73\n3.68\n3.84\n3.75\nQwen3-8B\nMedGemma-27B\n3.39 (±1.01)\n4.01 (±0.95)\n3.78 (±0.88)\n3.73\nGPT-4.1-mini\n3.42 (±0.90)\n4.49 (±0.75)\n4.22 (±0.81)\n4.04\nGemini-2.5-Pro\n3.45 (±1.10)\n4.11 (±0.99)\n3.81 (±0.95)\n3.79\nGemini-2.5-Flash\n3.39 (±1.25)\n4.01 (±1.15)\n3.75 (±1.05)\n3.72\nAvg. (Judges)\n3.41\n4.16\n3.89\n3.82\nLlama-4-Scout-17B-16E-Instruct\nMedGemma-27B\n3.68 (±0.88)\n4.08 (±0.85)\n3.85 (±0.80)\n3.87\nGPT-4.1-mini\n4.30 (±0.70)\n4.28 (±0.65)\n4.18 (±0.72)\n4.25\nGemini-2.5-Pro\n3.71 (±0.95)\n4.15 (±0.90)\n4.01 (±0.88)\n3.96\nGemini-2.5-Flash\n3.68 (±1.10)\n4.08 (±1.05)\n3.95 (±1.00)\n3.90\nAvg. (Judges)\n3.84\n4.15\n4.00\n4.00\nQwen3-32B\nMedGemma-27B\n3.55 (±0.75)\n4.64 (±0.60)\n3.99 (±0.65)\n4.06\nGPT-4.1-mini\n3.99 (±0.65)\n4.19 (±0.50)\n4.48 (±0.55)\n4.22\nGemini-2.5-Pro\n3.61 (±0.80)\n4.70 (±0.70)\n4.25 (±0.75)\n4.19\nGemini-2.5-Flash\n3.55 (±1.20)\n4.64 (±1.10)\n4.18 (±1.00)\n4.12\nAvg. (Judges)\n3.68\n4.54\n4.23\n4.15\nLlama-3.3-70B-Instruct\nMedGemma-27B\n3.64 (±0.68)\n3.66 (±0.61)\n3.79 (±0.55)\n3.70\nGPT-4.1-mini\n4.34 (±0.72)\n4.50 (±0.55)\n4.19 (±0.60)\n4.34\nGemini-2.5-Pro\n3.95 (±1.23)\n4.28 (±0.38)\n4.14 (±0.55)\n4.12\nGemini-2.5-Flash\n2.82 (±1.45)\n3.58 (±1.44)\n4.08 (±1.46)\n3.49\nAvg. (Judges)\n3.69\n4.01\n4.05\n3.91\nMedGemma-27B\nMedGemma-27B\n3.65 (±0.88)\n4.09 (±0.61)\n4.22 (±0.60)\n3.99\nGPT-4.1-mini\n4.21 (±0.81)\n4.79 (±0.48)\n4.60 (±0.51)\n4.53\nGemini-2.5-Pro\n3.84 (±1.18)\n4.18 (±0.45)\n4.15 (±0.72)\n4.06\nGemini-2.5-Flash\n3.05 (±1.58)\n4.61 (±0.90)\n4.47 (±1.18)\n4.04\nAvg. (Judges)\n3.69\n4.42\n4.36\n4.16\nTable 3: Aggregated LLM-as-a-Judge evaluation results across all datasets, comparing different judges. The Avg.\n(Judges) row indicates the mean of scores across the judges. All scores are on a 1-5 scale (5=best). Llama-4-Scout-\n17B-16E-Instruct(Meta, 2025) was inferenced in FP8.\n11\n"}, {"page": 12, "text": "B\nDetailed Inter-Annotator Agreement\nModel Context\nEvaluation Metric\nKrippendorff’s\nAlpha (α)\nPairwise\nKappa (κ)\n% Exact\nAgreement\n% Within\n±1 Point\nCorrelation\n(r)\nUpstream\nKeyword Appropriateness\n0.335\n0.627\n41%\n80%\n0.709\nPaper Relevance\n0.474\n0.675\n59%\n95%\n0.778\nLlama3-70B\nTextbook Summary\n0.347\n0.555\n48%\n84%\n0.587\nEducational Material\n-0.228\n0.382\n50%\n94%\n0.325\nMCQ\n-0.159\n0.222\n29%\n81%\n0.375\nMedGemma-27B\nTextbook Summary\n0.627\n0.812\n66%\n96%\n0.721\nEducational Material\n0.354\n0.673\n72%\n100%\n0.589\nMCQ\n0.114\n0.629\n46%\n90%\n0.596\nTable 4: Detailed Inter-Annotator Agreement (IAA) between three radiologists across different evaluation tasks.\nKrippendorff’s Alpha (α) and Avg. Pairwise Kappa (κ) measure reliability, while agreement percentages and\nPearson correlation (r) provide further insight into rater consistency.\nOverall, the agreement scores suggest that MedGemma-27B’s outputs were evaluated more consistently\nby the radiologists than those from Llama3.3-70B. As shown in Table 4, MedGemma-27B’s Textbook\nSummary achieved the highest reliability, with a Krippendorff’s Alpha of 0.627, approaching the threshold\nfor acceptable agreement, and a substantial average pairwise Kappa of 0.812. The upstream task of Paper\nRelevance also demonstrated moderate to substantial agreement across most measures.\nConversely, the outputs from Llama3.3-70B, particularly for more subjective tasks like Educational\nMaterial and MCQ evaluation, yielded negative Alpha values, indicating systematic disagreement among\nthe raters (Figure 3). The evaluation of MCQs proved challenging for both models, though agreement was\nnotably higher for MedGemma-27B (Figure 4). These findings highlight that while structured summariza-\ntion tasks can achieve high inter-rater reliability, evaluating more complex, subjective-generative tasks\nmay require more detailed guidelines to ensure rater consistency.\nKeyword Appropriateness\nPaper Relevance\nEvaluation Tasks\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCohen's Kappa ( )\n0.48\n0.63\n0.68\n0.76\n0.72\n0.63\nInter-Annotator Agreement: Upstream Tasks\nFair Agreement ( =0.4)\nSubstantial Agreement ( =0.6)\nAlmost Perfect Agreement ( =0.8)\nAnnotator 1 vs 2\nAnnotator 1 vs 3\nAnnotator 2 vs 3\nFigure 2: Pairwise Cohen’s Kappa (κ) scores for Upstream Tasks. This figure shows the agreement between three\npairs of annotators for keyword appropriateness and paper relevance.\n12\n"}, {"page": 13, "text": "Educational Material\nMCQ\nTextbook Summary\nEvaluation Tasks\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCohen's Kappa ( )\n0.31\n0.29\n0.52\n0.32\n0.24\n0.40\n0.52\n0.14\n0.75\nInter-Annotator Agreement: Llama3.3-70B-Instruct\nFair Agreement ( =0.4)\nSubstantial Agreement ( =0.6)\nAlmost Perfect Agreement ( =0.8)\nAnnotator 1 vs 2\nAnnotator 1 vs 3\nAnnotator 2 vs 3\nFigure 3: Pairwise Cohen’s Kappa (κ) scores for Llama3.3-70B-Instruct Generated Content.\nEducational Material\nMCQ\nTextbook Summary\nEvaluation Tasks\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCohen's Kappa ( )\n0.94\n0.38\n0.72\n0.58\n0.69\n0.88\n0.50\n0.82\n0.84\nInter-Annotator Agreement: MedGemma-27B-text-it\nFair Agreement ( =0.4)\nSubstantial Agreement ( =0.6)\nAlmost Perfect Agreement ( =0.8)\nAnnotator 1 vs 2\nAnnotator 1 vs 3\nAnnotator 2 vs 3\nFigure 4: Pairwise Cohen’s Kappa (κ) scores for MedGemma-27B-text-it Generated Content.\n13\n"}, {"page": 14, "text": "C\nMedTutor Dataset Samples and Public Dataset Information\nThis appendix provides the best inter-annotator agreement examples of the highest and lowest-scoring\ncases from the 50 cases evaluated by three expert radiologists. These cases were sampled (10 from each\nof the 5 datasets; Yale Internal, MIMIC-IV-note, MIMIC-CXR, CheXpert-Plus, ReXGradient-160K) and\ngenerated by two different models: Llama-3.3-70B-Instruct and MedGemma-27B-text-it. Each example\nincludes the original clinical report and its corresponding generated report from MedTutor.\nAlso, we publicly release a large-scale dataset(total 144K) generated by our system. This includes\nreports from CheXpert-Plus, MIMIC-IV-note, and MIMIC-CXR (2,000 reports each), processed by six\ndifferent generator models and 4 evaluator models. Due to licensing and de-identification challenges, the\nYale-internal and ReXGradient datasets are not included in the public release.\nC.1 Highest-scoring case generated by Llama-3.3-70B-Instruct.\nC.2 Lowest-scoring case generated by Llama-3.3-70B-Instruct.\nC.3 Highest-scoring case generated by MedGemma-27B-text-it.\nC.4 Lowest-scoring case generated by MedGemma-27B-text-it.\n14\n"}, {"page": 15, "text": "C.1\nHighest-Scoring Case with Llama-3.3-70B-Instruct\nCase Information\nDataset: MIMIC-IV-note\nGenerator Model: Llama-3.3-70B-Instruct\nCase ID: 19287224-RR-6\nOriginal Radiology Report\nINDICATION: NO_PO contrast; History: () with abd pain NO_PO contrast// abd\npain r/o appendicitiis\nTECHNIQUE: Single phase split bolus contrast: MDCT axial images were acquired\nthrough the abdomen and pelvis following intravenous contrast administration\nwith split bolus technique.\nOral contrast was administered.\nCoronal and sagittal reformations were performed and reviewed on PACS.\nDOSE: Acquisition sequence:\n1) Stationary Acquisition 7.5 s, 0.5 cm; CTDIvol = 35.2 mGy (Body) DLP =\n17.6 mGy-cm.\n2) Spiral Acquisition 7.3 s, 55.8 cm; CTDIvol = 9.8 mGy (Body) DLP = 548.4\nmGy-cm.\nTotal DLP (Body) = 566 mGy-cm.\nCOMPARISON: None.\nFINDINGS:\nLOWER CHEST: Visualized lung fields are within normal limits. There is no\nevidence of pleural or pericardial effusion.\nABDOMEN:\nHEPATOBILIARY: The liver demonstrates homogenous attenuation throughout.\nThere is no evidence of focal lesions. There is no evidence of intrahepatic\nor extrahepatic biliary dilatation. The gallbladder is within normal limits.\nPANCREAS: The pancreas has normal attenuation throughout, without evidence of\nfocal lesions or pancreatic ductal dilatation. There is no peripancreatic\nstranding.\nSPLEEN: The spleen shows normal size and attenuation throughout, without\nevidence of focal lesions.\nADRENALS: The right and left adrenal glands are normal in size and shape.\nURINARY: The kidneys are of normal and symmetric size with normal nephrogram.\nThere is no evidence of focal renal lesions or hydronephrosis. There is no\nperinephric abnormality.\nGASTROINTESTINAL: The stomach is unremarkable. Small bowel loops demonstrate\nnormal caliber, wall thickness, and enhancement throughout. There are\nscattered colonic diverticula. The appendix is fluid-filled and dilated with\nwall hyperemia. A measures 13 mm with periappendiceal stranding and a small\nappendicular at the base. There is no periappendiceal abscess.\nPELVIS: The urinary bladder and distal ureters are unremarkable. There is a\nsmall amount of free fluid in the pelvis.\nREPRODUCTIVE ORGANS: The visualized reproductive organs are unremarkable.\nLYMPH NODES: There is no retroperitoneal or mesenteric lymphadenopathy. There\nis no pelvic or inguinal lymphadenopathy.\nVASCULAR: There is no abdominal aortic aneurysm. No significant\natherosclerotic disease is noted.\n15\n"}, {"page": 16, "text": "BONES: There is no evidence of worrisome osseous lesions or acute fracture.\nModerate multilevel degenerative changes noted of the lumbar spine.\nSOFT TISSUES: The abdominal and pelvic wall is within normal limits.\nIMPRESSION:\n1. Acute appendicitis.\n2. Colonic diverticulosis without diverticulitis.\nNOTIFICATION: Findings wet read to the electronic dashboard and discussed\nwith ()\nGenerated Educational Materials\nKeywords: Acute appendicitis, Colonic diverticulosis\nRetrieved Paper 1: Retrospective case-control study to predict a potential underlying appen-\ndiceal tumor in an acute appendicitis context based on a CT-scoring system.\nOBJECTIVES: To assess CT signs to discriminate an appendiceal tumor versus a non-tumoral\nappendix in an acute appendicitis context.\nMETHODS: A 10-year bicentric retrospective case-control study was performed in adults. Patients\nwith a histopathological appendiceal tumor and appendicitis were paired for age and sex with\npatients with non-tumorous appendicitis (1/3 ratio, respectively). Two senior radiologists blindly\nanalyzed numerous CT findings... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/33454458/ | Source: PubMed\nRetrieved Paper 2: Dual energy CT in acute appendicitis: value of low mono-energy.\nOBJECTIVES: To assess the potential role of low monoenergetic images in the evaluation of acute\nappendicitis.\nMETHODS: A retrospective study of 42 patients with pathology proven acute appendicitis un-\nderwent contrast-enhanced-CT conducted on a single-source-DECT before surgery. Attenuation,\nSNR, and CNR were calculated on both monoenergetic and conventional images and compared to\n24 abdominal CT-scans with normal appendix. Representative... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/33992882/ | Source: PubMed\nRetrieved Paper 3: Association between colorectal cancer and colonic diverticulosis: case-\ncontrol study based on computed tomographic colonography.\nBACKGROUND: The association between colorectal cancer (CRC) and diverticulosis is uncertain,\nand there is little data reported in Asia. This study examined the association of both diseases based\non the computed tomographic colonography (CTC).\nMETHODS: This study was designed as a case-control study. Korean subjects undergoing CTC\nbetween April 2002 to April 2008 in Samsung Medical Center were eligible. Patients with\nhistologically proven... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/21516446/ | Source: PubMed\nRetrieved Paper 4: REVISED CONCEPTS ON DIVERTICULAR DISEASE OF THE\nCOLON. II. SO-CALLED DIVERTICULITIS: DIVERTICULAR SIGMOIDITIS AND\nPERISIGMOIDITIS; DIVERTICULAR ABSCESS, FISTULA, AND FRANK PERITONI-\nTIS.\nNo Abstract\nURL: https://pubmed.ncbi.nlm.nih.gov/14272097/ | Source: PubMed\n16\n"}, {"page": 17, "text": "Textbook Summary 1\nAcute appendicitis is inflammation typically caused by luminal obstruction (fecolith, lymphoid\nhyperplasia) leading to ischemia. Radiologic findings on plain films are infrequent (<50%) but\nmay include an appendicolith, cecal wall thickening, or signs of perforation like extraluminal\ngas or loss of fat planes. CT is highly accurate, showing a distended appendix (>7mm), wall\nthickening, periappendicular fat stranding, and potentially an appendicolith or complications like\nperforation. Nonvisualization of the appendix on CT in the setting of right lower quadrant pain\nmakes appendicitis highly unlikely.\nTextbook Summary 2\nColonic diverticulosis involves acquired herniations of the colonic mucosa and submucosa through\nthe muscularis propria, typically appearing as thin-walled, air-filled outpouchings on imaging.\nRadiographically, these diverticula are often seen as rounded structures, sometimes with a bubbly\nappearance, and may show residual contrast from prior studies. Ultrasound can show thin-\nwalled outpouchings with echogenic material and potential shadowing, while CT demonstrates\nrounded outpouchings containing air or contrast, often associated with circumferential colonic\nwall thickening due to muscular hypertrophy.\nAssessment Question 1\nAccording to the provided textbook summary, which CT finding is highly suggestive of acute\nappendicitis?\n• A. Normal appendix diameter\n• B. Appendix diameter > 7mm\n• C. Absence of periappendicular fat stranding\n• D. Nonvisualization of the appendix\nAnswer: B\nExplanation: The textbook summary states that CT findings suggestive of acute appendicitis\ninclude a distended appendix (>7mm).\nAssessment Question 2\nBased on the provided abstracts, which CT finding is a positive predictive factor for an underlying\nappendiceal neoplasm in the context of acute appendicitis?\n• A. Appendicolith\n• B. Fat stranding\n• C. Mural calcifications\n• D. Appendix diameter < 15mm\nAnswer: C\nExplanation: The abstract from the study on appendiceal tumors identifies mural calcifications as\na positive predictive factor (OR = 47, p = 0.0001) for an underlying tumor in patients presenting\nwith acute appendicitis.\n17\n"}, {"page": 18, "text": "Assessment Question 3\nWhat is the characteristic radiographic appearance of colonic diverticula as described in the\ntextbook summary?\n• A. Thick-walled outpouchings filled with fluid\n• B. Thin-walled outpouchings, often rounded, containing air or contrast\n• C. Irregular thickening of the colonic wall\n• D. Absence of air within the colon\nAnswer: B\nExplanation: The textbook summary describes colonic diverticula as acquired herniations appear-\ning as thin-walled, air-filled outpouchings on imaging, often rounded and potentially containing\ncontrast.\nAssessment Question 4\nAccording to the provided abstract analyzing the association between colorectal cancer and\ndiverticulosis, what was the main conclusion regarding this association?\n• A. Colorectal cancer is strongly associated with diverticulosis.\n• B. Diverticulosis is more common in patients with right-sided colon cancer.\n• C. Colorectal cancer is not associated with diverticulosis.\n• D. Computed tomographic colonography is not useful for assessing diverticulosis.\nAnswer: C\nExplanation: The abstract from the case-control study using CT colonography concluded that\ncolorectal cancer is not associated with diverticulosis, as the distribution of diverticulosis was\nsimilar between the cancer and control groups.\nFinal Educational Material\nRadiology Feedback\n# Acute Appendicitis\n• Imaging Findings: CT is the preferred modality. Key findings include an appendix diameter >7\nmm, wall thickening, periappendiceal fat stranding, and potentially an appendicolith.\n• Differential Diagnosis: Nonvisualization of the appendix on CT in the setting of right lower\nquadrant pain makes appendicitis highly unlikely.\n• Advanced Techniques: Low monoenergetic CT images can enhance visualization of inflamed\nbowel wall and mucosal enhancement, potentially improving diagnostic accuracy.\n• Clinical Correlation: While appendicitis is often caused by obstruction, CT can help identify\npotential underlying neoplasms, especially in atypical presentations. Features suggesting neoplasm\ninclude mural calcifications, an appendix mass, focal asymmetric wall abnormality, or diameter ≥\n15 mm.\n# Colonic Diverticulosis\n• Imaging Findings: Diverticula appear as thin-walled, air-filled or contrast-filled outpouchings\nfrom the colon, typically rounded. CT demonstrates these outpouchings, often associated with\ncircumferential colonic wall thickening due to muscular hypertrophy.\n• Clinical Correlation: Diverticulosis itself is the presence of these outpouchings and is generally\nasymptomatic. It is not directly associated with an increased risk of colorectal cancer.\n18\n"}, {"page": 19, "text": "C.2\nLowest-Scoring Case with Llama-3.3-70B-Instruct\nCase Information\nDataset: RexGradient-160K\nGenerator Model: Llama-3.3-70B-Instruct\nCase\nID:\npGRDNRZB2HZ56F0ZL_aGRDNZSUP68GUCQN2_s1.2.826.0.1.3680043.8.498.\n16841148712021794680399240737673267273\nOriginal Radiology Report\nIndication: Fall yesterday. Fever this morning.\nFindings: 4228 hours. The heart size and mediastinal contours are stable. There is stable mild\nsubsegmental atelectasis at both lung bases. No confluent airspace opacity, pleural effusion or\npneumothorax. Fracture of the proximal right humerus again noted.\nImpression: Stable mild bibasilar atelectasis. No acute cardiopulmonary process.\nGenerated Educational Materials\nKeywords: atelectasis, fracture\nRetrieved Paper 1: Debunking a mythology: Atelectasis is not a cause of postoperative fever.\nMost physicians appreciate that practicing medicine is a commitment to continuous learning.\nHowever, \"learning\" can be mistakenly understood as simply the acquisition of facts and new\nknowledge. But learning also necessitates the constant re-examination and challenging of one’s\nexisting body of knowledge, as misinformation persists when one’s beliefs are not challenged or\nquestioned in the light of new information. One example is the pervasive... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/39566396/ | Source: PubMed\nRetrieved Paper 2: Use of artificial intelligence in triaging of chest radiographs to reduce\nradiologists’ workload.\nOBJECTIVES: To evaluate whether deep learning-based detection algorithms (DLD)-based triag-\ning can reduce outpatient chest radiograph interpretation workload while maintaining noninferior\nsensitivity.\nMETHODS: This retrospective study included patients who underwent initial chest radiography\nat the outpatient clinic between June 1 and June 30, 2017. Readers interpreted radiographs\nwith/without a commercially available DLD that detects nine... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/37615766/ | Source: PubMed\nRetrieved Paper 3: Assessment of proximal tibial fractures with 3D FRACTURE (fast field\necho resembling a CT using restricted echo-spacing) MRI-intra-individual comparison with\nCT.\nOBJECTIVES: To evaluate the feasibility and diagnostic performance of a 3D FRACTURE (fast\nfield echo resembling a CT using restricted echo-spacing) MRI sequence for the detection and\nclassification of proximal tibial fractures compared with CT.\nMETHODS: We retrospectively included 126 patients (85 male; 39.6±14.5 years) from two centers\nfollowing acute knee injury. Patients underwent knee MRI at 3T including FRACTURE-MRI.\nAdditional CT was... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/40126605/ | Source: PubMed\n19\n"}, {"page": 20, "text": "Retrieved Paper 4: How I Do It: Evaluating Cardiac Implantable Devices and Noncardiac\nMimics on Chest Radiographs.\nCardiac implantable electronic devices (CIEDs), including pacemakers and defibrillators, are\nincreasingly used to manage various cardiac conditions. This article reviews the radiographic\nappearance, typical components, and placement of CIEDs, including newer technologies like\nleadless pacemakers and MRI-conditional devices. The article also highlights the imaging findings\nof common complications such as lead dislodgement, fracture, and... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/40358448/ | Source: PubMed\nTextbook Summary 1\nAtelectasis, or lung collapse, presents radiologically with increased lung density, vessel crowding,\nand potential fissure/mediastinal displacement. Obstructive atelectasis involves air resorption distal\nto a blockage (tumor, mucus plug, foreign body), while nonobstructive atelectasis retains some\nair. Other forms include passive (pleural effusion/pneumothorax), adhesive (surfactant deficiency),\ncicatrizing (fibrosis), and discoid/rounded atelectasis (often related to pleural inflammation or\nobstruction). Specific patterns, like the \"Luftsichel\" sign, can indicate left upper lobe collapse.\nTextbook Summary 2\nEnteropathy-associated T-cell lymphoma, a type of non-Hodgkin lymphoma, often presents with\nbowel wall thickening, ulceration, or strictures, particularly in the proximal small bowel. Radiologic\nfindings may include circumferential wall thickening, mesenteric fat infiltration, and nonbulky\nlymphadenopathy, with a high frequency of FDG uptake on PET scans. Complications like bowel\nperforation are common, especially in Type II lymphoma, and differentiating it from large B-cell\nlymphoma or refractory celiac disease is crucial.\nAssessment Question 1\nAccording to the provided abstract debunking a mythology, what is the common misconception\nregarding atelectasis?\n• A. Atelectasis is a common cause of postoperative consolidation.\n• B. Atelectasis is a common cause of postoperative fever.\n• C. Atelectasis is difficult to differentiate from pneumonia on imaging.\n• D. Atelectasis is primarily caused by pleural effusion.\nAnswer: B\nExplanation: The abstract explicitly states that the pervasive belief that postoperative atelectasis\ncauses fever is a myth, despite ample evidence to the contrary.\nAssessment Question 2\nWhich of the following is NOT a type of atelectasis mentioned in the textbook summary?\n• A. Obstructive atelectasis\n• B. Passive atelectasis\n• C. Adhesive atelectasis\n• D. Vascular atelectasis\nAnswer: D\nExplanation: The textbook summary lists obstructive, nonobstructive, passive, adhesive, cicatriz-\ning, and discoid/rounded atelectasis, but not vascular atelectasis.\n20\n"}, {"page": 21, "text": "Assessment Question 3\nAccording to the abstract comparing FRACTURE-MRI and CT for proximal tibial fractures, what\nwas the sensitivity of FRACTURE-MRI for detecting fractures?\n• A. 78.2\n• B. 85.7\n• C. 91.5\n• D. 97.1\nAnswer: C\nExplanation: The abstract states that fractures were detected by FRACTURE-MRI with a sensi-\ntivity of 91.5\nAssessment Question 4\nThe abstract on evaluating cardiac implantable devices mentions which potential complication\nrelated to device leads?\n• A. Dislodgement\n• B. Calcification\n• C. Consolidation\n• D. Cardiomegaly\nAnswer: A\nExplanation: The abstract explicitly lists lead dislodgement, fracture, and perforation as common\ncomplications of cardiac implantable electronic devices.\nFinal Educational Material\nOkay, here is the synthesized feedback focusing on atelectasis and fracture, based on the provided\ninformation:\n# Atelectasis\n• Clinical Context: Atelectasis is commonly seen postoperatively and in the context of fever, but it\nis not a direct cause of fever itself. This is a persistent misconception.\n• Imaging Appearance: Radiographically, atelectasis appears as increased lung density, often with\ncrowding of pulmonary vessels. It can be subsegmental, as seen in this case.\n• Differentiation: It’s important to differentiate atelectasis from consolidation (infection), which\ntypically presents with air bronchograms and may have a more lobar distribution.\n• AI Application: AI-based triaging tools can help identify atelectasis on chest radiographs,\npotentially reducing radiologist workload while maintaining diagnostic sensitivity.\n# Fracture\n• Imaging Modalities: While CT is the standard for fracture evaluation, specialized MRI sequences\nlike FRACTURE-MRI are being investigated as alternatives, potentially reducing radiation expo-\nsure and costs, especially when MRI is already indicated.\n• MRI Performance: Studies show FRACTURE-MRI has high sensitivity and specificity for\ndetecting and classifying fractures, comparable to CT in certain locations like the proximal tibia.\n• Complications: Imaging is crucial for identifying complications of implanted devices, such as\nlead fracture in pacemakers/defibrillators, which can be visualized on chest radiographs.\n21\n"}, {"page": 22, "text": "C.3\nHighest-Scoring Case with MedGemma-27B-text-it\nCase Information\nDataset: CheXpert-Plus\nGenerator Model: MedGemma-27B-text-it\nCase ID: 23803\nOriginal Radiology Report\nFindings: Two views of the chest demonstrate reticular opacities\nbilaterally, right greater than left, with a basilar predominance,\nlikely secondary to underlying fibrotic lung disease. There is no\nevidence of pulmonary edema or pleural effusion. There is\ncardiomegaly. Left anterior chest wall cardiac pacer appears\nunchanged in position, with two ventricular leads.\nImpression:\n1. FINDINGS CONSISTENT WITH FIBROTIC LUNG DISEASE AS DESCRIBED\nABOVE.\n2. NO EVIDENCE OF PULMONARY EDEMA.\n3. STABLE CARDIOMEGALY WITH STABLE ARRANGEMENT OF PACER LEADS.\nGenerated Educational Materials\nKeywords: Fibrotic Lung Disease\nRetrieved Paper 1: Collaborative radiologic and histopathologic assessment of fibrotic lung\ndisease.\nThe idiopathic interstitial pneumonias (IIPs) are a seemingly disconnected collection of diseases\nusually associated with the presence of pulmonary fibrosis. Categorization of the IIPs continues\nto be problematic despite recent attempts to refine the diagnostic criteria and suggests that rather\nthan separate diseases, these pneumonias represent a spectrum of injury and abnormal repair of the\nalveolar wall. Although the initiating injury or... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/20501710/ | Source: PubMed\nRetrieved Paper 2: From the radiologic pathology archives: organization and fibrosis as\na response to lung injury in diffuse alveolar damage, organizing pneumonia, and acute\nfibrinous and organizing pneumonia.\nOrganization, characterized by fibroblast proliferation, is a common and nearly universal response\nto lung injury whether it is focal or diffuse. Despite the vast range of injurious agents, the\nlung’s response to injury is quite limited, with a similar pattern of reaction seen radiologically\nand histologically regardless of the underlying cause. Although there is a tendency to divide\norganization into distinct entities, the underlying injury to... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/24224590/ | Source: PubMed\nTextbook Summary 1\nIn fibrotic lung disease, key radiologic findings include a reticulonodular pattern, ground-glass\nopacities, and honeycombing, with a basilar and subpleural distribution. Diagnostic considerations\nfor fibrotic lung disease, such as usual interstitial pneumonia (UIP), involve identifying a combi-\nnation of clinical and imaging features, including progressive dyspnea, clubbing, and bibasilar\ncrackles, along with characteristic HRCT findings like traction bronchiectasis and macrocystic\nhoneycombing. HRCT plays a crucial role in differentiating UIP from other idiopathic interstitial\npneumonias and guiding biopsy sites for definitive diagnosis.\n22\n"}, {"page": 23, "text": "Assessment Question 1\nWhat is a characteristic radiologic finding in fibrotic lung disease?\n• A. Pulmonary edema\n• B. Reticulonodular pattern\n• C. Cardiomegaly\n• D. Pleural effusion\nAnswer: B\nExplanation: Fibrotic lung disease is characterized by specific radiologic findings, including a\nreticulonodular pattern, which is a key feature of this condition.\nAssessment Question 2\nWhat is the typical distribution of radiologic findings in fibrotic lung disease?\n• A. Apical and central\n• B. Basilar and subpleural\n• C. Upper lobe and peripheral\n• D. Diffuse and homogeneous\nAnswer: B\nExplanation: Fibrotic lung disease typically shows a basilar and subpleural distribution of\nradiologic findings, which is an important characteristic for diagnosis.\nFinal Educational Material\nFibrotic Lung Disease\nFibrotic lung disease is characterized by key radiologic findings including a reticulonodular pattern,\nground-glass opacities, and honeycombing, typically with a basilar and subpleural distribution.\nThe diagnosis often involves identifying a combination of clinical features such as progressive\ndyspnea, clubbing, and bibasilar crackles, along with characteristic HRCT findings like traction\nbronchiectasis and macrocystic honeycombing. High-resolution computed tomography (HRCT)\nplays a crucial role in differentiating usual interstitial pneumonia (UIP) from other idiopathic\ninterstitial pneumonias and in guiding biopsy sites for a definitive diagnosis. A collaborative\ndiagnostic approach combining radiologic and histopathologic assessments can lead to a more\nreliable identification of the predominant pathways leading to pulmonary fibrosis, which has\nimplications for therapy and future research directions.\n23\n"}, {"page": 24, "text": "C.4\nLowest-Scoring Case with MedGemma-27B-text-it\nCase Information\nDataset: RexGradient-160K\nGenerator Model: MedGemma-27B-text-it\nCase\nID:\npGRDNRZB2HZ56F0ZL_aGRDNZSUP68GUCQN2_s1.2.826.0.1.3680043.8.498.\n16841148712021794680399240737673267273\nOriginal Radiology Report\nIndication: Fall yesterday. Fever this morning.\nFindings: 4228 hours. The heart size and mediastinal contours are stable. There is stable mild\nsubsegmental atelectasis at both lung bases. No confluent airspace opacity, pleural effusion or\npneumothorax. Fracture of the proximal right humerus again noted.\nImpression: Stable mild bibasilar atelectasis. No acute cardiopulmonary process.\nGenerated Educational Materials\nKeywords: atelectasis, fracture\nRetrieved Paper 1: Debunking a mythology: Atelectasis is not a cause of postoperative fever.\nMost physicians appreciate that practicing medicine is a commitment to continuous learning.\nHowever, \"learning\" can be mistakenly understood as simply the acquisition of facts and new\nknowledge. But learning also necessitates the constant re-examination and challenging of one’s\nexisting body of knowledge, as misinformation persists when one’s beliefs are not challenged or\nquestioned in the light of new information. One example is the pervasive... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/39566396/ | Source: PubMed\nRetrieved Paper 2: Use of artificial intelligence in triaging of chest radiographs to reduce\nradiologists’ workload.\nOBJECTIVES: To evaluate whether deep learning-based detection algorithms (DLD)-based triag-\ning can reduce outpatient chest radiograph interpretation workload while maintaining noninferior\nsensitivity.\nMETHODS: This retrospective study included patients who underwent initial chest radiography\nat the outpatient clinic between June 1 and June 30, 2017. Readers interpreted radiographs\nwith/without a commercially available DLD that detects nine... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/37615766/ | Source: PubMed\nRetrieved Paper 3: Assessment of proximal tibial fractures with 3D FRACTURE (fast field\necho resembling a CT using restricted echo-spacing) MRI-intra-individual comparison with\nCT.\nOBJECTIVES: To evaluate the feasibility and diagnostic performance of a 3D FRACTURE (fast\nfield echo resembling a CT using restricted echo-spacing) MRI sequence for the detection and\nclassification of proximal tibial fractures compared with CT.\nMETHODS: We retrospectively included 126 patients (85 male; 39.6±14.5 years) from two centers\nfollowing acute knee injury. Patients underwent knee MRI at 3T including FRACTURE-MRI.\nAdditional CT was... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/40126605/ | Source: PubMed\n24\n"}, {"page": 25, "text": "Retrieved Paper 4: How I Do It: Evaluating Cardiac Implantable Devices and Noncardiac\nMimics on Chest Radiographs.\nCardiac implantable electronic devices (CIEDs), including pacemakers and defibrillators, are\nincreasingly used to manage various cardiac conditions. This article reviews the radiographic\nappearance, typical components, and placement of CIEDs, including newer technologies like\nleadless pacemakers and MRI-conditional devices. The article also highlights the imaging findings\nof common complications such as lead dislodgement, fracture, and... (Omitted)\nURL: https://pubmed.ncbi.nlm.nih.gov/40358448/ | Source: PubMed\nTextbook Summary 1\nAtelectasis is characterized by radiologic findings of increased lung density, crowding of pul-\nmonary vessels, and displacement of fissures, hilum, and mediastinum, often with elevation of\nthe hemidiaphragm. On MRI, obstructive atelectasis shows high signal intensity on T2WI, while\nnonobstructive atelectasis shows low signal intensity. The diagnosis of atelectasis considers various\ncauses, including obstructive, nonobstructive, passive, adhesive, and cicatrizing types, each with\ndistinct pathophysiology and imaging features.\nTextbook Summary 2\nFracture is not a primary focus in the provided textbook pages, which mainly discuss lymphoma\nand malignant melanoma, but radiologic findings related to fractures are not mentioned. However,\nin the context of lymphoma, particularly enteropathy-associated T-cell lymphoma, complications\nsuch as bowel perforation can occur, which may be associated with a fracture of the bowel\nwall. Diagnostic considerations for lymphoma include imaging findings like circumferential wall\nthickening on CT and FDG uptake on PET.\nAssessment Question 1\nWhat is a characteristic radiologic finding of atelectasis?\n• A. Decreased lung density\n• B. Increased lung density\n• C. Dilatation of pulmonary vessels\n• D. Elevation of the diaphragm without crowding of pulmonary vessels\nAnswer: B\nExplanation: Atelectasis is characterized by radiologic findings of increased lung density, crowd-\ning of pulmonary vessels, and displacement of fissures, hilum, and mediastinum.\nAssessment Question 2\nWhich of the following statements about postoperative atelectasis is supported by the provided\ncontext?\n• A. Atelectasis is a common cause of postoperative fever.\n• B. Atelectasis is rarely seen in postoperative patients.\n• C. The belief that atelectasis causes postoperative fever is a pervasive myth.\n• D. Atelectasis always leads to significant respiratory complications.\nAnswer: C\nExplanation: The context mentions that the belief that postoperative atelectasis causes fever is\na myth, despite ample evidence to the contrary, as discussed in the abstract titled \"Debunking a\nmythology: Atelectasis is not a cause of postoperative fever.\"\n25\n"}, {"page": 26, "text": "Assessment Question 3\nWhat is the sensitivity of FRACTURE-MRI in detecting fractures, as mentioned in the provided\ncontext?\n• A. 70\n• B. 80\n• C. 91.5\n• D. 95\nAnswer: C\nExplanation: The context states that fractures were detected by FRACTURE-MRI with a sensitiv-\nity of 91.5\nAssessment Question 4\nWhat is a potential advantage of using FRACTURE-MRI for diagnosing proximal tibial fractures,\nas mentioned in the context?\n• A. Higher radiation exposure\n• B. Reduced accuracy in fracture classification\n• C. Reduced costs and radiation exposure\n• D. Increased need for CT scans\nAnswer: C\nExplanation: The context suggests that FRACTURE-MRI might be a helpful alternative to\ncomputed tomography in an acute trauma setting by reducing costs and radiation exposure in\npatients requiring a preoperative MRI anyway.\nFinal Educational Material\nAtelectasis\nAtelectasis is characterized by increased lung density, crowding of pulmonary vessels, and dis-\nplacement of fissures, hilum, and mediastinum, often with elevation of the hemidiaphragm. It’s\nessential to differentiate atelectasis from consolidation, as their imaging characteristics and clinical\nimplications differ. Atelectasis can be caused by various factors, including obstructive, nonobstruc-\ntive, passive, adhesive, and cicatrizing types. Notably, atelectasis is not a cause of postoperative\nfever, contrary to a common myth. The use of artificial intelligence in triaging chest radiographs\ncan help reduce the workload of radiologists while maintaining sensitivity for detecting atelectasis\nand other radiologic findings.\nFracture\nFracture diagnosis and classification can be accurately performed using 3D FRACTURE (fast\nfield echo resembling a CT using restricted echo-spacing) MRI sequences, which offer a feasible\nalternative to CT scans in acute trauma settings. This can help reduce radiation exposure and\ncosts, especially in patients requiring preoperative MRI. When evaluating fractures, particularly\nin the context of cardiac implantable devices, it’s crucial to identify potential complications such\nas lead dislodgement, fracture, and perforation. Accurate identification of these devices and\ntheir positioning on chest radiographs is vital for early detection of complications and directing\nappropriate patient care.\n26\n"}, {"page": 27, "text": "D\nDefault System Prompts for MedTutor\nD.1\nKeyword Generation Prompt\nKeyword Generation Prompt\nSystem Prompt:\nYou are an expert medical language model. Given the full radiology report\nand the extracted Impression section, extract all specific disease names,\ndiagnostic labels, and named pathological entities mentioned or implied in\neither section. Focus only on established or suspected diagnoses, such as\nnamed conditions.\nOnly include diagnoses that are positively identified or suspected in the\nreport. Do not include any conditions that are explicitly ruled out,\nnegated, or stated as absent.\nDo not include general phrases, symptoms, or clinical findings that are not\nformal diagnoses.\nOutput your answer as a valid JSON object with the following format:\n{ \"keywords\": [\"diagnosis 1\", \"diagnosis 2\", \"diagnosis 3\"] }\nIf no diagnoses are present, return:\n{\n\"keywords\": []\n}\nUser Instruction Template:\nFinal_report: {full_report_text}\nImpression: {impression_text}\nD.2\nTextbook Summary Prompt\nTextbook Summary Prompt\nSystem Prompt:\nYou are a concise and accurate radiology assistant, skilled in summarizing\nmedical texts.\nUser Instruction Template:\nPlease summarize the following textbook pages focusing on the keyword '{keyword}'.\nThe summary should highlight key radiologic findings and diagnostic\nconsiderations. Be concise, using 2-3 sentences and your own words.\nOutput only the summary text itself, with no additional conversational\ntext or headers.\nTextbook Pages Content:\n{pages_block_text}\n27\n"}, {"page": 28, "text": "D.3\nMCQ Generation Prompt\nMultiple Choice Q&A Generation Prompt\nSystem Prompt:\nYou are a specialized AI assistant for creating multiple-choice questions (MCQs)\nfor radiology education. You must focus *exclusively* on the provided\n**Primary Diagnostic Keywords**.\nUser Instruction Template:\n### Primary Diagnostic Keywords to Focus On:\n- {keywords_list_str}\n### Full Context (for reference)\n{mcq_input_context}\n### Your Task\nBased *only* on the provided context, generate 2 multiple-choice questions\n**for each Primary Diagnostic Keyword listed above**. Do not generate questions\nfor any other terms or topics mentioned in the context. Each question must\ntest understanding of the information related to the primary keywords.\nFollow this format exactly:\n### Multiple Choice Questions\n#### {{Diagnosis Keyword 1}}\nQ1. {{Question stem}}\nA. {{Option A}}\nB. {{Option B}}\nC. {{Option C}}\nD. {{Option D}}\nAnswer: {{Correct Option Letter}}\nExplanation: {{Brief explanation based on the provided context.}}\nD.4\nEducational Material Generation Prompt\nEducational Material Prompt\nSystem Prompt:\nYou are an expert radiology AI assistant. Your task is to synthesize the\nprovided information into concise, educational feedback focused *only* on the\nprimary diagnostic keywords provided. Do not explain or elaborate on other\nterms from the original report unless they are directly relevant to the\nprimary keywords.\nUser Instruction Template:\n### Primary Diagnostic Keywords\n- {keywords_list_str}\n### Original Reviewer Report (for context only)\n28\n"}, {"page": 29, "text": "{original_reviewer_report}\n### Supporting Educational Material\n{user_block_for_final_stages}\n### Your Task\nBased on all the information above, provide a concise, synthesized feedback.\nStructure your response with a section for each **Primary Diagnostic Keyword**.\nFocus only on clinical teaching points and imaging pearls related to these\nprimary keywords.\n29\n"}, {"page": 30, "text": "E\nMedTutor System UI\n(a) Main user interface of MedTutor.\n(b) Configuration settings for model selection and system prompts.\nFigure 5: The MedTutor UI (Part 1 of 2): Main dashboard and initial configuration settings.\n30\n"}, {"page": 31, "text": "(a) Further configuration for data sources and retrieval.\n(b) Finalizing configuration and execution options.\nFigure 6: The MedTutor UI (Part 2 of 2): Additional configuration panels for data processing and task execution.\n31\n"}, {"page": 32, "text": "F\nHuman Annotation Guideline\nI. Evaluation of Information Quality per Keyword\nFor each diagnostic keyword identified from the original report, we evaluate the following compo-\nnents:\nF.1\nRetrieved & Reranked Academic Papers\n• Relevance to Keyword & Original Report: How directly related is each paper or retrieved\nsnippet to the given keyword and the context of the original radiology report?\nF.2\nGenerated Textbook Summary\n• Accuracy & Factuality: Is the summary an accurate and factual representation of information\nrelated to the keyword (compared to general radiology knowledge or, if available, the source\ntextbook)?\n• Helpfulness & Relevance: Is the summary helpful and related to the case report provided as\ninput?\n• Coverage of Key Information: Does the summary include the most critical information (e.g.,\nkey imaging findings, diagnostic criteria) related to the keyword?\nF.3\nExample Multiple Choice Questions\n• Relevance & Correctness: Are the questions relevant to the keyword? Is the answer provided\nand rationale correct? Are the answer choices relevant?\nII. Evaluation of the \"Educational material\" Paragraph (per Keyword)\n• Clinical & Educational Utility: How clinically relevant, accurate, and educationally valuable\nis this educational material paragraph for a radiology trainee in understanding the keyword\nwithin the context of the original report? (This encompasses quality, clinical insight, contextual\nappropriateness, and trustworthiness.)\nIII. Evaluation of Overall Educational Material Structure & Quality\nFor the entire generated report:\n• Appropriateness of Keywords: Are the keywords (used to structure the feedback) appropriate\nand comprehensive for the given original radiology report? Specifically, is the keyword general\nenough that it can be searched in a textbook or Radiopaedia (e.g., “rib fracture”, not “anterior 4th\nrib fracture”), and related to a pathology worth learning more about (e.g., “cholangiocarcinoma”,\nnot “mass”)?\n32\n"}, {"page": 33, "text": "G\nHuman Annotator System UI\n(a) Keyword Evaluation Page\n(b) Paper Evaluation Page\nFigure 7: Annotation system UI (Part 1 of 3): Interfaces for evaluating keywords and retrieved papers.\n33\n"}, {"page": 34, "text": "(a) Textbook Summary Evaluation Page\n(b) MCQ Evaluation Page\nFigure 8: Annotation system UI (Part 2 of 3): Interfaces for evaluating textbook summaries and multiple-choice\nquestions.\n34\n"}, {"page": 35, "text": "(a) Educational Material Evaluation Page\n(b) Overall Quality Evaluation Page\nFigure 9: Annotation system UI (Part 3 of 3): Interfaces for evaluating the final synthesized educational material\nand overall quality.\n35\n"}]}