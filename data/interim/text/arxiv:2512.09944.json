{"doc_id": "arxiv:2512.09944", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.09944.pdf", "meta": {"doc_id": "arxiv:2512.09944", "source": "arxiv", "arxiv_id": "2512.09944", "title": "Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting", "authors": ["Moein Heidari", "Mohammad Amin Roohi", "Ilker Hacihaliloglu"], "published": "2025-12-06T23:27:54Z", "updated": "2025-12-15T19:58:48Z", "summary": "Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.09944v2", "url_pdf": "https://arxiv.org/pdf/2512.09944.pdf", "meta_path": "data/raw/arxiv/meta/2512.09944.json", "sha256": "e95f23b3877de684efffec8fba5a6a6d895af303542ee51f784a5011440a6b15", "status": "ok", "fetched_at": "2026-02-18T02:25:04.916104+00:00"}, "pages": [{"page": 1, "text": "Echo-CoPilot: A Multi-View, Multi-Task Agent\nfor Echocardiography Interpretation and\nReporting\nMoein Heidari1, Mohammad Amin Roohi2, and Ilker Hacihaliloglu3,4\n1 School of Biomedical Engineering, University of British Columbia, Vancouver, BC,\nCanada\n2 Department of Electrical and Computer Engineering, University of British\nColumbia, Vancouver, BC, Canada\n3 Department of Radiology, University of British Columbia, Vancouver, BC, Canada\n4 Department of Medicine, University of British Columbia, Vancouver, BC, Canada\nmoein.heidari@ubc.ca\nAbstract. Echocardiography is central to contemporary cardiovascu-\nlar care, but full-study interpretation remains a cognitively demanding,\nmulti-view task that is still performed manually. While recent foundation\nmodels for echocardiography can achieve strong performance on individ-\nual perceptual subtasks such as view classification, segmentation, or dis-\nease prediction, they typically operate in isolation and do not provide a\nunified, clinically coherent assessment. In this work, we introduce Echo-\nCoPilot, a multi-view, multi-task agent that uses a large language model\nto orchestrate a suite of specialized echocardiography tools. Within a\nReAct-style loop, the agent decomposes clinician queries, invokes tools\nfor view recognition, cardiac structure segmentation, measurement and\ndisease prediction, and report synthesis, and integrates their outputs into\nguideline-aware answers and narrative summaries. We evaluate Echo-\nCoPilot on the public MIMIC-EchoQA benchmark, where it achieves an\naccuracy of 50.8%, outperforming both general-purpose and biomedical\nvideo vision–language models. Qualitative analyses further show that\nthe agent leverages quantitative measurements and physiologic context\nto resolve challenging cases near clinical decision thresholds, such as bor-\nderline left ventricular hypertrophy or pericardial effusion severity. The\ncode will be released upon acceptance of the paper.\nKeywords: Cardiovascular Disease · Echocardiography · Multimodal\nLarge Language Models.\n1\nIntroduction\nEchocardiography is the cornerstone of cardiovascular diagnostics. Its non-invasive\nnature, high accessibility, and real-time operational capabilities position it as an\ninvaluable tool for the early detection and monitoring of cardiac abnormalities\n[24,39]. However, the clinical interpretation of echocardiograms is a complex,\narXiv:2512.09944v2  [cs.AI]  15 Dec 2025\n"}, {"page": 2, "text": "2\nHeidari et al.\ntime-consuming process that is highly dependent on expert human intervention.\nThis process is notoriously subject to high inter- and intra-observer variability\n[33,29]. A clinician must mentally synthesize findings from dozens of video clips,\nintegrating information from multiple views (e.g., PLAX, A4C) and modali-\nties (e.g., B-mode, Doppler) to formulate a single, coherent diagnostic assess-\nment [24]. This reliance on manual interpretation creates considerable workflow\nbottlenecks, potentially delaying diagnoses [9], and requires years of specialized\ntraining to achieve proficiency.\nRecent advancements in artificial intelligence, particularly deep learning,\nhave demonstrated significant success in automating perceptual echo-related\ntasks [39,6]. These models can achieve performance competitive with, or even\nexceeding, expert-level interpretation in well-defined, discrete tasks such as view\nclassification [23], cardiac segmentation [17,30], and functional quantification\n[31]. However, these solutions typically remain siloed. The output of these mod-\nels yields quantitative metrics and discrete labels but fails to provide a holistic\nand contextualized interpretation. Crucially, they lack the higher-order clinical\nreasoning required to synthesize these disparate findings into a comprehensive\nclinical report.\nTo bridge this gap, we introduce Echo-CoPilot, a novel reasoning-based\nagent for automated echocardiography interpretation. Our framework is designed\nto transcend simple perceptual tasks, and instead explicitly emulates the human\nexpert’s cognitive workflow through a structured, multi-step process:\n1. Decomposing the complete echocardiographic exam into a set of well-\ndefined sub-problems, such as view classification, structural assessment, and\nfunctional analysis.\n2. Executing a discrete set of specialized foundation models as \"tools\" for\nrobust feature extraction or for on-demand segmentation [18] to extract spe-\ncific, quantitative, and qualitative findings.\n3. Synthesizing these disparate and multimodal findings into a clinically co-\nherent narrative report, complete with a structured summary and final im-\npression.\nFurthermore, our framework is implemented as a modular tool graph: each\nperceptual or diagnostic model is wrapped as a callable “tool” with a fixed in-\nput–output schema [37,46]. New capabilities or improved foundation models can\ntherefore be integrated by adding or swapping tool wrappers, without retraining\nthe rest of the system.\n2\nRelated Work\nRecent progress in echocardiography automation spans three largely indepen-\ndent research threads: perceptual foundation models, multi-task predictors, and\nemerging agentic systems, yet these advances have evolved in parallel rather\nthan forming a unified interpretive framework. Early efforts concentrated on\n"}, {"page": 3, "text": "Title Suppressed Due to Excessive Length\n3\nsingle-task perception, with models such as EchoNet-Dynamic [31] establish-\ning benchmarks for video-based left ventricular ejection fraction (LVEF) esti-\nmation, promoting a broader range of dedicated quantification systems [39].\nBuilding on this trajectory, several large-scale vision foundation models (VFMs)\nhave recently been developed for echocardiography, including EchoApex [10],\nwhose self-supervised training strategy draws on paradigms like DINOv2 [28] to\nlearn generalizable video representations. Parallel efforts in medical segmenta-\ntion have followed a similar progression: the Segment-Anything Model (SAM)\n[16] inspired a rapid line of medical adaptations such as MedSAM [21] and its\nsuccessor MedSAM2 [18], which provide flexible, promptable segmentation capa-\nbilities across cardiac views. Although these vision and segmentation backbones\nsupply high-quality perceptual signals, they operate as isolated components and\ndo not themselves support the multi-step synthesis that clinicians perform during\ncomprehensive echo interpretation.\nTo reduce this fragmentation, multi-task predictors have been proposed.\nPanEcho [12] demonstrated that a single network can simultaneously infer dozens\nof structural and functional findings from multi-view echo videos, while vi-\nsion–language models (VLMs) such as EchoPrime [11] extend this idea by map-\nping entire studies to free-text narrative descriptions learned from more than 12\nmillion video–report pairs. These models broaden the predictive scope of auto-\nmated echo analysis, yet their end-to-end “black-box” nature [34] limits trans-\nparency, and critically, they offer no mechanism for decomposing problems, in-\nvoking specialized tools, or validating intermediate steps, all of which are central\nto real-world clinical reasoning.\nThis gap has motivated a complementary line of work on reasoning-centric\nmedical agents, where Large Language Models orchestrate external tools to per-\nform complex diagnostic tasks. Early frameworks such as Toolformer [37] and\nReAct [46] established the underlying paradigm of stepwise tool-augmented rea-\nsoning. More recent medical adaptations include Med-PaLM M [25] for mul-\ntimodal clinical analysis and domain-specific agents for radiology [32,45], with\nMedRAX [7] showcasing how structured tool-use can improve transparency in\nchest X-ray interpretation. Collectively, these works illustrate the promise of\nagentic LLM-driven workflows but also highlight the absence of a system tailored\nto echocardiography’s unique challenges, its multi-view structure, dependency on\nsegmentation and quantitative measurements, and the need to integrate findings\nacross modalities and temporal phases. Concurrent to our work, EchoAgent [5]\nexplores tool-augmented LLMs for guideline-centric echocardiographic measure-\nment. However, it is mainly tailored to single-video, measurement-feasibility\nqueries, rather than a unified interpretive workflow\nIn this work, we introduce Echo-CoPilot, a novel study-level, multi-view\nagent that integrates multiple tasks into a single, coordinated assessment work-\nflow for echocardiography, explicitly decomposing the complex interpretation\nprocess, mimicking a clinical practice. It uses specialized models, such as EchoPrime [11]\nfor view classification and measurement prediction, MedSAM2 [18] for on-demand\nsegmentation, PanEcho [12] for disease inference, and EchoNet-Synthetic [35] for\n"}, {"page": 4, "text": "4\nHeidari et al.\nFig. 1: Echo-CoPilot architecture. The LLM controller processes clinician\nqueries using a ReAct loop, interacts with a shared memory state, and invokes\nspecialized echocardiography tools for segmentation, view classification, mea-\nsurement prediction, disease prediction, and report/video generation.\ncontrollable echo video generation, to gather discrete quantitative and semantic\nevidence. This evidence is then synthesized into a final, verifiable, and clinically\ncoherent narrative report.\n3\nMethod\nEchocardiography interpretation requires coordinating multiple interdependent\ntasks, including view identification, chamber segmentation, quantitative mea-\nsurement, and diagnostic assessment, yet existing deep learning models typi-\ncally address these components as isolated, single-task problems, such as cardiac\nstructure segmentation, echocardiographic view classification, disease classifica-\ntion, or diagnostic outcome prediction [40,15,36,14,4]. At the same time, recent\nprogress in agentic clinical AI has shown that large language models can orches-\ntrate specialized tools through structured, sequential workflows, but these ad-\nvances have been demonstrated primarily in domains such as chest radiography,\nsurgery, and general multimodal medical analysis [7,20,8], leaving echocardio-\ngraphy, with its temporal dynamics, multi-view dependencies, and strong mea-\n"}, {"page": 5, "text": "Title Suppressed Due to Excessive Length\n5\nsurement–diagnosis coupling, without an agentic framework capable of unified,\nworkflow-level interpretation.\nTo fill this gap, we introduce Echo-CoPilot, an LLM-driven agentic system\nthat decomposes a clinician’s query into actionable steps, selectively invokes spe-\ncialized echocardiography tools, integrates intermediate findings, and synthesizes\na transparent, clinically aligned final assessment. As illustrated in Figure 1, the\nsystem processes an input echo video V and query Q through an iterative cycle\nof reasoning, tool execution, and memory updating to produce the final response\nR along with auxiliary artifacts such as segmentations, measurements, and di-\nagnostic predictions.\n3.1\nAgentic Reasoning Framework\nEcho-CoPilot employs an LLM as a high-level controller operating within a Re-\nAct loop [46]. At each iteration, the agent (1) observes the clinician query Q,\nthe video context from the echocardiography study V , and its current memory\nM, (2) generates a reasoning step Ψ proposing the next action, and (3) either\ninvokes one or more specialized tools or finalizes the response. Tool outputs are\nappended to the memory and incorporated into subsequent reasoning cycles,\nenabling iterative refinement and context-aware interpretation.\nThought →Tool Selection →Execution →Integration →Next Thought\nThis process is formalized in Algorithm 1. The agent runs within a fixed time\nbudget tmax and terminates when the LLM determines that either a final answer\ncan be generated or additional user clarification is required. This agentic for-\nmulation [46] enables Echo-CoPilot to break down complex cardiology queries,\ninvoke the appropriate tools, and combine their outputs into a unified clinical\nassessment, in contrast to one-shot LLM prompting. Echo-CoPilot’s controller is\nimplemented using a LangGraph-based workflow. The graph defines nodes cor-\nresponding to LLM-inference and tool-execution steps, with edges representing\nstate transitions triggered by tool-call instructions. The memory buffer stores\nall prior messages, reasoning steps, tool outputs, and temporal summaries of\necho clips, supporting multi-turn interaction and stable cross-tool dependency\nresolution.\n3.2\nTool Ecosystem, Orchestration, and Implementation\nInspired by MedRAX [7], Echo-CoPilot is equipped with a set of domain-specific\nechocardiography tools, each implemented as a callable module with a structured\ninput schema and standardized output format. This design enables the LLM\ncontroller to integrate heterogeneous models through a unified tool interface,\nconsistent with contemporary agentic frameworks. Rather than functioning as\nindependent perception models, these tools form a coordinated pipeline that\n"}, {"page": 6, "text": "6\nHeidari et al.\nAlgorithm 1 Echo-CoPilot Agentic Loop for Echocardiography\nInput:\n1: Q: Clinician query\n2: V : Echocardiography study (set of videos)\n3: T: Available echocardiography tools\n4: M: Memory buffer\n5: tmax: Maximum allowed time\nOutput:\n6: R: Final textual assessment\n7: A: Auxiliary artifacts (e.g., masks, measurements)\n8: Initialize:\n9: tstart = GetCurrentTime()\n10: S = InitializeState(Q, V, M)\n11: while GetCurrentTime() −tstart < tmax do\n12:\nΨ = InferAction(S, M)\n▷LLM reasoning\n13:\nif NeedsClarification(Ψ) then\n14:\nR = GenerateClarificationPrompt(Ψ, M)\n15:\nreturn R\n16:\nend if\n17:\nif ReadyToAnswer(Ψ) then\n18:\n(R, A) = ComposeAnswer(Ψ, S, M)\n19:\nreturn R\n20:\nend if\n21:\nU = SelectTools(Ψ, T, M)\n22:\nres = ExecuteTools(U, S)\n23:\nM = UpdateMemory(M, Ψ, U, res)\n24:\nS = UpdateState(S, res, M)\n25: end while\n26: (R, A) = TimeoutFallback(S, M)\n27: return R\nmirrors how clinicians sequentially extract structural, functional, and diagnostic\nevidence from an echocardiographic study.\nFoundational Perceptual Tools. Echo-CoPilot first leverages tools that\nprovide core perceptual signals. The Echo Segmentation Tool uses MedSAM2\n[18] for automatic delineation of cardiac structures, enabling downstream geo-\nmetric and functional quantification. Complementing this, the View Classifica-\ntion Tool identifies standard echocardiographic views (e.g., A4C, A2C, PLAX)\nusing an EchoPrime-based classifier [11], ensuring that subsequent tools oper-\nate on correctly contextualized video clips. Together, these modules establish\nthe structural and contextual foundation upon which further reasoning steps are\nbuilt.\nFunctional and Diagnostic Tools. Building on the perceptual outputs,\nEcho-CoPilot integrates tools for functional assessment and disease interpre-\ntation. The Measurement Prediction Tool provides numerical estimates of key\ncardiac parameters—such as ejection fraction and chamber dimensions—using\nEchoPrime-derived measurement heads [11]. In parallel, the Disease Prediction\n"}, {"page": 7, "text": "Title Suppressed Due to Excessive Length\n7\nTool, powered by PanEcho [12], infers a broad set of clinical abnormalities from\nvideo-level features. These modules supply quantitative and semantic evidence\nthat the agent incorporates into higher-order reasoning.\nNarrative and Generative Tools. Once perceptual and diagnostic infor-\nmation has been accumulated, the Report Generation Tool synthesizes these\nfindings into a coherent narrative aligned with cardiology reporting conventions,\ndriven by an EchoPrime model [11]. In addition, an Echo Video Generation Tool\nintegrates the EchoNet-Synthetic [35] model to generate high-fidelity, privacy-\npreserving echocardiography cine loops that closely reproduce real physiologi-\ncal patterns, motion dynamics, and structural relationships under user-specified\nconfigurations, thereby enabling explanation, rapid prototyping, and training of\ndownstream regression, classification, and segmentation models without relying\nexclusively on sensitive patient data.\nTool Orchestration. These tools operate not as isolated predictors but as\ncomponents of a coordinated workflow. At each reasoning step, the LLM selects\ntools based on the current state Ψ, available context in memory M, and the\nclinician’s query. For example, segmentation outputs may trigger targeted mea-\nsurement estimation, while view classification informs which diagnostic model\nis appropriate. Intermediate results are cached to avoid redundant computation\nand to maintain consistency across dependent tasks (e.g., using the same seg-\nmentation masks to derive both EF and structural measurements). Robustness\nis ensured through fallback strategies that address low-quality views, missing\nclips, or ambiguous instructions.\nImplementation Details. All tools are wrapped using LangChain/Lang-\nGraph abstractions, enabling the LLM to invoke them through structured func-\ntion calls. Echo videos are preprocessed via frame sampling, resolution nor-\nmalization, and temporal aggregation to ensure computational efficiency and\nstable cross-tool compatibility. The full system runs on a single GPU, and an\naccompanying Streamlit interface allows users to upload echo studies, view in-\ntermediate outputs (segmentations, measurements, predictions), and engage in\ninteractive multi-turn dialogue with Echo-CoPilot. This transparent integration\nensures that each tool contributes interpretable evidence toward the final clinical\nassessment.\n4\nExperimental Evaluation\n4.1\nBenchmark and Implementation Details\nWe evaluate Echo-CoPilot on the MIMICEchoQA benchmark [42]. MIMICE-\nchoQA is built from the publicly available MIMIC-IV-ECHO database and\nconsists of 622 transthoracic echocardiogram videos, each paired with a sin-\ngle multiple-choice question and four answer options (A–D) [42]. The dataset\nspans 38 distinct echocardiographic views, and this variability in imaging per-\nspective and anatomical focus enables MIMICEchoQA to evaluate whether a\nmodel can jointly perform view-aware visual reasoning and clinically meaning-\nful language comprehension. Questions focus on core clinical tasks such as left\n"}, {"page": 8, "text": "8\nHeidari et al.\nTable 1: Accuracy of general-purpose and biomed-specialized vision–language\nmodels on the MIMIC EchoQA benchmark. All baseline results are taken directly\nfrom OpenBiomedVid [42]. Echo-CoPilot achieves the highest accuracy among\nall evaluated models.\nModel\nAccuracy\nModel\nAccuracy\nVideo-ChatGPT [22]\n31.7\nVideo-LLaVA [19]\n32.0\nPhi-3.5-vision-instruct [1]\n41.1\nPhi-4-multimodal-instruct [2]\n37.8\nInternVideo2.5-Chat-8B [44]\n40.3\nQwen2-VL-7B-Instruct [43]\n37.9\nQwen2.5-VL-7B-Instruct [3]\n34.0\nQwen2-VL-72B-Instruct [43]\n37.5\nQwen2.5-VL-72B-Instruct [3]\n34.2\nGemini-2.0-Flash [41]\n38.4\nGPT-4o [13]\n41.6\no4-mini [27]\n43.9\nQwen2-VL-2B-biomed [42]\n42.0\nQwen2-VL-7B-biomed [42]\n49.0\nMedGemma-4B [38]\n33.4\nEcho-CoPilot\n50.8\nventricular systolic function, chamber size, valvular stenosis and regurgitation,\nand pericardial effusion, and are grounded in expert-validated diagnostic reports\nand view-consistent video clips. The benchmark is intended for evaluation only\nand does not provide a training split; we therefore treat all examples as held-out\ntest cases and report accuracy as the primary metric.\nEcho-CoPilot uses GPT-5.1 [26] as the backbone language model that coor-\ndinates the tool calls described in Section 3. At inference time, the agent receives\nthe echocardiogram video, the corresponding multiple-choice question, and the\nanswer options, and is allowed to invoke any subset of its tools (view classi-\nfication, segmentation, measurement prediction, and disease prediction) before\nemitting a final choice. Tool calls are implemented as structured JSON function\ncalls, and the intermediate outputs are only used internally by the agent. All\nexperiments are run on a single NVIDIA RTX 4070 GPU. We compare Echo-\nCoPilot against baselines evaluated under the same closed-ended, single-step\nquestion answering protocol as in [42].\n4.2\nQuantitative Analysis\nTable 1 summarizes performance on MIMICEchoQA. Echo-CoPilot achieves\nthe highest overall accuracy on the benchmark, outperforming both proprietary\n(e.g., GPT-4o) and open-source multimodal baselines. For MedGemma-4B [38],\na domain-specific medical VLM, we first used an EchoNet-based key-frame se-\nlector [31] to automatically extract end-diastolic and end-systolic frames for\neach video and feed them, together with the question, into the model, since\nit is strictly image-based and was not developed for processing full video se-\nquences. The gains are most pronounced for questions that require quantitative\n"}, {"page": 9, "text": "Title Suppressed Due to Excessive Length\n9\nFig. 2: Qualitative examples of Echo-CoPilot’s internal ReAct-style reasoning\non two MIMICEchoQA questions. Each panel shows the user query, the tools\nselected at each step, their outputs, and the final answer. In the first example,\nthe agent assesses mitral regurgitation severity by combining report, disease, and\nmeasurement tool outputs; in the second, it combines measurement and disease-\nprediction outputs to grade left ventricular hypertrophy.\nreasoning or integrating multiple findings, such as grading left ventricular sys-\ntolic dysfunction from ejection fraction, assessing the severity of left ventricular\nhypertrophy, or stratifying pericardial effusion severity. In contrast, GPT-4o and\nQwen2-VL models, which rely primarily on direct visual pattern recognition from\nthe video frames, perform competitively on simpler pattern recognition problems\nbut are more likely to misclassify borderline cases near clinical thresholds (e.g.,\nEF or wall thickness cut-offs). These results indicate that explicitly coupling the\nLLM with domain-specific echocardiography tools provides a systematic advan-\ntage over purely vision-language baselines. Rather than relying on subjective\nvisual impressions alone, Echo-CoPilot can defer to measurement and disease-\nprediction tools when the question depends on precise numeric values or subtle\nhemodynamic cues, and only then commit to a final answer.\n4.3\nQualitative Analysis\nBeyond aggregate accuracy, we qualitatively inspect how Echo-CoPilot behaves\non individual MIMICEchoQA questions. Figure 2 visualizes the internal ReAct\ntraces for representative queries on structural and functional assessment. In both\nexamples, the agent begins by clarifying which information is missing, then se-\nlectively invokes required tools. When direct measurements (e.g., wall thickness)\nare unavailable, the agent pivots to surrogate signals, such as binary dilation\nflags, and explicitly states that its conclusion is based on these model outputs\nrather than on a non-existent numeric value. The resulting answers are short, but\nthe reasoning chain shows that the agent consistently cross-checks tool outputs\nbefore committing to a decision.\n"}, {"page": 10, "text": "10\nHeidari et al.\nFigure 3 contrasts this behavior with GPT-4o on more challenging ques-\ntions involving regional left ventricular systolic dysfunction and circumferential\npericardial effusion. GPT-4o relies almost entirely on visual impression, often\nequating pronounced wall-motion abnormalities or a visually large effusion with\nthe highest severity category. Echo-CoPilot, in contrast, grounds its judgment in\nquantitative and physiologic context: it uses the measurement tool to reconcile\nejection fraction estimates with the severity scale for LV dysfunction, and it uses\ndisease-prediction outputs to verify the absence of tamponade physiology when\ngrading effusion severity. In both examples, this tool-anchored reasoning leads to\nanswers that match the benchmark labels, whereas GPT-4o over-calls severity.\nTaken together, Figures 2 and 3 illustrate a consistent pattern: Echo-CoPilot\ntreats tool outputs as primary evidence, uses them to correct or refine initial\nvisual impressions, and only then commits to an answer. This behavior comple-\nments the quantitative gains reported in Table 1, and suggests that the agent’s\nimprovements on MIMICEchoQA stem from more than raw pattern recogni-\ntion—they reflect a shift toward measurement- and guideline-aware reasoning.\nFig. 3: Comparison between Echo-CoPilot and GPT-4o on two challenging MIM-\nICEchoQA samples: regional left ventricular systolic dysfunction (Sample #340,\ntop) and circumferential pericardial effusion (Sample #321, bottom). For each\ncase, the figure shows the question, the echo cine, Echo-CoPilot’s tool-anchored\nreasoning and answer, and GPT-4o’s vision-only response. Echo-CoPilot grounds\nits predictions in measurements and hemodynamic context and matches the ref-\nerence labels, while GPT-4o tends to overestimate severity from visual impres-\nsion alone.\n5\nConclusion\nWe introduced Echo-CoPilot, an agentic framework that unifies recent echocar-\ndiography foundation models into a single, workflow-oriented system. By cou-\npling an LLM controller with tools for view classification, segmentation, mea-\nsurement and disease prediction, report generation, and synthetic video creation,\nEcho-CoPilot moves beyond isolated perceptual tasks towards guideline-aware\n"}, {"page": 11, "text": "Title Suppressed Due to Excessive Length\n11\nreasoning over full echo studies. On the MIMICEchoQA benchmark, the agent\nachieves state-of-the-art performance compared with strong video vision–language\nmodels, and qualitative analyses show that it leverages quantitative measure-\nments and physiologic context to resolve clinically borderline cases more reliably\nthan vision-only baselines. Future work will focus on prospective validation of\nthe Echo-CoPilot agent in real clinical workflows to assess reliability, efficiency,\nand user trust. In addition, each component tool (e.g., view classification, seg-\nmentation, measurement synthesis, and reporting) will be evaluated individually,\nenabling fine-grained benchmarking and guiding targeted improvements before\nlarge-scale deployment.\nACKNOWLEDGMENTS\nThis work was supported by the Canadian Foundation for Innovation-John R.\nEvans Leaders Fund (CFI-JELF) program grant number 42816. Mitacs Accel-\nerate program grant number AWD024298-IT33280. We also acknowledge the\nsupport of the Natural Sciences and Engineering Research Council of Canada\n(NSERC), [RGPIN-2023-03575]. Cette recherche a été financée par le Conseil\nde recherches en sciences naturelles et en génie du Canada (CRSNG), [RGPIN-\n2023-03575].\nReferences\n1. Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A.A., Bach, N., Bahree,\nA., Bakhtiari, A., Bao, J., Behl, H., et al.: Phi-3 technical report: A highly capable\nlanguage model locally on your phone. arXiv preprint arXiv:2404.14219 (2024)\n2. Abouelenin, A., Ashfaq, A., Atkinson, A., Awadalla, H., Bach, N., Bao, J., Ben-\nhaim, A., Cai, M., Chaudhary, V., Chen, C., et al.: Phi-4-mini technical report:\nCompact yet powerful multimodal language models via mixture-of-loras. arXiv\npreprint arXiv:2503.01743 (2025)\n3. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang,\nS., Tang, J., et al.: Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923\n(2025)\n4. Bao, M., Wang, Y., Wei, X., Jia, B., Fan, X., Lu, D., Gu, Y., Cheng, J., Zhang, Y.,\nWang, C., Zhu, H.: Real-world visual navigation for cardiac ultrasound view plan-\nning. In: Medical Image Computing and Computer Assisted Intervention – MIC-\nCAI 2024. Lecture Notes in Computer Science, vol. 15001, pp. 317–326. Springer\nNature Switzerland (Oct 2024). https://doi.org/10.1007/978-3-031-72378-0_\n30\n5. Daghyani, M., Wang, L., Hashemi, N., Medhat, B., Abdelsamad, B., Velez, E.R., Li,\nX., Tsang, M.Y., Luong, C., Tsang, T.S., et al.: Echoagent: Guideline-centric rea-\nsoning agent for echocardiography measurement and interpretation. arXiv preprint\narXiv:2511.13948 (2025)\n6. Dilsizian, V., Siegel, E.L.: Artificial intelligence in echocardiography: a clinical\nperspective. Journal of the American College of Cardiology 79(10), 1016–1032\n(2022)\n"}, {"page": 12, "text": "12\nHeidari et al.\n7. Fallahpour, A., Ma, J., Munim, A., Lyu, H., Wang, B.: Medrax: Medical reasoning\nagent for chest x-ray (2025), https://arxiv.org/abs/2502.02673\n8. Fathi, N., Kumar, A., Arbel, T.: Aura: A multi-modal medical agent for under-\nstanding, reasoning & annotation (2025), https://arxiv.org/abs/2507.16940\n9. Hiemstra, B., Lee, K., Gorski, S., Sze, D., D’Souza, R., , Soni, K.: Improving\nefficiency in the echocardiography laboratory: workflow optimization and lean six\nsigma. Journal of the American Society of Echocardiography 32(2), 189–197 (2019)\n10. Huang, Y., Zhang, D., Wessler, T., Hu, Z., Ouyang, D.a.: Echoapex: A\ngeneral-purpose vision foundation model for echocardiography. arXiv preprint\narXiv:2405.02340 (2024)\n11. Huang, Y., Wessler, T., Zhang, D., Hu, Z., Ouyang, D.: Echoprime: A 12-million-\nexample video-language foundation model for echocardiography. arXiv preprint\narXiv:2405.03433 (2024)\n12. Huang, Y., Wessler, T., Zhang, D., Hu, Z., Ouyang, D.: Panecho: A universal\nechocardiography foundation model for multi-task interpretation. arXiv preprint\narXiv:2405.03434 (2024)\n13. Hurst, A., Lerer, A., Goucher, A.P., Perelman, A., Ramesh, A., Clark, A., Os-\ntrow, A., Welihinda, A., Hayes, A., Radford, A., et al.: Gpt-4o system card. arXiv\npreprint arXiv:2410.21276 (2024)\n14. Judge, A., Judge, T., Duchateau, N., Sandler, R.A., Sokol, J.Z., Bernard, O.,\nJodoin, P.M.: Domain adaptation of echocardiography segmentation via reinforce-\nment learning (2024), https://arxiv.org/abs/2406.17902\n15. Kim, W.J.C., Oliveira, J., Beqiri, A., Thorley, A., Strom, J., O’Driscoll, J., Sharma,\nR., Slivnick, J., Lang, R., Gomez, A., Chartsias, A.: Learning to stop: Reinforce-\nment learning for efficient patient-level echocardiographic classification (2025),\nhttps://arxiv.org/abs/2509.19694\n16. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., ...: Segment anything. arXiv preprint\narXiv:2304.02643 (2023)\n17. Leclerc, S., Smistad, E., Pedrosa, J., Østvik, A., , Løvstakken, L.: Deep learning\nfor segmentation of the left ventricle in echocardiography. IEEE transactions on\nmedical imaging 38(9), 2112–2124 (2019)\n18. Li, Z., Liu, Y., Li, J., Zhang, T., , Wu, Y.: Medsam2: A general-purpose 3d medical\nimage segmentation model. arXiv preprint arXiv:2406.18274 (2024)\n19. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning\nunited visual representation by alignment before projection. In: Proceedings of\nthe 2024 Conference on Empirical Methods in Natural Language Processing. pp.\n5971–5984 (2024)\n20. Low, C.H., Wang, Z., Zhang, T., Zeng, Z., Zhuo, Z., Mazomenos, E.B., Jin, Y.:\nSurgraw: Multi-agent workflow with chain-of-thought reasoning for surgical intel-\nligence (2025), https://arxiv.org/abs/2503.10265\n21. Ma, J., Wang, B., Zhu, Y., , Chen, J.: Medsam: Segment anything in medical\nimages. arXiv preprint arXiv:2304.12306 (2023)\n22. Maaz, M., Rasheed, H., Khan, S., Khan, F.: Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models. In: Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). pp. 12585–12602 (2024)\n23. Madani, A., Padmakumar, M., Bejerano, G., , Arnaout, R.: Fast and accurate view\nclassification of echocardiograms using deep learning. NPJ digital medicine 1(1),\n6 (2018)\n"}, {"page": 13, "text": "Title Suppressed Due to Excessive Length\n13\n24. Mitchell, C., Schmid, R., Min, G., Saric, M., Picard, M.a.: Guidelines for per-\nforming a comprehensive adult transthoracic echocardiogram: recommendations\nfrom the american society of echocardiography. Journal of the American Society\nof Echocardiography 32(1), 1–64 (2019)\n25. Moor, M., Eickhoff, C., Sercu, T., Agrawal, M., Singhal, K., Kohli, P.: Med-palm m:\nA gato-inspired medical multimodal llm. arXiv preprint arXiv:2307.01633 (2023)\n26. OpenAI: Gpt-5.1 models. https://platform.openai.com/docs/models/gpt-5.1\n(2025), openAI API model documentation\n27. OpenAI: Openai o3 and o4-mini system card. Tech. rep., OpenAI (April 2025),\nhttps://openai.com/index/o3-o4-mini-system-card, accessed: 2025-05-13\n28. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Tcimpidis, V.,\nMairal, J., LeCun, Y., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual\nfeatures without supervision. arXiv preprint arXiv:2304.07193 (2023)\n29. Otto, C.M., Badano, L.P., Glineur, D., Tornos, P., Lancellotti, P.: A survey of\nvariability in echocardiographic measurements: a call for action for the profession.\nJACC: Cardiovascular Imaging 6(5), 630–634 (2013)\n30. Ouwendijk, J., Soons, J., , Bosch, J.: Dynamic u-net for segmentation of the left\nventricle in echocardiography. IEEE Transactions on Medical Imaging 41(1), 136–\n146 (2022)\n31. Ouyang, D., He, B., Ghorbani, A., Yuan, N., Ebinger, J., Langlotz, C.P., Heiden-\nreich, P.A., Harrington, R.A., Liang, D.H., Ashley, E.A., et al.: Video-based ai for\nbeat-to-beat assessment of cardiac function. Nature 580(7802), 252–256 (2020)\n32. Paranjape, A., Wu, A., , Zhang, Y.: Radagent: A framework for llm-driven au-\ntonomous tool use in radiology. arXiv preprint arXiv:2405.13251 (2024)\n33. Pellikka, P.A., Nagueh, S.F., Elhendy, A., Kuehl, C.A., Sawada, S.G.: Interob-\nserver variability in interpretation of stress echocardiography: a multicenter study.\nJournal of the American College of Cardiology 45(1), 48–53 (2005)\n34. Rajpurkar, P., Chen, E., Banerjee, O., Topol, E.J.: Explainable artificial intelli-\ngence for medical imaging. Nature Reviews Bioengineering 1(1), 38–51 (2022)\n35. Reynaud, H., Meng, Q., Dombrowski, M., Ghosh, A., Day, T., Gomez, A., Lee-\nson, P., Kainz, B.: Echonet-synthetic: Privacy-preserving video generation for safe\nmedical data sharing. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. pp. 285–295. Springer (2024)\n36. Saadat, A., Hashemi, N., Vaseli, H., Tsang, M.Y., Luong, C., de Panne, M.V.,\nTsang, T.S.M., Abolmaesumi, P.: Precise-as: Personalized reinforcement learn-\ning for efficient point-of-care echocardiography in aortic stenosis diagnosis (2025),\nhttps://arxiv.org/abs/2509.02898\n37. Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Tsvigun, M., Ciufo, P., ,\nScialom, T.: Toolformer: Language models can teach themselves to use tools. arXiv\npreprint arXiv:2302.04761 (2023)\n38. Sellergren, A., Kazemzadeh, S., Jaroensri, T., Kiraly, A., Traverse, M., Kohlberger,\nT., Xu, S., Jamil, F., Hughes, C., Lau, C., et al.: Medgemma technical report. arXiv\npreprint arXiv:2507.05201 (2025)\n39. Shvets, A., Bobrov, A., Gusev, G., Gombolevskiy, V., Kalinin, R., Sorokin, I.,\nChernina, A., Gusev, A., Klyuchnikov, I., Paleev, F., ...: Deep learning for echocar-\ndiography. Nature Reviews Cardiology 20(8), 535–550 (2023)\n40. Song, S., Qin, Y., Yang, H., Huang, T., Fei, H., Li, X.: Echoviewclip: Advanc-\ning video quality control through high-performance view recognition of echocar-\ndiography. In: Proceedings of Medical Image Computing and Computer Assisted\nIntervention (MICCAI) 2025. Lecture Notes in Computer Science, vol. 15972,\n"}, {"page": 14, "text": "14\nHeidari et al.\npp. 181–191. Springer Nature Switzerland (Sep 2025). https://doi.org/10.1007/\n978-3-032-05169-1_18\n41. Team, G., Anil, R., Borgeaud, S., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk,\nJ., Dai, A.M., Hauth, A., Millican, K., et al.: Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805 (2023)\n42. Thapa, R., Li, A., Wu, Q., He, B., Sahashi, Y., Binder, C., Zhang, A., Athi-\nwaratkun, B., Song, S.L., Ouyang, D., et al.: How well can general vision-language\nmodels learn medicine by watching public educational videos? arXiv preprint\narXiv:2504.14391 (2025)\n43. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang,\nJ., Ge, W., et al.: Qwen2-vl: Enhancing vision-language model’s perception of the\nworld at any resolution. arXiv preprint arXiv:2409.12191 (2024)\n44. Wang, Y., Li, X., Yan, Z., He, Y., Yu, J., Zeng, X., Wang, C., Ma, C., Huang,\nH., Gao, J., et al.: Internvideo2. 5: Empowering video mllms with long and rich\ncontext modeling. arXiv preprint arXiv:2501.12386 (2025)\n45. Wu, T., Wu, H., , Yuan, C.: Medagents: Large language models as collaborators\nfor zero-shot medical reasoning. arXiv preprint arXiv:2311.10537 (2023)\n46. Yao, S., Zhao, J., Yu, D., Du, N., Tsvetkov, Y., Le, Q.a.: React: Synergizing rea-\nsoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022)\n"}]}