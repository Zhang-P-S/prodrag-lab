{"doc_id": "arxiv:2601.12471", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.12471.pdf", "meta": {"doc_id": "arxiv:2601.12471", "source": "arxiv", "arxiv_id": "2601.12471", "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty", "authors": ["Sravanthi Machcha", "Sushrita Yerra", "Sahil Gupta", "Aishwarya Sahoo", "Sharmin Sultana", "Hong Yu", "Zonghai Yao"], "published": "2026-01-18T16:19:29Z", "updated": "2026-01-22T05:03:19Z", "summary": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.12471v2", "url_pdf": "https://arxiv.org/pdf/2601.12471.pdf", "meta_path": "data/raw/arxiv/meta/2601.12471.json", "sha256": "532c26e2b70e9d4eaa0a6ac0b9d999320926881fd9f13920527c3e593d1bc817", "status": "ok", "fetched_at": "2026-02-18T02:21:18.723855+00:00"}, "pages": [{"page": 1, "text": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty\nSravanthi Machcha* 1, Sushrita Yerra* 1, Sahil Gupta 1, Aishwarya Sahoo 1,\nSharmin Sultana 2,3, Hong Yu 1,2,3, Zonghai Yao 1,2\n1Manning College of Information and Computer Sciences, UMass Amherst, MA, USA\n2Center for Healthcare Organization and Implementation Research, VA Bedford Health Care\n3Miner School of Computer and Information Sciences, UMass Lowell, MA, USA\nsmachcha@umass.edu, sushrithay@gmail.com, zonghaiyao@umass.edu\nAbstract\nCurrent evaluation of large language models\n(LLMs) overwhelmingly prioritizes accuracy;\nhowever, in real-world and safety-critical ap-\nplications, the ability to abstain when uncer-\ntain is equally vital for trustworthy deployment.\nWe introduce MedAbstain, a unified bench-\nmark and evaluation protocol for abstention\nin medical multiple-choice question answering\n(MCQA) – a discrete-choice setting that gener-\nalizes to agentic action selection – integrating\nconformal prediction, adversarial question per-\nturbations, and explicit abstention options. Our\nsystematic evaluation of both open- and closed-\nsource LLMs reveals that even state-of-the-art,\nhigh-accuracy models often fail to abstain with\nuncertain. Notably, providing explicit absten-\ntion options consistently increases model un-\ncertainty and safer abstention, far more than\ninput perturbations, while scaling model size\nor advanced prompting brings little improve-\nment. These findings highlight the central role\nof abstention mechanisms for trustworthy LLM\ndeployment and offer practical guidance for im-\nproving safety in high-stakes applications. 1\n1\nIntroduction\nReliability has become the central challenge\nfor deploying large language models (LLMs) in\nreal-world\nNLP,\nparticularly\nin\nhigh-stakes\ndomains\nsuch\nas\nmedicine,\nlaw,\nand\nfi-\nnance (Thirunavukarasu et al., 2023; Guha\net al., 2023; Wu et al., 2023; Achiam et al., 2023;\nChang et al., 2024; Yao and Yu, 2025). Reliability\nproblems often show up as hallucinations and\nmiscalibrated uncertainty (Farquhar et al., 2024;\nKossen et al., 2024). While LLMs now match or\nexceed human experts on many tasks (Achiam\net al., 2023), a critical barrier remains: Can we\ntrust LLMs not only to answer correctly, but also\n*Equal contribution, alphabetical order\n1Our benchmark will be released at https://github.\ncom/sravanthi6m/MedAbstain with CC-BY-NC 4.0 license.\nMedAbstain\nAbstBench\nAbst-QA\nUQ-Bench\nDomain\nClinMCQ\nMixed\nGen-MCQ\nGen-NLP\nMed\n✓\n×\n×\n×\nAbst\n✓\n✓\n✓\n×\nUQ\n✓CP\n× LLM judge\n× VC\n✓CP\nPert\n✓\n✓\n×\n×\nC-LLMs\n✓logprobs\n✓\n✓\n×\nCoT+FS\n✓\n×\n×\n×\n4-way\n✓\n×\n×\n×\nTable 1: Qualitative comparison with recent works2.\nGreen indicates a feature present, and red indicates a\nfeature absent. To our knowledge, MedAbstain (ours)\nis the first to unite medical-QA evaluation with confor-\nmal prediction, explicit abstention analysis, and crucial\ncontext-omission perturbations, filling an essential gap\nin medical safety and LLM reliability.\nto recognize when they should abstain?\nPrior\nwork (Kadavath et al., 2022) shows LMs can\nsometimes predict whether their own answers are\ncorrect when asked in the right format, but this is\nnot a complete solution for high-stakes use.\nIn high-risk applications, accuracy alone is in-\nsufficient (Myers et al., 2020; Ye et al., 2024;\nWang et al., 2025).\nUsers often ask am-\nbiguous, underspecified, or unanswerable ques-\ntions (Thirunavukarasu et al., 2023), making it\nessential that LLMs can withhold an answer and\nadmit uncertainty. Such abstention is vital for pre-\nventing harmful errors and is increasingly recog-\nnized as key to trustworthy NLP (Kirichenko et al.,\n2025); for instance, in clinical decision support,\noverconfident or fabricated answers can jeopardize\npatient safety.\n2AbstBench = AbstentionBench(Kirichenko et al., 2025);\nAbst-QA = Abstain-QA(Madhusudhan et al., 2024); UQ-\nBench = LLM-Uncertainty Bench(Ye et al., 2024) ; Clin-\nMCQ = clinical MCQA; Gen-MCQ = generic MCQA;\nGen-NLP = generic NLP); Med: Medical Focus; Abst: ex-\nplicit abstention option; UQ: deterministic uncertainty quan-\ntification (CP = conformal-prediction uncertainty, VC =\nVerbal confidence); Pert: perturbed / underspecified items;\nC-LLMs: evaluation on Closed-Source LLMs; CoT+FS:\nchain-of-thought / few-shot analysis; 4-way: covers all four\narXiv:2601.12471v2  [cs.CL]  22 Jan 2026\n"}, {"page": 2, "text": "Despite its importance, abstention remains\nlargely unaddressed in current LLM evaluation.\nLeading benchmarks like MedQA (Jin et al.,\n2021), MedQA-CS (Yao et al., 2024), and MedM-\nCQA (Pal et al., 2022) focus on accuracy, overlook-\ning whether answers should have been withheld or\nif confidence was justified. Recent efforts in uncer-\ntainty quantification and calibration (Tomani et al.,\n2024) have made progress but lack unified, scal-\nable protocols, especially for black-box or closed-\nsource models, which are now common.\nThis gap is particularly consequential in medical\nNLP, where incomplete information, adversarial\ndistractors, and ambiguity are routine (Weidinger\net al., 2022). Here, prudent abstention is a necessity\nfor safe AI deployment, yet current benchmarks\nrarely assess a model’s ability to say “I don’t know,”\nand there are no standard methods to quantify or re-\nlate uncertainty and abstention (Xiong et al., 2023).\nClinical decision support settings can be vulnera-\nble to adversarial prompts that trigger hallucinated\nclinical content (Omar et al., 2025; Yang et al.,\n2025).\nTo address this, we propose MedAbstain, a\nunified benchmark and evaluation protocol for ab-\nstention in medical multiple-choice QA (MCQA).\nOur approach combines conformal prediction (An-\ngelopoulos et al., 2020) with adversarially per-\nturbed and abstention-augmented questions, en-\nabling nuanced uncertainty and abstention assess-\nment, even for black-box LLMs (Tomani et al.,\n2024). MedAbstain features both original and sys-\ntematically modified questions (e.g., with missing\nkey details or misleading distractors) (Madhusud-\nhan et al., 2024), and evaluates a diverse set of\nopen- and closed-source models under zero-shot,\nfew-shot, and chain-of-thought prompting (Kossen\net al., 2024).\nOur results reveal several important trends. First,\nwe generally observe a strong positive association\nbetween abstention awareness and model uncer-\ntainty: when most models are given the explicit\noption to abstain, their uncertainty typically in-\ncreases across both datasets (see Figures 2 and 3),\nunderscoring the link between abstention behavior\nand uncertainty quantification in LLMs. However,\nthere are notable exceptions to this trend, particu-\nlarly among certain closed-source or larger models\n(e.g., GPT-4.1), where abstention options do not al-\nways increase uncertainty or may even lead to coun-\npillars (Med+CP+Abst+Pert).\nterintuitive patterns. Notably, introducing informa-\ntion perturbations, such as omitting key question\ndetails, has a much smaller effect on uncertainty\nthan enabling abstention, further highlighting the\npivotal role of abstention mechanisms in LLM re-\nliability. We also find that neither scaling model\nsize nor applying instruction tuning consistently\nimproves abstention performance; in some cases,\nchain-of-thought prompting actually increases un-\ncertainty without making abstention safer. Finally,\nwe show that conformal prediction provides a gen-\nerally robust and scalable approach for quantifying\nLLM uncertainty and identifying overconfident an-\nswers, offering actionable guidance for safer LLM\ndeployment in high-stakes applications, while also\nrevealing the need for further investigation of cali-\nbration and uncertainty in certain proprietary mod-\nels.\n2\nRelated Work\nUncertainty Quantification and Conformal Pre-\ndiction\nModel uncertainty estimation is founda-\ntional for trustworthy AI, especially in decision-\ncritical settings (Fomicheva et al., 2020; Gaw-\nlikowski et al., 2023; Abdar et al., 2021). Classical\nmethods include entropy, calibration, Bayesian in-\nference, and ensembling (Hu et al., 2023; Wimmer\net al., 2023; Kwon et al., 2020; Rahaman et al.,\n2021), but these often fail to generalize to LLMs or\nare impractical for black-box access (Abdar et al.,\n2021). Conformal prediction (CP) has recently\nemerged as a robust, model-agnostic method pro-\nviding statistical guarantees (Angelopoulos and\nBates, 2021; Kumar et al., 2023; Kapoor et al.,\n2024), with successful applications in MCQA and\nother NLP tasks (Deutschmann et al., 2024; Ye\net al., 2024). For black-box LLMs, verbalized\nconfidence and output aggregation have been pro-\nposed (Tian et al., 2023; Xiong et al., 2023), but\nremain difficult to standardize or compare across\nmodels. MedAbstain extends CP-based evaluation\nto both open and closed models, directly linking un-\ncertainty to abstention in MCQA under real-world\nconditions.\nAbstention, Refusal, and Calibration in LLMs\nAbstention, withholding an answer under uncer-\ntainty, has been studied from classic classifica-\ntion to LLMs (Yin et al., 2023; Wimmer et al.,\n2023; Amayuelas et al., 2023).\nWhile recent\nLLM benchmarks include explicit abstention op-\ntions or synthetic “cannot answer” prompts (Brah-\n"}, {"page": 3, "text": "man et al., 2024; Madhusudhan et al., 2024), stan-\ndardized evaluation of abstention—especially for\nMCQA or proprietary models—remains rare. Ap-\nproaches such as verbalized uncertainty (Lin et al.,\n2022), prompt engineering (Xiong et al., 2023),\nfinetuning (Chen et al., 2024), or rejection post-\nprocessing (Varshney and Baral, 2023) have lim-\nited calibration or generalization (Vashurin et al.,\n2025). Most prior work emphasizes general QA,\nrarely addressing adversarial or clinical settings.\nMedAbstain bridges this gap by integrating absten-\ntion and uncertainty assessment for both open- and\nclosed-source LLMs in medical MCQA.\nReasoning, Prompting, and Hallucination in\nLLMs\nReasoning-finetuned LLMs and chain-of-\nthought (CoT) prompting have advanced state-\nof-the-art results in math, science, and clinical\nQA (Zelikman et al., 2022; Luo et al., 2023; Muen-\nnighoff et al., 2025; Guo et al., 2025; Cobbe\net al., 2021).\nHowever, most benchmarks re-\nmain accuracy-centric, overlooking overconfidence\nand the tendency to answer regardless of uncer-\ntainty (Kadavath et al., 2022; Yin et al., 2024).\nWhile the connection between hallucination and\nabstention has been explored (Wen et al., 2025;\nHuang et al., 2025), systematic studies on absten-\ntion, especially in MCQA with adversarial or per-\nturbed questions, are limited (Ma et al., 2024; Rah-\nman et al., 2024; Shi et al., 2023). Recent bench-\nmarks (e.g., AbstentionBench (Kirichenko et al.,\n2025), COCONOT (Brahman et al., 2024), Abstain-\nQA (Madhusudhan et al., 2024)) mainly focus on\nopen-domain tasks, seldom examining the inter-\nplay of model scale, reasoning, and abstention in\nclinical MCQA. MedAbstain systematically inves-\ntigates these factors, revealing nuanced interactions\nbetween prompting, scaling, and abstention relia-\nbility. In addition, related lines of work aim to\nreduce medical reasoning hallucinations through\nretrieval grounding (e.g., RAG (Lewis et al., 2020;\nShuster et al., 2021; Xiong et al., 2024; Wang et al.,\n2024)), test time scaling methods (Madaan et al.,\n2023; Yao et al., 2025; Zhang et al., 2024; Xie et al.,\n2024; Tran et al., 2025b; Liang et al., 2024; Chen\net al., 2025; Tran et al., 2025a), and post-training\nmethods (Ouyang et al., 2022; Rafailov et al., 2023;\nMishra et al., 2024; Bai et al., 2022; Shao et al.,\n2024; Zhang et al., 2025); we do not evaluate these\napproaches here due to space constraints and leave\ntheir integration with abstention-aware uncertainty\nevaluation for future work.\n3\nMethodology\nMedAbstain focuses on medical multiple-choice\nquestion answering (MCQA) tasks, consistent with\nthe evaluation structure of the Open Medical-LLM\nLeaderboard.3 The MCQ format is especially suit-\nable for uncertainty analysis via conformal predic-\ntion, which requires a well-defined output label\nspace Y.\n3.1\nDatasets\nWe select the following medical MCQA datasets\nfor evaluation: 1. MedQA (USMLE) (Jin et al.,\n2021): This is a large-scale, multiple-choice QA\nbenchmark derived from professional medical li-\ncensing exams, typically 4–5 answer options per\nquestion. 2. AMBOSS (Gilson et al., 2023) 4: This\nprivate dataset consists of clinical reasoning ques-\ntions designed to evaluate medical decision-making\nskills. It includes a wide range of MCQs reflecting\nreal-world diagnostic and therapeutic challenges\nfaced by medical professionals. It is used in aca-\ndemic and commercial research on medical ques-\ntion answering and reasoning.\nDataset variants To evaluate the model’s confi-\ndence, abstention behavior, and their correlation,\nwe construct multiple dataset variants. These vari-\nants are designed to probe how different condi-\ntions—such as missing information or the presence\nof an abstention option—affect model predictions.\nOriginal (NoAbstention) This variant, also hence-\nforth referred to as NA (No-Abstention Variant),\nserves as the baseline for the entire study. It eval-\nuates the model’s predictions and confidence on\nthe original dataset, without any modifications or\nperturbations.\nAbstention This variant, also henceforth referred\nto as A (Abstention Variant), introduces an explicit\nabstention option to each question, allowing the\nmodel to refrain from answering when uncertain.\nIt is intended to assess the model’s ability to recog-\nnize uncertainty and choose to abstain, as well as\nhow the presence of this option influences overall\nmodel confidence. For each question in the MedQA\nand AMBOSS datasets, a randomly positioned ab-\nstention option is added. Figure 1 ②illustrates\nadding the abstention option at a random position\nfor an example question.\nPerturbing This variant, also henceforth referred\n3https://huggingface.co/blog/\nleaderboard-medicalllm\n4https://www.amboss.com/us\n"}, {"page": 4, "text": "Figure 1: Overview of the MedAbstain evaluation pipeline. For each question, we begin with the original NA\nvariant ①and its options. An abstention option ②is inserted at a random position, forming the A variant. For\nperturbed variants, a SoTA LLM (gpt-4.1-mini) identifies and removes critical information (③) from the original\nquestion, making it more ambiguous; this yields the NAP variant. Adding the abstention option yields the AP\nVariant. For each variant, the model predicts the answer, and we extract logits/logprobs, shown as output bar charts.\nThe highlighted purple bar shows the abstention probability: with the complete question, the abstention option\nincreases model confusion; for the AP variant, uncertainty remains high, but the model favors abstention. The\nquantile threshold ˆq is set using 30% of the data as a calibration set and applied to the remaining 70%. This process\nis repeated for both open- and closed-source LLM families.\nto as NAP (No-Abstention + Perturbed Variant),\naims to assess the model’s confidence when es-\nsential information is missing. The questions are\nperturbed using GPT-4.1-mini to identify key de-\ntails required to arrive at the correct answer. These\ndetails are then removed, as depicted for an exam-\nple question in Fig 1 ③. Incomplete information\nreflects real clinical encounters, as patients seldom\npresent all relevant information and history in a sin-\ngle exchange. The model does not have the option\nto abstain with this dataset variant; we use it as\nthe reference for the subsequent abstention + per-\nturbation variant and hypothesize that the model’s\nuncertainty on this NAP variant will be higher than\non the NA variant baseline. More details on how a\ndataset is perturbed are discussed in Appendix B.\nAbstention + Perturbing This variant, also hence-\nforth referred to as AP, combines both abstention\nand perturbation. The model is presented with\nquestions that omit some necessary information,\nalong with the option to abstain from answering,\nas depicted in Fig 1 ④. This setup is designed to\nfurther challenge the model and examine whether\ncombining uncertainty with the ability to abstain\nreduces confidence and increases the tendency to\nabstain.\n3.2\nEvaluation Metrics\nThe models are evaluated using the following met-\nrics for each dataset and its variants.\nAccuracy\nAccuracy measures how often the\nmodel’s top prediction matches the correct label.\nConformal Prediction\nConformal Prediction\n(CP) provides a statistically rigorous way to quan-\ntify uncertainty (Angelopoulos and Bates, 2021).\nGiven a model f and a test instance xt, we compute\na prediction set C(xt) ⊆Y of plausible answers\n"}, {"page": 5, "text": "such that:\nP(yt ∈C(xt)) ≥1 −α\nwhere α is a user-set error rate. The size of the\nprediction set, or Set Size (SS), reflects the model’s\nconfidence: |C(xt)| = 1 implies the highest confi-\ndence, and larger sets reflect higher uncertainty.\nWe compute conformal scores using both the\nLeast Ambiguous Classifier (LAC) and Adaptive\nPrediction Set (APS) scoring functions:\n1) Adaptive Prediction Set (APS)\nAPS: s(x, y) =\nX\ny′:f(x)y′≥f(x)y\nf(x)y′\n2) Least Ambiguous Classifier (LAC)\nLAC: s(x, y) = 1 −f(x)y\nwhere f(x)y is the probability assigned to label\ny. Using a calibration set, we compute a quantile\nthreshold ˆqα and define the conformal prediction\nset for each test instance x as:\nC(x) = {y ∈Y | s(x, y) ≤ˆqα}\nwhere ˆqα is the (1 −α) quantile of calibration\nscores.\nLAC measures the size of the prediction set, re-\nflecting model uncertainty; larger sets typically in-\ndicate lower accuracy. APS measures the confi-\ndence and ranking quality of predictions, capturing\nhow well correct answers are prioritized within the\nset.\nAbstention Rate\nAbstention rate is the percent-\nage of test instances where the model outputs the\nabstention option. We report this value for the Ab-\nstention and Perturbed Abstention dataset variants.\n4\nExperiments\n4.1\nExperiment Models\nWe evaluate a broad set of both open-source and\nclosed-source LLMs, spanning multiple architec-\ntural families and model scales. This diverse selec-\ntion allows us to assess the generality of abstention\nand uncertainty behaviors across different LLM\nparadigms. For a full list of all models and config-\nurations, please refer to Appendix E.\n4.2\nExperimental Settings\nAll models are evaluated across four distinct ex-\nperimental settings, applied consistently across all\ndataset variants introduced in Section 3.1. These\nsettings are as follows:\nZero-shot setting\nIn the zero-shot setting, the\nmodel is presented with the question and answer\nchoices and instructed to make a prediction without\nany examples.\nFew-shot Setting\nIn the few-shot experiments,\nmodels receive several semantically relevant ex-\nample QA pairs for each test question, selected\ndynamically based on embedding-space similarity.\nWe use a fixed number of examples across all vari-\nants, and the sampling and selection procedures are\ndescribed in detail in Appendix F.\nChain-of-thought reasoning\nIn this setting, the\nmodel is instructed to reason step-by-step before\nselecting an answer, following prior work on chain-\nof-thought prompting (Wei et al., 2022). This set-\nting is intended to evaluate whether encouraging\nintermediate reasoning affects the model’s confi-\ndence or its ability to abstain.\nThinking mode - Reasoning Models Only\nTo\nfurther investigate the impact of internal reasoning\nmechanisms on the behavior of reasoning models,\nwe evaluate Qwen models with the “thinking mode”\nenabled and disabled. This comparison allows us to\nassess how internal reasoning influences both confi-\ndence calibration and abstention behavior. Closed-\nsource OpenAI models, such as o4, are excluded\nfrom this part of the study, as OpenAI does not\nexpose log-probabilities for its reasoning models,\nwhich are required for conformal prediction-based\nevaluation.\n4.3\nExperiment setup\nFor each experimental condition, models are\nprompted to output a single answer token (the se-\nlected option), and accuracy is computed by com-\nparing it with the gold label. The logit correspond-\ning to the emitted token, together with the logits\nfor the remaining candidate choices, is then ex-\ntracted to compute conformal-prediction scores.\nFor closed-source GPT-family models, these scores\nare derived from the API-exposed top-logprobs.\n4.3.1\nConformal Prediction Setup\nWe follow the methodology from Ye et al. (2024)\nto compute prediction sets using conformal predic-\ntion.\n• We set the coverage threshold α = 0.1, tar-\ngeting a 90% coverage guarantee: P(y ∈\nC(x)) ≥0.9. This means that the probability\n"}, {"page": 6, "text": "of the true correct answer being present in the\nprediction set is at least 0.9.\n• Each dataset is split into a calibration set\n(30%) taking into account the dataset size and\na test set (70%) by stratified random sampling.\nConformal scores are computed using the cal-\nibration set.\n• We compute conformal scores using both\nthe Least Ambiguous Classifier (LAC) and\nAdaptive Prediction Set (APS) scoring func-\ntions, and for each test instance, we evaluate\nthe Set Size (SS) of the prediction set (See\nSection 3.2).\n5\nResults and Discussion\nStudying the results, it is observed that uncertainty\nestimation using set size is a reliable indicator of\nthe model’s confidence in its generation and can be\nused as a signal to determine whether the model\nshould abstain from generating an answer. Across\nexperiments, both LAC and APS are negatively\ncorrelated with accuracy and positively correlated\nwith abstention, validating the stated hypothesis.\nFigure 2: Amboss: Comparing performance across Med-\nAbstain variants. The abstention option has the highest\nimpact on the model’s uncertainty as can be observed\nfrom A and AP variants.\n5.1\nPerformance across benchmark variants\nFigures 2 and 3 illustrate the relationship be-\ntween a model’s uncertainty, as demonstrated by\nAPS(green), LAC(orange), to abstention(red) and\naccuracy(blue) bars averaged for all the models\nacross both datasets. Generally, the largest increase\nin both abstention and set sizes is observed in the\nAP setting, and the smallest in the NAP setting,\nFigure 3: MedQA: Comparing performance across Med-\nAbstain variants. The abstention option has the highest\nimpact on the model’s uncertainty as can be observed\nfrom A and AP variants.\nFigure 4: Amboss: Zeroshot vs Fewshot settings com-\nparison. Few-shot gives modest accuracy gains while\nslightly tightening LAC (APS ≈0), especially under\nCoT. ots = median ∆x, bars = IQR\nsuggesting that making a model abstention-aware\ncan improve its ability to abstain. Perturbing, on\nthe other hand, has a comparatively lower impact\non the model’s ability to abstain.\n5.2\nZero-shot vs Few-shot\nFigures 4 and 5 illustrate the performance of few-\nshot over zero-shot in improving the model’s ability\nto abstain for the amboss and medqa datasets, re-\nspectively. As can be observed from the images,\nthe gains in abstention rate are negligible and may\nnot be an effective tool for enabling a model to\nabstain from the multiple-choice format.\n5.3\nCoT vs. No CoT\nSimilar to the few-shot setting, Chain-of-Thought\nhas little impact on accuracy or the model’s ability\nto abstain across both datasets, as can be observed\n"}, {"page": 7, "text": "Figure 5: MedQA: Zeroshot vs Fewshot settings com-\nparison. Few-shot improves accuracy marginally with\nthe highest in A CoT—and often shrinks LAC under\nCoT (APS ≈0); dots = median ∆x, bars = IQR.\nin Figures 6 and 7. There are negligible improve-\nments in abstention rates and accuracy, suggesting\nthat CoT reasoning alone is likely insufficient for\nenabling effective abstention in LLMs.\nFigure 6: Amboss: Cot vs NoCot settings comparison.\nCoT yields no accuracy change and slightly larger LAC\nacross A/NAP/AP (APS ≈0, variable); dots = median\n∆x, bars = IQR.\n5.4\nPerformance across models\nFor most models across datasets, larger set sizes\n(LAC/APS) are associated with lower accuracy, as\nshown in the images (Figs. 14, 15). However, this\nhas notable exceptions; there is an increase in both\naccuracy and LAC set size for gpt-4.1 from NA to\nA setting for the MedQA CoT few-shot setting, as\ncan be seen from Table 5. Similarly, for gpt-4.1 for\nAMBOSS CoT, few-shot setting from NA to A, as\ncan be observed here Table 2. These observations\nhighlight the need for further investigation into\nthe calibration of other closed-source models and\nlarger open-source models.\nFigure 7: MedQA: Cot vs NoCot settings comparison.\nCoT has negligible impacts on both accuracy and set\nsizes; dots = median ∆x, bars = IQR.\n5.5\nQwen thinking vs no thinking\nFigure 8: Comparison: Thinking Enabled vs NoThink-\ning Enabled (median ± IQR). Thinking reduces set\nsizes, slightly improves accuracy, and reduces absten-\ntion for both MedQA and AMBOSS.\nAcross both datasets, AMBOSS and MedQA,\nenabling thinking yields negligible impacts on\naccuracy and set sizes, as shown in Figure 8.\nThere are small accuracy gains and tighter sets, as\nshown by LAC, indicating that the thinking mode\nimproves the model’s reasoning capabilities and\nmakes it more confident. An exception emerges on\nMedQA–AP, where LAC shows a slight increase.\nAPS effects are more heterogeneous: near-zero on\nAmboss but higher under MedQA–NAP/AP, sug-\ngesting lower confidence in the predictions in this\nset. Abstention rate, however, decreases consis-\ntently across both datasets, despite having a small\nimpact, suggesting that thinking mode reduces the\n"}, {"page": 8, "text": "Figure 9: Accuracy vs LAC by mode. Negative correla-\ntion between Accuracy and LAC Set Size.\nFigure 10: Accuracy vs APS by mode. Negative Corre-\nlation between Accuracy and APS Set Size\nmodel’s ability to abstain even when it is more\nconfident.\nThis behaviour is slightly similar to the CoT vs\nNoCoT observations with minimal accuracy gains,\nslightly smaller sets, and less likely to abstain than\nno thinking mode or no CoT mode, in line with\nprevious work (Kirichenko et al., 2025)\n5.6\nAccuracy - Uncertainty (Set Size)\nRelationship\nOverall, there is a negative correlation between ac-\ncuracy and LAC, as shown in Figure 9, suggesting\nthat increased uncertainty is associated with lower\nmodel performance. A similar trend can also be\nobserved from the correlation between accuracy\nand APS Figure 10, reinforcing the hypothesis of\nnegative correlation between uncertainty and cor-\nrectness, thereby making it a suitable metric for\nstudying abstention.\n5.7\nHuman Evaluation Results\nWe conduct a human evaluation of a subset of\nmodel outputs to assess the clinical validity of the\nperturbation strategy (§ 3.1) and its implications for\nabstention in the presence of missing information.\nFull annotation guidelines and extended analyses\nare provided in Appendix H.\nPerturbation is designed to simulate clinically re-\nalistic ambiguity by removing information needed\nfor a confident, safe decision. Annotators rated\nthe importance of the removed context on a 1–3\nscale (1=irrelevant, 3=essential). Across all labeled\ninstances, the removed context achieved a mean\nimportance score of 2.388 with a median of 3, indi-\ncating that perturbations typically remove clinically\nessential information.\nAnnotators also judged whether abstention was\nthe medically appropriate action given the per-\nturbed question. Abstention was deemed appro-\npriate in 77.55% of labeled cases, and these judg-\nments exhibited a strong monotonic relationship\nwith context importance: over 90% of cases with\nmoderately or highly important missing informa-\ntion (importance ≥2) were labeled as requiring\nabstention. This confirms that the perturbation pro-\ncedure reliably induces scenarios where abstention\nis clinically justified.\nComparing model behavior against human ab-\nstention judgments on the perturbed, abstention-\nenabled subset, model abstention achieves a preci-\nsion of 71.43% and a recall of 13.16%. This indi-\ncates that while the model rarely abstains unneces-\nsarily, it often fails to abstain when abstention is\nclinically warranted, highlighting substantial head-\nroom for improving uncertainty-aware decision-\nmaking.\n6\nConclusion\nIn this work, we introduce MedAbstain to investi-\ngate the impact of introducing an abstention mech-\nanism on a model’s uncertainty, its ability to select\nthe abstention option, and the relationship between\nmodel uncertainty and abstention frequency. Our\nempirical analysis reveals a strong positive cor-\nrelation between uncertainty and abstention rate,\nindicating that equipping models with abstention-\nawareness is a promising approach to mitigating\nhallucinations by enabling models to abstain when\nuncertain. Furthermore, our results demonstrate\nthat the inclusion of an abstention option exerts a\ngreater influence on both uncertainty calibration\n"}, {"page": 9, "text": "and the model’s ability to refrain from providing\nunreliable outputs than input perturbations alone.\nNotably, combining abstention-awareness with per-\nturbations yields an even stronger effect. These\nfindings provide important insights into leveraging\nabstention-aware mechanisms to improve model\nreliability, offering a foundation for future research\naimed at enhancing uncertainty-aware abstention\nstrategies and abstention generally.\n7\nLimitations\nDespite MedAbstain’s comprehensive design for\nevaluating abstention and uncertainty in medical\nmultiple-choice QA, several limitations should be\nacknowledged. First, MedAbstain is restricted to\nEnglish-language datasets, which may not fully\nreflect the challenges faced in multilingual or non-\nEnglish medical contexts. Future work should ex-\ntend the benchmark to additional languages and\nhealthcare systems to ensure broader applicability.\nSecond, while we include both open- and closed-\nsource LLMs across multiple architectural families\nand scales, the coverage is necessarily finite. As\nmodel capabilities and training paradigms rapidly\nevolve, the performance and behavior reported here\nmay not generalize to future or as-yet-unreleased\nmodels.\nThird, our methodology focuses primarily on\nmultiple-choice QA, leveraging the well-defined\nlabel space to facilitate conformal prediction and\nabstention analysis. This may not capture the full\ncomplexity of real-world clinical reasoning or open-\nended medical tasks, where uncertainty and absten-\ntion manifest differently. Extending the MedAb-\nstain framework for abstention-aware evaluation to\ngenerative, free-form, or multi-modal medical tasks\nremains an important direction for future work.\nFourth, the introduction of adversarial perturba-\ntions and abstention options, while systematic, may\nnot exhaustively cover all clinically relevant am-\nbiguities or uncertainty scenarios. There may be\nreal-world cases where abstention is warranted but\nnot represented in our current protocols.\nFinally, for black-box models, our approach\nrelies on API-exposed confidence scores or log-\nprobabilities, which may be subject to implemen-\ntation artifacts or undocumented calibration proce-\ndures. Thus, uncertainty quantification for closed-\nsource models remains an open technical challenge.\n8\nEthics Statement\nThis work evaluates large language models for\nmedical question answering through the lens of\nabstention and uncertainty, using publicly avail-\nable benchmark datasets (MedQA) and a propri-\netary clinical QA dataset (AMBOSS). The MedQA\ndataset is fully open and distributed for research\npurposes, while the AMBOSS dataset is private\nand cannot be released publicly due to licensing\nrestrictions; it is used solely for internal bench-\nmarking and model evaluation within the terms of\nour research agreement.\nNo patient-identifiable or private clinical data are\nused, and all experimental protocols are consistent\nwith the ethical use of synthetic or de-identified\nmedical exam data. Our study aims to improve\nthe safety and reliability of LLMs in high-stakes\napplications, such as clinical decision support, by\nmitigating risks arising from overconfidence and\nhallucination. MedAbstain, including its dataset\nvariants and analysis tools, is intended for research\npurposes only and should not be deployed directly\nfor clinical care or patient-facing applications.\nWe note that while uncertainty-aware abstention\nmay reduce the risk of harmful errors, it does not\neliminate the possibility of bias or inaccuracy, par-\nticularly as LLMs can reflect biases present in their\ntraining data or benchmarks. The presence of an\nabstention mechanism should not be interpreted\nas a substitute for rigorous clinical validation or\nhuman oversight. All models and APIs used in this\nwork are unmodified off-the-shelf versions, and any\ndownstream use of the released benchmark should\ncomply with the respective licenses and terms of\nservice.\nWe release the MedAbstain codebase for re-\nsearch and transparency under the CC-BY-NC\n4.0 license, with the goal of fostering continued\nprogress on trustworthy and responsible AI for\nmedicine. The AMBOSS dataset is not included in\nthis release.\nReferences\nMoloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana\nRezazadegan, Li Liu, Mohammad Ghavamzadeh,\nPaul Fieguth, Xiaochun Cao, Abbas Khosravi, U Ra-\njendra Acharya, et al. 2021. A review of uncertainty\nquantification in deep learning: Techniques, appli-\ncations and challenges. Information fusion, 76:243–\n297.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\n"}, {"page": 10, "text": "Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nAlfonso Amayuelas, Kyle Wong, Liangming Pan,\nWenhu Chen, and William Wang. 2023. Knowledge\nof knowledge: Exploring known-unknowns uncer-\ntainty with large language models. arXiv preprint\narXiv:2305.13712.\nAnastasios Angelopoulos, Stephen Bates, Jitendra Ma-\nlik, and Michael I Jordan. 2020. Uncertainty sets for\nimage classifiers using conformal prediction. arXiv\npreprint arXiv:2009.14193.\nAnastasios N Angelopoulos and Stephen Bates. 2021.\nA gentle introduction to conformal prediction and\ndistribution-free uncertainty quantification. arXiv\npreprint arXiv:2107.07511.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen,\nAnna Goldie,\nAzalia Mirhoseini,\nCameron McKinnon, et al. 2022.\nConstitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nFaeze Brahman, Sachin Kumar, Vidhisha Balachan-\ndran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha\nRavichander, Sarah Wiegreffe, Nouha Dziri, Khyathi\nChandu, Jack Hessel, et al. 2024. The art of saying\nno: Contextual noncompliance in language models.\nAdvances in Neural Information Processing Systems,\n37:49706–49748.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\nvey on evaluation of large language models. ACM\ntransactions on intelligent systems and technology,\n15(3):1–45.\nLida Chen, Zujie Liang, Xintao Wang, Jiaqing Liang,\nYanghua Xiao, Feng Wei, Jinglei Chen, Zhenghong\nHao, Bing Han, and Wei Wang. 2024.\nTeach-\ning large language models to express knowledge\nboundary from their own signals. arXiv preprint\narXiv:2406.10881.\nXi Chen, Huahui Yi, Mingke You, WeiZhi Liu, Li Wang,\nHairui Li, Xue Zhang, Yingman Guo, Lei Fan, Gang\nChen, et al. 2025. Enhancing diagnostic capability\nwith multi-agents conversational large language mod-\nels. NPJ digital medicine, 8(1):159.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nNicolas Deutschmann, Marvin Alberts, and María Ro-\ndríguez Martínez. 2024. Conformal autoregressive\ngeneration: Beam search with coverage guarantees.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 38, pages 11775–11783.\nSebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and\nYarin Gal. 2024. Detecting hallucinations in large\nlanguage models using semantic entropy. Nature,\n630(8017):625–630.\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya,\nFrédéric Blain, Francisco Guzmán, Mark Fishel,\nNikolaos Aletras, Vishrav Chaudhary, and Lucia Spe-\ncia. 2020. Unsupervised quality estimation for neural\nmachine translation. Transactions of the Association\nfor Computational Linguistics, 8:539–555.\nJakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,\nMohsin Ali, Jongseok Lee, Matthias Humt, Jianxi-\nang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung,\nRibana Roscher, et al. 2023. A survey of uncertainty\nin deep neural networks. Artificial Intelligence Re-\nview, 56(Suppl 1):1513–1589.\nAidan Gilson, Conrad W Safranek, Thomas Huang,\nVimig Socrates, Ling Chi, Richard Andrew Taylor,\nDavid Chartash, et al. 2023. How does chatgpt per-\nform on the united states medical licensing exami-\nnation (usmle)? the implications of large language\nmodels for medical education and knowledge assess-\nment. JMIR medical education, 9(1):e45312.\nNeel Guha, Julian Nyarko, Daniel Ho, Christopher Ré,\nAdam Chilton, Alex Chohlas-Wood, Austin Peters,\nBrandon Waldon, Daniel Rockmore, Diego Zam-\nbrano, et al. 2023. Legalbench: A collaboratively\nbuilt benchmark for measuring legal reasoning in\nlarge language models. Advances in neural informa-\ntion processing systems, 36:44123–44279.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948.\nMengting Hu, Zhen Zhang, Shiwan Zhao, Minlie\nHuang, and Bingzhe Wu. 2023. Uncertainty in natu-\nral language processing: Sources, quantification, and\napplications. arXiv preprint arXiv:2306.04459.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025.\nA survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.\nACM Transactions on Information Systems, 43(2):1–\n55.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022.\nLanguage models\n(mostly) know what they know.\narXiv preprint\narXiv:2207.05221.\n"}, {"page": 11, "text": "Sanyam Kapoor, Nate Gruver, Manley Roberts, Katie\nCollins, Arka Pal, Umang Bhatt, Adrian Weller,\nSamuel Dooley, Micah Goldblum, and Andrew G\nWilson. 2024. Large language models must be taught\nto know what they don’t know. Advances in Neural\nInformation Processing Systems, 37:85932–85972.\nPolina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri,\nand Samuel J Bell. 2025. Abstentionbench: Rea-\nsoning llms fail on unanswerable questions. arXiv\npreprint arXiv:2506.09038.\nJannik Kossen, Jiatong Han, Muhammed Razzak, Lisa\nSchut, Shreshth Malik, and Yarin Gal. 2024. Seman-\ntic entropy probes: Robust and cheap hallucination\ndetection in llms. arXiv preprint arXiv:2406.15927.\nBhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu,\nDavid Bellamy, Ramesh Raskar, and Andrew Beam.\n2023.\nConformal prediction with large language\nmodels for multi-choice question answering. arXiv\npreprint arXiv:2305.18404.\nYongchan Kwon, Joong-Ho Won, Beom Joon Kim, and\nMyunghee Cho Paik. 2020. Uncertainty quantifica-\ntion using bayesian neural networks in classification:\nApplication to biomedical image segmentation. Com-\nputational Statistics & Data Analysis, 142:106816.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in neu-\nral information processing systems, 33:9459–9474.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,\nYan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and\nZhaopeng Tu. 2024. Encouraging divergent thinking\nin large language models through multi-agent debate.\nIn Proceedings of the 2024 conference on empiri-\ncal methods in natural language processing, pages\n17889–17904.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTeaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\narXiv preprint arXiv:2308.09583.\nJingyuan Ma, Damai Dai, Zihang Yuan, Weilin Luo,\nBin Wang, Qun Liu, Lei Sha, Zhifang Sui, et al. 2024.\nLarge language models struggle with unreasonability\nin math problems. arXiv preprint arXiv:2403.19346.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. Advances in Neural Information Pro-\ncessing Systems, 36:46534–46594.\nNishanth Madhusudhan, Sathwik Tejaswi Madhusud-\nhan, Vikas Yadav, and Masoud Hashemi. 2024. Do\nllms know when to not answer? investigating ab-\nstention abilities of large language models. arXiv\npreprint arXiv:2407.16221.\nPrakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun\nOuyang, Beining Wang, Vidhi Dhaval Mody, and\nHong Yu. 2024. Synfac-edit: Synthetic imitation edit\nfeedback for factual alignment in clinical summariza-\ntion. arXiv preprint arXiv:2402.13919.\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xi-\nang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Candès, and\nTatsunori Hashimoto. 2025. s1: Simple test-time\nscaling. arXiv preprint arXiv:2501.19393.\nPaul D Myers, Kenney Ng, Kristen Severson, Uri Kar-\ntoun, Wangzhi Dai, Wei Huang, Frederick A Ander-\nson, and Collin M Stultz. 2020. Identifying unreli-\nable predictions in clinical risk models. NPJ digital\nmedicine, 3(1):8.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carig-\nnan, Richard Edgar, Nicolo Fusi, Nicholas King,\nJonathan Larson, Yuanzhi Li, Weishung Liu, et al.\n2023. Can generalist foundation models outcom-\npete special-purpose tuning? case study in medicine.\narXiv preprint arXiv:2311.16452.\nMahmud Omar, Vera Sorin, Jeremy D Collins, David\nReich, Robert Freeman, Nicholas Gavin, Alexan-\nder Charney, Lisa Stump, Nicola Luigi Bragazzi,\nGirish N Nadkarni, et al. 2025.\nMulti-model as-\nsurance analysis showing large language models are\nhighly vulnerable to adversarial hallucination attacks\nduring clinical decision support. Communications\nMedicine, 5(1):330.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in neural in-\nformation processing systems, 35:27730–27744.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on health,\ninference, and learning, pages 248–260. PMLR.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in neural\ninformation processing systems, 36:53728–53741.\nRahul Rahaman et al. 2021. Uncertainty quantification\nand deep ensembles. Advances in neural information\nprocessing systems, 34:20063–20075.\nAM Rahman, Junyi Ye, Wei Yao, Sierra S Liu, Jesse Yu,\nJonathan Yu, Wenpeng Yin, and Guiling Wang. 2024.\n"}, {"page": 12, "text": "From blind solvers to logical thinkers: Benchmark-\ning llms’ logical integrity on faulty mathematical\nproblems. arXiv preprint arXiv:2410.18921.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, et al. 2024. Deepseekmath:\nPushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H Chi, Nathanael Schärli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning, pages\n31210–31227. PMLR.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. arXiv preprint\narXiv:2104.07567.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher D Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback. arXiv preprint arXiv:2305.14975.\nChristian Tomani, Kamalika Chaudhuri, Ivan Evti-\nmov, Daniel Cremers, and Mark Ibrahim. 2024.\nUncertainty-based abstention in llms improves\nsafety and reduces hallucinations. arXiv preprint\narXiv:2404.10960.\nHieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao\nYang, Feiyun Ouyang, Shuo Han, Razieh Rahimi,\nand Hong Yu. 2025a. Prime: Planning and retrieval-\nintegrated memory for enhanced reasoning. arXiv\npreprint arXiv:2509.22315.\nHieu Tran, Zonghai Yao, Zhichao Yang, Junda Wang,\nYifan Zhang, Shuo Han, Feiyun Ouyang, and Hong\nYu. 2025b. Rare: Retrieval-augmented reasoning\nenhancement for large language models. In Proceed-\nings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 18305–18330.\nNeeraj Varshney and Chitta Baral. 2023.\nPost-\nabstention:\nTowards reliably re-attempting the\nabstained\ninstances\nin\nqa.\narXiv\npreprint\narXiv:2305.01812.\nRoman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev,\nLyudmila Rvanova, Daniil Vasilev, Akim Tsvigun,\nSergey Petrakov, Rui Xing, Abdelrahman Sadallah,\nKirill Grishchenkov, et al. 2025. Benchmarking un-\ncertainty quantification methods for large language\nmodels with lm-polygraph. Transactions of the Asso-\nciation for Computational Linguistics, 13:220–248.\nBenlu Wang, Iris Xia, Yifan Zhang, Junda Wang, Feiyun\nOuyang, Shuo Han, Arman Cohan, Hong Yu, and\nZonghai Yao. 2025. From scores to steps: Diag-\nnosing and improving llm performance in evidence-\nbased medical calculations. In Proceedings of the\n2025 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10820–10844.\nJunda Wang, Zhichao Yang, Zonghai Yao, and Hong\nYu. 2024.\nJmlr: Joint medical llm and retrieval\ntraining for enhancing reasoning and professional\nquestion answering capability.\narXiv preprint\narXiv:2402.17887.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\net al. 2022. Taxonomy of risks posed by language\nmodels. In Proceedings of the 2022 ACM conference\non fairness, accountability, and transparency, pages\n214–229.\nBingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu,\nYulia Tsvetkov, Bill Howe, and Lucy Lu Wang. 2025.\nKnow your limits: A survey of abstention in large\nlanguage models. Transactions of the Association for\nComputational Linguistics, 13:529–556.\nLisa Wimmer, Yusuf Sale, Paul Hofman, Bernd Bischl,\nand Eyke Hüllermeier. 2023. Quantifying aleatoric\nand epistemic uncertainty in machine learning: Are\nconditional entropy and mutual information appro-\npriate measures? In Uncertainty in artificial intelli-\ngence, pages 2282–2292. PMLR.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\nBloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564.\nYuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen\nKan, Timothy P Lillicrap, Kenji Kawaguchi, and\nMichael Shieh. 2024. Monte carlo tree search boosts\nreasoning via iterative preference learning. arXiv\npreprint arXiv:2405.00451.\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong\nZhang. 2024. Benchmarking retrieval-augmented\ngeneration for medicine. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n6233–6251.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie\nFu, Junxian He, and Bryan Hooi. 2023. Can llms\nexpress their uncertainty? an empirical evaluation\nof confidence elicitation in llms.\narXiv preprint\narXiv:2306.13063.\n"}, {"page": 13, "text": "Zhichao Yang, Zonghai Yao, Mahbuba Tasmin, Parth\nVashisht, Won Seok Jang, Feiyun Ouyang, Beining\nWang, David McManus, Dan Berlowitz, and Hong\nYu. 2025. Unveiling gpt-4v’s hidden challenges be-\nhind high accuracy on usmle questions: Observa-\ntional study. Journal of Medical Internet Research,\n27:e65146.\nZonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok\nJang, Feiyun Ouyang, Zhichao Yang, and Hong Yu.\n2025. Mcqg-srefine: Multiple choice question gener-\nation and evaluation with iterative self-critique, cor-\nrection, and comparison feedback. In Proceedings\nof the 2025 Conference of the Nations of the Amer-\nicas Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume\n1: Long Papers), pages 10728–10777.\nZonghai Yao and Hong Yu. 2025. A survey on llm-\nbased multi-agent ai hospital.\nZonghai Yao, Zihao Zhang, Chaolong Tang, Xingyu\nBian, Youxia Zhao, Zhichao Yang, Junda Wang,\nHuixue Zhou, Won Seok Jang, Feiyun Ouyang, et al.\n2024. Medqa-cs: Benchmarking large language mod-\nels clinical skills using an ai-sce framework. arXiv\npreprint arXiv:2410.01553.\nFanghua Ye, Mingming Yang, Jianhui Pang, Longyue\nWang, Derek F. Wong, Emine Yilmaz, Shuming Shi,\nand Zhaopeng Tu. 2024. Benchmarking llms via\nuncertainty quantification. In Advances in Neural\nInformation Processing Systems, volume 37, pages\n15356–15385. Curran Associates, Inc.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know? arXiv\npreprint arXiv:2305.18153.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng,\nXiaonan Li, Junqi Dai, Qinyuan Cheng, Xuan-Jing\nHuang, and Xipeng Qiu. 2024. Reasoning in flux:\nEnhancing large language models reasoning through\nuncertainty-aware adaptive guidance. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2401–2416.\nArmel Zebaze, Benoît Sagot, and Rachel Bawden. 2024.\nIn-context example selection via similarity search\nimproves low-resource machine translation. arXiv\npreprint arXiv:2408.00397.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. Star: Bootstrapping reasoning with rea-\nsoning. Advances in Neural Information Processing\nSystems, 35:15476–15488.\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue,\nYuxiao Dong, and Jie Tang. 2024. Rest-mcts*: Llm\nself-training via process reward guided tree search.\nAdvances in Neural Information Processing Systems,\n37:64735–64772.\nSheng Zhang, Qianchu Liu, Guanghui Qin, Tris-\ntan Naumann, and Hoifung Poon. 2025.\nMed-\nrlvr: Emerging medical reasoning from a 3b base\nmodel via reinforcement learning. arXiv preprint\narXiv:2502.19655.\nA\nDataset Creation\nThe MedQA dataset included 1007 test examples,\nwhich we used to generate all variants across all\nexperiments. The AMBOSS dataset provides a\nsplit based on difficulty level. We sampled 200\nquestions from each of the 5 difficulty levels to\ncreate a test set of 1000 instances. The validation\nset used for few-shot tuning was created by ran-\ndomly sampling 100 instances from the provided\nvalidation splits of both datasets. Again, for AM-\nBOSS, 20 questions were sampled from the vali-\ndation sets of each difficulty level. The few-shot\npools from which the dynamic few-shot examples\nwere selected were created using the train data split\nof the datasets.\nB\nDataset perturbation\nFor each dataset, we construct a perturbed split to\nisolate the effect of unknown information on ab-\nstention. For every multiple-choice question, we\nremove the gold context (the single most informa-\ntive clue) while preserving the label. We prompt\nan LLM (GPT-4.1-mini) to (i) list the key facts in\nthe question, (ii) identify the fact whose absence\nwould most hinder deriving the known correct an-\nswer, and (iii) rewrite the question with only that\nfact removed. The model returns a structured re-\nsponse (key facts, selected gold context, brief ratio-\nnale, and the edited question), which we parse to\ncreate a new record that retains the original options\nand answer, stores the original question, and anno-\ntates metadata describing what was removed and\nwhy. The resulting perturbed datasets enable con-\ntrolled evaluation of the model’s ability to abstain\nunder scenarios where the model does not have the\nrequired information.\nC\nFew-shot pool generation\nTo support few-shot evaluation, exemplar pools are\nconstructed exclusively from the training split to\navoid any test-set exposure. From this base pool,\nwe derive four experimental conditions:\n• No-Abstention (NA) The pool comprises un-\nmodified training items.\n"}, {"page": 14, "text": "Figure 11: Accuracy, LAC set size (inverted axis) and\nAPS set size (inverted axis) across values of k =\n1, 2, 3, 4, 5. The inverted axis for the set size allows\nus to easily determine that points higher on the y-axis\nare considered better in all subplots - higher accuracy,\nlower uncertainty.\n• Abstention (A) Each item is augmented with\nan explicit “Abstain” option; gold labels re-\nmain unchanged.\n• Perturbed–No-Abstention (P-NA) Training\nitems are first perturbed as described above.\nThe final pool is a balanced mixture of 50%\nperturbed and 50% original items to equalize\nexposure to both formats.\n• Perturbed–Random-Abstention\n(P-RandAbst).\nSimilar\nto\nthe\nANP\nsetting above, the pool is created with a\ncombination of 50% from the original pool\nand 50% from the perturbed pool. Post that,\na random 50% subset of perturbed items is\nrelabeled such that “Abstain” is the correct\nresponse (i.e., the original correct option is\nreplaced by an abstain option), encouraging\nthe model to abstain when critical information\nis absent.\nD\nFew-shot tuning\nTo determine how many dynamic few-shot exam-\nples (see Appendix F for details) should be pro-\nvided to the test instances when running exper-\niments on the few-shot setting, we ran a set of\ntuning experiments on Llama-3.1-8B-Instruct\nusing a small set of 100 questions exclusively sam-\npled from the validation split. The resulting accu-\nracy, LAC set size, and APS set size are plotted\nacross all values of k in Figure 11. Based on these\nresults, k = 4 setting was chosen for all few-shot\nexperiments.\nE\nExperiment Models\nTo evaluate performance across varying model\nscales and architectural families, we benchmark\na diverse set of both open-source and closed-source\nmodels, listed below:\nOpen-source Models:\n• LLaMA Family: 5 6 Llama3.2-1B-Instruct,\nLlama3.2-3B-Instruct, Llama3.1-8B-Instruct\n• Phi Family: Phi-4-mini7, phi-48\n5https://huggingface.co/collections/\nmeta-llama/llama-32-66f448ffc8c32f949b04c8cf\n6https://huggingface.co/meta-llama/Llama-3.\n1-8B\n7https://huggingface.co/microsoft/\nPhi-4-mini-instruct\n8https://huggingface.co/microsoft/phi-4\n"}, {"page": 15, "text": "• Qwen Family:\n9\n10\nQwen2.5-0.5B-\nInstruct, Qwen2.5-1.5B-Instruct, Qwen2.5-\n3B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-\n14B-Instruct, Qwen2.5-32B-Instruct, Qwen3-\n0.6B, Qwen3-1.7B, Qwen3-4B, Qwen3-8B,\nQwen3-14B, Qwen3-32B\n• Gemma Family: gemma-3-4b11, medgemma-\n4b-it12\nClosed-source Models:\n• GPT Family: gpt-4.1-nano-2025-04-14, gpt-\n4o-mini-2024-07-18, gpt-4o-2024-08-06, gpt-\n4.1-2025-04-14\nF\nExperiment Few-shot setting details\nIn this setting, the model is prompted similarly\nto the zero-shot setup but is additionally provided\nwith a small number of semantically similar ex-\nample question-answer pairs (Zebaze et al., 2024).\nWe employ dynamic few-shot examples (Nori et al.,\n2023), i.e., for a given test instance, we select k\nsemantically similar examples from the train split\nof the respective dataset variant, determined us-\ning k-NN clustering based on cosine similarity in\nthe embedding space. The embeddings for test in-\nstances and training examples are generated using\ntext-embedding-ada-00213.\nWe use k = 4 dynamic few shot examples for all\ndataset variants. Appendix D describes the tuning\nprocedure used to select the value of k.\nTo mitigate potential bias toward or against se-\nlecting the abstention option, we modify the per-\nturbed abstention dataset variants by randomly sam-\npling 25% of the questions and replacing their cor-\nrect answer with the abstention option. More de-\ntails about this construction process are provided\nin Appendix C.\nG\nPrompts\nThe following prompts were used for zero-shot,\nfew-shot and cot settings:\n• Zero-shot prompt:\nf\"The\nfollowing\nis a multiple-choice question with\n9https://huggingface.co/collections/Qwen/\nqwen25-66e81a666513e518adb90d9e\n10https://huggingface.co/collections/Qwen/\nqwen3-67dd247413f0e2e4f653967f\n11https://huggingface.co/google/gemma-3-4b-it\n12https://huggingface.co/google/medgemma-4b-it\n13https://openai.com/index/\nnew-and-improved-embedding-model/\n{num_choices} potential answers. Only\none\nof\nthese\noptions\nis\ncorrect.\nPlease\nmake\nyour\nbest\neffort\nand\nselect the correct answer. You only\nneed to output the option.\"\n• Few-shot prompt:\n\"Below\nare\nsome\nexamples of multiple-choice questions\nalong with their associated options,\nwhich\nare\npotential\nanswers.\nFor\neach question,\nonly one option is\ncorrect.\"\n• CoT prompt:\nf\"The\nfollowing\nis\na\nmultiple-choice\nquestion\nwith\n{num_choices} potential answers. Only\none\nof\nthese\noptions\nis\ncorrect.\nPlease explain your reasoning step by\nstep and select the correct answer.\nYou only need to output the option.\"\nWe use combinations of the above prompts for\nthe respective experimental settings. For example,\na few-shot CoT experiment setting would use a\nfew-shot prompt, followed by few-shot examples,\na CoT prompt, and then the actual test instance 14.\nH\nHuman Evaluation\nH.1\nHuman Evaluation Setup\nHuman evaluation was conducted on a subset of 50\nmedical questions. Each question was presented\nin four variants: original vs. perturbed and with vs.\nwithout abstention enabled, resulting in a total of\n200 evaluated instances. The evaluation was for\nthe Qwen family of models.\nH.2\nAnnotation Guidelines\nTask 1: Importance of Removed Context (1–3\nScale)\nFor perturbed questions, annotators rated\nthe importance of the removed information for cor-\nrectly answering the original question.\n• 3 (Essential): The information is critical for\narriving at the correct answer.\n• 2 (Helpful): The information is useful but not\nstrictly necessary.\n• 1 (Irrelevant): The information is redundant\nor uninformative.\n14More\ndetails\ncan\nbe\nfound\nhere\nhttps://anonymous.4open.science/r/\nmed-llm-uncertainty-benchmark-5AFB/quantify_\nuncertainty/prompts/prompt_templates.py\n"}, {"page": 16, "text": "Task 2: Appropriateness of Abstention (Yes/No)\nAnnotators judged whether a human expert would\nabstain when answering the perturbed question.\n• Yes: A clinician would defer, request addi-\ntional information, or order further tests.\n• No: A clinician could reasonably answer with\nhigh confidence.\nH.3\nExtended Abstention Analysis\nAmong the 49 perturbed instances with abstention\nlabels, annotators judged abstention to be clinically\nappropriate in 38 cases (77.55%). When comparing\nmodel abstention decisions to human judgments on\nthe perturbed, abstention-enabled subset, the model\nabstained correctly in 5 cases and abstained unnec-\nessarily in 2 cases. This corresponds to an absten-\ntion precision of 71.43% and a recall of 13.16%.\nThese results indicate that, while model absten-\ntion is relatively conservative, it often fails to ab-\nstain when clinically warranted.\nH.4\nCoherence Score Distribution\nCoherence scores were collected for a limited sub-\nset of model configurations and examples. Rat-\nings primarily clustered around 2 and 3, suggesting\npartially coherent but incomplete reasoning. Due\nto sparse coverage and uneven annotation density,\nthese results are reported descriptively and are not\nused for quantitative comparison.\nI\nAdditional Results Discussion\nThis section consolidates additional discussions\nbased on the experiments. For medqa, Table 5 con-\nsolidates results across experiments for the COT\nsetting, and Table 6 consolidates them for the No-\nCOT setting. Tables 2 and 3 consolidate results for\nAmboss COT and NoCOT settings, respectively.\nThe experiments studying Qwen families, think-\ning mode enabled and disabled, are in Table 4 for\nAmboss and Table 7 for MedQA.\nFor each table, the darker the entry, the better\nacross all metrics. For accuracy, this means the\naccuracy is higher; for set sizes, this means the\nset size is smaller; and for the abstention rate, this\nmeans the abstention rate is higher.\nI.1\nAccuracy–set size relationships by regime\nAcross both datasets, the negative association be-\ntween accuracy and set size is stronger for LAC\nthan APS, as can be seen from Figure 10 and Fig-\nure 9, and it varies by regime, as can be seen from\nFigure 12: Accuracy v LAC by Regime mode\nFigure 13 and Figure 12. Abstention+Perturbed\n(AP) shows the steepest negative trend; A is milder;\nNAP is typically the weakest effect.\nFor APS, the CoT slope is, on average, more\nnegative than the NoCoT slope. For LAC, both\nmodes are negative and of similar magnitude, with\nsmall condition-specific shifts.\nFigure 13: Accuracy v APS by Regime mode\nI.2\nPerformance across benchmark variants\nFor both the datasets, as illustrated by the figures\n2 and 3 depicting the model’s accuracy, uncer-\ntainty(through set sizes), and abstention rate, the\nmodel’s uncertainty has a direct correlation with it\nbeing made abstention-aware. Set sizes increase in\nthe A and AP conditions across all panels. Both\nLAC (orange) and APS (green) are consistently\ngreater than zero for A and AP, with the AP variant\nproducing the largest increase. In contrast, NAP re-\nsults in a much smaller increase (often near zero for\nAPS), suggesting that abstention, rather than pertur-\nbation, is the primary driver of model uncertainty.\nThere are however exceptions to this behavior as\n"}, {"page": 17, "text": "can be observed from the Table 5 and Table 2 for\ngpt-4.1, there is an increase in both accuracy and\nset size from NA to A, indicating different calibra-\ntion resulting in an inverse correlation, demanding\nfurther investigation.\nAccuracy remains stable or shows mild degra-\ndation.\nThe blue medians for the A and NAP\nconditions hover near zero, whereas AP typically\nshows a slight negative shift. The interquartile\nranges (IQRs) are relatively narrow compared to\nthe spread seen in LAC and APS. Notably, MedQA\nshows slightly greater accuracy degradation than\nAMBOSS.\nThe direct correlation between the abstention\nrate and the increased set size indicates that uncer-\ntainty can serve as a signal enabling the model to\nabstain. There is a consistent increase for A and\nAP variants for both datasets.\nFew-shot prompting does not counteract the set-\nsize inflation observed under A and AP, and it in-\nduces only minor shifts in accuracy deltas. Simi-\nlarly, CoT prompting does not mitigate the inflation\nobserved under A and AP, indicating that explicit\nreasoning does not reduce the model’s uncertainty.\nOn MedQA, few-shot prompting tends to make the\naccuracy deltas slightly more negative.\nAmboss\nAs\nshown\nin\nFigure\n2,\nabsten-\ntion—particularly when combined with pertur-\nbation—substantially increases prediction set\nsizes, reflecting heightened model uncertainty.\nThe most pronounced increase occurs under the\nAP condition, followed by A, while NAP has a\nconsiderably smaller effect.\nThis supports the\nconclusion that abstention is the primary driver of\nuncertainty amplification. Accuracy, by contrast, is\naffected to a much lesser extent:\nδAcc(A −NA) ≈0\nδAcc(NAP −A) < 0\nδAcc(AP −NA) < 0\nAmong these, the AP–NA contrast is the most\nnegative, again aligning with the pattern that AP\nintroduces the greatest (though still modest) degra-\ndation in accuracy.\nMedQA\nA similar trend is observed for MedQA\nin Figure 3. Both LAC and APS increase under\nthe A and AP conditions, with AP producing the\nlargest inflation. While NAP also leads to larger set\nsizes, the effect is less pronounced than the other\nabstention-aware settings. In terms of accuracy,\nMedQA shows greater sensitivity than AMBOSS.\nThe largest drop in accuracy occurs under the AP\ncondition, followed by A, with NAP having the\nleast impact.\nI.3\nZero shot vs Few shot\nFew-shot seems to have a negligible impact on ab-\nstention and uncertainty; overall, a minimal im-\nprovement in accuracy can be observed with a\nslightly smaller set size for LAC (APS shows more\nvaried behavior). Marginal in both settings, it is\nmore prominent in the CoT setting, suggesting that\nfew-shot + CoT can improve the performance and\nlower the set size. However, the effect is heteroge-\nneous—some models in A/No-CoT show negligi-\nble or slightly negative accuracy deltas, as can be\nseen from Figure 4 and Figure 5; APS shifts are\ncentered near zero with wide IQRs; and in AP, es-\npecially under No-CoT, few-shot can increase LAC\n(wider sets). On MedQA specifically, the largest\naccuracy boost appears in AP with CoT, while AP\nunder No-CoT more often widens LAC; these ex-\nceptions are more common among smaller models\n(≤4–8B), which also exhibit greater dispersion.\nThere is a small increase in abstention rates from\nNA to A and from NA to AP, but the impact is\nsmall in both settings.\nAMBOSS\nAs can be seen from the Figure 4,\nFew-shot produces small positive median gains\nacross A/NAP/AP, in both No-CoT and CoT. Gains\nare largest under NAP/AP with CoT, but remain\nmodest overall (dots just to the right of 0 with tight\nIQRs).\nFew-shot tends to slightly shrink LAC (orange\nmedians left of 0) in both modes, with APS changes\ncentered near zero and wide IQRs, indicating\nmodel-to-model variability.\nOn Amboss, few-shot helps accuracy a bit and\ndoes not inflate sets; if anything, LAC is slightly\ntighter, especially when CoT is used.\nMedQA\nFor MedQA, Few-shot again yields pos-\nitive median gains for accuracy, with the largest\nboost under AP, especially in CoT (blue dot notice-\nably right of 0) as can be noted from the Figure 5.\nLAC generally shrinks under CoT (orange me-\ndians left of 0), while No-CoT shows smaller or\nmixed LAC shifts. APS medians sit near 0 with\nlong IQRs.\nOn MedQA, few-shot is consistently beneficial\nfor accuracy, and CoT+few-shot often pairs the\n"}, {"page": 18, "text": "gain with slight LAC tightening.\nI.3.1\nPerformance across models\nFor most models, across datasets, larger set sizes\n(LAC/APS) are generally associated with lower\naccuracy (Figs. 14, 15), with some notable excep-\ntions. At the top end, the GPT-4o family often\nmaintains near-zero or positive APS slopes and\nnear-zero LAC slopes—especially with CoT and\nfew-shot—breaking the usual trade-off. In contrast,\nGPT-4.1 shows consistently negative LAC (and typ-\nically negative APS), so larger sets align with lower\naccuracy for this model. Qwen3-32B and Gemma-\n3-27B-it look strongest in NoCoT (slopes ≈0 or\npositive), but CoT often pulls them toward zero or\nnegative.\nSmall–mid instruction models (e.g., Qwen25\n7–15B and smaller Llama-31/32 variants) exhibit\nnegative slopes across regimes; few-shot moves\nthem toward zero (better calibration) more reliably\nthan CoT. For these models, CoT widens sets but\nonly sometimes improves accuracy, making the ex-\ntra coverage less efficient. The negative coupling\nis stronger on MedQA (especially for LAC) than\non amboss; fewer models sustain near-neutral or\npositive APS on MedQA.\n• GPT-4o family: With CoT+few-shot, main-\ntains near-neutral LAC and non-negative APS\nslopes, i.e., modest set growth does not de-\ngrade accuracy.\n• GPT-4.1: Strongly negative LAC (and gen-\nerally negative APS), so larger sets correlate\nwith lower accuracy.\n• Qwen3-32B & Gemma-3-27B-it: Good in\nNoCoT (slopes ≈0 or positive) but drift to-\nward negative under CoT, attenuating the ad-\nvantage.\n• Small–mid instruction models: Negative\nslopes across regimes; few-shot improves cal-\nibration more consistently than CoT, while\nCoT often widens sets without commensurate\naccuracy gains.\nAmboss\nMost models exhibit negative slopes\nacross panels, especially for LAC, reaffirming that\nlarger sets tend to align with lower accuracy. Mov-\ning from zero to few-shot generally shifts models\ntoward less negative, indicating improved calibra-\ntion with a couple of examples; the effect is more\nvisible in NoCoT.\nUnder CoT, APS slopes are often more nega-\ntive than in NoCoT, consistent with reasoning pro-\nducing larger sets without commensurate accuracy\ngains for many models; LAC remains negative over-\nall.\nA small frontier group (e.g., GPT-4o variants)\nstays near-neutral on APS under CoT–few, suggest-\ning that modest set growth does not harm accuracy\nfor them. Dispersion grows under A/NAP/AP, re-\nflecting family-level heterogeneity.\nMedQA\nMedQA\nshows\na\nmore\nnega-\ntive\naccuracy–set-size\ncoupling\nthan\nAm-\nboss—particularly for LAC—across modes and\nshots. Few-shot still nudges slopes toward less\nnegative, yet the shift is smaller than on Amboss;\nmany models remain moderately negative even\nwith examples.\nAPS under CoT frequently becomes more neg-\native than in NoCoT, indicating that reasoning in-\ncreases set sizes without a consistent accuracy ben-\nefit in the harder MedQA setting. IQRs are widest\nin NAP/AP, underscoring that robustness stressors\nmagnify between-model differences.\nI.3.2\nQwen thinking vs nothinking\nAcross both datasets (Figure 8), enabling thinking\nyields negligible impact: small accuracy gains and\ntighter sets, as can be noted from LAC. An excep-\ntion emerges on MedQA–AP, where LAC shows\na slight increase. APS effects are more hetero-\ngeneous: near-zero on Amboss, but higher under\nMedQA–NAP/AP. The abstention rate decreases\nconsistently across both datasets, despite having a\nsmall impact.\nThe largest accuracy gains occur in the A setting\non both datasets. For LAC, the AP setting shows\nthe smallest reduction on Amboss and a slight in-\ncrease on MedQA. Overall, the reasoning mode\nappears to improve decision quality (higher accu-\nracy) and sharpen candidate sets (lower LAC); in\nnoisier regimes on MedQA (NAP/AP), it raises\nAPS, suggesting a trade-off of coverage for caution.\nEffects vary across model families, as reflected in\nthe wide IQRs.\nIn A/AP, AR decreases slightly (small negative\nmedians), NAP shows 0 by definition. Enabling\n“thinking” makes abstention a bit less likely when\nabstention is available.\nI.3.3\nExperiment Results\nThis section consolidates the results for the MedQA\nand AMBOSS datasets. The table 7 contains the\n"}, {"page": 19, "text": "Figure 14: Amboss figure averaging performance across all settings for all the models\nFigure 15: MedQA figure averaging performance across all settings for all the models\n"}, {"page": 20, "text": "results for the Qwen thinking mode, enabled, and\ndisabled evaluations for MedQA. Tables 5 and 6\ncontain the results for MedQA evaluations on the\nCoT and NoCoT settings, respectively.\nSimilarly, Table 4 contains the results for the\nQwen thinking mode: enabled/disabled evaluations\nfor AMBOSS. The tables: 2 and 3 display the ex-\nperiments on AMBOSS for the CoT and No CoT\nsettings.\n"}, {"page": 21, "text": "Table 2: AMBOSS: Experiment results for the Chain-of-thought setting. The darker the entry, the better across all\nevaluation metrics. (Higher accuracy, lower set size, higher abstention)\nNo Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\nLlama-31-8B-Instruct\nAccuracy\n0.5629\n0.5529\n0.5443\n0.5429\n0.4529\n0.4600\n0.4457\n0.4257\nLAC Set Size\n3.0857\n3.0557\n3.6057\n3.8100\n3.7586\n3.7743\n4.2586\n4.4400\nAPS Set Size\n3.9586\n3.8329\n4.3643\n4.4371\n4.0057\n4.0157\n4.6986\n4.4557\nAbstention Rate\n–\n–\n0.0143\n0.0114\n–\n–\n0.0300\n0.0857\nLlama-32-1B-Instruct\nAccuracy\n0.2814\n0.2814\n0.2686\n0.2400\n0.2557\n0.2286\n0.2514\n0.1986\nLAC Set Size\n4.8657\n4.9243\n5.5986\n5.9357\n4.7729\n4.9457\n5.3543\n5.9386\nAPS Set Size\n5.0214\n5.5743\n5.4000\n6.5200\n5.0014\n5.5700\n5.4457\n6.4986\nAbstention Rate\n–\n–\n0.0029\n0.0671\n–\n–\n0.0114\n0.0757\nLlama-32-3B-Instruct\nAccuracy\n0.4843\n0.4757\n0.4829\n0.4886\n0.3800\n0.4100\n0.4143\n0.3900\nLAC Set Size\n4.0243\n3.3814\n4.4057\n4.1571\n4.4429\n3.8971\n4.7786\n4.8171\nAPS Set Size\n4.3086\n3.9429\n4.2986\n4.3971\n4.5800\n4.4814\n4.8543\n5.2400\nAbstention Rate\n–\n–\n0.0000\n0.0000\n–\n–\n0.0000\n0.0314\nPhi-4-mini\nAccuracy\n0.3871\n0.4071\n0.3557\n0.4014\n0.3171\n0.3457\n0.3186\n0.3343\nLAC Set Size\n3.9429\n4.0857\n4.3829\n4.8271\n4.1971\n4.0900\n4.9100\n5.1857\nAPS Set Size\n4.7171\n4.5400\n4.7871\n4.9157\n4.5557\n4.8014\n5.5414\n5.3500\nAbstention Rate\n–\n–\n0.0014\n0.0029\n–\n–\n0.0029\n0.0086\nQwen25-05B-Instruct\nAccuracy\n0.1843\n0.2157\n0.1586\n0.1971\n0.1743\n0.2200\n0.1557\n0.1771\nLAC Set Size\n5.1586\n5.0857\n6.0943\n6.0343\n5.2229\n5.0729\n6.1300\n5.9971\nAPS Set Size\n5.5614\n5.5886\n6.0671\n6.0043\n5.5629\n5.5814\n6.0800\n6.0229\nAbstention Rate\n–\n–\n0.1414\n0.1057\n–\n–\n0.1414\n0.1129\nQwen25-14B-Instruct\nAccuracy\n0.4771\n0.5471\n0.4314\n0.5214\n0.3714\n0.4414\n0.3214\n0.3529\nLAC Set Size\n4.0114\n3.7114\n4.8743\n4.9400\n4.4886\n4.6100\n5.6186\n5.9729\nAPS Set Size\n4.3057\n3.9229\n5.6471\n5.2200\n5.0829\n4.5900\n6.0786\n5.8986\nAbstention Rate\n–\n–\n0.1486\n0.1057\n–\n–\n0.2143\n0.2943\nQwen25-15B-Instruct\nAccuracy\n0.2643\n0.3114\n0.2629\n0.2771\n0.2529\n0.2829\n0.2214\n0.2400\nLAC Set Size\n4.8771\n4.4200\n5.6743\n5.5514\n5.1057\n4.9257\n6.0486\n6.0357\nAPS Set Size\n5.0729\n5.0814\n6.1000\n6.0714\n5.5500\n5.0357\n6.5357\n6.0786\nAbstention Rate\n–\n–\n0.1014\n0.0214\n–\n–\n0.1243\n0.0414\nQwen25-3B-Instruct\nAccuracy\n0.3671\n0.3514\n0.2886\n0.3029\n0.2957\n0.3029\n0.2129\n0.2543\nLAC Set Size\n4.7671\n4.4943\n5.8443\n5.7171\n4.8814\n4.7114\n6.0571\n5.9557\nAPS Set Size\n5.0243\n4.6814\n5.9829\n5.8229\n5.5629\n5.0743\n6.5186\n6.5429\nAbstention Rate\n–\n–\n0.2314\n0.1329\n–\n–\n0.2786\n0.2229\nQwen25-7B-Instruct\nAccuracy\n0.4586\n0.4600\n0.2486\n0.4014\n0.3500\n0.3571\n0.1529\n0.2914\nLAC Set Size\n4.4200\n4.2386\n5.7743\n5.2500\n4.7200\n4.8400\n5.9543\n5.9257\nAPS Set Size\n4.5000\n4.4357\n6.0686\n5.6900\n4.7929\n4.9729\n6.5329\n6.0314\nAbstention Rate\n–\n–\n0.5643\n0.1743\n–\n–\n0.6586\n0.3014\nQwen3-06B\nAccuracy\n0.2314\n0.2443\n0.1329\n0.2100\n0.2100\n0.2543\n0.1314\n0.1671\nLAC Set Size\n5.2214\n4.7429\n5.9386\n5.8329\n5.2029\n4.6743\n5.9529\n5.6586\nAPS Set Size\n5.6014\n5.0314\n6.0086\n6.5786\n5.6057\n5.0200\n6.5529\n6.0343\nAbstention Rate\n–\n–\n0.3929\n0.1700\n–\n–\n0.3957\n0.3229\nQwen3-1-7B\nAccuracy\n0.2957\n0.3214\n0.2643\n0.3129\n0.2314\n0.2929\n0.2114\n0.2543\nLAC Set Size\n4.7743\n4.5986\n5.6700\n5.3957\n4.9529\n4.8086\n5.8057\n5.7614\nAPS Set Size\n5.0429\n5.0829\n6.0486\n5.6857\n5.0571\n5.0271\n6.5571\n6.0443\nAbstention Rate\n–\n–\n0.1486\n0.0214\n–\n–\n0.1471\n0.0871\nQwen3-14B\nAccuracy\n0.4157\n0.5500\n0.2871\n0.5157\n0.3614\n0.4471\n0.2000\n0.3714\nLAC Set Size\n4.0143\n3.5100\n5.3243\n3.9829\n4.6643\n4.0957\n5.8229\n5.4057\nAPS Set Size\n4.3129\n3.6214\n5.4814\n4.2429\n5.0757\n4.3714\n6.0486\n5.3086\nAbstention Rate\n–\n–\n0.2400\n0.0257\n–\n–\n0.3371\n0.1686\nQwen3-4B\nAccuracy\n0.4414\n0.4414\n0.4000\n0.4000\n0.3329\n0.3486\n0.3100\n0.2829\nLAC Set Size\n4.0271\n4.1129\n4.9657\n5.2114\n4.4471\n4.3171\n5.5000\n5.4357\nAPS Set Size\n4.2671\n4.5071\n5.2914\n5.5843\n5.0557\n5.0457\n6.0486\n6.0471\nAbstention Rate\n–\n–\n0.0686\n0.0943\n–\n–\n0.1200\n0.2686\nQwen3-8B\nAccuracy\n0.4900\n0.4986\n0.4629\n0.4771\n0.3914\n0.4057\n0.3486\n0.3057\nLAC Set Size\n3.7657\n3.9957\n4.7600\n4.8914\n4.2886\n4.4686\n5.3029\n5.6757\nAPS Set Size\n4.1471\n4.0371\n4.6629\n4.8486\n4.8029\n4.7871\n6.0471\n5.8171\nAbstention Rate\n–\n–\n0.0643\n0.0600\n–\n–\n0.0814\n0.2714\ngemma-3-4b\nAccuracy\n0.3171\n0.3314\n0.3157\n0.3500\n0.2600\n0.2957\n0.2586\n0.2771\nLAC Set Size\n4.8500\n4.7314\n5.6171\n5.6414\n4.7743\n4.6371\n5.8914\n5.7800\nAPS Set Size\n5.5071\n5.4714\n6.4557\n6.4543\n5.5043\n5.4829\n6.4857\n6.4871\nAbstention Rate\n–\n–\n0.0143\n0.0143\n–\n–\n0.0200\n0.0343\nmedgemma-4b-it\nAccuracy\n0.4286\n0.4271\n0.4300\n0.4157\n0.3600\n0.3643\n0.3500\n0.3314\nLAC Set Size\n4.5357\n4.5086\n5.5057\n5.3686\n4.7371\n4.5814\n5.6743\n5.6943\nAPS Set Size\n5.0143\n4.9943\n5.3600\n6.0257\n5.0200\n5.4871\n6.0271\n6.4929\nAbstention Rate\n–\n–\n0.0000\n0.0014\n–\n–\n0.0057\n0.0129\nphi-4\nAccuracy\n0.5471\n0.5757\n0.5457\n0.5614\n0.4414\n0.4671\n0.4057\n0.4229\nLAC Set Size\n3.4071\n2.9643\n4.2729\n4.1043\n4.0057\n3.9214\n5.3186\n5.2343\nAPS Set Size\n3.6243\n3.4214\n4.5443\n4.3714\n4.3329\n4.1314\n5.4857\n5.3814\nAbstention Rate\n–\n–\n0.0229\n0.0171\n–\n–\n0.0757\n0.1043\n"}, {"page": 22, "text": "No Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\ngpt-4.1\nAccuracy\n0.8243\n0.8186\n0.8186\n0.8157\n0.6743\n0.6786\n0.6543\n0.6500\nLAC Set Size\n5.1057\n5.0214\n5.0500\n5.0443\n5.1000\n5.0829\n5.3571\n5.3129\nAPS Set Size\n3.1271\n3.1729\n4.4100\n4.3343\n3.9329\n3.7786\n4.5743\n4.5757\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4.1-nano\nAccuracy\n0.2129\n0.2114\n0.2214\n0.2271\n0.2057\n0.2000\n0.2100\n0.2114\nLAC Set Size\n5.7100\n5.7100\n6.7100\n6.7100\n5.7100\n5.7100\n6.7100\n6.7100\nAPS Set Size\n5.2271\n5.2329\n6.1243\n6.2414\n5.2257\n5.2729\n6.0914\n6.0571\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o\nAccuracy\n0.6700\n0.6657\n0.5671\n0.7271\n0.5529\n0.5343\n0.5671\n0.5829\nLAC Set Size\n5.7100\n5.7100\n5.8914\n5.0743\n5.7100\n5.7100\n5.8914\n6.7100\nAPS Set Size\n5.1486\n5.1657\n6.1843\n6.1271\n5.1500\n5.1271\n6.1843\n6.1200\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o-mini\nAccuracy\n0.3029\n0.3986\n0.3643\n0.4514\n0.3029\n0.2957\n0.3643\n0.3529\nLAC Set Size\n5.7100\n5.7100\n6.7100\n6.7100\n5.7100\n5.7100\n6.7100\n6.7100\nAPS Set Size\n5.1586\n5.1429\n6.0300\n5.9943\n5.1586\n5.1100\n6.0300\n6.0614\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n"}, {"page": 23, "text": "Table 3: AMBOSS: Experiment results for the no Chain-of-thought setting. The darker the entry, the better across\nall evaluation metrics. (Higher accuracy, lower set size, higher abstention)\nNo Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\nLlama-31-8B-Instruct\nAccuracy\n0.5686\n0.5486\n0.5571\n0.5443\n0.4543\n0.4557\n0.4529\n0.4171\nLAC Set Size\n3.2529\n3.1457\n3.6143\n3.7129\n3.6857\n3.6329\n4.3686\n4.4929\nAPS Set Size\n3.8943\n4.0143\n4.3343\n4.6414\n3.9571\n4.0643\n4.8286\n4.7686\nAbstention Rate\n–\n–\n–\n0.0086\n–\n–\n0.0286\n0.0886\nLlama-32-1B-Instruct\nAccuracy\n0.2871\n0.2871\n0.2671\n0.2286\n0.2629\n0.2443\n0.2586\n0.1943\nLAC Set Size\n4.8386\n4.9043\n5.5757\n5.7400\n4.6729\n4.8786\n5.3714\n5.9157\nAPS Set Size\n4.9914\n5.5800\n5.4186\n6.5343\n5.5900\n5.5600\n5.5257\n6.5286\nAbstention Rate\n–\n–\n0.0057\n0.0629\n–\n–\n0.0129\n0.0886\nLlama-32-3B-Instruct\nAccuracy\n0.5000\n0.4771\n0.4857\n0.4786\n0.4014\n0.3900\n0.4114\n0.4000\nLAC Set Size\n3.8457\n3.2571\n4.3943\n4.0743\n4.5214\n4.0357\n4.8700\n4.7357\nAPS Set Size\n4.1614\n4.0457\n4.2300\n4.5386\n4.6486\n4.2914\n4.8857\n5.0043\nAbstention Rate\n–\n–\n0.0000\n0.0000\n–\n–\n0.0000\n0.0243\nPhi-4-mini\nAccuracy\n0.4014\n0.4200\n0.3800\n0.4086\n0.3186\n0.3400\n0.3329\n0.3471\nLAC Set Size\n3.8814\n3.9286\n4.5371\n4.8586\n4.1500\n4.2314\n5.0414\n5.0457\nAPS Set Size\n4.4400\n4.4143\n4.8157\n5.2300\n5.0371\n4.4800\n5.4486\n5.4114\nAbstention Rate\n–\n–\n–\n0.0029\n–\n–\n0.0043\n0.0100\nQwen25-05B-Instruct\nAccuracy\n0.1929\n0.2271\n0.1686\n0.1843\n0.1914\n0.2200\n0.1700\n0.1871\nLAC Set Size\n5.1443\n5.0871\n6.1014\n5.9314\n5.2200\n5.1014\n6.1100\n5.9871\nAPS Set Size\n5.5600\n5.5886\n6.5443\n6.5343\n5.5514\n5.5671\n6.0914\n5.9929\nAbstention Rate\n–\n–\n0.0943\n0.0943\n–\n–\n0.1029\n0.1057\nQwen25-14B-Instruct\nAccuracy\n0.5386\n0.5543\n0.4800\n0.5143\n0.4336\n0.4400\n0.3743\n0.3686\nLAC Set Size\n3.6657\n3.2243\n5.1371\n4.2057\n4.4821\n4.1643\n5.8450\n5.4571\nAPS Set Size\n3.8014\n3.9386\n5.3343\n4.8243\n4.5600\n4.7100\n6.0300\n5.8814\nAbstention Rate\n–\n–\n0.1429\n0.0986\n–\n–\n0.2107\n0.2800\nQwen25-15B-Instruct\nAccuracy\n0.2700\n0.3071\n0.2757\n0.2800\n0.2493\n0.2671\n0.2357\n0.2457\nLAC Set Size\n4.8543\n4.5857\n5.7429\n5.8414\n5.2236\n4.7586\n6.1007\n5.9586\nAPS Set Size\n5.5543\n5.5471\n6.5400\n6.0743\n5.5514\n5.0529\n6.5393\n6.0386\nAbstention Rate\n–\n–\n0.0929\n0.0314\n–\n–\n0.1229\n0.0443\nQwen25-3B-Instruct\nAccuracy\n0.3629\n0.3543\n0.2971\n0.3086\n0.3029\n0.3029\n0.2329\n0.2571\nLAC Set Size\n4.7686\n4.4557\n5.8100\n5.5400\n4.9243\n4.7214\n6.0357\n6.0414\nAPS Set Size\n5.0443\n4.6386\n6.0371\n6.0843\n5.0371\n5.0571\n6.0186\n6.5043\nAbstention Rate\n–\n–\n0.1929\n0.1343\n–\n–\n0.2643\n0.2214\nQwen25-7B-Instruct\nAccuracy\n0.4543\n0.4600\n0.3429\n0.4171\n0.3586\n0.3686\n0.2471\n0.3129\nLAC Set Size\n3.8457\n4.0586\n5.2943\n5.0771\n4.5043\n4.6000\n5.5929\n5.7614\nAPS Set Size\n4.4086\n4.2600\n5.3129\n5.9143\n5.0300\n4.9957\n6.0529\n6.5343\nAbstention Rate\n–\n–\n0.3671\n0.1386\n–\n–\n0.4543\n0.2371\nQwen3-06B\nAccuracy\n0.2171\n0.2657\n0.1429\n0.2014\n0.2043\n0.2600\n0.1357\n0.1614\nLAC Set Size\n4.9629\n4.8386\n5.9186\n5.5843\n5.1500\n4.6871\n5.9000\n5.7757\nAPS Set Size\n5.5943\n5.5700\n6.0621\n6.5571\n5.5771\n5.0114\n6.5671\n6.5414\nAbstention Rate\n–\n–\n0.3657\n0.1700\n–\n–\n0.3600\n0.3000\nQwen3-1-7B\nAccuracy\n0.3114\n0.3157\n0.2686\n0.3186\n0.2471\n0.2871\n0.2186\n0.2343\nLAC Set Size\n4.7643\n4.2800\n5.5714\n5.4057\n4.8857\n4.8600\n5.6443\n5.7543\nAPS Set Size\n5.0471\n4.7414\n6.0443\n5.7343\n5.0571\n5.5529\n6.0586\n6.0514\nAbstention Rate\n–\n–\n0.1529\n0.0243\n–\n–\n0.1500\n0.0943\nQwen3-14B\nAccuracy\n0.3671\n0.5486\n0.3229\n0.5271\n0.3214\n0.4571\n0.2414\n0.3829\nLAC Set Size\n4.1671\n3.3157\n5.3300\n3.7714\n4.7586\n4.0943\n5.9557\n5.3243\nAPS Set Size\n4.3686\n3.4757\n5.4729\n4.1843\n5.0457\n4.0743\n6.5486\n5.4600\nAbstention Rate\n–\n–\n0.2371\n0.0257\n–\n–\n0.3143\n0.1514\nQwen3-4B\nAccuracy\n0.4329\n0.4329\n0.3943\n0.4243\n0.3357\n0.3614\n0.3114\n0.2771\nLAC Set Size\n3.9871\n4.0729\n5.2829\n5.4071\n4.4329\n4.3357\n5.4700\n5.3386\nAPS Set Size\n4.1543\n4.6643\n5.2000\n5.7086\n5.0686\n5.0829\n6.0243\n6.5371\nAbstention Rate\n–\n–\n0.0957\n0.0871\n–\n–\n0.1414\n0.2471\nQwen3-8B\nAccuracy\n0.4957\n0.4957\n0.4529\n0.4829\n0.3971\n0.4171\n0.3400\n0.2943\nLAC Set Size\n3.7343\n4.0129\n4.7800\n4.6843\n4.3871\n4.3657\n5.4457\n5.4971\nAPS Set Size\n4.1943\n4.1286\n5.0186\n4.8629\n4.5671\n4.5300\n6.0771\n5.8157\nAbstention Rate\n–\n–\n0.0757\n0.0600\n–\n–\n0.1057\n0.2914\ngemma-3-27b-it\nAccuracy\n0.5471\n0.5600\n0.5514\n0.5400\n0.4329\n0.1829\n0.4214\n0.4100\nLAC Set Size\n3.6814\n3.7014\n4.2300\n4.3857\n4.2843\n1.0000\n4.8886\n5.2586\nAPS Set Size\n4.6029\n4.0814\n4.5400\n4.9271\n4.6986\n1.0000\n4.9829\n5.7557\nAbstention Rate\n–\n–\n0.0043\n0.0200\n–\n–\n0.0171\n0.0871\ngemma-3-4b\nAccuracy\n0.3314\n0.3457\n0.3100\n0.3329\n0.2657\n0.2914\n0.2557\n0.2671\nLAC Set Size\n4.9171\n4.6543\n5.6586\n5.7729\n4.8114\n4.6257\n5.7029\n5.7086\nAPS Set Size\n5.5014\n5.4700\n6.4471\n6.4600\n5.4943\n5.4829\n6.0114\n6.4786\nAbstention Rate\n–\n–\n–\n0.0300\n–\n–\n0.0143\n0.0443\nmedgemma-4b-it\nAccuracy\n0.4271\n0.4214\n0.4214\n0.3986\n0.3557\n0.3700\n0.3357\n0.3457\nLAC Set Size\n4.5100\n4.5600\n5.5814\n5.4629\n4.7500\n4.7114\n5.7400\n5.7386\nAPS Set Size\n5.0329\n5.0057\n5.3286\n5.9929\n4.9729\n4.9614\n6.0186\n6.5143\nAbstention Rate\n–\n–\n–\n0.0029\n–\n–\n0.0043\n0.0143\n"}, {"page": 24, "text": "No Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\nphi-4\nAccuracy\n0.5500\n0.5657\n0.5529\n0.5729\n0.4457\n0.4671\n0.4371\n0.4371\nLAC Set Size\n3.3129\n2.7486\n4.2386\n3.8557\n3.9314\n3.7529\n5.1129\n5.1200\nAPS Set Size\n3.4600\n3.4729\n4.4586\n4.2900\n4.1429\n4.2014\n5.4486\n5.3886\nAbstention Rate\n–\n–\n0.0143\n0.0143\n–\n–\n0.0386\n0.0986\ngpt-4.1\nAccuracy\n0.7643\n0.7614\n0.7443\n0.7629\n0.6014\n0.6300\n0.5957\n0.6043\nLAC Set Size\n2.4129\n2.6129\n2.7171\n3.1086\n3.2786\n3.5257\n4.1429\n4.0229\nAPS Set Size\n5.0971\n5.1971\n6.0071\n5.9543\n5.2014\n5.1171\n6.0943\n6.1286\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4.1-nano\nAccuracy\n0.4529\n0.3771\n0.4000\n0.3271\n0.3614\n0.3500\n0.3400\n0.2700\nLAC Set Size\n3.8571\n3.9829\n4.4314\n4.7057\n4.3457\n4.4071\n5.0414\n5.1771\nAPS Set Size\n5.0214\n4.8700\n5.8500\n5.7200\n5.0614\n4.8171\n5.8343\n5.8600\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o\nAccuracy\n0.6657\n0.6943\n0.6357\n0.6414\n0.5186\n0.5743\n0.4857\n0.4857\nLAC Set Size\n3.0143\n2.7543\n3.5729\n3.0529\n3.7071\n3.5000\n4.4829\n3.9657\nAPS Set Size\n4.9386\n5.1014\n5.5743\n5.9014\n5.0086\n5.0186\n5.7429\n5.6971\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o-mini\nAccuracy\n0.4543\n0.4786\n0.4086\n0.3957\n0.3843\n0.3871\n0.3257\n0.3529\nLAC Set Size\n4.0686\n4.2329\n4.7643\n4.9986\n4.5543\n4.6429\n5.3471\n5.6614\nAPS Set Size\n4.5814\n4.6943\n5.3986\n5.5657\n4.6229\n4.6786\n5.4429\n5.3186\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n"}, {"page": 25, "text": "Table 4: AMBOSS: Experiment results for the Qwen thinking mode. The darker the entry, the better across all\nevaluation metrics. (Higher accuracy, lower set size, higher abstention)\nNo Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nThinking NoThinking Thinking NoThinking Thinking\nNoThinking\nThinking NoThinking\nQwen25-05B-Instruct\nAccuracy\n0.2100\n0.1929\n0.1764\n0.1686\n0.2057\n0.1914\n0.1786\n0.1700\nLAC Set Size\n5.1157\n5.1443\n6.0164\n6.1014\n5.1607\n5.2200\n6.0486\n6.1100\nAPS Set Size\n5.5743\n5.5600\n6.5393\n6.5443\n5.5593\n5.5514\n6.0421\n6.0914\nAbstention Rate\n–\n–\n0.0943\n0.0943\n–\n–\n0.1043\n0.1029\nQwen25-14B-Instruct\nAccuracy\n0.5464\n0.5386\n0.4971\n0.4800\n0.4371\n0.4329\n0.3721\n0.3729\nLAC Set Size\n3.4450\n3.6657\n4.6714\n5.1371\n4.3207\n4.4871\n5.6536\n5.8400\nAPS Set Size\n3.8700\n3.8014\n5.0793\n5.3343\n4.6329\n4.5643\n5.9664\n6.0086\nAbstention Rate\n–\n–\n0.0986\n0.1429\n–\n–\n0.2450\n0.2114\nQwen25-15B-Instruct\nAccuracy\n0.2886\n0.2700\n0.2779\n0.2757\n0.2586\n0.2486\n0.2407\n0.2357\nLAC Set Size\n4.7200\n4.8543\n5.7921\n5.7429\n4.9900\n5.2257\n6.0286\n6.1029\nAPS Set Size\n5.5507\n5.5543\n6.3071\n6.5400\n5.2950\n5.5657\n6.2921\n6.5329\nAbstention Rate\n–\n–\n0.0314\n0.0929\n–\n–\n0.0836\n0.1229\nQwen25-3B-Instruct\nAccuracy\n0.3586\n0.3629\n0.3086\n0.2971\n0.3029\n0.3029\n0.2450\n0.2329\nLAC Set Size\n4.6121\n4.7686\n5.5400\n5.8100\n4.8229\n4.9243\n6.0386\n6.0357\nAPS Set Size\n4.8414\n5.0443\n6.0843\n6.0371\n5.0471\n5.0371\n6.2614\n6.0186\nAbstention Rate\n–\n–\n0.1343\n0.1929\n–\n–\n0.2214\n0.2643\nQwen25-7B-Instruct\nAccuracy\n0.4571\n0.4543\n0.3800\n0.3429\n0.3636\n0.3586\n0.2800\n0.2471\nLAC Set Size\n3.9521\n3.8457\n5.1857\n5.2943\n4.5521\n4.5043\n5.6771\n5.5929\nAPS Set Size\n4.3343\n4.4086\n5.6136\n5.3129\n5.0129\n5.0300\n6.2936\n6.0529\nAbstention Rate\n–\n–\n0.1386\n0.3671\n–\n–\n0.3457\n0.4543\nQwen3-06B\nAccuracy\n0.2414\n0.2171\n0.1721\n0.1429\n0.2321\n0.2043\n0.1486\n0.1357\nLAC Set Size\n4.9007\n4.9629\n5.7514\n5.9186\n4.9186\n5.1500\n5.8379\n5.9000\nAPS Set Size\n5.5821\n5.5943\n6.3093\n6.0629\n5.2943\n5.5771\n6.5543\n6.5671\nAbstention Rate\n–\n–\n0.2679\n0.3657\n–\n–\n0.3300\n0.3600\nQwen3-1-7B\nAccuracy\n0.3136\n0.3114\n0.2936\n0.2686\n0.2671\n0.2471\n0.2264\n0.2186\nLAC Set Size\n4.5221\n4.7643\n5.4886\n5.5714\n4.8729\n4.8857\n5.6993\n5.6443\nAPS Set Size\n4.8943\n5.0471\n5.8893\n6.0443\n5.3050\n5.0571\n6.0550\n6.0586\nAbstention Rate\n–\n–\n0.0886\n0.1529\n–\n–\n0.1221\n0.1500\nQwen3-14B\nAccuracy\n0.4579\n0.3671\n0.4250\n0.3229\n0.3893\n0.3214\n0.3121\n0.2414\nLAC Set Size\n3.7414\n4.1671\n4.5507\n5.3300\n4.4264\n4.7586\n5.6400\n5.9557\nAPS Set Size\n3.9221\n4.3686\n4.8286\n5.4729\n4.5600\n5.0457\n6.0043\n6.5486\nAbstention Rate\n–\n–\n0.1314\n0.2371\n–\n–\n0.2329\n0.3143\nQwen3-32B\nAccuracy\n0.5921\n0.1729\n0.5879\n0.1557\n0.4786\n0.1700\n0.4329\n0.1557\nLAC Set Size\n3.4843\n1.0000\n4.0929\n1.0000\n4.3607\n5.3100\n5.3129\n1.0000\nAPS Set Size\n3.5750\n1.0000\n4.4043\n1.0000\n4.5529\n5.7029\n5.2357\n1.0000\nAbstention Rate\n–\n–\n0.0207\n0.1471\n–\n–\n0.0693\n0.1471\nQwen3-4B\nAccuracy\n0.4329\n0.4329\n0.4093\n0.3943\n0.3486\n0.3357\n0.2943\n0.3114\nLAC Set Size\n4.0300\n3.9871\n5.3450\n5.2829\n4.3843\n4.4329\n5.4043\n5.4700\nAPS Set Size\n4.4093\n4.1543\n5.4543\n5.2000\n5.0757\n5.0686\n6.2807\n6.0243\nAbstention Rate\n–\n–\n0.0914\n0.0957\n–\n–\n0.1943\n0.1414\nQwen3-8B\nAccuracy\n0.4957\n0.4957\n0.4679\n0.4529\n0.4071\n0.3971\n0.3171\n0.3400\nLAC Set Size\n3.8736\n3.7343\n4.7321\n4.7800\n4.3764\n4.3871\n5.4714\n5.4457\nAPS Set Size\n4.1614\n4.1943\n4.9407\n5.0186\n4.5486\n4.5671\n5.9464\n6.0771\nAbstention Rate\n–\n–\n0.0679\n0.0757\n–\n–\n0.1986\n0.1057\n"}, {"page": 26, "text": "Table 5: MedQA: Experiment results for the Chain-of-thought setting. The darker the entry, the better across all\nevaluation metrics. (Higher accuracy, lower set size, higher abstention\nNo Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\nLlama-31-8B-Instruct\nAccuracy\n0.7078\n0.6879\n0.6894\n0.6667\n0.5461\n0.5333\n0.5291\n0.4837\nLAC Set Size\n2.2965\n2.5348\n2.6355\n2.9489\n2.7504\n3.1135\n3.5489\n3.9220\nAPS Set Size\n3.4142\n3.4894\n4.2270\n4.0638\n3.4142\n3.5574\n4.1858\n4.0085\nAbstention Rate\n–\n–\n0.0184\n0.0113\n–\n–\n0.0270\n0.0766\nLlama-32-1B-Instruct\nAccuracy\n0.3716\n0.3560\n0.3603\n0.3163\n0.3220\n0.3035\n0.3021\n0.2766\nLAC Set Size\n4.1418\n3.9603\n4.5546\n5.0596\n4.1050\n4.1660\n4.6369\n5.1461\nAPS Set Size\n4.2709\n4.8922\n4.6511\n5.8184\n4.2823\n4.8823\n4.7418\n5.2255\nAbstention Rate\n–\n–\n0.0142\n0.0610\n–\n–\n0.0156\n0.0851\nLlama-32-3B-Instruct\nAccuracy\n0.5773\n0.5844\n0.5986\n0.5872\n0.4738\n0.4695\n0.4638\n0.4596\nLAC Set Size\n3.0482\n2.6567\n3.1702\n3.1645\n3.3759\n3.0326\n3.4567\n3.7915\nAPS Set Size\n3.4596\n3.2440\n3.7574\n3.8142\n3.7348\n3.4709\n3.8539\n3.9546\nAbstention Rate\n–\n–\n0.0014\n0.0043\n–\n–\n0.0028\n0.0241\nPhi-4-mini\nAccuracy\n0.4766\n0.4993\n0.4553\n0.4879\n0.3957\n0.4199\n0.3702\n0.3915\nLAC Set Size\n3.5872\n3.3787\n3.5929\n3.5816\n3.8099\n3.5801\n4.1404\n3.7234\nAPS Set Size\n3.5929\n3.6709\n3.9376\n4.0993\n4.0624\n3.9007\n3.9688\n4.0965\nAbstention Rate\n–\n–\n0.0071\n0.0000\n–\n–\n0.0113\n0.0071\nQwen25-05B-Instruct\nAccuracy\n0.2426\n0.2950\n0.2199\n0.2468\n0.2241\n0.2780\n0.2142\n0.2454\nLAC Set Size\n4.4113\n4.3092\n5.2652\n5.1929\n4.4482\n4.3730\n5.2723\n5.2610\nAPS Set Size\n4.8752\n4.8908\n5.3418\n5.2993\n4.8638\n4.2738\n5.3716\n5.8610\nAbstention Rate\n–\n–\n0.1404\n0.0851\n–\n–\n0.1447\n0.0837\nQwen25-14B-Instruct\nAccuracy\n0.5277\n0.6369\n0.4582\n0.5702\n0.4071\n0.4894\n0.3291\n0.3574\nLAC Set Size\n3.3305\n2.6638\n4.0355\n3.1830\n3.7348\n3.3106\n4.7262\n4.3844\nAPS Set Size\n3.4128\n3.2809\n3.9589\n3.8241\n4.3277\n3.7546\n4.6284\n4.8652\nAbstention Rate\n–\n–\n0.1518\n0.0965\n–\n–\n0.2383\n0.2922\nQwen25-15B-Instruct\nAccuracy\n0.4113\n0.3887\n0.3674\n0.3730\n0.3447\n0.3277\n0.2908\n0.3092\nLAC Set Size\n4.0652\n3.9858\n5.0638\n4.8128\n4.2199\n4.0312\n5.1475\n5.0028\nAPS Set Size\n4.3319\n4.3475\n5.0993\n5.1021\n4.3135\n4.0738\n5.3759\n5.1007\nAbstention Rate\n–\n–\n0.1291\n0.0156\n–\n–\n0.1319\n0.0340\nQwen25-3B-Instruct\nAccuracy\n0.4454\n0.4539\n0.3660\n0.4028\n0.3660\n0.3631\n0.3050\n0.2908\nLAC Set Size\n3.8596\n3.6057\n4.9901\n4.8539\n4.0014\n4.0043\n5.3277\n5.0454\nAPS Set Size\n3.7858\n3.8610\n4.9645\n5.0908\n3.9220\n4.0794\n5.2624\n5.3305\nAbstention Rate\n–\n–\n0.2071\n0.1149\n–\n–\n0.2766\n0.1972\nQwen25-7B-Instruct\nAccuracy\n0.5277\n0.5191\n0.2723\n0.4553\n0.4099\n0.4397\n0.1433\n0.3106\nLAC Set Size\n3.5035\n2.9504\n4.9759\n4.2837\n3.8227\n3.3830\n5.1418\n4.7986\nAPS Set Size\n4.0057\n3.6340\n4.7702\n4.5305\n4.0780\n3.7007\n4.9730\n5.2780\nAbstention Rate\n–\n–\n0.6000\n0.1745\n–\n–\n0.7106\n0.2936\nQwen3-06B\nAccuracy\n0.2610\n0.2610\n0.1631\n0.2582\n0.2525\n0.2567\n0.1475\n0.1858\nLAC Set Size\n4.3645\n4.4099\n4.9475\n4.8482\n4.5262\n4.1844\n5.0780\n5.2340\nAPS Set Size\n4.9135\n4.8865\n5.3050\n5.3220\n4.9050\n4.8553\n5.3574\n5.8624\nAbstention Rate\n–\n–\n0.4227\n0.1674\n–\n–\n0.4142\n0.3007\nQwen3-1-7B\nAccuracy\n0.3716\n0.3787\n0.3135\n0.3674\n0.3149\n0.3277\n0.2511\n0.2894\nLAC Set Size\n4.1603\n4.0993\n4.8908\n5.0426\n4.1617\n4.1163\n5.1716\n5.2681\nAPS Set Size\n4.0851\n4.3206\n5.3376\n4.7277\n4.3504\n4.3007\n5.8383\n5.3305\nAbstention Rate\n–\n–\n0.1475\n0.0156\n–\n–\n0.1702\n0.0894\nQwen3-14B\nAccuracy\n0.3915\n0.6369\n0.3730\n0.6241\n0.4525\n0.4851\n0.2227\n0.4582\nLAC Set Size\n2.8312\n2.5730\n4.4440\n2.8043\n3.3177\n3.1801\n4.8950\n4.0596\nAPS Set Size\n3.4369\n3.1404\n4.4340\n3.8965\n3.9688\n3.3716\n5.1305\n4.3816\nAbstention Rate\n–\n–\n0.2255\n0.0128\n–\n–\n0.3206\n0.1035\nQwen3-4B\nAccuracy\n0.4993\n0.5149\n0.4766\n0.4809\n0.4099\n0.4213\n0.3532\n0.3106\nLAC Set Size\n2.8000\n2.9305\n3.7191\n3.9333\n3.4738\n3.7560\n4.5489\n4.7064\nAPS Set Size\n3.3688\n3.5191\n4.2908\n4.2894\n3.4340\n4.2851\n4.4965\n5.0965\nAbstention Rate\n–\n–\n0.0823\n0.0624\n–\n–\n0.1135\n0.2468\nQwen3-8B\nAccuracy\n0.5943\n0.5957\n0.5234\n0.5702\n0.4723\n0.4879\n0.4170\n0.3674\nLAC Set Size\n3.0695\n3.1404\n3.6128\n3.3716\n3.6794\n3.4624\n4.2624\n4.6525\nAPS Set Size\n3.5972\n3.5957\n4.2099\n4.3333\n3.7447\n3.8326\n4.4085\n4.7220\nAbstention Rate\n–\n–\n0.0837\n0.0482\n–\n–\n0.1206\n0.2482\ngemma-3-4b\nAccuracy\n0.3943\n0.3915\n0.3787\n0.3844\n0.3362\n0.3305\n0.3277\n0.3319\nLAC Set Size\n4.0752\n3.9333\n4.8355\n4.9461\n4.1872\n3.9631\n5.0440\n5.0326\nAPS Set Size\n4.0738\n3.9050\n5.1064\n4.8113\n4.0000\n4.2511\n5.2865\n5.0851\nAbstention Rate\n–\n–\n0.0170\n0.0184\n–\n–\n0.0255\n0.0156\nmedgemma-4b-it\nAccuracy\n0.5262\n0.4979\n0.5064\n0.5021\n0.4468\n0.4383\n0.4199\n0.3915\nLAC Set Size\n3.4667\n3.1972\n4.2752\n4.2156\n3.7773\n3.8227\n4.7957\n5.0525\nAPS Set Size\n3.5887\n3.6057\n4.3787\n4.4099\n3.6298\n3.6965\n4.6496\n4.8369\nAbstention Rate\n–\n–\n0.0057\n0.0071\n–\n–\n0.0099\n0.0028\nphi-4\nAccuracy\n0.6681\n0.6993\n0.6525\n0.6837\n0.5390\n0.5858\n0.4979\n0.5177\nLAC Set Size\n2.0865\n2.0879\n2.8667\n2.4397\n2.7546\n2.7390\n3.7702\n3.6496\nAPS Set Size\n2.5943\n2.8511\n3.1135\n3.5050\n3.0681\n3.0667\n3.9291\n3.8028\nAbstention Rate\n–\n–\n0.0270\n0.0099\n–\n–\n0.0723\n0.0894\n"}, {"page": 27, "text": "No Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\ngpt-4.1\nAccuracy\n0.7121\n0.8355\n0.8440\n0.8468\n0.7121\n0.7078\n0.7078\n0.7092\nLAC Set Size\n4.5858\n4.5645\n4.4482\n4.4738\n4.5858\n4.5433\n4.9121\n4.8482\nAPS Set Size\n2.7206\n2.3730\n3.6766\n3.6071\n2.7206\n2.7660\n3.6965\n3.7447\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4.1-nano\nAccuracy\n0.2496\n0.2482\n0.2270\n0.2511\n0.2397\n0.2397\n0.2270\n0.2255\nLAC Set Size\n5.0000\n5.0000\n6.0000\n6.0000\n5.0000\n5.0000\n6.0000\n6.0000\nAPS Set Size\n4.8014\n4.8071\n5.5887\n5.5532\n4.8468\n4.8525\n5.5887\n5.6298\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o\nAccuracy\n0.5830\n0.7177\n0.6624\n0.8014\n0.5830\n0.6043\n0.6624\n0.6638\nLAC Set Size\n5.0000\n5.0000\n6.0000\n3.9121\n5.0000\n5.0000\n6.0000\n6.0000\nAPS Set Size\n4.6596\n4.7234\n5.4298\n5.5035\n4.6596\n4.7191\n5.4298\n5.6057\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o-mini\nAccuracy\n0.4567\n0.4511\n0.5319\n0.5248\n0.3915\n0.3943\n0.4298\n0.4241\nLAC Set Size\n5.0000\n5.0000\n6.0000\n6.0000\n5.0000\n5.0000\n6.0000\n6.0000\nAPS Set Size\n4.6823\n4.6950\n5.4496\n5.4695\n4.8014\n4.7674\n5.5135\n5.4894\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n"}, {"page": 28, "text": "Table 6: MedQA: Experiment results for the no Chain-of-thought setting. The darker the entry, the better across all\nevaluation metrics. (Higher accuracy, lower set size, higher abstention\nNo Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\nLlama-31-8B-Instruct\nAccuracy\n0.7106\n0.6667\n0.6865\n0.6809\n0.5617\n0.5447\n0.5305\n0.5007\nLAC Set Size\n2.3220\n2.4440\n2.7035\n2.8241\n2.7433\n3.1745\n3.4823\n3.6270\nAPS Set Size\n3.3589\n3.4496\n4.1872\n4.0511\n3.2567\n3.4794\n4.2482\n4.2340\nAbstention Rate\n–\n–\n0.0270\n0.0071\n–\n–\n0.0355\n0.0738\nLlama-32-1B-Instruct\nAccuracy\n0.3773\n0.3362\n0.3560\n0.3078\n0.3121\n0.3177\n0.3035\n0.2709\nLAC Set Size\n4.0326\n3.9787\n4.6752\n4.9674\n4.0780\n4.1660\n4.6567\n5.1702\nAPS Set Size\n4.2865\n4.1461\n4.6979\n5.8369\n4.2879\n4.8837\n4.6965\n5.2213\nAbstention Rate\n–\n–\n0.0142\n0.0511\n–\n–\n0.0156\n0.0766\nLlama-32-3B-Instruct\nAccuracy\n0.6057\n0.5858\n0.5943\n0.5887\n0.4879\n0.4780\n0.4681\n0.4482\nLAC Set Size\n2.6610\n2.6184\n3.0525\n3.1801\n3.2965\n3.0567\n3.4397\n3.7617\nAPS Set Size\n3.4383\n3.2440\n3.6128\n3.6340\n3.4979\n3.5433\n3.8709\n3.9092\nAbstention Rate\n–\n–\n0.0028\n0.0028\n–\n–\n0.0028\n0.0298\nPhi-4-mini\nAccuracy\n0.4851\n0.5064\n0.4582\n0.4894\n0.3972\n0.4113\n0.3759\n0.4043\nLAC Set Size\n3.3972\n3.2014\n3.6709\n3.6865\n3.6936\n3.4965\n4.0043\n3.9816\nAPS Set Size\n3.7957\n3.8667\n4.0340\n4.0397\n3.8567\n3.7433\n4.0113\n4.2482\nAbstention Rate\n–\n–\n0.0043\n0.0000\n–\n–\n0.0057\n0.0099\nQwen25-05B-Instruct\nAccuracy\n0.2858\n0.2936\n0.2596\n0.2454\n0.2525\n0.2851\n0.2312\n0.2426\nLAC Set Size\n4.3766\n4.3206\n5.3170\n5.1887\n4.4596\n4.4851\n5.3887\n5.1844\nAPS Set Size\n4.8390\n4.3206\n5.8723\n5.3106\n4.8652\n4.8539\n5.3716\n5.3064\nAbstention Rate\n–\n–\n0.0894\n0.1007\n–\n–\n0.0908\n0.0965\nQwen25-14B-Instruct\nAccuracy\n0.6113\n0.6326\n0.5475\n0.5929\n0.4773\n0.5050\n0.3986\n0.3716\nLAC Set Size\n2.8872\n2.6440\n3.4255\n3.3234\n3.2972\n3.2383\n4.4191\n4.4993\nAPS Set Size\n3.1801\n3.3319\n4.1092\n3.5348\n3.4128\n3.7688\n4.4284\n4.3603\nAbstention Rate\n–\n–\n0.1312\n0.0894\n–\n–\n0.2135\n0.3007\nQwen25-15B-Instruct\nAccuracy\n0.4092\n0.3872\n0.3730\n0.3617\n0.3390\n0.2993\n0.3028\n0.3021\nLAC Set Size\n4.0525\n3.9319\n4.9248\n4.7617\n4.1787\n4.0184\n5.0972\n5.0894\nAPS Set Size\n4.3496\n4.0057\n5.0617\n5.0879\n4.6071\n4.0894\n5.3603\n5.3645\nAbstention Rate\n–\n–\n0.1021\n0.0270\n–\n–\n0.1241\n0.0511\nQwen25-3B-Instruct\nAccuracy\n0.4504\n0.4539\n0.3738\n0.4057\n0.3730\n0.3674\n0.2950\n0.2908\nLAC Set Size\n3.8929\n3.4809\n5.0511\n4.8809\n4.0979\n3.9362\n5.2709\n5.1234\nAPS Set Size\n3.7440\n3.7603\n4.9078\n5.1021\n4.0014\n3.9092\n4.9021\n5.3333\nAbstention Rate\n–\n–\n0.1936\n0.1191\n–\n–\n0.2652\n0.1929\nQwen25-7B-Instruct\nAccuracy\n0.5426\n0.5206\n0.3809\n0.4837\n0.4284\n0.4397\n0.2496\n0.3461\nLAC Set Size\n3.2596\n2.8411\n4.5858\n4.0496\n3.7333\n3.4667\n4.6752\n4.6624\nAPS Set Size\n3.5617\n3.7759\n4.4340\n4.0809\n3.6851\n3.7972\n5.0950\n5.2837\nAbstention Rate\n–\n–\n0.3816\n0.1248\n–\n–\n0.4894\n0.2184\nQwen3-06B\nAccuracy\n0.2631\n0.2752\n0.1681\n0.2496\n0.2284\n0.2383\n0.1574\n0.1915\nLAC Set Size\n4.3667\n4.4794\n4.9823\n4.9589\n4.3362\n4.3773\n4.9830\n5.3092\nAPS Set Size\n4.6092\n4.2922\n5.3043\n5.3007\n4.9135\n4.8965\n5.9007\n5.8596\nAbstention Rate\n–\n–\n0.4092\n0.1560\n–\n–\n0.3901\n0.2908\nQwen3-1-7B\nAccuracy\n0.3667\n0.3929\n0.3128\n0.3773\n0.3305\n0.3234\n0.2539\n0.3078\nLAC Set Size\n4.1922\n4.0794\n4.9489\n4.8851\n4.4397\n4.1532\n5.0340\n5.1589\nAPS Set Size\n4.0468\n4.3702\n5.3511\n4.7504\n4.3291\n4.2936\n5.8085\n5.3660\nAbstention Rate\n–\n–\n0.1411\n0.0241\n–\n–\n0.1660\n0.0879\nQwen3-14B\nAccuracy\n0.4780\n0.6355\n0.3723\n0.6227\n0.4057\n0.4950\n0.2738\n0.4553\nLAC Set Size\n2.9312\n2.4553\n4.4355\n2.8553\n3.4184\n3.0809\n4.7631\n4.0468\nAPS Set Size\n3.4248\n3.0113\n4.6184\n3.9050\n4.0085\n3.4979\n5.1418\n4.4809\nAbstention Rate\n–\n–\n0.2220\n0.0128\n–\n–\n0.2794\n0.0979\nQwen3-4B\nAccuracy\n0.5000\n0.5177\n0.4766\n0.4794\n0.4028\n0.4340\n0.3447\n0.3262\nLAC Set Size\n2.8028\n3.1277\n3.6695\n3.9801\n3.4014\n3.8440\n4.5078\n4.8440\nAPS Set Size\n3.3872\n3.5645\n4.2496\n4.3149\n3.5433\n3.8695\n4.4809\n5.0099\nAbstention Rate\n–\n–\n0.0858\n0.0652\n–\n–\n0.1262\n0.2156\nQwen3-8B\nAccuracy\n0.5950\n0.6113\n0.5291\n0.5617\n0.4695\n0.4894\n0.4128\n0.3674\nLAC Set Size\n3.0617\n2.8454\n3.6149\n3.5163\n3.5887\n3.2667\n4.4071\n4.4496\nAPS Set Size\n3.6184\n3.6142\n4.1667\n4.3206\n3.8213\n3.8567\n4.3957\n4.7447\nAbstention Rate\n–\n–\n0.0759\n0.0610\n–\n–\n0.1319\n0.2894\ngemma-3-4b\nAccuracy\n0.3872\n0.3957\n0.3787\n0.3716\n0.3262\n0.3390\n0.3163\n0.3262\nLAC Set Size\n4.1518\n3.9461\n4.7730\n4.9574\n4.2525\n4.1801\n4.9957\n5.0525\nAPS Set Size\n3.9943\n3.9305\n5.3149\n4.7589\n3.8752\n4.2496\n5.0780\n5.0610\nAbstention Rate\n–\n–\n0.0255\n0.0227\n–\n–\n0.0241\n0.0312\nmedgemma-4b-it\nAccuracy\n0.5262\n0.5035\n0.4993\n0.5106\n0.4511\n0.4298\n0.4085\n0.3943\nLAC Set Size\n3.3858\n3.1957\n4.2851\n4.2894\n3.6851\n3.8028\n4.7759\n4.7645\nAPS Set Size\n3.5319\n3.5390\n4.3504\n4.2695\n3.5716\n3.8468\n4.7106\n4.7262\nAbstention Rate\n–\n–\n0.0057\n0.0113\n–\n–\n0.0099\n0.0113\nphi-4\nAccuracy\n0.6908\n0.7050\n0.6695\n0.6879\n0.5447\n0.5844\n0.5163\n0.5078\nLAC Set Size\n2.0638\n2.0794\n2.4950\n2.4539\n2.7121\n2.7135\n3.4766\n3.9589\nAPS Set Size\n2.9007\n2.8468\n2.9390\n3.4723\n3.0652\n2.9887\n3.4965\n3.9447\nAbstention Rate\n–\n–\n0.0128\n0.0085\n–\n–\n0.0326\n0.0865\n"}, {"page": 29, "text": "No Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nZero-shot Few-shot Zero-shot Few-shot Zero-shot\nFew-shot\nZero-shot\nFew-shot\ngpt-4.1\nAccuracy\n0.8213\n0.8355\n0.8000\n0.7801\n0.6908\n0.6993\n0.6511\n0.6596\nLAC Set Size\n1.9376\n2.2071\n2.0496\n2.3447\n2.6525\n2.8213\n2.8709\n3.2071\nAPS Set Size\n4.5461\n4.3773\n5.6312\n5.6170\n4.5220\n4.4525\n5.4936\n5.3121\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4.1-nano\nAccuracy\n0.5404\n0.4979\n0.4099\n0.3390\n0.4511\n0.3957\n0.3688\n0.3024\nLAC Set Size\n3.8014\n3.5674\n4.2057\n4.1248\n3.7603\n4.1079\n4.5092\n4.4860\nAPS Set Size\n4.2426\n3.9929\n4.9021\n4.9518\n4.2270\n4.1906\n5.0411\n5.0577\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o\nAccuracy\n0.6766\n0.7206\n0.6596\n0.6468\n0.6170\n0.6213\n0.5702\n0.5518\nLAC Set Size\n2.5362\n2.4539\n2.9518\n2.4746\n3.0780\n3.0340\n3.7220\n3.3589\nAPS Set Size\n4.3163\n4.3986\n4.9943\n5.3355\n3.9901\n4.2511\n5.0738\n5.3404\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\ngpt-4o-mini\nAccuracy\n0.4823\n0.5674\n0.4610\n0.4823\n0.4298\n0.4809\n0.3674\n0.4184\nLAC Set Size\n3.5135\n3.6851\n4.0440\n4.3149\n3.8511\n4.0340\n4.5957\n4.8624\nAPS Set Size\n3.9603\n3.9887\n4.5972\n4.6142\n4.1277\n4.0227\n4.7773\n5.2014\nAbstention Rate\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n"}, {"page": 30, "text": "Table 7: MedQA: Experiment results for the Qwen thinking mode. The darker the entry, the better across all\nevaluation metrics. (Higher accuracy, lower set size, higher abstention\nNo Abstention\nAbstention\nNo Abstention + Perturbed Abstention + Perturbed\nModel\nMetric\nThinking NoThinking Thinking NoThinking Thinking\nNoThinking\nThinking NoThinking\nQwen25-05B-Instruct\nAccuracy\n0.2908\n0.2837\n0.2518\n0.2610\n0.2688\n0.2525\n0.2369\n0.2312\nLAC Set Size\n4.3397\n4.3943\n5.2617\n5.2993\n4.4723\n4.4596\n5.2865\n5.3887\nAPS Set Size\n4.5780\n4.8426\n5.5887\n5.8780\n4.8596\n4.8652\n5.3390\n5.3716\nAbstention Rate\n–\n–\n0.0965\n0.0865\n–\n–\n0.0936\n0.0908\nQwen25-14B-Instruct\nAccuracy\n0.6213\n0.6128\n0.5681\n0.5518\n0.4915\n0.4766\n0.3851\n0.3986\nLAC Set Size\n2.7489\n2.9206\n3.3794\n3.4156\n3.2695\n3.2936\n4.4645\n4.4085\nAPS Set Size\n3.2482\n3.1957\n3.8206\n4.1121\n3.5695\n3.4553\n4.3901\n4.4369\nAbstention Rate\n–\n–\n0.1113\n0.1291\n–\n–\n0.2567\n0.2142\nQwen25-15B-Instruct\nAccuracy\n0.3986\n0.4085\n0.3695\n0.3688\n0.3191\n0.3390\n0.3028\n0.3021\nLAC Set Size\n3.9872\n4.0624\n4.8149\n4.9816\n4.0943\n4.1872\n5.0887\n5.1064\nAPS Set Size\n4.1766\n4.3518\n5.0447\n5.1220\n4.4645\n4.3745\n5.3610\n5.3631\nAbstention Rate\n–\n–\n0.0574\n0.1163\n–\n–\n0.0872\n0.1248\nQwen25-3B-Instruct\nAccuracy\n0.4539\n0.4468\n0.3887\n0.3759\n0.3702\n0.3730\n0.2929\n0.2950\nLAC Set Size\n3.6596\n3.9475\n4.9837\n5.0156\n4.0170\n4.0979\n5.1972\n5.2709\nAPS Set Size\n3.7567\n3.7348\n5.0553\n4.8071\n3.9553\n4.0014\n5.1177\n4.9021\nAbstention Rate\n–\n–\n0.1617\n0.1830\n–\n–\n0.1929\n0.2652\nQwen25-7B-Instruct\nAccuracy\n0.5298\n0.5461\n0.4355\n0.3745\n0.4340\n0.4284\n0.2979\n0.2496\nLAC Set Size\n3.0383\n3.2837\n4.3213\n4.5787\n3.6000\n3.7333\n4.6688\n4.6752\nAPS Set Size\n3.6397\n3.6199\n4.2277\n4.4936\n3.7411\n3.6851\n5.1894\n5.0950\nAbstention Rate\n–\n–\n0.2475\n0.3929\n–\n–\n0.3539\n0.4894\nQwen3-06B\nAccuracy\n0.2681\n0.2652\n0.2064\n0.1730\n0.2333\n0.2284\n0.1745\n0.1574\nLAC Set Size\n4.4220\n4.3688\n4.9532\n5.0170\n4.3567\n4.3362\n5.1461\n4.9830\nAPS Set Size\n4.6028\n4.3050\n5.3028\n5.3035\n4.9050\n4.9135\n5.8801\n5.9007\nAbstention Rate\n–\n–\n0.2894\n0.3957\n–\n–\n0.3404\n0.3901\nQwen3-1-7B\nAccuracy\n0.3823\n0.3617\n0.3454\n0.3121\n0.3270\n0.3305\n0.2809\n0.2539\nLAC Set Size\n4.1199\n4.2241\n4.8879\n5.0071\n4.2965\n4.4397\n5.0965\n5.0340\nAPS Set Size\n4.2277\n4.0085\n5.0440\n5.3645\n4.3113\n4.3291\n5.5872\n5.8085\nAbstention Rate\n–\n–\n0.0858\n0.1348\n–\n–\n0.1270\n0.1660\nQwen3-14B\nAccuracy\n0.5567\n0.4780\n0.4979\n0.3716\n0.4504\n0.4057\n0.3645\n0.2738\nLAC Set Size\n2.6908\n2.9362\n3.6496\n4.4270\n3.2496\n3.4184\n4.4050\n4.7631\nAPS Set Size\n3.2191\n3.4227\n4.1695\n4.8028\n3.7532\n4.0085\n4.8113\n5.1418\nAbstention Rate\n–\n–\n0.1191\n0.2184\n–\n–\n0.1887\n0.2794\nQwen3-32B\nAccuracy\n0.6454\n0.4681\n0.6106\n0.5830\n0.5716\n0.2156\n0.5461\n0.1759\nLAC Set Size\n2.4851\n2.5858\n2.9794\n3.1887\n3.0099\n1.0000\n3.9092\n1.0000\nAPS Set Size\n2.8518\n3.0369\n3.1908\n3.1858\n3.2284\n1.0000\n4.1830\n1.0000\nAbstention Rate\n–\n–\n0.0149\n0.0369\n–\n–\n0.0695\n0.1631\nQwen3-4B\nAccuracy\n0.5085\n0.5007\n0.4780\n0.4766\n0.4184\n0.4028\n0.3355\n0.3447\nLAC Set Size\n2.9638\n2.8057\n3.8496\n3.6199\n3.6227\n3.4014\n4.6759\n4.5078\nAPS Set Size\n3.4667\n3.4057\n4.3028\n4.2085\n3.7064\n3.5433\n4.7454\n4.4809\nAbstention Rate\n–\n–\n0.0738\n0.0894\n–\n–\n0.1709\n0.1262\nQwen3-8B\nAccuracy\n0.6028\n0.5957\n0.5426\n0.5348\n0.4794\n0.4695\n0.3901\n0.4128\nLAC Set Size\n2.9574\n3.0539\n3.5645\n3.6170\n3.4277\n3.5887\n4.4284\n4.4071\nAPS Set Size\n3.6057\n3.6397\n4.2652\n4.1234\n3.8390\n3.8213\n4.5702\n4.3957\nAbstention Rate\n–\n–\n0.0723\n0.0681\n–\n–\n0.2106\n0.1319\n"}]}