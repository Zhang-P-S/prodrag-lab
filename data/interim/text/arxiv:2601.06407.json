{"doc_id": "arxiv:2601.06407", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.06407.pdf", "meta": {"doc_id": "arxiv:2601.06407", "source": "arxiv", "arxiv_id": "2601.06407", "title": "Value of Information: A Framework for Human-Agent Communication", "authors": ["Yijiang River Dong", "Tiancheng Hu", "Zheng Hui", "Caiqi Zhang", "Ivan Vulić", "Andreea Bobu", "Nigel Collier"], "published": "2026-01-10T03:07:41Z", "updated": "2026-01-10T03:07:41Z", "summary": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.06407v1", "url_pdf": "https://arxiv.org/pdf/2601.06407.pdf", "meta_path": "data/raw/arxiv/meta/2601.06407.json", "sha256": "524fc29714ed4155df72c558921df50a34dd5a252b1827e1ea5256562ffad62f", "status": "ok", "fetched_at": "2026-02-18T02:21:58.843293+00:00"}, "pages": [{"page": 1, "text": "Value of Information: A Framework for Human–Agent Communication\nYijiang River Dong1, Tiancheng Hu1, Zheng Hui1, Caiqi Zhang1\nIvan Vuli´c1, Andreea Bobu2*, Nigel Collier1*\n1University of Cambridge\n2MIT\n{yd358,th656,zh403,cz391,iv250,nhc30}@cam.ac.uk\nabobu@mit.edu\nAbstract\nLarge Language Model (LLM) agents de-\nployed for real-world tasks face a fundamental\ndilemma: user requests are underspecified, yet\nagents must decide whether to act on incom-\nplete information or interrupt users for clari-\nfication. Existing approaches either rely on\nbrittle confidence thresholds that require task-\nspecific tuning, or fail to account for the vary-\ning stakes of different decisions. We intro-\nduce a decision-theoretic framework that re-\nsolves this trade-off through the Value of Infor-\nmation (VoI), enabling agents to dynamically\nweigh the expected utility gain from asking\nquestions against the cognitive cost imposed\non users. Our inference-time method requires\nno hyperparameter tuning and adapts seam-\nlessly across contexts—from casual games\nto medical diagnosis.\nExperiments across\nfour diverse domains (20 Questions, medical\ndiagnosis, flight booking, and e-commerce)\nshow that VoI consistently matches or exceeds\nthe best manually-tuned baselines, achieving\nup to 1.36 utility points higher in high-cost\nsettings.\nThis work provides a parameter-\nfree framework for adaptive agent communi-\ncation that explicitly balances task risk, query\nambiguity, and user effort.\nOur code will\nbe available at https://github.com/\ndong-river/VOI_communication.\n1\nIntroduction\nLLM agents are increasingly deployed as au-\ntonomous collaborators in complex, real-world\ntasks. However, a fundamental bottleneck remains:\nuser requests are inherently underspecified, carry-\ning latent goals, contexts, and unstated preferences\n(Malaviya et al., 2024; Yao et al., 2024; Peng et al.,\n2024; Dong et al., 2024; Hui et al., 2025d). A re-\nquest to “book a flight to London” omits critical\ndetails, such as budget constraints, preferred depar-\nture times, tolerance for layovers. No amount of\n*Equal advising.\nmodel capability can resolve this ambiguity with-\nout external input; the agent must ask. Yet exces-\nsive questioning frustrates users and undermines\nthe agent’s value proposition. Effective collabo-\nration thus requires agents to balance two risks:\nacting on incomplete information and misaligning\nwith user intent, or interrupting frequently and im-\nposing cognitive burden.\nCurrent approaches fall short in navigating this\ntrade-off. Fixed-round strategies ask a predeter-\nmined number of questions regardless of context,\nignoring task-specific needs. Adaptive methods\ntrigger clarification when model confidence falls\nbelow a manually-tuned threshold, but this thresh-\nold selection is brittle and fails to generalize across\ndomains or cost structures. Neither approach ex-\nplicitly reasons about whether the information\ngained justifies the user’s effort.\nWe argue that agents should treat communica-\ntion as a rational decision, asking questions only\nwhen the expected improvement in task outcomes\njustifies the user’s time and effort. We adopt a Ra-\ntional Speech Act (RSA) perspective (Goodman\nand Frank, 2016; Frank and Goodman, 2012) view-\ning dialogue as a rational action. Building on prior\nRSA work on interactive questioning-answering\n(Hawkins et al., 2015) and utility-grounded prag-\nmatic reasoning (Sumers et al., 2021), the agent\nshould only ask questions when the expected ben-\nefit of improved downstream decisions outweighs\nthe cost of additional interaction—capturing both\ncost of communication (Hawkins et al., 2015) and\nutility of downstream decisions (Sumers et al.,\n2021). Under this lens, we formalize the clarify-or-\ncommit decision through three contextual factors:\n(1) Query Ambiguity: the degree of uncertainty\nabout the user’s true intent; (2) Task Risk: the\nseverity of the consequences of a wrong action; and\n(3) Cognitive Load: the cost, in time and effort,\nimposed on the user by asking for clarification.\nTo operationalize this reasoning, we propose\narXiv:2601.06407v1  [cs.CL]  10 Jan 2026\n"}, {"page": 2, "text": "a decision-theoretic framework grounded in the\nValue of Information (VoI), a classic principle from\ndecision theory (Raiffa and Schlaifer, 1961). Our\ninference-time method allows an LLM to explic-\nitly calculate the expected utility gain of asking\na potential question, weighing it directly against\nthe communication cost. This provides a princi-\npled mechanism for the agent to decide whether\nthe information it might receive is worth the user’s\nattention. Our contributions are threefold: (a) We\nformalize the adaptive communication problem in\nhuman-agent interaction from a decision-theoretic\nperspective, identifying three key factors: ambi-\nguity, risk, and cognitive load. (b) We propose a\npractical, inference-time VOI-based method that\nallows an LLM to estimate these contextual factors\nand dynamically decide whether to act or to seek\nclarifications (c) We demonstrate through experi-\nments across four distinct domains: 20 Questions,\nmedical diagnosis, flight booking, and online shop-\nping, that our parameter-free VoI method automati-\ncally identifies the optimal operating point. Across\nvarying communication costs, VoI matches or ex-\nceeds the best manually-tuned baselines in 18 of\n20 conditions, achieving utility gains of up to 1.36\npoints in high-cost settings.\n2\nRelated Work\nStandard LLM Agent Paradigm.\nOur work is\nsituated within the broader context of developing\nautonomous LLM agents. Much foundational re-\nsearch in this area focuses on improving agent rea-\nsoning, planning, and tool-use capabilities. Promi-\nnent paradigms like Yao et al. (2023) and others are\noften evaluated in benchmarks that, while complex,\nassume the user’s initial instruction is complete\nand unambiguous (Yao et al., 2022; Zhou et al.,\n2023; Xie et al., 2024). This focus on task execu-\ntion rather than the real-world productivity users\nexpect from agents, leaving a critical gap for truly\ndeploying agents (Sun et al., 2025; Shah and White,\n2024; Zhou and Sun, 2025; Hui et al., 2025c).\nRecently, a new wave of research has begun to\naddress agent reliability by introducing principled\nframeworks from decision theory (Liu et al., 2024;\nLin et al., 2024; Chen et al., 2025). However, these\napproaches typically focus on making an optimal\ndecision given a static, pre-defined state of infor-\nmation. Our work bridges these two areas: we\nadopt the rigor of decision theory but focus on the\nupstream problem of active information gathering,\nallowing the agent to dynamically resolve ambigu-\nity before committing to an action.\nLLM Proactive Communication.\nPrior work\nhas explored prompting techniques to improve\nLLM interactivity. These methods can elicit user\npreferences (Li et al., 2023) or encourage active\ndisambiguation of ambiguous queries (Deng et al.,\n2023; Zhang et al., 2024c). While prompting can\ndirectly induce clarifying behaviors, prior work\nshows that the resulting strategies are often subop-\ntimal without more principled planning or learning\nalgorithms. Our work provides such a principled\nalgorithm to govern the agent’s communication de-\ncisions.\nUncertainty-Gated and Information-Theoretic\nMethods.\nA more systematic approach uses\nmodel-uncertainty estimates to decide when to seek\nclarification, triggering a question when predic-\ntion confidence or entropy falls below a selected\nthreshold (Wang et al., 2025; Zhang and Choi,\n2023; Kuhn et al., 2022; Ren et al., 2023; Grand\net al., 2025). While an improvement over heuristics,\nthese information-centric views can be insufficient,\nas they do not directly consider the downstream\ntask’s stakes. Our method addresses this by em-\nploying the Value of Information (VoI) (Raiffa and\nSchlaifer, 1961; Howard, 1966), a core concept\nfrom decision theory. Instead of measuring infor-\nmation gain in isolation, VoI measures how that\ninformation is expected to improve the utility of\nthe final action, explicitly connecting the purpose\nof communication to the stakes of the decision.\nLearning-Based Approaches.\nDifferent from\nthe inference-time algorithms above, another line\nof research uses reinforcement learning to improve\nLLM collaboration with humans. Variants of Di-\nrect Preference Optimization (DPO) have been ap-\nplied to encourage models to request clarification\nwhen needed (Zhang et al., 2024b; Chen et al.,\n2024; Wu et al., 2025; Qian et al., 2025; Sun et al.,\n2025). However, RL is often task-specific, requir-\ning a carefully designed simulation environment\nand training pipeline, which is fundamentally dif-\nferent from our VOI-based method which operate\npurely at inference-time.\nRational Speech Act\nRSA-style pragmatic mod-\nels cast language as (approximately) rational action:\nspeakers choose utterances to shape a listener’s in-\nferences under explicit priors and costs (Frank and\n"}, {"page": 3, "text": "Figure 1: Illustration of different communication methods and user reaction. Given user flight history, an\nLLM agent is able to infer user latent preferences with some probability. Excessive questions that asks about every\naspect of preference would lead to user dissatisfaction (A) while directly acting without communication could lead\nto unexpected consequences (B). Decision-theoretic reasoning can balance expected utility gain via asking user\nquestions against communication cost to achieve efficient but effective communication at inference time (C).\nGoodman, 2012; Goodman and Frank, 2016). Be-\nyond single-shot reference, RSA has been extended\nto interactive question–answering, where questions\nare selected to trade off expected informativeness\nagainst asking cost (Hawkins et al., 2015), and to\naction-oriented settings where the point of com-\nmunication is not only belief change but improv-\ning downstream decisions (e.g., signaling bandits)\n(Sumers et al., 2021). Researchers then extend to\n“Neural RSA” that replace hand-specified literal\nmodels with learned speakers/listeners in grounded\ntasks (Andreas and Klein, 2016; Monroe et al.,\n2017). Most recently, RSA has been adapted to\nthe era of LLMs, serving both as an inference-time\ncontrol to guide generation (Wang and Demberg,\n2024; Cao et al., 2025).\n3\nProblem Formulation\nWe formulate the adaptive communication task as a\nsequential decision-making process where an LLM\nagent interacts with a user to select an optimal\naction.\nPreliminaries.\nThe agent receives an initial, po-\ntentially ambiguous, user query S. The user’s true\ngoals and preferences are represented by a latent\nstate θ ∈Θ, which is not directly observable by\nthe agent. The agent has access to a set of possi-\nble terminal actions a ∈A. To resolve ambiguity\nabout θ and choose the best action a∗, the agent\ncan engage in a multi-turn dialogue with the user.\nThe Clarify-or-Commit Process.\nThe inter-\naction proceeds in a sequence of turns.\nAt\neach turn t, given the dialogue history Ht =\n(q1, u1, . . . , qt−1, ut−1), the agent must make a de-\ncision:\n1. CLARIFY: Select and pose a question qt\nfrom a set of possible questions Q. Upon\nreceiving the user’s answer ut, the history is\nupdated to Ht+1 and the process continues.\n2. COMMIT: Terminate the dialogue and select\na final action a ∈A based on the current\nhistory Ht.\nThe agent’s strategy for making this choice at each\nturn is the clarify-or-commit policy, which is the\ncentral object of our study. This simple clarify-or-\ncommit choice lies at the heart of adaptive commu-\nnication: every question carries both the potential\nto reduce uncertainty and the cost of additional user\neffort.\nUtility and Objective.\nThe success of a com-\nmitted action a is measured by a utility func-\ntion U(θ, a), which quantifies how well the action\naligns with the user’s true latent state θ. Communi-\ncation incurs a cost c(H), representing the user’s\ncognitive load, which quantifies the time and effort\nuser spent on the dialogue. If the agent commits to\naction a after a final history H, the total utility is\nU(θ, a) −c(H). The agent’s objective is to devise\na policy that maximizes the expected total reward,\noptimally balancing the utility gain from asking\nquestions against cumulative communication cost.\n"}, {"page": 4, "text": "4\nMethods\nTo address the clarify-or-commit problem, an agent\nrequires a principled policy for deciding when the\npotential benefit of asking a question outweighs the\ncost of interaction. Simple heuristic-based strate-\ngies often fail because they do not explicitly reason\nabout the downstream consequences or the stakes\nof the decision. To overcome this limitation, we\npropose an adaptive policy grounded in the Value\nof Information (VoI), a core concept from decision\ntheory (Raiffa and Schlaifer, 1961).\n4.1\nValue of Information Framework\nThe baselines above are either non-adaptive or rely\non generic, task-agnostic heuristics like confidence.\nThey fail to explicitly reason about the value of\nthe information a question might provide in the\ncontext of heterogeneous task stakes and unequal\nfeature importance. To address this, we formalize\nour approach using the VoI framework.\nBeliefs and Expected Utility.\nLet Θ be the set\nof possible latent user intents (e.g., the specific\nproduct features preferred or the true medical con-\ndition). The agent maintains a belief distribution\nb(θ) over Θ. Given this belief, the expected utility\n(EU) of committing to a terminal action a ∈A is:\nEU(a | b) = Eθ∼b[U(θ, a)] =\nX\nθ∈Θ\nb(θ)U(θ, a).\n(1)\nIf the agent were to commit immediately, it would\nchoose the action a∗= arg maxa∈A EU(a | b).\nThe utility of this decision is the value of acting\nunder the current belief b:\nV (b) = max\na∈A EU(a | b).\n(2)\nCalculating the Value of a Question.\nTo evalu-\nate a potential question q, the agent considers the\nset of possible answers Y. For any given answer\ny ∈Y, the agent would update its belief to a poste-\nrior by(θ) = P(θ | H, q, y). The expected value of\nthe decision after receiving an answer to question\nq is the expectation over all possible answers y:\nVpost(b, q) =\nX\ny∈Y\np(y | q, b) · V (by),\n(3)\nwhere p(y | q, b) is the probability of receiving\nanswer y given the current belief. In practice, to\nmake computation feasible, we restrict the answer\nspace to a closed set of multiple choice or yes-\nno questions. For each sampled hypothesis θ, we\nquery the LLM to simulate the likelihood of each\nresponse y given question q, aggregating these to\nfind the marginal probability p(y | q, b).\nThe Value of Information for question q is the\ndifference between the expected utility after asking\nand the utility of acting now:\nVoI(q) = Vpost(b, q) −V (b).\n(4)\nThe Clarify-or-Commit Policy.\nOur framework\nuses this VoI calculation to establish a decision rule.\nAt each turn, the agent evaluates the net utility gain\nfor each candidate question:\nNetVoI(q) = VoI(q) −c,\n(5)\nwhere c is the per-question communication cost.\nThe agent selects the question q∗with the highest\npositive net value. If maxq NetVoI(q) ≤0, the\nexpected utility gain from further communication\nis not worth the cost. The agent terminates the\ndialogue and commits to the best action under its\ncurrent belief.\n4.2\nInstantiation with LLMs\nWhile Section 4.1 establishes the theoretical foun-\ndations of our approach, in this section, we describe\nhow we leverage LLMs to approximate these com-\nponents at inference time.\nEstimating and Updating Belief Distributions.\nGiven the set of candidate latent factors Θ, we\nprompt the LLM to explicitly quantify its un-\ncertainty by outputting a probability distribution\nb(θ) over these factors. Different from standard\nBayesian approaches update beliefs analytically via\na fixed likelihood function, we employ a LLM to es-\ntimate the probability distribution over Θ (Liu et al.,\n2024; Kobalczyk et al., 2025; Hu et al., 2025a;\nChen et al., 2026). To obtain the posterior belief by\nrequired for Eq. 3, we feed the history augmented\nwith a simulated interaction (question q and hypo-\nthetical answer y) back into the model and prompt\nit to re-estimate the distribution over Θ. This al-\nlows the agent to dynamically update its confidence\nbased on the semantic content of the answer.\nSimulating User Responses.\nTo calculate the\nexpected value of a question, we perform a one-\nstep lookahead simulation (Kobalczyk et al., 2025)\nto estimate the marginal likelihood of possible\nanswers p(y | q, b).\nTo ensure computational\n"}, {"page": 5, "text": "tractability in Eq. 3, we constrain the agent to ask\nclosed-ended questions (e.g., multiple-choice or\nYes-No questions), thereby defining a finite answer\nspace Y. The probability of each response is com-\nputed by marginalizing over the current beliefs:\np(y | q, b) ≈P\nθ∈Θ p(y | q, θ)b(θ), where the\nterm p(y | q, θ) represents the LLM’s prediction of\nthe user’s response assuming θ is the ground truth.\n5\nExperimental Setup\n5.1\nBaseline Methods\nNo-Question. This baseline represents the stan-\ndard agent paradigm. Given the initial query S, the\nagent commits to an action immediately without\nany communication with the user. It relies solely\non its initial understanding of the user’s intent.\nFixed-Round.\nThis non-adaptive baseline asks\na fixed number of k questions before committing\nto an action. It serves to isolate the benefit of in-\nteraction from the benefit of adaptive interaction\nby exploring a fixed trade-off between information\ngathering and communication cost.\nAdaptive Prompting.\nThis baseline prompts the\nLLM to reason about whether it feels confident\nenough to act or if it should ask a question. The\nnumber of questions is not predetermined, but the\ndecision to stop is based on the model’s heuristic\nself-assessment rather than a formal criterion.\nConfidence Thresholding.\nThis adaptive base-\nline formalizes the heuristic of Adaptive Prompting.\nThe agent continues to ask questions as long as its\npredictive confidence in the best action a∗remains\nbelow a tunable threshold τ. We measure con-\nfidence using the model’s verbalized confidence\nscores (Tian et al., 2023; Zhang et al., 2024a), a\ncommon practice for modern LLMs. This method\nis adaptive, but crucially, the threshold τ must be\nmanually tuned for each task and cost setting to\nachieve optimal performance.\n5.2\nTasks and Models\nMixed-Stakes 20 Questions.\nThe 20 Questions\ngame is a classic guessing game with a long history\nas a paradigm for studying human and artificial\ndecision-making under uncertainty. It provides a\ncontrolled environment to test how an agent per-\nforms strategic information gathering. Following\nthe setup of Hu et al. (2024), the agent must iden-\ntify a target concept from a known candidate set by\nasking a series of binary (yes/no) questions. Our\nkey modification is to explicitly test how the agent\nadapts to varying task risk. We create two parallel\nversions of this task:\n• Low-Stakes (Animal Guessing): The agent\nidentifies an animal from a set of 100. A cor-\nrect guess yields a terminal utility of U = 1.\n• High-Stakes (Medical Diagnosis): The agent\ndiagnoses a medical condition from a set of\n15 diseases, using real doctor-patient chat his-\ntories as input. A correct diagnosis yields a\nutility of U = 10.\nFlight Recommendation\nWe adopt a task de-\nsigned to model the elicitation of multi-faceted user\npreferences, a common challenge when aligning\nagents with diverse user values (Dong et al., 2025a).\nOur setup is inspired by the recent work of (Qiu\net al., 2025) is derived from the FLIGHTPREF\ndataset originally proposed by Lin et al. (2022).\nThe agent is presented with a user’s choice history\nover five rounds of flight selections. In a final, held-\nout round, the agent must predict which of three\nnew flight options the user will prefer. Each flight\nis defined by 8 features (e.g., price, stops, airline),\nand each user has a latent reward function defining\ntheir preferences over these features. The agent\ncan ask clarifying questions to uncover these pref-\nerences before making its final prediction. This\ntask tests the agent’s ability to strategically query a\ncomplex, multi-attribute preference space to infer a\nuser’s reward model from their contextual choices.\nThe agent’s prediction for the new round will be\nscored based on this reward function.\nAmbiguous WebShop\nTo test our agent in a\nmore realistic, interactive environment, we adapt\nthe WebShop benchmark (Yao et al., 2022). In the\noriginal setting, user instructions are created to be\nrelatively well-specified (e.g., “buy a red Adidas\nt-shirt, size medium”). We deliberately introduce\nquery ambiguity by removing details from the\nuser’s request (e.g., “buy a t-shirt”) to simulate un-\nderspecified real-world user query. The agent must\nthen decide whether to act on this partial informa-\ntion (e.g., search(\"t-shirt\")) or to ask clar-\nifying questions about attributes like size, color, or\nbrand. This task evaluates the agent’s ability to bal-\nance autonomous web navigation with strategic in-\nformation gathering to resolve under-specified user\nrequests. We use GPT-4o to provide a score ∈[0, 1]\nfor the purchased product against the ground-truth\nproduct provided in Yao et al. (2022).\n"}, {"page": 6, "text": "Algorithm 1 VOI Algorithm\nRequire: Instruction S; action set A; utility U(θ, a); question generator GenQ; belief updater Update; cost c(·); clarification budget Kmax\n1: H ←{S};\nb ←Prior(S)\n2: for t = 1, 2, . . . , Kmax do\n3:\nQ ←GenQ(H)\n▷small set of targeted questions\n4:\nV0 ←V (b) = maxa∈A Eθ∼b[U(θ, a)]\n5:\nfor all q ∈Q do\n6:\nSample plausible replies {(yk, πk)}K\nk=1 from P (· | b, q)\n7:\nVq ←PK\nk=1 πk V\n\u0000Update(b, q, yk)\n\u0001\n8:\nVoI(q) ←Vq −V0 −c(q)\n9:\nend for\n10:\nq∗←arg maxq∈Q VoI(q)\n11:\nif VoI(q∗) ≤0 then break\n▷clarification not worthwhile\n12:\nelse\n13:\nAsk q∗, observe y; H ←H ∪{(q∗, y)}; b ←Update(b, q∗, y)\n14:\nend if\n15: end for\n16: return a∗∈arg maxa∈A Eθ∼b[U(θ, a)]\n▷final commitment\nModels\nWe consider a selection of leading LLMs\nto evaluate the performance of our proposed\nmethod, including GPT-4.1 (OpenAI, 2025) and\nGemini-2.5-Flash (Comanici et al., 2025).\n6\nResults\n6.1\nMain Results\nOur central findings are summarized in Figure 2.\nAcross all tasks and communication cost settings,\nour VoI-based agent consistently achieves state-of-\nthe-art utility. Crucially, it does so without requir-\ning task-specific threshold tuning, showcasing its\nrobustness and practical advantages.\nVoI excels by finding the optimal utility-cost\nbalance.\nAs shown in Figure 2, our VoI agent\n(starred marker) consistently ranks as the top-\nperforming method across the Mixed 20Q, Flight\nRecommendation, and Ambiguous WebShop tasks.\nFor instance, in Mixed 20Q with a communica-\ntion cost of c = 0.01, VOI achieves a utility of\n14.14, significantly outperforming the best-tuned\nconfidence-thresholding baseline (11.49 at τ =\n0.90). This performance advantage stems from\nVOI’s ability to dynamically determine the optimal\nnumber of clarification questions, a stark contrast\nto fixed-round and confidence-based methods that\nrequire brittle, manual tuning of a threshold for\neach specific task and cost structure.\nAdaptive communication is essential for ambigu-\nous tasks.\nThe “No Question” baseline estab-\nlishes the necessity of proactive communication.\nOn the Mixed 20Q task, where the initial query is\ninherently underspecified, this baseline’s accuracy\nis near zero for both low-stakes (animal) and high-\nstakes (medical) variants. However, as shown in\nFigures 2(f) and 2(l), when communication costs\nare prohibitively high, avoiding questions becomes\na competitive strategy. In these scenarios, our VOI\nmethod correctly adapts by stopping communica-\ntion early, demonstrating its ability to gracefully\nhandle the full spectrum of cost-benefit scenarios.\nAdaptive prompting are insufficient for robust\nperformance.\nThe Adaptive Prompting baseline\nshows that simply instructing an LLM to “ask\nquestions when needed” offers an improvement\nover non-adaptive strategies. However, its perfor-\nmance is inconsistent and consistently lower than\nmore structured methods. This is because the deci-\nsion to communicate is based on the model’s inter-\nnal “feeling” of confidence, which is often poorly\ncalibrated (Hu et al., 2025b; Zhang et al., 2026),\nrather than a formal criterion. It lacks a principled\nmechanism to weigh the potential information gain\nagainst the explicit communication cost, leading to\nsuboptimal and unpredictable behavior.\nFixed-round communication strategies are fun-\ndamentally suboptimal.\nA fixed-round policy,\nwhich asks a predetermined number of questions,\nfails to adapt to the specific needs of a given query.\nAs illustrated in the inverted-U shape of the “Fixed\nRound” curves in Figure 2, utility initially increases\nwith more questions but then declines as communi-\ncation costs overwhelm the benefits of additional in-\nformation. The optimal number of questions varies\nsignificantly with the task and cost, highlighting\nthe necessity of an adaptive policy.\nConfidence thresholding is effective but brittle.\nThe confidence thresholding baseline provides a\nstrong, adaptive competitor. With the correctly\ntuned confidence threshold τ, its performance can\nbe comparable to our VOI method (e.g., on GPT-4\nfor Mixed 20Q and Webshop). However, this effec-\ntiveness is its Achilles’ heel; the optimal τ is highly\nsensitive and must be manually selected for each\n"}, {"page": 7, "text": "(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n(l)\nFigure 2: Utility vs. Communication Rounds. Final utility as a function of the number of clarification questions\nasked across our three tasks, for GPT-4 (top two rows) and Gemini-2.5-Flash (bottom two rows), with communication\ncosts c = 0.01 and c = 0.05. Utility is defined as U(θ, a) −T · c. The curves for Fixed Round and Confidence\nThresholding represent Pareto frontiers generated by varying their respective hyperparameters (k and τ). In contrast,\nour VoI agent (starred) is a parameter-free method. In nearly all settings, VoI automatically identifies an operating\npoint that matches or exceeds the performance of the best-tuned baseline, demonstrating its superior adaptability\nand practical value.\ntask and cost combination, making it impractical\nfor real-world deployment. Our VoI method pro-\nvides a principled solution that matches or exceeds\nthis performance without any such manual tuning.\n6.2\nAblation Study\nAblation on Communication Cost.\nAs shown\nin Table 1, across the cost sweep on Mixed 20-\nQuestion the VoI controller matches or exceeds the\nstrongest grid-searched baselines. We tune four\nbaselines over nine threshold settings, and while\n"}, {"page": 8, "text": "Table 1: VOI vs. Baselines Across Costs (Gemini-2.5-Flash, Mixed 20 Question). This table compares the VOI\npolicy’s expected reward (rVOI) against the best and second-best baselines via grid searching over 9 values. The ∆\ncolumns report VOI’s margin over each baseline (positive means VOI is better).\nCost\nBest Baseline\nrmax\nSecond Best\nrsecond\nrVOI\nrVOI–rmax\nrVOI–rsecond\n0.01\nConfidence (τ=0.9)\n8.30\nRound (τ=15)\n8.10\n8.64\n0.34\n0.54\n0.02\nConfidence (τ=0.9)\n6.88\nConfidence (τ=0.9)\n6.80\n7.72\n0.84\n0.92\n0.05\nRound (τ=5)\n3.65\nConfidence (τ=0.5)\n3.64\n5.01\n1.36\n1.37\n0.10\nConfidence (τ=0.5)\n2.28\nRound (τ=5)\n0.90\n1.38\n-0.90\n0.48\n0.20\nNo Question\n0\nRound (τ=5)\n-4.60\n-0.96\n-0.96\n3.64\n(a)\n(b)\n(c)\nFigure 3: Calibration Analysis The figure presents the calibration analysis of GPT-4 and Gemini-2.5-Flash on\nAnimal Guessing, Medical Diagnoiss, and Flight Recommendation. (In (c) the accuracy for predicted probability\nbetween 0 and 0.2 is omitted because very few samples fall in that range.\nthe best baseline shifts with the communication\ncost, VoI consistently selects an appropriate num-\nber of questions thst match the performance of the\nbest baseline. Importantly, this pattern is stable\nacross different choice of communication costs:\nVoI adapts smoothly to the stated cost rather than\nhinging on a brittle threshold choice.\nCalibration Analysis.\nA critical component of\nour VoI framework is the LLM’s ability to estimate\na belief distribution b(θ) over latent user states. To\nanalyze it, ideally we should compare model pre-\ndicted distribution to the ground truth distribution.\nHowever, in the absence of the ground truth distri-\nbution for our tasks, we instead measure the argmax\nfrom the distribution against the ground truth item\nas the standard calibration analysis to approximate\nits distribution estimation accuracy. As shown in\nFigure 3, The results reveal that models are reason-\nably calibrated in Animal Guessing game but less\ncalibrated for Medical Diagnosis which we suspect\nbecause of the inherent complication and noise in\nthe symptoms of diseases. Despite this, we see\nthat VOI are empirically effective and robust that\nconsistently matches if not perform the best base-\nlines after searching hyperparameters. We believe\nthat current and future work that could improving\nmodel calibration under missing context (Li et al.,\n2025; Zhang et al., 2026) could further improve the\nperformance of VOI.\n7\nConclusion\nCurrent LLM agents are often designed for well-\nspecified tasks, leaving them brittle when faced\nwith the inherent ambiguity of real-world user re-\nquests.\nIn this work, we argued that overcom-\ning this limitation requires agents to move beyond\nsimple execution and develop a principled strat-\negy for adaptive communication. We proposed\na formal framework for this problem, centered\non balancing three key factors: query ambigu-\nity, task risk, and user cognitive load. Our pri-\nmary contribution is a practical, inference-time\nmethod based on the Value of Information (VoI)\nthat operationalizes this framework. By explicitly\ncalculating the expected utility gain of a poten-\ntial question and weighing it against its commu-\nnication cost, our VoI-driven agent decides when\nto act and when to ask. Extensive experiments\nacross diverse domains—including medical diag-\nnosis and online shopping—demonstrate that our\napproach consistently outperforms non-adaptive\nand heuristic-based baselines. Crucially, it achieves\nthis without the need for the brittle, task-specific\nthreshold tuning that plagues other adaptive meth-\nods. Ultimately, this work provides a principled\n"}, {"page": 9, "text": "foundation for building LLM agents that are not\njust capable executors, but also thoughtful com-\nmunicators. By equipping agents with a formal\nunderstanding of when information is valuable, we\ncan create more aligned, efficient, and truly collab-\norative human-AI systems.\nLimitations\nScope of Interaction: Decision vs. Generation.\nOur work focuses on the core decision of when\nto communicate, rather than what questions to\ngenerate. To this end, our experiments utilize a\npredefined set of actions (a ∈A) and clarifying\nquestions, a methodological choice consistent with\nprior work (Hu et al., 2024; Kobalczyk et al., 2025).\nThis controlled setting isolates the performance of\nour VoI-based selection policy, providing an un-\nambiguous evaluation of our central claim. By\ncontrolling for the quality of question generation,\nwe demonstrate the effectiveness of the decision-\nmaking principle itself. Extending this framework\nto fully open-ended dialogue is an important next\nstep; establishing this selection principle is a nec-\nessary foundation. Our work provides the core\nengine around which more sophisticated generative\ncomponents can be built.\nModel of Communication Cost.\nWe employ a\nlinear communication cost model (c(H) = T · c).\nAccurately modeling the nuances of human cog-\nnitive load is a major, open research challenge\nin its own right, spanning HCI and cognitive sci-\nence. Therefore, in line with common practice in\ndecision-theoretic analyses, we adopt a simplified\nand interpretable cost function. This allows us to\nclearly illustrate the fundamental trade-off between\nutility gain and cost, without introducing confound-\ning variables from a more complex, speculative\ncognitive model. Importantly, the VoI framework\nitself is agnostic to the form of the cost function;\nthe core decision rule, VoI(q) −c(H), can read-\nily incorporate more sophisticated models as they\nare developed. We view the linear cost model as a\nreasonable first-order approximation that demon-\nstrates the framework’s viability, with refinement\nthrough empirical user research as a natural next\nstep.\nEthical Considerations\nWhile our VoI framework optimizes the trade-off\nbetween information gain and communication cost,\nuser agency must remain paramount: users should\nretain the ability to decline questions or proceed\nwithout clarification based on their own judgment.\nBeyond this, the act of questioning introduces crit-\nical considerations regarding user burden and pri-\nvacy. First, clarifying questions—even when the-\noretically optimal—inherently impose a cognitive\ndemand on the user; an agent that queries too\nfrequently or intrusively risks eroding trust and\ncausing frustration, necessitating cost models that\nstrictly penalize unnecessary interruptions. Second,\nthe pursuit of resolving ambiguity often requires\neliciting specific, potentially sensitive information\n(e.g., medical symptoms or personal preferences)\nto update the agent’s belief distribution. It is im-\nperative that future implementations incorporate\nstrict data minimization principles and privacy safe-\nguards to ensure that the agent’s drive for reduced\nuncertainty does not compromise user privacy or\ncomfort (Hui et al., 2025b,a; Dong et al., 2025b).\nWe acknowledge the use of AI tools for refining\nthe paper writing.\nAcknowledgements\nT.H is supported by Gates Cambridge Trust (grant\nOPP1144 from the Bill & Melinda Gates Foun-\ndation). This work was partially performed us-\ning resources provided by the Cambridge Service\nfor Data Driven Discovery (CSD3) operated by\nthe University of Cambridge Research Computing\nService (www.csd3.cam.ac.uk), provided by Dell\nEMC and Intel using Tier-2 funding from the En-\ngineering and Physical Sciences Research Council\n(capital grant EP/T022159/1), and DiRAC funding\nfrom the Science and Technology Facilities Coun-\ncil (www.dirac.ac.uk).\nReferences\nJacob Andreas and Dan Klein. 2016. Reasoning about\npragmatics with neural listeners and speakers. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1173–\n1182, Austin, Texas. Association for Computational\nLinguistics.\nZhuchen Cao, Sven Apel, Adish Singla, and Vera Dem-\nberg. 2025. Pragmatic reasoning improves llm code\ngeneration. Preprint, arXiv:2502.15835.\nBeiduo Chen, Tiancheng Hu, Caiqi Zhang, Robert\nLitschko, Anna Korhonen, and Barbara Plank. 2026.\nDecoupling the effect of chain-of-thought reasoning:\nA human label variation perspective. arXiv preprint\narXiv:2601.03154.\n"}, {"page": 10, "text": "Maximillian Chen, Ruoxi Sun, Sercan Ö Arık, and\nTomas Pfister. 2024. Learning to Clarify: Multi-\nturn Conversations with Action-Based Contrastive\nSelf-Training.\nXiusi Chen, Shanyong Wang, Cheng Qian, Hongru\nWang, Peixuan Han, and Heng Ji. 2025. Decision-\nflow: Advancing large language model as principled\ndecision maker. ArXiv preprint, abs/2505.21397.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann,\nIce Pasupat, Noveen Sachdeva, Inderjit Dhillon, Mar-\ncel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and\n1 others. 2025. Gemini 2.5: Pushing the frontier with\nadvanced reasoning, multimodality, long context, and\nnext generation agentic capabilities. arXiv preprint\narXiv:2507.06261.\nYang Deng, Lizi Liao, Liang Chen, Hongru Wang,\nWenqiang Lei, and Tat-Seng Chua. 2023. Prompt-\ning and evaluating large language models for proac-\ntive dialogues: Clarification, target-guided, and non-\ncollaboration. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages\n10602–10621, Singapore. Association for Compu-\ntational Linguistics.\nYijiang River Dong, Tiancheng Hu, and Nigel Collier.\n2024. Can llm be a personalized judge?\nArXiv\npreprint, abs/2406.11657.\nYijiang River Dong, Tiancheng Hu, Yinhong Liu, Ah-\nmet Üstün, and Nigel Collier. 2025a. When person-\nalization meets reality: A multi-faceted analysis of\npersonalized preference learning. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2025, pages 16880–16894, Suzhou, China. Associa-\ntion for Computational Linguistics.\nYijiang River Dong, Hongzhou Lin, Mikhail Belkin,\nRamon Huerta, and Ivan Vuli´c. 2025b. Undial: Self-\ndistillation with adjusted logits for robust unlearning\nin large language models. In Proceedings of the 2025\nConference of the Nations of the Americas Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Pa-\npers), pages 8827–8840.\nMichael C Frank and Noah D Goodman. 2012. Predict-\ning pragmatic reasoning in language games. Science,\n336(6084):998–998.\nNoah D Goodman and Michael C Frank. 2016. Prag-\nmatic language interpretation as probabilistic infer-\nence. Trends in cognitive sciences, 20(11):818–829.\nGabriel Grand, Valerio Pepe, Jacob Andreas, and\nJoshua B Tenenbaum. 2025. Shoot first, ask ques-\ntions later? building rational agents that explore and\nact like people. arXiv preprint arXiv:2510.20886.\nRobert XD Hawkins, Andreas Stuhlmuller, Judith De-\ngen, and Noah D Goodman. 2015. Why do you\nask? good questions provoke informative answers.\nIn Proceedings of the Annual Meeting of the Cogni-\ntive Science Society, volume 37.\nRonald A. Howard. 1966. Information value theory.\nIEEE Transactions on Systems Science and Cyber-\nnetics, 2(1):22–26.\nTiancheng Hu, Joachim Baumann, Lorenzo Lupo, Nigel\nCollier, Dirk Hovy, and Paul Röttger. 2025a. Sim-\nbench: Benchmarking the ability of large language\nmodels to simulate human behaviors. arXiv preprint\narXiv:2510.17516.\nTiancheng Hu, Benjamin Minixhofer, and Nigel Collier.\n2025b. Navigating the alignment-calibration trade-\noff: A pareto-superior frontier via model merging.\narXiv preprint arXiv:2510.17426.\nZhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao,\nSee-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei\nKoh, and Bryan Hooi. 2024. Uncertainty of thoughts:\nUncertainty-aware planning enhances information\nseeking in large language models. ArXiv preprint,\nabs/2402.03271.\nZheng Hui, Yijiang River Dong, Sanhanat Sivapirom-\nrat, Ehsan Shareghi, and Nigel Collier. 2025a. Pri-\nvacypad: A reinforcement learning framework for\ndynamic privacy-aware delegation. arXiv preprint\narXiv:2510.16054.\nZheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan,\nand Congrui Huang. 2025b.\nToxicraft: A novel\nframework for synthetic generation of harmful in-\nformation. Preprint, arXiv:2409.14740.\nZheng Hui, Yinheng Li, Dan Zhao, Colby Ban-\nbury, Tianyi Chen, and Kazuhito Koishida. 2025c.\nWinSpot: GUI grounding benchmark with multi-\nmodal large language models. In Proceedings of the\n63rd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n1086–1096, Vienna, Austria. Association for Compu-\ntational Linguistics.\nZheng Hui, Xiaokai Wei, Yexi Jiang, Kevin Gao,\nChen Wang, Frank Ong, Se eun Yoon, Rachit Pa-\nreek, and Michelle Gong. 2025d.\nToward safe\nand human-aligned game conversational recommen-\ndation via multi-agent decomposition.\nPreprint,\narXiv:2504.20094.\nKatarzyna Kobalczyk, Nicolas Astorga, Tennison Liu,\nand Mihaela van der Schaar. 2025. Active task disam-\nbiguation with llms. ArXiv preprint, abs/2502.04485.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2022.\nCLAM: Selective Clarification for Ambiguous Ques-\ntions with Generative Language Models.\nBelinda Z. Li, Alex Tamkin, Noah Goodman, and Jacob\nAndreas. 2023. Eliciting Human Preferences with\nLanguage Models.\nXiaomin Li, Zhou Yu, Ziji Zhang, Yingying Zhuang,\nSwair Shah, Narayanan Sadagopan, and Anurag Beni-\nwal. 2025. Semantic volume: Quantifying and de-\ntecting both external and internal uncertainty in llms.\nArXiv preprint, abs/2502.21239.\n"}, {"page": 11, "text": "Jessy Lin, Daniel Fried, Dan Klein, and Anca Dragan.\n2022. Inferring rewards from language in context.\narXiv preprint arXiv:2204.02515.\nJessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason\nEisner. 2024. Decision-oriented dialogue for human-\nAI collaboration. Transactions of the Association for\nComputational Linguistics, 12:892–911.\nOllie Liu, Deqing Fu, Dani Yogatama, and Willie\nNeiswanger. 2024. Dellma: Decision making un-\nder uncertainty with large language models. ArXiv\npreprint, abs/2402.02392.\nChaitanya Malaviya, Joseph Chee Chang, Dan Roth,\nMohit Iyyer, Mark Yatskar, and Kyle Lo. 2024. Con-\ntextualized Evaluations: Taking the Guesswork Out\nof Language Model Evaluations.\nWill Monroe, Robert X.D. Hawkins, Noah D. Good-\nman, and Christopher Potts. 2017. Colors in context:\nA pragmatic neural model for grounded language\nunderstanding. Transactions of the Association for\nComputational Linguistics, 5:325–338.\nOpenAI. 2025. Introducing gpt-4.1 in the api. Ac-\ncessed: 2025-09-18.\nAndi Peng, Andreea Bobu, Belinda Z. Li, Theodore R.\nSumers,\nIlia\nSucholutsky,\nNishanth\nKumar,\nThomas L. Griffiths, and Julie A. Shah. 2024.\nPreference-conditioned language-guided abstraction.\nIn Proceedings of the 2024 ACM/IEEE International\nConference on Human-Robot Interaction,\nHRI\n2024, Boulder, CO, USA, March 11-15, 2024, pages\n572–581. ACM.\nCheng Qian, Zuxin Liu, Akshara Prabhakar, Jielin\nQiu, Zhiwei Liu, Haolin Chen, Shirley Kokane,\nHeng Ji, Weiran Yao, Shelby Heinecke, and 1 oth-\ners. 2025. Userrl: Training interactive user-centric\nagent via reinforcement learning.\narXiv preprint\narXiv:2509.19736.\nLinlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen,\nand Sjoerd van Steenkiste. 2025. Bayesian teach-\ning enables probabilistic reasoning in large language\nmodels. arXiv preprint arXiv:2503.17523.\nH. Raiffa and R. Schlaifer. 1961. Applied Statistical\nDecision Theory. Studies in managerial economics.\nDivision of Research, Graduate School of Business\nAdministration, Harvard University.\nAllen Z Ren, Anushri Dixit, Alexandra Bodrova,\nSumeet Singh, Stephen Tu, Noah Brown, Peng Xu,\nLeila Takayama, Fei Xia, Jake Varley, and 1 oth-\ners. 2023. Robots that ask for help: Uncertainty\nalignment for large language model planners. arXiv\npreprint arXiv:2307.01928.\nChirag Shah and Ryen W White. 2024. Agents are not\nenough. ArXiv preprint, abs/2412.16241.\nTheodore R Sumers, Robert D Hawkins, Mark K Ho,\nand Thomas L Griffiths. 2021. Extending rational\nmodels of communication from beliefs to actions.\narXiv preprint arXiv:2105.11950.\nWeiwei Sun, Xuhui Zhou, Weihua Du, Xingyao Wang,\nSean Welleck, Graham Neubig, Maarten Sap, and\nYiming Yang. 2025. Training proactive and personal-\nized llm agents. arXiv preprint arXiv:2511.02208.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 5433–5442, Singapore. Association for\nComputational Linguistics.\nJimmy Wang, Thomas Zollo, Richard Zemel, and\nHongseok Namkoong. 2025. Adaptive elicitation\nof latent information using natural language. ArXiv\npreprint, abs/2504.04204.\nYifan Wang and Vera Demberg. 2024. RSA-control:\nA pragmatics-grounded lightweight controllable text\ngeneration framework. In Proceedings of the 2024\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5561–5582, Miami, Florida,\nUSA. Association for Computational Linguistics.\nShirley Wu, Michel Galley, Baolin Peng, Hao Cheng,\nGavin Li, Yao Dou, Weixin Cai, James Zou, Jure\nLeskovec, and Jianfeng Gao. 2025. Collabllm: From\npassive responders to active collaborators. ArXiv\npreprint, abs/2502.00640.\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan\nLi, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun\nCheng, Dongchan Shin, Fangyu Lei, and 1 others.\n2024. Osworld: Benchmarking multimodal agents\nfor open-ended tasks in real computer environments.\nAdvances in Neural Information Processing Systems,\n37:52040–52094.\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022. Webshop: Towards scalable real-\nworld web interaction with grounded language agents.\nIn Advances in Neural Information Processing Sys-\ntems 35: Annual Conference on Neural Information\nProcessing Systems 2022, NeurIPS 2022, New Or-\nleans, LA, USA, November 28 - December 9, 2022.\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik\nNarasimhan. 2024. tau-bench: A benchmark for tool-\nagent-user interaction in real-world domains. arXiv\npreprint arXiv:2406.12045.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\n"}, {"page": 12, "text": "Caiqi Zhang, Ruihan Yang, Zhisong Zhang, Xinting\nHuang, Sen Yang, Dong Yu, and Nigel Collier. 2024a.\nAtomic calibration of llms in long-form generations.\narXiv preprint arXiv:2410.13246.\nCaiqi Zhang, Ruihan Yang, Xiaochen Zhu, Chengzu Li,\nTiancheng Hu, Yijiang River Dong, Deqing Yang,\nand Nigel Collier. 2026.\nConfidence estimation\nfor llms in multi-turn interactions. arXiv preprint\narXiv:2601.02179.\nMichael J. Q. Zhang, W. Bradley Knox, and Eunsol\nChoi. 2024b. Modeling Future Conversation Turns\nto Teach LLMs to Ask Clarifying Questions.\nMichael JQ Zhang and Eunsol Choi. 2023. Clarify when\nnecessary: Resolving ambiguity through interaction\nwith lms. ArXiv preprint, abs/2311.09469.\nXuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng,\nand Tat-Seng Chua. 2024c. Ask-before-plan: Proac-\ntive language agents for real-world planning. ArXiv\npreprint, abs/2406.12639.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue\nOu, Yonatan Bisk, Daniel Fried, and 1 others. 2023.\nWebarena: A realistic web environment for building\nautonomous agents. ArXiv preprint, abs/2307.13854.\nXuhui Zhou and Weiwei Sun. 2025. The quest of User-\neffective AI agents. Blog post. Accessed: 2026-01-\n06.\nA\nCase Study: VoI is Risk-Aware\nFigure 4 provides a compelling qualitative example\nof why the VoI framework is superior to heuristic-\nbased methods like confidence thresholding. The\nexperiment contrasts a low-stakes task (guessing an\nanimal, reward=1) with a high-stakes task (medical\ndiagnosis, reward=10), using an identical commu-\nnication cost (c = 0.05).\nIn the high-stakes medical diagnosis (Fig. 4b),\nthe potential reward for a correct answer is high.\nThe VoI agent correctly calculates that even ques-\ntions with moderate information gain are valuable\nenough to outweigh the communication cost. It,\ntherefore, continues to ask clarifying questions un-\ntil it is highly confident, stopping several rounds\nafter the confidence-thresholding baseline would\nhave stopped, even though significant ambiguity\nremains, leading to an incorrect diagnosis.\nIn the low-stakes animal guessing game\n(Fig. 4a), the maximum potential utility is low.\nHere, the VoI agent correctly assesses that the po-\ntential utility gain from asking many questions is\nnot worth the cumulative communication cost. It,\ntherefore, halts the conversation earlier than the\nconfidence-thresholding method, avoiding unnec-\nessary cognitive load on the user for a low-risk task.\nThe confidence-based agent, blind to the low stakes,\nwould have continued asking questions, needlessly\nimposing cognitive load on the user for a trivial\ntask.\nThis case study reveals that effective commu-\nnication requires balancing two distinct pressures:\nthe drive to reduce uncertainty (an epistemic goal)\nand the need to consider the task’s stakes (a utilitar-\nian goal). Confidence-based methods address only\nthe former. The VoI framework excels because it\nnaturally unifies both: it quantifies the value of\nreducing uncertainty precisely in terms of its ex-\npected impact on the final, stake-weighted utility.\nThis principled balance enables the agent to be ap-\npropriately cautious in high-stakes scenarios and\nefficient in low-stakes ones—a critical capability\nfor building trustworthy and effective human-AI\ncollaborators.\nB\nMain Results in Tables\n"}, {"page": 13, "text": "Figure 4: A side by side comparison for different methods for Mixed 20 Question task. The figure contrasts four\ncontrollers—No-Ask, Fixed-Round, Confidence Thresholding (τ = 0.90), and our VOI policy—on a single Mixed\n20Q instance with communication cost c = 0.05. Task stakes are encoded directly in the terminal utility: a correct\nanimal guess yields reward 1 (low stakes), whereas a correct medical diagnosis yields reward 10 (high stakes). The\nobjective maximizes decision utility minus dialogue cost, U(θ, a) −c(ξ).\nFigure 5: GPT-4: results for different methods and thresholds across three tasks. For Webshop, LLM is normalized\nby 10 and utilities are Util = LLM −#T × {0.01, 0.05}. Mixed 20Q utilities are recomputed per spec. Within\neach method, the best utility is underlined. The global best per task/cost is bold+italic and the second best is bold.\nMethod\nMixed 20Q\nFlight Rec.\nWebshop\nτ\nAcc.\n(Animal)\nAcc.\n(Med)\n#T\n(Animal)\n#T\n(Med)\nUtil.\n(0.01)\nUtil.\n(0.05)\nτ\nReward\n#T\nUtil.\n(0.01)\nUtil.\n(0.05)\nτ\nLLM\n#T\nUtil.\n(0.01)\nUtil.\n(0.05)\nNo Question\n–\n0.01\n0.06\n0.00\n0.00\n0.70\n0.70\n–\n0.17\n0.00\n0.17\n0.17\n–\n0.54\n0.00\n0.54\n0.54\nAdaptive\n–\n0.68\n0.53\n17.80\n6.254\n10.26\n2.89\n–\n0.20\n0.56\n0.20\n0.17\n–\n0.57\n0.89\n0.56\n0.52\nFixed Round\n5\n0.24\n0.51\n5.00\n5.00\n6.95\n4.75\n1.00\n0.22\n1.00\n0.21\n0.17\n1.00\n0.56\n1.00\n0.55\n0.51\n10\n0.60\n0.78\n10.00\n10.00\n12.70\n8.30\n2.00\n0.32\n2.00\n0.30\n0.22\n2.00\n0.57\n2.00\n0.55\n0.47\n15\n0.77\n0.78\n15.00\n10.00\n13.90\n7.50\n3.00\n0.35\n3.00\n0.32\n0.20\n3.00\n0.62\n3.00\n0.59\n0.47\n20\n0.87\n0.78\n20.00\n10.00\n14.40\n6.00\n4.00\n0.36\n4.00\n0.32\n0.16\n4.00\n0.63\n4.00\n0.59\n0.43\nConfidence\n0.50\n0.20\n0.31\n4.01\n2.54\n4.67\n2.97\n0.50\n0.19\n0.71\n0.19\n0.16\n0.50\n0.55\n0.78\n0.54\n0.51\n0.70\n0.45\n0.60\n5.68\n4.56\n9.89\n7.43\n0.70\n0.23\n1.09\n0.22\n0.17\n0.70\n0.60\n1.31\n0.58\n0.53\n0.90\n0.59\n0.65\n8.48\n6.49\n11.49\n7.84\n0.90\n0.24\n2.82\n0.21\n0.10\n0.90\n0.63\n2.95\n0.60\n0.48\nVOI\n0.01\n0.76\n0.78\n11.80\n8.07\n14.14\n9.10\n0.01\n0.36\n1.49\n0.35\n0.28\n0.01\n0.63\n2.95\n0.60\n0.49\n0.05\n0.74\n0.78\n11.46\n7.99\n13.97\n9.07\n0.05\n0.29\n0.82\n0.29\n0.25\n0.05\n0.61\n1.74\n0.59\n0.52\nTable 2: Gemini-2.5-Flash: results for different methods and thresholds across three tasks. Format is the same as\nFigure 5\nMethod\nMixed 20Q\nFlight Rec.\nWebshop\nτ\nAcc.\n(Animal)\nAcc.\n(Med)\n#T\n(Animal)\n#T\n(Med)\nUtil.\n(0.01)\nUtil.\n(0.05)\nτ\nReward\n#T\nUtil.\n(0.01)\nUtil.\n(0.05)\nτ\nLLM\n#T\nUtil.\n(0.01)\nUtil.\n(0.05)\nNo Question\n–\n0.01\n0.06\n0.00\n0.00\n0.70\n0.70\n–\n0.16\n0.00\n0.16\n0.16\n–\n0.50\n0.00\n0.50\n0.50\nAdaptive\n–\n0.28\n0.37\n4.78\n6.36\n5.96\n3.79\n–\n0.22\n0.21\n0.22\n0.21\n–\n0.51\n0.55\n0.51\n0.48\nFixed Round\n5\n0.16\n0.29\n5.00\n5.00\n3.95\n1.75\n1.00\n0.18\n1.00\n0.17\n0.13\n1.00\n0.55\n1.00\n0.54\n0.50\n10\n0.33\n0.30\n10.00\n10.00\n5.20\n0.80\n2.00\n0.18\n2.00\n0.16\n0.08\n2.00\n0.57\n2.00\n0.55\n0.47\n15\n0.40\n0.30\n15.00\n10.00\n5.40\n-1.00\n3.00\n0.19\n3.00\n0.16\n0.04\n3.00\n0.59\n3.00\n0.56\n0.44\n20\n0.39\n0.30\n20.00\n10.00\n4.80\n-3.60\n4.00\n0.21\n4.00\n0.17\n0.01\n4.00\n0.61\n4.00\n0.57\n0.41\nConfidence\n0.50\n0.22\n0.27\n4.87\n5.12\n4.36\n2.21\n0.50\n0.14\n0.09\n0.14\n0.14\n0.50\n0.52\n0.48\n0.52\n0.50\n0.70\n0.16\n0.31\n5.06\n6.08\n4.13\n1.87\n0.70\n0.20\n0.99\n0.19\n0.15\n0.70\n0.54\n0.55\n0.54\n0.51\n0.90\n0.36\n0.30\n11.28\n9.25\n5.38\n0.50\n0.90\n0.25\n1.53\n0.24\n0.17\n0.90\n0.59\n2.73\n0.56\n0.45\nVOI\n0.01\n0.28\n0.55\n8.48\n7.63\n7.38\n3.68\n0.01\n0.30\n1.62\n0.28\n0.22\n0.01\n0.59\n2.15\n0.57\n0.48\n0.05\n0.15\n0.50\n4.20\n6.99\n6.01\n4.05\n0.05\n0.28\n1.07\n0.27\n0.23\n0.05\n0.56\n1.20\n0.55\n0.50\nC\nPrompts\n"}, {"page": 14, "text": "C.1\nMixed 20 Questions\nAnimal — Direct Prompting\nYou are playing 20 Questions as the guesser. Your goal is to figure out what animal I’m thinking of\nby asking questions.\nYou have asked {question_count} questions so far and have {remaining_questions} questions left.\nThe possible animals you’re trying to guess include: {answer_set}\nRules:\n1. Ask only yes/no questions (answerable with “Yes”/“No”).\n2. Ask one question at a time.\n3. Keep asking until you use all 20 questions.\n4. Do not ask the same question twice.\n5. Do not guess a specific animal early (e.g., “Is it a cat?”). Start broad to narrow options.\nFigure 6: Direct Prompting (Animal 20 Question)\nAnimal — Auto Stop\nYou are playing 20 Questions as the guesser. Your goal is to figure out what animal I’m thinking of.\nYou have asked {question_count} questions so far and have {remaining_questions} questions left.\nThe possible animals you’re trying to guess include: {answer_set}\nRules:\n1. Ask only yes/no questions.\n2. Ask one question at a time.\n3. When you’re ready to guess, output: My guess is:\n[animal].\n4. Do not ask the same question twice.\n5. Avoid premature specific guesses; use broader categories to narrow down.\nFigure 7: Auto Stop (Animal 20 Question)\n"}, {"page": 15, "text": "Animal — Confidence Thresholding\nYou are tasked with guessing the animal based on the entire conversation. Provide a final guess, even\nif uncertain. The set of possible animals is: {answer_set}.\nAlso estimate your confidence from 1 to 100 (100 = highest).\nOutput exactly: { \"guess\":\n\"<animal>\", \"confidence\":\n<number 1-100> }\nConfidence Scale (1–100):\n1–20: Very minimal evidence.\n21–40: Weak evidence.\n41–60: Moderate evidence.\n61–80: Strong evidence.\n81–100: Very strong evidence.\nFigure 8: Confidence Thresholding (Animal 20 Question)\nAnimal — VOI: Question Generation\nI’m playing a game of 20 Questions to identify an animal. Based on previous questions and answers:\n{previous_qa}\nGenerate 5 yes/no questions that help identify the animal.\nFormat as a numbered list.\nFigure 9: VOI: Question Generation (Animal 20 Question)\nAnimal — VOI: Batch Answer Simulation\nYou are playing a game of Twenty Questions. You will receive a yes/no question and a list of animals.\nAnswer for each animal with: Yes (clearly correct), No (clearly incorrect), or Maybe (uncertain/am-\nbiguous/varies by species or not confident). Prefer “Maybe” over guessing.\nFormat exactly as: Animal Name:\nYes/No/Maybe\nQuestion: {question}\nAnimals: {candidate_list}\nFigure 10: VOI: Batch Answer Simulation (Animal 20 Question)\nMedical — Direct Prompting\nYou are the doctor and must diagnose the patient using only yes/no questions.\nYou have asked {question_count} questions so far and have {remaining_questions} left.\nPossible diagnoses: {answer_set}\nYou may ask up to 20 yes/no questions to understand the condition. At the end, output your diagnosis.\nFigure 11: Direct Prompting (Medical Diagnosis)\n"}, {"page": 16, "text": "Medical — Auto Stop\nYou are the doctor and may ask up to 20 yes/no questions to diagnose the patient.\nYou have asked {question_count} questions so far and have {remaining_questions} left.\nPossible diagnoses: {answer_set}\nYou can ask up to 10 yes/no questions. Stop when you have enough information.\nFormat your guess as: My guess is:\n[diagnosis].\nFigure 12: Auto Stop (Medical Diagnosis)\nMedical — Confidence Thresholding\nDiagnose the patient based on the entire conversation. Provide a final diagnosis, even if uncertain.\nSet of diseases: {answer_set}.\nAlso estimate your confidence (1–100).\nOutput\nexactly:\n{ \"guess\":\n\"<diagnosis>\", \"confidence\":\n<number\n1-100> }\nConfidence Scale (1–100):\n1–20: Extremely uncertain.\n21–40: Weak evidence.\n41–60: Moderate evidence.\n61–80: Strong evidence.\n81–100: Very strong evidence.\nFigure 13: Confidence Thresholding (Medical Diagnosis)\nMedical — VOI: Question Generation\nI’m a doctor trying to diagnose a patient’s condition through a series of questions. Based on\nsymptoms and previous answers:\n{previous_qa}\nGenerate 5 yes/no questions that most effectively narrow the possible conditions (roughly halving\nthe set each time).\nFocus on distinguishing symptoms, risk factors, or medical history.\nFormat as a numbered list.\nFigure 14: VOI: Question Generation (Medical Diagnosis)\nMedical — VOI: Batch Answer Simulation\nI’m a medical diagnostician. Below is a yes/no question and a list of medical conditions.\nQuestion: “{question}”\nFor each condition, answer with just “Yes” or “No”, based on typical presentation.\nReply exactly as: Condition:\nAnswer\nConditions: {candidate_list}\nFigure 15: VOI: Batch Answer Simulation (Medical Diagnosis)\n"}, {"page": 17, "text": "C.2\nFlight Recommendation\nDirect Prompting and Confidence Thresholding\nOpening\nUser: Help me pick flights. My preferences are fixed; infer them and choose. Use your best\njudgement; don’t ask for more info.\n— SUPPORT HISTORY —\nUser: Which flight is best?\nFlight 1: {option 1}\nFlight 2: {option 2}\nFlight 3: {option 3}\nUser: I prefer flight {1/2/3}\nNEW Round (no answer shown)\nUser: Which flight is best?\nFlight 1: {option 1}\nFlight 2: {option 2}\nFlight 3: {option 3}\nRequired Output\nModel: The best option is Flight\nFigure 16: The prompt used for Direct Prompting and Confidence Thresholding. Logit is extracted as measure of\nconfidence.\nVOI — Prior over Feature States\nYou are calibrating a probabilistic user model.\nFeature: {feature}\nHistory (support + any clarifying Q&A):\n{history_ctx}\nBased only on this history, estimate P(state) for the feature.\nReturn STRICT JSON with keys exactly {{states}} that sum to 1. Example: {\"lower\":\n0.33, \"higher\":\n0.33, \"none\":\n0.34}\nJSON:\nFigure 17: Prior Estimation for VOI (Airline Preference Matching)\n"}, {"page": 18, "text": "VOI — Posterior with Options\nYou are calibrating a probabilistic user model.\nFeature: {feature}\nHistory (support + any clarifying Q&A):\n{history_ctx}\nCurrent options:\nA) {option A}\nB) {option B}\nC) {option C}\nEstimate the posterior distribution over the user’s {feature} state given full history.\nReturn STRICT JSON with keys exactly {{states}} that sum to 1.\nJSON:\nFigure 18: Posterior Estimation with Options (Airline Preference Matching)\nVOI — Candidate Preference Questions\nYou are an AI assistant helping a user choose between flight options A, B, and C. You’ve analyzed\nthe support examples but still have some uncertainty.\n{support_history}\n{qa_context}\nGenerate one multiple-choice question about a single aspect of the user’s preference that will help\ndecide among the options below.\nA) {option A}\nB) {option B}\nC) {option C}\nQuestion:\nFigure 19: VOI Candidate Questions (Airline Preference Matching)\n"}]}