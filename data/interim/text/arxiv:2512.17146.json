{"doc_id": "arxiv:2512.17146", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.17146.pdf", "meta": {"doc_id": "arxiv:2512.17146", "source": "arxiv", "arxiv_id": "2512.17146", "title": "Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors", "authors": ["Huixin Zhan"], "published": "2025-12-19T00:51:11Z", "updated": "2025-12-19T00:51:11Z", "summary": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.17146v1", "url_pdf": "https://arxiv.org/pdf/2512.17146.pdf", "meta_path": "data/raw/arxiv/meta/2512.17146.json", "sha256": "38458e9a990a634ea451db19ab3cfda7aaf4f31836437810eb2e8eeb658548c2", "status": "ok", "fetched_at": "2026-02-18T02:24:04.785633+00:00"}, "pages": [{"page": 1, "text": "Biosecurity-Aware AI: Agentic Risk Auditing of Soft\nPrompt Attacks on ESM-Based Variant Predictors\nHuixin Zhan‚àó\nDepartment of Computer Science and Engineering\nNew Mexico Institute of Mining and Technology\nSocorro, NM 87801\nhuixin.zhan@nmt.edu\nAbstract\nGenomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),\nhave demonstrated remarkable success in variant effect prediction. However, their\nsecurity and robustness under adversarial manipulation remain largely unexplored.\nTo address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE),\nan agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE\nfunctions through an interpretable and automated risk auditing loop. It injects\nsoft prompt perturbations, monitors model behavior across training checkpoints,\ncomputes risk metrics such as AUROC and AUPR, and generates structured reports\nwith large language model-based narrative explanations. This agentic process\nenables continuous evaluation of embedding-space robustness without modifying\nthe underlying model. Using SAGE, we find that even state-of-the-art GFMs\nlike ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable\nperformance degradation. These findings reveal critical and previously hidden vul-\nnerabilities in genomic foundation models, showing the importance of agentic risk\nauditing in securing biomedical applications such as clinical variant interpretation.\n1\nIntroduction\nGenomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have revolu-\ntionized variant effect prediction (VEP) by enabling large-scale, zero-shot generalization across\ndiverse genomic tasks. These models leverage protein and DNA sequences to predict the functional\nconsequence of genetic variation, offering substantial utility in clinical genomics, including disease\ndiagnostics and therapeutic target discovery. For instance, AlphaMissense [3] integrates evolution-\nary conservation and structural modeling for pathogenicity classification, while ESM1b has been\napplied to genome-wide prediction of disease variant effects in a zero-shot setting, without requiring\nfine-tuning on labeled clinical data [1].\nDespite this progress, current GFMs are generally optimized for predictive accuracy and scalability,\nwith limited attention to robustness, safety, or interpretability. As GFMs move closer to clinical\napplications, particularly in decision-making contexts such as rare disease diagnosis, there is a\ngrowing need to ensure these models remain trustworthy under distributional shifts, malicious inputs,\nor representation-space perturbations. While previous work in genomics has focused on protecting\ndata privacy [2, 7], comparatively little attention has been paid to auditing the model‚Äôs own failure\nmodes.\n‚àóCorresponding author.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards\nfor Generative AI.\narXiv:2512.17146v1  [cs.CR]  19 Dec 2025\n"}, {"page": 2, "text": "Figure 1: Soft prompt perturbation and agentic risk auditing with Secure Agentic Genomic Evaluator\n(SAGE). (a) The SAGE audits the model‚Äôs behavior in response to such perturbations. This agentic\nevaluation framework enables interpretable, automated analysis of robustness and misalignment in\ngenomic foundation models without interfering with their internal optimization dynamics. (b) A\nschematic of soft prompt-based adversarial perturbation in genomic foundation models.\nIn this work, we introduce the Secure Agentic Genomic Evaluator (SAGE), a novel agent for\nadversarial robustness auditing in genomic foundation models. Rather than directly modifying model\nweights or engaging in reinforcement-style intervention, SAGE operates in a monitor-and-report\nloop: it injects soft prompt perturbations into GFM inputs, monitors prediction responses across\ncheckpoints, computes risk metrics such as AUROC and AUPR, and generates narrative reports\nusing large language models (LLMs). This agentic framework enables scalable, interpretable, and\nreproducible evaluation of model security under adversarial settings, without requiring access to the\nmodel internals or ground-truth labels, as illustrated in Figure 1 (a).\nTo probe GFM robustness, we implement a targeted soft prompt attack that operates purely in the\nmodel‚Äôs embedding space. This attack prepends a trainable embedding sequence to wild-type and\nmutant protein sequences, selectively manipulating the pseudo-log-likelihood ratio (PLLR) for benign\nvariants to mimic pathogenic predictions. The variant effect prediction model follows a Siamese\nNeural Network (SNN) architecture, which processes the wild-type and mutant sequences in paral-\nlel using shared weights to compute comparative PLLR scores. Unlike token-level perturbations,\nthis latent-space attack preserves biological input integrity while degrading model decision bound-\naries [17], as illustrated in Figure 1 (b). Our experiments reveal that this targeted soft prompt attack\ndegrades model performance consistently across both cardiomyopathy (CM) and arrhythmia (ARM)\ndatasets‚Äîeven in large-scale models like ESM1b and ESM1v.\nIn summary, we make the following contributions:\n‚Ä¢ We introduce SAGE, a modular agentic framework for adversarial auditing of genomic foun-\ndation models via soft prompt-based input manipulation and LLM-driven interpretability.\n‚Ä¢ We demonstrate that GFMs are vulnerable to latent-space adversarial attacks, particularly\nin the form of targeted soft prompt optimization that induces confidence shifts in benign\nvariant classification.\n‚Ä¢ We benchmark the robustness of four GFM backbones (ESM2-150M, ESM2-650M, ESM1b,\nESM1v) under attack, demonstrating model-dependent variability in adversarial resilience.\n‚Ä¢ We provide a case study showing how SAGE generates interpretable multi-step audit reports,\nsupporting biosecurity research and safe deployment of genomic AI in clinical settings.\n2\nMethods\nGFMs, including protein language models such as ESM-1b, are typically pretrained using the\nMasked Language Modeling (MLM) objective. In this setup, specific amino acid residues in protein\nsequences are randomly masked, and the model is trained to predict the identity of these masked\nresidues based on surrounding context. For each masked position i, the model produces a vector of\nraw scores (referred to as MLM logits) corresponding to each possible amino acid substitution. When\npassed through a softmax activation, these logits yield a probability distribution over the amino acid\nvocabulary.\n2\n"}, {"page": 3, "text": "Pseudo-Log-Likelihood Ratio (PLLR)\nTo fine-tune GFMs for variant effect prediction, we adopt\nan SNN architecture composed of two identical, weight-sharing branches. Each branch processes\neither a wild-type sequence sWT or its corresponding mutant smut, producing token-level MLM\nlogits. These logits are aggregated into a pseudo-log-likelihood (PLL), defined for a sequence\ns = (s1, . . . , sL) as:\nPLL(s) =\nL\nX\ni=1\nlog P(xi = si | s),\n(1)\nwhere P(xi = si | s) is the model-assigned probability of observing amino acid si at position i given\nthe full sequence s. Since wild-type sequences are generally more compatible with pretrained models,\nthey tend to yield higher PLL values. We then define the PLLR between the wild-type and mutant\nsequences as:\nŒª =\n\f\fPLL(sWT) ‚àíPLL(smut)\n\f\f .\n(2)\nThis absolute difference captures the extent to which a mutation perturbs the model‚Äôs probabilistic\nunderstanding of the sequence.\nClassification Objective\nTo classify genetic variants as pathogenic or benign, we apply a binary\ncross-entropy (BCE) loss to the calibrated PLLR values. Since the sigmoid function œÉ(Œª) maps\nŒª ‚àà[0, ‚àû) to [0.5, 1), we rescale it to the full [0, 1] interval using:\nÀÜœÉ(Œª) = 2 ¬∑ œÉ(Œª) ‚àí1.\n(3)\nThis calibrated probability is then used in the BCE loss:\nLBCE = y ¬∑ log(ÀÜœÉ(Œª)) + (1 ‚àíy) ¬∑ log(1 ‚àíÀÜœÉ(Œª)),\n(4)\nwhere y ‚àà{0, 1} denotes the ground-truth pathogenicity label. The objective encourages larger\nPLLR values when a mutation is pathogenic (i.e., when it strongly disrupts the model‚Äôs expectations),\nand smaller values when the mutation is benign.\n2.1\nAttack Models\nTo evaluate the adversarial robustness of GFMs, we implement a targeted soft prompt attack that\noperates in the embedding space of the model. A trainable embedding sequence (i.e., soft prompt) is\nprepended to both the wild-type and mutant sequences prior to inference. Unlike standard prompt\ntuning where the model parameters remain fixed, we allow the entire protein language model\nto be fine-tuned jointly with the soft prompt. This end-to-end optimization setup enables more\naggressive perturbation of internal representations, amplifying potential vulnerabilities in model\ndecision boundaries.\nTargeted Soft Prompt Attack (Benign ‚ÜíPathogenic).\nIn the one-class targeted attack setting,\nthe soft prompt is trained specifically to misclassify benign variants as pathogenic. Let y = 0 denote\nbenign examples; we optimize the following attack loss:\nLbenign = ‚àílog(ÀÜœÉ(Œª)),\nfor y = 0.\n(5)\nThis objective encourages the model to produce high PLLR values for benign inputs, thereby forcing\nthe classifier to assign them high pathogenicity scores. During training, only benign examples receive\ngradient updates, while pathogenic examples are held fixed. This asymmetric optimization increases\nthe false positive rate without disturbing the model‚Äôs performance on known pathogenic variants.\nTo evaluate the model‚Äôs behavior under adversarial perturbation, we develop the SAGE framework.\nSAGE monitors the model‚Äôs output across multiple checkpoints, computes robustness metrics such\n3\n"}, {"page": 4, "text": "(a)\n(b)\n(c)\n(d)\nFigure 2: Targeted soft prompt attack results. (a‚Äìb) CM dataset; (c‚Äìd) ARM dataset. (a, c) PLLR\nbefore vs. after. (b, d) ‚àÜPLLR by label.\nas AUROC and AUPR, and generates interpretability-enhanced narrative reports using large lan-\nguage models. This agentic auditing framework provides a systematic and reproducible method for\nidentifying failure modes in genomic foundation models subjected to adversarial soft prompt attacks.\nOptimization and Evaluation Protocol\nDuring the adversarial training phase, only the soft prompt\nparameters are updated via gradient descent, while the input sequences and model weights remain\nfixed. The optimization objective is defined with respect to the original ground-truth labels, enabling\na controlled attack scenario. After training, we evaluate the model‚Äôs performance on a held-out test\nset, using metrics such as AUROC and AUPR to quantify degradation in classification accuracy.\nThis protocol isolates the impact of embedding-space perturbations introduced by the soft prompt,\nallowing us to assess adversarial susceptibility without altering the biological input or retraining the\nmodel.\n3\nExperimental Results\n3.1\nSettings\nTo evaluate the robustness of our variant effect prediction framework under targeted adversarial\nconditions, we implement a soft prompt attack focused specifically on benign variants. In this setup,\nn = 10 learnable soft prompt tokens are prepended to both the wild-type and mutant sequences.\nThese prompt embeddings are initialized using the Xavier uniform distribution [4] and optimized\nusing a targeted objective that increases the model‚Äôs pathogenicity score for benign variants. During\ntraining, the soft prompts are updated via gradient descent, while the backbone GFM and input\nsequences remain fixed. We use the Adam optimizer [6] with a learning rate of 1 √ó 10‚àí4 and a\nbatch size of 4 over 10 epochs. The binary cross-entropy loss is used to drive the targeted attack on\nbenign examples. All experiments are conducted on a single A100 GPU. This targeted optimization\nsetup enables us to isolate the impact of soft input perturbations on the model‚Äôs decision boundary\nwhile preserving biological sequence content. The code is open source at https://github.com/\nhuixin-zhan-ai/SAGE.\n4\n"}, {"page": 5, "text": "3.2\nTargeted Soft Prompt Attack Across CM and ARM Datasets\nWe evaluate the effectiveness and generalizability of a targeted soft prompt attack that selectively\nincreases the PLLR of benign variants, thereby inducing misclassification as pathogenic. This attack\noperates by prepending a learnable prompt to both wild-type and mutant sequences, optimized to\nelevate PLLR values for benign inputs while preserving predictions for pathogenic variants.\nFigure 2(a‚Äìb) summarizes the attack‚Äôs impact on the CM dataset for ESM1b. After training, benign\nvariants exhibit a substantial rightward shift in PLLR distribution (Figure 2(a)), confirmed by a\nsignificant paired t-test on benign samples (p = 9.23√ó10‚àí4). In contrast, the pathogenic distribution\nremains stable. The corresponding ‚àÜPLLR analysis (Figure 2(b)) reveals that benign variants\nexperience positive shifts, while pathogenic examples remain unaffected.\nTo assess generalizability, we apply the same attack to the ARM dataset using an identical setup.\nAs shown in Figure 2(c‚Äìd), the attack again induces a rightward PLLR shift for benign variants\n(Figure 2(c)), with little impact on the pathogenic class. The ‚àÜPLLR violin plot (Figure 2(d))\nconfirms this one-sided effect, consistent with the CM results.\nTogether, these results demonstrate that the targeted soft prompt attack not only succeeds in manipu-\nlating benign predictions on CM but also generalizes effectively to ARM. The consistent asymmetric\nimpact across datasets highlights a broader vulnerability in the representation space of protein lan-\nguage models, emphasizing the need for robustness-aware evaluation protocols in clinical genomics.\nModel\nESM2-650M [8]\nESM2-150M [8]\nESM1b-650M [12]\nESM1v-[1‚Äì5] [10]\nAUC (CM)\nBase\n0.74\n0.63\n0.81\n0.76\nTargeted SPA\n0.70\n0.56\n0.74\n0.71\n‚àÜ\n-0.04\n-0.07\n-0.07\n-0.05\nAUPR (CM)\nBase\n0.76\n0.69\n0.83\n0.80\nTargeted SPA\n0.69\n0.64\n0.78\n0.72\n‚àÜ\n-0.07\n-0.05\n-0.05\n-0.08\nAUC (ARM)\nBase\n0.85\n0.78\n0.89\n0.92\nTargeted SPA\n0.80\n0.68\n0.84\n0.84\n‚àÜ\n-0.05\n-0.10\n-0.05\n-0.08\nAUPR (ARM)\nBase\n0.89\n0.85\n0.91\n0.94\nTargeted SPA\n0.80\n0.79\n0.82\n0.81\n‚àÜ\n-0.09\n-0.06\n-0.09\n-0.13\nTable 1: Performance of different GFM backbones under targeted soft prompt attack. All models\nexperience degradation in both AUC and AUPR across CM and ARM datasets, with larger models\n(e.g., ESM1b, ESM1v) showing greater resilience than smaller counterparts (e.g., ESM2-150M).\n3.3\nComparative Analysis of GFM Robustness Under Targeted Attack\nTo evaluate how different GFMs respond to adversarial manipulation, we assess the impact of targeted\nsoft prompt attacks on four commonly used model architectures: ESM2-650M, ESM2-150M, ESM1b-\n650M, and ESM1v-[1‚Äì5]. Table 1 summarizes the AUC and AUPR performance for both CM and\nARM datasets before and after the attack.\nAcross all models and datasets, performance degradation is evident. Importantly, smaller models\nlike ESM2-150M suffer the most severe drop in both AUC (CM: -0.07, ARM: -0.10) and AUPR\n(CM: -0.05, ARM: -0.06), suggesting that their internal representations are more easily disrupted by\nsoft prompt perturbations. In contrast, larger pretrained models such as ESM1b-650M and ESM1v-\n5\n"}, {"page": 6, "text": "[1‚Äì5] demonstrate greater robustness, maintaining relatively higher accuracy and precision-recall\nperformance even under adversarial stress.\nAmong the more resilient models, ESM1b shows a consistent yet moderate decline (e.g., CM AUC\ndrop of 0.07), whereas ESM1v exhibits the largest AUPR drop on ARM (-0.13), possibly reflecting\nits broader output diversity across variants. These differences highlight that model size alone does\nnot fully determine adversarial resilience, i.e., architecture depth, pretraining corpus, and fine-tuning\ndynamics may also play critical roles.\nTogether, these results reveal that while targeted soft prompt attacks universally degrade model\ntrustworthiness, the magnitude of vulnerability varies across architectures. This underscores the\nimportance of model-aware adversarial testing when deploying GFMs in sensitive biomedical appli-\ncations.\n3.4\nCase Study: Layered Agentic Risk Auditing with SAGE\nWe illustrate the practical use of SAGE on a representative case study involving CM variant effect\nprediction using the ESM2-650M protein language model fine-tuned via the DYNA framework [17].\nIn this setting, a targeted soft prompt attack is applied to selectively elevate the PLLR scores of\nbenign variants, mimicking confident misclassifications as pathogenic. To assess model robustness\nunder this attack, we deploy SAGE, our modular, agentic risk auditing system, which monitors model\nbehavior across checkpoints and provides interpretable, reproducible reports. SAGE operates through\nfive sequential layers‚ÄîOBSERVE, INTERVENE, EVALUATE, REASON, and REPORT‚Äîeach\nhandling a distinct phase in the agentic loop. Table 2 summarizes each layer‚Äôs role and provides\nsample outputs from this case.\nLayer\nFunction\nExample Output\nOBSERVE\nLoad sequences, embed models, define\nprompt probes\nInput: wildtype + mutant protein pairs; load\nESM2 checkpoint; define random soft prompts\nINTERVENE\nInject soft prompts, schedule perturba-\ntion rounds\nPrompt injected: ‚Äúbioengineered strain‚Äù at step\n750; evaluated at 50-step intervals from step\n50‚Äì2000\nEVALUATE\nCompute AUROC, AUPR, PLLR\nStep 750 ‚ÜíAUROC = 0.588, AUPR = 0.663;\nStep 1500 ‚ÜíAUROC = 0.561, AUPR = 0.647\nREASON\nClassify risk, generate explanation\nThreshold-based logic:\nAUROC < 0.6 ‚Üí\n‚Äú ! HIGH‚Äù; LLM explanation: ‚Äúmodel shows\npartial sensitivity to prompt injection‚Äù\nREPORT\nCompile\nresults,\ngenerate\nmark-\ndown/HTML report\nGenerates multi-step risk report; Includes LLM\nexplanations per checkpoint\nTable 2: SAGE: Layered Functional Breakdown with Example Outputs. Each layer handles one\nphase in the agentic loop, from data intake to interpretability-enhanced reporting.\nThe OBSERVE layer initiates the pipeline by loading wild-type and mutant sequence pairs, embed-\nding them with a selected GFM, and defining the adversarial probe space through soft prompts. In\nthis case, we used randomly initialized prompts and a fine-tuned ESM2 checkpoint.\nIn the INTERVENE layer, the agent schedules and injects perturbations across training checkpoints.\nFor example, prompts such as ‚Äúbioengineered strain‚Äù were inserted at step 750, and evaluation was\nperformed at regular intervals (e.g., every 50 steps) from step 50 to 2000.\nThe EVALUATE layer computes quantitative robustness metrics such as AUROC, AUPR, and PLLR.\nFor instance, AUROC dropped from 0.588 at step 750 to 0.561 at step 1500, indicating a growing\nadversarial impact as training progresses.\nIn the REASON layer, these metrics are interpreted to classify the level of risk (e.g., AUROC below\n0.6 triggers a ‚Äú ! HIGH‚Äù risk label), and natural language explanations are generated using a large\nlanguage model (LLM). This enables human-interpretable insights into model vulnerabilities.\nFinally, the REPORT layer compiles all findings into structured markdown or HTML reports,\nincluding time-stamped results, metric trends, and explanatory narratives per checkpoint. This\n6\n"}, {"page": 7, "text": "automated reporting loop provides a reproducible, interpretable framework for auditing model\nbehavior under adversarial conditions.\nThis case study illustrates how SAGE integrates perturbation, observation, and reasoning into a\nunified agentic architecture, facilitating robust and interpretable evaluation of genomic foundation\nmodels in high-stakes biomedical contexts.\n4\nRelated Works\nGenomic Foundation Models and Variant Effect Prediction\nRecent years have seen the emer-\ngence of GFMs that leverage large-scale protein and DNA sequence data to predict variant effects in a\nzero- or few-shot setting. Models like ESM1b [12] and AlphaMissense [3] have demonstrated strong\ngeneralization capabilities across genomic tasks, including pathogenicity prediction and isoform-\naware annotation. However, these models are typically trained in a task-agnostic fashion using MLM,\nlimiting their direct clinical utility.\nTo improve disease-specific performance, methods such as DYNA [17] introduce modular fine-tuning\npipelines with siamese architectures and PLLR scoring, enabling adaptation of GFMs to rare variant\ndatasets for cardiomyopathy and regulatory genomics. Nevertheless, while these techniques improve\naccuracy, they do not address model robustness or security under adversarial settings.\nAdversarial Attacks in Genomic Machine Learning\nThe exploration of adversarial vulnerabilities\nin genomic models has gained traction, particularly in the context of data privacy and white-box\nperturbations. Early work emphasized data anonymity [7] and protection mechanisms such as\ndifferential privacy, which have since been shown susceptible to re-identification [2]. More recent\nstudies have shifted toward model-level attacks. Montserrat et al. [11] proposed gradient-based\nadversarial attacks targeting gene expression classifiers, highlighting risks in genomic prediction\npipelines.\nA notable advancement is FIMBA [14], which introduces a model-agnostic black-box attack that\nleverages SHAP-based feature importance [15] to perturb high-importance inputs. While effective,\nthese methods operate on shallow architectures (e.g., MLPs, CNNs) and primarily on tabular gene\nexpression data. In contrast, our work targets the latent representation space of deep pre-trained\nGFMs using embedding-space perturbations. These perturbations expose vulnerabilities invisible to\ntraditional input-space attacks.\nPrompt-Based Vulnerabilities and Alignment Failures\nPrompt engineering and tuning have\nbecome powerful tools for adapting pre-trained models to downstream tasks. However, they also\nexpose new failure modes. Soft prompt attacks and backdoor triggers can steer model predictions\nwithout altering the underlying input [9], posing risks in safety-critical domains. Such misalignments\nbetween training objectives and decision-making behavior have been observed in both NLP and\nmultimodal settings [18, 5].\nOur work extends this concern to the biological domain by demonstrating how soft prompt injections\ncan systematically manipulate pathogenicity predictions in GFMs. By operating in the model‚Äôs\nembedding space, we uncover semantic misalignment that standard accuracy metrics may not reveal.\nThis observation raises questions about model calibration, interpretability, and downstream reliability.\nAgentic AI for Robustness and Safety Auditing\nAgentic frameworks have gained attention in\nAI safety for their ability to perform structured evaluations of model behavior. Examples include\nautonomous tool-use agents [16], multi-agent collaboration systems [13], and benchmark-driven\nauditors such as OpenAI‚Äôs Evals and Risk-Sweeps. These systems often operate in active learning or\nreinforcement learning paradigms to probe model capabilities.\nIn contrast, our SAGE introduces an agentic loop tailored for genomic AI: it perturbs inputs via soft\nprompts, monitors responses across training checkpoints, and outputs structured, interpretable audit\nreports. By integrating metric-based risk scoring with LLM-based explanation, SAGE provides a\nreproducible and interpretable mechanism to assess latent vulnerabilities in clinical-grade genomic\nmodels. Moreover, it bridges the gap between large-scale model auditing and biomedical application\ndomains.\n7\n"}, {"page": 8, "text": "5\nConclusion\nGenomic Foundation Models (GFMs) such as ESM1b and ESM2 have revolutionized variant effect\nprediction through large-scale pretraining and zero-shot generalizability. However, their security under\nadversarial conditions remains an open question with direct implications for high-stakes biomedical\napplications. In this work, we introduce the Secure Agentic Genomic Evaluator (SAGE)‚Äîan\ninterpretable, agentic auditing framework that evaluates GFM robustness through targeted soft\nprompt perturbations. Our experiments demonstrate that soft prompt attacks systematically degrade\nmodel performance by selectively manipulating benign variant predictions while leaving pathogenic\npredictions largely unchanged. This asymmetric vulnerability manifests across multiple model\nbackbones and disease datasets, with smaller models (e.g., ESM2-150M) showing larger AUC\nand AUPR degradation than their larger counterparts (e.g., ESM1b, ESM1v). These differences\nsuggest that adversarial susceptibility is not solely dictated by model size but is also shaped by\npretraining dynamics and architectural design. The layered SAGE pipeline enables structured and\nautomated robustness auditing: from embedding-level intervention and checkpoint-wise evaluation\nto large language model (LLM)-based interpretability. Through this agentic framework, we expose\ncritical blind spots in foundation model trustworthiness and provide a reproducible methodology\nfor assessing real-world failure modes. Taken together, our results call for integrating adversarial\nrisk auditing into the development lifecycle of genomic AI systems. As GFMs continue to influence\nclinical genomics, agentic evaluators like SAGE will be essential for ensuring robust, secure, and\ninterpretable deployment of these models in practice.\nReferences\n[1] Nadav Brandes, Grant Goldman, Charlotte H Wang, Chun Jimmie Ye, and Vasilis Ntranos. Genome-wide\nprediction of disease variant effects with a deep protein language model. Nature Genetics, 55(9):1512‚Äì1522,\n2023.\n[2] Junjie Chen, Wendy Hui Wang, and Xinghua Shi. Differential privacy protection against membership\ninference attack on machine learning for genomic data. In BIOCOMPUTING 2021: Proceedings of the\nPacific Symposium, pages 26‚Äì37. World Scientific, 2020.\n[3] Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, AkvilÀôe ≈ΩemgulytÀôe, Taylor Applebaum, Alexander\nPritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, et al. Accurate proteome-wide missense variant\neffect prediction with alphamissense. Science, 381(6664):eadg7492, 2023.\n[4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics,\npages 249‚Äì256. JMLR Workshop and Conference Proceedings, 2010.\n[5] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples\nin neural networks. In International Conference on Learning Representations, 2017.\n[6] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[7] Tsung-Ting Kuo, Xiaoqian Jiang, Haixu Tang, XiaoFeng Wang, Arif Harmanci, Miran Kim, Kai Post,\nDiyue Bu, Tyler Bath, Jihoon Kim, et al. The evolving privacy and security concerns for genomic data\nanalysis and sharing as observed from the idash competition. Journal of the American Medical Informatics\nAssociation, 29(12):2182‚Äì2190, 2022.\n[8] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert\nVerkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure\nwith a language model. Science, 379(6637):1123‚Äì1130, 2023.\n[9] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing. ACM\ncomputing surveys, 55(9):1‚Äì35, 2023.\n[10] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models\nenable zero-shot prediction of the effects of mutations on protein function. Advances in neural information\nprocessing systems, 34:29287‚Äì29303, 2021.\n[11] Daniel Mas Montserrat and Alexander G Ioannidis. Adversarial attacks on genotype sequences. In ICASSP\n2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages\n1‚Äì5. IEEE, 2023.\n8\n"}, {"page": 9, "text": "[12] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle\nOtt, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsu-\npervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences,\n118(15):e2016239118, 2021.\n[13] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing Systems,\n36:8634‚Äì8652, 2023.\n[14] Heorhii Skovorodnikov and Hoda Alkhzaimi. Fimba: Evaluating the robustness of ai in genomics via\nfeature importance adversarial attacks. arXiv preprint arXiv:2401.10657, 2024.\n[15] Huanjing Wang, Qianxin Liang, John T Hancock, and Taghi M Khoshgoftaar. Feature selection strategies:\na comparative analysis of shap-value and importance-based methods. Journal of Big Data, 11(1):44, 2024.\n[16] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. Re-\nact: Synergizing reasoning and acting in language models. In International Conference on Learning\nRepresentations (ICLR), 2023.\n[17] Huixin Zhan, Jason H Moore, and Zijun Zhang. A disease-specific language model for variant pathogenicity\nin cardiac and regulatory genomics. Nature Machine Intelligence, pages 1‚Äì11, 2025.\n[18] Jiaming Zhang, Jitao Sang, Qi Yi, and Changsheng Xu. Introducing foundation models as surrogate\nmodels: Advancing towards more practical adversarial attacks. arXiv preprint arXiv:2307.06608, 2023.\nA\nTechnical Appendices and Supplementary Material\nAgentic Risk Report for Safe-Gene Evaluation.\nThe following appendix contains the full agent-\ngenerated markdown-to-PDF report, which summarizes model susceptibility under soft prompt\ninjection from training step 50 to 2000. It includes AUROC/AUPR metrics, risk classification, and\nLLM-generated explanations per checkpoint.\n9\n"}, {"page": 10, "text": "üß¨ Safe-Gene Agentic Risk Evaluation\nReport (Steps 50‚Äì2000)\nProject: Variant Effect Prediction under Soft Prompt Injection\nAgent Function: Monitor model susceptibility to soft prompt-based adversarial attacks across\ntraining epochs\nModel: ESM + PLLR under soft prompt injection\nScope: Steps 50 to 2000 at 50-step intervals\nüìä Summary Table\nStep\nAUROC\nAUPR\nRisk\n50\n0.617\n0.685‚ö†Ô∏è\n‚ö† LOW\n100\n0.604\n0.669‚ö†Ô∏è\n‚ö† LOW\n150\n0.600\n0.674‚ö†Ô∏è\n‚ö† LOW\n200\n0.605\n0.682‚ö†Ô∏è\n‚ö† LOW\n250\n0.600\n0.680‚ö†Ô∏è\n‚ö† LOW\n300\n0.597\n0.677\n‚ö†Ô∏è HIGH\n350\n0.600\n0.681‚ö†Ô∏è\n‚ö† LOW\n400\n0.599\n0.677\n‚ö†Ô∏è HIGH\n450\n0.595\n0.672\n‚ö†Ô∏è HIGH\n500\n0.595\n0.674\n‚ö†Ô∏è HIGH\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n1/8\n"}, {"page": 11, "text": "Step\nAUROC\nAUPR\nRisk\n550\n0.590\n0.668\n‚ö†Ô∏è HIGH\n600\n0.593\n0.670\n‚ö†Ô∏è HIGH\n650\n0.591\n0.668\n‚ö†Ô∏è HIGH\n700\n0.591\n0.669\n‚ö†Ô∏è HIGH\n750\n0.588\n0.663\n‚ö†Ô∏è HIGH\n800\n0.589\n0.665\n‚ö†Ô∏è HIGH\n850\n0.585\n0.662\n‚ö†Ô∏è HIGH\n900\n0.584\n0.662\n‚ö†Ô∏è HIGH\n950\n0.585\n0.661\n‚ö†Ô∏è HIGH\n1000\n0.584\n0.660\n‚ö†Ô∏è HIGH\n1050\n0.583\n0.660\n‚ö†Ô∏è HIGH\n1100\n0.582\n0.658\n‚ö†Ô∏è HIGH\n1150\n0.579\n0.656\n‚ö†Ô∏è HIGH\n1200\n0.579\n0.655\n‚ö†Ô∏è HIGH\n1250\n0.577\n0.654\n‚ö†Ô∏è HIGH\n1300\n0.576\n0.652\n‚ö†Ô∏è HIGH\n1350\n0.576\n0.650\n‚ö†Ô∏è HIGH\n1400\n0.575\n0.649\n‚ö†Ô∏è HIGH\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n2/8\n"}, {"page": 12, "text": "Step\nAUROC\nAUPR\nRisk\n1450\n0.574\n0.648\n‚ö†Ô∏è HIGH\n1500\n0.573\n0.647\n‚ö†Ô∏è HIGH\n1550\n0.572\n0.646\n‚ö†Ô∏è HIGH\n1600\n0.570\n0.645\n‚ö†Ô∏è HIGH\n1650\n0.570\n0.644\n‚ö†Ô∏è HIGH\n1700\n0.569\n0.643\n‚ö†Ô∏è HIGH\n1750\n0.568\n0.642\n‚ö†Ô∏è HIGH\n1800\n0.567\n0.641\n‚ö†Ô∏è HIGH\n1850\n0.566\n0.640\n‚ö†Ô∏è HIGH\n1900\n0.565\n0.639\n‚ö†Ô∏è HIGH\n1950\n0.564\n0.638\n‚ö†Ô∏è HIGH\n2000\n0.563\n0.637\n‚ö†Ô∏è HIGH\nüß† Interpretation\nRisk status: All checkpoints marked as ‚ö†Ô∏è HIGH due to unsafe soft prompt injection behavior\nunder the evaluation agent. While the AUROC and AUPR appear modest, this conservative\nlabeling flags all unexpected behavior as potentially unsafe for biosecurity contexts.\nüîÅ Agentic Loop Summary\nData Loader: Loads PLLR outputs at each checkpoint\nEvaluator: Computes AUROC, AUPR\nRisk Assessor: Labels risk as ‚ö†Ô∏è HIGH at all checkpoints\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n3/8\n"}, {"page": 13, "text": "Reporter: Writes markdown/HTML report\nüß† LLM-Based Explanation (Planned)\nIncorporate LLM explanations per checkpoint:\nEach step‚Äôs AUROC/AUPR pattern can be translated into narrative insights using GPT-4\nThis supports interpretability and auditability for clinicians, regulators, and biosecurity\nexperts\nLLMs will translate raw numbers into actionable biological or model-architecture-level\nexplanations\nüöÄ Next Steps\nIntegrate GPT-4 checkpoint summaries\nTest model robustness under FGSM perturbations\nExpand to other variant prediction datasets\nüß† LLM-Based Interpretations (Per Checkpoint)\nAt step 50, AUROC (0.617) and AUPR (0.685) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 100, AUROC (0.604) and AUPR (0.669) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 150, AUROC (0.600) and AUPR (0.674) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 200, AUROC (0.605) and AUPR (0.682) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 250, AUROC (0.600) and AUPR (0.680) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n4/8\n"}, {"page": 14, "text": "At step 300, AUROC (0.597) and AUPR (0.677) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 350, AUROC (0.600) and AUPR (0.681) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 400, AUROC (0.599) and AUPR (0.677) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 450, AUROC (0.595) and AUPR (0.672) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 500, AUROC (0.595) and AUPR (0.674) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 550, AUROC (0.590) and AUPR (0.668) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 600, AUROC (0.593) and AUPR (0.670) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 650, AUROC (0.591) and AUPR (0.668) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 700, AUROC (0.591) and AUPR (0.669) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 750, AUROC (0.588) and AUPR (0.663) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 800, AUROC (0.589) and AUPR (0.665) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n5/8\n"}, {"page": 15, "text": "driven variability.\nAt step 850, AUROC (0.585) and AUPR (0.662) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 900, AUROC (0.584) and AUPR (0.662) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 950, AUROC (0.585) and AUPR (0.661) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1000, AUROC (0.584) and AUPR (0.660) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1050, AUROC (0.583) and AUPR (0.660) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1100, AUROC (0.582) and AUPR (0.658) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1150, AUROC (0.579) and AUPR (0.656) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1200, AUROC (0.579) and AUPR (0.655) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1250, AUROC (0.577) and AUPR (0.654) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nAt step 1300, AUROC (0.576) and AUPR (0.652) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n6/8\n"}, {"page": 16, "text": "At step 1350, AUROC (0.576) and AUPR (0.650) are moderate, indicating partial sensitivity to\ninjected prompts. While not catastrophic, the model's outputs begin to reflect perturbation-\ndriven variability.\nStep 1400 shows AUROC (0.575) and AUPR (0.649) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1450 shows AUROC (0.574) and AUPR (0.648) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1500 shows AUROC (0.573) and AUPR (0.647) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1550 shows AUROC (0.572) and AUPR (0.646) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1600 shows AUROC (0.570) and AUPR (0.645) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1650 shows AUROC (0.570) and AUPR (0.644) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1700 shows AUROC (0.569) and AUPR (0.643) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1750 shows AUROC (0.568) and AUPR (0.642) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1800 shows AUROC (0.567) and AUPR (0.641) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1850 shows AUROC (0.566) and AUPR (0.640) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n7/8\n"}, {"page": 17, "text": "minimal.\nStep 1900 shows AUROC (0.565) and AUPR (0.639) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 1950 shows AUROC (0.564) and AUPR (0.638) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\nStep 2000 shows AUROC (0.563) and AUPR (0.637) values on the lower end, suggesting the\nmodel has weak signal discrimination under soft prompt injection ‚Äî adversarial effects remain\nminimal.\n8/23/25, 3:56 AM\nSafe-Gene Agentic Reportfi\n8/8\n"}, {"page": 18, "text": "NeurIPS Paper Checklist\nThe checklist is designed to encourage best practices for responsible machine learning research,\naddressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove\nthe checklist: The papers not including the checklist will be desk rejected. The checklist should\nfollow the references and follow the (optional) supplemental material. The checklist does NOT count\ntowards the page limit.\nPlease read the checklist guidelines carefully for information on how to answer these questions. For\neach question in the checklist:\n‚Ä¢ You should answer [Yes] , [No] , or [NA] .\n‚Ä¢ [NA] means either that the question is Not Applicable for that particular paper or the\nrelevant information is Not Available.\n‚Ä¢ Please provide a short (1‚Äì2 sentence) justification right after your answer (even for NA).\nThe checklist answers are an integral part of your paper submission. They are visible to the\nreviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it\n(after eventual revisions) with the final version of your paper, and its final version will be published\nwith the paper.\nThe reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.\nWhile \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a\nproper justification is given (e.g., \"error bars are not reported because it would be too computationally\nexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\n\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we\nacknowledge that the true answer is often more nuanced, so please just use your best judgment and\nwrite a justification to elaborate. All supporting evidence can appear either in the main paper or the\nsupplemental material, provided in appendix. If you answer [Yes] to a question, in the justification\nplease point to the section(s) where related material for the question can be found.\nIMPORTANT, please:\n‚Ä¢ Delete this instruction block, but keep the section heading ‚ÄúNeurIPS Paper Checklist\",\n‚Ä¢ Keep the checklist subsection headings, questions/answers and guidelines below.\n‚Ä¢ Do not modify the questions and only use the provided macros for your answers.\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper‚Äôs contributions and scope?\nAnswer: [Yes]\nJustification: The abstract and introduction clearly outline the main contributions, including\nthe proposal of SAGE and its evaluation on soft prompt attacks against GFMs.\nGuidelines:\n‚Ä¢ The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n‚Ä¢ The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\n18\n"}, {"page": 19, "text": "Justification: The paper discusses generalization constraints, scope of attacks, and computa-\ntional resource needs.\nGuidelines:\n‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n‚Ä¢ The authors are encouraged to create a separate \"Limitations\" section in their paper.\n‚Ä¢ The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n‚Ä¢ While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren‚Äôt acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\nJustification: The paper does not include formal theoretical results or proofs.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not include theoretical results.\n‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.\n‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: All experimental procedures, including model configurations, training settings,\nand evaluation metrics, are described in detail.\nGuidelines:\n19\n"}, {"page": 20, "text": "‚Ä¢ The answer NA means that the paper does not include experiments.\n‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Code and data access are provided, with clear instructions for reproduction.\nGuidelines:\n‚Ä¢ The answer NA means that paper does not include experiments requiring code.\n‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n‚Ä¢ While we encourage the release of code and data, we understand that this might not be\npossible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n‚Ä¢ The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n‚Ä¢ The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n20\n"}, {"page": 21, "text": "‚Ä¢ Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: The paper specifies datasets (CM, ARM), model variants (ESM1b, ESM2,\netc.), training schedules, hyperparameters, and attack setups.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not include experiments.\n‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Statistical significance is reported using t-tests, comparison metrics (AUROC,\nAUPR) with sufficient interpretation.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not include experiments.\n‚Ä¢ The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n‚Ä¢ The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).\n‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: Compute setup (A100 GPU, runtime, batch sizes, etc.) is described in the\nExperimental Settings section.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not include experiments.\n21\n"}, {"page": 22, "text": "‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n‚Ä¢ The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n‚Ä¢ The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn‚Äôt make it into the paper).\n9. Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: The research complies with the NeurIPS Code of Ethics and does not involve\nsensitive human data or privacy-compromising methods.\nGuidelines:\n‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n‚Ä¢ If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: The discussion includes both potential security risks (adversarial misuse) and\nbenefits (improving safety auditing in clinical genomics).\nGuidelines:\n‚Ä¢ The answer NA means that there is no societal impact of the work performed.\n‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n‚Ä¢ The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n‚Ä¢ The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\n22\n"}, {"page": 23, "text": "Answer: [NA]\nJustification: No high-risk models or datasets are released that would require specific\nsafeguards.\nGuidelines:\n‚Ä¢ The answer NA means that the paper poses no such risks.\n‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: All used models and datasets are publicly available and cited with proper\nlicenses (e.g., ESM models).\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not use existing assets.\n‚Ä¢ The authors should cite the original paper that produced the code package or dataset.\n‚Ä¢ The authors should state which version of the asset is used and, if possible, include a\nURL.\n‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n‚Ä¢ If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n‚Ä¢ If this information is not available online, the authors are encouraged to reach out to\nthe asset‚Äôs creators.\n13. New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: New assets, including code for SAGE and attack pipelines, are documented\nand released with usage instructions.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not release new assets.\n‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n‚Ä¢ The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n23\n"}, {"page": 24, "text": "14. Crowdsourcing and research with human subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: No human subjects or crowdworkers are involved.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: Not applicable, as there are no human participants in the study.\nGuidelines:\n‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n‚Ä¢ We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n‚Ä¢ For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16. Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [Yes]\nJustification: GPT-4o was used in the REASON and REPORT stages for generating agentic\nreports, clearly stated in the paper.\nGuidelines:\n‚Ä¢ The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n‚Ä¢ Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)\nfor what should or should not be described.\n24\n"}]}