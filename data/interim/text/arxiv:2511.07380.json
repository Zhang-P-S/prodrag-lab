{"doc_id": "arxiv:2511.07380", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.07380.pdf", "meta": {"doc_id": "arxiv:2511.07380", "source": "arxiv", "arxiv_id": "2511.07380", "title": "Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains", "authors": ["Pingjie Wang", "Hongcheng Liu", "Yusheng Liao", "Ziqing Fan", "Yaxin Du", "Shuo Tang", "Yanfeng Wang", "Yu Wang"], "published": "2025-11-10T18:41:23Z", "updated": "2025-11-10T18:41:23Z", "summary": "Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \\textbf{\\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \\textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \\textbf{gains of +8.7 and +5.1 points}, which corresponds to a \\textbf{10.9x and 5.7x improvement} over the domain-only setting.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.07380v1", "url_pdf": "https://arxiv.org/pdf/2511.07380.pdf", "meta_path": "data/raw/arxiv/meta/2511.07380.json", "sha256": "13880f6db3bd87aee2b9d2c9c7036935e60be8942871647e2636e9622f7d787f", "status": "ok", "fetched_at": "2026-02-18T02:27:25.725442+00:00"}, "pages": [{"page": 1, "text": "SELECTING AUXILIARY DATA VIA NEURAL TANGENT\nKERNELS FOR LOW-RESOURCE DOMAINS\nPingjie Wang1,2, Hongcheng Liu1, Yusheng Liao1, Ziqing Fan1, Yaxin Du1, Shuo Tang1,\nYanfeng Wang1,2, Yu Wang1,2∗\n1Shanghai Jiao Tong University\n2Shanghai Artificial Intelligence Laboratory\n{pingjiewang, yuwangsjtu}@sjtu.edu.cn\nABSTRACT\nLarge language models (LLMs) have achieved remarkable success across\nwidespread tasks, yet their application in low-resource domains remains a sig-\nnificant challenge due to data scarcity and the high risk of overfitting. While in-\ndomain data is limited, there exist vast amounts of similar general-domain data,\nand our initial findings reveal that they could potentially serve as auxiliary su-\npervision for domain enhancement. This observation leads us to our central re-\nsearch question: how to effectively select the most valuable auxiliary data to\nmaximize domain-specific performance, particularly when traditional methods\nare inapplicable due to a lack of large in-domain data pools or validation sets. To\naddress this, we propose NTK-Selector, a principled and efficient framework for\nselecting general-domain auxiliary data to enhance domain-specific performance\nvia neural tangent kernels (NTK). Our method tackles two challenges of directly\napplying NTK to LLMs, theoretical assumptions and prohibitive computational\ncost, by empirically demonstrating a stable NTK-like behavior in LLMs during\nLoRA fine-tuning and proposing a Jacobian-free approximation method. Exten-\nsive experiments across four low-resource domains (medical, financial, legal, and\npsychological) demonstrate that NTK-Selector consistently improves downstream\nperformance. Specifically, fine-tuning on 1,000 in-domain samples alone only\nyielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In\ncontrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to\nsubstantial gains of +8.7 and +5.1 points, which corresponds to a 10.9x and 5.7x\nimprovement over the domain-only setting.\n1\nINTRODUCTION\nMedMCQA\nMMLU-Med\nFPB\nTFNS\nHeadline\nContractNLI\nPrivacyQA\nRSDD\n56.7\n59.0\n72.3\n73.7\n81.7\n85.5\n54.5\n73.3\n78.5\n86.2\n59.9\n70.6\n57.9\n67.8\n78.8\n96.5\nBase\nDomain-Only\nRandom\nLESS\nNTK-Selector (Ours)\nFigure 1:\nPerformance of LLAMA3-\n8B-INSTRUCT evaluated on medical, fi-\nnancial, legal, and psychological tasks.\nEach task is augmented with 9K aux-\niliary samples selected by Random,\nLESS, and NTK-Selector from Cot Col-\nlection based on 1K domain samples.\nThe emergence of large language models (LLMs) has led\nto remarkable advancements across a wide spectrum of\nnatural language processing tasks (Touvron et al., 2023;\nChowdhery et al., 2023; Yang et al., 2025). However,\ntheir formidable capabilities are predominantly anchored\nin the availability of immense, high-quality pre-training\nand instruction-tuning datasets. This dependence is par-\nticularly problematic in low-resource domains, where\ndata is scarce, expensive to curate, and often entangled\nwith privacy constraints. In such settings, directly fine-\ntuning LLMs on limited domain-specific datasets fre-\nquently induces severe overfitting and poor generaliza-\ntion (Liu et al., 2023a; B´ethune et al., 2025), as reflected\nby the degradation in Figure 1. Consequently, the util-\nity of LLMs for specialized downstream applications re-\nmains fundamentally constrained.\n∗Corresponding author.\n1\narXiv:2511.07380v1  [cs.CL]  10 Nov 2025\n"}, {"page": 2, "text": "While in-domain data is scarce, there is often a wealth of general-domain data that shares similar\ntopics, question-answering formats, and reasoning patterns. This observation motivates us to inves-\ntigate whether such data can serve as auxiliary supervision for domain augmentation. As shown\nin Figure 1, our initial experiments using randomly sampled auxiliary data demonstrate promising\nperformance gains, outperforming fine-tuning with limited in-domain data alone. This key finding\nconfirms the potential of auxiliary data for domain enhancement and leads to our central research\nquestion: How can we effectively select the most valuable auxiliary data to maximize domain-\nspecific performance? While a number of task-specific data selection methods (Xie et al., 2023;\nLiu et al., 2024) have been proposed (e.g., LESS (Xia et al., 2024) in Figure 1), they are often not\napplicable to our problem, as they assume the availability of a large in-domain source pool and a\ncurated validation set. These critical resources are precisely what are absent in low-resource, cross-\ndomain scenarios, highlighting a significant gap and limiting the applicability of existing methods.\nTo bridge this gap, we seek a criterion to predict how training on a general-purpose instance will\ninfluence the target performance. The Neural Tangent Kernel (NTK) (Jacot et al., 2018) provides\nan ideal signal for this, as it stably quantifies the alignment of training dynamics between a general-\npurpose sample and a specific domain even without explicit similarity. Motivated by this, we develop\nan NTK-grounded data selection criterion that scores auxiliary samples by their NTK similarity to\nthe target domain, with the hypothesis that higher similarity leads to greater performance gains.\nHowever, applying NTK to LLMs presents two key challenges: (i) the theoretical assumptions (e.g.,\ninfinite width) are not directly met, and (ii) the computational cost of exact NTK computation for\nLLMs is prohibitive. To address the first challenge, we empirically demonstrate a stable NTK-like\nbehavior in LLMs during LoRA fine-tuning, showing that the training dynamics can be reliably\napproximated by the model’s initial state. To overcome the computational bottleneck, we propose\na simplified, Jacobian-free approximation method that is both memory- and time-efficient. To our\nknowledge, this is the first work to both empirically investigate the kernel behavior of LLMs in the\nfine-tuning regime and to leverage the insight for scalable data selection.\nBased on these key insights, we propose NTK-Selector, a novel two-stage data selection frame-\nwork. It first performs a coarse-grained pre-selection of candidates using embedding similarity.\nThis is followed by a fine-grained NTK selection stage, where we compute the NTK utility score\nfor each pre-selected sample. Our method incorporates LoRA-based training and random gradient\nprojections to significantly reduce memory and computational overhead, enabling efficient selection\nof large-scale auxiliary data. In summary, our work makes the following core contributions:\n• We formalize a novel problem of selecting high-value auxiliary data from massive, open-\nworld general corpora to enhance performance for low-resource domains, specifically un-\nder the realistic constraint of having no task-specific validation sets or large in-domain\ncorpora available (§2.1).\n• We propose NTK-Selector, a novel and efficient two-stage data selection framework\ngrounded in the NTK. To address two key challenges in applying NTK to LLMs, the-\noretical assumptions and computational cost, we provide the first empirical evidence of\nstable NTK-like behavior of LLMs under LoRA fine-tuning, and propose a Jacobian-free\napproximation to make it computationally feasible (§3 and §4).\n• We demonstrate that NTK-Selector delivers substantial and consistent gains across diverse\nlow-resource domains, including medicine, finance, law, and psychology. Our method\nachieves up to 10.9x and 5.7x average performance improvement over domain-only fine-\ntuning on LLAMA3-8B-INSTRUCT and QWEN3-8B, respectively (§5 and §6).\n2\nPRELIMINARIES\n2.1\nPROBLEM STATEMENT\nFormally, let D denote the small domain-specific dataset and C the large general-purpose candidate\ncorpus, where |D| ≪|C|. We aim to select a subset S ⊂C of size N. Let f(x, θ) : Rd →RD\nrepresent the output of LLM parameterized by θ with input x ∈Rd, L(θ; X) denote the training\nloss on a dataset X, and T(f(·; θ), Ttest) denote the task performance metric evaluated on a held-\nout test set Tval. Our objective is to identify an optimal subset S∗that maximizes downstream\ntask performance after fine-tuning on the combined dataset D ∪S∗. This can be formulated as the\n2\n"}, {"page": 3, "text": "following bi-level objective function:\nS∗= arg\nmax\nS⊆C,|S|=N T (f (·; θS) , Ttest) ,\nwhere\nθS = arg min\nθ\nL (θ; D ∪S) .\n(1)\n2.2\nNEURAL TANGENT KERNEL\nThe Neural Tangent Kernel (NTK) (Jacot et al., 2018) is a theoretical framework that proves that the\nevolution of an infinitely wide neural network under specific parameterization can be described by a\nlinear kernel regression. This provides a powerful tool for analyzing the dynamics and generalization\nof deep neural networks. Formally, for a model f(·; θt) with parameters θt in the training step t and\ntwo input examples x, x′ ∈Rd, the NTK Θ(x, x′; θt) is defined as:\nΘ(x, x′; θt) = ⟨∇θtf(x; θt), ∇θtf(x′; θt)⟩,\n(2)\nwhere ∇denotes the gradient and ⟨·⟩represents inner product. A high NTK value indicates that x\nand x′ will guide the model in similar directions during training, leading to correlated generalization.\nThis makes NTK an ideal metric for selecting the most relevant cross-domain auxiliary data. In\nthe infinite-width and Gaussian initialization limit, the NTK remains constant throughout training,\nwhich implies that such a measurement of data point similarity is stable as the model learns.\n3\nNEURAL TANGENT KERNEL FOR LARGE LANGUAGE MODELS\nWhile the NTK serves as a powerful tool for data selection, its direct application to LLMs faces two\nkey challenges: (i) LLMs don’t meet the theoretical assumptions of NTK, and (ii) the exact NTK\ncomputation is infeasible. In this section, we address these challenges by empirically demonstrating\nan NTK-like behavior in LLMs (§3.1) and proposing a scalable approximation method (§3.2).\n3.1\nNTK-LIKE BEHAVIOR IN LLMS\nWhile classic NTK theory assumes a Gaussian initialized infinite-width network with a constant\nkernel matrix throughout training, real-world pre-trained LLMs operating in the fine-tuning regime\ndo not satisfy these assumptions. Our empirical observations reveal that the NTK of a fine-tuning\nLLM is not strictly constant; however, its directional structure remains remarkably stable. This phe-\nnomenon is characterized by the time-evolved kernel being nearly collinear with the initial kernel,\nas quantified by a high Frobenius cosine similarity (e.g., greater than 0.99), which is visualized in\nFigure 2a. We formalize this observation as NTK-like behavior.\nDefinition 1 (NTK-like). Given a model f(·; θt) with NTK Θ(·, ·; θt), the model is said to exhibit\nNTK-like behavior on the interval [0, T] if there exists a small constant ϵ ∈(0, 1) such that for\nall t ∈[0, T], the Frobenius cosine similarity between the time-evolved NTK and the initial NTK\nsatisfies:\ncosF (Θ(·, ·; θt), Θ(·, ·; θ0)) ≥1 −ϵ.\n(3)\nThis NTK-like property suggests that during fine-tuning, the model primarily amplifies feature di-\nrections established at initialization rather than learning entirely new, orthogonal features. This ob-\nservation is crucial because it implies that, with a small ϵ, training dynamics can still be effectively\ndescribed by the initial kernel. We theoretically demonstrate that if a model satisfies this property,\nits training dynamics can be re-parameterized to be equivalent to training on a fixed kernel Θ(·, ·; θ0)\nwith a controllable perturbation term. This is formally established in the following theorem, with a\ndetailed proof provided in Appendix A.3.\nTheorem 1. Given a model f(·; θt) with NTK Θ(·, ·; θt) whose training dynamics are governed by\na gradient flow ˙f(·; θt) = −ηΘ(·, ·; θt)γ(t), where γ(t) = ∇fL(f(·; θt)) and η is the learning rate,\nif it exhibits NTK-like behavior on [0, T], its training dynamics can be re-parameterized onto a new\ntime axis u:\n˙f(·; θt(u)) = −ηΘ(·, ·; θ0)γ(t(u)) + ∆(u),\n(4)\nwhere u(t) :=\nR t\n0 a∗(τ)dτ, a∗(t) = ⟨Θ(·,·;θt),Θ(·,·;θ0)⟩\n||Θ(·,·;θ0)||2\n, and the perturbation term ∆(u) is bounded\nwith ||∆(u)|| ≤η||Θ(·, ·; θ0)||\n√\n2ϵ\n1−ϵ||γ(t(u))||.\n3\n"}, {"page": 4, "text": "0\n5\n10\n15\n20\nEpoch\n20\n15\n10\n5\n0\nEpoch\nLlama3-8B-Instruct\n0\n5\n10\n15\n20\nEpoch\n20\n15\n10\n5\n0\nEpoch\nQwen3-8B\n0.990\n0.995\n1.000\n(a)\nExact NTK values (x, x′;\n)\nJacobian-free approximation (x, x′;\n)\nData point\nIdentity line\n(b)\nFigure 2: (a) Frobenius cosine similarity between NTK of LLAMA3-8B-INSTRUCT and QWEN3-\n8B during LoRA-based instruction tuning towards financial sentiment analysis task. (b) Correlation\nbetween the exact NTK values and Jacobian-free approximation across a diverse set of input pairs.\nThis theorem justifies that, despite the kernel’s Frobenius norm changing, the training dynamics can\nstill be accurately approximated using the initial kernel. It allows us to use the NTK calculated at the\ninitial state of an LLM as a reliable proxy for data utility throughout the fine-tuning process, which\nis the core principle behind our data selection method detailed in §4.\n3.2\nJACOBIAN-FREE APPROXIMATION.\nComputing the exact NTK as defined in Equation (2) for LLMs is computationally prohibitive, as\nit requires constructing and storing the full Jacobian matrix. This is infeasible given the immense\ndimensionality of both model parameters and outputs. To overcome this challenge, we introduce a\nscalable, Jacobian-free NTK approximation.\nDefinition 2 (Jacobian-free NTK Approximation). Given a model f(·; θt) : Rd →RD, the\nJacobian-free NTK approximation between two inputs x and x′ is defined as:\neΘ(x, x′; θt) =\n*\n∇θt\nD\nX\nk=1\nfk(x; θt), ∇θt\nD\nX\nk=1\nfk(x′, θt)\n+\n,\n(5)\nThis approximation can be expanded to reveal its relationship with the exact NTK: eΘ(x, x′; θt) =\nPD\nk=1 ⟨∇θtfk(x; θt), ∇θtfk(x′; θt)⟩+ PD\nk=1\nPD\nm=1,m̸=k⟨∇θtfk(x; θt), ∇θtfm(x′, θt)⟩, whose\nconcrete derivation is provided in Appendix A.4. The first term precisely recovers the exact NTK,\nwhile the second term accounts for cross-output interactions. In practice, we empirically observe\nthat the gradient directions for different output dimensions tend to be nearly orthogonal, making the\ncross terms very small (approximately 6% of the exact NTK). As shown in Figure 2b, it demon-\nstrates a strong linear correlation between the exact NTK and our approximation. This confirms that\nthe relative magnitudes of NTK values are well-preserved, which provides a reliable proxy for the\nexact kernel and justifies our use of eΘ for efficient and scalable data selection.\n4\nMETHOD\n4.1\nTOTAL FRAMEWORK\nBased on the above insights, we propose NTK-Selector, a novel two-stage data selection framework.\nGiven a domain dataset D, candidate dataset C, pre-selection size M, number of nearest neighbors\nK, LLM f(·; θ), and random projection dimension matrix Π, our goal is to efficiently select a high-\nvalue subset S ⊂C. The process is outlined in Algorithm 1 and contains two main phases: (i)\nCoarse-grained pre-selection (§4.2): an efficient embedding-based filtering step that constructs a\nreduced candidate set Spre ⊂C; (ii) Fine-grained NTK selection (§4.3): a computationally re-\nfined stage to select S involving gradient computation, random projection, and Jacobian-free NTK\napproximation to estimate sample utility over the pre-selected set Spre.\n4.2\nCOARSE-GRAINED PRE-SELECTION\nThe computational burden of evaluating the NTK over a very large candidate set C (where |C| ≫\n105) necessitates an efficient pre-selection mechanism. We thus propose a two-stage procedure,\n4\n"}, {"page": 5, "text": "Algorithm 1 NTK-Selector\nInput: D, C, M, K f(·; θ), Π.\nOutput: S.\n1: Spre ←PRE-SELECT(D, C, M, K, f(·; θ));\n2: for x ∈D ∪Spre do\n3:\nCompute LoRA gradient: b∇θf(x; θ) = ∇θLoRA\n\u0010PD\nk=1 fk(x; θ)\n\u0011\n;\n4:\nProject gradient: e∇θf(x; θ) = Π⊤b∇θf(x; θ);\n5: end for\n6: for i = 1, ...|D| do\n7:\nfor j = 1, ...M do\n8:\neΘ(xi, xj; θ) ←\nD\ne∇θf(xi; θ), e∇θf(xj; θ)\nE\nwhere xi ∈D, xj ∈Spre;\n9:\nend for\n10: end for\n11: Compute NTK utility score: sj ←\n1\n|D|\nP\nxi∈D eΘ(xi, xj; θ) for j = 1, . . . , M;\n12: Select Top-N samples: S ←{Top-N samples from Spre ranked by their score sj}.\nbeginning with a coarse-grained filtering step based on embedding similarity, to reduce the candidate\nset to a tractable size prior to fine-grained NTK assessment. Let ϕ : X →Rm denote a feature\nmapping obtained by averaging the hidden states of the final transformer layer. To better capture\ndomain-specific contextual nuances, we perform a warm-up LoRA training phase on the available\ndomain data D before computing the embeddings. For a domain example xi ∈D and candidate\nxj ∈C, let di = ϕ(xi) and cj = ϕ(xj) be their respective embeddings. We define the Euclidean\ndistance between embeddings as d(di, cj) = ∥di −cj∥2. For each di, we identify the set NK(di)\nof indices corresponding to the K nearest neighbors in {cj}|C|\nj=1 under this metric. The relevance of\na candidate xj to the entire domain set D is quantified by the number of domain points for which\nit ranks among the top-K neighbors: r(xj) = P|D|\ni=1 1 (j ∈NK(di)), where 1(·) is the indicator\nfunction. The pre-selected set Spre ⊂C of size M is then constructed by taking the candidates\nwith the highest relevance scores: Spre = TopM\nx∈C\n(r(x)). This embedding-based pre-selection serves\nas a scalable proxy for semantic relevance, significantly reducing the number of samples while\npreserving high-utility candidates.\n4.3\nFINE-GRAINED NTK SELECTION\nGiven the pre-selected set Spre, we now proceed with the fine-grained selection using our NTK-\nbased utility score. We define the NTK utility score sj for a candidate point xj ∈Spre as its average\nNTK similarity to all points in the domain dataset D: sj =\n1\n|D|\nP\nxi∈D eΘ(xi, xj; θ), where eΘ is\nour Jacobian-free NTK approximation as described in §3.2. As justified in §2.2 and §3.1, selecting\ndata points that maximize this utility score serves as a practical surrogate for the intractable bi-level\nobjective function in Equation (1). This greedy maximization strategy, which selects N candidates\nwith the highest scores, avoids the need for expensive, iterative training and evaluation for every\ncandidate subset, providing a scalable and effective solution for selecting relevant auxiliary data.\nParameter Efficient NTK via LoRA.\nTo alleviate the memory pressure of gradient computations,\nwe leverage a low-rank adaptation (LoRA) (Hu et al., 2022) to restrict gradient estimation to a low-\nrank parameter subspace. By focusing on the gradients of the LoRA modules, which are denoted as\nˆ∇θf(·; θ) := ∇θLoRAf(·; θ) ∈RP with P LoRA parameters, we can work within a much lower-\ndimensional parameter space compared to the full model. For instance, in LLAMA3-8B-INSTRUCT,\nthe gradient dimensionality of the LoRA modules is only about 0.5% of the full model’s gradient\nspace. This significant reduction enables efficient and scalable NTK approximation without com-\npromising empirical performance.\nRandom Projection for Scalable NTK Estimation.\nTo scale our NTK approximation to large\ncandidate sets, we apply random projection to the LoRA gradients, further reducing memory\n5\n"}, {"page": 6, "text": "and computational requirements (Park et al., 2023). This approach is motivated by the Johnson-\nLindenstrauss Lemma (Johnson et al., 1984), which guarantees that inner products are approx-\nimately preserved under low-dimensional random projections.\nFormally, for each input x we\nproject its LoRA gradient b∇θf(·; θ) ∈RP into a p-dimensional space (p ≪P) via e∇θf(·; θ) =\nΠ⊤b∇θf(·; θ), where Π ∈RP ×p is a random projection matrix with entries drawn independently\nfrom a Rademacher distribution (Πi,j = ±1 with equal probability ). The same Π is applied consis-\ntently across all samples using a fixed random seed, ensuring that the approximated inner products\nused in Equation (5) remain consistent and valid for calculation of the utility score.\n5\nEXPERIMENTS\n5.1\nSETUP\nDatasets.\nWe evaluate on four low-resource domains: medical (MedMCQA (Pal et al., 2022),\nMMLU-Med (Hendrycks et al., 2021)), financial (FPB (Malo et al., 2014), TFNS (Magic, 2022),\nHeadline (Sinha & Khandait, 2021)), legal (ContractNLI (Koreeda & Manning, 2021), Priva-\ncyQA (Ravichander & Alan, 2019)), and psychological (RSDD (Yates et al., 2017)) domains. Ad-\nditional dataset details are provided in Appendix A.6.1. For candidate data, we use the Cot Col-\nlection (Kim et al., 2023) (1.8M instruction-response pairs) for auxiliary data selection. Domain-\nspecific training sets are used as instruction datasets when applicable, except for the medical domain,\nfor which we use UltraMedical (Zhang et al., 2024) for instruction tuning. For training instances\nlacking chain-of-thought responses, we generate such responses using GPT-4o-mini (Hurst et al.,\n2024). Further examples of query templates and generated responses are included in Appendix A.7.\nBaselines.\nWe compare NTK-Selector against the following baselines to evaluate its effectiveness.\nThe simplest is Base, which assesses the out-of-the-box performance of the pre-trained backbone\nmodel without any fine-tuning. Another baseline, Domain-Only, involves fine-tuning the model ex-\nclusively on a limited amount of domain-specific data without augmentation. For selecting auxiliary\ndata, we explore several strategies. The most straightforward is Random selection, where general-\npurpose data is randomly sampled and mixed with the domain-specific dataset. This baseline helps\ngauge the benefit of simply adding more data. We also compare with DSIR (Xie et al., 2023), which\nuses n-gram features to weight candidate data and samples based on these weights. LESS (Xia\net al., 2024) is another baseline that selects data by approximating the first-order influence of each\ncandidate sample on the validation set. Lastly, we use TSDS (Liu et al., 2024), which captures\nthe discrepancy between the candidate and target data distributions by aligning them using optimal\ntransport. Please refer to Appendix A.6.2 for more implementation details.\nModels & Settings.\nWe employ Llama3-8B-Instruct (Dubey et al., 2024) and QWEN3-8B (Yang\net al., 2025) as base models. All fine-tuning is performed using LoRA. We randomly sample 1K\ninstances from each of the domain datasets for augmentation, and select 9K instances from the\ncandidate pool for mixed training. Models are mixed training for 3 epochs, and the gradients are\nrandomly projected to 8192-dimensional features. The pre-selection stage is implemented using\nFaiss (Douze et al., 2024) library with M = 4N, and K = M/4 for coarse-grained pre-selection.\nTo eliminate the effect of the sequence length of each data point, we normalize the gradient in\nEquation (5) with the sequence length. For numerical stability, gradient sums are scaled by a factor\nof 10−5. Additional training hyperparameters are detailed in Appendix A.6.3 and A.6.4.\n5.2\nMAIN RESULTS\nTable 1 presents the main results of our approach and various baselines across different models and\ntarget domains. Below, we summarize our key findings.\nLimited domain data can degrade performance.\nFine-tuning with only 1K domain-specific data\npoints yielded surprising results. Instead of consistent improvement, we observed that this limited\ndata volume often led to a negligible performance gain or, in some cases, a degradation in task ca-\npability (e.g., LLAMA3-8B-INSTRUCT drops 18.8 points on ContractNLI; QWEN3-8B drops 7.9\npoints on FPB). We attribute this phenomenon to overfitting (B´ethune et al., 2025), where the fine-\n6\n"}, {"page": 7, "text": "Table 1: Performance of LLAMA3-8B-INSTRUCT and QWEN3-8B across 8 tasks on medical\n(MedMCQA and MMLU-Med), financial (FPB, TFNS, and Headline), legal (ContractNLI and Pri-\nvacyQA), and psychological (RSDD) domains. The best individual task results for each model are\nhighlighted in bold, and relative performance changes compared to the Base model are denoted by\n↑(increase), ↓(decrease), and →(no change).\nMedical\nFinancial\nLegal\nPsycho.\nMedMCQA\nMMLU-Med\nFPB\nTFNS\nHeadline\nContractNLI\nPrivacyQA\nRSDD\nAvg.\nLLAMA3-8B-INSTRUCT\nBase\n56.7\n72.3\n81.7\n57.2\n78.5\n59.9\n57.9\n78.8\n67.9\nDomain-Only\n56.5 0.2↓\n71.8 0.5↓\n80.1 1.6↓\n69.3 12.1↑\n82.4 3.9↑\n41.1 18.8↓\n63.6 5.7↑\n85.1 6.3↑\n68.7\nRandom\n57.1 0.4↑\n73.0 0.7↑\n80.6 1.1↓\n70.7 13.5↑\n85.2 6.7↑\n66.5 6.6↑\n66.2 8.3↑\n95.4 16.6↑\n74.3\nDSIR\n57.5 0.8↑\n72.7 0.4↑\n84.6 2.9↑\n71.2 14.0↑\n86.2 7.7↑\n64.5 4.6↑\n66.5 8.6↑\n94.7 15.9↑\n74.7\nLESS\n57.5 0.8↑\n73.1 0.8↑\n82.4 0.7↑\n70.7 13.5↑\n85.9 7.4↑\n67.8 7.9↑\n66.6 8.7↑\n94.7 15.9↑\n74.8\nTSDS\n57.0 0.3↑\n71.4 0.9↓\n83.3 1.6↑\n71.2 14.0↑\n85.0 6.5↑\n68.9 9.0↑\n66.3 8.4↑\n94.9 16.1↑\n74.8\nNTK-Selector\n59.1 2.4↑\n73.8 1.5↑\n85.5 3.8↑\n73.3 16.1↑\n86.2 7.7↑\n70.6 10.7↑\n67.8 9.9↑\n96.5 17.7↑\n76.6\nQWEN3-8B\nBase\n59.3\n83.4\n80.1\n70.2\n74.0\n73.8\n52.3\n94.6\n73.5\nDomain-Only\n61.1 1.8↑\n81.9 1.5↓\n72.2 7.9↓\n72.0 1.8↑\n75.3 1.3↑\n76.7 2.9↑\n66.5 14.2↑\n89.4 5.2↓\n74.4\nRandom\n60.8 1.5↑\n82.6 0.8↓\n65.2 14.9↓\n71.9 1.7↑\n80.9 6.9↑\n77.3 3.5↑\n53.9 1.6↑\n90.8 3.8↓\n72.9\nDSIR\n61.1 1.8↑\n82.7 0.7↓\n85.9 5.8↑\n69.2 1.0↓\n82.3 8.3↑\n78.8 5.0↑\n67.1 14.8↑\n94.6 0.0→\n77.7\nLESS\n60.3 1.0↑\n82.6 0.8↓\n30.3 49.8↓\n69.8 0.4↓\n81.8 7.8↑\n78.6 4.8↑\n62.3 10.0↑\n93.8 0.8↓\n69.9\nTSDS\n60.6 1.3↑\n82.3 1.1↓\n84.7 4.6↑\n68.1 2.1↓\n81.9 7.9↑\n79.8 6.0↑\n50.1 2.2↓\n92.5 2.1↓\n75.0\nNTK-Selector\n61.4 2.1↑\n83.8 0.4↑\n85.3 5.2↑\n72.2 2.0↑\n83.4 9.4↑\n79.9 6.1↑\n67.5 15.2↑\n95.1 0.5↑\n78.6\ntuning process overwrites the extensive general knowledge acquired during pre-training. Further-\nmore, the small dataset size fails to provide a stable optimization landscape, leading to training\ninstability, poor convergence, and a tendency for the model to overfit to noise within the data.\nAuxiliary data can usually, but not necessarily, boost the model performance.\nWe selected\n9K auxiliary samples using various methods and combined them with the 1K target data points\nfor mixed training. For LLAMA3-8B-INSTRUCT, even random selection improves performance\nacross most domains. In contrast, for QWEN3-8B, random selection often degrades performance,\nfor instance, reducing gains on PrivacyQA from 14.2 to just 1.6 points. This negative effect is even\nfurther exacerbated by some advanced selection methods (e.g., LESS induces a 49.8-point drop on\nFPB). We hypothesize that this difference stems from QWEN3-8B’s superior initial performance,\nwhich makes it more sensitive to the quality of auxiliary data, leading low-quality samples to more\neasily disrupt the training process of a powerful model.\nNTK-Selector is the only consistently effective approach.\nWhile conventional methods like\nDSIR (n-gram features), LESS (influence functions), and TSDS (representation alignment) can be\neffective for coreset selection, they often fail to provide stable improvements when selecting aux-\niliary data from a general, out-of-domain corpus. In contrast, NTK-Selector consistently enhances\nmodel performance across all domains and architectures. Our approach selects auxiliary data whose\noptimization trajectories align with those of the target domain, thereby stabilizing training and pro-\nmoting more robust convergence. As shown in Table 1, NTK-Selector achieves the highest average\nperformance on both LLAMA3-8B-INSTRUCT (76.6, from a base of 67.9) and QWEN3-8B (78.6,\nfrom 73.5). Compared to the domain-only baseline, which improves by only 0.8 and 0.9 points\nrespectively, our method delivers gains of 8.7 and 5.3 points, corresponding to a 10.9x and 5.7x\nimprovement. Notably, NTK-Selector is the only method that improves performance on every task,\nincluding MMLU-Med and RSDD for QWEN3-8B, where other techniques often degrade results.\nThis demonstrates that NTK-Selector reliably identifies high-value auxiliary examples that comple-\nment the target domain, leading to superior and generalizable gains.\n6\nANALYSIS\nThis section presents an analysis of the NTK-Selector framework through multiple ablations. We\nfirst examine how the quantity of auxiliary data affects performance, and how the size of the target\ndataset influences the utility of added auxiliary examples. We then ablate key hyperparameters to\nprovide practical recommendations, such as pre-selection size and projection dimension. Finally,\n7\n"}, {"page": 8, "text": "we compare the performance of our method against empirical NTK regression to further validate the\nNTK-like behavior of LLMs. Unless specified, all experiments use LLAMA3-8B-INSTRUCT with\n1K domain examples and 9K auxiliary samples.\n69\n70\n71\n72\n73\n74\n×1\n×2\n×5\n×10\n×20\n54\n55\nNumber of total samples\nAccuracy\nBase\nRandom\nDSIR\nLESS\nTSDS\nOurs\nFigure 3:\nAccuracy on TFNS task with\n1K domain samples and various numbers of\nauxiliary samples, where the total number is\nenriched by ×2, ×5, ×10, and ×20.\nScaling behavior with auxiliary data\nWe investi-\ngate how the volume of auxiliary data affects perfor-\nmance by holding the domain set fixed at 1K sam-\nples (×1) and scaling the total data size to ×2, ×5,\n×10, and ×20 via added auxiliary examples.\nAs\nshown in Figure 3, NTK-Selector consistently out-\nperforms all baselines at every scale. Notably, the\nperformance gains exhibit clear diminishing returns:\nwhile increasing data from ×1 to ×5 brings substan-\ntial gains (69.3→71.9), further expanding to ×10 and\n×20 yields minimal improvement (73.3).\nWe hy-\npothesize that the supply of highly relevant cross-\ndomain samples is inherently limited. Beyond a cer-\ntain point (e.g., ×10), incorporating less relevant data\ncontributes little additional signal and may even di-\nlute fine-tuning efficacy, suggesting the existence of\na practical saturation threshold for auxiliary data util-\nity. This observation emphasizes the importance of\nsample quality over sheer quantity in data selection.\n×1\n×2\n×5\n×10\n×20\nNumber of total samples\n57\n61\n65\n69\n73\n77\nAccuracy\nBase\n|\n| = 100\n|\n| = 200\n|\n| = 500\n|\n| = 1000\n|\n| = 2000\nFigure 4: Accuracy on TFNS task with 100,\n200, 500, 1K, 2K domain samples enriched\nby ×2, ×5, ×10, and ×20.\nLess target data, more pronounced improvement\nwith auxiliary data.\nTo further analyze the effec-\ntiveness of auxiliary data, we explored its impact on\ndifferent volumes of target data, ranging from a very\nsmall set |D| = 100 to a larger one |D| = 2000. As\nshown in Figure 4, the performance of the baseline\n(×1) predictably improves as the amount of target\ndata increases, and the results demonstrate that our\nmethod consistently provides significant gains across\nall domain data sizes. Crucially, we found that the\nrelative gain from auxiliary data is most pronounced\nin the most resource-constrained settings. For exam-\nple, with only |D| = 100 samples, performance im-\nproves from 54.6 to 68.6 at the ×10 scale, which re-\nsults in a gain of 14.0 points (25.6%). In contrast,\nwith |D| = 2000, the improvement is only from 70.9\nto 74.6, a gain of 3.7 points (5.2%). It empirically\ndemonstrates that NTK-Selector is especially power-\nful in very low-resource scenarios, where it compen-\nsates effectively for the lack of in-domain data.\n2000\n4000\n6000\n8000\nProjection dimension\n67\n70\n73\n76\nAccuracy\nDomain-Only\nNTK-Selector\nFigure 5: Average performance of NTK-\nSelector using different projected dimen-\nsions as 1024, 2048, 4096, and 8192.\nSmall projection dimension is sufficient.\nThis\npart investigates the influence of the gradient projec-\ntion dimension p, which varies across 1024, 2048,\n4096, and 8192 (default). As shown in Figure 5, even\nthe smallest dimension (p=1024) yields significant\nimprovement over the domain-only baseline.\nPer-\nformance increases steadily with larger dimensions,\nsuggesting that higher-dimensional projections better\npreserve gradient similarity information. Therefore,\nwe recommend using a larger projection dimension if\ncomputational resources allow.\n8\n"}, {"page": 9, "text": "Table 2: Accuracy with size M set as 2N, 4N (default),\nand 16N in the coarse-grained pre-selection stage.\nMMLU-Med\nFPB\nContractNLI\nRSDD\nBase\n72.3\n81.7\n59.9\n78.8\nDomain-Only\n71.8 0.5↓\n80.1 1.5↓\n41.1 18.8↓\n85.1 6.3↑\nM = 2N\n73.3 1.0↑\n84.5 2.8↑\n68.9 9.0↑\n95.5 16.7↑\nM = 4N\n73.7 1.4↑\n85.5 3.8↑\n70.6 10.7↑\n96.5 17.7↑\nM = 16N\n74.4 2.1↑\n86.9 5.2↑\n70.6 10.7↑\n95.9 17.1↑\nTable 3: The fine-tuning (FT.) and\neNTK accuracy on different domains.\n✓denotes that the kernel analog\nachieves >90% of FT. accuracy.\nFin.\nLegal\nPsycho.\nFT.\n77.27\n52.35\n85.10\neNTK\n71.57\n54.45\n88.40\n∆\n5.70 ✓\n2.10 ✓\n3.30 ✓\nTable 4: Asymptotic complexity and wall-clock runtime (measured as single NVIDIA A100 GPU\nminutes) with each step in NTK-Selector.\nCoarse-grained Pre-selection\nFine-grained NTK Selection\nWarm-up training\nEmbedding Computation\nKNN Selection\nGradient Computation\nNTK selection\nComplexity\nO(|D|)\nO(|C|)\nO(|C| · M)\nO(M)\nO(M · N)\nRuntime\n8 Min\n0.9 Min/K samples\n10 Min\n6 Min/K samples\n1 Min\nScaling pre-selection.\nTo enhance scalability over large candidate sets, NTK-Selector employs a\ncoarse-grained pre-selection stage that reduces the candidate pool from millions to a manageable\nset of M samples. We evaluate the effect of M in Table 2 using multiples of the final subset size\nN: 2N, 4N (default), and 16N. The results confirm that a larger pre-selection size consistently\nimproves accuracy across tasks. For example, increasing M from 2N to 16N brings substantial\ngains in MMLU-Med (73.3 →74.4) and FPB (84.5 →86.9), with more moderate improvements in\nContractNLI and RSDD. However, larger values of M also entail higher computational and memory\ncosts during the subsequent NTK scoring stage. We therefore recommend choosing M within the\nrange of 4N to 16N, which provides a practical trade-off between selection quality and efficiency.\nEmpirical NTK regression.\nTo further validate the NTK-like behavior of pre-trained LLMs, we\ncompare the fine-tuning accuracy against predictions from empirical Neural Tangent Kernel (eNTK)\nregression following Wei et al. (2022) and Malladi et al. (2023). For each task, we compute the\neNTK kernel matrix and solve the associated kernel regression problem (see Appendix A.5 for im-\nplementation details). As shown in Table 3, eNTK regression achieves 90% of fine-tuning accuracy\nin all cases across financial, legal, and psychological domains. This close agreement provides fur-\nther evidence that the gradient-based dynamics of LLMs during task-specific fine-tuning are well-\napproximated by a static kernel, supporting our observation in §3.1. These results reinforce the\npractical relevance of NTK-based analysis and selection mechanisms for LLMs.\nComputational complexity and runtime analysis.\nTable 4 outlines the asymptotic complexity\nand wall-clock runtime for each stage of NTK-Selector in Algorithm 1. The wall-clock time is\nmeasured in single NVIDIA A100 (80GB) GPU minutes. The most computationally expensive\nsteps are embedding and gradient computation, which scale linearly with the candidate size |C| and\nthe pre-selection size M. Under our default configuration, which selects N = 9000 samples from a\ncandidate pool of 1.8 million samples with M = 4N, the total runtime is approximately 31 hours.\nThis cost scales favorably and can be substantially reduced by using smaller candidate sets or pre-\nselection sizes, making the method practical for large-scale data selection.\n7\nCONCLUSION\nIn this paper, we address the challenge of data scarcity in low-resource domains by introducing a\nnovel framework, NTK-Selector, for selecting high-value auxiliary data from large general-domain\ncorpora. Our approach is grounded in the key observation that LLMs exhibit stable NTK-like be-\nhavior during LoRA-based fine-tuning, enabling efficient and theoretically motivated data selection.\nExtensive experiments demonstrate that NTK-Selector consistently outperforms existing data se-\nlection methods across multiple models and domains, with particularly strong gains in very low-\nresource settings. Our work establishes a new state-of-the-art in cross-domain data selection and\noffers deeper insights into the optimization dynamics of LLMs during fine-tuning.\n9\n"}, {"page": 10, "text": "REPRODUCIBILITY STATEMENT\nTo guarantee the reproducibility of this work, we provided theoretical proofs in Appendix A.3 and\nA.4, detailed experimental setup descriptions in Appendix A.5 and A.6, examples of query prompts\nin Appendix A.7, and cases of selected samples in Appendix A.8.\nREFERENCES\nZahra Rahimi Afzal, Tara Esmaeilbeig, Mojtaba Soltanalian, and Mesrob I Ohannessian. Can the\nspectrum of the neural tangent kernel anticipate fine-tuning performance? In Adaptive Foundation\nModels: Evolving AI for Personalized and Efficient Learning, 2024.\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparame-\nterized neural networks, going beyond two layers. Advances in neural information processing\nsystems, 32, 2019.\nAlain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and\nstatistics. Springer Science & Business Media, 2011.\nLouis B´ethune, David Grangier, Dan Busbridge, Eleonora Gualdoni, Pierre Ablin, et al. Scaling\nlaws for forgetting during finetuning with pretraining data injection. In Forty-second International\nConference on Machine Learning, 2025.\nYuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and\ndeep neural networks. Advances in neural information processing systems, 32, 2019.\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay\nSrinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data.\nIn The Twelfth International Conference on Learning Representations, 2024.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. Journal of Machine Learning Research, 24(240):\n1–113, 2023.\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-\nEmmanuel Mazar´e, Maria Lomeli, Lucas Hosseini, and Herv´e J´egou. The faiss library. arXiv\npreprint arXiv:2401.08281, 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv e-prints, pp. arXiv–2407, 2024.\nZiqing Fan, Siyuan Du, Shengchao Hu, Pingjie Wang, Li Shen, Ya Zhang, Dacheng Tao, and Yan-\nfeng Wang. Combatting dimensional collapse in llm pre-training data via submodular file selec-\ntion. In The Thirteenth International Conference on Learning Representations, 2025.\nYuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, and Minlie Huang. Data\nselection via optimal control for language models. In The Thirteenth International Conference on\nLearning Representations, 2025.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations, 2021.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\narXiv:2410.21276, 2024.\n10\n"}, {"page": 11, "text": "Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-\neralization in neural networks. Advances in neural information processing systems, 31, 2018.\nShuyang Jiang, Yusheng Liao, Ya Zhang, Yanfeng Wang, and Yu Wang. Taia: Large language\nmodels are out-of-distribution data learners. Advances in Neural Information Processing Systems,\n37:105200–105235, 2024.\nYiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, and J Zico Kolter. Adaptive data optimiza-\ntion: Dynamic sample selection with scaling laws. In The Thirteenth International Conference\non Learning Representations, 2025.\nWilliam B Johnson, Joram Lindenstrauss, et al. Extensions of lipschitz mappings into a hilbert\nspace. Contemporary mathematics, 26(189-206):1, 1984.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon\nSeo.\nThe cot collection: Improving zero-shot and few-shot learning of language models via\nchain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045, 2023.\nYuta Koreeda and Christopher D Manning. Contractnli: A dataset for document-level natural lan-\nguage inference for contracts.\nIn Findings of the Association for Computational Linguistics:\nEMNLP 2021, pp. 1907–1919, 2021.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the 29th symposium on operating systems princi-\nples, pp. 611–626, 2023.\nMing Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi\nZhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. In Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 14255–14273, 2024a.\nMing Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi\nZhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data\nselection for instruction tuning. In Proceedings of the 2024 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies (Vol-\nume 1: Long Papers), pp. 7595–7628, 2024b.\nLinlin Liu, Xingxuan Li, Megh Thakkar, Xin Li, Shafiq Joty, Luo Si, and Lidong Bing. Towards\nrobust low-resource fine-tuning with multi-view compressed representations. In The 61st Annual\nMeeting Of The Association For Computational Linguistics, 2023a.\nXiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha.\nFingpt: Democratizing\ninternet-scale data for financial large language models. arXiv preprint arXiv:2307.10485, 2023b.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nZifan Liu, Amin Karbasi, and Theodoros Rekatsinas. Tsds: Data selection for task-specific model\nfinetuning. Advances in Neural Information Processing Systems, 37:10117–10147, 2024.\nNeural Magic. Twitter financial news sentiment. https://huggingface.co/datasets/\nzeroshot/twitter-financialnews-sentiment, 2022.\nSadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based\nview of language model fine-tuning.\nIn International Conference on Machine Learning, pp.\n23610–23641. PMLR, 2023.\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. Good debt or bad\ndebt: Detecting semantic orientations in economic texts. Journal of the Association for Informa-\ntion Science and Technology, 65(4):782–796, 2014.\nOpenAI. ChatGPT, 2023. URL https://openai.com/blog/chatgpt.\n11\n"}, {"page": 12, "text": "Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical domain question answering. In Conference on\nhealth, inference, and learning, pp. 248–260. PMLR, 2022.\nXingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, and Shanbo Cheng. G-dig: To-\nwards gradient-based diverse and high-quality instruction data selection for machine translation.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 15395–15406, 2024.\nSung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:\nAttributing model behavior at scale.\nIn International Conference on Machine Learning, pp.\n27074–27113. PMLR, 2023.\nLev Semenovich Pontryagin. Mathematical theory of optimal processes. Routledge, 2018.\nAbhilasha Ravichander and W Alan. Question answering for privacy policies: Combining compu-\ntational and legal perspectives. In Empirical Methods in Natural Language Processing, 2019.\nAnkur Sinha and Tanmay Khandait. Impact of news on the commodity market: Dataset and results.\nIn Future of Information and Communication Conference, pp. 589–601. Springer, 2021.\nKushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretrain-\ning via document de-duplication and diversification. Advances in Neural Information Processing\nSystems, 36:53983–53995, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\nIn\nInternational Conference on Learning Representations, 2019.\nAlexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how\nreal-world neural representations generalize. In International conference on machine learning,\npp. 23549–23588. PMLR, 2022.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: se-\nlecting influential data for targeted instruction tuning. In Proceedings of the 41st International\nConference on Machine Learning, pp. 54104–54132, 2024.\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language\nmodels via importance resampling. Advances in Neural Information Processing Systems, 36:\n34201–34227, 2023.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al.\nQwen3 technical report.\narXiv preprint\narXiv:2505.09388, 2025.\nAndrew Yates, Arman Cohan, and Nazli Goharian. Depression and self-harm risk assessment in on-\nline forums. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pp. 2968–2978, 2017.\nJipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, and Tong Zhang. Tagcos: Task-\nagnostic gradient clustered coreset selection for instruction tuning data. In NAACL (Findings),\n2025.\nKaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li,\nGanqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou.\nUltramedical: Building specialized generalists in biomedicine, 2024.\n12\n"}, {"page": 13, "text": "CONTENTS\nA Appendix\n13\nA.1\nThe Use of Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.2\nRelated Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nA.3\nProof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.4\nDerivation of Jacobian-free NTK Approximation\n. . . . . . . . . . . . . . . . . .\n16\nA.5\nEmpirical NTK Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.5.1\nKernel ridge regression . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.5.2\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA.6\nExperimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA.6.1\nTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nA.6.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.6.3\nTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.6.4\nEvaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.7\nQuery Prompts\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.8\nCase Study of Selected Auxiliary Samples . . . . . . . . . . . . . . . . . . . . . .\n19\nA\nAPPENDIX\nA.1\nTHE USE OF LARGE LANGUAGE MODELS\nWe leveraged large language models as writing assistants for tasks such as rephrasing sentences,\nimproving grammatical flow, and refining technical descriptions for clarity.\nA.2\nRELATED WORKS\nTask-agnostic Data Selection.\nConventional data selection approaches are broadly classified into\ntwo main categories: task-agnostic and task-specific methods. Task-agnostic methods aim to curate\na high-quality and representative subset of data by filtering out low-quality and redundant samples,\nwithout a specific target task in mind. These methods can be further subdivided into three classes:\n(i) Quality-based approaches often utilize the training loss or other proxy metrics to score and select\ndata. For example, PDS (Gu et al., 2025) formulates data selection as an optimal control problem and\nsolves it using Pontryagin’s Maximum Principle theory (Pontryagin, 2018) to optimize the AUC un-\nder the training loss curve, while IFD (Li et al., 2024b) measures data quality by calculating the loss\ndifference when removing the instruction input. (ii) Diversity-based algorithms focus on reducing\ndataset redundancy. They measure data diversity using metrics such as feature distance (Tirumala\net al., 2023), eigenvalues (Fan et al., 2025), and gradient similarity (Zhang et al., 2025). (iii) Model-\nbased methods employ a proxy model, which can be a small model or a powerful large language\nmodel (e.g., ChatGPT (OpenAI, 2023)), to rate each candidate sample. Notable examples include\nAlpaGasus (Chen et al., 2024), SuperFiltering (Li et al., 2024a), and ADO (Jiang et al., 2025). A key\nlimitation of these task-agnostic approaches is their inability to provide targeted data augmentation\nfor a specific domain or downstream task, as they do not rely on a reference dataset to guide the\nselection process.\nTask-specific Data Selection.\nIn contrast to task-agnostic methods, task-specific approaches fo-\ncus on curating a subset of data from a large in-domain candidate pool specifically for a target task.\nThese methods often leverage sophisticated techniques to identify the most relevant data points. For\ninstance, LESS (Xia et al., 2024) utilizes a modified influence function tailored for the Adam opti-\nmizer, achieving high accuracy with as little as 5% of the full dataset. Similarly, G-DIG (Pan et al.,\n13\n"}, {"page": 14, "text": "2024), which is focused on machine translation, also adopts an influence function-based approach\nguided by a manually curated seed subset. Other methods formulate the problem as an optimization\ntask. TSDS (Liu et al., 2024) frames data selection for task-specific finetuning as an optimization\nproblem that minimizes the discrepancy between the selected data and the target distribution us-\ning an optimal transport-based distribution alignment loss. Alternatively, DSIR (Xie et al., 2023)\nbypasses the need for model features or gradients altogether. It instead constructs an importance\nweight estimator using n-gram features to evaluate the relevance of each candidate sample. While\nthese approaches have shown effectiveness, they all fundamentally rely on a substantial quantity\nof in-domain candidate pool and a reference validation set. They do not address scenarios with\nextremely limited resources or investigate how leveraging out-of-domain data could benefit model\ntraining and convergence.\nNeural Tangent Kernel.\nNeural Tangent Kernel (Jacot et al., 2018) is a foundational concept\ndemonstrating that training an neural network under certain parameterization is equivalent to per-\nforming kernel regression as the width of the network approaches infinity. This insight has become a\ncornerstone for understanding the generalization properties of deep networks. Following this semi-\nnal work, subsequent research (Allen-Zhu et al., 2019; Cao & Gu, 2019; Wei et al., 2022) has widely\napplied the NTK lens to analyze the optimization and generalization behavior of deep learning mod-\nels. However, this theoretical perspective has largely been confined to small, randomly initialized\nnetworks to satisfy the core assumptions of the NTK framework. There has been little investigation\ninto its application within the pre-training and fine-tuning paradigm, especially for large language\nmodels (LLMs). While some recent works, such as those by Malladi et al. (2023) and Afzal et al.\n(2024), have explored the kernel behavior of pre-trained language models (e.g., RoBERTa (Liu\net al., 2019)) on the GLUE benchmark (Wang et al., 2019), these studies are still limited to small-\nscale models and classification tasks. This is far from the real-world applications of modern LLMs.\nIn this paper, we present empirical evidence for stable NTK-like behavior in LLMs, and we leverage\nthis observation to design an NTK-based auxiliary data selection criterion that yields consistent en-\nhancement for low-resource domains. To our knowledge, this is the first systematic study of kernel\nbehavior in large language models under standard instruction tuning.\nA.3\nPROOF OF THEOREM 1\nTheorem 2. Given a model f(·; θt) with NTK Θ(·, ·; θt) whose training dynamics are governed by\na gradient flow ˙f(·; θt) = −ηΘ(·, ·; θt)γ(t), where γ(t) = ∇fL(f(·; θt)) and η is the learning rate,\nif it exhibits NTK-like behavior on [0, T], its training dynamics can be re-parameterized onto a new\ntime axis u:\n˙f(·; θt(u)) = −ηΘ(·, ·; θ0)γ(t(u)) + ∆(u),\n(6)\nwhere u(t) :=\nR t\n0 a∗(τ)dτ, a∗(t) = ⟨Θ(·,·;θt),Θ(·,·;θ0)⟩\n||Θ(·,·;θ0)||2\nand the perturbation term ∆(u) is bounded\nwith ||∆(u)|| ≤η||Θ(·, ·; θ0)||\n√\n2ϵ\n1−ϵ||γ(t(u))||.\nProof. For brevity, we let ˙f(t) := ˙f(·; θt), ˙f(u) := ˙f(·; θt(u)), Θ(t) := Θ(·, ·; θt) and Θ0 := Θ(0).\nWe decompose Θ(t) along span(Θ0) and its orthogonal complement:\nΘ(t) = a∗(t)Θ0 + R(t),\nwhere\na∗(t) = arg min\na∈R ||Θ(t) −aΘ0||.\nBy the orthogonality condition of least squares, we have:\na∗(t) = ⟨Θ(t), Θ0⟩\n∥Θ0∥2\nand\nR(t) = Θ(t) −a∗(t)Θ0.\nFrom the definition of Frobenius cosine similarity, we can obtain the similarity S(t) between Θ(t)\nand Θ0:\nS(t) := ⟨Θ(t), Θ0⟩\n∥Θ(t)∥∥Θ0∥,\nand by the Pythagorean theorem:\n∥Θ(t)∥2 = ∥a∗(t)Θ0∥2 + ∥R(t)∥2 = ⟨Θ(t), Θ0⟩2\n∥Θ0∥2\n+ ∥R(t)∥2,\n14\n"}, {"page": 15, "text": "we can derive the residual norm:\n∥R(t)∥2 = ∥Θ(t)∥2 −⟨Θ(t), Θ0⟩2\n∥Θ0∥2\n= ∥Θ(t)∥2(1 −S(t)2),\nso that\n∥R(t)∥= ∥Θ(t)∥\np\n1 −S(t)2.\nNow we define the scaled equivalent kernel Θeq(t) as:\nΘeq(t) :=\n1\na∗(t)Θ(t) = Θ0 + E(t),\nwhere\nE(t) = R(t)\na∗(t).\nUsing the identity:\na∗(t) = ∥Θ(t)∥\n∥Θ0∥S(t),\nwe bound the error term:\n∥E(t)∥= ∥R(t)∥\na∗(t) = ∥Θ0∥\np\n1 −S(t)2\nS(t)\n.\nSince S(t) ≥1 −ϵ and S(t)2 ≥1 −2ϵ + ϵ2, it follows that:\n∥E(t)∥≤∥Θ0∥\n√\n2ϵ −ϵ2\n1 −ϵ\n≤∥Θ0∥\n√\n2ϵ\n1 −ϵ.\nBased on the expansion of Θ(t), the gradient flow can be transformed as:\n˙f(t) = −ηΘ(t)γ(t) = −ηa∗(t)Θ0γ(t) −ηR(t)γ(t).\nHere we introduce the time reparameterization from t to u:\nu(t) =\nZ t\n0\na∗(τ)dτ\n⇒\ndu\ndt = a∗(t).\nThen, by the chain rule:\n˙f(u) = df/dt\ndu/dt = −ηΘ0γ(t(u)) −η R(t(u))\na∗(t(u))γ(t(u)) = −ηΘ0γ(t(u)) + ∆(u),\nwhere\n∆(u) = −η R(t(u))\na∗(t(u))γ(t(u)).\nUsing the spectral norm product bound, ∥E(t(u)) γ(t(u))∥≤∥E(t(u))∥op∥γ(t(u))∥, and the op-\nerator norm bound ∥· ∥op ≤∥· ∥, we obtain\n∥∆(u)∥≤η ∥E(t(u))∥op ∥γ(t(u))∥≤η ∥E(t(u))∥∥γ(t(u))∥≤η ∥Θ0∥\n√\n2ϵ\n1 −ϵ ∥γ(t(u))∥,\nwhere ∥· ∥denotes the Frobenius norm and ∥· ∥op is the spectral norm.\nThus, under the time variable u, the dynamics are equivalent to those of a fixed kernel Θ0 up to a\nperturbation ∆bounded as above, which completes the proof.\n15\n"}, {"page": 16, "text": "A.4\nDERIVATION OF JACOBIAN-FREE NTK APPROXIMATION\nProof. The derivation of Equation 5 is formulated as below:\neΘ(x, x′; θt) =\n*\n∇θt\nD\nX\nk=1\nfk(x; θt), ∇θt\nD\nX\nk=1\nfk(x′, θt)\n+\n,\n=\n* D\nX\nk=1\n∇θtfk(x; θt),\nD\nX\nk=1\n∇θtfk(x′, θt)\n+\n=\nD\nX\nk=1\nD\nX\nm=1\n⟨∇θtfk(x; θt), ∇θtfm(x′, θt)⟩\n=\nD\nX\nk=m=1\n⟨∇θtfk(x; θt), ∇θtfm(x′, θt)⟩+\nD\nX\nk=1\nD\nX\nm=1,m̸=k\n⟨∇θtfk(x; θt), ∇θtfm(x′, θt)⟩\n=\nD\nX\nk=1\n⟨∇θtfk(x; θt), ∇θtfk(x′; θt)⟩+\nD\nX\nk=1\nD\nX\nm=1,m̸=k\n⟨∇θtfk(x; θt), ∇θtfm(x′, θt)⟩.\nA.5\nEMPIRICAL NTK REGRESSION\nA.5.1\nKERNEL RIDGE REGRESSION\nGiven a model f(·; θ) with NTK Θ(·, ·; θ) ∈Rn×n on a dataset X = {xi, yi}, i = 1 . . . n, we can\nperform standard kernel ridge regression (KRR) to predict the unseen test sample. For brevity, the\nmodel parameter θt is omitted in this part. The objective of KRR is to find a function g within the\nReproducing Kernel Hilbert Space (RKHS) (Berlinet & Thomas-Agnan, 2011) HΘ that minimizes\na regularized squared error loss, which is formulated as\nmin\ng∈HΘ\n1\nn\nn\nX\ni=1\n(g(xi) −yi)2 + λ||g||2\nHΘ,\n(7)\nwhere λ > 0 is a user-defined regularization parameter. By the Representer Theorem, the optimal\nsolution g∗can be expressed as a linear combination of the kernel functions evaluated at the training\ndata points:\ng∗(x) =\nn\nX\ni=1\nαiΘ(x, xi).\n(8)\nSubstituting this representation into Equation (7) yields a convex optimization problem with respect\nto the coefficient vector α = [α1, . . . , αn]⊤. Therefore, Equation (7) can be rewritten as\nmin\nα\n1\nn||Θ(·, ·)α −y||2 + λα⊤Θα.\n(9)\nBy setting the gradient with respect to α to zero, we can obtain a closed-form solution as\nα = (Θ(·, ·) + nλI)−1y,\n(10)\nwhere I ∈Rn×n is the identity matrix. With the optimal coefficient vector α determined, we can\nobtain prediction ˆytest for a new, unseen test sample xtest by applying the learned linear combination\nto the kernel values between the test sample and the entire training set, which is formulated as\nˆytest = Θtest(xtest, ·)α,\n(11)\nwhere Θtest(xtest, ·) ∈R1×n is a vector containing the empirical NTK values between xtest and\neach of the training samples.\n16\n"}, {"page": 17, "text": "A.5.2\nIMPLEMENTATION\nDue to the intricate nature of LLM outputs, which involve generating a step-by-step reasoning path\nfollowed by a final answer, directly applying a kernel regression model to this process is challenging.\nTo address this, we simplify the problem by reframing it as a classification task. Specifically, for\neach task, we align the label sets of the training and test data to ensure class consistency. For\ninstance, in the FPB task, we unify the original seven sentiment classes (strong negative, moderately\nnegative, mildly negative, neutral, mildly positive, moderately positive, strong positive) from part of\nthe training set into three broader categories: negative, neutral, and positive. This re-labeling allows\nus to align the classes with the test set, enabling a direct comparison of performance.\nOur experiments are conducted by training the model on 1,000 domain samples and evaluating\nits performance on the corresponding test sets.\nWe compare the classification accuracy of our\nregression-based approach against standard fine-tuning methods. The optimal regularization param-\neter λ for our regression model is determined through a hyperparameter search over the range [1e-5,\n1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 1e-1]. It is important to note that this experimental setup could not\nbe applied to the medical question-answering tasks (MedMCQA and MMLU-Med), as their answer\noptions for individual instances are not consistent across the datasets. Consequently, direct label\nalignment for these tasks was not feasible. Therefore, we did not report results for these specific\ntasks in Table 3.\nA.6\nEXPERIMENTAL DETAILS\nA.6.1\nTASKS\nMedMCQA.\nMedical Multiple-Choice Question Answering (MedMCQA) (Pal et al., 2022) is a\nbenchmark for medical question answering, designed to address real-world medical entrance exam\nquestions. It comprises 2.4K healthcare topics across 21 medical disciplines, with each question\nfeaturing four answer choices. This task necessitates a sophisticated integration of semantic com-\nprehension, extensive factual recall, and advanced logical and causal reasoning. We train on Ultra-\nMedical (Zhang et al., 2024) and evaluate on the MedMCQA test set (4,183 samples).\nMMLU-Med.\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021)\nserves as a multitask language understanding benchmark consisting of 57 tasks. In this paper,\nwe exclusively adopt its medical-related subset (1,089 samples), MMLU-Med, which is sourced\nfrom various public and academic datasets, covering a wide range of medical subfields from clinical\nmedicine to professional ethics. Distinct from MedMCQA, this task is designed to assess the gen-\neralization and robust knowledge of a pre-trained model rather than its fine-tuned performance on a\nnarrow domain. We similarly leverage the UltraMedical (Zhang et al., 2024) corpus for training.\nFPB & TFNS.\nFinancial PhraseBank (FPB) (Malo et al., 2014) and Twitter Financial News Sen-\ntiment (TFNS) (Magic, 2022) constitute benchmarks for financial sentiment analysis, with FPB pro-\nviding sentence-level annotations and TFNS covering Twitter-based financial news. Models must\nclassify sentiment into positive, neutral, or negative, requiring robust semantic understanding of\ncontext-dependent financial language and fine-grained classification across formal documents and\ninformal social media. We employ their sentiment datasets for instruction tuning (Liu et al., 2023b)\nand evaluate on a random sample of 1,000 test instances.\nHeadline\n(Sinha & Khandait, 2021) is a fine-grained classification benchmark for commodity\nmarket news headlines. It comprises 11,412 human-annotated gold-related headlines spanning from\n2000 to 2019, requiring models to capture nuanced aspects including price direction, temporal con-\ntext, and asset comparisons. Unlike sentiment analysis, Headline necessitates precise classification\nof price movements through contextual cues and distinction between historical and forward-looking\ntrends, which enable the extraction of actionable insights for investors and policymakers. Consider-\ning the large quantity of the test set, we evaluate on a randomly sampled subset of 1,000 instances.\nContractNLI\n(Koreeda & Manning, 2021) is a benchmark for evaluating a model’s ability to\nperform document-level natural language inference (NLI) on contract texts to address the significant\ntime and cost associated with manual contract review. The model is given a contract and a set of\n17\n"}, {"page": 18, "text": "hypotheses, and for each hypothesis, it must classify whether it is entailed, contradictory, or neutral\nto the contract. This task is particularly challenging due to the unique linguistic properties of legal\ndocuments and the complexity of document-level reasoning. Similarly, we adopted a randomly\nsampled test set of 1,000 data points.\nPrivacyQA\n(Ravichander & Alan, 2019) assesses a model’s ability to answer questions based on\na privacy policy document. This task is framed as a question-answering problem where a model is\ngiven a question about a privacy policy and must provide a concise, factual answer. Building on this,\nit requires the model to not only read and understand a lengthy, complex legal document but also\nto locate specific information and synthesize a correct response. We use a random sample of 1,000\nexamples for held-out evaluation.\nRSDD.\nReddit Self-reported Depression Diagnosis (RSDD) (Yates et al., 2017) task is a bench-\nmark for evaluating a model’s capability to identify users with depression from their online forum\nlanguage alone. This task is framed as a user-level binary classification problem, where a system\nmust determine if a user, based on the corresponding posting history, has a self-reported depression\ndiagnosis or belongs to a matched control group. Beyond simple keyword matching, this task re-\nquires the model to capture nuanced linguistic and socio-linguistic patterns associated with mental\nhealth. As the dataset lacks predefined splits, we evaluate on a random sample of 1,000 instances\nand use the remainder for training.\nA.6.2\nBASELINES\nWe now detail the implementation of the data selection baselines. For random selection, a single\nauxiliary dataset is sampled once from the full candidate pool and shared across all tasks to ensure\nconsistency. For other task-specific selection approaches like DSIR1 (Xie et al., 2023), we consider\nthe domain dataset D as the high-quality targets and identify similar data from the candidate pool\nC. For LESS2 (Xia et al., 2024), due to the large scale of candidate pool C, we perform warm-up\ntraining on 0.1% (≈2K samples) of the candidate data. Following the original setup, Adam gradients\nare used for candidates and SGD gradients for domain samples. For TSDS3 (Liu et al., 2024), we\nadopt the recommended hyperparameters: scaling constant C = 5.0, diversity-alignment trade-off\ncoefficient α = 0.5, and prefetching/kernel density estimation neighborhood sizes K = 5000 and\nK = 1000, respectively.\nA.6.3\nTRAINING\nWe report the LoRA-based instruction tuning configuration in Table 5. The same training setup is\nconsistently applied across all data combinations generated via distinct selection strategies to ensure\nfair comparison. This LoRA configuration is also shared with the NTK selection stage.\n1https://github.com/p-lambda/dsir\n2https://github.com/princeton-nlp/LESS\n3https://github.com/ZifanL/TSDS\n18\n"}, {"page": 19, "text": "Table 5: Training details of LoRA-based instruction tuning.\nParameter Name\nValue\nLoRA α\n32\nLoRA r\n16\nLoRA dropout\n0.05\nLoRA target modules\nq proj, k proj, v proj, o proj, up proj, down proj, gate proj\nEpoch\n3\nWarm-up ratio\n0.03\nWeight decay\n0.0\nLearning rate\n5e-4\nLearning rate schedule\nCosine\nMax sequence length\n3072\nBatch size per device\n8\nGradient accumulation steps\n8\nPlatform\n4 NVIDIA A100 Tensor Core GPU\nA.6.4\nEVALUATION\nFor evaluation, we employ Chain-of-Thought (CoT) evaluation for each task. This approach first\ngenerates the reasoning process and subsequently produces the final answer by using the question\nconcatenated with the historical reasoning path as input. To facilitate a more efficient evaluation,\nwe leverage vLLM (Kwon et al., 2023) to accelerate the generation process. Additionally, drawing\nupon the observations of Jiang et al. (2024) to enhance tolerance to data mismatches, we adopt the\nTAIA (Jiang et al., 2024) evaluation strategy for medical and legal tasks when using mixed training\nwith auxiliary data.\nA.7\nQUERY PROMPTS\nFor the tasks as detailed in Appendix A.6.1, the corresponding instruction datasets often contain\nvery brief responses that lack a reasoning process, such as single-character choices (e.g., “A”, “B”,\n“C”) or several words. This brevity limits a model’s ability to learn complex reasoning chains and\ngenerate detailed explanations crucial for improving task accuracy. To guide the generation process,\nwe designed a specific prompt template that incorporates the original instruction dataset. We then\nused this template to query GPT-4o-mini (Hurst et al., 2024), as demonstrated in Figures 6, 7, 8, 9,\nand 10. Each template comprises three main components: a persona-defining system prompt (e.g.,\n“You are a financial analyst...”), a multi-step instructional prompt outlining the required analysis\n(e.g., “Analyze the sentiment...”), and placeholders for the original instruction and right answer. By\nleveraging these structured prompts, we consistently generated outputs that adhered to a predeter-\nmined format and analytical style.\nA.8\nCASE STUDY OF SELECTED AUXILIARY SAMPLES\nIn this part, we demonstrate examples of the domain sample and the selected auxiliary sample by\nNTK-Selector for each task as shown in Figures 11, 12, and 13, showcasing their shared topics,\nreasoning patterns, or inherent capabilities. For instance, in the ContractNLI task, although the\ndomain sample’s topic (non-disclosure agreements) and the auxiliary sample’s topic (constitutional\nissues) differ, both are textual entailment tasks and require a similar reasoning pattern to determine\nthe relationship between the premise and the hypothesis. This example highlights that our selec-\ntor effectively identifies samples that share underlying logical structures, even when their semantic\ncontent varies.\n19\n"}, {"page": 20, "text": "【Input】\n{news_text}: Teollisuuden Voima Oyj , the Finnish utility known as TVO , said it shortlisted Mitsubishi Heavy s EU-APWR\nmodel along with reactors from Areva , Toshiba Corp. , GE Hitachi Nuclear Energy and Korea Hydro & Nuclear Power Co.\n{question_text}: What is the sentiment of this news?\\nA.negative\\nB.neutral\\nC.positive\n{answer_text}: B\n【Instruction】\n{instruction_text}: {news_text}\\n{question_text}\\n\n【System Prompt】\nYou are a financial analyst with expertise in sentiment analysis.\n【User Prompt】\nAnalyze the sentiment of this financial news article by following these steps:\n1. Identify key financial terms and market indicators\n2. Examine contextual clues and narrative tone\n3. Assess potential market implications\n4. Determine overall sentiment\nBegin with \"Let's analyze this financial news step by step:\" and conclude with \"Therefore, the sentiment is [original_answer]\".\nKeep the analysis professional and finance-focused.\nInstruction: {instruction_text}\nAnswer: {answer_text}\n【Generated Answer】\nLet's analyze this financial news step by step:\\n\\n1. **Identify key financial terms and market indicators**: The article\nmentions Teollisuuden Voima Oyj (TVO), a Finnish utility, and highlights its shortlist of reactor models from notable companies\nsuch as Mitsubishi Heavy, Areva, Toshiba Corp., GE Hitachi Nuclear Energy, and Korea Hydro & Nuclear Power Co. Key terms\ninclude \\\"shortlisted,\\\" \\\"reactors,\\\" and the names of established companies in the nuclear energy sector.\\n\\n2. **Examine\ncontextual clues and narrative tone**: The tone of the article is factual and straightforward. It reports on the actions of TVO without\nexpressing any overtly positive or negative opinions. The use of terms like \\\"shortlisted\\\" suggests a selection process but does not\nindicate a definitive outcome or preference, which contributes to a neutral tone.\\n\\n3. **Assess potential market implications**: The\nnews indicates that TVO is in the process of evaluating options for nuclear reactors, which could have implications for the\ncompanies mentioned. However, without further information on the decision-making process or the criteria for selection, it is\ndifficult to ascertain any immediate market impact. The neutrality of the announcement suggests that it may not significantly affect\nstock prices or investor sentiment at this stage.\\n\\n4. **Determine overall sentiment**: Given the factual nature of the report, the\nabsence of strong language indicating optimism or pessimism, and the focus on the shortlist of options rather than a decisive action,\nthe overall sentiment can be classified as neutral.\\n\\nTherefore, the sentiment is B.neutral.\\nTherefore, the answer is B.\nFPB & TFNS\nFigure 6: Query prompt and example of generated answer for financial sentiment analysis (FPB and\nTFNS) tasks.\n20\n"}, {"page": 21, "text": "【Input】\n{headline_text}: london shakes up gold hoarders' list\n{answer_text}: B\n{question_text}:Does the news headline talk about price going down?\\nA.Yes\\nB.No\n【Instruction】\n{instruction_text}: {headline_text}\\n{question_text}\\n\n【System Prompt】\nYou are a financial analyst with expertise in news headline classification.\n【User Prompt】\nAnalyze whether this financial news headline discusses a price decline by following these steps:\n        1. Identify key terms related to price movement or market changes\n        2. Examine the context and implied action in the headline\n        3. Consider the entities involved and their potential reactions\n        4. Determine if there is a direct or implied mention of prices going down\n        Begin with \"Let's analyze this financial news step by step:\" and conclude with \"Therefore, the answer is [original_answer].\"\nKeep the analysis professional and finance-focused.\n        Instruction: {instruction_text}\n        Answer: {answer_text}\n【Generated Answer】\nLet's analyze this financial news step by step:\\n\\n1. **Identify key terms related to price movement or market changes**: The\nheadline \\\"london shakes up gold hoarders' list\\\" does not contain any explicit terms related to price movement, such as \\\"decline,\\\"\n\\\"drop,\\\" \\\"fall,\\\" or \\\"decrease.\\\" Instead, it suggests a change in the ranking or status of gold hoarders, which does not inherently\nimply a price movement.\\n\\n2. **Examine the context and implied action in the headline**: The phrase \\\"shakes up\\\" indicates a\ndisruption or change in the status quo regarding gold hoarders. However, it does not provide any specific information about the\nprices of gold or any related commodities. The context seems to focus more on the organizational or ranking aspect rather than\nmarket pricing.\\n\\n3. **Consider the entities involved and their potential reactions**: The headline mentions \\\"gold hoarders,\\\"\nwhich could refer to individuals or institutions that hold significant amounts of gold. While changes in their rankings may affect\nmarket perceptions or strategies, the headline does not suggest that these changes are linked to a decline in gold prices or any\nfinancial losses.\\n\\n4. **Determine if there is a direct or implied mention of prices going down**: There is no direct or implied\nmention of prices going down in the headline. The focus is on the list of gold hoarders rather than on the price of gold itself. Without\nany indication of price movements, we cannot conclude that the headline discusses a price decline.\\n\\nTherefore, the answer is B.\nHeadline\nFigure 7: Query prompt and example of generated answer for financial headline classification (Head-\nline) task.\n21\n"}, {"page": 22, "text": "【Input】\n{premise_text}: The aforesaid obligations of the receiving party shall survive the termination of this Agreement. 8. LIMITED\nWARRANTY Each party hereto warrants that it has the right to disclose the Proprietary/Confidential Information which it discloses to\nthe other parties and that the Proprietary/Confidential Information disclosed is to the best of its knowledge, correct.  Nothing\ncontained in this agreement shall be construed to obligate any party to disclose any information to the other parties. 9. REMEDY FOR\nBREACH It is understood and agreed between the parties that any breach of the obligations of confidentiality contained in this\nAgreement may cause the disclosing party irreparable loss.  Accordingly, and in addition to any other remedies a party may have in\nlaw or equity, the disclosing party shall be entitled to obtain injunctive relief against the receiving party to prevent any further or\ncontinuing breach of the receiving party's obligations or additional damage to the disclosing party in the event such loss is in fact\nincurred by the disclosing party as a result of the breach or is imminent. 10. SEVERABILITY If, for any reason, a court of competent\njurisdiction finds any provision of this Agreement, or any portion thereof, to be unenforceable, such decision shall not affect the\nvalidity of the remaining portion, which remaining portion shall continue in full force and effect as if this Agreement had been\nexecuted with the invalid portion thereof eliminated therefrom. In the event that a portion of this Agreement shall be declared to be\ninvalid, then the parties agree, that they shall, in good faith, negotiate with one another to replace such invalid provision with a valid\nprovision as similar as possible to that which had been said to be invalid. 11. TERMINATION This Agreement shall govern all matters\nreferred to herein until terminated by either party upon thirty days written notice to the other or in accordance with this Agreement. \nUpon termination, all information and materials shall be returned to the respective parties.  Notwithstanding the termination, each\nparty shall continue to fulfill its obligations hereunder for a period of five (5) years thereafter.\n{hypothesis_text}: Some obligations of Agreement may survive termination of Agreement.\n{question_text}: Given the following clause from a Non-Disclosure Agreement (premise), determine whether the hypothesis is\nentailed by the premise, contradicts it, or is unrelated.\\nA. entailment\\nB. contradiction\\nC. neutral\\n\n{answer_text}: A\n【Instruction】\n{question_text}\\nPremise:\\n{premise_text}\\nHypothesis:\\n{hypothesis_text}\n【System Prompt】\nYou are a legal reasoning assistant with expertise in regulatory analysis and corporate law.\n【User Prompt】\nAnalyze the logical relationship between the contract clause and the hypothesis by following these steps:\n        1. Identify key obligations, permissions, and conditions in the clause\n        2. Examine the scope and intent of the hypothesis\n        3. Compare the language and meaning of both texts\n        4. Determine whether the hypothesis is entailment, contradiction, or neutral\n        \n        Begin with \"Let's analyze this legal text step by step:\" and conclude with \"Therefore, the answer is\". Keep the analysis\nprofessional and law-focused.\n        Please make sure the result generated by the chain-of-thought leads to the correct answer, as if you are seeing this case for the\nfirst time.\nInstruction: {instruction_text}\n        Answer: {answer_text}\n【Generated Answer】\nLet's analyze this legal text step by step:\\n\\n1. **Identify key obligations, permissions, and conditions in the clause**:\\n   - The\nobligations of the receiving party regarding confidentiality survive the termination of the Agreement.\\n   - Each party warrants that it\nhas the right to disclose the Proprietary/Confidential Information and that such information is correct to the best of its knowledge.\\n   -\nNo party is obligated to disclose any information to the other parties.\\n   - Breach of confidentiality may cause irreparable loss to the\ndisclosing party, allowing for injunctive relief.\\n   - If any provision is found unenforceable, the remaining provisions remain valid,\nand the parties will negotiate a replacement.\\n   - The Agreement can be terminated by either party with thirty days written notice, and\nupon termination, all information must be returned. Obligations continue for five years post-termination.\\n\\n2. **Examine the scope\nand intent of the hypothesis**:\\n   - The hypothesis likely posits a scenario regarding the enforceability of the confidentiality\nobligations or the consequences of a breach of such obligations.\\n\\n3. **Compare the language and meaning of both texts**:\\n   - The\nclause explicitly states that confidentiality obligations survive termination, indicating a strong intent to protect proprietary information\neven after the Agreement ends.\\n   - The warranty of rights to disclose information and the acknowledgment of potential irreparable\nloss suggest that the parties recognize the importance of confidentiality and the potential consequences of breaches.\\n\\n4. **Determine\nwhether the hypothesis is entailment, contradiction, or neutral**:\\n   - If the hypothesis suggests that the confidentiality obligations are\nenforceable and that breaches can lead to significant consequences (like injunctive relief), it aligns with the language of the clause,\nindicating that it is an entailment.\\n   - If the hypothesis suggests that confidentiality obligations do not survive termination or that\nbreaches do not have serious consequences, it would contradict the clause.\\n\\nGiven the analysis, if the hypothesis supports the\nenforceability of confidentiality obligations and acknowledges the potential for irreparable harm due to breaches, it is consistent with\nthe clause. \\n\\nTherefore, the answer is A.\\n\nContractNLI\nFigure 8: Query prompt and example of generated answer for legal natural language inference\n(ContractNLI) task.\n22\n"}, {"page": 23, "text": "【Input】\n{clause_text}: Other times, we combine Usage Data collected from Cookies with that of other customers to improve your and\nother customers' experience.\n{question_text}: have you had any data breaches in the last couple of years?\n{answer_text}: A\n【Instruction】\n{instruction_text}: Classify if the clause is relevant to answering the question.\\nA. Irrelevant\\nB. Relevant\\n\\nClause:\n{clause_text}\\n{question_text}\\n\n【System Prompt】\nYou are a legal expert with expertise in privacy policy analysis and question answering.\n【User Prompt】\nAnalyze the policy text by following these steps:\n        1. Identify key obligations, permissions, and conditions mentioned in the policy segment. What does the policy explicitly state\nabout user data? Ensure your analysis aligns with the correct answer provided at the end.\n        2. Examine the scope and intent of the user question. What specific information is the user seeking? Cross-check your\ninterpretation against the correct answer to ensure consistency.\n        3. Compare the language and meaning of both the policy segment and the question. Are there any direct or indirect references\nin the policy that address the question's intent? Verify that your comparison matches the reasoning in the correct answer.\n        4. Based on your comparison, determine if the policy segment provides enough information to answer the question. Is it\ndirectly related, indirectly related, or unrelated? Finalize your conclusion **only after confirming it matches the correct answer**.\n        \n        Begin with \"Let's analyze this legal text step by step:\" and conclude with \"Therefore, the answer is [original_answer]\". Keep\nthe analysis professional and law-focused.\n        \n        Instruction: {instruction_text}\n        Answer: {answer_text}\n【Generated Answer】\nLet's analyze this legal text step by step:\\n\\n1. **Identify key obligations, permissions, and conditions mentioned in the policy\nsegment.** \\n   The policy segment states that the company combines Usage Data collected from Cookies with that of other\ncustomers to enhance the experience for both the user and other customers. This indicates a permission to use data for improvement\npurposes but does not specify any obligations regarding data security or breaches.\\n\\n2. **Examine the scope and intent of the user\nquestion.** \\n   The user is specifically inquiring about data breaches that may have occurred in the last couple of years. This\nquestion seeks information about the company's data security practices and any incidents that may have compromised user\ndata.\\n\\n3. **Compare the language and meaning of both the policy segment and the question.** \\n   The policy segment discusses\nthe use of Usage Data and Cookies for improving user experience but does not mention anything about data breaches, security\nincidents, or the handling of data in the context of breaches. There are no direct or indirect references in the policy that address the\nuser's question about data breaches.\\n\\n4. **Determine if the policy segment provides enough information to answer the question.**\n\\n   The policy segment is unrelated to the question about data breaches. It does not provide any information regarding the\noccurrence of breaches or the company's security practices. Therefore, it does not help in answering the user's inquiry.\\n\\nTherefore,\nthe answer is A. Irrelevant.\\n\nPrivacyQA\nFigure 9: Query prompt and example of generated answer for privacy policy-based question-\nanswering (PrivacyQA) task.\n23\n"}, {"page": 24, "text": "【Input】\n{post_text}: hi r anxiety i m a 0yr old male with what i would say is a decent amount of stress but nothing out of the norm yeah\nwork is stressful rent is increasing and a proposal to my gf are all looming but i ve always been able to accept those thing will come\nwith some mild stress since i m human i ve been experiencing what i can only describe a mild severe panic attack recently that are\ncompletely wiping me off the face of the earth this past saturday i found myself in a bathroom stall at noon after being out with\nfriend for lunch drink experiencing symptom last night after work i came home wa slapped with tunnel vision trembling tight chest\nand once i laid down in bed had what i would describe a the worst experience of my life for 0 minute borderline exorcism i m\ncompletely new to this experience and am obviously concerned what my trigger are or where this is coming from i plan on having a\nwellness check with my local physician but are there buzzword or thing i should include so i can do exactly explain what i am\nexperiencing i constantly drink water i take only about 00mg caffeine a day i exercise time a week i have what i would describe a a\npretty well balanced diet why is this happening all of a sudden appreciate any feedback this is more of a vent\n{question_text}: Classify whether a user has depression based on their Reddit posts and choose an option:\\nA. Yes\\nB. No\n{answer_text}: A\n【Instruction】\n{instruction_text}: {question_text}\\nUser post: {post_text}\n【System Prompt】\nYou are a clinical psychologist with expertise in mood disorders and digital behavioral analysis.\n【User Prompt】\nAnalyze the post by following these steps:\n        1. Identify key phrases indicating depressive symptoms or mental state\n        2. Examine behavioral patterns described (sleep habits, avoidance, etc.)\n        3. Evaluate expressed emotions and cognitive patterns\n        4. Consider diagnostic criteria for depression (DSM-5/ICD-10)\n        5. Determine if evidence supports depression classification\n        Begin with \"Let's analyze this Reddit post step by step:\" and conclude with \"Therefore, the answer is [original_answer]\". Keep\nthe analysis clinically focused and evidence-based.\n        Please make sure the result generated by the chain-of-thought leads to the given correct answer.\n        Instruction: {instruction_text}\n        Answer: {answer_text}\n【Generated Answer】\nLet's analyze this Reddit post step by step:\\n\\n1. **Identify key phrases indicating depressive symptoms or mental state**: The\nuser describes experiencing \\\"mild severe panic attacks,\\\" \\\"tunnel vision,\\\" \\\"trembling,\\\" and a \\\"tight chest.\\\" Phrases like\n\\\"completely wiping me off the face of the earth\\\" and \\\"the worst experience of my life\\\" suggest a significant level of distress. The\nmention of being \\\"obviously concerned\\\" about the triggers indicates a heightened level of anxiety and potential hopelessness.\\n\\n2.\n**Examine behavioral patterns described (sleep habits, avoidance, etc.)**: The user does not explicitly mention sleep habits, but the\ncontext implies that the panic attacks are affecting their overall functioning. The user describes a situation where they had to retreat\nto a bathroom stall, suggesting avoidance behavior in response to overwhelming anxiety. This indicates a disruption in daily\nactivities and social interactions.\\n\\n3. **Evaluate expressed emotions and cognitive patterns**: The user expresses feelings of fear\nand confusion about their sudden panic attacks. They mention feeling \\\"completely new to this experience,\\\" which indicates a lack\nof coping strategies for managing their anxiety. The cognitive pattern appears to be focused on worry about the future (work stress,\nrent increase, proposal), which can exacerbate feelings of anxiety and helplessness.\\n\\n4. **Consider diagnostic criteria for\ndepression (DSM-5/ICD-10)**: According to the DSM-5, symptoms of depression can include persistent sadness, loss of interest or\npleasure, feelings of worthlessness, and difficulty concentrating. While the user primarily describes anxiety symptoms, the distress\nand functional impairment they report could indicate a comorbid depressive episode, especially given the overwhelming nature of\ntheir panic attacks.\\n\\n5. **Determine if evidence supports depression classification**: The user's description of their experiences\nsuggests significant distress and impairment in functioning, which aligns with depressive symptoms. The combination of anxiety\nand the emotional turmoil expressed could support a diagnosis of depression, particularly if these symptoms persist over\ntime.\\n\\nTherefore, the answer is A.\\n\nRSDD\nFigure 10: Query prompt and example of generated answer for depression diagnosis (RSDD) task.\n24\n"}, {"page": 25, "text": "【Domain Sample】\n{instruction}: In patients with pancreas divisum, symptomatic obstructive pancreatopathy, and contraindications for immediate\ndefinitive surgery, which intervention is considered a bridge therapy to alleviate symptoms while awaiting a future surgical or\nendoscopic resolution?\\nA. Lesser papilla sphincteroplasty\\nB. Endoscopic stenting through the lesser papilla\\nC. Decompressive\noperation\\nD. Total pancreatectomy\\n\n{answer}: Pancreas divisum is a common congenital anomaly where the ductal systems of the dorsal and ventral pancreas fail\nto fuse during embryonic development. This can lead to a scenario where the majority of the pancreatic secretions must flow\nthrough the smaller duct of Santorini and the minor papilla, rather than through the duct of Wirsung and the major papilla. In some\npatients, this can cause recurrent episodes of pancreatitis due to the difficulty in draining pancreatic secretions.\\n\\nWhen patients\nwith pancreas divisum present with symptomatic obstructive pancreatopathy and are not immediate candidates for definitive\nsurgery, the goal is to relieve the obstruction and alleviate symptoms until a more permanent solution can be achieved.\\n\\nA. Lesser\npapilla sphincteroplasty is a surgical procedure that involves cutting the sphincter muscle to allow better drainage of the pancreatic\nduct. However, this is a form of definitive surgery and may not be indicated if there are contraindications for immediate\nsurgery.\\n\\nB. Endoscopic stenting through the lesser papilla involves placing a stent endoscopically to help drain the pancreatic\nduct. This is less invasive than surgery and can be used as a temporary measure to relieve symptoms.\\n\\nC. Decompressive\noperation is a surgical intervention aimed at relieving pressure or obstruction, but again, this is a form of definitive surgery and may\nnot be suitable if surgery is contraindicated at the moment.\\n\\nD. Total pancreatectomy is the complete surgical removal of the\npancreas and is a definitive and radical treatment for chronic pancreatitis or other severe pancreatic diseases. It is not a bridge\ntherapy and is associated with significant morbidity, including the development of diabetes mellitus.\\n\\nGiven the need for a bridge\ntherapy that is less invasive and can alleviate symptoms while awaiting future definitive resolution, endoscopic stenting through the\nlesser papilla (B) is the most appropriate intervention among the options listed. It can provide symptomatic relief and is reversible,\nserving as a bridge until the patient is fit for surgery or another definitive endoscopic treatment.\\n\\nSo, the answer is B.\n【Selected Auxiliary Sample】\n{instruction}: What is the most common symptom of MI?\\n\\nIs The most common symptom is chest pain or discomfort which\nmay travel into the shoulder, arm, back, neck, or jaw. a good answer to this question?\\n\\nOPTIONS:\\n- yes\\n- no\\n\n{answer}: The statement provided (\\\"the most common symptom is chest pain or discomfort which may travel into the\nshoulder, arm, back, neck, or jaw\\\") accurately answers the question \\\"What is the most common symptom of MI?\\\" So, in this case\nthe answer would be \\\"yes\\\".\n【Domain Sample】\n{instruction}: Starbucks says the workers violated safety policies while workers said they'd never heard of the policy before\nand are alleging retaliation.\\nWhat is the sentiment of this news?\\nA.strong negative\\nB.moderately negative\\nC.mildly\nnegative\\nD.neutral\\nE.mildly positive\\nF.moderately positive\\nG.strong positive\\n\\n\n{answer}: Let's analyze this financial news step by step:\\n\\n1. **Identify key financial terms and market indicators**: The\narticle mentions \\\"Starbucks,\\\" which is a publicly traded company, and refers to \\\"workers\\\" and \\\"safety policies.\\\" The mention of\nsafety policies and allegations of retaliation can indicate potential labor disputes, which may affect the company's operational\nefficiency and public perception.\\n\\n2. **Examine contextual clues and narrative tone**: The tone of the article appears to be\ncontentious, highlighting a conflict between the company and its workers. The phrase \\\"violated safety policies\\\" suggests a serious\nissue, while the workers' claim that they were unaware of these policies introduces a sense of confusion and potential\nmiscommunication. The mention of \\\"retaliation\\\" adds a layer of severity to the situation, indicating possible legal\nimplications.\\n\\n3. **Assess potential market implications**: Labor disputes and allegations of retaliation can lead to negative\npublicity for Starbucks, potentially affecting its stock price and investor sentiment. If the situation escalates, it could result in\noperational disruptions or increased costs related to legal proceedings and public relations efforts. Such factors may deter investors\nand impact the company's market performance.\\n\\n4. **Determine overall sentiment**: Considering the contentious nature of the\narticle, the serious implications of the allegations, and the potential negative impact on Starbucks' reputation and operations, the\noverall sentiment can be characterized as moderately negative.\\n\\nTherefore, the sentiment is B. moderately negative.\\nTherefore,\nthe answer is B.\n【Selected Auxiliary Sample】\n{instruction}: Given a piece of financial news and its polarity, classify it into 'true' if the polarity is correct and classify into\n'false' if the polarity is incorrect. Output must be 'true' or 'false'. \\n\\nnews:The acquisition price was not disclosed\n.\\npolarity:neutral\\n\n{answer}: The polarity of the news is neutral. This means that the news does not convey any positive or negative sentiment\ntowards the subject. In this example, the financial news is about an acquisition for which no price has been disclosed, and hence it\ndoes not convey any sentiment towards that acquisition. So, this financial news can be classified as \\\"true\\\".\nMedMCQA & MMLU-Med\nFPB & TFNS\nFigure 11: Example of domain sample and selected auxiliary sample of medical question answering\n(MedMCQA and MMLU-Med) and financial sentiment analysis (FPB and TFNS) tasks.\n25\n"}, {"page": 26, "text": "【Domain Sample】\n{instruction}: Gold prices settle lower after two-session gain\\nDoes the news headline talk about price staying constant?\n\\nA.Yes\\nB.No\\n\n{answer}: Let's analyze this financial news step by step:\\n\\n1. **Identify key terms related to price movement or market\nchanges**: The headline contains the term \\\"settle lower,\\\" which directly indicates a decrease in price. Additionally, \\\"two-session\ngain\\\" suggests that there was a previous increase in price before the current decline.\\n\\n2. **Examine the context and implied\naction in the headline**: The phrase \\\"settle lower\\\" implies that the price of gold has decreased after experiencing gains in the\nprevious sessions. This indicates a reversal in price movement, suggesting that the market has reacted negatively after a period of\nincrease.\\n\\n3. **Consider the entities involved and their potential reactions**: The entities involved here are likely investors and\ntraders in the gold market. A decline in gold prices may lead to reactions such as selling off positions, reevaluating investment\nstrategies, or increased market volatility as traders respond to the new price level.\\n\\n4. **Determine if there is a direct or implied\nmention of prices going down**: The headline explicitly states that gold prices have \\\"settled lower,\\\" which is a direct indication of\na price decline. There is no ambiguity in this phrasing; it clearly communicates that prices have decreased.\\n\\nTherefore, the answer\nis B.\n【Selected Auxiliary Sample】\n{instruction}: Classify the given a piece of financial news into three classes: positive, negative, and neutral. Output must be\n'positive', 'negative', or 'neutral'.\\n\\nSubscription sales decreased slightly .\\n\n{answer}: The financial news states that subscription sales have decreased. This indicates a drop in sales, which is generally\nconsidered as a negative development. So, this financial news can be classified as \\\"negative\\\".\\n\\\"\\\"\n【Domain Sample】\n{instruction}: Given the following clause from a Non-Disclosure Agreement (premise), determine whether the hypothesis is\nentailed by the premise, contradicts it, or is unrelated.\\nA. entailment\\nB. contradiction\\nC. neutral\\n\\nPremise:\\nEach party agrees\nthat all Confidential Information of the other party, including any copies thereof, shall be returned to the disclosing party upon\nrequest or destroyed within ten (10) business days of the expiration or other termination of the Project.  However, notwithstanding\nanything to the contrary herein,  1) neither party will be required to delete electronic Confidential Information stored in back-\nup/archival storage in accordance with its policies, provided that any such retained Confidential Information will continue to be\nsubject to the terms of this Confidentiality Agreement until it is destroyed by the receiving party; and  2) a single copy of all\nConfidential Information may be retained by the Receiving Party\\u2019s legal department for dispute resolution purposes only,\nprovided, however, that such Confidential Information shall remain subject to the terms and conditions of this Confidentiality\nAgreement.  In the event that a receiving party has or acquires actual knowledge of any breach of the confidentiality of, or the\nmisappropriation of, any Confidential Information received under this Confidentiality Agreement, such party shall promptly give\nnotice thereof to the other party. 5. Upon written demand by the disclosing party or upon termination of this Confidentiality\nAgreement, the receiving party shall:  (i) cease using the Confidential Information,  (ii) return the Confidential Information and all\ncopies, notes or extracts thereof to the disclosing party within ten (10) business days of receipt of demand; and \\nHypothesis:\\nAll\nConfidential Information shall be expressly identified by the Disclosing Party.\\n\n{answer}: Let's analyze this legal text step by step:\\n\\n1. **Identify key obligations, permissions, and conditions in the\nclause**:\\n   - Both parties are required to return or destroy all Confidential Information upon request or within ten business days\nafter the termination of the Project.\\n   - There are exceptions to this obligation:\\n     - Electronic Confidential Information stored in\nback-up/archival storage does not need to be deleted immediately, but it must remain subject to the confidentiality terms until\ndestroyed.\\n     - A single copy of Confidential Information may be retained by the Receiving Party\\u2019s legal department solely\nfor dispute resolution purposes, with the condition that it remains subject to the confidentiality terms.\\n   - If either party becomes\naware of a breach of confidentiality, they must promptly notify the other party.\\n   - Upon written demand or termination of the\nConfidentiality Agreement, the receiving party must cease using the Confidential Information and return it within ten business\ndays.\\n\\n2. **Examine the scope and intent of the hypothesis**:\\n   - The hypothesis likely posits a scenario regarding the\nobligations of the parties concerning the handling of Confidential Information, particularly in the context of breaches or the return of\ninformation.\\n\\n3. **Compare the language and meaning of both texts**:\\n   - The clause outlines specific obligations regarding the\nreturn and destruction of Confidential Information, as well as the conditions under which certain information may be retained. The\nlanguage is clear about the responsibilities of both parties and the exceptions that apply.\\n   - If the hypothesis suggests that one\nparty can disregard these obligations or that the confidentiality terms do not apply to retained information, it would contradict the\nexplicit terms of the clause.\\n\\n4. **Determine whether the hypothesis is entailment, contradiction, or neutral**:\\n   - If the\nhypothesis aligns with the obligations and exceptions outlined in the clause, it would be considered entailment. If it suggests that the\nobligations can be ignored or that the confidentiality terms do not apply, it would be a contradiction. If it presents a scenario that\ndoes not clearly align with the obligations or exceptions, it would be neutral.\\n\\nGiven the analysis, if the hypothesis contradicts the\nobligations and conditions set forth in the clause regarding the handling of Confidential Information, then it is a\ncontradiction.\\n\\nTherefore, the answer is C.\\n\n【Selected Auxiliary Sample】\n{instruction}: Premise: The court found it unconstitutional.\\n\\nHypothesis: It was found to be unconstitutional by the\ncourt.\\n\\nDoes the premise entail the hypothesis?\\n\\nOPTIONS:\\n- yes\\n- it is not possible to tell\\n- no\\n\n{answer}: The premise specifically states that the court found it unconstitutional. This implies that what \\\"it\\\" refers to was\ndeclared unconstitutional by the court, which is exactly what is stated in the hypothesis. Hence, we can conclude that this statement\nentails the hypothesis and so choose option A: yes\nHeadline\nContractNLI\nFigure 12: Example of domain sample and selected auxiliary sample of financial headline classifi-\ncation (Headline) and contract natural language inference (ContractNLI) tasks.\n26\n"}, {"page": 27, "text": "【Domain Sample】\n{instruction}: Classify if the clause is relevant to answering the question.\\nA. Irrelevant\\nB. Relevant\\n\\nClause: For personal\ndata subject to the European Union General Data Processing Regulations (GDPR), we rely on several legal bases to process the\ndata.\\nQuestion: can any one access my private information collected by the app?\\n\n{answer}: Let's analyze this legal text step by step:\\n\\n1. **Identify key obligations, permissions, and conditions mentioned in\nthe policy segment.** The policy states that for personal data governed by the GDPR, the organization relies on several legal bases\nto process that data. This implies that there are specific legal frameworks and conditions under which user data can be processed, but\nit does not explicitly address who can access the data or under what circumstances.\\n\\n2. **Examine the scope and intent of the user\nquestion.** The user is seeking information about whether anyone can access their private information collected by the app. This\nquestion is focused on the accessibility of personal data and who has the right to access it, which is not directly addressed in the\nprovided policy segment.\\n\\n3. **Compare the language and meaning of both the policy segment and the question.** The policy\nsegment discusses the legal bases for processing personal data under GDPR but does not mention access rights or who may access\nthe data. The user question specifically inquires about access to private information, which is not covered in the policy text.\nTherefore, there is no direct or indirect reference in the policy that addresses the user's concern about access to their private\ninformation.\\n\\n4. **Determine if the policy segment provides enough information to answer the question.** Given that the policy\ndoes not mention anything about access to private information or who can access it, it is clear that the clause is unrelated to the\nuser's question. \\n\\nTherefore, the answer is A. Irrelevant.\\n\n【Selected Auxiliary Sample】\n{instruction}: In this task, you are given a statement spoken by a politician in natural language and the subject of the discussion\nfor the given statement. Your task is to determine if the given subject of the discussion is correct for the given statement. The subject\ncan be multiple words. Answer with \\\"True\\\" if it's the correct subject of discussion or return \\\"False\\\" if it's incorrect.\\n\\nStatement:\nOn banning assault weapons. \\n Subject: guns\\n\n{answer}: The subject of discussion in the statement is guns and we can see that the politician is talking about banning assault\nweapons. Therefore, the answer to this question should be True.\n【Domain Sample】\n{instruction}: Classify whether a user has depression based on their Reddit posts and choose an option:\\nA. Yes\\nB. No\\nUser\npost: i wa scrolling around and read up on brain aneurysm and im terrified of having one now i saw a headache wa a sign and i read\nthat while having one it not severe but still now im on the brink of cry and i wan na ask my parent for a check up because of this\nanxiety but i know they aren t willing to do it can someone offer some relief\\n\n{answer}: Let's analyze this Reddit post step by step:\\n\\n1. **Identify key phrases indicating depressive symptoms or mental\nstate**: The user expresses feelings of terror and anxiety about health concerns, stating they are \\\"on the brink of cry.\\\" This\nsuggests a heightened emotional state that may be indicative of distress. The phrase \\\"I know they aren\\u2019t willing to do it\\\"\nindicates a sense of helplessness or lack of support, which can contribute to depressive feelings.\\n\\n2. **Examine behavioral\npatterns described (sleep habits, avoidance, etc.)**: While the user does not explicitly mention sleep habits, their anxiety about\nhealth issues may lead to avoidance behaviors, such as not wanting to discuss their fears with their parents. The act of scrolling\nthrough potentially distressing information about brain aneurysms suggests a compulsive behavior that can be linked to anxiety,\nwhich often coexists with depressive symptoms.\\n\\n3. **Evaluate expressed emotions and cognitive patterns**: The user expresses\nsignificant anxiety and fear regarding their health, which can lead to rumination\\u2014a cognitive pattern often associated with\ndepression. The desire to cry indicates a level of emotional distress that can be symptomatic of depression. The user's plea for relief\nsuggests feelings of desperation and a need for support.\\n\\n4. **Consider diagnostic criteria for depression (DSM-5/ICD-10)**:\nAccording to the DSM-5, symptoms of depression can include persistent sadness, feelings of hopelessness, and anxiety. The user's\nemotional state, coupled with their avoidance of seeking help from their parents, aligns with the criteria for mood disorders. The\ndistress caused by their health anxiety could also contribute to a depressive episode.\\n\\n5. **Determine if evidence supports\ndepression classification**: The combination of expressed anxiety, emotional distress, and avoidance behavior suggests that the user\nis experiencing significant psychological strain. While they may not meet all criteria for a major depressive episode, the symptoms\nthey describe indicate a likelihood of experiencing depressive feelings, particularly in the context of their anxiety.\\n\\nTherefore, the\nanswer is A.\\n\n【Selected Auxiliary Sample】\n{instruction}: Categorize the comment on the basis of threat. If the comment is threatening output Yes, otherwise output\nNo.\\n\\nGreat\\n\n{answer}: The comment is a positive response and does not contain any threatening language. Therefore, the answer is \\\"No\\\".\nPrivacyQA\nRSDD\nFigure 13: Example of domain sample and selected auxiliary sample of privacy policy question-\nanswering (PrivacyQA) and depression diagnosis (RSDD) tasks.\n27\n"}]}