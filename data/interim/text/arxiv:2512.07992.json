{"doc_id": "arxiv:2512.07992", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.07992.pdf", "meta": {"doc_id": "arxiv:2512.07992", "source": "arxiv", "arxiv_id": "2512.07992", "title": "Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis", "authors": ["Aaron D. Mullen", "Daniel R. Harris", "Svetla Slavova", "V. K. Cody Bumgardner"], "published": "2025-12-08T19:24:50Z", "updated": "2025-12-08T19:24:50Z", "summary": "Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.07992v1", "url_pdf": "https://arxiv.org/pdf/2512.07992.pdf", "meta_path": "data/raw/arxiv/meta/2512.07992.json", "sha256": "4a6d0b21a88e5669b6c4dd08e3728a6a95924568a2c0ae65b0b53d789f59e393", "status": "ok", "fetched_at": "2026-02-18T02:24:43.464055+00:00"}, "pages": [{"page": 1, "text": " \nBridging the Clinical Expertise Gap: Development of a Web-Based Platform \nfor Accessible Time Series Forecasting and Analysis \nAaron D. Mullen, M.S.1, Daniel R. Harris, Ph. D.2, Svetla Slavova, Ph. D.3, V.K. Cody \nBumgardner, Ph.D.1 \n1Center for Applied AI, University of Kentucky, Lexington, Kentucky, USA \n2College of Medicine, University of Kentucky, Lexington, Kentucky, USA \n3Department of Biostatistics, University of Kentucky, Lexington, Kentucky, USA \n \nAbstract \nTime series forecasting has applications across domains and industries, especially in healthcare, but the technical \nexpertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This \narticle presents a web platform that makes the process of analyzing and plotting data, training forecasting models, \nand interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate \nplots to showcase their variables and the relationships between them. The platform supports multiple forecasting \nmodels and training techniques which are highly customizable according to the user’s needs. Additionally, \nrecommendations and explanations can be generated from a large language model that can help the user choose \nappropriate parameters for their data and understand the results for each model. The goal is to integrate this platform \ninto learning health systems for continuous data collection and inference from clinical pipelines.  \nIntroduction \nTime series forecasting encompasses both statistical and machine learning methods for a wide variety of uses across \ndifferent domains. Forecasting techniques can be used for predicting stock prices and market trends1,2, planning power \nsupply in different areas based on demand forecasts3, predicting the weather to make operational decisions about \nrenewable energy systems4, predicting flooding and managing water resources5, and understanding how external \nfactors influence retail sales6. Time series forecasting is especially prevalent in healthcare at both a population level, \nsuch as for epidemiology and predicting pandemic-related data7, and at an individual level, like using prediction \ntechniques for personalized medicine based on continuous monitoring8. \nTechnical backgrounds are typically required to design and build the models to generate predictions and to interpret \nthe results of those predictions. Forecasting is primarily done with either statistical or machine learning methods. For \nstatistical models, such as Autoregressive Integrated Moving Average (ARIMA), a background in statistics is required \nto understand the requirements of data distribution and feature selection. For machine learning models, such as Long \nShort-Term Memory neural networks9, a background in computer science with a specialization in machine learning is \ntypically required to understand model architectures and parameter tuning. While machine learning models typically \noffer improved accuracy and predictive power when compared to statistical techniques, this can come at the cost of \ninterpretability10,11. Neither type of model is perfect on its own, and implementing these models effectively requires \ndeep understanding of the data and methods being used, frequently making it difficult to optimally utilize these \ntechniques12. \nThere are a variety of different tools and methods accessible to researchers and clinicians to perform time series \nforecasting, but these tools have their own limitations. Some are incredibly powerful and customizable, such as Python \nlibraries like statsmodels13, scikit-learn14, and PyTorch15. However, these libraries are only usable by individuals with \nprogramming expertise, and even then, they can be difficult to correctly implement. On the other hand, while tools \nsuch as Excel are very accessible and simple to use, they are limited in their capabilities to perform detailed custom \nanalysis. Thus, a tool for time series forecasting that is both powerful and accessible is needed in this field. \nThere have been other attempts to address this gap in the domain of time series forecasting. PyCaret is an example of \na “low-code” library that allows users to implement models easier with only a few lines of code, but this still \nnecessitates some understanding of programming workflows16. There are also commercial alternatives that can handle \ntime series forecasting through web-based interfaces, such as Amazon Forecast17, DataRobot18, or Google Cloud19, \n"}, {"page": 2, "text": " \nbut these tools are proprietary and often cost-prohibitive. Combining large language models (LLMs) with time series \nforecasting is a cutting-edge research topic with some existing work, but this is primarily focused on using the LLM \nto generate forecasts themselves20,21, which differs from the use case described in this article. \nThis article presents the Forecaster, an open-source, web-based platform that incorporates data pre-processing, model \ntraining, and results interpretation into a single, easy-to-use interface. Through this platform, users can upload a dataset \nand easily generate line plots or Gantt charts for any of their variables, or multiple variables at once, to identify trends \nand compare columns. After this, users can select which models they want to train and can modify parameters for \nthose models. To better assist the user with making these selections, an LLM assistant is provided that can use \nsummary statistics about the uploaded dataset to make recommendations on parameter choices and models, using stats \nsuch as dataset size, time periods, and distributions. Once the user makes these choices, they can begin the training \njob, which will use those parameters to train and evaluate each of the models. Once this job is finished, users can view \nresults on the site, including performance metrics for each model and graphs showing what the predictions look like. \nAgain, an LLM is incorporated to provide explanations for the results and make recommendations about which models \nwould be best to use. The user can also choose to easily re-run any training jobs with modified parameters or generate \nnew forecasts for the future with previously trained models. Overall, this system handles many different aspects of the \nprocess of generating predictions in an automated, user-friendly way, making these powerful tools more accessible to \nclinicians and researchers. \nAn additional goal with the development of the Forecaster is the integration of this platform with continuous health \nmonitoring systems. Trained models can be part of a larger learning health system, where data is passively collected \nfrom clinical pipelines and used to continually update forecasting models. For example, eICU data streams could be \nfed into the system via API for repeated re-training and evaluation, ensuring continuous monitoring and updated \nforecasting using vitals from patients. \nMethods \nThe Forecaster is built on a modular, dual-backend architecture. The primary frontend web-serving processes, such as \nuser management and database interactions, are performed with a PHP backend as part of a LAPP (Linux, Apache, \nPostgreSQL, PHP) stack architecture. This system is built using a Model-View-Controller design to appropriately \nrelate processes together. A PostgreSQL relational database is incorporated to store user metadata and overall time \nseries dataset information. CILogon is used for identity management and security of user credentials when accessing \nthe site22. \nFor other, more technical processes, such as analytical processing, job creation, and LLM interactions, a separate \nPython Flask service was developed and incorporated as well. This separation allows the platform to leverage robust \nPython libraries and API interactions for more computationally expensive or complex tasks. The PHP web service \nbackend interacts with this Python server using the Guzzle HTTP client to make requests to the Flask server’s RESTful \nAPI. The LLM is queried using an OpenAI-compatible API, meaning any quality reasoning model can be utilized. In \nthe Forecaster’s current implementation, the DeepSeek-R1 model is used23, implemented and hosted locally at the \nUniversity of Kentucky to ensure that all data remains secure and private. This implementation of the model leverages \nan 8-GPU H200 cluster with 141GB VRAM per GPU to host the models’ weights. DeepSeek was chosen due to its \nopen source nature and ability to perform complex reasoning using external context, but any commercial or open \nsource reasoning models could be substituted. \nThe training and evaluation of the chosen models is also performed with a separate Python agent. This agent is \nindependent of the machine that the web services are running on and can be set up on any other machine in the network. \nThe connection between this agent and the website is created using ClearML24, an infrastructure platform that can \nmanage training jobs by collecting parameters and sending information to agents in the system. The Python agent can \nalso send API requests back to the PHP backend to give updates on the training process, which are made visible to the \nuser through the web interface. \nObject storage for data and results is implemented with a self-hosted, S3-compatible system. This stores all large data \nartifacts, including raw user-uploaded time series files and pickled trained models. Both backends can interact with \nthis data store, using the AWS S3Client library in PHP25 and the boto3 library in Python26. Thus, these files can be \naccessed both by the web service and by the separate job agent. This overall system is represented in Figure 1. \n"}, {"page": 3, "text": " \n \nFigure 1. Architecture of the Forecaster system. \nTo begin the process of running a job, a user must first upload a .csv file containing their data. When the user uploads \nthis file, the system validates file size and naming requirements. If the file is valid, a call is made to the controller to \nsave the file to the object storage system, using a combination of the user ID and filename to create a unique identifier \n(users cannot upload more than one file of the same name). A record is also made in the PostgreSQL database that \nlinks the dataset with the user and contains other metadata, such as the time of upload. Additionally, the column names \nare extracted from the .csv and returned to the view to be displayed on the next page. \nThe user can then classify each of the columns of their dataset according to their roles in the forecasting task. The \npossible roles are ‘Not Included’, ‘Time Component’, ‘Grouping’, ‘Target’, ‘Past Covariate’, ‘Future Covariate’, and \n‘Static Covariate’. ‘Not Included’ indicates a column that should be ignored. The ‘Time Component’ column \nrepresents the date or timestep for the time series. ‘Grouping’ indicates a column that distinguishes between multiple \ngroupings that should be simultaneously forecasted; for example, a dataset may contain flu cases for different \ncountries, and the user wants to create forecasts for each country. In that case, the column indicating the country would \nbe the ‘Grouping’ column. The ‘Target’ can be one or multiple columns that should be forecasted. If multiple columns \nare selected, these components will be analyzed and forecasted using a single multivariate model, except for some \nmodels that will be described later that can only perform univariate forecasting, in which case multiple models will \nbe trained. The covariate columns indicate variables that should be used by the models to help with predictions by \nproviding additional trends to analyze and compare. ‘Past Covariates’ are variables for which only previous values \nwould be known, but values at prediction time would be unknown. Variables known ahead of prediction time, like \nday of the week or planned promotions, are known as ‘Future Covariates’. ‘Static Covariates’ are variables that stay \nconstant but can help distinguish between groupings. \nTo assist with the user’s choices during this stage, variables can be plotted using an additional tool provided on this \npage of the website. The user can select a variable (typically the time component) for the x-axis, and one or more \nvariables to plot along the y-axis. With this functionality, users can analyze the columns and compare features to \ndetermine how to best categorize those columns. Typically, these plots will take the form of a line graph, unless a \ncategorical variable is chosen. If so, a Gantt-like chart will be produced instead. The plotting is performed using the \nJavaScript Plotly library27. Once the data categories are submitted, a JSON string containing each column name and \nits corresponding category is saved to the PostgreSQL database. Figure 2 illustrates the page design, featuring an \nexample using a publicly available dataset containing daily counts of bike riders in Norway28. \n"}, {"page": 4, "text": " \n \nFigure 2. Displays columns and plotted data, with the plotting tab able to be shown or hidden on the page. \nAfter submitting these categories, the next page allows the user to make choices regarding the training job and \nparameters. The models the user can select include ARIMA29, exponential smoothing30, linear regression31, \nXGBoost32, random forest33, light gradient boosting machine34, N-Linear model35, and temporal fusion transformer36. \nSome of these models have different limitations; for example, the ARIMA model cannot use future covariates, the \ntemporal fusion transformer must have future covariates, and this implementation of exponential smoothing cannot \nuse any covariates and can only train on one time series at a time. These models have different parameters that can be \nmodified. Only the relevant parameters to the models that have been selected for training will be shown. Some \nparameters apply to many models, such as choices for input/output chunk length or number of epochs for training. \nThe user can also make other choices on this page that affect the training and evaluation process. Users can choose if \nthe models should, when possible, generate probabilistic forecasts, meaning that a distribution function is chosen, and \nmany predictions are made for each timestep before being averaged together. The user can also choose how the models \nwill be trained and evaluated. One option will evaluate the model over a holdout test set at the end of the series. \nAnother option, if the user is confident with the model and data, is to use the entire series for training and generate \npredictions for a certain number of timesteps after the end of the timeseries. Users can also choose to do this on a later \npage once their models have been fully trained. Finally, the user can choose an expanding window method of training \nand evaluation. With this technique, each model will be trained only on the beginning of the timeseries and will \nproduce forecasts up to a certain horizon after this subset. Subsequently, the training window will expand a certain \nnumber of timesteps and retrain, learning from the errors in the previous forecast, and it will generate new predictions \nover the next period. This cycle continues until the model has trained on the entire dataset. \nChoosing the correct parameters for training can be difficult because the ‘correct’ choices depend on the size and \nstructure of the dataset. The period of the timeseries can have a large influence on proper parameter choices, as specific \nvalues should be guided by the natural cycles in the timeseries data. For example, it would make sense to consider \nhourly data in 12 or 24 timestep windows because this aligns with natural daily cycles. If the timeseries has a daily \nperiod, then increments of 7 or 30 align with weekly or monthly periods, which frequently contribute important \nseasonality patterns to the data. Additionally, the size of the data and presence of certain types of covariates can change \nwhat models would be recommended to work with. Thus, it was important to incorporate a process for obtaining \nrecommendations for parameter and model choices. \nThis was accomplished using an LLM. If the user selects ’Get Recommendations’, a request will be made to a Flask \nserver function that reads in the data and produces overall summary statistics. This includes the number of \nobservations, the frequency of the time component, and statistics about the target variable, such as mean, distribution, \n"}, {"page": 5, "text": " \nand proportion of zeros. These statistics are then passed as context to a request made to the DeepSeek-R1 model. The \nLLM is instructed in the prompt to provide recommendations in a JSON format for a given list of parameters and \nmodels. Thus, the output from the LLM call contains keys for each parameter, which can be used to automatically fill \nin the values of the appropriate input elements on the website. The user is free to further modify these parameter \nchoices. In addition, the LLM is instructed to write a brief explanation for the parameter picks, which is provided to \nthe user through a text box on the webpage. An example screenshot for this webpage is provided in Figure 3. \nFigure 3. Section of parameter page with LLM recommendations. \nOnce satisfied with the choices, the user can begin the training job. When this happens, a request is first made to the \nPHP controller that contains the parameter choices from the webpage; the controller will also collect the dataset \ncolumn categorizations from the PostgreSQL database. All of the choices are sent as arguments to a ClearML job. \nThis includes information about the dataset necessary for the agent to pull the correct timeseries data from the object \nstorage. When this ClearML job begins, it will enter a queue. Any running agent that is not actively performing a job \nwill be listening to this queue. Thus, multiple agents can be set up to account for higher demand in the system. \nWhen an agent begins a job, it pulls down the parameters from ClearML and uses the information to pull the dataset \nfrom the object storage. It uses the column distinctions to classify each column of the data appropriately. The Python \nagent primarily uses the Darts library to build timeseries objects and train and evaluate each model type37. Once the \ndataset is read in and formatted correctly, the target variable(s) and covariates, if applicable, are individually \nnormalized to range from 0 to 1. Then, each model is iterated through, producing predictions for the specified time \nframe. Additionally, a Naïve Seasonal model that only copies the most recent seasonal values of the timeseries for its \nprediction is trained. This provides a baseline of results for comparison with the other models. \nAfter each model is trained and produces predictions, each prediction set is compared to the true values. Performance \nmetrics are calculated over the test set, or if the expanding window method was chosen, they are calculated over all \ntimesteps for which the model was evaluated during the training process. Two versions of each performance metric \nare calculated: a normalized value calculated over the scaled series and a de-normalized value calculated over the \n"}, {"page": 6, "text": " \noriginal, un-modified series. The normalized metrics are helpful for comparing performance between groupings and \ncomponents because they are independent of the scale of the data. The de-normalized metrics follow the scale of the \ndata and are more intuitive for real interpretations. Both the predicted and real target values for each model, as well \nas the performance metrics, are saved in a JSON format to the object storage. \nThroughout the training process, the agent sends API requests to the frontend PHP controller, modifying the view for \nthat dataset on the home page of the website to provide updates on how many models have been trained. This allows \nthe user to follow the progress of their job. Once the job has completed, a final request is sent, and the frontend is \nnotified to update the home page to allow the user to view results for that job. \nOn the results page, an overall table shows the performance metrics for each model. The columns for each metric are \nsortable and toggleable so the user can view only the metrics they are interested in. If the dataset contains multiple \ngroupings or components, dropdowns are provided to allow the user to look at an overall summary or view metrics \nspecific to each grouping/component. An output log is also viewable on this page and allows the user to see any \nwarnings or errors that occurred during the job. The user can generate visualizations for each model, which will show \nthe training data, testing data (if applicable), and predictions in a Plotly-generated graph. These graphs are generated \nat runtime using the JSON data stored in the object storage. Users can zoom and toggle different components and \ngroupings to get more detailed views of segments of data. These graphs can also be easily exported as images. \nInterpreting the performance metrics for each model can be overwhelming, so LLM assistance is also available on this \npage. If the user clicks the ‘Summarize Results’ button, a JSON of the performance metric data is passed to the \nDeepSeek-R1 model, and the model is instructed to summarize these results in an easy-to-understand way and provide \nrecommendations and explanations for which models appear best suited to the data. The LLM’s response is then \nshown in a text box on the results page, similarly to the parameter recommendations. \nNew predictions can be generated on this page using the trained models. The user can select which models to use and \nhow many timesteps to forecast for. A new .csv file can be uploaded containing future covariate values, and the models \nwill use these future values to make predictions for the new timesteps. This process operates similarly to the training \nworkflow. A job is created in ClearML for the prediction task, and parameters are passed through ClearML to a \nlistening agent. The agent reads in the dataset, trained model(s), and any future covariates from the object storage. If \nthe models were initially trained on future covariate data but this data is not provided or does not cover the entire \nforecast horizon, then it will be imputed for the modern timesteps using the Pandas interpolate method, which fills in \nnew data assuming a linear trend38. Once the covariate data is handled, the models will be iterated through to produce \nnew predictions for the specified number of timesteps, and these predictions are saved to object storage. \nOnce the forecasting job is completed, the user can download the results as a JSON file, which contains raw forecast \ncounts for each model. Additionally, if the user generates a visualization on the interface, it will now include the new \nforecast data as part of the resulting line graph for each applicable model. \nResults \nThe Forecaster was tested across multiple kinds of datasets to ensure performance was sufficient across different data \ntypes and complexities. The quality of the specific results on these datasets is not the focus of this paper, but the results \nare included to demonstrate the kinds of metrics available to users and to justify the effectiveness of the system. \nFour different datasets were used to test the forecasting performance of the site. The first, already described through \nthe examples in the previous section, contains daily counts of bike riders in Norway, with temperature and precipitation \ncolumns used as covariates. The second is a simple, one-dimensional time series of electrocardiogram (ECG) readings \nsampled at a rate of 100 Hz39. The third dataset contains daily sales information for stores in Ecuador, containing \ncounts of sales for different categories of products, as well as a covariate indicating whether that category of product \nhad an active promotion for that day40. This dataset was limited to only a single store for this analysis, and the \ncategories of products were considered distinct groupings. The fourth dataset contains daily COVID-19 data for a \nvariety of countries. This dataset contains multiple series that can be forecasted, such as new cases and new deaths \nresulting from COVID-19, and a variety of covariates, such as new vaccination rates and static covariates for each \ncountry like population density, median age, and rates of cardiovascular disease and smoking41. \n"}, {"page": 7, "text": " \nThese datasets were chosen to ensure a variety of industries, use cases, and dataset types were tested. It creates a \ncomprehensive evaluation of different types of covariates (past, future, static) and ensures multiple temporal periods, \ngrouping types, and target variable distributions are included. Table 1 shows the normalized results for the best-\nperforming model for each dataset, using both kinds of evaluation (train/test split, expanding window). The best-\nperforming model was chosen as the one with the lowest values across the most different metrics. For the expanding \nwindow evaluation, the metrics were calculated across the entire series, excluding the initial segment used for training \nonly. For datasets with multiple groupings (store sales, COVID-19 cases) or multiple target series (new COVID-19 \ncases and new deaths), the metrics are averaged across these distinctions. \nDataset \nEvaluation \nType \nMean \nAbsolute \nError \n(MAE) \nMean \nAbsolute \nPercentage \nError \n(MAPE) \nMean \nSquared \nError \n(MSE) \nRoot \nMean \nSquared \nError \n(RMSE) \nSymmetric \nMean \nAbsolute \nPercentage \nError \n(SMAPE) \nMean \nAbsolute \nScaled \nError \n(MASE) \nBest \nPerforming \nModel \nBike \nrides \nTrain/test \n0.110 \n0.000* \n0.021 \n0.143 \n0.000* \n0.944 \nRandom \nForest \nExpanding \nWindow \n0.086 \n0.000* \n0.014 \n0.117 \n0.000* \n1.184 \nRandom \nForest \nECG \nTrain/test \n0.031 \n7.845 \n0.007 \n0.085 \n8.095 \n24.235 \nRandom \nForest \nExpanding \nWindow \n0.064 \n28.518 \n0.013 \n0.112 \n19.065 \n16.152 \nARIMA \nSales \nTrain/test \n0.071 \n0.000* \n0.011 \n0.099 \n0.000* \n0.634 \nExponential \nSmoothing \nExpanding \nWindow \n0.065 \n0.000* \n0.009 \n0.094 \n0.000* \n0.766 \nExponential \nSmoothing \nCOVID-\n19 \nTrain/test \n0.393 \n215.887 \n0.923 \n0.615 \n77.589 \n50.386 \nARIMA \nExpanding \nWindow \n0.224 \n70.045 \n0.313 \n0.394 \n45.724 \n1.000* \nXGBoost \nTable 1. Summary of results across datasets and evaluation types. \nTable 1 shows that the expanding window method typically leads to better performance during evaluation than the \nstandard train/test split technique. In some cases, the MAPE and SMAPE values equal zero because the series they \nare calculated for contains values of zero, which causes the calculation for these percentage metrics to return an \nundefined value. Additionally, the MASE fails in rare cases due to discrepancies between the training set length and \nthe length of the seasonality period of the time series. Each dataset also has inherent differences that influence the \nperformance of each model. The COVID-19 dataset performs the worst overall when averaged together, but this is \nbecause several of the countries have unprecedented increases in cases towards the end of the series, which inflates \nthe error metrics during those periods. The country with the most accurate predictions was India, where the XGBoost \nmodel achieved metrics much closer to the results of the other datasets, such as an RMSE of 0.106. \nExamples of visualizations showcasing the predictions are given in Figure 5. Figure 5(A) shows the performance of \nthe random forest model when evaluated just on the test set for the bike riding dataset, while Figure 5(B) shows the \nexpanding window evaluation for the same model. \n"}, {"page": 8, "text": " \n \n \nFigure 5. Visualizations of predictions on bike riding dataset, for both train/test (A) and expanding window (B). \nDiscussion \nThe Forecaster system’s architecture allows for applicability to a variety of datasets like the ones tested here for \ndemonstration. Results can be easily compared between datasets and models due to the variety of metrics produced in \nboth normalized and de-normalized ways. Also, jobs can be easily re-run with different covariates or parameters to \ncompare performance on the same dataset. \nAcross the tested datasets, the same models would frequently be among the best or worst performing. Models such as \nrandom forest, XGBoost, and ARIMA were typically the ones that created the most accurate predictions. This likely \nspeaks more to their applicability to a variety of datasets and complexities than it does to any fundamental truth about \nforecasting models in general. Neural network models like N-Linear LSTF and Temporal Fusion Transformer \nfrequently performed among the worst models here, but there could be several reasons for this. These models may be \nmore complex than the tested datasets require, and they would be better suited to much larger time series. All of the \nmodels tested here were run with default parameters. These complex neural network models are more dependent on \nappropriate parameter choices, meaning they may underperform when time is not spent tuning the necessary \narguments. The Forecaster interface allows for modification of different parameters, so tuning these through the \nwebsite is likely a necessary step to achieve optimal performance with these kinds of models. \nIn the future, more work will be done to expand the flexibility of the input data format, rather than limiting the uploads \nto only .csv files. There are plans to develop an intake system that allows for continually updating streams of data to \nbe read in from timeseries databases. This would support the ability to make repeated predictions on a time series that \ncontinually extends with new data, ensuring that the predictions generated stay updated and relevant. An API system \nis planned to be further developed that would allow for programmatic interactions with the system, supporting the \nincorporation of this tool with learning health systems and continually streaming data sources. \nThe Forecaster has limitations concerning full model customization or advanced feature engineering techniques. These \ncomplex additions would come at the expense of usability for non-technical researchers, and the tool was designed \nwith the primary goal of making forecasting accessible. Experienced users hoping to implement more complex \ntechniques can still utilize the Forecaster for exploratory analysis, plotting, and generation of model and data files that \ncan be downloaded and further explored with custom code. \n"}, {"page": 9, "text": " \nConclusion \nThe Forecaster is a powerful tool for training and evaluating time series forecasting models, as well as interpreting \ndata and results through visualizations, wrapped into an easy-to-use web interface. It provides no-code solutions to \nresearchers and clinicians who wish to take advantage of the available tools for forecasting without possessing the \nrequired technical background to implement complex models. This interface allows users to simply upload data, \nchoose data options and parameters, and begin a training job without needing to worry about required computational \nresources or writing code. Parameters and results can be explained on the website with an LLM, lowering the barrier \nto understanding how these models work or what their results mean. This system is planned to be incorporated within \nlarger frameworks for continuous health monitoring using data streams and repeated re-training and evaluation of \nforecasting models. The Forecaster has a variety of applications, especially in healthcare research, where time series \ndata is common and the ability to create accurate forecasts can have strong positive effects on health outcomes. \nAcknowledgement \nThis research was supported in part by the National Institutes of Health under award number UL1TR001998. The \ncontent is solely the responsibility of the authors and does not necessarily represent the official views of the NIH. \nReferences \n1. Praggnya Kanungo. Time series forecasting in financial markets using Deep Learning Models. World Journal \nof \nAdvanced \nEngineering \nTechnology \nand \nSciences. \n2025 \nApr \n30;15(1):709–19. \ndoi:10.30574/wjaets.2025.15.1.0167 \n2. Sun G, Deng S. Financial time series forecasting: A comparison between traditional methods and AI-driven \ntechniques. Journal of Computer, Signal, and System Research. 2025 Mar 28;2(2):86–93. \ndoi:10.71222/339b9812 \n3. Sokannit P. Forecasting household electricity consumption using time series models. International Journal of \nMachine Learning and Computing. 2021 Nov;11(6):380–6. doi:10.18178/ijmlc.2021.11.6.1065 \n4. Neumann O, Turowski M, Mikut R, Hagenmeyer V, Ludwig N. Using weather data in energy time series \nforecasting: The benefit of input data transformations. Energy Informatics. 2023 Nov 2;6(1). \ndoi:10.1186/s42162-023-00299-8 \n5. Willard JD, Varadharajan C, Jia X, Kumar V. Time series predictions in unmonitored sites: A survey of \nmachine learning techniques in water resources. Environmental Data Science. 2025;4. \ndoi:10.1017/eds.2024.1 \n6. Mansur S, Sattar K, Hosseini SE, Pervez S, Ahmad I, Saleem K, et al. Sales forecasting for retail stores using \nhybrid neural networks and sales-affecting variables. PeerJ Computer Science. 2025 Sept 11;11. \ndoi:10.7717/peerj-cs.3058 \n7. Tomov L, Chervenkov L, Miteva DG, Batselova H, Velikova T. Applications of time series analysis in \nEpidemiology: Literature Review and our experience during COVID-19 pandemic. World Journal of Clinical \nCases. 2023 Oct 16;11(29):6974–83. doi:10.12998/wjcc.v11.i29.6974 \n8. Patharkar A, Cai F, Al-Hindawi F, Wu T. Predictive modeling of Biomedical Temporal Data in healthcare \napplications: \nReview \nand \nFuture \nDirections. \nFrontiers \nin \nPhysiology. \n2024 \nOct \n15;15. \ndoi:10.3389/fphys.2024.1386760 \n9. Hochreiter S, Schmidhuber J. Long Short-Term Memory. Neural Computation. 1997 Nov 1;9(8). \ndoi:https://doi.org/10.1162/neco.1997.9.8.1735 \n10. Rudin C. Stop explaining black box machine learning models for high stakes decisions and use interpretable \nmodels instead. Nature Machine Intelligence. 2019 May 13;1(5):206–15. doi:10.1038/s42256-019-0048-x \n11. Ahmad M, Rehman AA, Khan R, Bibi H. Interpretable machine learning for time series analysis: A \ncomparative study with statistical models. ACADEMIA International Journal for Social Sciences. 2025 Aug \n28;4(3):4001–9. doi:10.63056/acad.004.03.0681 \n12. Furizal F, Ma’arif A, Kariyamin, Firdaus AA, Wijaya SA, Nakib AM, et al. Understanding Time Series \nForecasting: \nA \nFundamental \nStudy. \nBuletin \nIlmiah \nSarjana \nTeknik \nElektro. \n2025 \nOct;7. \ndoi:10.12928/biste.v7i3.13318 \n13. Introduction [Internet]. [cited 2025 Oct 29]. Available from: https://www.statsmodels.org/stable/index.html \n14. Learn [Internet]. [cited 2025 Oct 29]. Available from: https://scikit-learn.org/stable/ \n15. Pytorch [Internet]. [cited 2025 Oct 29]. Available from: https://pytorch.org/ \n"}, {"page": 10, "text": " \n16. Home [Internet]. 2023 [cited 2025 Oct 29]. Available from: https://pycaret.org/ \n17. [Internet]. [cited 2025 Oct 29]. Available from: https://aws.amazon.com/forecast/ \n18. Time-series \nmodeling \n[Internet]. \n[cited \n2025 \nOct \n29]. \nAvailable \nfrom: \nhttps://docs.datarobot.com/en/docs/modeling/time/index.html \n19. Train a model with tabular workflow for forecasting  |  vertex AI  |  google cloud [Internet]. Google; [cited \n2025 \nOct \n29]. \nAvailable \nfrom: \nhttps://cloud.google.com/vertex-ai/docs/tabular-data/tabular-\nworkflows/forecasting-train \n20. Vishwas BV, Macharla SR. Time-LLM: Reprogramming large language model. Time Series Forecasting \nUsing Generative AI. 2025;131–54. doi:10.1007/979-8-8688-1276-7_4 \n21. Tang H, Zhang C, Jin M, Yu Q, Wang Z, Jin X, et al. Time series forecasting with LLMS: Understanding \nand enhancing model capabilities. ACM SIGKDD Explorations Newsletter. 2025 Jan 21;26(2):109–18. \ndoi:10.1145/3715073.3715083 \n22. [Internet]. [cited 2025 Oct 30]. Available from: https://www.cilogon.org/ \n23. DeepSeek-AI, Guo D, Yang D, Zhang H, Song J, Zhang R, et al. DeepSeek-R1: Incentivizing Reasoning \nCapability in LLMs via Reinforcement Learning. 2025 Jan 22; \ndoi:https://doi.org/10.48550/arXiv.2501.12948 \n24. AI Infrastructure Platform: Maximize AI Performance & Scalability [Internet]. 2025 [cited 2025 Oct 30]. \nAvailable from: https://clear.ml/ \n25. [Internet]. [cited 2025 Oct 30]. Available from: https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-\nAws.S3.S3Client.html \n26. Boto3 \ndocumentation¶ \n[Internet]. \n[cited \n2025 \nOct \n30]. \nAvailable \nfrom: \nhttps://boto3.amazonaws.com/v1/documentation/api/latest/index.html \n27. Data apps for production [Internet]. [cited 2025 Oct 30]. Available from: https://plotly.com/ \n28. Banachewicz K. Norway bicycles [Internet]. 2021 [cited 2025 Oct 30]. Available from: \nhttps://www.kaggle.com/datasets/konradb/norway-bicycles/data \n29. Arima \n[Internet]. \n[cited \n2025 \nOct \n30]. \nAvailable \nfrom: \nhttps://unit8co.github.io/darts/generated_api/darts.models.forecasting.arima.html \n30. Statsmodels.tsa.holtwinters.ExponentialSmoothing [Internet]. [cited 2025 Oct 30]. Available from: \nhttps://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html \n31. Linear \nRegression \nModel¶ \n[Internet]. \n[cited \n2025 \nOct \n30]. \nAvailable \nfrom: \nhttps://unit8co.github.io/darts/generated_api/darts.models.forecasting.linear_regression_model.html \n32. Chen T, Guestrin C. XGBoost. Proceedings of the 22nd ACM SIGKDD International Conference on \nKnowledge Discovery and Data Mining. 2016 Aug 13;785–94. doi:10.1145/2939672.2939785 \n33. Randomforestregressor \n[Internet]. \n[cited \n2025 \nOct \n30]. \nAvailable \nfrom: \nhttps://scikit-\nlearn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.Ran\ndomForestRegressor \n34. Ke G, Meng Q, Finley T, Wang T, Chen W, Ma W, et al. LightGBM: a highly efficient gradient boosting \ndecision tree. Proceedings of the 31st International Conference on Neural Information Processing Systems. \n2017;3149–57. \n35. Zeng A, Chen M, Zhang L, Xu Q. Are transformers effective for time series forecasting? Proceedings of the \nAAAI Conference on Artificial Intelligence. 2023 Jun 26;37(9):11121–8. doi:10.1609/aaai.v37i9.26317 \n36. Lim B, Arık S, Loeff N, Pfister T. Temporal Fusion Transformers for interpretable multi-horizon time series \nforecasting. \nInternational \nJournal \nof \nForecasting. \n2021 \nOct;37(4):1748–64. \ndoi:10.1016/j.ijforecast.2021.03.012 \n37. Time \nseries \nmade \neasy \nin \npython \n[Internet]. \n[cited \n2025 \nOct \n30]. \nAvailable \nfrom: \nhttps://unit8co.github.io/darts/ \n38. Pandas [Internet]. [cited 2025 Oct 30]. Available from: https://pandas.pydata.org/ \n39. Andrikov D. ECG timeseries for prediction [Internet]. 2025 [cited 2025 Nov 6]. Available from: \nhttps://www.kaggle.com/datasets/denisandrikov/ecg-timeseries-for-prediction \n40. Store \nsales \n- \ntime \nseries \nforecasting \n[Internet]. \n[cited \n2025 \nNov \n6]. \nAvailable \nfrom: \nhttps://www.kaggle.com/competitions/store-sales-time-series-forecasting/data?select=train.csv \n41. Anttal TS. Covid 19 dataset till 22/2/2022 [Internet]. 2022 [cited 2025 Nov 6]. Available from: \nhttps://www.kaggle.com/datasets/taranvee/covid-19-dataset-till-2222022 \n \n"}]}