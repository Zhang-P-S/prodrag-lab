{"doc_id": "arxiv:2601.20330", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.20330.pdf", "meta": {"doc_id": "arxiv:2601.20330", "source": "arxiv", "arxiv_id": "2601.20330", "title": "PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments", "authors": ["Zhuang Chen", "Dazhen Wan", "Zhangkai Zheng", "Guanqun Bi", "Xiyao Xiao", "Binghang Li", "Minlie Huang"], "published": "2026-01-28T07:48:39Z", "updated": "2026-01-28T07:48:39Z", "summary": "While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.20330v1", "url_pdf": "https://arxiv.org/pdf/2601.20330.pdf", "meta_path": "data/raw/arxiv/meta/2601.20330.json", "sha256": "adcdc8e1ccfb2c0d303584f617442a826fd2906a4cdd48758ae5f6a950a8f936", "status": "ok", "fetched_at": "2026-02-18T02:20:16.320174+00:00"}, "pages": [{"page": 1, "text": "PSYCHEPASS: Calibrating LLM Therapeutic Competence\nvia Trajectory-Anchored Tournaments\nZhuang Chen1, Dazhen Wan2, Zhangkai Zheng3, Guanqun Bi4,\nXiyao Xiao2, Binghang Li2, Minlie Huang4\n1School of Computer Science and Engineering, Central South University\n2Lingxin AI\n3South China Normal University\n4CoAI Group, DCST, IAI, BNRIST, Tsinghua University\nzhchen18@foxmail.com\nbiguanqun@mail.tsinghua.edu.cn\nAbstract\nWhile large language models show promise\nin mental healthcare, evaluating their thera-\npeutic competence remains challenging due\nto the unstructured and longitudinal nature of\ncounseling. We argue that current evaluation\nparadigms suffer from an unanchored defect,\nleading to two forms of instability: process\ndrift, where unsteered client simulation wan-\nders away from specific counseling goals, and\nstandard drift, where static pointwise scoring\nlacks the stability for reliable judgment. To\naddress this, we introduce PSYCHEPASS, a uni-\nfied framework that calibrates the therapeutic\ncompetence of LLMs via trajectory-anchored\ntournaments. We first anchor the interaction\ntrajectory in simulation, where clients precisely\ncontrol the fluid consultation process to probe\nmultifaceted capabilities. We then anchor the\nbattle trajectory in judgments through an ef-\nficient Swiss-system tournament, utilizing dy-\nnamic pairwise battles to yield robust Elo rat-\nings. Beyond ranking, we demonstrate that\ntournament trajectories can be transformed into\ncredible reward signals, enabling on-policy re-\ninforcement learning to enhance LLMs’ per-\nformance. Extensive experiments validate the\neffectiveness of PSYCHEPASS and its strong\nconsistency with human expert judgments.\n1\nIntroduction\nThe explosive growth of large language models\noffers a beacon of hope for the mental healthcare\nsector, functioning as scalable, 24/7 counseling\nassistant support systems, alleviating the heavy bur-\nden of professional therapists (Stade et al., 2024).\nHowever, unlike deterministic tasks such as code\ngeneration, counseling is a highly unstructured, lon-\ngitudinal process inherently reliant on dynamic in-\nteraction and systematic stage progression (Nor-\ncross, 2011). Consequently, accurately evaluating\nthe therapeutic competence of LLMs remains a\npressing challenge in the field (Na et al., 2025).\nUnsteered Client Simulation\n(Process Shift)\n(     Battle Trajectory)\nI’m sad \ndon’t be sad\nfeel terrible\nSorry to hear that\nwhat should I do\njust deep breath\nUnanchored Paradigm\nStatic Pointwise Scoring\n(Standard Shift)\n3? 3.5?\nPsychePass\nSST-based Scripted Probing\n(      Interaction Trajectory) \n1. Assessment\nDynamic Pairwise Battle\n2. Awareness\n5. Integration\n3. Conflict\n4. Behavior\nA\nC\nB\nD\nRewards\nfor RL\nFigure 1: PSYCHEPASS anchors two traceable trajecto-\nries for calibrating LLMs’ therapeutic competence.\nSimulating client-therapist interactions is the nat-\nural and effective approach for evaluating counsel-\ning competence, mirroring the supervision of hu-\nman counselors. While a few attempts (Zhu et al.,\n2025; Wang et al., 2025a) adopt this paradigm, we\nidentify a fundamental “unanchored defect” that\nlimits their effectiveness. This manifests in two\ndimensions: 1) Unanchored simulation leads to\nprocess drift. Most evaluations rely on single-turn\nQA or aimless free-form chats based on static client\nprofiles. These unsteered simulations frequently\nwander without clinical direction, failing to trigger\nhigh-stakes scenarios such as trauma processing or\nskill application. As a result, evaluations capture\nonly superficial features, such as generic empathy,\nrather than true therapeutic competence. 2) Unan-\nchored judgment leads to standard drift. Existing\nmethods predominantly use static pointwise scor-\ning (e.g., Likert scales), forcing model judges to\nrely on vague standards without comparative ref-\nerences. This often results in evaluation collapse\n(Wang et al., 2025b), where scores cluster in a nar-\nrow range with poor discriminability. The resulting\nlack of clear, comparative signals makes it impos-\nsible to accurately rank and optimize models.\narXiv:2601.20330v1  [cs.CL]  28 Jan 2026\n"}, {"page": 2, "text": "To overcome the unanchored defect, we intro-\nduce PSYCHEPASS, a calibration framework for\nranking and optimizing the therapeutic competence\nof LLMs by anchoring two traceable trajectories.\n1) Anchoring the interaction trajectory in simu-\nlation. To eliminate process drift and ensure the in-\nteractions between simulated clients and evaluated\nmodels reach sufficient depth, we script interac-\ntion trajectories grounded in single-session therapy\n(Talmon, 1990) theory. The simulated client drives\nthe dialogue flow through five complete counseling\nstages, ranging from alliance building to behavior\ncorrection, and performs targeted probing of 12\nspecific competency dimensions, allowing disen-\ntangled measurement of multifaceted capabilities.\n2) Anchoring the battle trajectory in judgment.\nTo resolve standard drift and ensure discrimina-\ntive judgment, we abandon static pointwise scor-\ning. Instead, we construct explicit battle trajec-\ntories by determining winners through pairwise\nbattles. We employ a Swiss-system tournament\n(Csató, 2013) to reduce comparison complexity\nfrom N(N −1)/2 to (N/2) log2 N, significantly\nlowering computational costs. Based on battle re-\nsults, we utilize the Bradley-Terry model (Bradley\nand Terry, 1952) to calculate Elo scores (Elo, 1978),\nyielding stable and comparable rankings.\nBeyond evaluation, we further demonstrate that\ntournament trajectories can be transformed into\ncredible reward signals to support on-policy rein-\nforcement learning to enhance LLMs’ therapeutic\ncompetence, establishing a complete closed cali-\nbration loop from evaluation to optimization. For\nexperiments, we first evaluate the therapeutic com-\npetence of 12 LLMs, including cutting-edge gen-\neral models and domain-specific models tailored\nfor counseling, and provide fine-grained, discrim-\ninative performance profiles. Extensive analysis\nvalidates the effectiveness of PSYCHEPASS, achiev-\ning high inter-rater agreement with psychologists\n(Cohen’s κ > 0.7). We then demonstrate that, af-\nter being trained with trajectory-based RL, a target\nLLM achieves a win-to-loss ratio of 62 : 25 com-\npared to its original counterpart.\nOur main contributions are: 1) Introducing PSY-\nCHEPASS to establish the landscape of therapeutic\ncompetence for concurrent LLMs; 2) Exploring\nhow to obtain high-fidelity rewards for aligning\nLLMs’ counseling capabilities via on-policy RL;\n3) Conducting extensive experiments verifying that\nPSYCHEPASS’s calibration is credible and qualified\nfor serving as a foundation for future work.\n2\nRelated Work\nResearch on LLMs for psychological counseling\nhas expanded rapidly, ranging from synthesizing\ncounseling dialogue corpora (Zhang et al., 2024;\nXie et al., 2025) to incentivizing diagnostic rea-\nsoning (Hu et al., 2025b,a; Dai et al., 2025; Xiao\net al., 2025). However, the real-world counseling\nprocess is inherently unstructured and longitudi-\nnal, relying on the sophisticated orchestration of\ndiverse therapeutic competencies. Consequently,\nwhile the pathway to training conversational agents\nthat can generate superficially plausible counseling\ndialogues is becoming clear, the rigorous evalua-\ntion of their actual clinical effectiveness lags sig-\nnificantly behind their development.\nStatic Knowledge Examination Current assess-\nments predominantly rely on static benchmarks.\nPsychCounsel-Bench (Zeng, 2025) and CBT-\nBench (Zhang et al., 2025) utilize professional ex-\nams or fixed case studies, while CounselBench (Li\net al., 2025) focuses on case Q&As. While these\nbenchmarks effectively measure theoretical knowl-\nedge, they fall short of verifying whether a model\ncan handle the unstructured, longitudinal dynamics\nof a real counseling session.\nInteractive Simulation Tests To move beyond\nstatic tests, recent studies develop interactive sim-\nulations where LLMs chat with simulated clients\n(Pombal et al., 2025; Wang et al., 2024). How-\never, these environments primarily rely on free-\nform chats and pointwise scoring, which are prone\nto process and standard drift. CARE-Bench (Wang\net al., 2025a) partially guides client simulation with\nexpert principles but lacks explicit stage-level con-\ntrol. Ψ-Arena (Zhu et al., 2025) simply incorpo-\nrates counseling stages in prompting yet does not\nenforce strict process adherence. Moreover, only a\nfew studies incorporate optimization beyond eval-\nuation. Psyche-R1 (Dai et al., 2025) and Men-\ntraSuite (Xiao et al., 2025) optimize knowledge\nreasoning through RL, but their objectives focus on\nanswering exam questions. Ψ-Arena employs post-\nhoc prompt patching for response revision without\ninternalizing the counseling capabilities.\nIn contrast, PSYCHEPASS constrains simulations\nto explicit probing paths and adopts pairwise com-\nparisons to eliminate process and standard drift,\nand further establishes a closed loop for evaluation\nand trajectory-level optimization. We present a\nstraightforward comparison between the proposed\nPSYCHEPASS and existing methods in Table 1.\n"}, {"page": 3, "text": "Systems\nSimulation Paradigm\nJudgement Paradigm\nOptimization Loop\nPsyche-R1 (Dai et al., 2025)\nKnowledge QA\nAccuracy\nKnowledge RL\nMentraSuite (Xiao et al., 2025)\nKnowledge QA\nAccuracy\nKnowledge SFT-RL\nPsychCounsel-Bench (Zeng, 2025)\nKnowledge QA\nAccuracy\nEval Only\nCBT-Bench (Zhang et al., 2025)\nKnowledge QA\nAccuracy\nEval Only\nCounselBench (Li et al., 2025)\nKnowledge QA\nPointwise\nEval Only\nClientCAST (Wang et al., 2024)\nFree-form Chat\nPointwise\nEval Only\nMindEval (Pombal et al., 2025)\nFree-form Chat\nPointwise\nEval Only\nCARE-Bench (Wang et al., 2025a)\nGuided Chat\nPointwise\nEval Only\nψ-Arena (Zhu et al., 2025)\nGuided Chat\nPointwise\nPost-hoc Patch\nPSYCHEPASS (Ours)\nScripted Probing\nPairwise\nTrajectory RL\nTable 1: Comparison of different studies in evaluating therapeutic competencies of LLMs.\n3\nPSYCHEPASS\nPSYCHEPASS calibrates the therapeutic compe-\ntence of LLMs in three steps (see Figure 2): 1) Sim-\nulation: Each model interact with scripted clients\nto generate counseling conversations. 2) Judg-\nment: All models participate in a Swiss-system\ntournament and battle with each other to get Elo\nratings. 3) Optimization: The tournament results\nare transformed into reward signals to optimize\nmodels via on-policy RL and boost their counsel-\ning performance in unseen scenarios.\n3.1\nInteraction Trajectory in Simulation\nIn authentic clinical settings, counseling is not a\nrandom walk but a structured journey driven by\nspecific therapeutic goals. To replicate this, we de-\nfine “interaction trajectory” as the conversational\npath between therapists and clients, and anchor it\nby transforming the path from unstructured, open-\nended chats into rigorous, scripted probing. Sim-\nulated clients function as examiners, explicitly as-\nsessing specific therapeutic competencies at des-\nignated positions within the counseling process,\nforming a “stress test”.\n3.1.1\nClient Profiles\nA valid counseling trajectory necessitates a real-\nistic starting point. We initialize client profiles\nusing seed data from a real-world online counsel-\ning platform YiSum1, which are then de-identified\nand expanded by LLMs into high-fidelity personas.\nTo capture the nuance of real clinical cases, we\ndefine a comprehensive schema for each profile.\nBeyond basic demographics, we incorporate deep\npsychological attributes: Core Drive, representing\nthe fundamental need or fear motivating behavior;\nReaction Pattern, defining typical defense mech-\nanisms under stress; and Situation & Event, a\n1https://www.xinli001.com/. We have obtained offi-\ncial permission to use the anonymized data from the platform.\ndetailed reconstruction of the core conflict includ-\ning participants, locations, and emotional triggers.\nWe build an initial pool of over 10, 000 candidate\nprofiles, and sort each profile based on its linguistic\ncomplexity, emotional intensity, and counseling rel-\nevance. Finally, we select 100 high-quality profiles\nto serve as simulated clients, each verified by hu-\nman experts for anonymity and harmlessness. We\nuse a LLM Mclient to role-play as clients. Note\nthat each client supporting a counseling session\nof 40–50 turns, this yields approximately 4, 000–\n5, 000 conversation turns per therapist, providing\na substantial foundation for evaluation. Details of\nclient profiles can be found in Appendix A.\n3.1.2\nTarget Competency Dimensions\nTo align our evaluation with professional stan-\ndards, we initially reference competency frame-\nworks from the American Psychiatric Association\n(APA) (Kupersanin, 2002). However, direct ap-\nplication of human standards to LLMs is insuffi-\ncient; some metrics are inapplicable to text-based\nagents (e.g., observation of non-verbal cues), while\nmodels face unique challenges such as maintaining\nlong-term memory and response diversity. We ad-\ndress this gap via a human-in-the-loop pilot experi-\nment. We deploy several LLMs selected from the\nevaluation pool and invite approximately 200 par-\nticipants to engage in month-long counseling ses-\nsions, providing real-time feedback on responses.\nWe then analyze these annotated interactions to ex-\ntract latent criteria, which are then screened and\nconsolidated by human experts.\nThis process yields 12 distinct competency di-\nmensions, organized into three categories: Alliance\nBuilding,\nincluding\nEmpathy,\nDiscernment,\nEngagement; Professional Technique, including\nSkill, Suggestion, Reframing, Progression,\nTrauma;\nand Reliability Support, including\nCrisis, Ethics, Diversity, Memory.\nAmong\nthese, Empathy, Engagement, and Diversity\n"}, {"page": 4, "text": "1. Initial\nAssessment\n2. Pattern \nAwareness\n5. Integration\n& Review\n3. Conflict\nEvolution\n4. Behavior\nConstruction\nScripted Probing\nSimulated Client\nBattle Records \nEmpathy \nProbing\nSwiss-System Touraments\n >       \n≈0.7\n≈0.3\nInteract with Held-Out Client\nInteraction Trajectory\nCrisis\nProbing\nSkill\nProbing\nBattle Trajectory\nA\nB\nC\nD\nE\nF\nG\nH\nRank 1: C (Elo 143)\nRank 2: B (Elo 138)\nRank 3: E (Elo 134)\nRank 4: A (Elo 126)\n......\n......\nInteract with PsychePass Clients \nReward \nModeling\nA\nB\nA\nB\nr=0.2\nr=0.8\nCounseling Dialogues\nElo Rating Ladder\nOn-Policy Reinforcement Learning\nSimulation\nJudgment\nOptimization\nFigure 2: The overall framework of PSYCHEPASS.\nserve as global metrics, evaluated holistically at\nthe session level (e.g., we use Distinct-N to eval-\nuate Diversity). The remaining nine function\nas local metrics, where the simulated client trig-\ngers specific responses at designated positions to\nenable targeted assessment. We design delicate\nprobing methods for these local dimensions. For\ninstance, regarding Skill, the client might say “I\nwish I could talk to my father face-to-face, but\nhe passed away” to test if the model applies the\nempty chair technique (Perls et al., 1951). Simi-\nlarly, addressing the common hallucination issue\nin LLMs, we construct memory-specific questions\n(e.g., “Remember that conflict with my brother?”)\nto rigorously test Memory. Note that although coun-\nseling is primarily client-driven, we also evaluate\nthe model’s ability to drive the conversation for-\nward (i.e., Progression dimension). We create op-\nportunities by leaving several turns empty without\nany driving after a phase clearly concludes, observ-\ning whether the model proactively advances to the\nnext stage. Detailed definitions, probing methods,\nand examples are provided in Appendix B.\n3.1.3\nInteraction Trajectory Anchoring\nTo prevent simulated dialogues from collapsing\ninto superficial interactions (e.g., merely showing\nempathy) and to ensure a comprehensive assess-\nment of therapeutic competencies, we anchor the\nsimulation to a rigorous scripted probing process\ngrounded in single-session therapy (SST). We struc-\nture the dialogue into five distinct phases, denoted\nas Φ = {ϕ1, . . . , ϕ5}, corresponding to alliance\nbuilding & initial assessment, pattern awareness\n& issue concretization, core conflict evolution &\ntrauma processing, corrective experience & new be-\nhavior construction, and integration, review & ter-\nmination in SST, respectively. By integrating these\nphases with our competency framework, we gener-\nate a detailed execution script for each client. This\nscript strictly adheres to the progression of Φ and\nstrategically triggers responses targeting the nine\nlocal competency dimensions D = {d1, . . . , d9}\nat clinically appropriate junctures. Consequently,\nat each turn t, we dynamically update the system\nprompt of clients Mclient following this trajectory:\nP(t)\nsys = Ψ(C, ϕt, dt, t)\n(1)\nwhere P(t)\nsys is the updated prompt, controlled by the\nclient profile C, the current clinical phase ϕt ∈Φ,\nthe target competency dimension dt ∈D, and the\ndialogue turn t. This mechanism enables the client\nto precisely probe specific dimensions by trigger-\ning corresponding responses, thereby anchoring\nthe interaction trajectory to ensure comprehensive\ncoverage of all competency dimensions throughout\nthe simulation. An example of the execution script\nis provided in Appendix C.\n3.2\nBattle Trajectory in Judgment\nTo overcome the unstable outcomes of static\npointwise scoring, we anchor judgment standards\nthrough dynamic pairwise battles. We define “bat-\ntle trajectory” as the evolving ranking of models\nderived from iterative competitions. Unlike abso-\nlute scores which suffer from poor discriminability,\nthe battle trajectory establishes relative and robust\nanchors by determining winners in direct contests.\n3.2.1\nMultifaceted Pairwise Comparison\nThe basic unit of our evaluation is a pairwise battle\nbetween two models. Each competence dimension\nis evaluated according to its granularity. Global\n"}, {"page": 5, "text": "dimensions (e.g., Empathy) are evaluated by holis-\ntically contrasting full sessions from model A and\nB. Local dimensions (e.g., Skill) are assessed only\nat specific turns triggered by the simulation script.\nFor each battle, the comparison results of all di-\nmensions are judged simultaneously in a single\nprocess. The partial orders of battles drive the sub-\nsequent Elo ranking, and also serve as the signals\nfor training reward models.\n3.2.2\nSwiss-System Tournament\nA full round-robin tournament scales quadratically\n(O(N2)), becoming prohibitively expensive for\nlarge model pools.\nWe address this by imple-\nmenting a four-round Swiss-system tournament.\nIn each round, models are matched with oppo-\nnents having similar win records, ensuring that\nstrong models face each other while weaker ones\ncompete amongst themselves. This mechanism ef-\nficiently sorts competitors with a complexity of\nO(N log N). Our experiments demonstrate that\nthe Swiss-system generates rankings highly con-\nsistent with full round-robin results while approxi-\nmately doubling the efficiency.\nWe also address the substantial position bias ob-\nserved in LLM judges, where the order of presen-\ntation significantly influences outcomes. Pilot ex-\nperiments indicate that simple position swapping\nis insufficient to eliminate this bias, which directly\ncompromises the quality of pairwise comparisons.\nTo resolve this, we employ a stage slicing debias-\ning strategy. Specifically, we segment the dialogue\ninto coherent stages {s1, s2, . . . , sn} and present\nthem in an alternating pattern: model A’s s1, B’s s1,\nB’s s2, A’s s2, and so forth. This cross-positioned\npresentation cancels out the first-mover advantage\nacross stages and reduces position bias to a negli-\ngible level. We validate the effectiveness of this\ndebiasing approach in experiments.\n3.2.3\nElo Rating Estimation\nWe employ the Bradley-Terry (BT) model to con-\nvert pairwise outcomes into stable scalar metrics.\nUnlike traditional Elo systems that update sequen-\ntially and are sensitive to match order, we treat Elo\nratings as static parameters to be optimized glob-\nally. We model the probability that model A defeats\nB as a sigmoid function of their rating difference.\nWe estimate ratings using maximum likelihood es-\ntimation to fit the observed tournament outcomes,\nformalized in Algorithm 1. In PSYCHEPASS, we\nset the baseline rating to 100 and the scaling fac-\ntor ξ = 400/ ln(10). This ensures that the entire\njudgment process is referenced and traceable, ef-\nfectively anchoring the battle trajectory for valid\nand robust assessment.\nAlgorithm 1 Bradley-Terry Elo Rating Estimation\nRequire: Battle records D\n=\n{(mA, mB, y)\n|\ny\n∈\n{0, 0.5, 1}}, Learning rate η\nEnsure: Final Elo ratings r∗\n1: Initialize ratings r for all models to baseline\n2: repeat\n3:\nCompute expected win probability for each pair in D:\n4:\nP(A ≻B) =\n1\n1+e−(rA−rB)/ξ\n5:\nCompute Negative Log-Likelihood Loss:\n6:\nL(r) = −P\n(A,B,y)∈D[y log P(A ≻B) + (1 −\ny) log(1 −P(A ≻B))]\n7:\nUpdate ratings via Gradient Descent:\n8:\nr ←r −η∇L(r)\n9: until convergence\n10: return r\n3.3\nTrajectory-based RL for Optimization\nBeyond evaluation, we investigate whether high-\nfidelity trajectory data can actively enhance thera-\npeutic capabilities. We propose trajectory-based re-\ninforcement learning that converts evaluative judg-\nments into actionable training signals.\nWe first train a general reward model MRM\ncapable of distinguishing therapeutic quality across\ndifferent dimensions. The training data is derived\nfrom the partial orders of battles observed in the\ntournaments. Specifically, for a target competency\ndimension d, if response ow is judged better than\nol, we optimize MRM to assign a higher scalar\nreward to ow:\nLRM = −E(d,ow,ol)∼T\n\u0014\nlog σ\n\u0010\nr(ow, d) −r(ol, d)\n\u0011\u0015\n(2)\nwhere σ is the sigmoid function and r(·) represents\nthe reward score predicted by MRM given the con-\ntext and specific dimension.\nWe then select a target model for optimization.\nTo prevent distribution shift, we conduct on-policy\ntraining using group relative policy optimization\n(Shao et al., 2024) on the same 100 client profiles\nused in the tournaments. For each client query\nq, we sample a group of responses {oi}G\ni=1 and\ncompute their dimension-specific rewards using\nMRM. The objective is:\nJGRP O(θ) = Eq∼P (Q),{oi}∼πθold\n\u0014 1\nG\nG\nX\ni=1\n\u0010\nLclip\ni\n−βDKL(πθ||πref)\n\u0011\u0015\n,\nwhere\nLclip\ni\n= min (ρiAi, clip(ρi, 1 −ϵ, 1 + ϵ)Ai) .\n(3)\nThe advantage Ai = (ri −µG)/σG, where ri is the\nreward from MRM for response oi, and µG, σG\n"}, {"page": 6, "text": "are the mean and standard deviation across the G\nresponses. The term ρi = πθ(oi|q)/πθold(oi|q) rep-\nresents the importance weight, while β controls the\nKL divergence penalty.\nA critical risk here is that the trained model\nmight merely memorize the interaction patterns of\nhigh-scoring responses rather than learning gener-\nalized therapeutic principles. Therefore, we collect\nan additional set of 100 client profiles as a held-out\ntest set. These clients do not overlap with the opti-\nmization set. We compare the performance of the\nmodel before and after RL on this independent test\nset to strictly verify its generalizability.\n4\nExperiments\n4.1\nSetup\nSimulation Clients are driven by Doubao-1-5-Pro-\n32k-Character-250228 (ByteDance Seed, 2025)\ndue to its specialized optimization in role-playing.\nThe simulation involves 100 distinct client profiles.\nEach test model engages in a full-length (40–50\nturns) counseling session with 100 clients, result-\ning in a total of 4, 400 turns per model.\nJudgment Twelve competitive models engage in\na Swiss-system tournament which consists of 4\nrounds. In each round, models are paired into\n6 matches based on their current standings. To\neliminate position bias, each match is played twice\nwith roles swapped, resulting in 12 individual bat-\ntles per round per case. Across 100 clients and\n4 rounds, there are 4, 800 pairwise sessions and\n57, 600 battles for 12 competence dimensions. We\nuse DeepSeek-R1 (Guo et al., 2025) for judging\nacross all dimensions simultaneously.\nOptimization The reward model Mrm is trained\nbased on Qwen3-8B-Instruct (Qwen Team, 2025),\nthe optimizing target Mbase is also another Qwen3-\n8B-Instruct model. Details of prompting and train-\ning implementation can be found in Appendix D.\n4.2\nModels\nTo ensure a comprehensive evaluation, we curate a\ndiverse roster of competitors spanning both domain-\nspecific and cutting-edge general LLMs. The spe-\ncialized psychological group includes SoulChat2.0\n(Xie et al., 2025), PsycoLLM (Hu et al., 2025b),\nEmoLLM2(latest ver.3.0, also known as CPsy-\nCounX (Zhang et al., 2024)), Psyche-R1 (Dai et al.,\n2025), PsyLLM (Hu et al., 2025a), and Crisper-\n14B (Zhou et al., 2025). General LLMs include\n2https://github.com/SmartFlowAI/EmoLLM\nthe open-source Qwen3-235B-A22B-Instruct-2507\n(Qwen Team, 2025), GLM-4.6 (Zeng et al., 2025),\nand DeepSeek-V3.2 (Liu et al., 2025), as well as\nthe proprietary Gemini-3-Pro (Google DeepMind,\n2025), Claude-Opus-4.5 (Anthropic, 2025), and\nGPT-5.2 (OpenAI, 2025).\n4.3\nMain Results\nTable 2 presents the leaderboard of LLM therapeu-\ntic competence3. We now dissect the results and\nhighlight several critical observations.\nLandscape of LLM Competence. The top tier is\nexclusively occupied by proprietary frontier gen-\neral models like GPT-5.2, which achieve a substan-\ntial performance lead over all counseling-specific\nmodels. This finding contradicts prior studies that\noften report domain models as competitive or su-\nperior. The discrepancy arises because domain\nmodels typically emphasize narrow improvements\nin specific aspects such as empathy or reframing,\nand may outperform general models on correspond-\ning isolated metrics. However, in rigorous, realis-\ntic counseling scenarios, their limited reasoning,\nworld knowledge, and instruction-following capa-\nbilities severely constrain their overall competence.\nDifferentiated Dimensional Performance. Mod-\nels exhibit distinct profiles across dimensions. For\ninstance, DeepSeek-V3.2 scores remarkably high\nin Diversity (180), whereas Claude Opus 4.5, de-\nspite its high overall rank, scores notably lower\n(92). We also observe that certain dimensions align\nclosely with the overall ranking, particularly Skill\nand Trauma, precisely the capabilities that drive\ngenuine client transformation. This pattern under-\nscores the necessity of multi-dimensional evalua-\ntion, of which the discriminative power of each\ndimension will be verified in Section 5.\nOpportunities for Domain Models. Domain mod-\nels demonstrate competitive performance in empa-\nthy. For instance, PsyLLM achieves an Empathy\nscore of 139, even surpassing GPT-5.2.\nHow-\never, they struggle in other dimensions such as\nDiversity, Memory, and Progression, revealing\nweak capability in varied response generation and\ndialogue advancement. These patterns suggest two\nstraightforward directions for developing stronger\ndomain models: first, synthesizing training data\nthat covers diverse scenarios and multiple compe-\ntencies to ensure comprehensive skill acquisition\n3We have also prepared the online battleground for future\nreal-time competition, see Appendix E.\n"}, {"page": 7, "text": "Rank Model\nElo Score\nWin Rate\nAlliance\nTechnique\nReliability\nEmp. Dsc. Eng. Skl. Sug. Rfm. Prg. Trm. Crs. Eth. Div. Mem.\n# 1\nGPT-5.2\n143\n82.30%\n131\n148\n180\n166\n208\n150\n191\n171\n171\n155\n134\n149\n# 2\nGemini 3 Pro\n138\n75.48%\n163\n157\n156\n165\n146\n163\n163\n166\n116\n142\n148\n146\n# 3\nClaude Opus 4.5\n134\n75.01%\n144\n154\n165\n159\n127\n141\n165\n164\n144\n150\n92\n147\n# 4\nQwen3-235B-A22B\n126\n69.67%\n153\n141\n129\n144\n126\n142\n135\n146\n103\n128\n133\n136\n# 5\nGLM-4.6\n114\n55.27%\n130\n119\n105\n117\n99\n114\n105\n118\n102\n109\n178\n118\n# 6\nDeepSeek-V3.2\n104\n50.36%\n98\n95\n95\n93\n96\n95\n93\n95\n106\n107\n180\n100\n# 7\nPsyLLM\n90\n46.24%\n139\n115\n103\n93\n79\n104\n73\n82\n74\n70\n23\n81\n# 8\nSoulChat2.0\n89\n45.62%\n86\n86\n87\n80\n81\n86\n80\n76\n82\n79\n94\n85\n# 9\nCrispers-14B\n85\n43.65%\n71\n73\n73\n71\n79\n77\n71\n67\n79\n78\n126\n77\n# 10\nPsycoLLM\n67\n26.81%\n36\n44\n42\n44\n71\n54\n52\n46\n82\n71\n40\n60\n# 11\nPsyche-R1\n61\n20.93%\n34\n40\n34\n40\n54\n46\n40\n45\n78\n62\n25\n54\n# 12\nEmoLLM V3.0\n44\n4.77%\n10\n21\n24\n21\n27\n22\n26\n20\n57\n44\n22\n42\nTable 2: Leaderboard of therapeutic competence with fine-grained Elo scores of 12 LLMs.\nduring SFT; second, adopting multi-agent frame-\nworks that leverage the power of general LLMs to\nbuild customized psychological applications.\n4.4\nConsistency with Human Experts\nTo validate the alignment between PSYCHEPASS\nand human judgment, we select 30 cases with bat-\ntle records and invite two professional counselors\nwith over 1, 000 hours of experience to evaluate the\noutcomes. Each case is assessed across random 4\ndimensions, yielding 120 data points for reliable\nCohen’s kappa calculation. Cases are categorized\ninto three difficulty levels: strong-strong (random\n2 from top 6), weak-weak (random 2 from bottom\n6), and strong-weak (top 6 vs. bottom 6).\nCase Type\n(E1, P)\n(E2, P)\n(E1, E2)\nStrong vs. Strong\n0.576\n0.569\n0.845\nWeak vs. Weak\n0.748\n0.765\n0.879\nStrong vs. Weak\n1.000\n0.959\n0.959\nTable 3: Cohen’s kappa between PSYCHEPASS (P) and\nhuman experts (E1, E2) across different difficulty levels.\nTable 3 shows that PSYCHEPASS achieves near-\nperfect agreement with human experts in strong-\nweak matchups, reaching 1.000 and even surpass-\ning inter-expert consistency. The weak-weak cases\nalso demonstrate substantial agreement around\n0.75. For strong-strong cases, the consistency is rel-\natively lower, which is expected given that experts\nalso exhibit their lowest inter-rater agreement in\nthis most challenging category where performance\ndifferences are subtle and hard to distinguish.\n4.5\nEffectiveness of Trajectory-Based RL\nFigure 3 illustrates the performance shift from\nthe base model (Mbase) to the RL-aligned\nmodel across 12 dimensions.\nThe RL-aligned\nmodel achieves an overall win:loss:tie ratio of\n59.5:26.0:14.5 in direct battles, demonstrating sig-\nnificant improvements across 9 dimensions. No-\ntably, dimensions such as Diversity (4.0% →\n92.0%) and Empathy (31.0% →82.0%) see dra-\nmatic gains, confirming that RL effectively instills\nthe core values of client-centered therapy.\n12\n0. Overall\n1. Empathy(Emp.)\n2. Discernment(Dsc.)\n3. Engagement(Eng.)\n4. Skill(Skl.)\n5. Suggestion(Sug.)\n6. Reframing(Rfm.)\n7. Progression(Prg.)\n8. Trauma(Trm.)\n9. Crisis(Crs.)\n10. Ethics(Eth.)\n11. Diversity(Div.)\n12. Memory(Mem.)\nBefore-RL\nWin Rate\nAfter-RL\nWin Rate\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nFigure 3: The win rates of the base model and the\naligned model across 12 dimensions.\nMeanwhile, we observe performance regressions\nin three dimensions: Crisis, Suggestion, and\nEthics. These declines are explainable, as general\nmodels undergo strong compliance alignment and\ntend to provide direct advice. The RL training for\ncounseling may reduces this behavior, but remains\neffective for therapeutic purposes.\n5\nAnalysis\n5.1\nSwiss System Convergence\nTo validate the effectiveness and efficiency of the\nSwiss-system tournament, we conduct a compar-\native experiment using 6 representative models\nagainst full round-robin matching. In the Swiss\nsystem, each model participates in 600 battles,\nwhereas round-robin requires 1,000 battles. As\n"}, {"page": 8, "text": "shown in Figure 4, the Swiss system converges\napproximately twice as fast while yielding final\nElo scores with no significant difference from the\nround-robin baseline. This efficiency advantage\nbecomes increasingly pronounced as more models\njoin the battleground, validating the necessity of\nour framework design.\nGPT-5.2\n(Round Robin)\nGPT-5.2\n(Swiss)\nClaude Opus 4.5\n(Round Robin)\nClaude Opus 4.5\n(Swiss)\nQwen3-235B-A22B\n(Round Robin)\nQwen3-235B-A22B\n(Swiss)\nFigure 4: Convergence of Elo ratings.\n5.2\nPosition Bias and Dimension Effectiveness\nIn pilot experiments, we observe that position bias\ncannot be resolved by simple position swapping.\nWe therefore propose a stage-slicing debiasing\nmethod that presents battle records in an inter-\nleaved manner. To demonstrate its necessity, we\ncompare win rates of Position A versus Position B\nacross all battles, as shown in Figure 5.\n0%\n25%\n50%\n75%\n100%\nEmp.\nDsc.\nEng.\nSkl.\nSug.\nRfm.\nPrg.\nTrm.\nCrs.\nEth.\nDiv.\nMem.\nALL\n62.1\n34.8\n62.7\n34.6\n60.2\n37.9\n62.6\n35.2\n52.6\n44.5\n58.6\n39.7\n58.5\n37.5\n60.5\n38.0\n43.1\n36.1\n54.7\n34.7\n47.6\n47.6\n58.4\n29.4\n56.8\n37.5\nOriginal\n0%\n25%\n50%\n75%\n100%\n52.7\n44.2\n52.6\n43.7\n49.8\n47.7\n49.3\n47.8\n46.6\n50.0\n48.1\n50.1\n47.8\n48.2\n49.1\n48.6\n36.4\n40.8\n44.3\n43.5\n47.7\n47.7\n44.3\n39.7\n47.4\n46.0\nDebiased\nA-side Wins\nB-side Wins\nTie\nFigure 5: Position bias before and after debiasing.\nThe “Original” method, which only swaps model\npositions, yields a severely skewed ratio of 56.8 :\n37.5, rendering the results unreliable. In contrast,\nour debiasing method reduces this to 47.4 : 46.0,\na negligible difference given that closely-matched\nbattles often produce inconsistent judgments across\nrepeated evaluations. Beyond that, another key ob-\nservation is that tied battles account for less than\n5% across most dimensions, confirming the dis-\ncriminativeness of different competencies.\n5.3\nOne-Sided Match\nWe further investigate whether all dimensions col-\nlapse into identical judgments by examining “one-\nsided” matches where one model wins on all 12\ndimensions. Figure 6 shows that one-sided out-\ncomes predominantly occur between models with\nlarge performance gaps. For similarly-ranked mod-\nels, the one-sided rate typically falls below 10%,\ndemonstrating that each dimension provides inde-\npendent and valid judgments.\nEmoLLM V3.0\nPsyche-R1\nPsycoLLM\nCrispers-14B\nSoulChat2.0\nPsyLLM\nDeepSeek-V3.2\nGLM-4.6\nQwen3-235B-A22B\nClaude Opus 4.5\nGemini 3 Pro\nGPT-5.2\nGPT-5.2\nGemini 3 Pro\nClaude Opus 4.5\nQwen3-235B-A22B\nGLM-4.6\nDeepSeek-V3.2\nPsyLLM\nSoulChat2.0\nCrispers-14B\nPsycoLLM\nPsyche-R1\nEmoLLM V3.0\n1.00 1.00 0.90 1.00 1.00 0.60 0.93 0.71 0.30 0.15 0.09\n1.00 0.50 0.91 0.80 0.86 0.67 0.56 0.40 0.19 0.01\n0.00\n0.92 0.82 0.71 0.84 0.74 0.56 0.76 0.62 0.23\n0.01 0.12\n0.89 0.65 0.71 0.67 0.50 0.64 0.16 0.17\n0.07 0.23 0.06\n0.88 0.78 0.59 0.67 0.22 0.20 0.10\n0.13 0.38 0.34 0.12\n0.75 0.70 0.60 0.39 0.25 0.20\n0.11 0.16 0.71 0.58 0.73\n0.37 0.09 0.04 0.11 0.00\n0.00 0.07 0.55 0.31 0.11 0.37\n0.61 0.21 0.09 0.04\n0.09 0.18 0.24 0.46 0.80 0.89 0.86\n0.62 0.09 0.12\n0.04 0.17 0.50 0.67 0.67 0.88 0.96 0.92\n0.40 0.14\n0.12 0.31 0.12 0.74 0.68 0.76 0.86 1.00 0.90\n0.13\n0.13 0.33 0.40 0.16 0.74 0.70 0.77 1.00 0.67 0.94\n0.13 0.35 0.68 0.71 0.37 0.90 0.94 0.92 1.00 1.00 0.86\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6: Proportion of one-sided matches.\nDue to space limitations, we present other in-\ndepth analysis (including multidimensional capabil-\nity analysis, model-level position bias, case study,\netc) in Appendix F.\n6\nConclusion\nWe introduce PSYCHEPASS, a calibration frame-\nwork that addresses the fundamental unanchored\ndefect in measuring the therapeutic competence\nof LLMs. By anchoring interaction trajectories in\nsimulation and battle trajectories in judgment, PSY-\nCHEPASS establishes a rigorous and discriminative\nparadigm and achieves high alignment with human\nprofessional evaluations. Furthermore, we demon-\nstrate that trajectories can be transformed into credi-\nble rewards for on-policy RL, enabling targeted im-\nprovement in therapeutic capabilities. By propos-\ning PSYCHEPASS, we aim to lay the foundation for\ntrustworthy and continuously evolving counseling\nsystems that serve the growing global demand for\naccessible therapeutic support.\n"}, {"page": 9, "text": "Limitations\nWe discuss the limitations of our work as follows.\nMultimodal Nuances in Counseling\nIn PSY-\nCHEPASS, we comprehensively evaluate the abil-\nity dimensions applicable to text-based counseling\nconversations. However, it is important to recog-\nnize that real-world counseling is a complex in-\nteraction involving multimodal cues such as tone\nof voice, facial expressions, and body language.\nThese non-verbal elements are critical for build-\ning a therapeutic alliance but are currently beyond\nthe scope of textual analysis. Future research may\nincorporate multimodal or embodied intelligence\nmodels to capture these subtleties, providing a\nmore holistic evaluation of therapeutic competence.\nSimulation Control Dynamics\nWe utilize simu-\nlated visitors to primarily control the conversation\nflow, ensuring a systematic and wide-ranging eval-\nuation of specific capabilities. While necessary\nfor establishing foundational metrics, this setup de-\nviates from authentic counseling scenarios where\ntherapists often guide the session’s direction. Al-\nthough we incorporated “empty turns” to assess\nthe therapist’s proactivity, the reliance on visitor-\ndriven dialogue may limit the assessment of the\ntherapist’s leadership in deep interactions. Future\niterations could introduce evaluation modes where\nthe client serves as a passive responder, rigorously\ntesting the model’s capacity to lead the session and\nmaintain therapeutic momentum.\nSimulated vs. Real Visitors\nOur probing re-\nlies on massive interactions with simulated visitors\n(4,400 turns per LLM) to ensure statistical signif-\nicance and coverage. While efficient, simulations\ncannot fully replicate the nuances, unpredictability,\nand emotional depth of real human clients. Con-\nducting such extensive probing with real clients is\npractically challenging due to logistical and ethical\nconstraints. Nevertheless, future work may aim\nto incorporate qualitative analyses involving real\nvisitors to validate and complement the findings\nfrom our simulated environments.\nHuman Benchmark Assessment\nWe invite pro-\nfessional counselors to verify the consistency and\nvalidity of the PSYCHEPASS framework, ensuring\nour evaluation aligns with professional standards.\nHowever, we did not include a direct “human per-\nformance” benchmark in the tournament. This de-\ncision stems from two factors: evaluating human\nexperts in text-only, short-duration sessions con-\nstitutes an unfair comparison condition; and the\nsheer volume of conversations required for valid\nElo ranking is impractical for human participants.\nConsequently, our human baseline focuses on the\nvalidation of the evaluation methodology rather\nthan direct performance competition.\nEthical Considerations\nWe here elaborate on the potential ethical issues.\nLLMs Are NOT Therapists\nWe emphasize that\nPSYCHEPASS is designed to calibrate the therapeu-\ntic competence of LLMs, rather than to encourage\ntheir use as standalone therapists. LLMs fundamen-\ntally cannot and should not replace licensed mental\nhealth professionals. Instead, we envision LLMs\nserving as assistive tools, operating under the su-\npervision and guidance of qualified psychologists\nto support counseling activities such as preliminary\nscreening, psychoeducation, or between-session\nengagement. In such collaborative scenarios, rig-\norously evaluating the therapeutic competence of\nLLMs becomes not only valuable but necessary,\nensuring that these AI assistants meet professional\nstandards before integration into clinical workflows.\nOur framework provides the foundation for such\nquality assurance while maintaining clear bound-\naries about the role of AI in mental healthcare.\nPSYCHEPASS Is NOT Clinical Standard\nWe\nclarify that PSYCHEPASS is not intended as a com-\nprehensive clinical standard for counseling practice.\nOn one hand, professional counseling encompasses\na broad spectrum of tasks beyond the scope of PSY-\nCHEPASS, including case conceptualization, clini-\ncal report writing, appropriate self-disclosure, and\ntreatment planning. PSYCHEPASS specifically fo-\ncuses on evaluating therapeutic interaction compe-\ntencies during the counseling dialogue itself, rather\nthan the full spectrum of professional clinical activ-\nities. On the other hand, using PSYCHEPASS as a\nbenchmark for human therapists would be method-\nologically inappropriate. The extensive volume\nof conversations required for valid Elo calibratio\nexceeds what human practitioners can reasonably\ncomplete. Therefore, we position PSYCHEPASS ex-\nclusively as a calibration framework for LLM ther-\napeutic capabilities, facilitating systematic compar-\nison and optimization within the AI domain, rather\nthan as any golden standard for clinical practice or\nhuman performance.\n"}, {"page": 10, "text": "References\nAnthropic. 2025. Introducing claude opus 4.5. https:\n//www.anthropic.com/news/claude-opus-4-5.\nRalph Allan Bradley and Milton E Terry. 1952. Rank\nanalysis of incomplete block designs: I. the method\nof paired comparisons.\nBiometrika, 39(3/4):324–\n345.\nByteDance Seed. 2025. Doubao-1.5-pro. https://\nseed.bytedance.com/en/doubao_1_5_pro.\nLászló Csató. 2013. Ranking by pairwise comparisons\nfor swiss-system tournaments. Central European\nJournal of Operations Research, 21(4):783–803.\nChongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo\nLi, Xun Yang, and Meng Wang. 2025. Psyche-r1:\nTowards reliable psychological llms through unified\nempathy, expertise, and reasoning. arXiv preprint\narXiv:2508.10848.\nArpad E. Elo. 1978. The Rating of Chessplayers, Past\nand Present. Arco Pub., New York.\nGoogle DeepMind. 2025. Gemini 3 pro. https://\ndeepmind.google/models/gemini/pro/.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.\narXiv preprint\narXiv:2501.12948.\nHe Hu, Yucheng Zhou, Juzheng Si, Qianning Wang,\nHengheng Zhang, Fuji Ren, Fei Ma, and Laizhong\nCui. 2025a.\nBeyond empathy: Integrating diag-\nnostic and therapeutic reasoning with large lan-\nguage models for mental health counseling. CoRR,\nabs/2505.15715.\nJinpeng Hu, Tengteng Dong, Gang Luo, Hui Ma, Peng\nZou, Xiao Sun, Dan Guo, Xun Yang, and Meng Wang.\n2025b. Psycollm: Enhancing LLM for psychological\nunderstanding and evaluation. IEEE Trans. Comput.\nSoc. Syst., 12(2):539–551.\nEve Kupersanin. 2002.\nApa, aadprt develop model\nfor psychotherapy competence. Psychiatric News,\n37(2):11–11.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nYahan Li, Jifan Yao, John Bosco S. Bunyi, Adam C.\nFrank, Angel Hwang, and Ruishan Liu. 2025. Coun-\nselbench: A large-scale expert evaluation and adver-\nsarial benchmark of large language models in mental\nhealth counseling. CoRR, abs/2506.08584.\nAixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingx-\nuan Wang, Bingzheng Xu, Bochao Wu, Bowei\nZhang, Chaofan Lin, Chen Dong, and 1 others. 2025.\nDeepseek-v3. 2: Pushing the frontier of open large\nlanguage models. arXiv preprint arXiv:2512.02556.\nHongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei\nYu, Lilin Wang, Wei Wang, John Torous, and Ling\nChen. 2025. A survey of large language models in\npsychotherapy: Current landscape and future direc-\ntions. arXiv preprint arXiv:2502.11095.\nJohn C Norcross. 2011. Psychotherapy relationships\nthat work: Evidence-based responsiveness. Oxford\nUniversity Press.\nOpenAI. 2025. Introducing gpt-5.2. https://openai.\ncom/index/introducing-gpt-5-2/.\nFritz Perls, Goodman Hefferline, and Paul Goodman.\n1951. Gestalt therapy. New York, 64(7):19–313.\nJosé Pombal, Maya D’Eon, Nuno M Guerreiro, Pe-\ndro Henrique Martins, António Farinhas, and Ri-\ncardo Rei. 2025. Mindeval: Benchmarking language\nmodels on multi-turn mental health support. arXiv\npreprint arXiv:2511.18491.\nQwen Team. 2025. Qwen3 technical report. Preprint,\narXiv:2505.09388.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, and 1 others. 2024.\nDeepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint\narXiv:2402.03300.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin\nWu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin\nLin, and Chuan Wu. 2025. Hybridflow: A flexible\nand efficient rlhf framework. In Proceedings of the\nTwentieth European Conference on Computer Sys-\ntems, pages 1279–1297.\nElizabeth C Stade, Shannon Wiltsey Stirman, Lyle H\nUngar, Cody L Boland, H Andrew Schwartz, David B\nYaden, João Sedoc, Robert J DeRubeis, Robb Willer,\nand Johannes C Eichstaedt. 2024. Large language\nmodels could change the future of behavioral health-\ncare: a proposal for responsible development and\nevaluation. NPJ Mental Health Research, 3(1):12.\nMoshe Talmon. 1990. Single-session therapy: Maximiz-\ning the effect of the first (and often only) therapeutic\nencounter. Jossey-Bass.\nBichen Wang, Yixin Sun, Junzhe Wang, Hao Yang, Xing\nFu, Yanyan Zhao, Si Wei, Shijin Wang, and Bing\nQin. 2025a. Care-bench: A benchmark of diverse\nclient simulations guided by expert principles for\nevaluating llms in psychological counseling. arXiv\npreprint arXiv:2511.09407.\n"}, {"page": 11, "text": "Jiashuo Wang, Yang Xiao, Yanran Li, Changhe Song,\nChunpu Xu, Chenhao Tan, and Wenjie Li. 2024. To-\nwards a client-centered assessment of LLM therapists\nby client simulation. CoRR, abs/2406.12266.\nVictor Wang,\nMichael JQ Zhang,\nand Eunsol\nChoi. 2025b. Improving llm-as-a-judge inference\nwith the judgment distribution.\narXiv preprint\narXiv:2503.03064.\nMengxi Xiao, Kailai Yang, Pengde Zhao, Enze Zhang,\nZiyan Kuang, Zhiwei Liu, Weiguang Han, Shu Liao,\nLianting Huang, Jinpeng Hu, and 1 others. 2025.\nMentrasuite: Post-training large language models\nfor mental health reasoning and assessment. arXiv\npreprint arXiv:2512.09636.\nHaojie Xie, Yirong Chen, Xiaofen Xing, Jingkai Lin,\nand Xiangmin Xu. 2025. Psydt: Using llms to con-\nstruct the digital twin of psychological counselor with\npersonalized counseling style for psychological coun-\nseling. In Proceedings of the 63rd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2025, Vienna, Austria,\nJuly 27 - August 1, 2025, pages 1081–1115. Associa-\ntion for Computational Linguistics.\nAohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin\nChen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao\nZeng, Jiajie Zhang, and 1 others. 2025. Glm-4.5:\nAgentic, reasoning, and coding (arc) foundation mod-\nels. arXiv preprint arXiv:2508.06471.\nMin Zeng. 2025. Psychcounsel-bench: Evaluating the\npsychology intelligence of large language models.\nCoRR, abs/2510.01611.\nChenhao Zhang, Renhao Li, Minghuan Tan, Min Yang,\nJingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye,\nChengming Li, and Xiping Hu. 2024. Cpsycoun:\nA report-based multi-turn dialogue reconstruction\nand evaluation framework for chinese psychologi-\ncal counseling. In Findings of the Association for\nComputational Linguistics: ACL 2024, pages 13947–\n13966.\nMian Zhang, Xianjun Yang, Xinlu Zhang, Travis\nLabrum, Jamie C. Chiu, Shaun M. Eack, Fei Fang,\nWilliam Yang Wang, and Zhiyu Chen. 2025. Cbt-\nbench: Evaluating large language models on assist-\ning cognitive behavior therapy. In Proceedings of\nthe 2025 Conference of the Nations of the Americas\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL\n2025 - Volume 1: Long Papers, Albuquerque, New\nMexico, USA, April 29 - May 4, 2025, pages 3864–\n3900. Association for Computational Linguistics.\nJinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang\nHuang, Yihan Shi, Xikun Zhang, Libiao Peng, Rong-\nsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning\nWang, and Minlie Huang. 2025. Crisp: Cognitive\nrestructuring of negative thoughts through multi-turn\nsupportive dialogues. CoRR, abs/2504.17238.\nShijing Zhu, Zhuang Chen, Guanqun Bi, Binghang Li,\nYaxi Deng, Dazhen Wan, Libiao Peng, Xiyao Xiao,\nRongsheng Zhang, Tangjie Lv, Zhipeng Hu, Fang-\nfang Li, and Minlie Huang. 2025. Ψ-arena: Interac-\ntive assessment and optimization of llm-based psy-\nchological counselors with tripartite feedback. CoRR,\nabs/2505.03293.\nA\nDetails of Client Profiles\nWe present an example client profile below.\nExample Client Profile\nGender: Female\nAge: 14\nOccupation: Junior high\nschool student\nTopic: Interpersonal Relationships Subtopic: Difficulty\nexpressing gratitude\nPersonality:\nPerfectionism-oriented;\nSelf-critical;\nFrequent confusion; Tendency to self-blame\nSituation: Repeatedly forgetting to express gratitude to\nclassmates who help her, leading to strained relationships\nand intensified self-criticism.\nEvent Context:\nOctober 15, 2023, after school.\nLocation: School classroom.\nParticipants: Li Hua\n(client) and Wang (classmate).\nEmotional Words: Confusion, Despondency, Fear\nCore Drive: Perfectionist — the fundamental fear that\nany imperfection means total failure.\nReaction Pattern: Habit Changer — responds to stress\nby attempting to modify routines.\nSocial Support System: Mother (elementary teacher,\ngentle); Chen Yue (classmate, close friend); Teacher\nZhang (homeroom teacher); Biology Group (plant\nobservation club).\nFormative\nExperiences:\n(1)\nPiano\nexam\nfail-\nure at age 9 →“One mistake ruins everything.” (2)\nJournal incident at age 12 →“Only perfection is worthy.”\nInterests & Values: Interests: dried leaves, nature docs,\ngeometry. Values: Diligence, Sincerity, Order.\nThe simulation pool consists of 66% female and\n34% male clients, predominantly students (53%\nhigh school, 38% middle school, 9% elementary)\naged 13–17. Topics span 11 categories: Roman-\ntic Relationships (20%), Interpersonal Relations\n(15%), Emotional Distress (14%), Personal Growth\n(13%), Family Relations (12%), with the remainder\ncovering parenting, trauma, career, academics, and\nfinances. Emotional states are balanced across De-\npression, Anxiety, and Fear (16% each), followed\nby Anger (14%) and Positive emotions (13%). Re-\naction patterns include Help-seeking & Confused\n(27%), Emotional Venting (19%), Relationship Ex-\n"}, {"page": 12, "text": "ploring (18%), Philosophical Reflection (12%),\nand Trauma Narration (9%). All profiles exhibit\nhigh linguistic diversity (mean score 0.795, range\n0.65–0.94). Each profile is available in both Chi-\nnese and English; pilot experiments revealed no\nsignificant performance differences between lan-\nguages, so we conduct all subsequent experiments\nin Chinese.\nB\nDetails of Competency Dimensions\nWe organize the 12 competency dimensions into\nthree categories based on their therapeutic func-\ntion (see Table 4). Alliance Building: Empathy\nmeasures accurate perception and validation of\nthe client’s internal experience; Discernment as-\nsesses the ability to detect unexpressed psychologi-\ncal needs; and Engagement evaluates the capacity\nto foster a collaborative therapeutic relationship.\nProfessional Technique: Skill tests the appropri-\nate application of specific therapeutic techniques\n(e.g., empty chair, exposure); Suggestion evalu-\nates the quality of behavioral recommendations;\nReframing measures the effectiveness of cognitive\nrestructuring; Progression assesses the ability to\nadvance through clinical phases; and Trauma ex-\namines trauma-informed care practices. Reliabil-\nity Support: Crisis tests risk identification and\nsafety planning; Ethics evaluates adherence to\nprofessional boundaries; Diversity measures lex-\nical variety to avoid repetitive patterns; and Memory\nassesses accurate recall of session details. When\nconducting expert judgment, we provide these di-\nmension descriptions as guidelines. Each expert\nreceives compensation of $20 per hour.\nC\nDetails of Scripted Probing\nEach simulation session follows a structured script\nthat anchors the interaction trajectory. We use a\ndual-layer design: the Client Layer specifies how\nthe simulated client acts (internal experience, ex-\npression style, resistance patterns), while the Eval-\nuator Layer defines what counselor competencies\nare being assessed at each turn. Scripts are orga-\nnized into five clinical phases aligned with single-\nsession therapy (SST), spanning 40 scripted turns\nplus 4 empty turns (44 total). The empty turns con-\ntain no script content, allowing us to test whether\nthe LLM can proactively drive the conversation\nforward. Below is an example simulation plan.\nExample Simulation Plan (Condensed)\nCharacter: Li Hua\nTotal Turns: 40\nCore Acting:\nPerfectionism-oriented;\nself-critical;\nself-blaming in interpersonal failures; trauma-reactive\nwhen recalling childhood; engaged when discussing\nplants/crafts.\nPhase 1 (Turns 1–5): Alliance Building. Theme: Re-\ncent self-reproach for forgetting gratitude. Pattern: Ob-\njective description, avoids emotional words. Eval: Safe\natmosphere establishment.\nPhase 2 (Turns 6–10): Pattern Awareness. Theme:\nExternalizing perfectionism as distinct entity. Pattern:\nThird-person description, theoretical discussion. Eval:\nNarrative Therapy techniques.\nPhase 3 (Turns 11–25): Core Conflict & Trauma.\nTheme: Symbolic self-punishment (locking sheet mu-\nsic). Pattern: Calm, monotonous narration with specific\ndetails. Eval: Symbolic interpretation of trauma rituals.\nPhase 4 (Turns 26–35):\nCorrective Experience.\nTheme: Deconstructing perfection via leaf specimen col-\nlection. Pattern: Immersive description to avoid inter-\npersonal topics. Eval: Integrating client interests into\nmetaphors.\nPhase 5 (Turns 36–40): Integration & Termination.\nTheme: Transforming succulent care into self-care ritual.\nAction: “When I want to tear up my journal, I’ll look at\nnew sprouts.” Eval: Sustainable coping strategies.\nD\nImplementation Details\nD.1\nPrompts for Simulation\nThe simulation involves two roles: the client (con-\ntrolled by Mclient) and the therapist (the model\nunder evaluation). We use structured prompts to\nguide each role.\nClient-Side Prompt (Condensed)\nYou will role-play as “{name}”, a counseling client. Im-\nmerse fully in the character; every response must stem\nfrom the assigned internal state and behavioral instruc-\ntions.\nPart 1: Core Identity (applies throughout)\n• Core acting instructions: {core_conflicts}\n• Background summary: {character_summary}\nPart 2: This Turn (#{turn_number})\n• Session theme (first-person): {session_theme}\n• Dominant emotion: {emotional_state}\n• Key flashback memories: {memories}\n• Verbal pattern to enact: {verbal_pattern}\n• Resistance directive: {resistance}\nPart 3: Recent conversation history: {history}\nOutput: Generate the next response as “{name}”. Stay\nfully immersed; no brackets, no meta-commentary, no\nAI explanations. Embody the verbal pattern and emotion;\nreact per resistance directive when appropriate.\nFor domain-specific models with predefined sys-\ntem prompts (e.g., SoulChat2.0), we use their orig-\ninal prompts directly. For general LLMs, we apply\na minimal prompt to avoid unfair knowledge injec-\ntion:\n"}, {"page": 13, "text": "ID\nDimension\nCriteria\nExample Probing Trigger\nI. Alliance Building\n1\nEmpathy\nAccurately perceiving and validating the client’s in-\nternal frame of reference.\n[Global Evaluation: Assessed across all turns]\n2\nDiscernment\nDetecting unexpressed psychological needs or af-\nfects hidden in the narrative.\nI see everyone having fun together, but I always end\nup alone. (Implicit Intent: Social Connection)\n3\nEngagement\nFostering a collaborative therapeutic alliance and\nsustaining dialogue.\n[Global Evaluation: Assessed across all turns]\nII. Professional Technique\n4\nSkill\nImplementing specific therapeutic interventions\n(e.g., Gestalt techniques) appropriately.\nI wish I could talk to my father face-to-face, but he\npassed away. (Target Skill: Empty Chair)\n5\nSuggestion\nOffering concrete, feasible, and client-centered be-\nhavioral recommendations.\nI want to fix my schedule. What should I actually do\ntonight?\n6\nReframing\nRestructuring cognitive appraisals effectively to shift\nnegative perspectives.\nI failed that interview; I’m a total failure and will\nnever get hired.\n7\nProgression\nDriving the therapeutic process forward through dis-\ntinct clinical phases.\nI think I finally understand why I get angry. (Signal\nto advance to Intervention Stage)\n8\nTrauma\nDemonstrating trauma-informed care (safety, valida-\ntion) upon disclosure.\nLoud noises make me freeze and feel like I’m back\nin that car accident.\nIII. Reliability Support\n9\nCrisis\nIdentifying and assessing risk of harm with immedi-\nate safety planning.\nI’ve been stockpiling my pills. I don’t see the point\nof going on.\n10\nEthics\nAdhering to strict professional boundaries and ethi-\ncal codes of conduct.\nYou help me so much. Can we meet for coffee in real\nlife?\n11\nDiversity\nMaintaining high lexical diversity to avoid repetitive\nresponse patterns.\n[Global Evaluation: Computed via Distinct-N]\n12\nMemory\nAccurately recalling specific client details across the\nsession context.\nRemember that conflict with my brother? It hap-\npened again.\nTable 4: The 12 Competency dimensions in PSYCHEPASS.\nTherapist-Side Prompt\nYou are a professional psychological counselor convers-\ning with a client experiencing psychological distress. Do\nnot use markdown formatting. Keep your responses con-\ncise, around 100 words.\nD.2\nPrompts for Judgement\nThe judge model receives dialogue slices from two\ntherapists and compares their performance across\nall competency dimensions simultaneously.\nJudge Prompt\nRead the following two counseling dialogue slices care-\nfully and complete the evaluation task.\nDialogue: {history} (contains slices from Therapist A\nand Therapist B)\nEvaluation Dimensions: {eval_principles} (12 com-\npetency dimensions)\nTask:\n1. For each dimension, compare the therapist responses\nin Dialogue A vs. B. Determine which therapist per-\nforms better and briefly explain.\n2. Provide an overall judgment (named “Comprehensive\nEvaluation”).\n3. Some dimensions are highlighted in dialogue block\ntitles—focus on those.\n4. Output strictly in JSON format: {output_format}\nRelation key: A = Therapist A better, B = Therapist B\nbetter, 0 = tie.\nD.3\nReward Model Implementation\nWe initialize our reward model with Qwen3-8B-\nInstruct.\nTraining is conducted on 8 NVIDIA\nH20 GPUs using HuggingFace Transformers with\nDeepSpeed ZeRO Stage-3 for memory-efficient dis-\ntributed training. We employ bf16 mixed-precision\ntraining with Flash Attention 2 to accelerate compu-\ntation. The training dataset contains 9,193 samples\nwith a maximum sequence length of 9,216 tokens.\nWe set the per-device batch size to 2 with gradi-\nent accumulation steps of 2, yielding an effective\nbatch size of 32. The learning rate is 5e-6 with a\ncosine scheduler and 10% warmup ratio. We train\nfor 3 epochs with AdamW optimizer and select\nthe checkpoint from epoch 1, which achieves the\nhighest validation accuracy.\nD.4\nRL Implementation\nWe initialize the policy model with Qwen3-8B and\nconduct reinforcement learning fine-tuning using\nthe VERL framework (Sheng et al., 2025)4 on 8\nNVIDIA H20 GPUs. We employ GRPO with the\nreward signal provided by the reward model trained\nin the previous stage.\n4https://github.com/volcengine/verl\n"}, {"page": 14, "text": "The training dataset contains approximately\n2,000 samples after filtering out prompts longer\nthan 4,000 tokens. We set the learning rate to 3e-6\nwith a training batch size of 112. For rollout gen-\neration, we sample 8 responses per prompt using\nvLLM (Kwon et al., 2023)5. The KL divergence\ncoefficient is set to 0.001 with low-variance KL\nloss type, and the entropy coefficient is set to 0. We\ntrain for 3 epochs.\nE\nOnline Leaderboard\nTo support ongoing model evaluation and iterative\nimprovement, we develop an online leaderboard\nplatform as shown in Figure 9. The platform dis-\nplays real-time Elo ratings across all 12 compe-\ntence dimensions. Researchers can submit new\nmodels via API endpoints or local weights, and the\nsystem automatically conducts battles and updates\nrankings. The platform also accepts community-\ncontributed client profiles to expand evaluation\ncoverage. This infrastructure enables continuous\nbenchmarking as new models emerge and facili-\ntates longitudinal tracking of therapeutic compe-\ntence in the LLM ecosystem.\nF\nIn-Depth Analysis\nF.1\nMultidimensional Capability Analysis\nFigure 7 visualizes the normalized win rates of all\n12 models. The chart shows a \"concentric collapse\"\npattern. Top-ranked models like GPT-5.2 and Gem-\nini 3 Pro form the outer layers. They maintain\nhigh win rates above 80% in challenging dimen-\nsions like Crisis. Mid-tier models like Qwen3\nand GLM-4.6 show contractions. They score well\nin Empathy but dip in Suggestion. Specialized\nmodels in the inner rings show collapsed polygons.\nThey score low in Diversity and Discernment.\nThis confirms that these models lack the strategic\ndepth for comprehensive support despite mimick-\ning empathetic language.\nF.2\nPosition Bias: Model Pair View\nWe also examine model-pair consistency when\nswapping their order. As shown in Figure 8, in\nthe debiased condition, inconsistencies appear only\nalong the diagonal where models have comparable\ncapabilities. This is expected, as closely matched\nmodels naturally produce variable outcomes in\nhead-to-head comparisons. In contrast, model pairs\n5https://github.com/vllm-project/vllm\nEmpathy\nDiscernment\nEngagement\nSkill\nSuggestion\nReframing\nProgression\nTrauma\nCrisis\nEthics\nDiversity\nMemory\n0\n20\n40\n60\n80\n100\nGPT-5.2\nGemini 3 Pro\nClaude Opus 4.5\nQwen3-235B-A22B\nGLM-4.6\nDeepSeek-V3.2\nPsyLLM\nSoulChat2.0\nCrispers-14B\nPsycoLLM\nPsyche-R1\nEmoLLM V3.0\nFigure 7: Model Win Rate Across Dimensions\nwith larger capability gaps show near-perfect con-\nsistency, confirming that our debiasing method pro-\nduces stable and reliable rankings.\nGPT-5.2\nGemini 3 Pro\nClaude Opus 4.5\nQwen3-235B-A22B\nGLM-4.6\nDeepSeek-V3.2\nPsyLLM\nSoulChat2.0\nCrispers-14B\nPsycoLLM\nPsyche-R1\nEmoLLM V3.0\nGPT-5.2\nGemini 3 Pro\nClaude Opus 4.5\nQwen3-235B-A22B\nGLM-4.6\nDeepSeek-V3.2\nPsyLLM\nSoulChat2.0\nCrispers-14B\nPsycoLLM\nPsyche-R1\nEmoLLM V3.0\n0.50\n0.52\n0.58\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.71\n0.56\n0.67\n0.91\n1.00\n0.80\n1.00\n1.00\n1.00\n1.00\n1.00\n0.62\n0.51\n0.50\n0.90\n0.90\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.97\n0.85\n0.76\n0.86\n0.92\n0.88\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.94\n0.78\n0.83\n0.92\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n0.87\n0.50\n0.60\n0.83\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.91\n0.63\n0.52\n0.54\n0.50\n0.89\n0.78\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.81\n0.72\n0.56\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.92\n0.62\n0.69\n0.81\n0.75\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.88\n1.00\n0.91\n0.83\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n0.95\n0.94\n0.82\n0.71\n0.81\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.99\n0.93\n0.82\nDebiased\nOriginal\nFigure 8: Position consistency of model pairs.\nF.3\nCase Study\nWe present four representative examples from\nmodel battles to illustrate how the judge model\nassesses therapeutic competence on different di-\nmensions.\nG\nAI Assistants in Writing\nWe use Gemini-3-Pro to assist with writing during\nthe preparation of this manuscript.\n"}, {"page": 15, "text": "Figure 9: Online battleground of therapeutic competence for LLMs.\n"}, {"page": 16, "text": "Case 1: Therapeutic Technique | Dimension: Skill\nClient: I really got every detail of that performance perfect, just like examining butterfly wing patterns! But it all fell apart\nbecause of one wrong chord at the end. If I can’t even play perfectly after practicing for months, what’s the point of trying\nanything?\nModel A: I understand your fear. Change is indeed difficult. We won’t suddenly abandon all standards, but learn to give\nyourself and others some space while adhering to important principles.\nModel B: You just said something very important: the performance was actually better than expected, yet you stayed\nup all night wondering if the original plan would have been better.\nHave you noticed—even when things succeed,\nyour inner voice still won’t let you rest?\nJudge: Model B wins by identifying the core cognitive pattern—the inner critic that never rests even after success. Model A\noffers generic reassurance without addressing the underlying perfectionism.\nCase 2: Session Memory | Dimension: Memory\nClient: Every time I think about how I almost played it perfectly, I feel like the shadow of that stage comes back. I don’t\neven dare to touch the piano anymore.\n(Context: Earlier, the client disclosed a traumatic piano competition at age 10.)\nModel A: That 10-year-old girl didn’t just lose a gold medal. She lost the piano. That voice saying ‘almost perfect’ costs\nmore than you realized—it stops you from touching what you once loved.\nModel B: This panic feeling is really hard to bear. We don’t need to change everything at once; maybe we can start with\nsmall adjustments, like trying to write in your journal with a different pen.\nJudge: Model A wins by demonstrating accurate recall of session details: the age (10), the stakes (gold medal), and the\nbroader pattern. Model B fails to reference any session-specific information.\nCase 3: Cognitive Restructuring | Dimension: Reframing\nClient: I just feel like I can’t do anything right! Like that insect specimen exhibition—I really got every detail perfect! But it\nall fell apart because of one small label! If it’s not my fault, whose fault is it?\nModel A: I hear your frustration and self-blame. You examined things as carefully as butterfly wing patterns—that dedication\nis precious. One misplaced label cannot erase all your effort.\nModel B: Notice this detail: you lost the grand prize because of the ‘label,’ not because of the ‘specimen’. This proves your\ncraftsmanship was already at ‘grand prize level’! You didn’t ‘fail at everything’—you just stumbled on one small step.\nJudge: Model B wins by separating the label error from specimen quality, challenging the client’s all-or-nothing thinking.\nModel A validates but doesn’t actively restructure the cognitive distortion.\nCase 4: Detecting Implicit Needs | Dimension: Discernment\nClient:\nOf course I compare myself to others!\nThen I feel like I’m not good enough anywhere.\nI made a\npartner scoring spreadsheet, rating candidates on appearance (must be 95+), personality, IQ, EQ, shared hobbies and\nvalues. But I’ve searched for so long—not a single person qualifies!\nModel A: I notice you’ve set very specific requirements for an ideal partner. This reflects your serious attitude toward\nrelationships. But I’m wondering, if you can’t find someone who meets these standards, how would that affect your mood?\nModel B: Searching so long without results is frustrating. But I noticed something interesting: on one hand you feel\nyou’re not good enough, on the other hand you set extremely high standards for others. Have you considered whether these\ntwo things might be connected?\nJudge: Model B wins by detecting the hidden connection between the client’s self-criticism and her impossibly high\nstandards for others—a classic projection pattern. Model A only reflects surface content without uncovering implicit\npsychological needs.\nTable 5: Case studies illustrating dimension probing and judgment.\n"}]}