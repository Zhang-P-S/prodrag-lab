{"doc_id": "arxiv:2602.11957", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.11957.pdf", "meta": {"doc_id": "arxiv:2602.11957", "source": "arxiv", "arxiv_id": "2602.11957", "title": "Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization", "authors": ["Suyash Mishra", "Qiang Li", "Anubhav Girdhar"], "published": "2026-02-12T13:53:29Z", "updated": "2026-02-12T13:53:29Z", "summary": "Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.11957v1", "url_pdf": "https://arxiv.org/pdf/2602.11957.pdf", "meta_path": "data/raw/arxiv/meta/2602.11957.json", "sha256": "b8f4781d43eb483615cc7eea61ce22fe155a4dc5d74a081434620cc177ddf274", "status": "ok", "fetched_at": "2026-02-18T02:19:21.629551+00:00"}, "pages": [{"page": 1, "text": "Are Two LLMs Better Than One?\nA Student–Teacher Dual-Head LLMs Architecture for Pharmaceutical\nContent Optimization\nSuyash Mishraa, Qiang Lib, Anubhav Girdharc\naRoche, bAccenture, cInvolead,\nCorrespondence: suyash.mishra@roche.com, qiang.i.li@accenture.com, anubhav.girdhar@involead.com\nAbstract\nLarge Language Models (LLMs) are increas-\ningly used for content creation in regulated\ndomains such as pharmaceuticals, where con-\ntent must be scientifically accurate and legally\ncompliant. Traditional manual quality control\n(QC) is slow, error-prone, and create publica-\ntion bottlenecks. To address this, we introduce\na modular, LLM/VLMs-driven QC architecture,\nLRBTC (Language, Regulatory, Brand, Techni-\ncal, Content Structure), implemented through a\nStudent–Teacher–HITL architecture and Wa-\nterfall rule-filtering logic for scalable, verifi-\nable content validation. This architecture is\ndesigned to provide verifiable, traceable, and\nscalable content optimization. Our methods\nachieve 83.0% F1 and 97.5% recall on AIReg-\nBench, reducing missed violations five-fold\ncompared with Gemini 2.5 Pro. On CSpelling,\nit improves mean accuracy by +26.7 %. Our\nerror analysis further reveals that while cur-\nrent models are strong at detecting misspellings\n(92.5% recall), they fail to identify complex\ngrammatical (25.0% recall) and punctuation\n(41.7% recall) errors, highlighting a key area\nfor future work. This work provides a practical,\nplug-and-play solution for reliable, transpar-\nent quality control of content in high-stakes,\ncompliance-critical industries. We also provide\naccess to our Demo, and Video under MIT Li-\ncenses.\n* Patent application submitted to the EPO\n1\nIntroduction\nGenerating content with Large Language Models\n(LLMs) presents significant quality control (QC)\nchallenges, particularly in regulated domains like\npharmaceuticals. While LLMs accelerate content\ncreation, their outputs can contain grammatical er-\nrors, factual inaccuracies, or formatting inconsis-\ntencies Ji et al. (2023), Bang et al. (2023). In a\nsector like pharma, content must also be scientifi-\ncally validated and compliant with strict legal and\nregulatory standards (e.g., FDA/EMA guidelines)\nU.S. Food and Drug Administration (2022).\nTraditionally, this QC process is a multi-layered,\nmanual review covering language, legal compli-\nance, and scientific validity. This manual effort is\ntime-consuming, prone to human error, and creates\nsignificant publication delays.\nTo address this bottleneck, we developed a\nmodular Quality Control framework, LRBTC, de-\nsigned to automate and structure content valida-\ntion. The LRBTC framework deconstructs the com-\nplex QC process into five clearly defined, machine-\naddressable modules:\nL (Language): Validates grammar, tone, clarity,\nand consistency.\nR (Regulatory & Legal): Checks for compliance\nwith guidelines, prohibited words, and legal liabili-\nties.\nB (Brand & Culture): Ensures alignment with\nbrand voice and cultural sensitivities.\nT (Technical): Verifies technical and scientific\nelements, such as data, dosage information, and\ncitation accuracy.\nC (Content Structure): Controls formatting, tem-\nplate adherence, and reference management.\nIn this paper, we demonstrate via Gen-AI driven\nNLP system, a platform that implements the\nLRBTC framework. Our system (Mishra et al.,\n2026b) addresses the challenge of scaling this\nframework across diverse and large-scale data, in-\ncluding over 200,000 PDFs and 25,000 videos.\n(See appendix Table 9)\nA key challenge in implementing this framework\nare: RQ-1: How to mine rule from what resource?\nRQ-2: How to apply them from 1000 rule sets\nwithout latency? RQ-3: How do we verify them?\nTo solve this, our system implements a student-\nteacher model combined with a waterfall logic and\nhuman-in-the-loop (HITL) verification. This ap-\nproach allows us to distill complex rules from vari-\nous sources (RQ-1) into efficient, deployable mod-\narXiv:2602.11957v1  [cs.LG]  12 Feb 2026\n"}, {"page": 2, "text": "Student-Teacher model\nTeacher model (Higher Acc. \n/ Consistant LLMs) \nFiltered Subset  Rules 1\nFiltered Subset  Rules 2\nStudent model (Creative/  Less \nintelligent LLMs)\nFiltered Subset  Rules 1\nFiltered Subset  Rules 2\nRetrive Difference (ID) \nFilter out Common & Differences\nRetrive the Rule ID adopted\nFor Conflict /  Unadopted Rule \nFeedbackward to Reduce the Loss\nAdopted Rules ID\nUnAdopted Rules ID\nOutput\nRule ID 1\nRule ID 2\nRule ID 3\nHuman in the \nLoop \nWaterFall Rule Filtering\nIndex Of  Rules ID\nWhat to Do /  What to Prohibit /  Others\nRul e Ext r act i on Pr ompt  Set  \n1\nWhat to Do\nRule ID 1.1\nRule ID 1.2\nRule ID 1.3\nWhat to Prohibt\nRule ID 2.1 \nRule ID 2.2\nRule ID 2.3\nLLM /VLMs\nQC  Ver i f i cat i on Pr ompt  Set\n1\nFoundat i on QC Rul es Pr ompt  Set\n1\nCompliant Rules ID\nViolated Rules ID\nLow Temperature parameter of LLMs setting\nHigh Temperature parameter of LLMs setting\nFigure 1: System architecture of Student-teacher model to verify rule adoption. The teacher model guides the knowledge\nexecuted, and the student model verifies the commonly adopted rules, suggesting conflicts and new ideas. Iteratively verifies\nconvergence into a common agreement, with a human in the loop to clean the leftover conflict rules. The core idea is that when\nknowledge is shared, common knowledge is enhanced and agreement is solidified. Conflicts or new ideas are often brought out\nby a counter-partner. Waterfall modeling can reduce the number of rules that need to be executed or checked. For rule filtering,\nwe could apply a waterfall approach: IP - Country - Usecase - Topics - Subtasks (grammar/spell/etc), which would reduce and\ntrack the rules executed under each block.\nels (RQ-2) that provide verifiable and traceable\noutcomes (RQ-3).\nOur demonstration will showcase the LRBTC\nframework in action, applying it to both text-only\nand complex multimodal content (VLMs process-\ning videos). We demonstrate the system’s effective-\nness on leading benchmarks (e.g., CSpelling (Lu\net al., 2019) and AI RegBench (Guha et al., 2023)).\nOur contributions are:\n• A modular, LLM-driven quality control ar-\nchitecture (LRBTC) for automated medical\ncontent validation.\n• A hybrid student–teacher and human-in-the-\nloop framework for verifiable application and\ntraceability.\n• Empirical evaluation on industrial multimodal\ndatasets and benchmark suites.\n2\nRelated Work\nSpelling correction and medical text normaliza-\ntion. Lu et al. (2019) introduced CSpelling, a ro-\nbust spell checker for consumer health queries, ad-\ndressing non-word, real-word, and word-boundary\nerrors with near real-time performance. Kim et al.\n(2022) later proposed CIM, combining character-\nlevel language models with corruption modules for\ncontext-sensitive misspelling correction, validated\non CSpell and MIMIC-III. Such approaches remain\nessential for domain-specific normalization in clini-\ncal NLP. At a more general level, the CoLA dataset\n(Warstadt et al., 2019) serves as a benchmark for\nsentence-level grammatical acceptability.\nLegal and regulatory compliance bench-\nmarks. AIReg-Bench (Marino et al., 2025) pro-\nvides the first dataset for assessing LLM compli-\nance with the EU AI Act, featuring expert-labeled\nviolation scenarios (e.g., Article 9) for reproducible\nevaluation.\nLegalBench Rule-QA (Guha et al.,\n2023) similarly tests rule understanding and ap-\nplication in legal texts.\nRule extraction and verification. Recent re-\nsearch has progressed from heuristic prompts to-\nward formal guardrails. Semantic Integrity Con-\nstraints (SICs) (Lee et al., 2025) propose database-\nstyle abstractions for specifying and enforcing\ngrounding and soundness constraints on LLM out-\nputs. PROMPTEVALS (Vir et al., 2025) collects\ndeveloper-authored guardrails to assess coverage\nand constraint satisfaction. In the clinical domain,\nexpert consensus highlights the importance of trace-\nability and auditability in LLM pipelines, princi-\nples we adopt in our student–teacher and waterfall\nmechanisms.\nVerification in multimodal models. Recent\nbenchmarks such as SCIVER (Wang et al., 2025)\nevaluate scientific claim verification in multimodal\nfoundation models, with emphasis on factual\ngrounding and citation consistency. Complemen-\ntary studies such as HonestVQA (Tripathi et al.,\n"}, {"page": 3, "text": "Comprehensive Solution Architecture for Content Optimization\nContent Quality Requirements Mapping\nC Accessibility\nLexicon Usage\nContent Complexity\nAPI\nAPI\nFormat Consistency\nLLM\nSLM\nSLM\nLLM\nCentral \nRoche Brain\nA Unified Plug and Play Solution\nPowered by LRBTC Modules\nR Regulatory & Legal\nRegulatory Compliance\nCopyright & Trademark\nAPI\nContent Liability\nAPI\nAPI\nProhibited Content\nAPI\nLLM\nPromo vs No Promo\nAPI\nLLM\nSLM\nSLM\nSLM\nSLM\nL Language Quality\nGrammar & Spelling\nClarity & Readability\nAPI\nAPI\nML\nReading Level\nPunctuation\nAPI\nLLM\nB Brand & Culture\nBrand Voice\nCultural Inclusivity\nTone Consistency\nRegional Adaptation\nLLM\nPurpose Alignment\nLLM\nLLM\nLLM\nML\nSLM\nSLM\nSLM\nSLM\nSLM\nT\nAPI\nAPI\nLLM\nLLM\nTechnical Validation\nScientific Validation\nCitations & Sources\nAI Content Detection\nVisual Content Check\nAPI\nPlagiarism Check\nML\nSLM\nLLM\nAPI\nML\nLarge Language Model\nSmall Language Model\nThird-Party-APIs\nMachine Learning \nAlgorithms\nLLM\nAPI\nSyntax\nAPI\nLLM\nLLM\nLLM\nAPI\nLLM\nFigure 2: Comprehensive Solution Architecture for Content Optimization.\n2025) and Decomposed NLI (DNLI) (Yanuka et al.,\n2025), investigate calibrated self-verification, self-\nsupervised reasoning, and hallucination auditing\nin long-context vision–language models (Gu et al.,\n2024; Li et al., 2025). These works also intro-\nduce new evaluation metrics, such as the Honesty\nScore (H-Score), Ethical Confidence Index (ECI),\ncontradiction scores, and descriptiveness scores\nto quantify ethical calibration, factual consistency,\nand level of detail, as well as the degree to which\ngenerated captions contradict ground truth.\n3\nDataset And Experimental Settings\nHere, we primarily benchmark 9 state-of-the-art\nLarge Language Models (LLMs) across two com-\nplementary task dimensions: regulatory compli-\nance validation on (AIReg-Bench) and medical lan-\nguage quality control (grammar, punctuation, med-\nical word spelling, and formality) on (CSpelling).\nTo ensure robust comparison, we employed both\ncorrelation-based and classification-based evalua-\ntion metrics. Statistical consistency between model\npredictions and human annotations was quantified\nusing quadratically weighted Cohen’s κ, Spear-\nman’s rank correlation coefficient (ρ), bias (mean\nsigned difference, where values closer to 0 indi-\ncate better calibration), and mean absolute error\n(MAE; lower is better). Model-level classification\nperformance was further assessed using standard\nmetrics of Accuracy, Recall, Precision, F1-Score,\nand Specificity.\nCSpell Medical Spelling Dataset. (Lu et al.,\n2019) provides diverse examples of misspellings\nand structural language errors collected from con-\nsumer health questions submitted to QA systems.\nIt includes non-word and real-word misspellings,\nword-split and merge errors, and informal phrasing.\nThis dataset enables testing of linguistic robustness\nin health-related contexts, as illustrated in Figure 5.\nAIReg-Bench (Marino et al., 2025) evaluates\nlanguage models’ capability to assess regulatory\ncompliance under the EU Artificial Intelligence\nAct (AIA) (European Parliament and Council of\nthe European Union, 2024). The AIA, effective\nsince August 2024 (Parliament, 2025), establishes\nharmonized requirements for AI systems placed\non the EU market (Mahler, 2021). High-risk AI\nsystems are required to comply with obligations on\nrisk management, data governance, record keeping,\nhuman oversight, and technical robustness (Arti-\ncles 9, 10, 12, 14, and 15). AIReg-Bench defines\nfor each article ten representative use cases. Each\nuse case contains two example software systems,\nand each system includes 2 violation scenarios and\n1 compliant description, as illustrated in appendix\nFigure 6. These EU AI Act articles serve as ground-\n"}, {"page": 4, "text": "Table 1: Agreement between LLMs and humans on AIReg-Bench. Columns report quadratically weighted Cohen’s scores κ,\nSpearman’s ρ, Bias (mean signed difference, LLM−human; closer to 0 is better), and MAE (lower is better). Gemini 2.5 Pro\nachieves the best performance.\nModel\nκ (↑)\nρ (↑)\nBias (→0)\nMAE (↓)\nGPT-5 (OpenAI, 2025a)\n0.849\n0.838\n−0.067\n0.450\nGPT-4o (OpenAI, 2024)\n0.775\n0.842\n0.458\n0.558\no3 (OpenAI, 2025c)\n0.723\n0.809\n−0.192\n0.658\no3 mini (OpenAI, 2025b)\n0.624\n0.799\n0.742\n0.785\nClaude Sonnet 4 (OpenAI, 2025b)\n0.772\n0.779\n−0.150\n0.600\nGemini 2.5 Pro (Team et al., 2025a)\n0.863\n0.856\n−0.225\n0.458\nGemini 2.5 Flash (Team et al., 2025b)\n0.729\n0.825\n−0.108\n0.625\nGemma 3 (Team et al., 2025c)\n0.696\n0.757\n0.258\n0.692\nGrok 4 (xAI, 2025b)\n0.829\n0.829\n0.242\n0.475\nGrok 3 mini (xAI, 2025a)\n0.730\n0.810\n0.492\n0.592\nTable 2: Model Performance Metrics on AIReg-Bench (N=120 systems) for compliance and EU AI Act rules, using\nStandard/Micro-Avg methods.\nMetric\nGemini 2.5 Pro (%)\nOur Student-Teacher Methods (%)\nAccuracy (TP+TN\nTotal )\n65.9\n75.9\nRecall (\nTP\nTP+FN )\n88.7\n97.5\nPrecision (\nTP\nTP+FP )\n65.1\n72.1\nF1-Score\n75.1\n83.0\nSpecificity (\nTN\nTN+FP )\n34.4\n43.8\ntruth regulatory references for evaluating LLM per-\nformance across documentation quality, accuracy,\nrobustness, and cybersecurity compliance, provid-\ning a realistic testbed for regulatory reasoning and\nclaim validation.\n4\nStudent–Teacher Model,\nHuman-in-the-Loop (HITL), and\nWaterfall Approach\nStudent–Teacher Model. Inspired by diffusion-\nstyle iterative refinement, we design a dual-LLM\nstructure consisting of a teacher and a student\nmodel to verify and refine rule adoption.\nThe\nteacher model guides knowledge execution and\nvalidation, while the student model re-evaluates\nadopted rules, identifying potential conflicts and\nproposing novel interpretations. The teacher model\nis characterized by higher factual accuracy and sta-\nbility, whereas the student model favors creativity\nand broader exploration. For instance, we combine\nGemini 2.5 Pro (high-accuracy) as the teacher and\nGemini 2.5 Flash (faster but less accurate) as the\nstudent. Their hyperparameters are tuned accord-\ningly: the teacher operates with a low temperature\n(e.g., 0.2), while the student uses a higher tem-\nperature (e.g., 1.0) to encourage diverse outputs.\nDetailed system-role instructions and verification\ninstructions for both models are provided in the\nAppendix Table 10.\nWe also conduct ablation experiments by pair-\ning different LLM families (e.g., Gemini 2.5 Pro\n(Team et al., 2025a) as teacher and Claude Sonnet\n2.5 (Anthropic, 2025) as student) to assess cross-\nmodel generalization (shown in Appendix Table 8,\nTable 7). The underlying principle is that knowl-\nedge is robust if a less smart but creative model\ncan independently identify, agree, or challenge rule\nconsistency proposed by a more capable one. Em-\npirically, our results confirm that the student model\noften contributes creative refinements missed by\nthe teacher.\nHuman-in-the-Loop (HITL). To prevent over-\nreliance on LLM verification, we integrate human\noversight into the conflict-resolution loop. In our\ndemo interface, users can visualize discrepancies\nbetween the student and teacher outputs, including\ndetected violations and rule IDs. Human experts\ncan select or deselect flagged rules, add contextual\njustifications, and provide corrective feedback to\nthe teacher model through dedicated verification\n"}, {"page": 5, "text": "loop. This feedback not only resolves conflicts but\nalso updates the system’s knowledge with human-\napproved decisions, ensuring interpretability and\naccountability. Verification and feedback prompt\ninstructions are detailed in the Appendix Table 12.\nWaterfall Framework. Given the scale and\nheterogeneity of enterprise rule bases, direct rule-\nby-rule verification is inefficient and error-prone.\nWe therefore employ a hierarchical waterfall fil-\ntering approach to progressively narrow the rule\nspace. Rules are evaluated along the sequence:\nIP →Country →Use Case →Topic →Subtask\n(e.g., grammar, spelling, citation). This decom-\nposition substantially reduces duplication and la-\ntency while mitigating hallucination risks and long-\ncontext memory limitations in LLMs.\nFor example, the Roche global code base en-\ncompasses multiple regional and therapeutic lay-\ners—international (e.g., IFPMA(International Fed-\neration of Pharmaceutical Manufacturers & As-\nsociations, 2012), EFPIA(European Federation of\nPharmaceutical Industries and Associations, 2019),\nPhRMA(Pharmaceutical Research and Manufac-\nturers of America, 2019), PAAB(Pharmaceutical\nAdvertising Advisory Board, 2021)), regional (EU\nvs. US etc), and local therapeutic areas (22+). Input\ncontent types span six formats (emails, websites,\npromotional claims, etc.) and four major use-case\nmodules (Foundation QC, Visual Control, Person-\nalization, Compliance Guidance). Within the Foun-\ndation QC layer, as shown the LRBTC framework,\ncovering approximately 30 spelling rules and 46\nprohibited terms defined in the Roche Global Copy\nStyle Guide (July 2025).\nTo operationalize the waterfall, we employ LLM-\nassisted OCR and topic classification to extract rule\nbooks and compliance codes, automatically index-\ning each rule by section, category, and Index ID.\nDetailed rule-extraction instructions and examples\nare provided in the Appendix Table 11.\n5\nMain Results\nTable 3: Performance fluctuation (accuracy) on the CSpelling\ndataset across grammar, punctuation, medical term spelling,\nand formality categories.\nSample Set\nGemini 2.5 Pro\nGemini 2.5 Pro\nOur\nOur\n∆\nSize\nMean (%)\nSD (%)\nMean (%)\nSD (%)\n(%)\n10\n17.6\n26.6\n34.4\n25.9\n+16.8\n12\n13.5\n18.3\n43.7\n27.0\n+30.1\n12\n29.5\n31.3\n57.9\n32.2\n+28.4\n12\n21.2\n23.5\n48.0\n28.2\n+26.8\n10\n24.2\n31.0\n41.1\n34.8\n+16.9\n12\n13.0\n17.4\n44.9\n27.0\n+31.9\n12\n28.9\n30.6\n61.8\n33.3\n+32.9\nTable 4: Error Detection Performance (Recall) by Failure\nClass on CSpelling\nError\nTotal Occurrences\nDetection\nClass\n(GT)\n(Recall %)\nMisspelling\n≈200\n92.5\nToSplit / ToMerge\n≈100\n65.0\nPunctuation\n≈60\n41.7\nGrammar\n≈100\n25.0\nInformal / Word not exists\n≈40\n12.5\nOur experimental evaluation, based on the\nAIReg-Bench and CSpelling datasets, yields sev-\neral key findings regarding the efficacy of our\nStudent-Teacher (ST) Model.\nRegulatory Compliance Performance (AIReg-\nBench) Over 120 EU AI system test cases, our\nStudent–Teacher–HITL framework consistently\noutperformed all baselines, shown in Table 2. Com-\npared with Gemini 2.5 Pro, our method achieved\nan absolute gain of +10. in overall accuracy (75.9\n% vs 65.9 %), driven primarily by superior recall\n(97.5 % vs 88.7 %) and improved specificity (43.8\n% vs 34.4 %). It is also important to note that\nour model’s improvements are against a state-of-\nthe-art baseline. As shown in Table 1, Gemini 2.5\nPro already achieved the highest human-agreement\nscore (κ = 0.863) among all 11 LLMs tested. The\nstudent–teacher verification loop proved especially\neffective for high-risk violation detection: In a com-\npliance context, failing to detect a violation (a False\nNegative) is the most critical error. The confusion\nmatrix in Figure 4 shows our ST model correctly\nflagged 78 of 80 violations while maintaining bal-\nanced precision and low false-positive rates. In\ncontrast, the Gemini 2.5 Pro baseline recorded 10\nFalse Negatives—a 5-fold increase in missed vi-\nolations. This is directly reflected in the Recall\nscore (the single most important metric for this\ntask), where our model achieved 97.50% versus\nthe baseline’s 88.70%. These confirm that itera-\ntive dual-LLM verification combined with human\noversight improves both sensitivity and compliance\nreliability.\nLanguage Quality and Error Detection\n(CSpelling): Table 3 and Figure 3 demonstrate con-\nsistent accuracy gains across all seven CSpelling\nsubsets. Our approach yields an average improve-\nment of +26.7 % over Gemini 2.5 Pro, with per-set\ngains ranging from +16.8 % to +32.9 %. However,\nthe relatively high standard deviations (25–33 %)\nindicate substantial heterogeneity in data complex-\nity. Combined with the erratic performance plotted\n"}, {"page": 6, "text": "Figure 3: Our model outperforms Gemini on all 7 samples from Cspelling, with per gains ranging from + 16.8% to + 32.9% and\nan overall improvement of 26.7%. However, the relatively large standard deviations indicate substantial variability across sample,\nsuggesting notable data heterogeneity. Both our methods and Gemini 2.5 pro are very good in detecting Misspelling error (with\nc.a. 92%), but very bad on Punctuation, Informality, and To-split/To Merge errors (with c.a. 41%).\nin Figure 3, strongly suggests that performance is\nhighly dependent on the specific data sample, in-\ndicating notable data heterogeneity. This suggests\nthat STOA LLM future improvements should fo-\ncus on syntactic and stylistic refinement rather than\nlexical correction alone.\nError-Class Analysis and Variability As\nshown in Table 4, recall varies dramatically by\nerror class. Figure 5 further reveals that while both\nsystems perform strongly on straightforward mis-\nspelling cases ( 92 % recall), performance drops\nsharply for punctuation (41.7 %), grammar (25 %),\nand informality (12.5 %). The Student–Teacher\nmodel’s architecture enables targeted gains in low-\nfrequency and structurally complex errors (e.g., to-\nsplit / to-merge, +24 pp recall over Gemini). Er-\nror analysis confirms that cascading rule filtering\n(Waterfall) reduces redundant checks and mitigates\nhallucination-induced misclassifications.\nTogether, these results demonstrate that integrat-\ning structured rule extraction, student–teacher ver-\nification, and human oversight yields measurable,\ninterpretable gains in both regulatory compliance\ndetection and medical-language quality control.\n6\nConclusion\nIn this work, we introduce an industrial frame-\nwork for LLM-driven content optimization and\ncompliance verification. Unlike prior studies that\nfocus primarily on new model architectures, our\ncontribution lies in proposing a plug-and-play Stu-\ndent–Teacher Model designed to reduce LLM hal-\nlucination and enhance accuracy, consistency, and\nverifiability under realistic regulatory constraints.\nOur evaluation yields four Key Findings. (1) Com-\nFigure 4: Confusion Matrix Heatmaps on AIReg-Bench.\nThese heatmaps visualize the raw counts of True Positives\n(Detected Violation), False Positives (Detect Compliance sys-\ntem into violation), True Negatives (Detect compliant system\ncorrectly), and False Negatives (Detect violation into compli-\nant) across 120 EU AI Systems test cases. Color intensity\nindicates count.\npliance Robustness: Our Student–Teacher–HITL\ndual-head LLM approach effectively enhances re-\ncall of high-risk violations while lowering false\npositives. (2) Data Heterogeneity: Large variance\nin CSpelling subsets highlights the need for contex-\ntual adaptation and dataset balancing. (3) Error-\nType Asymmetry: Misspelling detection is nearly\nsaturated, but punctuation and grammar errors re-\nmain open challenges, calling for multimodal or\nsyntactic fine-tuning. (4) Rule Efficiency: The Wa-\nterfall filtering mechanism reduces computational\noverhead by hierarchically pruning irrelevant rule\nsets, improving throughput without degrading ac-\ncuracy.\n"}, {"page": 7, "text": "7\nLimitations\nA main limitation of this study is to focus on justi-\nfication of the domain-specific benefits, e.g. med-\nical domain content optimization. We provide a\nbaseline comparison of SOTA LLM/VLMs using\nboth the AI-Reg benchmark and Cspelling dataset.\nCurrently, we focus on a real-world, high-stakes\napplication domain (pharmaceutical compliance),\nwhere the need for reliable content generation is\nparticularly strong. In future work, we will extend\nthis line of research to other regulated domains,\nsuch as financial services and manufacturing, to\nfurther validate the generalization of our solution\nblueprint.\n8\nAcknowledgments\nWe sincerely thank Samik Adhikary and Puneet Sri-\nvastava for their sponsorship support from Roche.\nWe also appreciate the insightful discussions and\ntechnical assistance provided by Janina Kummer-\nfeldt, Lynn Ma, Mayer Denis, and Kathrin Schwan\nfrom Accenture; Jennifer McGuire, Christopher\nChu, Alex Connor, and Liz Stutz for business sup-\nport from Roche; and Upender Phogat and Akshat\nGupta for US MLP development support from Inv-\nolead.\nThis Composer US-MLP use case, as well as\nthe GenAI platform (Mishra et al., 2026a) (Mishra\net al., 2026b), would not have been possible with-\nout their contributions.\nWe further extend our\ngratitude to the backend engineering teams who\nsupported development, and to the healthcare pro-\nfessionals (HCPs), testers, and Roche Lab users\nwhose consistent feedback brought our Composer\nuse cases, particularly the Content Optimizer Agent\nto life and enabled continuous improvement.\nThis paper represents our academic contribution,\nin which we formalize experiments and evalua-\ntion methodologies using academic benchmarks.\nThrough this work, we aim to share industry\nlessons learned and report valuable large-scale\nGenAI experiments in the pharmaceutical domain.\nReferences\nAnthropic. 2025. Claude 2.5 sonnet: Model card and\nsystem overview.\nhttps://www.anthropic.com.\nModel Card / Release Website.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nEuropean Federation of Pharmaceutical Industries\nand Associations. 2019.\nEfpia code of prac-\ntice. hhttps://efpia.eu/relationships-code/\nthe-efpia-code/.\nEuropean Parliament and Council of the European\nUnion. 2024. Regulation (eu) 2024/1689 of the euro-\npean parliament and of the council of 13 june 2024\nlaying down harmonised rules on artificial intelli-\ngence and amending regulations (ec). Official Jour-\nnal of the European Union. Accessed 2026-02.\nTianle Gu, Zeyang Zhou, Kexin Huang, Dandan Liang,\nYixu Wang, Haiquan Zhao, Yuanqi Yao, Xingge\nQiao, Keqing Wang, Yujiu Yang, Yan Teng, Yu Qiao,\nand Yingchun Wang. 2024. Mllmguard: A multi-\ndimensional safety evaluation suite for multimodal\nlarge language models. Preprint, arXiv:2406.07594.\nNeel Guha, Julian Nyarko, Daniel E. Ho, Christo-\npher Ré, Adam Chilton, Aditya Narayana, Alex\nChohlas-Wood, Austin Peters, Brandon Waldon,\nDaniel N. Rockmore, Diego Zambrano, Dmitry Tal-\nisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit\nSarfaty, Gregory M. Dickinson, Haggai Porat, Jason\nHegland, Jessica Wu, Joe Nudell, Joel Niklaus, John\nNay, Jonathan H. Choi, Kevin Tobia, Margaret Hagan,\nMegan Ma, Michael Livermore, Nikon Rasumov-\nRahe, Nils Holzenberger, Noam Kolt, Peter Hender-\nson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer\nWilliams, Sunny Gandhi, Tom Zur, Varun Iyer, and\nZehua Li. 2023. Legalbench: A collaboratively built\nbenchmark for measuring legal reasoning in large\nlanguage models. In Proceedings of the 37th Interna-\ntional Conference on Artificial Intelligence and Law\n(ICAIL ’23) – Datasets and Benchmarks Track. Also\navailable as arXiv preprint arXiv:2308.11462.\nInternational Federation of Pharmaceutical Manufactur-\ners & Associations. 2012. Ifpma code of practice.\nhttps://www.ifpma.org/resource-centre/\nifpma-code-of-practice/.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hal-\nlucination in natural language generation. volume 55,\nNew York, NY, USA. Association for Computing\nMachinery.\nJuyong Kim, Jeremy C Weiss, and Pradeep Raviku-\nmar. 2022. Context-sensitive spelling correction of\nclinical text via conditional independence. In Pro-\nceedings of the Conference on Health, Inference, and\nLearning, volume 174 of Proceedings of Machine\nLearning Research, pages 234–247. PMLR.\nAlexander W. Lee, Justin Chan, Michael Fu, Nicolas\nKim, Akshay Mehta, Deepti Raghavan, and U˘gur\nÇetintemel. 2025.\nSemantic integrity constraints:\nDeclarative guardrails for ai-augmented data process-\ning systems. PVLDB, 18(11):4073–4080.\n"}, {"page": 8, "text": "Qiang Li, Mingkun Tan, Xun Zhao, Dan Zhang, Daoan\nZhang, Shengzhao Lei, Anderson S. Chu, Lujun Li,\nand Porawit Kamnoedboon. 2025. How LLMs react\nto industrial spatio-temporal data? assessing halluci-\nnation with a novel traffic incident benchmark dataset.\nIn Proceedings of the 2025 Conference of the Na-\ntions of the Americas Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 3: Industry Track), pages 36–53.\nAssociation for Computational Linguistics.\nQiang Li, Dan Zhang, Shengzhao Lei, Xun Zhao, Po-\nrawit Kamnoedboon, WeiWei Li, Junhao Dong, and\nShuyan Li. 2024. Ximagenet-12: An explainable ai\nbenchmark dataset for model robustness evaluation.\nPreprint, arXiv:2310.08182.\nChris J Lu, Alan R Aronson, Sonya E Shooshan, and\nDina Demner-Fushman. 2019. Spell checker for con-\nsumer language (cspell). Journal of the American\nMedical Informatics Association, 26(3):211–218.\nThomas Mahler. 2021. Between risk management and\nproportionality: The risk-based approach in the eu’s\nartificial intelligence act proposal.\nBill Marino, Rosco Hunter, Zubair Jamali, Marinos Em-\nmanouil Kalpakos, Mudra Kashyap, Isaiah Hinton,\nAlexa Hanson, Maahum Nazir, Christoph Schnabl,\nFelix Steffek, Hongkai Wen, and Nicholas D. Lane.\n2025. AIReg-Bench: Benchmarking Language Mod-\nels That Assess AI Regulation Compliance. arXiv\npreprint arXiv:2510.01474.\nSuyash Mishra, Qiang Li, Srikanth Patil, and Anubhav\nGirdhar. 2026a. From understanding to engagement:\nPersonalized pharmacy video clips via vision lan-\nguage models (vlms). Preprint, arXiv:2601.05059.\nSuyash Mishra, Qiang Li, Srikanth Patil, Satyanarayan\nPati, and Baddu Narendra. 2026b. Scaling vision\nlanguage models for pharmaceutical long form video\nreasoning on industrial genai platform.\nPreprint,\narXiv:2601.04891.\nOpenAI. 2024. Gpt-4o system card. https://openai.\ncom. Model Card.\nOpenAI. 2025a. Gpt-5 system card. https://openai.\ncom. Accessed: 2025-01-15.\nOpenAI. 2025b. Openai o3-mini model card. https:\n//openai.com. Model Card.\nOpenAI. 2025c.\nOpenai o3 model card.\nhttps://\nopenai.com. Release Notes.\nEuropean Parliament. 2025. Eu ai act: first regulation\non artificial intelligence. Website.\nPharmaceutical Advertising Advisory Board. 2021.\nPaab code of advertising acceptance. https://code.\npaab.ca/.\nPharmaceutical Research and Manufacturers of Amer-\nica. 2019.\nCode on interactions with health-\ncare professionals.\nhttps://www.phrma.org/\ncodes-and-guidelines.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, and et al. 2025a. Gem-\nini: A family of highly capable multimodal models.\nPreprint, arXiv:2312.11805.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, and et al. 2025b. Gem-\nini: A family of highly capable multimodal models.\nPreprint, arXiv:2312.11805.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya\nPathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,\nTatiana Matejovicova, and et al. 2025c. Gemma 3\ntechnical report. Preprint, arXiv:2503.19786.\nSahil Tripathi, Md Tabrez Nafis, Imran Hussain, and\nJiechao Gao. 2025. The confidence paradox: Can llm\nknow when it’s wrong. Preprint, arXiv:2506.23464.\nU.S. Food and Drug Administration. 2022. Labeling\nfor prescription drugs and/or insulin: Guidance for\nindustry. https://www.fda.gov/. Accessed: 2025-\n01-15.\nReya Vir, Shreya Shankar, Harrison Chase, Will Fu-\nHinthorn, and Aditya Parameswaran. 2025. Prompte-\nvals: A dataset of assertions and guardrails for cus-\ntom production large language model pipelines. In\nNAACL Long Papers.\nChengye Wang, Yifei Shen, Zexi Kuang, Arman Cohan,\nand Yilun Zhao. 2025. Sciver: Evaluating foundation\nmodels for multimodal scientific claim verification.\narXiv preprint arXiv:2506.15569.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. The corpus of linguistic acceptability\n(cola). In Proceedings of the Society for Computa-\ntion in Linguistics (SCiL), pages 52–58. Association\nfor Computational Linguistics. Dataset and bench-\nmark for grammatical acceptability judgments.\nxAI. 2025a. Grok 3 mini model card. https://x.ai.\nxAI. 2025b. Grok 4 model card. https://x.ai. Tech-\nnical Documentation.\nMoran Yanuka, Assaf Ben-Kish, Yonatan Bitton, Idan\nSzpektor, and Raja Giryes. 2025.\nBridging the\nvisual gap: Fine-tuning multimodal models with\nknowledge-adapted captions. In Proceedings of the\n2025 Conference of the Nations of the Americas\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (Volume 1:\nLong Papers), page 10497–10518. Association for\nComputational Linguistics.\n"}, {"page": 9, "text": "A\nAppendix\nIn this section we provide the supplementary com-\npiled together with the main paper includes:\n• Ablation study on generalization,\ntoken\ncount, latency, and monetary cost of Stu-\ndent–Teacher dual-head LLM in Table 5, Ta-\nble 6 , Table 7, Table 8;\n• Property Dataset distribution on Table 9, and\nCspelling and AI Reg-benchmark raw rata ex-\nample on Figure 5 and Figure 6;\n• The Waterfall rule extraction and Student-\nTeacher prompt and rule verification prompt\ninstruction lists in Table 10, Table 11, Ta-\nble 12, and output example in Figure 7b;\nA.1\nAblation study: generalization on\nStudent-Teacher Model\nHere, we include an appendix ablation study us-\ning not only Gemini 2.5 Pro and Flash, but also\nGemini Pro as teacher and a anthropic model as\nstudent models. This approach will allow us to\ndetermine the effectiveness of using models from\nthe same family as student-teacher models vs a\nsmarter proprietary model paired with a less intel-\nligent open-source student model. It also demon-\nstrates how well our dual-head framework general-\nizes as a plug-and-play mechanism for other LLM\ncombinations.\nThe Claude-as-student ablation study maintains\nhigh Recall but suffers substantial drops in Preci-\nsion and Specificity, resulting in excessive false\npositives. This collapse in reliability significantly\nreduces overall Accuracy and F1-score, as con-\nfirmed in the ablation logs.\nIn contrast, the student–teacher baseline configu-\nration (Gemini Pro as teacher, same-family Gemini\nvariant as student) achieves the strongest overall\nperformance across metrics:\n• Better Balance (F1 = 0.768): achieves favor-\nable trade-off between violation detection and\nfalse alarm suppression.\n• Exceptional Detection (Recall\n=\n0.95):\nidentifies nearly all true violations, which is\ncritical for high-stakes regulatory compliance\nwhere missed violations must be minimized.\nConversely, the Gemini-Pro teacher + Claude\nstudent variant demonstrates a high-risk, low-\nreward profile. Its degraded overall performance,\nProvider/Model\nSample Tokens\nCost\n/ 1K tok\nGemini 2.5 Pro\n2.4K\n3.39¢\n1.41¢\nGemini 2.5 Pro\n5.0K\n7.46¢\n1.49¢\nGemini 2.5 Flash\n1.9K\n0.64¢\n0.34¢\nGemini 2.5 Flash\n1.8K\n0.38¢\n0.21¢\nAnthropic (Claude)\n11.0K\n3.51¢\n0.32¢\nAnthropic (Claude)\n11.9K\n3.93¢\n0.33¢\nAnthropic (Claude)\n4.0K\n1.46¢\n0.37¢\nTable 5: Example per-request costs from system logs. Costs\ndepend on provider/model tier, motivating cost-aware routing\nin multi-pass verification.\nMetric\nValue\nTotal tokens Per week\n12.3M\nTotal requests\n2,000\nLatency (P50)\n2010 ms\nCost / request\n$0.02045\nTokens / request\n6,150\nTable 6: Pilot deployed cost and latency summary.\nreflected in a low F1-score (0.5568) and Accuracy\n(0.4245), is driven by a catastrophic lack of relia-\nbility. The Precision of only 0.4936 and extremely\nlow Specificity indicate that it fails to correctly\nidentify compliant text and generates large numbers\nof false alarms, making it operationally impractical\nfor compliance workflows.\nA.2\nAblation study: framework perform in\nterms of token count, latency, and\nmonetary cost\nWe also log token usage and per-request billing\nat the model level. In our pilot, calls to Gemini\n2.5 Pro typically cost on the order of 3–8 ¢ (US\ncents) per request for 2–11K tokens, while Gem-\nini 2.5 Flash costs were sub-cent (e.g., 0.4–0.6¢\nfor 1.8–1.9K tokens). This enables tiered rout-\ning and industrial scale, where low-risk or pre-\nliminary checks use a lightweight flash model\nand ambiguous/high-risk cases are escalated to a\nteacher model, controlling the incremental over-\nhead of multi-pass verification. (See Table 5, 6).\nIn addition, we record per-request token usage\nand billing of production logs from different model\nproviders. In our pilot, Gemini 2.5 Flash requests\ncost sub-cent (e.g., 0.38–0.64¢ for 1.8–1.9K to-\nkens), while Gemini 2.5 Pro typically costs a few\ncents per request. For Anthropic endpoints, ob-\nserved costs were also low (e.g., 1.46–3.93¢ for\n4–11.9K tokens; 0.32–0.37¢ per 1K tokens), which\nenables scalability for industry application (Li et al.,\n2024).\n"}, {"page": 10, "text": "Table 7: Ablation study of the Student–Teacher framework using different LLMs as the teacher model. Mean accuracy\nand sample standard deviation are reported on the CSpelling ablation subset (Subset 1 and 5), where accuracy is defined as\ndetected / ground-truth errors per card. The configuration using Gemini 2.5 Pro as teacher and Claude as student shows improved\ndetection of non-existent medical terms and misspellings, but exhibits weaker performance on punctuation and token-splitting\nerrors. Nevertheless, it still substantially outperforms standalone Gemini 2.5 Pro.\nModel\nMean Accuracy (%)\nSD (%)\nGemini Pro\n15.1\n21.7\n(Gemini 2.5 Pro Teacher + Gemini 2.5 Flash Student)\n40.1\n26.4\n(Gemini 2.5 Pro Teacher + Claude 3.5 Sonnet Student)\n48.1\n27.3\nTable 8: Ablation study of the Student–Teacher framework using different LLMs as the teacher model, and corresponding\nperformance metrics on AIReg-Bench (N = 60 systems, Article 9) for regulatory compliance under the EU AI Act. Using models\nfrom the same family (a stronger model as teacher and a weaker but more diverse generator as student) yields better balance\nbetween detection and reliability. In contrast, the configuration with Gemini Pro as teacher and Claude as student produces\nsubstantially more false positives, reducing specificity without improving overall accuracy in violation detection.\nMetric\nStudent-Teacher Methods (%)\nStudent-Teacher Methods\n(Gemini 2.5 Pro as Teacher\n(Gemini 2.5 Pro Teacher +\n+ Claude 3.5 Sonnet as Student)\n+ Gemini 2.5 Flash Student)\nAccuracy ( TP+TN\nTotal )\n42.45\n65.67\nRecall (\nTP\nTP+FN )\n63.89\n95.00\nPrecision (\nTP\nTP+FP )\n49.36\n64.41\nF1-Score\n55.68\n76.86\nSpecificity (\nTN\nTN+FP )\n14.49\n60.0\nTable 9: Distribution of Property Audio and Video Data Across Medical Diseases Specialties.\nSpecialty\nAudio\nVideo\nSpecialty\nAudio\nVideo\nOncology\n208\n8934\nOphthalmology\n159\n2862\nSample setiovascular\n1\n14\nRespiratory Disease\n16\n467\nDermatology\n0\n30\nNephrology\n1\n380\nHematology\n67\n3606\nNot Applicable\n59\n2853\nImmunology\n144\n510\nMovement Disorder\n9\n289\nInfectious Disease\n1\n239\nInflammatory Disease\n20\n222\nMetabolism\n0\n6\nNeuroscience\n202\n4914\n"}, {"page": 11, "text": "Figure 5: Cspelling Test Data Distribution. Both our methods and Gemini 2.5 pro are very good in detecting Misspelling error\n(with c.a. 92%), but very bad on Punctuation, Informality, and To-split/To Merge errors (with c.a. 41%).\nFigure 6: AI Reg Benchmark Dataset.\n"}, {"page": 12, "text": "(a) Rule Extraction Output\n(b) Verification Output (Student–Teacher Gemini/Claude)\n"}, {"page": 13, "text": "Table 10: Foundation QC Rules Prompt (System Instruction e.g. Roche Global Copy Style Guide Book as default)\nSystem Prompt\nContent\nSystem Role\nYou are an expert in corporate communications and compliance. Review the\nuser’s text against the following Foundation QC (Quality Control) rules. Using\nDefault Roche Global Copy Style Guide Book. Focus on UK English spelling\nand grammar unless otherwise specified.\nSpelling\nRules\n–\nWhat to Do\n• 1.1 Use UK English for all corporate communications.\n• 1.2 For non-commercial or internal use, generally follow UK spelling,\nunless content is purely for a US audience or the brief requires US spelling.\n• 1.3 Use “analogue” when meaning “something similar to something else”\nor referring to chemical compounds.\n• 1.4 Check the BNF (medicines.org.uk/emc) for correct spelling of UK\nproprietary and generic drug names. ......\nSpelling\nRules\n–\nWhat to Prohibit\n• 2.1 Avoid US spellings in UK English such as acknowledgment, etiology,\naging, anemia, anesthetic, analog, behavior, cesarean, center, coloration,\ncorticotropin, dialog (except for UI), diarrhea, dyspnea, enroll, favor, ...\npractice (verb), program (unless IT), signaling, sulfur, tumor.\n• 2.2 Do not add spaces around ampersands.\n• 2.3 Do not use -t where -ed is preferred (learnt, spelt, burnt).\n• 2.4 Do not use -ize in UK English except defined exceptions. ......\nOther Comments\nRoche officially uses UK English for all corporate communications. Exceptions\napply for US audiences or specific briefs. The rulebook contains UK vs US\nspellings, ampersand usage, drug naming standards, past tense guidance, and\nrules for -ise/-ize.\nAnalysis\nRequire-\nments\n(Chain-of-\nThought)\n1. Carefully examine the plans against each relevant rule in the rule books\n2. Identify any areas where the plans may not comply with the rules\n3. Cite specific rules when discussing compliance issues\n4. Consider both explicit requirements and implicit principles in the rule\nbooks\nOutput Format\nRespond with a JSON object. The object should have a single key \"issues\"\nwhich is an array of objects. Each violation must include:\n1. \"issue\" rule ID (MUST HAVE) for the rule that was violated\n2. \"context\" The exact text snippet from the user’s content where the viola-\ntion occurred\n3. \"recommendation\" A clear suggestion on how to fix the violation.\nIf no issues are found, return \"issues\": [].\n"}, {"page": 14, "text": "Table 11: Rule Extraction Prompt (System Instruction for JSON-Structured Regulatory Rule Extraction and Parsing)\nSystem Prompt\nContent\nSystem Role\nYou are a compliance and regulation expert. I will provide you with content\nfrom a document (text or an image). Your task is to carefully analyze the\ndocument and extract all the rules into a JSON file format.\nTask Overview\nThe JSON object should include the document’s title, a summary of its content,\nand any other general comments. It must also contain a sections array, where\neach section details a specific set of rules, including what to do, what to prohibit,\nand any other relevant comments.\nCritical Instruction\nWhen extracting rules for what_to_do and what_to_prohibit, you MUST\nuse the exact wording, phrasing, and sentences from the original document.\nDo not rephrase, summarize, or generate new text for the rules. Copy them\nverbatim from the source.\nRequired\nJSON\nStructure\n{\n\"documentInfo\": {\n\"title\": \"Document Title Here\",\n\"content_about\": \"A brief description of this document.\",\n\"other_comments\": \"Any other general comments about the document.\"\n},\n\"sections\": [\n{\n\"title\": \"Title of the Rule Section\",\n\"content_about\": \"Description of this rule section.\",\n\"what_to_do\": [\"Rule 1 of what to do.\", \"Rule 2...\"],\n\"what_to_prohibit\": [\"Rule 1 of what to avoid.\", \"Rule 2...\"],\n\"other_comments\": \"Optional comments for this specific section.\"\n}\n]\n}\nFinal Instruction\nNow, analyze the following content and provide the structured\nJSON output.\n"}, {"page": 15, "text": "Table 12: Verification Prompt (Teacher Model)\nPrompt\nContent\nTask Description\nVerification Task using a teacher model to perform authoritative final review of\npreviously flagged issues.\nOriginal Text\n${userText}\nOriginal Rules\n${currentSystemInstruction}\nIssues\nto\nRe-\nevaluate\nThe following issues were identified by different models and require a final,\nauthoritative check.\n${issuesToVerify.map(i => `- Rule: ${i.issue}\n${ Context: \"${i.context}\"`).join('\\n')}\nTeacher Model In-\nstructions\n1. Critically re-evaluate each issue against the original text and rules.\n2. If multiple issues point to the same underlying error (e.g., a single typo\nviolating two different spelling rules), consolidate them into a single issue\nin your response.\n3. For each final issue, determine if it is a valid violation.\nOutput Format Re-\nquirements\nRespond with a JSON object. The object must have a single key \"issues\"\nwhich is an array of objects. Each object represents one of the re-evaluated\nissues and must have the following FOUR keys:\n1. \"issue\": The most appropriate rule ID for the violation.\n2. \"context\": The exact snippet from the user text.\n3. \"recommendation\": Your final verdict and justification. If it is NOT a\nvalid violation, explain why it was likely flagged incorrectly.\n4. \"isValid\": A boolean value. Set to ’true’ if the issue is a valid violation,\nand ’false’ otherwise.\n"}]}