{"doc_id": "arxiv:2602.06359", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.06359.pdf", "meta": {"doc_id": "arxiv:2602.06359", "source": "arxiv", "arxiv_id": "2602.06359", "title": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation", "authors": ["Xiyang Zhang", "Yuanhe Tian", "Hongzhi Wang", "Yan Song"], "published": "2026-02-06T03:41:40Z", "updated": "2026-02-06T03:41:40Z", "summary": "Fine-tuning large language models (LLMs) for specialized domains often necessitates a trade-off between acquiring domain expertise and retaining general reasoning capabilities, a phenomenon known as catastrophic forgetting. Existing remedies face a dichotomy: gradient surgery methods offer geometric safety but incur prohibitive computational costs via online projections, while efficient data selection approaches reduce overhead but remain blind to conflict-inducing gradient directions. In this paper, we propose Orthogonal Gradient Selection (OGS), a data-centric method that harmonizes domain performance, general capability retention, and training efficiency. OGS shifts the geometric insights of gradient projection from the optimizer to the data selection stage by treating data selection as a constrained decision-making process. By leveraging a lightweight Navigator model and reinforcement learning techniques, OGS dynamically identifies training samples whose gradients are orthogonal to a general-knowledge anchor. This approach ensures naturally safe updates for target models without modifying the optimizer or incurring runtime projection costs. Experiments across medical, legal, and financial domains demonstrate that OGS achieves excellent results, significantly improving domain performance and training efficiency while maintaining or even enhancing performance on general tasks such as GSM8K.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.06359v1", "url_pdf": "https://arxiv.org/pdf/2602.06359.pdf", "meta_path": "data/raw/arxiv/meta/2602.06359.json", "sha256": "830bd2e605e470c03fc4e0a3cd11c4c23da662c5d8d6524941f0ce765a8669f0", "status": "ok", "fetched_at": "2026-02-18T02:19:38.955851+00:00"}, "pages": [{"page": 1, "text": "Training Data Selection with Gradient Orthogonality for\nEfficient Domain Adaptation\nXiyang Zhang♦♣, Yuanhe Tian♥♣\nHongzhi Wang♦\nYan Song♠\n♦Harbin Institute of Technology\n♣Zhongguancun Academy\n♥Zhongguancun Institute of Artificial Intelligence\n♠University of Science and Technology of China\n♦s-zhangxiy24@bjzgca.edu.cn\n♥tianyuanhe@zgci.ac.cn\n♦wangzh@hit.edu.cn\n♠clksong@gmail.com\nAbstract\nFine-tuning large language models (LLMs) for\nspecialized domains often necessitates a trade-off\nbetween acquiring domain expertise and retain-\ning general reasoning capabilities, a phenomenon\nknown as catastrophic forgetting. Existing reme-\ndies face a dichotomy: gradient surgery methods\noffer geometric safety but incur prohibitive com-\nputational costs via online projections, while effi-\ncient data selection approaches reduce overhead\nbut remain blind to conflict-inducing gradient di-\nrections. In this paper, we propose Orthogonal\nGradient Selection (OGS), a data-centric method\nthat harmonizes domain performance, general ca-\npability retention, and training efficiency. OGS\nshifts the geometric insights of gradient projection\nfrom the optimizer to the data selection stage by\ntreating data selection as a constrained decision-\nmaking process.\nBy leveraging a lightweight\nNavigator model and reinforcement learning tech-\nniques, OGS dynamically identifies training sam-\nples whose gradients are orthogonal to a general-\nknowledge anchor. This approach ensures natu-\nrally safe updates for target models without mod-\nifying the optimizer or incurring runtime projec-\ntion costs. Experiments across medical, legal, and\nfinancial domains demonstrate that OGS achieves\nexcellent results, significantly improving domain\nperformance and training efficiency while main-\ntaining or even enhancing performance on general\ntasks such as GSM8K.\n1. Introduction\nThe remarkable success of large language models (LLMs)\nhas catalyzed their deployment across high-stakes vertical\ndomains, from clinical decision support (Singhal et al., 2023;\nLiu et al., 2024; Tian & Song, 2025) to legal document anal-\nysis (Colombo et al., 2024; Katz et al., 2024; Colombo\net al., 2024) and financial advisory systems (Wang et al.,\n2023; Wu et al., 2023; Xie et al., 2023a). However, domain-\nadapted LLMs face a fundamental paradox: the very process\nof specialization tends to erode the general-purpose capa-\nbilities that made these models valuable in the first place.\nPractitioners face what amounts to a zero-sum game be-\ntween specialists and generalists, which means optimizing\nfor medical question answering degrades mathematical rea-\nsoning, while preserving arithmetic skills constrains domain\nlearning (Luo et al., 2025). This phenomenon, known as\ncatastrophic forgetting (McCloskey & Cohen, 1989), has\nemerged as a central bottleneck in the efficient and practical\ndeployment of domain-adapted LLMs.\nThe geometric roots of this problem lie in gradient interfer-\nence (Yu et al., 2020). When a model updates its parameters\nto minimize domain-specific loss, the resulting gradient may\npoint in a direction that increases the loss on general tasks.\nFormally, forgetting occurs when the inner product of the\ndomain task’s gradient and the general task’s gradient is\nless than zero (Lopez-Paz & Ranzato, 2017), causing the\nmodel to drift away from regions of the parameter space that\nsupport general reasoning. This manifold drift accumulates\nover thousands of training steps, ultimately severing the\nmodel’s connection to its pre-trained knowledge (Kumar\net al., 2022), shown as catastrophic forgetting.\nPrior work has attempted to resolve this conflict from two\ndistinct angles, yet both suffer from limitations that force\na compromise between effectiveness and efficiency. The\nfirst family, optimizer-level gradient surgery such as GEM\n(Lopez-Paz & Ranzato, 2017) and SafeGrad (Yi et al., 2025),\nactively projects task gradients onto a safe subspace orthog-\nonal to protected knowledge (Saha et al., 2021). While\ntheoretically principled, their computational demands scale\npoorly: each optimization step requires computing refer-\nence gradients and executing high-dimensional projections\n(Chaudhry et al., 2018). For LLMs exceeding dozens of bil-\nlion parameters, this online overhead becomes prohibitive,\nrendering such methods impractical for scale-efficient train-\n1\narXiv:2602.06359v1  [cs.LG]  6 Feb 2026\n"}, {"page": 2, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\ning (Shi et al., 2025). We need a faster way.\nThe second route embraces data selection as a scalable al-\nternative (Song et al., 2012; Coleman et al., 2019; Liu et al.,\n2019; Killamsetty et al., 2021). Methods such as LESS\n(Xia et al., 2024) and GrADS (Liu et al., 2025b) improve\nefficiency by filtering data offline based on gradient similar-\nity or magnitude. However, these approaches often remain\nblind to the geometric nature of forgetting (Lopez-Paz &\nRanzato, 2017). LESS focuses on target relevance without\nassessing conflicts with general knowledge, while GrADS\nrelies on gradient norms which cannot distinguish between\northogonal (safe) and opposing (harmful) updates. Conse-\nquently, these methods may inadvertently select data that\nmaximizes domain gain at the cost of severe forgetting,\nfailing to solve the multi-objective challenge.\nThis analysis reveals a critical methodological gap: we need\na solution that possesses the geometric precision of gradient\nsurgery but maintains the computational efficiency of data\nselection. Considering that the small proxy models can\nguide the LLMs (Burns et al., 2023; Xie et al., 2023b),\nthe natural question emerges: Can we employ proxy model\nand intelligent decision-making to transfer the geometric\ninsights of gradient projection to the data selection phase?\nTo this end, we propose Orthogonal Gradient Selection\n(OGS), a novel data-centric method designed to simulta-\nneously boost domain performance, prevent catastrophic\nforgetting, and maximize training efficiency. OGS funda-\nmentally rethinks data selection not as a static filtering task,\nbut as a constrained optimization problem guided by gradi-\nent geometry. Our approach relies on three key innovations:\n1. Offline Geometric Awareness: We construct a general-\nknowledge anchor whose gradient defines the direction to\nprotect. Instead of projecting gradients during training, we\nselect data samples that naturally produce gradients orthog-\nonal to this anchor, effectively performing “data surgery” to\nensure safe updates in a efficient way.\n2. Navigator-Target Architecture: To bypass the cost of\ncomputing gradients on LLMs, we introduce a lightweight\n“Navigator” proxy model. We demonstrate that gradient\ngeometry features (orthogonality and conflict) computed on\nthe Navigator transfer reliably to much larger Target models,\nallowing us to perform expensive geometric analysis.\n3. RL-Driven Selection Policy: We formulate the selection\nprocess as a decision-making task solvable via reinforce-\nment learning (RL) mechanisms. By optimizing a reward\nfunction that balances domain learning speed with orthogo-\nnality constraints, OGS dynamically curates a curriculum\nthat navigates the trade-off between plasticity (learning new\ntasks) and stability (remembering old ones).\nThe paradigm shift to OGS carries profound practical impli-\ncations. By decoupling geometric analysis from the training\nloop, OGS remains compatible with standard optimizers and\nefficient training pipelines such as LoRA (Hu et al., 2022),\nincurring negligible runtime overhead. Theoretical analysis\nconfirms that our selection criterion acts as a first-order ap-\nproximation to the optimal solution of a bilevel optimization\nproblem minimizing post-update general loss.\nEmpirically, we evaluate OGS across three vertical domains\n(medical, law, and finance) using target models ranging from\n1.7B to 14B parameters. The results demonstrate that OGS\nyields Pareto-optimal improvements: it exceeds the domain\naccuracy of baselines while preserving pre-training perfor-\nmance on general benchmarks. Crucially, OGS achieves this\nwith higher training throughput than gradient surgery meth-\nods, validating its potential as a general-purpose method for\nefficient, safe, and effective domain adaptation.\nIn summary, we make the following contributions:\n• We propose OGS, a data-centric method that leverages\ngradient geometry to simultaneously enhance domain\nadaptation and prevent catastrophic forgetting, offering\na scalable alternative to online gradient surgery.\n• We introduce a Navigator-Target architecture coupled\nwith a reinforcement learning-based selection policy,\nenabling the efficient transfer of geometric insights\nfrom small proxy models to large-scale target models.\n• We provide theoretical grounding for OGS via an in-\nterference decomposition theorem and establish its se-\nlection rule as a solution to a constrained bilevel opti-\nmization problem with detailed proof.\n• We demonstrate that OGS achieves superior efficiency\nacross multiple domains, improving domain perfor-\nmance and training speed while effectively neutralizing\nthe risk of catastrophic forgetting.\n2. Related Work\n2.1. Continual Learning and Catastrophic Forgetting in\nLLMs\nAdapting LLMs to vertical domains often necessitates a\ntrade-off between acquiring domain-specific knowledge and\nretaining general capabilities (Wu et al., 2023; Shu et al.,\n2024; Tian et al., 2024; Colombo et al., 2024; Su et al., 2025;\nLiu et al., 2025a), a phenomenon known as the stability-\nplasticity dilemma (Mermillod et al., 2013). While con-\nventional parameter-efficient fine-tuning (PEFT) methods\nmitigate forgetting by freezing the backbone (Houlsby et al.,\n2019; Hu et al., 2022), they do not fundamentally resolve\ngradient conflicts between new and old tasks. Replay-based\nmethods (Rebuffi et al., 2017) remain the gold standard\nbut introduce the challenge of selecting a representative\n2\n"}, {"page": 3, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nreplay buffer. Recent works have explored prompt-based\napproaches (Wang et al., 2022) or model merging (Ilharco\net al., 2022; Yadav et al., 2023) to alleviate forgetting. How-\never, these methods often operate at the model level rather\nthan the data level, potentially missing the root cause of\ninterference: the training data itself (Zhou et al., 2023).\n2.2. Gradient Surgery and Conflicting Objectives\nGradient surgery methods aim to manipulate gradient up-\ndates during optimization to mitigate interference between\ntasks. Seminal works like GEM (Lopez-Paz & Ranzato,\n2017) and A-GEM (Chaudhry et al., 2018) project gradients\nof new tasks onto the feasible region defined by previous\ntasks. PCGrad (Yu et al., 2020) projects conflicting gradients\nonto the normal plane of each other to resolve interference\nin multi-task learning. More recently, SafeGrad (Yi et al.,\n2025) applied this concept to safety alignment, projecting\nuser-task gradients onto the subspace orthogonal to safety\nalignment gradients to prevent “safety unlearning.” While\ntheoretically elegant, these optimizer-level solutions suffer\nfrom a fatal bottleneck: they require computing and pro-\njecting gradients for reference tasks at every optimization\nstep. This introduces prohibitive computational and memory\noverhead (often 2× to 3× cost), making them impractical\nfor LLMs exceeding high-level parameters (Abbes et al.,\n2025). In contrast, OGS transfers the geometric insights of\ngradient surgery to the data selection stage. By identifying\nnaturally orthogonal data offline, OGS achieves the stability\nbenefits of gradient surgery with the efficiency of standard\nSFT, requiring no runtime overhead.\n2.3. Data Selection for Efficient Fine-tuning\nData selection has emerged as a crucial paradigm for effi-\ncient LLM training. Early methods focused on quality filter-\ning via heuristics or perplexity (Zhang et al., 2019; Wenzek\net al., 2020; Penedo et al., 2023). Recent gradient-based ap-\nproaches seek to select high-value samples mathematically.\nLESS (Xia et al., 2024) selects data that minimizes loss\non a target validation set by maximizing gradient similar-\nity. While effective for targeted induction, LESS is blind to\n“safety”: maximizing similarity to a target domain inadver-\ntently selects samples that maximally conflict with general\ncapabilities (e.g., selecting medical misconceptions that con-\ntradict general reasoning). GrADS (Liu et al., 2025b) pro-\nposes a gradient-aware selection strategy based on gradient\nmagnitude and uncertainty. However, gradient magnitude\ncan be misleading (Wu et al., 2025), i.e., a sample with mod-\nerate gradient magnitude can still be highly destructive if\nits direction is diametrically opposed to general knowledge\n(i.e., negative cosine similarity).\n3. Methodology\nWe present Orthogonal Gradient Selection (OGS) in de-\ntail. We begin by formalizing the constrained optimization\nproblem that captures the stability-plasticity trade-off, then\nintroduce our Navigator-Target architecture that enables ef-\nficient gradient geometry computation at scale. The core of\nour method lies in two geometric metrics—orthogonality\nand conflict—that guide data selection toward the safe sub-\nspace. We conclude with the selection strategies and the\nconstrained policy optimization method. Figure 1 shows the\nentire intuitive flow of the OGS method.\n3.1. Problem Formulation\nConsider a pre-trained language model with parameters\nθ, a domain-specific training pool Ddomain (e.g., medical\nquestion-answering pairs), and a general-knowledge val-\nidation set Vgen representing capabilities we wish to pre-\nserve (e.g., mathematical reasoning, world knowledge). Let\nRdomain(θ) and Rgen(θ) denote performance metrics on do-\nmain and general tasks respectively. The goal of contin-\nual domain adaptation is to maximize domain performance\nwhile maintaining general capabilities above a threshold.\nWe formalize this as a constrained optimization problem\nover data selection policies. Let π denote a policy that,\nat each training step t, selects a batch of samples from\nthe available data pools.\nThe training trajectory τ =\n{(st, at, st+1)}T −1\nt=0 induced by π produces final parame-\nters θT . Our objective is:\nmax\nπ\nEτ∼π [Rdomain(θT )]\n(1)\ns.t.\nEτ∼π [Rgen(θT )] ≥Rgen(θ0) + δ\n(2)\nwhere δ ≥0 specifies the minimum acceptable improve-\nment (or preservation when δ = 0) of general capabilities.\nThis formulation explicitly encodes the stability-plasticity\ntrade-off: the objective drives domain learning while the\nconstraint enforces capability preservation.\n3.2. Navigator-Target Architecture\nA naive approach to gradient-geometry-aware selection\nwould compute, for each candidate sample, its gradient on\nthe target model and measure geometric relationships with\nanchor gradients. For a huge size parameter target model,\nthis is computationally prohibitive, as each gradient com-\nputation requires a full forward-backward pass, consuming\nhundreds of gigabytes of memory.\nA key insight is that gradient geometric features exhibit\nremarkable transferability across model scales. While the\nabsolute gradient magnitudes differ substantially between\n3\n"}, {"page": 4, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nFigure 1. The entire intuitive flow of the OGS method, including Navigator Probing, Geometric Selection, Target Fine-tuning three stages,\naiming to choose the best data subset.\na 0.5B Navigator and a 70B Target, the relative geomet-\nric relationships, such as cosine similarities, orthogonality\npatterns, and conflict structures, remain highly correlated.\nThis phenomenon arises because models are from the same\narchitectural family, trained on similar corpora, and develop\naligned representation spaces where semantically similar\nsamples induce geometrically similar gradient directions.\nWe exploit this transferability through a two-phase method.\nIn Phase 1 (Strategy Learning), a lightweight Navigator\nmodel Mn with parameters θn (e.g., Qwen3-0.6B) serves as\nthe computational workhorse. We compute gradient geom-\netry features on Mn, train a selection policy via reinforce-\nment learning, and validate geometric predictions against\nground-truth forgetting measurements. In Phase 2 (Strategy\nApplication), the learned policy guides data selection for\nthe Target model Mt (e.g., Qwen3-14B).\nThis architecture converts the computational burden from\nmultiplicative (per training step on Target) to additive (one-\ntime preprocessing on Navigator). The Navigator processes\nthe entire candidate pool once, after which Target training\nproceeds at full speed without geometric overhead.\n3.3. Gradient Geometry Metrics\nThe foundation of OGS is a set of metrics that quantify how\na candidate sample’s gradient relates to protected general\nknowledge. We first construct an anchor that represents\nthe gradient direction of general capabilities, then define\northogonality and conflict metrics relative to this anchor.\nAnchor Gradient Construction.\nWe curate a compact\nanchor dataset Danchor comprising 300-500 exemplars from\ncapabilities we wish to protect: mathematical reasoning\nproblems from GSM8K, factual questions from MMLU, and\ninstruction-following examples from Alpaca. The anchor\ngradient is the average gradient over this set:\ngref =\n1\n|Danchor|\nX\nx∈Danchor\n∇θL(x; θ)\n(3)\nThis vector defines the “north star” direction in parameter\nspace, which means updating aligned with gref improves\ngeneral capabilities, while updates opposing it induce for-\ngetting. The anchor gradient is computed once and cached\nfor the entire selection process.\nFor enhanced sensitivity, we optionally employ active an-\nchor selection, preferentially including general samples that\nexhibit maximum conflict with domain data:\nDactive\nanchor = arg\nmin\nD⊂Dgen,|D|=k cos (¯gD, ¯gdomain)\n(4)\nwhere ¯gD denotes the mean gradient over subset D. This\nselects anchors representing knowledge most vulnerable to\ndomain-induced forgetting.\nOrthogonality Score.\nFor a candidate sample xi with\ngradient gi = ∇θL(xi; θ), we define orthogonality as:\n4\n"}, {"page": 5, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nOrth(xi) = 1 −|cos(gi, gref)| = 1 −\n|g⊤\ni gref|\n∥gi∥· ∥gref∥\n(5)\nOrthogonality ranges from 0 to 1, with higher values indicat-\ning that the sample’s gradient lies closer to the hyperplane\nperpendicular to gref. A sample with Orth(xi) ≈1 induces\nupdates in the safe subspace, which means these directions\nneither help nor harm general capabilities. Such samples\nare able to be trained freely without risking forgetting.\nConflict Score.\nWhile orthogonality measures proximity\nto the safe subspace, conflict directly quantifies interference:\nConf(xi) = −cos(gi, gref) = −\ng⊤\ni gref\n∥gi∥· ∥gref∥\n(6)\nConflict ranges from −1 to +1. Positive conflict (Conf > 0)\nindicates that the sample’s gradient opposes the anchor di-\nrection, meaning that training on this sample would increase\ngeneral-task loss, inducing forgetting. Negative conflict\n(Conf < 0) indicates synergy: the sample simultaneously\nbenefits both domain and general capabilities. Zero conflict\ncorresponds to perfect orthogonality.\nThese metrics decompose gradient relationships into action-\nable signals. It illustrates geometric intuition: the anchor\ngradient gref defines a protected direction, and candidate\ngradients are classified by their angle relative to direction.\n3.4. Selection Strategies\nArmed with geometric metrics, we define two complemen-\ntary selection strategies that together achieve robust capabil-\nity preservation. These two strategies can also be mixed.\nOrthogonal Protection.\nThe primary strategy selects sam-\nples residing within the safe subspace:\nDorth\nselected = {xi ∈Ddomain : Orth(xi) ≥τorth}\n(7)\nwhere τorth ∈[0, 1] is an orthogonality threshold. These\nsamples enable unimpeded domain learning: their gradi-\nents update parameters in directions that leave general ca-\npabilities essentially unchanged. This strategy maximizes\nlearning efficiency when sufficient orthogonal samples exist.\nConflict-Aware Replay.\nWhen domain training inevitably\nincludes some conflicting samples, we counteract their harm-\nful effects through targeted replay. We identify general-\nknowledge samples currently under attack:\nDreplay = {xj ∈Dgen : Conf(xj) > τconf}\n(8)\nThese are samples whose gradients most strongly oppose\nrecent domain updates—precisely the knowledge being for-\ngotten. By interleaving replay of these samples, we inject\ncorrective gradients that neutralize interference.\nHybrid Dynamic Strategy.\nWe employ a hybrid approach\nthat adapts the mixture ratio based on training dynamics:\nBt = α(t) · Borth\nt\n+ (1 −α(t)) · Breplay\nt\n(9)\nwhere α(t) evolves during training. Early phases empha-\nsize conflict-aware replay (α small) to establish stability,\nwhile later phases shift toward orthogonal domain samples\n(α large) to accelerate specialization. The policy network\nlearns to modulate α(t) based on observed performance\ndynamics to keep the balance.\n3.5. Constrained Policy Optimization\nWe cast data selection as a Constrained Markov Decision\nProcess (CMDP) and solve it via PPO-Lagrangian optimiza-\ntion. The state st encodes current model performance, gra-\ndient geometry statistics across data clusters, and training\nprogress. The action at selects a data cluster from which to\nsample the next batch. The reward rt reflects domain per-\nformance improvement, while the cost ct measures general\ncapability degradation.\nThe constrained objective from Equation 1 is converted to\nan unconstrained Lagrangian:\nL(π, λ) = Eπ\n\"X\nt\nγtrt\n#\n−λ ·\n \nEπ\n\"X\nt\nγtct\n#\n−ϵ\n!\n(10)\nwhere λ ≥0 is the Lagrange multiplier and ϵ is the con-\nstraint budget. We alternate between policy updates (max-\nimizing L with respect to π via PPO) and dual updates\n(adjusting λ based on constraint satisfaction):\nλk+1 = max (0, λk + ηλ · (JC(πk) −ϵ))\n(11)\nWhen the policy violates constraints (excessive forgetting),\nλ increases, penalizing unsafe selections more heavily.\nWhen constraints are comfortably satisfied, λ decreases,\nallowing more aggressive domain learning. This adaptive\nmechanism automatically navigates the stability-plasticity\ntrade-off without manual tuning.\nAlgorithm 1 summarizes the complete OGS pipeline. Phase\n1 trains the selection policy on the Navigator, while Phase 2\napplies it to the Target with standard fine-tuning.\n5\n"}, {"page": 6, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nAlgorithm 1 Orthogonal Gradient Selection (OGS)\n1: Input: Domain pool Ddomain, general pool Dgen, Navi-\ngator Mn, Target Mt\n2: Output: Fine-tuned Target parameters θ∗\nt\n3: // Phase 0: Preprocessing\n4: Construct anchor set Danchor from Dgen\n5: Compute anchor gradient gref via Eq. 3\n6: Cluster Ddomain and Dgen into K clusters\n7: // Phase 1: Policy Learning on Navigator\n8: for episode e = 1 to E do\n9:\nReset Navigator: θn ←θ(0)\nn\n10:\nfor step t = 0 to T −1 do\n11:\nCompute cluster geometries: {Orthk, Confk}K\nk=1\n12:\nConstruct state st from geometries and perfor-\nmance\n13:\nSample action at ∼πψ(·|st)\n14:\nSample batch Bt from cluster at\n15:\nUpdate Navigator: θn ←θn −η∇L(Bt)\n16:\nCompute reward rt and cost ct\n17:\nend for\n18:\nUpdate policy πψ via PPO-Lagrangian\n19: end for\n20: // Phase 2: Policy Application on Target\n21: for step t = 0 to Ttarget −1 do\n22:\nCompute state st for Target\n23:\nSelect cluster at = arg maxa πψ(a|st)\n24:\nSample batch Bt from cluster at\n25:\nUpdate Target via standard SFT: θt ←θt −η∇L(Bt)\n26: end for\n27: return θ∗\nt = θt\n4. Theoretical Analysis\nWe provide a theoretical foundation for OGS, demonstrating\nthat our geometric constraints are necessary to minimize\ncatastrophic forgetting and that our selection strategy is\na first-order optimal solution to the constrained continual\nlearning problem. We provide efficiency analysis as well.\nAll detailed proofs are deferred to Appendix A.\n4.1. Geometric Interference and Safety Subspace\nCatastrophic forgetting in fine-tuning arises from the in-\nterference between domain-specific updates and general\nknowledge representations. Let θ ∈Rd be the model pa-\nrameters, and Lmed, Lgen be the domain and general loss\nfunctions, respectively.\nTheorem 4.1 (Gradient Interference). The degradation of\ngeneral capabilities after a domain update step ∆θ =\n−η∇Lmed(θ) is governed, at the first order, by the inner\nproduct of gradients:\n∆Lgen = −η⟨∇Lgen(θ), ∇Lmed(θ)⟩+ O(η2).\n(12)\nTheorem 4.1 dictates to prevent forgetting (∆Lgen > 0), the\ndomain gradient must be non-conflicting with the general\ngradient. Accordingly, we define the Safety Subspace as\nS⊥= {v | ⟨v, E[∇Lgen]⟩≥0}. OGS explicitly selects\ndata gradients residing in S⊥, ensuring safety a priori.\n4.2. Optimality and Efficiency\nWe formulate valid data selection as a bilevel optimization\nproblem: minimizing general loss degradation (outer loop)\nsubject to selecting k samples for domain maximization\n(inner loop).\nTheorem 4.2 (First-Order Optimality). The optimal selec-\ntion strategy w∗that solves the bilevel objective under a\nfirst-order approximation is equivalent to maximizing the\nalignment P wi⟨∇ℓ(xi), ∇Lgen⟩.\nTheorem 4.2 proves that our “Conflict” metric (negative\ncosine similarity) is mathematically aligned with the optimal\npolicy. Furthermore, we address the feasibility of our dual-\nmodel architecture.\nProposition 4.3 (Asymptotic Efficiency). Let ρ\n=\nMn/Mt ≪1 be the parameter ratio between Navigator\nand Target models. The computational overhead of OGS\nscales with O(ρ), rendering it negligible compared to the\nO(1) overhead of online gradient surgery methods.\nThis establishes OGS as a Pareto-optimal solution, com-\nbining the theoretical rigor of gradient projection with the\nefficiency of standard fine-tuning.\n5. Experiments\nTo rigorously evaluate the efficacy of our method, we con-\nducted extensive experiments across healthcare, law, and\nfinance three distinct vertical domains on five LLMs. Our\nempirical analysis investigates following dimensions: the\ncapacity of OGS to achieve domain adaptation performance\ncomparable to full-data training while utilizing significantly\nfewer samples, and the efficacy of the geometric safety\nconstraint in mitigating catastrophic forgetting, particularly\nwithin fragile reasoning tasks.\n5.1. Experimental Setup\nDatasets and Models. We utilized MEDQA (Jin et al.,\n2021), LEGALBENCH (Guha et al., 2023), and FINQA\n(Chen et al., 2021) as domain-specific training sets. To moni-\ntor catastrophic forgetting, we evaluated models on GSM8K\n(mathematical reasoning) (Cobbe et al., 2021), MMLU\n(world knowledge) (Hendrycks et al., 2021), and ARC-\nCHALLENGE (scientific reasoning) (Clark et al., 2018), with\na specific focus on GSM8K as it is empirically most suscep-\ntible to erosion during fine-tuning. Consider the quality of\ndata representation plays an essential role in model perfor-\n6\n"}, {"page": 7, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nTable 1. Main Results on 8B Scale Models (10% Data Selection). Comparison of domain adaptation (MedQA, LegalBench, FinQA)\nand general capability retention (GSM8K, MMLU, ARC-C) using Llama-3.1-8B and Qwen3-8B. The “Avg.” columns represent the\nmacro-average across the three respective tasks. OGS achieves the best balance, significantly outperforming baselines on the critical\nGSM8K retention metric while matching or exceeding domain performance. Bold indicates the best performance among selection\nmethods.\nModel\nMethod\nDomain Performance\nGeneral Capability\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nMMLU\nARC-C\nAvg.\nLlama-3.1-8B\nFull Data (100%)\n65.2\n58.4\n61.3\n61.6\n45.1\n55.2\n58.9\n53.1\nRandom (10%)\n50.1\n45.3\n48.2\n47.9\n50.5\n62.1\n64.3\n59.0\nPerplexity\n52.3\n46.8\n49.5\n49.5\n51.2\n61.8\n63.5\n58.8\nInfluence Func.\n53.1\n47.2\n50.1\n50.1\n52.0\n62.5\n64.0\n59.5\nLESS\n58.4\n52.1\n55.3\n55.3\n48.2\n58.4\n60.1\n55.6\nGRADS\n57.9\n53.5\n56.1\n55.8\n53.5\n63.1\n64.8\n60.5\nOGS (Ours)\n60.2\n55.1\n58.4\n57.9\n55.2\n64.5\n66.2\n62.0\nQwen3-8B\nFull Data (100%)\n70.5\n62.1\n65.4\n66.0\n55.3\n60.1\n65.4\n60.3\nRandom (10%)\n55.2\n48.5\n52.1\n51.9\n65.1\n70.2\n72.1\n69.1\nPerplexity\n56.4\n50.1\n53.2\n53.2\n64.5\n69.5\n71.5\n68.5\nInfluence Func.\n57.1\n51.3\n54.5\n54.3\n65.2\n69.8\n72.0\n69.0\nLESS\n62.3\n55.4\n59.2\n59.0\n60.1\n65.4\n68.2\n64.6\nGRADS\n61.5\n57.2\n60.1\n59.6\n66.8\n71.2\n73.5\n70.5\nOGS (Ours)\n64.1\n59.5\n62.8\n62.1\n68.5\n72.8\n74.9\n72.1\nmance (Devlin et al., 2019; Brown et al., 2020; Song et al.,\n2021; Diao et al., 2021; Liu et al., 2023; Lu et al., 2023), we\nutilize state-of-the-art LLMs in experiments. Specifically,\nour primary target models are the Qwen3 family (1.7B, 8B,\n14B) and Llama-3 family (Llama-3.2-3B, Llama-3.1-8B),\nwith Qwen3-0.6B and Llama-3.2-1B serving as their respec-\ntive Navigators. All models were fine-tuned using LoRA\n(Hu et al., 2022) to simulate realistic, resource-constrained\nadaptation scenarios.\nBaselines. We compared OGS against three categories of\ndata selection methods: (1) Naive approaches, including\nRandom Selection and Full Data training; (2) Heuristic\napproaches, such as Perplexity-based selection and INFLU-\nENCE FUNCTIONS ; (3) Gradient-based approaches, specif-\nically LESS (Xia et al., 2024) (gradient similarity), and\nGRADS (Liu et al., 2025b) (gradient magnitude). Detailed\nhyperparameters and implementation details are provided\nin Appendix B.\n5.2. Main Results and Analysis\nPerformance on Stability-Plasticity. The core illustrate\nof this work is that gradient geometry provides a superior\nsignal for data selection compared to magnitude or semantic\nsimilarity alone. Table 1 presents the comparative perfor-\nmance of Llama-3.1-8B and Qwen3-8B fine-tuned on a\n10% subset of domain data. OGS consistently achieves a\nsatisfying performance, securing domain adaptation perfor-\nmance comparable to or exceeding the full-dataset baseline\nwhile maintaining general capabilities better than competing\nmethods. Specifically, on the Llama-3.1-8B model, OGS\noutperforms the strongest baseline (GRADS) by an aver-\nage of 2.1% across the three domain tasks while reducing\ncatastrophic forgetting on GSM8K by 1.7 points. This indi-\ncates that OGS successfully identifies a “safety subspace”,\nwhere domain knowledge is injected without overwriting\nthe critical weights responsible for general reasoning.\nThe Success of Gradient Geometry. A critical insight\nfrom our experiments is the distinct failure modes of ex-\nisting gradient-based approaches compared to our success.\nLESS, which selects data based on gradient similarity to the\ntarget domain, achieves respectable domain accuracy but\nsuffers severe regression in general capabilities, particularly\nin mathematical reasoning (GSM8K). We attribute this to\nthe “blindness” of similarity metrics: data that is seman-\ntically closest to the target domain often occupies a gradi-\nent direction that sharply conflicts with general knowledge\nmanifolds. Similarly, GRADS, which relies on gradient\nmagnitude, exhibits high variance across tasks. Our results\nsuggest that magnitude is a noisy proxy for utility; high-\nmagnitude gradients often correspond to outliers or difficult\nsamples that induce instability rather than efficient learning.\nBy explicitly optimizing for orthogonality, OGS effectively\nfilters out these high-conflict samples that magnitude-based\nmethods inadvertently promote.\nNavigator Transferability and Data Efficiency. The effec-\ntiveness of OGS relies on the assumption that gradient geo-\nmetric properties transfer from small “Navigator” models to\n7\n"}, {"page": 8, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nlarger “Target” models. Our results validate this cross-scale\nalignment. The Qwen3-0.6B Navigator successfully guided\nthe data selection for the Qwen3-14B target, achieving a\n64.8% average domain score with only 10% data, matching\nthe performance of Random Selection at 30% data. This\ndemonstrates a 3× data efficiency gain. Furthermore, the sta-\nble performance across the disparate architectures of Llama\nand Qwen families confirms that the geometric conflict be-\ntween domain-specific fine-tuning and general reasoning is\na structural phenomenon intrinsic to the data distribution,\nrather than an artifact of a specific model architecture.\n5.3. Ablation Studies\nTo disentangle the contributions of individual components\nwithin the OGS method, we conducted a comprehensive ab-\nlation study using the Qwen3-8B target model and Qwen3-\n0.6B Navigator. We investigate three critical questions: (1)\nthe synergistic effect of our geometric metrics (Orthogonal-\nity and Conflict); (2) the necessity of the RL-driven dynamic\npolicy compared to static heuristics; and (3) the validity of\nthe Navigator-Target transferability assumption. The results\nare summarized in Table 2.\nDecoupling Geometric Metrics. We first isolate the im-\npact of Orthogonal Selection and Conflict-Aware Replay.\nAs shown in the first block of Table 2, relying solely on\nOrthogonal Selection (w/o Replay) yields high domain per-\nformance (59.5%) but suffers a noticeable regression in\ngeneral capabilities (53.0%), indicating that while orthogo-\nnality facilitates efficient learning, it cannot fully neutralize\nthe most aggressive gradient conflicts. Conversely, employ-\ning only Conflict-Aware Replay (w/o Orthogonal) effec-\ntively preserves general knowledge but severely throttles\ndomain adaptation, as the model lacks a curriculum of ”safe”\nsamples to learn from. The full OGS method achieves an\nexcellent state better than others, confirming that these two\nmechanisms operate synergistically: orthogonal selection\nmaximizes plasticity within the safety subspace, while tar-\ngeted replay provides a crucial stability anchor.\nEfficacy of RL-Driven Policy. We then scrutinize the neces-\nsity of our PPO-Lagrangian policy optimization by compar-\ning it against a static greedy strategy (w/o RL) that maintains\na fixed mixing ratio of α = 0.8. The static approach under-\nperforms the full method by 1.4% in general accuracy and\n1.4% in domain accuracy. This performance gap validates\nour hypothesis that the optimal trade-off between plasticity\nand stability is non-stationary; the RL agent successfully\nlearns to prioritize stability in early training phases and\nshift towards aggressive domain acquisition in later stages,\na dynamic trajectory that static heuristics fail to capture.\nNavigator Transferability. Finally, we address the poten-\ntial precision loss introduced by the Navigator proxy. We\nimplemented a ”Target-Calculated” upper bound (w/o Nav-\nTable 2. Ablation Analysis on Qwen3-8B. We report average\naccuracy on Domain tasks (MedQA, Legal, FinQA) and General\ntasks (GSM8K, MMLU, ARC-C). “Upper Bound” calculates gra-\ndients directly on the target model.\nMethod\nDomain\nGeneral\nGSM8K\n∆Cost\nBaselines\nRandom Selection\n51.9\n69.1\n65.1\n-\nComponent Ablation\nw/o Conflict Replay\n59.5\n71.2\n66.8\n+5%\nw/o Orthogonal Prot.\n52.0\n71.8\n68.2\n+5%\nPolicy Ablation\nw/o RL (Static)\n58.8\n71.5\n67.5\n+2%\nArchitecture Ablation\nw/o Navigator (Bound)\n60.5\n72.3\n68.8\n+1400%\nOGS (Full)\n60.2\n72.1\n68.5\n+8%\nigator) where geometric metrics are computed directly on\nthe 8B target model, which is a computationally prohibitive\nsetting for practical use. Remarkably, the Navigator-guided\nOGS matches this theoretical upper bound within a 0.3%\nmargin on domain tasks and 0.2% on general tasks. Cru-\ncially, this near-lossless performance is achieved with a 15×\nreduction in selection cost (see Appendix D for detailed run-\ntime analysis). This result provides compelling evidence\nthat the topological structure of gradient conflicts is scale-\ninvariant, justifying the Navigator-Target paradigm as both\naccurate and highly efficient.\n6. Conclusion\nWe presented Orthogonal Gradient Selection (OGS), a\ndata-centric method that effectively resolves the plasticity-\nstability dilemma in large language model fine-tuning by\ntransferring the geometric insights of gradient surgery to the\ndata selection stage. By employing a lightweight Navigator\nmodel to identify training samples situated within the “safety\nsubspace” of general knowledge, OGS enables computation-\nally efficient, conflict-free updates without the prohibitive\ncost of online gradient projection. We established the theo-\nretical soundness of this approach by demonstrating that our\northogonality criterion serves as a first-order approximation\nto the optimal solution of a constrained bilevel optimization\nproblem. Extensive experiments across medical, legal, and\nfinancial domains confirm that OGS achieves excellent per-\nformance, significantly outperforming existing baselines in\ndomain performance and preventing catastrophic forgetting\nwhile maintaining high data efficiency.\nReferences\nAbbes, I., Subbaraj, G., Riemer, M., Islah, N., Therien,\nB., Tabaru, T., Kingetsu, H., Chandar, S., and Rish, I.\nRevisiting replay and gradient alignment for continual\npre-training of large language models. arXiv preprint\narXiv:2508.01908, 2025.\n8\n"}, {"page": 9, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877–1901, 2020.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L.,\nAschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M.,\nLeike, J., et al. Weak-to-strong generalization: Eliciting\nstrong capabilities with weak supervision. arXiv preprint\narXiv:2312.09390, 2023.\nChaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny,\nM. Efficient lifelong learning with a-gem. arXiv preprint\narXiv:1812.00420, 2018.\nChen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Lang-\ndon, D., Moussa, R., Beane, M., Huang, T.-H., Routledge,\nB. R., et al. Finqa: A dataset of numerical reasoning over\nfinancial data. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pp.\n3697–3711, 2021.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\nColeman, C., Yeh, C., Mussmann, S., Mirzasoleiman, B.,\nBailis, P., Liang, P., Leskovec, J., and Zaharia, M. Selec-\ntion via proxy: Efficient data selection for deep learning.\narXiv preprint arXiv:1906.11829, 2019.\nColombo, P., Pires, T. P., Boudiaf, M., Culver, D., Melo,\nR., Corro, C., Martins, A. F., Esposito, F., Raposo, V. L.,\nMorgado, S., et al. Saullm-7b: A pioneering large lan-\nguage model for law. arXiv preprint arXiv:2403.03883,\n2024.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 confer-\nence of the North American chapter of the association for\ncomputational linguistics: human language technologies,\nvolume 1 (long and short papers), pp. 4171–4186, 2019.\nDiao, S., Shen, X., Shum, K., Song, Y., and Zhang, T. Til-\ngan: transformer-based implicit latent gan for diverse and\ncoherent text generation. In Findings of the Association\nfor Computational linguistics: ACL-IJCNLP 2021, pp.\n4844–4858, 2021.\nGuha, N., Nyarko, J., Ho, D., R´e, C., Chilton, A., Chohlas-\nWood, A., Peters, A., Waldon, B., Rockmore, D., Zam-\nbrano, D., et al. Legalbench: A collaboratively built\nbenchmark for measuring legal reasoning in large lan-\nguage models. Advances in neural information process-\ning systems, 36:44123–44279, 2023.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D., and Steinhardt, J. Measuring massive multitask\nlanguage understanding. In International Conference on\nLearning Representations, 2021.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,\nDe Laroussilhe, Q., Gesmundo, A., Attariyan, M., and\nGelly, S. Parameter-efficient transfer learning for nlp. In\nInternational conference on machine learning, pp. 2790–\n2799. PMLR, 2019.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., Chen, W., et al. Lora: Low-rank adaptation\nof large language models. ICLR, 1(2):3, 2022.\nIlharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S.,\nSchmidt, L., Hajishirzi, H., and Farhadi, A. Editing mod-\nels with task arithmetic. arXiv preprint arXiv:2212.04089,\n2022.\nJin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and\nSzolovits, P. What disease does this patient have? a\nlarge-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14):6421, 2021.\nKatz, D. M., Bommarito, M. J., Gao, S., and Arredondo, P.\nGpt-4 passes the bar exam. Philosophical Transactions\nof the Royal Society A, 382(2270):20230254, 2024.\nKillamsetty, K., Sivasubramanian, D., Ramakrishnan, G.,\nand Iyer, R. Glister: Generalization based data subset se-\nlection for efficient and robust learning. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 35,\npp. 8110–8118, 2021.\nKumar, A., Raghunathan, A., Jones, R., Ma, T., and\nLiang, P.\nFine-tuning can distort pretrained features\nand underperform out-of-distribution.\narXiv preprint\narXiv:2202.10054, 2022.\nLiu, C., Tian, Y., Chen, W., Song, Y., and Zhang, Y. Boot-\nstrapping large language models for radiology report gen-\neration. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 38, pp. 18635–18643, 2024.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tun-\ning. Advances in neural information processing systems,\n36:34892–34916, 2023.\nLiu, J., Tian, Y., and Song, Y. Balanced training data aug-\nmentation for aspect-based sentiment analysis. arXiv\npreprint arXiv:2507.09485, 2025a.\n9\n"}, {"page": 10, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nLiu, M., Song, Y., Zou, H., and Zhang, T. Reinforced\ntraining data selection for domain adaptation. In Proceed-\nings of the 57th annual meeting of the association for\ncomputational linguistics, pp. 1957–1968, 2019.\nLiu, Y., Wang, S., Liu, Z., Song, Z., Wang, J., Liu, J., Liu,\nQ., and Wang, Y. Learn more, forget less: A gradient-\naware data selection approach for llm. arXiv preprint\narXiv:2511.08620, 2025b.\nLopez-Paz, D. and Ranzato, M. Gradient episodic memory\nfor continual learning. Advances in neural information\nprocessing systems, 30, 2017.\nLu, J., Zhang, D., Wu, X., Gao, X., Gan, R., Zhang, J.,\nSong, Y., and Zhang, P. Ziya-visual: Bilingual large\nvision-language model via multi-task instruction tuning.\narXiv preprint arXiv:2310.08166, 2023.\nLuo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., and Zhang, Y.\nAn empirical study of catastrophic forgetting in large\nlanguage models during continual fine-tuning.\nIEEE\nTransactions on Audio, Speech and Language Processing,\n2025.\nMcCloskey, M. and Cohen, N. J. Catastrophic interfer-\nence in connectionist networks: The sequential learning\nproblem. In Psychology of learning and motivation, vol-\nume 24, pp. 109–165. Elsevier, 1989.\nMermillod, M., Bugaiska, A., and Bonin, P. The stability-\nplasticity dilemma: Investigating the continuum from\ncatastrophic forgetting to age-limited learning effects,\n2013.\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Alobei-\ndli, H., Cappelli, A., Pannier, B., Almazrouei, E., and\nLaunay, J. The refinedweb dataset for falcon llm: Outper-\nforming curated corpora with web data only. Advances\nin Neural Information Processing Systems, 36:79155–\n79172, 2023.\nRebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H.\nicarl: Incremental classifier and representation learning.\nIn Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pp. 2001–2010, 2017.\nSaha, G., Garg, I., and Roy, K. Gradient projection memory\nfor continual learning. arXiv preprint arXiv:2103.09762,\n2021.\nShi, H., Xu, Z., Wang, H., Qin, W., Wang, W., Wang, Y.,\nWang, Z., Ebrahimi, S., and Wang, H. Continual learning\nof large language models: A comprehensive survey. ACM\nComputing Surveys, 58(5):1–42, 2025.\nShu, D., Zhao, H., Liu, X., Demeter, D., Du, M., and Zhang,\nY. Lawllm: Law large language model for the us legal\nsystem. In Proceedings of the 33rd ACM International\nConference on information and knowledge management,\npp. 4882–4889, 2024.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung,\nH. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\net al. Large language models encode clinical knowledge.\nNature, 620(7972):172–180, 2023.\nSong, Y., Klassen, P., Xia, F., and Kit, C. Entropy-based\nTraining Data Selection for Domain Adaptation. In Pro-\nceedings of COLING 2012: Posters, pp. 1191–1200, De-\ncember 2012.\nSong, Y., Zhang, T., Wang, Y., and Lee, K.-F. ZEN 2.0:\nContinue Training and Adaption for N-gram Enhanced\nText Encoders. arXiv preprint arXiv:2105.01279, 2021.\nSu, C., Tian, Y., Liu, Q., Zhang, J., and Song, Y. Fusing\nlarge language models with temporal transformers for\ntime series forecasting. arXiv preprint arXiv:2507.10098,\n2025.\nTian, Y. and Song, Y. Feature decomposition via shared\nlow-rank matrix recovery for ct report generation. IEEE\nTransactions on Medical Imaging, pp. 1–1, 2025.\nTian, Y., Gan, R., Song, Y., Zhang, J., and Zhang, Y.\nChiMed-GPT: A Chinese medical large language model\nwith full training regime and better alignment to human\npreferences. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 7156–7173, Bangkok, Thailand,\nAugust 2024.\nWang, N., Yang, H., and Wang, C. D. Fingpt: Instruction\ntuning benchmark for open-source large language models\nin financial datasets. arXiv preprint arXiv:2310.04793,\n2023.\nWang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren,\nX., Su, G., Perot, V., Dy, J., and Pfister, T. Learning\nto prompt for continual learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pp. 139–149, 2022.\nWenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V.,\nGuzm´an, F., Joulin, A., and Grave, E. Ccnet: Extracting\nhigh quality monolingual datasets from web crawl data.\nIn Proceedings of the twelfth language resources and\nevaluation conference, pp. 4003–4012, 2020.\nWu, R., Samanta, A., Jain, A., Fujimoto, S., Kwon, J.,\nKretzu, B., Yu, Y., Hassani, K., Vidolov, B., and Efroni,\nY. Imbalanced gradients in rl post-training of multi-task\nllms. arXiv preprint arXiv:2510.19178, 2025.\n10\n"}, {"page": 11, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nWu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M.,\nGehrmann, S., Kambadur, P., Rosenberg, D., and Mann,\nG. Bloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564, 2023.\nXia, M., Malladi, S., Gururangan, S., Arora, S., and Chen,\nD. Less: Selecting influential data for targeted instruction\ntuning. arXiv preprint arXiv:2402.04333, 2024.\nXie, Q., Han, W., Zhang, X., Lai, Y., Peng, M., Lopez-\nLira, A., and Huang, J. Pixiu: A large language model,\ninstruction data and evaluation benchmark for finance.\narXiv preprint arXiv:2306.05443, 2023a.\nXie, S. M., Santurkar, S., Ma, T., and Liang, P. S. Data\nselection for language models via importance resampling.\nAdvances in Neural Information Processing Systems, 36:\n34201–34227, 2023b.\nYadav, P., Tam, D., Choshen, L., Raffel, C. A., and Bansal,\nM. Ties-merging: Resolving interference when merging\nmodels.\nAdvances in Neural Information Processing\nSystems, 36:7093–7115, 2023.\nYi, B., Li, J., Zhang, B., Nie, L., Li, T., Huang, T., and\nLiu, Z. Gradient surgery for safe llm fine-tuning. arXiv\npreprint arXiv:2508.07172, 2025.\nYu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K.,\nand Finn, C. Gradient surgery for multi-task learning.\nAdvances in neural information processing systems, 33:\n5824–5836, 2020.\nZhang, H., Bai, J., Song, Y., Xu, K., Yu, C., Song, Y., Ng,\nW., and Yu, D. Multiplex Word Embeddings for Selec-\ntional Preference Acquisition. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp.\n5247–5256, Hong Kong, China, November 2019.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. Advances in Neural Information Processing\nSystems, 36:55006–55021, 2023.\n11\n"}, {"page": 12, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nA. Theoretical Appendix\nIn this appendix, we provide the complete derivations for the\ntheorems presented in Section 4, elaborate on the physical\ninterpretations of gradient geometry, and offer a detailed\ncomplexity analysis.\nA.1. Proof of Theorem 4.1 (Gradient Interference)\nSetup. Consider the parameter update at step t, θt+1 =\nθt −η∇Lmed(θt). We analyze the variation in the general\ntask loss Lgen.\nProof. Applying a multivariate Taylor series expansion to\nLgen(θt+1) around θt:\nLgen(θt+1) = Lgen(θt) + ∇Lgen(θt)⊤(θt+1 −θt)\n+1\n2(θt+1 −θt)⊤H(θt+1 −θt) + O(η3),\n(13)\nwhere H is the Hessian matrix. Substituting the update rule:\n∆Lgen = Lgen(θt+1) −Lgen(θt) =\n−η⟨∇Lgen(θt), ∇Lmed(θt)⟩+ O(η2).\n(14)\nRemark on Safety Subspace. The term ⟨∇Lgen, ∇Lmed⟩\nacts as the ”Interference Coefficient.”\n• Conflict: If the inner product is negative (obtuse angle),\n∆Lgen > 0, indicating forgetting.\n• Orthogonality: If the inner product is zero, ∆Lgen ≈\n0. This defines the geometric boundary of the Safety\nSubspace S⊥.\n• Synergy: If positive (acute angle), domain learning aids\ngeneral capabilities.\nExisting methods like SafeGrad enforce this orthogonality\nvia projection g ←g −projgref (g) during training. OGS,\nconversely, pre-filters data to ensure g ∈S⊥, achieving the\nsame theoretical guarantee without modifying the optimiza-\ntion dynamics.\nA.2. Proof of Theorem 4.2 (Optimality of OGS)\nWe show that our heuristic selection metrics are derived\nfrom a principled optimization objective.\nProblem Formulation. Let w ∈{0, 1}N be the binary\nselection vector for the candidate pool {xi}N\ni=1, constrained\nby P wi = k. The bilevel problem is:\nmin\nw\nLgen(θ −η\nX\nwi∇ℓ(xi))\n(15)\ns.t.\nX\nwi = k.\n(16)\nProof. Using the first-order expansion from Theorem 4.1,\nminimizing the future general loss is equivalent to:\nmin\nw\n \nLgen(θ) −η\nN\nX\ni=1\nwi⟨∇Lgen(θ), ∇ℓ(xi)⟩\n!\n.\n(17)\nEliminating constants, this reduces to the maximization\nproblem:\nmax\nw,∥w∥0=k\nN\nX\ni=1\nwi⟨∇Lgen(θ), ∇ℓ(xi)⟩.\n(18)\nThe global maximum for this linear objective is obtained by\na greedy selection of the indices i with the largest values of\n⟨∇Lgen(θ), ∇ℓ(xi)⟩.\nConnection to OGS Metrics. Our defined ”Conflict” score\nis −cos(g, gref). Therefore, maximizing the inner product\nis equivalent to minimizing the Conflict score (preferring\nnegative values, i.e., synergy) or selecting samples with\nzero Conflict (orthogonality) when synergy is unavailable.\nThis proves OGS is the optimal greedy strategy for one-step\nlookahead safety.\nA.3. Detailed Complexity Analysis (Proposition 4.3)\nHere we quantify the ”Efficiency-Safety Trade-off.”\nDefinitions. Let Cfwd(θ) denote FLOPs for a forward pass.\nFor Transformers, C(θ) ∝|θ|. Let ρ = |θn|/|θt| be the\nscale ratio.\nDerivation.\n1. Online Gradient Surgery (e.g., PCGrad/SafeGrad):\nRequires computing gradients for the auxiliary task\n(∇Lgen) at every step on the target model.\nCostGS ≈T·(2×Coststep(θt))+T·Costproj ≈2·CostSF T .\nThis implies a 100% overhead.\n2. OGS (Ours): Requires one pass over data N on Navi-\ngator, plus standard training on Target.\nCostOGS ≈N · Coststep(θn) + T · Coststep(θt).\n12\n"}, {"page": 13, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nComparison. The relative overhead of OGS is:\nCostOGS −CostSF T\nCostSF T\n≈N · |θn|\nN · |θt| = ρ.\nFor a Qwen-0.5B Navigator guiding a Llama-3-70B Target,\nρ ≈1/140 ≈0.7%. Thus, OGS achieves the safety ben-\nefits of gradient geometry with < 1% computational cost,\nwhereas online methods incur ∼100% cost.\nA.4. Bound on Navigator-Target Transferability\nA key assumption of OGS is that gradient geometry pre-\nserves rank ordering across model scales. While exact gra-\ndient vectors differ, the semantic orientation of gradients\nrelative to the anchor direction gref is largely determined\nby the data content.\nLet rn and rt be the vectors of cosine similarities for the\ndataset on the Navigator and Target models, respectively.\nWe posit that:\nSpearmanCorr(rn, rt) ≥γ > 0.\n(19)\nEmpirically, we observe γ ∈[0.4, 0.7] for models within the\nsame family. The probability of a ”false positive” (selecting\na sample that is safe on Navigator but conflicting on Target)\ndecreases as γ increases. Since OGS acts as a filter, even\na moderate γ ensures that the distribution of the selected\nsubset is significantly safer than random sampling, which is\nsufficient for robust continual learning.\nB. Detailed Experimental Setup\nB.1. Dataset Details\nWe evaluate our method on three high-stakes vertical do-\nmains:\n• Medical: We use the MEDQA (USMLE) dataset (Jin\net al., 2021), consisting of multiple-choice questions\nfrom the United States Medical Licensing Examination.\nThe training set contains approximately 10k samples.\n• Legal: We utilize LEGALBENCH (Guha et al., 2023),\na comprehensive benchmark for legal reasoning. We\nsampled a subset of 10k distinct tasks covering contract\ninterpretation and rule application.\n• Finance: We employ FINQA (Chen et al., 2021),\nwhich requires numerical reasoning over financial re-\nports. The training set consists of roughly 6k QA pairs.\nTo construct the General-Knowledge Anchor, we sam-\npled 400 instances following a stratified strategy: 150\nfrom GSM8K (train) for mathematical reasoning, 150 from\nMMLU (auxiliary train) for world knowledge, and 100 from\nALPACA-GPT4 for instruction following compliance. This\nanchor set remains fixed across all experiments to ensure\nfair comparison.\nB.2. Model Configuration and Training\nHyperparameters\nOur experiments primarily utilize the QWEN3 and LLAMA-\n3 model families.\n• Target Models: QWEN3-8B, QWEN3-14B, LLAMA-\n3.1-8B.\n• Navigator Models: QWEN3-0.6B is used as the proxy\nfor the Qwen family, and LLAMA-3.2-1B for the\nLlama family.\nAll fine-tuning is performed using Low-Rank Adaptation\n(LoRA) (Hu et al., 2022) to manage GPU memory con-\nstraints. We use the PEFT library with the following unified\nhyperparameters:\n• LoRA Rank (r): 16\n• LoRA Alpha: 32\n• Target Modules:\nq proj, k proj, v proj,\no proj, gate proj, up proj, down proj\n• Learning Rate: 2e −4 with a cosine scheduler.\n• Batch Size: 8 (effective batch size of 64 via gradient\naccumulation).\n• Epochs: 3\n• Warmup Ratio: 0.1\n• Optimizer: AdamW with weight decay 0.01.\nExperiments were conducted on a cluster of NVIDIA A100-\n80GB GPUs.\nB.3. Baseline Implementation Details\n• LESS: We followed the official implementation, com-\nputing the cosine similarity between the gradient of\neach training sample and the gradient of the domain\nvalidation set. We utilized the last-layer gradients com-\nputed via LoRA for efficiency.\n• GrADS: We calculated the L2 norm of the gradient for\neach sample. Following the author’s recommendation,\nwe selected samples whose gradient norms fell within\nthe [µ −σ, µ + σ] range of the distribution.\n• Influence Functions: We approximated influence us-\ning the TracInCP estimator, summing the dot product\nof gradients at 3 checkpoints.\n13\n"}, {"page": 14, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nC. Additional Main Results\nIn this section, we provide a comprehensive empirical eval-\nuation of OGS across an extensive experimental grid. We\nevaluate five target models ranging from 1.7B to 14B param-\neters (QWEN3-1.7B, QWEN3-8B, QWEN3-14B, LLAMA-\n3.2-3B, and LLAMA-3.1-8B) under four distinct data se-\nlection budgets (5%, 10%, 20%, and 30%). The detailed\nnumerical results are reported in Tables 3 through 7.\nOur analysis focuses on three critical dimensions: (1) the\nsensitivity of performance to data selection ratios, (2) the\nfailure modes of existing baseline methods, and (3) the\nscalability of the Navigator-Target paradigm.\nC.1. Sensitivity Analysis: Low-Data vs. High-Data\nRegimes\nThe Low-Data Regime (5% – 10%): Efficiency and Pre-\ncision. In scenarios with severely constrained compute\nbudgets, OGS demonstrates exceptional data efficiency. As\nevidenced in Table 4, with only 5% of the training data, OGS\nachieves a domain average of 58.8% on Qwen3-8B, effec-\ntively matching the performance of LESS at 10% (59.0%)\nand surpassing Random Selection at 20% (54.8%). This\nindicates that the ”safety subspace” identified by OGS is not\nmerely a region of low conflict, but also rich in high-utility\ndomain features. By prioritizing orthogonal gradients, OGS\ninherently selects samples that provide novel information\n(high loss on domain task) without traversing the gradient\nmanifold in directions detrimental to general capabilities.\nThe High-Data Regime (20% – 30%): Stability against\nSaturation. A critical observation in our experiments is the\ndiverging behavior of methods as the data budget increases.\nWhile baseline methods like LESS and GRADS continue\nto improve marginally on domain tasks, they suffer from a\nprecipitous decline in general reasoning capabilities. For\ninstance, on Llama-3.1-8B (Table 7), increasing the LESS\nselection ratio from 10% to 30% yields a domain gain of\n+5.5 points but results in a -4.7 point drop in GSM8K ac-\ncuracy. This confirms that simply adding more ”similar”\ndata exacerbates the gradient conflict. In contrast, OGS\nexhibits a robust stability profile; its GSM8K performance\nremains nearly flat (from 55.2% to 52.8%) even as domain\nperformance climbs to 62.2%. This proves that OGS acts\nas an effective geometric filter, ensuring that additional data\npoints contribute to domain adaptation only if they satisfy\nthe orthogonality constraint.\nC.2. Deconstructing Baseline Failures\nOur extensive comparison reveals specific pathological be-\nhaviors in existing data selection paradigms:\n• The ”Sycophantic” Selection of LESS: LESS selects\ndata based on gradient similarity to the target domain\nvalidation set. While this maximizes domain align-\nment, our results suggest it creates a ”sycophantic”\nupdate direction that overfits to the domain style while\naggressively overwriting the diverse activation patterns\nrequired for general reasoning. The consistently poor\nperformance of LESS on GSM8K across all models\nhighlights the danger of optimizing for similarity with-\nout a geometric safety constraint.\n• The ”Magnitude Trap” of GrADS: GRADS operates\non the assumption that moderate gradient magnitudes\nimply high utility. However, our analysis suggests\nthat magnitude is a scalar proxy that fails to capture\nvector directionality. High-magnitude gradients often\ncorrespond to samples that are either outliers or fun-\ndamentally conflicting with the pre-trained knowledge\nbase. By ignoring the cosine similarity with the anchor\ngradients, GrADS inadvertently promotes samples that\ninduce large, destructive updates to the model’s core\nreasoning circuits.\n• Heuristics Limitations: While heuristic methods like\nPerplexity and Influence Functions generally outper-\nform random selection, they lack a mechanism to ex-\nplicitly model the trade-off between plasticity (learning\nnew tasks) and stability (retaining old tasks). Conse-\nquently, their performance is inconsistent across differ-\nent domains and model architectures.\nC.3. Scalability of the Navigator-Target Paradigm\nA pivotal contribution of this work is the validation of cross-\nmodel transferability for gradient geometry. The results\nfor QWEN3-14B (Table 5) are particularly illuminating.\nDespite the Navigator model (QWEN3-0.6B) being approx-\nimately 23× smaller than the target, the data selected based\non the Navigator’s gradient subspace successfully steers the\n14B model towards Pareto-optimal performance.\nThis observation supports a strong theoretical hypothesis:\nthe topology of the gradient conflict manifold is largely\ninvariant across model scales within the same architec-\ntural family. The directions in parameter space that repre-\nsent ”medical knowledge” versus ”mathematical reasoning”\nmaintain a consistent geometric relationship (orthogonality),\nallowing a lightweight Navigator to serve as an effective\nproxy for significantly larger models. This property is cru-\ncial for the practical deployment of OGS in large-scale\ncontinual learning scenarios, where computing full gradi-\nents on 70B+ models for data selection is computationally\nprohibitive.\n14\n"}, {"page": 15, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nTable 3. Detailed Results for Qwen3-1.7B. Performance comparison across four data selection ratios. The Navigator used is Qwen3-0.6B.\nRatio\nMethod\nDomain Performance\nGeneral Capability\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nMMLU\nARC-C\nAvg.\nFull Data (100%)\n52.5\n48.2\n54.1\n51.6\n45.2\n50.5\n53.8\n49.8\n5%\nRandom\n38.2\n35.1\n39.4\n37.6\n48.1\n51.2\n54.3\n51.2\nPerplexity\n40.5\n36.8\n41.2\n39.5\n47.8\n51.0\n54.0\n50.9\nInfluence Func.\n41.2\n37.5\n41.8\n40.2\n48.0\n51.5\n54.2\n51.2\nLESS\n42.5\n39.8\n44.1\n42.1\n45.2\n49.1\n52.0\n48.8\nGRADS\n43.1\n40.5\n44.8\n42.8\n47.5\n50.8\n53.5\n50.6\nOGS (Ours)\n45.6\n42.3\n47.2\n45.0\n49.8\n52.5\n55.1\n52.5\n10%\nRandom\n41.5\n38.2\n42.6\n40.8\n46.5\n49.8\n52.8\n49.7\nPerplexity\n43.8\n40.1\n44.5\n42.8\n46.2\n49.5\n52.5\n49.4\nInfluence Func.\n44.5\n40.8\n45.2\n43.5\n46.8\n50.1\n53.0\n50.0\nLESS\n46.8\n43.2\n48.5\n46.2\n42.1\n46.5\n49.2\n45.9\nGRADS\n46.5\n44.1\n48.1\n46.2\n46.2\n49.5\n52.1\n49.3\nOGS (Ours)\n48.9\n45.8\n50.4\n48.4\n48.9\n51.8\n54.6\n51.8\n20%\nRandom\n44.2\n41.5\n45.1\n43.6\n44.1\n48.2\n51.5\n47.9\nPerplexity\n46.5\n43.2\n47.8\n45.8\n43.5\n47.8\n50.8\n47.4\nInfluence Func.\n47.2\n44.1\n48.5\n46.6\n44.5\n48.5\n51.2\n48.1\nLESS\n49.5\n46.8\n51.2\n49.2\n39.5\n44.2\n47.8\n43.8\nGRADS\n49.1\n47.5\n50.8\n49.1\n44.5\n48.1\n50.5\n47.7\nOGS (Ours)\n51.2\n48.5\n52.8\n50.8\n47.5\n50.5\n53.2\n50.4\n30%\nRandom\n46.5\n43.8\n47.5\n45.9\n42.5\n46.8\n50.1\n46.5\nPerplexity\n48.8\n45.5\n49.8\n48.0\n41.2\n45.5\n48.8\n45.2\nInfluence Func.\n49.5\n46.2\n50.5\n48.7\n42.5\n46.2\n49.5\n46.1\nLESS\n51.8\n48.9\n53.4\n51.4\n37.2\n42.5\n45.8\n41.8\nGRADS\n51.2\n49.5\n52.8\n51.2\n42.8\n46.5\n49.2\n46.2\nOGS (Ours)\n53.5\n50.8\n54.6\n53.0\n46.2\n49.8\n52.5\n49.5\nD. Additional Ablation Studies\nIn this section, we provide a granular breakdown of the\nablation experiments discussed in Section 5.3. We detail the\nexperimental setup for the baseline comparisons and offer\nan extended analysis of the computational efficiency gains\nprovided by the Navigator architecture.\nD.1. Experimental Setup for Ablations\nAll ablation studies were conducted using the QWEN3-8B\nas the Target model and QWEN3-0.6B as the Navigator. The\ntraining hyperparameters (learning rate, batch size, LoRA\nrank) were kept consistent with the main experiments to\nensure a fair comparison.\n• w/o Conflict Replay: This variant removes the replay\nbuffer entirely. The selection policy is restricted to\nsampling solely from the domain pool based on orthog-\nonality scores.\n• w/o Orthogonal Protection: This variant disables\nthe orthogonality filter for domain data (effectively\nrandom selection) but retains the conflict-aware replay\nmechanism for general data.\n• w/o RL (Static-Greedy): Instead of the PPO agent,\nwe use a fixed mixing coefficient α = 0.8 (derived\nfrom the average α of the trained policy). Data selec-\ntion within clusters is performed greedily based on the\nhighest orthogonality score rather than probabilistic\nsampling.\n• w/o Navigator (Upper Bound): We perform the\n”Strategy Learning” phase directly on the Target model.\nThis requires computing gradients for the entire candi-\ndate pool using the 8B parameter model, serving as a\ntheoretical performance ceiling.\n15\n"}, {"page": 16, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nTable 4. Detailed Results for Qwen3-8B. Performance comparison across four data selection ratios. The Navigator used is Qwen3-0.6B.\nRatio\nMethod\nDomain Performance\nGeneral Capability\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nMMLU\nARC-C\nAvg.\nFull Data (100%)\n70.5\n62.1\n65.4\n66.0\n55.3\n60.1\n65.4\n60.3\n5%\nRandom\n52.1\n45.8\n49.5\n49.1\n66.5\n71.2\n73.1\n70.3\nPerplexity\n53.5\n47.2\n50.8\n50.5\n66.2\n70.8\n72.8\n69.9\nInfluence Func.\n54.2\n47.8\n51.5\n51.2\n66.8\n71.5\n73.0\n70.4\nLESS\n58.5\n52.4\n55.8\n55.6\n62.1\n66.5\n69.2\n65.9\nGRADS\n57.8\n53.8\n56.5\n56.0\n67.5\n71.8\n74.1\n71.1\nOGS (Ours)\n60.5\n56.2\n59.8\n58.8\n69.2\n73.5\n75.4\n72.7\n10%\nRandom\n55.2\n48.5\n52.1\n51.9\n65.1\n70.2\n72.1\n69.1\nPerplexity\n56.4\n50.1\n53.2\n53.2\n64.5\n69.5\n71.5\n68.5\nInfluence Func.\n57.1\n51.3\n54.5\n54.3\n65.2\n69.8\n72.0\n69.0\nLESS\n62.3\n55.4\n59.2\n59.0\n60.1\n65.4\n68.2\n64.6\nGRADS\n61.5\n57.2\n60.1\n59.6\n66.8\n71.2\n73.5\n70.5\nOGS (Ours)\n64.1\n59.5\n62.8\n62.1\n68.5\n72.8\n74.9\n72.1\n20%\nRandom\n58.5\n51.2\n54.8\n54.8\n63.5\n68.8\n70.5\n67.6\nPerplexity\n59.8\n52.5\n56.2\n56.2\n62.5\n67.8\n69.8\n66.7\nInfluence Func.\n60.5\n53.2\n57.1\n56.9\n63.2\n68.5\n70.8\n67.5\nLESS\n65.1\n58.2\n61.5\n61.6\n56.5\n62.1\n65.8\n61.5\nGRADS\n64.2\n59.8\n62.4\n62.1\n65.2\n69.8\n71.8\n68.9\nOGS (Ours)\n66.8\n61.5\n64.5\n64.3\n67.2\n71.5\n73.8\n70.8\n30%\nRandom\n61.2\n53.8\n57.2\n57.4\n61.8\n67.2\n69.1\n66.0\nPerplexity\n62.5\n55.2\n58.5\n58.7\n60.5\n65.8\n68.2\n64.8\nInfluence Func.\n63.2\n56.5\n59.8\n59.8\n61.2\n66.5\n69.5\n65.7\nLESS\n67.5\n60.5\n63.8\n63.9\n53.2\n58.5\n62.4\n58.0\nGRADS\n66.8\n61.5\n64.2\n64.2\n63.8\n68.2\n70.5\n67.5\nOGS (Ours)\n68.5\n63.2\n66.5\n66.1\n66.5\n70.8\n72.5\n69.9\nD.2. Detailed Analysis of Components\nTable 8 presents the full breakdown of performance across\nindividual datasets. A critical observation is the behavior\nof the w/o Conflict Replay setting on fragile tasks like\nGSM8K. While this variant achieves a competitive MedQA\nscore (62.8%), it suffers a significant drop in GSM8K ac-\ncuracy (-1.7% compared to full OGS). This underscores\nthat even with orthogonal data selection, a small fraction of\nhigh-utility domain samples may still lie near the boundary\nof the safety subspace, necessitating corrective replay to\nprevent manifold drift. Conversely, the w/o Orthogonal\nProtection setting maintains high GSM8K performance but\nfails to improve LegalBench significantly (50.5% vs. 59.5%\nfor OGS), proving that replay alone is insufficient for effec-\ntive domain adaptation; the model requires a high density\nof ”safe” domain gradients to learn effectively.\nD.3. The Cost-Accuracy Trade-off of the Navigator\nA central contribution of OGS is the Navigator-Target archi-\ntecture. To quantify the value of this design, we measured\nthe total GPU hours required for the ”Strategy Learning”\n(data selection) phase.\nAs shown in the final column of Table 8, calculating geomet-\nric features directly on the Target model (w/o Navigator)\nconsumes approximately 24.5 GPU hours for the 10% sub-\nset selection. In contrast, the OGS pipeline using the 0.6B\nNavigator requires only 1.6 GPU hours—a speedup factor of\napproximately 15×. Despite this massive reduction in com-\npute, the performance degradation is negligible (< 0.3% on\naverage).\nThis result empirically validates the Gradient Alignment\nHypothesis discussed in Section 3.2: the relative geomet-\nric orientation of data samples with respect to a general\n16\n"}, {"page": 17, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nTable 5. Detailed Results for Qwen3-14B. Performance comparison across four data selection ratios. The Navigator used is Qwen3-0.6B.\nRatio\nMethod\nDomain Performance\nGeneral Capability\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nMMLU\nARC-C\nAvg.\nFull Data (100%)\n74.5\n68.2\n72.1\n71.6\n64.5\n68.8\n72.1\n68.5\n5%\nRandom\n55.4\n48.5\n53.2\n52.4\n69.8\n73.5\n75.8\n73.0\nPerplexity\n57.2\n50.1\n54.8\n54.0\n69.2\n73.1\n75.2\n72.5\nInfluence Func.\n57.8\n50.8\n55.5\n54.7\n69.5\n73.2\n75.5\n72.7\nLESS\n62.5\n56.2\n60.8\n59.8\n64.5\n70.2\n72.1\n68.9\nGRADS\n61.8\n57.5\n61.2\n60.2\n70.5\n74.2\n76.5\n73.7\nOGS (Ours)\n64.5\n59.8\n63.5\n62.6\n72.1\n75.5\n77.8\n75.1\n10%\nRandom\n58.4\n52.1\n56.5\n55.7\n68.2\n72.1\n74.5\n71.6\nPerplexity\n60.2\n53.8\n58.2\n57.4\n67.5\n71.5\n73.8\n70.9\nInfluence Func.\n60.8\n54.5\n58.9\n58.1\n67.8\n71.8\n74.2\n71.3\nLESS\n66.2\n59.8\n64.1\n63.4\n62.5\n68.4\n70.1\n67.0\nGRADS\n65.8\n61.2\n64.5\n63.8\n69.1\n73.2\n75.8\n72.7\nOGS (Ours)\n68.5\n63.4\n67.2\n66.4\n71.5\n74.8\n77.2\n74.5\n20%\nRandom\n61.5\n55.2\n59.5\n58.7\n66.5\n70.5\n72.8\n69.9\nPerplexity\n63.2\n56.8\n61.2\n60.4\n65.2\n69.5\n71.5\n68.7\nInfluence Func.\n64.1\n57.5\n62.1\n61.2\n65.8\n69.8\n72.1\n69.2\nLESS\n69.5\n62.8\n66.8\n66.4\n59.2\n65.1\n68.2\n64.2\nGRADS\n68.2\n63.5\n66.5\n66.1\n67.8\n71.5\n74.2\n71.2\nOGS (Ours)\n71.2\n65.8\n69.5\n68.8\n70.2\n73.5\n76.1\n73.3\n30%\nRandom\n64.2\n58.5\n61.8\n61.5\n64.8\n68.8\n71.2\n68.3\nPerplexity\n65.8\n60.1\n63.5\n63.1\n63.5\n67.5\n69.8\n66.9\nInfluence Func.\n66.5\n60.8\n64.2\n63.8\n64.2\n68.2\n70.5\n67.6\nLESS\n71.8\n65.2\n68.9\n68.6\n55.5\n61.2\n65.4\n60.7\nGRADS\n70.5\n65.5\n68.5\n68.2\n65.5\n70.1\n72.8\n69.5\nOGS (Ours)\n73.5\n67.8\n71.2\n70.8\n68.8\n72.4\n75.2\n72.1\nknowledge anchor is a semantic property that is largely pre-\nserved across model scales. The ”False Positives” (samples\ndeemed orthogonal by the Navigator but conflicting for the\nTarget) are statistically rare enough that they do not desta-\nbilize the training process, particularly when coupled with\nour conflict-aware replay mechanism.\n17\n"}, {"page": 18, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nTable 6. Detailed Results for Llama-3.2-3B. Performance comparison across four data selection ratios. The Navigator used is Llama-3.2-\n1B.\nRatio\nMethod\nDomain Performance\nGeneral Capability\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nMMLU\nARC-C\nAvg.\nFull Data (100%)\n57.2\n53.5\n56.8\n55.8\n40.5\n51.5\n55.2\n49.1\n5%\nRandom\n42.1\n38.5\n41.2\n40.6\n43.5\n55.8\n59.2\n52.8\nPerplexity\n43.8\n40.2\n42.8\n42.3\n43.2\n55.2\n58.8\n52.4\nInfluence Func.\n44.2\n40.5\n43.5\n42.7\n43.5\n55.5\n59.0\n52.7\nLESS\n48.2\n43.5\n46.8\n46.2\n40.2\n51.2\n55.4\n48.9\nGRADS\n48.5\n45.2\n48.5\n47.4\n46.5\n56.5\n60.1\n54.4\nOGS (Ours)\n50.5\n46.5\n50.2\n49.1\n47.8\n58.2\n61.5\n55.8\n10%\nRandom\n45.2\n41.5\n44.8\n43.8\n42.1\n54.2\n58.1\n51.5\nPerplexity\n46.8\n43.2\n46.5\n45.5\n41.5\n53.8\n57.5\n50.9\nInfluence Func.\n47.5\n43.8\n47.2\n46.2\n42.2\n54.1\n57.8\n51.4\nLESS\n51.5\n46.8\n50.2\n49.5\n38.5\n49.8\n53.2\n47.2\nGRADS\n51.1\n48.2\n51.5\n50.3\n45.2\n55.1\n58.8\n53.0\nOGS (Ours)\n53.8\n49.5\n53.1\n52.1\n46.5\n56.8\n60.2\n54.5\n20%\nRandom\n48.5\n44.2\n47.5\n46.7\n40.5\n52.5\n56.5\n49.8\nPerplexity\n50.2\n45.8\n49.2\n48.4\n39.5\n51.8\n55.8\n49.0\nInfluence Func.\n50.8\n46.5\n49.8\n49.0\n40.8\n52.1\n56.2\n49.7\nLESS\n54.2\n49.5\n53.5\n52.4\n35.8\n46.5\n50.8\n44.4\nGRADS\n53.8\n50.5\n53.2\n52.5\n43.5\n53.8\n57.5\n51.6\nOGS (Ours)\n56.5\n52.1\n55.8\n54.8\n45.2\n55.5\n58.8\n53.2\n30%\nRandom\n51.2\n46.8\n50.2\n49.4\n38.8\n50.8\n54.5\n48.0\nPerplexity\n52.8\n48.2\n52.1\n51.0\n37.5\n49.5\n53.2\n46.7\nInfluence Func.\n53.5\n49.1\n52.8\n51.8\n38.5\n50.2\n53.8\n47.5\nLESS\n55.8\n51.2\n55.2\n54.1\n32.5\n43.8\n48.2\n41.5\nGRADS\n55.2\n52.1\n54.8\n54.0\n41.5\n52.1\n55.5\n49.7\nOGS (Ours)\n57.8\n53.8\n56.5\n56.0\n43.5\n53.8\n57.2\n51.5\n18\n"}, {"page": 19, "text": "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation\nTable 7. Detailed Results for Llama-3.1-8B. Performance comparison across four data selection ratios. The Navigator used is Llama-3.2-\n1B.\nRatio\nMethod\nDomain Performance\nGeneral Capability\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nMMLU\nARC-C\nAvg.\nFull Data (100%)\n65.2\n58.4\n61.3\n61.6\n45.1\n55.2\n58.9\n53.1\n5%\nRandom\n47.5\n42.8\n45.5\n45.3\n51.8\n63.5\n65.5\n60.3\nPerplexity\n49.2\n44.5\n47.1\n46.9\n51.5\n63.1\n64.8\n59.8\nInfluence Func.\n49.8\n45.1\n47.8\n47.6\n51.8\n63.8\n65.2\n60.3\nLESS\n54.2\n48.5\n51.8\n51.5\n50.5\n60.5\n62.8\n57.9\nGRADS\n54.5\n50.5\n52.5\n52.5\n54.8\n64.5\n66.1\n61.8\nOGS (Ours)\n57.2\n52.2\n54.8\n54.7\n56.5\n65.8\n67.5\n63.3\n10%\nRandom\n50.1\n45.3\n48.2\n47.9\n50.5\n62.1\n64.3\n59.0\nPerplexity\n52.3\n46.8\n49.5\n49.5\n51.2\n61.8\n63.5\n58.8\nInfluence Func.\n53.1\n47.2\n50.1\n50.1\n52.0\n62.5\n64.0\n59.5\nLESS\n58.4\n52.1\n55.3\n55.3\n48.2\n58.4\n60.1\n55.6\nGRADS\n57.9\n53.5\n56.1\n55.8\n53.5\n63.1\n64.8\n60.5\nOGS (Ours)\n60.2\n55.1\n58.4\n57.9\n55.2\n64.5\n66.2\n62.0\n20%\nRandom\n53.5\n48.2\n51.5\n51.1\n48.8\n60.5\n63.2\n57.5\nPerplexity\n55.2\n49.8\n53.1\n52.7\n48.5\n60.1\n62.5\n57.0\nInfluence Func.\n56.1\n50.5\n53.8\n53.5\n49.5\n60.8\n63.1\n57.8\nLESS\n61.5\n55.4\n58.5\n58.5\n45.5\n55.8\n58.2\n53.2\nGRADS\n60.5\n56.2\n58.8\n58.5\n52.1\n61.8\n63.5\n59.1\nOGS (Ours)\n62.8\n57.5\n60.5\n60.3\n54.1\n63.2\n65.1\n60.8\n30%\nRandom\n56.2\n50.8\n54.2\n53.7\n47.2\n58.8\n61.5\n55.8\nPerplexity\n57.8\n52.2\n55.8\n55.3\n46.5\n58.2\n60.8\n55.2\nInfluence Func.\n58.5\n53.1\n56.5\n56.0\n47.8\n59.1\n61.5\n56.1\nLESS\n63.8\n57.5\n61.2\n60.8\n43.5\n53.2\n56.5\n51.1\nGRADS\n62.5\n58.2\n60.8\n60.5\n50.5\n60.2\n62.4\n57.7\nOGS (Ours)\n64.5\n59.2\n62.8\n62.2\n52.8\n61.8\n63.8\n59.5\nTable 8. Detailed Ablation Results. We report the specific breakdown across all three domain tasks and the critical GSM8K retention\nmetric. The ”Selection Cost” refers to the GPU hours required to process the candidate pool on a single A100-80GB GPU.\nConfiguration\nDomain Performance\nGeneral Capability\nSelection Cost\nSpeedup\nMEDQA\nLEGAL\nFINQA\nAvg.\nGSM8K\nAvg. Gen\n(GPU Hours)\nOGS (Full Method)\n64.1\n59.5\n62.8\n62.1\n68.5\n72.1\n1.6h\n15×\nComponent Ablations\nw/o Conflict Replay\n62.8\n58.2\n63.1\n61.4\n66.8\n71.2\n1.5h\n16×\nw/o Orthogonal Prot.\n54.5\n50.5\n53.8\n52.9\n69.1\n71.8\n1.5h\n16×\nPolicy Ablations\nw/o RL (Static α = 0.8)\n62.1\n57.8\n60.5\n60.1\n67.5\n71.5\n1.5h\n16×\nArchitecture Ablations\nw/o Navigator (Upper Bound)\n64.5\n60.1\n63.2\n62.6\n68.8\n72.3\n24.5h\n1×\nRandom Selection\n55.2\n48.5\n52.1\n51.9\n65.1\n69.1\n0h\n-\n19\n"}]}