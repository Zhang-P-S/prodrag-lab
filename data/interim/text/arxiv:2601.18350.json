{"doc_id": "arxiv:2601.18350", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.18350.pdf", "meta": {"doc_id": "arxiv:2601.18350", "source": "arxiv", "arxiv_id": "2601.18350", "title": "Adapter Merging Reactivates Latent Reasoning Traces: A Mechanism Analysis", "authors": ["Junyi Zou"], "published": "2026-01-26T10:54:06Z", "updated": "2026-02-11T01:16:07Z", "summary": "Large language models fine-tuned via a two-stage pipeline (domain adaptation followed by instruction alignment) can exhibit non-trivial interference after adapter merging, including the re-emergence of explicit reasoning traces under strict decoding. We study this phenomenon in medical LLM settings using lightweight, reproducible measurements of trace leakage and instruction-following behavior. Beyond marker-based proxies, we introduce a marker-forbidden, answer-only evaluation and define a correctness-based direction that does not rely on surface markers; a rank-1 logit-space intervention along this direction modulates decision distributions and improves multiple-choice accuracy beyond random-direction controls at sufficiently large intervention strength. We further provide layer-wise geometric evidence that domain and instruction adapters induce partially misaligned update directions, and present a proof-of-concept geometry-aware merge that can reduce leakage and/or improve accuracy in a toy setting. Our results characterize boundary conditions of trace leakage and provide practical diagnostics and interventions for safer adapter merging.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.18350v4", "url_pdf": "https://arxiv.org/pdf/2601.18350.pdf", "meta_path": "data/raw/arxiv/meta/2601.18350.json", "sha256": "7e2b05267cc1fa07d51dbfe557a151a58b3a322d1e8f5374c05d677d53205879", "status": "ok", "fetched_at": "2026-02-18T02:20:31.409628+00:00"}, "pages": [{"page": 1, "text": "Adapter Merging Reactivates Latent Reasoning Traces: A\nMechanism Analysis\nJunyi Zou\nZjydiary Group\nzoujunyi@zjydiary.cn\nFebruary 12, 2026\nAbstract\nWe investigate a critical interference phenomenon in modular Low-Rank Adaptation (LoRA)\nmerging, where combining a domain-adaptive pretraining (DAPT) adapter with an instruction-\ntuning (SFT) adapter inadvertently reactivates latent reasoning traces (e.g., <think> / </think>\ntokens), even when such behaviors are explicitly suppressed during inference. Through four\nrigorous experiments, we characterize this phenomenon and identify its mechanism. First, we\ndemonstrate the behavioral conflict across varying merge ratios and decoding presets, showing\nthat performance degradation is non-monotonic. Second, we validate the reproducibility of these\nfindings across different adapter versions, ruling out implementation artifacts. Third, we explore\nboundary conditions across 8 model families and 9 prompting configurations, revealing that\nthis leakage is not exclusive to “reasoning” models but reflects a general property of pretrained\nrepresentations in non-thinking models as well. Finally, we perform a mechanism analysis\nusing layer-wise CKA, PCA, and probing, localizing the conflicting subspace to the final 6–10\nlayers of the network. We further validate this mechanism via a functional causal intervention\nimplemented entirely within vLLM that suppresses the identified subspace, significantly reducing\nleakage. Our findings suggest that adapter merging can trigger structural incompatibility in the\nrepresentation space, challenging the assumption of linear composability in PEFT.\n1\nIntroduction\nParameter-Efficient Fine-Tuning (PEFT) methods like LoRA [Hu et al., 2022] and adapter merging\n[Wortsman et al., 2022] have become standard practices for modularly adding capabilities to\nLarge Language Models (LLMs).\nA common workflow involves merging a Domain-Adaptive\nPretraining (DAPT) adapter [Gururangan et al., 2020] with a Supervised Fine-Tuning (SFT)\nadapter [Ouyang et al., 2022] to combine domain knowledge with instruction-following ability.\nHowever, we identify a severe interference pattern where this combination leads to the reactivation\nof latent behaviors—specifically, reasoning traces—that were intended to be suppressed. This\npaper systematically analyzes this phenomenon through four experiments, moving from behavioral\nobservation to mechanism localization.\nWe hypothesize that these phenomena arise from interference within a latent trace-associated\nsubspace formed during pretraining (Fig. 1). Importantly, we localize this conflict to the final\n6–10 layers via complementary analyses (CKA, PCA, and linear probing), and then validate its\npretraining origin across eight model families before returning to behavioral validation under\nLoRA adapter composition. This mechanism-first order is central to our argument.\n1\narXiv:2601.18350v4  [cs.CL]  11 Feb 2026\n"}, {"page": 2, "text": "Figure 1: Schematic mechanism: a latent trace-associated subspace concentrated in mid-to-late\nlayers; DAPT reactivates, SFT suppresses. Weighted merging induces subspace interference and\nobservable shifts.\nAcross 8 model families and 9 prompting configurations, we consistently observe a 3–15× increase\nin reasoning leakage when domain-adapted and instruction-tuned adapters are merged, compared to\nSFT-only baselines (computed as Merged(α=1.0, PT-only endpoint)\nSFT-only\nunder Strict template; full results in\nAppendix Table A.1). This pattern generalizes across architectures, scales, and training paradigms,\nsuggesting a shared representational substrate rather than an artifact of a specific model family.\nContributions\n• We identify and characterize a non-monotonic interference pattern in adapter merging, where\nstrict suppression appears to give way to reasoning reactivation. Evidence ties to Fig. 3 and\nTable 1.\n• We localize the conflicting subspace to the final 6–10 layers and show it is low-dimensional,\nsupported by CKA/PCA/probe analyses in Figs. 4 and 7.\n• We demonstrate reproducibility across adapter variants and prompting/decoding configurations,\nsuggesting a shared representational substrate rather than a single-run artifact.\n• We demonstrate a functional causal intervention implemented entirely within vLLM that\nmitigates strict-fail/leakage, closing the loop from analysis to control.\n2\nRelated Work\nModel soups and linear merging [Wortsman et al., 2022] aim to combine capabilities by averaging\ncheckpoints or adapters, typically assuming monotonic improvements or neutral interactions. PEFT\nmethods such as LoRA [Hu et al., 2022] provide modular routes to add capabilities, but interference\nbetween modules is underexplored. Our observations emphasize that strict suppression can be\n2\n"}, {"page": 3, "text": "followed by reasoning reactivation, with non-monotonic degradation in instruction-following and\na late-layer low-dimensional signature, suggesting that compositionality in PEFT may require\nsubspace-aware controls rather than naive weighting.\n3\nBehavioral Interference in Adapter Merging\nProblem: Why does merging PT (DAPT) and SFT LoRA adapters lead to the reactivation of\nreasoning traces and mixed surface metrics?\n3.1\nExperimental Design: Controlled Adapter Merging\nTo isolate the effect of adapter merging, we train separate LoRA adapters for Domain-Adaptive\nPretraining (DAPT) and Supervised Fine-Tuning (SFT) on the same base model.\n• Base Model: We use Qwen3-14B (thinking family) as the primary backbone, given its strong\nbaseline performance and open weights [Yang et al., 2025]. Note that Qwen3-14B is used as\nthe base model for adapter training, whereas the cross-model family sweep (e.g., Table A.1)\nincludes Qwen3-8B as a separate evaluation family; we never transfer adapters across different\nbase parameterizations.\n• Adapter Configuration: All adapters are trained with Rank r = 16, LoRA scaling factor\n= 32, and Dropout p = 0.05. We target all linear modules (q proj, k proj, v proj, o proj,\ngate proj, up proj, down proj) to ensure comprehensive coverage [Dettmers et al., 2023].\n• Datasets: Both adapters are trained on data sourced from the MedicalGPT repository\n[Shibing624, 2023]. The DAPT adapter utilizes the pretraining corpus to inject domain\nknowledge, while the SFT adapter utilizes the instruction-tuning subset to enforce instruction\nfollowing.\nWe then merge these adapters using a single-coefficient interpolation: Wmerged = Wbase +\nα ∆WPT + (1 −α) ∆WSFT, where α ∈[0, 1] controls the PT:SFT contribution. We perform a sweep\nover α to observe the behavioral transition with α ∈{0.0, 0.2, 0.4, 0.6, 0.8, 1.0} and fixed seed = 42.\nMethodology: We evaluate merged checkpoints across α ∈[0, 1] under two prompt templates\nand decoding presets. We evaluate merged models with varying PT:SFT ratios under different\nconditions:\n• Merge Ratios: α-sweep to control the contribution of PT vs SFT.\n• Templates: Standard (“plain”) vs suppression (“nothink”).\n• Decoding Presets: Preset A (deterministic) vs Preset B (creative).\nEvidence: As PT weight increases, models begin to output reasoning tokens (e.g., <think>)\neven when the SFT adapter and system prompt explicitly forbid it. This “leakage” correlates with\na drop in instruction-following metrics but a maintenance or increase in domain knowledge metrics.\nConclusion: The interaction is not a simple trade-off but a structural conflict: the PT adapter\nreopens access to pretraining subspaces that SFT suppresses, and merging induces interference in\nthose subspaces.\n3\n"}, {"page": 4, "text": "3.2\nPrompt Templates & Metric Definitions\nPrompt Templates (exact strings used in all evaluations):\n• Plain:\nSystem: You are a helpful assistant.\nUser: {INPUT}\nAssistant:\n• Nothink-Soft:\nSystem: You are a helpful assistant. Do not show your reasoning.\nOnly output the final answer.\nUser: {INPUT}\nAssistant:\n• Nothink-Strict:\nSystem: You are a helpful assistant. PROHIBITION: Do NOT output any\nreasoning or <think> tags. Output ONLY the final answer.\nIf any reasoning is shown, it is considered a failure.\nUser: {INPUT}\nAssistant:\nLeakage Metric (precise definition):\n• Markers counted: strings such as <think>, </think>, Step 1:, Thought:, “let’s think”,\n“step by step”.\n• Regex (examples): <\\s*think\\s*>.*?<\\s*/\\s*think\\s*> (dotall, case-insensitive).\n• Matching rules: case-insensitive; ignore occurrences inside markdown code fences (‘‘‘...‘‘‘\nor ’’’...’’’) and quoted segments; collapse consecutive duplicates; strip whitespace-only\nmarkers; evaluate per-response.\n• Reporting: binary leakage rate (percentage of responses with any marker) and optional\ntoken-count summary (mean count per response); strict-fail is separately recorded when format\nconstraints are violated.\nSurface Metrics (datasets/benchmarks and evaluation method):\n• Instruction-following: format adherence and exact match on instruction-style QA (IFEval-\nlike dataset, N = 500, exact string match rubric).\n• Domain knowledge: arXiv-style QA accuracy and factuality checks (PubMedQA-subset,\nN = 200, model-based judge with reference answer).\n• Evaluation scripts and thresholds follow repository defaults for all reported results.\n4\n"}, {"page": 5, "text": "Model\nPlain Leakage\nStrict Leakage\nStrict Fail\nThink-marker Rate\nGemma-270M\n0.035\n0.006\n0.01\n0.00\nGemma-4B\n0.13\n0.01\n0.10\n0.00\nLlama-8B\n0.05\n0.01\n0.40\n0.00\nQwen-7B\n0.10\n0.002\n0.01\n0.00\nTable 1: Summary of reasoning leakage across models and templates.\n4\nReproducibility Across Adapter Variants\nUsing a distinct set of adapters (v2), the trend of reasoning trace reactivation and metric divergence\nremains robust across ratios, templates, and presets. This confirms that the observed interference is\nnot an implementation artifact but a reproducible phenomenon inherent to the adapter merging\nprocess.\n5\nEvidence of a Shared Reasoning Subspace\nTo verify that the mechanism is not an artifact of adapter training, we conduct a cross-model\nprobing study across eight LLMs without any finetuning. This provides theoretical evidence\nthat the trace-associated subspace is a property of pretrained representations rather than LoRA\nadapters.\n5.1\nCross-Model Experimental Setup\nWe extend our analysis to include both “thinking” models (e.g., DeepSeek-R1-Distill [DeepSeek AI,\n2024]) and standard “non-thinking” models (e.g., Llama-3, Qwen-2.5) to test the universality of the\nphenomenon. In total, we evaluate 8 model families. For each model, we construct 9 evaluation\nconfigurations:\n• 3 Prompt Templates: “Plain” (standard instruction), “Reasoning-focused” (explicitly\nasking for thought process), and “Strict” (explicitly forbidding reasoning).\n• 3 Decoding Presets: Greedy decoding (Temperature=0), Standard sampling (Tempera-\nture=0.7, Top-p=0.9), and Diverse sampling (Temperature=1.0).\nWe define two key metrics for this study:\n1. Leakage Ratio: The proportion of responses where the model generates reasoning markers\n(e.g., <think>, Step 1:) despite being prompted with a strict template. This measures the\nmodel’s inherent tendency to enter the trace-associated subspace.\n2. Strict Fail Rate: The percentage of cases where the model fails to adhere to the strict\nformat constraints, often correlated with the leakage of reasoning traces.\nComplete results covering 8 model families and 9 configurations are provided in the Appendix\n(Table A.1, Figure A.1). The main text presents representative subsets (selected to cover both\n“thinking” and “non-thinking” architectures across varying parameter scales 0.5B–70B) to highlight\nkey trends and demonstrate universality.\n5\n"}, {"page": 6, "text": "Figure 2: Cross-model Think-marker rate heatmap. The prevalence of reasoning traces varies by\nmodel but is reactivated by DAPT adapters.\nWe find that “non-thinking” models also exhibit leakage of latent behaviors when adapters\nare merged. The leakage ratio increases significantly in merged models compared to single-\nadapter baselines. The “trace-associated subspace” or similar latent behavioral manifolds are likely\ngeneral artifacts of large-scale pretraining. SFT adapters suppress these, but DAPT adapters can\ninadvertently unmask them, regardless of whether the model was branded as a “reasoning” model.\n6\nMechanistic Localization via Representation Analysis\nGoal: Identify where the conflicting trace-associated subspace resides in the transformer, and\nexplain why it produces observable behavioral shifts when adapters are composed.\n6.1\nMethodology: Layer-wise Similarity and Probing\nTo rigorously localize the interference, we employ three complementary analytical techniques:\nCentered Kernel Alignment (CKA), Principal Component Analysis (PCA), and Linear Probing.\nCentered Kernel Alignment (CKA) [Kornblith et al., 2019]: We compute the linear\nCKA similarity between the hidden states of the model running with the “plain” template (allowing\nreasoning) and the “strict” template (suppressing reasoning). For a given layer l, let Xl ∈Rn×d and\n6\n"}, {"page": 7, "text": "(a) Leakage Heatmap\n(b) Strict Fail Rate\nFigure 3: Cross-model leakage analysis. Left: Leakage heatmap showing template sensitivity. Right:\nStrict fail rate heatmap indicating robustness issues.\nYl ∈Rn×d denote the activation matrices for the two conditions over n sample sequences. CKA\nmeasures the similarity of the representational geometries invariant to orthogonal transformations.\nA drop in CKA score indicates a divergence in representation. We observe that while early layers\nmaintain high similarity (> 0.98), a significant divergence occurs in the final 6–10 layers across all\nmodels, suggesting that the behavioral conflict is not distributed uniformly but localized to the\nnetwork’s output stages.\nPrincipal Component Analysis (PCA): To understand the dimensionality of this divergence,\nwe perform PCA on the difference vectors ∆l = Xl −Yl. We analyze the explained variance\nratio of the first principal component (PC1). A high PC1 variance indicates that the difference\nin representations is dominated by a single direction or a low-dimensional subspace. Our results\nshow a sharp peak in PC1 explained variance exactly coinciding with the layers identified by CKA,\nconfirming that the “trace-associated subspace” is a low-rank manifold within the high-dimensional\nhidden state space.\nLinear Probing [Alain and Bengio, 2017]: Finally, to verify that this divergence corresponds\nto trace-associated features (explicit reasoning-trace signals), we train linear classifiers on the hidden\nstates to predict the presence of reasoning steps (e.g., <think> tokens) in the generation. We\nconstruct a dataset of 1000 generation pairs and train probes at each layer. The Area Under the\nCurve (AUC) of these probes rises sharply in the same mid-to-late layers, confirming that the\nlocalized subspace explicitly encodes trace-associated signals predictive of marker emission that are\nreactivated during adapter merging.\nEvidence (CKA): Centered Kernel Alignment between “plain” and “strict” (suppressed) paths\nshows small differences in early layers but a pronounced divergence in the final layers across model\nfamilies, indicating that the conflict lives late in the network.\nAcross model families, we observe consistent mid-to-late layer localization of the trace-associated\nsubspace. CKA tells us where the representations diverge; PCA shows the active region is low-\ndimensional; Probing confirms that this region encodes trace-associated signals predictive of marker\nemission. Together they form a single mechanistic narrative, not isolated results.\n7\n"}, {"page": 8, "text": "Figure 4: Localization of behavioral subspace divergence across transformer depth (CKA). Divergence\npeaks in the final 6–10 layers.\nFigure 5: Leakage vs layer index (cross-model average). Probe AUC averaged across three families\nrises sharply in the final layers, corroborating the localization observed with CKA.\n6.2\nFunctional Causal Intervention via Logit-Space Direction Suppression\nTo move beyond correlation, we implement a causal intervention entirely within the vLLM [Kwon\net al., 2023] inference engine (version 0.15.0, TP=4), ensuring consistency with our experimental\npipeline. We construct a custom logits processor that projects the logits onto the orthogonal\ncomplement of the identified trace-associated subspace. Formally, we define a unit direction u ∈R|V |\nin the logit space that aligns with the functional signature of reasoning traces (estimated here as the\ntop principal component of the difference vector between reasoning-active and reasoning-suppressed\nlogit states). For the logits zt ∈R|V | at step t, we apply the projection:\nz′\nt = zt −γ(uT zt)u\n(1)\nwhere γ ≥0 is the intervention strength (we use γ ∈{0, 1, 2, 3, 5} unless otherwise specified; see\nAppendix A.2 for a larger-γ sweep). Unlike keyword filtering that simply zeroes the logits of a small\nset of marker tokens, our intervention removes a global low-rank component from the entire logit\nvector zt, altering the functional realization of the reasoning behavior rather than banning arbitrary\nstrings. This modified state is then used for sampling.\nWe observe a striking, causal reduction in strict-fail rates for “thinking” models (e.g., DeepSeek-\nR1-Distill-Llama-8B) as γ increases, with the strict-fail rate dropping from over 80% to under\n8\n"}, {"page": 9, "text": "Figure 6: Functional causal intervention results. Suppressing the identified trace-associated direction\nin logit space significantly reduces strict-fail rates for thinking models (DeepSeek-R1-Distill), while\nhaving minimal impact on non-thinking baselines. This causally validates the mechanistic role of\nthe late-layer trace-associated subspace.\n35% (Figure 6). Crucially, this intervention has a much smaller effect on “non-thinking” models\n(Llama-3.1-8B, Qwen-2.5-7B), which aligns with our finding that their leakage is less severe. This\nresult causally confirms that the low-dimensional late-layer subspace identified in Section 5.1 is\nindeed the functional driver of the observed interference. We further provide layer-wise geometric\nevidence for adapter update misalignment and a toy geometry-aware merge baseline in Figure A.4\nand Figure A.5.\nMarker-free correctness-defined direction.\nA potential concern is that our primary supervision\nsignal for trace leakage relies on surface markers (e.g., <think>, Step 1), which could be interpreted\nas localizing a formatting/style axis rather than reasoning-relevant computation. To probe this,\nwe construct a marker-free direction ucorr using correctness labels only (correct vs. incorrect)\nunder marker-forbidden, answer-only decoding, and apply the same rank-1 logit suppression\nz′ = z −γ(u⊤z)u with a matched random-direction control. On MCQ benchmarks, sufficiently\nstrong suppression along ucorr changes the decision distribution (option entropy/margin) and yields\naccuracy gains beyond the random control (Appendix A.2). We interpret this as evidence that our\nintervention framework can target axes related to decision confidence/correctness in a marker-free\nsetting, while avoiding the stronger claim that reasoning correctness is governed by a single linear\ndirection.\nConclusion: The mechanism responsible for reasoning traces and the associated adapter\nconflict is primarily localized in the final 6–10 layers of the network, is low-dimensional, and encodes\ntrace-associated signals (predictive of trace/marker emission) detectable by simple probes.\n9\n"}, {"page": 10, "text": "(a) Layer-wise PCA explained variance (PC1).\n(b) Layer-wise probing AUC/Acc.\nFigure 7: Mechanism closure: PCA indicates low-dimensional activation in late layers; Probing\nconfirms reasoning features are encoded there.\n7\nDiscussion: Implications for PEFT and Alignment\nThe identification of a concentrated trace-associated subspace has direct implications for modular\nfine-tuning. First, subspace-aware merging suggests weighting and orthogonalization strategies\nthat explicitly avoid activating conflicting directions near the output head. Second, adapter\nplacement and shape—including per-layer ranks and routing choices—should be informed by the\nobserved mid-to-late layer sensitivity. Third, prompting and decoding can be made robust by\nexplicitly suppressing the identified latent directions when task requirements demand instruction\nadherence over chain-of-thought style behavior. Finally, evaluation must go beyond surface metrics:\nqualitative traces and layer-wise diagnostics are essential for detecting misalignment masked by\naggregate scores [Lin, 2004, Papineni et al., 2002].\n8\nLimitations and Future Directions\nMedicalGPT as a potential confound.\nMedicalGPT is a community-curated instruction\ncorpus and may contain heterogeneous templates, including explicit reasoning traces (e.g., <think>,\n“Step 1”) or formatting conventions. Therefore, our marker-based leakage proxy can partially\nreflect trace/style conventions inherited from the corpus rather than reasoning correctness per\nse. To mitigate this, we (i) evaluate under marker-forbidden, answer-only decoding, (ii) include\nmatched random-direction controls for interventions, and (iii) report a marker-free correctness-\ndefined direction experiment (Exp6) that modulates the decision distribution beyond random\ncontrols.\nThroughout, we treat surface markers as proxies for explicit reasoning traces rather than reasoning\ncorrectness; our causal claims concern modulating trace emission and decision distributions under\ncontrolled decoding.\nOur localization relies on proxy measures (CKA, PCA, linear probes) that, while consistent, are\nnot direct causal interventions. While we have demonstrated a functional causal intervention via\nlogit-space suppression, further work could explore earlier-layer interventions or learned projectors to\n10\n"}, {"page": 11, "text": "more precisely disentangle reasoning from domain knowledge. Additionally, cross-family averaging\ncompresses architectural differences; future work should incorporate architecture-aware normalization.\nFinally, designing subspace-aware PEFT methods that automatically identify and orthogonalize\nconflicting directions in the final layers is a promising direction for compositional alignment [Dao,\n2023].\n9\nConclusion\nWe show that reasoning leakage is not an artifact of adapter design, but a manifestation of interference\nwithin a latent trace-associated subspace that universally exists across pretrained language models.\nThis work provides a comprehensive mechanism analysis of LoRA adapter interference. We show\nthat merging adapters can reactivate latent reasoning traces due to conflicts in the representation\nspace, specifically in the final network layers. This challenges the linear compositionality assumption\nof PEFT and highlights the need for more sophisticated merging strategies that account for subspace\northogonality.\nReferences\nGuillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier\nprobes.\nIn International Conference on Learning Representations (Workshop), 2017.\nURL\nhttps://arxiv.org/abs/1610.01644.\nTri Dao.\nFlashattention-2:\nFaster attention with better parallelism and work partitioning.\narXiv:2307.08691, 2023.\nDeepSeek AI.\nDeepseek-r1 technical report, 2024.\nURL https://github.com/deepseek-ai/\nDeepSeek-R1. Technical report.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning\nof quantized LLMs. arXiv:2305.14314, 2023.\nSuchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks.\narXiv:2004.10964, 2020.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\nConference on Learning Representations, 2022.\nURL https://openreview.net/forum?id=\nnZeVKeeFYf9.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural\nnetwork representations revisited. In International Conference on Machine Learning, pages\n3519–3529, 2019. URL https://proceedings.mlr.press/v97/kornblith19a.html.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Ion Stoica, and Qirong Zhang. Efficient memory management for large language model\nserving with pagedattention. arXiv preprint arXiv:2309.06180, 2023. URL https://arxiv.org/\nabs/2309.06180.\n11\n"}, {"page": 12, "text": "Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out: Proceedings of the ACL-04 Workshop, pages 74–81, 2004.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback.\narXiv:2203.02155, 2022.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association\nfor Computational Linguistics, pages 311–318, 2002. doi: 10.3115/1073083.1073135.\nShibing624. Medicalgpt: Medical domain large language model training repository. https://\ngithub.com/shibing624/MedicalGPT, 2023. GitHub repository.\nMitchell Wortsman, Gabriel Ilharco, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi,\nand Ali Farhadi. Model soups: Averaging weights of multiple fine-tuned models improves accuracy\nwithout increasing inference time. In International Conference on Machine Learning, 2022.\nAn Yang, Baosong Yang, Yusheng Zhang, et al. Qwen3 technical report, 2025.\n12\n"}, {"page": 13, "text": "A\nAdditional Results\nTable A.1: Full cross-model results (8 families × 9 configurations). Metrics: Think-marker Rate\n(proportion of responses with reasoning markers) and Strict Fail Rate (format violation).\nModel\nTemplate\nPreset\nMarker Rate\nStrict Fail\nDS-R1-Distill-Llama-8b\nPlain\nDiverse\n0.95\n0.00\nPlain\nGreedy\n0.97\n0.00\nPlain\nStandard\n0.97\n0.00\nSoft\nDiverse\n0.84\n0.00\nSoft\nGreedy\n0.89\n0.00\nSoft\nStandard\n0.88\n0.00\nStrict\nDiverse\n0.85\n0.86\nStrict\nGreedy\n0.90\n0.90\nStrict\nStandard\n0.91\n0.92\nDS-R1-Distill-Qwen-7b\nPlain\nDiverse\n0.95\n0.00\nPlain\nGreedy\n0.98\n0.00\nPlain\nStandard\n0.98\n0.00\nSoft\nDiverse\n0.90\n0.00\nSoft\nGreedy\n0.93\n0.00\nSoft\nStandard\n0.93\n0.00\nStrict\nDiverse\n0.92\n0.92\nStrict\nGreedy\n0.93\n0.93\nStrict\nStandard\n0.92\n0.92\nDS-R1-Qwen3-8b\nPlain\nDiverse\n0.94\n0.00\nPlain\nGreedy\n0.93\n0.00\nPlain\nStandard\n0.94\n0.00\nSoft\nDiverse\n0.93\n0.00\nSoft\nGreedy\n0.97\n0.00\nSoft\nStandard\n0.95\n0.00\nStrict\nDiverse\n0.88\n1.00\nStrict\nGreedy\n0.95\n1.00\nStrict\nStandard\n0.93\n1.00\nGemma-270m\nPlain\nDiverse\n0.08\n0.00\nPlain\nGreedy\n0.04\n0.00\nPlain\nStandard\n0.05\n0.00\nSoft\nDiverse\n0.02\n0.00\nSoft\nGreedy\n0.01\n0.00\nSoft\nStandard\n0.02\n0.00\nStrict\nDiverse\n0.00\n0.00\nStrict\nGreedy\n0.02\n0.02\nStrict\nStandard\n0.00\n0.00\nGemma-4b\nPlain\nDiverse\n0.16\n0.00\nPlain\nGreedy\n0.15\n0.00\nPlain\nStandard\n0.17\n0.00\nSoft\nDiverse\n0.03\n0.00\nSoft\nGreedy\n0.03\n0.00\nSoft\nStandard\n0.03\n0.00\nStrict\nDiverse\n0.01\n0.09\n13\n"}, {"page": 14, "text": "Model\nTemplate\nPreset\nMarker Rate\nStrict Fail\nStrict\nGreedy\n0.02\n0.11\nStrict\nStandard\n0.02\n0.10\nLlama-8b\nPlain\nDiverse\n0.14\n0.00\nPlain\nGreedy\n0.09\n0.00\nPlain\nStandard\n0.09\n0.00\nSoft\nDiverse\n0.02\n0.00\nSoft\nGreedy\n0.01\n0.00\nSoft\nStandard\n0.02\n0.00\nStrict\nDiverse\n0.02\n0.32\nStrict\nGreedy\n0.01\n0.42\nStrict\nStandard\n0.00\n0.40\nQwen-7b\nPlain\nDiverse\n0.21\n0.00\nPlain\nGreedy\n0.19\n0.00\nPlain\nStandard\n0.16\n0.00\nSoft\nDiverse\n0.01\n0.00\nSoft\nGreedy\n0.00\n0.00\nSoft\nStandard\n0.01\n0.00\nStrict\nDiverse\n0.00\n0.00\nStrict\nGreedy\n0.00\n0.00\nStrict\nStandard\n0.00\n0.00\nQwen3-8b\nPlain\nDiverse\n0.99\n0.00\nPlain\nGreedy\n0.98\n0.00\nPlain\nStandard\n0.98\n0.00\nSoft\nDiverse\n0.02\n0.00\nSoft\nGreedy\n0.01\n0.00\nSoft\nStandard\n0.01\n0.00\nStrict\nDiverse\n0.64\n1.00\nStrict\nGreedy\n0.72\n1.00\nStrict\nStandard\n0.68\n1.00\nA.1\nRobustness Checks\nGamma sanity: On a small holdout set (N=40), increasing gamma shows an overall decreasing\n(largely non-increasing) trend in leakage across multiple model families.\nMarker subset: Leakage estimates depend on the marker set used to detect reasoning traces;\nusing only <think> can undercount leakage for some models compared to a broader marker set\n(e.g., Step/numbered patterns). This distinction concerns the leakage metric definition rather than\nthe efficacy of the intervention method.\nA.2\nMarker-free correctness-defined direction (Exp6)\nWe further test whether a direction defined without any surface-marker supervision can induce\nbehavioral changes under the same intervention operator. We define ucorr using correctness labels\nonly (correct vs. incorrect) on an MCQ training split under marker-forbidden, answer-only decoding\n(marker hit rate ≈0). We then apply rank-1 logit suppression z′ = z−γ(u⊤z)u and compare against\na matched random-direction control urand. Table A.2 reports accuracy under γ ∈{0, 10, 25, 40} on\ntwo MCQ benchmarks; we also observe that increasing γ increases option entropy and decreases the\n14\n"}, {"page": 15, "text": "Figure A.1: Gamma sweep for causal intervention. Increasing the suppression strength γ consistently\nreduces leakage and strict-fail rates across model families. The intervention is robust and controllable.\noption margin more strongly for ucorr than for urand, consistent with a marker-free modulation of\ndecision confidence.\nA.3\nLayer-wise alignment of LoRA updates\nWe analyze the layer-wise geometry of the PT and SFT LoRA updates by computing ∆W = BA\nfor each injected module and reporting (i) ∥∆W∥F and (ii) cosine similarity between vec(∆WPT)\nand vec(∆WSFT). Across layers, PT updates have consistently larger norms than SFT updates, and\nthe cosine similarity is generally small while increasing towards later layers, indicating partially\nmisaligned update directions that become more aligned in upper layers.\nA.4\nToy subspace-aware merge (proof-of-concept)\nAs a minimal proof-of-concept, we implement a layer-wise projection before merging: for each\nlayer we define a conflict direction from ∆WPT −∆WSFT, remove its component from ∆WSFT with\nstrength γ, and then merge. We compare against naive merge and a matched random-direction\nprojection control. While this toy setting is limited, the projection merge can improve accuracy\nand/or reduce leakage beyond the random control, suggesting that geometry-aware merging may\nmitigate interference.\n15\n"}, {"page": 16, "text": "Figure A.2: Robustness: Gamma sanity check (N=40). Leakage shows an overall decreasing trend\nas gamma increases.\nFigure A.3: Robustness: Marker-set sensitivity (gamma=1.0, N=40). Leakage varies with marker\ndefinitions; broader markers can capture additional trace patterns beyond <think>.\n16\n"}, {"page": 17, "text": "Setting\nγ=0\nγ=10\nγ=25\nγ=40\nMMLU (real ucorr)\n0.356\n0.388\n0.486\n0.546\nMMLU (rand urand)\n0.356\n0.356\n0.368\n0.386\nMedQA (real ucorr)\n0.332\n0.330\n0.364\n0.372\nMedQA (rand urand)\n0.332\n0.332\n0.330\n0.328\nTable A.2: Marker-free correctness-defined direction (Exp6, DeepSeek-R1-Distill-Qwen-7B, N=500).\nWe define ucorr from correctness labels only under marker-forbidden, answer-only decoding, then\napply rank-1 logit suppression with strength γ. Random-direction controls are matched by construc-\ntion. Results show accuracy gains at sufficiently large γ beyond the random control, alongside shifts\nin the option distribution (entropy/margin; described in text).\n(a) Cosine Similarity\n(b) Update Norms\nFigure A.4: Layer-wise geometric analysis of LoRA adapters.\nFigure A.5: Toy subspace-aware merge results. Projection merge (orange) vs Random control\n(green).\n17\n"}]}