{"doc_id": "arxiv:2601.02604", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.02604.pdf", "meta": {"doc_id": "arxiv:2601.02604", "source": "arxiv", "arxiv_id": "2601.02604", "title": "Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs", "authors": ["Cesar Felipe Martínez Cisneros", "Jesús Ulises Quiroz Bautista", "Claudia Anahí Guzmán Solano", "Bogdan Kaleb García Rivera", "Iván García Pacheco", "Yalbi Itzel Balderas Martínez", "Kolawole John Adebayoc", "Ignacio Arroyo Fernández"], "published": "2026-01-05T23:40:00Z", "updated": "2026-01-05T23:40:00Z", "summary": "The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.02604v1", "url_pdf": "https://arxiv.org/pdf/2601.02604.pdf", "meta_path": "data/raw/arxiv/meta/2601.02604.json", "sha256": "8e10227b18bc8870b0ce7052f8fe3ea4863435925c4c1139078a7a4b10ebf2bf", "status": "ok", "fetched_at": "2026-02-18T02:23:10.922650+00:00"}, "pages": [{"page": 1, "text": "2025 Mexican International Conference on Computer Science (ENC) & IEEE ICEV, Orizaba, Veracruz, M´exico\nScalable Construction of a Lung Cancer Knowledge\nBase: Profiling Semantic Reasoning in LLMs\nCesar Felipe Mart´ınez Cisnerosa, Jes´us Ulises Quiroz Bautistaa, Claudia Anah´ı Guzm´an Solanoa\nBogdan Kaleb Garc´ıa Riveraa, Iv´an Garc´ıa Pachecoa\nYalbi Itzel Balderas Mart´ınezb, Kolawole John Adebayoc, Ignacio Arroyo Fern´andeza\na Universidad Tecnol´ogica de la Mixteca\nb Instituto Nacional de Enfermedades Respiratorias (INER) Ismael Cos´ıo Villegas\nc Maynooth University\na Huajuapan de Le´on, Oaxaca, M´exico, b Ciudad de M´exico, M´exico, c Maynooth, Ireland\ncesarmtzcisne@gmail.com, qubj980830@gs.utm.mx, claudia.guzmansolano@gmail.com, bogdanrivera@gmail.com,\niaf@gs.utm.mx, ivan@gs.utm.mx, yalbibalderas@gmail.com, kolawole.adebayo@mu.ie\nAbstract—The integration of Large Language Models (LLMs)\ninto biomedical research offers new opportunities for domain-\nspecific reasoning and knowledge representation. However, their\nperformance depends heavily on the semantic quality of training\ndata. In oncology, where precision and interpretability are vital,\nscalable methods for constructing structured knowledge bases\nare essential for effective fine-tuning. This study presents a\npipeline for developing a lung cancer knowledge base using\nOpen Information Extraction (OpenIE). The process includes:\n(1) identifying medical concepts with the MeSH thesaurus; (2)\nfiltering open-access PubMed literature with permissive licenses\n(CC0); (3) extracting (subject, relation, object) triplets using\nOpenIE method; and (4) enriching triplet sets with Named Entity\nRecognition (NER) to ensure biomedical relevance. The resulting\ntriplet sets provide a domain-specific, large-scale, and noise-aware\nresource for fine-tuning LLMs. We evaluated T5 models fine-\ntuned on this dataset through Supervised Semantic Fine-Tuning.\nComparative assessments with ROUGE and BERTScore show\nsignificantly improved performance and semantic coherence,\ndemonstrating the potential of OpenIE-derived resources as\nscalable, low-cost solutions for enhancing biomedical NLP.\nIndex Terms—Large Language Models, Knowledge Base, Lung\nCancer, Supervised Semantic Fine-Tuning.\nI. INTRODUCTION\nThe rapid expansion of scientific knowledge, particularly\nin the biomedical domain, has created an urgent need for\ntools that facilitate efficient access to and understanding of\nlarge-scale information [1]. Large Language Models (LLMs)\nhave transformed Natural Language Processing (NLP) [2],\nproviding new opportunities for healthcare professionals and\nresearchers to engage with specialized literature.\nDespite these advances, LLMs face critical challenges re-\nlated to input data quality. A major limitation is the absence\nof high-coverage, accessible knowledge resources, which con-\nstrains knowledge-based Artificial Intelligence (AI) systems.\nAutomated data collection methods reduce time and cost but\noften propagate errors. Scaling model capacity mitigates some\nnoise, yet this strategy remains limited to applications tolerant\nof residual inaccuracies. Manual curation ensures high-quality\ndata but is prohibitively expensive and time-consuming, re-\nsulting in datasets with restricted coverage.\nKnowledge Graphs and Ontologies exemplify curated re-\nsources that can reduce hallucinations in LLMs. However, they\nreveal a trade-off across scalability, noise, flexibility, and preci-\nsion. While precise resources constrain hallucinations, they fail\nto support broader applications. Scalable resources, in contrast,\nenable wider coverage but inevitably contain noise, demanding\nadditional quality control. This paradox particularly challenges\npublic institutions and non-profits, which must maximize AI\nutility under resource constraints.\nTraditional approaches for knowledge base construction,\nsuch as relation extraction pipelines and curated graphs, also\nface limitations: the former are inflexible and annotation-\ndependent, while the latter are costly [3], [4]. Open Infor-\nmation Extraction (OpenIE) offers an alternative by providing\nautomated, low-cost, and scalable knowledge acquisition [5].\nUnlike schema-dependent methods, OpenIE processes large\nvolumes of text with extensive coverage, facilitating scalable\nknowledge generation.\nLLMs exhibit strong reasoning abilities, though the mecha-\nnisms remain unclear. A prevailing theory suggests these abili-\nties rely heavily on semantic structures implicitly learned from\ntraining data [6]. Consequently, lightweight, cost-effective\npipelines are needed to build large-scale knowledge bases,\nsupport small language models with robust reasoning, and\nprovide affordable means to evaluate semantic coherence.\nThis research addresses these challenges by proposing a\npipeline to construct a lightweight, domain-specific knowledge\nbase on lung cancer for semantic supervision. The pipeline\nemploys OpenIE to generate triplets and integrates filtering\nmechanisms to ensure biomedical relevance. To evaluate its\nimpact, we analyze the semantic reasoning abilities of LLMs\nfine-tuned with this knowledge base, specifically assessing\ntheir capacity to generate accurate object phrases from sub-\nject–relation pairs, simulating structured inference in biomedi-\ncal contexts. We evaluated T5 models fine-tuned on this dataset\nthrough Supervised Semantic Fine-Tuning. Comparative as-\nsessments with ROUGE and BERTScore show significantly\n© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\narXiv:2601.02604v1  [cs.CL]  5 Jan 2026\n"}, {"page": 2, "text": "improved performance and semantic coherence, demonstrating\nthe potential of OpenIE-derived resources as scalable, low-cost\nsolutions for enhancing biomedical reasoning.\nII. RELATED LITERATURE\nPrior work has focused on building medical knowledge\ngraphs using pattern-mining [7], electronic health records\n[8], and tensor factorization for comorbidity prediction [9].\nOther approaches utilize pipelines with tools like SciSpacy\nand BioBERT for specific tasks like protein-protein interaction\nextraction [10] or employ hybrid neural networks for relation\nextraction [11]. Our work is similar to [12] in its use of smaller\nmodels but differentiates itself by focusing on a scalable\nOpenIE pipeline to generate the knowledge base itself.\nIII. THEORETICAL BACKGROUND\nThe rapid evolution of AI and NLP has made it possible to\nprocess large amounts of information, and the quality of that\ninformation is crucial for working with LLMs ( [13], [14]).\nKnowledge resources, such as lexical databases like WordNet\n[15] and specialized biomedical ontologies like MeSH [16],\nalong with knowledge graphs like BioKG [17], are essential\nfor fine-tuning LLMs by providing high-quality data that\nreduces hallucinations and improves semantic accuracy [18].\nOpen Information Extraction (OpenIE) is a flexible and effi-\ncient alternative to conventional methods, as it does not require\npredefined schemas, offering greater recall and expressiveness\nin triplet extraction [4], [19]. While it may produce noisy\ntriplets, OpenIE is more economical and lowers barriers to\nresearch [5], [20], [21]. This method is crucial for developing\nrobust semantic reasoning, which involves inferring missing\nelements in subject-predicate-object triplets [22]–[24]. This\nprocess, known as a Knowledge Base Completion (KBC)\nproblem, is modeled using neural networks that learn dis-\ntributed phrase representations to predict an object given a\nsubject-predicate pair [22]–[24].\nIV. METHODOLOGY AND IMPLEMENTATION\nA. Methodology\nThe project’s methodology was divided into two phases:\nknowledge base acquisition and application and use of the\ndataset.\n• Knowledge base acquisition\n1) Corpus Acquisition: A corpus of approximately\n2,901,198 biomedical articles (190 GB) was down-\nloaded from the PubMed Central (PMC).\n2) Terminology and Filtering: Identified 5 key MeSH\nterms for ’lung cancer.’ Filtered the PMC corpus us-\ning a k-NN search, yielding 37,951 relevant articles.\n3) License Verification: Used the NCBI API to filter\nfor articles with permissive license (CC0), resulting\nin a final set of 2,967 articles.\n4) Triplet Extraction: Processed the licensed articles\nwith the Stanford CoreNLP OpenIE module to ex-\ntract 7.5 million (Subject, Relation, Object) triplets\nfrom CC0-licensed articles.\n5) Biomedical Entity Filtering: Employed a ”zero-\nshot” Hugging Face model for Named Entity Recog-\nnition (NER) to score triplets. We retained only\nthose where both the subject and object had a >80%\nprobability of being a biomedical entity, reducing\nthe dataset to 13k high-quality triplets.\n• Application and use of the dataset\nTo validate the dataset, we conducted a comprehensive\nevaluation of two T5 model variants: a base T5 model\nand a PubMed fine-tuned model with LoRa layers trained\non PubMed abstract summarization datasets. The evalu-\nation framework comprised three main methodological\ncomponents:\n1) Data\npreprocessing: We created both filtered\n(biomedical entity probability >80%) and unfiltered\ndatasets, followed by randomization and partition-\ning into training (10,000 triplets), testing (1,000\ntriplets), and validation (200 triplets) sets stored in\nCSV format.\n2) Model fine-tuning and evaluation: We imple-\nmented fine-tuning using the Trainer class with\nrespective tokenizers (T5Tokenizer for T5-base; T5\nTokenizer Fast for T5-PubMed). Both models were\nevaluated in zero-shot inference and post-fine-tuning\nscenarios using BERTScore (precision, recall, F1-\nscore) and ROUGE metrics on the validation set.\n3) Results analysis: We performed comparative eval-\nuation through statistical tables and visualization\nplots (histograms and KDE) to assess performance\ndifferences and the impact of randomness on F1-\nscore values across filtered and unfiltered condi-\ntions.\nAll experiments were conducted on commodity hardware\nfeaturing dual RTX-4090 GPUs with 24GB VRAM and 32GB\nCPU RAM.1\nFor the training configuration, a maximum sequence length\nof 50 tokens was utilized. Models were trained for a total\nof 4 epochs employing the Adafactor optimizer. A per-device\nbatch size of 50 was applied for both training and evaluation.\nThe evaluation, checkpoint saving, and logging strategies were\nuniformly set to “epoch”. Experiment metrics were tracked\nusing Weights & Biases. To ensure optimal performance, the\nbest model was identified based on the lowest evaluation loss\nand automatically loaded upon completion of training.\nV. RESULTS AND DISCUSSION\nThe pipeline filtering stages progressed from an initial acqui-\nsition of 2.9 million articles down to a final result of 13,000\nlung cancer knowledge triplets, as shown in Table I.\nThe\ncorpus\nacquisition\nstage\ngathered\ncomprehensive\nbiomedical literature, followed by terminology-based filtering\nthat reduced the dataset to 37.9K relevant articles. License\nverification ensured compliance with usage rights, resulting\n1Model details: T5-Base https://huggingface.co/google-t5/t5-base; T5-\nPubMed https://huggingface.co/Kevincp560/t5-base-finetuned-pubmed\n"}, {"page": 3, "text": "in 2.9K articles suitable for processing. The triplet extraction\nphase generated 7.5M candidate triplets from these articles.\nFinally, biomedical entity filtering refined the results to 13K\nhigh-quality knowledge triplets from 1.4K articles.\nTABLE I: Summary of the Knowledge Triplet Extraction\nProcess\nPipeline Stage\nArticles\nTriplets\nCorpus Acquisition\n2.9M\n–\nTerminology and Filtering\n37.9K\n–\nLicense Verification\n2.9K\n–\nTriplet Extraction\n2.9K\n7.5M\nBiomedical Entity Filtering\n1.4K\n13K\nThe results presented herein are the most representative of\nthe 8 experiments conducted in total. Although the remaining\nexperiments yielded plots similar to those shown in Figure 1,\nonly the most significant findings were included for brevity.\nThe omitted experiments exhibited behavior analogous to\nFigure 1a, wherein no discernible difference between the two\ndistributions was observed. However, this changed signifi-\ncantly with the use of filtered data, which produced plots\nsimilar to Figure 1b, illustrating a clear divergence in results.\nThe experiments not included in the main paper encompass the\ntraining of the T5-base model on both filtered and unfiltered\ndata, as well as the training of the T5-pubmed model under\nthese same two data conditions.\n1) Differential Metric Sensitivity to Semantic Fine-tuning:\nThe most striking finding is the differential response of\nevaluation metrics to semantic fine-tuning. ROUGE metrics\nexhibit dramatic improvements (see Table II): ROUGE-1 F1-\nScore increases from 0.0305 to 0.2003 for T5-Base (∼657%\nimprovement) and from 0.0274 to 0.2025 for T5-PubMed\n(∼739% improvement). Similarly, ROUGE-2 F1-Score shows\nincreases of 1275% and 1583% respectively. In contrast,\nBertScore F1-Score improvements are more moderate: 21%\nfor T5-Base (0.6289 to 0.7606) and 19% for T5-PubMed\n(0.6363 to 0.7582). See Table III. These comparisons were\nobserved using our NER probability filtered versions of the\ndata, which showed substantially better results compared to\nnon-filtered data where ROUGE-1 F1-Scores reached only\n0.0651 (T5-Base) and 0.0694 (T5-PubMed) after fine-tuning,\nand BertScore F1-Scores plateaued at 0.6892 (T5-Base) and\n0.6944 (T5-PubMed) respectively under non-filtered condi-\ntions.\nThis differential sensitivity reflects fundamental metric dif-\nferences: ROUGE measures lexical overlap, directly benefiting\nfrom structured triplet training and precise biomedical termi-\nnology learning, while BertScore captures semantic similarity\nthrough contextual embeddings, yielding higher baseline val-\nues but modest improvements. However, low ROUGE scores\nmay not indicate poor performance, as later verified using a\nMeaning-Based Selectional Preference Test (MSPT) [12].\n2) Model Convergence Following Domain-Specific Fine-\ntuning: A remarkable pattern emerges in the post fine-tuning\nperformance: T5-Base and T5-PubMed models converge to\nnearly identical results across all metrics. For BertScore F1,\nTABLE II: ROUGE Evaluation Results of T5-base models\nunder zero-shot inference and semantic fine-tuning with NER-\nprobability filtering\nMetric\nComponent\nT5-Base\nT5-Base\n(Fine-\ntuned)\nT5-\nPubMed\nT5-\nPubMed\n(Fine-\ntuned)\nROUGE-1\nPrecision\n0.1023\n0.2077\n0.1018\n0.2095\nRecall\n0.0197\n0.2227\n0.0165\n0.2265\nF1-Score\n0.0305\n0.2003\n0.0274\n0.2025\nROUGE-2\nPrecision\n0.0293\n0.1003\n0.0293\n0.1008\nRecall\n0.0047\n0.1019\n0.0034\n0.1012\nF1-Score\n0.0073\n0.0931\n0.0059\n0.0934\nROUGE-L\nPrecision\n0.1023\n0.2025\n0.1018\n0.2030\nRecall\n0.0197\n0.2181\n0.0165\n0.2208\nF1-Score\n0.0305\n0.1956\n0.0274\n0.1967\nTABLE III: BertScore Evaluation Results of T5-base models\nunder zero-shot and semantic fine-tuning with filtered data\nComponent\nT5-Base\nT5-Base\n(Fine-tuned)\nT5-PubMed\nT5-PubMed\n(Fine-tuned)\nPrecision\n0.5925\n0.7668\n0.6049\n0.7629\nRecall\n0.6722\n0.7562\n0.6727\n0.7554\nF1-Score\n0.6289\n0.7606\n0.6363\n0.7582\nthe difference narrows from 0.0074 (pre-training) to 0.0024\n(post fine-tuning). Similarly, ROUGE metrics show conver-\ngence, with ROUGE-1 F1 differences reducing from 0.0031\nto 0.0022. This convergence suggests that semantic fine-tuning\non domain-specific triplets acts as a powerful equalizing mech-\nanism. The structured nature of the OpenIE-derived knowledge\nbase provides explicit biomedical concept associations that\noverride the advantages of domain-specific pre-training, at\nleast in the case of biomedical text summarization. This find-\ning has important implications for practical applications: re-\nsearchers may achieve comparable performance using general-\npurpose models with targeted semantic fine-tuning rather than\ninvesting in domain-specific pretraining, reducing computa-\ntional costs and complexity. The convergence also indicates\nthat the triplet-based supervision provides sufficiently rich\nsemantic information to reshape model representations toward\ndomain-specific requirements, regardless of initial training\ndomain biases.\n3) Precision-Recall Rebalancing in ROUGE Metrics: The\nROUGE metrics reveal a significant rebalancing between\nprecision and recall components following fine-tuning. In\nthe zero-shot condition, ROUGE-1 precision (0.1023 for T5-\nBase) substantially exceeds recall (0.0197), creating a 5.2:1\nprecision-to-recall ratio. Post fine-tuning, this ratio dramat-\nically shifts to near parity: precision (0.2077) and recall\n(0.2227) achieve a balanced 0.93:1 ratio (see Table II). This\nrebalancing reflects a fundamental shift in model behavior.\nPre-fine-tuning models generate conservative outputs with\nlimited lexical overlap (low recall) but what they do gen-\nerate tends to be relevant (relatively higher precision). The\npoor recall suggests models struggle to produce the specific\nbiomedical terminology required for the task. Semantic fine-\ntuning addresses this limitation by teaching models to generate\n"}, {"page": 4, "text": "longer, more comprehensive outputs that better match refer-\nence texts while maintaining relevance. The improvement in\nrecall indicates that triplet-based training successfully expands\nthe model’s active biomedical vocabulary, enabling generation\nof domain-specific terms and phrases that were previously\ninaccessible, even with biomedical summarization fine-tuning.\nThis is particularly crucial for biomedical applications where\nprecise terminology and comprehensive coverage of relevant\nconcepts are essential for clinical utility.\nBoth for filtered and unfiltered data, fine-tuning improves\nthe scores compared to the zero-shot version. With unfil-\ntered triplets, the ROUGE metrics remain low, particularly\nROUGE-2, while BertScore reflects marginal gains. With fil-\ntered triplets, fine-tuning with only 10,000 examples enhances\nboth BertScore and ROUGE performance metrics. The jump\nin precision and F1 is remarkable, especially for T5-Base.\nUndoubtedly, filtering by probability of biomedical entities\nimproves the results. Notably, a slight superiority of the base\nmodel over T5-PubMed was observed in the ROUGE metric\nprior to fine-tuning. However, this trend reversed after fine-\ntuning, with the T5-PubMed model performing marginally\nbetter.\nThe improvement between using filtered and non-filtered\ndata was not evident from purely seeing at BERTscores\n(relatively high values in both cases). However, by seeing\nat Figure 1 this view is completely changed. To assess the\nstatistical significance of these improvements, we compared\nF1 BERTscores under two conditions: (1) model predictions\nevaluated against correct expected outputs, and (2) model\npredictions evaluated against randomized expected outputs\n(MSPT) (Table IV). This methodology enables us to determine\nwhether the models have acquired semantically meaningful\nknowledge rather than generating random meanings with in-\nflated BERTScores. These results are also summarized in Table\nV, which additionally include the p-value and gap values (the\ndifference between means) for tests conducted with the T5\nmodel using filtered data and the T5-PubMed model using\nunfiltered data. These values were similar to those obtained\nwith the T5-PubMed and T5 models (Figure 1).\nClear patterns are identified: a) Fine-tuning consistently\nhelps to generate relevant biomedical meanings. b) NER\nfiltering contributes more quality than quantity; despite using a\nmuch smaller dataset (11,200 triplets), the results with filtered\ntriplets are superior (p < 3 × 10−19). c) T5-PubMed does\nnot always clearly outperform T5-Base; the results show that\nthe difference is small, and in some cases, fine-tuned T5-Base\neven matches or slightly surpasses it; however, MSPT showed\nthe most significant (though small) improvement with Pubmed\nfine-tuning (p < 10−24). Regarding the low performance\nwith unfiltered data, we think that noise and ambiguity in\nthe unfiltered triplets likely hinder learning. Filtered data with\nNER probability performs better because the filtering removed\nnoise and redundancies, leaving more reliable noun phrases.\nThe small differences between T5-Base and T5-PubMed may\nbe due to the task (object prediction in triplets) being very\ndifferent from T5-PubMed’s biomedical pre-training. Finally,\nthe contrast between BertScore and ROUGE is explained by\nthe former capturing phrase meaning, while the latter depends\non exact token matches.\nTABLE IV: BERTScore Analysis of T5-base and T5-PubMed:\nZero-Shot versus Fine-Tuned Performance with Filtered Ran-\ndomized Gold Labels\nT5-Base\nT5-Base\n(Fine-tuned)\nT5-PubMed\nT5-PubMed\n(Fine-tuned)\nMetric\nFiltered\nFiltered\nFiltered\nFiltered\nPrecision\n0.5726\n0.6838\n0.5826\n0.6808\nRecall\n0.6400\n0.6757\n0.6409\n0.6744\nF1-Score\n0.6039\n0.6790\n0.6099\n0.6768\nMetric\nUnfiltered\nUnfiltered\nUnfiltered\nUnfiltered\nPrecision\n0.5431\n0.6570\n0.5477\n0.6621\nRecall\n0.6296\n0.6545\n0.6312\n0.6577\nF1-Score\n0.5824\n0.6546\n0.5857\n0.6588\nTABLE V: Comparison of models with filtered and unfiltered\ndata pre and post fine-tuning\nT5-base\nT5-Pubmed\nMetric\nUnfiltered\nFiltered\nUnfiltered\nFiltered\np-value (pre-FT)\n0.0049\n5.49e-08\n0.0085\n1.18e-08\np-value (post-FT)\n0.0016\n3.01e-19\n2.45e-06\n1.14e-24\np-value difference\n0.0033\n5.49e-08\n0.00849755\n1.18e-08\nGap pre-FT (%)\n2.00\n4.06\n1.96\n4.24\nGap post-FT (%)\n5.15\n11.33\n5.27\n11.34\nGap Increase (%)\n3.15\n7.27\n3.31\n7.10\nVI. CONCLUSIONS\nThe differential metric responses highlight mechanisms be-\nhind semantic fine-tuning effectiveness. Substantial ROUGE\ngains show that OpenIE-derived triplets effectively teach lex-\nical and syntactic patterns in biomedical discourse, reflecting\ntheir structured ability to encode relations between entities us-\ning natural language. More moderate BertScore improvements\nindicate that while semantic coherence improves, contextual\nsimilarity gains are less pronounced. This suggests that deeper\nsemantic understanding in specialized domains may require\nadditional supervision beyond triplet-based learning, including\ndiscourse-level coherence or causal reasoning. The conver-\ngence findings also carry practical implications: biomedical\nNLP projects may achieve competitive performance by com-\nbining general-purpose models with targeted semantic fine-\ntuning, optimizing resource allocation without always requir-\ning domain-specific pretraining.\nFor future work, we plan to assess the generalizability\nof our approach by applying it to corpora related to other\ndiseases beyond lung cancer. We also intend to explore and\ncompare more advanced Open Information Extraction (Open\nIE) methods. The baseline system used in this study may\nhave performance limitations; thus, investigating state-of-the-\nart Open IE techniques could significantly enhance triplet\nquality. In conjunction with this, integrating methods to es-\ntimate triplet factuality is a crucial step for improving the\nveracity of the extracted information. Furthermore, expanding\nthe training corpus using permissively licensed data sources\n"}, {"page": 5, "text": "(a) T5-Pubmed (Pre-finetuning)\n(b) T5-Pubmed (Post-finetuning)\nFig. 1: F1-BERTscore distributions for filtered data, show-\ning model performance before (left column) and after (right\ncolumn) fine-tuning. Each subplot compares two score distri-\nbutions calculated over 200 samples: (i) actual performance\n(orange curve), and (ii) random baseline (blue curve).\nwould likely yield significant performance gains by providing\na larger volume of training examples.\nSubsequent experiments will involve a multi-stage train-\ning strategy, entailing pre-training on large-scale, general-\npurpose datasets (such as ConceptNet or SNLI) that address\nanalogous tasks, prior to fine-tuning on our domain-specific\ncorpus. Finally, we plan to conduct a comprehensive analysis\ncomparing different model architectures and sizes, as well as\nvarying dataset scales, to thoroughly evaluate the robustness\nand scalability of our fine-tuning methodology.\nVII. ACKNOWLEDGMENTS\nWe express our deepest and most sincere gratitude to the\nSecretariat of Science, Humanities, Technology and Innovation\n(SECIHTI) for the trust placed in us and the valuable funding\ngranted to our project with key code CF-2023-I-2854. This\ncrucial support not only ensures the viability of our research\nbut also drives the advancement of the frontier of knowledge\nin our field, allowing us to work on solutions that aspire to\nhave a significant impact.\nREFERENCES\n[1] L. Harper, J. Campbell, E. K. Cannon, S. Jung, M. Poelchau, R. Walls,\nC. Andorf, E. Arnaud, T. Z. Berardini, C. Birkett et al., “Agbiodata\nconsortium recommendations for sustainable genomics and genetics\ndatabases for agriculture,” Database, vol. 2018, p. bay088, 2018.\n[2] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language\nprocessing via large pre-trained language models: A survey,” ACM\nComputing Surveys, vol. 56, no. 2, pp. 1–40, 2023.\n[3] B. Jehangir, S. Radhakrishnan, and R. Agarwal, “A survey on named en-\ntity recognition—datasets, tools, and methodologies,” Natural Language\nProcessing Journal, vol. 3, p. 100017, 2023.\n[4] Y. Xu, M.-Y. Kim, K. M. Quinn, R. Goebel, and D. Barbosa, “Open\ninformation extraction with tree kernels,” in Proceedings of the 2013\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 2013, pp.\n868–877.\n[5] J. Romero and S. Razniewski, “Mapping and cleaning open common-\nsense knowledge bases with generative translation,” in International\nSemantic Web Conference.\nSpringer, 2023, pp. 368–387.\n[6] X. Tang, Z. Zheng, J. Li, F. Meng, S.-C. Zhu, Y. Liang, and M. Zhang,\n“Large language models are in-context semantic reasoners rather than\nsymbolic reasoners,” arXiv preprint arXiv:2305.14825, 2023.\n[7] L. Shi, S. Li, X. Yang, J. Qi, G. Pan, and B. Zhou, “Semantic\nhealth knowledge graph: semantic integration of heterogeneous medical\nknowledge and services,” BioMed research international, vol. 2017,\nno. 1, p. 2858423, 2017.\n[8] L. Li, P. Wang, J. Yan, Y. Wang, S. Li, J. Jiang, Z. Sun, B. Tang, T.-\nH. Chang, S. Wang et al., “Real-world data medical knowledge graph:\nconstruction and applications,” Artificial intelligence in medicine, vol.\n103, p. 101817, 2020.\n[9] S. Biswas, P. Mitra, and K. S. Rao, “Relation prediction of co-morbid\ndiseases using knowledge graph completion,” IEEE/ACM Transactions\non Computational Biology and Bioinformatics, vol. 18, no. 2, pp. 708–\n717, 2019.\n[10] J. Parmar, W. Koehler, M. Bringmann, K. S. Volz, and B. Kapicioglu,\n“Biomedical information extraction for disease gene prioritization,”\narXiv preprint arXiv:2011.05188, 2020.\n[11] J. Wu, R. Zhang, T. Gong, Y. Liu, C. Wang, and C. Li, “Bioie:\nBiomedical information extraction with multi-head attention enhanced\ngraph convolutional network,” in 2021 IEEE International Conference\non Bioinformatics and Biomedicine (BIBM).\nIEEE, 2021, pp. 2080–\n2087.\n[12] Y. I. Balderas-Mart´ınez, J. A. S´anchez-Rojas, A. T´ellez-Vel´azquez,\nF. Ju´arez Mart´ınez, R. Cruz-Barbosa, E. Guzm´an-Ram´ırez, I. Garc´ıa-\nPacheco, and I. Arroyo-Fern´andez, “Semantic reasoning using standard\nattention-based models: An application to chronic disease literature,” Big\nData and Cognitive Computing, vol. 9, no. 6, p. 162, 2025.\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” in Pro-\nceedings of the 2019 conference of the North American chapter of the\nassociation for computational linguistics: human language technologies,\nvolume 1 (long and short papers), 2019, pp. 4171–4186.\n[14] R. Rejeleene, X. Xu, and J. Talburt, “Towards trustable language\nmodels: Investigating information quality of large language models,”\narXiv preprint arXiv:2401.13086, 2024.\n[15] G. A. Miller, “Wordnet: a lexical database for english,” Communications\nof the ACM, vol. 38, no. 11, pp. 39–41, 1995.\n[16] R. FB, “Medical subject headings.” Bulletin of the Medical Library\nAssociation, vol. 51, pp. 114–116, 1963.\n[17] B. Walsh, S. K. Mohamed, and V. Nov´aˇcek, “Biokg: A knowledge\ngraph for relational learning on biological data,” in Proceedings of\nthe 29th ACM International Conference on Information & Knowledge\nManagement, 2020, pp. 3173–3180.\n[18] T. Baldazzi, L. Bellomarini, S. Ceri, A. Colombo, A. Gentili, and\nE. Sallinger, “Fine-tuning large enterprise language models via on-\ntological reasoning,” in International Joint Conference on Rules and\nReasoning.\nSpringer, 2023, pp. 86–94.\n"}, {"page": 6, "text": "[19] S. Sheikhalishahi, R. Miotto, J. T. Dudley, A. Lavelli, F. Rinaldi, and\nV. Osmani, “Natural language processing of clinical notes on chronic\ndiseases: systematic review,” JMIR medical informatics, vol. 7, no. 2, p.\ne12239, 2019.\n[20] S. Ji, S. Pan, E. Cambria, P. Marttinen, and P. S. Yu, “A survey on\nknowledge graphs: Representation, acquisition, and applications,” IEEE\ntransactions on neural networks and learning systems, vol. 33, no. 2,\npp. 494–514, 2021.\n[21] F. Mesquita, J. Schmidek, and D. Barbosa, “Effectiveness and efficiency\nof open relation extraction,” in Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing, 2013, pp. 447–457.\n[22] T. Fang, W. Wang, S. Choi, S. Hao, H. Zhang, Y. Song, and B. He,\n“Benchmarking commonsense knowledge base population with an ef-\nfective evaluation dataset,” arXiv preprint arXiv:2109.07679, 2021.\n[23] M.-T.\nLuong,\nH.\nPham,\nand\nC.\nD.\nManning,\n“Effective\nap-\nproaches to attention-based neural machine translation,” arXiv preprint\narXiv:1508.04025, 2015.\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n"}]}